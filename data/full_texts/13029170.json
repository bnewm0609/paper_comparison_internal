{"id": 13029170, "updated": "2023-12-11 00:40:57.479", "metadata": {"title": "\u201cWhy Should I Trust You?\u201d: Explaining the Predictions of Any Classifier", "authors": "[{\"first\":\"Marco\",\"last\":\"Ribeiro\",\"middle\":[]},{\"first\":\"Sameer\",\"last\":\"Singh\",\"middle\":[]},{\"first\":\"Carlos\",\"last\":\"Guestrin\",\"middle\":[]}]", "venue": "NAACL", "journal": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations", "publication_date": {"year": 2016, "month": null, "day": null}, "abstract": "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": null, "mag": "2951501516", "acl": "N16-3020", "pubmed": null, "pubmedcentral": null, "dblp": "conf/naacl/Ribeiro0G16", "doi": "10.18653/v1/n16-3020"}}, "content": {"source": {"pdf_hash": "c0883f5930a232a9c1ad601c978caede29155979", "pdf_src": "Grobid", "pdf_uri": "[\"https://arxiv.org/pdf/1602.04938v3.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/N16-3020.pdf", "status": "HYBRID"}}, "grobid": {"id": "59aac4e3a30e72b9633c0ea2667187e78c5f2420", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/c0883f5930a232a9c1ad601c978caede29155979.txt", "contents": "\n\"Why Should I Trust You?\" Explaining the Predictions of Any Classifier\nJune 12-17, 2016\n\nMarco Tulio Ribeiro marcotcr@cs.uw.edu \nUniversity of Washington Seattle\n98105WAUSA\n\nSameer Singh sameer@cs.uw.edu \nUniversity of Washington Seattle\n98105WAUSA\n\nCarlos Guestrin guestrin@cs.uw.edu \nUniversity of Washington Seattle\n98105WAUSA\n\n\"Why Should I Trust You?\" Explaining the Predictions of Any Classifier\n\nProceedings of NAACL-HLT 2016 (Demonstrations)\nNAACL-HLT 2016 (Demonstrations)San Diego, CaliforniaJune 12-17, 2016\nDespite widespread adoption in NLP, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust in a model. Trust is fundamental if one plans to take action based on a prediction, or when choosing whether or not to deploy a new model. In this work, we describe LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner. We further present a method to explain models by presenting representative individual predictions and their explanations in a non-redundant manner. We propose a demonstration of these ideas on different NLP tasks such as document classification, politeness detection, and sentiment analysis, with classifiers like neural networks and SVMs. The user interactions include explanations of free-form text, challenging users to identify the better classifier from a pair, and perform basic feature engineering to improve the classifiers.\n\nIntroduction\n\nMachine learning is at the core of many recent advances in natural language processing. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models into products that need to be shipped, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e. whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed \"in the wild\". Both are directly impacted by how much the human understands a model's behavior, as opposed to seeing it as a black box. Recent resurgence of neural networks has resulted in state-of-art models whose working is quite opaque to the user, exacerbating this problem.\n\nA common surrogate for ascertaining trust in a model is to evaluate accuracy on held-out annotated data. However, there are several ways this evaluation can go wrong. Data leakage, for example, defined as the unintentional leakage of signal into the training (and validation) data that would not occur in the wild (Kaufman et al., 2011), potentially increases accuracy. Practitioners are also known to overestimate the accuracy of their models based on cross validation (Patel et al., 2008), as real-world data is often significantly different. Another particularly hard to detect problem is dataset shift (Candela et al., 2009), where training data is different than test data. Further, there is frequently a mismatch between that metrics that we can compute and optimize (e.g. accuracy) and the actual metrics of interest such as user engagement and retention. A practitioner may wish to choose a less accurate model for content recommendation that does not place high importance in features related to \"clickbait\" articles (which may hurt user retention), even if exploiting such features increases the accuracy of the model in cross validation.\n\nIn this paper, we describe a system that explains why a classifier made a prediction by identifying useful portions of the input. It has been observed that providing an explanation can increase the acceptance of computer-generated movie recommendations (Herlocker et al., 2000) and other automated systems (Dzindolet et al., 2003), and we explore their utility for NLP. Specifically, we present:\n\n\u2022 LIME, an algorithm that can explain the predictions of any classifier, by approximating it locally with an interpretable model.\n\n\u2022 SP-LIME, a method that selects a set of representative explanations to address the \"trusting the model\" problem, via submodular optimization.\n\n\u2022 A demonstration designed to present the benefits of these explanation methods, on multiple NLP classification applications, classifier algorithms, and trust-related tasks.\n\n\nExplaining Predictions and Models\n\nBy \"explaining a prediction\", we mean presenting visual artifacts that provide qualitative understanding of the relationship between the instance's components (e.g. words in text) and the model's prediction. Explaining predictions is an important aspect in getting humans to trust and use machine learning effectively, provided the explanations are faithful and intelligible. We summarize the techniques here; further details and experiments are available in Ribeiro et al. (2016).\n\n\nLocal Interpretable Model-Agnostic Explanations\n\nWe present Local Interpretable Model-agnostic Explanations (LIME). The overall goal of LIME is to identify an interpretable model over the interpretable representation that is locally faithful to predictions of any classifier. It is important to distinguish between features and interpretable data representations, the latter is a representation that is understandable to humans, regardless of the actual features used by the model. A possible interpretable representation for text is a binary vector indicating the presence or absence of a word, even though the classifier may use more complex (and incomprehensible) features such as word embeddings. We denote x \u2208 R d as the original instance, and x \u2208 {0, 1} d to denote a binary vector for its interpretable representation. Formally, we define an explanation as a model g \u2208 G, where G is the class of linear models, such that g(z ) = w g \u00b7 z Note that g acts over absence/presence of the interpretable components, i.e. we can readily present it to the user with visual artifacts. We let \u2126(g) be a measure of complexity (as opposed to interpretability) of the explanation g \u2208 G.\n\nFor text classification, we set a limit K on the number of words included, i.e.\n\u2126(g) = \u221e1[ w g 0 > K].\nLet the model being explained be denoted f , i.e. f (x) is the probability (or a binary indicator) that x belongs to a certain class. We further use \u03a0 x (z) as a proximity measure between an instance z to x, so as to define locality around x. Finally, let L(f, g, \u03a0 x ) be a measure of how unfaithful g is in approximating f in the locality defined by \u03a0 x . We use the locally weighted square loss as L, as defined in Eq. (1), where we let \u03a0 x (z) = exp(\u2212D(x, z) 2 /\u03c3 2 ) be an exponential kernel on cosine distance D.\nL(f, g, \u03a0 x ) = z,z \u2208Z \u03a0 x (z) f (z) \u2212 g(z ) 2(1)\nIn order to ensure both interpretability and local fidelity, we minimize L(f, g, \u03a0 x ) while having \u2126(g) be low enough to be interpretable by humans.\n\u03be(x) = argmin g\u2208G L(f, g, \u03a0 x ) + \u2126(g) (2)\nWe approximate L(f, g, \u03a0 x ) by drawing samples, weighted by \u03a0 x . Given a sample z \u2208 {0, 1} d (which contains a fraction of the nonzero elements of x ), we recover the sample in the original representation z \u2208 R d and obtain f (z), which is used as a label for the explanation model. Given this dataset Z of perturbed samples with the associated labels, we optimize Eq.\n\n(2) to get an explanation \u03be(x) by first selecting K features with Lasso (Efron et al., 2004), forward selection or some other method, and then learning the weights via least squares.\n\n\nSubmodular Pick for Explaining Models\n\nAlthough an explanation of a single prediction provides some understanding into the reliability of the classifier to the user, it is not sufficient to evaluate and assess trust in the model as a whole. We propose to give a global understanding of the model by explaining a set of individual instances. Even though explanations of multiple instances can be insightful, these instances need to be selected judiciously, since users may not have the time to examine a large number of explanations. We represent the number of explanations humans are willing to look at a budget B, i.e. given a set of instances X, we select B explanations for the user to inspect. We construct an n \u00d7 d explanation matrix W that represents the local importance of the interpretable components for each instance, i.e. for an instance x i and explanation g i = \u03be(x i ), we set W i = |w g i |. Further, for each component j in W, we let I j denote the global importance, I j = n i=1 W ij . While we want to pick instances that cover the important components, the set of explanations must not be redundant in the components they show the users, i.e. avoid selecting instances with similar explanations. We formalize this non-redundant coverage intuition in Eq. (3), where coverage C, given W and I, computes the total importance of the features that appear in at least one instance in a set V .\nC(V, W, I) = d j=1 1 [\u2203i\u2208V :W ij >0] I j (3)\nThe pick problem thus consists of finding the set V, |V | \u2264 B that achieves highest coverage.\nP ick(W, I) = argmax V,|V |\u2264B C(V, W, I) (4)\nThe problem in Eq. (4) is maximizing a weighted coverage function, and is NP-hard (Feige, 1998). Due to submodularity, a greedy algorithm that iteratively adds the instance with the highest coverage gain offers a constant-factor approximation guarantee of 1 \u2212 1/e to the optimum (Krause and Golovin, 2014).\n\n\nDemo Outline\n\nUsing this explanation system that is capable of providing visual explanations for predictions of any classifier, we present an outline for a demonstration using different NLP tasks, models, and user interactions.\n\nThe complete source code and documentation for installing and running the demonstration is available at https://github. com/uw-mode/naacl16-demo, which uses the code for explaining classifiers available as an open-source python implementation at https:// github.com/marcotcr/lime.\n\n\nApplications\n\nWe explore three NLP classification tasks, which differ in the types of text they apply to and predicted cat-egories: politeness detection for sentences (Danescu-Niculescu-Mizil et al., 2013), multi-class content classification for documents (20 newsgroups data), and sentiment analysis of sentences from movie reviews (Socher et al., 2013). We explore classifiers for these tasks that vary considerably in their underlying representation, such as LSTMs (Wieting et al., 2015), SVMs, and random forests, trained on bag of words or on word embeddings.\n\nWe will outline the specific user interactions using a running example of the 20 newsgroups dataset, and the interfaces for the other applications will look similar. In particular, we focus on differentiating between \"Christianity\" from \"Atheism\", and use an SVM with an RBF kernel here. Although this classifier achieves 94% held-out accuracy, and one would be tempted to trust it based on this, the explanation for an instance shows that predictions are made for quite arbitrary reasons (words \"Posting\", \"Host\" and \"Re\" have no connection to either Christianity or Atheism).\n\n\nExplaining Individual Predictions\n\nThe first part of the demo focuses on explaining individual predictions. Given an instance that the user either selects from the dataset or writes their own piece of text, we provide the set of words that are important for the prediction according to the classifier of their choosing. The interface for the user is shown in Figure 1, which embedded in an iPython notebook. For most datasets and classifiers, our current system can produce an explanation in under three seconds, and some simple further optimizations can be used to produce near-instant explanations.\n\n\nExplaining and Comparing Models\n\nFor the second part of the demonstration, we provide explanations of different models in order to compare them, based on explanations of a few selected instances. The interface presents the explanations one at a time, similar to that in Figure 1. By default the instances are selected using our submodular approach (SP-lime), however we also allow users to write their own text as well, and produce explanations for the classifiers for comparison.\n\n\nImproving Classifiers\n\nAs the final demonstration, we consider a simple version of feature engineering. Specifically, we initiate Figure 1: Example explanation for an instance of document classification. The bar chart represents the importance given to the most relevant words by the classifier, also highlighted in the text. Color indicates which class the word is important for (orange for \"Christianity\", blue for \"Atheism\").\n\nFigure 2: Interface for feature cleaning, a simple version of feature engineering where users select words to remove from the model by clicking on them (indicated by red, struck-out text). Here, green bars indicate importance of the word for \"Christianity\", and magenta \"Atheism\". each user with a classifier trained using all features, including both noisy and correct ones. We then show explanations of the classifier to the users, and ask them to select which words to remove from the classifier (see Figure 2 for the interface). Given this feedback, we retrain the classifier and provide the users with a score of how well their classifier performed on a hidden set, along with a leader board of the accuracy of all the participants.\n\n\nConclusions\n\nWe argue that trust is crucial for effective human interaction with machine learning based NLP systems, and that explaining individual predictions is important in assessing trust. We present a demonstration for LIME, a modular and extensible approach to faithfully explain the predictions of any model in an interpretable manner, and SP-LIME, a method to select representative and non-redundant explanations, providing a global view of the model to users. The user interactions on multiple applications and classifiers span a variety of trust-related tasks: getting insights into predictions, deciding between models, and improving untrustworthy models.\n AcknowledgmentsWe would like to thank Kevin Gimpel, John Wieting, and Cristian Danescu-Niculescu-Mizil for making their classifiers and datasets available for use. This work was supported in part by the TerraSwarm Research Center, one of six centers supported by the STARnet phase of the Focus Center Research Program (FCRP) a Semiconductor Research Corporation program sponsored by MARCO and DARPA.\n. J Candela, M Sugiyama, A Schwaighofer, N D Lawrence, Dataset Shift in Machine Learning. MIT. J. Qui\u00f1onero Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. 2009. Dataset Shift in Machine Learning. MIT.\n\nA computational approach to politeness with application to social factors. Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure Leskovec, Christopher Potts, Proceedings of ACL. ACLCristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure Leskovec, and Christopher Potts. 2013. A computational approach to politeness with applica- tion to social factors. In Proceedings of ACL.\n\nThe role of trust in automation reliance. Mary T Dzindolet, Scott A Peterson, Regina A Pomranky, Linda G Pierce, Hall P Beck, Int. J. Hum.-Comput. Stud. 658Mary T. Dzindolet, Scott A. Peterson, Regina A. Pom- ranky, Linda G. Pierce, and Hall P. Beck. 2003. The role of trust in automation reliance. Int. J. Hum.- Comput. Stud., 58(6).\n\nLeast angle regression. Trevor Bradley Efron, Iain Hastie, Robert Johnstone, Tibshirani, Annals of Statistics. 32Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. 2004. Least angle regression. Annals of Statistics, 32:407-499.\n\nA threshold of ln n for approximating set cover. Uriel Feige, J. ACM. 454Uriel Feige. 1998. A threshold of ln n for approximating set cover. J. ACM, 45(4), July.\n\nExplaining collaborative filtering recommendations. Jonathan L Herlocker, Joseph A Konstan, John Riedl, Conference on Computer Supported Cooperative Work. CSCWJonathan L. Herlocker, Joseph A. Konstan, and John Riedl. 2000. Explaining collaborative filtering recommenda- tions. In Conference on Computer Supported Coopera- tive Work (CSCW).\n\nLeakage in data mining: Formulation, detection, and avoidance. Shachar Kaufman, Saharon Rosset, Claudia Perlich, Knowledge Discovery and Data Mining (KDD). Shachar Kaufman, Saharon Rosset, and Claudia Perlich. 2011. Leakage in data mining: Formulation, detection, and avoidance. In Knowledge Discovery and Data Mining (KDD).\n\nSubmodular function maximization. Andreas Krause, Daniel Golovin, Tractability: Practical Approaches to Hard Problems. Cambridge University PressAndreas Krause and Daniel Golovin. 2014. Submodu- lar function maximization. In Tractability: Practical Approaches to Hard Problems. Cambridge University Press, February.\n\nInvestigating statistical machine learning as a tool for software development. Kayur Patel, James Fogarty, James A Landay, Beverly Harrison, Human Factors in Computing Systems (CHI). Kayur Patel, James Fogarty, James A. Landay, and Bev- erly Harrison. 2008. Investigating statistical machine learning as a tool for software development. In Human Factors in Computing Systems (CHI).\n\nWhy should I trust you?\": Explaining the predictions of any classifier. Sameer Marco Tulio Ribeiro, Carlos Singh, Guestrin, abs/1602.04938CoRRMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why should I trust you?\": Explaining the pre- dictions of any classifier. CoRR, abs/1602.04938.\n\nRecursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, Christopher Potts, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingStroudsburg, PA, October. Association for Computational LinguisticsRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christo- pher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Pro- ceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631-1642, Stroudsburg, PA, October. Association for Computa- tional Linguistics.\n\nTowards universal paraphrastic sentence embeddings. John Wieting, Mohit Bansal, Kevin Gimpel, Karen Livescu, abs/1511.08198CoRRJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2015. Towards universal paraphrastic sen- tence embeddings. CoRR, abs/1511.08198.\n", "annotations": {"author": "[{\"end\":174,\"start\":90},{\"end\":250,\"start\":175},{\"end\":331,\"start\":251},{\"end\":174,\"start\":90},{\"end\":250,\"start\":175},{\"end\":331,\"start\":251},{\"end\":174,\"start\":90},{\"end\":250,\"start\":175},{\"end\":331,\"start\":251}]", "publisher": null, "author_last_name": "[{\"end\":109,\"start\":102},{\"end\":187,\"start\":182},{\"end\":266,\"start\":258},{\"end\":109,\"start\":102},{\"end\":187,\"start\":182},{\"end\":266,\"start\":258},{\"end\":109,\"start\":102},{\"end\":187,\"start\":182},{\"end\":266,\"start\":258}]", "author_first_name": "[{\"end\":95,\"start\":90},{\"end\":101,\"start\":96},{\"end\":181,\"start\":175},{\"end\":257,\"start\":251},{\"end\":95,\"start\":90},{\"end\":101,\"start\":96},{\"end\":181,\"start\":175},{\"end\":257,\"start\":251},{\"end\":95,\"start\":90},{\"end\":101,\"start\":96},{\"end\":181,\"start\":175},{\"end\":257,\"start\":251}]", "author_affiliation": "[{\"end\":173,\"start\":130},{\"end\":249,\"start\":206},{\"end\":330,\"start\":287},{\"end\":173,\"start\":130},{\"end\":249,\"start\":206},{\"end\":330,\"start\":287},{\"end\":173,\"start\":130},{\"end\":249,\"start\":206},{\"end\":330,\"start\":287}]", "title": "[{\"end\":71,\"start\":1},{\"end\":402,\"start\":332},{\"end\":71,\"start\":1},{\"end\":402,\"start\":332},{\"end\":71,\"start\":1},{\"end\":402,\"start\":332}]", "venue": "[{\"end\":450,\"start\":404},{\"end\":450,\"start\":404},{\"end\":450,\"start\":404}]", "abstract": "[{\"end\":1522,\"start\":520},{\"end\":1522,\"start\":520},{\"end\":1522,\"start\":520}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2882,\"start\":2860},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3036,\"start\":3016},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3174,\"start\":3152},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3973,\"start\":3949},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4026,\"start\":4002},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5060,\"start\":5039},{\"end\":7574,\"start\":7548},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9354,\"start\":9341},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9564,\"start\":9538},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10285,\"start\":10247},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10434,\"start\":10413},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10570,\"start\":10548},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2882,\"start\":2860},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3036,\"start\":3016},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3174,\"start\":3152},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3973,\"start\":3949},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4026,\"start\":4002},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5060,\"start\":5039},{\"end\":7574,\"start\":7548},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9354,\"start\":9341},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9564,\"start\":9538},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10285,\"start\":10247},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10434,\"start\":10413},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10570,\"start\":10548},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2882,\"start\":2860},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3036,\"start\":3016},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3174,\"start\":3152},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3973,\"start\":3949},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4026,\"start\":4002},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5060,\"start\":5039},{\"end\":7574,\"start\":7548},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9354,\"start\":9341},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9564,\"start\":9538},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10285,\"start\":10247},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10434,\"start\":10413},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10570,\"start\":10548}]", "figure": null, "paragraph": "[{\"end\":2544,\"start\":1538},{\"end\":3694,\"start\":2546},{\"end\":4091,\"start\":3696},{\"end\":4222,\"start\":4093},{\"end\":4367,\"start\":4224},{\"end\":4542,\"start\":4369},{\"end\":5061,\"start\":4580},{\"end\":6243,\"start\":5113},{\"end\":6324,\"start\":6245},{\"end\":6866,\"start\":6348},{\"end\":7066,\"start\":6917},{\"end\":7480,\"start\":7110},{\"end\":7664,\"start\":7482},{\"end\":9074,\"start\":7706},{\"end\":9213,\"start\":9120},{\"end\":9565,\"start\":9259},{\"end\":9795,\"start\":9582},{\"end\":10077,\"start\":9797},{\"end\":10644,\"start\":10094},{\"end\":11223,\"start\":10646},{\"end\":11826,\"start\":11261},{\"end\":12309,\"start\":11862},{\"end\":12740,\"start\":12335},{\"end\":13479,\"start\":12742},{\"end\":14148,\"start\":13495},{\"end\":2544,\"start\":1538},{\"end\":3694,\"start\":2546},{\"end\":4091,\"start\":3696},{\"end\":4222,\"start\":4093},{\"end\":4367,\"start\":4224},{\"end\":4542,\"start\":4369},{\"end\":5061,\"start\":4580},{\"end\":6243,\"start\":5113},{\"end\":6324,\"start\":6245},{\"end\":6866,\"start\":6348},{\"end\":7066,\"start\":6917},{\"end\":7480,\"start\":7110},{\"end\":7664,\"start\":7482},{\"end\":9074,\"start\":7706},{\"end\":9213,\"start\":9120},{\"end\":9565,\"start\":9259},{\"end\":9795,\"start\":9582},{\"end\":10077,\"start\":9797},{\"end\":10644,\"start\":10094},{\"end\":11223,\"start\":10646},{\"end\":11826,\"start\":11261},{\"end\":12309,\"start\":11862},{\"end\":12740,\"start\":12335},{\"end\":13479,\"start\":12742},{\"end\":14148,\"start\":13495},{\"end\":2544,\"start\":1538},{\"end\":3694,\"start\":2546},{\"end\":4091,\"start\":3696},{\"end\":4222,\"start\":4093},{\"end\":4367,\"start\":4224},{\"end\":4542,\"start\":4369},{\"end\":5061,\"start\":4580},{\"end\":6243,\"start\":5113},{\"end\":6324,\"start\":6245},{\"end\":6866,\"start\":6348},{\"end\":7066,\"start\":6917},{\"end\":7480,\"start\":7110},{\"end\":7664,\"start\":7482},{\"end\":9074,\"start\":7706},{\"end\":9213,\"start\":9120},{\"end\":9565,\"start\":9259},{\"end\":9795,\"start\":9582},{\"end\":10077,\"start\":9797},{\"end\":10644,\"start\":10094},{\"end\":11223,\"start\":10646},{\"end\":11826,\"start\":11261},{\"end\":12309,\"start\":11862},{\"end\":12740,\"start\":12335},{\"end\":13479,\"start\":12742},{\"end\":14148,\"start\":13495}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6347,\"start\":6325},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6916,\"start\":6867},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7109,\"start\":7067},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9119,\"start\":9075},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9258,\"start\":9214},{\"attributes\":{\"id\":\"formula_0\"},\"end\":6347,\"start\":6325},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6916,\"start\":6867},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7109,\"start\":7067},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9119,\"start\":9075},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9258,\"start\":9214},{\"attributes\":{\"id\":\"formula_0\"},\"end\":6347,\"start\":6325},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6916,\"start\":6867},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7109,\"start\":7067},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9119,\"start\":9075},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9258,\"start\":9214}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1536,\"start\":1524},{\"attributes\":{\"n\":\"2\"},\"end\":4578,\"start\":4545},{\"end\":5111,\"start\":5064},{\"end\":7704,\"start\":7667},{\"attributes\":{\"n\":\"3\"},\"end\":9580,\"start\":9568},{\"end\":10092,\"start\":10080},{\"attributes\":{\"n\":\"3.1\"},\"end\":11259,\"start\":11226},{\"attributes\":{\"n\":\"3.2\"},\"end\":11860,\"start\":11829},{\"attributes\":{\"n\":\"3.3\"},\"end\":12333,\"start\":12312},{\"attributes\":{\"n\":\"4\"},\"end\":13493,\"start\":13482},{\"attributes\":{\"n\":\"1\"},\"end\":1536,\"start\":1524},{\"attributes\":{\"n\":\"2\"},\"end\":4578,\"start\":4545},{\"end\":5111,\"start\":5064},{\"end\":7704,\"start\":7667},{\"attributes\":{\"n\":\"3\"},\"end\":9580,\"start\":9568},{\"end\":10092,\"start\":10080},{\"attributes\":{\"n\":\"3.1\"},\"end\":11259,\"start\":11226},{\"attributes\":{\"n\":\"3.2\"},\"end\":11860,\"start\":11829},{\"attributes\":{\"n\":\"3.3\"},\"end\":12333,\"start\":12312},{\"attributes\":{\"n\":\"4\"},\"end\":13493,\"start\":13482},{\"attributes\":{\"n\":\"1\"},\"end\":1536,\"start\":1524},{\"attributes\":{\"n\":\"2\"},\"end\":4578,\"start\":4545},{\"end\":5111,\"start\":5064},{\"end\":7704,\"start\":7667},{\"attributes\":{\"n\":\"3\"},\"end\":9580,\"start\":9568},{\"end\":10092,\"start\":10080},{\"attributes\":{\"n\":\"3.1\"},\"end\":11259,\"start\":11226},{\"attributes\":{\"n\":\"3.2\"},\"end\":11860,\"start\":11829},{\"attributes\":{\"n\":\"3.3\"},\"end\":12333,\"start\":12312},{\"attributes\":{\"n\":\"4\"},\"end\":13493,\"start\":13482}]", "table": null, "figure_caption": null, "figure_ref": "[{\"end\":8944,\"start\":8937},{\"end\":11593,\"start\":11585},{\"end\":12107,\"start\":12099},{\"end\":12450,\"start\":12442},{\"end\":13254,\"start\":13246},{\"end\":8944,\"start\":8937},{\"end\":11593,\"start\":11585},{\"end\":12107,\"start\":12099},{\"end\":12450,\"start\":12442},{\"end\":13254,\"start\":13246},{\"end\":8944,\"start\":8937},{\"end\":11593,\"start\":11585},{\"end\":12107,\"start\":12099},{\"end\":12450,\"start\":12442},{\"end\":13254,\"start\":13246}]", "bib_author_first_name": "[{\"end\":14553,\"start\":14552},{\"end\":14564,\"start\":14563},{\"end\":14576,\"start\":14575},{\"end\":14592,\"start\":14591},{\"end\":14594,\"start\":14593},{\"end\":14847,\"start\":14839},{\"end\":14879,\"start\":14873},{\"end\":14891,\"start\":14888},{\"end\":14906,\"start\":14902},{\"end\":14928,\"start\":14917},{\"end\":15213,\"start\":15209},{\"end\":15215,\"start\":15214},{\"end\":15232,\"start\":15227},{\"end\":15234,\"start\":15233},{\"end\":15251,\"start\":15245},{\"end\":15253,\"start\":15252},{\"end\":15269,\"start\":15264},{\"end\":15271,\"start\":15270},{\"end\":15284,\"start\":15280},{\"end\":15286,\"start\":15285},{\"end\":15533,\"start\":15527},{\"end\":15553,\"start\":15549},{\"end\":15568,\"start\":15562},{\"end\":15804,\"start\":15799},{\"end\":15973,\"start\":15965},{\"end\":15975,\"start\":15974},{\"end\":15993,\"start\":15987},{\"end\":15995,\"start\":15994},{\"end\":16009,\"start\":16005},{\"end\":16324,\"start\":16317},{\"end\":16341,\"start\":16334},{\"end\":16357,\"start\":16350},{\"end\":16621,\"start\":16614},{\"end\":16636,\"start\":16630},{\"end\":16981,\"start\":16976},{\"end\":16994,\"start\":16989},{\"end\":17009,\"start\":17004},{\"end\":17011,\"start\":17010},{\"end\":17027,\"start\":17020},{\"end\":17358,\"start\":17352},{\"end\":17386,\"start\":17380},{\"end\":17668,\"start\":17661},{\"end\":17681,\"start\":17677},{\"end\":17697,\"start\":17693},{\"end\":17707,\"start\":17702},{\"end\":17727,\"start\":17716},{\"end\":17729,\"start\":17728},{\"end\":17745,\"start\":17739},{\"end\":17747,\"start\":17746},{\"end\":17763,\"start\":17752},{\"end\":18438,\"start\":18434},{\"end\":18453,\"start\":18448},{\"end\":18467,\"start\":18462},{\"end\":18481,\"start\":18476},{\"end\":14553,\"start\":14552},{\"end\":14564,\"start\":14563},{\"end\":14576,\"start\":14575},{\"end\":14592,\"start\":14591},{\"end\":14594,\"start\":14593},{\"end\":14847,\"start\":14839},{\"end\":14879,\"start\":14873},{\"end\":14891,\"start\":14888},{\"end\":14906,\"start\":14902},{\"end\":14928,\"start\":14917},{\"end\":15213,\"start\":15209},{\"end\":15215,\"start\":15214},{\"end\":15232,\"start\":15227},{\"end\":15234,\"start\":15233},{\"end\":15251,\"start\":15245},{\"end\":15253,\"start\":15252},{\"end\":15269,\"start\":15264},{\"end\":15271,\"start\":15270},{\"end\":15284,\"start\":15280},{\"end\":15286,\"start\":15285},{\"end\":15533,\"start\":15527},{\"end\":15553,\"start\":15549},{\"end\":15568,\"start\":15562},{\"end\":15804,\"start\":15799},{\"end\":15973,\"start\":15965},{\"end\":15975,\"start\":15974},{\"end\":15993,\"start\":15987},{\"end\":15995,\"start\":15994},{\"end\":16009,\"start\":16005},{\"end\":16324,\"start\":16317},{\"end\":16341,\"start\":16334},{\"end\":16357,\"start\":16350},{\"end\":16621,\"start\":16614},{\"end\":16636,\"start\":16630},{\"end\":16981,\"start\":16976},{\"end\":16994,\"start\":16989},{\"end\":17009,\"start\":17004},{\"end\":17011,\"start\":17010},{\"end\":17027,\"start\":17020},{\"end\":17358,\"start\":17352},{\"end\":17386,\"start\":17380},{\"end\":17668,\"start\":17661},{\"end\":17681,\"start\":17677},{\"end\":17697,\"start\":17693},{\"end\":17707,\"start\":17702},{\"end\":17727,\"start\":17716},{\"end\":17729,\"start\":17728},{\"end\":17745,\"start\":17739},{\"end\":17747,\"start\":17746},{\"end\":17763,\"start\":17752},{\"end\":18438,\"start\":18434},{\"end\":18453,\"start\":18448},{\"end\":18467,\"start\":18462},{\"end\":18481,\"start\":18476},{\"end\":14553,\"start\":14552},{\"end\":14564,\"start\":14563},{\"end\":14576,\"start\":14575},{\"end\":14592,\"start\":14591},{\"end\":14594,\"start\":14593},{\"end\":14847,\"start\":14839},{\"end\":14879,\"start\":14873},{\"end\":14891,\"start\":14888},{\"end\":14906,\"start\":14902},{\"end\":14928,\"start\":14917},{\"end\":15213,\"start\":15209},{\"end\":15215,\"start\":15214},{\"end\":15232,\"start\":15227},{\"end\":15234,\"start\":15233},{\"end\":15251,\"start\":15245},{\"end\":15253,\"start\":15252},{\"end\":15269,\"start\":15264},{\"end\":15271,\"start\":15270},{\"end\":15284,\"start\":15280},{\"end\":15286,\"start\":15285},{\"end\":15533,\"start\":15527},{\"end\":15553,\"start\":15549},{\"end\":15568,\"start\":15562},{\"end\":15804,\"start\":15799},{\"end\":15973,\"start\":15965},{\"end\":15975,\"start\":15974},{\"end\":15993,\"start\":15987},{\"end\":15995,\"start\":15994},{\"end\":16009,\"start\":16005},{\"end\":16324,\"start\":16317},{\"end\":16341,\"start\":16334},{\"end\":16357,\"start\":16350},{\"end\":16621,\"start\":16614},{\"end\":16636,\"start\":16630},{\"end\":16981,\"start\":16976},{\"end\":16994,\"start\":16989},{\"end\":17009,\"start\":17004},{\"end\":17011,\"start\":17010},{\"end\":17027,\"start\":17020},{\"end\":17358,\"start\":17352},{\"end\":17386,\"start\":17380},{\"end\":17668,\"start\":17661},{\"end\":17681,\"start\":17677},{\"end\":17697,\"start\":17693},{\"end\":17707,\"start\":17702},{\"end\":17727,\"start\":17716},{\"end\":17729,\"start\":17728},{\"end\":17745,\"start\":17739},{\"end\":17747,\"start\":17746},{\"end\":17763,\"start\":17752},{\"end\":18438,\"start\":18434},{\"end\":18453,\"start\":18448},{\"end\":18467,\"start\":18462},{\"end\":18481,\"start\":18476}]", "bib_author_last_name": "[{\"end\":14561,\"start\":14554},{\"end\":14573,\"start\":14565},{\"end\":14589,\"start\":14577},{\"end\":14603,\"start\":14595},{\"end\":14871,\"start\":14848},{\"end\":14886,\"start\":14880},{\"end\":14900,\"start\":14892},{\"end\":14915,\"start\":14907},{\"end\":14934,\"start\":14929},{\"end\":15225,\"start\":15216},{\"end\":15243,\"start\":15235},{\"end\":15262,\"start\":15254},{\"end\":15278,\"start\":15272},{\"end\":15291,\"start\":15287},{\"end\":15547,\"start\":15534},{\"end\":15560,\"start\":15554},{\"end\":15578,\"start\":15569},{\"end\":15590,\"start\":15580},{\"end\":15810,\"start\":15805},{\"end\":15985,\"start\":15976},{\"end\":16003,\"start\":15996},{\"end\":16015,\"start\":16010},{\"end\":16332,\"start\":16325},{\"end\":16348,\"start\":16342},{\"end\":16365,\"start\":16358},{\"end\":16628,\"start\":16622},{\"end\":16644,\"start\":16637},{\"end\":16987,\"start\":16982},{\"end\":17002,\"start\":16995},{\"end\":17018,\"start\":17012},{\"end\":17036,\"start\":17028},{\"end\":17378,\"start\":17359},{\"end\":17392,\"start\":17387},{\"end\":17402,\"start\":17394},{\"end\":17675,\"start\":17669},{\"end\":17691,\"start\":17682},{\"end\":17700,\"start\":17698},{\"end\":17714,\"start\":17708},{\"end\":17737,\"start\":17730},{\"end\":17750,\"start\":17748},{\"end\":17769,\"start\":17764},{\"end\":18446,\"start\":18439},{\"end\":18460,\"start\":18454},{\"end\":18474,\"start\":18468},{\"end\":18489,\"start\":18482},{\"end\":14561,\"start\":14554},{\"end\":14573,\"start\":14565},{\"end\":14589,\"start\":14577},{\"end\":14603,\"start\":14595},{\"end\":14871,\"start\":14848},{\"end\":14886,\"start\":14880},{\"end\":14900,\"start\":14892},{\"end\":14915,\"start\":14907},{\"end\":14934,\"start\":14929},{\"end\":15225,\"start\":15216},{\"end\":15243,\"start\":15235},{\"end\":15262,\"start\":15254},{\"end\":15278,\"start\":15272},{\"end\":15291,\"start\":15287},{\"end\":15547,\"start\":15534},{\"end\":15560,\"start\":15554},{\"end\":15578,\"start\":15569},{\"end\":15590,\"start\":15580},{\"end\":15810,\"start\":15805},{\"end\":15985,\"start\":15976},{\"end\":16003,\"start\":15996},{\"end\":16015,\"start\":16010},{\"end\":16332,\"start\":16325},{\"end\":16348,\"start\":16342},{\"end\":16365,\"start\":16358},{\"end\":16628,\"start\":16622},{\"end\":16644,\"start\":16637},{\"end\":16987,\"start\":16982},{\"end\":17002,\"start\":16995},{\"end\":17018,\"start\":17012},{\"end\":17036,\"start\":17028},{\"end\":17378,\"start\":17359},{\"end\":17392,\"start\":17387},{\"end\":17402,\"start\":17394},{\"end\":17675,\"start\":17669},{\"end\":17691,\"start\":17682},{\"end\":17700,\"start\":17698},{\"end\":17714,\"start\":17708},{\"end\":17737,\"start\":17730},{\"end\":17750,\"start\":17748},{\"end\":17769,\"start\":17764},{\"end\":18446,\"start\":18439},{\"end\":18460,\"start\":18454},{\"end\":18474,\"start\":18468},{\"end\":18489,\"start\":18482},{\"end\":14561,\"start\":14554},{\"end\":14573,\"start\":14565},{\"end\":14589,\"start\":14577},{\"end\":14603,\"start\":14595},{\"end\":14871,\"start\":14848},{\"end\":14886,\"start\":14880},{\"end\":14900,\"start\":14892},{\"end\":14915,\"start\":14907},{\"end\":14934,\"start\":14929},{\"end\":15225,\"start\":15216},{\"end\":15243,\"start\":15235},{\"end\":15262,\"start\":15254},{\"end\":15278,\"start\":15272},{\"end\":15291,\"start\":15287},{\"end\":15547,\"start\":15534},{\"end\":15560,\"start\":15554},{\"end\":15578,\"start\":15569},{\"end\":15590,\"start\":15580},{\"end\":15810,\"start\":15805},{\"end\":15985,\"start\":15976},{\"end\":16003,\"start\":15996},{\"end\":16015,\"start\":16010},{\"end\":16332,\"start\":16325},{\"end\":16348,\"start\":16342},{\"end\":16365,\"start\":16358},{\"end\":16628,\"start\":16622},{\"end\":16644,\"start\":16637},{\"end\":16987,\"start\":16982},{\"end\":17002,\"start\":16995},{\"end\":17018,\"start\":17012},{\"end\":17036,\"start\":17028},{\"end\":17378,\"start\":17359},{\"end\":17392,\"start\":17387},{\"end\":17402,\"start\":17394},{\"end\":17675,\"start\":17669},{\"end\":17691,\"start\":17682},{\"end\":17700,\"start\":17698},{\"end\":17714,\"start\":17708},{\"end\":17737,\"start\":17730},{\"end\":17750,\"start\":17748},{\"end\":17769,\"start\":17764},{\"end\":18446,\"start\":18439},{\"end\":18460,\"start\":18454},{\"end\":18474,\"start\":18468},{\"end\":18489,\"start\":18482}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":14762,\"start\":14550},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":12383721},\"end\":15165,\"start\":14764},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":29006669},\"end\":15501,\"start\":15167},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":121570279},\"end\":15748,\"start\":15503},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":52827488},\"end\":15911,\"start\":15750},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2057093},\"end\":16252,\"start\":15913},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":9168804},\"end\":16578,\"start\":16254},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6107490},\"end\":16895,\"start\":16580},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1971561},\"end\":17278,\"start\":16897},{\"attributes\":{\"doi\":\"abs/1602.04938\",\"id\":\"b9\"},\"end\":17580,\"start\":17280},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":990233},\"end\":18380,\"start\":17582},{\"attributes\":{\"doi\":\"abs/1511.08198\",\"id\":\"b11\"},\"end\":18651,\"start\":18382},{\"attributes\":{\"id\":\"b0\"},\"end\":14762,\"start\":14550},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":12383721},\"end\":15165,\"start\":14764},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":29006669},\"end\":15501,\"start\":15167},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":121570279},\"end\":15748,\"start\":15503},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":52827488},\"end\":15911,\"start\":15750},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2057093},\"end\":16252,\"start\":15913},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":9168804},\"end\":16578,\"start\":16254},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6107490},\"end\":16895,\"start\":16580},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1971561},\"end\":17278,\"start\":16897},{\"attributes\":{\"doi\":\"abs/1602.04938\",\"id\":\"b9\"},\"end\":17580,\"start\":17280},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":990233},\"end\":18380,\"start\":17582},{\"attributes\":{\"doi\":\"abs/1511.08198\",\"id\":\"b11\"},\"end\":18651,\"start\":18382},{\"attributes\":{\"id\":\"b0\"},\"end\":14762,\"start\":14550},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":12383721},\"end\":15165,\"start\":14764},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":29006669},\"end\":15501,\"start\":15167},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":121570279},\"end\":15748,\"start\":15503},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":52827488},\"end\":15911,\"start\":15750},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2057093},\"end\":16252,\"start\":15913},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":9168804},\"end\":16578,\"start\":16254},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6107490},\"end\":16895,\"start\":16580},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1971561},\"end\":17278,\"start\":16897},{\"attributes\":{\"doi\":\"abs/1602.04938\",\"id\":\"b9\"},\"end\":17580,\"start\":17280},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":990233},\"end\":18380,\"start\":17582},{\"attributes\":{\"doi\":\"abs/1511.08198\",\"id\":\"b11\"},\"end\":18651,\"start\":18382}]", "bib_title": "[{\"end\":14837,\"start\":14764},{\"end\":15207,\"start\":15167},{\"end\":15525,\"start\":15503},{\"end\":15797,\"start\":15750},{\"end\":15963,\"start\":15913},{\"end\":16315,\"start\":16254},{\"end\":16612,\"start\":16580},{\"end\":16974,\"start\":16897},{\"end\":17659,\"start\":17582},{\"end\":14837,\"start\":14764},{\"end\":15207,\"start\":15167},{\"end\":15525,\"start\":15503},{\"end\":15797,\"start\":15750},{\"end\":15963,\"start\":15913},{\"end\":16315,\"start\":16254},{\"end\":16612,\"start\":16580},{\"end\":16974,\"start\":16897},{\"end\":17659,\"start\":17582},{\"end\":14837,\"start\":14764},{\"end\":15207,\"start\":15167},{\"end\":15525,\"start\":15503},{\"end\":15797,\"start\":15750},{\"end\":15963,\"start\":15913},{\"end\":16315,\"start\":16254},{\"end\":16612,\"start\":16580},{\"end\":16974,\"start\":16897},{\"end\":17659,\"start\":17582}]", "bib_author": "[{\"end\":14563,\"start\":14552},{\"end\":14575,\"start\":14563},{\"end\":14591,\"start\":14575},{\"end\":14605,\"start\":14591},{\"end\":14873,\"start\":14839},{\"end\":14888,\"start\":14873},{\"end\":14902,\"start\":14888},{\"end\":14917,\"start\":14902},{\"end\":14936,\"start\":14917},{\"end\":15227,\"start\":15209},{\"end\":15245,\"start\":15227},{\"end\":15264,\"start\":15245},{\"end\":15280,\"start\":15264},{\"end\":15293,\"start\":15280},{\"end\":15549,\"start\":15527},{\"end\":15562,\"start\":15549},{\"end\":15580,\"start\":15562},{\"end\":15592,\"start\":15580},{\"end\":15812,\"start\":15799},{\"end\":15987,\"start\":15965},{\"end\":16005,\"start\":15987},{\"end\":16017,\"start\":16005},{\"end\":16334,\"start\":16317},{\"end\":16350,\"start\":16334},{\"end\":16367,\"start\":16350},{\"end\":16630,\"start\":16614},{\"end\":16646,\"start\":16630},{\"end\":16989,\"start\":16976},{\"end\":17004,\"start\":16989},{\"end\":17020,\"start\":17004},{\"end\":17038,\"start\":17020},{\"end\":17380,\"start\":17352},{\"end\":17394,\"start\":17380},{\"end\":17404,\"start\":17394},{\"end\":17677,\"start\":17661},{\"end\":17693,\"start\":17677},{\"end\":17702,\"start\":17693},{\"end\":17716,\"start\":17702},{\"end\":17739,\"start\":17716},{\"end\":17752,\"start\":17739},{\"end\":17771,\"start\":17752},{\"end\":18448,\"start\":18434},{\"end\":18462,\"start\":18448},{\"end\":18476,\"start\":18462},{\"end\":18491,\"start\":18476},{\"end\":14563,\"start\":14552},{\"end\":14575,\"start\":14563},{\"end\":14591,\"start\":14575},{\"end\":14605,\"start\":14591},{\"end\":14873,\"start\":14839},{\"end\":14888,\"start\":14873},{\"end\":14902,\"start\":14888},{\"end\":14917,\"start\":14902},{\"end\":14936,\"start\":14917},{\"end\":15227,\"start\":15209},{\"end\":15245,\"start\":15227},{\"end\":15264,\"start\":15245},{\"end\":15280,\"start\":15264},{\"end\":15293,\"start\":15280},{\"end\":15549,\"start\":15527},{\"end\":15562,\"start\":15549},{\"end\":15580,\"start\":15562},{\"end\":15592,\"start\":15580},{\"end\":15812,\"start\":15799},{\"end\":15987,\"start\":15965},{\"end\":16005,\"start\":15987},{\"end\":16017,\"start\":16005},{\"end\":16334,\"start\":16317},{\"end\":16350,\"start\":16334},{\"end\":16367,\"start\":16350},{\"end\":16630,\"start\":16614},{\"end\":16646,\"start\":16630},{\"end\":16989,\"start\":16976},{\"end\":17004,\"start\":16989},{\"end\":17020,\"start\":17004},{\"end\":17038,\"start\":17020},{\"end\":17380,\"start\":17352},{\"end\":17394,\"start\":17380},{\"end\":17404,\"start\":17394},{\"end\":17677,\"start\":17661},{\"end\":17693,\"start\":17677},{\"end\":17702,\"start\":17693},{\"end\":17716,\"start\":17702},{\"end\":17739,\"start\":17716},{\"end\":17752,\"start\":17739},{\"end\":17771,\"start\":17752},{\"end\":18448,\"start\":18434},{\"end\":18462,\"start\":18448},{\"end\":18476,\"start\":18462},{\"end\":18491,\"start\":18476},{\"end\":14563,\"start\":14552},{\"end\":14575,\"start\":14563},{\"end\":14591,\"start\":14575},{\"end\":14605,\"start\":14591},{\"end\":14873,\"start\":14839},{\"end\":14888,\"start\":14873},{\"end\":14902,\"start\":14888},{\"end\":14917,\"start\":14902},{\"end\":14936,\"start\":14917},{\"end\":15227,\"start\":15209},{\"end\":15245,\"start\":15227},{\"end\":15264,\"start\":15245},{\"end\":15280,\"start\":15264},{\"end\":15293,\"start\":15280},{\"end\":15549,\"start\":15527},{\"end\":15562,\"start\":15549},{\"end\":15580,\"start\":15562},{\"end\":15592,\"start\":15580},{\"end\":15812,\"start\":15799},{\"end\":15987,\"start\":15965},{\"end\":16005,\"start\":15987},{\"end\":16017,\"start\":16005},{\"end\":16334,\"start\":16317},{\"end\":16350,\"start\":16334},{\"end\":16367,\"start\":16350},{\"end\":16630,\"start\":16614},{\"end\":16646,\"start\":16630},{\"end\":16989,\"start\":16976},{\"end\":17004,\"start\":16989},{\"end\":17020,\"start\":17004},{\"end\":17038,\"start\":17020},{\"end\":17380,\"start\":17352},{\"end\":17394,\"start\":17380},{\"end\":17404,\"start\":17394},{\"end\":17677,\"start\":17661},{\"end\":17693,\"start\":17677},{\"end\":17702,\"start\":17693},{\"end\":17716,\"start\":17702},{\"end\":17739,\"start\":17716},{\"end\":17752,\"start\":17739},{\"end\":17771,\"start\":17752},{\"end\":18448,\"start\":18434},{\"end\":18462,\"start\":18448},{\"end\":18476,\"start\":18462},{\"end\":18491,\"start\":18476}]", "bib_venue": "[{\"end\":14959,\"start\":14956},{\"end\":17945,\"start\":17859},{\"end\":14959,\"start\":14956},{\"end\":17945,\"start\":17859},{\"end\":14959,\"start\":14956},{\"end\":17945,\"start\":17859},{\"end\":14643,\"start\":14605},{\"end\":14954,\"start\":14936},{\"end\":15318,\"start\":15293},{\"end\":15612,\"start\":15592},{\"end\":15818,\"start\":15812},{\"end\":16066,\"start\":16017},{\"end\":16408,\"start\":16367},{\"end\":16697,\"start\":16646},{\"end\":17078,\"start\":17038},{\"end\":17350,\"start\":17280},{\"end\":17857,\"start\":17771},{\"end\":18432,\"start\":18382},{\"end\":14643,\"start\":14605},{\"end\":14954,\"start\":14936},{\"end\":15318,\"start\":15293},{\"end\":15612,\"start\":15592},{\"end\":15818,\"start\":15812},{\"end\":16066,\"start\":16017},{\"end\":16408,\"start\":16367},{\"end\":16697,\"start\":16646},{\"end\":17078,\"start\":17038},{\"end\":17350,\"start\":17280},{\"end\":17857,\"start\":17771},{\"end\":18432,\"start\":18382},{\"end\":14643,\"start\":14605},{\"end\":14954,\"start\":14936},{\"end\":15318,\"start\":15293},{\"end\":15612,\"start\":15592},{\"end\":15818,\"start\":15812},{\"end\":16066,\"start\":16017},{\"end\":16408,\"start\":16367},{\"end\":16697,\"start\":16646},{\"end\":17078,\"start\":17038},{\"end\":17350,\"start\":17280},{\"end\":17857,\"start\":17771},{\"end\":18432,\"start\":18382}]"}}}, "year": 2023, "month": 12, "day": 17}