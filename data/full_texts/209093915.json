{"id": 209093915, "updated": "2022-02-01 14:49:05.152", "metadata": {"title": "SearcHD: A Memory-Centric Hyperdimensional Computing With Stochastic Training", "authors": "[{\"middle\":[],\"last\":\"Imani\",\"first\":\"Mohsen\"},{\"middle\":[],\"last\":\"Yin\",\"first\":\"Xunzhao\"},{\"middle\":[],\"last\":\"Messerly\",\"first\":\"John\"},{\"middle\":[],\"last\":\"Gupta\",\"first\":\"Saransh\"},{\"middle\":[],\"last\":\"Niemier\",\"first\":\"Michael\"},{\"middle\":[\"Sharon\"],\"last\":\"Hu\",\"first\":\"Xiaobo\"},{\"middle\":[],\"last\":\"Rosing\",\"first\":\"Tajana\"}]", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "journal": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Brain-inspired hyperdimensional (HD) computing emulates cognitive tasks by computing with long binary vectors\u2014also know as hypervectors\u2014as opposed to computing with numbers. However, we observed that in order to provide acceptable classification accuracy on practical applications, HD algorithms need to be trained and tested on nonbinary hypervectors. In this article, we propose SearcHD, a fully binarized HD computing algorithm with a fully binary training. SearcHD maps every data points to a high-dimensional space with binary elements. Instead of training an HD model with nonbinary elements, SearcHD implements a full binary training method which generates multiple binary hypervectors for each class. We also use the analog characteristic of nonvolatile memories (NVMs) to perform all encoding, training, and inference computations in memory. We evaluate the efficiency and accuracy of SearcHD on a wide range of classification applications. Our evaluation shows that SearcHD can provide on average <inline-formula> <tex-math notation=\"LaTeX\">$31.1\\times $ </tex-math></inline-formula> higher energy efficiency and <inline-formula> <tex-math notation=\"LaTeX\">$12.8\\times $ </tex-math></inline-formula> faster training as compared to the state-of-the-art HD computing algorithms.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2983032988", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tcad/ImaniYMGNHR20", "doi": "10.1109/tcad.2019.2952544"}}, "content": {"source": {"pdf_hash": "949c320cfebf8189b674e400b92ae38f1557554b", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "717bf5cfbcf67718edd58c569bc8944ac7f535c1", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/949c320cfebf8189b674e400b92ae38f1557554b.txt", "contents": "\nSearcHD: A Memory-Centric Hyperdimensional Computing With Stochastic Training\nOCTOBER 2020\n\nMohsen Imani \nXunzhao Yin \nJohn Messerly \nStudent Member, IEEESaransh Gupta \nSenior Member, IEEE, XiaoboMichael Niemier \nFellow, IEEESharon Hu \nFellow, IEEETajana Rosing \nSearcHD: A Memory-Centric Hyperdimensional Computing With Stochastic Training\n\nIEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS\n3910OCTOBER 202010.1109/TCAD.2019.29525442422\nBrain-inspired hyperdimensional (HD) computing emulates cognitive tasks by computing with long binary vectorsalso know as hypervectors-as opposed to computing with numbers. However, we observed that in order to provide acceptable classification accuracy on practical applications, HD algorithms need to be trained and tested on nonbinary hypervectors. In this article, we propose SearcHD, a fully binarized HD computing algorithm with a fully binary training. SearcHD maps every data points to a high-dimensional space with binary elements. Instead of training an HD model with nonbinary elements, SearcHD implements a full binary training method which generates multiple binary hypervectors for each class. We also use the analog characteristic of nonvolatile memories (NVMs) to perform all encoding, training, and inference computations in memory. We evaluate the efficiency and accuracy of SearcHD on a wide range of classification applications. Our evaluation shows that SearcHD can provide on average 31.1\u00d7 higher energy efficiency and 12.8\u00d7 faster training as compared to the state-of-the-art HD computing algorithms.Index Terms-Brain-inspired computing, hyperdimensional (HD) computing, processing in-memory (PIM).\n\nI. INTRODUCTION\n\nT HE EXISTING learning algorithms have been shown to be effective for many different tasks, e.g., object tracking [1], speech recognition [2], [3], image classification [4], [5], etc. For instance, deep neural networks (DNNs) have shown great potential to be used for complicated classification problems. DNN architectures, such as AlexNet [4] and GoogleNet [6] provide high classification accuracy for complex image classification tasks, e.g., ImageNet dataset [7]. However, the computational complexity and memory requirement of DNNs makes them inefficient for a broad variety of real-life (embedded) applications where the device resources and power budget is limited.\n\nBrain-inspired computing models in conjunction with recent advancements in memory technologies have opened new avenues for efficient execution of a wide variety of cognitive tasks on nano-scale fabrics [8]- [11]. Hyperdimensional (HD) computing is based on the understanding that brains compute with patterns of neural activity that are not readily associated with numbers [12]. HD computing builds upon a well-defined set of operations with random HD vectors and is extremely robust in the presence of hardware failures. HD computing offers a computational paradigm that can be easily applied to learning problems [12]- [19]. Its main differentiation from the conventional computing system is that in HD computing, data is represented as approximate patterns, which can favorably scale for many learning applications.\n\nHD computation runs in three steps: 1) encoding; 2) training; and 3) inference [20].\n\n1) The encoding module maps input data into highdimensional space using a set of randomly generated hypervectors. 2) In training, a traditional HD algorithm combines the encoded hypervectors in order to generate a hypervector representing each class. The algorithm simply performs element-wise additions on the hypervectors which belong to the same class. 3) In inference, an associative search checks the similarity of an encoded test hypervector with all trained class hypervectors and returns the class with the highest similarity score. While HD computing can be implemented in conventional digital hardware, implementation on approximate hardware can yield substantial efficiency gains with minimal to zero loss in accuracy [21].\n\nProcessing in-memory (PIM) is a promising solution to accelerate HD computations running for memory-centric applications by enabling parallelism [22]- [30]. PIM performs some or all of a set of computation tasks (e.g., bit-wise or search computations) inside the memory without using any processing cores. Thus, application performance may be accelerated significantly by avoiding the memory access bottleneck. In addition, PIM architectures enable analog-based computation in order to perform approximate but ultrafast computation (i.e., existing PIM architectures perform computation with binary vectors stored in memory rows [31]). Past efforts have tried to accelerate HD computing via PIM. For 0278-0070 c 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\n\nSee https://www.ieee.org/publications/rights/index.html for more information.\n\nexample, work in [8] and [32] designed in-memory hardware to accelerate the encoding module. Work in [21] designed a content-addressable memory which can perform the associative search operations for inference over binary hypervectors using a Hamming distance metric. However, the aforementioned accelerators can only work with binary vectors, which in turns only provide high classification accuracies on simpler problems, e.g., language recognition which uses small n-gram windows of size five to detect words in a language. In this article, we observed that acceptable classification accuracy can be achieved using nonbinary encoded hypervectors, nonbinary training, and associative search on a nonbinary model using metrics such as cosine similarity. This hinders the implementation of many steps of the existing HD computing algorithms using in-memory operations.\n\nIn order to fully exploit the advantage of in-memory architecture, we propose SearcHD, a fully binary HD computing algorithm with probability-based training. SearcHD maps every data point to high-dimensional space with binary elements and then it assigns multiple vectors representing each class. Instead of performing addition, SearcHD performs binary training by changing each class hypervector depending on how well it matches with a class that it belongs to. Unlike most recent learning algorithms, e.g., neural networks, SearcHD supports a single-pass training, where it trains a model by one time passing through a training dataset. The inference step is performed by using a Hamming distance similarity check of a binary query with all prestored class hypervectors. All HD computing blocks, including encoding, training, and inference are implemented fully in memory without using any nonbinary operation. SearcHD exploits the analog characteristic of ReRAMs to perform the encoding functionalities, such as X O R and majority functions, and training/inference functionalities such as the associative search on ReRAMs.\n\nWe have tested the accuracy and efficiency of the proposed SearcHD on four practical classification applications. Our evaluation shows that SearcHD can provide on average 31.1\u00d7 higher energy efficiency and 12.8\u00d7 faster training as compared to the state-of-the-art HD computing algorithms. In addition, during inference, SearcHD can achieve 178.7\u00d7 higher energy efficiency and 14.1\u00d7 faster computation while providing 6.5% higher classification accuracy than state-of-the-art HD computing algorithms.\n\n\nII. BACKGROUND AND MOTIVATION\n\nHD computation is a novel computational paradigm inspired by how the brain represents data. HD computing has previously shown to address energy bounds which plague deterministic computing [12], [33]. HD computing replaces the conventional computing approach with patterns of neural activity that are not readily associated with numbers. Due to the large size of brain circuits, this neurons pattern can be represented using vectors in thousands of dimensions, which are called hypervectors. Hypervectors are holographic and (pseudo)random with independent identically distributed components. Each hypervector stores the information across all its components, where no component has more responsibility to store any piece of information than another. This makes HD computing extremely robust against failures. HD computing supports a well-defined set of operations, such as binding that forms a new hypervector which associates two hypervectors, and bundling that combines several hypervectors into a single composite hypervector. Reasoning in HD computing is based on the similarity between the hypervectors.\n\nPrior work applied HD computing to a wide range of applications, including analogy-based reasoning [34], latent semantic analysis [35], language recognition [36], [37], text classification [38], gesture recognition [39], prediction from multimodal sensor fusion [14], [40], robotics [41], and speech recognition [15], [42]. Fig. 1 shows an overview of how HD computing performs a classification task. The first step in HD computing is to map (encode) raw data into a high-dimensional space. Various encoding methods have been proposed to handle different data types, such as time series, text-like data, and feature vectors [13], [15]- [17], [20], [42]. Regardless of the data type, the encoded data is represented with a D-dimensional vector (H \u2208 N D ) [15], [42]. Training is performed by computing the element-wise sum of all hypervectors corresponding to the same class ({C 1 , . . . , C K }, C i \u2208 N D ), as shown in Fig. 1. For example, in an application with k classes, the ith class hypervector can be computed as\nC i = \u2200j\u2208class i H j .\nThis training operation involves many integer (nonbinary) additions, which makes the HD computation costly. During inference, we simply compare a similarity score between an encoded query hypervector and each class hypervector, returning the most similar class. Prior work has typically used the cosine similarity (inner product) which involves a large number of nonbinary additions and multiplications. For example, for an application with k classes, this similarity check involves k \u00d7 D multiplication and addition operations, where the hypervector dimension is D, commonly 10 000.\n\nIn terms of hardware acceleration, prior work tried to binarize the HD trained model in order to simplify HD computing inference [20], [42]. Thanks to the inherent memory-centric operations of HD computing, several works design in-memory  [8], [21], [32]. The work in [8] fabricated a 3-D VRRAM/CMOS to support the central operations of HD computing on 4-layer 3-D VRRAM/FinFET. Work in [21] designed three popular digital, resistive and analog associative memories in order to accelerate Hamming distance similarity in HD computing inference. The work in [32] exploited carbon nanotube FETs and resistive memory to design an HD computing algorithm for text classification. However, these methods only support Hamming distance similarity between binarized hypervectors. In this article, we observe that HD computing algorithms require the use of integer values and cosine similarity metrics in order to provide acceptable accuracies on practical classification problems. Table I shows the classification accuracy and the inference efficiency of HD computing on four practical applications (large feature size) when using binary and nonbinary models. All efficiency results are reported for running the applications on digital ASIC hardware [15], [20]. Our evaluation shows that HD computing with the binary model has 4% lower classification accuracy than the nonbinary model. However, in terms of efficiency, HD computing with the binary model can achieve on average 6.1\u00d7 faster computation than the nonbinary model. In addition, HD computing with the binary model can use Hamming distance for similarity check of a query and class hypervectors which can be accelerated in a content addressable memory (CAM) [21], [47]. Our evaluation shows that such analog design can further speedup the inference performance by 6.9\u00d7 as compared to digital design. This motivates us to design a novel HD computing algorithm which can achieve the efficiency of a binary model as well as the accuracy of the nonbinary model.\n\n\nIII. SEARCHD: FULLY BINARY HD COMPUTING\n\nWe propose SearcHD, a fully binary HD computing algorithm, which can perform all HD computing operations, i.e., encoding, training, and inference, using binary operations. In the rest of the section, we explain the details of the proposed approach. We first explain the functionality of the encoding, training, and inference modules, and then illustrate how module functionality can be supported in hardware.\n\n\nA. SearcHD Encoding\n\nThere are different types of encoding methods to map data points into an HD space. For a general form of feature vectors, there are two popular encoding approaches: 1) record-based and 2) n-gram-based encoding [15]. Although SearcHD functionality is independent of here we use a recordbased encoding which is more hardware friendly and only involves bitwise operations as shown in Fig. 2 \nH = ID 1 \u2295 L 1 + ID 2 \u2295 L 2 + \u00b7 \u00b7 \u00b7 + ID n \u2295 L n\nwhere H is the nonbinary encoded hypervector, \u2295 is X O R operation, and L i \u2208 {L 1 , . . . , L m } is the binary hypervector corresponding to the ith feature of vector F. In this encoding, IDs preserve the position of each feature value in a combined set. SearcHD encodes this hypervector by passing each dimension through a majority function. This approach compares each dimension with a threshold value, where a threshold value is equal to half of the number of features (THR = n/2).\n\n\nB. Binary Stochastic Training\n\nIn HD computing, when two data hypervectors are randomly generated, the probability that their hypervectors are orthogonal to each other is high. In training, hypervectors of data in the same class are made appropriately more or less similar to each other. In more recent HD computing algorithms [15], [20], [39], training consists of additions of the encoded hypervectors, thus requiring a large number of arithmetic operations to generate nonbinary class hypervectors. Our proposed SearcHD is a framework for binarization of the HD computing algorithm during both training and inference. SearcHD removes the addition operation from training by exploiting bitwise substitution which trains a model by stochastically sharing the query hypervectors elements with each class hypervector. Since HD computing with a binary model provides low classification accuracy, SearcHD exploits vector quantization to represent an HD model using multiple vectors per class. This enables SearcHD to store more information in each class while keeping the model as binary vectors.\n\n1) SearcHD Bitwise Substitution: SearcHD removes all arithmetic operations from training by replacing addition with bitwise substitution. Assume A and B are two randomly generated vectors. In order to bring vector A closer to vector B, a random (typically small) subset of vector B's indices is forced onto vector A by setting those indices in vector A to match the bits in vector B. Therefore, the Hamming distance between vector A and B is made smaller through partial cloning. When vector A and B are already similar, then indices selected probably contain the same bits, and thus the information in A does not change. This operation is blind since we do not search for indices where A and B differ, and then \"fix\" those indices. Indices are chosen randomly and independently of whatever is in vector A or vector B. In addition, the operation is onedirectional. Only the bits in vector A are transformed to match those in vector B, while the bits in vector B stay the same. In this sense, A inherits an arbitrary section of vector B. We call vector A the binary accumulator and vector B the operand. We refer to this process as bitwise substitution.\n\n2) SearcHD Vector Quantization: Here, we present our fully binary stochastic training approach, which enables the entire HD training process to be performed in the binary domain. Similar to traditional HD computing algorithms, SearcHD trains a model by combining the encoded training hypervectors. As we explained in Section II, HD computing using binary model results in very low classification accuracy. In addition, moving to the nonbinary domain makes HD computing significantly more costly and inefficient. In this article, we propose a vector quantization. We exploit multiple vectors to represent each class in the training of SearcHD. The training keeps distinct information of each class in separated hypervectors, resulting in the learning of a more complex model when using multiple vectors per class. For each class, we generate N models (where N is generally between 4 and 64). Below we explain the details of the proposed algorithm. 1) Initialize the N model vectors to a class by randomly sampling from the encoded training hypervector of that class as shown in Fig. 2(b). For an application with k classes, the approach needs to store N \u00d7 k binary hypervectors as the HD model. For example, we can represent the with class using N initial binary hypervectors\n{C i 1 , C i 2 , . . . , C i N }, where C i \u2208 {0, 1} D 2)\nThe training in HD computing starts by checking the similarity of each encoded data point (training dataset) to the initial model. The similarity check only happens between the encoded data and N class hypervectors corresponding to that label. For each piece of training data Q in a class, find the model with the lowest Hamming distance and update the model using bitwise substitution (explained in Section III-B1). For example, in the ith class, if C i k is selected as a class with the highest similarity, we can update the model using\nC i k = C i k (+)Q.\nIn the above equation, (+) is the bitwise substitution operation, Q is the operand, and C i k is the binary accumulator. This algorithm helps reduce the memory access overhead introduced by using bitwise substitution. This approach accumulates training data more intelligently: given the choice of adding an incoming piece of data to one of N model vectors, we can select the model with the lowest Hamming distance to ensure that we do not needlessly encode information in our models.\n\n3) SearcHD Training Process: Binary substitution updates each dimension of the selected class stochastically with p = \u03b1 \u00d7 (1 \u2212 \u03b4) probability, where \u03b4 is a similarity between the query and the class hypervector and \u03b1 is a learning rate. In other words, with flip probability p, each element of the selected class hypervector will be replaced with the elements of the query hypervector. \u03b1 is the learning rate (0 < \u03b1) which determines how frequently the model needs to be updated during the training. Using a small learning rate is conservative, as the model will have minor changes during the training. A larger learning rate will result in a major change to a model after each iteration, resulting in a higher probability of divergence. We explore the impact of learning rate on the classification accuracy in Section V-B.\n\n\nC. Inference\n\nAfter updating the model on the entire training dataset, SearcHD uses the trained model for the rest of the classification during inference. The classification checks the similarity of each encoded test data vector to all class hypervectors. In other words, a query hypervector is compared with all N \u00d7 k class hyprevectors. Finally, a query identifies a class with the maximum Hamming distance similarity with the query data. and efficient when compared to floating point operations used by neural networks or other classification algorithms [8], [21]. This enables HD computing to be trained and tested on lightweight embedded devices. However, as traditional CPU/GPU cores have not been designed to efficiently perform bitwise operations over long vectors, we provide a custom hardware realization of SearcHD. Here, we show how to use the analog characteristics of ReRAMs, in particular ReRAM devices, to process all HD computing functionality within a memory.\n\n\nA. Overview\n\nHD computing operations can be supported using two main encoding and associative search blocks. In Section IV-B, we explain the details of the in-memory implementation of the encoding module.\n\nDuring training, SearcHD requires only a single pass over the training set. For each class, SearcHD first randomly selects N data points from the training dataset as representative class hypervectors. Then, SearcHD uses CAM blocks to check the similarity of each encoded hypervector (from the training dataset) with the class hypervectors. Depending on the tag of input data, SearcHD only needs to perform the similarity check on N hypervectors of the same class as input data. For each training sample, we find a hypervector in a class which has the highest similarity with the encoded hypervector using a memory block which supports the nearest Hamming distance search. Then, we update the class hypervector depending on how well/close it is matched with the query hypervector (\u03b4). To implement this, we exploit an analog characteristic of ReRAMs to identify how well a query data is matched with the most similar class. We exploit the feature of ReRAM devices to generate a random stochastic sequence with the desired probability. This sequence needs to have a similar number of zeros as the difference between a query and the class hypervector. Finally, we update the elements of a selected class by performing an in-memory XNOR operation between: 1) the class and 2) the stochastically generated hypervector. In the following section, we explain how SearcHD can support all of these operations in an analog fashion.\n\nDuring inference, we employ the same CAM block used in training to implement the nearest Hamming distance search between a query and all class hypervectors. For an application with k classes and N class hypervectors, SearcHD requires similarity comparisons between the query and k \u00d7 N stored class hypervectors. The hardware introduced in Section IV-C, explains the details of in-memory search implementation.\n\n\nB. In-Memory Encoding\n\nThe encoder, shown in Fig. 3(a), implements bitwise X O R operations between hypervectors P and L over different features, and thresholds the results. To support this functionality, our analog design assigns a small size crossbar memory (m+1 rows with D dimensions) to each input feature, where the crossbar memory stores the corresponding position hypervector (ID) along with all m possible level hypervectors that each feature can take (m is the number of level hypervectors, as defined in Section III). The results of all X O R operations are written to another crossbar memory. Next, the memory that stores the X O R results perform the bitwise majority operation on the entire memory. In conventional crossbar memory, the write-in the majority block needs to perform serially over different features. However, in this article, we use switches [shown in Fig. 3(a)] that enables parallel write in the majority block. These switches portioned memory rows into a separated block, enabling all XOR results to be written in majority block independently. We accomplish this operation by designing a sense amplifier for the crossbar memory that can detect whether the number of 1s is above a certain threshold (THR). This Majority function does not involve actual counting but uses the analog characteristics of ReRAM devices, making it a significantly faster and more efficient design.\n\nIn-Memory X O R: To enable an X O R operation as required by the encoding module, the row driver must activate the line corresponding to the position hypervector [ID shown in Fig. 3(a)]. Depending on the feature value, the row driver activates one more row in the crossbar memory which corresponds to the feature value. Our analog design supports bitwise X O R operations inside the crossbar memory among two activated rows. This design enables in-memory X O R operations by making a small modification to the sense amplifier of the crossbar memory, as shown in Fig. 3(b). We place a modified sense amplifier at the tail of each vertical bitline (BL). The BL current passes through the R OR and R AND , and changes the voltage in node x and y. A voltage larger than a threshold in node x and y results in inverting the output values of the inverters, realizing the AND and OR operations. We use the combination of AND and OR operations to generate X O R. In our design, R OR , R AND , and V R are tuned to ensure the correct functionality of the design considering process variations. It should be noted that the same X O R functionality could be implemented using a series of MAGIC NOR operation introduced by Kvatinsky et al. [49]. The advantage of this approach is that we do not need to make any changes to the sense amplifier. However, the clock cycle of MAGIC NOR is at the order of 1 ns, while the proposed approach computes X O R in less than 300 ps. In-Memory Majority: Fig. 3(c) shows the sense amplifier designed to implement the majority function. To evaluate this function, a row driver activates all rows of the crossbar memory. Any cell with low resistance injects current into the corresponding vertical BL. The number of 0s in each column determines the amount of current in the BL. The charging rate of the capacitor C m in the proposed sense amplifier depends on the number of zeroes in each column. The sense amplifier samples the capacitor voltage at a specific time, which intuitively is the same as comparing the BL current with a THR = n/2 value. Since the charging current is constant, the voltage grows linearly with time, thus the \"specific time\" can be equal to twice the time where C m is charged by the maximum current. Our design can use different predetermined THR values in order to tune the level of thresholding for applications with different feature sizes.\n\n\nC. In-Memory Associative Search\n\nThe goal of this search operation is to find a class with the highest similarity. We employ crossbar CAMs, which can search the nearest Hamming distance vector with respect to the stored class hypervectors. Traditionally, CAMs are only designed to search for an exact match and cannot look for a row with the smallest Hamming distance. Fig. 4(a) shows an architectural schematic of a conventional CAM. A search operation in CAM starts with precharging all CAM match-lines (MLs). An input data vector is applied to a CAM after passing through an input buffer. The goal of the buffer is to increase the driving strength of the input data and distribute the input data across the entire memory at approximately the same time. Finally, each CAM row is compared with the input data. Conventional CAM can detect a row that contains an exact matching, i.e., where all bits of the row exactly match with the bits in the input data.\n\nIn this article, we exploit the analog characteristics of CAM blocks to detect the row that has the minimum Hamming distance with the query vector. Each CAM row with stored bits that are different from the provided input will discharge the associated ML. The rate of ML discharging depends on the number of mismatch bits in each CAM row. A row with the nearest Hamming distance to the input data is the one with the lowest ML discharging current, resulting in the longest discharging time. To find this row, we need to keep track of all other MLs and determine when only a single ML is still discharging. Fig. 4(b) shows the general structure of the proposed CAM sense amplifier. We implement the nearest Hamming distance search functionality by detecting the CAM row (most closely matched line) which discharges last. This is realized with three main blocks: 1) detector circuitry which samples the voltage of all MLs and detects the ML with the slowest discharge rate; 2) a buffer stage which delays the ML voltage propagation to the output node; and 3) a latch block which samples buffer output when the detector circuit detects that all MLs are discharged. The last edge detection can be easily implemented by NORing the outputs of all matched lines, which is set when all MLs are discharged to zero. However, a conventional implementation of NOR leads to a huge number of transistors and increased latency as the number of inputs rises beyond 3.\n\nHere, we propose the use of a Ganged CMOS-based [50] NOR circuit, which not only provides faster results but can also support larger fan-in with acceptable noise margin. Fig. 3(b) and (c) shows the circuit consisting of skewed inverters with their outputs shorted together. The behavior of the circuit is defined by the ratio r = I/J, where I and J are the sizes of the pull-up and pull-down transistors, respectively. For r > q, the inverter is highly skewed and has a stronger pull-up, while for r < q, it is lowly skewed and has a stronger pull-down. (q is a technology-dependent parameter.) If r is sufficiently small, outputs of multiple skewed inverters can be shorted together to implement the NOR operation. Fig. 4(b) and (c) shows that the output of the circuit is set only when all the nMOS devices are off, i.e., all MLs are zero. The output of ganged NOR circuit controls the latch. The buffer stage used in the sense circuit adds delay to the ML voltage. This delay should be sufficiently large to ensure that we can latch on the last falling ML, when the ganged logic senses that all MLs have fallen to zero.\n\n\nD. In-Memory Distance Detector\n\nIn training, SearcHD uses the proposed CAM block to find a hypervector which has the highest similarity with a query data. Then, SearcHD needs to update the selected class hypervector with the probability that is proportional to how well a query is matched with the class. After finding a class hypervector with the highest similarity, SearcHD performs the search operation on the selected row. This search operation finds how closely the selected row matches with the query data. This can be sensed by the distance detector circuit shown in Fig. 4(d). Our analog implementation transfers the discharging current of a CAM row into a voltage (V k ) and compares it with a reference voltage (V TH ). The reference voltage is the minimum voltage that V K can take when all query dimensions of a query hypervector match with the class hypervector.\n\n\nE. In-Memory Random Generation and Model Update\n\nDepending on the distance similarity difference betweem a query and the class hypervector, SearcHD generates a random sequence of a bit stream which has a proportional number of 1s. For example, for a query with \u03b4 similarity between the query and the class hypervector, SearcHD generates a random sequence with p = \u03b1 \u00d7 (1 \u2212 \u03b4) probability of \"1\" bits. During training, SearcHD selects the class hypervector with the minimum Hamming distance from the query data, and updates the selected class hypervector by bitwise substitution of a query and the class hypervector. This bitwise substitution is performed stochastically on random p \u00d7 D of the class dimensions. This requires generating a random number with a specific probability.\n\nReRAM switching is a stochastic process, thus the write operation in a memristor device happens with a probability which follows a Poisson distribution [51], [52]. This probability depends on several factors, such as programming voltage and write pulse time. For a given programming voltage, we can define the switching probability as\nP(t) = 1 \u2212 e \u2212t/\u03c4 \u03c4 (V) = \u03c4 0 e \u2212V/V 0\nwhere \u03c4 is the characterized switching time that depends on the programming voltage, V, and \u03c4 0 and V 0 are the fitting parameters. To ensure memristor switching with high probability, the pulse width should be long enough. For example, using t = \u03c4 , the switching probability is as low as P(t = \u03c4 ) = 63%, while using t = 10 \u03c4 increases this probability to P(t = 10\u03c4 ) = 99.995%. We exploit the nondeterministic ReRAM switching property to generate random numbers [53], [54]. Depending on the applied voltage, a pulse time is assigned to set the device switching probability to the desired percentage. For example, to generate numbers with 50% probability, the pulse time (\u03b1) has been set to ensure p(t = \u03b1\u03c4 ) = 50%.\n\nAssume a class hypervector with D dimensions, C i = {c 1 , c 2 , . . . , c D }. Random generation creates a bit sequence with D dimensions, R = {r 1 , r 2 , . . . , r D } but with p probability of bits to be \"1.\" We update the selected class hypervector in two steps. 1) We read the random hypervector R using a memory sense amplifier to select the bits for the bitwise substitution operation. Then, we activate the row of a selected class hypervector and apply R as a BL buffer to reset corresponding class elements where R has \"1\" values there. 2) SearcHD reads the query hypervector (Q) and calculates the AND operation of the query and R hypervector. Our design uses the result of the AND operation as a BL buffer in order to set the class elements in all dimensions where the BL buffer has a \"1\" value. This is equivalent to injecting the query elements into a class hypervector in all dimensions where R has nonzero values.\n\n\nV. EVALUATION\n\nIn this section, we test the functionality of SearcHD in both software and hardware implementations. We first discuss the impact of learning rate and class configurations on SearcHD classification accuracy. We then compare the energy efficiency and performance of SearcHD with baseline HD computing during training and inference. We finally discuss the accuracyefficiency tradeoff with respect to hypervector dimensions.\n\n\nA. Experimental Setup\n\nWe test the functionality of SearcHD in both software and hardware implementations. In software, we verify SearcHD training and inference functionalities by a C++ implementation of the stochastic algorithm on an Intel Core i7 7600 CPU. For the hardware implementation, we have designed a cycle-accurate simulator which emulates HD computing functionality. Our simulator prestores the randomly generated level and position hypervectors in memory and performs the training and inference operations fully in the proposed in-memory architecture. We extract the circuit level characteristic of the hardware design from simulations based on a 45-nm CMOS process technology [55] using the Hewlett simulation program with integrated circuit emphasis (HSPICE) simulator. We use the VTEAM ReRAM model [56] for our memory. The model parameters of the device, as listed in Table II, are chosen to produce switching delay of 1 ns, a voltage pulse of 1 V and 2 V for RESET and SET operations in order to fit practical devices [49]. The energy of set and reset operations are E set = 23.8 fJ and E reset = 0.32 fJ, respectively. The functionality of all the circuits has been validated considering 10%  We test SearcHD accuracy and energy/performance efficiency on four practical classification applications. Table IV summarizes the configurations with various evaluation datasets. Table V shows the impact of the learning rate \u03b1 on SearcHD classification accuracy. Our evaluation shows that using a very small learning rate reduces the capability of a model to learn since each new data can only have a minor impact on the model update. Larger learning rates result in more substantial changes to a model, which can result in possible divergence. In other words, large \u03b1 values indicate that there is a higher chance that the latest training data point will change the model, but it does not preserve the changes that earlier training data made on the model. In this article, our evaluation shows that using \u03b1 values of 1 and 2 provide the maximum accuracy for all tested datasets.  As the figure shows, for all applications, increasing the number of hypervectors per class improves classification accuracy. For example, SearcHD using eight hypervectors per class (8/class) and 16 hypervectors per class (16/class) can achieve on average 9.2% and 12.7% higher classification accuracy, respectively, as compared to the case of using 1/class hypervector when running on four tested applications. However, SearcHD accuracy saturates when the number of hypervectors is larger than 32/class. In fact, 32/class is enough to get most common patterns in our datasets, thus adding new vectors cannot capture different patterns than the existing vectors in the class. The red line in each graph shows the classification accuracy that a k-nearest neighbor (kNN) algorithm can achieve. kNN does not have a training mode. During Inference, kNN looks at the similarity of a data point with all other training data. However, kNN is computationally expensive and requires a large memory footprint. In contrast, SearcHD provides similar classification accuracy by performing classification on a trained model. Fig. 5 also compares SearcHD classification accuracy with the best baseline HD computing algorithm using nonbinary class hypervectors [15]. The baseline HD model is trained using nonbinary encoded hypervectors. After the training, it uses a cosine similarity check for classification. Our evaluation shows that SearcHD with 32/class and 64/class provide 5.7% and 7.2% higher classification accuracy, respectively, as compared to the baseline HD computing with the nonbinary model. Table VI compares the memory footprint of SearcHD, kNN, and the baseline HD algorithm (nonbinary model). As we expect, kNN has the highest memory requirement, by taking on average 11.4 MB for each application. After that, SearcHD 32/class and the baseline HD algorithm require similar memory footprints, which are on average about 28.2\u00d7 lower than kNN. SearcHD can further reduce the memory footprint by reducing the number of hypervectors per class. For example, SearcHD with 8/class configuration provides 117.1 \u00d7 and 4.1\u00d7 lower memory than kNN and the baseline HD algorithm while providing similar classification accuracy.   Fig. 6 compares the energy efficiency and performance of SearcHD training and the baseline HD computing algorithm. Regardless of whether binary or nonbinary models are employed, the baseline HD computing approach has the same training cost. Baseline HD computing encodes data in the nonbinary domain and then adds the input data in order to create a hypervector for each class. This operation cannot map into a crossbar memory architecture as the memory only supports the bit-wise operation. In contrast, SearcHD simplifies the training operation by eliminating all nonbinary operations from HD training. In all reported results, we run SearcHD on the proposed in-memory architecture, while the baseline HD computing approach runs on optimized digital hardware proposed in [15] and [21]. Our evaluation shows that SearcHD with 64/class (32/class) configuration can achieve on average 12.2\u00d7 and 9.3\u00d7 (31.1\u00d7 and 12.8\u00d7) higher energy efficiency and speedup as compared to the baseline HD computing algorithm.  with all configurations can provide significantly faster and more energy-efficient computation as compared to the baseline HD algorithm. For example, SearcHD with 64/class (32/class) configuration can provide on average 66.2\u00d7 and 10.8\u00d7 (178.7\u00d7 and 14.1\u00d7) energy efficiency and speedup as compared to a baseline HD algorithm, while providing 7.9% (6.5%) higher classification accuracy. The higher energy and performance efficiency of SearcHD comes from the in-memory capability in parallelizing the similarity check among different rows. In addition, the approximate search in analog memory eliminates slower digital-based counting operations.\n\n\nB. SearcHD and Learning Rate\n\n\nC. SearcHD Accuracy in Different Configurations\n\n\nD. Training Efficiency\n\n\nE. Inference Efficiency\n\nIn SearcHD, the computation cost grows with the number of hypervectors in a class. For example, SearcHD with 32/class configuration consumes 14.1\u00d7 more energy and has 1.9\u00d7 slower execution time as compared to SearcHD with 4/class configuration. In addition, we already observed that SearcHD accuracy saturates when using models with more than 32/class hypervector.\n\n\n1) Detectable Hamming Distance:\n\nThe accuracy of the associative search depends on the bit precision of ganged-logic design. Using large-size transistors in the ganged logic will result in a faster response to ML discharging, thus improving the detection accuracy of the row with the minimum Hamming distance to the input. Fig. 8 shows the HD classification accuracy and the energy-delay product (EDP) of SearcHD associative memory, when we change the minimum detectable bits in design from 10 to 90 bits. The results are reported for the activity recognition dataset (UCIHAR). The EDP values are normalized to SearcHD using ten detectable Hamming distance.\n\nAs the graph shows, the design can provide acceptable accuracy when the minimum detectable number of bits is below 32. In this configuration, the associative memory can achieve an EDP improvement of 2.3\u00d7 when compared to using the design with a 10-bit minimum detectable Hamming distance. SearcHD classification accuracy and normalized EDP improvement when the associative memory works in different minimum detectable distances.\n\nThat said, ganged logic in low bit precision improves the EDP efficiency while degrading the classification accuracy. For instance, 50-bits and 70-bits minimum detectable Hamming distances can provide 3\u00d7 and 4.8\u00d7 EDP improvement as compared to the design with 10-bit detectable Hamming distances, while providing 1% and 3.7% lower than maximum SearcHD accuracy. To find the maximum required precision in CAM circuitry, we cross-checked the distances between all stored class hypervectors. We observed that the distance between any two stored classes is always higher than 71 bits. In other words, 71 is the minimum Hamming distance which needs to be detected in our design. This feature allows us to relax the bit precision of the analog search sense amplifier which results in further improvement in its efficiency.\n\nPrior work tried to modify the CAM structure in order to enable the nearest Hamming distance search capability [21]. However, that design is very sensitive to dimensionality and process variations. Our evaluation on a CAM with 10 000 bits shows that SearcHD associative memory can provide 8 bits Hamming-detectable error under 10% process variations, while work in [21] works with 22 bits Hammingdetectable error. In addition, our proposed approach provides 3.2\u00d7 higher energy efficiency and 2.0\u00d7 faster as compared to work in [21]. In Section V, we show the impact of SearcHD associative search on the training/inference efficiency.\n\n\nF. Accuracy-Efficiency Tradeoff\n\nSearcHD can exploit hypervector dimensions as a parameter to trade efficiency and accuracy. Regardless of the dimension of the model at training, SearcHD can use a model in lower dimensions in order to accelerate SearcHD inference. In HD computing, the dimensions are independent, thus SearcHD can drop any arbitrary dimension in order to accelerate the computation. Fig. 9 shows the classification accuracy, normalized energy consumption, and normalized execution time of SearcHD when the hypervector dimension changes from D = 2000 to 10 000. Our evaluation shows that SearcHD can achieve maximum accuracy using dimensions around D = 8000. In addition, SearcHD with D = 4000 (D = 6000) can achieve 2.0\u00d7 and 1.3\u00d7 (1.3\u00d7 and 1.2\u00d7) higher energy efficiency and speedup than SearcHD with D = 10 000 while providing only 2.1% (1.2%) lower classification accuracy. \n\n\nG. Area/Energy Breakdown\n\nHere, we compare the area and energy breakdown of digital HD computing with SearcHD analog implementation. Fig. 10(a) shows the area occupied by the encoding and associative search modules. In a digital implementation, encoding takes a large amount of chip area, as it requires to encode data points with up to 800 features. In contrast, analog implementation takes significantly lower area in both encoding and associative search modules. The analog majority computation in the encoding modules and the analog detector circuit in the associative search module eliminate large circuits for digital accumulation. This results in 6.5\u00d7 area efficiency of the analog as compared to digital implementation. Fig. 10(b) shows the area and energy breakdown of the encoding module in digital and analog implementations. In digital, XOR array and accumulator are taking the majority of the area and energy consumption. The accumulator has a higher portion of energy, as this block requires to sequentially add the XOR results. In analog implementation, the majority function dominating the total area and energy, while XOR computation takes about 32% of the area and 16% of energy. This is because the majority module uses a large sense amplifier and exploits switches to split the memory rows (enabling parallel write). Fig. 10(c) shows the area and energy breakdown of the associative search module in both digital and analog implementation. Similar to the encoding module, in digital implementation, XOR array and accumulator are dominating the total area and energy consumption. In analog, the CAM block is dominating the area, as it requires to store all class hypervectors. However, in terms of energy, the detector circuit takes over 64% of total energy. The ADC block takes about 10% area and 7.2% of the energy, as we only require a single ADC block in each associative search module.  \n\n\nH. Hardware Efficiency\n\nTo fairly show the advantage of the in-memory implementation, we compare the efficiency of SearcHD with the digital implementation in two configurations: 1) optimized digital implementation with 6.5\u00d7 larger area than analog and 2) digital which takes the similar area as analog. The results in Table VII are reported for both training and inference phases, when our implementation provides a similar accuracy as the baseline digital implementation. Our evaluation shows that analog design provides 25.4\u00d7 and 357.5\u00d7 (23.5\u00d7 and 1243.4\u00d7) speedup and energy efficiency as compared to optimized digital implementation during training (inference). In the same area, analog training (inference) efficiency improves to 99.4\u00d7 and 458.0\u00d7 (91.4\u00d7 and 1883.7\u00d7) speedup and energy efficiency, respectively.\n\n\nI. SearcHD and OFF/ON Ratio\n\nIn practice, the value of OFF/ON resistance ratio has important impact on the performance of SearcHD functionality. Although we used the VTEAM model with 1000 OFF/ON resistance ratio, in practice we may have memristor devices with a lower OFF/ON ratio. Using a lower OFF/ON ratio has direct impact on the SearcHD performance. In other words, a lower ratio makes the functionality of the detector circuit more complicated, specially, for thresholding functionality. We evaluate the impact of variation on resistance ratio when the OFF/ON ration reduces to 100. Our results show that this reduction results in 5.8\u00d7 and 12.5\u00d7 slower XOR and thresholding functionality, respectively.\n\n\nVI. CONCLUSION\n\nIn this article, we proposed SearcHD, a fully binary HD computing algorithm. SearcHD encodes every data point to HD space with binary elements and performs training by assigning multiple binary hypervectors to each class. During inference, SearcHD searches in the prestored class hypervectors for the closest class hypervector to the binary test hypervector in terms of Hamming distance similarity. We accordingly designed an in-memory architecture which can accelerate all SearcHD functionalities in memory. Our experimental evaluation of four practical classification applications shows that SearcHD implemented in-memory can achieve 31.1\u00d7 and 12.8\u00d7 (178.7\u00d7 and 14.1\u00d7) energy efficiency and speedup during training (inference) as compared to the stateof-the-art HD computing algorithms while providing 6.5% higher classification accuracy.\n\nFig. 1 .\n1Overview of HD computing in performing the classification task.\n\nFig. 2 .\n2Overview of SearcHD encoding and stochastic training. (a) Encoding. (b) Stochastic training.\n\nFig. 3 .\n3IV. IN-MEMORY CLASSIFICATION IN HD SPACE SearcHD requires bitwise computations over hypervectors in both training and inference modes. These operations are fast (a) In-memory implementation of SearcHD encoding module. (b) Sense amplifier supporting bitwise X O R operation. (c) Sense amplifier supporting majority functionality on the X O R results.\n\nFig. 4 .\n4(a) CAM-based associative memory. (b) Structure of the CAM sense amplifier. (c) Ganged circuit. (d) Distance detector circuit.\n\nFig. 5\n5shows the impact of the number of hypervectors per each class N on SearcHD classification accuracy in comparison with other approaches. The state-of-the-art HD computing approaches use a single hypervector representing each class.\n\nFig. 5 .\n5Classification accuracy of SearcHD, kNN, and the baseline HD algorithms. (a) ISOLET. (b) FACE. (c) UCIHAR. (d) IoT.\n\nFig. 6 .\n6Training execution time and energy consumption of the baseline HD computing and SearcHD with different configurations. (a) ISOLET. (b) FACE. (c) UCIHAR. (d) IOT.\n\nFig. 7 compares\n7SearcHD and baseline HD computing efficiency during inference. The y-axis shows the energy consumptions and execution times of the baseline HD computing and SearcHD algorithm with the number of hypervectors per class ranging from 4 to 64. The baseline HD algorithm uses cosine as the similarity metric, while SearcHD uses Hamming distance and accelerates this computation via analog, in-memory hardware. Our evaluation shows that SearcHD\n\nFig. 7 .\n7Inference execution time and energy consumption of the baseline HD algorithm and SearcHD with different configurations. (a) ISOLET. (b) FACE. (c) UCIHAR. (d) IOT.\n\nFig. 8 .\n8Fig. 8. SearcHD classification accuracy and normalized EDP improvement when the associative memory works in different minimum detectable distances.\n\nFig. 9 .\n9Impact of dimensionality on SearcHD accuracy and efficiency.\n\n\nSearcHD area and energy breakdown. (a) Occupied area by the encoding and associative search modules in digital design and analog SearcHD. (b) Area and energy breakdown of the encoding module. (c) Area and energy breakdown of the associative search module.\n\nTABLE I CLASSIFICATION\nIACCURACY AND EFFICIENCY OF HD COMPUTING USING BINARY AND NONBINARY MODELS architectures to accelerate HD computing\n\n\neach existing feature index, {ID 1 , . . . , ID n }, where ID \u2208 {0, 1} D . The encoding linearly combines the feature values over different indices(a). This encod-\ning maps any feature vector F = {f 1 , f 2 , . . . , f n } with n features \n(f i \u2208 N), into H = {h 1 , h 2 , . . . , h D } with D dimensions \n(h i \u2208 {0, 1}) [15], [48]. This encoding finds the minimum and \nmaximum feature values and quantizes that range linearly into \nm levels. Then, it assigns a random binary hypervector with \nD dimensions to each of the quantized level {L 1 , . . . , L m }. \nThe level hypervectors need to have correlation, such that \nthe neighbor levels are assigned to similar hypervectors. For \nexample, we generate the first level hypervector, L 1 , by sam-\npling uniformly at random from 0 or 1 values. The next level \nhypervectors are created by flipping D/m random bits of the \nprevious level. As a result, the level hypervectors have similar \nvalues if the corresponding original data are closer, while L 1 \nand L m will be nearly orthogonal. The orthogonality between \nthe bipolar/binary hypervectors defines when two vectors have \nexactly 50% similar bits. This results in a zero cosine similarity \nbetween the orthogonal vectors. \nSimilarly, the encoding module assigns a random binary \nhypervector to \n\nTABLE II VTEAM\nIIMODEL PARAMETERS FOR MEMRISTOR\n\nTABLE III CIRCUIT PARAMETERS TABLE IV\nIIIPARAMETERSIVIMPACT OF LEARNING RATE ON SEARCHD CLASSIFICATION ACCURACY process variations on threshold voltage, transistor sizes, and ReRAM OFF/ON resistance using 5000 Monte Carlo simulations.Table IIIlists the design parameters, including the transistor sizes and AND/OR resistance values.DATASETS (n: FEATURE SIZE, k: NUMBER OF CLASSES) \n\nTABLE V \n\n\nTABLE VI\nVIMEMORY FOOTPRINT OF DIFFERENT ALGORITHMS (MB) \n\n(a) \n\n(c) \n\n(b) \n\n(d) \n\n\n\nTABLE VII EFFICIENCY\nVIIOF SEARCHD AS COMPARED TO DIGITAL IMPLEMENTATION: @ OPTIMIZED AND @ SAME AREA CONFIGURATIONS\n\nLearning to track: Online multiobject tracking by decision making. Y Xiang, A Alahi, S Savarese, Proc. IEEE Int. Conf. Comput. Vis. (ICCV). IEEE Int. Conf. Comput. Vis. (ICCV)Y. Xiang, A. Alahi, and S. Savarese, \"Learning to track: Online multi- object tracking by decision making,\" in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), 2015, pp. 4705-4713.\n\nDeep speech 2: End-to-end speech recognition in english and mandarin. D Amodei, Proc. Int. Conf. Mach. Learn. Int. Conf. Mach. LearnD. Amodei et al., \"Deep speech 2: End-to-end speech recognition in english and mandarin,\" in Proc. Int. Conf. Mach. Learn., 2016, pp. 173-182.\n\nDeep speech: Scaling up end-to-end speech recognition. A Hannun, arXiv:1412.5567arXiv preprintA. Hannun et al., \"Deep speech: Scaling up end-to-end speech recog- nition,\" arXiv preprint arXiv:1412.5567, 2014.\n\nImageNet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Proc. Adv. Neural Inf. Process. Syst. Adv. Neural Inf. ess. SystA. Krizhevsky, I. Sutskever, and G. E. Hinton, \"ImageNet classifica- tion with deep convolutional neural networks,\" in Proc. Adv. Neural Inf. Process. Syst., 2012, pp. 1106-1114.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitK. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 770-778.\n\nGoing deeper with convolutions. C Szegedy, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitC. Szegedy et al., \"Going deeper with convolutions,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2015, pp. 1-9.\n\nImageNet large scale visual recognition challenge. O Russakovsky, Int. J. Comput. Vis. 1153O. Russakovsky et al., \"ImageNet large scale visual recognition chal- lenge,\" Int. J. Comput. Vis., vol. 115, no. 3, pp. 211-252, 2015.\n\nHyperdimensional computing with 3D VRRAM inmemory kernels: Device-architecture co-design for energy-efficient, error-resilient language recognition. H Li, Proc. IEEE Int. Electron Devices Meeting (IEDM). IEEE Int. Electron Devices Meeting (IEDM)H. Li et al., \"Hyperdimensional computing with 3D VRRAM in- memory kernels: Device-architecture co-design for energy-efficient, error-resilient language recognition,\" in Proc. IEEE Int. Electron Devices Meeting (IEDM), 2016, pp. 16-1.\n\nNeurocube: A programmable digital neuromorphic architecture with high-density 3D memory. D Kim, J Kung, S Chai, S Yalamanchili, S Mukhopadhyay, Proc. ACM/IEEE 43rd Annu. Int. Symp. Comput. Archit. (ISCA). ACM/IEEE 43rd Annu. Int. Symp. Comput. Archit. (ISCA)D. Kim, J. Kung, S. Chai, S. Yalamanchili, and S. Mukhopadhyay, \"Neurocube: A programmable digital neuromorphic architecture with high-density 3D memory,\" in Proc. ACM/IEEE 43rd Annu. Int. Symp. Comput. Archit. (ISCA), 2016, pp. 380-392.\n\nDesign considerations of synaptic device for neuromorphic computing. S Yu, D Kuzum, H.-S P Wong, Proc. IEEE Int. Symp. Circuits Syst. (ISCAS). IEEE Int. Symp. Circuits Syst. (ISCAS)S. Yu, D. Kuzum, and H.-S. P. Wong, \"Design considerations of synaptic device for neuromorphic computing,\" in Proc. IEEE Int. Symp. Circuits Syst. (ISCAS), 2014, pp. 1062-1065.\n\nA framework for collaborative learning in secure high-dimensional space. M Imani, Proc. IEEE 12th Int. Conf. Cloud Comput. IEEE 12th Int. Conf. Cloud ComputM. Imani et al., \"A framework for collaborative learning in secure high-dimensional space,\" in Proc. IEEE 12th Int. Conf. Cloud Comput. (CLOUD), 2019, pp. 435-446.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cogn. Comput. 12P. Kanerva, \"Hyperdimensional computing: An introduction to comput- ing in distributed representation with high-dimensional random vectors,\" Cogn. Comput., vol. 1, no. 2, pp. 139-159, 2009.\n\nFast and accurate multiclass inference for MI-BCIs using large multiscale temporal and spectral features. M Hersche, T Rellstab, P D Schiavone, L Cavigelli, L Benini, A Rahimi, 26th Eur. Signal Process. Conf. (EUSIPCO). M. Hersche, T. Rellstab, P. D. Schiavone, L. Cavigelli, L. Benini, and A. Rahimi, \"Fast and accurate multiclass inference for MI-BCIs using large multiscale temporal and spectral features,\" in 26th Eur. Signal Process. Conf. (EUSIPCO), 2018, pp. 1690-1694.\n\nSequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns. O J Rasanen, J P Saarinen, IEEE Trans. Neural Netw. Learn. Syst. 279O. J. Rasanen and J. P. Saarinen, \"Sequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns,\" IEEE Trans. Neural Netw. Learn. Syst., vol. 27, no. 9, pp. 1878-1889, Sep. 2016.\n\nHierarchical hyperdimensional computing for energy efficient classification. M Imani, C Huang, D Kong, T Rosing, Proc. 55th Annu. Design Autom. 55th Annu. Design AutomM. Imani, C. Huang, D. Kong, and T. Rosing, \"Hierarchical hyperdi- mensional computing for energy efficient classification,\" in Proc. 55th Annu. Design Autom. Conf., 2018, pp. 1-6.\n\nHyperdimensional Computing for Blind and One-Shot Classification of EEG Error-Related Potentials. A Rahimi, A Tchouprina, P Kanerva, J D R Mill\u00e1n, J M Rabaey, SpringerNew York, NY, USAA. Rahimi, A. Tchouprina, P. Kanerva, J. D. R. Mill\u00e1n, and J. M. Rabaey, Hyperdimensional Computing for Blind and One-Shot Classification of EEG Error-Related Potentials. New York, NY, USA: Springer, 2017, pp. 1-12.\n\nPULP-HD: Accelerating brain-inspired high-dimensional computing on a parallel ultra-low power platform. F Montagna, A Rahimi, S Benatti, D Rossi, L Benini, Proc. 55th Annu. Des. Autom. Conf. (DAC). 55th Annu. Des. Autom. Conf. (DAC)F. Montagna, A. Rahimi, S. Benatti, D. Rossi, and L. Benini, \"PULP- HD: Accelerating brain-inspired high-dimensional computing on a par- allel ultra-low power platform,\" in Proc. 55th Annu. Des. Autom. Conf. (DAC), 2018, pp. 111-117.\n\nBRIC: Locality-based encoding for energy-efficient brain-inspired hyperdimensional computing. M Imani, J Morris, J Messerly, H Shu, Y Deng, T Rosing, Proc. 56th ACM/IEEE Design Autom. Conf. (DAC). 56th ACM/IEEE Design Autom. Conf. (DAC)52M. Imani, J. Morris, J. Messerly, H. Shu, Y. Deng, and T. Rosing, \"BRIC: Locality-based encoding for energy-efficient brain-inspired hyperdimensional computing,\" in Proc. 56th ACM/IEEE Design Autom. Conf. (DAC), 2019, p. 52.\n\nA binary learning framework for hyperdimensional computing. M Imani, J Messerly, F Wu, W Pi, T Rosing, Proc. Design Autom. Test Europe Conf. Exhibition (DATE). Design Autom. Test Europe Conf. Exhibition (DATE)M. Imani, J. Messerly, F. Wu, W. Pi, and T. Rosing, \"A binary learning framework for hyperdimensional computing,\" in Proc. Design Autom. Test Europe Conf. Exhibition (DATE), 2019, pp. 126-131.\n\nA robust and energy-efficient classifier using brain-inspired hyperdimensional computing. A Rahimi, P Kanerva, J M Rabaey, Proc. Int. Symp. Low Power Electron. Design. Int. Symp. Low Power Electron. DesignA. Rahimi, P. Kanerva, and J. M. Rabaey, \"A robust and energy-efficient classifier using brain-inspired hyperdimensional computing,\" in Proc. Int. Symp. Low Power Electron. Design, 2016, pp. 64-69.\n\nExploring hyperdimensional associative memory. M Imani, A Rahimi, D Kong, T Rosing, J M Rabaey, Proc. IEEE Int. Symp. High Perform. Comput. Archit. (HPCA. IEEE Int. Symp. High Perform. Comput. Archit. (HPCAM. Imani, A. Rahimi, D. Kong, T. Rosing, and J. M. Rabaey, \"Exploring hyperdimensional associative memory,\" in Proc. IEEE Int. Symp. High Perform. Comput. Archit. (HPCA), 2017, pp. 445-456.\n\nFloatPIM: In-memory acceleration of deep neural network training with high precision. M Imani, S Gupta, Y Kim, T Rosing, Proc. 46th Int. Symp. 46th Int. SympM. Imani, S. Gupta, Y. Kim, and T. Rosing, \"FloatPIM: In-memory acceleration of deep neural network training with high precision,\" in Proc. 46th Int. Symp. Comput. Archit., 2019, pp. 802-815.\n\nPRIME: A novel processing-in-memory architecture for neural network computation in ReRAM-based main memory. P Chi, Proc. 43rd Int. Symp. 43rd Int. SympP. Chi et al., \"PRIME: A novel processing-in-memory architecture for neural network computation in ReRAM-based main memory,\" in Proc. 43rd Int. Symp. Comput. Archit., 2016, pp. 27-39.\n\nISAAC: A convolutional neural network accelerator with in-situ analog arithmetic in crossbars. A Shafiee, ACM SIGARCH Comput. Archit. News. 443A. Shafiee et al., \"ISAAC: A convolutional neural network accelerator with in-situ analog arithmetic in crossbars,\" ACM SIGARCH Comput. Archit. News, vol. 44, no. 3, pp. 14-26, 2016.\n\nPIM-enabled instructions: A low-overhead, locality-aware processing-in-memory architecture. J Ahn, S Yoo, O Mutlu, K Choi, Proc. IEEE 42nd Annu. Int. Symp. Comput. Archit. (ISCA). IEEE 42nd Annu. Int. Symp. Comput. Archit. (ISCA)J. Ahn, S. Yoo, O. Mutlu, and K. Choi, \"PIM-enabled instructions: A low-overhead, locality-aware processing-in-memory architecture,\" in Proc. IEEE 42nd Annu. Int. Symp. Comput. Archit. (ISCA), 2015, pp. 336-348.\n\nA scalable processing-in-memory accelerator for parallel graph processing. J Ahn, S Hong, S Yoo, O Mutlu, K Choi, Proc. ACM/IEEE 42nd Annu. Int. Symp. Comput. Archit. (ISCA). ACM/IEEE 42nd Annu. Int. Symp. Comput. Archit. (ISCA)J. Ahn, S. Hong, S. Yoo, O. Mutlu, and K. Choi, \"A scalable processing-in-memory accelerator for parallel graph processing,\" in Proc. ACM/IEEE 42nd Annu. Int. Symp. Comput. Archit. (ISCA), 2015, pp. 105-117.\n\nPinatubo: A processing-in-memory architecture for bulk bitwise operations in emerging non-volatile memories. S Li, C Xu, Q Zou, J Zhao, Y Lu, Y Xie, Proc. 53rd ACM/EDAC/IEEE Design Autom. Conf. (DAC). 53rd ACM/EDAC/IEEE Design Autom. Conf. (DAC)S. Li, C. Xu, Q. Zou, J. Zhao, Y. Lu, and Y. Xie, \"Pinatubo: A processing-in-memory architecture for bulk bitwise operations in emerg- ing non-volatile memories,\" in Proc. 53rd ACM/EDAC/IEEE Design Autom. Conf. (DAC), 2016, pp. 1-6.\n\nRAPIDNN: In-memory deep neural network acceleration framework. M Imani, M Samragh, Y Kim, S Gupta, F Koushanfar, T Rosing, Proc. IEEE Int. Symp. High Perform. Comput. Archit. (HPCA). IEEE Int. Symp. High Perform. Comput. Archit. (HPCA)M. Imani, M. Samragh, Y. Kim, S. Gupta, F. Koushanfar, and T. Rosing, \"RAPIDNN: In-memory deep neural network acceleration framework,\" in Proc. IEEE Int. Symp. High Perform. Comput. Archit. (HPCA), 2020, pp. 1-13.\n\nLazyPIM: An efficient cache coherence mechanism for processing-in-memory. A Boroumand, IEEE Comput. Archit. Lett. 161A. Boroumand et al., \"LazyPIM: An efficient cache coherence mech- anism for processing-in-memory,\" IEEE Comput. Archit. Lett., vol. 16, no. 1, pp. 46-50, Jan./Jun. 2017.\n\nFerroelectric FETs-based nonvolatile logic-in-memory circuits. X Yin, X Chen, M Niemier, X S Hu, IEEE Trans. Very Large Scale Integr. (VLSI) Syst. 271X. Yin, X. Chen, M. Niemier, and X. S. Hu, \"Ferroelectric FETs-based nonvolatile logic-in-memory circuits,\" IEEE Trans. Very Large Scale Integr. (VLSI) Syst., vol. 27, no. 1, pp. 159-172, Jan. 2019.\n\nUltra-efficient processing inmemory for data intensive applications. M Imani, S Gupta, T Rosing, Proc. 54th Annu. Design Autom. Conf. 54th Annu. Design Autom. Conf6M. Imani, S. Gupta, and T. Rosing, \"Ultra-efficient processing in- memory for data intensive applications,\" in Proc. 54th Annu. Design Autom. Conf., 2017, p. 6.\n\nBrain-inspired computing exploiting carbon nanotube FETs and resistive RAM: Hyperdimensional computing case study. T F Wu, Proc. IEEE Int. Solid-State Circuits Conf. (ISSCC). IEEE Int. Solid-State Circuits Conf. (ISSCC)T. F. Wu et al., \"Brain-inspired computing exploiting carbon nanotube FETs and resistive RAM: Hyperdimensional computing case study,\" in Proc. IEEE Int. Solid-State Circuits Conf. (ISSCC), 2018, pp. 492-494.\n\nResistive configurable associative memory for approximate computing. M Imani, A Rahimi, T S Rosing, Proc. Design Autom. Test Europe Conf. Exhibit. (DATE). Design Autom. Test Europe Conf. Exhibit. (DATE)M. Imani, A. Rahimi, and T. S. Rosing, \"Resistive configurable associa- tive memory for approximate computing,\" in Proc. Design Autom. Test Europe Conf. Exhibit. (DATE), 2016, pp. 1327-1332.\n\nWhat we mean when we say what's the dollar of Mexico?' Prototypes and mapping in concept space. P Kanerva, Proc. AAAI Fall Symp. Quantum Inf. AAAI Fall Symp. Quantum Inf1036P. Kanerva, \"What we mean when we say what's the dollar of Mexico?' Prototypes and mapping in concept space,\" in Proc. AAAI Fall Symp. Quantum Inf. Cogn. Soc. Semantic Process., vol. 1036, 2010, pp. 1-5.\n\nRandom indexing of text samples for latent semantic analysis. P Kanerva, J Kristofersson, A Holst, Proc. 22nd Annu. Conf. Cognitive Sci. Soci. 22nd Annu. Conf. Cognitive Sci. Soci1036P. Kanerva, J. Kristofersson, and A. Holst, \"Random indexing of text samples for latent semantic analysis,\" in Proc. 22nd Annu. Conf. Cognitive Sci. Soci., vol. 1036, 2000, pp. 1-2.\n\nLanguage geometry using random indexing. A Joshi, J Halseth, P Kanerva, Proc. Int. Symp. Quantum Interaction. Int. Symp. Quantum InteractionA. Joshi, J. Halseth, and P. Kanerva, \"Language geometry using random indexing,\" in Proc. Int. Symp. Quantum Interaction, 2016, pp. 265-274.\n\nA robust and energy efficient classifier using brain-inspired hyperdimensional computing. A Rahimi, P Kanerva, J M Rabaey, Proc. Int. Symp. Low Power Electron. Design. Int. Symp. Low Power Electron. DesignA. Rahimi, P. Kanerva, and J. M. Rabaey, \"A robust and energy efficient classifier using brain-inspired hyperdimensional computing,\" in Proc. Int. Symp. Low Power Electron. Design, Aug. 2016, pp. 64-69.\n\nHyperdimensional computing for text classification. F R Najafabadi, A Rahimi, P Kanerva, J M Rabaey, Proc. Design Autom. Test Europe Conf. Exhibit. (DATE) Univ. Booth. Design Autom. Test Europe Conf. Exhibit. (DATE) Univ. Booth1F. R. Najafabadi, A. Rahimi, P. Kanerva, and J. M. Rabaey, \"Hyperdimensional computing for text classification,\" in Proc. Design Autom. Test Europe Conf. Exhibit. (DATE) Univ. Booth, 2016, p. 1.\n\nHyperdimensional biosignal processing: A case study for EMG-based hand gesture recognition. A Rahimi, S Benatti, P Kanerva, L Benini, J M Rabaey, Proc. IEEE Int. Conf. Rebooting Comput. (ICRC). IEEE Int. Conf. Rebooting Comput. (ICRC)A. Rahimi, S. Benatti, P. Kanerva, L. Benini, and J. M. Rabaey, \"Hyperdimensional biosignal processing: A case study for EMG-based hand gesture recognition,\" in Proc. IEEE Int. Conf. Rebooting Comput. (ICRC), 2016, pp. 1-8.\n\nModeling dependencies in multiple parallel data streams with hyperdimensional computing. O R\u00e4s\u00e4nen, S Kakouros, IEEE Signal Process. Lett. 217O. R\u00e4s\u00e4nen and S. Kakouros, \"Modeling dependencies in multiple parallel data streams with hyperdimensional computing,\" IEEE Signal Process. Lett., vol. 21, no. 7, pp. 899-903, Jul. 2014.\n\nCrossmodal learning and prediction of autobiographical episodic experiences using a sparse distributed memory. S , Hamburg, GermanyDept. Informat., Hamburg Univ.Ph.D. dissertationS. Jockel, \"Crossmodal learning and prediction of autobiographical episodic experiences using a sparse distributed memory,\" Ph.D. disser- tation, Dept. Informat., Hamburg Univ., Hamburg, Germany, 2010.\n\nVoiceHD: Hyperdimensional computing for efficient speech recognition. M Imani, D Kong, A Rahimi, T Rosing, Proc. Int. Conf. Rebooting Comput. (ICRC. Int. Conf. Rebooting Comput. (ICRCM. Imani, D. Kong, A. Rahimi, and T. Rosing, \"VoiceHD: Hyperdimensional computing for efficient speech recognition,\" in Proc. Int. Conf. Rebooting Comput. (ICRC), 2017, pp. 1-8.\n\nUCI Machine Learning Repository. (2012). UCI Machine Learning Repository. [Online]. Available: http://archive.ics.uci.edu/ml/datasets/ISOLET\n\nSisPorto 2.0: A program for automated analysis of cardiotocograms. D A .-De Campos, J Bernardes, A Garrido, J Marques-De-S\u00e1, L Pereira-Leite, J. Matern. Fetal Med. 95D. A.-de Campos, J. Bernardes, A. Garrido, J. Marques-de-S\u00e1, and L. Pereira-Leite, \"SisPorto 2.0: A program for automated analysis of cardiotocograms,\" J. Matern. Fetal Med., vol. 9, no. 5, pp. 311-318, 2000.\n\nHuman activity recognition on smartphones using a multiclass hardware-friendly support vector machine. D Anguita, A Ghio, L Oneto, X Parra, J L Reyes-Ortiz, Proc. Int. Workshop Ambient Assist. Living. Int. Workshop Ambient Assist. LivingD. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz, \"Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine,\" in Proc. Int. Workshop Ambient Assist. Living, 2012, pp. 216-223.\n\nKitsune: An ensemble of autoencoders for online network intrusion detection. Y Mirsky, T Doitshman, Y Elovici, A Shabtai, arXiv:1802.09089Y. Mirsky, T. Doitshman, Y. Elovici, and A. Shabtai, \"Kitsune: An ensemble of autoencoders for online network intrusion detection,\" arXiv:1802.09089. Feb. 2018.\n\nAn ultradense 2FeFET TCAM design based on a multi-domain FeFET model. X Yin, K Ni, D Reis, S Datta, M Niemier, X S Hu, IEEE Trans. Circuits Syst. II, Exp. Briefs. 669X. Yin, K. Ni, D. Reis, S. Datta, M. Niemier, and X. S. Hu, \"An ultra- dense 2FeFET TCAM design based on a multi-domain FeFET model,\" IEEE Trans. Circuits Syst. II, Exp. Briefs, vol. 66, no. 9, pp. 1557-1581, Sep. 2019.\n\nRandom indexing of text samples for latent semantic analysis. P Kanerva, J Kristofersson, A Holst, Proc. 22nd Annu. Conf. Cognitive Sci. 22nd Annu. Conf. Cognitive Sci1036P. Kanerva, J. Kristofersson, and A. Holst, \"Random indexing of text samples for latent semantic analysis,\" in Proc. 22nd Annu. Conf. Cognitive Sci. Soc., vol. 1036, 2000, pp. 1-2.\n\nMAGIC-Memristor-aided logic. S Kvatinsky, IEEE Trans. Circuits Syst. II, Exp. Briefs. 6111S. Kvatinsky et al., \"MAGIC-Memristor-aided logic,\" IEEE Trans. Circuits Syst. II, Exp. Briefs, vol. 61, no. 11, pp. 895-899, Nov. 2014.\n\nGanged CMOS: Trading standby power for speed. K L Schultz, R J Francis, K C Smith, IEEE J. Solid-State Circuits. 253K. L. Schultz, R. J. Francis, and K. C. Smith, \"Ganged CMOS: Trading standby power for speed,\" IEEE J. Solid-State Circuits, vol. 25, no. 3, pp. 870-873, Jun. 1990.\n\nTrue random number generator using current difference based on a fractional stochastic model in 40-nm embedded ReRAM. Z Wei, Proc. IEEE Int. Electron Devices Meeting (IEDM). IEEE Int. Electron Devices Meeting (IEDM)Z. Wei et al., \"True random number generator using current dif- ference based on a fractional stochastic model in 40-nm embedded ReRAM,\" in Proc. IEEE Int. Electron Devices Meeting (IEDM), 2016, pp. 4-8.\n\nA novel true random number generator design leveraging emerging memristor technology. Y Wang, W Wen, H Li, M Hu, Proc. 25th Ed. Great Lakes Symp. (VLSI). 25th Ed. Great Lakes Symp. (VLSI)Y. Wang, W. Wen, H. Li, and M. Hu, \"A novel true random number generator design leveraging emerging memristor technology,\" in Proc. 25th Ed. Great Lakes Symp. (VLSI), 2015, pp. 271-276.\n\nProgrammable resistance switching in nanoscale two-terminal devices. S H Jo, K.-H Kim, W Lu, Nano Lett. 91S. H. Jo, K.-H. Kim, and W. Lu, \"Programmable resistance switching in nanoscale two-terminal devices,\" Nano Lett., vol. 9, no. 1, pp. 496-500, 2008.\n\nA novel true random number generator based on a stochastic diffusive memristor. H Jiang, Nature Commun. 81882H. Jiang et al., \"A novel true random number generator based on a stochastic diffusive memristor,\" Nature Commun., vol. 8, no. 1, p. 882, 2017.\n\nTSMC 45 nm Technology. (2010). TSMC 45 nm Technology. [Online]. Available: https:// www.synopsys.com/dw/emllselector.php?f=TSMC&n=45&s=wMoRVw\n\nVTEAM: A general model for voltage-controlled memristors. S Kvatinsky, M Ramadan, E G Friedman, A Kolodny, IEEE Trans. Circuits Syst. II, Exp. Briefs. 628S. Kvatinsky, M. Ramadan, E. G. Friedman, and A. Kolodny, \"VTEAM: A general model for voltage-controlled memristors,\" IEEE Trans. Circuits Syst. II, Exp. Briefs, vol. 62, no. 8, pp. 786-790, Aug. 2015.\n\nORCHARD: Visual object recognition accelerator based on approximate in-memory processing. Y Kim, M Imani, T Rosing, Proc. nullY. Kim, M. Imani, and T. Rosing, \"ORCHARD: Visual object recogni- tion accelerator based on approximate in-memory processing,\" in Proc.\n\n. IEEE/ACM Int. Conf. Comput.-Aided Design. IEEE/ACM Int. Conf. Comput.-Aided Design (ICCAD), 2017, pp. 25-32.\n", "annotations": {"author": "[{\"start\":\"93\",\"end\":\"106\"},{\"start\":\"107\",\"end\":\"119\"},{\"start\":\"120\",\"end\":\"134\"},{\"start\":\"135\",\"end\":\"169\"},{\"start\":\"170\",\"end\":\"213\"},{\"start\":\"214\",\"end\":\"236\"},{\"start\":\"237\",\"end\":\"263\"}]", "publisher": null, "author_last_name": "[{\"start\":\"100\",\"end\":\"105\"},{\"start\":\"115\",\"end\":\"118\"},{\"start\":\"125\",\"end\":\"133\"},{\"start\":\"163\",\"end\":\"168\"},{\"start\":\"233\",\"end\":\"235\"},{\"start\":\"256\",\"end\":\"262\"}]", "author_first_name": "[{\"start\":\"93\",\"end\":\"99\"},{\"start\":\"107\",\"end\":\"114\"},{\"start\":\"120\",\"end\":\"124\"},{\"start\":\"155\",\"end\":\"162\"},{\"start\":\"197\",\"end\":\"204\"},{\"start\":\"205\",\"end\":\"212\"},{\"start\":\"226\",\"end\":\"232\"},{\"start\":\"249\",\"end\":\"255\"}]", "author_affiliation": null, "title": "[{\"start\":\"1\",\"end\":\"78\"},{\"start\":\"264\",\"end\":\"341\"}]", "venue": "[{\"start\":\"343\",\"end\":\"420\"}]", "abstract": "[{\"start\":\"467\",\"end\":\"1688\"}]", "bib_ref": "[{\"start\":\"1821\",\"end\":\"1824\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"1845\",\"end\":\"1848\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"1850\",\"end\":\"1853\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"1876\",\"end\":\"1879\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"1881\",\"end\":\"1884\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"2047\",\"end\":\"2050\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"2065\",\"end\":\"2068\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"2169\",\"end\":\"2172\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"2582\",\"end\":\"2585\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"2587\",\"end\":\"2591\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"2753\",\"end\":\"2757\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"2995\",\"end\":\"2999\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"3001\",\"end\":\"3005\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"3279\",\"end\":\"3283\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"4015\",\"end\":\"4019\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"4167\",\"end\":\"4171\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"4173\",\"end\":\"4177\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"4650\",\"end\":\"4654\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"4927\",\"end\":\"4930\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"4935\",\"end\":\"4939\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"5011\",\"end\":\"5015\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"7628\",\"end\":\"7632\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"7634\",\"end\":\"7638\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"8649\",\"end\":\"8653\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"8680\",\"end\":\"8684\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"8707\",\"end\":\"8711\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"8713\",\"end\":\"8717\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"8739\",\"end\":\"8743\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"8765\",\"end\":\"8769\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"8812\",\"end\":\"8816\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"8818\",\"end\":\"8822\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"8833\",\"end\":\"8837\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"8862\",\"end\":\"8866\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"8868\",\"end\":\"8872\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"9174\",\"end\":\"9178\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"9180\",\"end\":\"9184\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"9186\",\"end\":\"9190\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"9192\",\"end\":\"9196\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"9198\",\"end\":\"9202\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"9304\",\"end\":\"9308\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"9310\",\"end\":\"9314\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"10309\",\"end\":\"10313\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"10315\",\"end\":\"10319\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"10419\",\"end\":\"10422\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"10424\",\"end\":\"10428\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"10430\",\"end\":\"10434\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"10448\",\"end\":\"10451\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"10567\",\"end\":\"10571\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"10736\",\"end\":\"10740\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"11420\",\"end\":\"11424\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"11426\",\"end\":\"11430\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"11888\",\"end\":\"11892\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"11894\",\"end\":\"11898\",\"attributes\":{\"ref_id\":\"b46\"}},{\"start\":\"12873\",\"end\":\"12877\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"13916\",\"end\":\"13920\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"13922\",\"end\":\"13926\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"13928\",\"end\":\"13932\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"19599\",\"end\":\"19602\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"19604\",\"end\":\"19608\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"24698\",\"end\":\"24702\",\"attributes\":{\"ref_id\":\"b48\"}},{\"start\":\"28324\",\"end\":\"28328\",\"attributes\":{\"ref_id\":\"b49\"}},{\"start\":\"31213\",\"end\":\"31217\",\"attributes\":{\"ref_id\":\"b50\"}},{\"start\":\"31219\",\"end\":\"31223\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"31900\",\"end\":\"31904\",\"attributes\":{\"ref_id\":\"b52\"}},{\"start\":\"31906\",\"end\":\"31910\",\"attributes\":{\"ref_id\":\"b53\"}},{\"start\":\"34214\",\"end\":\"34218\",\"attributes\":{\"ref_id\":\"b54\"}},{\"start\":\"34338\",\"end\":\"34342\",\"attributes\":{\"ref_id\":\"b55\"}},{\"start\":\"34559\",\"end\":\"34563\",\"attributes\":{\"ref_id\":\"b48\"}},{\"start\":\"36860\",\"end\":\"36864\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"38608\",\"end\":\"38612\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"38617\",\"end\":\"38621\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"42003\",\"end\":\"42007\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"42257\",\"end\":\"42261\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"42419\",\"end\":\"42423\",\"attributes\":{\"ref_id\":\"b20\"}}]", "figure": "[{\"start\":\"47725\",\"end\":\"47799\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"47800\",\"end\":\"47903\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"47904\",\"end\":\"48264\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"48265\",\"end\":\"48402\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"48403\",\"end\":\"48642\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"48643\",\"end\":\"48769\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"48770\",\"end\":\"48942\",\"attributes\":{\"id\":\"fig_6\"}},{\"start\":\"48943\",\"end\":\"49398\",\"attributes\":{\"id\":\"fig_7\"}},{\"start\":\"49399\",\"end\":\"49572\",\"attributes\":{\"id\":\"fig_8\"}},{\"start\":\"49573\",\"end\":\"49731\",\"attributes\":{\"id\":\"fig_9\"}},{\"start\":\"49732\",\"end\":\"49803\",\"attributes\":{\"id\":\"fig_10\"}},{\"start\":\"49804\",\"end\":\"50061\",\"attributes\":{\"id\":\"fig_11\"}},{\"start\":\"50062\",\"end\":\"50201\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}},{\"start\":\"50202\",\"end\":\"51503\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}},{\"start\":\"51504\",\"end\":\"51552\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}},{\"start\":\"51553\",\"end\":\"51946\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}},{\"start\":\"51947\",\"end\":\"52031\",\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"}},{\"start\":\"52032\",\"end\":\"52149\",\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"1707\",\"end\":\"2378\"},{\"start\":\"2380\",\"end\":\"3198\"},{\"start\":\"3200\",\"end\":\"3284\"},{\"start\":\"3286\",\"end\":\"4020\"},{\"start\":\"4022\",\"end\":\"4829\"},{\"start\":\"4831\",\"end\":\"4908\"},{\"start\":\"4910\",\"end\":\"5778\"},{\"start\":\"5780\",\"end\":\"6905\"},{\"start\":\"6907\",\"end\":\"7406\"},{\"start\":\"7440\",\"end\":\"8548\"},{\"start\":\"8550\",\"end\":\"9571\"},{\"start\":\"9595\",\"end\":\"10178\"},{\"start\":\"10180\",\"end\":\"12187\"},{\"start\":\"12231\",\"end\":\"12639\"},{\"start\":\"12663\",\"end\":\"13051\"},{\"start\":\"13101\",\"end\":\"13586\"},{\"start\":\"13620\",\"end\":\"14682\"},{\"start\":\"14684\",\"end\":\"15836\"},{\"start\":\"15838\",\"end\":\"17112\"},{\"start\":\"17171\",\"end\":\"17709\"},{\"start\":\"17730\",\"end\":\"18214\"},{\"start\":\"18216\",\"end\":\"19039\"},{\"start\":\"19056\",\"end\":\"20019\"},{\"start\":\"20035\",\"end\":\"20226\"},{\"start\":\"20228\",\"end\":\"21648\"},{\"start\":\"21650\",\"end\":\"22059\"},{\"start\":\"22085\",\"end\":\"23468\"},{\"start\":\"23470\",\"end\":\"25863\"},{\"start\":\"25899\",\"end\":\"26822\"},{\"start\":\"26824\",\"end\":\"28274\"},{\"start\":\"28276\",\"end\":\"29398\"},{\"start\":\"29433\",\"end\":\"30276\"},{\"start\":\"30328\",\"end\":\"31059\"},{\"start\":\"31061\",\"end\":\"31395\"},{\"start\":\"31435\",\"end\":\"32152\"},{\"start\":\"32154\",\"end\":\"33083\"},{\"start\":\"33101\",\"end\":\"33521\"},{\"start\":\"33547\",\"end\":\"39484\"},{\"start\":\"39618\",\"end\":\"39982\"},{\"start\":\"40018\",\"end\":\"40642\"},{\"start\":\"40644\",\"end\":\"41072\"},{\"start\":\"41074\",\"end\":\"41890\"},{\"start\":\"41892\",\"end\":\"42525\"},{\"start\":\"42561\",\"end\":\"43421\"},{\"start\":\"43450\",\"end\":\"45335\"},{\"start\":\"45362\",\"end\":\"46154\"},{\"start\":\"46186\",\"end\":\"46865\"},{\"start\":\"46884\",\"end\":\"47724\"}]", "formula": "[{\"start\":\"9572\",\"end\":\"9594\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"13052\",\"end\":\"13100\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"17113\",\"end\":\"17170\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"17710\",\"end\":\"17729\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"31396\",\"end\":\"31434\",\"attributes\":{\"id\":\"formula_4\"}}]", "table_ref": "[{\"start\":\"11151\",\"end\":\"11158\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"34408\",\"end\":\"34416\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"34841\",\"end\":\"34849\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"34914\",\"end\":\"34921\"},{\"start\":\"37207\",\"end\":\"37215\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"45656\",\"end\":\"45665\",\"attributes\":{\"ref_id\":\"tab_0\"}}]", "section_header": "[{\"start\":\"1690\",\"end\":\"1705\"},{\"start\":\"7409\",\"end\":\"7438\"},{\"start\":\"12190\",\"end\":\"12229\"},{\"start\":\"12642\",\"end\":\"12661\"},{\"start\":\"13589\",\"end\":\"13618\"},{\"start\":\"19042\",\"end\":\"19054\"},{\"start\":\"20022\",\"end\":\"20033\"},{\"start\":\"22062\",\"end\":\"22083\"},{\"start\":\"25866\",\"end\":\"25897\"},{\"start\":\"29401\",\"end\":\"29431\"},{\"start\":\"30279\",\"end\":\"30326\"},{\"start\":\"33086\",\"end\":\"33099\"},{\"start\":\"33524\",\"end\":\"33545\"},{\"start\":\"39487\",\"end\":\"39515\"},{\"start\":\"39518\",\"end\":\"39565\"},{\"start\":\"39568\",\"end\":\"39590\"},{\"start\":\"39593\",\"end\":\"39616\"},{\"start\":\"39985\",\"end\":\"40016\"},{\"start\":\"42528\",\"end\":\"42559\"},{\"start\":\"43424\",\"end\":\"43448\"},{\"start\":\"45338\",\"end\":\"45360\"},{\"start\":\"46157\",\"end\":\"46184\"},{\"start\":\"46868\",\"end\":\"46882\"},{\"start\":\"47726\",\"end\":\"47734\"},{\"start\":\"47801\",\"end\":\"47809\"},{\"start\":\"47905\",\"end\":\"47913\"},{\"start\":\"48266\",\"end\":\"48274\"},{\"start\":\"48404\",\"end\":\"48410\"},{\"start\":\"48644\",\"end\":\"48652\"},{\"start\":\"48771\",\"end\":\"48779\"},{\"start\":\"48944\",\"end\":\"48959\"},{\"start\":\"49400\",\"end\":\"49408\"},{\"start\":\"49574\",\"end\":\"49582\"},{\"start\":\"49733\",\"end\":\"49741\"},{\"start\":\"50063\",\"end\":\"50085\"},{\"start\":\"51505\",\"end\":\"51519\"},{\"start\":\"51554\",\"end\":\"51591\"},{\"start\":\"51948\",\"end\":\"51956\"},{\"start\":\"52033\",\"end\":\"52053\"}]", "table": "[{\"start\":\"50351\",\"end\":\"51503\"},{\"start\":\"51886\",\"end\":\"51946\"},{\"start\":\"51959\",\"end\":\"52031\"}]", "figure_caption": "[{\"start\":\"47736\",\"end\":\"47799\"},{\"start\":\"47811\",\"end\":\"47903\"},{\"start\":\"47915\",\"end\":\"48264\"},{\"start\":\"48276\",\"end\":\"48402\"},{\"start\":\"48412\",\"end\":\"48642\"},{\"start\":\"48654\",\"end\":\"48769\"},{\"start\":\"48781\",\"end\":\"48942\"},{\"start\":\"48961\",\"end\":\"49398\"},{\"start\":\"49410\",\"end\":\"49572\"},{\"start\":\"49584\",\"end\":\"49731\"},{\"start\":\"49743\",\"end\":\"49803\"},{\"start\":\"49806\",\"end\":\"50061\"},{\"start\":\"50087\",\"end\":\"50201\"},{\"start\":\"50204\",\"end\":\"50351\"},{\"start\":\"51522\",\"end\":\"51552\"},{\"start\":\"51607\",\"end\":\"51886\"},{\"start\":\"52057\",\"end\":\"52149\"}]", "figure_ref": "[{\"start\":\"8874\",\"end\":\"8880\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"9472\",\"end\":\"9478\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"13044\",\"end\":\"13050\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"16915\",\"end\":\"16924\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"22107\",\"end\":\"22116\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"22943\",\"end\":\"22952\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"23645\",\"end\":\"23654\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"24032\",\"end\":\"24041\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"24949\",\"end\":\"24958\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"26235\",\"end\":\"26244\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"27429\",\"end\":\"27438\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"28446\",\"end\":\"28455\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"28992\",\"end\":\"29001\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"29975\",\"end\":\"29984\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"36726\",\"end\":\"36732\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"37835\",\"end\":\"37841\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"40308\",\"end\":\"40314\",\"attributes\":{\"ref_id\":\"fig_9\"}},{\"start\":\"42928\",\"end\":\"42934\",\"attributes\":{\"ref_id\":\"fig_10\"}},{\"start\":\"43557\",\"end\":\"43567\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"44152\",\"end\":\"44162\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"44761\",\"end\":\"44771\",\"attributes\":{\"ref_id\":\"fig_0\"}}]", "bib_author_first_name": "[{\"start\":\"52218\",\"end\":\"52219\"},{\"start\":\"52227\",\"end\":\"52228\"},{\"start\":\"52236\",\"end\":\"52237\"},{\"start\":\"52572\",\"end\":\"52573\"},{\"start\":\"52833\",\"end\":\"52834\"},{\"start\":\"53053\",\"end\":\"53054\"},{\"start\":\"53067\",\"end\":\"53068\"},{\"start\":\"53080\",\"end\":\"53081\"},{\"start\":\"53082\",\"end\":\"53083\"},{\"start\":\"53382\",\"end\":\"53383\"},{\"start\":\"53388\",\"end\":\"53389\"},{\"start\":\"53397\",\"end\":\"53398\"},{\"start\":\"53404\",\"end\":\"53405\"},{\"start\":\"53688\",\"end\":\"53689\"},{\"start\":\"53959\",\"end\":\"53960\"},{\"start\":\"54285\",\"end\":\"54286\"},{\"start\":\"54706\",\"end\":\"54707\"},{\"start\":\"54713\",\"end\":\"54714\"},{\"start\":\"54721\",\"end\":\"54722\"},{\"start\":\"54729\",\"end\":\"54730\"},{\"start\":\"54745\",\"end\":\"54746\"},{\"start\":\"55183\",\"end\":\"55184\"},{\"start\":\"55189\",\"end\":\"55190\"},{\"start\":\"55198\",\"end\":\"55202\"},{\"start\":\"55203\",\"end\":\"55204\"},{\"start\":\"55546\",\"end\":\"55547\"},{\"start\":\"55919\",\"end\":\"55920\"},{\"start\":\"56243\",\"end\":\"56244\"},{\"start\":\"56254\",\"end\":\"56255\"},{\"start\":\"56266\",\"end\":\"56267\"},{\"start\":\"56268\",\"end\":\"56269\"},{\"start\":\"56281\",\"end\":\"56282\"},{\"start\":\"56294\",\"end\":\"56295\"},{\"start\":\"56304\",\"end\":\"56305\"},{\"start\":\"56737\",\"end\":\"56738\"},{\"start\":\"56739\",\"end\":\"56740\"},{\"start\":\"56750\",\"end\":\"56751\"},{\"start\":\"56752\",\"end\":\"56753\"},{\"start\":\"57122\",\"end\":\"57123\"},{\"start\":\"57131\",\"end\":\"57132\"},{\"start\":\"57140\",\"end\":\"57141\"},{\"start\":\"57148\",\"end\":\"57149\"},{\"start\":\"57492\",\"end\":\"57493\"},{\"start\":\"57502\",\"end\":\"57503\"},{\"start\":\"57516\",\"end\":\"57517\"},{\"start\":\"57527\",\"end\":\"57528\"},{\"start\":\"57529\",\"end\":\"57532\"},{\"start\":\"57541\",\"end\":\"57542\"},{\"start\":\"57543\",\"end\":\"57544\"},{\"start\":\"57899\",\"end\":\"57900\"},{\"start\":\"57911\",\"end\":\"57912\"},{\"start\":\"57921\",\"end\":\"57922\"},{\"start\":\"57932\",\"end\":\"57933\"},{\"start\":\"57941\",\"end\":\"57942\"},{\"start\":\"58356\",\"end\":\"58357\"},{\"start\":\"58365\",\"end\":\"58366\"},{\"start\":\"58375\",\"end\":\"58376\"},{\"start\":\"58387\",\"end\":\"58388\"},{\"start\":\"58394\",\"end\":\"58395\"},{\"start\":\"58402\",\"end\":\"58403\"},{\"start\":\"58786\",\"end\":\"58787\"},{\"start\":\"58795\",\"end\":\"58796\"},{\"start\":\"58807\",\"end\":\"58808\"},{\"start\":\"58813\",\"end\":\"58814\"},{\"start\":\"58819\",\"end\":\"58820\"},{\"start\":\"59219\",\"end\":\"59220\"},{\"start\":\"59229\",\"end\":\"59230\"},{\"start\":\"59240\",\"end\":\"59241\"},{\"start\":\"59242\",\"end\":\"59243\"},{\"start\":\"59580\",\"end\":\"59581\"},{\"start\":\"59589\",\"end\":\"59590\"},{\"start\":\"59599\",\"end\":\"59600\"},{\"start\":\"59607\",\"end\":\"59608\"},{\"start\":\"59617\",\"end\":\"59618\"},{\"start\":\"59619\",\"end\":\"59620\"},{\"start\":\"60016\",\"end\":\"60017\"},{\"start\":\"60025\",\"end\":\"60026\"},{\"start\":\"60034\",\"end\":\"60035\"},{\"start\":\"60041\",\"end\":\"60042\"},{\"start\":\"60388\",\"end\":\"60389\"},{\"start\":\"60711\",\"end\":\"60712\"},{\"start\":\"61035\",\"end\":\"61036\"},{\"start\":\"61042\",\"end\":\"61043\"},{\"start\":\"61049\",\"end\":\"61050\"},{\"start\":\"61058\",\"end\":\"61059\"},{\"start\":\"61460\",\"end\":\"61461\"},{\"start\":\"61467\",\"end\":\"61468\"},{\"start\":\"61475\",\"end\":\"61476\"},{\"start\":\"61482\",\"end\":\"61483\"},{\"start\":\"61491\",\"end\":\"61492\"},{\"start\":\"61931\",\"end\":\"61932\"},{\"start\":\"61937\",\"end\":\"61938\"},{\"start\":\"61943\",\"end\":\"61944\"},{\"start\":\"61950\",\"end\":\"61951\"},{\"start\":\"61958\",\"end\":\"61959\"},{\"start\":\"61964\",\"end\":\"61965\"},{\"start\":\"62364\",\"end\":\"62365\"},{\"start\":\"62373\",\"end\":\"62374\"},{\"start\":\"62384\",\"end\":\"62385\"},{\"start\":\"62391\",\"end\":\"62392\"},{\"start\":\"62400\",\"end\":\"62401\"},{\"start\":\"62414\",\"end\":\"62415\"},{\"start\":\"62825\",\"end\":\"62826\"},{\"start\":\"63102\",\"end\":\"63103\"},{\"start\":\"63109\",\"end\":\"63110\"},{\"start\":\"63117\",\"end\":\"63118\"},{\"start\":\"63128\",\"end\":\"63129\"},{\"start\":\"63130\",\"end\":\"63131\"},{\"start\":\"63458\",\"end\":\"63459\"},{\"start\":\"63467\",\"end\":\"63468\"},{\"start\":\"63476\",\"end\":\"63477\"},{\"start\":\"63830\",\"end\":\"63831\"},{\"start\":\"63832\",\"end\":\"63833\"},{\"start\":\"64212\",\"end\":\"64213\"},{\"start\":\"64221\",\"end\":\"64222\"},{\"start\":\"64231\",\"end\":\"64232\"},{\"start\":\"64233\",\"end\":\"64234\"},{\"start\":\"64633\",\"end\":\"64634\"},{\"start\":\"64977\",\"end\":\"64978\"},{\"start\":\"64988\",\"end\":\"64989\"},{\"start\":\"65005\",\"end\":\"65006\"},{\"start\":\"65322\",\"end\":\"65323\"},{\"start\":\"65331\",\"end\":\"65332\"},{\"start\":\"65342\",\"end\":\"65343\"},{\"start\":\"65653\",\"end\":\"65654\"},{\"start\":\"65663\",\"end\":\"65664\"},{\"start\":\"65674\",\"end\":\"65675\"},{\"start\":\"65676\",\"end\":\"65677\"},{\"start\":\"66024\",\"end\":\"66025\"},{\"start\":\"66026\",\"end\":\"66027\"},{\"start\":\"66040\",\"end\":\"66041\"},{\"start\":\"66050\",\"end\":\"66051\"},{\"start\":\"66061\",\"end\":\"66062\"},{\"start\":\"66063\",\"end\":\"66064\"},{\"start\":\"66488\",\"end\":\"66489\"},{\"start\":\"66498\",\"end\":\"66499\"},{\"start\":\"66509\",\"end\":\"66510\"},{\"start\":\"66520\",\"end\":\"66521\"},{\"start\":\"66530\",\"end\":\"66531\"},{\"start\":\"66532\",\"end\":\"66533\"},{\"start\":\"66944\",\"end\":\"66945\"},{\"start\":\"66955\",\"end\":\"66956\"},{\"start\":\"67296\",\"end\":\"67297\"},{\"start\":\"67637\",\"end\":\"67638\"},{\"start\":\"67646\",\"end\":\"67647\"},{\"start\":\"67654\",\"end\":\"67655\"},{\"start\":\"67664\",\"end\":\"67665\"},{\"start\":\"68138\",\"end\":\"68139\"},{\"start\":\"68140\",\"end\":\"68141\"},{\"start\":\"68155\",\"end\":\"68156\"},{\"start\":\"68168\",\"end\":\"68169\"},{\"start\":\"68179\",\"end\":\"68180\"},{\"start\":\"68196\",\"end\":\"68197\"},{\"start\":\"68550\",\"end\":\"68551\"},{\"start\":\"68561\",\"end\":\"68562\"},{\"start\":\"68569\",\"end\":\"68570\"},{\"start\":\"68578\",\"end\":\"68579\"},{\"start\":\"68587\",\"end\":\"68588\"},{\"start\":\"68589\",\"end\":\"68590\"},{\"start\":\"68997\",\"end\":\"68998\"},{\"start\":\"69007\",\"end\":\"69008\"},{\"start\":\"69020\",\"end\":\"69021\"},{\"start\":\"69031\",\"end\":\"69032\"},{\"start\":\"69290\",\"end\":\"69291\"},{\"start\":\"69297\",\"end\":\"69298\"},{\"start\":\"69303\",\"end\":\"69304\"},{\"start\":\"69311\",\"end\":\"69312\"},{\"start\":\"69320\",\"end\":\"69321\"},{\"start\":\"69331\",\"end\":\"69332\"},{\"start\":\"69333\",\"end\":\"69334\"},{\"start\":\"69669\",\"end\":\"69670\"},{\"start\":\"69680\",\"end\":\"69681\"},{\"start\":\"69697\",\"end\":\"69698\"},{\"start\":\"69989\",\"end\":\"69990\"},{\"start\":\"70234\",\"end\":\"70235\"},{\"start\":\"70236\",\"end\":\"70237\"},{\"start\":\"70247\",\"end\":\"70248\"},{\"start\":\"70249\",\"end\":\"70250\"},{\"start\":\"70260\",\"end\":\"70261\"},{\"start\":\"70262\",\"end\":\"70263\"},{\"start\":\"70588\",\"end\":\"70589\"},{\"start\":\"70976\",\"end\":\"70977\"},{\"start\":\"70984\",\"end\":\"70985\"},{\"start\":\"70991\",\"end\":\"70992\"},{\"start\":\"70997\",\"end\":\"70998\"},{\"start\":\"71333\",\"end\":\"71334\"},{\"start\":\"71335\",\"end\":\"71336\"},{\"start\":\"71341\",\"end\":\"71345\"},{\"start\":\"71351\",\"end\":\"71352\"},{\"start\":\"71600\",\"end\":\"71601\"},{\"start\":\"71975\",\"end\":\"71976\"},{\"start\":\"71988\",\"end\":\"71989\"},{\"start\":\"71999\",\"end\":\"72000\"},{\"start\":\"72001\",\"end\":\"72002\"},{\"start\":\"72013\",\"end\":\"72014\"},{\"start\":\"72364\",\"end\":\"72365\"},{\"start\":\"72371\",\"end\":\"72372\"},{\"start\":\"72380\",\"end\":\"72381\"}]", "bib_author_last_name": "[{\"start\":\"52220\",\"end\":\"52225\"},{\"start\":\"52229\",\"end\":\"52234\"},{\"start\":\"52238\",\"end\":\"52246\"},{\"start\":\"52574\",\"end\":\"52580\"},{\"start\":\"52835\",\"end\":\"52841\"},{\"start\":\"53055\",\"end\":\"53065\"},{\"start\":\"53069\",\"end\":\"53078\"},{\"start\":\"53084\",\"end\":\"53090\"},{\"start\":\"53384\",\"end\":\"53386\"},{\"start\":\"53390\",\"end\":\"53395\"},{\"start\":\"53399\",\"end\":\"53402\"},{\"start\":\"53406\",\"end\":\"53409\"},{\"start\":\"53690\",\"end\":\"53697\"},{\"start\":\"53961\",\"end\":\"53972\"},{\"start\":\"54287\",\"end\":\"54289\"},{\"start\":\"54708\",\"end\":\"54711\"},{\"start\":\"54715\",\"end\":\"54719\"},{\"start\":\"54723\",\"end\":\"54727\"},{\"start\":\"54731\",\"end\":\"54743\"},{\"start\":\"54747\",\"end\":\"54759\"},{\"start\":\"55185\",\"end\":\"55187\"},{\"start\":\"55191\",\"end\":\"55196\"},{\"start\":\"55205\",\"end\":\"55209\"},{\"start\":\"55548\",\"end\":\"55553\"},{\"start\":\"55921\",\"end\":\"55928\"},{\"start\":\"56245\",\"end\":\"56252\"},{\"start\":\"56256\",\"end\":\"56264\"},{\"start\":\"56270\",\"end\":\"56279\"},{\"start\":\"56283\",\"end\":\"56292\"},{\"start\":\"56296\",\"end\":\"56302\"},{\"start\":\"56306\",\"end\":\"56312\"},{\"start\":\"56741\",\"end\":\"56748\"},{\"start\":\"56754\",\"end\":\"56762\"},{\"start\":\"57124\",\"end\":\"57129\"},{\"start\":\"57133\",\"end\":\"57138\"},{\"start\":\"57142\",\"end\":\"57146\"},{\"start\":\"57150\",\"end\":\"57156\"},{\"start\":\"57494\",\"end\":\"57500\"},{\"start\":\"57504\",\"end\":\"57514\"},{\"start\":\"57518\",\"end\":\"57525\"},{\"start\":\"57533\",\"end\":\"57539\"},{\"start\":\"57545\",\"end\":\"57551\"},{\"start\":\"57901\",\"end\":\"57909\"},{\"start\":\"57913\",\"end\":\"57919\"},{\"start\":\"57923\",\"end\":\"57930\"},{\"start\":\"57934\",\"end\":\"57939\"},{\"start\":\"57943\",\"end\":\"57949\"},{\"start\":\"58358\",\"end\":\"58363\"},{\"start\":\"58367\",\"end\":\"58373\"},{\"start\":\"58377\",\"end\":\"58385\"},{\"start\":\"58389\",\"end\":\"58392\"},{\"start\":\"58396\",\"end\":\"58400\"},{\"start\":\"58404\",\"end\":\"58410\"},{\"start\":\"58788\",\"end\":\"58793\"},{\"start\":\"58797\",\"end\":\"58805\"},{\"start\":\"58809\",\"end\":\"58811\"},{\"start\":\"58815\",\"end\":\"58817\"},{\"start\":\"58821\",\"end\":\"58827\"},{\"start\":\"59221\",\"end\":\"59227\"},{\"start\":\"59231\",\"end\":\"59238\"},{\"start\":\"59244\",\"end\":\"59250\"},{\"start\":\"59582\",\"end\":\"59587\"},{\"start\":\"59591\",\"end\":\"59597\"},{\"start\":\"59601\",\"end\":\"59605\"},{\"start\":\"59609\",\"end\":\"59615\"},{\"start\":\"59621\",\"end\":\"59627\"},{\"start\":\"60018\",\"end\":\"60023\"},{\"start\":\"60027\",\"end\":\"60032\"},{\"start\":\"60036\",\"end\":\"60039\"},{\"start\":\"60043\",\"end\":\"60049\"},{\"start\":\"60390\",\"end\":\"60393\"},{\"start\":\"60713\",\"end\":\"60720\"},{\"start\":\"61037\",\"end\":\"61040\"},{\"start\":\"61044\",\"end\":\"61047\"},{\"start\":\"61051\",\"end\":\"61056\"},{\"start\":\"61060\",\"end\":\"61064\"},{\"start\":\"61462\",\"end\":\"61465\"},{\"start\":\"61469\",\"end\":\"61473\"},{\"start\":\"61477\",\"end\":\"61480\"},{\"start\":\"61484\",\"end\":\"61489\"},{\"start\":\"61493\",\"end\":\"61497\"},{\"start\":\"61933\",\"end\":\"61935\"},{\"start\":\"61939\",\"end\":\"61941\"},{\"start\":\"61945\",\"end\":\"61948\"},{\"start\":\"61952\",\"end\":\"61956\"},{\"start\":\"61960\",\"end\":\"61962\"},{\"start\":\"61966\",\"end\":\"61969\"},{\"start\":\"62366\",\"end\":\"62371\"},{\"start\":\"62375\",\"end\":\"62382\"},{\"start\":\"62386\",\"end\":\"62389\"},{\"start\":\"62393\",\"end\":\"62398\"},{\"start\":\"62402\",\"end\":\"62412\"},{\"start\":\"62416\",\"end\":\"62422\"},{\"start\":\"62827\",\"end\":\"62836\"},{\"start\":\"63104\",\"end\":\"63107\"},{\"start\":\"63111\",\"end\":\"63115\"},{\"start\":\"63119\",\"end\":\"63126\"},{\"start\":\"63132\",\"end\":\"63134\"},{\"start\":\"63460\",\"end\":\"63465\"},{\"start\":\"63469\",\"end\":\"63474\"},{\"start\":\"63478\",\"end\":\"63484\"},{\"start\":\"63834\",\"end\":\"63836\"},{\"start\":\"64214\",\"end\":\"64219\"},{\"start\":\"64223\",\"end\":\"64229\"},{\"start\":\"64235\",\"end\":\"64241\"},{\"start\":\"64635\",\"end\":\"64642\"},{\"start\":\"64979\",\"end\":\"64986\"},{\"start\":\"64990\",\"end\":\"65003\"},{\"start\":\"65007\",\"end\":\"65012\"},{\"start\":\"65324\",\"end\":\"65329\"},{\"start\":\"65333\",\"end\":\"65340\"},{\"start\":\"65344\",\"end\":\"65351\"},{\"start\":\"65655\",\"end\":\"65661\"},{\"start\":\"65665\",\"end\":\"65672\"},{\"start\":\"65678\",\"end\":\"65684\"},{\"start\":\"66028\",\"end\":\"66038\"},{\"start\":\"66042\",\"end\":\"66048\"},{\"start\":\"66052\",\"end\":\"66059\"},{\"start\":\"66065\",\"end\":\"66071\"},{\"start\":\"66490\",\"end\":\"66496\"},{\"start\":\"66500\",\"end\":\"66507\"},{\"start\":\"66511\",\"end\":\"66518\"},{\"start\":\"66522\",\"end\":\"66528\"},{\"start\":\"66534\",\"end\":\"66540\"},{\"start\":\"66946\",\"end\":\"66953\"},{\"start\":\"66957\",\"end\":\"66965\"},{\"start\":\"67639\",\"end\":\"67644\"},{\"start\":\"67648\",\"end\":\"67652\"},{\"start\":\"67656\",\"end\":\"67662\"},{\"start\":\"67666\",\"end\":\"67672\"},{\"start\":\"68142\",\"end\":\"68153\"},{\"start\":\"68157\",\"end\":\"68166\"},{\"start\":\"68170\",\"end\":\"68177\"},{\"start\":\"68181\",\"end\":\"68194\"},{\"start\":\"68198\",\"end\":\"68211\"},{\"start\":\"68552\",\"end\":\"68559\"},{\"start\":\"68563\",\"end\":\"68567\"},{\"start\":\"68571\",\"end\":\"68576\"},{\"start\":\"68580\",\"end\":\"68585\"},{\"start\":\"68591\",\"end\":\"68602\"},{\"start\":\"68999\",\"end\":\"69005\"},{\"start\":\"69009\",\"end\":\"69018\"},{\"start\":\"69022\",\"end\":\"69029\"},{\"start\":\"69033\",\"end\":\"69040\"},{\"start\":\"69292\",\"end\":\"69295\"},{\"start\":\"69299\",\"end\":\"69301\"},{\"start\":\"69305\",\"end\":\"69309\"},{\"start\":\"69313\",\"end\":\"69318\"},{\"start\":\"69322\",\"end\":\"69329\"},{\"start\":\"69335\",\"end\":\"69337\"},{\"start\":\"69671\",\"end\":\"69678\"},{\"start\":\"69682\",\"end\":\"69695\"},{\"start\":\"69699\",\"end\":\"69704\"},{\"start\":\"69991\",\"end\":\"70000\"},{\"start\":\"70238\",\"end\":\"70245\"},{\"start\":\"70251\",\"end\":\"70258\"},{\"start\":\"70264\",\"end\":\"70269\"},{\"start\":\"70590\",\"end\":\"70593\"},{\"start\":\"70978\",\"end\":\"70982\"},{\"start\":\"70986\",\"end\":\"70989\"},{\"start\":\"70993\",\"end\":\"70995\"},{\"start\":\"70999\",\"end\":\"71001\"},{\"start\":\"71337\",\"end\":\"71339\"},{\"start\":\"71346\",\"end\":\"71349\"},{\"start\":\"71353\",\"end\":\"71355\"},{\"start\":\"71602\",\"end\":\"71607\"},{\"start\":\"71977\",\"end\":\"71986\"},{\"start\":\"71990\",\"end\":\"71997\"},{\"start\":\"72003\",\"end\":\"72011\"},{\"start\":\"72015\",\"end\":\"72022\"},{\"start\":\"72366\",\"end\":\"72369\"},{\"start\":\"72373\",\"end\":\"72378\"},{\"start\":\"72382\",\"end\":\"72388\"}]", "bib_entry": "[{\"start\":\"52151\",\"end\":\"52500\",\"attributes\":{\"matched_paper_id\":\"10991044\",\"id\":\"b0\"}},{\"start\":\"52502\",\"end\":\"52776\",\"attributes\":{\"matched_paper_id\":\"11590585\",\"id\":\"b1\"}},{\"start\":\"52778\",\"end\":\"52986\",\"attributes\":{\"id\":\"b2\",\"doi\":\"arXiv:1412.5567\"}},{\"start\":\"52988\",\"end\":\"53334\",\"attributes\":{\"matched_paper_id\":\"195908774\",\"id\":\"b3\"}},{\"start\":\"53336\",\"end\":\"53654\",\"attributes\":{\"matched_paper_id\":\"206594692\",\"id\":\"b4\"}},{\"start\":\"53656\",\"end\":\"53906\",\"attributes\":{\"matched_paper_id\":\"206592484\",\"id\":\"b5\"}},{\"start\":\"53908\",\"end\":\"54134\",\"attributes\":{\"matched_paper_id\":\"2930547\",\"id\":\"b6\"}},{\"start\":\"54136\",\"end\":\"54615\",\"attributes\":{\"matched_paper_id\":\"25209638\",\"id\":\"b7\"}},{\"start\":\"54617\",\"end\":\"55112\",\"attributes\":{\"matched_paper_id\":\"9353275\",\"id\":\"b8\"}},{\"start\":\"55114\",\"end\":\"55471\",\"attributes\":{\"matched_paper_id\":\"20522276\",\"id\":\"b9\"}},{\"start\":\"55473\",\"end\":\"55792\",\"attributes\":{\"matched_paper_id\":\"197642766\",\"id\":\"b10\"}},{\"start\":\"55794\",\"end\":\"56135\",\"attributes\":{\"matched_paper_id\":\"733980\",\"id\":\"b11\"}},{\"start\":\"56137\",\"end\":\"56613\",\"attributes\":{\"matched_paper_id\":\"49293624\",\"id\":\"b12\"}},{\"start\":\"56615\",\"end\":\"57043\",\"attributes\":{\"matched_paper_id\":\"15258913\",\"id\":\"b13\"}},{\"start\":\"57045\",\"end\":\"57392\",\"attributes\":{\"matched_paper_id\":\"49301394\",\"id\":\"b14\"}},{\"start\":\"57394\",\"end\":\"57793\",\"attributes\":{\"id\":\"b15\"}},{\"start\":\"57795\",\"end\":\"58260\",\"attributes\":{\"matched_paper_id\":\"49291228\",\"id\":\"b16\"}},{\"start\":\"58262\",\"end\":\"58724\",\"attributes\":{\"matched_paper_id\":\"163164623\",\"id\":\"b17\"}},{\"start\":\"58726\",\"end\":\"59127\",\"attributes\":{\"matched_paper_id\":\"155109576\",\"id\":\"b18\"}},{\"start\":\"59129\",\"end\":\"59531\",\"attributes\":{\"matched_paper_id\":\"9812826\",\"id\":\"b19\"}},{\"start\":\"59533\",\"end\":\"59928\",\"attributes\":{\"matched_paper_id\":\"1677864\",\"id\":\"b20\"}},{\"start\":\"59930\",\"end\":\"60278\",\"attributes\":{\"matched_paper_id\":\"189818835\",\"id\":\"b21\"}},{\"start\":\"60280\",\"end\":\"60614\",\"attributes\":{\"matched_paper_id\":\"52824162\",\"id\":\"b22\"}},{\"start\":\"60616\",\"end\":\"60941\",\"attributes\":{\"matched_paper_id\":\"6329628\",\"id\":\"b23\"}},{\"start\":\"60943\",\"end\":\"61383\",\"attributes\":{\"matched_paper_id\":\"980581\",\"id\":\"b24\"}},{\"start\":\"61385\",\"end\":\"61820\",\"attributes\":{\"matched_paper_id\":\"1615412\",\"id\":\"b25\"}},{\"start\":\"61822\",\"end\":\"62299\",\"attributes\":{\"matched_paper_id\":\"10102589\",\"id\":\"b26\"}},{\"start\":\"62301\",\"end\":\"62749\",\"attributes\":{\"matched_paper_id\":\"49276456\",\"id\":\"b27\"}},{\"start\":\"62751\",\"end\":\"63037\",\"attributes\":{\"matched_paper_id\":\"10186160\",\"id\":\"b28\"}},{\"start\":\"63039\",\"end\":\"63387\",\"attributes\":{\"matched_paper_id\":\"56819955\",\"id\":\"b29\"}},{\"start\":\"63389\",\"end\":\"63713\",\"attributes\":{\"matched_paper_id\":\"19148962\",\"id\":\"b30\"}},{\"start\":\"63715\",\"end\":\"64141\",\"attributes\":{\"matched_paper_id\":\"3869844\",\"id\":\"b31\"}},{\"start\":\"64143\",\"end\":\"64535\",\"attributes\":{\"matched_paper_id\":\"15607045\",\"id\":\"b32\"}},{\"start\":\"64537\",\"end\":\"64913\",\"attributes\":{\"matched_paper_id\":\"7149851\",\"id\":\"b33\"}},{\"start\":\"64915\",\"end\":\"65279\",\"attributes\":{\"matched_paper_id\":\"60571601\",\"id\":\"b34\"}},{\"start\":\"65281\",\"end\":\"65561\",\"attributes\":{\"matched_paper_id\":\"39020350\",\"id\":\"b35\"}},{\"start\":\"65563\",\"end\":\"65970\",\"attributes\":{\"matched_paper_id\":\"9812826\",\"id\":\"b36\"}},{\"start\":\"65972\",\"end\":\"66394\",\"attributes\":{\"matched_paper_id\":\"201625025\",\"id\":\"b37\"}},{\"start\":\"66396\",\"end\":\"66853\",\"attributes\":{\"matched_paper_id\":\"12008695\",\"id\":\"b38\"}},{\"start\":\"66855\",\"end\":\"67183\",\"attributes\":{\"matched_paper_id\":\"1690456\",\"id\":\"b39\"}},{\"start\":\"67185\",\"end\":\"67565\",\"attributes\":{\"id\":\"b40\"}},{\"start\":\"67567\",\"end\":\"67927\",\"attributes\":{\"matched_paper_id\":\"21351739\",\"id\":\"b41\"}},{\"start\":\"67929\",\"end\":\"68069\",\"attributes\":{\"id\":\"b42\"}},{\"start\":\"68071\",\"end\":\"68445\",\"attributes\":{\"matched_paper_id\":\"22677559\",\"id\":\"b43\"}},{\"start\":\"68447\",\"end\":\"68918\",\"attributes\":{\"matched_paper_id\":\"13178535\",\"id\":\"b44\"}},{\"start\":\"68920\",\"end\":\"69218\",\"attributes\":{\"id\":\"b45\",\"doi\":\"arXiv:1802.09089\"}},{\"start\":\"69220\",\"end\":\"69605\",\"attributes\":{\"matched_paper_id\":\"116426137\",\"id\":\"b46\"}},{\"start\":\"69607\",\"end\":\"69958\",\"attributes\":{\"matched_paper_id\":\"60571601\",\"id\":\"b47\"}},{\"start\":\"69960\",\"end\":\"70186\",\"attributes\":{\"matched_paper_id\":\"206655274\",\"id\":\"b48\"}},{\"start\":\"70188\",\"end\":\"70468\",\"attributes\":{\"matched_paper_id\":\"62010565\",\"id\":\"b49\"}},{\"start\":\"70470\",\"end\":\"70888\",\"attributes\":{\"matched_paper_id\":\"37764051\",\"id\":\"b50\"}},{\"start\":\"70890\",\"end\":\"71262\",\"attributes\":{\"matched_paper_id\":\"207224831\",\"id\":\"b51\"}},{\"start\":\"71264\",\"end\":\"71518\",\"attributes\":{\"matched_paper_id\":\"16013918\",\"id\":\"b52\"}},{\"start\":\"71520\",\"end\":\"71772\",\"attributes\":{\"matched_paper_id\":\"3316677\",\"id\":\"b53\"}},{\"start\":\"71774\",\"end\":\"71915\",\"attributes\":{\"id\":\"b54\"}},{\"start\":\"71917\",\"end\":\"72272\",\"attributes\":{\"matched_paper_id\":\"206655679\",\"id\":\"b55\"}},{\"start\":\"72274\",\"end\":\"72535\",\"attributes\":{\"matched_paper_id\":\"4701912\",\"id\":\"b56\"}},{\"start\":\"72537\",\"end\":\"72647\",\"attributes\":{\"id\":\"b57\"}}]", "bib_title": "[{\"start\":\"52151\",\"end\":\"52216\"},{\"start\":\"52502\",\"end\":\"52570\"},{\"start\":\"52988\",\"end\":\"53051\"},{\"start\":\"53336\",\"end\":\"53380\"},{\"start\":\"53656\",\"end\":\"53686\"},{\"start\":\"53908\",\"end\":\"53957\"},{\"start\":\"54136\",\"end\":\"54283\"},{\"start\":\"54617\",\"end\":\"54704\"},{\"start\":\"55114\",\"end\":\"55181\"},{\"start\":\"55473\",\"end\":\"55544\"},{\"start\":\"55794\",\"end\":\"55917\"},{\"start\":\"56137\",\"end\":\"56241\"},{\"start\":\"56615\",\"end\":\"56735\"},{\"start\":\"57045\",\"end\":\"57120\"},{\"start\":\"57795\",\"end\":\"57897\"},{\"start\":\"58262\",\"end\":\"58354\"},{\"start\":\"58726\",\"end\":\"58784\"},{\"start\":\"59129\",\"end\":\"59217\"},{\"start\":\"59533\",\"end\":\"59578\"},{\"start\":\"59930\",\"end\":\"60014\"},{\"start\":\"60280\",\"end\":\"60386\"},{\"start\":\"60616\",\"end\":\"60709\"},{\"start\":\"60943\",\"end\":\"61033\"},{\"start\":\"61385\",\"end\":\"61458\"},{\"start\":\"61822\",\"end\":\"61929\"},{\"start\":\"62301\",\"end\":\"62362\"},{\"start\":\"62751\",\"end\":\"62823\"},{\"start\":\"63039\",\"end\":\"63100\"},{\"start\":\"63389\",\"end\":\"63456\"},{\"start\":\"63715\",\"end\":\"63828\"},{\"start\":\"64143\",\"end\":\"64210\"},{\"start\":\"64537\",\"end\":\"64631\"},{\"start\":\"64915\",\"end\":\"64975\"},{\"start\":\"65281\",\"end\":\"65320\"},{\"start\":\"65563\",\"end\":\"65651\"},{\"start\":\"65972\",\"end\":\"66022\"},{\"start\":\"66396\",\"end\":\"66486\"},{\"start\":\"66855\",\"end\":\"66942\"},{\"start\":\"67567\",\"end\":\"67635\"},{\"start\":\"68071\",\"end\":\"68136\"},{\"start\":\"68447\",\"end\":\"68548\"},{\"start\":\"69220\",\"end\":\"69288\"},{\"start\":\"69607\",\"end\":\"69667\"},{\"start\":\"69960\",\"end\":\"69987\"},{\"start\":\"70188\",\"end\":\"70232\"},{\"start\":\"70470\",\"end\":\"70586\"},{\"start\":\"70890\",\"end\":\"70974\"},{\"start\":\"71264\",\"end\":\"71331\"},{\"start\":\"71520\",\"end\":\"71598\"},{\"start\":\"71917\",\"end\":\"71973\"},{\"start\":\"72274\",\"end\":\"72362\"}]", "bib_author": "[{\"start\":\"52218\",\"end\":\"52227\"},{\"start\":\"52227\",\"end\":\"52236\"},{\"start\":\"52236\",\"end\":\"52248\"},{\"start\":\"52572\",\"end\":\"52582\"},{\"start\":\"52833\",\"end\":\"52843\"},{\"start\":\"53053\",\"end\":\"53067\"},{\"start\":\"53067\",\"end\":\"53080\"},{\"start\":\"53080\",\"end\":\"53092\"},{\"start\":\"53382\",\"end\":\"53388\"},{\"start\":\"53388\",\"end\":\"53397\"},{\"start\":\"53397\",\"end\":\"53404\"},{\"start\":\"53404\",\"end\":\"53411\"},{\"start\":\"53688\",\"end\":\"53699\"},{\"start\":\"53959\",\"end\":\"53974\"},{\"start\":\"54285\",\"end\":\"54291\"},{\"start\":\"54706\",\"end\":\"54713\"},{\"start\":\"54713\",\"end\":\"54721\"},{\"start\":\"54721\",\"end\":\"54729\"},{\"start\":\"54729\",\"end\":\"54745\"},{\"start\":\"54745\",\"end\":\"54761\"},{\"start\":\"55183\",\"end\":\"55189\"},{\"start\":\"55189\",\"end\":\"55198\"},{\"start\":\"55198\",\"end\":\"55211\"},{\"start\":\"55546\",\"end\":\"55555\"},{\"start\":\"55919\",\"end\":\"55930\"},{\"start\":\"56243\",\"end\":\"56254\"},{\"start\":\"56254\",\"end\":\"56266\"},{\"start\":\"56266\",\"end\":\"56281\"},{\"start\":\"56281\",\"end\":\"56294\"},{\"start\":\"56294\",\"end\":\"56304\"},{\"start\":\"56304\",\"end\":\"56314\"},{\"start\":\"56737\",\"end\":\"56750\"},{\"start\":\"56750\",\"end\":\"56764\"},{\"start\":\"57122\",\"end\":\"57131\"},{\"start\":\"57131\",\"end\":\"57140\"},{\"start\":\"57140\",\"end\":\"57148\"},{\"start\":\"57148\",\"end\":\"57158\"},{\"start\":\"57492\",\"end\":\"57502\"},{\"start\":\"57502\",\"end\":\"57516\"},{\"start\":\"57516\",\"end\":\"57527\"},{\"start\":\"57527\",\"end\":\"57541\"},{\"start\":\"57541\",\"end\":\"57553\"},{\"start\":\"57899\",\"end\":\"57911\"},{\"start\":\"57911\",\"end\":\"57921\"},{\"start\":\"57921\",\"end\":\"57932\"},{\"start\":\"57932\",\"end\":\"57941\"},{\"start\":\"57941\",\"end\":\"57951\"},{\"start\":\"58356\",\"end\":\"58365\"},{\"start\":\"58365\",\"end\":\"58375\"},{\"start\":\"58375\",\"end\":\"58387\"},{\"start\":\"58387\",\"end\":\"58394\"},{\"start\":\"58394\",\"end\":\"58402\"},{\"start\":\"58402\",\"end\":\"58412\"},{\"start\":\"58786\",\"end\":\"58795\"},{\"start\":\"58795\",\"end\":\"58807\"},{\"start\":\"58807\",\"end\":\"58813\"},{\"start\":\"58813\",\"end\":\"58819\"},{\"start\":\"58819\",\"end\":\"58829\"},{\"start\":\"59219\",\"end\":\"59229\"},{\"start\":\"59229\",\"end\":\"59240\"},{\"start\":\"59240\",\"end\":\"59252\"},{\"start\":\"59580\",\"end\":\"59589\"},{\"start\":\"59589\",\"end\":\"59599\"},{\"start\":\"59599\",\"end\":\"59607\"},{\"start\":\"59607\",\"end\":\"59617\"},{\"start\":\"59617\",\"end\":\"59629\"},{\"start\":\"60016\",\"end\":\"60025\"},{\"start\":\"60025\",\"end\":\"60034\"},{\"start\":\"60034\",\"end\":\"60041\"},{\"start\":\"60041\",\"end\":\"60051\"},{\"start\":\"60388\",\"end\":\"60395\"},{\"start\":\"60711\",\"end\":\"60722\"},{\"start\":\"61035\",\"end\":\"61042\"},{\"start\":\"61042\",\"end\":\"61049\"},{\"start\":\"61049\",\"end\":\"61058\"},{\"start\":\"61058\",\"end\":\"61066\"},{\"start\":\"61460\",\"end\":\"61467\"},{\"start\":\"61467\",\"end\":\"61475\"},{\"start\":\"61475\",\"end\":\"61482\"},{\"start\":\"61482\",\"end\":\"61491\"},{\"start\":\"61491\",\"end\":\"61499\"},{\"start\":\"61931\",\"end\":\"61937\"},{\"start\":\"61937\",\"end\":\"61943\"},{\"start\":\"61943\",\"end\":\"61950\"},{\"start\":\"61950\",\"end\":\"61958\"},{\"start\":\"61958\",\"end\":\"61964\"},{\"start\":\"61964\",\"end\":\"61971\"},{\"start\":\"62364\",\"end\":\"62373\"},{\"start\":\"62373\",\"end\":\"62384\"},{\"start\":\"62384\",\"end\":\"62391\"},{\"start\":\"62391\",\"end\":\"62400\"},{\"start\":\"62400\",\"end\":\"62414\"},{\"start\":\"62414\",\"end\":\"62424\"},{\"start\":\"62825\",\"end\":\"62838\"},{\"start\":\"63102\",\"end\":\"63109\"},{\"start\":\"63109\",\"end\":\"63117\"},{\"start\":\"63117\",\"end\":\"63128\"},{\"start\":\"63128\",\"end\":\"63136\"},{\"start\":\"63458\",\"end\":\"63467\"},{\"start\":\"63467\",\"end\":\"63476\"},{\"start\":\"63476\",\"end\":\"63486\"},{\"start\":\"63830\",\"end\":\"63838\"},{\"start\":\"64212\",\"end\":\"64221\"},{\"start\":\"64221\",\"end\":\"64231\"},{\"start\":\"64231\",\"end\":\"64243\"},{\"start\":\"64633\",\"end\":\"64644\"},{\"start\":\"64977\",\"end\":\"64988\"},{\"start\":\"64988\",\"end\":\"65005\"},{\"start\":\"65005\",\"end\":\"65014\"},{\"start\":\"65322\",\"end\":\"65331\"},{\"start\":\"65331\",\"end\":\"65342\"},{\"start\":\"65342\",\"end\":\"65353\"},{\"start\":\"65653\",\"end\":\"65663\"},{\"start\":\"65663\",\"end\":\"65674\"},{\"start\":\"65674\",\"end\":\"65686\"},{\"start\":\"66024\",\"end\":\"66040\"},{\"start\":\"66040\",\"end\":\"66050\"},{\"start\":\"66050\",\"end\":\"66061\"},{\"start\":\"66061\",\"end\":\"66073\"},{\"start\":\"66488\",\"end\":\"66498\"},{\"start\":\"66498\",\"end\":\"66509\"},{\"start\":\"66509\",\"end\":\"66520\"},{\"start\":\"66520\",\"end\":\"66530\"},{\"start\":\"66530\",\"end\":\"66542\"},{\"start\":\"66944\",\"end\":\"66955\"},{\"start\":\"66955\",\"end\":\"66967\"},{\"start\":\"67296\",\"end\":\"67300\"},{\"start\":\"67637\",\"end\":\"67646\"},{\"start\":\"67646\",\"end\":\"67654\"},{\"start\":\"67654\",\"end\":\"67664\"},{\"start\":\"67664\",\"end\":\"67674\"},{\"start\":\"68138\",\"end\":\"68155\"},{\"start\":\"68155\",\"end\":\"68168\"},{\"start\":\"68168\",\"end\":\"68179\"},{\"start\":\"68179\",\"end\":\"68196\"},{\"start\":\"68196\",\"end\":\"68213\"},{\"start\":\"68550\",\"end\":\"68561\"},{\"start\":\"68561\",\"end\":\"68569\"},{\"start\":\"68569\",\"end\":\"68578\"},{\"start\":\"68578\",\"end\":\"68587\"},{\"start\":\"68587\",\"end\":\"68604\"},{\"start\":\"68997\",\"end\":\"69007\"},{\"start\":\"69007\",\"end\":\"69020\"},{\"start\":\"69020\",\"end\":\"69031\"},{\"start\":\"69031\",\"end\":\"69042\"},{\"start\":\"69290\",\"end\":\"69297\"},{\"start\":\"69297\",\"end\":\"69303\"},{\"start\":\"69303\",\"end\":\"69311\"},{\"start\":\"69311\",\"end\":\"69320\"},{\"start\":\"69320\",\"end\":\"69331\"},{\"start\":\"69331\",\"end\":\"69339\"},{\"start\":\"69669\",\"end\":\"69680\"},{\"start\":\"69680\",\"end\":\"69697\"},{\"start\":\"69697\",\"end\":\"69706\"},{\"start\":\"69989\",\"end\":\"70002\"},{\"start\":\"70234\",\"end\":\"70247\"},{\"start\":\"70247\",\"end\":\"70260\"},{\"start\":\"70260\",\"end\":\"70271\"},{\"start\":\"70588\",\"end\":\"70595\"},{\"start\":\"70976\",\"end\":\"70984\"},{\"start\":\"70984\",\"end\":\"70991\"},{\"start\":\"70991\",\"end\":\"70997\"},{\"start\":\"70997\",\"end\":\"71003\"},{\"start\":\"71333\",\"end\":\"71341\"},{\"start\":\"71341\",\"end\":\"71351\"},{\"start\":\"71351\",\"end\":\"71357\"},{\"start\":\"71600\",\"end\":\"71609\"},{\"start\":\"71975\",\"end\":\"71988\"},{\"start\":\"71988\",\"end\":\"71999\"},{\"start\":\"71999\",\"end\":\"72013\"},{\"start\":\"72013\",\"end\":\"72024\"},{\"start\":\"72364\",\"end\":\"72371\"},{\"start\":\"72371\",\"end\":\"72380\"},{\"start\":\"72380\",\"end\":\"72390\"}]", "bib_venue": "[{\"start\":\"52248\",\"end\":\"52289\"},{\"start\":\"52582\",\"end\":\"52610\"},{\"start\":\"52778\",\"end\":\"52831\"},{\"start\":\"53092\",\"end\":\"53128\"},{\"start\":\"53411\",\"end\":\"53457\"},{\"start\":\"53699\",\"end\":\"53745\"},{\"start\":\"53974\",\"end\":\"53993\"},{\"start\":\"54291\",\"end\":\"54338\"},{\"start\":\"54761\",\"end\":\"54820\"},{\"start\":\"55211\",\"end\":\"55255\"},{\"start\":\"55555\",\"end\":\"55594\"},{\"start\":\"55930\",\"end\":\"55942\"},{\"start\":\"56314\",\"end\":\"56355\"},{\"start\":\"56764\",\"end\":\"56800\"},{\"start\":\"57158\",\"end\":\"57187\"},{\"start\":\"57394\",\"end\":\"57490\"},{\"start\":\"57951\",\"end\":\"57991\"},{\"start\":\"58412\",\"end\":\"58457\"},{\"start\":\"58829\",\"end\":\"58884\"},{\"start\":\"59252\",\"end\":\"59295\"},{\"start\":\"59629\",\"end\":\"59686\"},{\"start\":\"60051\",\"end\":\"60071\"},{\"start\":\"60395\",\"end\":\"60415\"},{\"start\":\"60722\",\"end\":\"60754\"},{\"start\":\"61066\",\"end\":\"61121\"},{\"start\":\"61499\",\"end\":\"61558\"},{\"start\":\"61971\",\"end\":\"62021\"},{\"start\":\"62424\",\"end\":\"62482\"},{\"start\":\"62838\",\"end\":\"62863\"},{\"start\":\"63136\",\"end\":\"63184\"},{\"start\":\"63486\",\"end\":\"63521\"},{\"start\":\"63838\",\"end\":\"63888\"},{\"start\":\"64243\",\"end\":\"64296\"},{\"start\":\"64644\",\"end\":\"64677\"},{\"start\":\"65014\",\"end\":\"65056\"},{\"start\":\"65353\",\"end\":\"65389\"},{\"start\":\"65686\",\"end\":\"65729\"},{\"start\":\"66073\",\"end\":\"66138\"},{\"start\":\"66542\",\"end\":\"66588\"},{\"start\":\"66967\",\"end\":\"66992\"},{\"start\":\"67185\",\"end\":\"67294\"},{\"start\":\"67674\",\"end\":\"67714\"},{\"start\":\"67929\",\"end\":\"67960\"},{\"start\":\"68213\",\"end\":\"68233\"},{\"start\":\"68604\",\"end\":\"68646\"},{\"start\":\"68920\",\"end\":\"68995\"},{\"start\":\"69339\",\"end\":\"69381\"},{\"start\":\"69706\",\"end\":\"69742\"},{\"start\":\"70002\",\"end\":\"70044\"},{\"start\":\"70271\",\"end\":\"70299\"},{\"start\":\"70595\",\"end\":\"70642\"},{\"start\":\"71003\",\"end\":\"71042\"},{\"start\":\"71357\",\"end\":\"71366\"},{\"start\":\"71609\",\"end\":\"71622\"},{\"start\":\"71774\",\"end\":\"71795\"},{\"start\":\"72024\",\"end\":\"72066\"},{\"start\":\"72390\",\"end\":\"72394\"},{\"start\":\"72539\",\"end\":\"72579\"},{\"start\":\"52291\",\"end\":\"52326\"},{\"start\":\"52612\",\"end\":\"52634\"},{\"start\":\"53130\",\"end\":\"53156\"},{\"start\":\"53459\",\"end\":\"53499\"},{\"start\":\"53747\",\"end\":\"53787\"},{\"start\":\"54340\",\"end\":\"54381\"},{\"start\":\"54822\",\"end\":\"54875\"},{\"start\":\"55257\",\"end\":\"55295\"},{\"start\":\"55596\",\"end\":\"55629\"},{\"start\":\"57189\",\"end\":\"57212\"},{\"start\":\"57993\",\"end\":\"58027\"},{\"start\":\"58459\",\"end\":\"58498\"},{\"start\":\"58886\",\"end\":\"58935\"},{\"start\":\"59297\",\"end\":\"59334\"},{\"start\":\"59688\",\"end\":\"59739\"},{\"start\":\"60073\",\"end\":\"60087\"},{\"start\":\"60417\",\"end\":\"60431\"},{\"start\":\"61123\",\"end\":\"61172\"},{\"start\":\"61560\",\"end\":\"61613\"},{\"start\":\"62023\",\"end\":\"62067\"},{\"start\":\"62484\",\"end\":\"62536\"},{\"start\":\"63523\",\"end\":\"63552\"},{\"start\":\"63890\",\"end\":\"63934\"},{\"start\":\"64298\",\"end\":\"64345\"},{\"start\":\"64679\",\"end\":\"64706\"},{\"start\":\"65058\",\"end\":\"65094\"},{\"start\":\"65391\",\"end\":\"65421\"},{\"start\":\"65731\",\"end\":\"65768\"},{\"start\":\"66140\",\"end\":\"66199\"},{\"start\":\"66590\",\"end\":\"66630\"},{\"start\":\"67716\",\"end\":\"67750\"},{\"start\":\"68648\",\"end\":\"68684\"},{\"start\":\"69744\",\"end\":\"69774\"},{\"start\":\"70644\",\"end\":\"70685\"},{\"start\":\"71044\",\"end\":\"71077\"},{\"start\":\"72396\",\"end\":\"72400\"}]"}}}, "year": 2023, "month": 12, "day": 17}