{"id": 253708271, "updated": "2023-10-05 07:55:07.553", "metadata": {"title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models", "authors": "[{\"first\":\"Guangxuan\",\"last\":\"Xiao\",\"middle\":[]},{\"first\":\"Ji\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Mickael\",\"last\":\"Seznec\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Julien\",\"last\":\"Demouth\",\"middle\":[]},{\"first\":\"Song\",\"last\":\"Han\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and LLaMA family. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at https://github.com/mit-han-lab/smoothquant.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2211.10438", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/XiaoLSWDH23", "doi": "10.48550/arxiv.2211.10438"}}, "content": {"source": {"pdf_hash": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2211.10438v5.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "37831fe3452ec0c9a354c674fb5b393329b50f78", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2c994fadbb84fb960d8306ee138dbeef41a5b323.txt", "contents": "\nSmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models\n\n\nGuangxuan Xiao \nJi Lin \nMickael Seznec \nHao Wu \nJulien Demouth \nSong Han \nSmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models\n\nLarge language models (LLMs) show excellent performance but are compute-and memoryintensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and generalpurpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and LLaMA family. We demonstrate up to 1.56\u00d7 speedup and 2\u00d7 memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs.\n\nIntroduction\n\nLarge-scale language models (LLMs) show excellent performance on various tasks (Brown et al., 2020a;Zhang et al., 2022). However, serving LLMs is budget and energyconsuming due to their gigantic model size. For example, the GPT-3 (Brown et al., 2020a) model contains 175B parameters, which will consume at least 350GB of memory to store and run in FP16, requiring 8\u00d748GB A6000 Figure 1: The model size of large language models is developing at a faster pace than the GPU memory in recent years, leading to a big gap between the supply and demand for memory. Quantization and model compression techniques can help bridge the gap.\n\nGPUs or 5\u00d780GB A100 GPUs just for inference. Due to the huge computation and communication overhead, the inference latency may also be unacceptable to real-world applications. Quantization is a promising way to reduce the cost of LLMs (Dettmers et al., 2022;Yao et al., 2022). By quantizing the weights and activations with low-bit integers, we can reduce GPU memory requirements, in size and bandwidth, and accelerate compute-intensive operations (i.e., GEMM in linear layers, BMM in attention). For instance, INT8 quantization of weights and activations can halve the GPU memory usage and nearly double the throughput of matrix multiplications compared to FP16.\n\nHowever, unlike CNN models or smaller transformer models like BERT (Devlin et al., 2019), the activations of LLMs are difficult to quantize. When we scale up LLMs beyond 6.7B parameters, systematic outliers with large magnitude will emerge in activations (Dettmers et al., 2022), leading to large quantization errors and accuracy degradation. Ze-roQuant (Yao et al., 2022) applies dynamic per-token activation quantization and group-wise weight quantization (defined in Figure 3 Sec. 2). It can be implemented efficiently and delivers good accuracy for GPT-3-350M and GPT-J-6B. However, it can not maintain the accuracy for the large OPT model with 175 billion parameters (see Section 5.2). LLM.int8() (Dettmers et al., 2022) \n\n\nPreliminaries\n\nQuantization maps a high-precision value into discrete levels. We study integer uniform quantization (Jacob et al., 2018) (specifically INT8) for better hardware support and efficiency. The quantization process can be expressed as:\nX INT8 = \u2308 X FP16 \u2206 \u230b, \u2206 = max(|X|) 2 N \u22121 \u2212 1 ,(1)\nwhere X is the floating-point tensor,X is the quantized counterpart, \u2206 is the quantization step size, \u2308\u00b7\u230b is the rounding function, and N is the number of bits (8 in our case).\n\nHere we assume the tensor is symmetric at 0 for simplicity; the discussion is similar for asymmetric cases (e.g., after ReLU) by adding a zero-point (Jacob et al., 2018).\n\nSuch quantizer uses the maximum absolute value to calculate \u2206 so that it preserves the outliers in activation, which are found to be important for accuracy (Dettmers et al., 2022). We can calculate \u2206 offline with the activations of some calibration samples, what we call static quantization. We can also use the runtime statistics of activations to get \u2206, what we call dynamic quantization. As shown in Figure 3, quantization has different granularity levels. The per-tensor quantization uses a single step size for the entire matrix. We can further enable finer-grained quantization by using different quantization step sizes for activations associated with each token (per-token quantization) or each output channel of weights (per-channel quantization). A coarse-grained version of per-channel quantization is to use different quantization steps for different channel groups, called group-wise quantization (Shen et al., 2020;Yao et al., 2022).\n\nFor a linear layer in Transformers (Vaswani et al., 2017)\nY = X \u00b7 W, Y \u2208 R T \u00d7Co , X \u2208 R T \u00d7Ci , W \u2208 R Ci\u00d7Co ,\nwhere T is the number of tokens, C i is the input channel, and C o is the output channel (see Figure 3, we omit the batch dimension for simplicity), we can reduce the storage by half compared to FP16 by quantizing the weights to INT8. However, to speed up the inference, we need to quantize both weights and activations into INT8 (i.e., W8A8) to utilize the integer kernels (e.g., INT8 GEMM), which are supported by a wide range of hardware (e.g., NVIDIA GPUs, Intel CPUs, Qualcomm DSPs, etc.).\nX * W T C i C o C i \u0394 W [1] \u0394 X [1]\nper-tensor quant.\n\nper-tensor quant.\n\n(a) per-tensor quantization \u0394 X\n[T\u00d71] X * W T C i C o C i \u0394 W [1\u00d7C 0 ]\nper-token quant.\n\nper-channel quant. (b) per-token + per-channel quantization Figure 3: Definition of per-tensor, per-token, and perchannel quantization. Per-tensor quantization is the most efficient to implement. For vector-wise quantization to efficiently utilize the INT8 GEMM kernels, we can only use scaling factors from the outer dimensions (i.e., token dimension T and out channel dimension C o ) but not inner dimension (i.e., in channel dimension C i ).\n\n\nReview of Quantization Difficulty\n\nLLMs are notoriously difficult to quantize due to the outliers in the activations (Dettmers et al., 2022;Wei et al., 2022;Bondarenko et al., 2021). We first review the difficulties of activation quantization and look for a pattern amongst outliers. We visualize the input activations and the weights of a linear layer that has a large quantization error in Figure 4 (left). We can find several patterns that motivate our method:\n\n1. Activations are harder to quantize than weights. The weight distribution is quite uniform and flat, which is easy to quantize. Previous work has shown that quantizing the weights of LLMs with INT8 or even with INT4 does not degrade accuracy (Dettmers et al., 2022;Yao et al., 2022;Zeng et al., 2022), which echoes our observation.\n\n\n2.\n\nOutliers make activation quantization difficult. The scale of outliers in activations is \u223c 100\u00d7 larger than most of the activation values. In the case of per-tensor quantization (Equation 1), the large outliers dominate the maximum magnitude measurement, leading to low effective quantization bits/levels ( Figure 2) for non-outlier channels: suppose the maximum magnitude of channel i is m i , and the maximum value of the whole matrix is m, the effective quantization levels of channel i is 2 8 \u00b7 m i /m. For non-outlier channels, the effective quantization levels would be very small (2-3), leading to large quantization errors.\n\n3. Outliers persist in fixed channels. Outliers appear in a small fraction of the channels. If one channel has an outlier, it persistently appears in all tokens (Figure 4, red). The variance amongst the channels for a given token is large (the activations in some channels are very large, but most are small), but the variance between the magnitudes of a given channel across tokens is small (outlier channels are consistently large).\n\nDue to the persistence of outliers and the small variance inside each channel, if we could perform per-channel quantization (Bondarenko et al., 2021) of the activation (i.e., using a different quantization step for each channel), the quantization error would be much smaller compared to per-tensor quantization, while per-token quantization helps little. In Table 1, we verify the assumption that simulated per-channel activation quantization successfully bridges the accuracy with the FP16 baseline, which echos the findings of Bondarenko et al..\n\nHowever, per-channel activation quantization does not map well to hardware-accelerated GEMM kernels, that rely on a sequence of operations executed at a high throughput (e.g., Tensor Core MMAs) and do not tolerate the insertion of instructions with a lower throughput (e.g., conversions or CUDA Core FMAs) in that sequence. In those kernels, scaling can only be performed along the outer dimensions of the matrix multiplication (i.e., token dimension of activations T , output channel dimension of weights C o , see Figure 3), which can be applied after the matrix multiplication finishes:\nY = diag(\u2206 FP16 X ) \u00b7 (X INT8 \u00b7W INT8 ) \u00b7 diag(\u2206 FP16 W )\n(2) Therefore, previous works all use per-token activation quantization for linear layers (Dettmers et al., 2022;Yao et al., 2022), although they cannot address the difficulty of activation quantization (only slightly better than per-tensor).\n\n\nSmoothQuant\n\nInstead of per-channel activation quantization (which is infeasible), we propose to \"smooth\" the input activation by dividing it by a per-channel smoothing factor s \u2208 R Ci . To keep the mathematical equivalence of a linear layer, we scale the weights accordingly in the reversed direction:\nY = (Xdiag(s) \u22121 ) \u00b7 (diag(s)W) =X\u0174(3)\nConsidering input X is usually produced from previous linear operations (e.g., linear layers, layer norms, etc.), we  (1) there are a few channels in the original activation map whose magnitudes are very large (greater than 70);\n\n(2) the variance in one activation channel is small; (3) the original weight distribution is flat and uniform. SmoothQuant migrates the outlier channels from activation to weight. In the end, the outliers in the activation are greatly smoothed while the weight is still pretty smooth and flat.\n\ncan easily fuse the smoothing factor into previous layers' parameters offline, which doe not incur kernel call overhead from an extra scaling. For some other cases, when the input is from a residual add, we can add an extra scaling to the residual branch similar to Wei et al. (2022).\n\nMigrate the quantization difficulty from activations to weights. We aim to choose a per-channel smoothing factor s such thatX = Xdiag(s) \u22121 is easy to quantize. To reduce the quantization error, we should increase the effective quantization bits for all the channels. The total effective quantization bits would be largest when all the channels have the same maximum magnitude. Therefore, a straight-forward choice is s j = max(|X j |), j = 1, 2, ..., C i , where j corresponds to j-th input channel. This choice ensures that after the division, all the activation channels will have the same maximum value, which is easy to quantize. Note that the range of activations is dynamic; it varies for different input samples. Here, we estimate the scale of activations channels using calibration samples from the pre-training dataset (Jacob et al., 2018). However, this formula pushes all the quantization difficulties to the weights. We find that, in this case, the quantization errors would be large for the weights (outlier channels are migrated to weights now), leading to a large accuracy degradation (see Figure 10). On the other hand, we can also push all the quantization difficulty from weights to activations by choosing s j = 1/ max(|W j |). Similarly, the model performance is bad due to the activation quantization errors. Therefore, we need to split the quantization difficulty between weights and activations so that they are both easy to quantize.\n\nHere we introduce a hyper-parameter, migration strength \u03b1, to control how much difficulty we want to migrate from activation to weights, using the following equation: We find that for most of the models, e.g., all OPT  and BLOOM (Scao et al., 2022) models, \u03b1 = 0.5 is a well-balanced point to evenly split the quantization difficulty, especially when we are using the same quantizer for weights and activations (e.g., per-tensor, static quantization). The formula ensures that the weights and activations at the corresponding channel share a similar maximum value, thus sharing the same quantization difficulty. Figure 5 illustrates the smoothing transformation when we take \u03b1 = 0.5. For some other models where activation outliers are more significant (e.g., GLM-130B (Zeng et al., 2022) has \u223c30% outliers, which are more difficult for activation quantization), we can choose a larger \u03b1 to migrate more quantization difficulty to weights (like 0.75).\ns j = max(|X j |) \u03b1 / max(|W j |) 1\u2212\u03b1(4)\nApplying SmoothQuant to Transformer blocks. Linear layers take up most of the parameters and computation of LLM models. By default, we perform scale smoothing for the input activations of self-attention and feed-forward layers and quantize all linear layers with W8A8. We also quantize BMM operators in the attention computation. We design a quantization flow for transformer blocks in Figure 6. We quantize the inputs and weights of compute-heavy operators like linear layers and BMM in attention layers with INT8,   (Yao et al., 2022), LLM.int8() (Dettmers et al., 2022), and Outlier Suppression (Wei et al., 2022). Since SmoothQuant is orthogonal to the quantization schemes, we provide gradually aggressive and efficient quantization levels from O1 to O3. The detailed quantization schemes of the baselines and SmoothQuant are shown in Table 2.\n\nModels and datasets. We choose three families of LLMs to evaluate SmoothQuant: OPT , BLOOM (Scao et al., 2022), and GLM-130B (Zeng et al., 2022). We use seven zero-shot evaluation tasks: LAMBADA (Paperno et al., 2016), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), WinoGrande (Sakaguchi et al., 2019), OpenBookQA (Mihaylov et al., 2018), RTE (Wang et al., 2018), COPA (Roemmele et al., 2011), and one language modeling dataset WikiText (Merity et al., 2016) to evaluate the OPT and BLOOM models. We use MMLU (Hendrycks et al., 2020), MNLI (Williams et al., 2018), QNLI (Wang et al., 2018) and LAMBADA to evaluate the GLM-130B model because some of the aforementioned benchmarks appear in the training set of GLM-130B. We use lm-eval-harness * to evaluate OPT and BLOOM models, and GLM-130B's official repo \u2020 for its own evaluation. Finally, we scale up our method to MT-NLG 530B (Smith et al., 2022) and for the first time enabling the serving of a >500B model within a single node. Note that we focus on the relative performance change before and after quantization but not the absolute value.\n\nActivation smoothing. The migration strength \u03b1 = 0.5 is a general sweet spot for all the OPT and BLOOM models, and \u03b1 = 0.75 for GLM-130B since its activations are more difficult to quantize (Zeng et al., 2022). We get a suitable \u03b1 by running a quick grid search on a subset of the Pile  validation set. To get the statistics of activations, we calibrate the smoothing factors and the static quantization step sizes once with 512 random sentences from the pre-training dataset Pile, and apply the same smoothed and quantized model for all downstream tasks. In this way, we can benchmark the generality and zero-shot performance of the quantized LLMs.\n\nImplementation. We implement SmoothQuant with two backends: (1) PyTorch Huggingface \u2021 for the proof of concept, and (2) FasterTransformer \u00a7 , as an example of a highperformance framework used in production environments. In both PyTorch Huggingface and FasterTransformer frameworks, we implement INT8 linear modules and the batched matrix multiplication (BMM) function with CUTLASS INT8 GEMM kernels. We simply replace the original floating point (FP16) linear modules and the bmm function with our INT8 kernels as the INT8 model.\n\n\nAccurate Quantization\n\nResults of OPT-175B. SmoothQuant can handle the quantization of very large LLMs, whose activations are more difficult to quantize. We study quantization on OPT-175B. As shown in Table 3 (Table 2). We extensively benchmark the performance on 7 zero-shot benchmarks (by reporting the average accuracy) and 1 language modeling benchmark (perplexity). *For ZeroQuant, we also tried leaving the input activation of self-attention in FP16 and quantizing the rest to INT8, which is their solution to the GPT-NeoX-20B. But this does not solve the accuracy degradation of OPT-175B.\n\n\nOPT-175B\n\nLAMBADA    Results on LLaMA models. LLaMA models are new open languange models with superior performance (Touvron et al., 2023). Through initial experiments, we find LLaMA models generally have less severe activation outlier issues compared to models like OPT and BLOOM. Nonetheless, SmoothQuant still works quite well for LLaMA models. We provide some initial results of LLaMA W8A8 quantization in Table 6. SmoothQuant enables W8A8 quantization at a negligible performance degradation.\n\n\nSpeedup and Memory Saving\n\nIn this section, we show the measured speedup and memory saving of SmoothQuant-O3 integrated into PyTorch and FasterTransformer.\n\nContext-stage: PyTorch Implementation. We measure the end-to-end latency of generating all hidden states for a batch of 4 sentences in one pass, i.e., the context stage latency. We record the (aggregated) peak GPU memory usage in this process. We only compare SmoothQuant with LLM.int8() because it is the only existing quantization method that can preserve LLM accuracy at all scales. Due to the lack of support for model parallelism in Huggingface, we only measure SmoothQuant's performance on a single GPU for the PyTorch implementation, so we choose In Figure 8, we show the inference latency and peak memory usage based on the PyTorch implementation. SmoothQuant is consistently faster than the FP16 baseline, getting a 1.51x speedup on OPT-30B when the sequence length is 256. We also see a trend that the larger the model, the more significant the acceleration. On the other hand, LLM.int8() is almost always slower than the FP16 baseline, which is due to the large overhead of the mixed-precision activation representation. In terms of memory, SmoothQuant and LLM.int8() can all nearly halve the memory usage of the FP16 model, while SmoothQuant saves slightly more memory because it uses fully INT8 GEMMs.\n\nContext-stage: FasterTransformer Implementation. As shown in Figure 9 (top), compared to FasterTransformer's FP16 implementation of OPT, SmoothQuant-O3 can further reduce the execution latency of OPT-13B and OPT-30B by up to 1.56\u00d7 when using a single GPU. This is challenging since FasterTransformer is already more than 3\u00d7 faster compared to the PyTorch implementation for OPT-30B. Remarkably, for bigger models that have to be distributed across multiple GPUs, SmoothQuant achieves similar or even better latency using only half the number of GPUs (1 GPU instead of 2 for OPT-66B, 4 GPUs instead of 8 for OPT-175B). This could greatly lower the cost of serving LLMs. The amount of memory needed when using SmoothQuant-O3 in FasterTransformer is reduced by a factor of almost 2\u00d7, as shown on Figure 9 (bottom).\n\nDecoding-stage. In Table 7 Memory (GB)\n\n\nOPT-13B\n\nOPT-30B OPT-66B OPT-175B Figure 9: Inference latency (top) and memory usage (bottom) of the FasterTransformer implementation on NVIDIA A100-80GB GPUs. For smaller models, the latency can be significantly reduced with SmoothQuant-O3 by up to 1.56x compared to FP16. For the bigger models (OPT-66B and 175B), we can achieve similar or even faster inference using only half number of GPUs. Memory footprint is almost halved compared to FP16.   (Smith et al., 2022). As shown in Table 8 and 9, SmoothQuant enables W8A8 quantization of the 530B model at a negligible accuracy loss. The reduced model size allows us to serve the model using half number of the GPUs (16 to 8) at a similar latency, enabling the serving of a >500B model within a single node (8\u00d7A100 80GB GPUs). \n\n\nAblation Study\n\nQuantization schemes. Table 10 shows the inference latency of different quantization schemes based on our Py-Torch implementation. We can see that the coarser the quantization granularity (from O1 to O3), the lower the latency. And static quantization can significantly accelerate inference compared with dynamic quantization because we no longer need to calculate the quantization step sizes at runtime. SmoothQuant is faster than FP16 baseline under all settings, while LLM.int8() is usually slower. We recommend using a coarser scheme if the accuracy permits.\n\nMigration strength. We need to find a suitable migration strength \u03b1 (see Equation 4) to balance the quantization difficulty of weights and activations. We ablate the effect of different \u03b1's on OPT-175B with LAMBADA in Figure 10. When \u03b1 is too small (<0.4), the activations are hard to quantize; when \u03b1 is too large (>0.6), the weights will be hard to quantize. Only when we choose \u03b1 from the sweet spot region (0.4-0.6) can we get small quantization errors for both weights and activations, and maintain the model performance after quantization.  Figure 10: A suitable migration strength \u03b1 (sweet spot) makes both activations and weights easy to quantize. If the \u03b1 is too large, weights will be hard to quantize; if too small, activations will be hard to quantize.\n\n\nRelated Work\n\nLarge language models (LLMs). Pre-trained language models have achieved remarkable performance on various benchmarks by scaling up. GPT-3 (Brown et al., 2020b) is the first LLM beyond 100B parameters and achieves impressive few-shot/zero-shot learning results. Later works (Rae et al., 2021;Smith et al., 2022;Du et al., 2022;Chowdhery et al., 2022) continue to push the frontier of scaling, going beyond 500B parameters. However, as the language model gets larger, serving such models for inference becomes expensive and challenging. In this work, we show that our proposed method can quantize the three largest, openly available LLMs: OPT-175B , BLOOM-176B (Scao et al., 2022) and GLM-130B (Zeng et al., 2022), and even MT-NLG 530B (Smith et al., 2022) to reduce the memory cost and accelerate inference.\n\nModel quantization. Quantization is an effective method for reducing the model size and accelerating inference. It proves to be effective for various convolutional neural works (CNNs) (Han et al., 2016;Jacob et al., 2018;Nagel et al., 2019;Wang et al., 2019;Lin et al., 2020) and transformers (Shen et al., 2020;Kim et al., 2021;Liu et al., 2021;Wang et al., 2020;Bondarenko et al., 2021). Weight equal-ization (Nagel et al., 2019) and channel splitting (Zhao et al., 2019) reduce quantization error by suppressing the outliers in weights. However, these techniques cannot address the activation outliers, which are the major quantization bottleneck for LLMs (Dettmers et al., 2022).\n\nQuantization of LLMs. GPTQ (Frantar et al., 2022) applies quantization only to weights but not activations (please find a short discussion in Appendix A). Zero-Quant (Yao et al., 2022) and nuQmm (Park et al., 2022) use a per-token and group-wise quantization scheme for LLMs, which requires customized CUDA kernels. Their largest evaluated models are 20B and 2.7B, respectively and fail to maintain the performance of LLMs like OPT-175B. LLM.int8() (Dettmers et al., 2022) uses mixed INT8/FP16 decomposition to address the activation outliers. However, such implementation leads to large latency overhead, which can be even slower than FP16 inference. Outlier Suppression (Wei et al., 2022) uses the non-scaling Layer-Norm and token-wise clipping to deal with the activation outliers. However, it only succeeds on small language models such as BERT (Devlin et al., 2019) and BART (Lewis et al., 2019) and fails to maintain the accuracy for LLMs (Table 4). Our algorithm preserves the performance of LLMs (up to 176B, the largest open-source LLM we can find) with an efficient per-tensor, static quantization scheme without retraining, allowing us to use off-the-shelf INT8 GEMM to achieve high hardware efficiency.\n\n\nConclusion\n\nWe propose SmoothQuant, an accurate and efficient posttraining quantization method to enable lossless 8-bit weight and activation quantization for LLMs up to 530B parameters.\n\nSmoothQuant enables the quantization for both weight and activations for all GEMMs in the LLMs, which significantly reduces the inference latency and memory usage compared with the mixed-precision activation quantization baseline. We integrate SmoothQuant into PyTorch and FasterTransformer, getting up to 1.56\u00d7 inference acceleration and halving the memory footprint. SmoothQuant democratizes the application of LLMs by offering a turnkey solution to reduce the serving cost.\n\nFigure 4 :\n4Magnitude of the input activations and weights of a linear layer in OPT-13B before and after SmoothQuant. Observations:\n\nFigure 5 :\n5Main idea of SmoothQuant when \u03b1 is 0.5. The smoothing factor s is obtained on calibration samples and the entire transformation is performed offline. At runtime, the activations are smooth without scaling.\n\nFigure 6 :\n6SmoothQuant's precision mapping for a Transformer block. All compute-intensive operators like linear layers and batched matmul (BMMs) use INT8 arithmetic.\n\nFigure 8 :\n8The PyTorch implementation of SmoothQuant-O3 achieves up to 1.51\u00d7 speedup and 1.96\u00d7 memory saving for OPT models on a single NVIDIA A100-80GB GPU, while LLM.int8() slows down the inference in most cases. OPT-6.7B, OPT-13B, and OPT-30B for evaluation. In the FasterTransformer library, SmoothQuant can seamlessly work with Tensor Parallelism (Shoeybi et al., 2019) algorithm, so we test SmoothQuant on OPT-13B, OPT-30B, OPT-66B, and OPT-175B for both single and multi-GPU benchmarks. All our experiments are conducted on NVIDIA A100 80GB GPU servers.\n\nTable 1 :\n1Among different activation quantization schemes, only per-channel quantization(Bondarenko et al., 2021) preserves the accuracy, but it is not compatible (marked in gray) with INT8 GEMM kernels. We report the average accuracy on WinoGrande, HellaSwag, PIQA, and LAMBADA.Model size (OPT-) 6.7B 13B \n30B \n66B 175B \n\nFP16 \n64.9% 65.6% 67.9% 69.5% 71.6% \n\nINT8 per-tensor \n39.9% 33.0% 32.8% 33.1% 32.3% \nINT8 per-token \n42.5% 33.0% 33.1% 32.9% 31.7% \nINT8 per-channel 64.8% 65.6% 68.0% 69.4% 71.4% \n\n\n\nTable 2 :\n2Outlier Suppression per-tensor per-tensor static SmoothQuant-O1 per-tensor per-token dynamic SmoothQuant-O2 per-tensor per-tensor dynamic SmoothQuant-O3 per-tensor per-tensor staticwhile keeping the activation as FP16 for other lightweight element-wise operations like ReLU, Softmax, and Layer-Norm. Such a design helps us to balance accuracy and inference efficiency. We compare with four baselines in the INT8 post-training quantization setting, i.e., without re-training of the model parameters: W8A8 naive quantization, Zero-QuantQuantization setting of the baselines and \nSmoothQuant. All weight and activations use INT8 repre-\nsentations unless specified. For SmoothQuant, the efficiency \nimproves from O1 to O3 (i.e., lower latency). \n\nMethod \nWeight \nActivation \n\nW8A8 \nper-tensor per-tensor dynamic \nZeroQuant \ngroup-wise per-token dynamic \nLLM.int8() \nper-channel per-token dynamic+FP16 \n5 Experiments \n\n5.1 Setups \nBaselines. \n\n\n, SmoothQuant can match the FP16 accuracy on all evaluation datasets with all quantization schemes. LLM.int8() can match the floating point accuracy because they use floating-point values to represent outliers, which leads to a large latency overhead(Table 10). * https://github.com/EleutherAI/lm-evaluation-harness \u2020 https://github.com/THUDM/GLM-130B \u2021 https://github.com/huggingface/transformers \u00a7 https://github.com/NVIDIA/FasterTransformer\n\nTable 3 :\n3SmoothQuant maintains the accuracy of OPT-175B model after INT8 quantization, even with the most aggressive and most efficient O3 setting\n\n\nHellaSwag PIQA WinoGrande OpenBookQA RTE COPA Average\u2191 WikiText\u2193FP16 \n74.7% \n59.3% 79.7% \n72.6% \n34.0% \n59.9% 88.0% 66.9% \n10.99 \n\nW8A8 \n0.0% \n25.6% 53.4% \n50.3% \n14.0% \n49.5% 56.0% 35.5% \n93080 \nZeroQuant \n0.0%* \n26.0% 51.7% \n49.3% \n17.8% \n50.9% 55.0% 35.8% \n84648 \nLLM.int8() \n74.7% \n59.2% 79.7% \n72.1% \n34.2% \n60.3% 87.0% 66.7% \n11.10 \nOutlier Suppression \n0.00% \n25.8% 52.5% \n48.6% \n16.6% \n53.4% 55.0% 36.0% \n96151 \n\nSmoothQuant-O1 \n74.7% \n59.2% 79.7% \n71.2% \n33.4% \n58.1% 89.0% 66.5% \n11.11 \nSmoothQuant-O2 \n75.0% \n59.0% 79.2% \n71.2% \n33.0% \n59.6% 88.0% 66.4% \n11.14 \nSmoothQuant-O3 \n74.6% \n58.9% 79.7% \n71.2% \n33.4% \n59.9% 90.0% 66.8% \n11.17 \n\n\n\nTable 4 :\n4MNLI, and QNLI. *Accuracy is not column-wise comparable due to different datasets.SmoothQuant works for different LLMs. We \ncan quantize the 3 largest, openly available LLM mod-\nels into INT8 without degrading the accuracy. For OPT-\n175B and BLOOM-176B, we show the average accuracy \non WinoGrande, HellaSwag, PIQA, and LAMBADA. For \nGLM-130B we show the average accuracy on LAMBADA, \nMMLU, Method \nOPT-175B BLOOM-176B GLM-130B* \n\nFP16 \n71.6% \n68.2% \n73.8% \n\nW8A8 \n32.3% \n64.2% \n26.9% \nZeroQuant \n31.7% \n67.4% \n26.7% \nLLM.int8() \n71.4% \n68.0% \n73.8% \nOutlier Suppression 31.7% \n54.1% \n63.5% \n\nSmoothQuant-O1 \n71.2% \n68.3% \n73.7% \nSmoothQuant-O2 \n71.1% \n68.4% \n72.5% \nSmoothQuant-O3 \n71.1% \n67.4% \n72.8% \n\nThe W8A8, ZeroQuant, and Outlier Suppression baselines \nproduce nearly random results, indicating that naively quan-\ntizing the activation of LLMs will destroy the performance. \n\nResults of different LLMs. SmoothQuant can be applied \nto various LLM designs. In Table 4, we show SmoothQuant \ncan quantize all existing open LLMs beyond 100B param-\neters. Compared with the OPT-175B model, the BLOOM-\n176B model is easier to quantize: none of the baselines \ncompletely destroys the model; even the naive W8A8 per-\ntensor dynamic quantization only degrades the accuracy by \n4%. The O1 and O2 levels of SmoothQuant successfully \nmaintain the floating point accuracy, while the O3 level (per-\ntensor static) degrades the average accuracy by 0.8%, which \nwe attribute to the discrepancy between the statically col-\nlected statistics and the real evaluation samples' activation \nstatistics. On the contrary, the GLM-130B model is more \ndifficult to quantize (which echos Zeng et al.). Nonethe-\n\nFigure 7: SmoothQuant-O3 (the most efficient setting, de-\nfined in Table 2) preserves the accuracy of OPT models \nacross different scales when quantized to INT8. LLM.int8() \nrequires mixed precision and suffers from slowing down. \n\nless, SmoothQuant-O1 can match the FP16 accuracy, while \nSmoothQuant-O3 only degrades the accuracy by 1%, which \nsignificantly outperforms the baselines. Note that we clip \nthe top 2% tokens when calibrating the static quantization \nstep sizes for GLM-130B following Wei et al. (2022). Note \nthat different model/training designs have different quantiza-\ntion difficulties, which we hope will inspire future research. \n\nResults on LLMs of different sizes. SmoothQuant works \nnot only for the very large LLMs beyond 100B parameters, \nbut it also works consistently for smaller LLMs. In Fig-\nure 7, we show that SmoothQuant can work on all scales \nof OPT models, matching the FP16 accuracy with INT8 \nquantization. \n\nResults on Instruction-Tuned LLM Shown in Table 5, \nSmoothQuant also works on instruction-tuned LLMs. We \ntest SmoothQuant on the OPT-IML-30B model using the \nWikiText-2 and LAMBADA datasets. Our results show \nthat SmoothQuant successfully preserves model accuracy \n\n\nTable 5 :\n5SmoothQuant's performance on the OPT-IML model.OPT-IML-30B \nLAMBADA \u2191 WikiText \u2193 \n\nFP16 \n69.12% \n14.26 \n\nW8A8 \n4.21% \n576.53 \nZeroQuant \n5.12% \n455.12 \nLLM.int8() \n69.14% \n14.27 \nOutlier Suppression \n0.00% \n9485.62 \n\nSmoothQuant-O3 \n69.77% \n14.37 \n\n\n\nTable 6 :\n6SmoothQuant can enable lossless W8A8 quanti-\nzation for LLaMA models (Touvron et al., 2023). Results \nare perplexity on WikiText-2 dataset. We used per-token \nactivation quantization and \u03b1=0.8 for SmoothQuant. \n\nWiki PPL\u2193 \n7B \n13B \n30B \n65B \n\nFP16 \n11.51 \n10.05 \n7.53 \n6.17 \nW8A8 SmoothQuant \n11.56 \n10.08 \n7.56 \n6.20 \n\nwith W8A8 quantization, whereas the baselines fail to do \nso. SmoothQuant is a general method designed to balance \nthe quantization difficulty for Transformer models. As the \narchitecture of instruction-tuned LLMs is not fundamen-\ntally different from vanilla LLMs, and their pre-training \nprocesses are very similar, SmoothQuant is applicable to \ninstruction-tuned LLMs as well. \n\n\n\nTable 7 :\n7SmoothQuant 's performance in the decoding stage.BS SeqLen \nLatency (ms) \nMemory (GB) \n\nFP16 Ours Speedup (\u2191) FP16 Ours Saving (\u2191) \n\nOPT-30B (1 GPU) \n1 \n512 \n422 314 \n1.35\u00d7 \n57 \n30 \n1.91\u00d7 \n1 1024 559 440 \n1.27\u00d7 \n58 \n31 \n1.87\u00d7 \n16 512 2488 1753 \n1.42\u00d7 \n69 \n44 \n1.59\u00d7 \n16 1024 OOM 3947 \n-\nOOM 61 \n-\n\nOPT-175B (8 GPUs) \n1 \n512 \n426 359 \n1.19\u00d7 \n44 \n23 \n1.87\u00d7 \n1 1024 571 475 \n1.20\u00d7 \n44 \n24 \n1.85\u00d7 \n16 512 2212 1628 \n1.36\u00d7 \n50 \n30 \n1.67\u00d7 \n16 1024 4133 3231 \n1.28\u00d7 \n56 \n37 \n1.52\u00d7 \n\nof LLMs. SmoothQuant constantly reduces the per-token \ndecoding latency compared to FP16 (up to 1.42x speedup). \nAdditionally, SmoothQuant halves the memory footprints \nfor LLM inference, enabling the deployment of LLMs at a \nsignificantly lower cost. \n\n\n\nTable 8 :\n8SmoothQuant can quantize MT-NLG 530B to W8A8 with negligible accuracy loss. We can further scale up SmoothQuant beyond 500B-level models, enabling efficient and accurate W8A8 quantization of MT-NLG 530BLAMBADA HellaSwag PIQA WinoGrande Average \n\nFP16 \n76.6% \n62.1% \n81.0% \n72.9% \n73.1% \nINT8 \n77.2% \n60.4% \n80.7% \n74.1% \n73.1% \n\n5.4 Scaling Up: 530B Model Within a Single Node \n\n\n\nTable 9 :\n9When serving MT-NLG 530B, SmoothQuant can reduce the memory by half at a similar latency using half number of GPUs, which allows serving the 530B model within a single node.SeqLen \nPrec. \n#GPUs \nLatency \nMemory \n\n128 \nFP16 \n16 \n232ms \n1040GB \nINT8 \n8 \n253ms \n527GB \n\n256 \nFP16 \n16 \n451ms \n1054GB \nINT8 \n8 \n434ms \n533GB \n\n512 \nFP16 \n16 \n838ms \n1068GB \nINT8 \n8 \n839ms \n545GB \n\n1024 \nFP16 \n16 \n1707ms \n1095GB \nINT8 \n8 \n1689ms \n570GB \n\n\n\nTable 10 :\n10GPU Latency (ms) of different quantization schemes. The coarser the quantization scheme (from pertoken to per-tensor, dynamic to static, O1 to O3, defined inTable 2), the lower the latency. SmoothQuant achieves lower latency compared to FP16 under all settings, while LLM.int8() is mostly slower. The batch size is 4.Model \nOPT-13B \nOPT-30B \n\nSequence Length \n256 \n512 \n256 \n512 \n\nFP16 \n152.6 296.3 343.0 659.9 \nLLM.int8() \n237.1 371.5 387.9 654.9 \n\nSmoothQuant-O1 124.5 243.3 246.7 490.7 \nSmoothQuant-O2 120.5 235.1 240.2 478.3 \nSmoothQuant-O3 112.1 223.1 227.6 458.4 \n\n\nAcknowledgementsWe thank MIT-IBM Watson AI Lab, MIT AI Hardware Program, Amazon and MIT Science Hub, NVIDIA Academic Partnership Award, Qualcomm Innovation Fellowship, Microsoft Turing Academic Program, and NSF for supporting this research. We thank Haotian Tang, Aohan Zeng, Eric Lin and Jilei Hou for the helpful discussions.A Discussion on Weight-Only QuantizationIn this work, we study W8A8 quantization so that we can utilize INT8 GEMM kernels to increase the throughput and accelerate inference. There is another line of work that only quantizes the weight of LLMs (e.g., GPTQ(Frantar et al., 2022)). It converts the quantized weights to FP16 on the fly for matmul during inference and can also lead to speed up due to the reduced data loading, especially for the generation stage with batch size 1.We mainly compare our method with existing work on weight-activation quantization (i.e., W8A8) like(Dettmers et al., 2022;Yao et al., 2022;Wei et al., 2022)since they are under the same setting. Here we would like to give a short discussion about the weight-only quantization methods in LLM settings:1. Firstly, we were trying to compare our method with GPTQ(Frantar et al., 2022)but found it difficult due to different implementations. GPTQ's low-bit kenerl \u00b6 only supports the generation stage with batch size 1 (i.e., only processing a single token at a time), and cannot support the context stage (widely used in different downstream tasks and chatbot) or batch-based setting. Furthermore, its low-bit kernel optimization only targets the OPT-175B model (as stated in the README). At the same time, our work utilizes FasterTransformer for serving large models, which may lead to an unfair advantage if we make a direct comparison.2. GPTQ may perform better at handling a small number of input tokens (1 in its experiments) since the process is highly memory-bounded. In contrast, SmoothQuant may serve better with a batching setting or for the context stage (i.e., when the number of processed tokens is more significant). Nonetheless, some work shows that in production, we can improve the throughput of serving GPT models by 37\u00d7 at similar latency with advanced batching . We believe in production, batching will be the future standard, and SmoothQuant will bring further improvement, even for the generation stage.3. Applications like chatbots need to handle a long context length and potentially run under a batch setting. Due to the two factors, the memory size of the KV cache can no longer be ignored (as shown in(Pope et al., 2022), the KV cache totals 3TB given batch size 512 and context length 2048, which is 3\u00d7 larger than the model weights). In this case, quantization of activation can also help reduce the memory cost from storing the KV cache. \u00b6 https://github.com/IST-DASLab/gptq 4. Finally, we think the two settings are somewhat orthogonal. We believe we can integrate GPTQ's method for a better weight quantization and potentially achieve W4A4 quantization, which will lead to even better hardware efficiency (INT4 instructions are supported on NVIDIA's Hopper GPU architecture). We leave this exploration to future work.\nReasoning about physical commonsense in natural language. Y Bisk, R Zellers, R L Bras, J Gao, Y Choi, Piqa, Thirty-Fourth AAAI Conference on Artificial Intelligence. Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.\n\nUnderstanding and overcoming the challenges of efficient transformer quantization. Y Bondarenko, M Nagel, T Blankevoort, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsBondarenko, Y., Nagel, M., and Blankevoort, T. Under- standing and overcoming the challenges of efficient trans- former quantization. In Proceedings of the 2021 Con- ference on Empirical Methods in Natural Language Pro- cessing, pp. 7947-7969, Online and Punta Cana, Domini- can Republic, November 2021. Association for Compu- tational Linguistics. URL https://aclanthology.org/2021. emnlp-main.627.\n\nLanguage models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, Amodei , D , Advances in Neural Information Processing Systems. Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.Curran Associates, Inc33Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCan- dlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Sys- tems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020a. URL https://proceedings.neurips.cc/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n\nLanguage models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 33Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877-1901, 2020b.\n\nA Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. arXiv preprintChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\nT Dettmers, M Lewis, Y Belkada, L Zettlemoyer, arXiv:2208.073398-bit matrix multiplication for transformers at scale. arXiv preprintint8(Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Llm.int8(): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n\nBERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M Chang, K Lee, K Toutanova, NAACL-HLT 2019. Association for Computational LinguisticsDevlin, J., Chang, M., Lee, K., and Toutanova, K. BERT: pre-training of deep bidirectional transformers for lan- guage understanding. In NAACL-HLT 2019, pp. 4171- 4186. Association for Computational Linguistics, 2019.\n\nEfficient scaling of language models with mixture-ofexperts. N Du, Y Huang, A M Dai, S Tong, D Lepikhin, Y Xu, M Krikun, Y Zhou, A W Yu, O Firat, International Conference on Machine Learning. PMLRDu, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., et al. Glam: Efficient scaling of language models with mixture-of- experts. In International Conference on Machine Learn- ing, pp. 5547-5569. PMLR, 2022.\n\nE Frantar, S Ashkboos, T Hoefler, Alistarh , D Gptq, arXiv:2210.17323Accurate post-training quantization for generative pretrained transformers. arXiv preprintFrantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for generative pre- trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n\nThe pile: An 800gb dataset of diverse text for language modeling. L Gao, S Biderman, S Black, L Golding, T Hoppe, C Foster, J Phang, H He, A Thite, N Nabeshima, arXiv:2101.00027arXiv preprintGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n\nDeep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. S Han, H Mao, W J Dally, ICLR. Han, S., Mao, H., and Dally, W. J. Deep Compression: Com- pressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. In ICLR, 2016.\n\nMeasuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, abs/2009.03300Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. CoRR, abs/2009.03300, 2020. URL https://arxiv.org/abs/2009.03300.\n\nQuantization and training of neural networks for efficient integerarithmetic-only inference. B Jacob, S Kligys, B Chen, M Zhu, M Tang, A Howard, H Adam, D Kalenichenko, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., and Kalenichenko, D. Quantization and training of neural networks for efficient integer- arithmetic-only inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2704-2713, 2018.\n\nInteger-only bert quantization. S Kim, A Gholami, Z Yao, M W Mahoney, K Keutzer, International conference on machine learning. PMLRKim, S., Gholami, A., Yao, Z., Mahoney, M. W., and Keutzer, K. I-bert: Integer-only bert quantization. In International conference on machine learning, pp. 5506- 5518. PMLR, 2021.\n\nDenoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, Bart, arXiv:1910.13461arXiv preprintLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo- hamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehen- sion. arXiv preprint arXiv:1910.13461, 2019.\n\nJ Lin, W.-M Chen, Y Lin, C Gan, S Han, Tiny deep learning on iot devices. Advances in Neural Information Processing Systems. 33Lin, J., Chen, W.-M., Lin, Y., Gan, C., Han, S., et al. Mcunet: Tiny deep learning on iot devices. Advances in Neural Information Processing Systems, 33:11711-11722, 2020.\n\nPost-training quantization for vision transformer. Z Liu, Y Wang, K Han, W Zhang, S Ma, W Gao, Advances in Neural Information Processing Systems. 34Liu, Z., Wang, Y., Han, K., Zhang, W., Ma, S., and Gao, W. Post-training quantization for vision transformer. Ad- vances in Neural Information Processing Systems, 34: 28092-28103, 2021.\n\nPointer sentinel mixture models. S Merity, C Xiong, J Bradbury, R Socher, Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models, 2016.\n\nCan a suit of armor conduct electricity? a new dataset for open book question answering. T Mihaylov, P Clark, T Khot, A Sabharwal, In EMNLP. Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP, 2018.\n\nData-free quantization through weight equalization and bias correction. M Nagel, M V Baalen, T Blankevoort, M Welling, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionNagel, M., Baalen, M. v., Blankevoort, T., and Welling, M. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1325- 1334, 2019.\n\nThe LAMBADA dataset: Word prediction requiring a broad discourse context. D Paperno, G Kruszewski, A Lazaridou, N Q Pham, R Bernardi, S Pezzelle, M Baroni, G Boleda, R Fern\u00e1ndez, 10.18653/v1/P16-1144Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational Linguistics1Paperno, D., Kruszewski, G., Lazaridou, A., Pham, N. Q., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fern\u00e1ndez, R. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pp. 1525- 1534, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1144. URL https://aclanthology.org/P16-1144.\n\nQuantized matmul for efficient inference of large-scale generative language models. G Park, B Park, S J Kwon, B Kim, Y Lee, D Lee, Nuqmm, arXiv:2206.09557arXiv preprintPark, G., Park, B., Kwon, S. J., Kim, B., Lee, Y., and Lee, D. nuqmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557, 2022.\n\nR Pope, S Douglas, A Chowdhery, J Devlin, J Bradbury, A Levskaya, J Heek, K Xiao, S Agrawal, J Dean, arXiv:2211.05102Efficiently scaling transformer inference. arXiv preprintPope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Levskaya, A., Heek, J., Xiao, K., Agrawal, S., and Dean, J. Efficiently scaling transformer inference. arXiv preprint arXiv:2211.05102, 2022.\n\nScaling language models: Methods, analysis & insights from training gopher. J W Rae, S Borgeaud, T Cai, K Millican, J Hoffmann, F Song, J Aslanides, S Henderson, R Ring, S Young, arXiv:2112.11446arXiv preprintRae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.\n\nChoice of plausible alternatives: An evaluation of commonsense causal reasoning. M Roemmele, C A Bejan, Gordon , A S , SS-11-06Logical Formalizations of Commonsense Reasoning, Papers from the 2011 AAAI Spring Symposium. Stanford, California, USATechnical ReportRoemmele, M., Bejan, C. A., and Gordon, A. S. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In Logical Formalizations of Common- sense Reasoning, Papers from the 2011 AAAI Spring Sym- posium, Technical Report SS-11-06, Stanford, California, USA, March 21-23, 2011. AAAI, 2011. URL http://www. aaai.org/ocs/index.php/SSS/SSS11/paper/view/2418.\n\nK Sakaguchi, R L Bras, C Bhagavatula, Y Choi, Winogrande, arXiv:1907.10641An adversarial winograd schema challenge at scale. arXiv preprintSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641, 2019.\n\nT L Scao, A Fan, C Akiki, E Pavlick, S Ili\u0107, D Hesslow, R Castagn\u00e9, A S Luccioni, F Yvon, M Gall\u00e9, arXiv:2211.05100A 176b-parameter open-access multilingual language model. arXiv preprintScao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili\u0107, S., Hesslow, D., Castagn\u00e9, R., Luccioni, A. S., Yvon, F., Gall\u00e9, M., et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n\nHessian based ultra low precision quantization of bert. S Shen, Z Dong, J Ye, L Ma, Z Yao, A Gholami, M W Mahoney, K Keutzer, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Shen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence, vol- ume 34, pp. 8815-8821, 2020.\n\nMegatron-lm: Training multibillion parameter language models using model parallelism. M Shoeybi, M Patwary, R Puri, P Legresley, J Casper, B Catanzaro, abs/1909.08053CoRRShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-lm: Training multi- billion parameter language models using model par- allelism. CoRR, abs/1909.08053, 2019. URL http: //arxiv.org/abs/1909.08053.\n\nUsing deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. S Smith, M Patwary, B Norick, P Legresley, S Rajbhandari, J Casper, Z Liu, S Prabhumoye, G Zerveas, V Korthikanti, arXiv:2201.11990arXiv preprintSmith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhan- dari, S., Casper, J., Liu, Z., Prabhumoye, S., Zerveas, G., Korthikanti, V., et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022.\n\nH Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozi\u00e8re, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Open and efficient foundation language models. arXiv preprintTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation lan- guage models. arXiv preprint arXiv:2302.13971, 2023.\n\nAttention is all you need. Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I Polosukhin, 30Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. At- tention is all you need. Advances in neural information processing systems, 30, 2017.\n\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S R Bowman, abs/1804.07461CoRRWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: A multi-task benchmark and anal- ysis platform for natural language understanding. CoRR, abs/1804.07461, 2018. URL http://arxiv.org/abs/1804. 07461.\n\nEfficient sparse attention architecture with cascade token and head pruning. H Wang, Z Zhang, S Han, Spatten, abs/2012.09852CoRRWang, H., Zhang, Z., and Han, S. Spatten: Efficient sparse attention architecture with cascade token and head pruning. CoRR, abs/2012.09852, 2020. URL https://arxiv.org/abs/2012.09852.\n\nHAQ: Hardware-Aware Automated Quantization with Mixed Precision. K Wang, Z Liu, Y Lin, J Lin, S Han, CVPR. Wang, K., Liu, Z., Lin, Y., Lin, J., and Han, S. HAQ: Hardware-Aware Automated Quantization with Mixed Precision. In CVPR, 2019.\n\nOutlier suppression: Pushing the limit of low-bit transformer language models. X Wei, Y Zhang, X Zhang, R Gong, S Zhang, Q Zhang, F Yu, X Liu, Wei, X., Zhang, Y., Zhang, X., Gong, R., Zhang, S., Zhang, Q., Yu, F., and Liu, X. Outlier suppression: Pushing the limit of low-bit transformer language models, 2022. URL https://arxiv.org/abs/2209.13325.\n\nA broadcoverage challenge corpus for sentence understanding through inference. A Williams, N Nangia, S Bowman, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesLong Papers1Williams, A., Nangia, N., and Bowman, S. A broad- coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Confer- ence of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long Papers), pp. 1112-1122. As- sociation for Computational Linguistics, 2018. URL http://aclweb.org/anthology/N18-1101.\n\nZeroquant: Efficient and affordable post-training quantization for large-scale transformers. Z Yao, R Y Aminabadi, M Zhang, X Wu, C Li, Y He, Yao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., and He, Y. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers, 2022. URL https://arxiv.org/abs/2206.01861.\n\nOrca: A distributed serving system for {Transformer-Based} generative models. G.-I Yu, J S Jeong, G.-W Kim, S Kim, Chun , B.-G , 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). Yu, G.-I., Jeong, J. S., Kim, G.-W., Kim, S., and Chun, B.- G. Orca: A distributed serving system for {Transformer- Based} generative models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pp. 521-538, 2022.\n\nHellaswag: Can a machine really finish your sentence? CoRR. R Zellers, A Holtzman, Y Bisk, A Farhadi, Y Choi, abs/1905.07830Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence? CoRR, abs/1905.07830, 2019. URL http://arxiv.org/abs/ 1905.07830.\n\nA Zeng, X Liu, Z Du, Z Wang, H Lai, M Ding, Z Yang, Y Xu, W Zheng, X Xia, arXiv:2210.02414Glm-130b: An open bilingual pre-trained model. arXiv preprintZeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y., Zheng, W., Xia, X., et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.\n\nOpt: Open pre-trained transformer language models. S Zhang, S Roller, N Goyal, M Artetxe, M Chen, S Chen, C Dewan, M Diab, X Li, X V Lin, T Mihaylov, M Ott, S Shleifer, K Shuster, D Simig, P S Koura, A Sridhar, T Wang, L Zettlemoyer, Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mi- haylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. URL https://arxiv.org/abs/2205.01068.\n\nImproving neural network quantization without retraining using outlier channel splitting. R Zhao, Y Hu, J Dotzel, C De Sa, Z Zhang, International conference on machine learning. PMLRZhao, R., Hu, Y., Dotzel, J., De Sa, C., and Zhang, Z. Im- proving neural network quantization without retraining using outlier channel splitting. In International confer- ence on machine learning, pp. 7543-7552. PMLR, 2019.\n", "annotations": {"author": "[{\"end\":107,\"start\":92},{\"end\":115,\"start\":108},{\"end\":131,\"start\":116},{\"end\":139,\"start\":132},{\"end\":155,\"start\":140},{\"end\":165,\"start\":156}]", "publisher": null, "author_last_name": "[{\"end\":106,\"start\":102},{\"end\":114,\"start\":111},{\"end\":130,\"start\":124},{\"end\":138,\"start\":136},{\"end\":154,\"start\":147},{\"end\":164,\"start\":161}]", "author_first_name": "[{\"end\":101,\"start\":92},{\"end\":110,\"start\":108},{\"end\":123,\"start\":116},{\"end\":135,\"start\":132},{\"end\":146,\"start\":140},{\"end\":160,\"start\":156}]", "author_affiliation": null, "title": "[{\"end\":89,\"start\":1},{\"end\":254,\"start\":166}]", "venue": null, "abstract": "[{\"end\":1359,\"start\":256}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1475,\"start\":1454},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":1494,\"start\":1475},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1626,\"start\":1605},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2263,\"start\":2240},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2280,\"start\":2263},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2758,\"start\":2737},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2948,\"start\":2925},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3042,\"start\":3024},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3395,\"start\":3372},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3535,\"start\":3515},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4045,\"start\":4025},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4227,\"start\":4204},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4977,\"start\":4958},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4994,\"start\":4977},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5053,\"start\":5032},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6353,\"start\":6330},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6370,\"start\":6353},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6394,\"start\":6370},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6945,\"start\":6922},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6962,\"start\":6945},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6980,\"start\":6962},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8236,\"start\":8211},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9397,\"start\":9374},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9414,\"start\":9397},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10679,\"start\":10662},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11531,\"start\":11511},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12389,\"start\":12371},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12930,\"start\":12911},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13671,\"start\":13653},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13707,\"start\":13684},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13751,\"start\":13733},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14095,\"start\":14076},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":14129,\"start\":14110},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14202,\"start\":14180},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14236,\"start\":14214},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14262,\"start\":14243},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14299,\"start\":14275},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14335,\"start\":14312},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14360,\"start\":14341},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14390,\"start\":14367},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14456,\"start\":14435},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14531,\"start\":14507},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14561,\"start\":14538},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14587,\"start\":14568},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14898,\"start\":14878},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15304,\"start\":15285},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17013,\"start\":16991},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20072,\"start\":20052},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21904,\"start\":21883},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22036,\"start\":22018},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22055,\"start\":22036},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22071,\"start\":22055},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22094,\"start\":22071},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22423,\"start\":22404},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22456,\"start\":22437},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22499,\"start\":22479},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22755,\"start\":22737},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22774,\"start\":22755},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22793,\"start\":22774},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":22811,\"start\":22793},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22828,\"start\":22811},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22865,\"start\":22846},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22882,\"start\":22865},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22899,\"start\":22882},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22917,\"start\":22899},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22941,\"start\":22917},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22984,\"start\":22964},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":23026,\"start\":23007},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23235,\"start\":23212},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23287,\"start\":23265},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":23422,\"start\":23404},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23452,\"start\":23433},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23710,\"start\":23687},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":23928,\"start\":23910},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24108,\"start\":24087},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24138,\"start\":24118},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26318,\"start\":26293}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25252,\"start\":25120},{\"attributes\":{\"id\":\"fig_1\"},\"end\":25471,\"start\":25253},{\"attributes\":{\"id\":\"fig_2\"},\"end\":25639,\"start\":25472},{\"attributes\":{\"id\":\"fig_3\"},\"end\":26202,\"start\":25640},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":26710,\"start\":26203},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":27660,\"start\":26711},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":28106,\"start\":27661},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":28256,\"start\":28107},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":28909,\"start\":28257},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":31827,\"start\":28910},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":32089,\"start\":31828},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":32804,\"start\":32090},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":33547,\"start\":32805},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":33939,\"start\":33548},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":34384,\"start\":33940},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":34970,\"start\":34385}]", "paragraph": "[{\"end\":2003,\"start\":1375},{\"end\":2668,\"start\":2005},{\"end\":3396,\"start\":2670},{\"end\":3645,\"start\":3414},{\"end\":3874,\"start\":3698},{\"end\":4046,\"start\":3876},{\"end\":4995,\"start\":4048},{\"end\":5054,\"start\":4997},{\"end\":5602,\"start\":5108},{\"end\":5656,\"start\":5639},{\"end\":5675,\"start\":5658},{\"end\":5708,\"start\":5677},{\"end\":5764,\"start\":5748},{\"end\":6210,\"start\":5766},{\"end\":6676,\"start\":6248},{\"end\":7011,\"start\":6678},{\"end\":7649,\"start\":7018},{\"end\":8085,\"start\":7651},{\"end\":8634,\"start\":8087},{\"end\":9225,\"start\":8636},{\"end\":9526,\"start\":9284},{\"end\":9831,\"start\":9542},{\"end\":10099,\"start\":9871},{\"end\":10394,\"start\":10101},{\"end\":10680,\"start\":10396},{\"end\":12140,\"start\":10682},{\"end\":13093,\"start\":12142},{\"end\":13983,\"start\":13135},{\"end\":15093,\"start\":13985},{\"end\":15744,\"start\":15095},{\"end\":16275,\"start\":15746},{\"end\":16873,\"start\":16301},{\"end\":17372,\"start\":16886},{\"end\":17530,\"start\":17402},{\"end\":18746,\"start\":17532},{\"end\":19559,\"start\":18748},{\"end\":19599,\"start\":19561},{\"end\":20381,\"start\":19611},{\"end\":20962,\"start\":20400},{\"end\":21728,\"start\":20964},{\"end\":22551,\"start\":21745},{\"end\":23236,\"start\":22553},{\"end\":24452,\"start\":23238},{\"end\":24641,\"start\":24467},{\"end\":25119,\"start\":24643}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3697,\"start\":3646},{\"attributes\":{\"id\":\"formula_1\"},\"end\":5107,\"start\":5055},{\"attributes\":{\"id\":\"formula_2\"},\"end\":5638,\"start\":5603},{\"attributes\":{\"id\":\"formula_3\"},\"end\":5747,\"start\":5709},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9283,\"start\":9226},{\"attributes\":{\"id\":\"formula_5\"},\"end\":9870,\"start\":9832},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13134,\"start\":13094}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":8452,\"start\":8445},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":13982,\"start\":13975},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":16486,\"start\":16479},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":16496,\"start\":16487},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":17292,\"start\":17285},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":19587,\"start\":19580},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":20093,\"start\":20086},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20430,\"start\":20422}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1373,\"start\":1361},{\"attributes\":{\"n\":\"2\"},\"end\":3412,\"start\":3399},{\"attributes\":{\"n\":\"3\"},\"end\":6246,\"start\":6213},{\"end\":7016,\"start\":7014},{\"attributes\":{\"n\":\"4\"},\"end\":9540,\"start\":9529},{\"attributes\":{\"n\":\"5.2\"},\"end\":16299,\"start\":16278},{\"end\":16884,\"start\":16876},{\"attributes\":{\"n\":\"5.3\"},\"end\":17400,\"start\":17375},{\"end\":19609,\"start\":19602},{\"attributes\":{\"n\":\"5.5\"},\"end\":20398,\"start\":20384},{\"attributes\":{\"n\":\"6\"},\"end\":21743,\"start\":21731},{\"attributes\":{\"n\":\"7\"},\"end\":24465,\"start\":24455},{\"end\":25131,\"start\":25121},{\"end\":25264,\"start\":25254},{\"end\":25483,\"start\":25473},{\"end\":25651,\"start\":25641},{\"end\":26213,\"start\":26204},{\"end\":26721,\"start\":26712},{\"end\":28117,\"start\":28108},{\"end\":28920,\"start\":28911},{\"end\":31838,\"start\":31829},{\"end\":32100,\"start\":32091},{\"end\":32815,\"start\":32806},{\"end\":33558,\"start\":33549},{\"end\":33950,\"start\":33941},{\"end\":34396,\"start\":34386}]", "table": "[{\"end\":26710,\"start\":26484},{\"end\":27660,\"start\":27257},{\"end\":28909,\"start\":28323},{\"end\":31827,\"start\":29004},{\"end\":32089,\"start\":31887},{\"end\":32804,\"start\":32102},{\"end\":33547,\"start\":32866},{\"end\":33939,\"start\":33762},{\"end\":34384,\"start\":34125},{\"end\":34970,\"start\":34716}]", "figure_caption": "[{\"end\":25252,\"start\":25133},{\"end\":25471,\"start\":25266},{\"end\":25639,\"start\":25485},{\"end\":26202,\"start\":25653},{\"end\":26484,\"start\":26215},{\"end\":27257,\"start\":26723},{\"end\":28106,\"start\":27663},{\"end\":28256,\"start\":28119},{\"end\":28323,\"start\":28259},{\"end\":29004,\"start\":28922},{\"end\":31887,\"start\":31840},{\"end\":32866,\"start\":32817},{\"end\":33762,\"start\":33560},{\"end\":34125,\"start\":33952},{\"end\":34716,\"start\":34399}]", "figure_ref": "[{\"end\":1760,\"start\":1752},{\"end\":3148,\"start\":3140},{\"end\":4459,\"start\":4451},{\"end\":5210,\"start\":5202},{\"end\":5834,\"start\":5826},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6613,\"start\":6605},{\"end\":7333,\"start\":7325},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7821,\"start\":7812},{\"end\":9160,\"start\":9152},{\"end\":11797,\"start\":11788},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12762,\"start\":12754},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13529,\"start\":13521},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18097,\"start\":18089},{\"end\":18817,\"start\":18809},{\"end\":19549,\"start\":19541},{\"end\":19644,\"start\":19636},{\"end\":21191,\"start\":21182},{\"end\":21520,\"start\":21511}]", "bib_author_first_name": "[{\"end\":38181,\"start\":38180},{\"end\":38189,\"start\":38188},{\"end\":38200,\"start\":38199},{\"end\":38202,\"start\":38201},{\"end\":38210,\"start\":38209},{\"end\":38217,\"start\":38216},{\"end\":38562,\"start\":38561},{\"end\":38576,\"start\":38575},{\"end\":38585,\"start\":38584},{\"end\":39240,\"start\":39239},{\"end\":39249,\"start\":39248},{\"end\":39257,\"start\":39256},{\"end\":39266,\"start\":39265},{\"end\":39277,\"start\":39276},{\"end\":39279,\"start\":39278},{\"end\":39289,\"start\":39288},{\"end\":39301,\"start\":39300},{\"end\":39316,\"start\":39315},{\"end\":39325,\"start\":39324},{\"end\":39335,\"start\":39334},{\"end\":39345,\"start\":39344},{\"end\":39356,\"start\":39355},{\"end\":39372,\"start\":39371},{\"end\":39383,\"start\":39382},{\"end\":39395,\"start\":39394},{\"end\":39404,\"start\":39403},{\"end\":39414,\"start\":39413},{\"end\":39425,\"start\":39424},{\"end\":39431,\"start\":39430},{\"end\":39441,\"start\":39440},{\"end\":39450,\"start\":39449},{\"end\":39458,\"start\":39457},{\"end\":39468,\"start\":39467},{\"end\":39478,\"start\":39477},{\"end\":39486,\"start\":39485},{\"end\":39495,\"start\":39494},{\"end\":39504,\"start\":39503},{\"end\":39514,\"start\":39513},{\"end\":39528,\"start\":39527},{\"end\":39539,\"start\":39538},{\"end\":39557,\"start\":39551},{\"end\":39561,\"start\":39560},{\"end\":40458,\"start\":40457},{\"end\":40467,\"start\":40466},{\"end\":40475,\"start\":40474},{\"end\":40484,\"start\":40483},{\"end\":40495,\"start\":40494},{\"end\":40497,\"start\":40496},{\"end\":40507,\"start\":40506},{\"end\":40519,\"start\":40518},{\"end\":40534,\"start\":40533},{\"end\":40543,\"start\":40542},{\"end\":40553,\"start\":40552},{\"end\":40862,\"start\":40861},{\"end\":40875,\"start\":40874},{\"end\":40885,\"start\":40884},{\"end\":40895,\"start\":40894},{\"end\":40904,\"start\":40903},{\"end\":40914,\"start\":40913},{\"end\":40925,\"start\":40924},{\"end\":40935,\"start\":40934},{\"end\":40937,\"start\":40936},{\"end\":40946,\"start\":40945},{\"end\":40956,\"start\":40955},{\"end\":41260,\"start\":41259},{\"end\":41272,\"start\":41271},{\"end\":41281,\"start\":41280},{\"end\":41292,\"start\":41291},{\"end\":41644,\"start\":41643},{\"end\":41654,\"start\":41653},{\"end\":41663,\"start\":41662},{\"end\":41670,\"start\":41669},{\"end\":42020,\"start\":42019},{\"end\":42026,\"start\":42025},{\"end\":42035,\"start\":42034},{\"end\":42037,\"start\":42036},{\"end\":42044,\"start\":42043},{\"end\":42052,\"start\":42051},{\"end\":42064,\"start\":42063},{\"end\":42070,\"start\":42069},{\"end\":42080,\"start\":42079},{\"end\":42088,\"start\":42087},{\"end\":42090,\"start\":42089},{\"end\":42096,\"start\":42095},{\"end\":42417,\"start\":42416},{\"end\":42428,\"start\":42427},{\"end\":42440,\"start\":42439},{\"end\":42458,\"start\":42450},{\"end\":42462,\"start\":42461},{\"end\":42823,\"start\":42822},{\"end\":42830,\"start\":42829},{\"end\":42842,\"start\":42841},{\"end\":42851,\"start\":42850},{\"end\":42862,\"start\":42861},{\"end\":42871,\"start\":42870},{\"end\":42881,\"start\":42880},{\"end\":42890,\"start\":42889},{\"end\":42896,\"start\":42895},{\"end\":42905,\"start\":42904},{\"end\":43282,\"start\":43281},{\"end\":43289,\"start\":43288},{\"end\":43296,\"start\":43295},{\"end\":43298,\"start\":43297},{\"end\":43524,\"start\":43523},{\"end\":43537,\"start\":43536},{\"end\":43546,\"start\":43545},{\"end\":43556,\"start\":43555},{\"end\":43563,\"start\":43562},{\"end\":43574,\"start\":43573},{\"end\":43582,\"start\":43581},{\"end\":43911,\"start\":43910},{\"end\":43920,\"start\":43919},{\"end\":43930,\"start\":43929},{\"end\":43938,\"start\":43937},{\"end\":43945,\"start\":43944},{\"end\":43953,\"start\":43952},{\"end\":43963,\"start\":43962},{\"end\":43971,\"start\":43970},{\"end\":44454,\"start\":44453},{\"end\":44461,\"start\":44460},{\"end\":44472,\"start\":44471},{\"end\":44479,\"start\":44478},{\"end\":44481,\"start\":44480},{\"end\":44492,\"start\":44491},{\"end\":44843,\"start\":44842},{\"end\":44852,\"start\":44851},{\"end\":44859,\"start\":44858},{\"end\":44868,\"start\":44867},{\"end\":44885,\"start\":44884},{\"end\":44896,\"start\":44895},{\"end\":44904,\"start\":44903},{\"end\":44916,\"start\":44915},{\"end\":45233,\"start\":45232},{\"end\":45243,\"start\":45239},{\"end\":45251,\"start\":45250},{\"end\":45258,\"start\":45257},{\"end\":45265,\"start\":45264},{\"end\":45584,\"start\":45583},{\"end\":45591,\"start\":45590},{\"end\":45599,\"start\":45598},{\"end\":45606,\"start\":45605},{\"end\":45615,\"start\":45614},{\"end\":45621,\"start\":45620},{\"end\":45901,\"start\":45900},{\"end\":45911,\"start\":45910},{\"end\":45920,\"start\":45919},{\"end\":45932,\"start\":45931},{\"end\":46123,\"start\":46122},{\"end\":46135,\"start\":46134},{\"end\":46144,\"start\":46143},{\"end\":46152,\"start\":46151},{\"end\":46406,\"start\":46405},{\"end\":46415,\"start\":46414},{\"end\":46417,\"start\":46416},{\"end\":46427,\"start\":46426},{\"end\":46442,\"start\":46441},{\"end\":46886,\"start\":46885},{\"end\":46897,\"start\":46896},{\"end\":46911,\"start\":46910},{\"end\":46924,\"start\":46923},{\"end\":46926,\"start\":46925},{\"end\":46934,\"start\":46933},{\"end\":46946,\"start\":46945},{\"end\":46958,\"start\":46957},{\"end\":46968,\"start\":46967},{\"end\":46978,\"start\":46977},{\"end\":47788,\"start\":47787},{\"end\":47796,\"start\":47795},{\"end\":47804,\"start\":47803},{\"end\":47806,\"start\":47805},{\"end\":47814,\"start\":47813},{\"end\":47821,\"start\":47820},{\"end\":47828,\"start\":47827},{\"end\":48066,\"start\":48065},{\"end\":48074,\"start\":48073},{\"end\":48085,\"start\":48084},{\"end\":48098,\"start\":48097},{\"end\":48108,\"start\":48107},{\"end\":48120,\"start\":48119},{\"end\":48132,\"start\":48131},{\"end\":48140,\"start\":48139},{\"end\":48148,\"start\":48147},{\"end\":48159,\"start\":48158},{\"end\":48523,\"start\":48522},{\"end\":48525,\"start\":48524},{\"end\":48532,\"start\":48531},{\"end\":48544,\"start\":48543},{\"end\":48551,\"start\":48550},{\"end\":48563,\"start\":48562},{\"end\":48575,\"start\":48574},{\"end\":48583,\"start\":48582},{\"end\":48596,\"start\":48595},{\"end\":48609,\"start\":48608},{\"end\":48617,\"start\":48616},{\"end\":48984,\"start\":48983},{\"end\":48996,\"start\":48995},{\"end\":48998,\"start\":48997},{\"end\":49012,\"start\":49006},{\"end\":49016,\"start\":49015},{\"end\":49018,\"start\":49017},{\"end\":49543,\"start\":49542},{\"end\":49556,\"start\":49555},{\"end\":49558,\"start\":49557},{\"end\":49566,\"start\":49565},{\"end\":49581,\"start\":49580},{\"end\":49843,\"start\":49842},{\"end\":49845,\"start\":49844},{\"end\":49853,\"start\":49852},{\"end\":49860,\"start\":49859},{\"end\":49869,\"start\":49868},{\"end\":49880,\"start\":49879},{\"end\":49888,\"start\":49887},{\"end\":49899,\"start\":49898},{\"end\":49911,\"start\":49910},{\"end\":49913,\"start\":49912},{\"end\":49925,\"start\":49924},{\"end\":49933,\"start\":49932},{\"end\":50319,\"start\":50318},{\"end\":50327,\"start\":50326},{\"end\":50335,\"start\":50334},{\"end\":50341,\"start\":50340},{\"end\":50347,\"start\":50346},{\"end\":50354,\"start\":50353},{\"end\":50365,\"start\":50364},{\"end\":50367,\"start\":50366},{\"end\":50378,\"start\":50377},{\"end\":50841,\"start\":50840},{\"end\":50852,\"start\":50851},{\"end\":50863,\"start\":50862},{\"end\":50871,\"start\":50870},{\"end\":50884,\"start\":50883},{\"end\":50894,\"start\":50893},{\"end\":51268,\"start\":51267},{\"end\":51277,\"start\":51276},{\"end\":51288,\"start\":51287},{\"end\":51298,\"start\":51297},{\"end\":51311,\"start\":51310},{\"end\":51326,\"start\":51325},{\"end\":51336,\"start\":51335},{\"end\":51343,\"start\":51342},{\"end\":51357,\"start\":51356},{\"end\":51368,\"start\":51367},{\"end\":51702,\"start\":51701},{\"end\":51713,\"start\":51712},{\"end\":51723,\"start\":51722},{\"end\":51734,\"start\":51733},{\"end\":51749,\"start\":51745},{\"end\":51760,\"start\":51759},{\"end\":51771,\"start\":51770},{\"end\":51782,\"start\":51781},{\"end\":51791,\"start\":51790},{\"end\":51801,\"start\":51800},{\"end\":52196,\"start\":52195},{\"end\":52207,\"start\":52206},{\"end\":52218,\"start\":52217},{\"end\":52228,\"start\":52227},{\"end\":52241,\"start\":52240},{\"end\":52250,\"start\":52249},{\"end\":52252,\"start\":52251},{\"end\":52261,\"start\":52260},{\"end\":52271,\"start\":52270},{\"end\":52574,\"start\":52573},{\"end\":52582,\"start\":52581},{\"end\":52591,\"start\":52590},{\"end\":52602,\"start\":52601},{\"end\":52610,\"start\":52609},{\"end\":52618,\"start\":52617},{\"end\":52620,\"start\":52619},{\"end\":52953,\"start\":52952},{\"end\":52961,\"start\":52960},{\"end\":52970,\"start\":52969},{\"end\":53255,\"start\":53254},{\"end\":53263,\"start\":53262},{\"end\":53270,\"start\":53269},{\"end\":53277,\"start\":53276},{\"end\":53284,\"start\":53283},{\"end\":53506,\"start\":53505},{\"end\":53513,\"start\":53512},{\"end\":53522,\"start\":53521},{\"end\":53531,\"start\":53530},{\"end\":53539,\"start\":53538},{\"end\":53548,\"start\":53547},{\"end\":53557,\"start\":53556},{\"end\":53563,\"start\":53562},{\"end\":53856,\"start\":53855},{\"end\":53868,\"start\":53867},{\"end\":53878,\"start\":53877},{\"end\":54670,\"start\":54669},{\"end\":54677,\"start\":54676},{\"end\":54679,\"start\":54678},{\"end\":54692,\"start\":54691},{\"end\":54701,\"start\":54700},{\"end\":54707,\"start\":54706},{\"end\":54713,\"start\":54712},{\"end\":55003,\"start\":54999},{\"end\":55009,\"start\":55008},{\"end\":55011,\"start\":55010},{\"end\":55023,\"start\":55019},{\"end\":55030,\"start\":55029},{\"end\":55040,\"start\":55036},{\"end\":55047,\"start\":55043},{\"end\":55436,\"start\":55435},{\"end\":55447,\"start\":55446},{\"end\":55459,\"start\":55458},{\"end\":55467,\"start\":55466},{\"end\":55478,\"start\":55477},{\"end\":55684,\"start\":55683},{\"end\":55692,\"start\":55691},{\"end\":55699,\"start\":55698},{\"end\":55705,\"start\":55704},{\"end\":55713,\"start\":55712},{\"end\":55720,\"start\":55719},{\"end\":55728,\"start\":55727},{\"end\":55736,\"start\":55735},{\"end\":55742,\"start\":55741},{\"end\":55751,\"start\":55750},{\"end\":56074,\"start\":56073},{\"end\":56083,\"start\":56082},{\"end\":56093,\"start\":56092},{\"end\":56102,\"start\":56101},{\"end\":56113,\"start\":56112},{\"end\":56121,\"start\":56120},{\"end\":56129,\"start\":56128},{\"end\":56138,\"start\":56137},{\"end\":56146,\"start\":56145},{\"end\":56152,\"start\":56151},{\"end\":56154,\"start\":56153},{\"end\":56161,\"start\":56160},{\"end\":56173,\"start\":56172},{\"end\":56180,\"start\":56179},{\"end\":56192,\"start\":56191},{\"end\":56203,\"start\":56202},{\"end\":56212,\"start\":56211},{\"end\":56214,\"start\":56213},{\"end\":56223,\"start\":56222},{\"end\":56234,\"start\":56233},{\"end\":56242,\"start\":56241},{\"end\":56671,\"start\":56670},{\"end\":56679,\"start\":56678},{\"end\":56685,\"start\":56684},{\"end\":56695,\"start\":56694},{\"end\":56704,\"start\":56703}]", "bib_author_last_name": "[{\"end\":38186,\"start\":38182},{\"end\":38197,\"start\":38190},{\"end\":38207,\"start\":38203},{\"end\":38214,\"start\":38211},{\"end\":38222,\"start\":38218},{\"end\":38228,\"start\":38224},{\"end\":38573,\"start\":38563},{\"end\":38582,\"start\":38577},{\"end\":38597,\"start\":38586},{\"end\":39246,\"start\":39241},{\"end\":39254,\"start\":39250},{\"end\":39263,\"start\":39258},{\"end\":39274,\"start\":39267},{\"end\":39286,\"start\":39280},{\"end\":39298,\"start\":39290},{\"end\":39313,\"start\":39302},{\"end\":39322,\"start\":39317},{\"end\":39332,\"start\":39326},{\"end\":39342,\"start\":39336},{\"end\":39353,\"start\":39346},{\"end\":39369,\"start\":39357},{\"end\":39380,\"start\":39373},{\"end\":39392,\"start\":39384},{\"end\":39401,\"start\":39396},{\"end\":39411,\"start\":39405},{\"end\":39422,\"start\":39415},{\"end\":39428,\"start\":39426},{\"end\":39438,\"start\":39432},{\"end\":39447,\"start\":39442},{\"end\":39455,\"start\":39451},{\"end\":39465,\"start\":39459},{\"end\":39475,\"start\":39469},{\"end\":39483,\"start\":39479},{\"end\":39492,\"start\":39487},{\"end\":39501,\"start\":39496},{\"end\":39511,\"start\":39505},{\"end\":39525,\"start\":39515},{\"end\":39536,\"start\":39529},{\"end\":39549,\"start\":39540},{\"end\":40464,\"start\":40459},{\"end\":40472,\"start\":40468},{\"end\":40481,\"start\":40476},{\"end\":40492,\"start\":40485},{\"end\":40504,\"start\":40498},{\"end\":40516,\"start\":40508},{\"end\":40531,\"start\":40520},{\"end\":40540,\"start\":40535},{\"end\":40550,\"start\":40544},{\"end\":40560,\"start\":40554},{\"end\":40872,\"start\":40863},{\"end\":40882,\"start\":40876},{\"end\":40892,\"start\":40886},{\"end\":40901,\"start\":40896},{\"end\":40911,\"start\":40905},{\"end\":40922,\"start\":40915},{\"end\":40932,\"start\":40926},{\"end\":40943,\"start\":40938},{\"end\":40953,\"start\":40947},{\"end\":40965,\"start\":40957},{\"end\":41269,\"start\":41261},{\"end\":41278,\"start\":41273},{\"end\":41289,\"start\":41282},{\"end\":41304,\"start\":41293},{\"end\":41651,\"start\":41645},{\"end\":41660,\"start\":41655},{\"end\":41667,\"start\":41664},{\"end\":41680,\"start\":41671},{\"end\":42023,\"start\":42021},{\"end\":42032,\"start\":42027},{\"end\":42041,\"start\":42038},{\"end\":42049,\"start\":42045},{\"end\":42061,\"start\":42053},{\"end\":42067,\"start\":42065},{\"end\":42077,\"start\":42071},{\"end\":42085,\"start\":42081},{\"end\":42093,\"start\":42091},{\"end\":42102,\"start\":42097},{\"end\":42425,\"start\":42418},{\"end\":42437,\"start\":42429},{\"end\":42448,\"start\":42441},{\"end\":42467,\"start\":42463},{\"end\":42827,\"start\":42824},{\"end\":42839,\"start\":42831},{\"end\":42848,\"start\":42843},{\"end\":42859,\"start\":42852},{\"end\":42868,\"start\":42863},{\"end\":42878,\"start\":42872},{\"end\":42887,\"start\":42882},{\"end\":42893,\"start\":42891},{\"end\":42902,\"start\":42897},{\"end\":42915,\"start\":42906},{\"end\":43286,\"start\":43283},{\"end\":43293,\"start\":43290},{\"end\":43304,\"start\":43299},{\"end\":43534,\"start\":43525},{\"end\":43543,\"start\":43538},{\"end\":43553,\"start\":43547},{\"end\":43560,\"start\":43557},{\"end\":43571,\"start\":43564},{\"end\":43579,\"start\":43575},{\"end\":43593,\"start\":43583},{\"end\":43917,\"start\":43912},{\"end\":43927,\"start\":43921},{\"end\":43935,\"start\":43931},{\"end\":43942,\"start\":43939},{\"end\":43950,\"start\":43946},{\"end\":43960,\"start\":43954},{\"end\":43968,\"start\":43964},{\"end\":43984,\"start\":43972},{\"end\":44458,\"start\":44455},{\"end\":44469,\"start\":44462},{\"end\":44476,\"start\":44473},{\"end\":44489,\"start\":44482},{\"end\":44500,\"start\":44493},{\"end\":44849,\"start\":44844},{\"end\":44856,\"start\":44853},{\"end\":44865,\"start\":44860},{\"end\":44882,\"start\":44869},{\"end\":44893,\"start\":44886},{\"end\":44901,\"start\":44897},{\"end\":44913,\"start\":44905},{\"end\":44928,\"start\":44917},{\"end\":44934,\"start\":44930},{\"end\":45237,\"start\":45234},{\"end\":45248,\"start\":45244},{\"end\":45255,\"start\":45252},{\"end\":45262,\"start\":45259},{\"end\":45269,\"start\":45266},{\"end\":45588,\"start\":45585},{\"end\":45596,\"start\":45592},{\"end\":45603,\"start\":45600},{\"end\":45612,\"start\":45607},{\"end\":45618,\"start\":45616},{\"end\":45625,\"start\":45622},{\"end\":45908,\"start\":45902},{\"end\":45917,\"start\":45912},{\"end\":45929,\"start\":45921},{\"end\":45939,\"start\":45933},{\"end\":46132,\"start\":46124},{\"end\":46141,\"start\":46136},{\"end\":46149,\"start\":46145},{\"end\":46162,\"start\":46153},{\"end\":46412,\"start\":46407},{\"end\":46424,\"start\":46418},{\"end\":46439,\"start\":46428},{\"end\":46450,\"start\":46443},{\"end\":46894,\"start\":46887},{\"end\":46908,\"start\":46898},{\"end\":46921,\"start\":46912},{\"end\":46931,\"start\":46927},{\"end\":46943,\"start\":46935},{\"end\":46955,\"start\":46947},{\"end\":46965,\"start\":46959},{\"end\":46975,\"start\":46969},{\"end\":46988,\"start\":46979},{\"end\":47793,\"start\":47789},{\"end\":47801,\"start\":47797},{\"end\":47811,\"start\":47807},{\"end\":47818,\"start\":47815},{\"end\":47825,\"start\":47822},{\"end\":47832,\"start\":47829},{\"end\":47839,\"start\":47834},{\"end\":48071,\"start\":48067},{\"end\":48082,\"start\":48075},{\"end\":48095,\"start\":48086},{\"end\":48105,\"start\":48099},{\"end\":48117,\"start\":48109},{\"end\":48129,\"start\":48121},{\"end\":48137,\"start\":48133},{\"end\":48145,\"start\":48141},{\"end\":48156,\"start\":48149},{\"end\":48164,\"start\":48160},{\"end\":48529,\"start\":48526},{\"end\":48541,\"start\":48533},{\"end\":48548,\"start\":48545},{\"end\":48560,\"start\":48552},{\"end\":48572,\"start\":48564},{\"end\":48580,\"start\":48576},{\"end\":48593,\"start\":48584},{\"end\":48606,\"start\":48597},{\"end\":48614,\"start\":48610},{\"end\":48623,\"start\":48618},{\"end\":48993,\"start\":48985},{\"end\":49004,\"start\":48999},{\"end\":49553,\"start\":49544},{\"end\":49563,\"start\":49559},{\"end\":49578,\"start\":49567},{\"end\":49586,\"start\":49582},{\"end\":49598,\"start\":49588},{\"end\":49850,\"start\":49846},{\"end\":49857,\"start\":49854},{\"end\":49866,\"start\":49861},{\"end\":49877,\"start\":49870},{\"end\":49885,\"start\":49881},{\"end\":49896,\"start\":49889},{\"end\":49908,\"start\":49900},{\"end\":49922,\"start\":49914},{\"end\":49930,\"start\":49926},{\"end\":49939,\"start\":49934},{\"end\":50324,\"start\":50320},{\"end\":50332,\"start\":50328},{\"end\":50338,\"start\":50336},{\"end\":50344,\"start\":50342},{\"end\":50351,\"start\":50348},{\"end\":50362,\"start\":50355},{\"end\":50375,\"start\":50368},{\"end\":50386,\"start\":50379},{\"end\":50849,\"start\":50842},{\"end\":50860,\"start\":50853},{\"end\":50868,\"start\":50864},{\"end\":50881,\"start\":50872},{\"end\":50891,\"start\":50885},{\"end\":50904,\"start\":50895},{\"end\":51274,\"start\":51269},{\"end\":51285,\"start\":51278},{\"end\":51295,\"start\":51289},{\"end\":51308,\"start\":51299},{\"end\":51323,\"start\":51312},{\"end\":51333,\"start\":51327},{\"end\":51340,\"start\":51337},{\"end\":51354,\"start\":51344},{\"end\":51365,\"start\":51358},{\"end\":51380,\"start\":51369},{\"end\":51710,\"start\":51703},{\"end\":51720,\"start\":51714},{\"end\":51731,\"start\":51724},{\"end\":51743,\"start\":51735},{\"end\":51757,\"start\":51750},{\"end\":51768,\"start\":51761},{\"end\":51779,\"start\":51772},{\"end\":51788,\"start\":51783},{\"end\":51798,\"start\":51792},{\"end\":51807,\"start\":51802},{\"end\":52204,\"start\":52197},{\"end\":52215,\"start\":52208},{\"end\":52225,\"start\":52219},{\"end\":52238,\"start\":52229},{\"end\":52247,\"start\":52242},{\"end\":52258,\"start\":52253},{\"end\":52268,\"start\":52262},{\"end\":52282,\"start\":52272},{\"end\":52579,\"start\":52575},{\"end\":52588,\"start\":52583},{\"end\":52599,\"start\":52592},{\"end\":52607,\"start\":52603},{\"end\":52615,\"start\":52611},{\"end\":52627,\"start\":52621},{\"end\":52958,\"start\":52954},{\"end\":52967,\"start\":52962},{\"end\":52974,\"start\":52971},{\"end\":52983,\"start\":52976},{\"end\":53260,\"start\":53256},{\"end\":53267,\"start\":53264},{\"end\":53274,\"start\":53271},{\"end\":53281,\"start\":53278},{\"end\":53288,\"start\":53285},{\"end\":53510,\"start\":53507},{\"end\":53519,\"start\":53514},{\"end\":53528,\"start\":53523},{\"end\":53536,\"start\":53532},{\"end\":53545,\"start\":53540},{\"end\":53554,\"start\":53549},{\"end\":53560,\"start\":53558},{\"end\":53567,\"start\":53564},{\"end\":53865,\"start\":53857},{\"end\":53875,\"start\":53869},{\"end\":53885,\"start\":53879},{\"end\":54674,\"start\":54671},{\"end\":54689,\"start\":54680},{\"end\":54698,\"start\":54693},{\"end\":54704,\"start\":54702},{\"end\":54710,\"start\":54708},{\"end\":54716,\"start\":54714},{\"end\":55006,\"start\":55004},{\"end\":55017,\"start\":55012},{\"end\":55027,\"start\":55024},{\"end\":55034,\"start\":55031},{\"end\":55444,\"start\":55437},{\"end\":55456,\"start\":55448},{\"end\":55464,\"start\":55460},{\"end\":55475,\"start\":55468},{\"end\":55483,\"start\":55479},{\"end\":55689,\"start\":55685},{\"end\":55696,\"start\":55693},{\"end\":55702,\"start\":55700},{\"end\":55710,\"start\":55706},{\"end\":55717,\"start\":55714},{\"end\":55725,\"start\":55721},{\"end\":55733,\"start\":55729},{\"end\":55739,\"start\":55737},{\"end\":55748,\"start\":55743},{\"end\":55755,\"start\":55752},{\"end\":56080,\"start\":56075},{\"end\":56090,\"start\":56084},{\"end\":56099,\"start\":56094},{\"end\":56110,\"start\":56103},{\"end\":56118,\"start\":56114},{\"end\":56126,\"start\":56122},{\"end\":56135,\"start\":56130},{\"end\":56143,\"start\":56139},{\"end\":56149,\"start\":56147},{\"end\":56158,\"start\":56155},{\"end\":56170,\"start\":56162},{\"end\":56177,\"start\":56174},{\"end\":56189,\"start\":56181},{\"end\":56200,\"start\":56193},{\"end\":56209,\"start\":56204},{\"end\":56220,\"start\":56215},{\"end\":56231,\"start\":56224},{\"end\":56239,\"start\":56235},{\"end\":56254,\"start\":56243},{\"end\":56676,\"start\":56672},{\"end\":56682,\"start\":56680},{\"end\":56692,\"start\":56686},{\"end\":56701,\"start\":56696},{\"end\":56710,\"start\":56705}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":208290939},\"end\":38476,\"start\":38122},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":237940329},\"end\":39198,\"start\":38478},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":218971783},\"end\":40416,\"start\":39200},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218971783},\"end\":40859,\"start\":40418},{\"attributes\":{\"doi\":\"arXiv:2204.02311\",\"id\":\"b4\"},\"end\":41257,\"start\":40861},{\"attributes\":{\"doi\":\"arXiv:2208.07339\",\"id\":\"b5\"},\"end\":41559,\"start\":41259},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52967399},\"end\":41956,\"start\":41561},{\"attributes\":{\"id\":\"b7\"},\"end\":42414,\"start\":41958},{\"attributes\":{\"doi\":\"arXiv:2210.17323\",\"id\":\"b8\"},\"end\":42754,\"start\":42416},{\"attributes\":{\"doi\":\"arXiv:2101.00027\",\"id\":\"b9\"},\"end\":43173,\"start\":42756},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2134321},\"end\":43469,\"start\":43175},{\"attributes\":{\"doi\":\"abs/2009.03300\",\"id\":\"b11\"},\"end\":43815,\"start\":43471},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":39867659},\"end\":44419,\"start\":43817},{\"attributes\":{\"id\":\"b13\"},\"end\":44731,\"start\":44421},{\"attributes\":{\"doi\":\"arXiv:1910.13461\",\"id\":\"b14\"},\"end\":45230,\"start\":44733},{\"attributes\":{\"id\":\"b15\"},\"end\":45530,\"start\":45232},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":235658553},\"end\":45865,\"start\":45532},{\"attributes\":{\"id\":\"b17\"},\"end\":46031,\"start\":45867},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":52183757},\"end\":46331,\"start\":46033},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":184487878},\"end\":46809,\"start\":46333},{\"attributes\":{\"doi\":\"10.18653/v1/P16-1144\",\"id\":\"b20\",\"matched_paper_id\":2381275},\"end\":47701,\"start\":46811},{\"attributes\":{\"doi\":\"arXiv:2206.09557\",\"id\":\"b21\"},\"end\":48063,\"start\":47703},{\"attributes\":{\"doi\":\"arXiv:2211.05102\",\"id\":\"b22\"},\"end\":48444,\"start\":48065},{\"attributes\":{\"doi\":\"arXiv:2112.11446\",\"id\":\"b23\"},\"end\":48900,\"start\":48446},{\"attributes\":{\"doi\":\"SS-11-06\",\"id\":\"b24\",\"matched_paper_id\":434646},\"end\":49540,\"start\":48902},{\"attributes\":{\"doi\":\"arXiv:1907.10641\",\"id\":\"b25\"},\"end\":49840,\"start\":49542},{\"attributes\":{\"doi\":\"arXiv:2211.05100\",\"id\":\"b26\"},\"end\":50260,\"start\":49842},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":202565587},\"end\":50752,\"start\":50262},{\"attributes\":{\"doi\":\"abs/1909.08053\",\"id\":\"b28\"},\"end\":51160,\"start\":50754},{\"attributes\":{\"doi\":\"arXiv:2201.11990\",\"id\":\"b29\"},\"end\":51699,\"start\":51162},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b30\"},\"end\":52115,\"start\":51701},{\"attributes\":{\"id\":\"b31\"},\"end\":52484,\"start\":52117},{\"attributes\":{\"doi\":\"abs/1804.07461\",\"id\":\"b32\"},\"end\":52873,\"start\":52486},{\"attributes\":{\"doi\":\"abs/2012.09852\",\"id\":\"b33\"},\"end\":53187,\"start\":52875},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":102350477},\"end\":53424,\"start\":53189},{\"attributes\":{\"id\":\"b35\"},\"end\":53774,\"start\":53426},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3432876},\"end\":54574,\"start\":53776},{\"attributes\":{\"id\":\"b37\"},\"end\":54919,\"start\":54576},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":251734964},\"end\":55373,\"start\":54921},{\"attributes\":{\"doi\":\"abs/1905.07830\",\"id\":\"b39\"},\"end\":55681,\"start\":55375},{\"attributes\":{\"doi\":\"arXiv:2210.02414\",\"id\":\"b40\"},\"end\":56020,\"start\":55683},{\"attributes\":{\"id\":\"b41\"},\"end\":56578,\"start\":56022},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":59413897},\"end\":56986,\"start\":56580}]", "bib_title": "[{\"end\":38178,\"start\":38122},{\"end\":38559,\"start\":38478},{\"end\":39237,\"start\":39200},{\"end\":40455,\"start\":40418},{\"end\":41641,\"start\":41561},{\"end\":42017,\"start\":41958},{\"end\":43279,\"start\":43175},{\"end\":43908,\"start\":43817},{\"end\":44451,\"start\":44421},{\"end\":45581,\"start\":45532},{\"end\":46120,\"start\":46033},{\"end\":46403,\"start\":46333},{\"end\":46883,\"start\":46811},{\"end\":48981,\"start\":48902},{\"end\":50316,\"start\":50262},{\"end\":53252,\"start\":53189},{\"end\":53853,\"start\":53776},{\"end\":54997,\"start\":54921},{\"end\":56668,\"start\":56580}]", "bib_author": "[{\"end\":38188,\"start\":38180},{\"end\":38199,\"start\":38188},{\"end\":38209,\"start\":38199},{\"end\":38216,\"start\":38209},{\"end\":38224,\"start\":38216},{\"end\":38230,\"start\":38224},{\"end\":38575,\"start\":38561},{\"end\":38584,\"start\":38575},{\"end\":38599,\"start\":38584},{\"end\":39248,\"start\":39239},{\"end\":39256,\"start\":39248},{\"end\":39265,\"start\":39256},{\"end\":39276,\"start\":39265},{\"end\":39288,\"start\":39276},{\"end\":39300,\"start\":39288},{\"end\":39315,\"start\":39300},{\"end\":39324,\"start\":39315},{\"end\":39334,\"start\":39324},{\"end\":39344,\"start\":39334},{\"end\":39355,\"start\":39344},{\"end\":39371,\"start\":39355},{\"end\":39382,\"start\":39371},{\"end\":39394,\"start\":39382},{\"end\":39403,\"start\":39394},{\"end\":39413,\"start\":39403},{\"end\":39424,\"start\":39413},{\"end\":39430,\"start\":39424},{\"end\":39440,\"start\":39430},{\"end\":39449,\"start\":39440},{\"end\":39457,\"start\":39449},{\"end\":39467,\"start\":39457},{\"end\":39477,\"start\":39467},{\"end\":39485,\"start\":39477},{\"end\":39494,\"start\":39485},{\"end\":39503,\"start\":39494},{\"end\":39513,\"start\":39503},{\"end\":39527,\"start\":39513},{\"end\":39538,\"start\":39527},{\"end\":39551,\"start\":39538},{\"end\":39560,\"start\":39551},{\"end\":39564,\"start\":39560},{\"end\":40466,\"start\":40457},{\"end\":40474,\"start\":40466},{\"end\":40483,\"start\":40474},{\"end\":40494,\"start\":40483},{\"end\":40506,\"start\":40494},{\"end\":40518,\"start\":40506},{\"end\":40533,\"start\":40518},{\"end\":40542,\"start\":40533},{\"end\":40552,\"start\":40542},{\"end\":40562,\"start\":40552},{\"end\":40874,\"start\":40861},{\"end\":40884,\"start\":40874},{\"end\":40894,\"start\":40884},{\"end\":40903,\"start\":40894},{\"end\":40913,\"start\":40903},{\"end\":40924,\"start\":40913},{\"end\":40934,\"start\":40924},{\"end\":40945,\"start\":40934},{\"end\":40955,\"start\":40945},{\"end\":40967,\"start\":40955},{\"end\":41271,\"start\":41259},{\"end\":41280,\"start\":41271},{\"end\":41291,\"start\":41280},{\"end\":41306,\"start\":41291},{\"end\":41653,\"start\":41643},{\"end\":41662,\"start\":41653},{\"end\":41669,\"start\":41662},{\"end\":41682,\"start\":41669},{\"end\":42025,\"start\":42019},{\"end\":42034,\"start\":42025},{\"end\":42043,\"start\":42034},{\"end\":42051,\"start\":42043},{\"end\":42063,\"start\":42051},{\"end\":42069,\"start\":42063},{\"end\":42079,\"start\":42069},{\"end\":42087,\"start\":42079},{\"end\":42095,\"start\":42087},{\"end\":42104,\"start\":42095},{\"end\":42427,\"start\":42416},{\"end\":42439,\"start\":42427},{\"end\":42450,\"start\":42439},{\"end\":42461,\"start\":42450},{\"end\":42469,\"start\":42461},{\"end\":42829,\"start\":42822},{\"end\":42841,\"start\":42829},{\"end\":42850,\"start\":42841},{\"end\":42861,\"start\":42850},{\"end\":42870,\"start\":42861},{\"end\":42880,\"start\":42870},{\"end\":42889,\"start\":42880},{\"end\":42895,\"start\":42889},{\"end\":42904,\"start\":42895},{\"end\":42917,\"start\":42904},{\"end\":43288,\"start\":43281},{\"end\":43295,\"start\":43288},{\"end\":43306,\"start\":43295},{\"end\":43536,\"start\":43523},{\"end\":43545,\"start\":43536},{\"end\":43555,\"start\":43545},{\"end\":43562,\"start\":43555},{\"end\":43573,\"start\":43562},{\"end\":43581,\"start\":43573},{\"end\":43595,\"start\":43581},{\"end\":43919,\"start\":43910},{\"end\":43929,\"start\":43919},{\"end\":43937,\"start\":43929},{\"end\":43944,\"start\":43937},{\"end\":43952,\"start\":43944},{\"end\":43962,\"start\":43952},{\"end\":43970,\"start\":43962},{\"end\":43986,\"start\":43970},{\"end\":44460,\"start\":44453},{\"end\":44471,\"start\":44460},{\"end\":44478,\"start\":44471},{\"end\":44491,\"start\":44478},{\"end\":44502,\"start\":44491},{\"end\":44851,\"start\":44842},{\"end\":44858,\"start\":44851},{\"end\":44867,\"start\":44858},{\"end\":44884,\"start\":44867},{\"end\":44895,\"start\":44884},{\"end\":44903,\"start\":44895},{\"end\":44915,\"start\":44903},{\"end\":44930,\"start\":44915},{\"end\":44936,\"start\":44930},{\"end\":45239,\"start\":45232},{\"end\":45250,\"start\":45239},{\"end\":45257,\"start\":45250},{\"end\":45264,\"start\":45257},{\"end\":45271,\"start\":45264},{\"end\":45590,\"start\":45583},{\"end\":45598,\"start\":45590},{\"end\":45605,\"start\":45598},{\"end\":45614,\"start\":45605},{\"end\":45620,\"start\":45614},{\"end\":45627,\"start\":45620},{\"end\":45910,\"start\":45900},{\"end\":45919,\"start\":45910},{\"end\":45931,\"start\":45919},{\"end\":45941,\"start\":45931},{\"end\":46134,\"start\":46122},{\"end\":46143,\"start\":46134},{\"end\":46151,\"start\":46143},{\"end\":46164,\"start\":46151},{\"end\":46414,\"start\":46405},{\"end\":46426,\"start\":46414},{\"end\":46441,\"start\":46426},{\"end\":46452,\"start\":46441},{\"end\":46896,\"start\":46885},{\"end\":46910,\"start\":46896},{\"end\":46923,\"start\":46910},{\"end\":46933,\"start\":46923},{\"end\":46945,\"start\":46933},{\"end\":46957,\"start\":46945},{\"end\":46967,\"start\":46957},{\"end\":46977,\"start\":46967},{\"end\":46990,\"start\":46977},{\"end\":47795,\"start\":47787},{\"end\":47803,\"start\":47795},{\"end\":47813,\"start\":47803},{\"end\":47820,\"start\":47813},{\"end\":47827,\"start\":47820},{\"end\":47834,\"start\":47827},{\"end\":47841,\"start\":47834},{\"end\":48073,\"start\":48065},{\"end\":48084,\"start\":48073},{\"end\":48097,\"start\":48084},{\"end\":48107,\"start\":48097},{\"end\":48119,\"start\":48107},{\"end\":48131,\"start\":48119},{\"end\":48139,\"start\":48131},{\"end\":48147,\"start\":48139},{\"end\":48158,\"start\":48147},{\"end\":48166,\"start\":48158},{\"end\":48531,\"start\":48522},{\"end\":48543,\"start\":48531},{\"end\":48550,\"start\":48543},{\"end\":48562,\"start\":48550},{\"end\":48574,\"start\":48562},{\"end\":48582,\"start\":48574},{\"end\":48595,\"start\":48582},{\"end\":48608,\"start\":48595},{\"end\":48616,\"start\":48608},{\"end\":48625,\"start\":48616},{\"end\":48995,\"start\":48983},{\"end\":49006,\"start\":48995},{\"end\":49015,\"start\":49006},{\"end\":49021,\"start\":49015},{\"end\":49555,\"start\":49542},{\"end\":49565,\"start\":49555},{\"end\":49580,\"start\":49565},{\"end\":49588,\"start\":49580},{\"end\":49600,\"start\":49588},{\"end\":49852,\"start\":49842},{\"end\":49859,\"start\":49852},{\"end\":49868,\"start\":49859},{\"end\":49879,\"start\":49868},{\"end\":49887,\"start\":49879},{\"end\":49898,\"start\":49887},{\"end\":49910,\"start\":49898},{\"end\":49924,\"start\":49910},{\"end\":49932,\"start\":49924},{\"end\":49941,\"start\":49932},{\"end\":50326,\"start\":50318},{\"end\":50334,\"start\":50326},{\"end\":50340,\"start\":50334},{\"end\":50346,\"start\":50340},{\"end\":50353,\"start\":50346},{\"end\":50364,\"start\":50353},{\"end\":50377,\"start\":50364},{\"end\":50388,\"start\":50377},{\"end\":50851,\"start\":50840},{\"end\":50862,\"start\":50851},{\"end\":50870,\"start\":50862},{\"end\":50883,\"start\":50870},{\"end\":50893,\"start\":50883},{\"end\":50906,\"start\":50893},{\"end\":51276,\"start\":51267},{\"end\":51287,\"start\":51276},{\"end\":51297,\"start\":51287},{\"end\":51310,\"start\":51297},{\"end\":51325,\"start\":51310},{\"end\":51335,\"start\":51325},{\"end\":51342,\"start\":51335},{\"end\":51356,\"start\":51342},{\"end\":51367,\"start\":51356},{\"end\":51382,\"start\":51367},{\"end\":51712,\"start\":51701},{\"end\":51722,\"start\":51712},{\"end\":51733,\"start\":51722},{\"end\":51745,\"start\":51733},{\"end\":51759,\"start\":51745},{\"end\":51770,\"start\":51759},{\"end\":51781,\"start\":51770},{\"end\":51790,\"start\":51781},{\"end\":51800,\"start\":51790},{\"end\":51809,\"start\":51800},{\"end\":52206,\"start\":52195},{\"end\":52217,\"start\":52206},{\"end\":52227,\"start\":52217},{\"end\":52240,\"start\":52227},{\"end\":52249,\"start\":52240},{\"end\":52260,\"start\":52249},{\"end\":52270,\"start\":52260},{\"end\":52284,\"start\":52270},{\"end\":52581,\"start\":52573},{\"end\":52590,\"start\":52581},{\"end\":52601,\"start\":52590},{\"end\":52609,\"start\":52601},{\"end\":52617,\"start\":52609},{\"end\":52629,\"start\":52617},{\"end\":52960,\"start\":52952},{\"end\":52969,\"start\":52960},{\"end\":52976,\"start\":52969},{\"end\":52985,\"start\":52976},{\"end\":53262,\"start\":53254},{\"end\":53269,\"start\":53262},{\"end\":53276,\"start\":53269},{\"end\":53283,\"start\":53276},{\"end\":53290,\"start\":53283},{\"end\":53512,\"start\":53505},{\"end\":53521,\"start\":53512},{\"end\":53530,\"start\":53521},{\"end\":53538,\"start\":53530},{\"end\":53547,\"start\":53538},{\"end\":53556,\"start\":53547},{\"end\":53562,\"start\":53556},{\"end\":53569,\"start\":53562},{\"end\":53867,\"start\":53855},{\"end\":53877,\"start\":53867},{\"end\":53887,\"start\":53877},{\"end\":54676,\"start\":54669},{\"end\":54691,\"start\":54676},{\"end\":54700,\"start\":54691},{\"end\":54706,\"start\":54700},{\"end\":54712,\"start\":54706},{\"end\":54718,\"start\":54712},{\"end\":55008,\"start\":54999},{\"end\":55019,\"start\":55008},{\"end\":55029,\"start\":55019},{\"end\":55036,\"start\":55029},{\"end\":55043,\"start\":55036},{\"end\":55050,\"start\":55043},{\"end\":55446,\"start\":55435},{\"end\":55458,\"start\":55446},{\"end\":55466,\"start\":55458},{\"end\":55477,\"start\":55466},{\"end\":55485,\"start\":55477},{\"end\":55691,\"start\":55683},{\"end\":55698,\"start\":55691},{\"end\":55704,\"start\":55698},{\"end\":55712,\"start\":55704},{\"end\":55719,\"start\":55712},{\"end\":55727,\"start\":55719},{\"end\":55735,\"start\":55727},{\"end\":55741,\"start\":55735},{\"end\":55750,\"start\":55741},{\"end\":55757,\"start\":55750},{\"end\":56082,\"start\":56073},{\"end\":56092,\"start\":56082},{\"end\":56101,\"start\":56092},{\"end\":56112,\"start\":56101},{\"end\":56120,\"start\":56112},{\"end\":56128,\"start\":56120},{\"end\":56137,\"start\":56128},{\"end\":56145,\"start\":56137},{\"end\":56151,\"start\":56145},{\"end\":56160,\"start\":56151},{\"end\":56172,\"start\":56160},{\"end\":56179,\"start\":56172},{\"end\":56191,\"start\":56179},{\"end\":56202,\"start\":56191},{\"end\":56211,\"start\":56202},{\"end\":56222,\"start\":56211},{\"end\":56233,\"start\":56222},{\"end\":56241,\"start\":56233},{\"end\":56256,\"start\":56241},{\"end\":56678,\"start\":56670},{\"end\":56684,\"start\":56678},{\"end\":56694,\"start\":56684},{\"end\":56703,\"start\":56694},{\"end\":56712,\"start\":56703}]", "bib_venue": "[{\"end\":38286,\"start\":38230},{\"end\":38685,\"start\":38599},{\"end\":39613,\"start\":39564},{\"end\":40611,\"start\":40562},{\"end\":41022,\"start\":40983},{\"end\":41375,\"start\":41322},{\"end\":41696,\"start\":41682},{\"end\":42148,\"start\":42104},{\"end\":42559,\"start\":42485},{\"end\":42820,\"start\":42756},{\"end\":43310,\"start\":43306},{\"end\":43521,\"start\":43471},{\"end\":44063,\"start\":43986},{\"end\":44546,\"start\":44502},{\"end\":44840,\"start\":44733},{\"end\":45355,\"start\":45271},{\"end\":45676,\"start\":45627},{\"end\":45898,\"start\":45867},{\"end\":46172,\"start\":46164},{\"end\":46523,\"start\":46452},{\"end\":47097,\"start\":47010},{\"end\":47785,\"start\":47703},{\"end\":48223,\"start\":48182},{\"end\":48520,\"start\":48446},{\"end\":49120,\"start\":49029},{\"end\":49665,\"start\":49616},{\"end\":50013,\"start\":49957},{\"end\":50449,\"start\":50388},{\"end\":50838,\"start\":50754},{\"end\":51265,\"start\":51162},{\"end\":51870,\"start\":51825},{\"end\":52193,\"start\":52117},{\"end\":52571,\"start\":52486},{\"end\":52950,\"start\":52875},{\"end\":53294,\"start\":53290},{\"end\":53503,\"start\":53426},{\"end\":54029,\"start\":53887},{\"end\":54667,\"start\":54576},{\"end\":55128,\"start\":55050},{\"end\":55433,\"start\":55375},{\"end\":55818,\"start\":55773},{\"end\":56071,\"start\":56022},{\"end\":56756,\"start\":56712},{\"end\":38758,\"start\":38687},{\"end\":44127,\"start\":44065},{\"end\":46581,\"start\":46525},{\"end\":47186,\"start\":47099},{\"end\":49147,\"start\":49122},{\"end\":50497,\"start\":50451},{\"end\":54158,\"start\":54031}]"}}}, "year": 2023, "month": 12, "day": 17}