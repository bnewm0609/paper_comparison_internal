{"id": 208634631, "updated": "2022-03-27 01:14:40.676", "metadata": {"title": "BHive: A Benchmark Suite and Measurement Framework for Validating x86-64 Basic Block Performance Models", "authors": "[{\"first\":\"Yishen\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Ajay\",\"last\":\"Brahmakshatriya\",\"middle\":[]},{\"first\":\"Charith\",\"last\":\"Mendis\",\"middle\":[]},{\"first\":\"Alex\",\"last\":\"Renda\",\"middle\":[]},{\"first\":\"Eric\",\"last\":\"Atkinson\",\"middle\":[]},{\"first\":\"Ond\u0159ej\",\"last\":\"S\u00fdkora\",\"middle\":[]},{\"first\":\"Saman\",\"last\":\"Amarasinghe\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Carbin\",\"middle\":[]}]", "venue": "2019 IEEE International Symposium on Workload Characterization (IISWC)", "journal": "2019 IEEE International Symposium on Workload Characterization (IISWC)", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Compilers and performance engineers use hardware performance models to simplify program optimizations. Performance models provide a necessary abstraction over complex modern processors. However, constructing and maintaining a performance model can be onerous, given the numerous microarchi-tectural optimizations employed by modern processors. Despite their complexity and reported inaccuracy (e.g., deviating from native measurement by more than 30%), existing performance models-such as IACA and llvm-mca-have not been systematically validated, because there is no scalable machine code profiler that can automatically obtain throughput of arbitrary basic blocks while conforming to common modeling assumptions. In this paper, we present a novel profiler that can profile arbitrary memory-accessing basic blocks without any user intervention. We used this profiler to build BHive, a benchmark for systematic validation of performance models of x86-64 basic blocks. We used BHive to evaluate four existing performance models: IACA, llvm-mca, Ithemal, and OSACA. We automatically cluster basic blocks in the benchmark suite based on their utilization of CPU resources. Using this clustering, our benchmark can give a detailed analysis of a performance model's strengths and weaknesses on different workloads (e.g., vectorized vs. scalar basic blocks). We additionally demonstrate that our dataset well captures basic properties of two Google applications: Spanner and Dremel.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3011000655", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iiswc/ChenBMRASAC19", "doi": "10.1109/iiswc47752.2019.9042166"}}, "content": {"source": {"pdf_hash": "0b97760dacf3cd4e7f0dd79cebf6506a3202134e", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBYNCSA", "open_access_url": "https://dspace.mit.edu/bitstream/1721.1/128755/2/ithemal-measurement.pdf", "status": "GREEN"}}, "grobid": {"id": "91b82cbaf236f69f72f278eca9f0e36ecaec2b2f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0b97760dacf3cd4e7f0dd79cebf6506a3202134e.txt", "contents": "\nBHive: A Benchmark Suite and Measurement Framework for Validating x86-64 Basic Block Performance Models\n\n\nYishen Chen \nMassachusetts Institute of Technology\n\n\nAjay Brahmakshatriya ajaybr@mit.edu \nMassachusetts Institute of Technology\n\n\nCharith Mendis charithm@mit.edu \nMassachusetts Institute of Technology\n\n\nAlex Renda renda@mit.edu \nMassachusetts Institute of Technology\n\n\nEric Atkinson eatkinson@mit.edu \nMassachusetts Institute of Technology\n\n\nOnd\u0159ej S\u00fdkora \nGoogle Research\n\n\nSaman Amarasinghe \nMassachusetts Institute of Technology\n\n\nMichael Carbin mcarbin@mit.edu \nMassachusetts Institute of Technology\n\n\nBHive: A Benchmark Suite and Measurement Framework for Validating x86-64 Basic Block Performance Models\nIndex Terms-Cost/performanceMeasurement techniquesModeling techniquesBenchmarking\nCompilers and performance engineers use hardware performance models to simplify program optimizations. Performance models provide a necessary abstraction over complex modern processors. However, constructing and maintaining a performance model can be onerous, given the numerous microarchitectural optimizations employed by modern processors. Despite their complexity and reported inaccuracy (e.g., deviating from native measurement by more than 30%), existing performance models-such as IACA and llvm-mca-have not been systematically validated, because there is no scalable machine code profiler that can automatically obtain throughput of arbitrary basic blocks while conforming to common modeling assumptions.In this paper, we present a novel profiler that can profile arbitrary memory-accessing basic blocks without any user intervention. We used this profiler to build BHive, a benchmark for systematic validation of performance models of x86-64 basic blocks. We used BHive to evaluate four existing performance models: IACA, llvm-mca, Ithemal, and OSACA. We automatically cluster basic blocks in the benchmark suite based on their utilization of CPU resources. Using this clustering, our benchmark can give a detailed analysis of a performance model's strengths and weaknesses on different workloads (e.g., vectorized vs. scalar basic blocks). We additionally demonstrate that our dataset well captures basic properties of two Google applications: Spanner and Dremel.\n\nI. INTRODUCTION\n\nProcessor performance models are analytical tools that statically predict program performance without execution. They can simplify compiler optimizations such as auto-vectorization and instruction scheduling, giving compiler writers an abstraction over the machines they are targeting. Performance models can also guide manual optimization by showing potential program bottlenecks. An inaccurate performance model can misguide optimizations. For instance, Mendis and Amarasinghe [1] show that in automatic vectorization, an inaccurate performance model can cause performance regression-even with an optimal solution relative to the performance model. Pohl et al. [2] show that predictions made by the vectorization cost model of LLVM only weakly correlate with measurements, with a Pearson correlation coefficient of 0.55. There has been no comprehensive evaluation of existing performance models.\n\nIn this work, we focus on validating the prediction of performance models that run on a single basic block. To do so, we collect an extensive dataset of basic blocksalong with reference timings-that represent a wide variety of characteristic workloads: scientific computing, databases, data compression, machine learning, compilers, and graphics.\n\n\nA. Automatically Profiling Basic Blocks\n\nA key technical challenge with collecting a large basic block dataset is that there is no existing approach to profile an arbitrary basic block that has been removed from its program context. Specifically, a basic block is likely to attempt to access memory and crash unless it is run in an appropriately created execution context that includes appropriately allocated memory. Additionally, one also needs to ensure that the basic block's execution conforms to common modeling invariants that performance models typically assume, such as that all memory accesses hit the L1 cache. However, existing machine code profilers [3,4] delegate the responsibility of ensuring a basic block's successful execution to the user and make no attempt to ensure that the block accesses memory without crashing or that its execution conforms to common modeling assumptions. For instance, we modified Agner Fog's Performance Test script [3] (a standard benchmarking tool) to be slightly more robust 1 , and found that it could only profile 16.65% of basic blocks in our dataset without crashing.\n\n\nB. Contributions\n\nIn this paper, we present a novel profiler that automatically profiles the throughput-the average number of cycles it takes to execute a basic block in steady-state-of arbitrary basic blocks, and develop a dataset to support performance model validation. Specifically, we make the following contributions in this paper:\n\n\u2022 We present a profiler for arbitrary, memory-accessing basic blocks. The profiler creates an appropriate execution context for a basic block by mapping virtual pages accessed by it to a single physical page, thus legalizing all memory accesses and preventing cache misses. Our profiler automatically profiles more than 90% of the basic blocks collected in our dataset. \u2022 We present BHive, a benchmark suite of over 300,000 basic blocks collected from applications that cover a wide range of domains such as numerical computation (e.g., OpenBLAS), databases (SQLite), machine learning (Ten-sorFlow), and cryptography (OpenSSL). \u2022 We present an approach for basic block clustering that leverages Latent Dirichlet Allocation [5] to automatically cluster basic blocks into categories that reflect their execution port usage. We show that the clusters we identify generalize to two data-center applications, Google's Spanner [6] and Dremel [7], demonstrating that our open source data set is representative of unseen data. \u2022 We evaluate four existing performance models-IACA [8], llvm-mca [9] (which exposes LLVM's performance model used for instruction scheduling), OS-ACA [10], and Ithemal [11]-on three recent Intel microarchitectures: Ivy Bridge, Haswell, and Skylake. Using our execution port usage clustering, we analyze the strengths and weaknesses of the performance models on different categories of basic blocks.\n\nOur dataset provides a new benchmark for evaluating performance models and holds out the promise of a future for performance modeling research that rests on large-scale, quantitative comparisons with other state-of-theart techniques. Our tool and dataset are open-source at https://www.github.com/ithemal/bhive.\n\n\nII. BACKGROUND\n\n\nA. Existing Performance Models\n\nBroadly speaking, there are two types of performance models: microarchitecture simulators that can produce an interpretable execution trace-a timeline detailing when each instruction is issued or retired-in addition to throughput predictions (e.g., IACA [8]); and per-instruction latency/throughput lookup tables (e.g, a spill-cost estimator used by a register allocator).\n\nIACA [8] (Intel Architecture Code Analyzer) is a static analyzer developed by Intel. It has support for Intel microarchitectures since Nehalem. Given a machine code snippet, IACA estimates the average number of cycles it take to execute the given basic block in an infinite loop. Unlike its alternatives, IACA takes advantage of its knowledge of Intel's proprietary processor optimization-such as zero-idioms and micro-op fusions-to make better predictions. Intel discontinued IACA in April 2019. llvm-mca [9] is a similar tool inspired by IACA. Implemented as an out-of-order super-scalar microarchitecture simulator, it uses parameters (e.g., instruction throughput) supplied by LLVM [12]'s backend scheduling model. The reuse of the scheduling model is an explicit design choice made to expose LLVM's cost model for testing. Thus the accuracy of llvm-mca has a bearing on that of LLVM's scheduling model.\n\nOSACA [10] is an open-source alternative to IACA. Similar to llvm-mca, it is implemented as a parametrized out-of-order simulator-in this case, the parameters come from the measured throughput and latency data for individual instructions.\n\nIthemal [11] is a basic block throughput predictor implemented as a deep neural network. Unlike other models discussed so far, Ithemal is blackbox: it outputs a single throughput prediction for each input basic block, but does not report an interpretable execution trace.\n\nProduction compilers such as LLVM [12] and GCC typically use cost models to guide optimizations. Unlike models such as IACA or llvm-mca, compiler cost models typically model the costs at the instruction level, rather than at the basic block level. These compilers also use multiple cost models. LLVM, for instance, uses at least three cost models: a generic, per-instruction IR (Intermediate Representation) cost model for its target-independent optimizations [13]; one for instruction scheduling (the scheduling model [14] is also used by llvm-mca); and another one for register allocation [15]. GCC employs analogous models [16,17]. To our knowledge, out of the models discussed so far, the scheduling model of LLVM is the only one exposed by an interface for testing in isolation from its client optimizations.\n\n\nB. Machine Code Profilers\n\nSeveral tools enable users to perform low-level microbenchmarking and to validate performance models by hand. Agner Fog's script [3] profiles small code snippets. The script reports the number of cycles taken to execute the code, as well as performance statistics such as the number of cache misses. nanoBench [4] is a profiler similar to Agner Fog's [3], with two notable improvements. It allows the user to specify which processor-specific performance counters to measure, in addition to the cycle counter. It also supports profiling in kernel-mode, removing potential noise due to context-switches and interrupts.\n\nUnrolling. The basic strategy these tools take to measure basic block throughput is to unroll a basic block multiple times and divide the latency of the unrolled basic block by the unroll factor. 2 Measuring the execution of multiple iterations serves two purposes: 1) a large iteration count marginalizes the latency of the first few iterations, when the processor is still warming up to its steady-state behavior, and 2) it diminishes the overhead of collecting performance counters. The formula for estimating throughput using this approach is shown in Equation 1, where n is the unroll factor 3 and cycles(b, n) is the number of cycles taken to execute basic block b unrolled n times:\nthroughput(b) \u2248 cycles(b, n) n(1)\nCompared to running the basic block inside a loop, unrolling has an advantage in that the measurements are not tainted by the control overhead incurred by looping. However, if the unroll factor is too large, then the execution may encounter a significant number of L1-instruction cache misses that then taint the measured latency.\n\nAbel and Reineke [18] suggest an alternative approach to address these issues. Equation 2 shows the formula they use to derive basic block throughput:\nthroughput(b) \u2248 cycles(b, n) \u2212 cycles(b, n ) n \u2212 n(2)\nEssentially, instead of using a single large unroll factor, they measure basic blocks with two unroll factors, n and n . They then measure the latency of the two unrolled basic blocks, calculate the difference in the measurements, and divide it by the difference of the unroll factors. The resulting number is the throughput of the basic block.\n\nLimitations. Although it is possible to calculate throughput for individual basic blocks, these profilers are in general unsuitable for automatically profiling a large set of arbitrary basic blocks for systematic validation. They require user intervention to profile arbitrary basic blocks. Specifically, users must provide code to allocate memory and initialize memoryaddressing registers to prevent crashing from invalid accesses.\n\n\nIII. PROFILING ARBITRARY BASIC BLOCKS\n\nOur goal is to profile arbitrary basic blocks-without requiring manual intervention-such that the measured throughput corresponds to the definitions and invariants commonly assumed by performance models. The key challenge is enabling these basic blocks to access arbitrary memory addresses without crashing.\n\n\nA. Handling Arbitrary Memory Accesses\n\nMost basic blocks access memory. Directly applying existing tools to profile these basic blocks out of their original program context is likely to result in crashes.\n\nConsider the basic block in Figure 1, which Gzip uses to compute a CRC code. Highlighted instructions show the flow of pointer values: essentially, bits of %rdx are used to index into a lookup table, the content of which is then used in the next iteration to update %rdx. This basic block can only execute successfully in an execution context that allocates memory at 0x4110a. Furthermore, since the index (%rax) of the table at 0x4110a is also XOR'd every iteration with the contents of a second array addressed by %rdi, one would also need to coordinate the initialization of the second array with the one at 0x4110a, so that the pointer always points to a valid memory address.  Our technique works by mapping all virtual memory pages accessed by a basic block to a single physical page, so that all data resides in the L1 cache. The basic block can then execute without crashing since all of memory accesses are valid virtual addresses. This allows us to execute 97% of basic blocks.\n\nRemapping Virtual Pages. Figure 2 shows the algorithm we use to remap virtual pages. We first unmap all virtual pages-this forces all subsequent memory accesses to trigger a segfault-except the pages containing the basic block's instructions. We then execute an unrolled basic block in a forked process monitored by a parent process using ptrace. Each access of an unmapped virtual page triggers a segmentation fault, which is intercepted by the monitoring process. The monitor process then instructs the executing process to create the appropriate mapping and to restart execution.\n\nMemory Initialization. We initialize the single physical page-which is shared by all active virtual pages-to be filled with a moderately sized constant (we used 0x12345600 in our experiments) to accommodate for indirect memory accesses. To see why this is necessary, consider a basic block that first loads a pointer p from memory and then de-references p. If the value of p is too low (e.g., 0) or too high (i.e., bigger than the highest address that a user space program is allowed to address), we cannot map the virtual page addressed by p. All general-purpose registers are initialized similarly.\n\nVirtual page aliasing. Under our page mapping scheme, two virtual addresses differing by a multiple of page size get mapped to the same physical address. Such aliasing can introduce extra memory dependences and cause slowdown. Consider the following code snippet: * p = x; y = * (p + page_size). Whereas in a standard execution context the load can be executed independently from the first store, in this context the load can only be issued after the store finishes, due to page aliasing introduced by our profiler. We remove basic blocks whose execution that could be affected by page aliasing out of our analysis. There are 20,729 (6.28%) such basic blocks. Because there is no hardware counter that tracks accesses serialized due to page aliasing, we trace all addresses accessed during profiling and mark a basic block if, within a conservative window, there are any loads following an aliased store. We note that we can reduce the occurrence of page aliasing by mapping the set of virtual pages to a larger range of physical memory (e.g., two pages instead of one).\n\n\nB. Overall Profiling Workflow\n\nOur profiler computes the throughput of a basic block by repeatedly measuring the block, filtering measurements that violate modeling invariants, then calculating throughput using the filtered measurements.\n\nRaw Measurement. The profiler first creates an execution context so the basic block executes without crashing, using the page mapping algorithm in Figure 2. The profiler then measures the unrolled latency using the unrolling heuristics described in Section II-B. It uses 100 and 200 as the unroll factors for basic blocks smaller than 100 bytes; 50 and 100 for basic blocks between 100 bytes and 200 bytes; and finally 16 and 32 for basic blocks larger than 200 bytes.\n\nFiltering. Performance models typically model an idealized execution of the code in which all data resides in L1 cache, and rare performance-degrading events do not occur. We have designed our measurement tool to deliver measurements that are consistent with such an idealized execution.\n\n\u2022 L1 Cache Misses. Our measurement tool monitors instruction cache and data cache misses with hardware counters and rejects any measurements with a cache miss. \u2022 Unaligned Loads. Unaligned memory accesses can be slower than aligned accesses. In particular, accesses straddling a cache line boundary can cause an order of magnitude slowdown. Our measurement tool detects (using a hardware counter) unaligned loads and rejects any measurements with a non-zero number of such loads. We removed 553 (0.183%) basic blocks that are affected by unaligned accesses from our dataset. \u2022 Subnormal Floating Point. Floating-point computations on subnormal numbers can be up to 20x slower than for normal numbers. We configured the MXCSR register to disable gradual underflow. We found 334 (0.1%) basic blocks that would have been affected by gradual underflow if we had not taken this measure. \u2022 Context Switches. We profile all basic blocks with hyperthreading disabled and monitor context switches during execution using a system-call provided by Linux. We reject all measurements with a detected context switch. If the profiler rejects more than 6 of the measurement attempts due to violations of its idealized execution model, then it fully rejects the basic block. In addition, if the coefficient of variation-i.e., standard deviation divided by mean-of the measurements is more than 10%, then it also rejects the block because there is residual measurement variance that the measurement methodology is unable to eliminate.\n\nThroughput Calculation. If the block survives filtering, then the profiler uses the minimum latency of the recorded measurements to calculate throughput according Equation 2. Environment Variance. It is possible for the profiler to encounter random but consistent noise that pollutes our latency measurements. E.g., a basic block (say unrolled 100 times) with a latency of 500 cycles would yield a measured latency of around 700 cycles consistently for 16 consecutive runs. Such noise occurs about 6% of the time. To overcome this, we profile the throughput of a basic block at least five times (i.e., each basic block is measured at least 5 \u00d7 16 times), and the minimum of the five is the throughput we report.\n\n\nC. Portability to Other Architectures\n\nWe briefly discuss how to port our profiling technique to other architectures. Our profiler requires being able to map multiple virtual pages to a few physical pages, detecting cache misses, and detecting or disabling floating-point underflow. To our knowledge, recent implementations of x86-64 (AMD and Intel) and ARM satisfy these requirements.\n\nPage Mapping. We require the ability to map multiple virtual pages to a few physical pages without incurring a performance penalty due to unnecessary cache invalidation (i.e., when two aliased virtual addresses are mapped to different cache entries, the cache evicts one entry to ensure coherence). We therefore require that the target processor has a physically tagged data cache. If the cache is virtually indexed, we additionally require that the page size is small enough so the indexing bits are not affected by address translation. These two conditions are sufficient to establish that any two aliased virtual addresses are always mapped to the same cache entry.\n\nIV. BASIC BLOCK DATASET Table I shows the applications from which we extracted basic blocks for BHive. We selected these applications with the following goals. First, the set of applications should cover a diverse range of domains to represent real-world workloads. Second, their basic blocks should reflect usage that concerns typical users of a performance model; compiler developers deal with basic blocks from general-purpose programs, which have different characteristics than those from, e.g., highperformance kernels.\n\nWe selected Clang/LLVM [12] (compiler), Redis (inmemory database), SQLite (database), and Gzip (compression) to collect basic blocks representative of applications that are written in general purpose languages like C and C++. These are some of the most used applications today and they all use sophisticated algorithms and data structures, yielding a large source of diverse basic blocks.\n\nNext, we chose the following applications that use handoptimized high performance kernels: OpenSSL [19] (cryptography), OpenBLAS [20], Eigen [21] (scientific computing), TensorFlow [22] (machine learning), Embree [23] (rendering), and FFmpeg (multimedia). OpenSSL, OpenBLAS, and FFmpeg use handwritten assembly for performance-critical inner loops. Embree is written in ispc [24], a data-parallel language designed specifically to target Intel's vector extensions.\n\nWe compiled all applications with the highest optimization settings defined by their build systems. Applications that use handwritten assembly (e.g., OpenBLAS) allow users to configure their builds to target specific ISA-extensions, and we configured these applications to target machines with AVX2.\n\nWe extracted basic blocks from these applications using a dynamic analysis implemented in DynamoRIO [25], which allows us to record every basic block executed at runtime. We used a dynamic analysis rather than static disassembly because we discovered in our experiments cases of static disassemblers unable to distinguish padding bytes from instructions. To simulate realistic execution when recording the basic blocks, we use the official benchmarking input of these applications, with the exception of FFmpeg and Gzip, which to our knowledge do not have official benchmarks. 4 We evaluated Eigen on two sparse linear algebra workloads: sparse matrix-matrix multiplication (SpMM) and sparse matrix-vector multiplication (SpMV).\n\n\nV. BASIC BLOCK CLUSTERING\n\nSome basic blocks are harder to model than others. For instance, our results in Section VI-B demonstrate that basic blocks with a memory dependence have higher prediction errors than blocks without for all performance models in our evaluation. We therefore present a technique that clusters basic blocks based on their use of processor resources. The technique enables performance model designers and users to have a finer-grained understanding of the behavior of the performance model, enabling them to focus their development resources on whole categories of blocks that pose challenges. For example, a data-driven performance model such as Ithemal [11] could be steered to oversample categories of basic blocks that it poorly models.\n\n\nA. Methodology\n\nThe approach we took to cluster basic blocks is as follows: 1) we map each basic block to a representation that reflects its use of hardware resources; we then 2) cluster the basic blocks based on this representation.\n\nWe compute a port-combination mapping for each instruction, using results from Abel and Reineke [18]. For instance, the port-combination mapping for xor %rax, %rbx in Haswell is {p0156 \u2192 1} (using Abel and Reineke's notation): in other words, this instruction is implemented with a single micro-op that can be executed at any of ports 0, 1, 5, or 6. In Haswell, there are 13 unique port combinations for all userlevel instructions.\n\nAfter translating each basic block to its port-mapping representation, we used Latent Dirichlet Allocation (LDA) [5] to build a topic model. A topic model in language processing assigns a topic to each word based on the word's frequency in a set of documents. In our context, words are microoperations differentiated based on their port combinations, topics are categories, and documents are basic blocks. We therefore assign a category to each micro-operation based on its port-combination frequency in a set of basic blocks. To infer the category for each micro-op, we used SciKit Learn's default implementation of stochastic variational inference for LDA [5], with six categories and the parameter values \u03b1 = 1/6 and \u03b2 = 1/13. We then computed the most common category for each basic block, taking this to be the category of the basic block itself.  Table II presents the number of unique basic blocks in each category discovered by our technique. Note that LDA does not give names or descriptions to the categories; we manually named the categories and developed their descriptions after inspecting their constituent basic blocks. Table III shows an example basic block for each category. Figure 3 shows the unique basic block category patterns for each application; we weigh each basic block by its runtime frequency (which is determined by a sample-based profiler [26]). As expected, applications with high-performance numerical kernels such as TensorFlow and OpenBLAS are composed primarily of vectorized basic blocks. The majority of SQLite and LLVM's basic blocks are not vectorized. OpenSSL and Gzip have many bit-manipulation basic blocks, consistent with our analysis.\n\n\nB. Results\n\n\nC. Case Study on Data-Center Applications\n\nWe next demonstrate that our clustering generalizes to unseen basic blocks, providing similar explanatory power for two data-center applications that are not within our dataset: Google's Spanner [6] and Dremel [7]. The key observation is that if our clusters generalize to these new applications, then our dataset captures properties of unseen workloads.\n\nMethodology. Spanner is a highly available, globally distributed database, designed to scale up to millions of machines and trillions of database rows [6]. Dremel is a scalable, interactive ad-hoc query system for analysis of read-only data, capable of running aggregation queries over trillion-row tables in seconds [7].\n\nTo evaluate our clustering's ability to generalize to unseen basic blocks, we use the following experiment. First, we assigned each basic block in Spanner and Dremel to the most appropriate cluster from our benchmark suite of open source applications (Section V). We performed this assignment using Blei et al. [5]'s unseen data classification technique as implemented in Scikit Learn's transform method. Second, we created a new set of clusters from the combination of our open source applications and Spanner and Dremel. Finally, we compared the perplexity [5] of each clustering on the Spanner and Dremel basic blocks. Perplexity is a measurement of how successfully LDA can discriminate between different data points in a data set, with lower being more successful. Therefore, our experimental hypothesis is that if the perplexity of our open source data clustering is the same as that of our combined data clustering, then our open source clustering generalizes because the Spanner and Dremel basic blocks are well captured by the clusters in our open source applications. Results. We found that, when trained just on the open source data, our clustering achieved a perplexity of 5.23, versus a perplexity of 5.26 for the clustering additionally trained with the Google data. This small increase of 0.5% shows that the perplexity of the clusterings are nearly the same, and that the basic blocks in the open-source data are diverse enough to cover previously unseen applications. Figure 4 shows Spanner's and Dremel's basic block categories inferred by the clustering from our benchmark dataset (Section V). The two applications spend almost half of the time executing load instructions-40% and 50% respectively. Compared to similar applications from our data set, Spanner and Dremel spend significantly more time-12% and 9%, respectively-executing vectorized basic blocks, compared to 5.0% and 0.2% for SQLite and Redis respectively.  VI. PERFORMANCE MODEL EVALUATION We evaluate four existing basic block-level performance models on three recent Intel microarchitectures: Ivy Bridge, Haswell, and Skylake.\n\n\nA. Methodology\n\nWe evaluated IACA [8], llvm-mca [9], Ithemal [11], and OSACA [10]. In Section II-A, we present a more detailed discussion regarding their design and usage. For Skylake and Haswell, we used IACA 3.0; and for Ivy Bridge we used IACA 2.0 because IACA discontinued its support for Ivy Bridge after version 2.0. For llvm-mca, we used version 8.0.1. OSACA is currently under development, and we took the latest version at the time of writing this paper. For Ithemal 5 , we used commit cb5fd79f6 from the master branch and retrained its model using the dataset described in [11], but timed with our profiler. We note that only 22% of the basic blocks in our dataset appear in Ithemal's dataset. Therefore our dataset is a valid test because it has limited exposure to the risk that Ithemal overfits on its own dataset.\n\nDataset. We evaluated the performance models using the basic block dataset discussed in Section IV. Some basic blocks in our dataset contain AVX2 instructions, which are not implemented in Ivy Bridge. These basic blocks are not included in the Ivy Bridge evaluation.\n\nPlatforms. For the Ivy Bridge measurements, we used a server with the Intel(R) Xeon(R) CPU E5-2695 v2 CPU and 128 GB of memory. For the Haswell measurements, we used a server with the Intel(R) Xeon(R) CPU E5-2680 v3 CPU and 128 GB of memory. Finally, for the Skylake measurements, we used a server with the Intel(R) Xeon(R) W-2123 CPU and 64 GB of memory.\n\nEvaluation Metrics. Given a measured throughput t and a predicted throughput t , we define error in this paper as the relative error: err(t, t ) = t\u2212t t . We report the prediction errors of the performance models in three different ways.\n\n\u2022 Overall error: We report the average error of all basic blocks in our dataset on a given microarchitecture. \u2022 Per-application error: We report the average error of all basic blocks in a given application on a given microarchitecture. \u2022 Per-category error: We cluster the basic blocks in our dataset into different categories (Section V) and report the average error of all basic blocks in a given category on a given microarchitecture.\n\nWe additionally evaluated how each model preserves the ordering of basic block throughput, using Kendall's tau coefficient [27], which measures the fraction of preserved pairwise ordering. Kendall's tau coefficient can be more useful than relative error, when the consumers of a performance model are not so much concerned with the precise predictions but the relative ordering of predictions-such as compiler optimizations that needs to select the best of candidate programs. Table IV shows the overall error and Kendall's tau coefficients for each model on different microarchitectures. Figures  5, 7, and 9 show the breakdown of the errors by application. Figure 6, 8, and 10 show the breakdown of the errors by category. The discrepancy between overall prediction error ( Table  IV) and that of more specialized types of basic blocks such as vectorized code (Figures 6, 8, and 10) highlights the need of basic block clustering and per-category error reporting. IACA is the second most accurate model for most categories. It is the best model at modeling purely vectorized basic blocks. In its weakest categories-Scalar/Vec and Ld/St-it is no worse than the most accurate model by more than 10%.\n\n\nB. Results\n\nllvm-mca is considerably worse on all microarchitectures compared to IACA and Ithemal, especially on modeling basic blocks involving loads.\n\nIthemal [11] consistently outperforms other models, except on vectorized basic blocks. Ithemal is also considerably better than other models at modeling basic blocks with memory dependence (Ld/St). The inconsistency between Ithemal's overall error and the error on vectorized basic blocks seems to be a result of an imbalance in its training dataset, the majority of which consists of non-vectorized basic blocks.\n\nOSACA [10] is, on average, more accurate than llvm-mca but considerably less than IACA [8] and Ithemal [11]. We note that this has less to do with OSACA's methodology than the fact that its instruction parser is under active development. During our evaluations, we found and reported five bugs in OSACA's instruction parser. In particular, OSACA does not recognize several instruction forms; depending on the cases, it either crashes or treats unrecognized instruction forms as nops. One such instruction form is any instruction that reads an immediate operand and writes to memory (e.g., add $1, (%rbx)). OSACA treats these instructions as nops, thus underreporting the throughput of many basic blocks.\n\n\nC. Examples of Modeling Bugs\n\nWe present a case study of three basic blocks where some of the evaluated models behave poorly. These examples showcase the microarchitectural complexity that these performance models face. Table V shows these basic blocks, their measured throughput (Haswell), and the throughput predicted by different models (Haswell). We manually inspected IACA's and llvm-mca's predicted instruction schedules; OSACA and Ithemal do not predict execution schedules. The first two examples highlight cases in which a model's prediction contradicts the instruction throughput specified by the manual. The last example shows a case where a model (llvm-mca [9] in this case) can significantly mispredict throughput due to mis-scheduling micro-ops, despite correct modeling of individual instructions.\n\nModeling bug due to unsigned division. The first example in Table V is bottlenecked by a 64-bit by 32-bit unsigned division. The throughput is bounded by the latency of the division (since it reads from and writes to %rax each iteration). Intel's manual [28] states that the latency of such a division ranges from 20 to 26 cycles, which is consistent with our measurement (21.62 cycles).\n\nAll models are wrong here. llvm-mca and IACA [8] significantly over-predict; Ithemal [11] and OSACA [10] underpredict-it appears that they ignore the data-dependence.\n\nOne can infer from llvm-mca's and IACA's predictions-98 and 99 cycles-that they mistake this 64-by-32 division (latency \u2248 12 cycles) with the 128-by-64 analog (latency \u2248 90 cycles). Their predictions would, however, still be wrong due to the preceding xorl %edx, %edx, which zeros %edx and enables a fastpath for the subsequent division (latency \u2248 30 cycles).\n\nModeling bug due to zero-idioms. The basic block in the second example in Table V is a single vectorized XOR of %xmm2 with itself. At the end of executing this instruction, the value of %xmm2 is always 0. All three microarchitectures have fast-paths for such zero-idioms. IACA and Ithemal recognize this idiom and make predictions close to the measured throughput, while llvm-mca and OSACA treat this instruction as a regular vectorized XOR.\n\nModeling bug due to mis-scheduling. The third example in Table V is a basic block whose instructions form a non-trivial dependence-chain. On the surface, every instruction depends on the previous one, but there is still some instruction-level parallelism because the memory addressed by %rcx in the third instruction does not depend on the rest of the computation (in fact, it is constant). IACA is the most accurate model. Ithemal's and OS-ACA's predictions-1 cycle per iteration-reveal that they ignored the dependence altogether. Figure 11 shows the schedules predicted by llvm-mca and IACA. The instruction xorq (%rcx), %rax is dispatched noticeably earlier in IACA's schedule. llvm-mca delays dispatching xorq due to its dependence on the previous computation, not recognizing that that the micro-op of xorq (%rcx), %rax that loads %rcx is independent and should be dispatched earlier.  . Schedules predicted by llvm-mca and IACA for an example basic block. Each red window marks the boundary of a single iteration of execution. The width of the windows represents the steady-state throughput.\n\nAs illustrated here, llvm-mca and IACA predicts two different schedules.\n\nNotice that the bolded instruction is dispatched earlier in IACA's schedule.\n\nVII. CONCLUSION We present a benchmark for validating performance models of x86-64 basic blocks. We describe the techniques with which we collected and profiled over 300,000 basic blocks from real-world applications. Our benchmark can be used to evaluate and tune performance models of x86-64 basic blocks systematically. We evaluated four throughput prediction models, including two recently published research models. Our evaluation shows that even the best models we evaluated can differ from the ground truth by more than 20%, and sometimes more, for certain classes of basic blocks; in particular, we show that existing throughput predictors have difficulty modeling memory dependency and vectorized basic blocks reliably.\n\n\nVIII. ACKNOWLEDGEMENTS\n\nWe thank Stephen Chou and all reviewers for their insightful comments. This work was supported by a Google Faculty Research Award and the Applications Driving Architectures (ADA) Research Center, a JUMP Center co-sponsored by SRC (Semiconductor Research Corporation) and DARPA. Table VI shows the accuracy of IACA, llvm-mca, and Ithemal's prediction errors on these basic blocks. Figure 12 shows the per-category error analysis. We gather the validation measurements on a Haswell machine. IACA's predictions are similar to those on our dataset. Ithemal's predictions on the two applications' vectorized basic blocks are 3% better than on our dataset, but its predictions on basic blocks in the Scalar and Ld/St categories are 7% worse. llvm-mca, which has an average prediction error of 25% on our benchmark, predicts noticeably better; we note that llvm-mca still lags behind the other models on vectorized basic blocks. \n\n\nAPPENDIX A MODEL PREDICTION ERRORS ON SPANNER AND DREMEL\n\nFig. 2 .\n2Pseudocode of the profiling routines.\n\nFig. 3 .\n3Breakdown of applications by basic block categories. The Y-axis is the total runtime frequency of the basic blocks in a given category (specified on the X-axis). We define the category of a basic block as the most common category of micro-ops contained in the block.\n\nFig. 4 .\n4Basic block composition of Spanner and Dremel. Each basic block is weighted by its execution frequency.\n\nFig. 5 .\n5Per-application error for each model on Ivy Bridge Fig. 6. Per-category error for each model on Ivy Bridge Fig. 7. Per-application error for each model on Haswell Fig. 8. Per-category error for each model on Haswell Fig. 9. Per-application error for each model on Skylake Fig. 10. Per-category error for each model on Skylake\n\nA\n------------------------sdeeeew----R-------p A------------------------------sdw----R-------p A-------------------------------w-----R-------ps---cdeeeew---------------------------R-------p A-------------------------------sdw----R-------pA-------------------------------sdeeeew----R-----(%rax), %rbx movq %rbx, %rax xorq (%rcx), %rax xorq 1000000(%rax), %rbxFig. 11\n\n\nFig. 1. The inner loop of updcrc from Gzip. This basic block cannot be directly executed because of its memory accesses.add $1, %rdi \nmov %edx, %eax \nshr $8, %rdx \nxor -1(%rdi), %al \nmovzx %al, %eax \nxor 0x4110a(, %rax, 8), %rdx \ncmp %rcx, %rdi \n\nfunction monitor \nnumF aults \u2190 0 \nmappedP ages \u2190 \u2205 \nwhile numF aults < maxN umF aults do \npid \u2190 launch(measure(mappedP ages)) \nif exitSuccess(pid) then \nbreak \nend if \nmemAddr \u2190 interceptSegF ault(pid) \nif isM appableAddr(memAddr) then \nmappedP ages.add(getP ageAddr(memAddr)) \nnumF aults \u2190 numF aults + 1 \nend if \nend while \nend function \nfunction measure(pagesT oM ap) \nmmapT oP hysP age(pagesT oM ap, ...) \ninitialize \n\nWait for preceding instructions to finish \nserialize \nbegin \u2190 readP erf ormanceCounters \ninitialize \nexecuteU nrolledBasicBlock \n\nWait for the basic block to finish \nserialize \nend \u2190 readP erf ormanceCounters \nanalyzeAndReportCounters(begin, end) \nend function \n\n\n\nTABLE I\nISource applications of BHive's basic blocks. Note that the basic blocks collected from each application are not necessarily unique. For instance, most applications use the same libc implementation.Application \nDomain \n# Basic Blocks \n\nOpenBLAS \nScientific Computing \n19032 \nRedis \nDatabase \n9343 \nSQLite \nDatabase \n8871 \nGZip \nCompression \n2272 \nTensorFlow \nMachine Learning \n71988 \nClang/LLVM \nCompiler \n212758 \nEigen \nScientific Computing \n4545 \nEmbree \nRay Tracing \n12602 \nFFmpeg \nMultimedia \n17150 \nTotal \n330018 \n\n\n\nTABLE II\nIIDescription of basic block categories. Categories were discovered by LDA, and category names are decided by manual inspection.Category Description \n# Basic Blocks \n\nScalar \nScalar ALU operations \n85208 \nVec \nPurely vector instructions \n1267 \nScalar/Vec Scalar and vector arithmetic \n7710 \nLd \nMostly loads \n121412 \nSt \nMostly stores \n55879 \nLd/St \nMix of loads and stores \n58540 \n\n\n\nTABLE III Example\nIIIbasic blocks for each category identified inTable IIScalar \nVec \n\nmovzbl 2(%rdi), %eax \nshrb $2, %al \nandl $31, %eax \ncmpl $1, %eax \n\nvmovss \n-4(%rax), %xmm2 \nvmovaps %xmm2, %xmm3 \nvmovss \n(%rax), %xmm1 \nvxorps \n%xmm4, %xmm3, \n%xmm3 \nvucomiss %xmm3, %xmm1 \n\nScalar/Vec \nLd \n\nmovsd \n(%rcx), %xmm1 \nmovsd \n(%rsi), %xmm0 \nmovaps %xmm1, %xmm2 \nmovaps %xmm0, %xmm3 \nmulps \n%xmm14, %xmm0 \n... \naddps \n%xmm1, %xmm0 \nsubps \n%xmm3, %xmm2 \nmovlps %xmm0, (%rsi) \nmovlps %xmm2, (%rcx) \naddq \n$8, %rsi \naddq \n$8, %rcx \nsubq \n$2, %rdi \n\nmovq (%rbp), %rax \nmovq %rbx, %rsi \nmovq %rbp, %rdi \npopq %rbx \npopq %rbp \npopq %r12 \nmovq 32(%rax), %rax \n\nSt \nLd/St \n\npushq %r14 \nmovq %rdi, %r14 \nleaq 8(%rdi), %rdi \npushq %r13 \npushq %r12 \npushq %rbp \npushq %rbx \n\nmovq (%rbp), %rax \nmovq %rbx, %rsi \nmovq %rbp, %rdi \npopq %rbx \npopq %rbp \npopq %r12 \nmovq 32(%rax), %rax \n\n\n\nTABLE IV Overall\nIVerror of evaluated models. Kendall's tau coefficient[27] measures the fraction of pairwise throughput ordering preserved by a given model.The bolded entry of each cell shows the best model.Microarchitecture \nModel \nAverage Error Kendall's Tau \n\nIvy Bridge \nIACA \n0.1664 \n0.8000 \nllvm-mca \n0.2813 \n0.7544 \nIthemal \n0.0973 \n0.8442 \nOSACA \n0.3299 \n0.6197 \n\nHaswell \nIACA \n0.1790 \n0.8043 \nllvm-mca \n0.2511 \n0.7829 \nIthemal \n0.0926 \n0.8544 \nOSACA \n0.3566 \n0.6067 \n\nSkylake \nIACA \n0.1566 \n0.8121 \nllvm-mca \n0.2683 \n0.7745 \nIthemal \n0.0980 \n0.8516 \nOSACA \n0.3573 \n0.6111 \n\n\n\nTABLE V\nVBasic blocks and their predicted throughputs, demonstrating modeling bugs.Basic Block \nModel \nThroughput \n\nxorl %edx, %edx \ndivl %ecx \ntestl %edx, %edx \n\nIACA \n98.00 \nllvm-mca \n99.04 \nIthemal \n16.28 \nOSACA \n12.25 \nMeasured \n21.62 \n\nvxorps %xmm2, %xmm2, %xmm2 \n\nIACA \n0.24 \nllvm-mca \n1.00 \nIthemal \n0.27 \nOSACA \n1.00 \nMeasured \n0.25 \n\nxorq 1000000(%rax), %rbx \nmovq %rbx, %rax \nxorq (%rcx), %rax \n\nIACA \n7.83 \nllvm-mca \n12.03 \nIthemal \n1.01 \nOSACA \n1.00 \nMeasured \n7.23 \n\n\n\nTABLE VI Accuracy\nVIof models on basic blocks from Spanner and Dremel. Bolded entries show the best models.Fig. 12. Per-category prediction error (vs. Haswell measurements) on Spanner and Dremel's basic blocks.Application \nModel \nAverage Error \nKendall's Tau \n\nSpanner \nIACA \n0.1892 \n0.7786 \nllvm-mca \n0.1764 \n0.7623 \nIthemal \n0.1276 \n0.7890 \n\nDremel \nIACA \n0.1883 \n0.7835 \nllvm-mca \n0.1777 \n0.7685 \nIthemal \n0.1316 \n0.7960 \n\n\nThe script uses several general-purpose registers for bookkeeping and assume that they are not used by the code that it's benchmarking. For a fair comparison, we modified the script so it could profile programs that use these registers.\nWe use IACA's definition of throughput: the average number of cycles required to execute a basic block at a steady state. Note that this definition is the reciprocal of the textbook definition of throughput.\nA typical unroll factor is 100[11,18] \nFor these applications, we use inputs from https://openbenchmarking.org\nhttps://github.com/ithemal/Ithemal\n\ngoslp: globally optimized superword level parallelism framework. C Mendis, S Amarasinghe, ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications. C. Mendis and S. Amarasinghe, \"goslp: globally opti- mized superword level parallelism framework,\" in ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications, 2018.\n\nPortable cost modeling for auto-vectorizers. A Pohl, B Cosenza, B Juurlink, IEEE International Symposium on the Modeling, Analysis, and Simulation of Computer and Telecommunication Systems. A. Pohl, B. Cosenza, and B. Juurlink, \"Portable cost modeling for auto-vectorizers,\" in IEEE International Symposium on the Modeling, Analysis, and Simulation of Computer and Telecommunication Systems, 2019.\n\nSoftware optimization resources. \"Software optimization resources,\" https://www.agner. org/optimize/, 2019.\n\nA tool for running small microbenchmarks on recent intel and amd x86 cpus. A Abel, A Wharton, A. Abel and A. Wharton, \"A tool for running small microbenchmarks on recent intel and amd x86 cpus.\" https://github.com/andreas-abel/nanoBench, 2019.\n\nLatent dirichlet allocation. D M Blei, A Y Ng, M I Jordan, Journal of machine Learning research. 3D. M. Blei, A. Y. Ng, and M. I. Jordan, \"Latent dirichlet allocation,\" Journal of machine Learning research, vol. 3, no. Jan, pp. 993-1022, 2003.\n\nSpanner: Google's globally-distributed database. J C Corbett, J Dean, M Epstein, A Fikes, C Frost, J Furman, S Ghemawat, A Gubarev, C Heiser, P Hochschild, W Hsieh, S Kanthak, E Kogan, H Li, A Lloyd, S Melnik, D Mwaura, D Nagle, S Quinlan, R Rao, L Rolig, D Woodford, Y Saito, C Taylor, M Szymaniak, R Wang, USENIX Conference on Operating Systems Design and Implementation. J. C. Corbett, J. Dean, M. Epstein, A. Fikes, C. Frost, J. Furman, S. Ghemawat, A. Gubarev, C. Heiser, P. Hochschild, W. Hsieh, S. Kanthak, E. Kogan, H. Li, A. Lloyd, S. Melnik, D. Mwaura, D. Nagle, S. Quin- lan, R. Rao, L. Rolig, D. Woodford, Y. Saito, C. Tay- lor, M. Szymaniak, and R. Wang, \"Spanner: Google's globally-distributed database,\" in USENIX Conference on Operating Systems Design and Implementation, 2012.\n\nDremel: Interactive analysis of web-scale datasets. S Melnik, A Gubarev, J J Long, G Romer, S Shivakumar, M Tolton, T Vassilakis, International Conference on Very Large Data Bases. S. Melnik, A. Gubarev, J. J. Long, G. Romer, S. Shivaku- mar, M. Tolton, and T. Vassilakis, \"Dremel: Interactive analysis of web-scale datasets,\" in International Confer- ence on Very Large Data Bases, 2010.\n\nIntel architecture code analyzer. \"Intel architecture code analyzer,\" https://software. intel.com/en-us/articles/intel-architecture-code-analyzer, 2017.\n\nllvm-mca -llvm machine code analyzer. \"llvm-mca -llvm machine code analyzer,\" https://llvm. org/docs/CommandGuide/llvm-mca.html.\n\nAutomated instruction stream throughput prediction for intel and amd microarchitectures. J Laukemann, J Hammer, J Hofmann, G Hager, G Wellein, IEEE/ACM Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems. J. Laukemann, J. Hammer, J. Hofmann, G. Hager, and G. Wellein, \"Automated instruction stream throughput prediction for intel and amd microarchitectures,\" in IEEE/ACM Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems, 2018.\n\nIthemal: Accurate, portable and fast basic block throughput estimation using deep neural networks. C Mendis, A Renda, D Amarasinghe, M Carbin, International Conference on Machine Learning. C. Mendis, A. Renda, D. Amarasinghe, and M. Carbin, \"Ithemal: Accurate, portable and fast basic block throughput estimation using deep neural networks,\" in International Conference on Machine Learning, 2019.\n\nLlvm: A compilation framework for lifelong program analysis & transformation. C Lattner, V Adve, International Symposium on Code Generation and Optimization. 15llvm::TargetRegisterInfo Class ReferenceC. Lattner and V. Adve, \"Llvm: A compilation frame- work for lifelong program analysis & transformation,\" [15] \"llvm::TargetRegisterInfo Class Reference,\" https://llvm. in International Symposium on Code Generation and Optimization, 2004.\n\nTargetTransformInfo Class Reference. \"llvm::TargetTransformInfo Class Reference,\" https: //llvm.org/doxygen/classllvm 1 1TargetTransformInfo. html.\n\nllvm::targetschedmodel class reference. \"llvm::targetschedmodel class reference,\" http://llvm.org/ doxygen/classllvm 1 1targetschedmodel.html. org/doxygen/classllvm 1 1TargetRegisterInfo.html.\n\n16 describing relative costs of operations. 18\"18.16 describing relative costs of operations,\" https:// gcc.gnu.org/onlinedocs/gccint/Costs.html.\n\n18.17 adjusting the instruction scheduler. \"18.17 adjusting the instruction scheduler,\" https://gcc. gnu.org/onlinedocs/gccint/Scheduling.html.\n\nuops.info: Characterizing latency, throughput, and port usage of instructions on intel microarchitectures. A Abel, J Reineke, ASPLOS. A. Abel and J. Reineke, \"uops.info: Characterizing la- tency, throughput, and port usage of instructions on intel microarchitectures,\" in ASPLOS, 2019.\n\nOpenssl, cryptography and ssl/tls toolkit. \"Openssl, cryptography and ssl/tls toolkit,\" https://www. openssl.org/.\n\nOpenblas, an optimized blas library. \"Openblas, an optimized blas library,\" https://www. openblas.net/.\n\nEigen. \"Eigen,\" http://eigen.tuxfamily.org/index.php?title= Main Page.\n\nTensorflow: A system for large-scale machine learning. M Abadi, P Barham, J Chen, Z Chen, A Davis, J Dean, M Devin, S Ghemawat, G Irving, M Isard, M Kudlur, J Levenberg, R Monga, S Moore, D G Murray, B Steiner, P Tucker, V Vasudevan, P Warden, M Wicke, Y Yu, X Zheng, USENIX Conference on Operating Systems Design and Implementation. M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng, \"Tensorflow: A system for large-scale machine learning,\" in USENIX Conference on Operating Systems Design and Implementation, 2016.\n\nEmbree: A kernel framework for efficient cpu ray tracing. I Wald, S Woop, C Benthin, G S Johnson, M Ernst, ACM Transactions on Graphics. I. Wald, S. Woop, C. Benthin, G. S. Johnson, and M. Ernst, \"Embree: A kernel framework for efficient cpu ray tracing,\" ACM Transactions on Graphics, 2014.\n\nispc: A spmd compiler for high-performance cpu programming. M Pharr, W R Mark, Innovative Parallel Computing. M. Pharr and W. R. Mark, \"ispc: A spmd compiler for high-performance cpu programming,\" in Innovative Parallel Computing, 2012.\n\nAn infrastructure for adaptive dynamic optimization. D Bruening, T Garnett, S Amarasinghe, International Symposium on Code Generation and Optimization. D. Bruening, T. Garnett, and S. Amarasinghe, \"An infras- tructure for adaptive dynamic optimization,\" in Interna- tional Symposium on Code Generation and Optimization, 2003.\n\nA portable sampling-based profiler for java virtual machines. J Whaley, ACM Conference on Java Grande. J. Whaley, \"A portable sampling-based profiler for java virtual machines,\" in ACM Conference on Java Grande, 2000.\n\nA new measure of rank correlation. M G Kendall, Biometrika. 301/2M. G. Kendall, \"A new measure of rank correlation,\" Biometrika, vol. 30, no. 1/2, pp. 81-93, 1938.\n\nIntel 64 and ia-32 architectures optimization reference manual. \"Intel 64 and ia-32 architectures optimiza- tion reference manual,\" https://software. intel.com/sites/default/files/managed/9e/bc/ 64-ia-32-architectures-optimization-manual.pdf.\n", "annotations": {"author": "[{\"end\":159,\"start\":107},{\"end\":236,\"start\":160},{\"end\":309,\"start\":237},{\"end\":375,\"start\":310},{\"end\":448,\"start\":376},{\"end\":481,\"start\":449},{\"end\":540,\"start\":482},{\"end\":612,\"start\":541}]", "publisher": null, "author_last_name": "[{\"end\":118,\"start\":114},{\"end\":180,\"start\":165},{\"end\":251,\"start\":245},{\"end\":320,\"start\":315},{\"end\":389,\"start\":381},{\"end\":462,\"start\":456},{\"end\":499,\"start\":488},{\"end\":555,\"start\":549}]", "author_first_name": "[{\"end\":113,\"start\":107},{\"end\":164,\"start\":160},{\"end\":244,\"start\":237},{\"end\":314,\"start\":310},{\"end\":380,\"start\":376},{\"end\":455,\"start\":449},{\"end\":487,\"start\":482},{\"end\":548,\"start\":541}]", "author_affiliation": "[{\"end\":158,\"start\":120},{\"end\":235,\"start\":197},{\"end\":308,\"start\":270},{\"end\":374,\"start\":336},{\"end\":447,\"start\":409},{\"end\":480,\"start\":464},{\"end\":539,\"start\":501},{\"end\":611,\"start\":573}]", "title": "[{\"end\":104,\"start\":1},{\"end\":716,\"start\":613}]", "venue": null, "abstract": "[{\"end\":2272,\"start\":799}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2773,\"start\":2770},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2957,\"start\":2954},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4205,\"start\":4202},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4207,\"start\":4205},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4503,\"start\":4500},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4563,\"start\":4562},{\"end\":5371,\"start\":5370},{\"end\":5629,\"start\":5628},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5726,\"start\":5723},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5924,\"start\":5921},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5939,\"start\":5936},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6074,\"start\":6071},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6088,\"start\":6085},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6174,\"start\":6170},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6192,\"start\":6188},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7040,\"start\":7037},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7165,\"start\":7162},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7666,\"start\":7663},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7847,\"start\":7843},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8076,\"start\":8072},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8318,\"start\":8314},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8617,\"start\":8613},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9043,\"start\":9039},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9102,\"start\":9098},{\"end\":9174,\"start\":9170},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9209,\"start\":9205},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9212,\"start\":9209},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9554,\"start\":9551},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9735,\"start\":9732},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9776,\"start\":9773},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10237,\"start\":10236},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11116,\"start\":11112},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20724,\"start\":20720},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21190,\"start\":21186},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21220,\"start\":21216},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21232,\"start\":21228},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21272,\"start\":21268},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21304,\"start\":21300},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21466,\"start\":21462},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21958,\"start\":21954},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22432,\"start\":22431},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23267,\"start\":23263},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23686,\"start\":23682},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24135,\"start\":24132},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24680,\"start\":24677},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25393,\"start\":25389},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25956,\"start\":25953},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25971,\"start\":25968},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26268,\"start\":26265},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26434,\"start\":26431},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26751,\"start\":26748},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26999,\"start\":26996},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28589,\"start\":28586},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28603,\"start\":28600},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28617,\"start\":28613},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28633,\"start\":28629},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29139,\"start\":29135},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30811,\"start\":30807},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32050,\"start\":32046},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32463,\"start\":32459},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":32543,\"start\":32540},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32560,\"start\":32556},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":33831,\"start\":33828},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":34231,\"start\":34227},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":34410,\"start\":34407},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":34451,\"start\":34447},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":34466,\"start\":34462},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":42276,\"start\":42272},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":44176,\"start\":44172},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":44179,\"start\":44176}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38370,\"start\":38322},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38648,\"start\":38371},{\"attributes\":{\"id\":\"fig_2\"},\"end\":38763,\"start\":38649},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39100,\"start\":38764},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39467,\"start\":39101},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":40403,\"start\":39468},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":40933,\"start\":40404},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":41327,\"start\":40934},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":42199,\"start\":41328},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":42786,\"start\":42200},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":43268,\"start\":42787},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":43696,\"start\":43269}]", "paragraph": "[{\"end\":3188,\"start\":2291},{\"end\":3536,\"start\":3190},{\"end\":4658,\"start\":3580},{\"end\":4998,\"start\":4679},{\"end\":6418,\"start\":5000},{\"end\":6731,\"start\":6420},{\"end\":7155,\"start\":6783},{\"end\":8064,\"start\":7157},{\"end\":8304,\"start\":8066},{\"end\":8577,\"start\":8306},{\"end\":9392,\"start\":8579},{\"end\":10038,\"start\":9422},{\"end\":10728,\"start\":10040},{\"end\":11093,\"start\":10763},{\"end\":11245,\"start\":11095},{\"end\":11644,\"start\":11300},{\"end\":12078,\"start\":11646},{\"end\":12427,\"start\":12120},{\"end\":12634,\"start\":12469},{\"end\":13623,\"start\":12636},{\"end\":14207,\"start\":13625},{\"end\":14809,\"start\":14209},{\"end\":15881,\"start\":14811},{\"end\":16121,\"start\":15915},{\"end\":16591,\"start\":16123},{\"end\":16880,\"start\":16593},{\"end\":18398,\"start\":16882},{\"end\":19111,\"start\":18400},{\"end\":19499,\"start\":19153},{\"end\":20169,\"start\":19501},{\"end\":20695,\"start\":20171},{\"end\":21085,\"start\":20697},{\"end\":21551,\"start\":21087},{\"end\":21852,\"start\":21553},{\"end\":22582,\"start\":21854},{\"end\":23348,\"start\":22612},{\"end\":23584,\"start\":23367},{\"end\":24017,\"start\":23586},{\"end\":25699,\"start\":24019},{\"end\":26112,\"start\":25758},{\"end\":26435,\"start\":26114},{\"end\":28549,\"start\":26437},{\"end\":29379,\"start\":28568},{\"end\":29647,\"start\":29381},{\"end\":30004,\"start\":29649},{\"end\":30243,\"start\":30006},{\"end\":30682,\"start\":30245},{\"end\":31882,\"start\":30684},{\"end\":32036,\"start\":31897},{\"end\":32451,\"start\":32038},{\"end\":33156,\"start\":32453},{\"end\":33971,\"start\":33189},{\"end\":34360,\"start\":33973},{\"end\":34528,\"start\":34362},{\"end\":34889,\"start\":34530},{\"end\":35332,\"start\":34891},{\"end\":36432,\"start\":35334},{\"end\":36506,\"start\":36434},{\"end\":36584,\"start\":36508},{\"end\":37313,\"start\":36586},{\"end\":38262,\"start\":37340}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10762,\"start\":10729},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11299,\"start\":11246}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20202,\"start\":20195},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24880,\"start\":24872},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25163,\"start\":25154},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31169,\"start\":31161},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31470,\"start\":31460},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":33386,\"start\":33379},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":34040,\"start\":34033},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":34972,\"start\":34965},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":35398,\"start\":35391},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":37626,\"start\":37618}]", "section_header": "[{\"end\":2289,\"start\":2274},{\"end\":3578,\"start\":3539},{\"end\":4677,\"start\":4661},{\"end\":6748,\"start\":6734},{\"end\":6781,\"start\":6751},{\"end\":9420,\"start\":9395},{\"end\":12118,\"start\":12081},{\"end\":12467,\"start\":12430},{\"end\":15913,\"start\":15884},{\"end\":19151,\"start\":19114},{\"end\":22610,\"start\":22585},{\"end\":23365,\"start\":23351},{\"end\":25712,\"start\":25702},{\"end\":25756,\"start\":25715},{\"end\":28566,\"start\":28552},{\"end\":31895,\"start\":31885},{\"end\":33187,\"start\":33159},{\"end\":37338,\"start\":37316},{\"end\":38321,\"start\":38265},{\"end\":38331,\"start\":38323},{\"end\":38380,\"start\":38372},{\"end\":38658,\"start\":38650},{\"end\":38773,\"start\":38765},{\"end\":39103,\"start\":39102},{\"end\":40412,\"start\":40405},{\"end\":40943,\"start\":40935},{\"end\":41346,\"start\":41329},{\"end\":42217,\"start\":42201},{\"end\":42795,\"start\":42788},{\"end\":43287,\"start\":43270}]", "table": "[{\"end\":40403,\"start\":39590},{\"end\":40933,\"start\":40611},{\"end\":41327,\"start\":41072},{\"end\":42199,\"start\":41402},{\"end\":42786,\"start\":42409},{\"end\":43268,\"start\":42871},{\"end\":43696,\"start\":43480}]", "figure_caption": "[{\"end\":38370,\"start\":38333},{\"end\":38648,\"start\":38382},{\"end\":38763,\"start\":38660},{\"end\":39100,\"start\":38775},{\"end\":39467,\"start\":39104},{\"end\":39590,\"start\":39470},{\"end\":40611,\"start\":40414},{\"end\":41072,\"start\":40946},{\"end\":41402,\"start\":41350},{\"end\":42409,\"start\":42220},{\"end\":42871,\"start\":42797},{\"end\":43480,\"start\":43290}]", "figure_ref": "[{\"end\":12672,\"start\":12664},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13658,\"start\":13650},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16278,\"start\":16270},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25220,\"start\":25212},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27930,\"start\":27922},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31293,\"start\":31273},{\"end\":31351,\"start\":31343},{\"end\":31567,\"start\":31546},{\"end\":35876,\"start\":35867},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37729,\"start\":37720}]", "bib_author_first_name": "[{\"end\":44355,\"start\":44354},{\"end\":44365,\"start\":44364},{\"end\":44748,\"start\":44747},{\"end\":44756,\"start\":44755},{\"end\":44767,\"start\":44766},{\"end\":45286,\"start\":45285},{\"end\":45294,\"start\":45293},{\"end\":45485,\"start\":45484},{\"end\":45487,\"start\":45486},{\"end\":45495,\"start\":45494},{\"end\":45497,\"start\":45496},{\"end\":45503,\"start\":45502},{\"end\":45505,\"start\":45504},{\"end\":45750,\"start\":45749},{\"end\":45752,\"start\":45751},{\"end\":45763,\"start\":45762},{\"end\":45771,\"start\":45770},{\"end\":45782,\"start\":45781},{\"end\":45791,\"start\":45790},{\"end\":45800,\"start\":45799},{\"end\":45810,\"start\":45809},{\"end\":45822,\"start\":45821},{\"end\":45833,\"start\":45832},{\"end\":45843,\"start\":45842},{\"end\":45857,\"start\":45856},{\"end\":45866,\"start\":45865},{\"end\":45877,\"start\":45876},{\"end\":45886,\"start\":45885},{\"end\":45892,\"start\":45891},{\"end\":45901,\"start\":45900},{\"end\":45911,\"start\":45910},{\"end\":45921,\"start\":45920},{\"end\":45930,\"start\":45929},{\"end\":45941,\"start\":45940},{\"end\":45948,\"start\":45947},{\"end\":45957,\"start\":45956},{\"end\":45969,\"start\":45968},{\"end\":45978,\"start\":45977},{\"end\":45988,\"start\":45987},{\"end\":46001,\"start\":46000},{\"end\":46548,\"start\":46547},{\"end\":46558,\"start\":46557},{\"end\":46569,\"start\":46568},{\"end\":46571,\"start\":46570},{\"end\":46579,\"start\":46578},{\"end\":46588,\"start\":46587},{\"end\":46602,\"start\":46601},{\"end\":46612,\"start\":46611},{\"end\":47259,\"start\":47258},{\"end\":47272,\"start\":47271},{\"end\":47282,\"start\":47281},{\"end\":47293,\"start\":47292},{\"end\":47302,\"start\":47301},{\"end\":47770,\"start\":47769},{\"end\":47780,\"start\":47779},{\"end\":47789,\"start\":47788},{\"end\":47804,\"start\":47803},{\"end\":48147,\"start\":48146},{\"end\":48158,\"start\":48157},{\"end\":49251,\"start\":49250},{\"end\":49259,\"start\":49258},{\"end\":49779,\"start\":49778},{\"end\":49788,\"start\":49787},{\"end\":49798,\"start\":49797},{\"end\":49806,\"start\":49805},{\"end\":49814,\"start\":49813},{\"end\":49823,\"start\":49822},{\"end\":49831,\"start\":49830},{\"end\":49840,\"start\":49839},{\"end\":49852,\"start\":49851},{\"end\":49862,\"start\":49861},{\"end\":49871,\"start\":49870},{\"end\":49881,\"start\":49880},{\"end\":49894,\"start\":49893},{\"end\":49903,\"start\":49902},{\"end\":49912,\"start\":49911},{\"end\":49914,\"start\":49913},{\"end\":49924,\"start\":49923},{\"end\":49935,\"start\":49934},{\"end\":49945,\"start\":49944},{\"end\":49958,\"start\":49957},{\"end\":49968,\"start\":49967},{\"end\":49977,\"start\":49976},{\"end\":49983,\"start\":49982},{\"end\":50489,\"start\":50488},{\"end\":50497,\"start\":50496},{\"end\":50505,\"start\":50504},{\"end\":50516,\"start\":50515},{\"end\":50518,\"start\":50517},{\"end\":50529,\"start\":50528},{\"end\":50784,\"start\":50783},{\"end\":50793,\"start\":50792},{\"end\":50795,\"start\":50794},{\"end\":51015,\"start\":51014},{\"end\":51027,\"start\":51026},{\"end\":51038,\"start\":51037},{\"end\":51351,\"start\":51350},{\"end\":51543,\"start\":51542},{\"end\":51545,\"start\":51544}]", "bib_author_last_name": "[{\"end\":44362,\"start\":44356},{\"end\":44377,\"start\":44366},{\"end\":44753,\"start\":44749},{\"end\":44764,\"start\":44757},{\"end\":44776,\"start\":44768},{\"end\":45291,\"start\":45287},{\"end\":45302,\"start\":45295},{\"end\":45492,\"start\":45488},{\"end\":45500,\"start\":45498},{\"end\":45512,\"start\":45506},{\"end\":45760,\"start\":45753},{\"end\":45768,\"start\":45764},{\"end\":45779,\"start\":45772},{\"end\":45788,\"start\":45783},{\"end\":45797,\"start\":45792},{\"end\":45807,\"start\":45801},{\"end\":45819,\"start\":45811},{\"end\":45830,\"start\":45823},{\"end\":45840,\"start\":45834},{\"end\":45854,\"start\":45844},{\"end\":45863,\"start\":45858},{\"end\":45874,\"start\":45867},{\"end\":45883,\"start\":45878},{\"end\":45889,\"start\":45887},{\"end\":45898,\"start\":45893},{\"end\":45908,\"start\":45902},{\"end\":45918,\"start\":45912},{\"end\":45927,\"start\":45922},{\"end\":45938,\"start\":45931},{\"end\":45945,\"start\":45942},{\"end\":45954,\"start\":45949},{\"end\":45966,\"start\":45958},{\"end\":45975,\"start\":45970},{\"end\":45985,\"start\":45979},{\"end\":45998,\"start\":45989},{\"end\":46006,\"start\":46002},{\"end\":46555,\"start\":46549},{\"end\":46566,\"start\":46559},{\"end\":46576,\"start\":46572},{\"end\":46585,\"start\":46580},{\"end\":46599,\"start\":46589},{\"end\":46609,\"start\":46603},{\"end\":46623,\"start\":46613},{\"end\":47269,\"start\":47260},{\"end\":47279,\"start\":47273},{\"end\":47290,\"start\":47283},{\"end\":47299,\"start\":47294},{\"end\":47310,\"start\":47303},{\"end\":47777,\"start\":47771},{\"end\":47786,\"start\":47781},{\"end\":47801,\"start\":47790},{\"end\":47811,\"start\":47805},{\"end\":48155,\"start\":48148},{\"end\":48163,\"start\":48159},{\"end\":49256,\"start\":49252},{\"end\":49267,\"start\":49260},{\"end\":49785,\"start\":49780},{\"end\":49795,\"start\":49789},{\"end\":49803,\"start\":49799},{\"end\":49811,\"start\":49807},{\"end\":49820,\"start\":49815},{\"end\":49828,\"start\":49824},{\"end\":49837,\"start\":49832},{\"end\":49849,\"start\":49841},{\"end\":49859,\"start\":49853},{\"end\":49868,\"start\":49863},{\"end\":49878,\"start\":49872},{\"end\":49891,\"start\":49882},{\"end\":49900,\"start\":49895},{\"end\":49909,\"start\":49904},{\"end\":49921,\"start\":49915},{\"end\":49932,\"start\":49925},{\"end\":49942,\"start\":49936},{\"end\":49955,\"start\":49946},{\"end\":49965,\"start\":49959},{\"end\":49974,\"start\":49969},{\"end\":49980,\"start\":49978},{\"end\":49989,\"start\":49984},{\"end\":50494,\"start\":50490},{\"end\":50502,\"start\":50498},{\"end\":50513,\"start\":50506},{\"end\":50526,\"start\":50519},{\"end\":50535,\"start\":50530},{\"end\":50790,\"start\":50785},{\"end\":50800,\"start\":50796},{\"end\":51024,\"start\":51016},{\"end\":51035,\"start\":51028},{\"end\":51050,\"start\":51039},{\"end\":51358,\"start\":51352},{\"end\":51553,\"start\":51546}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":13744421},\"end\":44700,\"start\":44289},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":204075247},\"end\":45099,\"start\":44702},{\"attributes\":{\"id\":\"b2\"},\"end\":45208,\"start\":45101},{\"attributes\":{\"id\":\"b3\"},\"end\":45453,\"start\":45210},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3177797},\"end\":45698,\"start\":45455},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1221800},\"end\":46493,\"start\":45700},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1720268},\"end\":46883,\"start\":46495},{\"attributes\":{\"id\":\"b7\"},\"end\":47037,\"start\":46885},{\"attributes\":{\"id\":\"b8\"},\"end\":47167,\"start\":47039},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":52156920},\"end\":47668,\"start\":47169},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":52070027},\"end\":48066,\"start\":47670},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":978769},\"end\":48506,\"start\":48068},{\"attributes\":{\"id\":\"b12\"},\"end\":48655,\"start\":48508},{\"attributes\":{\"id\":\"b13\"},\"end\":48849,\"start\":48657},{\"attributes\":{\"id\":\"b14\"},\"end\":48996,\"start\":48851},{\"attributes\":{\"id\":\"b15\"},\"end\":49141,\"start\":48998},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":52955537},\"end\":49428,\"start\":49143},{\"attributes\":{\"id\":\"b17\"},\"end\":49544,\"start\":49430},{\"attributes\":{\"id\":\"b18\"},\"end\":49649,\"start\":49546},{\"attributes\":{\"id\":\"b19\"},\"end\":49721,\"start\":49651},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6287870},\"end\":50428,\"start\":49723},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5913260},\"end\":50721,\"start\":50430},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7961799},\"end\":50959,\"start\":50723},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":5608511},\"end\":51286,\"start\":50961},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2068990},\"end\":51505,\"start\":51288},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":120478295},\"end\":51670,\"start\":51507},{\"attributes\":{\"id\":\"b26\"},\"end\":51914,\"start\":51672}]", "bib_title": "[{\"end\":44352,\"start\":44289},{\"end\":44745,\"start\":44702},{\"end\":45482,\"start\":45455},{\"end\":45747,\"start\":45700},{\"end\":46545,\"start\":46495},{\"end\":47256,\"start\":47169},{\"end\":47767,\"start\":47670},{\"end\":48144,\"start\":48068},{\"end\":49248,\"start\":49143},{\"end\":49776,\"start\":49723},{\"end\":50486,\"start\":50430},{\"end\":50781,\"start\":50723},{\"end\":51012,\"start\":50961},{\"end\":51348,\"start\":51288},{\"end\":51540,\"start\":51507}]", "bib_author": "[{\"end\":44364,\"start\":44354},{\"end\":44379,\"start\":44364},{\"end\":44755,\"start\":44747},{\"end\":44766,\"start\":44755},{\"end\":44778,\"start\":44766},{\"end\":45293,\"start\":45285},{\"end\":45304,\"start\":45293},{\"end\":45494,\"start\":45484},{\"end\":45502,\"start\":45494},{\"end\":45514,\"start\":45502},{\"end\":45762,\"start\":45749},{\"end\":45770,\"start\":45762},{\"end\":45781,\"start\":45770},{\"end\":45790,\"start\":45781},{\"end\":45799,\"start\":45790},{\"end\":45809,\"start\":45799},{\"end\":45821,\"start\":45809},{\"end\":45832,\"start\":45821},{\"end\":45842,\"start\":45832},{\"end\":45856,\"start\":45842},{\"end\":45865,\"start\":45856},{\"end\":45876,\"start\":45865},{\"end\":45885,\"start\":45876},{\"end\":45891,\"start\":45885},{\"end\":45900,\"start\":45891},{\"end\":45910,\"start\":45900},{\"end\":45920,\"start\":45910},{\"end\":45929,\"start\":45920},{\"end\":45940,\"start\":45929},{\"end\":45947,\"start\":45940},{\"end\":45956,\"start\":45947},{\"end\":45968,\"start\":45956},{\"end\":45977,\"start\":45968},{\"end\":45987,\"start\":45977},{\"end\":46000,\"start\":45987},{\"end\":46008,\"start\":46000},{\"end\":46557,\"start\":46547},{\"end\":46568,\"start\":46557},{\"end\":46578,\"start\":46568},{\"end\":46587,\"start\":46578},{\"end\":46601,\"start\":46587},{\"end\":46611,\"start\":46601},{\"end\":46625,\"start\":46611},{\"end\":47271,\"start\":47258},{\"end\":47281,\"start\":47271},{\"end\":47292,\"start\":47281},{\"end\":47301,\"start\":47292},{\"end\":47312,\"start\":47301},{\"end\":47779,\"start\":47769},{\"end\":47788,\"start\":47779},{\"end\":47803,\"start\":47788},{\"end\":47813,\"start\":47803},{\"end\":48157,\"start\":48146},{\"end\":48165,\"start\":48157},{\"end\":49258,\"start\":49250},{\"end\":49269,\"start\":49258},{\"end\":49787,\"start\":49778},{\"end\":49797,\"start\":49787},{\"end\":49805,\"start\":49797},{\"end\":49813,\"start\":49805},{\"end\":49822,\"start\":49813},{\"end\":49830,\"start\":49822},{\"end\":49839,\"start\":49830},{\"end\":49851,\"start\":49839},{\"end\":49861,\"start\":49851},{\"end\":49870,\"start\":49861},{\"end\":49880,\"start\":49870},{\"end\":49893,\"start\":49880},{\"end\":49902,\"start\":49893},{\"end\":49911,\"start\":49902},{\"end\":49923,\"start\":49911},{\"end\":49934,\"start\":49923},{\"end\":49944,\"start\":49934},{\"end\":49957,\"start\":49944},{\"end\":49967,\"start\":49957},{\"end\":49976,\"start\":49967},{\"end\":49982,\"start\":49976},{\"end\":49991,\"start\":49982},{\"end\":50496,\"start\":50488},{\"end\":50504,\"start\":50496},{\"end\":50515,\"start\":50504},{\"end\":50528,\"start\":50515},{\"end\":50537,\"start\":50528},{\"end\":50792,\"start\":50783},{\"end\":50802,\"start\":50792},{\"end\":51026,\"start\":51014},{\"end\":51037,\"start\":51026},{\"end\":51052,\"start\":51037},{\"end\":51360,\"start\":51350},{\"end\":51555,\"start\":51542}]", "bib_venue": "[{\"end\":44484,\"start\":44379},{\"end\":44890,\"start\":44778},{\"end\":45132,\"start\":45101},{\"end\":45283,\"start\":45210},{\"end\":45550,\"start\":45514},{\"end\":46072,\"start\":46008},{\"end\":46674,\"start\":46625},{\"end\":46917,\"start\":46885},{\"end\":47075,\"start\":47039},{\"end\":47407,\"start\":47312},{\"end\":47857,\"start\":47813},{\"end\":48224,\"start\":48165},{\"end\":48543,\"start\":48508},{\"end\":48695,\"start\":48657},{\"end\":48893,\"start\":48851},{\"end\":49039,\"start\":48998},{\"end\":49275,\"start\":49269},{\"end\":49471,\"start\":49430},{\"end\":49581,\"start\":49546},{\"end\":49656,\"start\":49651},{\"end\":50055,\"start\":49991},{\"end\":50565,\"start\":50537},{\"end\":50831,\"start\":50802},{\"end\":51111,\"start\":51052},{\"end\":51389,\"start\":51360},{\"end\":51565,\"start\":51555},{\"end\":51734,\"start\":51672}]"}}}, "year": 2023, "month": 12, "day": 17}