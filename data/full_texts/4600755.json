{"id": 4600755, "updated": "2023-11-11 00:50:44.257", "metadata": {"title": "High-Performance Sparse Matrix-Matrix Products on Intel KNL and Multicore Architectures", "authors": "[{\"first\":\"Yusuke\",\"last\":\"Nagasaka\",\"middle\":[]},{\"first\":\"Satoshi\",\"last\":\"Matsuoka\",\"middle\":[]},{\"first\":\"Ariful\",\"last\":\"Azad\",\"middle\":[]},{\"first\":\"Ayd\u0131n\",\"last\":\"Bulu\u00e7\",\"middle\":[]}]", "venue": "In 47th International Conference on Parallel Processing Workshops (ICPPW), 2018", "journal": "Workshop Proceedings of the 47th International Conference on Parallel Processing", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "Sparse matrix-matrix multiplication (SpGEMM) is a computational primitive that is widely used in areas ranging from traditional numerical applications to recent big data analysis and machine learning. Although many SpGEMM algorithms have been proposed, hardware specific optimizations for multi- and many-core processors are lacking and a detailed analysis of their performance under various use cases and matrices is not available. We firstly identify and mitigate multiple bottlenecks with memory management and thread scheduling on Intel Xeon Phi (Knights Landing or KNL). Specifically targeting multi- and many-core processors, we develop a hash-table-based algorithm and optimize a heap-based shared-memory SpGEMM algorithm. We examine their performance together with other publicly available codes. Different from the literature, our evaluation also includes use cases that are representative of real graph algorithms, such as multi-source breadth-first search or triangle counting. Our hash-table and heap-based algorithms are showing significant speedups from libraries in the majority of the cases while different algorithms dominate the other scenarios with different matrix size, sparsity, compression factor and operation type. We wrap up in-depth evaluation results and make a recipe to give the best SpGEMM algorithm for target scenario. A critical finding is that hash-table-based SpGEMM gets a significant performance boost if the nonzeros are not required to be sorted within each row of the output matrix.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2964336816", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icppw/NagasakaMAB18", "doi": "10.1145/3229710.3229720"}}, "content": {"source": {"pdf_hash": "b508193fac7f4fe010fb9c5d97e7052b69bb9787", "pdf_src": "ACM", "pdf_uri": "[\"https://arxiv.org/pdf/1804.01698v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/1804.01698", "status": "GREEN"}}, "grobid": {"id": "8f67b6a263d5b752482de2fdb37fb8808280f0b8", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/b508193fac7f4fe010fb9c5d97e7052b69bb9787.txt", "contents": "\nHigh-Performance Sparse Matrix-Matrix Products on Intel KNL and Multicore Architectures\nAugust 13-16. 2018\n\nYusuke Nagasaka nagasaka.y.aa@m.titech.ac.jp \nSatoshi Matsuoka \nAlso with Tokyo Institute of Technology\nDepartment of Mathematical and Computing Sciences\n\n\nAriful Azad azad@lbl.gov \nAyd\u0131n Bulu\u00e7 abuluc@lbl.gov \n\nRIKEN Center for Computational Science Kobe\nLawrence Berkeley National Laboratory Berkeley\nTokyo Institute of Technology Tokyo\nCaliforniaJapan, Japan, USA\n\n\nLawrence Berkeley National Laboratory Berkeley\nCaliforniaUSA\n\nHigh-Performance Sparse Matrix-Matrix Products on Intel KNL and Multicore Architectures\n\nICPP '18 Comp\nEugene, OR, USAAugust 13-16. 201810.1145/3229710.3229720Publication rights licensed to ACM. ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or affiliate of the United States government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. ACM ISBN 978-1-4503-6523-9/18/08. . . $15.00 ACM Reference Format: Yusuke Nagasaka, Satoshi Matsuoka, Ariful Azad, and Ayd\u0131n Bulu\u00e7. 2018. High-Performance Sparse Matrix-Matrix Products on Intel KNL and Multicore Architectures. In ICPP '18 Comp: 47th International Conference on Parallel Processing Companion, August 13-16, 2018, Eugene, OR, USA. ACM, New York, NY, USA, 10 pages. https://CCS CONCEPTS \u2022 Computing methodologies \u2192 Massively parallel algorithms; KEYWORDS Sparse matrixSpGEMMIntel KNL\nSparse matrix-matrix multiplication (SpGEMM) is a computational primitive that is widely used in areas ranging from traditional numerical applications to recent big data analysis and machine learning. Although many SpGEMM algorithms have been proposed, hardware specific optimizations for multi-and many-core processors are lacking and a detailed analysis of their performance under various use cases and matrices is not available. We firstly identify and mitigate multiple bottlenecks with memory management and thread scheduling on Intel Xeon Phi (Knights Landing or KNL). Specifically targeting multi-and many-core processors, we develop a hash-table-based algorithm and optimize a heap-based shared-memory SpGEMM algorithm. We examine their performance together with other publicly available codes. Different from the literature, our evaluation also includes use cases that are representative of real graph algorithms, such as multi-source breadth-first search or triangle counting. Our hash-table and heap-based algorithms are showing significant speedups from libraries in the majority of the cases while different algorithms dominate the other scenarios with different matrix size, sparsity, compression factor and operation type. We wrap up in-depth evaluation results and make a recipe to give the best SpGEMM algorithm for target scenario. A critical finding is that hash-table-based SpGEMM gets a significant performance boost if the nonzeros are not required to be sorted within each row of the output matrix.\n\nINTRODUCTION\n\nMultiplication of two sparse matrices (SpGEMM) is a recurrent kernel in many algorithms in machine learning, data analysis, and graph analysis. For example, bulk of the computation in multi-source breadth-first search [17], betweenness centrality [8], Markov clustering [5], label propagation [28], peer pressure clustering [31], clustering coefficients [4], high-dimensional similarity search [1], and topological similarity search [20] can be expressed as SpGEMM. Similarly, numerical applications such as scientific simulations also use SpGEMM as a subroutine. Typical examples include the Algebraic Multigrid (AMG) method for solving sparse system of linear equations [6], volumetric mesh processing [24], and linear-scaling electronic structure calculations [7].\n\nThe extensive use of SpGEMM in data-intensive applications has led to the development of several sequential and parallel algorithms. Most of these algorithms are based on Gustavson's row-wise SpGEMM algorithm [19] where a row of the output matrix is constructed by accumulating a subset of rows of the second input matrix (see Figure 1 for details). It is the accumulation (also called merging) technique that often distinguishes major classes of SpGEMM algorithms from one another. Popular data structures for accumulating rows or columns of the output matrix include heap [3], hash [26], and sparse accumulator (SPA) [16].\n\nRecently, researchers have developed parallel heap-, hash-, and SPA-based SpGEMM algorithms for shared-memory platforms [2,10,18,22,32]. These algorithms are also packaged in publicly-available software that can tremendously benefit many scientific applications. However, when using an SpGEMM algorithm and implementation for a scientific problem, one needs answers to the following questions: (a) what is the best algorithm/implementation for a problem at hand? (b) what is the best algorithm/implementation for the architecture to be used in solving the problem? These practically important questions remain mostly unanswered for many scientific applications running on highly-threaded architectures. This paper answers both questions in the context of existing SpGEMM algorithms. That means our focus is not to develop new parallel algorithms, but to characterize, optimize and evaluate existing algorithms for real-world applications on modern multicore and manycore architectures.\n\nFirst, previous algorithmic work did not pay close attention to architecture-specific optimizations that have big impacts on the performance of SpGEMM. We fill this gap by characterizing the performance of SpGEMM on shared-memory platforms and identifying bottlenecks in memory allocation and deallocation as well as overheads in thread scheduling. We propose solutions to mitigate those bottlenecks. Using microbenchmarks that model SpGEMM access patterns, we also uncover reasons behind the non-uniform performance boost provided by the MCDRAM on KNL. These optimizations resulted in efficient heap-based and hashtable-based SpGEMM algorithms that outperform state-of-the-art SpGEMM libraries including Intel MKL and Kokkos-Kernels [14] for many practical problems.\n\nSecond, previous work has narrowly focused on one or two real world application scenarios such as squaring a matrix and studying SpGEMM in the context of AMG solvers [14,27]. Different from the literature, our evaluation also includes use cases that are representative of real graph algorithms, such as the multiplication of a square matrix with a tall skinny one that represents multi-source breadth-first search and the multiplication of triangular matrices that is used in triangle counting. While in the majority of the cases the hash-table-based SpGEMM algorithm is dominant, we also find that different algorithms dominate depending on matrix size, sparsity, compression factor, and operation type. This in-depth analysis exposes many interesting features of algorithms, applications, and multithreaded platforms.\n\nThird, while many SpGEMM algorithms keep nonzeros sorted within each row (or column) in increasing column (or row) identifiers, this is not universally necessary for subsequent sparse matrix operations. For example, CSparse [11,12] assumes none of the input matrices are sorted. Clearly, if an algorithm accepts its inputs only in sorted format, then it must also emit sorted output for fairness. This is the case with the heap-based algorithms. However, hash-table-based algorithm do not need their inputs sorted. In this case we see a significant performance benefit due to skipping the sorting of the output as well.\n\nBased on these architecture-and application-centric optimizations and evaluations, we make a recipe for selecting the best-performing algorithm for a specific application scenario. Therefore, this paper brings various SpGEMM algorithms and libraries together, analyzes them based on algorithm, application, and architecture related features and provides exhaustive guidelines for SpGEMM-dependent applications.\n\n\nBACKGROUND AND RELATED WORK\n\nLet A, B be input matrices, and SpGEMM computes a matrix C such that C = AB. When analyzing algorithms in this paper, we assume n-by-n matrices for simplicity. The input and output matrices are sparse and they are stored in a sparse format. The number of nonzeros in matrix A is denoted with nnz(A). Figure 1 RowWise_SpGEMM(C, A, B) shows the skeleton of the most commonly implemented SpGEMM algorithm, which is due to Gustavson [19]. When the matrices are stored using the Compressed Sparse Rows (CSR) format, this SpGEMM algorithm proceeds row-by-row on matrix A (and hence on the output matrix C). Let a i j be the element in i-th row and j-th column of matrix A and a i * be the i-th row of matrix A. The row of matrix B corresponding to each non-zero element of matrix A is read, and each non-zero element of output matrix C is calculated. SpGEMM computation has two critical issues unlike dense matrix multiplication. Firstly, the pattern and the number of non-zero elements of output matrix are not known beforehand. For this reason, the memory allocation of output matrix becomes hard, and we need to select from two strategies. One is a two-phase method, which counts the number of non-zero elements of output matrix first (symbolic phase), and then allocates memory and computes output matrix (numeric phase). The other is an one-phase method, where we allocate large enough memory space for output matrix and compute. The former requires more computation cost, and the latter uses much more memory space. Second issue is about combining the intermediate products (value in Fig. 1) to non-zero elements of output matrix. Since the output matrix is also sparse, it is hard to efficiently accumulate intermediate products into non-zero elements. This procedure is a performance bottleneck of SpGEMM computation, and it is important to devise and select better accumulator for SpGEMM.\n1 // set matrix C to \u2205 2 for a i * in matrix A in parallel 3 do for a ik in row a i * 4 do for b k j in row b k * 5 do value \u2190 a ik b k j 6 if c i j c i * 7 then insert (c ij \u2190 value) 8 else c ij \u2190 c ij + value\nSince each row of C can be constructed independently of each other, Gustavson's algorithm is conceptually highly parallel. For accumulation, Gustavson's algorithm uses a dense vector and a list of indices that hold the nonzero entries in the current active row. This particular set of data structures used in accumulation are later formalized by Gilbert et al. under the name of sparse accumulator (SPA) [16]. Consequently, a naive parallelization of Gustavson's algorithm requires temporary storage of O(n t) where t is the number of threads. For matrices with large dimensions, a SPA-based algorithm can still achieve good performance by \"blocking\" SPA in order to decrease cache miss rates. Patwary et al. [27] achieved this by partitioning the data structure of B by columns.\n\nSulatycke and Ghose [32] presented the first shared-memory parallel algorithm for the SpGEMM problem, to the best of our knowledge. Their parallel algorithm, dubbed IKJ method due to the order of the loops, has a double-nested loop over the rows and the columns of the matrix A. Therefore, the IKJ method has work complexity O(n 2 + flop) where flop is the number of the non-trivial scalar multiplications (i.e. those multiplications where both operands are nonzero) required to compute the product. Consequently, the IKJ method is only competitive when flop \u2265 n 2 , which is rare for SpGEMM.\n\nSeveral GPU algorithms that are also based on the row-by-row formulation are presented [21,26]. These algorithms first bin the rows based on their density due to the peculiarities of the GPU architectures. Then, a poly-algorithm executes a different specialized kernel for each bin, depending on its density. Two recent algorithms that are implemented in both GPUs and CPUs also follow the same row-by-row pattern, only differing on how they perform the merging operation. ViennaCL [30] implementation, which was first described for GPUs [18], iteratively merges sorted lists, similar to merge sort. KokkosKernels implementation [14], which we also include in our evaluation, uses a multi-level hash map data structure.\n\nThe CSR format is composed of three arrays: row pointers array (rpts) of length n + 1, column indices (cols) of length nnz, and values (vals) of length nnz. Array rpts indexes the beginning and end locations of nonzeros within each row such that the range cols[rpts [i] . . . rpts[i + 1] \u2212 1] lists the column indices of row i. The CSR format does not specify whether this range should be sorted with increasing column indices; that decision has been left to the library implementation. As we will show in our results, there are significant performance benefits of operating on unsorted CSR format. Table 1 lists high-level properties of the codes we study in this paper. Heap and Hash are based on our prior work [3,26]. Since MKL code is proprietary, we do not know the accumulator. \n\n\nALGORITHMS OPTIMIZATIONS FOR OUR TARGET ARCHITECTURES\n\nOur experiments target Intel Xeon and Xeon Phi architectures. To extract the best performance from these architectures, we perform several optimizations in our SpGEMM algorithms covering light-weight thread scheduling with load-balancing, and inexpensive memory allocation and deallocation schemes. We conducted some preliminary experiments to tune optimization parameters. The effect of these architecture-specific optimizations on SpGEMM algorithm is clearly revealed in Section 4.3.1. Next, we show the optimization schemes for hash-table-based SpGEMM, which is proposed for GPU [26], and heap-based shared-memory SpGEMM algorithms [3]. Additionally, we extend the Hash SpGEMM with utilizing vector registers of Intel Xeon or Xeon Phi. Finally, we uncover the characteristic of MCDRAM based on the RowsToThreads(offset, A, B)  Table 3.\n1 // 1. Set flop vector 2 for i \u2190 0 to n in parallel 3 do flop[i] \u2190 0 4 for j \u2190 rpts A [i] to rpts A [i + 1] 5 do rnz \u2190 rpts B [cols A [j] + 1] \u2212 rpt B [cols A [j]] 6 flop[i] \u2190 flop[i] + rnz 7 // 2. Assign rows to thread 8 flop ps \u2190 ParallelPrefixSum(flop) 9 sum flop \u2190 flop ps [n] 10 tnum \u2190 omp_get_max_threads() 11 ave flop \u2190 sum flop /tnum 12 offset[0] \u2190 0 13 for tid \u2190 1 to tnum in parallel 14 do offset[tid] \u2190 lowbnd(flop ps , ave flop * tid) 15 offset[tnum] \u2190 n\n\nLight-weight Load-balancing Thread Scheduling Scheme\n\nWhen parallelizing a loop, OpenMP provides three thread scheduling choices: static, dynamic and guided. Static scheduling divides loop iterations equally among threads, dynamic scheduling allocates iterations to threads dynamically, and guided scheduling starts with static scheduling with smaller iterations and switch to dynamic scheduling in the later part. Here, we experimentally evaluate the cost of these three scheduling options on KNL processors. 1 In load-balanced situation with a large number of iterations, static scheduling takes very little scheduling overhead compared to dynamic scheduling on both Haswell and KNL, as expected. The guided scheduling is also as expensive as dynamic scheduling, especially on the KNL processor. Based on these evaluations, we opt to use static scheduling in our SpGEMM algorithms. To achieve good load-balance with static scheduling, the bundle of rows should be assigned to threads with equal computation complexity. Figure 2 shows how to assign rows to threads. First, we count flop of each row, then do prefix sum. Each thread can find the start point of rows by binary search. lowbnd(vec, value) in line 14 finds the minimum id such that vec[id] is larger than or equal to value. Each of these three operations can be executed in parallel.\n\n\nInexpensive Memory Allocation and Deallocation\n\nTo find a suitable memory allocation/deallocation scheme on KNL, we performed a simple experiment: allocate a memory space, access elements on the allocated memory and then deallocate it. To contrast this \"single\" memory management scheme, we considered a \"parallel\" approach where each thread independently allocates/deallocates equal portion of the total requested memory and accesses only its own allocated memory space. We examined two ways to allocate or deallocate memory; new/delete of C++ and scalable_malloc/_free provided by Intel TBB (Thread Building Block). Figure 3 shows the results of \"single\" deallocation and \"parallel\" deallocation with 256 threads on KNL. All \"single\" allocators have extremely high cost for large memory: over 100 milliseconds for the deallocation of 1GB memory space. The \"parallel\" deallocation for large memory chunks is much cheaper than \"single\" deallocation. The cost of \"parallel\" deallocation suddenly rises at 8GB (C++) or 64GB (TBB), where each thread allocates 32MB or 256MB, respectively. These thresholds match those of \"single\" deallocation. On the other hand, the cost of \"parallel\" deallocation for small memory space becomes larger than \"single\" deallocation since \"parallel\" deallocation causes the overheads of OpenMP scheduling and synchronization. From this result, we compute the amount of memory required by each thread and allocate this amount of thread-private memory in each thread independently in order to reduce deallocation cost in SpGEMM computation, which requires temporally memory allocation and deallocation. In the following experiments in this paper, the TBB is used for both \"single\" and \"parallel\" memory allocation/deallocation to simply have performance gain.\n\n\nSymbolic and Accumulation\n\nWe optimized two approaches of accumulation for KNL, one is hash-table-based algorithm and the other is heap-based algorithm. Furthermore, we add another version of Hash SpGEMM, where hash probing is vectorized with AVX-512 or AVX2 instructions.\n\n\nHash SpGEMM.\n\nWe use hash table for accumulator in SpGEMM computation, based on GPU work [26]. Figure 4 shows the algorithm of Hash SpGEMM for multi-and many-core processors. We count a flop per row of output matrix. The upper limit of any thread's local hash table size is the maximum number of flop per row within the rows assigned to it. Each thread once allocates the hash table based on its own upper limit and reuses that hash table throughout the computation by reinitializing for each row. Next is about hashing algorithm we adopted. A column index is inserted into hash table as key. Since the column index is no less than 0, the hash table is initialized by storing \u22121. The column index is multiplied by constant number and divided by hash table size to compute the remainder. In order to compute modulus operation efficiently, the hash table size is set as 2 n (n is a integer). The hashing\nHash_SpGEMM(C, A, B) 1 RowsToThreads(offset, A, B) 2 // Determine hash table size for each thread 3 for tid \u2190 0 to tnum 4 do size t \u2190 0 5 for i \u2190 offset[tid] to offset[tid + 1] 6 do size t \u2190 max(size t , flop[i]) 7 // Required maximum hash table size is N col 8 size t \u2190 min(N col , size t ) 9\n// Return minimum 2 m so that 2 m > size t 10 size t \u2190 lowest_p2(size t ) 11 // Allocate rpts C . After Symbolic, allocate cols C and vals C 12 Symbolic(rpts C , A, B) 13 Numeric(C, A, B)  In symbolic phase, it is enough to insert keys to the hash table. In numeric phase, however, we need to store the resulting value data. Once the computation on the hash table finishes, the results are sorted by column indices in ascending order (if necessary), and stored to memory as output. This Hash SpGEMM for multi/manycore processors differs from the GPU version as follows. While a row of output is computed by multiple threads in the GPU version to exploit massive number of threads on GPU, each row is processed by a single thread in the present algorithm. Hash SpGEMM on GPU requires some form of mutual exclusion since multiple threads access the same entry of the hash table concurrently. We were able to remove this overhead in our present Hash SpGEMM for multi/many-core processors.\n\n\nHashVector SpGEMM.\n\nIntel Xeon or Xeon Phi implements 256 and 512-bit wide vector register, respectively. This vector register reduces instruction counts and brings large benefit to algorithms and applications, which require contiguous memory access. However, sparse matrix computation has indirect memory access, and hence it is hard to utilize vector registers. In this paper, we utilize vector register for hash probing in our Hash SpGEMM algorithm. The vectorization of hash probing is based on Ross [29]. In order to examine the keys in the chunk, we use comparison instruction with vector register. If the entry with target key is found, the algorithm finishes the probing for the element in symbolic phase. In numeric phase, the target entry in chunk is identified by __builtin_ctz function, which counts trailing zeros, and the multiplied value is added to the value of the entry. If the algorithm finds no entry with the key, the element is pushed to the hash table. In HashVector, new element is pushed into the table in order from the beginning. The entries in chunk are compared with the initial value of hash table, -1, by using vector register. The beginning of empty entries can be found by counting the number of bit flags of comparison result. When the chunk is occupied with other keys, the next chunk is to be checked in accordance with linear probing. Since Hash vector SpGEMM can reduce the number of probing caused by hash collision, it can achieve better performance compared to Hash SpGEMM. However, HashVector requires a few more instructions for each check. Thus, HashVector may degrade the performance when the collisions in Hash SpGEMM are rare.\n\n\nHeap SpGEMM.\n\nIn another variant of SpGEMM [3], we use a priority queue (heap) -indexed by column indices -to accumulate each row of C. To construct c i * , a heap of size nnz(a i * ) is allocated. For every nonzero a ik , the first nonzero entry in b k * along with its column index is inserted into the heap. The algorithm iteratively extracts an entry with the minimum column index from the heap, accumulates it to c i * , and inserts the next nonzero entry from the last extracted row of B into the heap. When the heap becomes empty, the algorithm moves to construct the next row of C.\n\nHeap SpGEMM can be more expensive than hash-and SPAbased algorithms because it requires logarithmic time to extract elements from the heap. However, from the accumulator point of view, Heap SpGEMM is space efficient as it uses O(nnz(a i * )) memory to accumulate c i * instead of O(flop(c i * )) and O(n) memory used by hash-and SPA-based algorithms, respectively.\n\nOur implementation of Heap SpGEMM adopts the one-phase method, which requires larger memory usage for temporally keeping the output. In parallel Heap SpGEMM, because rows of C are independently constructed by different threads, this temporary memory use for keeping output is thread-independent and we can adapt \"parallel\" approach for memory management. Thread-private heaps are also managed with \"parallel\" approach. As with the Hash algorithm, Heap SpGEMM estimates flop via a symbolic step and uses it to balance computational load evenly among threads.\n\n\nEfficient Use of MCDRAM\n\nIntel KNL implements MCDRAM, which can accelerate bandwidth-bound algorithms and applications. While MCDRAM provides high bandwidth, its memory latency is larger than that of DDR4. In row-wise SpGEMM (Algorithm 1), there are three main types of data accesses for the formation of each row of C. First, there is a unit-stride streaming access pattern arising from access of the row pointers of A as well as the creation of the sparse output vector c i * . Second, access to rows of B follows a stanza-like Finally, updates to the accumulator exhibit different access pattern depending on the type of the accumulator (a hash table, SPA, or heap). The streaming access to the input vector is usually the cheapest of the three and the accumulator access depends on the data structure used. Hence, stanza access pattern is the most canonical of the three and provides a decent proxy to study.\n\nTo quantify the stanza bandwidth which we expect to be quite different than STREAM [23], we used a custom microbenchmark that provides stanza-like memory access patterns (read or update) with spatial locality varying from 8 bytes (random access) to the size of the array (i.e. asymptotically the STREAM benchmark). Figure 6 shows a comparison result between DDR only and use of MCDRAM as Cache with scaling the length of contiguous memory access. When the contiguous memory access is wider, both DDR only and MCDRAM as Cache achieve their peak bandwidth, and especially MCDRAM as Cache shows over 3.4\u00d7 superior bandwidth compared to DDR only. However, the use of MCDRAM as Cache is incompatible with fine-grained memory access. When the stanza length is small, there is little benefit of using MCDRAM. This benchmark hints that it would be hard to get the benefits of MCDRAM on very sparse matrices.\n\n\nEXPERIMENTAL SETUP 4.1 Input Types\n\nWe use two types of matrices for the evaluation. We generate synthetic matrix using matrix generator, and take matrices from SuiteSparse Matrix Collection [13]. For the evaluation of unsorted output, the column indices of input matrices are randomly permuted. We use 26 sparse matrices used in [4,14,21]. The matrices are listed in Table 2.\n\nWe use R-MAT [9], the recursive matrix generator, to generate two different non-zero patterns of synthetic matrices represented as ER and G500. ER matrix represents Erd\u0151s-R\u00e9nyi random graphs, and G500 represents graphs with power-law degree distributions used for Graph500 benchmark. These matrices are generated with R-MAT seed parameters; a = b = c = d = 0.25 for ER matrix and a = 0.57, b = c = 0.19, d = 0.05 for G500 matrix. A scale m matrix represents 2 m -by-2 m . The edge factor parameter for the generator is the average number of non-zero elements per row (or column) of the matrix. In other words, it is the ratio of nnz to n.\n\n\nExperimental Environment\n\nWe evaluate the performance of SpGEMM on a single node of the Cori supercomputer at NERSC. Cori system consists of two  Table 3. Each performance number in the following part is the average of ten executions. The Haswell and KNL processors provide hyperthreading with 2 or 4 threads for each core respectively. We set the number of threads as 68, 136, 204 or 272 for KNL, and 16, 32 or 64 for Haswell. For the evaluation of Kokkos on KNL, we set 256 threads instead of 272 threads since the execution fails on more than 256 threads. We show the result with the best thread count. For the evaluation on KNL, we set \"quadrant\" cluster mode, and mainly \"Cache\" memory mode. To select DDR4 or MCDRAM with \"Flat\" memory mode, we use \"numactl -p\". The thread affinity is set as \"KMP_AFFINITY='granularity=fine',scatter\".\n\n\nPreliminary Evaluation on KNL\n\n\nAdvantage of Performance Optimization on KNL for\n\nSpGEMM. We examined performance difference between OpenMP scheduling and ways to allocate memory. Figure 7 shows the performance of Heap SpGEMM for squaring G500 matrices with edge factor 16. When simply parallelizing SpGEMM by row, we cannot achieve higher performance because of load imbalance with static scheduling or expensive scheduling overhead with dynamic/guided scheduling. On the other hand, our light-weight load-balancing thread scheduling scheme, 'balanced', works well on SpGEMM. For larger inputs, Heap SpGEMM temporally  requires large memory use, whose deallocation causes performance degradation. Our \"parallel\" memory management scheme 'balanced parallel' reduces the overhead of memory deallocation for temporal memory use, and keeps high performance on large size inputs.\n\n\nDDR vs MCDRAM.\n\nWe examine the benefit of using MCDRAM over DDR memory by squaring G500 matrices with or without using MCDRAM on KNL. Figure 8 shows the speedup attained with the Cache mode against the Flat mode on DDR for various matrix densities. We observe that Hash SpGEMM algorithms can be benefitted, albeit moderately, from MCDRAM when denser matrices are multiplied. This observation is consistent with the benchmark shown in Figure 6. The limited benefit stems from the fact that SpGEMM frequently requires indirect fine-grained memory accesses often as small as 8 bytes. On denser matrices, MCDRAM can still bring benefit from contiguous memory accesses of input matrices. By contrast, Heap SpGEMM is not benefitted from high-bandwidth MCDRAM because of its fine-grained memory accesses. The performance of Heap SpGEMM even degrades when edge factor is 64 at which point the memory requirement of Heap SpGEMM surpasses the capacity of MCDRAM.\n\n\nEXPERIMENTAL RESULTS\n\nDifferent SpGEMM algorithms can dominate others depending on the aspect ratio (i.e. ratio of its dimensions), density, sparsity structure, and size (i.e. dimensions) of its inputs. To provide a comprehensive and fair assessment, we evaluate SpGEMM codes under several different scenarios. For the case where input and output matrices are sorted, we evaluate MKL, Heap and Hash/HashVector, and for the case where they are unsorted we evaluate MKL, MKL-inspector, KokkosKernels (with 'kkmem' option) and Hash/HashVector.\n\n\nSquaring a matrix\n\nMultiplying a sparse matrix by itself is a well-studied SpGEMM scenario. Markov clustering is an example of this case, which requires A 2 for a given doubly-stochastic similarity matrix. We evaluate this case extensively, under using real and synthetically generated data. For synthetic data, we provide experiments with varying density (for a fixed sized matrix) and with varying size (for a fixed density). Figure 9 shows the result of scaling with density. When output is sorted, MKL's performance degrades with increasing density. When the output is not sorted, increased density generally translates into increased performance. The performance of all codes except MKL increases significantly as the ER matrices get denser, but such performance gains are not so pronounced for G500 matrices. For G500 matrices, we see significant performance difference between KNL and Haswell results. While Hash shows superior performance on KNL, HashVector achieves much higher performance on Haswell. Also for G500 matrices, we see that MKL (both sorted and unsorted) and MKL-inspector achieves a peak in performance at edgecount 8, with performance degrading as the matrices get denser or sparser than that sweet spot. Hash and HashVector might not have peaked at these density ranges we experimented since they get faster as the matrices get denser.\n\n\nScaling with Density.\n\n\nScaling with Input\n\nSize. Evaluation is running on ER and G500 matrices with scaling the size from 7 to 20 or 17 respectively. The edge factor is fixed as 16. Figure 10 shows the results on KNL (top) and Haswell (bottom). On KNL, MKL family with unsorted output shows good performance for ER matrices with small scale. However, for large scale matrices, MKL goes down, and Heap and Hash overcome. Especially, Hash and HashVector keep high performance even for large scale matrices. This performance trend becomes more clear on Haswell. When the scale is about 13, the performance gap between sorted and unsorted is large, and it becomes smaller when the scale is getting large. This is because the cost of computation with hash table or heap becomes larger, and the advantage of removing sorting phase becomes relatively smaller. For G500 matrices, whose non-zero elements of each row are skewed, the performance of MKL is terrible even if its output is unsorted. Since there is no issue about load-balance in Heap and Hash kernels, they show stable performance as ER matrices.  Figure 11 shows the scalability analysis of KNL on ER and G500 matrices with scale=16 and edge factor=16. Each kernel is executed with 1,2,4,8,16,32,64,68,128,136,192,204,256 or 272 threads. We do not show the result of MKL with sorted output since it takes much longer execution time compared to other kernels. All kernels show good scalability until around 64 threads, but MKL with unsorted output has no improvement over 68 threads. On the other hand, Heap and Hash/HashVector get further improvement over 64 threads.\n\n\nScaling with Thread Count.\n\n\nSensitivity to Compression\n\nRatio on Real Matrices. We evaluate SpGEMM performance on 26 real matrices listed in Table 2 on KNL. Figure 12 shows the result with sorted output and unsorted output respectively in ascending order of compression ratio (= flop / number of non-zero elements of output). Lines in the graph are linear fitting for each kernel. First we discuss the result with sorted matrices. The performance of Heap is stable regardless of compression ratio while MKL gets better performance with higher compression ratio. The matrices about graph processing with low compression ratio cause load imbalance and performance degradation on MKL. In contrast, Hash outperforms MKL on most of matrices, and shows high performance independent from compression ratio. For unsorted matrices, we add KokkosKernels to the evaluation, but it underperforms other kernels in this test. The performance of Hash SpGEMM is best for the matrices with low compression ratio and becomes worse on high compression ratio matrices as well as the evaluation with sorted output. MKL-inspector shows significant improvement especially for the matrices with high compression ratio.\n\nComparing sorted and unsorted versions of algorithm that provide the flexibility, we see consistent performance boost of keeping the output sorted. In particular, the harmonic mean of the speedups achieved operating on unsorted data over all real matrices we have studied from the SuiteSparse collection on KNL is 1.58\u00d7 for MKL, 1.63\u00d7 for Hash, and 1.68\u00d7 for HashVector.\n\n\nProfile of Relative Performance of SpGEMM Algorithms.\n\nWe compare the relative performance of different SpGEMM algorithms with performance profile plots [15]. To profile the relative performance of algorithms, the best performing algorithm for each problem is identified and assigned a relative score of 1. Other algorithms are scored relative to the best performing algorithm, with a higher value denoting inferior performance for that particular problem. For example, if algorithm A and B solve the same problem in 1 and 3 seconds, their relative performance scores will be 1 and 3, respectively. Figure 13 shows the profiles of relative performance of different SpGEMM algorithms for all 26 matrices from Table 2. Hash is clearly the best performer for sorted matrices as it outperforms all other algorithms for 70% matrices and its runtime is always within 1.6\u00d7 of the best algorithm. Hash is followed by HashVector, MKL and Heap algorithms in decreasing order of overall performance. For unsorted matrices, Hash, HashVector and MKL-inspector all perform equally well for most matrices (each of them performs the best for about 40% matrices). They are followed by MKL and KokkosKernels, with the latter being the worst performer for unsorted matrices.\n\n\nSquare x Tall-skinny matrix\n\nMany graph processing algorithms perform multiple breadth-first searches (BFSs) in parallel, an example being Betweenness Centrality on unweighted graphs. In linear algebraic terms, this corresponds to multiplying a square sparse matrix with a tall-skinny one. The left-hand-side matrix represents the graph and the right-hand-side matrix represent the stack of frontiers, each column representing one BFS frontier. In the memory-efficient implementation of the Markov clustering  algorithm [5], a matrix is multiplied with a subset of its column, representing another use case of multiplying a square matrix with a tall-skinny matrix. In our evaluations, we generate the tall-skinny matrix by randomly selecting columns from the graph itself. Figure 14 shows the result of SpGEMM between square and tall-skinny matrices. We set scale as 18, 19 or 20 for square matrix, and as 10, 12, 14 or 16 for short size of tall-skinny matrix. The non-zero pattern of generated matrix is G500 with edge factor=16. The result of square x tall-skinny follows that of A 2 (upper right in Figure 10). Both for sorted and unsorted cases, Hash or HashVec is the best performer.\n\n\nTriangle counting\n\nWe also evaluate the performance of SpGEMM used in triangle counting [4]. The original input is the adjacency matrix of an undirected graph. For optimal performance in triangle counting, we reorder rows with increasing number of nonzeros. The algorithm then splits the reordered matrix A as A = L + U , where L is a lower triangular matrix and U is an upper triangular matrix. We evaluate the SpGEMM performance of the next step, where L \u00b7 U is computed to generate all wedges. After preprocessing the input matrix, we compute SpGEMM between the lower triangular matrix L and the upper triangular matrix U . Figure 15 shows the result with sorted output respectively in ascending order of compression ratio on KNL. Lines in the graph are linear fitting for each kernel. Basically, the result shows similar performance trend to that of A 2 . Hash and HashVector generally overwhelm MKL for any compression ratio. One big difference from A 2 is that Heap performs the best for inputs with low compression ratios.\n\n\nCONCLUSIONS\n\nWe studied the performance of computing the multiplication of two sparse matrices on multicore and Intel KNL architectures. This  Figure 15: The performance of SpGEMM between L and U triangular matrices when used to count triangles on KNL primitive, known as SpGEMM, has recently gained attention in the GPU community, but there has been relatively less work on CPUs and other accelerators. We have tried to fill that gap by evaluating publicly accessible implementations, including those in proprietary libraries. From architecture point of view, we develop the optimized Heap and Hash SpGEMM algorithms for multicore and Intel KNL architectures. Performance evaluation shows that our optimized SpGEMM algorithms largely overcome Intel MKL and Kokkos-kernel.\n\nOur work provides multiple recipes. One is for the implementers of new algorithms on highly-threaded x86 architectures. We have found that the impact of memory allocation and deallocation to be significant enough to warrant optimization as without them SpGEMM performance does not scale well with increasing number of threads. We have also uncovered the impact of MCDRAM for the SpGEMM primitives. When the matrices are sparser than a threshold (\u2248 4 nonzeros on average per row), the impact of MCDRAM is minimal because in that regime the computation becomes close to latency bound. On the other than, MCDRAM shines as matrices get denser because then SpGEMM becomes primarily bandwidth bound and can take advantage of\n\nFigure 1 :\n1Pseudo code of Gustavson's Row-wise SpGEMM algorithm. The in parallel keyword does not exist in the original algorithm but is used here to illustrate the common parallelization pattern of this algorithm used by all known implementations.\n\nFigure 2 :\n2Load-balanced Thread Assignment memory access pattern of SpGEMM. These microbenchmarks are especially valuable for Intel KNL that offers massive parallelism and has a 16GB high bandwidth memory (MCDRAM) along with traditional DDR memory. Details of evaluation environment are summarized in\n\nFigure 3 :\n3Cost of deallocation on KNL\n\nFigure 4 :\n4Hash\n\nFigure 5 :\n5Hash Probing in Hash and HashVector SpGEMM algorithm is based on linear probing. Figure 5-(a) shows an example of hash probing on 16 entries hash table.\n\nFigure 5 -\n5(b) shows how HashVector algorithm works hash probing. The size of hash table is 16, same as (a), and it is divided into chunks based on vector width. A chunk consists of 8 entries on Haswell since a key (= column index) is represented as 32-bit in our evaluation. In HashVector, the hash indicates the identifier of target chunk in hash table.\n\nFigure 6 :\n6Benchmark result of random memory access with DDR only or MCDRAM as Cache memory access pattern where small blocks (stanzas) of consecutive elements are fetched from effectively random locations in memory.\n\nFigure 7 :Figure 8 :\n78Performance of Heap SpGEMM scaling with size of G500 inputs on KNL with Speedups attained with the use of Cache mode on KNL compared to Flat mode on DDR4. G500 (scale 15) matrices are used with different edge factors.\n\nFigure 9 :Figure 10 :\n910Scaling with increasing density (scale 16) on KNL (left) and Haswell (right) Scaling with size on KNL (top) and Haswell (bottom), both with edge factor 16\n\nFigure 11 :Figure 12 :\n1112Strong scaling with thread count on KNL with ER (left) and G500 inputs (right). Data used is of scale 16 Scaling with compression ratio of SuiteSparse matrices on KNL. The algorithms that operate on sorted matrices (both input & output) are on the left and those that operate on unsorted matrices are on the right.\n\nFigure 13 :\n13Performance profiles of SuiteSparse matrices on KNL using sorted (left) and unsorted (right) algorithms.\n\nFigure 14 :\n14SpGEMM between square and tall-skinny matrices on KNL (scales18, 19, and 20)    \n\nTable 1 :\n1Summary of SpGEMM codes studied in this paperAlgorithm \nPhases Accumulator \n\nSortedness \n(Input/Output) \nMKL \n2 \n-\nAny/Select \nMKL-inspector \n1 \n-\nAny/Unsorted \nKokkosKernels \n2 \nHashMap \nAny/Unsorted \nHeap \n1 \nHeap \nSorted/Sorted \nHash/HashVector \n2 \nHash Table \nAny/Select \n\n\n\nTable 2 :\n2Matrix data used in our experiments (all numbersare in millions) \nMatrix \nn nnz(A) flop(A 2 ) nnz(A 2 ) \n2cubes_sphere \n0.101 \n1.65 \n27.45 \n8.97 \ncage12 \n0.130 \n2.03 \n34.61 \n15.23 \ncage15 \n5.155 \n99.20 2,078.63 \n929.02 \ncant \n0.062 \n4.01 \n269.49 \n17.44 \nconf5_4-8x8-05 \n0.049 \n1.92 \n74.76 \n10.91 \nconsph \n0.083 \n6.01 \n463.85 \n26.54 \ncop20k_A \n0.121 \n2.62 \n79.88 \n18.71 \ndelaunay_n24 \n16.777 \n100.66 \n633.91 \n347.32 \nfilter3D \n0.106 \n2.71 \n85.96 \n20.16 \nhood \n0.221 \n10.77 \n562.03 \n34.24 \nm133-b3 \n0.200 \n0.80 \n3.20 \n3.18 \nmac_econ_fwd500 \n0.207 \n1.27 \n7.56 \n6.70 \nmajorbasis \n0.160 \n1.75 \n19.18 \n8.24 \nmario002 \n0.390 \n2.10 \n12.83 \n6.45 \nmc2depi \n0.526 \n2.10 \n8.39 \n5.25 \nmono_500Hz \n0.169 \n5.04 \n204.03 \n41.38 \noffshore \n0.260 \n4.24 \n71.34 \n23.36 \npatents_main \n0.241 \n0.56 \n2.60 \n2.28 \npdb1HYS \n0.036 \n4.34 \n555.32 \n19.59 \npoisson3Da \n0.014 \n0.35 \n11.77 \n2.96 \npwtk \n0.218 \n11.63 \n626.05 \n32.77 \nrma10 \n0.047 \n2.37 \n156.48 \n7.90 \nscircuit \n0.171 \n0.96 \n8.68 \n5.22 \nshipsec1 \n0.141 \n7.81 \n450.64 \n24.09 \nwb-edu \n9.846 \n57.16 1,559.58 \n630.08 \nwebbase-1M \n1.000 \n3.11 \n69.52 \n51.11 \n\npartitions; one is Intel Xeon Haswell cluster (Haswell), and another \nis Intel KNL cluster. We use nodes from both partitions of Cori. \nDetails are summarized in \n\nTable 3 :\n3Overview of Evaluation Environment(Cori system)Haswell cluster \nKNL cluster \n\nCPU \n\nIntel Xeon \nProcessor E5-2698 v3 \n\nIntel Xeon Phi \nProcessor 7250 \n#Sockets \n2 \n1 \n#Cores/socket 16 \n68 \nClock \n2.3GHz \n1.4GHz \nL1 cache \n32KB/core \n32KB/core \nL2 cache \n256KB/core \n1MB/tile \nL3 cache \n40MB per socket \n-\nMemory \nDDR4 \n128GB \n96GB \nMCDRAM \n-\n16GB \nSoftware \nOS \nSuSE Linux Enterprise Server 12 SP3 \nCompiler \nIntel C++ Compiler (icpc) ver18.0.0 \nOption \n-g -O3 -qopenmp \n\n6 \n8 \n10 \n12 \n14 \n16 \n18 \nScale \n\n200 \n\n400 \n\n600 \n\n800 \n\nMFLOPS \n\nstatic \ndynamic \nguided \nbalanced single \nbalanced parallel \n\n\nDetailed benchmark results on scheduling costs can be found in Section 3.1 of our longer technical report[25] \n\nExploiting accelerators for efficient high dimensional similarity search. Sandeep R Agrawal, M Christopher, Alvin R Dee, Lebeck, PPoPP. ACMSandeep R Agrawal, Christopher M Dee, and Alvin R Lebeck. 2016. Exploiting accelerators for efficient high dimensional similarity search. In PPoPP. ACM.\n\nBalanced Hashing and Efficient GPU Sparse General Matrix-Matrix Multiplication. Rui Pham Nguyen Quang Anh, Yonggang Fan, Wen, ICS. New York, NY, USAACM36Pham Nguyen Quang Anh, Rui Fan, and Yonggang Wen. 2016. Balanced Hashing and Efficient GPU Sparse General Matrix-Matrix Multiplication. In ICS. ACM, New York, NY, USA, Article 36.\n\nExploiting multiple levels of parallelism in sparse matrix-matrix multiplication. Ariful Azad, Grey Ballard, Aydin Bulu\u00e7, James Demmel, Laura Grigori, Oded Schwartz, Sivan Toledo, Samuel Williams, SIAM Journal on Scientific Computing. 38Ariful Azad, Grey Ballard, Aydin Bulu\u00e7, James Demmel, Laura Grigori, Oded Schwartz, Sivan Toledo, and Samuel Williams. 2016. Exploiting multiple levels of parallelism in sparse matrix-matrix multiplication. SIAM Journal on Scientific Computing 38, 6 (2016), C624-C651.\n\nParallel triangle counting and enumeration using matrix algebra. Ariful Azad, Ayd\u0131n Bulu\u00e7, John Gilbert, IPDPSW. Ariful Azad, Ayd\u0131n Bulu\u00e7, and John Gilbert. 2015. Parallel triangle counting and enumeration using matrix algebra. In IPDPSW.\n\nHipMCL: a high-performance parallel implementation of the Markov clustering algorithm for large-scale networks. Ariful Azad, A Georgios, Christos A Pavlopoulos, Nikos C Ouzounis, Aydin Kyrpides, Bulu\u00e7, Nucleic acids research. Ariful Azad, Georgios A Pavlopoulos, Christos A Ouzounis, Nikos C Kyrpides, and Aydin Bulu\u00e7. 2018. HipMCL: a high-performance parallel implementation of the Markov clustering algorithm for large-scale networks. Nucleic acids research (2018).\n\nReducing communication costs for sparse matrix multiplication within algebraic multigrid. Grey Ballard, Christopher Siefert, Jonathan Hu, SIAM Journal on Scientific Computing. 38Grey Ballard, Christopher Siefert, and Jonathan Hu. 2016. Reducing communication costs for sparse matrix multiplication within algebraic multigrid. SIAM Journal on Scientific Computing 38, 3 (2016), C203-C231.\n\nSolvers for O(N) Electronic Structure in the Strong Scaling Limit. Nicolas Bock, Matt Challacombe, Laxmikant V Kal\u00e9, SIAM Journal on Scientific Computing. 38Nicolas Bock, Matt Challacombe, and Laxmikant V Kal\u00e9. 2016. Solvers for O(N) Electronic Structure in the Strong Scaling Limit. SIAM Journal on Scientific Computing 38, 1 (2016), C1-C21.\n\nAyd\u0131n Bulu\u00e7, John R Gilbert, The Combinatorial BLAS: Design, Implementation, and Applications. IJHPCA. 25Ayd\u0131n Bulu\u00e7 and John R. Gilbert. 2011. The Combinatorial BLAS: Design, Implementation, and Applications. IJHPCA 25, 4 (2011), 496-509.\n\nR-MAT: A recursive model for graph mining. Deepayan Chakrabarti, Yiping Zhan, Christos Faloutsos, Proceedings of the 2004 SIAM International Conference on Data Mining. SIAM. the 2004 SIAM International Conference on Data Mining. SIAMDeepayan Chakrabarti, Yiping Zhan, and Christos Faloutsos. 2004. R-MAT: A recursive model for graph mining. In Proceedings of the 2004 SIAM International Conference on Data Mining. SIAM, 442-446.\n\nOptimizing sparse matrixmatrix multiplication for the gpu. Steven Dalton, Luke Olson, Nathan Bell, ACM Transactions on Mathematical Software (TOMS). 4125Steven Dalton, Luke Olson, and Nathan Bell. 2015. Optimizing sparse matrix- matrix multiplication for the gpu. ACM Transactions on Mathematical Software (TOMS) 41, 4 (2015), 25.\n\n. A Timothy, Davis, n. d.. private communicationTimothy A Davis. [n. d.]. private communication.\n\nDirect methods for sparse linear systems. A Timothy, Davis, SIAM. Timothy A Davis. 2006. Direct methods for sparse linear systems. SIAM.\n\nThe University of Florida sparse matrix collection. A Timothy, Yifan Davis, Hu, ACM Transactions on Mathematical Software (TOMS). 381Timothy A Davis and Yifan Hu. 2011. The University of Florida sparse matrix collection. ACM Transactions on Mathematical Software (TOMS) 38, 1 (2011), 1.\n\nPerformance-Portable Sparse Matrix-Matrix Multiplication for Many-Core Architectures. Mehmet Deveci, Christian Trott, Sivasankaran Rajamanickam, IPDPSW. IEEE. Mehmet Deveci, Christian Trott, and Sivasankaran Rajamanickam. 2017. Performance-Portable Sparse Matrix-Matrix Multiplication for Many-Core Architectures. In IPDPSW. IEEE, 693-702.\n\nBenchmarking optimization software with performance profiles. D Elizabeth, Jorge J Dolan, Mor\u00e9, Mathematical programming. 91Elizabeth D Dolan and Jorge J Mor\u00e9. 2002. Benchmarking optimization software with performance profiles. Mathematical programming 91, 2 (2002), 201-213.\n\nSparse matrices in MATLAB: Design and implementation. Cleve John R Gilbert, Robert Moler, Schreiber, SIAM J. Matrix Anal. Appl. 13John R Gilbert, Cleve Moler, and Robert Schreiber. 1992. Sparse matrices in MATLAB: Design and implementation. SIAM J. Matrix Anal. Appl. 13, 1 (1992), 333-356.\n\nHigh performance graph algorithms from parallel sparse matrices. John R Gilbert, Steve Reinhardt, Viral B Shah, PARA. John R. Gilbert, Steve Reinhardt, and Viral B. Shah. 2007. High performance graph algorithms from parallel sparse matrices. In PARA. 260-269.\n\nGPU-Accelerated Sparse Matrix-Matrix Multiplication by Iterative Row Merging. Felix Gremse, Andreas Hofter, Lars Ole Schwen, Fabian Kiessling, Uwe Naumann, SIAM Journal on Scientific Computing. 37Felix Gremse, Andreas Hofter, Lars Ole Schwen, Fabian Kiessling, and Uwe Naumann. 2015. GPU-Accelerated Sparse Matrix-Matrix Multiplication by Iterative Row Merging. SIAM Journal on Scientific Computing 37, 1 (2015), C54- C71.\n\nTwo fast algorithms for sparse matrices: Multiplication and permuted transposition. Fred G Gustavson, ACM TOMS. 4Fred G Gustavson. 1978. Two fast algorithms for sparse matrices: Multiplication and permuted transposition. ACM TOMS 4, 3 (1978), 250-269.\n\nParallel SimRank computation on large graphs with iterative aggregation. Guoming He, Haijun Feng, Cuiping Li, Hong Chen, SIGKDD. ACMGuoming He, Haijun Feng, Cuiping Li, and Hong Chen. 2010. Parallel SimRank computation on large graphs with iterative aggregation. In SIGKDD. ACM.\n\nAn efficient GPU general sparse matrixmatrix multiplication for irregular data. Weifeng Liu, Brian Vinter, IPDPS. IEEE. Weifeng Liu and Brian Vinter. 2014. An efficient GPU general sparse matrix- matrix multiplication for irregular data. In IPDPS. IEEE, 370-381.\n\nSparse Matrix-Matrix Multiplication on Modern Architectures. Kiran Matam, Siva Rama Krishna Bharadwaj Indarapu, Kishore Kothapalli, HiPC. IEEEKiran Matam, Siva Rama Krishna Bharadwaj Indarapu, and Kishore Kothapalli. 2012. Sparse Matrix-Matrix Multiplication on Modern Architectures. In HiPC. IEEE.\n\nSTREAM: Sustainable Memory Bandwidth in High Performance Computers. John D Mccalpin, University of VirginiaTechnical ReportJohn D. McCalpin. 1991-2007. STREAM: Sustainable Memory Bandwidth in High Performance Computers. Technical Report. University of Virginia.\n\nTernary Sparse Matrix Representation for Volumetric Mesh Subdivision and Processing on GPUs. Johannes Sebastian Mueller-Roemer, Christian Altenhofen, Andr\u00e9 Stork, Computer Graphics Forum. 36Johannes Sebastian Mueller-Roemer, Christian Altenhofen, and Andr\u00e9 Stork. 2017. Ternary Sparse Matrix Representation for Volumetric Mesh Subdivision and Processing on GPUs. In Computer Graphics Forum, Vol. 36.\n\nHigh-performance sparse matrix-matrix products on Intel KNL and multicore architectures. Yusuke Nagasaka, Satoshi Matsuoka, Ariful Azad, Ayd\u0131n Bulu\u00e7, arXiv:1804.01698arXiv preprintYusuke Nagasaka, Satoshi Matsuoka, Ariful Azad, and Ayd\u0131n Bulu\u00e7. 2018. High-performance sparse matrix-matrix products on Intel KNL and multicore architectures. arXiv preprint arXiv:1804.01698 (2018).\n\nHigh-Performance and Memory-Saving Sparse General Matrix-Matrix Multiplication for NVIDIA Pascal GPU. Yusuke Nagasaka, Akira Nukada, Satoshi Matsuoka, ICPP. IEEE. Yusuke Nagasaka, Akira Nukada, and Satoshi Matsuoka. 2017. High-Performance and Memory-Saving Sparse General Matrix-Matrix Multiplication for NVIDIA Pascal GPU. In ICPP. IEEE, 101-110.\n\nParallel efficient sparse matrix-matrix multiplication on multicore platforms. Ali Md Mostofa, Patwary, Nadathur Rajagopalan, Narayanan Satish, Jongsoo Sundaram, Park, J Michael, Anderson, Gautam Satya, Dipankar Vadlamudi, Das, G Sergey, Pudov, O Vadim, Pradeep Pirogov, Dubey, ISC. SpringerMd Mostofa Ali Patwary, Nadathur Rajagopalan Satish, Narayanan Sundaram, Jongsoo Park, Michael J Anderson, Satya Gautam Vadlamudi, Dipankar Das, Sergey G Pudov, Vadim O Pirogov, and Pradeep Dubey. 2015. Parallel efficient sparse matrix-matrix multiplication on multicore platforms. In ISC. Springer, 48-57.\n\nNear linear time algorithm to detect community structures in large-scale networks. Usha Nandini Raghavan, R\u00e9ka Albert, Soundar Kumara, Physical review E. 7636106Usha Nandini Raghavan, R\u00e9ka Albert, and Soundar Kumara. 2007. Near linear time algorithm to detect community structures in large-scale networks. Physical review E 76, 3 (2007), 036106.\n\nEfficient Hash Probes on Modern Processors. A Kenneth, Ross, ICDE. IEEE. Kenneth A Ross. 2007. Efficient Hash Probes on Modern Processors. In ICDE. IEEE, 1297-1301.\n\nViennaCL-Linear Algebra Library for Multi-and Many-Core Architectures. Karl Rupp, Philippe Tillet, Florian Rudolf, Josef Weinbub, Andreas Morhammer, Tibor Grasser, Ansgar J\u00fcngel, and Siegfried Selberherr. 38Karl Rupp, Philippe Tillet, Florian Rudolf, Josef Weinbub, Andreas Morhammer, Tibor Grasser, Ansgar J\u00fcngel, and Siegfried Selberherr. 2016. ViennaCL-Linear Algebra Library for Multi-and Many-Core Architectures. SIAM Journal on Scientific Computing 38, 5 (2016), S412-S439.\n\nAn Interactive System for Combinatorial Scientific Computing with an Emphasis on Programmer Productivity. B Viral, Shah, Santa BarbaraD. Dissertation. University of CaliforniaViral B. Shah. 2007. An Interactive System for Combinatorial Scientific Computing with an Emphasis on Programmer Productivity. Ph.D. Dissertation. University of California, Santa Barbara.\n\nCaching-efficient multithreaded fast multiplication of sparse matrices. D Peter, Kanad Sulatycke, Ghose, IPPS/SPDP. IEEEPeter D Sulatycke and Kanad Ghose. 1998. Caching-efficient multithreaded fast multiplication of sparse matrices. In IPPS/SPDP. IEEE.\n", "annotations": {"author": "[{\"end\":154,\"start\":109},{\"end\":264,\"start\":155},{\"end\":290,\"start\":265},{\"end\":318,\"start\":291},{\"end\":475,\"start\":319},{\"end\":538,\"start\":476}]", "publisher": null, "author_last_name": "[{\"end\":124,\"start\":116},{\"end\":171,\"start\":163},{\"end\":276,\"start\":272},{\"end\":302,\"start\":297}]", "author_first_name": "[{\"end\":115,\"start\":109},{\"end\":162,\"start\":155},{\"end\":271,\"start\":265},{\"end\":296,\"start\":291}]", "author_affiliation": "[{\"end\":263,\"start\":173},{\"end\":474,\"start\":320},{\"end\":537,\"start\":477}]", "title": "[{\"end\":88,\"start\":1},{\"end\":626,\"start\":539}]", "venue": "[{\"end\":641,\"start\":628}]", "abstract": "[{\"end\":3058,\"start\":1537}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3296,\"start\":3292},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3324,\"start\":3321},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3347,\"start\":3344},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3371,\"start\":3367},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3402,\"start\":3398},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3431,\"start\":3428},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3471,\"start\":3468},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3511,\"start\":3507},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3749,\"start\":3746},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3782,\"start\":3778},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3840,\"start\":3837},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4056,\"start\":4052},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4420,\"start\":4417},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4431,\"start\":4427},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4466,\"start\":4462},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4592,\"start\":4589},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4595,\"start\":4592},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4598,\"start\":4595},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4601,\"start\":4598},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4604,\"start\":4601},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6194,\"start\":6190},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6395,\"start\":6391},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6398,\"start\":6395},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7274,\"start\":7270},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7277,\"start\":7274},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8542,\"start\":8538},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10620,\"start\":10616},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10925,\"start\":10921},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11017,\"start\":11013},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11678,\"start\":11674},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11681,\"start\":11678},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12073,\"start\":12069},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12129,\"start\":12125},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12220,\"start\":12216},{\"end\":12577,\"start\":12574},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13025,\"start\":13022},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13028,\"start\":13025},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13737,\"start\":13733},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13789,\"start\":13786},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14970,\"start\":14969},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17964,\"start\":17960},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20563,\"start\":20559},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21777,\"start\":21774},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24249,\"start\":24245},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25259,\"start\":25255},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25397,\"start\":25394},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25400,\"start\":25397},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25403,\"start\":25400},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25458,\"start\":25455},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31906,\"start\":31904},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31908,\"start\":31906},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31910,\"start\":31908},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":31912,\"start\":31910},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31915,\"start\":31912},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31918,\"start\":31915},{\"end\":31921,\"start\":31918},{\"end\":31924,\"start\":31921},{\"end\":31928,\"start\":31924},{\"end\":31932,\"start\":31928},{\"end\":31936,\"start\":31932},{\"end\":31940,\"start\":31936},{\"end\":31943,\"start\":31940},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":34019,\"start\":34015},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35643,\"start\":35640},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":36402,\"start\":36399},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":43446,\"start\":43442}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39086,\"start\":38836},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39389,\"start\":39087},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39430,\"start\":39390},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39448,\"start\":39431},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39614,\"start\":39449},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39972,\"start\":39615},{\"attributes\":{\"id\":\"fig_6\"},\"end\":40191,\"start\":39973},{\"attributes\":{\"id\":\"fig_7\"},\"end\":40433,\"start\":40192},{\"attributes\":{\"id\":\"fig_8\"},\"end\":40614,\"start\":40434},{\"attributes\":{\"id\":\"fig_9\"},\"end\":40957,\"start\":40615},{\"attributes\":{\"id\":\"fig_10\"},\"end\":41077,\"start\":40958},{\"attributes\":{\"id\":\"fig_11\"},\"end\":41173,\"start\":41078},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":41463,\"start\":41174},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":42722,\"start\":41464},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":43336,\"start\":42723}]", "paragraph": "[{\"end\":3841,\"start\":3074},{\"end\":4467,\"start\":3843},{\"end\":5454,\"start\":4469},{\"end\":6223,\"start\":5456},{\"end\":7044,\"start\":6225},{\"end\":7665,\"start\":7046},{\"end\":8077,\"start\":7667},{\"end\":10000,\"start\":8109},{\"end\":10991,\"start\":10212},{\"end\":11585,\"start\":10993},{\"end\":12306,\"start\":11587},{\"end\":13093,\"start\":12308},{\"end\":13989,\"start\":13151},{\"end\":15805,\"start\":14513},{\"end\":17593,\"start\":15856},{\"end\":17868,\"start\":17623},{\"end\":18772,\"start\":17885},{\"end\":20052,\"start\":19067},{\"end\":21728,\"start\":20075},{\"end\":22320,\"start\":21745},{\"end\":22686,\"start\":22322},{\"end\":23245,\"start\":22688},{\"end\":24160,\"start\":23273},{\"end\":25061,\"start\":24162},{\"end\":25440,\"start\":25100},{\"end\":26080,\"start\":25442},{\"end\":26923,\"start\":26109},{\"end\":27801,\"start\":27008},{\"end\":28756,\"start\":27820},{\"end\":29299,\"start\":28781},{\"end\":30663,\"start\":29321},{\"end\":32289,\"start\":30710},{\"end\":33487,\"start\":32349},{\"end\":33859,\"start\":33489},{\"end\":35117,\"start\":33917},{\"end\":36308,\"start\":35149},{\"end\":37340,\"start\":36330},{\"end\":38115,\"start\":37356},{\"end\":38835,\"start\":38117}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10211,\"start\":10001},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14457,\"start\":13990},{\"attributes\":{\"id\":\"formula_2\"},\"end\":19066,\"start\":18773}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":12914,\"start\":12907},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":13988,\"start\":13981},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25439,\"start\":25432},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26236,\"start\":26229},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":32441,\"start\":32434},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":34577,\"start\":34570}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3072,\"start\":3060},{\"attributes\":{\"n\":\"2\"},\"end\":8107,\"start\":8080},{\"attributes\":{\"n\":\"3\"},\"end\":13149,\"start\":13096},{\"attributes\":{\"n\":\"3.1\"},\"end\":14511,\"start\":14459},{\"attributes\":{\"n\":\"3.2\"},\"end\":15854,\"start\":15808},{\"attributes\":{\"n\":\"3.3\"},\"end\":17621,\"start\":17596},{\"attributes\":{\"n\":\"3.3.1\"},\"end\":17883,\"start\":17871},{\"attributes\":{\"n\":\"3.3.2\"},\"end\":20073,\"start\":20055},{\"attributes\":{\"n\":\"3.3.3\"},\"end\":21743,\"start\":21731},{\"attributes\":{\"n\":\"3.4\"},\"end\":23271,\"start\":23248},{\"attributes\":{\"n\":\"4\"},\"end\":25098,\"start\":25064},{\"attributes\":{\"n\":\"4.2\"},\"end\":26107,\"start\":26083},{\"attributes\":{\"n\":\"4.3\"},\"end\":26955,\"start\":26926},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":27006,\"start\":26958},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":27818,\"start\":27804},{\"attributes\":{\"n\":\"5\"},\"end\":28779,\"start\":28759},{\"attributes\":{\"n\":\"5.1\"},\"end\":29319,\"start\":29302},{\"attributes\":{\"n\":\"5.1.1\"},\"end\":30687,\"start\":30666},{\"attributes\":{\"n\":\"5.1.2\"},\"end\":30708,\"start\":30690},{\"attributes\":{\"n\":\"5.1.3\"},\"end\":32318,\"start\":32292},{\"attributes\":{\"n\":\"5.1.4\"},\"end\":32347,\"start\":32321},{\"attributes\":{\"n\":\"5.1.5\"},\"end\":33915,\"start\":33862},{\"attributes\":{\"n\":\"5.2\"},\"end\":35147,\"start\":35120},{\"attributes\":{\"n\":\"5.3\"},\"end\":36328,\"start\":36311},{\"attributes\":{\"n\":\"6\"},\"end\":37354,\"start\":37343},{\"end\":38847,\"start\":38837},{\"end\":39098,\"start\":39088},{\"end\":39401,\"start\":39391},{\"end\":39442,\"start\":39432},{\"end\":39460,\"start\":39450},{\"end\":39626,\"start\":39616},{\"end\":39984,\"start\":39974},{\"end\":40213,\"start\":40193},{\"end\":40456,\"start\":40435},{\"end\":40638,\"start\":40616},{\"end\":40970,\"start\":40959},{\"end\":41090,\"start\":41079},{\"end\":41184,\"start\":41175},{\"end\":41474,\"start\":41465},{\"end\":42733,\"start\":42724}]", "table": "[{\"end\":41463,\"start\":41231},{\"end\":42722,\"start\":41524},{\"end\":43336,\"start\":42782}]", "figure_caption": "[{\"end\":39086,\"start\":38849},{\"end\":39389,\"start\":39100},{\"end\":39430,\"start\":39403},{\"end\":39448,\"start\":39444},{\"end\":39614,\"start\":39462},{\"end\":39972,\"start\":39628},{\"end\":40191,\"start\":39986},{\"end\":40433,\"start\":40216},{\"end\":40614,\"start\":40460},{\"end\":40957,\"start\":40643},{\"end\":41077,\"start\":40973},{\"end\":41173,\"start\":41093},{\"end\":41231,\"start\":41186},{\"end\":41524,\"start\":41476},{\"end\":42782,\"start\":42735}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4178,\"start\":4170},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8417,\"start\":8409},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9699,\"start\":9693},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15488,\"start\":15480},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16434,\"start\":16426},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17974,\"start\":17966},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":24485,\"start\":24477},{\"end\":27114,\"start\":27106},{\"end\":27946,\"start\":27938},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":28246,\"start\":28238},{\"end\":29738,\"start\":29730},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30858,\"start\":30849},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31778,\"start\":31769},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32459,\"start\":32450},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34470,\"start\":34461},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35902,\"start\":35893},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36231,\"start\":36222},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36947,\"start\":36938},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37495,\"start\":37486}]", "bib_author_first_name": "[{\"end\":43543,\"start\":43542},{\"end\":43562,\"start\":43557},{\"end\":43564,\"start\":43563},{\"end\":43825,\"start\":43822},{\"end\":43857,\"start\":43849},{\"end\":44164,\"start\":44158},{\"end\":44175,\"start\":44171},{\"end\":44190,\"start\":44185},{\"end\":44203,\"start\":44198},{\"end\":44217,\"start\":44212},{\"end\":44231,\"start\":44227},{\"end\":44247,\"start\":44242},{\"end\":44262,\"start\":44256},{\"end\":44654,\"start\":44648},{\"end\":44666,\"start\":44661},{\"end\":44678,\"start\":44674},{\"end\":44941,\"start\":44935},{\"end\":44949,\"start\":44948},{\"end\":44968,\"start\":44960},{\"end\":44970,\"start\":44969},{\"end\":44989,\"start\":44984},{\"end\":44991,\"start\":44990},{\"end\":45007,\"start\":45002},{\"end\":45386,\"start\":45382},{\"end\":45407,\"start\":45396},{\"end\":45425,\"start\":45417},{\"end\":45755,\"start\":45748},{\"end\":45766,\"start\":45762},{\"end\":45791,\"start\":45780},{\"end\":46030,\"start\":46025},{\"end\":46042,\"start\":46038},{\"end\":46044,\"start\":46043},{\"end\":46317,\"start\":46309},{\"end\":46337,\"start\":46331},{\"end\":46352,\"start\":46344},{\"end\":46761,\"start\":46755},{\"end\":46774,\"start\":46770},{\"end\":46788,\"start\":46782},{\"end\":47031,\"start\":47030},{\"end\":47169,\"start\":47168},{\"end\":47317,\"start\":47316},{\"end\":47332,\"start\":47327},{\"end\":47644,\"start\":47638},{\"end\":47662,\"start\":47653},{\"end\":47682,\"start\":47670},{\"end\":47956,\"start\":47955},{\"end\":47973,\"start\":47968},{\"end\":47975,\"start\":47974},{\"end\":48229,\"start\":48224},{\"end\":48252,\"start\":48246},{\"end\":48531,\"start\":48527},{\"end\":48533,\"start\":48532},{\"end\":48548,\"start\":48543},{\"end\":48565,\"start\":48560},{\"end\":48567,\"start\":48566},{\"end\":48806,\"start\":48801},{\"end\":48822,\"start\":48815},{\"end\":48835,\"start\":48831},{\"end\":48839,\"start\":48836},{\"end\":48854,\"start\":48848},{\"end\":48869,\"start\":48866},{\"end\":49480,\"start\":49473},{\"end\":49491,\"start\":49485},{\"end\":49505,\"start\":49498},{\"end\":49514,\"start\":49510},{\"end\":49767,\"start\":49760},{\"end\":49778,\"start\":49773},{\"end\":50010,\"start\":50005},{\"end\":50022,\"start\":50018},{\"end\":50063,\"start\":50056},{\"end\":50316,\"start\":50312},{\"end\":50318,\"start\":50317},{\"end\":50608,\"start\":50600},{\"end\":50644,\"start\":50635},{\"end\":50662,\"start\":50657},{\"end\":51003,\"start\":50997},{\"end\":51021,\"start\":51014},{\"end\":51038,\"start\":51032},{\"end\":51050,\"start\":51045},{\"end\":51397,\"start\":51391},{\"end\":51413,\"start\":51408},{\"end\":51429,\"start\":51422},{\"end\":51720,\"start\":51717},{\"end\":51773,\"start\":51764},{\"end\":51789,\"start\":51782},{\"end\":51807,\"start\":51806},{\"end\":51833,\"start\":51827},{\"end\":51849,\"start\":51841},{\"end\":51867,\"start\":51866},{\"end\":51884,\"start\":51883},{\"end\":51899,\"start\":51892},{\"end\":52324,\"start\":52320},{\"end\":52347,\"start\":52343},{\"end\":52363,\"start\":52356},{\"end\":52629,\"start\":52628},{\"end\":52825,\"start\":52821},{\"end\":52840,\"start\":52832},{\"end\":52856,\"start\":52849},{\"end\":52870,\"start\":52865},{\"end\":52887,\"start\":52880},{\"end\":52904,\"start\":52899},{\"end\":53338,\"start\":53337},{\"end\":53668,\"start\":53667},{\"end\":53681,\"start\":53676}]", "bib_author_last_name": "[{\"end\":43540,\"start\":43523},{\"end\":43555,\"start\":43544},{\"end\":43568,\"start\":43565},{\"end\":43576,\"start\":43570},{\"end\":43847,\"start\":43826},{\"end\":43861,\"start\":43858},{\"end\":43866,\"start\":43863},{\"end\":44169,\"start\":44165},{\"end\":44183,\"start\":44176},{\"end\":44196,\"start\":44191},{\"end\":44210,\"start\":44204},{\"end\":44225,\"start\":44218},{\"end\":44240,\"start\":44232},{\"end\":44254,\"start\":44248},{\"end\":44271,\"start\":44263},{\"end\":44659,\"start\":44655},{\"end\":44672,\"start\":44667},{\"end\":44686,\"start\":44679},{\"end\":44946,\"start\":44942},{\"end\":44958,\"start\":44950},{\"end\":44982,\"start\":44971},{\"end\":45000,\"start\":44992},{\"end\":45016,\"start\":45008},{\"end\":45023,\"start\":45018},{\"end\":45394,\"start\":45387},{\"end\":45415,\"start\":45408},{\"end\":45428,\"start\":45426},{\"end\":45760,\"start\":45756},{\"end\":45778,\"start\":45767},{\"end\":45796,\"start\":45792},{\"end\":46036,\"start\":46031},{\"end\":46052,\"start\":46045},{\"end\":46329,\"start\":46318},{\"end\":46342,\"start\":46338},{\"end\":46362,\"start\":46353},{\"end\":46768,\"start\":46762},{\"end\":46780,\"start\":46775},{\"end\":46793,\"start\":46789},{\"end\":47039,\"start\":47032},{\"end\":47046,\"start\":47041},{\"end\":47177,\"start\":47170},{\"end\":47184,\"start\":47179},{\"end\":47325,\"start\":47318},{\"end\":47338,\"start\":47333},{\"end\":47342,\"start\":47340},{\"end\":47651,\"start\":47645},{\"end\":47668,\"start\":47663},{\"end\":47695,\"start\":47683},{\"end\":47966,\"start\":47957},{\"end\":47981,\"start\":47976},{\"end\":47987,\"start\":47983},{\"end\":48244,\"start\":48230},{\"end\":48258,\"start\":48253},{\"end\":48269,\"start\":48260},{\"end\":48541,\"start\":48534},{\"end\":48558,\"start\":48549},{\"end\":48572,\"start\":48568},{\"end\":48813,\"start\":48807},{\"end\":48829,\"start\":48823},{\"end\":48846,\"start\":48840},{\"end\":48864,\"start\":48855},{\"end\":48877,\"start\":48870},{\"end\":49247,\"start\":49231},{\"end\":49483,\"start\":49481},{\"end\":49496,\"start\":49492},{\"end\":49508,\"start\":49506},{\"end\":49519,\"start\":49515},{\"end\":49771,\"start\":49768},{\"end\":49785,\"start\":49779},{\"end\":50016,\"start\":50011},{\"end\":50054,\"start\":50023},{\"end\":50074,\"start\":50064},{\"end\":50327,\"start\":50319},{\"end\":50633,\"start\":50609},{\"end\":50655,\"start\":50645},{\"end\":50668,\"start\":50663},{\"end\":51012,\"start\":51004},{\"end\":51030,\"start\":51022},{\"end\":51043,\"start\":51039},{\"end\":51056,\"start\":51051},{\"end\":51406,\"start\":51398},{\"end\":51420,\"start\":51414},{\"end\":51438,\"start\":51430},{\"end\":51731,\"start\":51721},{\"end\":51740,\"start\":51733},{\"end\":51762,\"start\":51742},{\"end\":51780,\"start\":51774},{\"end\":51798,\"start\":51790},{\"end\":51804,\"start\":51800},{\"end\":51815,\"start\":51808},{\"end\":51825,\"start\":51817},{\"end\":51839,\"start\":51834},{\"end\":51859,\"start\":51850},{\"end\":51864,\"start\":51861},{\"end\":51874,\"start\":51868},{\"end\":51881,\"start\":51876},{\"end\":51890,\"start\":51885},{\"end\":51907,\"start\":51900},{\"end\":51914,\"start\":51909},{\"end\":52341,\"start\":52325},{\"end\":52354,\"start\":52348},{\"end\":52370,\"start\":52364},{\"end\":52637,\"start\":52630},{\"end\":52643,\"start\":52639},{\"end\":52830,\"start\":52826},{\"end\":52847,\"start\":52841},{\"end\":52863,\"start\":52857},{\"end\":52878,\"start\":52871},{\"end\":52897,\"start\":52888},{\"end\":52912,\"start\":52905},{\"end\":53344,\"start\":53339},{\"end\":53350,\"start\":53346},{\"end\":53674,\"start\":53669},{\"end\":53691,\"start\":53682},{\"end\":53698,\"start\":53693}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":43740,\"start\":43449},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":18380075},\"end\":44074,\"start\":43742},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7755440},\"end\":44581,\"start\":44076},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2796736},\"end\":44821,\"start\":44583},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4775370},\"end\":45290,\"start\":44823},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":18274179},\"end\":45679,\"start\":45292},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":8275836},\"end\":46023,\"start\":45681},{\"attributes\":{\"id\":\"b7\"},\"end\":46264,\"start\":46025},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":16652959},\"end\":46694,\"start\":46266},{\"attributes\":{\"id\":\"b9\"},\"end\":47026,\"start\":46696},{\"attributes\":{\"id\":\"b10\"},\"end\":47124,\"start\":47028},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":60831637},\"end\":47262,\"start\":47126},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":207191190},\"end\":47550,\"start\":47264},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":22364822},\"end\":47891,\"start\":47552},{\"attributes\":{\"id\":\"b14\"},\"end\":48168,\"start\":47893},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":119805771},\"end\":48460,\"start\":48170},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":52874378},\"end\":48721,\"start\":48462},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":18099698},\"end\":49145,\"start\":48723},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7107184},\"end\":49398,\"start\":49147},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":18068730},\"end\":49678,\"start\":49400},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":206929389},\"end\":49942,\"start\":49680},{\"attributes\":{\"id\":\"b21\"},\"end\":50242,\"start\":49944},{\"attributes\":{\"id\":\"b22\"},\"end\":50505,\"start\":50244},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":27025390},\"end\":50906,\"start\":50507},{\"attributes\":{\"doi\":\"arXiv:1804.01698\",\"id\":\"b24\"},\"end\":51287,\"start\":50908},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":21005717},\"end\":51636,\"start\":51289},{\"attributes\":{\"id\":\"b26\"},\"end\":52235,\"start\":51638},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":18741059},\"end\":52582,\"start\":52237},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":2292054},\"end\":52748,\"start\":52584},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":10322795},\"end\":53229,\"start\":52750},{\"attributes\":{\"id\":\"b30\"},\"end\":53593,\"start\":53231},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":8988867},\"end\":53847,\"start\":53595}]", "bib_title": "[{\"end\":43820,\"start\":43742},{\"end\":44156,\"start\":44076},{\"end\":44646,\"start\":44583},{\"end\":44933,\"start\":44823},{\"end\":45380,\"start\":45292},{\"end\":45746,\"start\":45681},{\"end\":46307,\"start\":46266},{\"end\":46753,\"start\":46696},{\"end\":47166,\"start\":47126},{\"end\":47314,\"start\":47264},{\"end\":47636,\"start\":47552},{\"end\":47953,\"start\":47893},{\"end\":48222,\"start\":48170},{\"end\":48525,\"start\":48462},{\"end\":48799,\"start\":48723},{\"end\":49229,\"start\":49147},{\"end\":49471,\"start\":49400},{\"end\":49758,\"start\":49680},{\"end\":50598,\"start\":50507},{\"end\":51389,\"start\":51289},{\"end\":52318,\"start\":52237},{\"end\":52626,\"start\":52584},{\"end\":52819,\"start\":52750},{\"end\":53665,\"start\":53595}]", "bib_author": "[{\"end\":43542,\"start\":43523},{\"end\":43557,\"start\":43542},{\"end\":43570,\"start\":43557},{\"end\":43578,\"start\":43570},{\"end\":43849,\"start\":43822},{\"end\":43863,\"start\":43849},{\"end\":43868,\"start\":43863},{\"end\":44171,\"start\":44158},{\"end\":44185,\"start\":44171},{\"end\":44198,\"start\":44185},{\"end\":44212,\"start\":44198},{\"end\":44227,\"start\":44212},{\"end\":44242,\"start\":44227},{\"end\":44256,\"start\":44242},{\"end\":44273,\"start\":44256},{\"end\":44661,\"start\":44648},{\"end\":44674,\"start\":44661},{\"end\":44688,\"start\":44674},{\"end\":44948,\"start\":44935},{\"end\":44960,\"start\":44948},{\"end\":44984,\"start\":44960},{\"end\":45002,\"start\":44984},{\"end\":45018,\"start\":45002},{\"end\":45025,\"start\":45018},{\"end\":45396,\"start\":45382},{\"end\":45417,\"start\":45396},{\"end\":45430,\"start\":45417},{\"end\":45762,\"start\":45748},{\"end\":45780,\"start\":45762},{\"end\":45798,\"start\":45780},{\"end\":46038,\"start\":46025},{\"end\":46054,\"start\":46038},{\"end\":46331,\"start\":46309},{\"end\":46344,\"start\":46331},{\"end\":46364,\"start\":46344},{\"end\":46770,\"start\":46755},{\"end\":46782,\"start\":46770},{\"end\":46795,\"start\":46782},{\"end\":47041,\"start\":47030},{\"end\":47048,\"start\":47041},{\"end\":47179,\"start\":47168},{\"end\":47186,\"start\":47179},{\"end\":47327,\"start\":47316},{\"end\":47340,\"start\":47327},{\"end\":47344,\"start\":47340},{\"end\":47653,\"start\":47638},{\"end\":47670,\"start\":47653},{\"end\":47697,\"start\":47670},{\"end\":47968,\"start\":47955},{\"end\":47983,\"start\":47968},{\"end\":47989,\"start\":47983},{\"end\":48246,\"start\":48224},{\"end\":48260,\"start\":48246},{\"end\":48271,\"start\":48260},{\"end\":48543,\"start\":48527},{\"end\":48560,\"start\":48543},{\"end\":48574,\"start\":48560},{\"end\":48815,\"start\":48801},{\"end\":48831,\"start\":48815},{\"end\":48848,\"start\":48831},{\"end\":48866,\"start\":48848},{\"end\":48879,\"start\":48866},{\"end\":49249,\"start\":49231},{\"end\":49485,\"start\":49473},{\"end\":49498,\"start\":49485},{\"end\":49510,\"start\":49498},{\"end\":49521,\"start\":49510},{\"end\":49773,\"start\":49760},{\"end\":49787,\"start\":49773},{\"end\":50018,\"start\":50005},{\"end\":50056,\"start\":50018},{\"end\":50076,\"start\":50056},{\"end\":50329,\"start\":50312},{\"end\":50635,\"start\":50600},{\"end\":50657,\"start\":50635},{\"end\":50670,\"start\":50657},{\"end\":51014,\"start\":50997},{\"end\":51032,\"start\":51014},{\"end\":51045,\"start\":51032},{\"end\":51058,\"start\":51045},{\"end\":51408,\"start\":51391},{\"end\":51422,\"start\":51408},{\"end\":51440,\"start\":51422},{\"end\":51733,\"start\":51717},{\"end\":51742,\"start\":51733},{\"end\":51764,\"start\":51742},{\"end\":51782,\"start\":51764},{\"end\":51800,\"start\":51782},{\"end\":51806,\"start\":51800},{\"end\":51817,\"start\":51806},{\"end\":51827,\"start\":51817},{\"end\":51841,\"start\":51827},{\"end\":51861,\"start\":51841},{\"end\":51866,\"start\":51861},{\"end\":51876,\"start\":51866},{\"end\":51883,\"start\":51876},{\"end\":51892,\"start\":51883},{\"end\":51909,\"start\":51892},{\"end\":51916,\"start\":51909},{\"end\":52343,\"start\":52320},{\"end\":52356,\"start\":52343},{\"end\":52372,\"start\":52356},{\"end\":52639,\"start\":52628},{\"end\":52645,\"start\":52639},{\"end\":52832,\"start\":52821},{\"end\":52849,\"start\":52832},{\"end\":52865,\"start\":52849},{\"end\":52880,\"start\":52865},{\"end\":52899,\"start\":52880},{\"end\":52914,\"start\":52899},{\"end\":53346,\"start\":53337},{\"end\":53352,\"start\":53346},{\"end\":53676,\"start\":53667},{\"end\":53693,\"start\":53676},{\"end\":53700,\"start\":53693}]", "bib_venue": "[{\"end\":43521,\"start\":43449},{\"end\":43871,\"start\":43868},{\"end\":44309,\"start\":44273},{\"end\":44694,\"start\":44688},{\"end\":45047,\"start\":45025},{\"end\":45466,\"start\":45430},{\"end\":45834,\"start\":45798},{\"end\":46126,\"start\":46054},{\"end\":46438,\"start\":46364},{\"end\":46843,\"start\":46795},{\"end\":47190,\"start\":47186},{\"end\":47392,\"start\":47344},{\"end\":47709,\"start\":47697},{\"end\":48013,\"start\":47989},{\"end\":48296,\"start\":48271},{\"end\":48578,\"start\":48574},{\"end\":48915,\"start\":48879},{\"end\":49257,\"start\":49249},{\"end\":49527,\"start\":49521},{\"end\":49798,\"start\":49787},{\"end\":50003,\"start\":49944},{\"end\":50310,\"start\":50244},{\"end\":50693,\"start\":50670},{\"end\":50995,\"start\":50908},{\"end\":51450,\"start\":51440},{\"end\":51715,\"start\":51638},{\"end\":52389,\"start\":52372},{\"end\":52655,\"start\":52645},{\"end\":52953,\"start\":52914},{\"end\":53335,\"start\":53231},{\"end\":53709,\"start\":53700},{\"end\":43890,\"start\":43873},{\"end\":46499,\"start\":46440}]"}}}, "year": 2023, "month": 12, "day": 17}