{"id": 247446806, "updated": "2023-10-05 15:59:28.084", "metadata": {"title": "Chart-to-Text: A Large-Scale Benchmark for Chart Summarization", "authors": "[{\"first\":\"Shankar\",\"last\":\"Kantharaj\",\"middle\":[]},{\"first\":\"Rixie Tiffany\",\"last\":\"Leong\",\"middle\":[]},{\"first\":\"Xiang\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Ahmed\",\"last\":\"Masry\",\"middle\":[]},{\"first\":\"Megh\",\"last\":\"Thakkar\",\"middle\":[]},{\"first\":\"Enamul\",\"last\":\"Hoque\",\"middle\":[]},{\"first\":\"Shafiq\",\"last\":\"Joty\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Charts are commonly used for exploring data and communicating insights. Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts. We present Chart-to-text, a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types. We explain the dataset construction process and analyze the datasets. We also introduce a number of state-of-the-art neural models as baselines that utilize image captioning and data-to-text generation techniques to tackle two problem variations: one assumes the underlying data table of the chart is available while the other needs to extract data from chart images. Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2203.06486", "mag": null, "acl": "2022.acl-long.277", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/KantharajLLMTHJ22", "doi": "10.18653/v1/2022.acl-long.277"}}, "content": {"source": {"pdf_hash": "7fc180c890405c4eebf85bdfb93c69cda6a19ca9", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2022.acl-long.277.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "3b7982a6eab8597dfb0db88b608765317ff9a885", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7fc180c890405c4eebf85bdfb93c69cda6a19ca9.txt", "contents": "\nChart-to-Text: A Large-Scale Benchmark for Chart Summarization\nLong PapersCopyright Long PapersMay 22-27, 2022\n\nShankar Kantharaj shankark@yorku.ca \nYork University\nCanada\n\nRixie Tiffany \nKo Leong \nNanyang Technological University\nSingapore\n\nXiang Lin \nNanyang Technological University\nSingapore\n\nAhmed Masry masry20@yorku.ca \nYork University\nCanada\n\nMegh Thakkar \nNanyang Technological University\nSingapore\n\nEnamul Hoque enamulh@yorku.ca \nYork University\nCanada\n\n\u2663 \nShafiq Joty srjoty@ntu.edu.sgmegh.1211@gmail.com \nNanyang Technological University\nSingapore\n\nSalesforce Research Asia\nSingapore\n\nChart-to-Text: A Large-Scale Benchmark for Chart Summarization\n\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nthe 60th Annual Meeting of the Association for Computational LinguisticsLong Papers1May 22-27, 2022\nCharts are commonly used for exploring data and communicating insights. Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts. We present Chart-totext, a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types. We explain the dataset construction process and analyze the datasets. We also introduce a number of stateof-the-art neural models as baselines that utilize image captioning and data-to-text generation techniques to tackle two problem variations: one assumes the underlying data table of the chart is available while the other needs to extract data from chart images. Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts. * Equal contribution. Listing order is based on the alphabetical ordering of author surnames.Gold: In 2019, Singapore imported approximately 236.8 billion Singapore dollars worth of machinery and equipment, making it the country's largest import commodity by value. This was followed by the import of mineral fuels and lubricants, valued at 102.7 billion Singapore dollars. TAB-T5: Machinery and equipment was the most valuable commodity for Singapore in 2019, with an import value of 236.8 billion Singapore dollars. Mineral fuels and lubricants were the second most valuable commodity for Singapore, with an import value of 102.7 billion Singapore dollars.\n\nIntroduction\n\nData visualizations such as bar charts, line charts, and pie charts are very popular for presenting quantitative data. Often people use such charts to get important insights from data and make informed decisions. However, it is well-known that inferring key insights from the charts can be quite challenging and time-consuming, as it may require a lot of cognitive and perceptual efforts (P\u00e9rez-Echeverr\u00eda et al., 2018;Whitaker and Jacobbe, 2017).\n\nAutomatic chart summarization is a task where the goal is to explain a chart and summarize key takeaways from it in natural language. Chart summarization has several key benefits and potential applications. First, chart summaries can help people identify key insights from charts that they might have missed otherwise. In a study on a chart corpus, Carberry et al. (2006) found that chart authors often failed to convey key insights from charts in their corresponding textual captions. Thus, automatic summarization could help authors write effective reports and articles on data facts by suggesting explanatory texts. Similarly, readers could benefit from such summaries, as studies have found that captions help readers find important points by explaining visually prominent features in charts (Kim et al., 2021). Chart summarization offers another important benefit of making charts more accessible to people who are visually impaired since they can use screen readers to understand what is being presented in the chart (Ferres et al., 2013). Finally, the generated summaries can be leveraged for indexing documents containing charts to improve information retrieval algorithms (Li et al., 2013).\n\nDespite its numerous benefits and applications, the chart summarization problem has not received much attention in the NLP community. Early approaches relied on template-based text generation methods that combine statistical techniques and planning-based architecture (Reiter, 2007) to generate captions from bar and line charts (Fasciano and Lapalme, 1996;Mittal et al., 1998;Green et al., 2004;Demir et al., 2012). Recently, researchers considered data-driven neural models for describing tabular data (Mei et al., 2016;Gong et al., 2019). However, compared to tables, charts serve a different communication goal, and so is the chart-to-text problem. Unlike tables which simply list raw data, charts create visual representation of data that can draw a reader's attention to various prominent features such as trends and outliers (Kim et al., 2021). For example, a line chart may depict an important trend whereas a scatterplot may visually communicate correlations and outliers. Existing table-to-text approaches are not designed to explain such visually salient chart features in summaries.\n\nThere are two main impediments to addressing the chart summarization task. First, the lack of large-scale datasets makes it difficult to solve the task using data-driven neural models. Second, there are no strong baselines that utilize the latest advances in neural text generation tasks. Obeid and Hoque (2020) made an initial attempt to address this problem with a dataset and a model that utilizes a Transformer (Vaswani et al., 2017) architecture. However, their dataset was built by collecting a small set of charts (8,305) from a single source covering only two types of charts (bar and line). Also, their approach does not exploit the recent advances in large-scale language model pretraining, which has been shown to be very beneficial for many vision and language tasks (Devlin et al., 2019;Touvron et al., 2021). To our knowledge, there is no large-scale benchmark with a wider range of topics from multiple sources, covering many different chart types, and with models that employ large-scale pretraining.\n\nIn this work, we present a large-scale benchmark for chart-to-text with two datasets consisting of 44,096 charts covering a broad range of topics and a variety of chart types. We introduce two variations of the problem. The first variation assumes that the underlying data table of a chart is available, while the other introduces a more challenging and realistic scenario by assuming that the chart is in image format and the underlying table is not available. These two problem scenarios motivated us to adapt a variety of state-of-the-art models that combine computer vision and natural language generation techniques as strong baselines; see Fig. 1 for a sample model output.\n\nOur primary contributions are: (i) a new largescale benchmark covering a wide range of topics and chart types; (ii) a set of state-of-the-art neural models which can act as a starting point for other researchers to expand and improve upon; and (iii) a series of automatic and human evaluations as well as in-depth qualitative analysis to identify further challenges. Our code and benchmark datasets are publicly available at https://github.com/visnlp/Chart-to-text.\n\n\nRelated Work\n\nChart Summarization Early work (Mittal et al., 1998;Ferres et al., 2013) followed a planningbased architecture (Reiter, 2007) and used templates to generate texts. These systems only describe how to read the chart rather than explain key insights conveyed by the chart. Recently, commercial systems such as Quill and Wordsmith 1 as well as research prototypes, e.g., (Cui et al., 2019) and (Srinivasan et al., 2018) computed statistics (e.g., extrema, outliers) to present facts from a dataset. Demir et al. (2012) also compute statistics to generates bar chart summaries in a bottom-up manner to simultaneously construct the discourse and sentence structures. Recently, Chen et al. (2019) used the ResNet (He et al., 2016) to encode the chart image and an LSTM decoder to create the caption.\n\nA key limitation of the above bodies of work is that sentences are generated using predefined templates, which may lack generality and offer little variation in terms of reported insights, grammatical styles and lexical choices compared to datadriven models. Moving beyond template-based summaries, Obeid and Hoque (2020) adapted a transformer-based model on a dataset of 8,305 charts, while Spreafico and Carenini (2020) applied an LSTM based encoder-decoder model on a dataset of 306 chart summaries. Both studies used much smaller datasets and did not consider the computer vision aspects of the problem. Hsu et al. (2021) recently use a CNN+LSTM based image captioning model for scientific figure captioning. In contrast, we focus on the generic chart-to-text problem and train several neural models that combine computer vision and data2text generation.\n\nData2text Generation Data2text models generate a descriptive summary for a table of records. They have been used for various domain-specific tasks such as summarizing sports data (Barzilay and Lapata, 2005;Wiseman et al., 2017), weather-forecast data (Reiter et al., 2005), recipe generation (Yang et al., 2017) and biography generation (Lebret et al., 2016) as well as open-domain tasks (Parikh et al., 2020;Chen et al., 2020a). Recent methods have primarily used an LSTM-based encoder-decoder architecture (Mei et al., 2016;Lebret et al., 2016;Wiseman et al., 2017). Gong et al. (2019) found that transformers (Vaswani et al., 2017) yielded more fluent and coherent outputs compared to their LSTM counterparts. Others focused on controlling the structure of the summary using a planning approach (Su et al., 2021) as well as generating facts by preforming logical inference over the given table (Chen et al., 2020a,b).\n\nImage Captioning There has been swift progress in image captioning largely due to the availability of large-scale datasets (Agrawal et al., 2019;Chen et al., 2015). Zhang et al. (2021) developed an object detection model to summarize objects in images while Sidorov et al. (2020) utilized texts extracted from images using OCR to generate captions. Unlike images with real-world objects and scenes, charts have marks (e.g., bars, lines) that map quantitative data. This makes the chart-to-text problem different from image captioning.\n\n\nChart-to-text Datasets\n\nAfter searching through various sources including news sites, textbooks, and websites containing data facts, we found two suitable sources with sufficiently large numbers and varieties of charts with textual descriptions as we describe below.\n\n\nData Collection\n\n\u2022 Statista Statista (statista.com) is an online platform that regularly publishes charts on a wide range of topics including economics, market and opinion research. We crawled 34,810 publicly accessible webpages in December 2020, yielding a total of 34,811 charts. For each chart, we took a screenshot of the chart image, downloaded the data table, the title, axis labels and the human-written descriptions about the chart. We classified the charts into two groups based on the number of columns in their underlying data tables: Data tables of simple charts have only two columns, whereas complex charts involve at least three columns (e.g., stacked or group bar charts, line charts with multiple lines).\n\n\u2022 Pew The Pew Research (pewresearch.org) publishes data-driven articles about social issues, pub-lic opinion and demographic trends. The articles are often accompanied by multiple charts along with high-quality descriptions written by professional editors. We scraped 3,999 publicly accessible pages in January 2021, which gave a total of 9,285 charts. Unlike Statista, the Pew reports do not provide the underlying data tables for most of the charts. Among 9,285 charts, only 143 have underlying data tables. For each chart, we downloaded the chart image, the surrounding paragraphs and the alternative text associated with the image (using the alt attribute), if it was available. Like a title, the alt text often gives a very short chart description. Finally, we classified the charts into simple and complex manually since underlying data tables were unavailable.\n\n\nData Annotation\n\nBelow we describe two main steps of the data annotation process for each chart: (i) identify the relevant summary, and (ii) extract data. Additional details of these steps are provided in Appendix A.1.\n\n\u2022 Statista We chose the first part of the text (from the chart icon to the next heading) as the chart summary. This is based on the observation that the first part provides a succinct summary of the chart while the remaining parts often contain background information (e.g., the history of a company).\n\nExtracting data from the Statista charts was relatively straightforward as the underlying data tables were available. However, most charts (32,660 out of 34,811) did not provide x-axis labels. To assign representative labels for them, we first used regular expressions on the cell values of such a column to see if it represents common entities (e.g., year, location). Still, there were 7,170 missing labels remaining. We then applied the Wikidata knowledge base (Wik, 2021) to automatically derive an entity type label based on the data values plotted on x-axis. However, sometimes the resulting labels were too generic (e.g., human, business). Hence, we manually annotated each label by either accepting the entity type label, if it represents the x-axis accurately, or entering a more specific name.\n\n\u2022 Pew The annotation for Pew was more challenging as often a webpage contains many charts and paragraphs do not explicitly refer to their relevant chart. Also, most charts did not have underlying data tables. To address these challenges, we construct the dataset in three stages (Fig. 2). (i) Data extraction from chart images: We first extracted the text from the charts using CRAFT (Baek et al., 2019a,b), a state-of-the-art OCR model. We then extracted the bounding boxes of the detected texts to extract geometric features (e.g., normalized width and height of the text) and used them to train a gradient boosting classifier that categorizes the recognized text into one of the following categories: title, axis labels, legends, and data labels. Since the visual style and structure vary among chart types, we trained a separate classifier for each chart type. We manually labeled 319 examples (171 bar, 68 line, and 80 pie charts) and split them into train, validation, and test splits with 8:1:1 ratios, respectively. Our models achieved a precision of 95.0% overall and 97.6% for title classification on our test set. We then used our models to predict the text roles for the remaining charts in the Pew dataset.\n\nWe used the extracted title as the final chart title if there was no associated alt text with the chart image. If the alt text was available, we took the longer one by comparing it with the extracted title.\n\n(ii) Identification of candidate paragraphs: We observed that relevant paragraphs tend to appear in close proximity to a given chart and share some content with the chart (e.g., axis labels, data values). We first used this proximity criteria to form a list of candidate paragraphs L c . Specifically, for each chart, we selected the paragraph adjacent to the chart as well as the five paragraphs before and after it as candidates (maximum of 11 in total).\n\nNext, we used a heuristic-based approach to automatically select a subset of relevant paragraphs L r \u2282 L c . We estimated the relevance score of each paragraph in L c to its corresponding chart as rel = content \u00d7 proximity, where content takes a weighted sum of the number of tokens matched between the paragraph and the OCR-extracted text (numerical tokens were given a higher weight than lexical tokens as they were better indicators of relevance), and proximity is based on the distance between the chart and the paragraph. If rel exceeds a threshold and some minimum number of lexical and numerical tokens are matched between the paragraph and chart, we consider such a paragraph to be relevant to the chart. We set this threshold empirically and chose it to be aggressively high to prioritize precision over recall. We evaluated the efficacy of our approach against a randomly sampled set of 95 charts and 769 surrounding paragraphs and found a recall of 21.1% and a precision of 100%. Given the perfect precision score, we considered the paragraphs in L r to be relevant and to confirm the relevance of the remaining paragraphs, we performed a human study.\n\n(iii) Selection of relevant paragraphs: We asked crowdworkers on Amazon Mechanical Turk to label how relevant each paragraph is to its chart. A total of 5,478 charts and 13,237 paragraphs were annotated. Each chart received two annotations from two workers. If both workers labeled a paragraph as either completely irrelevant or relevant (partially/completely), we used the label that they agreed upon as the final label. 2 For the remaining 2,888 paragraphs where the workers disagreed, we resolved them through internal annotation.\n\n\nDataset Analysis\n\nOur chart-to-text datasets contain a diverse range of chart types (Table 1). Bar charts make up the majority of the charts both in Statista (87.9%) and Pew (67.9%) for both simple as well as stacked and group bar charts. The next most common type is line charts (10.2% in Statista and 26.4% in Pew).\n\nTo analyze the topic distribution, we extracted the topic of each chart using its webpage's metadata (e.g., breadcrumbs, meta-tags). Our datasets cover a broad range of topics including politics, society and health (see Fig. 9 in Appendix A.3).   The topics in Statista are more evenly distributed than the ones in Pew, which is dominated by U.S. Politics & Policy (45.4%). Table 2 presents basic linguistic statistics about the datasets. The summaries in Pew are about twice as long as the those in Statista, in terms of average character, token and sentence count. Unsurprisingly, complex charts generally have longer summaries than their simple counterparts.\n\nWe further analyzed the semantic content of the summaries using 100 randomly sampled chartsummary pairs from each dataset. Table 3 shows the distribution of sentences across the four main types of semantic content. 3 We notice that statistical and comparative information (e.g., min, max, avg.) is the most common type of content in both datasets. Summaries in Pew tend to report more insights that require more perceptual and cognitive efforts (e.g., trends and causal relations) which are arguably more challenging to generate compared to simple statistics. Both datasets contain comparable proportions of sentences covering contextual and domain-specific information. Unlike Statista, Pew summaries rarely explain the chart types and encodings (e.g., what do the x-and y-axes represent). We randomly selected 70%, 15%, and 15% of the datasets to create the corresponding train, test and validation splits, respectively.\n\n\nChart-to-text Baseline Models\n\nProblem Definition We consider two variations of the chart-to-text problem. In the first variation, we assume that the underlying data table of the chart is available, where the dataset can be represented as a set of 4-element tuples D = {\u27e8C, T, M, S\u27e9 n } |D| n=1 with C, T , M and S representing the chart image, data table, metadata and textual summary, respectively. For each cell in the data table T , we have the following information: (i) the string value, (ii) the row and column positions, and (iii) whether it is a header cell or not. The metadata M = (C title , C type , C labels ) consists of the title, type (e.g., bar, line) and axis labels.\n\nIn the second variation, we assume that the data table is not available which makes the problem more challenging as well as realistic because most charts online are in image format and do not have the underlying data tables. For a given input X = \u27e8C, T, M \u27e9 or \u27e8C, M \u27e9, our goal is to generate a textual description\u015c which is a good summary of the chart according to a set of evaluation measures.\n\nWe consider three categories of models to tackle the task. The first category is image captioning models, where the task is formulated as generating a textual description for the given chart image. The second category is data-to-text models, which rely on the underlying data tables of the charts to produce the corresponding descriptions. Finally, we consider a combination of vision and text models, where the models first extract the text using the CRAFT OCR model (Baek et al., 2019b) and then train with a data-to-text setup. We present three categories of models below (hyperparameter settings for all the models are provided in Appendix A.3).\n\n\nImage Captioning Models\n\nWe develop over the Show, Attend, and Tell (SAT) model (Xu et al., 2015) to probe the effectiveness of this category of models for our task. Following Xu et al. (2015), we use the ResNet50 (He et al., 2016) as the image encoder and a unidirectional LSTM (Hochreiter and Schmidhuber, 1997) as the decoder for text. As the pretrained ResNet50 model is trained on object detection tasks on ImageNet (Deng et al., 2009), directly applying it to chart images gave poor results in our experiments. Also, we do not have any object labels for the chart images to train the encoder. Hence, we employ the recently proposed self-supervised strategy called Barlow Twins (Zbontar et al., 2021) which tries to make the embedding vectors of distorted versions of an image sample to be similar, while minimizing the redundancy between the components of these vectors. It achieves state-of-the-art results for Ima-geNet classification with an accuracy gap of only 3.3% from the supervised model. We pretrain a separate ResNet50 with Barlow Twins for each of our datasets and use it as an encoder in the model.\n\n\nData-to-text Models\n\n\u2022 Chart2text (Obeid and Hoque, 2020) is an adapted transformer model for chart-to-text based on the data-to-text model of Gong et al. (2019). It takes a sequence of data records as input with each record being a set of tuples (e.g., column header, cell value, column index) and embeds them into feature vectors with positional encodings to distinguish orders (Fig. 3a). The model includes an auxiliary training objective (binary labels indicating the presence of the record in the output sequence) on the encoder to maximize the content selection score. It also implements a templating strategy of target text with data variables (e.g., cells, axis labels) to alleviate hallucination problems. Since in Pew data tables are not available, we use OCR-generated texts as inputs which are linearized and embedded into feature vectors. The bounding box information of OCR-generated data of each chart is also embedded and concatenated to the table vectors to provide positional information to the model.\n\n\u2022 Field-Infusing Model (Chen et al., 2020a) is inspired by the concept-to-text work (Lebret et al., 2016). The values in a cell are first encoded with an LSTM, which is then concatenated with the embeddings of row index and column heading. These table representations (h 1 , h 2 in Fig. 3b) are then fed into a 3-layer Transformer encoder-decoder model to generate the target summaries. Additionally, for Pew, we embed the bounding box information of the chart OCR-texts and concatenate it to the LSTM-based field representation as an auxiliary positional information to the model.\n\n\u2022 BART (Lewis et al., 2020) adopts a seq2seq Transformer architecture with denoising pretraining objectives. It is particularly pretrained to be effective for text generation tasks. For our chartto-text tasks, we flatten the data table row by row and concatenate the title with table content as the input to the encoder (Fig. 3c). In the absence of data tables, we concatenate all the OCR-texts in a top to bottom order and fed it to the model as input.\n\n\u2022 T5 (Raffel et al., 2020) is a unified seq2seq Transformer model that converts various NLP tasks into a text2text generation format. It is first pretrained with a 'fill-in-the-blank' denoising objective, where 15% of the input tokens are randomly dropped out.\n\nThe spans of consecutive dropped-out tokens are replaced by a sentinel token. The decoder then has to predict all of the dropped-out token spans, delimited by the same sentinel tokens used in the input. This is different from the pretraining objective of BART where the decoder predicts the entire original sequence (not just the dropped spans). T5 is fine-tuned with several supervised multi-task training objectives (e.g., machine translation, text summarization). We format the input in the same way as for the BART models. Specifically, we add \"translate Chart to Text: \" to the prefix of the input to mimic the pretraining process (see Fig. 3c).\n\nFor OCR-based input, we experiment with two T5 model variants. In the first variant, we concatenate all the OCR-extracted sentences from the chart image in a top to bottom order and fed it to the model as input. In the second, we modify the input to accommodate the spatial information of the detected texts. Inspired by Tan and Bansal (2019), we feed the bounding box coordinates of each detected text token into a linear layer to produce positional embeddings which are then added to their corresponding embeddings of the OCR tokens as input.\n\n\nEvaluation\n\n\nAutomatic Evaluation\n\nMeasures For automatic evaluation of the summary quality, we utilized five measures. BLEU (Post, 2018) and CIDEr  measure n-gram overlaps between the model generated text and the reference text. CIDEr computes TF-IDF weighted n-gram overlaps. BLEURT (Sellam et al., 2020) is a model-based evaluation metric that indicates to what extent the candidate is grammatical and conveys the meaning of the reference. We use BLEURT-base-128. Content Selection (CS) metric measures how well the generated summaries match the gold summaries in terms of selecting records to generate (Wiseman et al., 2017). Since both the BLEURT and CS are calculated at the sentence-level, we average these scores over the whole test set. Finally, for readability and fluency, we measure Perplexity (PPL) using a pre-trained GPT-2 Medium (Radford et al., 2019).\n\nResults In general, from the results in Table 4, we notice that large-scale unsupervised pretraining (i.e., \" -BART\", \" -T5\") helps to boost the performance significantly. In terms of the model variants, the image captioning model has failed to capture  relevant information from charts (low CS score) even though it generates fluent text (low PPL). On Statista, when the data tables are available, Chart2text and Field-Infuse models are able to extract information from the data table, but they struggle to produce texts with good quality. This could be because these models did not use any large-scale pretraining. On the other hand, TAB-BART and TAB-T5 are able to produce well-structured and relevant summaries. The OCR-based models can generally generate fluent summaries but they are slightly less effective in extracting the relevant information since the OCR process introduces some noise in the input data.\n\nWe also experiment with automatically extracted tables to see how the models perform in the absence of gold data tables. To this end, we extended Char-tOCR (Luo et al., 2021), which predicts the raw data values of chart elements, to extract the fullystructured data table. The accuracy of automatic data extraction was 77.31% (see Appendix A.5 for details). We find that similar to OCR-based models, TAB_OCR-based models tend to be less effective in extracting the relevant information compared to their TAB-based counterparts which use ground truth data tables.\n\nPew, on the other hand, is much challenging because it contains many charts with ill-defined structure and the underlying data tables are not available. Unsurprisingly, the performance of all the models has dropped significantly compared to that on Statista. Nonetheless, we can see that without the presence of the underlying data table, the vision+text (OCR-based) models have brought notable improvements over the vision only model. Further breakdown of model performance based on chart types is provided in Appendix A.4.2.\n\nWe also evaluate the transferability of the models and the datasets, where we first pretrain a model on a source dataset and fine-tune it on the target dataset. In addition to our two datasets (Statista or Pew), we experiment with ToTTo (Parikh et al., 2020) as another source dataset, which is a largescale open-domain English table-to-text dataset. Our results show that pretraining on other datasets only brings about marginal improvement. Details of this experiment can be found in Appendix A.4.1.\n\n\nHuman Evaluation\n\nTo further assess the summary quality we performed a human evaluation on 150 randomly sampled charts from the Statista dataset with four internal annotators who are native speakers of English. For each chart, annotators performed pairwise comparisons between the outputs of TAB-T5, OCR-T5 and the original gold summary (served as a control), resulting in a total of 450 pairwise comparisons (Appendix A.4.3). They compared the summaries based on three criteria: (i) Factual correctness: Which summary is more factually TAB-T5 (1) vs. OCR-T5 (2) Gold (1) vs. TAB-T5 (2) Gold (1) Table 5: Human evaluation results for comparing between the outputs of TAB-T5, OCR-T5 and the gold summary.\n\nGold: Footballer Cristiano Ronaldo heads the ranking of the most popular Instagram accounts as of December 2020. He is the most-followed person on the photo sharing app platform with nearly 244 million followers. Instagram's own account was ranked first with over 369 million followers. OCR-BART People in wealthier countries are more likely than those in poorer nations to get news online at least once a day. This is especially the case in Venezuela, where about seven-in-ten people (71%) get news daily.\n\nOCR-T5 However, it is not the case that the correlation between digital news consumption and personal income is not quite so strong. The study found that in wealthier countries, people were more likely to use the internet for news on a daily basis than in poorer countries. The finding that people in wealthier countries tended to do this more often than those in poorer nations to get news online. correct (i.e., facts mentioned are supported by the chart)? (ii) Coherence: Which summary is more coherent (i.e., sentences are well connected)? and (iii) Fluency: Which summary is more fluent and grammatically correct? For each criterion, the annotator picked the better one (win) or equally good (tie). Each comparison was performed by one annotator, except the first 150 comparisons for which we had two annotators to measure the agreement. The agreement for these 150 comparisons, excluding ties, was 74.3% (ties were excluded since they do not affect the overall ranking of the summaries). Table 5 shows that the TAB-T5 performed significantly better than OCR-T5 based on all three criteria, especially on factual correctness. This is likely because, without the data table as input, OCR-T5 model often fails to generate factually correct statements from the OCR text. We also observe that while the fluency of the model outputs is comparable to the gold summary, their factual correctness and coherence were significantly worse, especially for the OCR-T5 model.\n\n\nError Analysis and Challenges\n\nWe manually analyzed 200 random samples from Statista and Pew. We chose TAB-T5 and OCR-T5 for Statista and OCR-BART and OCR-T5 models for Pew. This analysis helps us to understand model errors and identify key challenges that existing models face as we describe below.\n\nPerceptual and reasoning aspects As mentioned in \u00a71, charts often describe complex patterns and trends which can be perceived by humans easily but they are not necessarily easy to derive through analysis of raw data tables. In Fig. 4b, the OCR-T5 model manages to describe a trend correctly in the first sentence but describes a trend incorrectly in the last sentence. These examples demonstrate the shortcomings of existing models. In order to explain perceptual and reasoning aspects effectively, we need more sophisticated models that better capture prominent visual relationships in charts. In particular, we aim to develop better representations including semantic graph representation of the chart that encodes numerical and logical relationships among chart objects.\n\nHallucinations Sometimes, the model outputs tokens that are irrelevant to the chart. For example, while the model outputs in Fig. 4a,b are quite fluent, they contain hallucination errors. This problem is commonly observed in other data-to-text work as well (Wiseman et al., 2017;Parikh et al., 2020).\n\nFactual errors Factually incorrect statements are more common for the OCR-based models (e.g., in Fig. 4a-b) since they do not take the data table as input, thus fail to associate the data values correctly. In contrast, TAB-T5 which utilizes the data table as input tends to generate less factual errors. This confirms that summarizing charts when the data table is not available is usually more challenging.\n\nComputer vision challenges The factual errors illustrate some unique computer vision challenges. First, charts do not always show data values as text labels, thus the OCR models cannot access those values. Even if the data values are labeled, the absence of association between data values (e.g., Instagram is related to 380.09M in Fig. 4a) leads to factual errors. This problem might be alleviated if the model can extract the data table from a chart image. While there are some initial attempts in this direction (e.g., Luo et al. (2021); Choi et al. (2019)), more accurate data extraction from charts is necessary.\n\nGeneralizability The charts in our benchmark cover several different chart types and a wide variety of topics ( fig. 9). The charts in the Pew in particular have a wide variety of visual styles in terms of color, layout and typography as they were created over several years by different authors (see examples in fig. 1). Nevertheless, finding more chartsummary pairs with more diverse visual styles is an open challenge. In future, we aim to find more different sources of chart-summaries and perform cross-domain experiments across those different sources to evaluate the generalizability of models.\n\n\nConclusion\n\nWe have presented two large-scale datasets for chart summarization. We also provided several state-of-the-art baselines and measures. Our evaluation highlights the promise of these baselines and also reveals several unique challenges for the chart summarization task. We hope that Chart-totext will serve as a useful research benchmark for model and metric development and motivate other researchers to explore this relatively new area.\n\nDuring the dataset collection and annotation process, we had many ethical issues to take into consideration. To respect the intellectual property of the chart publishers, we only used publicly available charts from resources that provide publication rights of downloaded content for academic purposes. According to the terms of use and publication rights for Statista, 4 users are granted publication rights only to free studies of Statista, so we only used the free publicly available webpages. According to the terms and conditions for Pew, 5 users are allowed to use the content as long as they are attributed to the Center or are not attributed to a different party.\n\nTo fairly compensate the Mechanical Turk annotators, we compensated the annotators based on the minimum wage in the United States at the time (7.25 US$ per hour) and the estimated time taken for each task (1 minute). Hence, these annotators received 0.10 -0.15 US$ for each chart, depending on the number of candidate paragraphs associated with it. Additionally, to protect the privacy of these annotators, all of their annotations were anonymized.\n\nTo ensure the reproducibility of our experimental results, we have provided the hyperparameter settings and estimated training time in Appendix A.3.\n\nWe foresee one possible misuse of our models that is to spread misinformation. Currently, our model outputs tend to appear fluent but contain some hallucinations and factual errors, as detailed in \u00a75.3. Hence, if such model outputs are published without being corrected, it may mislead and misinform the general public.   Let s i be the relevance score for sentence i in the paragraph.\n\nLet l i be the number of lexical token matches between sentence i and the chart.\n\nLet n i be the number of numerical token matches, excluding year tokens, between sentence i and the chart.\n\nLet y i be the number of year token matches between sentence i and the chart.\n\nLet u i be the number of numerical tokens that appear in sentence i but not in the chart.\n\nLet c be the number of sentences in the paragraph.\ns i = 0.58l i + 1.4n i \u2212 0.5u i\nLet content be the content score of the paragraph.\ncontent = 1 1 + exp (0.3 \u00d7 (\u2212max i (s i ) + 1.7))\nLet proximity be the proximity score of the paragraph.\n\nLet dist be the proximity of the paragraph to the chart. \u2212 5 \u2264 dist \u2264 5\n\nFor example, dist = \u22121 if the paragraph is directly before the chart, dist = 0 if it contains the chart and dist = 1 if it is directly after the chart.\nproximity = 0.4 \u00d7 exp (\u22120.1 \u00d7 |dist| 2 ) + 0.6\nLet rel be the relevance score of the paragraph. rel = content \u00d7 proximity\n\nHeuristic: A paragraph is relevant if it satisfies the following conditions:    \n\u2211 i l i > 3 \u2211 i n i + y i > 0 \u2211 i u i = 0 rel > 0.72 c > 0\n\nChart2text\n\nWe follow the same settings of Obeid and Hoque (2020) with 1 encoder layer, 6 decoder layers and a dropout ratio of 0.1, and train the model for 80 epochs with a batch size of 6. For inference, we use beam search with a beam size of 4.\n\nField-Infusing Model We follow the same settings as Chen et al. (2020a) and train the model for 10 epochs with a dropout ratio of 0.1 and batch size of 1.\n\nBART We fine-tune BART-Base 6 (140M, 6layers) for 500K iterations with a batch size of 4 and evaluate after every 2,000 iterations on the validation set. The initial learning rate is set to 0.0005. For inference, we use the model with the lowest validation loss and decode with a beam size of 4.\n\nT5 Similar to BART, we fine-tune T5-Base 6 (220M, 12-layer Transformer as the encoder and decoder) for 500K iteration with a batch size of 4 and an initial learning rate of 0.0005, evaluate after every 2,000 iterations on validation set, and use the model with best validation loss for testing. Inference is done with beam search with a beam size 4.  Chart types can influence the performance of the model. We present the performance breakdown on Statista of our best model (i.e., TAB-T5) based on chart types in Table 7. We observe that the model is good at summarizing simple and frequent chart types (e.g., line chart), whereas the model is less effective in generating informative summaries for complex and less frequent charts (e.g., pie charts) in our datasets.\n\n\nA.4.3 Human Evaluation\n\nThe user interface for the human evaluation annotation task of comparing chart summaries is given in Fig. 10.\n\n\nA.5 Automatic Data Extraction from Charts\n\nModel: We extend ChartOCR (Luo et al., 2021) which combines deep-learning and rule-based methods to extract the underlying data values from the chart images. First, key-point detection networks detects the chart main elements (e.g. plot area, y-axis-title, x-axis-title, and legend area) and marks (e.g. bars, line points, and pie slices). We extend the detection network to detect textual labels and the legend marks in the chart (see an example in Figure 11). For the rectangular objects, the network outputs the top-left and bottom-right points which are grouped together based on the distance. For lines, the network outputs the coordinates of the line points which are grouped together based on the color. For pie charts, the network outputs the separating points between the slices along the perimeter of the pie. As shown in Figure 11, the scale of the chart is estimated using the y-axislabels' values and y coordinates. Finally, the data values of the chart marks (e.g. bars, line points) are calculated using the scale of the chart. For pie charts, the values are estimated by calculating the angle between each two neighbouring points.   Since the original ChartOCR model only outputs the raw data values, we we further extend their approach to output the fully-structured data table as follows. First, we utilize the CRAFT model (Baek et al., 2019a) to recognize the texts of the detected textual chart elements (x-axis labels, and legend labels). Then, we associate the data values with their closest x-axis-label and the data series (e.g. a group of bars or line points) with the legend labels based on the color. For example, in Figure 11b, the bars are matched with their closest x-axis-labels ('Sunday' and 'Daily'). Moreover, the values of dark blue bars are associated with '2019' legend-label and the values of light blue bars are associated with '2018' legend-label based on the matched colors. In this way, our approach recovers the fully structured data table from the chart as shown in Figure 11c.\n\nEvaluation Metric: We evaluate our extracted data table using the following metric (adapted from ChartOCR (Luo et al., 2021)). We define the distance function between two data points as: D(gt, pr) = min(1, || gt \u2212 pr gt ||)\n\nwhere gt is the ground truth value and pr is the predicted value. We then compute the cost matrix C, where C n,m = D(gt n , pr m ). The total minimum cost is then estimated by solving the linear sum assignment problem as follows:\ncost = K \u2211 i=1 K \u2211 j=1 C i,j X i,j\nWhere K = max(N, M ) and X is a binary assignment matrix. The final score is then computed using the following equation:\nscore = 1 \u2212 cost K\nFinally, we average the scores of all the charts to compute the overall score.\n\nA.6 Additional Examples from Statista and Pew datasets Figure 12 presents additional samples from our chart-to-text benchmark covering a diverse range of chart types and styles.\n\nAmericans overwhelmingly support limits on political campaign spending, and most think new laws could effectively reduce the role of money in politics. And there is extensive support for reining in campaign spending: 77% of the public says \"there should be limits on the amount of money individuals and organizations\" can spend on political campaigns; just 20% say they should be able to spend as much as they want. A somewhat smaller majority (65%) says that new campaign finance laws could be written that would be effective in reducing the role of money in politics, while 31% say any new laws would not be effective.\n\nIn a recent survey of what Americans know about science, we asked people to interpret the chart you see here and tell us what it showed. Six-in-ten (63%) identify the best interpretation of this chart as \"the more sugar people eat, the more likely they are to get cavities.\"\n\nThe statistic shows the distribution of employment in Brazil by economic sector from 2010 to 2020. In 2020, 9.12 percent of the employees in Brazil were active in the agricultural sector, 19.59 percent in industry and 71.29 percent in the service sector.\n\nAs of 2019, a third of online users worldwide were aged between 25 and 34 years. Website visitors in this age bracket constituted the biggest group of online users worldwide. Also, 18 percent of global online users were aged 18 to 24 years.\n\nThe cost of fossil fuels in the electric power industry can vary depending on the source that is used. In general, fossil fuels cost about 2.50 U.S. dollars per million British thermal units (Btu) but can range from 2.02 U.S. dollars per million Btu for coal to 9.07 U.S. dollars per million Btu for petroleum. \n\nFigure 1 :\n1An example chart-summary pair from our Benchmark and the output from one of the best models (TAB-T5).\n\nFigure 2 :\n2Stages of the Pew dataset construction process.\n\nFigure 4 :\n4Sample outputs from Statista (first column) and Pew datasets (last two columns). Red indicates hallucination errors and blue indicates tokens that are resulted in factual errors in the model output.\n\nFigure 5 :\n5A screenshot of a webpage from Statista.\n\nFigure 6 :\n6The user interface for labeling the x-axis labels in the Statista dataset.\n\nFigure 7 :\n7The computation of a paragraph's relevance score to a chart, and the conditions for the heuristic in the Pew dataset.\n\nFigure 8 :\n8The user interface for the Mechanical Turk annotation task in the Pew dataset.\n\nFigure 9 :\n9Distribution of topics in the two datasets.\n\nFigure 10 :\n10The user interface for human evaluation: it presents two summaries at a time and asks the participant to compare between them based on three measures.\n\nFigure 11 :\n11Data Extraction example from Statista.\n\nFigure 12 :\n12Examples of chart-summary pairs in our benchmark. The top two examples are from the Pew research dataset and the rest of the examples are from the Statista dataset.\n\nTable 2 :\n2Chart-to-text dataset statistics.Content Level \nStatista \nPew \n\nVisual encodings \n32.03% \n0.98% \nStatistical and comparative \n50.00% 54.63% \nPerceptual and cognitive \n8.98% \n30.49% \nContextual and domain-specific 10.94% 12.93% \n\n\n\nTable 3 :\n3Distribution of different types of semantic content.\n\nTable 4 :\n4Evaluation results for different models on Statista and Pew test sets. \u2191 : Higher is better, \u2193 : Lower is better. \"TAB-\" models have access to theunderlying data table and \"OCR-\" models use OCR-\nextracted data. OCR variants with \u22c6 superscript use \nbounding box information. \"TAB_OCR-\" models use \nautomatically generated data tables. \n\n\n\n\nPre-train Dataset Fine-tune Dataset BLEUTotto \nPew \n10.66 \nTotto \nStatista \n37.19 \nPew \nStatista \n37.32 \nStatista \nPew \n10.73 \n\n\n\nTable 6 :\n6Results measured by BLEU for transferability based on the T5 model.\n\n\nA.4 Additional Results from EvaluationA.4.1 Transfer ResultsSince both Statista and Pew share some of the topics with each other, we conduct transfer experiment to verify if pretraining on one dataset could help to improve the final results on the other. In addition, since table-to-text has similarities with our task, we also experiment with pretraining on a large-scale open-domain English table-to-text dataset ToTTo(Parikh et al., 2020)  before training on our datasets. We use full table for ToTTo since our task does not contain highlighted cell. Pretraining and finetuning use T5-based models and have the same training procedure as described in \u00a74.2. From Ta-6 huggingface.co/transformers ble 6, we see that pretraining on other datasets only improves the final performance by a small margin.A.4.2 Performance by Chart Types \n\nChart Types Bar \nLine \nPie \nTable \n\nBLEU \n36.46 45.28 21.35 26.12 \nPPL \n10.08 7.53 \n8.79 11.34 \nCIDEr \n4.62 \n5.59 \n3.27 \n3.67 \nBLEURT \n0.14 \n0.27 -0.13 -0.22 \n\n\n\nTable 7 :\n7Results on Statista test set w.r.t. chart types.\nNarrative Science Quill; Automated Insights Wordsmith\nThe overall agreement for the crowd workers was 78.2%.\nOur categorization of content is inspired by a recent study(Lundgard and Satyanarayan, 2022).\nhttps://www.statista.com/getting-started/publishingstatista-content-terms-of-use-and-publication-rights 5 https://www.pewresearch.org/about/terms-andconditions/\nAcknowledgementThe authors would like to thank the anonymous reviewers for their helpful comments. This research was supported by the Natural Sciences & Engineering Research Council (NSERC) of Canada.Ethical ConsiderationsA AppendicesA.1 Additional Details on Data AnnotationA.1.1 Example Webpage from StatistaAn example of a webpage from Statista is given inFig. 5. It contains a chart image and its accompanying description text. The first part of the text (highlighted in blue) provides a succinct summary of the chart while the remaining parts of the text (not highlighted) provides irrelevant background information, such as Facebook's history.A.1.2 Annotation of x-axis Labels in StatistaThe user interface for the annotation task of labeling the x-axis labels in the Statista dataset is given inFig. 6.A.1.3 Identify Candidate Paragraphs in PewThe details for computing the relevance score of a paragraph to the given chart, and the heuristic for finding relevant paragraphs in the Pew dataset are given inFig. 7.A.1.4 Relevant Paragraph Selection in PewFor the relevant paragraph selection task, the annotators received 0.10 -0.15 US$ for each chart, depending on the number of candidate paragraphs associated with it. To ensure the quality, we recruited participants with at least 95% approval rate and 5000 approved HITs (Human Intelligence Tasks) and they were only allowed to complete the tasks after they successfully completed a sample task.The user interface for the Mechanical Turk annotation task of selecting paragraphs relevant to charts in the Pew dataset is given inFig. 8.Figure 9shows the distribution of topics in two datasets.A.2 Dataset AnalysisA.3 Chart-to-text Baseline ModelsThe experiments are done on our machine (CPU: Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz, GPU: 4 \u00d7 NVIDIA GTX 2080Ti). Training T5 is the most computationally costly task, which takes around 16-20 hours on 4\u00d7 GPUs.Image Captioning Models For pretraining the image encoders and captioning model, we follow the same training setup as presented in the original papers. Inference is done with beam search with a beam size of 4.\n. Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, DeviHarsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi\n\nnocaps: novel object captioning at scale. Stefan Parikh, Peter Lee, Anderson, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionParikh, Stefan Lee, and Peter Anderson. 2019. no- caps: novel object captioning at scale. In Proceed- ings of the IEEE International Conference on Com- puter Vision, pages 8948-8957.\n\nWhat is wrong with scene text recognition model comparisons? dataset and model analysis. Jeonghun Baek, Geewook Kim, Junyeop Lee, Sungrae Park, Dongyoon Han, Sangdoo Yun, Hwalsuk Seong Joon Oh, Lee, International Conference on Computer Vision (ICCV). Jeonghun Baek, Geewook Kim, Junyeop Lee, Sungrae Park, Dongyoon Han, Sangdoo Yun, Seong Joon Oh, and Hwalsuk Lee. 2019a. What is wrong with scene text recognition model comparisons? dataset and model analysis. In International Conference on Computer Vision (ICCV).\n\nCharacter region awareness for text detection. Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, Hwalsuk Lee, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYoungmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee. 2019b. Character region awareness for text detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9365-9374.\n\nCollective content selection for concept-to-text generation. Regina Barzilay, Mirella Lapata, Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing. Human Language Technology Conference and Conference on Empirical Methods in Natural Language ProcessingBritish Columbia, CanadaAssociation for Computational LinguisticsRegina Barzilay and Mirella Lapata. 2005. Collective content selection for concept-to-text generation. In Proceedings of Human Language Technology Con- ference and Conference on Empirical Methods in Natural Language Processing, pages 331-338, Van- couver, British Columbia, Canada. Association for Computational Linguistics.\n\nInformation graphics: an untapped resource for digital libraries. Sandra Carberry, Stephanie Elzer, Seniz Demir, Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. the 29th annual international ACM SIGIR conference on Research and development in information retrievalSandra Carberry, Stephanie Elzer, and Seniz Demir. 2006. Information graphics: an untapped resource for digital libraries. In Proceedings of the 29th an- nual international ACM SIGIR conference on Re- search and development in information retrieval, pages 581-588.\n\nFigure captioning with reasoning and sequence-level training. Charles Chen, Ruiyi Zhang, Eunyee Koh, Sungchul Kim, Scott Cohen, Tong Yu, Ryan A Rossi, Razvan C Bunescu, abs/1906.02850CoRRCharles Chen, Ruiyi Zhang, Eunyee Koh, Sungchul Kim, Scott Cohen, Tong Yu, Ryan A. Rossi, and Razvan C. Bunescu. 2019. Figure captioning with reasoning and sequence-level training. CoRR, abs/1906.02850.\n\nLogical natural language generation from open-domain tables. Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, William Yang Wang, 10.18653/v1/2020.acl-main.708Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and William Yang Wang. 2020a. Logical natural lan- guage generation from open-domain tables. In Pro- ceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 7929- 7942, Online. Association for Computational Lin- guistics.\n\nMicrosoft coco captions: Data collection and evaluation server. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, C Lawrence Zitnick, Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr- ishna Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick. 2015. Microsoft coco captions: Data collection and evaluation server.\n\nLogic2Text: High-fidelity natural language generation from logical forms. Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam Sundaresan, William Yang Wang, 10.18653/v1/2020.findings-emnlp.190Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational LinguisticsZhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam Sundaresan, and William Yang Wang. 2020b. Logic2Text: High-fidelity natural lan- guage generation from logical forms. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2096-2111, Online. Association for Computational Linguistics.\n\nVisualizing for the non-visual: Enabling the visually impaired to use visualization. J Choi, Sanghun Jung, Deok Gun, J Park, N Choo, Elmqvist, Computer Graphics Forum. 38J. Choi, Sanghun Jung, Deok Gun Park, J. Choo, and N. Elmqvist. 2019. Visualizing for the non-visual: Enabling the visually impaired to use visualization. Computer Graphics Forum, 38.\n\nDatasite: Proactive visual data exploration with computation of insightbased recommendations. Zhe Cui, Karthik Sriram, Badam, Niklas Yal\u00e7in, Elmqvist, Information Visualization. 182Zhe Cui, Sriram Karthik Badam, M Adil Yal\u00e7in, and Niklas Elmqvist. 2019. Datasite: Proactive vi- sual data exploration with computation of insight- based recommendations. Information Visualization, 18(2):251-267.\n\nSummarizing information graphics textually. Seniz Demir, Sandra Carberry, Kathleen F Mccoy, 10.1162/COLI_a_00091Computational Linguistics. 383Seniz Demir, Sandra Carberry, and Kathleen F. McCoy. 2012. Summarizing information graphics textually. Computational Linguistics, 38(3):527-574.\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 10.1109/CVPR.2009.52068482009 IEEE Conference on Computer Vision and Pattern Recognition. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hier- archical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248-255.\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.\n\nPostgraphe: a system for the generation of statistical graphics and text. Massimo Fasciano, Guy Lapalme, Eighth International Natural Language Generation Workshop. Massimo Fasciano and Guy Lapalme. 1996. Postgraphe: a system for the generation of statistical graphics and text. In Eighth International Natural Language Generation Workshop.\n\nEvaluating a tool for improving accessibility to charts and graphs. Leo Ferres, Gitte Lindgaard, Livia Sumegi, Bruce Tsuji, 10.1145/2533682.2533683ACM Trans. Comput.-Hum. Interact. 205Leo Ferres, Gitte Lindgaard, Livia Sumegi, and Bruce Tsuji. 2013. Evaluating a tool for improving acces- sibility to charts and graphs. ACM Trans. Comput.- Hum. Interact., 20(5).\n\nEnhanced transformer model for data-to-text generation. Li Gong, Josep Crego, Jean Senellart, 10.18653/v1/D19-5615Proceedings of the 3rd Workshop on Neural Generation and Translation. the 3rd Workshop on Neural Generation and TranslationHong KongAssociation for Computational LinguisticsLi Gong, Josep Crego, and Jean Senellart. 2019. En- hanced transformer model for data-to-text generation. In Proceedings of the 3rd Workshop on Neural Gen- eration and Translation, pages 148-156, Hong Kong. Association for Computational Linguistics.\n\nAutobrief: an experimental system for the automatic generation of briefings in integrated text and information graphics. Giuseppe Nancy L Green, Stephan Carenini, Joe Kerpedjiev, Johanna D Mattis, Steven F Moore, Roth, International Journal of Human-Computer Studies. 611Nancy L Green, Giuseppe Carenini, Stephan Kerpedjiev, Joe Mattis, Johanna D Moore, and Steven F Roth. 2004. Autobrief: an experimental system for the automatic generation of briefings in integrated text and information graphics. International Journal of Human-Computer Studies, 61(1):32-70.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770- 778.\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, 10.1162/neco.1997.9.8.1735Neural Comput. 98Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Comput., 9(8):1735-1780.\n\nScicap: Generating captions for scientific figures. E Ting-Yao, C Lee Hsu, Giles, K Ting-Hao, Huang, Findings of 2021 Conference on Empirical Methods in Natural Language Processing. EMNLP 2021 FindingsTing-Yao E. Hsu, C. Lee Giles, and Ting-Hao K. Huang. 2021. Scicap: Generating captions for scientific fig- ures. In Findings of 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021 Findings).\n\nTraining data-efficient image transformers & distillation through attention. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, PMLRProceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine Learning139Alexandre Sablayrolles, and Herve Jegou. 2021Hugo Touvron, Matthieu Cord, Matthijs Douze, Fran- cisco Massa, Alexandre Sablayrolles, and Herve Je- gou. 2021. Training data-efficient image transform- ers & distillation through attention. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 10347-10357. PMLR.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Illia Kaiser, Polosukhin, Advances in Neural Information Processing Systems. Curran Associates, Inc30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, volume 30. Curran Associates, Inc.\n\nCider: Consensus-based image description evaluation. Ramakrishna Vedantam, Lawrence Zitnick, Devi Parikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image de- scription evaluation. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 4566-4575.\n\nStudents' understanding of bar graphs and histograms: Results from the locus assessments. Douglas Whitaker, Tim Jacobbe, 10.1080/10691898.2017.1321974Journal of Statistics Education. 252Douglas Whitaker and Tim Jacobbe. 2017. Students' understanding of bar graphs and histograms: Results from the locus assessments. Journal of Statistics Education, 25(2):90-102.\n\n. Sam Wiseman, M Stuart, Alexander M Shieber, Rush, arXiv:1707.08052Challenges in data-to-document generation. arXiv preprintSam Wiseman, Stuart M Shieber, and Alexander M Rush. 2017. Challenges in data-to-document genera- tion. arXiv preprint arXiv:1707.08052.\n\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio, arXiv:1502.03044Show, attend and tell: Neural image caption generation with visual attention. arXiv preprintKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044.\n\nReference-aware language models. Zichao Yang, Phil Blunsom, Chris Dyer, Wang Ling, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingZichao Yang, Phil Blunsom, Chris Dyer, and Wang Ling. 2017. Reference-aware language models. In Pro- ceedings of the 2017 Conference on Empirical Meth- ods in Natural Language Processing, pages 1850- 1859.\n\nJure Zbontar, Li Jing, Ishan Misra, arXiv:2103.03230Yann LeCun, and St\u00e9phane Deny. 2021. Barlow twins: Self-supervised learning via redundancy reduction. arXiv preprintJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\u00e9phane Deny. 2021. Barlow twins: Self-supervised learning via redundancy reduction. arXiv preprint arXiv:2103.03230.\n\nVinvl: Making visual representations matter in vision-language models. Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao, Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian- feng Gao. 2021. Vinvl: Making visual representa- tions matter in vision-language models. CVPR 2021.\n", "annotations": {"author": "[{\"end\":173,\"start\":113},{\"end\":188,\"start\":174},{\"end\":242,\"start\":189},{\"end\":297,\"start\":243},{\"end\":351,\"start\":298},{\"end\":409,\"start\":352},{\"end\":464,\"start\":410},{\"end\":467,\"start\":465},{\"end\":597,\"start\":468}]", "publisher": "[{\"end\":75,\"start\":64},{\"end\":833,\"start\":822}]", "author_last_name": "[{\"end\":130,\"start\":121},{\"end\":187,\"start\":180},{\"end\":197,\"start\":192},{\"end\":252,\"start\":249},{\"end\":309,\"start\":304},{\"end\":364,\"start\":357},{\"end\":422,\"start\":417},{\"end\":479,\"start\":475}]", "author_first_name": "[{\"end\":120,\"start\":113},{\"end\":179,\"start\":174},{\"end\":191,\"start\":189},{\"end\":248,\"start\":243},{\"end\":303,\"start\":298},{\"end\":356,\"start\":352},{\"end\":416,\"start\":410},{\"end\":466,\"start\":465},{\"end\":474,\"start\":468}]", "author_affiliation": "[{\"end\":172,\"start\":150},{\"end\":241,\"start\":199},{\"end\":296,\"start\":254},{\"end\":350,\"start\":328},{\"end\":408,\"start\":366},{\"end\":463,\"start\":441},{\"end\":560,\"start\":518},{\"end\":596,\"start\":562}]", "title": "[{\"end\":63,\"start\":1},{\"end\":660,\"start\":598}]", "venue": "[{\"end\":749,\"start\":662}]", "abstract": "[{\"end\":2564,\"start\":850}]", "bib_ref": "[{\"end\":2999,\"start\":2968},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3026,\"start\":2999},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3400,\"start\":3378},{\"end\":3843,\"start\":3825},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4073,\"start\":4052},{\"end\":4227,\"start\":4210},{\"end\":4512,\"start\":4498},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4587,\"start\":4559},{\"end\":4607,\"start\":4587},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4626,\"start\":4607},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4645,\"start\":4626},{\"end\":4752,\"start\":4734},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4770,\"start\":4752},{\"end\":5080,\"start\":5062},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5763,\"start\":5741},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6126,\"start\":6105},{\"end\":6147,\"start\":6126},{\"end\":7559,\"start\":7538},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7579,\"start\":7559},{\"end\":7632,\"start\":7618},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7892,\"start\":7874},{\"end\":7922,\"start\":7897},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8021,\"start\":8002},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8196,\"start\":8178},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8230,\"start\":8213},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8926,\"start\":8909},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9367,\"start\":9340},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9388,\"start\":9367},{\"end\":9433,\"start\":9412},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9472,\"start\":9453},{\"end\":9519,\"start\":9498},{\"end\":9570,\"start\":9549},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9589,\"start\":9570},{\"end\":9687,\"start\":9669},{\"end\":9707,\"start\":9687},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9728,\"start\":9707},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9748,\"start\":9730},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9795,\"start\":9773},{\"end\":9976,\"start\":9959},{\"end\":10080,\"start\":10058},{\"end\":10228,\"start\":10206},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10246,\"start\":10228},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10267,\"start\":10248},{\"end\":14215,\"start\":14193},{\"end\":18594,\"start\":18593},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20876,\"start\":20856},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21137,\"start\":21120},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21232,\"start\":21216},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21271,\"start\":21254},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21353,\"start\":21319},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21480,\"start\":21461},{\"end\":21745,\"start\":21723},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22321,\"start\":22303},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23224,\"start\":23204},{\"end\":23286,\"start\":23265},{\"end\":25817,\"start\":25805},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26308,\"start\":26286},{\"end\":26547,\"start\":26512},{\"end\":27641,\"start\":27623},{\"end\":28817,\"start\":28796},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":33100,\"start\":33078},{\"end\":33120,\"start\":33100},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":34091,\"start\":34073},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":38273,\"start\":38254},{\"end\":39648,\"start\":39630},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":40965,\"start\":40945},{\"end\":41751,\"start\":41724}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":44338,\"start\":44224},{\"attributes\":{\"id\":\"fig_1\"},\"end\":44399,\"start\":44339},{\"attributes\":{\"id\":\"fig_2\"},\"end\":44611,\"start\":44400},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44665,\"start\":44612},{\"attributes\":{\"id\":\"fig_4\"},\"end\":44753,\"start\":44666},{\"attributes\":{\"id\":\"fig_5\"},\"end\":44884,\"start\":44754},{\"attributes\":{\"id\":\"fig_6\"},\"end\":44976,\"start\":44885},{\"attributes\":{\"id\":\"fig_7\"},\"end\":45033,\"start\":44977},{\"attributes\":{\"id\":\"fig_8\"},\"end\":45199,\"start\":45034},{\"attributes\":{\"id\":\"fig_9\"},\"end\":45253,\"start\":45200},{\"attributes\":{\"id\":\"fig_10\"},\"end\":45433,\"start\":45254},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":45675,\"start\":45434},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":45740,\"start\":45676},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":46089,\"start\":45741},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":46220,\"start\":46090},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":46300,\"start\":46221},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":47299,\"start\":46301},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":47360,\"start\":47300}]", "paragraph": "[{\"end\":3027,\"start\":2580},{\"end\":4228,\"start\":3029},{\"end\":5324,\"start\":4230},{\"end\":6342,\"start\":5326},{\"end\":7023,\"start\":6344},{\"end\":7490,\"start\":7025},{\"end\":8299,\"start\":7507},{\"end\":9159,\"start\":8301},{\"end\":10081,\"start\":9161},{\"end\":10617,\"start\":10083},{\"end\":10886,\"start\":10644},{\"end\":11610,\"start\":10906},{\"end\":12479,\"start\":11612},{\"end\":12700,\"start\":12499},{\"end\":13003,\"start\":12702},{\"end\":13807,\"start\":13005},{\"end\":15028,\"start\":13809},{\"end\":15236,\"start\":15030},{\"end\":15694,\"start\":15238},{\"end\":16858,\"start\":15696},{\"end\":17393,\"start\":16860},{\"end\":17713,\"start\":17414},{\"end\":18376,\"start\":17715},{\"end\":19300,\"start\":18378},{\"end\":19988,\"start\":19334},{\"end\":20386,\"start\":19990},{\"end\":21037,\"start\":20388},{\"end\":22157,\"start\":21065},{\"end\":23179,\"start\":22181},{\"end\":23762,\"start\":23181},{\"end\":24217,\"start\":23764},{\"end\":24479,\"start\":24219},{\"end\":25131,\"start\":24481},{\"end\":25677,\"start\":25133},{\"end\":26548,\"start\":25715},{\"end\":27465,\"start\":26550},{\"end\":28029,\"start\":27467},{\"end\":28557,\"start\":28031},{\"end\":29060,\"start\":28559},{\"end\":29766,\"start\":29081},{\"end\":30274,\"start\":29768},{\"end\":31742,\"start\":30276},{\"end\":32044,\"start\":31776},{\"end\":32819,\"start\":32046},{\"end\":33121,\"start\":32821},{\"end\":33530,\"start\":33123},{\"end\":34149,\"start\":33532},{\"end\":34752,\"start\":34151},{\"end\":35203,\"start\":34767},{\"end\":35875,\"start\":35205},{\"end\":36325,\"start\":35877},{\"end\":36475,\"start\":36327},{\"end\":36862,\"start\":36477},{\"end\":36944,\"start\":36864},{\"end\":37052,\"start\":36946},{\"end\":37131,\"start\":37054},{\"end\":37222,\"start\":37133},{\"end\":37274,\"start\":37224},{\"end\":37357,\"start\":37307},{\"end\":37462,\"start\":37408},{\"end\":37535,\"start\":37464},{\"end\":37688,\"start\":37537},{\"end\":37810,\"start\":37736},{\"end\":37892,\"start\":37812},{\"end\":38200,\"start\":37965},{\"end\":38356,\"start\":38202},{\"end\":38653,\"start\":38358},{\"end\":39422,\"start\":38655},{\"end\":39558,\"start\":39449},{\"end\":41625,\"start\":39604},{\"end\":41850,\"start\":41627},{\"end\":42081,\"start\":41852},{\"end\":42237,\"start\":42117},{\"end\":42335,\"start\":42257},{\"end\":42514,\"start\":42337},{\"end\":43136,\"start\":42516},{\"end\":43412,\"start\":43138},{\"end\":43668,\"start\":43414},{\"end\":43910,\"start\":43670},{\"end\":44223,\"start\":43912}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":37306,\"start\":37275},{\"attributes\":{\"id\":\"formula_1\"},\"end\":37407,\"start\":37358},{\"attributes\":{\"id\":\"formula_2\"},\"end\":37735,\"start\":37689},{\"attributes\":{\"id\":\"formula_3\"},\"end\":37951,\"start\":37893},{\"attributes\":{\"id\":\"formula_4\"},\"end\":42116,\"start\":42082},{\"attributes\":{\"id\":\"formula_5\"},\"end\":42256,\"start\":42238}]", "table_ref": "[{\"end\":17489,\"start\":17480},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18096,\"start\":18089},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":18508,\"start\":18501},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":26597,\"start\":26590},{\"end\":29666,\"start\":29659},{\"end\":31277,\"start\":31270},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":39175,\"start\":39168}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2578,\"start\":2566},{\"attributes\":{\"n\":\"2\"},\"end\":7505,\"start\":7493},{\"attributes\":{\"n\":\"3\"},\"end\":10642,\"start\":10620},{\"attributes\":{\"n\":\"3.1\"},\"end\":10904,\"start\":10889},{\"attributes\":{\"n\":\"3.2\"},\"end\":12497,\"start\":12482},{\"attributes\":{\"n\":\"3.3\"},\"end\":17412,\"start\":17396},{\"attributes\":{\"n\":\"4\"},\"end\":19332,\"start\":19303},{\"attributes\":{\"n\":\"4.1\"},\"end\":21063,\"start\":21040},{\"attributes\":{\"n\":\"4.2\"},\"end\":22179,\"start\":22160},{\"attributes\":{\"n\":\"5\"},\"end\":25690,\"start\":25680},{\"attributes\":{\"n\":\"5.1\"},\"end\":25713,\"start\":25693},{\"attributes\":{\"n\":\"5.2\"},\"end\":29079,\"start\":29063},{\"attributes\":{\"n\":\"5.3\"},\"end\":31774,\"start\":31745},{\"attributes\":{\"n\":\"6\"},\"end\":34765,\"start\":34755},{\"end\":37963,\"start\":37953},{\"end\":39447,\"start\":39425},{\"end\":39602,\"start\":39561},{\"end\":44235,\"start\":44225},{\"end\":44350,\"start\":44340},{\"end\":44411,\"start\":44401},{\"end\":44623,\"start\":44613},{\"end\":44677,\"start\":44667},{\"end\":44765,\"start\":44755},{\"end\":44896,\"start\":44886},{\"end\":44988,\"start\":44978},{\"end\":45046,\"start\":45035},{\"end\":45212,\"start\":45201},{\"end\":45266,\"start\":45255},{\"end\":45444,\"start\":45435},{\"end\":45686,\"start\":45677},{\"end\":45751,\"start\":45742},{\"end\":46231,\"start\":46222},{\"end\":47310,\"start\":47301}]", "table": "[{\"end\":45675,\"start\":45479},{\"end\":46089,\"start\":45899},{\"end\":46220,\"start\":46132},{\"end\":47299,\"start\":47104}]", "figure_caption": "[{\"end\":44338,\"start\":44237},{\"end\":44399,\"start\":44352},{\"end\":44611,\"start\":44413},{\"end\":44665,\"start\":44625},{\"end\":44753,\"start\":44679},{\"end\":44884,\"start\":44767},{\"end\":44976,\"start\":44898},{\"end\":45033,\"start\":44990},{\"end\":45199,\"start\":45049},{\"end\":45253,\"start\":45215},{\"end\":45433,\"start\":45269},{\"end\":45479,\"start\":45446},{\"end\":45740,\"start\":45688},{\"end\":45899,\"start\":45753},{\"end\":46132,\"start\":46092},{\"end\":46300,\"start\":46233},{\"end\":47104,\"start\":46303},{\"end\":47360,\"start\":47312}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6996,\"start\":6990},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14095,\"start\":14088},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":17941,\"start\":17935},{\"end\":22549,\"start\":22540},{\"end\":23470,\"start\":23463},{\"end\":24093,\"start\":24084},{\"end\":25129,\"start\":25122},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":32280,\"start\":32273},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":32953,\"start\":32946},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":33230,\"start\":33220},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":33871,\"start\":33864},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":34269,\"start\":34263},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34470,\"start\":34464},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39557,\"start\":39550},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40063,\"start\":40054},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40445,\"start\":40436},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41258,\"start\":41248},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41624,\"start\":41614},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42401,\"start\":42392}]", "bib_author_first_name": "[{\"end\":49858,\"start\":49853},{\"end\":49873,\"start\":49868},{\"end\":49886,\"start\":49881},{\"end\":49899,\"start\":49893},{\"end\":49913,\"start\":49906},{\"end\":49924,\"start\":49920},{\"end\":49939,\"start\":49934},{\"end\":50099,\"start\":50093},{\"end\":50113,\"start\":50108},{\"end\":50531,\"start\":50523},{\"end\":50545,\"start\":50538},{\"end\":50558,\"start\":50551},{\"end\":50571,\"start\":50564},{\"end\":50586,\"start\":50578},{\"end\":50599,\"start\":50592},{\"end\":50612,\"start\":50605},{\"end\":51006,\"start\":50998},{\"end\":51017,\"start\":51013},{\"end\":51031,\"start\":51023},{\"end\":51044,\"start\":51037},{\"end\":51057,\"start\":51050},{\"end\":51494,\"start\":51488},{\"end\":51512,\"start\":51505},{\"end\":52207,\"start\":52201},{\"end\":52227,\"start\":52218},{\"end\":52240,\"start\":52235},{\"end\":52806,\"start\":52799},{\"end\":52818,\"start\":52813},{\"end\":52832,\"start\":52826},{\"end\":52846,\"start\":52838},{\"end\":52857,\"start\":52852},{\"end\":52869,\"start\":52865},{\"end\":52878,\"start\":52874},{\"end\":52880,\"start\":52879},{\"end\":52894,\"start\":52888},{\"end\":52896,\"start\":52895},{\"end\":53194,\"start\":53189},{\"end\":53208,\"start\":53201},{\"end\":53217,\"start\":53215},{\"end\":53227,\"start\":53222},{\"end\":53241,\"start\":53234},{\"end\":53246,\"start\":53242},{\"end\":53819,\"start\":53813},{\"end\":53829,\"start\":53826},{\"end\":53844,\"start\":53836},{\"end\":53861,\"start\":53850},{\"end\":53879,\"start\":53872},{\"end\":53892,\"start\":53887},{\"end\":53902,\"start\":53901},{\"end\":53911,\"start\":53903},{\"end\":54186,\"start\":54181},{\"end\":54198,\"start\":54193},{\"end\":54211,\"start\":54205},{\"end\":54222,\"start\":54217},{\"end\":54235,\"start\":54229},{\"end\":54249,\"start\":54243},{\"end\":54269,\"start\":54262},{\"end\":54274,\"start\":54270},{\"end\":54852,\"start\":54851},{\"end\":54866,\"start\":54859},{\"end\":54884,\"start\":54883},{\"end\":54892,\"start\":54891},{\"end\":55218,\"start\":55215},{\"end\":55231,\"start\":55224},{\"end\":55253,\"start\":55247},{\"end\":55565,\"start\":55560},{\"end\":55579,\"start\":55573},{\"end\":55598,\"start\":55590},{\"end\":55600,\"start\":55599},{\"end\":55860,\"start\":55857},{\"end\":55870,\"start\":55867},{\"end\":55884,\"start\":55877},{\"end\":55899,\"start\":55893},{\"end\":55907,\"start\":55904},{\"end\":55914,\"start\":55912},{\"end\":56317,\"start\":56312},{\"end\":56334,\"start\":56326},{\"end\":56348,\"start\":56342},{\"end\":56362,\"start\":56354},{\"end\":57234,\"start\":57227},{\"end\":57248,\"start\":57245},{\"end\":57565,\"start\":57562},{\"end\":57579,\"start\":57574},{\"end\":57596,\"start\":57591},{\"end\":57610,\"start\":57605},{\"end\":57916,\"start\":57914},{\"end\":57928,\"start\":57923},{\"end\":57940,\"start\":57936},{\"end\":58525,\"start\":58517},{\"end\":58548,\"start\":58541},{\"end\":58562,\"start\":58559},{\"end\":58582,\"start\":58575},{\"end\":58584,\"start\":58583},{\"end\":58601,\"start\":58593},{\"end\":59012,\"start\":59005},{\"end\":59024,\"start\":59017},{\"end\":59040,\"start\":59032},{\"end\":59050,\"start\":59046},{\"end\":59433,\"start\":59429},{\"end\":59452,\"start\":59446},{\"end\":59665,\"start\":59664},{\"end\":59677,\"start\":59676},{\"end\":59681,\"start\":59678},{\"end\":59695,\"start\":59694},{\"end\":60115,\"start\":60111},{\"end\":60133,\"start\":60125},{\"end\":60148,\"start\":60140},{\"end\":60165,\"start\":60156},{\"end\":60728,\"start\":60722},{\"end\":60742,\"start\":60738},{\"end\":60756,\"start\":60752},{\"end\":60770,\"start\":60765},{\"end\":60787,\"start\":60782},{\"end\":60800,\"start\":60795},{\"end\":60802,\"start\":60801},{\"end\":60815,\"start\":60810},{\"end\":61226,\"start\":61215},{\"end\":61245,\"start\":61237},{\"end\":61259,\"start\":61255},{\"end\":61728,\"start\":61721},{\"end\":61742,\"start\":61739},{\"end\":62000,\"start\":61997},{\"end\":62011,\"start\":62010},{\"end\":62031,\"start\":62020},{\"end\":62264,\"start\":62258},{\"end\":62274,\"start\":62269},{\"end\":62283,\"start\":62279},{\"end\":62300,\"start\":62291},{\"end\":62311,\"start\":62306},{\"end\":62329,\"start\":62323},{\"end\":62352,\"start\":62345},{\"end\":62366,\"start\":62360},{\"end\":62761,\"start\":62755},{\"end\":62772,\"start\":62768},{\"end\":62787,\"start\":62782},{\"end\":62798,\"start\":62794},{\"end\":63175,\"start\":63171},{\"end\":63187,\"start\":63185},{\"end\":63199,\"start\":63194},{\"end\":63591,\"start\":63582},{\"end\":63605,\"start\":63599},{\"end\":63617,\"start\":63610},{\"end\":63629,\"start\":63622},{\"end\":63639,\"start\":63636},{\"end\":63653,\"start\":63647},{\"end\":63665,\"start\":63660},{\"end\":63680,\"start\":63672}]", "bib_author_last_name": "[{\"end\":49866,\"start\":49859},{\"end\":49879,\"start\":49874},{\"end\":49891,\"start\":49887},{\"end\":49904,\"start\":49900},{\"end\":49918,\"start\":49914},{\"end\":49932,\"start\":49925},{\"end\":49945,\"start\":49940},{\"end\":50106,\"start\":50100},{\"end\":50117,\"start\":50114},{\"end\":50127,\"start\":50119},{\"end\":50536,\"start\":50532},{\"end\":50549,\"start\":50546},{\"end\":50562,\"start\":50559},{\"end\":50576,\"start\":50572},{\"end\":50590,\"start\":50587},{\"end\":50603,\"start\":50600},{\"end\":50626,\"start\":50613},{\"end\":50631,\"start\":50628},{\"end\":51011,\"start\":51007},{\"end\":51021,\"start\":51018},{\"end\":51035,\"start\":51032},{\"end\":51048,\"start\":51045},{\"end\":51061,\"start\":51058},{\"end\":51503,\"start\":51495},{\"end\":51519,\"start\":51513},{\"end\":52216,\"start\":52208},{\"end\":52233,\"start\":52228},{\"end\":52246,\"start\":52241},{\"end\":52811,\"start\":52807},{\"end\":52824,\"start\":52819},{\"end\":52836,\"start\":52833},{\"end\":52850,\"start\":52847},{\"end\":52863,\"start\":52858},{\"end\":52872,\"start\":52870},{\"end\":52886,\"start\":52881},{\"end\":52904,\"start\":52897},{\"end\":53199,\"start\":53195},{\"end\":53213,\"start\":53209},{\"end\":53220,\"start\":53218},{\"end\":53232,\"start\":53228},{\"end\":53251,\"start\":53247},{\"end\":53824,\"start\":53820},{\"end\":53834,\"start\":53830},{\"end\":53848,\"start\":53845},{\"end\":53870,\"start\":53862},{\"end\":53885,\"start\":53880},{\"end\":53899,\"start\":53893},{\"end\":53919,\"start\":53912},{\"end\":54191,\"start\":54187},{\"end\":54203,\"start\":54199},{\"end\":54215,\"start\":54212},{\"end\":54227,\"start\":54223},{\"end\":54241,\"start\":54236},{\"end\":54260,\"start\":54250},{\"end\":54279,\"start\":54275},{\"end\":54857,\"start\":54853},{\"end\":54871,\"start\":54867},{\"end\":54881,\"start\":54873},{\"end\":54889,\"start\":54885},{\"end\":54897,\"start\":54893},{\"end\":54907,\"start\":54899},{\"end\":55222,\"start\":55219},{\"end\":55238,\"start\":55232},{\"end\":55245,\"start\":55240},{\"end\":55260,\"start\":55254},{\"end\":55270,\"start\":55262},{\"end\":55571,\"start\":55566},{\"end\":55588,\"start\":55580},{\"end\":55606,\"start\":55601},{\"end\":55865,\"start\":55861},{\"end\":55875,\"start\":55871},{\"end\":55891,\"start\":55885},{\"end\":55902,\"start\":55900},{\"end\":55910,\"start\":55908},{\"end\":55922,\"start\":55915},{\"end\":56324,\"start\":56318},{\"end\":56340,\"start\":56335},{\"end\":56352,\"start\":56349},{\"end\":56372,\"start\":56363},{\"end\":57243,\"start\":57235},{\"end\":57256,\"start\":57249},{\"end\":57572,\"start\":57566},{\"end\":57589,\"start\":57580},{\"end\":57603,\"start\":57597},{\"end\":57616,\"start\":57611},{\"end\":57921,\"start\":57917},{\"end\":57934,\"start\":57929},{\"end\":57950,\"start\":57941},{\"end\":58539,\"start\":58526},{\"end\":58557,\"start\":58549},{\"end\":58573,\"start\":58563},{\"end\":58591,\"start\":58585},{\"end\":58607,\"start\":58602},{\"end\":58613,\"start\":58609},{\"end\":59015,\"start\":59013},{\"end\":59030,\"start\":59025},{\"end\":59044,\"start\":59041},{\"end\":59054,\"start\":59051},{\"end\":59444,\"start\":59434},{\"end\":59464,\"start\":59453},{\"end\":59674,\"start\":59666},{\"end\":59685,\"start\":59682},{\"end\":59692,\"start\":59687},{\"end\":59704,\"start\":59696},{\"end\":59711,\"start\":59706},{\"end\":60123,\"start\":60116},{\"end\":60138,\"start\":60134},{\"end\":60154,\"start\":60149},{\"end\":60171,\"start\":60166},{\"end\":60736,\"start\":60729},{\"end\":60750,\"start\":60743},{\"end\":60763,\"start\":60757},{\"end\":60780,\"start\":60771},{\"end\":60793,\"start\":60788},{\"end\":60808,\"start\":60803},{\"end\":60822,\"start\":60816},{\"end\":60834,\"start\":60824},{\"end\":61235,\"start\":61227},{\"end\":61253,\"start\":61246},{\"end\":61266,\"start\":61260},{\"end\":61737,\"start\":61729},{\"end\":61750,\"start\":61743},{\"end\":62008,\"start\":62001},{\"end\":62018,\"start\":62012},{\"end\":62039,\"start\":62032},{\"end\":62045,\"start\":62041},{\"end\":62267,\"start\":62265},{\"end\":62277,\"start\":62275},{\"end\":62289,\"start\":62284},{\"end\":62304,\"start\":62301},{\"end\":62321,\"start\":62312},{\"end\":62343,\"start\":62330},{\"end\":62358,\"start\":62353},{\"end\":62373,\"start\":62367},{\"end\":62766,\"start\":62762},{\"end\":62780,\"start\":62773},{\"end\":62792,\"start\":62788},{\"end\":62803,\"start\":62799},{\"end\":63183,\"start\":63176},{\"end\":63192,\"start\":63188},{\"end\":63205,\"start\":63200},{\"end\":63597,\"start\":63592},{\"end\":63608,\"start\":63606},{\"end\":63620,\"start\":63618},{\"end\":63634,\"start\":63630},{\"end\":63645,\"start\":63640},{\"end\":63658,\"start\":63654},{\"end\":63670,\"start\":63666},{\"end\":63684,\"start\":63681}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":50049,\"start\":49851},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":56517630},\"end\":50432,\"start\":50051},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":102481180},\"end\":50949,\"start\":50434},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":102480461},\"end\":51425,\"start\":50951},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1589010},\"end\":52133,\"start\":51427},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":10329235},\"end\":52735,\"start\":52135},{\"attributes\":{\"doi\":\"abs/1906.02850\",\"id\":\"b6\"},\"end\":53126,\"start\":52737},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.708\",\"id\":\"b7\",\"matched_paper_id\":216056509},\"end\":53747,\"start\":53128},{\"attributes\":{\"id\":\"b8\"},\"end\":54105,\"start\":53749},{\"attributes\":{\"doi\":\"10.18653/v1/2020.findings-emnlp.190\",\"id\":\"b9\",\"matched_paper_id\":216914911},\"end\":54764,\"start\":54107},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":199004599},\"end\":55119,\"start\":54766},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3620071},\"end\":55514,\"start\":55121},{\"attributes\":{\"doi\":\"10.1162/COLI_a_00091\",\"id\":\"b12\",\"matched_paper_id\":13898507},\"end\":55802,\"start\":55516},{\"attributes\":{\"doi\":\"10.1109/CVPR.2009.5206848\",\"id\":\"b13\",\"matched_paper_id\":57246310},\"end\":56228,\"start\":55804},{\"attributes\":{\"doi\":\"10.18653/v1/N19-1423\",\"id\":\"b14\",\"matched_paper_id\":52967399},\"end\":57151,\"start\":56230},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3177185},\"end\":57492,\"start\":57153},{\"attributes\":{\"doi\":\"10.1145/2533682.2533683\",\"id\":\"b16\",\"matched_paper_id\":5571882},\"end\":57856,\"start\":57494},{\"attributes\":{\"doi\":\"10.18653/v1/D19-5615\",\"id\":\"b17\",\"matched_paper_id\":207892291},\"end\":58394,\"start\":57858},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":9740537},\"end\":58957,\"start\":58396},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":206594692},\"end\":59403,\"start\":58959},{\"attributes\":{\"doi\":\"10.1162/neco.1997.9.8.1735\",\"id\":\"b20\",\"matched_paper_id\":1915014},\"end\":59610,\"start\":59405},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":237578994},\"end\":60032,\"start\":59612},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b22\",\"matched_paper_id\":229363322},\"end\":60693,\"start\":60034},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":13756489},\"end\":61160,\"start\":60695},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":9026666},\"end\":61629,\"start\":61162},{\"attributes\":{\"doi\":\"10.1080/10691898.2017.1321974\",\"id\":\"b25\",\"matched_paper_id\":125188957},\"end\":61993,\"start\":61631},{\"attributes\":{\"doi\":\"arXiv:1707.08052\",\"id\":\"b26\"},\"end\":62256,\"start\":61995},{\"attributes\":{\"doi\":\"arXiv:1502.03044\",\"id\":\"b27\"},\"end\":62720,\"start\":62258},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1899153},\"end\":63169,\"start\":62722},{\"attributes\":{\"doi\":\"arXiv:2103.03230\",\"id\":\"b29\"},\"end\":63509,\"start\":63171},{\"attributes\":{\"id\":\"b30\"},\"end\":63885,\"start\":63511}]", "bib_title": "[{\"end\":50091,\"start\":50051},{\"end\":50521,\"start\":50434},{\"end\":50996,\"start\":50951},{\"end\":51486,\"start\":51427},{\"end\":52199,\"start\":52135},{\"end\":53187,\"start\":53128},{\"end\":54179,\"start\":54107},{\"end\":54849,\"start\":54766},{\"end\":55213,\"start\":55121},{\"end\":55558,\"start\":55516},{\"end\":55855,\"start\":55804},{\"end\":56310,\"start\":56230},{\"end\":57225,\"start\":57153},{\"end\":57560,\"start\":57494},{\"end\":57912,\"start\":57858},{\"end\":58515,\"start\":58396},{\"end\":59003,\"start\":58959},{\"end\":59427,\"start\":59405},{\"end\":59662,\"start\":59612},{\"end\":60109,\"start\":60034},{\"end\":60720,\"start\":60695},{\"end\":61213,\"start\":61162},{\"end\":61719,\"start\":61631},{\"end\":62753,\"start\":62722}]", "bib_author": "[{\"end\":49868,\"start\":49853},{\"end\":49881,\"start\":49868},{\"end\":49893,\"start\":49881},{\"end\":49906,\"start\":49893},{\"end\":49920,\"start\":49906},{\"end\":49934,\"start\":49920},{\"end\":49947,\"start\":49934},{\"end\":50108,\"start\":50093},{\"end\":50119,\"start\":50108},{\"end\":50129,\"start\":50119},{\"end\":50538,\"start\":50523},{\"end\":50551,\"start\":50538},{\"end\":50564,\"start\":50551},{\"end\":50578,\"start\":50564},{\"end\":50592,\"start\":50578},{\"end\":50605,\"start\":50592},{\"end\":50628,\"start\":50605},{\"end\":50633,\"start\":50628},{\"end\":51013,\"start\":50998},{\"end\":51023,\"start\":51013},{\"end\":51037,\"start\":51023},{\"end\":51050,\"start\":51037},{\"end\":51063,\"start\":51050},{\"end\":51505,\"start\":51488},{\"end\":51521,\"start\":51505},{\"end\":52218,\"start\":52201},{\"end\":52235,\"start\":52218},{\"end\":52248,\"start\":52235},{\"end\":52813,\"start\":52799},{\"end\":52826,\"start\":52813},{\"end\":52838,\"start\":52826},{\"end\":52852,\"start\":52838},{\"end\":52865,\"start\":52852},{\"end\":52874,\"start\":52865},{\"end\":52888,\"start\":52874},{\"end\":52906,\"start\":52888},{\"end\":53201,\"start\":53189},{\"end\":53215,\"start\":53201},{\"end\":53222,\"start\":53215},{\"end\":53234,\"start\":53222},{\"end\":53253,\"start\":53234},{\"end\":53826,\"start\":53813},{\"end\":53836,\"start\":53826},{\"end\":53850,\"start\":53836},{\"end\":53872,\"start\":53850},{\"end\":53887,\"start\":53872},{\"end\":53901,\"start\":53887},{\"end\":53921,\"start\":53901},{\"end\":54193,\"start\":54181},{\"end\":54205,\"start\":54193},{\"end\":54217,\"start\":54205},{\"end\":54229,\"start\":54217},{\"end\":54243,\"start\":54229},{\"end\":54262,\"start\":54243},{\"end\":54281,\"start\":54262},{\"end\":54859,\"start\":54851},{\"end\":54873,\"start\":54859},{\"end\":54883,\"start\":54873},{\"end\":54891,\"start\":54883},{\"end\":54899,\"start\":54891},{\"end\":54909,\"start\":54899},{\"end\":55224,\"start\":55215},{\"end\":55240,\"start\":55224},{\"end\":55247,\"start\":55240},{\"end\":55262,\"start\":55247},{\"end\":55272,\"start\":55262},{\"end\":55573,\"start\":55560},{\"end\":55590,\"start\":55573},{\"end\":55608,\"start\":55590},{\"end\":55867,\"start\":55857},{\"end\":55877,\"start\":55867},{\"end\":55893,\"start\":55877},{\"end\":55904,\"start\":55893},{\"end\":55912,\"start\":55904},{\"end\":55924,\"start\":55912},{\"end\":56326,\"start\":56312},{\"end\":56342,\"start\":56326},{\"end\":56354,\"start\":56342},{\"end\":56374,\"start\":56354},{\"end\":57245,\"start\":57227},{\"end\":57258,\"start\":57245},{\"end\":57574,\"start\":57562},{\"end\":57591,\"start\":57574},{\"end\":57605,\"start\":57591},{\"end\":57618,\"start\":57605},{\"end\":57923,\"start\":57914},{\"end\":57936,\"start\":57923},{\"end\":57952,\"start\":57936},{\"end\":58541,\"start\":58517},{\"end\":58559,\"start\":58541},{\"end\":58575,\"start\":58559},{\"end\":58593,\"start\":58575},{\"end\":58609,\"start\":58593},{\"end\":58615,\"start\":58609},{\"end\":59017,\"start\":59005},{\"end\":59032,\"start\":59017},{\"end\":59046,\"start\":59032},{\"end\":59056,\"start\":59046},{\"end\":59446,\"start\":59429},{\"end\":59466,\"start\":59446},{\"end\":59676,\"start\":59664},{\"end\":59687,\"start\":59676},{\"end\":59694,\"start\":59687},{\"end\":59706,\"start\":59694},{\"end\":59713,\"start\":59706},{\"end\":60125,\"start\":60111},{\"end\":60140,\"start\":60125},{\"end\":60156,\"start\":60140},{\"end\":60173,\"start\":60156},{\"end\":60738,\"start\":60722},{\"end\":60752,\"start\":60738},{\"end\":60765,\"start\":60752},{\"end\":60782,\"start\":60765},{\"end\":60795,\"start\":60782},{\"end\":60810,\"start\":60795},{\"end\":60824,\"start\":60810},{\"end\":60836,\"start\":60824},{\"end\":61237,\"start\":61215},{\"end\":61255,\"start\":61237},{\"end\":61268,\"start\":61255},{\"end\":61739,\"start\":61721},{\"end\":61752,\"start\":61739},{\"end\":62010,\"start\":61997},{\"end\":62020,\"start\":62010},{\"end\":62041,\"start\":62020},{\"end\":62047,\"start\":62041},{\"end\":62269,\"start\":62258},{\"end\":62279,\"start\":62269},{\"end\":62291,\"start\":62279},{\"end\":62306,\"start\":62291},{\"end\":62323,\"start\":62306},{\"end\":62345,\"start\":62323},{\"end\":62360,\"start\":62345},{\"end\":62375,\"start\":62360},{\"end\":62768,\"start\":62755},{\"end\":62782,\"start\":62768},{\"end\":62794,\"start\":62782},{\"end\":62805,\"start\":62794},{\"end\":63185,\"start\":63171},{\"end\":63194,\"start\":63185},{\"end\":63207,\"start\":63194},{\"end\":63599,\"start\":63582},{\"end\":63610,\"start\":63599},{\"end\":63622,\"start\":63610},{\"end\":63636,\"start\":63622},{\"end\":63647,\"start\":63636},{\"end\":63660,\"start\":63647},{\"end\":63672,\"start\":63660},{\"end\":63686,\"start\":63672}]", "bib_venue": "[{\"end\":50250,\"start\":50198},{\"end\":51204,\"start\":51142},{\"end\":51768,\"start\":51641},{\"end\":52471,\"start\":52368},{\"end\":53443,\"start\":53371},{\"end\":56687,\"start\":56538},{\"end\":58104,\"start\":58042},{\"end\":59197,\"start\":59135},{\"end\":60300,\"start\":60247},{\"end\":61409,\"start\":61347},{\"end\":62964,\"start\":62893},{\"end\":50196,\"start\":50129},{\"end\":50683,\"start\":50633},{\"end\":51140,\"start\":51063},{\"end\":51639,\"start\":51521},{\"end\":52366,\"start\":52248},{\"end\":52797,\"start\":52737},{\"end\":53369,\"start\":53282},{\"end\":53811,\"start\":53749},{\"end\":54385,\"start\":54316},{\"end\":54932,\"start\":54909},{\"end\":55297,\"start\":55272},{\"end\":55653,\"start\":55628},{\"end\":56012,\"start\":55949},{\"end\":56536,\"start\":56394},{\"end\":57315,\"start\":57258},{\"end\":57673,\"start\":57641},{\"end\":58040,\"start\":57972},{\"end\":58662,\"start\":58615},{\"end\":59133,\"start\":59056},{\"end\":59505,\"start\":59492},{\"end\":59792,\"start\":59713},{\"end\":60245,\"start\":60177},{\"end\":60885,\"start\":60836},{\"end\":61345,\"start\":61268},{\"end\":61812,\"start\":61781},{\"end\":62467,\"start\":62391},{\"end\":62891,\"start\":62805},{\"end\":63323,\"start\":63223},{\"end\":63580,\"start\":63511}]"}}}, "year": 2023, "month": 12, "day": 17}