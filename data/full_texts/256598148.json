{"id": 256598148, "updated": "2023-10-05 04:43:53.591", "metadata": {"title": "Explicit Box Detection Unifies End-to-End Multi-Person Pose Estimation", "authors": "[{\"first\":\"Jie\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Ailing\",\"last\":\"Zeng\",\"middle\":[]},{\"first\":\"Shilong\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Feng\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Ruimao\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Lei\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "This paper presents a novel end-to-end framework with Explicit box Detection for multi-person Pose estimation, called ED-Pose, where it unifies the contextual learning between human-level (global) and keypoint-level (local) information. Different from previous one-stage methods, ED-Pose re-considers this task as two explicit box detection processes with a unified representation and regression supervision. First, we introduce a human detection decoder from encoded tokens to extract global features. It can provide a good initialization for the latter keypoint detection, making the training process converge fast. Second, to bring in contextual information near keypoints, we regard pose estimation as a keypoint box detection problem to learn both box positions and contents for each keypoint. A human-to-keypoint detection decoder adopts an interactive learning strategy between human and keypoint features to further enhance global and local feature aggregation. In general, ED-Pose is conceptually simple without post-processing and dense heatmap supervision. It demonstrates its effectiveness and efficiency compared with both two-stage and one-stage methods. Notably, explicit box detection boosts the pose estimation performance by 4.5 AP on COCO and 9.9 AP on CrowdPose. For the first time, as a fully end-to-end framework with a L1 regression loss, ED-Pose surpasses heatmap-based Top-down methods under the same backbone by 1.2 AP on COCO and achieves the state-of-the-art with 76.6 AP on CrowdPose without bells and whistles. Code is available at https://github.com/IDEA-Research/ED-Pose.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2302.01593", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/YangZLLZ023", "doi": "10.48550/arxiv.2302.01593"}}, "content": {"source": {"pdf_hash": "54d67bd651eb76cef1224f9f9d7c167ae9266c7d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2302.01593v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "778b458c25e405d7c7ecc69f29d798477ecff1c8", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/54d67bd651eb76cef1224f9f9d7c167ae9266c7d.txt", "contents": "\nPublished as a conference paper at ICLR 2023 EXPLICIT BOX DETECTION UNIFIES END-TO-END MULTI-PERSON POSE ESTIMATION\n\n\nJie Yang jieyang5@link \nInternational Digital Economy Academy (IDEA)\n\n\nShenzhen Research Institute of Big Data\nThe Chinese University of Hong Kong\nShenzhen\n\nAiling Zeng zengailing@idea.edu.cn \nInternational Digital Economy Academy (IDEA)\n\n\n\u2020 \nShilong Liu liushilong@idea.edu.cn \nInternational Digital Economy Academy (IDEA)\n\n\nFeng Li lifeng@idea.edu.cn \nRuimao Zhang zhangruimao@cuhk.edu.cn \nInternational Digital Economy Academy (IDEA)\n\n\nShenzhen Research Institute of Big Data\nThe Chinese University of Hong Kong\nShenzhen\n\n\u2020 \nLei Zhang leizhang@idea.edu.cn \nInternational Digital Economy Academy (IDEA)\n\n\nPublished as a conference paper at ICLR 2023 EXPLICIT BOX DETECTION UNIFIES END-TO-END MULTI-PERSON POSE ESTIMATION\n\nThis paper presents a novel end-to-end framework with Explicit box Detection for multi-person Pose estimation, called ED-Pose, where it unifies the contextual learning between human-level (global) and keypoint-level (local) information. Different from previous one-stage methods, ED-Pose re-considers this task as two explicit box detection processes with a unified representation and regression supervision. First, we introduce a human detection decoder from encoded tokens to extract global features. It can provide a good initialization for the latter keypoint detection, making the training process converge fast. Second, to bring in contextual information near keypoints, we regard pose estimation as a keypoint box detection problem to learn both box positions and contents for each keypoint. A human-to-keypoint detection decoder adopts an interactive learning strategy between human and keypoint features to further enhance global and local feature aggregation. In general, ED-Pose is conceptually simple without postprocessing and dense heatmap supervision. It demonstrates its effectiveness and efficiency compared with both two-stage and one-stage methods. Notably, explicit box detection boosts the pose estimation performance by 4.5 AP on COCO and 9.9 AP on CrowdPose. For the first time, as a fully end-to-end framework with a L1 regression loss, ED-Pose surpasses heatmap-based Top-down methods under the same backbone by 1.2 AP on COCO and achieves the state-of-theart with 76.6 AP on CrowdPose without bells and whistles.\n\nINTRODUCTION\n\n\nGlobal:\n\nWhere is the \"person\"?  Multi-person human pose estimation has attracted much attention in the computer vision community for decades for its wide applications in areas of augmented reality (AR), virtual reality (VR), and human-computer interaction (HCI). Given an image, it targets to localize the 2D keypoint positions for every person in the image. Although many methods have been developed (Xiao et al., 2018; Intuitively, as shown in Figure 1, this task needs to focus on both global (human-level) and local (keypoint-level) dependencies, which concentrate on different levels of semantic granularity. Mainstream solutions are normally two-stage methods, which divide the problem into two separate subproblems (e.g., the global person detection and local keypoint regression). Such solutions include Top-Down (TD) methods (Xiao et al., 2018;Sun et al., 2019;Li et al., 2021b; which are of high performance yet with a high inference cost and Bottom-Up (BU) solutions (Cao et al., 2017;Newell et al., 2017;Cheng et al., 2020) which are fast in inference yet with relatively lower precision. However, all of these methods are non-differentiable between their global and local stages due to hand-crafted operations, like Non-Maximum Suppression (NMS), Region of Interest (RoI) cropping, and keypoint grouping post-processing. Lately, Poseur  tries to directly apply top-down methods to an end-to-end framework and finds that there will be a significant performance drop (about 8.7 AP on COCO), indicating the optimization conflicts between the learning of global and local relations.\n\nExploring a fully end-to-end trainable method to unify the two disassembled subproblems is attractive and important. Inspired by the success of recent end-to-end object detection methods, like DETR (Carion et al., 2020), there is a surge of related approaches that regard human pose estimation as a direct set prediction problem. They utilize a bipartite matching for one-to-one prediction with Transformers to avoid cumbersome post-processings (Li et al., 2021b;Mao et al., 2021a;2022;Stoffl et al., 2021;. Recently, PETR  proposes a fully end-to-end framework to predict instance-aware poses without any post-processings and shows a favorable potential. Nevertheless, it directly uses a pose decoder with randomly initialized pose queries to query local features from images. The only local dependency makes keypoint matching across persons ambiguous and thus leading to inferior performance, especially for occlusions, complex poses, and diverse human scales in crowded scenes. Moreover, either two-stage methods or DETR-based estimators suffer from slow training convergence and need more epochs (e.g., train a model above a week) to achieve high precision. Additionally, the convergence speed of DETR-based methods is even slower than bottom-up methods (Cheng et al., 2020). We address the details in Sec.3.\n\nBased on the above observations, this work re-considers multi-person pose estimation as two Explicit box Detection processes named ED-Pose. We realize each box detection by using a decoder and cascade them to form an end-to-end framework, which makes the model fast in convergence, precise, and scalable. Specifically, to obtain global dependencies, the first process detects boxes for all persons via human box queries selected from the encoded image tokens. This simple step can provide a good initialization for the latter keypoint detection to accelerate the training convergence. Then, to capture local contextual relations and reduce ambiguities in the feature alignment, we regard the following pose estimation task as a keypoint box detection problem, where it learns both box positions and local contents for each keypoint. Such an approach can leverage contextual information near a keypoint by directly regressing the keypoint box position and learning local keypoint content queries without dense supervision (e.g., heatmap). To further enhance the global-local interactivity among external human-human, internal human-keypoint, and internal keypoint-keypoint, we design interactive learning between human detection and keypoint detection.\n\nFollowing the two Explicit box Detection processes, we can unify the global and local feature learning using the consistent regression loss and the same box representation in an end-to-end framework. We summarize the related methods from the supervisions and representations. Compared with previous works, ED-Pose is more conceptually simple. Notably, we find that explicit global box detection will gain 4.5 AP on COCO and 9.9 AP on CrowdPose compared with a solution without such a scheme. In comparison to top-down methods, ED-Pose makes the human and keypoint detection share the same encoders to avoid redundant costs from human detection and further boost performance by 1.2 AP on COCO and 9.1 AP on CrowdPose under the same ResNet-50 backbone. Moreover, ED-Pose surpasses the previous end-to-end model PETR significantly by 2.8 AP on COCO and 5.0 AP on CrowdPose. In crowded scenes, ED-Pose achieves the state-of-the-art with 76.6 AP (by 4.2 AP improvement over the previous SOTA (Yuan et al., 2021)) without any bells and whistles (e.g., without multi-scale test and flipping). We hope this simple attempt at explicit box detection, simplification of losses, and no post-processing to unify the whole pipeline could bring in new perspectives to further one-stage framework designs. \n\n\nRELATED WORK\n\nOne-stage Multi-Person Pose Estimation: With the development of anchor-free object detectors (Tian et al., 2019b;Huang et al., 2015), DirectPose  directly predicts instanceaware keypoints for all persons from an image. The direct end-to-end framework provides a new perspective to avoid the above cumbersome issues met in two-stage methods. Generally speaking, these methods densely locate a set of pose candidates, which consist of joint positions from the same person. FCPose  builds upon dynamic filters (Jia et al., 2016) in compact keypoint heads to boost both accuracy and speed. Meanwhile, Inspose (Shi et al., 2021) designs instance-aware dynamic networks to adaptively adjust part of the network parameters for each instance. Nevertheless, these one-stage methods still need NMS to remove duplicates in the postprocessing stage. To further remove such hand-crafted components, PETR  views pose estimation as a hierarchical set prediction problem and proposes the first fully end-to-end pose estimation framework with the advent of DETR (Carion et al., 2020).\n\nDetection Transformers: For the first time, DETR (Carion et al., 2020) performs object detection in an end-to-end manner by using a set-based global loss that forces directly unique predictions via bipartite matching and a Transformer encoder-decoder architecture. It simplifies object detection as a direct set prediction problem, dropping multiple hand-designed components and prior knowledge. Due to the effectiveness of DETR and its varieties (e.g., Deformable DETR (Zhu et al., 2020)), their frameworks have been widely transferred in many complex tasks, such as Mask DINO (Li et al., 2022b) for segmentation and PETR  for pose estimation. Following the top-down methods, PRTR (Li et al., 2021b) and TFPose (Mao et al., 2021a) adopt Detection Transformers to estimate the cropped single-person images as a query-based regression task. To capture the underlying output distribution and further improve performance in the regression paradigm, Poseur  brings Residual Log-likelihood Estimation (RLE) (Li et al., 2021a) into the DETR-based top-down framework, achieving the state-of-the-art performance of regression-based methods. For the one-stage manner, POET (Stoffl et al., 2021) utilizes the property of DETR to directly regress the poses (instead of bounding boxes) of all person instances in an image. Recently, PETR ) designs a fully end-to-end paradigm with hierarchical attention decoders to capture both relations between poses and kinematic joints.\n\nAll the aforementioned methods take advantage of Detection Transformers and densely regress a set of poses (only local relations). However, they ignore the importance of introducing explicit box detection in pose estimation to model both global and local dependencies well.\n\n\nRETHINKING ONE-STAGE MULTI-PERSON POSE ESTIMATION\n\nThe Necessities of One-stage Methods: For a long time, the two-stage paradigm dominates the mainstream methods for multi-person pose estimators. It can be generally divided into top-down methods and bottom-up methods. Top-Down (TD) methods (Xiao et al., 2018;Sun et al., 2019; disentangle the task into detecting and cropping each person with an object detector (e.g., Mask RCNN (He et al., 2017)) in an image from the global (person-level) dependency and then conduct the single-person pose estimation via another model. They focus on local (keypointlevel) relation modeling and improving the accuracy of single-person pose estimation. However, these methods still suffer from 1). heavy reliance on the performance of the human detector, 2). redundantly computational costs for additional human detection and RoI operations, and 3). the separate training for the human detector and the corresponding pose estimator. Instead, Bottom-Up (BU) methods (Cao et al., 2017;Newell et al., 2017;Cheng et al., 2020) first detect all the keypoints in an instance-agnostic fashion. Next, they employ a heuristical grouping algorithm to associate the detected keypoints that belong to the same person, which improves efficiency. Even so, the complicated grouping scheme makes bottom-up methods hard to handle heavy occlusions and multiple-person scales, resulting in inferior performance. More importantly, both of them suffer from non-differentiable optimization between the global and local features, which is not perceptive. Intuitively, one-stage methods could alleviate the above issues since all modules can be optimized in an end-to-end manner, and balance effectiveness and efficiency. Interestingly, the recent DETRbased top-down method Poseur  tries to directly apply it to a one-stage framework and finds that it suffers from a significant performance drop. It might be the optimization conflicts between the global and local dependency learning using the shared encoder. Thus, how to design a one-stage framework effectively is still challenging and questionable.\n\nThe Bottlenecks of Existing One-stage Methods: In terms of existing DETR-based methods, most of them still adopt the top-down framework, and improve the second single-person pose estimation via regarding it as a sequence prediction problem Shi et al., 2021). PETR is the first work to make the whole pipeline end-to-end without any post-processing. However, existing methods still have some limitations. First, all of them only utilize local dependencies to regress keypoints. Directly regressing keypoints for each person via pose queries is semantically ambiguous as the bottom-up strategies to find all keypoints from raw images instead of from cropped images. Second, the pose or keypoint queries proposed in the above methods are randomly initialized without utilizing previously extracted features, making the training phase slow and ineffective. Third, the keypoint representation as a point lacks contextual information when it queries from the encoded features, leading to feature misalignment. Last, the interactions among global-to-global, global-tolocal, and local-to-local are complex, especially in crowd scenes. The current models do not pay attention to handling these complicated relations. In this article, we try to tackle the above issues by using unified box representations and regression losses in a one-stage process. \n\n\nFine Human Query Selection\n\nHuman-to-Keypoint Query Expansion Figure 2: The overview architecture of our ED-Pose, which contains a Human Detection Decoder and a Human-to-Keypoint Detection Decoder to detect human and keypoint boxes explicitly.\n\nAs illustrated in Figure 2, the proposed ED-Pose is a fully end-to-end multi-person pose estimation framework without any post-processing. Given an input image, we first utilize a backbone to extract multi-scale features and obtain tokenized representations F, and then feed them into the Transformer encoder (e.g., deformable attention modules (Zhu et al., 2020)) with Positional Embeddings (PE) to calculate the refined tokens F . To improve the efficiency of the following decoding process, we conduct coarse human query selection from F to obtain the sparse human content queries Q c H . Then we utilize Q c H to generate the corresponding position queries Q p H via a Feed-Forward Network (FFN) and feed both Q c H and Q p H into the Human Detection Decoder to update into the corresponding queries Q c H and Q p H , respectively (see Sec.4.2). For each output human content query, we attach a human box regression and class entropy supervision (L l h and L l c ) at the l-th decoder layer. Next, we further perform fine human query selection to discard the redundant human queries, obtaining content queries Q c Hs and position queries Q p Hs . We initialize keypoint queries based on these retained high-quality human queries and concatenate the human and keypoint queries together to form human-keypoint queries. We name such a scheme as human-to-keypoint query expansion, and the obtained human-keypoint content queries Q c H,K and position queries Q p H,K are fed into Human-to-Keypoint Detection Decoder. In this way, we realize the keypoint detection at an instance level and update these queries to Q c H,K and Q p H,K layer by layer (see Sec.4.3). Also, the keypoint and human box regression loss and class loss (L l k , L l h and L l c ) are added to the l-th decoder layer. Finally, we use several FFN heads to regress the keypoint positions in each human box.\n\nLoss: Following the DETR (Carion et al., 2020), we employ a set-based Hungarian loss that forces a unique prediction for each ground-truth box and keypoint. Our overall loss functions contain classification L c , human box regression L h , and keypoint regression loss L k . Notably, L k simply consists of the normal L1 loss and the constrained L1 loss named Object Keypoint Similarity (OKS) loss  without any dense supervision (e.g., heatmap). \n\n\nHUMAN DETECTION DECODER\n\nIn Figure 3 (a), the proposed Human Detection Decoder aims to predict the candidate bounding box positions for each person in the image, as well as the corresponding content representation by leveraging image context from all encoded tokens. Given N input human content queries Q c H \u2208 R N \u00d7D and human position queries Q p H \u2208 R N \u00d74 , where D is the channel dimension, the Human Detection Decoder outputs N refined human box positions Q p H and content representations Q c H . Coarse-to-Fine Human Query Selection: Unlike previous work  to randomly initialize the input queries, we adopt the query selection (QS) strategy for better initialization . Specifically, we propose a coarse-to-fine scheme to progressively select high-quality human queries. In practice, the coarse human QS selects first from a large number of refined tokens F \u2208 R T \u00d7D , where T is the number of the tokens (e.g., about 15K). Thus, the human content queries Q c H \u2208 R N \u00d7D are initialized by the top-N refined tokens' features, ranked by their instance classification scores. Here N indicates the number of candidate human boxes (e.g., 900). Then we use these selected human content queries to calculate their position queries Q p H \u2208 R N \u00d74 by employing a simple detection head. After passing through the Human Detection Decoder, we further conduct the fine human QS by only retaining M (e.g., 100) refined human content queries and their position queries according to the classification scores, denoted as Q c Hs \u2208 R M \u00d7D and Q p Hs \u2208 R M \u00d74 . Human Box Detection: As illustrated in Figure 3 (a), we combine Q c H with Q p H and feed them into Human-to-Human Attention (i.e. a self-attention layer) to calculate the relations among human queries, and output contextual enhanced ones. Motivated by the deformable DETR Zhu et al. (2020), these enhanced human content queries, together with their position queries are further fed into Deformable Token-to-Human Attention (i.e. a cross-attention layer) to update human queries by conducting interaction with multi-scale tokens. Next, to adjust the human box positions, we leverage updated human queries to calculate the 4D offsets following the DAB-DETR , and add them back to previous human position queries. In this way, the Human Detection Decoder refine the human content and position queries progressively by stacking multiple aforementioned layers and output Q c H \u2208 R N \u00d7D and Q p H \u2208 R N \u00d74 .\n\n\nHUMAN-TO-KEYPOINT DETECTION DECODER\n\nTo unify both human and keypoint representations as the box and facilitate follow-up interactive learning, we regard multi-person pose estimation as the multiple set keypoint box detection problems. Based on the selected high quality human queries Q p Hs and Q c Hs , we initialize multiple sets of keypoint queries (i.e. including both content and position queries) through the Human-to-Keypoint Query Expansion process, and concatenate the human and keypoint queries together as inputs of the Human-to-Keypoint Detection Decoder ( Figure. 3 (b)) to calculate the precise keypoint positions for each person.\n\nHuman-to-Keypoint Query Expansion: After obtaining Q p Hs and Q c Hs , the keypoint content query and positional query are initialized by the relevant human query instead of random initialization. In practice, we first initialize a set of learnable keypoint embeddings V e \u2208 R 1\u00d7K\u00d7D where K is the total number of keypoints (e.g., 17). To concretize these embeddings to multiple specific persons' keypoint queries, we broadcast the first dimension of V e to the number of human queries M and then add it with Q c Hs \u2208 R M \u00d71\u00d7D to obtain M set of keypoint content queries. For a specific set of keypoint position queries, we separate the initialization process into center coordinate initialization (e.g., {x, y}) and box size initialization (e.g., {w, h}). For the former, we adopt an FFN to regress all of the K positions' coordinates by employing the corresponding human content query. For the latter, the sizes of K boxes are conditioned by the width and height of the corresponding human box via dot-multiplying dynamic weights W \u2208 R K\u00d72 . After that, we consider each human box and the corresponding set of keypoint boxes as a whole and generate the human-keypoint content queries Q c H,K \u2208 R (M +M * K)\u00d7D and position queries Q p H,K \u2208 R (M +M * K)\u00d74 for further predictions. The Interactive Learning between Human and Keypoint Detection: As illustrated in Figure 3 (b) and (c), after generating human-keypoint queries, we feed them into Human-to-Keypoint Interactive Attention to learn relations from internal human-keypoint, internal keypoint-keypoint, and external human-human. Such an interactive learning method between global and local feature aggregations has been ingeniously introduced to ensure the effectiveness of keypoint feature extraction. In practice, we find that the fully connected interactive learning with external keypoint-keypoint would cause a large disturbance, especially in crowded situations, similar to problems encountered in bottom-up methods. In contrast, our external human-human interactions effectively propagate global context across different candidates and distinguish their relations clearly. Then we take the enhanced human-to-keypoint content queries to conduct the interaction with multi-scale encoded tokens to obtain updated human-to-keypoint queries. By stacking multiple layers illustated in Figure 3 (b), we finally obtain the refined human-to-keypoint queries, denoted as Q c H,K and Q p H,K .\n\n\nEXPERIMENTS\n\nDue to the page limit, we leave the detailed experiment setup in A, comparison results on COCO test-dev in B, comparisons on human detection in C, more qualitative results and analyses in D, and more discussion for ED-Pose (in Sec. E).\n\n\nRESULTS ON CROWDPOSE\n\nWe first verify the effectiveness of ED-Pose with other state-of-the-art methods on the CrowdPose test set in Table 2. Compared with the top-down methods, we surpass the SimpleBaseline (Xiao et al., 2018) under the same backbone by 9.1 AP. we also show superiority on all previous methods combined with Swin-L (Liu et al., 2021) backbone and outperform PETR by 1.5 AP under the same backbone. When we enlarge the scales of backbones, ED-Pose will achieve the state-of-the-art 76.6 AP without multi-scale and flip tests.\n\n\nRESULTS ON COCO\n\nWe further make comparisons with the state-of-the-art methods on COCO val2017 and test-dev in Table. 3 and Table. 8 respectively. In general, our ED-Pose outperforms all existing bottom-up methods and one-stage methods under the same backbone without any tricks, even the dense heatmap-based top-down methods. The proposed method achieves 71.6 AP (by 2.8 AP improvement) with a 51.4% inference time reduction via the ResNet-50 backbone. Additionally, our best model with the Swin-L* backbone achieves a 75.8 AP, showing a consistent performance improvement. The improvements in the test-dev are similar.  \n\n\nCOMPARISON OF EFFECTIVENESS\n\nComparison with one-stage methods: Our method significantly outperforms all existing one-stage methods, especially in Table. 8, such as DirecetPose , FCPose , Inspose (Shi et al., 2021), CenterNet , and PETR , showing that ED-Pose with explicit box detection is an effective solution for the end-to-end framework.\n\nComparison with two-stage methods: For bottom-up methods, ED-Pose outperforms state-of-theart methods by a large margin, such as HigherHRNet (Cheng et al., 2020), DEKR (Geng et al., 2021), and SWAHR (Luo et al., 2021). Specifically, ED-Pose significantly surpasses the recently proposed SWAHR by 3.7 AP (71.6 AP vs. 67.9 AP) even with a much smaller backbone (ResNet-50 vs. HRNet-w32). For top-down methods, ED-Pose is 1.2 AP and 3.4 AP higher than SimpleBaseline (Xiao et al., 2018) (Sim.Base.) and PRTR (Li et al., 2021b), respectively. To the best of our knowledge, this is the first time that a fully end-to-end framework can surpass top-down methods.\n\nComparison with end-to-end top-down methods: Poseur  extends its framework to end-to-end human pose estimation with Mask-RCNN. Similarly, PRTR presents its end-to-end variant. However, compared to their two-stage paradigms, their end-to-end frameworks produce substantially inferior performance. Our fully end-to-end framework achieves much higher AP scores than theirs, indicating the effectiveness of our proposed methods.\n\nQualitative results: With explicit box detection, ED-Pose can perform well on a wide range of poses, containing viewpoint change, occlusion, motion blur, and crowded scenes. Fig. 4 demonstrates the results of the detected person and the corresponding keypoints. As can be observed, the human boxes are precise under many severe cases, thus they can provide effective global human information for further keypoint detection. The local regions of the keypoint boxes are also reasonable for bringing abundant contextual information near the keypoint. We present both explicitly detected person boxes and keypoint boxes to understand how they work. Figure 5: Comparisons of convergence speeds in the training stage (the left) and trade-offs between inference time and performance (the right) of existing mainstream methods. Our proposed one-stage method ED-Pose shows the superiority of efficiency compared with the Bottom-Up (BU) model HigherHRNet (Cheng et al., 2020), Top-Down (TD) models Sim.Base. (Xiao et al., 2018) and DETR-based Poseur , the one-stage method PETR .\n\n\nCOMPARISON OF EFFICIENCY\n\nFor the inference time, ED-Pose surpasses all the bottom-up and one-stage methods in both speed and accuracy fields from Table. 3 and 8. Notably, from the right of Fig. 5 (Li et al., 2021a)), ED-Pose can boost inference speed by 30.7%. Moreover, the left of Fig. 5 shows the convergence speeds in the training stage that are rarely discussed before. EP-Pose introducing explicit detection is faster than all methods and obtains better performance under the early epochs. Interestingly, all DETR-based methods are slower than even bottom-up methods.\n\n\nABLATION STUDY\n\nExplicit human detection: We first analyze the effectiveness of explicit human detection on both the COCO val2017 and the CrowdPose test, using the ResNet-50 backbone. We remove all human detection losses in decoder layers to verify its effectiveness. Results in Table. 4 clearly verify that explicit human detection supervision significantly improves the convergence speed of keypoint detection and precision, yielding +4.5 AP on COCO and +9.9 AP on CrowdPose. Keypoint detection representation: ED-Pose reformulates pose estimation as the keypoint box detection, where the 2D center-point coordinate (x, y) of keypoint is extended to the 4D representation (x, y, w, h). As the region of feature aggregation for keypoint depends on the width and height of the keypoint box, we explore the effect of (w, h) under five initialization ways: 1). None discards width and height and keeps the original 2D coordinate representation ; 2). Min. denotes that the keypoint is initialized by 1% width and height of the corresponding human box; 3). Max. means that a keypoint is directly initialized by the width and height of its corresponding human box; 4). FFN. is that we apply an FFN network upon the human content query to regress a 2D weight of each keypoint within the human box ; 5). Ours means that we initialize learnable embeddings for each keypoint across the whole dataset and utilize it to weigh the width and height of the human box. As shown in Table. 5, the result without width and height representation is much lower than those of the other four settings, leading to a 13.3 AP drop compared with the best initialization way (Ours) since it will lose contextual information to query the local and global relations. Learnable width and height obtain better performance than the fixed sizes.\n\nThe interactive learning between human and keypoint detection: Our human-to-keypoint detection decoder enhances the interactivity among external human-to-human, internal human-tokeypoint, and internal keypoint-to-keypoint. Thus, we explore the impact of different interactive learning strategies: 1) Full makes all human queries and keypoint queries interact one by one; 2) w/o H-K removes the internal human-to-keypoint interaction; 3) w/o H-H discards the external human-to-human interaction; 4) Ours is our full interactive learning strategy. As shown in Table. 6, direct full connection for all queries could lead to 1.2 AP degradation. The reason is that external keypoint-keypoint interaction in a full connection strategy may cause a large disturbance, especially in crowded situations. Besides, other interactive strategies slightly affect performance as well.\n\nHyperparameter tuning: To explore the effect on the number M of selected queries in Sec. 4.2, we conduct three numbers with 50, 100, and 200, respectively. As shown in Table. 7, increasing selected human queries can improve performance, but it meets a bottleneck for further improvement.   \n\n\nCONCLUSION\n\nIn this work, we re-consider the multi-person pose estimation task as two explicit box detection processes. We unify the global person and local keypoint into the same box representation, and they can be optimized by the consistent regression loss in a fully end-to-end manner. Based on the novel methods, the proposed ED-Pose surpasses existing end-to-end methods by a large margin and shows superiority over the long-standing two-stage methods on both COCO and CrowdPose. This is a simple attempt to make the whole pipeline succinct and effective. We hope this work could inspire further one-stage designs.\n\n\nAppendix: Explicit Box Detection Unifies End-to-End Multi-Person Pose Estimation\n\nIn this Appendix, we provide descriptions of a detailed experimental setup (in Sec. A), more comparisons under COCO test-dev (in Sec. B), results of human detection on both COCO and CrowdPose datasets (in Sec. C), more visualization to show the effect of explicit box detection and comparison with PETR (in Sec. D), more discussion for ED-Pose (in Sec. E). We also append our code to reproduce the results.\n\n\nA EXPERIMENT SETUP\n\nDataset. Our experiments are mainly conducted on the popular COCO2017 Keypoint Detection benchmark (Lin et al., 2014), which contains about 250K person instances with 17 keypoints. We compare with other state-of-the-art methods on both the val2017 set and test-dev set. To verify the superiority of explicit detection, we also evaluate our approach on the CrowdedPose dataset (Li et al., 2019) which is more challenging and includes many crowded and occlusion scenes. It consists of 20K images containing about 80K persons with 14 keypoints. For ablation studies, we report results on the COCO val2017 set. The OKS-based Average Precision (AP) is employed as the main evaluation metric on both datasets.\n\nImplementation details. In the training stage, we augment input images by random crop, random flip, and random resize with the shorter sides in [480,800] and the longer sides less or equal to 1333 following DETR (Carion et al., 2020) and PETR . To accelerate the early explicit human detection, we use a human query denoising training strategy from DN-DETR (Li et al., 2022a). We use the AdamW (Kingma & Ba, 2014;Loshchilov & Hutter, 2017) optimizer with weight decay of 1 \u00d7 10 \u22124 and train our model on Nvidia A100 GPUs with batch size 16 for 60 epochs and 80 epochs on COCO and CrowdPose, respectively. The initial learning rate is 1 \u00d7 10 \u22124 and is decayed at the 55th epoch and 75th epoch by a factor of 0.1 on COCO and CrowdPose, respectively. The channel dimension D is set to 256. The number of layers in Human Detection Decoder and Human-to-Keypoint Detection Decoder are 2 and 4 respectively. In the test stage, the input images are resized to have their shorter sides being 800 and their longer sides less or equal to 1333.\n\nLoss Function. the overall loss function of ED-Pose can be formulated as:\nL = L h + L c + L k (1) L h = \u00b5 |H \u2212 H * | + \u03b2(1 \u2212 GIOU) (2) L c = \u2212\u03bb\u03b1(1 \u2212 p t ) \u03b3 log(p t ), where p t = p if y = 1, p t = 1 \u2212 p if y = 1 (3) L k = \u03c9 |P \u2212 P * | + \u03b8 K i exp(\u2212 |P i \u2212 P * i | /2s 2 k 2 i )\u03b4(v i > 0) K i \u03b4(v i > 0)(4)\nwhere L h is for human box regression that contains L1 loss and GIOU (Rezatofighi et al., 2019) loss, L c is for human classification that is focal loss (Lin et al., 2017) with \u03b1 = 0.25, \u03b3 = 2, and L k is for keypoint regression that includes L1 loss and the constrained L1 loss-OKS loss . |H \u2212 H * | is the L1 distance between the predicted human boxes and the ground-truth ones. y \u2208 \u00b11 specifies the ground-truth class, and p \u2208 [0, 1] is the ED-Pose's estimated probability for the class with label y = 1. |P \u2212 P * | is the L1 distance between predicted keypoints inside a human and the ground-truth ones. |P i \u2212 P * i | is the L1 distance between the i-th predicted keypoint and groundtruth one, v i is the visibility flag of the ground truth, s is the object scale, and k i is a per-keypoint constant that controls falloff.\n\nThe loss coefficients \u00b5, \u03b2, \u03bb, \u03c9, \u03b8 are 5, 2, 2, 10, 4.\n\nThe Detailed Interactive Learning between Human Detection and Keypoint Detection. The relations from internal human-keypoint, internal keypoint-keypoint, and external human-human are learned from the self-attention mechanism. As shown in Figure. 3-(c), Human-to-Keypoint Interac- on medium-scale human detection. Different from previous observations of severe optimization conflicts between human detection and keypoint detection, ED-Pose unifies the contextual learning between human-level and keypoint-level information and gains benefits from each other.  \n\n\nD QUALITATIVE RESULTS\n\nQualitative ablations for explicit human detection: In the main article, we verify the effectiveness of explicit human detection for convergence speed and precision of the keypoint detection. Here, we present the corresponding qualitative results on the CrowdPose test set as shown in Fig. 6. In general, explicit human detection has several advantages: 1). It can provide global human-level information, enabling the model to be aware of flipping easily like the case in the first row. 2). It can give a clear human identity to the crowded scene to avoid the keypoint mismatching across persons, such as the case in the second row. 3) It is friendly for small human pose estimation as it can provide a precise prior of human box position, e.g., the case in the third row.\n\nQualitative comparisons between PETR and ED-Pose: We present the visualization comparisons of ED-Pose and PETR  on the COCO dataset, as shown in Fig 7. From the first row, ED-Pose can pay attention to the flipping issue due to the introduction of explicit detection and the realization of human-keypoint feature propagation. The second row further reflects that explicit detection in ED-Pose can make the model aware of the human position when conducting pose estimation, rather than the chaotic and unconscious query on a full image like PETR. All possible detected keypoints from PETR have low scores, making it hard to distinguish the right and wrong results. The third row shows that the enhancement of the global-local interactivity could relieve the estimation problem of the hard pose with heavy occlusions.\n\n\nE MORE DISCUSSION\n\nDiscussion for Additional Related Works.\n\n(1) N. xue (Xue et al., 2022) focuses on improving the performance of the heatmap-based bottom-up method by learning the local-global contextual adaptation without any explicit box detection.\n\n(2) Different from the one-stage method, D. Wang (Wang & Zhang, 2022) decouples persons into multiple instance-aware feature maps to obtain the keypoints without any explicit box detection, and they also use heatmap-based supervision.\n\nDifferent from their works, ED-Pose aims to utilize two explicit box detection processes with a unified box representation and light L1 losses. It abandons all post-processings in a fully end-toend manner. In terms of performance, ED-Pose surpasses them by a margin and even outperforms heatmap-based Top-down methods under the same backbone.\n\nThe More Detailed Advantage of ED-Pose. From the results, ED-Pose is efficient and effective even with the same backbone compared with previous works, indicating it really helps with pose estimation. From the method with two box detection processes, we have admitted it is inspired by recent DETR-based methods. However, the motivation and usage have obvious differences due to the task differences between object detection and multi-person pose estimation. Multi-person pose estimation focuses on either human-level (global) or keypoint-level (local) information. ED-Pose re-considers this task as two explicit box detection processes with a unified representation and regression supervision. Moreover, we also compare with other DETR-based pose estimators, and ED-Pose shows its superiorities in both efficiency and performance. The first human box detection can extract global features and provide a good initialization for the latter keypoint detection, making the training process converge fast. The second keypoint box detection can bring in local contextual information near keypoints. However, previous DETR-based pose estimation methods only directly regressed the keypoints' 2D coordinates, we argue that the keypoint representation as a point lacks local contextual information. They directly regress local keypoints without global feature extraction, making the convergence slow and inaccurate estimation. In Figure. 6 and Figure. 7, we demonstrate the qualitative comparisons of whether or not we needed to use explicit detection and PETR with ours. Lastly, we conduct comprehensive experiments on how explicit detection works in Table. 4  and Table. 5. Figure 6: Visualization results of the ablation study for explicit human detection on the CrowdPose dataset. Notably, we ignore the keypoint box to compare the keypoint localization of the two settings clearly. The white circle is the failure part with a description of the type of failure. \n\nFigure 1 :\n1Illustration of (a) the perception of the pose estimation task that usually captures global and local contexts concurrently; (b) a taxonomy of existing estimators. ED-Pose (Ours) is a novel one-stage method of learning both global and local relations in an end-to-end manner.\n\nFigure 3 :\n3The detailed illustration of (a) Human Detection Decoder, (b) Human-to-Keypoint Detection Decoder and (c) the detailed Interactive Learning in Human-to-Keypoint Detection Decoder.\n\nFigure 4 :\n4Qualitative results of ED-Pose on COCO (the first row) and CrowdPose (the second row).\n\nFigure 7 :\n7Visualization results of PETR and ED-Pose on COCO dataset. Both methods are based on ResNet-50 as the backbone. Notably, there are several candidates in the PETR's result at the second line as their classification scores are close and relatively low.\n\nTable 1 :\n1Comparisons of existing estimators from the losses and representations of the Human and Keypoints. Our proposed ED-Pose unifies both Human and Keypoint detection under the consistent L1 regression loss and the box representation, making the end-to-end training simple yet effective.Methodology \nHuman Loss \nKeypoint Loss \nHuman Representation Keypoint Representation \n\nTwo-Stage Methods \nTop-Down \nRegression \nHeatmap \n(x, y, h, w) \n(x, y) \nBottom-Up \n-\nHeatmap \n-\n(x, y) \n\nOne-Stage Methods \nPrevious \n-\nRegression + Heatmap \n-\n(x, y) \nOurs \nRegression \nRegression \n(x, y, h, w) \n(x, y, h, w) \n\n\n\nTable 2 :\n2Comparisons with state-of-the-art methods on CrowdPose test dataset. TD, BU, OS mean top-down, bottom-up, and one-stage methods, respectively. We use \"HM.\" and \"R.\" for heatmap-based losses and regression losses. \u2020 denotes the flipping test. The model with * is pretrained on Objects365(Shao et al., 2019a) with 5 feature scales. The underlined highlights the compared results. The best results are highlighted in bold. AP 75 AP E AP M AP HMethod \nLoss \nAP \nAP 50 TD \n\nSim.Base. (ResNet-50) \nHM. \n60.8 \n81.4 \n65.7 \n71.4 \n61.2 \n51.2 \nHRNet (HRNet-w48)  \u2020 \nHM. \n71.3 \n91.1 \n77.5 \n80.5 \n71.4 \n62.5 \nTransPose-H \nHM. \n71.8 \n91.5 \n77.8 \n79.5 \n72.9 \n62.2 \nHRFormer-B \nHM. \n72.4 \n91.5 \n77.9 \n80.0 \n73.5 \n62.4 \n\nBU \nHrHRNet-w32 \u2020 \nHM. \n65.9 \n86.4 \n70.6 \n73.3 \n66.5 \n57.9 \nDEKR (HrHRNet-w32) \u2020 \nHM. \n65.7 \n85.7 \n70.4 \n73.0 \n66.4 \n57.5 \nSWAHR (HrHRNet-w32) \u2020 HM. \n71.6 \n88.5 \n77.6 \n78.9 \n72.4 \n63.0 \n\nOS \n\nPETR (Swin-L) \nR.+HM. \n71.6 \n90.4 \n78.3 \n77.3 \n72.0 \n65.8 \n\nED-Pose (ResNet-50) \nR. \n69.9\u2191 9.1 \n88.6 \n75.8 \n77.7 \n70.6 \n60.9 \nED-Pose (Swin-L) \nR. \n73.1\u2191 1.5 \n90.5 \n79.8 \n80.5 \n73.8 \n63.8 \nED-Pose (Swin-L*) \nR. \n76.6\u2191 5.0 \n92.4 \n83.3 \n83.0 \n77.3 \n68.3 \n\n\n\nTable 3 :\n3Comparisons with state-of-the-art methods on COCO val2017 dataset. \u2020 denotes the flipping test. \u2021 removes the prediction uncertainty estimation in Poseur as a fair regression comparison. The underlined highlights the compared results. The inference time of all methods is tested on an A100, except that the detector of top-down methods is tested by the MMdetection (i.e., 45ms).Method \nBackbone \nLoss \nAP \nAP 50 AP 75 AP M AP L Time [ms] \n\nTwo-stage \nTD \n\nSim.Base.  \u2020 \nResNet-50 \nHM. \n70.4 \n88.6 \n78.3 \n67.1 \n77.2 \n45+86 \nHRNet  \u2020 \nHRNet-w32 HM. \n74.4 \n90.5 \n81.9 \n70.8 \n81.0 \n45+112 \nPRTR  \u2020 \nResNet-50 \nR. \n68.2 \n88.2 \n75.2 \n63.2 \n76.2 \n45+85 \nPoseur \nResNet-50 \nRLE  \u2021 \n70.0 \n-\n-\n-\n-\n45+82 \nPoseur \nResNet-50 \nRLE \n74.2 \n89.8 \n81.3 \n71.1 \n80.1 \n45+82 \n\nBU \nHrHRNet  \u2020 \nHRNet-w32 HM. \n67.1 \n86.2 \n73.0 \n61.5 \n76.1 \n322 \nDEKR  \u2020 \nHRNet-w32 HM. \n68.0 \n86.7 \n74.5 \n62.1 \n77.7 \n-\nSWAHR  \u2020 \nHRNet-w32 HM. \n68.9 \n87.8 \n74.9 \n63.0 \n77.4 \n-\n\nE2E TD \n\nMask R-CNN ResNet-101 HM. \n66.0 \n86.9 \n71.5 \n-\n-\n-\nPRTR  \u2020 \nResNet-101 HM. \n64.8 \n85.1 \n70.2 \n60.4 \n73.8 \n-\nPoseur  \u2020 \nResNet-101 RLE \n68.6 \n87.5 \n74.8 \n-\n-\n-\nPoseur  \u2020 \nHRNet-w48 RLE \n70.1 \n88.0 \n76.5 \n-\n-\n-\n\nOS \n\nInsPose \nResNet-50 \nR.+HM. \n65.2 \n87.2 \n71.3 \n60.6 \n72.2 \n78 \nPETR \nResNet-50 \nR.+HM. \n68.8 \n87.5 \n76.3 \n62.7 \n77.7 \n105 \nPETR \nSwin-L \nR.+HM. \n73.1 \n90.7 \n80.9 \n67.2 \n81.7 \n206 \n\nED-Pose \nResNet-50 \nR. \n71.6\u2191 2.8 \n89.6 \n78.1 \n65.9 \n79.8 \n51\u2193 51.4% \nED-Pose \nSwin-L \nR. \n74.3\u2191 1.2 \n91.5 \n81.6 \n68.6 \n82.6 \n88\u2193 57.3% \nED-Pose \nSwin-L* \nR. \n75.8\u2191 2.7 \n92.3 \n82.9 \n70.4 \n83.5 142\u2193 31.1% \n\n\n\nTable 4 :\n4Impact on explicit human detection (Human Det.) for convergence speed and precision on the COCO dataset (the left) and CrowdPose dataset (the right).Human Det. 12e \n24e \n36e \n48e \n60e \n\n41.1 56.6 61.0 64.7 67.1 \n60.5 67.5 69.7 70.8 71.6 \n\nHuman Det. 12e \n24e \n36e \n48e \n60e \n80e \n\n13.1 20.5 31.1 42.6 51.3 60.0 \n37.1 54.8 62.4 66.4 68.2 69.9 \n\n\n\nTable 5 :\n5Impact on the width and \nweight initialization of the keypoint \nbox. \n\n(w, h) None Min. Max. FFN. Ours \n\nAP \n58.3 70.8 70.8 \n71.2 71.6 \nAP M \n57.5 65.1 64.9 \n65.7 65.9 \nAP L \n60.0 79.2 79.3 \n79.2 79.8 \n\n\n\nTable 6 :\n6Impact on interactive learn-\ning between human and keypoint de-\ntection. \n\nStrategy Full w/o H-K w/o H-H Ours \n\nAP \n70.4 \n71.2 \n71.3 \n71.6 \nAP M \n64.6 \n65.6 \n66.0 \n65.9 \nAP L \n78.8 \n79.3 \n79.0 \n79.8 \n\n\n\nTable 7 :\n7Impact on \nthe number of M se-\nlected queries. \n\nM \n50 \n100 200 \n\nAP \n70.9 71.6 71.6 \nAP M 65.2 65.9 66.0 \nAP L \n79.2 79.8 79.6 \n\n\n\nTable 8 :\n8Comparisons with state-of-the-art one-stage methods on COCO test-dev dataset. underlined highlights the compared results. Emphasized bold number in inference time is tested by our A100 machine. AP 75 AP M AP L Time[ms]    Method \nBackbone \nLoss \nAP \nAP 50 Non E-2-E \n\nDirectPose ResNet-50 \nR. \n62.2 \n86.4 \n68.2 \n56.7 \n69.8 \n74 \nDirectPose ResNet-101 R. \n63.3 \n86.7 \n69.4 \n57.8 \n71.2 \n-\nFCPose \nResNet-50 \nR+HM. \n64.3 \n87.3 \n71.0 \n61.6 \n70.5 \n68 \nFCPose \nResNet-101 R+HM. \n65.6 \n87.9 \n72.6 \n62.1 \n72.3 \n93 \nInsPose \nResNet-50 \nR+HM. \n65.4 \n88.9 \n71.7 \n60.2 \n72.7 \n78 \nInsPose \nResNet-101 R+HM. \n66.3 \n89.2 \n73.0 \n61.2 \n73.9 \n100 \nCenterNet \nHourglass \nR. \n63.0 \n86.8 \n69.6 \n58.9 \n70.4 \n160 \n\nFully E-2-E \n\n\nTable 9 :\n9The Average Precision comparisons of the commonly used human detection methods (e.g., Faster-RCNN and YOLOv3) with ours on COCO (the left) and CrowdPose (the right). Notably, We only compare AP M (medium object) and AP L (large object) here as small objects in the two datasets are not labeled with keypoints. ED-Pose can provide better human detection to serve future work, especially for top-down and one-stage methodsMethodsAP M AP LFaster-RCNN \n63.3 \n74.5 \nED-Pose (ResNet-50) \n65.9 \n77.6 \nED-Pose (Swin-L) \n69.7 \n80.2 \nED-Pose (Swin-L*) \n71.0 \n81.2 \n\nMethods \nAP M AP L \n\nYOLOv3 \n37.4 \n46.5 \nED-Pose (ResNet-50) \n58.0 \n75.3 \nED-Pose (Swin-L) \n61.8 \n78.4 \nED-Pose (Swin-L*) \n66.8 \n82.2 \n\n\n\nTable 10 :\n10The performance comparisons of results from different decoders (i.e., human detection decoder and human-keypoint detection decoder) on both COCO and CrowdPose datasets with the Swin-L as the backbone. ED-Pose CrowdPose Human Det.+Human-Keypoint Det.Method Dataset \nDecoder \nAP M (Human) AP L (Human) AP (Keypoint) \n\nED-Pose COCO \nHuman Det. \n70.4 \n79.6 \n70.4 \nED-Pose COCO \nHuman Det.+Human-Keypoint Det. \n69.8 \n80.1 \n74.3 \n\nED-Pose CrowdPose Human Det. \n61.7 \n76.9 \n68.0 \n61.6 \n78.4 \n73.1 \n\n\nThe inference time without Emphasized bold number is from PETR's paper because there is no public code for replication.\nPublished as a conference paper at ICLR 2023 tive Attention can be computed as follow:where PE denotes positional encoding, f (\u00b7) is linear projection, Q c H,K \u2208 R (M +M * K)\u00d7D and Q p H,K \u2208 R (M +M * K)\u00d74 are human-keypoint content queries and human-keypoint position queries for M human candidates and the corresponding K keypoints. D is 256 by default. In practice, to implement the three interactive learning processes in a simple way, we use an attention mask M \u2208 R (M +M * K)\u00d7(M +M * K) to block the interactiveness between external keypoint-keypoint. Thus, M can be formulated as:where M(i, j) is the location of the attention mask. We use the M to keep internal human-keypoint attention (True), internal keypoint-keypoint attention (True), and external human-human attention (True) while avoiding external keypoint-keypoint attention (False).Inference time. 1) Comparison Methods: All of the methods inTable.3 are tested on our A100 machine for a fair comparison and the detector of top-down methods is tested by MMdetection. In tabel. 8, the inference time with emphasized bold number is tested by our A100 machine, otherwise, it is from PETR's paper using a V100 GPU due to no available source code. 2) Testing Rules: We omit the time for data pre-processing and only measure the time for model forwarding and data postprocessing (i.e., grouping operation in bottom-up methods). For bottom-up methods and one-stage methods, we set the batch size to 1. For top-down methods, we set the batch size to 5 for simulating the multi-person situation.\nRealtime multi-person 2d pose estimation using part affinity fields. Zhe Cao, Tomas Simon, Shih-En Wei, Yaser Sheikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionZhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using part affinity fields. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7291-7299, 2017.\n\nEnd-to-end object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, European conference on computer vision. SpringerNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pp. 213-229. Springer, 2020.\n\nHigherhrnet: Scale-aware representation learning for bottom-up human pose estimation. Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi, S Thomas, Lei Huang, Zhang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionBowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi, Thomas S Huang, and Lei Zhang. Higherhr- net: Scale-aware representation learning for bottom-up human pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5386-5395, 2020.\n\nBottom-up human pose estimation via disentangled keypoint regression. Zigang Geng, Ke Sun, Bin Xiao, Zhaoxiang Zhang, Jingdong Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionZigang Geng, Ke Sun, Bin Xiao, Zhaoxiang Zhang, and Jingdong Wang. Bottom-up human pose estimation via disentangled keypoint regression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14676-14686, 2021.\n\nMask r-cnn. Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 2961-2969, 2017.\n\nDensebox: Unifying landmark localization with end to end object detection. arXiv: Computer Vision and Pattern Recognition. Lichao Huang, Yi Yang, Yafeng Deng, Yinan Yu, Lichao Huang, Yi Yang, Yafeng Deng, and Yinan Yu. Densebox: Unifying landmark localization with end to end object detection. arXiv: Computer Vision and Pattern Recognition, 2015.\n\nDynamic filter networks. neural information processing systems. Xu Jia, Bert De Brabandere, Tinne Tuytelaars, Luc Van Gool, Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc Van Gool. Dynamic filter networks. neural information processing systems, 2016.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nDn-detr: Accelerate detr training by introducing query denoising. Feng Li, Hao Zhang, Shilong Liu, Jian Guo, M Lionel, Lei Ni, Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionFeng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-detr: Accelerate detr training by introducing query denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13619-13627, 2022a.\n\nMask dino: Towards a unified transformer-based framework for object detection and segmentation. Feng Li, Hao Zhang, Shilong Liu, Lei Zhang, Lionel M Ni, Heung-Yeung Shum, arXiv:2206.02777arXiv preprintFeng Li, Hao Zhang, Shilong Liu, Lei Zhang, Lionel M Ni, Heung-Yeung Shum, et al. Mask dino: Towards a unified transformer-based framework for object detection and segmentation. arXiv preprint arXiv:2206.02777, 2022b.\n\nCrowdpose: Efficient crowded scenes pose estimation and a new benchmark. Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu Fang, Cewu Lu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionJiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu Fang, and Cewu Lu. Crowdpose: Effi- cient crowded scenes pose estimation and a new benchmark. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10863-10872, 2019.\n\nHuman pose regression with residual log-likelihood estimation. Jiefeng Li, Siyuan Bian, Ailing Zeng, Can Wang, Bo Pang, Wentao Liu, Cewu Lu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionJiefeng Li, Siyuan Bian, Ailing Zeng, Can Wang, Bo Pang, Wentao Liu, and Cewu Lu. Human pose regression with residual log-likelihood estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 11025-11034, 2021a.\n\nPose recognition with cascade transformers. Ke Li, Shijie Wang, Xiang Zhang, Yifan Xu, Weijian Xu, Zhuowen Tu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionKe Li, Shijie Wang, Xiang Zhang, Yifan Xu, Weijian Xu, and Zhuowen Tu. Pose recognition with cascade transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1944-1953, 2021b.\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European conference on computer vision. SpringerTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740-755. Springer, 2014.\n\nKaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pp. 2980-2988, 2017.\n\nDab-detr: Dynamic anchor boxes are better queries for detr. Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, Lei Zhang, arXiv:2201.12329arXiv preprintShilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329, 2022.\n\nSwin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2021Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.\n\n. Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. arXiv preprintIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\nRethinking the heatmap regression for bottom-up human pose estimation. Zhengxiong Luo, Zhicheng Wang, Yan Huang, Liang Wang, Tieniu Tan, Erjin Zhou, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionZhengxiong Luo, Zhicheng Wang, Yan Huang, Liang Wang, Tieniu Tan, and Erjin Zhou. Rethinking the heatmap regression for bottom-up human pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13264-13273, 2021.\n\nWeian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang, Zhibin Wang, Tfpose, arXiv:2103.15320Direct human pose estimation with transformers. arXiv preprintWeian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang, and Zhibin Wang. Tfpose: Direct human pose estimation with transformers. arXiv preprint arXiv:2103.15320, 2021a.\n\nFcpose: Fully convolutional multi-person pose estimation with dynamic instance-aware convolutions. arXiv: Computer Vision and Pattern Recognition. Weian Mao, Zhi Tian, Xinlong Wang, Chunhua Shen, Weian Mao, Zhi Tian, Xinlong Wang, and Chunhua Shen. Fcpose: Fully convolutional multi-person pose estimation with dynamic instance-aware convolutions. arXiv: Computer Vision and Pattern Recognition, 2021b.\n\nWeian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang, arXiv:2201.07412Zhibin Wang, and Anton van den Hengel. Poseur: Direct human pose regression with transformers. arXiv preprintWeian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang, Zhibin Wang, and Anton van den Hengel. Poseur: Direct human pose regression with transformers. arXiv preprint arXiv:2201.07412, 2022.\n\nAssociative embedding: End-to-end learning for joint detection and grouping. Advances in neural information processing systems. Alejandro Newell, Zhiao Huang, Jia Deng, 30Alejandro Newell, Zhiao Huang, and Jia Deng. Associative embedding: End-to-end learning for joint detection and grouping. Advances in neural information processing systems, 30, 2017.\n\nJoseph Redmon, Ali Farhadi, arXiv:1804.02767Yolov3: An incremental improvement. arXiv preprintJoseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, IEEE Transactions on Pattern Analysis and Machine Intelligence. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015.\n\nGeneralized intersection over union: A metric and a loss for bounding box regression. Hamid Rezatofighi, Nathan Tsoi, Junyoung Gwak, Amir Sadeghian, Ian Reid, Silvio Savarese, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionHamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized intersection over union: A metric and a loss for bounding box regression. In Pro- ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 658-666, 2019.\n\nObjects365: A large-scale, high-quality dataset for object detection. Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, Jian Sun, international conference on computer visionShuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. international confer- ence on computer vision, 2019a.\n\nObjects365: A large-scale, high-quality dataset for object detection. Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, Jian Sun, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer visionShuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 8430-8439, 2019b.\n\nInspose: Instanceaware networks for single-stage multi-person pose estimation. acm multimedia. Dahu Shi, Xing Wei, Xiaodong Yu, Wenming Tan, Ye Ren, Shiliang Pu, 2021Dahu Shi, Xing Wei, Xiaodong Yu, Wenming Tan, Ye Ren, and Shiliang Pu. Inspose: Instance- aware networks for single-stage multi-person pose estimation. acm multimedia, 2021.\n\nEnd-to-end multi-person pose estimation with transformers. Dahu Shi, Xing Wei, Liangqi Li, Ye Ren, Wenming Tan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionDahu Shi, Xing Wei, Liangqi Li, Ye Ren, and Wenming Tan. End-to-end multi-person pose esti- mation with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11069-11078, 2022.\n\nEnd-to-end trainable multi-instance pose estimation with transformers. Lucas Stoffl, Maxime Vidal, Alexander Mathis, arXiv:2103.12115arXiv preprintLucas Stoffl, Maxime Vidal, and Alexander Mathis. End-to-end trainable multi-instance pose esti- mation with transformers. arXiv preprint arXiv:2103.12115, 2021.\n\nDeep high-resolution representation learning for human pose estimation. Ke Sun, Bin Xiao, Dong Liu, Jingdong Wang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionKe Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5693-5703, 2019.\n\nDirectpose: Direct end-to-end multi-person pose estimation. Zhi Tian, Hao Chen, Chunhua Shen, arXiv:1911.07451arXiv preprintZhi Tian, Hao Chen, and Chunhua Shen. Directpose: Direct end-to-end multi-person pose estima- tion. arXiv preprint arXiv:1911.07451, 2019a.\n\nZhi Tian, Chunhua Shen, Hao Chen, Tong He, Fcos: Fully convolutional one-stage object detection. international conference on computer vision. Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. international conference on computer vision, 2019b.\n\nContextual instance decoupling for robust multi-person pose estimation. Dongkai Wang, Shiliang Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionDongkai Wang and Shiliang Zhang. Contextual instance decoupling for robust multi-person pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition, pp. 11060-11068, 2022.\n\nSimple baselines for human pose estimation and tracking. Bin Xiao, Haiping Wu, Yichen Wei, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines for human pose estimation and tracking. In Proceedings of the European conference on computer vision (ECCV), pp. 466-481, 2018.\n\nLearning local-global contextual adaptation for multi-person pose estimation. Nan Xue, Tianfu Wu, Gui-Song Xia, Liangpei Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionNan Xue, Tianfu Wu, Gui-Song Xia, and Liangpei Zhang. Learning local-global contextual adapta- tion for multi-person pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13065-13074, 2022.\n\nHrformer: High-resolution vision transformer for dense predict. Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang, Xilin Chen, Jingdong Wang, Advances in Neural Information Processing Systems. 34Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang, Xilin Chen, and Jingdong Wang. Hrformer: High-resolution vision transformer for dense predict. Advances in Neural Information Processing Systems, 34:7281-7293, 2021.\n\nHao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, M Lionel, Heung-Yeung Ni, Shum, Dino, arXiv:2203.03605Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprintHao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022.\n\n. Xingyi Zhou, Dequan Wang, Philipp Kr\u00e4henb\u00fchl, arXiv:1904.07850Objects as points. arXiv preprintXingyi Zhou, Dequan Wang, and Philipp Kr\u00e4henb\u00fchl. Objects as points. arXiv preprint arXiv:1904.07850, 2019.\n\nDeformable detr: Deformable transformers for end-to-end object detection. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai, arXiv:2010.04159arXiv preprintXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\n\nED-Pose has a unique merit compared to other existing two-stage methods and one-stage methods, which can pre-train human detection to improve the detection performance to assist the subsequent pose estimation in a fully end-to-end manner. Objects365 (Shao et al., 2019b) is a large-scale detection dataset with over 1.7M annotated images for training and 80, 000 annotated images for validation. A Unique Merit of ED-Pose for Human Detection Pre-training. Thanks to the explicit introduction of human detection. We pre-train the human detection of ED-Pose (i.e., backbone, encoder, human detection decoder) on Objects365 for 26 epochs using 64 Nvidia A100 GPUsA Unique Merit of ED-Pose for Human Detection Pre-training. Thanks to the explicit introduc- tion of human detection, ED-Pose has a unique merit compared to other existing two-stage methods and one-stage methods, which can pre-train human detection to improve the detection performance to assist the subsequent pose estimation in a fully end-to-end manner. Objects365 (Shao et al., 2019b) is a large-scale detection dataset with over 1.7M annotated images for training and 80, 000 annotated images for validation. We pre-train the human detection of ED-Pose (i.e., backbone, encoder, human detection decoder) on Objects365 for 26 epochs using 64 Nvidia A100 GPUs.\n\n. B Comparison, Coco Test-Dev, B COMPARISON OF COCO TEST-DEV.\n\nTo be specific, our proposed ED-Pose has 2.2 AP gains compared with the fully end-to-end method PETR. Tian, From Table. 8, our method without dense heatmap loss can significantly outperform all existing onestage methods, such as DirecetPose. with a ResNet-50 backbone or Swin-L backbone with about 50% inference time reductionFrom Table. 8, our method without dense heatmap loss can significantly outperform all existing one- stage methods, such as DirecetPose (Tian et al., 2019a), FCPose (Mao et al., 2021b), Inspose (Shi et al., 2021), CenterNet (Zhou et al., 2019), and PETR (Shi et al., 2022). To be specific, our proposed ED-Pose has 2.2 AP gains compared with the fully end-to-end method PETR (Shi et al., 2022) with a ResNet-50 backbone or Swin-L backbone with about 50% inference time reduction.\n\nFor the CrowdPose dataset, most top-down methods directly use YOLOv3 (Redmon & Farhadi, 2018) pre-trained on COCO trainval to generate the detected human bounding boxes, making their performance even worse than recent bottom-up methods. ED-Pose can provide human detection results with nearly doubling performance on AP M and AP L to serve future work. Moreover, in Table 10, we study the impact of different decoders (i.e., human detection decoder and human-keypoint detection decoder) for human detection and keypoint detection. First, we discover that the keypoint initialization provided by the human detection decoder has already achieved good results on COCO and CrowdPose datasets. C Results Of ; Ren, HUMAN DETECTION Due to the explicit person detection in our methods, we also report the human detection performance on the COCO val2017 set and CrowdPose test set in Figure. 9. For the COCO dataset, ED-Pose with Swin-L* backbone gains 7.7 AP M and 6.7 AP L improvement compared with Faster-RCNN. which proves the effectiveness of explicit human detection. Second, the keypoint detection introduced via the human-keypoint detection decoder is also helpful to improve large-scale human detection and obtain similar performance PETR ResNet-50C RESULTS OF HUMAN DETECTION Due to the explicit person detection in our methods, we also report the human detection per- formance on the COCO val2017 set and CrowdPose test set in Figure. 9. For the COCO dataset, ED-Pose with Swin-L* backbone gains 7.7 AP M and 6.7 AP L improvement compared with Faster-RCNN (Ren et al., 2015). For the CrowdPose dataset, most top-down methods directly use YOLOv3 (Redmon & Farhadi, 2018) pre-trained on COCO trainval to generate the detected human bounding boxes, making their performance even worse than recent bottom-up methods. ED- Pose can provide human detection results with nearly doubling performance on AP M and AP L to serve future work. Moreover, in Table 10, we study the impact of different decoders (i.e., human detection decoder and human-keypoint detection decoder) for human detection and keypoint detec- tion. First, we discover that the keypoint initialization provided by the human detection decoder has already achieved good results on COCO and CrowdPose datasets, which proves the effectiveness of explicit human detection. Second, the keypoint detection introduced via the human-keypoint detec- tion decoder is also helpful to improve large-scale human detection and obtain similar performance PETR ResNet-50\n", "annotations": {"author": "[{\"end\":275,\"start\":119},{\"end\":358,\"start\":276},{\"end\":361,\"start\":359},{\"end\":444,\"start\":362},{\"end\":472,\"start\":445},{\"end\":643,\"start\":473},{\"end\":646,\"start\":644},{\"end\":725,\"start\":647}]", "publisher": null, "author_last_name": "[{\"end\":127,\"start\":123},{\"end\":287,\"start\":283},{\"end\":373,\"start\":370},{\"end\":452,\"start\":450},{\"end\":485,\"start\":480},{\"end\":656,\"start\":651}]", "author_first_name": "[{\"end\":122,\"start\":119},{\"end\":282,\"start\":276},{\"end\":360,\"start\":359},{\"end\":369,\"start\":362},{\"end\":449,\"start\":445},{\"end\":479,\"start\":473},{\"end\":645,\"start\":644},{\"end\":650,\"start\":647}]", "author_affiliation": "[{\"end\":188,\"start\":143},{\"end\":274,\"start\":190},{\"end\":357,\"start\":312},{\"end\":443,\"start\":398},{\"end\":556,\"start\":511},{\"end\":642,\"start\":558},{\"end\":724,\"start\":679}]", "title": "[{\"end\":116,\"start\":1},{\"end\":841,\"start\":726}]", "venue": null, "abstract": "[{\"end\":2381,\"start\":843}]", "bib_ref": "[{\"end\":2660,\"start\":2655},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2819,\"start\":2800},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3252,\"start\":3233},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3269,\"start\":3252},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3286,\"start\":3269},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3395,\"start\":3377},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3415,\"start\":3395},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3434,\"start\":3415},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4211,\"start\":4190},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4455,\"start\":4437},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4473,\"start\":4455},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4478,\"start\":4473},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4498,\"start\":4478},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5270,\"start\":5250},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7565,\"start\":7546},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7979,\"start\":7959},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7998,\"start\":7979},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8391,\"start\":8373},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8489,\"start\":8471},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8932,\"start\":8911},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9005,\"start\":8984},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9423,\"start\":9405},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9531,\"start\":9513},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9635,\"start\":9617},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9666,\"start\":9647},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9955,\"start\":9937},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10120,\"start\":10099},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10985,\"start\":10966},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11002,\"start\":10985},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11122,\"start\":11105},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11693,\"start\":11675},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11713,\"start\":11693},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11732,\"start\":11713},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13048,\"start\":13031},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":14744,\"start\":14726},{\"end\":16041,\"start\":16033},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16305,\"start\":16284},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":18549,\"start\":18532},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22738,\"start\":22719},{\"end\":22862,\"start\":22844},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":23895,\"start\":23877},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24186,\"start\":24166},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24212,\"start\":24193},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24242,\"start\":24224},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24508,\"start\":24489},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24548,\"start\":24530},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26073,\"start\":26053},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":26125,\"start\":26106},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26395,\"start\":26377},{\"end\":30575,\"start\":30550},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":30985,\"start\":30967},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":31261,\"start\":31244},{\"end\":31722,\"start\":31717},{\"end\":31726,\"start\":31722},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31806,\"start\":31785},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":31948,\"start\":31930},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":31986,\"start\":31967},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32012,\"start\":31986},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":33009,\"start\":32983},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33085,\"start\":33067},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":36066,\"start\":36048},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":36299,\"start\":36279},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":40542,\"start\":40522}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":39057,\"start\":38769},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39250,\"start\":39058},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39350,\"start\":39251},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39614,\"start\":39351},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":40223,\"start\":39615},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":41386,\"start\":40224},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":42946,\"start\":41387},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":43303,\"start\":42947},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":43519,\"start\":43304},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":43733,\"start\":43520},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":43876,\"start\":43734},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":44593,\"start\":43877},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":45298,\"start\":44594},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":45805,\"start\":45299}]", "paragraph": "[{\"end\":3990,\"start\":2407},{\"end\":5304,\"start\":3992},{\"end\":6557,\"start\":5306},{\"end\":7849,\"start\":6559},{\"end\":8933,\"start\":7866},{\"end\":10397,\"start\":8935},{\"end\":10672,\"start\":10399},{\"end\":12789,\"start\":10726},{\"end\":14133,\"start\":12791},{\"end\":14379,\"start\":14164},{\"end\":16257,\"start\":14381},{\"end\":16705,\"start\":16259},{\"end\":19161,\"start\":16733},{\"end\":19809,\"start\":19201},{\"end\":22258,\"start\":19811},{\"end\":22509,\"start\":22274},{\"end\":23053,\"start\":22534},{\"end\":23678,\"start\":23073},{\"end\":24023,\"start\":23710},{\"end\":24680,\"start\":24025},{\"end\":25106,\"start\":24682},{\"end\":26177,\"start\":25108},{\"end\":26754,\"start\":26206},{\"end\":28569,\"start\":26773},{\"end\":29439,\"start\":28571},{\"end\":29731,\"start\":29441},{\"end\":30354,\"start\":29746},{\"end\":30845,\"start\":30439},{\"end\":31571,\"start\":30868},{\"end\":32605,\"start\":31573},{\"end\":32680,\"start\":32607},{\"end\":33741,\"start\":32914},{\"end\":33798,\"start\":33743},{\"end\":34359,\"start\":33800},{\"end\":35157,\"start\":34385},{\"end\":35973,\"start\":35159},{\"end\":36035,\"start\":35995},{\"end\":36228,\"start\":36037},{\"end\":36464,\"start\":36230},{\"end\":36808,\"start\":36466},{\"end\":38768,\"start\":36810}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":32913,\"start\":32681}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":22651,\"start\":22644},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23186,\"start\":23167},{\"end\":23834,\"start\":23828},{\"end\":26333,\"start\":26327},{\"end\":27042,\"start\":27036},{\"end\":28229,\"start\":28223},{\"end\":29135,\"start\":29129},{\"end\":29615,\"start\":29609},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":38473,\"start\":38453}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2395,\"start\":2383},{\"end\":2405,\"start\":2398},{\"attributes\":{\"n\":\"2\"},\"end\":7864,\"start\":7852},{\"attributes\":{\"n\":\"3\"},\"end\":10724,\"start\":10675},{\"end\":14162,\"start\":14136},{\"attributes\":{\"n\":\"4.2\"},\"end\":16731,\"start\":16708},{\"attributes\":{\"n\":\"4.3\"},\"end\":19199,\"start\":19164},{\"attributes\":{\"n\":\"5\"},\"end\":22272,\"start\":22261},{\"attributes\":{\"n\":\"5.1\"},\"end\":22532,\"start\":22512},{\"attributes\":{\"n\":\"5.2\"},\"end\":23071,\"start\":23056},{\"attributes\":{\"n\":\"5.2.1\"},\"end\":23708,\"start\":23681},{\"attributes\":{\"n\":\"5.2.2\"},\"end\":26204,\"start\":26180},{\"attributes\":{\"n\":\"5.3\"},\"end\":26771,\"start\":26757},{\"attributes\":{\"n\":\"6\"},\"end\":29744,\"start\":29734},{\"end\":30437,\"start\":30357},{\"end\":30866,\"start\":30848},{\"end\":34383,\"start\":34362},{\"end\":35993,\"start\":35976},{\"end\":38780,\"start\":38770},{\"end\":39069,\"start\":39059},{\"end\":39262,\"start\":39252},{\"end\":39362,\"start\":39352},{\"end\":39625,\"start\":39616},{\"end\":40234,\"start\":40225},{\"end\":41397,\"start\":41388},{\"end\":42957,\"start\":42948},{\"end\":43314,\"start\":43305},{\"end\":43530,\"start\":43521},{\"end\":43744,\"start\":43735},{\"end\":43887,\"start\":43878},{\"end\":44604,\"start\":44595},{\"end\":45310,\"start\":45300}]", "table": "[{\"end\":40223,\"start\":39909},{\"end\":41386,\"start\":40676},{\"end\":42946,\"start\":41777},{\"end\":43303,\"start\":43108},{\"end\":43519,\"start\":43316},{\"end\":43733,\"start\":43532},{\"end\":43876,\"start\":43746},{\"end\":44593,\"start\":44111},{\"end\":45298,\"start\":45042},{\"end\":45805,\"start\":45562}]", "figure_caption": "[{\"end\":39057,\"start\":38782},{\"end\":39250,\"start\":39071},{\"end\":39350,\"start\":39264},{\"end\":39614,\"start\":39364},{\"end\":39909,\"start\":39627},{\"end\":40676,\"start\":40236},{\"end\":41777,\"start\":41399},{\"end\":43108,\"start\":42959},{\"end\":44111,\"start\":43889},{\"end\":45042,\"start\":44606},{\"end\":45562,\"start\":45313}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":2853,\"start\":2845},{\"end\":14206,\"start\":14198},{\"end\":14407,\"start\":14399},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16744,\"start\":16736},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18306,\"start\":18298},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19747,\"start\":19734},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21182,\"start\":21174},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22163,\"start\":22155},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25288,\"start\":25282},{\"end\":25761,\"start\":25753},{\"end\":26376,\"start\":26370},{\"end\":26470,\"start\":26464},{\"end\":34045,\"start\":34038},{\"end\":34676,\"start\":34670},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":35310,\"start\":35304},{\"end\":38240,\"start\":38231},{\"end\":38252,\"start\":38245},{\"end\":38485,\"start\":38477}]", "bib_author_first_name": "[{\"end\":47552,\"start\":47549},{\"end\":47563,\"start\":47558},{\"end\":47578,\"start\":47571},{\"end\":47589,\"start\":47584},{\"end\":48019,\"start\":48012},{\"end\":48037,\"start\":48028},{\"end\":48052,\"start\":48045},{\"end\":48070,\"start\":48063},{\"end\":48089,\"start\":48080},{\"end\":48106,\"start\":48100},{\"end\":48487,\"start\":48482},{\"end\":48498,\"start\":48495},{\"end\":48513,\"start\":48505},{\"end\":48527,\"start\":48520},{\"end\":48534,\"start\":48533},{\"end\":48546,\"start\":48543},{\"end\":49064,\"start\":49058},{\"end\":49073,\"start\":49071},{\"end\":49082,\"start\":49079},{\"end\":49098,\"start\":49089},{\"end\":49114,\"start\":49106},{\"end\":49536,\"start\":49529},{\"end\":49548,\"start\":49541},{\"end\":49564,\"start\":49559},{\"end\":49577,\"start\":49573},{\"end\":50007,\"start\":50001},{\"end\":50017,\"start\":50015},{\"end\":50030,\"start\":50024},{\"end\":50042,\"start\":50037},{\"end\":50293,\"start\":50291},{\"end\":50303,\"start\":50299},{\"end\":50324,\"start\":50319},{\"end\":50340,\"start\":50337},{\"end\":50531,\"start\":50530},{\"end\":50547,\"start\":50542},{\"end\":50774,\"start\":50770},{\"end\":50782,\"start\":50779},{\"end\":50797,\"start\":50790},{\"end\":50807,\"start\":50803},{\"end\":50814,\"start\":50813},{\"end\":50826,\"start\":50823},{\"end\":51335,\"start\":51331},{\"end\":51343,\"start\":51340},{\"end\":51358,\"start\":51351},{\"end\":51367,\"start\":51364},{\"end\":51381,\"start\":51375},{\"end\":51383,\"start\":51382},{\"end\":51399,\"start\":51388},{\"end\":51735,\"start\":51728},{\"end\":51743,\"start\":51740},{\"end\":51753,\"start\":51750},{\"end\":51765,\"start\":51759},{\"end\":51778,\"start\":51771},{\"end\":51789,\"start\":51785},{\"end\":52268,\"start\":52261},{\"end\":52279,\"start\":52273},{\"end\":52292,\"start\":52286},{\"end\":52302,\"start\":52299},{\"end\":52311,\"start\":52309},{\"end\":52324,\"start\":52318},{\"end\":52334,\"start\":52330},{\"end\":52760,\"start\":52758},{\"end\":52771,\"start\":52765},{\"end\":52783,\"start\":52778},{\"end\":52796,\"start\":52791},{\"end\":52808,\"start\":52801},{\"end\":52820,\"start\":52813},{\"end\":53249,\"start\":53241},{\"end\":53262,\"start\":53255},{\"end\":53275,\"start\":53270},{\"end\":53291,\"start\":53286},{\"end\":53304,\"start\":53298},{\"end\":53317,\"start\":53313},{\"end\":53332,\"start\":53327},{\"end\":53351,\"start\":53341},{\"end\":53726,\"start\":53718},{\"end\":53737,\"start\":53732},{\"end\":53749,\"start\":53745},{\"end\":54153,\"start\":54146},{\"end\":54163,\"start\":54159},{\"end\":54171,\"start\":54168},{\"end\":54183,\"start\":54179},{\"end\":54198,\"start\":54190},{\"end\":54207,\"start\":54203},{\"end\":54215,\"start\":54212},{\"end\":54224,\"start\":54221},{\"end\":54527,\"start\":54525},{\"end\":54539,\"start\":54533},{\"end\":54548,\"start\":54545},{\"end\":54557,\"start\":54554},{\"end\":54568,\"start\":54562},{\"end\":54579,\"start\":54574},{\"end\":54594,\"start\":54587},{\"end\":54607,\"start\":54600},{\"end\":55021,\"start\":55017},{\"end\":55039,\"start\":55034},{\"end\":55311,\"start\":55301},{\"end\":55325,\"start\":55317},{\"end\":55335,\"start\":55332},{\"end\":55348,\"start\":55343},{\"end\":55361,\"start\":55355},{\"end\":55372,\"start\":55367},{\"end\":55796,\"start\":55791},{\"end\":55809,\"start\":55802},{\"end\":55821,\"start\":55814},{\"end\":55831,\"start\":55828},{\"end\":55845,\"start\":55838},{\"end\":55858,\"start\":55852},{\"end\":56278,\"start\":56273},{\"end\":56287,\"start\":56284},{\"end\":56301,\"start\":56294},{\"end\":56315,\"start\":56308},{\"end\":56535,\"start\":56530},{\"end\":56548,\"start\":56541},{\"end\":56560,\"start\":56553},{\"end\":56570,\"start\":56567},{\"end\":56584,\"start\":56577},{\"end\":57049,\"start\":57040},{\"end\":57063,\"start\":57058},{\"end\":57074,\"start\":57071},{\"end\":57273,\"start\":57267},{\"end\":57285,\"start\":57282},{\"end\":57555,\"start\":57548},{\"end\":57574,\"start\":57570},{\"end\":57583,\"start\":57579},{\"end\":57960,\"start\":57955},{\"end\":57980,\"start\":57974},{\"end\":57995,\"start\":57987},{\"end\":58006,\"start\":58002},{\"end\":58021,\"start\":58018},{\"end\":58034,\"start\":58028},{\"end\":58557,\"start\":58552},{\"end\":58570,\"start\":58564},{\"end\":58583,\"start\":58575},{\"end\":58595,\"start\":58591},{\"end\":58606,\"start\":58602},{\"end\":58618,\"start\":58611},{\"end\":58630,\"start\":58626},{\"end\":58639,\"start\":58635},{\"end\":58985,\"start\":58980},{\"end\":58998,\"start\":58992},{\"end\":59011,\"start\":59003},{\"end\":59023,\"start\":59019},{\"end\":59034,\"start\":59030},{\"end\":59046,\"start\":59039},{\"end\":59058,\"start\":59054},{\"end\":59067,\"start\":59063},{\"end\":59567,\"start\":59563},{\"end\":59577,\"start\":59573},{\"end\":59591,\"start\":59583},{\"end\":59603,\"start\":59596},{\"end\":59611,\"start\":59609},{\"end\":59625,\"start\":59617},{\"end\":59872,\"start\":59868},{\"end\":59882,\"start\":59878},{\"end\":59895,\"start\":59888},{\"end\":59902,\"start\":59900},{\"end\":59915,\"start\":59908},{\"end\":60374,\"start\":60369},{\"end\":60389,\"start\":60383},{\"end\":60406,\"start\":60397},{\"end\":60682,\"start\":60680},{\"end\":60691,\"start\":60688},{\"end\":60702,\"start\":60698},{\"end\":60716,\"start\":60708},{\"end\":61162,\"start\":61159},{\"end\":61172,\"start\":61169},{\"end\":61186,\"start\":61179},{\"end\":61367,\"start\":61364},{\"end\":61381,\"start\":61374},{\"end\":61391,\"start\":61388},{\"end\":61402,\"start\":61398},{\"end\":61739,\"start\":61732},{\"end\":61754,\"start\":61746},{\"end\":62188,\"start\":62185},{\"end\":62202,\"start\":62195},{\"end\":62213,\"start\":62207},{\"end\":62599,\"start\":62596},{\"end\":62611,\"start\":62605},{\"end\":62624,\"start\":62616},{\"end\":62638,\"start\":62630},{\"end\":63108,\"start\":63103},{\"end\":63118,\"start\":63115},{\"end\":63127,\"start\":63123},{\"end\":63142,\"start\":63135},{\"end\":63152,\"start\":63148},{\"end\":63165,\"start\":63160},{\"end\":63180,\"start\":63172},{\"end\":63467,\"start\":63464},{\"end\":63479,\"start\":63475},{\"end\":63491,\"start\":63484},{\"end\":63500,\"start\":63497},{\"end\":63512,\"start\":63508},{\"end\":63520,\"start\":63517},{\"end\":63527,\"start\":63526},{\"end\":63547,\"start\":63536},{\"end\":63895,\"start\":63889},{\"end\":63908,\"start\":63902},{\"end\":63922,\"start\":63915},{\"end\":64173,\"start\":64167},{\"end\":64185,\"start\":64179},{\"end\":64195,\"start\":64190},{\"end\":64203,\"start\":64200},{\"end\":64216,\"start\":64208},{\"end\":64229,\"start\":64223},{\"end\":65779,\"start\":65778},{\"end\":67335,\"start\":67334}]", "bib_author_last_name": "[{\"end\":47556,\"start\":47553},{\"end\":47569,\"start\":47564},{\"end\":47582,\"start\":47579},{\"end\":47596,\"start\":47590},{\"end\":48026,\"start\":48020},{\"end\":48043,\"start\":48038},{\"end\":48061,\"start\":48053},{\"end\":48078,\"start\":48071},{\"end\":48098,\"start\":48090},{\"end\":48116,\"start\":48107},{\"end\":48493,\"start\":48488},{\"end\":48503,\"start\":48499},{\"end\":48518,\"start\":48514},{\"end\":48531,\"start\":48528},{\"end\":48541,\"start\":48535},{\"end\":48552,\"start\":48547},{\"end\":48559,\"start\":48554},{\"end\":49069,\"start\":49065},{\"end\":49077,\"start\":49074},{\"end\":49087,\"start\":49083},{\"end\":49104,\"start\":49099},{\"end\":49119,\"start\":49115},{\"end\":49539,\"start\":49537},{\"end\":49557,\"start\":49549},{\"end\":49571,\"start\":49565},{\"end\":49586,\"start\":49578},{\"end\":50013,\"start\":50008},{\"end\":50022,\"start\":50018},{\"end\":50035,\"start\":50031},{\"end\":50045,\"start\":50043},{\"end\":50297,\"start\":50294},{\"end\":50317,\"start\":50304},{\"end\":50335,\"start\":50325},{\"end\":50349,\"start\":50341},{\"end\":50540,\"start\":50532},{\"end\":50554,\"start\":50548},{\"end\":50558,\"start\":50556},{\"end\":50777,\"start\":50775},{\"end\":50788,\"start\":50783},{\"end\":50801,\"start\":50798},{\"end\":50811,\"start\":50808},{\"end\":50821,\"start\":50815},{\"end\":50829,\"start\":50827},{\"end\":50836,\"start\":50831},{\"end\":51338,\"start\":51336},{\"end\":51349,\"start\":51344},{\"end\":51362,\"start\":51359},{\"end\":51373,\"start\":51368},{\"end\":51386,\"start\":51384},{\"end\":51404,\"start\":51400},{\"end\":51738,\"start\":51736},{\"end\":51748,\"start\":51744},{\"end\":51757,\"start\":51754},{\"end\":51769,\"start\":51766},{\"end\":51783,\"start\":51779},{\"end\":51792,\"start\":51790},{\"end\":52271,\"start\":52269},{\"end\":52284,\"start\":52280},{\"end\":52297,\"start\":52293},{\"end\":52307,\"start\":52303},{\"end\":52316,\"start\":52312},{\"end\":52328,\"start\":52325},{\"end\":52337,\"start\":52335},{\"end\":52763,\"start\":52761},{\"end\":52776,\"start\":52772},{\"end\":52789,\"start\":52784},{\"end\":52799,\"start\":52797},{\"end\":52811,\"start\":52809},{\"end\":52823,\"start\":52821},{\"end\":53253,\"start\":53250},{\"end\":53268,\"start\":53263},{\"end\":53284,\"start\":53276},{\"end\":53296,\"start\":53292},{\"end\":53311,\"start\":53305},{\"end\":53325,\"start\":53318},{\"end\":53339,\"start\":53333},{\"end\":53359,\"start\":53352},{\"end\":53730,\"start\":53727},{\"end\":53743,\"start\":53738},{\"end\":53758,\"start\":53750},{\"end\":54157,\"start\":54154},{\"end\":54166,\"start\":54164},{\"end\":54177,\"start\":54172},{\"end\":54188,\"start\":54184},{\"end\":54201,\"start\":54199},{\"end\":54210,\"start\":54208},{\"end\":54219,\"start\":54216},{\"end\":54230,\"start\":54225},{\"end\":54531,\"start\":54528},{\"end\":54543,\"start\":54540},{\"end\":54552,\"start\":54549},{\"end\":54560,\"start\":54558},{\"end\":54572,\"start\":54569},{\"end\":54585,\"start\":54580},{\"end\":54598,\"start\":54595},{\"end\":54611,\"start\":54608},{\"end\":55032,\"start\":55022},{\"end\":55046,\"start\":55040},{\"end\":55315,\"start\":55312},{\"end\":55330,\"start\":55326},{\"end\":55341,\"start\":55336},{\"end\":55353,\"start\":55349},{\"end\":55365,\"start\":55362},{\"end\":55377,\"start\":55373},{\"end\":55800,\"start\":55797},{\"end\":55812,\"start\":55810},{\"end\":55826,\"start\":55822},{\"end\":55836,\"start\":55832},{\"end\":55850,\"start\":55846},{\"end\":55863,\"start\":55859},{\"end\":55871,\"start\":55865},{\"end\":56282,\"start\":56279},{\"end\":56292,\"start\":56288},{\"end\":56306,\"start\":56302},{\"end\":56320,\"start\":56316},{\"end\":56539,\"start\":56536},{\"end\":56551,\"start\":56549},{\"end\":56565,\"start\":56561},{\"end\":56575,\"start\":56571},{\"end\":56589,\"start\":56585},{\"end\":57056,\"start\":57050},{\"end\":57069,\"start\":57064},{\"end\":57079,\"start\":57075},{\"end\":57280,\"start\":57274},{\"end\":57293,\"start\":57286},{\"end\":57568,\"start\":57556},{\"end\":57577,\"start\":57575},{\"end\":57592,\"start\":57584},{\"end\":57597,\"start\":57594},{\"end\":57972,\"start\":57961},{\"end\":57985,\"start\":57981},{\"end\":58000,\"start\":57996},{\"end\":58016,\"start\":58007},{\"end\":58026,\"start\":58022},{\"end\":58043,\"start\":58035},{\"end\":58562,\"start\":58558},{\"end\":58573,\"start\":58571},{\"end\":58589,\"start\":58584},{\"end\":58600,\"start\":58596},{\"end\":58609,\"start\":58607},{\"end\":58624,\"start\":58619},{\"end\":58633,\"start\":58631},{\"end\":58643,\"start\":58640},{\"end\":58990,\"start\":58986},{\"end\":59001,\"start\":58999},{\"end\":59017,\"start\":59012},{\"end\":59028,\"start\":59024},{\"end\":59037,\"start\":59035},{\"end\":59052,\"start\":59047},{\"end\":59061,\"start\":59059},{\"end\":59071,\"start\":59068},{\"end\":59571,\"start\":59568},{\"end\":59581,\"start\":59578},{\"end\":59594,\"start\":59592},{\"end\":59607,\"start\":59604},{\"end\":59615,\"start\":59612},{\"end\":59628,\"start\":59626},{\"end\":59876,\"start\":59873},{\"end\":59886,\"start\":59883},{\"end\":59898,\"start\":59896},{\"end\":59906,\"start\":59903},{\"end\":59919,\"start\":59916},{\"end\":60381,\"start\":60375},{\"end\":60395,\"start\":60390},{\"end\":60413,\"start\":60407},{\"end\":60686,\"start\":60683},{\"end\":60696,\"start\":60692},{\"end\":60706,\"start\":60703},{\"end\":60721,\"start\":60717},{\"end\":61167,\"start\":61163},{\"end\":61177,\"start\":61173},{\"end\":61191,\"start\":61187},{\"end\":61372,\"start\":61368},{\"end\":61386,\"start\":61382},{\"end\":61396,\"start\":61392},{\"end\":61405,\"start\":61403},{\"end\":61744,\"start\":61740},{\"end\":61760,\"start\":61755},{\"end\":62193,\"start\":62189},{\"end\":62205,\"start\":62203},{\"end\":62217,\"start\":62214},{\"end\":62603,\"start\":62600},{\"end\":62614,\"start\":62612},{\"end\":62628,\"start\":62625},{\"end\":62644,\"start\":62639},{\"end\":63113,\"start\":63109},{\"end\":63121,\"start\":63119},{\"end\":63133,\"start\":63128},{\"end\":63146,\"start\":63143},{\"end\":63158,\"start\":63153},{\"end\":63170,\"start\":63166},{\"end\":63185,\"start\":63181},{\"end\":63473,\"start\":63468},{\"end\":63482,\"start\":63480},{\"end\":63495,\"start\":63492},{\"end\":63506,\"start\":63501},{\"end\":63515,\"start\":63513},{\"end\":63524,\"start\":63521},{\"end\":63534,\"start\":63528},{\"end\":63550,\"start\":63548},{\"end\":63556,\"start\":63552},{\"end\":63562,\"start\":63558},{\"end\":63900,\"start\":63896},{\"end\":63913,\"start\":63909},{\"end\":63933,\"start\":63923},{\"end\":64177,\"start\":64174},{\"end\":64188,\"start\":64186},{\"end\":64198,\"start\":64196},{\"end\":64206,\"start\":64204},{\"end\":64221,\"start\":64217},{\"end\":64233,\"start\":64230},{\"end\":65790,\"start\":65780},{\"end\":65805,\"start\":65792},{\"end\":65945,\"start\":65941},{\"end\":67352,\"start\":67336}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":16224674},\"end\":47963,\"start\":47480},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":218889832},\"end\":48394,\"start\":47965},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":212675120},\"end\":48986,\"start\":48396},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":233033921},\"end\":49515,\"start\":48988},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":54465873},\"end\":49876,\"start\":49517},{\"attributes\":{\"id\":\"b5\"},\"end\":50225,\"start\":49878},{\"attributes\":{\"id\":\"b6\"},\"end\":50484,\"start\":50227},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b7\"},\"end\":50702,\"start\":50486},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":247218212},\"end\":51233,\"start\":50704},{\"attributes\":{\"doi\":\"arXiv:2206.02777\",\"id\":\"b9\"},\"end\":51653,\"start\":51235},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":54443684},\"end\":52196,\"start\":51655},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":236318539},\"end\":52712,\"start\":52198},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":233231207},\"end\":53196,\"start\":52714},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":14113767},\"end\":53647,\"start\":53198},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206771220},\"end\":54084,\"start\":53649},{\"attributes\":{\"doi\":\"arXiv:2201.12329\",\"id\":\"b15\"},\"end\":54450,\"start\":54086},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":232352874},\"end\":55013,\"start\":54452},{\"attributes\":{\"doi\":\"arXiv:1711.05101\",\"id\":\"b17\"},\"end\":55228,\"start\":55015},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":229923779},\"end\":55789,\"start\":55230},{\"attributes\":{\"doi\":\"arXiv:2103.15320\",\"id\":\"b19\"},\"end\":56124,\"start\":55791},{\"attributes\":{\"id\":\"b20\"},\"end\":56528,\"start\":56126},{\"attributes\":{\"doi\":\"arXiv:2201.07412\",\"id\":\"b21\"},\"end\":56910,\"start\":56530},{\"attributes\":{\"id\":\"b22\"},\"end\":57265,\"start\":56912},{\"attributes\":{\"doi\":\"arXiv:1804.02767\",\"id\":\"b23\"},\"end\":57466,\"start\":57267},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":10328909},\"end\":57867,\"start\":57468},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":67855581},\"end\":58480,\"start\":57869},{\"attributes\":{\"id\":\"b26\"},\"end\":58908,\"start\":58482},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":207967883},\"end\":59466,\"start\":58910},{\"attributes\":{\"id\":\"b28\"},\"end\":59807,\"start\":59468},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":250042546},\"end\":60296,\"start\":59809},{\"attributes\":{\"doi\":\"arXiv:2103.12115\",\"id\":\"b30\"},\"end\":60606,\"start\":60298},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":67856425},\"end\":61097,\"start\":60608},{\"attributes\":{\"doi\":\"arXiv:1911.07451\",\"id\":\"b32\"},\"end\":61362,\"start\":61099},{\"attributes\":{\"id\":\"b33\"},\"end\":61658,\"start\":61364},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":249917711},\"end\":62126,\"start\":61660},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":4934594},\"end\":62516,\"start\":62128},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":247218711},\"end\":63037,\"start\":62518},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":248498368},\"end\":63462,\"start\":63039},{\"attributes\":{\"doi\":\"arXiv:2203.03605\",\"id\":\"b38\"},\"end\":63885,\"start\":63464},{\"attributes\":{\"doi\":\"arXiv:1904.07850\",\"id\":\"b39\"},\"end\":64091,\"start\":63887},{\"attributes\":{\"doi\":\"arXiv:2010.04159\",\"id\":\"b40\"},\"end\":64449,\"start\":64093},{\"attributes\":{\"id\":\"b41\"},\"end\":65774,\"start\":64451},{\"attributes\":{\"id\":\"b42\"},\"end\":65837,\"start\":65776},{\"attributes\":{\"id\":\"b43\"},\"end\":66643,\"start\":65839},{\"attributes\":{\"id\":\"b44\"},\"end\":69160,\"start\":66645}]", "bib_title": "[{\"end\":47547,\"start\":47480},{\"end\":48010,\"start\":47965},{\"end\":48480,\"start\":48396},{\"end\":49056,\"start\":48988},{\"end\":49527,\"start\":49517},{\"end\":50768,\"start\":50704},{\"end\":51726,\"start\":51655},{\"end\":52259,\"start\":52198},{\"end\":52756,\"start\":52714},{\"end\":53239,\"start\":53198},{\"end\":53716,\"start\":53649},{\"end\":54523,\"start\":54452},{\"end\":55299,\"start\":55230},{\"end\":57546,\"start\":57468},{\"end\":57953,\"start\":57869},{\"end\":58978,\"start\":58910},{\"end\":59866,\"start\":59809},{\"end\":60678,\"start\":60608},{\"end\":61730,\"start\":61660},{\"end\":62183,\"start\":62128},{\"end\":62594,\"start\":62518},{\"end\":63101,\"start\":63039},{\"end\":64845,\"start\":64451},{\"end\":65939,\"start\":65839},{\"end\":67332,\"start\":66645}]", "bib_author": "[{\"end\":47558,\"start\":47549},{\"end\":47571,\"start\":47558},{\"end\":47584,\"start\":47571},{\"end\":47598,\"start\":47584},{\"end\":48028,\"start\":48012},{\"end\":48045,\"start\":48028},{\"end\":48063,\"start\":48045},{\"end\":48080,\"start\":48063},{\"end\":48100,\"start\":48080},{\"end\":48118,\"start\":48100},{\"end\":48495,\"start\":48482},{\"end\":48505,\"start\":48495},{\"end\":48520,\"start\":48505},{\"end\":48533,\"start\":48520},{\"end\":48543,\"start\":48533},{\"end\":48554,\"start\":48543},{\"end\":48561,\"start\":48554},{\"end\":49071,\"start\":49058},{\"end\":49079,\"start\":49071},{\"end\":49089,\"start\":49079},{\"end\":49106,\"start\":49089},{\"end\":49121,\"start\":49106},{\"end\":49541,\"start\":49529},{\"end\":49559,\"start\":49541},{\"end\":49573,\"start\":49559},{\"end\":49588,\"start\":49573},{\"end\":50015,\"start\":50001},{\"end\":50024,\"start\":50015},{\"end\":50037,\"start\":50024},{\"end\":50047,\"start\":50037},{\"end\":50299,\"start\":50291},{\"end\":50319,\"start\":50299},{\"end\":50337,\"start\":50319},{\"end\":50351,\"start\":50337},{\"end\":50542,\"start\":50530},{\"end\":50556,\"start\":50542},{\"end\":50560,\"start\":50556},{\"end\":50779,\"start\":50770},{\"end\":50790,\"start\":50779},{\"end\":50803,\"start\":50790},{\"end\":50813,\"start\":50803},{\"end\":50823,\"start\":50813},{\"end\":50831,\"start\":50823},{\"end\":50838,\"start\":50831},{\"end\":51340,\"start\":51331},{\"end\":51351,\"start\":51340},{\"end\":51364,\"start\":51351},{\"end\":51375,\"start\":51364},{\"end\":51388,\"start\":51375},{\"end\":51406,\"start\":51388},{\"end\":51740,\"start\":51728},{\"end\":51750,\"start\":51740},{\"end\":51759,\"start\":51750},{\"end\":51771,\"start\":51759},{\"end\":51785,\"start\":51771},{\"end\":51794,\"start\":51785},{\"end\":52273,\"start\":52261},{\"end\":52286,\"start\":52273},{\"end\":52299,\"start\":52286},{\"end\":52309,\"start\":52299},{\"end\":52318,\"start\":52309},{\"end\":52330,\"start\":52318},{\"end\":52339,\"start\":52330},{\"end\":52765,\"start\":52758},{\"end\":52778,\"start\":52765},{\"end\":52791,\"start\":52778},{\"end\":52801,\"start\":52791},{\"end\":52813,\"start\":52801},{\"end\":52825,\"start\":52813},{\"end\":53255,\"start\":53241},{\"end\":53270,\"start\":53255},{\"end\":53286,\"start\":53270},{\"end\":53298,\"start\":53286},{\"end\":53313,\"start\":53298},{\"end\":53327,\"start\":53313},{\"end\":53341,\"start\":53327},{\"end\":53361,\"start\":53341},{\"end\":53732,\"start\":53718},{\"end\":53745,\"start\":53732},{\"end\":53760,\"start\":53745},{\"end\":54159,\"start\":54146},{\"end\":54168,\"start\":54159},{\"end\":54179,\"start\":54168},{\"end\":54190,\"start\":54179},{\"end\":54203,\"start\":54190},{\"end\":54212,\"start\":54203},{\"end\":54221,\"start\":54212},{\"end\":54232,\"start\":54221},{\"end\":54533,\"start\":54525},{\"end\":54545,\"start\":54533},{\"end\":54554,\"start\":54545},{\"end\":54562,\"start\":54554},{\"end\":54574,\"start\":54562},{\"end\":54587,\"start\":54574},{\"end\":54600,\"start\":54587},{\"end\":54613,\"start\":54600},{\"end\":55034,\"start\":55017},{\"end\":55048,\"start\":55034},{\"end\":55317,\"start\":55301},{\"end\":55332,\"start\":55317},{\"end\":55343,\"start\":55332},{\"end\":55355,\"start\":55343},{\"end\":55367,\"start\":55355},{\"end\":55379,\"start\":55367},{\"end\":55802,\"start\":55791},{\"end\":55814,\"start\":55802},{\"end\":55828,\"start\":55814},{\"end\":55838,\"start\":55828},{\"end\":55852,\"start\":55838},{\"end\":55865,\"start\":55852},{\"end\":55873,\"start\":55865},{\"end\":56284,\"start\":56273},{\"end\":56294,\"start\":56284},{\"end\":56308,\"start\":56294},{\"end\":56322,\"start\":56308},{\"end\":56541,\"start\":56530},{\"end\":56553,\"start\":56541},{\"end\":56567,\"start\":56553},{\"end\":56577,\"start\":56567},{\"end\":56591,\"start\":56577},{\"end\":57058,\"start\":57040},{\"end\":57071,\"start\":57058},{\"end\":57081,\"start\":57071},{\"end\":57282,\"start\":57267},{\"end\":57295,\"start\":57282},{\"end\":57570,\"start\":57548},{\"end\":57579,\"start\":57570},{\"end\":57594,\"start\":57579},{\"end\":57599,\"start\":57594},{\"end\":57974,\"start\":57955},{\"end\":57987,\"start\":57974},{\"end\":58002,\"start\":57987},{\"end\":58018,\"start\":58002},{\"end\":58028,\"start\":58018},{\"end\":58045,\"start\":58028},{\"end\":58564,\"start\":58552},{\"end\":58575,\"start\":58564},{\"end\":58591,\"start\":58575},{\"end\":58602,\"start\":58591},{\"end\":58611,\"start\":58602},{\"end\":58626,\"start\":58611},{\"end\":58635,\"start\":58626},{\"end\":58645,\"start\":58635},{\"end\":58992,\"start\":58980},{\"end\":59003,\"start\":58992},{\"end\":59019,\"start\":59003},{\"end\":59030,\"start\":59019},{\"end\":59039,\"start\":59030},{\"end\":59054,\"start\":59039},{\"end\":59063,\"start\":59054},{\"end\":59073,\"start\":59063},{\"end\":59573,\"start\":59563},{\"end\":59583,\"start\":59573},{\"end\":59596,\"start\":59583},{\"end\":59609,\"start\":59596},{\"end\":59617,\"start\":59609},{\"end\":59630,\"start\":59617},{\"end\":59878,\"start\":59868},{\"end\":59888,\"start\":59878},{\"end\":59900,\"start\":59888},{\"end\":59908,\"start\":59900},{\"end\":59921,\"start\":59908},{\"end\":60383,\"start\":60369},{\"end\":60397,\"start\":60383},{\"end\":60415,\"start\":60397},{\"end\":60688,\"start\":60680},{\"end\":60698,\"start\":60688},{\"end\":60708,\"start\":60698},{\"end\":60723,\"start\":60708},{\"end\":61169,\"start\":61159},{\"end\":61179,\"start\":61169},{\"end\":61193,\"start\":61179},{\"end\":61374,\"start\":61364},{\"end\":61388,\"start\":61374},{\"end\":61398,\"start\":61388},{\"end\":61407,\"start\":61398},{\"end\":61746,\"start\":61732},{\"end\":61762,\"start\":61746},{\"end\":62195,\"start\":62185},{\"end\":62207,\"start\":62195},{\"end\":62219,\"start\":62207},{\"end\":62605,\"start\":62596},{\"end\":62616,\"start\":62605},{\"end\":62630,\"start\":62616},{\"end\":62646,\"start\":62630},{\"end\":63115,\"start\":63103},{\"end\":63123,\"start\":63115},{\"end\":63135,\"start\":63123},{\"end\":63148,\"start\":63135},{\"end\":63160,\"start\":63148},{\"end\":63172,\"start\":63160},{\"end\":63187,\"start\":63172},{\"end\":63475,\"start\":63464},{\"end\":63484,\"start\":63475},{\"end\":63497,\"start\":63484},{\"end\":63508,\"start\":63497},{\"end\":63517,\"start\":63508},{\"end\":63526,\"start\":63517},{\"end\":63536,\"start\":63526},{\"end\":63552,\"start\":63536},{\"end\":63558,\"start\":63552},{\"end\":63564,\"start\":63558},{\"end\":63902,\"start\":63889},{\"end\":63915,\"start\":63902},{\"end\":63935,\"start\":63915},{\"end\":64179,\"start\":64167},{\"end\":64190,\"start\":64179},{\"end\":64200,\"start\":64190},{\"end\":64208,\"start\":64200},{\"end\":64223,\"start\":64208},{\"end\":64235,\"start\":64223},{\"end\":65792,\"start\":65778},{\"end\":65807,\"start\":65792},{\"end\":65947,\"start\":65941},{\"end\":67354,\"start\":67334}]", "bib_venue": "[{\"end\":47739,\"start\":47677},{\"end\":48710,\"start\":48644},{\"end\":49270,\"start\":49204},{\"end\":49709,\"start\":49657},{\"end\":50987,\"start\":50921},{\"end\":51943,\"start\":51877},{\"end\":52468,\"start\":52412},{\"end\":52974,\"start\":52908},{\"end\":53881,\"start\":53829},{\"end\":54756,\"start\":54693},{\"end\":55528,\"start\":55462},{\"end\":58194,\"start\":58128},{\"end\":59202,\"start\":59146},{\"end\":60070,\"start\":60004},{\"end\":60872,\"start\":60806},{\"end\":61911,\"start\":61845},{\"end\":62334,\"start\":62285},{\"end\":62795,\"start\":62729},{\"end\":47675,\"start\":47598},{\"end\":48156,\"start\":48118},{\"end\":48642,\"start\":48561},{\"end\":49202,\"start\":49121},{\"end\":49655,\"start\":49588},{\"end\":49999,\"start\":49878},{\"end\":50289,\"start\":50227},{\"end\":50528,\"start\":50486},{\"end\":50919,\"start\":50838},{\"end\":51329,\"start\":51235},{\"end\":51875,\"start\":51794},{\"end\":52410,\"start\":52339},{\"end\":52906,\"start\":52825},{\"end\":53399,\"start\":53361},{\"end\":53827,\"start\":53760},{\"end\":54144,\"start\":54086},{\"end\":54691,\"start\":54613},{\"end\":55460,\"start\":55379},{\"end\":55935,\"start\":55889},{\"end\":56271,\"start\":56126},{\"end\":56700,\"start\":56607},{\"end\":57038,\"start\":56912},{\"end\":57345,\"start\":57311},{\"end\":57661,\"start\":57599},{\"end\":58126,\"start\":58045},{\"end\":58550,\"start\":58482},{\"end\":59144,\"start\":59073},{\"end\":59561,\"start\":59468},{\"end\":60002,\"start\":59921},{\"end\":60367,\"start\":60298},{\"end\":60804,\"start\":60723},{\"end\":61157,\"start\":61099},{\"end\":61504,\"start\":61407},{\"end\":61843,\"start\":61762},{\"end\":62283,\"start\":62219},{\"end\":62727,\"start\":62646},{\"end\":63236,\"start\":63187},{\"end\":63653,\"start\":63580},{\"end\":64165,\"start\":64093},{\"end\":64961,\"start\":64847},{\"end\":66079,\"start\":65947},{\"end\":67648,\"start\":67354}]"}}}, "year": 2023, "month": 12, "day": 17}