{"id": 53081554, "updated": "2023-10-01 06:27:14.861", "metadata": {"title": "Diversity-Promoting GAN: A Cross-Entropy Based Generative Adversarial Network for Diversified Text Generation", "authors": "[{\"first\":\"Jingjing\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Xuancheng\",\"last\":\"Ren\",\"middle\":[]},{\"first\":\"Junyang\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Xu\",\"last\":\"Sun\",\"middle\":[]}]", "venue": "EMNLP", "journal": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "Existing text generation methods tend to produce repeated and \u201dboring\u201d expressions. To tackle this problem, we propose a new text generation model, called Diversity-Promoting Generative Adversarial Network (DP-GAN). The proposed model assigns low reward for repeatedly generated text and high reward for \u201dnovel\u201d and fluent text, encouraging the generator to produce diverse and informative text. Moreover, we propose a novel language-model based discriminator, which can better distinguish novel text from repeated text without the saturation problem compared with existing classifier-based discriminators. The experimental results on review generation and dialogue generation tasks demonstrate that our model can generate substantially more diverse and informative text than existing baselines.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1802.01345", "mag": "2892153332", "acl": "D18-1428", "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/XuRL018", "doi": "10.18653/v1/d18-1428"}}, "content": {"source": {"pdf_hash": "745ed3d3ea872bc68a7955074362c428deb01692", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/D18-1428.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/D18-1428.pdf", "status": "HYBRID"}}, "grobid": {"id": "6d3f8bf5e259c5d225ccc56cf281778b9a6c7e6a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/745ed3d3ea872bc68a7955074362c428deb01692.txt", "contents": "\nDiversity-Promoting GAN: A Cross-Entropy Based Generative Adversarial Network for Diversified Text Generation\nAssociation for Computational LinguisticsCopyright Association for Computational LinguisticsOctober 31 -November 4. 2018. 2018\n\nJingjing Xu jingjingxu@pku.edu.cn \nSchool of EECS\nMOE Key Lab of Computational Linguistics\nPeking University\n\n\nXuancheng Ren \nSchool of EECS\nMOE Key Lab of Computational Linguistics\nPeking University\n\n\nJunyang Lin linjunyang@pku.edu.cn \nSchool of EECS\nMOE Key Lab of Computational Linguistics\nPeking University\n\n\nXu Sun xusun@pku.edu.cn \nSchool of EECS\nMOE Key Lab of Computational Linguistics\nPeking University\n\n\nDiversity-Promoting GAN: A Cross-Entropy Based Generative Adversarial Network for Diversified Text Generation\n\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\nthe 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsOctober 31 -November 4. 2018. 20183940\nExisting text generation methods tend to produce repeated and \"boring\" expressions. To tackle this problem, we propose a new text generation model, called Diversity-Promoting Generative Adversarial Network (DP-GAN). The proposed model assigns low reward for repeatedly generated text and high reward for \"novel\" and fluent text, encouraging the generator to produce diverse and informative text. Moreover, we propose a novel languagemodel based discriminator, which can better distinguish novel text from repeated text without the saturation problem compared with existing classifier-based discriminators. The experimental results on review generation and dialogue generation tasks demonstrate that our model can generate substantially more diverse and informative text than existing baselines. 1\n\nIntroduction\n\nText generation is an important task in Natural Language Processing (NLP) as it lays the foundation for many applications, such as dialogue generation, machine translation (Ma et al., 2018b;, text summarization (Ma et al., 2018a), and table summarization (Liu et al., 2017). In these tasks, most of the systems are built upon the sequence-to-sequence paradigm (Sutskever et al., 2014), which is an end-to-end model that encodes a source sentence to a dense vector and then decodes the vector to a target sentence. The standard training method is based on Maximum Likelihood Estimation (MLE).\n\nAlthough being widely applied, the conventional MLE training causes systems to repeatedly generate \"boring\" sentences, which usually are expressions with high frequency (e.g., \"I am sorry\" in dialogue generation (Li et al., 2016)). The major reason is that MLE encourages the model to overproduce high-frequency words. 2 The overestimation of high-frequency words discourages the model from generating low-frequency but meaningful words in real data, which makes generated text tend to be repeated and \"boring\".\n\nTo tackle this problem, we propose a new model for diversified text generation, called DP-GAN.\n\nThe key idea is to build a discriminator that is responsible for giving reward to the generator based on the novelty of generated text. We consider the text that is frequently generated by the generator as the low-novelty text and the text that is uncommon in the generated data as the high-novelty text. Considering most of the real-world sentences are novel and fluent, we treat the real-world text as the positive example and the generated text as the negative example to train the discriminator. Such training mechanism encourages the discriminator to give higher reward for the text that looks like real-world data. The reward is fed back to the generator, which promotes the generator to generate diverse and fluent text via policy gradient. In this framework, a good discriminator that can assign reasonable reward for the generator is a critical component.\n\nHowever, directly applying a classifier as the discriminator like most existing GAN models (e.g., SeqGAN (Yu et al., 2017)) cannot achieve satisfactory performance. The main problem is that the reward given by the classifier cannot reflect the novelty of text accurately. First, most existing classifier-based discriminators take the probability of a sequence being true as the reward. When a sentence fits the distribution of real-world text and is far from the generated data, the reward saturates and scarcely distinguishes the difference between these novel sentences. For example, for a sentence A with mildly high novelty and a sentence B with extremely high novelty, the classifier cannot tell the difference and gives them saturated reward: 0.997 and 0.998. Second, in our tasks, we find that a simple classifier can reach very high accuracy (almost 99%), which makes most generated text receive reward around zero because the discriminator can identify them with high confidence. It shows that the classifier also cannot distinguish the difference between low-novelty text. The reason for this problem is that the training objective of the classifier-based GAN is in fact minimizing the Jensen-Shannon Divergence (JSD) between the distributions of the real data and the generated data (Nowozin et al., 2016). If the accuracy of classifier is too high, JSD fails to measure the distance between the two distributions, and cannot give reasonable reward to the model for generating real and diverse text .\n\nInstead of using a classifier, we propose a novel language-model based discriminator and use the output of the language model, cross-entropy, as the reward. The main advantage of our model lies in that the cross-entropy based reward for novel text is high and does not saturate, while the reward for text with low novelty is small but discriminative. The analysis of the experimental results shows that our discriminator can better distinguish novel text compared with traditional classifier-based discriminators.\n\nOur contributions are listed as follows:\n\n\u2022 We propose a new model, called DP-GAN, for diversified text generation, which assigns low reward for repeated text and high reward for novel and fluent text.\n\n\u2022 We propose a novel language-model based discriminator that can better distinguish novel text from repeated text without the saturation problem.\n\n\u2022 The experimental results on review generation and dialogue generation tasks show that our method can generate substantially more diverse and informative text than existing methods.\n\n\nRelated Work\n\nA great deal of attention has been paid to developing data-driven methods for natural language dialogue generation. Conventional statistical approaches tend to rely extensively on hand-crafted rules and templates, require interaction with humans or simulated users to optimize parameters, or produce conversation responses in an information retrieval fashion. Such properties prevent training on the large corpora that are becoming increasingly available, or fail to produce novel natural language responses. Currently, a popular model for text generation is the sequence-to-sequence model (Sutskever et al., 2014;. However, the sequenceto-sequence model tends to generate short, repetitive , and dull text (Luo et al., 2018). Recent researches have focused on developing methods to generate informative  and diverse text (Li et al., 2017(Li et al., , 2016Guu et al., 2017;Shao et al., 2017). Reinforcement learning is incorporated into the model of conversation generation to generate more humanlike speeches (Li et al., 2017). Moreover, there are also other methods to improve the diversity of the generated text by using mutual-information, prototype editing, and self attention (Li et al., 2016;Guu et al., 2017;Shao et al., 2017).\n\nIn this paper, to handle this problem, we propose to use adversarial training (Goodfellow et al., 2014;Denton et al., 2015;Li et al., 2017), which has achieved success in image generation (Radford et al., 2015;Gulrajani et al., 2017;Berthelot et al., 2017). However, training GAN is a non-trivial task and there are some previous researches that investigate methods to improve training performance, such as Wasserstein GAN (WGAN)  and Energybased GAN (EGAN) (Salimans et al., 2016;Gulrajani et al., 2017;Zhao et al., 2017;Berthelot et al., 2017). GAN in text generation has not shown significant improvement as it has in computer vision. This is partially because text generation is a process of sampling in discrete space where the normal gradient descent solution is not available, which makes it difficult to train. There are some researches that focus on tackling this problem. SeqGAN (Yu et al., 2017)  trains the sequence-to-sequence model with policy gradient for neural machine translation. Bahdanau et al. (2017) applies the actor-critic model on the same task.\n\n\nDiversity-Promoting GAN\n\nThe basic structure of our DP-GAN contains a generator that is responsible for generating text and a discriminator that discriminates between the generated text and the real text. The sketch of DP-GAN is shown in Figure 1.\n\n\nOverview\n\nThe generator G \u03b8 is based on a sequence-tosequence structure. Given a sentence as input, the generator is capable of generating long text, which contains multiple sentences of various lengths. To put it formally, given the input sentence x 1:m = (x 1 , x 2 , x 3 , ..., x m ) of m words from \u0393, the vocabulary of words, the model generates the text of T sentences Y 1:T = (y 1 , ..., y t , ..., y T ), where y t from \u039b, the set of candidate sentence. The term y t = (y t,1 , ..., y t,K ) is the t th sentence, where y t,K is the K th word.\n\nThe discriminator D \u03c6 is a language model. The output of the language model, cross entropy, is defined as the reward to train the generator. Our reward consists of two parts, the reward at the sentence level and that at the word level. With the discriminator and the reward function, we train the generator by reinforcement learning. A sketch of training DP-GAN is shown in Algorithm 1. The details are described as follows.\n\nAlgorithm 1 The adversarial reinforcement learning algorithm for training the generator G \u03b8 and the discriminator D \u03c6 . 1: Initialize G \u03b8 , D \u03c6 with random weights \u03b8, \u03c6 2: Pre-train G \u03b8 using MLE on a sequence dataset D = (X, Y ) 3: Generate samples using G \u03b8 for training D \u03c6 4: Pre-train D \u03c6 by Eq. (1) 5: N = number of training iterations 6: M = number of training generator 7: K = number of training discriminator 8: for each i = 1, 2, ..., N do 9:\n\nfor each j = 1, 2, ..., M do 10:\n\nGenerate a sequence Y1:T \u223c G \u03b8 11:\n\nCompute rewards by Eq.\n\n(2) and Eq. (3) 12:\n\nUpdate generator via policy gradient Eq. (5) 13:\n\nSample a sequence Y1:T \u223c D 14:\n\nCompute rewards by Eq.\n\n(2) and Eq. (3) 15:\n\nUpdate generator parameters via Eq. (5) 16:\n\nend for 17:\n\nfor each j = 1, 2, ..., K do 18:\n\nGenerate samples using G \u03b8 19:\n\nTrain discriminator D \u03c6 by Eq. (1) 20: end for 21: end for\n\n\nGenerator\n\nFor the concern of real-world applications, this paper assumes that the output of the model can be long text made up of multiple sentences. In order to generate multiple sentences, we build a standard hierarchical LSTM decoder (Li et al., 2015). The two layers of the LSTM are structured hierarchically. The bottom layer decodes the sentence representation and the top layer decodes each word based on the output of the bottom layer. The attention mechanism is used for word decoding .\n\n\nDiscriminator\n\nMost existing GAN models use a binary classifier as the discriminator. The probability of being true is regarded as the reward (Li et al., 2016;Yu et al., 2017). Different from that, we propose a language-model based discriminator D \u03c6 that builds on a unidirectional LSTM. We use the output of the language model, cross-entropy, as the reward. Specifically, given a sentence y t , the cross-entropy based reward for the k th word is calculated as\nR(y t,k ) = \u2212 log D \u03c6 (y t,k |y t,<k )\nWe maximize the reward of real-world text and minimize the reward of generated text to train the discriminator. The reason of minimizing the reward of generated text is that, we expect the text that is repeatedly generated by the generator can be identified by the discriminator and get lower reward. The motivation of maximizing the reward of real-world data lies in that, we expect not only the uncommon text in the generated data can get high reward, but also low-quality text can be punished to some extend. Considering the real-world text is diverse and fluent, we maximize the reward of real-world text to encourage the discriminator to give high reward for the text that looks like the real-world data. Therefore, such training mechanism avoids the problem of novel but low-quality text getting high reward. The loss function of the discriminator is formulated as follows:\nJ(\u03c6) = \u2212 (E Y \u223cp data [R(Y )] \u2212 E Y \u223cG \u03b8 [R(Y )])(1)\nwhere R(Y ) stands for the averaged reward of Y .\n\n\nReward\n\nOur reward function consists of two parts, the sentence-level reward and the word-level reward, which are illustrated as follows.\n\n\nSentence-Level Reward\n\nFor a sentence y t of K words, the reward at the sentence level is the averaged reward of each word:\nR(y t ) = \u2212 1 K K k=1 log D \u03c6 (y t,k |y t,<k ) (2)\nIn contrast, the reward of the existing classifierbased discriminators (Li et al., 2016;Yu et al., 2017) is calculated as follows:\nR(y t ) = D \u03c6 (true|y t )\nwhere D \u03c6 is a binary classifier judging how likely y t is from the real-world data.\n\nThe major problem of the classifier-based discriminator is that the reward cannot reflect the novelty of text accurately. First, the reward for high-novelty text is easy to saturate, which scarcely distinguishes the difference between novel text. Second, we find that the discriminator can easily achieve very high accuracy on identifying the generated text, which makes most of them get reward around zero. It shows that the classifier still cannot tell the difference between the text with low novelty.\n\nOn the contrary, the analysis of experimental result shows that our proposed discriminator can better distinguish high-novelty text from lownovelty text without the saturation problem. The reward for high-novelty text is high and does not saturate while the reward for low-novelty text is small but discriminative.\n\n\nWord-Level Reward\n\nConsidering that the reward for different words in a sentence y t should be different, we further propose to use the reward at the word level as follows:\nR(y t,k |y t,<k ) = \u2212 log D \u03c6 (y t,k |y t,<k ) (3)\nIt can be found that the classifier-based discriminator only provides reward for the finished sequence. Thus, for a sequence of length T , to evaluate the action-value for a word at the time step t, Monte Carlo Search (MCS) with a roll-out policy G \u03b8 is usually applied to sample the unknown last T \u2212 t tokens (Yu et al., 2017). However, this could be computationally expensive because the time complexity is O(T 2 ). On the contrary, our discriminator can calculate the reward of all words with the time complexity of O(T ), which is more computationally efficient.\n\n\nPolicy Gradient Training\n\nThe loss function of the generator (policy) is to maximize the reward from the start state s 0 to the end state (Sutton et al., 1999):\nJ(\u03b8) = T t=1 E[R t,K |s t\u22121 , \u03b8] = T t=1 y t,1 G \u03b8 (y t,1 |s t\u22121 )Q G \u03b8 D \u03c6 (s t\u22121 , y t,1 )(4)\nwhere R t,K = K k=1 \u03b3 k\u22121 R(y t )R(y t,k ) is the total reward for a complete sentence, including both the sentence-level and the word-level rewards. The term Q G \u03b8 D \u03c6 (s t\u22121 , y t,1 ) is estimated by R t,1 . The term \u03b3 is the discount rate and s t is the initial state.\n\nIn this paper, we use the policy gradient method (Williams, 1992). The gradient of Eq. (4) is approximated as follows:\n\u2207 \u03b8 J(\u03b8) T t=1 K k=1 \u03b3 k\u22121 R t,k \u2207 \u03b8 log G \u03b8 (y t,k |y t,<k )(5)\nwhere R t,k = K i=k \u03b3 i\u22121 R(y t )R(y t,i ) is the total reward starting from step k.\n\nFollowing previous work (Li et al., 2017), we also use teacher forcing (Bengio et al., 2015) to train the generator. In teacher forcing, the decoder receives the real-world text as input at each time step. The loss function of teacher forcing is the same with that of policy gradient training. The only difference is that the text is generated from G \u03b8 in policy gradient training but from the real data in teacher forcing.\n\n\nExperiment\n\nWe evaluate DP-GAN on two real-world natural language generation tasks, review generation and dialogue generation. We first introduce the dataset, the training details, the baselines, and the evaluation metrics. Then, we compare our model with the state-of-the-art models. Finally, we show the experimental results and provide the detailed analysis.\n\n\nDatasets\n\nYelp Review Generation Dataset (Yelp): This dataset is provided by Yelp Dataset Challenge. 3 In our version of review generation, the model should generate a paragraph based on a given sentence. We build a new dataset for this task by splitting the data into two parts. In each review, we take the first sentence as the input text, and the following sentences as the target text. The processed Yelp dataset contains 1,400K, 400K, and 12K pairs for training, validation, and testing, respectively. Amazon Review Generation Dataset (Amazon): This dataset is provided by McAuley and Leskovec (2013). It consists of review information of fine foods from Amazon. Like Yelp, we process this dataset by extracting the first sentence as the source text and the rest as the target text. The processed Amazon dataset contains 400K, 100K, and 12K pairs for training, validation, and testing, respectively.\n\n\nOpenSubtitles Dialogue Dataset (Dialogue):\n\nThis dataset 4 is used for dialogue generation. Following previous work, we treat each turn in the dataset as the target text and the two previous sentences as the source text. We remove the pairs  Table 1: Performance of the DP-GAN and three baselines on review generation and dialogue generation tasks. Higher is better. DP-GAN(S), DP-GAN(W), and DP-GAN(SW) represent DP-GAN with only sentencelevel reward, only word-level reward, and combined reward, respectively. Token represents the number of generated words. Dist-1, Dist-2, Dist-3, and Dist-S are respectively the number of distinct unigrams, bigrams, trigrms, and sentences in the generated text. For example, 1.2K in Dist-1 means 1200 distinct unigrams. whose response is shorter than 5 words. We randomly sample 1,800K, 500K, and 12K turns for training, validation, and testing, respectively.\n\n\nBaselines\n\nWe compare the proposed DP-GAN with the following baseline models: MLE: The generator is a sequence-to-sequence model. The generator is trained with traditional MLE. PG-BLEU: The generator is a sequence-tosequence model. It is trained by policy gradient with the BLEU score of the generated text as the reward (Bahdanau et al., 2017). The advantage is that this model can directly optimize the taskspecific score: BLEU. SeqGAN: Sequence GAN (Yu et al., 2017) uses a binary classifier as the discriminator. Since it is originally for unconditional generation, for a fair comparison, we expand it to the version of conditional generation. We re-implement the generator by replacing a language model with a sequenceto-sequence model.  Table 2: Results of human evaluation on the three datasets. The score represents the averaged ranking of each model and lower is better. All represents the ranking given by annotators based on a comprehensive consideration. It can be seen that DP-GAN results in the largest improvement in terms of diversity and relevance while slightly reducing fluency.\n\n\nTraining Details\n\nFor review generation, we set the number of generated sentences to 6 with the maximum length of 40 words for each generated sentence. Based on the performance on the validation set, we set the hidden size to 256, embedding size to 128, vocabulary size to 50K, and batch size to 64 for the proposed model and the baselines. We use the Adagrad (Duchi et al., 2011) optimizer with the initial learning rate 0.1. In adversarial training, the step for training the generator is 1K, the step for training the discriminator is 5K. Both the generator and the discriminator are pre-trained for 10 epochs before adversarial learning. In particular, for PG-BLEU and SeqGAN, before reinforcement learning or adversarial learning, we pre-train the sequence-to-sequence model for 10 epochs like DP-GAN. For dialogue generation, the settings are the same with review generation, except that we set the number of generated sentences to 1 with the maximum length of 40 words because there is only one sentence in the response.\n\n\nExperimental Results\n\nWe conduct two kinds of evaluations in this work, automatic evaluation and human evaluation. The details of evaluation results are shown as follows.\n\n\nAutomatic Evaluation\n\nWe evaluate the proposed model in terms of several metrics that can reflect the diversity. The results are shown in Table 1. Token represents the total number of generated words. Dist-1, Dist-2, Dist-3, and Dist-S are respectively the number of distinct unigrams, bigrams, trigrms, and sentences. DP-GAN(S), DP-GAN(W), and DP-GAN(SW) represent DP-GAN with only sentence-level reward, only word-level reward, and combined reward, respectively. From the results, it is obvious that the proposed model substantially outperforms the existing models. PG-BLEU achieves slightly weaker results compared with MLE. The reason is that PG-BLEU uses BLEU score as the reward for reinforcement learning. However, the BLEU score is low for most of the generated text. The low reward makes it hard to learn from the real data. SeqGAN does not achieve better results, which suggests that the classifier-based discriminator fails to encourage the generator to produce diverse text.\n\nIn terms of the total number of generated words, DP-GAN(S) achieves better results than DP-GAN(W). Since the sentence-level reward reflects the novelty of the whole sentence, it gives repeated and short text low reward while novel and longer text high reward. Thus, the generator is encouraged to generate novel text. In terms of the number of distinct n-grams, DP-GAN(W) achieves better results than DP-GAN(S). It is because the wordlevel reward gives each word more precise score and novel n-grams could be better encouraged. As we can see, DP-GAN(SW), which combines the advantages of sentence-level and word-level rewards, generates not only more diverse n-grams than DP-GAN(S) but also longer text than DP-GAN(W). Since combining the word-level and sentence-level rewards achieves better results than using just one of them, we focus more on the combined reward in the following parts.\n\nIn review generation and dialogue generation tasks, it is a widely debated question how well the BLEU score against a single reference can reflect the quality of the generated text (Liu et al., 2016). Thus, although the proposed model achieves better BLEU scores compared with baselines, we omit the detailed comparisons in terms of BLEU for space.\n\n\nHuman Evaluation\n\nWe conduct a human evaluation on the test set. For all tasks, we randomly extract 200 samples from the test sets. Each item contains the input text and the text generated by the different systems. The items are distributed to three anno- True Data: Lyons Roofing needs to spend less effort on charming their customers and concentrate on their lack of business ethics and skill.\n\nTrue Data: Now after a revamp, redecoration, renaming and it becomes another southeast asian restaurant. Figure 2: Distribution of rewards between SeqGAN and DP-GAN. The upper two sentences are sampled from the real-world data and the lower two sentences are sampled from the generated data. It is important to note that the sentence-level reward of DP-GAN is averaged word-level reward and a long sentence does not indicate a high score. As we can see, the reward distribution of SeqGAN saturates and cannot distinguish the novelty of the text accurately. In contrast, DP-GAN has a strong ability of resisting reward saturation and can give more precise reward for text in terms of novelty. : Cosine similarity between the real-world data distribution and the generated data distributions of various models. For example, the first column represents the cosine similarity on top 500 words with the highest frequencies in real-world data. As we can see, the generated data distribution of DP-GAN is closer to the real-world data distribution, especially considering the words of low frequency.\n\ntators who have no knowledge about which system the text is from. Following the work of Li et al. (2017), we require them to rank the generated text considering relevance, diversity, and fluency. It is important to note that all the annotators have linguistic background. Relevance means that how likely the generated text is related to the input text. Diversity means that how much the generated text provides specific information, rather than \"dull\" and repeated information. Fluency means that how likely the generated text is produced by human. All represents the ranking given by annotators based on a comprehensive consideration of all human evaluation metrics. The results of human evaluation are shown in Table 2. It needs to be mentioned that in the special case that several pieces of generated text are exactly the same, they are given the same ranking. The inter-annotator agreement is satisfactory considering the difficulty in the human evaluation. The Pearson's correlation coefficient is 0.76 on diversity, 0.59 on fluency and 0.27 on relevance, with p < 0.0001. The p-value is all below 0.001, indicating high correlation and agreement. Table 2 shows that DP-GAN brings the largest improvement in terms of diversity and relevance while scarcely reducing fluency. First, DP-GAN significantly outperforms baselines in term of diversity. Second, DP-GAN achieves the best performance in terms of relevance. Diverse text generated by DP-GAN brings more specific expressions, increasing the performance of relevance. For example, given input sentence\"food is good\", the relevance of \"love it, beef is the best\" is obviously better than that of \"love it\". Third, the fluency of DP-GAN is slightly worse than that of baselines. It is mainly due to the fact that the text generated by DP-GAN is much longer. From Table 1, we can see that the number of words generated by DP-GAN is almost three times than that of baselines in the Yelp dataset.\n\n\nAnalysis: Why It Works\n\nIn this section, we provide detailed analysis to see why our proposed method works better.\n\nIn Figure 2, we demonstrate the reward distributions of our model and SeqGAN. It can be seen that the reward of SeqGAN cannot reflect the novelty of text accurately. First, when the novelty of text is relatively high, the reward given by Seq-GAN saturates and cannot tell the difference between high-novelty text. Even with different degrees of novelty, the two high-novelty sentences sampled from the true data in the figure get nearly Input: One of my favorite places to eat. MLE: Service is great. PG-BLEU: Service is always good. SeqGAN: Love the chicken and waffles. Service is always great. DP-GAN: Love the fact that they have a large selection of food. Service is always great and the food is always fresh. I've been to this place a few times and have never been disappointed. Input: Excellent service, clean, nice ambience. MLE: Food is very good. PG-BLEU: Food is very good. SeqGAN: Food is very good. DP-GAN: I had the chicken and beef and it was delicious. Food is good and the service is great. I would recommend this place to anyone who wants to eat here. Input: I brought friends -seven of them -who'd never been to a Bar Louie before. MLE: Food was mediocre, but the service was great. I will not be returning. PG-BLEU: Food was terrible. Wouldn't recommend this place to anyone. SeqGAN: Food was just okay. I wouldn't recommend this place. DP-GAN: Ordered a chicken salad with a side of fries, and it was just ok. Food was good, but the service was slow. I had to ask for a refill on my drink, and the bartender was rude. I wouldn't recommend this place to anyone. Input: I don't think it's too spicy, so i add a little hot curry spice to the sauce. MLE: It's great for cooking. I have to say, I'm not sure if it is the same. PG-BLEU: Love it! SeqGAN: Love it! Love this stuff. I have used it for years and it is very good. DP-GAN:I've tried many different brands of hot sauces but this one is the best. I've also used it in soups and stews. I also like that it is organic and has a great flavor. Table 3: Examples generated by DP-GAN and the state-of-the-art models on the Yelp dataset. It can be found that the text generated by baselines is more generic and repeated, while our model generates text with more specific details and higher diversity. the same reward in SeqGAN. Second, most generated text receives reward around zero because of the high accuracy of classifier. It is hard for such reward to distinguish the difference between lownovelty text. For example, as shown in the figure, \"Both had the brisket and it was delicious\" is much more informative than \"Love it! \". The discriminator of SeqGAN gives them practically the same reward, while the proposed discriminator can better distinguish the two sentences in terms of novelty. In fact, the classifier in SeqGAN trained for 10 epochs can reach very high accuracy, that is, 98.35% and 99.63% for Yelp and Amazon, respectively. If the accuracy of classifier is too high, the classifier cannot give reasonable reward to the generator for generating real and diverse text .\n\nIn contrast, the language-model based reward given by DP-GAN better reflect the novelty of the text. The novel text is given high reward that does not saturate. The generated data, which can be less novel, is given relatively low but nonzero reward that can encourage the generator to generate diverse expressions. The refined reward leads to more efficient training, thus resulting in better performance.\n\nWe also compare the cosine similarity between the real-world data distribution and the generated data distributions of various models. Figure 3 shows the results. We calculate the cosine distance between two vectors, where each element is the frequency of a word indexed by its rank in realworld data. For example, the first element in the vector means the frequency of the word that ranks first in real-world data. The word frequency vector is divided into 4 vectors to show the similarity of words of different frequencies. The distribution of words are more similar when they occur more frequently in real-world data. As DP-GAN promotes diversity, words of low frequency in realworld data are better learned and the similarity is much better than that of MLE. In all, the generated data distribution of DP-GAN is closer to the realworld data distribution in all intervals, especially considering the words of low frequency. Table 3 presents the examples generated by different models on the Yelp dataset. It can be found that the text generated by MLE is more generic and repeated, while PG-BLEU and SeqGAN do not perform obviously better than MLE. Moreover, it can be clearly seen that our model generates text with more specific details and higher diversity.\n\n\nConclusions\n\nIn this paper, we propose a new model, called DP-GAN, to promote the diversity of the generated text. DP-GAN assigns low reward for repeated text and high reward for novel and fluent text, encouraging the generator to produce novel and diverse text. We evaluate DP-GAN on two tasks and the findings are concluded as follows: First, the proposed method substantially outperforms the baseline methods in automatic and human evaluations. It shows that DP-GAN is capable of producing more diverse and informative text. Second, the proposed discriminator can better distinguish novel text from repeated text with the saturation problem compared without traditional classifierbased discriminators. Third, with the improvement of diversity, the generated data distribution of DP-GAN is closer to the real-world data distribution compared with that of MLE.\n\nFigure 1 :\n1incorporates the policy gradient into the model by treating the procedure of generation as a stochastic policy in reinforcement learning. Ranzato et al. (2016) Illustration of DP-GAN. Lower: The generator is trained by policy gradient where the reward is provided by the discriminator. Upper: The discriminator is based on the language model trained over the real text and the generated text.\n\n\nBoth had the brisket and it was delicious.\n\nFigure 3\n3Figure 3: Cosine similarity between the real-world data distribution and the generated data distributions of various models. For example, the first column represents the cosine similarity on top 500 words with the highest frequencies in real-world data. As we can see, the generated data distribution of DP-GAN is closer to the real-world data distribution, especially considering the words of low frequency.\nThe code is available at https://github.com/ lancopku/DPGAN\nFor example, the frequency ratios of \"the\", \"and\", \"was\" are 4.2%, 3.2%, 1.5% in real data, and they go up to 7.1%, 4.6%, 5.3% in the MLE generated data on our review generation task.\nAcknowledgementsThis work was supported in part by National Natural Science Foundation of China (No. 61673028). We thank Wei Li and Bingzhen Wei for providing the thoughtful suggestions. Xu Sun is the corresponding author of this paper.\nWasserstein generative adversarial networks. Mart\u00edn Arjovsky, Soumith Chintala, L\u00e9on Bottou, Mart\u00edn Arjovsky, Soumith Chintala, and L\u00e9on Bottou. 2017. Wasserstein generative adversarial networks. In ICML 2017, pages 214-223.\n\n. Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron CDzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron C.\n\nAn actor-critic algorithm for sequence prediction. Yoshua Courville, Bengio, Courville, and Yoshua Bengio. 2017. An actor-critic algorithm for sequence prediction. In ICLR 2017.\n\nNeural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learning to align and translate. In ICLR 2014.\n\nScheduled sampling for sequence prediction with recurrent neural networks. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer, NIPS 2015. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for se- quence prediction with recurrent neural networks. In NIPS 2015, pages 1171-1179.\n\nBEGAN: boundary equilibrium generative adversarial networks. David Berthelot, Tom Schumm, Luke Metz, abs/1703.10717CoRRDavid Berthelot, Tom Schumm, and Luke Metz. 2017. BEGAN: boundary equilibrium generative adversar- ial networks. CoRR, abs/1703.10717.\n\nInfogan: Interpretable representation learning by information maximizing generative adversarial nets. Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel, NIPS 2016. Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. 2016. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In NIPS 2016, pages 2172-2180.\n\nLearning phrase representations using RNN encoder-decoder for statistical machine translation. Kyunghyun Cho, Bart Van Merrienboer, Dzmitry Aglar G\u00fcl\u00e7ehre, Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua BengioKyunghyun Cho, Bart van Merrienboer, \u00c7 aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Hol- ger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In EMNLP 2014, pages 1724-1734.\n\nDeep generative image models using a laplacian pyramid of adversarial networks. Emily L Denton, Soumith Chintala, Arthur Szlam, Rob Fergus, NIPS 2015. Emily L. Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus. 2015. Deep generative image models using a laplacian pyramid of adversarial networks. In NIPS 2015, pages 1486-1494.\n\nAdaptive subgradient methods for online learning and stochastic optimization. John C Duchi, Elad Hazan, Yoram Singer, Journal of Machine Learning Research. 12John C. Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121-2159.\n\nGenerative adversarial nets. Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C Courville, Yoshua Bengio, NIPS 2014. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. Gen- erative adversarial nets. In NIPS 2014, pages 2672- 2680.\n\nImproved training of wasserstein gans. Ishaan Gulrajani, Faruk Ahmed, Mart\u00edn Arjovsky, Vincent Dumoulin, Aaron C Courville, NIPS 2017. Ishaan Gulrajani, Faruk Ahmed, Mart\u00edn Arjovsky, Vin- cent Dumoulin, and Aaron C. Courville. 2017. Im- proved training of wasserstein gans. In NIPS 2017, pages 5769-5779.\n\nGenerating sentences by editing prototypes. Kelvin Guu, B Tatsunori, Yonatan Hashimoto, Percy Oren, Liang, abs/1709.08878CoRRKelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. 2017. Generating sentences by editing prototypes. CoRR, abs/1709.08878.\n\nA diversity-promoting objective function for neural conversation models. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan, NAACL 2016. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A diversity-promoting ob- jective function for neural conversation models. In NAACL 2016, pages 110-119.\n\nA hierarchical neural autoencoder for paragraphs and documents. Jiwei Li, Minh-Thang Luong, Dan Jurafsky, ACL 2015. Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. 2015. A hierarchical neural autoencoder for paragraphs and documents. In ACL 2015, pages 1106-1115.\n\nAdversarial learning for neural dialogue generation. Jiwei Li, Will Monroe, Tianlin Shi, S\u00e9bastien Jean, Alan Ritter, Dan Jurafsky, Jiwei Li, Will Monroe, Tianlin Shi, S\u00e9bastien Jean, Alan Ritter, and Dan Jurafsky. 2017. Adversarial learning for neural dialogue generation. In EMNLP 2017, pages 2157-2169.\n\nGlobal encoding for abstractive summarization. Junyang Lin, Xu Sun, Shuming Ma, Qi Su, abs/1805.03989CoRRJunyang Lin, Xu Sun, Shuming Ma, and Qi Su. 2018. Global encoding for abstractive summariza- tion. CoRR, abs/1805.03989.\n\nHow NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. Chia-Wei Liu, Ryan Lowe, Iulian Serban, Michael Noseworthy, Laurent Charlin, Joelle Pineau, EMNLP 2016. Chia-Wei Liu, Ryan Lowe, Iulian Serban, Michael Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation met- rics for dialogue response generation. In EMNLP 2016, pages 2122-2132.\n\nTable-to-text generation by structure-aware seq2seq learning. Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang, Zhifang Sui, abs/1711.09724CoRRTianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang, and Zhifang Sui. 2017. Table-to-text genera- tion by structure-aware seq2seq learning. CoRR, abs/1711.09724.\n\nAn auto-encoder matching model for learning utterance-level semantic dependency in dialogue generation. Liangchen Luo, Jingjing Xu, Junyang Lin, Qi Zeng, Xu Sun, In EMNLP. Liangchen Luo, Jingjing Xu, Junyang Lin, Qi Zeng, and Xu Sun. 2018. An auto-encoder matching model for learning utterance-level semantic depen- dency in dialogue generation. In EMNLP, 2018.\n\nEffective approaches to attention-based neural machine translation. Thang Luong, Hieu Pham, Christopher D Manning, EMNLP 2015. Thang Luong, Hieu Pham, and Christopher D. Man- ning. 2015. Effective approaches to attention-based neural machine translation. In EMNLP 2015, pages 1412-1421.\n\nAutoencoder as assistant supervisor: Improving text representation for chinese social media text summarization. Shuming Ma, Xu Sun, Junyang Lin, Houfeng Wang, abs/1805.04869CoRRShuming Ma, Xu Sun, Junyang Lin, and Houfeng Wang. 2018a. Autoencoder as assistant supervisor: Improving text representation for chinese social me- dia text summarization. CoRR, abs/1805.04869.\n\nBag-of-words as target for neural machine translation. Shuming Ma, Xu Sun, Yizhong Wang, Junyang Lin, abs/1805.04871CoRRShuming Ma, Xu Sun, Yizhong Wang, and Junyang Lin. 2018b. Bag-of-words as target for neural ma- chine translation. CoRR, abs/1805.04871.\n\nFrom amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. Julian John Mcauley , Jure Leskovec, WWW 2013. Julian John McAuley and Jure Leskovec. 2013. From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. In WWW 2013, pages 897-908.\n\nf-gan: Training generative neural samplers using variational divergence minimization. Sebastian Nowozin, Botond Cseke, Ryota Tomioka, NIPS 2016. Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. 2016. f-gan: Training generative neural samplers using variational divergence minimization. In NIPS 2016, pages 271-279.\n\nUnsupervised representation learning with deep convolutional generative adversarial networks. Alec Radford, Luke Metz, Soumith Chintala, abs/1511.06434CoRRAlec Radford, Luke Metz, and Soumith Chintala. 2015. Unsupervised representation learning with deep convolutional generative adversarial networks. CoRR, abs/1511.06434.\n\nSequence level training with recurrent neural networks. Aurelio Marc, Sumit Ranzato, Michael Chopra, Wojciech Auli, Zaremba, Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence level train- ing with recurrent neural networks. In ICLR 2016.\n\nImproved techniques for training gans. Tim Salimans, Ian J Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, NIPS 2016. Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. 2016. Improved techniques for training gans. In NIPS 2016, pages 2226-2234.\n\nGenerating high-quality and informative conversation responses with sequence-to-sequence models. Yuanlong Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope, Ray Kurzweil, Yuanlong Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope, and Ray Kurzweil. 2017. Generating high-quality and informative conversa- tion responses with sequence-to-sequence models. In EMNLP 2017, pages 2210-2219.\n\nSequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, V Quoc, Le, NIPS 2014. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural net- works. In NIPS 2014, pages 3104-3112.\n\nPolicy gradient methods for reinforcement learning with function approximation. Richard S Sutton, David A Mcallester, P Satinder, Yishay Singh, Mansour, NIPS 1999. Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. 1999. Policy gradient methods for reinforcement learning with function approximation. In NIPS 1999, pages 1057-1063.\n\nSimple statistical gradientfollowing algorithms for connectionist reinforcement learning. Ronald J Williams, Machine Learning. 8Ronald J. Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning. Machine Learning, 8:229-256.\n\nUnpaired sentiment-to-sentiment translation: A cycled reinforcement learning approach. Jingjing Xu, Xu Sun, Qi Zeng, Xuancheng Ren, Xiaodong Zhang, Houfeng Wang, Wenjie Li, ACL. Jingjing Xu, Xu Sun, Qi Zeng, Xuancheng Ren, Xi- aodong Zhang, Houfeng Wang, and Wenjie Li. 2018a. Unpaired sentiment-to-sentiment transla- tion: A cycled reinforcement learning approach. In ACL, 2018.\n\nA skeleton-based model for promoting coherence among sentences in narrative story generation. Jingjing Xu, Yi Zhang, Qi Zeng, Xuancheng Ren, Xiaoyan Cai, Xu Sun, In EMNLP. Jingjing Xu, Yi Zhang, Qi Zeng, Xuancheng Ren, Xi- aoyan Cai, and Xu Sun. 2018b. A skeleton-based model for promoting coherence among sentences in narrative story generation. In EMNLP, 2018.\n\nSeqgan: Sequence generative adversarial nets with policy gradient. Lantao Yu, Weinan Zhang, Jun Wang, Yong Yu, AAAI 2017. Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence generative adversarial nets with policy gradient. In AAAI 2017, pages 2852- 2858.\n\nEnergy-based generative adversarial network. Jake Junbo, Micha\u00ebl Zhao, Yann Mathieu, Lecun, Junbo Jake Zhao, Micha\u00ebl Mathieu, and Yann LeCun. 2017. Energy-based generative adversarial network. In ICLR 2017.\n", "annotations": {"author": "[{\"end\":349,\"start\":239},{\"end\":440,\"start\":350},{\"end\":551,\"start\":441},{\"end\":652,\"start\":552}]", "publisher": "[{\"end\":152,\"start\":111},{\"end\":980,\"start\":939}]", "author_last_name": "[{\"end\":250,\"start\":248},{\"end\":363,\"start\":360},{\"end\":452,\"start\":449},{\"end\":558,\"start\":555}]", "author_first_name": "[{\"end\":247,\"start\":239},{\"end\":359,\"start\":350},{\"end\":448,\"start\":441},{\"end\":554,\"start\":552}]", "author_affiliation": "[{\"end\":348,\"start\":274},{\"end\":439,\"start\":365},{\"end\":550,\"start\":476},{\"end\":651,\"start\":577}]", "title": "[{\"end\":110,\"start\":1},{\"end\":762,\"start\":653}]", "venue": "[{\"end\":850,\"start\":764}]", "abstract": "[{\"end\":1815,\"start\":1019}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2021,\"start\":2003},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2060,\"start\":2042},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2104,\"start\":2086},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2215,\"start\":2191},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2653,\"start\":2636},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4021,\"start\":4004},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5215,\"start\":5193},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7090,\"start\":7066},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7201,\"start\":7183},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7314,\"start\":7298},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7332,\"start\":7314},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7349,\"start\":7332},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7367,\"start\":7349},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7503,\"start\":7486},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7675,\"start\":7658},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7692,\"start\":7675},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7710,\"start\":7692},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7816,\"start\":7791},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7836,\"start\":7816},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7851,\"start\":7836},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7923,\"start\":7901},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7946,\"start\":7923},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7969,\"start\":7946},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8194,\"start\":8171},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8217,\"start\":8194},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8235,\"start\":8217},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8258,\"start\":8235},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8619,\"start\":8602},{\"end\":8734,\"start\":8712},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11150,\"start\":11133},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11553,\"start\":11536},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11569,\"start\":11553},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13283,\"start\":13266},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13299,\"start\":13283},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14812,\"start\":14795},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15213,\"start\":15192},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":15649,\"start\":15633},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15895,\"start\":15878},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15946,\"start\":15925},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17249,\"start\":17222},{\"end\":18795,\"start\":18772},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18919,\"start\":18903},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19931,\"start\":19911},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22833,\"start\":22815},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24580,\"start\":24564}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32543,\"start\":32138},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32588,\"start\":32544},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33008,\"start\":32589}]", "paragraph": "[{\"end\":2422,\"start\":1831},{\"end\":2935,\"start\":2424},{\"end\":3031,\"start\":2937},{\"end\":3897,\"start\":3033},{\"end\":5410,\"start\":3899},{\"end\":5925,\"start\":5412},{\"end\":5967,\"start\":5927},{\"end\":6128,\"start\":5969},{\"end\":6275,\"start\":6130},{\"end\":6459,\"start\":6277},{\"end\":7711,\"start\":6476},{\"end\":8783,\"start\":7713},{\"end\":9033,\"start\":8811},{\"end\":9586,\"start\":9046},{\"end\":10012,\"start\":9588},{\"end\":10466,\"start\":10014},{\"end\":10500,\"start\":10468},{\"end\":10536,\"start\":10502},{\"end\":10560,\"start\":10538},{\"end\":10581,\"start\":10562},{\"end\":10631,\"start\":10583},{\"end\":10663,\"start\":10633},{\"end\":10687,\"start\":10665},{\"end\":10708,\"start\":10689},{\"end\":10753,\"start\":10710},{\"end\":10766,\"start\":10755},{\"end\":10800,\"start\":10768},{\"end\":10832,\"start\":10802},{\"end\":10892,\"start\":10834},{\"end\":11391,\"start\":10906},{\"end\":11855,\"start\":11409},{\"end\":12774,\"start\":11895},{\"end\":12877,\"start\":12828},{\"end\":13017,\"start\":12888},{\"end\":13143,\"start\":13043},{\"end\":13325,\"start\":13195},{\"end\":13436,\"start\":13352},{\"end\":13942,\"start\":13438},{\"end\":14258,\"start\":13944},{\"end\":14433,\"start\":14280},{\"end\":15051,\"start\":14485},{\"end\":15214,\"start\":15080},{\"end\":15582,\"start\":15311},{\"end\":15702,\"start\":15584},{\"end\":15852,\"start\":15768},{\"end\":16277,\"start\":15854},{\"end\":16641,\"start\":16292},{\"end\":17548,\"start\":16654},{\"end\":18448,\"start\":17595},{\"end\":19548,\"start\":18462},{\"end\":20578,\"start\":19569},{\"end\":20751,\"start\":20603},{\"end\":21740,\"start\":20776},{\"end\":22632,\"start\":21742},{\"end\":22982,\"start\":22634},{\"end\":23380,\"start\":23003},{\"end\":24474,\"start\":23382},{\"end\":26427,\"start\":24476},{\"end\":26544,\"start\":26454},{\"end\":29601,\"start\":26546},{\"end\":30008,\"start\":29603},{\"end\":31273,\"start\":30010},{\"end\":32137,\"start\":31289}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11894,\"start\":11856},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12827,\"start\":12775},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13194,\"start\":13144},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13351,\"start\":13326},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14484,\"start\":14434},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15310,\"start\":15215},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15767,\"start\":15703}]", "table_ref": "[{\"end\":17800,\"start\":17793},{\"end\":19201,\"start\":19194},{\"end\":20899,\"start\":20892},{\"end\":25196,\"start\":25189},{\"end\":25637,\"start\":25630},{\"end\":26304,\"start\":26297},{\"end\":28567,\"start\":28560},{\"end\":30944,\"start\":30937}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1829,\"start\":1817},{\"attributes\":{\"n\":\"2\"},\"end\":6474,\"start\":6462},{\"attributes\":{\"n\":\"3\"},\"end\":8809,\"start\":8786},{\"attributes\":{\"n\":\"3.1\"},\"end\":9044,\"start\":9036},{\"attributes\":{\"n\":\"3.2\"},\"end\":10904,\"start\":10895},{\"attributes\":{\"n\":\"3.3\"},\"end\":11407,\"start\":11394},{\"attributes\":{\"n\":\"3.4\"},\"end\":12886,\"start\":12880},{\"attributes\":{\"n\":\"3.4.1\"},\"end\":13041,\"start\":13020},{\"attributes\":{\"n\":\"3.4.2\"},\"end\":14278,\"start\":14261},{\"attributes\":{\"n\":\"3.5\"},\"end\":15078,\"start\":15054},{\"attributes\":{\"n\":\"4\"},\"end\":16290,\"start\":16280},{\"attributes\":{\"n\":\"4.1\"},\"end\":16652,\"start\":16644},{\"end\":17593,\"start\":17551},{\"attributes\":{\"n\":\"4.2\"},\"end\":18460,\"start\":18451},{\"attributes\":{\"n\":\"4.3\"},\"end\":19567,\"start\":19551},{\"attributes\":{\"n\":\"4.4\"},\"end\":20601,\"start\":20581},{\"attributes\":{\"n\":\"4.4.1\"},\"end\":20774,\"start\":20754},{\"attributes\":{\"n\":\"4.4.2\"},\"end\":23001,\"start\":22985},{\"attributes\":{\"n\":\"4.4.3\"},\"end\":26452,\"start\":26430},{\"attributes\":{\"n\":\"5\"},\"end\":31287,\"start\":31276},{\"end\":32149,\"start\":32139},{\"end\":32598,\"start\":32590}]", "table": null, "figure_caption": "[{\"end\":32543,\"start\":32151},{\"end\":32588,\"start\":32546},{\"end\":33008,\"start\":32600}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9032,\"start\":9024},{\"end\":23495,\"start\":23487},{\"end\":26557,\"start\":26549},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30153,\"start\":30145}]", "bib_author_first_name": "[{\"end\":33541,\"start\":33535},{\"end\":33559,\"start\":33552},{\"end\":33574,\"start\":33570},{\"end\":33725,\"start\":33718},{\"end\":33744,\"start\":33736},{\"end\":33759,\"start\":33753},{\"end\":33771,\"start\":33764},{\"end\":33783,\"start\":33779},{\"end\":33796,\"start\":33790},{\"end\":33966,\"start\":33960},{\"end\":34166,\"start\":34159},{\"end\":34186,\"start\":34177},{\"end\":34198,\"start\":34192},{\"end\":34432,\"start\":34428},{\"end\":34446,\"start\":34441},{\"end\":34463,\"start\":34456},{\"end\":34476,\"start\":34472},{\"end\":34740,\"start\":34735},{\"end\":34755,\"start\":34752},{\"end\":34768,\"start\":34764},{\"end\":35033,\"start\":35031},{\"end\":35043,\"start\":35040},{\"end\":35054,\"start\":35050},{\"end\":35070,\"start\":35066},{\"end\":35085,\"start\":35081},{\"end\":35103,\"start\":35097},{\"end\":35452,\"start\":35443},{\"end\":35487,\"start\":35480},{\"end\":35908,\"start\":35903},{\"end\":35910,\"start\":35909},{\"end\":35926,\"start\":35919},{\"end\":35943,\"start\":35937},{\"end\":35954,\"start\":35951},{\"end\":36239,\"start\":36235},{\"end\":36241,\"start\":36240},{\"end\":36253,\"start\":36249},{\"end\":36266,\"start\":36261},{\"end\":36529,\"start\":36526},{\"end\":36531,\"start\":36530},{\"end\":36548,\"start\":36544},{\"end\":36569,\"start\":36564},{\"end\":36581,\"start\":36577},{\"end\":36591,\"start\":36586},{\"end\":36613,\"start\":36606},{\"end\":36626,\"start\":36621},{\"end\":36628,\"start\":36627},{\"end\":36646,\"start\":36640},{\"end\":36916,\"start\":36910},{\"end\":36933,\"start\":36928},{\"end\":36947,\"start\":36941},{\"end\":36965,\"start\":36958},{\"end\":36981,\"start\":36976},{\"end\":36983,\"start\":36982},{\"end\":37227,\"start\":37221},{\"end\":37234,\"start\":37233},{\"end\":37253,\"start\":37246},{\"end\":37270,\"start\":37265},{\"end\":37520,\"start\":37515},{\"end\":37531,\"start\":37525},{\"end\":37545,\"start\":37540},{\"end\":37564,\"start\":37556},{\"end\":37574,\"start\":37570},{\"end\":37846,\"start\":37841},{\"end\":37861,\"start\":37851},{\"end\":37872,\"start\":37869},{\"end\":38098,\"start\":38093},{\"end\":38107,\"start\":38103},{\"end\":38123,\"start\":38116},{\"end\":38138,\"start\":38129},{\"end\":38149,\"start\":38145},{\"end\":38161,\"start\":38158},{\"end\":38401,\"start\":38394},{\"end\":38409,\"start\":38407},{\"end\":38422,\"start\":38415},{\"end\":38429,\"start\":38427},{\"end\":38712,\"start\":38704},{\"end\":38722,\"start\":38718},{\"end\":38735,\"start\":38729},{\"end\":38751,\"start\":38744},{\"end\":38771,\"start\":38764},{\"end\":38787,\"start\":38781},{\"end\":39143,\"start\":39137},{\"end\":39156,\"start\":39149},{\"end\":39166,\"start\":39163},{\"end\":39178,\"start\":39172},{\"end\":39193,\"start\":39186},{\"end\":39489,\"start\":39480},{\"end\":39503,\"start\":39495},{\"end\":39515,\"start\":39508},{\"end\":39523,\"start\":39521},{\"end\":39532,\"start\":39530},{\"end\":39812,\"start\":39807},{\"end\":39824,\"start\":39820},{\"end\":39842,\"start\":39831},{\"end\":39844,\"start\":39843},{\"end\":40146,\"start\":40139},{\"end\":40153,\"start\":40151},{\"end\":40166,\"start\":40159},{\"end\":40179,\"start\":40172},{\"end\":40461,\"start\":40454},{\"end\":40468,\"start\":40466},{\"end\":40481,\"start\":40474},{\"end\":40495,\"start\":40488},{\"end\":40772,\"start\":40753},{\"end\":40779,\"start\":40775},{\"end\":41065,\"start\":41056},{\"end\":41081,\"start\":41075},{\"end\":41094,\"start\":41089},{\"end\":41387,\"start\":41383},{\"end\":41401,\"start\":41397},{\"end\":41415,\"start\":41408},{\"end\":41677,\"start\":41670},{\"end\":41689,\"start\":41684},{\"end\":41706,\"start\":41699},{\"end\":41723,\"start\":41715},{\"end\":41932,\"start\":41929},{\"end\":41946,\"start\":41943},{\"end\":41948,\"start\":41947},{\"end\":41969,\"start\":41961},{\"end\":41984,\"start\":41979},{\"end\":41997,\"start\":41993},{\"end\":42009,\"start\":42007},{\"end\":42301,\"start\":42293},{\"end\":42315,\"start\":42308},{\"end\":42328,\"start\":42323},{\"end\":42340,\"start\":42336},{\"end\":42354,\"start\":42349},{\"end\":42366,\"start\":42363},{\"end\":42659,\"start\":42655},{\"end\":42676,\"start\":42671},{\"end\":42687,\"start\":42686},{\"end\":42935,\"start\":42928},{\"end\":42937,\"start\":42936},{\"end\":42951,\"start\":42946},{\"end\":42953,\"start\":42952},{\"end\":42967,\"start\":42966},{\"end\":42984,\"start\":42978},{\"end\":43305,\"start\":43299},{\"end\":43307,\"start\":43306},{\"end\":43582,\"start\":43574},{\"end\":43589,\"start\":43587},{\"end\":43597,\"start\":43595},{\"end\":43613,\"start\":43604},{\"end\":43627,\"start\":43619},{\"end\":43642,\"start\":43635},{\"end\":43655,\"start\":43649},{\"end\":43970,\"start\":43962},{\"end\":43977,\"start\":43975},{\"end\":43987,\"start\":43985},{\"end\":44003,\"start\":43994},{\"end\":44016,\"start\":44009},{\"end\":44024,\"start\":44022},{\"end\":44305,\"start\":44299},{\"end\":44316,\"start\":44310},{\"end\":44327,\"start\":44324},{\"end\":44338,\"start\":44334},{\"end\":44557,\"start\":44553},{\"end\":44572,\"start\":44565},{\"end\":44583,\"start\":44579}]", "bib_author_last_name": "[{\"end\":33550,\"start\":33542},{\"end\":33568,\"start\":33560},{\"end\":33581,\"start\":33575},{\"end\":33734,\"start\":33726},{\"end\":33751,\"start\":33745},{\"end\":33762,\"start\":33760},{\"end\":33777,\"start\":33772},{\"end\":33788,\"start\":33784},{\"end\":33803,\"start\":33797},{\"end\":33976,\"start\":33967},{\"end\":33984,\"start\":33978},{\"end\":34175,\"start\":34167},{\"end\":34190,\"start\":34187},{\"end\":34205,\"start\":34199},{\"end\":34439,\"start\":34433},{\"end\":34454,\"start\":34447},{\"end\":34470,\"start\":34464},{\"end\":34484,\"start\":34477},{\"end\":34750,\"start\":34741},{\"end\":34762,\"start\":34756},{\"end\":34773,\"start\":34769},{\"end\":35038,\"start\":35034},{\"end\":35048,\"start\":35044},{\"end\":35064,\"start\":35055},{\"end\":35079,\"start\":35071},{\"end\":35095,\"start\":35086},{\"end\":35110,\"start\":35104},{\"end\":35456,\"start\":35453},{\"end\":35478,\"start\":35458},{\"end\":35502,\"start\":35488},{\"end\":35512,\"start\":35504},{\"end\":35917,\"start\":35911},{\"end\":35935,\"start\":35927},{\"end\":35949,\"start\":35944},{\"end\":35961,\"start\":35955},{\"end\":36247,\"start\":36242},{\"end\":36259,\"start\":36254},{\"end\":36273,\"start\":36267},{\"end\":36542,\"start\":36532},{\"end\":36562,\"start\":36549},{\"end\":36575,\"start\":36570},{\"end\":36584,\"start\":36582},{\"end\":36604,\"start\":36592},{\"end\":36619,\"start\":36614},{\"end\":36638,\"start\":36629},{\"end\":36653,\"start\":36647},{\"end\":36926,\"start\":36917},{\"end\":36939,\"start\":36934},{\"end\":36956,\"start\":36948},{\"end\":36974,\"start\":36966},{\"end\":36993,\"start\":36984},{\"end\":37231,\"start\":37228},{\"end\":37244,\"start\":37235},{\"end\":37263,\"start\":37254},{\"end\":37275,\"start\":37271},{\"end\":37282,\"start\":37277},{\"end\":37523,\"start\":37521},{\"end\":37538,\"start\":37532},{\"end\":37554,\"start\":37546},{\"end\":37568,\"start\":37565},{\"end\":37580,\"start\":37575},{\"end\":37849,\"start\":37847},{\"end\":37867,\"start\":37862},{\"end\":37881,\"start\":37873},{\"end\":38101,\"start\":38099},{\"end\":38114,\"start\":38108},{\"end\":38127,\"start\":38124},{\"end\":38143,\"start\":38139},{\"end\":38156,\"start\":38150},{\"end\":38170,\"start\":38162},{\"end\":38405,\"start\":38402},{\"end\":38413,\"start\":38410},{\"end\":38425,\"start\":38423},{\"end\":38432,\"start\":38430},{\"end\":38716,\"start\":38713},{\"end\":38727,\"start\":38723},{\"end\":38742,\"start\":38736},{\"end\":38762,\"start\":38752},{\"end\":38779,\"start\":38772},{\"end\":38794,\"start\":38788},{\"end\":39147,\"start\":39144},{\"end\":39161,\"start\":39157},{\"end\":39170,\"start\":39167},{\"end\":39184,\"start\":39179},{\"end\":39197,\"start\":39194},{\"end\":39493,\"start\":39490},{\"end\":39506,\"start\":39504},{\"end\":39519,\"start\":39516},{\"end\":39528,\"start\":39524},{\"end\":39536,\"start\":39533},{\"end\":39818,\"start\":39813},{\"end\":39829,\"start\":39825},{\"end\":39852,\"start\":39845},{\"end\":40149,\"start\":40147},{\"end\":40157,\"start\":40154},{\"end\":40170,\"start\":40167},{\"end\":40184,\"start\":40180},{\"end\":40464,\"start\":40462},{\"end\":40472,\"start\":40469},{\"end\":40486,\"start\":40482},{\"end\":40499,\"start\":40496},{\"end\":40788,\"start\":40780},{\"end\":41073,\"start\":41066},{\"end\":41087,\"start\":41082},{\"end\":41102,\"start\":41095},{\"end\":41395,\"start\":41388},{\"end\":41406,\"start\":41402},{\"end\":41424,\"start\":41416},{\"end\":41682,\"start\":41678},{\"end\":41697,\"start\":41690},{\"end\":41713,\"start\":41707},{\"end\":41728,\"start\":41724},{\"end\":41737,\"start\":41730},{\"end\":41941,\"start\":41933},{\"end\":41959,\"start\":41949},{\"end\":41977,\"start\":41970},{\"end\":41991,\"start\":41985},{\"end\":42005,\"start\":41998},{\"end\":42014,\"start\":42010},{\"end\":42306,\"start\":42302},{\"end\":42321,\"start\":42316},{\"end\":42334,\"start\":42329},{\"end\":42347,\"start\":42341},{\"end\":42361,\"start\":42355},{\"end\":42375,\"start\":42367},{\"end\":42669,\"start\":42660},{\"end\":42684,\"start\":42677},{\"end\":42692,\"start\":42688},{\"end\":42696,\"start\":42694},{\"end\":42944,\"start\":42938},{\"end\":42964,\"start\":42954},{\"end\":42976,\"start\":42968},{\"end\":42990,\"start\":42985},{\"end\":42999,\"start\":42992},{\"end\":43316,\"start\":43308},{\"end\":43585,\"start\":43583},{\"end\":43593,\"start\":43590},{\"end\":43602,\"start\":43598},{\"end\":43617,\"start\":43614},{\"end\":43633,\"start\":43628},{\"end\":43647,\"start\":43643},{\"end\":43658,\"start\":43656},{\"end\":43973,\"start\":43971},{\"end\":43983,\"start\":43978},{\"end\":43992,\"start\":43988},{\"end\":44007,\"start\":44004},{\"end\":44020,\"start\":44017},{\"end\":44028,\"start\":44025},{\"end\":44308,\"start\":44306},{\"end\":44322,\"start\":44317},{\"end\":44332,\"start\":44328},{\"end\":44341,\"start\":44339},{\"end\":44563,\"start\":44558},{\"end\":44577,\"start\":44573},{\"end\":44591,\"start\":44584},{\"end\":44598,\"start\":44593}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":33714,\"start\":33490},{\"attributes\":{\"id\":\"b1\"},\"end\":33907,\"start\":33716},{\"attributes\":{\"id\":\"b2\"},\"end\":34086,\"start\":33909},{\"attributes\":{\"id\":\"b3\"},\"end\":34351,\"start\":34088},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1820089},\"end\":34672,\"start\":34353},{\"attributes\":{\"doi\":\"abs/1703.10717\",\"id\":\"b5\"},\"end\":34927,\"start\":34674},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":5002792},\"end\":35346,\"start\":34929},{\"attributes\":{\"id\":\"b7\"},\"end\":35821,\"start\":35348},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1282515},\"end\":36155,\"start\":35823},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":538820},\"end\":36495,\"start\":36157},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1033682},\"end\":36869,\"start\":36497},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":10894094},\"end\":37175,\"start\":36871},{\"attributes\":{\"doi\":\"abs/1709.08878\",\"id\":\"b12\"},\"end\":37440,\"start\":37177},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":7287895},\"end\":37775,\"start\":37442},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":207468},\"end\":38038,\"start\":37777},{\"attributes\":{\"id\":\"b15\"},\"end\":38345,\"start\":38040},{\"attributes\":{\"doi\":\"abs/1805.03989\",\"id\":\"b16\"},\"end\":38572,\"start\":38347},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":9197196},\"end\":39073,\"start\":38574},{\"attributes\":{\"doi\":\"abs/1711.09724\",\"id\":\"b18\"},\"end\":39374,\"start\":39075},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":52099015},\"end\":39737,\"start\":39376},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1998416},\"end\":40025,\"start\":39739},{\"attributes\":{\"doi\":\"abs/1805.04869\",\"id\":\"b21\"},\"end\":40397,\"start\":40027},{\"attributes\":{\"doi\":\"abs/1805.04871\",\"id\":\"b22\"},\"end\":40655,\"start\":40399},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3047006},\"end\":40968,\"start\":40657},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":107645},\"end\":41287,\"start\":40970},{\"attributes\":{\"doi\":\"abs/1511.06434\",\"id\":\"b25\"},\"end\":41612,\"start\":41289},{\"attributes\":{\"id\":\"b26\"},\"end\":41888,\"start\":41614},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1687220},\"end\":42194,\"start\":41890},{\"attributes\":{\"id\":\"b28\"},\"end\":42601,\"start\":42196},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":7961699},\"end\":42846,\"start\":42603},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":1211821},\"end\":43207,\"start\":42848},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2332513},\"end\":43485,\"start\":43209},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":46889991},\"end\":43866,\"start\":43487},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":52056110},\"end\":44230,\"start\":43868},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":3439214},\"end\":44506,\"start\":44232},{\"attributes\":{\"id\":\"b35\"},\"end\":44714,\"start\":44508}]", "bib_title": "[{\"end\":34426,\"start\":34353},{\"end\":35029,\"start\":34929},{\"end\":35901,\"start\":35823},{\"end\":36233,\"start\":36157},{\"end\":36524,\"start\":36497},{\"end\":36908,\"start\":36871},{\"end\":37513,\"start\":37442},{\"end\":37839,\"start\":37777},{\"end\":38702,\"start\":38574},{\"end\":39478,\"start\":39376},{\"end\":39805,\"start\":39739},{\"end\":40751,\"start\":40657},{\"end\":41054,\"start\":40970},{\"end\":41927,\"start\":41890},{\"end\":42653,\"start\":42603},{\"end\":42926,\"start\":42848},{\"end\":43297,\"start\":43209},{\"end\":43572,\"start\":43487},{\"end\":43960,\"start\":43868},{\"end\":44297,\"start\":44232}]", "bib_author": "[{\"end\":33552,\"start\":33535},{\"end\":33570,\"start\":33552},{\"end\":33583,\"start\":33570},{\"end\":33736,\"start\":33718},{\"end\":33753,\"start\":33736},{\"end\":33764,\"start\":33753},{\"end\":33779,\"start\":33764},{\"end\":33790,\"start\":33779},{\"end\":33805,\"start\":33790},{\"end\":33978,\"start\":33960},{\"end\":33986,\"start\":33978},{\"end\":34177,\"start\":34159},{\"end\":34192,\"start\":34177},{\"end\":34207,\"start\":34192},{\"end\":34441,\"start\":34428},{\"end\":34456,\"start\":34441},{\"end\":34472,\"start\":34456},{\"end\":34486,\"start\":34472},{\"end\":34752,\"start\":34735},{\"end\":34764,\"start\":34752},{\"end\":34775,\"start\":34764},{\"end\":35040,\"start\":35031},{\"end\":35050,\"start\":35040},{\"end\":35066,\"start\":35050},{\"end\":35081,\"start\":35066},{\"end\":35097,\"start\":35081},{\"end\":35112,\"start\":35097},{\"end\":35458,\"start\":35443},{\"end\":35480,\"start\":35458},{\"end\":35504,\"start\":35480},{\"end\":35514,\"start\":35504},{\"end\":35919,\"start\":35903},{\"end\":35937,\"start\":35919},{\"end\":35951,\"start\":35937},{\"end\":35963,\"start\":35951},{\"end\":36249,\"start\":36235},{\"end\":36261,\"start\":36249},{\"end\":36275,\"start\":36261},{\"end\":36544,\"start\":36526},{\"end\":36564,\"start\":36544},{\"end\":36577,\"start\":36564},{\"end\":36586,\"start\":36577},{\"end\":36606,\"start\":36586},{\"end\":36621,\"start\":36606},{\"end\":36640,\"start\":36621},{\"end\":36655,\"start\":36640},{\"end\":36928,\"start\":36910},{\"end\":36941,\"start\":36928},{\"end\":36958,\"start\":36941},{\"end\":36976,\"start\":36958},{\"end\":36995,\"start\":36976},{\"end\":37233,\"start\":37221},{\"end\":37246,\"start\":37233},{\"end\":37265,\"start\":37246},{\"end\":37277,\"start\":37265},{\"end\":37284,\"start\":37277},{\"end\":37525,\"start\":37515},{\"end\":37540,\"start\":37525},{\"end\":37556,\"start\":37540},{\"end\":37570,\"start\":37556},{\"end\":37582,\"start\":37570},{\"end\":37851,\"start\":37841},{\"end\":37869,\"start\":37851},{\"end\":37883,\"start\":37869},{\"end\":38103,\"start\":38093},{\"end\":38116,\"start\":38103},{\"end\":38129,\"start\":38116},{\"end\":38145,\"start\":38129},{\"end\":38158,\"start\":38145},{\"end\":38172,\"start\":38158},{\"end\":38407,\"start\":38394},{\"end\":38415,\"start\":38407},{\"end\":38427,\"start\":38415},{\"end\":38434,\"start\":38427},{\"end\":38718,\"start\":38704},{\"end\":38729,\"start\":38718},{\"end\":38744,\"start\":38729},{\"end\":38764,\"start\":38744},{\"end\":38781,\"start\":38764},{\"end\":38796,\"start\":38781},{\"end\":39149,\"start\":39137},{\"end\":39163,\"start\":39149},{\"end\":39172,\"start\":39163},{\"end\":39186,\"start\":39172},{\"end\":39199,\"start\":39186},{\"end\":39495,\"start\":39480},{\"end\":39508,\"start\":39495},{\"end\":39521,\"start\":39508},{\"end\":39530,\"start\":39521},{\"end\":39538,\"start\":39530},{\"end\":39820,\"start\":39807},{\"end\":39831,\"start\":39820},{\"end\":39854,\"start\":39831},{\"end\":40151,\"start\":40139},{\"end\":40159,\"start\":40151},{\"end\":40172,\"start\":40159},{\"end\":40186,\"start\":40172},{\"end\":40466,\"start\":40454},{\"end\":40474,\"start\":40466},{\"end\":40488,\"start\":40474},{\"end\":40501,\"start\":40488},{\"end\":40775,\"start\":40753},{\"end\":40790,\"start\":40775},{\"end\":41075,\"start\":41056},{\"end\":41089,\"start\":41075},{\"end\":41104,\"start\":41089},{\"end\":41397,\"start\":41383},{\"end\":41408,\"start\":41397},{\"end\":41426,\"start\":41408},{\"end\":41684,\"start\":41670},{\"end\":41699,\"start\":41684},{\"end\":41715,\"start\":41699},{\"end\":41730,\"start\":41715},{\"end\":41739,\"start\":41730},{\"end\":41943,\"start\":41929},{\"end\":41961,\"start\":41943},{\"end\":41979,\"start\":41961},{\"end\":41993,\"start\":41979},{\"end\":42007,\"start\":41993},{\"end\":42016,\"start\":42007},{\"end\":42308,\"start\":42293},{\"end\":42323,\"start\":42308},{\"end\":42336,\"start\":42323},{\"end\":42349,\"start\":42336},{\"end\":42363,\"start\":42349},{\"end\":42377,\"start\":42363},{\"end\":42671,\"start\":42655},{\"end\":42686,\"start\":42671},{\"end\":42694,\"start\":42686},{\"end\":42698,\"start\":42694},{\"end\":42946,\"start\":42928},{\"end\":42966,\"start\":42946},{\"end\":42978,\"start\":42966},{\"end\":42992,\"start\":42978},{\"end\":43001,\"start\":42992},{\"end\":43318,\"start\":43299},{\"end\":43587,\"start\":43574},{\"end\":43595,\"start\":43587},{\"end\":43604,\"start\":43595},{\"end\":43619,\"start\":43604},{\"end\":43635,\"start\":43619},{\"end\":43649,\"start\":43635},{\"end\":43660,\"start\":43649},{\"end\":43975,\"start\":43962},{\"end\":43985,\"start\":43975},{\"end\":43994,\"start\":43985},{\"end\":44009,\"start\":43994},{\"end\":44022,\"start\":44009},{\"end\":44030,\"start\":44022},{\"end\":44310,\"start\":44299},{\"end\":44324,\"start\":44310},{\"end\":44334,\"start\":44324},{\"end\":44343,\"start\":44334},{\"end\":44565,\"start\":44553},{\"end\":44579,\"start\":44565},{\"end\":44593,\"start\":44579},{\"end\":44600,\"start\":44593}]", "bib_venue": "[{\"end\":33533,\"start\":33490},{\"end\":33958,\"start\":33909},{\"end\":34157,\"start\":34088},{\"end\":34495,\"start\":34486},{\"end\":34733,\"start\":34674},{\"end\":35121,\"start\":35112},{\"end\":35441,\"start\":35348},{\"end\":35972,\"start\":35963},{\"end\":36311,\"start\":36275},{\"end\":36664,\"start\":36655},{\"end\":37004,\"start\":36995},{\"end\":37219,\"start\":37177},{\"end\":37592,\"start\":37582},{\"end\":37891,\"start\":37883},{\"end\":38091,\"start\":38040},{\"end\":38392,\"start\":38347},{\"end\":38806,\"start\":38796},{\"end\":39135,\"start\":39075},{\"end\":39546,\"start\":39538},{\"end\":39864,\"start\":39854},{\"end\":40137,\"start\":40027},{\"end\":40452,\"start\":40399},{\"end\":40798,\"start\":40790},{\"end\":41113,\"start\":41104},{\"end\":41381,\"start\":41289},{\"end\":41668,\"start\":41614},{\"end\":42025,\"start\":42016},{\"end\":42291,\"start\":42196},{\"end\":42707,\"start\":42698},{\"end\":43010,\"start\":43001},{\"end\":43334,\"start\":43318},{\"end\":43663,\"start\":43660},{\"end\":44038,\"start\":44030},{\"end\":44352,\"start\":44343},{\"end\":44551,\"start\":44508}]"}}}, "year": 2023, "month": 12, "day": 17}