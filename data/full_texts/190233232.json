{"id": 190233232, "updated": "2022-08-09 18:33:45.726", "metadata": {"title": "IA-SpGEMM: an input-aware auto-tuning framework for parallel sparse matrix-matrix multiplication", "authors": "[{\"first\":\"Zhen\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Guangming\",\"last\":\"Tan\",\"middle\":[]},{\"first\":\"Weifeng\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Ninghui\",\"last\":\"Sun\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the ACM International Conference on Supercomputing", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Sparse matrix-matrix multiplication (SpGEMM) is a sparse kernel that is used in a number of scientific applications. Although several SpGEMM algorithms have been proposed, almost all of them are restricted to the compressed sparse row (CSR) format, and the possible performance gain from exploiting other formats has not been well studied. The particular format and algorithm that yield the best performance for SpGEMM also remain undetermined. In this work, we conduct a prospective study on format-specific parallel SpGEMM algorithms, and analyze their pros and cons. We then propose IA-SpGEMM, an input-aware auto-tuning Framework for SpGEMM, that provides a unified programming interface in the CSR format and automatically determines the best format and algorithm for arbitrary sparse matrices. For this purpose, we set-up an algorithm set and design a deep learning model called MatNet that is trained by over 2,700 matrices from the SuiteSparse Matrix Collection to quickly and accurately predict the best solution by using sparse features and density representations. We evaluate our framework on CPUs and a GPU, and the results show that IA-SpGEMM is on average 3.27x and 13.17x faster than MKL on an Intel and an AMD platform, respectively, and is 2.23x faster than cuSPARSE on an NVIDIA GPU.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2952598959", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ics/XieT0S19", "doi": "10.1145/3330345.3330354"}}, "content": {"source": {"pdf_hash": "e02e4d48c886c0d41a837e77ecb16cb70eacc464", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "40c169c95668372bf954544a9f9bf540afd3d23e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e02e4d48c886c0d41a837e77ecb16cb70eacc464.txt", "contents": "\nIA-SpGEMM: An Input-aware Auto-tuning Framework for Parallel Sparse Matrix-Matrix Multiplication SpGEMM: An Input-aware Auto-tuning Framework for Parallel Sparse Matrix-Matrix Multiplication\nJune 26-28, 2019. June 26-28, 2019\n\nZhen Xie xiezhen@ncic.ac.cn \nInstitute of Computing Technology\nDepartment of Computer Science and Technology\nState Key Laboratory of Computer Architecture\nChinese Academy of Sciences\nUniversity of Chinese Academy of Sciences\nChina University of Petroleum\nBeijing\n\nACM Reference format\n\n\n\u2666 \u2020 \nGuangming Tan \nInstitute of Computing Technology\nDepartment of Computer Science and Technology\nState Key Laboratory of Computer Architecture\nChinese Academy of Sciences\nUniversity of Chinese Academy of Sciences\nChina University of Petroleum\nBeijing\n\nACM Reference format\n\n\n\u2666 \u2020 \nWeifeng Liu weifeng.liu@cup.edu.cn \nNinghui Sun \nInstitute of Computing Technology\nDepartment of Computer Science and Technology\nState Key Laboratory of Computer Architecture\nChinese Academy of Sciences\nUniversity of Chinese Academy of Sciences\nChina University of Petroleum\nBeijing\n\nACM Reference format\n\n\n\u2666 \nZhen Xie \nInstitute of Computing Technology\nDepartment of Computer Science and Technology\nState Key Laboratory of Computer Architecture\nChinese Academy of Sciences\nUniversity of Chinese Academy of Sciences\nChina University of Petroleum\nBeijing\n\nACM Reference format\n\n\n\u2666 \u2020 \nGuangming Tan \nInstitute of Computing Technology\nDepartment of Computer Science and Technology\nState Key Laboratory of Computer Architecture\nChinese Academy of Sciences\nUniversity of Chinese Academy of Sciences\nChina University of Petroleum\nBeijing\n\nACM Reference format\n\n\n\u2666 \u2020 \nWeifeng Liu \nNinghui Sun \nInstitute of Computing Technology\nDepartment of Computer Science and Technology\nState Key Laboratory of Computer Architecture\nChinese Academy of Sciences\nUniversity of Chinese Academy of Sciences\nChina University of Petroleum\nBeijing\n\nACM Reference format\n\n\nIA-SpGEMM: An Input-aware Auto-tuning Framework for Parallel Sparse Matrix-Matrix Multiplication SpGEMM: An Input-aware Auto-tuning Framework for Parallel Sparse Matrix-Matrix Multiplication\n\nProceedings of 2019 International Conference on Supercomputing\n2019 International Conference on SupercomputingPhoenix, AZ, USA; Phoenix, AZ, USAJune 26-28, 2019. June 26-28, 201910.1145/3330345.3330354ACM ISBN 978-1-4503-6079-1/19/06. . . $15.00CCS CONCEPTS \u2022 Mathematics of computing \u2192 Mathematical software per- formanceComputations on matrices\u2022 Theory of computa- tion \u2192 Parallel algorithmsKEYWORDS Auto-tuning, SpGEMM, Sparse Format, Neural Network\nSparse matrix-matrix multiplication (SpGEMM) is a sparse kernel that is used in a number of scientific applications. Although several SpGEMM algorithms have been proposed, almost all of them are restricted to the compressed sparse row (CSR) format, and the possible performance gain from exploiting other formats has not been well studied. The particular format and algorithm that yield the best performance for SpGEMM also remain undetermined.In this work, we conduct a prospective study on format-specific parallel SpGEMM algorithms, and analyze their pros and cons. We then propose IA-SpGEMM, an input-aware auto-tuning Framework for SpGEMM, that provides a unified programming interface in the CSR format and automatically determines the best format and algorithm for arbitrary sparse matrices. For this purpose, we set-up an algorithm set and design a deep learning model called MatNet that is trained by over 2,700 matrices from the SuiteSparse Matrix Collection to quickly and accurately predict the best solution by using sparse features and density representations. We evaluate our framework on CPUs and a GPU, and the results show that IA-SpGEMM is on average 3.27x and 13.17x faster than MKL on an Intel and an AMD platform, respectively, and is 2.23x faster than cuSPARSE on an NVIDIA GPU.\n\nINTRODUCTION\n\nSparse matrix-matrix multiplication (SpGEMM) is a key kernels in a number of applications. For example, it often accounts for more than half of the cost of the setup phase for restricting and interpolating matrices in algebraic multigrid methods (AMG) [10]. Many graph processing operations, such as breadth-first search [21], Markov clustering [6], graph contraction [21], subgraph extraction [13], peer pressure clustering [49], and cycle detection [58], can be expressed as SpGEMM. GraphBLAS [7] also defines matrix-based graph algorithms. Efficient SpGEMM algorithms are thus crucial for these applications to achieve higher performance. Several SpGEMM libraries are widely used, including the Intel MKL [26], vector-based sparse accumulator (SPA) [20], hash-based method [41], heap-based method [5], cuSPARSE [17], and CUSP [16] proposed for Nvidia and NSPARSE [42]. However, these libraries are sensitive to sparse input matrices and thus exhibit significant fluctuations in performance. In Figure 1(a), we compare the performance of various algorithms by calculating A * A T on an Intel CPU (as in Section 5.1). It is clear that different algorithms deliver their best performance on different matrices, and no single algorithm dominates on all datasets in terms of performance. This problem transfers the burden of identifying the optimal library onto application programmers and poses special challenges for the automatic library selector.\n\nOn the contrary, Figure 1(b) shows two sources of overhead: the proportion of time spent on sparse accumulation and memory access for the SPA SpGEMM algorithm. It is clear that memory access takes up a significant amount of execution time. However, research in the area has largely ignored the potential for improving performance by optimizing memory access, and has preferred instead to continue to develop new sparse accumulation algorithms [13,18] for the compute part. To some extent, SpGEMM is similar to sparse matrix-vector multiplication (SpMV) and sparse triangular solve (SpTRSV) for irregular and indirect memory access patterns [31]. Much of the research on SpMV and SpTRSV has been dedicated to optimizing memory access by excavating classic storage formats [27,35,39,55,57] with promising results [51,52]. Back to SpGEMM, such classic storage formats, as DIA, COO and ELL, can reduce memory requirements or accelerate memory access on vector architectures and change the order of the calculation process, or can even reduce the number of sparse accumulation operations. The other motivation of this work is to explore the influence on several classic storage formats on SpGEMM.\n\nIn this paper we design multiple SpGEMM algorithms based on a variety of widely used sparse storage formats and analyze the conditions conducive to better performance. Therefore, in order to integrate our implementations with the existing SpGEMM libraries and choose the optimal algorithm, we propose an inputaware auto-tuning framework for SpGEMM (IA-SpGEMM), which classifies two input matrices into the most appropriate category among the assembled SpGEMM algorithm set by employing a novel convolutional neural network called MatNet. We thus build a large number of matrix multiplication pairs by using all matrices from the current version of SuiteSparse Matrix Collection and collect the performance data of the SpGEMM algorithm set as the output of the training data on a given architecture. Moreover, we extract the sparse features and density representation of the two input matrices as input to the training data. MatNet is then generated by using the collected performance data, extracted features, and density representations. Compared with traditional machine learning or empirical models, MatNet is suitable for solving this problem, and can be easily migrated to other architectures with nearly equivalent prediction accuracy.\n\nIn addition, as a lightweight SpGEMM library, IA-SpGEMM provides a unified interface in the CSR format to quickly predict the best format and algorithm for two input matrices, and the matrices are finally executed by the corresponding implementation with possible format conversion. We evaluate the IA-SpGEMM on three processors (an Intel CPU, an AMD CPU, and an Nvidia GPU), and show that it achieves significantly better performance that is on average 3.27x and 13.17x faster than the Intel MKL on dual Intel Xeon E5-2620 and dual AMD EPYC 7501, respectively, with an accuracy of 93%, and 2.23x faster than the NVIDIA cuSPARSE library on Tesla P100 with an accuracy of 91%. The main contributions of this paper are as follows:\n\n\u2022 We propose multiple SpGEMM algorithms based on a variety of widely used sparse storage formats, and redesign the sparse accumulation and memory access methods that represent two main overheads in the SpGEMM. We also analyze the advantages and disadvantages of various format-specific algorithms. By comparing it with current libraries by running all matrices from the SuitSparse Matrix Collection, a significant performance gaps naturally leads to the adoption of an auto-tuning model. \u2022 We propose a convolutional neural network called MatNet to select the best format and algorithm from a large algorithm set. In benchmarking more than 8,000 matrix multiplication pairs, the predictive accuracy of MatNet was over 93%. It largely avoids the tedious work of manual choice and improves scalability to benefit from all algorithms with an acceptable overhead. \u2022 We develop an input-aware auto-tuning Framework for SpGEMM (IA-SpGEMM) with a general interface based on the CSR format. Users can thus transparently obtain the best performance. We implement our framework on three processors and yield average speedups of 3.27x, 13.17x, and 2.23x.\n\n\nBACKGROUND 2.1 Sparse Matrix Storage Format\n\nThe sparse storage format defines the structure used to storage the distributions and values of a sparse matrix, with the goal of balancing the reduction in storage space by storing only non-zero elements and implementing efficient memory access by placing the accessed data into a continuous memory space. To achieve higher efficiency in sparse routines, at least tens of formats have been developed since the 1970s. In particular, most have been derived from the four classic formats which are described below (refer to [45] for a more detailed illustration). Figure 2 shows an example of multiple formats on matrix A.\n\n\u2022 Coordinate (COO) Format: The coordinate format is the most flexible and simplest format. Only non-zero elements are stored, and the coordinates of each non-zero element are given explicitly. \u2022 Compressed Sparse Row (CSR) Format: The most popular representation contains three arrays: the beginning position of each row is stored in \"ptr\", and the column indices and values of each non-zero element are stored in \"col_ind\" and \"data\", respectively. \u2022 Diagonal (DIA) Format: Values of diagonals are stored as columns in a dense matrix. Another \"offsets\" array saves offsets from the main diagonal. \u2022 ELLPACK (ELL) Format: It uses two matrices to pack all nonzeros to the left with the same number of rows. The first \"col_ind\" matrix stores the column indices and the second \"data\" matrix stores the values.   Step1. \n\n\nCompute Max Nnz Per Row\n\n\nParallel SpGEMM Method\n\nLet matrix A have size m * n and B have size n * k. The matrix product is C = AB. The element in the i-th row and j-th column in matrix C can be expressed as: c i j = n\u22121 k =0 a ik b k j . The parallel SpGEMM method was proposed by Gustavson [25] and improved on MATLAB by Gilbert et al. [20]. This algorithm (Algorithm 1) in parallel multiplies rows of A by the entire B matrix to calculate rows of C by summing the product of all non-zero elements as the sparse accumulation operation. Similarly, many GPU SpGEMM algorithms improve the sparse accumulation operation for accumulating partial results by using distributed memory [13], a hash table [17,42], or the \"expansion, sorting, and compression\" (ESC) method [16]. Some of these algorithms are included in our algorithm set. Algorithm 1 Row-wise SpGEMM algorithm for C = A * B. We use C/C++ notation, i.e., C[i,j] refers to the (i + 1)-th row and the (j + 1)-th column element in the matrix C. //accumulate par tial r esults in r ow 5:\nC[i, j] \u2190 C[i, j] + A[i, j] * B[j, :]\n\nSPGEMM ALGORITHM\n\nIn this section, three format-special SpGEMM algorithms, as well as their advantages and disadvantages, are introduced by using A * A as an example 1 . We also compare six CPU algorithms and three GPU algorithms by running all matrices from the SuiteSparse Matrix Collection such that the motivation for auto-tuning naturally emerges.\n\n\nSpGEMM in DIA, COO and ELL formats\n\nBecause the DIA format continuously stores diagonal elements, it appears impossible to multiply two diagonals directly. To connect these diagonals, we first append a \"pos\" array to the original DIA format to record the order of each diagonal line, which can be used to quickly and easily convert diagonal coordinates into real coordinates. As shown in Figure 3, the multiplication proceeds generally as follows:\n\nStep 1: Each element of the diagonal line is first converted into real coordinates by coordinate transformation to obtain the multiplied row number (an example of the second diagonal of A is given in the figure).\n\nStep 2: The real coordinates of the outputs are mapped to the corresponding diagonal numbers, and the bitmap where the diagonal numbers are located is marked as \"T \". Step 3: The memory of C is allocated according to the number of \"T \"s in the bitmap, and the partial results are added to the corresponding positions in the same manner as in the first step. Algorithm 2 significantly reduces the overhead due to memory access for the diagonal matrix and directly adds the intermediate results to the target address without extra memory consumption. Note that row-based thread scheduling takes a row as the minimum unit such that it so it achieves load balancing and avoids write-write conflicts among threads. This \"lock-free scheduling\" method can avoid altogether the use of the lock. We call this method the \"DIA method. \" for i \u2208 A.r ow do 4: for j \u2208 A.num_dia\u0434onals do 5: A_j \u2190 i + A.of f sets[j] //Conver t DI A to REAL 6: for k \u2208 B .num_dia\u0434onals do 7:\nB_j \u2190 B_i + B .of f sets[k ] //Conver t DI A to REAL 8: out _dia \u2190 A.r ow \u2212 A_i + B_j \u2212 1 //Mappin\u0434 9:\nif Bit Mask[out _dia] == f alse then 10: C .output _dia \u2190 C .output _dia + 1 11: Bit Mask[out _dia] \u2190 true 12: Malloc_DI A(C .output _dia) 13: #parallel for 14: for i \u2208 A.r ow do 15: for j \u2208 A.num_dia\u0434onals do 16: 17: for k \u2208 B .num_dia\u0434onals do 18:\nA_j \u2190 i + A.of f sets[j]B_j \u2190 B_i + B .of f sets[k ] 19: out _dia \u2190 A.r ow \u2212 A_i + B_j \u2212 1 20: [out _i, out _j] \u2190 [A_i, C .pos[out _j \u2212out _i +C .r ow \u22121]] 21: C .dat a[out _i, out _j] \u2190 C .dat a[out _i, out _j] + A.dat a[i, j] * B .dat a[B_i, k ]\nThe COO format separately stores non-zero elements of the same row, because of which the flexible format can more easily to be split and merged. Algorithm 3 first divides matrix A into k parts by row, and matrix B into k parts by column (k is two or four). Each partition of A and B is successively computed for a part of C by the SPA method [20] and all the partial results are finally merged. As shown in Figure 3, matrix A is first divided into four row matrices and matrix A is divided into four column matrices. Taking two partitions as an example, four threads perform the multiplication calculation of each part respectively. Given that the number of columns of matrix A is divided into a quarter of those of the SPA algorithm, the memory consumption of each thread is a quarter of that of the SPA algorithm. Finally, the partial results between threads are merged into the remelting result matrix. The most significant advantage of this algorithm is that it greatly reduces the length of the dense vector B_col k times over that of the SPA method and improves the efficiency of the cache, but incurs additional overhead in partitioning and merging the matrices. We call this the \"COO method. \"\n\nThe ELL format packages the original matrix into two rectangular matrices of the same size by shifting all non-zero elements to left for more efficient memory access. Because each line of the ELL format contains the same non-zero number, this format makes it possible to reduce the overhead of the symbol phase of the SpGEMM algorithm. In Step 1, we first equally assign matrix rows to threads and use the Col_ind of two matrices to compute the maximum non-zero elements per row of C by a bitmap (an example of the first row of A is given). The memory of C is then determined. In Step 2, the memory of C is allocated by the maximum number of non-zero elements per row, and the newly allocated memory is used as hash table to store and accumulate the intermediate results. All partial Divide A to A 1 , ..., A k by r ow 3: Divide B to B 1 , ..., B k by column 4: for m \u2208 k do 5: for n \u2208 k do 6: Malloc Dense V ector [B_n.col ] and I nit by \u2212 1 7: #parallel for 8: for i \u2208 A m .r ow do 9: 13: num_nnz \u2190 num_nnz + 1 14: Malloc_CSR(C mn ) 15: #parallel for 16: for i \u2208 A m .r ow do 17:\nfor j \u2208 A m .ptr [i] to A m .ptr [i + 1] do 10: for k \u2208 B n .ptr [A m .cols[j] : A m .cols[j] + 1] do 11: if mask[B n .cols[k ]] i then 12: mask[B n .cols[k ]] \u2190 ifor j \u2208 A m .ptr [i] to A m .ptr [i + 1] do 18: for k \u2208 B n .ptr [A m .cols[j] : A m .cols[j] + 1] do 19: output \u2190 B n .cols[k ] 20: Sums[output ] += A m .dat a[j] * B n .dat a[output ] 21:\nSpar sif y Sums to C mn\n\n\n22:\n\nMer\u0434e C 11 , ..., C kk to C results are mapped to the corresponding positions by calculating the hash values of the column indices or keeping plus one whenever collision occurs. Finally, the disordered matrix C is sorted. Algorithm 4 has two main advantages: (1) Because the Col_ind is placed in continuous memory space, the symbolic phase can make full use of the SIMD instructions to speed-up the efficiency of loading and assigning data. (2) In the numeric phase, the memory space pre-allocated to C is used as hash table, which not only benefits the advantage of the hash table as the sparse accumulator, but also avoids memory consumption. We call this the \"ELL method. \" for k \u2208 A.r ow do 5: \nfor i \u2208 A.nnz[k ] do 6: for j \u2208 B .nnz[A_col [k * B_max + i]] do 7: if Mask[B .cols[i * B_max + j]] k then 8: Mask[B .cols[i * B_max + j]] \u2190 k 9:\nnnz_r ow \u2190 nnz_r ow + 1 10: C .nnz_r ow[i] \u2190 nnz_r ow 11: C_max \u2190 MAX (C .nnz_r ow) 12: Malloc_ELL(C .r ow * C .max _nnz_per _r ow) 13: #parallel for 14: for k \u2208 C .r ow do 15: for i \u2208 A.nnz[k ] do 16: \nfor j \u2208 B .nnz[A_col [k * A_max + i]] do 17: Output _hash \u2190 hash_f unction(B .cols[A.col [k, i], j]) 18: C .cols[i, Output _hash] \u2190 B .cols[A.col [k, i], j] 19: C .dat a[i, Output _hash] += A.dat a[k, i] * B .dat a[A.col [k, i], j] 20:\nSor t C .cols and C .dat a\n\n\nOverview of algorithm set\n\nThus far, we have constructed multiple format-specific algorithms. By integrating them with currently available popular algorithm libraries, as shown in Table 1, seven SpGEMM algorithms are developed for the CPU and three for the GPU. As benchmark, we Table 1: Performance statistics : \"Dominance\" and \"Percentage\" represent the number and proportion of the best and the better than baseline for various algorithms. \"Average Speedup\" calculates the average speedup in cases where the best perform is attained on a specific algorithm, and \"Ideal Tool\" uses the best performance to obtain the global speedup on three platforms. build 8000+ matrix multiplication pairs by using all the matrices in the SuiteSparse Matrix Collection and compare the performance of these algorithms.  [20] Hash based method (CSR) [41] Heap based method (CSR) [ \n\n\nPerformance comparison\n\nWe compare the performance of various algorithms on three architectures (as in Section 5.1). To achieve accurate results and complete the task in a controllable time, the run time is the average of 10 trials, and we restrict memory expansion to no more than five times due to format conversion. The execution times of all algorithms could be no longer than five times than of the MKL or cuSPARSE, which also means that these matrix pairs are not suitable for a specific format or algorithm. As shown in Table 2, a general view of the experiments clearly shows significant differences in performance with varying inputs, formats, algorithms, and platforms. In addition, no single format and algorithm can constantly deliver the best performance on all matrix pairs. In the case of the Intel CPU, we mark MKL's performance as the baseline, which delivers the best performance on only 35% of the matrix pairs. The DIA method outperforms the baseline on 1,107 matrix pairs better than baseline and yields the best performance on 491, e.g., dw256A*dwa512 and qpband*Trefethen_20000. These matrices are almost composed of one or multiple diagonal lines. The COO method outperforms the baseline on 506 matrix pairs and delivers the best results on approximately half of them, e.g., human_gene2*appu and msc10848*crystk02. The non-zero rate (\u22488%) and the number of columns of these matrices are large. The ELL method exceeds the baseline on 2,879 matrix pairs and performs optimally on 1,496, e.g., G48*G49 and ch7-9-b3*ch7-9-b2. The vector-, hash-, and heap-based methods perform better than the baseline on 7,006 matrices, and on 3,051 matrix pairs yield the best performance among all methods. For the AMD platform, using the same method to sort the performance of the algorithms, the seven algorithms perform best on 20.96%, 9.14%, 4.20%, 24.40%, 10.18%, 21.56%, and 9.56% of the cases, respectively. In comparison, AMD benefits more from the diversity of formats and algorithms. For the GPU, the two algorithms (cuSPARSE and NSPARSE) are suitable for almost half of the matrix pairs. NSPARSE shows advantages in performance on large matrices, whereas the ESC algorithm is effective on only on a few matrix pairs.\n\n\nPerformance analysis\n\nThe popular SpGEMM libraries do not yield the best performance on all matrix pairs. On the Intel architecture, MKL delivers the best performance on approximately 35% of the dataset, and almost all matrix pairs can be executed within a reasonable time. The improvement in performance is highly correlated with data size, but the overhead of the MKL framework is not expected, especially for small matrix pairs. Our \"DIA method\" modifies the order of memory access, and reduces the number of sparse accumulation operations and memory consumption when the input matrix pairs satisfy a diagonal distribution. It thus exhibits impressive performance with an average speedup of 72.04x. The \"ELL method\" is based on the most efficient sparse format for memory access. It significantly improves the efficiency of memory access and saves time in the symbol phase with an average speedup of 9.92x. But this format will still introduce overhead due to padding data for unbalanced row distribution in the matrix pairs. Thus, this method works well for about 35% of the dataset. The \"COO method\" is suitable for specific cases and some matrix pairs still stand out. Furthermore, the vector-, hash-, and heap-based methods run on their \"best of  all\" matrix pairs to obtain average speedups of 1.31x, 6.37x, and 6.21x, respectively. In contrast to Intel, AMD's performance has the same proportions, but its absolute performance is slightly lower than Intel's. In addition, because of the higher memory bandwidth needed for the AMD architecture, the \"DIA\" and \"ELL\" algorithms deliver better performance. On the GPU platform, compared with the cuSPARSE library, the \"ESC\" method and the NSPARSE algorithm obtain average speedups of 6.27x and 3.71x, respectively on their \"best of all\" cases. We ideally assume that there is an \"absolutely perfect\" tool that can accurately predict the best choice without any overhead. It would achieve average speedups of 8.94x, 46.16x, and 2.40x for all matrix pairs on the three platforms. Such performance improvements urgently drive us to design an auto-tuning framework.\n\n\nOVERVIEW OF IA-SPGEMM\n\nIn the previous section, our experimental results demonstrate the enormous potential for leveraging various formats and algorithms. We thus develop an Input-aware Auto-tuning Framework for SpGEMM (IA-SpGEMM) to select the best format and algorithm on multiple architectures. The framework is shown in Figure 4. It considers the impact on performance of matrix patterns and machine configurations for the SpGEMM kernel, and is evaluated by thousands of matrix pairs. To achieve this goal, we need a learning model to combine a large number of matrix patterns, algorithms, and machine configurations to find the optimal matching solution. However, it is challenging for a general algorithm to find the most suitable solution in a large search space. Therefore, we first convert the auto-tuning problem into a feature and image classification problem, and select an outstanding convolutional neural network for classification to achieve this goal.\n\nRecognizing the best format and algorithm is a complex task that requires a large amount of data for training. We use all 2,726 matrices from the SuitSparse Matrix Collection to build 8000+ matrix multiplication pairs, and extract the matrix features and density representations (Sections 4.1 and 4.2) as the input to the training data. We then collect the execution times for various formats and algorithms as the output of the training data. Thus, this method incorporates matrix features and algorithms together to automatically generate a highly accurate classifier. As shown in Figure 4, the IA-SpGEMM system is divided into two parts: training and prediction. It first trains the neural network MatNet (Section 4.3) by using the collected training data, and the prediction part indicates the probability of each algorithm to generate the best SpGEMM kernel.\n\nConveniently, the IA-SpGEMM provides a unified interface based on the CSR format, which lends usability and portability to it. It can quickly replace libraries in the IA-SpGEMM framework. It also supports two usage methods to fit unique needs. The first is one where the framework automatically selects the optimal algorithm, whereas the other supports the inspector-executor approach. This difference brings two benefits. First, the developer can transparently benefit from multiple formats and algorithms. Second, the framework can save the best choice by automatic tuning and reuse the known best algorithm on the same matrix to significantly reduce the overhead due to feature extraction and forward propagation of the neural network. Extensibility is also an advantage of the IA-SpGEMM. Given the inherent characteristics of the neural network, it is open to the addition of new algorithms and training data to improve performance and robustness.\n\nWe now introduce the three components of the IA-SpGEMM: feature extraction, density representation, and the design of the neural network.\n\n\nFeature Extraction\n\nAs an automatic input-tuning system, the IA-SpGEMM first considers 13 fine-grained features related to the distribution and characteristics of four formats for CPU, and eight features of two formats for GPU. Some of them intuitively affect the performance of SpGEMM, e.g., the number and ratio of non-zero elements. Other features reflect memory consumption and algorithm performance resulting from the storage structure. Table 3 summarizes all the sparse features used in the IA-SpGEMM. The first eight features represent the most common structure, including the number of rows and columns, and non-zero elements, which suit for all four formats. The ninth to 11th describe diagonal features of the DIA format, including the number of diagonals and the fill ratio of the added zero elements. The 12th expresses the fill ratio of the ELL format for aligned memory access. The 13th feature represents the coefficient \n\n\nDensity Representation\n\nSparse matrices usually have high sparsity and different sizes while the convolutional neural network (CNN) generally requires fixedsize input data. This difference leads to two problems. The first is that sparse matrices are usually very large, which causes a large inference overhead for the neural network if complete matrices are used as input. The second problem is that matrix pairs contain a large and different numbers of rows and columns, which need to be transformed to the same size. For the image field, the general approach is to shrink large pixels or enlarge small ones to resize an image. This method can also be used to convert the sparse matrix into a small density representation that can represent the coarse-grained patterns of the original matrix with an acceptable size. The density representation as the primary image input to the CNN represents a snapshot matrix that abstracts most of the sparse patterns. As shown in Figure 5, we apply this method to map an 8*8 matrix to a 4*4 matrix as an example. The original matrix is divided into 4*4 blocks, and each block is counted by non-zero elements that fill into the corresponding new matrix. Then, the original 8*8 matrix and the 4*4 density representation both contain several diagonals with some irregular non-zero elements. Block count is related to non-zero elements on the matrix, and normalization restricts their number to within a reasonable range (0~255).\n\nTo ensure sufficient accuracy and acceptable overhead for the neural network, we define 128*128 (by comparing with the size of 64*64, 128*128, and 256*256, and choosing the best one) as the size of the density representation and apply the scaling method to map the sparse matrix to the density representation. Note that any sampling method (such as distance histogram representation [61]) may also lose potential features, which can affect the choice of format and algorithm. An approach is thus is needed to make full or systematic use of these data from different dimensions (fine and coarse grained) and complement the loss of accuracy caused by data abstraction (feature extraction and scaling method).\n\n\nMatNet Design\n\nThe traditional CNN has delivered impressive results in image classification [22,28,47]. Several convolutional layers and pooling layers are used in it to extract high-level features [46,59]. The feedforward neural network (FFNN) is applied to classify multidimensional data [56]. The standard FFNN is a multi-layer feedforward network with an input, a hidden, and an output layer. It can be used to learn and store a large number of mappings between the input and output layers. With regard to our questions, we found that these two inputs, in Section 4.1 and 4.2, respectively, are not perfect and have shortcomings. Features only capture the fine-grained patterns of the matrix, whereas density representation abstracts from coarse-grained patterns but ignores details. We thus explore a learning model to combine the two patterns, and this is described below.\n\nBased on the powerful classification capability of neural networks, we design the MatNet model, which combines the CNN and the FFNN to enhance the ability to classify images and features simultaneously. As show in Figure 6, this structure consists of four separate inputs, two of which are the density representations of matrices A and B, and are marked as A_DR and B_DR, respectively, and the others are features of matrices A and B, and are marked as A_F and B_F, respectively. This is so the CNN can produce a number of filters to discriminate local features by using the conv1 layer and holistic characteristics by using the conv2/conv3 layer (some kernels are visualized). The extended FFNN can aggregate sparse features.\n\nWe then define the training data, which include features (13 for CPU and nine for GPU), density representations (128*128), and the probability of each algorithm. For example, if the execution times of Step Total MKL DIA ELL COO Others\n\nStep\n\nStep the five algorithms areT 1,T 2,T 3,T 4, andT 5, the probability of each algorithm can be calculated as:\nPi = 1 T i 1 T 1 + 1 T 2 +...+ 1 T 5\n, respectively\n\n(if a specific algorithm cannot be executed in a reasonable time, then 1 T i is set to zero). Then, the \"best choice\" corresponds to the algorithm with the highest probability. Unlike past work on training learning models absolutely, which can cause confusion between algorithms delivering similar performance, the probability of the output fairly preserves differences that are critical for selection. From 2,726 matrices, we randomly select 5,000 records as the training data (fewer records for GPU with limited memory). These data have two characteristics: 1) The training data are from two matrices that may be completely unrelated. 2) Similar density representations may lead to completely different results. Moreover, the relationship between the two types of input (feature and density representation) is not obvious. These unrelated data thus affect each other during the training phase, which affect the accuracy of the network, so we adopt a two-way strategy to eliminate interference. Therefore, training is divided into two separate stages. The first phase trains two kinds of neural networks (CNN and FFNN) independently. The second phase maintains all parameters of the previous training and merges all components to update the parameters at the last level. In this way, the mutual influence of features and density representations can be significantly reduced. With the gradual addition of more training information, the accuracy of prediction can be improved step by step.\n\nAnother major advantage of this model is scalability. With the same training method, the IA-SpGEMM can easily be deployed on new platforms and new algorithms can be added to it to improve diversity. Because the configurations of the chosen platforms are completely different, the collected training data and the \"best result\" can vary significantly from one platform to the other. With retraining, MatNet can also achieve high accuracy on these platforms.\n\n\nEVALUATION\n\nIn this section, we evaluate the speedup of the IA-SpGEMM by running all matrices in the SuiteSparse Matrix Collection on the three architectures, and analyze the accuracy and overhead of MatNet.\n\n\nSetup\n\nPlatform: We compare the performance of the IA-SpGEMM on three architectures, as shown in Table 4. Two of them are x86 multicore processors and the other is a manycore processor.\n\nBaseline: The IA-SpGEMM is compared with several state-ofthe-art SpGEMM libraries, such as Intel MKL v19.0.0.117 and the hash-based method [41] for CPU, and NVIDIA cuSPARSE v8.0.61 and NSPARSE [42] for GPU. We enable the OpenMP threading model on both CPU platforms with 28 threads on the dual Intel E5-2690 v4 and 64 threads on the dual AMD EPYC 7501 with the \"-O3\" option.\n\nDataset: A total of 2,726 matrices from the SuiteSparse matrix collection are used to randomly construct 8,195 matrix pairs for evaluation, for a total of 220 GB in total. Of this, 60% is used for training, 20% for validation, and 20% for testing. The matrices range in size from 56KB to 33GB, and the number of non-zero elements  ranges from 4,000 to 2 billion. The dataset includes large and actively growing sets of sparse matrices that arise in practice. Figure 7 gives an overview of the loss and accuracy of MatNet discussed in Section 4.2 for classifying matrix pairs into the best format and algorithm on the three platforms. Several aspects are compared below.\n\n\nResults of Training\n\n\nLoss and Accuracy. The loss function (categorical_crossentropy)\n\nis used to indicate how far prediction deviates from the target value. During the training phase, weights are updated based on this quantity. Another indicator is accuracy, which monitors how many cases are correctly predicted. While the network is being trained, loss decreases and accuracy increases.\n\nThe top half of Figure 7 compares the losses and accuracies for the training, validation, and testing data. For the Intel platform, as the number of steps of iteration increases for independent training (first phase), the loss of MatNet decreases gradually. After 2,000 iterations, the network converges to an almost 85% accuracy. We then merge the two independent trainings and adjust the learning rate (second phase). Loss continues to decline and accuracy increases slowly, and finally the training process stabilizes at 4,000 iterations. An accuracy of 93% is achieved for the training set, and 91% and 90% for the validation and testing sets, respectively. On the AMD platform, the network demonstrates similar learning proficiency, and the final accuracy values are 92%, 90%, and 89%. For the GPU platform, MatNet incurs slightly higher loss but has higher accuracy. The network converges at close to 2,000 iterations. At this time, the intervention is interpolated and training continues. After 3,000 steps, the accuracy values are stable at 92%, 90%, and 87%.\n\nThe results show that MatNet quickly learned the characteristics of the matrices and maintained continuous convergence on the three platforms, which also indicates that the density representation and features contain potential connections to the best format and algorithm. In addition, the two-stage training method significantly improves accuracy. With the combination of the two types of training data by using the two-way strategy, MatNet is thus further upgraded.\n\n\nBest Format and Algorithm.\n\nWe now provide details of the convergence of various formats and algorithms by MatNet. It is clear that as the overall accuracy of the network gradually increases, various formats and algorithms exhibit different tendencies of convergence. The main reason for this is that these formats and algorithms account for an unbalanced proportion of records for the training data. For example, the Intel platform's MKL algorithm and the AMD platform's ELL method first converges to high precision by occupying the largest proportion of the training data, and the GPU's cuSPARSE and NSPARSE algorithms exhibit similar rates of convergence using similar numbers of records. Finally, all formats and algorithms achieve accuracy higher than 90%. In addition, we used a widely used traditional decision tree algorithm (CART approach [12]) to compare with MatNet. The decision tree is constructed by features of the two matrices. Table 5 shows the performance of the two classifiers on two indicators, where pre. represents precision and recall measures the number of correct results returned. The result shows that MatNet outperforms the decision tree on the two indicators with an average precision of 91.50% and recall of 86.57%, whereas the decision tree has a precision of 74.19% and recall of 67.79%. The results in terms of accuracy on the three platforms also show that our MatNet is an effective cross-platform model.\n\n\nSpeedups and Overhead\n\nThe results of speedup of the IA-SpGEMM are presented in Figure  8. The predicted formats and algorithms are generated by the Mat-Net model, and each execution of the IA-SpGEMM includes the complete overhead incurred by feature extraction, prediction, and format conversion (if needed). The x-axis represents the sequence of matrix pairs with incremental non-zeros, and the y-axis on the left side represents the GFLOPS of SpGEMM and the y-axis on the right side calculates the proportion of various optimization methods that deliver the best performance. Compared with the MKL and cuSPARSE method, our algorithm achieves average speedups of 3.27x, 13.17x, and 2.23x. Furthermore, we test the speedups of IA-SPGEMM in comparison with state-of-the-art methods ( [41] and [42]), and they are 2.45x, 8.22x, and 1.94x on average. The performance gain consists of several parts: (1) A variety of formats significantly reduce the time needed for memory access for the corresponding matrix pairs. (2) The three proposed algorithms change the number of sparse accumulations or reduce memory consumption. (3) Our framework also makes full use of currently available algorithms. Compared with the \"ideal tool\" mentioned in Section 3.4, the IA-SpGEMM achieves an accuracy of 94% without overhead and 37% with overhead as its best performance on the same dataset. The main reason for the reduction in speedups is that the fixed time for predicting the best format and algorithm is expensive, especially for small matrix pairs. Overhead is still in our discussion. Note that after collecting the training data, it takes approximately 27 minutes to train the complete MatNet for 4,900 records on two NVIDIA P100 GPUs. In addition, SpGEMM using the IA-SpGEMM framework features multiple stages: 1) extracting the density representation and sparse features of the two matrices; 2) predicting the best format and algorithm by MatNet; 3) conversion into various formats (if necessary); and 4) executing the corresponding matrix multiplication kernel. In Figure 9, a proportion chart shows the average performance breakdown of 12 groups of matrices with increasing sizes. The overhead of the first and the third parts is proportional to the size of the matrix pair, and the second part takes about 0.18 milliseconds per matrix pair. It is clear that the first three performance overheads account for a smaller proportion of the total time as the matrix pairs become larger. Most of the extra overhead incurred by the IA-SpGEMM is below 20%. We thus recommend not using our framework on very small matrix pairs so that the overhead incurred by the auto-tuner does not become another system bottleneck. In addition, the inspector-executor method divides SpGEMM into two stages: analysis and execution. The inspector inspects the matrix patterns and applies format changes, and the executor calls the routine by reusing the predicted results. As the number of computations increases, these overheads are almost completely diluted and the proportion of overhead is significantly reduced.\n\n\nUsage\n\nIn this section, we open-source IA-SpGEMM with a unified interface for the SPGEMM kernel and provide a test case to compute A * B for validating the results of prediction of MatNet. The source code is available at https://github.com/zhen-xie/IA-SpGEMM.git. In addition, our model can be easily extended to more platforms and algorithms by collecting more training records and fine-tuning the MatNet model.\n\n\nRELATED WORK\n\nSparse kernels have been widely used to improve higher efficiency in a number of applications [14,23,33]. Various approaches have been proposed to optimize data dependence and unbalanced sparse computations. Venkat et al. [2,53,54,60] developed several techniques for dependence analysis and data transformation optimization for sparse computations during the compilation phase, Arash et al. [3,4] used a performance model and a blocking mechanism to resolve the problem of load imbalance. This paper focuses on the format, algorithm, and auto-tunner for the SpGEMM kernel.\n\nSpGEMM was parallelized and optimized on CPUs. The most significant difference between these algorithms is the method used for nonzero accumulation. As in the COO algorithms used in this paper, the dense accumulator [20,43] is a general solution, whereas other methods involve sorting a heap [5] or merging rows [44]. Moreover, a few GPU algorithms have been proposed, CUSP [16] uses an expand-sort-compress (ESC) algorithm that pre-allocates and collects all intermediate results, and accumulates them through sort and compression operations. cuSPARSE [17], NSPARSE [42] and Kokkos [19] uses a hash table to combine the intermediate results in global memory. bhPARSE [34] first assigns rows into bins by the size of the intermediate result and output, and launches various kernels. The hybrid method [36,38], multiple-levels algorithm [5], and row merge algorithm [24] can also show good performance on partial matrices. These algorithms can be added to our IA-SpGEMM to yield better performance. Moreover, our current system selects four main formats on the CPU and two formats (COO and CSR) on the GPU. But there are 10 popular formats [50], including Compressed Sparse Column (CSC), Sliced ELL (SELL) [40], Block CSR (BCSR) [45], Hybrid (HYB) [11] and CSR5 [37]. However, this work focuses on building an IA-SpGEMM framework compatible with various formats and algorithms. Using this framework, we can still design new algorithms for these formats to speed-up this kernel, and the SpGEMM algorithms can advance the IA-SpGEMM system. In addition, the IA-SpGEMM framework proposed in this paper resolves the problem on a single node and, in many cases, dominate the whole overhead. We would like to see that the following work could integrate our framework into distributed SpGEMM implementations [8,9,13].\n\nSelecting the best format and algorithm has received considerable attention in recent years. The work closest to this study is that by Zhao et al. [61], which for the first time used a CNN to select the matrix format for SpMV and yielded an accuracy of 93%. Several studies [15,32,48] have been devoted to the best storage formats through auto-tuning methods, but some methods may be limited owing to the learning ability of the models applied. Moreover, choosing the best format can be seen as a classification problem, and is similar to recognizing handwritten digits, which was one of the first applications of the CNN. The LeNET-5 [29] was developed for this task. The FFNN [30] is also widely used for the classification model. Unlike SpMV auto-tuners, our algorithm needs to consider the patterns of the two arbitrary matrices at the same time and classify them into appropriate directories. We thus introduced these two neural networks to automatic tuning and designed a new convolutional neural network (MatNet) to connect them for the SpGEMM. We found that sparse kernels can benefit from the neural network method. Extending neural networks to more sparse kernels can also help reveal connections between optimization methods and specific parameters.\n\n\nCONCLUSION\n\nIn this work, we proposed a variety of SpGEMM algorithms for DIA, COO, and ELL formats, and presented an Input-aware Auto-tuning Framework for SpGEMM (IA-SpGEMM) that can automatically determine the best format and algorithm for any sparse matrix pairs. It gathers a set of SpGEMM algorithms that naturally allow for the use of a deep learning model (MatNet) to predict the best choice by using features and density representation. The results show that the IA-SpGEMM yields better performance than four other state-of-the-art libraries. We also expect more sparse and input-sensitive algorithms can be inspired by our method.\n\nFigure 2 :\n2An example of the four sparse matrix formats, where the italics represent small changes. The COO format adds a row offset array, the DIA format adds a diagonal position array, and the ELL format adds an array for counting nnzs per row. All formats are sorted in row order.\n\nFigure 3 :\n3Flowchart of three format-specific algorithms and some examples of A *A . The DIA method shows the processes of coordinate transformation and partial accumulation. The COO method divides matrices by k=1 and shows that the length of the dense vector is reduced to that of the previous quarter. The ELL method also uses a line as an example to demonstrate the fast symbol phase by BitMap and hash functions used in the numeric phase.\n\n\nMalloc Dense Bit Map[A.r ow + B .col \u2212 1] and I nit by f alses 3:\n\n\nMalloc Dense Bit Map[B .col ] and I nit by F alse\n\nFigure 4 :\n4IA-SpGEMM overview: The solid line indicates the collection and training phase, and the dotted line is the execution flow of the user interface. The collection phase includes extracting two patterns of input and the execution times of all algorithms. The training phase generates the MatNet model by the two-way strategy.\n\nFigure 5 :\n5An example to convert 8 \u00d7 8 matrix to 4 \u00d7 4 density representation.\n\nFigure 6 :\n6Details of the parameters of the neural network, and visualization of some kernels of MatNet.\n\nFigure 7 :\n7Loss and accuracy of the MatNet, and details of various formats and algorithms during the training phase.\n\nFigure 8 :\n8Performance and proportion of different formats and algorithms on the three architectures. MKL and hash-based method for CPU, and cuSPARSE and NSPARSE for GPU are state-of-the-art libraries.\n\nFigure 9 :\n9Performance breakdown of several groups of matrices of different sizes.\n\n\nfor j = A.r ow_ind[i] to A.r ow_ind[i + 1] do1: #parallel for \n2: for i = 0 to C .r ow do \n\n3: \n\n4: \n\n\n\nTable 2 :\n2Seven algorithms for CPU and three algorithms for GPU.CPU \n\nIntel MKL v19.0.0.117 mkl_sparse_sp2m (CSR) [26] \nDIA method (Algorithm 2) \nCOO method (Algorithm 3) \nELL method (Algorithm 4) \nSPA vector based method (CSR) \n\nTable 3 :\n3Sparse features and description.Feature \nDescription \nrow, col, \nnnz \nthe number of rows, columns and non-zero elements \n\nnnz_ratio \nthe ratio of non-zero elements in CSR format \nmax, min, \naverage \n\nthe maximun, minimun, and average numbers of \nnon-zero elements \nVAR \nthe variance of non-zero elements \ndia_num \nthe number of diagonals in DIA format \ndia_ratio \nthe number of diagonals divided by all diagonals \ndia_pad, \nell_pad \nthe ratio of padding data in DIA and ELL formats \n\nCV \nthe coefficient of variation of non-zero elements \n\nof variation (CV) of the COO format used in [1] to evaluate the \ndiversity of the number of non-zero elements per row. \n\n\n\nTable 4 :\n4Two CPUs and one GPU used for evaluation.Intel CPU \nAMD CPU \nNVIDIA GPU \n\nCore \n\nXeon E5-\n2690 v4 \n2 processors, \n28 cores \n@2.60 GHz \n\nEPYC 7501 \n2 processors, \n64 cores \n@2.00 GHz \n\nTesla P100 \n56 SMs \n@1328 MHz \n\nCaches \n\nL1: 32 KB*14 \nL2: 256 KB*14 \nL3: 35 MB \n\nL1: 32 KB*32 \nL2: 512 KB*32 \nL3: 64 MB \n\nL2:4096 KB \n\nMemory \n\n128 GB \nDDR4-2133 \n2*4 channels \n\n256 GB \nDDR4-2666 \n2*8 channels \n\n16 GB \n1.4 Gbps HBM2 \n\nBandwidth \n136.6 GB/s \n341 GB/s \n732 GB/s \n\n\nTable 5 :\n5Comparison of results of prediction of the two machine learning classification methods.Platform \nMethod \nDecision Tree \nMatNet \npre.(%) recall(%) pre.(%) recall(%) \n\nIntel \nCPU \n\nMKL \n79.669 \n86.294 \n88.137 \n96.160 \nDIA \n81.037 \n66.384 \n94.284 \n76.741 \nELL \n54.731 \n63.157 \n96.039 \n86.607 \nCOO \n67.105 \n70.223 \n92.531 \n78.636 \nOthers \n71.875 \n45.098 \n89.201 \n85.201 \n\nAMD \nCPU \n\nMKL \n75.021 \n73.972 \n93.277 \n87.763 \nDIA \n80.174 \n53.336 \n89.512 \n90.319 \nELL \n71.875 \n88.461 \n95.424 \n85.887 \nCOO \n76.288 \n86.064 \n87.570 \n93.392 \nOthers \n79.613 \n58.461 \n86.841 \n89.129 \n\nNVIDIA \nGPU \n\nCUSP \n66.327 \n28.571 \n92.5 \n74.326 \nCUSPARSE \n76.428 \n82.294 \n95.215 \n93.675 \nNSPARSE \n82.667 \n78.981 \n91.961 \n87.620 \n\n\nA and A have the same structure and different values. The values of A range from 1 to 7 and those of A are from a to \u0434.\nACKNOWLEDGMENTSWe would like to express our gratitude to all reviewers for their constructive comments. This work is supported by the National Key\nAuto-tuning of sparse matrixvector multiplication on graphics processors. International Supercomputing Conference. Walid Abu-Sufah and Asma Abdel KarimSpringerWalid Abu-Sufah and Asma Abdel Karim. 2013. Auto-tuning of sparse matrix- vector multiplication on graphics processors. In International Supercomputing Conference. Springer, 151-164.\n\nOptimizing LOBPCG: Sparse Matrix Loop and Data Transformations in Action. Khalid Ahmad, Anand Venkat, Mary Hall, International Workshop on Languages and Compilers for Parallel Computing. SpringerKhalid Ahmad, Anand Venkat, and Mary Hall. 2016. Optimizing LOBPCG: Sparse Matrix Loop and Data Transformations in Action. In International Workshop on Languages and Compilers for Parallel Computing. Springer, 218-232.\n\nAn efficient two-dimensional blocking strategy for sparse matrix-vector multiplication on GPUs. Arash Ashari, Naser Sedaghati, John Eisenlohr, P Sadayappan, Proceedings of the 28th ACM international conference on Supercomputing. the 28th ACM international conference on SupercomputingACMArash Ashari, Naser Sedaghati, John Eisenlohr, and P Sadayappan. 2014. An effi- cient two-dimensional blocking strategy for sparse matrix-vector multiplication on GPUs. In Proceedings of the 28th ACM international conference on Supercom- puting. ACM, 273-282.\n\nA modeldriven blocking strategy for load balanced sparse matrix-vector multiplication on GPUs. Arash Ashari, Naser Sedaghati, John Eisenlohr, P Sadayappan, J. Parallel and Distrib. Comput. 76Arash Ashari, Naser Sedaghati, John Eisenlohr, and P Sadayappan. 2015. A model- driven blocking strategy for load balanced sparse matrix-vector multiplication on GPUs. J. Parallel and Distrib. Comput. 76 (2015), 3-15.\n\nExploiting multiple levels of parallelism in sparse matrix-matrix multiplication. Ariful Azad, Grey Ballard, Aydin Buluc, James Demmel, Laura Grigori, Oded Schwartz, Sivan Toledo, Samuel Williams, SIAM Journal on Scientific Computing. 38Ariful Azad, Grey Ballard, Aydin Buluc, James Demmel, Laura Grigori, Oded Schwartz, Sivan Toledo, and Samuel Williams. 2016. Exploiting multiple levels of parallelism in sparse matrix-matrix multiplication. SIAM Journal on Scientific Computing 38, 6 (2016), C624-C651.\n\nHipMCL: a high-performance parallel implementation of the Markov clustering algorithm for large-scale networks. Ariful Azad, A Georgios, Christos A Pavlopoulos, Nikos C Ouzounis, Aydin Kyrpides, Bulu\u00e7, Nucleic acids research. 46Ariful Azad, Georgios A Pavlopoulos, Christos A Ouzounis, Nikos C Kyrpides, and Aydin Bulu\u00e7. 2018. HipMCL: a high-performance parallel implementation of the Markov clustering algorithm for large-scale networks. Nucleic acids research 46, 6 (2018), e33-e33.\n\n. David Bader, n. d.David Bader. [n. d.].\n\n. Blas Graph, Forum, Graph BLAS Forum. https://graphblas.org. ([n. d.]).\n\nBrief announcement: Hypergraph partitioning for parallel sparse matrix-matrix multiplication. Grey Ballard, Alex Druinsky, Nicholas Knight, Oded Schwartz, Proceedings of the 27th ACM symposium on Parallelism in Algorithms and Architectures. the 27th ACM symposium on Parallelism in Algorithms and ArchitecturesACMGrey Ballard, Alex Druinsky, Nicholas Knight, and Oded Schwartz. 2015. Brief announcement: Hypergraph partitioning for parallel sparse matrix-matrix multi- plication. In Proceedings of the 27th ACM symposium on Parallelism in Algorithms and Architectures. ACM, 86-88.\n\nReducing communication costs for sparse matrix multiplication within algebraic multigrid. Grey Ballard, Christopher Siefert, Jonathan Hu, SIAM Journal on Scientific Computing. 38Grey Ballard, Christopher Siefert, and Jonathan Hu. 2016. Reducing communi- cation costs for sparse matrix multiplication within algebraic multigrid. SIAM Journal on Scientific Computing 38, 3 (2016), C203-C231.\n\nExposing fine-grained parallelism in algebraic multigrid methods. Nathan Bell, Steven Dalton, Luke N Olson, SIAM Journal on Scientific Computing. 34Nathan Bell, Steven Dalton, and Luke N Olson. 2012. Exposing fine-grained parallelism in algebraic multigrid methods. SIAM Journal on Scientific Computing 34, 4 (2012), C123-C152.\n\nImplementing sparse matrix-vector multiplication on throughput-oriented processors. Nathan Bell, Michael Garland, Proceedings of the conference on high performance computing networking, storage and analysis. the conference on high performance computing networking, storage and analysisACM18Nathan Bell and Michael Garland. 2009. Implementing sparse matrix-vector multiplication on throughput-oriented processors. In Proceedings of the conference on high performance computing networking, storage and analysis. ACM, 18.\n\nClassification and Regression Trees. L Breiman, J Friedman, C J Stone, R A Olshen, Taylor & FrancisL. Breiman, J. Friedman, C.J. Stone, and R.A. Olshen. 1984. Classification and Regression Trees. Taylor & Francis. https://books.google.com/books?id= JwQx-WOmSyQC\n\nParallel sparse matrix-matrix multiplication and indexing: Implementation and experiments. Aydin Bulu\u00e7, John R Gilbert, SIAM Journal on Scientific Computing. 34Aydin Bulu\u00e7 and John R Gilbert. 2012. Parallel sparse matrix-matrix multiplication and indexing: Implementation and experiments. SIAM Journal on Scientific Computing 34, 4 (2012), C170-C191.\n\nSparse representation and learning in visual recognition: Theory and applications. Hong Cheng, Zicheng Liu, Lu Yang, Xuewen Chen, Signal Processing. 93Hong Cheng, Zicheng Liu, Lu Yang, and Xuewen Chen. 2013. Sparse repre- sentation and learning in visual recognition: Theory and applications. Signal Processing 93, 6 (2013), 1408-1425.\n\nModel-driven autotuning of sparse matrix-vector multiply on GPUs. W Jee, Amik Choi, Richard W Singh, Vuduc, ACM sigplan notices. ACM45Jee W Choi, Amik Singh, and Richard W Vuduc. 2010. Model-driven autotuning of sparse matrix-vector multiply on GPUs. In ACM sigplan notices, Vol. 45. ACM, 115-126.\n\nOptimizing sparse matrix\u00e2\u0102\u0164matrix multiplication for the gpu. Steven Dalton, Luke Olson, Nathan Bell, ACM Transactions on Mathematical Software (TOMS). 4125Steven Dalton, Luke Olson, and Nathan Bell. 2015. Optimizing sparse matrix\u00e2\u0102\u0164- matrix multiplication for the gpu. ACM Transactions on Mathematical Software (TOMS) 41, 4 (2015), 25.\n\nSparse matrix-matrix multiplication on the gpu. Julien Demouth, Proceedings of the GPU Technology Conference. the GPU Technology Conference3Julien Demouth. 2012. Sparse matrix-matrix multiplication on the gpu. In Pro- ceedings of the GPU Technology Conference, Vol. 3.\n\nPerformance-portable sparse matrix-matrix multiplication for many-core architectures. Mehmet Deveci, Christian Trott, Sivasankaran Rajamanickam, Parallel and Distributed Processing Symposium Workshops. IPDPSWMehmet Deveci, Christian Trott, and Sivasankaran Rajamanickam. 2017. Performance-portable sparse matrix-matrix multiplication for many-core archi- tectures. In Parallel and Distributed Processing Symposium Workshops (IPDPSW), 2017 IEEE International. IEEE, 693-702.\n\nMultithreaded sparse matrix-matrix multiplication for many-core and GPU architectures. Mehmet Deveci, Christian Trott, Sivasankaran Rajamanickam, Parallel Comput. 78Mehmet Deveci, Christian Trott, and Sivasankaran Rajamanickam. 2018. Multi- threaded sparse matrix-matrix multiplication for many-core and GPU architec- tures. Parallel Comput. 78 (2018), 33-46.\n\nSparse matrices in MATLAB: Design and implementation. Cleve John R Gilbert, Robert Moler, Schreiber, SIAM J. Matrix Anal. Appl. 13John R Gilbert, Cleve Moler, and Robert Schreiber. 1992. Sparse matrices in MATLAB: Design and implementation. SIAM J. Matrix Anal. Appl. 13, 1 (1992), 333-356.\n\nA unified framework for numerical and combinatorial computing. Steve John R Gilbert, Reinhardt, Shah, Computing in Science & Engineering. 10John R Gilbert, Steve Reinhardt, and Viral B Shah. 2008. A unified framework for numerical and combinatorial computing. Computing in Science & Engineering 10, 2 (2008).\n\nRich feature hierarchies for accurate object detection and semantic segmentation. Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionRoss Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. 2014. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition. 580-587.\n\nDeep sparse rectifier neural networks. Xavier Glorot, Antoine Bordes, Yoshua Bengio, Proceedings of the fourteenth international conference on artificial intelligence and statistics. the fourteenth international conference on artificial intelligence and statisticsXavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Deep sparse rectifier neural networks. In Proceedings of the fourteenth international conference on artificial intelligence and statistics. 315-323.\n\nGPU-accelerated sparse matrix-matrix multiplication by iterative row merging. Felix Gremse, Andreas Hofter, Lars Ole Schwen, Fabian Kiessling, Uwe Naumann, SIAM Journal on Scientific Computing. 37Felix Gremse, Andreas Hofter, Lars Ole Schwen, Fabian Kiessling, and Uwe Naumann. 2015. GPU-accelerated sparse matrix-matrix multiplication by iterative row merging. SIAM Journal on Scientific Computing 37, 1 (2015), C54-C71.\n\nTwo fast algorithms for sparse matrices: Multiplication and permuted transposition. Fred G Gustavson, ACM Transactions on Mathematical Software (TOMS). 4Fred G Gustavson. 1978. Two fast algorithms for sparse matrices: Multiplication and permuted transposition. ACM Transactions on Mathematical Software (TOMS) 4, 3 (1978), 250-269.\n\nIntel math kernel library reference manual. Intel, Tech. RepR Intel. 2019. Intel math kernel library reference manual. Technical Report. Tech. Rep.[Online]. Available: https://software.intel.com/sites/default/files/mkl-2019- developer-reference-c_0.pdf.\n\nA unified sparse matrix data format for efficient general sparse matrix-vector multiplication on modern processors with wide SIMD units. Moritz Kreutzer, Georg Hager, Gerhard Wellein, Holger Fehske, Alan R Bishop, SIAM Journal on Scientific Computing. 36Moritz Kreutzer, Georg Hager, Gerhard Wellein, Holger Fehske, and Alan R Bishop. 2014. A unified sparse matrix data format for efficient general sparse matrix-vector multiplication on modern processors with wide SIMD units. SIAM Journal on Scientific Computing 36, 5 (2014), C401-C423.\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in neural information processing systems. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifica- tion with deep convolutional neural networks. In Advances in neural information processing systems. 1097-1105.\n\nGradientbased learning applied to document recognition. Yann Lecun, L\u00e9on Bottou, Yoshua Bengio, Patrick Haffner, Proc. IEEE. 86Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient- based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278- 2324.\n\nA theoretical framework for back-propagation. Yann Lecun, G Touresky, T Hinton, Sejnowski, Proceedings of the 1988 connectionist models summer school. the 1988 connectionist models summer schoolPittsburgh, PaMorgan Kaufmann1CMUYann LeCun, D Touresky, G Hinton, and T Sejnowski. 1988. A theoretical frame- work for back-propagation. In Proceedings of the 1988 connectionist models summer school, Vol. 1. CMU, Pittsburgh, Pa: Morgan Kaufmann, 21-28.\n\nExploring and Analyzing the Real Impact of Modern On-package Memory on HPC Scientific Kernels. Ang Li, Weifeng Liu, R B Mads, Brian Kristensen, Hao Vinter, Kaixi Wang, Andres Hou, Shuaiwen Leon Marquez, Song, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '17). the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '17)2614Ang Li, Weifeng Liu, Mads R. B. Kristensen, Brian Vinter, Hao Wang, Kaixi Hou, Andres Marquez, and Shuaiwen Leon Song. 2017. Exploring and Analyzing the Real Impact of Modern On-package Memory on HPC Scientific Kernels. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '17). 26:1-26:14.\n\nSMAT: an input adaptive auto-tuner for sparse matrix-vector multiplication. Jiajia Li, Guangming Tan, Mingyu Chen, Ninghui Sun, In ACM SIGPLAN Notices. 48ACMJiajia Li, Guangming Tan, Mingyu Chen, and Ninghui Sun. 2013. SMAT: an input adaptive auto-tuner for sparse matrix-vector multiplication. In ACM SIGPLAN Notices, Vol. 48. ACM, 117-126.\n\nSparse convolutional neural networks. Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, Marianna Pensky, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionBaoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. 2015. Sparse convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 806-814.\n\nParallel and Scalable Sparse Basic Linear Algebra Subprograms. Weifeng Liu, Ph.D. Dissertation. University of CopenhagenWeifeng Liu. 2015. Parallel and Scalable Sparse Basic Linear Algebra Subprograms. Ph.D. Dissertation. University of Copenhagen.\n\nFast Synchronization-Free Algorithms for Parallel Sparse Triangular Solves with Multiple Right-Hand Sides. Weifeng Liu, Ang Li, Jonathan D Hogg, Iain S Duff, Brian Vinter, Concurrency and Computation: Practice and Experience. 294244Weifeng Liu, Ang Li, Jonathan D. Hogg, Iain S. Duff, and Brian Vinter. 2017. Fast Synchronization-Free Algorithms for Parallel Sparse Triangular Solves with Multiple Right-Hand Sides. Concurrency and Computation: Practice and Experience 29, 21 (2017), e4244-n/a.\n\nAn Efficient GPU General Sparse Matrix-Matrix Multiplication for Irregular Data. Weifeng Liu, Brian Vinter, Proceedings of the 2014 IEEE 28th International Parallel and Distributed Processing Symposium (IPDPS '14. the 2014 IEEE 28th International Parallel and Distributed Processing Symposium (IPDPS '14Weifeng Liu and Brian Vinter. 2014. An Efficient GPU General Sparse Matrix- Matrix Multiplication for Irregular Data. In Proceedings of the 2014 IEEE 28th International Parallel and Distributed Processing Symposium (IPDPS '14). 370-381.\n\nCSR5: An Efficient Storage Format for Cross-Platform Sparse Matrix-Vector Multiplication. Weifeng Liu, Brian Vinter, Proceedings of the 29th ACM International Conference on Supercomputing (ICS '15. the 29th ACM International Conference on Supercomputing (ICS '15Weifeng Liu and Brian Vinter. 2015. CSR5: An Efficient Storage Format for Cross- Platform Sparse Matrix-Vector Multiplication. In Proceedings of the 29th ACM International Conference on Supercomputing (ICS '15). 339-350.\n\nA Framework for General Sparse Matrix-Matrix Multiplication on GPUs and Heterogeneous Processors. Weifeng Liu, Brian Vinter, J. Parallel and Distrib. Comput. 85CWeifeng Liu and Brian Vinter. 2015. A Framework for General Sparse Matrix- Matrix Multiplication on GPUs and Heterogeneous Processors. J. Parallel and Distrib. Comput. 85, C (2015), 47-61.\n\nSpeculative Segmented Sum for Sparse Matrix-vector Multiplication on Heterogeneous Processors. Weifeng Liu, Brian Vinter, Parallel Comput. 49Weifeng Liu and Brian Vinter. 2015. Speculative Segmented Sum for Sparse Matrix-vector Multiplication on Heterogeneous Processors. Parallel Comput. 49, C (2015), 179-193.\n\nAutomatically tuning sparse matrix-vector multiplication for GPU architectures. Alexander Monakov, Anton Lokhmotov, Arutyun Avetisyan, International Conference on High-Performance Embedded Architectures and Compilers. SpringerAlexander Monakov, Anton Lokhmotov, and Arutyun Avetisyan. 2010. Au- tomatically tuning sparse matrix-vector multiplication for GPU architectures. In International Conference on High-Performance Embedded Architectures and Compilers. Springer, 111-125.\n\nHighperformance sparse matrix-matrix products on Intel KNL and multicore architectures. Yusuke Nagasaka, Satoshi Matsuoka, Ariful Azad, Ayd\u0131n Bulu\u00e7, arXiv:1804.01698arXiv preprintYusuke Nagasaka, Satoshi Matsuoka, Ariful Azad, and Ayd\u0131n Bulu\u00e7. 2018. High- performance sparse matrix-matrix products on Intel KNL and multicore architec- tures. arXiv preprint arXiv:1804.01698 (2018).\n\nHigh-Performance and Memory-Saving Sparse General Matrix-Matrix Multiplication for NVIDIA Pascal GPU. Yusuke Nagasaka, Akira Nukada, Satoshi Matsuoka, 2017 46th International Conference on Parallel Processing (ICPP). IEEEYusuke Nagasaka, Akira Nukada, and Satoshi Matsuoka. 2017. High-Performance and Memory-Saving Sparse General Matrix-Matrix Multiplication for NVIDIA Pascal GPU. In 2017 46th International Conference on Parallel Processing (ICPP). IEEE, 101-110.\n\nParallel efficient sparse matrix-matrix multiplication on multicore platforms. Ali Md Mostofa, Patwary, Nadathur Rajagopalan, Narayanan Satish, Jongsoo Sundaram, Park, J Michael, Anderson, Gautam Satya, Dipankar Vadlamudi, Das, G Sergey, Pudov, O Vadim, Pradeep Pirogov, Dubey, International Conference on High Performance Computing. SpringerMd Mostofa Ali Patwary, Nadathur Rajagopalan Satish, Narayanan Sundaram, Jongsoo Park, Michael J Anderson, Satya Gautam Vadlamudi, Dipankar Das, Sergey G Pudov, Vadim O Pirogov, and Pradeep Dubey. 2015. Parallel efficient sparse matrix-matrix multiplication on multicore platforms. In International Conference on High Performance Computing. Springer, 48-57.\n\nViennaCL-a high level linear algebra library for GPUs and multi-core CPUs. Karl Rupp, Florian Rudolf, Josef Weinbub, Intl. Workshop on GPUs and Scientific Applications. Karl Rupp, Florian Rudolf, and Josef Weinbub. 2010. ViennaCL-a high level linear algebra library for GPUs and multi-core CPUs. In Intl. Workshop on GPUs and Scientific Applications. 51-56.\n\nSPARSKIT: A basic tool kit for sparse matrix computations. Youcef Saad, Youcef Saad. 1990. SPARSKIT: A basic tool kit for sparse matrix computations. (1990).\n\nEvaluation of pooling operations in convolutional architectures for object recognition. Dominik Scherer, Andreas M\u00fcller, Sven Behnke, Artificial Neural Networks-ICANN 2010. SpringerDominik Scherer, Andreas M\u00fcller, and Sven Behnke. 2010. Evaluation of pooling operations in convolutional architectures for object recognition. In Artificial Neural Networks-ICANN 2010. Springer, 92-101.\n\nDeep learning in neural networks: An overview. J\u00fcrgen Schmidhuber, Neural networks. 61J\u00fcrgen Schmidhuber. 2015. Deep learning in neural networks: An overview. Neural networks 61 (2015), 85-117.\n\nAutomatic selection of sparse matrix representation on GPUs. Naser Sedaghati, Te Mu, Louis-Noel Pouchet, Srinivasan Parthasarathy, P Sadayappan, Proceedings of the 29th ACM on International Conference on Supercomputing. the 29th ACM on International Conference on SupercomputingACMNaser Sedaghati, Te Mu, Louis-Noel Pouchet, Srinivasan Parthasarathy, and P Sadayappan. 2015. Automatic selection of sparse matrix representation on GPUs. In Proceedings of the 29th ACM on International Conference on Supercomputing. ACM, 99-108.\n\nAn interactive system for combinatorial scientific computing with an emphasis on programmer productivity. B Viral, Shah, Santa BarbaraUniversity of CaliforniaViral B Shah. 2007. An interactive system for combinatorial scientific computing with an emphasis on programmer productivity. University of California, Santa Barbara.\n\nSparse matrix storage format. Fs Smailbegovic, N Georgi, Stamatis Gaydadjiev, Vassiliadis, Proceedings of the 16th Annual Workshop on Circuits, Systems and Signal Processing. the 16th Annual Workshop on Circuits, Systems and Signal ProcessingProRiscFS Smailbegovic, Georgi N Gaydadjiev, and Stamatis Vassiliadis. 2005. Sparse matrix storage format. In Proceedings of the 16th Annual Workshop on Circuits, Systems and Signal Processing, ProRisc, Vol. 2005. 445-448.\n\nclSpMV: A cross-platform OpenCL SpMV framework on GPUs. Yiing Bor, Kurt Su, Keutzer, Proceedings of the 26th ACM international conference on Supercomputing. the 26th ACM international conference on SupercomputingACMBor-Yiing Su and Kurt Keutzer. 2012. clSpMV: A cross-platform OpenCL SpMV framework on GPUs. In Proceedings of the 26th ACM international conference on Supercomputing. ACM, 353-364.\n\nDesign and Implementation of Adaptive SpMV Library for Multicore and Many-Core Architecture. Guangming Tan, Junhong Liu, Jiajia Li, ACM Trans. Math. Softw. 4425Guangming Tan, Junhong Liu, and Jiajia Li. 2018. Design and Implementation of Adaptive SpMV Library for Multicore and Many-Core Architecture. ACM Trans. Math. Softw. 44, 4 (2018), 46:1-46:25.\n\nLoop and data transformations for sparse matrix code. Anand Venkat, Mary Hall, Michelle Strout, In ACM SIGPLAN Notices. 50ACMAnand Venkat, Mary Hall, and Michelle Strout. 2015. Loop and data transforma- tions for sparse matrix code. In ACM SIGPLAN Notices, Vol. 50. ACM, 521-532.\n\nAutomating wavefront parallelization for sparse matrix computations. Anand Venkat, Jongsoo Mahdi Soltan Mohammadi, Hongbo Park, Rajkishore Rong, Michelle Mills Barik, Mary Strout, Hall, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. the International Conference for High Performance Computing, Networking, Storage and AnalysisIEEE Press41Anand Venkat, Mahdi Soltan Mohammadi, Jongsoo Park, Hongbo Rong, Rajk- ishore Barik, Michelle Mills Strout, and Mary Hall. 2016. Automating wavefront parallelization for sparse matrix computations. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE Press, 41.\n\nswSpTRSV: A Fast Sparse Triangular Solve with Sparse Level Tile Layout on Sunway Architectures. Xinliang Wang, Weifeng Liu, Wei Xue, Li Wu, Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP '18. the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP '18Xinliang Wang, Weifeng Liu, Wei Xue, and Li Wu. 2018. swSpTRSV: A Fast Sparse Triangular Solve with Sparse Level Tile Layout on Sunway Architectures. In Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP '18). 338-353.\n\nA study on several machine-learning methods for classification of malignant and benign clustered microcalcifications. Liyang Wei, Yongyi Yang, M Robert, Yulei Nishikawa, Jiang, IEEE transactions on medical imaging. 24Liyang Wei, Yongyi Yang, Robert M Nishikawa, and Yulei Jiang. 2005. A study on several machine-learning methods for classification of malignant and benign clustered microcalcifications. IEEE transactions on medical imaging 24, 3 (2005), 371-380.\n\nyaSpMV: yet another SpMV framework on GPUs. Shengen Yan, Chao Li, Yunquan Zhang, Huiyang Zhou, PPOPP. Shengen Yan, Chao Li, Yunquan Zhang, and Huiyang Zhou. 2014. yaSpMV: yet another SpMV framework on GPUs. In PPOPP.\n\nDetecting short directed cycles using rectangular matrix multiplication and dynamic programming. Raphael Yuster, Uri Zwick, Proceedings of the fifteenth annual ACM-SIAM symposium on Discrete algorithms. the fifteenth annual ACM-SIAM symposium on Discrete algorithmsRaphael Yuster and Uri Zwick. 2004. Detecting short directed cycles using rectangular matrix multiplication and dynamic programming. In Proceedings of the fifteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Mathematics, 254-260.\n\nVisualizing and understanding convolutional networks. D Matthew, Rob Zeiler, Fergus, European conference on computer vision. SpringerMatthew D Zeiler and Rob Fergus. 2014. Visualizing and understanding convolu- tional networks. In European conference on computer vision. Springer, 818-833.\n\nUnderstanding the gpu microarchitecture to achieve bare-metal performance tuning. Xiuxia Zhang, Guangming Tan, Shuangbai Xue, Jiajia Li, Keren Zhou, Mingyu Chen, ACM SIGPLAN Notices. 52Xiuxia Zhang, Guangming Tan, Shuangbai Xue, Jiajia Li, Keren Zhou, and Mingyu Chen. 2017. Understanding the gpu microarchitecture to achieve bare-metal performance tuning. ACM SIGPLAN Notices 52, 8 (2017), 31-43.\n\nBridging the gap between deep learning and sparse matrix format selection. Yue Zhao, Chunhua Liao, Jiajia Li, Xipeng Shen, Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel ProgrammingACMYue Zhao, Chunhua Liao, Jiajia Li, and Xipeng Shen. 2018. Bridging the gap between deep learning and sparse matrix format selection. In Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. ACM, 94-108.\n", "annotations": {"author": "[{\"end\":514,\"start\":228},{\"end\":519,\"start\":515},{\"end\":792,\"start\":520},{\"end\":797,\"start\":793},{\"end\":833,\"start\":798},{\"end\":1104,\"start\":834},{\"end\":1107,\"start\":1105},{\"end\":1375,\"start\":1108},{\"end\":1380,\"start\":1376},{\"end\":1653,\"start\":1381},{\"end\":1658,\"start\":1654},{\"end\":1671,\"start\":1659},{\"end\":1942,\"start\":1672}]", "publisher": null, "author_last_name": "[{\"end\":236,\"start\":233},{\"end\":533,\"start\":530},{\"end\":809,\"start\":806},{\"end\":845,\"start\":842},{\"end\":1116,\"start\":1113},{\"end\":1394,\"start\":1391},{\"end\":1670,\"start\":1667},{\"end\":1683,\"start\":1680}]", "author_first_name": "[{\"end\":232,\"start\":228},{\"end\":516,\"start\":515},{\"end\":518,\"start\":517},{\"end\":529,\"start\":520},{\"end\":794,\"start\":793},{\"end\":796,\"start\":795},{\"end\":805,\"start\":798},{\"end\":841,\"start\":834},{\"end\":1106,\"start\":1105},{\"end\":1112,\"start\":1108},{\"end\":1377,\"start\":1376},{\"end\":1379,\"start\":1378},{\"end\":1390,\"start\":1381},{\"end\":1655,\"start\":1654},{\"end\":1657,\"start\":1656},{\"end\":1666,\"start\":1659},{\"end\":1679,\"start\":1672}]", "author_affiliation": "[{\"end\":490,\"start\":257},{\"end\":513,\"start\":492},{\"end\":768,\"start\":535},{\"end\":791,\"start\":770},{\"end\":1080,\"start\":847},{\"end\":1103,\"start\":1082},{\"end\":1351,\"start\":1118},{\"end\":1374,\"start\":1353},{\"end\":1629,\"start\":1396},{\"end\":1652,\"start\":1631},{\"end\":1918,\"start\":1685},{\"end\":1941,\"start\":1920}]", "title": "[{\"end\":191,\"start\":1},{\"end\":2133,\"start\":1943}]", "venue": "[{\"end\":2197,\"start\":2135}]", "abstract": "[{\"end\":3889,\"start\":2588}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4161,\"start\":4157},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4230,\"start\":4226},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4253,\"start\":4250},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4277,\"start\":4273},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4303,\"start\":4299},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":4334,\"start\":4330},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":4360,\"start\":4356},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4403,\"start\":4400},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4617,\"start\":4613},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4661,\"start\":4657},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4685,\"start\":4681},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4708,\"start\":4705},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4723,\"start\":4719},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4738,\"start\":4734},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4775,\"start\":4771},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5802,\"start\":5798},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5805,\"start\":5802},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5999,\"start\":5995},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6130,\"start\":6126},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6133,\"start\":6130},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6136,\"start\":6133},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":6139,\"start\":6136},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":6142,\"start\":6139},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":6170,\"start\":6166},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":6173,\"start\":6170},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10238,\"start\":10234},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11449,\"start\":11445},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11495,\"start\":11491},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11836,\"start\":11832},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11855,\"start\":11851},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11858,\"start\":11855},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11922,\"start\":11918},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12193,\"start\":12192},{\"end\":14098,\"start\":14096},{\"end\":14128,\"start\":14126},{\"end\":14180,\"start\":14178},{\"end\":14355,\"start\":14352},{\"end\":14395,\"start\":14392},{\"end\":14425,\"start\":14422},{\"end\":14457,\"start\":14454},{\"end\":14475,\"start\":14472},{\"end\":14497,\"start\":14494},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14527,\"start\":14525},{\"end\":14532,\"start\":14529},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14563,\"start\":14561},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15159,\"start\":15155},{\"end\":16837,\"start\":16835},{\"end\":16877,\"start\":16875},{\"end\":16893,\"start\":16891},{\"end\":16909,\"start\":16907},{\"end\":16978,\"start\":16976},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17001,\"start\":17000},{\"end\":17006,\"start\":17003},{\"end\":17032,\"start\":17029},{\"end\":17054,\"start\":17051},{\"end\":17072,\"start\":17069},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17096,\"start\":17094},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17926,\"start\":17923},{\"end\":18179,\"start\":18177},{\"end\":18354,\"start\":18351},{\"end\":18384,\"start\":18381},{\"end\":18414,\"start\":18411},{\"end\":18462,\"start\":18459},{\"end\":18480,\"start\":18477},{\"end\":18503,\"start\":18500},{\"end\":18528,\"start\":18525},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19605,\"start\":19601},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19634,\"start\":19630},{\"end\":19660,\"start\":19659},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":29737,\"start\":29733},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":30155,\"start\":30151},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":30158,\"start\":30155},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":30161,\"start\":30158},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":30261,\"start\":30257},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":30264,\"start\":30261},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":30353,\"start\":30349},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":34559,\"start\":34555},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":34613,\"start\":34609},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":38246,\"start\":38242},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":39625,\"start\":39621},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":39634,\"start\":39630},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":42454,\"start\":42450},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":42457,\"start\":42454},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":42460,\"start\":42457},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":42581,\"start\":42578},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":42584,\"start\":42581},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":42587,\"start\":42584},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":42590,\"start\":42587},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":42751,\"start\":42748},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":42753,\"start\":42751},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":43151,\"start\":43147},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":43154,\"start\":43151},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":43226,\"start\":43223},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":43247,\"start\":43243},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":43309,\"start\":43305},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":43488,\"start\":43484},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":43502,\"start\":43498},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":43518,\"start\":43514},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":43603,\"start\":43599},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":43736,\"start\":43732},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":43739,\"start\":43736},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":43770,\"start\":43767},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":43800,\"start\":43796},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":44074,\"start\":44070},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":44140,\"start\":44136},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":44163,\"start\":44159},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":44182,\"start\":44178},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":44196,\"start\":44192},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":44733,\"start\":44730},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":44735,\"start\":44733},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":44738,\"start\":44735},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":44892,\"start\":44888},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":45019,\"start\":45015},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":45022,\"start\":45019},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":45025,\"start\":45022},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":45380,\"start\":45376},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":45423,\"start\":45419}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":46928,\"start\":46643},{\"attributes\":{\"id\":\"fig_3\"},\"end\":47373,\"start\":46929},{\"attributes\":{\"id\":\"fig_4\"},\"end\":47441,\"start\":47374},{\"attributes\":{\"id\":\"fig_6\"},\"end\":47493,\"start\":47442},{\"attributes\":{\"id\":\"fig_7\"},\"end\":47828,\"start\":47494},{\"attributes\":{\"id\":\"fig_8\"},\"end\":47909,\"start\":47829},{\"attributes\":{\"id\":\"fig_9\"},\"end\":48016,\"start\":47910},{\"attributes\":{\"id\":\"fig_10\"},\"end\":48135,\"start\":48017},{\"attributes\":{\"id\":\"fig_11\"},\"end\":48339,\"start\":48136},{\"attributes\":{\"id\":\"fig_12\"},\"end\":48424,\"start\":48340},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":48529,\"start\":48425},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":48760,\"start\":48530},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":49434,\"start\":48761},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":49910,\"start\":49435},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":50625,\"start\":49911}]", "paragraph": "[{\"end\":5353,\"start\":3905},{\"end\":6546,\"start\":5355},{\"end\":7789,\"start\":6548},{\"end\":8519,\"start\":7791},{\"end\":9664,\"start\":8521},{\"end\":10332,\"start\":9712},{\"end\":11150,\"start\":10334},{\"end\":12194,\"start\":11203},{\"end\":12586,\"start\":12252},{\"end\":13036,\"start\":12625},{\"end\":13250,\"start\":13038},{\"end\":14211,\"start\":13252},{\"end\":14564,\"start\":14315},{\"end\":16014,\"start\":14813},{\"end\":17097,\"start\":16016},{\"end\":17474,\"start\":17451},{\"end\":18180,\"start\":17482},{\"end\":18529,\"start\":18327},{\"end\":18792,\"start\":18766},{\"end\":19661,\"start\":18822},{\"end\":21897,\"start\":19688},{\"end\":24016,\"start\":21922},{\"end\":24986,\"start\":24042},{\"end\":25851,\"start\":24988},{\"end\":26804,\"start\":25853},{\"end\":26943,\"start\":26806},{\"end\":27882,\"start\":26966},{\"end\":29348,\"start\":27909},{\"end\":30056,\"start\":29350},{\"end\":30937,\"start\":30074},{\"end\":31665,\"start\":30939},{\"end\":31901,\"start\":31667},{\"end\":31907,\"start\":31903},{\"end\":32017,\"start\":31909},{\"end\":32069,\"start\":32055},{\"end\":33559,\"start\":32071},{\"end\":34016,\"start\":33561},{\"end\":34226,\"start\":34031},{\"end\":34414,\"start\":34236},{\"end\":34790,\"start\":34416},{\"end\":35461,\"start\":34792},{\"end\":35853,\"start\":35551},{\"end\":36922,\"start\":35855},{\"end\":37391,\"start\":36924},{\"end\":38834,\"start\":37422},{\"end\":41924,\"start\":38860},{\"end\":42339,\"start\":41934},{\"end\":42929,\"start\":42356},{\"end\":44739,\"start\":42931},{\"end\":46001,\"start\":44741},{\"end\":46642,\"start\":46016}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12232,\"start\":12195},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14314,\"start\":14212},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14589,\"start\":14565},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14812,\"start\":14589},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17261,\"start\":17098},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17450,\"start\":17261},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18326,\"start\":18181},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18765,\"start\":18530},{\"attributes\":{\"id\":\"formula_8\"},\"end\":32054,\"start\":32018}]", "table_ref": "[{\"end\":18982,\"start\":18975},{\"end\":19081,\"start\":19074},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":20198,\"start\":20191},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":27395,\"start\":27388},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":34333,\"start\":34326},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":38345,\"start\":38338}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3903,\"start\":3891},{\"attributes\":{\"n\":\"2\"},\"end\":9710,\"start\":9667},{\"end\":11176,\"start\":11153},{\"attributes\":{\"n\":\"2.2\"},\"end\":11201,\"start\":11179},{\"attributes\":{\"n\":\"3\"},\"end\":12250,\"start\":12234},{\"attributes\":{\"n\":\"3.1\"},\"end\":12623,\"start\":12589},{\"end\":17480,\"start\":17477},{\"attributes\":{\"n\":\"3.2\"},\"end\":18820,\"start\":18795},{\"attributes\":{\"n\":\"3.3\"},\"end\":19686,\"start\":19664},{\"attributes\":{\"n\":\"3.4\"},\"end\":21920,\"start\":21900},{\"attributes\":{\"n\":\"4\"},\"end\":24040,\"start\":24019},{\"attributes\":{\"n\":\"4.1\"},\"end\":26964,\"start\":26946},{\"attributes\":{\"n\":\"4.2\"},\"end\":27907,\"start\":27885},{\"attributes\":{\"n\":\"4.3\"},\"end\":30072,\"start\":30059},{\"attributes\":{\"n\":\"5\"},\"end\":34029,\"start\":34019},{\"attributes\":{\"n\":\"5.1\"},\"end\":34234,\"start\":34229},{\"attributes\":{\"n\":\"5.2\"},\"end\":35483,\"start\":35464},{\"attributes\":{\"n\":\"5.2.1\"},\"end\":35549,\"start\":35486},{\"attributes\":{\"n\":\"5.2.2\"},\"end\":37420,\"start\":37394},{\"attributes\":{\"n\":\"5.3\"},\"end\":38858,\"start\":38837},{\"attributes\":{\"n\":\"5.4\"},\"end\":41932,\"start\":41927},{\"attributes\":{\"n\":\"6\"},\"end\":42354,\"start\":42342},{\"attributes\":{\"n\":\"7\"},\"end\":46014,\"start\":46004},{\"end\":46654,\"start\":46644},{\"end\":46940,\"start\":46930},{\"end\":47505,\"start\":47495},{\"end\":47840,\"start\":47830},{\"end\":47921,\"start\":47911},{\"end\":48028,\"start\":48018},{\"end\":48147,\"start\":48137},{\"end\":48351,\"start\":48341},{\"end\":48540,\"start\":48531},{\"end\":48771,\"start\":48762},{\"end\":49445,\"start\":49436},{\"end\":49921,\"start\":49912}]", "table": "[{\"end\":48529,\"start\":48472},{\"end\":48760,\"start\":48596},{\"end\":49434,\"start\":48805},{\"end\":49910,\"start\":49488},{\"end\":50625,\"start\":50010}]", "figure_caption": "[{\"end\":46928,\"start\":46656},{\"end\":47373,\"start\":46942},{\"end\":47441,\"start\":47376},{\"end\":47493,\"start\":47444},{\"end\":47828,\"start\":47507},{\"end\":47909,\"start\":47842},{\"end\":48016,\"start\":47923},{\"end\":48135,\"start\":48030},{\"end\":48339,\"start\":48149},{\"end\":48424,\"start\":48353},{\"end\":48472,\"start\":48427},{\"end\":48596,\"start\":48542},{\"end\":48805,\"start\":48773},{\"end\":49488,\"start\":49447},{\"end\":50010,\"start\":49923}]", "figure_ref": "[{\"end\":4910,\"start\":4902},{\"end\":5380,\"start\":5372},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10282,\"start\":10274},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12985,\"start\":12977},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15228,\"start\":15220},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":24351,\"start\":24343},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":25579,\"start\":25571},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":28861,\"start\":28853},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":31161,\"start\":31153},{\"end\":33190,\"start\":33176},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":35259,\"start\":35251},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":35879,\"start\":35871},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":38926,\"start\":38917},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":40904,\"start\":40896}]", "bib_author_first_name": "[{\"end\":51316,\"start\":51310},{\"end\":51329,\"start\":51324},{\"end\":51342,\"start\":51338},{\"end\":51752,\"start\":51747},{\"end\":51766,\"start\":51761},{\"end\":51782,\"start\":51778},{\"end\":51795,\"start\":51794},{\"end\":52299,\"start\":52294},{\"end\":52313,\"start\":52308},{\"end\":52329,\"start\":52325},{\"end\":52342,\"start\":52341},{\"end\":52697,\"start\":52691},{\"end\":52708,\"start\":52704},{\"end\":52723,\"start\":52718},{\"end\":52736,\"start\":52731},{\"end\":52750,\"start\":52745},{\"end\":52764,\"start\":52760},{\"end\":52780,\"start\":52775},{\"end\":52795,\"start\":52789},{\"end\":53234,\"start\":53228},{\"end\":53242,\"start\":53241},{\"end\":53261,\"start\":53253},{\"end\":53263,\"start\":53262},{\"end\":53282,\"start\":53277},{\"end\":53284,\"start\":53283},{\"end\":53300,\"start\":53295},{\"end\":53609,\"start\":53604},{\"end\":53651,\"start\":53647},{\"end\":53817,\"start\":53813},{\"end\":53831,\"start\":53827},{\"end\":53850,\"start\":53842},{\"end\":53863,\"start\":53859},{\"end\":54395,\"start\":54391},{\"end\":54416,\"start\":54405},{\"end\":54434,\"start\":54426},{\"end\":54764,\"start\":54758},{\"end\":54777,\"start\":54771},{\"end\":54792,\"start\":54786},{\"end\":55111,\"start\":55105},{\"end\":55125,\"start\":55118},{\"end\":55579,\"start\":55578},{\"end\":55590,\"start\":55589},{\"end\":55602,\"start\":55601},{\"end\":55604,\"start\":55603},{\"end\":55613,\"start\":55612},{\"end\":55615,\"start\":55614},{\"end\":55900,\"start\":55895},{\"end\":55914,\"start\":55908},{\"end\":56243,\"start\":56239},{\"end\":56258,\"start\":56251},{\"end\":56266,\"start\":56264},{\"end\":56279,\"start\":56273},{\"end\":56560,\"start\":56559},{\"end\":56570,\"start\":56566},{\"end\":56584,\"start\":56577},{\"end\":56586,\"start\":56585},{\"end\":56860,\"start\":56854},{\"end\":56873,\"start\":56869},{\"end\":56887,\"start\":56881},{\"end\":57184,\"start\":57178},{\"end\":57492,\"start\":57486},{\"end\":57510,\"start\":57501},{\"end\":57530,\"start\":57518},{\"end\":57968,\"start\":57962},{\"end\":57986,\"start\":57977},{\"end\":58006,\"start\":57994},{\"end\":58295,\"start\":58290},{\"end\":58318,\"start\":58312},{\"end\":58596,\"start\":58591},{\"end\":58924,\"start\":58920},{\"end\":58939,\"start\":58935},{\"end\":58955,\"start\":58949},{\"end\":58973,\"start\":58965},{\"end\":59412,\"start\":59406},{\"end\":59428,\"start\":59421},{\"end\":59443,\"start\":59437},{\"end\":59920,\"start\":59915},{\"end\":59936,\"start\":59929},{\"end\":59949,\"start\":59945},{\"end\":59953,\"start\":59950},{\"end\":59968,\"start\":59962},{\"end\":59983,\"start\":59980},{\"end\":60991,\"start\":60985},{\"end\":61007,\"start\":61002},{\"end\":61022,\"start\":61015},{\"end\":61038,\"start\":61032},{\"end\":61051,\"start\":61047},{\"end\":61053,\"start\":61052},{\"end\":61458,\"start\":61454},{\"end\":61475,\"start\":61471},{\"end\":61495,\"start\":61487},{\"end\":61497,\"start\":61496},{\"end\":61812,\"start\":61808},{\"end\":61824,\"start\":61820},{\"end\":61839,\"start\":61833},{\"end\":61855,\"start\":61848},{\"end\":62093,\"start\":62089},{\"end\":62102,\"start\":62101},{\"end\":62114,\"start\":62113},{\"end\":62590,\"start\":62587},{\"end\":62602,\"start\":62595},{\"end\":62609,\"start\":62608},{\"end\":62611,\"start\":62610},{\"end\":62623,\"start\":62618},{\"end\":62639,\"start\":62636},{\"end\":62653,\"start\":62648},{\"end\":62666,\"start\":62660},{\"end\":62685,\"start\":62672},{\"end\":63363,\"start\":63357},{\"end\":63377,\"start\":63368},{\"end\":63389,\"start\":63383},{\"end\":63403,\"start\":63396},{\"end\":63669,\"start\":63662},{\"end\":63678,\"start\":63675},{\"end\":63691,\"start\":63685},{\"end\":63709,\"start\":63701},{\"end\":63726,\"start\":63718},{\"end\":64159,\"start\":64152},{\"end\":64452,\"start\":64445},{\"end\":64461,\"start\":64458},{\"end\":64474,\"start\":64466},{\"end\":64476,\"start\":64475},{\"end\":64487,\"start\":64483},{\"end\":64489,\"start\":64488},{\"end\":64501,\"start\":64496},{\"end\":64922,\"start\":64915},{\"end\":64933,\"start\":64928},{\"end\":65472,\"start\":65465},{\"end\":65483,\"start\":65478},{\"end\":65964,\"start\":65957},{\"end\":65975,\"start\":65970},{\"end\":66312,\"start\":66305},{\"end\":66323,\"start\":66318},{\"end\":66612,\"start\":66603},{\"end\":66627,\"start\":66622},{\"end\":66646,\"start\":66639},{\"end\":67096,\"start\":67090},{\"end\":67114,\"start\":67107},{\"end\":67131,\"start\":67125},{\"end\":67143,\"start\":67138},{\"end\":67493,\"start\":67487},{\"end\":67509,\"start\":67504},{\"end\":67525,\"start\":67518},{\"end\":67934,\"start\":67931},{\"end\":67987,\"start\":67978},{\"end\":68003,\"start\":67996},{\"end\":68021,\"start\":68020},{\"end\":68047,\"start\":68041},{\"end\":68063,\"start\":68055},{\"end\":68081,\"start\":68080},{\"end\":68098,\"start\":68097},{\"end\":68113,\"start\":68106},{\"end\":68632,\"start\":68628},{\"end\":68646,\"start\":68639},{\"end\":68660,\"start\":68655},{\"end\":68977,\"start\":68971},{\"end\":69166,\"start\":69159},{\"end\":69183,\"start\":69176},{\"end\":69196,\"start\":69192},{\"end\":69510,\"start\":69504},{\"end\":69718,\"start\":69713},{\"end\":69732,\"start\":69730},{\"end\":69747,\"start\":69737},{\"end\":69767,\"start\":69757},{\"end\":69784,\"start\":69783},{\"end\":70287,\"start\":70286},{\"end\":70554,\"start\":70553},{\"end\":70571,\"start\":70563},{\"end\":71033,\"start\":71028},{\"end\":71043,\"start\":71039},{\"end\":71472,\"start\":71463},{\"end\":71485,\"start\":71478},{\"end\":71497,\"start\":71491},{\"end\":71782,\"start\":71777},{\"end\":71795,\"start\":71791},{\"end\":71810,\"start\":71802},{\"end\":72078,\"start\":72073},{\"end\":72094,\"start\":72087},{\"end\":72125,\"start\":72119},{\"end\":72142,\"start\":72132},{\"end\":72157,\"start\":72149},{\"end\":72163,\"start\":72158},{\"end\":72175,\"start\":72171},{\"end\":72837,\"start\":72829},{\"end\":72851,\"start\":72844},{\"end\":72860,\"start\":72857},{\"end\":72868,\"start\":72866},{\"end\":73471,\"start\":73465},{\"end\":73483,\"start\":73477},{\"end\":73491,\"start\":73490},{\"end\":73505,\"start\":73500},{\"end\":73862,\"start\":73855},{\"end\":73872,\"start\":73868},{\"end\":73884,\"start\":73877},{\"end\":73899,\"start\":73892},{\"end\":74133,\"start\":74126},{\"end\":74145,\"start\":74142},{\"end\":74622,\"start\":74621},{\"end\":74635,\"start\":74632},{\"end\":74946,\"start\":74940},{\"end\":74963,\"start\":74954},{\"end\":74978,\"start\":74969},{\"end\":74990,\"start\":74984},{\"end\":75000,\"start\":74995},{\"end\":75013,\"start\":75007},{\"end\":75335,\"start\":75332},{\"end\":75349,\"start\":75342},{\"end\":75362,\"start\":75356},{\"end\":75373,\"start\":75367}]", "bib_author_last_name": "[{\"end\":51322,\"start\":51317},{\"end\":51336,\"start\":51330},{\"end\":51347,\"start\":51343},{\"end\":51759,\"start\":51753},{\"end\":51776,\"start\":51767},{\"end\":51792,\"start\":51783},{\"end\":51806,\"start\":51796},{\"end\":52306,\"start\":52300},{\"end\":52323,\"start\":52314},{\"end\":52339,\"start\":52330},{\"end\":52353,\"start\":52343},{\"end\":52702,\"start\":52698},{\"end\":52716,\"start\":52709},{\"end\":52729,\"start\":52724},{\"end\":52743,\"start\":52737},{\"end\":52758,\"start\":52751},{\"end\":52773,\"start\":52765},{\"end\":52787,\"start\":52781},{\"end\":52804,\"start\":52796},{\"end\":53239,\"start\":53235},{\"end\":53251,\"start\":53243},{\"end\":53275,\"start\":53264},{\"end\":53293,\"start\":53285},{\"end\":53309,\"start\":53301},{\"end\":53316,\"start\":53311},{\"end\":53615,\"start\":53610},{\"end\":53657,\"start\":53652},{\"end\":53664,\"start\":53659},{\"end\":53825,\"start\":53818},{\"end\":53840,\"start\":53832},{\"end\":53857,\"start\":53851},{\"end\":53872,\"start\":53864},{\"end\":54403,\"start\":54396},{\"end\":54424,\"start\":54417},{\"end\":54437,\"start\":54435},{\"end\":54769,\"start\":54765},{\"end\":54784,\"start\":54778},{\"end\":54798,\"start\":54793},{\"end\":55116,\"start\":55112},{\"end\":55133,\"start\":55126},{\"end\":55587,\"start\":55580},{\"end\":55599,\"start\":55591},{\"end\":55610,\"start\":55605},{\"end\":55622,\"start\":55616},{\"end\":55906,\"start\":55901},{\"end\":55922,\"start\":55915},{\"end\":56249,\"start\":56244},{\"end\":56262,\"start\":56259},{\"end\":56271,\"start\":56267},{\"end\":56284,\"start\":56280},{\"end\":56564,\"start\":56561},{\"end\":56575,\"start\":56571},{\"end\":56592,\"start\":56587},{\"end\":56599,\"start\":56594},{\"end\":56867,\"start\":56861},{\"end\":56879,\"start\":56874},{\"end\":56892,\"start\":56888},{\"end\":57192,\"start\":57185},{\"end\":57499,\"start\":57493},{\"end\":57516,\"start\":57511},{\"end\":57543,\"start\":57531},{\"end\":57975,\"start\":57969},{\"end\":57992,\"start\":57987},{\"end\":58019,\"start\":58007},{\"end\":58310,\"start\":58296},{\"end\":58324,\"start\":58319},{\"end\":58335,\"start\":58326},{\"end\":58611,\"start\":58597},{\"end\":58622,\"start\":58613},{\"end\":58628,\"start\":58624},{\"end\":58933,\"start\":58925},{\"end\":58947,\"start\":58940},{\"end\":58963,\"start\":58956},{\"end\":58979,\"start\":58974},{\"end\":59419,\"start\":59413},{\"end\":59435,\"start\":59429},{\"end\":59450,\"start\":59444},{\"end\":59927,\"start\":59921},{\"end\":59943,\"start\":59937},{\"end\":59960,\"start\":59954},{\"end\":59978,\"start\":59969},{\"end\":59991,\"start\":59984},{\"end\":60360,\"start\":60344},{\"end\":60642,\"start\":60637},{\"end\":61000,\"start\":60992},{\"end\":61013,\"start\":61008},{\"end\":61030,\"start\":61023},{\"end\":61045,\"start\":61039},{\"end\":61060,\"start\":61054},{\"end\":61469,\"start\":61459},{\"end\":61485,\"start\":61476},{\"end\":61504,\"start\":61498},{\"end\":61818,\"start\":61813},{\"end\":61831,\"start\":61825},{\"end\":61846,\"start\":61840},{\"end\":61863,\"start\":61856},{\"end\":62099,\"start\":62094},{\"end\":62111,\"start\":62103},{\"end\":62121,\"start\":62115},{\"end\":62132,\"start\":62123},{\"end\":62593,\"start\":62591},{\"end\":62606,\"start\":62603},{\"end\":62616,\"start\":62612},{\"end\":62634,\"start\":62624},{\"end\":62646,\"start\":62640},{\"end\":62658,\"start\":62654},{\"end\":62670,\"start\":62667},{\"end\":62693,\"start\":62686},{\"end\":62699,\"start\":62695},{\"end\":63366,\"start\":63364},{\"end\":63381,\"start\":63378},{\"end\":63394,\"start\":63390},{\"end\":63407,\"start\":63404},{\"end\":63673,\"start\":63670},{\"end\":63683,\"start\":63679},{\"end\":63699,\"start\":63692},{\"end\":63716,\"start\":63710},{\"end\":63733,\"start\":63727},{\"end\":64163,\"start\":64160},{\"end\":64456,\"start\":64453},{\"end\":64464,\"start\":64462},{\"end\":64481,\"start\":64477},{\"end\":64494,\"start\":64490},{\"end\":64508,\"start\":64502},{\"end\":64926,\"start\":64923},{\"end\":64940,\"start\":64934},{\"end\":65476,\"start\":65473},{\"end\":65490,\"start\":65484},{\"end\":65968,\"start\":65965},{\"end\":65982,\"start\":65976},{\"end\":66316,\"start\":66313},{\"end\":66330,\"start\":66324},{\"end\":66620,\"start\":66613},{\"end\":66637,\"start\":66628},{\"end\":66656,\"start\":66647},{\"end\":67105,\"start\":67097},{\"end\":67123,\"start\":67115},{\"end\":67136,\"start\":67132},{\"end\":67149,\"start\":67144},{\"end\":67502,\"start\":67494},{\"end\":67516,\"start\":67510},{\"end\":67534,\"start\":67526},{\"end\":67945,\"start\":67935},{\"end\":67954,\"start\":67947},{\"end\":67976,\"start\":67956},{\"end\":67994,\"start\":67988},{\"end\":68012,\"start\":68004},{\"end\":68018,\"start\":68014},{\"end\":68029,\"start\":68022},{\"end\":68039,\"start\":68031},{\"end\":68053,\"start\":68048},{\"end\":68073,\"start\":68064},{\"end\":68078,\"start\":68075},{\"end\":68088,\"start\":68082},{\"end\":68095,\"start\":68090},{\"end\":68104,\"start\":68099},{\"end\":68121,\"start\":68114},{\"end\":68128,\"start\":68123},{\"end\":68637,\"start\":68633},{\"end\":68653,\"start\":68647},{\"end\":68668,\"start\":68661},{\"end\":68982,\"start\":68978},{\"end\":69174,\"start\":69167},{\"end\":69190,\"start\":69184},{\"end\":69203,\"start\":69197},{\"end\":69522,\"start\":69511},{\"end\":69728,\"start\":69719},{\"end\":69735,\"start\":69733},{\"end\":69755,\"start\":69748},{\"end\":69781,\"start\":69768},{\"end\":69795,\"start\":69785},{\"end\":70293,\"start\":70288},{\"end\":70299,\"start\":70295},{\"end\":70551,\"start\":70536},{\"end\":70561,\"start\":70555},{\"end\":70582,\"start\":70572},{\"end\":70595,\"start\":70584},{\"end\":71037,\"start\":71034},{\"end\":71046,\"start\":71044},{\"end\":71055,\"start\":71048},{\"end\":71476,\"start\":71473},{\"end\":71489,\"start\":71486},{\"end\":71500,\"start\":71498},{\"end\":71789,\"start\":71783},{\"end\":71800,\"start\":71796},{\"end\":71817,\"start\":71811},{\"end\":72085,\"start\":72079},{\"end\":72117,\"start\":72095},{\"end\":72130,\"start\":72126},{\"end\":72147,\"start\":72143},{\"end\":72169,\"start\":72164},{\"end\":72182,\"start\":72176},{\"end\":72188,\"start\":72184},{\"end\":72842,\"start\":72838},{\"end\":72855,\"start\":72852},{\"end\":72864,\"start\":72861},{\"end\":72871,\"start\":72869},{\"end\":73475,\"start\":73472},{\"end\":73488,\"start\":73484},{\"end\":73498,\"start\":73492},{\"end\":73515,\"start\":73506},{\"end\":73522,\"start\":73517},{\"end\":73866,\"start\":73863},{\"end\":73875,\"start\":73873},{\"end\":73890,\"start\":73885},{\"end\":73904,\"start\":73900},{\"end\":74140,\"start\":74134},{\"end\":74151,\"start\":74146},{\"end\":74630,\"start\":74623},{\"end\":74642,\"start\":74636},{\"end\":74650,\"start\":74644},{\"end\":74952,\"start\":74947},{\"end\":74967,\"start\":74964},{\"end\":74982,\"start\":74979},{\"end\":74993,\"start\":74991},{\"end\":75005,\"start\":75001},{\"end\":75018,\"start\":75014},{\"end\":75340,\"start\":75336},{\"end\":75354,\"start\":75350},{\"end\":75365,\"start\":75363},{\"end\":75378,\"start\":75374}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6601923},\"end\":51234,\"start\":50893},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":31133175},\"end\":51649,\"start\":51236},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3946259},\"end\":52197,\"start\":51651},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":46112150},\"end\":52607,\"start\":52199},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":7755440},\"end\":53114,\"start\":52609},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":4775370},\"end\":53600,\"start\":53116},{\"attributes\":{\"id\":\"b6\"},\"end\":53643,\"start\":53602},{\"attributes\":{\"id\":\"b7\"},\"end\":53717,\"start\":53645},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":15177650},\"end\":54299,\"start\":53719},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":18274179},\"end\":54690,\"start\":54301},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14408688},\"end\":55019,\"start\":54692},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":14531936},\"end\":55539,\"start\":55021},{\"attributes\":{\"id\":\"b12\"},\"end\":55802,\"start\":55541},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":12191214},\"end\":56154,\"start\":55804},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":205168621},\"end\":56491,\"start\":56156},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":10561693},\"end\":56790,\"start\":56493},{\"attributes\":{\"id\":\"b16\"},\"end\":57128,\"start\":56792},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":59617487},\"end\":57398,\"start\":57130},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":22364822},\"end\":57873,\"start\":57400},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":46104081},\"end\":58234,\"start\":57875},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":119805771},\"end\":58526,\"start\":58236},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":35685405},\"end\":58836,\"start\":58528},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":215827080},\"end\":59365,\"start\":58838},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2239473},\"end\":59835,\"start\":59367},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":18099698},\"end\":60258,\"start\":59837},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":7107184},\"end\":60591,\"start\":60260},{\"attributes\":{\"id\":\"b26\"},\"end\":60846,\"start\":60593},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":2043345},\"end\":61387,\"start\":60848},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":195908774},\"end\":61750,\"start\":61389},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":64294544},\"end\":62041,\"start\":61752},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":16775098},\"end\":62490,\"start\":62043},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1365401},\"end\":63279,\"start\":62492},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":9158581},\"end\":63622,\"start\":63281},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1617104},\"end\":64087,\"start\":63624},{\"attributes\":{\"id\":\"b34\"},\"end\":64336,\"start\":64089},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":4646931},\"end\":64832,\"start\":64338},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":206929389},\"end\":65373,\"start\":64834},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":398288},\"end\":65857,\"start\":65375},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":5172502},\"end\":66208,\"start\":65859},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":18293182},\"end\":66521,\"start\":66210},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":206743611},\"end\":67000,\"start\":66523},{\"attributes\":{\"doi\":\"arXiv:1804.01698\",\"id\":\"b41\"},\"end\":67383,\"start\":67002},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":21005717},\"end\":67850,\"start\":67385},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":41832739},\"end\":68551,\"start\":67852},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":36302772},\"end\":68910,\"start\":68553},{\"attributes\":{\"id\":\"b45\"},\"end\":69069,\"start\":68912},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":18388506},\"end\":69455,\"start\":69071},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":11715509},\"end\":69650,\"start\":69457},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":2063521},\"end\":70178,\"start\":69652},{\"attributes\":{\"id\":\"b49\"},\"end\":70504,\"start\":70180},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":5914279},\"end\":70970,\"start\":70506},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":2968351},\"end\":71368,\"start\":70972},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":52185615},\"end\":71721,\"start\":71370},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":14138730},\"end\":72002,\"start\":71723},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":31059518},\"end\":72731,\"start\":72004},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":3553349},\"end\":73345,\"start\":72733},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":36691320},\"end\":73809,\"start\":73347},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":1493982},\"end\":74027,\"start\":73811},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":6696796},\"end\":74565,\"start\":74029},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":3960646},\"end\":74856,\"start\":74567},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":18757573},\"end\":75255,\"start\":74858},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":3439616},\"end\":75808,\"start\":75257}]", "bib_title": "[{\"end\":50965,\"start\":50893},{\"end\":51308,\"start\":51236},{\"end\":51745,\"start\":51651},{\"end\":52292,\"start\":52199},{\"end\":52689,\"start\":52609},{\"end\":53226,\"start\":53116},{\"end\":53811,\"start\":53719},{\"end\":54389,\"start\":54301},{\"end\":54756,\"start\":54692},{\"end\":55103,\"start\":55021},{\"end\":55893,\"start\":55804},{\"end\":56237,\"start\":56156},{\"end\":56557,\"start\":56493},{\"end\":56852,\"start\":56792},{\"end\":57176,\"start\":57130},{\"end\":57484,\"start\":57400},{\"end\":57960,\"start\":57875},{\"end\":58288,\"start\":58236},{\"end\":58589,\"start\":58528},{\"end\":58918,\"start\":58838},{\"end\":59404,\"start\":59367},{\"end\":59913,\"start\":59837},{\"end\":60342,\"start\":60260},{\"end\":60983,\"start\":60848},{\"end\":61452,\"start\":61389},{\"end\":61806,\"start\":61752},{\"end\":62087,\"start\":62043},{\"end\":62585,\"start\":62492},{\"end\":63355,\"start\":63281},{\"end\":63660,\"start\":63624},{\"end\":64443,\"start\":64338},{\"end\":64913,\"start\":64834},{\"end\":65463,\"start\":65375},{\"end\":65955,\"start\":65859},{\"end\":66303,\"start\":66210},{\"end\":66601,\"start\":66523},{\"end\":67485,\"start\":67385},{\"end\":67929,\"start\":67852},{\"end\":68626,\"start\":68553},{\"end\":69157,\"start\":69071},{\"end\":69502,\"start\":69457},{\"end\":69711,\"start\":69652},{\"end\":70534,\"start\":70506},{\"end\":71026,\"start\":70972},{\"end\":71461,\"start\":71370},{\"end\":71775,\"start\":71723},{\"end\":72071,\"start\":72004},{\"end\":72827,\"start\":72733},{\"end\":73463,\"start\":73347},{\"end\":73853,\"start\":73811},{\"end\":74124,\"start\":74029},{\"end\":74619,\"start\":74567},{\"end\":74938,\"start\":74858},{\"end\":75330,\"start\":75257}]", "bib_author": "[{\"end\":51324,\"start\":51310},{\"end\":51338,\"start\":51324},{\"end\":51349,\"start\":51338},{\"end\":51761,\"start\":51747},{\"end\":51778,\"start\":51761},{\"end\":51794,\"start\":51778},{\"end\":51808,\"start\":51794},{\"end\":52308,\"start\":52294},{\"end\":52325,\"start\":52308},{\"end\":52341,\"start\":52325},{\"end\":52355,\"start\":52341},{\"end\":52704,\"start\":52691},{\"end\":52718,\"start\":52704},{\"end\":52731,\"start\":52718},{\"end\":52745,\"start\":52731},{\"end\":52760,\"start\":52745},{\"end\":52775,\"start\":52760},{\"end\":52789,\"start\":52775},{\"end\":52806,\"start\":52789},{\"end\":53241,\"start\":53228},{\"end\":53253,\"start\":53241},{\"end\":53277,\"start\":53253},{\"end\":53295,\"start\":53277},{\"end\":53311,\"start\":53295},{\"end\":53318,\"start\":53311},{\"end\":53617,\"start\":53604},{\"end\":53659,\"start\":53647},{\"end\":53666,\"start\":53659},{\"end\":53827,\"start\":53813},{\"end\":53842,\"start\":53827},{\"end\":53859,\"start\":53842},{\"end\":53874,\"start\":53859},{\"end\":54405,\"start\":54391},{\"end\":54426,\"start\":54405},{\"end\":54439,\"start\":54426},{\"end\":54771,\"start\":54758},{\"end\":54786,\"start\":54771},{\"end\":54800,\"start\":54786},{\"end\":55118,\"start\":55105},{\"end\":55135,\"start\":55118},{\"end\":55589,\"start\":55578},{\"end\":55601,\"start\":55589},{\"end\":55612,\"start\":55601},{\"end\":55624,\"start\":55612},{\"end\":55908,\"start\":55895},{\"end\":55924,\"start\":55908},{\"end\":56251,\"start\":56239},{\"end\":56264,\"start\":56251},{\"end\":56273,\"start\":56264},{\"end\":56286,\"start\":56273},{\"end\":56566,\"start\":56559},{\"end\":56577,\"start\":56566},{\"end\":56594,\"start\":56577},{\"end\":56601,\"start\":56594},{\"end\":56869,\"start\":56854},{\"end\":56881,\"start\":56869},{\"end\":56894,\"start\":56881},{\"end\":57194,\"start\":57178},{\"end\":57501,\"start\":57486},{\"end\":57518,\"start\":57501},{\"end\":57545,\"start\":57518},{\"end\":57977,\"start\":57962},{\"end\":57994,\"start\":57977},{\"end\":58021,\"start\":57994},{\"end\":58312,\"start\":58290},{\"end\":58326,\"start\":58312},{\"end\":58337,\"start\":58326},{\"end\":58613,\"start\":58591},{\"end\":58624,\"start\":58613},{\"end\":58630,\"start\":58624},{\"end\":58935,\"start\":58920},{\"end\":58949,\"start\":58935},{\"end\":58965,\"start\":58949},{\"end\":58981,\"start\":58965},{\"end\":59421,\"start\":59406},{\"end\":59437,\"start\":59421},{\"end\":59452,\"start\":59437},{\"end\":59929,\"start\":59915},{\"end\":59945,\"start\":59929},{\"end\":59962,\"start\":59945},{\"end\":59980,\"start\":59962},{\"end\":59993,\"start\":59980},{\"end\":60362,\"start\":60344},{\"end\":60644,\"start\":60637},{\"end\":61002,\"start\":60985},{\"end\":61015,\"start\":61002},{\"end\":61032,\"start\":61015},{\"end\":61047,\"start\":61032},{\"end\":61062,\"start\":61047},{\"end\":61471,\"start\":61454},{\"end\":61487,\"start\":61471},{\"end\":61506,\"start\":61487},{\"end\":61820,\"start\":61808},{\"end\":61833,\"start\":61820},{\"end\":61848,\"start\":61833},{\"end\":61865,\"start\":61848},{\"end\":62101,\"start\":62089},{\"end\":62113,\"start\":62101},{\"end\":62123,\"start\":62113},{\"end\":62134,\"start\":62123},{\"end\":62595,\"start\":62587},{\"end\":62608,\"start\":62595},{\"end\":62618,\"start\":62608},{\"end\":62636,\"start\":62618},{\"end\":62648,\"start\":62636},{\"end\":62660,\"start\":62648},{\"end\":62672,\"start\":62660},{\"end\":62695,\"start\":62672},{\"end\":62701,\"start\":62695},{\"end\":63368,\"start\":63357},{\"end\":63383,\"start\":63368},{\"end\":63396,\"start\":63383},{\"end\":63409,\"start\":63396},{\"end\":63675,\"start\":63662},{\"end\":63685,\"start\":63675},{\"end\":63701,\"start\":63685},{\"end\":63718,\"start\":63701},{\"end\":63735,\"start\":63718},{\"end\":64165,\"start\":64152},{\"end\":64458,\"start\":64445},{\"end\":64466,\"start\":64458},{\"end\":64483,\"start\":64466},{\"end\":64496,\"start\":64483},{\"end\":64510,\"start\":64496},{\"end\":64928,\"start\":64915},{\"end\":64942,\"start\":64928},{\"end\":65478,\"start\":65465},{\"end\":65492,\"start\":65478},{\"end\":65970,\"start\":65957},{\"end\":65984,\"start\":65970},{\"end\":66318,\"start\":66305},{\"end\":66332,\"start\":66318},{\"end\":66622,\"start\":66603},{\"end\":66639,\"start\":66622},{\"end\":66658,\"start\":66639},{\"end\":67107,\"start\":67090},{\"end\":67125,\"start\":67107},{\"end\":67138,\"start\":67125},{\"end\":67151,\"start\":67138},{\"end\":67504,\"start\":67487},{\"end\":67518,\"start\":67504},{\"end\":67536,\"start\":67518},{\"end\":67947,\"start\":67931},{\"end\":67956,\"start\":67947},{\"end\":67978,\"start\":67956},{\"end\":67996,\"start\":67978},{\"end\":68014,\"start\":67996},{\"end\":68020,\"start\":68014},{\"end\":68031,\"start\":68020},{\"end\":68041,\"start\":68031},{\"end\":68055,\"start\":68041},{\"end\":68075,\"start\":68055},{\"end\":68080,\"start\":68075},{\"end\":68090,\"start\":68080},{\"end\":68097,\"start\":68090},{\"end\":68106,\"start\":68097},{\"end\":68123,\"start\":68106},{\"end\":68130,\"start\":68123},{\"end\":68639,\"start\":68628},{\"end\":68655,\"start\":68639},{\"end\":68670,\"start\":68655},{\"end\":68984,\"start\":68971},{\"end\":69176,\"start\":69159},{\"end\":69192,\"start\":69176},{\"end\":69205,\"start\":69192},{\"end\":69524,\"start\":69504},{\"end\":69730,\"start\":69713},{\"end\":69737,\"start\":69730},{\"end\":69757,\"start\":69737},{\"end\":69783,\"start\":69757},{\"end\":69797,\"start\":69783},{\"end\":70295,\"start\":70286},{\"end\":70301,\"start\":70295},{\"end\":70553,\"start\":70536},{\"end\":70563,\"start\":70553},{\"end\":70584,\"start\":70563},{\"end\":70597,\"start\":70584},{\"end\":71039,\"start\":71028},{\"end\":71048,\"start\":71039},{\"end\":71057,\"start\":71048},{\"end\":71478,\"start\":71463},{\"end\":71491,\"start\":71478},{\"end\":71502,\"start\":71491},{\"end\":71791,\"start\":71777},{\"end\":71802,\"start\":71791},{\"end\":71819,\"start\":71802},{\"end\":72087,\"start\":72073},{\"end\":72119,\"start\":72087},{\"end\":72132,\"start\":72119},{\"end\":72149,\"start\":72132},{\"end\":72171,\"start\":72149},{\"end\":72184,\"start\":72171},{\"end\":72190,\"start\":72184},{\"end\":72844,\"start\":72829},{\"end\":72857,\"start\":72844},{\"end\":72866,\"start\":72857},{\"end\":72873,\"start\":72866},{\"end\":73477,\"start\":73465},{\"end\":73490,\"start\":73477},{\"end\":73500,\"start\":73490},{\"end\":73517,\"start\":73500},{\"end\":73524,\"start\":73517},{\"end\":73868,\"start\":73855},{\"end\":73877,\"start\":73868},{\"end\":73892,\"start\":73877},{\"end\":73906,\"start\":73892},{\"end\":74142,\"start\":74126},{\"end\":74153,\"start\":74142},{\"end\":74632,\"start\":74621},{\"end\":74644,\"start\":74632},{\"end\":74652,\"start\":74644},{\"end\":74954,\"start\":74940},{\"end\":74969,\"start\":74954},{\"end\":74984,\"start\":74969},{\"end\":74995,\"start\":74984},{\"end\":75007,\"start\":74995},{\"end\":75020,\"start\":75007},{\"end\":75342,\"start\":75332},{\"end\":75356,\"start\":75342},{\"end\":75367,\"start\":75356},{\"end\":75380,\"start\":75367}]", "bib_venue": "[{\"end\":51006,\"start\":50967},{\"end\":51421,\"start\":51349},{\"end\":51878,\"start\":51808},{\"end\":52386,\"start\":52355},{\"end\":52842,\"start\":52806},{\"end\":53340,\"start\":53318},{\"end\":53958,\"start\":53874},{\"end\":54475,\"start\":54439},{\"end\":54836,\"start\":54800},{\"end\":55227,\"start\":55135},{\"end\":55576,\"start\":55541},{\"end\":55960,\"start\":55924},{\"end\":56303,\"start\":56286},{\"end\":56620,\"start\":56601},{\"end\":56942,\"start\":56894},{\"end\":57238,\"start\":57194},{\"end\":57600,\"start\":57545},{\"end\":58036,\"start\":58021},{\"end\":58362,\"start\":58337},{\"end\":58664,\"start\":58630},{\"end\":59058,\"start\":58981},{\"end\":59548,\"start\":59452},{\"end\":60029,\"start\":59993},{\"end\":60410,\"start\":60362},{\"end\":60635,\"start\":60593},{\"end\":61098,\"start\":61062},{\"end\":61555,\"start\":61506},{\"end\":61875,\"start\":61865},{\"end\":62192,\"start\":62134},{\"end\":62818,\"start\":62701},{\"end\":63431,\"start\":63409},{\"end\":63812,\"start\":63735},{\"end\":64150,\"start\":64089},{\"end\":64562,\"start\":64510},{\"end\":65046,\"start\":64942},{\"end\":65571,\"start\":65492},{\"end\":66015,\"start\":65984},{\"end\":66347,\"start\":66332},{\"end\":66739,\"start\":66658},{\"end\":67088,\"start\":67002},{\"end\":67600,\"start\":67536},{\"end\":68184,\"start\":68130},{\"end\":68720,\"start\":68670},{\"end\":68969,\"start\":68912},{\"end\":69242,\"start\":69205},{\"end\":69539,\"start\":69524},{\"end\":69870,\"start\":69797},{\"end\":70284,\"start\":70180},{\"end\":70679,\"start\":70597},{\"end\":71127,\"start\":71057},{\"end\":71524,\"start\":71502},{\"end\":71841,\"start\":71819},{\"end\":72298,\"start\":72190},{\"end\":72980,\"start\":72873},{\"end\":73560,\"start\":73524},{\"end\":73911,\"start\":73906},{\"end\":74230,\"start\":74153},{\"end\":74690,\"start\":74652},{\"end\":75039,\"start\":75020},{\"end\":75476,\"start\":75380},{\"end\":51935,\"start\":51880},{\"end\":54029,\"start\":53960},{\"end\":55306,\"start\":55229},{\"end\":57269,\"start\":57240},{\"end\":59122,\"start\":59060},{\"end\":59631,\"start\":59550},{\"end\":62251,\"start\":62194},{\"end\":62922,\"start\":62820},{\"end\":63876,\"start\":63814},{\"end\":65137,\"start\":65048},{\"end\":65637,\"start\":65573},{\"end\":69930,\"start\":69872},{\"end\":70755,\"start\":70681},{\"end\":71184,\"start\":71129},{\"end\":72393,\"start\":72300},{\"end\":73074,\"start\":72982},{\"end\":74294,\"start\":74232},{\"end\":75559,\"start\":75478}]"}}}, "year": 2023, "month": 12, "day": 17}