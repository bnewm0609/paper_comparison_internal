{"id": 258291930, "updated": "2023-10-09 05:34:59.913", "metadata": {"title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models", "authors": "[{\"first\":\"Deyao\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Xiaoqian\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Xiang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Mohamed\",\"last\":\"Elhoseiny\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2304.10592", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2304-10592", "doi": "10.48550/arxiv.2304.10592"}}, "content": {"source": {"pdf_hash": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2304.10592v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "aaee5e58f98554dd7dbd08b7c92a7560cf03335e", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/ca6a2bc279be5a3349a22bfd6866ed633d18734b.txt", "contents": "\nMINIGPT-4: ENHANCING VISION-LANGUAGE UNDERSTANDING WITH ADVANCED LARGE LANGUAGE MODELS\n2 Oct 2023\n\nDeyao Zhu deyao.zhu@kaust.edu.sa \nKing Abdullah University of Science and Technology\n\n\nJun Chen jun.chen@kaust.edu.sa \nKing Abdullah University of Science and Technology\n\n\nXiaoqian Shen xiaoqian.shen@kaust.edu.sa \nKing Abdullah University of Science and Technology\n\n\nXiang Li xiang.li.1@kaust.edu.sa \nKing Abdullah University of Science and Technology\n\n\nMohamed Elhoseiny mohamed.elhoseiny@kaust.edu.sa \nKing Abdullah University of Science and Technology\n\n\nMINIGPT-4: ENHANCING VISION-LANGUAGE UNDERSTANDING WITH ADVANCED LARGE LANGUAGE MODELS\n2 Oct 20235D5ACB005D7DE904C7A15F632E30472CarXiv:2304.10592v2[cs.CV]\nThe recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images.These features are rarely observed in previous visionlanguage models.However, the technical details behind GPT-4 continue to remain undisclosed.We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM).To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer.Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts.Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on.In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation).To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability.Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/.\n\nINTRODUCTION\n\nIn recent years, large language models (LLMs) have experienced rapid advancements (Ouyang et al., 2022;OpenAI, 2022;Brown et al., 2020;Scao et al., 2022a;Touvron et al., 2023;Chowdhery et al., 2022;Hoffmann et al., 2022).With exceptional language understanding capabilities, these models can perform a variety of intricate linguistic tasks in a zero-shot manner.Notably, GPT-4, a large-scale multimodal model, has been recently introduced and demonstrated several impressive capabilities of vision-language understanding and generation (OpenAI, 2023).For example, GPT-4 can produce detailed and accurate image descriptions, explain unusual visual phenomena, and even construct websites based on handwritten text instructions.\n\nAlthough GPT-4 has exhibited remarkable vision language capabilities, the methods behind its exceptional abilities are still a mystery (OpenAI, 2023).We believe that these impressive skills may stem from the utilization of a more advanced large language model (LLM).LLMs have demonstrated various emergent abilities, as evidenced in GPT-3's few-shot prompting setup (Brown et al., 2020) and the findings of Wei et al. (2022) (Wei et al., 2022).Such emergent properties are hard to find in smaller-scale models.It is conjectured that these emergent abilities are also applicable to multi-modal models, which could be the foundation of GPT-4's impressive visual description capabilities.\n\nTo substantiate our hypothesis, we present a novel vision-language model named MiniGPT-4.It utilizes an advanced large language model (LLM), Vicuna (Chiang et al., 2023), which is built upon LLaMA (Touvron et al., 2023) and reported to achieve 90% of ChatGPT's quality as per GPT-4's evaluation, as the language decoder.In terms of visual perception, we employ the same pretrained vision components of BLIP-2 (Li et al., 2023) that consists of a ViT-G/14 from EVA-CLIP (Fang et al., 2022) and a Q-Former network.MiniGPT-4 adds a single projection layer to align the encoded visual features with the Vicuna language model and freezes all the other vision and language components.MiniGPT-4 is initially trained for 20k steps using a batch size of 256 on 4 A100 GPUs, leveraging a combined image captioning dataset that includes images from LAION (Schuhmann et al., 2021), Conceptual Captions (Changpinyo et al., 2021;Sharma et al., 2018), and SBU (Ordonez et al., 2011) to align visual features with the Vicuna language model.Nevertheless, merely aligning visual features with the language model (LLM) is inadequate to ensure robust visual conversation capabilities, resembling that of a chatbot.The presence of underlying noise in raw image-text pairs can lead to subpar language outputs.Therefore, we collect another 3,500 detailed image description pairs to further fine-tune the model with a designed conversational template in order to improve the naturalness of the generated language and its usability.\n\nIn our experiments, we discovered that MiniGPT-4 possesses numerous capabilities similar to those demonstrated by GPT-4.For instance, MiniGPT-4 can generate intricate image descriptions, create websites based on handwritten text instructions, and explain unusual visual phenomena.Furthermore, our findings revealed that MiniGPT-4 also has a variety of other intriguing abilities not showcased in the GPT-4 demonstrations.For example, MiniGPT-4 can directly generate detailed cooking recipes from food photos, write stories or poems inspired by images, write advertisements for products in images, identify problems shown in photos and provide corresponding solutions, and retrieve rich facts about people, movies, or art directly from images, among other capabilities.These abilities are absent in previous vision-language models like Kosmos-1 (Huang et al., 2023) and BLIP-2 (Li et al., 2023) that use less powerful language models.This further validates that integrating visual features with an advanced language model is one of the keys to enhancing vision-language models.\n\nWe present a summary of our key findings:\n\n\u2022 Our research reveals with compelling evidence that by aligning visual features with advanced large language models like Vicuna, MiniGPT-4 can achieve advanced vision-language capabilities comparable to those exhibited in the GPT-4 demonstrations.\n\n\u2022 Our findings suggest that training merely one projection layer can effectively align a pretrained vision encoder with the large language model.Our MiniGPT-4 only requires training approximately 10 hours on 4 A100 GPUs.\n\n\u2022 We discovered that simply aligning visual features with large language models using short image caption pairs is not sufficient for developing a well-performing model and leads to unnatural language generation.Further finetuning with a small but detailed image description pairs can address this limitation and significantly improves its usability.\n\n\nRELATED WORKS\n\nLarge language models have experienced tremendous success in recent years due to the scaling up of training data and an increase in the number of parameters.Early models, such as BERT (Devlin et al., 2018), GPT-2 (Radford et al., 2019), and T5 (Raffel et al., 2020), laid the foundation for this progress.Subsequently, GPT-3 (Brown et al., 2020), with a massive scale of 175 billion parameters, was introduced, demonstrating significant breakthroughs across numerous language benchmarks.This development inspired the creation of various other large language models, including Megatron-Turing NLG (Smith et al., 2022), Chinchilla (Hoffmann et al., 2022), PaLM (Chowdhery et al., 2022), OPT (Zhang et al., 2022), BLOOM (Scao et al., 2022b), andLLaMA (Touvron et al., 2023), among others.Wei et al. (Wei et al., 2022) further discovered several emergent abilities, which appear exclusively in large models.The emergence of these abilities underscores the importance of scaling up in the development of large language models.Moreover, by aligning the pre-trained large language model GPT-3 with human intent, instructions and human feedback, InstructGPT (Ouyang et al., 2022) and ChatGPT (OpenAI, 2022) enable conversational interactions with humans and can answer a wide range of diverse and complex questions.More recently, several open-sourced models, such as Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023), have been developed based on LLaMA (Touvron et al., 2023) and also exhibit similar performance.\n\nLeveraging Pre-trained LLMs in Vision-Language Tasks.In recent years, the trend of using autoregressive language models as decoders in vision-language tasks has gained significant traction (Chen et al., 2022;Huang et al., 2023;Yang et al., 2022;Tiong et al., 2022;Alayrac et al., 2022;Li et al., 2023;2022;Driess et al., 2023).This approach takes advantage of cross-modal transfer, allowing knowledge to be shared between language and multimodal domains.Pioneering studies like VisualGPT (Chen et al., 2022) and Frozen (Tsimpoukelli et al., 2021) have demonstrated the benefits of employing a pre-trained language model as a vision-language model decoder.Flamingo (Alayrac et al., 2022) was then developed to align a pre-trained vision encoder and language model using gated cross-attention, and was trained on billions of image-text pairs, showcasing impressive in-context few-shot learning capabilities.Following that, BLIP-2 (Li et al., 2023) was introduced, employing a Flan-T5 (Chung et al., 2022) with a Q-Former to efficiently align visual features with the language model.Most recently, PaLM-E (Driess et al., 2023), featuring 562 billion parameters, has been developed to integrate real-world continuous sensor modalities into an LLM, thereby establishing a connection between real-world perceptions and human languages.GPT-4 (OpenAI, 2023) has also been recently released, showcasing more powerful visual understanding and reasoning abilities after pre-training on a vast collection of aligned image-text data.\n\nLLMs, such as ChatGPT, have proven to be powerful tools in enhancing the performance of visionlanguage tasks by collaborating with other specialized models.For instance, Visual ChatGPT (Wu et al., 2023) and MM-REACT (Yang* et al., 2023) showcase how ChatGPT can act as a coordinator, integrating with diverse visual foundation models and facilitating their collaboration to tackle more complex challenges.ChatCaptioner (Zhu et al., 2023) treats ChatGPT as a questioner, prompting diverse questions for BLIP-2 to answer.Through multi-round conversations, ChatGPT extracts visual information from BLIP-2 and effectively summarizes the image content.Video ChatCaptioner (Chen et al., 2023) extends this approach, applying it to video spatiotemporal understanding.\n\nViperGPT (Sur\u00eds et al., 2023) demonstrates the potential of combining an LLM with different vision models to address complex visual queries programmatically.In contrast, MiniGPT-4 directly aligns visual information with the language model to accomplish diverse vision-language tasks without the usage of external vision models.\n\n\nMETHOD\n\nMiniGPT-4 aims to align visual information from a pretrained vision encoder with an advanced large language model (LLM).Specifically, we utilize the Vicuna (Chiang et al., 2023) as our language decoder, which is constructed upon LLaMA (Touvron et al., 2023) and can perform a wide range of complex linguistic tasks.For visual perception, we employ the same visual encoder as used in Preprint BLIP-2 (Li et al., 2023), a ViT backbone (Fang et al., 2022) coupled with their pre-trained Q-Former.Both language and vision models are open-sourced.We target to bridge the gap between the visual encoder and LLM using a linear projection layer, with an overview of our model displayed in Fig. 1.\n\nTo achieve an effective MiniGPT-4, we propose a two-stage training approach.The initial stage involves pretraining the model on a large collection of aligned image-text pairs to acquire visionlanguage knowledge.In the second stage, we finetune the pretrained model with a smaller but high-quality image-text dataset with a designed conversational template to enhance generation reliability and usability.\n\n\nFIRST PRETRAINING STAGE\n\nDuring the initial pretraining stage, the model is designed to acquire vision-language knowledge from a large collection of aligned image-text pairs.We regard the output from the injected projection layer as a soft prompt for the LLM, prompting it to generate the corresponding ground-truth texts.\n\nThroughout the entire pretraining process, both the pretrained vision encoder and the LLM remain frozen, with only the linear projection layer being pretrained.We use a combined dataset of Conceptual Caption (Changpinyo et al., 2021;Sharma et al., 2018), SBU (Ordonez et al., 2011) and LAION (Schuhmann et al., 2021) to train our model.Our model undergoes 20,000 training steps with a batch size of 256, covering approximately 5 million image-text pairs.The entire process takes about 10 hours to complete, utilizing 4 A100 (80GB) GPUs.\n\nIssues of the first pretraining stage Following the first pretraining stage, our MiniGPT-4 demonstrates the capacity to possess a wealth of knowledge and offer reasonable responses to human inquiries.However, we have observed instances where it produces incoherent linguistic outputs, such as repetitive words or sentences, fragmented sentences, or irrelevant content.These issues hinder MiniGPT-4's ability to engage in a fluent visual conversation with humans.\n\nWe also observed similar challenges encountered in GPT-3.Despite its pretraining on a extensive language dataset, GPT-3 struggles to generate language outputs that are accurately aligned with users' intentions.Through a process of instruction fine-tuning and reinforcement learning from human feedback, GPT-3 evolves into GPT-3.5 (Ouyang et al., 2022;OpenAI, 2022) and becomes capable of producing more human-friendly outputs.This phenomenon bears a resemblance to the current state of MiniGPT-4 following its initial pretraining stage.As such, it is not surprising that our model may struggle to generate fluent and natural human language outputs at this stage.\n\n\nCURATING A HIGH-QUALITY ALIGNMENT DATASET FOR VISION-LANGUAGE DOMAIN.\n\nTo achieve greater naturalness in the generated language and enhance the model's usability, a secondstage alignment process is essential.While in the realm of NLP, instruction fine-tuning datasets (Taori et al., 2023) and conversations (sha, 2023) are easily accessible, no equivalent datasets exist for the vision-language domain.To address this deficiency, we carefully curated a detailed image description dataset, specifically tailored for vision-language alignment purposes.This dataset is subsequently utilized to fine-tune our MiniGPT-4 during the second-stage alignment process.\n\n\nInitial aligned image-text generation\n\nIn the initial phase, we employ the model derived from the first pretraining stage to generate comprehensive descriptions of input images.To enable our model to produce more detailed image descriptions, we designed a prompt that adheres to the conversational format of the Vicuna (Chiang et al., 2023) language model, as shown below.In this prompt, <ImageFeature> represents the visual features produced by the linear projection layer.\n\n###Human: <Img><ImageFeature></Img>Describe this image in detail.Give as many details as possible.Say everything you see.###Assistant:\n\nTo identify incomplete sentences, we examine whether the generated sentence exceeds 80 tokens.If it does not, we incorporate an additional prompt, ###Human: Continue ###Assistant: , prompting our MiniGPT-4 to extend the generation process.By concatenating the outputs from both steps, we can create a more comprehensive image description.This approach enables us to generate image-text pairs with detailed and informative image descriptions.We randomly select 5,000 images from the Preprint Conceptual Caption dataset (Changpinyo et al., 2021;Sharma et al., 2018) and use the pretrained model to generate corresponding language descriptions for each image.\n\nData post-processing The above automatically generated image descriptions contain noisy or incoherent descriptions, such as repetition of words or sentences, fragmented sentences, or irrelevant content.In order to fix these issues, we employ ChatGPT to mend the descriptions by utilizing the following prompt:\n\nFix the error in the given paragraph.Remove any repeating sentences, meaningless characters, not English sentences, and so on.Remove unnecessary repetition.Rewrite any incomplete sentences.\n\nReturn directly the results without explanation.Return directly the input paragraph if it is already correct without explanation.\n\nUpon completing the post-processing stage, we manually verify the correctness of each image description to guarantee its high quality.Specifically, we first identified several frequently shown errors (\"I'm sorry I made a mistake...\", or \"I apologize for that ...\") and then hard-coded rules to automatically filter them out.We also manually refine the generated captions by eliminating redundant words or sentences that ChatGPT fails to detect.Finally, only approximately 3,500 out of 5,000 image-text pairs satisfy our requirement, and these pairs are subsequently utilized for the second-stage alignment process.\n\n\nSECOND-STAGE FINETUNING\n\nDuring the second stage, we finetune our pretrained model with the curated high-quality image-text pairs.During the finetuning, we use the predefined prompts in the following template:\n###Human: <Img><ImageFeature></Img><Instruction>###Assistant:\nIn this prompt, <Instruction> represents a randomly sampled instruction from our predefined instruction set containing variant forms of instructions such as \"Describe this image in detail\" or \"Could you describe the contents of this image for me\".It is important to note that we do not calculate the regression loss for this specific text-image prompt.\n\nAs a result, MiniGPT-4 is now capable of producing more natural and reliable language outputs.Furthermore, we observed that this fine-tuning process is remarkably efficient, only requiring a mere 400 training steps with a batch size of 12, which takes around 7 minutes with a single A100 GPU.\n\n\nEXPERIMENTS\n\nIn the experiment, we aim to showcase the diverse and emergent capabilities of our MiniGPT-4 model through various qualitative examples.These abilities include generating detailed image descriptions, identifying amusing aspects within memes, providing food recipes from photos, writing poems for images, etc.Additionally, we present quantitative results on the task of image captioning.\n\n\nUNCOVERING EMERGENT ABILITIES WITH MINIGPT-4 THROUGH QUALITATIVE\n\n\nEXAMPLES\n\nMiniGPT-4 demonstrates many advanced abilities compared to traditional vision-language models.For example, it can describe images in detail and interpret the humorous aspects of a given meme.\n\nHere, we qualitatively compared our model to one of the leading vision-language models, BLIP-2 (Li et al., 2023), with eight distinct examples, each highlighting a different ability.\n\nAn example in Fig. 2 demonstrates that MiniGPT-4 effectively identifies various elements within the image, such as busy city streets, clock towers, shops, restaurants, motorcycles, people, streetlights, and clouds.In contrast, BLIP-2 can only cover city streets, people, and motorcycles in its image caption generation.Another example presented in Fig. 4a shows that MiniGPT-4 successfully explains why the meme is humorous.It interprets that the lying dog is feeling the same way as many people do on Monday, which is often considered to be the most dreaded day of the week.In contrast, BLIP-2 only briefly describes the image content and fails to comprehend the amusing aspects of the image.\n\n\nPreprint\n\nDescribe this image as detailed as possible.\n\nThe image shows a busy city street with a clock tower in the background.The street is lined with shops and restaurants on both sides.There are several motorcycles parked on the side of the road, and people are walking down the street.The clock tower has a large clock face with Roman numerals and a small spire on top.The buildings on either side of the street have ornate facades and balconies.The street is paved with cobblestones and there are streetlights on either side of the road.The sky is clear and blue, and there are a few clouds in the distance.\n\nA city street with many people walking and riding motorcycles.Are you looking for a unique and stylish lighting solution for your home or office?Look no further than our brass toucan lamp!This beautifully crafted bird lamp is sure to add a touch of whimsy and charm to any room.The toucan's beak is designed to hold a light bulb, providing a warm and inviting glow.The base is made of wood, adding a natural touch to the overall design.The toucan lamp is a great addition to any room, whether you're looking to brighten up a dark corner or add a touch of personality to your space.Order yours today and bring a touch of the tropics to your home or office.\n\nA toucan lamp on a table\n\nMiniGPT-4\n\n\nBLIP-2\n\nHuman Input Image\n\n\nFigure 3: Advertisement promotion\n\nWe also showcase MiniGPT-4's other abilities by demonstrating other distinctive abilities.These include creating advertising promotions based on a given image (Fig. 3), retrieving factual information from a movie photograph (Fig. 8), generating a food recipe from a food image (Fig. 11), diagnosing plant diseases and suggesting treatment plans (Fig. 12), creating a website from a hand-written draft (Fig. 4b), and writing poems inspired by an image (Fig. 10).These abilities are absent in traditional vision-language models like BLIP-2 (utilizing Flan-T5 XXL (Chung et al., 2022) as a language model), which use less powerful language models (LLMs).This contrast indicates that those advanced vision-language abilities only emerge when the visual features are properly aligned with an advanced LLM such as Vicuna (Chiang et al., 2023).\n\n\nQUANTITATIVE ANALYSIS\n\nAdvanced Abilities To quantify performance on advanced vision-language tasks, we compiled a small evaluation dataset comprising 4 tasks: meme interpretation with the question \"Explain why this meme is funny.\",recipe generation with the question \"How should I make something like this?\", advertisement creation with the prompt \"Help me draft a professional advertisement for this.\", and poem composition with \"Can you craft a beautiful poem about this image?\".In total, we collect 100 diverse images, with 25 images allocated to each task.We asked human evaluators to determine whether the model generation satisfies the request.We compared our results with BLIP-2 (Li et al., 2023) and present the findings in Tab.1.In meme interpretation, poem writing, and advertisement creation, BLIP-2 largely struggles to fulfill any requests.For recipe generation, BLIP-2 succeeds in 4 out of 25 cases.In contrast, MiniGPT-4 manages to address the requests in recipes, advertisements, and poem generation in nearly 80% of the instances.Furthermore, MiniGPT-4 correctly comprehends the challenging humor understanding in memes in 8 out of 25 cases.\n\nImage Captioning We evaluate the performance of MiniGPT-4 on the COCO caption benchmark and compare it with BLIP-2 (Li et al., 2023).Our model's generated captions typically contain rich visual details.As such, conventional similarity-based image-caption evaluation metrics struggle to provide an accurate evaluation of our models.In this regard, we evaluate the performance by ent of page 1 punchline 1.\n\n\n\u2026 \u2026\n\nExplain why this meme is funny.\n\nThis meme is funny because it shows a dog laying on the floor with its head resting on its paws, as if it is tired or sleepy.The caption, \"monday just monday,\" adds to the humor by suggesting that the dog is feeling the same way as many people do on Mondays, which are often considered to be the most dreaded day of the week.The meme is relatable and humorous, making it a popular one among internet users.\n\nA dog laying down on the floor with the caption monday just Monday.\n\n\nMiniGPT-4 BLIP-2 Human\n\nA dog is a dog.\n\n\nBLIP-2 FT\n\nThis meme is funny because it shows a dog sleeping on the floor.\n\n\nMiniGPT-4 LocNa\n\nThis meme is funny because it shows a small dog laying on the floor with its legs stretched out, seemingly enjoying a lazy day.The caption, \"Monday just monday,\" adds to the humor by suggesting that the dog is so relaxed that it doesn't even have the energy to get up.The image is also humorous because it depicts a common scenario that many people can relate tothe feeling of being so tired that you just want to lay in bed all day.Overall, this meme is a lighthearted and relatable depiction of the joys of relaxation.\n\n\nMiniGPT-4 No Q-Former\n\nDescribe this image as detailed as possible.\n\nThe image shows a busy city street with a clock tower in the background.The street is lined with shops and restaurants on both sides.There are several motorcycles parked on the side of the road, and people are walking down the street.The clock tower has a large clock face with Roman numerals and a small spire on top.The buildings on either side of the street have ornate facades and balconies.The street is paved with cobblestones and there are streetlights on either side of the road.The sky is clear and blue, and there are a few clouds in the distance.\n\nA city street with many people walking and riding motorcycles.\n\n\nMiniGPT-4 BLIP-2 Human\n\nA city street with many people riding on motorcycles and scooters.\n\n\nBLIP-2 FT\n\n\nMiniGPT-4 LocNa\n\nIn this image, we can see a busy street with many shops and restaurants.\n\nOn the left side of the image, we can see a clock tower.\n\nOn the right side of the image, we can see many motorcycles parked on the road.\n\nIn the middle of the image, we can see many people walking on the road.\n\nOn the left side of the image, we can see many shops and restaurants.\n\nOn the right side of the image, we can see many motorcycles parked on the road.\n\nIn the middle of the image, we can see many people walking on the road.\n\nOn the left side of the image, we can see many shops and restaurants.\n\nOn the right side of the image, \u2026\u2026\n\n\nMiniGPT-4 No Q-Former\n\nThe image shows a busy street in a city at night.There are many people walking on the sidewalk and riding bicycles.The buildings on either side of the street are tall and have many windows.There is a large clock tower in the center of the image.The sky is clear and there are stars visible in the sky.The image is taken from a high angle, looking down on the street.Explain why this meme is funny.\n\nThis meme is funny because it shows a do with its head resting on its paws, as if it is caption, \"monday just monday,\" adds suggesting that the dog is feeling the people do on Mondays, which are often c most dreaded day of the week.The me humorous, making it a popular one among A dog laying down on the floor with the ca Monday.\n\n\nMiniGPT-4 BLIP-2 Human\n\nA dog is a dog.\n\n\nBLIP-2 FT\n\nThis meme is funny because it shows a do floor.\n\n\nMiniGPT-4 LocNa\n\nThis meme is funny because it shows a sma floor with its legs stretched out, seemingly The caption, \"Monday just monday,\" adds suggesting that the dog is so relaxed that it the energy to get up.The image is also hum depicts a common scenario that many peo the feeling of being so tired that you just w day.Overall, this meme is a lighthearted an depiction of the joys of relaxation.\n\n\nMiniGPT-4 No Q-Former\n\nQualitative comparison between the original MiniG variant finetuned on the Local Narrative dataset (M original BLIP-2 with language model FlanT5 XXL.checking if the generated captions cover all the ground truth captions' information with the help of ChatGPT and details can be found in Appx.A.3.Results in Tab.2 shows that MiniGPT-4 outperforms BLIP-2 in generating captions that are more closely aligned with the ground-truth visual objects and relationships.With a success rate of 66.2%, MiniGPT-4 is considerably more accurate than BLIP-2, which achieves only 27.5%.Further evaluation on traditional VQA tasks can be found in Appx.A.2.\n\n\nANALYSIS ON THE SECOND-STAGE FINETUNING\n\nEffectiveness of the second-stage finetuning The utilization of only the model pretrained after the first pretraining stage may result in failures, such as the occurrence of repetitive words or sentences, fragmented sentences, or irrelevant content.However, these issues have been largely mitigated through the second-stage finetuning process.This can be observed in Fig. 5, where MiniGPT-4\n\n\nPreprint\n\nDescribe this image as detailed as possible.\n\nThis image is a close-up shot of a wooden bench sitting on a snow-covered hill.The bench is made of wood and appears to be fairly new, with no signs of wear or damage.The snow on the ground is piled up around the bench, giving the impression of a winter scene.In the background, there are trees and mountains visible, with a glow in the sky indicating a sunset.The image is beautiful, with the snow adding a serene atmosphere to the scene.Describe this image in detail.\n\nYes, the windows are on the left side of the photo.\n\nThe image shows a dining area with several tables and chairs set up for diners.The tables are covered in white tablecloths and have silverware, glasses, and bottles of liquor on them.The walls are painted a light blue and have large windows that look out onto a cityscape.There is a chandelier hanging from the ceiling and a few potted plants on the windowsill.The floors are made of wood and the room is well-lit.\n\n\nMiniGPT-4\n\nMiniGPT-4\n\n\nHuman\n\n\nInput Image\n\nAre the windows on the left side of the photo?Human generates incomplete captions before the second-stage finetuning.However, after the second-stage finetuning, MiniGPT-4 is capable of generating complete and fluent captions.In this section, we investigate the importance and effectiveness of the second-stage finetuning approach.\n\nTo quantify the impact of second-stage finetuning, we randomly sampled 100 images from the COCO test set and investigated the model performance on two tasks: detailed description generation and poem writing.The prompts used were \"Describe the image in detail.\"and \"Can you write a beautiful poem about this image?\".These tasks were performed by both the models before and after second-stage finetuning.We manually counted the number of failure generations for the model in each stage.The results are presented in Tab.3.Prior to the second-stage finetuning, approximately 1/3 of the generated outputs failed to match ground truth captions or poems.In contrast, the model after second-stage fineuning has less than two failure cases out of the 100 test images for both tasks.These experimental results demonstrate that second-stage finetuning yields a significant improvement in the quality of generated outputs.A qualitative example of the model generation before and after the second-stage finetuning is shown in Fig. 5.\n\nCan the original BLIP-2 benefit from the second-stage data?In this study, we finetune BLIP-2 (Li et al., 2023) with our second-stage data in the same way as MiniGPT-4, and check if it can obtain similar advanced abilities as MiniGPT-4.The finetuned BLIP-2 is denoted as BLIP-2 FT.Note that MiniGPT-4 uses the same visual module as BLIP-2; while BLIP-2 uses FlanT5 XXL (Chung et al., 2022) as the language model, which is not as strong as the Vicuna (Chiang et al., 2023) model used in our MiniGPT-4 model.We rely on the same prompts to assess the advanced capabilities of our model.Qualitative results are shown in Fig. 4, 13, and 14.We discover that BLIP-2 FT still generates short responses and fails to generalize to advanced tasks like meme explaining and website coding (Fig. 4).Our finding suggests that BLIP-2's relatively weaker language model FlanT5 XXL benefits less from such a small dataset, and highlights the effectiveness of a more advanced LLM in a VLM system.\n\nSecond stage with Localized Narratives The dataset Localized Narratives (Pont-Tuset et al., 2020) is a detailed image description dataset where annotators describe images while simultaneously localizing the corresponding regions.Here, we test the performance of our model by replacing our self-collected dataset in the second-stage with the Localized Narratives dataset.The model is denoted Preprint as MiniGPT-4 LocNa.Qualitative results in Fig. 4, 13, and 14 show that MiniGPT-4 LocNa can generate long image descriptions (Fig. 14).However, the generated outputs have lower quality with monotonous expressions.Besides, MiniGPT-4 LocNa does not generalize as well as the original MiniGPT-4 in other complex tasks like explaining why the meme is funny (Fig. 4a).The performance gap may be due to the monotonous and repeated image descriptions in Localized Narratives.\n\n\nABLATION ON THE ARCHITECTURE DESIGNS\n\nTo further demonstrate the effectiveness of using one single linear layer to align visual features with LLM, we conduct experiments with different architecture designs, including (a) removing the Q-Former and directly mapping the VIT's output to Vicuna's embedding space (i.e., without Q-former), (b) using three linear layers instead of one layer, and (c) additionally finetuning the Q-Former in the vision module.All the variants are trained in the same way as the original design.Results on AOK-VQA (Schwenk et al., 2022) and GQA (Hudson & Manning, 2019) datasets in Tab.4 show that the variant (a) MiniGPT-4 w/o Q-Former has a similar performance to the original design.\n\nQualitative results of this variant in Fig. 4, 13, and 14 also show similar advanced skills.This reveals that the Q-Former from BLIP-2 doesn't plays a critical roles for advanced skills.Besides, both variants (b) MiniGPT-4+ 3 Layers and (c) MiniGPT-4 + finetuning Q-Former, perform slightly worse than the original MiniGPT-4.This indicates a single projection layer is sufficient to align the vision encoder and the large language model in our limited training data setting.\n\n\nLIMITATION ANALYSIS\n\nHallucination As MiniGPT-4 is built upon LLMs, it inherits LLM's limitations like hallucinating nonexistent knowledge.An example in Fig. 6 shows that MiniGPT-4 incorrectly identifies the presence of white tablecloths in the image, despite their absence.Here, we use the metric CHAIR i (Rohrbach et al., 2018) to gauge the hallucination rate of the generation, with the two distinct prompts to control the model generation length: MiniGPT-4 (long): Please describe this image as detailed as possible.MiniGPT-4 (short): Please describe the image shortly and precisely, in less than 20 words.\n\nResults in Tab.5 show that longer captions tend to have higher hallucination rates.For example, MiniGPT-4 (long) generates captions averaging 175 words with a higher hallucination rate, while MiniGPT-4 (short) averages 28.8 words with a lower rate.BLIP-2, averaging 6.5 words, hallucinates less but covers fewer objects as seen in Tab.2.Hallucination in detailed image descriptions is still an unresolved issue.Using Reinforcement Learning with AI feadback with hallucination detection modules may be a potential solution.\n\nSpatial Information Understanding MiniGPT-4's visual perception remains limited.It may struggle to differentiate spatial localization.For example, MiniGPT-4 in Fig. 6 fails to identify the location of the windows.This limitation may stem from a lack of aligned image-text data designed for spatial information understanding.Training on such datasets like RefCOCO (Kazemzadeh et al., 2014) or Visual Genome (Krishna et al., 2017) could potentially alleviate this issue.\n\n\nDISCUSSION\n\nHow does MiniGPT-4 obtain these advanced abilities?Many of the advanced vision-language capabilities demonstrated by GPT-4 can be understood as compositional skills rooted in two foundational skills: image understanding and language generation.Take the task of image-based poem writing as an example.Advanced LLMs like ChatGPT and Vicuna can already craft poems based on users' instructions.If they acquire the ability to understand images, compositionally generalizing to the task of image-based poem writing even without having image-poem pairs in their training data is possible.\n\nIn the first pretraining stage, MiniGPT-4 learns to understand images by modeling the correlation between images and short image descriptions from image caption datasets.However, the language style in these image caption datasets differs from that of modern LLMs' generation, which leads to distorted language generation and hinders successful compositional generalization.Therefore, we introduce a second-stage finetuning to restore the language generation ability.MiniGPT-4 after the two-stage training successfully generalizes to many advanced compositional vision-language abilities like website coding from drafts or meme interpretation, verifies our assumption.Future research\n\n\nA APPENDIX A.1 MORE QUALITATIVE RESULTS\n\nExplain why this meme is funny.\n\nThis meme is funny because it shows a dog laying on the floor with its head resting on its paws, as if it is tired or sleepy.The caption, \"monday just monday,\" adds to the humor by suggesting that the dog is feeling the same way as many people do on Mondays, which are often considered to be the most dreaded day of the week.The meme is relatable and humorous, making it a popular one among internet users.\n\nA dog laying down on the floor with the caption monday just Monday.Sure, here is a poem about the image you provided:\n\n\nMiniGPT\n\nA man sits on a hill, with his dog by his side The sun sets behind them, casting a golden light The city below is bustling, but they are at peace Enjoying the moment, in each other's embrace The man's arms are wrapped around his furry friend As they watch the world go by, with a gentle bend The dog's head rests on the man's shoulder A symbol of their unbreakable bond, forever\n\nThe sky is ablaze with color, a beautiful sight But its the love between the man and his dog that shines bright It's a moment of pure joy, a memory to keep Forever etched in their hearts, as they sit and weep So let us cherish these moments, with our four-legged friends For they bring us so much happiness, until the very end And even when they're gone, their love will live on In the memories we hold, forever and strong.\n\nA man and his dog sit on a hill overlooking the city.To make this dish, you will need the following ingredients:\n\n\nMiniGPT\n\n1. Preheat the oven to 400\u00b0F (200\u00b0C).2. In a large baking dish, toss the lobsters with the olive oil, garlic, thyme, oregano, paprika, salt, and pepper.The image shows a leaf with brown spots on it.The spots are likely caused by a fungal infection, which can be treated with a fungicide.It's important to identify the type of fungus causing the spots in order to choose the appropriate treatment.\n\nTo treat the plant, you can try the following steps: 1. Identify the type of fungus causing the spots on the leaves.2. Use a fungicide specifically labeled for the type of fungus you have identified.3. Apply the fungicide according to the product's instructions, making sure to cover all affected leaves.4. Repeat the treatment every 7 to 14 days, or as directed by the product's instructions, until the fungus is completely gone.5. Water the plant well after treatment to help the fungicide penetrate the leaves.\n\nIt's also important to keep the plant healthy by providing it with proper care, such as adequate watering, fertilization, and pruning.This will help the plan to recover and prevent future fungal infections.The aim of this study is to replicate the remarkable multi-modal capabilities demonstrated in GPT-4, such as generating detailed image descriptions and creating websites from hand-drawn drafts.To emphasize the most crucial component of advanced vision-language skills, the methodology of MiniGPT-4 is intentionally kept minimal.For instance, the learnable model capacity is limited (only one linear layer), and MiniGPT-4 is trained with just 5 million pairs, in contrast to BLIP-2 with 129 million image-text pairs.Such a pared-down approach is anticipated to yield suboptimal results on traditional benchmarks.While this isn't our primary goal, we offer a quantitative analysis of the VQA datasets A-OKVQA (multi-choice) (Schwenk et al., 2022) andGQA (Hudson &Manning, 2019).Additionally, to showcase the potential of MiniGPT-4 with traditional benchmarks, we conduct a straightforward ablation study.Here, we simply unfreeze the LLM using LoRA (Hu et al., 2021) and incorporate more training data from the VQAv2, OKVQA, and A-OKVQA datasets during the second finetuning stage.Results in Tab.6 indicate that the original MiniGPT-4 lags behind BLIP-2 by a reasonable margin, and merely augmenting the learning capacity and the training data results in a substantial performance improvement, which confirms our expectations.We believe our model's performance on conventional vision benchmarks can be enhanced with a carefully designed training strategy (e.g., dataset sample ratios, learning rate schedule, etc.), more training data/datasets, and additional learnable parameters.Since enhancing performance on traditional vision benchmarks isn't this project's objective, we reserve this aspect for future research.\n\nFigure 1 :\n1\nFigure 1: The architecture of MiniGPT-4.It consists of a vision encoder with a pretrained ViT and Q-Former, a single linear projection layer, and an advanced Vicuna large language model.MiniGPT-4 only requires training the linear projection layer to align the visual features with the Vicuna.\n\n\nFigure 2 :\n2\nFigure 2: Detailed description\n\n\n\n\ncomparison between the original MiniGPT-4, the MiniGPT-4 variant without Q-Former (MiniGPTtuned on the Local Narrative dataset (MiniGPT-4 LocNa), BLIP-2 (FlanT5 XXL) finetuned with o -2 with language model FlanT5 XXL.Write brief HTML/JS to turn this mock-up into a colorful website, where the jokes are replaced by real joke.A joke website content of page 1 punchline 1.\n\n\nFigure 4 :\n4\nFigure 4: Model generations from BLIP-2, BLIP-2 finetuned our second stage data (BLIP-2 FT), MiniGPT-4 finetuned with Local Narrative data in the second stage (MiniGPT-4 LocNa), MiniGPT-4 model without Q-Former (MiniGPT-4 No Q-Former), and MiniGPT-4.\n\n\nFigure 5 :\n5\nFigure 5: MiniGPT-4 before second-stage finetuning fails to output completed texts.The generation is improved after the finetuning.\n\n\nFigure 6 :\n6\nFigure 6: An example of MiniGPT-4's limitations.MiniGPT-4 hallucinates unexisting tablecloths and can't locate the windows correctly.\n\n\n\n\nFigure 7: Advertisement promotion\n\n\n\n\nFigure 10: Poem writing\n\n\n3 .Figure 11 :\n311\nFigure 11: Food recipe generation\n\n\n\n\nFigure 12: Plant cultivating\n\n\n1 .\n1\nFigure 14: Ablation Study on Detailed Description\n\n\nTable 1 :\n1\nQuantitative results on advanced vision-language tasks.MiniGPT-4 shows strong performance and successfully responses to 65% of the requests.\nMeme RecipesAdsPoemAvg.BLIP-20/254/251/250/255/100MiniGPT-48/2518/2519/25 20/25 65/100\nS to turn this mock-up into a colorful jokes are replaced by real joke.\n\n\nTable 2 :\n2\nCOCO caption evaluation.We use ChatGPT to judge if the generated caption covers all the visual objects and relations in the ground-truth caption.\nBLIP-2MiniGPT-4Correctness 1376/5000 3310/5000Percentage27.5%66.2%\n\nTable 3 :\n3\nFailure rates of detailed caption and poem generation tasks before and after second-stage finetuning.The finetuning stage significantly reduces generation failures.\nFailure rateDetailed caption PoemBefore stage-235%32%After stage-22%1%\n\nTable 4 :\n4\nAblation on architecture designs\nTable 5: Hallucination EvaluationModelAOK-VQA GQACHAIR i Avg. LengthMiniGPT-4 (a) MiniGPT-4 w/o Q-Former (b) MiniGPT-4 + 3 Layers (c) MiniGPT-4 + Finetune Q-Former58.2 56.9 49.7 52.132.2 33.4 31.0 28.0Blip-2 MiniGPT-4 (short) MiniGPT-4 (long)1.3 7.2 9.66.5 28.8 175\n\nTable 6 :\n6\nPerformance Comparison between BLIP-2 and MiniGPT-4\nModelTraining dataAOK-VQA GQABlip-2129M image-text pairs80.242.4MiniGPT-45M image-text pairs58.232.2MiniGPT-4 (Finetune Vicuna)5M image-text pairs67.243.5\nPreprint might delve deeper into the mechanism of compositional generalization and seek ways to enhance them.We hope our work, as an early exploration of these vision-based LLM capabilities, will spur further investigations in this domain.PreprintA.3 DETAILS OF CAPTION EVALUATIONWe employ ChatGPT to determine whether the baseline models cover all the objects and visual relations presented in the ground-truth captions.For the COCO evaluation dataset, we randomly choose one ground-truth caption and treat it as the reference caption.We apply the following prompt to perform the evaluation.There is one image caption1 '{ground-truth caption}', and there is another image caption2 '{comparison caption}'.Does image caption2 cover all the objects and visual relations shown in image caption1?Only answer yes or no without any explanation.This meme is funny because it shows a dog laying on the floor with its head resting on its paws, as if it is tired or sleepy.The caption, \"monday just monday,\" adds to the humor by suggesting that the dog is feeling the same way as many people do on Mondays, which are often considered to be the most dreaded day of the week.The meme is relatable and humorous, making it a popular one among internet users.A.4 MORE QUALITATIVE ABLATION RESULTSA dog laying down on the floor with the caption monday just Monday.MiniGPT-4 BLIP-2 HumanA dog is a dog.BLIP-2 FTThis meme is funny because it shows a dog sleeping on the floor.MiniGPT-4 LocNaThis meme is funny because it shows a small dog laying on the floor with its legs stretched out, seemingly enjoying a lazy day.The caption, \"Monday just monday,\" adds to the humor by suggesting that the dog is so relaxed that it doesn't even have the energy to get up.The image is also humorous because it depicts a common scenario that many people can relate tothe feeling of being so tired that you just want to lay in bed all day.Overall, this meme is a lighthearted and relatable depiction of the joys of relaxation.MiniGPT-4 No Q-FormerDescribe this image as detailed as possible.The image shows a busy city street with a clock tower in the background.The street is lined with shops and restaurants on both sides.There are several motorcycles parked on the side of the road, and people are walking down the street.The clock tower has a large clock face with Roman numerals and a small spire on top.The buildings on either side of the street have ornate facades and balconies.The street is paved with cobblestones and there are streetlights on either side of the road.The sky is clear and blue, and there are a few clouds in the distance.A city street with many people walking and riding motorcycles.MiniGPT-4 BLIP-2 HumanA city street with many people riding on motorcycles and scooters.BLIP-2 FTMiniGPT-4 LocNaIn this image, we can see a busy street with many shops and restaurants.On the left side of the image, we can see a clock tower.On the right side of the image, we can see many motorcycles parked on the road.In the middle of the image, we can see many people walking on the road.On the left side of the image, we can see many shops and restaurants.On the right side of the image, we can see many motorcycles parked on the road.In the middle of the image, we can see many people walking on the road.On the left side of the image, we can see many shops and restaurants.On the right side of the image, \u2026\u2026MiniGPT-4 No Q-FormerThe image shows a busy street in a city at night.There are many people walking on the sidewalk and riding bicycles.The buildings on either side of the street are tall and have many windows.There is a large clock tower in the center of the image.The sky is clear and there are stars visible in the sky.The image is taken from a high angle, looking down on the street.litative comparison between the original MiniGPT-4, the MiniGPT-4 variant without Q-Former (MiniGPT-4 No Q-Former), iant finetuned on the Local Narrative dataset (MiniGPT-4 LocNa), BLIP-2 (FlanT5 XXL) finetuned with our dataset (BLIP-2 inal BLIP-2 with language model FlanT5 XXL.How should I make something like this?To make this dish, you will need the follow\nSharegpt. 2023\n\nFlamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in Neural Information Processing Systems. 2022\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033\n\nConceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. Soravit Changpinyo, Piyush Sharma, Nan Ding, Radu Soricut, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021\n\nVisualgpt: Data-efficient adaptation of pretrained language models for image captioning. Jun Chen, Han Guo, Kai Yi, Boyang Li, Mohamed Elhoseiny, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022\n\nJun Chen, Deyao Zhu, Kilichbek Haydarov, Xiang Li, Mohamed Elhoseiny, arXiv:2304.04227Video chatcaptioner: Towards the enriched spatiotemporal descriptions. 2023arXiv preprint\n\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, March 2023\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint\n\nScaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.114162022arXiv preprint\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint\n\nPalm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, arXiv:2303.033782023arXiv preprint\n\nExploring the limits of masked visual representation learning at scale. Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao, Eva, arXiv:2211.076362022arXiv preprint\n\nTraining compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.155562022arXiv preprint\n\nJ Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint\n\nLanguage is not all you need: Aligning perception with language models. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, arXiv:2302.140452023arXiv preprint\n\nGqa: A new dataset for real-world visual reasoning and compositional question answering. Preprint Drew, A Hudson, Christopher D Manning, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019\n\nReferitgame: Referring to objects in photographs of natural scenes. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, Tamara Berg, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). the 2014 conference on empirical methods in natural language processing (EMNLP)2014\n\nVisual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, International journal of computer vision. 1232017\n\nBlip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, International Conference on Machine Learning. PMLR2022\n\nBlip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXiv:2301.12597OpenAI. Introducing chatgpt. 2023. 2023arXiv preprint\n\nIm2text: Describing images using 1 million captioned photographs. Vicente Ordonez, Girish Kulkarni, Tamara Berg, Advances in neural information processing systems. 242011\n\nTraining language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235\n\nConnecting vision and language with localized narratives. Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, Vittorio Ferrari, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020Proceedings, Part V 16\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020\n\nAnna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, Kate Saenko, arXiv:1809.02156Object hallucination in image captioning. 2018arXiv preprint\n\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Matthias Franc \u00b8ois Yvon, Gall\u00e9, arXiv:2211.05100A 176b-parameter open-access multilingual language model. 2022aarXiv preprint\n\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Matthias Franc \u00b8ois Yvon, Gall\u00e9, arXiv:2211.05100A 176b-parameter open-access multilingual language model. 2022barXiv preprint\n\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki, arXiv:2111.02114Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. 2021arXiv preprint\n\nA-okvqa: A benchmark for visual question answering using world knowledge. Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, Roozbeh Mottaghi, European Conference on Computer Vision. Springer2022\n\nConceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. Piyush Preprint, Nan Sharma, Sebastian Ding, Radu Goodman, Soricut, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational Linguistics20181\n\nUsing deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick Legresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, arXiv:2201.119902022arXiv preprint\n\nVipergpt: Visual inference via python execution for reasoning. D\u00eddac Sur\u00eds, Sachit Menon, Carl Vondrick, arXiv:2303.081282023arXiv preprint\n\nStanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023\n\nPlug-andplay vqa: Zero-shot vqa by conjoining large pretrained models with zero training. Anthony Meng, Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, Steven Ch Hoi, arXiv:2210.087732022arXiv preprint\n\nThibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timoth\u00e9e Lachaux, Baptiste Lacroix, Naman Rozi\u00e8re, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint\n\nMultimodal few-shot learning with frozen language models. Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, Oriol Eslami, Felix Vinyals, Hill, Advances in Neural Information Processing Systems. 202134\n\nEmergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Transactions on Machine Learning Research. 2835-88562022\n\nVisual chatgpt: Talking, drawing and editing with visual foundation models. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan, arXiv:2303.046712023arXiv preprint\n\nZero-shot video question answering via frozen bidirectional language models. Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid, arXiv:2206.081552022arXiv preprint\n\nMm-react: Prompting chatgpt for multimodal reasoning and action. Zhengyuan Yang, * , Linjie Li, * , Jianfeng Wang, * , Kevin Lin, * , Ehsan Azarnasab, * , Faisal Ahmed, * , Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang, 2023\n\nOpt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.010682022arXiv preprint\n\nDeyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, Mohamed Elhoseiny, arXiv:2303.06594Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions. 2023arXiv preprint\n\nPreheat the oven to. 200\u00b0C400\n\nRoast the lobsters in the preheated oven are cooked through. \n\nRemove the lobsters from the oven and 5. Serve the lobsters with the lemon wedg I hope this helps! Let me know if you have Preheat the oven to 400 degrees f and line parchment paper MiniGPT-4. \n\nHuman A roasted lobster with a roasted garlic sauc BLIP-2 FT MiniGPT-4. \n\nLocNa To make this dish, you will need the followi MiniGPT-4. \n\nFormer To make this dish, you will need the follow. Q- No, \n", "annotations": {"author": "[{\"end\":186,\"start\":100},{\"end\":271,\"start\":187},{\"end\":366,\"start\":272},{\"end\":453,\"start\":367},{\"end\":556,\"start\":454}]", "publisher": null, "author_last_name": "[{\"end\":109,\"start\":106},{\"end\":195,\"start\":191},{\"end\":285,\"start\":281},{\"end\":375,\"start\":373},{\"end\":471,\"start\":462}]", "author_first_name": "[{\"end\":105,\"start\":100},{\"end\":190,\"start\":187},{\"end\":280,\"start\":272},{\"end\":372,\"start\":367},{\"end\":461,\"start\":454}]", "author_affiliation": "[{\"end\":185,\"start\":134},{\"end\":270,\"start\":219},{\"end\":365,\"start\":314},{\"end\":452,\"start\":401},{\"end\":555,\"start\":504}]", "title": "[{\"end\":87,\"start\":1},{\"end\":643,\"start\":557}]", "venue": null, "abstract": "[{\"end\":2261,\"start\":712}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2380,\"start\":2359},{\"end\":2393,\"start\":2380},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2412,\"start\":2393},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2431,\"start\":2412},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2452,\"start\":2431},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2475,\"start\":2452},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2497,\"start\":2475},{\"end\":2827,\"start\":2813},{\"end\":3153,\"start\":3139},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3390,\"start\":3370},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3428,\"start\":3411},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3447,\"start\":3429},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3860,\"start\":3839},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3910,\"start\":3888},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4117,\"start\":4100},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4179,\"start\":4160},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4559,\"start\":4535},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4606,\"start\":4581},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4626,\"start\":4606},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4658,\"start\":4636},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6064,\"start\":6044},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6093,\"start\":6076},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7366,\"start\":7345},{\"end\":7396,\"start\":7368},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7426,\"start\":7405},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7506,\"start\":7486},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7777,\"start\":7757},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7813,\"start\":7790},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7844,\"start\":7820},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7870,\"start\":7850},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7903,\"start\":7872},{\"end\":7931,\"start\":7903},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7975,\"start\":7957},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8332,\"start\":8311},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8547,\"start\":8527},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8580,\"start\":8559},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8639,\"start\":8617},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8887,\"start\":8868},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8906,\"start\":8887},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8924,\"start\":8906},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8943,\"start\":8924},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8964,\"start\":8943},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8980,\"start\":8964},{\"end\":8985,\"start\":8980},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9005,\"start\":8985},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9186,\"start\":9167},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9225,\"start\":9198},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9365,\"start\":9343},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9624,\"start\":9607},{\"end\":9681,\"start\":9661},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9802,\"start\":9781},{\"end\":10028,\"start\":10008},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10403,\"start\":10386},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10437,\"start\":10417},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10638,\"start\":10620},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10887,\"start\":10868},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10992,\"start\":10972},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11478,\"start\":11457},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11558,\"start\":11536},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11717,\"start\":11700},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11753,\"start\":11734},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12955,\"start\":12930},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12975,\"start\":12955},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13003,\"start\":12981},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13038,\"start\":13014},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14075,\"start\":14054},{\"end\":14088,\"start\":14075},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14677,\"start\":14657},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15389,\"start\":15368},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16204,\"start\":16179},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16224,\"start\":16204},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19274,\"start\":19257},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22251,\"start\":22230},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22959,\"start\":22942},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23548,\"start\":23531},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31295,\"start\":31278},{\"end\":31573,\"start\":31553},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31655,\"start\":31634},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":32260,\"start\":32235},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":33595,\"start\":33573},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":33628,\"start\":33604},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":34553,\"start\":34530},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35748,\"start\":35723},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":35788,\"start\":35766},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":40519,\"start\":40493},{\"end\":40532,\"start\":40519},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40546,\"start\":40532},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":40734,\"start\":40717}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41793,\"start\":41486},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41839,\"start\":41794},{\"attributes\":{\"id\":\"fig_2\"},\"end\":42214,\"start\":41840},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42480,\"start\":42215},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42627,\"start\":42481},{\"attributes\":{\"id\":\"fig_6\"},\"end\":42776,\"start\":42628},{\"attributes\":{\"id\":\"fig_7\"},\"end\":42814,\"start\":42777},{\"attributes\":{\"id\":\"fig_8\"},\"end\":42842,\"start\":42815},{\"attributes\":{\"id\":\"fig_9\"},\"end\":42897,\"start\":42843},{\"attributes\":{\"id\":\"fig_10\"},\"end\":42930,\"start\":42898},{\"attributes\":{\"id\":\"fig_11\"},\"end\":42988,\"start\":42931},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":43302,\"start\":42989},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":43528,\"start\":43303},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":43777,\"start\":43529},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":44089,\"start\":43778},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":44309,\"start\":44090}]", "paragraph": "[{\"end\":3002,\"start\":2277},{\"end\":3689,\"start\":3004},{\"end\":5198,\"start\":3691},{\"end\":6276,\"start\":5200},{\"end\":6319,\"start\":6278},{\"end\":6569,\"start\":6321},{\"end\":6791,\"start\":6571},{\"end\":7143,\"start\":6793},{\"end\":8677,\"start\":7161},{\"end\":10199,\"start\":8679},{\"end\":10961,\"start\":10201},{\"end\":11290,\"start\":10963},{\"end\":11989,\"start\":11301},{\"end\":12395,\"start\":11991},{\"end\":12720,\"start\":12423},{\"end\":13258,\"start\":12722},{\"end\":13722,\"start\":13260},{\"end\":14386,\"start\":13724},{\"end\":15046,\"start\":14460},{\"end\":15523,\"start\":15088},{\"end\":15659,\"start\":15525},{\"end\":16317,\"start\":15661},{\"end\":16628,\"start\":16319},{\"end\":16819,\"start\":16630},{\"end\":16950,\"start\":16821},{\"end\":17566,\"start\":16952},{\"end\":17778,\"start\":17594},{\"end\":18193,\"start\":17841},{\"end\":18487,\"start\":18195},{\"end\":18889,\"start\":18503},{\"end\":19160,\"start\":18969},{\"end\":19344,\"start\":19162},{\"end\":20039,\"start\":19346},{\"end\":20096,\"start\":20052},{\"end\":20655,\"start\":20098},{\"end\":21312,\"start\":20657},{\"end\":21338,\"start\":21314},{\"end\":21349,\"start\":21340},{\"end\":21377,\"start\":21360},{\"end\":22252,\"start\":21415},{\"end\":23414,\"start\":22278},{\"end\":23820,\"start\":23416},{\"end\":23859,\"start\":23828},{\"end\":24267,\"start\":23861},{\"end\":24336,\"start\":24269},{\"end\":24378,\"start\":24363},{\"end\":24456,\"start\":24392},{\"end\":24996,\"start\":24476},{\"end\":25066,\"start\":25022},{\"end\":25625,\"start\":25068},{\"end\":25689,\"start\":25627},{\"end\":25782,\"start\":25716},{\"end\":25886,\"start\":25814},{\"end\":25944,\"start\":25888},{\"end\":26025,\"start\":25946},{\"end\":26098,\"start\":26027},{\"end\":26169,\"start\":26100},{\"end\":26250,\"start\":26171},{\"end\":26323,\"start\":26252},{\"end\":26394,\"start\":26325},{\"end\":26430,\"start\":26396},{\"end\":26853,\"start\":26456},{\"end\":27184,\"start\":26855},{\"end\":27226,\"start\":27211},{\"end\":27287,\"start\":27240},{\"end\":27689,\"start\":27307},{\"end\":28353,\"start\":27715},{\"end\":28787,\"start\":28397},{\"end\":28844,\"start\":28800},{\"end\":29315,\"start\":28846},{\"end\":29368,\"start\":29317},{\"end\":29784,\"start\":29370},{\"end\":29807,\"start\":29798},{\"end\":30161,\"start\":29831},{\"end\":31183,\"start\":30163},{\"end\":32161,\"start\":31185},{\"end\":33030,\"start\":32163},{\"end\":33745,\"start\":33071},{\"end\":34221,\"start\":33747},{\"end\":34834,\"start\":34245},{\"end\":35358,\"start\":34836},{\"end\":35828,\"start\":35360},{\"end\":36425,\"start\":35843},{\"end\":37109,\"start\":36427},{\"end\":37184,\"start\":37153},{\"end\":37592,\"start\":37186},{\"end\":37711,\"start\":37594},{\"end\":38101,\"start\":37723},{\"end\":38526,\"start\":38103},{\"end\":38640,\"start\":38528},{\"end\":39048,\"start\":38652},{\"end\":39563,\"start\":39050},{\"end\":41485,\"start\":39565},{\"end\":41792,\"start\":41500},{\"end\":41838,\"start\":41808},{\"end\":42213,\"start\":41843},{\"end\":42479,\"start\":42229},{\"end\":42626,\"start\":42495},{\"end\":42775,\"start\":42642},{\"end\":42813,\"start\":42780},{\"end\":42841,\"start\":42818},{\"end\":42896,\"start\":42863},{\"end\":42929,\"start\":42901},{\"end\":42987,\"start\":42938},{\"end\":43142,\"start\":43002},{\"end\":43301,\"start\":43230},{\"end\":43461,\"start\":43316},{\"end\":43706,\"start\":43542},{\"end\":43823,\"start\":43791},{\"end\":44154,\"start\":44103}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17840,\"start\":17779}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2275,\"start\":2263},{\"attributes\":{\"n\":\"2\"},\"end\":7159,\"start\":7146},{\"attributes\":{\"n\":\"3\"},\"end\":11299,\"start\":11293},{\"attributes\":{\"n\":\"3.1\"},\"end\":12421,\"start\":12398},{\"attributes\":{\"n\":\"3.2\"},\"end\":14458,\"start\":14389},{\"end\":15086,\"start\":15049},{\"attributes\":{\"n\":\"3.3\"},\"end\":17592,\"start\":17569},{\"attributes\":{\"n\":\"4\"},\"end\":18501,\"start\":18490},{\"attributes\":{\"n\":\"4.1\"},\"end\":18956,\"start\":18892},{\"end\":18967,\"start\":18959},{\"end\":20050,\"start\":20042},{\"end\":21358,\"start\":21352},{\"end\":21413,\"start\":21380},{\"attributes\":{\"n\":\"4.2\"},\"end\":22276,\"start\":22255},{\"end\":23826,\"start\":23823},{\"end\":24361,\"start\":24339},{\"end\":24390,\"start\":24381},{\"end\":24474,\"start\":24459},{\"end\":25020,\"start\":24999},{\"end\":25714,\"start\":25692},{\"end\":25794,\"start\":25785},{\"end\":25812,\"start\":25797},{\"end\":26454,\"start\":26433},{\"end\":27209,\"start\":27187},{\"end\":27238,\"start\":27229},{\"end\":27305,\"start\":27290},{\"end\":27713,\"start\":27692},{\"attributes\":{\"n\":\"4.3\"},\"end\":28395,\"start\":28356},{\"end\":28798,\"start\":28790},{\"end\":29796,\"start\":29787},{\"end\":29815,\"start\":29810},{\"end\":29829,\"start\":29818},{\"attributes\":{\"n\":\"4.4\"},\"end\":33069,\"start\":33033},{\"attributes\":{\"n\":\"4.5\"},\"end\":34243,\"start\":34224},{\"attributes\":{\"n\":\"5\"},\"end\":35841,\"start\":35831},{\"end\":37151,\"start\":37112},{\"end\":37721,\"start\":37714},{\"end\":38650,\"start\":38643},{\"end\":41497,\"start\":41487},{\"end\":41805,\"start\":41795},{\"end\":42226,\"start\":42216},{\"end\":42492,\"start\":42482},{\"end\":42639,\"start\":42629},{\"end\":42858,\"start\":42844},{\"end\":42935,\"start\":42932},{\"end\":42999,\"start\":42990},{\"end\":43313,\"start\":43304},{\"end\":43539,\"start\":43530},{\"end\":43788,\"start\":43779},{\"end\":44100,\"start\":44091}]", "table": "[{\"end\":43229,\"start\":43143},{\"end\":43528,\"start\":43462},{\"end\":43777,\"start\":43707},{\"end\":44089,\"start\":43824},{\"end\":44309,\"start\":44155}]", "figure_caption": "[{\"end\":41793,\"start\":41499},{\"end\":41839,\"start\":41807},{\"end\":42214,\"start\":41842},{\"end\":42480,\"start\":42228},{\"end\":42627,\"start\":42494},{\"end\":42776,\"start\":42641},{\"end\":42814,\"start\":42779},{\"end\":42842,\"start\":42817},{\"end\":42897,\"start\":42862},{\"end\":42930,\"start\":42900},{\"end\":42988,\"start\":42937},{\"end\":43143,\"start\":43001},{\"end\":43462,\"start\":43315},{\"end\":43707,\"start\":43541},{\"end\":43824,\"start\":43790},{\"end\":44155,\"start\":44102}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11988,\"start\":11987},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19366,\"start\":19365},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19701,\"start\":19699},{\"end\":21581,\"start\":21580},{\"end\":21646,\"start\":21645},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":21700,\"start\":21698},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":21768,\"start\":21766},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21824,\"start\":21822},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":21874,\"start\":21872},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28770,\"start\":28769},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":31182,\"start\":31181},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31806,\"start\":31805},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31967,\"start\":31966},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32611,\"start\":32610},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":32695,\"start\":32693},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32923,\"start\":32921},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":33792,\"start\":33791},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":34383,\"start\":34382},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":35526,\"start\":35525}]", "bib_author_first_name": "[{\"end\":48533,\"start\":48520},{\"end\":48547,\"start\":48543},{\"end\":48564,\"start\":48557},{\"end\":48577,\"start\":48570},{\"end\":48589,\"start\":48585},{\"end\":48600,\"start\":48596},{\"end\":48614,\"start\":48609},{\"end\":48627,\"start\":48621},{\"end\":48645,\"start\":48636},{\"end\":48663,\"start\":48656},{\"end\":48773,\"start\":48770},{\"end\":48789,\"start\":48781},{\"end\":48800,\"start\":48796},{\"end\":48815,\"start\":48808},{\"end\":48830,\"start\":48825},{\"end\":48832,\"start\":48831},{\"end\":48849,\"start\":48841},{\"end\":48866,\"start\":48860},{\"end\":48886,\"start\":48880},{\"end\":48900,\"start\":48894},{\"end\":48915,\"start\":48909},{\"end\":49088,\"start\":49081},{\"end\":49107,\"start\":49101},{\"end\":49119,\"start\":49116},{\"end\":49130,\"start\":49126},{\"end\":49387,\"start\":49384},{\"end\":49397,\"start\":49394},{\"end\":49406,\"start\":49403},{\"end\":49417,\"start\":49411},{\"end\":49429,\"start\":49422},{\"end\":49599,\"start\":49596},{\"end\":49611,\"start\":49606},{\"end\":49626,\"start\":49617},{\"end\":49642,\"start\":49637},{\"end\":49654,\"start\":49647},{\"end\":49855,\"start\":49848},{\"end\":49871,\"start\":49864},{\"end\":49878,\"start\":49876},{\"end\":49888,\"start\":49884},{\"end\":49904,\"start\":49896},{\"end\":49912,\"start\":49909},{\"end\":49927,\"start\":49920},{\"end\":49941,\"start\":49935},{\"end\":49957,\"start\":49950},{\"end\":49972,\"start\":49966},{\"end\":49974,\"start\":49973},{\"end\":49988,\"start\":49985},{\"end\":50001,\"start\":49997},{\"end\":50003,\"start\":50002},{\"end\":50031,\"start\":50022},{\"end\":50049,\"start\":50043},{\"end\":50063,\"start\":50058},{\"end\":50079,\"start\":50072},{\"end\":50093,\"start\":50087},{\"end\":50106,\"start\":50102},{\"end\":50120,\"start\":50116},{\"end\":50134,\"start\":50129},{\"end\":50138,\"start\":50135},{\"end\":50153,\"start\":50146},{\"end\":50171,\"start\":50162},{\"end\":50311,\"start\":50306},{\"end\":50325,\"start\":50323},{\"end\":50337,\"start\":50331},{\"end\":50353,\"start\":50347},{\"end\":50362,\"start\":50360},{\"end\":50375,\"start\":50368},{\"end\":50387,\"start\":50383},{\"end\":50398,\"start\":50392},{\"end\":50412,\"start\":50405},{\"end\":50433,\"start\":50423},{\"end\":50565,\"start\":50560},{\"end\":50582,\"start\":50574},{\"end\":50596,\"start\":50590},{\"end\":50610,\"start\":50602},{\"end\":50710,\"start\":50705},{\"end\":50722,\"start\":50719},{\"end\":50729,\"start\":50728},{\"end\":50731,\"start\":50730},{\"end\":50744,\"start\":50739},{\"end\":50763,\"start\":50754},{\"end\":50776,\"start\":50771},{\"end\":50794,\"start\":50788},{\"end\":50811,\"start\":50803},{\"end\":50823,\"start\":50819},{\"end\":50839,\"start\":50833},{\"end\":50964,\"start\":50959},{\"end\":50974,\"start\":50971},{\"end\":50987,\"start\":50981},{\"end\":50997,\"start\":50993},{\"end\":51009,\"start\":51003},{\"end\":51022,\"start\":51014},{\"end\":51035,\"start\":51029},{\"end\":51050,\"start\":51043},{\"end\":51060,\"start\":51057},{\"end\":51161,\"start\":51155},{\"end\":51181,\"start\":51172},{\"end\":51198,\"start\":51192},{\"end\":51212,\"start\":51207},{\"end\":51232,\"start\":51226},{\"end\":51243,\"start\":51238},{\"end\":51261,\"start\":51256},{\"end\":51274,\"start\":51270},{\"end\":51279,\"start\":51275},{\"end\":51295,\"start\":51287},{\"end\":51312,\"start\":51307},{\"end\":51364,\"start\":51363},{\"end\":51379,\"start\":51373},{\"end\":51391,\"start\":51384},{\"end\":51404,\"start\":51398},{\"end\":51420,\"start\":51413},{\"end\":51437,\"start\":51432},{\"end\":51444,\"start\":51442},{\"end\":51457,\"start\":51451},{\"end\":51637,\"start\":51630},{\"end\":51647,\"start\":51645},{\"end\":51660,\"start\":51654},{\"end\":51671,\"start\":51667},{\"end\":51684,\"start\":51677},{\"end\":51701,\"start\":51694},{\"end\":51714,\"start\":51706},{\"end\":51722,\"start\":51719},{\"end\":51733,\"start\":51728},{\"end\":51754,\"start\":51749},{\"end\":51893,\"start\":51885},{\"end\":51901,\"start\":51900},{\"end\":51921,\"start\":51910},{\"end\":51923,\"start\":51922},{\"end\":52161,\"start\":52156},{\"end\":52181,\"start\":52174},{\"end\":52195,\"start\":52191},{\"end\":52210,\"start\":52204},{\"end\":52494,\"start\":52488},{\"end\":52508,\"start\":52504},{\"end\":52520,\"start\":52514},{\"end\":52534,\"start\":52528},{\"end\":52549,\"start\":52544},{\"end\":52562,\"start\":52556},{\"end\":52581,\"start\":52572},{\"end\":52594,\"start\":52588},{\"end\":52613,\"start\":52607},{\"end\":52623,\"start\":52618},{\"end\":52625,\"start\":52624},{\"end\":52796,\"start\":52790},{\"end\":52807,\"start\":52801},{\"end\":52819,\"start\":52812},{\"end\":52833,\"start\":52827},{\"end\":53004,\"start\":52998},{\"end\":53015,\"start\":53009},{\"end\":53026,\"start\":53020},{\"end\":53043,\"start\":53037},{\"end\":53193,\"start\":53186},{\"end\":53209,\"start\":53203},{\"end\":53226,\"start\":53220},{\"end\":53365,\"start\":53361},{\"end\":53381,\"start\":53374},{\"end\":53388,\"start\":53386},{\"end\":53401,\"start\":53396},{\"end\":53418,\"start\":53411},{\"end\":53437,\"start\":53431},{\"end\":53452,\"start\":53447},{\"end\":53468,\"start\":53460},{\"end\":53486,\"start\":53478},{\"end\":53498,\"start\":53494},{\"end\":53626,\"start\":53621},{\"end\":53645,\"start\":53639},{\"end\":53663,\"start\":53656},{\"end\":53680,\"start\":53676},{\"end\":53698,\"start\":53690},{\"end\":53885,\"start\":53881},{\"end\":53902,\"start\":53895},{\"end\":53912,\"start\":53907},{\"end\":53925,\"start\":53920},{\"end\":53937,\"start\":53932},{\"end\":53950,\"start\":53946},{\"end\":54072,\"start\":54067},{\"end\":54085,\"start\":54081},{\"end\":54099,\"start\":54095},{\"end\":54118,\"start\":54109},{\"end\":54130,\"start\":54124},{\"end\":54146,\"start\":54139},{\"end\":54160,\"start\":54155},{\"end\":54170,\"start\":54167},{\"end\":54180,\"start\":54175},{\"end\":54182,\"start\":54181},{\"end\":54243,\"start\":54239},{\"end\":54258,\"start\":54254},{\"end\":54263,\"start\":54259},{\"end\":54281,\"start\":54275},{\"end\":54295,\"start\":54289},{\"end\":54309,\"start\":54305},{\"end\":54401,\"start\":54396},{\"end\":54417,\"start\":54411},{\"end\":54434,\"start\":54423},{\"end\":54447,\"start\":54442},{\"end\":54463,\"start\":54457},{\"end\":54476,\"start\":54470},{\"end\":54491,\"start\":54486},{\"end\":54511,\"start\":54502},{\"end\":54517,\"start\":54512},{\"end\":54536,\"start\":54528},{\"end\":54661,\"start\":54656},{\"end\":54677,\"start\":54671},{\"end\":54694,\"start\":54683},{\"end\":54707,\"start\":54702},{\"end\":54723,\"start\":54717},{\"end\":54736,\"start\":54730},{\"end\":54751,\"start\":54746},{\"end\":54771,\"start\":54762},{\"end\":54777,\"start\":54772},{\"end\":54796,\"start\":54788},{\"end\":54925,\"start\":54916},{\"end\":54944,\"start\":54937},{\"end\":54958,\"start\":54952},{\"end\":54975,\"start\":54969},{\"end\":54996,\"start\":54989},{\"end\":55011,\"start\":55005},{\"end\":55023,\"start\":55019},{\"end\":55038,\"start\":55033},{\"end\":55051,\"start\":55047},{\"end\":55253,\"start\":55247},{\"end\":55269,\"start\":55263},{\"end\":55293,\"start\":55282},{\"end\":55308,\"start\":55301},{\"end\":55324,\"start\":55317},{\"end\":55494,\"start\":55488},{\"end\":55508,\"start\":55505},{\"end\":55526,\"start\":55517},{\"end\":55537,\"start\":55533},{\"end\":55848,\"start\":55842},{\"end\":55863,\"start\":55856},{\"end\":55880,\"start\":55873},{\"end\":55896,\"start\":55889},{\"end\":55914,\"start\":55908},{\"end\":55933,\"start\":55928},{\"end\":55946,\"start\":55942},{\"end\":55959,\"start\":55952},{\"end\":55978,\"start\":55972},{\"end\":55993,\"start\":55988},{\"end\":56111,\"start\":56106},{\"end\":56125,\"start\":56119},{\"end\":56137,\"start\":56133},{\"end\":56244,\"start\":56239},{\"end\":56258,\"start\":56252},{\"end\":56276,\"start\":56270},{\"end\":56288,\"start\":56284},{\"end\":56304,\"start\":56297},{\"end\":56315,\"start\":56309},{\"end\":56331,\"start\":56326},{\"end\":56348,\"start\":56339},{\"end\":56350,\"start\":56349},{\"end\":56465,\"start\":56458},{\"end\":56476,\"start\":56472},{\"end\":56490,\"start\":56484},{\"end\":56501,\"start\":56495},{\"end\":56512,\"start\":56506},{\"end\":56529,\"start\":56523},{\"end\":56532,\"start\":56530},{\"end\":56581,\"start\":56574},{\"end\":56603,\"start\":56596},{\"end\":56618,\"start\":56612},{\"end\":56638,\"start\":56628},{\"end\":56657,\"start\":56649},{\"end\":56675,\"start\":56667},{\"end\":56690,\"start\":56685},{\"end\":56704,\"start\":56700},{\"end\":56894,\"start\":56889},{\"end\":56914,\"start\":56909},{\"end\":56916,\"start\":56915},{\"end\":56931,\"start\":56925},{\"end\":56943,\"start\":56938},{\"end\":56957,\"start\":56952},{\"end\":57082,\"start\":57077},{\"end\":57090,\"start\":57088},{\"end\":57101,\"start\":57096},{\"end\":57118,\"start\":57113},{\"end\":57133,\"start\":57127},{\"end\":57149,\"start\":57140},{\"end\":57164,\"start\":57160},{\"end\":57182,\"start\":57175},{\"end\":57195,\"start\":57190},{\"end\":57208,\"start\":57202},{\"end\":57220,\"start\":57218},{\"end\":57222,\"start\":57221},{\"end\":57237,\"start\":57228},{\"end\":57254,\"start\":57249},{\"end\":57269,\"start\":57264},{\"end\":57281,\"start\":57277},{\"end\":57295,\"start\":57288},{\"end\":57444,\"start\":57437},{\"end\":57458,\"start\":57449},{\"end\":57471,\"start\":57464},{\"end\":57484,\"start\":57476},{\"end\":57498,\"start\":57491},{\"end\":57508,\"start\":57505},{\"end\":57635,\"start\":57628},{\"end\":57649,\"start\":57642},{\"end\":57662,\"start\":57657},{\"end\":57674,\"start\":57670},{\"end\":57691,\"start\":57683},{\"end\":57810,\"start\":57801},{\"end\":57818,\"start\":57817},{\"end\":57827,\"start\":57821},{\"end\":57833,\"start\":57832},{\"end\":57844,\"start\":57836},{\"end\":57852,\"start\":57851},{\"end\":57860,\"start\":57855},{\"end\":57867,\"start\":57866},{\"end\":57875,\"start\":57870},{\"end\":57888,\"start\":57887},{\"end\":57897,\"start\":57891},{\"end\":57906,\"start\":57905},{\"end\":57916,\"start\":57909},{\"end\":57924,\"start\":57922},{\"end\":57937,\"start\":57930},{\"end\":57950,\"start\":57944},{\"end\":58019,\"start\":58014},{\"end\":58034,\"start\":58027},{\"end\":58048,\"start\":58043},{\"end\":58061,\"start\":58056},{\"end\":58075,\"start\":58071},{\"end\":58089,\"start\":58082},{\"end\":58107,\"start\":58096},{\"end\":58119,\"start\":58115},{\"end\":58130,\"start\":58126},{\"end\":58137,\"start\":58135},{\"end\":58193,\"start\":58188},{\"end\":58202,\"start\":58199},{\"end\":58218,\"start\":58209},{\"end\":58237,\"start\":58229},{\"end\":58251,\"start\":58244},{\"end\":58266,\"start\":58259},{\"end\":58885,\"start\":58883}]", "bib_author_last_name": "[{\"end\":48541,\"start\":48534},{\"end\":48555,\"start\":48548},{\"end\":48568,\"start\":48565},{\"end\":48583,\"start\":48578},{\"end\":48594,\"start\":48590},{\"end\":48607,\"start\":48601},{\"end\":48619,\"start\":48615},{\"end\":48634,\"start\":48628},{\"end\":48654,\"start\":48646},{\"end\":48672,\"start\":48664},{\"end\":48779,\"start\":48774},{\"end\":48794,\"start\":48790},{\"end\":48806,\"start\":48801},{\"end\":48823,\"start\":48816},{\"end\":48839,\"start\":48833},{\"end\":48858,\"start\":48850},{\"end\":48878,\"start\":48867},{\"end\":48892,\"start\":48887},{\"end\":48907,\"start\":48901},{\"end\":48922,\"start\":48916},{\"end\":49099,\"start\":49089},{\"end\":49114,\"start\":49108},{\"end\":49124,\"start\":49120},{\"end\":49138,\"start\":49131},{\"end\":49392,\"start\":49388},{\"end\":49401,\"start\":49398},{\"end\":49409,\"start\":49407},{\"end\":49420,\"start\":49418},{\"end\":49439,\"start\":49430},{\"end\":49604,\"start\":49600},{\"end\":49615,\"start\":49612},{\"end\":49635,\"start\":49627},{\"end\":49645,\"start\":49643},{\"end\":49664,\"start\":49655},{\"end\":49862,\"start\":49856},{\"end\":49874,\"start\":49872},{\"end\":49882,\"start\":49879},{\"end\":49894,\"start\":49889},{\"end\":49907,\"start\":49905},{\"end\":49918,\"start\":49913},{\"end\":49933,\"start\":49928},{\"end\":49948,\"start\":49942},{\"end\":49964,\"start\":49958},{\"end\":49983,\"start\":49975},{\"end\":49995,\"start\":49989},{\"end\":50008,\"start\":50004},{\"end\":50041,\"start\":50032},{\"end\":50056,\"start\":50050},{\"end\":50070,\"start\":50064},{\"end\":50085,\"start\":50080},{\"end\":50100,\"start\":50094},{\"end\":50114,\"start\":50107},{\"end\":50127,\"start\":50121},{\"end\":50144,\"start\":50139},{\"end\":50160,\"start\":50154},{\"end\":50180,\"start\":50172},{\"end\":50321,\"start\":50312},{\"end\":50329,\"start\":50326},{\"end\":50345,\"start\":50338},{\"end\":50358,\"start\":50354},{\"end\":50366,\"start\":50363},{\"end\":50381,\"start\":50376},{\"end\":50390,\"start\":50388},{\"end\":50403,\"start\":50399},{\"end\":50421,\"start\":50413},{\"end\":50440,\"start\":50434},{\"end\":50572,\"start\":50566},{\"end\":50588,\"start\":50583},{\"end\":50600,\"start\":50597},{\"end\":50620,\"start\":50611},{\"end\":50717,\"start\":50711},{\"end\":50726,\"start\":50723},{\"end\":50737,\"start\":50732},{\"end\":50752,\"start\":50745},{\"end\":50769,\"start\":50764},{\"end\":50786,\"start\":50777},{\"end\":50801,\"start\":50795},{\"end\":50817,\"start\":50812},{\"end\":50831,\"start\":50824},{\"end\":50845,\"start\":50840},{\"end\":50849,\"start\":50847},{\"end\":50969,\"start\":50965},{\"end\":50979,\"start\":50975},{\"end\":50991,\"start\":50988},{\"end\":51001,\"start\":50998},{\"end\":51012,\"start\":51010},{\"end\":51027,\"start\":51023},{\"end\":51041,\"start\":51036},{\"end\":51055,\"start\":51051},{\"end\":51064,\"start\":51061},{\"end\":51069,\"start\":51066},{\"end\":51170,\"start\":51162},{\"end\":51190,\"start\":51182},{\"end\":51205,\"start\":51199},{\"end\":51224,\"start\":51213},{\"end\":51236,\"start\":51233},{\"end\":51254,\"start\":51244},{\"end\":51268,\"start\":51262},{\"end\":51285,\"start\":51280},{\"end\":51305,\"start\":51296},{\"end\":51318,\"start\":51313},{\"end\":51325,\"start\":51320},{\"end\":51371,\"start\":51365},{\"end\":51382,\"start\":51380},{\"end\":51396,\"start\":51392},{\"end\":51411,\"start\":51405},{\"end\":51430,\"start\":51421},{\"end\":51440,\"start\":51438},{\"end\":51449,\"start\":51445},{\"end\":51462,\"start\":51458},{\"end\":51468,\"start\":51464},{\"end\":51643,\"start\":51638},{\"end\":51652,\"start\":51648},{\"end\":51665,\"start\":51661},{\"end\":51675,\"start\":51672},{\"end\":51692,\"start\":51685},{\"end\":51704,\"start\":51702},{\"end\":51717,\"start\":51715},{\"end\":51726,\"start\":51723},{\"end\":51747,\"start\":51734},{\"end\":51758,\"start\":51755},{\"end\":51898,\"start\":51894},{\"end\":51908,\"start\":51902},{\"end\":51931,\"start\":51924},{\"end\":52172,\"start\":52162},{\"end\":52189,\"start\":52182},{\"end\":52202,\"start\":52196},{\"end\":52215,\"start\":52211},{\"end\":52502,\"start\":52495},{\"end\":52512,\"start\":52509},{\"end\":52526,\"start\":52521},{\"end\":52542,\"start\":52535},{\"end\":52554,\"start\":52550},{\"end\":52570,\"start\":52563},{\"end\":52586,\"start\":52582},{\"end\":52605,\"start\":52595},{\"end\":52616,\"start\":52614},{\"end\":52632,\"start\":52626},{\"end\":52799,\"start\":52797},{\"end\":52810,\"start\":52808},{\"end\":52825,\"start\":52820},{\"end\":52837,\"start\":52834},{\"end\":53007,\"start\":53005},{\"end\":53018,\"start\":53016},{\"end\":53035,\"start\":53027},{\"end\":53047,\"start\":53044},{\"end\":53201,\"start\":53194},{\"end\":53218,\"start\":53210},{\"end\":53231,\"start\":53227},{\"end\":53372,\"start\":53366},{\"end\":53384,\"start\":53382},{\"end\":53394,\"start\":53389},{\"end\":53409,\"start\":53402},{\"end\":53429,\"start\":53419},{\"end\":53445,\"start\":53438},{\"end\":53458,\"start\":53453},{\"end\":53476,\"start\":53469},{\"end\":53492,\"start\":53487},{\"end\":53502,\"start\":53499},{\"end\":53637,\"start\":53627},{\"end\":53654,\"start\":53646},{\"end\":53674,\"start\":53664},{\"end\":53688,\"start\":53681},{\"end\":53706,\"start\":53699},{\"end\":53893,\"start\":53886},{\"end\":53905,\"start\":53903},{\"end\":53918,\"start\":53913},{\"end\":53930,\"start\":53926},{\"end\":53944,\"start\":53938},{\"end\":53960,\"start\":53951},{\"end\":54079,\"start\":54073},{\"end\":54093,\"start\":54086},{\"end\":54107,\"start\":54100},{\"end\":54122,\"start\":54119},{\"end\":54137,\"start\":54131},{\"end\":54153,\"start\":54147},{\"end\":54165,\"start\":54161},{\"end\":54173,\"start\":54171},{\"end\":54186,\"start\":54183},{\"end\":54252,\"start\":54244},{\"end\":54273,\"start\":54264},{\"end\":54287,\"start\":54282},{\"end\":54303,\"start\":54296},{\"end\":54316,\"start\":54310},{\"end\":54409,\"start\":54402},{\"end\":54421,\"start\":54418},{\"end\":54440,\"start\":54435},{\"end\":54455,\"start\":54448},{\"end\":54468,\"start\":54464},{\"end\":54484,\"start\":54477},{\"end\":54500,\"start\":54492},{\"end\":54526,\"start\":54518},{\"end\":54552,\"start\":54537},{\"end\":54559,\"start\":54554},{\"end\":54669,\"start\":54662},{\"end\":54681,\"start\":54678},{\"end\":54700,\"start\":54695},{\"end\":54715,\"start\":54708},{\"end\":54728,\"start\":54724},{\"end\":54744,\"start\":54737},{\"end\":54760,\"start\":54752},{\"end\":54786,\"start\":54778},{\"end\":54812,\"start\":54797},{\"end\":54819,\"start\":54814},{\"end\":54935,\"start\":54926},{\"end\":54950,\"start\":54945},{\"end\":54967,\"start\":54959},{\"end\":54987,\"start\":54976},{\"end\":55003,\"start\":54997},{\"end\":55017,\"start\":55012},{\"end\":55031,\"start\":55024},{\"end\":55045,\"start\":55039},{\"end\":55063,\"start\":55052},{\"end\":55261,\"start\":55254},{\"end\":55280,\"start\":55270},{\"end\":55299,\"start\":55294},{\"end\":55315,\"start\":55309},{\"end\":55333,\"start\":55325},{\"end\":55503,\"start\":55495},{\"end\":55515,\"start\":55509},{\"end\":55531,\"start\":55527},{\"end\":55545,\"start\":55538},{\"end\":55554,\"start\":55547},{\"end\":55854,\"start\":55849},{\"end\":55871,\"start\":55864},{\"end\":55887,\"start\":55881},{\"end\":55906,\"start\":55897},{\"end\":55926,\"start\":55915},{\"end\":55940,\"start\":55934},{\"end\":55950,\"start\":55947},{\"end\":55970,\"start\":55960},{\"end\":55986,\"start\":55979},{\"end\":56005,\"start\":55994},{\"end\":56117,\"start\":56112},{\"end\":56131,\"start\":56126},{\"end\":56146,\"start\":56138},{\"end\":56250,\"start\":56245},{\"end\":56268,\"start\":56259},{\"end\":56282,\"start\":56277},{\"end\":56295,\"start\":56289},{\"end\":56307,\"start\":56305},{\"end\":56324,\"start\":56316},{\"end\":56337,\"start\":56332},{\"end\":56360,\"start\":56351},{\"end\":56470,\"start\":56466},{\"end\":56482,\"start\":56477},{\"end\":56493,\"start\":56491},{\"end\":56504,\"start\":56502},{\"end\":56521,\"start\":56513},{\"end\":56536,\"start\":56533},{\"end\":56594,\"start\":56582},{\"end\":56610,\"start\":56604},{\"end\":56626,\"start\":56619},{\"end\":56647,\"start\":56639},{\"end\":56665,\"start\":56658},{\"end\":56683,\"start\":56676},{\"end\":56698,\"start\":56691},{\"end\":56710,\"start\":56705},{\"end\":56718,\"start\":56712},{\"end\":56907,\"start\":56895},{\"end\":56923,\"start\":56917},{\"end\":56936,\"start\":56932},{\"end\":56950,\"start\":56944},{\"end\":56965,\"start\":56958},{\"end\":56971,\"start\":56967},{\"end\":57086,\"start\":57083},{\"end\":57094,\"start\":57091},{\"end\":57111,\"start\":57102},{\"end\":57125,\"start\":57119},{\"end\":57138,\"start\":57134},{\"end\":57158,\"start\":57150},{\"end\":57173,\"start\":57165},{\"end\":57188,\"start\":57183},{\"end\":57200,\"start\":57196},{\"end\":57216,\"start\":57209},{\"end\":57226,\"start\":57223},{\"end\":57247,\"start\":57238},{\"end\":57262,\"start\":57255},{\"end\":57275,\"start\":57270},{\"end\":57286,\"start\":57282},{\"end\":57301,\"start\":57296},{\"end\":57447,\"start\":57445},{\"end\":57462,\"start\":57459},{\"end\":57474,\"start\":57472},{\"end\":57489,\"start\":57485},{\"end\":57503,\"start\":57499},{\"end\":57513,\"start\":57509},{\"end\":57640,\"start\":57636},{\"end\":57655,\"start\":57650},{\"end\":57668,\"start\":57663},{\"end\":57681,\"start\":57675},{\"end\":57698,\"start\":57692},{\"end\":57815,\"start\":57811},{\"end\":57830,\"start\":57828},{\"end\":57849,\"start\":57845},{\"end\":57864,\"start\":57861},{\"end\":57885,\"start\":57876},{\"end\":57903,\"start\":57898},{\"end\":57920,\"start\":57917},{\"end\":57928,\"start\":57925},{\"end\":57942,\"start\":57938},{\"end\":57955,\"start\":57951},{\"end\":58025,\"start\":58020},{\"end\":58041,\"start\":58035},{\"end\":58054,\"start\":58049},{\"end\":58069,\"start\":58062},{\"end\":58080,\"start\":58076},{\"end\":58094,\"start\":58090},{\"end\":58113,\"start\":58108},{\"end\":58124,\"start\":58120},{\"end\":58133,\"start\":58131},{\"end\":58150,\"start\":58138},{\"end\":58197,\"start\":58194},{\"end\":58207,\"start\":58203},{\"end\":58227,\"start\":58219},{\"end\":58242,\"start\":58238},{\"end\":58257,\"start\":58252},{\"end\":58276,\"start\":58267},{\"end\":58888,\"start\":58886}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":48461,\"start\":48447},{\"attributes\":{\"id\":\"b1\"},\"end\":48729,\"start\":48463},{\"attributes\":{\"id\":\"b2\"},\"end\":48981,\"start\":48731},{\"attributes\":{\"id\":\"b3\"},\"end\":49293,\"start\":48983},{\"attributes\":{\"id\":\"b4\"},\"end\":49594,\"start\":49295},{\"attributes\":{\"id\":\"b5\"},\"end\":49771,\"start\":49596},{\"attributes\":{\"id\":\"b6\"},\"end\":50020,\"start\":49773},{\"attributes\":{\"id\":\"b7\"},\"end\":50257,\"start\":50022},{\"attributes\":{\"id\":\"b8\"},\"end\":50476,\"start\":50259},{\"attributes\":{\"id\":\"b9\"},\"end\":50656,\"start\":50478},{\"attributes\":{\"id\":\"b10\"},\"end\":50885,\"start\":50658},{\"attributes\":{\"id\":\"b11\"},\"end\":51105,\"start\":50887},{\"attributes\":{\"id\":\"b12\"},\"end\":51361,\"start\":51107},{\"attributes\":{\"id\":\"b13\"},\"end\":51556,\"start\":51363},{\"attributes\":{\"id\":\"b14\"},\"end\":51794,\"start\":51558},{\"attributes\":{\"id\":\"b15\"},\"end\":52086,\"start\":51796},{\"attributes\":{\"id\":\"b16\"},\"end\":52396,\"start\":52088},{\"attributes\":{\"id\":\"b17\"},\"end\":52683,\"start\":52398},{\"attributes\":{\"id\":\"b18\"},\"end\":52893,\"start\":52685},{\"attributes\":{\"id\":\"b19\"},\"end\":53118,\"start\":52895},{\"attributes\":{\"id\":\"b20\"},\"end\":53290,\"start\":53120},{\"attributes\":{\"id\":\"b21\"},\"end\":53561,\"start\":53292},{\"attributes\":{\"id\":\"b22\"},\"end\":53826,\"start\":53563},{\"attributes\":{\"id\":\"b23\"},\"end\":53982,\"start\":53828},{\"attributes\":{\"id\":\"b24\"},\"end\":54237,\"start\":53984},{\"attributes\":{\"id\":\"b25\"},\"end\":54394,\"start\":54239},{\"attributes\":{\"id\":\"b26\"},\"end\":54654,\"start\":54396},{\"attributes\":{\"id\":\"b27\"},\"end\":54914,\"start\":54656},{\"attributes\":{\"id\":\"b28\"},\"end\":55171,\"start\":54916},{\"attributes\":{\"id\":\"b29\"},\"end\":55387,\"start\":55173},{\"attributes\":{\"id\":\"b30\"},\"end\":55735,\"start\":55389},{\"attributes\":{\"id\":\"b31\"},\"end\":56041,\"start\":55737},{\"attributes\":{\"id\":\"b32\"},\"end\":56182,\"start\":56043},{\"attributes\":{\"id\":\"b33\"},\"end\":56366,\"start\":56184},{\"attributes\":{\"id\":\"b34\"},\"end\":56572,\"start\":56368},{\"attributes\":{\"id\":\"b35\"},\"end\":56829,\"start\":56574},{\"attributes\":{\"id\":\"b36\"},\"end\":57030,\"start\":56831},{\"attributes\":{\"id\":\"b37\"},\"end\":57359,\"start\":57032},{\"attributes\":{\"id\":\"b38\"},\"end\":57549,\"start\":57361},{\"attributes\":{\"id\":\"b39\"},\"end\":57734,\"start\":57551},{\"attributes\":{\"id\":\"b40\"},\"end\":57961,\"start\":57736},{\"attributes\":{\"id\":\"b41\"},\"end\":58186,\"start\":57963},{\"attributes\":{\"id\":\"b42\"},\"end\":58402,\"start\":58188},{\"attributes\":{\"id\":\"b43\"},\"end\":58433,\"start\":58404},{\"attributes\":{\"id\":\"b44\"},\"end\":58496,\"start\":58435},{\"attributes\":{\"id\":\"b45\"},\"end\":58691,\"start\":58498},{\"attributes\":{\"id\":\"b46\"},\"end\":58765,\"start\":58693},{\"attributes\":{\"id\":\"b47\"},\"end\":58829,\"start\":58767},{\"attributes\":{\"id\":\"b48\"},\"end\":58890,\"start\":58831}]", "bib_title": "[{\"end\":48518,\"start\":48463},{\"end\":48768,\"start\":48731},{\"end\":49079,\"start\":48983},{\"end\":49382,\"start\":49295},{\"end\":51883,\"start\":51796},{\"end\":52154,\"start\":52088},{\"end\":52486,\"start\":52398},{\"end\":52788,\"start\":52685},{\"end\":52996,\"start\":52895},{\"end\":53184,\"start\":53120},{\"end\":53359,\"start\":53292},{\"end\":53619,\"start\":53563},{\"end\":53879,\"start\":53828},{\"end\":54065,\"start\":53984},{\"end\":55245,\"start\":55173},{\"end\":55486,\"start\":55389},{\"end\":56887,\"start\":56831},{\"end\":57075,\"start\":57032}]", "bib_author": "[{\"end\":48543,\"start\":48520},{\"end\":48557,\"start\":48543},{\"end\":48570,\"start\":48557},{\"end\":48585,\"start\":48570},{\"end\":48596,\"start\":48585},{\"end\":48609,\"start\":48596},{\"end\":48621,\"start\":48609},{\"end\":48636,\"start\":48621},{\"end\":48656,\"start\":48636},{\"end\":48674,\"start\":48656},{\"end\":48781,\"start\":48770},{\"end\":48796,\"start\":48781},{\"end\":48808,\"start\":48796},{\"end\":48825,\"start\":48808},{\"end\":48841,\"start\":48825},{\"end\":48860,\"start\":48841},{\"end\":48880,\"start\":48860},{\"end\":48894,\"start\":48880},{\"end\":48909,\"start\":48894},{\"end\":48924,\"start\":48909},{\"end\":49101,\"start\":49081},{\"end\":49116,\"start\":49101},{\"end\":49126,\"start\":49116},{\"end\":49140,\"start\":49126},{\"end\":49394,\"start\":49384},{\"end\":49403,\"start\":49394},{\"end\":49411,\"start\":49403},{\"end\":49422,\"start\":49411},{\"end\":49441,\"start\":49422},{\"end\":49606,\"start\":49596},{\"end\":49617,\"start\":49606},{\"end\":49637,\"start\":49617},{\"end\":49647,\"start\":49637},{\"end\":49666,\"start\":49647},{\"end\":49864,\"start\":49848},{\"end\":49876,\"start\":49864},{\"end\":49884,\"start\":49876},{\"end\":49896,\"start\":49884},{\"end\":49909,\"start\":49896},{\"end\":49920,\"start\":49909},{\"end\":49935,\"start\":49920},{\"end\":49950,\"start\":49935},{\"end\":49966,\"start\":49950},{\"end\":49985,\"start\":49966},{\"end\":49997,\"start\":49985},{\"end\":50010,\"start\":49997},{\"end\":50043,\"start\":50022},{\"end\":50058,\"start\":50043},{\"end\":50072,\"start\":50058},{\"end\":50087,\"start\":50072},{\"end\":50102,\"start\":50087},{\"end\":50116,\"start\":50102},{\"end\":50129,\"start\":50116},{\"end\":50146,\"start\":50129},{\"end\":50162,\"start\":50146},{\"end\":50182,\"start\":50162},{\"end\":50323,\"start\":50306},{\"end\":50331,\"start\":50323},{\"end\":50347,\"start\":50331},{\"end\":50360,\"start\":50347},{\"end\":50368,\"start\":50360},{\"end\":50383,\"start\":50368},{\"end\":50392,\"start\":50383},{\"end\":50405,\"start\":50392},{\"end\":50423,\"start\":50405},{\"end\":50442,\"start\":50423},{\"end\":50574,\"start\":50560},{\"end\":50590,\"start\":50574},{\"end\":50602,\"start\":50590},{\"end\":50622,\"start\":50602},{\"end\":50719,\"start\":50705},{\"end\":50728,\"start\":50719},{\"end\":50739,\"start\":50728},{\"end\":50754,\"start\":50739},{\"end\":50771,\"start\":50754},{\"end\":50788,\"start\":50771},{\"end\":50803,\"start\":50788},{\"end\":50819,\"start\":50803},{\"end\":50833,\"start\":50819},{\"end\":50847,\"start\":50833},{\"end\":50851,\"start\":50847},{\"end\":50971,\"start\":50959},{\"end\":50981,\"start\":50971},{\"end\":50993,\"start\":50981},{\"end\":51003,\"start\":50993},{\"end\":51014,\"start\":51003},{\"end\":51029,\"start\":51014},{\"end\":51043,\"start\":51029},{\"end\":51057,\"start\":51043},{\"end\":51066,\"start\":51057},{\"end\":51071,\"start\":51066},{\"end\":51172,\"start\":51155},{\"end\":51192,\"start\":51172},{\"end\":51207,\"start\":51192},{\"end\":51226,\"start\":51207},{\"end\":51238,\"start\":51226},{\"end\":51256,\"start\":51238},{\"end\":51270,\"start\":51256},{\"end\":51287,\"start\":51270},{\"end\":51307,\"start\":51287},{\"end\":51320,\"start\":51307},{\"end\":51327,\"start\":51320},{\"end\":51373,\"start\":51363},{\"end\":51384,\"start\":51373},{\"end\":51398,\"start\":51384},{\"end\":51413,\"start\":51398},{\"end\":51432,\"start\":51413},{\"end\":51442,\"start\":51432},{\"end\":51451,\"start\":51442},{\"end\":51464,\"start\":51451},{\"end\":51470,\"start\":51464},{\"end\":51645,\"start\":51630},{\"end\":51654,\"start\":51645},{\"end\":51667,\"start\":51654},{\"end\":51677,\"start\":51667},{\"end\":51694,\"start\":51677},{\"end\":51706,\"start\":51694},{\"end\":51719,\"start\":51706},{\"end\":51728,\"start\":51719},{\"end\":51749,\"start\":51728},{\"end\":51760,\"start\":51749},{\"end\":51900,\"start\":51885},{\"end\":51910,\"start\":51900},{\"end\":51933,\"start\":51910},{\"end\":52174,\"start\":52156},{\"end\":52191,\"start\":52174},{\"end\":52204,\"start\":52191},{\"end\":52217,\"start\":52204},{\"end\":52504,\"start\":52488},{\"end\":52514,\"start\":52504},{\"end\":52528,\"start\":52514},{\"end\":52544,\"start\":52528},{\"end\":52556,\"start\":52544},{\"end\":52572,\"start\":52556},{\"end\":52588,\"start\":52572},{\"end\":52607,\"start\":52588},{\"end\":52618,\"start\":52607},{\"end\":52634,\"start\":52618},{\"end\":52801,\"start\":52790},{\"end\":52812,\"start\":52801},{\"end\":52827,\"start\":52812},{\"end\":52839,\"start\":52827},{\"end\":53009,\"start\":52998},{\"end\":53020,\"start\":53009},{\"end\":53037,\"start\":53020},{\"end\":53049,\"start\":53037},{\"end\":53203,\"start\":53186},{\"end\":53220,\"start\":53203},{\"end\":53233,\"start\":53220},{\"end\":53374,\"start\":53361},{\"end\":53386,\"start\":53374},{\"end\":53396,\"start\":53386},{\"end\":53411,\"start\":53396},{\"end\":53431,\"start\":53411},{\"end\":53447,\"start\":53431},{\"end\":53460,\"start\":53447},{\"end\":53478,\"start\":53460},{\"end\":53494,\"start\":53478},{\"end\":53504,\"start\":53494},{\"end\":53639,\"start\":53621},{\"end\":53656,\"start\":53639},{\"end\":53676,\"start\":53656},{\"end\":53690,\"start\":53676},{\"end\":53708,\"start\":53690},{\"end\":53895,\"start\":53881},{\"end\":53907,\"start\":53895},{\"end\":53920,\"start\":53907},{\"end\":53932,\"start\":53920},{\"end\":53946,\"start\":53932},{\"end\":53962,\"start\":53946},{\"end\":54081,\"start\":54067},{\"end\":54095,\"start\":54081},{\"end\":54109,\"start\":54095},{\"end\":54124,\"start\":54109},{\"end\":54139,\"start\":54124},{\"end\":54155,\"start\":54139},{\"end\":54167,\"start\":54155},{\"end\":54175,\"start\":54167},{\"end\":54188,\"start\":54175},{\"end\":54254,\"start\":54239},{\"end\":54275,\"start\":54254},{\"end\":54289,\"start\":54275},{\"end\":54305,\"start\":54289},{\"end\":54318,\"start\":54305},{\"end\":54411,\"start\":54396},{\"end\":54423,\"start\":54411},{\"end\":54442,\"start\":54423},{\"end\":54457,\"start\":54442},{\"end\":54470,\"start\":54457},{\"end\":54486,\"start\":54470},{\"end\":54502,\"start\":54486},{\"end\":54528,\"start\":54502},{\"end\":54554,\"start\":54528},{\"end\":54561,\"start\":54554},{\"end\":54671,\"start\":54656},{\"end\":54683,\"start\":54671},{\"end\":54702,\"start\":54683},{\"end\":54717,\"start\":54702},{\"end\":54730,\"start\":54717},{\"end\":54746,\"start\":54730},{\"end\":54762,\"start\":54746},{\"end\":54788,\"start\":54762},{\"end\":54814,\"start\":54788},{\"end\":54821,\"start\":54814},{\"end\":54937,\"start\":54916},{\"end\":54952,\"start\":54937},{\"end\":54969,\"start\":54952},{\"end\":54989,\"start\":54969},{\"end\":55005,\"start\":54989},{\"end\":55019,\"start\":55005},{\"end\":55033,\"start\":55019},{\"end\":55047,\"start\":55033},{\"end\":55065,\"start\":55047},{\"end\":55263,\"start\":55247},{\"end\":55282,\"start\":55263},{\"end\":55301,\"start\":55282},{\"end\":55317,\"start\":55301},{\"end\":55335,\"start\":55317},{\"end\":55505,\"start\":55488},{\"end\":55517,\"start\":55505},{\"end\":55533,\"start\":55517},{\"end\":55547,\"start\":55533},{\"end\":55556,\"start\":55547},{\"end\":55856,\"start\":55842},{\"end\":55873,\"start\":55856},{\"end\":55889,\"start\":55873},{\"end\":55908,\"start\":55889},{\"end\":55928,\"start\":55908},{\"end\":55942,\"start\":55928},{\"end\":55952,\"start\":55942},{\"end\":55972,\"start\":55952},{\"end\":55988,\"start\":55972},{\"end\":56007,\"start\":55988},{\"end\":56119,\"start\":56106},{\"end\":56133,\"start\":56119},{\"end\":56148,\"start\":56133},{\"end\":56252,\"start\":56239},{\"end\":56270,\"start\":56252},{\"end\":56284,\"start\":56270},{\"end\":56297,\"start\":56284},{\"end\":56309,\"start\":56297},{\"end\":56326,\"start\":56309},{\"end\":56339,\"start\":56326},{\"end\":56362,\"start\":56339},{\"end\":56472,\"start\":56458},{\"end\":56484,\"start\":56472},{\"end\":56495,\"start\":56484},{\"end\":56506,\"start\":56495},{\"end\":56523,\"start\":56506},{\"end\":56538,\"start\":56523},{\"end\":56596,\"start\":56574},{\"end\":56612,\"start\":56596},{\"end\":56628,\"start\":56612},{\"end\":56649,\"start\":56628},{\"end\":56667,\"start\":56649},{\"end\":56685,\"start\":56667},{\"end\":56700,\"start\":56685},{\"end\":56712,\"start\":56700},{\"end\":56720,\"start\":56712},{\"end\":56909,\"start\":56889},{\"end\":56925,\"start\":56909},{\"end\":56938,\"start\":56925},{\"end\":56952,\"start\":56938},{\"end\":56967,\"start\":56952},{\"end\":56973,\"start\":56967},{\"end\":57088,\"start\":57077},{\"end\":57096,\"start\":57088},{\"end\":57113,\"start\":57096},{\"end\":57127,\"start\":57113},{\"end\":57140,\"start\":57127},{\"end\":57160,\"start\":57140},{\"end\":57175,\"start\":57160},{\"end\":57190,\"start\":57175},{\"end\":57202,\"start\":57190},{\"end\":57218,\"start\":57202},{\"end\":57228,\"start\":57218},{\"end\":57249,\"start\":57228},{\"end\":57264,\"start\":57249},{\"end\":57277,\"start\":57264},{\"end\":57288,\"start\":57277},{\"end\":57303,\"start\":57288},{\"end\":57449,\"start\":57437},{\"end\":57464,\"start\":57449},{\"end\":57476,\"start\":57464},{\"end\":57491,\"start\":57476},{\"end\":57505,\"start\":57491},{\"end\":57515,\"start\":57505},{\"end\":57642,\"start\":57628},{\"end\":57657,\"start\":57642},{\"end\":57670,\"start\":57657},{\"end\":57683,\"start\":57670},{\"end\":57700,\"start\":57683},{\"end\":57817,\"start\":57801},{\"end\":57821,\"start\":57817},{\"end\":57832,\"start\":57821},{\"end\":57836,\"start\":57832},{\"end\":57851,\"start\":57836},{\"end\":57855,\"start\":57851},{\"end\":57866,\"start\":57855},{\"end\":57870,\"start\":57866},{\"end\":57887,\"start\":57870},{\"end\":57891,\"start\":57887},{\"end\":57905,\"start\":57891},{\"end\":57909,\"start\":57905},{\"end\":57922,\"start\":57909},{\"end\":57930,\"start\":57922},{\"end\":57944,\"start\":57930},{\"end\":57957,\"start\":57944},{\"end\":58027,\"start\":58014},{\"end\":58043,\"start\":58027},{\"end\":58056,\"start\":58043},{\"end\":58071,\"start\":58056},{\"end\":58082,\"start\":58071},{\"end\":58096,\"start\":58082},{\"end\":58115,\"start\":58096},{\"end\":58126,\"start\":58115},{\"end\":58135,\"start\":58126},{\"end\":58152,\"start\":58135},{\"end\":58199,\"start\":58188},{\"end\":58209,\"start\":58199},{\"end\":58229,\"start\":58209},{\"end\":58244,\"start\":58229},{\"end\":58259,\"start\":58244},{\"end\":58278,\"start\":58259},{\"end\":58890,\"start\":58883}]", "bib_venue": "[{\"end\":48455,\"start\":48447},{\"end\":48723,\"start\":48674},{\"end\":48973,\"start\":48924},{\"end\":49221,\"start\":49140},{\"end\":49522,\"start\":49441},{\"end\":49751,\"start\":49682},{\"end\":49846,\"start\":49773},{\"end\":50237,\"start\":50198},{\"end\":50304,\"start\":50259},{\"end\":50558,\"start\":50478},{\"end\":50703,\"start\":50658},{\"end\":50957,\"start\":50887},{\"end\":51153,\"start\":51107},{\"end\":51536,\"start\":51486},{\"end\":51628,\"start\":51558},{\"end\":52014,\"start\":51933},{\"end\":52311,\"start\":52217},{\"end\":52674,\"start\":52634},{\"end\":52883,\"start\":52839},{\"end\":53092,\"start\":53065},{\"end\":53282,\"start\":53233},{\"end\":53553,\"start\":53504},{\"end\":53759,\"start\":53708},{\"end\":53973,\"start\":53962},{\"end\":54228,\"start\":54188},{\"end\":54374,\"start\":54334},{\"end\":54633,\"start\":54577},{\"end\":54893,\"start\":54837},{\"end\":55151,\"start\":55081},{\"end\":55373,\"start\":55335},{\"end\":55643,\"start\":55556},{\"end\":55656,\"start\":55645},{\"end\":55840,\"start\":55737},{\"end\":56104,\"start\":56043},{\"end\":56237,\"start\":56184},{\"end\":56456,\"start\":56368},{\"end\":56809,\"start\":56736},{\"end\":57022,\"start\":56973},{\"end\":57344,\"start\":57303},{\"end\":57435,\"start\":57361},{\"end\":57626,\"start\":57551},{\"end\":57799,\"start\":57736},{\"end\":58012,\"start\":57963},{\"end\":58382,\"start\":58294},{\"end\":58423,\"start\":58404},{\"end\":58494,\"start\":58435},{\"end\":58689,\"start\":58498},{\"end\":58763,\"start\":58693},{\"end\":58827,\"start\":58767},{\"end\":58881,\"start\":58831},{\"end\":49289,\"start\":49223},{\"end\":49590,\"start\":49524},{\"end\":52082,\"start\":52016},{\"end\":52392,\"start\":52313},{\"end\":53772,\"start\":53761},{\"end\":55730,\"start\":55658}]"}}}, "year": 2023, "month": 12, "day": 17}