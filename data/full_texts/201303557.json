{"id": 201303557, "updated": "2023-10-06 23:56:17.13", "metadata": {"title": "Transferability and Hardness of Supervised Classification Tasks", "authors": "[{\"first\":\"Anh\",\"last\":\"Tran\",\"middle\":[\"T.\"]},{\"first\":\"Cuong\",\"last\":\"Nguyen\",\"middle\":[\"V.\"]},{\"first\":\"Tal\",\"last\":\"Hassner\",\"middle\":[]}]", "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2019, "month": 8, "day": 21}, "abstract": "We propose a novel approach for estimating the difficulty and transferability of supervised classification tasks. Unlike previous work, our approach is solution agnostic and does not require or assume trained models. Instead, we estimate these values using an information theoretic approach: treating training labels as random variables and exploring their statistics. When transferring from a source to a target task, we consider the conditional entropy between two such variables (i.e., label assignments of the two tasks). We show analytically and empirically that this value is related to the loss of the transferred model. We further show how to use this value to estimate task hardness. We test our claims extensively on three large scale data sets -- CelebA (40 tasks), Animals with Attributes 2 (85 tasks), and Caltech-UCSD Birds 200 (312 tasks) -- together representing 437 classification tasks. We provide results showing that our hardness and transferability estimates are strongly correlated with empirical hardness and transferability. As a case study, we transfer a learned face recognition model to CelebA attribute classification tasks, showing state of the art accuracy for tasks estimated to be highly transferable.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1908.08142", "mag": "2981848390", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/TranNH19", "doi": "10.1109/iccv.2019.00148"}}, "content": {"source": {"pdf_hash": "41db7308930ea8bf2fbefcab41294fac6f929721", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1908.08142v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1908.08142", "status": "GREEN"}}, "grobid": {"id": "7cf893c6e4aafcb5dc5398aa7938a8454bdd8b59", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/41db7308930ea8bf2fbefcab41294fac6f929721.txt", "contents": "\nTransferability and Hardness of Supervised Classification Tasks\n\n\nAnh T Tran \nFacebook AI\nVinAI Research\n\n\nTal Hassner talhassner@gmail.com \nFacebook AI\nVinAI Research\n\n\nTransferability and Hardness of Supervised Classification Tasks\nCuong V. Nguyen Amazon Web Services\nWe propose a novel approach for estimating the difficulty and transferability of supervised classification tasks. Unlike previous work, our approach is solution agnostic and does not require or assume trained models. Instead, we estimate these values using an information theoretic approach: treating training labels as random variables and exploring their statistics. When transferring from a source to a target task, we consider the conditional entropy between two such variables (i.e., label assignments of the two tasks). We show analytically and empirically that this value is related to the loss of the transferred model. We further show how to use this value to estimate task hardness. We test our claims extensively on three large scale data sets-CelebA (40 tasks), Animals with Attributes 2 (85 tasks), and Caltech-UCSD Birds 200 (312 tasks)-together representing 437 classification tasks. We provide results showing that our hardness and transferability estimates are strongly correlated with empirical hardness and transferability. As a case study, we transfer a learned face recognition model to CelebA attribute classification tasks, showing state of the art accuracy for tasks estimated to be highly transferable.\n\nIntroduction\n\nHow easy is it to transfer a representation learned for one task to another? How can we tell which of several tasks is hardest to solve? Answers to these questions are vital in planning model transfer and reuse, and can help reveal fundamental properties of tasks and their relationships in the process of developing universal perception engines [3]. The importance of these questions is therefore driving research efforts, with several answers proposed in recent years.\n\nSome of the answers to these questions established task relationship indices, as in the Taskonomy [71] and Task2Vec [1,2] projects. Others analyzed task relationships in the context of multi-task learning [31, 37,61,68,73]. Importantly, however, these and other efforts are computa- * Work at Amazon Web Services, prior to joining current affiliation. tional in nature, and so build on specific machine learning solutions as proxy task representations.\n\nBy relying on such proxy task representations, these approaches are naturally limited in their application: Rather than insights on the tasks themselves, they may reflect relationships between the specific solutions chosen to represent them, as noted by previous work [71]. Some, moreover, establish task relationships by maintaining model zoos, with existing trained models already available. They may therefore also be computationally expensive [1,71]. Finally, in some scenarios, establishing task relationships requires multi-task learning of the models, to measure the influence different tasks have on each other [31, 37,61,68,73].\n\nWe propose a radically different, solution agnostic approach: We seek underlying relationships, irrespective of the particular models trained to solve these tasks or whether these models even exist. We begin by noting that supervised learning problems are defined not by the models trained to solve them, but rather by the data sets of labeled examples and a choice of loss functions. We therefore go to the source and explore tasks directly, by examining their data sets rather than the models they were used to train.\n\nTo this end, we consider supervised classification tasks defined over the same input domain. As a loss, we assume the cross entropy function, thereby including most commonly used loss functions. We offer the following surprising result: By assuming an optimal loss on two tasks, the conditional entropy (CE) between the label sequences of their training sets provides a bound on the transferability of the two tasks-that is, the log-likelihood on a target task for a trained representation transferred from a source task. We then use this result to obtain a-priori estimates of task transferability and hardness.\n\nImportantly, we obtain effective transferability and hardness estimates by evaluating only training labels; we do not consider the solutions trained for each task or the input domain. This result is surprising considering that it greatly simplifies estimating task hardness and task relationships, yet, as far as we know, was overlooked by previous work.\n\nWe verify our claims with rigorous tests on a total of 437 tasks from the CelebA [35], Animals with Attributes 2 (AwA2) [67], and Caltech-UCSD Birds 200 (CUB) [66] sets. We show that our approach reliably predicts task transferability and hardness. As a case study, we evaluate transferability from face recognition to facial attribute classification. On attributes estimated to be highly transferable from recognition, our results outperform the state of the art despite using a simple approach, involving training a linear support vector machine per attribute.\n\n\nRelated work\n\nOur work is related to many fields in machine learning and computer vision, including transfer learning [69], meta learning [56], domain shifting [54], and multi-task learning [29]. Below we provide only a cursory overview of several methods directly related to us. For more principled surveys on transfer learning, we refer to others [4,48,65,70].\n\nTransfer learning. This paper is related to transfer learning [48,65,69] and our work can be used to select good source tasks and data sets when transferring learned models. Previous theoretical analysis of transfer learning is extensive [2,5,6,7,8,39]. These papers allowed generalization bounds to be proven but they are abstract and hard to compute in practice. Our transferability measure, on the other hand, is easily computed from the training sets and can potentially be useful also for continual learning [44,45,52].\n\nTask spaces. Tasks in machine learning are often represented as labeled data sets and a loss function. For some applications, qualitative exploration of the training data can reveal relationships between two tasks and, in particular, the biases between them [59].\n\nEfforts to obtain more complex task relationships involved trained models. Data sets were compared using fixed dimensional lists of statistics, produced using an autoencoder trained for this purpose [19]. The successful Taskonomy project [71], like us, assumes multiple task labels for the same input images (same input domain). They train one model per-task and then evaluate transfers between tasks thereby creating a task hypergraph-their taxonomy.\n\nFinally, Task2Vec constructs vector representations for tasks, obtained by mapping partially trained probe networks down to low dimensional task embeddings [1,2]. Unlike these methods, we consider only the labels provided in the training data for each task, without using trained models.\n\nMulti-task learning. Training a single model to solve multiple tasks can be mutually beneficial to the individual tasks [24, 51,64]. When two tasks are only weakly related, however, attempting to train a model for them both can produce a model which under-performs compared to models trained for each task separately. Early multi-branch networks and their variants encoded human knowledge on the relationships of tasks in their design, joining related tasks or separating unrelated tasks [28, 50,53].\n\nOthers adjusted for related vs. unrelated tasks during training of a deep multi-task network. Deep cross residual learning does this by introducing cross-residuals for regularization [28], cross-stitch combines activations from multiple task-specific networks [43], and UberNet proposed a task-specific branching scheme [30].\n\nSome sought to discover what and how should be shared across tasks during training by automatic discovery of network designs that would group similar tasks together [37] or by solving tensor factorization problems [68]. Alternatively, parts of the input rather than the network were masked according to the task at hand [61]. Finally, modulation modules were proposed to seek destructive interferences between unrelated tasks [73].\n\n\nTransferability via conditional entropy\n\nWe seek information on the transferability and hardness of supervised classification tasks. Previous work obtained this information by examining machine learning models developed for these tasks [1,2,73]. Such models are produced by training on labeled data sets that represent the tasks. These models can therefore be considered views on their training data. In this work we instead use information theory to produce estimates from the source: the data itself.\n\nLike others [71], we assume our tasks share the same input instances and are different only in the labels they assign to each input. Such settings describe many practical scenarios. A set of face images, for instance, can have multiple labels for each image, representing tasks such as recognition [40,41] and classification of various attributes [35].\n\nWe estimate transferability using the CE between the label sequences of the target and source tasks. Task hardness is similarly estimated: by computing transferability from a trivial task. We next formalize our assumptions and claims.\n\n\nTask transferability\n\nWe assume a single input sequence of training samples, X = (x 1 , x 2 , . . . , x n ) \u2208 X n , along with two label sequences Y = (y 1 , y 2 , . . . , y n ) \u2208 Y n and Z = (z 1 , z 2 , . . . , z n ) \u2208 Z n , where y i and z i are labels assigned to x i under two separate tasks: source task T Z = (X, Z) and target task T Y = (X, Y ). Here, X is the domain of the values of X, while Y = range(Y ) and Z = range(Z) are the sets of different values in Y and Z respectively. Thus, if Z contains binary labels, then Z = {0, 1}.\n\nWe consider a classification model M = (w, h) on the source task, T Z . The first part, w : X \u2192 R D , is some transformation function, possibly learned, that outputs a Ddimensional representation r = w(x) \u2208 R D for an input x \u2208 X . The second part, h : R D \u2192 P(Z), is a classifier that takes a representation r and produces a probability distribution h(r) \u2208 P(Z), where P(Z) is the space of all probability distributions over Z.\n\nThis description emphasizes the two canonical stages of a machine learning system [17]: representation followed by classification. As an example, a deep neural network with Softmax output is represented by a learned w which maps the input into some feature space, producing a deep embedding, r, followed by classification layers (one or more), h, which maps the embedding into the prediction probability. Now, assume we train a model (w Z , h Z ) to solve T Z by minimizing the cross entropy loss on Z:\nw Z , h Z = argmin w,h\u2208(W,H) L Z (w, h),(1)\nwhere W and H are our chosen spaces of possible values for w and h, and L Z (w, h) is the cross entropy loss (equivalently, the negative log-likelihood) of the parameters (w, h):\nL Z (w, h) = \u2212l Z (w, h) = \u2212 1 n n i=1 log P (z i |x i ; w, h), (2) where l Z (w, h) is the log-likelihood of (w, h).\nTo transfer this model to target task T Y , we fix the function w Z and retrain only the classifier on the labels of T Y . Denote the new classifier k Y , selected from our chosen space K of target classifiers. Note that k Y does not necessarily share the same architecture as h Z . We train k Y by minimizing the cross entropy loss on Y with the fixed w Z :\nk Y = argmin k\u2208K L Y (w Z , k),(3)\nwhere L Y (w Z , k) is defined similarly to Eq. (2) but for the label set Y . Under this setup, we define the transferability of task T Z to task T Y as follows.\n\nDefinition 1 The transferability of task T Z to task T Y is measured by the expected accuracy of the model (w Z , k Y ) on a random test example (x, y) of task T Y :\nTrf(T Z \u2192 T Y ) = E [acc(y, x; w Z , k Y )] ,(4)\nwhich indicates how well a representation w Z trained on task T Z performs on task T Y .\n\nIn practice, if the trained model does not overfit, the log-likelihood on the training set, l Y (w Z , k Y ), provides a good indicator of Eq (4), that is, how well the representation w Z and the classifier k Y performs on task T Y . This nonoverfitting assumption holds even for large networks that are properly trained and tested on datasets sampled from the same distribution [72]. Thus, in the subsequent sections, we instead consider the following log-likelihood as an alternative measure of transferability:\nTrf(T Z \u2192 T Y ) = l Y (w Z , k Y ).(5)\n\nThe conditional entropy of label sequences\n\nFrom the label sequences Y and Z, we can compute the empirical joint distributionP (y, z) for all (y, z) \u2208 Y \u00d7 Z by counting, as follows:\nP (y, z) = 1 n |{i : y i = y and z i = z}|.(6)\nWe now adopt the definition of CE between two random variables [14] to define the CE between our label sequences Y and Z.\n\n\nDefinition 2\n\nThe CE of a label sequence Y given a label sequence Z, H(Y |Z), is the CE of a random variable (or random label)\u0233 given a random variable (or random label) z, where (\u0233,z) are drawn from the empirical joint distribu-tionP (y, z) of Eq. (6):\nH(Y |Z) = \u2212 y\u2208Y z\u2208ZP (y, z) logP (y, z) P (z) ,(7)\nwhereP (z) is the empirical marginal distribution on Z:\nP (z) = y\u2208YP (y, z) = 1 n |{i : z i = z}|.(8)\nCE represents a measure of the amount of information provided by the value of one random variable on the value of another. By treating the labels assigned to both tasks as random variables and measuring the CE between them, we are measuring the information required to estimate a label in one task given a (known) label in another task.\n\nWe now prove a relationship between the CE of Eq. (7) and the tranferability of Eq. (5). In particular, we show that the log-likelihood on the target task T Y is lower bounded by log-likelihood on the source task T Z minus H(Y |Z), if the optimal input representation w Z trained on T Z is transferred to T Y .\n\nTo prove our theorem, we assume the space K of target classifiers contains a classifierk whose log-likelihood lower bounds that of k Y . We constructk as follows. For each input x, we compute the Softmax output p Z = h Z (w Z (x)), which is a probability distribution on Z. We then convert p Z into a Softmax on Y by taking the expectation of the empirical conditional probabilityP (y|z) =P (y, z)/P (z) with respect to p Z . That is, for all y \u2208 Y, we define: (9) where p Z (z) is the probability of the label z returned by p Z . For any input w Z (x), we let the output ofk be p Y . That is, k(w Z (x)) = p Y . We can now prove the following theorem. Theorem 1 Under the training procedure described in Sec. 3.1, we have:\np Y (y) = E z\u223cp Z [P (y|z)] = z\u2208ZP (y|z) p Z (z),Trf(T Z \u2192 T Y ) \u2265 l Z (w Z , h Z ) \u2212 H(Y |Z).(10)\nProof sketch. 1 From the definition of k Y and the assump-\ntion thatk \u2208 K, we have Trf(T Z \u2192 T Y ) = l Y (w Z , k Y ) \u2265 l Y (w Z ,k)\n. From the construction ofk, we have:\nl Y (w Z ,k) = 1 n n i=1 log z\u2208ZP (y i |z)P (z|x i ; w Z , h Z ) \u2265 1 n n i=1 log P (y i |z i )P (z i |x i ; w Z , h Z ) (11) = 1 n n i=1 logP (y i |z i ) + 1 n n i=1 log P (z i |x i ; w Z , h Z ). (12)\nWe can easily show that the first term in Eq. (12) equals \u2212H(Y |Z), while the second term is l Z (w Z , h Z ).\n\nDiscussion 1: Generality of our settings. Our settings for spaces W, H, K are general and include a variety of practical use cases. For example, neural networks W will include all possible (vector) values of the network weights until the penultimate layer, while H and K would include all possible (vector) values of the last layer's weights. Alternatively, we can use support vector machines (SVM) for K. In this case, K would include all possible values of the SVM parameters [57]. Our result even holds when the features are fixed, as when using tailored representations such as SIFT [36]. In these cases, space W would contain only one transformation function from raw input to the features.\n\nDiscussion 2: Assumptions. We can easily satisfy the assumptionk \u2208 K by first choosing a space K (e.g., the SVMs) which will play the role of K \\ {k}. We solve the optimization problem of Eq. (3) on K instead of K to obtain the optimal classifier k . To get the optimal classifier k Y on K = K \u222a {k}, we simply compare the losses of k andk and select the best one as k Y . The optimization problems of Eq. (1) and (3) are global optimization problems. In practice, for complex deep networks trained with stochastic gradient descent, we often 1 Full derivations provided in the appendix. only obtain the local optima of the loss. In this case, we can easily change and prove Theorem 1 which would include the differences in the losses between the local optimum and the global optimum in the right-hand-side of Eq. (10). In many practical applications, the difference between local optimum and global optimum is not significant [13,46].\n\n\nDiscussion 3: Extending our result to test log-likelihood.\n\nIn Theorem 1, we consider the empirical log-likelihood, which is generally unbounded. If we make the (strong) assumption of bounded differences between empirical loglikelihoods, we can apply McDiarmids inequality [42] to get an upper-bound on the left hand side of Eq. (10) by the expected log-likelihood with some probability.\n\nDiscussion 4: Implications. Theorem 1 shows that the transferability from task T Z to task T Y depends on both the CE H(Y |Z) and the log-likelihood l Z (w Z , h Z ). Note that the log-likelihood l Z (w Z , h Z ) is optimal for task T Z and so it represents the hardness (or easiness) of task T Z . Thus, from the theorem, if l Z (w Z , h Z ) is small (i.e., the source task is hard), transferability would reduce. Besides, if the CE H(Y |Z) is small, transferability would increase.\n\nFinally, we note that when the source task T Z is fixed, the log-likelihood l Z (w Z , h Z ) is a constant. In this case, the transferability only depends on the CE H(Y |Z). Thus, we can estimate the transferability from one source task to multiple target tasks by considering only the CE.\n\n\nIntuition and toy examples\n\nTo gain intuition on CE and transferability, consider the toy examples illustrated in Fig. 1. The (joint) input set is represented by the X axis. Each input x i \u2208 X is assigned with two labels, y i \u2208 Y and z i \u2208 Z, for the two tasks. In Fig. 1(a,b), task T Z is the trivial task with a constant label value (red line) and in Fig. 1(c-e) T Z is a binary classification task, whereas T Y is binary in Fig. 1(a-d) and multilabel in Fig. 1(e). In which of these examples would transferring a representation trained for T Z to T Y be hardest?\n\nOf the five examples, (c) is the easiest transfer as it provides a 1-1 mapping from Z to Y . Appropriately, in this case, H(Y |Z) = 0. Next up are (d) and (e) with H(Y |Z) = log 2: In both cases each class in T Z is mapped to two classes in T Y . Note that T Y being non-binary is naturally handled by the CE. Finally, transfers (a) and (b) have H(Y |Z) = 4 log 2; the highest CE. Because T Z is trivial, the transfer must account for the greatest difference in the information between the tasks and so the transfer is hardest.\n\n\nTask hardness\n\nA potential application of transferability is task hardness. In Sec. 3.2, we mentioned that the hardness of a task can be measured from the optimal log-likelihood on that task. Formally, we can measure the hardness of a task T Z by:\nHard(T Z ) = min w,h\u2208(W,H) L Z (w, h) = \u2212l Z (w Z , h Z ). (13)\nThis definition of hardness depends on our choice of (W, H), which may determine various factors such as representation size or network architecture. The intuition behind the definition is that if the task T Z is hard for all models in (W, H), we should expect higher loss even after training.\n\nUsing Theorem 1, we can bound l Z (w Z , h Z ) in Eq. (13) by transferring from a trivial task T C to T Z . We define a trivial task as the task for which all input values are assigned the same, constant label. Let C be the (constant) label sequences of the trivial task T C . From Theorem 1 and Eq. (13), we can easily show that:\nHard(T Z ) = \u2212l Z (w Z , h Z ) \u2264 H(Z|C).(14)\nThus, we can approximate the hardness of task T Z by looking at the CE H(Z|C). We note that the CE H(Z|C) is also used to estimate the transferability Trf(T C \u2192 T Z ). So, Hard(T Z ) is closely related to Trf(T C \u2192 T Z ). Particularly, if task T Z is hard, we expect it is more difficult to transfer from a trivial task to T Z . This relationship between hardness and transferability from a trivial task is similar to the one proposed by Task2Vec [1]. They too indexed task hardness as the distance from a trivial task. To compute task hardness, however, they required training deep models, whereas we obtain this measure by simply computing H(Z|C) using Eq. (7).\n\nOf course, estimating the hardness by H(Z|C) ignores the input and is hence only an approximation. In particular, one could possibly design scenarios where this measure would not accurately reflect the hardness of a given task. Our results in Sec. 5.3 show, however, that these label statistics provide a strong cue for task hardness.\n\n\nExperiments\n\nWe rigorously evaluate our claims using three large scale, widely used data sets representing 437 classification tasks. Although Sec. 3.2 provides a bound on the training loss, test accuracy is generally more important. We thus report results on test images not included in the training data.\n\nBenchmarks. The Celeb Faces Attributes (CelebA) set [35] was extensively used to evaluate transfer learning [18, 33,34,58]. CelebA contains over 202k face images of 10,177 subjects. Each image is labeled with subject identity as well as 40 binary attributes. We used the standard train / test splits (182,626 / 19,961 images, respectively). To our knowledge, of the three sets, it is the only one that provides baseline results for attribute classification.\n\nAnimals with Attributes 2 (AwA2) [67] includes over 37k images labeled as belonging to one of 50 animals classes. Images are labeled based on their class association with 85 different attributes. Models were trained on 33,568 training images and tested on 3,754 separate test images.\n\nFinally, Caltech-UCSD Birds 200 (CUB) [66] offers 11,788 images of 200 bird species, labeled with 312 attributes as well as Turker Confidence attributes. Labels were averaged across multiple Turkers using confidences. Finally, we kept only reliable labels, using a threshold of 0.5 on the average confidence value. We used 5,994 images for training and 5,794 images for testing.\n\nWe note that the Task Bank set with its 26 tasks was also used for evaluating task relationships [71]. We did not use it here as it mostly contains regression tasks rather than the classification problems we are concerned with.\n\n\nEvaluating task transferability\n\nWe compared our transferability estimates from the CE to the actual transferability of Eq. (4). To this end, for each attribute T Z in a data set, we measure the actual transferability, Trf(T Z \u2192 T Y ), to all other attributes T Y in that set using the test split. We then compare these transferability scores to the corresponding CE estimates of Eq. (7) using an existing correlation analysis [44].\n\nWe note again that when the source task is fixed, as in this case, the transferability estimates can be obtained by considering only the CE. Furthermore, since Trf(T Z \u2192 T Y ) and the CE H(Y |Z) are negative correlated, we compare the correlation between the test error rate, 1 \u2212 Trf(T Z \u2192 T Y ), and the CE H(Y |Z) instead.\n\nTransferring representations. We keep the learned representation, w Z , and produce a new classifier k Y by training on the target task (Sec. 3.1). We used ResNet18 [25], trained with standard cross entropy loss, on each source task T Z (source attribute). These networks were selected as they were deep enough to obtain good accuracy on our benchmarks, but not too deep to overfit [72]. The penultimate layer of these networks produce embeddings r \u2208 R 2048 which the networks classified using h Z -their last, fully connected (FC) layers-to binary attribute values.\n\nWe transferred from source to target task by freezing the networks, only replacing their FC layers with linear SVM (lSVM). These lSVM were trained to predict the binary labels of target tasks given the embeddings produced for the source tasks by w Z as their input. The test errors of the lSVM, which are measures of then compared with the CE, H(Y |Z).\n1 \u2212 Trf(T Z \u2192 T Y ),\nWe use lSVM as it allows us to focus on the information passed from T Z to T Y . A more complex classifier could potentially mask this information by being powerful enough to offset any loss of information due to the transfer. In practical use cases, when transferring a deep network from one task to another, it may be preferable to fine tune the last layers of the network or its entirety, provided that the training data on the target task is large enough.\n\nTransferability results. Fig. 2 reports selected quantitative transferability results on the three sets. 2 Each point in these graphs represents the CE, H(Y |Z), vs. the target test error,\n1 \u2212 Trf(T Z \u2192 T Y ).\nThe graphs also provide the linear regression model fit with 95% confidence interval, the Pearson correlation coefficients between the two values, and the statistical significance of the correlation, p.\n\nIn all cases, the CE and target test error are highly positively correlated with statistical significance. These results testify that the CE of Eq. (7) is indeed a good predictor for the actual transferability of Eq. (4). This is remarkable es-   Table 1).\n\npecially since the relationship between tasks is evaluated without considering the input domain or the machine learning models trained to solve these tasks.\n\n\nCase study: Identity to facial attributes\n\nA key challenge when training effective attribute classifiers is the difficulty of obtaining labeled attribute training data. Whereas face images are often uploaded to the Internet along with subject names [9,21], it is far less common to find images labeled with attributes such as high cheek bones, bald, or even male [32]. It is consequently harder to assemble training sets for attribute classification at the same scale and diversity as those used to train other tasks.\n\nTo reduce the burden of collecting attribute data, we therefore explore transferring a representation learned for face recognition. In this setting, we can also compute estimated transferability scores (via the CE) between the subject labels provided by CelebA and the labels of each attribute. We note that unlike the previous examples, the source labels are not binary and include over 10k values.\n\nFace recognition network. We compare our estimated transferability vs. actual transferability using a deep face recognition network. To this end, we use a ResNet101 architecture trained for face recognition on the union of the  MS-Celeb-1M [21] and VGGFace2 [9] training sets (following removal of subjects included in CelebA), with a cosine margin loss (m = 0.4) [62]. This network achieves accuracy comparable to the state of the art reported by others, with different systems, on standard benchmarks [15].\n\nTransferability results: recognition to attributes. Table 1 reports results for the five attributes most transferable from recognition (smallest CE; Eq. (7)) and the five least transferable (largest CE). Columns are sorted by increasing CE values (decreasing transferability), listed in row 9. Row 11 reports accuracy of the transferred network with the lSVM trained on the target task. Estimated vs. actual transferability is further visualized in Fig. 3. Evidently, correlation between the two is statistically significant, testifying that Eq. (7) is a good predictor of actual transferability, here demonstrated on a source task with multiple labels.\n\nFor reference, Also, notice that for the transferable attributes, our results are comparable to dedicated networks trained for each attribute, although they gradually drop off for the less transferable attributes in the last columns. This effect is visualized in Fig. 4 which shows the growing differences in attribute classification accuracy for a transferred face recognition model and models trained for each attribute. Results are sorted by decreasing transferability (same as in Table 1). Fig. 4 show a few notable exceptions where transfer performs substantially better than dedicated models (e.g., the two positive peaks representing attributes young and big nose). These and other occasional discrepancies in our results can be explained in the difference between the true transferability of Eq. (4), which we measure on the test sets, and Eq. (5), defined on the training sets and shown in Sec. 3.2 to be bounded by the CE.\n\n\nResults in\n\nFinally, we note that our goal is not to develop a state of the art facial attribute classification scheme. Nevertheless, results obtained by training an lSVM on embeddings transferred from a face recognition network are only 2.4% lower than the best scores reported by DMTL 2018 [22] (last column of Table 1). The effort involved in developing a state of the art face recognition network can be substantial. By transferring this network to attributes these efforts are amortized in training multiple facial attribute classifiers.\n\nTo emphasize this last point, consider Fig. 5 which reports classification accuracy on male and double chin for growing training set sizes. These attributes were selected as they are highly transferable from recognition (see Table 1). The figure compares the accuracy obtained by training a dedicated network (in blue) to a network transferred from recognition (red). Evidently, on these attributes, transferred accuracy is much higher with far less training data.\n\n\nEvaluating task hardness\n\nWe evaluate our hardness estimates for all attribute classification tasks in the three data sets, using the CE H(Z|C) in Eq. (14). Fig. 6 compares the hardness estimates for each task vs. the errors of our dedicated networks, trained from scratch to classify each attribute. Results are provided for CelebA, AwA2, and CUB.\n\nThe correlation between estimated hardness and classification errors is statistically significant with p < 0.001, suggesting that the CE H(Z|C) in Eq. (14) indeed captures the hardness of these tasks. That is, in the three data sets, test error rates strongly correlate with our estimated hardness: the harder a task is estimated to be, the higher the errors produced by the model trained for the task. Of course, this result does not imply that the input domain has no impact on task hardness; only that the distribution of training labels already provides a strong predictor for task hardness.\n\n\nConclusions\n\nWe present a practical method for estimating the hardness and transferability of supervised classification tasks. We show that, in both cases, we produce reliable estimates by exploring training label statistics, particularly the conditional entropy between the sequences of labels assigned to the training data of each task. This approach is simpler than existing work, which obtains similar estimates by assuming the existence of trained models or by careful inspection of the training process. In our approach, computing conditional entropy is cheaper than training deep models, required by others for the same purpose.\n\nWe assume that different tasks share the same input domain (the same input images). It would be useful to extend our work to settings where the two tasks are defined over different domains (e.g., face vs. animal images). Our work further assumes discrete labels. Conditional entropy was originally defined over distributions. It is therefore reasonable that CE could be extended to non-discrete labeled tasks, such as, for faces, 3D reconstruction [60], pose estimation [10,11] or segmentation [47]. \n\n\nA. Proof of theorem 1\n\nFrom the definition of Trf(T Z \u2192 T Y ), we have:\nTrf(T Z \u2192 T Y ) = l Y (w Z , k Y ) (definition of Trf) \u2265 l Y (w Z ,k) (definition of k Y andk \u2208 K) = 1 n n i=1 log z\u2208ZP (y i |z)P (z|x i ; w Z , h Z ) (construction ofk) \u2265 1 n n i=1 log P (y i |z i )P (z i |x i ; w Z , h Z )\n(replacing the sum by one of its elements)\n= 1 n n i=1 logP (y i |z i ) + 1 n n i=1 log P (z i |x i ; w Z , h Z ).(15)\nNote that the second term in Eq. (15) is:\n1 n n i=1 log P (z i |x i ; w Z , h Z ) = l Z (w Z , h Z ).(16)\nFurthermore, the first term in Eq. (15) is: B. More details on task hardness On the definition of task hardness. In our paper, we assume non-overfitting of trained models. When train and test sets are sampled from the same distribution, this assumption typically holds for appropriately trained models [72]. This property also shows that our definition of hardness, Eq. (13), does not conflict with the results of Zhang et al. [72]: In such cases, the training loss of Eq. (13) correlates with the test error, and thus this definition indeed reflects task hardness, explaining the relationships between train and test errors observed in our hardness results.\n\nOn the representation for trivial tasks. Any representation for a trivial source task can fit the constant label perfectly (zero training loss). In theory, if we choose the optimal w Z in Eq. (1) as our representation, we can show Eq. (14). In practice, of course we cannot infer the optimal w Z from the trivial source task, but Eq. (14) shows that we can still connect it to H(Z|C).\n\n\nC. Technical implementation details\n\nComputing the CE. Computing the CE is straightforward and involves the following steps:\n\n1. Loop through the training labels of both tasks T Z and T Y and compute the empirical joint distribution P (y, z) by counting (Eq. (6) in the paper).\n\n2. Loop through the training labels again and compute the CE using Eq. (17) above. That is,\nH(Y |Z) = \u2212 1 n n i=1 logP (y i |z i ).\nThus, computing the CE only requires running two loops through the training labels. This process is computationally efficient. In the most extreme case, computing the transferability of face recognition (|Z| > 10k) to a facial attribute, with |Y| = 2, required less than a second on a standard CPU. This run time should be compared with the hours (or days) required to train deep models in order to empirically measure transferability following the process described by previous work. In particular, Taskonomy [71] reported over 47 thousand hours of GPU runtime in order to establish relationships between their 26 tasks.\n\nDedicated attribute training. Given a source task T Z , we train a dedicated CNN for this task with standard ResNet-18 V2 implemented in the MXNet deep learning library [12]. 3 We set the initial learning rate to 0.01. Learning rate was then divided by 10 after each 12 epochs. Training converged in less than 40 epochs in all 437 tasks.\n\nTask transfer with linear SVM. After training a deep representation for a source task T Z , we transfer it to a target task T Y using linear support vector machines (lSVM).\n\nFirst, we use the trained CNN, denoted in the paper as w Z , to extract deep embeddings for the entire training data (one embedding per input image. Each embedding is a vector r \u2208 R 2048 , which we obtain from the penultimate layer of the network. We then use these embeddings, along with the corresponding labels for target task, T Y , to train a standard lSVM classifier, implemented by SK-Learn [49]. The lSVM parameters were kept unchanged from their default values.\n\nGiven unseen testing data, we first extract their embeddings with w Z . We then apply the trained lSVM classifier on these features to predict labels for target task, T Y .\n\n\nD. Additional results: Generalization to multiclass\n\nTransferability generalizes well to multi-class, as evident in our face recognition (10k labels)-to-attribute tests in Sec. 5.2. Table 2 below reports hardness tests with multiclass, CelebA, attribute aggregates. Generally speaking, the harder the task, the lower the accuracy obtained. \n\n\nE. Full transferability results\n\n\u2022 Attribute prediction on CelebA [35]: see Fig. 7.\n\n\u2022 CelebA: Transferability from identity to attributes: see Table 3.\n\n\u2022 Attribute prediction on AwA2 [67]: see Fig. 8, 9, and 10.\n\n\nF. Full hardness results\n\n\u2022 CelebA [35] attribute prediction hardness: see Table 4.\n\n\u2022 AwA2 [67] attribute prediction hardness: see Table 5.\n\n\u2022 CUB [66] attribute prediction hardness: see Table 6.  Table 3. Transferability from face recognition to facial attributes. (Extended from Table 1 in the paper) Results for CelebA attributes, sorted in ascending order of row 9 (decreasing transferability). Classification accuracies are shown for all 40 attributes. Subject specific attributes, e.g., male and bald, are more transferable than expression related attributes such as smiling and mouth open. These identity specific attributes corresponds to the automatic grouping presented in the original CelebA paper [35]. Unlike them, however, we obtain this grouping without necessitating the training of a deep attribute classification model. Unsurprisingly, transfer results (row 11) are best on these subject specific attributes and worst for less related attributes. Rows Table 4. CelebA task hardness. CelebA facial attributes sorted in ascending order of hardness along with their respective hardness scores. Hardness scores listed above are compared with empirical test errors for each task and shown to be strongly correlated ( Fig. 6(a) in the paper). Note that the male classification task, appearing here as relatively hard, is the easiest task to transfer from face recognition ( Table 1).  Table 5. AWA2 task hardness. AWA2 attributes sorted in ascending order of hardness along with their respective hardness scores. Hardness scores listed above are compared with empirical test errors for each task and shown to be strongly correlated ( Fig. 6(b) in the paper).  Fig. 2(a-d) in the paper). The source attribute, T Z , in each plot is named in the plot title. Points represent different target tasks T Y . Corr is the Pearson correlation coefficient between the two variables and p is the statistical significance of the correlation. In all cases, the correlation is statistically significant.   Table 6. CUB task hardness. CUB attributes sorted in ascending order of hardness along with their respective hardness scores. Hardness scores listed above are compared with empirical test errors for each task and shown to be strongly correlated (Fig. 6(c) in the paper). Attribute names are abbreviated due to space concerns. Full names are provided in Table 7. bls has bill shape uptc has upper tail color untc has under tail color tp has tail pattern wc has wing color hp has head pattern nc has nape color bep has belly pattern upc has upperparts color brc has breast color bec has belly color pc has primary color unc has underparts color tc has throat color ws has wing shape lc has leg color brp has breast pattern ec has eye color si has size blc has bill color bkc has back color bll has bill length sh has shape cc has crown color ts has tail shape fc has forehead color bkp has back pattern wp has wing pattern Table 7. CUB attribute name abbreviations. Abbreviations used in Table 6 for the attributes in the CUB dataset [66].\n\nFigure 1 .\n1Visualizing toy examples. The transferability between two tasks, represented as sequences (X, Y ) and (X, Z). The horizontal axis represent instances and the values for Z (in red) and Y (cyan). In which of these examples would it be easiest to transfer a model trained for task T Z to task T Y ? See discussion and details in Sec. 3.3.\n\nFigure 3 .\n3Identity to attribute; CE vs. test errors on target tasks. Predicting 40 CelebA attributes using a face recognition network. Corr is the Pearson correlation coefficient between the two variables, and p is the statistical significance of the correlation.\n\nFigure 4 .\n4Identity to attribute; transferred \u2212 dedicated accuracy. Differences between CelebA accuracy of transferred recognition model and models trained for each attribute. Results are sorted by decreasing transferability (same as\n\nFigure 5 .\n5Classification accuracy for varying training set sizes. Top: male; bottom: double chin. Dedicated classification networks trained from scratch (blue) vs. face recognition network transferred to the attributes with an lSVM (red). Because recognition transfers well to these attributes, we obtain accurate classification with a fraction of the training data and effort.\n\nFrom\nthe summands by values of y i and z i ) |{i : y i = y and z i = z}| logP (y|z) (by counting) = y\u2208Y z\u2208Z |{i : y i = y and z i Eq. (15), (16), and (17), we haveTrf(T Z \u2192 T Y ) \u2265 l Z (w Z , h Z ) \u2212 H(Y |Z).Hence, the theorem holds.\n\nFigure 10 .\n10Attribute prediction; CE vs. test errors on AwA2 (Extended fromFig. 2(e-h) in the paper; part 3). The source attribute, T Z , in each plot is named in the plot title. Points represent different target tasks T Y . Corr is the Pearson correlation coefficient between the two variables and p is the statistical significance of the correlation. In all cases, the correlation is statistically significant.\n\n\nAttribute prediction; CE vs. test errors on target tasks. Examples from CelebA (a-d), AwA2 (e-h), and CUB (i-l). Plot titles name the source tasks T Z ; points represent different target tasks T Y . Corr is the Pearson correlation coefficient between the two variables and p is the statistical significance of the correlation. In all cases, the correlation is statistically significant. See Sec. 5.1 for details.Table 1. Transferability from face recognition to facial attributes. Results for CelebA attributes, sorted in ascending order of row 9 (decreasing transferability). Results are shown for the five attributes most and least transferable from recognition. Subject specific attributes, e.g., male and bald, are more transferable than expression related attributes such as smiling and mouth open. Unsurprisingly, transfer results (row 11) are best on the former than the latter. Rows 1-8 provide published state of the art results. Despite training only an lSVM for attribute, row 11 results are comparable with more elaborate attribute classification systems. For details, see Sec. 5.2.were \n0.2 \n\n0.4 \n0.6 \nConditional entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 18 \ncorr=0.93, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 20 \ncorr=0.93, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 26 \ncorr=0.93, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 36 \ncorr=0.94, p < 0.001 \n\n(a) Heavy makeup \n(b) Male \n(c) Pale skin \n(d) Wearing lipstick \n\n0.0 \n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 36 \ncorr=0.97, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 21 \ncorr=0.95, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 22 \ncorr=0.92, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 41 \ncorr=0.95, p < 0.001 \n\n(e) Swim \n(f) Pads \n(g) Paws \n(h) Strong \n\n0.0 \n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\n0.4 \n\nError on target \n\nSource att: 0 \ncorr=0.95, p < 0.001 \n\n0.0 \n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\n0.4 \n\nError on target \n\nSource att: 11 \ncorr=0.94, p < 0.001 \n\n0.0 \n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\n0.4 \n\nError on target \n\nSource att: 25 \ncorr=0.94, p < 0.001 \n\n0.0 \n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\n0.4 \n\nError on target \n\nSource att: 46 \ncorr=0.95, p < 0.001 \n\n(i) Curved Bill \n(j) Iridescent Wings \n(k) Brown Upper Parts \n(l) Olive Under Parts \n\nFigure 2. Attribute: \nMale Bald Gray Hair Mustache Double Chin . . . Attractive Wavy Hair High Cheeks Smiling Mouth Open Average (all) \n1 LNets+ANet 2015 [35] \n0.980 0.980 \n0.970 \n0.950 \n0.920 \n\n. . . \n\n0.810 \n0.800 \n0.870 \n0.920 \n0.920 \n0.873 \n2 Walk and Learn 2016 [63] \n0.960 0.920 \n0.950 \n0.900 \n0.930 \n0.840 \n0.850 \n0.950 \n0.980 \n0.970 \n0.887 \n3 MOON 2016 [55] \n0.981 0.988 \n0.981 \n0.968 \n0.963 \n0.817 \n0.825 \n0.870 \n0.926 \n0.935 \n0.909 \n4 LMLE 2016 [26] \n0.990 0.900 \n0.910 \n0.730 \n0.740 \n0.880 \n0.830 \n0.920 \n0.990 \n0.960 \n0.838 \n5 CR-I 2017 [16] \n0.960 0.970 \n0.950 \n0.940 \n0.890 \n0.830 \n0.790 \n0.890 \n0.930 \n0.950 \n0.866 \n6 MCNN-AUX 2017 [23] \n0.982 0.989 \n0.982 \n0.969 \n0.963 \n0.831 \n0.839 \n0.876 \n0.927 \n0.937 \n0.913 \n7 DMTL 2018 [22] \n0.980 0.990 \n0.960 \n0.970 \n0.990 \n0.850 \n0.870 \n0.880 \n0.940 \n0.940 \n0.926 \n8 Face-SSD 2019 [27] \n0.973 0.986 \n0.976 \n0.960 \n0.960 \n0.813 \n0.851 \n0.868 \n0.918 \n0.919 \n0.903 \n9 CE\u2191 (decreasing transferability) 0.017 0.026 \n0.052 \n0.062 \n0.083 \n0.361 \n0.381 \n0.476 \n0.521 \n0.551 \n-\n10 Dedicated Res18 \n0.985 0.990 \n0.980 \n0.968 \n0.959 \n0.823 \n0.842 \n0.878 \n0.933 \n0.943 \n0.911 \n11 Transfer \n0.992 0.991 \n0.981 \n0.968 \n0.963 \n0.820 \n0.800 \n0.859 \n0.909 \n0.901 \n0.902 \n\n\n\n\nTable 1provides in Row 10 the accuracy of the dedicated ResNet18 networks trained for each attribute. Finally, rows 1 through 8 provide results for published state of the art on the same tasks.Analysis of results. Subject specific attributes such as male and bald are evidently more transferable from recognition (left columns ofTable 1) than attributes that are related to Estimated task hardness vs. empirical errors on the three benchmarks. Estimated hardness is well correlated with empirical hardness with significance p < 0.001.expressions (e.g., smiling and mouth open, right columns). Although this relationship has been noted by others, previous work used domain knowledge to determine which attributes are more transferable from identity[35], as others have done in other domains[20,38]. By comparison, our work shows how these relationships emerge from our estimation of transferability.0.2 \n\n0.4 \n0.6 \nHardness estimate \n\n0.0 \n\n0.1 \n\n0.2 \n\n0.3 \n\nTest error \n\ncorr=0.58, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nHardness estimate \n\n0.0 \n\n0.1 \n\n0.2 \n\nTest error \n\ncorr=0.82, p < 0.001 \n\n0.0 \n0.2 \n0.4 \n0.6 \nHardness estimate \n\n0.0 \n\n0.2 \n\n0.4 \n\nTest error \n\ncorr=0.96, p < 0.001 \n\n(a) CelebA \n(b) AwA2 \n(c) CUB \n\nFigure 6. \n\n[ 16 ]\n16Qi Dong, Shaogang Gong, and Xiatian Zhu. Class rectification hard mining for imbalanced deep learning. In Proc. Conf. Comput. Vision Pattern Recognition, pages 1851-1860, 2017. [17] Richard O Duda, Peter E Hart, and David G Stork. Pattern classification. John Wiley & Sons, 2012. [18] Emilien Dupont. Learning disentangled joint continuous and discrete representations. In Neural Inform. Process. Emily M Hand and Rama Chellappa. Attributes for improved attributes: A multi-task network utilizing implicit and explicit relationships for facial attribute classification. In AAAI Conf. on Artificial Intelligence, 2017. [24] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proc. Int. Conf. Comput. Jing Wang, Yu Cheng, and Rogerio Schmidt Feris. Walk and learn: Facial attribute representation learning from egocentric video and contextual data. In Proc. Conf. Comput. Vision Pattern Recognition, pages 2295-2304, 2016. [64] Peng Wang, Xiaohui Shen, Zhe Lin, Scott Cohen, Brian Price, and Alan L Yuille. Towards unified depth and semantic prediction from a single image. In Proc. Conf. Comput. Yongxin Yang and Timothy Hospedales. Deep multi-task representation learning: A tensor factorisation approach. In Int. Conf. on Learning Representations, 2017. [69] Wei Ying, Yu Zhang, Junzhou Huang, and Qiang Yang. Transfer learning via learning to transfer. In Int. Conf. Mach. Learning, pages 5072-5081, 2018. [70] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Neural Inform. Process. Syst., pages 3320-3328, 2014. [71] Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In Proc. Conf. Comput. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In Int. Conf. on Learning Representations, 2017. [73] Xiangyun Zhao, Haoxiang Li, Xiaohui Shen, Xiaodan Liang, and Ying Wu. A modulation module for multi-task learning with applications in image retrieval. In European Conf. Comput. Vision, pages 401-416, 2018.Syst., \npages 708-718, 2018. \n[19] Harrison Edwards and Amos Storkey. Towards a neural \nstatistician. arXiv preprint arXiv:1606.02185, 2016. \n[20] Deepti Ghadiyaram, Du Tran, and Dhruv Mahajan. Large-\nscale weakly-supervised pre-training for video action recog-\nnition. In Proc. Conf. Comput. Vision Pattern Recognition, \npages 12046-12055, 2019. \n[21] Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and \nJianfeng Gao. MS-Celeb-1M: A dataset and benchmark for \nlarge scale face recognition. In European Conf. Comput. Vi-\nsion. Springer, 2016. \n[22] Hu Han, Anil K Jain, Fang Wang, Shiguang Shan, and Xilin \nChen. Heterogeneous face attribute estimation: A deep \nmulti-task learning approach. Trans. Pattern Anal. Mach. \nIntell., 40(11):2597-2609, 2018. \n[23] Vision, pages \n2961-2969, 2017. \n[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \nDeep residual learning for image recognition. In Proc. Conf. \nComput. Vision Pattern Recognition, June 2016. \n[26] Chen Huang, Yining Li, Chen Change Loy, and Xiaoou \nTang. Learning deep representation for imbalanced classi-\nfication. In Proc. Conf. Comput. Vision Pattern Recognition, \npages 5375-5384, 2016. \n[27] Youngkyoon Jang, Hatice Gunes, and Ioannis Patras. \nRegistration-free face-ssd: Single shot analysis of smiles, fa-\ncial attributes, and affect in the wild. Comput. Vision Image \nUnderstanding, 2019. \n[28] Brendan Jou and Shih-Fu Chang. Deep cross residual learn-\ning for multitask visual recognition. In Int. Conf. Multime-\ndia, pages 998-1007. ACM, 2016. \n[29] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task \nlearning using uncertainty to weigh losses for scene geome-\ntry and semantics. In Proceedings of the IEEE Conference \non Computer Vision and Pattern Recognition, pages 7482-\n7491, 2018. \n[30] Iasonas Kokkinos. Ubernet: Training a universal convo-\nlutional neural network for low-, mid-, and high-level vi-\nsion using diverse datasets and limited memory. In Proc. \nConf. Comput. Vision Pattern Recognition, pages 6129-\n6138, 2017. \n[31] Giwoong Lee, Eunho Yang, and Sung Hwang. Asymmetric \nmulti-task learning based on task relatedness and loss. In Int. \nConf. Mach. Learning, pages 230-238, 2016. \nIn Proc. Conf. Comput. Vision Pattern Recognition, pages \n5265-5274, 2018. \n[63] Vision Pattern Recognition, pages 2800-2809, 2015. \n[65] Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. \nA survey of transfer learning. Journal of Big Data, 3(1):9, \n2016. \n[66] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Be-\nlongie, and P. Perona. Caltech-UCSD Birds 200. Technical \nReport CNS-TR-2010-001, California Institute of Technol-\nogy, 2010. \n[67] Yongqin Xian, Christoph H Lampert, Bernt Schiele, and \nZeynep Akata. Zero-shot learning-a comprehensive evalu-\nation of the good, the bad and the ugly. Trans. Pattern Anal. \nMach. Intell., 2018. \n[68] Vision Pattern Recognition, pages 3712-3722, 2018. \n[72] \n\n\n1-8 provide published state of the art results. Despite training only an lSVM for attribute, row 11 results are comparable with more elaborate attribute classification systems. For details, see Sec. 5.2.1 Attribute \nBald \nMustache \nGray Hair \nPale Skin \nDouble Chin \nWearing Hat \nBlurry \nSideburns \nChubby \nGoatee \n\u2192 \n2 Conditional Entropy\u2191 \n0.107 \n0.173 \n0.174 \n0.177 \n0.189 \n0.194 \n0.201 \n0.217 \n0.22 \n0.235 \n\u2192 \n\u2192 \nEyeglasses \nRosy Cheeks \nWearing Necktie Receding Hairline 5 oClock Shadow \nNarrow Eyes \nWearing Necklace Bushy Eyebrows \nBlond Hair \nBangs \n\u2192 \n\u2192 \n0.241 \n0.242 \n0.261 \n0.278 \n0.349 \n0.357 \n0.373 \n0.409 \n0.419 \n0.425 \n\u2192 \n\u2192 \nNo Beard \nWearing Earrings Bags Under Eyes \nBrown Hair \nStraight Hair \nYoung \nBig Nose \nBlack Hair \nBig Lips \nArched Eyebrows \u2192 \n\u2192 \n0.448 \n0.485 \n0.507 \n0.508 \n0.512 \n0.535 \n0.545 \n0.55 \n0.552 \n0.58 \n\u2192 \n\u2192 \nPointy Nose \nOval Face \nWavy Hair \nHeavy Makeup \nMale \nHigh Cheekbones Wearing Lipstick \nSmiling \nMouth Slightly Open \nAttractive \n\u2192 \n0.591 \n0.597 \n0.627 \n0.667 \n0.679 \n0.689 \n0.692 \n0.693 \n0.693 \n0.693 \n\n\n\n\n, in each plot is named in the plot title. Points represent different target tasks T Y . Corr is the Pearson correlation coefficient between the two variables and p is the statistical significance of the correlation. In all cases, the correlation is statistically significant.Error on target Source att: 50 corr=0.92, p < 0.001 Error on target Source att: 51 corr=0.97, p < 0.001 Error on target Source att: 52 corr=0.92, p < 0.001 Error on target Source att: 53 corr=0.96, p < 0.001 Error on target Source att: 53 corr=0.96, p < 0.001 Attribute prediction; CE vs. test errors on AwA2 (Extended fromFig. 2(e-h) in the paper; part 2). The source attribute, T Z , in each plot is named in the plot title. Points represent different target tasks T Y . Corr is the Pearson correlation coefficient between the two variables and p is the statistical significance of the correlation. In all cases, the correlation is statistically significant.Error on target Source att: 75 corr=0.97, p < 0.001 Error on target Source att: 76 corr=0.95, p < 0.001 Error on target Source att: 77 corr=0.95, p < 0.001 Error on target Source att: 78 corr=0.93, p < 0.001 Error on target Source att: 78 corr=0.93, p < 0.0010.2 \n\n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 0 \ncorr=0.96, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 1 \ncorr=0.94, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\n0.4 \n\nError on target \n\nSource att: 2 \ncorr=0.97, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 3 \ncorr=0.96, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 3 \ncorr=0.96, p < 0.001 \n\n(0) Black \n(1) White \n(2) Blue \n(3) Brown \n(4) Gray \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 5 \ncorr=0.97, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\n0.4 \n\nError on target \n\nSource att: 6 \ncorr=0.97, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 7 \ncorr=0.96, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 8 \ncorr=0.96, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 8 \ncorr=0.96, p < 0.001 \n\n(5) Orange \n(6) Red \n(7) Yellow \n(8) Patches \n(9) Spots \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 10 \ncorr=0.96, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 11 \ncorr=0.96, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 12 \ncorr=0.97, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 13 \ncorr=0.95, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 13 \ncorr=0.95, p < 0.001 \n\n(10) Stripes \n(11) Furry \n(12) Hairless \n(13) Toughskin \n(14) Big \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 15 \ncorr=0.95, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 16 \ncorr=0.94, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 17 \ncorr=0.94, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 18 \ncorr=0.97, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 18 \ncorr=0.97, p < 0.001 \n\n(15) Small \n(16) Bulbous \n(17) Lean \n(18) Flippers \n(19) Hands \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 20 \ncorr=0.94, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 21 \ncorr=0.95, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 22 \ncorr=0.92, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 23 \ncorr=0.95, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 23 \ncorr=0.95, p < 0.001 \n\n(20) Hooves \n(21) Pads \n(22) Paws \n(23) Longleg \n(24) Longneck \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 25 \ncorr=0.95, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 26 \ncorr=0.96, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 27 \ncorr=0.91, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 28 \ncorr=0.96, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 28 \ncorr=0.96, p < 0.001 \n\n(25) Tail \n(26) Chewteeth \n(27) Meatteeth \n(28) Buckteeth \n(29) Strainteeth \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 30 \ncorr=0.95, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 31 \ncorr=0.93, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\n0.4 \n\nError on target \n\nSource att: 32 \ncorr=0.97, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 33 \ncorr=0.95, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 33 \ncorr=0.95, p < 0.001 \n\n(30) Horns \n(31) Claws \n(32) Tusks \n(33) Smelly \n(34) Flys \n\nFigure 8. Attribute prediction; CE vs. test errors on AwA2 (Extended from Fig. 2(e-h) in the paper; part 1). The source attribute, \nT Z 0.2 \n\n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 35 \ncorr=0.97, p < 0.001 \n\n0.0 \n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 36 \ncorr=0.97, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\n0.4 \n\nError on target \n\nSource att: 37 \ncorr=0.97, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 38 \ncorr=0.97, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 38 \ncorr=0.97, p < 0.001 \n\n(35) Hops \n(36) Swims \n(37) Tunnels \n(38) Walks \n(39) Fast \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 40 \ncorr=0.92, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 41 \ncorr=0.95, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 42 \ncorr=0.95, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 43 \ncorr=0.95, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 43 \ncorr=0.95, p < 0.001 \n\n(40) Slow \n(41) Strong \n(42) Weak \n(43) Muscle \n(44) Bipedal \n\n0.0 \n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 45 \ncorr=0.98, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 46 \ncorr=0.92, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 47 \ncorr=0.94, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 48 \ncorr=0.95, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 48 \ncorr=0.95, p < 0.001 \n\n(45) Quadrapedal \n(46) Active \n(47) Inactive \n(48) Nocturnal \n(49) Hibernate \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\n0.0 \n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\n0.4 \n\n0.0 \n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\n0.4 \n\n(50) Agility \n(51) Fish \n(52) Meat \n(53) Plankton \n(54) Vegetation \n\n0.0 \n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 55 \ncorr=0.96, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 56 \ncorr=0.95, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 57 \ncorr=0.93, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 58 \ncorr=0.93, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 58 \ncorr=0.93, p < 0.001 \n\n(55) Insects \n(56) Forager \n(57) Grazer \n(58) Hunter \n(59) Scavenger \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\n0.4 \n\nError on target \n\nSource att: 60 \ncorr=0.96, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 61 \ncorr=0.93, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 62 \ncorr=0.95, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 63 \ncorr=0.96, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 63 \ncorr=0.96, p < 0.001 \n\n(60) Skimmer \n(61) Stalker \n(62) Newworld \n(63) Oldworld \n(64) Arctic \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 65 \ncorr=0.97, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\n0.4 \n\nError on target \n\nSource att: 66 \ncorr=0.97, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 67 \ncorr=0.96, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 68 \ncorr=0.97, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 68 \ncorr=0.97, p < 0.001 \n\n(65) Coastal \n(66) Desert \n(67) Bush \n(68) Plains \n(69) Forest \n\nFigure 9. 0.2 \n\n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 70 \ncorr=0.96, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 71 \ncorr=0.96, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 72 \ncorr=0.96, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 73 \ncorr=0.98, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 73 \ncorr=0.98, p < 0.001 \n\n(70) Fields \n(71) Jungle \n(72) Mountains \n(73) Ocean \n(74) Ground \n\n0.0 \n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\n0.0 \n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\n(75) Water \n(76) Tree \n(77) Cave \n(78) Fierce \n(79) Timid \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 80 \ncorr=0.94, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 81 \ncorr=0.93, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 82 \ncorr=0.94, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 83 \ncorr=0.95, p < 0.001 \n\n0.2 \n0.4 \n0.6 \nConditional Entropy \n\n0.0 \n\n0.2 \n\nError on target \n\nSource att: 83 \ncorr=0.95, p < 0.001 \n\n(80) Smart \n(81) Group \n(82) Solitary \n(83) Nestspot \n(84) Domestic \n\n\nFor full results see the appendix.\nModel available from: https://mxnet.apache.org/api/ python/gluon/model_zoo.html.\nAcknowledgements. We thank Alessandro Achille, Pietro Perona, and the reviewers for their helpful discussions.\nAlessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless Fowlkes, Stefano Soatto, Pietro Perona, arXiv:1902.03545Task embedding for meta-learning. 2arXiv preprintAlessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless Fowlkes, Stefano Soatto, and Pietro Perona. Task2Vec: Task embedding for meta-learning. arXiv preprint arXiv:1902.03545, 2019.\n\nThe information complexity of learning tasks, their structure and their distance. Alessandro Achille, Giovanni Paolini, Glen Mbeng, Stefano Soatto, arXiv:1904.03292arXiv preprintAlessandro Achille, Giovanni Paolini, Glen Mbeng, and Stefano Soatto. The information complexity of learning tasks, their structure and their distance. arXiv preprint arXiv:1904.03292, 2019.\n\nProvable bounds for learning some deep representations. Sanjeev Arora, Aditya Bhaskara, Rong Ge, Tengyu Ma, Int. Conf. Mach. Learning. Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep representations. In Int. Conf. Mach. Learning, pages 584-592, 2014.\n\nFactors of transferability for a generic convnet representation. Hossein Azizpour, Ali Sharif Razavian, Josephine Sullivan, Atsuto Maki, Stefan Carlsson, Trans. Pattern Anal. Mach. Intell. 389Hossein Azizpour, Ali Sharif Razavian, Josephine Sullivan, Atsuto Maki, and Stefan Carlsson. Factors of transferability for a generic convnet representation. Trans. Pattern Anal. Mach. Intell., 38(9):1790-1802, 2015.\n\nRegularized learning for domain adaptation under label shifts. Kamyar Azizzadenesheli, Anqi Liu, Fanny Yang, Animashree Anandkumar, Int. Conf. on Learning Representations. Kamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and An- imashree Anandkumar. Regularized learning for domain adaptation under label shifts. In Int. Conf. on Learning Rep- resentations, 2019.\n\nA theory of learning from different domains. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, Jennifer Wortman Vaughan, Mach. Learn. 791-2Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Mach. Learn., 79(1-2):151-175, 2010.\n\nExploiting task relatedness for multiple task learning. Shai Ben, - David, Reba Schuller, Learning Theory and Kernel Machines. SpringerShai Ben-David and Reba Schuller. Exploiting task relat- edness for multiple task learning. In Learning Theory and Kernel Machines, pages 567-580. Springer, 2003.\n\nLearning bounds for domain adaptation. John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, Jennifer Wortman, Neural Inform. Process. Syst. John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman. Learning bounds for domain adaptation. In Neural Inform. Process. Syst., pages 129-136, 2008.\n\nVGGFace2: A dataset for recognising faces across pose and age. Q Cao, L Shen, W Xie, O M Parkhi, A Zisserman, Automatic Face and Gesture Recognition. Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman. VGGFace2: A dataset for recognising faces across pose and age. In Automatic Face and Gesture Recognition, 2018.\n\nFaceposenet: Making a case for landmark-free face alignment. Feng-Ju Chang, Anh Tran, Tal Hassner, Iacopo Masi, Ram Nevatia, G\u00e9rard Medioni, Proc. Int. Conf. Comput. Vision Workshops. Int. Conf. Comput. Vision WorkshopsFeng-Ju Chang, Anh Tran, Tal Hassner, Iacopo Masi, Ram Nevatia, and G\u00e9rard Medioni. Faceposenet: Making a case for landmark-free face alignment. In Proc. Int. Conf. Com- put. Vision Workshops, 2017.\n\nDeep, landmark-free fame: Face alignment, modeling, and expression estimation. Feng-Ju Chang, Anh Tuan Tran, Tal Hassner, Iacopo Masi, Ram Nevatia, G\u00e9rard Medioni, Int. J. Comput. Vision. 1276-7Feng-Ju Chang, Anh Tuan Tran, Tal Hassner, Iacopo Masi, Ram Nevatia, and G\u00e9rard Medioni. Deep, landmark-free fame: Face alignment, modeling, and expression estimation. Int. J. Comput. Vision, 127(6-7):930-956, 2019.\n\nMxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, Zheng Zhang, arXiv:1512.01274arXiv preprintTianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. arXiv preprint arXiv:1512.01274, 2015.\n\nThe loss surfaces of multilayer networks. Anna Choromanska, Mikael Henaff, Michael Mathieu, G\u00e9rard Ben Arous, Yann Lecun, Artificial Intelligence and Statistics. Anna Choromanska, Mikael Henaff, Michael Mathieu, G\u00e9rard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pages 192-204, 2015.\n\nElements of information theory. M Thomas, Cover, A Joy, Thomas, John Wiley & SonsThomas M Cover and Joy A Thomas. Elements of informa- tion theory. John Wiley & Sons, 2012.\n\nOn measuring the iconicity of a face. Prithviraj Dhar, Carlos Castillo, Rama Chellappa, Winter Conf. on App. of Comput. Vision. IEEEPrithviraj Dhar, Carlos Castillo, and Rama Chellappa. On measuring the iconicity of a face. In Winter Conf. on App. of Comput. Vision, pages 2137-2145. IEEE, 2019.\n\nAge and gender classification using convolutional neural networks. Gil Levi, Tal Hassner, Proc. Conf. Comput. Vision Pattern Recognition Workshops. Conf. Comput. Vision Pattern Recognition WorkshopsGil Levi and Tal Hassner. Age and gender classification us- ing convolutional neural networks. In Proc. Conf. Comput. Vision Pattern Recognition Workshops, June 2015.\n\nExploring disentangled feature representation beyond face identification. Yu Liu, Fangyin Wei, Jing Shao, Lu Sheng, Junjie Yan, Xiaogang Wang, Proc. Conf. Comput. Vision Pattern Recognition. Conf. Comput. Vision Pattern RecognitionYu Liu, Fangyin Wei, Jing Shao, Lu Sheng, Junjie Yan, and Xiaogang Wang. Exploring disentangled feature represen- tation beyond face identification. In Proc. Conf. Comput. Vision Pattern Recognition, pages 2080-2089, 2018.\n\nDetach and adapt: Learning cross-domain disentangled deep representation. Yen-Cheng Liu, Yu-Ying Yeh, Tzu-Chien Fu, Sheng-De Wang, Wei-Chen Chiu, Yu-Chiang Frank Wang, Proc. Conf. Comput. Vision Pattern Recognition. Conf. Comput. Vision Pattern RecognitionYen-Cheng Liu, Yu-Ying Yeh, Tzu-Chien Fu, Sheng-De Wang, Wei-Chen Chiu, and Yu-Chiang Frank Wang. Detach and adapt: Learning cross-domain disentangled deep repre- sentation. In Proc. Conf. Comput. Vision Pattern Recogni- tion, pages 8867-8876, 2018.\n\nDeep learning face attributes in the wild. Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang, Proc. Int. Conf. Comput. Vision. Int. Conf. Comput. VisionZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proc. Int. Conf. Comput. Vision, 2015.\n\nObject recognition from local scale-invariant features. G David, Lowe, Proc. Int. Conf. Comput. Vision. Int. Conf. Comput. Vision1150David G Lowe. Object recognition from local scale-invariant features. In Proc. Int. Conf. Comput. Vision, page 1150, 1999.\n\nFully-adaptive feature sharing in multi-task networks with applications in person attribute classification. Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi, Rogerio Feris, Proc. Conf. Comput. Vision Pattern Recognition. Conf. Comput. Vision Pattern RecognitionYongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi, and Rogerio Feris. Fully-adaptive feature shar- ing in multi-task networks with applications in person at- tribute classification. In Proc. Conf. Comput. Vision Pattern Recognition, pages 5334-5343, 2017.\n\nAshwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, European Conf. Comput. Vision. Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In European Conf. Comput. Vision, pages 181-196, 2018.\n\nDomain adaptation: Learning bounds and algorithms. Yishay Mansour, Mehryar Mohri, Afshin Rostamizadeh, Conference on Learning Theory. Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In Conference on Learning Theory, 2009.\n\nLearning pose-aware models for pose-invariant face recognition in the wild. I Masi, F J Chang, J Choi, S Harel, J Kim, K Kim, J Leksut, S Rawls, Y Wu, T Hassner, W Abdalmageed, G Medioni, L P Morency, P Natarajan, R Nevatia, Trans. Pattern Anal. Mach. Intell. I. Masi, F. J. Chang, J. Choi, S. Harel, J. Kim, K. Kim, J. Leksut, S. Rawls, Y. Wu, T. Hassner, W. AbdAlmageed, G. Medioni, L. P. Morency, P. Natarajan, and R. Nevatia. Learn- ing pose-aware models for pose-invariant face recognition in the wild. Trans. Pattern Anal. Mach. Intell., 2018.\n\nFace-specific data augmentation for unconstrained face recognition. Iacopo Masi, Anh Tuan Tran, Tal Hassner, Gozde Sahin, G\u00e9rard Medioni, Int. J. Comput. Vision. 1276-7Iacopo Masi, Anh Tuan Tran, Tal Hassner, Gozde Sahin, and G\u00e9rard Medioni. Face-specific data augmentation for un- constrained face recognition. Int. J. Comput. Vision, 127(6- 7):642-667, 2019.\n\nOn the method of bounded differences. Colin Mcdiarmid, 141Surveys in combinatoricsColin McDiarmid. On the method of bounded differences. Surveys in combinatorics, 141(1):148-188, 1989.\n\nCross-stitch networks for multi-task learning. Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, Martial Hebert, Proc. Conf. Comput. Vision Pattern Recognition. Conf. Comput. Vision Pattern RecognitionIshan Misra, Abhinav Shrivastava, Abhinav Gupta, and Mar- tial Hebert. Cross-stitch networks for multi-task learning. In Proc. Conf. Comput. Vision Pattern Recognition, pages 3994-4003, 2016.\n\nToward understanding catastrophic forgetting in continual learning. Alessandro Cuong V Nguyen, Michael Achille, Tal Lam, Vijay Hassner, Stefano Mahadevan, Soatto, arXiv:1908.01091Cuong V Nguyen, Alessandro Achille, Michael Lam, Tal Hassner, Vijay Mahadevan, and Stefano Soatto. Toward understanding catastrophic forgetting in continual learning. arXiv:1908.01091, 2019.\n\nVariational continual learning. Yingzhen Cuong V Nguyen, Li, D Thang, Richard E Bui, Turner, Int. Conf. on Learning Representations. Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual learning. In Int. Conf. on Learning Representations, 2018.\n\nThe loss surface of deep and wide neural networks. Quynh Nguyen, Matthias Hein, Int. Conf. Mach. Learning. Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Int. Conf. Mach. Learning, pages 2603-2612, 2017.\n\nOn face segmentation, face swapping, and face perception. Yuval Nirkin, Iacopo Masi, Anh Tran Tuan, Tal Hassner, Gerard Medioni, Int. Conf. on Automatic Face and Gesture Recognition. IEEEYuval Nirkin, Iacopo Masi, Anh Tran Tuan, Tal Hassner, and Gerard Medioni. On face segmentation, face swapping, and face perception. In Int. Conf. on Automatic Face and Gesture Recognition, pages 98-105. IEEE, 2018.\n\nA survey on transfer learning. Qiang Sinno Jialin Pan, Yang, Trans. Knowledge and Data Eng. 2210Sinno Jialin Pan and Qiang Yang. A survey on transfer learn- ing. Trans. Knowledge and Data Eng., 22(10):1345-1359, 2010.\n\nScikit-learn: Machine learning in Python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, J. Mach. Learning Research. 12F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. J. Mach. Learning Research, 12:2825- 2830, 2011.\n\nHyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition. Rajeev Ranjan, M Vishal, Rama Patel, Chellappa, Trans. Pattern Anal. Mach. Intell. 411Rajeev Ranjan, Vishal M Patel, and Rama Chellappa. Hy- perface: A deep multi-task learning framework for face de- tection, landmark localization, pose estimation, and gender recognition. Trans. Pattern Anal. Mach. Intell., 41(1):121- 135, 2019.\n\nAn all-in-one convolutional neural network for face analysis. Rajeev Ranjan, Swami Sankaranarayanan, D Carlos, Rama Castillo, Chellappa, Int. Conf. on Automatic Face and Gesture Recognition. IEEERajeev Ranjan, Swami Sankaranarayanan, Carlos D Castillo, and Rama Chellappa. An all-in-one convolutional neural net- work for face analysis. In Int. Conf. on Automatic Face and Gesture Recognition, pages 17-24. IEEE, 2017.\n\nCHILD: A first step towards continual learning. B Mark, Ring, Mach. Learn. 281Mark B Ring. CHILD: A first step towards continual learn- ing. Mach. Learn., 28(1):77-104, 1997.\n\nDex: Deep expectation of apparent age from a single image. Radu Rasmus Rothe, Luc Timofte, Van Gool, Proc. Int. Conf. Comput. Vision Workshops. Int. Conf. Comput. Vision WorkshopsRasmus Rothe, Radu Timofte, and Luc Van Gool. Dex: Deep expectation of apparent age from a single image. In Proc. Int. Conf. Comput. Vision Workshops, pages 10-15, 2015.\n\nBeyond sharing weights for deep domain adaptation. Artem Rozantsev, Mathieu Salzmann, Pascal Fua, Trans. Pattern Anal. Mach. Intell. 414Artem Rozantsev, Mathieu Salzmann, and Pascal Fua. Be- yond sharing weights for deep domain adaptation. Trans. Pattern Anal. Mach. Intell., 41(4):801-814, 2019.\n\nMoon: A mixed objective optimization network for the recognition of facial attributes. M Ethan, Manuel Rudd, Terrance E G\u00fcnther, Boult, European Conf. Comput. Vision. SpringerEthan M Rudd, Manuel G\u00fcnther, and Terrance E Boult. Moon: A mixed objective optimization network for the recognition of facial attributes. In European Conf. Comput. Vision, pages 19-35. Springer, 2016.\n\nOriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization. Dushyant Andrei A Rusu, Jakub Rao, Sygnowski, Int. Conf. on Learning Representations. Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Had- sell. Meta-learning with latent embedding optimization. In Int. Conf. on Learning Representations, 2019.\n\nLearning with kernels: support vector machines, regularization, optimization, and beyond. Bernhard Scholkopf, Alexander J Smola, MIT pressBernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines, regularization, optimiza- tion, and beyond. MIT press, 2001.\n\nRefining architectures of deep convolutional neural networks. Sukrit Shankar, Duncan Robertson, Yani Ioannou, Antonio Criminisi, Roberto Cipolla, Proc. Conf. Comput. Vision Pattern Recognition. Conf. Comput. Vision Pattern RecognitionSukrit Shankar, Duncan Robertson, Yani Ioannou, Antonio Criminisi, and Roberto Cipolla. Refining architectures of deep convolutional neural networks. In Proc. Conf. Comput. Vision Pattern Recognition, pages 2212-2220, 2016.\n\nUnbiased look at dataset bias. A Torralba, Efros, Proc. Conf. Comput. Vision Pattern Recognition. Conf. Comput. Vision Pattern RecognitionIEEE Computer SocietyA Torralba and AA Efros. Unbiased look at dataset bias. In Proc. Conf. Comput. Vision Pattern Recognition, pages 1521-1528. IEEE Computer Society, 2011.\n\nExtreme 3D face reconstruction: Looking past occlusions. Anh Tuan Tran, Tal Hassner, Iacopo Masi, Eran Paz, Yuval Nirkin, G\u00e9rard Medioni, Proc. Conf. Comput. Vision Pattern Recognition. Conf. Comput. Vision Pattern RecognitionAnh Tuan Tran, Tal Hassner, Iacopo Masi, Eran Paz, Yuval Nirkin, and G\u00e9rard Medioni. Extreme 3D face reconstruc- tion: Looking past occlusions. In Proc. Conf. Comput. Vision Pattern Recognition, 2018.\n\nConditional similarity networks. Andreas Veit, Serge Belongie, Theofanis Karaletsos, Proc. Conf. Comput. Vision Pattern Recognition. Conf. Comput. Vision Pattern RecognitionAndreas Veit, Serge Belongie, and Theofanis Karaletsos. Conditional similarity networks. In Proc. Conf. Comput. Vi- sion Pattern Recognition, pages 830-838, 2017.\n\nCosface: Large margin cosine loss for deep face recognition. Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, Wei Liu, Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cos- face: Large margin cosine loss for deep face recognition.\n", "annotations": {"author": "[{\"end\":107,\"start\":67},{\"end\":170,\"start\":108}]", "publisher": null, "author_last_name": "[{\"end\":77,\"start\":73},{\"end\":119,\"start\":112}]", "author_first_name": "[{\"end\":70,\"start\":67},{\"end\":72,\"start\":71},{\"end\":111,\"start\":108}]", "author_affiliation": "[{\"end\":106,\"start\":79},{\"end\":169,\"start\":142}]", "title": "[{\"end\":64,\"start\":1},{\"end\":234,\"start\":171}]", "venue": null, "abstract": "[{\"end\":1498,\"start\":271}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1863,\"start\":1860},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2105,\"start\":2102},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2107,\"start\":2105},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2199,\"start\":2196},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2202,\"start\":2199},{\"end\":2205,\"start\":2202},{\"end\":2208,\"start\":2205},{\"end\":2712,\"start\":2708},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2890,\"start\":2887},{\"end\":2893,\"start\":2890},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3067,\"start\":3064},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3070,\"start\":3067},{\"end\":3073,\"start\":3070},{\"end\":3076,\"start\":3073},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4655,\"start\":4651},{\"end\":4694,\"start\":4690},{\"end\":4733,\"start\":4729},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5277,\"start\":5273},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5299,\"start\":5295},{\"end\":5329,\"start\":5325},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5487,\"start\":5484},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5490,\"start\":5487},{\"end\":5493,\"start\":5490},{\"end\":5496,\"start\":5493},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5565,\"start\":5561},{\"end\":5568,\"start\":5565},{\"end\":5571,\"start\":5568},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5740,\"start\":5737},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5742,\"start\":5740},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5744,\"start\":5742},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5746,\"start\":5744},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5748,\"start\":5746},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5751,\"start\":5748},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6016,\"start\":6012},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6019,\"start\":6016},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6022,\"start\":6019},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6287,\"start\":6283},{\"end\":6493,\"start\":6489},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6902,\"start\":6899},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6904,\"start\":6902},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7160,\"start\":7157},{\"end\":7163,\"start\":7160},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7528,\"start\":7525},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7531,\"start\":7528},{\"end\":7721,\"start\":7717},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7798,\"start\":7794},{\"end\":7858,\"start\":7854},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8030,\"start\":8026},{\"end\":8079,\"start\":8075},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8185,\"start\":8181},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8534,\"start\":8531},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8536,\"start\":8534},{\"end\":8539,\"start\":8536},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9101,\"start\":9097},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9104,\"start\":9101},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9150,\"start\":9146},{\"end\":12453,\"start\":12449},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12920,\"start\":12916},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14498,\"start\":14495},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14872,\"start\":14871},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15824,\"start\":15820},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15933,\"start\":15929},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16582,\"start\":16581},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16969,\"start\":16965},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16972,\"start\":16969},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17253,\"start\":17249},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20672,\"start\":20669},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20884,\"start\":20881},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21587,\"start\":21583},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21647,\"start\":21644},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21650,\"start\":21647},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21653,\"start\":21650},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23316,\"start\":23312},{\"end\":24031,\"start\":24027},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25154,\"start\":25153},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26131,\"start\":26128},{\"end\":26134,\"start\":26131},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":26246,\"start\":26242},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27060,\"start\":27057},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":27167,\"start\":27163},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":27306,\"start\":27302},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":30065,\"start\":30061},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":30415,\"start\":30411},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":31947,\"start\":31943},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31969,\"start\":31965},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":31972,\"start\":31969},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":31993,\"start\":31989},{\"end\":32826,\"start\":32822},{\"end\":32951,\"start\":32947},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33419,\"start\":33415},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33518,\"start\":33514},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":34774,\"start\":34770},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":34777,\"start\":34776},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":35516,\"start\":35512},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":36174,\"start\":36170},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":36359,\"start\":36355},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":37034,\"start\":37030},{\"end\":37290,\"start\":37286},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":45832,\"start\":45828},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":45877,\"start\":45874}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39711,\"start\":39363},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39978,\"start\":39712},{\"attributes\":{\"id\":\"fig_2\"},\"end\":40214,\"start\":39979},{\"attributes\":{\"id\":\"fig_4\"},\"end\":40595,\"start\":40215},{\"attributes\":{\"id\":\"fig_5\"},\"end\":40830,\"start\":40596},{\"attributes\":{\"id\":\"fig_6\"},\"end\":41246,\"start\":40831},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":45078,\"start\":41247},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":46300,\"start\":45079},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":51404,\"start\":46301},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":52458,\"start\":51405},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":63445,\"start\":52459}]", "paragraph": "[{\"end\":1984,\"start\":1514},{\"end\":2438,\"start\":1986},{\"end\":3077,\"start\":2440},{\"end\":3598,\"start\":3079},{\"end\":4212,\"start\":3600},{\"end\":4568,\"start\":4214},{\"end\":5132,\"start\":4570},{\"end\":5497,\"start\":5149},{\"end\":6023,\"start\":5499},{\"end\":6288,\"start\":6025},{\"end\":6741,\"start\":6290},{\"end\":7030,\"start\":6743},{\"end\":7532,\"start\":7032},{\"end\":7859,\"start\":7534},{\"end\":8292,\"start\":7861},{\"end\":8797,\"start\":8336},{\"end\":9151,\"start\":8799},{\"end\":9387,\"start\":9153},{\"end\":9932,\"start\":9412},{\"end\":10362,\"start\":9934},{\"end\":10866,\"start\":10364},{\"end\":11089,\"start\":10911},{\"end\":11566,\"start\":11208},{\"end\":11763,\"start\":11602},{\"end\":11930,\"start\":11765},{\"end\":12068,\"start\":11980},{\"end\":12583,\"start\":12070},{\"end\":12805,\"start\":12668},{\"end\":12974,\"start\":12853},{\"end\":13230,\"start\":12991},{\"end\":13337,\"start\":13282},{\"end\":13720,\"start\":13384},{\"end\":14032,\"start\":13722},{\"end\":14757,\"start\":14034},{\"end\":14915,\"start\":14857},{\"end\":15027,\"start\":14990},{\"end\":15340,\"start\":15230},{\"end\":16037,\"start\":15342},{\"end\":16973,\"start\":16039},{\"end\":17363,\"start\":17036},{\"end\":17848,\"start\":17365},{\"end\":18139,\"start\":17850},{\"end\":18707,\"start\":18170},{\"end\":19236,\"start\":18709},{\"end\":19486,\"start\":19254},{\"end\":19844,\"start\":19551},{\"end\":20176,\"start\":19846},{\"end\":20885,\"start\":20222},{\"end\":21221,\"start\":20887},{\"end\":21529,\"start\":21237},{\"end\":21988,\"start\":21531},{\"end\":22273,\"start\":21990},{\"end\":22653,\"start\":22275},{\"end\":22882,\"start\":22655},{\"end\":23317,\"start\":22918},{\"end\":23643,\"start\":23319},{\"end\":24211,\"start\":23645},{\"end\":24565,\"start\":24213},{\"end\":25046,\"start\":24587},{\"end\":25236,\"start\":25048},{\"end\":25460,\"start\":25258},{\"end\":25718,\"start\":25462},{\"end\":25876,\"start\":25720},{\"end\":26396,\"start\":25922},{\"end\":26797,\"start\":26398},{\"end\":27307,\"start\":26799},{\"end\":27962,\"start\":27309},{\"end\":28896,\"start\":27964},{\"end\":29441,\"start\":28911},{\"end\":29907,\"start\":29443},{\"end\":30258,\"start\":29936},{\"end\":30855,\"start\":30260},{\"end\":31493,\"start\":30871},{\"end\":31995,\"start\":31495},{\"end\":32069,\"start\":32021},{\"end\":32337,\"start\":32295},{\"end\":32455,\"start\":32414},{\"end\":33178,\"start\":32520},{\"end\":33564,\"start\":33180},{\"end\":33691,\"start\":33604},{\"end\":33844,\"start\":33693},{\"end\":33937,\"start\":33846},{\"end\":34599,\"start\":33978},{\"end\":34938,\"start\":34601},{\"end\":35112,\"start\":34940},{\"end\":35584,\"start\":35114},{\"end\":35758,\"start\":35586},{\"end\":36101,\"start\":35814},{\"end\":36187,\"start\":36137},{\"end\":36256,\"start\":36189},{\"end\":36317,\"start\":36258},{\"end\":36403,\"start\":36346},{\"end\":36460,\"start\":36405},{\"end\":39362,\"start\":36462}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10910,\"start\":10867},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11207,\"start\":11090},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11601,\"start\":11567},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11979,\"start\":11931},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12622,\"start\":12584},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12852,\"start\":12806},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13281,\"start\":13231},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13383,\"start\":13338},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14807,\"start\":14758},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14856,\"start\":14807},{\"attributes\":{\"id\":\"formula_10\"},\"end\":14989,\"start\":14916},{\"attributes\":{\"id\":\"formula_11\"},\"end\":15229,\"start\":15028},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19550,\"start\":19487},{\"attributes\":{\"id\":\"formula_13\"},\"end\":20221,\"start\":20177},{\"attributes\":{\"id\":\"formula_14\"},\"end\":24586,\"start\":24566},{\"attributes\":{\"id\":\"formula_15\"},\"end\":25257,\"start\":25237},{\"attributes\":{\"id\":\"formula_16\"},\"end\":32294,\"start\":32070},{\"attributes\":{\"id\":\"formula_17\"},\"end\":32413,\"start\":32338},{\"attributes\":{\"id\":\"formula_18\"},\"end\":32519,\"start\":32456},{\"attributes\":{\"id\":\"formula_19\"},\"end\":33977,\"start\":33938}]", "table_ref": "[{\"end\":2088,\"start\":2074},{\"end\":25716,\"start\":25709},{\"end\":27368,\"start\":27361},{\"end\":28455,\"start\":28448},{\"end\":29219,\"start\":29212},{\"end\":29675,\"start\":29668},{\"end\":35950,\"start\":35943},{\"end\":36255,\"start\":36248},{\"end\":36402,\"start\":36395},{\"end\":36459,\"start\":36452},{\"end\":36515,\"start\":36508},{\"end\":36525,\"start\":36518},{\"end\":36609,\"start\":36602},{\"end\":37298,\"start\":37291},{\"end\":37714,\"start\":37707},{\"end\":37725,\"start\":37718},{\"end\":38332,\"start\":38325},{\"end\":38685,\"start\":38678},{\"end\":39253,\"start\":39246},{\"end\":39318,\"start\":39311}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1512,\"start\":1500},{\"attributes\":{\"n\":\"2.\"},\"end\":5147,\"start\":5135},{\"attributes\":{\"n\":\"3.\"},\"end\":8334,\"start\":8295},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9410,\"start\":9390},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12666,\"start\":12624},{\"end\":12989,\"start\":12977},{\"end\":17034,\"start\":16976},{\"attributes\":{\"n\":\"3.3.\"},\"end\":18168,\"start\":18142},{\"attributes\":{\"n\":\"4.\"},\"end\":19252,\"start\":19239},{\"attributes\":{\"n\":\"5.\"},\"end\":21235,\"start\":21224},{\"attributes\":{\"n\":\"5.1.\"},\"end\":22916,\"start\":22885},{\"attributes\":{\"n\":\"5.2.\"},\"end\":25920,\"start\":25879},{\"end\":28909,\"start\":28899},{\"attributes\":{\"n\":\"5.3.\"},\"end\":29934,\"start\":29910},{\"attributes\":{\"n\":\"6.\"},\"end\":30869,\"start\":30858},{\"end\":32019,\"start\":31998},{\"end\":33602,\"start\":33567},{\"end\":35812,\"start\":35761},{\"end\":36135,\"start\":36104},{\"end\":36344,\"start\":36320},{\"end\":39374,\"start\":39364},{\"end\":39723,\"start\":39713},{\"end\":39990,\"start\":39980},{\"end\":40226,\"start\":40216},{\"end\":40601,\"start\":40597},{\"end\":40843,\"start\":40832},{\"end\":46308,\"start\":46302}]", "table": "[{\"end\":45078,\"start\":42343},{\"end\":46300,\"start\":45979},{\"end\":51404,\"start\":48499},{\"end\":52458,\"start\":51610},{\"end\":63445,\"start\":53656}]", "figure_caption": "[{\"end\":39711,\"start\":39376},{\"end\":39978,\"start\":39725},{\"end\":40214,\"start\":39992},{\"end\":40595,\"start\":40228},{\"end\":40830,\"start\":40602},{\"end\":41246,\"start\":40846},{\"end\":42343,\"start\":41249},{\"end\":45979,\"start\":45081},{\"end\":48499,\"start\":46311},{\"end\":51610,\"start\":51407},{\"end\":53656,\"start\":52461}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18262,\"start\":18256},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18418,\"start\":18407},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18501,\"start\":18495},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18580,\"start\":18569},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18608,\"start\":18599},{\"end\":25079,\"start\":25073},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27764,\"start\":27758},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":28233,\"start\":28227},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":28464,\"start\":28458},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29488,\"start\":29482},{\"end\":30073,\"start\":30067},{\"end\":36186,\"start\":36180},{\"end\":36308,\"start\":36299},{\"end\":37560,\"start\":37551},{\"end\":37976,\"start\":37967},{\"end\":38004,\"start\":37993},{\"end\":38580,\"start\":38570}]", "bib_author_first_name": "[{\"end\":63683,\"start\":63673},{\"end\":63700,\"start\":63693},{\"end\":63711,\"start\":63706},{\"end\":63727,\"start\":63720},{\"end\":63751,\"start\":63742},{\"end\":63766,\"start\":63758},{\"end\":63783,\"start\":63776},{\"end\":63798,\"start\":63792},{\"end\":64186,\"start\":64176},{\"end\":64204,\"start\":64196},{\"end\":64218,\"start\":64214},{\"end\":64233,\"start\":64226},{\"end\":64527,\"start\":64520},{\"end\":64541,\"start\":64535},{\"end\":64556,\"start\":64552},{\"end\":64567,\"start\":64561},{\"end\":64835,\"start\":64828},{\"end\":64849,\"start\":64846},{\"end\":64876,\"start\":64867},{\"end\":64893,\"start\":64887},{\"end\":64906,\"start\":64900},{\"end\":65242,\"start\":65236},{\"end\":65264,\"start\":65260},{\"end\":65275,\"start\":65270},{\"end\":65292,\"start\":65282},{\"end\":65584,\"start\":65580},{\"end\":65600,\"start\":65596},{\"end\":65614,\"start\":65610},{\"end\":65628,\"start\":65624},{\"end\":65646,\"start\":65638},{\"end\":65664,\"start\":65656},{\"end\":65672,\"start\":65665},{\"end\":65949,\"start\":65945},{\"end\":65956,\"start\":65955},{\"end\":65968,\"start\":65964},{\"end\":66231,\"start\":66227},{\"end\":66245,\"start\":66241},{\"end\":66259,\"start\":66255},{\"end\":66277,\"start\":66269},{\"end\":66295,\"start\":66287},{\"end\":66576,\"start\":66575},{\"end\":66583,\"start\":66582},{\"end\":66591,\"start\":66590},{\"end\":66598,\"start\":66597},{\"end\":66600,\"start\":66599},{\"end\":66610,\"start\":66609},{\"end\":66900,\"start\":66893},{\"end\":66911,\"start\":66908},{\"end\":66921,\"start\":66918},{\"end\":66937,\"start\":66931},{\"end\":66947,\"start\":66944},{\"end\":66963,\"start\":66957},{\"end\":67337,\"start\":67330},{\"end\":67348,\"start\":67345},{\"end\":67353,\"start\":67349},{\"end\":67363,\"start\":67360},{\"end\":67379,\"start\":67373},{\"end\":67389,\"start\":67386},{\"end\":67405,\"start\":67399},{\"end\":67764,\"start\":67758},{\"end\":67773,\"start\":67771},{\"end\":67784,\"start\":67778},{\"end\":67792,\"start\":67789},{\"end\":67804,\"start\":67798},{\"end\":67817,\"start\":67811},{\"end\":67831,\"start\":67824},{\"end\":67842,\"start\":67838},{\"end\":67854,\"start\":67847},{\"end\":67867,\"start\":67862},{\"end\":68208,\"start\":68204},{\"end\":68228,\"start\":68222},{\"end\":68244,\"start\":68237},{\"end\":68260,\"start\":68254},{\"end\":68276,\"start\":68272},{\"end\":68548,\"start\":68547},{\"end\":68565,\"start\":68564},{\"end\":68737,\"start\":68727},{\"end\":68750,\"start\":68744},{\"end\":68765,\"start\":68761},{\"end\":69056,\"start\":69053},{\"end\":69066,\"start\":69063},{\"end\":69428,\"start\":69426},{\"end\":69441,\"start\":69434},{\"end\":69451,\"start\":69447},{\"end\":69460,\"start\":69458},{\"end\":69474,\"start\":69468},{\"end\":69488,\"start\":69480},{\"end\":69890,\"start\":69881},{\"end\":69903,\"start\":69896},{\"end\":69918,\"start\":69909},{\"end\":69931,\"start\":69923},{\"end\":69946,\"start\":69938},{\"end\":69968,\"start\":69953},{\"end\":70362,\"start\":70357},{\"end\":70372,\"start\":70368},{\"end\":70386,\"start\":70378},{\"end\":70399,\"start\":70393},{\"end\":70660,\"start\":70659},{\"end\":70974,\"start\":70968},{\"end\":70987,\"start\":70979},{\"end\":71004,\"start\":70995},{\"end\":71013,\"start\":71011},{\"end\":71025,\"start\":71021},{\"end\":71041,\"start\":71034},{\"end\":71514,\"start\":71509},{\"end\":71528,\"start\":71524},{\"end\":71546,\"start\":71539},{\"end\":71566,\"start\":71559},{\"end\":71578,\"start\":71571},{\"end\":71593,\"start\":71587},{\"end\":71931,\"start\":71925},{\"end\":71948,\"start\":71941},{\"end\":71962,\"start\":71956},{\"end\":72233,\"start\":72232},{\"end\":72241,\"start\":72240},{\"end\":72243,\"start\":72242},{\"end\":72252,\"start\":72251},{\"end\":72260,\"start\":72259},{\"end\":72269,\"start\":72268},{\"end\":72276,\"start\":72275},{\"end\":72283,\"start\":72282},{\"end\":72293,\"start\":72292},{\"end\":72302,\"start\":72301},{\"end\":72308,\"start\":72307},{\"end\":72319,\"start\":72318},{\"end\":72334,\"start\":72333},{\"end\":72345,\"start\":72344},{\"end\":72347,\"start\":72346},{\"end\":72358,\"start\":72357},{\"end\":72371,\"start\":72370},{\"end\":72781,\"start\":72775},{\"end\":72791,\"start\":72788},{\"end\":72796,\"start\":72792},{\"end\":72806,\"start\":72803},{\"end\":72821,\"start\":72816},{\"end\":72835,\"start\":72829},{\"end\":73112,\"start\":73107},{\"end\":73307,\"start\":73302},{\"end\":73322,\"start\":73315},{\"end\":73343,\"start\":73336},{\"end\":73358,\"start\":73351},{\"end\":73726,\"start\":73716},{\"end\":73750,\"start\":73743},{\"end\":73763,\"start\":73760},{\"end\":73774,\"start\":73769},{\"end\":73791,\"start\":73784},{\"end\":74059,\"start\":74051},{\"end\":74081,\"start\":74080},{\"end\":74096,\"start\":74089},{\"end\":74098,\"start\":74097},{\"end\":74354,\"start\":74349},{\"end\":74371,\"start\":74363},{\"end\":74605,\"start\":74600},{\"end\":74620,\"start\":74614},{\"end\":74630,\"start\":74627},{\"end\":74635,\"start\":74631},{\"end\":74645,\"start\":74642},{\"end\":74661,\"start\":74655},{\"end\":74982,\"start\":74977},{\"end\":75208,\"start\":75207},{\"end\":75221,\"start\":75220},{\"end\":75234,\"start\":75233},{\"end\":75246,\"start\":75245},{\"end\":75256,\"start\":75255},{\"end\":75267,\"start\":75266},{\"end\":75277,\"start\":75276},{\"end\":75288,\"start\":75287},{\"end\":75304,\"start\":75303},{\"end\":75313,\"start\":75312},{\"end\":75324,\"start\":75323},{\"end\":75338,\"start\":75337},{\"end\":75348,\"start\":75347},{\"end\":75362,\"start\":75361},{\"end\":75373,\"start\":75372},{\"end\":75383,\"start\":75382},{\"end\":75863,\"start\":75857},{\"end\":75873,\"start\":75872},{\"end\":75886,\"start\":75882},{\"end\":76257,\"start\":76251},{\"end\":76271,\"start\":76266},{\"end\":76291,\"start\":76290},{\"end\":76304,\"start\":76300},{\"end\":76658,\"start\":76657},{\"end\":76848,\"start\":76844},{\"end\":76866,\"start\":76863},{\"end\":77191,\"start\":77186},{\"end\":77210,\"start\":77203},{\"end\":77227,\"start\":77221},{\"end\":77521,\"start\":77520},{\"end\":77535,\"start\":77529},{\"end\":77550,\"start\":77542},{\"end\":77552,\"start\":77551},{\"end\":77934,\"start\":77926},{\"end\":77955,\"start\":77950},{\"end\":78323,\"start\":78315},{\"end\":78344,\"start\":78335},{\"end\":78346,\"start\":78345},{\"end\":78583,\"start\":78577},{\"end\":78599,\"start\":78593},{\"end\":78615,\"start\":78611},{\"end\":78632,\"start\":78625},{\"end\":78651,\"start\":78644},{\"end\":79006,\"start\":79005},{\"end\":79347,\"start\":79344},{\"end\":79362,\"start\":79359},{\"end\":79378,\"start\":79372},{\"end\":79389,\"start\":79385},{\"end\":79400,\"start\":79395},{\"end\":79415,\"start\":79409},{\"end\":79755,\"start\":79748},{\"end\":79767,\"start\":79762},{\"end\":79787,\"start\":79778},{\"end\":80116,\"start\":80113},{\"end\":80129,\"start\":80123},{\"end\":80141,\"start\":80136},{\"end\":80152,\"start\":80148},{\"end\":80163,\"start\":80157},{\"end\":80178,\"start\":80170},{\"end\":80192,\"start\":80185},{\"end\":80200,\"start\":80197}]", "bib_author_last_name": "[{\"end\":63691,\"start\":63684},{\"end\":63704,\"start\":63701},{\"end\":63718,\"start\":63712},{\"end\":63740,\"start\":63728},{\"end\":63756,\"start\":63752},{\"end\":63774,\"start\":63767},{\"end\":63790,\"start\":63784},{\"end\":63805,\"start\":63799},{\"end\":64194,\"start\":64187},{\"end\":64212,\"start\":64205},{\"end\":64224,\"start\":64219},{\"end\":64240,\"start\":64234},{\"end\":64533,\"start\":64528},{\"end\":64550,\"start\":64542},{\"end\":64559,\"start\":64557},{\"end\":64570,\"start\":64568},{\"end\":64844,\"start\":64836},{\"end\":64865,\"start\":64850},{\"end\":64885,\"start\":64877},{\"end\":64898,\"start\":64894},{\"end\":64915,\"start\":64907},{\"end\":65258,\"start\":65243},{\"end\":65268,\"start\":65265},{\"end\":65280,\"start\":65276},{\"end\":65303,\"start\":65293},{\"end\":65594,\"start\":65585},{\"end\":65608,\"start\":65601},{\"end\":65622,\"start\":65615},{\"end\":65636,\"start\":65629},{\"end\":65654,\"start\":65647},{\"end\":65680,\"start\":65673},{\"end\":65953,\"start\":65950},{\"end\":65962,\"start\":65957},{\"end\":65977,\"start\":65969},{\"end\":66239,\"start\":66232},{\"end\":66253,\"start\":66246},{\"end\":66267,\"start\":66260},{\"end\":66285,\"start\":66278},{\"end\":66303,\"start\":66296},{\"end\":66580,\"start\":66577},{\"end\":66588,\"start\":66584},{\"end\":66595,\"start\":66592},{\"end\":66607,\"start\":66601},{\"end\":66620,\"start\":66611},{\"end\":66906,\"start\":66901},{\"end\":66916,\"start\":66912},{\"end\":66929,\"start\":66922},{\"end\":66942,\"start\":66938},{\"end\":66955,\"start\":66948},{\"end\":66971,\"start\":66964},{\"end\":67343,\"start\":67338},{\"end\":67358,\"start\":67354},{\"end\":67371,\"start\":67364},{\"end\":67384,\"start\":67380},{\"end\":67397,\"start\":67390},{\"end\":67413,\"start\":67406},{\"end\":67769,\"start\":67765},{\"end\":67776,\"start\":67774},{\"end\":67787,\"start\":67785},{\"end\":67796,\"start\":67793},{\"end\":67809,\"start\":67805},{\"end\":67822,\"start\":67818},{\"end\":67836,\"start\":67832},{\"end\":67845,\"start\":67843},{\"end\":67860,\"start\":67855},{\"end\":67873,\"start\":67868},{\"end\":68220,\"start\":68209},{\"end\":68235,\"start\":68229},{\"end\":68252,\"start\":68245},{\"end\":68270,\"start\":68261},{\"end\":68282,\"start\":68277},{\"end\":68555,\"start\":68549},{\"end\":68562,\"start\":68557},{\"end\":68569,\"start\":68566},{\"end\":68577,\"start\":68571},{\"end\":68742,\"start\":68738},{\"end\":68759,\"start\":68751},{\"end\":68775,\"start\":68766},{\"end\":69061,\"start\":69057},{\"end\":69074,\"start\":69067},{\"end\":69432,\"start\":69429},{\"end\":69445,\"start\":69442},{\"end\":69456,\"start\":69452},{\"end\":69466,\"start\":69461},{\"end\":69478,\"start\":69475},{\"end\":69493,\"start\":69489},{\"end\":69894,\"start\":69891},{\"end\":69907,\"start\":69904},{\"end\":69921,\"start\":69919},{\"end\":69936,\"start\":69932},{\"end\":69951,\"start\":69947},{\"end\":69973,\"start\":69969},{\"end\":70366,\"start\":70363},{\"end\":70376,\"start\":70373},{\"end\":70391,\"start\":70387},{\"end\":70404,\"start\":70400},{\"end\":70666,\"start\":70661},{\"end\":70672,\"start\":70668},{\"end\":70977,\"start\":70975},{\"end\":70993,\"start\":70988},{\"end\":71009,\"start\":71005},{\"end\":71019,\"start\":71014},{\"end\":71032,\"start\":71026},{\"end\":71047,\"start\":71042},{\"end\":71522,\"start\":71515},{\"end\":71537,\"start\":71529},{\"end\":71557,\"start\":71547},{\"end\":71569,\"start\":71567},{\"end\":71585,\"start\":71579},{\"end\":71596,\"start\":71594},{\"end\":71939,\"start\":71932},{\"end\":71954,\"start\":71949},{\"end\":71975,\"start\":71963},{\"end\":72238,\"start\":72234},{\"end\":72249,\"start\":72244},{\"end\":72257,\"start\":72253},{\"end\":72266,\"start\":72261},{\"end\":72273,\"start\":72270},{\"end\":72280,\"start\":72277},{\"end\":72290,\"start\":72284},{\"end\":72299,\"start\":72294},{\"end\":72305,\"start\":72303},{\"end\":72316,\"start\":72309},{\"end\":72331,\"start\":72320},{\"end\":72342,\"start\":72335},{\"end\":72355,\"start\":72348},{\"end\":72368,\"start\":72359},{\"end\":72379,\"start\":72372},{\"end\":72786,\"start\":72782},{\"end\":72801,\"start\":72797},{\"end\":72814,\"start\":72807},{\"end\":72827,\"start\":72822},{\"end\":72843,\"start\":72836},{\"end\":73122,\"start\":73113},{\"end\":73313,\"start\":73308},{\"end\":73334,\"start\":73323},{\"end\":73349,\"start\":73344},{\"end\":73365,\"start\":73359},{\"end\":73741,\"start\":73727},{\"end\":73758,\"start\":73751},{\"end\":73767,\"start\":73764},{\"end\":73782,\"start\":73775},{\"end\":73801,\"start\":73792},{\"end\":73809,\"start\":73803},{\"end\":74074,\"start\":74060},{\"end\":74078,\"start\":74076},{\"end\":74087,\"start\":74082},{\"end\":74102,\"start\":74099},{\"end\":74110,\"start\":74104},{\"end\":74361,\"start\":74355},{\"end\":74376,\"start\":74372},{\"end\":74612,\"start\":74606},{\"end\":74625,\"start\":74621},{\"end\":74640,\"start\":74636},{\"end\":74653,\"start\":74646},{\"end\":74669,\"start\":74662},{\"end\":74999,\"start\":74983},{\"end\":75005,\"start\":75001},{\"end\":75218,\"start\":75209},{\"end\":75231,\"start\":75222},{\"end\":75243,\"start\":75235},{\"end\":75253,\"start\":75247},{\"end\":75264,\"start\":75257},{\"end\":75274,\"start\":75268},{\"end\":75285,\"start\":75278},{\"end\":75301,\"start\":75289},{\"end\":75310,\"start\":75305},{\"end\":75321,\"start\":75314},{\"end\":75335,\"start\":75325},{\"end\":75345,\"start\":75339},{\"end\":75359,\"start\":75349},{\"end\":75370,\"start\":75363},{\"end\":75380,\"start\":75374},{\"end\":75393,\"start\":75384},{\"end\":75870,\"start\":75864},{\"end\":75880,\"start\":75874},{\"end\":75892,\"start\":75887},{\"end\":75903,\"start\":75894},{\"end\":76264,\"start\":76258},{\"end\":76288,\"start\":76272},{\"end\":76298,\"start\":76292},{\"end\":76313,\"start\":76305},{\"end\":76324,\"start\":76315},{\"end\":76663,\"start\":76659},{\"end\":76669,\"start\":76665},{\"end\":76861,\"start\":76849},{\"end\":76874,\"start\":76867},{\"end\":76884,\"start\":76876},{\"end\":77201,\"start\":77192},{\"end\":77219,\"start\":77211},{\"end\":77231,\"start\":77228},{\"end\":77527,\"start\":77522},{\"end\":77540,\"start\":77536},{\"end\":77560,\"start\":77553},{\"end\":77567,\"start\":77562},{\"end\":77948,\"start\":77935},{\"end\":77959,\"start\":77956},{\"end\":77970,\"start\":77961},{\"end\":78333,\"start\":78324},{\"end\":78352,\"start\":78347},{\"end\":78591,\"start\":78584},{\"end\":78609,\"start\":78600},{\"end\":78623,\"start\":78616},{\"end\":78642,\"start\":78633},{\"end\":78659,\"start\":78652},{\"end\":79015,\"start\":79007},{\"end\":79022,\"start\":79017},{\"end\":79357,\"start\":79348},{\"end\":79370,\"start\":79363},{\"end\":79383,\"start\":79379},{\"end\":79393,\"start\":79390},{\"end\":79407,\"start\":79401},{\"end\":79423,\"start\":79416},{\"end\":79760,\"start\":79756},{\"end\":79776,\"start\":79768},{\"end\":79798,\"start\":79788},{\"end\":80121,\"start\":80117},{\"end\":80134,\"start\":80130},{\"end\":80146,\"start\":80142},{\"end\":80155,\"start\":80153},{\"end\":80168,\"start\":80164},{\"end\":80183,\"start\":80179},{\"end\":80195,\"start\":80193},{\"end\":80204,\"start\":80201}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1902.03545\",\"id\":\"b0\"},\"end\":64092,\"start\":63673},{\"attributes\":{\"doi\":\"arXiv:1904.03292\",\"id\":\"b1\"},\"end\":64462,\"start\":64094},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8049057},\"end\":64761,\"start\":64464},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1846738},\"end\":65171,\"start\":64763},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":68220930},\"end\":65533,\"start\":65173},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":8577357},\"end\":65887,\"start\":65535},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":13967968},\"end\":66186,\"start\":65889},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":2497886},\"end\":66510,\"start\":66188},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":216009},\"end\":66830,\"start\":66512},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":4756557},\"end\":67249,\"start\":66832},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":61156330},\"end\":67660,\"start\":67251},{\"attributes\":{\"doi\":\"arXiv:1512.01274\",\"id\":\"b11\"},\"end\":68160,\"start\":67662},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2266226},\"end\":68513,\"start\":68162},{\"attributes\":{\"id\":\"b13\"},\"end\":68687,\"start\":68515},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":67876984},\"end\":68984,\"start\":68689},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":15398231},\"end\":69350,\"start\":68986},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4758668},\"end\":69805,\"start\":69352},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":30128128},\"end\":70312,\"start\":69807},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":459456},\"end\":70601,\"start\":70314},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5258236},\"end\":70858,\"start\":70603},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":2057685},\"end\":71407,\"start\":70860},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":13751202},\"end\":71872,\"start\":71409},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6178817},\"end\":72154,\"start\":71874},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":51610168},\"end\":72705,\"start\":72156},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":89616527},\"end\":73067,\"start\":72707},{\"attributes\":{\"id\":\"b25\"},\"end\":73253,\"start\":73069},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1923223},\"end\":73646,\"start\":73255},{\"attributes\":{\"doi\":\"arXiv:1908.01091\",\"id\":\"b27\"},\"end\":74017,\"start\":73648},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":13570924},\"end\":74296,\"start\":74019},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3286674},\"end\":74540,\"start\":74298},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":15169802},\"end\":74944,\"start\":74542},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":740063},\"end\":75163,\"start\":74946},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":10659969},\"end\":75723,\"start\":75165},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":14273023},\"end\":76187,\"start\":75725},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":5896299},\"end\":76607,\"start\":76189},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2879680},\"end\":76783,\"start\":76609},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":12609235},\"end\":77133,\"start\":76785},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3840500},\"end\":77431,\"start\":77135},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":9530176},\"end\":77809,\"start\":77433},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":49868626},\"end\":78223,\"start\":77811},{\"attributes\":{\"id\":\"b40\"},\"end\":78513,\"start\":78225},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":417154},\"end\":78972,\"start\":78515},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":2777306},\"end\":79285,\"start\":78974},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":195346562},\"end\":79713,\"start\":79287},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":18447312},\"end\":80050,\"start\":79715},{\"attributes\":{\"id\":\"b45\"},\"end\":80365,\"start\":80052}]", "bib_title": "[{\"end\":64518,\"start\":64464},{\"end\":64826,\"start\":64763},{\"end\":65234,\"start\":65173},{\"end\":65578,\"start\":65535},{\"end\":65943,\"start\":65889},{\"end\":66225,\"start\":66188},{\"end\":66573,\"start\":66512},{\"end\":66891,\"start\":66832},{\"end\":67328,\"start\":67251},{\"end\":68202,\"start\":68162},{\"end\":68725,\"start\":68689},{\"end\":69051,\"start\":68986},{\"end\":69424,\"start\":69352},{\"end\":69879,\"start\":69807},{\"end\":70355,\"start\":70314},{\"end\":70657,\"start\":70603},{\"end\":70966,\"start\":70860},{\"end\":71507,\"start\":71409},{\"end\":71923,\"start\":71874},{\"end\":72230,\"start\":72156},{\"end\":72773,\"start\":72707},{\"end\":73300,\"start\":73255},{\"end\":74049,\"start\":74019},{\"end\":74347,\"start\":74298},{\"end\":74598,\"start\":74542},{\"end\":74975,\"start\":74946},{\"end\":75205,\"start\":75165},{\"end\":75855,\"start\":75725},{\"end\":76249,\"start\":76189},{\"end\":76655,\"start\":76609},{\"end\":76842,\"start\":76785},{\"end\":77184,\"start\":77135},{\"end\":77518,\"start\":77433},{\"end\":77924,\"start\":77811},{\"end\":78575,\"start\":78515},{\"end\":79003,\"start\":78974},{\"end\":79342,\"start\":79287},{\"end\":79746,\"start\":79715}]", "bib_author": "[{\"end\":63693,\"start\":63673},{\"end\":63706,\"start\":63693},{\"end\":63720,\"start\":63706},{\"end\":63742,\"start\":63720},{\"end\":63758,\"start\":63742},{\"end\":63776,\"start\":63758},{\"end\":63792,\"start\":63776},{\"end\":63807,\"start\":63792},{\"end\":64196,\"start\":64176},{\"end\":64214,\"start\":64196},{\"end\":64226,\"start\":64214},{\"end\":64242,\"start\":64226},{\"end\":64535,\"start\":64520},{\"end\":64552,\"start\":64535},{\"end\":64561,\"start\":64552},{\"end\":64572,\"start\":64561},{\"end\":64846,\"start\":64828},{\"end\":64867,\"start\":64846},{\"end\":64887,\"start\":64867},{\"end\":64900,\"start\":64887},{\"end\":64917,\"start\":64900},{\"end\":65260,\"start\":65236},{\"end\":65270,\"start\":65260},{\"end\":65282,\"start\":65270},{\"end\":65305,\"start\":65282},{\"end\":65596,\"start\":65580},{\"end\":65610,\"start\":65596},{\"end\":65624,\"start\":65610},{\"end\":65638,\"start\":65624},{\"end\":65656,\"start\":65638},{\"end\":65682,\"start\":65656},{\"end\":65955,\"start\":65945},{\"end\":65964,\"start\":65955},{\"end\":65979,\"start\":65964},{\"end\":66241,\"start\":66227},{\"end\":66255,\"start\":66241},{\"end\":66269,\"start\":66255},{\"end\":66287,\"start\":66269},{\"end\":66305,\"start\":66287},{\"end\":66582,\"start\":66575},{\"end\":66590,\"start\":66582},{\"end\":66597,\"start\":66590},{\"end\":66609,\"start\":66597},{\"end\":66622,\"start\":66609},{\"end\":66908,\"start\":66893},{\"end\":66918,\"start\":66908},{\"end\":66931,\"start\":66918},{\"end\":66944,\"start\":66931},{\"end\":66957,\"start\":66944},{\"end\":66973,\"start\":66957},{\"end\":67345,\"start\":67330},{\"end\":67360,\"start\":67345},{\"end\":67373,\"start\":67360},{\"end\":67386,\"start\":67373},{\"end\":67399,\"start\":67386},{\"end\":67415,\"start\":67399},{\"end\":67771,\"start\":67758},{\"end\":67778,\"start\":67771},{\"end\":67789,\"start\":67778},{\"end\":67798,\"start\":67789},{\"end\":67811,\"start\":67798},{\"end\":67824,\"start\":67811},{\"end\":67838,\"start\":67824},{\"end\":67847,\"start\":67838},{\"end\":67862,\"start\":67847},{\"end\":67875,\"start\":67862},{\"end\":68222,\"start\":68204},{\"end\":68237,\"start\":68222},{\"end\":68254,\"start\":68237},{\"end\":68272,\"start\":68254},{\"end\":68284,\"start\":68272},{\"end\":68557,\"start\":68547},{\"end\":68564,\"start\":68557},{\"end\":68571,\"start\":68564},{\"end\":68579,\"start\":68571},{\"end\":68744,\"start\":68727},{\"end\":68761,\"start\":68744},{\"end\":68777,\"start\":68761},{\"end\":69063,\"start\":69053},{\"end\":69076,\"start\":69063},{\"end\":69434,\"start\":69426},{\"end\":69447,\"start\":69434},{\"end\":69458,\"start\":69447},{\"end\":69468,\"start\":69458},{\"end\":69480,\"start\":69468},{\"end\":69495,\"start\":69480},{\"end\":69896,\"start\":69881},{\"end\":69909,\"start\":69896},{\"end\":69923,\"start\":69909},{\"end\":69938,\"start\":69923},{\"end\":69953,\"start\":69938},{\"end\":69975,\"start\":69953},{\"end\":70368,\"start\":70357},{\"end\":70378,\"start\":70368},{\"end\":70393,\"start\":70378},{\"end\":70406,\"start\":70393},{\"end\":70668,\"start\":70659},{\"end\":70674,\"start\":70668},{\"end\":70979,\"start\":70968},{\"end\":70995,\"start\":70979},{\"end\":71011,\"start\":70995},{\"end\":71021,\"start\":71011},{\"end\":71034,\"start\":71021},{\"end\":71049,\"start\":71034},{\"end\":71524,\"start\":71509},{\"end\":71539,\"start\":71524},{\"end\":71559,\"start\":71539},{\"end\":71571,\"start\":71559},{\"end\":71587,\"start\":71571},{\"end\":71598,\"start\":71587},{\"end\":71941,\"start\":71925},{\"end\":71956,\"start\":71941},{\"end\":71977,\"start\":71956},{\"end\":72240,\"start\":72232},{\"end\":72251,\"start\":72240},{\"end\":72259,\"start\":72251},{\"end\":72268,\"start\":72259},{\"end\":72275,\"start\":72268},{\"end\":72282,\"start\":72275},{\"end\":72292,\"start\":72282},{\"end\":72301,\"start\":72292},{\"end\":72307,\"start\":72301},{\"end\":72318,\"start\":72307},{\"end\":72333,\"start\":72318},{\"end\":72344,\"start\":72333},{\"end\":72357,\"start\":72344},{\"end\":72370,\"start\":72357},{\"end\":72381,\"start\":72370},{\"end\":72788,\"start\":72775},{\"end\":72803,\"start\":72788},{\"end\":72816,\"start\":72803},{\"end\":72829,\"start\":72816},{\"end\":72845,\"start\":72829},{\"end\":73124,\"start\":73107},{\"end\":73315,\"start\":73302},{\"end\":73336,\"start\":73315},{\"end\":73351,\"start\":73336},{\"end\":73367,\"start\":73351},{\"end\":73743,\"start\":73716},{\"end\":73760,\"start\":73743},{\"end\":73769,\"start\":73760},{\"end\":73784,\"start\":73769},{\"end\":73803,\"start\":73784},{\"end\":73811,\"start\":73803},{\"end\":74076,\"start\":74051},{\"end\":74080,\"start\":74076},{\"end\":74089,\"start\":74080},{\"end\":74104,\"start\":74089},{\"end\":74112,\"start\":74104},{\"end\":74363,\"start\":74349},{\"end\":74378,\"start\":74363},{\"end\":74614,\"start\":74600},{\"end\":74627,\"start\":74614},{\"end\":74642,\"start\":74627},{\"end\":74655,\"start\":74642},{\"end\":74671,\"start\":74655},{\"end\":75001,\"start\":74977},{\"end\":75007,\"start\":75001},{\"end\":75220,\"start\":75207},{\"end\":75233,\"start\":75220},{\"end\":75245,\"start\":75233},{\"end\":75255,\"start\":75245},{\"end\":75266,\"start\":75255},{\"end\":75276,\"start\":75266},{\"end\":75287,\"start\":75276},{\"end\":75303,\"start\":75287},{\"end\":75312,\"start\":75303},{\"end\":75323,\"start\":75312},{\"end\":75337,\"start\":75323},{\"end\":75347,\"start\":75337},{\"end\":75361,\"start\":75347},{\"end\":75372,\"start\":75361},{\"end\":75382,\"start\":75372},{\"end\":75395,\"start\":75382},{\"end\":75872,\"start\":75857},{\"end\":75882,\"start\":75872},{\"end\":75894,\"start\":75882},{\"end\":75905,\"start\":75894},{\"end\":76266,\"start\":76251},{\"end\":76290,\"start\":76266},{\"end\":76300,\"start\":76290},{\"end\":76315,\"start\":76300},{\"end\":76326,\"start\":76315},{\"end\":76665,\"start\":76657},{\"end\":76671,\"start\":76665},{\"end\":76863,\"start\":76844},{\"end\":76876,\"start\":76863},{\"end\":76886,\"start\":76876},{\"end\":77203,\"start\":77186},{\"end\":77221,\"start\":77203},{\"end\":77233,\"start\":77221},{\"end\":77529,\"start\":77520},{\"end\":77542,\"start\":77529},{\"end\":77562,\"start\":77542},{\"end\":77569,\"start\":77562},{\"end\":77950,\"start\":77926},{\"end\":77961,\"start\":77950},{\"end\":77972,\"start\":77961},{\"end\":78335,\"start\":78315},{\"end\":78354,\"start\":78335},{\"end\":78593,\"start\":78577},{\"end\":78611,\"start\":78593},{\"end\":78625,\"start\":78611},{\"end\":78644,\"start\":78625},{\"end\":78661,\"start\":78644},{\"end\":79017,\"start\":79005},{\"end\":79024,\"start\":79017},{\"end\":79359,\"start\":79344},{\"end\":79372,\"start\":79359},{\"end\":79385,\"start\":79372},{\"end\":79395,\"start\":79385},{\"end\":79409,\"start\":79395},{\"end\":79425,\"start\":79409},{\"end\":79762,\"start\":79748},{\"end\":79778,\"start\":79762},{\"end\":79800,\"start\":79778},{\"end\":80123,\"start\":80113},{\"end\":80136,\"start\":80123},{\"end\":80148,\"start\":80136},{\"end\":80157,\"start\":80148},{\"end\":80170,\"start\":80157},{\"end\":80185,\"start\":80170},{\"end\":80197,\"start\":80185},{\"end\":80206,\"start\":80197}]", "bib_venue": "[{\"end\":67051,\"start\":67016},{\"end\":69184,\"start\":69134},{\"end\":69583,\"start\":69543},{\"end\":70063,\"start\":70023},{\"end\":70464,\"start\":70439},{\"end\":70732,\"start\":70707},{\"end\":71137,\"start\":71097},{\"end\":73455,\"start\":73415},{\"end\":76964,\"start\":76929},{\"end\":78749,\"start\":78709},{\"end\":79112,\"start\":79072},{\"end\":79513,\"start\":79473},{\"end\":79888,\"start\":79848},{\"end\":63855,\"start\":63823},{\"end\":64174,\"start\":64094},{\"end\":64597,\"start\":64572},{\"end\":64950,\"start\":64917},{\"end\":65343,\"start\":65305},{\"end\":65693,\"start\":65682},{\"end\":66014,\"start\":65979},{\"end\":66333,\"start\":66305},{\"end\":66660,\"start\":66622},{\"end\":67014,\"start\":66973},{\"end\":67437,\"start\":67415},{\"end\":67756,\"start\":67662},{\"end\":68322,\"start\":68284},{\"end\":68545,\"start\":68515},{\"end\":68815,\"start\":68777},{\"end\":69132,\"start\":69076},{\"end\":69541,\"start\":69495},{\"end\":70021,\"start\":69975},{\"end\":70437,\"start\":70406},{\"end\":70705,\"start\":70674},{\"end\":71095,\"start\":71049},{\"end\":71627,\"start\":71598},{\"end\":72006,\"start\":71977},{\"end\":72414,\"start\":72381},{\"end\":72867,\"start\":72845},{\"end\":73105,\"start\":73069},{\"end\":73413,\"start\":73367},{\"end\":73714,\"start\":73648},{\"end\":74150,\"start\":74112},{\"end\":74403,\"start\":74378},{\"end\":74723,\"start\":74671},{\"end\":75036,\"start\":75007},{\"end\":75421,\"start\":75395},{\"end\":75938,\"start\":75905},{\"end\":76378,\"start\":76326},{\"end\":76682,\"start\":76671},{\"end\":76927,\"start\":76886},{\"end\":77266,\"start\":77233},{\"end\":77598,\"start\":77569},{\"end\":78010,\"start\":77972},{\"end\":78313,\"start\":78225},{\"end\":78707,\"start\":78661},{\"end\":79070,\"start\":79024},{\"end\":79471,\"start\":79425},{\"end\":79846,\"start\":79800},{\"end\":80111,\"start\":80052}]"}}}, "year": 2023, "month": 12, "day": 17}