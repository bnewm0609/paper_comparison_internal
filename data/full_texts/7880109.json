{"id": 7880109, "updated": "2023-11-11 00:19:22.207", "metadata": {"title": "Deep convolutional acoustic word embeddings using word-pair side information", "authors": "[{\"first\":\"Herman\",\"last\":\"Kamper\",\"middle\":[]},{\"first\":\"Weiran\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Karen\",\"last\":\"Livescu\",\"middle\":[]}]", "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "journal": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "publication_date": {"year": 2016, "month": null, "day": null}, "abstract": "Recent studies have been revisiting whole words as the basic modelling unit in speech recognition and query applications, instead of phonetic units. Such whole-word segmental systems rely on a function that maps a variable-length speech segment to a vector in a fixed-dimensional space; the resulting acoustic word embeddings need to allow for accurate discrimination between different word types, directly in the embedding space. We compare several old and new approaches in a word discrimination task. Our best approach uses side information in the form of known word pairs to train a Siamese convolutional neural network (CNN): a pair of tied networks that take two speech segments as input and produce their embeddings, trained with a hinge loss that separates same-word pairs and different-word pairs by some margin. A word classifier CNN performs similarly, but requires much stronger supervision. Both types of CNNs yield large improvements over the best previously published results on the word discrimination task.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2952155034", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icassp/KamperWL16", "doi": "10.1109/icassp.2016.7472619"}}, "content": {"source": {"pdf_hash": "9dc5da2af44e52c7669d6ce92f11d1bdbb32f0cd", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1510.01032v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1510.01032", "status": "GREEN"}}, "grobid": {"id": "7aee5d1a2df255d96417ca0d08b6881ce2577026", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9dc5da2af44e52c7669d6ce92f11d1bdbb32f0cd.txt", "contents": "\nDEEP CONVOLUTIONAL ACOUSTIC WORD EMBEDDINGS USING WORD-PAIR SIDE INFORMATION\n\n\nHerman Kamper h.kamper@sms.ed.ac.uk \nSchool of Informatics\nCSTR and ILCC\nUniversity of Edinburgh\nUK\n\nWeiran Wang weiranwang@ttic.edu \nToyota Technological Institute at Chicago\nUSA\n\nKaren Livescu klivescu@ttic.edu \nToyota Technological Institute at Chicago\nUSA\n\nDEEP CONVOLUTIONAL ACOUSTIC WORD EMBEDDINGS USING WORD-PAIR SIDE INFORMATION\n\nRecent studies have been revisiting whole words as the basic modelling unit in speech recognition and query applications, instead of phonetic units. Such whole-word segmental systems rely on a function that maps a variable-length speech segment to a vector in a fixed-dimensional space; the resulting acoustic word embeddings need to allow for accurate discrimination between different word types, directly in the embedding space. We compare several old and new approaches in a word discrimination task. Our best approach uses side information in the form of known word pairs to train a Siamese convolutional neural network (CNN): a pair of tied networks that take two speech segments as input and produce their embeddings, trained with a hinge loss that separates same-word pairs and different-word pairs by some margin. A word classifier CNN performs similarly, but requires much stronger supervision. Both types of CNNs yield large improvements over the best previously published results on the word discrimination task.Index Terms-Acoustic word embeddings, segmental acoustic models, fixed-dimensional representations, query-by-example search.\n\nINTRODUCTION\n\nMost current speech processing systems rely on a deep architecture to classify speech frames into subword units (often phone states). This approach still relies on frame-level independence assumptions as well as a pronunciation lexicon for breaking up words into their subword constituents. As an alternative, some researchers [1][2][3][4][5][6][7] have started to reconsider using whole words as the basic modelling unit.\n\nSome of the earliest speech recognition systems were based on template-based whole-word modelling [8]. This idea has been revisited in modern template-based automatic speech recognition (ASR) systems [1,2], as well as modern speech indexing applications such as query-by-example search [9,10]. These systems typically use dynamic time warping (DTW) to quantify the similarity of phone or word segments of variable length. Recent work has also considered frame-level embeddings which map acoustic features to a new frame-level representation that is tailored to word discrimination when combined with DTW [11][12][13]. DTW, however, has known inadequacies [14] and is quadratic-time in the duration of the segments.\n\nLevin et al. [3] proposed a segmental approach where an arbitrarylength speech segment is embedded in a fixed-dimensional space such that segments of the same word type have similar embeddings. Segments can then be compared by simply calculating a distance in the embedding space, a linear time operation in the embedding dimensionality. Several approaches were developed in [3], and in [15] these were successfully applied in a query-by-example search system. Bengio and Heigold [4] similarly used whole-word fixeddimensional representations in a segmental ASR lattice rescoring system. Their acoustic embeddings are obtained from a convolutional neural network (CNN), trained with a combination of a word classification and a ranking loss. When combining the hypotheses of the baseline system with the embedding-based scores, ASR performance was improved. A similar approach was followed in [5], where long short-term memory (LSTM) networks were used to obtain wholeword embeddings for a query-by-example search task. Finally, Maas et al. [6] trained a regression CNN that reconstructs a semantic word embedding from acoustic speech input; these features were used in a segmental conditional random field ASR system.\n\nIn this paper we compare several CNN-based approaches to each other and to the best approach of Levin et al. [3], on a word discrimination task. This task has been used in several other studies [11,12,16] to assess the accuracy of acoustic embedding approaches without the need to train a complete recognition or search system. Building on ideas from earlier CNN-based approaches, we propose new networks that make use of weaker supervision in the form of known word pairs. The approach is based on Siamese networks: tied networks that take in pairs of input vectors and minimize or maximize a distance depending on whether a pair comes from the same or different classes [17]. We show that a Siamese CNN trained with with a hinge-like contrastive loss function outperforms the best approach of Levin et al. [3], and performs similarly to a word classifier CNN, despite the weaker form of supervision. By reducing the Siamese CNN embedding dimensionality with a post-processing linear discriminant analysis, we also obtain a more compact embedding that maintains best performance.\n\n\nACOUSTIC WORD EMBEDDING APPROACHES\n\nFor speech applications using fixed-dimensional representations of whole words, it is desirable to find a mapping such that word segments of the same type are close in the embedding space while those of different types are far from each other. Formally, we use the notation Y = y 1:T to denote a vector time series, where each y t \u2208 R b is a b-dimensional frame-level feature vector (e.g. MFCCs). An acoustic word embedding approach is a function f (Y ) that maps arbitrarylength time series Y into a fixed-dimensional space R d ; if Y1 and Y2 are two word segments, the distance between the vectors f (Y1) and f (Y2) should indicate whether they are of the same word type or not.\n\nTypical embedding approaches use a training set of known word segments Ytrain = {Yi} N train i=1 to learn f . Different degrees of supervision can be assumed, ranging from unsupervised, where the only knowledge of Ytrain is that it contains unidentified word segments, to supervised, where the word label for each segment is known. Below we review previous work (Sections 2.1 and 2.2) and then present our own approaches which use weak supervision in the form of known word pairs (Section 2.3), and can also additionally use word labels to find lower-dimensional but still accurate embeddings (Section 2.4).  \nx i = f (Y i ) \u00d7n full \u00d7n conv (b) Y 1 x 1 = f (Y 1 ) Y 2 x 2 = f (Y 2 ) distance or similarity l(x 1 , x 2 )\n\nReference vector methods\n\nSeveral embedding approaches were proposed and compared in Levin et al. [3] based on the idea of using a reference vector to construct the mapping f . For a target speech segment, a reference vector consists of the DTW alignment cost to every exemplar in a reference set Yref \u2286 Ytrain. Applying dimensionality reduction to the reference vector yields the desired embedding in R d . The intuition is that the content of a speech segment should be characterized well through its similarity to other segments. Such embeddings have subsequently been used for keyword search [15] and unsupervised term discovery [18]. One drawback is the need to compute a large number of DTW alignments. Several dimensionality reduction approaches were considered in [3], and in Section 3.3 we compare to their best overall approach which uses a combination of Laplacian eigenmaps (a nonlinear graph embedding approach) and linear discriminant analysis.\n\n\nWord classification CNN\n\nFor the whole-word speech recognition system in [4], Bengio and Heigold proposed that, when word labels Wtrain = {wi} N train i=1 are available for the training segments Ytrain, a supervised neural network can simply be trained to predict the word class (type) given the speech as input. The softmax prediction layer of such a neural network then gives a fixed-dimensional representation in R d , where d here is the number of distinct word types (the vocabulary size). During testing, some inputs may correspond to unseen words, but even in these cases the softmax layer gives a fixed-dimensional distributional representation of the input in terms of seen word types.\n\nA standard feed-forward neural network classifier, however, requires fixed-dimensional input. A simple solution was used in [4]: All word segments are padded to the same length, given by the maximum duration of a word segment in Ytrain. Instead of using fully-connected layers, convolutional and pooling layers are used to alleviate the effect of the padding. A convolutional neural network (CNN) such as this is shown in Figure 1(a). Our implementation uses mean-normalized MFCCs which are zero-padded to npad frames. One-dimensional convolution is performed only over time, covering a number of frames and all features, and is followed by max-pooling. These layers can be repeated. A number of fully-connected layers is used next, which feeds into the final softmax layer. Formally, the whole CNN defines a mapping function f : Yi \u2208 R b\u00d7n pad \u2192 xi \u2208 R d , that takes input Yi, obtained by padding the variable-length b-dimensional vector time series Yi, and produces the acoustic embedding xi.\n\nInstead of using the representation from the word classification CNN directly, Bengio and Heigold [4] used a paired network with a ranking loss to map acoustic word embeddings into a common space with orthography-based word embeddings obtained by also mapping the word labels Wtrain into a lower-dimensional space. This was done to make it possible to use the classifier outputs in a particular lattice rescoring architecture, which requires scores for lattice arcs. The evaluation framework we use (Section 3.1) is designed to be decoupled from a recognition architecture, and we can therefore use the distributional representation from the classifier CNN directly. An investigation of whether embeddings of word labels can be additionally used to improve acoustic word embeddings is left for future work.\n\n\nWord similarity Siamese CNNs\n\nIf the labels Wtrain for the training set Ytrain are not known, a weaker form of supervision that has also been used [11][12][13]19] is the knowledge that pairs of word segments in Ytrain share the same unknown word type: Strain = {(m, n) : (Ym, Yn) are of the same type}. This type of side information is appealing since it is often easier to obtain in low-resource settings, for example by using an unsupervised term discovery system [20,21] to find unidentified matching word pairs.\n\nSuch paired supervision has been used for several problems and domains, including phonetic discovery [13,22], semantic word embeddings [23][24][25] and vision applications [26]. Many of these studies use Siamese networks, a term used since the early 1990s to describe a pair of networks with tied parameters which is trained to optimize a distance function between representations of two data instances [17]. To train these networks it is sometimes assumed that pairs not in Strain belong to different types; we also make this assumption here. Figure 1(b) illustrates how we apply this idea to obtain acoustic word embeddings. The two sides of our Siamese network take padded inputs Y1 and Y2. For the two sides we use CNNs similar to that of the word classification CNN. But instead of terminating in a softmax layer, the final fully connected layer on each side gives the desired acoustic embedding. In initial experiments on development data, we considered several loss functions, and here we focus only on the most successful ones. We found that losses based on cosine similarity outperformed Euclidean-based losses, and in particular the coscos 2 loss from [27] gave the best performance of the losses in [17,27]:\nl cos cos 2 (x1, x2) = 1\u2212cos(x 1 ,x 2 ) 2 if same cos 2 (x1, x2) if different(1)\nThis loss pushes the angle between embeddings of the same type to be zero, while embeddings of different types are pushed to be orthogonal. In discrimination tasks, the decision of whether two data instances are of the same type is not based on their absolute distance, but rather their relative distance compared to other pairs. This motivates a margin-based (hinge) loss, similar to that of [24,25]:\nlcos hinge = max {0, m + dcos(x1, x2) \u2212 dcos(x1, x3)}(2)\nwhere dcos(x1, x2) = 1\u2212cos(x 1 ,x 2 ) 2 is the cosine distance between x1 and x2, and m is a margin parameter. Here, x1 and x2 are always of the same type while x1 and x3 are of different types. This loss is therefore at a minimum when all embeddings x1 and x2 of the same type are more similar by a margin m than embeddings x1 and x3 of different types. The margin also gives some leeway to the model.\n\nAlthough Siamese networks have been used widely, to our knowledge this is the first work which uses Siamese networks (in particular Siamese CNNs) to obtain acoustic word embeddings from speech.\n\n\nControlling embedding dimensionality\n\nWe aim to learn word embeddings that are both discriminative and compact (low-dimensional). The desired dimensionality may be guided by both computational and data constraints, and we may wish to be able to adjust it. For word classification networks (Section 2.2), the output dimensionality is given by the vocabulary size. In our experiments (next section), we explore adjusting the dimensionality by inserting an additional linear bottleneck layer before the final softmax, with the number of units corresponding to the desired final dimensionality. In Siamese networks (Section 2.3) the final dimensionality can be directly tuned. If we have access to word labels Wtrain in addition to word pairs Strain, we can also perform additional dimensionlity reduction on the Siamese CNN outputs using a supervised technique; in our experiments we use linear discriminant analysis (LDA).\n\n\nEXPERIMENTS\n\n\nEvaluation and experimental setup\n\nUltimately we would like to evaluate the different acoustic embedding approaches for downstream speech recognition and search tasks. However, we do not want to be tied to a specific recognition architecture, and we would like to quickly compare many embedding approaches. We therefore use a word discrimination task developed for this purpose [16]; in the same-different task, we are given a pair of acoustic segments, each corresponding to a word, and we must decide whether the segments are examples of the same or different words.\n\nThis task can be approached in a number of ways, but typically it is done either via a DTW score between segments (when using frameby-frame embeddings), or via a Euclidean or cosine distance between vectors (when embedding complete segments). In our evaluation, after training a model on Ytrain, the acoustic word embeddings of a disjoint test set Ytest are computed. For every word pair in this set, the cosine distance 1 is calculated between their embeddings. Two words can then be classified as being of the same or different type based on some threshold, and a precision-recall curve is obtained by varying the threshold. To evaluate embeddings across different operating points, the area under the precision-recall curve is calculated to yield the final evaluation metric, referred to as the average precision (AP).\n\nWe use data from the Switchboard corpus of English conversational telephone speech. Data is parameterized as Mel-frequency cepstral coefficients (MFCCs) with first and second order derivatives, yielding 39-dimensional feature vectors. Cepstral mean and variance normalization (CMVN) is applied per conversation side. For the training set Ytrain we use the set of about 10k word tokens from [11,12]; it consists of word segments of at least 5 characters and 0.5 seconds in duration extracted from a forced alignment of the transcriptions, and comprises about 105 minutes of speech. For the Siamsese CNNs, this set results in about 100k word segment pairs for Strain. For testing, we use the 11k-token set Ytest from [3,11,12], making the results from these studies directly comparable to the results obtained here. 2 This set was extracted from a portion of Switchboard distinct from Ytrain. Similarly, we extracted an 11k-token development set.\n\nAs mentioned in Section 1, recent studies [11,12] have also been using frame-level embedding approaches in combination with DTW to perform the same-different task. These approaches map the original features to a new frame-level representation that is tailored to word discrimination. We compare our results to that of [11], which uses posteriograms over a partitioned universal background model (UBM), as well as [12], which uses a correspondence autoencoder.\n\n\nNetwork architectures\n\nWe used the Theano [28] toolkit to implement the CNN-based models of Sections 2.2 and 2.3. 3 Models are trained using ADADELTA [29], an adaptive learning rate stochastic optimization method that adapts over time based on an accumulation of past gradients; we set the momentum hyper-parameter to \u03c1 = 0.9 and the precision parameter to = 10 \u22126 . Input speech segments are padded to npad = 200 frames (2 s), which corresponds to the longest word segment in Ytrain. The architectures of the CNNs were optimized separately on the development data for each network type, resulting in the following structures:\n\n\u2022 Word classifier CNN: 1-D convolution with 96 filters over 9 frames; ReLU; max pooling over 3 units; 1-D convolution with 96 filters over 8 units; ReLU; max pooling over 3 units; 1024-unit fully-connected ReLU; softmax layer over 1061 word types. \u2022 Word similarity Siamese CNN: two convolutional and max pooling layers as above; 2048-unit fully-connected ReLU; 1024-unit fully-connected linear linear; terminates in loss l(x1, x2). For the word classifier CNN, we only train on words in Ytrain that occur at least three times; this gives a subset of 87% of all tokens with 1061 unique word types. This minimum count was tuned on the development set. To see the effect of the convolutions, we also train a word classifier deep neural network (DNN) using two 2048-unit fully-connected ReLU layers and a 1061-unit softmax layer. For the Siamese CNN using lcos hinge, we use a margin m = 0.15 (tuned on the development set). If we had used ReLUs in the final layer in the Siamese CNNs, the angles between embeddings would be restricted to [0, \u03c0/2]; we therefore use a final linear layer. All weights are initialized randomly; we run all models with five different initializations and report average performance and standard deviations. Table 1 shows AP performance on the test set from previous studies (models 1 to 4) as well as our newly implemented models (5 to 11).\n\n\nResults\n\nThe first three models perform word discrimination using DTW on frame-level embeddings of word segments; model 1 works directly on acoustic features, while models 2 and 3 work on features optimized for word discrimination. Model 3 yields the best previously reported result on this task. Model 4 is the best acoustic word embedding approach from [3] (Section 2.1), representing the best previous result for an approach that produces embeddings of whole word segments.\n\nModels 5 to 11 are the neural network-based approaches. The effect of using the convolutional layers is evident from the large improvement in AP of model 6 over model 5. Both of these models are trained on the word type labels Wtrain, which is also the type of supervision used for model 4, making the improvement of model Table 1. Average precision (AP) on the test set. Models 1 to 3: best prior DTW-based approaches; model 4: best prior acoustic word embedding approach based on reference vectors; models 5 to 11: this work. For models 1 to 3, dimensionality (dim.) is at the frame level; for models 4 to 11 it is the segment embedding dimensionality.\n\n\n# Representation\n\nDim. AP  4 The dimensionality of the acoustic embeddings of model 6, however, is much larger than that of model 4.\n\nWe therefore also trained a version of model 6 where the embedding is obtained from a linear bottleneck layer inserted just before the final softmax layer. The lower-dimensional embeddings from this approach (model 7) still improves on model 4 by a sizable margin.\n\nOf the Siamese CNNs, the model with the lcos hinge loss (model 9, Section 2.3) outperforms its l cos cos 2 counterpart, and yields a large improvement over model 4, which was the previous best acoustic embedding approach. It also gives similar performance to the word classification CNN (model 6), even though the pair-wise side information Strain used for model 9 is a weaker form of supervision than the fully labelled supervision Wtrain used in model 6. When reducing the embedding dimensionality to 50 (model 10), AP is still higher than any of the d = 50 competitors. Model 9's improvement over model 3 is also interesting since the former does not use any DTW alignment information. Finally, model 11 shows that LDA on the output of model 9 does not yield any improvement, but does produce a much smaller embedding without loss in performance. This model uses exactly the same word class supervision Wtrain as models 6 and 7.\n\n\nFurther discussion and analysis\n\nAlthough the structures of models 8 and 9 are identical, the model using lcos hinge significantly outperforms its counterpart using l cos cos 2 . This is in line with the fact that a loss like lcos hinge, which optimizes embeddings based on relative distances between positive and negative pairs, is much more closely aligned with the discrimination task than a loss like l cos cos 2 , which looks at distances of word pairs in isolation (without regard to their distances relative to other pairs). The lcos hinge loss also allows more freedom in the model since it does not penalize same-word pairs (x1, x2) if they are already more similar by the margin m than the corresponding different-word pairs (x1, x3).\n\nThe closer match between the same-different task and the training loss lcos hinge could also explain the improvements over the DTW-based model 3 (both using exactly the same supervision Strain); this latter model aims to learn better features at the local frame level, but does 4 For model 6, embeddings are taken from the final softmax output. We also experimented with embeddings from the softmax layer but before applying the exponential normalization; this gave worse development results. so without regard to the (relative) similarities of complete segments. Figure 2 shows AP on the development set when varying the target dimensionalities of the different CNN-based approaches (see Section 2.4). The lcos hinge Siamese CNN outperforms all the other models (apart from the post-processed LDA model) at all operating points, and gives stable performance over a range of dimensionalities (300 and onwards). The word classifier CNN does much worse in this case (compared to the result of model 6), perhaps since embeddings here are not taken from the final layer, which is explicitly optimized for word classification, but from an intermediate layer. In contrast, for the Siamese CNNs, embeddings are always obtained directly from the layer that is optimized in the target loss. The figure also shows that when word labels Wtrain are available, compact embeddings can be obtained by performing LDA on top of the Siamese CNN representation, without loss in performance; this can prove to be important for downstream tasks which might require smaller embeddings.\n\nHere, a relatively small set of labelled word examples Ytrain is used to train the word classifier networks (as also done in the studies we compare to). In contrast, by using pairs of words and relative comparisons between them, a much larger set Strain is used for training Siamese networks. This type of paired supervision is ideal for generalizing to unseen word types, and is often easier to obtain in low-resource settings (see Section 2.3). While frame-level feature learning (model 3) can also use the larger pair-wise training set Strain, such approaches need to be coupled with DTW, which is limiting.\n\n\nCONCLUSION\n\nWe studied several acoustic word embedding approaches based on convolutional neural networks (CNNs); these networks take a wholeword speech segment as input and produce a fixed-dimensional vector. Our best new approach is a Siamese CNN that uses a hinge-based loss function to minimize the distance between word pairs of the same type relative to the distance between pairs of different types. On the same-different word discrimination task, this approach yields an average precision (AP) of 0.549, an improvement over the best previously published results on this task with whole-word embeddings (0.365 AP) and DTW with learned frame features (0.469 AP). A word classifier CNN performs similarly (0.532 AP) to the Siamese CNN, but requires much stronger labelled supervision, and performs worse at smaller dimensionalties. Future work will consider sequence models (e.g. RNNs, LSTMs), and will apply these embeddings to downstream tasks such as term discovery, speech recognition, and search.\n\n\nThis research was supported by NSF grant IIS-1321015. The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency. HK is funded by a Commonwealth Scholarship.\n\nFig. 1 .\n1(a) Word classification CNN and (b) word similarity Siamese CNN for obtaining acoustic word embeddings from padded speech input.\n\nFig. 2 .\n2Average precision (AP) on the development set for different CNN embedding approaches when varying the target dimensionality.\nWe also tried Euclidean distance, but as in[3,12,13], cosine worked better.\nIn[3], a slightly different training set was used. Nevertheless, the size of their training set is comparable to the set used here.3 CNN code: https://github.com/kamperh/couscous. Complete recipe: https://github.com/kamperh/recipe_swbd_wordembeds.\n\nTemplate-based continuous speech recognition. M De Wachter, M Matton, K Demuynck, P Wambacq, R Cools, D Van Compernolle, IEEE Trans. Audio, Speech, Language Process. 154M. De Wachter, M. Matton, K. Demuynck, P. Wambacq, R. Cools, and D. Van Compernolle, \"Template-based continu- ous speech recognition,\" IEEE Trans. Audio, Speech, Language Process., vol. 15, no. 4, pp. 1377-1390, 2007.\n\nInvestigations on exemplar-based features for speech recognition towards thousands of hours of unsupervised, noisy data. G Heigold, P Nguyen, M Weintraub, V Vanhoucke, Proc. ICASSP. ICASSPG. Heigold, P. Nguyen, M. Weintraub, and V. Vanhoucke, \"In- vestigations on exemplar-based features for speech recognition towards thousands of hours of unsupervised, noisy data,\" in Proc. ICASSP, 2012.\n\nFixeddimensional acoustic embeddings of variable-length segments in low-resource settings. K Levin, K Henry, A Jansen, K Livescu, Proc. ASRU. ASRUK. Levin, K. Henry, A. Jansen, and K. Livescu, \"Fixed- dimensional acoustic embeddings of variable-length segments in low-resource settings,\" in Proc. ASRU, 2013.\n\nWord embeddings for speech recognition. S Bengio, G Heigold, Proc. Interspeech. InterspeechS. Bengio and G. Heigold, \"Word embeddings for speech recognition,\" in Proc. Interspeech, 2014.\n\nQuery-by-example keyword spotting using long short-term memory networks. G Chen, C Parada, T N Sainath, Proc. ICASSP. ICASSPG. Chen, C. Parada, and T. N. Sainath, \"Query-by-example keyword spotting using long short-term memory networks,\" in Proc. ICASSP, 2015.\n\nWord-level acoustic modeling with convolutional vector regression. A L Maas, S D Miller, T M O&apos;neil, A Y Ng, P Nguyen, Proc. ICML Workshop Representation Learn. ICML Workshop Representation LearnA. L. Maas, S. D. Miller, T. M. O'neil, A. Y. Ng, and P. Nguyen, \"Word-level acoustic modeling with convolutional vector regres- sion,\" in Proc. ICML Workshop Representation Learn., 2012.\n\nGenerating hyperdimensional distributed representations from continuous-valued multivariate sensory input. O J R\u00e4s\u00e4nen, Proc. CogSci. CogSciO. J. R\u00e4s\u00e4nen, \"Generating hyperdimensional distributed repre- sentations from continuous-valued multivariate sensory input,\" in Proc. CogSci, 2015.\n\nConnected digit recognition using a level-building DTW algorithm. C S Myers, L R Rabiner, IEEE Trans. Acoust., Speech, Signal Process. 293C. S. Myers and L. R. Rabiner, \"Connected digit recognition using a level-building DTW algorithm,\" IEEE Trans. Acoust., Speech, Signal Process., vol. 29, no. 3, pp. 351-363, 1981.\n\nUnsupervised spoken keyword spotting via segmental DTW on Gaussian posteriorgrams. Y Zhang, J R Glass, Proc. ASRU. ASRUY. Zhang and J. R. Glass, \"Unsupervised spoken keyword spotting via segmental DTW on Gaussian posteriorgrams,\" in Proc. ASRU, 2009.\n\nResource configurable spoken query detection using deep Boltzmann machines. Y Zhang, R Salakhutdinov, H.-A Chang, J R Glass, Proc. ICASSP. ICASSPY. Zhang, R. Salakhutdinov, H.-A. Chang, and J. R. Glass, \"Re- source configurable spoken query detection using deep Boltz- mann machines,\" in Proc. ICASSP, 2012.\n\nWeak top-down constraints for unsupervised acoustic model training. A Jansen, S Thomas, H Hermansky, Proc. ICASSP. ICASSPA. Jansen, S. Thomas, and H. Hermansky, \"Weak top-down constraints for unsupervised acoustic model training,\" in Proc. ICASSP, 2013.\n\nUnsupervised neural network based feature extraction using weak top-down constraints. H Kamper, M Elsner, A Jansen, S J Goldwater, Proc. ICASSP. ICASSPH. Kamper, M. Elsner, A. Jansen, and S. J. Goldwater, \"Unsu- pervised neural network based feature extraction using weak top-down constraints,\" in Proc. ICASSP, 2015.\n\nA hybrid dynamic time warping-deep neural network architecture for unsupervised acoustic modeling. R Thiolli\u00e8re, E Dunbar, G Synnaeve, M Versteegh, E Dupoux, Proc. Interspeech. InterspeechR. Thiolli\u00e8re, E. Dunbar, G. Synnaeve, M. Versteegh, and E. Dupoux, \"A hybrid dynamic time warping-deep neural net- work architecture for unsupervised acoustic modeling,\" in Proc. Interspeech, 2015.\n\nConsiderations in dynamic time warping algorithms for discrete word recognition. L R Rabiner, A E Rosenberg, S E Levinson, IEEE Trans. Acoust., Speech, Signal Process. 266L. R. Rabiner, A. E. Rosenberg, and S. E. Levinson, \"Consid- erations in dynamic time warping algorithms for discrete word recognition,\" IEEE Trans. Acoust., Speech, Signal Process., vol. 26, no. 6, pp. 575-582, 1978.\n\nSegmental acoustic indexing for zero resource keyword search. K Levin, A Jansen, B Van Durme, Proc. ICASSP. ICASSPK. Levin, A. Jansen, and B. Van Durme, \"Segmental acoustic indexing for zero resource keyword search,\" in Proc. ICASSP, 2015.\n\nRapid evaluation of speech representations for spoken term discovery. M A Carlin, S Thomas, A Jansen, H Hermansky, Proc. Interspeech. InterspeechM. A. Carlin, S. Thomas, A. Jansen, and H. Hermansky, \"Rapid evaluation of speech representations for spoken term discovery,\" in Proc. Interspeech, 2011.\n\nSignature verification using a 'Siamese' time delay neural network. J Bromley, J W Bentz, L Bottou, I Guyon, Y Lecun, C Moore, E S\u00e4ckinger, R Shah, Int. J. Pattern Rec. 74J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun, C. Moore, E. S\u00e4ckinger, and R. Shah, \"Signature verification using a 'Siamese' time delay neural network,\" Int. J. Pattern Rec., vol. 7, no. 4, pp. 669-688, 1993.\n\nFully unsupervised small-vocabulary speech recognition using a segmental Bayesian model. H Kamper, A Jansen, S Goldwater, Proc. Interspeech. InterspeechH. Kamper, A. Jansen, and S. Goldwater, \"Fully unsuper- vised small-vocabulary speech recognition using a segmental Bayesian model,\" in Proc. Interspeech, 2015.\n\nA comparison of neural network methods for unsupervised representation learning on the Zero Resource Speech Challenge. D Renshaw, H Kamper, A Jansen, S J Goldwater, Proc. Interspeech. InterspeechD. Renshaw, H. Kamper, A. Jansen, and S. J. Goldwater, \"A comparison of neural network methods for unsupervised repre- sentation learning on the Zero Resource Speech Challenge,\" in Proc. Interspeech, 2015.\n\nUnsupervised pattern discovery in speech. A S Park, J R Glass, IEEE Trans. Audio, Speech, Language Process. 161A. S. Park and J. R. Glass, \"Unsupervised pattern discovery in speech,\" IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 1, pp. 186-197, 2008.\n\nEfficient spoken term discovery using randomized algorithms. A Jansen, B Van Durme, Proc. ASRU. ASRUA. Jansen and B. Van Durme, \"Efficient spoken term discovery using randomized algorithms,\" in Proc. ASRU, 2011.\n\nAn autoencoder based approach to unsupervised learning of subword units. L Badino, C Canevari, L Fadiga, G Metta, Proc. ICASSP. ICASSPL. Badino, C. Canevari, L. Fadiga, and G. Metta, \"An auto- encoder based approach to unsupervised learning of subword units,\" in Proc. ICASSP, 2014.\n\nLearning deep structured semantic models for web search using clickthrough data. P.-S Huang, X He, J Gao, L Deng, A Acero, L Heck, Proc. CIMK. CIMKP.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck, \"Learning deep structured semantic models for web search using clickthrough data,\" in Proc. CIMK, 2013.\n\nEfficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.3781arXiv preprintT. Mikolov, K. Chen, G. Corrado, and J. Dean, \"Efficient estimation of word representations in vector space,\" arXiv preprint arXiv:1301.3781, 2013.\n\nFrom paraphrase database to compositional paraphrase model and back. J Wieting, M Bansal, K Gimpel, K Livescu, Trans. ACL. 3J. Wieting, M. Bansal, K. Gimpel, and K. Livescu, \"From paraphrase database to compositional paraphrase model and back,\" Trans. ACL, vol. 3, pp. 345-358, 2015.\n\nDimensionality reduction by learning an invariant mapping. R Hadsell, S Chopra, Y Lecun, Proc. CVPR. CVPRR. Hadsell, S. Chopra, and Y. LeCun, \"Dimensionality reduc- tion by learning an invariant mapping,\" in Proc. CVPR, 2006.\n\nPhonetics embedding learning with side information. G Synnaeve, T Schatz, E Dupoux, Proc. SLT. SLTG. Synnaeve, T. Schatz, and E. Dupoux, \"Phonetics embedding learning with side information,\" in Proc. SLT, 2014.\n\nTheano: a CPU and GPU math expression compiler. J Bergstra, O Breuleux, F Bastien, P Lamblin, R Pascanu, G Desjardins, J Turian, D Warde-Farley, Y Bengio, Proc. SciPy. SciPyJ. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio, \"Theano: a CPU and GPU math expression compiler,\" in Proc. SciPy, 2010.\n\nADADELTA: An adaptive learning rate method. M D Zeiler, arXiv:1212.5701arXiv preprintM. D. Zeiler, \"ADADELTA: An adaptive learning rate method,\" arXiv preprint arXiv:1212.5701, 2012.\n", "annotations": {"author": "[{\"end\":180,\"start\":80},{\"end\":260,\"start\":181},{\"end\":340,\"start\":261}]", "publisher": null, "author_last_name": "[{\"end\":93,\"start\":87},{\"end\":192,\"start\":188},{\"end\":274,\"start\":267}]", "author_first_name": "[{\"end\":86,\"start\":80},{\"end\":187,\"start\":181},{\"end\":266,\"start\":261}]", "author_affiliation": "[{\"end\":179,\"start\":117},{\"end\":259,\"start\":214},{\"end\":339,\"start\":294}]", "title": "[{\"end\":77,\"start\":1},{\"end\":417,\"start\":341}]", "venue": null, "abstract": "[{\"end\":1566,\"start\":419}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1912,\"start\":1909},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1915,\"start\":1912},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1918,\"start\":1915},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1921,\"start\":1918},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1924,\"start\":1921},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1927,\"start\":1924},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1930,\"start\":1927},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2107,\"start\":2104},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2209,\"start\":2206},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2211,\"start\":2209},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2295,\"start\":2292},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2298,\"start\":2295},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2614,\"start\":2610},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2618,\"start\":2614},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2622,\"start\":2618},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2665,\"start\":2661},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2738,\"start\":2735},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3100,\"start\":3097},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3113,\"start\":3109},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3205,\"start\":3202},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3618,\"start\":3615},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3766,\"start\":3763},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4054,\"start\":4051},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4140,\"start\":4136},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4143,\"start\":4140},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4146,\"start\":4143},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4618,\"start\":4614},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4753,\"start\":4750},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6565,\"start\":6562},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7064,\"start\":7060},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7101,\"start\":7097},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7239,\"start\":7236},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7501,\"start\":7498},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8248,\"start\":8245},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9219,\"start\":9216},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10078,\"start\":10074},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10082,\"start\":10078},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10086,\"start\":10082},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10089,\"start\":10086},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10397,\"start\":10393},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10400,\"start\":10397},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10549,\"start\":10545},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10552,\"start\":10549},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10583,\"start\":10579},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10587,\"start\":10583},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10591,\"start\":10587},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10620,\"start\":10616},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10851,\"start\":10847},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11610,\"start\":11606},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11658,\"start\":11654},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11661,\"start\":11658},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12141,\"start\":12137},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12144,\"start\":12141},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14122,\"start\":14118},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15527,\"start\":15523},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15530,\"start\":15527},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15851,\"start\":15848},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15854,\"start\":15851},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15857,\"start\":15854},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15948,\"start\":15947},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16125,\"start\":16121},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16128,\"start\":16125},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16401,\"start\":16397},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16496,\"start\":16492},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16587,\"start\":16583},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16656,\"start\":16655},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16695,\"start\":16691},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18896,\"start\":18893},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19701,\"start\":19700},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22032,\"start\":22031},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25484,\"start\":25481},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25487,\"start\":25484},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25490,\"start\":25487},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25519,\"start\":25516},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25646,\"start\":25645}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25161,\"start\":24937},{\"attributes\":{\"id\":\"fig_2\"},\"end\":25301,\"start\":25162},{\"attributes\":{\"id\":\"fig_3\"},\"end\":25437,\"start\":25302}]", "paragraph": "[{\"end\":2004,\"start\":1582},{\"end\":2720,\"start\":2006},{\"end\":3940,\"start\":2722},{\"end\":5022,\"start\":3942},{\"end\":5741,\"start\":5061},{\"end\":6352,\"start\":5743},{\"end\":7422,\"start\":6490},{\"end\":8119,\"start\":7450},{\"end\":9116,\"start\":8121},{\"end\":9924,\"start\":9118},{\"end\":10442,\"start\":9957},{\"end\":11662,\"start\":10444},{\"end\":12145,\"start\":11744},{\"end\":12605,\"start\":12203},{\"end\":12800,\"start\":12607},{\"end\":13723,\"start\":12841},{\"end\":14308,\"start\":13775},{\"end\":15131,\"start\":14310},{\"end\":16077,\"start\":15133},{\"end\":16538,\"start\":16079},{\"end\":17167,\"start\":16564},{\"end\":18535,\"start\":17169},{\"end\":19014,\"start\":18547},{\"end\":19670,\"start\":19016},{\"end\":19805,\"start\":19691},{\"end\":20071,\"start\":19807},{\"end\":21004,\"start\":20073},{\"end\":21751,\"start\":21040},{\"end\":23316,\"start\":21753},{\"end\":23928,\"start\":23318},{\"end\":24936,\"start\":23943}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6462,\"start\":6353},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11743,\"start\":11663},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12202,\"start\":12146}]", "table_ref": "[{\"end\":18409,\"start\":18402},{\"end\":19346,\"start\":19339}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1580,\"start\":1568},{\"attributes\":{\"n\":\"2.\"},\"end\":5059,\"start\":5025},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6488,\"start\":6464},{\"attributes\":{\"n\":\"2.2.\"},\"end\":7448,\"start\":7425},{\"attributes\":{\"n\":\"2.3.\"},\"end\":9955,\"start\":9927},{\"attributes\":{\"n\":\"2.4.\"},\"end\":12839,\"start\":12803},{\"attributes\":{\"n\":\"3.\"},\"end\":13737,\"start\":13726},{\"attributes\":{\"n\":\"3.1.\"},\"end\":13773,\"start\":13740},{\"attributes\":{\"n\":\"3.2.\"},\"end\":16562,\"start\":16541},{\"attributes\":{\"n\":\"3.3.\"},\"end\":18545,\"start\":18538},{\"end\":19689,\"start\":19673},{\"attributes\":{\"n\":\"3.4.\"},\"end\":21038,\"start\":21007},{\"attributes\":{\"n\":\"4.\"},\"end\":23941,\"start\":23931},{\"end\":25171,\"start\":25163},{\"end\":25311,\"start\":25303}]", "table": null, "figure_caption": "[{\"end\":25161,\"start\":24939},{\"end\":25301,\"start\":25173},{\"end\":25437,\"start\":25313}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":8551,\"start\":8543},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10996,\"start\":10988},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22325,\"start\":22317}]", "bib_author_first_name": "[{\"end\":25810,\"start\":25809},{\"end\":25813,\"start\":25811},{\"end\":25824,\"start\":25823},{\"end\":25834,\"start\":25833},{\"end\":25846,\"start\":25845},{\"end\":25857,\"start\":25856},{\"end\":25866,\"start\":25865},{\"end\":26273,\"start\":26272},{\"end\":26284,\"start\":26283},{\"end\":26294,\"start\":26293},{\"end\":26307,\"start\":26306},{\"end\":26635,\"start\":26634},{\"end\":26644,\"start\":26643},{\"end\":26653,\"start\":26652},{\"end\":26663,\"start\":26662},{\"end\":26894,\"start\":26893},{\"end\":26904,\"start\":26903},{\"end\":27115,\"start\":27114},{\"end\":27123,\"start\":27122},{\"end\":27133,\"start\":27132},{\"end\":27135,\"start\":27134},{\"end\":27371,\"start\":27370},{\"end\":27373,\"start\":27372},{\"end\":27381,\"start\":27380},{\"end\":27383,\"start\":27382},{\"end\":27393,\"start\":27392},{\"end\":27395,\"start\":27394},{\"end\":27410,\"start\":27409},{\"end\":27412,\"start\":27411},{\"end\":27418,\"start\":27417},{\"end\":27800,\"start\":27799},{\"end\":27802,\"start\":27801},{\"end\":28049,\"start\":28048},{\"end\":28051,\"start\":28050},{\"end\":28060,\"start\":28059},{\"end\":28062,\"start\":28061},{\"end\":28385,\"start\":28384},{\"end\":28394,\"start\":28393},{\"end\":28396,\"start\":28395},{\"end\":28630,\"start\":28629},{\"end\":28639,\"start\":28638},{\"end\":28659,\"start\":28655},{\"end\":28668,\"start\":28667},{\"end\":28670,\"start\":28669},{\"end\":28931,\"start\":28930},{\"end\":28941,\"start\":28940},{\"end\":28951,\"start\":28950},{\"end\":29204,\"start\":29203},{\"end\":29214,\"start\":29213},{\"end\":29224,\"start\":29223},{\"end\":29234,\"start\":29233},{\"end\":29236,\"start\":29235},{\"end\":29536,\"start\":29535},{\"end\":29550,\"start\":29549},{\"end\":29560,\"start\":29559},{\"end\":29572,\"start\":29571},{\"end\":29585,\"start\":29584},{\"end\":29906,\"start\":29905},{\"end\":29908,\"start\":29907},{\"end\":29919,\"start\":29918},{\"end\":29921,\"start\":29920},{\"end\":29934,\"start\":29933},{\"end\":29936,\"start\":29935},{\"end\":30277,\"start\":30276},{\"end\":30286,\"start\":30285},{\"end\":30296,\"start\":30295},{\"end\":30526,\"start\":30525},{\"end\":30528,\"start\":30527},{\"end\":30538,\"start\":30537},{\"end\":30548,\"start\":30547},{\"end\":30558,\"start\":30557},{\"end\":30824,\"start\":30823},{\"end\":30835,\"start\":30834},{\"end\":30837,\"start\":30836},{\"end\":30846,\"start\":30845},{\"end\":30856,\"start\":30855},{\"end\":30865,\"start\":30864},{\"end\":30874,\"start\":30873},{\"end\":30883,\"start\":30882},{\"end\":30896,\"start\":30895},{\"end\":31236,\"start\":31235},{\"end\":31246,\"start\":31245},{\"end\":31256,\"start\":31255},{\"end\":31580,\"start\":31579},{\"end\":31591,\"start\":31590},{\"end\":31601,\"start\":31600},{\"end\":31611,\"start\":31610},{\"end\":31613,\"start\":31612},{\"end\":31905,\"start\":31904},{\"end\":31907,\"start\":31906},{\"end\":31915,\"start\":31914},{\"end\":31917,\"start\":31916},{\"end\":32189,\"start\":32188},{\"end\":32199,\"start\":32198},{\"end\":32414,\"start\":32413},{\"end\":32424,\"start\":32423},{\"end\":32436,\"start\":32435},{\"end\":32446,\"start\":32445},{\"end\":32709,\"start\":32705},{\"end\":32718,\"start\":32717},{\"end\":32724,\"start\":32723},{\"end\":32731,\"start\":32730},{\"end\":32739,\"start\":32738},{\"end\":32748,\"start\":32747},{\"end\":32999,\"start\":32998},{\"end\":33010,\"start\":33009},{\"end\":33018,\"start\":33017},{\"end\":33029,\"start\":33028},{\"end\":33284,\"start\":33283},{\"end\":33295,\"start\":33294},{\"end\":33305,\"start\":33304},{\"end\":33315,\"start\":33314},{\"end\":33559,\"start\":33558},{\"end\":33570,\"start\":33569},{\"end\":33580,\"start\":33579},{\"end\":33779,\"start\":33778},{\"end\":33791,\"start\":33790},{\"end\":33801,\"start\":33800},{\"end\":33987,\"start\":33986},{\"end\":33999,\"start\":33998},{\"end\":34011,\"start\":34010},{\"end\":34022,\"start\":34021},{\"end\":34033,\"start\":34032},{\"end\":34044,\"start\":34043},{\"end\":34058,\"start\":34057},{\"end\":34068,\"start\":34067},{\"end\":34084,\"start\":34083},{\"end\":34349,\"start\":34348},{\"end\":34351,\"start\":34350}]", "bib_author_last_name": "[{\"end\":25821,\"start\":25814},{\"end\":25831,\"start\":25825},{\"end\":25843,\"start\":25835},{\"end\":25854,\"start\":25847},{\"end\":25863,\"start\":25858},{\"end\":25882,\"start\":25867},{\"end\":26281,\"start\":26274},{\"end\":26291,\"start\":26285},{\"end\":26304,\"start\":26295},{\"end\":26317,\"start\":26308},{\"end\":26641,\"start\":26636},{\"end\":26650,\"start\":26645},{\"end\":26660,\"start\":26654},{\"end\":26671,\"start\":26664},{\"end\":26901,\"start\":26895},{\"end\":26912,\"start\":26905},{\"end\":27120,\"start\":27116},{\"end\":27130,\"start\":27124},{\"end\":27143,\"start\":27136},{\"end\":27378,\"start\":27374},{\"end\":27390,\"start\":27384},{\"end\":27407,\"start\":27396},{\"end\":27415,\"start\":27413},{\"end\":27425,\"start\":27419},{\"end\":27810,\"start\":27803},{\"end\":28057,\"start\":28052},{\"end\":28070,\"start\":28063},{\"end\":28391,\"start\":28386},{\"end\":28402,\"start\":28397},{\"end\":28636,\"start\":28631},{\"end\":28653,\"start\":28640},{\"end\":28665,\"start\":28660},{\"end\":28676,\"start\":28671},{\"end\":28938,\"start\":28932},{\"end\":28948,\"start\":28942},{\"end\":28961,\"start\":28952},{\"end\":29211,\"start\":29205},{\"end\":29221,\"start\":29215},{\"end\":29231,\"start\":29225},{\"end\":29246,\"start\":29237},{\"end\":29547,\"start\":29537},{\"end\":29557,\"start\":29551},{\"end\":29569,\"start\":29561},{\"end\":29582,\"start\":29573},{\"end\":29592,\"start\":29586},{\"end\":29916,\"start\":29909},{\"end\":29931,\"start\":29922},{\"end\":29945,\"start\":29937},{\"end\":30283,\"start\":30278},{\"end\":30293,\"start\":30287},{\"end\":30306,\"start\":30297},{\"end\":30535,\"start\":30529},{\"end\":30545,\"start\":30539},{\"end\":30555,\"start\":30549},{\"end\":30568,\"start\":30559},{\"end\":30832,\"start\":30825},{\"end\":30843,\"start\":30838},{\"end\":30853,\"start\":30847},{\"end\":30862,\"start\":30857},{\"end\":30871,\"start\":30866},{\"end\":30880,\"start\":30875},{\"end\":30893,\"start\":30884},{\"end\":30901,\"start\":30897},{\"end\":31243,\"start\":31237},{\"end\":31253,\"start\":31247},{\"end\":31266,\"start\":31257},{\"end\":31588,\"start\":31581},{\"end\":31598,\"start\":31592},{\"end\":31608,\"start\":31602},{\"end\":31623,\"start\":31614},{\"end\":31912,\"start\":31908},{\"end\":31923,\"start\":31918},{\"end\":32196,\"start\":32190},{\"end\":32209,\"start\":32200},{\"end\":32421,\"start\":32415},{\"end\":32433,\"start\":32425},{\"end\":32443,\"start\":32437},{\"end\":32452,\"start\":32447},{\"end\":32715,\"start\":32710},{\"end\":32721,\"start\":32719},{\"end\":32728,\"start\":32725},{\"end\":32736,\"start\":32732},{\"end\":32745,\"start\":32740},{\"end\":32753,\"start\":32749},{\"end\":33007,\"start\":33000},{\"end\":33015,\"start\":33011},{\"end\":33026,\"start\":33019},{\"end\":33034,\"start\":33030},{\"end\":33292,\"start\":33285},{\"end\":33302,\"start\":33296},{\"end\":33312,\"start\":33306},{\"end\":33323,\"start\":33316},{\"end\":33567,\"start\":33560},{\"end\":33577,\"start\":33571},{\"end\":33586,\"start\":33581},{\"end\":33788,\"start\":33780},{\"end\":33798,\"start\":33792},{\"end\":33808,\"start\":33802},{\"end\":33996,\"start\":33988},{\"end\":34008,\"start\":34000},{\"end\":34019,\"start\":34012},{\"end\":34030,\"start\":34023},{\"end\":34041,\"start\":34034},{\"end\":34055,\"start\":34045},{\"end\":34065,\"start\":34059},{\"end\":34081,\"start\":34069},{\"end\":34091,\"start\":34085},{\"end\":34358,\"start\":34352}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1819708},\"end\":26149,\"start\":25763},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2209533},\"end\":26541,\"start\":26151},{\"attributes\":{\"id\":\"b2\"},\"end\":26851,\"start\":26543},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":9299470},\"end\":27039,\"start\":26853},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":772349},\"end\":27301,\"start\":27041},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":9677213},\"end\":27690,\"start\":27303},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":15600360},\"end\":27980,\"start\":27692},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6192941},\"end\":28299,\"start\":27982},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":5866530},\"end\":28551,\"start\":28301},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":509115},\"end\":28860,\"start\":28553},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":824237},\"end\":29115,\"start\":28862},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":14021296},\"end\":29434,\"start\":29117},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":7154538},\"end\":29822,\"start\":29436},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":15817246},\"end\":30212,\"start\":29824},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2646247},\"end\":30453,\"start\":30214},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":12998642},\"end\":30753,\"start\":30455},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":16394033},\"end\":31144,\"start\":30755},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":14378404},\"end\":31458,\"start\":31146},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":21812454},\"end\":31860,\"start\":31460},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5344879},\"end\":32125,\"start\":31862},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":17498735},\"end\":32338,\"start\":32127},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":11231368},\"end\":32622,\"start\":32340},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":8384258},\"end\":32934,\"start\":32624},{\"attributes\":{\"doi\":\"arXiv:1301.3781\",\"id\":\"b23\"},\"end\":33212,\"start\":32936},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":57564106},\"end\":33497,\"start\":33214},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":8281592},\"end\":33724,\"start\":33499},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":2645231},\"end\":33936,\"start\":33726},{\"attributes\":{\"id\":\"b27\"},\"end\":34302,\"start\":33938},{\"attributes\":{\"doi\":\"arXiv:1212.5701\",\"id\":\"b28\"},\"end\":34486,\"start\":34304}]", "bib_title": "[{\"end\":25807,\"start\":25763},{\"end\":26270,\"start\":26151},{\"end\":26632,\"start\":26543},{\"end\":26891,\"start\":26853},{\"end\":27112,\"start\":27041},{\"end\":27368,\"start\":27303},{\"end\":27797,\"start\":27692},{\"end\":28046,\"start\":27982},{\"end\":28382,\"start\":28301},{\"end\":28627,\"start\":28553},{\"end\":28928,\"start\":28862},{\"end\":29201,\"start\":29117},{\"end\":29533,\"start\":29436},{\"end\":29903,\"start\":29824},{\"end\":30274,\"start\":30214},{\"end\":30523,\"start\":30455},{\"end\":30821,\"start\":30755},{\"end\":31233,\"start\":31146},{\"end\":31577,\"start\":31460},{\"end\":31902,\"start\":31862},{\"end\":32186,\"start\":32127},{\"end\":32411,\"start\":32340},{\"end\":32703,\"start\":32624},{\"end\":33281,\"start\":33214},{\"end\":33556,\"start\":33499},{\"end\":33776,\"start\":33726},{\"end\":33984,\"start\":33938}]", "bib_author": "[{\"end\":25823,\"start\":25809},{\"end\":25833,\"start\":25823},{\"end\":25845,\"start\":25833},{\"end\":25856,\"start\":25845},{\"end\":25865,\"start\":25856},{\"end\":25884,\"start\":25865},{\"end\":26283,\"start\":26272},{\"end\":26293,\"start\":26283},{\"end\":26306,\"start\":26293},{\"end\":26319,\"start\":26306},{\"end\":26643,\"start\":26634},{\"end\":26652,\"start\":26643},{\"end\":26662,\"start\":26652},{\"end\":26673,\"start\":26662},{\"end\":26903,\"start\":26893},{\"end\":26914,\"start\":26903},{\"end\":27122,\"start\":27114},{\"end\":27132,\"start\":27122},{\"end\":27145,\"start\":27132},{\"end\":27380,\"start\":27370},{\"end\":27392,\"start\":27380},{\"end\":27409,\"start\":27392},{\"end\":27417,\"start\":27409},{\"end\":27427,\"start\":27417},{\"end\":27812,\"start\":27799},{\"end\":28059,\"start\":28048},{\"end\":28072,\"start\":28059},{\"end\":28393,\"start\":28384},{\"end\":28404,\"start\":28393},{\"end\":28638,\"start\":28629},{\"end\":28655,\"start\":28638},{\"end\":28667,\"start\":28655},{\"end\":28678,\"start\":28667},{\"end\":28940,\"start\":28930},{\"end\":28950,\"start\":28940},{\"end\":28963,\"start\":28950},{\"end\":29213,\"start\":29203},{\"end\":29223,\"start\":29213},{\"end\":29233,\"start\":29223},{\"end\":29248,\"start\":29233},{\"end\":29549,\"start\":29535},{\"end\":29559,\"start\":29549},{\"end\":29571,\"start\":29559},{\"end\":29584,\"start\":29571},{\"end\":29594,\"start\":29584},{\"end\":29918,\"start\":29905},{\"end\":29933,\"start\":29918},{\"end\":29947,\"start\":29933},{\"end\":30285,\"start\":30276},{\"end\":30295,\"start\":30285},{\"end\":30308,\"start\":30295},{\"end\":30537,\"start\":30525},{\"end\":30547,\"start\":30537},{\"end\":30557,\"start\":30547},{\"end\":30570,\"start\":30557},{\"end\":30834,\"start\":30823},{\"end\":30845,\"start\":30834},{\"end\":30855,\"start\":30845},{\"end\":30864,\"start\":30855},{\"end\":30873,\"start\":30864},{\"end\":30882,\"start\":30873},{\"end\":30895,\"start\":30882},{\"end\":30903,\"start\":30895},{\"end\":31245,\"start\":31235},{\"end\":31255,\"start\":31245},{\"end\":31268,\"start\":31255},{\"end\":31590,\"start\":31579},{\"end\":31600,\"start\":31590},{\"end\":31610,\"start\":31600},{\"end\":31625,\"start\":31610},{\"end\":31914,\"start\":31904},{\"end\":31925,\"start\":31914},{\"end\":32198,\"start\":32188},{\"end\":32211,\"start\":32198},{\"end\":32423,\"start\":32413},{\"end\":32435,\"start\":32423},{\"end\":32445,\"start\":32435},{\"end\":32454,\"start\":32445},{\"end\":32717,\"start\":32705},{\"end\":32723,\"start\":32717},{\"end\":32730,\"start\":32723},{\"end\":32738,\"start\":32730},{\"end\":32747,\"start\":32738},{\"end\":32755,\"start\":32747},{\"end\":33009,\"start\":32998},{\"end\":33017,\"start\":33009},{\"end\":33028,\"start\":33017},{\"end\":33036,\"start\":33028},{\"end\":33294,\"start\":33283},{\"end\":33304,\"start\":33294},{\"end\":33314,\"start\":33304},{\"end\":33325,\"start\":33314},{\"end\":33569,\"start\":33558},{\"end\":33579,\"start\":33569},{\"end\":33588,\"start\":33579},{\"end\":33790,\"start\":33778},{\"end\":33800,\"start\":33790},{\"end\":33810,\"start\":33800},{\"end\":33998,\"start\":33986},{\"end\":34010,\"start\":33998},{\"end\":34021,\"start\":34010},{\"end\":34032,\"start\":34021},{\"end\":34043,\"start\":34032},{\"end\":34057,\"start\":34043},{\"end\":34067,\"start\":34057},{\"end\":34083,\"start\":34067},{\"end\":34093,\"start\":34083},{\"end\":34360,\"start\":34348}]", "bib_venue": "[{\"end\":25927,\"start\":25884},{\"end\":26331,\"start\":26319},{\"end\":26683,\"start\":26673},{\"end\":26931,\"start\":26914},{\"end\":27157,\"start\":27145},{\"end\":27467,\"start\":27427},{\"end\":27824,\"start\":27812},{\"end\":28115,\"start\":28072},{\"end\":28414,\"start\":28404},{\"end\":28690,\"start\":28678},{\"end\":28975,\"start\":28963},{\"end\":29260,\"start\":29248},{\"end\":29611,\"start\":29594},{\"end\":29990,\"start\":29947},{\"end\":30320,\"start\":30308},{\"end\":30587,\"start\":30570},{\"end\":30922,\"start\":30903},{\"end\":31285,\"start\":31268},{\"end\":31642,\"start\":31625},{\"end\":31968,\"start\":31925},{\"end\":32221,\"start\":32211},{\"end\":32466,\"start\":32454},{\"end\":32765,\"start\":32755},{\"end\":32996,\"start\":32936},{\"end\":33335,\"start\":33325},{\"end\":33598,\"start\":33588},{\"end\":33819,\"start\":33810},{\"end\":34104,\"start\":34093},{\"end\":34346,\"start\":34304},{\"end\":26339,\"start\":26333},{\"end\":26689,\"start\":26685},{\"end\":26944,\"start\":26933},{\"end\":27165,\"start\":27159},{\"end\":27503,\"start\":27469},{\"end\":27832,\"start\":27826},{\"end\":28420,\"start\":28416},{\"end\":28698,\"start\":28692},{\"end\":28983,\"start\":28977},{\"end\":29268,\"start\":29262},{\"end\":29624,\"start\":29613},{\"end\":30328,\"start\":30322},{\"end\":30600,\"start\":30589},{\"end\":31298,\"start\":31287},{\"end\":31655,\"start\":31644},{\"end\":32227,\"start\":32223},{\"end\":32474,\"start\":32468},{\"end\":32771,\"start\":32767},{\"end\":33604,\"start\":33600},{\"end\":33824,\"start\":33821},{\"end\":34111,\"start\":34106}]"}}}, "year": 2023, "month": 12, "day": 17}