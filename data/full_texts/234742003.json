{"id": 234742003, "updated": "2023-12-14 07:25:31.565", "metadata": {"title": "Towards a Better Tradeoff between Effectiveness and Efficiency in Pre-Ranking: A Learnable Feature Selection based Approach", "authors": "[{\"first\":\"Xu\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Pengjie\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Hui\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Shaoguo\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Chuhan\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Kuang-Chih\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Jian\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Bo\",\"last\":\"Zheng\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "In real-world search, recommendation, and advertising systems, the multi-stage ranking architecture is commonly adopted. Such architecture usually consists of matching, pre-ranking, ranking, and re-ranking stages. In the pre-ranking stage, vector-product based models with representation-focused architecture are commonly adopted to account for system efficiency. However, it brings a significant loss to the effectiveness of the system. In this paper, a novel pre-ranking approach is proposed which supports complicated models with interaction-focused architecture. It achieves a better tradeoff between effectiveness and efficiency by utilizing the proposed learnable Feature Selection method based on feature Complexity and variational Dropout (FSCD). Evaluations in a real-world e-commerce sponsored search system for a search engine demonstrate that utilizing the proposed pre-ranking, the effectiveness of the system is significantly improved. Moreover, compared to the systems with conventional pre-ranking models, an identical amount of computational resource is consumed.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/sigir/MaWZLZLLXZ21", "doi": "10.1145/3404835.3462979"}}, "content": {"source": {"pdf_hash": "c47c47bf5d00100f73f33435dc116172cf250aae", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2105.07706v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2105.07706", "status": "GREEN"}}, "grobid": {"id": "64a9aeafda1619245d622e53da4c348c4f56cdaf", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c47c47bf5d00100f73f33435dc116172cf250aae.txt", "contents": "\nTowards a Better Tradeoff between Effectiveness and Efficiency in Pre-Ranking: A Learnable Feature Selection based Approach\nVirtual Event, CanadaCopyright Virtual Event, Canada2021. July 11-15, 2021. July 11-15, 2021\n\nXu Ma \nAlibaba Group\n\n\nPengjie Wang \nAlibaba Group\n\n\nHui Zhao \nAlibaba Group\n\n\nShaoguo Liu \nAlibaba Group\n\n\nChuhan Zhao \nAlibaba Group\n\n\nWei Lin \nAlibaba Group\n\n\nKuang-Chih Lee \nAlibaba Group\n\n\nJian Xu \nAlibaba Group\n\n\nBo Zheng bozheng@alibaba-inc.com \nAlibaba Group\n\n\nXu Ma \nAlibaba Group\n\n\nPengjie Wang \nAlibaba Group\n\n\nHui Zhao \nAlibaba Group\n\n\nShaoguo Liu \nAlibaba Group\n\n\nChuhan Zhao \nAlibaba Group\n\n\nWei Lin \nAlibaba Group\n\n\nKuang-Chih Lee \nAlibaba Group\n\n\nJian Xu \nAlibaba Group\n\n\nBo Zheng \nAlibaba Group\n\n\nTowards a Better Tradeoff between Effectiveness and Efficiency in Pre-Ranking: A Learnable Feature Selection based Approach\n\nProceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '21)\nthe 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '21)New York, NY, USA,Virtual Event, Canada2021. July 11-15, 2021. July 11-15, 202110.1145/3404835.3462979ACM Reference Format: \u2020 Joint first authors. \u2021 This author gave a lot of guidance in the work. * Corresponding author. Project funded by China Postdoctoral Science Foundation (2019TQ0290). ACM ISBN 978-1-4503-8037-9/21/07. . . $15.00pre-rankingeffectivenessefficiencyfeature selection\nIn real-world search, recommendation, and advertising systems, the multi-stage ranking architecture is commonly adopted. Such architecture usually consists of matching, pre-ranking, ranking, and re-ranking stages. In the pre-ranking stage, vector-product based models with representation-focused architecture are commonly adopted to account for system efficiency. However, it brings a significant loss to the effectiveness of the system. In this paper, a novel pre-ranking approach is proposed which supports complicated models with interaction-focused architecture. It achieves a better tradeoff between effectiveness and efficiency by utilizing the proposed learnable Feature Selection method based on feature Complexity and variational Dropout (FSCD). Evaluations in a realworld e-commerce sponsored search system for a search engine demonstrate that utilizing the proposed pre-ranking, the effectiveness of the system is significantly improved. Moreover, compared to the systems with conventional pre-ranking models, an identical amount of computational resource is consumed.CCS CONCEPTS\u2022 Information systems \u2192 Learning to rank.\n\nINTRODUCTION\n\nAs important internet services, large-scale search engine and recommendation systems play important roles in information retrieval and item recommendation, where a ranking system selects only a few items from tens of millions of candidates. Under the constraint of extremely low system latency, a single complicated ranking model cannot rank the entire candidate set. Therefore, multistage ranking architecture is commonly adopted [2,7,9] (shown in Figure 1 (a)). Large-scale deep neural networks (DNNs) with interaction-focused architecture [3] are usually employed for the ranking model to maintain good system performance, while only less complicated models with representation-focused architecture [3] are adopted in pre-ranking to ensure efficiency.\n\nHowever, simple pre-ranking models with representation-focused architecture will inevitably diminish the model expression ability. Vector-product based model [4,13], which is classified as representationfocused architecture, is often employed in pre-ranking (see Figure 1 (b)). The limitation is that neither explicit interactive features nor implicit interactive semantics, which are less efficient for computation but very effective for the expression ability [11], can be used. The pre-ranking models with representation-focused architecture focus excessively on the efficiency optimization and remain a large effectiveness gap to models with interaction-focused architecture. Moreover, as the online feature generation process consume as much resource as that needed for the online inference of the model, the utilization for the pre-ranking with interaction-focused architecture is possible by using feature selection considering both effectiveness and efficiency. In this context, the pre-ranking with an interaction-focused architecture is studied in this paper.\n\nIn this paper, a pre-ranking model with tradeoff of both effectiveness and efficiency is proposed for a real-world e-commerce application, where the effectiveness improves significantly with slight decrease of efficiency shown in Figure 1 (b). The contributions of this paper are summarized as follows. 1) A pre-ranking model with interaction-focused architecture is proposed by inheriting the architecture of ranking model to solve the problem of performance loss of the model with representation-focused architecture. 2) A learnable Feature Selection method based on feature Complexity and variational Dropout (FSCD) is proposed to search for a set of effective and efficient feature fields for the pre-ranking model. 3) Extensive experiments are carried out which show the proposed approach achieves great improvement in effectiveness of the system compared to the conventional baselines, while the efficiency of the system is also maintained. Multi-stage architecture including matching, pre-ranking, ranking, and re-ranking with scoring item numbers. (b) Intuitive view for effectiveness and efficiency of pre-ranking and ranking, where the pre-ranking is derived from ranking and optimized to an interaction-focused architecture.\n\n\nTHE PROPOSED APPROACH\n\nThe proposed pre-ranking is derived from the ranking model. Both models utilize interaction-focused architectures, which shares an identical feature set = { 1 , 2 , ..., }, where is the -th feature field in , and is the number of feature fields. In the system, the ranking model employs all feature fields in to maintain effectiveness, while the pre-ranking model utilizes only a subset to reduce computational complexity and scores more items. In this context, offline processes such as sample generation can be reused for both models, which saves offline computational resources. The interaction-focused architecture of pre-ranking is inherited from that of ranking model, where both explicit interactive features and implicit interactive semantics can be utilized, which reduces the gap in the models' optimization objectives and achieves significant improvement of the model effectiveness for pre-ranking compared to the model with representation-focused architecture.\n\nFor introducing interaction-focused architecture into pre-ranking, a great challenge is the model efficiency. The overall efficiency of the pre-ranking model, whose learnable variables can be divided into feature embeddings and dense weights , is strongly influenced by the feature embeddings . For example, in real-world scenarios, storage for feature embeddings exceeds over 95% of that for the whole model, while feature generation process for embeddings consumes as much resources as that needed for the online inference of the model. Therefore, the feature selection for features with both high effectiveness and efficiency is important for the utilization of pre-ranking with interaction-focused architecture.\n\n\nFSCD for Pre-Ranking Model\n\nInspired by the Dropout FR method based on the variational dropout layer [1] where the efficiency of the model is ignored, FSCD is proposed that considers both effectiveness and efficiency in a learnable process as illustrated in Figure 2. In the proposed FSCD, the effectiveness is optimized by the cross entropy based loss function, while the efficiency is optimized by the feature-wise regularization term in Eq. (3). Both effective and efficient features can be selected by FSCD in one single training process, while the expression ability of pre-ranking model is improved utilizing these features compared to the vector-product based model. The details are as follows. To select feature fields with both high effectiveness and high efficiency, each feature field is expected to learn a dropout parameter \u2208 {0, 1} to indicate whether the feature field is dropped ( = 0) or preserved ( = 1). 's embeddings are multiplied by the dropout to form the embedding layer, and is subject to a Bernoulli distribution parameterized by , i.e.,\n\u223c Bern( ),(1)\nwhere the hyperparameter is the priori probability for the preservation of feature field and is configured as function of feature complexity , i.e.,\n= H ( ) = 1 \u2212 ( ), = G( , , )(2)\nwhere (\u00b7) is the sigmoid function. = 1 \u2212 ( ) is one of alternatives to relate and which works well in practice. The feature complexity measures the computational and storage complexity of the -th feature field including but not limited to the online computational complexity , the embedding dimension , and the number of keys for one feature field, where is configured according to the feature type specified in Section 3.1. Linear combination = 1 + 2 + 3 is one of the alternatives with hyperparameter 1,2,3 . According to Eq. (2), a feature with large complexity has a small value of , and vice versa.\n\nGiven the training samples D = {( , )| = 1, 2, ..., }, where is the number of samples, the overall loss function for the learnable feature selection is derived by Bayesian rule [10] as,\n( , , ) = \u2212 1 log (D| , , ) + (|| || 2 +|| || 2 ) + \u2211\ufe01 =1 ,(3)\nwhere is the regularization weight for and can be derived as (See derivation details in Appendix A)\n= log(1 \u2212 ) \u2212 log( ).(4)\nis a function that decreases with and increases with . Therefore, a feature with larger complexity is penalized with a larger value of , and more likely to be dropped. In this way, the feature complexity is included in the proposed FSCD, which previous works do not address to the best of the authors' knowledge.\n\nSubject to Bernoulli distribution, is discrete and not differentiable, it is relaxed to a differentiable function as [5] \n= F ( ) = ( 1 (log( ) \u2212 log(1 \u2212 ) + log( ) \u2212 log(1 \u2212 ))),(5)\nwhere \u223c Uniform(0, 1) is subject to a uniform distribution and changes during the training process, while = 0.1 is a constant that works well in the experiments. F ( ) is close to 0 or 1 for most values, which approximates the discrete Bernoulli distribution. In contrast to the dropout , \u2208 (0, 1) is a differentiable parameter. Moreover, it acts as the posterior probability for feature preservation, which is influenced by the priori probability for feature preservation , and can be learned as the feature importance.\n\nIn this context, the entire process of training for feature selection is built in Eq. (3) and (5). Note that, the learnable variables are , and , which are trained simultaneously. Through a fast convergence of , the feature set of the pre-ranking model can then be obtained by selecting the feature fields with top-of values.\n\n\nFine-Tuning the Pre-Ranking Model\n\nAfter feature selection, the feature fields not in the pre-ranking set acquired by FSCD are masked, and the models are fine-tuned using weights and as initialization parameters. Concretely, the model can be trained with the following loss function\n( \u2032 , \u2032 ) = \u2212 \u2211\ufe01 =1 log ( | ( \u2032 , \u2032 , \u2032 )),(6)\nwhere \u2032 , \u2032 , and \u2032 are the samples with the selected feature fields, the remaining feature embeddings, and dense weights for the preranking model, respectively. \u2032 and \u2032 are initialized by and , which accelerates the training. The Bernoulli dropout is omitted in the fine-tuning process.\n\nIn this way, the pre-ranking model is obtained. As the model is in an interaction-focused architecture and adopts both effective and efficient feature fields, its expression ability significantly improves. Therefore, the effectiveness of the system can be improved with high efficiency. The entire method is illustrated in Algorithm 1. \n\n\nEXPERIMENTS 3.1 Experiment Configurations\n\nEvaluations are mainly based on the online sponsored search system for a real-world e-commerce app named Taobao, where both the click-through rate (CTR) and response per mile (RPM), which evaluates platform revenue, are optimized. The proposed effective and efficient pre-ranking are compared with the baselines of the vector-product based model [4] and COLD [12] based pre-ranking.\n\nBoth the pre-ranking and ranking models are based on an interactionfocused architecture. After the feature fields are transformed into embeddings, hidden layers with sizes of 1024, 512, and 256 are adopted for the ranking, while only 2 hidden layers with sizes of 1024 and 256 are used for the pre-ranking to ensure high efficiency. Finally, the sigmoid function is utilized to predict the final CTRs for both models.\n\nThe feature set for the pre-ranking model is a subset of that for the ranking model, which is selected by the proposed FSCD. All feature sets, consisting of 246 feature fields in their entirety,   \n\n\nAnalysis of FSCD\n\nAfter the proposed FSCD process, the feature fields are ranked using the output parameter . Table 2 lists the minimum, median, and maximum ranking indices for different types of features using Dropout FR [1] and deep feature selection (DFS) method [6]. For the proposed FSCD in this paper, the ranking indices for the simple features are much smaller than those for the complex feature types (see median and maximum). The simple features tend to have a more forward ranking, while the complicated features are backward, which emphasizes the efficiency of the features. However, the minimum ranking index of type IV features is only 8, which means that a feature with large complexity can still rank forward if it is truly effective for the training task. For the conventional methods, the rankings of the features are less dependent on the feature type. The pre-ranking models with different feature field number are well trained based on Eq. (6) with 2 billion samples. The area under the curve (AUC), latency, and CPU consumption for online inference of the different models are illustrated in Table 3. The results of the conventional feature selection methods [1,6] are also provided for comparison. Specifically, when = 246, the feature set of pre-ranking model becomes exactly the same as that of the ranking model. The results show that the proposed FSCD has slightly smaller AUC than the other methods when identical values are considered. However, the complexity is extremely   Table 5: Online effectiveness and efficiency of the proposed pre-ranking model and the conventional models on realworld mobile online queries.\n\nreduced. When = 100, the AUC difference relative to that of the model for the other methods is only 0.0026, while the complexity is approximately 30% lower than that of the other methods for both latency and CPU cost. When > 100, the AUC increases slowly, while the complexity increases significantly. Therefore, the pre-ranking utilizes the feature fields that have top-100 of values as the final feature set. Table 4 and Table 5 show the offline and online experimental results using the proposed effectiveness and efficiency based pre-ranking, while the original vector-product and COLD based pre-rankings are configured as benchmarks. Each model is trained with more than 200 billion real-world e-commerce samples from a log system to achieve the best online performance. Therefore, the offline AUCs are much larger than those shown in Table 3. The recall rate is defined as the preserved probability of pre-ranking model for the top-5 items ranked by the ranking model. The offline results in Table 4 show that the proposed model is much more effective than the vector-product based model, while it is slightly less effective than COLD due to efficiency considerations. For the online effect, 30 continuous days of mobile real-world requests are evaluated and the results are shown in Table 5. It shows that the proposed pre-ranking model obtains a good balance for online CTR and RPM performance and gains significant improvement in both CTR and RPM compared with its counterparts.\n\n\nPerformance Comparisons\n\nFinally, the efficiency is analyzed. Although a complicated preranking with an interaction-focused architecture is adopted in the proposed approach, the feature selection method based on effectiveness and efficiency optimizes the computational complexity to a low degree. Moreover, the number of input item is reduced from 6000 to 800, which further reduces the overall complexity of the system. In this context, the online CPU consumption of the system is almost the same as that of the vector-product based model, while the response time is slightly increased. In regard to the COLD model, the response time and CPU consumption are both greater than those of the proposed approach, which means the proposed pre-ranking model is more efficient. The key metrics for efficiency are listed in Table 5 at a peak number of queries per second (QPS).\n\n\nCONCLUSIONS\n\nIn this paper, to solve the problem of performance loss caused by the pre-ranking model with representation-focused architecture, a pre-ranking model based on feature selection with joint optimization for both effectiveness and efficiency is proposed in an interaction-focused architecture. The pre-ranking model with interaction-focused architecture, which is inherited from that of ranking model, utilizes the feature subset selected by the proposed learnable FSCD, which includes not only simple features but also interactive features with significant effectiveness. The experiments on offline training demonstrate the validity of the proposed FSCD method in both effectiveness and efficiency. Moreover, the offline and online effects in the real-world sponsored search system illustrate the performance improvements in the proposed pre-ranking model compared to the conventional benchmarks. The proposed pre-ranking has been utilized as an online running model for a real-world sponsored search system and has generated substantial revenue for the company.\n\n\nA DERIVATION FOR EQ. (3) AND (4)\n\nAssuming that and are subject to a joint Gaussian distribution [8], the joint distribution for , and is \n\nwhere , and are all learnable variables. To optimize these variables, the loss function can be concluded by maximizing the posterior probability of , and given the training samples D. In this context, by applying Bayesian rule [10], maximizing ( , , |D) \u221d (D| , , ) ( , , ) is equivalent to: * , * , * = arg min \n\nThe first part is the cross entropy, while the second part \u2212log ( , , ) can be rewritten according to Eq. (7) as \u2212log ( , , ) = (|| || 2 + || || 2 ) + \u2211\ufe01 =1 + ,\n\nwhere , and are all constants derived by Eq. (7). (|| || 2 + || || 2 ) is the common 2 regularization term with weight . The nontrivial term =1 is a new regularization term derived by the Bernoulli distribution, where the regularization factor is derived by Eq. (1) and (7) as = log(1 \u2212 ) \u2212 log( ).\n\nThen the total loss function can be written as Eq. (3).\n\nFigure 1 :\n1Real-world multi-stage ranking architecture. (a)\n\nFigure 2 :\n2The proposed FSCD for pre-ranking model.\n\nAlgorithm 1\n1The Proposed Pre-ranking based on FSCD Input: Feature set = { 1 , 2 , ..., }, feature complexity metrics { , , }, training samples D, hyperparameters 1,2,3 . Output: Pre-ranking model M. 1: Calculate , and for each feature field using Eq. (2) and (4). 2: Build the model with interaction-focused architecture where the feature embeddings are multiplied by parameterized by in Eq. (5). 3: Train using Eq. (3), and then select the feature fields with top-of . 4: Fine-tune according to Eq. (6) and obtain M.\n\n(\n, , ) = N ( , |0 0 0, \u03a3) =1 Bern( | ),\n\n\n(D| , , ) \u2212 log ( , , ).\n\nTable 1 :\n1Different types of features for the entire feature set with the values of .feature type \nindex \n\nnumber of \nfeature fields \n\nminimum \nranking \n\nmedian \nranking \n\nmaximum \nranking \nI \n141 \n1/2/1 \n129/137/85 \n244/246/156 \nII \n5 \n15/31/18 \n36/44/42 \n99/52/159 \nIII \n68 \n3/1/4 \n137/131/185 246/243/231 \nIV \n32 \n7/3/8 \n108/58/229 \n178/239/246 \n\n\n\nTable 2 :\n2Each type can be divided into either user/query features or item correlated features which include item features and interactive features between item and user/query. The user/query features are computed only once for online inference regardless of the number of candidate advertisements and require less computational consumption, while the item correlated features should be computed as many times as the number of advertisements to be scored, thus consuming much more computational resources.Feature ranking and distributions for different \ntypes of features, where the ranking results for different \nmethods are provided and divided by slashes in order, in-\ncluding the Dropout FR method [1], the DFS method [6], and \nthe proposed FSCD. \ninclude different types, e.g., simple features that directly look up \nembeddings and complex features that require complicated compu-\ntations for online embeddings. The values are determined by the above feature types and the \ndetailed configurations are listed in Table 1. For other hyperparam-\neters, 1 = 1, 2 = 10 \u22122 , and 3 = 10 \u22127 are configured. \n\n\n\nTable 3 :\n3Offline results of the proposed pre-ranking model and the conventional models. Each model is trained with more than 200 billion samples.AUC results, latency, and CPU consumption for the \nfirst feature fields of the proposed FSCD and conventional \nmethods. Each model is trained with 2 billion samples. \nMethods \nrecall rate offline AUC \nVector-product based model \n88% \n0.695 \nCOLD model \n96% \n0.738 \nThe proposed model \n95% \n0.737 \nTable 4: Methods \ninput item \nnumber \nCTR \nRPM \nresponse \ntime \n\nCPU \nconsumption \nvector-product \nbased model \n6000 \n/ \n/ \n58.4 ms \n79% \n\nCOLD model \n600 \n+1.11% +1.04% \n62.3 ms \n85% \nThe proposed model \n800 \n+1.54% +2.76% \n59.9 ms \n79% \n\n\n\nDropout feature ranking for deep learning models. Chun-Hao Chang, Ladislav Rampasek, Anna Goldenberg, arXiv:1712.08645arXiv preprintChun-Hao Chang, Ladislav Rampasek, and Anna Goldenberg. Dropout feature ranking for deep learning models. arXiv preprint arXiv:1712.08645, 2017.\n\nJoint optimization of cascade ranking models. Luke Gallagher, Ruey-Cheng Chen, Roi Blanco, J Shane Culpepper, Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining. the Twelfth ACM International Conference on Web Search and Data MiningLuke Gallagher, Ruey-Cheng Chen, Roi Blanco, and J Shane Culpepper. Joint optimization of cascade ranking models. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, pages 15-23, 2019.\n\nA deep look into neural ranking models for information retrieval. Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, Chen Wu, Bruce Croft, Xueqi Cheng, Information Processing & Management. 576102067Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, Chen Wu, W Bruce Croft, and Xueqi Cheng. A deep look into neural ranking models for information retrieval. Information Processing & Management, 57(6):102067, 2020.\n\nLearning deep structured semantic models for web search using clickthrough data. Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, Larry Heck, Proceedings of the 22nd ACM international conference on Information & Knowledge Management. the 22nd ACM international conference on Information & Knowledge ManagementPo-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management, pages 2333-2338, 2013.\n\nCategorical reparameterization with gumbel-softmax. Eric Jang, Shixiang Gu, Ben Poole, arXiv:1611.01144arXiv preprintEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.\n\nDeep feature selection: theory and application to identify enhancers and promoters. Yifeng Li, Chih-Yu Chen, Wyeth W Wasserman, Journal of Computational Biology. 235Yifeng Li, Chih-Yu Chen, and Wyeth W Wasserman. Deep feature selection: theory and application to identify enhancers and promoters. Journal of Compu- tational Biology, 23(5):322-336, 2016.\n\nCascade ranking for operational e-commerce search. Shichen Liu, Fei Xiao, Wenwu Ou, Luo Si, Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningShichen Liu, Fei Xiao, Wenwu Ou, and Luo Si. Cascade ranking for operational e-commerce search. In Proceedings of the 23rd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining, pages 1557-1565, 2017.\n\nPbodl: Parallel bayesian online deep learning for click-through rate prediction in tencent advertising system. Xun Liu, Wei Xue, Lei Xiao, Bo Zhang, arXiv:1707.00802arXiv preprintXun Liu, Wei Xue, Lei Xiao, and Bo Zhang. Pbodl: Parallel bayesian online deep learning for click-through rate prediction in tencent advertising system. arXiv preprint arXiv:1707.00802, 2017.\n\nDesigning efficient cascaded classifiers: tradeoff between accuracy and cost. Balaji Vikas C Raykar, Shipeng Krishnapuram, Yu, Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining. the 16th ACM SIGKDD international conference on Knowledge discovery and data miningVikas C Raykar, Balaji Krishnapuram, and Shipeng Yu. Designing efficient cascaded classifiers: tradeoff between accuracy and cost. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 853-860, 2010.\n\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme, arXiv:1205.2618Bpr: Bayesian personalized ranking from implicit feedback. arXiv preprintSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. Bpr: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618, 2012.\n\nBillion-scale commodity embedding for e-commerce recommendation in alibaba. Jizhe Wang, Pipei Huang, Huan Zhao, Zhibo Zhang, Binqiang Zhao, Dik Lun Lee, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningACMJizhe Wang, Pipei Huang, Huan Zhao, Zhibo Zhang, Binqiang Zhao, and Dik Lun Lee. Billion-scale commodity embedding for e-commerce recommendation in alibaba. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 839-848. ACM, 2018.\n\nCold: Towards the next generation of pre-ranking system. Zhe Wang, Liqin Zhao, Biye Jiang, Guorui Zhou, Xiaoqiang Zhu, Kun Gai, arXiv:2007.16122arXiv preprintZhe Wang, Liqin Zhao, Biye Jiang, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. Cold: Towards the next generation of pre-ranking system. arXiv preprint arXiv:2007.16122, 2020.\n\nEenmf: An end-to-end neural matching framework for e-commerce sponsored search. Wenjin Wu, Guojun Liu, Hui Ye, Chenshuang Zhang, Tianshu Wu, Daorui Xiao, Wei Lin, Xiaoyu Zhu, arXiv:1812.01190arXiv preprintWenjin Wu, Guojun Liu, Hui Ye, Chenshuang Zhang, Tianshu Wu, Daorui Xiao, Wei Lin, and Xiaoyu Zhu. Eenmf: An end-to-end neural matching framework for e-commerce sponsored search. arXiv preprint arXiv:1812.01190, 2018.\n", "annotations": {"author": "[{\"end\":241,\"start\":219},{\"end\":271,\"start\":242},{\"end\":297,\"start\":272},{\"end\":326,\"start\":298},{\"end\":355,\"start\":327},{\"end\":380,\"start\":356},{\"end\":412,\"start\":381},{\"end\":437,\"start\":413},{\"end\":487,\"start\":438},{\"end\":510,\"start\":488},{\"end\":540,\"start\":511},{\"end\":566,\"start\":541},{\"end\":595,\"start\":567},{\"end\":624,\"start\":596},{\"end\":649,\"start\":625},{\"end\":681,\"start\":650},{\"end\":706,\"start\":682},{\"end\":732,\"start\":707}]", "publisher": "[{\"end\":146,\"start\":125},{\"end\":1129,\"start\":1108}]", "author_last_name": "[{\"end\":224,\"start\":222},{\"end\":254,\"start\":250},{\"end\":280,\"start\":276},{\"end\":309,\"start\":306},{\"end\":338,\"start\":334},{\"end\":363,\"start\":360},{\"end\":395,\"start\":392},{\"end\":420,\"start\":418},{\"end\":446,\"start\":441},{\"end\":493,\"start\":491},{\"end\":523,\"start\":519},{\"end\":549,\"start\":545},{\"end\":578,\"start\":575},{\"end\":607,\"start\":603},{\"end\":632,\"start\":629},{\"end\":664,\"start\":661},{\"end\":689,\"start\":687},{\"end\":715,\"start\":710}]", "author_first_name": "[{\"end\":221,\"start\":219},{\"end\":249,\"start\":242},{\"end\":275,\"start\":272},{\"end\":305,\"start\":298},{\"end\":333,\"start\":327},{\"end\":359,\"start\":356},{\"end\":391,\"start\":381},{\"end\":417,\"start\":413},{\"end\":440,\"start\":438},{\"end\":490,\"start\":488},{\"end\":518,\"start\":511},{\"end\":544,\"start\":541},{\"end\":574,\"start\":567},{\"end\":602,\"start\":596},{\"end\":628,\"start\":625},{\"end\":660,\"start\":650},{\"end\":686,\"start\":682},{\"end\":709,\"start\":707}]", "author_affiliation": "[{\"end\":240,\"start\":226},{\"end\":270,\"start\":256},{\"end\":296,\"start\":282},{\"end\":325,\"start\":311},{\"end\":354,\"start\":340},{\"end\":379,\"start\":365},{\"end\":411,\"start\":397},{\"end\":436,\"start\":422},{\"end\":486,\"start\":472},{\"end\":509,\"start\":495},{\"end\":539,\"start\":525},{\"end\":565,\"start\":551},{\"end\":594,\"start\":580},{\"end\":623,\"start\":609},{\"end\":648,\"start\":634},{\"end\":680,\"start\":666},{\"end\":705,\"start\":691},{\"end\":731,\"start\":717}]", "title": "[{\"end\":124,\"start\":1},{\"end\":856,\"start\":733}]", "venue": "[{\"end\":981,\"start\":858}]", "abstract": "[{\"end\":2609,\"start\":1477}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3059,\"start\":3056},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3061,\"start\":3059},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3063,\"start\":3061},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3170,\"start\":3167},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3330,\"start\":3327},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3542,\"start\":3539},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3545,\"start\":3542},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3847,\"start\":3843},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5174,\"start\":5172},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7509,\"start\":7506},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9451,\"start\":9447},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10078,\"start\":10075},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10760,\"start\":10757},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12341,\"start\":12338},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12355,\"start\":12351},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13220,\"start\":13217},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13264,\"start\":13261},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14179,\"start\":14176},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14181,\"start\":14179},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18181,\"start\":18178},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18452,\"start\":18448},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18745,\"start\":18742}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":19114,\"start\":19053},{\"attributes\":{\"id\":\"fig_1\"},\"end\":19168,\"start\":19115},{\"attributes\":{\"id\":\"fig_2\"},\"end\":19688,\"start\":19169},{\"attributes\":{\"id\":\"fig_3\"},\"end\":19730,\"start\":19689},{\"attributes\":{\"id\":\"fig_4\"},\"end\":19757,\"start\":19731},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":20110,\"start\":19758},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":21219,\"start\":20111},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":21905,\"start\":21220}]", "paragraph": "[{\"end\":3379,\"start\":2625},{\"end\":4450,\"start\":3381},{\"end\":5687,\"start\":4452},{\"end\":6685,\"start\":5713},{\"end\":7402,\"start\":6687},{\"end\":8468,\"start\":7433},{\"end\":8631,\"start\":8483},{\"end\":9268,\"start\":8665},{\"end\":9455,\"start\":9270},{\"end\":9618,\"start\":9519},{\"end\":9956,\"start\":9644},{\"end\":10079,\"start\":9958},{\"end\":10661,\"start\":10141},{\"end\":10988,\"start\":10663},{\"end\":11273,\"start\":11026},{\"end\":11608,\"start\":11321},{\"end\":11946,\"start\":11610},{\"end\":12374,\"start\":11992},{\"end\":12793,\"start\":12376},{\"end\":12992,\"start\":12795},{\"end\":14641,\"start\":13013},{\"end\":16130,\"start\":14643},{\"end\":17002,\"start\":16158},{\"end\":18078,\"start\":17018},{\"end\":18219,\"start\":18115},{\"end\":18533,\"start\":18221},{\"end\":18695,\"start\":18535},{\"end\":18995,\"start\":18697},{\"end\":19052,\"start\":18997}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8482,\"start\":8469},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8664,\"start\":8632},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9518,\"start\":9456},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9643,\"start\":9619},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10140,\"start\":10080},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11320,\"start\":11274}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":13112,\"start\":13105},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":14116,\"start\":14109},{\"end\":14506,\"start\":14499},{\"end\":15061,\"start\":15054},{\"end\":15073,\"start\":15066},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":15490,\"start\":15483},{\"end\":15648,\"start\":15641},{\"end\":15940,\"start\":15933},{\"end\":16956,\"start\":16949}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2623,\"start\":2611},{\"attributes\":{\"n\":\"2\"},\"end\":5711,\"start\":5690},{\"attributes\":{\"n\":\"2.1\"},\"end\":7431,\"start\":7405},{\"attributes\":{\"n\":\"2.2\"},\"end\":11024,\"start\":10991},{\"attributes\":{\"n\":\"3\"},\"end\":11990,\"start\":11949},{\"attributes\":{\"n\":\"3.2\"},\"end\":13011,\"start\":12995},{\"attributes\":{\"n\":\"3.3\"},\"end\":16156,\"start\":16133},{\"attributes\":{\"n\":\"4\"},\"end\":17016,\"start\":17005},{\"end\":18113,\"start\":18081},{\"end\":19064,\"start\":19054},{\"end\":19126,\"start\":19116},{\"end\":19181,\"start\":19170},{\"end\":19691,\"start\":19690},{\"end\":19768,\"start\":19759},{\"end\":20121,\"start\":20112},{\"end\":21230,\"start\":21221}]", "table": "[{\"end\":20110,\"start\":19845},{\"end\":21219,\"start\":20618},{\"end\":21905,\"start\":21368}]", "figure_caption": "[{\"end\":19114,\"start\":19066},{\"end\":19168,\"start\":19128},{\"end\":19688,\"start\":19183},{\"end\":19730,\"start\":19692},{\"end\":19757,\"start\":19733},{\"end\":19845,\"start\":19770},{\"end\":20618,\"start\":20123},{\"end\":21368,\"start\":21232}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3086,\"start\":3074},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3652,\"start\":3644},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4690,\"start\":4682},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7671,\"start\":7663}]", "bib_author_first_name": "[{\"end\":21965,\"start\":21957},{\"end\":21981,\"start\":21973},{\"end\":21996,\"start\":21992},{\"end\":22235,\"start\":22231},{\"end\":22257,\"start\":22247},{\"end\":22267,\"start\":22264},{\"end\":22283,\"start\":22276},{\"end\":22749,\"start\":22742},{\"end\":22761,\"start\":22755},{\"end\":22772,\"start\":22767},{\"end\":22782,\"start\":22779},{\"end\":22796,\"start\":22789},{\"end\":22806,\"start\":22801},{\"end\":22819,\"start\":22815},{\"end\":22829,\"start\":22824},{\"end\":22842,\"start\":22837},{\"end\":23221,\"start\":23215},{\"end\":23237,\"start\":23229},{\"end\":23250,\"start\":23242},{\"end\":23258,\"start\":23256},{\"end\":23269,\"start\":23265},{\"end\":23282,\"start\":23277},{\"end\":23790,\"start\":23786},{\"end\":23805,\"start\":23797},{\"end\":23813,\"start\":23810},{\"end\":24072,\"start\":24066},{\"end\":24084,\"start\":24077},{\"end\":24098,\"start\":24091},{\"end\":24395,\"start\":24388},{\"end\":24404,\"start\":24401},{\"end\":24416,\"start\":24411},{\"end\":24424,\"start\":24421},{\"end\":24951,\"start\":24948},{\"end\":24960,\"start\":24957},{\"end\":24969,\"start\":24966},{\"end\":24978,\"start\":24976},{\"end\":25293,\"start\":25287},{\"end\":25317,\"start\":25310},{\"end\":25782,\"start\":25775},{\"end\":25800,\"start\":25791},{\"end\":25820,\"start\":25816},{\"end\":25834,\"start\":25830},{\"end\":26198,\"start\":26193},{\"end\":26210,\"start\":26205},{\"end\":26222,\"start\":26218},{\"end\":26234,\"start\":26229},{\"end\":26250,\"start\":26242},{\"end\":26264,\"start\":26257},{\"end\":26797,\"start\":26794},{\"end\":26809,\"start\":26804},{\"end\":26820,\"start\":26816},{\"end\":26834,\"start\":26828},{\"end\":26850,\"start\":26841},{\"end\":26859,\"start\":26856},{\"end\":27153,\"start\":27147},{\"end\":27164,\"start\":27158},{\"end\":27173,\"start\":27170},{\"end\":27188,\"start\":27178},{\"end\":27203,\"start\":27196},{\"end\":27214,\"start\":27208},{\"end\":27224,\"start\":27221},{\"end\":27236,\"start\":27230}]", "bib_author_last_name": "[{\"end\":21971,\"start\":21966},{\"end\":21990,\"start\":21982},{\"end\":22007,\"start\":21997},{\"end\":22245,\"start\":22236},{\"end\":22262,\"start\":22258},{\"end\":22274,\"start\":22268},{\"end\":22293,\"start\":22284},{\"end\":22753,\"start\":22750},{\"end\":22765,\"start\":22762},{\"end\":22777,\"start\":22773},{\"end\":22787,\"start\":22783},{\"end\":22799,\"start\":22797},{\"end\":22813,\"start\":22807},{\"end\":22822,\"start\":22820},{\"end\":22835,\"start\":22830},{\"end\":22848,\"start\":22843},{\"end\":23227,\"start\":23222},{\"end\":23240,\"start\":23238},{\"end\":23254,\"start\":23251},{\"end\":23263,\"start\":23259},{\"end\":23275,\"start\":23270},{\"end\":23287,\"start\":23283},{\"end\":23795,\"start\":23791},{\"end\":23808,\"start\":23806},{\"end\":23819,\"start\":23814},{\"end\":24075,\"start\":24073},{\"end\":24089,\"start\":24085},{\"end\":24108,\"start\":24099},{\"end\":24399,\"start\":24396},{\"end\":24409,\"start\":24405},{\"end\":24419,\"start\":24417},{\"end\":24427,\"start\":24425},{\"end\":24955,\"start\":24952},{\"end\":24964,\"start\":24961},{\"end\":24974,\"start\":24970},{\"end\":24984,\"start\":24979},{\"end\":25308,\"start\":25294},{\"end\":25330,\"start\":25318},{\"end\":25334,\"start\":25332},{\"end\":25789,\"start\":25783},{\"end\":25814,\"start\":25801},{\"end\":25828,\"start\":25821},{\"end\":25849,\"start\":25835},{\"end\":26203,\"start\":26199},{\"end\":26216,\"start\":26211},{\"end\":26227,\"start\":26223},{\"end\":26240,\"start\":26235},{\"end\":26255,\"start\":26251},{\"end\":26268,\"start\":26265},{\"end\":26802,\"start\":26798},{\"end\":26814,\"start\":26810},{\"end\":26826,\"start\":26821},{\"end\":26839,\"start\":26835},{\"end\":26854,\"start\":26851},{\"end\":26863,\"start\":26860},{\"end\":27156,\"start\":27154},{\"end\":27168,\"start\":27165},{\"end\":27176,\"start\":27174},{\"end\":27194,\"start\":27189},{\"end\":27206,\"start\":27204},{\"end\":27219,\"start\":27215},{\"end\":27228,\"start\":27225},{\"end\":27240,\"start\":27237}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1712.08645\",\"id\":\"b0\"},\"end\":22183,\"start\":21907},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":59528278},\"end\":22674,\"start\":22185},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":81977235},\"end\":23132,\"start\":22676},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":8384258},\"end\":23732,\"start\":23134},{\"attributes\":{\"doi\":\"arXiv:1611.01144\",\"id\":\"b4\"},\"end\":23980,\"start\":23734},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":7700371},\"end\":24335,\"start\":23982},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":34574131},\"end\":24835,\"start\":24337},{\"attributes\":{\"doi\":\"arXiv:1707.00802\",\"id\":\"b7\"},\"end\":25207,\"start\":24837},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3341922},\"end\":25773,\"start\":25209},{\"attributes\":{\"doi\":\"arXiv:1205.2618\",\"id\":\"b9\"},\"end\":26115,\"start\":25775},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3742725},\"end\":26735,\"start\":26117},{\"attributes\":{\"doi\":\"arXiv:2007.16122\",\"id\":\"b11\"},\"end\":27065,\"start\":26737},{\"attributes\":{\"doi\":\"arXiv:1812.01190\",\"id\":\"b12\"},\"end\":27489,\"start\":27067}]", "bib_title": "[{\"end\":22229,\"start\":22185},{\"end\":22740,\"start\":22676},{\"end\":23213,\"start\":23134},{\"end\":24064,\"start\":23982},{\"end\":24386,\"start\":24337},{\"end\":25285,\"start\":25209},{\"end\":26191,\"start\":26117}]", "bib_author": "[{\"end\":21973,\"start\":21957},{\"end\":21992,\"start\":21973},{\"end\":22009,\"start\":21992},{\"end\":22247,\"start\":22231},{\"end\":22264,\"start\":22247},{\"end\":22276,\"start\":22264},{\"end\":22295,\"start\":22276},{\"end\":22755,\"start\":22742},{\"end\":22767,\"start\":22755},{\"end\":22779,\"start\":22767},{\"end\":22789,\"start\":22779},{\"end\":22801,\"start\":22789},{\"end\":22815,\"start\":22801},{\"end\":22824,\"start\":22815},{\"end\":22837,\"start\":22824},{\"end\":22850,\"start\":22837},{\"end\":23229,\"start\":23215},{\"end\":23242,\"start\":23229},{\"end\":23256,\"start\":23242},{\"end\":23265,\"start\":23256},{\"end\":23277,\"start\":23265},{\"end\":23289,\"start\":23277},{\"end\":23797,\"start\":23786},{\"end\":23810,\"start\":23797},{\"end\":23821,\"start\":23810},{\"end\":24077,\"start\":24066},{\"end\":24091,\"start\":24077},{\"end\":24110,\"start\":24091},{\"end\":24401,\"start\":24388},{\"end\":24411,\"start\":24401},{\"end\":24421,\"start\":24411},{\"end\":24429,\"start\":24421},{\"end\":24957,\"start\":24948},{\"end\":24966,\"start\":24957},{\"end\":24976,\"start\":24966},{\"end\":24986,\"start\":24976},{\"end\":25310,\"start\":25287},{\"end\":25332,\"start\":25310},{\"end\":25336,\"start\":25332},{\"end\":25791,\"start\":25775},{\"end\":25816,\"start\":25791},{\"end\":25830,\"start\":25816},{\"end\":25851,\"start\":25830},{\"end\":26205,\"start\":26193},{\"end\":26218,\"start\":26205},{\"end\":26229,\"start\":26218},{\"end\":26242,\"start\":26229},{\"end\":26257,\"start\":26242},{\"end\":26270,\"start\":26257},{\"end\":26804,\"start\":26794},{\"end\":26816,\"start\":26804},{\"end\":26828,\"start\":26816},{\"end\":26841,\"start\":26828},{\"end\":26856,\"start\":26841},{\"end\":26865,\"start\":26856},{\"end\":27158,\"start\":27147},{\"end\":27170,\"start\":27158},{\"end\":27178,\"start\":27170},{\"end\":27196,\"start\":27178},{\"end\":27208,\"start\":27196},{\"end\":27221,\"start\":27208},{\"end\":27230,\"start\":27221},{\"end\":27242,\"start\":27230}]", "bib_venue": "[{\"end\":22452,\"start\":22382},{\"end\":23456,\"start\":23381},{\"end\":24612,\"start\":24529},{\"end\":25519,\"start\":25436},{\"end\":26449,\"start\":26368},{\"end\":21955,\"start\":21907},{\"end\":22380,\"start\":22295},{\"end\":22885,\"start\":22850},{\"end\":23379,\"start\":23289},{\"end\":23784,\"start\":23734},{\"end\":24142,\"start\":24110},{\"end\":24527,\"start\":24429},{\"end\":24946,\"start\":24837},{\"end\":25434,\"start\":25336},{\"end\":25923,\"start\":25866},{\"end\":26366,\"start\":26270},{\"end\":26792,\"start\":26737},{\"end\":27145,\"start\":27067}]"}}}, "year": 2023, "month": 12, "day": 17}