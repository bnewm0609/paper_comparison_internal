{"id": 123722079, "updated": "2023-10-19 00:39:05.404", "metadata": {"title": "Estimating the Dimension of a Model", "authors": "[{\"first\":\"Gideon\",\"last\":\"Schwarz\",\"middle\":[]}]", "venue": null, "journal": "Annals of Statistics", "publication_date": {"year": 1978, "month": null, "day": null}, "abstract": null, "fields_of_study": "[\"Mathematics\"]", "external_ids": {"arxiv": null, "mag": "2146855196", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": "10.1214/aos/1176344136"}}, "content": {"source": {"pdf_hash": "c3af37f3dc37fac88bfa8ea29f9c74dc8424ee5e", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "implied-oa", "open_access_url": "https://doi.org/10.1214/aos/1176344136", "status": "HYBRID"}}, "grobid": {"id": "a866a314cd8107eedb11204c0060c5cf90b8d52a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c3af37f3dc37fac88bfa8ea29f9c74dc8424ee5e.txt", "contents": "\nEstimating the Dimension of a Model\nMar., 1978\n\nGideon Schwarz \nThe Annals of Statistics\nVol\n\nHebrew University\n\n\nEstimating the Dimension of a Model\n62Mar., 1978\n\n\nThe problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution. \n\n\n1.\n\nIntroduction. Statisticians are often faced with the problem of choosing the appropriate dimensionality of a model that will fit a given set of observations. Typical examples of this problem are the choice of degree for a polynomial regression and the choice of order for a multi-step Markov chain.\n\nIn such cases the maximum likelihood principle invariably leads to choosing the highest possible dimension. Therefore it cannot be the right formalization of the intuitive notion of choosing the \"right\" dimension. An extension of the maximum likelihood principle is suggested by Akaike [l] for the slightly more general problem of choosing among different models with different numbers of parameters. His suggestion amounts to maximizing the likelihood function separately for each model j, obtaining, say, Mj(Xl, . . ., X,), and then choosing the model for which log Mj(Xl, . ..,X,)kj is largest, where kj is the dimension of the model. We present an alternative approach to the problem.\n\nIn a model of given dimension maximum likelihood estimators can be obtained as large-sample limits of the Bayes estimators for arbitrary nowhere vanishing a priori distributions.\n\nTherefore we look for the appropriate modification of maximum likelihood for our case, by studying the asymptotic beavior of Bayes estimators under a special class of priors. These priors are not absolutely continuous, since they put positive probability on some lower-dimentional subspaces of the parameter space, namely the subspaces that correspond to the competing models. In the large-sample limit, the leading term of the Bayes estimator turns out to be just the maximum likelihood estimator. Only in the next term something new is obtained. This was to be expected, sinc,e (as was shown in [2] and [3], albeit for sequential testing) the leading term depends on the prior only through its support, while the second order term does reflect singularities of the a priori distribution. We shall arrive at the following procedure:\n\nwill be established here for the case of independent, identically distributed observations, and linear models.\n\n2. The exact Bayes procedure. In a general parameter space, there is no intrinsic linear structure. We therefore assume that observations come from a Koopman-Darmois family, i.e., relative to some fixed measure on the sample space they possess a density of the form where t 9 ranges over the natural parameter space O, a convex subset of the Kdimensional Euclidean space, and y is the sufficient K-dimensional statistic. The competing models are given by sets of the form mi n O, where each mi is a kidimensional linear submanifold of K-dimensional space.\n\nFitting the asymptotic nature of the result, the a priori distribution need not be known exactly. It suffices to assume that it is of the form C a j p j , where ai is the a priori probability of the jth model being the true one, and pi, the conditional a priori distribution of 0 given the jth model, has a kj-dimensional density that is bounded and locally bounded away form zero throughout mi n @.\n\nThis implies mutual orthogonality of the pj, since the intersection of two distinct linear manifolds either is one of them, or has lower dimensions than both. Finally, we assume a fixed penalty for guessing the wrong model. (Actually, a loss that depends on 0 and on the guess would yield the same asymptotic results, provided the loss function stays between two fixed positive bounds for all wrong decisions.) Under this assumption, the Bayes solution consists of selecting the model that is a posteriori most probable. Via Bayes' formula that is equivalent to choosing the j that maximizes S(Y, n, j) = log \\ ai exp((Y o t 9b(t9))n) dpi(t9) , where the integral extends over mj n 0 , and Y is the averaged y-statistic (lln) C y(Xi)-3. Asymptotics. The asymptotic expansion of S(y, n, j) could be obtained from results in an earlier paper [3] as a special case. We shall, however, keep this paper self-contained by outlining a proof of the necessary result directly.\n\nPROPOSITION. For fixed Y and j, as n tends to co, S(Y, n, j) = n sup (Y o t9 -b(B)) -+kj log n + R where the remainder R = R(Y, n, j) is bounded in n for fixed Y and j.\n\nPROOF. We shall proceed in steps. LEMMA 1. The proposition holds when Y o Bb(t9) = A -lllt9 -Bolla where R > 0, 8, is a fixed vector in mi, and p j is Lebesgue measure on mi.\n\n\nESTIMATING THE DIMENSION OF A MODEL\n\nExplicit evaluation of the integral yields ~x~(lr/nR)~i 2e*A, and sup A -R 1 1 8 -6,1j2= A . Therefore S(Y, n, j) = nA -$kj log (nl/lr) + log ai establishes the proposition for this case, with R = $ki log (lr/R) + log aj. Clearly it suffices to show that this holds for V that vanishes where U ( p.\n\nIn this case 0 ( U\" -V\" 5 p\", and therefore and we only have to show log (1 + (pn/E(V\"))) -+ 0. Now (E(Vm))'/\" --+ sup V (a well-known fact on L, norms) and sup V = sup U > p yield for p/(E(Vn))'/\" a limit strictly less than 1, hence p\"/E(V*) tends to zero, and so does log (1 + (P\"/E(V\"))).\n\nLEMMA 3. For some 0 < p < eA, where A = sup (Y o 6 -b(B)), a vector 6,, and some positive 2, and I,, the following holds wherever exp(Y o 6 -b(6)) > p:\n\nAs is well known, the matrix of second-order derivatives of b(6) is the covariance matrix of y, and hence positive definite. Therefore Y o 6 -b(6) is strictly convex, and is easily seen to attain its maximum. Let 8, be the point where the maximum A is attained. The Taylor expansion of Y o 6 -b(6) around 8, now yields the stated inequalities for some neighborhood of B,, if 21, and 21, are larger and smaller than all the eigenvalues of the matrix of second order derivatives of b(6) at 6,. By strict convexity it is now easy to determine p < eA so that it will bound exp(Y o 6 -b(6)) outside that neighborhood.\n\nThe proposition is now proved by combining the lemmas, and the assumption of local boundedness of the density function of ,ujon mi n 0.\n\nQualitatively both our procedure and Akaike's give \"a mathematical formulation of the principle of parsimony in model building.\" Quantitatively, since our procedure differs from Akaike's only in that the dimension is multiplied by\n\n.\n\n\nLEMMA 2 .\n2If two bounded positive random variables U and V agree on the set where either exceeds p for some 0 < p < sup U, then log E(U*) -log E(VX) 0 --t as n--+ co.\n\n\nThe Annals of Statistics 1978, Vol. 6 , No.2, 461-464 ESTIMATING THE DIMENSION OF A MODEL1BY GIDEON SCHWARZ \nHebrew University \n\n\nlog n, our procedure leans more than Akaike's towards lower-dimensional models (when there are 8 or more observations). For large numbers of observations the procedures differ markedly from each other. If the assumptions we made in Section 2 are accepted, Akaike's criterion cannot be asymptotically optimal. This would contradict any proof of its optimality, but no such proof seems to have been published, and the heuristics of Akaike[I]  and of Tong[4] do not seem to lead to any such proof.\n\nA new look at the statistical identification model. H Akaike, ZEEE Trans. Auto. Control. 19AKAIKE, H. (1974). A new look at the statistical identification model. ZEEE Trans. Auto. Control 19 7 16-723.\n\nA second order approximation to optimal sampling regions. G Schwarz, Ann. Math. Statist. 40SCHWARZ, G. (1969). A second order approximation to optimal sampling regions. Ann. Math. Statist. 40 313-315.\n\n. Ann Schwarz, Math. Statist. 42SCHWARZ, Ann. Math. Statist. 42 1003-1009.\n\nA sequential Student test. G , G. (1971). A sequential Student test.\n\nDetermination of the order of a Markov chain by Akaike's information criterion. H Tong, J. Appl. Prob. 12TONG, H. (1975). Determination of the order of a Markov chain by Akaike's information criterion. J. Appl. Prob. 12 488-497.\n", "annotations": {"author": "[{\"end\":114,\"start\":49}]", "publisher": null, "author_last_name": "[{\"end\":63,\"start\":56}]", "author_first_name": "[{\"end\":55,\"start\":49}]", "author_affiliation": "[{\"end\":93,\"start\":65},{\"end\":113,\"start\":95}]", "title": "[{\"end\":36,\"start\":1},{\"end\":150,\"start\":115}]", "venue": null, "abstract": null, "bib_ref": "[{\"end\":1072,\"start\":1062},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7399,\"start\":7396}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":6642,\"start\":6639},{\"attributes\":{\"id\":\"fig_1\"},\"end\":6811,\"start\":6643},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":6943,\"start\":6812}]", "paragraph": "[{\"end\":476,\"start\":166},{\"end\":781,\"start\":483},{\"end\":1471,\"start\":783},{\"end\":1651,\"start\":1473},{\"end\":2486,\"start\":1653},{\"end\":2598,\"start\":2488},{\"end\":3155,\"start\":2600},{\"end\":3556,\"start\":3157},{\"end\":4525,\"start\":3558},{\"end\":4695,\"start\":4527},{\"end\":4871,\"start\":4697},{\"end\":5209,\"start\":4911},{\"end\":5502,\"start\":5211},{\"end\":5655,\"start\":5504},{\"end\":6269,\"start\":5657},{\"end\":6406,\"start\":6271},{\"end\":6638,\"start\":6408}]", "formula": null, "table_ref": null, "section_header": "[{\"end\":481,\"start\":479},{\"end\":4909,\"start\":4874},{\"end\":6641,\"start\":6640},{\"end\":6653,\"start\":6644}]", "table": "[{\"end\":6943,\"start\":6904}]", "figure_caption": "[{\"end\":6811,\"start\":6655},{\"end\":6904,\"start\":6814}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":7493,\"start\":7492},{\"end\":7701,\"start\":7700},{\"end\":7849,\"start\":7846},{\"end\":7948,\"start\":7947},{\"end\":8071,\"start\":8070}]", "bib_author_last_name": "[{\"end\":7500,\"start\":7494},{\"end\":7709,\"start\":7702},{\"end\":7857,\"start\":7850},{\"end\":8076,\"start\":8072}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":7640,\"start\":7440},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":119732671},\"end\":7842,\"start\":7642},{\"attributes\":{\"id\":\"b2\"},\"end\":7918,\"start\":7844},{\"attributes\":{\"id\":\"b3\"},\"end\":7988,\"start\":7920},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":201234899},\"end\":8218,\"start\":7990}]", "bib_title": "[{\"end\":7490,\"start\":7440},{\"end\":7698,\"start\":7642},{\"end\":8068,\"start\":7990}]", "bib_author": "[{\"end\":7502,\"start\":7492},{\"end\":7711,\"start\":7700},{\"end\":7859,\"start\":7846},{\"end\":7951,\"start\":7947},{\"end\":8078,\"start\":8070}]", "bib_venue": "[{\"end\":7527,\"start\":7502},{\"end\":7729,\"start\":7711},{\"end\":7872,\"start\":7859},{\"end\":7945,\"start\":7920},{\"end\":8091,\"start\":8078}]"}}}, "year": 2023, "month": 12, "day": 17}