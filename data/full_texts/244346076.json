{"id": 244346076, "updated": "2023-10-05 19:25:04.381", "metadata": {"title": "Swin Transformer V2: Scaling Up Capacity and Resolution", "authors": "[{\"first\":\"Ze\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Han\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Yutong\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Zhuliang\",\"last\":\"Yao\",\"middle\":[]},{\"first\":\"Zhenda\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Yixuan\",\"last\":\"Wei\",\"middle\":[]},{\"first\":\"Jia\",\"last\":\"Ning\",\"middle\":[]},{\"first\":\"Yue\",\"last\":\"Cao\",\"middle\":[]},{\"first\":\"Zheng\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Li\",\"last\":\"Dong\",\"middle\":[]},{\"first\":\"Furu\",\"last\":\"Wei\",\"middle\":[]},{\"first\":\"Baining\",\"last\":\"Guo\",\"middle\":[]}]", "venue": "CVPR2022", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536$\\times$1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time. Code is available at \\url{https://github.com/microsoft/Swin-Transformer}.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2111.09883", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/Liu0LYXWN000WG22", "doi": "10.1109/cvpr52688.2022.01170"}}, "content": {"source": {"pdf_hash": "be0fbb810583930c071d0b9b2c5187fe260783f5", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2111.09883v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4129c96efc494f8ada3a9661a2f3dddc6bacd976", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/be0fbb810583930c071d0b9b2c5187fe260783f5.txt", "contents": "\nSwin Transformer V2: Scaling Up Capacity and Resolution\n\n\nZe Liu \nMicrosoft Research Asia\n\n\nHan Hu hanhu@microsoft.com \nMicrosoft Research Asia\n\n\nYutong Lin t-yutonglin@microsoft.com \nMicrosoft Research Asia\n\n\nZhuliang Yao \nMicrosoft Research Asia\n\n\nZhenda Xie Yixuan \nMicrosoft Research Asia\n\n\nWei Jia \nMicrosoft Research Asia\n\n\nNing Yue \nMicrosoft Research Asia\n\n\nCao Zheng \nMicrosoft Research Asia\n\n\nZhang Li \nMicrosoft Research Asia\n\n\nDong Furu \nMicrosoft Research Asia\n\n\nWei Baining Guo \nMicrosoft Research Asia\n\n\nSwin Transformer V2: Scaling Up Capacity and Resolution\n\nLarge-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pretraining method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536\u00d71,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time. Code is available at https://github.com/ microsoft/Swin-Transformer.\n\nIntroduction\n\nScaling up language models has been incredibly successful. It significantly improves a model's performance on language tasks [19,24,49,50,52,53] and the model demon-* Equal. \u2020 Project lead. Ze, Yutong, Zhuliang, Zhenda, Yixuan, Jia are long-term interns at MSRA. strates amazing few-shot capabilities similar to that of human beings [7]. Since the BERT large model with 340 million parameters [19], language models are quickly scaled up by more than 1,000 times in a few years, reaching 530 billion dense parameters [50] and 1.6 trillion sparse parameters [24]. These large language models are also found to possess increasingly strong few-shot capabilities akin to human intelligence for a broad range of language tasks [7]. On the other hand, the scaling up of vision models has been lagging behind. While it has long been recognized that larger vision models usually perform better on vision tasks [29,60], the absolute model size was just able to reach about 1-2 billion parameters very recently [17,27,39,56,80]. More importantly, unlike large language models, the exist-ing large vision models are applied to the image classification task only [17,56,80].\n\nTo successfully train large and general vision model, we need to address a few key issues. Firstly, our experiments with large vision models reveal an instability issue in training. We find that the discrepancy of activation amplitudes across layers becomes significantly greater in large models. A closer look at the original architecture reveals that this is caused by the output of the residual unit directly added back to the main branch. The result is that the activation values are accumulated layer by layer, and the amplitudes at deeper layers are thus significantly larger than those at early layers. To address this issue, we propose a new normalization configuration, called res-post-norm, which moves the LN layer from the beginning of each residual unit to the backend, as shown in Figure 1. We find this new configuration produces much milder activation values across the network layers. We also propose a scaled cosine attention to replace the previous dot product attention. The scaled cosine attention makes the computation irrelevant to amplitudes of block inputs, and the attention values are less likely to fall into extremes. In our experiments, the proposed two techniques not only make the training process more stable but also improve the accuracy especially for larger models.\n\nSecondly, many downstream vision tasks such as object detection and semantic segmentation require high resolution input images or large attention windows. The window size variations between low-resolution pre-training and high-resolution fine-tuning can be quite large. The current common practice is to perform a bi-cubic interpolation of the position bias maps [22,46]. This simple fix is somewhat ad-hoc and the result is usually sub-optimal. We introduce a log-spaced continuous position bias (Log-CPB), which generates bias values for arbitrary coordinate ranges by applying a small meta network on the log-spaced coordinate inputs. Since the meta network takes any coordinates, a pre-trained model will be able to freely transfer across window sizes by sharing weights of the meta network. A critical design of our approach is to transform the coordinates into the log-space so that the extrapolation ratio can be low even when the target window size is significantly larger than that of pre-training. The scaling up of model capacity and resolution also leads to prohibitively high GPU memory consumption with existing vision models. To resolve the memory issue, we incorporate several important techniques including zero-optimizer [54], activation check pointing [12] and a novel implementation of sequential self-attention computation. With these techniques, the GPU memory consumption of large models and resolutions is significantly reduced with only marginal effect on the training speed.\n\nWith the above techniques, we successfully trained a 3 billion Swin Transformer model and effectively transferred it to various vision tasks with image resolution as large as 1,536\u00d71,536, using Nvidia A100-40G GPUs. In our model pre-training, we also employ self-supervised pre-training to reduce the dependency on super-huge labeled data. With 40\u00d7 less labelled data than that in previous practice (JFT-3B), the 3 billion model achieves the state-of-the-art accuracy on a broad range of vision benchmarks. Specifically, it obtains 84.0% top-1 accuracy on the ImageNet-V2 image classification validation set [55], 63.1 / 54.4 box / mask AP on the COCO test-dev set of object detection, 59.9 mIoU on ADE20K semantic segmentation, and 86.8% top-1 accuracy on Kinetics-400 video action classification, which are +NA%, +4.4/+3.3, +6.3 and +1.9 higher than the best numbers in the original Swin Transformers [46,47], and surpass previous best records by +0.8% ( [80]), +1.8/+1.4 ( [74]), +1.5 ( [4]) and +1.4% ( [57]).\n\nBy scaling up both capacity and resolution of vision models with strong performance on general vision tasks, just like a good language model's performance on general NLP tasks, we aim to stimulate more research in this direction so that we can eventually close the capacity gap between vision and language models and facilitate the joint modeling of the two domains.\n\n\nRelated Works\n\nLanguage networks and scaling up Transformer has served the standard network since the pioneer work of [65]. The exploration of scaling this architecture has since begun, and the progress has been accelerated by the invention of effective self-supervised learning approaches, such as masked or auto-regressive language modeling [19,52], and has been further encouraged by the discovery of a scaling law [36]. Since then, the capacity of language models has increased dramatically by more than 1,000 times in a few years, from BERT-340M to the Megatron-Turing-530B [7, 49,50,53] and sparse Switch-Transformer-1.6T [24]. With increased capacity, the accuracy of various language benchmarks has been significantly improved. The zero-shot or few-shot performance is also significantly improved [7], which is a foundation of human generic intelligence.\n\nVision networks and scaling up CNNs have long been the standard computer vision networks [40,41]. Since AlexNet [40], architectures have become deeper and larger, which has greatly advanced various visual tasks and largely fueled the wave of deep learning in computer vision, such as VGG [60], GoogleNet [62] and ResNet citehe2015resnet. In the past two years, the CNN architectures have been further scaled up to about 1 billion parameters [27,39], however, absolute performance may not be so encouraging, perhaps due to inductive biases in the CNN architecture limiting modeling power.\n\nLast year, Transformers started taking over one representative visual benchmark after another, including ImageNet-1K image-level classification benchmarks [22], COCO region-level object detection benchmark [46], ADE20K pixel-level semantic segmentation benchmark [46,83], Kinetics-400 video action classification benchmark [2], etc. Since these works, numerous vision Transformer variants have been proposed to improve the accuracy at relatively small scale [14,21,34,42,63,68,71,75,77,78,82]. Only a few works have attempted to scale up the vision Transformers [17,56,80]. However, they rely on a huge image dataset with classification labels, i.e., JFT-3B, and are only applied to image classification problems.\n\nTransferring across window / kernel resolution For CNNs, previous works typically fixed kernel size during pre-training and fine-tuning. Global vision Transformers, such as ViT [22], compute attention globally, with the equivalent attention window size linearly proportional to the increased input image resolution. For local vision Transformer architectures, such as Swin Transformer [46], the window size can be either fixed or changed during finetuning. Allowing variable window sizes is more convenient in use, so as to be divisible by the probably variable entire feature map and to tune receptive fields for better accuracy. To handle the variable window sizes between pretraining and fine-tuning, bi-cubic interpolation was the previous common practice [22,46]. In this paper, we propose a log-spaced continuous position bias approach (Log-CPB) that more smoothly transfers pre-trained model weights at low resolution to deal-with higher resolution windows.\n\nStudy on bias terms In NLP, the relative position bias method proved beneficial [53], compared to the absolute position embedding used in the original Transformer [65]. In computer vision, the relative positional bias method is more commonly used [31,46,75], probably because the spatial relationships of visual signals play a more important role in visual modeling. A common practice is to directly learn the bias values as model weights. There are also a few works particularly study how to set and learn the bias terms [38,69].\n\n\nContinuous convolution and variants\n\nOur Log-CPB approach is also related to earlier works on continuous convolution and variants [30,45,58,67], which utilize a meta network to handle irregular data points. Our Log-CPB approach is inspired by these efforts while solving a different problem of transferring relative position biases in vision Transformers across arbitrary window sizes. We also propose log-spaced coordinates to alleviate the difficulty of extrapolation when transferring between large size changes.\n\n\nSwin Transformer V2\n\n\nA Brief Review of Swin Transformer\n\nSwin Transformer is a general-purpose computer vision backbone that has achieved strong performance in various granular recognition tasks such as region-level object detection, pixel-level semantic segmentation, and imagelevel image classification. The main idea of Swin Transformer is to introduce several important visual priors into the vanilla Transformer encoder, including hierarchy, locality, and translation invariance, which combines the strength of both: the basic Transformer unit has strong modeling capabilities, and the visual priors make it friendly to a variety of visual tasks.\n\nNormalization configuration It is widely known that normalization technologies [3,35,64,70] are crucial in stably training deeper architectures. The original Swin Transformer inherits the common practice in the language Transformers [52] and vanilla ViT [22] to utilize a prenormalization configuration without extensive study, as shown in the figure 1. In the following subsections, we will examine this default normalization configuration 1 .\n\n\nRelative position bias is a key component in the original\n\nSwin Transformer which introduces an additional parametric bias term to encode the geometric relationship in selfattention calculation:\nAttention(Q, K, V ) = SoftMax(QK T / \u221a d + B)V,(1)\nwhere B \u2208 R M 2 \u00d7M 2 is the relative position bias term for each head; Q, K, V \u2208 R M 2 \u00d7d are the query, key and value matrices; d is the query/key dimension, and M 2 is the number of patches in a window. The relative position bias encodes relative spatial configurations of visual elements and is shown critical in a variety of visual tasks, especially for dense recognition tasks such as object detection.\n\nIn Swin Transformer, the relative positions along each axis are within the range of [\u2212M + 1, M \u2212 1] and the relative position bias is parameterized as a bias matrixB \u2208 R (2M \u22121)\u00d7(2M \u22121) , and the elements in B are taken fromB. When transferring across different window sizes, the learnt relative position bias matrix in pre-training is used to initialize the bias matrix of a different size in fine-tuning by bi-cubic interpolation.\n\nIssues in scaling up model capacity and window resolution We observe two issues when we scale up the capacity and window resolution of the Swin Transformer.  Table 1. Comparison of different position bias computation approaches using Swin-T. * indicates the top-1 accuracy on ImageNet-1k trained from scratch. The models in * column will be used for testing on the ImageNet-1K image classification task using larger image/window resolutions, marked by \u2020. For these results, we report both the results w.o./with fine-tuning. These models are also used for fine-tuning on COCO object detection and ADE20K semantic segmentation tasks. \u2022 An instability issue when scaling up model capacity.\n\nAs shown in Figure 2, when we scale up the original Swin Transformer model from small size to large size, the activation values at deeper layers increase dramatically. The discrepancy between layers with the highest and the lowest amplitudes has reached an extreme value of 10 4 . When we scale it up further to a huge size (658 million parameters), it cannot complete the training, as shown in Figure 3.\n\n\u2022 Degraded performance when transferring models across window resolutions. As shown in the first row of Table 1, the accuracy decreases significantly when we directly test the accuracy of a pre-trained ImageNet-1K model (256 \u00d7 256 images with 8 \u00d7 8 window size) at larger image resolutions and window sizes through the bi-cubic interpolation approach. It may be worth re-examining the relative position bias approach in the original Swin Transformer.\n\nIn the following subsections, we present techniques to address these issues, including residual post normalization and scaled cosine attention to address the instability issue, and a log-spaced continuous position bias approach to address the issue in transferring across window resolutions.\n\n\nScaling Up Model Capacity\n\nAs mentioned in Section 3.1, the original Swin Transformer (and most vision Transformers) adopts a layer norm layer at the beginning of each block, inherited from vanilla ViT. When we scale up the model capacity, a significant increase in activation values is observed at deeper layers. In fact, in a pre-normalization configuration, the output activation values of each residual block are merged directly back to the main branch, and the amplitude of the main branch grows larger and larger at deeper layers. Large amplitude discrepancy in different layers causes training instability.\n\nPost normalization To ease this problem, we propose to use a residual post normalization approach instead, as shown in Figure 1. In this approach, the output of each residual block is normalized before merging back into the main branch, and the amplitude of the main branch does not accumulate when the layer goes deeper. As shown in Figure 2, the activation amplitudes by this approach are much milder than in the original pre-normalization configuration.\n\nIn our largest model training, we introduce an additional layer normalization layer on the main branch every 6 Transformer blocks, to further stabilize training.\n\nScaled cosine attention In the original self-attention computation, the similarity terms of the pixel pairs are computed as a dot product of the query and key vectors. We find that when this approach is used in large visual models, the learnt attention maps of some blocks and heads are frequently dominated by a few pixel pairs, especially in the res-post-norm configuration. To ease this issue, we propose a scaled cosine attention approach that computes the attention logit of a pixel pair i and j by a scaled cosine function:\nSim(q i , k j ) = cos(q i , k j )/\u03c4 + B ij ,(2)\nwhere B ij is the relative position bias between pixel i and j; \u03c4 is a learnable scalar, non-shared across heads and layers. \u03c4 is set larger than 0.01. The cosine function is naturally normalized, and thus can have milder attention values. \n\n\nScaling Up Window Resolution\n\nIn this subsection, we introduce a log-spaced continuous position bias approach, so that the relative position bias can be smoothly transferred across window resolutions.\n\nContinuous relative position bias Instead of directly optimizing the parameterized biases, the continuous position bias approach adopts a small meta network on the relative coordinates:\nB(\u2206x, \u2206y) = G(\u2206x, \u2206y),(3)\nwhere G is a small network, e.g., a 2-layer MLP with a ReLU activation in between by default. The meta network G generates bias values for arbitrary relative coordinates, and thus can be naturally transferred to fine-tuning tasks with arbitrarily varying window sizes. In inference, the bias values at each relative position can be pre-computed and stored as model parameters, such that the inference is the same as the original parameterized bias approach.\n\nLog-spaced coordinates When transferring across largely varying window sizes, a large portion of the relative coordinate range needs to be extrapolated. To ease this issue, we propose using log-spaced coordinates instead of the original linear-spaced ones:\n\u2206x = sign(x) \u00b7 log(1 + |\u2206x|), \u2206y = sign(y) \u00b7 log(1 + |\u2206y|),(4)\nwhere \u2206x, \u2206y and \u2206x, \u2206y are the linear-scaled and logspaced coordinates, respectively.\n\nBy using the log-spaced coordinates, when we transfer the relative position biases across window resolutions, the required extrapolation ratio will be much smaller than that of using the original linear-spaced coordinates. For an example of transferring from a pre-trained 8 \u00d7 8 window size to a fine-tuned 16  The extrapolation ratio is 0.33\u00d7 of the original range, which is an about 4 times smaller extrapolation ratio than that using the original linear-spaced coordinates. Table 1 compares the transferring performance of different position bias computation approaches. It can be seen that the log-spaced CPB (continuous position bias) approach performs best, particularly when transferred to larger window sizes.\n\n\nSelf-Supervised Pre-training\n\nLarger models are more data hungry. To address the data hungry problem, previous large vision models typically utilize huge labelled data such as JFT-3B [17,56,80]. In this work, we exploit a self-supervised pre-training method, SimMIM [72], to alleviate the demands on labelled data. By this approach, we successfully trained a powerful Swin Transformer model of 3 billion parameters which achieves state-of-the-art (SOTA) on 4 representative visual benchmarks, by using only 70 million labelled images (1/40 of that in JFT-3B).\n\n\nImplementation to Save GPU Memory\n\nAnother issue lies in the unaffordable GPU memory consumption with a regular implementation when both the capacity and resolution are large. To facility the memory issue, we adopt the following implementations:\n\n\u2022 Zero-Redundancy Optimizer (ZeRO) [54]. In a general data-parallel implementation of optimizers, the model parameters and optimization states are broadcasted to every GPU. This implementation is very unfriendly on GPU memory consumption, for example, a model of 3 billion parameters will consume 48G GPU memory when an AdamW optimizer and fp32 weights/states are used. With a ZeRO optimizer, the model parameters and the corresponding optimization states will be split and distributed to multiple GPUs, which significantly reduces memory consumption. We adopt the DeepSpeed framework and use the ZeRO stage-1 option in our experiments. This optimization has little effect on training speed.\n\n\u2022 Activation check-pointing [12]. Feature maps in the Transformer layers also consume a lot of GPU memory, which can create bottlenecks when image and window resolutions are high. The activation checkpointing technology can significantly reduce the memory consumption, while the training speed is up to 30% slower.\n\n\u2022 Sequential self-attention computation. To train large models on very large resolutions, for example, an image of 1,536\u00d71,536 resolution with a window size of 32\u00d732, regular A100 GPUs (40GB memory) are still unaffordable, even with the above two optimization technologies. We found that in this case, the selfattention module constitutes a bottleneck. To alleviate this problem, we implement self-attention computation sequentially, instead of using the previous batch computation approach. This optimization is applied to the layers in the first two stages and has little impact on the overall training speed.\n\nWith these implementations, we managed to train a 3B model using the Nvidia A100-40G GPUs for COCO object detection with an input image resolution of 1,536\u00d71,536, and Kinetics-400 action classification with an input resolution of 320 \u00d7 320 \u00d7 8.\n\n\nModel configurations\n\nWe maintain the stage, block, and channel settings of the original Swin Transformer for 4 configurations of Swin Transformer V2: with C the number of channels in the first stage.\n\nWe further scale up Swin Transformer V2 to its huge size and giant size, with 658 million parameters and 3 billion parameters, respectively: \n\n\nExperiments\n\n\nTasks and Datasets\n\nWe conduct experiments on ImageNet-1K image classification (V1 and V2) [18,55], COCO object detection [44], and ADE20K semantic segmentation [85]. For the 3B model experiments, we also report the accuracy on Kinetics-400 video action recognition [37].\n\n\u2022 Image classification. ImageNet-1K V1 and V2 val are used [18,55] for evaluation. ImageNet-22K [18] which has 14M images and 22K categories is optionally employed for pre-training. For the pre-training our largest model SwinV2-G, a privately collected ImageNet-22K-ext dataset with 70 million images is used. For this dataset, a duplicate removal process [51] is conducted to exclude overlapping images with ImageNet-1K V1 and V2 validation sets.\n\n\u2022 Object detection. COCO [44] is used for evaluation. For our largest model experiments, we employ an additional detection pre-training phase using Object 365 v2 dataset [59], in-between the image classification pretraining phase and the COCO fine-tuning phase.\n\n\u2022 Semantic segmentation. ADE20K [85] is used.\n\n\u2022 Video action classification. Kinetics-400 (K400) [37] is used in evaluation.\n\nThe pre-training and fine-tuning settings will be detailed in Appendix.\n\n\nScaling Up Experiments\n\nWe first present the results on various representative visual benchmarks by scaling up models to 3 billion parameters and to high image/window resolutions.\n\n\nSettings for SwinV2-G experiments\n\nWe adopt a smaller 192 \u00d7 192 image resolution in pre-training to save on training costs. We take a 2-step pre-training approach. First, the model is pre-trained using a self-supervised method [72] on the ImageNet-22K-ext dataset by 20 epochs. Second, the model is further pre-trained by 30 epochs using the image classification task on this dataset. Detailed pre-training and fine-tuning setups are described in the appendix.\n\nIn the following paragraphs, we report the accuracy of SwinV2-G on representative vision benchmarks. Note that since our main goal is to explore how to feasibly scale up model capacity and window resolution, and whether the vision tasks can benefit from significantly larger capacity, we did not particularly align complexities or pre-training data in comparisons. Table 2 compares the SwinV2-G model with previously largest/best vision models on ImageNet-1K V1 and V2 classification. SwinV2-G is the largest dense vision model to present. It achieves a top-1 accuracy of 84.0% on the ImageNet V2 benchmark, which is +0.7% higher than previous best one (83.3%). Our accuracy on ImageNet-1K V1 is marginally lower (90.17% vs 90.88%). The performance difference might come from different degrees of dataset overtuning [55]. Also note we employ much less training iterations and lower image resolutions than those in previous efforts, while performing very well. We also compare the SwinV2-B and SwinV2-L to the original SwinV1-B and SwinV1-L, respectively, where a +0.8% and +0.4% gains are observed. The shrunken gains by SwinV2-L than that of SwinV2-B may imply that if exceeding this size, more labeled data, stronger regularization, or advanced self-supervised learning methods are required.  Table 3 compares the SwinV2-G model with previous best results on COCO object detection and instance segmentation. It achieves 63.1/54.4 box/max AP on COCO test-dev, which is +1.8/1.4 higher than previous best numberw (61.3/53.0 by [74]). This suggests that scaling up vision model is beneficial for the dense vision recognition task of object detection. Our approach can use a different window size at test to additionally benefit, probably attributed to the effective Log-spaced CPB approach. Table 4 compares the SwinV2-G model with previous best results on the ADE20K semantic segmentation benchmark. It achieves 59.9 mIoU on ADE20K val set, +1.5 higher than the previous best number (58.4 by [4]). This suggests scaling up vision model is beneficial for pixel-level vision recognition tasks. Using a larger window size at test time can additionally bring +0.2 gains, probably attributed to the effective Log-spaced CPB approach. Table 5 compares the SwinV2-G model with previous best results on the Kinetics-400 action classification benchmark. It achieves 86.8% top-1 accuracy, +1.4% higher than previous best number [57]. This suggests that scaling up vision models also benefits video recognition tasks. In this scenario, using a larger window size at test time can also bring additional benefits of +0.2%, probably attributed to the effective Log-spaced CPB approach.\n\n\nImageNet-1K image classification results\n\n\nCOCO object detection results\n\n\nADE20K semantic segmentation results\n\n\nKinetics-400 video action classification results\n\n\nAblation Study\n\nAblation on res-post-norm and scaled cosine attention Table 6 ablates the performance of applying the proposed res-post-norm and scaled cosine attention approaches to Swin Transformer. Both techniques improve the accuracy at all the tiny, small and base size, and the overall improvements are +0.2%, +0.4% and +0.5% respectively, indicating the techniques are more beneficial for larger models. It also turns out to benefit ViT architecture (+0.4%). The proposed normalization approach also performs better than some other normalization methods, as shown in Table 7. More importantly, the combination of post-norm and scaled cosine attention stabilize the training. As shown in Figure 2, while the activation values at deeper layers for the original Swin Transformer are almost exploded at large (L) size, those of the new version have much milder behavior. On a huge size model, the self-supervised pre-training [72] diverges using the original Swin Transformer, while it trains well by a Swin Transformer V2 model. Table 1 and 8 ablate the performance of 3 approaches by scaling window resolutions from 256 \u00d7 256 in pretraining to larger sizes in 3 down-stream vision tasks of ImageNet-1K image classification, COCO object detection, and ADE20K semantic segmentation, respectively. It can be seen that: 1) Different approaches have similar accuracy in pre-training (81.7%-81.8%); 2) When transferred to downstream tasks, the two continuous position bias (CPB) ap- proaches perform consistently better than the parameterized position bias approach used in Swin Transformer V1. Compared to the linear-spaced approach, the log-spaced version is marginally better; 3) The larger the change in resolutions between pre-training and fine-tuning, the larger the benefit of the proposed log-spaced CPB approach.\n\n\nScaling up window resolution by different approaches\n\nIn Table 1 and 8, we also report the accuracy using targeted window resolutions without fine-tuning (see the first number in each column in the ImageNet-1K experiments). The recognition accuracy remains not bad even when the window size is enlarged from 8 to 24 (78.9% versus 81.8%), while the top-1 accuracy of the original approach significantly degrades from 81.7% to 68.7%. Also note that without fine-tuning, using a window size of 12 that the pretrained model has never seen before can even be +0.4% higher that the original accuracy. This suggests that we can improve accuracy through test-time window adjustment, as also observed in Table 3, 4 and 5.\n\n\nConclusion\n\nWe have presented techniques for scaling Swin Transformer up to 3 billion parameters and making it capable of training with images of up to 1,536\u00d71,536 resolution, including the res-post-norm and scaled cosine attention to make the model easier to be scaled up in capacity, as well a log-spaced continuous relative position bias approach which lets the model more effectively transferred across window resolutions. The adapted architecture is named Swin Transformer V2, and by scaling up capacity and resolution, it sets new records on 4 representative vision benchmarks. By these strong results, we hope to stimulate more research in this direction so that we can eventually close the capacity gap between vision and language models and facilitate the joint modeling of the two domains.\n\n\nA1. Experimental Settings for Ablation\n\nThis section describes the experimental settings for ablation, including models of SwinV2-T, SwinV2-S, and SwinV2-B, and tasks of ImageNet-1K image classification, COCO object detection and ADE semantic segmentation.\n\n\nA1.1. ImageNet-1K Pre-training\n\nAll ablation study use the ImageNet-1K image classification task for pre-training. We adopt an input image size (window size) of 256\u00d7256 (8\u00d78) 2 . Following [46], we employ an AdamW [48] optimizer for 300 epochs using a cosine decay learning rate scheduler with 20 epochs of linear warm-up. A batch size of 1024, an initial learning rate of 1\u00d710 \u22123 , a weight decay of 0.05, and gradient clipping with a max norm of 5.0 are used. Augmentation and regularization strategies include RandAugment [15], Mixup [81], Cutmix [79], random erasing [84] and stochastic depth [32]. An increasing degree of stochastic depth augmentation is employed for larger models, i.e. 0.2, 0.3, 0.5 for tiny, small, and base models, respectively.\n\n\nA1.2. Fine-tuning on various tasks\n\nImageNet-1K image classification For ImageNet-1K image classification experiments, we conduct a fine-tuning step if the input image resolution is larger than that in the pre-training step. The fine-tuning lasts for 30 epochs, with an AdamW [48] optimizer, a cosine decay learning rate scheduler with an initial learning rate of 4 \u00d7 10 \u22125 , a weight decay of 1 \u00d7 10 \u22128 , and the same data augmentation and regularizations as those in the first stage.\n\nCOCO object detection We use cascade mask R-CNN [8,28] implemented in mmdetection [11] as the object detection framework. In training, a multi-scale augmentation [9,61] with the shorter side between 480 and 800 and the longer side of 1333 is used. The window size is set 16\u00d716. An AdamW [48] optimizer with an initial learning rate of 1 \u00d7 10 \u22124 , a weight decay of 0.05, a batch size of 16, and a 3\u00d7 scheduler are used.\n\n\nADE20K semantic segmentation\n\nWe adopt an image size (window size) of 512\u00d7512 (16\u00d716). In training, we employ an AdamW [48] optimizer with an initial learning rate of 4 \u00d7 10 \u22125 , a weight decay of 0.05, a learning rate scheduler that uses linear learning rate decay and a linear warm-up of 1,500 iterations. Models are trained with batch size of 16 for 160K iterations. We follow the mmsegmentation codebase to adopt augmentations of random horizontal 2 Most of our experiments have the window size as an even number to make the window shifting offset divisible by the window size. Nevertheless, an odd number of window size also works well, as is right the case in the original Swin Transformer (7 \u00d7 7). flipping, random re-scaling within ratio range [0.5, 2.0] and a random photometric distortion. Stochastic depth with ratio of 0.3 is applied for all models. A layer-wise learning rate decay [4] of 0.95 is adopted for all experiments.\n\n\nA2. Experimental Settings for System-Level Comparison\n\nA2.1. SwinV2-B and SwinV2-L Settings Table 2, 3 and 4 include results of SwinV2-B and SwinV2-L. For these experiments, we first conduct ImageNet-22K pre-training, and then fine-tune the pretrained models on individual down-stream recognition tasks.\n\nImageNet-22K pre-training Both models use an input image size (window size) of 192\u00d7192 (12\u00d712). We employ an AdamW optimizer [48] for 90 epochs using a cosine learning rate scheduler with 5-epoch linear warm-up. A batch size of 4096, an initial learning rate of 0.001, a weight decay of 0.1, and gradient clipping with a max norm of 5.0 are used. Augmentation and regularization strategies include RandAugment [15], Mixup [81], Cutmix [79], random erasing [84] and stochastic depth [32] with ratio of 0.2.\n\nImageNet-1K image classification We consider input image sizes of 256\u00d7256 and 384\u00d7384. The training length is set 30 epochs, with a batch size of 1024, a cosine decay learning rate scheduler with an initial learning rate of 4 \u00d7 10 \u22125 , and a weight decay of 1 \u00d7 10 \u22128 . The ImageNet-1K classification weights are also initialized from the corresponding ones in the ImageNet-22K model.\n\n\nCOCO object detection\n\nWe adopt HTC++ [10,46] for experiments. In data pre-processing, Instaboost [23], a multi-scale training [26] with an input image size of 1536\u00d71536, a window size of 32\u00d732, and a random scale between [0.1, 2.0] are used. An AdamW optimizer [48] with an initial learning rate of 4 \u00d7 10 \u22124 on batch size of 64, a weight decay of 0.05, and a 3\u00d7 scheduler are used. The backbone learning rate is set 0.1\u00d7 of the head learning rate. In inference, soft-NMS [5] is used. Both single-scale and multi-scale test results are reported.\n\nADE20K semantic segmentation The input image size (window size) is set 640\u00d7640 (40\u00d740). We employ an AdamW [48] optimizer with an initial learning rate of 6 \u00d7 10 \u22125 , a weight decay of 0.05, a linear decayed learning rate scheduler with 375-iteration linear warm-up. The model is trained with batch size of 64 for 40K iterations. We follow the default settings in mmsegmentation for data augmentation, including random horizontal flipping, random re-scaling within ratio range [0.5, 2.0] and random photometric distortion. Stochastic depth with ratio of 0.3 is applied.\n\n\nA2.2. SwinV2-G Settings\n\nStage-1 self-supervised pre-training The model is first pre-trained using a self-supervised learning approach [1] on the ImageNet-22K-ext dataset (70 million images) for 20 epochs. To reduce experimental overheads, we adopt a smaller image size of 192\u00d7192. The model is trained using the AdamW [48] optimizer with a cosine decay learning rate scheduler with 30000 steps of linear warm-up. A batch size of 9216, an initial learning rate of 1.4 \u00d7 10 \u22123 , a weight decay of 0.1, and gradient clipping with a max norm of 100.0 are used. A light data augmentation strategy is employed: random resize cropping with scale range of [0.67, 1] and a aspect ratio range of [3/4, 4/3], followed by a random flipping and a color normalization steps.\n\nStage-2 supervised pre-training The model is further pre-trained using the class labels on the ImageNet-22Kext dataset. We employ an AdamW [48] optimizer for 30 epochs, using a cosine decayed learning rate scheduler with 20000 steps of linear warm-up. A batch size of 9216, an initial learning rate of 1.4 \u00d7 10 \u22123 , a layer-wise learning rate decay of 0.87, a weight decay of 0.1, and gradient clipping with a max norm of 100.0 are used. Augmentation and regularization strategies include RandAugment [15], random erasing [84] and a stochastic depth [32] ratio of 0.3.\n\nFine-tuning on ImageNet-1K image classification We adopt an input image size of 640\u00d7640 for experiments. An AdamW [48] optimizer is employed for 10 epochs, using a cosine decayed learning rate scheduler and a 2-epoch linear warm-up. A batch size of 576, an initial learning rate of 2.1 \u00d7 10 \u22125 , a weight decay of 0.1, and gradient clipping with a max norm of 100.0 are used. Augmentation and regularization strategies include RandAugment [15], random erasing [84] and a stochastic depth [32] ratio of 0.5.\n\nIn evaluation, we test top-1 accuracy on both ImageNet-1K V1 and V2.\n\nFine-tuning on COCO object detection We first conduct inter-mediate fine-tuning using the Objects-365 V2 dataset. In this stage, we remove the mask branch of the HTC++ framework [10,46] because there are no mask annotations. The input image resolution and window size are set as [800, 1024] and 32 \u00d7 32, respectively. In training, an AdamW [48] optimizer with initial learning rate of 1.2 \u00d7 10 \u22123 , a weight decay of 0.05 and a batch size of 96 are used, and the training length is set 67,500 steps.\n\nThen we fine-tune the HTC++ model on COCO dataset, with the mask branch randomly initialized and other model weights loaded from the Objects-365-V2 pre-trained model. In this training stage, the input image resolution is set 1536\u00d71536 with a multi-scale ratio of [0.1, 2.0]. The window size is set 32\u00d732. The AdamW [48] optimizer is employed, with an initial learning rate of 6 \u00d7 10 \u22124 , a weight decay of 0.05, and a batch size of 96, and is trained 45,000 steps.\n\nIn test, Soft-NMS [5] is used. Both window sizes of 32\u00d7 32 and 48 \u00d7 48 are considered.\n\nFine-tuning on ADE20K semantic segmentation The input image size (window size) is set 640\u00d7640 (40\u00d740). An AdamW optimizer [48] is employed, with an initial learning rate of 4 \u00d7 10 \u22125 , a weight decay of 0.05, a linear decayed learning rate scheduler with 80K iterations, a batch size of 32, and a linear warm-up of 750 iterations. For augmentations, we follow the default settings in mmsegmentation to include random horizontal flipping, random re-scaling within ratio range [0.5, 2.0] and random photometric distortion. The stochastic depth ratio is set 0.4.\n\nFine-tuning on Kinetics-400 video action recognition A 2-stage fine-tuning process is employed. In the first stage, an input resolution of 256\u00d7256\u00d78 with 16\u00d716\u00d78 window size is adopted. We employ the AdamW optimizer for 20 epochs using a cosine decayed learning rate scheduler with 2.5-epoch linear warm-up. Other training hyper-parameters are: batch-size 80, an initial learning rate of 3.6\u00d710 \u22124 , and a weight decay of 0.1.\n\nIn the second stage, we further fine-tune the model using a larger input video resolution of 320\u00d7320\u00d78 with 20\u00d720\u00d78 window size. We employ the AdamW optimizer for 5 epochs using a cosine decayed learning rate scheduler with 1-epoch linear warm-up. A batch-size of 64, an initial learning rate of 5 \u00d7 10 \u22125 and a weight decay of 0.1 are set. Figure 4 visualizes the relative position bias matrices (B \u2208 R (2M \u22121)\u00d7(2M \u22121) ) learnt by different bias computation approaches, using a SwinV2-T model. The bias matrices of the 3 heads in the first block are visualized. The left shows the bias matrices learnt by using an input image size of 256\u00d7256 and a window size of 8 \u00d7 8. The right shows the bias matrices after fine-tuning on a larger input image resolution of 512\u00d7512 and a larger window size of 16\u00d716. It turns out that the bias matrices learnt by two CPB(continuous position bias) approaches are more smoothly than that learnt by P-RPE (parameterized relative position bias). Figure 5 shows more examples using the last block of this model. \n\n\nA3. Learnt Relative Position Bias by Different Approaches\n\nFigure 1 .\n1To better scale up model capacity and window resolution, several adaptions are made on the original Swin Transformer architecture (V1): 1) A res-post-norm to replace the previous prenorm configuration; 2) A scaled cosine attention to replace the original dot product attention; 3) A log-spaced continuous relative position bias approach to replace the previous parameterized approach. Adaptions 1) and 2) make it easier for the model to scale up capacity. Adaption 3) makes the model to be transferred more effectively across window resolutions. The adapted architecture is named Swin Transformer V2.\n\nFigure 2 .\n2The Signal Propagation Plot[6,76] for various model sizes. H-size models are trained at a self-supervised learning phase, and other sizes are trained by an image classification task. * indicates that we use a 40-epoch model before it crashes.\n\nFigure 3 .\n3SwinV1-H versus SwinV2-H in training [72].\n\n\u2022\nSwinV2-T: C = 96, #. block = {2, 2, 6, 2} \u2022 SwinV2-S/B/L: C=96/128/192, #.block={2, 2, 18, 2}\n\n\u2022\nSwinV2-H: C = 352, #. block = {2, 2, 18, 2} \u2022 SwinV2-G: C = 512, #. block = {2, 2, 42, 4} For SwinV2-H and SwinV2-G, we add an additional layer normalization layer on the main branch every 6 layers. To save experimental time, we only employ SwinV2-G for large-scale experiments. SwinV2-H is employed for another parallel study about self-supervised learning [72].\n\nFigure 4 .\n4Visualization of the learnt relative position bias matrices by different approaches, using a SwinV2-T model and the 3 heads in the first block. Left: the bias matrices by pre-training on a 256\u00d7256 image and a 8\u00d78 window; Right: the bias matrices after fine-tuning using a 512\u00d7512 image size and 16\u00d716 window size. H-x indicates the x-th head.\n\n\n\u00d7 16 window size, using the original raw coordinates, the input coordinate range will be from [\u22127, 7]\u00d7[\u22127, 7] to [\u221215, 15]\u00d7[\u221215, 15]. The extrapolation ratio is 8 7 = 1.14\u00d7 of the original range. Using log-spaced coordinates, the input range will be from [\u22122.079, 2.079] \u00d7 [\u22122.079, 2.079] to [\u22122.773, 2.773] \u00d7 [\u22122.773, 2.773].\n\n\nTable 3. Comparison with previous best results on COCO object detection and instance segmentation. I(W) indicates the image and window size. ms indicate multi-scale testing is employed.Method \n\nparam \npre-train \nimages \n\npre-train \nlength (#im) \n\npre-train \nim size \n\npre-train \ntime \n\nfine-tune \nim size \n\nImageNet-1K-V1 \ntop-1 acc \n\nImaegNet-1K-V2 \ntop-1 acc \nSwinV1-B \n88M \nIN-22K-14M \n1.3B \n224 2 \n<30  \u2020 \n384 2 \n86.4 \n76.58 \nSwinV1-L \n197M \nIN-22K-14M \n1.3B \n224 2 \n<10  \u2020 \n384 2 \n87.3 \n77.46 \nViT-G [80] \n1.8B \nJFT-3B \n164B \n224 2 \n>30k \n518 2 \n90.45 \n83.33 \nV-MoE [56] \n14.7B* \nJFT-3B \n-\n224 2 \n16.8k \n518 2 \n90.35 \n-\nCoAtNet-7 [17] 2.44B \nJFT-3B \n-\n224 2 \n20.1k \n512 2 \n90.88 \n-\nSwinV2-B \n88M \nIN-22K-14M \n1.3B \n192 2 \n<30  \u2020 \n384 2 \n87.1 \n78.08 \nSwinV2-L \n197M \nIN-22K-14M \n1.3B \n192 2 \n<20  \u2020 \n384 2 \n87.7 \n78.31 \nSwinV2-G \n3.0B IN-22K-ext-70M \n3.5B \n192 2 \n<0.5k  \u2020 \n640 2 \n90.17 \n84.00 \n\nTable 2. Comparison with previous largest vision models on ImageNet-1K V1 and V2 classification. * indicates the sparse model; the \n\"pre-train time\" column is measured by the TPUv3 core days with numbers copied from the original papers.  \u2020 That of SwinV2-G is \nestimated according to training iterations and FLOPs. \n\nMethod \ntrain \nI(W) size \n\ntest \nI(W) size \n\nmini-val (AP) test-dev (AP) \n\nbox mask box mask \nCopyPaste [25] 1280(-) 1280(-) 57.0 48.9 57.3 49.1 \nSwinV1-L [46] 800(7) \nms(7) 58.0 50.4 58.7 51.1 \nYOLOR [66] 1280(-) 1280(-) \n-\n-57.3 -\nCBNet [43] \n1400(7) \nms(7) 59.6 51.8 60.1 52.3 \nDyHead [16] 1200(-) \nms(-) 60.3 -60.6 -\nSoftTeacher [74] 1280(12) ms(12) 60.7 52.5 61.3 53.0 \n\nSwinV2-L \n(HTC++) \n1536(32) \n\n1100(32) 58.8 51.1 -\n-\n1100 (48) 58.9 51.2 -\n-\nms (48) 60.2 52.1 60.8 52.7 \n\nSwinV2-G \n(HTC++) \n1536(32) \n\n1100(32) 61.7 53.3 -\n-\n1100 (48) 61.9 53.4 -\n-\nms (48) 62.5 53.7 63.1 54.4 \n\nMethod \ntrain I(W) size test I(W) size mIoU \nSwinV1-L [46] \n640(7) \n640(7) \n53.5* \nFocal-L [75] \n640(40) \n640(40) \n55.4* \nCSwin-L [21] \n640(40) \n640(40) \n55.7* \nMaskFormer [13] \n640(7) \n640(7) \n55.6* \nFaPN [33] \n640(7) \n640(7) \n56.7* \nBEiT [4] \n640(40) \n640(40) \n58.4* \nSwinV2-L \n(UperNet) \n640(40) \n640(40) \n55.9* \n\nSwinV2-G \n(UperNet) \n640(40) \n\n640(40) \n59.1 \n896 (56) \n59.3 \n896 (56) \n59.9* \n\nTable 4. Comparison with previous best results on ADE20K se-\nmantic segmentation. * indicates multi-scale testing is used. \n\n\n\n\nTable 5. Comparison with previous best results on Kinetics-400 video action classification.Method \ntrain I(W) size test I(W) size views top-1 \nViViT [2] \n-(-) \n-(-) \n4\u00d73 84.8 \nSwinV1-L [47] 480(12) 2 \u00d716(8) 480(12) 2 \u00d716(8) 10\u00d75 84.9 \nTokenLearner [57] 256(8) 2 \u00d764(64) 256(8) 2 \u00d764(64) 4\u00d73 85.4 \n\nVideo-SwinV2-G 320(20) 2 \u00d78(8) \n\n320(20) 2 \u00d78(8) 1\u00d71 83.2 \n384(24) 2 \u00d78(8) 1\u00d71 83.4 \n384(24) 2 \u00d78(8) 4\u00d75 86.8 \n\n\nThere have been a few alternative normalization configurations, such as post-normalization[65] and sandwich normalization[20]. Postnormalization harms training stability[73], and sandwich normalization sacrifices representation power due to too many normalization layers.\nAcknowledgementWe thank many colleagues at Microsoft for their help, in particular, Eric Chang, Lidong Zhou, Jing Tao, Aaron Zhang, Edward Cui, Bin Xiao, Lu Yuan, Peng Cheng, Fan Yang for useful discussion and the help on GPU resources and datasets.\nSimmim: A simple framework for masked image modeling. Anonymous, CVPR submission. Anonymous. Simmim: A simple framework for masked im- age modeling. In CVPR submission, 2022. 10\n\nVivit: A video vision transformer. Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u010di\u0107, Cordelia Schmid, 37Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u010di\u0107, and Cordelia Schmid. Vivit: A video vi- sion transformer, 2021. 3, 7\n\nLayer normalization. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. 3\n\nBeit: Bert pre-training of image transformers. Hangbo Bao, Li Dong, Furu Wei, 79Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers, 2021. 2, 7, 9\n\nSoft-nms -improving object detection with one line of code. Navaneeth Bodla, Bharat Singh, Rama Chellappa, Larry S Davis, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)910Navaneeth Bodla, Bharat Singh, Rama Chellappa, and Larry S. Davis. Soft-nms -improving object detection with one line of code. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017. 9, 10\n\nCharacterizing signal propagation to close the performance gap in unnormalized resnets. Andrew Brock, Soham De, Samuel L Smith, arXiv:2101.08692arXiv preprintAndrew Brock, Soham De, and Samuel L Smith. Character- izing signal propagation to close the performance gap in un- normalized resnets. arXiv preprint arXiv:2101.08692, 2021.\n\n. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskeverand Dario Amodei. Language models are few-shot learners, 2020. 1, 2Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan- tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand- hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad- ford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. 1, 2\n\nCascade r-cnn: Delving into high quality object detection. Zhaowei Cai, Nuno Vasconcelos, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv- ing into high quality object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 6154-6162, 2018. 9\n\nEnd-toend object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, European Conference on Computer Vision. SpringerNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to- end object detection with transformers. In European Confer- ence on Computer Vision, pages 213-229. Springer, 2020. 9\n\nHybrid task cascade for instance segmentation. Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition910Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaox- iao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, et al. Hybrid task cascade for instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4974- 4983, 2019. 9, 10\n\nKai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, arXiv:1906.07155Open mmlab detection toolbox and benchmark. arXiv preprintKai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, et al. Mmdetection: Open mmlab detection tool- box and benchmark. arXiv preprint arXiv:1906.07155, 2019.\n\nTraining deep nets with sublinear memory cost. Tianqi Chen, Bing Xu, Chiyuan Zhang, Carlos Guestrin, 25Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost, 2016. 2, 5\n\nPer-pixel classification is not all you need for semantic segmentation. Bowen Cheng, Alexander G Schwing, Alexander Kirillov, arXivBowen Cheng, Alexander G. Schwing, and Alexander Kir- illov. Per-pixel classification is not all you need for semantic segmentation. arXiv, 2021. 7\n\nTwins: Revisiting the design of spatial attention in vision transformers. Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, Chunhua Shen, Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haib- ing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers, 2021. 3\n\nRandaugment: Practical automated data augmentation with a reduced search space. Barret Ekin D Cubuk, Jonathon Zoph, Quoc V Shlens, Le, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops910Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmenta- tion with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702-703, 2020. 9, 10\n\n. P-Rpe Linear-Cpb Log-Cpb Window, P-RPE Linear-CPB Log-CPB Window: 8x8 Window: 16x16\n\nLeft: the bias matrices by pre-training on a 256\u00d7256 image and a 8\u00d78 window; Right: the bias matrices after fine-tuning using a 512\u00d7512 image size and 16\u00d716 window size. Figure 5. Visualization of the learnt relative position bias matrices by different approaches, using a SwinV2-T model and the 24 heads in the last block. H-x indicates the x-th headFigure 5. Visualization of the learnt relative position bias matrices by different approaches, using a SwinV2-T model and the 24 heads in the last block. Left: the bias matrices by pre-training on a 256\u00d7256 image and a 8\u00d78 window; Right: the bias matrices after fine-tuning using a 512\u00d7512 image size and 16\u00d716 window size. H-x indicates the x-th head.\n\nDynamic head: Unifying object detection heads with attentions. Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen, Mengchen Liu, Lu Yuan, Lei Zhang, Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen, Mengchen Liu, Lu Yuan, and Lei Zhang. Dynamic head: Unifying object detection heads with attentions, 2021. 7\n\nCoatnet: Marrying convolution and attention for all data sizes. Zihang Dai, Hanxiao Liu, Quoc V Le, Mingxing Tan, Zihang Dai, Hanxiao Liu, Quoc V. Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all data sizes, 2021. 1, 2, 3, 5, 7\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009. 6\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. 1arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 1, 2\n\nCogview: Mastering textto-image generation via transformers. Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, Jie Tang, arXiv:2105.132903arXiv preprintMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. Cogview: Mastering text- to-image generation via transformers. arXiv preprint arXiv:2105.13290, 2021. 3, 8\n\nCswin transformer: A general vision transformer backbone with cross-shaped windows. Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, Baining Guo, 37Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows, 2021. 3, 7\n\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, International Conference on Learning Representations. 23Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representa- tions, 2021. 2, 3\n\nInstaboost: Boosting instance segmentation via probability map guided copypasting. Jianhua Hao-Shu Fang, Runzhong Sun, Minghao Wang, Yong-Lu Gou, Cewu Li, Lu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionHao-Shu Fang, Jianhua Sun, Runzhong Wang, Minghao Gou, Yong-Lu Li, and Cewu Lu. Instaboost: Boosting instance segmentation via probability map guided copy- pasting. In Proceedings of the IEEE/CVF International Con- ference on Computer Vision, pages 682-691, 2019. 9\n\nSwitch transformers: Scaling to trillion parameter models with simple and efficient sparsity. William Fedus, Barret Zoph, Noam Shazeer, 1William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with sim- ple and efficient sparsity, 2021. 1, 2\n\nSimple copy-paste is a strong data augmentation method for instance segmentation. Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, D Ekin, Cubuk, V Quoc, Barret Le, Zoph, arXiv:2012.07177arXiv preprintGolnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung- Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation. arXiv preprint arXiv:2012.07177, 2020. 7\n\nNas-fpn: Learning scalable feature pyramid architecture for object detection. Golnaz Ghiasi, Tsung-Yi Lin, Quoc V Le, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionGolnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Nas-fpn: Learning scalable feature pyramid architecture for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7036- 7045, 2019. 9\n\nSelf-supervised pretraining of visual features in the wild. Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, Piotr Bojanowski, 1Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchin- sky, Ishan Misra, Armand Joulin, and Piotr Bojanowski. Self-supervised pretraining of visual features in the wild, 2021. 1, 2\n\nPiotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. Kaiming He, Georgia Gkioxari, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961-2969, 2017. 9\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. 1\n\nRelation networks for object detection. Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionHan Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object detection. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3588-3597, 2018. 3\n\nLocal relation networks for image recognition. Han Hu, Zheng Zhang, Zhenda Xie, Stephen Lin, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 3464-3473, October 2019. 3\n\nDeep networks with stochastic depth. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Q Weinberger, European conference on computer vision. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil- ian Q Weinberger. Deep networks with stochastic depth. In European conference on computer vision, pages 646-661.\n\n. Springer, 910Springer, 2016. 9, 10\n\nFapn: Feature-aligned pyramid network for dense image prediction. Shihua Huang, Zhichao Lu, Ran Cheng, Cheng He, Shihua Huang, Zhichao Lu, Ran Cheng, and Cheng He. Fapn: Feature-aligned pyramid network for dense image pre- diction, 2021. 7\n\nShuffle transformer: Rethinking spatial shuffle for vision transformer. Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, Bin Fu, Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, and Bin Fu. Shuffle transformer: Rethinking spa- tial shuffle for vision transformer, 2021. 3\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift, 2015. 3\n\nScaling laws for neural language models. Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. 2\n\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, arXiv:1705.06950The kinetics human action video dataset. arXiv preprintWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics hu- man action video dataset. arXiv preprint arXiv:1705.06950, 2017. 6\n\nRethinking positional encoding in language pre-training. Guolin Ke, Di He, Tie-Yan Liu, Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training, 2021. 3\n\nBig transfer (bit): General visual representation learning. Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby, arXiv:1912.113706arXiv preprintAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. arXiv preprint arXiv:1912.11370, 6(2):8, 2019. 1, 2\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in neural information processing systems. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural net- works. In Advances in neural information processing sys- tems, pages 1097-1105, 2012. 2\n\nGradient-based learning applied to document recognition. Yann Lecun, L\u00e9on Bottou, Yoshua Bengio, Patrick Haffner, Proceedings of the IEEE. 8611Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to document recog- nition. Proceedings of the IEEE, 86(11):2278-2324, 1998. 2\n\nLocalvit: Bringing locality to vision transformers. Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, Luc Van Gool, Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit: Bringing locality to vision trans- formers, 2021. 3\n\nCb-netv2: A composite backbone network architecture for object detection. Tingting Liang, Xiaojie Chu, Yudong Liu, Yongtao Wang, Zhi Tang, Wei Chu, Jingdong Chen, Haibin Ling, Tingting Liang, Xiaojie Chu, Yudong Liu, Yongtao Wang, Zhi Tang, Wei Chu, Jingdong Chen, and Haibin Ling. Cb- netv2: A composite backbone network architecture for ob- ject detection, 2021. 7\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European conference on computer vision. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755.\n\n. Springer, Springer, 2014. 6\n\nA closer look at local aggregation operators in point cloud analysis. Ze Liu, Han Hu, Yue Cao, Zheng Zhang, Xin Tong, 2020Ze Liu, Han Hu, Yue Cao, Zheng Zhang, and Xin Tong. A closer look at local aggregation operators in point cloud anal- ysis, 2020. 3\n\nSwin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, 10Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin trans- former: Hierarchical vision transformer using shifted win- dows, 2021. 2, 3, 4, 7, 9, 10\n\nVideo swin transformer. Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, Han Hu, 27Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer, 2021. 2, 7\n\nDecoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. 910Ilya Loshchilov and Frank Hutter. Decoupled weight de- cay regularization. In International Conference on Learning Representations, 2019. 9, 10\n\nTuring-nlg: A 17-billion-parameter language model by microsoft. Microsoft, 1Microsoft. Turing-nlg: A 17-billion-parameter language model by microsoft, 2020. 1, 2\n\nUsing deepspeed and megatron to train megatronturing nlg 530b, the world's largest and most powerful generative language model. Microsoft, 1Microsoft. Using deepspeed and megatron to train megatron- turing nlg 530b, the world's largest and most powerful gen- erative language model, 2021. 1, 2\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 6\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 13Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsuper- vised multitask learners. 2019. 1, 2, 3\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 211403Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learn- ing Research, 21(140):1-67, 2020. 1, 2, 3\n\nZero: Memory optimizations toward training trillion parameter models. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He, 25Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models, 2020. 2, 5\n\nDo imagenet classifiers generalize to imagenet?. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar, 26Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to im- agenet?, 2019. 2, 6\n\nScaling vision with sparse mixture of experts. Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr\u00e9 Susano Pinto, Daniel Keysers, Neil Houlsby, Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr\u00e9 Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mix- ture of experts, 2021. 1, 2, 3, 5, 7\n\nTokenlearner: What can 8 learned tokens do for images and videos?. Michael S Ryoo, Anurag Piergiovanni, Mostafa Arnab, Anelia Dehghani, Angelova, 27Michael S. Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia Angelova. Tokenlearner: What can 8 learned tokens do for images and videos?, 2021. 2, 7\n\nT Kristof, Pieter-Jan Sch\u00fctt, Kindermans, E Huziel, Stefan Sauceda, Alexandre Chmiela, Klaus-Robert Tkatchenko, M\u00fcller, arXiv:1706.08566Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. arXiv preprintKristof T Sch\u00fctt, Pieter-Jan Kindermans, Huziel E Sauceda, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert M\u00fcller. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. arXiv preprint arXiv:1706.08566, 2017. 3\n\nObjects365: A large-scale, high-quality dataset for object detection. Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, Jian Sun, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019. 6\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, International Conference on Learning Representations. 1K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, May 2015. 1, 2\n\nSparse r-cnn: End-to-end object detection with learnable proposals. Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, arXiv:2011.12450arXiv preprintPeize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chen- feng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end object detection with learnable proposals. arXiv preprint arXiv:2011.12450, 2020. 9\n\nGoing deeper with convolutions. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1-9, 2015.\n\nTraining data-efficient image transformers & distillation through attention. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv\u00e9 J\u00e9gou, arXiv:2012.12877arXiv preprintHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through at- tention. arXiv preprint arXiv:2012.12877, 2020. 3\n\nInstance normalization: The missing ingredient for fast stylization. Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky, Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. In- stance normalization: The missing ingredient for fast styliza- tion, 2017. 3\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998-6008, 2017. 2, 3, 8\n\nYou only learn one representation: Unified network for multiple tasks. Chien-Yao Wang, I-Hau Yeh, Hong-Yuan Mark Liao, Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. You only learn one representation: Unified network for mul- tiple tasks, 2021. 7\n\nDeep parametric continuous convolutional neural networks. Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei Pokrovsky, Raquel Urtasun, IEEE/CVF Conference on Computer Vision and Pattern Recognition. Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei Pokrovsky, and Raquel Urtasun. Deep parametric continu- ous convolutional neural networks. 2018 IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, Jun 2018.\n\nPyramid vision transformer: A versatile backbone for dense prediction without convolutions. Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao, Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra- mid vision transformer: A versatile backbone for dense pre- diction without convolutions, 2021. 3\n\nRethinking and improving relative position encoding for vision transformer. Kan Wu, Houwen Peng, Minghao Chen, Jianlong Fu, Hongyang Chao, Kan Wu, Houwen Peng, Minghao Chen, Jianlong Fu, and Hongyang Chao. Rethinking and improving relative position encoding for vision transformer, 2021. 3\n\nGroup normalization. Yuxin Wu, Kaiming He, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Yuxin Wu and Kaiming He. Group normalization. In Pro- ceedings of the European Conference on Computer Vision (ECCV), pages 3-19, 2018. 3\n\nEarly convolutions help transformers see better. Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Doll\u00e1r, Ross Girshick, arXiv:2106.14881arXiv preprintTete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Doll\u00e1r, and Ross Girshick. Early convolutions help trans- formers see better. arXiv preprint arXiv:2106.14881, 2021.\n\nSimmim: A simple framework for masked image modeling. Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, Han Hu, 6Tech reportZhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A sim- ple framework for masked image modeling. In Tech report, 2022. 5, 6, 8\n\n. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, Tie-Yan Liu, On layer normalization in the transformer architecture. 2020. 3Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the trans- former architecture. 2020. 3\n\nEnd-toend semi-supervised object detection with soft teacher. Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, Zicheng Liu, 27Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to- end semi-supervised object detection with soft teacher, 2021. 2, 7\n\nFocal self-attention for local-global interactions in vision transformers. Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, Jianfeng Gao, 37Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention for local-global interactions in vision transformers, 2021. 3, 7\n\nLeveraging batch normalization for vision transformers. Zhuliang Yao, Yue Cao, Yutong Lin, Ze Liu, Zheng Zhang, Han Hu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionZhuliang Yao, Yue Cao, Yutong Lin, Ze Liu, Zheng Zhang, and Han Hu. Leveraging batch normalization for vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 413-422, 2021. 4\n\nTokens-to-token vit: Training vision transformers from scratch on imagenet. Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, E H Francis, Jiashi Tay, Shuicheng Feng, Yan, Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet, 2021. 3\n\nLi Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, Shuicheng Yan, arXiv:2106.13112Vision outlooker for visual recognition. arXiv preprintLi Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and Shuicheng Yan. Volo: Vision outlooker for visual recog- nition. arXiv preprint arXiv:2106.13112, 2021. 3\n\nCutmix: Regularization strategy to train strong classifiers with localizable features. Sangdoo Yun, Dongyoon Han, Sanghyuk Seong Joon Oh, Junsuk Chun, Youngjoon Choe, Yoo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular- ization strategy to train strong classifiers with localizable fea- tures. In Proceedings of the IEEE/CVF International Con- ference on Computer Vision, pages 6023-6032, 2019. 9\n\nScaling vision transformers. Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, Lucas Beyer, Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu- cas Beyer. Scaling vision transformers, 2021. 1, 2, 3, 5, 7\n\nHongyi Zhang, Moustapha Cisse, David Yann N Dauphin, Lopez-Paz, arXiv:1710.09412mixup: Beyond empirical risk minimization. arXiv preprintHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. arXiv preprint arXiv:1710.09412, 2017. 9\n\nMulti-scale vision longformer: A new vision transformer for high-resolution image encoding. Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, Jianfeng Gao, Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision long- former: A new vision transformer for high-resolution image encoding, 2021. 3\n\nRethinking semantic segmentation from a sequence-to-sequence perspective with transformers. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, H S Philip, Torr, arXiv:2012.15840arXiv preprintSixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmen- tation from a sequence-to-sequence perspective with trans- formers. arXiv preprint arXiv:2012.15840, 2020. 3\n\nRandom erasing data augmentation. Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, Yi Yang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence3410Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 13001-13008, 2020. 9, 10\n\nSemantic understanding of scenes through the ade20k dataset. Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, Antonio Torralba, International Journal on Computer Vision. 6Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi- dler, Adela Barriuso, and Antonio Torralba. Semantic under- standing of scenes through the ade20k dataset. International Journal on Computer Vision, 2018. 6\n", "annotations": {"author": "[{\"end\":92,\"start\":59},{\"end\":146,\"start\":93},{\"end\":210,\"start\":147},{\"end\":250,\"start\":211},{\"end\":295,\"start\":251},{\"end\":330,\"start\":296},{\"end\":366,\"start\":331},{\"end\":403,\"start\":367},{\"end\":439,\"start\":404},{\"end\":476,\"start\":440},{\"end\":519,\"start\":477}]", "publisher": null, "author_last_name": "[{\"end\":65,\"start\":62},{\"end\":99,\"start\":97},{\"end\":157,\"start\":154},{\"end\":223,\"start\":220},{\"end\":268,\"start\":262},{\"end\":303,\"start\":300},{\"end\":339,\"start\":336},{\"end\":376,\"start\":371},{\"end\":412,\"start\":410},{\"end\":449,\"start\":445},{\"end\":492,\"start\":489}]", "author_first_name": "[{\"end\":61,\"start\":59},{\"end\":96,\"start\":93},{\"end\":153,\"start\":147},{\"end\":219,\"start\":211},{\"end\":257,\"start\":251},{\"end\":261,\"start\":258},{\"end\":299,\"start\":296},{\"end\":335,\"start\":331},{\"end\":370,\"start\":367},{\"end\":409,\"start\":404},{\"end\":444,\"start\":440},{\"end\":480,\"start\":477},{\"end\":488,\"start\":481}]", "author_affiliation": "[{\"end\":91,\"start\":67},{\"end\":145,\"start\":121},{\"end\":209,\"start\":185},{\"end\":249,\"start\":225},{\"end\":294,\"start\":270},{\"end\":329,\"start\":305},{\"end\":365,\"start\":341},{\"end\":402,\"start\":378},{\"end\":438,\"start\":414},{\"end\":475,\"start\":451},{\"end\":518,\"start\":494}]", "title": "[{\"end\":56,\"start\":1},{\"end\":575,\"start\":520}]", "venue": null, "abstract": "[{\"end\":2118,\"start\":577}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2263,\"start\":2259},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2266,\"start\":2263},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2269,\"start\":2266},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":2272,\"start\":2269},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":2275,\"start\":2272},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2278,\"start\":2275},{\"end\":2470,\"start\":2467},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2531,\"start\":2527},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":2654,\"start\":2650},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2694,\"start\":2690},{\"end\":2858,\"start\":2855},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3039,\"start\":3035},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":3042,\"start\":3039},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3138,\"start\":3134},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3141,\"start\":3138},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3144,\"start\":3141},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":3147,\"start\":3144},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":3150,\"start\":3147},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3288,\"start\":3284},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":3291,\"start\":3288},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":3294,\"start\":3291},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4967,\"start\":4963},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":4970,\"start\":4967},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":5843,\"start\":5839},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":6714,\"start\":6710},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7009,\"start\":7005},{\"end\":7012,\"start\":7009},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":7063,\"start\":7059},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7095,\"start\":7092},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7113,\"start\":7109},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":7608,\"start\":7604},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7833,\"start\":7829},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7836,\"start\":7833},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7908,\"start\":7904},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8072,\"start\":8069},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8075,\"start\":8072},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":8078,\"start\":8075},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8118,\"start\":8114},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8443,\"start\":8439},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8446,\"start\":8443},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8466,\"start\":8462},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":8642,\"start\":8638},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":8658,\"start\":8654},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8795,\"start\":8791},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8798,\"start\":8795},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9098,\"start\":9094},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9149,\"start\":9145},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9206,\"start\":9202},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":9209,\"start\":9206},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9265,\"start\":9262},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9401,\"start\":9397},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9404,\"start\":9401},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9407,\"start\":9404},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9410,\"start\":9407},{\"end\":9413,\"start\":9410},{\"end\":9416,\"start\":9413},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":9419,\"start\":9416},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":9422,\"start\":9419},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":9425,\"start\":9422},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":9428,\"start\":9425},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":9431,\"start\":9428},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9505,\"start\":9501},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":9508,\"start\":9505},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":9511,\"start\":9508},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9835,\"start\":9831},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10043,\"start\":10039},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10418,\"start\":10414},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10421,\"start\":10418},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":10704,\"start\":10700},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":10787,\"start\":10783},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10871,\"start\":10867},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10874,\"start\":10871},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":10877,\"start\":10874},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11146,\"start\":11142},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":11149,\"start\":11146},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11287,\"start\":11283},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":11290,\"start\":11287},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":11293,\"start\":11290},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":11296,\"start\":11293},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12407,\"start\":12404},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12410,\"start\":12407},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":12413,\"start\":12410},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":12416,\"start\":12413},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":12562,\"start\":12558},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12583,\"start\":12579},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19348,\"start\":19346},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19946,\"start\":19942},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":19949,\"start\":19946},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":19952,\"start\":19949},{\"end\":20029,\"start\":20025},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":20607,\"start\":20603},{\"end\":21293,\"start\":21289},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22892,\"start\":22888},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":22895,\"start\":22892},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":22923,\"start\":22919},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":22962,\"start\":22958},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23067,\"start\":23063},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23133,\"start\":23129},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":23136,\"start\":23133},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23170,\"start\":23166},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":23430,\"start\":23426},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":23548,\"start\":23544},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":23693,\"start\":23689},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":23818,\"start\":23814},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23884,\"start\":23880},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":25447,\"start\":25443},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":26158,\"start\":26154},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26622,\"start\":26619},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":27049,\"start\":27045},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":31258,\"start\":31254},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":31283,\"start\":31279},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31594,\"start\":31590},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":31606,\"start\":31602},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":31619,\"start\":31615},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":31640,\"start\":31636},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":31666,\"start\":31662},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":32102,\"start\":32098},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":32360,\"start\":32357},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":32363,\"start\":32360},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32395,\"start\":32391},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":32474,\"start\":32471},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":32477,\"start\":32474},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":32600,\"start\":32596},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":32854,\"start\":32850},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":33184,\"start\":33183},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":33629,\"start\":33626},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":34106,\"start\":34102},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":34391,\"start\":34387},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":34403,\"start\":34399},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":34416,\"start\":34412},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":34437,\"start\":34433},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":34463,\"start\":34459},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":34913,\"start\":34909},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":34916,\"start\":34913},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":34973,\"start\":34969},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":35002,\"start\":34998},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":35137,\"start\":35133},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35347,\"start\":35344},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":35530,\"start\":35526},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":36129,\"start\":36126},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":36314,\"start\":36310},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":36897,\"start\":36893},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":37259,\"start\":37255},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":37280,\"start\":37276},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":37308,\"start\":37304},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":37442,\"start\":37438},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":37767,\"start\":37763},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":37788,\"start\":37784},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":37816,\"start\":37812},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":38084,\"start\":38080},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":38087,\"start\":38084},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":38246,\"start\":38242},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":38722,\"start\":38718},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":38890,\"start\":38887},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":39083,\"start\":39079},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":41708,\"start\":41705},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":41711,\"start\":41708},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":45965,\"start\":45961},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":45996,\"start\":45992},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":46044,\"start\":46040}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41664,\"start\":41051},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41920,\"start\":41665},{\"attributes\":{\"id\":\"fig_2\"},\"end\":41976,\"start\":41921},{\"attributes\":{\"id\":\"fig_3\"},\"end\":42073,\"start\":41977},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42440,\"start\":42074},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42796,\"start\":42441},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":43125,\"start\":42797},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":45457,\"start\":43126},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":45870,\"start\":45458}]", "paragraph": "[{\"end\":3295,\"start\":2134},{\"end\":4598,\"start\":3297},{\"end\":6100,\"start\":4600},{\"end\":7115,\"start\":6102},{\"end\":7483,\"start\":7117},{\"end\":8348,\"start\":7501},{\"end\":8937,\"start\":8350},{\"end\":9652,\"start\":8939},{\"end\":10618,\"start\":9654},{\"end\":11150,\"start\":10620},{\"end\":11668,\"start\":11190},{\"end\":12323,\"start\":11729},{\"end\":12769,\"start\":12325},{\"end\":12966,\"start\":12831},{\"end\":13425,\"start\":13018},{\"end\":13859,\"start\":13427},{\"end\":14547,\"start\":13861},{\"end\":14953,\"start\":14549},{\"end\":15405,\"start\":14955},{\"end\":15698,\"start\":15407},{\"end\":16314,\"start\":15728},{\"end\":16772,\"start\":16316},{\"end\":16935,\"start\":16774},{\"end\":17466,\"start\":16937},{\"end\":17755,\"start\":17515},{\"end\":17958,\"start\":17788},{\"end\":18145,\"start\":17960},{\"end\":18629,\"start\":18172},{\"end\":18887,\"start\":18631},{\"end\":19037,\"start\":18951},{\"end\":19756,\"start\":19039},{\"end\":20318,\"start\":19789},{\"end\":20566,\"start\":20356},{\"end\":21259,\"start\":20568},{\"end\":21575,\"start\":21261},{\"end\":22188,\"start\":21577},{\"end\":22434,\"start\":22190},{\"end\":22637,\"start\":22459},{\"end\":22780,\"start\":22639},{\"end\":23068,\"start\":22817},{\"end\":23517,\"start\":23070},{\"end\":23780,\"start\":23519},{\"end\":23827,\"start\":23782},{\"end\":23907,\"start\":23829},{\"end\":23980,\"start\":23909},{\"end\":24162,\"start\":24007},{\"end\":24625,\"start\":24200},{\"end\":27298,\"start\":24627},{\"end\":29286,\"start\":27482},{\"end\":30001,\"start\":29343},{\"end\":30803,\"start\":30016},{\"end\":31062,\"start\":30846},{\"end\":31819,\"start\":31097},{\"end\":32307,\"start\":31858},{\"end\":32728,\"start\":32309},{\"end\":33669,\"start\":32761},{\"end\":33975,\"start\":33727},{\"end\":34482,\"start\":33977},{\"end\":34868,\"start\":34484},{\"end\":35417,\"start\":34894},{\"end\":35988,\"start\":35419},{\"end\":36752,\"start\":36016},{\"end\":37322,\"start\":36754},{\"end\":37830,\"start\":37324},{\"end\":37900,\"start\":37832},{\"end\":38401,\"start\":37902},{\"end\":38867,\"start\":38403},{\"end\":38955,\"start\":38869},{\"end\":39516,\"start\":38957},{\"end\":39944,\"start\":39518},{\"end\":40990,\"start\":39946}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13017,\"start\":12967},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17514,\"start\":17467},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18171,\"start\":18146},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18950,\"start\":18888}]", "table_ref": "[{\"end\":14026,\"start\":14019},{\"end\":15066,\"start\":15059},{\"end\":19523,\"start\":19516},{\"end\":24999,\"start\":24992},{\"end\":25929,\"start\":25922},{\"end\":26424,\"start\":26417},{\"end\":26863,\"start\":26856},{\"end\":27543,\"start\":27536},{\"end\":28047,\"start\":28040},{\"end\":28506,\"start\":28499},{\"end\":29353,\"start\":29346},{\"end\":29991,\"start\":29984},{\"end\":33771,\"start\":33764}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2132,\"start\":2120},{\"attributes\":{\"n\":\"2.\"},\"end\":7499,\"start\":7486},{\"end\":11188,\"start\":11153},{\"attributes\":{\"n\":\"3.\"},\"end\":11690,\"start\":11671},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11727,\"start\":11693},{\"end\":12829,\"start\":12772},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15726,\"start\":15701},{\"attributes\":{\"n\":\"3.3.\"},\"end\":17786,\"start\":17758},{\"attributes\":{\"n\":\"3.4.\"},\"end\":19787,\"start\":19759},{\"attributes\":{\"n\":\"3.5.\"},\"end\":20354,\"start\":20321},{\"attributes\":{\"n\":\"3.6.\"},\"end\":22457,\"start\":22437},{\"attributes\":{\"n\":\"4.\"},\"end\":22794,\"start\":22783},{\"attributes\":{\"n\":\"4.1.\"},\"end\":22815,\"start\":22797},{\"attributes\":{\"n\":\"4.2.\"},\"end\":24005,\"start\":23983},{\"end\":24198,\"start\":24165},{\"end\":27341,\"start\":27301},{\"end\":27373,\"start\":27344},{\"end\":27412,\"start\":27376},{\"end\":27463,\"start\":27415},{\"attributes\":{\"n\":\"4.3.\"},\"end\":27480,\"start\":27466},{\"end\":29341,\"start\":29289},{\"attributes\":{\"n\":\"5.\"},\"end\":30014,\"start\":30004},{\"end\":30844,\"start\":30806},{\"end\":31095,\"start\":31065},{\"end\":31856,\"start\":31822},{\"end\":32759,\"start\":32731},{\"end\":33725,\"start\":33672},{\"end\":34892,\"start\":34871},{\"end\":36014,\"start\":35991},{\"end\":41050,\"start\":40993},{\"end\":41062,\"start\":41052},{\"end\":41676,\"start\":41666},{\"end\":41932,\"start\":41922},{\"end\":41979,\"start\":41978},{\"end\":42076,\"start\":42075},{\"end\":42452,\"start\":42442}]", "table": "[{\"end\":45457,\"start\":43313},{\"end\":45870,\"start\":45551}]", "figure_caption": "[{\"end\":41664,\"start\":41064},{\"end\":41920,\"start\":41678},{\"end\":41976,\"start\":41934},{\"end\":42073,\"start\":41980},{\"end\":42440,\"start\":42077},{\"end\":42796,\"start\":42454},{\"end\":43125,\"start\":42799},{\"end\":43313,\"start\":43128},{\"end\":45551,\"start\":45460}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4100,\"start\":4092},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14569,\"start\":14561},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14952,\"start\":14944},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16443,\"start\":16435},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16658,\"start\":16650},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":28168,\"start\":28160},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":40295,\"start\":40287},{\"end\":40933,\"start\":40925}]", "bib_author_first_name": "[{\"end\":46613,\"start\":46607},{\"end\":46628,\"start\":46621},{\"end\":46644,\"start\":46639},{\"end\":46658,\"start\":46654},{\"end\":46669,\"start\":46664},{\"end\":46685,\"start\":46677},{\"end\":46862,\"start\":46857},{\"end\":46866,\"start\":46863},{\"end\":46876,\"start\":46871},{\"end\":46881,\"start\":46877},{\"end\":46897,\"start\":46889},{\"end\":46899,\"start\":46898},{\"end\":47047,\"start\":47041},{\"end\":47055,\"start\":47053},{\"end\":47066,\"start\":47062},{\"end\":47240,\"start\":47231},{\"end\":47254,\"start\":47248},{\"end\":47266,\"start\":47262},{\"end\":47283,\"start\":47278},{\"end\":47285,\"start\":47284},{\"end\":47748,\"start\":47742},{\"end\":47761,\"start\":47756},{\"end\":47774,\"start\":47766},{\"end\":47993,\"start\":47990},{\"end\":47995,\"start\":47994},{\"end\":48011,\"start\":48003},{\"end\":48022,\"start\":48018},{\"end\":48037,\"start\":48030},{\"end\":48052,\"start\":48047},{\"end\":48069,\"start\":48061},{\"end\":48086,\"start\":48080},{\"end\":48106,\"start\":48100},{\"end\":48120,\"start\":48114},{\"end\":48135,\"start\":48129},{\"end\":48152,\"start\":48144},{\"end\":48167,\"start\":48162},{\"end\":48190,\"start\":48182},{\"end\":48203,\"start\":48200},{\"end\":48219,\"start\":48214},{\"end\":48233,\"start\":48227},{\"end\":48248,\"start\":48242},{\"end\":48250,\"start\":48249},{\"end\":48267,\"start\":48260},{\"end\":48279,\"start\":48272},{\"end\":48299,\"start\":48288},{\"end\":48311,\"start\":48307},{\"end\":48322,\"start\":48318},{\"end\":48338,\"start\":48331},{\"end\":49126,\"start\":49119},{\"end\":49136,\"start\":49132},{\"end\":49549,\"start\":49542},{\"end\":49567,\"start\":49558},{\"end\":49582,\"start\":49575},{\"end\":49600,\"start\":49593},{\"end\":49619,\"start\":49610},{\"end\":49636,\"start\":49630},{\"end\":49983,\"start\":49980},{\"end\":49999,\"start\":49990},{\"end\":50011,\"start\":50006},{\"end\":50020,\"start\":50018},{\"end\":50036,\"start\":50028},{\"end\":50048,\"start\":50041},{\"end\":50060,\"start\":50054},{\"end\":50072,\"start\":50067},{\"end\":50086,\"start\":50078},{\"end\":50097,\"start\":50092},{\"end\":50560,\"start\":50557},{\"end\":50572,\"start\":50567},{\"end\":50588,\"start\":50579},{\"end\":50601,\"start\":50595},{\"end\":50609,\"start\":50607},{\"end\":50625,\"start\":50617},{\"end\":50637,\"start\":50630},{\"end\":50649,\"start\":50643},{\"end\":50661,\"start\":50656},{\"end\":50673,\"start\":50667},{\"end\":51032,\"start\":51026},{\"end\":51043,\"start\":51039},{\"end\":51055,\"start\":51048},{\"end\":51069,\"start\":51063},{\"end\":51276,\"start\":51271},{\"end\":51293,\"start\":51284},{\"end\":51295,\"start\":51294},{\"end\":51314,\"start\":51305},{\"end\":51563,\"start\":51553},{\"end\":51572,\"start\":51569},{\"end\":51585,\"start\":51579},{\"end\":51594,\"start\":51592},{\"end\":51609,\"start\":51602},{\"end\":51622,\"start\":51615},{\"end\":51634,\"start\":51628},{\"end\":51647,\"start\":51640},{\"end\":51930,\"start\":51924},{\"end\":51953,\"start\":51945},{\"end\":51966,\"start\":51960},{\"end\":52442,\"start\":52418},{\"end\":53277,\"start\":53271},{\"end\":53290,\"start\":53283},{\"end\":53300,\"start\":53297},{\"end\":53315,\"start\":53307},{\"end\":53330,\"start\":53322},{\"end\":53338,\"start\":53336},{\"end\":53348,\"start\":53345},{\"end\":53587,\"start\":53581},{\"end\":53600,\"start\":53593},{\"end\":53610,\"start\":53606},{\"end\":53612,\"start\":53611},{\"end\":53625,\"start\":53617},{\"end\":53827,\"start\":53824},{\"end\":53837,\"start\":53834},{\"end\":53851,\"start\":53844},{\"end\":53866,\"start\":53860},{\"end\":53874,\"start\":53871},{\"end\":53881,\"start\":53879},{\"end\":54187,\"start\":54182},{\"end\":54204,\"start\":54196},{\"end\":54218,\"start\":54212},{\"end\":54232,\"start\":54224},{\"end\":54242,\"start\":54233},{\"end\":54614,\"start\":54610},{\"end\":54627,\"start\":54621},{\"end\":54639,\"start\":54634},{\"end\":54651,\"start\":54646},{\"end\":54664,\"start\":54659},{\"end\":54673,\"start\":54671},{\"end\":54686,\"start\":54679},{\"end\":54694,\"start\":54692},{\"end\":54704,\"start\":54700},{\"end\":54718,\"start\":54711},{\"end\":54728,\"start\":54725},{\"end\":55093,\"start\":55087},{\"end\":55107,\"start\":55100},{\"end\":55121,\"start\":55113},{\"end\":55135,\"start\":55128},{\"end\":55150,\"start\":55143},{\"end\":55157,\"start\":55155},{\"end\":55168,\"start\":55164},{\"end\":55182,\"start\":55175},{\"end\":55523,\"start\":55517},{\"end\":55542,\"start\":55537},{\"end\":55559,\"start\":55550},{\"end\":55576,\"start\":55572},{\"end\":55597,\"start\":55590},{\"end\":55610,\"start\":55604},{\"end\":55631,\"start\":55624},{\"end\":55650,\"start\":55642},{\"end\":55666,\"start\":55661},{\"end\":56180,\"start\":56173},{\"end\":56203,\"start\":56195},{\"end\":56216,\"start\":56209},{\"end\":56230,\"start\":56223},{\"end\":56240,\"start\":56236},{\"end\":56746,\"start\":56739},{\"end\":56760,\"start\":56754},{\"end\":56771,\"start\":56767},{\"end\":57024,\"start\":57018},{\"end\":57036,\"start\":57033},{\"end\":57049,\"start\":57042},{\"end\":57063,\"start\":57060},{\"end\":57078,\"start\":57070},{\"end\":57085,\"start\":57084},{\"end\":57100,\"start\":57099},{\"end\":57113,\"start\":57107},{\"end\":57471,\"start\":57465},{\"end\":57488,\"start\":57480},{\"end\":57500,\"start\":57494},{\"end\":57954,\"start\":57949},{\"end\":57970,\"start\":57962},{\"end\":57986,\"start\":57978},{\"end\":58001,\"start\":57998},{\"end\":58014,\"start\":58006},{\"end\":58026,\"start\":58021},{\"end\":58038,\"start\":58032},{\"end\":58053,\"start\":58046},{\"end\":58072,\"start\":58067},{\"end\":58086,\"start\":58080},{\"end\":58100,\"start\":58095},{\"end\":58408,\"start\":58401},{\"end\":58420,\"start\":58413},{\"end\":58780,\"start\":58773},{\"end\":58792,\"start\":58785},{\"end\":58808,\"start\":58800},{\"end\":58818,\"start\":58814},{\"end\":59217,\"start\":59214},{\"end\":59229,\"start\":59222},{\"end\":59239,\"start\":59234},{\"end\":59253,\"start\":59247},{\"end\":59265,\"start\":59259},{\"end\":59673,\"start\":59670},{\"end\":59683,\"start\":59678},{\"end\":59697,\"start\":59691},{\"end\":59710,\"start\":59703},{\"end\":60113,\"start\":60110},{\"end\":60123,\"start\":60121},{\"end\":60135,\"start\":60129},{\"end\":60147,\"start\":60141},{\"end\":60163,\"start\":60155},{\"end\":60494,\"start\":60488},{\"end\":60509,\"start\":60502},{\"end\":60517,\"start\":60514},{\"end\":60530,\"start\":60525},{\"end\":60741,\"start\":60735},{\"end\":60757,\"start\":60749},{\"end\":60771,\"start\":60763},{\"end\":60780,\"start\":60777},{\"end\":60792,\"start\":60788},{\"end\":60800,\"start\":60797},{\"end\":61062,\"start\":61056},{\"end\":61079,\"start\":61070},{\"end\":61276,\"start\":61271},{\"end\":61288,\"start\":61285},{\"end\":61304,\"start\":61301},{\"end\":61318,\"start\":61315},{\"end\":61320,\"start\":61319},{\"end\":61336,\"start\":61328},{\"end\":61349,\"start\":61344},{\"end\":61362,\"start\":61357},{\"end\":61373,\"start\":61369},{\"end\":61390,\"start\":61383},{\"end\":61400,\"start\":61395},{\"end\":61606,\"start\":61602},{\"end\":61616,\"start\":61612},{\"end\":61632,\"start\":61627},{\"end\":61648,\"start\":61643},{\"end\":61661,\"start\":61656},{\"end\":61681,\"start\":61671},{\"end\":61705,\"start\":61700},{\"end\":61716,\"start\":61713},{\"end\":61730,\"start\":61724},{\"end\":61741,\"start\":61737},{\"end\":62124,\"start\":62118},{\"end\":62131,\"start\":62129},{\"end\":62143,\"start\":62136},{\"end\":62319,\"start\":62310},{\"end\":62337,\"start\":62332},{\"end\":62352,\"start\":62345},{\"end\":62363,\"start\":62359},{\"end\":62383,\"start\":62376},{\"end\":62397,\"start\":62390},{\"end\":62409,\"start\":62405},{\"end\":62745,\"start\":62741},{\"end\":62762,\"start\":62758},{\"end\":62782,\"start\":62774},{\"end\":62784,\"start\":62783},{\"end\":63110,\"start\":63106},{\"end\":63122,\"start\":63118},{\"end\":63137,\"start\":63131},{\"end\":63153,\"start\":63146},{\"end\":63424,\"start\":63419},{\"end\":63432,\"start\":63429},{\"end\":63448,\"start\":63440},{\"end\":63458,\"start\":63454},{\"end\":63471,\"start\":63468},{\"end\":63694,\"start\":63686},{\"end\":63709,\"start\":63702},{\"end\":63721,\"start\":63715},{\"end\":63734,\"start\":63727},{\"end\":63744,\"start\":63741},{\"end\":63754,\"start\":63751},{\"end\":63768,\"start\":63760},{\"end\":63781,\"start\":63775},{\"end\":64031,\"start\":64023},{\"end\":64044,\"start\":64037},{\"end\":64057,\"start\":64052},{\"end\":64073,\"start\":64068},{\"end\":64086,\"start\":64080},{\"end\":64099,\"start\":64095},{\"end\":64114,\"start\":64109},{\"end\":64133,\"start\":64123},{\"end\":64512,\"start\":64510},{\"end\":64521,\"start\":64518},{\"end\":64529,\"start\":64526},{\"end\":64540,\"start\":64535},{\"end\":64551,\"start\":64548},{\"end\":64770,\"start\":64768},{\"end\":64782,\"start\":64776},{\"end\":64791,\"start\":64788},{\"end\":64800,\"start\":64797},{\"end\":64811,\"start\":64805},{\"end\":64822,\"start\":64817},{\"end\":64837,\"start\":64830},{\"end\":64850,\"start\":64843},{\"end\":65078,\"start\":65076},{\"end\":65087,\"start\":65084},{\"end\":65097,\"start\":65094},{\"end\":65109,\"start\":65103},{\"end\":65120,\"start\":65115},{\"end\":65135,\"start\":65128},{\"end\":65144,\"start\":65141},{\"end\":65307,\"start\":65303},{\"end\":65325,\"start\":65320},{\"end\":66069,\"start\":66065},{\"end\":66083,\"start\":66079},{\"end\":66088,\"start\":66084},{\"end\":66099,\"start\":66094},{\"end\":66115,\"start\":66109},{\"end\":66131,\"start\":66124},{\"end\":66145,\"start\":66137},{\"end\":66161,\"start\":66155},{\"end\":66176,\"start\":66170},{\"end\":66191,\"start\":66185},{\"end\":66205,\"start\":66201},{\"end\":66221,\"start\":66213},{\"end\":66235,\"start\":66231},{\"end\":66570,\"start\":66566},{\"end\":66584,\"start\":66580},{\"end\":66594,\"start\":66589},{\"end\":66607,\"start\":66602},{\"end\":66619,\"start\":66614},{\"end\":66632,\"start\":66628},{\"end\":66886,\"start\":66881},{\"end\":66899,\"start\":66895},{\"end\":66913,\"start\":66909},{\"end\":66932,\"start\":66923},{\"end\":66944,\"start\":66938},{\"end\":66960,\"start\":66953},{\"end\":66974,\"start\":66969},{\"end\":66984,\"start\":66981},{\"end\":66994,\"start\":66989},{\"end\":66996,\"start\":66995},{\"end\":67400,\"start\":67394},{\"end\":67418,\"start\":67414},{\"end\":67435,\"start\":67427},{\"end\":67451,\"start\":67444},{\"end\":67663,\"start\":67655},{\"end\":67678,\"start\":67671},{\"end\":67694,\"start\":67688},{\"end\":67712,\"start\":67704},{\"end\":67911,\"start\":67905},{\"end\":67926,\"start\":67922},{\"end\":67944,\"start\":67939},{\"end\":67959,\"start\":67954},{\"end\":67977,\"start\":67969},{\"end\":68000,\"start\":67988},{\"end\":68014,\"start\":68008},{\"end\":68028,\"start\":68024},{\"end\":68319,\"start\":68312},{\"end\":68321,\"start\":68320},{\"end\":68334,\"start\":68328},{\"end\":68356,\"start\":68349},{\"end\":68370,\"start\":68364},{\"end\":68560,\"start\":68559},{\"end\":68580,\"start\":68570},{\"end\":68602,\"start\":68601},{\"end\":68617,\"start\":68611},{\"end\":68636,\"start\":68627},{\"end\":68658,\"start\":68646},{\"end\":69132,\"start\":69127},{\"end\":69145,\"start\":69139},{\"end\":69158,\"start\":69150},{\"end\":69170,\"start\":69166},{\"end\":69181,\"start\":69177},{\"end\":69193,\"start\":69186},{\"end\":69205,\"start\":69201},{\"end\":69214,\"start\":69210},{\"end\":69699,\"start\":69698},{\"end\":69711,\"start\":69710},{\"end\":70022,\"start\":70017},{\"end\":70034,\"start\":70028},{\"end\":70044,\"start\":70042},{\"end\":70055,\"start\":70052},{\"end\":70070,\"start\":70062},{\"end\":70078,\"start\":70075},{\"end\":70094,\"start\":70085},{\"end\":70108,\"start\":70105},{\"end\":70119,\"start\":70113},{\"end\":70133,\"start\":70126},{\"end\":70453,\"start\":70444},{\"end\":70466,\"start\":70463},{\"end\":70480,\"start\":70472},{\"end\":70492,\"start\":70486},{\"end\":70508,\"start\":70503},{\"end\":70523,\"start\":70515},{\"end\":70541,\"start\":70534},{\"end\":70556,\"start\":70549},{\"end\":70574,\"start\":70568},{\"end\":71088,\"start\":71084},{\"end\":71106,\"start\":71098},{\"end\":71121,\"start\":71113},{\"end\":71138,\"start\":71129},{\"end\":71155,\"start\":71146},{\"end\":71175,\"start\":71170},{\"end\":71512,\"start\":71506},{\"end\":71528,\"start\":71522},{\"end\":71544,\"start\":71538},{\"end\":71725,\"start\":71719},{\"end\":71739,\"start\":71735},{\"end\":71753,\"start\":71749},{\"end\":71767,\"start\":71762},{\"end\":71784,\"start\":71779},{\"end\":71797,\"start\":71792},{\"end\":71799,\"start\":71798},{\"end\":71813,\"start\":71807},{\"end\":71827,\"start\":71822},{\"end\":72211,\"start\":72202},{\"end\":72223,\"start\":72218},{\"end\":72243,\"start\":72229},{\"end\":72450,\"start\":72442},{\"end\":72462,\"start\":72457},{\"end\":72476,\"start\":72468},{\"end\":72487,\"start\":72481},{\"end\":72505,\"start\":72499},{\"end\":72896,\"start\":72890},{\"end\":72907,\"start\":72903},{\"end\":72918,\"start\":72913},{\"end\":72932,\"start\":72923},{\"end\":72944,\"start\":72938},{\"end\":72955,\"start\":72951},{\"end\":72967,\"start\":72963},{\"end\":72976,\"start\":72972},{\"end\":72986,\"start\":72982},{\"end\":73284,\"start\":73281},{\"end\":73295,\"start\":73289},{\"end\":73309,\"start\":73302},{\"end\":73324,\"start\":73316},{\"end\":73337,\"start\":73329},{\"end\":73522,\"start\":73517},{\"end\":73534,\"start\":73527},{\"end\":73845,\"start\":73841},{\"end\":73858,\"start\":73852},{\"end\":73870,\"start\":73866},{\"end\":73885,\"start\":73879},{\"end\":73900,\"start\":73895},{\"end\":73913,\"start\":73909},{\"end\":74192,\"start\":74186},{\"end\":74203,\"start\":74198},{\"end\":74214,\"start\":74211},{\"end\":74226,\"start\":74220},{\"end\":74239,\"start\":74232},{\"end\":74253,\"start\":74245},{\"end\":74261,\"start\":74259},{\"end\":74270,\"start\":74267},{\"end\":74475,\"start\":74469},{\"end\":74491,\"start\":74483},{\"end\":74500,\"start\":74498},{\"end\":74508,\"start\":74505},{\"end\":74522,\"start\":74516},{\"end\":74534,\"start\":74530},{\"end\":74549,\"start\":74541},{\"end\":74563,\"start\":74557},{\"end\":74574,\"start\":74569},{\"end\":74588,\"start\":74581},{\"end\":74921,\"start\":74915},{\"end\":74931,\"start\":74926},{\"end\":74942,\"start\":74939},{\"end\":74955,\"start\":74947},{\"end\":74968,\"start\":74962},{\"end\":74982,\"start\":74975},{\"end\":74993,\"start\":74988},{\"end\":75006,\"start\":74999},{\"end\":75273,\"start\":75266},{\"end\":75288,\"start\":75280},{\"end\":75302,\"start\":75293},{\"end\":75316,\"start\":75310},{\"end\":75325,\"start\":75322},{\"end\":75334,\"start\":75332},{\"end\":75349,\"start\":75341},{\"end\":75601,\"start\":75593},{\"end\":75610,\"start\":75607},{\"end\":75622,\"start\":75616},{\"end\":75630,\"start\":75628},{\"end\":75641,\"start\":75636},{\"end\":75652,\"start\":75649},{\"end\":76088,\"start\":76086},{\"end\":76102,\"start\":76095},{\"end\":76112,\"start\":76109},{\"end\":76125,\"start\":76119},{\"end\":76135,\"start\":76130},{\"end\":76147,\"start\":76141},{\"end\":76156,\"start\":76155},{\"end\":76158,\"start\":76157},{\"end\":76174,\"start\":76168},{\"end\":76189,\"start\":76180},{\"end\":76405,\"start\":76403},{\"end\":76417,\"start\":76412},{\"end\":76429,\"start\":76423},{\"end\":76443,\"start\":76437},{\"end\":76459,\"start\":76450},{\"end\":76787,\"start\":76780},{\"end\":76801,\"start\":76793},{\"end\":76815,\"start\":76807},{\"end\":76837,\"start\":76831},{\"end\":76853,\"start\":76844},{\"end\":77314,\"start\":77307},{\"end\":77330,\"start\":77321},{\"end\":77347,\"start\":77343},{\"end\":77362,\"start\":77357},{\"end\":77495,\"start\":77489},{\"end\":77512,\"start\":77503},{\"end\":77525,\"start\":77520},{\"end\":77882,\"start\":77873},{\"end\":77896,\"start\":77890},{\"end\":77909,\"start\":77902},{\"end\":77919,\"start\":77916},{\"end\":77928,\"start\":77926},{\"end\":77938,\"start\":77935},{\"end\":77954,\"start\":77946},{\"end\":78252,\"start\":78246},{\"end\":78267,\"start\":78260},{\"end\":78282,\"start\":78272},{\"end\":78296,\"start\":78289},{\"end\":78307,\"start\":78302},{\"end\":78319,\"start\":78313},{\"end\":78332,\"start\":78326},{\"end\":78345,\"start\":78337},{\"end\":78355,\"start\":78352},{\"end\":78364,\"start\":78363},{\"end\":78366,\"start\":78365},{\"end\":78727,\"start\":78723},{\"end\":78740,\"start\":78735},{\"end\":78756,\"start\":78748},{\"end\":78769,\"start\":78763},{\"end\":78776,\"start\":78774},{\"end\":79169,\"start\":79164},{\"end\":79180,\"start\":79176},{\"end\":79193,\"start\":79187},{\"end\":79204,\"start\":79200},{\"end\":79216,\"start\":79211},{\"end\":79230,\"start\":79225},{\"end\":79248,\"start\":79241}]", "bib_author_last_name": "[{\"end\":46456,\"start\":46447},{\"end\":46619,\"start\":46614},{\"end\":46637,\"start\":46629},{\"end\":46652,\"start\":46645},{\"end\":46662,\"start\":46659},{\"end\":46675,\"start\":46670},{\"end\":46692,\"start\":46686},{\"end\":46869,\"start\":46867},{\"end\":46887,\"start\":46882},{\"end\":46906,\"start\":46900},{\"end\":47051,\"start\":47048},{\"end\":47060,\"start\":47056},{\"end\":47070,\"start\":47067},{\"end\":47246,\"start\":47241},{\"end\":47260,\"start\":47255},{\"end\":47276,\"start\":47267},{\"end\":47291,\"start\":47286},{\"end\":47754,\"start\":47749},{\"end\":47764,\"start\":47762},{\"end\":47780,\"start\":47775},{\"end\":48001,\"start\":47996},{\"end\":48016,\"start\":48012},{\"end\":48028,\"start\":48023},{\"end\":48045,\"start\":48038},{\"end\":48059,\"start\":48053},{\"end\":48078,\"start\":48070},{\"end\":48098,\"start\":48087},{\"end\":48112,\"start\":48107},{\"end\":48127,\"start\":48121},{\"end\":48142,\"start\":48136},{\"end\":48160,\"start\":48153},{\"end\":48180,\"start\":48168},{\"end\":48198,\"start\":48191},{\"end\":48212,\"start\":48204},{\"end\":48225,\"start\":48220},{\"end\":48240,\"start\":48234},{\"end\":48258,\"start\":48251},{\"end\":48270,\"start\":48268},{\"end\":48286,\"start\":48280},{\"end\":48305,\"start\":48300},{\"end\":48316,\"start\":48312},{\"end\":48329,\"start\":48323},{\"end\":48345,\"start\":48339},{\"end\":49130,\"start\":49127},{\"end\":49148,\"start\":49137},{\"end\":49556,\"start\":49550},{\"end\":49573,\"start\":49568},{\"end\":49591,\"start\":49583},{\"end\":49608,\"start\":49601},{\"end\":49628,\"start\":49620},{\"end\":49646,\"start\":49637},{\"end\":49988,\"start\":49984},{\"end\":50004,\"start\":50000},{\"end\":50016,\"start\":50012},{\"end\":50026,\"start\":50021},{\"end\":50039,\"start\":50037},{\"end\":50052,\"start\":50049},{\"end\":50065,\"start\":50061},{\"end\":50076,\"start\":50073},{\"end\":50090,\"start\":50087},{\"end\":50104,\"start\":50098},{\"end\":50565,\"start\":50561},{\"end\":50577,\"start\":50573},{\"end\":50593,\"start\":50589},{\"end\":50605,\"start\":50602},{\"end\":50615,\"start\":50610},{\"end\":50628,\"start\":50626},{\"end\":50641,\"start\":50638},{\"end\":50654,\"start\":50650},{\"end\":50665,\"start\":50662},{\"end\":50676,\"start\":50674},{\"end\":51037,\"start\":51033},{\"end\":51046,\"start\":51044},{\"end\":51061,\"start\":51056},{\"end\":51078,\"start\":51070},{\"end\":51282,\"start\":51277},{\"end\":51303,\"start\":51296},{\"end\":51323,\"start\":51315},{\"end\":51567,\"start\":51564},{\"end\":51577,\"start\":51573},{\"end\":51590,\"start\":51586},{\"end\":51600,\"start\":51595},{\"end\":51613,\"start\":51610},{\"end\":51626,\"start\":51623},{\"end\":51638,\"start\":51635},{\"end\":51652,\"start\":51648},{\"end\":51943,\"start\":51931},{\"end\":51958,\"start\":51954},{\"end\":51973,\"start\":51967},{\"end\":51977,\"start\":51975},{\"end\":52449,\"start\":52443},{\"end\":53281,\"start\":53278},{\"end\":53295,\"start\":53291},{\"end\":53305,\"start\":53301},{\"end\":53320,\"start\":53316},{\"end\":53334,\"start\":53331},{\"end\":53343,\"start\":53339},{\"end\":53354,\"start\":53349},{\"end\":53591,\"start\":53588},{\"end\":53604,\"start\":53601},{\"end\":53615,\"start\":53613},{\"end\":53629,\"start\":53626},{\"end\":53832,\"start\":53828},{\"end\":53842,\"start\":53838},{\"end\":53858,\"start\":53852},{\"end\":53869,\"start\":53867},{\"end\":53877,\"start\":53875},{\"end\":53889,\"start\":53882},{\"end\":54194,\"start\":54188},{\"end\":54210,\"start\":54205},{\"end\":54222,\"start\":54219},{\"end\":54247,\"start\":54243},{\"end\":54619,\"start\":54615},{\"end\":54632,\"start\":54628},{\"end\":54644,\"start\":54640},{\"end\":54657,\"start\":54652},{\"end\":54669,\"start\":54665},{\"end\":54677,\"start\":54674},{\"end\":54690,\"start\":54687},{\"end\":54698,\"start\":54695},{\"end\":54709,\"start\":54705},{\"end\":54723,\"start\":54719},{\"end\":54733,\"start\":54729},{\"end\":55098,\"start\":55094},{\"end\":55111,\"start\":55108},{\"end\":55126,\"start\":55122},{\"end\":55141,\"start\":55136},{\"end\":55153,\"start\":55151},{\"end\":55162,\"start\":55158},{\"end\":55173,\"start\":55169},{\"end\":55186,\"start\":55183},{\"end\":55535,\"start\":55524},{\"end\":55548,\"start\":55543},{\"end\":55570,\"start\":55560},{\"end\":55588,\"start\":55577},{\"end\":55602,\"start\":55598},{\"end\":55622,\"start\":55611},{\"end\":55640,\"start\":55632},{\"end\":55659,\"start\":55651},{\"end\":55674,\"start\":55667},{\"end\":56193,\"start\":56181},{\"end\":56207,\"start\":56204},{\"end\":56221,\"start\":56217},{\"end\":56234,\"start\":56231},{\"end\":56243,\"start\":56241},{\"end\":56247,\"start\":56245},{\"end\":56752,\"start\":56747},{\"end\":56765,\"start\":56761},{\"end\":56779,\"start\":56772},{\"end\":57031,\"start\":57025},{\"end\":57040,\"start\":57037},{\"end\":57058,\"start\":57050},{\"end\":57068,\"start\":57064},{\"end\":57082,\"start\":57079},{\"end\":57090,\"start\":57086},{\"end\":57097,\"start\":57092},{\"end\":57105,\"start\":57101},{\"end\":57116,\"start\":57114},{\"end\":57122,\"start\":57118},{\"end\":57478,\"start\":57472},{\"end\":57492,\"start\":57489},{\"end\":57503,\"start\":57501},{\"end\":57960,\"start\":57955},{\"end\":57976,\"start\":57971},{\"end\":57996,\"start\":57987},{\"end\":58004,\"start\":58002},{\"end\":58019,\"start\":58015},{\"end\":58030,\"start\":58027},{\"end\":58044,\"start\":58039},{\"end\":58065,\"start\":58054},{\"end\":58078,\"start\":58073},{\"end\":58093,\"start\":58087},{\"end\":58111,\"start\":58101},{\"end\":58411,\"start\":58409},{\"end\":58429,\"start\":58421},{\"end\":58783,\"start\":58781},{\"end\":58798,\"start\":58793},{\"end\":58812,\"start\":58809},{\"end\":58822,\"start\":58819},{\"end\":59220,\"start\":59218},{\"end\":59232,\"start\":59230},{\"end\":59245,\"start\":59240},{\"end\":59257,\"start\":59254},{\"end\":59269,\"start\":59266},{\"end\":59676,\"start\":59674},{\"end\":59689,\"start\":59684},{\"end\":59701,\"start\":59698},{\"end\":59714,\"start\":59711},{\"end\":60119,\"start\":60114},{\"end\":60127,\"start\":60124},{\"end\":60139,\"start\":60136},{\"end\":60153,\"start\":60148},{\"end\":60174,\"start\":60164},{\"end\":60394,\"start\":60386},{\"end\":60500,\"start\":60495},{\"end\":60512,\"start\":60510},{\"end\":60523,\"start\":60518},{\"end\":60533,\"start\":60531},{\"end\":60747,\"start\":60742},{\"end\":60761,\"start\":60758},{\"end\":60775,\"start\":60772},{\"end\":60786,\"start\":60781},{\"end\":60795,\"start\":60793},{\"end\":60803,\"start\":60801},{\"end\":61068,\"start\":61063},{\"end\":61087,\"start\":61080},{\"end\":61283,\"start\":61277},{\"end\":61299,\"start\":61289},{\"end\":61313,\"start\":61305},{\"end\":61326,\"start\":61321},{\"end\":61342,\"start\":61337},{\"end\":61355,\"start\":61350},{\"end\":61367,\"start\":61363},{\"end\":61381,\"start\":61374},{\"end\":61393,\"start\":61391},{\"end\":61407,\"start\":61401},{\"end\":61610,\"start\":61607},{\"end\":61625,\"start\":61617},{\"end\":61641,\"start\":61633},{\"end\":61654,\"start\":61649},{\"end\":61669,\"start\":61662},{\"end\":61698,\"start\":61682},{\"end\":61711,\"start\":61706},{\"end\":61722,\"start\":61717},{\"end\":61735,\"start\":61731},{\"end\":61748,\"start\":61742},{\"end\":62127,\"start\":62125},{\"end\":62134,\"start\":62132},{\"end\":62147,\"start\":62144},{\"end\":62330,\"start\":62320},{\"end\":62343,\"start\":62338},{\"end\":62357,\"start\":62353},{\"end\":62374,\"start\":62364},{\"end\":62388,\"start\":62384},{\"end\":62403,\"start\":62398},{\"end\":62417,\"start\":62410},{\"end\":62756,\"start\":62746},{\"end\":62772,\"start\":62763},{\"end\":62791,\"start\":62785},{\"end\":63116,\"start\":63111},{\"end\":63129,\"start\":63123},{\"end\":63144,\"start\":63138},{\"end\":63161,\"start\":63154},{\"end\":63427,\"start\":63425},{\"end\":63438,\"start\":63433},{\"end\":63452,\"start\":63449},{\"end\":63466,\"start\":63459},{\"end\":63480,\"start\":63472},{\"end\":63700,\"start\":63695},{\"end\":63713,\"start\":63710},{\"end\":63725,\"start\":63722},{\"end\":63739,\"start\":63735},{\"end\":63749,\"start\":63745},{\"end\":63758,\"start\":63755},{\"end\":63773,\"start\":63769},{\"end\":63786,\"start\":63782},{\"end\":64035,\"start\":64032},{\"end\":64050,\"start\":64045},{\"end\":64066,\"start\":64058},{\"end\":64078,\"start\":64074},{\"end\":64093,\"start\":64087},{\"end\":64107,\"start\":64100},{\"end\":64121,\"start\":64115},{\"end\":64141,\"start\":64134},{\"end\":64419,\"start\":64411},{\"end\":64516,\"start\":64513},{\"end\":64524,\"start\":64522},{\"end\":64533,\"start\":64530},{\"end\":64546,\"start\":64541},{\"end\":64556,\"start\":64552},{\"end\":64774,\"start\":64771},{\"end\":64786,\"start\":64783},{\"end\":64795,\"start\":64792},{\"end\":64803,\"start\":64801},{\"end\":64815,\"start\":64812},{\"end\":64828,\"start\":64823},{\"end\":64841,\"start\":64838},{\"end\":64854,\"start\":64851},{\"end\":65082,\"start\":65079},{\"end\":65092,\"start\":65088},{\"end\":65101,\"start\":65098},{\"end\":65113,\"start\":65110},{\"end\":65126,\"start\":65121},{\"end\":65139,\"start\":65136},{\"end\":65147,\"start\":65145},{\"end\":65318,\"start\":65308},{\"end\":65332,\"start\":65326},{\"end\":65609,\"start\":65600},{\"end\":65836,\"start\":65827},{\"end\":66077,\"start\":66070},{\"end\":66092,\"start\":66089},{\"end\":66107,\"start\":66100},{\"end\":66122,\"start\":66116},{\"end\":66135,\"start\":66132},{\"end\":66153,\"start\":66146},{\"end\":66168,\"start\":66162},{\"end\":66183,\"start\":66177},{\"end\":66199,\"start\":66192},{\"end\":66211,\"start\":66206},{\"end\":66229,\"start\":66222},{\"end\":66245,\"start\":66236},{\"end\":66578,\"start\":66571},{\"end\":66587,\"start\":66585},{\"end\":66600,\"start\":66595},{\"end\":66612,\"start\":66608},{\"end\":66626,\"start\":66620},{\"end\":66642,\"start\":66633},{\"end\":66893,\"start\":66887},{\"end\":66907,\"start\":66900},{\"end\":66921,\"start\":66914},{\"end\":66936,\"start\":66933},{\"end\":66951,\"start\":66945},{\"end\":66967,\"start\":66961},{\"end\":66979,\"start\":66975},{\"end\":66987,\"start\":66985},{\"end\":67000,\"start\":66997},{\"end\":67412,\"start\":67401},{\"end\":67425,\"start\":67419},{\"end\":67442,\"start\":67436},{\"end\":67454,\"start\":67452},{\"end\":67669,\"start\":67664},{\"end\":67686,\"start\":67679},{\"end\":67702,\"start\":67695},{\"end\":67720,\"start\":67713},{\"end\":67920,\"start\":67912},{\"end\":67937,\"start\":67927},{\"end\":67952,\"start\":67945},{\"end\":67967,\"start\":67960},{\"end\":67986,\"start\":67978},{\"end\":68006,\"start\":68001},{\"end\":68022,\"start\":68015},{\"end\":68036,\"start\":68029},{\"end\":68326,\"start\":68322},{\"end\":68347,\"start\":68335},{\"end\":68362,\"start\":68357},{\"end\":68379,\"start\":68371},{\"end\":68389,\"start\":68381},{\"end\":68568,\"start\":68561},{\"end\":68587,\"start\":68581},{\"end\":68599,\"start\":68589},{\"end\":68609,\"start\":68603},{\"end\":68625,\"start\":68618},{\"end\":68644,\"start\":68637},{\"end\":68669,\"start\":68659},{\"end\":68677,\"start\":68671},{\"end\":69137,\"start\":69133},{\"end\":69148,\"start\":69146},{\"end\":69164,\"start\":69159},{\"end\":69175,\"start\":69171},{\"end\":69184,\"start\":69182},{\"end\":69199,\"start\":69194},{\"end\":69208,\"start\":69206},{\"end\":69218,\"start\":69215},{\"end\":69708,\"start\":69700},{\"end\":69721,\"start\":69712},{\"end\":70026,\"start\":70023},{\"end\":70040,\"start\":70035},{\"end\":70050,\"start\":70045},{\"end\":70060,\"start\":70056},{\"end\":70073,\"start\":70071},{\"end\":70083,\"start\":70079},{\"end\":70103,\"start\":70095},{\"end\":70111,\"start\":70109},{\"end\":70124,\"start\":70120},{\"end\":70138,\"start\":70134},{\"end\":70461,\"start\":70454},{\"end\":70470,\"start\":70467},{\"end\":70484,\"start\":70481},{\"end\":70501,\"start\":70493},{\"end\":70513,\"start\":70509},{\"end\":70532,\"start\":70524},{\"end\":70547,\"start\":70542},{\"end\":70566,\"start\":70557},{\"end\":70585,\"start\":70575},{\"end\":71096,\"start\":71089},{\"end\":71111,\"start\":71107},{\"end\":71127,\"start\":71122},{\"end\":71144,\"start\":71139},{\"end\":71168,\"start\":71156},{\"end\":71181,\"start\":71176},{\"end\":71520,\"start\":71513},{\"end\":71536,\"start\":71529},{\"end\":71554,\"start\":71545},{\"end\":71733,\"start\":71726},{\"end\":71747,\"start\":71740},{\"end\":71760,\"start\":71754},{\"end\":71777,\"start\":71768},{\"end\":71790,\"start\":71785},{\"end\":71805,\"start\":71800},{\"end\":71820,\"start\":71814},{\"end\":71838,\"start\":71828},{\"end\":72216,\"start\":72212},{\"end\":72227,\"start\":72224},{\"end\":72248,\"start\":72244},{\"end\":72455,\"start\":72451},{\"end\":72466,\"start\":72463},{\"end\":72479,\"start\":72477},{\"end\":72497,\"start\":72488},{\"end\":72513,\"start\":72506},{\"end\":72901,\"start\":72897},{\"end\":72911,\"start\":72908},{\"end\":72921,\"start\":72919},{\"end\":72936,\"start\":72933},{\"end\":72949,\"start\":72945},{\"end\":72961,\"start\":72956},{\"end\":72970,\"start\":72968},{\"end\":72980,\"start\":72977},{\"end\":72991,\"start\":72987},{\"end\":73287,\"start\":73285},{\"end\":73300,\"start\":73296},{\"end\":73314,\"start\":73310},{\"end\":73327,\"start\":73325},{\"end\":73342,\"start\":73338},{\"end\":73525,\"start\":73523},{\"end\":73537,\"start\":73535},{\"end\":73850,\"start\":73846},{\"end\":73864,\"start\":73859},{\"end\":73877,\"start\":73871},{\"end\":73893,\"start\":73886},{\"end\":73907,\"start\":73901},{\"end\":73922,\"start\":73914},{\"end\":74196,\"start\":74193},{\"end\":74209,\"start\":74204},{\"end\":74218,\"start\":74215},{\"end\":74230,\"start\":74227},{\"end\":74243,\"start\":74240},{\"end\":74257,\"start\":74254},{\"end\":74265,\"start\":74262},{\"end\":74273,\"start\":74271},{\"end\":74481,\"start\":74476},{\"end\":74496,\"start\":74492},{\"end\":74503,\"start\":74501},{\"end\":74514,\"start\":74509},{\"end\":74528,\"start\":74523},{\"end\":74539,\"start\":74535},{\"end\":74555,\"start\":74550},{\"end\":74567,\"start\":74564},{\"end\":74579,\"start\":74575},{\"end\":74592,\"start\":74589},{\"end\":74924,\"start\":74922},{\"end\":74937,\"start\":74932},{\"end\":74945,\"start\":74943},{\"end\":74960,\"start\":74956},{\"end\":74973,\"start\":74969},{\"end\":74986,\"start\":74983},{\"end\":74997,\"start\":74994},{\"end\":75010,\"start\":75007},{\"end\":75278,\"start\":75274},{\"end\":75291,\"start\":75289},{\"end\":75308,\"start\":75303},{\"end\":75320,\"start\":75317},{\"end\":75330,\"start\":75326},{\"end\":75339,\"start\":75335},{\"end\":75353,\"start\":75350},{\"end\":75605,\"start\":75602},{\"end\":75614,\"start\":75611},{\"end\":75626,\"start\":75623},{\"end\":75634,\"start\":75631},{\"end\":75647,\"start\":75642},{\"end\":75655,\"start\":75653},{\"end\":76093,\"start\":76089},{\"end\":76107,\"start\":76103},{\"end\":76117,\"start\":76113},{\"end\":76128,\"start\":76126},{\"end\":76139,\"start\":76136},{\"end\":76153,\"start\":76148},{\"end\":76166,\"start\":76159},{\"end\":76178,\"start\":76175},{\"end\":76194,\"start\":76190},{\"end\":76199,\"start\":76196},{\"end\":76410,\"start\":76406},{\"end\":76421,\"start\":76418},{\"end\":76435,\"start\":76430},{\"end\":76448,\"start\":76444},{\"end\":76463,\"start\":76460},{\"end\":76791,\"start\":76788},{\"end\":76805,\"start\":76802},{\"end\":76829,\"start\":76816},{\"end\":76842,\"start\":76838},{\"end\":76858,\"start\":76854},{\"end\":76863,\"start\":76860},{\"end\":77319,\"start\":77315},{\"end\":77341,\"start\":77331},{\"end\":77355,\"start\":77348},{\"end\":77368,\"start\":77363},{\"end\":77501,\"start\":77496},{\"end\":77518,\"start\":77513},{\"end\":77540,\"start\":77526},{\"end\":77551,\"start\":77542},{\"end\":77888,\"start\":77883},{\"end\":77900,\"start\":77897},{\"end\":77914,\"start\":77910},{\"end\":77924,\"start\":77920},{\"end\":77933,\"start\":77929},{\"end\":77944,\"start\":77939},{\"end\":77958,\"start\":77955},{\"end\":78258,\"start\":78253},{\"end\":78270,\"start\":78268},{\"end\":78287,\"start\":78283},{\"end\":78300,\"start\":78297},{\"end\":78311,\"start\":78308},{\"end\":78324,\"start\":78320},{\"end\":78335,\"start\":78333},{\"end\":78350,\"start\":78346},{\"end\":78361,\"start\":78356},{\"end\":78373,\"start\":78367},{\"end\":78379,\"start\":78375},{\"end\":78733,\"start\":78728},{\"end\":78746,\"start\":78741},{\"end\":78761,\"start\":78757},{\"end\":78772,\"start\":78770},{\"end\":78781,\"start\":78777},{\"end\":79174,\"start\":79170},{\"end\":79185,\"start\":79181},{\"end\":79198,\"start\":79194},{\"end\":79209,\"start\":79205},{\"end\":79223,\"start\":79217},{\"end\":79239,\"start\":79231},{\"end\":79257,\"start\":79249}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":244346275},\"end\":46570,\"start\":46393},{\"attributes\":{\"id\":\"b1\"},\"end\":46834,\"start\":46572},{\"attributes\":{\"id\":\"b2\"},\"end\":46992,\"start\":46836},{\"attributes\":{\"id\":\"b3\"},\"end\":47169,\"start\":46994},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":15155826},\"end\":47652,\"start\":47171},{\"attributes\":{\"doi\":\"arXiv:2101.08692\",\"id\":\"b5\"},\"end\":47986,\"start\":47654},{\"attributes\":{\"id\":\"b6\"},\"end\":49058,\"start\":47988},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206596979},\"end\":49494,\"start\":49060},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":218889832},\"end\":49931,\"start\":49496},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":58981777},\"end\":50555,\"start\":49933},{\"attributes\":{\"doi\":\"arXiv:1906.07155\",\"id\":\"b10\"},\"end\":50977,\"start\":50557},{\"attributes\":{\"id\":\"b11\"},\"end\":51197,\"start\":50979},{\"attributes\":{\"id\":\"b12\"},\"end\":51477,\"start\":51199},{\"attributes\":{\"id\":\"b13\"},\"end\":51842,\"start\":51479},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":208006202},\"end\":52414,\"start\":51844},{\"attributes\":{\"id\":\"b15\"},\"end\":52501,\"start\":52416},{\"attributes\":{\"id\":\"b16\"},\"end\":53206,\"start\":52503},{\"attributes\":{\"id\":\"b17\"},\"end\":53515,\"start\":53208},{\"attributes\":{\"id\":\"b18\"},\"end\":53769,\"start\":53517},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":57246310},\"end\":54180,\"start\":53771},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b20\"},\"end\":54547,\"start\":54182},{\"attributes\":{\"doi\":\"arXiv:2105.13290\",\"id\":\"b21\"},\"end\":55001,\"start\":54549},{\"attributes\":{\"id\":\"b22\"},\"end\":55389,\"start\":55003},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":225039882},\"end\":56088,\"start\":55391},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":201126039},\"end\":56643,\"start\":56090},{\"attributes\":{\"id\":\"b25\"},\"end\":56934,\"start\":56645},{\"attributes\":{\"doi\":\"arXiv:2012.07177\",\"id\":\"b26\"},\"end\":57385,\"start\":56936},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":118634258},\"end\":57887,\"start\":57387},{\"attributes\":{\"id\":\"b28\"},\"end\":58354,\"start\":57889},{\"attributes\":{\"id\":\"b29\"},\"end\":58725,\"start\":58356},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":206594692},\"end\":59172,\"start\":58727},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":37158713},\"end\":59621,\"start\":59174},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":131776850},\"end\":60071,\"start\":59623},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":6773885},\"end\":60382,\"start\":60073},{\"attributes\":{\"id\":\"b34\"},\"end\":60420,\"start\":60384},{\"attributes\":{\"id\":\"b35\"},\"end\":60661,\"start\":60422},{\"attributes\":{\"id\":\"b36\"},\"end\":60960,\"start\":60663},{\"attributes\":{\"id\":\"b37\"},\"end\":61228,\"start\":60962},{\"attributes\":{\"id\":\"b38\"},\"end\":61600,\"start\":61230},{\"attributes\":{\"doi\":\"arXiv:1705.06950\",\"id\":\"b39\"},\"end\":62059,\"start\":61602},{\"attributes\":{\"id\":\"b40\"},\"end\":62248,\"start\":62061},{\"attributes\":{\"doi\":\"arXiv:1912.11370\",\"id\":\"b41\"},\"end\":62674,\"start\":62250},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":195908774},\"end\":63047,\"start\":62676},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":14542261},\"end\":63365,\"start\":63049},{\"attributes\":{\"id\":\"b44\"},\"end\":63610,\"start\":63367},{\"attributes\":{\"id\":\"b45\"},\"end\":63978,\"start\":63612},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":14113767},\"end\":64407,\"start\":63980},{\"attributes\":{\"id\":\"b47\"},\"end\":64438,\"start\":64409},{\"attributes\":{\"id\":\"b48\"},\"end\":64693,\"start\":64440},{\"attributes\":{\"id\":\"b49\"},\"end\":65050,\"start\":64695},{\"attributes\":{\"id\":\"b50\"},\"end\":65262,\"start\":65052},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":53592270},\"end\":65534,\"start\":65264},{\"attributes\":{\"id\":\"b52\"},\"end\":65697,\"start\":65536},{\"attributes\":{\"id\":\"b53\"},\"end\":65992,\"start\":65699},{\"attributes\":{\"id\":\"b54\"},\"end\":66511,\"start\":65994},{\"attributes\":{\"id\":\"b55\"},\"end\":66796,\"start\":66513},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":204838007},\"end\":67322,\"start\":66798},{\"attributes\":{\"id\":\"b57\"},\"end\":67604,\"start\":67324},{\"attributes\":{\"id\":\"b58\"},\"end\":67856,\"start\":67606},{\"attributes\":{\"id\":\"b59\"},\"end\":68243,\"start\":67858},{\"attributes\":{\"id\":\"b60\"},\"end\":68557,\"start\":68245},{\"attributes\":{\"doi\":\"arXiv:1706.08566\",\"id\":\"b61\"},\"end\":69055,\"start\":68559},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":207967883},\"end\":69628,\"start\":69057},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":14124313},\"end\":69947,\"start\":69630},{\"attributes\":{\"doi\":\"arXiv:2011.12450\",\"id\":\"b64\"},\"end\":70410,\"start\":69949},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":206592484},\"end\":71005,\"start\":70412},{\"attributes\":{\"doi\":\"arXiv:2012.12877\",\"id\":\"b66\"},\"end\":71435,\"start\":71007},{\"attributes\":{\"id\":\"b67\"},\"end\":71690,\"start\":71437},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":13756489},\"end\":72129,\"start\":71692},{\"attributes\":{\"id\":\"b69\"},\"end\":72382,\"start\":72131},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":49367415},\"end\":72796,\"start\":72384},{\"attributes\":{\"id\":\"b71\"},\"end\":73203,\"start\":72798},{\"attributes\":{\"id\":\"b72\"},\"end\":73494,\"start\":73205},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":4076251},\"end\":73790,\"start\":73496},{\"attributes\":{\"doi\":\"arXiv:2106.14881\",\"id\":\"b74\"},\"end\":74130,\"start\":73792},{\"attributes\":{\"id\":\"b75\"},\"end\":74465,\"start\":74132},{\"attributes\":{\"id\":\"b76\"},\"end\":74851,\"start\":74467},{\"attributes\":{\"id\":\"b77\"},\"end\":75189,\"start\":74853},{\"attributes\":{\"id\":\"b78\"},\"end\":75535,\"start\":75191},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":244532018},\"end\":76008,\"start\":75537},{\"attributes\":{\"id\":\"b80\"},\"end\":76401,\"start\":76010},{\"attributes\":{\"doi\":\"arXiv:2106.13112\",\"id\":\"b81\"},\"end\":76691,\"start\":76403},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":152282661},\"end\":77276,\"start\":76693},{\"attributes\":{\"id\":\"b83\"},\"end\":77487,\"start\":77278},{\"attributes\":{\"doi\":\"arXiv:1710.09412\",\"id\":\"b84\"},\"end\":77779,\"start\":77489},{\"attributes\":{\"id\":\"b85\"},\"end\":78152,\"start\":77781},{\"attributes\":{\"doi\":\"arXiv:2012.15840\",\"id\":\"b86\"},\"end\":78687,\"start\":78154},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":2035600},\"end\":79101,\"start\":78689},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":11371972},\"end\":79515,\"start\":79103}]", "bib_title": "[{\"end\":46445,\"start\":46393},{\"end\":47229,\"start\":47171},{\"end\":49117,\"start\":49060},{\"end\":49540,\"start\":49496},{\"end\":49978,\"start\":49933},{\"end\":51922,\"start\":51844},{\"end\":52671,\"start\":52503},{\"end\":53822,\"start\":53771},{\"end\":55515,\"start\":55391},{\"end\":56171,\"start\":56090},{\"end\":57463,\"start\":57387},{\"end\":58399,\"start\":58356},{\"end\":58771,\"start\":58727},{\"end\":59212,\"start\":59174},{\"end\":59668,\"start\":59623},{\"end\":60108,\"start\":60073},{\"end\":62739,\"start\":62676},{\"end\":63104,\"start\":63049},{\"end\":64021,\"start\":63980},{\"end\":65301,\"start\":65264},{\"end\":66879,\"start\":66798},{\"end\":69125,\"start\":69057},{\"end\":69696,\"start\":69630},{\"end\":70442,\"start\":70412},{\"end\":71717,\"start\":71692},{\"end\":72440,\"start\":72384},{\"end\":73515,\"start\":73496},{\"end\":75591,\"start\":75537},{\"end\":76778,\"start\":76693},{\"end\":78721,\"start\":78689},{\"end\":79162,\"start\":79103}]", "bib_author": "[{\"end\":46458,\"start\":46447},{\"end\":46621,\"start\":46607},{\"end\":46639,\"start\":46621},{\"end\":46654,\"start\":46639},{\"end\":46664,\"start\":46654},{\"end\":46677,\"start\":46664},{\"end\":46694,\"start\":46677},{\"end\":46871,\"start\":46857},{\"end\":46889,\"start\":46871},{\"end\":46908,\"start\":46889},{\"end\":47053,\"start\":47041},{\"end\":47062,\"start\":47053},{\"end\":47072,\"start\":47062},{\"end\":47248,\"start\":47231},{\"end\":47262,\"start\":47248},{\"end\":47278,\"start\":47262},{\"end\":47293,\"start\":47278},{\"end\":47756,\"start\":47742},{\"end\":47766,\"start\":47756},{\"end\":47782,\"start\":47766},{\"end\":48003,\"start\":47990},{\"end\":48018,\"start\":48003},{\"end\":48030,\"start\":48018},{\"end\":48047,\"start\":48030},{\"end\":48061,\"start\":48047},{\"end\":48080,\"start\":48061},{\"end\":48100,\"start\":48080},{\"end\":48114,\"start\":48100},{\"end\":48129,\"start\":48114},{\"end\":48144,\"start\":48129},{\"end\":48162,\"start\":48144},{\"end\":48182,\"start\":48162},{\"end\":48200,\"start\":48182},{\"end\":48214,\"start\":48200},{\"end\":48227,\"start\":48214},{\"end\":48242,\"start\":48227},{\"end\":48260,\"start\":48242},{\"end\":48272,\"start\":48260},{\"end\":48288,\"start\":48272},{\"end\":48307,\"start\":48288},{\"end\":48318,\"start\":48307},{\"end\":48331,\"start\":48318},{\"end\":48347,\"start\":48331},{\"end\":49132,\"start\":49119},{\"end\":49150,\"start\":49132},{\"end\":49558,\"start\":49542},{\"end\":49575,\"start\":49558},{\"end\":49593,\"start\":49575},{\"end\":49610,\"start\":49593},{\"end\":49630,\"start\":49610},{\"end\":49648,\"start\":49630},{\"end\":49990,\"start\":49980},{\"end\":50006,\"start\":49990},{\"end\":50018,\"start\":50006},{\"end\":50028,\"start\":50018},{\"end\":50041,\"start\":50028},{\"end\":50054,\"start\":50041},{\"end\":50067,\"start\":50054},{\"end\":50078,\"start\":50067},{\"end\":50092,\"start\":50078},{\"end\":50106,\"start\":50092},{\"end\":50567,\"start\":50557},{\"end\":50579,\"start\":50567},{\"end\":50595,\"start\":50579},{\"end\":50607,\"start\":50595},{\"end\":50617,\"start\":50607},{\"end\":50630,\"start\":50617},{\"end\":50643,\"start\":50630},{\"end\":50656,\"start\":50643},{\"end\":50667,\"start\":50656},{\"end\":50678,\"start\":50667},{\"end\":51039,\"start\":51026},{\"end\":51048,\"start\":51039},{\"end\":51063,\"start\":51048},{\"end\":51080,\"start\":51063},{\"end\":51284,\"start\":51271},{\"end\":51305,\"start\":51284},{\"end\":51325,\"start\":51305},{\"end\":51569,\"start\":51553},{\"end\":51579,\"start\":51569},{\"end\":51592,\"start\":51579},{\"end\":51602,\"start\":51592},{\"end\":51615,\"start\":51602},{\"end\":51628,\"start\":51615},{\"end\":51640,\"start\":51628},{\"end\":51654,\"start\":51640},{\"end\":51945,\"start\":51924},{\"end\":51960,\"start\":51945},{\"end\":51975,\"start\":51960},{\"end\":51979,\"start\":51975},{\"end\":52451,\"start\":52418},{\"end\":53283,\"start\":53271},{\"end\":53297,\"start\":53283},{\"end\":53307,\"start\":53297},{\"end\":53322,\"start\":53307},{\"end\":53336,\"start\":53322},{\"end\":53345,\"start\":53336},{\"end\":53356,\"start\":53345},{\"end\":53593,\"start\":53581},{\"end\":53606,\"start\":53593},{\"end\":53617,\"start\":53606},{\"end\":53631,\"start\":53617},{\"end\":53834,\"start\":53824},{\"end\":53844,\"start\":53834},{\"end\":53860,\"start\":53844},{\"end\":53871,\"start\":53860},{\"end\":53879,\"start\":53871},{\"end\":53891,\"start\":53879},{\"end\":54196,\"start\":54182},{\"end\":54212,\"start\":54196},{\"end\":54224,\"start\":54212},{\"end\":54249,\"start\":54224},{\"end\":54621,\"start\":54610},{\"end\":54634,\"start\":54621},{\"end\":54646,\"start\":54634},{\"end\":54659,\"start\":54646},{\"end\":54671,\"start\":54659},{\"end\":54679,\"start\":54671},{\"end\":54692,\"start\":54679},{\"end\":54700,\"start\":54692},{\"end\":54711,\"start\":54700},{\"end\":54725,\"start\":54711},{\"end\":54735,\"start\":54725},{\"end\":55100,\"start\":55087},{\"end\":55113,\"start\":55100},{\"end\":55128,\"start\":55113},{\"end\":55143,\"start\":55128},{\"end\":55155,\"start\":55143},{\"end\":55164,\"start\":55155},{\"end\":55175,\"start\":55164},{\"end\":55188,\"start\":55175},{\"end\":55537,\"start\":55517},{\"end\":55550,\"start\":55537},{\"end\":55572,\"start\":55550},{\"end\":55590,\"start\":55572},{\"end\":55604,\"start\":55590},{\"end\":55624,\"start\":55604},{\"end\":55642,\"start\":55624},{\"end\":55661,\"start\":55642},{\"end\":55676,\"start\":55661},{\"end\":56195,\"start\":56173},{\"end\":56209,\"start\":56195},{\"end\":56223,\"start\":56209},{\"end\":56236,\"start\":56223},{\"end\":56245,\"start\":56236},{\"end\":56249,\"start\":56245},{\"end\":56754,\"start\":56739},{\"end\":56767,\"start\":56754},{\"end\":56781,\"start\":56767},{\"end\":57033,\"start\":57018},{\"end\":57042,\"start\":57033},{\"end\":57060,\"start\":57042},{\"end\":57070,\"start\":57060},{\"end\":57084,\"start\":57070},{\"end\":57092,\"start\":57084},{\"end\":57099,\"start\":57092},{\"end\":57107,\"start\":57099},{\"end\":57118,\"start\":57107},{\"end\":57124,\"start\":57118},{\"end\":57480,\"start\":57465},{\"end\":57494,\"start\":57480},{\"end\":57505,\"start\":57494},{\"end\":57962,\"start\":57949},{\"end\":57978,\"start\":57962},{\"end\":57998,\"start\":57978},{\"end\":58006,\"start\":57998},{\"end\":58021,\"start\":58006},{\"end\":58032,\"start\":58021},{\"end\":58046,\"start\":58032},{\"end\":58067,\"start\":58046},{\"end\":58080,\"start\":58067},{\"end\":58095,\"start\":58080},{\"end\":58113,\"start\":58095},{\"end\":58413,\"start\":58401},{\"end\":58431,\"start\":58413},{\"end\":58785,\"start\":58773},{\"end\":58800,\"start\":58785},{\"end\":58814,\"start\":58800},{\"end\":58824,\"start\":58814},{\"end\":59222,\"start\":59214},{\"end\":59234,\"start\":59222},{\"end\":59247,\"start\":59234},{\"end\":59259,\"start\":59247},{\"end\":59271,\"start\":59259},{\"end\":59678,\"start\":59670},{\"end\":59691,\"start\":59678},{\"end\":59703,\"start\":59691},{\"end\":59716,\"start\":59703},{\"end\":60121,\"start\":60110},{\"end\":60129,\"start\":60121},{\"end\":60141,\"start\":60129},{\"end\":60155,\"start\":60141},{\"end\":60176,\"start\":60155},{\"end\":60396,\"start\":60386},{\"end\":60502,\"start\":60488},{\"end\":60514,\"start\":60502},{\"end\":60525,\"start\":60514},{\"end\":60535,\"start\":60525},{\"end\":60749,\"start\":60735},{\"end\":60763,\"start\":60749},{\"end\":60777,\"start\":60763},{\"end\":60788,\"start\":60777},{\"end\":60797,\"start\":60788},{\"end\":60805,\"start\":60797},{\"end\":61070,\"start\":61056},{\"end\":61089,\"start\":61070},{\"end\":61285,\"start\":61271},{\"end\":61301,\"start\":61285},{\"end\":61315,\"start\":61301},{\"end\":61328,\"start\":61315},{\"end\":61344,\"start\":61328},{\"end\":61357,\"start\":61344},{\"end\":61369,\"start\":61357},{\"end\":61383,\"start\":61369},{\"end\":61395,\"start\":61383},{\"end\":61409,\"start\":61395},{\"end\":61612,\"start\":61602},{\"end\":61627,\"start\":61612},{\"end\":61643,\"start\":61627},{\"end\":61656,\"start\":61643},{\"end\":61671,\"start\":61656},{\"end\":61700,\"start\":61671},{\"end\":61713,\"start\":61700},{\"end\":61724,\"start\":61713},{\"end\":61737,\"start\":61724},{\"end\":61750,\"start\":61737},{\"end\":62129,\"start\":62118},{\"end\":62136,\"start\":62129},{\"end\":62149,\"start\":62136},{\"end\":62332,\"start\":62310},{\"end\":62345,\"start\":62332},{\"end\":62359,\"start\":62345},{\"end\":62376,\"start\":62359},{\"end\":62390,\"start\":62376},{\"end\":62405,\"start\":62390},{\"end\":62419,\"start\":62405},{\"end\":62758,\"start\":62741},{\"end\":62774,\"start\":62758},{\"end\":62793,\"start\":62774},{\"end\":63118,\"start\":63106},{\"end\":63131,\"start\":63118},{\"end\":63146,\"start\":63131},{\"end\":63163,\"start\":63146},{\"end\":63429,\"start\":63419},{\"end\":63440,\"start\":63429},{\"end\":63454,\"start\":63440},{\"end\":63468,\"start\":63454},{\"end\":63482,\"start\":63468},{\"end\":63702,\"start\":63686},{\"end\":63715,\"start\":63702},{\"end\":63727,\"start\":63715},{\"end\":63741,\"start\":63727},{\"end\":63751,\"start\":63741},{\"end\":63760,\"start\":63751},{\"end\":63775,\"start\":63760},{\"end\":63788,\"start\":63775},{\"end\":64037,\"start\":64023},{\"end\":64052,\"start\":64037},{\"end\":64068,\"start\":64052},{\"end\":64080,\"start\":64068},{\"end\":64095,\"start\":64080},{\"end\":64109,\"start\":64095},{\"end\":64123,\"start\":64109},{\"end\":64143,\"start\":64123},{\"end\":64421,\"start\":64411},{\"end\":64518,\"start\":64510},{\"end\":64526,\"start\":64518},{\"end\":64535,\"start\":64526},{\"end\":64548,\"start\":64535},{\"end\":64558,\"start\":64548},{\"end\":64776,\"start\":64768},{\"end\":64788,\"start\":64776},{\"end\":64797,\"start\":64788},{\"end\":64805,\"start\":64797},{\"end\":64817,\"start\":64805},{\"end\":64830,\"start\":64817},{\"end\":64843,\"start\":64830},{\"end\":64856,\"start\":64843},{\"end\":65084,\"start\":65076},{\"end\":65094,\"start\":65084},{\"end\":65103,\"start\":65094},{\"end\":65115,\"start\":65103},{\"end\":65128,\"start\":65115},{\"end\":65141,\"start\":65128},{\"end\":65149,\"start\":65141},{\"end\":65320,\"start\":65303},{\"end\":65334,\"start\":65320},{\"end\":65611,\"start\":65600},{\"end\":65838,\"start\":65827},{\"end\":66079,\"start\":66065},{\"end\":66094,\"start\":66079},{\"end\":66109,\"start\":66094},{\"end\":66124,\"start\":66109},{\"end\":66137,\"start\":66124},{\"end\":66155,\"start\":66137},{\"end\":66170,\"start\":66155},{\"end\":66185,\"start\":66170},{\"end\":66201,\"start\":66185},{\"end\":66213,\"start\":66201},{\"end\":66231,\"start\":66213},{\"end\":66247,\"start\":66231},{\"end\":66580,\"start\":66566},{\"end\":66589,\"start\":66580},{\"end\":66602,\"start\":66589},{\"end\":66614,\"start\":66602},{\"end\":66628,\"start\":66614},{\"end\":66644,\"start\":66628},{\"end\":66895,\"start\":66881},{\"end\":66909,\"start\":66895},{\"end\":66923,\"start\":66909},{\"end\":66938,\"start\":66923},{\"end\":66953,\"start\":66938},{\"end\":66969,\"start\":66953},{\"end\":66981,\"start\":66969},{\"end\":66989,\"start\":66981},{\"end\":67002,\"start\":66989},{\"end\":67414,\"start\":67394},{\"end\":67427,\"start\":67414},{\"end\":67444,\"start\":67427},{\"end\":67456,\"start\":67444},{\"end\":67671,\"start\":67655},{\"end\":67688,\"start\":67671},{\"end\":67704,\"start\":67688},{\"end\":67722,\"start\":67704},{\"end\":67922,\"start\":67905},{\"end\":67939,\"start\":67922},{\"end\":67954,\"start\":67939},{\"end\":67969,\"start\":67954},{\"end\":67988,\"start\":67969},{\"end\":68008,\"start\":67988},{\"end\":68024,\"start\":68008},{\"end\":68038,\"start\":68024},{\"end\":68328,\"start\":68312},{\"end\":68349,\"start\":68328},{\"end\":68364,\"start\":68349},{\"end\":68381,\"start\":68364},{\"end\":68391,\"start\":68381},{\"end\":68570,\"start\":68559},{\"end\":68589,\"start\":68570},{\"end\":68601,\"start\":68589},{\"end\":68611,\"start\":68601},{\"end\":68627,\"start\":68611},{\"end\":68646,\"start\":68627},{\"end\":68671,\"start\":68646},{\"end\":68679,\"start\":68671},{\"end\":69139,\"start\":69127},{\"end\":69150,\"start\":69139},{\"end\":69166,\"start\":69150},{\"end\":69177,\"start\":69166},{\"end\":69186,\"start\":69177},{\"end\":69201,\"start\":69186},{\"end\":69210,\"start\":69201},{\"end\":69220,\"start\":69210},{\"end\":69710,\"start\":69698},{\"end\":69723,\"start\":69710},{\"end\":70028,\"start\":70017},{\"end\":70042,\"start\":70028},{\"end\":70052,\"start\":70042},{\"end\":70062,\"start\":70052},{\"end\":70075,\"start\":70062},{\"end\":70085,\"start\":70075},{\"end\":70105,\"start\":70085},{\"end\":70113,\"start\":70105},{\"end\":70126,\"start\":70113},{\"end\":70140,\"start\":70126},{\"end\":70463,\"start\":70444},{\"end\":70472,\"start\":70463},{\"end\":70486,\"start\":70472},{\"end\":70503,\"start\":70486},{\"end\":70515,\"start\":70503},{\"end\":70534,\"start\":70515},{\"end\":70549,\"start\":70534},{\"end\":70568,\"start\":70549},{\"end\":70587,\"start\":70568},{\"end\":71098,\"start\":71084},{\"end\":71113,\"start\":71098},{\"end\":71129,\"start\":71113},{\"end\":71146,\"start\":71129},{\"end\":71170,\"start\":71146},{\"end\":71183,\"start\":71170},{\"end\":71522,\"start\":71506},{\"end\":71538,\"start\":71522},{\"end\":71556,\"start\":71538},{\"end\":71735,\"start\":71719},{\"end\":71749,\"start\":71735},{\"end\":71762,\"start\":71749},{\"end\":71779,\"start\":71762},{\"end\":71792,\"start\":71779},{\"end\":71807,\"start\":71792},{\"end\":71822,\"start\":71807},{\"end\":71840,\"start\":71822},{\"end\":72218,\"start\":72202},{\"end\":72229,\"start\":72218},{\"end\":72250,\"start\":72229},{\"end\":72457,\"start\":72442},{\"end\":72468,\"start\":72457},{\"end\":72481,\"start\":72468},{\"end\":72499,\"start\":72481},{\"end\":72515,\"start\":72499},{\"end\":72903,\"start\":72890},{\"end\":72913,\"start\":72903},{\"end\":72923,\"start\":72913},{\"end\":72938,\"start\":72923},{\"end\":72951,\"start\":72938},{\"end\":72963,\"start\":72951},{\"end\":72972,\"start\":72963},{\"end\":72982,\"start\":72972},{\"end\":72993,\"start\":72982},{\"end\":73289,\"start\":73281},{\"end\":73302,\"start\":73289},{\"end\":73316,\"start\":73302},{\"end\":73329,\"start\":73316},{\"end\":73344,\"start\":73329},{\"end\":73527,\"start\":73517},{\"end\":73539,\"start\":73527},{\"end\":73852,\"start\":73841},{\"end\":73866,\"start\":73852},{\"end\":73879,\"start\":73866},{\"end\":73895,\"start\":73879},{\"end\":73909,\"start\":73895},{\"end\":73924,\"start\":73909},{\"end\":74198,\"start\":74186},{\"end\":74211,\"start\":74198},{\"end\":74220,\"start\":74211},{\"end\":74232,\"start\":74220},{\"end\":74245,\"start\":74232},{\"end\":74259,\"start\":74245},{\"end\":74267,\"start\":74259},{\"end\":74275,\"start\":74267},{\"end\":74483,\"start\":74469},{\"end\":74498,\"start\":74483},{\"end\":74505,\"start\":74498},{\"end\":74516,\"start\":74505},{\"end\":74530,\"start\":74516},{\"end\":74541,\"start\":74530},{\"end\":74557,\"start\":74541},{\"end\":74569,\"start\":74557},{\"end\":74581,\"start\":74569},{\"end\":74594,\"start\":74581},{\"end\":74926,\"start\":74915},{\"end\":74939,\"start\":74926},{\"end\":74947,\"start\":74939},{\"end\":74962,\"start\":74947},{\"end\":74975,\"start\":74962},{\"end\":74988,\"start\":74975},{\"end\":74999,\"start\":74988},{\"end\":75012,\"start\":74999},{\"end\":75280,\"start\":75266},{\"end\":75293,\"start\":75280},{\"end\":75310,\"start\":75293},{\"end\":75322,\"start\":75310},{\"end\":75332,\"start\":75322},{\"end\":75341,\"start\":75332},{\"end\":75355,\"start\":75341},{\"end\":75607,\"start\":75593},{\"end\":75616,\"start\":75607},{\"end\":75628,\"start\":75616},{\"end\":75636,\"start\":75628},{\"end\":75649,\"start\":75636},{\"end\":75657,\"start\":75649},{\"end\":76095,\"start\":76086},{\"end\":76109,\"start\":76095},{\"end\":76119,\"start\":76109},{\"end\":76130,\"start\":76119},{\"end\":76141,\"start\":76130},{\"end\":76155,\"start\":76141},{\"end\":76168,\"start\":76155},{\"end\":76180,\"start\":76168},{\"end\":76196,\"start\":76180},{\"end\":76201,\"start\":76196},{\"end\":76412,\"start\":76403},{\"end\":76423,\"start\":76412},{\"end\":76437,\"start\":76423},{\"end\":76450,\"start\":76437},{\"end\":76465,\"start\":76450},{\"end\":76793,\"start\":76780},{\"end\":76807,\"start\":76793},{\"end\":76831,\"start\":76807},{\"end\":76844,\"start\":76831},{\"end\":76860,\"start\":76844},{\"end\":76865,\"start\":76860},{\"end\":77321,\"start\":77307},{\"end\":77343,\"start\":77321},{\"end\":77357,\"start\":77343},{\"end\":77370,\"start\":77357},{\"end\":77503,\"start\":77489},{\"end\":77520,\"start\":77503},{\"end\":77542,\"start\":77520},{\"end\":77553,\"start\":77542},{\"end\":77890,\"start\":77873},{\"end\":77902,\"start\":77890},{\"end\":77916,\"start\":77902},{\"end\":77926,\"start\":77916},{\"end\":77935,\"start\":77926},{\"end\":77946,\"start\":77935},{\"end\":77960,\"start\":77946},{\"end\":78260,\"start\":78246},{\"end\":78272,\"start\":78260},{\"end\":78289,\"start\":78272},{\"end\":78302,\"start\":78289},{\"end\":78313,\"start\":78302},{\"end\":78326,\"start\":78313},{\"end\":78337,\"start\":78326},{\"end\":78352,\"start\":78337},{\"end\":78363,\"start\":78352},{\"end\":78375,\"start\":78363},{\"end\":78381,\"start\":78375},{\"end\":78735,\"start\":78723},{\"end\":78748,\"start\":78735},{\"end\":78763,\"start\":78748},{\"end\":78774,\"start\":78763},{\"end\":78783,\"start\":78774},{\"end\":79176,\"start\":79164},{\"end\":79187,\"start\":79176},{\"end\":79200,\"start\":79187},{\"end\":79211,\"start\":79200},{\"end\":79225,\"start\":79211},{\"end\":79241,\"start\":79225},{\"end\":79259,\"start\":79241}]", "bib_venue": "[{\"end\":46473,\"start\":46458},{\"end\":46605,\"start\":46572},{\"end\":46855,\"start\":46836},{\"end\":47039,\"start\":46994},{\"end\":47367,\"start\":47293},{\"end\":47740,\"start\":47654},{\"end\":49227,\"start\":49150},{\"end\":49686,\"start\":49648},{\"end\":50187,\"start\":50106},{\"end\":50736,\"start\":50694},{\"end\":51024,\"start\":50979},{\"end\":51269,\"start\":51199},{\"end\":51551,\"start\":51479},{\"end\":52070,\"start\":51979},{\"end\":52825,\"start\":52673},{\"end\":53269,\"start\":53208},{\"end\":53579,\"start\":53517},{\"end\":53954,\"start\":53891},{\"end\":54339,\"start\":54265},{\"end\":54608,\"start\":54549},{\"end\":55085,\"start\":55003},{\"end\":55728,\"start\":55676},{\"end\":56320,\"start\":56249},{\"end\":56737,\"start\":56645},{\"end\":57016,\"start\":56936},{\"end\":57586,\"start\":57505},{\"end\":57947,\"start\":57889},{\"end\":58498,\"start\":58431},{\"end\":58901,\"start\":58824},{\"end\":59348,\"start\":59271},{\"end\":59794,\"start\":59716},{\"end\":60214,\"start\":60176},{\"end\":60486,\"start\":60422},{\"end\":60733,\"start\":60663},{\"end\":61054,\"start\":60962},{\"end\":61269,\"start\":61230},{\"end\":61805,\"start\":61766},{\"end\":62116,\"start\":62061},{\"end\":62308,\"start\":62250},{\"end\":62842,\"start\":62793},{\"end\":63186,\"start\":63163},{\"end\":63417,\"start\":63367},{\"end\":63684,\"start\":63612},{\"end\":64181,\"start\":64143},{\"end\":64508,\"start\":64440},{\"end\":64766,\"start\":64695},{\"end\":65074,\"start\":65052},{\"end\":65386,\"start\":65334},{\"end\":65598,\"start\":65536},{\"end\":65825,\"start\":65699},{\"end\":66063,\"start\":65994},{\"end\":66564,\"start\":66513},{\"end\":67038,\"start\":67002},{\"end\":67392,\"start\":67324},{\"end\":67653,\"start\":67606},{\"end\":67903,\"start\":67858},{\"end\":68310,\"start\":68245},{\"end\":68785,\"start\":68695},{\"end\":69298,\"start\":69220},{\"end\":69775,\"start\":69723},{\"end\":70015,\"start\":69949},{\"end\":70664,\"start\":70587},{\"end\":71082,\"start\":71007},{\"end\":71504,\"start\":71437},{\"end\":71889,\"start\":71840},{\"end\":72200,\"start\":72131},{\"end\":72577,\"start\":72515},{\"end\":72888,\"start\":72798},{\"end\":73279,\"start\":73205},{\"end\":73603,\"start\":73539},{\"end\":73839,\"start\":73792},{\"end\":74184,\"start\":74132},{\"end\":74913,\"start\":74853},{\"end\":75264,\"start\":75191},{\"end\":75728,\"start\":75657},{\"end\":76084,\"start\":76010},{\"end\":76520,\"start\":76481},{\"end\":76936,\"start\":76865},{\"end\":77305,\"start\":77278},{\"end\":77610,\"start\":77569},{\"end\":77871,\"start\":77781},{\"end\":78244,\"start\":78154},{\"end\":78844,\"start\":78783},{\"end\":79299,\"start\":79259},{\"end\":47428,\"start\":47369},{\"end\":49291,\"start\":49229},{\"end\":50255,\"start\":50189},{\"end\":52148,\"start\":52072},{\"end\":56378,\"start\":56322},{\"end\":57654,\"start\":57588},{\"end\":58552,\"start\":58500},{\"end\":58965,\"start\":58903},{\"end\":59412,\"start\":59350},{\"end\":59859,\"start\":59796},{\"end\":69363,\"start\":69300},{\"end\":70728,\"start\":70666},{\"end\":73654,\"start\":73605},{\"end\":75786,\"start\":75730},{\"end\":76994,\"start\":76938},{\"end\":78892,\"start\":78846}]"}}}, "year": 2023, "month": 12, "day": 17}