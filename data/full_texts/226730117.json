{"id": 226730117, "updated": "2022-01-18 03:03:39.701", "metadata": {"title": "Importance-Aware Data Selection and Resource Allocation in Federated Edge Learning System", "authors": "[{\"middle\":[],\"last\":\"He\",\"first\":\"Yinghui\"},{\"middle\":[],\"last\":\"Ren\",\"first\":\"Jinke\"},{\"middle\":[],\"last\":\"Yu\",\"first\":\"Guanding\"},{\"middle\":[],\"last\":\"Yuan\",\"first\":\"Jiantao\"}]", "venue": "IEEE Transactions on Vehicular Technology", "journal": "IEEE Transactions on Vehicular Technology", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "The implementation of artificial intelligence (AI) in wireless networks is becoming more and more popular because of the growing number of mobile devices and the availability of huge amount of data. However, directly transmitting data for centralized learning will cause long communication latency owing to the limited communication resource and may incur severe privacy issue as well. To address these issues, we consider the federated edge learning (FEEL) system in this paper and develop an importance-aware joint data selection and resource allocation algorithm to maximize the learning efficiency. Aiming at selecting important data for local training, we first analyze the relation between loss decay and gradient norm, which indicates that larger gradient norm generally leads to faster learning speed. Based on this, a learning efficiency maximization problem is formulated by jointly considering the communication resource allocation and data selection. The closed-form results for optimal communication resource allocation and data selection are both developed, where some insights are also highlighted. Furthermore, an optimal algorithm with low computational complexity is developed to obtain the optimal end-to-end latency in one training period. We show that the sample size should be set to its upper limit in order to maximize the learning performance. Finally, we conduct extensive experiments on three popular convolutional neural network (CNN) models. The results show that the proposed algorithm can effectively reduce the training latency and improve the learning accuracy as compared with some benchmark algorithms.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3048492641", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tvt/HeRYY20", "doi": "10.1109/tvt.2020.3015268"}}, "content": {"source": {"pdf_hash": "e194de376cd7050826a516ba7f3fe16cb019fe05", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "415fed13543ab87f2b360f1ae44a13ab3eae88f2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e194de376cd7050826a516ba7f3fe16cb019fe05.txt", "contents": "\nImportance-Aware Data Selection and Resource Allocation in Federated Edge Learning System\nNOVEMBER 2020\n\nYinghui He \nStudent Member, IEEEJinke Ren \nSenior Member, IEEEGuanding Yu \nJiantao Yuan \nImportance-Aware Data Selection and Resource Allocation in Federated Edge Learning System\n\nIEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY\n6911NOVEMBER 202010.1109/TVT.2020.301526813593Index Terms-Federated edge learninglearning efficiencylearning accuracydata selectiondata importanceresource allocation\nThe implementation of artificial intelligence (AI) in wireless networks is becoming more and more popular because of the growing number of mobile devices and the availability of huge amount of data. However, directly transmitting data for centralized learning will cause long communication latency owing to the limited communication resource and may incur severe privacy issue as well. To address these issues, we consider the federated edge learning (FEEL) system in this paper and develop an importance-aware joint data selection and resource allocation algorithm to maximize the learning efficiency. Aiming at selecting important data for local training, we first analyze the relation between loss decay and gradient norm, which indicates that larger gradient norm generally leads to faster learning speed. Based on this, a learning efficiency maximization problem is formulated by jointly considering the communication resource allocation and data selection. The closed-form results for optimal communication resource allocation and data selection are both developed, where some insights are also highlighted. Furthermore, an optimal algorithm with low computational complexity is developed to obtain the optimal end-to-end latency in one training period. We show that the sample size should be set to its upper limit in order to maximize the learning performance. Finally, we conduct extensive experiments on three popular convolutional neural network (CNN) models. The results show that the proposed algorithm can effectively reduce the training latency and improve the learning accuracy as compared with some benchmark algorithms.\n\nI. INTRODUCTION\n\nO VER the past few years, artificial intelligence (AI) has achieved remarkable success in various areas, such as face recognition, image classification, and natural language processing [1]. AI has also been widely adopted in communication networks for improving the communication performance [2], [3]. The success of AI mainly comes from the large amount of data that are collected for training. However, data in wireless Manuscript  networks are generally distributed over a large amount of mobile devices, which can contribute to the network intelligentization for the future 6G network [4], [5]. To fully utilize these data, conventional methods request devices to upload the raw data to a remote cloud server for centralized learning. However, direct data transmission would suffer from two major disadvantages, i.e., the privacy disclosure and the long communication latency, and will eventually degrade the learning performance, such as convergence time and learning accuracy. To address both issues, federated edge learning (FEEL) [6]- [8] has been recently proposed by combining federated learning (FL) [9], a specific distributed training framework, with the mobile edge computing (MEC) [10]. By periodically collecting the local learning updates (either gradient vectors or model parameters) at the network edge, FEEL not only preserves user privacy but also reduces the communication delay. However, calculating and transmitting learning updates may still cause large computation and communication overheads, i.e., high energy consumption and long latency, due to the limited communication resource and computation capacity of the mobile device. To deal with this challenge, several recent works [11]- [16] have investigated the communication-efficient FEEL. A joint bandwidth allocation and scheduling algorithm was developed in [11] to attain certain model accuracy by minimizing the total latency. A broadband analog aggregation technique was proposed in [12] to achieve a low-latency FEEL system based on the over-the-air computation technique and two communication-and-learning tradeoffs were revealed therein. Aiming at reducing energy consumption, the authors in [13] proposed an energy-efficient radio resource management strategy by optimizing bandwidth allocation and user scheduling in each training period. Besides, the authors in [14] investigated the energy-efficient radio resource management for analog aggregation in the FEEL system, where an online energyaware dynamic worker scheduling policy was proposed under a long-term energy constraint. The tradeoff between energy consumption and end-to-end latency in the FEEL system was studied in both [15] and [16] and some closed-form results were also derived.\n\nThe above works mainly aim at reducing the energy consumption and end-to-end latency. However, the learning performance in the FEEL system cannot be guaranteed due to the dynamic channel fading. Towards this end, several works [17], [18] have studied the FEEL system from the perspective of learning performance improvement in wireless fading scenarios. The authors in [17] analyzed the impact of packet errors on the 0018-9545 \u00a9 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\n\nSee https://www.ieee.org/publications/rights/index.html for more information.\n\nlearning performance and minimized the loss function by jointly considering the communication resource allocation and user selection. A novel learning criterion, namely learning efficiency, was proposed in [18], where the batchsize was optimized to dynamically adapt to the wireless channel condition and device computation capacity to improve the learning performance. The aforementioned works mainly focus on reducing the communication consumption or improving the learning performance by joint resource allocation and user selection, where the specific data structure is not exploited. However, different data are not equally important to the learning process. To further improve the learning performance, a straightforward way is to select only a part of important data based on their importance level, such as the loss value [19], the change in parameters [20], and the gradient [21], [22]. Some prior works have used the data importance for packet retransmission and user scheduling [23], [24]. In [23], the authors proposed a data-importance aware automatic-repeatrequest for both support vector machine (SVM) and convolutional neural networks (CNNs), where the data importance is measured by the uncertainty. Later, based on the elegant communication-learning relation between the signal-to-noise ratio (SNR) and the data importance, an importance-aware user scheduling was developed for the edge learning system in [24] and some principles were proposed to achieve fast convergence.\n\nAlthough the aforementioned works have developed several data importance indicators, none of them has considered to improve the learning efficiency by exploiting data importance in the FEEL system. Inspired by this, we propose a joint data selection and communication resource allocation algorithm based on the data importance to reduce end-to-end latency and improve learning efficiency in the FEEL system. Our study shows that the communication resource should be allocated dynamically based on wireless channel condition and data importance. The main contributions of this work are summarized as follows.\n\nr We theoretically analyze the impact of the gradient norm on the loss decay, which drives us to use the square of estimated gradient norm after the forward propagation step as the data importance indicator. In this way, the computation latency for local training can be greatly reduced by properly selecting important data.\n\nr To improve the learning performance, we formulate a learning efficiency maximization problem by jointly considering the communication resource allocation and data selection, which is difficult to solve. To tackle this challenging problem, we first develop the optimal data selection strategy and communication resource allocation for given end-to-end latency and sample size. Then, the optimal end-to-end latency can be found by the Golden-section search algorithm with low computational complexity.\n\nr Through theoretical analysis, we find that the expected learning efficiency increases with the sample size, which implies that the sample size should be set to its upper limit to maximize the learning performance. Finally, test results on three popular CNN models show that the proposed scheme can reduce the training latency and improve the learning accuracy at the same time. Moreover, its generalization ability is also demonstrated. The rest of this paper is organized as follows. In Section II, we introduce the FEEL system with data selection and analyze the delay in each training period. In Section III, we propose a data importance criterion based on the loss decay and formulate an optimization problem to maximize the learning efficiency. The optimal resource allocation and data selection policy is developed in Section IV. Section V presents the test results and the whole paper is concluded in Section VI.\n\n\nII. SYSTEM MODEL AND DELAY ANALYSIS\n\nIn this section, we will introduce an FEEL system with data selection. After that, the detailed procedures and the corresponding latency in each training period are analyzed.\n\n\nA. FEEL System\n\nIn an FEEL system, N devices, denoted by the set N = {1, 2, . . . , N}, collaborate with an edge cloud located at the base station (BS) for training an identical CNN model, as shown in Fig. 1. To achieve it, each device collects data and the dataset of device n is denoted by D n = {(x 1 , y 1 ), (x 2 , y 2 ), . . . , (x M n , y M n )}, where M n is the size of the n-th device's dataset. In the training process, each device first calculates the local gradient based on its sampled data and then uploads the local gradient to the edge cloud for gradient aggregation. After that, the edge cloud broadcasts the global gradient to all devices and each device updates its CNN model based on the global gradient. However, due to the limited device computation capacity, calculating local gradient is usually timeconsuming. Therefore, we are motivated to propose a scheme to reduce the computation consumption by selecting the important data for local training.\n\n\nB. CNN Model\n\nIn this paper, we use \u03a8(x, w) to represent the CNN model with parameter vector w and measure the training error of data sample (x i , y i ) by the loss function (\u03a8(x i , w), y i ). Then, the local loss function of each device can be given by\nL n (w, D n ) = (x i ,y i )\u2208D n (\u03a8(x i , w), y i ) , \u2200n \u2208 N .\n(1)\n\nConsequently, the global loss at the edge cloud can be expressed as the average of all local loss functions, as\nL(w) = 1 |\u222a n\u2208N D n | n\u2208N L n (w, D n ) .(2)\nTo minimize the global loss, stochastic gradient descent (SGD) algorithm is widely used. Specifically, a subdataset D is sampled from the dataset to calculate the gradient in each iteration. Then, the gradient vector of each device can be expressed as\ng w n =\u2207L n w, D n = (x i ,y i )\u2208 D n \u2202 (\u03a8(x i , w), y i ) \u2202w , \u2200n \u2208 N .\n(3) According to [25], the gradient is calculated by two steps: forward propagation and back propagation. The forward propagation calculates the loss of each data and the backward propagation calculates the gradient based on the loss value. Moreover, in the k-th training period, the parameter updating at the edge cloud is given by\nw[k + 1] = w[k] \u2212 \u03b7[k]g w [k] = w[k] \u2212 \u03b7[k] 1 \u222a n D n n\u2208N (x i ,y i )\u2208 D n \u00d7 \u2202 (\u03a8(x i , w[k]), y i ) \u2202w[k] ,(4)\nwhere g w [k] is the global gradient and \u03b7[k] is the learning rate. According to [7], model training can be accelerated by using important data for training. Let \u03c3 n,m (m \u2208 {1, 2, . . . , M n }, n \u2208 N ) measure the importance of the m-th data of device n. Moreover, we assume that {\u03c3 n,m } is sorted in a non-increasing order, i.e., \u03c3 n,m \u2265 \u03c3 n,m+1 , \u2200n, m. To measure the data importance, we need to calculate the loss of the data [23], [26], which can be derived after forward propagation. By selecting the important data, the gradients of these unimportant data are not needed to be calculated in the backward propagation step. In this way, the training latency can be reduced and the learning efficiency can be improved.\n\n\nC. Wireless Channel Model\n\nIn the FEEL system, there exist two transmission stages in each training period, i.e., local gradient uploading and global gradient broadcasting.\n\nIn the local gradient uploading stage, we adopt the time division multiple access (TDMA) method for data transmission, where each time frame is divided into N time-slots. Denote W and N 0 as the system bandwidth and the noise power, respectively. Let h U n denote the channel power gain of the n-th device and p U n denote the corresponding transmit power. Since the data size of the gradient vector is usually large, the latency for gradient uploading (more than one second) is much longer than each time frame (10 ms in LTE standard [27]). 1 Therefore, we use the average achievable data rate to evaluate the device n's latency of gradient uploading [29], as\nR U n = W E h log 2 1 + p U n h U n 2 N 0 ,(5)\nwhere E h {\u00b7} is the expectation over the channel power gain. We should note that TDMA method is adopted in this paper since it has been widely used in current communication systems. The synchronization issue during the training period can be well guaranteed by the TDMA method. Nevertheless, our results can be extended to other access methods, such as non-orthogonal multiple access (NOMA) and orthogonal frequency division multiple access (OFDMA), with some modifications on the data rate model. In the global gradient broadcasting stage, we adopt broadcasting for data transmission since the global gradient is the same for all devices. Then, the achievable data rate for all devices can be expressed as\nR D = min n\u2208N W E h log 2 1 + p B h B n 2 N 0 ,(6)\nwhere h B n denotes the downlink channel power gain of device n and p B denotes the corresponding transmit power of the BS.\n\n\nD. Latency Analysis\n\nAs mentioned before, we aim at reducing the local computation latency and improving the learning efficiency via data selection based on data importance. Therefore, the end-to-end latency is essential and should be quantitatively analyzed. As shown in Fig. 1, the detailed procedures and the corresponding latency in each training period can be analyzed as follows.\n\n1) Forward propagation: A subdataset is first sampled from each local dataset with the sample size B F n . After that, each device calculates the loss of its sampled data and the importance \u03c3 n,m . Let t F n denote the forward propagation speed of device n, i.e., computation latency of forward propagation per data. Then, the latency for the forward propagation can be expressed as\nT F n = B F n t F n , \u2200n \u2208 N .(7)\n2) Data importance uploading: After forward propagation, each device uploads the data importance \u03c3 n,m to the edge cloud via TDMA method. 2 Since the data importance is only a scalar, its size is small enough so that its transmission delay can be ignored. 3) Data selection: After receiving the data importance from all devices, the edge cloud then selects data based on their importance values and channel data rates. Specifically, the edge cloud feeds back the number of selected data, denoted by B n , to each device. 4) Backward propagation: After receiving the number of selected data, device n will pick B n data with the largest data importance to perform backward propagation and calculate the local gradient vector, g w n [k]. Let t B n denote the backward propagation speed of device n, i.e., computation latency of backward propagation per data. Then, the total latency for each device to perform backward propagation is\nT B n = t B n B n , \u2200n \u2208 N .(8)\n5) Local gradient uploading: Each device sends its local gradient to the edge cloud. As we have mentioned before, we adopt TDMA method for local gradient uploading. Let \u03c4 n denote the proportion of device n's slot in one time frame. Then, according to [18], [29], the transmission delay of device n can be expressed as\nT U n = V \u03c4 n R U n ,(9)\nwhere V is the data size of local gradient and is a constant for all devices. 6) Gradient aggregation: After receiving the local gradient vectors from all devices, the edge cloud aggregates them to calculate the global gradient, as\ng w [k] = 1 n\u2208N B F n n\u2208N g w n [k].(10)\nSince gradient aggregation is only an average operation, the latency of this stage can be neglected. 7) Global gradient broadcasting: After gradient aggregation, the edge cloud broadcasts the global gradient vector to all devices. Thus, the latency for global gradient broadcasting is given as follows for all devices\nT D = V R D .(11)\n8) Model updating: Each device then updates its model according to (4). Since the initial parameters w[0] is the same, each device shares the same parameter after updating. The updating latency of device n is denoted by T M n , which cannot be neglected due to its limited computation capacity. Based on the above analysis, we can draw the timing chart as shown in Fig. 2. Note that the data selection step cannot be performed until receiving the data importance from all devices and the gradient aggregation step cannot be performed until receiving the local gradient vectors from all devices. Besides, although one training period starts from forward propagation and ends with model updating, it can be better understood if we assume that one training period starts from model updating and ends with global gradient broadcasting. Let T M,F denote the total latency for model updating and forward propagation, i.e.,\nT M,F = max n\u2208N {T M n + T F n },\nrepresenting the latency of step 1 and step 8. Then, the total latency of one training period can be \nT = T M,F + max n\u2208N T B n + T U n + T D .(12)\nTo better utilize the time for forward propagation, each device should calculate the loss of its sampled data as many as possible in order to offer more data choices. Therefore, the sample size should be\nB F n = T M,F \u2212 T M n t F n , \u2200n \u2208 N .(13)\n\nIII. LEARNING EFFICIENCY ANALYSIS\n\nIn this section, we will first propose the data importance metric. Then, an optimization problem is formulated to maximize the learning efficiency.\n\n\nA. Data Importance and Learning Efficiency\n\nIn this paper, we aim at reducing the local computation latency and improving the learning efficiency by data selection. To this end, we should first define the data importance based on model updating, which eventually influences the learning performance. From (4), we can find that the gradient vector influences the model updating, indicating that the data importance can be measured by its gradient. With a greater gradient, the data can contribute more to the parameter updating and is therefore more important to the model convergence. However, the real gradient vector is obtained after two steps, forward propagation and back propagation. Therefore, the computational consumption would be very large if we select data based on the real gradient vector. Fortunately, we can estimate the norm of gradient vector based on the loss of each data after forward propagation according to [26], as\n||g w [k](x i , y i )|| 2 = \u2202 (\u03a8(x i , w), y i ) \u2202w 2 \u2248 \u03c1 \u2202 (\u03a8(x i , w), y i ) \u2202x L i 2 ,(14)\nwhere x L i is the input of the active function in the output layer of CNN, \u03c1 is a coefficient determined by the CNN model, and || \u00b7 || 2 is the L2 norm. According to [26], the backward propagation requires about twice the amount of time as the forward propagation since it needs to compute full gradients. Since the proposed method only calculates the gradient of the parameter in the output layer, it can significantly reduce the computation cost. Therefore, the latency for estimating the gradient norm can be greatly reduced.\n\nTill now, we have found that the gradient vector influences the learning performance. However, the mathematical relation between them is not clear. To tackle it, we first measure the learning performance improvement by the global loss decay in one training period, as\n\u0394L[k] = L(w[k \u2212 1]) \u2212 L(w[k]).(15)\nAccording to [30], the relation between the global loss decay and the gradient norm is\n\u0394L[k] = \u03b3 ||g w [k]|| 2 2 ,(16)\nwhere \u03b3 is a coefficient determined by the specific CNN model. Then, by combining (14) and (16), we can conclude that the square of estimated gradient norm can measure the loss decay. Therefore, we can define the data importance in the following. Definition 1: The importance of data (x i , y i ) in device n can be evaluated by the square of its estimated gradient norm, as\n\u03c3 n,i = \u03c1\u03b3 \u2202 (\u03a8(x i , w), y i ) \u2202x L i 2 2 .(17)\nWe should note that although our analysis is based on CNN, the proposed data importance can be utilized for other neural networks that adopt SGD algorithm to minimize the loss function. From the above definition, the importance of data represents the loss decay it can bring to the global loss function. Then, the loss decay function of device n can be defined as\nf n (B n ) = B n m=1 \u03c3 n,m , B n \u2264 B F n ,(18)\nwhich represents the total loss decay brought by device n when B n data with the largest importance are selected. It should be noted that it is challenging to analyze the function f n (B n ) since B n is a discrete variable. However, we can relax B n into a continuous variable since B F n is typically large, such as 128. Correspondingly, f n (B n ) can be fitted as a piecewise linear function, as shown in Fig. 3. Then, we have the following lemma.\n\nLemma 1: f n (B n ) is a concave function when B n is relaxed into a continuous variable.\n\nProof: Please see Appendix A. Based on the lemma, the global loss decay in one training period is given by \u0394L = n\u2208N f n (B n ), which is the summation of the loss decay brought by all devices. Therefore, according to [18], the learning efficiency can be defined as\nE = \u0394L T = n\u2208N f n (B n ) T M,F + max n\u2208N {T B n + T U n } + T D ,(19)\nwhich represents the global loss decay rate in each training period. We should note that the learning efficiency jointly considers the learning performance (\u0394L) and the communication performance (T ). The improvement of learning efficiency represents that both the learning performance and communication performance are improved. By maximizing the learning efficiency, the training process can be accelerated.\n\n\nB. Problem Formulation\n\nIn this paper, we aim at maximizing the learning efficiency. The optimization problem can be mathematically formulated as\nP1 : max {B n ,\u03c4 n ,T,T M,F } N n=1 f n (B n ) T M,F + max n\u2208N {T B n + T U n } + T D , (20a) s.t. N n=1 \u03c4 n \u2264 1, (20b) B min n \u2264 B n \u2264 B F n , \u2200n \u2208 N ,(20c)T M,F \u2264 T M,F,max , \u2200n \u2208 N ,(20d)B F n , \u03c4 n , T, T M,F \u2265 0, \u2200n \u2208 N .(20e)\nIn the above, (20b) represents the uplink communication resource limitation, (20c) guarantees that the number of selected data should not exceed the sample size and should not be less than the minimum number requirement B min n to ensure the participation of each device, and (20d) bounds the maximum latency of model updating and forward propagation, denoted by T M,F,max , due to the hardware limitation, such as memory size.\n\nThe optimization variables in problem P1 contain the data selection (B n ), the communication resource allocation (\u03c4 n ), the total latency of model updating and the forward propagation (T M,F ), and the total latency of one training period (T ). Note that the reason for optimizing T M,F here is that T M,F determines the sample size, which will also influence the data selection.\n\nWe should note that problem P1 is not easy to solve for the following reasons. First, T M,F can influence the sample size {B F n } and the loss decay function. With different T M,F , the learning efficiency would be different but it is hard to give a detailed expression between the learning efficiency and T M,F . Secondly, T M,F should be decided before the forward propagation. Last but not the least, even if T M,F is given, problem P1 is still non-convex and cannot be solved directly.\n\nBased on the above considerations, we will first solve the problem when T M,F is given and then analyze the impact of T M,F on the learning efficiency in the following section.\n\n\nIV. OPTIMAL SOLUTION\n\nIn this section, we first analyze the problem for given the latency of model updating and forward propagation, i.e., T M,F . Then, the optimal data selection strategy and resource allocation policy are proposed. After that, the optimal solution to problem P1 is obtained by leveraging the probability theory.\n\n\nA. Optimal Data Selection and Resource Allocation\n\nAs mentioned before, T M,F is hard to be optimized directly. Therefore, we first consider the following problem with given T M,F , as\nP2 : max {B n ,\u03c4 n ,T } N n=1 f n (B n ) T ,(21a)s.t. T M,F + T B n + T U n + T D \u2264 T, \u2200n \u2208 N ,(21b)\n(20b), (20c), and (20e).\n\nNote that problem P2 is still non-convex. To solve it, we further assume that the total latency of one training period, T , is given. Then problem P2 becomes convex since the objective function is concave and all constraints are convex. Therefore, we can utilize the Lagrangian method to find the optimal solution. The partial Lagrange function can be defined as (22) where \u03bb and \u03bc n are the Lagrange multipliers associated with the constraints (20b) and (21b), respectively. Before presenting the optimal solution, we should note that f n (B n ) is not differentiable at some points so that its subgradient can be expressed as\nL = \u2212 N n=1 f n (B n ) T + \u03bb N n=1 \u03c4 n \u2212 1 + N n=1 \u03bc n T M,F + B n t B n + V \u03c4 n R U n + T D \u2212 T ,\u2202f n (B n ) \u2202B n = \u03c3 n, B n , if B n / \u2208 Z, \u2208 [\u03c3 n,B n +1 , \u03c3 n,B n ] , otherwise,(23)\nwhere Z is the set of non-negative integers and \u00b7 represents the rounding up operation. Moreover, we define the maximum number of selected data as\nB max n = min B F n , T \u2212 T M,F \u2212 T D t B n ,(24)\nwhich is decided by the sample size and the latency for backward propagation. Then, the data selection function can also be defined as\ng n (B n ) = \u2202f n (B n ) \u2202B n T \u2212 T M,F \u2212 T D \u2212 B n t B n 2 R U n V T ,(25)\nwhich can be derived by solving problem P2 under given T . Denote {B * n , \u03c4 * n } as the optimal solution to problem P2 under given T . Then, by applying the Karush-Kuhn-Tucker (KKT) conditions and simple mathematical calculation, we can obtain the optimal data selection and communication resource allocation policy, as shown in Theorem 1.\n\n\nTheorem 1:\n\nThe optimal data selection strategy and communication resource allocation policy can be expressed as\n\u23a7 \u23a8 \u23a9 B * n = [\u03c6 n (\u03bb * )] B max n B min n , \u2200n \u2208 N , (26a) \u03c4 * n = V (T \u2212T M,F \u2212T D \u2212B * n t B n )R U n + , \u2200n \u2208 N ,(26b)\nwhere \u03c6 n (x) is the inverse function of g n (B n ) and \u03bb * is the optimal value of the Lagrange multiplier satisfying the communication resource limitation: N n=1 \u03c4 n = 1. Note that [X] a b = max{min{X, a}, b} and (X) + = max{X, 0}. Proof: Please see Appendix B. Remark 1: The optimal data selection is achieved when g n (B n ) = \u03bb * is satisfied. Without loss of generality, we can consider the case that B * n / \u2208 Z, where the gradient of f n (B n ) is equal to the importance of the B n -th data, i.e., \u03c3 n, B n . Then, the date selection strategy is mainly determined by the backward propagation speed, the data importance, and the data rate. First of all, g n (B n ) decreases with B n since \u03c3 n, B n decreases with B n . More specifically, g n (B n ) decreases with B n in the power of 2 even if \u03c3 n, B n is fixed. Based on this, we can conclude that one device with more important data would have more data to be selected to perform backward propagation, which consists with our intuition. On the other hand, g n (B n ) decreases with the backward propagation speed t B n in the power of 2. The higher the backward propagation speed is, the more data the device can calculate. Besides, a device with higher data rate is likely to have more selected data.\n\nMoreover, from (26b), the communication resource allocation policy is related with the backward propagation speed, the data rate, and the optimal number of selected data. A device with higher backward propagation speed and data rate needs less communication resource to satisfy the latency requirement. Furthermore, the optimal communication resource allocation policy in (26b) indicates that the end of each device's transmission is synchronized, which can fully utilizes the available time to improve the learning efficiency.\n\nThe Lagrange multiplier \u03bb * in (26a) can be determined by classical bisection search algorithm. To reduce the computational complexity, we first analyze the upper and lower bounds for \u03bb * . Since \u03bb * is the Lagrange multiplier associated with the inequality constraint (20b), the lower bound is zero. The upper bound of \u03bb * is derived when the number of selected data of each device is the minimum one, B min n . Then, by simple mathematical calculation, we have the range of \u03bb * as shown in the following lemma.\n\nLemma 2: The range of \u03bb * is given by\n\u23a7 \u23aa \u23aa \u23aa \u23a8 \u23aa \u23aa \u23aa \u23a9 \u03bb * \u2265 \u03bb min = 0, \u03bb * \u2264 \u03bb max (27a) = max n\u2208N \u03c3 n,B min n (T \u2212T M,F \u2212T D \u2212B min n t B n ) 2 R U n V T .(27b)\nThe detailed algorithm for searching \u03bb * is given in Algorithm 1. The main idea is to update the value of \u03bb until the communication resource allocation constraint (20b) is satisfied. The computational complexity is O(N log(1/ )), where is the maximal error tolerance. \n\n\nB. Optimal Selection of T\n\nThus far, we have obtained the optimal E(T ) under the given total latency requirement T . Then, we optimize T to develop an optimal solution to problem P2. Note that the optimal data selection strategy and resource allocation policy are influenced by the total latency requirement. Therefore, we introduce the following theorem to ensure that our algorithm can find the optimal solution to problem P2.\n\nTheorem 2: E(T ) is a strictly unimodal function with T \u2265 0. Proof: Please see Appendix C.\n\n\nRemark 2:\n\nA unimodal function is a function that has only one peak (maximum) or valley (minimum) in a given interval. Specifically, E(T ) has only one peak as it first increases and then decreases with T , as shown in Fig. 4 . From Theorem 2, the local maximum is the global one in the given interval. Since the gradient of E(T ) cannot be directly expressed, we can utilize the Golden-section search algorithm [31] to find the optimal T * . By narrowing the range of values, Golden-section search algorithm can efficiently find an extremum of a function inside a specified interval. As shown in Fig. 4, the extremum would not be in [T 2 , T max ] since E(T 1 ) > E(T 2 ). Therefore, the new range becomes [T min , T 2 ]. Moreover, T 1 and T 2 are decided based on the golden ratio.\n\nTo better perform the Golden-section search algorithm, we have the following lemma about the range of T * .\n\nLemma 3: The range of T * is given by\n\u23a7 \u23a8 \u23a9 T * \u2265 T min = T M,F + N n=1 V R n + T D ,(28a)T * \u2264 T max = T M,F + max n\u2208N B F n t B n + V N R U n + T D . (28b) Proof: Please see Appendix D.\nThe lower bound in (28a) corresponds to the case where no data is selected for backward propagation and the communication resource allocation is obtained to minimize the total latency. The upper bound in (28b) corresponds to the case where all data are selected for backward propagation and the communication resource is equally allocated to each device.\n\nBased on above analysis, we can obtain the optimal algorithm to problem P2, as described in Algorithm 2.\n\n\nC. Optimal Selection of T M,F\n\nThus far, we have obtained the optimal solution to problem P1 for the given value of T M,F . In the following, we will analyze how to choose T M,F for further performance improvement.\n\nAs mentioned in Section III, the specific impact of T M,F on the loss decay function is hard to mathematically expressed. Thus, we consider using the probability theory for analysis. Recall that f n (B n ) is the summation of the B n most important data for device n. Then, according to [32], when the proportion of the selected data to the sample size, denoted by q n = B n /B F n , is fixed, we have\nlim B n \u2192\u221e E {f n (B n )} B n = C n , \u2200n \u2208 N ,(29)\nwhere C n is the mean value of loss decay of the selected data. From (29), the expectation of f n (B n ) is proportional to B n when B n is large. Then, the total loss decay is also proportional to the sample size. Meanwhile, the total latency of one training period is composed of two parts: the latency for local computation and the latency for communication. The former is proportional to the sample size while the latter is a constant. Therefore, the learning efficiency will increase with T M,F until it reaches its limit, as described in the following theorem. Theorem 3: Let E * denote the optimal learning efficiency to problem P1. Then, the expectation of E * increases with T M,F and its limit is\nlim T M,F \u2192\u221e E {E * } = N n=1 q n C n q n t B n + t F n .(30)\nProof: Please see Appendix E. Remark 3: From (30), the limit of the learning efficiency is mainly determined by the computation speed and the proportion of the selected data. With a faster computation speed, less time is required to perform a training period. Thus, the learning efficiency can be improved. Moreover, the learning efficiency increases with the proportion of important data, q n . The reason is explained as follows. The proportion of the selected data in the training process is related to the model accuracy of CNN. Specifically, fewer data become important as the model accuracy increases, which eventually reduces the learning efficiency.\n\nBased on Theorem 3, we should sample as many data as possible in the forward propagation. However, due to the hardware limitation, such as memory size, T M,F has its upper limit, as\nT M,F,max = min n\u2208N T M + t F n B F,max n ,(31)\nwhere B F,max n is the maximal sample size for device n. It should be noted that the maximal sample size is determined before the forward propagation stage. Therefore, T M,F can be set to its upper limit in advance to maximize the learning efficiency.\n\n\nV. TEST RESULTS\n\nIn this section, we conduct experiments to evaluate the performance of the proposed data-importance-aware FEEL scheme.\n\n\nA. Methodology\n\nWireless communication system: The BS covers a circle area with a radius of 300 m and 6 devices are randomly located in the coverage. The system bandwidth (W ) is 10 MHz and the noise spectral density (N 0 ) is \u2212174 dBm/Hz. The channel gains of cellular links (h U n , h B n ) are all generated according to the path loss model, 128.1 + 37.6 log(d [km]), and the small-scale fading follows the Rayleigh distributed with uniform variance. The transmit power (p U n ) is set as 24 dBm for all devices. CNN model: We consider three common CNN models for image classification, ResNet18, DenseNet121, and MobileNetV2. The corresponding training dataset is CIFAR-10 [28], which consists of 60,000 32\u00d732 colour images in 10 different classes. Specifically, it contains 50,000 training images and 10,000 test images. All training images are randomly partitioned into six equal parts, which are assigned to six devices, respectively. Due to the memory size limitation, the maximum sample size (B F,max n ) is set as 128 for each device. To ensure the participation of each device, we set the minimum sample size (B min n ) as 40. Moreover, the learning rate (\u03b7) is set as 0.001 for all CNN models unless otherwise specified.  The computation frequency of each device is set as: 2 devices with 3.4 GHz, 2 devices with 3.8 GHz, and 2 devices with 4.3 GHz.\n\n\nB. Generalization Tests\n\nIn this part, we first test the generalization ability of the proposed scheme. We implement our proposed scheme on the three CNN models mentioned above and compare the proposed algorithm with the conventional one where there is no data selection and all sampled data join the backward propagation. The learning accuracy and loss with different CNN models are shown in Fig. 5 and Fig. 6, respectively. We should note that curves in Fig. 5 and Fig. 6 vary with training period instead of training time. From Fig. 5, the proposed scheme achieves a significantly better performance than the conventional one for all three CNN models. Correspondingly, the loss of the proposed scheme decreases faster than that of conventional scheme, as depicted in Fig. 6. The results in both figures indicate two points. First, our proposed scheme is practical and has a good generalization ability. Secondly, the proposed scheme can accelerate the training process by selecting important data.\n\nTo better verify the second point, we first introduce the gradient norm ratio that is the ratio of the selected data's gradient norm to all data's gradient norm. It can describe whether the  I  THE PROPORTION OF SELECTED DATA AND THE CORRESPONDING GRADIENT  NORM RATIO selected data can represent all data. The selected data can exactly represent all data when the gradient norm achieves one. Table I illustrates the proportion of the selected data and the corresponding gradient norm ratio. Since they are almost the same during the training period in Fig. 5 and Fig. 6, we only show the average values for different CNN models. It can be observed that although only a small proportion (around 36%) of data are selected, the norm of those selected data's gradient is almost the same as if all data are selected. The results confirm that our proposal can speed up the training period by data selection since only important data are selected for backward propagation. In Fig. 5, we can see that the proposed scheme achieves a higher final learning accuracy than the conventional scheme for all three CNN models. The reason can be explained as follows. During each training period, we select the important data with large gradient norm, which is not predicted well by CNN model. By only training those data, the model can improve its accuracy. However, we can find that the loss of our proposed scheme in Fig. 6 slightly increases at the end of training. It is because most data are unimportant at the end of training and the loss of unimportant data may increase though the loss of important data can be reduced.\n\n\nC. Performance Comparison\n\nIn this part, to show the performance improvement of our proposed algorithm, we will compare it with several benchmarks as follows.\n\nr Equal resource scheme: This scheme is similar to our proposal except that the communication resource is equally allocated to each device, i.e., \u03c4 n = 1/N . r All selected scheme: This scheme is similar to our proposal except that all sampled data are selected for gradient calculation, i.e., B n = B F n .\n\nr Conventional scheme: In this scheme, all sampled data join the backward propagation in each device, i.e., B n = B F n , and the communication resource is equally allocated to each device, i.e., \u03c4 n = 1/N . r Half sample size scheme: This scheme is similar to our proposal except that the sample size is set as 64 that is half of the maximum sample size. We consider 6 immobile devices whose distances to BS are ranked as: device 1 < device 2 < device 3 < device 4 < device 5 < device 6, which influence the large-scale fading. The small-scale fading varies with time for each device. Without loss of generality, we adopt ResNet18 for the following tests since different CNN models have the same comparative results.\n\nAt the beginning of the training process, we set the learning rate as 0.01. Fig. 7 shows the learning accuracy with different schemes. From the figure, all schemes have almost the same  performance when the learning accuracy is low. The reasons are twofold. First, the model cannot fit dataset well and most of the data are important to be selected with low learning accuracy, which means that data selection has little performance gain in this case. Secondly, the communication latency is shorter than the computation latency when the number of selected data is large.\n\nAs training period continues, the learning accuracy increases and the number of selected data decreases. Fig. 8 plots the variation of the number of selected data with time and the gradient norm ratio for device 6, as an example. Since some data can be predicted well by the model with high learning accuracy, they become unimportant. Meanwhile, we should note that the gradient norm ratio is always above 90%, even almost 100%, although the number of selected data decreases, which means that the selected data can well represent all data.\n\nAt the end of the training process in Fig. 7, the increment of learning accuracy becomes small but the algorithm still needs long training time to converge. Therefore, we consider a pre-trained ResNet18 that has achieved an initial accuracy of 86%. Then, we plot the learning accuracy curve in Fig. 9. Here, the learning rate is set to be 0.001 for achieving higher accuracy. From Fig. 9, the proposed scheme can achieve its peak learning accuracy fast and is the best one among all schemes. By  comparing the all selected scheme with our proposed scheme, one can clearly see the gain obtained by selecting important data. By comparing with the equal resource scheme, the gain obtained by optimal communication resource allocation can be clearly seen. Besides, the accuracy increment speed of the half sample size scheme is lower than that of our proposed scheme, which verifies Theorem 3. Furthermore, after 250 minutes of training, our proposed scheme almost achieves the peak learning accuracy, 0.923. However, in the same time, the conventional scheme only has an accuracy of 0.91 and still needs 150 minutes of training to converge to an accuracy of 0.916, which validates the performance improvement of the proposed scheme. Finally, the schemes with data selection (i.e., the proposed scheme, the half sample size scheme, and the equal resource scheme) achieve higher learning accuracy than those without data selection, demonstrating that importance-aware data selection can indeed improve the learning accuracy.\n\nTo analyze the influence of wireless channel fading, we finally illustrate the communication resource allocation for each device in Fig. 10. We should note that the communication resource allocation is mainly determined by the channel gain according to (26b). As we have mentioned before, the channel gain is mainly determined by the large-scale fading. Therefore, device 1 is allocated with the least average communication resource due to its shortest distance to the BS. Meanwhile, since the channel gain is influenced by small-scale fading, the resource allocation varies with time, as can be observed in the figure.\n\n\nVI. CONCLUSION\n\nIn this paper, we propose a joint data selection and resource allocation scheme based on the data importance in the FEEL system to improve the learning efficiency. The relation between the gradient norm and loss decay is first analyzed, which suggests us to measure the data importance by its gradient norm. Afterwards, we formulate a learning efficiency maximization problem by jointly considering the wireless resource allocation and data selection. For given sample size and end-to-end latency, the optimal data selection and communication resource allocation policy is derived in closed-form. Based on this, the optimal end-to-end latency is obtained by the Golden-section search algorithm. Furthermore, by analyzing the relation between the learning efficiency and the sample size, we observe that each device should sample as many data as possible in the forward propagation step for learning performance improvement. Finally, test results verify the generalization ability of the proposed scheme and show that our proposal can accelerate the training process and improve the learning accuracy. \n0 \u2264 B n \u2264 B F n ,(32)where h n,B m (B n ) = \u03c3 n, B m (B n \u2212 B m ) + B m m=1 \u03c3 n,m , 0 \u2264 B n \u2264 B F n .\nNote that \u00b7 represents the ceil operation and \u00b7 represents the floor operation. According to [33], the pointwise minimum f n (B n ) is concave since h n,1 (B n ), h n,2 (B n ), . . . , h n,B F n (B n ) are all linear.\n\n\nAPPENDIX B PROOF OF THEOREM 1\n\nSince problem P2 is convex, we can utilize the Lagrangian method to solve it and the partial Lagrange function is given in (22). Then, based on the KKT conditions, we can obtain the following necessary and sufficient conditions, as\n\u2202L \u2202B * n = \u2212 \u2202f n (B * n ) \u2202B * n \u00b7 1 T + \u03bc * n \u23a7 \u23a8 \u23a9 \u2265 0, B * n = B min n , = 0, B min n \u2264 B * n \u2264 B F n , \u2264 0, B * n = B F n , \u2200n \u2208 N ,(33)\u2202L \u2202\u03c4 * n = \u2212\u03bc * n V R U n (\u03c4 * n ) 2 + \u03bb * \u2265 0, \u03c4 * n = 0, = 0, 0 \u2264 \u03c4 * n , \u2200n \u2208 N , (34) \u03bc * n T M,F + B * n t B n + V \u03c4 * n R U n + T D \u2212 T = 0, \u2200n \u2208 N , (35) \u03bb * N n=1 \u03c4 * n \u2212 1 = 0, \u03bb * , \u03bc * n \u2265 0.(36)\nNote that we have fitted the loss decay function f n (B n ) as shown in Fig. 3 and the subgradient of f n (B n ) is\n\u2202f n (B n ) \u2202B n = \u03c3 n, B n , if B n / \u2208 Z, \u2208 [\u03c3 n,B n +1 , \u03c3 n,B n ] , otherwise,(37)\nwhere Z is the set of non-negative integers. With simple mathematical calculation, we can derive three cases about the optimal data selection strategy as follows\n1) If g n (B min n ) < \u03bb * , B * n = B min n ; 2) If g n (B max n ) > \u03bb * and B max n = min(B F n , T \u2212 T M,F \u2212 T D t B n ), B * n = B max n ; 3) If g n (B min n ) \u2265 \u03bb * and g n (B max n ) \u2264 \u03bb * , B * n = \u03c6 n (\u03bb * ), where \u03c6 n (x) is the inverse function of g n (B n ) in the interval [B min n , B max n ]\n. According to the above three cases, we can derive the optimal data selection strategy as shown in (26a). Furthermore, the resource allocation policy achieves the optimum when\nT M,F + B * n t B n + V \u03c4 * n R U n + T D = T, \u2200n \u2208 N .(38)\nTherefore, \u03c4 * n is given by\n\u03c4 * n = V (T \u2212 T M,F \u2212 T D \u2212 B * n t B n ) R U n + ,(39)\nwhich ends the proof.\n\n\nAPPENDIX C PROOF OF THEOREM 2\n\nIt is easy to prove that problem P2 is a concave-convex fractional programming problem. Then, we can transform P2 into a convex problem via the Charnes-Cooper transformation [34] by introducing the following auxiliary variables\nz = 1 T , q n = B n T , x n = \u03c4 n T .(40)\nNow, the problem P2 can be rewritten as x n \u2264 z,\n\nB min n z \u2264 q n \u2264 B F n z, \u2200n \u2208 N ,\n\nq n , x n , z \u2265 0.\n\nGiven z (or T ), the optimal value obtained by optimizing q n and x n in P3 is exactly equal to E(T ). Since problem P3 is convex, E(T ) is also convex with z, i.e., 1 T . Therefore, we can conclude that E(T ) is a strictly unimodal function with T when T is bigger than zero since E(T ) > 0. This ends the proof.\n\n\nAPPENDIX D PROOF OF LEMMA 3\n\nTo prove this lemma, we consider the following two cases. Case A: In this case, there is no data selected for backward propagation and each device's latency is given by\nT n = T M,F + V \u03c4 n R U n + T D , \u2200n \u2208 N ,(42)\nwhere \u03c4 n satisfies N n=1 \u03c4 n \u2264 1. Then, by optimizing \u03c4 n , we can obtain the minimal total latency as\nT min = T M,F + N n=1 V R n + T D .(43)\nSince T min is the minimal total latency when no data is selected, it can be regarded as a lower bound of T * . Case B: The maximal total latency would happen when all data are selected to join the backward propagation. In this case, the loss decay achieves its maximum. Then, to maximize the learning efficiency, the total latency should be minimized by optimizing \u03c4 n . By allocating equal communication resource to all devices, we can obtain the total latency as\nT max = T M,F + max n\u2208N B F n t B n + V N R U n + T D ,(44)\nwhich can be regarded as an upper bound of T * .\n\n\nAPPENDIX E PROOF OF THEOREM 3\n\nThe expectation of learning efficiency can be expressed as \nE {E} = n\u2208N E {f n (B n )} T .(45)\nwhere E * , (1) is the optimal value when T M,F = T M,F, (1) . Let E * , (2) denote the optimal value when T M,F = T M,F, (2) . Then we have E (2) \u2264 E * , (2) . Combining this with (48), we can conclude that E * , (2) > E * , (1) when T M,F,(2) > T M,F, (1) . In conclusion, the expectation of learning efficiency increases with T M,F . Furthermore, the limit of learning efficiency can be given by \n\nThis ends the proof.\n\nFig. 1 .\n1System model.\n\nFig. 2 .\n2Timing chart of one training period. Steps 2, 3, and 6 are not shown in the timing chart since the latency is too short that can be neglected. expressed as\n\nFig. 3 .\n3The loss decay function of device n.\n\nFig. 4 .\n4The relation between learning efficiency E(T ) and total latency of one training period T .\n\nFig. 5 .\n5The test accuracy with different CNN models.\n\nFig. 6 .\n6The test loss with different CNN models.\n\nFig. 7 .\n7The test accuracy of different schemes.\n\nFig. 8 .\n8The number of selected data and the gradient norm ratio for device 6.\n\nFig. 9 .\n9The test accuracy of different schemes.\n\nFig. 10 .\n10An example of communication resource allocation with the proposed scheme. The boundary between adjacent areas represents instantaneous communication resource allocation result whereas the dashed line represents the average resource allocated for each device.\n\n\nwe can rewrite f n (B n ) as the pointwise minimum of linear functions, as f (B n ) = min h n,1 (B n ), h n,2 (B n ), . . . , h n,B F n (B n ) ,\n\n\nt. zT M,F + q n t B n + V z 2 R U n x n + zT D \u2264 1, \u2200n \u2208 N , (41b) N n=1\n\nFTT(\nWe mark two different values of T M,F as T M,F,(1) and T M,F,(2) that satisfy T M,F,(1) < T M,F,(2) and the corresponding sample sizes are {B M , \u2200n \u2208 N .(46) Let {B * ,(1) n , \u03c4 * ,(1) n } denote the optimal solution to problem P2 when T M,F = T M,F,(1) . Then, when T M,F = T M,F,(2) , a feasible solution to problem P2 is { B * ,(1) n B F,(2) n B F,(1) n , \u03c4 * ,(1) n B F,(1) n B F,(2) n }. According to (29), the expectation of f n ( B * ,(1) D + T M , \u2200n \u2208 N . T D + T M ) > n\u2208N E f n B * ,(1) n T * ,(1) = E * ,(1) ,\n\n\nn C n q n t B n + t F n .\n\n\nreceived May 2, 2020; revised June 24, 2020; accepted August 4, 2020. Date of publication August 10, 2020; date of current version November 12, 2020. This work was supported in part by the Natural Science Foundation of China under Grant 61671407 and in part by the Zhejiang Lab's International Talent Fund for Young Professionals. The review of this article was coordinated by Dr. Bomin Mao. (Corresponding author: Jiantao Yuan.) Yinghui He, Jinke Ren, and Guanding Yu are with the College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China (e-mail: 2014hyh@zju.edu.cn; renjinke@zju.edu.cn; yuguanding@zju.edu.cn). Jiantao Yuan is with the Institute of Ocean Sensing and Networking, Ocean College, Zhejiang University, Zhoushan 316021, China (e-mail: 11331030@zju.edu.cn). Digital Object Identifier 10.1109/TVT.2020.3015268\n\nTABLE\n\nTake ResNet18 as an example. The data size of the gradient vector is about 342 MBits. The uplink data rate in LTE standard is 50 Mbps and the transmit delay is more than one second. Even if we use the gradient compression method in[28] the transmission delay is still about 0.5 second.2 If one device fails to upload the data importance, it cannot join this training period. However, this device can still receive the global gradient vector from the edge cloud, which ensures that it can join the next training period.\n\nDeep learning. Y Lecun, Y Bengio, G Hinton, Nature. 521Y. LeCun, Y. Bengio, and G. Hinton, \"Deep learning,\" Nature, vol. 521, pp. 436-444, May 2015.\n\nRouting or computing? The paradigm shift towards intelligent computer network packet transmission based on deep learning. B Mao, IEEE Trans. Comput. 6611B. Mao et al., \"Routing or computing? The paradigm shift towards in- telligent computer network packet transmission based on deep learning,\" IEEE Trans. Comput., vol. 66, no. 11, pp. 1946-1960, Nov. 2017.\n\nEnvisioning device-todevice communications in 6G. S Zhang, J Liu, H Guo, M Qi, N Kato, IEEE Netw. 343S. Zhang, J. Liu, H. Guo, M. Qi, and N. Kato, \"Envisioning device-to- device communications in 6G,\" IEEE Netw., vol. 34, no. 3, pp. 86-91, May 2020.\n\n6G: Opening new horizons for integration of comfort, security, and intelligence. G Gui, M Liu, F Tang, N Kato, F Adachi, 10.1109/MWC.001.1900516IEEE Wireless Commun. to be publishedG. Gui, M. Liu, F. Tang, N. Kato, and F. Adachi, \"6G: Opening new horizons for integration of comfort, security, and intelligence,\" IEEE Wireless Commun., to be published, doi: 10.1109/MWC.001.1900516.\n\nTen challenges in advancing machine learning technologies towards 6G. N Kato, B Mao, F Tang, Y Kawamoto, J Liu, IEEE Wireless Commun. 273N. Kato, B. Mao, F. Tang, Y. Kawamoto, and J. Liu, \"Ten challenges in advancing machine learning technologies towards 6G,\" IEEE Wireless Commun., vol. 27, no. 3, pp. 96-103, Jun. 2020.\n\nWireless network intelligence at the edge. J Park, S Samarakoon, M Bennis, M Debbah, Proc. IEEE. IEEE107J. Park, S. Samarakoon, M. Bennis, and M. Debbah, \"Wireless network intelligence at the edge,\" Proc. IEEE, vol. 107, no. 11, pp. 2204-2239, Nov. 2019.\n\nTowards an intelligent edge: Wireless communication meets machine learning. G Zhu, D Liu, Y Du, C You, J Zhang, K Huang, IEEE Commun. Mag. 581G. Zhu, D. Liu, Y. Du, C. You, J. Zhang, and K. Huang, \"Towards an intelligent edge: Wireless communication meets machine learning,\" IEEE Commun. Mag., vol. 58, no. 1, pp. 19-25, Jan. 2020.\n\nFederated learning in mobile edge networks: A comprehensive survey. W Y B Lim, 10.1109/COMST.2020.2986024IEEE Commun. Surveys Tuts. to be publishedW. Y. B. Lim et al. \"Federated learning in mobile edge networks: A comprehensive survey,\" IEEE Commun. Surveys Tuts., to be published, doi: 10.1109/COMST.2020.2986024.\n\nFederated optimization: Distributed optimization beyond the datacenter. J Kone\u010dn\u1ef3, B Mcmahan, D Ramage, arXiv:1511.03575J. Kone\u010dn\u1ef3, B. McMahan, and D. Ramage, \"Federated optimization: Distributed optimization beyond the datacenter,\" 2015, arXiv:1511.03575.\n\nA survey on mobile edge computing: The communication perspective. Y Mao, C You, J Zhang, K Huang, K B Letaief, IEEE Commun. Surv. Tutor. 194Y. Mao, C. You, J. Zhang, K. Huang, and K. B. Letaief, \"A survey on mobile edge computing: The communication perspective,\" IEEE Commun. Surv. Tutor., vol. 19, no. 4, pp. 2322-2358, Aug. 2017.\n\nDevice scheduling with fast convergence for wireless federated learning. W Shi, S Zhou, Z Niu, Proc. IEEE Int. Conf. Commun. IEEE Int. Conf. CommunDublin, IrelandW. Shi, S. Zhou, and Z. Niu, \"Device scheduling with fast convergence for wireless federated learning,\" in Proc. IEEE Int. Conf. Commun., Dublin, Ireland, Jun. 2020, pp. 1-6.\n\nBroadband analog aggregation for lowlatency federated edge learning. G Zhu, Y Wang, K Huang, IEEE Trans. Wireless Commun. 191G. Zhu, Y. Wang, and K. Huang, \"Broadband analog aggregation for low- latency federated edge learning,\" IEEE Trans. Wireless Commun., vol. 19, no. 1, pp. 491-506, Jan. 2020.\n\nEnergy-efficient radio resource allocation for federated edge learning. Q Zeng, Y Du, K K Leung, K Huang, Proc. IEEE Int. Conf. Commun. Workshops. IEEE Int. Conf. Commun. WorkshopsDublin, IrelandQ. Zeng, Y. Du, K. K. Leung, and K. Huang, \"Energy-efficient radio resource allocation for federated edge learning,\" in Proc. IEEE Int. Conf. Commun. Workshops, Dublin, Ireland, Jun. 2020, pp. 1-6.\n\nEnergy-aware analog aggregation for federated learning with redundant data. Y Sun, S Zhou, D G\u00fcnd\u00fcz, Proc. IEEE Int. Conf. Commun. IEEE Int. Conf. CommunDublin, IrelandY. Sun, S. Zhou, and D. G\u00fcnd\u00fcz, \"Energy-aware analog aggregation for federated learning with redundant data,\" in Proc. IEEE Int. Conf. Commun., Dublin, Ireland, Jun. 2020, pp. 1-6.\n\nFederated learning over wireless networks: Optimization model design and analysis. N H Tran, W Bao, A Y Zomaya, N N H Minh, C S Hong, Proc. IEEE Conf. Comput. Commun. IEEE Conf. Comput. CommunN. H. Tran, W. Bao, A. Y. Zomaya, N. N. H. Minh, and C. S. Hong, \"Federated learning over wireless networks: Optimization model design and analysis,\" in Proc. IEEE Conf. Comput. Commun., pp. 1387-1395, Apr. 2019.\n\nEnergy efficient federated learning over wireless communication networks. Z Yang, M Chen, W Saad, C S Hong, M Shikh-Bahaei, arXiv:1911.02417Z. Yang, M. Chen, W. Saad, C. S. Hong, and M. Shikh-Bahaei, \"Energy efficient federated learning over wireless communication networks,\" 2019, arXiv:1911.02417.\n\nA joint learning and communications framework for federated learning over wireless networks. M Chen, Z Yang, W Saad, C Yin, H V Poor, S Cui, arXiv:1909.07972M. Chen, Z. Yang, W. Saad, C. Yin, H. V. Poor, and S. Cui, \"A joint learn- ing and communications framework for federated learning over wireless networks,\" 2019, arXiv:1909.07972.\n\nAccelerating DNN training in wireless federated edge learning system. J Ren, G Yu, G Ding, arXiv:1905.09712J. Ren, G. Yu, and G. Ding, \"Accelerating DNN training in wireless federated edge learning system,\" 2019, arXiv:1905.09712.\n\nBiased importance sampling for deep neural network training. A Katharopoulos, F Fleuret, arXiv:1706.00043A. Katharopoulos and F. Fleuret, \"Biased importance sampling for deep neural network training,\" 2017, arXiv:1706.00043.\n\nSample importance in training deep neural networks. T Gao, V Jojic, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. RepresentationsToulon, FranceT. Gao and V. Jojic, \"Sample importance in training deep neural net- works,\" in Proc. Int. Conf. Learn. Representations, Toulon, France, Apr. 2017, pp. 1-12.\n\nVariance reduction in SGD by distributed importance sampling. G Alain, A Lamb, C Sankar, A Courville, Y Bengio, arXiv:1511.06481G. Alain, A. Lamb, C. Sankar, A. Courville, and Y. Bengio, \"Vari- ance reduction in SGD by distributed importance sampling,\" 2015, arXiv:1511.06481.\n\nScheduling for cellular federated edge learning with importance and channel awareness. J Ren, Y He, D Wen, G Yu, K Huang, D Guo, IEEE Trans. Wireless Commun. to be publishedJ. Ren, Y. He, D. Wen, G. Yu, K. Huang, and D. Guo, \"Scheduling for cellular federated edge learning with importance and channel awareness,\" IEEE Trans. Wireless Commun., to be published.\n\nWireless data acquisition for edge learning: Importance aware retransmission. D Liu, G Zhu, J Zhang, K Huang, Proc. IEEE 20th Int. Workshop Signal Process. IEEE 20th Int. Workshop Signal essCannes, FranceD. Liu, G. Zhu, J. Zhang, and K. Huang, \"Wireless data acquisition for edge learning: Importance aware retransmission,\" in Proc. IEEE 20th Int. Workshop Signal Process. Adv. Wireless Commun., Cannes, France, Jul. 2019, pp. 1-5.\n\nData-importance aware user scheduling for communication-efficient edge machine learning. D Liu, G Zhu, J Zhang, K Huang, 10.1109/TCCN.2020.2999606IEEE Trans. Cogn. Commun. Netw. to be publishedD. Liu, G. Zhu, J. Zhang, and K. Huang, \"Data-importance aware user scheduling for communication-efficient edge machine learn- ing,\" IEEE Trans. Cogn. Commun. Netw., to be published, doi: 10.1109/TCCN.2020.2999606.\n\nI Goodfellow, Y Bengio, A Courville, Deep Learning. Cambridge, MA, USAMIT PressI. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. Cambridge, MA, USA: MIT Press, 2016.\n\nNot all samples are created equal: Deep learning with importance sampling. A Katharopoulos, F Fleuret, arXiv:1803.00942A. Katharopoulos and F. Fleuret, \"Not all samples are created equal: Deep learning with importance sampling,\" 2018, arXiv:1803.00942.\n\nLTE; Evolved universal terrestrial radio access (E-UTRA); Physical channels and modulation (3GPP TS 36.211 version 15.6.0 Release 15). TS 36 211 V15.6.0Sophia Antipolis Cedex, FranceTech. Rep.\"LTE; Evolved universal terrestrial radio access (E-UTRA); Physical channels and modulation (3GPP TS 36.211 version 15.6.0 Release 15),\" Tech. Rep. TS 36 211 V15.6.0, Sophia Antipolis Cedex, France, Jul. 2019.\n\nModel compression via distillation and quantization. A Polino, R Pascanu, D Alistarh, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. RepresentationsVancouver, CanadaA. Polino, R. Pascanu, and D. Alistarh, \"Model compression via distillation and quantization,\" in Proc. Int. Conf. Learn. Representations, Vancouver, Canada, Apr. 2018, pp. 1-21.\n\nLatency optimization for resource allocation in mobile-edge computation offloading. J Ren, G Yu, Y Cai, Y He, IEEE Trans. Wireless Commun. 178J. Ren, G. Yu, Y. Cai, and Y. He, \"Latency optimization for resource allocation in mobile-edge computation offloading,\" IEEE Trans. Wireless Commun., vol. 17, no. 8, pp. 5506-5519, Aug. 2018.\n\nLAG: Lazily aggregated gradient for communication-efficient distributed learning. T Chen, G B Giannakis, T Sun, W Yin, Proc. Advances Neural Inf. Advances Neural InfMontreal, CanadaT. Chen, G. B. Giannakis, T. Sun, and W. Yin, \"LAG: Lazily aggregated gradient for communication-efficient distributed learning,\" in Proc. Ad- vances Neural Inf. Process. Syst., Montreal, Canada, Dec. 2018, pp. 1-11.\n\nSequential minimax search for a maximum. J Kiefer, Proc. Amer. Math. Soc. J. Kiefer, \"Sequential minimax search for a maximum,\" in Proc. Amer. Math. Soc., 1953, pp. 502-506.\n\nAn introduction to extreme order statistics and actuarial applications. H N Nagaraja, Proc. ERM Symp. ERM SympChicagoH. N. Nagaraja, \"An introduction to extreme order statistics and actuarial applications,\" in Proc. ERM Symp., Chicago, Apr. 2004.\n\nS Boyd, L Vandenberghe, Convex Optimization. Cambridge, U.K.Cambridge Univ. PressS. Boyd and L. Vandenberghe, Convex Optimization. Cambridge, U.K.: Cambridge Univ. Press, 2004.\n\nFractional programming for communication systems-Part I: power control and beamforming. K Shen, W Yu, IEEE Trans. Signal Process. 6610K. Shen and W. Yu, \"Fractional programming for communication systems- Part I: power control and beamforming,\" IEEE Trans. Signal Process., vol. 66, no. 10, pp. 2616-2630, May 2018.\n", "annotations": {"author": "[{\"start\":\"106\",\"end\":\"117\"},{\"start\":\"118\",\"end\":\"148\"},{\"start\":\"149\",\"end\":\"180\"},{\"start\":\"181\",\"end\":\"194\"}]", "publisher": null, "author_last_name": "[{\"start\":\"114\",\"end\":\"116\"},{\"start\":\"144\",\"end\":\"147\"},{\"start\":\"189\",\"end\":\"193\"}]", "author_first_name": "[{\"start\":\"106\",\"end\":\"113\"},{\"start\":\"138\",\"end\":\"143\"},{\"start\":\"168\",\"end\":\"176\"},{\"start\":\"177\",\"end\":\"179\"},{\"start\":\"181\",\"end\":\"188\"}]", "author_affiliation": null, "title": "[{\"start\":\"1\",\"end\":\"90\"},{\"start\":\"195\",\"end\":\"284\"}]", "venue": "[{\"start\":\"286\",\"end\":\"327\"}]", "abstract": "[{\"start\":\"494\",\"end\":\"2131\"}]", "bib_ref": "[{\"start\":\"2335\",\"end\":\"2338\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"2442\",\"end\":\"2445\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"2447\",\"end\":\"2450\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"2572\",\"end\":\"2582\"},{\"start\":\"2739\",\"end\":\"2742\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"2744\",\"end\":\"2747\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"3188\",\"end\":\"3191\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"3193\",\"end\":\"3196\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"3261\",\"end\":\"3264\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"3346\",\"end\":\"3350\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"3857\",\"end\":\"3861\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"3863\",\"end\":\"3867\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"3991\",\"end\":\"3995\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"4119\",\"end\":\"4123\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"4331\",\"end\":\"4335\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"4504\",\"end\":\"4508\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"4825\",\"end\":\"4829\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"4834\",\"end\":\"4838\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"5115\",\"end\":\"5119\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"5121\",\"end\":\"5125\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"5257\",\"end\":\"5261\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"5701\",\"end\":\"5705\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"6325\",\"end\":\"6329\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"6356\",\"end\":\"6360\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"6379\",\"end\":\"6383\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"6385\",\"end\":\"6389\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"6484\",\"end\":\"6488\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"6490\",\"end\":\"6494\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"6499\",\"end\":\"6503\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"6919\",\"end\":\"6923\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"11362\",\"end\":\"11366\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"11871\",\"end\":\"11874\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"12222\",\"end\":\"12226\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"12228\",\"end\":\"12232\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"13226\",\"end\":\"13230\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"13233\",\"end\":\"13234\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"13343\",\"end\":\"13347\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"15226\",\"end\":\"15227\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"16304\",\"end\":\"16308\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"16310\",\"end\":\"16314\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"17072\",\"end\":\"17075\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"19468\",\"end\":\"19472\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"19738\",\"end\":\"19742\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"20418\",\"end\":\"20422\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"22120\",\"end\":\"22124\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"25520\",\"end\":\"25524\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"30636\",\"end\":\"30640\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"32272\",\"end\":\"32276\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"32507\",\"end\":\"32511\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"35164\",\"end\":\"35168\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"44224\",\"end\":\"44228\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"44505\",\"end\":\"44509\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"46187\",\"end\":\"46191\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"47810\",\"end\":\"47813\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"47855\",\"end\":\"47858\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"47871\",\"end\":\"47874\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"47920\",\"end\":\"47923\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"47953\",\"end\":\"47956\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"48012\",\"end\":\"48015\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"48024\",\"end\":\"48027\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"48052\",\"end\":\"48055\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"51014\",\"end\":\"51018\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"51068\",\"end\":\"51069\",\"attributes\":{\"ref_id\":\"b1\"}}]", "figure": "[{\"start\":\"48220\",\"end\":\"48244\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"48245\",\"end\":\"48411\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"48412\",\"end\":\"48459\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"48460\",\"end\":\"48562\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"48563\",\"end\":\"48618\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"48619\",\"end\":\"48670\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"48671\",\"end\":\"48721\",\"attributes\":{\"id\":\"fig_6\"}},{\"start\":\"48722\",\"end\":\"48802\",\"attributes\":{\"id\":\"fig_7\"}},{\"start\":\"48803\",\"end\":\"48853\",\"attributes\":{\"id\":\"fig_8\"}},{\"start\":\"48854\",\"end\":\"49125\",\"attributes\":{\"id\":\"fig_9\"}},{\"start\":\"49126\",\"end\":\"49272\",\"attributes\":{\"id\":\"fig_10\"}},{\"start\":\"49273\",\"end\":\"49347\",\"attributes\":{\"id\":\"fig_11\"}},{\"start\":\"49348\",\"end\":\"49876\",\"attributes\":{\"id\":\"fig_12\"}},{\"start\":\"49877\",\"end\":\"49904\",\"attributes\":{\"id\":\"fig_13\"}},{\"start\":\"49905\",\"end\":\"50774\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}},{\"start\":\"50775\",\"end\":\"50782\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"2150\",\"end\":\"4886\"},{\"start\":\"4888\",\"end\":\"5414\"},{\"start\":\"5416\",\"end\":\"5493\"},{\"start\":\"5495\",\"end\":\"6986\"},{\"start\":\"6988\",\"end\":\"7595\"},{\"start\":\"7597\",\"end\":\"7921\"},{\"start\":\"7923\",\"end\":\"8424\"},{\"start\":\"8426\",\"end\":\"9347\"},{\"start\":\"9387\",\"end\":\"9561\"},{\"start\":\"9580\",\"end\":\"10537\"},{\"start\":\"10554\",\"end\":\"10795\"},{\"start\":\"10858\",\"end\":\"10861\"},{\"start\":\"10863\",\"end\":\"10974\"},{\"start\":\"11020\",\"end\":\"11271\"},{\"start\":\"11345\",\"end\":\"11677\"},{\"start\":\"11790\",\"end\":\"12514\"},{\"start\":\"12544\",\"end\":\"12689\"},{\"start\":\"12691\",\"end\":\"13351\"},{\"start\":\"13399\",\"end\":\"14106\"},{\"start\":\"14158\",\"end\":\"14281\"},{\"start\":\"14305\",\"end\":\"14669\"},{\"start\":\"14671\",\"end\":\"15053\"},{\"start\":\"15088\",\"end\":\"16019\"},{\"start\":\"16052\",\"end\":\"16370\"},{\"start\":\"16396\",\"end\":\"16627\"},{\"start\":\"16669\",\"end\":\"16986\"},{\"start\":\"17005\",\"end\":\"17921\"},{\"start\":\"17956\",\"end\":\"18057\"},{\"start\":\"18104\",\"end\":\"18307\"},{\"start\":\"18387\",\"end\":\"18534\"},{\"start\":\"18581\",\"end\":\"19476\"},{\"start\":\"19571\",\"end\":\"20100\"},{\"start\":\"20102\",\"end\":\"20369\"},{\"start\":\"20405\",\"end\":\"20491\"},{\"start\":\"20524\",\"end\":\"20898\"},{\"start\":\"20948\",\"end\":\"21311\"},{\"start\":\"21359\",\"end\":\"21810\"},{\"start\":\"21812\",\"end\":\"21901\"},{\"start\":\"21903\",\"end\":\"22167\"},{\"start\":\"22239\",\"end\":\"22648\"},{\"start\":\"22675\",\"end\":\"22796\"},{\"start\":\"23029\",\"end\":\"23456\"},{\"start\":\"23458\",\"end\":\"23839\"},{\"start\":\"23841\",\"end\":\"24331\"},{\"start\":\"24333\",\"end\":\"24509\"},{\"start\":\"24534\",\"end\":\"24842\"},{\"start\":\"24896\",\"end\":\"25029\"},{\"start\":\"25131\",\"end\":\"25155\"},{\"start\":\"25157\",\"end\":\"25784\"},{\"start\":\"25970\",\"end\":\"26116\"},{\"start\":\"26167\",\"end\":\"26301\"},{\"start\":\"26378\",\"end\":\"26719\"},{\"start\":\"26734\",\"end\":\"26834\"},{\"start\":\"26958\",\"end\":\"28220\"},{\"start\":\"28222\",\"end\":\"28749\"},{\"start\":\"28751\",\"end\":\"29263\"},{\"start\":\"29265\",\"end\":\"29302\"},{\"start\":\"29429\",\"end\":\"29697\"},{\"start\":\"29727\",\"end\":\"30129\"},{\"start\":\"30131\",\"end\":\"30221\"},{\"start\":\"30235\",\"end\":\"31007\"},{\"start\":\"31009\",\"end\":\"31116\"},{\"start\":\"31118\",\"end\":\"31155\"},{\"start\":\"31306\",\"end\":\"31660\"},{\"start\":\"31662\",\"end\":\"31766\"},{\"start\":\"31800\",\"end\":\"31983\"},{\"start\":\"31985\",\"end\":\"32386\"},{\"start\":\"32438\",\"end\":\"33144\"},{\"start\":\"33207\",\"end\":\"33864\"},{\"start\":\"33866\",\"end\":\"34047\"},{\"start\":\"34096\",\"end\":\"34347\"},{\"start\":\"34367\",\"end\":\"34485\"},{\"start\":\"34504\",\"end\":\"35848\"},{\"start\":\"35876\",\"end\":\"36851\"},{\"start\":\"36853\",\"end\":\"38464\"},{\"start\":\"38494\",\"end\":\"38625\"},{\"start\":\"38627\",\"end\":\"38934\"},{\"start\":\"38936\",\"end\":\"39653\"},{\"start\":\"39655\",\"end\":\"40224\"},{\"start\":\"40226\",\"end\":\"40766\"},{\"start\":\"40768\",\"end\":\"42287\"},{\"start\":\"42289\",\"end\":\"42908\"},{\"start\":\"42927\",\"end\":\"44028\"},{\"start\":\"44131\",\"end\":\"44348\"},{\"start\":\"44382\",\"end\":\"44613\"},{\"start\":\"44964\",\"end\":\"45079\"},{\"start\":\"45167\",\"end\":\"45328\"},{\"start\":\"45635\",\"end\":\"45811\"},{\"start\":\"45872\",\"end\":\"45900\"},{\"start\":\"45958\",\"end\":\"45979\"},{\"start\":\"46013\",\"end\":\"46240\"},{\"start\":\"46283\",\"end\":\"46331\"},{\"start\":\"46333\",\"end\":\"46368\"},{\"start\":\"46370\",\"end\":\"46388\"},{\"start\":\"46390\",\"end\":\"46703\"},{\"start\":\"46735\",\"end\":\"46903\"},{\"start\":\"46951\",\"end\":\"47054\"},{\"start\":\"47095\",\"end\":\"47560\"},{\"start\":\"47621\",\"end\":\"47669\"},{\"start\":\"47703\",\"end\":\"47762\"},{\"start\":\"47798\",\"end\":\"48197\"},{\"start\":\"48199\",\"end\":\"48219\"}]", "formula": "[{\"start\":\"10796\",\"end\":\"10857\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"10975\",\"end\":\"11019\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"11272\",\"end\":\"11344\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"11678\",\"end\":\"11789\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"13352\",\"end\":\"13398\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"14107\",\"end\":\"14157\",\"attributes\":{\"id\":\"formula_5\"}},{\"start\":\"15054\",\"end\":\"15087\",\"attributes\":{\"id\":\"formula_6\"}},{\"start\":\"16020\",\"end\":\"16051\",\"attributes\":{\"id\":\"formula_7\"}},{\"start\":\"16371\",\"end\":\"16395\",\"attributes\":{\"id\":\"formula_8\"}},{\"start\":\"16628\",\"end\":\"16668\",\"attributes\":{\"id\":\"formula_9\"}},{\"start\":\"16987\",\"end\":\"17004\",\"attributes\":{\"id\":\"formula_10\"}},{\"start\":\"17922\",\"end\":\"17955\",\"attributes\":{\"id\":\"formula_11\"}},{\"start\":\"18058\",\"end\":\"18103\",\"attributes\":{\"id\":\"formula_12\"}},{\"start\":\"18308\",\"end\":\"18350\",\"attributes\":{\"id\":\"formula_13\"}},{\"start\":\"19477\",\"end\":\"19570\",\"attributes\":{\"id\":\"formula_14\"}},{\"start\":\"20370\",\"end\":\"20404\",\"attributes\":{\"id\":\"formula_15\"}},{\"start\":\"20492\",\"end\":\"20523\",\"attributes\":{\"id\":\"formula_16\"}},{\"start\":\"20899\",\"end\":\"20947\",\"attributes\":{\"id\":\"formula_17\"}},{\"start\":\"21312\",\"end\":\"21358\",\"attributes\":{\"id\":\"formula_18\"}},{\"start\":\"22168\",\"end\":\"22238\",\"attributes\":{\"id\":\"formula_19\"}},{\"start\":\"22797\",\"end\":\"22954\",\"attributes\":{\"id\":\"formula_20\"}},{\"start\":\"22954\",\"end\":\"22987\",\"attributes\":{\"id\":\"formula_21\"}},{\"start\":\"22987\",\"end\":\"23028\",\"attributes\":{\"id\":\"formula_22\"}},{\"start\":\"25030\",\"end\":\"25079\",\"attributes\":{\"id\":\"formula_23\"}},{\"start\":\"25079\",\"end\":\"25130\",\"attributes\":{\"id\":\"formula_24\"}},{\"start\":\"25785\",\"end\":\"25883\",\"attributes\":{\"id\":\"formula_25\"}},{\"start\":\"25883\",\"end\":\"25969\",\"attributes\":{\"id\":\"formula_26\"}},{\"start\":\"26117\",\"end\":\"26166\",\"attributes\":{\"id\":\"formula_27\"}},{\"start\":\"26302\",\"end\":\"26377\",\"attributes\":{\"id\":\"formula_28\"}},{\"start\":\"26835\",\"end\":\"26957\",\"attributes\":{\"id\":\"formula_29\"}},{\"start\":\"29303\",\"end\":\"29428\",\"attributes\":{\"id\":\"formula_30\"}},{\"start\":\"31156\",\"end\":\"31208\",\"attributes\":{\"id\":\"formula_31\"}},{\"start\":\"31208\",\"end\":\"31305\",\"attributes\":{\"id\":\"formula_32\"}},{\"start\":\"32387\",\"end\":\"32437\",\"attributes\":{\"id\":\"formula_33\"}},{\"start\":\"33145\",\"end\":\"33206\",\"attributes\":{\"id\":\"formula_34\"}},{\"start\":\"34048\",\"end\":\"34095\",\"attributes\":{\"id\":\"formula_35\"}},{\"start\":\"44029\",\"end\":\"44050\",\"attributes\":{\"id\":\"formula_36\"}},{\"start\":\"44050\",\"end\":\"44130\",\"attributes\":{\"id\":\"formula_37\"}},{\"start\":\"44614\",\"end\":\"44756\",\"attributes\":{\"id\":\"formula_38\"}},{\"start\":\"44756\",\"end\":\"44963\",\"attributes\":{\"id\":\"formula_39\"}},{\"start\":\"45080\",\"end\":\"45166\",\"attributes\":{\"id\":\"formula_40\"}},{\"start\":\"45329\",\"end\":\"45634\",\"attributes\":{\"id\":\"formula_41\"}},{\"start\":\"45812\",\"end\":\"45871\",\"attributes\":{\"id\":\"formula_42\"}},{\"start\":\"45901\",\"end\":\"45957\",\"attributes\":{\"id\":\"formula_43\"}},{\"start\":\"46241\",\"end\":\"46282\",\"attributes\":{\"id\":\"formula_44\"}},{\"start\":\"46904\",\"end\":\"46950\",\"attributes\":{\"id\":\"formula_48\"}},{\"start\":\"47055\",\"end\":\"47094\",\"attributes\":{\"id\":\"formula_49\"}},{\"start\":\"47561\",\"end\":\"47620\",\"attributes\":{\"id\":\"formula_50\"}},{\"start\":\"47763\",\"end\":\"47797\",\"attributes\":{\"id\":\"formula_51\"}}]", "table_ref": "[{\"start\":\"37044\",\"end\":\"37121\"},{\"start\":\"37246\",\"end\":\"37253\"}]", "section_header": "[{\"start\":\"2133\",\"end\":\"2148\"},{\"start\":\"9350\",\"end\":\"9385\"},{\"start\":\"9564\",\"end\":\"9578\"},{\"start\":\"10540\",\"end\":\"10552\"},{\"start\":\"12517\",\"end\":\"12542\"},{\"start\":\"14284\",\"end\":\"14303\"},{\"start\":\"18352\",\"end\":\"18385\"},{\"start\":\"18537\",\"end\":\"18579\"},{\"start\":\"22651\",\"end\":\"22673\"},{\"start\":\"24512\",\"end\":\"24532\"},{\"start\":\"24845\",\"end\":\"24894\"},{\"start\":\"26722\",\"end\":\"26732\"},{\"start\":\"29700\",\"end\":\"29725\"},{\"start\":\"30224\",\"end\":\"30233\"},{\"start\":\"31769\",\"end\":\"31798\"},{\"start\":\"34350\",\"end\":\"34365\"},{\"start\":\"34488\",\"end\":\"34502\"},{\"start\":\"35851\",\"end\":\"35874\"},{\"start\":\"38467\",\"end\":\"38492\"},{\"start\":\"42911\",\"end\":\"42925\"},{\"start\":\"44351\",\"end\":\"44380\"},{\"start\":\"45982\",\"end\":\"46011\"},{\"start\":\"46706\",\"end\":\"46733\"},{\"start\":\"47672\",\"end\":\"47701\"},{\"start\":\"48221\",\"end\":\"48229\"},{\"start\":\"48246\",\"end\":\"48254\"},{\"start\":\"48413\",\"end\":\"48421\"},{\"start\":\"48461\",\"end\":\"48469\"},{\"start\":\"48564\",\"end\":\"48572\"},{\"start\":\"48620\",\"end\":\"48628\"},{\"start\":\"48672\",\"end\":\"48680\"},{\"start\":\"48723\",\"end\":\"48731\"},{\"start\":\"48804\",\"end\":\"48812\"},{\"start\":\"48855\",\"end\":\"48864\"},{\"start\":\"49349\",\"end\":\"49353\"},{\"start\":\"50776\",\"end\":\"50781\"}]", "table": null, "figure_caption": "[{\"start\":\"48231\",\"end\":\"48244\"},{\"start\":\"48256\",\"end\":\"48411\"},{\"start\":\"48423\",\"end\":\"48459\"},{\"start\":\"48471\",\"end\":\"48562\"},{\"start\":\"48574\",\"end\":\"48618\"},{\"start\":\"48630\",\"end\":\"48670\"},{\"start\":\"48682\",\"end\":\"48721\"},{\"start\":\"48733\",\"end\":\"48802\"},{\"start\":\"48814\",\"end\":\"48853\"},{\"start\":\"48867\",\"end\":\"49125\"},{\"start\":\"49128\",\"end\":\"49272\"},{\"start\":\"49275\",\"end\":\"49347\"},{\"start\":\"49354\",\"end\":\"49876\"},{\"start\":\"49879\",\"end\":\"49904\"},{\"start\":\"49907\",\"end\":\"50774\"}]", "figure_ref": "[{\"start\":\"9765\",\"end\":\"9771\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"14556\",\"end\":\"14562\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"17370\",\"end\":\"17376\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"21768\",\"end\":\"21774\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"30443\",\"end\":\"30449\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"30821\",\"end\":\"30827\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"36244\",\"end\":\"36250\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"36255\",\"end\":\"36261\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"36307\",\"end\":\"36313\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"36318\",\"end\":\"36324\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"36377\",\"end\":\"36388\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"36621\",\"end\":\"36627\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"37406\",\"end\":\"37412\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"37417\",\"end\":\"37423\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"37823\",\"end\":\"37829\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"38256\",\"end\":\"38262\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"39731\",\"end\":\"39737\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"40331\",\"end\":\"40337\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"40806\",\"end\":\"40812\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"41062\",\"end\":\"41068\",\"attributes\":{\"ref_id\":\"fig_8\"}},{\"start\":\"41144\",\"end\":\"41155\",\"attributes\":{\"ref_id\":\"fig_8\"}},{\"start\":\"42421\",\"end\":\"42428\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"45036\",\"end\":\"45042\",\"attributes\":{\"ref_id\":\"fig_2\"}}]", "bib_author_first_name": "[{\"start\":\"51318\",\"end\":\"51319\"},{\"start\":\"51327\",\"end\":\"51328\"},{\"start\":\"51337\",\"end\":\"51338\"},{\"start\":\"51575\",\"end\":\"51576\"},{\"start\":\"51862\",\"end\":\"51863\"},{\"start\":\"51871\",\"end\":\"51872\"},{\"start\":\"51878\",\"end\":\"51879\"},{\"start\":\"51885\",\"end\":\"51886\"},{\"start\":\"51891\",\"end\":\"51892\"},{\"start\":\"52144\",\"end\":\"52145\"},{\"start\":\"52151\",\"end\":\"52152\"},{\"start\":\"52158\",\"end\":\"52159\"},{\"start\":\"52166\",\"end\":\"52167\"},{\"start\":\"52174\",\"end\":\"52175\"},{\"start\":\"52517\",\"end\":\"52518\"},{\"start\":\"52525\",\"end\":\"52526\"},{\"start\":\"52532\",\"end\":\"52533\"},{\"start\":\"52540\",\"end\":\"52541\"},{\"start\":\"52552\",\"end\":\"52553\"},{\"start\":\"52813\",\"end\":\"52814\"},{\"start\":\"52821\",\"end\":\"52822\"},{\"start\":\"52835\",\"end\":\"52836\"},{\"start\":\"52845\",\"end\":\"52846\"},{\"start\":\"53102\",\"end\":\"53103\"},{\"start\":\"53109\",\"end\":\"53110\"},{\"start\":\"53116\",\"end\":\"53117\"},{\"start\":\"53122\",\"end\":\"53123\"},{\"start\":\"53129\",\"end\":\"53130\"},{\"start\":\"53138\",\"end\":\"53139\"},{\"start\":\"53427\",\"end\":\"53428\"},{\"start\":\"53429\",\"end\":\"53432\"},{\"start\":\"53747\",\"end\":\"53748\"},{\"start\":\"53758\",\"end\":\"53759\"},{\"start\":\"53769\",\"end\":\"53770\"},{\"start\":\"53999\",\"end\":\"54000\"},{\"start\":\"54006\",\"end\":\"54007\"},{\"start\":\"54013\",\"end\":\"54014\"},{\"start\":\"54022\",\"end\":\"54023\"},{\"start\":\"54031\",\"end\":\"54032\"},{\"start\":\"54033\",\"end\":\"54034\"},{\"start\":\"54339\",\"end\":\"54340\"},{\"start\":\"54346\",\"end\":\"54347\"},{\"start\":\"54354\",\"end\":\"54355\"},{\"start\":\"54673\",\"end\":\"54674\"},{\"start\":\"54680\",\"end\":\"54681\"},{\"start\":\"54688\",\"end\":\"54689\"},{\"start\":\"54976\",\"end\":\"54977\"},{\"start\":\"54984\",\"end\":\"54985\"},{\"start\":\"54990\",\"end\":\"54991\"},{\"start\":\"54992\",\"end\":\"54993\"},{\"start\":\"55001\",\"end\":\"55002\"},{\"start\":\"55374\",\"end\":\"55375\"},{\"start\":\"55381\",\"end\":\"55382\"},{\"start\":\"55389\",\"end\":\"55390\"},{\"start\":\"55731\",\"end\":\"55732\"},{\"start\":\"55733\",\"end\":\"55734\"},{\"start\":\"55741\",\"end\":\"55742\"},{\"start\":\"55748\",\"end\":\"55749\"},{\"start\":\"55750\",\"end\":\"55751\"},{\"start\":\"55760\",\"end\":\"55761\"},{\"start\":\"55762\",\"end\":\"55765\"},{\"start\":\"55772\",\"end\":\"55773\"},{\"start\":\"55774\",\"end\":\"55775\"},{\"start\":\"56128\",\"end\":\"56129\"},{\"start\":\"56136\",\"end\":\"56137\"},{\"start\":\"56144\",\"end\":\"56145\"},{\"start\":\"56152\",\"end\":\"56153\"},{\"start\":\"56154\",\"end\":\"56155\"},{\"start\":\"56162\",\"end\":\"56163\"},{\"start\":\"56448\",\"end\":\"56449\"},{\"start\":\"56456\",\"end\":\"56457\"},{\"start\":\"56464\",\"end\":\"56465\"},{\"start\":\"56472\",\"end\":\"56473\"},{\"start\":\"56479\",\"end\":\"56480\"},{\"start\":\"56481\",\"end\":\"56482\"},{\"start\":\"56489\",\"end\":\"56490\"},{\"start\":\"56763\",\"end\":\"56764\"},{\"start\":\"56770\",\"end\":\"56771\"},{\"start\":\"56776\",\"end\":\"56777\"},{\"start\":\"56986\",\"end\":\"56987\"},{\"start\":\"57003\",\"end\":\"57004\"},{\"start\":\"57203\",\"end\":\"57204\"},{\"start\":\"57210\",\"end\":\"57211\"},{\"start\":\"57528\",\"end\":\"57529\"},{\"start\":\"57537\",\"end\":\"57538\"},{\"start\":\"57545\",\"end\":\"57546\"},{\"start\":\"57555\",\"end\":\"57556\"},{\"start\":\"57568\",\"end\":\"57569\"},{\"start\":\"57831\",\"end\":\"57832\"},{\"start\":\"57838\",\"end\":\"57839\"},{\"start\":\"57844\",\"end\":\"57845\"},{\"start\":\"57851\",\"end\":\"57852\"},{\"start\":\"57857\",\"end\":\"57858\"},{\"start\":\"57866\",\"end\":\"57867\"},{\"start\":\"58184\",\"end\":\"58185\"},{\"start\":\"58191\",\"end\":\"58192\"},{\"start\":\"58198\",\"end\":\"58199\"},{\"start\":\"58207\",\"end\":\"58208\"},{\"start\":\"58628\",\"end\":\"58629\"},{\"start\":\"58635\",\"end\":\"58636\"},{\"start\":\"58642\",\"end\":\"58643\"},{\"start\":\"58651\",\"end\":\"58652\"},{\"start\":\"58948\",\"end\":\"58949\"},{\"start\":\"58962\",\"end\":\"58963\"},{\"start\":\"58972\",\"end\":\"58973\"},{\"start\":\"59199\",\"end\":\"59200\"},{\"start\":\"59216\",\"end\":\"59217\"},{\"start\":\"59834\",\"end\":\"59835\"},{\"start\":\"59844\",\"end\":\"59845\"},{\"start\":\"59855\",\"end\":\"59856\"},{\"start\":\"60222\",\"end\":\"60223\"},{\"start\":\"60229\",\"end\":\"60230\"},{\"start\":\"60235\",\"end\":\"60236\"},{\"start\":\"60242\",\"end\":\"60243\"},{\"start\":\"60555\",\"end\":\"60556\"},{\"start\":\"60563\",\"end\":\"60564\"},{\"start\":\"60565\",\"end\":\"60566\"},{\"start\":\"60578\",\"end\":\"60579\"},{\"start\":\"60585\",\"end\":\"60586\"},{\"start\":\"60913\",\"end\":\"60914\"},{\"start\":\"61119\",\"end\":\"61120\"},{\"start\":\"61121\",\"end\":\"61122\"},{\"start\":\"61295\",\"end\":\"61296\"},{\"start\":\"61303\",\"end\":\"61304\"},{\"start\":\"61561\",\"end\":\"61562\"},{\"start\":\"61569\",\"end\":\"61570\"}]", "bib_author_last_name": "[{\"start\":\"51320\",\"end\":\"51325\"},{\"start\":\"51329\",\"end\":\"51335\"},{\"start\":\"51339\",\"end\":\"51345\"},{\"start\":\"51577\",\"end\":\"51580\"},{\"start\":\"51864\",\"end\":\"51869\"},{\"start\":\"51873\",\"end\":\"51876\"},{\"start\":\"51880\",\"end\":\"51883\"},{\"start\":\"51887\",\"end\":\"51889\"},{\"start\":\"51893\",\"end\":\"51897\"},{\"start\":\"52146\",\"end\":\"52149\"},{\"start\":\"52153\",\"end\":\"52156\"},{\"start\":\"52160\",\"end\":\"52164\"},{\"start\":\"52168\",\"end\":\"52172\"},{\"start\":\"52176\",\"end\":\"52182\"},{\"start\":\"52519\",\"end\":\"52523\"},{\"start\":\"52527\",\"end\":\"52530\"},{\"start\":\"52534\",\"end\":\"52538\"},{\"start\":\"52542\",\"end\":\"52550\"},{\"start\":\"52554\",\"end\":\"52557\"},{\"start\":\"52815\",\"end\":\"52819\"},{\"start\":\"52823\",\"end\":\"52833\"},{\"start\":\"52837\",\"end\":\"52843\"},{\"start\":\"52847\",\"end\":\"52853\"},{\"start\":\"53104\",\"end\":\"53107\"},{\"start\":\"53111\",\"end\":\"53114\"},{\"start\":\"53118\",\"end\":\"53120\"},{\"start\":\"53124\",\"end\":\"53127\"},{\"start\":\"53131\",\"end\":\"53136\"},{\"start\":\"53140\",\"end\":\"53145\"},{\"start\":\"53433\",\"end\":\"53436\"},{\"start\":\"53749\",\"end\":\"53756\"},{\"start\":\"53760\",\"end\":\"53767\"},{\"start\":\"53771\",\"end\":\"53777\"},{\"start\":\"54001\",\"end\":\"54004\"},{\"start\":\"54008\",\"end\":\"54011\"},{\"start\":\"54015\",\"end\":\"54020\"},{\"start\":\"54024\",\"end\":\"54029\"},{\"start\":\"54035\",\"end\":\"54042\"},{\"start\":\"54341\",\"end\":\"54344\"},{\"start\":\"54348\",\"end\":\"54352\"},{\"start\":\"54356\",\"end\":\"54359\"},{\"start\":\"54675\",\"end\":\"54678\"},{\"start\":\"54682\",\"end\":\"54686\"},{\"start\":\"54690\",\"end\":\"54695\"},{\"start\":\"54978\",\"end\":\"54982\"},{\"start\":\"54986\",\"end\":\"54988\"},{\"start\":\"54994\",\"end\":\"54999\"},{\"start\":\"55003\",\"end\":\"55008\"},{\"start\":\"55376\",\"end\":\"55379\"},{\"start\":\"55383\",\"end\":\"55387\"},{\"start\":\"55391\",\"end\":\"55397\"},{\"start\":\"55735\",\"end\":\"55739\"},{\"start\":\"55743\",\"end\":\"55746\"},{\"start\":\"55752\",\"end\":\"55758\"},{\"start\":\"55766\",\"end\":\"55770\"},{\"start\":\"55776\",\"end\":\"55780\"},{\"start\":\"56130\",\"end\":\"56134\"},{\"start\":\"56138\",\"end\":\"56142\"},{\"start\":\"56146\",\"end\":\"56150\"},{\"start\":\"56156\",\"end\":\"56160\"},{\"start\":\"56164\",\"end\":\"56176\"},{\"start\":\"56450\",\"end\":\"56454\"},{\"start\":\"56458\",\"end\":\"56462\"},{\"start\":\"56466\",\"end\":\"56470\"},{\"start\":\"56474\",\"end\":\"56477\"},{\"start\":\"56483\",\"end\":\"56487\"},{\"start\":\"56491\",\"end\":\"56494\"},{\"start\":\"56765\",\"end\":\"56768\"},{\"start\":\"56772\",\"end\":\"56774\"},{\"start\":\"56778\",\"end\":\"56782\"},{\"start\":\"56988\",\"end\":\"57001\"},{\"start\":\"57005\",\"end\":\"57012\"},{\"start\":\"57205\",\"end\":\"57208\"},{\"start\":\"57212\",\"end\":\"57217\"},{\"start\":\"57530\",\"end\":\"57535\"},{\"start\":\"57539\",\"end\":\"57543\"},{\"start\":\"57547\",\"end\":\"57553\"},{\"start\":\"57557\",\"end\":\"57566\"},{\"start\":\"57570\",\"end\":\"57576\"},{\"start\":\"57833\",\"end\":\"57836\"},{\"start\":\"57840\",\"end\":\"57842\"},{\"start\":\"57846\",\"end\":\"57849\"},{\"start\":\"57853\",\"end\":\"57855\"},{\"start\":\"57859\",\"end\":\"57864\"},{\"start\":\"57868\",\"end\":\"57871\"},{\"start\":\"58186\",\"end\":\"58189\"},{\"start\":\"58193\",\"end\":\"58196\"},{\"start\":\"58200\",\"end\":\"58205\"},{\"start\":\"58209\",\"end\":\"58214\"},{\"start\":\"58630\",\"end\":\"58633\"},{\"start\":\"58637\",\"end\":\"58640\"},{\"start\":\"58644\",\"end\":\"58649\"},{\"start\":\"58653\",\"end\":\"58658\"},{\"start\":\"58950\",\"end\":\"58960\"},{\"start\":\"58964\",\"end\":\"58970\"},{\"start\":\"58974\",\"end\":\"58983\"},{\"start\":\"59201\",\"end\":\"59214\"},{\"start\":\"59218\",\"end\":\"59225\"},{\"start\":\"59836\",\"end\":\"59842\"},{\"start\":\"59846\",\"end\":\"59853\"},{\"start\":\"59857\",\"end\":\"59865\"},{\"start\":\"60224\",\"end\":\"60227\"},{\"start\":\"60231\",\"end\":\"60233\"},{\"start\":\"60237\",\"end\":\"60240\"},{\"start\":\"60244\",\"end\":\"60246\"},{\"start\":\"60557\",\"end\":\"60561\"},{\"start\":\"60567\",\"end\":\"60576\"},{\"start\":\"60580\",\"end\":\"60583\"},{\"start\":\"60587\",\"end\":\"60590\"},{\"start\":\"60915\",\"end\":\"60921\"},{\"start\":\"61123\",\"end\":\"61131\"},{\"start\":\"61297\",\"end\":\"61301\"},{\"start\":\"61305\",\"end\":\"61317\"},{\"start\":\"61563\",\"end\":\"61567\"},{\"start\":\"61571\",\"end\":\"61573\"}]", "bib_entry": "[{\"start\":\"51303\",\"end\":\"51451\",\"attributes\":{\"matched_paper_id\":\"1779661\",\"id\":\"b0\"}},{\"start\":\"51453\",\"end\":\"51810\",\"attributes\":{\"matched_paper_id\":\"21288754\",\"id\":\"b1\"}},{\"start\":\"51812\",\"end\":\"52061\",\"attributes\":{\"id\":\"b2\"}},{\"start\":\"52063\",\"end\":\"52445\",\"attributes\":{\"matched_paper_id\":\"212431253\",\"id\":\"b3\",\"doi\":\"10.1109/MWC.001.1900516\"}},{\"start\":\"52447\",\"end\":\"52768\",\"attributes\":{\"matched_paper_id\":\"216370554\",\"id\":\"b4\"}},{\"start\":\"52770\",\"end\":\"53024\",\"attributes\":{\"matched_paper_id\":\"54457410\",\"id\":\"b5\"}},{\"start\":\"53026\",\"end\":\"53357\",\"attributes\":{\"matched_paper_id\":\"52155776\",\"id\":\"b6\"}},{\"start\":\"53359\",\"end\":\"53673\",\"attributes\":{\"matched_paper_id\":\"202888951\",\"id\":\"b7\",\"doi\":\"10.1109/COMST.2020.2986024\"}},{\"start\":\"53675\",\"end\":\"53931\",\"attributes\":{\"id\":\"b8\",\"doi\":\"arXiv:1511.03575\"}},{\"start\":\"53933\",\"end\":\"54264\",\"attributes\":{\"matched_paper_id\":\"206578365\",\"id\":\"b9\"}},{\"start\":\"54266\",\"end\":\"54602\",\"attributes\":{\"matched_paper_id\":\"207869825\",\"id\":\"b10\"}},{\"start\":\"54604\",\"end\":\"54902\",\"attributes\":{\"matched_paper_id\":\"58004591\",\"id\":\"b11\"}},{\"start\":\"54904\",\"end\":\"55296\",\"attributes\":{\"matched_paper_id\":\"196623634\",\"id\":\"b12\"}},{\"start\":\"55298\",\"end\":\"55646\",\"attributes\":{\"matched_paper_id\":\"207869996\",\"id\":\"b13\"}},{\"start\":\"55648\",\"end\":\"56052\",\"attributes\":{\"matched_paper_id\":\"86439367\",\"id\":\"b14\"}},{\"start\":\"56054\",\"end\":\"56353\",\"attributes\":{\"id\":\"b15\",\"doi\":\"arXiv:1911.02417\"}},{\"start\":\"56355\",\"end\":\"56691\",\"attributes\":{\"id\":\"b16\",\"doi\":\"arXiv:1909.07972\"}},{\"start\":\"56693\",\"end\":\"56923\",\"attributes\":{\"id\":\"b17\",\"doi\":\"arXiv:1905.09712\"}},{\"start\":\"56925\",\"end\":\"57149\",\"attributes\":{\"id\":\"b18\",\"doi\":\"arXiv:1706.00043\"}},{\"start\":\"57151\",\"end\":\"57464\",\"attributes\":{\"matched_paper_id\":\"51805403\",\"id\":\"b19\"}},{\"start\":\"57466\",\"end\":\"57742\",\"attributes\":{\"id\":\"b20\",\"doi\":\"arXiv:1511.06481\"}},{\"start\":\"57744\",\"end\":\"58104\",\"attributes\":{\"matched_paper_id\":\"214743231\",\"id\":\"b21\"}},{\"start\":\"58106\",\"end\":\"58537\",\"attributes\":{\"matched_paper_id\":\"125922387\",\"id\":\"b22\"}},{\"start\":\"58539\",\"end\":\"58946\",\"attributes\":{\"matched_paper_id\":\"203837065\",\"id\":\"b23\",\"doi\":\"10.1109/TCCN.2020.2999606\"}},{\"start\":\"58948\",\"end\":\"59122\",\"attributes\":{\"id\":\"b24\"}},{\"start\":\"59124\",\"end\":\"59376\",\"attributes\":{\"id\":\"b25\",\"doi\":\"arXiv:1803.00942\"}},{\"start\":\"59378\",\"end\":\"59779\",\"attributes\":{\"id\":\"b26\",\"doi\":\"TS 36 211 V15.6.0\"}},{\"start\":\"59781\",\"end\":\"60136\",\"attributes\":{\"matched_paper_id\":\"3323727\",\"id\":\"b27\"}},{\"start\":\"60138\",\"end\":\"60471\",\"attributes\":{\"matched_paper_id\":\"7617670\",\"id\":\"b28\"}},{\"start\":\"60473\",\"end\":\"60870\",\"attributes\":{\"matched_paper_id\":\"44061071\",\"id\":\"b29\"}},{\"start\":\"60872\",\"end\":\"61045\",\"attributes\":{\"matched_paper_id\":\"33241462\",\"id\":\"b30\"}},{\"start\":\"61047\",\"end\":\"61293\",\"attributes\":{\"id\":\"b31\"}},{\"start\":\"61295\",\"end\":\"61471\",\"attributes\":{\"id\":\"b32\"}},{\"start\":\"61473\",\"end\":\"61787\",\"attributes\":{\"matched_paper_id\":\"3614172\",\"id\":\"b33\"}}]", "bib_title": "[{\"start\":\"51303\",\"end\":\"51316\"},{\"start\":\"51453\",\"end\":\"51573\"},{\"start\":\"51812\",\"end\":\"51860\"},{\"start\":\"52063\",\"end\":\"52142\"},{\"start\":\"52447\",\"end\":\"52515\"},{\"start\":\"52770\",\"end\":\"52811\"},{\"start\":\"53026\",\"end\":\"53100\"},{\"start\":\"53359\",\"end\":\"53425\"},{\"start\":\"53933\",\"end\":\"53997\"},{\"start\":\"54266\",\"end\":\"54337\"},{\"start\":\"54604\",\"end\":\"54671\"},{\"start\":\"54904\",\"end\":\"54974\"},{\"start\":\"55298\",\"end\":\"55372\"},{\"start\":\"55648\",\"end\":\"55729\"},{\"start\":\"57151\",\"end\":\"57201\"},{\"start\":\"57744\",\"end\":\"57829\"},{\"start\":\"58106\",\"end\":\"58182\"},{\"start\":\"58539\",\"end\":\"58626\"},{\"start\":\"59781\",\"end\":\"59832\"},{\"start\":\"60138\",\"end\":\"60220\"},{\"start\":\"60473\",\"end\":\"60553\"},{\"start\":\"60872\",\"end\":\"60911\"},{\"start\":\"61047\",\"end\":\"61117\"},{\"start\":\"61473\",\"end\":\"61559\"}]", "bib_author": "[{\"start\":\"51318\",\"end\":\"51327\"},{\"start\":\"51327\",\"end\":\"51337\"},{\"start\":\"51337\",\"end\":\"51347\"},{\"start\":\"51575\",\"end\":\"51582\"},{\"start\":\"51862\",\"end\":\"51871\"},{\"start\":\"51871\",\"end\":\"51878\"},{\"start\":\"51878\",\"end\":\"51885\"},{\"start\":\"51885\",\"end\":\"51891\"},{\"start\":\"51891\",\"end\":\"51899\"},{\"start\":\"52144\",\"end\":\"52151\"},{\"start\":\"52151\",\"end\":\"52158\"},{\"start\":\"52158\",\"end\":\"52166\"},{\"start\":\"52166\",\"end\":\"52174\"},{\"start\":\"52174\",\"end\":\"52184\"},{\"start\":\"52517\",\"end\":\"52525\"},{\"start\":\"52525\",\"end\":\"52532\"},{\"start\":\"52532\",\"end\":\"52540\"},{\"start\":\"52540\",\"end\":\"52552\"},{\"start\":\"52552\",\"end\":\"52559\"},{\"start\":\"52813\",\"end\":\"52821\"},{\"start\":\"52821\",\"end\":\"52835\"},{\"start\":\"52835\",\"end\":\"52845\"},{\"start\":\"52845\",\"end\":\"52855\"},{\"start\":\"53102\",\"end\":\"53109\"},{\"start\":\"53109\",\"end\":\"53116\"},{\"start\":\"53116\",\"end\":\"53122\"},{\"start\":\"53122\",\"end\":\"53129\"},{\"start\":\"53129\",\"end\":\"53138\"},{\"start\":\"53138\",\"end\":\"53147\"},{\"start\":\"53427\",\"end\":\"53438\"},{\"start\":\"53747\",\"end\":\"53758\"},{\"start\":\"53758\",\"end\":\"53769\"},{\"start\":\"53769\",\"end\":\"53779\"},{\"start\":\"53999\",\"end\":\"54006\"},{\"start\":\"54006\",\"end\":\"54013\"},{\"start\":\"54013\",\"end\":\"54022\"},{\"start\":\"54022\",\"end\":\"54031\"},{\"start\":\"54031\",\"end\":\"54044\"},{\"start\":\"54339\",\"end\":\"54346\"},{\"start\":\"54346\",\"end\":\"54354\"},{\"start\":\"54354\",\"end\":\"54361\"},{\"start\":\"54673\",\"end\":\"54680\"},{\"start\":\"54680\",\"end\":\"54688\"},{\"start\":\"54688\",\"end\":\"54697\"},{\"start\":\"54976\",\"end\":\"54984\"},{\"start\":\"54984\",\"end\":\"54990\"},{\"start\":\"54990\",\"end\":\"55001\"},{\"start\":\"55001\",\"end\":\"55010\"},{\"start\":\"55374\",\"end\":\"55381\"},{\"start\":\"55381\",\"end\":\"55389\"},{\"start\":\"55389\",\"end\":\"55399\"},{\"start\":\"55731\",\"end\":\"55741\"},{\"start\":\"55741\",\"end\":\"55748\"},{\"start\":\"55748\",\"end\":\"55760\"},{\"start\":\"55760\",\"end\":\"55772\"},{\"start\":\"55772\",\"end\":\"55782\"},{\"start\":\"56128\",\"end\":\"56136\"},{\"start\":\"56136\",\"end\":\"56144\"},{\"start\":\"56144\",\"end\":\"56152\"},{\"start\":\"56152\",\"end\":\"56162\"},{\"start\":\"56162\",\"end\":\"56178\"},{\"start\":\"56448\",\"end\":\"56456\"},{\"start\":\"56456\",\"end\":\"56464\"},{\"start\":\"56464\",\"end\":\"56472\"},{\"start\":\"56472\",\"end\":\"56479\"},{\"start\":\"56479\",\"end\":\"56489\"},{\"start\":\"56489\",\"end\":\"56496\"},{\"start\":\"56763\",\"end\":\"56770\"},{\"start\":\"56770\",\"end\":\"56776\"},{\"start\":\"56776\",\"end\":\"56784\"},{\"start\":\"56986\",\"end\":\"57003\"},{\"start\":\"57003\",\"end\":\"57014\"},{\"start\":\"57203\",\"end\":\"57210\"},{\"start\":\"57210\",\"end\":\"57219\"},{\"start\":\"57528\",\"end\":\"57537\"},{\"start\":\"57537\",\"end\":\"57545\"},{\"start\":\"57545\",\"end\":\"57555\"},{\"start\":\"57555\",\"end\":\"57568\"},{\"start\":\"57568\",\"end\":\"57578\"},{\"start\":\"57831\",\"end\":\"57838\"},{\"start\":\"57838\",\"end\":\"57844\"},{\"start\":\"57844\",\"end\":\"57851\"},{\"start\":\"57851\",\"end\":\"57857\"},{\"start\":\"57857\",\"end\":\"57866\"},{\"start\":\"57866\",\"end\":\"57873\"},{\"start\":\"58184\",\"end\":\"58191\"},{\"start\":\"58191\",\"end\":\"58198\"},{\"start\":\"58198\",\"end\":\"58207\"},{\"start\":\"58207\",\"end\":\"58216\"},{\"start\":\"58628\",\"end\":\"58635\"},{\"start\":\"58635\",\"end\":\"58642\"},{\"start\":\"58642\",\"end\":\"58651\"},{\"start\":\"58651\",\"end\":\"58660\"},{\"start\":\"58948\",\"end\":\"58962\"},{\"start\":\"58962\",\"end\":\"58972\"},{\"start\":\"58972\",\"end\":\"58985\"},{\"start\":\"59199\",\"end\":\"59216\"},{\"start\":\"59216\",\"end\":\"59227\"},{\"start\":\"59834\",\"end\":\"59844\"},{\"start\":\"59844\",\"end\":\"59855\"},{\"start\":\"59855\",\"end\":\"59867\"},{\"start\":\"60222\",\"end\":\"60229\"},{\"start\":\"60229\",\"end\":\"60235\"},{\"start\":\"60235\",\"end\":\"60242\"},{\"start\":\"60242\",\"end\":\"60248\"},{\"start\":\"60555\",\"end\":\"60563\"},{\"start\":\"60563\",\"end\":\"60578\"},{\"start\":\"60578\",\"end\":\"60585\"},{\"start\":\"60585\",\"end\":\"60592\"},{\"start\":\"60913\",\"end\":\"60923\"},{\"start\":\"61119\",\"end\":\"61133\"},{\"start\":\"61295\",\"end\":\"61303\"},{\"start\":\"61303\",\"end\":\"61319\"},{\"start\":\"61561\",\"end\":\"61569\"},{\"start\":\"61569\",\"end\":\"61575\"}]", "bib_venue": "[{\"start\":\"52867\",\"end\":\"52871\"},{\"start\":\"54391\",\"end\":\"54428\"},{\"start\":\"55051\",\"end\":\"55099\"},{\"start\":\"55429\",\"end\":\"55466\"},{\"start\":\"55815\",\"end\":\"55840\"},{\"start\":\"57260\",\"end\":\"57307\"},{\"start\":\"58262\",\"end\":\"58310\"},{\"start\":\"59000\",\"end\":\"59018\"},{\"start\":\"59908\",\"end\":\"59958\"},{\"start\":\"60619\",\"end\":\"60654\"},{\"start\":\"61149\",\"end\":\"61164\"},{\"start\":\"61340\",\"end\":\"61355\"},{\"start\":\"51347\",\"end\":\"51353\"},{\"start\":\"51582\",\"end\":\"51600\"},{\"start\":\"51899\",\"end\":\"51908\"},{\"start\":\"52207\",\"end\":\"52227\"},{\"start\":\"52559\",\"end\":\"52579\"},{\"start\":\"52855\",\"end\":\"52865\"},{\"start\":\"53147\",\"end\":\"53163\"},{\"start\":\"53464\",\"end\":\"53489\"},{\"start\":\"53675\",\"end\":\"53745\"},{\"start\":\"54044\",\"end\":\"54068\"},{\"start\":\"54361\",\"end\":\"54389\"},{\"start\":\"54697\",\"end\":\"54724\"},{\"start\":\"55010\",\"end\":\"55049\"},{\"start\":\"55399\",\"end\":\"55427\"},{\"start\":\"55782\",\"end\":\"55813\"},{\"start\":\"56054\",\"end\":\"56126\"},{\"start\":\"56355\",\"end\":\"56446\"},{\"start\":\"56693\",\"end\":\"56761\"},{\"start\":\"56925\",\"end\":\"56984\"},{\"start\":\"57219\",\"end\":\"57258\"},{\"start\":\"57466\",\"end\":\"57526\"},{\"start\":\"57873\",\"end\":\"57900\"},{\"start\":\"58216\",\"end\":\"58260\"},{\"start\":\"58685\",\"end\":\"58715\"},{\"start\":\"58985\",\"end\":\"58998\"},{\"start\":\"59124\",\"end\":\"59197\"},{\"start\":\"59378\",\"end\":\"59511\"},{\"start\":\"59867\",\"end\":\"59906\"},{\"start\":\"60248\",\"end\":\"60275\"},{\"start\":\"60592\",\"end\":\"60617\"},{\"start\":\"60923\",\"end\":\"60944\"},{\"start\":\"61133\",\"end\":\"61147\"},{\"start\":\"61319\",\"end\":\"61338\"},{\"start\":\"61575\",\"end\":\"61601\"}]"}}}, "year": 2023, "month": 12, "day": 17}