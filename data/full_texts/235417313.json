{"id": 235417313, "updated": "2023-10-06 01:49:47.358", "metadata": {"title": "FedBABU: Towards Enhanced Representation for Federated Image Classification", "authors": "[{\"first\":\"Jaehoon\",\"last\":\"Oh\",\"middle\":[]},{\"first\":\"Sangmook\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Se-Young\",\"last\":\"Yun\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Federated learning has evolved to improve a single global model under data heterogeneity (as a curse) or to develop multiple personalized models using data heterogeneity (as a blessing). However, little research has considered both directions simultaneously. In this paper, we first investigate the relationship between them by analyzing Federated Averaging at the client level and determine that a better federated global model performance does not constantly improve personalization. To elucidate the cause of this personalization performance degradation problem, we decompose the entire network into the body (extractor), which is related to universality, and the head (classifier), which is related to personalization. We then point out that this problem stems from training the head. Based on this observation, we propose a novel federated learning algorithm, coined FedBABU, which only updates the body of the model during federated training (i.e., the head is randomly initialized and never updated), and the head is fine-tuned for personalization during the evaluation process. Extensive experiments show consistent performance improvements and an efficient personalization of FedBABU. The code is available at https://github.com/jhoon-oh/FedBABU.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2106.06042", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/OhKY22", "doi": null}}, "content": {"source": {"pdf_hash": "3cd5b67f7bc25a14a9505d27a5b18cfb5592769b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2106.06042v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b7c11975290b961c4f97dd0be421f5d05164ad4d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3cd5b67f7bc25a14a9505d27a5b18cfb5592769b.txt", "contents": "\nFEDBABU: TOWARD ENHANCED REPRESENTATION FOR FEDERATED IMAGE CLASSIFICATION\n\n\nJaehoon Oh jhoon.oh@kaist.ac.kr \nGraduate School of AI\nGraduate School of KSE\nKAIST\nKAIST\n\n\nSangmook Kim sangmook.kim@kaist.ac.kr \nGraduate School of AI\nGraduate School of KSE\nKAIST\nKAIST\n\n\nSe-Young Yun yunseyoung@kaist.ac.kr \nGraduate School of AI\nGraduate School of KSE\nKAIST\nKAIST\n\n\nFEDBABU: TOWARD ENHANCED REPRESENTATION FOR FEDERATED IMAGE CLASSIFICATION\nPublished as a conference paper at ICLR 2022\nFederated learning has evolved to improve a single global model under data heterogeneity (as a curse) or to develop multiple personalized models using data heterogeneity (as a blessing). However, little research has considered both directions simultaneously. In this paper, we first investigate the relationship between them by analyzing Federated Averaging (McMahan et al., 2017)  at the client level and determine that a better federated global model performance does not constantly improve personalization. To elucidate the cause of this personalization performance degradation problem, we decompose the entire network into the body (extractor), which is related to universality, and the head (classifier), which is related to personalization. We then point out that this problem stems from training the head. Based on this observation, we propose a novel federated learning algorithm, coined FedBABU, which only updates the body of the model during federated training (i.e., the head is randomly initialized and never updated), and the head is fine-tuned for personalization during the evaluation process. Extensive experiments show consistent performance improvements and an efficient personalization of FedBABU.Same classifierBroadcastingLocal Update AggregationClient 1 Client 2 Client 3Central Server (b) FedBABU. We control FL environments with three hyperparameters: client fraction ratio f , local epochs \u03c4 , and shards per user s. f is the number of participating clients out of the total number of clients in every round and a small f is natural in the FL settings because the total number of clients is numerous. The local epochs \u03c4 are equal to the interval between two consecutive communication rounds. To fix the number of total updates to ensure the consistency in all experiments, we fix the product of communication rounds and local epochs to 320 (e.g., if local epochs are four, then the total number of communication rounds is 80). The learning rate starts with 0.1 and is decayed by a factor of 0.1 at half and three-quarters of total updates. \u03c4 is closely related to the trade-off between accuracy and communication costs. A small \u03c4 provides an accurate federation but requires considerable communication costs. s is related to the maximum number of classes each user can have; hence, as s decreases, the degree of data heterogeneity increases.Evaluation We calculate the initial accuracy and personalized accuracy of FedAvg and FedBABU following the federated personalization evaluation procedure proposed in Wang et al. (2019)  to analyze the algorithms at the client level: (1) the learned global model is broadcast to all clients and is then evaluated on the test data set of each client D ts i (referred to as the initial accuracy), (2) the learned global model is personalized using the training data set of each client D tr i by fine-tuning with the fine-tuning epochs of \u03c4 f ; the personalized models are then evaluated on the test data set of each client D ts i (referred to as the personalized accuracy). In addition, we calculate the personalized accuracy of other personalized FL algorithms (such as FedPer, LG-FedAvg, and FedRep). Algorithm 2 in Appendix A describes the evaluation procedure. The values (X\u00b1Y) in all tables indicate the mean\u00b1standard deviation of the accuracies across all clients, not across multiple seeds. Here, reducing the variance over the clients could be interesting but goes beyond the scope of this study.Published as a conference paper at ICLR 2022 : Pre-training of deep bidirectional transformers for language understanding. In Astraea: Self-balancing federated learning for improving classification accuracy of mobile deep learning applications. federated learning: A metalearning approach. arXiv preprint arXiv:2002.07948, 2020.Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. . Improving federated learning personalization via model agnostic meta learning. arXiv preprint arXiv:\n\nINTRODUCTION\n\nFederated learning (FL) (McMahan et al., 2017), which is a distributed learning framework with personalized data, has become an attractive field of research. From the early days of this field, improving a single global model across devices has been the main objective (Zhao et al., 2018;Duan et al., 2019;Li et al., 2018;Acar et al., 2021), where the global model suffers from data heterogeneity. Many researchers have recently focused on multiple personalized models by leveraging data heterogeneity across devices as a blessing in disguise (Chen et al., 2018;Dinh et al., 2021;Zhang et al., 2020;Fallah et al., 2020;Shamsian et al., 2021;Smith et al., 2017). Although many studies have been conducted on each research line, a lack of research remains on how to train a good global model for personalization purposes (Ji et al., 2021;Jiang et al., 2019). In this study, for personalized training, each local client model starts from a global model that learns information from all clients and leverages the global model to fit its own data distribution. Jiang et al. (2019) analyzed personalization methods that adapt to the global model through finetuning on each device. They observed that the effects of fine-tuning are encouraging and that training in a central location increases the initial accuracy (of a single global model) while decreasing the personalized accuracy (of on-device fine-tuned models). We focus on why opposite changes in the two performances appear. This suggests that the factors for universality and personalization must be dealt with separately, inspiring us to decouple the entire network into the body (i.e., extractor) related to generality and the head (i.e., classifier) related to specialty, as in previous studies (Kang et al., 2019;Yu et al., 2020;Yosinski et al., 2014;Devlin et al., 2019) for advanced analysis. Kang et al. (2019) and Yu et al. (2020) demonstrated that the head is biased in class-imbalanced environments. Note that popular networks such as MobileNet (Howard et al., 2017) and ResNet (He et al., 2016) have only one linear layer at the end of the model, and the head is defined as this linear layer whereas the body is defined as all of the layers except the head. The body of the model is related to representation learning and the head of the model is related to linear decision boundary learning. We shed light on the cause of the personalization performance degradation by decoupling parameters, pointing out that such a problem stems from training the head. In the figure, the lines represent the decision boundaries defined by the head (i.e., the last linear classifier) of the network; different shapes indicate different classes. It is assumed that each client has two classes. (a) FedAvg updates the entire network during local updates on each client and then the local networks are aggregated entirely. Therefore, the heads of all clients and the head of the server are different. Whereas, (b) FedBABU only updates the body (i.e., all of the layers except the head) during local updates on each client and then the local networks are aggregated body-partially. Therefore, the heads of all clients and the head of the server are the same. Inspired by the above observations, we propose an algorithm to learn a single global model that can be efficiently personalized by simply changing the Federated Averaging (FedAvg) (McMahan et al., 2017) algorithm. FedAvg consists of four stages: client sampling, broadcasting, local update, and aggregation. In the client sampling stage, clients are sampled because the number of clients is so large that not all clients can participate in each round. In the broadcasting stage, the server sends a global model (i.e., an initial random model at the first broadcast stage or an aggregated model afterward) to participating clients. In the local update stage, the broadcast model of each device is trained based on its own data set. In the aggregation stage, locally updated models are sent to the server and are aggregated by averaging. Among the four stages, we focus on the local update stage from both the universality and personalization perspectives. Here, we only update the body of the model in the local update stage, i.e., the head is never updated during federated training. From this, we propose FedBABU, Federated Averaging with Body Aggregation and Body Update. Figure 1 describes the difference during the local update and aggregation stages between FedAvg and FedBABU. Intuitively, the fixed head can be interpreted as the criteria or guideline for each class and our approach is representation learning based on the same fixed criteria across all clients during federated training. This simple change improves the representation power of a single global model and enables the more accurate and rapid personalization of the trained global model than FedAvg. Our contributions are summarized as follows:\n\n\u2022 We investigate the connection between a single global model and fine-tuned personalized models by analyzing the FedAvg algorithm at the client level and show that training the head using shared data on the server negatively impacts personalization. \u2022 We demonstrate that the fixed random head can have comparable performance to the learned head in the centralized setting. From this observation, we suggest that sharing the fixed random head across all clients can be more potent than averaging each learned head in federated settings. \u2022 We propose a novel algorithm, FedBABU, that reduces the update and aggregation parts from the entire model to the body of the model during federated training. We show that FedBABU is efficient, particularly under more significant data heterogeneity. Furthermore, a single global model trained with the FedBABU algorithm can be personalized rapidly (even with one fine-tuning epoch). We observe that FedAvg outperforms most of preexisting personalization FL algorithms and FedBABU outperforms FedAvg in various FL settings. \u2022 We adapt the body update and body aggregation idea to the regularization-based global federated learning algorithm (such as FedProx (Li et al., 2018)). We show that regularization reduces the personalization capabilities and that this problem is mitigated through BABU.\n\n\nRELATED WORKS\n\nFL for a Single Global Model Canonical federated learning, FedAvg proposed by McMahan et al. (2017), aims to learn a single global model that collects the benefits of affluent data without storing the clients' raw data in a central server, thus reducing the communication costs through local updates. However, it is difficult to develop a globally optimal model for non-independent and identically distributed (non-IID) data derived from various clients. To solve this problem, studies have been conducted that make the data distribution of the clients IID-like or add regularization to the distance from the global model during local updates. Zhao et al. (2018) suggested that all clients share a subset of public data, and Duan et al. (2019) augmented data to balance the label distribution of clients. Recently, two studies (Li et al., 2018;Acar et al., 2021) penalized local models that have a large divergence from the global model, adding a regularization term to the local optimization process and allowing the global model to converge more reliably. However, it should be noted that the single global model trained using the above methods is not optimized for each client.\n\nPersonalized FL Personalized federated learning aims to learn personalized local models that are stylized to each client. Although local models can be developed without federation, this method suffers from data limitations. Therefore, to maintain the benefits of the federation and personalized models, many other methods have been applied to FL: clustering, multi-task learning, transfer learning, regularized loss function, and meta-learning. Two studies (Briggs et al., 2020;Mansour et al., 2020) clustered similar clients to match the data distribution within a cluster and learned separate models for each cluster without inter-cluster federation. Similar to clustering, multi-task learning aims to learn models for related clients simultaneously. Note that clustering ties related clients into a single cluster, whereas multi-task learning does not.   learns the entire network sequentially during local updates and only aggregates the body. In the local update stage, each client first learns the head only with the aggregated representation and then learns the body only with its own head within a single epoch. Unlike the above decoupling methods, we propose a FedBABU algorithm that learns only the body with the randomly initialized head during local updates and aggregates only the body. It is thought that fixing the head during the entire federated training provides the same guidelines on learning representations across all clients. Then, personalized local models can be obtained by fine-tuning the head.   Table 1 describes the accuracy of FedAvg on CIFAR100 according to different FL settings (f , \u03c4 , and s) with 100 clients. The initial and personalized accuracies indicate the evaluated performance without fine-tuning and with five fine-tuning epochs for each client, respectively. As previous studies have shown, the more realistic the setting (i.e., a smaller f , larger \u03c4 , and smaller s), the lower the initial accuracy. Moreover, the tendency of the personalized models to converge higher than the global model observed in Jiang et al. (2019) is the same. More interestingly, it is shown that the gap between the initial accuracy and the personalized accuracy increases significantly as the data become more heterogeneous. It is thought that a small s makes local tasks easier because the label distribution owned by each client is limited and the number of samples per class increases. In addition, we conduct an intriguing experiment in which the initial accuracy increases but the personalized accuracy decreases, maintaining the FL training procedures. In Jiang et al. (2019), the authors showed that centralized trained models are more difficult to personalize. Similarly, we design an experiment in which the federated trained models are more difficult to personalize. We assume that the server has a small portion of p of the non-private client data of the clients 4 such that the non-private data can be used in the server to mitigate the degradation derived from data heterogeneity.\n\nWe update the global model using this shared non-private data after every aggregation with only one epoch. Table 2 describes the result of this experiment. We update the entire network (F in Table 2) on the server. As p increases, the initial accuracy increases, as expected. However, the personalized accuracy decreases under significant data heterogeneity (s=10). This result implies that boosting a single global model can hurt personalization, which may be considered more important than the initial performance. Therefore, we agree that the development of an excellent global model should consider the ability to be adequately fine-tuned or personalized.\n\nTo investigate the cause of personalization performance degradation, we hypothesize that unnecessary and disturbing information for personalization (i.e., the information of similar classes that a certain client does not have) is injected into the head, when a global model is trained on the server. 5 To capture this, we only update the body (B in Table 2) on the server by zeroing the learning rate corresponding to the head. By narrowing the update parts, the personalization degradation problem can be remedied significantly without affecting the initial accuracy. From this observation, we argue that training the head using shared data negatively impacts the personalization. 6\n\n\nFEDBABU: FEDERATED AVERAGING WITH BODY AGGREGATION AND BODY UPDATE\n\nWe propose a novel algorithm that learns a better single global model to be personalized efficiently. Inspired by prior studies on long-tailed recognition (Yu et al., 2020;Kang et al., 2019), fine-tuning (Devlin et al., 2019), and self-supervised learning (He et al., 2020), as well as our data-sharing experiment, we decouple the entire network into the body and the head. The body is trained for generalization and the head is then trained for specialization. We apply this idea to federated learning by never training the head in the federated training phase (i.e., developing a single global model) and by fine-tuning the head for personalization (in the evaluation process).  Before proposing our algorithm, we empirically demonstrate that a model with the initialized and fixed head (body in Figure 2) has comparable performance to a model that jointly learns the body and the head (full in Figure 2). Figure 2 depicts the test accuracy curve of MobileNet on CIFAR100 for various training scenarios in the centralized setting, where total epochs are 160 and learning rate starts with 0.1 and is decayed by a factor of 0.1 at 80 and 120 epochs. The blue line represents the accuracy when all layers are trained, the red line represents the accuracy when only the body of the model is trained, and the green line represents the accuracy when only the head of the model is trained. The fully trained model and the fixed head have almost the same performance, whereas the fixed body performs poorly. Thus, we claim that the randomly initialized fixed head is acceptable, whereas the randomly initialized fixed body is unacceptable; initialized head is thought to serve as guidelines. This characteristic is derived from the orthogonality on the head, as is explained in Appendix B. These results are the reasons we can update only the body during local training in FL settings.\n\n\nFROZEN HEAD IN THE CENTRALIZED SETTING\n\n\nFEDBABU ALGORITHM\n\nAlgorithm 1 Training procedure of FedBABU.\n\n1: initialize \u03b8 0 G = {\u03b8 0 G,ext , \u03b8 0 G,cls } 2: for each round k = 1, \u00b7 \u00b7 \u00b7 , K do 3:\n\nm \u2190 max(\u230aN f \u230b, 1)\n\n\n4:\n\nC k \u2190 random subset of m clients 5:\n\nfor each client C k i \u2208 C k in parallel do 6:\n\u03b8 k i (0) \u2190 \u03b8 k\u22121 G = {\u03b8 k\u22121 G,ext , \u03b8 0 G,cls } 7: \u03b8 k i,ext (\u03c4 I k i ) \u2190 ClientBodyUpdate(\u03b8 k i (0), \u03c4 ) 8: end for 9: \u03b8 k G,ext \u2190 m i=1 n C k i n \u03b8 k i,ext (\u03c4 I k i ), n = m i=1 n C k i 10: end for 11: return \u03b8 K G = {\u03b8 K G,ext , \u03b8 0 G,cls } 12: function CLIENTBODYUPDATE(\u03b8 k i , \u03c4 )\n13:\nI k i \u2190 \u2308 n C k i B \u2309\n\n14:\n\nfor each local epoch 1, \u00b7 \u00b7 \u00b7 , \u03c4 do 15:\n\nfor each iteration 1, \u00b7 \u00b7 \u00b7 , I k i do 16: . Only the body is trained, whereas the head is never trained during federated training; therefore, there is no need to aggregate the head. Formally, the model parameters \u03b8 are decoupled into the body (extractor) parameters \u03b8 ext and the head (classifier) parameters \u03b8 cls . Note that \u03b8 cls on any client is fixed with the head parameters of a randomly initialized global parameters \u03b8 0 G until a single global model converges. This is implemented by zeroing the learning rate that corresponds to the head. Intuitively, it is believed that the same fixed head on all clients serves as the same criteria on learning representations across all clients despite the passage of training time.\n\u03b8 k i,ext \u2190 SGD(\u03b8 k i,ext , \u03b8 0 G,\nAlgorithm 1 describes the training procedure of FedBABU. All notations are explained in the FL training procedure paragraph in Section 3. Lines 3-4, Line 6, Line 7, and Line 9 correspond to the client sampling, broadcasting, local update, and aggregation stage, respectively. In the ClientBodyUpdate function, the local body parameter \u03b8 k i,ext is always updated based on the same head \u03b8 0 G,cls (Line 16). Then, only the global body parameter \u03b8 k G,ext is aggregated (Line 9). Therefore, the final global parameter \u03b8 K G (Line 11) and the initialized global parameter \u03b8 0 G (Line 1) have the same head parameter \u03b8 0 G,cls .\n\nIn this section, we first investigate the representation power after federated training (Section 5.2.1). Next, we demonstrate the ability related to personalization (Section 5.2.2 to Section 5.2.4). We further show our algorithm's applicability (Section 5.2.5). Higher initial accuracy is often required because some clients may not have data for personalization in practice. In addition, the representation power of the initial models is related to the performance of downstream or personal tasks. To capture the representation power, the \"without (w/o) head\" accuracy is calculated by replacing the trained head with the nearest template following previous studies (Kang et al., 2019;Snell et al., 2017;Raghu et al., 2019;Oh et al., 2021):\n\n\nREPRESENTATION POWER OF GLOBAL MODELS TRAINED BY FEDAVG AND FEDBABU\n\n(1) Using the trained global model and training data set on each client, the representations of each class are averaged into the template of each class. Then, (2) the cosine similarity between the test samples and the templates are measured and the test samples are classified into the nearest template's class. Templates are created for each client because raw data cannot be moved. Table 3 describes the initial accuracy of FedAvg and FedBABU according to the existence of the head. Because all clients share the same criteria and learn representations based on that criteria during federated training in FedBABU, improved representation power is expected. 7 When evaluated without the head, FedBABU has improved performance compared to FedAvg in all cases, particularly under large data heterogeneity. Specifically, when s = 100 (i.e., each client has most of the classes of CIFAR100), the performance gap depends on whether the head exists in FedAvg, but not in FedBABU. This means that the features through FedBABU are well-represented to the extent that there is nothing to be supplemented through the head. When s is small, the performance improves when there is no head. Because templates are constructed based on the training data set for each client, it is not possible to classify test samples into classes that the client does not have. In other words, restrictions on the output distribution of each client lead to tremendous performance gains, even without training the global model. Therefore, it is expected that fine-tuning the global models with a strong representation can boost personalization. Appendix O provides the results of FedAvg with different learning rates between the head and the body, which shows the freezing the head completely is critical. Appendix P further provides class-wise analysis during federated training, which shows that freezing the head makes less damage to out-of-class during local updates.\n\n\nPERSONALIZATION OF FEDAVG AND FEDBABU\n\nTo investigate the dominant factor for personalization of FedBABU, we compare the performance according to the fine-tuned part. Table 4a describes the results of this experiment. Global models are fine-tuned with five epochs based on the training data set of each client. It is shown that fine-tuning including the head (i.e., Head or Full in Table 4a) is better for personalization than body-partially fine-tuning (i.e., Body in Table 4a). It is noted that the computational costs can be reduced by fine-tuning only the head in the case of FedBABU without performance degradation.\n\nHowever, in the case of FedAvg, a performance gap appears. Table 4b describes the personalized accuracy of FedAvg according to the fine-tuned parts. It is shown that fine-tuning the entire network (Full in Table 4b) is better for personalization than partial fine-tuning (Body or Head in Table 4b). Therefore, for FedAvg, it is recommended to personalize global models by fine-tuning the entire network for better performance. For consistency of the evaluation, for both FedAvg and FedBABU, we fine-tuned the entire model in this paper. \n\n\nPERSONALIZATION PERFORMANCE COMPARISON\n\n\nFL settings\n\nPersonalized accuracy\ns f \u03c4 FedBABU (Ours) FedAvg(2017)\nFedPer (2019) LG-FedAvg (2020) FedRep (2021) Per-FedAvg We compare FedBABU with existing methods from the perspective of personalization. Appendix A presents details of the evaluation procedure and implementations of each algorithm. Table 5 describes the personalized accuracy of various algorithms. Notably, FedAvg is a remarkably strong baseline, overwhelming other recent personalized FL algorithms on CIFAR100 in most cases. 8 These results are similar to recent trends in the field of meta-learning, where fine-tuning based on well-trained representations overwhelms advanced few-shot classification algorithms for heterogeneous tasks (Chen et al., 2019a;2020a;Dhillon et al., 2019;Tian et al., 2020). FedBABU (ours) further outshines FedAvg; it is believed that enhancing representations by freezing the head improves the performance.\n\nIn detail, when \u03c4 =1, the performance gap between FedBABU and FedRep demonstrates the importance of fixing the head across clients. Note that when \u03c4 =1, if there is no training process on the head in FedRep, FedRep is reduced to FedBABU. Moreover, we attribute the performance degradation of FedRep to the low epochs of training on the body. In addition, the efficiency of personalized FL algorithms increases when all clients participate; therefore, FedPer (Arivazhagan et al., 2019) assumes the activation of all clients during every communication round. The Per-FedAvg algorithm might suffer from dividing the test set into a support set and query set when there are many classes and from small fine-tuning iterations. In summary, FedAvg are comparable and better than even recent personalized FL algorithms such as Per-FedAvg and Ditto, and FedBABU (ours) achieves the higher performance than FedAvg and other decoupling algorithms. We investigate the personalization speed of FedAvg and FedBABU by controlling the fine-tuning epochs \u03c4 f in the evaluation process. Table 6 describes the initial (when \u03c4 f is 0) and personalized (otherwise) accuracy of FedAvg and FedBABU. Here, one epoch is equal to 10 updates in our case because each client has 500 training samples and the batch size is 50. It has been shown that FedAvg requires sufficient epochs for fine-tuning, as reported in Jiang et al. (2019). Notably, FedBABU achieves better accuracy with a small number of epochs, which means that FedBABU can personalize accurately and rapidly, especially when fine-tuning is either costly or restricted. This characteristic is explained further based on cosine similarity in Appendix K. FedProx (Li et al., 2018) regularizes the distance between a global model and local models to prevent local models from deviating. The degree of regularization is controlled by \u00b5. Note that when \u00b5 is 0.0, FedProx is reduced to FedAvg.  \n\n\nPERSONALIZATION SPEED OF FEDAVG AND FEDBABU\n\n\nBODY AGGREGATION AND BODY UPDATE ON THE FEDPROX\n\n\nA.2 DATASETS\n\nWe used two public datasets, the CIFAR (Krizhevsky et al., 2009) 11 and EMNIST (Cohen et al., 2017) 12 , for performance evaluation. The composition of each dataset is summarized in Table 8. We applied linear transformations such as horizontal flipping and random cropping. LG-FedAvg, FedAvg models corresponding to the FL settings in Table 3 are used as an initialization. It is then trained about a quarter of FedAvg training with a learning rate of 0.001. For FedRep, the number of local epochs for head and body updates were set to \u03c4 and 1, respectively. For Per-FedAvg, we used the first-order (FO) version, and the \u03b2 value was set as the learning rate value corresponding to the communication round. For Ditto, the \u03bb used to control the regularization term was set to 0.75.\n\n\nA.4 EVALUATION OF FL ALGORITHMS\n\nIn FL algorithms, because the parts that need to be shared over all clients after federated training may not be shared due to client fraction, we evaluate these algorithms as follows:\n\nAlgorithm 2 Evaluation procedure of FedAvg, FedBABU, FedPer, LG-FedAvg, and FedRep. \u03b8i(\u03c4 f Ii) \u2190 Fine-tune(alg, \u03b8i(0), \u03c4 f ; D tr i )\n\n\n8:\n\nAccper \u2190 Eval(\u03b8i(\u03c4 f Ii); D ts i )\n\n\n9:\n\nperlist.append(Accper) 10: end for 11: return initlist, perlist 12: function FINE-TUNE(alg, \u03b8i, \u03c4 f ) 13:\nIi \u2190 \u2308 n C i B \u2309\n\n14:\n\nfor each fine-tune epoch 1, \u00b7 \u00b7 \u00b7 , \u03c4 f do\n\n\n15:\n\nfor each iteration 1, \u00b7 \u00b7 \u00b7 , Ii do 16: where \u03b8 K G,s indicates the shared parts after federated training (i.e., full in the cases of FedAvg and FedBABU, body in the cases of FedPer and FedRep, and head in the case of LG-FedAvg). All notations are explained in the evaluation paragraph in Section 3. Because communication round k is not related to evaluation, it is emitted. For FedAvg and FedBABU, initial accuracy is also calculated. Although all clients have different personalized models because some parts are not shared after broadcasting except for FedAvg and FedBABU, it is not completely personalized because there is a common part \u03b8 K G,s . Therefore, we fine-tune all algorithms. FedRep trains models sequentially (Line 17 and 21), while others jointly (Line 16). Finally, the mean and standard deviation of initlist and perlist are reported as initial accuracy and personalized accuracy in our paper. We hypothesize that the orthogonal initialization (Saxe et al., 2013) on the head, through which the row vectors of the classifier parameters \u03b8 cls are orthogonal, is the best for the body-only update rule. At each update, the output of the extractor is pulled toward its label's row of \u03b8 cls and pushed apart from the other rows of \u03b8 cls through backpropagation. Therefore, orthogonal weight initialization can separate the outputs of the extractor well based on their class labels. Note that distribution-based random initializations, such as Xaiver (  are representative distribution-based random initialization methods, controlling the variance of parameters using network size to avoid the exploding/vanishing gradient problem. Two initialization methods use uniform and normal distribution. In detail, each element follows U(\u2212a, a) or N (0, \u03c3 2 ) independently, where a and \u03c3 are defined by non-linearity (such as ReLU and Tanh) and layer input-output size. Because we used the ReLU non-linear function in our implementations, we calculate a and \u03c3 by multiplying \u221a 2, for scaling of ReLU non-linearity.\n\n\nB ORTHOGONALITY OF RANDOM INITIALIZATION\n\nBecause we focus on initialization on the head, we specify \u03b8 cls \u2208 R C\u00d7d , where C is the number of classes (i.e., layer output size) and d is the dimension of the last representation (i.e., layer input size). Namely, each element random variable {w i,j } i\u2208[1,C],j\u2208 [1,d] follows U(\u2212a, a) or N (0, \u03c3 2 ) independently.\n\nWhen w i,j \u223c U(\u2212a, a), in Xavier initialization, a is defined as \u221a 2 \u00d7 6 d+C . Similarly, in He initialization, a is defined as \u221a 2 \u00d7 3 d or \u221a 2 \u00d7 3 C depending on which to be preserved, forward pass or backward pass. When w i,j \u223c N (0, \u03c3 2 ), in Xavier initialization, \u03c3 is defined as \u221a 2 \u00d7 2 d+C . Similarly, in He initialization, \u03c3 is defined as \u221a 2 \u00d7 1 d or \u221a 2 \u00d7 1 C depending on which to be preserved, forward pass or backward pass. However, It is noted that a in the uniform distribution and \u03c3 in the normal distribution do not affect proof on orthogonality, therefore we keep the form of U(\u2212a, a) and N (0, \u03c3 2 ) for simplicity.\n\nWe first consider 2d independent random variables {w i,j } i\u2208{p,q},j\u2208 [1,d] for any p \u0338 = q \u2208 [1, C]. Then, we can define d independent random variables {w p,j w q,j } j\u2208 [1,d] from the above 2d independent random variables. Then, for all j \u2208 a) or N (0, \u03c3 2 ) independently. Let S d = 1 d d j=1 w p,j w q,j . Then, by the Weak Law of Large Numbers, lim d\u2192\u221e P (|S d \u2212 E[S d ]| > \u03f5) = lim d\u2192\u221e P (|S d | > \u03f5) = 0 for all \u03f5 > 0 because {w p,j w q,j } j\u2208 [1,d] are independent random variables and there is V such that V[w p,j w q,j ] < V for all j.\n[1, d], E[w p,j w q,j ] = E[w p,j ]E[w q,j ] = 0 and V[w p,j w q,j ] = V[w p,j ]E[w q,j ] 2 +V[w p,j ]V[w q,j ]+E[w p,j ] 2 V[w q,j ] = V[w p,j ]V[w q,j ] because each element in \u03b8 cls follows U(\u2212a,\n\nC PERSONALIZATION OF THE CENTRALIZED MODEL\n\nIn (Jiang et al., 2019), they showed that localizing centralized initial models is harder. Similarly, we investigate the personalization of the centralized model under different heterogeneity. First, we train two models with 10 epochs, one is entirely updated (F in Table 9), and another is body-partially updated (B in Table 9) using all training data set. Then, the trained model is broadcast to each client and then evaluated. Table 9 describes the results of this experiment. It is demonstrated that if models are updated body-partially, then personalization ability does not hurt even under centralized training. \n\n\nD EFFECT OF MOMENTUM DURING LOCAL UPDATES ON PERFORMANCE\n\nWe investigate the effect of momentum during local updates on the performance of FedAvg and FedBABU. Table 10 describes the initial and personalized accuracy according to the momentum during local updates. The momentum for personalization fine-tuning is the same as the momentum in federated training. In both cases of FedAvg and FedBABU, appropriate momentum improves the performance, especially personalized accuracy. However, when the extreme momentum (m=0.99) is used, FedAvg completely loses the ability to train models, while FedBABU has robust performance. \n\n\nE EFFECT OF MASSIVENESS ON PERFORMANCE\n\nWe increase the number of clients from 100 to 500, and then each client has 100 training data and 20 test data. Table 11 describes the initial and personalized accuracy of FedAvg and FedBABU on CIFAR100 under various FL settings with 500 clients. The performance of FedAvg is slightly better than that of FedBABU in many cases; however, FedBABU overwhelms FedBABU in the case of extreme FL setting (s=10, f =0.1, and \u03c4 =10). More interestingly, if each client has a few data samples, then local epochs \u03c4 should be more than 1. It is thought that massiveness makes data size insufficient on each client (when the total number of data is fixed) and brings out other problems. When 4convNet is used on CIFAR10, a similar tendency appears. Comparing Figure 2 and Figure 4, the importance of learning representation is emphasized more when tasks are difficult. In addition, Figure 5 shows the necessity of orthogonality when the body is only trained and approximation of random initialization to the orthogonal initialization using 4convNet on CIFAR10.   \n\n\nF.2 EXPERIMENTAL RESULTS\n\nWe also evaluate algorithms on CIFAR10 using 4convNet. Table 12 and Table 13 are the experiment  results corresponding to the Table 3 and Table 5 in the main paper, respectively. The tendency of performance comparison and the superiority of our algorithm appears similarly when using 4convNet on CIFAR10.  \n\n\nF.3 ABLATION STUDY ACCORDING TO THE LOCAL UPDATE PARTS\n\nWe decouple the entire network in more detail during local updates to investigate whether the body should be totally trained. For this ablation study, we used 4convNet on CIFAR10. Table 14 and  Table 15 show the initial and personalized accuracy according to the local update parts, respectively. For consistency, all cases are personalized by fine-tuning the entire network in this experiment. In all cases, FedBABU has the best performance, which implies that the body must be totally trained at least when using 4convNet. It is believed that the same (body-totally) or similar (body except for a few top layers) trends appear when large networks are used.  We experiment with ResNet18 and ResNet50. In the centralized setting, test accuracy curves according to the update part ( Figure 6 and Figure 8) and according to the initialization method (Figure 7 and Figure 9) have the same trend as we have seen before. G.2 EXPERIMENTAL RESULTS Table 16 and Table 17 are the results of ResNet18 and ResNet50, respectively. It is shown that the performance gap between FedAvg and FedBABU increases when ResNet is used rather than when MobileNet (in the main paper) is used. Furthermore, it is observed that as the complexity of the model increases (ResNet18 \u2192 ResNet50), the performance of FedAvg decreases, while that of FedBABU does not.  We evaluate ours and existing algorithms under unbalanced and non-IID settings derived from the Dirichlet distribution. Figure 10 and 11 describe the distributions of train and test data points and non-IID depending on \u03b2, which is the hyperparameter of the Dirichlet distribution. Table 18 describes the results on Dirichlet distribution-based non-IID CIFAR100 in the realistic FL setting (f =0.1 and \u03c4 =10). The lower \u03b2 implies the larger heterogeneity.   Table 19 describes the results on EMNIST using a network that consists of three convolution layers and a linear classifier. We set the number of users to 1488, and the number of data points per user was approximately 450. We fixed the fraction ratio at 0.1 and local epochs at 10 (which is the realistic setting used in our paper). The results demonstrate that FedBABU also has the best performance on EMNIST. \n\n\nJ RESULTS ON IN-DISTRIBUTION (ID) AND OUT-OF-DISTRIBUTION (OOD) CLASSES\n\nTo investigate the accuracy of in-distribution (ID) and out-of-distribution (OOD) classes, we scatter IID-like test set (s=100) to all clients. Namely, some classes are in test data set (s=100), but not in training data set (s=50 or s=10). Table 20 describes the results of FedAvg and FedBABU on this experiment. Here, in-distribution accuracy is the test accuracy of the classes present in the training data set; otherwise, out-of-distribution accuracy. Before personalization (Before in tables), FedBABU beats FedAvg from both ID and OOD. After personalization (After in tables), FedBABU becomes fit strongly for the ID classes than OOD classes. The OOD accuracy of FedBABU decreases more than that of FedAvg, but the accuracy of FedAvg simiarly drops after personalization. These results indicate that no personalization is the best strategy for both FedBABU and FedAvg. Therefore, because the OOD accuracy of FedBABU is higher than that of FedAvg before personalization, it is concluded that FedBABU has better performance than FedAvg when the optimal strategy for OOD is used.  We investigate the cosine similarity between different clients during 20 fine-tuning epochs, described in Figure 12. During fine-tuning for personalization evaluation, the entire network is updated in the cases of both FedAvg and FedBABU. From the results, it is found that classifier (black line in Figure 12) is closely related to personalization. In other words, all clients have similar extractors during fine-tuning (i.e., large cosine similarity), while make different classifiers based on their own data. Furthermore, it is observed that the cosine similarity on the head between different clients decreases more rapidly in the case of FedBABU than FedAvg. It is believed that this is because the classifier of FedAvg starts personalization from the federated convergent classifier, while that of FedBABU starts personalization from the initialized orthogonal classifier. This characteristic explains rapid personalization, introduced in Section 5.2.4. Let us repeat the experiment in Section 4 again, where the server has a small portion p of the nonprivacy data of the clients. Table 21 describes the initial and personalized accuracy of FedBABU when the global model is body-partially updated on the server using available data (such as experiments (B) in Table 2). FedBABU with body updates on the server improves personalization compared to FedAvg with body updates on the server, maintaining the initial accuracy (refer to (B) in Table 2). This result demonstrates that training the head negatively affects personalization even when trained locally. In addition, the performance of FedBABU improves as p increases. This implies that there is room for enhancing the representation beyond that of FedBABU.\n\n\nL FEDBABU WITH BODY UPDATE ON THE SERVER\n\n\nM BODY AGGREGATION AND BODY UPDATE ON THE FEDPROX\n\nEven when all clients are active every communication round (i.e., f =1.0), the personalized performance of FedProx+BABU beats not only that of FedAvg but also that of FedBABU. FedProx are a bit more local-only-based. This is because FedAvg+Fine-tuning is the same as FedAvg before personalization, whereas personalized FL algorithms develop different clients' models from scratch like local-only. Therefore, we believe that FedAvg+Fine-tuning earns more benefits (particularly for representation) from federation than personalized FL algorithms from this difference (FedAvgbased v.s. local-only-based). An additional difference between FedAvg+Fine-tuning and regularized personalized FL algorithms is that FedAvg+Fine-tuning optimizes clients separately using their own data set based on the well-federated extractor, whereas regularized personalized FL algorithms optimize clients jointly from scratch. In other words, FedAvg+Fine-tuning can easily optimize all clients' models based on their own data sets. However, it is too difficult to optimize all clients' models simultaneously. Therefore, FedAvg+Fine-tuning can achieve better personalized performance than regularized personalized FL algorithms.\n\nFurthermore, very recent work (Cheng et al., 2021) has addressed the effectiveness of FedAvg+Finetuning theoretically. In Cheng et al. (2021), they compared local-only (zero collaboration), FedAvg (zero personalization) (McMahan et al., 2017), FedAvg+Fine-tuning, Per-FedAvg (Fallah et al., 2020, and pFedMe (T Dinh et al., 2020) in the high-dimensional asymptotic limit by analyzing bias-variance. They demonstrated that the asymptotic test loss of FedAvg+Fine-tuning (FTFA in Cheng et al. (2021)) matches that of Per-FedAvg and that the asymptotic test loss of FedAvg+Fine-tuning with a ridge regularizer (RTFA in Cheng et al. (2021)) matches that of pFedMe on their stylized linear regression model.\n\nAlthough Cheng et al. (2021) thoroughly addressed this problem on their stylized linear regression model, there are few results on real data sets except for test accuracies. It is believed that \"why FedAvg+Fine-tuning is effective\" itself needs to be studied more using real data sets, as \"why MAML (  What we want to emphasize is the importance of making all clients have the same class boundary by freezing the head. We argue that this shared classifier enhances the representation power of the federated model, which is an important factor for personalization. From this perspective, if the head moves even a little bit client by client, the different class boundaries to train feature extractor are used during local updates. Therefore, the federated model's feature extractor might hurt.\n\n\nO FEDAVG WITH DIFFERENT LEARNING RATES\n\nTo verify this hypothesis, we experiment using different learning rates depending on the parts. The above table describes the federated model's accuracy without a classifier (i.e., w/o classifier accuracy in Table 3) to investigate representation power of models. We set the body's initial learning rate as 0.1, the fraction ratio as 0.1, and local epochs as 10. \u03b1 h and \u03b1 b are the head's learning rate and the body's learning rate, respectively.\n\nFrom this result, we believe that the federated model's representation power increases as the head's learning rate decreases, i.e., as a learning signal for the body is larger. In this trend, we highlight that the best performance is due to the same classifier on all clients.\n\n\nP CLASS-WISE ANALYSIS DURING FEDERATED TRAINING\n\nTo explain why FedBABU has better performance than FedAvg after federated training (i.e., before fine-tuning), we analyze the federated training procedure of FedAvg and FedBABU class-wisely under the realistic federated settings (f =0.1 and \u03c4 =10). For this analysis, we assume that each client has 500 training data and 10,000 test data (i.e., the whole test data) of CIFAR-100. Test data set is divided into two groups for each client; in-class test data and out-of-class test data. In-class test  data indicates test data whose class is in the classes in training data, whereas out-of-class test data indicates test data whose class is not in the classes in training data. For instance, if client A has classes 0-9 as training data, then in-class test data indicates test data whose class is one of 0-9 and out-of-class test data indicates test data whose class is one of 10-99. Figure 13 describes the in-class ((a)- (c)) and out-of-class ((d)-(f)) test accuracy curves according to the heterogeneity. Client sampling, broadcasting, local updates, and aggregation stages are repeated during federated training. The accuracies per epoch are averaged right after broadcasting the aggregated model to the selected clients and during local updates on them. It is observed that in-class accuracy of the aggregated model is significantly low and increases dramatically during local updates, whereas out-of-class accuracy of the aggregated model is significantly high and decreases dramatically during local updates, as heterogeneity is larger. In all cases, FedBABU has higher out-of-class accuracy than FedAvg after local updates. It implies that freezing the head makes less damage to out-of-class during local updates and can lead to better aggregation. Furthermore, when heterogeneity is large (s=10), FedBABU has higher in-class accuracy than FedAvg after local updates.\n\n\nQ FEDBABU WITH THE NON-ORTHOGONAL CLASSIFIERS\n\nWe demonstrate that an orthogonal initialization on the head is required for desirable performance in the centralized setting (Appendix B). To investigate whether this characteristic is still required under FL settings, we compare FedBABU with the orthogonal head (which is proposed) to FedBABU without the orthogonal head. FedBABU without the orthogonal head is designed in the same way as 'similar' in Appendix B. Table 24 describes the results. As we expected, if the head does not consist of the orthogonal row vectors, FedBABU cannot achieve desirable performance.  Figure 14 describes examples of data distribution of clients when s is 10 (left) or 2 (right). Consider that there are 10 clients and CIFAR10 dataset is scattered to each client. When s is 10, total shards 100 because 10 users and 10 shards per user. Then, the CIFAR10 dataset is divided into 100 shards, hence each shard consits of 500 samples with the same class. Here, each client has 10 shards randomly. In this situation, each client can have up to 10 classes. On the contrary, when s is 2, total shards are 20 because 10 users and 2 shards per user. Then, the CIFAR10 dataset is divided into 20 shards, hence each shard consits of 2500 samples with the same class. Here, each client has 2 shards randomly. In this situation, each client can have 2 classes at most. Namely, as s gets smaller, the number of classes that each clients has is limited (i.e., label distribution owned by each client is limited) and the number of samples per class increases.  \n\n\nS PERFORMANCE WITH LARGER TOTAL EPOCHS\n\nWe fixed the total number of epochs to 320, i.e., K (total communication rounds) \u00d7 \u03c4 (local epochs) = 320. This is because the performance gap between algorithms appeared and  used total number of epochs to 500, where K=100 and \u03c4 =5 on CIFAR100. However, because algorithms may not converge, we evaluate them with larger total epochs of 640. Furthermore, we add the results when s is 20 and 5. Table 25 describes the results with the total epochs of 640 when f is 0.1. The results demonstrate that FedBABU is the best under large heterogeneity (i.e., s=20, 10, and 5), which is the situation federated learning researchers attempt to solve.  , 2019). For CIFAR100, the sparse classes are converted into coarse classes (e.g., bicycle \u2192 vehicle). However, the difference cannot be captured by the naked eye. Therefore, we further provide t-SNE visualization using only sub-classes of 'vehicle' super-class, described as Figure 16. For CIFAR10, airplane, automobile, ship, and truck classes are used, and for CIFAR100, bicycle, bus, motorcycle, pickup truck, train, lawn-mower, rocket, streetcar, tank, and tractor are used.     \n\nFigure 1 :\n1Difference in the local update and aggregation stages between FedAvg and FedBABU.\n\nFigure 2 :\n2Test accuracy curves according to the update part in the centralized setting.\n\n, e p s =1e \u2212 05 , momentum = 0 . 1 , a f f i n e = True , t r a c k _ r u n n i n g _ s t a t s = F a l s( k e r n e l _ s i z e =2 , s t r i d e =2 , p a d d i n g =0 , d i l a t i o n =1 , c e i l _ m o d e = F a l s, k e r n e l _ s i z e = ( 3 , 3 ) , s t r i d e = ( 1 , 1 ) , p a d d i n g = ( 1 , 1 ) ) ( 1 ) : BatchNorm2d ( 6 4 , e p s =1e \u2212 05 , momentum = 0 . 1 , a f f i n e = True , t r a c k _ r u n n i n g _ s t a t s = F a l s\n053105i n e a r ) : L i n e a r ( i n _ f e a t u r e s =256 , o u t _ f e a t u r e s =10 , b i a s = T r u e ) )\n\nFigure 3 :\n3Test accuracy curves according to the initialization method when the body is only trained in the centralized setting. The values in parentheses indicate the average of cosine similarities between row vectors.\n\n\nGlorot & Bengio, 2010)  andHe (He  et al., 2015), have almost orthogonal rows because the orthogonality of two random vectors is observed in high dimensions with high probability(Lezama  et al., 2018).\n\nFigure 3\n3shows test accuracy curves for many different initialization methods when only the body is trained in the centralized setting. 13 When the row vectors are initialized similarly, a convergence gap appears (green and red lines inFigure 3), as expected. More experiments in the centralized setting are reported in subsection F.1 and Appendix G. Based on these empirical and theoretical results, we used He (uniform) initialization, the default setting in PyTorch (Paszke et al., 2019), in our paper.This characteristic can be proved. Xavier(Glorot & Bengio, 2010)  andHe (He et al., 2015)  \n\nFigure 4 :\n4Test accuracy curves of 4convNet on CIFAR10 according to the update part in the centralized setting.\n\nFigure 5 :\n5Test accuracy curves of 4convNet on CIFAR10 according to the initialization method when the body is only trained.\n\nFigure 6 :Figure 7 :Figure 8 :Figure 9 :\n6789Test accuracy curves of ResNet18 on CIFAR100 according to the update part in the centralized setting. Test accuracy curves of ResNet18 on CIFAR100 according to the initialization method when the body is only trained. Test accuracy curves of ResNet50 on CIFAR100 according to the update part in the centralized setting. Test accuracy curves of ResNet50 on CIFAR100 according to the initialization method when the body is only trained.\n\nFigure 10 :\n10Data distribution (\u03b2 = 1.0).Figure 11: Data distribution (\u03b2 = 0.5).\n\nFigure 12 :\n12Layer-wise cosine similarity of FedAvg and Fed-BABU trained under the realistic FL setting (f =0.1 and \u03c4 =10) during fine-tuning. The blue-, green-, and red-like lines represent low-, middle-, and high-level convolution layers, respectively. The black line represents the last linear layer (i.e., head).\n\n\nFinn et al., 2017)  is effective\" has been addressed in(Raghu et al., 2019).\n\n\ns=10 (out-of-class).\n\nFigure 13 :\n13In-class and out-of-class test accuracy curves according to the heterogeneity. The models are trained under the realistic federated settings (f =0.1 and \u03c4 =10).\n\nFigure 14 :\n14Examples of data distribution of users according to the shards per user.\n\n( c )\ncFedBABU (CIFAR10, train). (d) FedBABU (CIFAR10, test).\n\n( e )\neFedAvg (CIFAR100, train). (f) FedAvg (CIFAR100, test). (g) FedBABU (CIFAR100, train). (h) FedBABU (CIFAR100, test).\n\nFigure 15 :\n15t-SNE visualizations of representations learned by FedAvg and FedBABU. For CIFAR10 and CIFAR100, s is set to 2 and 10, respectively. The models are trained under the realistic federated settings (f =0.1 and \u03c4 =10).\n\nFigure 16 :\n16t-SNE visualizations of representations learned by FedAvg and FedBABU using only sub-classes of 'vehicle' super-class. For CIFAR10 and CIFAR100, s is set to 2 and 10, respectively. The models are trained under the realistic federated settings (f =0.1 and \u03c4 =10).\n\n\nThe generalization of each model can be improved by sharing representations between related clients.Smith et al. (2017)  andDinh et al. (2021) showed that multi-task learning is an appropriate learning scheme for personalized FL. Transfer learning is also a recommended learning scheme because it aims to deliver knowledge among clients. In addition,Yang et al. (2020)  andChen et al. (2020b) utilized transfer learning to enhance local models by transferring knowledge between related clients. T Dinh et al. (2020) and Li et al. (2021) added a regularizer toward the global or average personalized model to prevent clients' models from simply overfitting their own dataset. Unlike the aforementioned methods that developed local models during training, Chen et al. (2018) and Fallah et al. (2020) attempted to develop a good initialized shared global model using bi-level optimization through a Model-Agnostic Meta-Learning (MAML) approach (Finn et al., 2017). A well-initialized model can be personalized through updates on each client (such as inner updates in MAML). Jiang et al. (2019) argued that the FedAvg algorithm could be interpreted as a meta-learning algorithm, and a personalized local model with high accuracy can be obtained through fine-tuning from a global model learned using FedAvg. In addition, various technologies and algorithms for personalized FL have been presented and will be helpful to read; see Tan et al. (2021) and Kulkarni et al. (2020) for more details. Decoupling the Body and the Head for Personalized FL The training scheme that involves decoupling the entire network into the body and the head has been used in various fields, including long-tail recognition (Kang et al., 2019; Yu et al., 2020), noisy label learning (Zhang & Yao, 2020), and meta-learning (Oh et al., 2021; Raghu et al., 2019). 1 For personalized FL, there have been attempts to use this decoupling approach. For a consistent explanation, we describe each algorithm from the perspective of local update and aggregation parts. FedPer (Arivazhagan et al., 2019), similar to FedPav (Zhuang et al., 2020), learns the entire network jointly during local updates and only aggregates the bottom layers. When the bottom layers are matched with the body, the body is shared on all clients and the head is personalized to each client. LG-FedAvg (Liang et al., 2020) learns the entire network jointly during local updates and only aggregates the top layers based on the pre-trained global network via FedAvg. When the top layers are matched with the head, the body is personalized to each client and the head is shared on all clients. FedRep\n\nTable 1 :\n1Initial and personalized accuracy of FedAvg on CIFAR100 under various FL settings with 100 clients. MobileNet is used. The initial and personalized accuracy indicate the evaluated performance without fine-tuning and after five fine-tuning epochs for each client, respectively.FL settings \ns=100 (heterogeneity \u2193) \ns=50 \ns=10 (heterogeneity \u2191) \n\nf \n\u03c4 \nInitial \nPersonalized \nInitial \nPersonalized \nInitial \nPersonalized \n\n1.0 \n\n1 \n46.93\u00b15.47 \n51.93\u00b15.19 \n45.68\u00b15.50 \n57.84\u00b15.08 \n37.27\u00b16.97 \n77.46\u00b15.78 \n4 \n37.44\u00b14.98 \n42.66\u00b15.09 \n36.05\u00b14.04 \n47.17\u00b14.26 \n24.17\u00b15.50 \n70.41\u00b16.83 \n10 \n29.58\u00b14.87 \n34.62\u00b14.97 \n29.57\u00b14.29 \n40.59\u00b15.23 \n17.85\u00b17.38 \n63.51\u00b17.38 \n\n0.1 \n\n1 \n39.07\u00b15.22 \n43.92\u00b15.55 \n38.20\u00b15.73 \n49.55\u00b15.36 \n29.12\u00b17.11 \n71.24\u00b17.82 \n4 \n35.39\u00b14.58 \n39.67\u00b15.21 \n33.49\u00b14.72 \n43.63\u00b14.77 \n21.14\u00b16.86 \n67.14\u00b16.72 \n10 \n28.18\u00b14.83 \n33.13\u00b15.22 \n27.34\u00b14.96 \n38.09\u00b15.17 \n14.40\u00b15.64 \n62.67\u00b16.52 \n\n\n\nTable 2 :\n2Initial and personalized accuracy of FedAvg on CIFAR100 under a realistic FL setting (N =100, f =0.1, \u03c4 =10) according to p, which is the percentage of all client data that the server also has. Here, the entire (or, full) network or body is updated on the server using the available data.p \ns=100 (heterogeneity \u2193) \ns=50 \ns=10 (heterogeneity \u2191) \n\nInitial \nPersonalized \nInitial \nPersonalized \nInitial \nPersonalized \n\n0.00 \n28.18\u00b14.83 \n33.13\u00b15.22 \n27.34\u00b14.96 \n38.09\u00b15.17 \n14.40\u00b15.64 \n62.67\u00b16.52 \n\n0.05 (Full) \n29.23\u00b15.03 \n32.59\u00b15.24 \n27.13\u00b14.34 \n34.34\u00b14.78 \n18.22\u00b15.64 \n54.68\u00b16.77 \n0.05 (Body) 28.50\u00b14.93 \n33.03\u00b15.36 \n27.96\u00b14.86 \n39.10\u00b15.55 \n14.78\u00b15.59 \n60.19\u00b16.46 \n\n0.10 (Full) \n30.59\u00b14.93 \n33.34\u00b15.30 \n29.62\u00b14.27 \n35.50\u00b14.84 \n19.24\u00b15.15 \n49.62\u00b17.48 \n0.10 (Body) 32.90\u00b14.77 \n36.82\u00b14.66 \n32.81\u00b14.97 \n40.80\u00b15.62 \n18.35\u00b16.75 \n60.94\u00b17.30 \n\n\n\nTable 3 :\n340\u00b15.64 50.25\u00b16.27 18.50\u00b17.82 54.93\u00b17.85Initial accuracy of FedAvg and FedBABU \non CIFAR100 under various settings with 100 \nclients. The trained head is replaced with the near-\nest template to calculate \"w/o head\" accuracy and \nMobileNet is used. \n\nFL settings \nFedAvg \nFedBABU \n\ns \nf \n\u03c4 \nw/ head \nw/o head \nw/ head \nw/o head \n\n100 \n\n1.0 \n\n1 \n46.93\u00b15.47 46.23\u00b14.53 48.61\u00b14.75 49.97\u00b14.69 \n4 \n37.44\u00b14.98 33.48\u00b15.09 37.32\u00b14.39 37.20\u00b14.35 \n10 29.58\u00b14.87 25.11\u00b14.60 26.69\u00b14.50 27.70\u00b14.51 \n\n0.1 \n\n1 \n39.07\u00b15.22 36.69\u00b15.82 41.02\u00b14.99 41.19\u00b14.96 \n4 \n35.39\u00b14.58 32.58\u00b14.37 36.77\u00b14.47 36.61\u00b14.64 \n10 28.18\u00b14.83 24.34\u00b14.58 29.38\u00b14.74 29.36\u00b14.46 \n\n50 \n\n1.0 \n\n1 \n45.68\u00b15.50 53.87\u00b15.39 47.19\u00b14.77 55.70\u00b15.48 \n4 \n36.05\u00b14.04 42.65\u00b14.76 37.27\u00b15.25 45.25\u00b15.45 \n10 29.57\u00b14.29 34.13\u00b14.44 28.43\u00b14.72 36.19\u00b14.93 \n\n0.1 \n\n1 \n38.20\u00b15.73 44.57\u00b15.34 41.33\u00b15.10 49.18\u00b15.73 \n4 \n33.49\u00b14.72 40.01\u00b15.49 34.68\u00b14.58 42.43\u00b15.11 \n10 27.34\u00b14.96 33.10\u00b15.08 27.91\u00b15.27 36.49\u00b15.37 \n\n10 \n\n1.0 \n\n1 \n37.27\u00b16.97 67.18\u00b17.27 45.32\u00b18.52 71.23\u00b16.71 \n4 \n24.17\u00b15.50 58.70\u00b16.74 32.91\u00b17.07 64.41\u00b17.44 \n10 17.85\u00b17.38 51.72\u00b17.65 22.15\u00b15.72 55.63\u00b17.24 \n\n0.1 \n\n1 \n29.12\u00b17.11 60.42\u00b17.89 35.05\u00b17.63 65.98\u00b16.43 \n4 \n21.14\u00b16.86 54.91\u00b16.72 25.67\u00b17.31 59.44\u00b16.43 \n10 14.\n\nTable 4 :\n4Personalized accuracy of FedAvg and FedBABU on CIFAR100 according to the fine-tuned part. The fine-tuning epochs is 5 and f is 0.1.(a) FedBABU. \n\nFL settings \nUpdate part for personalization \n\ns \n\u03c4 \nBody \nHead \nFull \n\n100 \n\n1 \n44.26\u00b15.12 49.76\u00b15.03 49.67\u00b14.92 \n4 \n39.61\u00b14.68 44.74\u00b15.02 44.74\u00b15.10 \n10 \n32.45\u00b15.42 36.48\u00b15.04 35.94\u00b15.06 \n\n50 \n\n1 \n48.54\u00b15.23 56.76\u00b15.68 56.69\u00b15.16 \n4 \n41.27\u00b15.04 49.45\u00b15.41 49.55\u00b15.58 \n10 \n35.42\u00b15.60 42.55\u00b15.70 42.63\u00b15.59 \n\n10 \n\n1 \n72.81\u00b17.32 75.97\u00b16.29 76.02\u00b16.29 \n4 \n69.12\u00b16.70 70.74\u00b16.47 71.00\u00b16.63 \n10 \n64.77\u00b17.14 66.28\u00b16.77 66.32\u00b17.02 \n\n(b) FedAvg. \n\nFL settings \nUpdate part for personalization \n\ns \n\u03c4 \nBody \nHead \nFull \n\n100 \n\n1 \n41.00\u00b15.35 43.18\u00b15.34 43.92\u00b15.55 \n4 \n37.43\u00b14.98 38.29\u00b14.96 39.67\u00b15.21 \n10 \n30.62\u00b14.95 31.92\u00b15.04 33.13\u00b15.22 \n\n50 \n\n1 \n43.61\u00b15.54 47.51\u00b15.61 49.55\u00b15.36 \n4 \n37.99\u00b14.68 41.48\u00b14.61 43.63\u00b14.77 \n10 \n32.20\u00b14.92 36.06\u00b15.31 38.09\u00b15.17 \n\n10 \n\n1 \n56.00\u00b18.66 69.70\u00b17.88 71.24\u00b17.82 \n4 \n34.49\u00b17.92 65.32\u00b16.81 67.14\u00b16.72 \n10 \n27.94\u00b16.96 60.24\u00b16.16 62.67\u00b16.52 \n\n7 \n\nTable 5 :\n5Personalized accuracy comparison on CIFAR100 under various settings with 100 clients and MobileNet is used.\n\nTable 6 :\n6Performance according to the fine-tune epochs (FL setting: f =0.1, and \u03c4 =10). 34\u00b14.96 29.17\u00b15.01 32.39\u00b14.77 34.97\u00b15.13 36.78\u00b15.13 38.09\u00b15.17 40.56\u00b15.43 41.20\u00b15.51 40.86\u00b15.13 FedBABU 27.91\u00b15.27 35.20\u00b15.58 40.60\u00b15.47 42.12\u00b15.61 42.74\u00b15.60 42.63\u00b15.59 41.94\u00b15.68 41.19\u00b15.52 40.61\u00b15.28 10 FedAvg 14.40\u00b15.64 27.43\u00b16.46 48.63\u00b17.30 58.08\u00b16.11 61.27\u00b16.15 62.67\u00b16.52 63.91\u00b16.49 64.56\u00b16.45 64.89\u00b16.53 FedBABU 18.50\u00b17.82 63.29\u00b17.55 66.05\u00b16.93 66.10\u00b16.54 66.40\u00b17.24 66.32\u00b17.02 66.07\u00b17.57 66.24\u00b17.67 66.32\u00b17.71s \nAlgorithm \nFine-tune epochs (\u03c4f ) \n\n0 (Initial) \n1 \n2 \n3 \n4 \n5 \n10 \n15 \n20 \n\n50 \nFedAvg \n27.\n\nTable 7 :\n7Initial and personalized accuracy of FedProx and FedProx+BABU with \u00b5=0.01 on CIFAR100 with 100 clients and f =0.1.Algorithm \n\u03c4 \ns=100 (heterogeneity \u2193) \ns=50 \ns=10 (heterogeneity \u2191) \n\nInitial \nPersonalized \nInitial \nPersonalized \nInitial \nPersonalized \n\nFedProx \n\n1 \n46.52\u00b14.56 \n50.95\u00b14.65 \n42.20\u00b14.90 \n51.29\u00b15.20 \n28.16\u00b19.00 \n66.39\u00b17.79 \n4 \n36.54\u00b14.74 \n39.83\u00b14.71 \n33.59\u00b14.80 \n40.17\u00b15.11 \n18.20\u00b17.62 \n41.56\u00b19.34 \n10 28.63\u00b14.40 \n31.90\u00b14.16 \n26.88\u00b14.59 \n32.92\u00b15.00 \n13.62\u00b17.73 \n43.48\u00b19.32 \n\nFedProx \n+BABU \n\n1 \n48.53\u00b15.15 \n57.44\u00b14.72 \n46.25\u00b15.31 \n63.12\u00b15.25 \n33.13\u00b18.11 \n78.86\u00b15.70 \n4 \n37.17\u00b14.41 \n45.26\u00b14.76 \n33.86\u00b15.44 \n50.18\u00b15.14 \n22.94\u00b19.90 \n75.71\u00b15.33 \n10 27.79\u00b13.95 \n35.68\u00b14.34 \n27.48\u00b15.22 \n42.37\u00b16.10 \n15.66\u00b18.29 \n67.15\u00b17.10 \n\n\n\nTable 7\n7describes the initial and personalized accuracy of FedProx and FedProx+BABU with \u00b5=0.01 when f =0.1, and Appendix M reports the results when f =1.0. The global models trained by FedProx reduce the personalization capabilities compared to FedAvg (refer to Table 1 when f =0.1), particularly under realistic FL settings. We adapt the body aggregation and body update idea to the FedProx algorithm, referred to as FedProx+BABU, which performs better than the personalization of FedAvg. Furthermore, FedProx+BABU improves personalized performance compared to FedBABU (refer to Table 5 when f =0.1). This means that the regularization of the body is still meaningful. Our algorithm and various experiments suggest future directions of federated learning: Which parts should be federated and enhanced? Representation! In this study, we focused on how to train a good federated global model for the purpose of personalization. Namely, this problem boils down to how to pre-train a better backbone in a federated learning manner for downstream personal tasks. We first showed that existing methods to improve a global model (e.g., data sharing or regularization) could reduce ability to personalize. To mitigate this personalization performance degradation problem, we decoupled the entire network into the body, related to generality, and the head, related to personalization. Then, we demonstrated that training the head creates this problem. Furthermore, we proposed the FedBABU algorithm, which learns a shared global model that can rapidly adapt to heterogeneous data on each client. FedBABU only updates the body in the local update stage and only aggregates the body in the aggregation stage, thus developing a single global model with a strong representation. This global model can be efficiently personalized by fine-tuning each client's model using its own data set. Extensive experimental results showed that FedBABU overwhelmed various personalized FL algorithms. Our improvement emphasizes the importance of federating and enhancing the representation for FL.Yoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric and subspace. In International Conference on Machine Learning, pp. 2927-2936. PMLR, 2018. Jos\u00e9 Lezama, Qiang Qiu, Pablo Mus\u00e9, and Guillermo Sapiro. Ole: Orthogonal low-rank embeddinga plug and play geometric loss for deep learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8109-8118, 2018. Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018. Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through personalization. In International Conference on Machine Learning, pp. 6357-6368. PMLR, 2021. Paul Pu Liang, Terrance Liu, Liu Ziyin, Nicholas B Allen, Randy P Auerbach, David Brent, Ruslan Salakhutdinov, and Louis-Philippe Morency. Think locally, act globally: Federated learning with local and global representations. arXiv preprint arXiv:2001.01523, 2020. Mi Luo, Fei Chen, Dapeng Hu, Yifan Zhang, Jian Liang, and Jiashi Feng. No fear of heterogeneity: Classifier calibration for federated learning with non-iid data. arXiv preprint arXiv:2106.05001, 2021. Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for personalization with applications to federated learning. arXiv preprint arXiv:2002.10619, 2020. H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient learning of deep networks from decentralized data. In Proc. 20th Int'l Conf. Artificial Intelligence and Statistics (AISTATS), pp. 1273-1282, 2017. Jaehoon Oh, Hyungjun Yoo, ChangHwan Kim, and Se-Young Yun. Boil: Towards representation change for few-shot learning. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=umIdUL8rMH. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems, 32: 8026-8037, 2019. Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse? towards understanding the effectiveness of maml. In International Conference on Learning Representations, 2019. Michael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung, and Jose M Alvarez. Personalized federated learning with first order model optimization. arXiv preprint arXiv:2012.08565, 2020. Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018.Our implementations are based on the code presented by(Liang et al., 2020). 9 All the reported results are based on the last model. For computation costs, we used a single TITAN RTX and the entire training time of FedBABU on CIFAR100 using MobileNet takes \u223c2 hours when f =1.0 and \u03c4 =1.Therefore, as f decreases and \u03c4 increases, it takes less time.6 CONCLUSION \n\n\n\nTable 8 :\n8Composition of CIFAR and EMNIST.Dataset \n# of Training # of Validation # of Classes \n\nCIFAR-10 \n50,000 \n10,000 \n10 \nCIFAR-100 \n50,000 \n10,000 \n100 \nEMNIST \n731,668 \n82,587 \n62 \n\nA.3 HYPERPARAMETERS \n\nFor \n\nTable 9 :\n9Personalization of the centralized models. F is trained entirely, and B is trained body-partially in the centralized setting.s \nModel \nFine-tune epochs (\u03c4f ) \n\n0 (Initial) \n1 \n2 \n3 \n4 \n5 \n\n100 \nF \n40.25\u00b14.75 40.77\u00b14.72 41.80\u00b14.86 42.43\u00b14.78 43.32\u00b14.72 44.15\u00b14.61 \nB \n39.64\u00b14.96 43.37\u00b15.41 47.66\u00b15.08 50.16\u00b15.24 51.09\u00b15.00 52.15\u00b15.04 \n\n50 \nF \n37.92\u00b15.48 38.93\u00b15.52 40.68\u00b15.48 42.40\u00b15.33 43.86\u00b15.39 45.27\u00b15.43 \nB \n37.18\u00b15.21 43.66\u00b15.34 51.14\u00b15.49 54.67\u00b15.06 56.34\u00b14.82 57.36\u00b15.21 \n\n10 \nF \n25.46\u00b16.23 28.04\u00b16.88 33.43\u00b17.32 39.56\u00b17.66 44.74\u00b17.43 50.32\u00b17.05 \nB \n23.86\u00b15.08 48.63\u00b15.39 69.14\u00b15.78 74.32\u00b15.35 75.90\u00b15.49 76.47\u00b15.48 \n\n\n\nTable 10 :\n10Initial and personalized accuracy according to momentum (m) during local updates under a realistic FL setting (N =100, f =0.1, and \u03c4 =10).m \n\nInitial accuracy \n\ns = 100 \ns = 50 \ns = 10 \nFedAvg \nFedBABU \nFedAvg \nFedBABU \nFedAvg \nFedBABU \n\n0.0 \n26.07\u00b13.83 26.38\u00b13.98 24.71\u00b14.15 24.72\u00b14.67 15.44\u00b17.16 13.66\u00b16.15 \n0.5 \n26.63\u00b14.61 27.01\u00b14.62 26.27\u00b14.87 27.58\u00b14.75 16.60\u00b16.14 17.98\u00b16.55 \n0.9 \n28.18\u00b14.83 29.38\u00b14.74 27.34\u00b14.96 27.91\u00b15.27 14.40\u00b15.64 18.50\u00b17.82 \n0.99 19.84\u00b14.12 30.62\u00b14.60 \n1.00\u00b11.40 \n30.01\u00b15.19 \n1.00\u00b13.32 \n15.68\u00b17.11 \n\nm \n\nPersonalized accuracy \n\ns = 100 \ns = 50 \ns = 10 \nFedAvg \nFedBABU \nFedAvg \nFedBABU \nFedAvg \nFedBABU \n\n0.0 \n28.46\u00b14.05 30.24\u00b14.39 30.02\u00b14.65 33.43\u00b15.26 40.80\u00b19.00 59.85\u00b17.41 \n0.5 \n30.44\u00b14.37 32.70\u00b14.65 33.64\u00b14.78 38.65\u00b15.14 54.32\u00b18.33 67.34\u00b16.32 \n0.9 \n33.13\u00b15.22 35.94\u00b15.06 38.09\u00b15.17 42.63\u00b15.59 62.67\u00b16.52 66.32\u00b17.02 \n0.99 23.88\u00b14.34 34.17\u00b14.75 \n1.12\u00b11.48 \n42.17\u00b15.32 \n1.00\u00b13.32 \n61.02\u00b17.84 \n\n\n\nTable 11 :\n11Initial and personalized accuracy of FedAvg and FedBABU on CIFAR100 under various settings with 500 clients. The used network is MobileNet. 39\u00b17.65 13.29\u00b17.40 32.41\u00b110.91 49.83\u00b111.74 F RESULTS OF 4CONVNET ON CIFAR10 F.1 ACCURACY CURVES IN THE CENTRALIZED SETTINGFL settings \nInitial accuracy \nPersonalized accuracy \n\ns \nf \n\u03c4 \nFedAvg \nFedBABU \nFedAvg \nFedBABU \n\n100 \n\n1.0 \n\n1 \n13.71\u00b17.58 11.85\u00b17.13 \n16.12\u00b17.99 \n15.00\u00b17.54 \n4 \n18.98\u00b18.57 16.79\u00b18.38 \n21.17\u00b19.05 \n19.94\u00b18.64 \n10 15.70\u00b18.18 15.66\u00b18.23 \n17.54\u00b18.91 \n18.47\u00b18.61 \n\n0.1 \n\n1 \n14.59\u00b17.72 10.53\u00b17.25 \n16.46\u00b18.27 \n13.90\u00b17.96 \n4 \n18.90\u00b18.33 17.36\u00b18.85 \n20.88\u00b18.85 \n20.30\u00b19.27 \n10 16.79\u00b17.90 14.38\u00b18.14 \n18.22\u00b18.37 \n16.76\u00b18.33 \n\n50 \n\n1.0 \n\n1 \n13.67\u00b18.05 11.04\u00b17.01 \n19.29\u00b19.31 \n17.39\u00b18.11 \n4 \n18.36\u00b18.60 16.60\u00b18.43 \n22.76\u00b19.05 \n23.15\u00b19.51 \n10 15.95\u00b18.57 14.81\u00b18.24 \n19.57\u00b19.28 \n21.38\u00b19.78 \n\n0.1 \n\n1 \n14.58\u00b18.33 10.72\u00b16.81 \n19.65\u00b19.14 \n16.05\u00b18.11 \n4 \n18.28\u00b18.79 16.79\u00b19.04 \n22.39\u00b19.78 \n23.57\u00b19.79 \n10 15.79\u00b18.21 15.07\u00b17.97 \n18.52\u00b18.69 \n20.83\u00b18.90 \n\n10 \n\n1.0 \n\n1 \n9.91\u00b17.14 \n7.41\u00b16.55 \n37.62\u00b111.76 35.02\u00b110.71 \n4 \n15.34\u00b18.49 12.67\u00b17.58 39.91\u00b112.43 49.07\u00b111.13 \n10 12.99\u00b17.98 12.93\u00b18.15 35.47\u00b111.29 48.07\u00b112.25 \n\n0.1 \n\n1 \n10.96\u00b17.22 \n8.42\u00b16.58 \n38.95\u00b111.63 35.13\u00b110.85 \n4 \n15.03\u00b18.27 13.99\u00b17.82 37.92\u00b111.64 49.96\u00b111.94 \n10 12.\n\nTable 12 :\n12Initial accuracy of FedAvg and FedBABU according to the existence of classifier on CIFAR10 under various settings with 100 clients. The used network is 4convNet.FL settings \nFedAvg \nFedBABU \n\ns \nf \n\u03c4 \nw/ classifier w/o classifier w/ classifier w/o classifier \n\n10 \n\n1.0 \n\n1 \n68.89\u00b16.35 \n74.41\u00b16.25 \n71.50\u00b17.35 \n75.00\u00b17.46 \n4 \n61.56\u00b16.84 \n66.17\u00b17.44 \n68.40\u00b16.67 \n71.77\u00b17.29 \n10 \n60.06\u00b15.68 \n65.21\u00b17.94 \n64.66\u00b16.00 \n69.04\u00b16.68 \n\n0.1 \n\n1 \n63.79\u00b17.59 \n69.48\u00b16.72 \n69.36\u00b15.61 \n71.21\u00b16.87 \n4 \n59.65\u00b18.03 \n66.11\u00b17.37 \n66.80\u00b17.43 \n69.27\u00b18.05 \n10 \n57.32\u00b16.76 \n63.19\u00b16.81 \n62.68\u00b17.06 \n67.58\u00b16.42 \n\n5 \n\n1.0 \n\n1 \n66.56\u00b19.39 \n80.73\u00b18.27 \n68.54\u00b18.45 \n80.53\u00b17.99 \n4 \n52.60\u00b17.79 \n70.53\u00b19.15 \n63.79\u00b19.12 \n78.34\u00b18.30 \n10 \n51.55\u00b18.23 \n71.03\u00b18.77 \n61.10\u00b19.80 \n77.13\u00b17.10 \n\n0.1 \n\n1 \n55.13\u00b18.57 \n71.27\u00b110.03 \n63.44\u00b19.82 \n77.64\u00b18.26 \n4 \n48.77\u00b18.54 \n68.64\u00b111.36 \n59.93\u00b18.32 \n74.61\u00b18.81 \n10 46.32\u00b110.52 \n67.07\u00b18.77 \n55.55\u00b19.90 \n73.29\u00b19.25 \n\n2 \n\n1.0 \n\n1 \n59.47\u00b115.16 \n88.60\u00b16.96 \n63.51\u00b114.63 \n88.34\u00b17.05 \n4 \n41.08\u00b113.42 \n83.87\u00b110.92 \n55.43\u00b115.76 \n88.80\u00b18.57 \n10 39.47\u00b113.44 \n82.41\u00b110.79 \n48.52\u00b113.47 \n86.33\u00b18.93 \n\n0.1 \n\n1 \n40.48\u00b114.59 \n81.43\u00b112.74 \n57.22\u00b114.05 \n86.81\u00b18.31 \n4 \n26.72\u00b114.54 \n78.56\u00b112.58 \n44.39\u00b116.49 \n83.12\u00b19.68 \n10 22.29\u00b116.39 \n73.54\u00b110.97 \n36.57\u00b117.23 \n84.37\u00b110.52 \n\n\n\nTable 13 :\n13Personalized accuracy comparison on CIFAR10 under various settings with 100 clients. The used network is 4convNet.FL settings \nPersonalized accuracy \n\ns \nf \n\u03c4 \nFedBABU (Ours) \nFedAvg \nFedPer \nLG-FedAvg \nFedRep \nPer-FedAvg \nDitto \nLocal-Only \n\n10 \n\n1.0 \n\n1 \n80.52\u00b15.51 \n79.72\u00b15.24 \n78.51\u00b15.66 \n80.37\u00b15.32 \n71.96\u00b16.62 \n79.89\u00b15.84 \n79.89\u00b16.49 \n\n55.40\u00b111.78 \n\n4 \n79.30\u00b15.67 \n73.35\u00b16.15 \n71.04\u00b17.50 \n74.75\u00b15.87 \n63.71\u00b17.01 \n72.10\u00b16.57 \n77.99\u00b16.48 \n10 \n76.86\u00b15.08 \n71.03\u00b15.86 \n69.25\u00b17.07 \n71.91\u00b15.72 \n60.27\u00b17.13 \n66.52\u00b17.64 \n72.88\u00b16.11 \n\n0.1 \n\n1 \n78.80\u00b15.51 \n73.05\u00b15.94 \n77.22\u00b15.57 \n67.95\u00b17.00 \n78.21\u00b15.38 \n77.23\u00b15.48 \n75.27\u00b16.64 \n4 \n77.45\u00b16.23 \n71.06\u00b16.19 \n71.13\u00b16.78 \n64.17\u00b17.29 \n68.78\u00b16.91 \n67.32\u00b17.99 \n71.49\u00b16.34 \n10 \n76.25\u00b16.16 \n69.11\u00b15.70 \n66.43\u00b16.59 \n61.15\u00b17.32 \n60.70\u00b18.76 \n61.22\u00b18.14 \n64.97\u00b17.07 \n\n5 \n\n1.0 \n\n1 \n85.17\u00b16.43 \n83.82\u00b16.75 \n84.50\u00b15.70 \n85.04\u00b16.35 \n78.98\u00b18.29 \n74.96\u00b17.36 \n82.64\u00b16.85 \n\n68.79\u00b113.38 \n\n4 \n83.65\u00b15.99 \n77.47\u00b18.32 \n76.96\u00b18.65 \n77.54\u00b18.14 \n73.05\u00b19.11 \n65.17\u00b110.71 \n83.77\u00b16.93 \n10 \n83.48\u00b16.13 \n77.01\u00b17.80 \n76.17\u00b18.11 \n77.28\u00b17.52 \n70.62\u00b17.36 \n53.31\u00b112.12 \n79.01\u00b17.56 \n\n0.1 \n\n1 \n82.76\u00b16.47 \n75.08\u00b19.34 \n80.69\u00b17.63 \n63.30\u00b110.62 \n83.71\u00b17.05 \n72.15\u00b110.22 \n72.08\u00b19.92 \n4 \n81.31\u00b17.17 \n74.87\u00b19.64 \n74.74\u00b18.04 \n59.17\u00b110.18 \n77.64\u00b17.51 \n58.32\u00b111.28 \n66.36\u00b19.53 \n10 \n80.99\u00b17.64 \n74.29\u00b18.29 \n74.92\u00b18.32 \n55.19\u00b113.91 68.71\u00b110.47 47.48\u00b113.76 58.66\u00b112.51 \n\n2 \n\n1.0 \n\n1 \n91.41\u00b16.50 \n91.40\u00b16.12 \n91.75\u00b16.90 \n92.35\u00b15.44 \n91.11\u00b18.21 \n72.02\u00b114.48 \n91.41\u00b16.82 \n\n90.85\u00b19.10 \n\n4 \n91.24\u00b17.33 \n88.91\u00b19.06 \n89.14\u00b18.44 \n88.67\u00b19.03 \n87.74\u00b18.08 \n43.41\u00b123.51 \n92.17\u00b16.42 \n10 \n90.53\u00b17.20 \n88.48\u00b18.44 \n88.15\u00b19.20 \n87.38\u00b18.56 \n86.19\u00b19.50 \n36.42\u00b118.35 \n90.15\u00b17.57 \n\n0.1 \n\n1 \n90.98\u00b16.22 \n86.36\u00b110.57 \n90.33\u00b17.52 \n68.63\u00b114.34 \n91.88\u00b17.37 \n63.28\u00b113.77 64.16\u00b118.12 \n4 \n87.85\u00b18.57 \n85.94\u00b110.72 86.84\u00b110.20 47.00\u00b124.75 86.68\u00b110.13 32.19\u00b119.63 54.69\u00b117.67 \n10 \n88.93\u00b19.33 \n85.14\u00b110.73 \n86.69\u00b18.04 \n33.30\u00b122.29 83.85\u00b111.20 26.61\u00b119.52 43.64\u00b121.03 \n\n\n\nTable 14 :\n14Initial accuracy comparison on CIFAR10 under various settings with 100 clients. The used network is 4convNet.FL settings \nLocal update parts \n\ns \nf \n\u03c4 \nConv1 \nConv12 \nConv123 \nConv1234 \nConv1234+Linear \n(FedBABU) \n(FedAvg) \n\n10 \n\n1.0 \n\n1 \n25.22\u00b18.34 \n47.23\u00b16.91 \n68.44\u00b16.65 \n71.50\u00b17.35 \n68.89\u00b16.35 \n4 \n23.02\u00b19.57 \n45.73\u00b17.55 \n60.25\u00b17.69 \n68.40\u00b16.67 \n61.56\u00b16.84 \n10 \n17.80\u00b18.15 \n42.51\u00b17.03 \n60.48\u00b16.96 \n64.66\u00b16.00 \n60.06\u00b15.68 \n\n0.1 \n\n1 \n19.33\u00b17.71 \n42.84\u00b18.15 \n62.71\u00b17.67 \n69.36\u00b15.61 \n63.79\u00b17.59 \n4 \n19.85\u00b18.96 \n41.58\u00b18.37 \n55.26\u00b16.65 \n66.80\u00b17.43 \n59.65\u00b18.03 \n10 \n18.99\u00b19.10 \n44.38\u00b17.49 \n54.61\u00b16.66 \n62.68\u00b17.06 \n57.32\u00b16.76 \n\n5 \n\n1.0 \n\n1 \n23.42\u00b18.35 \n44.35\u00b19.59 \n64.27\u00b110.48 \n68.54\u00b18.45 \n66.56\u00b19.39 \n4 \n22.34\u00b19.48 \n40.07\u00b18.83 \n54.50\u00b19.21 \n63.79\u00b19.12 \n52.60\u00b17.79 \n10 18.94\u00b110.38 \n39.59\u00b19.03 \n51.25\u00b19.22 \n61.10\u00b19.80 \n51.55\u00b18.23 \n\n0.1 \n\n1 \n20.78\u00b17.96 \n38.90\u00b110.02 \n52.14\u00b18.95 \n63.44\u00b19.82 \n55.13\u00b18.57 \n4 \n19.33\u00b110.59 \n40.71\u00b19.19 \n47.66\u00b111.17 \n59.93\u00b18.32 \n48.77\u00b18.54 \n10 \n23.71\u00b18.86 \n36.74\u00b19.72 \n46.97\u00b19.81 \n55.55\u00b19.90 \n46.32\u00b110.52 \n\n2 \n\n1.0 \n\n1 \n18.72\u00b116.35 45.01\u00b113.57 58.86\u00b115.93 63.51\u00b114.63 \n59.47\u00b115.16 \n4 \n20.08\u00b110.09 34.58\u00b111.84 40.56\u00b112.73 55.43\u00b115.76 \n41.08\u00b113.42 \n10 18.81\u00b114.56 29.74\u00b110.94 37.75\u00b112.08 48.52\u00b113.47 \n39.47\u00b113.44 \n\n0.1 \n\n1 \n16.17\u00b118.73 37.08\u00b112.45 45.57\u00b113.36 57.22\u00b114.05 \n40.48\u00b114.59 \n4 \n14.09\u00b118.82 28.75\u00b114.13 25.12\u00b115.63 44.39\u00b116.49 \n26.72\u00b114.54 \n10 17.40\u00b110.53 23.73\u00b117.87 23.16\u00b119.27 36.57\u00b117.23 \n22.29\u00b116.39 \n\n\n\nTable 15 :\n15Personalized accuracy comparison on CIFAR10 under various settings with 100 clients. The used network is 4convNet.FL settings \nLocal update parts \n\ns \nf \n\u03c4 \nConv1 \nConv12 \nConv123 \nConv1234 \nConv1234+Linear \n(FedBABU) \n(FedAvg) \n\n10 \n\n1.0 \n\n1 \n55.94\u00b17.28 \n64.88\u00b18.34 \n78.74\u00b15.24 \n80.52\u00b15.51 \n79.72\u00b15.24 \n4 \n53.06\u00b17.17 \n62.84\u00b16.57 \n71.85\u00b16.11 \n79.30\u00b15.67 \n73.35\u00b16.15 \n10 \n52.89\u00b17.14 \n62.02\u00b16.48 \n71.16\u00b16.24 \n76.86\u00b15.08 \n71.03\u00b15.86 \n\n0.1 \n\n1 \n53.85\u00b18.04 \n64.40\u00b16.47 \n72.79\u00b16.46 \n78.80\u00b15.51 \n73.05\u00b15.94 \n4 \n50.75\u00b18.21 \n61.79\u00b16.84 \n66.45\u00b17.31 \n77.45\u00b16.23 \n71.06\u00b16.19 \n10 \n51.95\u00b17.45 \n62.72\u00b17.64 \n67.41\u00b16.31 \n76.25\u00b16.16 \n69.11\u00b15.70 \n\n5 \n\n1.0 \n\n1 \n67.49\u00b19.74 \n74.39\u00b17.74 \n82.55\u00b16.50 \n85.17\u00b16.43 \n83.82\u00b16.75 \n4 \n66.12\u00b110.02 \n71.77\u00b18.48 \n79.68\u00b16.77 \n83.65\u00b15.99 \n77.47\u00b18.32 \n10 \n64.87\u00b18.21 \n69.73\u00b19.10 \n76.25\u00b18.51 \n83.48\u00b16.13 \n77.01\u00b17.80 \n\n0.1 \n\n1 \n65.50\u00b19.00 \n74.50\u00b18.77 \n74.50\u00b18.77 \n82.76\u00b16.47 \n75.08\u00b19.34 \n4 \n65.05\u00b18.56 \n70.80\u00b19.29 \n75.44\u00b18.06 \n81.31\u00b17.17 \n74.87\u00b19.64 \n10 \n65.61\u00b18.73 \n69.66\u00b18.68 \n74.79\u00b17.85 \n80.99\u00b17.64 \n74.29\u00b18.29 \n\n2 \n\n1.0 \n\n1 \n85.44\u00b19.20 \n88.20\u00b18.86 \n90.61\u00b16.48 \n91.41\u00b16.50 \n91.40\u00b16.12 \n4 \n84.46\u00b19.30 \n83.94\u00b110.18 \n88.86\u00b18.62 \n91.24\u00b17.33 \n88.91\u00b19.06 \n10 \n81.62\u00b19.66 \n86.20\u00b110.72 \n88.58\u00b19.01 \n90.53\u00b17.20 \n88.48\u00b18.44 \n\n0.1 \n\n1 \n85.46\u00b19.68 \n85.78\u00b19.73 \n88.18\u00b18.41 \n90.98\u00b16.22 \n86.36\u00b110.57 \n4 \n82.74\u00b110.45 \n85.13\u00b19.77 \n85.91\u00b110.87 \n87.85\u00b18.57 \n85.94\u00b110.72 \n10 82.52\u00b110.02 \n84.98\u00b19.25 \n85.46\u00b19.74 \n88.93\u00b19.33 \n85.14\u00b110.73 \n\nG RESULTS OF RESNET18 AND RESNET50 ON CIFAR100 \n\nG.1 ACCURACY CURVES IN THE CENTRALIZED SETTING \n\n\n\nTable 16 :\n16Initial and personalized accuracy of FedAvg and FedBABU on CIFAR100 under various \nsettings with 100 clients. The used network is ResNet18. f is 0.1. \n\nFL settings \nInitial accuracy \nPersonalized accuracy \n\ns \n\u03c4 \nFedAvg \nFedBABU \nFedAvg \nFedBABU \n\n100 \n\n1 \n57.26\u00b14.71 59.87\u00b14.27 60.39\u00b15.16 66.48\u00b14.43 \n4 \n44.79\u00b14.70 49.02\u00b15.01 49.32\u00b14.90 55.65\u00b14.81 \n10 \n34.52\u00b14.73 40.60\u00b15.62 39.04\u00b14.77 47.42\u00b15.66 \n\n50 \n\n1 \n53.73\u00b14.97 59.23\u00b15.10 63.21\u00b15.33 71.16\u00b14.75 \n4 \n42.47\u00b15.21 49.03\u00b14.56 52.76\u00b15.99 62.54\u00b14.92 \n10 \n37.09\u00b14.20 40.71\u00b14.98 47.13\u00b14.47 54.99\u00b15.37 \n\n10 \n\n1 \n42.88\u00b18.08 53.65\u00b17.53 76.84\u00b17.08 83.83\u00b15.22 \n4 \n29.15\u00b18.18 39.76\u00b18.23 67.29\u00b16.42 77.40\u00b16.45 \n10 \n26.54\u00b17.66 30.96\u00b18.09 66.18\u00b17.29 72.49\u00b16.19 \n\n\nTable 17 :\n17Initial and personalized accuracy of FedAvg and FedBABU on CIFAR100 under various settings with 100 clients. The used network is ResNet50. f is 0.1.FL settings \nInitial accuracy \nPersonalized accuracy \n\ns \n\u03c4 \nFedAvg \nFedBABU \nFedAvg \nFedBABU \n\n100 \n\n1 \n42.89\u00b15.34 60.78\u00b14.74 47.59\u00b15.16 65.74\u00b14.72 \n4 \n33.59\u00b14.35 50.71\u00b14.37 39.56\u00b14.43 56.35\u00b14.43 \n10 \n32.88\u00b14.23 40.59\u00b14.65 37.32\u00b14.47 45.52\u00b15.29 \n\n50 \n\n1 \n42.14\u00b15.27 59.51\u00b14.69 53.04\u00b15.28 70.58\u00b15.06 \n4 \n30.97\u00b14.43 48.65\u00b14.94 42.76\u00b14.95 59.64\u00b15.74 \n10 \n30.90\u00b14.76 41.14\u00b15.47 43.28\u00b15.49 52.13\u00b15.14 \n\n10 \n\n1 \n28.18\u00b17.99 51.17\u00b16.97 65.50\u00b16.92 81.43\u00b16.20 \n4 \n19.31\u00b17.56 38.45\u00b19.50 58.92\u00b17.40 73.86\u00b16.96 \n10 \n18.41\u00b17.42 30.22\u00b18.26 59.50\u00b17.30 68.42\u00b18.04 \n\nH RESULTS OF RESNET10 ON DIRICHLET DISTRIBUTION-BASED CIFAR100 \n\n\n\nTable 18 :\n18Personalized accuracy comparison on Dirichlet distribution-based non-IID CIFAR100 with 100 clients (FL setting: f =0.1, and \u03c4 =10). The used network is MobileNet.\u03b2 \nPersonalized accuracy \n\nFedBABU (Ours) \nFedAvg \nFedPer \nLG-FedAvg \nFedRep \nPer-FedAvg \nDitto \nLocal-only \n\n1.0 \n34.92\u00b16.22 \n32.10\u00b16.15 34.21\u00b15.68 \n25.00\u00b16.12 \n12.68\u00b15.27 \n18.85\u00b14.97 \n7.11\u00b14.84 23.73\u00b15.71 \n0.5 \n44.67\u00b15.80 \n40.21\u00b15.68 35.62\u00b16.17 \n27.24\u00b16.73 \n12.52\u00b14.32 \n18.36\u00b15.01 \n8.13\u00b14.69 31.24\u00b15.49 \n\nI RESULTS OF 3CONVNET ON EMNIST \n\n\n\nTable 19 :\n19Personalized accuracy comparison on EMNIST with 1488 clients (FL setting: f =0.1, and \n\u03c4 =10). The used network is 3convNet. \n\ns \nPersonalized accuracy \n\nFedBABU (Ours) \nFedAvg \nFedPer \nLG-FedAvg \nFedRep \nPer-FedAvg \nDitto \nLocal-Only \n\n60 \n85.27\u00b15.33 \n84.52\u00b14.62 \n54.68\u00b16.77 \n82.75\u00b14.78 \n75.28\u00b15.21 \n78.45\u00b15.22 \n83.38\u00b15.46 \n72.50\u00b15.91 \n30 \n89.30\u00b15.03 \n85.46\u00b15.16 \n58.72\u00b18.04 \n81.64\u00b15.80 \n82.03\u00b15.34 \n80.61\u00b16.02 \n83.23\u00b16.06 \n81.63\u00b15.73 \n6 \n95.96\u00b15.00 \n89.83\u00b17.72 64.86\u00b115.09 73.50\u00b113.02 94.61\u00b15.11 62.94\u00b114.86 76.27\u00b112.72 96.06\u00b13.89 \n\n\nTable 20 :\n20In-distribution (ID) and out-of-distribution (OOD) accuracy of FedAvg and FedBABU \nbefore/after personalization under various settings with 100 clients. The used network is MobileNet. \n\nFL settings \nFedAvg \nFedBABU \n\nID accuracy \nOOD accuracy \nID accuracy \nOOD accuracy \n\ns \nf \n\u03c4 \nBefore \nAfter \nBefore \nAfter \nBefore \nAfter \nBefore \nAfter \n\n50 \n\n1.0 \n\n1 \n47.30\u00b17.05 \n55.14\u00b17.57 \n46.46\u00b16.74 32.85\u00b16.67 \n48.30\u00b17.75 \n57.92\u00b17.98 \n47.81\u00b16.56 24.06\u00b15.27 \n4 \n37.30\u00b18.56 \n45.30\u00b18.61 \n37.24\u00b16.29 27.05\u00b15.71 \n38.99\u00b17.73 \n49.70\u00b18.00 \n37.76\u00b15.71 13.88\u00b14.78 \n10 \n30.98\u00b17.57 \n38.46\u00b19.04 \n30.54\u00b15.98 20.14\u00b15.18 \n27.63\u00b17.41 \n37.40\u00b18.06 \n29.69\u00b16.24 \n5.72\u00b13.32 \n\n0.1 \n\n1 \n39.90\u00b17.55 \n47.06\u00b17.96 \n39.06\u00b16.17 28.32\u00b15.76 \n42.98\u00b18.20 \n53.35\u00b17.73 \n42.14\u00b17.49 16.04\u00b15.08 \n4 \n33.23\u00b17.88 \n41.25\u00b18.19 \n35.70\u00b16.15 27.38\u00b15.63 \n37.07\u00b18.02 \n47.04\u00b19.18 \n36.34\u00b16.10 11.67\u00b14.32 \n10 \n28.70\u00b17.77 \n36.32\u00b17.41 \n27.15\u00b16.00 18.05\u00b15.44 \n28.26\u00b18.44 \n38.74\u00b19.08 \n29.06\u00b15.61 \n6.70\u00b12.94 \n\n10 \n\n1.0 \n\n1 \n42.16\u00b118.36 78.14\u00b116.62 41.51\u00b15.12 14.25\u00b13.37 45.76\u00b116.41 79.60\u00b111.35 44.93\u00b15.13 \n3.93\u00b12.24 \n4 \n30.60\u00b115.77 68.38\u00b116.14 31.43\u00b15.02 \n7.13\u00b13.10 \n38.43\u00b117.27 71.59\u00b115.29 37.05\u00b15.19 \n0.13\u00b10.36 \n10 26.30\u00b116.40 61.21\u00b117.16 23.70\u00b14.50 \n2.27\u00b11.48 \n27.20\u00b115.12 64.21\u00b116.24 25.49\u00b14.42 \n0.00\u00b10.00 \n\n0.1 \n\n1 \n34.47\u00b118.00 69.23\u00b114.87 34.10\u00b14.87 \n9.74\u00b12.95 \n42.57\u00b119.11 75.18\u00b116.46 39.64\u00b15.04 \n3.32\u00b11.89 \n4 \n26.86\u00b116.09 64.24\u00b116.45 27.09\u00b14.55 \n6.03\u00b12.73 \n31.31\u00b117.27 68.38\u00b115.55 30.75\u00b15.23 \n0.06\u00b10.24 \n10 18.42\u00b112.83 56.21\u00b117.46 19.67\u00b13.89 \n2.95\u00b11.92 \n22.30\u00b114.65 64.73\u00b117.16 22.46\u00b14.71 \n0.00\u00b10.00 \n\nK SIMILARITY BETWEEN CLIENTS DURING FINE-TUNING \n\n0.97 \n\n0.98 \n\n0.99 \n\n1.00 \n\nFedAvg, shard per user: 100 \nFedAvg, shard per user: 50 \nFedAvg, shard per user: 10 \n\n0 \n5 \n10 \n15 \n20 \n\n0.97 \n\n0.98 \n\n0.99 \n\n1.00 \n\nFedBABU, shard per user: 100 \n\n0 \n5 \n10 \n15 \n20 \n\nFedBABU, shard per user: 50 \n\n0 \n5 \n10 \n15 \n20 \n\nFedBABU, shard per user: 10 \n\nconv1.weight \nlayers.0.conv1.weight \nlayers.0.conv2.weight \nlayers.1.conv1.weight \nlayers.1.conv2.weight \nlayers.2.conv1.weight \nlayers.2.conv2.weight \nlayers.3.conv1.weight \nlayers.3.conv2.weight \nlayers.4.conv1.weight \nlayers.4.conv2.weight \nlayers.5.conv1.weight \nlayers.5.conv2.weight \nlayers.6.conv1.weight \nlayers.6.conv2.weight \nlayers.7.conv1.weight \nlayers.7.conv2.weight \nlayers.8.conv1.weight \nlayers.8.conv2.weight \nlayers.9.conv1.weight \nlayers.9.conv2.weight \nlayers.10.conv1.weight \nlayers.10.conv2.weight \nlayers.11.conv1.weight \nlayers.11.conv2.weight \nlayers.12.conv1.weight \nlayers.12.conv2.weight \nlinear.weight \n\n\n\nTable 21 :\n21Initial and personalized accuracy of FedBABU on CIFAR100 under realistic FL settings (N =100, f =0.1, and \u03c4 =10) according to the p, which is the percentage of all client data that the server also has. Here, the body is updated only on the server using available data.p \ns=100 (heterogeneity \u2193) \ns=50 \ns=10 (heterogeneity \u2191) \n\nInitial \nPersonalized \nInitial \nPersonalized \nInitial \nPersonalized \n\n0.00 29.38\u00b14.74 \n35.94\u00b15.06 \n27.91\u00b15.27 \n42.63\u00b15.59 \n18.50\u00b17.82 \n66.32\u00b17.02 \n0.05 28.68\u00b14.65 \n36.25\u00b14.77 \n26.94\u00b14.98 \n41.01\u00b15.34 \n21.32\u00b15.85 \n66.56\u00b17.24 \n0.10 32.49\u00b14.52 \n39.93\u00b14.76 \n31.39\u00b14.84 \n47.02\u00b15.54 \n24.34\u00b15.32 \n68.95\u00b16.92 \n\n\n\nTable 22 :\n22Initial and personalized accuracy of FedProx and FedProx+BABU with \u00b5 of 0.01 on CIFAR100 with 100 clients and f of 1.0.Algorithm \n\u03c4 \ns=100 (heterogeneity \u2193) \ns=50 \ns=10 (heterogeneity \u2191) \n\nInitial \nPersonalized \nInitial \nPersonalized \nInitial \nPersonalized \n\n\n\nTable 23 :\n23FedAvg with different learning rates under realistic FL setting (f =0.1 and \u03c4 =10). We set the body's initial learning rate (\u03b1 b ) as 0.1. FedAvg (\u03b1 h =\u03b1 b ) FedAvg (\u03b1 h =0.1 \u00d7 \u03b1 b ) FedAvg (\u03b1 h =0.01 \u00d7 \u03b1 b ) FedBABU (\u03b1 h =0)s \n100 \n24.34\u00b14.58 \n27.10\u00b14.43 \n28.37\u00b14.60 \n29.36\u00b14.46 \n50 \n33.10\u00b15.08 \n35.97\u00b15.17 \n36.21\u00b15.08 \n36.49\u00b15.37 \n10 \n50.25\u00b16.27 \n52.96\u00b17.52 \n54.04\u00b17.60 \n54.93\u00b17.85 \n\n\n\nTable 24 :\n24Initial and personalized accuracy of FedBABU on CIFAR100 according to the head's orthogonality under various FL settings with 100 clients (f =0.1) . MobileNet is used. R DESCRIPTION OF DATA DISTRIBUTION ACCORDING TO THE SHARDS PER USER s To help understanding data distribution of clients according to the shard per user s, we provide examples of them.Orthogonal \n\u03c4 \ns=100 (heterogeneity \u2193) \ns=50 \ns=10 (heterogeneity \u2191) \n\nInitial \nPersonalized \nInitial \nPersonalized \nInitial \nPersonalized \n\nO (proposed) \n\n\nTable 25 :\n25Personalized accuracy comparison on CIFAR100 under various settings with 100 clients and MobileNet is used with the total epochs of 640 (f =0.1). 67\u00b16.08 73.38\u00b16.42 74.00\u00b16.11 54.67\u00b110.20 57.42\u00b18.14 19.81\u00b111.12 31.14\u00b115.06 10 72.72\u00b16.77 68.04\u00b16.81 68.41\u00b16.46 42.44\u00b112.41 48.72\u00b18.30 T QUALITATIVE COMPARISON BETWEEN FEDAVG AND FEDBABU Figure 15 describes t-SNE visualization (Van der Maaten & Hinton, 2008) of representations learned by FedAvg and FedBABU on CIFAR10 and CIFAR100 for a qualitative comparison. For speed acceleration, we used t-SNE-CUDA (Chan et al.FL settings \nPersonalized accuracy \n\ns \n\u03c4 \nFedBABU \n(Ours) \n\nFedAvg \n(2017) \n\nFedPer \n(2019) \n\nLG-FedAvg \n(2020) \n\nFedRep \n(2021) \n\nPer-FedAvg \n(2020) \n\nDitto \n(2021) \n\n100 \n\n1 \n59.90\u00b14.52 54.34\u00b14.83 57.34\u00b14.66 \n53.24\u00b14.71 \n24.19\u00b13.88 \n48.41\u00b17.23 \n51.86\u00b14.98 \n4 \n45.65\u00b14.67 43.70\u00b14.72 47.00\u00b14.62 \n43.87\u00b14.85 \n18.19\u00b13.86 \n41.79\u00b17.08 \n36.46\u00b14.18 \n10 \n36.88\u00b14.62 36.25\u00b14.17 39.66\u00b14.96 \n35.65\u00b14.48 \n14.60\u00b13.41 \n32.27\u00b17.42 \n29.11\u00b14.59 \n\n50 \n\n1 \n65.15\u00b15.20 57.10\u00b14.69 61.67\u00b14.91 \n53.89\u00b14.90 \n32.94\u00b15.10 \n46.11\u00b17.84 \n50.78\u00b15.84 \n4 \n52.66\u00b15.79 49.42\u00b15.11 52.70\u00b15.14 \n48.18\u00b15.06 \n25.83\u00b15.06 \n36.51\u00b17.83 \n36.09\u00b15.49 \n10 \n47.00\u00b15.19 40.33\u00b15.32 47.00\u00b15.36 \n38.31\u00b15.62 \n21.59\u00b14.18 \n30.31\u00b18.10 \n30.08\u00b15.73 \n\n20 \n\n1 \n75.12\u00b15.33 66.87\u00b15.16 71.51\u00b15.53 \n59.82\u00b15.63 \n50.95\u00b16.17 \n41.77\u00b18.93 \n47.76\u00b18.87 \n4 \n67.71\u00b15.26 63.01\u00b15.50 65.87\u00b16.71 \n54.76\u00b16.35 \n42.71\u00b16.04 \n28.71\u00b19.00 \n36.05\u00b110.58 \n10 \n57.55\u00b16.95 52.44\u00b14.73 56.47\u00b16.03 \n42.11\u00b18.77 \n37.09\u00b15.75 \n23.39\u00b17.91 \n27.49\u00b18.32 \n\n10 \n\n1 \n82.26\u00b15.49 76.32\u00b16.22 79.44\u00b15.44 \n69.23\u00b16.97 \n66.19\u00b18.16 38.64\u00b110.37 42.82\u00b114.08 \n4 \n78.15.39\u00b19.44 \n23.08\u00b114.55 \n\n5 \n\n1 \n88.16\u00b15.70 84.42\u00b16.43 85.04\u00b16.30 \n81.03\u00b16.84 \n77.80\u00b19.18 42.10\u00b115.62 42.94\u00b119.36 \n4 \n84.91\u00b17.03 80.26\u00b17.35 78.65\u00b18.61 52.84\u00b117.17 69.88\u00b18.43 \n8.36\u00b111.70 \n22.61\u00b121.98 \n10 \n81.05\u00b16.89 74.70\u00b17.50 75.19\u00b17.81 30.55\u00b119.75 62.77\u00b18.05 \n5.92\u00b19.08 \n14.80\u00b121.38 \n\nAlthough there are more studies related to decoupling parameters(Rusu et al., 2018; Lee & Choi, 2018;  Flennerhag et al., 2019;Chen et al., 2019b), we focus on decoupling the entire network into the body and head.\nAppendix H reports the results under unbalanced and non-IID derived Dirichlet distribution.3  We also use 3convNet on EMNIST, 4convNet on CIFAR10, and ResNet on CIFAR100. Appendix I, F, and G report results, respectively. Appendix A presents the details of the architectures used.\nNon-private data are sampled randomly in our experiment, which can violate the FL environments. However, this experiment is conducted simply as a motivation for our study.5  We also investigate personalization of centralized trained models such as(Jiang et al., 2019), and the results are reported in Appendix C. Even in this case, training the head also has a negative impact on personalization.6 Zhuang et al. (2021) and Luo et al. (2021) posed a similar problem that the head can be easily biased because it is closest to the client's label distribution. Zhuang et al. (2021) proposed a divergence-aware predictor update module, and Luo et al. (2021) and Achituve et al. (2021) proposed a head calibration method.\nSimilar results were reported in the recent paper, showing that FedAvg beats recent personalized algorithms(Smith et al., 2017; Fallah et al., 2020; Liang et al., 2020; Hanzely & Richt\u00e1rik, 2020;Deng et al., 2020;Li et al., 2021;Arivazhagan et al., 2019) in many cases. Appendix N further discusses the effectiveness of FedAvg.\nhttps://github.com/pliang279/LG-FedAvg 10 We use two architectures from https://github.com/kuangliu/pytorch-cifar\nhttps://www.cs.toronto.edu/ kriz/cifar.html 12 https://www.nist.gov/itl/products-and-services/emnist-dataset\nAll initialization methods except for 'similar' are provided by PyTorch (Paszke et al., 2019). Here, 'similar' initialization is implemented using a uniform distribution on [0.45, 0.55], and then each row vector is unitized by dividing the norm corresponding to the row vector.\nPERSONALIZATION OF A SINGLE GLOBAL MODELWe first investigate personalization of a single global model using the FedAvg algorithm, following Wang et al. (2019)  and Jiang et al. (2019), to connect a single global model to multiple personalized models. We evaluate both the initial accuracy and the personalized accuracy, assuming that the test data sets are not gathered in the server but scattered on the clients.REPRODUCIBILITY STATEMENTOur code is available at https://github.com/jhoon-oh/FedBABU. For convenience reproducibility, shell files of each algorithm are also included.\nFederated learning based on dynamic regularization. Yue Durmus Alp Emre Acar, Ramon Matas Zhao, Matthew Navarro, Mattina, N Paul, Venkatesh Whatmough, Saligrama, International Conference on Learning Representations. Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro, Matthew Mattina, Paul N Whatmough, and Venkatesh Saligrama. Federated learning based on dynamic regularization. In International Conference on Learning Representations, 2021.\n\nAviv Idan Achituve, Shamsian, arXiv:2106.15482Aviv Navon, Gal Chechik, and Ethan Fetaya. Personalized federated learning with gaussian processes. arXiv preprintIdan Achituve, Aviv Shamsian, Aviv Navon, Gal Chechik, and Ethan Fetaya. Personalized federated learning with gaussian processes. arXiv preprint arXiv:2106.15482, 2021.\n\nFederated learning with personalization layers. Vinay Manoj Ghuhan Arivazhagan, Aaditya Kumar Aggarwal, Sunav Singh, Choudhary, arXiv:1912.00818arXiv preprintManoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. Feder- ated learning with personalization layers. arXiv preprint arXiv:1912.00818, 2019.\n\nFederated learning with hierarchical clustering of local updates to improve training on non-iid data. Christopher Briggs, Zhong Fan, Peter Andras, 2020 International Joint Conference on Neural Networks (IJCNN). IEEEChristopher Briggs, Zhong Fan, and Peter Andras. Federated learning with hierarchical clustering of local updates to improve training on non-iid data. In 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1-9. IEEE, 2020.\n\nGpu accelerated t-distributed stochastic neighbor embedding. Roshan David M Chan, Forrest Rao, John F Huang, Canny, Journal of Parallel and Distributed Computing. 131David M Chan, Roshan Rao, Forrest Huang, and John F Canny. Gpu accelerated t-distributed stochastic neighbor embedding. Journal of Parallel and Distributed Computing, 131:1-13, 2019.\n\nFederated meta-learning with fast convergence and efficient communication. Fei Chen, Mi Luo, Zhenhua Dong, Zhenguo Li, Xiuqiang He, arXiv:1802.07876arXiv preprintFei Chen, Mi Luo, Zhenhua Dong, Zhenguo Li, and Xiuqiang He. Federated meta-learning with fast convergence and efficient communication. arXiv preprint arXiv:1802.07876, 2018.\n\nA closer look at few-shot classification. Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, Jia-Bin Huang, International Conference on Learning Representations. Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at few-shot classification. In International Conference on Learning Representations, 2019a.\n\nA new meta-baseline for few-shot learning. Yinbo Chen, Xiaolong Wang, Zhuang Liu, Huijuan Xu, Trevor Darrell, arXiv:2003.04390arXiv preprintYinbo Chen, Xiaolong Wang, Zhuang Liu, Huijuan Xu, and Trevor Darrell. A new meta-baseline for few-shot learning. arXiv preprint arXiv:2003.04390, 2020a.\n\nFedhealth: A federated transfer learning framework for wearable healthcare. Yiqiang Chen, Xin Qin, Jindong Wang, Chaohui Yu, Wen Gao, IEEE Intelligent Systems. 354Yiqiang Chen, Xin Qin, Jindong Wang, Chaohui Yu, and Wen Gao. Fedhealth: A federated transfer learning framework for wearable healthcare. IEEE Intelligent Systems, 35(4):83-93, 2020b.\n\nYutian Chen, Abram L Friesen, Feryal Behbahani, Arnaud Doucet, David Budden, W Matthew, Nando Hoffman, De Freitas, arXiv:1909.05557Modular meta-learning with shrinkage. arXiv preprintYutian Chen, Abram L Friesen, Feryal Behbahani, Arnaud Doucet, David Budden, Matthew W Hoffman, and Nando de Freitas. Modular meta-learning with shrinkage. arXiv preprint arXiv:1909.05557, 2019b.\n\nFine-tuning is fine in federated learning. Gary Cheng, Karan Chadha, John Duchi, arXiv:2108.07313arXiv preprintGary Cheng, Karan Chadha, and John Duchi. Fine-tuning is fine in federated learning. arXiv preprint arXiv:2108.07313, 2021.\n\nEmnist: Extending mnist to handwritten letters. Gregory Cohen, Saeed Afshar, Jonathan Tapson, Andre Van Schaik, 2017 International Joint Conference on Neural Networks (IJCNN). IEEEGregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp. 2921-2926. IEEE, 2017.\n\nExploiting shared representations for personalized federated learning. Liam Collins, Hamed Hassani, Aryan Mokhtari, Sanjay Shakkottai, PMLRProceedings of the 38th International Conference on Machine Learning, ICML 2021. Marina Meila and Tong Zhangthe 38th International Conference on Machine Learning, ICML 2021139Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared represen- tations for personalized federated learning. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 2089-2099. PMLR, 2021. URL http://proceedings.mlr.press/v139/collins21a.html.\n\nAdaptive personalized federated learning. Yuyang Deng, Mohammad Mahdi Kamani, Mehrdad Mahdavi, arXiv:2003.13461arXiv preprintYuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated learning. arXiv preprint arXiv:2003.13461, 2020.\n\n. N Discussion, The, Of Fedavg, N DISCUSSION ON THE EFFECTIVENESS OF FEDAVG\n\npersonalized parts), can be explained similarly. If there is no personalized part, these personalized FL algorithms reduce to the FedAvg; on the other extreme, i.e., if the entire network is not aggregated, these personalized FL algorithms reduce to the local-only algorithm. Mocha (smith, summary, personalized FL algorithms are in-between local-only and FedAvg algorithms and have better personalized performance than both local-only and FedAvg in general. 2017) is a representative personalized FL paper using regularization based on relationships between tasks. pFedMeMOCHA (Smith et al., 2017) is a representative personalized FL paper using regularization based on relationships between tasks. pFedMe (T Dinh et al., 2020) and Ditto (Li et al., 2021) train local models with a regularization based on the divergence between a global model and local models. If the weight on regularization is set to zero, these regularized personalized FL algorithms reduce to the local-only algorithm; on the other extreme, these reduce to the FedAvg algorithm (i.e., \u03b8 1 = \u00b7 \u00b7 \u00b7 = \u03b8 N ). The personalized FL algorithms such as FedPer (Collins et al., 2021), in which each client has parts that are not aggregated (i.e., personalized parts), can be explained similarly. If there is no personalized part, these personalized FL algorithms reduce to the FedAvg; on the other extreme, i.e., if the entire network is not aggregated, these personalized FL algorithms reduce to the local-only algorithm. In summary, personalized FL algorithms are in-between local-only and FedAvg algorithms and have better personalized performance than both local-only and FedAvg in general.\n\nFedAvg+Fine-tuning is also in-between local-only and FedAvg because this algorithm uses two extreme algorithms sequentially (i.e., FedAvg followed by local-only). However, intuitively, FedAvg+Fine-tuning is a bit more FedAvg-based. In the same vein. whereas personalized FL algorithms 1In the same vein, FedAvg+Fine-tuning is also in-between local-only and FedAvg because this algorithm uses two extreme algorithms sequentially (i.e., FedAvg followed by local-only). However, intuitively, FedAvg+Fine-tuning is a bit more FedAvg-based, whereas personalized FL algorithms 1\n\nFedAvg (CIFAR10, train). FedAvg (CIFAR10, train).\n\nFedAvg (CIFAR10, test). FedAvg (CIFAR10, test).\n\nFedBABU (CIFAR10, train). FedBABU (CIFAR10, train).\n\nFedBABU (CIFAR10, test). FedBABU (CIFAR10, test).\n\nFedAvg (CIFAR100, train). FedAvg (CIFAR100, train).\n\nFedAvg (CIFAR100, test). FedAvg (CIFAR100, test).\n\nFedBABU (CIFAR100, train). FedBABU (CIFAR100, train).\n\nFedBABU (CIFAR100, test). FedBABU (CIFAR100, test).\n", "annotations": {"author": "[{\"end\":169,\"start\":78},{\"end\":267,\"start\":170},{\"end\":363,\"start\":268}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":86},{\"end\":182,\"start\":179},{\"end\":280,\"start\":277}]", "author_first_name": "[{\"end\":85,\"start\":78},{\"end\":178,\"start\":170},{\"end\":276,\"start\":268}]", "author_affiliation": "[{\"end\":168,\"start\":111},{\"end\":266,\"start\":209},{\"end\":362,\"start\":305}]", "title": "[{\"end\":75,\"start\":1},{\"end\":438,\"start\":364}]", "venue": null, "abstract": "[{\"end\":4497,\"start\":484}]", "bib_ref": "[{\"end\":4559,\"start\":4537},{\"end\":4800,\"start\":4781},{\"end\":4818,\"start\":4800},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4834,\"start\":4818},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4852,\"start\":4834},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5074,\"start\":5055},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5092,\"start\":5074},{\"end\":5111,\"start\":5092},{\"end\":5131,\"start\":5111},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5153,\"start\":5131},{\"end\":5172,\"start\":5153},{\"end\":5348,\"start\":5331},{\"end\":5367,\"start\":5348},{\"end\":5587,\"start\":5568},{\"end\":6282,\"start\":6263},{\"end\":6298,\"start\":6282},{\"end\":6320,\"start\":6298},{\"end\":6340,\"start\":6320},{\"end\":6382,\"start\":6364},{\"end\":6403,\"start\":6387},{\"end\":6541,\"start\":6510},{\"end\":6570,\"start\":6546},{\"end\":7258,\"start\":7255},{\"end\":7472,\"start\":7469},{\"end\":7919,\"start\":7897},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10649,\"start\":10632},{\"end\":10886,\"start\":10865},{\"end\":11449,\"start\":11431},{\"end\":11530,\"start\":11512},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11631,\"start\":11614},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11649,\"start\":11631},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12447,\"start\":12426},{\"end\":12468,\"start\":12447},{\"end\":16577,\"start\":16560},{\"end\":16595,\"start\":16577},{\"end\":16630,\"start\":16609},{\"end\":16678,\"start\":16661},{\"end\":21027,\"start\":21008},{\"end\":21046,\"start\":21027},{\"end\":21065,\"start\":21046},{\"end\":21081,\"start\":21065},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25030,\"start\":25010},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25036,\"start\":25030},{\"end\":25057,\"start\":25036},{\"end\":25075,\"start\":25057},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26926,\"start\":26909},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27349,\"start\":27329},{\"end\":27352,\"start\":27350},{\"end\":30948,\"start\":30943},{\"end\":31710,\"start\":31705},{\"end\":31811,\"start\":31806},{\"end\":31880,\"start\":31878},{\"end\":32091,\"start\":32086},{\"end\":32448,\"start\":32428},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":41586,\"start\":41566},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":41677,\"start\":41658},{\"end\":41777,\"start\":41756},{\"end\":41831,\"start\":41777},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":41865,\"start\":41844},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":42033,\"start\":42014},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":42171,\"start\":42152},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":42268,\"start\":42249},{\"end\":44776,\"start\":44773},{\"end\":47998,\"start\":47991},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":52575,\"start\":52557},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":52825,\"start\":52806},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":84045,\"start\":84026},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":85324,\"start\":85306},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":85340,\"start\":85324},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":85365,\"start\":85340}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":48570,\"start\":48476},{\"attributes\":{\"id\":\"fig_2\"},\"end\":48661,\"start\":48571},{\"attributes\":{\"id\":\"fig_3\"},\"end\":49221,\"start\":48662},{\"attributes\":{\"id\":\"fig_4\"},\"end\":49443,\"start\":49222},{\"attributes\":{\"id\":\"fig_5\"},\"end\":49647,\"start\":49444},{\"attributes\":{\"id\":\"fig_6\"},\"end\":50246,\"start\":49648},{\"attributes\":{\"id\":\"fig_8\"},\"end\":50360,\"start\":50247},{\"attributes\":{\"id\":\"fig_9\"},\"end\":50487,\"start\":50361},{\"attributes\":{\"id\":\"fig_10\"},\"end\":50967,\"start\":50488},{\"attributes\":{\"id\":\"fig_11\"},\"end\":51050,\"start\":50968},{\"attributes\":{\"id\":\"fig_12\"},\"end\":51369,\"start\":51051},{\"attributes\":{\"id\":\"fig_13\"},\"end\":51448,\"start\":51370},{\"attributes\":{\"id\":\"fig_14\"},\"end\":51471,\"start\":51449},{\"attributes\":{\"id\":\"fig_15\"},\"end\":51647,\"start\":51472},{\"attributes\":{\"id\":\"fig_17\"},\"end\":51735,\"start\":51648},{\"attributes\":{\"id\":\"fig_19\"},\"end\":51798,\"start\":51736},{\"attributes\":{\"id\":\"fig_20\"},\"end\":51922,\"start\":51799},{\"attributes\":{\"id\":\"fig_21\"},\"end\":52152,\"start\":51923},{\"attributes\":{\"id\":\"fig_22\"},\"end\":52430,\"start\":52153},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":55069,\"start\":52431},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":55969,\"start\":55070},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":56818,\"start\":55970},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":58039,\"start\":56819},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":59068,\"start\":58040},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":59188,\"start\":59069},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":59793,\"start\":59189},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":60539,\"start\":59794},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":65746,\"start\":60540},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":65963,\"start\":65747},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":66601,\"start\":65964},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":67541,\"start\":66602},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":68831,\"start\":67542},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":70104,\"start\":68832},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":72049,\"start\":70105},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":73499,\"start\":72050},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":75045,\"start\":73500},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":75761,\"start\":75046},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":76539,\"start\":75762},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":77057,\"start\":76540},{\"attributes\":{\"id\":\"tab_24\",\"type\":\"table\"},\"end\":77606,\"start\":77058},{\"attributes\":{\"id\":\"tab_25\",\"type\":\"table\"},\"end\":80140,\"start\":77607},{\"attributes\":{\"id\":\"tab_26\",\"type\":\"table\"},\"end\":80784,\"start\":80141},{\"attributes\":{\"id\":\"tab_27\",\"type\":\"table\"},\"end\":81058,\"start\":80785},{\"attributes\":{\"id\":\"tab_28\",\"type\":\"table\"},\"end\":81459,\"start\":81059},{\"attributes\":{\"id\":\"tab_29\",\"type\":\"table\"},\"end\":81981,\"start\":81460},{\"attributes\":{\"id\":\"tab_30\",\"type\":\"table\"},\"end\":83898,\"start\":81982}]", "paragraph": "[{\"end\":9433,\"start\":4513},{\"end\":10769,\"start\":9435},{\"end\":11967,\"start\":10787},{\"end\":14988,\"start\":11969},{\"end\":15649,\"start\":14990},{\"end\":16334,\"start\":15651},{\"end\":18284,\"start\":16405},{\"end\":18389,\"start\":18347},{\"end\":18478,\"start\":18391},{\"end\":18498,\"start\":18480},{\"end\":18540,\"start\":18505},{\"end\":18587,\"start\":18542},{\"end\":18878,\"start\":18875},{\"end\":18947,\"start\":18907},{\"end\":19679,\"start\":18949},{\"end\":20339,\"start\":19715},{\"end\":21082,\"start\":20341},{\"end\":23095,\"start\":21154},{\"end\":23718,\"start\":23137},{\"end\":24257,\"start\":23720},{\"end\":24335,\"start\":24314},{\"end\":25210,\"start\":24370},{\"end\":27137,\"start\":25212},{\"end\":28029,\"start\":27250},{\"end\":28248,\"start\":28065},{\"end\":28383,\"start\":28250},{\"end\":28424,\"start\":28390},{\"end\":28536,\"start\":28431},{\"end\":28602,\"start\":28560},{\"end\":30631,\"start\":28610},{\"end\":30995,\"start\":30676},{\"end\":31633,\"start\":30997},{\"end\":32180,\"start\":31635},{\"end\":33043,\"start\":32425},{\"end\":33668,\"start\":33104},{\"end\":34761,\"start\":33711},{\"end\":35096,\"start\":34790},{\"end\":37358,\"start\":35155},{\"end\":40233,\"start\":37434},{\"end\":41534,\"start\":40330},{\"end\":42238,\"start\":41536},{\"end\":43032,\"start\":42240},{\"end\":43522,\"start\":43075},{\"end\":43800,\"start\":43524},{\"end\":45725,\"start\":43852},{\"end\":47306,\"start\":45775},{\"end\":48475,\"start\":47349}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18874,\"start\":18588},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18900,\"start\":18879},{\"attributes\":{\"id\":\"formula_2\"},\"end\":19714,\"start\":19680},{\"attributes\":{\"id\":\"formula_3\"},\"end\":24369,\"start\":24336},{\"attributes\":{\"id\":\"formula_5\"},\"end\":28553,\"start\":28537},{\"attributes\":{\"id\":\"formula_6\"},\"end\":32379,\"start\":32181}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":13500,\"start\":13493},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":15104,\"start\":15097},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":15188,\"start\":15181},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":16007,\"start\":16000},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":21545,\"start\":21538},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23273,\"start\":23265},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23488,\"start\":23480},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23575,\"start\":23567},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23787,\"start\":23779},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23934,\"start\":23926},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":24016,\"start\":24008},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":24610,\"start\":24603},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":26288,\"start\":26281},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":27439,\"start\":27432},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":27592,\"start\":27585},{\"end\":30076,\"start\":30075},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":32698,\"start\":32691},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":32752,\"start\":32745},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":32862,\"start\":32855},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":33213,\"start\":33205},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":33831,\"start\":33823},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":34935,\"start\":34845},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":35357,\"start\":35335},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":36104,\"start\":36096},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":36117,\"start\":36109},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":36780,\"start\":36772},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":36956,\"start\":36948},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":37682,\"start\":37674},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":39612,\"start\":39604},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":39790,\"start\":39783},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":39967,\"start\":39960},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":43290,\"start\":43283},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":46199,\"start\":46191},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":47751,\"start\":47743}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":4511,\"start\":4499},{\"attributes\":{\"n\":\"2\"},\"end\":10785,\"start\":10772},{\"attributes\":{\"n\":\"5\"},\"end\":16403,\"start\":16337},{\"attributes\":{\"n\":\"5.1\"},\"end\":18325,\"start\":18287},{\"attributes\":{\"n\":\"5.2\"},\"end\":18345,\"start\":18328},{\"end\":18503,\"start\":18501},{\"end\":18905,\"start\":18902},{\"attributes\":{\"n\":\"5.2.1\"},\"end\":21152,\"start\":21085},{\"attributes\":{\"n\":\"5.2.2\"},\"end\":23135,\"start\":23098},{\"attributes\":{\"n\":\"5.2.3\"},\"end\":24298,\"start\":24260},{\"end\":24312,\"start\":24301},{\"attributes\":{\"n\":\"5.2.4\"},\"end\":27183,\"start\":27140},{\"attributes\":{\"n\":\"5.2.5\"},\"end\":27233,\"start\":27186},{\"end\":27248,\"start\":27236},{\"end\":28063,\"start\":28032},{\"end\":28388,\"start\":28386},{\"end\":28429,\"start\":28427},{\"end\":28558,\"start\":28555},{\"end\":28608,\"start\":28605},{\"end\":30674,\"start\":30634},{\"end\":32423,\"start\":32381},{\"end\":33102,\"start\":33046},{\"end\":33709,\"start\":33671},{\"end\":34788,\"start\":34764},{\"end\":35153,\"start\":35099},{\"end\":37432,\"start\":37361},{\"end\":40276,\"start\":40236},{\"end\":40328,\"start\":40279},{\"end\":43073,\"start\":43035},{\"end\":43850,\"start\":43803},{\"end\":45773,\"start\":45728},{\"end\":47347,\"start\":47309},{\"end\":48487,\"start\":48477},{\"end\":48582,\"start\":48572},{\"end\":49106,\"start\":48663},{\"end\":49233,\"start\":49223},{\"end\":49657,\"start\":49649},{\"end\":50258,\"start\":50248},{\"end\":50372,\"start\":50362},{\"end\":50529,\"start\":50489},{\"end\":50980,\"start\":50969},{\"end\":51063,\"start\":51052},{\"end\":51484,\"start\":51473},{\"end\":51660,\"start\":51649},{\"end\":51742,\"start\":51737},{\"end\":51805,\"start\":51800},{\"end\":51935,\"start\":51924},{\"end\":52165,\"start\":52154},{\"end\":55080,\"start\":55071},{\"end\":55980,\"start\":55971},{\"end\":56829,\"start\":56820},{\"end\":58050,\"start\":58041},{\"end\":59079,\"start\":59070},{\"end\":59199,\"start\":59190},{\"end\":59804,\"start\":59795},{\"end\":60548,\"start\":60541},{\"end\":65757,\"start\":65748},{\"end\":65974,\"start\":65965},{\"end\":66613,\"start\":66603},{\"end\":67553,\"start\":67543},{\"end\":68843,\"start\":68833},{\"end\":70116,\"start\":70106},{\"end\":72061,\"start\":72051},{\"end\":73511,\"start\":73501},{\"end\":75057,\"start\":75047},{\"end\":75773,\"start\":75763},{\"end\":76551,\"start\":76541},{\"end\":77069,\"start\":77059},{\"end\":77618,\"start\":77608},{\"end\":80152,\"start\":80142},{\"end\":80796,\"start\":80786},{\"end\":81070,\"start\":81060},{\"end\":81471,\"start\":81461},{\"end\":81993,\"start\":81983}]", "table": "[{\"end\":55969,\"start\":55358},{\"end\":56818,\"start\":56270},{\"end\":58039,\"start\":56871},{\"end\":59068,\"start\":58183},{\"end\":59793,\"start\":59698},{\"end\":60539,\"start\":59920},{\"end\":65746,\"start\":65731},{\"end\":65963,\"start\":65791},{\"end\":66601,\"start\":66101},{\"end\":67541,\"start\":66754},{\"end\":68831,\"start\":67818},{\"end\":70104,\"start\":69007},{\"end\":72049,\"start\":70233},{\"end\":73499,\"start\":72173},{\"end\":75045,\"start\":73628},{\"end\":75761,\"start\":75060},{\"end\":76539,\"start\":75924},{\"end\":77057,\"start\":76716},{\"end\":77606,\"start\":77072},{\"end\":80140,\"start\":77621},{\"end\":80784,\"start\":80423},{\"end\":81058,\"start\":80918},{\"end\":81459,\"start\":81298},{\"end\":81981,\"start\":81826},{\"end\":83898,\"start\":82560}]", "figure_caption": "[{\"end\":48570,\"start\":48489},{\"end\":48661,\"start\":48584},{\"end\":49221,\"start\":49113},{\"end\":49443,\"start\":49235},{\"end\":49647,\"start\":49446},{\"end\":50246,\"start\":49659},{\"end\":50360,\"start\":50260},{\"end\":50487,\"start\":50374},{\"end\":50967,\"start\":50534},{\"end\":51050,\"start\":50983},{\"end\":51369,\"start\":51066},{\"end\":51448,\"start\":51372},{\"end\":51471,\"start\":51451},{\"end\":51647,\"start\":51487},{\"end\":51735,\"start\":51663},{\"end\":51798,\"start\":51744},{\"end\":51922,\"start\":51807},{\"end\":52152,\"start\":51938},{\"end\":52430,\"start\":52168},{\"end\":55069,\"start\":52433},{\"end\":55358,\"start\":55082},{\"end\":56270,\"start\":55982},{\"end\":56871,\"start\":56831},{\"end\":58183,\"start\":58052},{\"end\":59188,\"start\":59081},{\"end\":59698,\"start\":59201},{\"end\":59920,\"start\":59806},{\"end\":65731,\"start\":60550},{\"end\":65791,\"start\":65759},{\"end\":66101,\"start\":65976},{\"end\":66754,\"start\":66616},{\"end\":67818,\"start\":67556},{\"end\":69007,\"start\":68846},{\"end\":70233,\"start\":70119},{\"end\":72173,\"start\":72064},{\"end\":73628,\"start\":73514},{\"end\":75924,\"start\":75776},{\"end\":76716,\"start\":76554},{\"end\":80423,\"start\":80155},{\"end\":80918,\"start\":80799},{\"end\":81298,\"start\":81073},{\"end\":81826,\"start\":81474},{\"end\":82560,\"start\":81996}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8899,\"start\":8891},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17211,\"start\":17203},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17310,\"start\":17302},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17321,\"start\":17313},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":34465,\"start\":34457},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":34478,\"start\":34470},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":34588,\"start\":34580},{\"attributes\":{\"ref_id\":\"fig_20\"},\"end\":35945,\"start\":35937},{\"attributes\":{\"ref_id\":\"fig_20\"},\"end\":35958,\"start\":35950},{\"attributes\":{\"ref_id\":\"fig_20\"},\"end\":36012,\"start\":36003},{\"attributes\":{\"ref_id\":\"fig_20\"},\"end\":36025,\"start\":36017},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36620,\"start\":36611},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38826,\"start\":38817},{\"end\":42540,\"start\":42539},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":44743,\"start\":44734},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46355,\"start\":46346},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":48276,\"start\":48267}]", "bib_author_first_name": "[{\"end\":86577,\"start\":86574},{\"end\":86605,\"start\":86600},{\"end\":86611,\"start\":86606},{\"end\":86625,\"start\":86618},{\"end\":86645,\"start\":86644},{\"end\":86661,\"start\":86652},{\"end\":86971,\"start\":86967},{\"end\":87350,\"start\":87345},{\"end\":87390,\"start\":87377},{\"end\":87406,\"start\":87401},{\"end\":87742,\"start\":87731},{\"end\":87756,\"start\":87751},{\"end\":87767,\"start\":87762},{\"end\":88151,\"start\":88145},{\"end\":88173,\"start\":88166},{\"end\":88183,\"start\":88179},{\"end\":88185,\"start\":88184},{\"end\":88512,\"start\":88509},{\"end\":88521,\"start\":88519},{\"end\":88534,\"start\":88527},{\"end\":88548,\"start\":88541},{\"end\":88561,\"start\":88553},{\"end\":88820,\"start\":88814},{\"end\":88836,\"start\":88827},{\"end\":88847,\"start\":88842},{\"end\":88869,\"start\":88854},{\"end\":88883,\"start\":88876},{\"end\":89181,\"start\":89176},{\"end\":89196,\"start\":89188},{\"end\":89209,\"start\":89203},{\"end\":89222,\"start\":89215},{\"end\":89233,\"start\":89227},{\"end\":89511,\"start\":89504},{\"end\":89521,\"start\":89518},{\"end\":89534,\"start\":89527},{\"end\":89548,\"start\":89541},{\"end\":89556,\"start\":89553},{\"end\":89782,\"start\":89776},{\"end\":89794,\"start\":89789},{\"end\":89796,\"start\":89795},{\"end\":89812,\"start\":89806},{\"end\":89830,\"start\":89824},{\"end\":89844,\"start\":89839},{\"end\":89854,\"start\":89853},{\"end\":89869,\"start\":89864},{\"end\":90203,\"start\":90199},{\"end\":90216,\"start\":90211},{\"end\":90229,\"start\":90225},{\"end\":90447,\"start\":90440},{\"end\":90460,\"start\":90455},{\"end\":90477,\"start\":90469},{\"end\":90491,\"start\":90486},{\"end\":90858,\"start\":90854},{\"end\":90873,\"start\":90868},{\"end\":90888,\"start\":90883},{\"end\":90905,\"start\":90899},{\"end\":91577,\"start\":91571},{\"end\":91592,\"start\":91584},{\"end\":91598,\"start\":91593},{\"end\":91614,\"start\":91607},{\"end\":91796,\"start\":91795}]", "bib_author_last_name": "[{\"end\":86598,\"start\":86578},{\"end\":86616,\"start\":86612},{\"end\":86633,\"start\":86626},{\"end\":86642,\"start\":86635},{\"end\":86650,\"start\":86646},{\"end\":86671,\"start\":86662},{\"end\":86682,\"start\":86673},{\"end\":86985,\"start\":86972},{\"end\":86995,\"start\":86987},{\"end\":87375,\"start\":87351},{\"end\":87399,\"start\":87391},{\"end\":87412,\"start\":87407},{\"end\":87423,\"start\":87414},{\"end\":87749,\"start\":87743},{\"end\":87760,\"start\":87757},{\"end\":87774,\"start\":87768},{\"end\":88164,\"start\":88152},{\"end\":88177,\"start\":88174},{\"end\":88191,\"start\":88186},{\"end\":88198,\"start\":88193},{\"end\":88517,\"start\":88513},{\"end\":88525,\"start\":88522},{\"end\":88539,\"start\":88535},{\"end\":88551,\"start\":88549},{\"end\":88564,\"start\":88562},{\"end\":88825,\"start\":88821},{\"end\":88840,\"start\":88837},{\"end\":88852,\"start\":88848},{\"end\":88874,\"start\":88870},{\"end\":88889,\"start\":88884},{\"end\":89186,\"start\":89182},{\"end\":89201,\"start\":89197},{\"end\":89213,\"start\":89210},{\"end\":89225,\"start\":89223},{\"end\":89241,\"start\":89234},{\"end\":89516,\"start\":89512},{\"end\":89525,\"start\":89522},{\"end\":89539,\"start\":89535},{\"end\":89551,\"start\":89549},{\"end\":89560,\"start\":89557},{\"end\":89787,\"start\":89783},{\"end\":89804,\"start\":89797},{\"end\":89822,\"start\":89813},{\"end\":89837,\"start\":89831},{\"end\":89851,\"start\":89845},{\"end\":89862,\"start\":89855},{\"end\":89877,\"start\":89870},{\"end\":89889,\"start\":89879},{\"end\":90209,\"start\":90204},{\"end\":90223,\"start\":90217},{\"end\":90235,\"start\":90230},{\"end\":90453,\"start\":90448},{\"end\":90467,\"start\":90461},{\"end\":90484,\"start\":90478},{\"end\":90502,\"start\":90492},{\"end\":90866,\"start\":90859},{\"end\":90881,\"start\":90874},{\"end\":90897,\"start\":90889},{\"end\":90916,\"start\":90906},{\"end\":91582,\"start\":91578},{\"end\":91605,\"start\":91599},{\"end\":91622,\"start\":91615},{\"end\":91807,\"start\":91797},{\"end\":91812,\"start\":91809},{\"end\":91823,\"start\":91814},{\"end\":92158,\"start\":92146}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":235614315},\"end\":86965,\"start\":86522},{\"attributes\":{\"doi\":\"arXiv:2106.15482\",\"id\":\"b1\"},\"end\":87295,\"start\":86967},{\"attributes\":{\"doi\":\"arXiv:1912.00818\",\"id\":\"b2\"},\"end\":87627,\"start\":87297},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":216144447},\"end\":88082,\"start\":87629},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":145976615},\"end\":88432,\"start\":88084},{\"attributes\":{\"doi\":\"arXiv:1802.07876\",\"id\":\"b5\"},\"end\":88770,\"start\":88434},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":102351185},\"end\":89131,\"start\":88772},{\"attributes\":{\"doi\":\"arXiv:2003.04390\",\"id\":\"b7\"},\"end\":89426,\"start\":89133},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":198147615},\"end\":89774,\"start\":89428},{\"attributes\":{\"doi\":\"arXiv:1909.05557\",\"id\":\"b9\"},\"end\":90154,\"start\":89776},{\"attributes\":{\"doi\":\"arXiv:2108.07313\",\"id\":\"b10\"},\"end\":90390,\"start\":90156},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":30587588},\"end\":90781,\"start\":90392},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b12\",\"matched_paper_id\":231924497},\"end\":91527,\"start\":90783},{\"attributes\":{\"doi\":\"arXiv:2003.13461\",\"id\":\"b13\"},\"end\":91791,\"start\":91529},{\"attributes\":{\"id\":\"b14\"},\"end\":91868,\"start\":91793},{\"attributes\":{\"id\":\"b15\"},\"end\":93528,\"start\":91870},{\"attributes\":{\"id\":\"b16\"},\"end\":94102,\"start\":93530},{\"attributes\":{\"id\":\"b17\"},\"end\":94153,\"start\":94104},{\"attributes\":{\"id\":\"b18\"},\"end\":94202,\"start\":94155},{\"attributes\":{\"id\":\"b19\"},\"end\":94255,\"start\":94204},{\"attributes\":{\"id\":\"b20\"},\"end\":94306,\"start\":94257},{\"attributes\":{\"id\":\"b21\"},\"end\":94359,\"start\":94308},{\"attributes\":{\"id\":\"b22\"},\"end\":94410,\"start\":94361},{\"attributes\":{\"id\":\"b23\"},\"end\":94465,\"start\":94412},{\"attributes\":{\"id\":\"b24\"},\"end\":94518,\"start\":94467}]", "bib_title": "[{\"end\":86572,\"start\":86522},{\"end\":87729,\"start\":87629},{\"end\":88143,\"start\":88084},{\"end\":88812,\"start\":88772},{\"end\":89502,\"start\":89428},{\"end\":90438,\"start\":90392},{\"end\":90852,\"start\":90783},{\"end\":92144,\"start\":91870}]", "bib_author": "[{\"end\":86600,\"start\":86574},{\"end\":86618,\"start\":86600},{\"end\":86635,\"start\":86618},{\"end\":86644,\"start\":86635},{\"end\":86652,\"start\":86644},{\"end\":86673,\"start\":86652},{\"end\":86684,\"start\":86673},{\"end\":86987,\"start\":86967},{\"end\":86997,\"start\":86987},{\"end\":87377,\"start\":87345},{\"end\":87401,\"start\":87377},{\"end\":87414,\"start\":87401},{\"end\":87425,\"start\":87414},{\"end\":87751,\"start\":87731},{\"end\":87762,\"start\":87751},{\"end\":87776,\"start\":87762},{\"end\":88166,\"start\":88145},{\"end\":88179,\"start\":88166},{\"end\":88193,\"start\":88179},{\"end\":88200,\"start\":88193},{\"end\":88519,\"start\":88509},{\"end\":88527,\"start\":88519},{\"end\":88541,\"start\":88527},{\"end\":88553,\"start\":88541},{\"end\":88566,\"start\":88553},{\"end\":88827,\"start\":88814},{\"end\":88842,\"start\":88827},{\"end\":88854,\"start\":88842},{\"end\":88876,\"start\":88854},{\"end\":88891,\"start\":88876},{\"end\":89188,\"start\":89176},{\"end\":89203,\"start\":89188},{\"end\":89215,\"start\":89203},{\"end\":89227,\"start\":89215},{\"end\":89243,\"start\":89227},{\"end\":89518,\"start\":89504},{\"end\":89527,\"start\":89518},{\"end\":89541,\"start\":89527},{\"end\":89553,\"start\":89541},{\"end\":89562,\"start\":89553},{\"end\":89789,\"start\":89776},{\"end\":89806,\"start\":89789},{\"end\":89824,\"start\":89806},{\"end\":89839,\"start\":89824},{\"end\":89853,\"start\":89839},{\"end\":89864,\"start\":89853},{\"end\":89879,\"start\":89864},{\"end\":89891,\"start\":89879},{\"end\":90211,\"start\":90199},{\"end\":90225,\"start\":90211},{\"end\":90237,\"start\":90225},{\"end\":90455,\"start\":90440},{\"end\":90469,\"start\":90455},{\"end\":90486,\"start\":90469},{\"end\":90504,\"start\":90486},{\"end\":90868,\"start\":90854},{\"end\":90883,\"start\":90868},{\"end\":90899,\"start\":90883},{\"end\":90918,\"start\":90899},{\"end\":91584,\"start\":91571},{\"end\":91607,\"start\":91584},{\"end\":91624,\"start\":91607},{\"end\":91809,\"start\":91795},{\"end\":91814,\"start\":91809},{\"end\":91825,\"start\":91814},{\"end\":92160,\"start\":92146}]", "bib_venue": "[{\"end\":91094,\"start\":91030},{\"end\":86736,\"start\":86684},{\"end\":87111,\"start\":87013},{\"end\":87343,\"start\":87297},{\"end\":87838,\"start\":87776},{\"end\":88245,\"start\":88200},{\"end\":88507,\"start\":88434},{\"end\":88943,\"start\":88891},{\"end\":89174,\"start\":89133},{\"end\":89586,\"start\":89562},{\"end\":89943,\"start\":89907},{\"end\":90197,\"start\":90156},{\"end\":90566,\"start\":90504},{\"end\":91001,\"start\":90922},{\"end\":91569,\"start\":91529},{\"end\":92327,\"start\":92160},{\"end\":93760,\"start\":93530},{\"end\":94127,\"start\":94104},{\"end\":94177,\"start\":94155},{\"end\":94228,\"start\":94204},{\"end\":94280,\"start\":94257},{\"end\":94332,\"start\":94308},{\"end\":94384,\"start\":94361},{\"end\":94437,\"start\":94412},{\"end\":94491,\"start\":94467}]"}}}, "year": 2023, "month": 12, "day": 17}