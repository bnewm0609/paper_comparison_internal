{"id": 235016565, "updated": "2023-10-03 05:28:41.11", "metadata": {"title": "Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion", "authors": "[{\"first\":\"Bo\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Tao\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Guodong\",\"last\":\"Long\",\"middle\":[]},{\"first\":\"Tianyi\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Ying\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Yi\",\"last\":\"Chang\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the Web Conference 2021", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Human-curated knowledge graphs provide critical supportive information to various natural language processing tasks, but these graphs are usually incomplete, urging auto-completion of them (a.k.a. knowledge graph completion). Prevalent graph embedding approaches, e.g., TransE, learn structured knowledge via representing graph elements (i.e., entities/relations) into dense embeddings and capturing their triple-level relationship with spatial distance. However, they are hardly generalizable to the elements never visited in training and are intrinsically vulnerable to graph incompleteness. In contrast, textual encoding approaches, e.g., KG-BERT, resort to graph triple\u2019s text and triple-level contextualized representations. They are generalizable enough and robust to the incompleteness, especially when coupled with pre-trained encoders. But two major drawbacks limit the performance: (1) high overheads due to the costly scoring of all possible triples in inference, and (2) a lack of structured knowledge in the textual encoder. In this paper, we follow the textual encoding paradigm and aim to alleviate its drawbacks by augmenting it with graph embedding techniques \u2013 a complementary hybrid of both paradigms. Specifically, we partition each triple into two asymmetric parts as in translation-based graph embedding approach, and encode both parts into contextualized representations by a Siamese-style textual encoder. Built upon the representations, our model employs both deterministic classifier and spatial measurement for representation and structure learning respectively. It thus reduces the overheads by reusing graph elements\u2019 embeddings to avoid combinatorial explosion, and enhances structured knowledge by exploring the spatial characteristics. Moreover, we develop a self-adaptive ensemble scheme to further improve the performance by incorporating triple scores from an existing graph embedding model. In experiments, we achieve state-of-the-art performance on three benchmarks and a zero-shot dataset for link prediction, with highlights of inference costs reduced by 1-2 orders of magnitude compared to a sophisticated textual encoding method.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3130909864", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/www/WangSLZW021", "doi": "10.1145/3442381.3450043"}}, "content": {"source": {"pdf_hash": "0f53660d5e93a46e7d8c966c06b6aaea095d5536", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2004.14781", "status": "GREEN"}}, "grobid": {"id": "ae562952e878c9ba0aa66a78abddd2e8b3741d34", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0f53660d5e93a46e7d8c966c06b6aaea095d5536.txt", "contents": "\nStructure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion\nACMCopyright ACMApril 19-23, 2021. April 19-23, 2021\n\nBo Wang bowang19@mails.jlu.edu.cn \nSchool of Artificial Intelligence\nJilin University\n\n\nTao Shen tao.shen@student.uts.edu.au \nAustralian AI Institute\nSchool of CS\nFEIT\nUniversity of Technology Sydney\n\n\nGuodong Long guodong.long@uts.edu.au \nAustralian AI Institute\nSchool of CS\nFEIT\nUniversity of Technology Sydney\n\n\nTianyi Zhou tianyizh@uw.edu \nPaul G. Allen School of Computer Science & Engineering\nUniversity of Washington\nSeattle\n\nYing Wang wangying2010@jlu.edu.cn \nCollege of Computer Science and Technology\nJilin University\n\n\nYi Chang yichang@jlu.edu.cn \nSchool of Artificial Intelligence\nJilin University\n\n\nInternational Center of Future Science\nJilin University\n\n\nBo Wang \nTao Shen \nGuodong Long \nTianyi Zhou \nYing Wang \nYi Chang \nStructure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion\n\nSlovenia \u00a9 2021 IW3C2 (International World Wide Web Conference Committee)\nLjubljana; Ljubljana, Slovenia; New York, NY, USAACM12April 19-23, 2021. April 19-23, 202110.1145/3442381.3450043ACM Reference Format:Knowledge Graph CompletionLink PredictionKnowledge Graph EmbeddingContextualized RepresentationStructured Knowledge\nHuman-curated knowledge graphs provide critical supportive information to various natural language processing tasks, but these graphs are usually incomplete, urging auto-completion of them (a.k.a. knowledge graph completion). Prevalent graph embedding approaches, e.g., TransE, learn structured knowledge via representing graph elements (i.e., entities/relations) into dense embeddings and capturing their triple-level relationship with spatial distance. However, they are hardly generalizable to the elements never visited in training and are intrinsically vulnerable to graph incompleteness. In contrast, textual encoding approaches, e.g., KG-BERT, resort to graph triple's text and triple-level contextualized representations. They are generalizable enough and robust to the incompleteness, especially when coupled with pre-trained encoders. But two major drawbacks limit the performance: (1) high overheads due to the costly scoring of all possible triples in inference, and (2) a lack of structured knowledge in the textual encoder. In this paper, we follow the textual encoding paradigm and aim to alleviate its drawbacks by augmenting it with graph embedding techniques -a complementary hybrid of both paradigms. Specifically, we partition each triple into two asymmetric parts as in translation-based graph embedding approach, and encode both parts into contextualized representations by a Siamese-style textual encoder. Built upon the representations, our model employs both deterministic classifier and spatial measurement for representation and structure learning respectively. It thus reduces the overheads by reusing graph elements' embeddings to avoid combinatorial explosion, and enhances structured knowledge by exploring the spatial characteristics. Moreover, we develop a self-adaptive ensemble scheme to further improve the performance by incorporating triple scores from an existing graph embedding model. In experiments, we achieve state-of-the-art performance on three benchmarks and a zero-shot dataset for link prediction, with highlights of inference costs reduced by 1-2 orders of magnitude compared to a sophisticated textual encoding method. * Joint Corresponding Author. This paper is published under the Creative Commons Attribution 4.0 International (CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.\n\nINTRODUCTION\n\nKnowledge graph (KG) is a ubiquitous format of knowledge base (KB). It is structured as a directed graph whose vertices and edges respectively stand for entities and their relations. It is usually represented as a set of triples in the form of (head entity, relation, tail entity), or (h, r, t) for short. KGs as supporting knowledge play significant roles across a wide range of natural language processing (NLP) tasks, such as dialogue system [15], information retrieval [40], recommendation system [46], etc. However, human-curated knowledge graphs usually suffer from incompleteness [29], inevitably limiting their practical applications. To mitigate this issue, knowledge graph completion (KGC) aims to predict the missing triples in a knowledge graph. In this paper, we particularly target link prediction task for KGC, whose goal is to predict the missing head (tail) given the relation and tail (head) in a triple.\n\nIt is noteworthy that KG [2,30,36] is usually at a scale of billions and the number of involved entities is up to millions, so most graph neural networks (e.g., GCN [16]) operating on the whole graph are not scalable in computation. Thus, approaches for KGC often operate at triple level, which can be grouped into two paradigms, i.e., graph embedding and textual encoding approaches.\n\nGraph embedding approaches attempt to learn the representations for graph elements (i.e., entities/relations) as low-dimension vectors by exploring their structured knowledge in a KG. Typically, they directly exploit the spatial relationship of the three embeddings in a triple to learn structured knowledge, which can be classified into two sub-categories. (1) Translation-based approaches, e.g., TransE [3] and RotatE [31], score the plausibility of a triple by applying a translation function to the embeddings of head and relation, and then measuring how close the resulting embedding to the tail embedding, i.e., \u2212||\u0434(h, r ) \u2212 t || p ; And (2) semantic matching approaches, e.g., DistMult [44] and QuatE [47], derive the plausibility BERT are state-of-the-art approaches of graph embedding and textual encoding paradigms respectively. StAR is our model, and \"StAR (Self-Adp)\" is our model plus our proposed self-adaptive ensemble.\n\nof a graph triple via a matching function that directly operates on the triple, i.e., f (h, r , t). Despite their success in structure learning, they completely ignore contextualized information and thus have several drawbacks: (1) The trained models are not applicable to entities/relations unseen in training; And (2) they are intrinsically vulnerable to the graph incompleteness. These drawbacks severely weaken their generalization capability and prediction quality.\n\nTextual encoding 1 approaches, e.g., KG-BERT [45], predict the missing parts for KGC using the contextualized representation of triples' natural language text. The text can refer to textual contents of entities and relations (e.g., their names or descriptions). Coupled with pre-trained word embedding [19,24] or language model [12,17], the textual encoder can easily generalize to unseen graph elements and is invulnerable to graph incompleteness issue. However, they are limited by two inherent constraints: (1) Applying textual encoder to link prediction requires costly inference on all possible triples, causing a combinatorial explosion; (2) The textual encoder is incompetent in structure learning, leading to a lack of structured knowledge and the entity ambiguity problem [10].\n\nThe experiments of these two paradigms also reflect their individual Pros and Cons. As shown in Figure 1 (left), KG-BERT achieves a high Hits@N (i.e., Top-N recall) when N is slightly large, but fails for small N due to entity ambiguity problem. In contrast, RotatE achieves a high Hits@1/@3 since it purely learns from structured knowledge without exposure to the ambiguity problem. But it still underperforms due to a lack of text contextualized information. And in Figure 1 (right), although KG-BERT outperforms RotatE, it requires much higher overheads due to combinatorial explosion. Therefore, it is natural to integrate both the contextualized and structured knowledge into one model, while in previous works they are respectively achieved by textual encoding and graph embedding paradigms. To this end, we start with a textual encoding paradigm with better generalization and then aim at alleviating its intrinsic drawbacks, i.e., overwhelming overheads and insufficient structured knowledge. Specifically, taking the inspiration from translation-based graph embedding approach (e.g., TransE), we first partition each triple into two parts: one with a combination of head and relation, while the other with tail. Then, by applying a Siamese-style textual encoder to their text, we encode each part into separate contextualized representation. Lastly, we concatenate the 1 \"Textual encoding\" in this paper refers to capturing contextualized information across entities and relations in a triple [45], despite previous works (as detailed in \u00a73.5) using the text of a stand-alone entity/relation to enhance corresponding graph embedding. two representations in an interactive manner [26] to form the final representation of the triple and train a binary neural classifier upon it. In the meantime, as we encode the triple by separated parts, we can measure their spatial relations like translation function [3,31] and then conduct a structure learning using contrastive objective.\n\nConsequently, on the one hand, our model can re-use the same graph elements' embeddings for different triples to avoid evaluating the combinatorial number of triples required in link prediction. On the other hand, it also augments the textual encoding paradigm by modeling structured knowledge, which is essential to graph-related tasks. In addition, our empirical studies on link prediction show that introducing such structured knowledge can effectively reduce false positive predictions and help entity disambiguation. As shown in Figure 1, our model improves the KG-BERT baseline in both performance and efficiency, but given a small N (e.g., \u2264 2), the performance is not that satisfactory. Motivated by this, we propose a self-adaptive ensemble scheme that incorporates our model's outputs with the triple scores produced by an existing graph embedding model (e.g., RotatE). Thereby, we can benefit from the advantages of both the graph embedding and textual encoding. Hence, as shown in Figure 1, our model plus the proposed self-adaptive ensemble with RotatE achieves more. Our main contributions are:\n\n\u2022 We propose a hybrid model of textual encoding and graph embedding paradigms to learn both contextualized and structured knowledge for their mutual benefits: A Siamese-style textual encoder generalizes graph embeddings to unseen entities/relations, while augmenting it with structure learning contributes to entity disambiguation and high efficiency. \u2022 We develop a self-adaptive ensemble scheme to merge scores from graph embedding approach and boost the performance. \u2022 We achieve state-of-the-art results on three benchmarks and a zero-shot dataset; We show a remarkable speedup (6.5h vs. 30d on FB15k-237 [33]) over recent KG-BERT [45]; We provide a comparative analysis of the two paradigms.\n\n\nBACKGROUND\n\nWe start this section with a formal definition of the link prediction task for KGC. Then, we summarize the pre-trained masked language model and its fine-tuning. And lastly we give a brief introduction to a state-of-the-art textual encoding approach, KG-BERT [45].\n\nLink Prediction. Formally, a KG G = {E, R} consists of a set of triples (h, r, t), where h, t \u2208 E are head and tail entity respectively while r \u2208 R is the relation between them. Given a head h (tail t) and a relation r , the goal of link prediction is to find the most accurate tail t (head h) from E to make a new triple (h, r, t) plausible in G. And during inference, given an incomplete triple (h, r, ?) for example, a trained model is asked to score all candidature triples {(h, r , t \u2032 )|t \u2032 \u2208 E} and required to rank the only oracle triple (h, r , t * ) as high as possible. This is why the combinatorial explosion appears in a computation-intensive model defined at triple level.\n\nPre-Trained Masked Language Model. To obtain powerful textual encoders, masked language models (MLMs) pre-trained on largescale raw corpora learn generic contextualized representations in a self-supervised fashion (e.g., BERT [12] and RoBERTa [17]). MLMs randomly mask some tokens and predict the masked tokens by considering their contexts on both sides. Specifically, given tokenized text [w 1 , . . . , w n ], a certain percentage (e.g., 15% in [12]) of the original tokens are then masked and replaced: of those, 80% with special token [MASK], 10% with a token sampled from the vocabulary V, and the remaining kept unchanged. The masked sequence of embed-\nded tokens [x (m) 1 , . . . , x (m)\nn ] is passed into a Transformer encoder [35] to produce contextualized representations for the sequence:\nC = Transformer-Enc([x (m) 1 , . . . , x (m) n ]) \u2208 R d h \u00d7n .(1)\nThe pre-training loss is defined as\nL m = \u2212 1 |M| i \u2208M log P(w i |C :,i ),(2)\nwhere M is the set of masked token indices, and P(w i |C :,i ) is the probability of predicting the masked w i . After pre-trained, they act as initializations of textual encoders, performing very well on various NLP tasks with task-specific modules and fine-tuning [12].\n\nKG-BERT. As a recent textual encoding approach [45] for KGC, instead of using embeddings of entities/relations, it scores a triple upon triple-level contextualized representation. Specifically, a tokenizer with a word2vec [19,24] first transforms the text x of each entity/relation to a sequence of word embeddings X = [x 1 , . . . , x n ] \u2208 R d \u00d7n . So, the text of a triple (x (h) , x (r ) , x (t ) ) can be denoted as (X (h) , X (r ) , X (t ) ). Then, KG-BERT applies the Transformer encoder [35] to a concatenation of the head, relation and tail. The encoder is initialized by a pre-trained MLM, BERT, and the concatenation is\nX = [x [CLS] , X (h) , x [SEP] , X (r ) , x [SEP] , X (t ) , x [SEP] ], where [CLS]\nand [SEP] are special tokens defined by Devlin et al. [12]. Based on this, KG-BERT produces a contextualized representation c for the entire triple, i.e., c = Pool(Transformer-Enc(X )),\n\nwhere Pool(\u00b7), defined in [12], collects the resulting of [CLS] to denote a contextualized representation for the sequence. Next, c is passed into a two-way classifier to determine if the triple is plausible or not. Lastly, the model is fine-tuned by minimizing a cross entropy loss. In inference, positive probability (i.e., confidence) of a triple is used as a ranking score. Such a simple approach shows its effectiveness for KGC, highlighting the significance of text representation learning. We thus follow this line and propose our model by avoiding combinatorial explosion and enhancing structure learning.\n\n\nPROPOSED APPROACH\n\nIn this section, we first elaborate on a structure-aware triple encoder ( \u00a73.1) and a structure-augmented triple scoring module ( \u00a73.2), which compose our Structure-Augmented Text Representation (StAR) model to tackle link prediction for KBC (as illustrated in Figure 2). And we provide the details about training and inference, e.g., training objectives and efficiency, in \u00a73.3. Then, we develop a self-adaptive ensemble scheme in \u00a73.4 to make the best of an existing graph embedding approach and boost the performance. Lastly, in \u00a73.5, we provide comparative analyses between our model and previous text-based approaches for graph-related tasks.\n\n\nStructure-Aware Triple Encoding\n\nIn this subsection, we aim at encoding a graph triple into vector representation(s) in latent semantic space, with consideration of subsequent structure learning and inference speedup. The representation(s), similar to graph embeddings, can be fed into any downstream objective-specific module to fulfil triple scoring.\n\nRecently, to accelerate the inference of a deep Transformer-based model [12,35] in an information retrieval (IR) task, Reimers and Gurevych [26] adopted a two-branch Siamese architecture [9] to bypass pairwise input via encoding the query and candidate separately. This enables pre-computing the representations for all candidates and uses a light-weight matching net [26] to calculate relatedness. We take this inspiration to link prediction to avoid combinatorial explosion, but several open questions arise: (1) How to preserve contextualized knowledge across the entities and relation in a triple;\n\n(2) How to apply Siamese architecture to a triple with three components; and (3) How to facilitate structure learning in downstream modules.\n\nThese questions can be dispelled by digesting several techniques from translation-based graph embedding approaches, e.g., TransE [3] and RotatE [31]. The techniques include applying a translation function to the embeddings of head and relation, and structure learning via exploring spatial relationship (e.g., distance) between the function's output and tail embedding.\n\nSpecifically, TransE and RotatE explicitly define the translation function as real vector addition and complex vector product, respectively. In contrast, as a textual encoding approach, we implicitly formulate the function as applying a Transformer-based encoder to a text concatenation of head and relation. The concatenation is:\nX (h) = [x [CLS] , X (h) , x [SEP] , X (r ) , x [SEP] ],(4)\nwhere x [CLS] and x [SEP] are embedded special tokens defined in [12]. Refer to KG-BERT in \u00a72 for the details about pre-processing. Then, such \"contextualizing\" translation function is defined as\nu = Pool(Transformer-Enc(X (h) )),(5)\nwhere Transformer-Enc(\u00b7) denotes the Transformer encoder consisting of multi-head self-attention layers [35]. We keep using the segment identifier given by Devlin et al. [12] to mark if a token is from an entity (i.e., 0) or a relation (i.e., 1). And again, Pool(\u00b7) collects the resulting of [CLS] to denote sequence-level contextualized representation. So, u, a contextualized representation across head and relation, can be viewed as the translation function's output.\n\nOn the other side, we also encode tail by applying the Transformer encoder to its text, which is written as v = Pool(Transformer-Enc(X (t ) )),\n\nwhere,\nX (t ) = [x [CLS] , X (t ) , x [SEP] ].(7)\nConsequently, v, a contextualized representation of tail, is viewed as tail embedding. In our experiment, we keep the two Transformer encoders (i.e., in Eq.(5) and (6)) parameter-tied for parameter efficiency [26]. And it is noteworthy that the Transformer encoder can be initialized by a pre-trained language model to further boost its capacity for representation learning, which alternates between BERT [12] and RoBERTa [17] in our experiments. on a corruption of tail entity, and in the same way for the corruption of a head entity or even relation. Note that a notation whose superscript includes \" \u2032 \" denotes it is derived from a negative example, otherwise from a positive one.\n\nTo sum up, also as answers to the questions above, (1) we trade off the contextualized knowledge with efficiency: keeping context across head and relation, while separating tail for reusable embeddings to avoid combinatorial explosion; (2) we partition each triple into two asymmetric parts as in TransE: a concatenation of head and relation, versus tail; and (3) we derive two contextualized embeddings for the two parts respectively, and aim to learn structured knowledge by exploring spatial characteristics between them.\n\n\nStructure-Augmented Scoring Module\n\nGiven u and v, we present two parallel scoring strategies as at the top of Figure 2 for both contextualized and structured knowledge.\n\n\nDeterministic Representation Learning.\n\nRecently, some semantic matching graph embedding approaches for KGC use deterministic strategy [22,23] to learn the representation of entities and relations. This strategy refers to using a binary classifier that determines if a triple is plausible or not. Such representation learning is especially significant to a text-based model, which has been adopted in KG-BERT for KGC and proven effective. But, this strategy cannot be applied to the pair of contextualized representations u and v produced by our Siamese-style encoder.\n\nFortunately, a common practice in NLP literature is to apply an interactive concatenation [4,18,26] to the pair of representations and then perform a neural binary classifier. Formally, we adopt the interactive concatenation written as\nc = [u; u \u00d7 v; u \u2212 v; v],(8)\nwhere c is used to represent the semantic relationship between the two parts of a triple. Then, similar to the top layer in KG-BERT, a two-way classifier is then applied to c and produces a twodimensional categorical distribution corresponding to the negative and positive probabilities respectively, i.e.,\np = P(z|c; \u03b8 ) \u225c softmax(MLP(c; \u03b8 )) \u2208 R 2 ,(9)\nwhere MLP(\u00b7) stands for a multi-layer perceptron, and \u03b8 is its learnable parameters. During the inference of link prediction, the 2nd dimension of p, i.e., the positive probability,\ns c = p 2(10)\ncan serve as a score of the input triple to perform candidate ranking.\n\n\nSpatial Structure\n\nLearning. In the meantime, it is possible to augment structured knowledge in the encoder by exploring the spatial characteristics between the two contextualized representations. Typically, translation-based graph embedding approaches conduct structure learning by measuring spatial distance. In particular, TransE [3] and RotatE [31] score a triple inversely proportional to the spatial distance between \u0434(h, r ) and t, i.e., \u2212||\u0434(h, r ) \u2212 t ||.\n\nAnd structured knowledge is acquired via maximizing the score margin between a positive triple and its corruptions (i.e., negative triples).\n\nHere, as a triple is partitioned into two asymmetric parts by imitating the translation-based approaches, we can formulate u \u2190 f (h, r ) and v \u2190 f (t), where f (\u00b7) denotes the textual encoder in \u00a73.1. So, to acquire structured knowledge, we can score a triple by\ns d = \u2212Distance(f (h, r ), f (t)) \u225c \u2212||u \u2212 v ||,(11)\nwhere || \u00b7 || denotes L2-norm, and s d is the plausible score based on the two contextualized representations, u and v , of a triple.\n\n\nTraining and Inference\n\n3.3.1 Training Objectives and Inference Details. Before presenting two training objectives, it is necessary to perform negative sampling and generate wrong triples. In detail, given a correct triple tp = (h, r , t), we corrupt the triple and generate its corresponding wrong triple tp \u2032 by replacing either the head or tail entity with another entity randomly sampled from the entities E on G during training, which satisfies\ntp \u2032 \u2208 {(h, r , t \u2032 )|t \u2032 \u2208 E \u2227 (h, r , t \u2032 ) G} or tp \u2032 \u2208 {(h \u2032 , r , t)|h \u2032 \u2208 E \u2227 (h \u2032 , r, t) G},\nwhere E denotes the ground set of all unique entities on G. In the remainder, a variable with superscript \" \u2032 \" means that it is derived from a negative example.\n\nTriple Classification Objective. Given the resulting confidence score s c from Eq.(10) in deterministic representation learning ( \u00a73.2.1), we employ the following binary cross entropy loss to train the encoder w.r.t this objective, i.e.,\nL c = \u2212 1 |D | tp \u2208 D 1 1+|N (tp)| log s c + tp \u2032 \u2208N(tp) log(1\u2212s c \u2032 ) ,(12)\nwhere D denotes the training set containing only correct triples, N (tp) denotes a set of wrong triples generated from tp, s c denotes positive probability of tp and (1 \u2212 s c \u2032 ) denotes negative probability of the wrong triple tp \u2032 . We empirically find such representation learning using the deterministic strategy is critical to the success of textual encoding KGC, consistent with previous works [26,45]. However, s c might not contain sufficient information for ranking during inference since it is only the confidence for a single triple's correctness that does not take other triple candidates into account. This may cause inconsistency between the model's training and inference. To compromise, loss weights in Eq.(12) must be imbalanced between positive and negative examples, i.e., |N (tp)| \u226b 1, to distinguish the only positive triple among hundreds of thousands of corrupted ones during inference. Nonetheless, over-confident false positive predictions for a corruption (i.e., assigning a corrupted triple with s c \u2192 1.0) still frequently appear to hurt the performance. These thus emphasize the importance of structure learning.\n\nTriple Contrastive Objective. Given the distance-based score s d from Eq.(11) in spatial structure learning ( \u00a73.2.1), we also train the encoder by using a contrastive objective. The contrastive objective considers a pairwise ranking between a correct triple and a wrong triple, where the latter is corrupted from the former by negative sampling. Formally, let s d denote the score derived from a positive triple tp and s d \u2032 denote the score derived from a wrong triple tp \u2032 , we define the loss by using a margin-based hinge loss function, i.e.,\nL d = 1 |D | tp \u2208 D 1 |N (tp)| tp \u2032 \u2208N(tp) max(0, \u03bb \u2212 s d + s d \u2032 ). (13)\nIn experiments, we qualitatively reveal that structure learning is significant to reducing false positive and disambiguating entities, and pushes our model to produce more reliable ranking scores.\n\nTraining and Inference Strategies. The loss L to train the StAR is a sum of the two losses defined in Eq. (12) and Eq.(13), i.e.,\nL = L c + \u03b3 L d ,(14)\nwhere \u03b3 is the weight. After optimizing StAR w.r.t L, s c , s d or their integration can be used as ranking basis during inference. We will present a thorough empirical study of the possible options of ranking score based on s c and s d in \u00a74.4.\n\n\nModel Efficiency.\n\nIn the following, we analyze why our proposed model is significantly faster than its baseline, KG-BERT.\n\nTraining Efficiency. As overheads are dominated by the computations happening inside the Transformer encoder, we focus on analyzing the complexity of computing the contextualized embeddings by the encoder. In practice, the sequence lengths of the two asymmetric parts of a triple are similar because the length of an entity's text is usually much longer than a relation's text, especially Table 1: Inference efficiency comparison. L is the length of triple text. | E | and | R | are the numbers of all unique entities and relations in the graph respectively. Usually, | E | exceeds hundreds of thousands and is much greater than | R |.\n\n\nInference on Method\n\nComplexity Speed up\n\nOne Triple\nKG-BERT O (L 2 | E |) \u223c 4\u00d7 StAR O ((L/2) 2 (1 + | E |)) Entire Graph KG-BERT O (L 2 | E | 2 | R |) \u223c 4| E |\u00d7 StAR O ((L/2) 2 | E |(1 + | R |))\nwhen the entity description is included [38,45]. Hence, Siamesestyle StAR is 2\u00d7 faster than KG-BERT in training as the complexity of Transformer encoder grows quadratically with sequence length.\n\nInference Efficiency. Similarly, we also focus on analyzing the overheads used in the encoder during inference. As shown in Table  1, we list the complexities of both KG-BERT baseline and proposed StAR, and analyze the acceleration in two cases. In practice, on the test set of a benchmark, our approach, without combinatorial explosion, is faster than KG-BERT by two orders of magnitude.\n\n\nSelf-Adaptive Ensemble Scheme\n\nStAR improves previous textual encoding approaches by introducing structure learning. It reduces those overconfident but false positive predictions and mitigates the entity ambiguity problem. However, compared to graph embedding operating at entity or relation level, our StAR based on the text inherently suffers from entity ambiguity. Fortunately, combining textual encoding with graph embedding paradigms can provide a remedy: Despite entity ambiguity existing, a textual encoding approach achieves a high recall in top-k with slightly large k (e.g., k > 5), whereas a graph embedding approach can then precisely allocate the correct one from the k candidates due to robustness to ambiguity. Note, k \u226a |E |.\n\nTo the best of our knowledge, this is the first work to ensemble the two paradigms for mutual benefits. Surprisingly, simple averaging of the scores from the two paradigms significantly improves the performance. This motivates us to take a step further and develop a self-adaptive ensemble scheme.\n\nGiven an incomplete triple (i.e., (h, r, ?) or (?, r, t)), we aim to learn a weight \u03b1 \u2208 [0, 1] to generate the final triple-specific score:\ns (sa) = \u03b1 \u00d7 s (tc) + (1 \u2212 \u03b1) \u00d7 s (\u0434e) ,(15)\nwhere s (tc) is derived from StAR and s (\u0434e) is derived from RotatE [31]. Since s (tc) = s c \u2208 [0, 1] from Eq.(10) is normalized, we rescale all candidates' scores of RotatE into [0, 1] to obtain s (\u0434e) . Specifically, for an incomplete triple, we first take the top-k candidates ranked by StAR and fetch their scores from the two models, which are denoted as s (tc) \u2208 [0, 1] k and s (\u0434e) \u2208 [0, 1] k respectively. Then, we set an unseen indicator to force \u03b1 = 1 if an unseen entity/relation occurs in the incomplete triple. Next, to learn a triple-specific \u03b1, we build an MLP based upon two kinds of features: ambiguity degree x (ad) and score consistency x (sc) . Particularly, the ambiguity degree x (ad) \u225c [Std(V ); Mean(M)] where \"Std(V \u2208 R d \u00d7k ) \u2208 R d \" is the standard deviation of the top-k entities' representations, and \"Mean(M \u2208 R k\u00d7100 ) \u2208 R k \" averages the largest 100 cosine similarities between each candidate and all entities in E. Note each entity is denoted by its contextualized representation from Eq. (6). And, the score consistency\nx (sc) \u225c [|s (t c) \u2212 s (\u0434e) |, s (t c) + s (\u0434e) , s (t c) , s (\u0434e) ].\nLastly, we pass the features into an MLP with activation \u03c3 , i.e.,\n\u03b1 = \u03c3 (MLP([x (ad) ; x (sc) ]; \u03b8 (\u03b1 ) )) \u2208 [0, 1].(16)\nIn training, we fix the parameters of both our model and RotatE, and only optimize \u03b8 (\u03b1 ) by a margin-based hinge loss. In inference, we use the resulting s (sa) to re-rank the top-k candidates while keep the remaining unchanged. In experiments, we evaluated two variants of StAR: (1) StAR (Ensemble): k \u2190 \u221e and \u03b1 \u2190 0.5, equivalent to score average, as our ensemble baseline. (2) StAR (Self-Adp): k \u2190 1000 and \u03b1 is learnable.\n\n\nCompared to Prior Text-Based Approach\n\nSharing a similar motivation, some previous approaches also use textual information to represent entities and relations. However, they are distinct from textual encoding approaches like KG-BERT or StAR and can be coarsely categorized into two groups:\n\nStand-alone Embedding. These approaches [28,29] directly replace an entity/relation embedding in graph with its text representation. The representation is derived from applying a shallow encoder (e.g., CBoW and CNN) to text, regardless of contextual information across entities and relations. But, deep contextualized features are proven effective and critical to text representation for various NLP tasks [12,25]. For KBC, the features are significant for entity disambiguation. Therefore, despite slightly improving generalization, they still deliver an inferior performance. In contrast, our model achieves a better trade-off between deep contextualized features and efficiency by the carefully designed triple encoder.\n\nJoint Embedding. More similar to our work, some other approaches [13, 33, 37-39, 42, 43] also bring text representation learning into graph embedding paradigm. Standing opposite our model, they start with graph embeddings and aim at enriching the embeddings with text representations. Typically, they either use text embeddings to represent entities/relations and align heterogeneous representations into the same space [37,39,43], or employ largescale raw corpora containing co-occurrence of entities to enrich the graph embeddings [42]. However, due to graph embeddings involved, they inevitably inherit the generalization problem and incompleteness issue. And same as the above, the representation learning here is also based on shallow networks without deep contextualized knowledge. In contrast, our model, based solely on text's contextualized representations and coupled with structure learning, is able to achieve mutual benefits of the two paradigms for KBC.\n\n\nEXPERIMENT\n\nIn this section 2 , we evaluate StAR on several popular benchmarks ( \u00a74.1), and verify the model's efficiency ( \u00a74.2) and generalization ( \u00a74.3). Then, we conduct an extensive ablation study in \u00a74.4 to test various model selections and verify the significance of each proposed module. Lastly, in \u00a74.5 we comprehensively analyze the difference between graph embedding approach and textual encoding approach, and assess the self-adaptive ensemble scheme.\n\nBenchmark Datasets. We assessed the proposed approach on three popular and one zero-shot link prediction benchmarks, whose statistics are listed in Table 2. First, WN18RR [11] is a link prediction dataset from WordNet [20]. It consists of English phrases and their semantic relations. Second, FB15k-237 [33] is a subset of Freebase [2], consisting of real-world named entities and their relations. Note, WN18RR and FB15k-237 are updated from WN18 and FB15k [3] respectively by removing inverse relations and data leakage, which are the most popular benchmarks. 3 And third, UMLS [11] is a small KG containing medical semantic entities and their relations. Finally, to verify model's generalization, NELL-One [41] is a few-shot link prediction dataset derived from NELL [6], where the relations in dev/test set never appear in train set. We adopted \"In-Train\" scheme by Chen et al. [8] and used zero-shot setting. And, in line with prior approaches [38,45], we employed entity descriptions as their text for WN18RR and FB15k-237 from synonym definitions and Wikipedia paragraph [39] respectively. As for the text of relations and other datasets' entities, we directly used their text contents. Please refer to Appendix A for our training setups.\n\nEvaluation Metrics. In the inference phase, given a test triple of a KG as the correct candidate, all other entities in the KG act as wrong candidates to corrupt its either head or tail entity. The trained model aims at ranking correct triple over corrupted ones with \"filtered\" setting [3]. For evaluation metrics, there are two aspects: (1) Mean rank (MR) and mean reciprocal rank (MRR) reflect the absolute ranking; and (2) Hits@N stands for the ratio of test examples whose correct candidate is ranked in top-N . And, although there are two ranking scores from Eq.(10) and Eq. (11), only s c is used for ranking, and other options will be discussed in \u00a74.4.\n\nEvaluation Protocol. We must emphasize that, as stated by Sun et al. [32], previous methods (e.g., ConvKB, KBAT and CapsE) use an inappropriate evaluation protocol and thus mistakenly report very high results. The mistake frequently appears in a method whose score is normalized, says [0, 1], due to float precision problem. So, we strictly follow the \"RANDOM\" protocol proposed by Sun et al. [32] to evaluate our models, and avoid comparisons with vulnerable methods that have not been re-evaluated.\n\n\nEvaluations on Link Prediction\n\nThe link prediction results of competitive approaches and ours on the three benchmarks are shown in Table 3. It is observed our Table 3: Link prediction results on WN18RR, FB15k-237 and UMLS. \u2020Resulting numbers are reported by Nathani et al. [21], \u2666Resulting numbers are re-evaluated by [32], and others are taken from the original papers; UMLS results are reported by Yao et al. [45], except ConvE from our re-implementation. The bold numbers denote the best results in each genre while the underlined ones are state-of-the-art performance.\n\n\nWN18RR\n\nFB15k  proposed StAR is able to achieve state-of-the-art or competitive performance on all these datasets. The improvement is especially significant in terms of MR due to the great generalization performance of textual encoding approach, which will be further analyzed in the section below. And on WN18RR, StAR surpasses all other methods by a large margin in terms of Hits@10. Although it only achieves inferior performance on Hits@1 compared to graph embedding approaches, it still remarkably outperforms KG-BERT from the same genre by introducing structured knowledge. Further, coupled with the proposed self-adaptive scheme, the proposed model delivers new state-of-the-art performance on all metrics. Specifically, our self-adaptive model \"StAR (Self-Adp)\" significantly surpasses its ensemble baseline \"StAR (Ensemble)\" on most metrics. And, even if Hits@1 is the main weakness for a textual encoding paradigm, our self-adaptive model is still superior than the best semantic matching graph embedding approach TuckER. \n\n\nComparison with KG-BERT Baseline\n\nSince our approach is an update from the non-Siamese-style baseline, says KG-BERT, we compared StAR with KG-BERT on WN18RR in detail, including different initializations. As shown in Table 4, our proposed StAR consistently achieves superior performance over most metrics. As for empirical efficiency, it is observed our model is faster than KG-BERT despite training or inference, which is roughly consistent with the theoretical analysis in \u00a73.3.2.\n\n\nGeneralization to Unseen Graph Elements\n\nTextual encoding approaches are more generalizable to unseen entities/relations than graph embedding ones. This can be more significant when the set of entities or relations is not closed, i.e., unseen graph elements (i.e., entities/relations) appear during inference. For example, 209 out of 3134 and 29 out of 20466 test triples involve unseen entities on WN18RR and FB15k-237 respectively. This inevitably hurts the performance of graph embedding approaches, especially for the unnormalized metric MR. First, we employed a few-shot dataset, NELL-One, to perform a zero-shot evaluation where relations in test never appear in training set. As shown in Table 5, StAR with zero-shot setting is competitive with graph embedding approaches with one/five-shot setting.\n\nThen, to verify the generalization to unseen entities, we built two probing settings on WN18RR. The first probing task keeps training set unchanged and makes the test set only consist of the triples with unseen entities. And in the second probing task, we randomly removed 1900 entities from training set to support inductive entity representations [14] during test for TransE. The setting is detailed in Appendix B. As shown in Table 6, StAR is competitive across the settings but advanced graph embedding approaches (e.g., RotatE) show a substantial drop in the first task. Even if we used translation formula to inductively complete unseen entities' embeddings in the second probing task, the degeneration of TransE is significant. These verify StAR's promising generalization to unseen elements.\n\nLastly, to verify the proposed model is still competitive even if applied to close sets of entities/relations, we built the third probing task as in Table 6. We only kept the WN18RR test triples with entities/relations visited during training while removed the others.\n\n\nAblation Study\n\nTo explore each module's contribution, we conducted an extensive ablation study about StAR and the self-adaptive ensemble scheme as shown in Table 7. For single StAR, (1) Ablating Objective: First, each of the components in Eq. (14) were removed to estimate the significance of structure and representation learning. (2) Contexts' concatenation: Then, how to concatenate and encode the text from a triple is also non-trivial for learning structured knowledge. Two other options only achieved sub-optimal results. (3) Distance measurement: Two other methods, i.e., Bilinear and Cosine, similarity were also applied to Eq.(11) to measure the distance for structure learning. (4) Ranking Basis: Since two scores can be derived from the two objectives respectively, it is straightforward to integrate them in either additive or multiplicative way. As these ranking bases achieve similar performance, we further calculated the Pearson correlation between s c and s d and found the coefficient is 0.939 (p-value=7\u00d710 \u22124 ), which means the two scores are linearly related.  For \"StAR (Self-Adp)\" (in \u00a7 3.4), we ablated its features: (1) unseen indicator, (2) ambiguity degree, and (3) score consistency.\n\n\nFurther Analyses\n\nHere, we analyze the effect of structure learning on our textual encoding approach, and compare the proposed model with a graph embedding approach. And we also attempt to qualitatively measure the effectiveness of the proposed self-adaptive ensemble scheme.\n\nWhat is the effect of introducing structure learning into a textual encoding approach. As shown in Figure 3, we compared the frequency histograms of ranking score s c \u2032 derived from classification objective for negative triples from either the \"full StAR\" or \"StAR w/o contrastive objective\". It is observed, the textual encoding model augmented with structure learning reduces the number 66, (cook a , season, ready, cook b , preserve)\n\n(mechanical system, hypernym, ?) \u2190 system a 2, (mechanical system, system a , mechanism, system b , machine) 3, (system b , mechanical system, system a ,mechanism, system c ) 24, (mechanical system, production line, suspension system, . . . ) How does StAR bring improvements. As shown in Figure 4, a detailed comparison regarding different relations is conducted between StAR and RotatE. It is observed StAR achieves more consistent results than RotatE. However, StAR performs worse on several certain relations, e.g., the 8th relation in Figure 4, verb group. After checking the test triples falling into verb group, we found \"polysemy\" occurs in half of the triples, e.g., (strike a , verb group, strike b ), which hinders StAR from correctly ranking. These imply that even coupled with structured knowledge, a textual encoding approach is still vulnerable to entity ambiguity or word polysemy, and emphasize the importance of our self-adaptive ensemble scheme.\n\nWhy does StAR achieve better Hits@10 but worse Hits@1 than Ro-tatE. As shown in Table 3, it is observed that the textual encoding approach (e.g., KG-BERT, StAR) can outperform graph embedding approach (e.g., TransE, RotatE) by a large margin on Hits@10 but underperform on Hits@1. To dig this out, we conducted a case study based on the inference on WN18RR. In particular, given an oracle test triple, (sensitive a , derivationally related form, sense), after corrupting its tail and ranked by our StAR, the top-12 tail candidates are (sensitive a , sensitivity, sensibility, sensing, sense impression, sentiency, sensitive b , sense, feel, sensory, sensitive c , perceptive), where gold tail is only ranked 8th. It is observed there are many semantically-similar tail entities that can fit the oracle triple, which Table 9: An example of polysemy in WordNet: three meanings of \"sensitive\" are viewed as three separate nodes.\n\n\u2022 sensitive a : able to feel or perceive.\n\n\u2022 sensitive b : responsive to physical stimuli.\n\n\u2022 sensitive c : being susceptible to the attitudes, feelings, or circumstances of others. seem to be false negative labels for a context encoder. But this is not a matter for graph embedding approaches since they only consider graph's structure despite text. It is worth mentioning \"polysemy\" or \"ambiguity\" issue usually appears in WN18RR (an example in Table 9). The issue is more severe in FB15K-237, which partially explains why StAR only achieves competitive results. Fortunately, this issue can be significantly alleviated by the self-adaptive ensemble scheme. And, it is interesting the oracle head is ranked 1st for tail in this case but self-loop will never appear in WN18RR's test set. Hence, as shown in Table 10, after filtering self-loop candidates during inference, the performance is improved.\n\nHow does the self-adaptive ensemble scheme bring improvements. As shown in Table 3, \"StAR (Self-Adp)\" improves the performance than \"StAR\" or RotatE used alone. Intuitively, the improvement is brought from the mutual benefits of representation and structure learning. For further confirmation, we randomly listed some triples in WN18RR test, where the triples experience a certain improvement when applying self-adaptive ensemble scheme. As shown in Table 8, as demonstrated in the 1st and 3rd examples, it is observed that graph structure helps distinguish semantically-similar candidate entities and alleviate the \"polysemy\" problem. In addition, since the rich contextualized information empowers model with a high top-k recall, the self-adaptive ensemble model still achieves a satisfactory ranking result as shown in the 2nd example, even if the graph embedding model underperforms. As a result, due to the complementary benefits, the self-adaptive ensemble scheme offers significant improvements over previous approaches.\n\n\nRELATED WORK\n\nStructure Learning for Link Prediction. Previous graph embedding approaches explore structured knowledge through spatial measurement or latent matching in low-dimension vector space. Specifically, on the one hand, translation-based graph embedding approach [3,31] applies a translation function to head and relation, and compares the resulting with tail via spatial measurement. The most well-known one, TransE [3], implements the function and the measurement with real vector addition and L2 norm respectivelyscoring a triple by \u2212||(h + r ) \u2212 t ||. However, the graph embeddings defined in real vector space hardly deal with the symmetry relation pattern, and thereby underperform. To remedy this, RotatE [31] defines the graph embeddings in complex vector space, and implements the translation function with the production of two complex numbers in each dimension. On the other hand, semantic matching graph embedding approach [1,44,47] uses a matching function f (h, r , t) operating on whole triple to directly derive its plausibility score. For example, DistMult [44] applies a bilinear function to each triple's components and uses the latent similarity in vector space as the plausibility score. In spite of their success, the rich text contextualized knowledge is entirely ignored, leading to less generalization.\n\nText Representation Learning. In an NLP literature, text representation learning is fundamental to any NLP task, which aims to produce expressively powerful text embedding with contextualized knowledge [12,25]. When applied to KGC, some approaches [28,29] directly replace the graph embeddings with their text embedding. For example, Socher et al. [29] simply use continuous CBoW as the representation of triple's component, and then proposed a neural tensor network for relation classification. ConMask [28] learns relationship-dependent entity embeddings of the entity's name and parts of description based on fully CNN. These approaches are not competitive since the deep contextualized representation of a triple is not leveraged. In contrast, KG-BERT [45], as a textual encoding approach, applies pre-trained encoder to a concatenation of triples' text for deep contextualized representations. Such a simple method is very effective, but unfortunately suffers from high overheads.\n\nJointly Learning Methods. Unlike the approaches above learning either knowledge solely, several works explore jointly learning both text and structured knowledge. Please refer to the end of \u00a73.5 for more detail. For example, taking into account the sharing of substructure in the textual relations in a large-scale corpus, Toutanova et al. [33] applied a CNN to the lexicalized dependency paths of the textual relation, for augmented relation representations. Xie et al. [39] propose a representation learning method for KGs via embedding entity descriptions, and explored CNN encoder in addition to CBoW. They used the objective across this representation and graph embeddings that a vector integration of head and relation was close to the vector of tail to learn the model, as in translationbased graph embedding approaches [3]. In contrast, our work only operates on homogeneous textual data and employs the contexts for entities/relations themselves (i.e., only their own text contents or description), rather than acquiring textual knowledge (e.g., textual relations by Toutanova et al. [33]) from large-scale corpora to enrich traditional graph embeddings via joint embedding.\n\n\nCONCLUSION\n\nIn this work, we propose a structure-augmented text representation (StAR) model to tackle link prediction task for knowledge graph completion. Inspired by translation-based graph embedding designed for structure learning, we first apply a Siamese-style textual encoder to a triple for two contextualized representations. Then, based on the two representations, we present a scoring module where two parallel scoring strategies are used to learn both contextualized and structured knowledge. Moreover, we propose a self-adaptive ensemble scheme with graph embedding approach, to further boost the performance. The empirical evaluations and thorough analyses on several mainstream benchmarks show our approach achieves state-of-the-art performance with high efficiency. \n\n\nA TRAINING SETUPS\n\nIn training phase, the initialization of Transformer encoder is alternated between BERT and RoBERTa. The model is fine-tuned by Adam optimizer. For the hyperparameters in StAR, based on the best Hits@10 on dev set, we set batch size = 16, learning rate = 10 \u22125 /5 \u00d7 10 \u22125 for the models initialized with RoBERTa and BERT respectively, number of training epochs = 7 on WN18RR and FB15k-237, 8 on NELL-One, 20 on UMLS, |N (tp)| = 5, \u03bb = 1 in Eq. (13), and \u03b3 = 1 in Eq. (14). As for grid searching of hyperparameters, we list the searching scopes and the tuned hyperparameters for best in Table 11. Note, we sampled 5 negative triples for each positive triple by following Yao et al. [45] without any tuning, and we also did not tune the random seed while kept the same among the experiments. For the hyperparameters in self-adaptive ensemble scheme, based on the best Hits@10 on WN18RR/FB15k-237 dev set, we set batch size = 32/64, learning rate = 10 \u22123 /10 \u22125 , number of training epochs = 1, number of negative samples = 5/10, and margin = 0.60/0.44 in hinge loss function.\n\n\nB PROBING TASKS\n\nThe first probing task keeps training set unchanged but makes the test set only consist of the test triples involving unseen entities. And, in second probing task, we conducted a more reasonable comparison by supporting inductive representations [14] for unseen entities in a translation-based approach, and thus made following changes : (1) 1900 entities were sampled from test set, and only a test triple containing at least one of the sampled entities can be kept, resulting in 1758 test triples in this probing task; (2) Those training triples that do not contain the sampled entities are used as new training set; and (3) Those training triples containing exact one of the sampled entities are used as support set to inductively generate the embedding for the unseen entities via translation formula, such as \"h + r = t\" in TransE [3]. Using the second probing setting can assign the unseen entities with competent embeddings, thus leading to a fairer comparison than the first one. Note, if an unseen entity is involved in multiple triple on the support set, an average over the multiple inductive representations is used as its single vector representation.\n\nFigure 1 :\n1Summary comparisons on WN18RR test. RotatE and KG-\n\nFigure 2 :\n2An overview of the proposed Structure-Augmented Text Representation (StAR) model for link prediction. This illustration is based\n\nFigure 3 :\n3Three random comparative cases of frequency histogram for s c \u2032 assigned to a triple's all tail corruptions. x -axis denotes s c \u2032 and y-axis denotes frequency over the number of all corruptions.The text above each histogram shows the ranking and s c for the corresponding un-corrupted (i.e., oracle) triple. Note, interval of [0.0, 0.1] is removed since most negative triples' s c \u2032 will fall into it.\n\nFigure 4 :\n4A comparison between StAR and RotatE regarding different relations on WN18RR test set. Relations corresponding to the indices are 1) hypernym, 2) derivationally related form, 3) member meronym, 4) has part, 5) instance hypernym, 6) synset domain topic of, 7) also see, 8) verb group, 9) member of domain region, 10) member of domain usage. The number in a parenthesis denotes its proportion of test triples with the corresponding relation. Note relation \"similar to\" is ignored since its proportion is less than 0.1%. of false positive predictions and produces more accurate ranking scores. This also verifies the structured knowledge can alleviate the over-confidence problem ( \u00a73.3.1).\n\nTable 2 :\n2Summary statistics of benchmark datasets.Dataset \n# Ent # Rel # Train # Dev # Test \nWN18RR 40,943 \n11 \n86,835 \n3,034 \n3,134 \nFB15k-237 14,541 237 \n272,115 17,535 20,466 \nUMLS \n135 \n46 \n5,216 \n652 \n661 \nNELL-One 68,545 822 \n189,635 1,004 \n2,158 \n\n\n\nTable 4 :\n4Comparisons with KG-BERT on WN18RR. \"T/Ep\" stands for time per training epoch and \"Infer\" denotes inference time on test set. The time was collected on RTX6000 with mixed precision.Hits@1 @3 @10 MR MRR T/Ep Infer \nKG-BERT BERT-base \n.041 .302 .524 97 .216 40m 32h \nStAR BERT-base \n.222 .436 .647 99 .364 20m 0.9h \nKG-BERT RoBERTa-base .130 .320 .636 84 .278 40m 32h \nStAR RoBERTa-base \n.202 .410 .621 71 .343 20m 0.9h \nKG-BERT RoBERTa-large .119 .387 .698 95 .297 79m 92h \nStAR RoBERTa-large \n.243 .491 .709 51 .401 55m 1.0h \n\n\n\nTable 5 :\n5Link prediction results on NELL-One. StAR with zero-shot setting is competitive with few-shot GMatching[41] and MetaR[8].Methods \nN -Shot Hits@1 Hits@5 Hits@10 MRR \nGMatching ComplEx Five-Shot \n.14 \n.26 \n.31 \n.20 \nMetaR \n.17 \n.35 \n.44 \n.26 \nGMatching TransE \n\nOne-Shot \n\n.12 \n.21 \n.26 \n.17 \nGMatching DistMult \n.11 \n.22 \n.30 \n.17 \nGMatching ComplEx \n.12 \n.26 \n.31 \n.19 \nMetaR \n.17 \n.34 \n.40 \n.25 \nStAR BERT-base \nZero-Shot .17 \n.35 \n.45 \n.26 \n\n\n\nTable 6 :\n6Probing tasks based on WN18RR for analyzing models'generalization performance. \n\nHits@1 Hits@3 Hits@10 MR MRR \n\nOriginal Task \n\nStAR \n.243 \n.491 \n.709 \n51 .401 \nRotatE .428 \n.492 \n.571 \n3340 .476 \nTransE .042 \n.441 \n.532 \n2300 .243 \n\nFirst \nProbing Task \n\nStAR \n.240 \n.452 \n.673 \n71 .384 \nRotatE .005 \n.007 \n.012 17955 .007 \nTransE .000 \n.007 \n.016 20721 .007 \nSecond \nProbing Task \n\nStAR \n.301 \n.497 \n.676 \n99 .427 \nTransE .005 \n.121 \n.210 13102 .078 \nThird \nProbing Task \n\nStAR \n.244 \n.493 \n.712 \n49 .402 \nRotatE .455 \n.523 \n.612 \n1657 .507 \n\n\n\nTable 7 :\n7Ablation study on WN18RR. Note that * Full model denotes using two objectives for training, \" [h, r] vs. [t]\" as concatenation scheme, L2 norm as measurement, and s c as ranking basis during inference. And Rescale(\u00b7) denotes scaling all scores to [0, 1].Perspective Detail \nHits@10 MR MRR \n\nSingle Model: Module Ablation and Selection in \"StAR\" \nFull model  *  \nStAR RoBERTa-large \n.709 \n51 .401 \n\nObjective \n\u00b7 w/o contrastive obj \n.685 \n68 .399 \n\u00b7 w/o classification obj \n.653 \n67 .337 \nConcatenation, \u00b7 [h, r] vs. [r, t] \n.520 \n106 .204 \ne.g., Eq.(4, 7) \n\u00b7 [h] vs. [r, t] \n.668 \n51 .402 \nDistance \n\u00b7 Bilinear \n.605 \n79 .354 \nin Eq.(11) \n\u00b7 Cosine Similarity \n.691 \n76 .439 \n\nRanking Basis \n\n\u00b7 s d \n.701 \n62 .406 \n\u00b7 Rescale(s d ) + s c \n.706 \n48 .408 \n\u00b7 Rescale(s d ) \u00d7 s c \n.704 \n51 .408 \nEnsemble Model: Feature Ablation in \"StAR (Self-Adp)\" \nFull model \nStAR (Self-Adp) \n.732 \n46 .551 \n\nFeature \n\n\u00b7 w/o hard indicator \n.712 \n50 .540 \n\u00b7 w/o ambiguity degree \n.734 \n45 .537 \n\u00b7 w/o score consistency \n.720 \n45 .540 \n\u00b7 w/o self-Adp \u03b1 in Eq.(16) \n.675 \n540 .524 \n\n0.00% \n\n0.20% \n\n0.40% \n\n0.60% \n\n\n\nTable 8 :\n8Top-5 ranking results of candidate entities for different approaches. The first column includes incomplete triples for inference, and their labels. And the others include the ranking position and Top-5 ranked candidates where an underline denotes it is the gold entity.Incomplete Triple \n\nPositive entity ranking position & Top-5 ranked candidate entities \nStAR (Self-Adp) [Ensemble] \nStAR [Textual Encoding] \nRotatE [Graph Embedding] \n(world war ii, has part, ?) \n\u2190 tarawa-makin \n\n5, (world war ii, jutland, meuse river, \nsoissons, tarawa-makin) \n\n12, (world war ii, world war i, world \nwar, seven years' war, meuse river) \n\n10, (jutland, world war ii, \nsomme river, verdun, soissons) \n(clarify, hypernym, ?) \n\u2190 modify \n\n2, (clarify, modify, change integrity, \nconvert a , convert b ) \n\n3, (clarify, straighten out, modify, \nalter, transubstantiate) \n\n\n\nTable 10 :\n10Applying self-loop filter to WN18RR.Hits@1 @3 @10 MR MRR \nStAR \n.243 \n.491 .709 \n51 \n.401 \n+ Self-loop Filter \n.328 \n.533 .719 \n50 \n.460 \n\n\n\nTable 11 :\n11The grid searching of hyperparameters. Note, the hyperparameters in the first part, i.e., Batch Size and \u03b3 , were tuned based on WN18RR benchmark, RoBERTa initialization, learning rate = 10 \u22125 , number of training epochs = 6. After the first was part tuned, the remaining was tuned subsequently.Hyperparm \nNote \nValue Search scope \nBatch Size \n-\n16 \n{16, 32, 64} \n| N(tp) | \n-\n5 \n{5} \n\u03bb \n-\n1.0 \n{1.0} \n\u03b3 \n-\n1.0 \n{0.5, 1.0, 2.0} \n\nLearning Rate \n\nRoBERTa \n10 \u22125 \n{10 \u22125 } \nBERT \n5 \u00d7 10 \u22125 {5 \u00d7 10 \u22125 } \nWN18RR \n7 \n{6, 7, 8, 9} \nNumber of \nFB15k-237 \n7 \nTraining Epochs NELL-One \n8 \nUMLS \n20 \n{5-25} \n\n\nThe source code is available at https://github.com/wangbo9719/StAR_KGC. 3 WN18 and FB15k suffer from informative value[11,33], which causes > 80% of the test triples (e 1 , r 1 , e 2 ) can be found in the training set with another relation: (e 1 , r 2 , e 2 ) or (e 2 , r 2 , e 1 ). Dettmers et al.[11] used a rule-based model that learned the inverse relation and achieved state-of-the-art results on the dataset. Thereby it is suggested they should not be used for link prediction evaluation anymore.\nACKNOWLEDGMENTSThe authors would like to thank the anonymous referees for their valuable comments. This work is supported by the National Nat-\nTuckER: Tensor Factorization for Knowledge Graph Completion. Ivana Balazevic, Carl Allen, Timothy M Hospedales, 10.18653/v1/D19-1522Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wanthe 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational LinguisticsIvana Balazevic, Carl Allen, and Timothy M. Hospedales. 2019. TuckER: Ten- sor Factorization for Knowledge Graph Completion. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, 5184-5193. https://doi.org/10.18653/v1/D19-1522\n\nFreebase: a collaboratively created graph database for structuring human knowledge. Kurt D Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, Jamie Taylor, 10.1145/1376616.1376746Proceedings of the ACM SIGMOD International Conference on Management of Data. Jason Tsong-Li Wangthe ACM SIGMOD International Conference on Management of DataVancouver, BC, CanadaACMKurt D. Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Tay- lor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2008, Vancouver, BC, Canada, June 10-12, 2008, Jason Tsong-Li Wang (Ed.). ACM, 1247-1250. https://doi.org/10.1145/1376616. 1376746\n\nTranslating Embeddings for Modeling Multi-relational Data. Antoine Bordes, Nicolas Usunier, Alberto Garc\u00eda-Dur\u00e1n, Jason Weston, Oksana Yakhnenko, Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems. Christopher J. C. Burges, L\u00e9on Bottou, Zoubin Ghahramani, and Kilian Q. WeinbergerLake Tahoe, Nevada, United StatesProceedings of a meeting heldAntoine Bordes, Nicolas Usunier, Alberto Garc\u00eda-Dur\u00e1n, Jason Weston, and Ok- sana Yakhnenko. 2013. Translating Embeddings for Modeling Multi-relational Data. In Advances in Neural Information Processing Systems 26: 27th Annual Con- ference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, Christopher J. C. Burges, L\u00e9on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger (Eds.). 2787- 2795. http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling- multi-relational-data\n\nA large annotated corpus for learning natural language inference. R Samuel, Gabor Bowman, Christopher Angeli, Christopher D Potts, Manning, 10.18653/v1/d15-1075Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalThe Association for Computational LinguisticsSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Man- ning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015. The Association for Computational Linguistics, 632-642. https://doi.org/10.18653/v1/d15-1075\n\nKBGAN: Adversarial Learning for Knowledge Graph Embeddings. Liwei Cai, William Yang Wang, 10.18653/v1/n18-1133Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Marilyn A. Walker, Heng Ji, and Amanda Stentthe 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, Louisiana, USAAssociation for Computational Linguistics1Long PapersLiwei Cai and William Yang Wang. 2018. KBGAN: Adversarial Learning for Knowledge Graph Embeddings. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), Marilyn A. Walker, Heng Ji, and Amanda Stent (Eds.). Association for Computational Linguistics, 1470-1480. https://doi.org/10. 18653/v1/n18-1133\n\nToward an Architecture for Never-Ending Language Learning. Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R HruschkaJr, Tom M Mitchell, Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010. Maria Fox and David Poolethe Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010Atlanta, Georgia, USAAAAI PressAndrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hr- uschka Jr., and Tom M. Mitchell. 2010. Toward an Architecture for Never-Ending Language Learning. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta, Georgia, USA, July 11-15, 2010, Maria Fox and David Poole (Eds.). AAAI Press. http://www.aaai.org/ocs/index.php/ AAAI/AAAI10/paper/view/1879\n\nLow-Dimensional Hyperbolic Knowledge Graph Embeddings. Ines Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi, Christopher R\u00e9, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreaultthe 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational Linguistics2020Ines Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi, and Christo- pher R\u00e9. 2020. Low-Dimensional Hyperbolic Knowledge Graph Embeddings. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.). Association for Computational Linguistics, 6901-6914. https://www.aclweb.org/anthology/2020.acl-main.617/\n\nMeta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs. Mingyang Chen, Wen Zhang, Wei Zhang, Qiang Chen, Huajun Chen, Mingyang Chen, Wen Zhang, Wei Zhang, Qiang Chen, and Huajun Chen. 2019. Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs.\n\n10.18653/v1/D19-1431Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wanthe 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, ChinaAssociation for Computational LinguisticsIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Process- ing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, 4216-4225. https://doi.org/10.18653/v1/D19-1431\n\nLearning a Similarity Metric Discriminatively, with Application to Face Verification. Sumit Chopra, Raia Hadsell, Yann Lecun, 10.1109/CVPR.2005.202IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005. San Diego, CA, USAIEEE Computer SocietySumit Chopra, Raia Hadsell, and Yann LeCun. 2005. Learning a Similarity Metric Discriminatively, with Application to Face Verification. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005), 20- 26 June 2005, San Diego, CA, USA. IEEE Computer Society, 539-546. https: //doi.org/10.1109/CVPR.2005.202\n\nLarge-Scale Named Entity Disambiguation Based on Wikipedia Data. Silviu Cucerzan, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language LearningPrague, Czech Republic, Jason EisnerEMNLP-CoNLLSilviu Cucerzan. 2007. Large-Scale Named Entity Disambiguation Based on Wikipedia Data. In EMNLP-CoNLL 2007, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, June 28-30, 2007, Prague, Czech Republic, Jason Eisner (Ed.).\n\n. Acl, ACL, 708-716. https://www.aclweb.org/anthology/D07-1074/\n\nConvolutional 2D Knowledge Graph Embeddings. Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). Sheila A. McIlraith and Kilian Q. Weinbergerthe Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)New Orleans, Louisiana, USATim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018. Convolutional 2D Knowledge Graph Embeddings. In Proceedings of the Thirty- Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, Sheila A. McIlraith and Kilian Q. Weinberger (Eds.).\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/n19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Jill Burstein, Christy Doran, and Thamar Soloriothe 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USALong and Short Papers1Association for Computational LinguisticsJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Associa- tion for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computa- tional Linguistics, 4171-4186. https://doi.org/10.18653/v1/n19-1423\n\nExploring the Combination of Contextual Word Embeddings and Knowledge Graph Embeddings. Lea Dieudonat, Kelvin Han, Phyllicia Leavitt, Esteban Marquer, arXiv:2004.08371arXiv preprintLea Dieudonat, Kelvin Han, Phyllicia Leavitt, and Esteban Marquer. 2020. Explor- ing the Combination of Contextual Word Embeddings and Knowledge Graph Embeddings. arXiv preprint arXiv:2004.08371 (2020).\n\nInductive Representation Learning on Large Graphs. William L Hamilton, Zhitao Ying, Jure Leskovec, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman GarnettLong Beach, CA, USA, Isabelle GuyonUlrike von Luxburg, Samy BengioWilliam L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Represen- tation Learning on Large Graphs. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Ro- man Garnett (Eds.). 1024-1034. http://papers.nips.cc/paper/6703-inductive- representation-learning-on-large-graphs\n\nLearning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings. He He, Anusha Balakrishnan, Mihail Eric, Percy Liang, 10.18653/v1/P17-1162Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers, Regina Barzilay and Min-Yen Kanthe 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics1He He, Anusha Balakrishnan, Mihail Eric, and Percy Liang. 2017. Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 -August 4, Volume 1: Long Papers, Regina Barzilay and Min-Yen Kan (Eds.). Association for Computational Linguistics, 1766-1776. https://doi.org/10.18653/v1/P17-1162\n\nSemi-Supervised Classification with Graph Convolutional Networks. N Thomas, Max Kipf, Welling, 5th International Conference on Learning Representations. Toulon, FranceConference Track Proceedings. OpenReview.netThomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. https://openreview.net/forum?id=SJU4ayYgl\n\nRoBERTa: A Robustly Optimized BERT Pretraining Approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. CoRR abs/1907.11692 (2019). arXiv:1907.11692 http://arxiv.org/abs/1907.11692\n\nLearning Natural Language Inference using Bidirectional LSTM model and Inner-Attention. Yang Liu, Chengjie Sun, Lei Lin, Xiaolong Wang, arXiv:1605.09090Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang. 2016. Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention. CoRR abs/1605.09090 (2016). arXiv:1605.09090 http://arxiv.org/abs/1605.09090\n\nDistributed Representations of Words and Phrases and their Compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S Corrado, Jeffrey Dean, Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems. Lake Tahoe, Nevada, United StatesProceedings of a meeting heldTomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed Representations of Words and Phrases and their Compo- sitionality. In Advances in Neural Information Processing Systems 26: 27th An- nual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States. 3111- 3119. http://papers.nips.cc/paper/5021-distributed-representations-of-words- and-phrases-and-their-compositionality\n\nWordNet: An electronic lexical database. A George, Miller, MIT pressGeorge A Miller. 1998. WordNet: An electronic lexical database. MIT press.\n\nLearning Attention-based Embeddings for Relation Prediction in Knowledge Graphs. Deepak Nathani, Jatin Chauhan, Charu Sharma, Manohar Kaul, 10.18653/v1/p19-1466Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Long Papers, Anna Korhonen, David R. Traum, and Llu\u00eds M\u00e0rquezthe 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, ItalyAssociation for Computational Linguistics1Deepak Nathani, Jatin Chauhan, Charu Sharma, and Manohar Kaul. 2019. Learn- ing Attention-based Embeddings for Relation Prediction in Knowledge Graphs. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-August 2, 2019, Volume 1: Long Papers, Anna Ko- rhonen, David R. Traum, and Llu\u00eds M\u00e0rquez (Eds.). Association for Computational Linguistics, 4710-4723. https://doi.org/10.18653/v1/p19-1466\n\nA Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network. Tu Dinh Dai Quoc Nguyen, Dat Nguyen, Dinh Q Quoc Nguyen, Phung, 10.18653/v1/n18-2053Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT. Marilyn A. Walker, Heng Ji, and Amanda Stentthe 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLTNew Orleans, Louisiana, USAAssociation for Computational Linguistics2Short PapersDai Quoc Nguyen, Tu Dinh Nguyen, Dat Quoc Nguyen, and Dinh Q. Phung. 2018. A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short Papers), Marilyn A. Walker, Heng Ji, and Amanda Stent (Eds.). Association for Computational Linguistics, 327-333. https://doi.org/10.18653/ v1/n18-2053\n\nA Capsule Network-based Embedding Model for Knowledge Graph Completion and Search Personalization. Thanh Dai Quoc Nguyen, Tu Dinh Vu, Dat Nguyen, Dinh Q Quoc Nguyen, Phung, 10.18653/v1/n19-1226Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Jill Burstein, Christy Doran, and Thamar Soloriothe 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational Linguistics1Dai Quoc Nguyen, Thanh Vu, Tu Dinh Nguyen, Dat Quoc Nguyen, and Dinh Q. Phung. 2019. A Capsule Network-based Embedding Model for Knowledge Graph Completion and Search Personalization. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, 2180-2189. https://doi.org/10.18653/v1/n19-1226\n\nGlove: Global Vectors for Word Representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, 10.3115/v1/d14-1162Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. the 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, QatarA meeting of SIGDAT, a Special Interest Group of the ACL. ACLJeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL. ACL, 1532-1543. https://doi.org/10.3115/v1/d14-1162\n\nDeep Contextualized Word Representations. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, 10.18653/v1/n18-1202Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018. Marilyn A. Walker, Heng Ji, and Amanda Stentthe 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018New Orleans, Louisiana, USAAssociation for Computational Linguistics1Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Rep- resentations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), Marilyn A. Walker, Heng Ji, and Amanda Stent (Eds.). Association for Computational Linguistics, 2227-2237. https://doi.org/10.18653/v1/n18-1202\n\nSentence-BERT: Sentence Embeddings using Siamese BERT-Networks. Nils Reimers, Iryna Gurevych, 10.18653/v1/D19-1410Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wanthe 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational LinguisticsNils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embed- dings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, 3980-3990. https://doi.org/10.18653/v1/D19-1410\n\nModeling Relational Data with Graph Convolutional Networks. Michael Sejr, Thomas N Schlichtkrull, Peter Kipf, Rianne Bloem, Van Den, Ivan Berg, Max Titov, Welling, 10.1007/978-3-319-93417-4_38The Semantic Web -15th International Conference. Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler, Rapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish AlamHeraklion, Crete, GreeceSpringer10843Proceedings (Lecture Notes in Computer ScienceMichael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2018. Modeling Relational Data with Graph Convo- lutional Networks. In The Semantic Web -15th International Conference, ESWC 2018, Heraklion, Crete, Greece, June 3-7, 2018, Proceedings (Lecture Notes in Com- puter Science, Vol. 10843), Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler, Rapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam (Eds.). Springer, 593-607. https://doi.org/10.1007/978-3-319-93417-4_38\n\nOpen-World Knowledge Graph Completion. Baoxu Shi, Tim Weninger, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). Sheila A. McIlraith and Kilian Q. Weinbergerthe Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)New Orleans, Louisiana, USAAAAI PressBaoxu Shi and Tim Weninger. 2018. Open-World Knowledge Graph Completion. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, Sheila A. McIlraith and Kilian Q. Weinberger (Eds.). AAAI Press, 1957-1964. https://www.aaai.org/ ocs/index.php/AAAI/AAAI18/paper/view/16055\n\nReasoning With Neural Tensor Networks for Knowledge Base Completion. Richard Socher, Danqi Chen, Christopher D Manning, Andrew Y Ng, Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems. Christopher J. C. Burges, L\u00e9on Bottou, Zoubin Ghahramani, and Kilian Q. WeinbergerLake Tahoe, Nevada, United StatesProceedings of a meeting heldRichard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. 2013. Reasoning With Neural Tensor Networks for Knowledge Base Completion. In Advances in Neural Information Processing Systems 26: 27th Annual Confer- ence on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, Christopher J. C. Burges, L\u00e9on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger (Eds.). 926- 934. http://papers.nips.cc/paper/5028-reasoning-with-neural-tensor-networks- for-knowledge-base-completion\n\nYago: a core of semantic knowledge. Fabian M Suchanek, Gjergji Kasneci, Gerhard Weikum, Proceedings of the 16th International Conference on World Wide Web. the 16th International Conference on World Wide WebBanff, Alberta, CanadaCarey LFabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In Proceedings of the 16th International Conference on World Wide Web, WWW 2007, Banff, Alberta, Canada, May 8-12, 2007, Carey L.\n\n. Mary Ellen Williamson, Peter F Zurko, Patel-Schneider, 10.1145/1242572.1242667Prashant J. ShenoyACMWilliamson, Mary Ellen Zurko, Peter F. Patel-Schneider, and Prashant J. Shenoy (Eds.). ACM, 697-706. https://doi.org/10.1145/1242572.1242667\n\nRotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space. Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, Jian Tang, 7th International Conference on Learning Representations. New Orleans, LA, USAZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019. RotatE: Knowl- edge Graph Embedding by Relational Rotation in Complex Space. In 7th Interna- tional Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=HkgEQnRqYQ\n\nA Re-evaluation of Knowledge Graph Completion Methods. Zhiqing Sun, Shikhar Vashishth, Soumya Sanyal, Partha P Talukdar, Yiming Yang, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreaultthe 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational Linguistics2020Zhiqing Sun, Shikhar Vashishth, Soumya Sanyal, Partha P. Talukdar, and Yiming Yang. 2020. A Re-evaluation of Knowledge Graph Completion Methods. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.). Association for Computational Linguistics, 5516-5522. https://www.aclweb.org/anthology/2020.acl-main.489/\n\nRepresenting Text for Joint Embedding of Text and Knowledge Bases. Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, Michael Gamon, 10.18653/v1/d15-1174Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Llu\u00eds M\u00e0rquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Martonthe 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalThe Association for Computational LinguisticsKristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choud- hury, and Michael Gamon. 2015. Representing Text for Joint Embedding of Text and Knowledge Bases. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, Llu\u00eds M\u00e0rquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton (Eds.). The Association for Computational Linguistics, 1499-1509. https://doi.org/10.18653/v1/d15-1174\n\nComplex Embeddings for Simple Link Prediction. Th\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, Guillaume Bouchard, Proceedings of the 33nd International Conference on Machine Learning. the 33nd International Conference on Machine LearningNew York City, NY, USATh\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, and Guillaume Bouchard. 2016. Complex Embeddings for Simple Link Prediction. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016 (JMLR Workshop and Conference Proceedings,\n\nJMLR.org. Maria-Florina Balcan and Kilian Q. Weinberger48Vol. 48), Maria-Florina Balcan and Kilian Q. Weinberger (Eds.). JMLR.org, 2071- 2080. http://proceedings.mlr.press/v48/trouillon16.html\n\nAttention is All you Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman GarnettLong Beach, CA, USA, Isabelle GuyonAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30: Annual Con- ference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 5998-6008. http://papers.nips.cc/paper/7181-attention-is-all-you-need\n\nWikidata: a free collaborative knowledgebase. Denny Vrandecic, Markus Kr\u00f6tzsch, 10.1145/2629489Commun. ACM. 57Denny Vrandecic and Markus Kr\u00f6tzsch. 2014. Wikidata: a free collaborative knowledgebase. Commun. ACM 57, 10 (2014), 78-85. https://doi.org/10.1145/ 2629489\n\nKnowledge Graph and Text Jointly Embedding. Zhen Wang, Jianwen Zhang, Jianlin Feng, Zheng Chen, 10.3115/v1/d14-1167Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Alessandro Moschitti, Bo Pang, and Walter Daelemansthe 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, QatarA meeting of SIGDAT, a Special Interest Group of the ACLZhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge Graph and Text Jointly Embedding. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, Alessandro Moschitti, Bo Pang, and Walter Daelemans (Eds.). ACL, 1591-1601. https://doi.org/10.3115/v1/d14-1167\n\nSSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions. Han Xiao, Minlie Huang, Lian Meng, Xiaoyan Zhu, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence. Satinder P. Singh and Shaul Markovitchthe Thirty-First AAAI Conference on Artificial IntelligenceSan Francisco, California, USAAAAI PressHan Xiao, Minlie Huang, Lian Meng, and Xiaoyan Zhu. 2017. SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA, Satinder P. Singh and Shaul Markovitch (Eds.). AAAI Press, 3104-3110. http://aaai.org/ocs/index.php/AAAI/AAAI17/ paper/view/14306\n\nRepresentation Learning of Knowledge Graphs with Entity Descriptions. Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, Maosong Sun, Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. Michael P. Wellmanthe Thirtieth AAAI Conference on Artificial IntelligencePhoenix, Arizona, USAAAAI PressRuobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and Maosong Sun. 2016. Repre- sentation Learning of Knowledge Graphs with Entity Descriptions. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA, Dale Schuurmans and Michael P. Wellman (Eds.). AAAI Press, 2659-2665. http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/ view/12216\n\nExplicit Semantic Ranking for Academic Search via Knowledge Graph Embedding. Chenyan Xiong, Russell Power, Jamie Callan, 10.1145/3038912.3052558Proceedings of the 26th International Conference on World Wide Web. Rick Barrett, Rick Cummings, Eugene Agichtein, and Evgeniy Gabrilovichthe 26th International Conference on World Wide WebPerth, AustraliaACMChenyan Xiong, Russell Power, and Jamie Callan. 2017. Explicit Semantic Ranking for Academic Search via Knowledge Graph Embedding. In Proceedings of the 26th International Conference on World Wide Web, WWW 2017, Perth, Australia, April 3-7, 2017, Rick Barrett, Rick Cummings, Eugene Agichtein, and Evgeniy Gabrilovich (Eds.). ACM, 1271-1279. https://doi.org/10.1145/3038912.3052558\n\nOne-Shot Relational Learning for Knowledge Graphs. Wenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo, William Yang Wang, 10.18653/v1/d18-1223Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujiithe 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsWenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo, and William Yang Wang. 2018. One-Shot Relational Learning for Knowledge Graphs. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 -November 4, 2018, Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii (Eds.). Association for Computational Linguistics, 1980-1990. https://doi.org/10.18653/v1/d18-1223\n\nKnowledge Graph Representation with Jointly Structural and Textual Encoding. Jiacheng Xu, Xipeng Qiu, Kan Chen, Xuanjing Huang, 10.24963/ijcai.2017/183Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence. Carles Sierrathe Twenty-Sixth International Joint Conference on Artificial IntelligenceMelbourne, Australiaijcai.orgJiacheng Xu, Xipeng Qiu, Kan Chen, and Xuanjing Huang. 2017. Knowledge Graph Representation with Jointly Structural and Textual Encoding. In Proceed- ings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017, Carles Sierra (Ed.). ijcai.org, 1318-1324. https://doi.org/10.24963/ijcai.2017/183\n\nJoint Learning of the Embedding of Words and Entities for Named Entity Disambiguation. Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji, 10.18653/v1/k16-1025Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning. Goldberg and Stefan Riezlerthe 20th SIGNLL Conference on Computational Natural Language LearningCoNLL; Berlin, GermanyACLIkuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and Yoshiyasu Takefuji. 2016. Joint Learning of the Embedding of Words and Entities for Named Entity Disam- biguation. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016, Berlin, Germany, August 11-12, 2016, Yoav Gold- berg and Stefan Riezler (Eds.). ACL, 250-259. https://doi.org/10.18653/v1/k16- 1025\n\nEmbedding Entities and Relations for Learning and Inference in Knowledge Bases. Bishan Yang, Wen-Tau Yih, Xiaodong He, Jianfeng Gao, Li Deng, 3rd International Conference on Learning Representations. Bengio and Yann LeCunSan Diego, CA, USAConference Track ProceedingsBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Em- bedding Entities and Relations for Learning and Inference in Knowledge Bases. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1412.6575\n\nKG-BERT: BERT for Knowledge Graph Completion. Liang Yao, Chengsheng Mao, Yuan Luo, arXiv:1909.03193Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. KG-BERT: BERT for Knowledge Graph Completion. CoRR abs/1909.03193 (2019). arXiv:1909.03193 http://arxiv. org/abs/1909.03193\n\nCollaborative Knowledge Base Embedding for Recommender Systems. Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, Wei-Ying Ma, 10.1145/2939672.2939673Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Balaji Krishnapuram, Mohak Shah, Alexander J. Smola, Charu C. Aggarwal, Dou Shen, and Rajeev Rastogithe 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningSan Francisco, CA, USAACMFuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma. 2016. Collaborative Knowledge Base Embedding for Recommender Systems. In Proceed- ings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, Balaji Krishnapuram, Mohak Shah, Alexander J. Smola, Charu C. Aggarwal, Dou Shen, and Rajeev Rastogi (Eds.). ACM, 353-362. https://doi.org/10.1145/2939672.2939673\n\nQuaternion Knowledge Graph Embeddings. Shuai Zhang, Yi Tay, Lina Yao, Qi Liu, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman GarnettVancouver, BC, Canada, Hanna M. Wallach, Hugo LarochelleShuai Zhang, Yi Tay, Lina Yao, and Qi Liu. 2019. Quaternion Knowledge Graph Embeddings. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett (Eds.). 2731-2741. http://papers.nips.cc/paper/8541-quaternion-knowledge- graph-embeddings\n", "annotations": {"author": "[{\"end\":232,\"start\":145},{\"end\":346,\"start\":233},{\"end\":460,\"start\":347},{\"end\":578,\"start\":461},{\"end\":675,\"start\":579},{\"end\":815,\"start\":676},{\"end\":824,\"start\":816},{\"end\":834,\"start\":825},{\"end\":848,\"start\":835},{\"end\":861,\"start\":849},{\"end\":872,\"start\":862},{\"end\":882,\"start\":873}]", "publisher": "[{\"end\":94,\"start\":91},{\"end\":1100,\"start\":1097}]", "author_last_name": "[{\"end\":152,\"start\":148},{\"end\":241,\"start\":237},{\"end\":359,\"start\":355},{\"end\":472,\"start\":468},{\"end\":588,\"start\":584},{\"end\":684,\"start\":679},{\"end\":823,\"start\":819},{\"end\":833,\"start\":829},{\"end\":847,\"start\":843},{\"end\":860,\"start\":856},{\"end\":871,\"start\":867}]", "author_first_name": "[{\"end\":147,\"start\":145},{\"end\":236,\"start\":233},{\"end\":354,\"start\":347},{\"end\":467,\"start\":461},{\"end\":583,\"start\":579},{\"end\":678,\"start\":676},{\"end\":818,\"start\":816},{\"end\":828,\"start\":825},{\"end\":842,\"start\":835},{\"end\":855,\"start\":849},{\"end\":866,\"start\":862},{\"end\":875,\"start\":873},{\"end\":881,\"start\":876}]", "author_affiliation": "[{\"end\":231,\"start\":180},{\"end\":345,\"start\":271},{\"end\":459,\"start\":385},{\"end\":577,\"start\":490},{\"end\":674,\"start\":614},{\"end\":756,\"start\":705},{\"end\":814,\"start\":758}]", "title": "[{\"end\":90,\"start\":1},{\"end\":972,\"start\":883}]", "venue": "[{\"end\":1047,\"start\":974}]", "abstract": "[{\"end\":3728,\"start\":1298}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4193,\"start\":4189},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4221,\"start\":4217},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":4249,\"start\":4245},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4335,\"start\":4331},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4696,\"start\":4693},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4699,\"start\":4696},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4702,\"start\":4699},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4837,\"start\":4833},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5462,\"start\":5459},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5478,\"start\":5474},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":5752,\"start\":5748},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":5767,\"start\":5763},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6222,\"start\":6219},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6512,\"start\":6508},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6769,\"start\":6765},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6772,\"start\":6769},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6795,\"start\":6791},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6798,\"start\":6795},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7110,\"start\":7107},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7248,\"start\":7244},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8757,\"start\":8753},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8943,\"start\":8939},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9166,\"start\":9163},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9169,\"start\":9166},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10961,\"start\":10957},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10987,\"start\":10983},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":11322,\"start\":11318},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12243,\"start\":12239},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12260,\"start\":12256},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12465,\"start\":12461},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12754,\"start\":12750},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13229,\"start\":13225},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":13283,\"start\":13279},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13458,\"start\":13454},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13461,\"start\":13458},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":13731,\"start\":13727},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14005,\"start\":14001},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14164,\"start\":14160},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15849,\"start\":15845},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15852,\"start\":15849},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15917,\"start\":15913},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15963,\"start\":15960},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16145,\"start\":16141},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16650,\"start\":16647},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16666,\"start\":16662},{\"end\":17305,\"start\":17300},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17349,\"start\":17345},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":17622,\"start\":17618},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17688,\"start\":17684},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18394,\"start\":18390},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18590,\"start\":18586},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18607,\"start\":18603},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19705,\"start\":19701},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":19708,\"start\":19705},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20229,\"start\":20226},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20232,\"start\":20229},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20235,\"start\":20232},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21361,\"start\":21358},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":21377,\"start\":21373},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23518,\"start\":23514},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":23521,\"start\":23518},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25187,\"start\":25183},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":26479,\"start\":26475},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":26482,\"start\":26479},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":28321,\"start\":28317},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29275,\"start\":29272},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":30259,\"start\":30255},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":30262,\"start\":30259},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":30625,\"start\":30621},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30628,\"start\":30625},{\"end\":31027,\"start\":31004},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":31363,\"start\":31359},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":31366,\"start\":31363},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":31369,\"start\":31366},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":31476,\"start\":31472},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32550,\"start\":32546},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":32597,\"start\":32593},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":32682,\"start\":32678},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":32710,\"start\":32707},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":32835,\"start\":32832},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":32937,\"start\":32936},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32958,\"start\":32954},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":33087,\"start\":33083},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33147,\"start\":33144},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":33259,\"start\":33256},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":33327,\"start\":33323},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":33330,\"start\":33327},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":33456,\"start\":33452},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":33911,\"start\":33908},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":34357,\"start\":34353},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":34681,\"start\":34677},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":35065,\"start\":35061},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":35110,\"start\":35106},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":35203,\"start\":35199},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":38044,\"start\":38040},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":39011,\"start\":39007},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":44792,\"start\":44789},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":44795,\"start\":44792},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":44946,\"start\":44943},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":45242,\"start\":45238},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":45464,\"start\":45461},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":45467,\"start\":45464},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":45470,\"start\":45467},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":45604,\"start\":45600},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":46061,\"start\":46057},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":46064,\"start\":46061},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":46107,\"start\":46103},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":46110,\"start\":46107},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":46207,\"start\":46203},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":46363,\"start\":46359},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":46615,\"start\":46611},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":47186,\"start\":47182},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":47317,\"start\":47313},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":47672,\"start\":47669},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":47939,\"start\":47935},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":49278,\"start\":49274},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":49301,\"start\":49297},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":49515,\"start\":49511},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":50173,\"start\":50169},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":50762,\"start\":50759},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":53329,\"start\":53325},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":53342,\"start\":53339},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":57089,\"start\":57085},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":57092,\"start\":57089},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":57269,\"start\":57265}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":51151,\"start\":51088},{\"attributes\":{\"id\":\"fig_1\"},\"end\":51293,\"start\":51152},{\"attributes\":{\"id\":\"fig_2\"},\"end\":51709,\"start\":51294},{\"attributes\":{\"id\":\"fig_3\"},\"end\":52410,\"start\":51710},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":52669,\"start\":52411},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":53209,\"start\":52670},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":53666,\"start\":53210},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":54224,\"start\":53667},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":55331,\"start\":54225},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":56197,\"start\":55332},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":56351,\"start\":56198},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":56966,\"start\":56352}]", "paragraph": "[{\"end\":4666,\"start\":3744},{\"end\":5052,\"start\":4668},{\"end\":5989,\"start\":5054},{\"end\":6461,\"start\":5991},{\"end\":7249,\"start\":6463},{\"end\":9236,\"start\":7251},{\"end\":10346,\"start\":9238},{\"end\":11044,\"start\":10348},{\"end\":11323,\"start\":11059},{\"end\":12011,\"start\":11325},{\"end\":12672,\"start\":12013},{\"end\":12814,\"start\":12709},{\"end\":12916,\"start\":12881},{\"end\":13230,\"start\":12959},{\"end\":13862,\"start\":13232},{\"end\":14132,\"start\":13947},{\"end\":14747,\"start\":14134},{\"end\":15416,\"start\":14769},{\"end\":15771,\"start\":15452},{\"end\":16374,\"start\":15773},{\"end\":16516,\"start\":16376},{\"end\":16887,\"start\":16518},{\"end\":17219,\"start\":16889},{\"end\":17475,\"start\":17280},{\"end\":17984,\"start\":17514},{\"end\":18129,\"start\":17986},{\"end\":18137,\"start\":18131},{\"end\":18865,\"start\":18181},{\"end\":19391,\"start\":18867},{\"end\":19563,\"start\":19430},{\"end\":20134,\"start\":19606},{\"end\":20371,\"start\":20136},{\"end\":20707,\"start\":20401},{\"end\":20937,\"start\":20756},{\"end\":21022,\"start\":20952},{\"end\":21489,\"start\":21044},{\"end\":21631,\"start\":21491},{\"end\":21895,\"start\":21633},{\"end\":22082,\"start\":21949},{\"end\":22534,\"start\":22109},{\"end\":22797,\"start\":22636},{\"end\":23036,\"start\":22799},{\"end\":24255,\"start\":23114},{\"end\":24804,\"start\":24257},{\"end\":25075,\"start\":24879},{\"end\":25206,\"start\":25077},{\"end\":25474,\"start\":25229},{\"end\":25599,\"start\":25496},{\"end\":26236,\"start\":25601},{\"end\":26279,\"start\":26260},{\"end\":26291,\"start\":26281},{\"end\":26629,\"start\":26435},{\"end\":27019,\"start\":26631},{\"end\":27763,\"start\":27053},{\"end\":28062,\"start\":27765},{\"end\":28203,\"start\":28064},{\"end\":29303,\"start\":28249},{\"end\":29440,\"start\":29374},{\"end\":29921,\"start\":29496},{\"end\":30213,\"start\":29963},{\"end\":30937,\"start\":30215},{\"end\":31906,\"start\":30939},{\"end\":32373,\"start\":31921},{\"end\":33619,\"start\":32375},{\"end\":34282,\"start\":33621},{\"end\":34784,\"start\":34284},{\"end\":35360,\"start\":34819},{\"end\":36395,\"start\":35371},{\"end\":36880,\"start\":36432},{\"end\":37689,\"start\":36924},{\"end\":38490,\"start\":37691},{\"end\":38760,\"start\":38492},{\"end\":39975,\"start\":38779},{\"end\":40253,\"start\":39996},{\"end\":40691,\"start\":40255},{\"end\":41657,\"start\":40693},{\"end\":42584,\"start\":41659},{\"end\":42627,\"start\":42586},{\"end\":42676,\"start\":42629},{\"end\":43486,\"start\":42678},{\"end\":44515,\"start\":43488},{\"end\":45853,\"start\":44532},{\"end\":46840,\"start\":45855},{\"end\":48025,\"start\":46842},{\"end\":48808,\"start\":48040},{\"end\":49903,\"start\":48830},{\"end\":51087,\"start\":49923}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12708,\"start\":12673},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12880,\"start\":12815},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12958,\"start\":12917},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13946,\"start\":13863},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17279,\"start\":17220},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17513,\"start\":17476},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18180,\"start\":18138},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20400,\"start\":20372},{\"attributes\":{\"id\":\"formula_10\"},\"end\":20755,\"start\":20708},{\"attributes\":{\"id\":\"formula_11\"},\"end\":20951,\"start\":20938},{\"attributes\":{\"id\":\"formula_12\"},\"end\":21948,\"start\":21896},{\"attributes\":{\"id\":\"formula_13\"},\"end\":22635,\"start\":22535},{\"attributes\":{\"id\":\"formula_14\"},\"end\":23113,\"start\":23037},{\"attributes\":{\"id\":\"formula_15\"},\"end\":24878,\"start\":24805},{\"attributes\":{\"id\":\"formula_16\"},\"end\":25228,\"start\":25207},{\"attributes\":{\"id\":\"formula_17\"},\"end\":26434,\"start\":26292},{\"attributes\":{\"id\":\"formula_18\"},\"end\":28248,\"start\":28204},{\"attributes\":{\"id\":\"formula_19\"},\"end\":29373,\"start\":29304},{\"attributes\":{\"id\":\"formula_20\"},\"end\":29495,\"start\":29441}]", "table_ref": "[{\"end\":25997,\"start\":25990},{\"end\":26763,\"start\":26755},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":32530,\"start\":32523},{\"end\":34926,\"start\":34919},{\"end\":34954,\"start\":34947},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":36622,\"start\":36615},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":37585,\"start\":37578},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":38127,\"start\":38120},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":38648,\"start\":38641},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":38927,\"start\":38920},{\"end\":41746,\"start\":41739},{\"end\":42482,\"start\":42475},{\"end\":43040,\"start\":43033},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":43401,\"start\":43393},{\"end\":43570,\"start\":43563},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":43945,\"start\":43938},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":49424,\"start\":49416}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3742,\"start\":3730},{\"attributes\":{\"n\":\"2\"},\"end\":11057,\"start\":11047},{\"attributes\":{\"n\":\"3\"},\"end\":14767,\"start\":14750},{\"attributes\":{\"n\":\"3.1\"},\"end\":15450,\"start\":15419},{\"attributes\":{\"n\":\"3.2\"},\"end\":19428,\"start\":19394},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":19604,\"start\":19566},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":21042,\"start\":21025},{\"attributes\":{\"n\":\"3.3\"},\"end\":22107,\"start\":22085},{\"attributes\":{\"n\":\"3.3.2\"},\"end\":25494,\"start\":25477},{\"end\":26258,\"start\":26239},{\"attributes\":{\"n\":\"3.4\"},\"end\":27051,\"start\":27022},{\"attributes\":{\"n\":\"3.5\"},\"end\":29961,\"start\":29924},{\"attributes\":{\"n\":\"4\"},\"end\":31919,\"start\":31909},{\"attributes\":{\"n\":\"4.1\"},\"end\":34817,\"start\":34787},{\"end\":35369,\"start\":35363},{\"attributes\":{\"n\":\"4.2\"},\"end\":36430,\"start\":36398},{\"attributes\":{\"n\":\"4.3\"},\"end\":36922,\"start\":36883},{\"attributes\":{\"n\":\"4.4\"},\"end\":38777,\"start\":38763},{\"attributes\":{\"n\":\"4.5\"},\"end\":39994,\"start\":39978},{\"attributes\":{\"n\":\"5\"},\"end\":44530,\"start\":44518},{\"attributes\":{\"n\":\"6\"},\"end\":48038,\"start\":48028},{\"end\":48828,\"start\":48811},{\"end\":49921,\"start\":49906},{\"end\":51099,\"start\":51089},{\"end\":51163,\"start\":51153},{\"end\":51305,\"start\":51295},{\"end\":51721,\"start\":51711},{\"end\":52421,\"start\":52412},{\"end\":52680,\"start\":52671},{\"end\":53220,\"start\":53211},{\"end\":53677,\"start\":53668},{\"end\":54235,\"start\":54226},{\"end\":55342,\"start\":55333},{\"end\":56209,\"start\":56199},{\"end\":56363,\"start\":56353}]", "table": "[{\"end\":52669,\"start\":52464},{\"end\":53209,\"start\":52863},{\"end\":53666,\"start\":53343},{\"end\":54224,\"start\":53730},{\"end\":55331,\"start\":54491},{\"end\":56197,\"start\":55613},{\"end\":56351,\"start\":56248},{\"end\":56966,\"start\":56661}]", "figure_caption": "[{\"end\":51151,\"start\":51101},{\"end\":51293,\"start\":51165},{\"end\":51709,\"start\":51307},{\"end\":52410,\"start\":51723},{\"end\":52464,\"start\":52423},{\"end\":52863,\"start\":52682},{\"end\":53343,\"start\":53222},{\"end\":53730,\"start\":53679},{\"end\":54491,\"start\":54237},{\"end\":55613,\"start\":55344},{\"end\":56248,\"start\":56212},{\"end\":56661,\"start\":56366}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7355,\"start\":7347},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7727,\"start\":7719},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9780,\"start\":9772},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10239,\"start\":10231},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15038,\"start\":15030},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19513,\"start\":19505},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":40362,\"start\":40354},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":40990,\"start\":40982},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":41241,\"start\":41233}]", "bib_author_first_name": "[{\"end\":57679,\"start\":57674},{\"end\":57695,\"start\":57691},{\"end\":57710,\"start\":57703},{\"end\":57712,\"start\":57711},{\"end\":58751,\"start\":58747},{\"end\":58753,\"start\":58752},{\"end\":58770,\"start\":58765},{\"end\":58785,\"start\":58778},{\"end\":58799,\"start\":58796},{\"end\":58813,\"start\":58808},{\"end\":59485,\"start\":59478},{\"end\":59501,\"start\":59494},{\"end\":59518,\"start\":59511},{\"end\":59538,\"start\":59533},{\"end\":59553,\"start\":59547},{\"end\":60463,\"start\":60462},{\"end\":60477,\"start\":60472},{\"end\":60497,\"start\":60486},{\"end\":60517,\"start\":60506},{\"end\":60519,\"start\":60518},{\"end\":61232,\"start\":61227},{\"end\":61245,\"start\":61238},{\"end\":61250,\"start\":61246},{\"end\":62214,\"start\":62208},{\"end\":62230,\"start\":62224},{\"end\":62248,\"start\":62243},{\"end\":62261,\"start\":62257},{\"end\":62278,\"start\":62271},{\"end\":62280,\"start\":62279},{\"end\":62296,\"start\":62293},{\"end\":62298,\"start\":62297},{\"end\":63001,\"start\":62997},{\"end\":63013,\"start\":63009},{\"end\":63028,\"start\":63020},{\"end\":63043,\"start\":63035},{\"end\":63056,\"start\":63050},{\"end\":63074,\"start\":63063},{\"end\":63896,\"start\":63888},{\"end\":63906,\"start\":63903},{\"end\":63917,\"start\":63914},{\"end\":63930,\"start\":63925},{\"end\":63943,\"start\":63937},{\"end\":65042,\"start\":65037},{\"end\":65055,\"start\":65051},{\"end\":65069,\"start\":65065},{\"end\":66379,\"start\":66376},{\"end\":66398,\"start\":66390},{\"end\":66416,\"start\":66410},{\"end\":66437,\"start\":66428},{\"end\":67563,\"start\":67558},{\"end\":67580,\"start\":67572},{\"end\":67594,\"start\":67588},{\"end\":67608,\"start\":67600},{\"end\":68707,\"start\":68704},{\"end\":68725,\"start\":68719},{\"end\":68740,\"start\":68731},{\"end\":68757,\"start\":68750},{\"end\":69059,\"start\":69052},{\"end\":69061,\"start\":69060},{\"end\":69078,\"start\":69072},{\"end\":69089,\"start\":69085},{\"end\":69942,\"start\":69940},{\"end\":69953,\"start\":69947},{\"end\":69974,\"start\":69968},{\"end\":69986,\"start\":69981},{\"end\":70794,\"start\":70793},{\"end\":70806,\"start\":70803},{\"end\":71301,\"start\":71295},{\"end\":71311,\"start\":71307},{\"end\":71322,\"start\":71317},{\"end\":71337,\"start\":71330},{\"end\":71348,\"start\":71342},{\"end\":71361,\"start\":71356},{\"end\":71372,\"start\":71368},{\"end\":71383,\"start\":71379},{\"end\":71395,\"start\":71391},{\"end\":71416,\"start\":71409},{\"end\":71812,\"start\":71808},{\"end\":71826,\"start\":71818},{\"end\":71835,\"start\":71832},{\"end\":71849,\"start\":71841},{\"end\":72178,\"start\":72173},{\"end\":72192,\"start\":72188},{\"end\":72207,\"start\":72204},{\"end\":72221,\"start\":72214},{\"end\":72223,\"start\":72222},{\"end\":72240,\"start\":72233},{\"end\":72971,\"start\":72970},{\"end\":73160,\"start\":73154},{\"end\":73175,\"start\":73170},{\"end\":73190,\"start\":73185},{\"end\":73206,\"start\":73199},{\"end\":74084,\"start\":74082},{\"end\":74089,\"start\":74085},{\"end\":74110,\"start\":74107},{\"end\":74123,\"start\":74119},{\"end\":74125,\"start\":74124},{\"end\":75228,\"start\":75223},{\"end\":75248,\"start\":75246},{\"end\":75253,\"start\":75249},{\"end\":75261,\"start\":75258},{\"end\":75274,\"start\":75270},{\"end\":75276,\"start\":75275},{\"end\":76352,\"start\":76345},{\"end\":76372,\"start\":76365},{\"end\":76392,\"start\":76381},{\"end\":76394,\"start\":76393},{\"end\":77068,\"start\":77061},{\"end\":77070,\"start\":77069},{\"end\":77083,\"start\":77079},{\"end\":77098,\"start\":77093},{\"end\":77110,\"start\":77106},{\"end\":77131,\"start\":77120},{\"end\":77145,\"start\":77139},{\"end\":77155,\"start\":77151},{\"end\":78213,\"start\":78209},{\"end\":78228,\"start\":78223},{\"end\":79237,\"start\":79231},{\"end\":79239,\"start\":79238},{\"end\":79260,\"start\":79255},{\"end\":79273,\"start\":79267},{\"end\":79294,\"start\":79290},{\"end\":79304,\"start\":79301},{\"end\":80196,\"start\":80191},{\"end\":80205,\"start\":80202},{\"end\":81367,\"start\":81360},{\"end\":81381,\"start\":81376},{\"end\":81399,\"start\":81388},{\"end\":81401,\"start\":81400},{\"end\":81417,\"start\":81411},{\"end\":81419,\"start\":81418},{\"end\":82292,\"start\":82286},{\"end\":82294,\"start\":82293},{\"end\":82312,\"start\":82305},{\"end\":82329,\"start\":82322},{\"end\":82722,\"start\":82718},{\"end\":82728,\"start\":82723},{\"end\":82746,\"start\":82741},{\"end\":82748,\"start\":82747},{\"end\":83041,\"start\":83034},{\"end\":83055,\"start\":83047},{\"end\":83070,\"start\":83062},{\"end\":83080,\"start\":83076},{\"end\":83538,\"start\":83531},{\"end\":83551,\"start\":83544},{\"end\":83569,\"start\":83563},{\"end\":83584,\"start\":83578},{\"end\":83586,\"start\":83585},{\"end\":83603,\"start\":83597},{\"end\":84415,\"start\":84407},{\"end\":84432,\"start\":84427},{\"end\":84446,\"start\":84439},{\"end\":84462,\"start\":84455},{\"end\":84476,\"start\":84469},{\"end\":84495,\"start\":84488},{\"end\":85374,\"start\":85370},{\"end\":85394,\"start\":85386},{\"end\":85411,\"start\":85402},{\"end\":85424,\"start\":85420},{\"end\":85444,\"start\":85435},{\"end\":86138,\"start\":86132},{\"end\":86152,\"start\":86148},{\"end\":86166,\"start\":86162},{\"end\":86180,\"start\":86175},{\"end\":86197,\"start\":86192},{\"end\":86210,\"start\":86205},{\"end\":86212,\"start\":86211},{\"end\":86226,\"start\":86220},{\"end\":86240,\"start\":86235},{\"end\":87067,\"start\":87062},{\"end\":87085,\"start\":87079},{\"end\":87331,\"start\":87327},{\"end\":87345,\"start\":87338},{\"end\":87360,\"start\":87353},{\"end\":87372,\"start\":87367},{\"end\":88177,\"start\":88174},{\"end\":88190,\"start\":88184},{\"end\":88202,\"start\":88198},{\"end\":88216,\"start\":88209},{\"end\":88917,\"start\":88910},{\"end\":88930,\"start\":88923},{\"end\":88939,\"start\":88936},{\"end\":88951,\"start\":88945},{\"end\":88965,\"start\":88958},{\"end\":89632,\"start\":89625},{\"end\":89647,\"start\":89640},{\"end\":89660,\"start\":89655},{\"end\":90340,\"start\":90334},{\"end\":90350,\"start\":90348},{\"end\":90360,\"start\":90355},{\"end\":90376,\"start\":90368},{\"end\":90389,\"start\":90382},{\"end\":90394,\"start\":90390},{\"end\":91224,\"start\":91216},{\"end\":91235,\"start\":91229},{\"end\":91244,\"start\":91241},{\"end\":91259,\"start\":91251},{\"end\":91961,\"start\":91956},{\"end\":91978,\"start\":91970},{\"end\":91994,\"start\":91987},{\"end\":92012,\"start\":92003},{\"end\":92736,\"start\":92730},{\"end\":92750,\"start\":92743},{\"end\":92764,\"start\":92756},{\"end\":92777,\"start\":92769},{\"end\":92785,\"start\":92783},{\"end\":93328,\"start\":93323},{\"end\":93344,\"start\":93334},{\"end\":93354,\"start\":93350},{\"end\":93619,\"start\":93612},{\"end\":93635,\"start\":93627},{\"end\":93640,\"start\":93636},{\"end\":93651,\"start\":93647},{\"end\":93662,\"start\":93658},{\"end\":93676,\"start\":93668},{\"end\":94512,\"start\":94507},{\"end\":94522,\"start\":94520},{\"end\":94532,\"start\":94528},{\"end\":94540,\"start\":94538}]", "bib_author_last_name": "[{\"end\":57689,\"start\":57680},{\"end\":57701,\"start\":57696},{\"end\":57723,\"start\":57713},{\"end\":58763,\"start\":58754},{\"end\":58776,\"start\":58771},{\"end\":58794,\"start\":58786},{\"end\":58806,\"start\":58800},{\"end\":58820,\"start\":58814},{\"end\":59492,\"start\":59486},{\"end\":59509,\"start\":59502},{\"end\":59531,\"start\":59519},{\"end\":59545,\"start\":59539},{\"end\":59563,\"start\":59554},{\"end\":60470,\"start\":60464},{\"end\":60484,\"start\":60478},{\"end\":60504,\"start\":60498},{\"end\":60525,\"start\":60520},{\"end\":60534,\"start\":60527},{\"end\":61236,\"start\":61233},{\"end\":61255,\"start\":61251},{\"end\":62222,\"start\":62215},{\"end\":62241,\"start\":62231},{\"end\":62255,\"start\":62249},{\"end\":62269,\"start\":62262},{\"end\":62289,\"start\":62281},{\"end\":62307,\"start\":62299},{\"end\":63007,\"start\":63002},{\"end\":63018,\"start\":63014},{\"end\":63033,\"start\":63029},{\"end\":63048,\"start\":63044},{\"end\":63061,\"start\":63057},{\"end\":63077,\"start\":63075},{\"end\":63901,\"start\":63897},{\"end\":63912,\"start\":63907},{\"end\":63923,\"start\":63918},{\"end\":63935,\"start\":63931},{\"end\":63948,\"start\":63944},{\"end\":65049,\"start\":65043},{\"end\":65063,\"start\":65056},{\"end\":65075,\"start\":65070},{\"end\":65648,\"start\":65633},{\"end\":66271,\"start\":66268},{\"end\":66388,\"start\":66380},{\"end\":66408,\"start\":66399},{\"end\":66426,\"start\":66417},{\"end\":66444,\"start\":66438},{\"end\":67570,\"start\":67564},{\"end\":67586,\"start\":67581},{\"end\":67598,\"start\":67595},{\"end\":67618,\"start\":67609},{\"end\":68717,\"start\":68708},{\"end\":68729,\"start\":68726},{\"end\":68748,\"start\":68741},{\"end\":68765,\"start\":68758},{\"end\":69070,\"start\":69062},{\"end\":69083,\"start\":69079},{\"end\":69098,\"start\":69090},{\"end\":69945,\"start\":69943},{\"end\":69966,\"start\":69954},{\"end\":69979,\"start\":69975},{\"end\":69992,\"start\":69987},{\"end\":70801,\"start\":70795},{\"end\":70811,\"start\":70807},{\"end\":70820,\"start\":70813},{\"end\":71305,\"start\":71302},{\"end\":71315,\"start\":71312},{\"end\":71328,\"start\":71323},{\"end\":71340,\"start\":71338},{\"end\":71354,\"start\":71349},{\"end\":71366,\"start\":71362},{\"end\":71377,\"start\":71373},{\"end\":71389,\"start\":71384},{\"end\":71407,\"start\":71396},{\"end\":71425,\"start\":71417},{\"end\":71816,\"start\":71813},{\"end\":71830,\"start\":71827},{\"end\":71839,\"start\":71836},{\"end\":71854,\"start\":71850},{\"end\":72186,\"start\":72179},{\"end\":72202,\"start\":72193},{\"end\":72212,\"start\":72208},{\"end\":72231,\"start\":72224},{\"end\":72245,\"start\":72241},{\"end\":72978,\"start\":72972},{\"end\":72986,\"start\":72980},{\"end\":73168,\"start\":73161},{\"end\":73183,\"start\":73176},{\"end\":73197,\"start\":73191},{\"end\":73211,\"start\":73207},{\"end\":74105,\"start\":74090},{\"end\":74117,\"start\":74111},{\"end\":74137,\"start\":74126},{\"end\":74144,\"start\":74139},{\"end\":75244,\"start\":75229},{\"end\":75256,\"start\":75254},{\"end\":75268,\"start\":75262},{\"end\":75288,\"start\":75277},{\"end\":75295,\"start\":75290},{\"end\":76363,\"start\":76353},{\"end\":76379,\"start\":76373},{\"end\":76402,\"start\":76395},{\"end\":77077,\"start\":77071},{\"end\":77091,\"start\":77084},{\"end\":77104,\"start\":77099},{\"end\":77118,\"start\":77111},{\"end\":77137,\"start\":77132},{\"end\":77149,\"start\":77146},{\"end\":77167,\"start\":77156},{\"end\":78221,\"start\":78214},{\"end\":78237,\"start\":78229},{\"end\":79229,\"start\":79217},{\"end\":79253,\"start\":79240},{\"end\":79265,\"start\":79261},{\"end\":79279,\"start\":79274},{\"end\":79288,\"start\":79281},{\"end\":79299,\"start\":79295},{\"end\":79310,\"start\":79305},{\"end\":79319,\"start\":79312},{\"end\":80200,\"start\":80197},{\"end\":80214,\"start\":80206},{\"end\":81374,\"start\":81368},{\"end\":81386,\"start\":81382},{\"end\":81409,\"start\":81402},{\"end\":81422,\"start\":81420},{\"end\":82303,\"start\":82295},{\"end\":82320,\"start\":82313},{\"end\":82336,\"start\":82330},{\"end\":82739,\"start\":82729},{\"end\":82754,\"start\":82749},{\"end\":82771,\"start\":82756},{\"end\":83045,\"start\":83042},{\"end\":83060,\"start\":83056},{\"end\":83074,\"start\":83071},{\"end\":83085,\"start\":83081},{\"end\":83542,\"start\":83539},{\"end\":83561,\"start\":83552},{\"end\":83576,\"start\":83570},{\"end\":83595,\"start\":83587},{\"end\":83608,\"start\":83604},{\"end\":84425,\"start\":84416},{\"end\":84437,\"start\":84433},{\"end\":84453,\"start\":84447},{\"end\":84467,\"start\":84463},{\"end\":84486,\"start\":84477},{\"end\":84501,\"start\":84496},{\"end\":85384,\"start\":85375},{\"end\":85400,\"start\":85395},{\"end\":85418,\"start\":85412},{\"end\":85433,\"start\":85425},{\"end\":85453,\"start\":85445},{\"end\":86146,\"start\":86139},{\"end\":86160,\"start\":86153},{\"end\":86173,\"start\":86167},{\"end\":86190,\"start\":86181},{\"end\":86203,\"start\":86198},{\"end\":86218,\"start\":86213},{\"end\":86233,\"start\":86227},{\"end\":86251,\"start\":86241},{\"end\":87077,\"start\":87068},{\"end\":87094,\"start\":87086},{\"end\":87336,\"start\":87332},{\"end\":87351,\"start\":87346},{\"end\":87365,\"start\":87361},{\"end\":87377,\"start\":87373},{\"end\":88182,\"start\":88178},{\"end\":88196,\"start\":88191},{\"end\":88207,\"start\":88203},{\"end\":88220,\"start\":88217},{\"end\":88921,\"start\":88918},{\"end\":88934,\"start\":88931},{\"end\":88943,\"start\":88940},{\"end\":88956,\"start\":88952},{\"end\":88969,\"start\":88966},{\"end\":89638,\"start\":89633},{\"end\":89653,\"start\":89648},{\"end\":89667,\"start\":89661},{\"end\":90346,\"start\":90341},{\"end\":90353,\"start\":90351},{\"end\":90366,\"start\":90361},{\"end\":90380,\"start\":90377},{\"end\":90399,\"start\":90395},{\"end\":91227,\"start\":91225},{\"end\":91239,\"start\":91236},{\"end\":91249,\"start\":91245},{\"end\":91265,\"start\":91260},{\"end\":91968,\"start\":91962},{\"end\":91985,\"start\":91979},{\"end\":92001,\"start\":91995},{\"end\":92021,\"start\":92013},{\"end\":92741,\"start\":92737},{\"end\":92754,\"start\":92751},{\"end\":92767,\"start\":92765},{\"end\":92781,\"start\":92778},{\"end\":92790,\"start\":92786},{\"end\":93332,\"start\":93329},{\"end\":93348,\"start\":93345},{\"end\":93358,\"start\":93355},{\"end\":93625,\"start\":93620},{\"end\":93645,\"start\":93641},{\"end\":93656,\"start\":93652},{\"end\":93666,\"start\":93663},{\"end\":93679,\"start\":93677},{\"end\":94518,\"start\":94513},{\"end\":94526,\"start\":94523},{\"end\":94536,\"start\":94533},{\"end\":94544,\"start\":94541}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.18653/v1/D19-1522\",\"id\":\"b0\",\"matched_paper_id\":59316623},\"end\":58661,\"start\":57613},{\"attributes\":{\"doi\":\"10.1145/1376616.1376746\",\"id\":\"b1\",\"matched_paper_id\":207167677},\"end\":59417,\"start\":58663},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14941970},\"end\":60394,\"start\":59419},{\"attributes\":{\"doi\":\"10.18653/v1/d15-1075\",\"id\":\"b3\",\"matched_paper_id\":14604520},\"end\":61165,\"start\":60396},{\"attributes\":{\"doi\":\"10.18653/v1/n18-1133\",\"id\":\"b4\",\"matched_paper_id\":3401524},\"end\":62147,\"start\":61167},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":8423494},\"end\":62940,\"start\":62149},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":218487100},\"end\":63811,\"start\":62942},{\"attributes\":{\"id\":\"b7\"},\"end\":64096,\"start\":63813},{\"attributes\":{\"doi\":\"10.18653/v1/D19-1431\",\"id\":\"b8\"},\"end\":64949,\"start\":64098},{\"attributes\":{\"doi\":\"10.1109/CVPR.2005.202\",\"id\":\"b9\",\"matched_paper_id\":5555257},\"end\":65566,\"start\":64951},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7577640},\"end\":66264,\"start\":65568},{\"attributes\":{\"id\":\"b11\"},\"end\":66329,\"start\":66266},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4328400},\"end\":67474,\"start\":66331},{\"attributes\":{\"doi\":\"10.18653/v1/n19-1423\",\"id\":\"b13\",\"matched_paper_id\":52967399},\"end\":68614,\"start\":67476},{\"attributes\":{\"doi\":\"arXiv:2004.08371\",\"id\":\"b14\"},\"end\":68999,\"start\":68616},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4755450},\"end\":69848,\"start\":69001},{\"attributes\":{\"doi\":\"10.18653/v1/P17-1162\",\"id\":\"b16\",\"matched_paper_id\":3051772},\"end\":70725,\"start\":69850},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3144218},\"end\":71236,\"start\":70727},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b18\"},\"end\":71718,\"start\":71238},{\"attributes\":{\"doi\":\"arXiv:1605.09090\",\"id\":\"b19\"},\"end\":72094,\"start\":71720},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":16447573},\"end\":72927,\"start\":72096},{\"attributes\":{\"id\":\"b21\"},\"end\":73071,\"start\":72929},{\"attributes\":{\"doi\":\"10.18653/v1/p19-1466\",\"id\":\"b22\",\"matched_paper_id\":174797737},\"end\":73987,\"start\":73073},{\"attributes\":{\"doi\":\"10.18653/v1/n18-2053\",\"id\":\"b23\",\"matched_paper_id\":3882054},\"end\":75122,\"start\":73989},{\"attributes\":{\"doi\":\"10.18653/v1/n19-1226\",\"id\":\"b24\",\"matched_paper_id\":51984717},\"end\":76296,\"start\":75124},{\"attributes\":{\"doi\":\"10.3115/v1/d14-1162\",\"id\":\"b25\",\"matched_paper_id\":1957433},\"end\":77017,\"start\":76298},{\"attributes\":{\"doi\":\"10.18653/v1/n18-1202\",\"id\":\"b26\",\"matched_paper_id\":3626819},\"end\":78143,\"start\":77019},{\"attributes\":{\"doi\":\"10.18653/v1/D19-1410\",\"id\":\"b27\",\"matched_paper_id\":201646309},\"end\":79155,\"start\":78145},{\"attributes\":{\"doi\":\"10.1007/978-3-319-93417-4_38\",\"id\":\"b28\",\"matched_paper_id\":5458500},\"end\":80150,\"start\":79157},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":19139252},\"end\":81289,\"start\":80152},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":8429835},\"end\":82248,\"start\":81291},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":207163173},\"end\":82714,\"start\":82250},{\"attributes\":{\"doi\":\"10.1145/1242572.1242667\",\"id\":\"b32\"},\"end\":82957,\"start\":82716},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":67855617},\"end\":83474,\"start\":82959},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":207852450},\"end\":84338,\"start\":83476},{\"attributes\":{\"doi\":\"10.18653/v1/d15-1174\",\"id\":\"b35\",\"matched_paper_id\":2127100},\"end\":85321,\"start\":84340},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":15150247},\"end\":85909,\"start\":85323},{\"attributes\":{\"id\":\"b37\"},\"end\":86103,\"start\":85911},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":13756489},\"end\":87014,\"start\":86105},{\"attributes\":{\"doi\":\"10.1145/2629489\",\"id\":\"b39\",\"matched_paper_id\":14494942},\"end\":87281,\"start\":87016},{\"attributes\":{\"doi\":\"10.3115/v1/d14-1167\",\"id\":\"b40\",\"matched_paper_id\":5241137},\"end\":88087,\"start\":87283},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":3150829},\"end\":88838,\"start\":88089},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":31606602},\"end\":89546,\"start\":88840},{\"attributes\":{\"doi\":\"10.1145/3038912.3052558\",\"id\":\"b43\",\"matched_paper_id\":1644335},\"end\":90281,\"start\":89548},{\"attributes\":{\"doi\":\"10.18653/v1/d18-1223\",\"id\":\"b44\",\"matched_paper_id\":52110037},\"end\":91137,\"start\":90283},{\"attributes\":{\"doi\":\"10.24963/ijcai.2017/183\",\"id\":\"b45\",\"matched_paper_id\":3919235},\"end\":91867,\"start\":91139},{\"attributes\":{\"doi\":\"10.18653/v1/k16-1025\",\"id\":\"b46\",\"matched_paper_id\":5267356},\"end\":92648,\"start\":91869},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":2768038},\"end\":93275,\"start\":92650},{\"attributes\":{\"doi\":\"arXiv:1909.03193\",\"id\":\"b48\"},\"end\":93546,\"start\":93277},{\"attributes\":{\"doi\":\"10.1145/2939672.2939673\",\"id\":\"b49\",\"matched_paper_id\":7062707},\"end\":94466,\"start\":93548},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":202763266},\"end\":95253,\"start\":94468}]", "bib_title": "[{\"end\":57672,\"start\":57613},{\"end\":58745,\"start\":58663},{\"end\":59476,\"start\":59419},{\"end\":60460,\"start\":60396},{\"end\":61225,\"start\":61167},{\"end\":62206,\"start\":62149},{\"end\":62995,\"start\":62942},{\"end\":65035,\"start\":64951},{\"end\":65631,\"start\":65568},{\"end\":66374,\"start\":66331},{\"end\":67556,\"start\":67476},{\"end\":69050,\"start\":69001},{\"end\":69938,\"start\":69850},{\"end\":70791,\"start\":70727},{\"end\":72171,\"start\":72096},{\"end\":73152,\"start\":73073},{\"end\":74080,\"start\":73989},{\"end\":75221,\"start\":75124},{\"end\":76343,\"start\":76298},{\"end\":77059,\"start\":77019},{\"end\":78207,\"start\":78145},{\"end\":79215,\"start\":79157},{\"end\":80189,\"start\":80152},{\"end\":81358,\"start\":81291},{\"end\":82284,\"start\":82250},{\"end\":83032,\"start\":82959},{\"end\":83529,\"start\":83476},{\"end\":84405,\"start\":84340},{\"end\":85368,\"start\":85323},{\"end\":86130,\"start\":86105},{\"end\":87060,\"start\":87016},{\"end\":87325,\"start\":87283},{\"end\":88172,\"start\":88089},{\"end\":88908,\"start\":88840},{\"end\":89623,\"start\":89548},{\"end\":90332,\"start\":90283},{\"end\":91214,\"start\":91139},{\"end\":91954,\"start\":91869},{\"end\":92728,\"start\":92650},{\"end\":93610,\"start\":93548},{\"end\":94505,\"start\":94468}]", "bib_author": "[{\"end\":57691,\"start\":57674},{\"end\":57703,\"start\":57691},{\"end\":57725,\"start\":57703},{\"end\":58765,\"start\":58747},{\"end\":58778,\"start\":58765},{\"end\":58796,\"start\":58778},{\"end\":58808,\"start\":58796},{\"end\":58822,\"start\":58808},{\"end\":59494,\"start\":59478},{\"end\":59511,\"start\":59494},{\"end\":59533,\"start\":59511},{\"end\":59547,\"start\":59533},{\"end\":59565,\"start\":59547},{\"end\":60472,\"start\":60462},{\"end\":60486,\"start\":60472},{\"end\":60506,\"start\":60486},{\"end\":60527,\"start\":60506},{\"end\":60536,\"start\":60527},{\"end\":61238,\"start\":61227},{\"end\":61257,\"start\":61238},{\"end\":62224,\"start\":62208},{\"end\":62243,\"start\":62224},{\"end\":62257,\"start\":62243},{\"end\":62271,\"start\":62257},{\"end\":62293,\"start\":62271},{\"end\":62309,\"start\":62293},{\"end\":63009,\"start\":62997},{\"end\":63020,\"start\":63009},{\"end\":63035,\"start\":63020},{\"end\":63050,\"start\":63035},{\"end\":63063,\"start\":63050},{\"end\":63079,\"start\":63063},{\"end\":63903,\"start\":63888},{\"end\":63914,\"start\":63903},{\"end\":63925,\"start\":63914},{\"end\":63937,\"start\":63925},{\"end\":63950,\"start\":63937},{\"end\":65051,\"start\":65037},{\"end\":65065,\"start\":65051},{\"end\":65077,\"start\":65065},{\"end\":65650,\"start\":65633},{\"end\":66273,\"start\":66268},{\"end\":66390,\"start\":66376},{\"end\":66410,\"start\":66390},{\"end\":66428,\"start\":66410},{\"end\":66446,\"start\":66428},{\"end\":67572,\"start\":67558},{\"end\":67588,\"start\":67572},{\"end\":67600,\"start\":67588},{\"end\":67620,\"start\":67600},{\"end\":68719,\"start\":68704},{\"end\":68731,\"start\":68719},{\"end\":68750,\"start\":68731},{\"end\":68767,\"start\":68750},{\"end\":69072,\"start\":69052},{\"end\":69085,\"start\":69072},{\"end\":69100,\"start\":69085},{\"end\":69947,\"start\":69940},{\"end\":69968,\"start\":69947},{\"end\":69981,\"start\":69968},{\"end\":69994,\"start\":69981},{\"end\":70803,\"start\":70793},{\"end\":70813,\"start\":70803},{\"end\":70822,\"start\":70813},{\"end\":71307,\"start\":71295},{\"end\":71317,\"start\":71307},{\"end\":71330,\"start\":71317},{\"end\":71342,\"start\":71330},{\"end\":71356,\"start\":71342},{\"end\":71368,\"start\":71356},{\"end\":71379,\"start\":71368},{\"end\":71391,\"start\":71379},{\"end\":71409,\"start\":71391},{\"end\":71427,\"start\":71409},{\"end\":71818,\"start\":71808},{\"end\":71832,\"start\":71818},{\"end\":71841,\"start\":71832},{\"end\":71856,\"start\":71841},{\"end\":72188,\"start\":72173},{\"end\":72204,\"start\":72188},{\"end\":72214,\"start\":72204},{\"end\":72233,\"start\":72214},{\"end\":72247,\"start\":72233},{\"end\":72980,\"start\":72970},{\"end\":72988,\"start\":72980},{\"end\":73170,\"start\":73154},{\"end\":73185,\"start\":73170},{\"end\":73199,\"start\":73185},{\"end\":73213,\"start\":73199},{\"end\":74107,\"start\":74082},{\"end\":74119,\"start\":74107},{\"end\":74139,\"start\":74119},{\"end\":74146,\"start\":74139},{\"end\":75246,\"start\":75223},{\"end\":75258,\"start\":75246},{\"end\":75270,\"start\":75258},{\"end\":75290,\"start\":75270},{\"end\":75297,\"start\":75290},{\"end\":76365,\"start\":76345},{\"end\":76381,\"start\":76365},{\"end\":76404,\"start\":76381},{\"end\":77079,\"start\":77061},{\"end\":77093,\"start\":77079},{\"end\":77106,\"start\":77093},{\"end\":77120,\"start\":77106},{\"end\":77139,\"start\":77120},{\"end\":77151,\"start\":77139},{\"end\":77169,\"start\":77151},{\"end\":78223,\"start\":78209},{\"end\":78239,\"start\":78223},{\"end\":79231,\"start\":79217},{\"end\":79255,\"start\":79231},{\"end\":79267,\"start\":79255},{\"end\":79281,\"start\":79267},{\"end\":79290,\"start\":79281},{\"end\":79301,\"start\":79290},{\"end\":79312,\"start\":79301},{\"end\":79321,\"start\":79312},{\"end\":80202,\"start\":80191},{\"end\":80216,\"start\":80202},{\"end\":81376,\"start\":81360},{\"end\":81388,\"start\":81376},{\"end\":81411,\"start\":81388},{\"end\":81424,\"start\":81411},{\"end\":82305,\"start\":82286},{\"end\":82322,\"start\":82305},{\"end\":82338,\"start\":82322},{\"end\":82741,\"start\":82718},{\"end\":82756,\"start\":82741},{\"end\":82773,\"start\":82756},{\"end\":83047,\"start\":83034},{\"end\":83062,\"start\":83047},{\"end\":83076,\"start\":83062},{\"end\":83087,\"start\":83076},{\"end\":83544,\"start\":83531},{\"end\":83563,\"start\":83544},{\"end\":83578,\"start\":83563},{\"end\":83597,\"start\":83578},{\"end\":83610,\"start\":83597},{\"end\":84427,\"start\":84407},{\"end\":84439,\"start\":84427},{\"end\":84455,\"start\":84439},{\"end\":84469,\"start\":84455},{\"end\":84488,\"start\":84469},{\"end\":84503,\"start\":84488},{\"end\":85386,\"start\":85370},{\"end\":85402,\"start\":85386},{\"end\":85420,\"start\":85402},{\"end\":85435,\"start\":85420},{\"end\":85455,\"start\":85435},{\"end\":86148,\"start\":86132},{\"end\":86162,\"start\":86148},{\"end\":86175,\"start\":86162},{\"end\":86192,\"start\":86175},{\"end\":86205,\"start\":86192},{\"end\":86220,\"start\":86205},{\"end\":86235,\"start\":86220},{\"end\":86253,\"start\":86235},{\"end\":87079,\"start\":87062},{\"end\":87096,\"start\":87079},{\"end\":87338,\"start\":87327},{\"end\":87353,\"start\":87338},{\"end\":87367,\"start\":87353},{\"end\":87379,\"start\":87367},{\"end\":88184,\"start\":88174},{\"end\":88198,\"start\":88184},{\"end\":88209,\"start\":88198},{\"end\":88222,\"start\":88209},{\"end\":88923,\"start\":88910},{\"end\":88936,\"start\":88923},{\"end\":88945,\"start\":88936},{\"end\":88958,\"start\":88945},{\"end\":88971,\"start\":88958},{\"end\":89640,\"start\":89625},{\"end\":89655,\"start\":89640},{\"end\":89669,\"start\":89655},{\"end\":90348,\"start\":90334},{\"end\":90355,\"start\":90348},{\"end\":90368,\"start\":90355},{\"end\":90382,\"start\":90368},{\"end\":90401,\"start\":90382},{\"end\":91229,\"start\":91216},{\"end\":91241,\"start\":91229},{\"end\":91251,\"start\":91241},{\"end\":91267,\"start\":91251},{\"end\":91970,\"start\":91956},{\"end\":91987,\"start\":91970},{\"end\":92003,\"start\":91987},{\"end\":92023,\"start\":92003},{\"end\":92743,\"start\":92730},{\"end\":92756,\"start\":92743},{\"end\":92769,\"start\":92756},{\"end\":92783,\"start\":92769},{\"end\":92792,\"start\":92783},{\"end\":93334,\"start\":93323},{\"end\":93350,\"start\":93334},{\"end\":93360,\"start\":93350},{\"end\":93627,\"start\":93612},{\"end\":93647,\"start\":93627},{\"end\":93658,\"start\":93647},{\"end\":93668,\"start\":93658},{\"end\":93681,\"start\":93668},{\"end\":94520,\"start\":94507},{\"end\":94528,\"start\":94520},{\"end\":94538,\"start\":94528},{\"end\":94546,\"start\":94538}]", "bib_venue": "[{\"end\":57905,\"start\":57745},{\"end\":58921,\"start\":58845},{\"end\":59682,\"start\":59565},{\"end\":60642,\"start\":60556},{\"end\":61419,\"start\":61277},{\"end\":62395,\"start\":62309},{\"end\":63166,\"start\":63079},{\"end\":63886,\"start\":63813},{\"end\":64297,\"start\":64118},{\"end\":65184,\"start\":65098},{\"end\":65786,\"start\":65650},{\"end\":66692,\"start\":66446},{\"end\":67798,\"start\":67640},{\"end\":68702,\"start\":68616},{\"end\":69212,\"start\":69100},{\"end\":70101,\"start\":70014},{\"end\":70878,\"start\":70822},{\"end\":71293,\"start\":71238},{\"end\":71806,\"start\":71720},{\"end\":72364,\"start\":72247},{\"end\":72968,\"start\":72929},{\"end\":73326,\"start\":73233},{\"end\":74319,\"start\":74166},{\"end\":75475,\"start\":75317},{\"end\":76509,\"start\":76423},{\"end\":77347,\"start\":77189},{\"end\":78419,\"start\":78259},{\"end\":79396,\"start\":79349},{\"end\":80462,\"start\":80216},{\"end\":81541,\"start\":81424},{\"end\":82404,\"start\":82338},{\"end\":83143,\"start\":83087},{\"end\":83697,\"start\":83610},{\"end\":84609,\"start\":84523},{\"end\":85523,\"start\":85455},{\"end\":85919,\"start\":85911},{\"end\":86365,\"start\":86253},{\"end\":87122,\"start\":87111},{\"end\":87484,\"start\":87398},{\"end\":88296,\"start\":88222},{\"end\":89042,\"start\":88971},{\"end\":89758,\"start\":89692},{\"end\":90507,\"start\":90421},{\"end\":91379,\"start\":91290},{\"end\":92127,\"start\":92043},{\"end\":92848,\"start\":92792},{\"end\":93321,\"start\":93277},{\"end\":93802,\"start\":93704},{\"end\":94658,\"start\":94546},{\"end\":58121,\"start\":57960},{\"end\":59024,\"start\":58942},{\"end\":59799,\"start\":59766},{\"end\":60731,\"start\":60644},{\"end\":61619,\"start\":61465},{\"end\":62514,\"start\":62422},{\"end\":63311,\"start\":63233},{\"end\":64532,\"start\":64352},{\"end\":65204,\"start\":65186},{\"end\":65945,\"start\":65788},{\"end\":66996,\"start\":66738},{\"end\":68011,\"start\":67848},{\"end\":69319,\"start\":69284},{\"end\":70236,\"start\":70147},{\"end\":70894,\"start\":70880},{\"end\":72399,\"start\":72366},{\"end\":73482,\"start\":73389},{\"end\":74530,\"start\":74365},{\"end\":75688,\"start\":75525},{\"end\":76593,\"start\":76511},{\"end\":77563,\"start\":77393},{\"end\":78635,\"start\":78474},{\"end\":79549,\"start\":79525},{\"end\":80766,\"start\":80508},{\"end\":81658,\"start\":81625},{\"end\":82479,\"start\":82406},{\"end\":83165,\"start\":83145},{\"end\":83842,\"start\":83764},{\"end\":84776,\"start\":84689},{\"end\":85600,\"start\":85525},{\"end\":86494,\"start\":86459},{\"end\":87619,\"start\":87537},{\"end\":88425,\"start\":88336},{\"end\":89139,\"start\":89062},{\"end\":89897,\"start\":89830},{\"end\":90663,\"start\":90575},{\"end\":91488,\"start\":91394},{\"end\":92247,\"start\":92156},{\"end\":92889,\"start\":92871},{\"end\":94009,\"start\":93904},{\"end\":94788,\"start\":94732}]"}}}, "year": 2023, "month": 12, "day": 17}