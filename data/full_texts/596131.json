{"id": 596131, "updated": "2023-09-28 14:41:41.371", "metadata": {"title": "Low-Rank Tensor Completion by Truncated Nuclear Norm Regularization", "authors": "[{\"first\":\"Shengke\",\"last\":\"Xue\",\"middle\":[]},{\"first\":\"Wenyuan\",\"last\":\"Qiu\",\"middle\":[]},{\"first\":\"Fan\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Xinyu\",\"last\":\"Jin\",\"middle\":[]}]", "venue": "2018 24th International Conference on Pattern Recognition (ICPR)", "journal": "2018 24th International Conference on Pattern Recognition (ICPR)", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "Currently, low-rank tensor completion has gained cumulative attention in recovering incomplete visual data whose partial elements are missing. By taking a color image or video as a three-dimensional (3D) tensor, previous studies have suggested several definitions of tensor nuclear norm. However, they have limitations and may not properly approximate the real rank of a tensor. Besides, they do not explicitly use the low-rank property in optimization. It is proved that the recently proposed truncated nuclear norm (TNN) can replace the traditional nuclear norm, as a better estimation to the rank of a matrix. Thus, this paper presents a new method called the tensor truncated nuclear norm (T-TNN), which proposes a new definition of tensor nuclear norm and extends the truncated nuclear norm from the matrix case to the tensor case. Beneficial from the low rankness of TNN, our approach improves the efficacy of tensor completion. We exploit the previously proposed tensor singular value decomposition and the alternating direction method of multipliers in optimization. Extensive experiments on real-world videos and images demonstrate that the performance of our approach is superior to those of existing methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1712.00704", "mag": "2962759270", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icpr/XueQLJ18", "doi": "10.1109/icpr.2018.8546008"}}, "content": {"source": {"pdf_hash": "e9445dd4f16b67af657ed293a9e405965ca5da7c", "pdf_src": "Arxiv", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1712.00704", "status": "GREEN"}}, "grobid": {"id": "76a4d74f50b75525a5bdab732a9bcf23758b66df", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e9445dd4f16b67af657ed293a9e405965ca5da7c.txt", "contents": "\nLow-Rank Tensor Completion by Truncated Nuclear Norm Regularization\n\n\nShengke Xue \nCollege of Information Science and Electronic Engineering\nZhejiang University\nHangzhouChina\n\nWenyuan Qiu \nCollege of Information Science and Electronic Engineering\nZhejiang University\nHangzhouChina\n\nFan Liu \nCollege of Information Science and Electronic Engineering\nZhejiang University\nHangzhouChina\n\nXinyu Jin jinxinyuzju@gmail.com \nCollege of Information Science and Electronic Engineering\nZhejiang University\nHangzhouChina\n\nLow-Rank Tensor Completion by Truncated Nuclear Norm Regularization\n\nCurrently, low-rank tensor completion has gained cumulative attention in recovering incomplete visual data whose partial elements are missing. By taking a color image or video as a three-dimensional (3D) tensor, previous studies have suggested several definitions of tensor nuclear norm. However, they have limitations and may not properly approximate the real rank of a tensor. Besides, they do not explicitly use the low-rank property in optimization. It is proved that the recently proposed truncated nuclear norm (TNN) can replace the traditional nuclear norm, as a better estimation to the rank of a matrix. Thus, this paper presents a new method called the tensor truncated nuclear norm (T-TNN), which proposes a new definition of tensor nuclear norm and extends the truncated nuclear norm from the matrix case to the tensor case. Beneficial from the low rankness of TNN, our approach improves the efficacy of tensor completion. We exploit the previously proposed tensor singular value decomposition and the alternating direction method of multipliers in optimization. Extensive experiments on real-world videos and images demonstrate that the performance of our approach is superior to those of existing methods.\n\nI. INTRODUCTION\n\nRecovering missing values in high dimensional data has become increasingly attractive in computer vision. Exploring the intrinsic low-rank nature of incomplete data with limited observed elements has been widely applied in various applications, e.g., motion capture [1], image alignment [2], and image classification [3].\n\nSince real visual data (e.g., an image) lies in a low dimensional space, estimating missing values in images used to be considered as a low-rank matrix approximation problem [4]. The nuclear norm of a matrix is usually adopted to replace the rank function, which is non-convex and NP-hard in general. Recent studies show that the nuclear norm is suitable for a number of low-rank optimization problems. However, Hu et al. [5] clarified that the nuclear norm may not approximate well to the rank function, since it simultaneously minimizes all of the singular values.\n\nIn [5], the truncated nuclear norm regularization (TNNR) was proposed as a more accurate and much tighter substitute to the rank function, compared with the traditional nuclear norm. It apparently improves the efficacy of image recovery. Specifically, TNNR ignores the largest r singular values of data and attempts to minimize the smallest min(m, n) \u2212 r singular values, where m \u00d7 n are the dimensions of the twodimensional data, and r is defined as the number of truncated values. Some researches were derived from it. Liu et al. [6] devised a weighted TNNR using a gradient descent scheme, which further accelerated the speed of its algorithm. Lee and Lam [7] exploited the ghost-free high dynamic range imaging from irradiance maps by adopting TNNR method.\n\nMoreover, most of existing low-rank matrix approximation approaches deal with the input data in the two-dimensional fashion. Specifically, algorithms are applied on each channel individually and then the results are combined together, when recovering a color image. It reveals an explicit deficiency that the structural information between channels is not considered. Hence, recent researches treat a color image as 3D data and formulate it as a low-rank tensor completion problem.\n\nAs an elegant extension of the matrix case, tensor completion earns much interests gradually. However, the definition of the nuclear norm of a tensor is a complicated issue, since it cannot be intuitively derived from the matrix case. Several versions of tensor nuclear norm have been proposed but they are quite different to each other. Liu et al. [8] first proposed a tensor completion approach based on the sum of matricized nuclear norms (SMNN) of the tensor, which is defined as the following formulation:\nmin X k i=1 \u03b1 i ||X [i] || * , s.t. X \u2126 = (X 0 ) \u2126 ,(1)\nwhere X [i] denotes the matrix of the unfolded tensor along the ith dimension (also known as the mode-i matricization of X ), \u03b1 i > 0 is a constant that satisfies n i=1 \u03b1 i = 1, X 0 is the initial incomplete data, and \u2126 is the set of positions with respect to known elements. But no theoretical analysis interprets the nuclear norm of each matricization of X is plausible because the spacial structure may lose by the matricization. Moreover, it is unclear how to choose the optimal \u03b1 i 's [9], though they control the weights of k norms in problem (1). Generally, \u03b1 i 's are determined empirically.\n\nRecently, a novel tensor decomposition scheme, defined as tensor singular value decomposition (t-SVD), was suggested in [10]. Based on the new definition of tensor-tensor product, the t-SVD holds some properties that are similar to the matrix case. Therefore, Zhang et al. [11] defined their tubal nuclear norm (Tubal-NN) as the sum of nuclear norms of all frontal slices in the Fourier domain and declared that it is a convex relaxation to the 1 norm of tensor rank. It can be formulated as the following problem:\nmin X n3 i=1 ||X (i) || * , s.t. X \u2126 = (X 0 ) \u2126 ,(2)\nwhereX (i) is defined in Section II. Semerci et al. [12] applied this model to multienergy computed tomography images and obtained promising effects of reconstruction. However, it does not explicitly use the low-rank property in optimization and requires numerous iterations to converge. Since the t-SVD is a sophisticated function, the overall computational cost of (2) will be quite expensive.\n\nTo this end, this paper proposes a new approach called the tensor truncated nuclear norm (T-TNN). Based on the t-SVD, we define that our tensor nuclear norm equals to the sum of all singular values in an f-diagonal tensor, which is extended directly from the matrix case. Furthermore, we prove that it can be computed efficiently in the Fourier domain. To further take the advantage of TNNR, our T-TNN method generalizes it to the 3D case. In accordance with common strategies, we adopt the universal alternating direction method of multipliers (ADMM) [13] to solve our optimization. Experimental results show that our approach outperforms the previous methods.\n\n\nII. NOTATIONS AND PRELIMINARIES\n\nIn this paper, tensors are denoted by boldface Euler script letters, e.g., A. Matrices are denoted by boldface capital letters, e.g., A. Vectors are denoted by boldface lowercase letters, e.g., a. Scalars are denoted by lowercase letters, e.g., a. I n denotes an n \u00d7 n identity matrix. R and C are the fields of real number and complex number, respectively. For threeorder tensor A \u2208 C n1\u00d7n2\u00d7n3 , its (i, j, k)th element is denoted as A ijk . The ith horizontal, lateral, and frontal slices of A are denoted by the Matlab notations A(i, :, :), A(:, i, :), and A(:, :, i), respectively. In addition, the frontal slice A(:, :, i) is concisely denoted as A (i) . The inner product of matrices A and B is defined as A, B tr(A T B), where A T is the conjugate transpose of A and tr(\u00b7) denotes the trace function. The inner product of A and B in C n1\u00d7n2\u00d7n3 is defined as\nA, B n3 i=1 A (i) , B (i) . The trace of A is defined as tr(A) = n3 i=1 tr(A (i) ).\nSeveral norms of a tensor are introduced. The 1 norm is defined as ||A|| 1 ijk |A ijk |. The infinity norm is defined as ||A|| \u221e max ijk |A ijk |. The Frobenius norm is denoted as ||A|| F ijk |A ijk | 2 . The above norms are consistent with the definitions in vectors or matrices. The matrix nuclear norm is defined as\n||A|| * i \u03c3 i (A), where \u03c3 i (A) is the ith largest singular value of A.\nFor A \u2208 R n1\u00d7n2\u00d7n3 , by using the Matlab notation, we define\u0100 3), which is the discrete Fourier transformation of A along the third dimension. Likewise, we can compute A ifft(\u0100, [ ], 3) through the inverse fft function. Besides, we define\u0100 as a block diagonal matrix, where each frontal slice\u0100 (i) of\u0100 lies on the diagonal in order, i.e.,\nfft(A, [ ],A bdiag(\u0100) \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0\u0100 (1)\u0100 (2) . . .\u0100 (n3) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb .\n(\n\nThe block circulant matrix of tensor A is defined as\nbcirc(A) \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 A (1) A (n3) \u00b7 \u00b7 \u00b7 A (2) A (2) A (1) \u00b7 \u00b7 \u00b7 A (3) . . . . . . . . . . . . A (n3) A (n3\u22121) \u00b7 \u00b7 \u00b7 A (1) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb .(4)\nHere, a pair of folding operators are defined as follows\nunfold(A) \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 A (1) A (2) . . . A (n3) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb , fold(unfold(A)) A .(5)\nDefinition 2.1 (tensor product): [10] Let A \u2208 R n1\u00d7n2\u00d7n3 and B \u2208 R n2\u00d7n4\u00d7n3 , then the tensor product A * B is defined as a tensor with size n 1 \u00d7 n 4 \u00d7 n 3 , i.e.,\nA * B fold(bcirc(A) \u00b7 unfold(B)).(6)\nThe tensor product is analogous to the matrix product except that the multiplication between elements is superseded by the circular convolution. Note that the tensor product simplifies to the standard matrix product when n 3 = 1. Definition 2.2 (conjugate transpose): [10] If tensor A \u2208 R n1\u00d7n2\u00d7n3 , the conjugate transpose of which is defined as A T \u2208 R n2\u00d7n1\u00d7n3 . It is obtained by conjugate transposing each frontal slice and then reversing the order of transposed frontal slices 2 through n 3 :\nA T (1) A (1) T , A T (i) A (n3+2\u2212i) T , i = 2, . . . , n 3 .(7)\nDefinition 2.3 (identity tensor): [10] Let I \u2208 R n\u00d7n\u00d7n3 be an identity tensor whose first frontal slice I (1) is an n \u00d7 n identity matrix and the other slices are zero.\n\nDefinition 2.4 (orthogonal tensor):\n[10] Define tensor Q \u2208 R n\u00d7n\u00d7n3 is orthogonal if Q * Q T Q T * Q I.(8)Definition 2.5 (f-diagonal tensor): [10] Tensor A is called f-diagonal if each frontal slice A (i) is a diagonal matrix.\nTheorem 2.6 (tensor singular value decomposition): [10] Let A \u2208 R n1\u00d7n2\u00d7n3 , then it can be decomposed as\nA U * S * V T ,(9)\nwhere U \u2208 R n1\u00d7n1\u00d7n3 and V \u2208 R n2\u00d7n2\u00d7n3 are orthogonal, and S \u2208 R n1\u00d7n2\u00d7n3 is an f-diagonal tensor. Fig. 1 illustrates the t-SVD. However, t-SVD can be efficiently performed on the basis of matrix SVD in the Fourier domain. It comes from an important property that the block circulant matrix can be transformed to a block diagonal matrix in the Fourier domain, i.e.,\n(F n3 \u2297 I n1 ) \u00b7 bcirc(A) \u00b7 (F T n3 \u2297 I n2 ) =\u0100,(10)\nwhere F n3 denotes the n 3 \u00d7 n 3 discrete Fourier transform matrix and \u2297 denotes the Kronecker product. Note that the matrix SVD can be performed on each frontal slice of\u0100, Definition 2.7 (tensor tubal rank and nuclear norm): For tensor A \u2208 R n1\u00d7n2\u00d7n3 , its t-SVD is U * S * V T . The tensor tubal rank of A is defined as the maximum rank among all frontal slices of the f-diagonal S, i.e., max i rank(S (i) ). Our tensor nuclear norm ||A|| * is defined as the sum of singular values of all frontal slices of the f-diagonal S, i.e.,\n||A|| * tr(S) = n3 i=1 tr(S (i) ).(11)\nNote that our tensor nuclear norm becomes standard matrix nuclear norm when n 3 = 1. Thus, our tensor nuclear norm can be considered as a direct extension from the matrix case to the tensor case. Due to the fft function in the third dimension, we exploit the property that the trace of tensor product A * B equals to the trace of product of\u0100 (1) andB (1) , which are the frontal slices of tensor\u0100 andB in the Fourier domain, i.e., tr(A * B) = tr(\u0100 (1)B(1) ).\n\nInspired by (12), we can further simplify our tensor nuclear norm as follows ||A|| * tr(S) = tr(S (1) ) = ||\u0100 (1) || * .\n\nIt suggests that our tensor nuclear norm can be efficiently computed by one matrix SVD in the Fourier domain, instead of running the sophisticated t-SVD to obtain S. Our definition is different from those of previous work [9], [14], which are also calculated in the Fourier domain. The tubal nuclear norm in [9] comprises computing each frontal slice ofS. Similarly, Lu et al. [14] further proved that the tubal nuclear norm can be computed by the nuclear norm of the block circulant matrix of the tensor with factor 1/n 3 , i.e., ||A|| * = 1 n3 ||bcirc(A)|| * . However, bcirc(A) \u2208 R n1n3\u00d7n2n3 requires a huge amount of memory if n 1 , n 2 , or n 3 is large, which makes the matrix SVD slower.\n\nDefinition 2.8 (singular value thresholding): Assume the t-SVD of tensor X \u2208 R n1\u00d7n2\u00d7n3 is U * S * V T . The singular value thresholding (SVT) operator, denoted as D \u03c4 , is applied on each frontal slice of the f-diagonal tensorS:\nD \u03c4 (X ) U * D \u03c4 (S) * V T , D \u03c4 (S) ifft(D \u03c4 (S)), D \u03c4 (S (i) ) diag max{\u03c3 t \u2212 \u03c4, 0} 1\u2264t\u2264r , i = 1, 2, . . . , n 3 .(14)\nIII. TENSOR TRUNCATED NUCLEAR NORM\n\n\nA. Problem Formulation\n\nWith tensor X \u2208 R n1\u00d7n2\u00d7n3 , we define our tensor truncated nuclear norm ||X || r as follows\n||X || r ||X (1) || r = min(n1,n2) j=r+1 \u03c3 j (X (1) ) = min(n1,n2) j=1 \u03c3 j (X (1) ) \u2212 r j=1 \u03c3 j (X (1) ).(15)\nBy using the Theorem 3.1 in [5], Theorem 2.6, and Definition 2.7, we can convert (15) to:\n||X || r = ||X (1) || * \u2212 max A (1)\u0100(1)T =I, B (1)B(1)T =I tr(\u0100 (1)X (1)B(1)T ) = ||X || * \u2212 max A * A T =I, B * B T =I tr(A * X * B T ),(16)\nwhere A and B are generated from the t-SVD of X . Define the operator of selecting the first r columns in the second dimension of U and V (using Matlab notation) as follows \n\nThus, (16) can be formulated as the following problem:\nmin X ||X || * \u2212 max A * A T =I, B * B T =I tr(A * X * B T ) s.t. X \u2126 = M \u2126 ,(18)\nwhere A \u2208 R n1\u00d7r\u00d7n3 and B \u2208 R n2\u00d7r\u00d7n3 . It is not easy to directly solve (18), so we divide this optimization into two individual steps. First, we set X 1 = M \u2126 as the initial value. Assume in the th iteration, we update A and B as (17) by means of t-SVD (Theorem 2.6). Next, by fixing A and B , we compute X through a much simpler problem:\nmin X ||X || * \u2212 tr(A * X * B T ) s.t. X \u2126 = M \u2126 .(19)\nThe detail of solving (19) is provided in the next subsection. By alternately taking two steps above, the optimization converges to a local minimum of (18). The framework of our method is summarized in Algorithm 1.\n\n\nB. Optimization\n\nDue to the convergence guarantee in polynomial time, the ADMM is widely adopted in solving constrained optimization problems, such as Step 2 in Algorithm 1. First, we introduce an auxiliary variable W to relax the objective. Then, (19) can be rewritten as  Step 1: given X \u2208 R n 1 \u00d7n 2 \u00d7n 3 ,\n[ U , S , V ] = t-SVD(X ),\nwhere the orthogonal tensors are U \u2208 R n 1 \u00d7n 1 \u00d7n 3 , V \u2208 R n 2 \u00d7n 2 \u00d7n 3 . \n\n\n4:\n\nStep 2: solve the problem:\nX +1 = arg min X ||X || * \u2212 tr(A * X * B T ) s.t.\nX \u2126 = M \u2126 .\n\n5: until ||X +1 \u2212 X ||F \u2264 \u03b5 or > L 6: Output: the recovered tensor.\n\nThe augmented Lagrangian function of (20) becomes\nL(X , W, Y) = ||X || * \u2212 tr(A * W * B T ) + Y, X \u2212 W + \u00b5 2 ||X \u2212 W|| 2 F ,(21)\nwhere Y is the Lagrange multiplier and \u00b5 > 0 is the scalar penalty parameter. Let X 1 = M \u2126 , W 1 = X 1 , and Y 1 = X 1 as the initialization. The optimization of (20) consists of the following three steps:\n\nStep 1: Keep W k and Y k invariant and update X k+1 from L(X , W k , Y k ):\nX k+1 = arg min X ||X || * + \u00b5 2 ||X \u2212 W k || 2 F + Y k , X \u2212 W k = arg min X ||X || * + \u00b5 2 X \u2212 W k \u2212 1 \u00b5 Y k 2 F .(22)\nBased on the SVT operator in Definition 2.8, (22) can be solved efficiently by\nX k+1 = D 1 \u00b5 W k \u2212 1 \u00b5 Y k .(23)\nStep 2: By fixing X k+1 and Y k , we can solve W through\nW k+1 = arg min W L(X k+1 , W, Y k ) = arg min W \u00b5 2 ||X k+1 \u2212 W|| 2 F \u2212 tr(A * W * B T ) + Y k , X k+1 \u2212 W .(24)\nApparently, (24) is quadratic with respect to W. Therefore, by setting the derivative of (24) to zero, we obtain the closedform solution as follows\nW k+1 = X k+1 + 1 \u00b5 A T * B + Y k .(25)\nFurthermore, the values of all observed elements should be constant in each iteration, i.e.,\nW k+1 = (W k+1 ) \u2126 c + M \u2126 .(26)\nStep 3: Update Y k+1 directly through\nY k+1 = Y k + \u00b5(X k+1 \u2212 W k+1 ).(27)\n\nIV. EXPERIMENTS\n\nIn this section, we conduct extensive experiments to demonstrate the efficacy of our proposed algorithm. The compared approaches are: 1) Low-rank matrix completion (LRMC) [4]; 2) Matrix completion by TNNR [5]; 3) Tensor completion by SMNN [8]; 4) Tensor completion by Tubal-NN [11]; 5) Tensor completion by T-TNN [ours]; The code of our algorithm is available online at https://github. com/xueshengke/Tensor-TNNR. All experiments are executed in Matlab R2015b based on Windows 10, equipped with an Intel Core i7 CPU @ 2.60 GHz and 12 GB Memory. Each parameter of compared methods is readjusted to be optimal and the best results are reported. For fair comparison, each number is averaged over 10 separate runs.\n\nIn this paper, every algorithm stops when ||X k+1 \u2212 X k || F is sufficiently small or the maximum number of iterations has reached. Denote ||X || rec as the final output. Let \u03b5 = 10 \u22123 and L = 50 for our method. Let \u00b5 = 5 \u00d7 10 \u22124 to balance the efficiency and the accuracy of our approach. In practice, the real rank of incomplete data is unknown. Due to the absence of prior knowledge to the number of truncated singular values, r is tested from [1,30] to choose an optimal value for each case manually.\n\nIn general, the peak signal-to-noise ratio (PSNR) is a commonly used metric to evaluate the performances of different approaches, which is defined as follows:\nMSE ||(X rec \u2212 M) \u2126 c || 2 F /T,(28)\nPSNR 10 \u00d7 log 10\n255 2 MSE ,(29)\nwhere T is the total number of missing elements in a tensor, and we presume the maximum pixel value in X is 255.\n\n\nA. Video Recovery\n\nFor video, we consider it as a 3D tensor, where the first and second dimensions denote space, and the last dimension represents time. In this experiment, we use a basket video (source: YouTube) with size 144 \u00d7 256 \u00d7 40, which is taken from a horizontally moving camera. 65% elements are randomly lost. Fig. 2a shows the 20th incomplete frame of the basket video.\n\nOur T-TNN method compares to the LRMC, TNNR, SMNN, and Tubal-NN methods. The PSNR, the iteration number, and the 20th frame of the recovered video are reported in Fig. 2 to validate the effects of five approaches.\n\nObviously, our T-TNN approach performs best than other methods, though our iteration number is not the least. Fig. 2b shows that the result of LRMC is the worst, since the matrix completion deals with each frame separately and doest not use the spacial structure in the third dimension. Thus, it is not applicable for the tensor case. Beneficial from the truncated nuclear norm, the TNNR (Fig. 2c) is slightly better than the LRMC. In tensor cases, Fig. 2d is the worst than the others, which indicates that the SMNN is not a proper definition for tensor nuclear norm. In addition, Fig. 2f is a bit clearer than Fig. 2e. It implies that our T-TNN method performs best in video recovery.\n\n\nB. Image Recovery\n\nIn real scenarios, some images may be corrupted due to the random loss. Since a color image has three channels, we hereby regard it as a 3D tensor rather than separating them.\n\nWe use 10 images, as shown in Fig. 3, all of which have same size 400 \u00d7 300. PSNR is adopted to evaluate the effects of image recovery by different algorithms. 50% pixels in each image are randomly missing.  resulting images, and the running time are reported in Table I, Fig. 4, and Fig. 5, respectively. Table I presents the PSNR of five approaches applied on ten images (Fig. 3) with 50% random entries lost. Obviously, LRMC requires more than 1000 iterations to converge. Using the truncated nuclear norm, the TNNR notably improves the PSNR of recovery on each image and facilitates the convergence, compared with the LRMC. In tensor case, the SMNN performs much inferior than then the Tubal-NN in PSNR and iterations, sometimes worse than the LRMC in PSNR, which indicates that the SMNN may not be an appropriate definition for tensor completion. The Tubal-NN holds better PSNR and much faster convergence than the LRMC and SMNN, which implies that tensor tubal nuclear norm is practical for image recovery. However, our T-TNN is slightly superior in PSNR than the Tubal-NN in all cases and converges much faster. Fig. 4a presents the first image of Fig. 3 with 50% element loss. In addition, the recovered results by five approaches are illustrated in Figs. 4b-4f, respectively. Obviously, the result of LRMC (Fig. 4b) contains quite blurry parts and is the worst than the others. With the advantage of the TNN, Fig. 4c is visually much clearer than Fig. 4b. However, a certain amount of noise still exists. In tensor case, Fig. 4e is pretty clearer than Fig. 4d, which indicates that the Tubal-NN is much more appropriate than the SMNN. Moreover, the result of SMNN is apparently inferior than the result of TNNR. The result of our method (Fig. 4f) is visually competitive to the result of Tubal-NN (Fig. 4e), both of which contain a few outliers, compared with the original image. Fig. 5 demonstrates the running time on ten images by five methods. Apparently, the TNNR runs much slower than the others and is erratic on different images (from 82.2 s to 160.3 s). Because the ADMM inherently converges slower and consumes more time on SVD operation. The SMNN performs Running time by five methods on ten images (Fig. 3) highly stable and spends about 53.0 s on each image. Similarly, the Tubal-NN and LRMC reveal nearly identical stability as the SMNN, and they are roughly 3.0 s and 6.0 s in average faster than the SMNN, respectively. In all cases, our T-TNN method runs quite faster (less than 40 s) than the Tubal-NN and LRMC, though ours are not sufficiently stable. Because our approach is based on the t-SVD, which achieves the better convergence. Thus, our T-TNN method is efficient for image recovery and is superior than the compared approaches.\n\n\nV. CONCLUSIONS\n\nThis paper proposes the tensor truncated nuclear norm for low-rank tensor completion. Specifically, we present a new definition of tensor nuclear norm, which is a generalization of the standard matrix nuclear norm. In addition, the truncated nuclear norm scheme is integrated in our approach. We adopt previously proposed tensor singular value decomposition and the alternating direction method of multipliers to efficiently solve the problem. Hence, the performance of our method is substantially improved. Experimental results indicate that our approach outperforms the existing methods both in recovering videos and images. Furthermore, the comparisons on running time demonstrate that the efficiency of our approach is further accelerated with the advantage of truncated nuclear norm.\n\nFig. 1 .\n1Illustration of the t-SVD of an n 1 \u00d7 n 2 \u00d7 n 3 tensor i.e.,\u0100 (i) =\u016a (i)S(i)V (i)T , where\u016a (i) ,S (i) , andV (i) are the frontal slices of\u016a ,S, andV, respectively. More briefly,\u0100 =\u016aSV T . Using the ifft function along the third dimension, we have U = ifft(\u016a , [ ], 3), S = ifft(S, [ ], 3), and V = ifft(V, [ ], 3).\n\nA\nU (:, 1 : r, :) T , B V(:, 1 : r, :) T .\n\n\n* \u2212 tr(A * W * B T ) s.t. X = W, W \u2126 = M \u2126 .\n\n\nM, the original incomplete data; \u2126, the index set of the known elements; \u2126 c , the index set of the unknown elements. Initialize: X 1 = M \u2126 , \u03b5 = 10 \u22123 ,\n\n\nand B as follows (r \u2264 min{n1, n2}): A = U (:, 1 : r, :) T , B = V(:, 1 : r, :) T .\n\nFig. 2 .Fig. 3 .\n23The 20th frame of the basket video recovered by five methods: (a) incomplete frame; (b) LRMC; (c) TNNR; (d) SMNN; (e) Tubal-NN; (f) T-TNN Ten images used in our experiments\n\n\nFig. 4a presents an example of the incomplete images. Under this configuration, our T-TNN approach compares to the LRMC, TNNR, SMNN, and Tubal-NN methods. The PSNR (iteration), the visualized examples of\n\nFig. 4 .\n4Recovered results of the first image of Fig. 3 with 50% random loss by five methods: (a) incomplete image; (b) LRMC; (c) TNNR; (d) SMNN; (e) Tubal-NN; (f)\n\nTABLE I PSNR\nIOF TEN RECOVERED IMAGES BY FIVE METHODS WITH 50% RANDOM ELEMENT LOSS. ITERATION NUMBER IN PARENTHESESNo. \nLRMC \nTNNR \nSMNN \nTubal-NN \nT-TNN \n\n1 \n24.00 \n25.56 \n17.19 \n29.21 \n29.65 \n(1251) \n(665) \n(343) \n(264) \n(165) \n\n2 \n26.51 \n29.79 \n19.19 \n32.00 \n32.55 \n(1232) \n(541) \n(346) \n(258) \n(148) \n\n3 \n20.95 \n24.33 \n22.23 \n26.37 \n27.67 \n(1274) \n(878) \n(341) \n(262) \n(178) \n\n4 \n27.28 \n30.82 \n27.40 \n34.19 \n35.32 \n(1248) \n(614) \n(351) \n(244) \n(181) \n\n5 \n25.91 \n28.96 \n23.95 \n30.46 \n31.20 \n(1248) \n(656) \n(346) \n(246) \n(180) \n\n6 \n22.21 \n23.23 \n22.95 \n25.54 \n26.24 \n(1251) \n(813) \n(339) \n(241) \n(212) \n\n7 \n27.32 \n30.55 \n29.85 \n33.66 \n34.45 \n(1230) \n(624) \n(345) \n(247) \n(188) \n\n8 \n23.85 \n26.04 \n18.80 \n29.12 \n29.93 \n(1256) \n(573) \n(344) \n(249) \n(173) \n\n9 \n22.68 \n24.17 \n22.53 \n28.22 \n28.98 \n(1261) \n(639) \n(347) \n(260) \n(140) \n\n10 \n23.52 \n26.60 \n23.40 \n31.59 \n32.60 \n(1262) \n(745) \n(349) \n(254) \n(184) \n\n\n\nMotion capture data completion via truncated nuclear norm regularization. W Hu, Z Wang, S Liu, X Yang, G Yu, J J Zhang, IEEE Signal Processing Lett. 99W. Hu, Z. Wang, S. Liu, X. Yang, G. Yu, and J. J. Zhang, \"Motion capture data completion via truncated nuclear norm regularization,\" IEEE Signal Processing Lett., vol. PP, no. 99, pp. 1-1, 2017.\n\nImage alignment by online robust PCA via stochastic gradient descent. W Song, J Zhu, Y Li, C Chen, IEEE Trans. Circuits Syst. Video Technol. 267W. Song, J. Zhu, Y. Li, and C. Chen, \"Image alignment by online robust PCA via stochastic gradient descent,\" IEEE Trans. Circuits Syst. Video Technol., vol. 26, no. 7, pp. 1241-1250, 2016.\n\nRobust classwise and projective low-rank representation for image classification. S Xue, X Jin, Signal, Image and Video Process. 1S. Xue and X. Jin, \"Robust classwise and projective low-rank repre- sentation for image classification,\" Signal, Image and Video Process., vol. 1, no. 1, pp. 1-9, 2017.\n\nExact matrix completion via convex optimization. E J Cand\u00e8s, B Recht, Found. Comput. Math. 96E. J. Cand\u00e8s and B. Recht, \"Exact matrix completion via convex optimization,\" Found. Comput. Math., vol. 9, no. 6, pp. 717-772, 2009.\n\nFast and accurate matrix completion via truncated nuclear norm regularization. Y Hu, D Zhang, J Ye, X Li, X He, IEEE Trans. Pattern Anal. Mach. Intell. 359Y. Hu, D. Zhang, J. Ye, X. Li, and X. He, \"Fast and accurate matrix completion via truncated nuclear norm regularization,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 9, pp. 2117-2130, 2013.\n\nA truncated nuclear norm regularization method based on weighted residual error for matrix completion. Q Liu, Z Lai, Z Zhou, F Kuang, Z Jin, IEEE Trans. Image Process. 251Q. Liu, Z. Lai, Z. Zhou, F. Kuang, and Z. Jin, \"A truncated nuclear norm regularization method based on weighted residual error for matrix completion,\" IEEE Trans. Image Process., vol. 25, no. 1, pp. 316-330, 2016.\n\nComputationally efficient truncated nuclear norm minimization for high dynamic range imaging. C Lee, E Y Lam, IEEE Trans. Image Process. 259C. Lee and E. Y. Lam, \"Computationally efficient truncated nuclear norm minimization for high dynamic range imaging,\" IEEE Trans. Image Process., vol. 25, no. 9, pp. 4145-4157, 2016.\n\nTensor completion for estimating missing values in visual data. J Liu, P Musialski, P Wonka, J Ye, IEEE Trans. Pattern Anal. Mach. Intell. 351J. Liu, P. Musialski, P. Wonka, and J. Ye, \"Tensor completion for estimating missing values in visual data,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 1, pp. 208-220, 2013.\n\nExact tensor completion using t-SVD. Z Zhang, S Aeron, IEEE Trans. Signal Process. 656Z. Zhang and S. Aeron, \"Exact tensor completion using t-SVD,\" IEEE Trans. Signal Process., vol. 65, no. 6, pp. 1511-1526, 2017.\n\nThird-order tensors as operators on matrices: A theoretical and computational framework with applications in imaging. M E Kilmer, K Braman, N Hao, R C Hoover, SIAM J. Matrix Anal. Appl. 341M. E. Kilmer, K. Braman, N. Hao, and R. C. Hoover, \"Third-order tensors as operators on matrices: A theoretical and computational framework with applications in imaging,\" SIAM J. Matrix Anal. Appl., vol. 34, no. 1, pp. 148-172, 2013.\n\nNovel methods for multilinear data completion and de-noising based on tensor-SVD. Z Zhang, G Ely, S Aeron, N Hao, M Kilmer, IEEE Conference on CVPR. Z. Zhang, G. Ely, S. Aeron, N. Hao, and M. Kilmer, \"Novel methods for multilinear data completion and de-noising based on tensor-SVD,\" in IEEE Conference on CVPR, June 2014, pp. 3842-3849.\n\nTensor-based formulation and nuclear norm regularization for multienergy computed tomography. O Semerci, N Hao, M E Kilmer, E L Miller, IEEE Trans. Image Process. 234O. Semerci, N. Hao, M. E. Kilmer, and E. L. Miller, \"Tensor-based formulation and nuclear norm regularization for multienergy computed tomography,\" IEEE Trans. Image Process., vol. 23, no. 4, pp. 1678- 1693, 2014.\n\nLinearized alternating direction method with adaptive penalty for low-rank representation. Z Lin, R Liu, Z Su, Proc. NIPS. NIPSZ. Lin, R. Liu, and Z. Su, \"Linearized alternating direction method with adaptive penalty for low-rank representation,\" in Proc. NIPS, 2011, pp. 612-620.\n\nTensor robust principal component analysis: Exact recovery of corrupted low-rank tensors via convex optimization. C Lu, J Feng, Y Chen, W Liu, Z Lin, S Yan, IEEE Conference on CVPR. C. Lu, J. Feng, Y. Chen, W. Liu, Z. Lin, and S. Yan, \"Tensor robust principal component analysis: Exact recovery of corrupted low-rank tensors via convex optimization,\" in IEEE Conference on CVPR, 2016, pp. 5249-5257.\n", "annotations": {"author": "[{\"end\":176,\"start\":71},{\"end\":282,\"start\":177},{\"end\":384,\"start\":283},{\"end\":510,\"start\":385}]", "publisher": null, "author_last_name": "[{\"end\":82,\"start\":79},{\"end\":188,\"start\":185},{\"end\":290,\"start\":287},{\"end\":394,\"start\":391}]", "author_first_name": "[{\"end\":78,\"start\":71},{\"end\":184,\"start\":177},{\"end\":286,\"start\":283},{\"end\":390,\"start\":385}]", "author_affiliation": "[{\"end\":175,\"start\":84},{\"end\":281,\"start\":190},{\"end\":383,\"start\":292},{\"end\":509,\"start\":418}]", "title": "[{\"end\":68,\"start\":1},{\"end\":578,\"start\":511}]", "venue": null, "abstract": "[{\"end\":1799,\"start\":580}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2087,\"start\":2084},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2108,\"start\":2105},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2138,\"start\":2135},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2318,\"start\":2315},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2566,\"start\":2563},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2715,\"start\":2712},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3244,\"start\":3241},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3371,\"start\":3368},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4306,\"start\":4303},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5014,\"start\":5011},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5246,\"start\":5242},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5399,\"start\":5395},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5746,\"start\":5742},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6643,\"start\":6639},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8189,\"start\":8187},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8895,\"start\":8891},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9332,\"start\":9328},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9662,\"start\":9658},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11483,\"start\":11480},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11492,\"start\":11489},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11945,\"start\":11942},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11951,\"start\":11947},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12031,\"start\":12028},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12101,\"start\":12097},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13063,\"start\":13060},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16168,\"start\":16165},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16202,\"start\":16199},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16236,\"start\":16233},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16275,\"start\":16271},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17156,\"start\":17153},{\"end\":17159,\"start\":17156}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":22936,\"start\":22610},{\"attributes\":{\"id\":\"fig_1\"},\"end\":22980,\"start\":22937},{\"attributes\":{\"id\":\"fig_2\"},\"end\":23027,\"start\":22981},{\"attributes\":{\"id\":\"fig_3\"},\"end\":23183,\"start\":23028},{\"attributes\":{\"id\":\"fig_4\"},\"end\":23268,\"start\":23184},{\"attributes\":{\"id\":\"fig_5\"},\"end\":23461,\"start\":23269},{\"attributes\":{\"id\":\"fig_6\"},\"end\":23667,\"start\":23462},{\"attributes\":{\"id\":\"fig_7\"},\"end\":23833,\"start\":23668},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":24742,\"start\":23834}]", "paragraph": "[{\"end\":2139,\"start\":1818},{\"end\":2707,\"start\":2141},{\"end\":3469,\"start\":2709},{\"end\":3952,\"start\":3471},{\"end\":4464,\"start\":3954},{\"end\":5120,\"start\":4521},{\"end\":5636,\"start\":5122},{\"end\":6085,\"start\":5690},{\"end\":6748,\"start\":6087},{\"end\":7648,\"start\":6784},{\"end\":8051,\"start\":7733},{\"end\":8463,\"start\":8125},{\"end\":8531,\"start\":8530},{\"end\":8585,\"start\":8533},{\"end\":8777,\"start\":8721},{\"end\":9022,\"start\":8858},{\"end\":9558,\"start\":9060},{\"end\":9792,\"start\":9624},{\"end\":9829,\"start\":9794},{\"end\":10126,\"start\":10021},{\"end\":10512,\"start\":10146},{\"end\":11098,\"start\":10566},{\"end\":11596,\"start\":11138},{\"end\":11718,\"start\":11598},{\"end\":12414,\"start\":11720},{\"end\":12645,\"start\":12416},{\"end\":12802,\"start\":12768},{\"end\":12921,\"start\":12829},{\"end\":13121,\"start\":13032},{\"end\":13437,\"start\":13264},{\"end\":13493,\"start\":13439},{\"end\":13916,\"start\":13576},{\"end\":14186,\"start\":13972},{\"end\":14498,\"start\":14206},{\"end\":14603,\"start\":14526},{\"end\":14636,\"start\":14610},{\"end\":14698,\"start\":14687},{\"end\":14767,\"start\":14700},{\"end\":14818,\"start\":14769},{\"end\":15104,\"start\":14898},{\"end\":15181,\"start\":15106},{\"end\":15381,\"start\":15303},{\"end\":15472,\"start\":15416},{\"end\":15734,\"start\":15587},{\"end\":15867,\"start\":15775},{\"end\":15938,\"start\":15901},{\"end\":16704,\"start\":15994},{\"end\":17210,\"start\":16706},{\"end\":17370,\"start\":17212},{\"end\":17424,\"start\":17408},{\"end\":17553,\"start\":17441},{\"end\":17937,\"start\":17575},{\"end\":18152,\"start\":17939},{\"end\":18840,\"start\":18154},{\"end\":19037,\"start\":18862},{\"end\":21802,\"start\":19039},{\"end\":22609,\"start\":21821}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":4520,\"start\":4465},{\"attributes\":{\"id\":\"formula_1\"},\"end\":5689,\"start\":5637},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7732,\"start\":7649},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8124,\"start\":8052},{\"attributes\":{\"id\":\"formula_4\"},\"end\":8475,\"start\":8464},{\"attributes\":{\"id\":\"formula_5\"},\"end\":8529,\"start\":8475},{\"attributes\":{\"id\":\"formula_7\"},\"end\":8720,\"start\":8586},{\"attributes\":{\"id\":\"formula_8\"},\"end\":8857,\"start\":8778},{\"attributes\":{\"id\":\"formula_9\"},\"end\":9059,\"start\":9023},{\"attributes\":{\"id\":\"formula_10\"},\"end\":9623,\"start\":9559},{\"attributes\":{\"id\":\"formula_11\"},\"end\":9900,\"start\":9830},{\"attributes\":{\"id\":\"formula_12\"},\"end\":10020,\"start\":9900},{\"attributes\":{\"id\":\"formula_13\"},\"end\":10145,\"start\":10127},{\"attributes\":{\"id\":\"formula_14\"},\"end\":10565,\"start\":10513},{\"attributes\":{\"id\":\"formula_15\"},\"end\":11137,\"start\":11099},{\"attributes\":{\"id\":\"formula_18\"},\"end\":12767,\"start\":12646},{\"attributes\":{\"id\":\"formula_19\"},\"end\":13031,\"start\":12922},{\"attributes\":{\"id\":\"formula_20\"},\"end\":13263,\"start\":13122},{\"attributes\":{\"id\":\"formula_22\"},\"end\":13575,\"start\":13494},{\"attributes\":{\"id\":\"formula_23\"},\"end\":13971,\"start\":13917},{\"attributes\":{\"id\":\"formula_24\"},\"end\":14525,\"start\":14499},{\"attributes\":{\"id\":\"formula_25\"},\"end\":14686,\"start\":14637},{\"attributes\":{\"id\":\"formula_26\"},\"end\":14897,\"start\":14819},{\"attributes\":{\"id\":\"formula_27\"},\"end\":15302,\"start\":15182},{\"attributes\":{\"id\":\"formula_28\"},\"end\":15415,\"start\":15382},{\"attributes\":{\"id\":\"formula_29\"},\"end\":15586,\"start\":15473},{\"attributes\":{\"id\":\"formula_30\"},\"end\":15774,\"start\":15735},{\"attributes\":{\"id\":\"formula_31\"},\"end\":15900,\"start\":15868},{\"attributes\":{\"id\":\"formula_32\"},\"end\":15975,\"start\":15939},{\"attributes\":{\"id\":\"formula_33\"},\"end\":17407,\"start\":17371},{\"attributes\":{\"id\":\"formula_34\"},\"end\":17440,\"start\":17425}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19309,\"start\":19302},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19352,\"start\":19345}]", "section_header": "[{\"end\":1816,\"start\":1801},{\"end\":6782,\"start\":6751},{\"end\":12827,\"start\":12805},{\"end\":14204,\"start\":14189},{\"end\":14608,\"start\":14606},{\"end\":15992,\"start\":15977},{\"end\":17573,\"start\":17556},{\"end\":18860,\"start\":18843},{\"end\":21819,\"start\":21805},{\"end\":22619,\"start\":22611},{\"end\":22939,\"start\":22938},{\"end\":23286,\"start\":23270},{\"end\":23677,\"start\":23669},{\"end\":23847,\"start\":23835}]", "table": "[{\"end\":24742,\"start\":23950}]", "figure_caption": "[{\"end\":22936,\"start\":22621},{\"end\":22980,\"start\":22940},{\"end\":23027,\"start\":22983},{\"end\":23183,\"start\":23030},{\"end\":23268,\"start\":23186},{\"end\":23461,\"start\":23289},{\"end\":23667,\"start\":23464},{\"end\":23833,\"start\":23679},{\"end\":23950,\"start\":23849}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10252,\"start\":10246},{\"end\":17884,\"start\":17877},{\"end\":18108,\"start\":18102},{\"end\":18271,\"start\":18264},{\"end\":18551,\"start\":18542},{\"end\":18610,\"start\":18603},{\"end\":18743,\"start\":18736},{\"end\":18773,\"start\":18766},{\"end\":19075,\"start\":19069},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":19317,\"start\":19311},{\"end\":19329,\"start\":19323},{\"end\":19420,\"start\":19412},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":20165,\"start\":20158},{\"end\":20200,\"start\":20194},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":20308,\"start\":20297},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":20363,\"start\":20354},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":20464,\"start\":20457},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":20502,\"start\":20495},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":20576,\"start\":20569},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":20607,\"start\":20600},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":20794,\"start\":20785},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":20854,\"start\":20845},{\"end\":20934,\"start\":20928},{\"end\":21266,\"start\":21258}]", "bib_author_first_name": "[{\"end\":24819,\"start\":24818},{\"end\":24825,\"start\":24824},{\"end\":24833,\"start\":24832},{\"end\":24840,\"start\":24839},{\"end\":24848,\"start\":24847},{\"end\":24854,\"start\":24853},{\"end\":24856,\"start\":24855},{\"end\":25162,\"start\":25161},{\"end\":25170,\"start\":25169},{\"end\":25177,\"start\":25176},{\"end\":25183,\"start\":25182},{\"end\":25508,\"start\":25507},{\"end\":25515,\"start\":25514},{\"end\":25775,\"start\":25774},{\"end\":25777,\"start\":25776},{\"end\":25787,\"start\":25786},{\"end\":26033,\"start\":26032},{\"end\":26039,\"start\":26038},{\"end\":26048,\"start\":26047},{\"end\":26054,\"start\":26053},{\"end\":26060,\"start\":26059},{\"end\":26414,\"start\":26413},{\"end\":26421,\"start\":26420},{\"end\":26428,\"start\":26427},{\"end\":26436,\"start\":26435},{\"end\":26445,\"start\":26444},{\"end\":26792,\"start\":26791},{\"end\":26799,\"start\":26798},{\"end\":26801,\"start\":26800},{\"end\":27086,\"start\":27085},{\"end\":27093,\"start\":27092},{\"end\":27106,\"start\":27105},{\"end\":27115,\"start\":27114},{\"end\":27387,\"start\":27386},{\"end\":27396,\"start\":27395},{\"end\":27683,\"start\":27682},{\"end\":27685,\"start\":27684},{\"end\":27695,\"start\":27694},{\"end\":27705,\"start\":27704},{\"end\":27712,\"start\":27711},{\"end\":27714,\"start\":27713},{\"end\":28071,\"start\":28070},{\"end\":28080,\"start\":28079},{\"end\":28087,\"start\":28086},{\"end\":28096,\"start\":28095},{\"end\":28103,\"start\":28102},{\"end\":28422,\"start\":28421},{\"end\":28433,\"start\":28432},{\"end\":28440,\"start\":28439},{\"end\":28442,\"start\":28441},{\"end\":28452,\"start\":28451},{\"end\":28454,\"start\":28453},{\"end\":28800,\"start\":28799},{\"end\":28807,\"start\":28806},{\"end\":28814,\"start\":28813},{\"end\":29105,\"start\":29104},{\"end\":29111,\"start\":29110},{\"end\":29119,\"start\":29118},{\"end\":29127,\"start\":29126},{\"end\":29134,\"start\":29133},{\"end\":29141,\"start\":29140}]", "bib_author_last_name": "[{\"end\":24822,\"start\":24820},{\"end\":24830,\"start\":24826},{\"end\":24837,\"start\":24834},{\"end\":24845,\"start\":24841},{\"end\":24851,\"start\":24849},{\"end\":24862,\"start\":24857},{\"end\":25167,\"start\":25163},{\"end\":25174,\"start\":25171},{\"end\":25180,\"start\":25178},{\"end\":25188,\"start\":25184},{\"end\":25512,\"start\":25509},{\"end\":25519,\"start\":25516},{\"end\":25784,\"start\":25778},{\"end\":25793,\"start\":25788},{\"end\":26036,\"start\":26034},{\"end\":26045,\"start\":26040},{\"end\":26051,\"start\":26049},{\"end\":26057,\"start\":26055},{\"end\":26063,\"start\":26061},{\"end\":26418,\"start\":26415},{\"end\":26425,\"start\":26422},{\"end\":26433,\"start\":26429},{\"end\":26442,\"start\":26437},{\"end\":26449,\"start\":26446},{\"end\":26796,\"start\":26793},{\"end\":26805,\"start\":26802},{\"end\":27090,\"start\":27087},{\"end\":27103,\"start\":27094},{\"end\":27112,\"start\":27107},{\"end\":27118,\"start\":27116},{\"end\":27393,\"start\":27388},{\"end\":27402,\"start\":27397},{\"end\":27692,\"start\":27686},{\"end\":27702,\"start\":27696},{\"end\":27709,\"start\":27706},{\"end\":27721,\"start\":27715},{\"end\":28077,\"start\":28072},{\"end\":28084,\"start\":28081},{\"end\":28093,\"start\":28088},{\"end\":28100,\"start\":28097},{\"end\":28110,\"start\":28104},{\"end\":28430,\"start\":28423},{\"end\":28437,\"start\":28434},{\"end\":28449,\"start\":28443},{\"end\":28461,\"start\":28455},{\"end\":28804,\"start\":28801},{\"end\":28811,\"start\":28808},{\"end\":28817,\"start\":28815},{\"end\":29108,\"start\":29106},{\"end\":29116,\"start\":29112},{\"end\":29124,\"start\":29120},{\"end\":29131,\"start\":29128},{\"end\":29138,\"start\":29135},{\"end\":29145,\"start\":29142}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":30804578},\"end\":25089,\"start\":24744},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":27198202},\"end\":25423,\"start\":25091},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":39940},\"end\":25723,\"start\":25425},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":8061516},\"end\":25951,\"start\":25725},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":10957595},\"end\":26308,\"start\":25953},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":17140804},\"end\":26695,\"start\":26310},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":12472034},\"end\":27019,\"start\":26697},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":82504},\"end\":27347,\"start\":27021},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":14143439},\"end\":27562,\"start\":27349},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9254348},\"end\":27986,\"start\":27564},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14617449},\"end\":28325,\"start\":27988},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":563208},\"end\":28706,\"start\":28327},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":11089238},\"end\":28988,\"start\":28708},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":7469586},\"end\":29389,\"start\":28990}]", "bib_title": "[{\"end\":24816,\"start\":24744},{\"end\":25159,\"start\":25091},{\"end\":25505,\"start\":25425},{\"end\":25772,\"start\":25725},{\"end\":26030,\"start\":25953},{\"end\":26411,\"start\":26310},{\"end\":26789,\"start\":26697},{\"end\":27083,\"start\":27021},{\"end\":27384,\"start\":27349},{\"end\":27680,\"start\":27564},{\"end\":28068,\"start\":27988},{\"end\":28419,\"start\":28327},{\"end\":28797,\"start\":28708},{\"end\":29102,\"start\":28990}]", "bib_author": "[{\"end\":24824,\"start\":24818},{\"end\":24832,\"start\":24824},{\"end\":24839,\"start\":24832},{\"end\":24847,\"start\":24839},{\"end\":24853,\"start\":24847},{\"end\":24864,\"start\":24853},{\"end\":25169,\"start\":25161},{\"end\":25176,\"start\":25169},{\"end\":25182,\"start\":25176},{\"end\":25190,\"start\":25182},{\"end\":25514,\"start\":25507},{\"end\":25521,\"start\":25514},{\"end\":25786,\"start\":25774},{\"end\":25795,\"start\":25786},{\"end\":26038,\"start\":26032},{\"end\":26047,\"start\":26038},{\"end\":26053,\"start\":26047},{\"end\":26059,\"start\":26053},{\"end\":26065,\"start\":26059},{\"end\":26420,\"start\":26413},{\"end\":26427,\"start\":26420},{\"end\":26435,\"start\":26427},{\"end\":26444,\"start\":26435},{\"end\":26451,\"start\":26444},{\"end\":26798,\"start\":26791},{\"end\":26807,\"start\":26798},{\"end\":27092,\"start\":27085},{\"end\":27105,\"start\":27092},{\"end\":27114,\"start\":27105},{\"end\":27120,\"start\":27114},{\"end\":27395,\"start\":27386},{\"end\":27404,\"start\":27395},{\"end\":27694,\"start\":27682},{\"end\":27704,\"start\":27694},{\"end\":27711,\"start\":27704},{\"end\":27723,\"start\":27711},{\"end\":28079,\"start\":28070},{\"end\":28086,\"start\":28079},{\"end\":28095,\"start\":28086},{\"end\":28102,\"start\":28095},{\"end\":28112,\"start\":28102},{\"end\":28432,\"start\":28421},{\"end\":28439,\"start\":28432},{\"end\":28451,\"start\":28439},{\"end\":28463,\"start\":28451},{\"end\":28806,\"start\":28799},{\"end\":28813,\"start\":28806},{\"end\":28819,\"start\":28813},{\"end\":29110,\"start\":29104},{\"end\":29118,\"start\":29110},{\"end\":29126,\"start\":29118},{\"end\":29133,\"start\":29126},{\"end\":29140,\"start\":29133},{\"end\":29147,\"start\":29140}]", "bib_venue": "[{\"end\":24891,\"start\":24864},{\"end\":25230,\"start\":25190},{\"end\":25552,\"start\":25521},{\"end\":25814,\"start\":25795},{\"end\":26103,\"start\":26065},{\"end\":26476,\"start\":26451},{\"end\":26832,\"start\":26807},{\"end\":27158,\"start\":27120},{\"end\":27430,\"start\":27404},{\"end\":27748,\"start\":27723},{\"end\":28135,\"start\":28112},{\"end\":28488,\"start\":28463},{\"end\":28829,\"start\":28819},{\"end\":29170,\"start\":29147},{\"end\":28835,\"start\":28831}]"}}}, "year": 2023, "month": 12, "day": 17}