{"id": 6813232, "updated": "2023-09-29 11:21:42.407", "metadata": {"title": "Multi-View Deep Learning for Consistent Semantic Mapping with RGB-D Cameras", "authors": "[{\"first\":\"Lingni\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Jorg\",\"last\":\"Stuckler\",\"middle\":[]},{\"first\":\"Christian\",\"last\":\"Kerl\",\"middle\":[]},{\"first\":\"Daniel\",\"last\":\"Cremers\",\"middle\":[]}]", "venue": "2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "journal": "2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "publication_date": {"year": 2017, "month": 3, "day": 26}, "abstract": "Visual scene understanding is an important capability that enables robots to purposefully act in their environment. In this paper, we propose a novel approach to object-class segmentation from multiple RGB-D views using deep learning. We train a deep neural network to predict object-class semantics that is consistent from several view points in a semi-supervised way. At test time, the semantics predictions of our network can be fused more consistently in semantic keyframe maps than predictions of a network trained on individual views. We base our network architecture on a recent single-view deep learning approach to RGB and depth fusion for semantic object-class segmentation and enhance it with multi-scale loss minimization. We obtain the camera trajectory using RGB-D SLAM and warp the predictions of RGB-D images into ground-truth annotated frames in order to enforce multi-view consistency during training. At test time, predictions from multiple views are fused into keyframes. We propose and analyze several methods for enforcing multi-view consistency during training and testing. We evaluate the benefit of multi-view consistency training and demonstrate that pooling of deep features and fusion over multiple views outperforms single-view baselines on the NYUDv2 benchmark for semantic segmentation. Our end-to-end trained network achieves state-of-the-art performance on the NYUDv2 dataset in single-view segmentation as well as multi-view semantic fusion.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1703.08866", "mag": "2951620021", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/MaSKC17", "doi": "10.1109/iros.2017.8202213"}}, "content": {"source": {"pdf_hash": "7d54fe9c008733f1f19eec2a0b0eddf7a646ab52", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1703.08866v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1703.08866", "status": "GREEN"}}, "grobid": {"id": "ed182db370c2e38f554341ab582487f167018224", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7d54fe9c008733f1f19eec2a0b0eddf7a646ab52.txt", "contents": "\nMulti-View Deep Learning for Consistent Semantic Mapping with RGB-D Cameras\n\n\nLingni Ma \nJ\u00f6rg St\u00fcckler \nChristian Kerl \nDaniel Cremers \nMulti-View Deep Learning for Consistent Semantic Mapping with RGB-D Cameras\n\nVisual scene understanding is an important capability that enables robots to purposefully act in their environment. In this paper, we propose a novel approach to object-class segmentation from multiple RGB-D views using deep learning. We train a deep neural network to predict object-class semantics that is consistent from several view points in a semi-supervised way. At test time, the semantics predictions of our network can be fused more consistently in semantic keyframe maps than predictions of a network trained on individual views. We base our network architecture on a recent single-view deep learning approach to RGB and depth fusion for semantic object-class segmentation and enhance it with multi-scale loss minimization. We obtain the camera trajectory using RGB-D SLAM and warp the predictions of RGB-D images into ground-truth annotated frames in order to enforce multi-view consistency during training. At test time, predictions from multiple views are fused into keyframes. We propose and analyze several methods for enforcing multi-view consistency during training and testing. We evaluate the benefit of multi-view consistency training and demonstrate that pooling of deep features and fusion over multiple views outperforms single-view baselines on the NYUDv2 benchmark for semantic segmentation. Our end-to-end trained network achieves stateof-the-art performance on the NYUDv2 dataset in single-view segmentation as well as multi-view semantic fusion.\n\nI. INTRODUCTION\n\nIntelligent robots require the ability to understand their environment through parsing and segmenting the 3D scene into meaningful objects. The rich appearance-based information contained in images renders vision a primary sensory modality for this task.\n\nIn recent years, large progress has been achieved in semantic object-class segmentation of images. Most current stateof-the-art approaches apply deep learning for this task. With RGB-D cameras, appearance as well as shape modalities can be combined to improve the semantic segmentation performance. Less explored, however, is the usage and fusion of multiple views onto the same scene which appears naturally in the domains of 3D reconstruction and robotics. Here, the camera is moving through the environment and captures the scene from multiple view points. Semantic SLAM aims at aggregating several views in a consistent 3D geometric and semantic reconstruction of the environment.\n\nIn this paper, we propose a novel approach for using multiview context for deep learning of semantic segmentation of RGB-D images. We base our network architecture on a 1 Lingni Ma, Christian Kerl \n\u00b7 \u00b7 \u00b7 MVCL F i \u03c9(x, \u03be i ) F j \u03c9(x, \u03be j )\n\nFig. 1:\n\nWe train CNN to predict multi-view consistent semantic segmentations for RGB-D images. The key innovations are the multiview consistency layers (MVCL), which warp semantic prediction or feature maps at multiple scales into a common reference view based on the SLAM trajectory. Our approach improves performance for single-view segmentation and is specifically beneficial for multiview fused segmentation at test time.\n\nrecently proposed deep convolutional neural network (CNN) for RGB and depth fusion [1] and enhance the approach with multi-scale loss minimization. Based on the trajectory estimate obtained through RGB-D simultaneous localization and mapping (SLAM), we train our CNN to predict multi-view consistent semantics in individual images. We propose and evaluate several approaches for enforcing multi-view consistency during training. A shared principle in our approaches is to use the SLAM trajectory estimate for warping network outputs or feature maps from nearby frames to keyframes with ground truth annotations. By this, the network not only learns features that are invariant under view-point change.\n\nOur semi-supervised training approach also makes better use of the annotated ground truth data than single-view learning. This alleviates the need for large amounts of annotated training data which is expensive to obtain for real imagery.  The CNN encoder-decoder architecture used in our approach. Input to our network are RGB-D images. The network extracts features from depth images in a separate encoder whose features are fused with RGB features in a fused encoder network. The encoded features at the lowest resolution are successively refined through deconvolutions in a decoder. To guide the refinement, we train the network in a deeply-supervised manner in which segmentation loss is computed at all scales of the decoder.\n\nenforcing multi-view consistency during training improves fusion at test time significantly over fusing predictions from networks trained on single views. Our end-to-end trained network achieves state-of-the-art performance on the NYUDv2 dataset in single-view segmentation as well as multi-view semantic fusion. While the fused keyframe segmentation can be directly used in robotic perception, our approach can also be useful as a building block for geometric and semantic SLAM using RGB-D cameras.\n\nII. RELATED WORK Recently, remarkable progress has been achieved in semantic image segmentation using deep neural networks and, in particular, CNNs. On many benchmarks, these approaches excell previous techniques by a great margin. As one early attempt, Couprie et al. [2] propose a multiscale CNN architecture to combine information at different perceptive field resolutions and achieved reasonable segmentation results. They were also one of the first to train a CNN with depth information for RGB-D image segmentation. Gupta et al. [3] integrate depth into the R-CNN approach by Girshick et al. [4] to detect objects in RGB-D images. To apply a CNN pretrained on ImageNet on the depth images, they propose to transform depth into a disparity, height and angle encoding. For semantic segmentation, they train a classifier to label superpixels based on the CNN features. Long et al. [5] propose a fully convolutional network (FCN) which enables end-to-end training of a deep CNN for semantic segmentation. Their FCN architecture reduces the input's spatial resolution by a great factor through layers of filtering and pooling. It fuses low with high resolution predictions to obtain the final prediction. Inspired by FCN and autoencoder networks [6], encoder-decoder architectures have been proposed for semantic segmentation [7]. For RGB-D images, Eigen et al. [8] propose multi-task CNN training that aims to predict depth, surface normals and semantics with one uniform network and achieve very good performance.\n\nFuseNet [1] proposes a principled approach for fusing RGB and depth cues in a single encoder-decoder CNN trained end-to-end for semantic image segmentation. Li et al. [9] use a LSTM recurrent neural network to fuse RGB and depth cues and obtain smooth predictions. Lin et al. [10] design a CNN that corresponds to a conditional random field (CRF) and use piecewise training to learn both unary and pairwise potentials end-to-end. While this method produces very good results, it requires a mean-field approximation for coarse inference and high-resolution refinement using a dense CRF [11]. Our approach trains a network on multiview consistency and fuses the results from multiple view points. It is complementary to the above mentioned singleview CNN approaches.\n\nIn the domain of semantic SLAM, Salas-Moreno et al. [12] developed the SLAM++ algorithm to perform RGB-D tracking and mapping at the object instance level. This method works well for indoor scenes which contain many repeated objects with predefined CAD models in a database. Hermans et al. [13] proposed 3D semantic mapping for indoor RGB-D sequences based on RGB-D visual odometry and a random forest classifier that performs semantic image segmentation. The individual frame segmentations are projected into 3D and a dense CRF [11] on the point cloud smoothes the semantic segmentation in 3D. St\u00fcckler et al. [14] perform RGB-D SLAM and probabilistically fuse the semantic segmentations of individual frames obtained with a random forest in multi-resolution voxel maps. Recently, Armeni et al. [15] propose a hierarchical parsing method for large-scale 3D point clouds of indoor environments. They first seperate point clouds into disjoint spaces, i.e., single rooms, and then further cluster points at the object level according to handcrafted features.\n\nIn contrast to the popularity of CNNs for image-based segmentation, it is less common to apply CNNs for semantic segmentation on multi-view 3D reconstructions. This is partially due to the lack of an organized structure in point clouds or the less managable scale of volumetric representations for training a deep neural network. Recently, Riegler et al. [16] apply 3D CNNs on sparse octree data structures to perform semantic segmentation on voxels. Nevertheless, the volumetric representations may discard details through the voxelization which are present at the original image resolution. McCormac et al. [17] proposed to fuse CNN semantic image segmentations on a 3D surfel map [18]. In contrast to our approach, this method does not use multiview consistency during CNN training and cannot leverage the view-point invariant features learned by our network. Closely related to our approach for enforcing multi-view consistency is the approach by Su et al. [19] who investigate the task of 3D shape recognition. They render multiple views onto 3D shape models which are fed into a CNN feature extraction stage that is shared across views. The features are max-pooled across view-points and fed into a second CNN stage that is trained for shape recognition. Our approach uses multi-view pooling for the task of semantic segmentation and is trained using realistic imagery and SLAM pose estimates. Our trained network is able to classify single views, but we demonstrate that multi-view fusion using the network trained on multi-view consistency improves segmentation performance over single-view trained networks.\n\n\nIII. CNN APPROACH TO SEMANTIC RGB-D IMAGE SEGMENTATION\n\nIn this section, we detail our CNN architecture for semantic segmentation in RGB-D images. We base our approach on FuseNet [1] which consistently fuses RGB and depth images for semantic segmentation, and enhance the approach with multi-scale loss minimization. By this, we already achieve a significant improvement in semantic segmentation performance on single views.\n\n\nA. Network Architecture\n\nFig. 2 illustrates our CNN architecture for semantic image segmentation in RGB-D images. The network follows an encoder-decoder design, similar to previous work on semantic segmentation [7]. The encoder extracts a hierarchy of features through convolutional layers and aggregates spatial information by pooling over a local neighborhood to increase the perceptive field. The last layer of the encoder outputs high dimensional feature maps with low spatial resolution. The decoder then upsamples the low-resolution feature maps through several layers of memorized unpooling and deconvolution and successively refines the low-resolution feature maps back to the input resolution. To learn features from RGB-D images, we adopt the FuseNet architecture [1] which is shown to be more efficient in learning features from RGB-D images in comparison to simple concatenation of RGB and depth or to the use of HHA [3] representation. As demonstrated in Fig. 2, the network contains two branches each learning features from RGB (F rgb ) and depth (F d ), respectively. The feature maps from the depth branch are consistently fused into the RGB branch at each scale. We denote the fusion of the feature maps by F rgb \u2295 F d .\n\nFor semantic segmentation, the label set is denoted as L = {1, 2, . . . , K} and the category index is indicated with subscript j. Following notation convention, we compute the classification score S = (s 1 , s 2 , . . . , s K ) at spatial location x and map it to the probability distribution P = (p 1 , p 2 , . . . , p K ) with the softmax function \u03c3(\u00b7). Network inference obtains the probability\np j (x, W | I) = \u03c3(s j (x, W)) = exp(s j (x, W)) K k exp(s k (x, W)) ,(1)\nof all pixels x in the image for being labelled as class j, given input RGB-D image I and network parameters W.\n\n\nB. Multi-Scale Loss Minimization\n\nWe use the cross-entropy loss to learn network parameters for semantic segmentation from ground-truth annotations l gt ,\nL(W) = \u2212 1 N N i K j j = l gt log p j (x i , W | I) ,(2)\nwhere N is the number of pixels. This loss minimizes the Kullback-Leibler (KL) divergence between predicted distribution and the ground-truth, assuming the ground-truth has a one-hot distribution on the true label. The encoder of our network contains five pooling layers of 2\u00d72 filter size and downsamples the input resolution by a factor of 32. The decoder learns to refine the low resolution back to the original with five scales of memoried unpooling layers followed by deconvolution. In order to guide the decoder through the successive refinement, we adopt a deeply supervised learning method [20], [21] and compute the crossentropy loss at all upsample scales. To this end, we append a classification layer at each deconvolution scale and compute the loss for the respective resolution ground-truth which is obtained through stochastic pooling [22] of the full resolution annotation (see Fig. 3 for an example).\n\n\nIV. MULTI-VIEW CONSISTENCY\n\nThe key innovation of this work is to explore the use of temporal multi-view consistency within an RGB-D sequence for CNN training and prediction. While convolutional neural networks (CNN) have been shown to obtain the state-of-theart semantic segmentation performances for many datasets, most of these studies focus on single views. When observing a scene from a moving camera such as on a mobile robot, the system obtains multiple different views onto the same objects. We aim to use this for increasing the consistency of semantic maps by fusing semantic image segmentations in keyframes from multiple view points. Moreover, we can make use of the multi-view information in RGB-D video for training a CNN to produce consistent semantic segmentations under view-point changes.\n\nWe define each training sequence to contain one reference keyframe I k with ground-truth semantic annotation and several nearby overlapping frames I i . The relative poses \u03be of the nearby frames towards the reference keyframe are estimated through a SLAM method such as [23]. In order to impose temporal consistency, we adopt the warping concept from multiview geometry to associate pixels between view points. To this end, we introduce warping layers into the CNN architecture that synthesize the CNN output at any stage in one view point by sampling the output of another view point based on the SLAM pose estimate. These layers can be seen as a variant of spatial transformers [24]. Through these warping layers, it is possible to impose temporal multi-view consistency. In the following, we describe our warping layers and introduce several variants of multi-view consistency constraints based on warping.\n\n\nA. Multiview Association Through Warping\n\nGiven the normalized 2D image coordinate x \u2208 R 2 , its warped image location\nx \u03c9 := \u03c9(x, \u03be) = \u03c0 T(\u03be) \u03c0 \u22121 (x, Z i (x))(3)\nis determined through the warping function \u03c9(x, \u03be) which transforms the location from one camera frame to the other based on the depth Z i (x) at x in image I i and the SLAM pose estimate \u03be. The functions \u03c0 and its inverse \u03c0 \u22121 project homogeneous 3D coordinates to normalized image coordinates and vice versa, while T(\u03be) denotes the homogeneous transformation matrix for pose \u03be.\n\nThe warping function associates pixels between two viewpoints. Using this association, it is possible to synthesize the output of any CNN layer in one view point by sampling the output of another view point. For a network with several spatial resolutions, the warping grid only needs to be computed once at the input resolution. For this purpose, we normalize the warping grid by the input resolution to obtain a canonical representation within the range of [\u22121, 1]. The canonical representation enables efficient generation of warping grids at any lower resolution through average pooling layers. Using bilinear interpolation, it is then straight-forward to synthesize the output at any scale and gradients can be back-propagated through the warping layer. With a slight abuse of notation, we denote the operation of synthesizing the layer output F given the warping by F \u03c9 := F(\u03c9(x, \u03be)).\n\n\nB. Consistency Through Warp Augmentation\n\nOne way to enforce multi-view consistency in the segmentation is to warp the predictions of nearby frames into the ground-truth annotated keyframe and computing a supervised loss there. This approach can be interpreted as a kind of data augmentation using the available nearby frames.\n\nWe implement this consistency method by warping the keyframe into the nearby frame, and synthesize the classification score of the nearby frame from the keyframe's view point. We compute the cross-entropy loss on this synthesized semantic segmentation. Within RGB-D sequences, objects can appear at various scales, image locations and view angles. Propagating the keyframe annotation into the other frames implicitly regulates the network predictions to be invariant under these transformations.\n\n\nC. Consistency Through Bayesian Fusion\n\nGiven a sequence of measurements and predictions at test time, Bayesian fusion is frequently applied to aggregate the semantic segmentations of individual views. Without a loss of generality, let us denote the semantic labelling of a pixel by y and its measurement in frame i by z i . We use the notation z i for the set of measurements up to frame i. According to Bayes rule,\np(y | z i ) = p(z i | y, z i\u22121 ) p(y | z i\u22121 ) p(z i | z i\u22121 ) (4) = \u03b7 i p(z i | y, z i\u22121 ) p(y | z i\u22121 )(5)\nSuppose measurements satisfy the i.i.d. condition, i.e. p(z i | y, z i\u22121 ) = p(z i | y), and equal a-priori probability for each class, then Equation (4) simplifies to\np(y | z i ) = \u03b7 i p(z i | y) p(y | z i\u22121 ) = i \u03b7 i p(z i | y). (6)\nPut simple, Bayesian fusion is implemented by taking the product over the individual frame semantic labelling likelihoods at a pixel and normalizing the product to yield a valid probability distribution. This process can also be implemented recursively on a sequence of frames. When training our CNN for multi-view consistency using Bayesian fusion, we warp the predictions of nearby frames into the keyframe using the SLAM pose estimate. We obtain the fused prediction at each keyframe pixel by summing the unnormalized log labelling likelihoods instead of the individual frame softmax outputs. Applying softmax on the sum of log labelling likelihoods yields the fused labelling probability distribution. This method is equivalent to Equation (6) since\ni p \u03c9 i,j K k i p \u03c9 i,k = i \u03c3(s \u03c9 i,j ) K k i \u03c3(s \u03c9 i,k ) = \u03c3 i s \u03c9 i,j ,(7)\nwhere s \u03c9 i,j and p \u03c9 i,j denote the warped classification scores and probabilities, respectively, and \u03c3(\u00b7) is the softmax function as defined in Equation (1).\n\n\nD. Consistency Through Multi-View Max-Pooling\n\nWhile Bayesian fusion provides an approach to integrate several measurements in the probability space, we also explore direct fusion in the feature space using multi-view maxpooling of the warped feature maps. We warp the feature maps preceding the classification layers at each scale in our decoder into the keyframe and apply max-pooling over corresponding feature activations at the same warped location to obtain a pooled feature map in the keyframe,\nF = max pool(F \u03c9 1 , F \u03c9 2 , . . . , F \u03c9 N ).(8)\nThe fused feature maps are classified and the resulting semantic segmentation is compared to the keyframe groundtruth for loss calculation.\n\n\nV. EVALUATION\n\nWe evaluate our proposed approach using the NYUDv2 [25] RGB-D dataset. The NYUDv2 dataset provides 1449 pixel-wise annotated RGB-D images that is commonly split into a subset of 795 frames for training/validation (trainval) and 654 frames for testing. The dataset contains various indoor environments captured with consumer RGB-D cameras. The original sequences that contain these 1449 images are also available. Using the DVO-SLAM algorithm [23], we determine the camera motion around each annotated keyframe to obtain training and test sequences. As a result, we obtain sequences with in total 267,675 frames, despite that tracking fails for 30 out of 1449 keyframes. Following the original trainval/test split, we use 770 sequences consisting of 143,670 total frames for training and 649 sequences with 124,005 frames for testing. For benchmarking, we evaluate our method for the NYUDv2 13-class [2] and 40-class [26] semantic segmentation tasks. For training and testing, we use the raw depth images without inpainted missing values. While larger RGB-D datasets with ground-truth annotation are available, unfortunately they do not provide sequences.\n\n\nA. Training Details\n\nWe implemented our approach using the Caffe framework [27]. For all experiments, the network parameters are initialized as follows. For the convolutional kernels in the encoder, we use the pretrained 16-layer VGGNet model [28] and for the deconvolutional kernels in the decoder, we use He initialization [29]. We train the network with stochastic gradient descent (SGD) [30] with 0.9 momentum, 0.0005 weight decay and set the batch size to 4. The learning rate is set to 0.001 and multiplied by a factor of 0.9 every 30,000 iterations. We apply random shuffling after each epoch and train the network until convergence. All the images are resized to a resolution of 320 \u00d7 240 pixels as input to the network and the predictions are also up to this scale. We use cubic interpolation to downsample RGB images and nearest-neighbor interpolation to downsample depth and label images. Most of the keyframes have long tracking sequences, where tracking drift typically accumulates along the sequence. Hence for multi-view training, we feed the close-by frames first to the network and gradually include 10 further-away frames in 5 epochs.\n\n\nB. Evaluation Criteria\n\nWe measure the semantic segmentation performance of our network with three criteria: global pixelwise accuracy, average classwise accuracy and average intersectionover-union (IoU) scores. These three criteria can be calculated from the confusion matrix C \u2208 R K\u00d7K . Each element in the confusion matrix c ij is the total amount of pixels belonging to class i which are predicted to be class j. The global pixelwise accuracy is computed by i c ii / ij c ij and the average classwise accuracy is computed by 1 K i (c ii / j c ij ). The average IoU score is calculated according to 1\nK i c ii /( i c ij + j c ij \u2212 c ii ) .\n\nC. Single Frame Segmentation\n\nIn a first set of experiments, we evaluate the performance of several variants of our network for direct semantic segmentation of frames. This means we do not fuse predictions from nearby frames to obtain the final prediction in frame. We predict semantic segmentation with our trained models on the 654 test images of the NYUDv2 dataset and compare groundtruth Eigen et al. [8] FuseNet-SF3 [1] MVCNet-Mono\n\n\nMVCNet-Augment\n\n\nMVCNet-Bayesian\n\n\nMVCNet-MaxPool\n\nMVCNet-MaxPool-F Fig. 4: Qualitative semantic segmentation results of our methods and several state-of-the-art baselines on NYUDv2 13-class segmentation (see Table III for color coding, left columns: semantic segmentation, right columns: falsely classified pixels, black is void). Our multi-view consistency trained models produce more accurate and homogeneous results than single-view methods. Bayesian fusion further improves segmentation quality (e.g. MVCNet-MaxPool-F).\n\nour methods with state-of-art approaches. The results are shown in Table I. Unless otherwise stated, we take the results from the original papers for comparison and report their best results (i.e. SceneNet-FT-NYU-DO-DHA model for SceneNet [31], VGG-based model for Eigen et al. [8]). The result of Hermans et al. [13] is obtained after applying a dense CRF [11] for each image and in-between neighboring 3D points to further smoothen their results. We also remark that the results reported here for the Context-CRF model are finetuned on NYUDv2 like in our approach to facilitate comparison. Furthermore, the network output is refined using a dense CRF [11] which is claimed to increase the accuracy of the network by approximately 2%. The results for FuseNet-SF3 are obtained by our own implementation. Our baseline model MVCNet-Mono is trained without multiview consistency, which amounts to FuseNet with multiscale deeply supervised loss at decoder. However, we apply single image augmentation to train the FuseNet-SF3 and MVCNet- \n\n\nD. Multi-View Fused Segmentation\n\nSince we train on sequences, in the second set of experiment, we also evaluate the fused semantic segmentation over the test sequences. The number of fused frames is fixed to 50, which are uniformly sampled over the entire sequence. Due to the lack of ground-truth for neighboring frames, we fuse the prediction of neighboring frames in the keyframes using Bayesian fusion according to Equation (7). This fusion is typically applied for semantic mapping using RGB-D SLAM. The results are shown in Table II. Bayesian multi-view fusion improves the semantic segmentation by approx. 2% on all evaluation measures towards single-view segmentation. Also, the training for multi-view consistency achieves a stronger gain over single-view training (MVCNet-Mono) when fusing segmentations compared to single-view segmentation. This performance gain is observed in the qualitative results in Fig. 4. It can be seen that our multi-view consistency training and Bayesian fusion produces more accurate classifications and more homogeneous segmentations. Fig. 5 shows typical challenging cases for our model.\n\nWe also compare classwise and average IoU scores for 13class semantic segmentation on NYUDv2 in Table III. The results of Eigen et al. [8] are from their publicly available model tested on 320\u00d7240 resolution. The results demonstrate that our approach gives high performance gains across all occurence frequencies of the classes in the dataset.\n\n\nVI. CONCLUSION\n\nIn this paper we propose methods for enforcing multiview consistency during the training of CNN models for semantic RGB-D image segmentation. We base our CNN design on FuseNet [1], a recently proposed CNN architecture in an encoder-decoder scheme for semantic segmentation of RGB-D images. We augment the network with multi-scale RGB image, ground-truth, single-view prediction on keyframe, multi-view prediction fused in keyframe). On the left, the network fails to classify the objects for all frames. In the middle, the network makes some errors in single-view prediction, but through multi-view fusion, some mistakes are corrected. On the right, multi-view fusion degenerates performance due to the mirror reflections. loss supervision to improve its performance. We present and evaluate three different approaches for multi-view consistency training. Our methods use an RGB-D SLAM trajectory estimate to warp semantic segmentations or feature maps from one view point to another. Multi-view max-pooling of feature maps overall provides the best performance gains in single-view segmentation and fusion of multiple views.\n\nWe demonstrate the superior performance of multi-view consistency training and Bayesian fusion on the NYUDv2 13-class and 40-class semantic segmentation benchmark. All multi-view consistency training approaches outperform single-view trained baselines. They are key to boosting segmentation performance when fusing network predictions from multiple view points during testing. On NYUDv2, our model sets a new state-of-the-art performance using an endto-end trained network for single-view predictions as well as multi-view fused semantic segmentation without further postprocessing stages such as dense CRFs.\n\nIn future work, we want to further investigate integration of our approach in a semantic SLAM system, for example, through seamless coupling of pose tracking and SLAM with our semantic predictions.\n\nFig. 2 :\n2Fig. 2: The CNN encoder-decoder architecture used in our approach. Input to our network are RGB-D images. The network extracts features from depth images in a separate encoder whose features are fused with RGB features in a fused encoder network. The encoded features at the lowest resolution are successively refined through deconvolutions in a decoder. To guide the refinement, we train the network in a deeply-supervised manner in which segmentation loss is computed at all scales of the decoder.\n\nFig. 3 :\n3Example of multi-scale ground-truth and predictions. Upper row: successive subsampled of ground-truth annotation obtained through stochastic pooling. Lower row: CNN prediction on each scale. The resolutions are coarse to fine from left to right with 20 \u00d7 15, 40 \u00d7 30, 80 \u00d7 60, 160 \u00d7 120 and 320 \u00d7 240.\n\nFig. 5 :\n5Challenging cases for MVCNet-MaxPool-F (top to bottom:\n\n\nand Daniel Cremers are with the Computer Vision Group, Department of Computer Science, Technical University of Munich {lingni,kerl,cremers}@in.tum.de2 \n\nJ\u00f6rg St\u00fcckler is with the Computer Vision Group, \nVisual \nComputing \nInstitute, \nRWTH \nAachen \nUniversity, \n\nstueckler@vision.rwth-aachen.de \n\nframe I i \n\nCNN: f (I i , W) \n\nframe I i \n\nCNN: f (I j , W) \n\n\n\n\nComplementary to our training approach, we aggregate the predictions of our trained network in consistent semantic segmentations of keyframes at test time. The predictions of nearby overlapping images along the camera trajectory are fused into the keyframe based on the SLAM estimate in a probabilistic way.In experiments, we evaluate the performance gain achieved through multi-view training and fusion at test time over single-view approaches. Our results demonstrate that multiview max-pooling of feature maps during training best supports multi-view fusion at test time. Overall we find that arXiv:1703.08866v1 [cs.CV] 26 Mar 2017RGB sequence \n\nconv1 \nconv2 \nconv3 \nconv4 \nconv5 \n\nF rgb \u2295 F d \n\ndepth sequence \n\nconv1-d \n\nconv2-d \nconv3-d \n\ndeconv5 \ndeconv4 \n\ndeconv3 \n\ndeconv2 \n\ndeconv1 keyframe \nsegmentation \n\nL 1 \nL 2 \nL 3 \nL 4 \nL 5 \n\n15 \u00d7 20 \n30 \u00d7 40 \n60 \u00d7 80 120 \u00d7 160 240 \u00d7 320 \n\nencoder \ndepth fusion \ndecoder \nmultiscale loss supervision \n\n\n\nTABLE I :\nISingle-view semantic segmentation accuracy of our network in comparison to the state-of-the-art methods for NYUDv2 13-class and 40-class segmentation tasks.methods \ninput \npixelwise classwise IoU \n\nNYUDv2 \n\n13 classes \n\nCouprie et al. [2] \nRGB-D \n52.4 \n36.2 \n-\nHermans et al. [13] \nRGB-D \n54.2 \n48.0 \n-\nSceneNet [31] \nDHA \n67.2 \n52.5 \n-\nEigen et al. [8] \nRGB-D-N \n75.4 \n66.9 \n52.6 \nFuseNet-SF3 [1] \nRGB-D \n75.8 \n66.2 \n54.2 \nMVCNet-Mono \nRGB-D \n77.6 \n68.7 \n56.9 \nMVCNet-Augment \nRGB-D \n77.6 \n69.3 \n57.2 \nMVCNet-Bayesian \nRGB-D \n77.8 \n69.4 \n57.3 \nMVCNet-MaxPool \nRGB-D \n77.7 \n69.5 \n57.3 \n\nNYUDv2 \n\n40 classes \n\nRCNN [3] \nRGB-HHA \n60.3 \n35.1 \n28.6 \nFCN-16s [5] \nRGB-HHA \n65.4 \n46.1 \n34.0 \nEigen et al. [8] \nRGB-D-N \n65.6 \n45.1 \n34.1 \nFuseNet-SF3 [1] \nRGB-D \n66.4 \n44.2 \n34.0 \nContext-CRF [10] \nRGB \n67.6 \n49.6 \n37.1 \nMVCNet-Mono \nRGB-D \n68.6 \n48.7 \n37.6 \nMVCNet-Augment \nRGB-D \n68.6 \n49.9 \n38.0 \nMVCNet-Bayesian \nRGB-D \n68.4 \n49.5 \n37.4 \nMVCNet-MaxPool \nRGB-D \n69.1 \n50.1 \n38.0 \n\n\n\nTABLE II :\nIIMulti-view segmentation accuracy of our network using Bayesian fusion for NYUDv2 13-class and 40-class segmentation. This data augmentation is not used fro multi-view training. Nevertherless, our results show that the different variants of multi-view consistency training outperform the state-of-art methods for single image semantic segmentation. Overall, multi-view max-pooling (MVCNet-MaxPool) has a small advantage over the other multi-view consistency training approaches (MVCNet-Augment and MVCNet-Bayesian).methods \npixelwise \nclasswise \nIoU \n\nNYUDv2 \n\n13 classes \n\nFuseNet-SF3 [1] \n77.19 \n67.46 \n56.01 \nMVCNet-Mono \n78.70 \n69.61 \n58.29 \nMVCNet-Augment \n78.94 \n70.48 \n58.93 \nMVCNet-Bayesian \n79.13 \n70.48 \n59.04 \nMVCNet-MaxPool \n79.13 \n70.59 \n59.07 \n\nNYUDv2 \n\n40 classes \n\nFuseNet-SF3 [1] \n67.74 \n44.92 \n35.36 \nMVCNet-Mono \n70.03 \n49.73 \n39.12 \nMVCNet-Augment \n70.34 \n51.73 \n40.19 \nMVCNet-Bayesian \n70.24 \n51.18 \n39.74 \nMVCNet-MaxPool \n70.66 \n51.78 \n40.07 \n\nMono with random scaling between [0.8, 1.2], random crop \nand mirror. \n\nTABLE III :\nIIINYUDv2 13-class semantic segmentation IoU scores. Our method achieves best per-class accuracy and average IoU. 38.29 50.23 54.76 64.50 89.76 45.20 47.85 42.47 74.34 56.24 45.72 34.34 53.88 FuseNet-SF3 [1] 61.52 37.95 52.67 53.97 64.73 89.01 47.11 57.17 39.20 75.08 58.06 37.64 29.77 54.14 MVCNet-Mono 65.27 37.82 54.09 59.39 65.26 89.15 49.47 57.00 44.14 75.31 57.22 49.21 36.14 56.88 MVCNet-Augment 65.33 38.30 54.15 59.54 67.65 89.26 49.27 55.18 43.39 74.59 58.46 49.35 38.84 57.18 MVCNet-Bayesian 65.76 38.79 54.60 59.28 67.58 89.69 48.98 56.72 42.42 75.26 59.55 49.27 36.51 57.26 MVCNet-MaxPool 65.71 39.10 54.59 59.23 66.41 89.94 49.50 56.30 43.51 75.33 59.11 49.18 37.37 57.33method \nbed \nobjects \nchair \nfurniture \nceiling \nfloor \ndecorat. \n\nsofa \ntable \nwall \nwindow \nbooks \nTV \naverage \naccuracy \n\nclass frequency \n4.08 7.31 \n3.45 12.71 \n1.47 \n9.88 \n3.40 \n2.84 \n3.42 24.57 \n4.91 \n2.78 \n0.99 \n\nsingle-view \nEigen et al. [8] \n56.71 multi-view \nFuseNet-SF3 [1] \n64.95 39.62 55.28 55.90 64.99 89.88 47.99 60.17 42.40 76.24 59.97 39.80 30.91 56.01 \nMVCNet-Mono \n67.11 40.14 56.39 60.90 66.07 89.77 50.32 59.49 46.12 76.51 59.03 48.80 37.13 58.29 \nMVCNet-Augment \n68.22 40.04 56.55 61.82 67.88 90.06 50.85 58.00 45.98 75.85 60.43 50.50 39.89 58.93 \nMVCNet-Bayesian \n68.38 40.87 57.10 61.84 67.98 90.64 50.05 59.70 44.73 76.50 61.75 51.01 36.99 59.04 \nMVCNet-MaxPool \n68.09 41.58 56.88 61.56 67.21 90.64 50.69 59.73 45.46 76.68 61.28 50.60 37.51 59.07 \n\n\n\nFusenet: incorporating depth into semantic segmentation via fusion-based cnn architecture. C Hazirbas, L Ma, C Domokos, D Cremers, Proceedings of the 13th Asian Conference on Computer Vision (ACCV). the 13th Asian Conference on Computer Vision (ACCV)C. Hazirbas, L. Ma, C. Domokos, and D. Cremers, \"Fusenet: incor- porating depth into semantic segmentation via fusion-based cnn archi- tecture,\" in Proceedings of the 13th Asian Conference on Computer Vision (ACCV), 2016.\n\nIndoor semantic segmentation using depth information. C Couprie, C Farabet, L Najman, Y Lecun, C. Couprie, C. Farabet, L. Najman, and Y. Lecun, Indoor semantic segmentation using depth information. April 2013.\n\nLearning rich features from RGB-D images for object detection and segmentation. S Gupta, R Girshick, P Arbel\u00e1ez, J Malik, European Conference on Computer Vision (ECCV). S. Gupta, R. Girshick, P. Arbel\u00e1ez, and J. Malik, \"Learning rich features from RGB-D images for object detection and segmentation,\" in European Conference on Computer Vision (ECCV), 2014.\n\nRich feature hierarchies for accurate object detection and semantic segmentation. R Girshick, J Donahue, T Darrell, J Malik, Computer Vision and Pattern Recognition (CVPR). R. Girshick, J. Donahue, T. Darrell, and J. Malik, \"Rich feature hierarchies for accurate object detection and semantic segmentation,\" in Computer Vision and Pattern Recognition (CVPR), 2014.\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). J. Long, E. Shelhamer, and T. Darrell, \"Fully convolutional networks for semantic segmentation,\" IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Nov. 2015.\n\nGreedy layerwise training of deep networks. Y Bengio, P Lamblin, D Popovici, H Larochelle, Advances in Neural Information Processing Systems 19 (NIPS). Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, \"Greedy layer- wise training of deep networks,\" in Advances in Neural Information Processing Systems 19 (NIPS), pp. 153-160, 2007.\n\nLearning deconvolution network for semantic segmentation. H Noh, S Hong, B Han, Proceedings of the IEEE International Conference on Computer Vision (CVPR). the IEEE International Conference on Computer Vision (CVPR)H. Noh, S. Hong, and B. Han, \"Learning deconvolution network for semantic segmentation,\" in Proceedings of the IEEE International Conference on Computer Vision (CVPR), pp. 1520-1528, 2015.\n\nPredicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. D Eigen, R Fergus, IEEE International Conference on Computer Vision (ICCV). D. Eigen and R. Fergus, \"Predicting depth, surface normals and se- mantic labels with a common multi-scale convolutional architecture,\" in IEEE International Conference on Computer Vision (ICCV), 2015.\n\nLSTM-CF: Unifying context modeling and fusion with LSTMs for RGB-D scene labeling. Z Li, Y Gan, X Liang, Y Yu, H Cheng, L Lin, European Conference on Computer Vision (ECCV). SpringerZ. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin, \"LSTM- CF: Unifying context modeling and fusion with LSTMs for RGB-D scene labeling,\" in European Conference on Computer Vision (ECCV), pp. 541-557, Springer, 2016.\n\nExploring context with deep structured models for semantic segmentation. G Lin, C Shen, A Van Den Hengel, I D Reid, abs/1603.03183CoRR. G. Lin, C. Shen, A. van den Hengel, and I. D. Reid, \"Exploring context with deep structured models for semantic segmentation,\" CoRR, vol. abs/1603.03183, 2016.\n\nEfficient inference in fully connected crfs with gaussian edge potentials. P Kr\u00e4henb\u00fchl, V Koltun, P. Kr\u00e4henb\u00fchl and V. Koltun, \"Efficient inference in fully connected crfs with gaussian edge potentials,\" pp. 109-117, 2011.\n\nSLAM++: Simultaneous localisation and mapping at the level of objects. R F Salas-Moreno, R A Newcombe, H Strasdat, P H Kelly, A J Davison, IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). R. F. Salas-Moreno, R. A. Newcombe, H. Strasdat, P. H. Kelly, and A. J. Davison, \"SLAM++: Simultaneous localisation and mapping at the level of objects,\" IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2013.\n\nDense 3D semantic mapping of indoor scenes from rgb-d images. A Hermans, G Floros, B Leibe, IEEE International Conference on Robotics and Automation (ICRA). A. Hermans, G. Floros, and B. Leibe, \"Dense 3D semantic mapping of indoor scenes from rgb-d images,\" in IEEE International Conference on Robotics and Automation (ICRA), pp. 2631-2638, 2014.\n\nDense realtime mapping of object-class semantics from RGB-D video. J St\u00fcckler, B Waldvogel, H Schulz, S Behnke, Journal of Real-Time Image Processing. J. St\u00fcckler, B. Waldvogel, H. Schulz, and S. Behnke, \"Dense real- time mapping of object-class semantics from RGB-D video,\" Journal of Real-Time Image Processing, 2015.\n\n3D semantic parsing of large-scale indoor spaces. I Armeni, O Sener, A R Zamir, H Jiang, I Brilakis, M Fischer, S Savarese, Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer, and S. Savarese, \"3D semantic parsing of large-scale indoor spaces,\" in Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nOctNet: Learning deep 3d representations at high resolutions. G Riegler, A O Ulusoy, A Geiger, abs/1611.05009CoRR. G. Riegler, A. O. Ulusoy, and A. Geiger, \"OctNet: Learning deep 3d representations at high resolutions,\" CoRR, vol. abs/1611.05009, 2016.\n\nSe-manticFusion: Dense 3D semantic mapping with convolutional neural networks. J Mccormac, A Handa, A J Davison, S Leutenegger, abs/1609.05130CoRR. J. McCormac, A. Handa, A. J. Davison, and S. Leutenegger, \"Se- manticFusion: Dense 3D semantic mapping with convolutional neural networks,\" CoRR, vol. abs/1609.05130, 2016.\n\nElasticFusion: Real-time dense SLAM and light source estimation. T Whelan, R F Salas-Moreno, B Glocker, A J Davison, S Leutenegger, Intl. J. of Robotics Research. T. Whelan, R. F. Salas-Moreno, B. Glocker, A. J. Davison, and S. Leutenegger, \"ElasticFusion: Real-time dense SLAM and light source estimation,\" Intl. J. of Robotics Research (IJRR), 2016.\n\nMultiview convolutional neural networks for 3d shape recognition. H Su, S Maji, E Kalogerakis, E G Learned-Miller, Proceedings of the IEEE Int. Conf. on Computer Vision (ICCV). the IEEE Int. Conf. on Computer Vision (ICCV)H. Su, S. Maji, E. Kalogerakis, and E. G. Learned-Miller, \"Multi- view convolutional neural networks for 3d shape recognition,\" in Proceedings of the IEEE Int. Conf. on Computer Vision (ICCV), 2015.\n\nDeeplysupervised nets. C Lee, S Xie, P W Gallagher, Z Zhang, Z Tu, Proc. of the 18th Int. Conf. on Artificial Intelligence and Statistics (AISTATS). of the 18th Int. Conf. on Artificial Intelligence and Statistics (AISTATS)C. Lee, S. Xie, P. W. Gallagher, Z. Zhang, and Z. Tu, \"Deeply- supervised nets,\" in Proc. of the 18th Int. Conf. on Artificial Intel- ligence and Statistics (AISTATS), 2015.\n\nFlownet: Learning optical flow with convolutional networks. A Dosovitskiy, P Fischer, E Ilg, P Hausser, C Hazirbas, V Golkov, P Van Der Smagt, D Cremers, T Brox, The IEEE International Conference on Computer Vision (ICCV). A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov, P. van der Smagt, D. Cremers, and T. Brox, \"Flownet: Learning optical flow with convolutional networks,\" in The IEEE International Conference on Computer Vision (ICCV), December 2015.\n\nStochastic pooling for regularization of deep convolutional neural networks. M Zeiler, R Fergus, M. Zeiler and R. Fergus, Stochastic pooling for regularization of deep convolutional neural networks. 2013.\n\nDense visual slam for RGB-D cameras. C Kerl, J Sturm, D Cremers, Proc. of IROS. of IROSC. Kerl, J. Sturm, and D. Cremers, \"Dense visual slam for RGB-D cameras,\" in Proc. of IROS, 2013.\n\nSpatial transformer networks. M Jaderberg, K Simonyan, A Zisserman, K Kavukcuoglu, Advances in Neural Information Processing Systems 28 (NIPS. C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. GarnettM. Jaderberg, K. Simonyan, A. Zisserman, and k. kavukcuoglu, \"Spatial transformer networks,\" in Advances in Neural Information Processing Systems 28 (NIPS) (C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, eds.), pp. 2017-2025, 2015.\n\nIndoor segmentation and support inference from RGBD images. P K Nathan Silberman, Derek Hoiem, R Fergus, European Conference on Computer Vision (ECCV). P. K. Nathan Silberman, Derek Hoiem and R. Fergus, \"Indoor seg- mentation and support inference from RGBD images,\" in European Conference on Computer Vision (ECCV), 2012.\n\nPerceptual organization and recognition of indoor scenes from RGB-D images. S Gupta, P Arbelaez, J Malik, IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). S. Gupta, P. Arbelaez, and J. Malik, \"Perceptual organization and recognition of indoor scenes from RGB-D images,\" in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2013.\n\nCaffe: Convolutional architecture for fast feature embedding. Y Jia, E Shelhamer, J Donahue, S Karayev, J Long, R Girshick, S Guadarrama, T Darrell, arXiv:1408.5093arXiv preprintY. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell, \"Caffe: Convolutional architecture for fast feature embedding,\" arXiv preprint arXiv:1408.5093, 2014.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, abs/1409.1556CoRR. K. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" CoRR, vol. abs/1409.1556, 2014.\n\nDelving deep into rectifiers: Surpassing human-level performance on imagenet classification. K He, X Zhang, S Ren, J Sun, IEEE International Conference on Computer Vision (ICCV). K. He, X. Zhang, S. Ren, and J. Sun, \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification,\" in IEEE International Conference on Computer Vision (ICCV), 2015.\n\nStochastic gradient descent tricks. L Bottou, Neural Networks: Tricks of the Trade. SpringerL. Bottou, \"Stochastic gradient descent tricks,\" in Neural Networks: Tricks of the Trade, pp. 421-436, Springer, 2012.\n\nScenenet: Understanding real world indoor scenes with synthetic data. A Handa, V Patraucean, V Badrinarayanan, S Stent, R Cipolla, IEEE Int. Conf. on Computer Vision (CVPR). A. Handa, V. Patraucean, V. Badrinarayanan, S. Stent, and R. Cipolla, \"Scenenet: Understanding real world indoor scenes with synthetic data,\" in IEEE Int. Conf. on Computer Vision (CVPR), 2016.\n", "annotations": {"author": "[{\"end\":89,\"start\":79},{\"end\":104,\"start\":90},{\"end\":120,\"start\":105},{\"end\":136,\"start\":121}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":86},{\"end\":103,\"start\":95},{\"end\":119,\"start\":115},{\"end\":135,\"start\":128}]", "author_first_name": "[{\"end\":85,\"start\":79},{\"end\":94,\"start\":90},{\"end\":114,\"start\":105},{\"end\":127,\"start\":121}]", "author_affiliation": null, "title": "[{\"end\":76,\"start\":1},{\"end\":212,\"start\":137}]", "venue": null, "abstract": "[{\"end\":1688,\"start\":214}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2819,\"start\":2818},{\"end\":2845,\"start\":2841},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3403,\"start\":3400},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5526,\"start\":5523},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5792,\"start\":5789},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5855,\"start\":5852},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6141,\"start\":6138},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6504,\"start\":6501},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6584,\"start\":6581},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6620,\"start\":6617},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6783,\"start\":6780},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6942,\"start\":6939},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7052,\"start\":7048},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7361,\"start\":7357},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7594,\"start\":7590},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7832,\"start\":7828},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8071,\"start\":8067},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8153,\"start\":8149},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8338,\"start\":8334},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8955,\"start\":8951},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9209,\"start\":9205},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9283,\"start\":9279},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9561,\"start\":9557},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10397,\"start\":10394},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10856,\"start\":10853},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11419,\"start\":11416},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11574,\"start\":11571},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13282,\"start\":13278},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13288,\"start\":13284},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13534,\"start\":13530},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14682,\"start\":14678},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15092,\"start\":15088},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20100,\"start\":20096},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20491,\"start\":20487},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20947,\"start\":20944},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20965,\"start\":20961},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21281,\"start\":21277},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21449,\"start\":21445},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21531,\"start\":21527},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21597,\"start\":21593},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23409,\"start\":23406},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23425,\"start\":23422},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24209,\"start\":24205},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24247,\"start\":24244},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24283,\"start\":24279},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24327,\"start\":24323},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24623,\"start\":24619},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26272,\"start\":26269},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26675,\"start\":26672}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28941,\"start\":28431},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29254,\"start\":28942},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29320,\"start\":29255},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":29681,\"start\":29321},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30637,\"start\":29682},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31627,\"start\":30638},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":32677,\"start\":31628},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34150,\"start\":32678}]", "paragraph": "[{\"end\":1961,\"start\":1707},{\"end\":2647,\"start\":1963},{\"end\":2846,\"start\":2649},{\"end\":3315,\"start\":2898},{\"end\":4018,\"start\":3317},{\"end\":4751,\"start\":4020},{\"end\":5252,\"start\":4753},{\"end\":6770,\"start\":5254},{\"end\":7536,\"start\":6772},{\"end\":8594,\"start\":7538},{\"end\":10212,\"start\":8596},{\"end\":10639,\"start\":10271},{\"end\":11879,\"start\":10667},{\"end\":12279,\"start\":11881},{\"end\":12465,\"start\":12354},{\"end\":12622,\"start\":12502},{\"end\":13597,\"start\":12680},{\"end\":14406,\"start\":13628},{\"end\":15317,\"start\":14408},{\"end\":15438,\"start\":15362},{\"end\":15863,\"start\":15484},{\"end\":16754,\"start\":15865},{\"end\":17083,\"start\":16799},{\"end\":17580,\"start\":17085},{\"end\":17999,\"start\":17623},{\"end\":18276,\"start\":18109},{\"end\":19097,\"start\":18344},{\"end\":19334,\"start\":19175},{\"end\":19838,\"start\":19384},{\"end\":20027,\"start\":19888},{\"end\":21199,\"start\":20045},{\"end\":22354,\"start\":21223},{\"end\":22960,\"start\":22381},{\"end\":23437,\"start\":23031},{\"end\":23964,\"start\":23491},{\"end\":25000,\"start\":23966},{\"end\":26132,\"start\":25037},{\"end\":26477,\"start\":26134},{\"end\":27621,\"start\":26496},{\"end\":28231,\"start\":27623},{\"end\":28430,\"start\":28233}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":2887,\"start\":2847},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12353,\"start\":12280},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12679,\"start\":12623},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15483,\"start\":15439},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18108,\"start\":18000},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18343,\"start\":18277},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19174,\"start\":19098},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19887,\"start\":19839},{\"attributes\":{\"id\":\"formula_8\"},\"end\":22999,\"start\":22961}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23658,\"start\":23649},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24040,\"start\":24033},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25542,\"start\":25534},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26239,\"start\":26230}]", "section_header": "[{\"end\":1705,\"start\":1690},{\"end\":2896,\"start\":2889},{\"end\":10269,\"start\":10215},{\"end\":10665,\"start\":10642},{\"end\":12500,\"start\":12468},{\"end\":13626,\"start\":13600},{\"end\":15360,\"start\":15320},{\"end\":16797,\"start\":16757},{\"end\":17621,\"start\":17583},{\"end\":19382,\"start\":19337},{\"end\":20043,\"start\":20030},{\"end\":21221,\"start\":21202},{\"end\":22379,\"start\":22357},{\"end\":23029,\"start\":23001},{\"end\":23454,\"start\":23440},{\"end\":23472,\"start\":23457},{\"end\":23489,\"start\":23475},{\"end\":25035,\"start\":25003},{\"end\":26494,\"start\":26480},{\"end\":28440,\"start\":28432},{\"end\":28951,\"start\":28943},{\"end\":29264,\"start\":29256},{\"end\":30648,\"start\":30639},{\"end\":31639,\"start\":31629},{\"end\":32690,\"start\":32679}]", "table": "[{\"end\":29681,\"start\":29472},{\"end\":30637,\"start\":30318},{\"end\":31627,\"start\":30806},{\"end\":32677,\"start\":32156},{\"end\":34150,\"start\":33376}]", "figure_caption": "[{\"end\":28941,\"start\":28442},{\"end\":29254,\"start\":28953},{\"end\":29320,\"start\":29266},{\"end\":29472,\"start\":29323},{\"end\":30318,\"start\":29684},{\"end\":30806,\"start\":30650},{\"end\":32156,\"start\":31642},{\"end\":33376,\"start\":32694}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11616,\"start\":11610},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13580,\"start\":13574},{\"end\":23514,\"start\":23508},{\"end\":25926,\"start\":25920},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26085,\"start\":26079}]", "bib_author_first_name": "[{\"end\":34244,\"start\":34243},{\"end\":34256,\"start\":34255},{\"end\":34262,\"start\":34261},{\"end\":34273,\"start\":34272},{\"end\":34680,\"start\":34679},{\"end\":34691,\"start\":34690},{\"end\":34702,\"start\":34701},{\"end\":34712,\"start\":34711},{\"end\":34917,\"start\":34916},{\"end\":34926,\"start\":34925},{\"end\":34938,\"start\":34937},{\"end\":34950,\"start\":34949},{\"end\":35277,\"start\":35276},{\"end\":35289,\"start\":35288},{\"end\":35300,\"start\":35299},{\"end\":35311,\"start\":35310},{\"end\":35617,\"start\":35616},{\"end\":35625,\"start\":35624},{\"end\":35638,\"start\":35637},{\"end\":35936,\"start\":35935},{\"end\":35946,\"start\":35945},{\"end\":35957,\"start\":35956},{\"end\":35969,\"start\":35968},{\"end\":36289,\"start\":36288},{\"end\":36296,\"start\":36295},{\"end\":36304,\"start\":36303},{\"end\":36744,\"start\":36743},{\"end\":36753,\"start\":36752},{\"end\":37106,\"start\":37105},{\"end\":37112,\"start\":37111},{\"end\":37119,\"start\":37118},{\"end\":37128,\"start\":37127},{\"end\":37134,\"start\":37133},{\"end\":37143,\"start\":37142},{\"end\":37498,\"start\":37497},{\"end\":37505,\"start\":37504},{\"end\":37513,\"start\":37512},{\"end\":37531,\"start\":37530},{\"end\":37533,\"start\":37532},{\"end\":37797,\"start\":37796},{\"end\":37811,\"start\":37810},{\"end\":38018,\"start\":38017},{\"end\":38020,\"start\":38019},{\"end\":38036,\"start\":38035},{\"end\":38038,\"start\":38037},{\"end\":38050,\"start\":38049},{\"end\":38062,\"start\":38061},{\"end\":38064,\"start\":38063},{\"end\":38073,\"start\":38072},{\"end\":38075,\"start\":38074},{\"end\":38433,\"start\":38432},{\"end\":38444,\"start\":38443},{\"end\":38454,\"start\":38453},{\"end\":38786,\"start\":38785},{\"end\":38798,\"start\":38797},{\"end\":38811,\"start\":38810},{\"end\":38821,\"start\":38820},{\"end\":39090,\"start\":39089},{\"end\":39100,\"start\":39099},{\"end\":39109,\"start\":39108},{\"end\":39111,\"start\":39110},{\"end\":39120,\"start\":39119},{\"end\":39129,\"start\":39128},{\"end\":39141,\"start\":39140},{\"end\":39152,\"start\":39151},{\"end\":39657,\"start\":39656},{\"end\":39668,\"start\":39667},{\"end\":39670,\"start\":39669},{\"end\":39680,\"start\":39679},{\"end\":39928,\"start\":39927},{\"end\":39940,\"start\":39939},{\"end\":39949,\"start\":39948},{\"end\":39951,\"start\":39950},{\"end\":39962,\"start\":39961},{\"end\":40236,\"start\":40235},{\"end\":40246,\"start\":40245},{\"end\":40248,\"start\":40247},{\"end\":40264,\"start\":40263},{\"end\":40275,\"start\":40274},{\"end\":40277,\"start\":40276},{\"end\":40288,\"start\":40287},{\"end\":40590,\"start\":40589},{\"end\":40596,\"start\":40595},{\"end\":40604,\"start\":40603},{\"end\":40619,\"start\":40618},{\"end\":40621,\"start\":40620},{\"end\":40969,\"start\":40968},{\"end\":40976,\"start\":40975},{\"end\":40983,\"start\":40982},{\"end\":40985,\"start\":40984},{\"end\":40998,\"start\":40997},{\"end\":41007,\"start\":41006},{\"end\":41404,\"start\":41403},{\"end\":41419,\"start\":41418},{\"end\":41430,\"start\":41429},{\"end\":41437,\"start\":41436},{\"end\":41448,\"start\":41447},{\"end\":41460,\"start\":41459},{\"end\":41470,\"start\":41469},{\"end\":41487,\"start\":41486},{\"end\":41498,\"start\":41497},{\"end\":41901,\"start\":41900},{\"end\":41911,\"start\":41910},{\"end\":42067,\"start\":42066},{\"end\":42075,\"start\":42074},{\"end\":42084,\"start\":42083},{\"end\":42246,\"start\":42245},{\"end\":42259,\"start\":42258},{\"end\":42271,\"start\":42270},{\"end\":42284,\"start\":42283},{\"end\":42737,\"start\":42736},{\"end\":42739,\"start\":42738},{\"end\":42763,\"start\":42758},{\"end\":42772,\"start\":42771},{\"end\":43077,\"start\":43076},{\"end\":43086,\"start\":43085},{\"end\":43098,\"start\":43097},{\"end\":43418,\"start\":43417},{\"end\":43425,\"start\":43424},{\"end\":43438,\"start\":43437},{\"end\":43449,\"start\":43448},{\"end\":43460,\"start\":43459},{\"end\":43468,\"start\":43467},{\"end\":43480,\"start\":43479},{\"end\":43494,\"start\":43493},{\"end\":43804,\"start\":43803},{\"end\":43816,\"start\":43815},{\"end\":44074,\"start\":44073},{\"end\":44080,\"start\":44079},{\"end\":44089,\"start\":44088},{\"end\":44096,\"start\":44095},{\"end\":44395,\"start\":44394},{\"end\":44641,\"start\":44640},{\"end\":44650,\"start\":44649},{\"end\":44664,\"start\":44663},{\"end\":44682,\"start\":44681},{\"end\":44691,\"start\":44690}]", "bib_author_last_name": "[{\"end\":34253,\"start\":34245},{\"end\":34259,\"start\":34257},{\"end\":34270,\"start\":34263},{\"end\":34281,\"start\":34274},{\"end\":34688,\"start\":34681},{\"end\":34699,\"start\":34692},{\"end\":34709,\"start\":34703},{\"end\":34718,\"start\":34713},{\"end\":34923,\"start\":34918},{\"end\":34935,\"start\":34927},{\"end\":34947,\"start\":34939},{\"end\":34956,\"start\":34951},{\"end\":35286,\"start\":35278},{\"end\":35297,\"start\":35290},{\"end\":35308,\"start\":35301},{\"end\":35317,\"start\":35312},{\"end\":35622,\"start\":35618},{\"end\":35635,\"start\":35626},{\"end\":35646,\"start\":35639},{\"end\":35943,\"start\":35937},{\"end\":35954,\"start\":35947},{\"end\":35966,\"start\":35958},{\"end\":35980,\"start\":35970},{\"end\":36293,\"start\":36290},{\"end\":36301,\"start\":36297},{\"end\":36308,\"start\":36305},{\"end\":36750,\"start\":36745},{\"end\":36760,\"start\":36754},{\"end\":37109,\"start\":37107},{\"end\":37116,\"start\":37113},{\"end\":37125,\"start\":37120},{\"end\":37131,\"start\":37129},{\"end\":37140,\"start\":37135},{\"end\":37147,\"start\":37144},{\"end\":37502,\"start\":37499},{\"end\":37510,\"start\":37506},{\"end\":37528,\"start\":37514},{\"end\":37538,\"start\":37534},{\"end\":37808,\"start\":37798},{\"end\":37818,\"start\":37812},{\"end\":38033,\"start\":38021},{\"end\":38047,\"start\":38039},{\"end\":38059,\"start\":38051},{\"end\":38070,\"start\":38065},{\"end\":38083,\"start\":38076},{\"end\":38441,\"start\":38434},{\"end\":38451,\"start\":38445},{\"end\":38460,\"start\":38455},{\"end\":38795,\"start\":38787},{\"end\":38808,\"start\":38799},{\"end\":38818,\"start\":38812},{\"end\":38828,\"start\":38822},{\"end\":39097,\"start\":39091},{\"end\":39106,\"start\":39101},{\"end\":39117,\"start\":39112},{\"end\":39126,\"start\":39121},{\"end\":39138,\"start\":39130},{\"end\":39149,\"start\":39142},{\"end\":39161,\"start\":39153},{\"end\":39665,\"start\":39658},{\"end\":39677,\"start\":39671},{\"end\":39687,\"start\":39681},{\"end\":39937,\"start\":39929},{\"end\":39946,\"start\":39941},{\"end\":39959,\"start\":39952},{\"end\":39974,\"start\":39963},{\"end\":40243,\"start\":40237},{\"end\":40261,\"start\":40249},{\"end\":40272,\"start\":40265},{\"end\":40285,\"start\":40278},{\"end\":40300,\"start\":40289},{\"end\":40593,\"start\":40591},{\"end\":40601,\"start\":40597},{\"end\":40616,\"start\":40605},{\"end\":40636,\"start\":40622},{\"end\":40973,\"start\":40970},{\"end\":40980,\"start\":40977},{\"end\":40995,\"start\":40986},{\"end\":41004,\"start\":40999},{\"end\":41010,\"start\":41008},{\"end\":41416,\"start\":41405},{\"end\":41427,\"start\":41420},{\"end\":41434,\"start\":41431},{\"end\":41445,\"start\":41438},{\"end\":41457,\"start\":41449},{\"end\":41467,\"start\":41461},{\"end\":41484,\"start\":41471},{\"end\":41495,\"start\":41488},{\"end\":41503,\"start\":41499},{\"end\":41908,\"start\":41902},{\"end\":41918,\"start\":41912},{\"end\":42072,\"start\":42068},{\"end\":42081,\"start\":42076},{\"end\":42092,\"start\":42085},{\"end\":42256,\"start\":42247},{\"end\":42268,\"start\":42260},{\"end\":42281,\"start\":42272},{\"end\":42296,\"start\":42285},{\"end\":42756,\"start\":42740},{\"end\":42769,\"start\":42764},{\"end\":42779,\"start\":42773},{\"end\":43083,\"start\":43078},{\"end\":43095,\"start\":43087},{\"end\":43104,\"start\":43099},{\"end\":43422,\"start\":43419},{\"end\":43435,\"start\":43426},{\"end\":43446,\"start\":43439},{\"end\":43457,\"start\":43450},{\"end\":43465,\"start\":43461},{\"end\":43477,\"start\":43469},{\"end\":43491,\"start\":43481},{\"end\":43502,\"start\":43495},{\"end\":43813,\"start\":43805},{\"end\":43826,\"start\":43817},{\"end\":44077,\"start\":44075},{\"end\":44086,\"start\":44081},{\"end\":44093,\"start\":44090},{\"end\":44100,\"start\":44097},{\"end\":44402,\"start\":44396},{\"end\":44647,\"start\":44642},{\"end\":44661,\"start\":44651},{\"end\":44679,\"start\":44665},{\"end\":44688,\"start\":44683},{\"end\":44699,\"start\":44692}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":178063},\"end\":34623,\"start\":34152},{\"attributes\":{\"id\":\"b1\"},\"end\":34834,\"start\":34625},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":13259596},\"end\":35192,\"start\":34836},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":215827080},\"end\":35558,\"start\":35194},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1629541},\"end\":35889,\"start\":35560},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14201947},\"end\":36228,\"start\":35891},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":623137},\"end\":36633,\"start\":36230},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":102496818},\"end\":37020,\"start\":36635},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":5073720},\"end\":37422,\"start\":37022},{\"attributes\":{\"doi\":\"abs/1603.03183\",\"id\":\"b9\",\"matched_paper_id\":5446275},\"end\":37719,\"start\":37424},{\"attributes\":{\"id\":\"b10\"},\"end\":37944,\"start\":37721},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9002995},\"end\":38368,\"start\":37946},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":14281310},\"end\":38716,\"start\":38370},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":12742386},\"end\":39037,\"start\":38718},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9649070},\"end\":39592,\"start\":39039},{\"attributes\":{\"doi\":\"abs/1611.05009\",\"id\":\"b15\",\"matched_paper_id\":206596552},\"end\":39846,\"start\":39594},{\"attributes\":{\"doi\":\"abs/1609.05130\",\"id\":\"b16\"},\"end\":40168,\"start\":39848},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":21124365},\"end\":40521,\"start\":40170},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":2407217},\"end\":40943,\"start\":40523},{\"attributes\":{\"id\":\"b19\"},\"end\":41341,\"start\":40945},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":12552176},\"end\":41821,\"start\":41343},{\"attributes\":{\"id\":\"b21\"},\"end\":42027,\"start\":41823},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10038098},\"end\":42213,\"start\":42029},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6099034},\"end\":42674,\"start\":42215},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":545361},\"end\":42998,\"start\":42676},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":12061055},\"end\":43353,\"start\":43000},{\"attributes\":{\"doi\":\"arXiv:1408.5093\",\"id\":\"b26\"},\"end\":43733,\"start\":43355},{\"attributes\":{\"doi\":\"abs/1409.1556\",\"id\":\"b27\",\"matched_paper_id\":14124313},\"end\":43978,\"start\":43735},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":13740328},\"end\":44356,\"start\":43980},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":121049},\"end\":44568,\"start\":44358},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":47368421},\"end\":44937,\"start\":44570}]", "bib_title": "[{\"end\":34241,\"start\":34152},{\"end\":34914,\"start\":34836},{\"end\":35274,\"start\":35194},{\"end\":35614,\"start\":35560},{\"end\":35933,\"start\":35891},{\"end\":36286,\"start\":36230},{\"end\":36741,\"start\":36635},{\"end\":37103,\"start\":37022},{\"end\":37495,\"start\":37424},{\"end\":38015,\"start\":37946},{\"end\":38430,\"start\":38370},{\"end\":38783,\"start\":38718},{\"end\":39087,\"start\":39039},{\"end\":39654,\"start\":39594},{\"end\":39925,\"start\":39848},{\"end\":40233,\"start\":40170},{\"end\":40587,\"start\":40523},{\"end\":40966,\"start\":40945},{\"end\":41401,\"start\":41343},{\"end\":42064,\"start\":42029},{\"end\":42243,\"start\":42215},{\"end\":42734,\"start\":42676},{\"end\":43074,\"start\":43000},{\"end\":43801,\"start\":43735},{\"end\":44071,\"start\":43980},{\"end\":44392,\"start\":44358},{\"end\":44638,\"start\":44570}]", "bib_author": "[{\"end\":34255,\"start\":34243},{\"end\":34261,\"start\":34255},{\"end\":34272,\"start\":34261},{\"end\":34283,\"start\":34272},{\"end\":34690,\"start\":34679},{\"end\":34701,\"start\":34690},{\"end\":34711,\"start\":34701},{\"end\":34720,\"start\":34711},{\"end\":34925,\"start\":34916},{\"end\":34937,\"start\":34925},{\"end\":34949,\"start\":34937},{\"end\":34958,\"start\":34949},{\"end\":35288,\"start\":35276},{\"end\":35299,\"start\":35288},{\"end\":35310,\"start\":35299},{\"end\":35319,\"start\":35310},{\"end\":35624,\"start\":35616},{\"end\":35637,\"start\":35624},{\"end\":35648,\"start\":35637},{\"end\":35945,\"start\":35935},{\"end\":35956,\"start\":35945},{\"end\":35968,\"start\":35956},{\"end\":35982,\"start\":35968},{\"end\":36295,\"start\":36288},{\"end\":36303,\"start\":36295},{\"end\":36310,\"start\":36303},{\"end\":36752,\"start\":36743},{\"end\":36762,\"start\":36752},{\"end\":37111,\"start\":37105},{\"end\":37118,\"start\":37111},{\"end\":37127,\"start\":37118},{\"end\":37133,\"start\":37127},{\"end\":37142,\"start\":37133},{\"end\":37149,\"start\":37142},{\"end\":37504,\"start\":37497},{\"end\":37512,\"start\":37504},{\"end\":37530,\"start\":37512},{\"end\":37540,\"start\":37530},{\"end\":37810,\"start\":37796},{\"end\":37820,\"start\":37810},{\"end\":38035,\"start\":38017},{\"end\":38049,\"start\":38035},{\"end\":38061,\"start\":38049},{\"end\":38072,\"start\":38061},{\"end\":38085,\"start\":38072},{\"end\":38443,\"start\":38432},{\"end\":38453,\"start\":38443},{\"end\":38462,\"start\":38453},{\"end\":38797,\"start\":38785},{\"end\":38810,\"start\":38797},{\"end\":38820,\"start\":38810},{\"end\":38830,\"start\":38820},{\"end\":39099,\"start\":39089},{\"end\":39108,\"start\":39099},{\"end\":39119,\"start\":39108},{\"end\":39128,\"start\":39119},{\"end\":39140,\"start\":39128},{\"end\":39151,\"start\":39140},{\"end\":39163,\"start\":39151},{\"end\":39667,\"start\":39656},{\"end\":39679,\"start\":39667},{\"end\":39689,\"start\":39679},{\"end\":39939,\"start\":39927},{\"end\":39948,\"start\":39939},{\"end\":39961,\"start\":39948},{\"end\":39976,\"start\":39961},{\"end\":40245,\"start\":40235},{\"end\":40263,\"start\":40245},{\"end\":40274,\"start\":40263},{\"end\":40287,\"start\":40274},{\"end\":40302,\"start\":40287},{\"end\":40595,\"start\":40589},{\"end\":40603,\"start\":40595},{\"end\":40618,\"start\":40603},{\"end\":40638,\"start\":40618},{\"end\":40975,\"start\":40968},{\"end\":40982,\"start\":40975},{\"end\":40997,\"start\":40982},{\"end\":41006,\"start\":40997},{\"end\":41012,\"start\":41006},{\"end\":41418,\"start\":41403},{\"end\":41429,\"start\":41418},{\"end\":41436,\"start\":41429},{\"end\":41447,\"start\":41436},{\"end\":41459,\"start\":41447},{\"end\":41469,\"start\":41459},{\"end\":41486,\"start\":41469},{\"end\":41497,\"start\":41486},{\"end\":41505,\"start\":41497},{\"end\":41910,\"start\":41900},{\"end\":41920,\"start\":41910},{\"end\":42074,\"start\":42066},{\"end\":42083,\"start\":42074},{\"end\":42094,\"start\":42083},{\"end\":42258,\"start\":42245},{\"end\":42270,\"start\":42258},{\"end\":42283,\"start\":42270},{\"end\":42298,\"start\":42283},{\"end\":42758,\"start\":42736},{\"end\":42771,\"start\":42758},{\"end\":42781,\"start\":42771},{\"end\":43085,\"start\":43076},{\"end\":43097,\"start\":43085},{\"end\":43106,\"start\":43097},{\"end\":43424,\"start\":43417},{\"end\":43437,\"start\":43424},{\"end\":43448,\"start\":43437},{\"end\":43459,\"start\":43448},{\"end\":43467,\"start\":43459},{\"end\":43479,\"start\":43467},{\"end\":43493,\"start\":43479},{\"end\":43504,\"start\":43493},{\"end\":43815,\"start\":43803},{\"end\":43828,\"start\":43815},{\"end\":44079,\"start\":44073},{\"end\":44088,\"start\":44079},{\"end\":44095,\"start\":44088},{\"end\":44102,\"start\":44095},{\"end\":44404,\"start\":44394},{\"end\":44649,\"start\":44640},{\"end\":44663,\"start\":44649},{\"end\":44681,\"start\":44663},{\"end\":44690,\"start\":44681},{\"end\":44701,\"start\":44690}]", "bib_venue": "[{\"end\":34349,\"start\":34283},{\"end\":34677,\"start\":34625},{\"end\":35003,\"start\":34958},{\"end\":35365,\"start\":35319},{\"end\":35713,\"start\":35648},{\"end\":36041,\"start\":35982},{\"end\":36384,\"start\":36310},{\"end\":36817,\"start\":36762},{\"end\":37194,\"start\":37149},{\"end\":37558,\"start\":37554},{\"end\":37794,\"start\":37721},{\"end\":38145,\"start\":38085},{\"end\":38525,\"start\":38462},{\"end\":38867,\"start\":38830},{\"end\":39261,\"start\":39163},{\"end\":39707,\"start\":39703},{\"end\":39994,\"start\":39990},{\"end\":40331,\"start\":40302},{\"end\":40698,\"start\":40638},{\"end\":41092,\"start\":41012},{\"end\":41564,\"start\":41505},{\"end\":41898,\"start\":41823},{\"end\":42107,\"start\":42094},{\"end\":42356,\"start\":42298},{\"end\":42826,\"start\":42781},{\"end\":43166,\"start\":43106},{\"end\":43415,\"start\":43355},{\"end\":43845,\"start\":43841},{\"end\":44157,\"start\":44102},{\"end\":44440,\"start\":44404},{\"end\":44742,\"start\":44701},{\"end\":34402,\"start\":34351},{\"end\":36445,\"start\":36386},{\"end\":39346,\"start\":39263},{\"end\":40745,\"start\":40700},{\"end\":41168,\"start\":41094},{\"end\":42116,\"start\":42109}]"}}}, "year": 2023, "month": 12, "day": 17}