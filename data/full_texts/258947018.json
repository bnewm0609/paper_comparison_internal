{"id": 258947018, "updated": "2023-10-05 04:28:31.079", "metadata": {"title": "ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation", "authors": "[{\"first\":\"Zhewei\",\"last\":\"Yao\",\"middle\":[]},{\"first\":\"Xiaoxia\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Cheng\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Stephen\",\"last\":\"Youn\",\"middle\":[]},{\"first\":\"Yuxiong\",\"last\":\"He\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Post-training quantization (PTQ) has emerged as a promising technique for mitigating memory consumption and computational costs in large language models (LLMs). However, a systematic examination of various quantization schemes, model families, and quantization bit precision has been absent from the literature. In this paper, we conduct a comprehensive analysis of these factors by investigating the effects of PTQ on weight-only, activation-only, and weight-and-activation quantization using diverse methods such as round-to-nearest (RTN), GPTQ, ZeroQuant, and their variants. We apply these methods to two distinct model families with parameters ranging from 125M to 176B. Our contributions include: (1) a sensitivity analysis revealing that activation quantization is generally more susceptible to weight quantization, with smaller models often outperforming larger models in terms of activation quantization; (2) an evaluation and comparison of existing PTQ methods to optimize model size reduction while minimizing the impact on accuracy, revealing that none of the current methods can achieve the original model quality for quantization with either INT4-weight or INT4-weight-and-INT8-activation; (3) based on these insights, we propose an optimized method called Low-Rank Compensation (LoRC), which employs low-rank matrices to enhance model quality recovery with a minimal increase in model size.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2303.08302", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": null}}, "content": {"source": {"pdf_hash": "8f48c75e1354c88a84a67abb60789083c12e5037", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2303.08302v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "564fcd465f2f678daee949b05b7a690a523ac8cc", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8f48c75e1354c88a84a67abb60789083c12e5037.txt", "contents": "\nZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation\n\n\nZhewei Yao zheweiyao@microsoft.com \nXiaoxia Wu xiaoxiawu@microsoft.com \nCheng Li chengli1@microsoft.com \nStephen Youn stephen.youn@microsoft.com \nYuxiong He Microsoft \nZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation\n\nPost-training quantization (PTQ) has emerged as a promising technique for mitigating memory consumption and computational costs in large language models (LLMs). However, a systematic examination of various quantization schemes, model families, and quantization bit precision has been absent from the literature. In this paper, we conduct a comprehensive analysis of these factors by investigating the effects of PTQ on weight-only, activation-only, and weight-and-activation quantization using diverse methods such as round-to-nearest (RTN), GPTQ, ZeroQuant, and their variants. We apply these methods to two distinct model families with parameters ranging from 125M to 176B. Our contributions include:(1) a sensitivity analysis revealing that activation quantization is generally more susceptible to weight quantization, with smaller models often outperforming larger models in terms of activation quantization;(2) an evaluation and comparison of existing PTQ methods to optimize model size reduction while minimizing the impact on accuracy, revealing that none of the current methods can achieve the original model quality for quantization with either   (3) based on these insights, we propose an optimized method called Low-Rank Compensation (LoRC), which employs low-rank matrices to enhance model quality recovery with a minimal increase in model size. * Equal Contribution. Code will be released as a part of https://github.com/microsoft/DeepSpeed\n\nIntroduction\n\nLarge language models (LLMs) like Codex [15] and ChatGPT [24] have demonstrated breakthrough performance across various benchmarks, such as natural language understanding and generation, and are now integrated into everyday applications. However, efficiently serving LLMs has become a pressing concern due to their significant memory consumption and computational demands. Unlike classification or diffusion models, LLMs present unique challenges, as they involve two distinct phases: prompt and generation. The prompt phase is primarily compute-bound, while the generation phase, with low batch size and KV cache, is mainly memory-bound [26].\n\nAs the progression of hardware bandwidth lags behind that of computational demand [14], the resource demands of extra-large models such as MT-NLG-530B [30]-which necessitates the deployment of multiple nodes for operation-escalate, adding to the complexities of cross-node communication. This has emphasized the urgency to curtail both the size and computational expense of Large Language Models (LLMs). An increasingly effective solution to these issues is post-training quantization (PTQ). This method aids in the reduction of training prerequisites while simultaneously lowering the bit precision of weights and activations to either INT4 or INT8.\n\nWhile the effectiveness of post-training quantization (PTQ) has been underscored in a number of recent studies [36,12,35,7], a comprehensive, systematic investigation into several key dimensions of this technique remains to be undertaken. Specifically, the extant literature falls short in providing thorough coverage of the functionality of various PTQ methods or the sensitivity of disparate models. Moreover, despite current quantization methods demonstrating promising results in the reduction of model sizes, the question persists Here PTQ (with fine-grained quantization) represents the method from [36,12], RTN means the naive round-to-nearest baseline (with fine-grained quantization as well), and FP16/INT8 is used as the no-accuracy-loss baseline. LoRC is our proposed method that works seamless with PTQ. Note that we drop all diverged points for better visualization. For all detailed numbers, please see Appendix E.\n\nas to whether these methods are achieving their optimal potential in minimizing Large Language Models (LLMs) sizes. With these observations in mind, our study sets forth to address two salient questions: (1) When subjected to quantization, do LLMs of varying sizes and pretraining data exhibit similar behavior? (2) Are existing quantization methods truly leveraging their full potential in reducing the sizes of LLMs?\n\nContribution. To elucidate these queries, we undertake an exhaustive examination of the impact of PTQ on weight-only, activation-only, and combined weight-and-activation quantization. This investigation incorporates a range of PTQ methods, including round-to-nearest (RTN), GPTQ [12], ZeroQuant [36], and their respective variants. To broaden the scope of our analysis, we focus on two distinct model families, OPT [40] and BLOOM [28], spanning model sizes from 125M to a massive 176B. Our code will be made available for reproduction. In summary, we make the following contributions:\n\n(1) We provide a thorough sensitivity analysis to demonstrate that a) Activation quantization is generally more sensitive to weight quantization; Smaller models usually have better activation quantization performance than the relative larger model. b) Different model families show different INT8 activation quantization behaviors; Particularly for large models, BLOOM-176B has small accuracy drops (about 1 perplexity or PPL) but OPT-30B and -66B experience worse performance.\n\n(2) We carry out a detailed evaluation and comparison of current PTQ methods, utilizing optimal configurations to maximize model size reduction while minimizing accuracy impact. We found that the current existing method can barely achieve less than 0.1 PPL points degradation for quantization with either INT4-weight or INT4-weight-and-INT8-activation (W4A8). To recover the 0.1 PPL, we strive to push the boundaries of employing fine-grained quantization (FGQ) techniques. We observe FGQ is able to recovered points degradation of <0.1 PPL for large models (>13B) for INT4 weight quantization, but there are still non-negligible model quality drops.\n\n(3) Based on the above understanding, we further optimize existing methods and introduce a technique called Low Rank Compensation (LoRC), which employs low-rank matrix factorization on the quantization error matrix. Complementary to FGQ, LoRC plays a crucial role in enhancing the full model quality recovery, while there is little increase of the model size.\n\nIn Figure 1, we provide model size and quality trade-offs for both OPT and BLOOM families. As can be seen, using LoRC on top of PTQ methods from [36,12] and fine-grained quantization, we set a new quantization Pareto frontier for LLMs. Meanwhile, we recommend the following setting for quantizing LLMs with LoRC (Note that activation quantization should be only applied if necessary): (1) For larger models (>10B), fine-grained (block size 64-256) 4-bit weight quantization plus 8-bit activation quantization (block size 64-256) with PTQ can be used for real deployment; (2) For middle-size models (<10B and >1B), per-row INT8 quantization plus fine-grained (block size 64-256) INT8 activation quantization can be used with PTQ from [12,36] \n\n\nRelated Work\n\nDifferent quantization methods [29,38,9,41,1,8,31,19] for transformer-based models [32] have been explored for a while. However, most of those works need quantization-aware finetuning or even expensive quantization-aware knowledge distillation [17]. Due to the cost of training/finetuning LLMs [25,18,31,34,33], it is a challenge for practitioners/researchers to do finetuning/distillation on those LLMs, particularly for models like GPT-3-175B [4] and BLOOM-176B [28].\n\nPost-training quantization (PTQ) [37,3] is an alternative way to quantize the model with no/minimal finetuning requirement. Along this line, several recent works focus on LLMs (beyond the million-parameter scale). [36] proposes vector-based INT8 quantization with layer-by-layer knowledge distillation to overcome the training cost and quantization error introduced by LLMs. [6] uses similar vector-based INT8 quantization weight plus mixed-precision (INT8/FP16) quantization for activation to overcome the sensitivity of activation quantization. However, the inference speed of [6] is generally even slower than FP16 baseline [2] due to the difficulty of implementing mixed-precision calculation within a single tensor. More recently, [12] extends OBQ [10,16,21] on LLMs for INT4 weight-only quantization and shows great efficiency on quantization and latency, and [35] shows the outliers from activations can be smoothed out by migrating the quantization difficulty from activations to its associated weights. However, [35] can only work for W8A8 quantization as lower weight precision (INT4) itself already leads to significant accuracy degradation, and the accuracy drop is larger than 0.1 PPL points, which as discussed in the later section is sub-optimal. [7] shows the scaling law of weight-only quantization with the simplest round-to-nearest baseline, but it does not consider the weight-and-activation quantization and/or the above PTQ optimization methods. As can be seen from Figure 1, by using PTQ optimization methods, the model quality can be significantly improved. Please also see Appendix E for more detailed numbers.\n\nDifferent than existing works, our paper extensively tests the effect of (1) different quantization schemes, e.g., symmetric and asymmetric quantization, (2) different PTQ methods, e.g., [36,12], (3) different model families, e.g., [28,40], (4) different quantization coverage, e.g., weight-only and weight-and-activation quantization, and (5) other discussions, e.g., the effect of quantization granularity. As such, we provide a much more comprehensive understanding of post-training quantization for large language models compared to the previous works.\n\n\nWould different model families behave similarly on quantization?\n\nThere are mainly two categories of PTQ for LLMs, i.e., weight-only quantization [12] and weight-and-activation quantization [6,36,35]. In the latter, it is uniformly observed across all studies that activation quantization demonstrates greater sensitivity than weight quantization. However, prior research tends to concentrate on a single (family) model to emphasize the necessity of their proposed quantization technique. A comprehensive and systematic evaluation of this PTQ methodology, particularly the sensitivity of weight/activation quantization for varying model sizes and distinct model families, has yet to be undertaken. Hence, we conduct an examination on both the OPT [40] and BLOOM [28] families to elucidate the quantization sensitivity of weight and activation. Table 1: Classification of quantization sensitivity (or quantization loss). The sensitivity increases from Class-1 to Class-3.\n\n\nClass\n\nClass-1 Class-2 Class-3\nPPL Degradation \u22640.1 >0.1 & \u22640.5 >0.5\nSensitivity setting. We use the zero-shot validation perplexity (PPL) differential on three datasets, namely, Wikitext-2 [23], PTB [22], and C4 [27], before and after the quantization of these LLMs to illustrate their sensitivity, as PPL is significantly correlated to zero-shot/few-shot accuracy measurement [7]. Specifically, a higher PPL drop indicates enhanced quantization sensitivity. For simplicity, we also categorize quantization sensitivity (or quantization loss) into three different classes as depicted in Table 1. Notably, the threshold is chosen because when the model size approximately doubles (e.g., 13B vs. 30B, and 30B vs. 66B), the PPL improvement is about 0.5 (see Table 2). The sensitivity (or loss) incrementally increases as the class number ascends. From a practical standpoint, we favor lower quantization sensitivity (accuracy loss), making Class-1 the optimal-loss post-training quantization.\n\nWe employ both symmetric and asymmetric quantization to gauge the quantization sensitivity and highlight the advantage of asymmetric quantization. Particularly, we implement per-row quantization [12] for weight quantization and per-token quantization for activation [36].\n\nRobustness of Weight-only Quantization for Large Models. The results of weight-only quantization in OPT and BLOOM models are summarized in Table 2. INT8 weight-only quantization, either symmetric or asymmetric, results in negligible accuracy loss (less than 0.05, i.e., Class-1). Consequently, for tasks oriented towards generation, FP16 weight can simply be replaced with INT8 weight to reduce memory usage. For INT4 quantization, the asymmetric method outperforms the symmetric approach in accuracy, attributable to its superior utilization of the quantization range. Interestingly, larger models exhibit better tolerance to low-precision quantization (i.e., INT4) than smaller models, with a few exceptions such as OPT-66B. 1 Particularly, BLOOM-176B shows PPL degradation (around 0.3 points) in Class-2, which could explain why the large GLM-130B [39] can operate with INT4 weight-only quantization out of the box with acceptable accuracy impact. Challenge Encountered in Activation Quantization for Large Models. Activation quantization has consistently proven more difficult than weight quantization [36,6], as illustrated in Table 2. When compared to weight-only quantization, activation-only quantization indicates that asymmetric quantization can significantly improved performance over symmetric quantization. Moreover, contrary to weight-only quantization, smaller models typically exhibit better tolerance to activation quantization, as their hidden dimension is smaller and the activation dynamic range is also narrower than larger models [36]. It should be 1 [12] discovered that OPT-66B has a high proportion of dead neurons in the early layers, which might influence the compression capability. We also identify another potential reason: the Layer Norm of the OPT-family is not well trained (except OPT-350M), with the weight and the bias being all 1's and 0's, respectively. noted that for models larger than 10B, all fall into Class-3, indicating a degradation of more than 0.5 PPL points.\n\nThe last two rows of Table 2 show that different model families exhibit significantly different behaviors.  BLOOM does not exhibit divergence issues even up to a model size of 176B, whereas OPT displays very  poor performance from a model size of 6.7B (larger models with INT8 activation have even worse PPL). This could again be attributed to the Layer Norm issue within the OPT-family 1 .\n\nFindings 1 on Sensitivity Analysis. (1) INT8 weight-only quantization can serve as a standard method for reducing memory costs in LLMs, with negligible degradation in accuracy.\n\n(2) INT4 weight-only quantization for small models results in substantial accuracy degradation (Class-3), but this effect lessens as the model size increases (Class-2). (3) Contrary to (2), INT8 activation results in minimal accuracy drops for small models (Class-1) but larger models exhibit greater drops (Class-3).  RTN directly applies PTQ on the trained data and follows the procedure detailed in Section A to perform the quantization. Specifically, for symmetric quantization, we set S = max(abs(x)) and Z = 0; for asymmetric quantization, we set S = max(x) \u2212 min(x) and Z = min(x).\n\nGPTQ extends the OBQ [10]. It tries to optimize the following non-linear least square problem, min\u0174 \u2225W x \u2212\u0174 x\u2225 2 2 where W is the weight, x is the activation, and\u0174 is a quantized weight. GPTQ employs second-order methods to obtain a closed-form solution. In addition, the quantization for each weight matrix is performed column-/row-wisely and the quantization errors from previous columns will be passed to those columns not yet quantized. See [10,12] for more details.\n\nZQ-Global is the original method proposed in [36], where authors treat each layer as a small neural network (a.k.a., subnetwork) and use the FP16 subnetwork as the teacher model to distill the quantized one with a few hundred iterations, i.e., min\u03b8 |f \u03b8 (x) \u2212 f\u03b8(x)|2 2 , where \u03b8 is a set of weights,\u03b8 is the quantized version, f \u03b8 is the subnetwork with parameters \u03b8, and x is the input. Thus, it can significantly reduce the GPU resource requirement and time cost.\n\nZQ-Local is an extension mode of ZQ-Global for further GPU requirement reduction and training cost reduction. Particularly, instead of using each transformer layer as the subnetwork, we treat each linear layer as the subnetwork. This method can be viewed as an iterative first-order optimization method (e.g., SGD) to solve min\u0174 \u2225W x \u2212\u0174 x\u2225 2 2 . Experimental Setup. We compare the four methods mentioned above on weight-only and weight-andactivation quantization. As weight quantization is always static (i.e., it does not change during inference), there is virtually no system performance difference between symmetric and asymmetric quantization. 3 We use asymmetric quantization for better accuracy, and the conclusions would hold similarly for symmetric quantization. For parameters used for GPTQ, ZQ-Local, and ZQ-Global, please refer to Appendix B. An interesting finding for ZeroQuant is that the hyperparameters (e.g., learning rate and its scheduler) provided in the original work [36] are sub-optimal. In this work, we find the best configurations for ZQ-Local and ZQ-Global and denote them as ZQ-Local * and ZQ-Global * , respectively, with the best tuned results. To ensure consistent and comparable results, we set a fixed random seed for our experiments. In the context of post-training quantization, varying the random seed has minimal impact on the final results, as indicated in more detail in Table B.1.\n\nEvaluation of Weight-only Quantization. The results from weight-only quantization using OPT and Bloom are presented in Table 3. The findings indicate that the larger models tend to be less sensitive to INT4 weight-only quantization. This observation holds true across all methods (RTN, GPTQ, ZQ-Local * , and ZQ-Global * ) with the exception of OPT-66B, which shows greater degradation than OPT-30B. It is noteworthy that light-weight optimization-based methods significantly outperform the RTN baseline in terms of accuracy. For instance, these methods substantially reduce the degradation in perplexity of OPT-30B/66B compared to baseline. Most quantized models with parameters greater than 6.7B fall under Class II, indicating their potential for real-world applications. For instance, the quality of INT4 OPT-30B (66B) is superior to that of INT8 OPT-13B (30B).\n\nAmong the optimization-based methods, ZQ-Global * generally performs better on smaller models (those with fewer than 1B parameters), while GPTQ excels on larger models. ZQ-Local * does not outperform GPTQ or ZQ-Global * --a reasonable outcome given that GPTQ employs a closed-form solution to solve the non-linear quadratic problem and ZQ-Global * optimizes a larger subnetwork. The inferior performance of ZQ-Global * compared to GPTQ for larger models is unexpected since ZQ-Global * optimizes an entire transformer layer while GPTQ only optimizes a single linear layer. A plausible explanation is that larger models are more sensitive to weight updates, necessitating more advanced fine-tuning methods.\n\nEvaluation of Weight and Activation Quantization. The evaluation results for existing methods using W4A8 quantization are presented in Table 3. The three light-weight optimization-based methods outperform RTN significantly, underscoring their efficacy. However, all of the results fall into either Class-2 or Class-3. This suggests that for certain applications, it might be more beneficial to use smaller models with fewer parameters rather than larger, quantized models.\n\nAmong quantization-based methods, ZQ-Global * and ZQ-Local * generally outperform GPTQ, which is anticipated given that GPTQ was originally designed for weight-only quantization. ZQ-Global * performs better than ZQ-Local * in most cases except for the two largest models, OPT-66B and Bloom-176B, despite having larger trainable parameters in one step. This again signifies the need for a more suitable and advanced optimization method for large language models (LLMs).\n\nFinding 2 on Comparisons. (1) GPTQ typically performs better for weight-only quantization, while ZeroQuant (including both ZQ-Global * and ZQ-Local * ) yields superior results for weight and activation quantization.\n\n(2) The tested optimization-based methods cannot achieve Class-1 quantization error for either INT4 weight-only or W4A8 quantization with the exception of GPTQ on OPT-30B with weight-only quantization. \n\n\nFine-grained Quantization and Its Evaluation\n\nWith PTQ and row-wise quantization, achieving Class-1 quantization error is challenging for both weight-only and weight-and-activation quantization. Generally, utilizing a smaller model with INT8 weight is more advantageous than employing a model that is twice as large with INT4 weight.\n\nOne potential solution to this issue is the implementation of finer-grained quantization schemes [5], where every k elements possess their own scaling factor and/or zero point. This approach can significantly reduce quantization error. In the extreme case, where every single element has its own scaling factor, the original FP16 number can be precisely recovered. Importantly, block-k quantization can be implemented on modern GPUs, one of the most prevalent deep learning architectures, since the compute unit (streaming multiprocessor) of GPUs processes tiles of data (e.g., 128 by 128 tiling size) for matrix computation.\n\nAlthough fine-grained quantization can substantially narrow the gap between the quantized tensor and its floating-point counterpart, the application of RTN still results in a non-trivial accuracy gap. Consequently, we build upon fine-grained quantization by employing existing optimization-based methods to further enhance accuracy. Specifically, we utilize GPTQ and ZQ-Global for all models and settings and apply ZQ-Local to OPT-66B and Bloom-176B. For the hyperparameters used in ZQ-Global and ZQ-Local, we select the top three identified in Section 4 for all models, except for Bloom-176B, for which we only use the top-performing hyperparameter to reduce training costs.\n\n\n4-bit Weight Quantization.\n\nWe hereby present the W4A16 results for OPT and BLOOM, as delineated in Table 4, corresponding to an array of quantization block sizes. The performance sees a significant improvement with smaller block sizes compared to per-row quantization. The point of diminishing returns, however, varies for different model sizes. For example, smaller models (such as OPT-6.7B and BLOOM-1.7b) continue to see substantial gains until the block size reduces to 32. In contrast, for larger models (those exceeding 10B, with OPT-66B as the exception), the benefits derived from smaller block sizes wane rapidly around block-256/512. Most crucially, for models equal to or larger than 13B, a smaller quantization block size results in quantization error being classified under Class-1, indicating virtually negligible degradation in accuracy.  Activation Quantization (W4A8). To comprehend the benefits of fine-grained quantization on activation, we analyze the quantization between perrow and a block size of 128, with INT4 weight, as highlighted in Table 5. For models of considerable size, specifically those equal to or exceeding 1B, the application of such fine-grained activation quantization (Case-1) results in a substantial reduction in quantization error compared to per-row activation (Case-2). By implementing fine-grained activation quantization with weight quantization (Case-3), we are able to almost restore the performance to the level of their W4A16 counterparts.\n\nFurthermore, we detail the impacts of varying activation quantization block sizes in Table 6 on BLOOM-176B, with INT4 weight. A trend of superior accuracy is observed with smaller block sizes in contrast to larger ones. However, the enhancement in performance reaches a saturation point when the size smaller or equal to 256, which corresponds to the range of values INT8 can represent. Despite INT8's capability to signify 256 distinct values, activation quantization errors persist due to the application of uniform quantization.\n\nFinding 3 on FGQ. (1) Larger models (\u226510B) are capable of attaining Class-1 error for 4-bit quantization. These models can leverage low-precision quantization as the model size with INT4 is similar to an INT8 model that is half its size, with improved accuracy. On the other hand, smaller models (\u226410B) typically reach only Class-2 or Class-3 error levels.\n\n(2) For larger models (>10B), the difference between fine-grained weight-and-activation quantization and fine-grained weight-only quantization is insignificant. (3) The advantage of fine-grained activation quantization fades for larger models when the block size reaches 256.\n\n\nProposed Method to Further Push the Limit of Post-training Quantization\n\nBuilding on the investigation and conclusions drawn from previous sections, it has become apparent that there is still a need for an advanced methodology to further refine the existing methods, with the objective of fully realizing the original fp16 PPL quality. In this section, we introduce a simple yet effective method called LoRC (Low Rank Compensation) to optimize the current existing quantization error and further bridge the gap between the quality of the original model and its quantized counterparts. LoRC is inspired by the employment of low-rank matrix factorization on the quantization error matrix E := W \u2212\u0174 , where W represents the original weight and\u0174 is the quantized weight. LoRC approximates the error E with\u00ca =\u00dbV by using two low-rank matrices\u00db andV . This results in a more accurate approximation of the original weight matrix W by\u0174 lorc =\u0174 +\u00ca, thereby reducing quantization errors: \u2225W \u2212\u0174 \u2225 \u2265 \u2225W \u2212\u0174 lorc \u2225. LoRC consists of two steps:\n\nStep I: Implement Singular Value Decomposition (SVD) on the error matrix E = U \u03a3V , where U \u2208 R din\u00d7din and V \u2208 R dout\u00d7dout are unitary matrices, and \u03a3 \u2208 R din\u00d7dout is a diagonal matrix with its diagonal elements ordered in a descending manner.\n\nStep II:\nWe formulate the matrix\u00ca =\u00dbV where\u00db = U m (\u03a3 m ) 1 2 andV = (\u03a3 m ) 1 2 V m . Here, U m = U :,1:m \u2208 R din\u00d7m , V m = V 1:m,: \u2208 R m\u00d7dout , and \u03a3 m = \u03a3 1:m,1:m \u2208 R m\u00d7m .\nThe objective of LoRC is to achieve a good approximation of the error matrix E using low-rank matrices, with minimal impact on the increase in model size. For instance, consider the standard transformer models [32], where each layer is comprised of a multi-headed attention (MHA) module and a multi-linear perception (MLP) module. Let h represent the hidden dimension and l the number of layers. The total number of parameters is 12lh 2 as each layer contains 4h 2 for MHA (for key, query, value, and projection matrices), and 8h 2 for MLP (two matrices of sizes h \u00d7 4h and 4h \u00d7 h). With the addition of low-rank LoRC to the six matrices in each layer, the total number of parameters for l layers would amount to 18hml. 4 Consequently, the ratio of parameters added to the existing model is 3m/2h. It's important to note that the low-rank dimension m can be as small as 4 or 8 (which we will discuss in detail in a later section) while the standard hidden dimension h \u2265 768, making the number 3m/2h \u2264 0.016. Significantly, LoRC can be viewed as a supplementary feature to existing quantization methodologies such as RTN, GPTQ, and ZeroQuant-Local/Global, and can be seamlessly integrated with FGQ. We have conducted experiments to evaluate the performance of LoRC on both OPT and BLOOM, applying 4-bit, 3-bit, and 2-bit weights by setting the activation to FP16. 5 Based on the discoveries in the preceding sections, we utilize the GPTQ quantization strategy. To gain a comprehensive understanding of LoRC, we include the results with and without the application of FGQ. The datasets and hyperparameters are consistent with those detailed in earlier sections.\n\nEvaluation Results. The findings are showcased in Table 7, split into two sections: coarse-grained weight quantization (per-row) and fine-grained quantization (block-size 256). Notably, we observe that the two low-rank matrices,\u00db andV , can be quantized to 8-bit without any performance discrepancy (Table 8). Thus, the two low-rank matrices for LoRC in Table 7 are INT8 with a low-rank dimension of m = 8. Several key observations can be made. Firstly, LoRC consistently boosts performance across all bit sizes and block sizes, as indicated by the lower perplexity scores when LoRC is activated. Secondly, the enhancement brought about by LoRC becomes more substantial as the bit size diminishes, especially noticeable for W2A16, which displays a markedly greater impact compared to W4A16 and W3A16 in most scenarios. Lastly, the combination of fine-grained quantization with LoRC yields the most impressive results, underscoring the efficacy of LoRC when integrated with FGQ. Overall, the results emphasize the benefits of using LoRC for enhanced performance in weight quantization and its compatibility with FGQ. Notably, recovering the last 0.05-0.1 perplexity can be challenging, but with LoRC, we are able to nearly recover the original model quality for INT4 quantization.\n\nAblation Study on the Low Rank Dimension m. An essential aspect of the LoRC method is on the optimal low-rank dimension, denoted as m, explained in Step II. To explore this, we varied m in the range of 1, 4, 8, 16, and 32 for OPT-1.3b/6.7b/30b models, and applied W4A16 GPTQ quantization. The outcomes are depicted in Table 9, indicating that the enhancements achieved through LoRC begin to plateau as the dimension m surpasses 4. The most optimal performance for OPT-6.7b is realized when m = 8.\n\nThis observation may seem counterintuitive initially, as one might anticipate that larger LoRC dimensions would yield more significant improvements. To gain a more comprehensive understanding, we conducted an analysis of the eigenvalues of the actual error matrix E = W \u2212\u0174 for each matrix. By randomly selecting 20 matrices from MHA and MLP layers, we plotted the eigenvalues of E as a curve, depicted in Figure 2. The two plots reveal a rapid flattening of eigenvalues after index 8, which elucidates why increasing the LoRC dimension does not considerably enhance performance. Hence, a sensible dimension for\u00db andV in the LoRC methodology could be 8. 6\n\n\nDiscussion\n\nConclusion. In this work, we provide a comprehensive study of post-training quantization (PTQ) on large language models with different PTQ methods (e.g., RTN, GPTQ, ZeroQuant), and with different quantization coverage (weight-only and weight-and-activation quantization), etc. We find that PTQ methods are critical to improving the quantized model quality, and that fine-grained quantization (FGQ) can bring acceptable accuracy and model size trade-off. Finally, we introduced an optimization technique called Low Rank Compensation (LoRC), which works synergistically with PTQ and FGQ, playing a crucial role in enhancing full model quality recovery with a minimal increase in model size.\n\nLimitation. Despite quantizing over 10,000 experiments, our study was constrained by our computing resources. This restriction made us choose between diversifying the model sizes and varying the tasks. We strategically limited our datasets to WikiText, PTB, and C4 to concentrate on a broad range of quantization methods. Consequently, our general findings are more robust concerning the two model families and three datasets examined in this paper. However, caution should be exercised when generalizing these findings to tasks that are dissimilar to those covered in this study.\n\nFuture Opportunity. Throughout the paper, we see several unresolved problems from current quantization schemes and/or algorithms, and we find potential directions for LLM compression: (1) Although we use fine-grained quantization schemes in the paper, the real implementation is missing. Moreover, how to efficiently implement odd bit precision is challenging. [12] demonstrated that 3-bit can achieve better throughput in the generation phase by packing all 3-bit numbers in continuous memory space. However, this method is sub-optimal as the dequantization step needs to connect bits from different bytes. One possible way to implement odd bits, e.g., 5 bits, is to use two integer matrices with INT4 and INT1. During the dequantization stage, we couple the two matrices together.\n\n(2) How to combine PTQ with other lightweight compression techniques, e.g., post-training pruning [20,11], is an interesting direction to further reduce the memory consumption and compute cost.\n\n[ \n\n\nA Background of Quantization\n\nQuantization maps floating point (e.g., FP16/FP32) numbers to integer numbers (e.g., INT4/INT8) so that lower memory usage (weight quantization) and faster integer arithmetic (weight-and-activation quantization) can be achieved compared to the floating point format. In this work, we are focusing on uniform quantization, i.e.,\nQ(x) = INT (x \u2212 Z)/S \u2212 Z,(1)\nwhere Q is the quantization function, x is a floating point input vector/tensor, S is a real valued scaling factor, and Z is an integer zero point. Based on different settings, the quantization method can be viewed as (1) symmetric vs. asymmetric quantization (Z = 0 or not), (2) fine-grained vs. coarse-grained quantization (how to partition the input x and get its associated scaling factor, e.g., matrix wise or row wise). See [13] for more details. Throughout this work, we focus on post-training quantization (PTQ), i.e., no or minimal training effort is applied after quantization, for which large accuracy degradation usually exhibits for coarse-grained quantization (per matrix/tensor) due to their large quantization error. As such, we focus on fine-grained quantization. Particularly, we use the per-row quantization (one row of the weight matrix or one token for the activation) from [36] as our coarsest-grained quantization method, and we use block-k quantization (for every k elements, they have their own scaling factor and/or zero point) as our finer-grained quantization scheme.\n\n\nB Detailed Setting Used in Section 4\n\nSame as [12], for all methods, we use C4 dataset to randomly select 128 sentences for training and each of them has 2048 tokens.\n\nFor GPTQ, we check its main hyperparameter, i.e., the dampening factor, and find out the method is not sensitive to it. As such, we use the hyparameter suggested by the author for all of our experiments. For ZQ-Global and ZQ-Local, as mentioned the in main text, the hyperparameters suggested by the original work [36] is suboptimal. We find that a linear decay learning rate schedule is very helpful in our initial test. As such, we add this as our default setting. Meanwhile, we extensively test a wide range (1e-3 to 5e-8) of learning rate for different models until we find the best learning rate (i.e., larger or smaller learning rate leads to worse accuracy performance).We employed the Adam optimizer and set the default batch size to 1 for our experiments.\n\nWe conducted tests to assess whether changes in random seeds would introduce substantial variations in the outcomes. As per the findings detailed in Table Table B.1, the modifications in random seeds resulted in only minimal effects on the final quality of the models. This effect was particularly negligible in the context of larger models, such as OPT-30b, where the standard deviation was only 0.01. Therefore, in consideration of these results, we elected to standardize the random seed for the subsequent experiments presented in this paper, setting it uniformly at 123 or 0. The code will be made publicly available to facilitate reproducibility of our results.\n\nFor all three methods, we run them on a single GPU (either V100-32GB or A100-80GB). For the largest model tested in the paper, i.e., BLOOM-176B, the cost of all methods is lower than one GPU-day on A100-80G.   \n\n\nD Quantization-aware training with LoRC\n\nIn order to better understand our proposed algorithm, LoRC, particularly in relation to the dimensions of low-rank matrices, we applied quantize-aware training alongside knowledge distillation. This approach builds upon the methodology of row-wise weight quantization and token-wise quantization. For the optimization process, we employed the Adam optimizer, setting the learning rate at 1e-4 and a dropout rate of 0.05. These settings were identified as the most effective in our context (additional details can be found in [33]). We performed fine-tuning on the WikiText dataset using pre-trained GPT2 models with 125M and 350M parameters, which were obtained from Hugging Face as our initial models. 7 The results are illustrated in Figure Figure D.1. As observed, the quantized models tend to overfit swiftly. However, implementing higher dropout values, such as 0.1, does not result in a significantly improved performance with regards to the best perplexity over the entire training duration. Now when examining the best perplexity associated with each dimension of LoRC (also indicated in the figure's legend), it becomes evident that the larger the dimension, the better the W4A8 models perform. This suggests that augmenting the dimension of LoRC can enhance the model quality for QAT, a finding that deviates from the trends observed in PTQ.  \n\n\nE Tables and Figures\n\nWe put the full results of our evaluations in this section.   \n\nFigure 1 :\n1The model size and quality trade-off of different quantization methods on models from OPT and BLOOM families.\n\n; ( 3 )\n3For smaller models (<1B), per-row W8A8 (INT8 weight and INT8 activation) RTN is enough based on [36].\n\nFigure D. 1 :\n1The graph on the left represents the results for a smaller model size (GPT2-125M), while the one on the right corresponds to the GPT2-350M model. The dimension (refer to the legend) in the LoRC algorithm, which is represented by different color curves, plays a pivotal role in approximating the original quality of the fp16 model.\n\nTable 2 :\n2Average PPL of OPT and BLOOM (BLM). See Table E.1 for all results.Precision \nOPT-6.7b OPT-13b OPT-30b OPT-66b BLM-1.7b BLM-3b BLM-7.1b BLM-176b \n\nW16-A16 \n11.90 \n11.22 \n10.70 \n10.33 \n20.43 \n17.58 \n14.96 \n10.90 \n\nW8 sym -A16 \n11.90 \n11.22 \n10.70 \n10.33 \n20.43 \n17.59 \n14.97 \n10.90 \nW8 asym -A16 \n11.90 \n11.22 \n10.70 \n10.33 \n20.45 \n17.59 \n14.97 \n10.90 \n\nW4 sym -A16 \n14.36 \n12.73 \n11.77 \n97.05 \n23.18 \n19.36 \n16.27 \n11.28 \nW4 asym -A16 \n13.44 \n12.09 \n11.52 \n31.52 \n22.47 \n19.01 \n15.90 \n11.20 \n\nW16-A8 sym \n26.04 \n3171.49 \n2048.21 \n2638.09 \n20.68 \n17.73 \n15.28 \n12.10 \nW16-A8 asym \n12.62 \n15.36 \n23.57 \n561.35 \n20.52 \n17.65 \n15.14 \n11.62 \n\n\n\n4 )\n4With INT8 activation, BLOOM shows no divergence issues up to a model size of 176B, whereas OPT performs poorly from \u2265 6.7B model sizes. to be effective and efficient in terms of GPU resources, time cost, and data usage for INT4 weight quantization. 2 In this work, we focus on the variants of GPTQ and ZeroQuant as well as the most straightforward baseline, round-to-nearest neighborhood (RTN).4 Are existing quantization methods optimally harnessing the po-\ntential to minimize LLMs sizes? \n\nNumerous lightweight optimization-based methods have been proposed, which update the model weights \nduring quantization. These methods such as [36, 12, 35], unlike quantization-aware training, only require a \nsmall portion of the training data and a limited training time. Particularly, GPTQ [12] and ZeroQuant [36], \nhave proven \n\nTable 3 :\n3The evaluation results of different PTQ methods on OPT and BLOOM (BLM) with asymmmetric quantization on weight or (and) activation. See more details inTable E.3 and Table E.6.Precision \nMethod \nOPT-6.7b OPT-13b OPT-30b OPT-66b BLM-1.7b BLM-3b BLM-7.1b BLM-176b \n\nW16A16 \n11.90 \n11.22 \n10.70 \n10.33 \n20.43 \n17.58 \n14.96 \n10.90 \n\nW4A16 \n\nRTN \n13.44 \n12.09 \n11.52 \n31.52 \n22.47 \n19.01 \n15.90 \n11.20 \nGPTQ \n12.28 \n11.42 \n10.78 \n10.52 \n21.58 \n18.33 \n15.50 \n11.02 \nZQ-Local  *  \n12.46 \n11.64 \n11.05 \n10.79 \n21.70 \n18.50 \n15.55 \n11.11 \nZQ-Global  *  \n12.38 \n11.62 \n11.04 \n10.68 \n21.38 \n18.33 \n15.52 \n11.05 \n\nW4A8 \n\nRTN \n14.80 \n26.36 \n86.26 \n815.00 \n22.75 \n19.17 \n16.19 \n12.22 \nGPTQ \n13.88 \n17.28 \n20.71 \n648.69 \n21.71 \n18.44 \n15.75 \n11.86 \nZQ-Local  *  \n13.24 \n14.23 \n18.53 \n16.32 \n21.86 \n18.66 \n15.75 \n11.19 \nZQ-Global  *  \n13.17 \n13.07 \n14.65 \n37.82 \n21.43 \n18.39 \n15.58 \n11.49 \n\n\n\nTable 4 :\n4Results of W4 asym -A16 quantization with various block-size out of the best result from optimizationbased methods on OPT and BLOOM (BLM). SeeTable E.15 and Table E.16 for full results including RTN. N/A means that the block size is not divisible by the hidden size.Block-size OPT-6.7b OPT-13b OPT-30b OPT-66b BLM-1.7b BLM-3b BLM-7.1b BLM-176bW16A16 \n11.90 \n11.22 \n10.70 \n10.33 \n20.43 \n17.58 \n14.96 \n10.90 \nPer-row \n12.28 \n11.42 \n10.78 \n10.52 \n21.38 \n18.33 \n15.50 \n11.02 \n\n1024 \n12.16 \n11.36 \n10.75 \n10.52 \n31.03 \nN/A \n15.24 \n10.96 \n512 \n12.08 \n11.32 \n10.73 \n10.52 \n20.93 \n17.99 \n15.20 \n10.95 \n256 \n12.05 \n11.28 \n10.74 \n10.50 \n20.95 \n17.97 \n15.18 \n10.95 \n128 \n12.10 \n11.28 \n10.74 \n10.44 \n20.92 \n17.90 \n15.17 \n10.94 \n32 \n12.03 \n11.28 \n10.72 \n10.41 \n20.82 \n17.88 \n15.16 \n10.95 \n\n\n\nTable 5 :\n5OPT W4 asym -A8 with various block-size out of the best result from GPTQ, ZQ-Local, and ZQ-Global on OPT and BLOOM (BLM). SeeTable E.20 for full results including RTN.Precision block-size (W|A) \nOPT-6.7b OPT-13b OPT-30b OPT-66b BLM-1.7b BLM-3b BLM-7.1b BLM-176b \n\nW4A16 \n128 | NA \n12.10 \n11.28 \n10.74 \n10.44 \n20.92 \n17.90 \n15.17 \n10.94 \n\nW4A8 \n\nCase-1: per-row | per-row \n13.17 \n13.07 \n14.65 \n16.32 \n21.43 \n18.39 \n15.58 \n11.19 \nCase-2: per-row | 128 \n12.29 \n11.45 \n10.80 \n10.61 \n21.59 \n18.31 \n15.52 \n11.03 \nCase-3: 128 | 128 \n12.04 \n11.31 \n10.75 \n10.45 \n21.27 \n17.86 \n15.19 \n10.96 \n\n\nTable 6 :\n6BLOOM-176B with different quantization \nblock sizes on activation. Here weight is asymmetri-\ncally quantized with block size 128. See more in Ta-\nble E.22. \n\nA8 Block Size 1024 \n512 \n256 \n128 \n32 \n\nPPL \n10.98 10.97 10.95 10.95 10.95 \n\n\n\nTable 7 :\n7W# asym -A16 quantization with # being 4-bit, 3-bit and 2-bit on OPT and BLOOM (BLM).LoRCCoarse-grained weight quantization (per-row block-size)Fine-grained quantization on weight (256 block-size )OPT-6.7b OPT-13b OPT-30b OPT-66b BLM-176b OPT-6.7b OPT-13b OPT-30b OPT-66b BLM-176bBits \n\nW8A16 \n11.90 \n11.22 \n10.70 \n10.33 \n10.90 \n11.90 \n11.22 \n10.70 \n10.33 \n10.90 \n\nW4A16 \n\u2717 \n12.28 \n11.42 \n10.78 \n10.78 \n11.02 \n12.05 \n11.28 \n10.74 \n10.50 \n10.95 \n\u2713 \n12.10 \n11.36 \n10.76 \n10.34 \n10.98 \n11.99 \n11.29 \n10.70 \n10.29 \n10.93 \n\nW3A16 \n\u2717 \n14.18 \n12.43 \n11.28 \n17.77 \n49.46 \n12.79 \n11.63 \n10.9 \n11.34 \n11.13 \n\u2713 \n13.00 \n11.90 \n11.14 \n10.63 \n11.30 \n12.40 \n11.57 \n10.83 \n10.42 \n11.08 \n\nW2A16 \n\u2717 \n120.56 \n40.17 \n25.74 \n225.45 \nExplode \n23.13 \n15.55 \n12.68 \n308.49 \n12.64 \n\u2713 \n24.17 \n18.53 \n14.39 \n13.01 \n14.15 \n16.27 \n14.30 \n12.37 \n11.54 \n12.21 \n\n\n\nTable 8 :\n8Results of W4 asym A16 quantization with LoRC approximating\u00ca =\u00dbV on OPT model family.\u00db and V can be represented with FP16 or INT8, of which the performance are represented below. There is hardly any difference between FP16 and INT8. Coarse-grained weight quantization Fain-grained weight QuantizationLoRC U ,V \n6.7b \n13b \n30b \n66b \n6.7b \n13b \n30b \n\nFP16 12.08 11.35 10.76 \n10.31 \n11.993 11.290 \n10.703 \nINT8 12.10 11.36 10.76 \n10.34 \n11.987 11.290 \n10.700 \n\n\n\nTable 9 :\n9W4A16 quantization with LoRC by varying the low-rank dimension m. Eigenvalues of the Error matrix E for W4A16LoRC-dim m \nOPT-1.3b OPT-6.7b OPT-30b \n\nm = 0 basline \n15.95 \n12.06 \n10.73 \n\nm = 1 \n15.93 \n12.01 \n10.73 \nm = 4 \n15.73 \n12.00 \n10.72 \nm = 8 \n15.76 \n11.99 \n10.70 \nm = 16 \n15.74 \n12.00 \n10.69 \nm = 32 \n15.71 \n12.01 \n10.69 \n\nFigure 2: \n\n\n19] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert: Integer-only bert quantization. In International conference on machine learning, pages 5506-5518. PMLR, 2021. Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2019. [28] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.[20] Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun, Kurt Keutzer, and Amir Gholami. \nA fast post-training pruning framework for transformers. arXiv preprint arXiv:2204.09656, 2022. \n\n[21] Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural information \nprocessing systems, pages 598-605, 1990. \n\n[22] Mary Ann Marcinkiewicz. Building a large annotated corpus of english: The penn treebank. Using \nLarge Corpora, page 273, 1994. \n\n[23] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. \nIn International Conference on Learning Representations, 2017. \n\n[24] OpenAI. Openai chatgpt. https://openai.com/blog/chatgpt/, 2022. \n\n[25] Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and quantization. \narXiv preprint arXiv:1802.05668, 2018. \n\n[26] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, \nJonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. \narXiv preprint arXiv:2211.05102, 2022. \n\n[27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi \nZhou, [29] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, \nand Kurt Keutzer. Q-BERT: Hessian based ultra low precision quantization of bert. In AAAI, pages \n8815-8821, 2020. \n\n[30] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared \nCasper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and \nmegatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint \narXiv:2201.11990, 2022. \n\n[31] Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, and Ngai Wong. \nCompression of generative pre-trained language models via quantization. arXiv preprint arXiv:2203.10705, \n2022. \n\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz \nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing \nsystems, pages 5998-6008, 2017. \n\n[33] Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong He. Understanding int4 \nquantization for transformer models: Latency speedup, composability, and failure cases. arXiv preprint \narXiv:2301.12017, 2023. \n\n[34] Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, and Yuxiong He. Extreme compression for \npre-trained transformers made simple and efficient. arXiv preprint arXiv:2206.01859, 2022. \n\n[35] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and \nefficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022. \n[36] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. \nZeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint \narXiv:2206.01861, 2022. \n\n[37] Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos. Gobo: Quantizing attention-\nbased nlp models for low latency and energy efficient inference. In 2020 53rd Annual IEEE/ACM \nInternational Symposium on Microarchitecture (MICRO), pages 811-824. IEEE, 2020. \n\n[38] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8BERT: Quantized 8bit bert. arXiv \npreprint arXiv:1910.06188, 2019. \n\n[39] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan \nXu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint \narXiv:2210.02414, 2022. \n\n[40] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher \nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. \narXiv preprint arXiv:2205.01068, 2022. \n\n[41] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. Ternarybert: \nDistillation-aware ultra-low bit bert. arXiv preprint arXiv:2009.12812, 2020. \n\n\nTable C .\nC1: Best optimization method of OPT family in Section 4. Weight & Activation (W4A8) ZQ-Global ZQ-Global ZQ-Global GPTQ ZQ-Global ZQ-Global ZQ-Global ZQ-LocalTable C.2: Best optimization method of BLOOM family in Section 4.Precision \n125m \n350m \n1.3b \n2.7b \n6.7b \n13b \n30b \n66b \n\nWeight Only (INT4) \nZQ-Global ZQ-Global \nGPTQ \nGPTQ \nGPTQ \nGPTQ \nGPTQ \nGPTQ \n\nPrecision \n560m \n1.1b \n1.7b \n3b \n7.1b \n176b \n\nWeight Only (INT4) \nGPTQ \nZQ-Global ZQ-Global ZQ-Global/GPTQ \nGPTQ \nGPTQ \n\nWeight & Activation (W4A8) ZQ-Global ZQ-Global ZQ-Global \nZQ-Global \nZQ-Global ZQ-Local \n\n\n\nTable B .\nB1: The table on the left illustrates the outcomes of each task, evaluated using three different random seeds. On the right, we present a table detailing the mean and standard deviation of the Task-mean values (which can be found in the final column of the left table) over the three random seeds, accompanied by additional quantization results. The quantization methodologies employed in this context are based on the GPTQ algorithm.Precision Random Seed WikiText PTB \nC4 \nTask-mean \n\nOPT-13b \n123 \n10.31 \n12.62 11.35 \n11.43 \n\nW4A16 \n234 \n10.25 \n12.57 11.35 \n11.39 \n456 \n10.37 \n12.61 11.36 \n11.44 \n\nOPT-30b \n123 \n9.56 \n11.95 10.79 \n10.77 \n\nW4A16 \n234 \n9.6 \n11.95 10.79 \n10.78 \n456 \n9.52 \n11.97 10.79 \n10.76 \n\nPrecision \nItems \nOPT-1.3b OPT-13b OPT-30b \n\nW4A16 \nmean over three random seeds \n16.39 \n11.42 \n10.77 \nstandard deviation \n0.019 \n0.027 \n0.010 \n\nW4A8 \nmean over three random seeds \n16.76 \n17.16 \n21.64 \nstandard deviation \n0.048 \n0.048 \n1.277 \n\nC Best PTQ Methods with Per-row Quantization \n\n\n\nTable C .\nC1 and C.2 summarize the best PTQ methods with per-row optimization.\n\nTable E .\nE1: OPT ppl on wikitext/ptb/c4 (full results ofTable 2).Precision \n125m \n350m \n1.3b \n2.7b \n6.7b \n13b \n30b \n66b \n\nW16-A16 \n27.65/32.55/24.61 22.00/26.08/20.71 14.62/16.97/14.72 12.47/15.11/13.17 10.86/13.09/11.74 \n10.13/12.34/11.20 \n9.56/11.84/10.69 \n9.34/11.36/10.28 \n\nW8A8 sym -A16 27.64/32.53/24.65 22.06/26.10/20.72 14.63/16.98/14.73 12.48/15.13/13.17 10.85/13.11/11.75 \n10.12/12.34/11.20 \n9.55/11.85/10.70 \n9.34/11.36/10.29 \nW8 asym -A16 \n27.71/32.58/24.64 22.04/26.12/20.73 14.67/16.99/14.73 12.50/15.14/13.17 10.86/13.11/11.75 \n10.11/12.34/11.20 \n9.55/11.84/10.69 \n9.35/11.36/10.29 \nW4 sym -A16 \n45.89/53.68/36.68 25.95/31.11/23.94 19.85/23.61/18.90 22.86/30.01/22.29 12.41/17.05/13.62 \n11.06/14.90/12.23 \n10.18/13.26/11.86 \n57.73/134.91/98.51 \nW4 asym -A16 \n36.71/44.76/30.92 25.51/30.90/23.86 19.38/21.95/17.93 17.92/22.48/18.32 11.91/15.39/13.01 \n10.67/13.53/12.07 \n10.10/13.13/11.33 \n20.24/48.45/25.86 \n\nW16-A8 sym \n27.96/32.57/24.69 22.06/26.42/20.95 15.21/18.18/15.81 12.98/16.01/13.89 20.99/25.94/31.18 3341.50/2618.38/3554.59 1681.48/2221.62/2241.53 2696.91/2647.41/2569.94 \nW16-A8 asym \n27.84/32.60/24.66 22.04/26.22/20.81 15.14/17.65/15.39 12.51/15.38/13.38 11.24/14.17/12.45 \n11.83/18.87/15.39 \n14.08/31.54/25.09 \n442.66/524.57/716.83 \n\nTable E.2: BLOOM ppl on wikitext/ptb/c4 (full results of Table 2). \n\nPrecision \n560m \n1.1b \n1.7b \n3b \n7.1b \n176b \n\nW16-A16 \n22.43/41.25/24.38 17.69/46.98/20.29 15.39/27.93/17.97 13.48/23.12/16.14 11.37/19.40/14.13 8.11/13.62/10.97 \n\nW8 sym -A16 \n22.44/41.28/24.39 17.70/47.01/20.29 15.40/27.91/17.98 13.49/23.14/16.14 11.37/19.40/14.13 8.11/13.63/10.98 \nW8 asym -A16 22.43/41.24/24.40 17.69/47.00/20.29 15.40/27.96/17.97 13.48/23.14/16.14 11.37/19.40/14.13 8.10/13.62/10.98 \nW4 sym -A16 \n26.49/49.73/27.98 20.27/56.64/22.81 17.47/32.20/19.88 14.96/25.59/17.51 12.38/21.36/15.06 8.40/14.15/11.30 \nW4 asym -A16 25.31/46.79/27.10 23.90/68.31/25.99 16.93/31.02/19.47 14.65/25.12/17.26 12.06/20.83/14.83 8.34/14.03/11.23 \n\nW16-A8 sym \n22.50/41.58/24.46 17.78/47.28/20.38 15.57/28.36/18.13 13.57/23.38/16.25 11.58/19.92/14.35 8.75/14.94/12.61 \nW16-A8 asym 22.45/41.37/24.42 17.71/47.05/20.32 15.45/28.09/18.02 13.52/23.24/16.19 11.47/19.71/14.25 8.41/14.52/11.93 \n\n\n\nTable E .\nE3: OPT ppl on wikitext/opt/c4 with W4 asym -A16(full table of Table 3). SeeTable E.4 for all learning rate results of ZQ-Local and Table E.5 of ZQ-Global. Table E.6: BLOOM ppl on wikitext/opt/c4 with W4 asym -A16 (full table of Table 3). See Table E.4 for all learning rate results of ZQ-Local and Table E.5 of ZQ-Global. Global * 23.84/44.17/25.60 19.50/51.33/21.72 16.19/29.28/18.66 14.14/24.16/16.69 11.77/20.27/14.52 8.24/13.82/11.10Table E.7: BLOOM ppl on wikitext/opt/c4 with W4 asym -A16 and ZQ-Local.Precision \n125m \n350m \n1.3b \n2.7b \n6.7b \n13b \n30b \n66b \n\nRTN \n36.71/44.76/30.92 25.51/30.90/23.86 19.38/21.95/17.93 17.92/22.48/18.32 11.91/15.39/13.01 10.67/13.53/12.07 10.10/13.13/11.33 20.24/48.45/25.86 \nGPTQ \n32.52/40.25/27.78 23.50/29.14/22.41 15.52/18.16/15.56 13.02/15.84/13.73 11.16/13.59/12.08 10.29/12.61/11.35 9.61/11.95/10.79 \n9.54/11.67/10.52 \nZQ-Local  *  \n33.05/39.34/28.11 24.40/29.22/22.82 15.81/18.66/15.76 13.22/16.19/13.96 11.32/13.79/12.26 10.42/12.90/11.60 9.97/12.32/11.03 \n9.91/11.87/10.59 \nZQ-Global  *  31.44/36.66/27.21 23.32/28.05/21.98 15.46/18.31/15.67 13.03/16.04/13.83 11.30/13.69/12.17 10.38/12.85/11.62 9.90/12.24/10.99 \n9.62/11.81/10.61 \n\nTable E.4: OPT ppl on wikitext/opt/c4 with W4 asym -A16 and ZQ-Local. \n\nLR (W4 asym -A16) \n125m \n350m \n1.3b \n2.7b \n6.7b \n13b \n30b \n66b \n\n0.001 \n33.67/39.45/29.11 26.33/31.94/24.49 16.27/19.91/16.46 14.34/17.76/14.93 11.87/15.04/13.06 13.68/18.89/14.46 171.35/151.55/46.14 814.22/601.74/308.53 \n0.0005 \n32.76/39.51/28.64 25.88/30.95/23.96 16.29/19.82/16.27 14.16/17.65/14.79 11.92/15.23/12.95 10.93/13.82/12.03 \n10.23/13.46/11.44 \n10.10/12.27/10.81 \n0.0001 \n33.86/40.01/28.29 24.64/30.26/23.33 16.07/19.25/15.93 14.36/17.38/14.41 11.85/14.64/12.74 10.93/13.48/11.88 \n10.18/12.67/11.13 \n10.12/12.01/10.67 \n5e-05 \n33.05/39.34/28.11 25.42/29.65/23.22 15.79/19.16/15.88 13.70/16.80/14.16 11.71/14.32/12.41 10.75/13.38/11.77 \n9.95/12.54/11.09 \n10.02/11.89/10.64 \n1e-05 \n33.78/40.41/28.84 24.40/29.22/22.82 15.81/18.66/15.76 13.55/16.46/13.96 11.32/13.79/12.26 10.54/13.05/11.61 \n9.98/12.22/10.99 \n9.91/11.87/10.59 \n5e-06 \n34.47/41.04/29.02 24.50/29.27/23.00 16.01/18.73/15.91 13.22/16.19/13.96 11.33/13.86/12.29 10.42/12.90/11.60 \n9.86/12.33/10.97 \n9.97/11.86/10.60 \n1e-06 \n35.88/43.69/30.35 24.54/29.87/23.17 16.77/19.45/16.47 13.60/17.02/14.46 11.41/14.10/12.41 10.53/13.01/11.70 \n9.97/12.33/11.04 \n10.01/11.93/10.66 \n\nTable E.5: OPT ppl on wikitext/opt/c4 with W4 asym -A16 and ZQ-Global. NaN here means the PPL is larger \nthan 1e6. \n\nLR (W4 asym -A16) \n125m \n350m \n1.3b \n2.7b \n6.7b \n13b \n30b \n66b \n\n0.001 \n4057.13/2718.91/1247.78 5071.35/5229.93/687.35 12105.25/10154.73/7893.43 18965.76/17112.60/16316.31 60014.66/56041.86/78085.84 232421.09/98305.32/119762.73 \n93917.09/70170.34/51124.06 \nNaN \n0.0005 \n31.94/38.61/27.17 \n27.11/33.91/24.07 \n10900.84/8322.65/8425.10 \n14412.30/8676.76/10154.55 18527.46/13530.12/13029.95 109006.53/62584.41/125349.50 303235.75/230599.62/430480.03 36439.32/30554.19/33756.93 \n0.0001 \n31.44/36.66/27.21 \n24.08/29.08/22.27 \n15.91/20.08/16.35 \n118.38/53.47/54.08 \n7604.92/5339.10/5161.49 \n12638.86/7639.95/8243.63 \n16276.68/9890.26/6176.27 \n8367.31/4728.13/5533.59 \n5e-05 \n31.97/36.93/27.12 \n23.55/28.06/22.02 \n15.82/18.65/15.65 \n13.40/16.44/13.97 \n26.54/25.67/17.60 \n909.99/316.82/370.84 \n6238.21/3291.04/3743.01 \n9296.98/6687.44/5363.29 \n1e-05 \n32.31/37.93/27.38 \n23.32/28.05/21.98 \n15.60/18.42/15.64 \n13.09/16.05/13.78 \n11.41/13.82/12.20 \n10.80/13.16/11.66 \n10.06/12.44/11.07 \n9.73/12.09/10.98 \n5e-06 \n32.69/38.91/27.76 \n23.26/28.33/22.05 \n15.46/18.31/15.67 \n13.03/16.04/13.83 \n11.30/13.69/12.17 \n10.50/12.89/11.58 \n9.95/12.28/11.01 \n9.62/11.81/10.61 \n1e-06 \n34.63/41.75/29.43 \n23.82/28.96/22.48 \n16.12/19.46/16.27 \n13.03/16.27/14.04 \n11.29/13.88/12.27 \n10.38/12.85/11.62 \n9.90/12.24/10.99 \n9.58/12.17/10.78 \n5e-07 \nNaN \nNaN \nNaN \nNaN \nNaN \n10.51/12.96/11.70 \n9.89/12.41/11.04 \n9.90/12.45/11.00 \n1e-07 \nNaN \nNaN \nNaN \nNaN \nNaN \n10.63/13.29/11.89 \n10.02/12.82/11.18 \n11.03/13.91/11.73 \n5e-08 \nNaN \nNaN \nNaN \nNaN \nNaN \n10.66/13.42/11.97 \n10.05/13.00/11.24 \n12.41/17.45/13.02 \n\nPrecision \n560m \n1.1b \n1.7b \n3b \n7.1b \n176b \n\nRTN \n25.31/46.79/27.10 23.90/68.31/25.99 16.93/31.02/19.47 14.65/25.12/17.26 12.06/20.83/14.83 8.34/14.03/11.23 \nGPTQ \n23.90/43.76/25.59 24.34/68.10/26.58 16.36/29.58/18.79 14.10/24.23/16.66 11.80/20.23/14.47 8.22/13.78/11.07 \nZQ-Local  *  \n24.23/44.94/26.05 19.22/52.36/21.59 16.37/29.89/18.86 14.23/24.41/16.86 11.80/20.28/14.56 8.27/13.91/11.16 \nZQ-LR (W4 asym -A16) \n560m \n1.1b \n1.7b \n3b \n7.1b \n176b \n\n0.001 \n25.37/47.36/27.03 19.89/53.86/22.11 16.70/31.19/19.30 14.45/25.28/17.16 12.22/21.34/15.04 8.82/15.77/11.98 \n0.0005 \n25.17/46.83/26.87 19.57/53.66/21.92 16.58/30.27/19.15 14.43/25.47/17.07 11.94/20.54/14.67 8.35/14.01/11.20 \n0.0001 \n24.59/46.11/26.32 19.22/52.36/21.59 16.41/30.29/18.90 14.35/24.81/16.87 11.83/20.34/14.58 8.28/13.92/11.14 \n5e-05 \n24.44/46.04/26.16 23.28/65.68/25.42 16.39/30.01/18.86 14.34/24.43/16.83 11.80/20.28/14.56 8.27/13.93/11.15 \n1e-05 \n24.23/44.94/26.05 23.45/66.29/25.52 16.37/29.89/18.86 14.23/24.41/16.86 11.84/20.39/14.58 8.27/13.91/11.16 \n5e-06 \n24.21/45.21/26.10 23.26/65.72/25.42 16.42/30.09/18.94 14.25/24.55/16.87 11.87/20.50/14.61 8.29/13.98/11.16 \n1e-06 \n24.71/45.86/26.50 23.45/66.28/25.56 16.64/30.52/19.15 14.46/24.76/17.04 11.94/20.55/14.70 8.29/13.97/11.18 \n\nTable E.8: BLOOM ppl on wikitext/opt/c4 with W4 asym -A16 and ZQ-Global. \n\nLR (W4 asym -A16) \n560m \n1.1b \n1.7b \n3b \n7.1b \n176b \n\n0.001 \n6853935.00/30441738.00/3222857.25 528072.88/828428.62/356031.97 597410.50/973155.88/1280478.12 878460.69/2175974.25/441401.94 \nnan/nan/nan \nNaN \n0.0005 \n29671.52/1795030.88/4653.35 \n28112.96/87515.64/1826.82 \n141110.14/204295.86/40146.11 \n265457.25/741326.38/99882.45 \n944784.19/774538.25/395960.03 \nNaN \n0.0001 \n23.92/45.68/25.72 \n19.34/52.78/21.63 \n16.35/29.22/18.76 \n14.27/24.46/16.80 \n12.17/22.16/14.80 \nNaN \n5e-05 \n23.84/44.17/25.60 \n19.50/51.33/21.72 \n16.19/29.28/18.66 \n14.14/24.16/16.69 \n11.81/20.41/14.50 \nNaN \n1e-05 \n23.85/44.20/25.65 \n22.64/56.79/23.41 \n16.23/29.73/18.73 \n14.14/24.31/16.74 \n11.77/20.27/14.52 \n8.24/13.82/11.10 \n5e-06 \n24.02/44.62/25.79 \n23.46/63.27/24.88 \n16.28/29.83/18.81 \n14.19/24.38/16.80 \n11.77/20.33/14.54 \n8.24/13.82/11.10 \n1e-06 \n24.46/45.41/26.20 \n24.62/70.16/26.64 \n16.48/30.15/19.02 \n14.35/24.56/16.95 \n11.89/20.54/14.67 \n8.23/13.82/11.12 \n5e-07 \nNaN \nNaN \nNaN \nNaN \nNaN \n8.26/13.86/11.13 \n\n\nTable E .\nE9: OPT ppl on wikitext/opt/c4 with W4 asym -A8 sym /A8 asym . SeeTable E.10 for all learning rate results of ZQ-Local andTable E.11 of ZQ-Global.Table E.11: OPT ppl on wikitext/opt/c4 with W4 asym -A8 sym /A8 asym and ZQ-Global.Precision \n125m \n350m \n1.3b \n2.7b \n6.7b \n13b \n30b \n66b \n\nW4 asym -A8 sym Block \nRTN \n36.69/44.34/30.60 26.59/32.13/24.81 25.31/26.89/22.01 30.84/35.73/29.01 164.51/110.85/162.94 4460.61/3145.51/4255.84 3216.45/2929.40/3570.19 3038.22/2930.92/3001.82 \nGPTQ \n32.20/38.49/27.47 24.35/29.82/23.24 16.28/19.64/16.73 13.86/17.51/15.00 \n46.22/53.98/55.13 \n3611.71/2796.71/3820.57 1738.44/1810.08/2119.82 5992.87/4115.01/4360.16 \nZQ-Local  *  \n32.88/38.23/28.20 25.18/30.06/23.62 16.78/20.25/17.09 14.82/18.77/15.61 \n16.08/21.15/18.77 \n2680.33/1876.48/3052.51 1884.90/1603.23/1348.08 \n575.20/499.42/437.94 \nZQ-Global  *  \n32.04/37.48/27.23 24.01/28.81/22.57 16.12/19.15/16.23 13.98/17.70/14.87 \n38.27/39.77/52.26 \n117.83/141.63/96.83 \n253.71/700.40/337.15 \n1715.98/1546.50/1799.35 \n\nW4 asym -A8 asym Block \nRTN \n36.61/44.48/30.64 25.79/31.28/24.13 21.23/23.54/19.19 23.82/29.77/22.60 \n13.18/17.04/14.19 \n19.87/32.93/26.28 \n36.07/136.88/85.84 \n627.15/880.79/937.08 \nGPTQ \n32.22/38.83/27.43 23.90/29.29/22.63 15.75/18.74/15.93 13.23/16.31/14.03 \n12.50/15.86/13.29 \n12.79/21.99/17.05 \n12.96/25.03/24.14 \n495.70/681.68/768.69 \nZQ-Local  *  \n33.60/38.57/28.02 24.57/29.27/22.98 15.98/19.13/16.20 13.44/16.81/14.26 \n11.76/14.97/13.00 \n11.69/16.98/14.01 \n12.38/24.25/18.96 \n12.19/23.31/13.47 \nZQ-Global  *  \n31.61/37.00/27.10 23.66/28.56/22.21 15.77/18.61/15.83 13.09/16.56/14.00 \n12.03/14.60/12.86 \n11.80/15.01/12.41 \n12.94/17.61/13.41 \n31.51/58.00/23.95 \n\nTable E.10: OPT ppl on wikitext/opt/c4 with W4 asym -A8 sym /A8 asym and ZQ-Local. \n\nPrecision \n125m \n350m \n1.3b \n2.7b \n6.7b \n13b \n30b \n66b \n\nW4 asym -A8 sym Block \n0.001 \n34.91/40.43/29.37 26.82/32.68/25.24 17.68/21.72/18.11 19.40/27.59/20.05 36.70/59.32/45.17 \n7240.89/5506.67/4889.34 \n8229.32/5068.14/5005.13 \nDiverge \n0.0005 \n34.16/39.00/28.58 26.75/32.05/24.60 17.19/21.42/17.55 19.43/25.54/19.41 29.33/48.38/43.28 56836.57/36810.64/31073.67 5448.96/3826.63/3196.49 \n575.20/499.42/437.94 \n0.0001 \n32.88/38.23/28.20 25.31/31.60/23.98 16.93/20.77/17.36 17.05/21.50/17.42 25.24/31.66/26.82 \n6125.07/3817.01/4121.70 \n1884.90/1603.23/1348.08 \n5427.12/3449.58/3289.01 \n5e-05 \n32.86/39.17/27.91 25.91/31.24/24.07 16.99/20.02/17.23 15.07/19.00/15.54 16.08/21.15/18.77 \n6037.51/3617.64/3819.63 \n3266.46/2533.64/2463.21 11631.78/10489.81/7880.43 \n1e-05 \n34.00/39.76/28.62 25.40/30.60/23.75 16.87/20.26/17.11 14.82/18.77/15.61 26.60/32.09/28.76 \n5346.85/3788.29/4903.31 \n3364.70/2372.71/3370.97 \n5793.44/3544.90/3925.34 \n5e-06 \n34.37/41.46/28.71 25.18/30.06/23.62 16.78/20.25/17.09 14.87/19.42/15.86 34.53/39.98/38.22 \n2680.33/1876.48/3052.51 \n3566.45/2532.54/3678.75 \n4916.96/3783.69/3716.49 \n1e-06 \n36.05/43.46/30.00 25.73/30.69/24.05 19.58/22.57/19.04 18.66/24.19/19.98 77.99/62.27/83.19 \n3893.00/2672.11/3849.59 \n3233.72/2944.44/3732.18 \n4238.57/3621.09/3541.33 \n\nW4 asym -A8 asym Block \n0.001 \n33.57/40.84/29.00 27.29/32.48/24.68 17.41/20.70/17.07 15.98/20.45/16.23 12.63/17.21/14.25 \n9889.96/7605.54/6328.91 \n2009.66/1637.69/2011.15 \n5070.07/3124.56/2683.19 \n0.0005 \n34.58/40.45/28.69 25.81/31.56/24.09 16.89/20.66/16.93 15.00/19.47/15.61 12.55/17.00/14.29 \n13.18/19.65/15.18 \n36.51/75.89/60.58 \n3249.10/63.17/119.55 \n0.0001 \n33.91/38.39/28.12 25.37/31.24/23.66 16.78/20.09/16.72 14.26/18.49/14.90 12.13/15.97/13.48 \n13.48/20.42/16.68 \n110.20/117.28/257.96 \n12.19/23.31/13.47 \n5e-05 \n33.60/38.57/28.02 24.67/29.60/23.34 16.31/19.56/16.42 13.90/19.16/15.05 12.30/15.95/13.56 \n12.05/18.00/15.77 \n37.68/59.83/124.75 \n29.72/95.99/69.60 \n1e-05 \n33.80/40.21/28.56 24.57/29.27/22.98 15.98/19.13/16.20 13.44/16.81/14.26 11.76/14.97/13.00 \n11.69/16.98/14.01 \n14.39/31.47/24.45 \n217.93/313.13/298.24 \n5e-06 \n34.62/41.07/28.93 24.68/29.46/23.12 16.26/19.23/16.27 13.44/17.00/14.36 11.96/14.86/13.10 \n12.31/18.55/15.16 \n12.38/24.25/18.96 \n85.96/185.07/180.88 \n1e-06 \n35.94/43.35/30.00 24.92/30.18/23.45 17.98/20.89/17.45 14.79/18.90/15.52 12.10/15.47/13.35 \n15.48/22.00/17.84 \n14.86/31.16/26.21 \n411.89/620.52/652.55 \n\nPrecision \n125m \n350m \n1.3b \n2.7b \n6.7b \n13b \n30b \n66b \n\nW4 asym -A8 sym Block \n0.001 \n34.90/44.82/28.27 8988.08/5862.33/384.69 \nnan/nan/nan \n18290.16/9784.37/12099.01 16014.50/8655.69/12304.55 248961.98/84832.78/104880.55 \n56675.05/23709.03/33007.17 \n29782.43/20410.10/23559.66 \n0.0005 \n31.78/38.56/27.20 \n39.24/54.15/29.76 \n10610.96/9438.99/6752.84 12499.29/8411.26/10677.01 \nnan/nan/nan \n74731.13/44494.68/29286.49 \n51871.73/28548.95/23056.78 \n18717.63/11744.97/12903.33 \n0.0001 \n32.04/37.48/27.23 \n24.14/29.21/22.47 \n17.04/23.64/17.13 \n175.67/165.81/162.24 \n12305.50/11472.90/10223.89 \n16303.04/10731.12/10669.52 \n22548.81/12474.28/7405.46 \n7926.43/4377.36/4805.98 \n5e-05 \n32.16/37.54/27.27 \n24.15/28.87/22.46 \n16.02/19.61/16.59 \n13.88/20.27/14.79 \n5241.10/3284.47/2187.15 \n13297.25/7781.85/7467.30 \n9542.44/4543.45/5373.00 \nNaN \n1e-05 \n32.57/38.43/27.53 \n24.01/28.81/22.57 \n16.12/19.15/16.23 \n13.98/17.70/14.87 \n99.27/118.19/88.74 \n529.82/361.44/256.46 \n1936.12/1388.68/947.45 \n10077.70/9208.34/11462.28 \n5e-06 \n32.83/38.37/27.71 \n24.13/29.30/22.68 \n16.45/19.64/16.57 \n14.42/18.01/15.27 \n70.26/62.28/54.47 \n373.82/494.33/170.40 \n820.90/847.19/543.59 \n1867.57/1878.76/4117.49 \n1e-06 \n34.79/41.79/29.30 \n24.68/30.01/23.23 \n17.90/21.94/18.01 \n14.83/18.63/15.70 \n38.27/39.77/52.26 \n117.83/141.63/96.83 \n261.19/844.40/272.04 \n1500.51/1275.54/1649.50 \n5e-07 \nNaN \nNaN \nNaN \nNaN \nNaN \nNaN \n253.71/700.40/337.15 \n1715.98/1546.50/1799.35 \n1e-07 \nNaN \nNaN \nNaN \nNaN \nNaN \nNaN \n913.95/1117.58/1065.87 \n2012.91/1917.48/1817.92 \n\nW4 asym -A8 asym Block \n0.001 \n37.89/47.68/30.43 9023.01/4309.50/1186.96 \n12638.86/nan/9164.64 \n11285.86/6477.19/nan \n12222.01/6933.34/8989.30 \n132962.69/73768.05/59268.76 328993.91/187752.97/163157.59 48298.52/30548.89/42797.96 \n0.0005 \n32.65/39.86/27.20 \n28.46/36.94/24.68 \nnan/nan/nan \nnan/nan/nan \n23287.96/15508.32/16243.28 \n22052.30/10852.90/11588.02 \n63084.59/39919.41/42499.90 \nNaN \n0.0001 \n31.61/37.00/27.10 \n24.64/29.13/22.28 \n16.31/19.71/16.44 \n43.76/29.11/33.35 \n22024.01/13962.04/14130.94 \n10171.49/7200.78/7954.12 \n18603.08/11639.42/10798.26 \nnan/nan/nan \n5e-05 \n32.21/37.46/27.18 \n23.66/28.56/22.21 \n16.02/19.02/15.92 \n13.48/17.57/14.24 \n839.48/213.76/286.05 \n1035.13/nan/1472.08 \n8085.92/3545.21/4893.07 \nnan/nan/nan \n1e-05 \n32.35/38.21/27.38 \n23.59/28.66/22.24 \n15.77/18.61/15.83 \n13.09/16.56/14.00 \n12.09/14.69/12.90 \n11.80/15.01/12.41 \n13.76/22.87/15.72 \n974.58/1557.95/1039.65 \n5e-06 \n32.59/38.49/27.68 \n23.62/28.63/22.33 \n15.78/18.80/15.95 \n13.23/16.65/14.12 \n12.03/14.60/12.86 \n12.72/16.31/13.20 \n12.94/17.61/13.41 \n83.35/137.83/128.11 \n1e-06 \n34.68/41.56/29.26 \n24.08/29.21/22.68 \n16.66/20.03/16.69 \n13.30/16.74/14.33 \n12.43/15.52/13.36 \n12.28/16.13/13.19 \n16.00/19.60/14.88 \n31.51/58.00/23.95 \n5e-07 \nNaN \nNaN \nNaN \nNaN \nNaN \nNaN \nNaN \n31.09/73.23/24.44 \n1e-07 \nNaN \nNaN \nNaN \nNaN \nNaN \nNaN \nNaN \n241.81/544.81/505.58 \n\nTable E.12: BLOOM ppl on wikitext/opt/c4 with W4 asym -A8 sym /A8 asym . See Table E.13 for all learning \nrate results of ZQ-Local and Table E.14 of ZQ-Global. \n\nPrecision \n560m \n1.1b \n1.7b \n3b \n7.1b \n176b \n\nW4 asym -A8 sym Block \nRTN \n25.56/47.53/27.31 24.80/70.99/26.71 17.36/31.95/19.89 14.82/25.63/17.47 12.33/21.62/15.13 9.12/15.58/14.04 \nGPTQ \n24.13/44.79/25.86 25.69/68.65/27.08 16.63/30.54/19.12 14.18/24.42/16.82 12.04/21.07/14.75 8.92/15.16/13.56 \nZQ-Local  *  \n24.45/45.73/26.22 19.50/52.67/21.73 16.71/30.23/19.09 14.37/24.72/16.99 12.00/20.79/14.78 8.52/14.29/11.41 \nZQ-Global  *  \n23.93/44.31/25.68 19.71/51.98/21.85 16.34/29.36/18.82 14.13/24.34/16.76 11.84/20.58/14.59 8.76/14.60/11.68 \n\nW4 asym -A8 asym Block \nRTN \n25.37/46.99/27.16 24.08/68.95/26.17 17.12/31.46/19.67 14.74/25.38/17.37 12.22/21.36/15.00 8.73/15.10/12.83 \nGPTQ \n24.09/44.29/25.66 24.50/67.37/26.62 16.39/29.83/18.91 14.13/24.47/16.73 11.91/20.72/14.62 8.55/14.74/12.31 \nZQ-Local  *  \n24.29/45.19/26.10 19.13/52.89/21.63 16.54/30.11/18.92 14.32/24.73/16.94 11.94/20.63/14.68 8.33/14.01/11.22 \nZQ-Global  *  \n23.86/44.16/25.62 19.54/51.72/21.79 16.23/29.40/18.68 14.15/24.29/16.72 11.80/20.37/14.56 8.62/14.40/11.49 \n\n\nTable E .\nE13: BLOOM ppl on wikitext/opt/c4 with W4 asym -A8 sym /A8 asym and ZQ-Local.36.13/26.52 23.29/27.94/21.98 14.93/17.51/15.10 12.49/15.59/13.46 10.87/13.34/11.90 10.11/12.47/11.27 9.60/11.88/10.73 9.44/11.53/10Global * 29.52/34.63/25.98 22.78/27.56/21.65 15.02/17.50/15.07 12.67/15.37/13.45 10.92/13.42/11.96 10.16/12.61/11.41 9.74/12.01/10.82 9.43/11.49/10.40 36.68/26.87 24.28/28.95/22.59 15.21/18.15/15.47 13.20/16.13/13.75 11.01/13.71/12.17 10.27/12.79/11.49 9.82/12.05/10.89 9.46/11.70/10Global * 29.69/34.24/25.72 22.94/27.49/21.54 14.90/17.43/15.01 12.80/15.47/13.44 10.92/13.33/11.93 10.21/12.58/11.38 9.69/12.01/10.81 9.41/11.49/10.39 36.32/26.64 23.88/28.66/22.36 14.99/17.87/15.32 12.89/16.00/13.67 10.89/13.70/12.13 10.32/12.73/11.45 9.76/12.00/10.85 9.56/11.55/10Global * 28.93/34.29/25.63 22.85/27.23/21.50 14.80/17.34/14.99 12.74/15.32/13.40 10.82/13.36/11.91 10.23/12.61/11.37 9.68/11.95/10.80 9.37/11.47/10.38 Table E.16: BLOOM W4 asym -A16 with various block-size out of the best result from GPTQ and ZQ-Global. Global 23.50/43.53/25.23 18.31/49.06/20.82 15.93/28.47/18.38 13.82/23.92/16.47 09/45.13/26.02 18.87/52.29/21.44 16.27/29.72/18.76 14.16/24.42/16.90 121.09/281.67/88.59 12.55/27.29/15.60 31/43.43/25.12 18.36/49.13/20.79 16.07/29.10/18.46 13.76/23.61/16.38 11.55/19.72/14.29 8.14/13.70/11.01 ZQ-Global 23.17/43.16/25.13 18.24/48.78/20.75 15.81/28.71/18.32 13.79/23.69/16.42 11.59/19.92/14.36 8.17/13.80/11.06 Global 23.14/42.95/24.97 18.17/48.53/20.70 15.75/28.71/18.29 13.73/23.65/16.37 ZQ-Global 23.00/42.80/24.91 18.10/48.30/20.64 15.68/28.55/18.25 13.70/23.63/16.36 11.53/19.67/14.27 8.17/13.72/11.02 Global 23.07/42.63/24.82 18.07/48.07/20.59 15.66/28.58/18.21 13.72/23.59/16.33 Table E.17: OPT full results of three-bit weight with various block-size. 74/45.10/31.85 1777.53/1304.55/852.03 1604.07/1407.49/1487.78 25.13/40.56/40.08 130.75/175.33/135.67 Table E.18: BLOOM W3 asym -A16 with various block-size out of the best result from GPTQ and ZQ-Global.Table E.20: OPT full results of Table 5. W4 asym full row and A8 sym 128 RTN 36.64/44.84/30.90 25.58/31.06/23.99 19.96/22.31/18.20 18.42/23.01/18.56 12.04/15.92/13.20 10.79/13.65/12.11 10.10/13.17/11.37 20.50/45.58/25.37 82/38.82/27.54 23.78/28.96/22.61 15.56/18.27/15.62 13.02/15.88/13.76 11.22/13.59/12.11 10.25/12.65/11.37 9.56/11.94/10.79 69/36.66/27.19 23.47/28.18/22.03 15.53/18.35/15.73 13.02/16.11/13.82 11.29/13.70/12.19 10.43/12.91/11.64 9.86/12.28/11.00 W4 asym 128 and A8 sym 128 RTN 30.61/36.57/27.08 24.14/29.47/22.80 15.46/18.68/15.77 13.24/16.36/13.95 11.16/14.08/12.35 10.35/12.89/11.57 9.95/12.15/10.95 47/36.45/26.45 23.43/28.12/22.06 14.90/17.62/15.17 12.51/15.63/13.48 10.88/13.35/11.93 10.17/12.48/11.28 9.58/11.86/10.74 59/34.68/25.91 22.59/27.93/21.68 14.87/17.55/15.11 12.65/15.45/13.48 10.88/13.40/11.94 10.20/12.67/11.43 9.74/12.W4 asym full row and A8 asym 128 RTN 36.61/44.71/30.85 25.50/30.93/23.88 19.58/22.08/18.01 19.53/24.38/19.68 11.91/15.35/13.01 10.68/13.50/12.02 10.13/13.21/11.37 17.90/32.15/39.58/27.65 23.48/28.92/22.46 15.43/18.24/15.55 12.92/15.94/13.74 11.17/13.59/12.09 10.35/12.63/11.36 9.65/11.55/37.49/27.25 23.34/28.33/22.08 15.52/18.55/15.61 13.07/16.09/13.82 11.32/13.65/12.16 10.42/12.86/11.63 9.86/12.30/11W4 asym 128 and A8 asym 128 RTN 30.59/36.56/27.07 24.11/29.43/22.74 15.38/18.57/15.69 13.22/16.32/13.91 11.13/13.97/12.30 10.34/12.82/11.55 9.98/12.15/1047/36.19/26.40 23.35/27.96/21.94 14.92/17.57/15.12 12.48/15.60/13.46 10.87/13.34/11.91 10.20/12.45/11.28 9.62/11.88/1085/34.52/26.10 22.70/27.72/21.64 14.96/17.55/15.09 12.64/15.40/13.47 10.93/13.43/11.95 10.18/12.68/11.42 9.74/12.02/10Table E.21: BLOOM full results of Table 6. W4 asym full row and A8 sym 128 RTN 25.32/46.98/27.12 23.87/68.29/25.97 16.99/31.15/19.51 14.69/25.22/17.30 12.07/20.86/14.84 8.34/14.05/11.24 00/44.47/25.66 24.14/66.95/26.17 16.38/29.64/18.79 14.10/24.19/16.67 11.77/20.22/14.48 8.20/13.82/11.07 92/44.23/25.69 22.53/57.71/23.51 16.25/29.72/18.74 14.12/24.26/16.74 11.78/20.30/14.53 8.24/13.82/11.10 W4 asym 128 and A8 sym 128 RTN 23.84/44.94/25.79 18.65/51.54/21.21 16.18/30.03/18.70 14.04/24.32/16.77 23.05/48.33/23.69 8.87/15.68/11.72 22/43.24/25.01 18.25/48.89/20.74 16.00/29.44/18.41 13.77/23.68/16.35 11.54/19.76/14.27 8.13/13.69/11.01 Global 23.12/43.22/25.03 18.19/48.96/20.72 15.75/28.81/18.30 13.73/23.65/16.39 11.57/19.85/14.32 8.17/13.76/11.03 W4 asym full row and A8 asym 128 RTN 25.30/46.87/27.10 23.90/68.31/25.98 16.96/31.09/19.48 14.68/25.19/17.28 12.07/20.86/14.84 8.34/14.06/11.24 97/44.15/25.62 24.61/68.19/26.53 16.36/29.77/18.81 14.10/24.17/16.66 11.78/20.32/14.49 8.20/13.82/11.07 88/44.40/25.68 22.63/57.91/23.39 16.25/29.77/18.74 14.17/24.24/16.74 11.77/20.28/14.52 8.25/13.82/11.10 W4 asym 128 and A8 asym 128 RTN 23.83/44.89/25.77 18.63/51.46/21.19 16.16/29.95/18.68 14.03/24.27/16.75 23.51/49.07/23.96 8.85/15.65/11.72 26/43.24/25.00 18.18/48.84/20.73 16.05/29.34/18.42 13.69/23.56/16.34 11.54/19.75/14.28 8.14/13.71/11.02 Global 23.12/43.14/25.01 18.18/48.99/20.73 15.71/28.73/18.30 13.74/23.68/16.39 11.56/19.85/14.31 8.17/13.78/11.04 Table E.22: Full results of Table 6. PPL 8.16/13.75/11.04 8.15/13.75/11.02 8.15/13.70/11.01 8.13/13.69/11.01 8.14/13.69/11.01 8.14/13.69/11.01 Table E.23: Results of applying LoRC on top of ZQ-Global for INT8 Activation. Learning Rate LoRC-dim 5.00E-05 1.00E-05 5.00E-06 1.00E-06 5.00E-07 bestPrecision \n560m \n1.1b \n1.7b \n3b \n7.1b \n176b \n\nW4 asym -A8 sym Block \n0.001 \n25.51/47.89/27.15 19.73/54.63/22.18 16.96/31.47/19.44 14.59/25.69/17.32 12.51/21.85/15.34 8.62/14.42/11.50 \n0.0005 \n25.18/47.35/26.95 19.62/53.64/22.03 16.98/31.75/19.47 14.52/25.22/17.18 12.03/21.01/14.82 8.59/14.38/11.45 \n0.0001 \n24.79/46.37/26.44 19.50/52.67/21.73 16.68/30.51/19.18 14.44/25.12/17.05 12.00/20.79/14.78 8.52/14.29/11.41 \n5e-05 \n24.56/46.29/26.34 23.93/69.17/26.19 16.71/30.23/19.09 14.37/24.72/16.99 12.05/20.92/14.82 8.55/14.34/11.44 \n1e-05 \n24.45/45.73/26.22 23.65/66.73/25.80 16.66/30.69/19.16 14.40/24.94/17.02 12.12/21.14/14.86 8.65/14.97/12.01 \n5e-06 \n24.48/45.66/26.33 23.87/67.26/25.84 16.78/30.72/19.23 14.44/24.91/17.07 12.15/21.23/14.88 8.70/15.04/12.37 \n1e-06 \n24.91/46.35/26.72 24.09/68.13/26.05 17.03/31.28/19.52 14.60/25.18/17.24 12.22/21.31/14.99 8.91/15.25/13.35 \n\nW4 asym -A8 asym Block \n0.001 \n25.26/46.43/26.98 19.69/54.26/22.14 16.88/32.16/19.40 15.15/26.58/17.76 12.40/22.29/15.28 8.40/14.06/11.26 \n0.0005 \n24.89/47.99/26.82 19.54/53.57/21.98 16.73/31.02/19.29 14.50/25.52/17.11 11.94/20.70/14.76 8.33/14.01/11.22 \n0.0001 \n24.60/45.75/26.44 19.13/52.89/21.63 16.54/30.36/19.10 14.37/24.91/16.93 11.94/20.63/14.68 8.35/14.04/11.24 \n5e-05 \n24.41/45.08/26.23 23.59/67.14/25.79 16.54/30.11/18.92 14.29/24.83/16.92 11.95/20.71/14.71 8.36/14.10/11.25 \n1e-05 \n24.29/45.19/26.10 23.35/65.26/25.38 16.51/30.20/19.00 14.32/24.73/16.94 11.97/20.93/14.74 8.44/14.30/11.45 \n5e-06 \n24.31/45.25/26.15 23.41/66.18/25.48 16.63/30.37/19.09 14.33/24.74/16.96 12.03/20.95/14.78 8.52/14.66/11.86 \n1e-06 \n24.76/45.92/26.62 23.52/66.38/25.66 16.81/30.71/19.30 14.53/24.92/17.14 12.10/21.07/14.87 8.62/14.92/12.41 \n\nTable E.14: BLOOM ppl on wikitext/opt/c4 with W4 asym -A8 sym /A8 asym and ZQ-Global. \n\nPrecision \n560m \n1.1b \n1.7b \n3b \n7.1b \n176b \n\nW4 asym -A8 sym Block \n0.001 \n174250016.00/201477664.00/1348168.88 423532.56/906908.06/322995.69 573201.81/1089364.38/498071.91 \n544376.56/696942.56/540949.06 \nnan/nan/nan \nNaN \n0.0005 \n70978.52/29214230.00/1151.72 \n2880.81/15732.60/309.13 \n505479.44/629035.56/29283.36 \n140595.53/181082.25/33785.79 \n378033.53/789890.00/191543.91 \nNaN \n0.0001 \n24.04/45.38/25.83 \n19.44/52.38/21.77 \n16.34/29.36/18.82 \n14.32/24.74/16.88 \n12.12/22.00/14.80 \n249.47/26690.76/26.96 \n5e-05 \n23.93/44.31/25.68 \n19.71/51.98/21.85 \n16.18/29.71/18.71 \n14.13/24.34/16.76 \n11.84/20.58/14.59 \n9.00/15.57/11.61 \n1e-05 \n23.99/44.44/25.77 \n22.75/58.31/23.63 \n16.28/29.96/18.81 \n14.29/24.53/16.87 \n11.87/20.57/14.64 \n8.76/14.60/11.68 \n5e-06 \n24.14/44.77/25.90 \n23.90/64.81/25.29 \n16.36/30.03/18.91 \n14.32/24.68/16.95 \n11.91/20.60/14.71 \n9.07/15.12/11.98 \n1e-06 \n24.62/45.70/26.33 \n25.55/71.49/27.44 \n16.61/30.47/19.17 \n14.51/24.91/17.11 \n12.06/20.93/14.86 \n11.25/19.93/15.76 \n\nW4 asym -A8 asym Block \n0.001 \n9059092.00/2932002.50/131873960.00 \n499829.19/393190.53/346682.47 1260531.12/2019747.88/460627.16 1022130.19/872164.88/679662.62 \nnan/nan/nan \nNaN \n0.0005 \n7633.14/378055.53/1032.16 \n4271.83/85847.50/1555.66 \n87087.04/217513.30/37000.13 \n575008.56/814032.50/230285.80 1212241.00/2389840.25/1504266.50 \nNaN \n0.0001 \n23.96/45.36/25.80 \n19.37/52.25/21.88 \n16.29/29.36/18.81 \n14.32/24.66/16.86 \n12.05/22.30/14.77 \n1400.84/11880.12/392.79 \n5e-05 \n23.86/44.16/25.62 \n19.54/51.72/21.79 \n16.23/29.40/18.68 \n14.15/24.29/16.72 \n11.82/20.44/14.54 \n8.73/20.30/11.41 \n1e-05 \n23.96/44.24/25.72 \n22.55/58.10/23.49 \n16.27/29.82/18.78 \n14.16/24.35/16.80 \n11.80/20.37/14.56 \n8.62/14.40/11.49 \n5e-06 \n24.01/44.68/25.83 \n23.67/64.20/25.08 \n16.30/29.96/18.85 \n14.24/24.49/16.86 \n11.81/20.50/14.60 \n8.69/14.56/11.58 \n1e-06 \n24.53/45.60/26.26 \n24.82/71.17/26.84 \n16.55/30.35/19.10 \n14.40/24.76/17.01 \n11.97/20.83/14.77 \n9.14/16.63/17.69 \n\nTable E.15: OPT full results of Table 4. \n\nMethod \n125m \n350m \n1.3b \n2.7b \n6.7b \n13b \n30b \n66b \n\nBS=1024 \nRTN \nN/A \n25.42/30.62/23.61 16.90/19.78/16.59 \nN/A \n11.63/14.41/12.65 10.47/13.09/11.75 9.97/12.40/11.09 9.83/12.31/10.77 \nN/A \n26.55 \n17.76 \nN/A \n12.90 \n11.77 \n11.15 \n10.97 \nGPTQ \nN/A \n23.65/29.09/22.43 15.16/18.00/15.34 \nN/A \n11.10/13.40/11.99 10.28/12.49/11.29 9.58/11.91/10.75 9.56/11.61/10.44 \nN/A \n25.05 \n16.17 \nN/A \n12.16 \n11.36 \n10.75 \n10.54 \nZQ-Global  *  \nN/A \n23.27/27.97/21.93 12.93/15.90/13.64 \nN/A \n10.98/13.60/12.04 10.33/12.69/11.50 9.78/12.16/10.90 9.52/11.58/10.46 \nN/A \n24.39 \n16.18 \nN/A \n12.21 \n11.50 \n10.95 \n10.52 \n\nBS=512 \nRTN \nN/A \n25.05/29.74/23.21 15.71/19.05/16.09 13.67/16.93/14.23 11.32/14.22/12.50 10.45/12.99/11.68 10.03/12.27/11.03 9.83/12.15/10.67 \nN/A \n26.00 \n16.95 \n14.94 \n12.68 \n11.71 \n11.11 \n10.89 \nGPTQ \nN/A \n23.33/28.48/22.13 15.15/17.95/15.26 12.65/15.61/13.53 10.94/13.37/11.94 10.18/12.49/11.29 9.58/11.87/10.75 9.53/11.59/10.43 \nN/A \n24.65 \n16.12 \n13.93 \n12.08 \n11.32 \n10.73 \n10.52 \nZQ-Global  *  \nN/A \n23.41/27.67/21.92 14.91/17.73/15.25 12.92/15.59/13.55 11.08/13.51/11.99 10.29/12.68/11.46 9.79/12.16/10.87 9.51/11.65/10.44 \nN/A \n24.34 \n15.97 \n14.02 \n12.19 \n11.48 \n10.94 \n10.53 \n\nBS=256 \nRTN \n31.62/38.19/27.62 24.76/29.44/22.96 15.54/18.96/15.90 13.56/16.62/14.02 11.19/14.12/12.40 10.39/12.93/11.61 9.95/12.24/10.98 9.70/12.09/10.62 \n32.48 \n25.72 \n16.80 \n14.73 \n12.57 \n11.64 \n11.06 \n10.80 \nGPTQ \n30.56/37.20/26.68 23.37/28.33/21.97 14.95/17.63/15.16 12.59/15.60/13.49 10.93/13.29/11.92 10.15/12.43/11.27 9.58/11.91/10.74 9.49/11.60/10.40 \n31.48 \n24.56 \n15.91 \n13.89 \n12.05 \n11.28 \n10.74 \n10.50 \nZQ-Global  *  30.45/35.35/26.24 23.06/27.72/21.74 14.93/17.45/15.15 12.99/15.47/13.50 10.96/13.45/12.00 10.25/12.61/11.43 9.73/12.14/10.89 9.49/11.58/10.42 \n30.68 \n24.17 \n15.84 \n13.99 \n12.14 \n11.43 \n10.92 \n10.50 \n\nBS=128 \nRTN \n30.62/36.67/27.10 24.12/29.34/22.70 15.35/18.52/15.66 13.19/16.24/13.88 11.11/13.94/12.28 10.31/12.82/11.54 9.93/12.12/10.93 9.56/11.85/10.56 \n31.47 \n25.39 \n16.51 \n14.43 \n12.44 \n11.56 \n11.00 \n10.65 \nGPTQ \n30.76/.40 \n31.14 \n24.40 \n15.85 \n13.85 \n12.03 \n11.28 \n10.74 \n10.45 \nZQ-30.04 \n23.99 \n15.86 \n13.83 \n12.10 \n11.39 \n10.86 \n10.44 \n\nBS=64 \nRTN \n30.74/.49 \n31.43 \n25.27 \n16.28 \n14.36 \n12.30 \n11.52 \n10.92 \n10.55 \nGPTQ \n30.25/35.72/26.43 23.39/27.55/21.75 14.81/17.40/15.06 12.54/15.54/13.44 10.87/13.29/11.89 10.09/12.44/11.27 9.55/11.89/10.72 9.33/11.49/10.38 \n30.80 \n24.23 \n15.76 \n13.84 \n12.02 \n11.27 \n10.72 \n10.40 \nZQ-29.88 \n23.99 \n15.78 \n13.90 \n12.06 \n11.39 \n10.84 \n10.43 \n\nBS=32 \nRTN \n30.48/.44 \n31.14 \n24.97 \n16.06 \n14.18 \n12.24 \n11.50 \n10.87 \n10.52 \nGPTQ \n29.13/34.89/25.90 23.09/27.59/21.65 14.80/17.41/15.04 12.45/15.55/13.42 10.89/13.32/11.89 10.08/12.48/11.27 9.51/11.92/10.73 \nDiverge \n29.97 \n24.11 \n15.75 \n13.81 \n12.03 \n11.28 \n10.72 \nDiverge \nZQ-29.62 \n23.86 \n15.71 \n13.82 \n12.03 \n11.41 \n10.81 \n10.41 \nMethod \n560m \n1.1b \n1.7b \n3b \n7.1b \n176b \n\nBS=1024 \nRTN \n24.90/46.37/26.68 \nN/A \n16.57/30.14/19.00 \nN/A \n1019.51/1351.45/601.35 53.41/160.05/43.64 \n32.65 \nN/A \n21.90 \nN/A \n990.77 \n85.70 \nGPTQ \n23.90/43.99/25.47 \nN/A \n16.12/29.13/18.61 \nN/A \n11.57/19.82/14.33 \n8.16/13.70/11.02 \n31.12 \nN/A \n21.29 \nN/A \n15.24 \n10.96 \nZQ-Global 23.62/43.90/25.41 \nN/A \n15.98/28.67/18.44 \nN/A \n11.91/20.84/14.58 \n8.23/13.94/11.09 \n30.98 \nN/A \n21.03 \nN/A \n15.78 \n11.09 \n\nBS=512 \nRTN \n24.78/46.07/26.45 19.41/53.64/21.85 16.47/29.84/18.88 14.29/24.84/17.05 \n142.38/314.10/100.09 \n33.88/103.57/31.02 \n32.44 \n31.63 \n21.73 \n18.73 \n185.52 \n56.16 \nGPTQ \n23.63/43.96/25.36 18.52/49.73/20.91 16.07/29.87/18.50 13.79/23.77/16.41 \n11.54/19.75/14.30 \n8.14/13.70/11.02 \n30.98 \n29.72 \n21.48 \n17.99 \n15.20 \n10.95 \nZQ-11.85/20.17/14.42 \n8.20/13.86/11.07 \n30.75 \n29.40 \n20.93 \n18.07 \n15.48 \n11.04 \n\nBS=256 \nRTN \n24.31.75 \n30.87 \n21.58 \n18.49 \n163.78 \n18.48 \nGPTQ \n23.30.62 \n29.42 \n21.21 \n17.92 \n15.18 \n10.95 \n30.49 \n29.26 \n20.95 \n17.97 \n15.29 \n11.01 \n\nBS=128 \nRTN \n23.82/44.78/25.75 18.62/51.31/21.17 16.13/29.89/18.66 14.00/24.19/16.71 \n23.90/49.80/24.15 \n8.84/15.62/11.70 \n31.45 \n30.37 \n21.56 \n18.30 \n32.62 \n12.06 \nGPTQ \n23.27/43.10/24.99 18.14/48.72/20.73 16.03/28.96/18.41 13.72/23.65/16.34 \n11.52/19.73/14.26 \n8.14/13.67/11.01 \n30.45 \n29.20 \n21.13 \n17.90 \n15.17 \n10.94 \nZQ-11.56/19.77/14.32 \n8.17/13.78/11.03 \n30.35 \n29.13 \n20.92 \n17.92 \n15.22 \n10.99 \n\nBS=64 \nRTN \n23.65/44.04/25.51 18.53/50.02/21.03 16.06/29.57/18.60 13.93/23.95/16.60 \n11.85/20.51/14.65 \n8.31/14.14/11.18 \n31.07 \n29.86 \n21.41 \n18.16 \n15.67 \n11.21 \nGPTQ \n23.11/42.95/24.94 18.14/48.87/20.65 16.00/28.91/18.38 13.72/23.68/16.33 \n11.51/19.70/14.27 \n8.14/13.69/11.00 \n30.33 \n29.22 \n21.10 \n17.91 \n15.16 \n10.94 \n30.24 \n29.01 \n20.82 \n17.90 \n15.16 \n10.97 \n\nBS=32 \nRTN \n23.60/43.91/25.50 18.63/50.13/21.04 15.98/29.56/18.56 13.92/23.90/16.53 \n11.65/20.01/14.43 \n8.20/13.86/11.07 \n31.00 \n29.93 \n21.37 \n18.12 \n15.36 \n11.04 \nGPTQ \n23.10/43.19/24.91 18.17/48.35/20.66 15.95/28.95/18.36 13.76/23.60/16.33 \n11.53/19.71/14.27 \n8.14/13.70/11.00 \n30.40 \n29.06 \n21.08 \n17.89 \n15.17 \n10.95 \nZQ-11.52/19.71/14.26 \n8.16/13.69/11.01 \n30.18 \n28.91 \n20.82 \n17.88 \n15.16 \n10.95 \nMethod \n125m \n350m \n1.3b \n2.7b \n6.7b \n13b \n30b \n66b \n\nFull Row \nRTN \n2095.20/1848.83/1222.00 47.43/53.38/36.93 4399.18/4400.98/3551.88 8326.78/4208.57/4895.83 878.00/735.86/910.10 1953.43/1953.60/1669.76 439.39/691.94/437.96 1465.06/1564.59/1282.58 \n1722.01 \n45.91 \n4117.35 \n5810.40 \n841.32 \n1858.93 \n523.09 \n1437.41 \nGPTQ \n845.81/599.71/496.14 \n30.65/34.09/26.15 \n20.23/27.39/19.45 \n15.91/19.26/16.01 \n12.69/15.90/13.96 \n11.36/13.71/12.21 \n10.10/12.54/11.20 \n16.77/21.16/15.39 \n647.22 \n30.30 \n22.36 \n17.06 \n14.18 \n12.43 \n11.28 \n17.77 \nZQ-Global  *  \n46.47/58.55/35.45 \n29.64/36.51/25.55 \n32.48/94.57/28.97 \n60.91/116.22/36.45 \n23.87/29.75/23.88 \n44.70/60.78/46.18 \n13.16/20.49/13.48 \n28.93/75.91/27.28 \n46.82 \n30.57 \n52.01 \n71.19 \n25.83 \n50.55 \n15.71 \n44.04 \n\nBS=1024 \nRTN \nN/A \n44.57/49.58/35.09 1950.00/2317.55/1913.55 3810.79/2563.06/3054.91 \n50.01/70.17/99.21 \n265.62/417.03/261.93 \n362.47/252.33/364.45 \n523.81/846.60/1021.17 \nN/A \n43.08 \n2060.37 \n3142.92 \n73.13 \n314.86 \n326.42 \n797.20 \nGPTQ \nN/A \n29.78/33.76/25.66 \n19.03/23.32/18.14 \nN/A \n11.69/14.31/12.70 \n10.56/12.96/11.70 \n9.89/12.19/11.02 \n12.84/16.17/13.02 \nN/A \n29.73 \n20.16 \nN/A \n12.90 \n11.74 \n11.03 \n14.01 \nZQ-Global  *  \nN/A \n29.19/34.57/25.11 \n19.83/29.77/19.79 \nN/A \n13.99/18.82/14.76 \n13.43/19.28/13.76 \n11.10/14.46/11.94 \n11.87/14.86/12.13 \nN/A \n29.62 \n23.13 \nN/A \n15.86 \n15.49 \n12.50 \n12.95 \n\nBS=512 \nRTN \nN/A \n37.620.53/340.68/416.28 \n198.01/457.78/426.15 \nN/A \n38.23 \n1311.37 \n1499.78 \n35.26 \n147.25 \n459.16 \n360.65 \nGPTQ \nN/A \n28.46/32.54/25.14 \n18.02/21.35/17.46 \n14.38/17.24/14.79 \n11.57/14.33/12.57 \n10.41/12.97/11.64 \n9.77/12.18/10.97 \n11.89/14.48/12.40 \nN/A \n28.71 \n18.94 \n15.47 \n12.82 \n11.67 \n10.97 \n12.92 \nZQ-Global  *  \nN/A \n27.81/33.57/24.55 \n18.31/23.54/17.99 \n18.10/29.47/17.15 \n12.54/16.60/13.62 \n11.82/15.98/12.81 \n10.48/13.36/11.66 \n11.26/13.95/11.79 \nN/A \n28.65 \n19.95 \n21.57 \n14.25 \n13.54 \n11.83 \n12.33 \nMethod \n560m \n1.1b \n1.7b \n3b \n7.1b \n176b \nFull row \nRTN \n68.45/132.83/59.22 118.61/317.41/99.65 31.15/67.23/34.02 31.07/59.03/32.17 66140.72/78568.16/44504.19 100371.84/166012.19/137892.34 \n86.83 \n178.56 \n44.14 \n40.76 \n63071.02 \n134758.79 \nGPTQ \n46.92/84.69/39.50 \n49.78/142.95/43.84 19.70/41.35/21.74 22.84/46.49/22.90 52966.59/52979.88/37115.48 \nDiverge \n57.04 \n78.85 \n27.59 \n30.74 \n47687.32 \nDiverge \nZQ-Global 33.20/64.61/32.30 \n34.16/100.05/29.22 19.22/36.30/21.25 18.41/33.10/20.79 \n273.55/439.59/100.79 \n27.19/75.74/45.45 \n43.37 \n54.48 \n25.59 \n24.10 \n271.31 \n49.46 \n\nBS=1024 \nRTN \n47.00/86.57/43.37 \n70.81/230.74/70.78 35.41/65.75/33.54 22.12/40.65/24.55 25654.77/25531.66/15868.46 141324.41/183583.73/200436.33 \n58.98 \n124.11 \n44.90 \n29.11 \n22351.63 \n175114.82 \nGPTQ \n31.25/58.80/30.94 \nN/A \n19.11/37.07/20.90 \nN/A \n12.59/21.95/15.21 \n8.31/13.96/11.17 \n40.33 \nN/A \n25.69 \nN/A \n16.58 \n11.15 \nZQ-Global 28.91/55.81/29.59 \nN/A \n18.20/34.13/20.40 \nN/A \n30.94/119.98/21.39 \n15.98/32.85/19.85 \n38.10 \nN/A \n24.24 \nN/A \n57.44 \n22.89 \nMethod \n125m \n350m \n1.3b \n2.7b \n6.7b \n13b \n30b \n66b \n\n37.46 \n26.88 \n20.16 \n20.00 \n13.72 \n12.18 \n11.54 \n30.48 \nGPTQ \n31.9.62/11.72/10.54 \n32.73 \n25.12 \n16.48 \n14.22 \n12.31 \n11.42 \n10.76 \n10.63 \nZQ-Local \n9.79/11.94/10.65 \n10.79 \nZQ-Global \n31.9.62/11.84/10.63 \n31.85 \n24.56 \n16.54 \n14.32 \n12.39 \n11.66 \n11.05 \n10.70 \n\n9.58/11.90/10.58 \n31.42 \n25.47 \n16.64 \n14.52 \n12.53 \n11.60 \n11.02 \n10.69 \nGPTQ \n30.9.35/11.54/10.40 \n31.12 \n24.54 \n15.90 \n13.87 \n12.05 \n11.31 \n10.73 \n10.43 \nZQ-Local \n9.40/11.63/10.51 \n10.51 \nZQ-Global \n29.03/10.83 \n9.40/11.51/10.42 \n30.06 \n24.07 \n15.84 \n13.86 \n12.08 \n11.43 \n10.87 \n10.44 \n\n15/20.02 \n37.39 \n26.77 \n19.89 \n21.20 \n13.42 \n12.07 \n11.57 \n23.36 \nGPTQ \n32.95/10.79 \n9.58/11.71/10.55 \n33.13 \n24.95 \n16.40 \n14.20 \n12.29 \n11.45 \n10.80 \n10.61 \nZQ-Local \n10.05/11.91/10.61 \n10.86 \nZQ-Global \n31..00 \n9.67/12.22/10.86 \n32.10 \n24.58 \n16.56 \n14.33 \n12.37 \n11.64 \n11.05 \n10.91 \n\n.96 \n9.57/11.86/10.58 \n31.41 \n25.43 \n16.55 \n14.49 \n12.47 \n11.57 \n11.03 \n10.67 \nGPTQ \n30..74 \n9.39/11.55/10.41 \n31.02 \n24.42 \n15.87 \n13.85 \n12.04 \n11.31 \n10.75 \n10.45 \nZQ-Local \n9.37/11.70/10.49 \n10.52 \nZQ-Global \n29..83 \n9.39/11.53/10.42 \n30.16 \n24.02 \n15.86 \n13.84 \n12.10 \n11.42 \n10.86 \n10.45 \n\nMethod \n560m \n1.1b \n1.7b \n3b \n7.1b \n176b \n\n33.14 \n39.38 \n22.55 \n19.07 \n15.92 \n11.21 \nGPTQ \n24.31.37 \n39.09 \n21.61 \n18.32 \n15.49 \n11.03 \nZQ-Local \n8.30/14.01/11.20 \n11.17 \nZQ-Global \n23.31.28 \n34.58 \n21.57 \n18.38 \n15.53 \n11.05 \n\n31.53 \n30.46 \n21.64 \n18.38 \n31.69 \n12.09 \nGPTQ \n23.30.49 \n29.29 \n21.29 \n17.93 \n15.19 \n10.95 \nZQ-Local \n8.20/13.87/11.08 \n11.05 \nZQ-30.45 \n29.29 \n20.95 \n17.92 \n15.25 \n10.99 \n\n33.09 \n39.39 \n22.51 \n19.05 \n15.92 \n11.21 \nGPTQ \n23.31.24 \n39.78 \n21.65 \n18.31 \n15.53 \n11.03 \nZQ-Local \n8.32/13.97/11.20 \n11.16 \nZQ-Global \n23.31.32 \n34.64 \n21.59 \n18.38 \n15.52 \n11.06 \n\n31.50 \n30.43 \n21.60 \n18.35 \n32.18 \n12.08 \nGPTQ \n23.30.50 \n29.25 \n21.27 \n17.86 \n15.19 \n10.96 \nZQ-Local \n8.19/13.90/11.07 \n11.06 \nZQ-30.42 \n29.30 \n20.91 \n17.94 \n15.24 \n11.00 \nBlock SIze \n1024 \n512 \n256 \n128 \n64 \n32 \n\nLearning Rate \nmodel-size precision LoRC-dim \n0.0005 \n0.0001 \n5.00E-05 1.00E-05 5.00E-06 \nBest \n\n125m \nW4A8 \n\n0 \n4482.1 \n31.15 \n30.40 \n30.55 \n30.72 \n30.40 \n8 \n5996.14 \n30.96 \n30.24 \n30.37 \n30.61 \n30.24 \n16 \n3577.12 \n31.02 \n30.26 \n30.2 \n30.37 \n30.20 \n\n125m \nW3A8 \n\n0 \n4283.28 \n41.03 \n40.93 \n55.74 \n86.34 \n40.93 \n8 \n2396.92 \n37.25 \n36.65 \n37.85 \n39.06 \n36.65 \n16 \n1787.74 \n36.66 \n36.55 \n37.46 \n38.21 \n36.55 \n\n125m \nW2A8 \n\n0 \n3473.18 \n583.72 \n996.76 \n2480.69 \n3203.11 583.72 \n8 \n3815.37 \n144.85 \n160.71 \n362.17 \n466.98 \n144.85 \n16 \n3324.23 \n135.25 \n156.28 \n295.78 \n372.7 \n135.25 \n\n350m \nW4A8 \n\n0 \n25.65 \n24.38 \n24.34 \n24.55 \n24.75 \n24.34 \n8 \n25.56 \n24.3 \n24.24 \n24.45 \n24.66 \n24.24 \n16 \n25.45 \n24.39 \n24.21 \n24.39 \n24.63 \n24.21 \n\n350m \nW3A8 \n\n0 \n30.59 \n28.45 \n28.94 \n31.51 \n32.39 \n28.45 \n8 \n30.1 \n28.22 \n28.71 \n30.81 \n32.09 \n28.22 \n16 \n30.64 \n28.02 \n28.50 \n30.62 \n31.69 \n28.02 \n\n350m \nW2A8 \n\n0 \n97.40 \n177.43 \n257.61 \n668.19 \n722.19 \n97.4 \n8 \n95.79 \n139.68 \n194.36 \n437.18 \n459.92 \n95.79 \n16 \n106.51 \n137.81 \n172.93 \n400.91 \n421.59 \n106.51 \n\nWe tested the method proposed by [35] but did not find it better than others for INT4 weight quantization.3 The bias term (a.k.a., the zero point) can be simply fused into the previous activation quantization kernel[36].\nIn the MHA module, LoRC contributes 2hm to each of key, query, value, and the projection matrices. In the MLP module, LoRC contributes 8hm and 2hm respectively to the matrices of dimensions h \u00d7 4h and 4h \u00d7 h.5 For INT8 Activation, please seeTable E.23, the observation for FP16 holds similarly for INT8 Activation.\nPlease note that this observation is only true for PTQ. If one uses quantize-aware training (QAT) and let\u00db andV updated during QAT, we arrive at contrasting conclusions. For more details, please refer to Appendix D.\nhttps://huggingface.co/gpt2\n\nHaoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, Irwin King, arXiv:2012.15701Binarybert: Pushing the limit of bert quantization. arXiv preprintHaoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin King. Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701, 2020.\n\nUnderstanding and overcoming the challenges of efficient transformer quantization. Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort, arXiv:2109.12948arXiv preprintYelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges of efficient transformer quantization. arXiv preprint arXiv:2109.12948, 2021.\n\nLanguage models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, arXiv:2005.14165arXiv preprintTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n\nPushing the limits of narrow precision inferencing at cloud scale with microsoft floating point. Daniel Bita Darvish Rouhani, Ritchie Lo, Ming Zhao, Jeremy Liu, Kalin Fowers, Anna Ovtcharov, Sarah Vinogradsky, Lita Massengill, Ray Yang, Bittner, Advances in neural information processing systems. 33Bita Darvish Rouhani, Daniel Lo, Ritchie Zhao, Ming Liu, Jeremy Fowers, Kalin Ovtcharov, Anna Vinogradsky, Sarah Massengill, Lita Yang, Ray Bittner, et al. Pushing the limits of narrow precision inferencing at cloud scale with microsoft floating point. Advances in neural information processing systems, 33:10271-10281, 2020.\n\nTim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, arXiv:2208.073398-bit matrix multiplication for transformers at scale. arXiv preprintint8 (Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multipli- cation for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n\nThe case for 4-bit precision: k-bit inference scaling laws. Tim Dettmers, Luke Zettlemoyer, arXiv:2212.09720arXiv preprintTim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720, 2022.\n\nK Steven, Jeffrey L Esser, Deepika Mckinstry, Rathinakumar Bablani, Dharmendra S Appuswamy, Modha, arXiv:1902.08153Learned step size quantization. arXiv preprintSteven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019.\n\nTraining with quantization noise for extreme fixed-point compression. Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Remi Gribonval, Herve Jegou, Armand Joulin, arXiv:2004.07320arXiv preprintAngela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Remi Gribonval, Herve Jegou, and Armand Joulin. Training with quantization noise for extreme fixed-point compression. arXiv preprint arXiv:2004.07320, 2020.\n\nOptimal brain compression: A framework for accurate post-training quantization and pruning. Elias Frantar, Dan Alistarh, arXiv:2208.11580arXiv preprintElias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning. arXiv preprint arXiv:2208.11580, 2022.\n\nMassive language models can be accurately pruned in one-shot. Elias Frantar, Dan Alistarh, arXiv:2301.00774arXiv preprintElias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023.\n\nGptq: Accurate post-training quantization for generative pre-trained transformers. Elias Frantar, Torsten Saleh Ashkboos, Dan Hoefler, Alistarh, arXiv:2210.17323arXiv preprintElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n\nA survey of quantization methods for efficient neural network inference. Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, W Michael, Kurt Mahoney, Keutzer, arXiv:2103.13630arXiv preprintAmir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021.\n\nAi and memory wall. Amir Gholami, Zhewei Yao, Sehoon Kim, W Michael, Kurt Mahoney, Keutzer, RiseLab Medium Post. Amir Gholami, Zhewei Yao, Sehoon Kim, Michael W Mahoney, and Kurt Keutzer. Ai and memory wall. RiseLab Medium Post, 2021.\n\n. Github, Github, GitHub. Github copilot. https://github.com/features/copilot/, 2021.\n\nSecond order derivatives for network pruning: Optimal brain surgeon. Babak Hassibi, G David, Stork, Advances in neural information processing systems. Babak Hassibi and David G Stork. Second order derivatives for network pruning: Optimal brain surgeon. In Advances in neural information processing systems, pages 164-171, 1993.\n\nDistilling the knowledge in a neural network. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, Workshop paper in NIPSGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. Workshop paper in NIPS, 2014.\n\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu, Tinybert, arXiv:1909.10351,2019.BS=256RTN4349.14/2907.61/2510.7535.36/42Distilling bert for natural language understanding. arXiv preprintXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351, 2019. BS=256 RTN 4349.14/2907.61/2510.75 35.36/42\n\n. Bs=128 Rtn, 3446.89/2156.26/1484.15 33.13/41.23/29.51BS=128 RTN 3446.89/2156.26/1484.15 33.13/41.23/29.51\n\n19: Full results of BLOOM-176B. E Table, Table E.19: Full results of BLOOM-176B\n\n. N/A N/A N/A, N/A N/A N/A\n", "annotations": {"author": "[{\"end\":147,\"start\":112},{\"end\":183,\"start\":148},{\"end\":216,\"start\":184},{\"end\":257,\"start\":217},{\"end\":279,\"start\":258}]", "publisher": null, "author_last_name": "[{\"end\":122,\"start\":119},{\"end\":158,\"start\":156},{\"end\":192,\"start\":190},{\"end\":229,\"start\":225},{\"end\":278,\"start\":269}]", "author_first_name": "[{\"end\":118,\"start\":112},{\"end\":155,\"start\":148},{\"end\":189,\"start\":184},{\"end\":224,\"start\":217},{\"end\":265,\"start\":258},{\"end\":268,\"start\":266}]", "author_affiliation": null, "title": "[{\"end\":109,\"start\":1},{\"end\":388,\"start\":280}]", "venue": null, "abstract": "[{\"end\":1843,\"start\":390}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1903,\"start\":1899},{\"end\":1920,\"start\":1908},{\"end\":2501,\"start\":2497},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2590,\"start\":2586},{\"end\":3271,\"start\":3267},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3274,\"start\":3271},{\"end\":3277,\"start\":3274},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3279,\"start\":3277},{\"end\":3765,\"start\":3761},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3768,\"start\":3765},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4789,\"start\":4785},{\"end\":4805,\"start\":4791},{\"end\":4925,\"start\":4917},{\"end\":4940,\"start\":4936},{\"end\":6733,\"start\":6729},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6736,\"start\":6733},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7321,\"start\":7317},{\"end\":7324,\"start\":7321},{\"end\":7377,\"start\":7373},{\"end\":7380,\"start\":7377},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7382,\"start\":7380},{\"end\":7385,\"start\":7382},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7387,\"start\":7385},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7389,\"start\":7387},{\"end\":7392,\"start\":7389},{\"end\":7395,\"start\":7392},{\"end\":7429,\"start\":7425},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7590,\"start\":7586},{\"end\":7640,\"start\":7636},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7643,\"start\":7640},{\"end\":7646,\"start\":7643},{\"end\":7649,\"start\":7646},{\"end\":7652,\"start\":7649},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7790,\"start\":7787},{\"end\":7810,\"start\":7806},{\"end\":7850,\"start\":7846},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7852,\"start\":7850},{\"end\":8031,\"start\":8027},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8191,\"start\":8188},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8395,\"start\":8392},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8553,\"start\":8549},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8570,\"start\":8566},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8573,\"start\":8570},{\"end\":8576,\"start\":8573},{\"end\":8838,\"start\":8834},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9078,\"start\":9075},{\"end\":9641,\"start\":9637},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9644,\"start\":9641},{\"end\":9686,\"start\":9682},{\"end\":9689,\"start\":9686},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10159,\"start\":10155},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10202,\"start\":10199},{\"end\":10205,\"start\":10202},{\"end\":10208,\"start\":10205},{\"end\":11186,\"start\":11182},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11363,\"start\":11360},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12172,\"start\":12168},{\"end\":12243,\"start\":12239},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12974,\"start\":12973},{\"end\":13356,\"start\":13352},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13358,\"start\":13356},{\"end\":13802,\"start\":13798},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13823,\"start\":13819},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15440,\"start\":15436},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15864,\"start\":15860},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15867,\"start\":15864},{\"end\":15936,\"start\":15932},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17004,\"start\":17003},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21152,\"start\":21149},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27193,\"start\":27192},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27836,\"start\":27835},{\"end\":29739,\"start\":29732},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32218,\"start\":32214},{\"end\":32739,\"start\":32735},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32742,\"start\":32739},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":33658,\"start\":33654},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":34372,\"start\":34368},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":36883,\"start\":36882},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":40749,\"start\":40747},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":67184,\"start\":67182},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":87454,\"start\":87453},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":87777,\"start\":87776}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":37742,\"start\":37620},{\"attributes\":{\"id\":\"fig_1\"},\"end\":37854,\"start\":37743},{\"attributes\":{\"id\":\"fig_3\"},\"end\":38201,\"start\":37855},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":38851,\"start\":38202},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":39681,\"start\":38852},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":40569,\"start\":39682},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":41359,\"start\":40570},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":41954,\"start\":41360},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":42202,\"start\":41955},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":43046,\"start\":42203},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":43517,\"start\":43047},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":43869,\"start\":43518},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":48588,\"start\":43870},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":49168,\"start\":48589},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":50181,\"start\":49169},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":50261,\"start\":50182},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":52486,\"start\":50262},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":58912,\"start\":52487},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":67169,\"start\":58913},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":87346,\"start\":67170}]", "paragraph": "[{\"end\":2502,\"start\":1859},{\"end\":3154,\"start\":2504},{\"end\":4084,\"start\":3156},{\"end\":4504,\"start\":4086},{\"end\":5090,\"start\":4506},{\"end\":5569,\"start\":5092},{\"end\":6221,\"start\":5571},{\"end\":6582,\"start\":6223},{\"end\":7325,\"start\":6584},{\"end\":7811,\"start\":7342},{\"end\":9448,\"start\":7813},{\"end\":10006,\"start\":9450},{\"end\":10979,\"start\":10075},{\"end\":11012,\"start\":10989},{\"end\":11971,\"start\":11051},{\"end\":12244,\"start\":11973},{\"end\":14253,\"start\":12246},{\"end\":14645,\"start\":14255},{\"end\":14823,\"start\":14647},{\"end\":15413,\"start\":14825},{\"end\":15885,\"start\":15415},{\"end\":16353,\"start\":15887},{\"end\":17775,\"start\":16355},{\"end\":18642,\"start\":17777},{\"end\":19349,\"start\":18644},{\"end\":19823,\"start\":19351},{\"end\":20293,\"start\":19825},{\"end\":20510,\"start\":20295},{\"end\":20714,\"start\":20512},{\"end\":21050,\"start\":20763},{\"end\":21677,\"start\":21052},{\"end\":22354,\"start\":21679},{\"end\":23849,\"start\":22385},{\"end\":24382,\"start\":23851},{\"end\":24740,\"start\":24384},{\"end\":25017,\"start\":24742},{\"end\":26049,\"start\":25093},{\"end\":26295,\"start\":26051},{\"end\":26305,\"start\":26297},{\"end\":28131,\"start\":26472},{\"end\":29412,\"start\":28133},{\"end\":29910,\"start\":29414},{\"end\":30566,\"start\":29912},{\"end\":31269,\"start\":30581},{\"end\":31851,\"start\":31271},{\"end\":32635,\"start\":31853},{\"end\":32830,\"start\":32637},{\"end\":32834,\"start\":32832},{\"end\":33194,\"start\":32867},{\"end\":34319,\"start\":33224},{\"end\":34488,\"start\":34360},{\"end\":35254,\"start\":34490},{\"end\":35923,\"start\":35256},{\"end\":36135,\"start\":35925},{\"end\":37532,\"start\":36179},{\"end\":37619,\"start\":37557}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11050,\"start\":11013},{\"attributes\":{\"id\":\"formula_1\"},\"end\":26471,\"start\":26306},{\"attributes\":{\"id\":\"formula_2\"},\"end\":33223,\"start\":33195}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":10860,\"start\":10853},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":11576,\"start\":11569},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":11744,\"start\":11737},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":12392,\"start\":12385},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":13385,\"start\":13378},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":14563,\"start\":14276},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":17772,\"start\":17765},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":17903,\"start\":17896},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19493,\"start\":19486},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22464,\"start\":22457},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":23426,\"start\":23419},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23943,\"start\":23936},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":28190,\"start\":28183},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":28441,\"start\":28432},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":28494,\"start\":28487},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":35418,\"start\":35405}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1857,\"start\":1845},{\"attributes\":{\"n\":\"2\"},\"end\":7340,\"start\":7328},{\"attributes\":{\"n\":\"3\"},\"end\":10073,\"start\":10009},{\"end\":10987,\"start\":10982},{\"attributes\":{\"n\":\"4.1\"},\"end\":20761,\"start\":20717},{\"end\":22383,\"start\":22357},{\"attributes\":{\"n\":\"5\"},\"end\":25091,\"start\":25020},{\"attributes\":{\"n\":\"6\"},\"end\":30579,\"start\":30569},{\"end\":32865,\"start\":32837},{\"end\":34358,\"start\":34322},{\"end\":36177,\"start\":36138},{\"end\":37555,\"start\":37535},{\"end\":37631,\"start\":37621},{\"end\":37751,\"start\":37744},{\"end\":37869,\"start\":37856},{\"end\":38212,\"start\":38203},{\"end\":38856,\"start\":38853},{\"end\":39692,\"start\":39683},{\"end\":40580,\"start\":40571},{\"end\":41370,\"start\":41361},{\"end\":41965,\"start\":41956},{\"end\":42213,\"start\":42204},{\"end\":43057,\"start\":43048},{\"end\":43528,\"start\":43519},{\"end\":48599,\"start\":48590},{\"end\":49179,\"start\":49170},{\"end\":50192,\"start\":50183},{\"end\":50272,\"start\":50263},{\"end\":52497,\"start\":52488},{\"end\":58923,\"start\":58914},{\"end\":67180,\"start\":67171}]", "table": "[{\"end\":38851,\"start\":38280},{\"end\":39681,\"start\":39252},{\"end\":40569,\"start\":39869},{\"end\":41359,\"start\":40925},{\"end\":41954,\"start\":41539},{\"end\":42202,\"start\":41967},{\"end\":43046,\"start\":42495},{\"end\":43517,\"start\":43359},{\"end\":43869,\"start\":43639},{\"end\":48588,\"start\":44462},{\"end\":49168,\"start\":48822},{\"end\":50181,\"start\":49614},{\"end\":52486,\"start\":50329},{\"end\":58912,\"start\":53007},{\"end\":67169,\"start\":59153},{\"end\":87346,\"start\":72569}]", "figure_caption": "[{\"end\":37742,\"start\":37633},{\"end\":37854,\"start\":37753},{\"end\":38201,\"start\":37871},{\"end\":38280,\"start\":38214},{\"end\":39252,\"start\":38858},{\"end\":39869,\"start\":39694},{\"end\":40925,\"start\":40582},{\"end\":41539,\"start\":41372},{\"end\":42495,\"start\":42215},{\"end\":43359,\"start\":43059},{\"end\":43639,\"start\":43530},{\"end\":44462,\"start\":43872},{\"end\":48822,\"start\":48601},{\"end\":49614,\"start\":49181},{\"end\":50261,\"start\":50194},{\"end\":50329,\"start\":50274},{\"end\":53007,\"start\":52499},{\"end\":59153,\"start\":58925},{\"end\":72569,\"start\":67182}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6595,\"start\":6587},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9309,\"start\":9301},{\"end\":30325,\"start\":30317},{\"end\":36931,\"start\":36915}]", "bib_author_first_name": "[{\"end\":88133,\"start\":88128},{\"end\":88142,\"start\":88139},{\"end\":88152,\"start\":88150},{\"end\":88164,\"start\":88158},{\"end\":88176,\"start\":88172},{\"end\":88185,\"start\":88182},{\"end\":88196,\"start\":88193},{\"end\":88209,\"start\":88202},{\"end\":88220,\"start\":88215},{\"end\":88594,\"start\":88587},{\"end\":88613,\"start\":88607},{\"end\":88627,\"start\":88621},{\"end\":88899,\"start\":88891},{\"end\":88917,\"start\":88913},{\"end\":88931,\"start\":88924},{\"end\":88944,\"start\":88939},{\"end\":88962,\"start\":88954},{\"end\":88977,\"start\":88971},{\"end\":88994,\"start\":88988},{\"end\":89014,\"start\":89008},{\"end\":89028,\"start\":89022},{\"end\":89418,\"start\":89412},{\"end\":89448,\"start\":89441},{\"end\":89457,\"start\":89453},{\"end\":89470,\"start\":89464},{\"end\":89481,\"start\":89476},{\"end\":89494,\"start\":89490},{\"end\":89511,\"start\":89506},{\"end\":89529,\"start\":89525},{\"end\":89545,\"start\":89542},{\"end\":89944,\"start\":89941},{\"end\":89959,\"start\":89955},{\"end\":89973,\"start\":89967},{\"end\":89987,\"start\":89983},{\"end\":90330,\"start\":90327},{\"end\":90345,\"start\":90341},{\"end\":90525,\"start\":90524},{\"end\":90541,\"start\":90534},{\"end\":90543,\"start\":90542},{\"end\":90558,\"start\":90551},{\"end\":90582,\"start\":90570},{\"end\":90604,\"start\":90592},{\"end\":90935,\"start\":90929},{\"end\":90947,\"start\":90941},{\"end\":90963,\"start\":90955},{\"end\":90979,\"start\":90972},{\"end\":90991,\"start\":90987},{\"end\":91008,\"start\":91003},{\"end\":91022,\"start\":91016},{\"end\":91374,\"start\":91369},{\"end\":91387,\"start\":91384},{\"end\":91659,\"start\":91654},{\"end\":91672,\"start\":91669},{\"end\":91935,\"start\":91930},{\"end\":91952,\"start\":91945},{\"end\":91972,\"start\":91969},{\"end\":92288,\"start\":92284},{\"end\":92304,\"start\":92298},{\"end\":92314,\"start\":92310},{\"end\":92327,\"start\":92321},{\"end\":92334,\"start\":92333},{\"end\":92348,\"start\":92344},{\"end\":92620,\"start\":92616},{\"end\":92636,\"start\":92630},{\"end\":92648,\"start\":92642},{\"end\":92655,\"start\":92654},{\"end\":92669,\"start\":92665},{\"end\":92993,\"start\":92988},{\"end\":93004,\"start\":93003},{\"end\":93302,\"start\":93294},{\"end\":93316,\"start\":93311},{\"end\":93330,\"start\":93326},{\"end\":93489,\"start\":93483},{\"end\":93502,\"start\":93496},{\"end\":93514,\"start\":93508},{\"end\":93525,\"start\":93522},{\"end\":93537,\"start\":93533},{\"end\":93550,\"start\":93544},{\"end\":93559,\"start\":93555},{\"end\":93569,\"start\":93566},{\"end\":94097,\"start\":94096}]", "bib_author_last_name": "[{\"end\":88137,\"start\":88134},{\"end\":88148,\"start\":88143},{\"end\":88156,\"start\":88153},{\"end\":88170,\"start\":88165},{\"end\":88180,\"start\":88177},{\"end\":88191,\"start\":88186},{\"end\":88200,\"start\":88197},{\"end\":88213,\"start\":88210},{\"end\":88225,\"start\":88221},{\"end\":88605,\"start\":88595},{\"end\":88619,\"start\":88614},{\"end\":88639,\"start\":88628},{\"end\":88911,\"start\":88900},{\"end\":88922,\"start\":88918},{\"end\":88937,\"start\":88932},{\"end\":88952,\"start\":88945},{\"end\":88969,\"start\":88963},{\"end\":88986,\"start\":88978},{\"end\":89006,\"start\":88995},{\"end\":89020,\"start\":89015},{\"end\":89035,\"start\":89029},{\"end\":89043,\"start\":89037},{\"end\":89439,\"start\":89419},{\"end\":89451,\"start\":89449},{\"end\":89462,\"start\":89458},{\"end\":89474,\"start\":89471},{\"end\":89488,\"start\":89482},{\"end\":89504,\"start\":89495},{\"end\":89523,\"start\":89512},{\"end\":89540,\"start\":89530},{\"end\":89550,\"start\":89546},{\"end\":89559,\"start\":89552},{\"end\":89953,\"start\":89945},{\"end\":89965,\"start\":89960},{\"end\":89981,\"start\":89974},{\"end\":89999,\"start\":89988},{\"end\":90339,\"start\":90331},{\"end\":90357,\"start\":90346},{\"end\":90532,\"start\":90526},{\"end\":90549,\"start\":90544},{\"end\":90568,\"start\":90559},{\"end\":90590,\"start\":90583},{\"end\":90614,\"start\":90605},{\"end\":90621,\"start\":90616},{\"end\":90939,\"start\":90936},{\"end\":90953,\"start\":90948},{\"end\":90970,\"start\":90964},{\"end\":90985,\"start\":90980},{\"end\":91001,\"start\":90992},{\"end\":91014,\"start\":91009},{\"end\":91029,\"start\":91023},{\"end\":91382,\"start\":91375},{\"end\":91396,\"start\":91388},{\"end\":91667,\"start\":91660},{\"end\":91681,\"start\":91673},{\"end\":91943,\"start\":91936},{\"end\":91967,\"start\":91953},{\"end\":91980,\"start\":91973},{\"end\":91990,\"start\":91982},{\"end\":92296,\"start\":92289},{\"end\":92308,\"start\":92305},{\"end\":92319,\"start\":92315},{\"end\":92331,\"start\":92328},{\"end\":92342,\"start\":92335},{\"end\":92356,\"start\":92349},{\"end\":92365,\"start\":92358},{\"end\":92628,\"start\":92621},{\"end\":92640,\"start\":92637},{\"end\":92652,\"start\":92649},{\"end\":92663,\"start\":92656},{\"end\":92677,\"start\":92670},{\"end\":92686,\"start\":92679},{\"end\":92840,\"start\":92834},{\"end\":92848,\"start\":92842},{\"end\":93001,\"start\":92994},{\"end\":93010,\"start\":93005},{\"end\":93017,\"start\":93012},{\"end\":93309,\"start\":93303},{\"end\":93324,\"start\":93317},{\"end\":93335,\"start\":93331},{\"end\":93494,\"start\":93490},{\"end\":93506,\"start\":93503},{\"end\":93520,\"start\":93515},{\"end\":93531,\"start\":93526},{\"end\":93542,\"start\":93538},{\"end\":93553,\"start\":93551},{\"end\":93564,\"start\":93560},{\"end\":93573,\"start\":93570},{\"end\":93583,\"start\":93575},{\"end\":93967,\"start\":93957},{\"end\":94103,\"start\":94098},{\"end\":94158,\"start\":94147}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2012.15701\",\"id\":\"b0\"},\"end\":88502,\"start\":88128},{\"attributes\":{\"doi\":\"arXiv:2109.12948\",\"id\":\"b1\"},\"end\":88850,\"start\":88504},{\"attributes\":{\"doi\":\"arXiv:2005.14165\",\"id\":\"b2\"},\"end\":89313,\"start\":88852},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":227276514},\"end\":89939,\"start\":89315},{\"attributes\":{\"doi\":\"arXiv:2208.07339\",\"id\":\"b4\"},\"end\":90265,\"start\":89941},{\"attributes\":{\"doi\":\"arXiv:2212.09720\",\"id\":\"b5\"},\"end\":90522,\"start\":90267},{\"attributes\":{\"doi\":\"arXiv:1902.08153\",\"id\":\"b6\"},\"end\":90857,\"start\":90524},{\"attributes\":{\"doi\":\"arXiv:2004.07320\",\"id\":\"b7\"},\"end\":91275,\"start\":90859},{\"attributes\":{\"doi\":\"arXiv:2208.11580\",\"id\":\"b8\"},\"end\":91590,\"start\":91277},{\"attributes\":{\"doi\":\"arXiv:2301.00774\",\"id\":\"b9\"},\"end\":91845,\"start\":91592},{\"attributes\":{\"doi\":\"arXiv:2210.17323\",\"id\":\"b10\"},\"end\":92209,\"start\":91847},{\"attributes\":{\"doi\":\"arXiv:2103.13630\",\"id\":\"b11\"},\"end\":92594,\"start\":92211},{\"attributes\":{\"id\":\"b12\"},\"end\":92830,\"start\":92596},{\"attributes\":{\"id\":\"b13\"},\"end\":92917,\"start\":92832},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":7057040},\"end\":93246,\"start\":92919},{\"attributes\":{\"id\":\"b15\"},\"end\":93481,\"start\":93248},{\"attributes\":{\"doi\":\"arXiv:1909.10351,2019.BS=256RTN4349.14/2907.61/2510.7535.36/42\",\"id\":\"b16\"},\"end\":93953,\"start\":93483},{\"attributes\":{\"doi\":\"3446.89/2156.26/1484.15 33.13/41.23/29.51\",\"id\":\"b17\"},\"end\":94062,\"start\":93955},{\"attributes\":{\"id\":\"b18\"},\"end\":94143,\"start\":94064},{\"attributes\":{\"id\":\"b19\"},\"end\":94171,\"start\":94145}]", "bib_title": "[{\"end\":89410,\"start\":89315},{\"end\":92614,\"start\":92596},{\"end\":92986,\"start\":92919}]", "bib_author": "[{\"end\":88139,\"start\":88128},{\"end\":88150,\"start\":88139},{\"end\":88158,\"start\":88150},{\"end\":88172,\"start\":88158},{\"end\":88182,\"start\":88172},{\"end\":88193,\"start\":88182},{\"end\":88202,\"start\":88193},{\"end\":88215,\"start\":88202},{\"end\":88227,\"start\":88215},{\"end\":88607,\"start\":88587},{\"end\":88621,\"start\":88607},{\"end\":88641,\"start\":88621},{\"end\":88913,\"start\":88891},{\"end\":88924,\"start\":88913},{\"end\":88939,\"start\":88924},{\"end\":88954,\"start\":88939},{\"end\":88971,\"start\":88954},{\"end\":88988,\"start\":88971},{\"end\":89008,\"start\":88988},{\"end\":89022,\"start\":89008},{\"end\":89037,\"start\":89022},{\"end\":89045,\"start\":89037},{\"end\":89441,\"start\":89412},{\"end\":89453,\"start\":89441},{\"end\":89464,\"start\":89453},{\"end\":89476,\"start\":89464},{\"end\":89490,\"start\":89476},{\"end\":89506,\"start\":89490},{\"end\":89525,\"start\":89506},{\"end\":89542,\"start\":89525},{\"end\":89552,\"start\":89542},{\"end\":89561,\"start\":89552},{\"end\":89955,\"start\":89941},{\"end\":89967,\"start\":89955},{\"end\":89983,\"start\":89967},{\"end\":90001,\"start\":89983},{\"end\":90341,\"start\":90327},{\"end\":90359,\"start\":90341},{\"end\":90534,\"start\":90524},{\"end\":90551,\"start\":90534},{\"end\":90570,\"start\":90551},{\"end\":90592,\"start\":90570},{\"end\":90616,\"start\":90592},{\"end\":90623,\"start\":90616},{\"end\":90941,\"start\":90929},{\"end\":90955,\"start\":90941},{\"end\":90972,\"start\":90955},{\"end\":90987,\"start\":90972},{\"end\":91003,\"start\":90987},{\"end\":91016,\"start\":91003},{\"end\":91031,\"start\":91016},{\"end\":91384,\"start\":91369},{\"end\":91398,\"start\":91384},{\"end\":91669,\"start\":91654},{\"end\":91683,\"start\":91669},{\"end\":91945,\"start\":91930},{\"end\":91969,\"start\":91945},{\"end\":91982,\"start\":91969},{\"end\":91992,\"start\":91982},{\"end\":92298,\"start\":92284},{\"end\":92310,\"start\":92298},{\"end\":92321,\"start\":92310},{\"end\":92333,\"start\":92321},{\"end\":92344,\"start\":92333},{\"end\":92358,\"start\":92344},{\"end\":92367,\"start\":92358},{\"end\":92630,\"start\":92616},{\"end\":92642,\"start\":92630},{\"end\":92654,\"start\":92642},{\"end\":92665,\"start\":92654},{\"end\":92679,\"start\":92665},{\"end\":92688,\"start\":92679},{\"end\":92842,\"start\":92834},{\"end\":92850,\"start\":92842},{\"end\":93003,\"start\":92988},{\"end\":93012,\"start\":93003},{\"end\":93019,\"start\":93012},{\"end\":93311,\"start\":93294},{\"end\":93326,\"start\":93311},{\"end\":93337,\"start\":93326},{\"end\":93496,\"start\":93483},{\"end\":93508,\"start\":93496},{\"end\":93522,\"start\":93508},{\"end\":93533,\"start\":93522},{\"end\":93544,\"start\":93533},{\"end\":93555,\"start\":93544},{\"end\":93566,\"start\":93555},{\"end\":93575,\"start\":93566},{\"end\":93585,\"start\":93575},{\"end\":93969,\"start\":93957},{\"end\":94105,\"start\":94096},{\"end\":94160,\"start\":94147}]", "bib_venue": "[{\"end\":88293,\"start\":88243},{\"end\":88585,\"start\":88504},{\"end\":88889,\"start\":88852},{\"end\":89610,\"start\":89561},{\"end\":90070,\"start\":90017},{\"end\":90325,\"start\":90267},{\"end\":90669,\"start\":90639},{\"end\":90927,\"start\":90859},{\"end\":91367,\"start\":91277},{\"end\":91652,\"start\":91592},{\"end\":91928,\"start\":91847},{\"end\":92282,\"start\":92211},{\"end\":92707,\"start\":92688},{\"end\":93068,\"start\":93019},{\"end\":93292,\"start\":93248},{\"end\":93697,\"start\":93647},{\"end\":94094,\"start\":94064}]"}}}, "year": 2023, "month": 12, "day": 17}