{"id": 207240186, "updated": "2023-07-18 20:58:17.386", "metadata": {"title": "Deep-based Ingredient Recognition for Cooking Recipe Retrieval", "authors": "[{\"first\":\"Jingjing\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Chong-wah\",\"last\":\"Ngo\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 24th ACM international conference on Multimedia", "publication_date": {"year": 2016, "month": null, "day": null}, "abstract": "Retrieving recipes corresponding to given dish pictures facilitates the estimation of nutrition facts, which is crucial to various health relevant applications. The current approaches mostly focus on recognition of food category based on global dish appearance without explicit analysis of ingredient composition. Such approaches are incapable for retrieval of recipes with unknown food categories, a problem referred to as zero-shot retrieval. On the other hand, content-based retrieval without knowledge of food categories is also difficult to attain satisfactory performance due to large visual variations in food appearance and ingredient composition. As the number of ingredients is far less than food categories, understanding ingredients underlying dishes in principle is more scalable than recognizing every food category and thus is suitable for zero-shot retrieval. Nevertheless, ingredient recognition is a task far harder than food categorization, and this seriously challenges the feasibility of relying on them for retrieval. This paper proposes deep architectures for simultaneous learning of ingredient recognition and food categorization, by exploiting the mutual but also fuzzy relationship between them. The learnt deep features and semantic labels of ingredients are then innovatively applied for zero-shot retrieval of recipes. By experimenting on a large Chinese food dataset with images of highly complex dish appearance, this paper demonstrates the feasibility of ingredient recognition and sheds light on this zero-shot problem peculiar to cooking recipe retrieval.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2526198870", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/mm/ChenN16", "doi": "10.1145/2964284.2964315"}}, "content": {"source": {"pdf_hash": "23fd82c04852b74d655015ff0876e6c5defc6e61", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBYNCND", "open_access_url": "https://ink.library.smu.edu.sg/sis_research/6498", "status": "GREEN"}}, "grobid": {"id": "3a2fae50f80319298ac14f06a67d01919f2c28e9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/23fd82c04852b74d655015ff0876e6c5defc6e61.txt", "contents": "\nDeep-based Ingredient Recognition for Cooking Recipe Retrieval\n\n\nJingjing Chen jingjchen9-c@my.city.edu.hk \nCity University of HongKong Kowloon\nHongKong\n\nChong-Wah Ngo \nCity University of HongKong Kowloon\nHongKong\n\nDeep-based Ingredient Recognition for Cooking Recipe Retrieval\n10.1145/2964284.2964315Food categorizationingredient recognitionzero-shot retrievalmulti- task deep learning\nRetrieving recipes corresponding to given dish pictures facilitates the estimation of nutrition facts, which is crucial to various health relevant applications. The current approaches mostly focus on recognition of food category based on global dish appearance without explicit analysis of ingredient composition. Such approaches are incapable for retrieval of recipes with unknown food categories, a problem referred to as zero-shot retrieval. On the other hand, content-based retrieval without knowledge of food categories is also difficult to attain satisfactory performance due to large visual variations in food appearance and ingredient composition. As the number of ingredients is far less than food categories, understanding ingredients underlying dishes in principle is more scalable than recognizing every food category and thus is suitable for zero-shot retrieval. Nevertheless, ingredient recognition is a task far harder than food categorization, and this seriously challenges the feasibility of relying on them for retrieval. This paper proposes deep architectures for simultaneous learning of ingredient recognition and food categorization, by exploiting the mutual but also fuzzy relationship between them. The learnt deep features and semantic labels of ingredients are then innovatively applied for zero-shot retrieval of recipes. By experimenting on a large Chinese food dataset with images of highly complex dish appearance, this paper demonstrates the feasibility of ingredient recognition and sheds light on this zero-shot problem peculiar to cooking recipe retrieval.\n\nINTRODUCTION\n\nWhile there is a large number of cooking recipes posted on the Internet, finding a right recipe given a picture of dish remains a challenge yet to be fully explored. The major problem underlying this challenge is the recognition of food categories as well as their ingredients. Indeed, the problem is commonly shared among health-related applications. For example, food-log management [1], which records daily food intake for dietary habit monitoring, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Figure 1: Variations in visual appearance and composition of ingredients show the challenges of predicting ingredients even for dishes within the same food category. The first row shows three examples of dishes for the category \"fried green peppers\", followed by \"yuba salad\" ad \"steam egg custard\" in second and third rows respectively. often requires manual input of food intake. In addition to timeconsuming, the process is error-prone. As investigated in [11], selfreporting data obtained from unfriendly acquired process tends to underestimate the actual food intake. These concerns motivate the use of mobile devices as a convenient means in capturing pictures of food intake for automatic recognition [24] [14] [25] [3] [16].\n\nThis paper studies the recognition of ingredients for recipe retrieval in the domain of Chinese dishes. Different from food categorization, which is to identify the name of a dish (e.g., fried green pepper shown in Figure 1), ingredient recognition is to uncover the ingredients inside a dish (e.g., green pepper, black bean, chopped garlic). In the literature, associating food categories to their respective recipes is regarded as a general pipeline that facilitates the estimation of calories and nutrition facts [35] [14]. The pipeline is effective for recognizing restaurant dishes and the food categories with standardized cooking method (e.g., fast food) that often have similar visual appearance with the same ingredients. However, most dishes in Chinese food have no standardized cooking method, food presentation and ingredient composition. Direct mapping between dishes and recipes, by using the names of food categories, is not likely to attain satisfactory retrieval rate, not mentioning the imperfect performance in food recognition. The difficulty of this task is probably alleviated, nevertheless, with the presence of GPS and restaurant menus as utilized by Im2Calories [24] and Menu-Match [2]. However, restaurant information are difficult to acquire as stated in [24] and such context-aware recognition is only limited to restaurant food. Therefore, this paper argues the need of ingredient recognition beyond food categorization for general recipe retrieval.\n\nIn the domain of Chinese food, two major obstacles in recognition are diverse appearances of dishes and wild composition of ingredients. Figure 1 shows some examples of Chinese dishes. Automatic recognition is challenged by the wildly different ways of mixing and placing ingredients even for the same food category. For the food category \"steamed egg custard\" (last row of Figure  1), there is even no overlap in ingredients except egg. Retrieving recipes without explicitly naming the underneath ingredients is expected to include false positives. Basically, ingredients can be treated as attributes of food categories. As the number of food categories is generally far larger than the number of ingredients, recognizing attributes is more feasible than food categories in terms of scale. Furthermore, ingredient recognition also gives light to the retrieval of recipes for unknown food categories during model training, a problem generally referred to as zero-shot recognition or retrieval [29].\n\nGenerally speaking, ingredient recognition is more difficult than food categorization. As observed in Figure 1, the size, shape and color of ingredients can exhibit large visual differences due to diverse ways of cutting and cooking, in addition to changes in viewpoints and lighting conditions. Recognizing ingredients alone without food category in mind is likely to result in unsatisfactory performance. This paper considers simultaneous recognition of food and ingredients, aiming to exploit the mutual relationship between them for enhancing the robustness of recognition. The key ingredients of a category remain similar despite composing with different auxiliary ingredients. Knowing food category basically eases the recognition of ingredients. On the other hand, the prediction of ingredients also helps food categorization, for example, the ingredient \"fungus\" has a higher chance than \"pork\" to appear in the food \"yuba salad\". Hence, learning food categories with the composition of ingredients in mind, and vice versa, in principle shall lead to better performance. Figure 2 gives an overview of the proposed framework, which is composed of two modules: ingredient recognition and zero-shot recipe retrieval. The first module formulates the recognition of ingredients as a problem of multi-task learning using deep convolution neural network (DCNN). Given a picture of dish, the module outputs the name of dish along with a histogram of ingredients. The developed DCNN can recognize 172 Chinese food categories and 353 ingredients. To the best of our knowledge, there is no result published yet for ingredient recognition on such a large scale. The second module performs zero-shot retrieval, by matching the predicted ingredients against a large corpus containing more than 60,000 recipes. The corpus includes some food categories as well as ingredients unknown to the multi-task DCNN. To boost retrieval performance, a graph encoding the contextual relationship among ingredients is learnt from the recipe corpus. Using this graph, conditional random field (CRF) is employed to probabilistically tune the probability distribution of ingredients to reduce potential recognition error due to unseen food category.\n\nTo summary, this paper contributes by developing multi-task learning technique for ingredient recognition and demonstrates its application for zero-shot recipe retrieval. Our work differs from the existing works, which mostly focus on recognition of food categories and operate in domains such as western and Japanese food [4] [22]. To our knowledge, zero-shot recipe retrieval, which requires knowledge of ingredients, has not yet been considered in the literature. Along with this paper, we will release the collected Chinese food dataset, VIREO Food-172, which contains 172 food and 353 ingredient labels. The dataset is larger than the publicly available datasets such as Food-101 [4], UEC Food-100 [22] and PFID [6], each with around 100 western or Japanese food categories but without ingredient labels.\n\n\nRELATED WORK\n\nVariants of recognition-centric approaches have been investigated for different food-related applications. These efforts include food quantity estimation based on depth images [7], image segmentation for volume estimation [25], context-based recognition by GPS and restaurant menus [3], taste estimation [23], multi-food recognition [22], multi-modal fusion [13] and real-time recognition [14]. This section mainly reviews previous works on recognition of food and ingredients using deep and hand-crafted features.\n\nThe challenge of food recognition comes from visual variations in shape, color and texture layout. These variations are hard to be tackled by hand-crafted features such as SIFT [21], HOG [8] and color [30]. Instead, deep features extracted from DCNN [17], which is trained on ImageNet [9] and fine-tuned on food images, often exhibit impressive recognition performance [24] [36] [34]. Combination of multi-modal features sometimes also leads to better recognition performance, as reported in [15] [34]. One of best the performances on UEC Food-100 dataset is achieved by fusion of DCNN features with RootHOG and color moment [15], and similarly for UPMC Food-101 dataset by fusion of textual and deep features [34]. Different from these works which directly adopt DCNN for food recognition, this paper proposes new architectures based upon DCNN for simultaneous recognition of food categories and ingredients.\n\nCompared to food categorization, recognition of ingredients receives fewer attentions. One early model is PFD (pairwise local feature distribution) [37], which leverages the result of ingredient recognition for food categorization. In PFD, based upon the appearance of image patches, pixels are softly labeled with ingredient categories. The spatial relationship between pixels is then modeled as a multi-dimensional histogram, characterized by label cooccurrence and their geometric properties such as distance and orientation. With this histogram representation, PFD shows impressive food recognition performance. PFD, nevertheless, is hardly scalable to the number of ingredients. Using only eight categories of ingredients as demonstrated in [37], the histogram already grows up to tens of thousands of dimensions, not mentioning 353 categories as in our paper where the number of dimensions could be as high as ten millions. Our work is more scalable and based on multi-task learning, in contrast to the two-step recognition in PFD without the feedback loop.\n\nFew recent works explore spatial layout [12], feature mining [4] and image segmentation [24] for ingredient or food item recognition. In [12], ingredient regions are detected by shape and texture models, where the shape is based on DPM (deformable part-based model) while the texture is based on STF (semantic texton forest). Similar to PFD [37], the regions are encoded into a histogram modeling spatial relationship between them for food recognition. The spatial relationship is not statistically encoded as in [37], but rather explicit relationships such as \"above\", \"below\" and \"overlapping\" are modeled. Such relationships are helpful for recognizing food such as dessert and fast food, but difficult to be generalized such as for Chinese dishes. In [4], an interesting work which mines the composition of ingredients as discriminative patterns is proposed for food classification. A drawback of this approach is the requirement of image segmentation, which is sensitive to parameter setting ... and can impact recognition performance. As reported in [4], the performance is not better than of DCNN without image segmentation on Food-101 dataset. Similar to [25], image segmentation is employed in [24], but using a more advanced technique based on conditional random field (CRF) with unary potentials provided by DCNN [5]. The promising performance in segmentation for western food, nevertheless, comes from the price for requiring training labels that need manual segmentation of food items for model learning. For Chinese food, collecting such training labels is extremely difficult, given the fuzzy composition and placement of ingredients as shown in Figure 1.\n\n\nMulti-task DCNN\n\nOur work is also related to multi-task learning [10] [31] [32] and fine-grained classification [18] [20] [19]. In [10], multi-task DCNN models are proposed for simultaneous categorization and pose estimation of general objects (e.g., airplane, sofa, car). In [31], a deep network is designed for the tasks of face classification and verification. As concluded by [31], adding one more supervisory signals for feature learning greatly improves the performance of face verification. Similarly for cascade network [18] [20] and bilinear model [19] which couple multiple deep models for finegrained classification. Nevertheless, these models mostly focus on localization, alignment and classification of object parts, which are not directly applicable to our problem. As ingredients can scatter around a dish and occlude each other (e.g., \"yuba salad\" in Figure  1), localization and alignment of ingredients are hardly applied for food domain. To the best of our knowledge, there is no multi-task learning model yet developed for food recognition.\n\n\nMULTI-TASK DEEP LEARNING\n\nThe conventional DCNN is an end-to-end system with input as picture and output as the prediction scores of class labels. DCNN models such as AlexNet [17] and VGG [28] are trained under the single-label scenario, specifically, there is an assumption of exactly one label for each input picture. As ingredient recognition is a multi-label problem, i.e., more than one labels per image, a different loss function needs to be used for training DCNN. On the other hand, directly revising DCNN with appropriate loss function for ingredient recognition may not yield satisfactory performance, given the varying appearances of an ingredient in different dishes. To this end, we propose to couple food categorization problem, which is a single-label problem, together with ingredient recognition for simultaneous learning.\n\n\nArchitecture Design\n\nWe formulate food categorization and ingredient recognition as a multi-task deep learning problem and modify the architecture of DCNN for our purpose. The modification is not straightforward for involvement of two design issues. The first issue is about whether the prediction scores of both tasks should directly or indirectly influence each other. Direct influence means that the input of one task is connected as the output of another task. Indirect influence decouples the connection such that each task is on a different path of the network. Both tasks influence each other through updating the shared intermediate layers. The second issue is about the degree in which the intermediate layers should be shared. Ideally, each task should have its own private layer(s) given that the nature of both tasks, single versus multi-labeling, is different. In such a way, the updating of parameters can be done more freely for optimization of individual performance.\n\nBased on the two design issues, we derive four different deep architectures as depicted in Figure 3, respectively name as Arch-A to Arch-D. The first design (Arch-A) considers stacked architecture by placing food categorization on top of ingredient recognition, and vice versa. As the composition of ingredients for different dishes under the same food category can be different, this architecture has the risk that model learning converges slowly as observed in the experiment. The second design (Arch-B) is similar except that indirect influence is adopted and both tasks are at different pathways. Both designs are relatively straightforward to implement by adding additional layers to DCNN. The next two architectures consider the decoupling of some intermediate layers. The third design (Arch-C) allows each task to privately own two intermediate layers on top of the convolutional layers for parameter learning. The last design\n\n\nConvolutional layers Convolutional layers Convolutional layers\nArch-D Arch-C Arch-B \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026\n\nConvolutional layers\n\nArch (Arch-D) is a compromise version between the second and third architectures, by having one shared and one private layer. Arch-D has the peculiarity that the shared layer can correspond to the high or mid-level features common between two tasks at the early stage of learning, while the private layer preserves the learning of specialized features useful for optimizing the performance of each task.\n\n\nImplementation\n\nThe architectures are modified from VGG 16-layers network [28]. In terms of design, the major modification is made on the fully connected layers. For the private layers in Arch-D, there are 4,096 neurons for food categorization, and 1024 neurons for ingredient layers. Due to different natures of the tasks, we adopt multinomial logistic loss function L1 for single-label food categorization, and cross-entropy as the loss function L2 for multi-label ingredient recognition. Denote N as the total number of training images, the overall loss function L is as following:\nL = \u2212 1 N N n=1 (L1 + \u03bbL2)(1)\nwhere \u03bb is a parameter trading off the loss terms. This loss function is also widely used in other works such as [31]. During training, the errors propagated from two branches are linearly combined and the weights of first 11 layers shared between two tasks will be updated accordingly. The updating will subsequently affect the last two layers simultaneously, adjustify the features separately owned by food and ingredient recognition. Letqn,y as the predicted score of an image xn for its ground-truth food label y, L1 is defined as following:\nL1 = log(qn,y)(2)\nwhereqn,y is obtained from softmax activation function. Furthermore, denote pn \u2208 {0, 1} I , represented as a vector in I dimensions, as the ground-truth ingredients for an image xn. Basically pn is a binary vector with entries of value 1 or 0 indicating the presence or absence of an ingredient. The loss function L2 is defined as\nL2 = I c=1 pn,clog(pn,c) + (1 \u2212 pn,c)log(1 \u2212pn,c)(3)\nwherepn,c denotes the probability of having ingredient category c for xn, obtained through sigmoid activation function.\n\n\nZERO-SHOT RETRIEVAL\n\nTraining a deep network for recognizing all available food categories is not feasible. In addition to the reality that there exist more than tens of thousands of categories, collecting training examples for each of the categories can be a daunting task. Hence, a practical problem is how to leverage the limited knowledge learnt in a network for recognizing dishes of previously unseen category. As the proposed architectures are capable of predicted ingredients, in principle the problem can be addressed by retrieving recipes through matching of ingredients. We refer this problem to as zero-shot retrieval, which is to find recipes for test pictures of unseen food categories. Two scenarios are considered here. Suppose each recipe is associated with a picture of the dish. The first scenario is to use the FC7 features, specifically the features extracted from the private layer(s) of Arch-C or Arch-D, to represent images for retrieval. In other words, the search of recipe is equivalent to image retrieval. The second scenario assumes absence of pictures in recipes, and uses the predicted scores of ingredients as the semantic labels for text-based retrieval of recipes. As the approach for the first scenario can be straightforwardly implemented, this section focuses on the presentation of the second scenario. The idea is to incorporate external knowledge to refine the predicted ingredient scores for more realistic way of zero-shot retrieval.\n\n\nIngredient Refinement with CRF\n\nWhile the composition of ingredients is fuzzy in Chinese food, the mixing is not purely random. Intuitively, certain groups of ingredients co-occur more often (e.g., corn and carrot), while some ingredients are likely exclusive of each other (e.g., fish and beef). Such statistics can be mined from training data and utilized for adjusting the predicting scores of ingredients. Nevertheless, considering the zero-shot problem and potentially the limited knowledge in deep network, we mine the statistics from a large corpus composed of more than 60,000 Chinese cooking recipes. The major advantage of doing so is to learn a graph modeling ingredient relationships, where their correlations are more generalizable and not restricted by training data, and hence enhance the success rate of zero-shot retrieval.\n\nWe extract ingredients from recipes and construct a graph modeling their co-occurrences based on conditional random field (CRF). Denote N = {c1, ..., cI } as the set of available ingredients and I as its set cardinality. The graph G is composed of the elements of N as vertices and their pairwise relationships, denoted as \u03c6i(\u00b7), as edges. Further let li as an indication function that signals the presence or absence of an ingredient ci. The joint probability of ingredients given the graph is\np(l1, ...lI ) = 1 Z(\u03c6) exp( i,j\u2208N lilj\u03c6(i, j))(4)\nwhere Z(\u00b7) is a partitioning function. To learn the graph, we employ Monte Carlo integration to approximate Z(\u00b7) and the gradient descent to estimate \u03c6(\u00b7) to optimize the data likelihood [26]. Given a test image, CRF inferences a label sequence y based on the graph G. The energy function for inference is composed of unary and pairwise potentials, defined as\nE(y) = c\u2208N \u03c8u(yc) + (c,v)\u2208\u03b5 \u03c8p(yc, yv)(5)\nwhere \u03b5 denotes the set of pairwise cliques. The unary term is set as \u03c8u(yc) = \u2212log(xc), where xc is the predicted score by the deep network for ingredient c. The pairwise potential is defined as\n\u03c8(yu, yv) = 0 if yu = yv \u03c6(yu, yv) if yc = yv(6)\nwhere the value of \u03c6(\u00b7) is obtained from the graph G. Through inferencing, CRF searchs for the optimal label sequence of y that agrees with the predicted scores and the contextual relationship captured in the graph G. We employ off-the-shelf algorithm, loopy belief propagation [33], for minimize Eqn-5. The output label sequence y will indicate the presences or absences of ingredients and their probabilities.\n\n\nRecipe Search\n\nWith the output sequence y by CRF, a query image is represented as a vector Q i . Every element in Q i corresponds to an ingredient and its value indicates the probability output by CRF. On the other hand, the ingredients extracted from a recipe is represented as a binary vector O. The matching score, si, between them is defined as\nsi = c\u2208O\u2229c\u2208Q i xc(7)\nNote that the score is not normalized in order not to bias recipes with a small number of ingredients. As a result, Eqn-7 tends to give a higher score for the recipes with excessive number of ingredients. To prevent such cases, the matching between Q i and O is performed only for the top-k predicted ingredients with higher probability scores. The value of k is empirically set to 10 as there are few recipes with more than 10 ingredients in our dataset.\n\n\nDATASET COLLECTION\n\nWe construct a large food dataset specifically for Chinese dishes, namely VIREO Food-172 1 , which is made publicly available. Different from other publicly available datasets [4] [22] [6], both food category and ingredient labels are included. In addition, a large corpus of recipes along with dish pictures is also collected.\n\n\nVIREO Food-172\n\nThe food categories were compiled from \"Go Cooking\" 2 and \"Meishi\" 3 , which are two websites for popular Chinese dishes. We combine the categories from both websites by removing duplication. All the images in the dataset were crawled from Baidu and Google image search. For each category, the name was issued as keywords in Chinese to search engines. Categories with no more than 100 images returned were removed from the list. For the remaining categories, we manually checked each crawled images up to the depth of 1,300, for excluding images with the resolution lower than 256 \u00d7 256 or suffer from blurring, images with more than one dishes, and false positives. This process ends up with 172 food categories in the dataset.\n\nThe 172 categories cover eight major groups of food, as shown in Figure 5. The group meat contains the most number of categories, with examples include \"braised pork\" and \"saut\u00e9ed shredded pork in sweet bean sauce\". On the other hand, there are only eight categories under the group bean product, and examples include \"Mapo tofu\" and \"braised tofu\". All the images in the dataset were crawled from Baidu and Google image search. The names of food categories, were issued as keywords in Chinese to search engines, and 1,300 images are crawled per food category. Figure 4 shows some examples of food categories in VIREO Food-172.\n\n\nIngredient labeling\n\nWe compiled a list of more than 300 ingredients based on the recipes of 172 food categories. The ingredients range from popular items such as \"shredded pork\" and \"shredded pepper\" to rare items such as \"codonopsis pilosula\" and \"radix astragali\". Labeling over hundreds of ingredients for over hundred thousands of images could be extremely tedious, not mentioning the challenge of ingredient annotation. First, some ingredients are difficult to be recognized, for example, ingredients under soup or sauce. Second, some ingredients are invisible in flour-made food categories such as dumpling and noodle. Third, certain ingredients such as egg exhibit large visual variations (see Figure 6) due to different ways of cutting and cooking. Hence, the labeling considers only the annotation of visible ingredients. In addition, we create additional labels for ingredients with large visual appearance, for example, we have 13 different labels for \"egg\", such as \"preserved egg slices\" and \"boiled egg\".\n\nWe recruited 10 homemakers who have cooking experience for ingredient labeling. The homemakers were instructed to label only visible and recognizable ingredients. They were also allowed to annotate new ingredients not in the list, which would be explicitly checked by us. To guarantee the accuracy of labeling, we purposely awarded homemakers with cash bonus as incentives to provide quality annotation, in addition to regular payment. For this purpose, we checked a small subset of labels and provided immediate feedback to homemakers such that they were aware of their performance. The whole labeling process ended in two weeks. By excluding images with no ingredient labels, VIREO Food-172 contains a total of 353 ingredient labels and 110,241 images, with the average of 3 ingredients per image. Figure 7 shows the distribution of positive examples in food and ingredient categories. On average, there are 640 positive samples per food category, and 745 per ingredient.\n\n\nRecipe Corpus\n\nThe corpus was compiled from a popular website \"Xinshipu\" 4 . The website offers ontology for 530 key ingredients in Chinese food. Using all of these ingredients as queries, a total of 65,284 Chinese cooking recipes were crawled from this website. Each recipe basically contains four sections, including brief introduction, ingredient list, cooking procedure, and a picture showing the appearance of the dish. The recipes were uploaded by Internet users, and thus there may be multiple recipes sharing the same name but with different ingredient lists. Conversely, there are also few recipes about the same dish but in different names.\n\n\nEXPERIMENTS\n\nWe split the experiments into three parts, verifying the performances of multi-task learning (Section 6.1), the impact of CRF (Section 6.2) and the application for zero-shot retrieval (Section 6.3). The first part aims to evaluate different deep architectures for multi-task learning in comparison to single-task DCNN. The last   part aims to demonstrate the merit of leveraging ingredient labels for novel recipe retrieval.\n\n\nDeep Architectures\n\nThe experiments are conducted mainly on VIREO Food-172 dataset. In each food category, 60% of images are randomly picked for training, while 10% for validation and the remaining 30% of images for testing. For performance evaluation, the average top-1 and top-5 accuracies are adopted for food categorization, which are standard measures for the single-label task. For ingredient recognition which belongs to multi-label, micro-F1 and macro-F1 that take into account both precision and recall for each ingredient are employed.  The evaluation compares baseline, single and multi-task learnings. The baseline includes SVM classifiers trained using handcrafted (Gist [27] and color moment [30]) and deep (FC7 of DCNN [17]) features. The single-task learning includes the AlexNet and VGG networks fine-tuned on training and validation sets. Note that for baseline and single-task, different classifiers and networks need to be trained separately for food categorization and ingredient recognition. Specifically, multi-label SVM (MSVM) is trained for baseline, and cross entropy loss function (Eqn-3) is used for single-task DNN. The multi-task learning includes the four deep architectures illustrated in Figure 3. Note that we experiment two variants of Arch-A, with the layer of food categorization on top of ingredient recognition (Arch-A1) and vice versa (Arch-A2).\n\nGrid search of parameters is performed to find the best possible model settings for all the compared approaches, based on the training and verification sets. As ingredient recognition involves multiple labels, a threshold is required to gate the selection of labels. The threshold is set to be the value of 0.5 following the standard setting when sigmoid is used as the activation function. For multi-task deep architectures, the learning rate is set to 0.001 and the batch size to 50. The learning rate decays after every 8,000 iterations. Using Arch-D as example, Figure 8 Table 1 lists the performance for food categorization. The general trend is that deep architectures significantly outperform baselines with either deep or hand-crafted features, while large performance gap is also observed between the results of VGG network and AlexNet. Among the deep architectures for multi-task learning, the designs based on simple modification of DCNN, i.e., Arch-A and Arch-B, show slightly worse performance in Top-1 accuracy compared with single-task VGG. Since the recognition results for both food and ingredients are imperfect, layer stacking as in Arch-A actually could hurt each other's performance. Specifically, the inaccurate prediction in one task will directly affect the other task. On the other hand, while having separate paths as in Arch-B leads to better performance, the improvement is rather minor by the fact that both tasks share the same lower layers. Basically the performances of Arch-C and Arch-D show the merit of having separate paths and layers for both tasks. Arch-C, which only shares convolution layers, improves slightly over single-task VGG. We speculate that the design of Arch-C eventually trains two independent leaners and hence the advantage over single-task is not obvious. Arch-D, which shares one layer while also learning separate layers tailormade for different tasks, attains the best performance among all the compared approach for both average Top-1 and Top-5 accuracies. Table 2 shows the performance of ingredient recognition, and similar trends are observed as food categorization. For multi-task learning, all deep architectures but except Arch-A outperform singletask VGG, and with larger performance gaps compared with food categorization. The result basically verifies the merit of joint learning for both tasks. Different from food categorization, sharing layers appears to be a better design choice for ingredient recognition when comparing Arch-B and Arch-C. The best result is attained by Arch-D, which could be viewed as a compromised design between Arch-B and Arch-C. To verify that the improvement is not by chance, we conduct significance test to compare multi-task (Arch-D) and single-task (VGG) using the source code provided by TRECVID 5 . The test is performed by partial randomization with 100,000 numbers of iterations, with the null hypothesis that the improvement is due to chance. At a significance level of 0.05, Arch-D is significantly different from VGG in both food categorization and ingredient recognition by Top-1 accuracy and Macro-F1, respectively. The p-values are close to 0, which reject the null hypothesis.\n\nTo validate the proposed work on other food domain, we also conduct experiments on UEC Food-100 [22] dataset for Japanese dishes. The dataset contains 100 categories of food and totally 12,564 images. Each category has at least 100 positive examples. Nevertheless, ingredient labels are not provided. Similar to VIREO  Food-172, we compiled a list of 190 ingredients for Japanese food and conducted manual labeling. A total of 1,997 images are excluded from experiments for no ingredient labels. The experiment is conducted based on 5-fold cross-validation, using the same data split and settings as [36]. In [36], DCNN based on AlexNet is first pre-trained with 2,000 categories in ImageNet, including 1,000 food-related categories. The network is then fine-tuned with training examples in the dataset. Table 3 lists the detailed performance. Note that, although not using 1,000 food categories for pre-training, Arch-D still manages to outperform [36] by 3.5% in terms of average top-1 accuracy for food categorization. Overall, similar to the performance on VIREO Food-172, Arch-D attains the best performances for both tasks.  Table 3: Performance comparision on UEC Food-100 dataset.\n\n\nEffect of CRF\n\nThis section verifies the use of CRF in refining the predicted ingredients. All the 65,284 recipes are used for the construction of CRF. A special note is that most recipes do not include the finegrained description of ingredients. For example, a recipe will simply list \"egg\" as ingredient, instead of explicitly stating whether the ingredient as either \"sliced egg\" or \"boiled egg\". Rather such information can only be inferred from cooking procedure, such as \"leaving the eggs boil for 4 minutes\". In this experiment, we do not perform natural language processing to obtain the fine-grained description of ingredients. As a consequence, some labels in VIREO Food-172 are merged and this ends up to 257 ingredient labels for experimentation. For the deep architectures, max pooling is adopted to merge the results of fine-grained ingredients. Specifically, if a network predicts \"boiled egg\" with the probability of 0.5 and \"sliced egg\" with 0.1, the probability for \"egg\" is set to be 0.5 in the CRF.  Table 4: Ingredient recognition with contextual modeling using CRF.\n\nIn addition to assessing the effect of CRF for single and multitask learnings, we also compare the results against the baseline that directly infers the ingredients from a retrieved recipe. More specifically, given a predicted food category by VGG network, the corresponding recipe is retrieved based on name matching. The predicted labels are then based on the ingredients listed in the recipes. This strategy is often used by some approaches [14] for estimation of nutrition facts. We compare to two baselines, based on the predicted names of food categories or ingredients. Note that as a few recipes have the same name despite using different ingredients, and hence multiple recipes could be retrieved. In this case, we only show the result for the recipe which obtains the highest F1 score. Table 4 lists the performance of different approaches. Note that the performance of multi-task is based on Arch-D. Basically, CRF improves the performance of both single and multi-task learnings. All variants of baseline perform poorly in this experiment, far lower than directly using the ingredients predicted by deep architectures. The result is not surprising due to the fact that, for Chinese food, the composition of ingredients for dishes under the same category can vary depending on factors such as geographical regions, weather and culture.\n\nA few ingredients record large improvement as shown in Figure 9. The examples include \"black rice\" (F1 score = 0.1 to 0.33), \"sweet potatoes\" (F1 score = 0 to 0.14), and \"cherry tomato\" (F1 score = 0.13 to 0.2). CRF successfully captures the knowledge that \"black rice\" often co-occurs with \"rice\" and \"soybeans\" for food categories involving \"cereal porridge\". Similarly for the cooccurrence among \"cherry tomato\", \"corn\" and \"lettuce\" for food categories related to \"vegetable salad\". Figure 10 shows a few of success and fail examples refined by CRF. While CRF successfully improves the F1 score and particularly the precision of detection, the recall for few labels is also dropped as noticed in Figure 10(e) and Figure 10(f). Overall, the average precision (micro) is boosted from 0.795 to 0.833, with 193 out of 257 ingredients show improvement. The average recall (micro) is also boosted from 0.607 to 0.623, with 91 ingredients show improvement and 88 ingredients dropped.\n\n\nZero-shot Recipe Retrieval\n\nThis section assesses the use of predicted ingredients for retrieving recipes for food categories unknown in VIREO Food-172 dataset. We compile a list of 20 food categories as shown in Table 6 for the experiment. Each category is associated with 1 to 20 recipes. For each category, we make sure that at least its key ingredients are known to VIREO Food-172. On average, there are 3 key ingredients per category. Among the 20 categories, 4 out of them include ingredients that are not seen in VIREO Food-172. For each category, a total of 50 images are crawled from Baidu for testing. The experiment is conducted by, given a test image, the system searches against 65,284 recipes in the corpus and returns top-10 recipes. The performance is measured by top-10 hit rate, which counts the percentage of test images where the ground-truth recipes is found in the top-10 rank list.\n\nWe compare three major groups of approaches: image retrieval, ingredient matching (Eqn-7) and their combination. For image retrieval, only the pictures associated with recipes are involved. We compare the effectiveness of different features for retrieval. For VGG, FC7 feature is extracted from the model trained for ingredient recognition. Similarly for Arch-D, where the deep feature is extracted from the private layer specialized for ingredient labels. For ingredient matching, we compare the performances of single (VGG) and multi-task (Arch-D) learnings, where the ingredient prediction scores are both adjusted by CRF. Finally, late fusion is performed for Arch-D and VGG by combining the scores obtained from image retrieval and ingredient matching. Min-max normalization is employed to convert the scores into the range of [0,1]. The fusion is based on joint probability, specifically 1 \u2212 (1 \u2212 pi) \u00d7 (1 \u2212 pj), where pi and pj are scores from different approaches.\n\n\nMethod\n\nTop-10 hit rate  Table 5: Performance of zero-shot recipe retrieval. Table 5 lists the performance of different approaches. For image retrieval, deep features perform significantly better than handcrafted features. Our proposed model Arch-D outperforms VGG, showing the superiority of multi-task learning not only in recognition but also feature learning. For text-based ingredient matching, Arch-D also shows better performance than VGG, attributed mainly to the lower recognition error made in ingredient predic-  Figure 10: Example of test images showing effect of CRF in refining ingredient labels. The \"-\" sign means the false positives that are successfully excluded after CRF, while the \"+\" sign means the false negatives that are recalled by CRF. The \"!\" sign indicates true positives that are erroneously removed by CRF.\n\ntion, especially after CRF refinement. Further fusion of both results from Arch-D achieves the overall best performance among all the compared approaches.  Table 6: Recipe retrieval performance on 20 unknown food category. The parenthesis indicates the number of recipes for a category. The categories containing unseen ingredients in VIREO Food-172 are indicated by \"*\". Table 6 shows the detailed performance of Arch-D on 20 unknown food categories. The performance of image retrieval is influenced by the quality of pictures associated with recipes, particularly for the pictures in low resolution, having different appearances or lighting conditions than the queries. Such examples include \"mustard pork noodle\" and \"tomato & egg noodles\". On the other hand, solely matching ingredient lists is limited by the fact that the same set of ingredients can be used for different food categories. One such example is \"cucumber & fungus with eggs\", where the ingredients are also found in several other food categories, despite different visual appearance due to different ways of cooking and cutting. Image retrieval using the deep features, which are trained to deal with these visual variations, generally shows better performance. Fusion basically compromises both performances and produces the overall best performance. There are four categories where fusion successfully boosts the performances of both approaches. In these cases, image retrieval helps by \"disambiguating\" the rank lists generated by ingredient matching.\n\nThe retrieval performance is also affected by occlusion of ingredients. For example, the \"chicken\" in \"shredded chicken & pea sprouts\" is hardly visible under \"pea sprouts\", which is an ingredient unseen in VIREO Food-172. In this case, ingredient matching performs poorly as seen in Table 6. Image retrieval also performs unsatisfactorily due to diverse dish appearances for test images under this category. Another example is \"spicy crab\", where crab is hidden under other ingredients. Image retrieval, however, performs surprisingly well for this category because of the unique color and texture of the dishes. Finally, there are four categories that have unseen ingredients. Except \"spicy crab\", the performance of these categories is below average, showing the challenges of retrieval for recipes with unknown ingredients.\n\n\nCONCLUSIONS\n\nWe have presented two main pieces of our work: ingredient recognition and zero-shot recipe retrieval. The former is grounded on a deep architecture (Arch-D) that exploits the joint relationship between food and ingredient labels through multi-task learning. The latter extends the knowledge of Arch-D for the out-ofvocabulary scenario, by learning contextual relationships of ingredients from a large textual corpus of recipes. Experimental results on a challenging Chinese food dataset (VIREO Food-172) show that, while the performance of food categorization is enhanced slightly, the improvement in ingredient recognition is statistically significant compared to the best single-task VGG model. The superiority in performance is not only noticed in VIREO Food-172 but also UEC Food-100, a large-scale Japanese food dataset. More importantly, when extracting the deep features (FC7) from the specialized or private layer learnt for ingredient recognition, the features show highly favorable performance for zero-shot recipe retrieval, in comparison to hand-crafted features and single-task model. The performance of ingredient recognition is also successfully enhanced with the contextual relationship modeling of ingredients and CRF. The experiment also indicates that using our proposed architecture and CRF for ingredient prediction can produce better performance than directly inferring ingredients from recipes searched by VGG. When further using the predicted ingredients for matching recipes of unknown food categories, our model also demonstrates impressive performance, including when fusing with the deep features.\n\nWhile encouraging, the current work is worth further investigation in two directions. First, cooking method (e.g., frying, steaming, grilling) is not explicitly considered in the developed deep architecture. In the experiment, we notice that some dishes have the same ingredients but appear visually different mainly due to different cooking methods. Our current approach basically cannot distinguish recipes for this kind of dishes. Similarly for ways of cutting ingredients (e.g., chop, slice, mince) which may demand hierarchical way of ingredient recognition in deep network. In addition, our multi-task model could not deal with ingredients (e.g., honey, soy-bean oil) that are not observable or visible from dishes. Secondly, while this paper considers the zero-shot problem of unknown food categories, how to couple this problem together with unseen ingredients remains unclear. Future work may include learning of embedded space that can capture the inherent \"translation\" between dish pictures and textual recipes, for dealing with the problem of unknown food and ingredient labels.\n\nFigure 2 :\n2Framework overview: (a) ingredient recognition, (b) zero-shot recipe retrieval. Given a picture of dish with unknown food category, the framework retrieves a recipe for the dish. The recipe is originally in Chinese and Google translated to English.\n\nFigure 3 :\n3Four different deep architectures for multi-task learning of food category and ingredient recognition.\n\nFigure 4 :Figure 5 :\n45Examples of food categories in VIREO Food-172. The distribution of food categories under eight major food groups in VIREO Food-172.\n\nFigure 6 :\n6The ingredient \"egg\" shows large difference in visual appearance across different kinds of dishes.\n\nFigure 7 :\n7The distribution of food categories (a) and ingredients (b).\n\n\nshows the impact of \u03bb parameter in Eqn-1. Basically, the Top-1 and Mirco-F1 measures fluctuate within the range of 0.06, when the value of \u03bb varies from 0.1 to 1.0. The best performances attained for food categorization (Top-1) is when \u03bb = 0.1, and for ingredient recognition (Micro-F1) when \u03bb = 0.3. To balance the performances, we use F1 of Top-1 and Micro-F1 measures to pick the optimal value, where \u03bb = 0.2 as shown in Figure 8.\n\nFigure 8 :\n8Sensitivity of \u03bb parameter in Eqn-1 for multi-task deep architecture Arch-D.\n\nFigure 9 :\n9The F1 scores of 15 ingredients that achieve large margin of improvement after CRF.\n\nTable 2 :\n2Performance of multi-label ingredient recognition on VIREO Food-172 dataset.\nhttp://vireo.cs.cityu.edu.hk/VireoFood172/ 2 https://www.xiachufang.com/category/ 3 http://www.meishij.net\nhttp://www.xinshipu.com\nhttp://www-nlpir.nist.gov/projects/t01v/trecvid.tools/ randomization.testing\nACKNOWLEDGMENTS\nFoodlog: Multimedia tool for healthcare applications. K Aizawa, M Ogawa, IEEE MultiMedia. 222K. Aizawa and M. Ogawa. Foodlog: Multimedia tool for healthcare applications. IEEE MultiMedia, 22(2):4-8, 2015.\n\nMenu-match: Restaurant-specific food logging from images. O Beijbom, N Joshi, D Morris, S Saponas, S Khullar, Applications of Computer Vision (WACV), 2015 IEEE Winter Conference on. IEEEO. Beijbom, N. Joshi, D. Morris, S. Saponas, and S. Khullar. Menu-match: Restaurant-specific food logging from images. In Applications of Computer Vision (WACV), 2015 IEEE Winter Conference on, pages 844-851. IEEE, 2015.\n\nLeveraging context to support automated food recognition in restaurants. V Bettadapura, E Thomaz, A Parnami, G D Abowd, I Essa, Applications of Computer Vision (WACV), 2015 IEEE Winter Conference on. IEEEV. Bettadapura, E. Thomaz, A. Parnami, G. D. Abowd, and I. Essa. Leveraging context to support automated food recognition in restaurants. In Applications of Computer Vision (WACV), 2015 IEEE Winter Conference on, pages 580-587. IEEE, 2015.\n\nFood-101-mining discriminative components with random forests. L Bossard, M Guillaumin, L Van Gool, Computer Vision-ECCV 2014. SpringerL. Bossard, M. Guillaumin, and L. Van Gool. Food-101-mining discriminative components with random forests. In Computer Vision-ECCV 2014, pages 446-461. Springer, 2014.\n\nSemantic image segmentation with deep convolutional nets and fully connected crfs. L.-C Chen, G Papandreou, I Kokkinos, K Murphy, A L Yuille, arXiv:1412.7062arXiv preprintL.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. arXiv preprint arXiv:1412.7062, 2014.\n\nPfid: Pittsburgh fast-food image dataset. M Chen, K Dhingra, W Wu, L Yang, R Sukthankar, J Yang, 16th IEEE International Conference on. IEEEImage Processing (ICIP)M. Chen, K. Dhingra, W. Wu, L. Yang, R. Sukthankar, and J. Yang. Pfid: Pittsburgh fast-food image dataset. In Image Processing (ICIP), 2009 16th IEEE International Conference on, pages 289-292. IEEE, 2009.\n\nAutomatic chinese food identification and quantity estimation. M.-Y Chen, Y.-H Yang, C.-J Ho, S.-H Wang, S.-M Liu, E Chang, C.-H Yeh, M Ouhyoung, SIGGRAPH Asia 2012 Technical Briefs. ACM29M.-Y. Chen, Y.-H. Yang, C.-J. Ho, S.-H. Wang, S.-M. Liu, E. Chang, C.-H. Yeh, and M. Ouhyoung. Automatic chinese food identification and quantity estimation. In SIGGRAPH Asia 2012 Technical Briefs, page 29. ACM, 2012.\n\nHistograms of oriented gradients for human detection. Computer Vision and Pattern Recognition. N Dalal, B Triggs, N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. Computer Vision and Pattern Recognition, pages 886-893, 2005.\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, Computer Vision and Pattern Recognition. IEEECVPR 2009. IEEE Conference onJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248-255. IEEE, 2009.\n\nConvolutional models for joint object categorization and pose estimation. M Elhoseiny, T El-Gaaly, A Bakry, A Elgammal, arXiv:1511.05175arXiv preprintM. Elhoseiny, T. El-Gaaly, A. Bakry, and A. Elgammal. Convolutional models for joint object categorization and pose estimation. arXiv preprint arXiv:1511.05175, 2015.\n\nUndereating and underrecording of habitual food intake in obese men: selective underreporting of fat intake. A H Goris, M S Westerterp-Plantenga, K R Westerterp, The American journal of clinical nutrition. 711A. H. Goris, M. S. Westerterp-Plantenga, and K. R. Westerterp. Undereating and underrecording of habitual food intake in obese men: selective underreporting of fat intake. The American journal of clinical nutrition, 71(1):130-134, 2000.\n\nDietcam: Multi-view food recognition using a multi-kernel svm. H He, F Kong, J Tan, Journal of Biomedical and Health Informatics. H. He, F. Kong, and J. Tan. Dietcam: Multi-view food recognition using a multi-kernel svm. Journal of Biomedical and Health Informatics, 2015.\n\nImage recognition of 85 food categories by feature fusion. H Hoashi, T Joutou, K Yanai, Multimedia (ISM), 2010 IEEE International Symposium on. IEEEH. Hoashi, T. Joutou, and K. Yanai. Image recognition of 85 food categories by feature fusion. In Multimedia (ISM), 2010 IEEE International Symposium on, pages 296-301. IEEE, 2010.\n\nReal-time mobile food recognition system. Y Kawano, K Yanai, Computer Vision and Pattern Recognition Workshop. Y. Kawano and K. Yanai. Real-time mobile food recognition system. In Computer Vision and Pattern Recognition Workshop, 2013.\n\nFood image recognition with deep convolutional features. Y Kawano, CEAK Yanai, CEAProc. of ACM UbiComp Workshop on Cooking and Eating Activities. of ACM UbiComp Workshop on Cooking and Eating ActivitiesY. Kawano and K. Yanai. Food image recognition with deep convolutional features. Proc. of ACM UbiComp Workshop on Cooking and Eating Activities (CEA), 2014.\n\nFood log by analyzing food images. K Kitamura, T Yamasaki, K Aizawa, Proceedings of the 16th ACM international conference on Multimedia. the 16th ACM international conference on MultimediaACMK. Kitamura, T. Yamasaki, and K. Aizawa. Food log by analyzing food images. In Proceedings of the 16th ACM international conference on Multimedia, pages 999-1000. ACM, 2008.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, pages 1097-1105, 2012.\n\nDeep lac: Deep localization, alignment and classification for fine-grained recognition. D Lin, X Shen, C Lu, J Jia, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionD. Lin, X. Shen, C. Lu, and J. Jia. Deep lac: Deep localization, alignment and classification for fine-grained recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1666-1674, 2015.\n\nBilinear cnn models for fine-grained visual recognition. T.-Y Lin, A Roychowdhury, S Maji, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionT.-Y. Lin, A. RoyChowdhury, and S. Maji. Bilinear cnn models for fine-grained visual recognition. In Proceedings of the IEEE International Conference on Computer Vision, pages 1449-1457, 2015.\n\nDeep learning face attributes in the wild. Z Liu, P Luo, X Wang, X Tang, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionZ. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE International Conference on Computer Vision, pages 3730-3738, 2015.\n\nObject recognition from local scale-invariant features. D G Lowe, The proceedings of the seventh IEEE international conference on Computer Vision. D. G. Lowe. Object recognition from local scale-invariant features. The proceedings of the seventh IEEE international conference on Computer Vision,, pages 1150-1157, 1999.\n\nMultiple-food recognition considering co-occurrence employing manifold ranking. Y Matsuda, K Yanai, Pattern Recognition (ICPR), 2012 21st International Conference on. IEEEY. Matsuda and K. Yanai. Multiple-food recognition considering co-occurrence employing manifold ranking. In Pattern Recognition (ICPR), 2012 21st International Conference on, pages 2017-2020. IEEE, 2012.\n\nTastes and textures estimation of foods based on the analysis of its ingredients list and image. H Matsunaga, K Doman, T Hirayama, I Ide, D Deguchi, H Murase, New Trends in Image Analysis and Processing-ICIAP 2015 Workshops. H. Matsunaga, K. Doman, T. Hirayama, I. Ide, D. Deguchi, and H. Murase. Tastes and textures estimation of foods based on the analysis of its ingredients list and image. In New Trends in Image Analysis and Processing-ICIAP 2015 Workshops, pages 326-333.\n\n. Springer, Springer, 2015.\n\nIm2calories: towards an automated mobile vision food diary. A Meyers, N Johnston, V Rathod, A Korattikara, A Gorban, N Silberman, S Guadarrama, G Papandreou, J Huang, K P Murphy, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionA. Meyers, N. Johnston, V. Rathod, A. Korattikara, A. Gorban, N. Silberman, S. Guadarrama, G. Papandreou, J. Huang, and K. P. Murphy. Im2calories: towards an automated mobile vision food diary. In Proceedings of the IEEE International Conference on Computer Vision, pages 1233-1241, 2015.\n\nRecognition and volume estimation of food intake using a mobile device. M Puri, Z Zhu, Q Yu, A Divakaran, H Sawhney, M. Puri, Z. Zhu, Q. Yu, A. Divakaran, and H. Sawhney. Recognition and volume estimation of food intake using a mobile device, 2009.\n\nMonte Carlo statistical methods. C Robert, G Casella, Springer Science & Business MediaC. Robert and G. Casella. Monte Carlo statistical methods. Springer Science & Business Media, 2013.\n\nRapid biologically-inspired scene classification using features shared with visual attention. Pattern Analysis and Machine Intelligence. C Siagian, L Itti, IEEE Transactions on. 292C. Siagian and L. Itti. Rapid biologically-inspired scene classification using features shared with visual attention. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(2):300-312, 2007.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556arXiv preprintK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nZero-shot learning through cross-modal transfer. R Socher, M Ganjoo, C D Manning, A Ng, Advances in neural information processing systems. R. Socher, M. Ganjoo, C. D. Manning, and A. Ng. Zero-shot learning through cross-modal transfer. In Advances in neural information processing systems, pages 935-943, 2013.\n\nSimilarity of color images. M A Stricker, M Orengo, IS&T/SPIE's Symposium on Electronic Imaging: Science & Technology. M. A. Stricker and M. Orengo. Similarity of color images. In IS&T/SPIE's Symposium on Electronic Imaging: Science & Technology, pages 381-392. International Society for Optics and Photonics, 1995.\n\nDeep learning face representation by joint identification-verification. Y Sun, Y Chen, X Wang, X Tang, Advances in Neural Information Processing Systems. Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning face representation by joint identification-verification. In Advances in Neural Information Processing Systems, pages 1988-1996, 2014.\n\nDeep learning face representation from predicting 10,000 classes. Y Sun, X Wang, X Tang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionY. Sun, X. Wang, and X. Tang. Deep learning face representation from predicting 10,000 classes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1891-1898, 2014.\n\nGraphical models, exponential families, and variational inference. Foundations and Trends R in Machine Learning. M J Wainwright, M I Jordan, 1M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends R in Machine Learning, 1(1-2):1-305, 2008.\n\nRecipe recognition with large multimodal food dataset. X Wang, D Kumar, N Thome, M Cord, F Precioso, Multimedia & Expo Workshops (ICMEW), 2015 IEEE International Conference on. IEEEX. Wang, D. Kumar, N. Thome, M. Cord, and F. Precioso. Recipe recognition with large multimodal food dataset. In Multimedia & Expo Workshops (ICMEW), 2015 IEEE International Conference on, pages 1-6. IEEE, 2015.\n\nFast food recognition from videos of eating for calorie estimation. W Wu, J Yang, IEEE International Conference on. IEEEMultimedia and ExpoW. Wu and J. Yang. Fast food recognition from videos of eating for calorie estimation. In Multimedia and Expo, 2009. ICME 2009. IEEE International Conference on, pages 1210-1213. IEEE, 2009.\n\nFood image recognition using deep convolutional network with pre-training and fine-tuning. K Yanai, Y Kawano, Multimedia & Expo Workshops (ICMEW), 2015 IEEE International Conference on. IEEEK. Yanai and Y. Kawano. Food image recognition using deep convolutional network with pre-training and fine-tuning. In Multimedia & Expo Workshops (ICMEW), 2015 IEEE International Conference on, pages 1-6. IEEE, 2015.\n\nFood recognition using statistics of pairwise local features. S Yang, M Chen, D Pomerleau, R Sukthankar, Computer Vision and Pattern Recognition (CVPR). S. Yang, M. Chen, D. Pomerleau, and R. Sukthankar. Food recognition using statistics of pairwise local features. Computer Vision and Pattern Recognition (CVPR), pages 2249-2256, 2010.\n", "annotations": {"author": "[{\"end\":154,\"start\":66},{\"end\":215,\"start\":155},{\"end\":154,\"start\":66},{\"end\":215,\"start\":155}]", "publisher": null, "author_last_name": "[{\"end\":79,\"start\":75},{\"end\":168,\"start\":165},{\"end\":79,\"start\":75},{\"end\":168,\"start\":165}]", "author_first_name": "[{\"end\":74,\"start\":66},{\"end\":164,\"start\":155},{\"end\":74,\"start\":66},{\"end\":164,\"start\":155}]", "author_affiliation": "[{\"end\":153,\"start\":109},{\"end\":214,\"start\":170},{\"end\":153,\"start\":109},{\"end\":214,\"start\":170}]", "title": "[{\"end\":63,\"start\":1},{\"end\":278,\"start\":216},{\"end\":63,\"start\":1},{\"end\":278,\"start\":216}]", "venue": null, "abstract": "[{\"end\":1978,\"start\":388},{\"end\":1978,\"start\":388}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2382,\"start\":2379},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3485,\"start\":3481},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3734,\"start\":3730},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3739,\"start\":3735},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3744,\"start\":3740},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3753,\"start\":3749},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4276,\"start\":4272},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4281,\"start\":4277},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4947,\"start\":4943},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4966,\"start\":4963},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5042,\"start\":5038},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6233,\"start\":6229},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8790,\"start\":8787},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8795,\"start\":8791},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9152,\"start\":9149},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9171,\"start\":9167},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9184,\"start\":9181},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9469,\"start\":9466},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9516,\"start\":9512},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9575,\"start\":9572},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9598,\"start\":9594},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9627,\"start\":9623},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9652,\"start\":9648},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9683,\"start\":9679},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9987,\"start\":9983},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9996,\"start\":9993},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10011,\"start\":10007},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10060,\"start\":10056},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10094,\"start\":10091},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10179,\"start\":10175},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10189,\"start\":10185},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10302,\"start\":10298},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10307,\"start\":10303},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10435,\"start\":10431},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10520,\"start\":10516},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10869,\"start\":10865},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11467,\"start\":11463},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11826,\"start\":11822},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11846,\"start\":11843},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11874,\"start\":11870},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11923,\"start\":11919},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12127,\"start\":12123},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12299,\"start\":12295},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12540,\"start\":12537},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12841,\"start\":12838},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12949,\"start\":12945},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12989,\"start\":12985},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13109,\"start\":13106},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13524,\"start\":13520},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13534,\"start\":13530},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13571,\"start\":13567},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13581,\"start\":13577},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13590,\"start\":13586},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13735,\"start\":13731},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13839,\"start\":13835},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13987,\"start\":13983},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14016,\"start\":14012},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14698,\"start\":14694},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14711,\"start\":14707},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17903,\"start\":17899},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18557,\"start\":18553},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22566,\"start\":22562},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":23304,\"start\":23300},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24463,\"start\":24460},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24472,\"start\":24469},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29768,\"start\":29764},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":29790,\"start\":29786},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":29818,\"start\":29814},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":33757,\"start\":33753},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34261,\"start\":34257},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34270,\"start\":34266},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34610,\"start\":34606},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":36385,\"start\":36381},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2382,\"start\":2379},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3485,\"start\":3481},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3734,\"start\":3730},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3739,\"start\":3735},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3744,\"start\":3740},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3753,\"start\":3749},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4276,\"start\":4272},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4281,\"start\":4277},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4947,\"start\":4943},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4966,\"start\":4963},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5042,\"start\":5038},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6233,\"start\":6229},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8790,\"start\":8787},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8795,\"start\":8791},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9152,\"start\":9149},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9171,\"start\":9167},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9184,\"start\":9181},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9469,\"start\":9466},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9516,\"start\":9512},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9575,\"start\":9572},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9598,\"start\":9594},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9627,\"start\":9623},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9652,\"start\":9648},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9683,\"start\":9679},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9987,\"start\":9983},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9996,\"start\":9993},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10011,\"start\":10007},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10060,\"start\":10056},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10094,\"start\":10091},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10179,\"start\":10175},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10189,\"start\":10185},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10302,\"start\":10298},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10307,\"start\":10303},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10435,\"start\":10431},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10520,\"start\":10516},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10869,\"start\":10865},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11467,\"start\":11463},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11826,\"start\":11822},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11846,\"start\":11843},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11874,\"start\":11870},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11923,\"start\":11919},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12127,\"start\":12123},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12299,\"start\":12295},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12540,\"start\":12537},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12841,\"start\":12838},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12949,\"start\":12945},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12989,\"start\":12985},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13109,\"start\":13106},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13524,\"start\":13520},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13534,\"start\":13530},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13571,\"start\":13567},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13581,\"start\":13577},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13590,\"start\":13586},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13735,\"start\":13731},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13839,\"start\":13835},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13987,\"start\":13983},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14016,\"start\":14012},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14698,\"start\":14694},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14711,\"start\":14707},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17903,\"start\":17899},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18557,\"start\":18553},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22566,\"start\":22562},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":23304,\"start\":23300},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24463,\"start\":24460},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24472,\"start\":24469},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29768,\"start\":29764},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":29790,\"start\":29786},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":29818,\"start\":29814},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":33757,\"start\":33753},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34261,\"start\":34257},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34270,\"start\":34266},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34610,\"start\":34606},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":36385,\"start\":36381}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":46337,\"start\":46076},{\"attributes\":{\"id\":\"fig_1\"},\"end\":46453,\"start\":46338},{\"attributes\":{\"id\":\"fig_2\"},\"end\":46609,\"start\":46454},{\"attributes\":{\"id\":\"fig_3\"},\"end\":46721,\"start\":46610},{\"attributes\":{\"id\":\"fig_5\"},\"end\":46795,\"start\":46722},{\"attributes\":{\"id\":\"fig_6\"},\"end\":47231,\"start\":46796},{\"attributes\":{\"id\":\"fig_7\"},\"end\":47321,\"start\":47232},{\"attributes\":{\"id\":\"fig_8\"},\"end\":47418,\"start\":47322},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":47507,\"start\":47419},{\"attributes\":{\"id\":\"fig_0\"},\"end\":46337,\"start\":46076},{\"attributes\":{\"id\":\"fig_1\"},\"end\":46453,\"start\":46338},{\"attributes\":{\"id\":\"fig_2\"},\"end\":46609,\"start\":46454},{\"attributes\":{\"id\":\"fig_3\"},\"end\":46721,\"start\":46610},{\"attributes\":{\"id\":\"fig_5\"},\"end\":46795,\"start\":46722},{\"attributes\":{\"id\":\"fig_6\"},\"end\":47231,\"start\":46796},{\"attributes\":{\"id\":\"fig_7\"},\"end\":47321,\"start\":47232},{\"attributes\":{\"id\":\"fig_8\"},\"end\":47418,\"start\":47322},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":47507,\"start\":47419}]", "paragraph": "[{\"end\":3754,\"start\":1994},{\"end\":5234,\"start\":3756},{\"end\":6234,\"start\":5236},{\"end\":8462,\"start\":6236},{\"end\":9273,\"start\":8464},{\"end\":9804,\"start\":9290},{\"end\":10715,\"start\":9806},{\"end\":11780,\"start\":10717},{\"end\":13452,\"start\":11782},{\"end\":14516,\"start\":13472},{\"end\":15358,\"start\":14545},{\"end\":16344,\"start\":15382},{\"end\":17279,\"start\":16346},{\"end\":17822,\"start\":17419},{\"end\":18409,\"start\":17841},{\"end\":18985,\"start\":18440},{\"end\":19334,\"start\":19004},{\"end\":19507,\"start\":19388},{\"end\":20985,\"start\":19531},{\"end\":21828,\"start\":21020},{\"end\":22324,\"start\":21830},{\"end\":22734,\"start\":22375},{\"end\":22972,\"start\":22777},{\"end\":23433,\"start\":23022},{\"end\":23784,\"start\":23451},{\"end\":24261,\"start\":23806},{\"end\":24611,\"start\":24284},{\"end\":25358,\"start\":24630},{\"end\":25987,\"start\":25360},{\"end\":27009,\"start\":26011},{\"end\":27984,\"start\":27011},{\"end\":28637,\"start\":28002},{\"end\":29077,\"start\":28653},{\"end\":30465,\"start\":29100},{\"end\":33655,\"start\":30467},{\"end\":34845,\"start\":33657},{\"end\":35935,\"start\":34863},{\"end\":37283,\"start\":35937},{\"end\":38265,\"start\":37285},{\"end\":39172,\"start\":38296},{\"end\":40146,\"start\":39174},{\"end\":40986,\"start\":40157},{\"end\":42512,\"start\":40988},{\"end\":43341,\"start\":42514},{\"end\":44982,\"start\":43357},{\"end\":46075,\"start\":44984},{\"end\":3754,\"start\":1994},{\"end\":5234,\"start\":3756},{\"end\":6234,\"start\":5236},{\"end\":8462,\"start\":6236},{\"end\":9273,\"start\":8464},{\"end\":9804,\"start\":9290},{\"end\":10715,\"start\":9806},{\"end\":11780,\"start\":10717},{\"end\":13452,\"start\":11782},{\"end\":14516,\"start\":13472},{\"end\":15358,\"start\":14545},{\"end\":16344,\"start\":15382},{\"end\":17279,\"start\":16346},{\"end\":17822,\"start\":17419},{\"end\":18409,\"start\":17841},{\"end\":18985,\"start\":18440},{\"end\":19334,\"start\":19004},{\"end\":19507,\"start\":19388},{\"end\":20985,\"start\":19531},{\"end\":21828,\"start\":21020},{\"end\":22324,\"start\":21830},{\"end\":22734,\"start\":22375},{\"end\":22972,\"start\":22777},{\"end\":23433,\"start\":23022},{\"end\":23784,\"start\":23451},{\"end\":24261,\"start\":23806},{\"end\":24611,\"start\":24284},{\"end\":25358,\"start\":24630},{\"end\":25987,\"start\":25360},{\"end\":27009,\"start\":26011},{\"end\":27984,\"start\":27011},{\"end\":28637,\"start\":28002},{\"end\":29077,\"start\":28653},{\"end\":30465,\"start\":29100},{\"end\":33655,\"start\":30467},{\"end\":34845,\"start\":33657},{\"end\":35935,\"start\":34863},{\"end\":37283,\"start\":35937},{\"end\":38265,\"start\":37285},{\"end\":39172,\"start\":38296},{\"end\":40146,\"start\":39174},{\"end\":40986,\"start\":40157},{\"end\":42512,\"start\":40988},{\"end\":43341,\"start\":42514},{\"end\":44982,\"start\":43357},{\"end\":46075,\"start\":44984}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17395,\"start\":17345},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18439,\"start\":18410},{\"attributes\":{\"id\":\"formula_2\"},\"end\":19003,\"start\":18986},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19387,\"start\":19335},{\"attributes\":{\"id\":\"formula_4\"},\"end\":22374,\"start\":22325},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22776,\"start\":22735},{\"attributes\":{\"id\":\"formula_6\"},\"end\":23021,\"start\":22973},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23805,\"start\":23785},{\"attributes\":{\"id\":\"formula_0\"},\"end\":17395,\"start\":17345},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18439,\"start\":18410},{\"attributes\":{\"id\":\"formula_2\"},\"end\":19003,\"start\":18986},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19387,\"start\":19335},{\"attributes\":{\"id\":\"formula_4\"},\"end\":22374,\"start\":22325},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22776,\"start\":22735},{\"attributes\":{\"id\":\"formula_6\"},\"end\":23021,\"start\":22973},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23805,\"start\":23785}]", "table_ref": "[{\"end\":31049,\"start\":31042},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32490,\"start\":32483},{\"end\":34468,\"start\":34461},{\"end\":34795,\"start\":34788},{\"end\":35875,\"start\":35868},{\"end\":36740,\"start\":36733},{\"end\":40181,\"start\":40174},{\"end\":40233,\"start\":40226},{\"end\":41151,\"start\":41144},{\"end\":41367,\"start\":41360},{\"end\":42805,\"start\":42798},{\"end\":31049,\"start\":31042},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32490,\"start\":32483},{\"end\":34468,\"start\":34461},{\"end\":34795,\"start\":34788},{\"end\":35875,\"start\":35868},{\"end\":36740,\"start\":36733},{\"end\":40181,\"start\":40174},{\"end\":40233,\"start\":40226},{\"end\":41151,\"start\":41144},{\"end\":41367,\"start\":41360},{\"end\":42805,\"start\":42798}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1992,\"start\":1980},{\"attributes\":{\"n\":\"2.\"},\"end\":9288,\"start\":9276},{\"end\":13470,\"start\":13455},{\"attributes\":{\"n\":\"3.\"},\"end\":14543,\"start\":14519},{\"attributes\":{\"n\":\"3.1\"},\"end\":15380,\"start\":15361},{\"end\":17344,\"start\":17282},{\"end\":17417,\"start\":17397},{\"attributes\":{\"n\":\"3.2\"},\"end\":17839,\"start\":17825},{\"attributes\":{\"n\":\"4.\"},\"end\":19529,\"start\":19510},{\"attributes\":{\"n\":\"4.1\"},\"end\":21018,\"start\":20988},{\"attributes\":{\"n\":\"4.2\"},\"end\":23449,\"start\":23436},{\"attributes\":{\"n\":\"5.\"},\"end\":24282,\"start\":24264},{\"attributes\":{\"n\":\"5.1\"},\"end\":24628,\"start\":24614},{\"attributes\":{\"n\":\"5.2\"},\"end\":26009,\"start\":25990},{\"attributes\":{\"n\":\"5.3\"},\"end\":28000,\"start\":27987},{\"attributes\":{\"n\":\"6.\"},\"end\":28651,\"start\":28640},{\"attributes\":{\"n\":\"6.1\"},\"end\":29098,\"start\":29080},{\"attributes\":{\"n\":\"6.2\"},\"end\":34861,\"start\":34848},{\"attributes\":{\"n\":\"6.3\"},\"end\":38294,\"start\":38268},{\"end\":40155,\"start\":40149},{\"attributes\":{\"n\":\"7.\"},\"end\":43355,\"start\":43344},{\"end\":46087,\"start\":46077},{\"end\":46349,\"start\":46339},{\"end\":46475,\"start\":46455},{\"end\":46621,\"start\":46611},{\"end\":46733,\"start\":46723},{\"end\":47243,\"start\":47233},{\"end\":47333,\"start\":47323},{\"end\":47429,\"start\":47420},{\"attributes\":{\"n\":\"1.\"},\"end\":1992,\"start\":1980},{\"attributes\":{\"n\":\"2.\"},\"end\":9288,\"start\":9276},{\"end\":13470,\"start\":13455},{\"attributes\":{\"n\":\"3.\"},\"end\":14543,\"start\":14519},{\"attributes\":{\"n\":\"3.1\"},\"end\":15380,\"start\":15361},{\"end\":17344,\"start\":17282},{\"end\":17417,\"start\":17397},{\"attributes\":{\"n\":\"3.2\"},\"end\":17839,\"start\":17825},{\"attributes\":{\"n\":\"4.\"},\"end\":19529,\"start\":19510},{\"attributes\":{\"n\":\"4.1\"},\"end\":21018,\"start\":20988},{\"attributes\":{\"n\":\"4.2\"},\"end\":23449,\"start\":23436},{\"attributes\":{\"n\":\"5.\"},\"end\":24282,\"start\":24264},{\"attributes\":{\"n\":\"5.1\"},\"end\":24628,\"start\":24614},{\"attributes\":{\"n\":\"5.2\"},\"end\":26009,\"start\":25990},{\"attributes\":{\"n\":\"5.3\"},\"end\":28000,\"start\":27987},{\"attributes\":{\"n\":\"6.\"},\"end\":28651,\"start\":28640},{\"attributes\":{\"n\":\"6.1\"},\"end\":29098,\"start\":29080},{\"attributes\":{\"n\":\"6.2\"},\"end\":34861,\"start\":34848},{\"attributes\":{\"n\":\"6.3\"},\"end\":38294,\"start\":38268},{\"end\":40155,\"start\":40149},{\"attributes\":{\"n\":\"7.\"},\"end\":43355,\"start\":43344},{\"end\":46087,\"start\":46077},{\"end\":46349,\"start\":46339},{\"end\":46475,\"start\":46455},{\"end\":46621,\"start\":46611},{\"end\":46733,\"start\":46723},{\"end\":47243,\"start\":47233},{\"end\":47333,\"start\":47323},{\"end\":47429,\"start\":47420}]", "table": null, "figure_caption": "[{\"end\":46337,\"start\":46089},{\"end\":46453,\"start\":46351},{\"end\":46609,\"start\":46478},{\"end\":46721,\"start\":46623},{\"end\":46795,\"start\":46735},{\"end\":47231,\"start\":46798},{\"end\":47321,\"start\":47245},{\"end\":47418,\"start\":47335},{\"end\":47507,\"start\":47431},{\"end\":46337,\"start\":46089},{\"end\":46453,\"start\":46351},{\"end\":46609,\"start\":46478},{\"end\":46721,\"start\":46623},{\"end\":46795,\"start\":46735},{\"end\":47231,\"start\":46798},{\"end\":47321,\"start\":47245},{\"end\":47418,\"start\":47335},{\"end\":47507,\"start\":47431}]", "figure_ref": "[{\"end\":3030,\"start\":3022},{\"end\":3979,\"start\":3971},{\"end\":5381,\"start\":5373},{\"end\":5619,\"start\":5610},{\"end\":6346,\"start\":6338},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7323,\"start\":7315},{\"end\":13451,\"start\":13443},{\"end\":14332,\"start\":14323},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16445,\"start\":16437},{\"end\":25433,\"start\":25425},{\"end\":25929,\"start\":25921},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26700,\"start\":26692},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27819,\"start\":27811},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":30309,\"start\":30301},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":31041,\"start\":31033},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":37348,\"start\":37340},{\"end\":37781,\"start\":37772},{\"end\":37994,\"start\":37985},{\"end\":38011,\"start\":38002},{\"end\":40682,\"start\":40673},{\"end\":3030,\"start\":3022},{\"end\":3979,\"start\":3971},{\"end\":5381,\"start\":5373},{\"end\":5619,\"start\":5610},{\"end\":6346,\"start\":6338},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7323,\"start\":7315},{\"end\":13451,\"start\":13443},{\"end\":14332,\"start\":14323},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16445,\"start\":16437},{\"end\":25433,\"start\":25425},{\"end\":25929,\"start\":25921},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26700,\"start\":26692},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27819,\"start\":27811},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":30309,\"start\":30301},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":31041,\"start\":31033},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":37348,\"start\":37340},{\"end\":37781,\"start\":37772},{\"end\":37994,\"start\":37985},{\"end\":38011,\"start\":38002},{\"end\":40682,\"start\":40673}]", "bib_author_first_name": "[{\"end\":47787,\"start\":47786},{\"end\":47797,\"start\":47796},{\"end\":47997,\"start\":47996},{\"end\":48008,\"start\":48007},{\"end\":48017,\"start\":48016},{\"end\":48027,\"start\":48026},{\"end\":48038,\"start\":48037},{\"end\":48420,\"start\":48419},{\"end\":48435,\"start\":48434},{\"end\":48445,\"start\":48444},{\"end\":48456,\"start\":48455},{\"end\":48458,\"start\":48457},{\"end\":48467,\"start\":48466},{\"end\":48855,\"start\":48854},{\"end\":48866,\"start\":48865},{\"end\":48880,\"start\":48879},{\"end\":49182,\"start\":49178},{\"end\":49190,\"start\":49189},{\"end\":49204,\"start\":49203},{\"end\":49216,\"start\":49215},{\"end\":49226,\"start\":49225},{\"end\":49228,\"start\":49227},{\"end\":49500,\"start\":49499},{\"end\":49508,\"start\":49507},{\"end\":49519,\"start\":49518},{\"end\":49525,\"start\":49524},{\"end\":49533,\"start\":49532},{\"end\":49547,\"start\":49546},{\"end\":49894,\"start\":49890},{\"end\":49905,\"start\":49901},{\"end\":49916,\"start\":49912},{\"end\":49925,\"start\":49921},{\"end\":49936,\"start\":49932},{\"end\":49943,\"start\":49942},{\"end\":49955,\"start\":49951},{\"end\":49962,\"start\":49961},{\"end\":50330,\"start\":50329},{\"end\":50339,\"start\":50338},{\"end\":50543,\"start\":50542},{\"end\":50551,\"start\":50550},{\"end\":50559,\"start\":50558},{\"end\":50572,\"start\":50568},{\"end\":50578,\"start\":50577},{\"end\":50584,\"start\":50583},{\"end\":50967,\"start\":50966},{\"end\":50980,\"start\":50979},{\"end\":50992,\"start\":50991},{\"end\":51001,\"start\":51000},{\"end\":51320,\"start\":51319},{\"end\":51322,\"start\":51321},{\"end\":51331,\"start\":51330},{\"end\":51333,\"start\":51332},{\"end\":51357,\"start\":51356},{\"end\":51359,\"start\":51358},{\"end\":51721,\"start\":51720},{\"end\":51727,\"start\":51726},{\"end\":51735,\"start\":51734},{\"end\":51991,\"start\":51990},{\"end\":52001,\"start\":52000},{\"end\":52011,\"start\":52010},{\"end\":52304,\"start\":52303},{\"end\":52314,\"start\":52313},{\"end\":52556,\"start\":52555},{\"end\":52569,\"start\":52568},{\"end\":52894,\"start\":52893},{\"end\":52906,\"start\":52905},{\"end\":52918,\"start\":52917},{\"end\":53290,\"start\":53289},{\"end\":53304,\"start\":53303},{\"end\":53317,\"start\":53316},{\"end\":53319,\"start\":53318},{\"end\":53655,\"start\":53654},{\"end\":53662,\"start\":53661},{\"end\":53670,\"start\":53669},{\"end\":53676,\"start\":53675},{\"end\":54114,\"start\":54110},{\"end\":54121,\"start\":54120},{\"end\":54137,\"start\":54136},{\"end\":54503,\"start\":54502},{\"end\":54510,\"start\":54509},{\"end\":54517,\"start\":54516},{\"end\":54525,\"start\":54524},{\"end\":54887,\"start\":54886},{\"end\":54889,\"start\":54888},{\"end\":55232,\"start\":55231},{\"end\":55243,\"start\":55242},{\"end\":55625,\"start\":55624},{\"end\":55638,\"start\":55637},{\"end\":55647,\"start\":55646},{\"end\":55659,\"start\":55658},{\"end\":55666,\"start\":55665},{\"end\":55677,\"start\":55676},{\"end\":56096,\"start\":56095},{\"end\":56106,\"start\":56105},{\"end\":56118,\"start\":56117},{\"end\":56128,\"start\":56127},{\"end\":56143,\"start\":56142},{\"end\":56153,\"start\":56152},{\"end\":56166,\"start\":56165},{\"end\":56180,\"start\":56179},{\"end\":56194,\"start\":56193},{\"end\":56203,\"start\":56202},{\"end\":56205,\"start\":56204},{\"end\":56698,\"start\":56697},{\"end\":56706,\"start\":56705},{\"end\":56713,\"start\":56712},{\"end\":56719,\"start\":56718},{\"end\":56732,\"start\":56731},{\"end\":56909,\"start\":56908},{\"end\":56919,\"start\":56918},{\"end\":57201,\"start\":57200},{\"end\":57212,\"start\":57211},{\"end\":57518,\"start\":57517},{\"end\":57530,\"start\":57529},{\"end\":57758,\"start\":57757},{\"end\":57768,\"start\":57767},{\"end\":57778,\"start\":57777},{\"end\":57780,\"start\":57779},{\"end\":57791,\"start\":57790},{\"end\":58049,\"start\":58048},{\"end\":58051,\"start\":58050},{\"end\":58063,\"start\":58062},{\"end\":58410,\"start\":58409},{\"end\":58417,\"start\":58416},{\"end\":58425,\"start\":58424},{\"end\":58433,\"start\":58432},{\"end\":58747,\"start\":58746},{\"end\":58754,\"start\":58753},{\"end\":58762,\"start\":58761},{\"end\":59226,\"start\":59225},{\"end\":59228,\"start\":59227},{\"end\":59242,\"start\":59241},{\"end\":59244,\"start\":59243},{\"end\":59479,\"start\":59478},{\"end\":59487,\"start\":59486},{\"end\":59496,\"start\":59495},{\"end\":59505,\"start\":59504},{\"end\":59513,\"start\":59512},{\"end\":59886,\"start\":59885},{\"end\":59892,\"start\":59891},{\"end\":60240,\"start\":60239},{\"end\":60249,\"start\":60248},{\"end\":60619,\"start\":60618},{\"end\":60627,\"start\":60626},{\"end\":60635,\"start\":60634},{\"end\":60648,\"start\":60647},{\"end\":47787,\"start\":47786},{\"end\":47797,\"start\":47796},{\"end\":47997,\"start\":47996},{\"end\":48008,\"start\":48007},{\"end\":48017,\"start\":48016},{\"end\":48027,\"start\":48026},{\"end\":48038,\"start\":48037},{\"end\":48420,\"start\":48419},{\"end\":48435,\"start\":48434},{\"end\":48445,\"start\":48444},{\"end\":48456,\"start\":48455},{\"end\":48458,\"start\":48457},{\"end\":48467,\"start\":48466},{\"end\":48855,\"start\":48854},{\"end\":48866,\"start\":48865},{\"end\":48880,\"start\":48879},{\"end\":49182,\"start\":49178},{\"end\":49190,\"start\":49189},{\"end\":49204,\"start\":49203},{\"end\":49216,\"start\":49215},{\"end\":49226,\"start\":49225},{\"end\":49228,\"start\":49227},{\"end\":49500,\"start\":49499},{\"end\":49508,\"start\":49507},{\"end\":49519,\"start\":49518},{\"end\":49525,\"start\":49524},{\"end\":49533,\"start\":49532},{\"end\":49547,\"start\":49546},{\"end\":49894,\"start\":49890},{\"end\":49905,\"start\":49901},{\"end\":49916,\"start\":49912},{\"end\":49925,\"start\":49921},{\"end\":49936,\"start\":49932},{\"end\":49943,\"start\":49942},{\"end\":49955,\"start\":49951},{\"end\":49962,\"start\":49961},{\"end\":50330,\"start\":50329},{\"end\":50339,\"start\":50338},{\"end\":50543,\"start\":50542},{\"end\":50551,\"start\":50550},{\"end\":50559,\"start\":50558},{\"end\":50572,\"start\":50568},{\"end\":50578,\"start\":50577},{\"end\":50584,\"start\":50583},{\"end\":50967,\"start\":50966},{\"end\":50980,\"start\":50979},{\"end\":50992,\"start\":50991},{\"end\":51001,\"start\":51000},{\"end\":51320,\"start\":51319},{\"end\":51322,\"start\":51321},{\"end\":51331,\"start\":51330},{\"end\":51333,\"start\":51332},{\"end\":51357,\"start\":51356},{\"end\":51359,\"start\":51358},{\"end\":51721,\"start\":51720},{\"end\":51727,\"start\":51726},{\"end\":51735,\"start\":51734},{\"end\":51991,\"start\":51990},{\"end\":52001,\"start\":52000},{\"end\":52011,\"start\":52010},{\"end\":52304,\"start\":52303},{\"end\":52314,\"start\":52313},{\"end\":52556,\"start\":52555},{\"end\":52569,\"start\":52568},{\"end\":52894,\"start\":52893},{\"end\":52906,\"start\":52905},{\"end\":52918,\"start\":52917},{\"end\":53290,\"start\":53289},{\"end\":53304,\"start\":53303},{\"end\":53317,\"start\":53316},{\"end\":53319,\"start\":53318},{\"end\":53655,\"start\":53654},{\"end\":53662,\"start\":53661},{\"end\":53670,\"start\":53669},{\"end\":53676,\"start\":53675},{\"end\":54114,\"start\":54110},{\"end\":54121,\"start\":54120},{\"end\":54137,\"start\":54136},{\"end\":54503,\"start\":54502},{\"end\":54510,\"start\":54509},{\"end\":54517,\"start\":54516},{\"end\":54525,\"start\":54524},{\"end\":54887,\"start\":54886},{\"end\":54889,\"start\":54888},{\"end\":55232,\"start\":55231},{\"end\":55243,\"start\":55242},{\"end\":55625,\"start\":55624},{\"end\":55638,\"start\":55637},{\"end\":55647,\"start\":55646},{\"end\":55659,\"start\":55658},{\"end\":55666,\"start\":55665},{\"end\":55677,\"start\":55676},{\"end\":56096,\"start\":56095},{\"end\":56106,\"start\":56105},{\"end\":56118,\"start\":56117},{\"end\":56128,\"start\":56127},{\"end\":56143,\"start\":56142},{\"end\":56153,\"start\":56152},{\"end\":56166,\"start\":56165},{\"end\":56180,\"start\":56179},{\"end\":56194,\"start\":56193},{\"end\":56203,\"start\":56202},{\"end\":56205,\"start\":56204},{\"end\":56698,\"start\":56697},{\"end\":56706,\"start\":56705},{\"end\":56713,\"start\":56712},{\"end\":56719,\"start\":56718},{\"end\":56732,\"start\":56731},{\"end\":56909,\"start\":56908},{\"end\":56919,\"start\":56918},{\"end\":57201,\"start\":57200},{\"end\":57212,\"start\":57211},{\"end\":57518,\"start\":57517},{\"end\":57530,\"start\":57529},{\"end\":57758,\"start\":57757},{\"end\":57768,\"start\":57767},{\"end\":57778,\"start\":57777},{\"end\":57780,\"start\":57779},{\"end\":57791,\"start\":57790},{\"end\":58049,\"start\":58048},{\"end\":58051,\"start\":58050},{\"end\":58063,\"start\":58062},{\"end\":58410,\"start\":58409},{\"end\":58417,\"start\":58416},{\"end\":58425,\"start\":58424},{\"end\":58433,\"start\":58432},{\"end\":58747,\"start\":58746},{\"end\":58754,\"start\":58753},{\"end\":58762,\"start\":58761},{\"end\":59226,\"start\":59225},{\"end\":59228,\"start\":59227},{\"end\":59242,\"start\":59241},{\"end\":59244,\"start\":59243},{\"end\":59479,\"start\":59478},{\"end\":59487,\"start\":59486},{\"end\":59496,\"start\":59495},{\"end\":59505,\"start\":59504},{\"end\":59513,\"start\":59512},{\"end\":59886,\"start\":59885},{\"end\":59892,\"start\":59891},{\"end\":60240,\"start\":60239},{\"end\":60249,\"start\":60248},{\"end\":60619,\"start\":60618},{\"end\":60627,\"start\":60626},{\"end\":60635,\"start\":60634},{\"end\":60648,\"start\":60647}]", "bib_author_last_name": "[{\"end\":47794,\"start\":47788},{\"end\":47803,\"start\":47798},{\"end\":48005,\"start\":47998},{\"end\":48014,\"start\":48009},{\"end\":48024,\"start\":48018},{\"end\":48035,\"start\":48028},{\"end\":48046,\"start\":48039},{\"end\":48432,\"start\":48421},{\"end\":48442,\"start\":48436},{\"end\":48453,\"start\":48446},{\"end\":48464,\"start\":48459},{\"end\":48472,\"start\":48468},{\"end\":48863,\"start\":48856},{\"end\":48877,\"start\":48867},{\"end\":48889,\"start\":48881},{\"end\":49187,\"start\":49183},{\"end\":49201,\"start\":49191},{\"end\":49213,\"start\":49205},{\"end\":49223,\"start\":49217},{\"end\":49235,\"start\":49229},{\"end\":49505,\"start\":49501},{\"end\":49516,\"start\":49509},{\"end\":49522,\"start\":49520},{\"end\":49530,\"start\":49526},{\"end\":49544,\"start\":49534},{\"end\":49552,\"start\":49548},{\"end\":49899,\"start\":49895},{\"end\":49910,\"start\":49906},{\"end\":49919,\"start\":49917},{\"end\":49930,\"start\":49926},{\"end\":49940,\"start\":49937},{\"end\":49949,\"start\":49944},{\"end\":49959,\"start\":49956},{\"end\":49971,\"start\":49963},{\"end\":50336,\"start\":50331},{\"end\":50346,\"start\":50340},{\"end\":50548,\"start\":50544},{\"end\":50556,\"start\":50552},{\"end\":50566,\"start\":50560},{\"end\":50575,\"start\":50573},{\"end\":50581,\"start\":50579},{\"end\":50592,\"start\":50585},{\"end\":50977,\"start\":50968},{\"end\":50989,\"start\":50981},{\"end\":50998,\"start\":50993},{\"end\":51010,\"start\":51002},{\"end\":51328,\"start\":51323},{\"end\":51354,\"start\":51334},{\"end\":51370,\"start\":51360},{\"end\":51724,\"start\":51722},{\"end\":51732,\"start\":51728},{\"end\":51739,\"start\":51736},{\"end\":51998,\"start\":51992},{\"end\":52008,\"start\":52002},{\"end\":52017,\"start\":52012},{\"end\":52311,\"start\":52305},{\"end\":52320,\"start\":52315},{\"end\":52563,\"start\":52557},{\"end\":52575,\"start\":52570},{\"end\":52903,\"start\":52895},{\"end\":52915,\"start\":52907},{\"end\":52925,\"start\":52919},{\"end\":53301,\"start\":53291},{\"end\":53314,\"start\":53305},{\"end\":53326,\"start\":53320},{\"end\":53659,\"start\":53656},{\"end\":53667,\"start\":53663},{\"end\":53673,\"start\":53671},{\"end\":53680,\"start\":53677},{\"end\":54118,\"start\":54115},{\"end\":54134,\"start\":54122},{\"end\":54142,\"start\":54138},{\"end\":54507,\"start\":54504},{\"end\":54514,\"start\":54511},{\"end\":54522,\"start\":54518},{\"end\":54530,\"start\":54526},{\"end\":54894,\"start\":54890},{\"end\":55240,\"start\":55233},{\"end\":55249,\"start\":55244},{\"end\":55635,\"start\":55626},{\"end\":55644,\"start\":55639},{\"end\":55656,\"start\":55648},{\"end\":55663,\"start\":55660},{\"end\":55674,\"start\":55667},{\"end\":55684,\"start\":55678},{\"end\":56016,\"start\":56008},{\"end\":56103,\"start\":56097},{\"end\":56115,\"start\":56107},{\"end\":56125,\"start\":56119},{\"end\":56140,\"start\":56129},{\"end\":56150,\"start\":56144},{\"end\":56163,\"start\":56154},{\"end\":56177,\"start\":56167},{\"end\":56191,\"start\":56181},{\"end\":56200,\"start\":56195},{\"end\":56212,\"start\":56206},{\"end\":56703,\"start\":56699},{\"end\":56710,\"start\":56707},{\"end\":56716,\"start\":56714},{\"end\":56729,\"start\":56720},{\"end\":56740,\"start\":56733},{\"end\":56916,\"start\":56910},{\"end\":56927,\"start\":56920},{\"end\":57209,\"start\":57202},{\"end\":57217,\"start\":57213},{\"end\":57527,\"start\":57519},{\"end\":57540,\"start\":57531},{\"end\":57765,\"start\":57759},{\"end\":57775,\"start\":57769},{\"end\":57788,\"start\":57781},{\"end\":57794,\"start\":57792},{\"end\":58060,\"start\":58052},{\"end\":58070,\"start\":58064},{\"end\":58414,\"start\":58411},{\"end\":58422,\"start\":58418},{\"end\":58430,\"start\":58426},{\"end\":58438,\"start\":58434},{\"end\":58751,\"start\":58748},{\"end\":58759,\"start\":58755},{\"end\":58767,\"start\":58763},{\"end\":59239,\"start\":59229},{\"end\":59251,\"start\":59245},{\"end\":59484,\"start\":59480},{\"end\":59493,\"start\":59488},{\"end\":59502,\"start\":59497},{\"end\":59510,\"start\":59506},{\"end\":59522,\"start\":59514},{\"end\":59889,\"start\":59887},{\"end\":59897,\"start\":59893},{\"end\":60246,\"start\":60241},{\"end\":60256,\"start\":60250},{\"end\":60624,\"start\":60620},{\"end\":60632,\"start\":60628},{\"end\":60645,\"start\":60636},{\"end\":60659,\"start\":60649},{\"end\":47794,\"start\":47788},{\"end\":47803,\"start\":47798},{\"end\":48005,\"start\":47998},{\"end\":48014,\"start\":48009},{\"end\":48024,\"start\":48018},{\"end\":48035,\"start\":48028},{\"end\":48046,\"start\":48039},{\"end\":48432,\"start\":48421},{\"end\":48442,\"start\":48436},{\"end\":48453,\"start\":48446},{\"end\":48464,\"start\":48459},{\"end\":48472,\"start\":48468},{\"end\":48863,\"start\":48856},{\"end\":48877,\"start\":48867},{\"end\":48889,\"start\":48881},{\"end\":49187,\"start\":49183},{\"end\":49201,\"start\":49191},{\"end\":49213,\"start\":49205},{\"end\":49223,\"start\":49217},{\"end\":49235,\"start\":49229},{\"end\":49505,\"start\":49501},{\"end\":49516,\"start\":49509},{\"end\":49522,\"start\":49520},{\"end\":49530,\"start\":49526},{\"end\":49544,\"start\":49534},{\"end\":49552,\"start\":49548},{\"end\":49899,\"start\":49895},{\"end\":49910,\"start\":49906},{\"end\":49919,\"start\":49917},{\"end\":49930,\"start\":49926},{\"end\":49940,\"start\":49937},{\"end\":49949,\"start\":49944},{\"end\":49959,\"start\":49956},{\"end\":49971,\"start\":49963},{\"end\":50336,\"start\":50331},{\"end\":50346,\"start\":50340},{\"end\":50548,\"start\":50544},{\"end\":50556,\"start\":50552},{\"end\":50566,\"start\":50560},{\"end\":50575,\"start\":50573},{\"end\":50581,\"start\":50579},{\"end\":50592,\"start\":50585},{\"end\":50977,\"start\":50968},{\"end\":50989,\"start\":50981},{\"end\":50998,\"start\":50993},{\"end\":51010,\"start\":51002},{\"end\":51328,\"start\":51323},{\"end\":51354,\"start\":51334},{\"end\":51370,\"start\":51360},{\"end\":51724,\"start\":51722},{\"end\":51732,\"start\":51728},{\"end\":51739,\"start\":51736},{\"end\":51998,\"start\":51992},{\"end\":52008,\"start\":52002},{\"end\":52017,\"start\":52012},{\"end\":52311,\"start\":52305},{\"end\":52320,\"start\":52315},{\"end\":52563,\"start\":52557},{\"end\":52575,\"start\":52570},{\"end\":52903,\"start\":52895},{\"end\":52915,\"start\":52907},{\"end\":52925,\"start\":52919},{\"end\":53301,\"start\":53291},{\"end\":53314,\"start\":53305},{\"end\":53326,\"start\":53320},{\"end\":53659,\"start\":53656},{\"end\":53667,\"start\":53663},{\"end\":53673,\"start\":53671},{\"end\":53680,\"start\":53677},{\"end\":54118,\"start\":54115},{\"end\":54134,\"start\":54122},{\"end\":54142,\"start\":54138},{\"end\":54507,\"start\":54504},{\"end\":54514,\"start\":54511},{\"end\":54522,\"start\":54518},{\"end\":54530,\"start\":54526},{\"end\":54894,\"start\":54890},{\"end\":55240,\"start\":55233},{\"end\":55249,\"start\":55244},{\"end\":55635,\"start\":55626},{\"end\":55644,\"start\":55639},{\"end\":55656,\"start\":55648},{\"end\":55663,\"start\":55660},{\"end\":55674,\"start\":55667},{\"end\":55684,\"start\":55678},{\"end\":56016,\"start\":56008},{\"end\":56103,\"start\":56097},{\"end\":56115,\"start\":56107},{\"end\":56125,\"start\":56119},{\"end\":56140,\"start\":56129},{\"end\":56150,\"start\":56144},{\"end\":56163,\"start\":56154},{\"end\":56177,\"start\":56167},{\"end\":56191,\"start\":56181},{\"end\":56200,\"start\":56195},{\"end\":56212,\"start\":56206},{\"end\":56703,\"start\":56699},{\"end\":56710,\"start\":56707},{\"end\":56716,\"start\":56714},{\"end\":56729,\"start\":56720},{\"end\":56740,\"start\":56733},{\"end\":56916,\"start\":56910},{\"end\":56927,\"start\":56920},{\"end\":57209,\"start\":57202},{\"end\":57217,\"start\":57213},{\"end\":57527,\"start\":57519},{\"end\":57540,\"start\":57531},{\"end\":57765,\"start\":57759},{\"end\":57775,\"start\":57769},{\"end\":57788,\"start\":57781},{\"end\":57794,\"start\":57792},{\"end\":58060,\"start\":58052},{\"end\":58070,\"start\":58064},{\"end\":58414,\"start\":58411},{\"end\":58422,\"start\":58418},{\"end\":58430,\"start\":58426},{\"end\":58438,\"start\":58434},{\"end\":58751,\"start\":58748},{\"end\":58759,\"start\":58755},{\"end\":58767,\"start\":58763},{\"end\":59239,\"start\":59229},{\"end\":59251,\"start\":59245},{\"end\":59484,\"start\":59480},{\"end\":59493,\"start\":59488},{\"end\":59502,\"start\":59497},{\"end\":59510,\"start\":59506},{\"end\":59522,\"start\":59514},{\"end\":59889,\"start\":59887},{\"end\":59897,\"start\":59893},{\"end\":60246,\"start\":60241},{\"end\":60256,\"start\":60250},{\"end\":60624,\"start\":60620},{\"end\":60632,\"start\":60628},{\"end\":60645,\"start\":60636},{\"end\":60659,\"start\":60649}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":33361645},\"end\":47936,\"start\":47732},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":206858072},\"end\":48344,\"start\":47938},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1582990},\"end\":48789,\"start\":48346},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":12726540},\"end\":49093,\"start\":48791},{\"attributes\":{\"doi\":\"arXiv:1412.7062\",\"id\":\"b4\"},\"end\":49455,\"start\":49095},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1548631},\"end\":49825,\"start\":49457},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14987220},\"end\":50232,\"start\":49827},{\"attributes\":{\"id\":\"b7\"},\"end\":50487,\"start\":50234},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":57246310},\"end\":50890,\"start\":50489},{\"attributes\":{\"doi\":\"arXiv:1511.05175\",\"id\":\"b9\"},\"end\":51208,\"start\":50892},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":4432811},\"end\":51655,\"start\":51210},{\"attributes\":{\"id\":\"b11\"},\"end\":51929,\"start\":51657},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":13207256},\"end\":52259,\"start\":51931},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":12472196},\"end\":52496,\"start\":52261},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15628580},\"end\":52856,\"start\":52498},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":19308084},\"end\":53222,\"start\":52858},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":195908774},\"end\":53564,\"start\":53224},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":9813245},\"end\":54051,\"start\":53566},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1331231},\"end\":54457,\"start\":54053},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":459456},\"end\":54828,\"start\":54459},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":5258236},\"end\":55149,\"start\":54830},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":4067210},\"end\":55525,\"start\":55151},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7041302},\"end\":56004,\"start\":55527},{\"attributes\":{\"id\":\"b23\"},\"end\":56033,\"start\":56006},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":206770267},\"end\":56623,\"start\":56035},{\"attributes\":{\"id\":\"b25\"},\"end\":56873,\"start\":56625},{\"attributes\":{\"id\":\"b26\"},\"end\":57061,\"start\":56875},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":222982005},\"end\":57447,\"start\":57063},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b28\"},\"end\":57706,\"start\":57449},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2808203},\"end\":58018,\"start\":57708},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":16156344},\"end\":58335,\"start\":58020},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1395439},\"end\":58678,\"start\":58337},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":206592295},\"end\":59110,\"start\":58680},{\"attributes\":{\"id\":\"b33\"},\"end\":59421,\"start\":59112},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":206822288},\"end\":59815,\"start\":59423},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":16208156},\"end\":60146,\"start\":59817},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":206822389},\"end\":60554,\"start\":60148},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":304354},\"end\":60892,\"start\":60556},{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":33361645},\"end\":47936,\"start\":47732},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":206858072},\"end\":48344,\"start\":47938},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1582990},\"end\":48789,\"start\":48346},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":12726540},\"end\":49093,\"start\":48791},{\"attributes\":{\"doi\":\"arXiv:1412.7062\",\"id\":\"b4\"},\"end\":49455,\"start\":49095},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1548631},\"end\":49825,\"start\":49457},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14987220},\"end\":50232,\"start\":49827},{\"attributes\":{\"id\":\"b7\"},\"end\":50487,\"start\":50234},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":57246310},\"end\":50890,\"start\":50489},{\"attributes\":{\"doi\":\"arXiv:1511.05175\",\"id\":\"b9\"},\"end\":51208,\"start\":50892},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":4432811},\"end\":51655,\"start\":51210},{\"attributes\":{\"id\":\"b11\"},\"end\":51929,\"start\":51657},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":13207256},\"end\":52259,\"start\":51931},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":12472196},\"end\":52496,\"start\":52261},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15628580},\"end\":52856,\"start\":52498},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":19308084},\"end\":53222,\"start\":52858},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":195908774},\"end\":53564,\"start\":53224},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":9813245},\"end\":54051,\"start\":53566},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1331231},\"end\":54457,\"start\":54053},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":459456},\"end\":54828,\"start\":54459},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":5258236},\"end\":55149,\"start\":54830},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":4067210},\"end\":55525,\"start\":55151},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7041302},\"end\":56004,\"start\":55527},{\"attributes\":{\"id\":\"b23\"},\"end\":56033,\"start\":56006},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":206770267},\"end\":56623,\"start\":56035},{\"attributes\":{\"id\":\"b25\"},\"end\":56873,\"start\":56625},{\"attributes\":{\"id\":\"b26\"},\"end\":57061,\"start\":56875},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":222982005},\"end\":57447,\"start\":57063},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b28\"},\"end\":57706,\"start\":57449},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2808203},\"end\":58018,\"start\":57708},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":16156344},\"end\":58335,\"start\":58020},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1395439},\"end\":58678,\"start\":58337},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":206592295},\"end\":59110,\"start\":58680},{\"attributes\":{\"id\":\"b33\"},\"end\":59421,\"start\":59112},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":206822288},\"end\":59815,\"start\":59423},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":16208156},\"end\":60146,\"start\":59817},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":206822389},\"end\":60554,\"start\":60148},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":304354},\"end\":60892,\"start\":60556}]", "bib_title": "[{\"end\":47784,\"start\":47732},{\"end\":47994,\"start\":47938},{\"end\":48417,\"start\":48346},{\"end\":48852,\"start\":48791},{\"end\":49497,\"start\":49457},{\"end\":49888,\"start\":49827},{\"end\":50540,\"start\":50489},{\"end\":51317,\"start\":51210},{\"end\":51718,\"start\":51657},{\"end\":51988,\"start\":51931},{\"end\":52301,\"start\":52261},{\"end\":52553,\"start\":52498},{\"end\":52891,\"start\":52858},{\"end\":53287,\"start\":53224},{\"end\":53652,\"start\":53566},{\"end\":54108,\"start\":54053},{\"end\":54500,\"start\":54459},{\"end\":54884,\"start\":54830},{\"end\":55229,\"start\":55151},{\"end\":55622,\"start\":55527},{\"end\":56093,\"start\":56035},{\"end\":57198,\"start\":57063},{\"end\":57755,\"start\":57708},{\"end\":58046,\"start\":58020},{\"end\":58407,\"start\":58337},{\"end\":58744,\"start\":58680},{\"end\":59476,\"start\":59423},{\"end\":59883,\"start\":59817},{\"end\":60237,\"start\":60148},{\"end\":60616,\"start\":60556},{\"end\":47784,\"start\":47732},{\"end\":47994,\"start\":47938},{\"end\":48417,\"start\":48346},{\"end\":48852,\"start\":48791},{\"end\":49497,\"start\":49457},{\"end\":49888,\"start\":49827},{\"end\":50540,\"start\":50489},{\"end\":51317,\"start\":51210},{\"end\":51718,\"start\":51657},{\"end\":51988,\"start\":51931},{\"end\":52301,\"start\":52261},{\"end\":52553,\"start\":52498},{\"end\":52891,\"start\":52858},{\"end\":53287,\"start\":53224},{\"end\":53652,\"start\":53566},{\"end\":54108,\"start\":54053},{\"end\":54500,\"start\":54459},{\"end\":54884,\"start\":54830},{\"end\":55229,\"start\":55151},{\"end\":55622,\"start\":55527},{\"end\":56093,\"start\":56035},{\"end\":57198,\"start\":57063},{\"end\":57755,\"start\":57708},{\"end\":58046,\"start\":58020},{\"end\":58407,\"start\":58337},{\"end\":58744,\"start\":58680},{\"end\":59476,\"start\":59423},{\"end\":59883,\"start\":59817},{\"end\":60237,\"start\":60148},{\"end\":60616,\"start\":60556}]", "bib_author": "[{\"end\":47796,\"start\":47786},{\"end\":47805,\"start\":47796},{\"end\":48007,\"start\":47996},{\"end\":48016,\"start\":48007},{\"end\":48026,\"start\":48016},{\"end\":48037,\"start\":48026},{\"end\":48048,\"start\":48037},{\"end\":48434,\"start\":48419},{\"end\":48444,\"start\":48434},{\"end\":48455,\"start\":48444},{\"end\":48466,\"start\":48455},{\"end\":48474,\"start\":48466},{\"end\":48865,\"start\":48854},{\"end\":48879,\"start\":48865},{\"end\":48891,\"start\":48879},{\"end\":49189,\"start\":49178},{\"end\":49203,\"start\":49189},{\"end\":49215,\"start\":49203},{\"end\":49225,\"start\":49215},{\"end\":49237,\"start\":49225},{\"end\":49507,\"start\":49499},{\"end\":49518,\"start\":49507},{\"end\":49524,\"start\":49518},{\"end\":49532,\"start\":49524},{\"end\":49546,\"start\":49532},{\"end\":49554,\"start\":49546},{\"end\":49901,\"start\":49890},{\"end\":49912,\"start\":49901},{\"end\":49921,\"start\":49912},{\"end\":49932,\"start\":49921},{\"end\":49942,\"start\":49932},{\"end\":49951,\"start\":49942},{\"end\":49961,\"start\":49951},{\"end\":49973,\"start\":49961},{\"end\":50338,\"start\":50329},{\"end\":50348,\"start\":50338},{\"end\":50550,\"start\":50542},{\"end\":50558,\"start\":50550},{\"end\":50568,\"start\":50558},{\"end\":50577,\"start\":50568},{\"end\":50583,\"start\":50577},{\"end\":50594,\"start\":50583},{\"end\":50979,\"start\":50966},{\"end\":50991,\"start\":50979},{\"end\":51000,\"start\":50991},{\"end\":51012,\"start\":51000},{\"end\":51330,\"start\":51319},{\"end\":51356,\"start\":51330},{\"end\":51372,\"start\":51356},{\"end\":51726,\"start\":51720},{\"end\":51734,\"start\":51726},{\"end\":51741,\"start\":51734},{\"end\":52000,\"start\":51990},{\"end\":52010,\"start\":52000},{\"end\":52019,\"start\":52010},{\"end\":52313,\"start\":52303},{\"end\":52322,\"start\":52313},{\"end\":52568,\"start\":52555},{\"end\":52580,\"start\":52568},{\"end\":52905,\"start\":52893},{\"end\":52917,\"start\":52905},{\"end\":52927,\"start\":52917},{\"end\":53303,\"start\":53289},{\"end\":53316,\"start\":53303},{\"end\":53328,\"start\":53316},{\"end\":53661,\"start\":53654},{\"end\":53669,\"start\":53661},{\"end\":53675,\"start\":53669},{\"end\":53682,\"start\":53675},{\"end\":54120,\"start\":54110},{\"end\":54136,\"start\":54120},{\"end\":54144,\"start\":54136},{\"end\":54509,\"start\":54502},{\"end\":54516,\"start\":54509},{\"end\":54524,\"start\":54516},{\"end\":54532,\"start\":54524},{\"end\":54896,\"start\":54886},{\"end\":55242,\"start\":55231},{\"end\":55251,\"start\":55242},{\"end\":55637,\"start\":55624},{\"end\":55646,\"start\":55637},{\"end\":55658,\"start\":55646},{\"end\":55665,\"start\":55658},{\"end\":55676,\"start\":55665},{\"end\":55686,\"start\":55676},{\"end\":56018,\"start\":56008},{\"end\":56105,\"start\":56095},{\"end\":56117,\"start\":56105},{\"end\":56127,\"start\":56117},{\"end\":56142,\"start\":56127},{\"end\":56152,\"start\":56142},{\"end\":56165,\"start\":56152},{\"end\":56179,\"start\":56165},{\"end\":56193,\"start\":56179},{\"end\":56202,\"start\":56193},{\"end\":56214,\"start\":56202},{\"end\":56705,\"start\":56697},{\"end\":56712,\"start\":56705},{\"end\":56718,\"start\":56712},{\"end\":56731,\"start\":56718},{\"end\":56742,\"start\":56731},{\"end\":56918,\"start\":56908},{\"end\":56929,\"start\":56918},{\"end\":57211,\"start\":57200},{\"end\":57219,\"start\":57211},{\"end\":57529,\"start\":57517},{\"end\":57542,\"start\":57529},{\"end\":57767,\"start\":57757},{\"end\":57777,\"start\":57767},{\"end\":57790,\"start\":57777},{\"end\":57796,\"start\":57790},{\"end\":58062,\"start\":58048},{\"end\":58072,\"start\":58062},{\"end\":58416,\"start\":58409},{\"end\":58424,\"start\":58416},{\"end\":58432,\"start\":58424},{\"end\":58440,\"start\":58432},{\"end\":58753,\"start\":58746},{\"end\":58761,\"start\":58753},{\"end\":58769,\"start\":58761},{\"end\":59241,\"start\":59225},{\"end\":59253,\"start\":59241},{\"end\":59486,\"start\":59478},{\"end\":59495,\"start\":59486},{\"end\":59504,\"start\":59495},{\"end\":59512,\"start\":59504},{\"end\":59524,\"start\":59512},{\"end\":59891,\"start\":59885},{\"end\":59899,\"start\":59891},{\"end\":60248,\"start\":60239},{\"end\":60258,\"start\":60248},{\"end\":60626,\"start\":60618},{\"end\":60634,\"start\":60626},{\"end\":60647,\"start\":60634},{\"end\":60661,\"start\":60647},{\"end\":47796,\"start\":47786},{\"end\":47805,\"start\":47796},{\"end\":48007,\"start\":47996},{\"end\":48016,\"start\":48007},{\"end\":48026,\"start\":48016},{\"end\":48037,\"start\":48026},{\"end\":48048,\"start\":48037},{\"end\":48434,\"start\":48419},{\"end\":48444,\"start\":48434},{\"end\":48455,\"start\":48444},{\"end\":48466,\"start\":48455},{\"end\":48474,\"start\":48466},{\"end\":48865,\"start\":48854},{\"end\":48879,\"start\":48865},{\"end\":48891,\"start\":48879},{\"end\":49189,\"start\":49178},{\"end\":49203,\"start\":49189},{\"end\":49215,\"start\":49203},{\"end\":49225,\"start\":49215},{\"end\":49237,\"start\":49225},{\"end\":49507,\"start\":49499},{\"end\":49518,\"start\":49507},{\"end\":49524,\"start\":49518},{\"end\":49532,\"start\":49524},{\"end\":49546,\"start\":49532},{\"end\":49554,\"start\":49546},{\"end\":49901,\"start\":49890},{\"end\":49912,\"start\":49901},{\"end\":49921,\"start\":49912},{\"end\":49932,\"start\":49921},{\"end\":49942,\"start\":49932},{\"end\":49951,\"start\":49942},{\"end\":49961,\"start\":49951},{\"end\":49973,\"start\":49961},{\"end\":50338,\"start\":50329},{\"end\":50348,\"start\":50338},{\"end\":50550,\"start\":50542},{\"end\":50558,\"start\":50550},{\"end\":50568,\"start\":50558},{\"end\":50577,\"start\":50568},{\"end\":50583,\"start\":50577},{\"end\":50594,\"start\":50583},{\"end\":50979,\"start\":50966},{\"end\":50991,\"start\":50979},{\"end\":51000,\"start\":50991},{\"end\":51012,\"start\":51000},{\"end\":51330,\"start\":51319},{\"end\":51356,\"start\":51330},{\"end\":51372,\"start\":51356},{\"end\":51726,\"start\":51720},{\"end\":51734,\"start\":51726},{\"end\":51741,\"start\":51734},{\"end\":52000,\"start\":51990},{\"end\":52010,\"start\":52000},{\"end\":52019,\"start\":52010},{\"end\":52313,\"start\":52303},{\"end\":52322,\"start\":52313},{\"end\":52568,\"start\":52555},{\"end\":52580,\"start\":52568},{\"end\":52905,\"start\":52893},{\"end\":52917,\"start\":52905},{\"end\":52927,\"start\":52917},{\"end\":53303,\"start\":53289},{\"end\":53316,\"start\":53303},{\"end\":53328,\"start\":53316},{\"end\":53661,\"start\":53654},{\"end\":53669,\"start\":53661},{\"end\":53675,\"start\":53669},{\"end\":53682,\"start\":53675},{\"end\":54120,\"start\":54110},{\"end\":54136,\"start\":54120},{\"end\":54144,\"start\":54136},{\"end\":54509,\"start\":54502},{\"end\":54516,\"start\":54509},{\"end\":54524,\"start\":54516},{\"end\":54532,\"start\":54524},{\"end\":54896,\"start\":54886},{\"end\":55242,\"start\":55231},{\"end\":55251,\"start\":55242},{\"end\":55637,\"start\":55624},{\"end\":55646,\"start\":55637},{\"end\":55658,\"start\":55646},{\"end\":55665,\"start\":55658},{\"end\":55676,\"start\":55665},{\"end\":55686,\"start\":55676},{\"end\":56018,\"start\":56008},{\"end\":56105,\"start\":56095},{\"end\":56117,\"start\":56105},{\"end\":56127,\"start\":56117},{\"end\":56142,\"start\":56127},{\"end\":56152,\"start\":56142},{\"end\":56165,\"start\":56152},{\"end\":56179,\"start\":56165},{\"end\":56193,\"start\":56179},{\"end\":56202,\"start\":56193},{\"end\":56214,\"start\":56202},{\"end\":56705,\"start\":56697},{\"end\":56712,\"start\":56705},{\"end\":56718,\"start\":56712},{\"end\":56731,\"start\":56718},{\"end\":56742,\"start\":56731},{\"end\":56918,\"start\":56908},{\"end\":56929,\"start\":56918},{\"end\":57211,\"start\":57200},{\"end\":57219,\"start\":57211},{\"end\":57529,\"start\":57517},{\"end\":57542,\"start\":57529},{\"end\":57767,\"start\":57757},{\"end\":57777,\"start\":57767},{\"end\":57790,\"start\":57777},{\"end\":57796,\"start\":57790},{\"end\":58062,\"start\":58048},{\"end\":58072,\"start\":58062},{\"end\":58416,\"start\":58409},{\"end\":58424,\"start\":58416},{\"end\":58432,\"start\":58424},{\"end\":58440,\"start\":58432},{\"end\":58753,\"start\":58746},{\"end\":58761,\"start\":58753},{\"end\":58769,\"start\":58761},{\"end\":59241,\"start\":59225},{\"end\":59253,\"start\":59241},{\"end\":59486,\"start\":59478},{\"end\":59495,\"start\":59486},{\"end\":59504,\"start\":59495},{\"end\":59512,\"start\":59504},{\"end\":59524,\"start\":59512},{\"end\":59891,\"start\":59885},{\"end\":59899,\"start\":59891},{\"end\":60248,\"start\":60239},{\"end\":60258,\"start\":60248},{\"end\":60626,\"start\":60618},{\"end\":60634,\"start\":60626},{\"end\":60647,\"start\":60634},{\"end\":60661,\"start\":60647}]", "bib_venue": "[{\"end\":52700,\"start\":52644},{\"end\":53046,\"start\":52995},{\"end\":53823,\"start\":53761},{\"end\":54265,\"start\":54213},{\"end\":54653,\"start\":54601},{\"end\":56335,\"start\":56283},{\"end\":58910,\"start\":58848},{\"end\":52700,\"start\":52644},{\"end\":53046,\"start\":52995},{\"end\":53823,\"start\":53761},{\"end\":54265,\"start\":54213},{\"end\":54653,\"start\":54601},{\"end\":56335,\"start\":56283},{\"end\":58910,\"start\":58848},{\"end\":47820,\"start\":47805},{\"end\":48118,\"start\":48048},{\"end\":48544,\"start\":48474},{\"end\":48916,\"start\":48891},{\"end\":49176,\"start\":49095},{\"end\":49591,\"start\":49554},{\"end\":50008,\"start\":49973},{\"end\":50327,\"start\":50234},{\"end\":50633,\"start\":50594},{\"end\":50964,\"start\":50892},{\"end\":51414,\"start\":51372},{\"end\":51785,\"start\":51741},{\"end\":52073,\"start\":52019},{\"end\":52370,\"start\":52322},{\"end\":52642,\"start\":52580},{\"end\":52993,\"start\":52927},{\"end\":53377,\"start\":53328},{\"end\":53759,\"start\":53682},{\"end\":54211,\"start\":54144},{\"end\":54599,\"start\":54532},{\"end\":54975,\"start\":54896},{\"end\":55316,\"start\":55251},{\"end\":55750,\"start\":55686},{\"end\":56281,\"start\":56214},{\"end\":56695,\"start\":56625},{\"end\":56906,\"start\":56875},{\"end\":57239,\"start\":57219},{\"end\":57515,\"start\":57449},{\"end\":57845,\"start\":57796},{\"end\":58137,\"start\":58072},{\"end\":58489,\"start\":58440},{\"end\":58846,\"start\":58769},{\"end\":59223,\"start\":59112},{\"end\":59598,\"start\":59524},{\"end\":59931,\"start\":59899},{\"end\":60332,\"start\":60258},{\"end\":60707,\"start\":60661},{\"end\":47820,\"start\":47805},{\"end\":48118,\"start\":48048},{\"end\":48544,\"start\":48474},{\"end\":48916,\"start\":48891},{\"end\":49176,\"start\":49095},{\"end\":49591,\"start\":49554},{\"end\":50008,\"start\":49973},{\"end\":50327,\"start\":50234},{\"end\":50633,\"start\":50594},{\"end\":50964,\"start\":50892},{\"end\":51414,\"start\":51372},{\"end\":51785,\"start\":51741},{\"end\":52073,\"start\":52019},{\"end\":52370,\"start\":52322},{\"end\":52642,\"start\":52580},{\"end\":52993,\"start\":52927},{\"end\":53377,\"start\":53328},{\"end\":53759,\"start\":53682},{\"end\":54211,\"start\":54144},{\"end\":54599,\"start\":54532},{\"end\":54975,\"start\":54896},{\"end\":55316,\"start\":55251},{\"end\":55750,\"start\":55686},{\"end\":56281,\"start\":56214},{\"end\":56695,\"start\":56625},{\"end\":56906,\"start\":56875},{\"end\":57239,\"start\":57219},{\"end\":57515,\"start\":57449},{\"end\":57845,\"start\":57796},{\"end\":58137,\"start\":58072},{\"end\":58489,\"start\":58440},{\"end\":58846,\"start\":58769},{\"end\":59223,\"start\":59112},{\"end\":59598,\"start\":59524},{\"end\":59931,\"start\":59899},{\"end\":60332,\"start\":60258},{\"end\":60707,\"start\":60661}]"}}}, "year": 2023, "month": 12, "day": 17}