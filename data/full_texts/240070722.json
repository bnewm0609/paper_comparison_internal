{"id": 240070722, "updated": "2023-12-01 12:16:11.734", "metadata": {"title": "UltraGCN: Ultra Simplification of Graph Convolutional Networks for Recommendation", "authors": "[{\"first\":\"Kelong\",\"last\":\"Mao\",\"middle\":[]},{\"first\":\"Jieming\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Xi\",\"last\":\"Xiao\",\"middle\":[]},{\"first\":\"Biao\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Zhaowei\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Xiuqiang\",\"last\":\"He\",\"middle\":[]}]", "venue": "ArXiv", "journal": "Proceedings of the 30th ACM International Conference on Information & Knowledge Management", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "With the recent success of graph convolutional networks (GCNs), they have been widely applied for recommendation, and achieved impressive performance gains. The core of GCNs lies in its message passing mechanism to aggregate neighborhood information. However, we observed that message passing largely slows down the convergence of GCNs during training, especially for large-scale recommender systems, which hinders their wide adoption. LightGCN makes an early attempt to simplify GCNs for collaborative filtering by omitting feature transformations and nonlinear activations. In this paper, we take one step further to propose an ultra-simplified formulation of GCNs (dubbed UltraGCN), which skips infinite layers of message passing for efficient recommendation. Instead of explicit message passing, UltraGCN resorts to directly approximate the limit of infinite-layer graph convolutions via a constraint loss. Meanwhile, UltraGCN allows for more appropriate edge weight assignments and flexible adjustment of the relative importances among different types of relationships. This finally yields a simple yet effective UltraGCN model, which is easy to implement and efficient to train. Experimental results on four benchmark datasets show that UltraGCN not only outperforms the state-of-the-art GCN models but also achieves more than 10x speedup over LightGCN. Our source code will be available at https://reczoo.github.io/UltraGCN.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2110-15114", "doi": "10.1145/3459637.3482291"}}, "content": {"source": {"pdf_hash": "685ccdc883fa5a6c64267175cd7a20b891e68fa6", "pdf_src": "ArXiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2110.15114v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": null, "status": "CLOSED"}}, "grobid": {"id": "c6514f8e972cd43bc680d1cf90eafd3e1157b01a", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/685ccdc883fa5a6c64267175cd7a20b891e68fa6.txt", "contents": "\nUltraGCN: Ultra Simplification of Graph Convolutional Networks for Recommendation\n30 Nov 2023\n\nKelong Mao \nJieming Zhu jiemingzhu@ieee.org \nXi Xiao xiaox@sz.tsinghua.edu.cn \nBiao Lu lubiao4@huawei.com \nZhaowei Wang wangzhaowei3@huawei.com \n\nGaoling School\nAI Renmin University of China\n\n\n\nHuawei Noah's Ark Lab Shenzhen\nChina\n\n\nTsinghua University Peng Cheng Laboratory\n\n\n\nHuawei Noah's Ark Lab\nChina\n\n\nHuawei Noah's Ark Lab\n\n\n\nTsinghua University\n\n\nUltraGCN: Ultra Simplification of Graph Convolutional Networks for Recommendation\n30 Nov 2023853FC763E45793DD7A97968F12A820ADarXiv:2110.15114v2[cs.IR]CCS CONCEPTSInformation systems \u2192 Recommender systemsCollaborative filtering Recommender systemscollaborative filteringgraph convolutional networks\nWith the recent success of graph convolutional networks (GCNs), they have been widely applied for recommendation, and achieved impressive performance gains.The core of GCNs lies in its message passing mechanism to aggregate neighborhood information.However, we observed that message passing largely slows down the convergence of GCNs during training, especially for large-scale recommender systems, which hinders their wide adoption.LightGCN makes an early attempt to simplify GCNs for collaborative filtering by omitting feature transformations and nonlinear activations.In this paper, we take one step further to propose an ultra-simplified formulation of GCNs (dubbed UltraGCN), which skips infinite layers of message passing for efficient recommendation.Instead of explicit message passing, UltraGCN resorts to directly approximate the limit of infinite-layer graph convolutions via a constraint loss.Meanwhile, UltraGCN allows for more appropriate edge weight assignments and flexible adjustment of the relative importances among different types of relationships.This finally yields a simple yet effective UltraGCN model, which is easy to implement and efficient to train.Experimental results on four benchmark datasets show that UltraGCN not only outperforms the state-of-the-art GCN models but also achieves more than 10x speedup over LightGCN.Our source code will be available at https://reczoo.github.io/UltraGCN.\n\nINTRODUCTION\n\nNowadays, personalized recommendation has become a prevalent way to help users find information of their interests in various applications, such as e-commerce, online news, and social media.The core of recommendation is to precisely match a user's preference with candidate items.Collaborative filtering (CF) [11], as a fundamental recommendation task, has been widely studied in both academia and industry.A common paradigm of CF is to learn vector representations (i.e., embeddings) of users and items from historical interaction data and then perform top-k recommendation based on the pairwise similarity between user and item embeddings.\n\nAs the interaction data can be naturally modelled as graphs, such as user-item bipartite graph and item-item co-occurrence graph, recent studies [10,13,24,27] opt for powerful graph convolutional/neural networks (GCNs, or GNNs in general) to learn user and item node representations.These GCN-based models are capable of exploiting higher-order connectivity between users and items, and therefore have achieved impressive performance gains for recommendation.PinSage [31] and M2GRL [26] are two successful use cases in industrial applications.\n\nDespite the promising results obtained, we argue that current model designs are heavy and burdensome.In order to capture higher-order collaborative signals and better model the interaction process between users and items, current GNN-based CF models [1,24,27,32] tend to seek for more and more sophisticated network encoders.However, we observed that these GCN-based models are hard to train with large graphs, which hinders their wide adoption in industry.Industrial recommender systems usually involve massive graphs due to the large numbers of users and items.This brings efficiency and scalability challenges for model designs.Towards this end, some research efforts [4,10,17] have been made to simplify the design of GCN-based CF models, mainly by removing feature transformations and non-linear activations that are not necessary for CF.These simplified models not only obtain much better performance than those complex ones, but also brings some benefits on training efficiency.\n\nInspired by these pioneer studies, we performed further empirical analysis on the training process of GCN-based models and found that message passing (i.e., neighborhood aggregation) on a large graph is usually time-consuming for CF.In particular, stacking multiple layers of message passing could lead to the slow convergence of GCN-based models on CF tasks.Although the aforementioned models such as LightGCN [10] have already been simplified for training, the message passing operations still dominate their training.For example, in our experiments, three-layer LightGCN takes more than 700 epochs to converge to its best result on the Amazon-Books dataset [9], which would be unacceptable in an industrial setting.How to improve the efficiency of GCN models yet retain their effectiveness on recommendation is still an open problem.\n\nTo tackle this challenge, in this work, we question the necessity of explicit message passing layers in CF, and finally propose an ultra-simplified form of GCNs (dubbed UltraGCN) without message passing for efficient recommendation.More specifically, we analyzed the message passing formula of LightGCN and identified three critical limitations: 1) The weights assigned on edges during message passing are counter-intuitive, which may not be appropriate for CF. 2) The propagation process recursively combines different types of relationship pairs (including user-item pairs, item-item pairs, and user-user pairs) into the model, but fails to capture their varying importance.This may also introduce noisy and uninformative relationships that confuse the model training.\n\n3) The over-smoothing issue limits the use of too many layers of message passing in LightGCN.Therefore, instead of performing explicit message passing, we seek to directly approximate the limit of infinite-layer graph convolutions via a constraint loss, which leads to the ultra-simplified GCN model, UltraGCN.The loss-based design of UltraGCN is very flexible, allowing us to manually adjust the relative importances of different types of relationships and also avoid the over-smoothing problem by negative sampling.This finally yields a simple yet effective UltraGCN model, which is easy to implement and efficient to train.Furthermore, we show that Ultra-GCN achieves significant improvements over the state-of-the-art CF models.For instance, UltraGCN attains up to 76.6% improvement in NDCG@20 and more than 10x speedup in training over LightGCN on the Amazon-Books dataset.\n\nIn summary, this work makes the following main contributions:\n\n\u2022 We empirically analyze the training inefficiency of LightGCN and further attribute its cause to the critical limitations of the message passing mechanism.\u2022 We propose an ultra simplified formulation of GCN, namely UltraGCN, which skips infinite layers of explicit message passing for efficient recommendation.\u2022 Extensive experiments have been conducted on four benchmark datasets to show the effectiveness and efficiency of UltraGCN.\n\n\nMOTIVATION\n\nIn this section, we revisit the GCN and LightGCN models, and further identify the limitations resulted from the inherent message passing mechanism, which also justify the motivation of our work.\n\n\nRevisiting GCN and LightGCN\n\nGCN [14] is a representative model of graph neural networks that applies message passing to aggregate neighborhood information.\n\nThe message passing layer with self-loops is defined as follows:\n\ud835\udc38 (\ud835\udc59+1) = \ud835\udf0e D \u2212 1 2 \u00c2 D \u2212 1 2 \ud835\udc38 (\ud835\udc59 ) \ud835\udc4a (\ud835\udc59 )(1)\nwith \u00c2 =  +  and D =  +  ., ,  are the adjacency matrix, the diagonal node degree matrix, and the identity matrix, respectively.\n\n is used to integrate self-loop connections on nodes. ( ) and  ( )  denote the representation matrix and the weight matrix for the -th layer. (\u2022) is a non-linear activation function (e.g., ReLU).Despite the wide success of GCN in graph learning, several recent studies [4,10,17,29] found that simplifying GCN appropriately can further boost the performance on CF tasks.LightGCN [10] is one such simplified GCN model that removes feature transformations (i.e.,  ( ) ) and non-linear activations (i.e., ).Its message passing layer can thus be expressed as follows:\n\ud835\udc38 (\ud835\udc59+1) = ( D \u2212 1 2 \u00c2 D \u2212 1 2 )\ud835\udc38 (\ud835\udc59 )(2)\nIt is worth noting that although LightGCN also removes self-loop connections on nodes, its layer combination operation has a similar effect to self-loops used in Equation 2, becauase both of them output a weighted sum of the embeddings propagated at each layer as the final output representation.Given self-loop connections, we can rewrite the message passing operations for user  and item  as follows:\n\ud835\udc52 (\ud835\udc59+1) \ud835\udc62 = 1 \ud835\udc51 \ud835\udc62 + 1 \ud835\udc52 (\ud835\udc59 ) \ud835\udc62 + \u2211\ufe01 \ud835\udc58 \u2208 N (\ud835\udc62 ) 1 \u221a \ud835\udc51 \ud835\udc62 + 1 \u221a\ufe01 \ud835\udc51 \ud835\udc58 + 1 \ud835\udc52 (\ud835\udc59 ) \ud835\udc58 ,(3)\ud835\udc52 (\ud835\udc59+1) \ud835\udc56 = 1 \ud835\udc51 \ud835\udc56 + 1 \ud835\udc52 (\ud835\udc59 ) \ud835\udc56 + \u2211\ufe01 \ud835\udc63 \u2208 N (\ud835\udc56 ) 1 \u221a \ud835\udc51 \ud835\udc63 + 1 \u221a \ud835\udc51 \ud835\udc56 + 1 \ud835\udc52 (\ud835\udc59 ) \ud835\udc63(4)\nwhere \n\n( )\n\n\n\ud835\udc62 and \ud835\udc52 (\ud835\udc59 )\n\n denote the embeddings of user  and item  at layer .N () and N () represent their neighbor node sets, respectively.  denotes the original degree of the node .\n\nAs shown in the left part of Figure 1, LightGCN performs a stack of message passing layers to obtain the embeddings and finally uses their dot product for training.\n\n\nLimitations of Message Passing\n\nWe argue that such message passing layers have potential limitations that hinder the effective and efficient training of GCN-based models in recommendation tasks.To illustrate it, we take the -th layer message passing of LightGCN in Equation 3 and 4 for example.Note that  and  denote users while  and  denote items.LightGCN takes the dot product of the two embedding as the final logit to capture the preference of user  on item .Thus we obtain:\n\ud835\udc52 (\ud835\udc59+1) \ud835\udc62 \u2022 \ud835\udc52 (\ud835\udc59+1) \ud835\udc56 = \ud835\udefc \ud835\udc62\ud835\udc56 (\ud835\udc52 (\ud835\udc59 ) \ud835\udc62 \u2022 \ud835\udc52 (\ud835\udc59 ) \ud835\udc56 ) + \u2211\ufe01 \ud835\udc58 \u2208 N (\ud835\udc62 ) \ud835\udefc \ud835\udc56\ud835\udc58 (\ud835\udc52 (\ud835\udc59 ) \ud835\udc56 \u2022 \ud835\udc52 (\ud835\udc59 ) \ud835\udc58 ) + \u2211\ufe01 \ud835\udc63 \u2208 N (\ud835\udc56 ) \ud835\udefc \ud835\udc62\ud835\udc63 (\ud835\udc52 (\ud835\udc59 ) \ud835\udc62 \u2022 \ud835\udc52 (\ud835\udc59 ) \ud835\udc63 ) + \u2211\ufe01 \ud835\udc58 \u2208 N (\ud835\udc62 ) \u2211\ufe01 \ud835\udc63 \u2208 N (\ud835\udc56 ) \ud835\udefc \ud835\udc58\ud835\udc63 (\ud835\udc52 (\ud835\udc59 ) \ud835\udc58 \u2022 \ud835\udc52 (\ud835\udc59 ) \ud835\udc63 ) ,(5)\nwhere   ,   ,   , and   can be derived as follows:\n\ud835\udefc \ud835\udc62\ud835\udc56 = 1 (\ud835\udc51 \ud835\udc62 + 1)(\ud835\udc51 \ud835\udc56 + 1) , \ud835\udefc \ud835\udc56\ud835\udc58 = 1 \u221a \ud835\udc51 \ud835\udc62 + 1 \u221a\ufe01 \ud835\udc51 \ud835\udc58 + 1(\ud835\udc51 \ud835\udc56 + 1) , \ud835\udefc \ud835\udc62\ud835\udc63 = 1 \u221a \ud835\udc51 \ud835\udc63 + 1 \u221a \ud835\udc51 \ud835\udc56 + 1(\ud835\udc51 \ud835\udc62 + 1) , \ud835\udefc \ud835\udc58\ud835\udc63 = 1 \u221a \ud835\udc51 \ud835\udc62 + 1 \u221a\ufe01 \ud835\udc51 \ud835\udc58 + 1 \u221a \ud835\udc51 \ud835\udc63 + 1 \u221a \ud835\udc51 \ud835\udc56 + 1\nTherefore, we can observe that multiple different types of collaborative signals, including user-item relationships (- and -), item-item relationships (-), and user-user relationships (-), are captured when training GCN-based models with message passing layers.This also reveals why GCN-based models are effective for CF.However, we found that the edge weights assigned on various types of relationships are not justified to be appropriate for CF tasks.Based on our empirical analysis, we identify three critical limitations of the message passing layers in GCN-based models:\n\n\u2022 Limitation I: The weight   is used to model the item-item relationships.However, given the user , the factors of item  and item  are asymmetric ( 1\n\u221a \ud835\udc51 \ud835\udc58 +1\nfor item  while 1\n\ud835\udc51 \ud835\udc56 +1\nfor item ).This is not reasonable since it is counter-intuitive to treat the item  and item  unequally.Similarly,   that models the user-user relationships also suffer this issue.Such unreasonable weight assignments may mislead the model training and finally result in sub-optimal performance.[10].We partially attribute it to the over-smoothing problem of message passing.As graph convolution is a special form of Laplacian smoothing [16], performing too many layers of message passing will make the nodes with the same degrees tend to have exactly the same embeddings.According to Theorem 1 in [5], we can derive the infinite powers of message passing which take the following limit:\nlim \ud835\udc59\u2192\u221e ( D \u2212 1 2 \u00c2 D \u2212 1 2 ) \ud835\udc59 \ud835\udc56,\ud835\udc57 = \u221a\ufe01 (\ud835\udc51 \ud835\udc56 + 1)(\ud835\udc51 \ud835\udc57 + 1) 2\ud835\udc5a + \ud835\udc5b(6)\nwhere  and  are the total numbers of nodes and edges in the graph, respectively.\n\nThe above limitations of message passing motivate our work.We question the necessity of explicit message passing layers in CF and further propose an ultra-simplified formulation of GCN, dubbed UltraGCN.\n\n\nUltraGCN\n\nIn this section, we present our ultra-simplified UltraGCN model and demonstrate how to incorporate different types of relationships in a flexible manner.We also elaborate on how it overcomes the above limitations and analyze its connections to other related models.\n\n\nLearning on User-Item Graph\n\nDue to the limitations of message passing, in this work, we take one step forward to question the necessity of explicit message passing in CF.Considering that the limit of infinite powers of message passing exists as shown in Equation 6, we wonder whether it is possible to skip the infinite-layer message passing yet approximate the convergence state reached.\n\nAfter repeating infinite layers of message passing, we express the final convergence condition as follows:\n\ud835\udc52 \ud835\udc62 = lim \ud835\udc59\u2192\u221e \ud835\udc52 (\ud835\udc59+1) \ud835\udc62 = lim \ud835\udc59\u2192\u221e \ud835\udc52 (\ud835\udc59 ) \ud835\udc62 (7)\nThat is, the representations of the last two layers keep unchanged, since the vector generated from neighborhood aggregation equals to the node representation itself.We use   (or   ) to denote the final converged representation of user  (or item ).Then, Equation 3 can be rewritten as:\n\ud835\udc52 \ud835\udc62 = 1 \ud835\udc51 \ud835\udc62 + 1 \ud835\udc52 \ud835\udc62 + \u2211\ufe01 \ud835\udc56 \u2208 N (\ud835\udc62 ) 1 \u221a \ud835\udc51 \ud835\udc62 + 1 \u221a \ud835\udc51 \ud835\udc56 + 1 \ud835\udc52 \ud835\udc56(8)\nAfter some simplifications, we derive the following convergence state:\n\ud835\udc52 \ud835\udc62 = \u2211\ufe01 \ud835\udc56 \u2208 N (\ud835\udc62 ) \ud835\udefd \ud835\udc62,\ud835\udc56 \ud835\udc52 \ud835\udc56 , \ud835\udefd \ud835\udc62,\ud835\udc56 = 1 \ud835\udc51 \ud835\udc62 \u221a\ufe04 \ud835\udc51 \ud835\udc62 + 1 \ud835\udc51 \ud835\udc56 + 1(9)\nIn other words, if Equation 9 is satisfied for each node, it reaches the convergence state of message passing.\n\nInstead of performing explicit message passing, we aim to directly approximate such convergence state.To this end, a straightforward way is to minimize the difference of both sides of Equation 9.In this work, we normalize the embeddings to unit vectors and then maximize the dot product of both terms:\nmax \u2211\ufe01 \ud835\udc56 \u2208 N (\ud835\udc62 ) \ud835\udefd \ud835\udc62,\ud835\udc56 \ud835\udc52 \u22a4 \ud835\udc62 \ud835\udc52 \ud835\udc56 , \u2200\ud835\udc62 \u2208 \ud835\udc48 ,(10)\nwhich is equivalent to maximize the cosine similarity between   and   .For ease of optimization, we further incorporate sigmoid activation and negative log likelihood [2], and derive the following loss:\nL \ud835\udc36 = \u2212 \u2211\ufe01 \ud835\udc62 \u2208\ud835\udc48 \u2211\ufe01 \ud835\udc56 \u2208 N (\ud835\udc62 ) \ud835\udefd \ud835\udc62,\ud835\udc56 log \ud835\udf0e (\ud835\udc52 \u22a4 \ud835\udc62 \ud835\udc52 \ud835\udc56 ) , (11)\nwhere  is the sigmoid function.The loss is optimized to fulfill the structure constraint imposed by Equation 9.As such, we denote L  as the constraint loss and denote  , as the constraint coefficient.However, optimizing L  could also suffer from the over-smoothing problem as L  requires all connected pairs ( , > 0) to be similar.In this way, users and items could easily converge to the same embeddings.To alleviate the over-smoothing problem, conventional GCN-based CF models usually fix a small number of message passing layers, e.g., 2\u223c4 layers in LightGCN.Instead, as UltraGCN approximates the limit of infinite-layer message passing via a constraint loss, we choose to perform negative sampling during training.This is inspired from the negative sampling strategy used in Word2Vec [19], which provides a more simple and effective way to counteract the over-smoothing problem.After performing negative sampling, we finally derive the following constraint loss:\nL \ud835\udc36 = \u2212 \u2211\ufe01 (\ud835\udc62,\ud835\udc56 ) \u2208\ud835\udc41 + \ud835\udefd \ud835\udc62,\ud835\udc56 log \ud835\udf0e (\ud835\udc52 \u22a4 \ud835\udc62 \ud835\udc52 \ud835\udc56 ) \u2212 \u2211\ufe01 (\ud835\udc62,\ud835\udc57 ) \u2208\ud835\udc41 \u2212 \ud835\udefd \ud835\udc62,\ud835\udc57 log \ud835\udf0e (\u2212\ud835\udc52 \u22a4 \ud835\udc62 \ud835\udc52 \ud835\udc57 )(12)\nwhere  + and  \u2212 represent the sets of positive pairs and randomly sampled negative pairs.Note that we omit the summation over  for ease of presentation.The constraint loss L  enables UltraGCN to directly approximate the limit of infinite-layer message passing to capture arbitrary high-order collaborative signals in the user-item bipartite graph, while effectively avoiding the troublesome oversmoothing issue via negative sampling.Furthermore, we note that  , acts as the loss weight in L  , which is inversely proportional to   and   with similar magnitudes.This is interpretable for CF.If a user interacts with many items or an item is interacted by many users, the influence of their interaction would be small, and thus the loss weight of this (, ) pair should be small.\n\n3.1.1Optimization.Typically, CF models perform item recommendation by applying either pairwise BPR (Bayesian personalized ranking) loss [22] or pointwise BCE (binary cross-entropy) loss [11] for optimization.We formulate CF as a link prediction problem in graph learning.Therefore, we choose the following BCE loss as the main optimization objective.It is also consistent with the loss format of L  .\nL \ud835\udc42 = \u2212 \u2211\ufe01 (\ud835\udc62,\ud835\udc56 ) \u2208\ud835\udc41 + log \ud835\udf0e (\ud835\udc52 \u22a4 \ud835\udc62 \ud835\udc52 \ud835\udc56 ) \u2212 \u2211\ufe01 (\ud835\udc62,\ud835\udc57 ) \u2208\ud835\udc41 \u2212 log \ud835\udf0e (\u2212\ud835\udc52 \u22a4 \ud835\udc62 \ud835\udc52 \ud835\udc57 )(13)\nwhere  + and  \u2212 represent positive and randomly sampled negative links (i.e., - pairs).Note that for simplicity, we use the same sets of sample pairs with L  , but they could also be made different conveniently.\n\nAs L  and L  depends only on the user-item relationships, we define it as the base version of UltraGCN, denoted as UltraGCN  , which has the following optimization objective.\nL = L \ud835\udc42 + \ud835\udf06L \ud835\udc36 , (14)\nwhere  is the hyper-parameter to control the importance weights of two losse terms.\n\n\nLearning on Item-Item Graph\n\nAs Equation 5shows, except for user-item relationships, some other relationships (e.g., item-item and user-user relationships) also greatly contribute to the effectiveness of GCN-based models on CF.However, in conventional GCN-based models, these relationships are implicitly learned through the same message passing layers with user-item relationships.This not only leads to the unreasonable edge weight assignments as discussed in Section 2.2, but also fails to capture the relative importances of different types of relationships.In contrast, UltraGCN does not rely on explicit message passing so that we can separately learn other relationships in a more flexible way.This also enables us to manually adjust the relative importances of different relationships.\n\nWe emphasize that UltraGCN is flexible to extend to model many different relation graphs, such as user-user graphs, itemitem graphs, and even knowlege graphs.In this work, we mainly demonstrate its use on the item-item co-occurrence graph, which has been shown to be useful for recommendation in [26].We first build the item-item co-occurrence graph by linking items that have co-occurrences, which produces the following weighted adjacent matrix\n\ud835\udc3a \u2208 R |\ud835\udc3c | \u00d7 |\ud835\udc3c | . \ud835\udc3a = \ud835\udc34 \u22a4 \ud835\udc34 (15)\nwhere each entry denotes the co-occurrences of two items.We follow Equation 9to approximate infinite-layer graph convolution on  and derive the new coefficient  , :\n\ud835\udf14 \ud835\udc56,\ud835\udc57 = \ud835\udc3a \ud835\udc56,\ud835\udc57 \ud835\udc54 \ud835\udc56 \u2212 \ud835\udc3a \ud835\udc56,\ud835\udc56 \u221a\ufe02 \ud835\udc54 \ud835\udc56 \ud835\udc54 \ud835\udc57 , \ud835\udc54 \ud835\udc56 = \u2211\ufe01 \ud835\udc58 \ud835\udc3a \ud835\udc56,\ud835\udc58(16)\nwhere   and   denote the degrees (sum by column) of item  and item  in , respectively.Similar to Equation 12, we can derive the constraint loss on the item-item graph to learn the item-item relationships in an explicit way.However, as the adjacency matrix  of the item-item graph is usually much denser compared to the sparse adjacency matrix  of the user-item graph, directly minimizing the constraint loss on  would introduce too many unreliable or noisy item-item pairs into optimization, which may make UltraGCN difficult to train.This is also similar to the Limitation II of conventional GCN-based models described in Section 2.2.But thanks to the flexible design of UltraGCN, we choose to select only informative pairs for training.\n\nSpecifically, to keep sparse item connections and retain training efficiency, we first select top- most similar items  () for item  according to  , .Intuitively,  , measures the similarity of item  and item , since it is proportional to the co-occurrence number of item  and item , yet inversely proportional to the total degrees of both items.Instead of directly learning item-item pairs, we propose to augment positive user-item pairs to capture item-item relationships.This keeps the training terms of UltraGCN being unified and decrease the possible difficulty in multi-task learning.We also empirically show that such a way can achieve better performance in Section 4.4.For each positive (, ) pair, we first construct  weighted positive (, ) pairs, for  \u2208  ().Then, we penalize the learning of these pairs with the more reasonable similarity score  , and derive the constraint loss L  on the item-item graph as follow:\nL \ud835\udc3c = \u2212 \u2211\ufe01 (\ud835\udc62,\ud835\udc56 ) \u2208\ud835\udc41 + \u2211\ufe01 \ud835\udc57 \u2208\ud835\udc46 (\ud835\udc56 ) \ud835\udf14 \ud835\udc56,\ud835\udc57 log(\ud835\udf0e (\ud835\udc52 \u22a4 \ud835\udc62 \ud835\udc52 \ud835\udc57 ))(17)\nwhere | ()| = .We omit the negative sampling here as the negative sampling in L  and L  has already enabled UltraGCN to counteract over-smoothing.With this constraint loss, we extend UltraGCN to better learn item-item relationships, and finally derive the following training objective of UltraGCN,\nL = L \ud835\udc42 + \ud835\udf06L \ud835\udc36 + \ud835\udefe L \ud835\udc3c(18)\nwhere  and  are hyper-parameters to adjust the relative importances of user-item and item-item relationships, respectively.Figure 1 illustrates the simple architecture of UltraGCN in contrast to LightGCN.Similarly, in inference, we use the dot product \u0177 =  \u22a4    between user  and item  as the ranking score for recommendation.\n\n\nDiscussion\n\n\nModel Analysis.\n\nWe first analyze the strengths of our Ultra-GCN model: 1) The weights assigned on edges in UltraGCN, i.e.,  , and  , , are more reasonable and interpretable for CF, which are helpful to better learn user-item and item-item relationships, respectively.2) Without explicit message passing, UltraGCN is flexible to separately customize its learning with different types of relationships.It is also able to select valuable training pairs (as in Section 3.2), rather than learn from all neighbor pairs indistinguishably, which may be mislead by noise.3) Although UltraGCN is trained with different types of relationships in a multi-task learning way, its training losses (i.e., L  , L  , and L  ) are actually unified, following the form of binary cross entropy.Such unification facilitates the training of UltraGCN, which converges fast.4) The design of UltraGCN is flexible, by setting  to 0, it reduces to UltraGCN  , which only learns on the user-item graph.The performance comparison between UltraGCN and UltraGCN  is provided in Table 2.\n\nNote that in the current version, we do not incorporate the modeling of user-user relationships in UltraGCN.This is mainly because that users' interests are much broader than items' attributes.We found that it is harder to capture the user-user relationships from the user-user co-occurrence graph only.In Section 4.4, we empirically show that learning on the user-user co-occurrence graph does not bring noticeable improvements to UltraGCN.In contrast, conventional GCN-based CF models indistinguishably learn over all relationships from the user-item graph (i.e., Limitation II) likely suffer from performance degradation.The user-user relationships may be better modeled from a social network graph, and we leave it for future work.\n\n\nRelations to Other Models.\n\nIn this part, we discuss the relations between our UltraGCN and some other existing models.\n\nRelation to MF. UltraGCN is formally to be a new weighted MF model with BCE loss tailored for CF.In contrast to previous MF models (e.g., NeuMF [11]), UltraGCN can more deeply mine the collaborative information using graphs, yet keep the same concise architecture and model efficiency as MF.\n\nRelation to Network Embedding Methods.Qiu et al. [21] have proved that many popular network embedding methods with negative sampling (e.g., DeepWalk [20], LINE [25], and Node2Vec [7]) all can be unified into the MF framework.However, in contrast to these network embedding methods, the edge weights used in UltraGCN are more meaningful and reasonable for CF, and thus lead to much better performance.In addition, the random walk in many network embedding methods will also uncontrollably introduce uninformative relationships that affect the performance.We empirically show the superiority of UltraGCN over three typical network embedding methods on CF in Section 4.2.\n\nRelation to One-Layer LightGCN.We emphasize that Ultra-GCN is also different from one-layer LightGCN with BCE loss, because LightGCN applies weight combination to embeddings aggregation while our constraint coefficients are imposed on the constraint loss function, which aims to learn the essence of infinitelayer graph convolution.On the contrary, UltraGCN can overcome the limitations of one-layer LightGCN as described in Section 3.2.\n\n\nModel Complexity.\n\nGiven the embedding size ,  similar items for each  (),  as the number of negative samples for each positive pair, and | + | as the number of valid non-zero entries in the user-item interaction matrix, we can derive the training time complexity of UltraGCN: O (( +  + 1) * | + | * ( 2 + 1)).We note that the time complexities to calculate  and  are O (1), since we can pre-calculate them offline before training.As we usually limit  to be small (e.g., 10 in our experiments) in practice, the time complexity of UltraGCN lies in the same level with MF, which is O (( + 1) * | + | *  2 ).Besides, the only trainable parameters in UltraGCN are the embeddings of users and items, which is also the same with MF and LightGCN.As a result, our low-complexity UltraGCN brings great efficiency for model training and should be more practically applicable to large-scale recommender systems.\n\n\nEXPERIMENTS\n\nWe first compare UltraGCN with various state-of-the-art CF methods to demonstrate its effectiveness and high efficiency.We also perform detailed ablation studies to justify the rationality and effectiveness of the design choices of UltraGCN.\n\n\nExperimental Setup\n\nDatasets and Evaluation Protocol.We use four publicly available datasets, including Amazon-Book, Yelp2018, Gowalla, and MovieLens-1M to conduct our experiments, as many recent GCNbased CF models [10,27,28,32] are evaluated on these four datasets.We closely follow these GCN-based CF studies and use the same data split as them.Table 1 shows the statistics of the used datasets.\n\nFor the evaluation protocol, Recall@20 and NDCG@20 are chosen as the evaluation metrics as they are popular in the evaluation of GCN-based CF models.We treat all items not interacted by a user as the candidates, and report the average results over all users.Baselines.In total, we compare UltraGCN with various types of the state-of-the-art models, covering MF-based (MF-BPR [15], ENMF [3]), metric learing-based (CML [12]), network embedding methods (DeepWalk [20], LINE [25], and Node2Vec [7]), and GCNbased (NGCF [27], NIA-GCN [24], LR-GCCF [4], LightGCN [10], and DGCF [28]).\n\nParameter Settings.Generally, we adopt Gaussian distribution with 0 mean and 10 \u22124 standard deviation to initialize embeddings.In many cases, we adopt  2 regularization with 10 \u22124 weight and we set the learning rate to 10 \u22124 , the batch size to 1024, the negative sampling ratio  to 300, and the size of the neighbor set  to 10.In particular, we fix the embedding size to 64 which is identical to recent GCN-based work [10,24,27,28] to keep the same level of the number of parameters for fair comparison.We tune  in [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4], and  in [0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3, 3.5].For some baselines, we report the results from their papers to keep consistency.They are also comparable since we use the exactly same datasets and experimental settings provided by them.For other baselines, we mainly use their official open-source code and carefully tune the parameters to achieve the best performance for fair comparisons.To allow for reproduciblility, we have released the source code and benchmark settings of UltraGCN at RecZoo.\n\n\nPerformance Comparison\n\nTable 2 reports the performance comparison results.We have the following observations:\n\n\u2022 UltraGCN consistently yields the best performance across all four datasets.In particular, UltraGCN hugely improves over the strongest GCN-based baseline (i.e., DGCF) on Amazon-Book by 61.4% and 71.6% w.r.t.Recall@20 and NDCG@20 respectively.The results of significance testing indicates that our improvements over the current strongest GCN-based baseline are statistically significant (-value < 0.05).With additional learning on the item-item graph, UltraGCN performs consistently better than its simpler variant UltraGCN  .We attribute such good performance of UltraGCN to the following reasons: 1) Compared with network embedding models and the other GCN-based models, UltraGCN can respectively filter uninformative user-item and item-item relationships in a soft way (i.e., optimize with ) and a hard way (i.e., only select  most similar item pairs).The edge weights for the learning of user-item and item-item relationships in UltraGCN are also more reasonable; 2) Compared with other baselines, UltraGCN can leverage powerful graph convolution to exploit useful and deeper collaborative information in graphs.These advantages together lead to the superiority of UltraGCN than compared state-of-the-art models.\n\n\u2022 Overall, network embedding models perform worse than GCN-based models, especially on Gowalla.The reason might be that the powerful graph convolution is more effective than traditional random walk or heuristic mining strategies in many network embedding methods, to capture collaborative information for recommendation.\u2022 Since UltraGCN is a special MF which only needs the dot product operation for embeddings, its architecture is orthogonal to some state-of-the-art models (e.g., DGCF).Therefore, similar to MF, UltraGCN can be deemed as an effective and efficient CF framework which is possible to be incorporated with other methods, such as enabling disentangled representation for users and items as DGCF, to achieve better performance.We leave such study in future work.\n\n\nEfficiency Comparison\n\nAs highlighted in Section 3.3, UltraGCN is endowed with high training efficiency for CF thanks to its concise and unified designs.\n\nWe have also theoretically demonstrated that the training time complexity of UltraGCN is on the same level as MF in Section 3.3.3.\n\nIn this section, we further empirically demonstrate the superiority of UltraGCN on training efficiency compared with other CF models, especially GCN-based models.To be specific, we select MF-BPR, ENMF, LightGCN, and LR-GCCF as the competitors, which are relatively efficient models in their respective categories.To be more convincing, we compare their training efficiency from two views:\n\n\u2022 The total training time and epochs for achieving their best performance.\n\n\u2022 Training them with the same epochs to see what performance they can achieve.\n\nNote that the validation time is not included in the training time.\n\nConsidering the fact that the official implementations of the compared models can be optimized to be more efficient, we use a uniform code framework implemented by ourselves for all models for fair comparison.In particular, our implementations refer to their official versions and optimize them with uniform acceleration methods (e.g, parallel sampling) to ensure the fairness of comparison.We will release all of our code.Experiments are conducted on Amazon-Book with the same Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz machine with one GeForce RTX 2080 GPU for all compared models.Results of the two experiments are shown in Table 3 and Table 4 respectively.We have the following conclusions:\n\n(1) Table 3 shows that the training speed (i.e., Time/Epoch) of Ul-traGCN is close to MF-BPR, which empirically justifies our analysis that the time complexities of UltraGCN and MF are on the same level.UltraGCN needs 75 epochs to converge which is much less than LR-GCCF and LightGCN, leading to only 45 minutes for total training.Finally, UltraGCN has around 14x, 4x, 4x speedup compared with LightGCN, LR-GCCF, and ENMF respectively, demonstrating the big efficiency superiority of UltraGCN.\n\n(2) Table 4 shows that when UltraGCN converges (i.e., train the fixed 75 epochs), the performances of all the other compared models are much worse than UltraGCN.That is to say, UltraGCN can achieve much better performance with less time, which further demonstrates the higher efficiency of UltraGCN than the other GCN-based CF models.\n\n\nModel\n\nAmazon-Books Yelp2018 Gowalla Movielens-1M Recall@20 NDCG@20 Recall@20 NDCG@20 F1@20 NDCG@20 Recall@20 NDCG@20\n\n\nAblation Study\n\nWe perform ablation studies on Amazon-Book to justify the following opinions: (i) The designs of UltraGCN is effective, which can flexibly and separately learn the user-item relationships and item-item relationships to improve recommendation performance;\n\n(ii) Augmenting positive user-item pairs for training to learn itemitem relationships can achieve better performance than optimizing between item-item pairs; (iii) User-user co-occurrence information is probably not very informative to help recommendation.\n\nFor opinion (i), we compare UltraGCN with the following variants to show the effectiveness of our designs in UltraGCN: Table 5: Performance comparison of whether learning on the user-user co-occurrence graph.\n\n\nModel\n\nUltraGCN( = 0) UltraGCN( = 0) UltraGCN Recall@20 NDCG@20 Recall@20 NDCG@20 Recall@20 NDCG@20 \u2022 UltraGCN( = 0,  = 0): when setting  and  to 0, UltraGCN is simply reduced to MF training with BCE loss function, which does not leverage graph information and cannot capture higher-order collaborative signals.\n\n\u2022 UltraGCN( = 0): this variant is identical to UltraGCN  , which only learns on the user-item graph and lacks more effective learning for item-item relationships.\u2022 UltraGCN( = 0): this variant lacks the graph convolution ability for learning on the user-item graph to more deeply mine the collaborative information.\n\nResults are shown in Figure 2. We have the following observations:\n\n(1) UltraGCN( = 0) and UltraGCN( = 0) all perform better than UltraGCN( = 0,  = 0), demonstrating that the designs of UltraGCN can effectively learn on both the user-item graph and item-item graph to improve recommendation; (2) Relatively, UltraGCN( = 0) is inferior to UltraGCN( = 0), indicating that user-item relationships may be better modeled than item-item relationships in UltraGCN; (3) UltraGCN performs much better than all the other three variants, demonstrating that our idea to disassemble various relationships, eliminate uninformative things which may disturb the model learning, and finally conduct multi-task learning in a clearer way, is effective to overcome the limitations (see Section 2.2) of previous GCN-based CF models.\n\nFor opinion (ii), we change L  to L \u2032  :   which is instead to optimize between the target positive item and its most  similar items.We compare the performance of UltraGCN using L  and L \u2032  respectively with careful parameters tuning.Results are shown in Figure 3.It is clear that no matter incorporating L  or not, using L  can achieves obvious better performance than using L \u2032  , which proves that our designed strategy to learn on item-item graph is more effective.Furthermore, the performance gap between using L  and using L \u2032  becomes large when incorporating L  , indicating that our strategy which makes the objective of UltraGCN unified can thus facilitate training and improve performance.\nL \u2032 \ud835\udc3c = \u2212 \u2211\ufe01 (\ud835\udc62,\ud835\udc56 ) \u2208\ud835\udc41 + \u2211\ufe01 \ud835\udc57 \u2208\ud835\udc46 (\ud835\udc56 ) \ud835\udf14 \ud835\udc56,\ud835\udc57 log(\ud835\udf0e (\ud835\udc52 \u22a4 \ud835\udc56 \ud835\udc52 \ud835\udc57 ))(19)\nFor opinion (iii), we derive the user-user constraint loss L  with the similar method of Section 3.2 and combine it to the final objective.We carefully re-tune the parameters and show the comparison results of whether using L  in Table 5.As can be seen, incorporating L  to learn user-user relationships does not bring obvious benefits to UltraGCN.We attribute this phenomenon to the fact that the users' interests are broader than items' properties, and thus it is much harder to capture user-user relationships just from the user-user co-occurrence graph.Therefore, we do not introduce the modeling of user-user relationships into UltraGCN in this paper, and we will continue to study it in the future.\n\n\nParameter Analysis\n\nWe investigate the influence of the number of selected neighbors  and the weights of the two constraint losses (i.e.,  and ) on the performance for a better understanding of UltraGCN.4 shows the experimental results.We can find that when  increases from 5 to 50, the performance shows a trend of increasing first and then decreasing.This is because that when  is 5, the item-item relationships are not sufficiently exploited.While when  becomes large, there may introduce some less similar or less confident item-item relationships into the learning process that affect model performance.Such phenomenon also confirms that conventional GCN-based CF models inevitably take into account too many low-confidence relationships, thus hurting performance.4.5.2Impact of  and .We first set  = 0 and show the performance of different  from 0.2 to 1.4 (0.2 as the interval).Then we test with different  in [0.1, 0.5, 1, 1.5, 2, 2.5, 3, 3.5] based on the best .Experiments are conducted on Amazon-Book, and we show results in Figure 5.For , we find that the small value limits the exertion of the user-item constraint loss, and a value of 1 or so would be suitable for .For the impact of , its trend is similar to  but is more significant, and 2.5 is a suitable choice for .In general, our investigations for  and  show that these two parameters are important to UltraGCN, which can flexibly adjust the learning weights for different relationships and should be carefully set.\n\n\nRELATED WORK\n\nIn this section, we briefly review some representative GNN-based methods and their efforts for model simplification toward recommendation tasks.\n\nWith the development and success of GNN in various machine learning areas, there appears a lot of excellent work in recommendation community since the interaction of users and items could be naturally formed to a user-item bipartite graph.Rianne van den Berg et al. [1] propose graph convolutional matrix completion (GC-MC), a graph-based auto-encoder framework for explicit matrix completion.The encoder of GC-MC aggregates the information from neighbors based on the types of ratings, and then combine it to the new embeddings of the next layer.It is the first work using graph convolutional neural networks for recommendation.Ying et al. [31] first applys GCN on web-scale recommender systems and propose an efficient GCN-based method named Pinsage, which combines efficient random walks and graph convolutions to generate embeddings of items that incorporate both graph structure as well as item feature information.Then, Wang et al. [27] design NGCF which is a new graph-based framework for collaborative filtering.NGCF has a crafted interaction encoder to capture the collaborative signals among users and items.Although NGCF achieves good performance compared with previous non-GNN based methods, its heavy designs limit its efficiency and full exertion of GCN.To model the diversity of user intents on items, Wang et al. [28] devise Disentangled Graph Collaborative Filtering (DGCF), which considers user-item relationships at the finer granularity of user intents and generates disentangled user and item representations to get better recommendation performance.\n\nAlthough GNN-based recommendation models have achieved impressive performance, their efficiencies are still unsatisfactory when facing large-scale recommendation scenarios.How to improve the efficiency of GNNs and reserve their high performance for recommendation becomes a hot research problem.Recently, Dai et al. [6] and Gu et al. [8] extend fixed-point theory on GNN for better representation learning.Liu et al. [18] propose UCMF that simplifies GCN for the node classification task.Wu et al. [29] find the non-necessity of nonlinear activation and feature transformation in GCN, proposing a simplified GCN (SGCN) model by removing these two parts.Inspired by SGC, He et al. [10] devise LightGCN for recommendation by removing nonlinear activation and feature transformation too.However, its efficiency is still limited by the time-consuming message passing.Qiu et al. [21] demonstrate that many network embedding algorithms with negative sampling can be unified into the MF framework which may be efficient, however, their performances still have a gap between that of GCNs.We are inspired by these instructive studies, and propose UltraGCN for both efficient and effective recommendation.\n\n\nCONCLUSION\n\nIn this work, we propose an ultra-simplified formulation of GCN, dubbed UltraGCN.UltraGCN skips explicit message passing and directly approximate the limit of infinite message passing layers.Extensive experimental results demonstrate that UltraGCN achieves impressive improvements over the state-of-the-art CF models in terms of both accuracy and efficiency.\n\n\nACKNOWLEDGEMENT\n\n\nAPPENDIX\n\nTo further demonstrate the effectiveness of UltraGCN, we additionally provide the results compared to some more recent stateof-the-art CF models, including NBPO [33], BGCF [23], SCF [34], LCFN [32], and SGL-ED [30].For simplicity and fairness of comparison, we use the same dataset and evaluation protocol provided by each paper.We also duplicate the results reported in their papers to keep consistency.The results in Table 6 again validate the effectiveness of UltraGCN, which outperforms the most recent CF models by a large margin.\n\nFigure 2 :\n2\nFigure 2: Performance comparison of variants of UltraGCN.\n\n\nFigure 3 :\n3\nFigure 3: Performance comparison of using L \u2032  and L  .\n\n\nFigure 4 :Figure 5 :\n45\nFigure 4: Performance comparison of setting different .\n\n\n\n\nThis work was supported in part by the National Natural Science Foundation of China (61972219), the Research and Development Program of Shenzhen (JCYJ20190813174403598, SGDX20190918101201696), the National Key Research and Development Program of China (2018YFB1800601), and the Overseas Research Cooperation Fund of Tsinghua Shenzhen International Graduate School (HW2021013).\n\n\nTable 1 :\n1\nStatistics of the datasets.\nDataset#Users #Items #Interactions DensityAmazon-Book 52, 643 91, 5992, 984, 108 0.062%Yelp201831, 668 38, 0481, 561, 406 0.130%Gowalla29, 858 40, 9811, 027, 370 0.084%Movielens-1M6,0223,043995, 154 5.431%\n\nTable 2 :\n2\nOverall performance comparison.Improv.denotes the relative improvements over the best GNN-based baselines.\n\n\nTable 3 :\n3\nEfficiency comparison from the first view.\nMF-BPR0.03380.02610.05490.04450.16160.13660.21530.2175CML0.05220.04280.06220.05360.16700.12920.17300.1563ENMF0.03590.02810.06240.05150.15230.13150.23150.2069DeepWalk0.03460.02640.04760.03780.10340.07400.13480.1057LINE0.04100.03180.05490.04460.13350.10560.23360.2226Node2Vec0.04020.03090.04520.03600.10190.07090.14750.1186NGCF0.03440.02630.05790.04770.15700.13270.25130.2511NIA-GCN0.03690.02870.05990.04910.13590.11060.23590.2242LR-GCCF0.03350.02650.05610.03430.15190.12850.22310.2124LightGCN0.04110.03150.06490.05300.18300.15540.25760.2427DGCF0.04220.03240.06540.05340.18420.15610.26400.2504UltraGCN \ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc520.05040.03930.06670.05520.18450.15660.26710.2539UltraGCN0.06810.05560.06830.05610.18620.15800.27870.2642Improv.61.37%71.60%4.43%5.06%1.09%1.22%5.57%5.51%\ud835\udc5d-value4.03e-85.64e-81.61e-41.24e-47.21e-33.44e-44.19e-52.23e-5ModelTime/Epoch #Epoch Training TimeMF-BPR30s2312mENMF129s812h54mLR-GCCF67s1653h5mLightGCN51s78011h3mUltraGCN36s7545m\n\nTable 4 :\n4\nEfficiency comparison from the second view.All models are trained with the fixed 75 epochs except MF-BPR.Since MF-BPR needs less than 75 epochs to converge, we report its actual training time.\nModelTraining Time Recall@20 NDCG@20MF-BPR12m0.03380.0261ENMF2h41m0.03550.0277LR-GCCF1h13m0.03040.0185LightGCN1h4m0.03420.0262UltraGCN45m0.06810.0556\n\nTable 6 :\n6\nPerformance comparison with some more models, including SCF, LCFN, NBPO, BGCF, and SGL-ED.\nMovielens-1MModelF1@20NDCG@20NGCF0.15820.2511SCF0.16000.2560LCFN0.16250.2603UltraGCN0.20040.2642Improv.23.3%1.5%Amazon-ElectronicsModelF1@20NDCG@20MF-BPR0.02750.0680ENMF0.03140.0823NBPO0.03130.0810UltraGCN0.03300.0829Improv.5.1%0.7%Amazon-CDsModelRecall@20 NDCG@20NGCF0.12580.0792NIA-GCN0.14870.0932BGCF0.15060.0948UltraGCN0.16220.1043Improv.7.7%10.0%Amazon-BooksModelRecall@20 NDCG@20NGCF0.03440.0263LightGCN0.04110.0315SGL-ED0.04780.0379UltraGCN0.06810.0556Improv.42.5%46.7%\n\nGraph Convolutional Matrix Completion. Rianne Van Den, Thomas N Berg, Max Kipf, Welling, KDD'18 Deep Learning Day. 2018\n\nAn Analysis of the Softmax Cross Entropy Loss for Learning-to-Rank with Binary Relevance. Sebastian Bruch, Xuanhui Wang, Michael Bendersky, Marc Najork, Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval (SIGIR). the 2019 ACM SIGIR International Conference on Theory of Information Retrieval (SIGIR)2019\n\nEfficient Neural Matrix Factorization without Sampling for Recommendation. Chong Chen, Min Zhang, Yongfeng Zhang, Yiqun Liu, Shaoping Ma, ACM Transactions on Information Systems (TOIS). 382020. 2020\n\nRevisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach. Lei Chen, Le Wu, Richang Hong, Kun Zhang, Meng Wang, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)2020\n\nSimple and Deep Graph Convolutional Networks. Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, Yaliang Li, International Conference on Machine Learning (ICML). 2020\n\nLearning Steady-states of Iterative Algorithms over Graphs. Hanjun Dai, Zornitsa Kozareva, Bo Dai, Alex Smola, Le Song, International Conference on Machine Learning (ICML). 2018\n\nNode2vec: Scalable Feature Learning for Networks. Aditya Grover, Jure Leskovec, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD). the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD)2016\n\nImplicit Graph Neural Networks. Fangda Gu, Heng Chang, Wenwu Zhu, Somayeh Sojoudi, Laurent El Ghaoui, Advances in Neural Information Processing Systems (NeurIPS). 2020. 2020\n\nUps and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering. Ruining He, Julian Mcauley, Proceedings of the 25th International Conference on World Wide Web (WWW). the 25th International Conference on World Wide Web (WWW)2016\n\nLightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, Meng Wang, Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval (SIGIR). the 43rd International ACM SIGIR conference on research and development in Information Retrieval (SIGIR)2020\n\nNeural Collaborative Filtering. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua, Proceedings of the 26th International Conference on World Wide Web (WWW). the 26th International Conference on World Wide Web (WWW)2017\n\nCollaborative Metric Learning. Cheng-Kang Hsieh, Longqi Yang, Yin Cui, Tsung-Yi Lin, Serge Belongie, Deborah Estrin, Proceedings of the 26th International Conference on World Wide Web (WWW). the 26th International Conference on World Wide Web (WWW)2017\n\nDual Channel Hypergraph Collaborative Filtering. Shuyi Ji, Yifan Feng, Rongrong Ji, Xibin Zhao, Wanwan Tang, Yue Gao, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (SIGKDD). the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (SIGKDD)2020\n\nSemi-Supervised Classification with Graph Convolutional Networks. Thomas N Kipf, Max Welling, International Conference on Learning Representations (ICLR). 2017\n\nMatrix Factorization Techniques for Recommender Systems. Yehuda Koren, Robert Bell, Chris Volinsky, Computer. 422009. 2009\n\nDeeper Insights into Graph Convolutional Networks for Semi-Supervised Learning. Qimai Li, Zhichao Han, Xiao-Ming Wu, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)201832\n\nRGCF: Refined Graph Convolution Collaborative Filtering with Concise and Expressive embedding. Kang Liu, Feng Xue, Richang Hong, arXiv:2007.033832020. 2020arXiv preprint\n\nQiang Liu, Haoli Zhang, Zhaocheng Liu, arXiv:2007.09036Simplification of Graph Convolutional Networks: A Matrix Factorization-based Perspective. 2020. 2020arXiv preprint\n\nDistributed Representations of Words and Phrases and their Compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, Advances in Neural Information Processing Systems (NeurIPS). 2013\n\nDeepwalk: Online Learning of Social Representations. Bryan Perozzi, Rami Al-Rfou, Steven Skiena, Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD). the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD)2014\n\nNetwork Embedding as Matrix Factorization: Unifying Deepwalk, LINE, PTE, and Node2vec. Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, Jie Tang, Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining (WSDM). the Eleventh ACM International Conference on Web Search and Data Mining (WSDM)2018\n\n. Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme, \n\nBPR: Bayesian Personalized Ranking from Implicit Feedback. Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI). the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI)\n\nA Framework for Recommending Accurate and Diverse Items Using Bayesian Graph Convolutional Neural Networks. Jianing Sun, Wei Guo, Dengcheng Zhang, Yingxue Zhang, Florence Regol, Yaochen Hu, Huifeng Guo, Ruiming Tang, Han Yuan, Xiuqiang He, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (SIGKDD). the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (SIGKDD)2020\n\nNeighbor Interaction Aware Graph Convolution Networks for Recommendation. Jianing Sun, Yingxue Zhang, Wei Guo, Huifeng Guo, Ruiming Tang, Xiuqiang He, Chen Ma, Mark Coates, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)2020\n\nLine: Large-scale Information Network Embedding. Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Qiaozhu Mei, Proceedings of the 24th International Conference on World Wide Web (WWW). the 24th International Conference on World Wide Web (WWW)2015\n\nM2GRL: A Multi-task Multi-view Graph Representation Learning Framework for Web-scale Recommender Systems. Menghan Wang, Yujie Lin, Guli Lin, Keping Yang, Xiao-Ming Wu, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (SIGKDD). the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (SIGKDD)2020\n\nNeural Graph Collaborative Filtering. Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua, Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)2019\n\nDisentangled Graph Collaborative Filtering. Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, Tat-Seng Chua, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)2020\n\nSimplifying Graph Convolutional Networks. Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, Kilian Weinberger, International Conference on Machine Learning (ICML). 2019\n\nSelf-supervised Graph Learning for Recommendation. Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, Xing Xie, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)2021\n\nGraph Convolutional Neural Networks for Web-scale Recommender Systems. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, Jure Leskovec, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (SIGKDD). the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (SIGKDD)2018\n\nGraph Convolutional Network for Recommendation with Low-pass Collaborative Filters. Wenhui Yu, Zheng Qin, International Conference on Machine Learning (ICML). 2020\n\nSampler Design for Implicit Feedback Data by Noisy-label Robust Learning. Wenhui Yu, Zheng Qin, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)2020\n\nSpectral Collaborative Filtering. Lei Zheng, Chun-Ta Lu, Fei Jiang, Jiawei Zhang, Philip S Yu, Proceedings of the 12th ACM Conference on Recommender Systems (RecSys). the 12th ACM Conference on Recommender Systems (RecSys)2018\n", "annotations": {"author": "[{\"end\":107,\"start\":96},{\"end\":140,\"start\":108},{\"end\":174,\"start\":141},{\"end\":202,\"start\":175},{\"end\":240,\"start\":203},{\"end\":288,\"start\":241},{\"end\":327,\"start\":289},{\"end\":372,\"start\":328},{\"end\":402,\"start\":373},{\"end\":427,\"start\":403},{\"end\":450,\"start\":428}]", "publisher": null, "author_last_name": "[{\"end\":106,\"start\":103},{\"end\":119,\"start\":116},{\"end\":148,\"start\":144},{\"end\":182,\"start\":180},{\"end\":215,\"start\":211}]", "author_first_name": "[{\"end\":102,\"start\":96},{\"end\":115,\"start\":108},{\"end\":143,\"start\":141},{\"end\":179,\"start\":175},{\"end\":210,\"start\":203}]", "author_affiliation": "[{\"end\":287,\"start\":242},{\"end\":326,\"start\":290},{\"end\":371,\"start\":329},{\"end\":401,\"start\":374},{\"end\":426,\"start\":404},{\"end\":449,\"start\":429}]", "title": "[{\"end\":82,\"start\":1},{\"end\":532,\"start\":451}]", "venue": null, "abstract": "[{\"end\":2171,\"start\":749}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2500,\"start\":2496},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2979,\"start\":2975},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2982,\"start\":2979},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2985,\"start\":2982},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2988,\"start\":2985},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3301,\"start\":3297},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3316,\"start\":3312},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3628,\"start\":3625},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3631,\"start\":3628},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3634,\"start\":3631},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3637,\"start\":3634},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4049,\"start\":4046},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4052,\"start\":4049},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4055,\"start\":4052},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4777,\"start\":4773},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5025,\"start\":5022},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7599,\"start\":7595},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8234,\"start\":8231},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8237,\"start\":8234},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8240,\"start\":8237},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8243,\"start\":8240},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8344,\"start\":8340},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11440,\"start\":11436},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11582,\"start\":11578},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11742,\"start\":11739},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14132,\"start\":14129},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":15019,\"start\":15015},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16207,\"start\":16203},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16257,\"start\":16253},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18142,\"start\":18138},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23010,\"start\":23006},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23208,\"start\":23204},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23308,\"start\":23304},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23319,\"start\":23315},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23337,\"start\":23334},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25644,\"start\":25640},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25647,\"start\":25644},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25650,\"start\":25647},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25653,\"start\":25650},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26203,\"start\":26199},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26213,\"start\":26210},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26246,\"start\":26242},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":26289,\"start\":26285},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26300,\"start\":26296},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26318,\"start\":26315},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26344,\"start\":26340},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26358,\"start\":26354},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26371,\"start\":26368},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26386,\"start\":26382},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26401,\"start\":26397},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26828,\"start\":26824},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26831,\"start\":26828},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26834,\"start\":26831},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26837,\"start\":26834},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":37696,\"start\":37693},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":38072,\"start\":38068},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":38369,\"start\":38365},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":38760,\"start\":38756},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":39319,\"start\":39316},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":39337,\"start\":39334},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":39421,\"start\":39417},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":39502,\"start\":39498},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":39684,\"start\":39680},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":39878,\"start\":39874},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":40764,\"start\":40760},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":40775,\"start\":40771},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":40785,\"start\":40781},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":40796,\"start\":40792},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":40813,\"start\":40809}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":41207,\"start\":41135},{\"attributes\":{\"id\":\"fig_2\"},\"end\":41278,\"start\":41208},{\"attributes\":{\"id\":\"fig_4\"},\"end\":41360,\"start\":41279},{\"attributes\":{\"id\":\"fig_5\"},\"end\":41741,\"start\":41361},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":41988,\"start\":41742},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":42109,\"start\":41989},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":43103,\"start\":42110},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":43459,\"start\":43104},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":44040,\"start\":43460}]", "paragraph": "[{\"end\":2828,\"start\":2187},{\"end\":3373,\"start\":2830},{\"end\":4360,\"start\":3375},{\"end\":5198,\"start\":4362},{\"end\":5970,\"start\":5200},{\"end\":6850,\"start\":5972},{\"end\":6913,\"start\":6852},{\"end\":7350,\"start\":6915},{\"end\":7559,\"start\":7365},{\"end\":7718,\"start\":7591},{\"end\":7784,\"start\":7720},{\"end\":7960,\"start\":7832},{\"end\":8524,\"start\":7962},{\"end\":8968,\"start\":8566},{\"end\":9139,\"start\":9133},{\"end\":9144,\"start\":9141},{\"end\":9319,\"start\":9161},{\"end\":9485,\"start\":9321},{\"end\":9966,\"start\":9520},{\"end\":10220,\"start\":10170},{\"end\":10957,\"start\":10382},{\"end\":11108,\"start\":10959},{\"end\":11135,\"start\":11118},{\"end\":11828,\"start\":11143},{\"end\":11979,\"start\":11899},{\"end\":12183,\"start\":11981},{\"end\":12461,\"start\":12196},{\"end\":12853,\"start\":12493},{\"end\":12961,\"start\":12855},{\"end\":13294,\"start\":13009},{\"end\":13430,\"start\":13360},{\"end\":13609,\"start\":13499},{\"end\":13912,\"start\":13611},{\"end\":14164,\"start\":13962},{\"end\":15193,\"start\":14227},{\"end\":16065,\"start\":15289},{\"end\":16467,\"start\":16067},{\"end\":16762,\"start\":16551},{\"end\":16938,\"start\":16764},{\"end\":17044,\"start\":16961},{\"end\":17840,\"start\":17076},{\"end\":18288,\"start\":17842},{\"end\":18488,\"start\":18324},{\"end\":19287,\"start\":18549},{\"end\":20212,\"start\":19289},{\"end\":20576,\"start\":20279},{\"end\":20930,\"start\":20604},{\"end\":22001,\"start\":20963},{\"end\":22738,\"start\":22003},{\"end\":22860,\"start\":22769},{\"end\":23153,\"start\":22862},{\"end\":23823,\"start\":23155},{\"end\":24262,\"start\":23825},{\"end\":25165,\"start\":24284},{\"end\":25422,\"start\":25181},{\"end\":25822,\"start\":25445},{\"end\":26403,\"start\":25824},{\"end\":27455,\"start\":26405},{\"end\":27568,\"start\":27482},{\"end\":28786,\"start\":27570},{\"end\":29564,\"start\":28788},{\"end\":29720,\"start\":29590},{\"end\":29852,\"start\":29722},{\"end\":30242,\"start\":29854},{\"end\":30318,\"start\":30244},{\"end\":30398,\"start\":30320},{\"end\":30467,\"start\":30400},{\"end\":31163,\"start\":30469},{\"end\":31659,\"start\":31165},{\"end\":31995,\"start\":31661},{\"end\":32115,\"start\":32005},{\"end\":32388,\"start\":32134},{\"end\":32646,\"start\":32390},{\"end\":32856,\"start\":32648},{\"end\":33170,\"start\":32866},{\"end\":33487,\"start\":33172},{\"end\":33555,\"start\":33489},{\"end\":34300,\"start\":33557},{\"end\":35002,\"start\":34302},{\"end\":35775,\"start\":35071},{\"end\":37264,\"start\":35798},{\"end\":37425,\"start\":37281},{\"end\":38998,\"start\":37427},{\"end\":40195,\"start\":39000},{\"end\":40568,\"start\":40210},{\"end\":41134,\"start\":40599},{\"end\":41206,\"start\":41149},{\"end\":41277,\"start\":41222},{\"end\":41359,\"start\":41304},{\"end\":41740,\"start\":41364},{\"end\":41782,\"start\":41755},{\"end\":42108,\"start\":42002},{\"end\":42165,\"start\":42123},{\"end\":43309,\"start\":43117},{\"end\":43563,\"start\":43473}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7831,\"start\":7785},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8565,\"start\":8525},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9052,\"start\":8969},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9132,\"start\":9052},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10169,\"start\":9967},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10381,\"start\":10221},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11117,\"start\":11109},{\"attributes\":{\"id\":\"formula_7\"},\"end\":11142,\"start\":11136},{\"attributes\":{\"id\":\"formula_8\"},\"end\":11898,\"start\":11829},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13008,\"start\":12962},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13359,\"start\":13295},{\"attributes\":{\"id\":\"formula_11\"},\"end\":13498,\"start\":13431},{\"attributes\":{\"id\":\"formula_12\"},\"end\":13961,\"start\":13913},{\"attributes\":{\"id\":\"formula_13\"},\"end\":14225,\"start\":14165},{\"attributes\":{\"id\":\"formula_14\"},\"end\":14226,\"start\":14225},{\"attributes\":{\"id\":\"formula_15\"},\"end\":15288,\"start\":15194},{\"attributes\":{\"id\":\"formula_16\"},\"end\":16550,\"start\":16468},{\"attributes\":{\"id\":\"formula_17\"},\"end\":16959,\"start\":16939},{\"attributes\":{\"id\":\"formula_18\"},\"end\":16960,\"start\":16959},{\"attributes\":{\"id\":\"formula_19\"},\"end\":18322,\"start\":18289},{\"attributes\":{\"id\":\"formula_20\"},\"end\":18323,\"start\":18322},{\"attributes\":{\"id\":\"formula_21\"},\"end\":18548,\"start\":18489},{\"attributes\":{\"id\":\"formula_22\"},\"end\":20278,\"start\":20213},{\"attributes\":{\"id\":\"formula_23\"},\"end\":20603,\"start\":20577},{\"attributes\":{\"id\":\"formula_24\"},\"end\":35070,\"start\":35003}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":22000,\"start\":21999},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25779,\"start\":25778},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27489,\"start\":27488},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":31103,\"start\":31102},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":31115,\"start\":31114},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":31176,\"start\":31175},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":31672,\"start\":31671},{\"end\":32774,\"start\":32773},{\"end\":35308,\"start\":35307},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":41025,\"start\":41024}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2185,\"start\":2173},{\"attributes\":{\"n\":\"2\"},\"end\":7363,\"start\":7353},{\"attributes\":{\"n\":\"2.1\"},\"end\":7589,\"start\":7562},{\"end\":9159,\"start\":9147},{\"attributes\":{\"n\":\"2.2\"},\"end\":9518,\"start\":9488},{\"attributes\":{\"n\":\"3\"},\"end\":12194,\"start\":12186},{\"attributes\":{\"n\":\"3.1\"},\"end\":12491,\"start\":12464},{\"attributes\":{\"n\":\"3.2\"},\"end\":17074,\"start\":17047},{\"attributes\":{\"n\":\"3.3\"},\"end\":20943,\"start\":20933},{\"attributes\":{\"n\":\"3.3.1\"},\"end\":20961,\"start\":20946},{\"attributes\":{\"n\":\"3.3.2\"},\"end\":22767,\"start\":22741},{\"attributes\":{\"n\":\"3.3.3\"},\"end\":24282,\"start\":24265},{\"attributes\":{\"n\":\"4\"},\"end\":25179,\"start\":25168},{\"attributes\":{\"n\":\"4.1\"},\"end\":25443,\"start\":25425},{\"attributes\":{\"n\":\"4.2\"},\"end\":27480,\"start\":27458},{\"attributes\":{\"n\":\"4.3\"},\"end\":29588,\"start\":29567},{\"end\":32003,\"start\":31998},{\"attributes\":{\"n\":\"4.4\"},\"end\":32132,\"start\":32118},{\"end\":32864,\"start\":32859},{\"attributes\":{\"n\":\"4.5\"},\"end\":35796,\"start\":35778},{\"attributes\":{\"n\":\"5\"},\"end\":37279,\"start\":37267},{\"attributes\":{\"n\":\"6\"},\"end\":40208,\"start\":40198},{\"attributes\":{\"n\":\"7\"},\"end\":40586,\"start\":40571},{\"attributes\":{\"n\":\"8\"},\"end\":40597,\"start\":40589},{\"end\":41146,\"start\":41136},{\"end\":41219,\"start\":41209},{\"end\":41300,\"start\":41280},{\"end\":41752,\"start\":41743},{\"end\":41999,\"start\":41990},{\"end\":42120,\"start\":42111},{\"end\":43114,\"start\":43105},{\"end\":43470,\"start\":43461}]", "table": "[{\"end\":41988,\"start\":41783},{\"end\":43103,\"start\":42166},{\"end\":43459,\"start\":43310},{\"end\":44040,\"start\":43564}]", "figure_caption": "[{\"end\":41207,\"start\":41148},{\"end\":41278,\"start\":41221},{\"end\":41360,\"start\":41303},{\"end\":41741,\"start\":41363},{\"end\":41783,\"start\":41754},{\"end\":42109,\"start\":42001},{\"end\":42166,\"start\":42122},{\"end\":43310,\"start\":43116},{\"end\":43564,\"start\":43472}]", "figure_ref": "[{\"end\":9358,\"start\":9357},{\"end\":20735,\"start\":20734},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33518,\"start\":33517},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":34565,\"start\":34564},{\"end\":35982,\"start\":35981},{\"end\":36822,\"start\":36821}]", "bib_author_first_name": "[{\"end\":44087,\"start\":44081},{\"end\":44103,\"start\":44097},{\"end\":44105,\"start\":44104},{\"end\":44115,\"start\":44112},{\"end\":44262,\"start\":44253},{\"end\":44277,\"start\":44270},{\"end\":44291,\"start\":44284},{\"end\":44307,\"start\":44303},{\"end\":44591,\"start\":44586},{\"end\":44601,\"start\":44598},{\"end\":44617,\"start\":44609},{\"end\":44630,\"start\":44625},{\"end\":44644,\"start\":44636},{\"end\":44818,\"start\":44815},{\"end\":44827,\"start\":44825},{\"end\":44839,\"start\":44832},{\"end\":44849,\"start\":44846},{\"end\":44861,\"start\":44857},{\"end\":45047,\"start\":45043},{\"end\":45060,\"start\":45054},{\"end\":45074,\"start\":45066},{\"end\":45087,\"start\":45082},{\"end\":45101,\"start\":45094},{\"end\":45231,\"start\":45225},{\"end\":45245,\"start\":45237},{\"end\":45258,\"start\":45256},{\"end\":45268,\"start\":45264},{\"end\":45278,\"start\":45276},{\"end\":45400,\"start\":45394},{\"end\":45413,\"start\":45409},{\"end\":45669,\"start\":45663},{\"end\":45678,\"start\":45674},{\"end\":45691,\"start\":45686},{\"end\":45704,\"start\":45697},{\"end\":45721,\"start\":45714},{\"end\":45724,\"start\":45722},{\"end\":45916,\"start\":45909},{\"end\":45927,\"start\":45921},{\"end\":46163,\"start\":46155},{\"end\":46172,\"start\":46168},{\"end\":46184,\"start\":46179},{\"end\":46194,\"start\":46191},{\"end\":46208,\"start\":46199},{\"end\":46220,\"start\":46216},{\"end\":46498,\"start\":46490},{\"end\":46507,\"start\":46503},{\"end\":46521,\"start\":46514},{\"end\":46536,\"start\":46529},{\"end\":46545,\"start\":46542},{\"end\":46558,\"start\":46550},{\"end\":46743,\"start\":46733},{\"end\":46757,\"start\":46751},{\"end\":46767,\"start\":46764},{\"end\":46781,\"start\":46773},{\"end\":46792,\"start\":46787},{\"end\":46810,\"start\":46803},{\"end\":47010,\"start\":47005},{\"end\":47020,\"start\":47015},{\"end\":47035,\"start\":47027},{\"end\":47045,\"start\":47040},{\"end\":47058,\"start\":47052},{\"end\":47068,\"start\":47065},{\"end\":47349,\"start\":47343},{\"end\":47351,\"start\":47350},{\"end\":47361,\"start\":47358},{\"end\":47501,\"start\":47495},{\"end\":47515,\"start\":47509},{\"end\":47527,\"start\":47522},{\"end\":47647,\"start\":47642},{\"end\":47659,\"start\":47652},{\"end\":47674,\"start\":47665},{\"end\":47909,\"start\":47905},{\"end\":47919,\"start\":47915},{\"end\":47932,\"start\":47925},{\"end\":47986,\"start\":47981},{\"end\":47997,\"start\":47992},{\"end\":48014,\"start\":48005},{\"end\":48234,\"start\":48229},{\"end\":48248,\"start\":48244},{\"end\":48263,\"start\":48260},{\"end\":48274,\"start\":48270},{\"end\":48276,\"start\":48275},{\"end\":48290,\"start\":48286},{\"end\":48422,\"start\":48417},{\"end\":48436,\"start\":48432},{\"end\":48452,\"start\":48446},{\"end\":48763,\"start\":48755},{\"end\":48775,\"start\":48769},{\"end\":48785,\"start\":48782},{\"end\":48794,\"start\":48790},{\"end\":48806,\"start\":48799},{\"end\":48816,\"start\":48813},{\"end\":49011,\"start\":49004},{\"end\":49029,\"start\":49020},{\"end\":49049,\"start\":49045},{\"end\":49063,\"start\":49059},{\"end\":49425,\"start\":49418},{\"end\":49434,\"start\":49431},{\"end\":49449,\"start\":49440},{\"end\":49464,\"start\":49457},{\"end\":49480,\"start\":49472},{\"end\":49495,\"start\":49488},{\"end\":49507,\"start\":49500},{\"end\":49520,\"start\":49513},{\"end\":49530,\"start\":49527},{\"end\":49545,\"start\":49537},{\"end\":49834,\"start\":49827},{\"end\":49847,\"start\":49840},{\"end\":49858,\"start\":49855},{\"end\":49871,\"start\":49864},{\"end\":49884,\"start\":49877},{\"end\":49899,\"start\":49891},{\"end\":49908,\"start\":49904},{\"end\":49917,\"start\":49913},{\"end\":50210,\"start\":50206},{\"end\":50221,\"start\":50217},{\"end\":50233,\"start\":50226},{\"end\":50244,\"start\":50240},{\"end\":50255,\"start\":50252},{\"end\":50268,\"start\":50261},{\"end\":50524,\"start\":50517},{\"end\":50536,\"start\":50531},{\"end\":50546,\"start\":50542},{\"end\":50558,\"start\":50552},{\"end\":50574,\"start\":50565},{\"end\":50825,\"start\":50820},{\"end\":50840,\"start\":50832},{\"end\":50849,\"start\":50845},{\"end\":50860,\"start\":50856},{\"end\":50875,\"start\":50867},{\"end\":51162,\"start\":51157},{\"end\":51175,\"start\":51169},{\"end\":51183,\"start\":51181},{\"end\":51199,\"start\":51191},{\"end\":51208,\"start\":51204},{\"end\":51221,\"start\":51213},{\"end\":51506,\"start\":51501},{\"end\":51517,\"start\":51511},{\"end\":51531,\"start\":51525},{\"end\":51550,\"start\":51539},{\"end\":51561,\"start\":51558},{\"end\":51572,\"start\":51566},{\"end\":51702,\"start\":51695},{\"end\":51712,\"start\":51707},{\"end\":51723,\"start\":51719},{\"end\":51738,\"start\":51730},{\"end\":51748,\"start\":51743},{\"end\":51762,\"start\":51755},{\"end\":51773,\"start\":51769},{\"end\":52084,\"start\":52081},{\"end\":52098,\"start\":52091},{\"end\":52110,\"start\":52103},{\"end\":52121,\"start\":52117},{\"end\":52143,\"start\":52136},{\"end\":52145,\"start\":52144},{\"end\":52160,\"start\":52156},{\"end\":52464,\"start\":52458},{\"end\":52474,\"start\":52469},{\"end\":52619,\"start\":52613},{\"end\":52629,\"start\":52624},{\"end\":52903,\"start\":52900},{\"end\":52918,\"start\":52911},{\"end\":52926,\"start\":52923},{\"end\":52940,\"start\":52934},{\"end\":52954,\"start\":52948},{\"end\":52956,\"start\":52955}]", "bib_author_last_name": "[{\"end\":44095,\"start\":44088},{\"end\":44110,\"start\":44106},{\"end\":44120,\"start\":44116},{\"end\":44129,\"start\":44122},{\"end\":44268,\"start\":44263},{\"end\":44282,\"start\":44278},{\"end\":44301,\"start\":44292},{\"end\":44314,\"start\":44308},{\"end\":44596,\"start\":44592},{\"end\":44607,\"start\":44602},{\"end\":44623,\"start\":44618},{\"end\":44634,\"start\":44631},{\"end\":44647,\"start\":44645},{\"end\":44823,\"start\":44819},{\"end\":44830,\"start\":44828},{\"end\":44844,\"start\":44840},{\"end\":44855,\"start\":44850},{\"end\":44866,\"start\":44862},{\"end\":45052,\"start\":45048},{\"end\":45064,\"start\":45061},{\"end\":45080,\"start\":45075},{\"end\":45092,\"start\":45088},{\"end\":45104,\"start\":45102},{\"end\":45235,\"start\":45232},{\"end\":45254,\"start\":45246},{\"end\":45262,\"start\":45259},{\"end\":45274,\"start\":45269},{\"end\":45283,\"start\":45279},{\"end\":45407,\"start\":45401},{\"end\":45422,\"start\":45414},{\"end\":45672,\"start\":45670},{\"end\":45684,\"start\":45679},{\"end\":45695,\"start\":45692},{\"end\":45712,\"start\":45705},{\"end\":45731,\"start\":45725},{\"end\":45919,\"start\":45917},{\"end\":45935,\"start\":45928},{\"end\":46166,\"start\":46164},{\"end\":46177,\"start\":46173},{\"end\":46189,\"start\":46185},{\"end\":46197,\"start\":46195},{\"end\":46214,\"start\":46209},{\"end\":46225,\"start\":46221},{\"end\":46501,\"start\":46499},{\"end\":46512,\"start\":46508},{\"end\":46527,\"start\":46522},{\"end\":46540,\"start\":46537},{\"end\":46548,\"start\":46546},{\"end\":46563,\"start\":46559},{\"end\":46749,\"start\":46744},{\"end\":46762,\"start\":46758},{\"end\":46771,\"start\":46768},{\"end\":46785,\"start\":46782},{\"end\":46801,\"start\":46793},{\"end\":46817,\"start\":46811},{\"end\":47013,\"start\":47011},{\"end\":47025,\"start\":47021},{\"end\":47038,\"start\":47036},{\"end\":47050,\"start\":47046},{\"end\":47063,\"start\":47059},{\"end\":47072,\"start\":47069},{\"end\":47356,\"start\":47352},{\"end\":47369,\"start\":47362},{\"end\":47507,\"start\":47502},{\"end\":47520,\"start\":47516},{\"end\":47536,\"start\":47528},{\"end\":47650,\"start\":47648},{\"end\":47663,\"start\":47660},{\"end\":47677,\"start\":47675},{\"end\":47913,\"start\":47910},{\"end\":47923,\"start\":47920},{\"end\":47937,\"start\":47933},{\"end\":47990,\"start\":47987},{\"end\":48003,\"start\":47998},{\"end\":48018,\"start\":48015},{\"end\":48242,\"start\":48235},{\"end\":48258,\"start\":48249},{\"end\":48268,\"start\":48264},{\"end\":48284,\"start\":48277},{\"end\":48295,\"start\":48291},{\"end\":48430,\"start\":48423},{\"end\":48444,\"start\":48437},{\"end\":48459,\"start\":48453},{\"end\":48767,\"start\":48764},{\"end\":48780,\"start\":48776},{\"end\":48788,\"start\":48786},{\"end\":48797,\"start\":48795},{\"end\":48811,\"start\":48807},{\"end\":48821,\"start\":48817},{\"end\":49018,\"start\":49012},{\"end\":49043,\"start\":49030},{\"end\":49057,\"start\":49050},{\"end\":49078,\"start\":49064},{\"end\":49429,\"start\":49426},{\"end\":49438,\"start\":49435},{\"end\":49455,\"start\":49450},{\"end\":49470,\"start\":49465},{\"end\":49486,\"start\":49481},{\"end\":49498,\"start\":49496},{\"end\":49511,\"start\":49508},{\"end\":49525,\"start\":49521},{\"end\":49535,\"start\":49531},{\"end\":49548,\"start\":49546},{\"end\":49838,\"start\":49835},{\"end\":49853,\"start\":49848},{\"end\":49862,\"start\":49859},{\"end\":49875,\"start\":49872},{\"end\":49889,\"start\":49885},{\"end\":49902,\"start\":49900},{\"end\":49911,\"start\":49909},{\"end\":49924,\"start\":49918},{\"end\":50215,\"start\":50211},{\"end\":50224,\"start\":50222},{\"end\":50238,\"start\":50234},{\"end\":50250,\"start\":50245},{\"end\":50259,\"start\":50256},{\"end\":50272,\"start\":50269},{\"end\":50529,\"start\":50525},{\"end\":50540,\"start\":50537},{\"end\":50550,\"start\":50547},{\"end\":50563,\"start\":50559},{\"end\":50577,\"start\":50575},{\"end\":50830,\"start\":50826},{\"end\":50843,\"start\":50841},{\"end\":50854,\"start\":50850},{\"end\":50865,\"start\":50861},{\"end\":50880,\"start\":50876},{\"end\":51167,\"start\":51163},{\"end\":51179,\"start\":51176},{\"end\":51189,\"start\":51184},{\"end\":51202,\"start\":51200},{\"end\":51211,\"start\":51209},{\"end\":51226,\"start\":51222},{\"end\":51509,\"start\":51507},{\"end\":51523,\"start\":51518},{\"end\":51537,\"start\":51532},{\"end\":51556,\"start\":51551},{\"end\":51564,\"start\":51562},{\"end\":51583,\"start\":51573},{\"end\":51705,\"start\":51703},{\"end\":51717,\"start\":51713},{\"end\":51728,\"start\":51724},{\"end\":51741,\"start\":51739},{\"end\":51753,\"start\":51749},{\"end\":51767,\"start\":51763},{\"end\":51777,\"start\":51774},{\"end\":52089,\"start\":52085},{\"end\":52101,\"start\":52099},{\"end\":52115,\"start\":52111},{\"end\":52134,\"start\":52122},{\"end\":52154,\"start\":52146},{\"end\":52169,\"start\":52161},{\"end\":52467,\"start\":52465},{\"end\":52478,\"start\":52475},{\"end\":52622,\"start\":52620},{\"end\":52633,\"start\":52630},{\"end\":52909,\"start\":52904},{\"end\":52921,\"start\":52919},{\"end\":52932,\"start\":52927},{\"end\":52946,\"start\":52941},{\"end\":52959,\"start\":52957}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":36809545},\"end\":44161,\"start\":44042},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":199134224},\"end\":44509,\"start\":44163},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":211041929},\"end\":44709,\"start\":44511},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":210932292},\"end\":44995,\"start\":44711},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":220363476},\"end\":45163,\"start\":44997},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":51681108},\"end\":45342,\"start\":45165},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":207238980},\"end\":45629,\"start\":45344},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":221655683},\"end\":45804,\"start\":45631},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1964279},\"end\":46072,\"start\":45806},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":211043589},\"end\":46456,\"start\":46074},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":13907106},\"end\":46700,\"start\":46458},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":11129957},\"end\":46954,\"start\":46702},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":221191170},\"end\":47275,\"start\":46956},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":3144218},\"end\":47436,\"start\":47277},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":58370896},\"end\":47560,\"start\":47438},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":11118105},\"end\":47808,\"start\":47562},{\"attributes\":{\"doi\":\"arXiv:2007.03383\",\"id\":\"b16\"},\"end\":47979,\"start\":47810},{\"attributes\":{\"doi\":\"arXiv:2007.09036\",\"id\":\"b17\"},\"end\":48150,\"start\":47981},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":16447573},\"end\":48362,\"start\":48152},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3051291},\"end\":48666,\"start\":48364},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3952914},\"end\":49000,\"start\":48668},{\"attributes\":{\"id\":\"b21\"},\"end\":49080,\"start\":49002},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10795036},\"end\":49308,\"start\":49082},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":221191420},\"end\":49751,\"start\":49310},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":220730279},\"end\":50155,\"start\":49753},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":8399404},\"end\":50409,\"start\":50157},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":218719692},\"end\":50780,\"start\":50411},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":150380651},\"end\":51111,\"start\":50782},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":220347145},\"end\":51457,\"start\":51113},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":67752026},\"end\":51642,\"start\":51459},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":224814335},\"end\":52008,\"start\":51644},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":46949657},\"end\":52372,\"start\":52010},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":220250619},\"end\":52537,\"start\":52374},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":220514234},\"end\":52864,\"start\":52539},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":52145692},\"end\":53092,\"start\":52866}]", "bib_title": "[{\"end\":44079,\"start\":44042},{\"end\":44251,\"start\":44163},{\"end\":44584,\"start\":44511},{\"end\":44813,\"start\":44711},{\"end\":45041,\"start\":44997},{\"end\":45223,\"start\":45165},{\"end\":45392,\"start\":45344},{\"end\":45661,\"start\":45631},{\"end\":45907,\"start\":45806},{\"end\":46153,\"start\":46074},{\"end\":46488,\"start\":46458},{\"end\":46731,\"start\":46702},{\"end\":47003,\"start\":46956},{\"end\":47341,\"start\":47277},{\"end\":47493,\"start\":47438},{\"end\":47640,\"start\":47562},{\"end\":48227,\"start\":48152},{\"end\":48415,\"start\":48364},{\"end\":48753,\"start\":48668},{\"end\":49139,\"start\":49082},{\"end\":49416,\"start\":49310},{\"end\":49825,\"start\":49753},{\"end\":50204,\"start\":50157},{\"end\":50515,\"start\":50411},{\"end\":50818,\"start\":50782},{\"end\":51155,\"start\":51113},{\"end\":51499,\"start\":51459},{\"end\":51693,\"start\":51644},{\"end\":52079,\"start\":52010},{\"end\":52456,\"start\":52374},{\"end\":52611,\"start\":52539},{\"end\":52898,\"start\":52866}]", "bib_author": "[{\"end\":44097,\"start\":44081},{\"end\":44112,\"start\":44097},{\"end\":44122,\"start\":44112},{\"end\":44131,\"start\":44122},{\"end\":44270,\"start\":44253},{\"end\":44284,\"start\":44270},{\"end\":44303,\"start\":44284},{\"end\":44316,\"start\":44303},{\"end\":44598,\"start\":44586},{\"end\":44609,\"start\":44598},{\"end\":44625,\"start\":44609},{\"end\":44636,\"start\":44625},{\"end\":44649,\"start\":44636},{\"end\":44825,\"start\":44815},{\"end\":44832,\"start\":44825},{\"end\":44846,\"start\":44832},{\"end\":44857,\"start\":44846},{\"end\":44868,\"start\":44857},{\"end\":45054,\"start\":45043},{\"end\":45066,\"start\":45054},{\"end\":45082,\"start\":45066},{\"end\":45094,\"start\":45082},{\"end\":45106,\"start\":45094},{\"end\":45237,\"start\":45225},{\"end\":45256,\"start\":45237},{\"end\":45264,\"start\":45256},{\"end\":45276,\"start\":45264},{\"end\":45285,\"start\":45276},{\"end\":45409,\"start\":45394},{\"end\":45424,\"start\":45409},{\"end\":45674,\"start\":45663},{\"end\":45686,\"start\":45674},{\"end\":45697,\"start\":45686},{\"end\":45714,\"start\":45697},{\"end\":45733,\"start\":45714},{\"end\":45921,\"start\":45909},{\"end\":45937,\"start\":45921},{\"end\":46168,\"start\":46155},{\"end\":46179,\"start\":46168},{\"end\":46191,\"start\":46179},{\"end\":46199,\"start\":46191},{\"end\":46216,\"start\":46199},{\"end\":46227,\"start\":46216},{\"end\":46503,\"start\":46490},{\"end\":46514,\"start\":46503},{\"end\":46529,\"start\":46514},{\"end\":46542,\"start\":46529},{\"end\":46550,\"start\":46542},{\"end\":46565,\"start\":46550},{\"end\":46751,\"start\":46733},{\"end\":46764,\"start\":46751},{\"end\":46773,\"start\":46764},{\"end\":46787,\"start\":46773},{\"end\":46803,\"start\":46787},{\"end\":46819,\"start\":46803},{\"end\":47015,\"start\":47005},{\"end\":47027,\"start\":47015},{\"end\":47040,\"start\":47027},{\"end\":47052,\"start\":47040},{\"end\":47065,\"start\":47052},{\"end\":47074,\"start\":47065},{\"end\":47358,\"start\":47343},{\"end\":47371,\"start\":47358},{\"end\":47509,\"start\":47495},{\"end\":47522,\"start\":47509},{\"end\":47538,\"start\":47522},{\"end\":47652,\"start\":47642},{\"end\":47665,\"start\":47652},{\"end\":47679,\"start\":47665},{\"end\":47915,\"start\":47905},{\"end\":47925,\"start\":47915},{\"end\":47939,\"start\":47925},{\"end\":47992,\"start\":47981},{\"end\":48005,\"start\":47992},{\"end\":48020,\"start\":48005},{\"end\":48244,\"start\":48229},{\"end\":48260,\"start\":48244},{\"end\":48270,\"start\":48260},{\"end\":48286,\"start\":48270},{\"end\":48297,\"start\":48286},{\"end\":48432,\"start\":48417},{\"end\":48446,\"start\":48432},{\"end\":48461,\"start\":48446},{\"end\":48769,\"start\":48755},{\"end\":48782,\"start\":48769},{\"end\":48790,\"start\":48782},{\"end\":48799,\"start\":48790},{\"end\":48813,\"start\":48799},{\"end\":48823,\"start\":48813},{\"end\":49020,\"start\":49004},{\"end\":49045,\"start\":49020},{\"end\":49059,\"start\":49045},{\"end\":49080,\"start\":49059},{\"end\":49431,\"start\":49418},{\"end\":49440,\"start\":49431},{\"end\":49457,\"start\":49440},{\"end\":49472,\"start\":49457},{\"end\":49488,\"start\":49472},{\"end\":49500,\"start\":49488},{\"end\":49513,\"start\":49500},{\"end\":49527,\"start\":49513},{\"end\":49537,\"start\":49527},{\"end\":49550,\"start\":49537},{\"end\":49840,\"start\":49827},{\"end\":49855,\"start\":49840},{\"end\":49864,\"start\":49855},{\"end\":49877,\"start\":49864},{\"end\":49891,\"start\":49877},{\"end\":49904,\"start\":49891},{\"end\":49913,\"start\":49904},{\"end\":49926,\"start\":49913},{\"end\":50217,\"start\":50206},{\"end\":50226,\"start\":50217},{\"end\":50240,\"start\":50226},{\"end\":50252,\"start\":50240},{\"end\":50261,\"start\":50252},{\"end\":50274,\"start\":50261},{\"end\":50531,\"start\":50517},{\"end\":50542,\"start\":50531},{\"end\":50552,\"start\":50542},{\"end\":50565,\"start\":50552},{\"end\":50579,\"start\":50565},{\"end\":50832,\"start\":50820},{\"end\":50845,\"start\":50832},{\"end\":50856,\"start\":50845},{\"end\":50867,\"start\":50856},{\"end\":50882,\"start\":50867},{\"end\":51169,\"start\":51157},{\"end\":51181,\"start\":51169},{\"end\":51191,\"start\":51181},{\"end\":51204,\"start\":51191},{\"end\":51213,\"start\":51204},{\"end\":51228,\"start\":51213},{\"end\":51511,\"start\":51501},{\"end\":51525,\"start\":51511},{\"end\":51539,\"start\":51525},{\"end\":51558,\"start\":51539},{\"end\":51566,\"start\":51558},{\"end\":51585,\"start\":51566},{\"end\":51707,\"start\":51695},{\"end\":51719,\"start\":51707},{\"end\":51730,\"start\":51719},{\"end\":51743,\"start\":51730},{\"end\":51755,\"start\":51743},{\"end\":51769,\"start\":51755},{\"end\":51779,\"start\":51769},{\"end\":52091,\"start\":52081},{\"end\":52103,\"start\":52091},{\"end\":52117,\"start\":52103},{\"end\":52136,\"start\":52117},{\"end\":52156,\"start\":52136},{\"end\":52171,\"start\":52156},{\"end\":52469,\"start\":52458},{\"end\":52480,\"start\":52469},{\"end\":52624,\"start\":52613},{\"end\":52635,\"start\":52624},{\"end\":52911,\"start\":52900},{\"end\":52923,\"start\":52911},{\"end\":52934,\"start\":52923},{\"end\":52948,\"start\":52934},{\"end\":52961,\"start\":52948}]", "bib_venue": "[{\"end\":44505,\"start\":44419},{\"end\":44991,\"start\":44938},{\"end\":45625,\"start\":45533},{\"end\":46068,\"start\":46011},{\"end\":46452,\"start\":46348},{\"end\":46696,\"start\":46639},{\"end\":46950,\"start\":46893},{\"end\":47271,\"start\":47181},{\"end\":47802,\"start\":47749},{\"end\":48662,\"start\":48570},{\"end\":48996,\"start\":48918},{\"end\":49308,\"start\":49233},{\"end\":49747,\"start\":49657},{\"end\":50151,\"start\":50047},{\"end\":50405,\"start\":50348},{\"end\":50776,\"start\":50686},{\"end\":51107,\"start\":51003},{\"end\":51453,\"start\":51349},{\"end\":52004,\"start\":51900},{\"end\":52368,\"start\":52278},{\"end\":52860,\"start\":52756},{\"end\":53088,\"start\":53033},{\"end\":44155,\"start\":44131},{\"end\":44417,\"start\":44316},{\"end\":44695,\"start\":44649},{\"end\":44936,\"start\":44868},{\"end\":45157,\"start\":45106},{\"end\":45336,\"start\":45285},{\"end\":45531,\"start\":45424},{\"end\":45792,\"start\":45733},{\"end\":46009,\"start\":45937},{\"end\":46346,\"start\":46227},{\"end\":46637,\"start\":46565},{\"end\":46891,\"start\":46819},{\"end\":47179,\"start\":47074},{\"end\":47430,\"start\":47371},{\"end\":47546,\"start\":47538},{\"end\":47747,\"start\":47679},{\"end\":47903,\"start\":47810},{\"end\":48124,\"start\":48036},{\"end\":48356,\"start\":48297},{\"end\":48568,\"start\":48461},{\"end\":48916,\"start\":48823},{\"end\":49231,\"start\":49141},{\"end\":49655,\"start\":49550},{\"end\":50045,\"start\":49926},{\"end\":50346,\"start\":50274},{\"end\":50684,\"start\":50579},{\"end\":51001,\"start\":50882},{\"end\":51347,\"start\":51228},{\"end\":51636,\"start\":51585},{\"end\":51898,\"start\":51779},{\"end\":52276,\"start\":52171},{\"end\":52531,\"start\":52480},{\"end\":52754,\"start\":52635},{\"end\":53031,\"start\":52961}]"}}}, "year": 2023, "month": 12, "day": 17}