{"id": 257952516, "updated": "2023-10-05 02:20:19.446", "metadata": {"title": "A Bibliometric Review of Large Language Models Research from 2017 to 2023", "authors": "[{\"first\":\"Lizhou\",\"last\":\"Fan\",\"middle\":[]},{\"first\":\"Lingyao\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Zihui\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Sanggyu\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Huizi\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Libby\",\"last\":\"Hemphill\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Large language models (LLMs) are a class of language models that have demonstrated outstanding performance across a range of natural language processing (NLP) tasks and have become a highly sought-after research area, because of their ability to generate human-like language and their potential to revolutionize science and technology. In this study, we conduct bibliometric and discourse analyses of scholarly literature on LLMs. Synthesizing over 5,000 publications, this paper serves as a roadmap for researchers, practitioners, and policymakers to navigate the current landscape of LLMs research. We present the research trends from 2017 to early 2023, identifying patterns in research paradigms and collaborations. We start with analyzing the core algorithm developments and NLP tasks that are fundamental in LLMs research. We then investigate the applications of LLMs in various fields and domains including medicine, engineering, social science, and humanities. Our review also reveals the dynamic, fast-paced evolution of LLMs research. Overall, this paper offers valuable insights into the current state, impact, and potential of LLMs research and its applications.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2304.02020", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2304-02020", "doi": "10.48550/arxiv.2304.02020"}}, "content": {"source": {"pdf_hash": "07f07d4d59fdbc3596284f51057cb006779d42c1", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2304.02020v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "3b1f42ef8bf1ccd43e60afb78fae7a4302e2c7fa", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/07f07d4d59fdbc3596284f51057cb006779d42c1.txt", "contents": "\nA Bibliometric Review of Large Language Models Research from 2017 to 2023\n\n\nLizhou Fan \nSchool of Information\nUniversity of Michigan\nAnn ArborMI\n\nLingyao Li \nZihui Ma \nSchool of Information\nUniversity of Michigan\nAnn ArborMI\n\nDepartment of Civil and Environmental Engineering\nUniversity of Maryland\nCollege ParkMD\n\nSanggyu Lee \nDepartment of Civil and Environmental Engineering\nUniversity of Maryland\nCollege ParkMD\n\nHuizi Yu \nSchool of Information\nUniversity of Michigan\nAnn ArborMI\n\nLibby Hemphill \nSchool of Information\nUniversity of Michigan\nAnn ArborMI\n\nA Bibliometric Review of Large Language Models Research from 2017 to 2023\nBibliometric analysisDiscourse analysisLarge language modelsScholarly Collaboration networksTopic modeling\nLarge language models (LLMs) are a class of language models that have demonstrated outstanding performance across a range of natural language processing (NLP) tasks and have become a highly sought-after research area, because of their ability to generate human-like language and their potential to revolutionize science and technology. In this study, we conduct bibliometric and discourse analyses of scholarly literature on LLMs. Synthesizing over 5,000 publications, this paper serves as a roadmap for researchers, practitioners, and policymakers to navigate the current landscape of LLMs research. We present the research trends from 2017 to early 2023, identifying patterns in research paradigms and collaborations. We start with analyzing the core algorithm developments and NLP tasks that are fundamental in LLMs research. We then investigate the applications of LLMs in various fields and domains including medicine, engineering, social science, and humanities. Our review also reveals the dynamic, fast-paced evolution of LLMs research. Overall, this paper offers valuable insights into the current state, impact, and potential of LLMs research and its applications.In our discourse and bibliometric analysis, we have identified the research paradigms and collaborations of LLMs research using computational methods, namely topic modeling and\n\nIntroduction\n\nOn March 14, 2023, OpenAI announced the release of their newest version of the large language model (LLM) - GPT-4 (OpenAI, n.d.;Sanderson, 2023). This state-of-the-art LLM powers many of OpenAI's popular AI applications, including the widely used ChatGPT, and brings much attention to the research of LLMs. An LLM is a class of language models that employs neural networks with billions of parameters, trained on gigantic amounts of unlabelled text data through self-supervised learning (Y. Shen, Heacock, et al., 2023;Zhao et al., 2023). LLMs are often based on transformers, a type of neural network architecture that is designed to process sequential data. Transformers use self-attention mechanisms to compute contextual relationships between the input tokens, allowing them to effectively capture long-range dependencies and contextual information (Vaswani et al., 2017). The emergence of LLMs in 2018 has ushered in a paradigm shift in natural language processing (NLP) research, as they have demonstrated outstanding performance across a range of tasks (Devlin et al., 2018;Radford et al., 2018). LLMs are designed to have general-purpose capabilities, which enable them to excel across a broad spectrum of NLP tasks (Wei et al., 2022), rather than being designed solely for a single NLP task, such as sentiment analysis, named entity recognition, or text classification. Typical LLMs include Bidirectional Encoder Representations from Transformers (BERT) developed by Google (Devlin et al., 2018), Generative Pre-trained Transformer (GPT) families developed by OpenAI (Eloundou et al., 2023), and Large Language Model Meta AI (LLaMa) by Meta (Meta, 2023).\n\nAlthough previous scientific literature has emphasized the potential of LLMs in various NLP tasks, including specialized applications in fields such as medical and health sciences (Ding et al., 2022;Khare et al., 2021;Yu et al., 2022) and politics (Y. Hu et al., 2022;, much of the current research has been limited to specific NLP tasks or applications. With the recent release of the latest and most advanced GPT model (OpenAI, n.d.;Sanderson, 2023), LLMs have become a highly sought-after research area, attracting researchers to develop state-of-the-art LLMs, e.g. LLaMa and Bard (Meta, 2023;Pichai, 2023), and to explore their capabilities, e.g. Alpaca and GPTHuggingface (Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto, March, 13 2023;Y. Shen, Song, et al., 2023). Consequently, a bibliometric review to examine current LLMs research has become increasingly essential. While previous research has highlighted the potential and superiority of LLMs in NLP tasks, few studies have conducted a systematic analysis of the latest trends, opportunities, and challenges within the field of LLMs.\n\nTo gain insight into the state of LLMs research, this paper presents a comprehensive overview of current studies covering the research paradigms and collaborations in their development and applications. In particular, we focus on the discourse and bibliometric aspects, including,\n\n\u2022 Research paradigms, the themes through topic modeling and discourse analysis of LLMs, from algorithms and NLP tasks to applications, infrastructures, and critical studies; \u2022 Research collaborations, the scholarly collaboration networks, from the international and organizational perspectives.\n\nThe significance of this paper lies in two main aspects. Firstly, it presents an up-to-date bibliometric analysis of the state-of-the-art studies in LLMs, identifying trends and patterns that deepen understanding of the topic. Secondly, by analyzing the existing literature, our paper serves as a roadmap for researchers, practitioners, and policymakers to navigate the current landscape, pinpoint knowledge gaps and research opportunities, thereby fostering innovation and advancing the field toward breakthroughs.\n\n\nBackground\n\nLLMs are pre-trained language models models that use deep learning techniques to process and comprehend natural language (Y. Shen, Heacock, et al., 2023;Zhao et al., 2023). LLMs are trained and fine-tuned on vast amounts of text data, which allows them to learn patterns in unstructured sequences and build a knowledge base of language Radford et al., 2019). LLMs offer outstanding advantages over conventional NLP models. In contrast to the conventional approach for NLP tasks, which involves fine-tuning models through supervised learning on small, task-specific datasets, LLMs can effectively perform a wide range of tasks with only a few prompts (Manning, 2022). By providing them with human language descriptions or several examples of the desired task, they can execute tasks for which they were not explicitly trained (Manning, 2022). Thus, LLMs require fewer resources and less training time compared to conventional models with similar performance, as they can learn more from the same amount of data (M. Chen et al., 2021).\n\nAs such, LLMs have a broad range of capabilities in performing language-related tasks, such as text generation, translation, and summarization (Ollivier et al., 2023), as well as real-world applications, such as virtual assistants, chatbots, and language translation systems. To better outline the emerging landscapes of LLMs from 2017 to early 2023, in this section, we introduce the history of their developments, followed by their current applications across fields of research.\n\n\nHistory\n\nTraditionally, NLP models such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) had difficulty capturing long-range dependencies between words in a sentence (Hochreiter & Schmidhuber, 1997). This limitation negatively affected language models' performance on NLP tasks such as machine translation, summarization, and question-answering (Sutskever et al., 2014). However, in 2017, Vaswani et al. introduced the Transformer model (Vaswani et al., 2017). The self-attention mechanism used in the model allowed it to attend to all the other tokens in the input sequence by assigning weights to each token. The ability to capture long-range dependencies and parallelizable architecture of the model made it successful in various NLP applications (Devlin et al., 2018). Since the development of the Transformer model, researchers have built on top of the Transformer, developing more advanced language models.\n\nBERT, which stands for Bidirectional Encoder Representations from Transformers, was introduced in 2018 by Devlin et al (Devlin et al., 2018). It is a pre-training technique that utilizes deep bidirectional representations by conditioning on both left and right contexts of all layers. This allowed the pre-trained BERT model to be fine-tuned with one additional output layer, making it suitable for a wide range of tasks such as question answering and language inference. BERT's success has led to its widespread adoption and other pre-trained language models (Y. Liu et al., 2019;Z. Yang et al., 2019). However, its limitation is that the pre-training process is computationally expensive. In 2019, Alec Radford, et al. presented Generative Pre-trained Transformer 2, also known as GPT-2, which was trained on a deep neural network with 1.5 billion parameters (Radford et al., 2019). GPT-2 utilizes a transformer architecture that employs self-attention mechanisms to gather data from various locations in the input sequence. Although the model is computationally expensive to train and run, its large size enables it to understand and generate a wide range of linguistic nuances and diverse outputs. Megatron-LM is another LLM that was developed in 2019, by (Shoeybi et al., 2019). It has 8.3 billion parameters, which is significantly larger than GPT-2's 1.5 billion parameters. This size enables the model to capture and generate more complex linguistic patterns. The model features a new parallelization scheme, which enables faster training compared to other models of comparable size. However, due to its large size, Megatron-LM requires significant computational resources for both training and inference.\n\nIn 2020, the introduction of GPT-3 by OpenAI marked a significant milestone in the development of LLMs . GPT-3 has 175 billion parameters, which is significantly larger than any other LLMs at that time. It can generate high-quality natural language text with little to no fine-turning due to the use of advanced techniques such as a higher layer count and more diverse training data. The introduction of GPT-3 has propelled the field of natural language processing forward. Following the success of GPT-3, researchers have continued to push the boundaries of LLMs. While these recent models are out of the time range for our analysis, they are important proofs that the advancement in LLMs research is increasingly faster -the burst of these research projects is making substantial changes to not only NLP or AI research, but also In 2023, OpenAI announced the development of a new multimodal model called GPT-4, capable of processing both text and image inputs to generate textual outputs . As the field is highly competitive and there are potential safety concerns, the technical paper does not disclose details about the model's architecture, hardware, dataset construction, and training method. However, its performance was evaluated on various professional and academic exams designed for humans. GPT-4 demonstrated human-level performance on most of the exams, and notably, it achieved a score in the top 10% of test takers on a simulated version of the Uniform Bar Examination . There are also newly released open-source LLMs, e.g. LLaMa (Meta, 2023), which are smaller in size and number of parameters but freely available to researchers.\n\n\nApplications\n\nThe advantages of LLMs in language understanding and their ability to generalize to new tasks have resulted in increased application and ongoing development in the field of NLP. Recent research using LLMs has focused on themes such as relation extraction (Gu et al., 2021), dialogue analysis (Thoppilan et al., 2022), text summarization (H. Zhang et al., 2019), sentiment analysis (Araci, 2019), named entity recognition (Nguyen et al., 2020), and text classification (Jin et al., 2020). These research studies have demonstrated that LLMs have the potential to significantly enhance the accuracy and fluency of natural language processing tasks, thereby improving our understanding of human language (Beltagy et al., 2019;Nguyen et al., 2020). In addition, current research on downstream tasks using LLMs has focused on several directions. One direction is fine-tuning, which involves modifying an existing pre-trained language model such as changing the weights in the neural layers by training it in a supervised fashion on a specific NLP task (Jurafsky & Martin, 2023). Another direction is the prompting interactions with LLMs (Reynolds & McDonell, 2021), where the problem to be solved is formulated via \"few-shot prompting\"  or instruction tuning (Maarten Bosma, 2021), in order to enhance the LLMs' performance on given NLP tasks.\n\nThe versatility of LLMs makes them a promising tool for diverse disciplines and research fields. Rather than training specialized supervised models for specific tasks, researchers have utilized LLMs to handle a broad range of applications across multi-disciplinary domains. In the medical field, LLMs are used to analyze electronic health records, laboratory reports, and clinical notes to provide diagnostic assistance (Rasmy et al., 2021;Tang et al., 2021) and offer treatment suggestions (Shang et al., 2019) to healthcare professionals. They are also demonstrated to have the potential to provide AI-assisted medical education (Kung et al., 2023). In engineering, LLMs are utilized to analyze large volumes of engineering documents (Qiu & Jin, 2022), generate emergency plans (X. Liu et al., 2022), and detect and classify defects in maintaining the performance of buildings (D. U. Yang et al., 2022). Similarly, LLMs are applied to analyze social media posts, survey responses, and news articles, facilitating data-driven research in research areas such as sociology (Kawashima & Yamaguchi, 2021;Mustakim et al., 2022), economics (Jagdish et al., 2022;Li et al., 2021), and politics (Y. Hu et al., 2022;Salam et al., 2020).  Web of Science (WoS) Core Collection is a widely recognized platform for retrieving comprehensive academic literature metadata for bibliometric study (Birkle et al., 2020). To gather relevant papers for our analysis of LLMs research, we conducted an advanced search on WoS records. As Table 1 shows, we then used a combination of different keywords related to the LLMs and specific models respectively on the article titles (TI) and the topics (TS), i.e. the combinations of article titles, abstracts, and keywords. The query is as follows, TI=((large or big or massive) and language and (model or models)) or TS = (\"large language model\" or \"large language models\" or BERT or GPT-1 or GPT-2 or GPT-3 or ChatGPT)\n\n\nData and Methods\n\nWe then limited the date range to the start of 2017 (2017-01-01) to early 2023 (2023-02-20) and obtained 5752 publications. \n\n\nTS\n\n\"large language model\" or \"large language models\"\n\nCombining the fixed components that describe possible names of LLMs (detectable only if consecutive) and popular individual LLMs names. BERT or GPT-1 or GPT-2 or GPT-3 or ChatGPT\n\n\nMethods\n\nWe first used topic modeling to analyze the research paradigm of LLMs. Topic modeling is a method that discovers and summarizes latent semantic topics from large-scale unstructured text data, for example, academic literature. It assumes that each text document, for example, a publication, is a combination of multiple topics, where each topic is represented by a probability distribution of words that can be grouped as clusters with similarities (Blei & Lafferty, 2007;Steyvers, 2007).\n\nIn particular, we used BERTopic (Grootendorst, 2022), a neural topic modeling method, to analyze publications in the corpus of LLMs research. First, we use Sentence-BERT (SBERT) (Reimers & Gurevych, 2019), a transformer-based pre-trained NLP model, to obtain sentence embeddings for each of the combinations of title and abstract. In particular, we used the SBERT Python package 1 and the pre-trained model \"all-MiniLM-L6-v2\" 2 . To better handle the high dimensional tweet vectors for clustering, We then used a dimensionality reduction technique (UMAP) to handle the curse of dimensionality problem in our clustering model (McInnes et al., 2018), which enabled us to use the Lloyd's K-Means clustering algorithm to group similar sentences embedding vectors into topics. 3 Given the size of the corpus (more than 500 documents), we experimented with three different numbers of clusters (100, 200, and 400) and chose to use 200 clusters for our analysis since it provides publication groups that are sharing similar topics in each cluster while avoiding too many small clusters. We then proceed to represent LLMs topics and the corresponding research themes. Using the count vectorizer in the scikit-learn Python package, we tokenized topics through cluster-level (topic-level) bag-of-words representation that calculates and vectorizes the frequency of each word in each cluster (Y. Zhang et al., 2010). We then used the class-based term frequency-inverse document frequency (c-TF-IDF) to extract the difference of topical keywords (Grootendorst, 2022), distinguishing among the clusters and representing topics with the unique and frequent words as the keywords of LLMs research themes.\n\nFinally, we characterized the 200 topics based on the keywords, titles, and abstracts of publications in each topic. After removing irrelevant contents, e.g. empty docs with \"nan\" and topics not about LLMs research, we summarized these topics into five higher-level categories, i.e. research themes, as follows:\n\n\u2022 Algorithm and NLP tasks: The computational methods and techniques used to process, analyze, and generate human language in LLMs, performing tasks such as translation, summarization, sentiment analysis, and question-answering, among others; \u2022 Medical and Engineering Applications: Applications leverage LLMs to enhance domain-specific tasks related to healthcare and engineering fields, such as analyzing medical literature, aiding diagnosis, predicting patient outcomes, and facilitating engineering design processes or problem-solving; \u2022 Social and Humanitarian Applications: Applications use LLMs to address societal and humanitarian challenges, from analyzing social issues, supporting disaster response, and enhancing communication, and to promoting educational initiatives; \u2022 Critical Studies: Reflections and examinations of the ethical, social, and political implications of LLMs, which scrutinize LLMs' potential biases, transparency, and impact on society, while also exploring governance, accountability, and strategies for ensuring responsible and equitable AI development and deployment; \u2022 Infrastructures: The underlying systems and resources required for developing, deploying, and maintaining LLMs, examining aspects such as computing power, data storage, networking, and the policies and frameworks that govern their use and development.\n\nTo precisely identify these five research themes, as well as remove the irrelevant publications, two authors annotated all 200 topics respectively. We reached a comparatively high agreement in the first round of annotation (Krippendorff's \u03b1 = 0.76) (Krippendorff, 2018), and reached the full agreement after discussion. For the corresponding visualizations of the annotated results, as well as the LLM publications corpus, we used Tableau to create the trend line, the pie chart, and the document map (in Section 4.1).\n\nTo further study collaborations in LLMs research, we also used network methods and visualization features to study scholarly collaboration networks. In particular, we leverage a bibliometrics analysis software, CiteSpace (C. Chen, 2016), to generate co-citation and collaboration networks of LLMs publications (in Section 4.2). CiteSpace offers an essential co-citation analysis function to identify significant publications in a research field. Co-citation relationships occur when two or more papers are cited by one or more later papers at the same time. To cluster network nodes, the software employs the expectation maximization (EM) algorithm, which is an iterative algorithm that partitions data into clusters by maximizing the likelihood function based on attributes such as citation frequency and betweenness centrality (BC). The EM algorithm is a hard clustering method, which means that each reference can only belong to one cluster (C. Chen et al., 2010).\n\nTo start the clustering process, the algorithm assigns each reference to an initial random cluster, and then iteratively updates the cluster assignments based on the likelihood of the data given the cluster assignments. This process continues until the algorithm converges to a stable solution. The resultant clusters are non-overlapping and are subsequently labeled and summarized by the built-in algorithm. The co-citation knowledge graph visualizes the connections between the literature, and nodes that are closely linked in the co-citation mapping frequently appear in the same literature (Niu et al., 2022). This indicates that the co-cited articles must be similar in content, and a higher co-citation value reflects a stronger connection between them due to greater similarity in content.\n\nCollaboration network analysis is based on social network theory, which originated from the anthropological and sociological exploration of interpersonal relationships in complex social clusters (Z. . By analyzing the collaborative relationships between countries, institutions, and authors, CiteSpace can provide insight into the overall social status in the research field and facilitates the understanding of scholarly communication and knowledge diffusion in a particular research field. In addition, CiteSpace can track the development of a research field over time by analyzing publications from different years. This feature allows researchers to identify emerging trends and track the evolution of research areas. The field of LLMs has gained significant attention and interest from researchers in recent years.\n\nAs Figure 2(a) shows, there is a steady increase in the number of publications on LLMs from 2017 to 2023, 4 with a sharp spike from 2019 to 2020, likely due to increased interest in transformer-based NLP algorithms, e.g., SBERT and BERTopic (first released on September 2020) (Grootendorst, 2022;Reimers & Gurevych, 2019), and the public release of advanced LLM models, e.g., GPT-3 (Brown et al., 2020). The trend continues to rise in 2021 and after, indicating that the field of LLMs is still growing and evolving. This trend suggests that there is still much to explore and discover in the field of LLMs, and researchers are likely to continue studying and developing these models in the coming years.\n\n\nFigure 2. LLMs research trends and themes\n\nResearch on LLMs also spans a wide range of themes, including Algorithm and NLP tasks, Social and Humanitarian Applications, Medical and Engineering Applications, Critical Studies, and Infrastructures. As the pie chart in Figure 2(b) shows, publications in the field of LLMs can be divided into several themes, each representing a specific theme or subfield. The largest research theme, Algorithm and NLP tasks, represents more than half (54%, 2980 out of 5527) of all publications in the LLM field. This theme focuses on the development and refinement of LLM architectures and modeling techniques, some of which are applicable to specific NLP tasks. The next largest research theme, Social and Humanitarian Applications, accounts for about a quarter (25%, 1387 out of 5527) of the publications. This theme includes studies that apply LLMs to specific social issues, such as controversial speech and the COVID-19 pandemic, and humanities research, such as sentiment analysis and language translation. The third largest theme, Medical and Engineering Applications, represents around 18% (1006/5527) of the publications. This theme involves the use of pre-trained LLMs and fine-tuning them to automate specific medical and engineering tasks, such as health record processing and software similarity analysis. The remaining two themes of the pie chart are relatively smaller in size, each representing less than 2% of the publications. These themes are Critical Studies, which focuses on the ethical and social implications of LLMs, and Infrastructures, which focuses on developing and enhancing hardware and cloud computing resources that can support LLMs.\n\nOverall, the snapshot of themes of publications demonstrates the different areas of focus in LLMs research, showing that the field is diverse and covers a wide range of topics and subdomains. Figure 3 shows a 2D mapping of topic modeling results of publications in the LLMs from 2017 to 2023. 5 Each point on the figure represents a publication. 6 This colored documentation map is divided into research theme clusters, each representing a specific group of publications that share similar topics on LLMs as defined in Section 4.1.1. In general, there are no standing-alone clusters for the larger research themes such as Algorithm and NLP Tasks and Social and Humanitarian Applications. These two themes are mixed around the map, indicating the high semantic closeness among many topics in these two clusters. For example, as the two black frames highlight, sentiment and emotion analysis are NLP tasks that require algorithm development (Topic 65 and Topic 103), which also have corresponding applications in social and humanitarian aspects (Topic 63 and Topic 28). These two themes (marked as blue and orange dots) are also scattered all around the map, which indicates that there are topics in research themes of Algorithm and NLP Tasks and Social and Humanitarian Applications themes that are related to the other three themes, demonstrating the active scholarly communication among different subdomains in LLMs research.\n\n\nTopical research themes and key discourses\n\nThe map also reveals several interesting patterns in the landscape of LLMs publications, as highlighted by frames with corresponding colors. First, Medical and Engineering applications are often located on the upper middle part of the map, indicating a comprehensive cluster of various highly professional and semantically related sub-domains in LLMs research. For example, Topic 64 focuses on using LLMs to study specific categories of diseases such as Alzheimer's and Dementia, Topic 95 focuses on drug use and health services, Topic 87 focuses on biomedical advice and precision medicine, Topic 45 focuses on technique in biomedical relations extraction, and Topic 23 focuses on processing electronic medical records in different languages such as Chinese, all of which are related to medical and health research. Other subdomains in this theme, mostly engineering applications are located on other parts of the map (outside of the dotted green frame), which are closely related to the Algorithms and NLP Tasks and the Social and Humanitarian Applications research themes. Second, Critical studies (highlighted in purple dashed frames) are also semantically close to what they critically analyzed. For example, Topic 60 contains critical studies on bias in LLMs, which are closely related to LLM applications that deal with cyberbullying and abusive comments (Topic 10). Similarly, Topic 125 covers privacy concerns related to LLMs, which is closely related to anti-attack explorations in LLM applications (Topic 141). Finally, the Infrastructure theme (highlighted in red frames) focuses on parallel and distributed computing with GPU (Topic 58) and hardware and accelerator (Topic 94), which enable scalability and enhance efficiency in LLMs research.\n\nAs a whole, the map provides a visual representation of the different topics and themes that have emerged within the LLMs research community, revealing patterns and subfields that may not be immediately apparent from a simple analysis of publication keywords or titles. To further demonstrate how the topic modeling results and research themes correspond, we provide the details of the topical keywords and theme labels in Appendix B.\n\n\nFigure 3. A 2D map of LLMs publication embeddings with research themes\n\nTo elaborate on the key discourse under each major theme, we analyze the keywords 7 in each of the corresponding co-citation networks (Figure 4). In the Algorithm and NLP Tasks co-citation network (Figure 4(a)), the keywords of the central clusters are related to general aspects of NLP and machine learning algorithms, such as \"natural language inference\" (#12) and \"machine learning comprehension\" (#3). The peripheral clusters often have keywords with specific NLP tasks. Both the central and peripheral keywords indicate important and promising directions that have attracted attention, which are great references to new researchers and other stakeholders like publishers and funders caring about LLMs research.\n\nIn the two other co-citation networks of LLM applications, there are less obvious center clusters, which show diverse and multifaceted development among subdomains. In the Medical and Engineering Applications co-citation network (Figure 4(b)), the keywords suggest that the most important LLMs research themes in medical and engineering areas are related to the application of pre-trained models and NLP techniques. These applications depend on a few core NLP tasks such as named entity recognition (NER) and contextualized word embedding to support a wide range of use cases from medical tasks (e.g. clinical textual semantic similarity) to engineering (e.g. software similarity). In the Social and Humanitarian Applications co-citation network (Figure 4(c)), some representative keywords include \"fake news\", \"twitter\", \"hate speech\", \"rumor detection\", and \"argumentation mining\". These keywords suggest that the popular themes in this sub-domain are related to the analysis of social media and news data, particularly with respect to sentiment, opinion, and controversial content. showcases a knowledge mapping of the distribution network for national and international collaborations, which was generated using CiteSpace. In this mapping, the centrality of a country in the collaboration network is represented by its degree, while the number of papers published from the country in our dataset is denoted by its publication frequency. Since the selected papers were sourced from recognized international journals in Web of Science, it is reasonable to conclude that the degree of centrality and frequency of publications identified in this bibliometric study reflects the importance of studies in LLM to some extent. These findings can provide valuable insights for researchers working in the field of LLM, both in current and future international collaborations.\n\nIn   To gain a comprehensive understanding of the collaboration process over time, we plotted a cluster analysis of international collaboration over years, as shown in Figure 6. The major collaborations began in 2018 among countries in different continents (Figure 6(a)), including USA, China, Germany, South Korea, India, England, and Japan. In 2019 (Figure 6(b)), additional countries and regions participated in the international collaboration related to LLMs research, including Sweden, Greece, Spain, and Italy, which were mostly from Europe.\n\nThe increasing number of participating countries in LLMs research collaborations until 2022 suggests a growing interest in this research area among researchers worldwide (Figure 6(c)). This trend not only highlights the popularity of LLMs research, but also demonstrates its global significance as a research domain. Overall, Figure   Building on the insights gained from previous analysis, we conducted further analysis and plotted the collaboration networks of active countries and regions for the identified research themes in Section 4.1. The results are presented in Figure 7. In examining the networks for the three most popular research themes, we found that the USA and China are at the forefront of research on algorithm and NLP tasks (Figure 7(a)) as well as medical and engineering applications, as illustrated by the relatively larger circle sizes in comparison to others. Additionally, we observed that several countries and regions such as Germany, England, and India remain central across all three themes, indicating their continued importance in the collaboration network.\n\nSpecifically, when examining the theme of Algorithms and NLP Tasks, we found that the USA and China are the two leading countries. However, Japan, Canada, South Korea, England, and Germany are also central in the network (Figure 7(b)). Russia tends to collaborate on Algorithms and Medical and Engineering Applications but does not show frequent participation in the other two research themes. In the theme of Medical and Engineering Applications, countries active in Algorithm and NLP Tasks are still active in this area. We also observed that countries or regions, such as Saudi Arabia, Australia, and Sweden, which are not actively engaged in algorithm studies, show frequent collaboration in this research theme. In comparison to the first two research themes presented in Figure 7, the Social and Humanitarian\n\nApplications theme shows a more distributed network, with several countries occupying central positions (Figure 7(c)). For instance, India is predominantly involved in collaborations on such applications. The decentralized nature of this theme indicates that research in social and humanitarian areas is not dominated by a single country or region, but rather involves collaborations between many researchers around the world.  Although these works laid the foundation for subsequent collaborations, some of the institutions that collaborated during that time did not remain at the center of the network. For instance, in 2017, New York University, the United States Navy, the United States Department of Defense, and Beijing University of Technology were involved in early works but did not stay at the center of the collaboration network. We also observed that certain institutions could carry out the implementations separately. For instance, in 2018, the Max Planck Society collaborated with other institutions to create a cluster, but its impact did not seem to endure.\n\nMost recent research projects have been conducted through partnerships between academic and industrial organizations. Notable examples include collaborations between major tech companies and university systems, such as the joint efforts between the California University System and Stanford University with Microsoft and Google in the United States, as well as between Tsinghua University and Peking University with Tencent in China. It was also worth noting that these recent significant research projects might not have directly involved the earlier institutions mentioned. Moreover, in later years, the central entities in the collaboration network remained relatively constant, while various peripheral organizations began to contribute to the research efforts.\n\nRegarding collaboration patterns, it was discovered that universities have been the primary contributors to LLMs research collaborations, as illustrated by that 14 out of the top 20 organizations in Table 3 are universities. Although universities continue to play a crucial role in these endeavors, large tech corporations such as Google, Microsoft, Meta (formerly Facebook), and Tencent have also become increasingly significant collaborators. As previously noted, the combination of academic and industrial organizations has resulted in numerous significant works in this field, emphasizing the importance of cooperation between academia and industry.  The dynamic nature and fast evolution of LLMs research have led to significant advancements in natural language understanding and processing capabilities, with applications across diverse domains such as Medical, Engineering, Social, and Humanitarian fields. The synergistic workforce in LLMs research involving international and organizational collaborations plays a crucial role in the growth and development of this research area.\n\nHowever, challenges remain due to the current movements and tensions in the development and application of LLMs. The power of LLMs is not yet clearly or openly analyzed before the release of end-user tools, such as ChatGPT and GPT-4 (Fridman, 2023). There is also a division between proponents and opponents of LLMs research and application, for instance, the open letter to pause giant AI experiments (Future of Life Institute, n.d.).\n\nWe regard our study as a glimpse of the modern history of LLMs research, which can be informative to newcomers of this field, policy makers of AI regulations, as well as researchers in science and technology studies. It is hard to predict the future of LLMs, while understanding its past can at least provide the knowledge foundation and warnings for future research.\n\n\nThe dynamic nature and fast evolution of LLMs research: from algorithms to applications and beyond\n\nIt is vital to recognize the dynamic nature and fast evolution, as well as the corresponding opportunities and challenges, of the research field of LLMs. The growing interest and the diverse range of themes indicate a promising future for new discoveries and advancements. As researchers continue to explore and develop novel algorithms, techniques, and applications, it is vital to recognize the dynamic nature of the research field of LLMs. The current ability of LLM algorithms has significantly improved natural language understanding and processing capabilities. These advancements have enabled researchers to tackle complex language tasks that are applicable to handle a wide range of applications across domains, including Medical, Engineering, Social, and Humanitarian fields of research. In the Social and Humanitarian domain, LLMs have been applied to analyze social media and news data, particularly with respect to sentiment, opinion, and controversial content. In the Medical and Engineering domain, LLMs are utilized to solve complex problems, from processing electronic medical records and studying specific categories of diseases to automating software similarity analysis. These diverse applications showcase the power and versatility of LLMs in addressing real-world challenges and driving advancements in various fields.\n\nFrom algorithms to applications in LLMs research, there is smooth knowledge transfer among different subdomains, including specialized applications. The high semantic closeness among Algorithm and NLP Tasks and Social and Humanitarian Applications (Figure 3), for instance, indicates that researchers in these fields work across disciplines and share insights and expertise to develop novel solutions. This interdisciplinary approach implies that challenges in LLMs research related to Social and Humanitarian Applications are not confined to a single solution, while researchers from different backgrounds can contribute to and benefit from shared knowledge and expertise. Such collaboration can lead to more efficient problem-solving approaches and result in more impactful and far-reaching social and humanitarian applications. Moreover, the Medical and Engineering Applications theme is a comprehensive cluster of various highly professional and semantically related sub-domains in LLMs research, showing the adaptivity of LLMs algorithms. By applying pre-trained LLMs and fine-tuning them for specific tasks, researchers can leverage the power of LLMs to improve healthcare and advance engineering practices. Specifically, many top keywords among publications in Medical and Engineering Applications are general LLMs algorithms and NLP tasks, such as named entity recognition (NER) and question-answering (QA) (Figure 4(b)). This adaptability highlights the immense potential for LLMs to revolutionize various industries and contribute to overall societal progress.\n\nAt the same time, challenges remain in the development and application of LLMs, many of which are due the complexity and uncertainty in the dynamic nature and fast evolution of LLMs (Johanna Okerlund, Evan Klasky, Aditya Middha, Sujin Kim, Hannah Rosenfeld, Molly Kleinman, Shobita Parthasarathy, 2022;Weidinger et al., 2021). Some algorithms are not widely applied due to a variety of factors, such as computational complexity, lack of interpretability, or ethical concerns. Computational complexity can limit the scalability of certain LLMs, making them less accessible for researchers with limited resources, as well as exacerbating environmental injustice and social fragmentation. Vast computing power is required to train LLMs, coming at a significant environmental cost, while these models are not outperforming more eco-friendly models in many use cases (Goetze & Abramson, 2021). In addition, the lack of interpretability in LLMs may hinder trust and adoption in critical applications, as users may be hesitant to rely on \"black box\" solutions. For example, in question-answering systems or chatbots, models can mimic human-like thought and behavior, like \"stochastic parrots\", without fully understanding the implications of such technology (Bender et al., 2021).\n\nFurthermore, ethical concerns regarding biases, privacy, and other unintended consequences may prevent the widespread use of certain LLMs. First, these models can carry on existing biases in society and exacerbate them through fast and low-cost applications. For example, there is persistent anti-Muslim bias in some LLMs (Abid et al., 2021). Second, it is possible to extract personally identifiable information (PII) and other sensitive information from LLMs, raising the possibility that the massive dataset use in models can result in privacy information leaks. (Carlini et al., 2020). Other malicious uses of LLMs, such as spreading disinformation or creating fake news, can also strengthen bias and lead to social factorization problems (Guembe et al., 2022;Yamin et al., 2021). Therefore, addressing these challenges is essential to ensure the responsible and inclusive development and application of LLMs in the future.\n\n\nThe synergistic workforce in LLMs research: international and organizational collaborations\n\nThe degree of centrality and frequency shown by the scholarly collaboration networks reflect the importance of collaborations among countries and institutions studying LLMs, which can serve as a guide for researchers seeking to explore and engage in relevant research activities. These findings can also inspire and inform other stakeholders, e.g. funding agents, science and technology policymakers, and non-profit organizations, to adjust their agenda for more impactful presences in LLMs research. In general, there are valuable opportunities for researchers and other stakeholders to work together, exchange ideas, and generate knowledge that can inform policy and practice in addressing LLM needs.\n\nThe growing interest and participation in LLMs research collaborations demonstrate the global significance of this research area. We have observed that the trend towards international cooperation up to 2022 in LLM studies is gaining momentum, with an increasing number of countries and institutions joining the effort. While some countries and institutions remain at the forefront of this movement, it is encouraging to see a growing inclusive and diverse research community that brings scholars together from various backgrounds. We strongly support international collaboration for applying LLM to different contexts. In particular, one advantage we have observed is that many researchers have applied LLMs to address applications focusing on linguistic and cultural differences (J. Hu & Sun, 2020;Kim et al., 2021;Le et al., 2019).\n\nIn addition, our study provides some valuable insights for researchers who seek to identify potential partners, assess the research landscape, and discover new opportunities for collaboration. In particular, we have observed that certain institutions, such as the California University system and Stanford University in the United States, as well as Tsinghua University and Peking University in China, and companies like Microsoft and Google, have high publications and degrees in the collaboration networks. We have also observed that some institutions may have special research strength, for instance, the Indian Institute of Technology System has a strong record of publications in social and humanitarian applications. In sum, these institutions possess a strong foundation of knowledge, professional researchers, and computing resources that support LLM studies. It is essential for these leading institutions to take responsibility for the development of LLMs and provide opportunities for other institutions to join LLMs research topics in the future. For institutions interested in participating in LLMs research, seeking collaboration opportunities with these leading universities or companies could obtain access to cutting-edge resources and tools. As we have seen that some late participants in LLM studies have become influential, we believe that by leveraging these collaborations, latecomers can expand their research capabilities and contribute to the advancement of LLM studies.\n\nMoreover, our analysis of institutional collaboration networks reveals that academia and industry maintain a close relationship in the field of LLM studies. This collaboration presents significant opportunities for both parties. Industry can provide academic researchers with access to advanced computing resources, such as cloud computing and graphics processing units, as well as financial support. Meanwhile, academia can leverage these resources to explore algorithms and solutions and help industry to test and validate real-world language processing problems. We believe that this collaboration can foster knowledge sharing between academia and industry, which can help bridge the gap between academic research and industry applications. Our findings are consistent with a previous study that emphasized the importance of strengthening the public AI research sphere in university-industry interactions to ensure equitable development of AI technology (Jurowetzki et al., 2021).\n\nFinally, to ensure successful collaboration, we believe that it is crucial for institutions and corporations involved to understand and fulfill their roles in LLMs research. For instance, government agencies such as the United States Navy and the Department of Defense in the network, play an important role in shaping science and technology policies to regulate the applications of LLM applications in real cases. Universities, as the main body of collaboration in networks, should bring a multidisciplinary perspective to explore the research frontier such as identifying new areas of inquiry and optimizing the development of LLMs. Industry companies, which have more resources than other institutions in the network, should take social responsibility when deploying LLMs and ensure adequate supervision in place to mitigate potential risks. Data creators, whether researchers or companies, should provide specific instructions and regulations for those who use their data so that data is used ethically and in ways that align with the goals of the collaboration. Infrastructure service providers need to take into account the needs of LLMs and ensure that their infrastructure system is optimized to support these needs, such as ensuring necessary computing power and storage capacity.\n\n\nLimitations and research outlook\n\nThere are several limitations in our study due to the scope, the method, and the availability of bibliometric data. One limitation is regarding the paper selection. Through full-text query in Web of Science Core Collection, a few papers may be irrelevant to LLMs research but got included because of including similar keywords or abbreviations. For example, a paper is selected because of the inclusion of \"Bert et al.\", a citation of an author whose last name happens to be \"BERT\", the abbreviation of Bidirectional Encoder Representations from Transformers (Devlin et al., 2018). We removed them based on the topic modeling results and human annotations of research themes.\n\nOne other limitation is the topic modeling process. A few papers in a topic cluster don't look the same as other papers, not the category of the topic. For example, under the Critical Studies research theme (Topic 125), several papers have words indicating critical analyses or concerns of LLMs, e.g. \"malicious\", and \"social connectedness\", while they actually focus on specific engineering concepts that happened to include those keywords or relevant applications. We experimented with using SciBERT (Beltagy et al., 2019), which can improve some peripheral clustering results, while the overall topical coherence is less than the default BERT model as word embeddings. We thus stick to the default BERT model \"all-MiniLM-L6-v2\" which generates overall informative and comprehensive word embeddings for clustering.\n\nThere is another limitation because of the availability of LLMs research on the Web of Science. First, it is important to notice that not every large language model provides timely and public access to academia. The technical details of some LLMs are not represented by their bibliometric data. For example, the training and testing of GPT-4 are finished months before its report is available (Eloundou et al., 2023;OpenAI, n.d.). Because many LLMs research is not freely open to the public, some relevant research, especially those critics and large-scale experiments from non-partner organizations that develop the LLM, have to be delayed until there is funding or resources available. Second, some research articles on LLMs are conference papers and only exist on preprint websites such as Arxiv.org. While some LLMs are not included in our publications, the publications on the WoS core collection is representative for our analysis of international and organizational collaborations.\n\nThe field of LLMs is rapidly evolving, with new research and developments emerging at a fast pace. While this paper discusses the state-of-the-art techniques, it is likely that some of these will be surpassed by more recent advancements. As such, it is important to not only address the limitations outlined in this paper but also to stay up to date with the latest developments in LLM research. With the expected growth in the number of publications related to LLMs in 2023, we anticipate a potential publication and citation burst in the coming years, and therefore, we aim to continue monitoring this trend to ensure that our research remains relevant and impactful.\n\n\nConclusion\n\nIn this study, we have applied discourse and bibliometric analysis on over 5,000 LLMs research papers from 2017 to early 2023, surveying the emerging and expanding landscapes of their paradigms and collaborations. The rapid evolution of LLMs has resulted in significant advancements in NLP, with diverse applications across domains. Interdisciplinary, inter-organizational, and international collaborations drive these developments, fostering an inclusive research community and enabling smooth knowledge sharing. However, challenges persist, such as computational complexity, lack of interpretability, and ethical concerns in designing and applying LLMs. There is a need to call for further openness and cooperation among stakeholders, including government agencies, universities, companies, data creators, and infrastructure service providers, to ensure the responsible development and application of LLMs.\n\n\nAppendices\n\nA. Topic word scores  \n\nFigure 1\n1indicates our workflow in collecting scholarly literature metadata and analyzing them. We first collected bibliometric data of LLMs research literature. Then, we analyze research paradigms and collaborations using the discourse under research themes and the scholarly collaboration networks.\n\nFigure 1 .\n1Overall\n\nFigure 4 .Figure 5\n45Keywords and representative publications of major LLMs research themes\n\nFigure 5 .\n5Overall with top active countries and regions\n\n\nX offers valuable insights into the evolution of international collaboration in LLMs research. Although the figure only depicts the current international collaboration status, it conveys the expansion of LLMs research beyond traditional boundaries and the emergence of new collaborative networks in this field.\n\nFigure 6 .\n6Collaboration networks of active countries and regions in selected years\n\nFigure 7 .Figure 8\n78Collaboration networks of active countries and regions by research themes displays the distribution network of contributing organizations for research papers related to LLM.Table 3lists the top organizations in the institutional collaboration network. The Chinese Academy of Sciences ranks first with 205 published papers, followed by the University of California System and Microsoft. Other notable institutions in the center of the network include Tsinghua University, Stanford University, Google, the University of Texas System, and Massachusetts Institute of Technology. The ranking of these institutions in terms of centrality mostly aligns with the number of papers they have published. As such, the organizations with the most published papers are also the most important and located at the center of the collaboration network. Out of the top 20 organizations with the highest degree, 14 are universities, 2 are research institutions, and 4 are tech companies.\n\nFigure 8\n8also displays the collaboration over different years. Interestingly, we observed that early works, such as publications in 2017 and 2018, are often peripheral to the network.\n\nFigure 8 .\n8Collaboration networks of active organizations, from overview and selected years\n\nFigure 9 .\n9Example topic word scores B. Topic modeling and research themes\n\nTable 1 .\n1Web of Science search keywordsSearch Fields Keywords \nSearch Logic and Purposes \n\nTI \nlarge or big or massive \nCombining the three components that \ndescribe possible names of LLMs \nusing and. \nlanguage \n\nmodel or models \n\n\n\n4 .\n4Results 4.1 Research paradigms of LLMs: from algorithms and NLP tasks to applications, infrastructures, and critical studies 4.1.1 Overview of research trends and themes\n\nTable 2 ,\n2In addition, our analysis reveals that the USA, England, and India have the highest degree values of 51, 41, and 35, respectively. This suggests that these countries have the most connections with other nations in relation to LLMs research. Regarding frequency, China and the USA have the highest number of recognized publications, 1828 and 1344, respectively, greatly surpassing other countries. However, the USA has a higher degree value than China, showing a more centralized position in the collaboration network and greater outreach with other countries. Other countries and regions, such as Japan, the Netherlands, Singapore, and South Korea, are actively involved in collaboration efforts. Overall, we observed that most papers on LLMs research have been published in countries in the Asia-Pacific region, North America, and Europe.we present the top 10 countries that have contributed the most to high-yield degree \nand frequency research. These countries include the United States (USA), United Kingdom \n(England), India, Canada, France, China, Germany, Spain, Australia, and Russia, each with \ninfluence on the collaboration network. Specifically, their respective degree values are 51, 41, \n35, 34, 33, 33, 29, 28, 28, and 27. \n\nTable 2 .\n2Top countries and regions in the international collaboration networkCountry \nDegree \nFrequency \n\nUSA \n51 \n1344 \n\nENGLAND \n41 \n205 \n\nINDIA \n35 \n377 \n\nCANADA \n34 \n179 \n\nFRANCE \n33 \n112 \n\nPEOPLES R CHINA \n33 \n1828 \n\nGERMANY \n29 \n219 \n\nSPAIN \n28 \n89 \n\nAUSTRALIA \n28 \n131 \n\nRUSSIA \n27 \n81 \n\n\n\nTable 3 .\n3Top organizations in the institutional collaboration network The implementation of these methods aims to provide a high-level and accurate depiction of the emerging and expanding landscapes of LLMs research.Organization \nFrequency Degree \nType \n\nChinese Academy of Sciences \n205 \n60 \nResearch Institute \n\nUniversity of California System \n104 \n59 \nUniversity \n\n\nTable 4 .\n4Topics and keywords of LLMs publicationsTopic Count Theme \nKeywords \n\n0 \n98 Algorithm and NLP Tasks \n0_aspect_sentiment_aspectbased_absa \n\n1 \n98 Algorithm and NLP Tasks \n1_ranking_retrieval_query_document \n\n2 \n89 Algorithm and NLP Tasks \n2_visual_image_vqa_captioning \n\n3 \n82 Medical and Engineering Applications 3_protein_proteins_molecular_dna \n\n4 \n80 Social and Humanitarian Applications 4_hate_speech_offensive_hateful \n\n5 \n75 Algorithm and NLP Tasks \n5_summarization_summary_abstractive_extractive \n\n6 \n75 Social and Humanitarian Applications 6_legal_law_case_judicial \n\n7 \n75 Algorithm and NLP Tasks \n7_relation_extraction_entity_relations \n\nWe implemented the clustering using the scikit-learn Python package: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html 2 https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 1 https://www.sbert.net/\nSince the total number of publications in 2023 is not yet accessible when the paper is written, we use the analytics model (additive) in Tableau to forecast the number of the publication. Based on the data from January 1, 2017 to February 20, 2023, it is predicted to have 2486 LLMs publications in 2023. We assume this forecast is conservative because of the rocketing of research interests in LLMs after the debut of ChatGPT and GPT-4 in early 2023.\nThe position of point (a publication) is determined by its topic distribution based on SBERT embedding in BERTopic. The x-axis represents the first principal component (PC) of the topic distribution, while the y-axis represents the second PC. Topic distributions are collections of topic word scores. Examples of topic word scores are provided in Appendix A.5  For granular analysis, we also refer to the interactive version of this visualization on Tableau: https://public.tableau.com/app/profile/lizhou/viz/LLM_bib_categories/Documents_dash_online?publish=ye s. We provide the research themes, the topic number, and the article title of each LLMs publication.\nNote that topical the keywords here are the Web of Science (WoS) keywords, not the topical keywords generated by the BERTopic algorithm.\nCompanyTsinghua University\n\nPersistent Anti-Muslim Bias in Large Language Models. A Abid, M Farooqi, J Zou, Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. the 2021 AAAI/ACM Conference on AI, Ethics, and SocietyAbid, A., Farooqi, M., & Zou, J. (2021). Persistent Anti-Muslim Bias in Large Language Models. Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, 298-306.\n\nFinBERT: Financial Sentiment Analysis with Pre-trained Language Models. D Araci, In arXiv [cs.CLAraci, D. (2019). FinBERT: Financial Sentiment Analysis with Pre-trained Language Models. In arXiv [cs.CL]. arXiv. http://arxiv.org/abs/1908.10063\n\nSciBERT: A Pretrained Language Model for Scientific Text. I Beltagy, K Lo, A Cohan, In arXiv [cs.CLBeltagy, I., Lo, K., & Cohan, A. (2019). SciBERT: A Pretrained Language Model for Scientific Text. In arXiv [cs.CL]. arXiv. http://arxiv.org/abs/1903.10676\n\nOn the Dangers of Stochastic Parrots: Can Language Models Be Too Big. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. the 2021 ACM Conference on Fairness, Accountability, and TransparencyBender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? . Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610-623.\n\nWeb of Science as a data source for research on scientific and scholarly activity. C Birkle, D A Pendlebury, J Schnell, J Adams, Quantitative Science Studies. 11Birkle, C., Pendlebury, D. A., Schnell, J., & Adams, J. (2020). Web of Science as a data source for research on scientific and scholarly activity. Quantitative Science Studies, 1(1), 363-376.\n\nA correlated topic model of Science. D M Blei, J D Lafferty, The Annals of Applied Statistics. 11Blei, D. M., & Lafferty, J. D. (2007). A correlated topic model of Science. The Annals of Applied Statistics, 1(1), 17-35.\n\nT B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, D Amodei, Language Models are Few-Shot Learners. In arXiv [cs.CLBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., \u2026 Amodei, D. (2020). Language Models are Few-Shot Learners. In arXiv [cs.CL]. arXiv. http://arxiv.org/abs/2005.14165\n\nExtracting training data from large language models. N Carlini, F Tramer, E Wallace, M Jagielski, A Herbert-Voss, K Lee, A Roberts, T Brown, D Song, U Erlingsson, A Oprea, C Raffel, In arXiv [cs.CRCarlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., Oprea, A., & Raffel, C. (2020). Extracting training data from large language models. In arXiv [cs.CR]. arXiv. https://www.usenix.org/system/files/sec21-carlini-extracting.pdf\n\nCiteSpace: a practical guide for mapping scientific literature. C Chen, Chen, C. (2016). CiteSpace: a practical guide for mapping scientific literature. http://www.dobraca.com/wp-content/uploads/2019/03/CiteSpacePracticalGuide-Nova-Samp le1-50pp.pdf\n\nThe structure and dynamics of cocitation clusters: A multiple-perspective cocitation analysis. C Chen, F Ibekwe-Sanjuan, J Hou, Journal of the American Society for Information Science and Technology. 7Chen, C., Ibekwe-SanJuan, F., & Hou, J. (2010). The structure and dynamics of cocitation clusters: A multiple-perspective cocitation analysis. Journal of the American Society for Information Science and Technology , 61(7), 1386-1409.\n\nEvaluating Large Language Models Trained on Code. M Chen, J Tworek, H Jun, Q Yuan, H P De Oliveira Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, A Ray, R Puri, G Krueger, M Petrov, H Khlaaf, G Sastry, P Mishkin, B Chan, S Gray, W Zaremba, arXiv [cs.LGChen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., \u2026 Zaremba, W. (2021). Evaluating Large Language Models Trained on Code. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2107.03374\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, arXiv [cs.CLDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In arXiv [cs.CL]. arXiv. http://arxiv.org/abs/1810.04805\n\nDiagnosing crop diseases based on domain-adaptive pre-training BERT of electronic medical records. J Ding, B Li, C Xu, Y Qiao, L Zhang, 10.1007/s10489-022-04346-xDing, J., Li, B., Xu, C., Qiao, Y., & Zhang, L. (2022). Diagnosing crop diseases based on domain-adaptive pre-training BERT of electronic medical records. Applied Intelligence. https://doi.org/10.1007/s10489-022-04346-x\n\nGPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models. T Eloundou, S Manning, P Mishkin, D Rock, In arXiv [econ.GNEloundou, T., Manning, S., Mishkin, P., & Rock, D. (2023). GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models. In arXiv [econ.GN]. arXiv. http://arxiv.org/abs/2303.10130\n\nSam Altman: OpenAI CEO on GPT-4, ChatGPT, and the Future of AI. L Fridman, Fridman, L. (2023, March 25). Sam Altman: OpenAI CEO on GPT-4, ChatGPT, and the Future of AI. YouTube. https://www.youtube.com/watch?v=L_Guz73e6fw\n\nPause Giant AI Experiments: An Open Letter. Future of Life Institute. Future of Life InstituteRetrievedFuture of Life Institute. (n.d.). Pause Giant AI Experiments: An Open Letter. Future of Life Institute. Retrieved April 2, 2023, from https://futureoflife.org/open-letter/pause-giant-ai-experiments/\n\nBigger Isn't Better: The Ethical and Scientific Vices of Extra-Large Datasets in Language Models. T S Goetze, D Abramson, 13th ACM Web Science Conference 2021. Goetze, T. S., & Abramson, D. (2021). Bigger Isn't Better: The Ethical and Scientific Vices of Extra-Large Datasets in Language Models. 13th ACM Web Science Conference 2021, 69-75.\n\nBERTopic: Neural topic modeling with a class-based TF-IDF procedure. M Grootendorst, In arXiv [cs.CLGrootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based TF-IDF procedure. In arXiv [cs.CL]. arXiv. http://arxiv.org/abs/2203.05794\n\n. B Guembe, A Azeta, S Misra, V C Osamor, L Fernandez-Sanz, V Pospelova, Guembe, B., Azeta, A., Misra, S., Osamor, V. C., Fernandez-Sanz, L., & Pospelova, V. (2022).\n\n. The Emerging Threat of Ai-driven Cyber Attacks: A Review. Applied Artificial Intelligence: AAI. 3612037254The Emerging Threat of Ai-driven Cyber Attacks: A Review. Applied Artificial Intelligence: AAI, 36(1), 2037254.\n\nDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing. Y Gu, R Tinn, H Cheng, M Lucas, N Usuyama, X Liu, T Naumann, J Gao, H Poon, ACM Trans. Comput. Healthcare. 31Gu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., Naumann, T., Gao, J., & Poon, H. (2021). Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. ACM Trans. Comput. Healthcare, 3(1), 1-23.\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural Computation. 98Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8), 1735-1780.\n\nGenerating Major Types of Chinese Classical Poetry in a Uniformed Framework. J Hu, M Sun, arXiv [cs.CL]. arXivHu, J., & Sun, M. (2020). Generating Major Types of Chinese Classical Poetry in a Uniformed Framework. In arXiv [cs.CL]. arXiv. http://arxiv.org/abs/2003.11528\n\nConfliBERT: A Pre-trained Language Model for Political Conflict and Violence. Y Hu, M Hosseini, E Skorupa Parolin, J Osorio, L Khan, P Brandt, V Orazio, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesHu, Y., Hosseini, M., Skorupa Parolin, E., Osorio, J., Khan, L., Brandt, P., & D'Orazio, V. (2022). ConfliBERT: A Pre-trained Language Model for Political Conflict and Violence. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 5469-5482.\n\nIdentification of End-User Economical Relationship Graph Using Lightweight Blockchain-Based BERT Model. M Jagdish, D U Shah, V Agarwal, G B Loganathan, A Alqahtani, S A Rahin, Computational Intelligence and Neuroscience. 6546913Jagdish, M., Shah, D. U., Agarwal, V., Loganathan, G. B., Alqahtani, A., & Rahin, S. A. (2022). Identification of End-User Economical Relationship Graph Using Lightweight Blockchain-Based BERT Model. Computational Intelligence and Neuroscience, 2022, 6546913.\n\nIs BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment. D Jin, Z Jin, J T Zhou, P Szolovits, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Jin, D., Jin, Z., Zhou, J. T., & Szolovits, P. (2020). Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05), 8018-8025.\n\nWhat's in the Chatterbox? Large Language Models, Why They Matter, and What We Should Do About Them. Johanna Okerlund, Evan Klasky, Aditya Middha, Sujin Kim, Hannah Rosenfeld, Molly Kleinman, Shobita Parthasarathy, University of MichiganJohanna Okerlund, Evan Klasky, Aditya Middha, Sujin Kim, Hannah Rosenfeld, Molly Kleinman, Shobita Parthasarathy. (2022). What's in the Chatterbox? Large Language Models, Why They Matter, and What We Should Do About Them. University of Michigan. https://stpp.fordschool.umich.edu/sites/stpp/files/2022-05/large-language-models-TAP-2022 -final-051622.pdf\n\nSpeech and Language Processing: An Introduction to Natural Language Processing. D Jurafsky, J H Martin, Computational Linguistics, and Speech Recognition. Jurafsky, D., & Martin, J. H. (2023). Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. https://web.stanford.edu/~jurafsky/slp3/ed3book_jan72023.pdf\n\nThe Privatization of AI Research(-ers): Causes and Potential Consequences --From university-industry interaction to public research brain-drain?. R Jurowetzki, D Hain, J Mateos-Garcia, K Stathoulopoulos, In arXiv [cs.CYJurowetzki, R., Hain, D., Mateos-Garcia, J., & Stathoulopoulos, K. (2021). The Privatization of AI Research(-ers): Causes and Potential Consequences --From university-industry interaction to public research brain-drain? In arXiv [cs.CY]. arXiv. http://arxiv.org/abs/2102.01648\n\nUnderstanding Language Model from Questions in Social Studies for Students. K Kawashima, S Yamaguchi, 2021 IEEE International Conference on Big Data (Big Data). Kawashima, K., & Yamaguchi, S. (2021). Understanding Language Model from Questions in Social Studies for Students. 2021 IEEE International Conference on Big Data (Big Data), 5932-5934.\n\nMMBERT: Multimodal BERT Pretraining for Improved Medical VQA. Y Khare, V Bagal, M Mathew, A Devi, U Deva Priyakumar, C V Jawahar, In arXiv [cs.CVKhare, Y., Bagal, V., Mathew, M., Devi, A., Deva Priyakumar, U., & Jawahar, C. V. (2021). MMBERT: Multimodal BERT Pretraining for Improved Medical VQA. In arXiv [cs.CV]. arXiv. http://arxiv.org/abs/2104.01394\n\nB Kim, H Kim, S.-W Lee, G Lee, D Kwak, D H Jeon, S Park, S Kim, S Kim, D Seo, H Lee, M Jeong, S Lee, M Kim, S H Ko, S Kim, T Park, J Kim, S Kang, N Sung, What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers. In arXiv [cs.CLKim, B., Kim, H., Lee, S.-W., Lee, G., Kwak, D., Jeon, D. H., Park, S., Kim, S., Kim, S., Seo, D., Lee, H., Jeong, M., Lee, S., Kim, M., Ko, S. H., Kim, S., Park, T., Kim, J., Kang, S., \u2026 Sung, N. (2021). What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers. In arXiv [cs.CL].\n\nContent Analysis: An Introduction to Its Methodology. K Krippendorff, SAGE PublicationsKrippendorff, K. (2018). Content Analysis: An Introduction to Its Methodology. SAGE Publications.\n\nPerformance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models. T H Kung, M Cheatham, A Medenilla, C Sillos, L De Leon, C Elepa\u00f1o, M Madriaga, R Aggabao, G Diaz-Candido, J Maningo, V Tseng, PLOS Digital Health. 22198Kung, T. H., Cheatham, M., Medenilla, A., Sillos, C., De Leon, L., Elepa\u00f1o, C., Madriaga, M., Aggabao, R., Diaz-Candido, G., Maningo, J., & Tseng, V. (2023). Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models. PLOS Digital Health, 2(2), e0000198.\n\nFlauBERT: Unsupervised Language Model Pre-training for French. H Le, L Vial, J Frej, V Segonne, M Coavoux, B Lecouteux, A Allauzen, B Crabb\u00e9, L Besacier, D Schwab, arXiv [cs.CLLe, H., Vial, L., Frej, J., Segonne, V., Coavoux, M., Lecouteux, B., Allauzen, A., Crabb\u00e9, B., Besacier, L., & Schwab, D. (2019). FlauBERT: Unsupervised Language Model Pre-training for French. In arXiv [cs.CL]. arXiv. http://arxiv.org/abs/1912.05372\n\nApplying BERT to analyze investor sentiment in stock market. M Li, W Li, F Wang, X Jia, G Rui, Neural Computing & Applications. 3310Li, M., Li, W., Wang, F., Jia, X., & Rui, G. (2021). Applying BERT to analyze investor sentiment in stock market. Neural Computing & Applications, 33(10), 4663-4676.\n\nMitigating Political Bias in Language Models through Reinforced Calibration. R Liu, C Jia, J Wei, G Xu, L Wang, S Vosoughi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35Liu, R., Jia, C., Wei, J., Xu, G., Wang, L., & Vosoughi, S. (2021). Mitigating Political Bias in Language Models through Reinforced Calibration. Proceedings of the AAAI Conference on Artificial Intelligence, 35(17), 14857-14866.\n\nIntelligent generation method of emergency plan for hydraulic engineering based on knowledge graph -take the South-to-North Water Diversion Project as an example. X Liu, H Lu, H Li, LHB. 10812153629Liu, X., Lu, H., & Li, H. (2022). Intelligent generation method of emergency plan for hydraulic engineering based on knowledge graph -take the South-to-North Water Diversion Project as an example. LHB, 108(1), 2153629.\n\nRoBERTa: A Robustly Optimized BERT Pretraining Approach. Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, In arXiv [cs.CLLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. In arXiv [cs.CL]. arXiv. http://arxiv.org/abs/1907.11692\n\nIntroducing FLAN: More generalizable Language Models with Instruction Fine-Tuning. Maarten Bosma, J W , Maarten Bosma, J. W. (2021). Introducing FLAN: More generalizable Language Models with Instruction Fine-Tuning.\n\nHuman language understanding & reasoning. C D Manning, Daedalus. 1512Manning, C. D. (2022). Human language understanding & reasoning. Daedalus, 151(2), 127-138.\n\nUMAP: Uniform Manifold Approximation and Projection. L Mcinnes, J Healy, N Saul, L Gro\u00dfberger, 10.21105/joss.00861Journal of Open Source Software. 3861McInnes, L., Healy, J., Saul, N., & Gro\u00dfberger, L. (2018). UMAP: Uniform Manifold Approximation and Projection. In Journal of Open Source Software (Vol. 3, Issue 29, p. 861). https://doi.org/10.21105/joss.00861\n\nIntroducing LLaMA: A foundational, 65-billion-parameter large language model. A I Meta, Meta AI. Meta, A. I. (2023, February 24). Introducing LLaMA: A foundational, 65-billion-parameter large language model. Meta AI. https://ai.facebook.com/blog/large-language-model-llama-meta-ai/\n\n. N Mustakim, R Rabu, Md, G Mursalin, E Hossain, O Sharif, M M Hoque, Mustakim, N., Rabu, R., Md. Mursalin, G., Hossain, E., Sharif, O., & Hoque, M. M. (2022).\n\nMulti-Class Textual Emotion Detection from Social Media using Transformer. Cuet-Nlp@tamilnlp-Acl2022, Proceedings of the Second Workshop on Speech and Language Technologies for Dravidian Languages. the Second Workshop on Speech and Language Technologies for Dravidian LanguagesCUET-NLP@TamilNLP-ACL2022: Multi-Class Textual Emotion Detection from Social Media using Transformer. Proceedings of the Second Workshop on Speech and Language Technologies for Dravidian Languages, 199-206.\n\nBERTweet: A pre-trained language model for English Tweets. D Q Nguyen, T Vu, A T Nguyen, arXiv [cs.CLNguyen, D. Q., Vu, T., & Nguyen, A. T. (2020). BERTweet: A pre-trained language model for English Tweets. In arXiv [cs.CL]. arXiv. http://arxiv.org/abs/2005.10200\n\nConnecting Urban Green Spaces with Children: A Scientometric Analysis Using CiteSpace. Y Niu, M Adam, H Hussein, 1259Niu, Y., Adam, M., & Hussein, H. (2022). Connecting Urban Green Spaces with Children: A Scientometric Analysis Using CiteSpace. Land, 11(8), 1259.\n\nA deeper dive into ChatGPT: history, use and future perspectives for orthopaedic research. M Ollivier, A Pareek, J Dahmen, M E Kayaalp, P W Winkler, M T Hirschmann, J Karlsson, Arthroscopy: Official Journal of the ESSKA. 314Knee Surgery, Sports TraumatologyOllivier, M., Pareek, A., Dahmen, J., Kayaalp, M. E., Winkler, P. W., Hirschmann, M. T., & Karlsson, J. (2023). A deeper dive into ChatGPT: history, use and future perspectives for orthopaedic research. Knee Surgery, Sports Traumatology, Arthroscopy: Official Journal of the ESSKA, 31(4), 1190-1192.\n\nGPT-4 is OpenAI's most advanced system, producing safer and more useful responses. Openai, OpenAI. OpenAI. (n.d.). GPT-4 is OpenAI's most advanced system, producing safer and more useful responses. OpenAI. Retrieved March 23, 2023, from https://openai.com/product/gpt-4\n\nGPT-4 Technical Report. Openai, arXiv [cs.CLOpenAI. (2023). GPT-4 Technical Report. In arXiv [cs.CL]. arXiv. http://arxiv.org/abs/2303.08774\n\nAn important next step on our AI journey. The Keyword. S Pichai, Pichai, S. (2023, February 6). An important next step on our AI journey. The Keyword, Google. https://blog.google/technology/ai/bard-google-ai-search-updates/\n\nEngineering Document Summarization: A Bidirectional Language Model-Based Approach. Y Qiu, Y Jin, Journal of Computing and Information Science in Engineering. 22661004Qiu, Y., & Jin, Y. (2022). Engineering Document Summarization: A Bidirectional Language Model-Based Approach. Journal of Computing and Information Science in Engineering, 22(6), 061004.\n\nImproving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018, June 11). Improving language understanding by generative pre-training. OpenAI. https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_p aper.pdf\n\nLanguage Models are Unsupervised Multitask Learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language Models are Unsupervised Multitask Learners. https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsup ervised_multitask_learners.pdf\n\nMed-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction. L Rasmy, Y Xiang, Z Xie, C Tao, D Zhi, NPJ Digital Medicine. 4186Rasmy, L., Xiang, Y., Xie, Z., Tao, C., & Zhi, D. (2021). Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction. NPJ Digital Medicine, 4(1), 86.\n\nSentence-BERT: Sentence Embeddings using Siamese BERT-Networks. N Reimers, I Gurevych, In arXiv [cs.CLReimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In arXiv [cs.CL]. arXiv. http://arxiv.org/abs/1908.10084\n\nPrompt Programming for Large Language Models: Beyond the Few-Shot Paradigm. L Reynolds, K Mcdonell, Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, Article Article. 314Reynolds, L., & McDonell, K. (2021). Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm. Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, Article Article 314.\n\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Alpaca: A Strong, Replicable Instruction-Following Model. Stanford Center for Research on Foundation Models. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto. (March, 13 2023). Alpaca: A Strong, Replicable Instruction-Following Model. Stanford Center for Research on Foundation Models. https://crfm.stanford.edu/2023/03/13/alpaca.html\n\nAutomatic Event Coding Framework for Spanish Political News Articles. S Salam, IDSL Khan, IDSA El-Ghamry, IDSP Brandt, IDSJ Holmes, IDSV D&apos;orazio, IDSJ Osorio, IDSIEEE 6th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security. Salam, S., Khan, L., El-Ghamry, A., Brandt, P., Holmes, J., D'Orazio, V., & Osorio, J. (2020). Automatic Event Coding Framework for Spanish Political News Articles. 2020 IEEE 6th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS), 246-253.\n\nGPT-4 is here: what scientists think. K Sanderson, 10.1038/d41586-023-00816-5Sanderson, K. (2023). GPT-4 is here: what scientists think. Nature. https://doi.org/10.1038/d41586-023-00816-5\n\nPre-training of Graph Augmented Transformers for Medication Recommendation. J Shang, T Ma, C Xiao, J Sun, arXiv [cs.AIShang, J., Ma, T., Xiao, C., & Sun, J. (2019). Pre-training of Graph Augmented Transformers for Medication Recommendation. In arXiv [cs.AI]. arXiv. http://arxiv.org/abs/1906.00346\n\nChatGPT and Other Large Language Models Are Double-edged Swords. Y Shen, L Heacock, J Elias, K D Hentel, B Reig, G Shih, L Moy, Radiology. 230163Shen, Y., Heacock, L., Elias, J., Hentel, K. D., Reig, B., Shih, G., & Moy, L. (2023). ChatGPT and Other Large Language Models Are Double-edged Swords. Radiology, 230163.\n\nHuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. Y Shen, K Song, X Tan, D Li, W Lu, Y Zhuang, In arXiv [cs.CLShen, Y., Song, K., Tan, X., Li, D., Lu, W., & Zhuang, Y. (2023). HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. In arXiv [cs.CL]. arXiv. http://arxiv.org/abs/2303.17580\n\nMapping the knowledge of traffic collision Reconstruction: A scientometric analysis in CiteSpace, VOSviewer, and SciMAT. Z Shen, W Ji, S Yu, G Cheng, Q Yuan, Z Han, H Liu, T Yang, Science & Justice: Journal of the Forensic Science Society. 631Shen, Z., Ji, W., Yu, S., Cheng, G., Yuan, Q., Han, Z., Liu, H., & Yang, T. (2023). Mapping the knowledge of traffic collision Reconstruction: A scientometric analysis in CiteSpace, VOSviewer, and SciMAT. Science & Justice: Journal of the Forensic Science Society, 63(1), 19-37.\n\n. M Shoeybi, M Patwary, R Puri, P Legresley, J Casper, B Catanzaro, Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., & Catanzaro, B. (2019).\n\nTraining Multi-Billion Parameter Language Models Using Model Parallelism. Megatron-Lm, In arXiv [cs.CLMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. In arXiv [cs.CL]. arXiv. http://arxiv.org/abs/1909.08053\n\nProbabilistic topic models. taylorfrancis. M Steyvers, 10.4324/9780203936399-29/probabilistic-topic-models-mark-steyvers-tom-griffithsSteyvers, M. (2007). Probabilistic topic models. taylorfrancis.com. https://doi.org/10.4324/9780203936399-29/probabilistic-topic-models-mark-steyvers-tom-gri ffiths\n\nSequence to sequence learning with neural networks. I Sutskever, O Vinyals, Q V Le, Advances in Neural Information Processing Systems. Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in Neural Information Processing Systems, 27. https://proceedings.neurips.cc/paper/5346-sequence-to-sequence-learning-with-neural-\n\nEmbedding Electronic Health Records to Learn BERT-based Models for Diagnostic Decision Support. R Tang, H Yao, Z Zhu, X Sun, G Hu, Y Li, G Xie, IEEE 9th International Conference on Healthcare Informatics (ICHI). Tang, R., Yao, H., Zhu, Z., Sun, X., Hu, G., Li, Y., & Xie, G. (2021). Embedding Electronic Health Records to Learn BERT-based Models for Diagnostic Decision Support. 2021 IEEE 9th International Conference on Healthcare Informatics (ICHI), 311-319.\n\nLaMDA: Language Models for Dialog Applications. R Thoppilan, D De Freitas, J Hall, N Shazeer, A Kulshreshtha, H.-T Cheng, A Jin, T Bos, L Baker, Y Du, Y Li, H Lee, H S Zheng, A Ghafouri, M Menegali, Y Huang, M Krikun, D Lepikhin, J Qin, Q Le, In arXiv [cs.CLThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Lee, H., Zheng, H. S., Ghafouri, A., Menegali, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J., \u2026 Le, Q. (2022). LaMDA: Language Models for Dialog Applications. In arXiv [cs.CL]. arXiv. http://arxiv.org/abs/2201.08239\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, arXiv [cs.CLVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention is all you need. In arXiv [cs.CL]. arXiv. https://proceedings.neurips.cc/paper/7181-attention-is-all\n\nEthical and social risks of harm from Language Models. L Weidinger, J Mellor, M Rauh, C Griffin, J Uesato, P.-S Huang, M Cheng, M Glaese, B Balle, A Kasirzadeh, Z Kenton, S Brown, W Hawkins, T Stepleton, C Biles, A Birhane, J Haas, L Rimell, L A Hendricks, I Gabriel, In arXiv [cs.CLWeidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P.-S., Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A., Kenton, Z., Brown, S., Hawkins, W., Stepleton, T., Biles, C., Birhane, A., Haas, J., Rimell, L., Hendricks, L. A., \u2026 Gabriel, I. (2021). Ethical and social risks of harm from Language Models. In arXiv [cs.CL]. arXiv. http://arxiv.org/abs/2112.04359\n\nJ Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, E H Chi, T Hashimoto, O Vinyals, P Liang, J Dean, W Fedus, Emergent Abilities of Large Language Models. In arXiv [cs.CLWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., & Fedus, W. (2022). Emergent Abilities of Large Language Models. In arXiv [cs.CL]. arXiv. http://arxiv.org/abs/2206.07682\n\nWeaponized AI for cyber attacks. M M Yamin, M Ullah, H Ullah, B Katt, Journal of Information Security and Applications. 57102722Yamin, M. M., Ullah, M., Ullah, H., & Katt, B. (2021). Weaponized AI for cyber attacks. Journal of Information Security and Applications, 57, 102722.\n\nAutoDefect: defect text classification in residential buildings using a multi-task channel attention network. D U Yang, B Kim, S H Lee, Y H Ahn, H Y Kim, Sustainable Cities and Society. Yang, D. U., Kim, B., Lee, S. H., Ahn, Y. H., & Kim, H. Y. (2022). AutoDefect: defect text classification in residential buildings using a multi-task channel attention network. Sustainable Cities and Society. https://www.sciencedirect.com/science/article/pii/S2210670722001329\n\nXlnet: Generalized autoregressive pretraining for language understanding. Z Yang, Z Dai, Y Yang, J Carbonell, R R Salakhutdinov, Q V Le, Advances in Neural Information Processing Systems. 32Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., & Le, Q. V. (2019). Xlnet: Generalized autoregressive pretraining for language understanding. Advances in Neural Information Processing Systems, 32. https://proceedings.neurips.cc/paper/2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Ab stract.html\n\nDisparities and resilience: analyzing online Health information provision, behaviors and needs of LBGTQ + elders during COVID-19. H Yu, L Fan, A J Gilliland, BMC Public Health. 2212338Yu, H., Fan, L., & Gilliland, A. J. (2022). Disparities and resilience: analyzing online Health information provision, behaviors and needs of LBGTQ + elders during COVID-19. BMC Public Health, 22(1), 2338.\n\nPretraining-Based Natural Language Generation for Text Summarization. H Zhang, J Xu, J Wang, In arXiv [cs.CLZhang, H., Xu, J., & Wang, J. (2019). Pretraining-Based Natural Language Generation for Text Summarization. In arXiv [cs.CL]. arXiv. http://arxiv.org/abs/1902.09243\n\nUnderstanding bag-of-words model: a statistical framework. Y Zhang, R Jin, Z.-H Zhou, International Journal of Machine Learning and Cybernetics. 11-4Zhang, Y., Jin, R., & Zhou, Z.-H. (2010). Understanding bag-of-words model: a statistical framework. International Journal of Machine Learning and Cybernetics, 1(1-4), 43-52.\n\nA Survey of Large Language Models. W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, Y Du, C Yang, Y Chen, Z Chen, J Jiang, R Ren, Y Li, X Tang, Z Liu, J.-R Wen, In arXiv [cs.CLZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., \u2026 Wen, J.-R. (2023). A Survey of Large Language Models. In arXiv [cs.CL]. arXiv. http://arxiv.org/abs/2303.18223\n\nAlgorithm and NLP Tasks 82_image_multimodal_images_crossmodal. Algorithm and NLP Tasks 82_image_multimodal_images_crossmodal\n\nAlgorithm and NLP Tasks 156_formality_sentence_dibert_khasi. Algorithm and NLP Tasks 156_formality_sentence_dibert_khasi\n", "annotations": {"author": "[{\"end\":146,\"start\":77},{\"end\":158,\"start\":147},{\"end\":315,\"start\":159},{\"end\":417,\"start\":316},{\"end\":485,\"start\":418},{\"end\":559,\"start\":486}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":84},{\"end\":157,\"start\":155},{\"end\":167,\"start\":165},{\"end\":327,\"start\":324},{\"end\":426,\"start\":424},{\"end\":500,\"start\":492}]", "author_first_name": "[{\"end\":83,\"start\":77},{\"end\":154,\"start\":147},{\"end\":164,\"start\":159},{\"end\":323,\"start\":316},{\"end\":423,\"start\":418},{\"end\":491,\"start\":486}]", "author_affiliation": "[{\"end\":145,\"start\":89},{\"end\":225,\"start\":169},{\"end\":314,\"start\":227},{\"end\":416,\"start\":329},{\"end\":484,\"start\":428},{\"end\":558,\"start\":502}]", "title": "[{\"end\":74,\"start\":1},{\"end\":633,\"start\":560}]", "venue": null, "abstract": "[{\"end\":2091,\"start\":741}]", "bib_ref": "[{\"end\":2235,\"start\":2215},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":2251,\"start\":2235},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":2626,\"start\":2598},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":2644,\"start\":2626},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":2982,\"start\":2960},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3188,\"start\":3167},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":3209,\"start\":3188},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":3349,\"start\":3331},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3611,\"start\":3590},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3706,\"start\":3683},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3769,\"start\":3757},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3971,\"start\":3952},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3990,\"start\":3971},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":4006,\"start\":3990},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4040,\"start\":4024},{\"end\":4207,\"start\":4193},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":4223,\"start\":4207},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4368,\"start\":4356},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":4381,\"start\":4368},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":4589,\"start\":4456},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":4617,\"start\":4589},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":6204,\"start\":6176},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":6222,\"start\":6204},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":6408,\"start\":6387},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6716,\"start\":6701},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6891,\"start\":6876},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7083,\"start\":7065},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7252,\"start\":7229},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7796,\"start\":7764},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":7967,\"start\":7943},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":8057,\"start\":8035},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8369,\"start\":8348},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8652,\"start\":8631},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9093,\"start\":9076},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":9114,\"start\":9093},{\"end\":9242,\"start\":9212},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9395,\"start\":9373},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":9794,\"start\":9772},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11784,\"start\":11772},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12162,\"start\":12145},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":12206,\"start\":12182},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":12250,\"start\":12231},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12284,\"start\":12271},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":12332,\"start\":12311},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12376,\"start\":12358},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12612,\"start\":12590},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":12632,\"start\":12612},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12961,\"start\":12936},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":13048,\"start\":13021},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13164,\"start\":13152},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":13669,\"start\":13649},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":13687,\"start\":13669},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":13740,\"start\":13720},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13879,\"start\":13860},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":13982,\"start\":13965},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14030,\"start\":14013},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":14133,\"start\":14115},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14330,\"start\":14301},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":14352,\"start\":14330},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14386,\"start\":14364},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14402,\"start\":14386},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14437,\"start\":14421},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":14456,\"start\":14437},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14630,\"start\":14609},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16035,\"start\":16012},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":16050,\"start\":16035},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16105,\"start\":16085},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":16257,\"start\":16231},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":16700,\"start\":16678},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":17456,\"start\":17437},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17606,\"start\":17586},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19682,\"start\":19662},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20169,\"start\":20158},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20899,\"start\":20881},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":21514,\"start\":21496},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22817,\"start\":22797},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":22842,\"start\":22817},{\"end\":22923,\"start\":22897},{\"end\":36891,\"start\":36870},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":40765,\"start\":40698},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":40788,\"start\":40765},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":41350,\"start\":41325},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":41735,\"start\":41714},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":42079,\"start\":42060},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":42326,\"start\":42304},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":42502,\"start\":42481},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":42521,\"start\":42502},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":44264,\"start\":44249},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":44281,\"start\":44264},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":44297,\"start\":44281},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":46779,\"start\":46754},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":48688,\"start\":48667},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":49309,\"start\":49287},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":50019,\"start\":49996},{\"end\":50032,\"start\":50019}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":52525,\"start\":52223},{\"attributes\":{\"id\":\"fig_1\"},\"end\":52546,\"start\":52526},{\"attributes\":{\"id\":\"fig_2\"},\"end\":52639,\"start\":52547},{\"attributes\":{\"id\":\"fig_3\"},\"end\":52698,\"start\":52640},{\"attributes\":{\"id\":\"fig_4\"},\"end\":53011,\"start\":52699},{\"attributes\":{\"id\":\"fig_5\"},\"end\":53097,\"start\":53012},{\"attributes\":{\"id\":\"fig_6\"},\"end\":54087,\"start\":53098},{\"attributes\":{\"id\":\"fig_7\"},\"end\":54273,\"start\":54088},{\"attributes\":{\"id\":\"fig_8\"},\"end\":54367,\"start\":54274},{\"attributes\":{\"id\":\"fig_9\"},\"end\":54444,\"start\":54368},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":54679,\"start\":54445},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":54855,\"start\":54680},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":56106,\"start\":54856},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":56405,\"start\":56107},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":56777,\"start\":56406},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":57437,\"start\":56778}]", "paragraph": "[{\"end\":3770,\"start\":2107},{\"end\":4941,\"start\":3772},{\"end\":5223,\"start\":4943},{\"end\":5519,\"start\":5225},{\"end\":6036,\"start\":5521},{\"end\":7084,\"start\":6051},{\"end\":7567,\"start\":7086},{\"end\":8510,\"start\":7579},{\"end\":10225,\"start\":8512},{\"end\":11873,\"start\":10227},{\"end\":13227,\"start\":11890},{\"end\":15171,\"start\":13229},{\"end\":15316,\"start\":15192},{\"end\":15372,\"start\":15323},{\"end\":15552,\"start\":15374},{\"end\":16051,\"start\":15564},{\"end\":17741,\"start\":16053},{\"end\":18054,\"start\":17743},{\"end\":19411,\"start\":18056},{\"end\":19931,\"start\":19413},{\"end\":20900,\"start\":19933},{\"end\":21698,\"start\":20902},{\"end\":22519,\"start\":21700},{\"end\":23224,\"start\":22521},{\"end\":24924,\"start\":23270},{\"end\":26352,\"start\":24926},{\"end\":28155,\"start\":26399},{\"end\":28591,\"start\":28157},{\"end\":29381,\"start\":28666},{\"end\":31252,\"start\":29383},{\"end\":31801,\"start\":31254},{\"end\":32892,\"start\":31803},{\"end\":33708,\"start\":32894},{\"end\":34784,\"start\":33710},{\"end\":35551,\"start\":34786},{\"end\":36641,\"start\":35553},{\"end\":37078,\"start\":36643},{\"end\":37447,\"start\":37080},{\"end\":38889,\"start\":37550},{\"end\":40461,\"start\":38891},{\"end\":41736,\"start\":40463},{\"end\":42665,\"start\":41738},{\"end\":43463,\"start\":42761},{\"end\":44298,\"start\":43465},{\"end\":45795,\"start\":44300},{\"end\":46780,\"start\":45797},{\"end\":48071,\"start\":46782},{\"end\":48783,\"start\":48108},{\"end\":49601,\"start\":48785},{\"end\":50591,\"start\":49603},{\"end\":51262,\"start\":50593},{\"end\":52185,\"start\":51277},{\"end\":52222,\"start\":52200}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":14751,\"start\":14744},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":35759,\"start\":35752}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2105,\"start\":2093},{\"attributes\":{\"n\":\"2.\"},\"end\":6049,\"start\":6039},{\"attributes\":{\"n\":\"2.1\"},\"end\":7577,\"start\":7570},{\"attributes\":{\"n\":\"2.2\"},\"end\":11888,\"start\":11876},{\"attributes\":{\"n\":\"3.\"},\"end\":15190,\"start\":15174},{\"end\":15321,\"start\":15319},{\"attributes\":{\"n\":\"3.2\"},\"end\":15562,\"start\":15555},{\"end\":23268,\"start\":23227},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":26397,\"start\":26355},{\"end\":28664,\"start\":28594},{\"attributes\":{\"n\":\"5.1\"},\"end\":37548,\"start\":37450},{\"attributes\":{\"n\":\"5.2\"},\"end\":42759,\"start\":42668},{\"attributes\":{\"n\":\"5.3\"},\"end\":48106,\"start\":48074},{\"attributes\":{\"n\":\"6.\"},\"end\":51275,\"start\":51265},{\"end\":52198,\"start\":52188},{\"end\":52232,\"start\":52224},{\"end\":52537,\"start\":52527},{\"end\":52566,\"start\":52548},{\"end\":52651,\"start\":52641},{\"end\":53023,\"start\":53013},{\"end\":53117,\"start\":53099},{\"end\":54097,\"start\":54089},{\"end\":54285,\"start\":54275},{\"end\":54379,\"start\":54369},{\"end\":54455,\"start\":54446},{\"end\":54684,\"start\":54681},{\"end\":54866,\"start\":54857},{\"end\":56117,\"start\":56108},{\"end\":56416,\"start\":56407},{\"end\":56788,\"start\":56779}]", "table": "[{\"end\":54679,\"start\":54487},{\"end\":56106,\"start\":55707},{\"end\":56405,\"start\":56187},{\"end\":56777,\"start\":56625},{\"end\":57437,\"start\":56830}]", "figure_caption": "[{\"end\":52525,\"start\":52234},{\"end\":52546,\"start\":52539},{\"end\":52639,\"start\":52569},{\"end\":52698,\"start\":52653},{\"end\":53011,\"start\":52701},{\"end\":53097,\"start\":53025},{\"end\":54087,\"start\":53120},{\"end\":54273,\"start\":54099},{\"end\":54367,\"start\":54287},{\"end\":54444,\"start\":54381},{\"end\":54487,\"start\":54457},{\"end\":54855,\"start\":54686},{\"end\":55707,\"start\":54868},{\"end\":56187,\"start\":56119},{\"end\":56625,\"start\":56418},{\"end\":56830,\"start\":56790}]", "figure_ref": "[{\"end\":22535,\"start\":22524},{\"end\":23500,\"start\":23492},{\"end\":25126,\"start\":25118},{\"end\":28810,\"start\":28800},{\"end\":28875,\"start\":28863},{\"end\":29624,\"start\":29612},{\"end\":30141,\"start\":30129},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":31430,\"start\":31422},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":31523,\"start\":31511},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":31617,\"start\":31605},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":31985,\"start\":31973},{\"end\":32135,\"start\":32129},{\"end\":32383,\"start\":32375},{\"end\":32559,\"start\":32547},{\"end\":33127,\"start\":33115},{\"end\":33679,\"start\":33671},{\"end\":33826,\"start\":33814},{\"end\":39149,\"start\":39139},{\"end\":40318,\"start\":40306}]", "bib_author_first_name": "[{\"end\":59008,\"start\":59007},{\"end\":59016,\"start\":59015},{\"end\":59027,\"start\":59026},{\"end\":59410,\"start\":59409},{\"end\":59640,\"start\":59639},{\"end\":59651,\"start\":59650},{\"end\":59657,\"start\":59656},{\"end\":59908,\"start\":59907},{\"end\":59910,\"start\":59909},{\"end\":59920,\"start\":59919},{\"end\":59929,\"start\":59928},{\"end\":59947,\"start\":59946},{\"end\":60438,\"start\":60437},{\"end\":60448,\"start\":60447},{\"end\":60450,\"start\":60449},{\"end\":60464,\"start\":60463},{\"end\":60475,\"start\":60474},{\"end\":60746,\"start\":60745},{\"end\":60748,\"start\":60747},{\"end\":60756,\"start\":60755},{\"end\":60758,\"start\":60757},{\"end\":60930,\"start\":60929},{\"end\":60932,\"start\":60931},{\"end\":60941,\"start\":60940},{\"end\":60949,\"start\":60948},{\"end\":60958,\"start\":60957},{\"end\":60969,\"start\":60968},{\"end\":60979,\"start\":60978},{\"end\":60991,\"start\":60990},{\"end\":61006,\"start\":61005},{\"end\":61015,\"start\":61014},{\"end\":61025,\"start\":61024},{\"end\":61035,\"start\":61034},{\"end\":61046,\"start\":61045},{\"end\":61062,\"start\":61061},{\"end\":61073,\"start\":61072},{\"end\":61085,\"start\":61084},{\"end\":61094,\"start\":61093},{\"end\":61104,\"start\":61103},{\"end\":61106,\"start\":61105},{\"end\":61117,\"start\":61116},{\"end\":61123,\"start\":61122},{\"end\":61133,\"start\":61132},{\"end\":61611,\"start\":61610},{\"end\":61622,\"start\":61621},{\"end\":61632,\"start\":61631},{\"end\":61643,\"start\":61642},{\"end\":61656,\"start\":61655},{\"end\":61672,\"start\":61671},{\"end\":61679,\"start\":61678},{\"end\":61690,\"start\":61689},{\"end\":61699,\"start\":61698},{\"end\":61707,\"start\":61706},{\"end\":61721,\"start\":61720},{\"end\":61730,\"start\":61729},{\"end\":62125,\"start\":62124},{\"end\":62407,\"start\":62406},{\"end\":62415,\"start\":62414},{\"end\":62433,\"start\":62432},{\"end\":62798,\"start\":62797},{\"end\":62806,\"start\":62805},{\"end\":62816,\"start\":62815},{\"end\":62823,\"start\":62822},{\"end\":62831,\"start\":62830},{\"end\":62833,\"start\":62832},{\"end\":62854,\"start\":62853},{\"end\":62864,\"start\":62863},{\"end\":62875,\"start\":62874},{\"end\":62884,\"start\":62883},{\"end\":62894,\"start\":62893},{\"end\":62906,\"start\":62905},{\"end\":62913,\"start\":62912},{\"end\":62921,\"start\":62920},{\"end\":62932,\"start\":62931},{\"end\":62942,\"start\":62941},{\"end\":62952,\"start\":62951},{\"end\":62962,\"start\":62961},{\"end\":62973,\"start\":62972},{\"end\":62981,\"start\":62980},{\"end\":62989,\"start\":62988},{\"end\":63454,\"start\":63453},{\"end\":63467,\"start\":63463},{\"end\":63476,\"start\":63475},{\"end\":63483,\"start\":63482},{\"end\":63806,\"start\":63805},{\"end\":63814,\"start\":63813},{\"end\":63820,\"start\":63819},{\"end\":63826,\"start\":63825},{\"end\":63834,\"start\":63833},{\"end\":64182,\"start\":64181},{\"end\":64194,\"start\":64193},{\"end\":64205,\"start\":64204},{\"end\":64216,\"start\":64215},{\"end\":64516,\"start\":64515},{\"end\":65076,\"start\":65075},{\"end\":65078,\"start\":65077},{\"end\":65088,\"start\":65087},{\"end\":65389,\"start\":65388},{\"end\":65574,\"start\":65573},{\"end\":65584,\"start\":65583},{\"end\":65593,\"start\":65592},{\"end\":65602,\"start\":65601},{\"end\":65604,\"start\":65603},{\"end\":65614,\"start\":65613},{\"end\":65632,\"start\":65631},{\"end\":66047,\"start\":66046},{\"end\":66053,\"start\":66052},{\"end\":66061,\"start\":66060},{\"end\":66070,\"start\":66069},{\"end\":66079,\"start\":66078},{\"end\":66090,\"start\":66089},{\"end\":66097,\"start\":66096},{\"end\":66108,\"start\":66107},{\"end\":66115,\"start\":66114},{\"end\":66414,\"start\":66413},{\"end\":66428,\"start\":66427},{\"end\":66646,\"start\":66645},{\"end\":66652,\"start\":66651},{\"end\":66918,\"start\":66917},{\"end\":66924,\"start\":66923},{\"end\":66936,\"start\":66935},{\"end\":66955,\"start\":66954},{\"end\":66965,\"start\":66964},{\"end\":66973,\"start\":66972},{\"end\":66983,\"start\":66982},{\"end\":67702,\"start\":67701},{\"end\":67713,\"start\":67712},{\"end\":67715,\"start\":67714},{\"end\":67723,\"start\":67722},{\"end\":67734,\"start\":67733},{\"end\":67736,\"start\":67735},{\"end\":67750,\"start\":67749},{\"end\":67763,\"start\":67762},{\"end\":67765,\"start\":67764},{\"end\":68195,\"start\":68194},{\"end\":68202,\"start\":68201},{\"end\":68209,\"start\":68208},{\"end\":68211,\"start\":68210},{\"end\":68219,\"start\":68218},{\"end\":68695,\"start\":68688},{\"end\":68710,\"start\":68706},{\"end\":68725,\"start\":68719},{\"end\":68739,\"start\":68734},{\"end\":68751,\"start\":68745},{\"end\":68768,\"start\":68763},{\"end\":68786,\"start\":68779},{\"end\":69260,\"start\":69259},{\"end\":69272,\"start\":69271},{\"end\":69274,\"start\":69273},{\"end\":69712,\"start\":69711},{\"end\":69726,\"start\":69725},{\"end\":69734,\"start\":69733},{\"end\":69751,\"start\":69750},{\"end\":70139,\"start\":70138},{\"end\":70152,\"start\":70151},{\"end\":70472,\"start\":70471},{\"end\":70481,\"start\":70480},{\"end\":70490,\"start\":70489},{\"end\":70500,\"start\":70499},{\"end\":70508,\"start\":70507},{\"end\":70527,\"start\":70526},{\"end\":70529,\"start\":70528},{\"end\":70765,\"start\":70764},{\"end\":70772,\"start\":70771},{\"end\":70782,\"start\":70778},{\"end\":70789,\"start\":70788},{\"end\":70796,\"start\":70795},{\"end\":70804,\"start\":70803},{\"end\":70806,\"start\":70805},{\"end\":70814,\"start\":70813},{\"end\":70822,\"start\":70821},{\"end\":70829,\"start\":70828},{\"end\":70836,\"start\":70835},{\"end\":70843,\"start\":70842},{\"end\":70850,\"start\":70849},{\"end\":70859,\"start\":70858},{\"end\":70866,\"start\":70865},{\"end\":70873,\"start\":70872},{\"end\":70875,\"start\":70874},{\"end\":70881,\"start\":70880},{\"end\":70888,\"start\":70887},{\"end\":70896,\"start\":70895},{\"end\":70903,\"start\":70902},{\"end\":70911,\"start\":70910},{\"end\":71494,\"start\":71493},{\"end\":71732,\"start\":71731},{\"end\":71734,\"start\":71733},{\"end\":71742,\"start\":71741},{\"end\":71754,\"start\":71753},{\"end\":71767,\"start\":71766},{\"end\":71777,\"start\":71776},{\"end\":71788,\"start\":71787},{\"end\":71799,\"start\":71798},{\"end\":71811,\"start\":71810},{\"end\":71822,\"start\":71821},{\"end\":71838,\"start\":71837},{\"end\":71849,\"start\":71848},{\"end\":72249,\"start\":72248},{\"end\":72255,\"start\":72254},{\"end\":72263,\"start\":72262},{\"end\":72271,\"start\":72270},{\"end\":72282,\"start\":72281},{\"end\":72293,\"start\":72292},{\"end\":72306,\"start\":72305},{\"end\":72318,\"start\":72317},{\"end\":72328,\"start\":72327},{\"end\":72340,\"start\":72339},{\"end\":72674,\"start\":72673},{\"end\":72680,\"start\":72679},{\"end\":72686,\"start\":72685},{\"end\":72694,\"start\":72693},{\"end\":72701,\"start\":72700},{\"end\":72989,\"start\":72988},{\"end\":72996,\"start\":72995},{\"end\":73003,\"start\":73002},{\"end\":73010,\"start\":73009},{\"end\":73016,\"start\":73015},{\"end\":73024,\"start\":73023},{\"end\":73540,\"start\":73539},{\"end\":73547,\"start\":73546},{\"end\":73553,\"start\":73552},{\"end\":73852,\"start\":73851},{\"end\":73859,\"start\":73858},{\"end\":73866,\"start\":73865},{\"end\":73875,\"start\":73874},{\"end\":73881,\"start\":73880},{\"end\":73890,\"start\":73889},{\"end\":73898,\"start\":73897},{\"end\":73906,\"start\":73905},{\"end\":73915,\"start\":73914},{\"end\":73930,\"start\":73929},{\"end\":74280,\"start\":74273},{\"end\":74289,\"start\":74288},{\"end\":74291,\"start\":74290},{\"end\":74450,\"start\":74449},{\"end\":74452,\"start\":74451},{\"end\":74623,\"start\":74622},{\"end\":74634,\"start\":74633},{\"end\":74643,\"start\":74642},{\"end\":74651,\"start\":74650},{\"end\":75011,\"start\":75010},{\"end\":75013,\"start\":75012},{\"end\":75218,\"start\":75217},{\"end\":75230,\"start\":75229},{\"end\":75242,\"start\":75241},{\"end\":75254,\"start\":75253},{\"end\":75265,\"start\":75264},{\"end\":75275,\"start\":75274},{\"end\":75277,\"start\":75276},{\"end\":75921,\"start\":75920},{\"end\":75923,\"start\":75922},{\"end\":75933,\"start\":75932},{\"end\":75939,\"start\":75938},{\"end\":75941,\"start\":75940},{\"end\":76214,\"start\":76213},{\"end\":76221,\"start\":76220},{\"end\":76229,\"start\":76228},{\"end\":76483,\"start\":76482},{\"end\":76495,\"start\":76494},{\"end\":76505,\"start\":76504},{\"end\":76515,\"start\":76514},{\"end\":76517,\"start\":76516},{\"end\":76528,\"start\":76527},{\"end\":76530,\"start\":76529},{\"end\":76541,\"start\":76540},{\"end\":76543,\"start\":76542},{\"end\":76557,\"start\":76556},{\"end\":77418,\"start\":77417},{\"end\":77671,\"start\":77670},{\"end\":77678,\"start\":77677},{\"end\":78002,\"start\":78001},{\"end\":78013,\"start\":78012},{\"end\":78027,\"start\":78026},{\"end\":78039,\"start\":78038},{\"end\":78346,\"start\":78345},{\"end\":78357,\"start\":78356},{\"end\":78363,\"start\":78362},{\"end\":78372,\"start\":78371},{\"end\":78380,\"start\":78379},{\"end\":78390,\"start\":78389},{\"end\":78774,\"start\":78773},{\"end\":78783,\"start\":78782},{\"end\":78792,\"start\":78791},{\"end\":78799,\"start\":78798},{\"end\":78806,\"start\":78805},{\"end\":79117,\"start\":79116},{\"end\":79128,\"start\":79127},{\"end\":79389,\"start\":79388},{\"end\":79401,\"start\":79400},{\"end\":79742,\"start\":79737},{\"end\":79756,\"start\":79750},{\"end\":79774,\"start\":79768},{\"end\":79786,\"start\":79782},{\"end\":79802,\"start\":79795},{\"end\":79813,\"start\":79807},{\"end\":79829,\"start\":79824},{\"end\":79846,\"start\":79837},{\"end\":79848,\"start\":79847},{\"end\":80341,\"start\":80340},{\"end\":80353,\"start\":80352},{\"end\":80364,\"start\":80363},{\"end\":80380,\"start\":80379},{\"end\":80393,\"start\":80392},{\"end\":80406,\"start\":80405},{\"end\":80426,\"start\":80425},{\"end\":81067,\"start\":81066},{\"end\":81294,\"start\":81293},{\"end\":81303,\"start\":81302},{\"end\":81309,\"start\":81308},{\"end\":81317,\"start\":81316},{\"end\":81582,\"start\":81581},{\"end\":81590,\"start\":81589},{\"end\":81601,\"start\":81600},{\"end\":81610,\"start\":81609},{\"end\":81612,\"start\":81611},{\"end\":81622,\"start\":81621},{\"end\":81630,\"start\":81629},{\"end\":81638,\"start\":81637},{\"end\":81908,\"start\":81907},{\"end\":81916,\"start\":81915},{\"end\":81924,\"start\":81923},{\"end\":81931,\"start\":81930},{\"end\":81937,\"start\":81936},{\"end\":81943,\"start\":81942},{\"end\":82287,\"start\":82286},{\"end\":82295,\"start\":82294},{\"end\":82301,\"start\":82300},{\"end\":82307,\"start\":82306},{\"end\":82316,\"start\":82315},{\"end\":82324,\"start\":82323},{\"end\":82331,\"start\":82330},{\"end\":82338,\"start\":82337},{\"end\":82691,\"start\":82690},{\"end\":82702,\"start\":82701},{\"end\":82713,\"start\":82712},{\"end\":82721,\"start\":82720},{\"end\":82734,\"start\":82733},{\"end\":82744,\"start\":82743},{\"end\":83135,\"start\":83134},{\"end\":83444,\"start\":83443},{\"end\":83457,\"start\":83456},{\"end\":83468,\"start\":83467},{\"end\":83470,\"start\":83469},{\"end\":83864,\"start\":83863},{\"end\":83872,\"start\":83871},{\"end\":83879,\"start\":83878},{\"end\":83886,\"start\":83885},{\"end\":83893,\"start\":83892},{\"end\":83899,\"start\":83898},{\"end\":83905,\"start\":83904},{\"end\":84278,\"start\":84277},{\"end\":84291,\"start\":84290},{\"end\":84305,\"start\":84304},{\"end\":84313,\"start\":84312},{\"end\":84324,\"start\":84323},{\"end\":84343,\"start\":84339},{\"end\":84352,\"start\":84351},{\"end\":84359,\"start\":84358},{\"end\":84366,\"start\":84365},{\"end\":84375,\"start\":84374},{\"end\":84381,\"start\":84380},{\"end\":84387,\"start\":84386},{\"end\":84394,\"start\":84393},{\"end\":84396,\"start\":84395},{\"end\":84405,\"start\":84404},{\"end\":84417,\"start\":84416},{\"end\":84429,\"start\":84428},{\"end\":84438,\"start\":84437},{\"end\":84448,\"start\":84447},{\"end\":84460,\"start\":84459},{\"end\":84467,\"start\":84466},{\"end\":84866,\"start\":84865},{\"end\":84877,\"start\":84876},{\"end\":84888,\"start\":84887},{\"end\":84898,\"start\":84897},{\"end\":84911,\"start\":84910},{\"end\":84920,\"start\":84919},{\"end\":84922,\"start\":84921},{\"end\":84931,\"start\":84930},{\"end\":84941,\"start\":84940},{\"end\":85249,\"start\":85248},{\"end\":85262,\"start\":85261},{\"end\":85272,\"start\":85271},{\"end\":85280,\"start\":85279},{\"end\":85291,\"start\":85290},{\"end\":85304,\"start\":85300},{\"end\":85313,\"start\":85312},{\"end\":85322,\"start\":85321},{\"end\":85332,\"start\":85331},{\"end\":85341,\"start\":85340},{\"end\":85355,\"start\":85354},{\"end\":85365,\"start\":85364},{\"end\":85374,\"start\":85373},{\"end\":85385,\"start\":85384},{\"end\":85398,\"start\":85397},{\"end\":85407,\"start\":85406},{\"end\":85418,\"start\":85417},{\"end\":85426,\"start\":85425},{\"end\":85436,\"start\":85435},{\"end\":85438,\"start\":85437},{\"end\":85451,\"start\":85450},{\"end\":85853,\"start\":85852},{\"end\":85860,\"start\":85859},{\"end\":85867,\"start\":85866},{\"end\":85880,\"start\":85879},{\"end\":85890,\"start\":85889},{\"end\":85898,\"start\":85897},{\"end\":85910,\"start\":85909},{\"end\":85922,\"start\":85921},{\"end\":85931,\"start\":85930},{\"end\":85939,\"start\":85938},{\"end\":85950,\"start\":85949},{\"end\":85952,\"start\":85951},{\"end\":85959,\"start\":85958},{\"end\":85972,\"start\":85971},{\"end\":85983,\"start\":85982},{\"end\":85992,\"start\":85991},{\"end\":86000,\"start\":85999},{\"end\":86403,\"start\":86402},{\"end\":86405,\"start\":86404},{\"end\":86414,\"start\":86413},{\"end\":86423,\"start\":86422},{\"end\":86432,\"start\":86431},{\"end\":86759,\"start\":86758},{\"end\":86761,\"start\":86760},{\"end\":86769,\"start\":86768},{\"end\":86776,\"start\":86775},{\"end\":86778,\"start\":86777},{\"end\":86785,\"start\":86784},{\"end\":86787,\"start\":86786},{\"end\":86794,\"start\":86793},{\"end\":86796,\"start\":86795},{\"end\":87187,\"start\":87186},{\"end\":87195,\"start\":87194},{\"end\":87202,\"start\":87201},{\"end\":87210,\"start\":87209},{\"end\":87223,\"start\":87222},{\"end\":87225,\"start\":87224},{\"end\":87242,\"start\":87241},{\"end\":87244,\"start\":87243},{\"end\":87744,\"start\":87743},{\"end\":87750,\"start\":87749},{\"end\":87757,\"start\":87756},{\"end\":87759,\"start\":87758},{\"end\":88075,\"start\":88074},{\"end\":88084,\"start\":88083},{\"end\":88090,\"start\":88089},{\"end\":88338,\"start\":88337},{\"end\":88347,\"start\":88346},{\"end\":88357,\"start\":88353},{\"end\":88639,\"start\":88638},{\"end\":88641,\"start\":88640},{\"end\":88649,\"start\":88648},{\"end\":88657,\"start\":88656},{\"end\":88663,\"start\":88662},{\"end\":88671,\"start\":88670},{\"end\":88679,\"start\":88678},{\"end\":88686,\"start\":88685},{\"end\":88693,\"start\":88692},{\"end\":88702,\"start\":88701},{\"end\":88711,\"start\":88710},{\"end\":88719,\"start\":88718},{\"end\":88725,\"start\":88724},{\"end\":88733,\"start\":88732},{\"end\":88741,\"start\":88740},{\"end\":88749,\"start\":88748},{\"end\":88758,\"start\":88757},{\"end\":88765,\"start\":88764},{\"end\":88771,\"start\":88770},{\"end\":88779,\"start\":88778},{\"end\":88789,\"start\":88785}]", "bib_author_last_name": "[{\"end\":59013,\"start\":59009},{\"end\":59024,\"start\":59017},{\"end\":59031,\"start\":59028},{\"end\":59416,\"start\":59411},{\"end\":59648,\"start\":59641},{\"end\":59654,\"start\":59652},{\"end\":59663,\"start\":59658},{\"end\":59917,\"start\":59911},{\"end\":59926,\"start\":59921},{\"end\":59944,\"start\":59930},{\"end\":59958,\"start\":59948},{\"end\":60445,\"start\":60439},{\"end\":60461,\"start\":60451},{\"end\":60472,\"start\":60465},{\"end\":60481,\"start\":60476},{\"end\":60753,\"start\":60749},{\"end\":60767,\"start\":60759},{\"end\":60938,\"start\":60933},{\"end\":60946,\"start\":60942},{\"end\":60955,\"start\":60950},{\"end\":60966,\"start\":60959},{\"end\":60976,\"start\":60970},{\"end\":60988,\"start\":60980},{\"end\":61003,\"start\":60992},{\"end\":61012,\"start\":61007},{\"end\":61022,\"start\":61016},{\"end\":61032,\"start\":61026},{\"end\":61043,\"start\":61036},{\"end\":61059,\"start\":61047},{\"end\":61070,\"start\":61063},{\"end\":61082,\"start\":61074},{\"end\":61091,\"start\":61086},{\"end\":61101,\"start\":61095},{\"end\":61114,\"start\":61107},{\"end\":61120,\"start\":61118},{\"end\":61130,\"start\":61124},{\"end\":61140,\"start\":61134},{\"end\":61619,\"start\":61612},{\"end\":61629,\"start\":61623},{\"end\":61640,\"start\":61633},{\"end\":61653,\"start\":61644},{\"end\":61669,\"start\":61657},{\"end\":61676,\"start\":61673},{\"end\":61687,\"start\":61680},{\"end\":61696,\"start\":61691},{\"end\":61704,\"start\":61700},{\"end\":61718,\"start\":61708},{\"end\":61727,\"start\":61722},{\"end\":61737,\"start\":61731},{\"end\":62130,\"start\":62126},{\"end\":62412,\"start\":62408},{\"end\":62430,\"start\":62416},{\"end\":62437,\"start\":62434},{\"end\":62803,\"start\":62799},{\"end\":62813,\"start\":62807},{\"end\":62820,\"start\":62817},{\"end\":62828,\"start\":62824},{\"end\":62851,\"start\":62834},{\"end\":62861,\"start\":62855},{\"end\":62872,\"start\":62865},{\"end\":62881,\"start\":62876},{\"end\":62891,\"start\":62885},{\"end\":62903,\"start\":62895},{\"end\":62910,\"start\":62907},{\"end\":62918,\"start\":62914},{\"end\":62929,\"start\":62922},{\"end\":62939,\"start\":62933},{\"end\":62949,\"start\":62943},{\"end\":62959,\"start\":62953},{\"end\":62970,\"start\":62963},{\"end\":62978,\"start\":62974},{\"end\":62986,\"start\":62982},{\"end\":62997,\"start\":62990},{\"end\":63461,\"start\":63455},{\"end\":63473,\"start\":63468},{\"end\":63480,\"start\":63477},{\"end\":63493,\"start\":63484},{\"end\":63811,\"start\":63807},{\"end\":63817,\"start\":63815},{\"end\":63823,\"start\":63821},{\"end\":63831,\"start\":63827},{\"end\":63840,\"start\":63835},{\"end\":64191,\"start\":64183},{\"end\":64202,\"start\":64195},{\"end\":64213,\"start\":64206},{\"end\":64221,\"start\":64217},{\"end\":64524,\"start\":64517},{\"end\":65085,\"start\":65079},{\"end\":65097,\"start\":65089},{\"end\":65402,\"start\":65390},{\"end\":65581,\"start\":65575},{\"end\":65590,\"start\":65585},{\"end\":65599,\"start\":65594},{\"end\":65611,\"start\":65605},{\"end\":65629,\"start\":65615},{\"end\":65642,\"start\":65633},{\"end\":66050,\"start\":66048},{\"end\":66058,\"start\":66054},{\"end\":66067,\"start\":66062},{\"end\":66076,\"start\":66071},{\"end\":66087,\"start\":66080},{\"end\":66094,\"start\":66091},{\"end\":66105,\"start\":66098},{\"end\":66112,\"start\":66109},{\"end\":66120,\"start\":66116},{\"end\":66425,\"start\":66415},{\"end\":66440,\"start\":66429},{\"end\":66649,\"start\":66647},{\"end\":66656,\"start\":66653},{\"end\":66921,\"start\":66919},{\"end\":66933,\"start\":66925},{\"end\":66952,\"start\":66937},{\"end\":66962,\"start\":66956},{\"end\":66970,\"start\":66966},{\"end\":66980,\"start\":66974},{\"end\":66990,\"start\":66984},{\"end\":67710,\"start\":67703},{\"end\":67720,\"start\":67716},{\"end\":67731,\"start\":67724},{\"end\":67747,\"start\":67737},{\"end\":67760,\"start\":67751},{\"end\":67771,\"start\":67766},{\"end\":68199,\"start\":68196},{\"end\":68206,\"start\":68203},{\"end\":68216,\"start\":68212},{\"end\":68229,\"start\":68220},{\"end\":68704,\"start\":68696},{\"end\":68717,\"start\":68711},{\"end\":68732,\"start\":68726},{\"end\":68743,\"start\":68740},{\"end\":68761,\"start\":68752},{\"end\":68777,\"start\":68769},{\"end\":68800,\"start\":68787},{\"end\":69269,\"start\":69261},{\"end\":69281,\"start\":69275},{\"end\":69723,\"start\":69713},{\"end\":69731,\"start\":69727},{\"end\":69748,\"start\":69735},{\"end\":69767,\"start\":69752},{\"end\":70149,\"start\":70140},{\"end\":70162,\"start\":70153},{\"end\":70478,\"start\":70473},{\"end\":70487,\"start\":70482},{\"end\":70497,\"start\":70491},{\"end\":70505,\"start\":70501},{\"end\":70524,\"start\":70509},{\"end\":70537,\"start\":70530},{\"end\":70769,\"start\":70766},{\"end\":70776,\"start\":70773},{\"end\":70786,\"start\":70783},{\"end\":70793,\"start\":70790},{\"end\":70801,\"start\":70797},{\"end\":70811,\"start\":70807},{\"end\":70819,\"start\":70815},{\"end\":70826,\"start\":70823},{\"end\":70833,\"start\":70830},{\"end\":70840,\"start\":70837},{\"end\":70847,\"start\":70844},{\"end\":70856,\"start\":70851},{\"end\":70863,\"start\":70860},{\"end\":70870,\"start\":70867},{\"end\":70878,\"start\":70876},{\"end\":70885,\"start\":70882},{\"end\":70893,\"start\":70889},{\"end\":70900,\"start\":70897},{\"end\":70908,\"start\":70904},{\"end\":70916,\"start\":70912},{\"end\":71507,\"start\":71495},{\"end\":71739,\"start\":71735},{\"end\":71751,\"start\":71743},{\"end\":71764,\"start\":71755},{\"end\":71774,\"start\":71768},{\"end\":71785,\"start\":71778},{\"end\":71796,\"start\":71789},{\"end\":71808,\"start\":71800},{\"end\":71819,\"start\":71812},{\"end\":71835,\"start\":71823},{\"end\":71846,\"start\":71839},{\"end\":71855,\"start\":71850},{\"end\":72252,\"start\":72250},{\"end\":72260,\"start\":72256},{\"end\":72268,\"start\":72264},{\"end\":72279,\"start\":72272},{\"end\":72290,\"start\":72283},{\"end\":72303,\"start\":72294},{\"end\":72315,\"start\":72307},{\"end\":72325,\"start\":72319},{\"end\":72337,\"start\":72329},{\"end\":72347,\"start\":72341},{\"end\":72677,\"start\":72675},{\"end\":72683,\"start\":72681},{\"end\":72691,\"start\":72687},{\"end\":72698,\"start\":72695},{\"end\":72705,\"start\":72702},{\"end\":72993,\"start\":72990},{\"end\":73000,\"start\":72997},{\"end\":73007,\"start\":73004},{\"end\":73013,\"start\":73011},{\"end\":73021,\"start\":73017},{\"end\":73033,\"start\":73025},{\"end\":73544,\"start\":73541},{\"end\":73550,\"start\":73548},{\"end\":73556,\"start\":73554},{\"end\":73856,\"start\":73853},{\"end\":73863,\"start\":73860},{\"end\":73872,\"start\":73867},{\"end\":73878,\"start\":73876},{\"end\":73887,\"start\":73882},{\"end\":73895,\"start\":73891},{\"end\":73903,\"start\":73899},{\"end\":73912,\"start\":73907},{\"end\":73927,\"start\":73916},{\"end\":73939,\"start\":73931},{\"end\":74286,\"start\":74281},{\"end\":74460,\"start\":74453},{\"end\":74631,\"start\":74624},{\"end\":74640,\"start\":74635},{\"end\":74648,\"start\":74644},{\"end\":74662,\"start\":74652},{\"end\":75018,\"start\":75014},{\"end\":75227,\"start\":75219},{\"end\":75235,\"start\":75231},{\"end\":75239,\"start\":75237},{\"end\":75251,\"start\":75243},{\"end\":75262,\"start\":75255},{\"end\":75272,\"start\":75266},{\"end\":75283,\"start\":75278},{\"end\":75476,\"start\":75451},{\"end\":75930,\"start\":75924},{\"end\":75936,\"start\":75934},{\"end\":75948,\"start\":75942},{\"end\":76218,\"start\":76215},{\"end\":76226,\"start\":76222},{\"end\":76237,\"start\":76230},{\"end\":76492,\"start\":76484},{\"end\":76502,\"start\":76496},{\"end\":76512,\"start\":76506},{\"end\":76525,\"start\":76518},{\"end\":76538,\"start\":76531},{\"end\":76554,\"start\":76544},{\"end\":76566,\"start\":76558},{\"end\":77038,\"start\":77032},{\"end\":77250,\"start\":77244},{\"end\":77425,\"start\":77419},{\"end\":77675,\"start\":77672},{\"end\":77682,\"start\":77679},{\"end\":78010,\"start\":78003},{\"end\":78024,\"start\":78014},{\"end\":78036,\"start\":78028},{\"end\":78049,\"start\":78040},{\"end\":78354,\"start\":78347},{\"end\":78360,\"start\":78358},{\"end\":78369,\"start\":78364},{\"end\":78377,\"start\":78373},{\"end\":78387,\"start\":78381},{\"end\":78400,\"start\":78391},{\"end\":78780,\"start\":78775},{\"end\":78789,\"start\":78784},{\"end\":78796,\"start\":78793},{\"end\":78803,\"start\":78800},{\"end\":78810,\"start\":78807},{\"end\":79125,\"start\":79118},{\"end\":79137,\"start\":79129},{\"end\":79398,\"start\":79390},{\"end\":79410,\"start\":79402},{\"end\":79748,\"start\":79743},{\"end\":79766,\"start\":79757},{\"end\":79780,\"start\":79775},{\"end\":79793,\"start\":79787},{\"end\":79805,\"start\":79803},{\"end\":79822,\"start\":79814},{\"end\":79835,\"start\":79830},{\"end\":79858,\"start\":79849},{\"end\":80347,\"start\":80342},{\"end\":80358,\"start\":80354},{\"end\":80374,\"start\":80365},{\"end\":80387,\"start\":80381},{\"end\":80400,\"start\":80394},{\"end\":80420,\"start\":80407},{\"end\":80433,\"start\":80427},{\"end\":81077,\"start\":81068},{\"end\":81300,\"start\":81295},{\"end\":81306,\"start\":81304},{\"end\":81314,\"start\":81310},{\"end\":81321,\"start\":81318},{\"end\":81587,\"start\":81583},{\"end\":81598,\"start\":81591},{\"end\":81607,\"start\":81602},{\"end\":81619,\"start\":81613},{\"end\":81627,\"start\":81623},{\"end\":81635,\"start\":81631},{\"end\":81642,\"start\":81639},{\"end\":81913,\"start\":81909},{\"end\":81921,\"start\":81917},{\"end\":81928,\"start\":81925},{\"end\":81934,\"start\":81932},{\"end\":81940,\"start\":81938},{\"end\":81950,\"start\":81944},{\"end\":82292,\"start\":82288},{\"end\":82298,\"start\":82296},{\"end\":82304,\"start\":82302},{\"end\":82313,\"start\":82308},{\"end\":82321,\"start\":82317},{\"end\":82328,\"start\":82325},{\"end\":82335,\"start\":82332},{\"end\":82343,\"start\":82339},{\"end\":82699,\"start\":82692},{\"end\":82710,\"start\":82703},{\"end\":82718,\"start\":82714},{\"end\":82731,\"start\":82722},{\"end\":82741,\"start\":82735},{\"end\":82754,\"start\":82745},{\"end\":82929,\"start\":82918},{\"end\":83144,\"start\":83136},{\"end\":83454,\"start\":83445},{\"end\":83465,\"start\":83458},{\"end\":83473,\"start\":83471},{\"end\":83869,\"start\":83865},{\"end\":83876,\"start\":83873},{\"end\":83883,\"start\":83880},{\"end\":83890,\"start\":83887},{\"end\":83896,\"start\":83894},{\"end\":83902,\"start\":83900},{\"end\":83909,\"start\":83906},{\"end\":84288,\"start\":84279},{\"end\":84302,\"start\":84292},{\"end\":84310,\"start\":84306},{\"end\":84321,\"start\":84314},{\"end\":84337,\"start\":84325},{\"end\":84349,\"start\":84344},{\"end\":84356,\"start\":84353},{\"end\":84363,\"start\":84360},{\"end\":84372,\"start\":84367},{\"end\":84378,\"start\":84376},{\"end\":84384,\"start\":84382},{\"end\":84391,\"start\":84388},{\"end\":84402,\"start\":84397},{\"end\":84414,\"start\":84406},{\"end\":84426,\"start\":84418},{\"end\":84435,\"start\":84430},{\"end\":84445,\"start\":84439},{\"end\":84457,\"start\":84449},{\"end\":84464,\"start\":84461},{\"end\":84470,\"start\":84468},{\"end\":84874,\"start\":84867},{\"end\":84885,\"start\":84878},{\"end\":84895,\"start\":84889},{\"end\":84908,\"start\":84899},{\"end\":84917,\"start\":84912},{\"end\":84928,\"start\":84923},{\"end\":84938,\"start\":84932},{\"end\":84952,\"start\":84942},{\"end\":85259,\"start\":85250},{\"end\":85269,\"start\":85263},{\"end\":85277,\"start\":85273},{\"end\":85288,\"start\":85281},{\"end\":85298,\"start\":85292},{\"end\":85310,\"start\":85305},{\"end\":85319,\"start\":85314},{\"end\":85329,\"start\":85323},{\"end\":85338,\"start\":85333},{\"end\":85352,\"start\":85342},{\"end\":85362,\"start\":85356},{\"end\":85371,\"start\":85366},{\"end\":85382,\"start\":85375},{\"end\":85395,\"start\":85386},{\"end\":85404,\"start\":85399},{\"end\":85415,\"start\":85408},{\"end\":85423,\"start\":85419},{\"end\":85433,\"start\":85427},{\"end\":85448,\"start\":85439},{\"end\":85459,\"start\":85452},{\"end\":85857,\"start\":85854},{\"end\":85864,\"start\":85861},{\"end\":85877,\"start\":85868},{\"end\":85887,\"start\":85881},{\"end\":85895,\"start\":85891},{\"end\":85907,\"start\":85899},{\"end\":85919,\"start\":85911},{\"end\":85928,\"start\":85923},{\"end\":85936,\"start\":85932},{\"end\":85947,\"start\":85940},{\"end\":85956,\"start\":85953},{\"end\":85969,\"start\":85960},{\"end\":85980,\"start\":85973},{\"end\":85989,\"start\":85984},{\"end\":85997,\"start\":85993},{\"end\":86006,\"start\":86001},{\"end\":86411,\"start\":86406},{\"end\":86420,\"start\":86415},{\"end\":86429,\"start\":86424},{\"end\":86437,\"start\":86433},{\"end\":86766,\"start\":86762},{\"end\":86773,\"start\":86770},{\"end\":86782,\"start\":86779},{\"end\":86791,\"start\":86788},{\"end\":86800,\"start\":86797},{\"end\":87192,\"start\":87188},{\"end\":87199,\"start\":87196},{\"end\":87207,\"start\":87203},{\"end\":87220,\"start\":87211},{\"end\":87239,\"start\":87226},{\"end\":87247,\"start\":87245},{\"end\":87747,\"start\":87745},{\"end\":87754,\"start\":87751},{\"end\":87769,\"start\":87760},{\"end\":88081,\"start\":88076},{\"end\":88087,\"start\":88085},{\"end\":88095,\"start\":88091},{\"end\":88344,\"start\":88339},{\"end\":88351,\"start\":88348},{\"end\":88362,\"start\":88358},{\"end\":88646,\"start\":88642},{\"end\":88654,\"start\":88650},{\"end\":88660,\"start\":88658},{\"end\":88668,\"start\":88664},{\"end\":88676,\"start\":88672},{\"end\":88683,\"start\":88680},{\"end\":88690,\"start\":88687},{\"end\":88699,\"start\":88694},{\"end\":88708,\"start\":88703},{\"end\":88716,\"start\":88712},{\"end\":88722,\"start\":88720},{\"end\":88730,\"start\":88726},{\"end\":88738,\"start\":88734},{\"end\":88746,\"start\":88742},{\"end\":88755,\"start\":88750},{\"end\":88762,\"start\":88759},{\"end\":88768,\"start\":88766},{\"end\":88776,\"start\":88772},{\"end\":88783,\"start\":88780},{\"end\":88793,\"start\":88790}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":231603388},\"end\":59335,\"start\":58953},{\"attributes\":{\"id\":\"b1\"},\"end\":59579,\"start\":59337},{\"attributes\":{\"id\":\"b2\"},\"end\":59835,\"start\":59581},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":232040593},\"end\":60352,\"start\":59837},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":210866760},\"end\":60706,\"start\":60354},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":8872108},\"end\":60927,\"start\":60708},{\"attributes\":{\"id\":\"b6\"},\"end\":61555,\"start\":60929},{\"attributes\":{\"id\":\"b7\"},\"end\":62058,\"start\":61557},{\"attributes\":{\"id\":\"b8\"},\"end\":62309,\"start\":62060},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":854678},\"end\":62745,\"start\":62311},{\"attributes\":{\"doi\":\"arXiv [cs.LG\",\"id\":\"b10\"},\"end\":63369,\"start\":62747},{\"attributes\":{\"doi\":\"arXiv [cs.CL\",\"id\":\"b11\"},\"end\":63704,\"start\":63371},{\"attributes\":{\"doi\":\"10.1007/s10489-022-04346-x\",\"id\":\"b12\"},\"end\":64087,\"start\":63706},{\"attributes\":{\"id\":\"b13\"},\"end\":64449,\"start\":64089},{\"attributes\":{\"id\":\"b14\"},\"end\":64672,\"start\":64451},{\"attributes\":{\"id\":\"b15\"},\"end\":64975,\"start\":64674},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":235494910},\"end\":65317,\"start\":64977},{\"attributes\":{\"id\":\"b17\"},\"end\":65569,\"start\":65319},{\"attributes\":{\"id\":\"b18\"},\"end\":65736,\"start\":65571},{\"attributes\":{\"id\":\"b19\"},\"end\":65957,\"start\":65738},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":220919723},\"end\":66387,\"start\":65959},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1915014},\"end\":66566,\"start\":66389},{\"attributes\":{\"doi\":\"arXiv [cs.CL]. arXiv\",\"id\":\"b22\"},\"end\":66837,\"start\":66568},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":250390647},\"end\":67595,\"start\":66839},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":248656269},\"end\":68084,\"start\":67597},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":202539059},\"end\":68586,\"start\":68086},{\"attributes\":{\"id\":\"b26\"},\"end\":69177,\"start\":68588},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":60811638},\"end\":69563,\"start\":69179},{\"attributes\":{\"id\":\"b28\"},\"end\":70060,\"start\":69565},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":245935159},\"end\":70407,\"start\":70062},{\"attributes\":{\"id\":\"b30\"},\"end\":70762,\"start\":70409},{\"attributes\":{\"id\":\"b31\"},\"end\":71437,\"start\":70764},{\"attributes\":{\"id\":\"b32\"},\"end\":71623,\"start\":71439},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":254876189},\"end\":72183,\"start\":71625},{\"attributes\":{\"doi\":\"arXiv [cs.CL\",\"id\":\"b34\"},\"end\":72610,\"start\":72185},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":225110283},\"end\":72909,\"start\":72612},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":233476528},\"end\":73374,\"start\":72911},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":256077799},\"end\":73792,\"start\":73376},{\"attributes\":{\"id\":\"b38\"},\"end\":74188,\"start\":73794},{\"attributes\":{\"id\":\"b39\"},\"end\":74405,\"start\":74190},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":248377870},\"end\":74567,\"start\":74407},{\"attributes\":{\"doi\":\"10.21105/joss.00861\",\"id\":\"b41\",\"matched_paper_id\":53244226},\"end\":74930,\"start\":74569},{\"attributes\":{\"id\":\"b42\"},\"end\":75213,\"start\":74932},{\"attributes\":{\"id\":\"b43\"},\"end\":75374,\"start\":75215},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":248780433},\"end\":75859,\"start\":75376},{\"attributes\":{\"doi\":\"arXiv [cs.CL\",\"id\":\"b45\"},\"end\":76124,\"start\":75861},{\"attributes\":{\"id\":\"b46\"},\"end\":76389,\"start\":76126},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":257429474},\"end\":76947,\"start\":76391},{\"attributes\":{\"id\":\"b48\"},\"end\":77218,\"start\":76949},{\"attributes\":{\"doi\":\"arXiv [cs.CL\",\"id\":\"b49\"},\"end\":77360,\"start\":77220},{\"attributes\":{\"id\":\"b50\"},\"end\":77585,\"start\":77362},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":247815425},\"end\":77938,\"start\":77587},{\"attributes\":{\"id\":\"b52\"},\"end\":78290,\"start\":77940},{\"attributes\":{\"id\":\"b53\"},\"end\":78648,\"start\":78292},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":218889776},\"end\":79050,\"start\":78650},{\"attributes\":{\"id\":\"b55\"},\"end\":79310,\"start\":79052},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":231925131},\"end\":79735,\"start\":79312},{\"attributes\":{\"id\":\"b57\"},\"end\":80268,\"start\":79737},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":220073138},\"end\":81026,\"start\":80270},{\"attributes\":{\"doi\":\"10.1038/d41586-023-00816-5\",\"id\":\"b59\"},\"end\":81215,\"start\":81028},{\"attributes\":{\"doi\":\"arXiv [cs.AI\",\"id\":\"b60\"},\"end\":81514,\"start\":81217},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":256272939},\"end\":81831,\"start\":81516},{\"attributes\":{\"id\":\"b62\"},\"end\":82163,\"start\":81833},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":253317929},\"end\":82686,\"start\":82165},{\"attributes\":{\"id\":\"b64\"},\"end\":82842,\"start\":82688},{\"attributes\":{\"id\":\"b65\"},\"end\":83089,\"start\":82844},{\"attributes\":{\"doi\":\"10.4324/9780203936399-29/probabilistic-topic-models-mark-steyvers-tom-griffiths\",\"id\":\"b66\"},\"end\":83389,\"start\":83091},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":7961699},\"end\":83765,\"start\":83391},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":238993716},\"end\":84227,\"start\":83767},{\"attributes\":{\"id\":\"b69\"},\"end\":84836,\"start\":84229},{\"attributes\":{\"doi\":\"arXiv [cs.CL\",\"id\":\"b70\"},\"end\":85191,\"start\":84838},{\"attributes\":{\"id\":\"b71\"},\"end\":85850,\"start\":85193},{\"attributes\":{\"id\":\"b72\"},\"end\":86367,\"start\":85852},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":231732437},\"end\":86646,\"start\":86369},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":247142711},\"end\":87110,\"start\":86648},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":195069387},\"end\":87611,\"start\":87112},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":254668263},\"end\":88002,\"start\":87613},{\"attributes\":{\"id\":\"b77\"},\"end\":88276,\"start\":88004},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":12255087},\"end\":88601,\"start\":88278},{\"attributes\":{\"id\":\"b79\"},\"end\":89108,\"start\":88603},{\"attributes\":{\"id\":\"b80\"},\"end\":89234,\"start\":89110},{\"attributes\":{\"id\":\"b81\"},\"end\":89356,\"start\":89236}]", "bib_title": "[{\"end\":59005,\"start\":58953},{\"end\":59905,\"start\":59837},{\"end\":60435,\"start\":60354},{\"end\":60743,\"start\":60708},{\"end\":62404,\"start\":62311},{\"end\":65073,\"start\":64977},{\"end\":66044,\"start\":65959},{\"end\":66411,\"start\":66389},{\"end\":66915,\"start\":66839},{\"end\":67699,\"start\":67597},{\"end\":68192,\"start\":68086},{\"end\":69257,\"start\":69179},{\"end\":70136,\"start\":70062},{\"end\":71729,\"start\":71625},{\"end\":72671,\"start\":72612},{\"end\":72986,\"start\":72911},{\"end\":73537,\"start\":73376},{\"end\":74447,\"start\":74407},{\"end\":74620,\"start\":74569},{\"end\":75008,\"start\":74932},{\"end\":75449,\"start\":75376},{\"end\":76480,\"start\":76391},{\"end\":77030,\"start\":76949},{\"end\":77668,\"start\":77587},{\"end\":78771,\"start\":78650},{\"end\":79386,\"start\":79312},{\"end\":80338,\"start\":80270},{\"end\":81579,\"start\":81516},{\"end\":82284,\"start\":82165},{\"end\":83441,\"start\":83391},{\"end\":83861,\"start\":83767},{\"end\":86400,\"start\":86369},{\"end\":86756,\"start\":86648},{\"end\":87184,\"start\":87112},{\"end\":87741,\"start\":87613},{\"end\":88335,\"start\":88278}]", "bib_author": "[{\"end\":59015,\"start\":59007},{\"end\":59026,\"start\":59015},{\"end\":59033,\"start\":59026},{\"end\":59418,\"start\":59409},{\"end\":59650,\"start\":59639},{\"end\":59656,\"start\":59650},{\"end\":59665,\"start\":59656},{\"end\":59919,\"start\":59907},{\"end\":59928,\"start\":59919},{\"end\":59946,\"start\":59928},{\"end\":59960,\"start\":59946},{\"end\":60447,\"start\":60437},{\"end\":60463,\"start\":60447},{\"end\":60474,\"start\":60463},{\"end\":60483,\"start\":60474},{\"end\":60755,\"start\":60745},{\"end\":60769,\"start\":60755},{\"end\":60940,\"start\":60929},{\"end\":60948,\"start\":60940},{\"end\":60957,\"start\":60948},{\"end\":60968,\"start\":60957},{\"end\":60978,\"start\":60968},{\"end\":60990,\"start\":60978},{\"end\":61005,\"start\":60990},{\"end\":61014,\"start\":61005},{\"end\":61024,\"start\":61014},{\"end\":61034,\"start\":61024},{\"end\":61045,\"start\":61034},{\"end\":61061,\"start\":61045},{\"end\":61072,\"start\":61061},{\"end\":61084,\"start\":61072},{\"end\":61093,\"start\":61084},{\"end\":61103,\"start\":61093},{\"end\":61116,\"start\":61103},{\"end\":61122,\"start\":61116},{\"end\":61132,\"start\":61122},{\"end\":61142,\"start\":61132},{\"end\":61621,\"start\":61610},{\"end\":61631,\"start\":61621},{\"end\":61642,\"start\":61631},{\"end\":61655,\"start\":61642},{\"end\":61671,\"start\":61655},{\"end\":61678,\"start\":61671},{\"end\":61689,\"start\":61678},{\"end\":61698,\"start\":61689},{\"end\":61706,\"start\":61698},{\"end\":61720,\"start\":61706},{\"end\":61729,\"start\":61720},{\"end\":61739,\"start\":61729},{\"end\":62132,\"start\":62124},{\"end\":62414,\"start\":62406},{\"end\":62432,\"start\":62414},{\"end\":62439,\"start\":62432},{\"end\":62805,\"start\":62797},{\"end\":62815,\"start\":62805},{\"end\":62822,\"start\":62815},{\"end\":62830,\"start\":62822},{\"end\":62853,\"start\":62830},{\"end\":62863,\"start\":62853},{\"end\":62874,\"start\":62863},{\"end\":62883,\"start\":62874},{\"end\":62893,\"start\":62883},{\"end\":62905,\"start\":62893},{\"end\":62912,\"start\":62905},{\"end\":62920,\"start\":62912},{\"end\":62931,\"start\":62920},{\"end\":62941,\"start\":62931},{\"end\":62951,\"start\":62941},{\"end\":62961,\"start\":62951},{\"end\":62972,\"start\":62961},{\"end\":62980,\"start\":62972},{\"end\":62988,\"start\":62980},{\"end\":62999,\"start\":62988},{\"end\":63463,\"start\":63453},{\"end\":63475,\"start\":63463},{\"end\":63482,\"start\":63475},{\"end\":63495,\"start\":63482},{\"end\":63813,\"start\":63805},{\"end\":63819,\"start\":63813},{\"end\":63825,\"start\":63819},{\"end\":63833,\"start\":63825},{\"end\":63842,\"start\":63833},{\"end\":64193,\"start\":64181},{\"end\":64204,\"start\":64193},{\"end\":64215,\"start\":64204},{\"end\":64223,\"start\":64215},{\"end\":64526,\"start\":64515},{\"end\":65087,\"start\":65075},{\"end\":65099,\"start\":65087},{\"end\":65404,\"start\":65388},{\"end\":65583,\"start\":65573},{\"end\":65592,\"start\":65583},{\"end\":65601,\"start\":65592},{\"end\":65613,\"start\":65601},{\"end\":65631,\"start\":65613},{\"end\":65644,\"start\":65631},{\"end\":66052,\"start\":66046},{\"end\":66060,\"start\":66052},{\"end\":66069,\"start\":66060},{\"end\":66078,\"start\":66069},{\"end\":66089,\"start\":66078},{\"end\":66096,\"start\":66089},{\"end\":66107,\"start\":66096},{\"end\":66114,\"start\":66107},{\"end\":66122,\"start\":66114},{\"end\":66427,\"start\":66413},{\"end\":66442,\"start\":66427},{\"end\":66651,\"start\":66645},{\"end\":66658,\"start\":66651},{\"end\":66923,\"start\":66917},{\"end\":66935,\"start\":66923},{\"end\":66954,\"start\":66935},{\"end\":66964,\"start\":66954},{\"end\":66972,\"start\":66964},{\"end\":66982,\"start\":66972},{\"end\":66992,\"start\":66982},{\"end\":67712,\"start\":67701},{\"end\":67722,\"start\":67712},{\"end\":67733,\"start\":67722},{\"end\":67749,\"start\":67733},{\"end\":67762,\"start\":67749},{\"end\":67773,\"start\":67762},{\"end\":68201,\"start\":68194},{\"end\":68208,\"start\":68201},{\"end\":68218,\"start\":68208},{\"end\":68231,\"start\":68218},{\"end\":68706,\"start\":68688},{\"end\":68719,\"start\":68706},{\"end\":68734,\"start\":68719},{\"end\":68745,\"start\":68734},{\"end\":68763,\"start\":68745},{\"end\":68779,\"start\":68763},{\"end\":68802,\"start\":68779},{\"end\":69271,\"start\":69259},{\"end\":69283,\"start\":69271},{\"end\":69725,\"start\":69711},{\"end\":69733,\"start\":69725},{\"end\":69750,\"start\":69733},{\"end\":69769,\"start\":69750},{\"end\":70151,\"start\":70138},{\"end\":70164,\"start\":70151},{\"end\":70480,\"start\":70471},{\"end\":70489,\"start\":70480},{\"end\":70499,\"start\":70489},{\"end\":70507,\"start\":70499},{\"end\":70526,\"start\":70507},{\"end\":70539,\"start\":70526},{\"end\":70771,\"start\":70764},{\"end\":70778,\"start\":70771},{\"end\":70788,\"start\":70778},{\"end\":70795,\"start\":70788},{\"end\":70803,\"start\":70795},{\"end\":70813,\"start\":70803},{\"end\":70821,\"start\":70813},{\"end\":70828,\"start\":70821},{\"end\":70835,\"start\":70828},{\"end\":70842,\"start\":70835},{\"end\":70849,\"start\":70842},{\"end\":70858,\"start\":70849},{\"end\":70865,\"start\":70858},{\"end\":70872,\"start\":70865},{\"end\":70880,\"start\":70872},{\"end\":70887,\"start\":70880},{\"end\":70895,\"start\":70887},{\"end\":70902,\"start\":70895},{\"end\":70910,\"start\":70902},{\"end\":70918,\"start\":70910},{\"end\":71509,\"start\":71493},{\"end\":71741,\"start\":71731},{\"end\":71753,\"start\":71741},{\"end\":71766,\"start\":71753},{\"end\":71776,\"start\":71766},{\"end\":71787,\"start\":71776},{\"end\":71798,\"start\":71787},{\"end\":71810,\"start\":71798},{\"end\":71821,\"start\":71810},{\"end\":71837,\"start\":71821},{\"end\":71848,\"start\":71837},{\"end\":71857,\"start\":71848},{\"end\":72254,\"start\":72248},{\"end\":72262,\"start\":72254},{\"end\":72270,\"start\":72262},{\"end\":72281,\"start\":72270},{\"end\":72292,\"start\":72281},{\"end\":72305,\"start\":72292},{\"end\":72317,\"start\":72305},{\"end\":72327,\"start\":72317},{\"end\":72339,\"start\":72327},{\"end\":72349,\"start\":72339},{\"end\":72679,\"start\":72673},{\"end\":72685,\"start\":72679},{\"end\":72693,\"start\":72685},{\"end\":72700,\"start\":72693},{\"end\":72707,\"start\":72700},{\"end\":72995,\"start\":72988},{\"end\":73002,\"start\":72995},{\"end\":73009,\"start\":73002},{\"end\":73015,\"start\":73009},{\"end\":73023,\"start\":73015},{\"end\":73035,\"start\":73023},{\"end\":73546,\"start\":73539},{\"end\":73552,\"start\":73546},{\"end\":73558,\"start\":73552},{\"end\":73858,\"start\":73851},{\"end\":73865,\"start\":73858},{\"end\":73874,\"start\":73865},{\"end\":73880,\"start\":73874},{\"end\":73889,\"start\":73880},{\"end\":73897,\"start\":73889},{\"end\":73905,\"start\":73897},{\"end\":73914,\"start\":73905},{\"end\":73929,\"start\":73914},{\"end\":73941,\"start\":73929},{\"end\":74288,\"start\":74273},{\"end\":74294,\"start\":74288},{\"end\":74462,\"start\":74449},{\"end\":74633,\"start\":74622},{\"end\":74642,\"start\":74633},{\"end\":74650,\"start\":74642},{\"end\":74664,\"start\":74650},{\"end\":75020,\"start\":75010},{\"end\":75229,\"start\":75217},{\"end\":75237,\"start\":75229},{\"end\":75241,\"start\":75237},{\"end\":75253,\"start\":75241},{\"end\":75264,\"start\":75253},{\"end\":75274,\"start\":75264},{\"end\":75285,\"start\":75274},{\"end\":75478,\"start\":75451},{\"end\":75932,\"start\":75920},{\"end\":75938,\"start\":75932},{\"end\":75950,\"start\":75938},{\"end\":76220,\"start\":76213},{\"end\":76228,\"start\":76220},{\"end\":76239,\"start\":76228},{\"end\":76494,\"start\":76482},{\"end\":76504,\"start\":76494},{\"end\":76514,\"start\":76504},{\"end\":76527,\"start\":76514},{\"end\":76540,\"start\":76527},{\"end\":76556,\"start\":76540},{\"end\":76568,\"start\":76556},{\"end\":77040,\"start\":77032},{\"end\":77252,\"start\":77244},{\"end\":77427,\"start\":77417},{\"end\":77677,\"start\":77670},{\"end\":77684,\"start\":77677},{\"end\":78012,\"start\":78001},{\"end\":78026,\"start\":78012},{\"end\":78038,\"start\":78026},{\"end\":78051,\"start\":78038},{\"end\":78356,\"start\":78345},{\"end\":78362,\"start\":78356},{\"end\":78371,\"start\":78362},{\"end\":78379,\"start\":78371},{\"end\":78389,\"start\":78379},{\"end\":78402,\"start\":78389},{\"end\":78782,\"start\":78773},{\"end\":78791,\"start\":78782},{\"end\":78798,\"start\":78791},{\"end\":78805,\"start\":78798},{\"end\":78812,\"start\":78805},{\"end\":79127,\"start\":79116},{\"end\":79139,\"start\":79127},{\"end\":79400,\"start\":79388},{\"end\":79412,\"start\":79400},{\"end\":79750,\"start\":79737},{\"end\":79768,\"start\":79750},{\"end\":79782,\"start\":79768},{\"end\":79795,\"start\":79782},{\"end\":79807,\"start\":79795},{\"end\":79824,\"start\":79807},{\"end\":79837,\"start\":79824},{\"end\":79860,\"start\":79837},{\"end\":80352,\"start\":80340},{\"end\":80363,\"start\":80352},{\"end\":80379,\"start\":80363},{\"end\":80392,\"start\":80379},{\"end\":80405,\"start\":80392},{\"end\":80425,\"start\":80405},{\"end\":80438,\"start\":80425},{\"end\":81079,\"start\":81066},{\"end\":81302,\"start\":81293},{\"end\":81308,\"start\":81302},{\"end\":81316,\"start\":81308},{\"end\":81323,\"start\":81316},{\"end\":81589,\"start\":81581},{\"end\":81600,\"start\":81589},{\"end\":81609,\"start\":81600},{\"end\":81621,\"start\":81609},{\"end\":81629,\"start\":81621},{\"end\":81637,\"start\":81629},{\"end\":81644,\"start\":81637},{\"end\":81915,\"start\":81907},{\"end\":81923,\"start\":81915},{\"end\":81930,\"start\":81923},{\"end\":81936,\"start\":81930},{\"end\":81942,\"start\":81936},{\"end\":81952,\"start\":81942},{\"end\":82294,\"start\":82286},{\"end\":82300,\"start\":82294},{\"end\":82306,\"start\":82300},{\"end\":82315,\"start\":82306},{\"end\":82323,\"start\":82315},{\"end\":82330,\"start\":82323},{\"end\":82337,\"start\":82330},{\"end\":82345,\"start\":82337},{\"end\":82701,\"start\":82690},{\"end\":82712,\"start\":82701},{\"end\":82720,\"start\":82712},{\"end\":82733,\"start\":82720},{\"end\":82743,\"start\":82733},{\"end\":82756,\"start\":82743},{\"end\":82931,\"start\":82918},{\"end\":83146,\"start\":83134},{\"end\":83456,\"start\":83443},{\"end\":83467,\"start\":83456},{\"end\":83475,\"start\":83467},{\"end\":83871,\"start\":83863},{\"end\":83878,\"start\":83871},{\"end\":83885,\"start\":83878},{\"end\":83892,\"start\":83885},{\"end\":83898,\"start\":83892},{\"end\":83904,\"start\":83898},{\"end\":83911,\"start\":83904},{\"end\":84290,\"start\":84277},{\"end\":84304,\"start\":84290},{\"end\":84312,\"start\":84304},{\"end\":84323,\"start\":84312},{\"end\":84339,\"start\":84323},{\"end\":84351,\"start\":84339},{\"end\":84358,\"start\":84351},{\"end\":84365,\"start\":84358},{\"end\":84374,\"start\":84365},{\"end\":84380,\"start\":84374},{\"end\":84386,\"start\":84380},{\"end\":84393,\"start\":84386},{\"end\":84404,\"start\":84393},{\"end\":84416,\"start\":84404},{\"end\":84428,\"start\":84416},{\"end\":84437,\"start\":84428},{\"end\":84447,\"start\":84437},{\"end\":84459,\"start\":84447},{\"end\":84466,\"start\":84459},{\"end\":84472,\"start\":84466},{\"end\":84876,\"start\":84865},{\"end\":84887,\"start\":84876},{\"end\":84897,\"start\":84887},{\"end\":84910,\"start\":84897},{\"end\":84919,\"start\":84910},{\"end\":84930,\"start\":84919},{\"end\":84940,\"start\":84930},{\"end\":84954,\"start\":84940},{\"end\":85261,\"start\":85248},{\"end\":85271,\"start\":85261},{\"end\":85279,\"start\":85271},{\"end\":85290,\"start\":85279},{\"end\":85300,\"start\":85290},{\"end\":85312,\"start\":85300},{\"end\":85321,\"start\":85312},{\"end\":85331,\"start\":85321},{\"end\":85340,\"start\":85331},{\"end\":85354,\"start\":85340},{\"end\":85364,\"start\":85354},{\"end\":85373,\"start\":85364},{\"end\":85384,\"start\":85373},{\"end\":85397,\"start\":85384},{\"end\":85406,\"start\":85397},{\"end\":85417,\"start\":85406},{\"end\":85425,\"start\":85417},{\"end\":85435,\"start\":85425},{\"end\":85450,\"start\":85435},{\"end\":85461,\"start\":85450},{\"end\":85859,\"start\":85852},{\"end\":85866,\"start\":85859},{\"end\":85879,\"start\":85866},{\"end\":85889,\"start\":85879},{\"end\":85897,\"start\":85889},{\"end\":85909,\"start\":85897},{\"end\":85921,\"start\":85909},{\"end\":85930,\"start\":85921},{\"end\":85938,\"start\":85930},{\"end\":85949,\"start\":85938},{\"end\":85958,\"start\":85949},{\"end\":85971,\"start\":85958},{\"end\":85982,\"start\":85971},{\"end\":85991,\"start\":85982},{\"end\":85999,\"start\":85991},{\"end\":86008,\"start\":85999},{\"end\":86413,\"start\":86402},{\"end\":86422,\"start\":86413},{\"end\":86431,\"start\":86422},{\"end\":86439,\"start\":86431},{\"end\":86768,\"start\":86758},{\"end\":86775,\"start\":86768},{\"end\":86784,\"start\":86775},{\"end\":86793,\"start\":86784},{\"end\":86802,\"start\":86793},{\"end\":87194,\"start\":87186},{\"end\":87201,\"start\":87194},{\"end\":87209,\"start\":87201},{\"end\":87222,\"start\":87209},{\"end\":87241,\"start\":87222},{\"end\":87249,\"start\":87241},{\"end\":87749,\"start\":87743},{\"end\":87756,\"start\":87749},{\"end\":87771,\"start\":87756},{\"end\":88083,\"start\":88074},{\"end\":88089,\"start\":88083},{\"end\":88097,\"start\":88089},{\"end\":88346,\"start\":88337},{\"end\":88353,\"start\":88346},{\"end\":88364,\"start\":88353},{\"end\":88648,\"start\":88638},{\"end\":88656,\"start\":88648},{\"end\":88662,\"start\":88656},{\"end\":88670,\"start\":88662},{\"end\":88678,\"start\":88670},{\"end\":88685,\"start\":88678},{\"end\":88692,\"start\":88685},{\"end\":88701,\"start\":88692},{\"end\":88710,\"start\":88701},{\"end\":88718,\"start\":88710},{\"end\":88724,\"start\":88718},{\"end\":88732,\"start\":88724},{\"end\":88740,\"start\":88732},{\"end\":88748,\"start\":88740},{\"end\":88757,\"start\":88748},{\"end\":88764,\"start\":88757},{\"end\":88770,\"start\":88764},{\"end\":88778,\"start\":88770},{\"end\":88785,\"start\":88778},{\"end\":88795,\"start\":88785}]", "bib_venue": "[{\"end\":59103,\"start\":59033},{\"end\":59407,\"start\":59337},{\"end\":59637,\"start\":59581},{\"end\":60044,\"start\":59960},{\"end\":60511,\"start\":60483},{\"end\":60801,\"start\":60769},{\"end\":61179,\"start\":61142},{\"end\":61608,\"start\":61557},{\"end\":62122,\"start\":62060},{\"end\":62509,\"start\":62439},{\"end\":62795,\"start\":62747},{\"end\":63451,\"start\":63371},{\"end\":63803,\"start\":63706},{\"end\":64179,\"start\":64089},{\"end\":64513,\"start\":64451},{\"end\":64742,\"start\":64674},{\"end\":65135,\"start\":65099},{\"end\":65386,\"start\":65319},{\"end\":65834,\"start\":65740},{\"end\":66151,\"start\":66122},{\"end\":66460,\"start\":66442},{\"end\":66643,\"start\":66568},{\"end\":67134,\"start\":66992},{\"end\":67816,\"start\":67773},{\"end\":68292,\"start\":68231},{\"end\":68686,\"start\":68588},{\"end\":69332,\"start\":69283},{\"end\":69709,\"start\":69565},{\"end\":70221,\"start\":70164},{\"end\":70469,\"start\":70409},{\"end\":71057,\"start\":70918},{\"end\":71491,\"start\":71439},{\"end\":71876,\"start\":71857},{\"end\":72246,\"start\":72185},{\"end\":72738,\"start\":72707},{\"end\":73096,\"start\":73035},{\"end\":73561,\"start\":73558},{\"end\":73849,\"start\":73794},{\"end\":74271,\"start\":74190},{\"end\":74470,\"start\":74462},{\"end\":74714,\"start\":74683},{\"end\":75027,\"start\":75020},{\"end\":75572,\"start\":75478},{\"end\":75918,\"start\":75861},{\"end\":76211,\"start\":76126},{\"end\":76610,\"start\":76568},{\"end\":77046,\"start\":77040},{\"end\":77242,\"start\":77220},{\"end\":77415,\"start\":77362},{\"end\":77743,\"start\":77684},{\"end\":77999,\"start\":77940},{\"end\":78343,\"start\":78292},{\"end\":78832,\"start\":78812},{\"end\":79114,\"start\":79052},{\"end\":79512,\"start\":79412},{\"end\":79967,\"start\":79860},{\"end\":80638,\"start\":80438},{\"end\":81064,\"start\":81028},{\"end\":81291,\"start\":81217},{\"end\":81653,\"start\":81644},{\"end\":81905,\"start\":81833},{\"end\":82403,\"start\":82345},{\"end\":82916,\"start\":82844},{\"end\":83132,\"start\":83091},{\"end\":83524,\"start\":83475},{\"end\":83977,\"start\":83911},{\"end\":84275,\"start\":84229},{\"end\":84863,\"start\":84838},{\"end\":85246,\"start\":85193},{\"end\":86051,\"start\":86008},{\"end\":86487,\"start\":86439},{\"end\":86832,\"start\":86802},{\"end\":87298,\"start\":87249},{\"end\":87788,\"start\":87771},{\"end\":88072,\"start\":88004},{\"end\":88421,\"start\":88364},{\"end\":88636,\"start\":88603},{\"end\":89171,\"start\":89110},{\"end\":89295,\"start\":89236},{\"end\":59160,\"start\":59105},{\"end\":60115,\"start\":60046},{\"end\":67263,\"start\":67136},{\"end\":68340,\"start\":68294},{\"end\":73144,\"start\":73098},{\"end\":75653,\"start\":75574}]"}}}, "year": 2023, "month": 12, "day": 17}