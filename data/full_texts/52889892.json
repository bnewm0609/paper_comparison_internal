{"id": 52889892, "updated": "2023-10-02 23:19:30.798", "metadata": {"title": "Patient Risk Assessment and Warning Symptom Detection Using Deep Attention-Based Neural Networks", "authors": "[{\"first\":\"Ivan\",\"last\":\"Girardi\",\"middle\":[]},{\"first\":\"Pengfei\",\"last\":\"Ji\",\"middle\":[]},{\"first\":\"An-phi\",\"last\":\"Nguyen\",\"middle\":[]},{\"first\":\"Nora\",\"last\":\"Hollenstein\",\"middle\":[]},{\"first\":\"Adam\",\"last\":\"Ivankay\",\"middle\":[]},{\"first\":\"Lorenz\",\"last\":\"Kuhn\",\"middle\":[]},{\"first\":\"Chiara\",\"last\":\"Marchiori\",\"middle\":[]},{\"first\":\"Ce\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "We present an operational component of a real-world patient triage system. Given a specific patient presentation, the system is able to assess the level of medical urgency and issue the most appropriate recommendation in terms of best point of care and time to treat. We use an attention-based convolutional neural network architecture trained on 600,000 doctor notes in German. We compare two approaches, one that uses the full text of the medical notes and one that uses only a selected list of medical entities extracted from the text. These approaches achieve 79% and 66% precision, respectively, but on a confidence threshold of 0.6, precision increases to 85% and 75%, respectively. In addition, a method to detect warning symptoms is implemented to render the classification task transparent from a medical perspective. The method is based on the learning of attention scores and a method of automatic validation using the same data.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1809.10804", "mag": "2962891276", "acl": "W18-5616", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl-louhi/GirardiJNHIKMZ18", "doi": "10.18653/v1/w18-5616"}}, "content": {"source": {"pdf_hash": "2328ef7daef8abdbe255cb360ec3b9bf48b913c7", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1809.10804v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/W18-5616.pdf", "status": "HYBRID"}}, "grobid": {"id": "4936c4d50572c1be814fd397894917fec402d4cf", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2328ef7daef8abdbe255cb360ec3b9bf48b913c7.txt", "contents": "\nPatient Risk Assessment and Warning Symptom Detection Using Deep Attention-Based Neural Networks\n\n\nIvan Girardi \nIBM Research Zurich\nSwitzerland\n\nPengfei Ji \nIBM Research Zurich\nSwitzerland\n\nETH Zurich\nSwitzerland\n\nAn-Phi Nguyen \nIBM Research Zurich\nSwitzerland\n\nNora Hollenstein noraho@ethz.ch \nIBM Research Zurich\nSwitzerland\n\nETH Zurich\nSwitzerland\n\nAdam Ivankay \nIBM Research Zurich\nSwitzerland\n\nLorenz Kuhn kuhnl@student.ethz.ch \nIBM Research Zurich\nSwitzerland\n\nChiara Marchiori \nIBM Research Zurich\nSwitzerland\n\nCe Zhang ce.zhang@ethz.ch \nETH Zurich\nSwitzerland\n\nPatient Risk Assessment and Warning Symptom Detection Using Deep Attention-Based Neural Networks\n\nWe present an operational component of a real-world patient triage system. Given a specific patient presentation, the system is able to assess the level of medical urgency and issue the most appropriate recommendation in terms of best point of care and time to treat. We use an attention-based convolutional neural network architecture trained on 600,000 doctor notes in German. We compare two approaches, one that uses the full text of the medical notes and one that uses only a selected list of medical entities extracted from the text. These approaches achieve 79% and 66% precision, respectively, but on a confidence threshold of 0.6, precision increases to 85% and 75%, respectively. In addition, a method to detect warning symptoms is implemented to render the classification task transparent from a medical perspective. The method is based on the learning of attention scores and a method of automatic validation using the same data.\n\nIntroduction\n\nSeveral intelligent triage systems have recently been developed that attempt to evaluate automatically the risk related to specific patient conditions and direct patients to the appropriate care provider (Semigran et al., 2015). The work presented here is part of an interactive triage system being developed for industrial applications. The system takes patient demographics and symptoms as input, assesses their current medical conditions and suggests where and by when the patients should seek medical care. A key feature of the system is the detection of warning symptoms, namely, red flags. This is crucial to distinguish potential emergencies from common or less urgent cases and therefore provides the medical rationale behind a given recommendation. In addition, for triage systems that involve a dialogue with patients through multiple question-and-answer interactions (such as Ada (2018)), warning symptom detection is fundamental to determine the most informative questions to ask patients.\n\nWe propose a model that assesses patient risk and detects warning symptoms based on a large volume of doctor notes in German, sometimes even mixed with Swiss German expressions. In this context, assessing patient risk can be regarded as a supervised text classification task, where the content of the medical records represents the feature space, and the recommendations assigned by medical professionals are the ground truth labels. The use of recurrent neural networks (RNN) has been proposed to solve text classification tasks (Tang et al., 2015). However, the proposed RNN models must be modified to be consistent with the requirement that warning symptoms must be detected, because in RNNs it is generally not possible to know which hidden states are most relevant.\n\nTo address these challenges, we propose an integrated approach to assess patient risk and detect warning symptoms simultaneously using an attention-based convolutional neural network (ACNN), which is a combination of a convolutional neural network (CNN) and an attention mechanism (Kim, 2014;Yang et al., 2016;Du et al., 2017). To the best of our knowledge, such an integrated approach is applied for the first time to the medical domain.\n\nThe main contributions of this paper are twofold. First, we propose a neural network architecture that can be used simultaneously for text classification and the detection of important words. Comparing our model to other neural architectures of similar complexity, we achieve competitive classification results. The model is especially useful to explain the recommendation rationale in classification scenarios, where the given input consists of a set of extracted entities, rather than full text. Second, a formal pipeline to detect warning symptoms based on learned importance factors is applied in an industrial application. Our model identifies symptoms that indicate a medical emergency. These warning symptoms can then be used by intelligent medical care services or in an ontology.\n\n2 Related Work 2.1 Text Classification with Deep Learning Traditional text classification approaches represent documents with sparse lexical features, such as n-grams, and use a linear model or kernel methods on this representation (Wang and Manning, 2012;Joachims, 1998). More recently, deep learning technologies have been applied to text categorization problems. RNNs are designed to handle sequences of any length and capture long-term dependencies. Like sequence-based (Tang et al., 2015) and tree-structured (Tai et al., 2015) models, they have achieved remarkable results in document modeling.\n\nMoreover, CNN models have achieved high accuracy on text categorization. For example, Kim (2014) used one convolutional layer (with multiple widths and filters) followed by a max pooling layer over time. Johnson and Zhang (2015) built a model that uses up to six convolutional layers, followed by three fully connected classification layers. Conneau et al. (2016) published a model with a 32-layer character-level CNN, that achieved a significant improvement on a large dataset. Models that combine CNN and RNN components for document classification also yield competitive results on several public datasets (Zhou et al., 2015;Lai et al., 2015).\n\nTo the best of our knowledge, not many research efforts have focused on augmenting CNNs for text classification with attention mechanisms. In fact, attention layers are more typically coupled with RNNs in order to better handle long-term dependencies (Yang et al., 2016). Interestingly, Du et al. (2017) used a CNN not as a classifier, but to compute the attention weights to apply to the hidden layers of a RNN. An example of combining attention layers with a CNN is the work by Shen and Huang (2016). However, the authors do not augment the CNN features using attention weights. They use an attention mechanism to compute sentence-level features, which they then concatenate to the convolutional features to ultimately perform the classification.\n\n\nIntelligent Triage Systems\n\nIntelligent triage systems inform patients where and when they should seek medical care, based on methods such as expert rules, Bayesian inference and deep learning (Semigran et al., 2015). For example, Symptomate (2018) uses a Bayesian network and a medical database for triage advice. Clinical records written by medical experts have also been used to make triage suggestions with deep learning technologies. Li et al. (2017) uses a shallow CNN model to predict a patient's diseases using the corresponding admission notes. Nigam (2016) applied a LSTM model to the multilabel classification task of assigning ICD-9 labels to medical notes.\n\n\nMethodology\n\n\nData Processing\n\nTo build the triage application described here, we used 600,000 case records written in German and collected over the past five years. This is only 50% of the total available data, as we selected only those cases treated by top-ranked doctors. Case records contain demographic data such as age and gender, previous illnesses, and a full-text description of the patient's current medical condition. Potential diagnoses consistent with the symptom description are listed.\n\nThe descriptions in the records are expressed in formal medical language as well as in layman's terminology. The notes are not always written in complete sentences and include misspellings, dialect vocabulary, non-standard medical abbreviations and inconsistent punctuation. This is a challenge for the linguistic processing of case files.\n\nThe original case records are very unevenly distributed over ten recommendation classes (a combination of a point-of-care and a time-to-treat class). To mitigate this problem and for the purpose of this work, the original classes, (emergency, urgent), (grundversorger, urgent), (specialist, urgent), (grundversorger, within a day), (specialist, within a day), (grundversorger, not urgent), (specialist, not urgent), (telecare, -), were merged, with the help of healthcare professionals, into three categories: Urgent Care, General Practice, Telecare. The categorization of cases is shown in Table 1.  \n\n\nNLP Pipeline\n\nA natural language processing (NLP) pipeline extracted medically relevant concepts associated with each written case. The pipeline consisted of the following stages: (1) data preprocessing for misspelling correction and abbreviation expansion, (2) named entity recognition (NER) and\n\n(3) concept clustering. Acronyms and abbreviations used unambiguously were linked to the corresponding entities directly in the dictionaries. Ambiguous acronyms and abbreviations were resolved, when possible, using algorithms that include context for disambiguation. For NER, we used a rule-based medical entity extraction system built with IBM Watson Explorer, using algorithms based on dictionary look-up and advanced rules. This allowed us to detect 51 entity types in the following categories: anatomy, physiology, symptoms, diseases, medical procedures, medicines, negated symptoms, negated diseases, ability/inability of, foreign-body objects, negations, patient information, symptom characterization, disease characterization, time expressions. The distinction between symptoms and diagnosis was made using existing ontologies, where these semantic types were assigned with the help of a team of clinical experts. The dictionaries used in the NER were built partially based on existing German-language medical dictionaries and ontologies (UMLS mapped German terms, ICD10, Meddra, etc.) and partially using the list of words contained in the case records. The dictionaries therefore contain a mapping of technical and layman's terms. The NLP pipeline was designed to detect and resolve the negated mentions of the entities listed above (using German language-specific negation particles or expressions), which are very frequent in this type of records. Only 31 entity types in the categories symptoms, diseases, ability/inability of, negated symptoms, negated diseases were included in the current final list. The average number of extracted annotations per case was 70 for all entities, but only 17 for the selected entities. Performance was evaluated using the manual annotations of a set of ground truth cases performed by a team of clinical experts. Concept clustering is a hierarchical procedure that allowed us to group annotations describing the same medical concept. The same entity may be expressed in a variety of forms (compound vs. simple nouns, dialect or common language vs. medical terminology). Concept clustering is performed either at the dictionary level or by algorithms based on similarity between lemmas associated with the annotations.  In this paper, we will benchmark the classification approach of using the extracted concepts with respect to the one of using the full text.\n\n\nModel Architecture\n\nThe overall architecture of the attention-based CNN is shown in Fig. 1. It consists of several components: a word embedding look-up layer obtained using word2vec (Mikolov et al., 2013), a CNN-based n-gram encoder, an n-gram level attention layer and several fully-connected layers. By means of word embeddings, each word is represented as a real-valued vector. The word embedding look-up layer is a word embedding table T \u2208 R n\u00d7k , where n is the total vocabulary size and k is the embedding dimension. The parame- ters of the embedding table were fine-tuned during the training phase.\n\n\nN-Gram Encoder\n\nWe used a 2D convolution layer (Kim, 2014) to encode the word sequence into n-gram representations, thus capturing contextual information. For a given document, a 2D convolution filter w \u2208 R m\u00d7k was applied to a window of m words to produce a new feature. A feature c i was generated from a window of words x i:i+m\u22121 by\nc i = Relu(w \u00b7 x i:i+m\u22121 + b).(1)\nThis filter was applied to each possible window of words in the sentence x 1:m , x 2:m+1 , .....x n\u2212m+1:n to produce a feature map:\nc = [c 1 , c 2 , ......, c n\u2212m+1 ],(2)\nwith c \u2208 R n\u2212m+1 . By applying multiple filters (denoted f ) on x i:i+m\u22121 , we obtained a new representation of the document. By setting different values for m, we obtained different n-gram representations of the documents. This operation was useful in our application setting because these layers create local region embeddings by n-grams. Moreover, this allowed us to compute the attention factors for a combination of several symptoms. This in turn enabled us to detect pairs and even triplets of symptoms that are harmless if they appear individually, yet become red flags when they appear together. For example, the individual symptoms pain in arm and sudden nausea are no cause for concern. However, if a patient experiences both, this might indicate an impending heart attack.\n\n\nN-Gram Level Attention Layer\n\nFor each n-gram representation, we wanted to derive a corresponding fully-connected represen-tation for the document. As different n-grams are of different importance to the document, we introduced an attention mechanism to extract ngrams that are relevant to the meaning of the document and aggregated the representation of those informative n-grams to form a document vector. The relevant n-grams then became candidates for warning symptoms. More specifically, the attention mechanism was defined such that:\nu it = tanh(W w v it + b w ),(3)\nwhere v it refers to the tth row of ith-gram representation. That is, we first fed the n-gram annotations v it through a one-layer neural network to obtain u it as a hidden representation of v it . Then we measured the importance of the word as the similarity of u it with a word-level context vector u w and obtained a normalized importance weight \u03b1 it through a softmax function:\n\u03b1 it = exp(u T it u w ) t exp(u T it u w ) .(4)\nThe context vector u w can be regarded as a highlevel representation of a fixed query \"what is the most informative word?\" used in memory networks (Sukhbaatar et al., 2015;Kumar et al., 2016). Context vector u w was randomly initialized and jointly learned during the training process. Thereafter, we computed the document vector s i as a weighted sum of the n-gram annotations based on the weights:\ns i = t \u03b1 it v it .(5)\nFinally, all n-gram document level representations were flattened into a one-dimensional vector (flat connection layer in Fig. 1) plus patient gender and age (a + 1 in Fig. 1). This vector was then fed into a multilayer perceptron (MLP) for classification.\n\n\nWarning Symptom Detection\n\nWarning symptoms, or red flags, indicate the need for urgent medical care. The ACNN model is able to distinguish the importance of each symptom in the final classification. Thereafter, we calculated the attention score for each symptom as follows:\nscore(s j ) = c i \u2208C \u03a6(c i , s j )f (c i , s j ) occur(s j ) , (6) f (c i , s j ) = att(s j ) max s k \u2208c i att(s k ) ,(7)\nwhere \u03a6(c i , s j ) is equal to 1 if symptom s j is contained in case record c i and zero elsewhere; C is the set of urgent care cases in the data; occur(s i ) is the total occurrences of symptom s i ; att(s k ) are the attention weights returned by the ACNN. The attention weights gave us a measurement of the warning level of the symptoms. This procedure was applied for all classes to detect the most important symptoms that drive the model's prediction. As expected for the other classes, the model assigns high attention weights to non-warning symptoms.\n\n\nResults\n\n\nPatient Risk Assessment Experiment\n\n\nTraining Details\n\nWe conducted a detailed evaluation of this model on both the original full-text dataset and a dataset of a few selected medical entities (see Section 3.1.1 for details) denoted for simplicity as a symptoms dataset. The machine learning framework where all the neural network models have been implemented was based on TensorFlow and Keras. The vocabulary size, average document size and maximum document length are 134,000, 62.9 and 959 words for the full-text dataset; and 20,000, 14.15 and 94 for the symptoms dataset. We used 90% of the data for training, 5% for validation, and 5% for test randomly sampled. Both datasets were preprocessed by removing stop words and low-occurrence words and zero-padding the documents. We learned 200dimensional word embeddings on our datasets with word2vec over 25 iterations. The embeddings were different for each dataset.\n\nWe tuned our parameters on a 30,000 validation set and report the result on another 30,000 test set. For model-specific parameters, we used grid search to find the optimal values. We used a cross-entropy loss function with 256-mini-batch updating and Adam optimizer for five epochs. The learning rate was between 0.001 and 0.003; regularization was performed by weight decay of 0.0001 and a dropout of 0.8 was applied to every MLP layer. The attention vector size was set up to 100, and the window size was set from 1 to 5. For each n-gram extraction, we used up to 128 filters for 2D convolution.\n\n\nModel Comparison\n\nIn this section, we compare our system to the following approaches:\n\nCLSTM (Zhou et al., 2015) applies a CNN model on text and feeds consecutive window features directly to a LSTM model. Kim CNN (Kim, 2014) uses 2D convolution windows to extract an n-gram representation followed by max-pooling.\n\nBiGRU Attention Network (Yang et al., 2016) consists of RNNs applied on both word and sentence level to extract a hidden state. An attention mechanism is applied after the bidirectional gated recurrent units.\n\nThe results on the datasets with the full text and the symptoms only are shown in Tables 3 and 4, respectively. All the analyzed models show similar performance in the classification task. For all models, the performance decreases as we move from the full text dataset to the symptoms dataset because the medical and contextual information also diminishes by taking into account only the extracted symptom concepts.\n\n\nResult Analysis\n\nIn this section, we compare our ACNN model with the state-of-the-art deep learning models to obtain a benchmark on our triage use case. We also describe how our approach, a combination of convolutional neural networks and attention mechanisms, equals the performance of existing models with the advantage of being explainable.\n\nKim CNN uses 2D convolution windows to extract n-gram representations. Max pooling was then applied to each of the filter outputs. A single value was retained for each feature map. This might work well for short sentences containing only a few \"leading\" words indicating the cate-    Table 3 but on symptoms dataset, where s 1 , s 2 , s 3 are urgent care, general practice and telecare cases, respectively. gory. For longer documents, however, all information about n-grams is lost apart from the strongest signal. The presence of highly important symptoms in clinical data is the reason why this model performs well especially for urgent care and telecare classes. This hypothesis is supported by the number of symptoms with large attention scores found in the ACNN model for these classes.\nModel P(f 1 ) R(f 1 ) F(f 1 ) P(f 2 ) R(f 2 ) F(f 2 ) P(f 3 ) R(f 3 ) F(f 3 ) KIM\nThe BiGRU Attention Network applies an attention layer after bidirectional GRU components. For a given word in a sentence, it encodes information about the word context in that sentence. However, compared to a 2D convolution window, only a single context window is used. It is not trivial to choose the optimal window size. Thus, it is difficult to detect warning symptom pairs or triplets. For 2D convolution in our model, identifying such pairs or triplets would be more straightforward because attention factors are also learned for 2 and 3grams. Another limitation of GRU models is that they rely on fully sequential data. In our use case, however, the data is composed of several separate phrases, words or incomplete sentences.\n\nOur ACNN combines the merits of 2D convolution and attention mechanisms by stacking 2D convolution layers to extract contextual information and an attention mechanism to assign importance factors to different symptoms and combinations thereof.\n\n\nWarning Symptom Detection\n\nOwing to the lack of ground truth, we used the following evaluation method to detect warning symptoms with the ACNN. First, we measured the recall of the ACNN on urgent care cases containing only symptom concepts. Then, a new dataset was created by removing from each case record the 1-gram with the highest attention score, calculated as described in Section 3.3. For urgent cases, we expected the removed 1-grams to be highly important signals of medical urgency, hence warning symptoms. For instance, starke Brustschmerzen would be removed from the case described in Section 3.1.1. We then compared the ACNN recall for the urgent cases on the new dataset (Attention Drop) with respect to the recall on the original symptoms dataset (Baseline). This procedure is performed on all the classes for validation. The decrease in recall demonstrates the importance of the detected warning symptoms in order to classify urgent cases correctly. To verify that the detected warning symptoms are indeed highly informative, we furthermore generated datasets in which either random symptoms (Random Drop) or symptoms that appear most frequently in urgent cases (Frequency Drop) are dropped.\n\nAs shown in Table 5, dropping the attentiondetected warning symptoms led to the largest decrease in performance. The difference became even more distinct if two symptoms instead of one were removed from the cases.\n\nPerformance also decreased for the urgent care and general practice classes, whereas almost a flat behavior was found for telecare class, as expected. In the latter case, random, frequency, attention drops showed the same results because several features had the same attention scores. Manual inspection of the symptoms with the highest attention scores further supports these results. The darker the color of the symptom in Figure 2, the higher its attention factor in the model. In the examined samples, darker colors did indeed correlate with symptoms that made patients require urgent care, such as vomiting blood and electric shock.\n\nWith single or double removals for the full-text dataset, a much lower decrease in performance was observed because of the higher number of features per case.\n\n\nExplainable Deep Learning\n\nIn current research, but especially in medical industry applications, transparent or explainable machine learning models are becoming increasingly important. Some machine learning models have become so complex, they are black boxes. End users need to understand why a certain recommendation was made.\n\nIn our application, the attention mechanism on which we based our warning (and nonwarning) symptom detection represents a transparent method of reasoning why a given case belongs to a certain class.\n\nFor instance, by analyzing the patient symptoms with the highest attention scores, it becomes apparent why a case would be predicted to be urgent, general practice or telecare. Table 6 shows some examples with high/low attention scores computed using 1-gram attention values for urgent care, general practice and telecare classes. As can be seen, the symptoms with the highest score in the urgent cases are the most severe, whereas the symptoms in the telecare cases are less severe. In other words, symptoms with a high/low score for a given class are the most/least relevant ones for that class. As expected, if the model predicts an urgent (non-urgent) class, the model assigns a higher weight to warning (non-warning) symptoms. The computation of 1-gram feature scores results in 2,000 (3,600), 734 (3,700), 1,500 (3,800) features with scores of > 0.8 (< 0.2) for s 1 , s 2 and s 3 , respectively. The use of an attention layer on n-gram representations allowed us to compute feature relevance including correlations between pairs, triplets, etc. An example of scores of feature pairs obtained by extracting the attention weights for the 2-grams is shown in Tables 7 and 8. Strong correlation between feature pairs is found for the cases where the score of the pair is much higher than those of the single features. The computation of 2-gram feature scores results in 12,000 (28,000), 4,800 (13,000), 10,000 (24,000) features with scores of > 0.8 (< 0.2) for s 1 , s 2 and s 3 , respectively.\n\n\nConfidence\n\nTo reach higher performance in an operative triage application, we define a confidence score in the classification based on which the system decides whether to trust the recommendation. In Table 9 and Table 10 we show the same results obtained  in Tables 3 and 4, respectively, discarding all test cases in which the predicted probability of the classifier was lower than 0.6. With the chosen threshold, we discarded roughly 30% cases. Overall a performance improvement of between 5% and 10% is observed. In future work, we plan to apply additional techniques, e.g., based on hierarchical decision trees, to minimize medical risk even further.\n\n\nConclusion\n\nWe have described an attention-based CNN model to assess patient risk and to detect warning symptoms, which will be used in an industrial application for medical triage. We achieved a precision of 79% on the full-text dataset and Dataset P(s 1 ) R(s 1 ) F(s 1 ) P(s 2 ) R(s 2 ) F(s 2 ) P(s 3 ) R(s 3 ) F(s 3 )   (f i , f j ) score of f i score of f j score of (f i , f j )  66% on the symptoms set. On a confidence threshold of 0.6, precision increases to 85% and 75%, respectively. The learned attention weights allowed us to compute the symptom relevance, i.e., the attention score, which is then used to extract warning symptoms more precisely and to make the recommendation rationale transparent.\n\n(f i , f j ) score of f i score of f j score of (f i , f j )     Table 4 applying a threshold to the probabilities of 0.6.\n\nFigure 1 :\n1Model Architecture.\n\nFigure 2 :\n2Visualization of attention factors from neural network used to explain recommendation rationale. Each line represents the (translated) symptoms extracted for a patient case file. The darker the color, the higher the attention factor for a symptom.\n\nTable 1 :\n1Ground truth distribution for the reduced \nclasses. Urgent Care = Patient needs to seek medi-\ncal care within a short time period; General Practice = \nPatient requires medical attention in a physical consul-\ntation, but not urgently; Telecare = In-person medical \nappointment not required, instructions over the phone \nare sufficient. \n\n\n\nTable 2\n2lists the concepts extracted from an original case record after preprocessing by the described NLP pipeline.key \nvalue \nGender Male \n\nClass \nUrgent Care \n\nContent \"Seit heute beschwert sich \nder Patient\u00fcber heftige \nBrustschmerzen; Fieber 37,4\u00b0C; \nSchwierigkeiten beim Atmen, \nleichte Kopfschmerzen.\" \n\nEntities starke Brustschmerzen \nFieber 36-38\u00b0C \nAtembeschwerde \nleichte Kopfschmerzen \n\n\n\nTable 2 :\n2(key,value)-pairs of an original patient case file and extracted entities.\n\nTable 3 :\n3Prediction results in % for the different architectures on full text, where P(f k ), R(f k ), F(f k ) are precision, \nrecall and f-score divided by class, and where f 1 , f 2 , f 3 are urgent care, general practice and telecare, respectively. \nSimilar values were obtained by conducting several experiments and averaging the results. \n\nModel \nP(s 1 ) R(s 1 ) F(s 1 ) P(s 2 ) R(s 2 ) F(s 2 ) P(s 3 ) R(s 3 ) F(s 3 ) \n\nKIM CNN \n70.5 \n73.6 \n71.1 55.2 \n40.6 \n51.5 66.5 \n70.6 \n67.3 \nCLSTM \n70.0 \n71.6 \n70.3 53.8 \n40.1 \n50.4 65.4 \n70.8 \n66.4 \nBiGRU Attention Net 69.2 \n72.5 \n69.9 53.0 \n43.1 \n50.7 66.7 \n68.4 \n67.0 \nACNN \n72.5 \n68.2 \n71.6 51.9 \n47.8 \n51.0 65.5 \n72.0 \n66.5 \n\n\n\nTable 4 :\n4Same as\n\nTable 5 :\n5Different datasets and the model's precision, recall and f-score in %, where s 1 , s 2 , s 3 are urgent care, general practice and telecare symptoms dataset, respectively.s1 \nscore s2 \nscore \ns3 \nscore \n\nshortness of breath \n1.0 \nintermittent shoulder pain 1.0 \nback distortion \n1.0 \n(Atemnot) \n(intermittierende Schulterschmerzen) (R\u00fcckenzerrung) \npain after accident \n1.0 \nseverely itchy wound \n1.0 \nabrasion on the back \n1.0 \n(Schmerzen nach Unfall) \n(stark juckende Wunde) \n(Sch\u00fcrfung am R\u00fccken) \nforeign body in esophagus 1.0 \npurulent nasal discharge \n1.0 \ntoenail injury \n1.0 \n(Fremdk\u00f6rper im\u00d6sophagus) \n(eitriger Nasenausfluss) \n(Zehennagelverletzung) \nsevere rectal bleed \n1.0 \nneck abscess \n1.0 \nitching forehead \n1.0 \n(blutet stark rektal) \n(Abszess am Nacken) \n(Juckreiz an der Stirn) \nitching back \n0.05 \nno pain when walking \n0.03 \nstabbed with knife \n0.03 \n(Juckreiz am R\u00fccken) \n(beim Laufen keine Schmerzen) \n(Messerstich) \npain in thumb \n0.04 \nthroat is normal \n0.01 \near bleeding \n0.02 \n(Daumenschmerz) \n(Hals normal) \n(Ohrenblutung) \nnail injury \n0.03 \nblister on tongue \n0.01 \nhardened lower abdomen 0.01 \n(Nagelverletzung) \n(Blase auf der Zunge) \n(verh\u00e4rteter Unterbauch) \nwart on foot \n0.01 \ncan drink normally \n0.003 \ndifficulty breathing \n0.005 \n(Warze am Fuss) \n(kann normal trinken) \n(Schwierigkeiten beim Atmen) \n\n\n\nTable 6 :\n6Symptoms (translated from German into English) scores divided by class using 1-gram attention values (only a few examples with high/low scores shown here). The corresponding German terms are given in parentheses.\n\n\nBlutdruck stark erh\u00f6ht, hypertensive Krise)(acute abdominal pain, severe abdominal pain) 0.86 \n0.39 \n1.0 \n(akute Bauchschmerzen, starke Bauchschmerzen) \n(loss of consciousness, head injury) \n0.32 \n0.55 \n1.0 \n(Bewusstseinsverlust, Sch\u00e4delverletzungen) \n(epigastric pain, colic) \n0.35 \n0.26 \n1.0 \n(Oberbauchschmerzen, Kolik) \n(pneumonia, respiratory tract inflammation) \n0.45 \n0.24 \n0.92 \n(Pneumonie, Atemwegentz\u00fcndung) \n(severe vomiting, dehydration) \n0.44 \n0.56 \n0.87 \n(starkes Erbrechen, Dehydration) \n(very high blood pressure, hypertensive crisis) 0.49 \n0.78 \n0.86 \n(\n\nTable 7 :\n7Symptoms (translated from German into English) scores using 2-gram attention values (only a few high scores shown here) for s 1 . The corresponding German terms are given in parentheses.\n\nTable 8 :\n8Symptoms (translated from German into English) scores divided by class using 2-gram attention values (only a few high scores shown here) for s 2 (upper), s 3 (lower) panel. The corresponding German terms are given in parentheses.Model \nP(f 1 ) R(f 1 ) F(f 1 ) P(f 2 ) R(f 2 ) F(f 2 ) P(f 3 ) R(f 3 ) F(f 3 ) \n\nKIM CNN \n87.8 \n86.3 \n87.5 73.0 \n78.6 \n74.0 90.6 \n90.0 \n90.4 \nCLSTM \n84.4 \n88.4 \n85.2 76.2 \n66.5 \n74.0 88.4 \n87.6 \n88.3 \nBiGRU Attention Net 76.5 \n82.2 \n77.6 65.1 \n60.7 \n64.2 82.5 \n78.2 \n81.6 \nACNN \n85.3 \n87.3 \n85.6 77.3 \n65.1 \n74.5 87.1 \n89.5 \n87.6 \n\n\n\nTable 9 :\n9Same as Table 3applying a threshold to the probabilities of 0.6.Model \nP(s 1 ) R(s 1 ) F(s 1 ) P(s 2 ) R(s 2 ) F(s 2 ) P(s 3 ) R(s 3 ) F(s 3 ) \n\nKIM CNN \n75.6 \n86.0 \n77.5 65.0 \n45.5 \n59.8 77.5 \n72.0 \n76.3 \nCLSTM \n76.5 \n83.2 \n77.8 66.8 \n44.5 \n60.7 75.2 \n75.2 \n75.2 \nBiGRU Attention Net 73.2 \n76.8 \n73.4 58.4 \n45.1 \n55.1 70.5 \n72.5 \n70.9 \nACNN \n77.0 \n81.5 \n77.9 62.8 \n60.0 \n60.3 75.7 \n74.9 \n75.5 \n\n\n\nTable 10 :\n10Same as\nAcknowledgements. We deeply acknowledge D. Dykeman, D. Ortiz-Yepes and K. Thandiackal.\n. Ada , Ada. 2018. https://ada.com.\n\nAlexis Conneau, Holger Schwenk, Lo\u00efc Barrault, Yann Lecun, arXiv:1606.01781Very deep convolutional networks for natural language processing. arXiv preprintAlexis Conneau, Holger Schwenk, Lo\u00efc Barrault, and Yann Lecun. 2016. Very deep convolutional net- works for natural language processing. arXiv preprint arXiv:1606.01781.\n\nA convolutional attention model for text classification. Jiachen Du, Lin Gui, Ruifeng Xu, Yulan He, National CCF Conference on Natural Language Processing and Chinese Computing. SpringerJiachen Du, Lin Gui, Ruifeng Xu, and Yulan He. 2017. A convolutional attention model for text classifica- tion. In National CCF Conference on Natural Lan- guage Processing and Chinese Computing, pages 183-195. Springer.\n\nText categorization with support vector machines: Learning with many relevant features. Thorsten Joachims, Machine learning: ECML-98. Thorsten Joachims. 1998. Text categorization with support vector machines: Learning with many rel- evant features. Machine learning: ECML-98, pages 137-142.\n\nSemi-supervised convolutional neural networks for text categorization via region embedding. Rie Johnson, Tong Zhang, Advances in neural information processing systems. Rie Johnson and Tong Zhang. 2015. Semi-supervised convolutional neural networks for text categoriza- tion via region embedding. In Advances in neural information processing systems, pages 919-927.\n\nConvolutional neural networks for sentence classification. Yoon Kim, arXiv:1408.5882arXiv preprintYoon Kim. 2014. Convolutional neural net- works for sentence classification. arXiv preprint arXiv:1408.5882.\n\nAsk me anything: Dynamic memory networks for natural language processing. Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher, International Conference on Machine Learning. Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. 2016. Ask me anything: Dynamic memory networks for natural language processing. In International Con- ference on Machine Learning, pages 1378-1387.\n\nRecurrent convolutional neural networks for text classification. Siwei Lai, Liheng Xu, Kang Liu, Jun Zhao, AAAI. 333Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015. Recurrent convolutional neural networks for text classification. In AAAI, volume 333, pages 2267- 2273.\n\nChristy Li, Dimitris Konomis, Graham Neubig, Pengtao Xie, Carol Cheng, Eric Xing, arXiv:1712.02768Convolutional neural networks for medical diagnosis from admission notes. arXiv preprintChristy Li, Dimitris Konomis, Graham Neubig, Peng- tao Xie, Carol Cheng, and Eric Xing. 2017. Convo- lutional neural networks for medical diagnosis from admission notes. arXiv preprint arXiv:1712.02768.\n\nDistributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean, Advances in neural information processing systems. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor- rado, and Jeffrey Dean. 2013. Distributed represen- tations of words and phrases and their composition- ality. In Advances in neural information processing systems, pages 3111-3119.\n\nApplying deep learning to ICD-9 Multi-label Classification from Medical Records. Priyanka Nigam, Stanford UniversityTechnical reportPriyanka Nigam. 2016. Applying deep learning to ICD-9 Multi-label Classification from Medical Records. Technical report, Stanford University.\n\nEvaluation of symptom checkers for self diagnosis and triage: audit study. Jeffrey A Hannah L Semigran, Courtney Linder, Ateev Gidengil, Mehrotra, bmj. 3513480Hannah L Semigran, Jeffrey A Linder, Courtney Gi- dengil, and Ateev Mehrotra. 2015. Evaluation of symptom checkers for self diagnosis and triage: au- dit study. bmj, 351:h3480.\n\nAttentionbased convolutional neural network for semantic relation extraction. Yatian Shen, Xuanjing Huang, COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers. Osaka, JapanYatian Shen and Xuanjing Huang. 2016. Attention- based convolutional neural network for semantic re- lation extraction. In COLING 2016, 26th Inter- national Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16, 2016, Osaka, Japan, pages 2526- 2536.\n\nEnd-to-end memory networks. Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, Advances in neural information processing systems. Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. 2015. End-to-end memory networks. In Advances in neural information processing systems, pages 2440-2448.\n\n. Symptomate, Symptomate. 2018. https://symptomate.com.\n\nImproved semantic representations from tree-structured long short-term memory networks. Kai Sheng Tai, Richard Socher, Christopher D Manning, arXiv:1503.00075arXiv preprintKai Sheng Tai, Richard Socher, and Christopher D Manning. 2015. Improved semantic representations from tree-structured long short-term memory net- works. arXiv preprint arXiv:1503.00075.\n\nDocument modeling with gated recurrent neural network for sentiment classification. Duyu Tang, Bing Qin, Ting Liu, EMNLP. Duyu Tang, Bing Qin, and Ting Liu. 2015. Document modeling with gated recurrent neural network for sentiment classification. In EMNLP, pages 1422- 1432.\n\nBaselines and bigrams: Simple, good sentiment and topic classification. Sida Wang, D Christopher, Manning, Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers. the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers2Association for Computational LinguisticsSida Wang and Christopher D Manning. 2012. Base- lines and bigrams: Simple, good sentiment and topic classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Lin- guistics: Short Papers-Volume 2, pages 90-94. As- sociation for Computational Linguistics.\n\nHierarchical attention networks for document classification. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, J Alexander, Eduard H Smola, Hovy, HLT-NAACL. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J Smola, and Eduard H Hovy. 2016. Hi- erarchical attention networks for document classifi- cation. In HLT-NAACL, pages 1480-1489.\n\nChunting Zhou, Chonglin Sun, Zhiyuan Liu, Francis Lau, arXiv:1511.08630A C-LSTM neural network for text classification. arXiv preprintChunting Zhou, Chonglin Sun, Zhiyuan Liu, and Fran- cis Lau. 2015. A C-LSTM neural network for text classification. arXiv preprint arXiv:1511.08630.\n", "annotations": {"author": "[{\"end\":146,\"start\":100},{\"end\":215,\"start\":147},{\"end\":263,\"start\":216},{\"end\":353,\"start\":264},{\"end\":400,\"start\":354},{\"end\":468,\"start\":401},{\"end\":519,\"start\":469},{\"end\":570,\"start\":520}]", "publisher": null, "author_last_name": "[{\"end\":112,\"start\":105},{\"end\":157,\"start\":155},{\"end\":229,\"start\":223},{\"end\":280,\"start\":269},{\"end\":366,\"start\":359},{\"end\":412,\"start\":408},{\"end\":485,\"start\":476},{\"end\":528,\"start\":523}]", "author_first_name": "[{\"end\":104,\"start\":100},{\"end\":154,\"start\":147},{\"end\":222,\"start\":216},{\"end\":268,\"start\":264},{\"end\":358,\"start\":354},{\"end\":407,\"start\":401},{\"end\":475,\"start\":469},{\"end\":522,\"start\":520}]", "author_affiliation": "[{\"end\":145,\"start\":114},{\"end\":190,\"start\":159},{\"end\":214,\"start\":192},{\"end\":262,\"start\":231},{\"end\":328,\"start\":297},{\"end\":352,\"start\":330},{\"end\":399,\"start\":368},{\"end\":467,\"start\":436},{\"end\":518,\"start\":487},{\"end\":569,\"start\":547}]", "title": "[{\"end\":97,\"start\":1},{\"end\":667,\"start\":571}]", "venue": null, "abstract": "[{\"end\":1609,\"start\":669}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1852,\"start\":1829},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3177,\"start\":3158},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3692,\"start\":3681},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3710,\"start\":3692},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3726,\"start\":3710},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4886,\"start\":4862},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4901,\"start\":4886},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5123,\"start\":5104},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5162,\"start\":5144},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5328,\"start\":5318},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5460,\"start\":5436},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5595,\"start\":5574},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5859,\"start\":5840},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5876,\"start\":5859},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6149,\"start\":6130},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6182,\"start\":6166},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6380,\"start\":6359},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6846,\"start\":6823},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7085,\"start\":7069},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7196,\"start\":7184},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11660,\"start\":11638},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12122,\"start\":12111},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14566,\"start\":14541},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14585,\"start\":14566},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17674,\"start\":17656},{\"end\":17787,\"start\":17768},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17921,\"start\":17902}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":26485,\"start\":26453},{\"attributes\":{\"id\":\"fig_1\"},\"end\":26746,\"start\":26486},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":27096,\"start\":26747},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":27498,\"start\":27097},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":27585,\"start\":27499},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":28266,\"start\":27586},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":28286,\"start\":28267},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":29640,\"start\":28287},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":29865,\"start\":29641},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":30438,\"start\":29866},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":30637,\"start\":30439},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":31211,\"start\":30638},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":31620,\"start\":31212},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":31642,\"start\":31621}]", "paragraph": "[{\"end\":2626,\"start\":1625},{\"end\":3398,\"start\":2628},{\"end\":3838,\"start\":3400},{\"end\":4628,\"start\":3840},{\"end\":5230,\"start\":4630},{\"end\":5877,\"start\":5232},{\"end\":6627,\"start\":5879},{\"end\":7299,\"start\":6658},{\"end\":7802,\"start\":7333},{\"end\":8143,\"start\":7804},{\"end\":8746,\"start\":8145},{\"end\":9045,\"start\":8763},{\"end\":11453,\"start\":9047},{\"end\":12061,\"start\":11476},{\"end\":12399,\"start\":12080},{\"end\":12565,\"start\":12434},{\"end\":13388,\"start\":12605},{\"end\":13930,\"start\":13421},{\"end\":14345,\"start\":13964},{\"end\":14793,\"start\":14394},{\"end\":15073,\"start\":14817},{\"end\":15350,\"start\":15103},{\"end\":16031,\"start\":15473},{\"end\":16961,\"start\":16099},{\"end\":17560,\"start\":16963},{\"end\":17648,\"start\":17581},{\"end\":17876,\"start\":17650},{\"end\":18086,\"start\":17878},{\"end\":18503,\"start\":18088},{\"end\":18849,\"start\":18523},{\"end\":19642,\"start\":18851},{\"end\":20458,\"start\":19725},{\"end\":20703,\"start\":20460},{\"end\":21913,\"start\":20733},{\"end\":22128,\"start\":21915},{\"end\":22767,\"start\":22130},{\"end\":22927,\"start\":22769},{\"end\":23257,\"start\":22957},{\"end\":23457,\"start\":23259},{\"end\":24955,\"start\":23459},{\"end\":25613,\"start\":24970},{\"end\":26328,\"start\":25628},{\"end\":26452,\"start\":26330}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12433,\"start\":12400},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12604,\"start\":12566},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13963,\"start\":13931},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14393,\"start\":14346},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14816,\"start\":14794},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15472,\"start\":15351},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19724,\"start\":19643}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":8743,\"start\":8736},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":18184,\"start\":18170},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":19142,\"start\":19135},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":21934,\"start\":21927},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":23643,\"start\":23636},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":25166,\"start\":25159},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25232,\"start\":25171},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":26402,\"start\":26395}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1623,\"start\":1611},{\"attributes\":{\"n\":\"2.2\"},\"end\":6656,\"start\":6630},{\"attributes\":{\"n\":\"3\"},\"end\":7313,\"start\":7302},{\"attributes\":{\"n\":\"3.1\"},\"end\":7331,\"start\":7316},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":8761,\"start\":8749},{\"attributes\":{\"n\":\"3.2\"},\"end\":11474,\"start\":11456},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":12078,\"start\":12064},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":13419,\"start\":13391},{\"attributes\":{\"n\":\"3.3\"},\"end\":15101,\"start\":15076},{\"attributes\":{\"n\":\"4\"},\"end\":16041,\"start\":16034},{\"attributes\":{\"n\":\"4.1\"},\"end\":16078,\"start\":16044},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":16097,\"start\":16081},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":17579,\"start\":17563},{\"attributes\":{\"n\":\"4.1.3\"},\"end\":18521,\"start\":18506},{\"attributes\":{\"n\":\"4.2\"},\"end\":20731,\"start\":20706},{\"attributes\":{\"n\":\"4.3\"},\"end\":22955,\"start\":22930},{\"attributes\":{\"n\":\"4.4\"},\"end\":24968,\"start\":24958},{\"attributes\":{\"n\":\"5\"},\"end\":25626,\"start\":25616},{\"end\":26464,\"start\":26454},{\"end\":26497,\"start\":26487},{\"end\":26757,\"start\":26748},{\"end\":27105,\"start\":27098},{\"end\":27509,\"start\":27500},{\"end\":27596,\"start\":27587},{\"end\":28277,\"start\":28268},{\"end\":28297,\"start\":28288},{\"end\":29651,\"start\":29642},{\"end\":30449,\"start\":30440},{\"end\":30648,\"start\":30639},{\"end\":31222,\"start\":31213},{\"end\":31632,\"start\":31622}]", "table": "[{\"end\":27096,\"start\":26759},{\"end\":27498,\"start\":27215},{\"end\":28266,\"start\":27598},{\"end\":29640,\"start\":28470},{\"end\":30438,\"start\":29911},{\"end\":31211,\"start\":30879},{\"end\":31620,\"start\":31288}]", "figure_caption": "[{\"end\":26485,\"start\":26466},{\"end\":26746,\"start\":26499},{\"end\":27215,\"start\":27107},{\"end\":27585,\"start\":27511},{\"end\":28286,\"start\":28279},{\"end\":28470,\"start\":28299},{\"end\":29865,\"start\":29653},{\"end\":29911,\"start\":29868},{\"end\":30637,\"start\":30451},{\"end\":30879,\"start\":30650},{\"end\":31288,\"start\":31224},{\"end\":31642,\"start\":31635}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11546,\"start\":11540},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14945,\"start\":14939},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14991,\"start\":14985},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22563,\"start\":22555}]", "bib_author_first_name": "[{\"end\":31735,\"start\":31732},{\"end\":31773,\"start\":31767},{\"end\":31789,\"start\":31783},{\"end\":31803,\"start\":31799},{\"end\":31818,\"start\":31814},{\"end\":32157,\"start\":32150},{\"end\":32165,\"start\":32162},{\"end\":32178,\"start\":32171},{\"end\":32188,\"start\":32183},{\"end\":32596,\"start\":32588},{\"end\":32887,\"start\":32884},{\"end\":32901,\"start\":32897},{\"end\":33221,\"start\":33217},{\"end\":33445,\"start\":33440},{\"end\":33457,\"start\":33453},{\"end\":33470,\"start\":33465},{\"end\":33486,\"start\":33481},{\"end\":33499,\"start\":33494},{\"end\":33516,\"start\":33510},{\"end\":33534,\"start\":33528},{\"end\":33548,\"start\":33542},{\"end\":33564,\"start\":33557},{\"end\":33975,\"start\":33970},{\"end\":33987,\"start\":33981},{\"end\":33996,\"start\":33992},{\"end\":34005,\"start\":34002},{\"end\":34185,\"start\":34178},{\"end\":34198,\"start\":34190},{\"end\":34214,\"start\":34208},{\"end\":34230,\"start\":34223},{\"end\":34241,\"start\":34236},{\"end\":34253,\"start\":34249},{\"end\":34650,\"start\":34645},{\"end\":34664,\"start\":34660},{\"end\":34679,\"start\":34676},{\"end\":34690,\"start\":34686},{\"end\":34707,\"start\":34700},{\"end\":35088,\"start\":35080},{\"end\":35356,\"start\":35349},{\"end\":35358,\"start\":35357},{\"end\":35386,\"start\":35378},{\"end\":35400,\"start\":35395},{\"end\":35695,\"start\":35689},{\"end\":35710,\"start\":35702},{\"end\":36191,\"start\":36182},{\"end\":36209,\"start\":36204},{\"end\":36221,\"start\":36218},{\"end\":36596,\"start\":36587},{\"end\":36609,\"start\":36602},{\"end\":36631,\"start\":36618},{\"end\":36947,\"start\":36943},{\"end\":36958,\"start\":36954},{\"end\":36968,\"start\":36964},{\"end\":37211,\"start\":37207},{\"end\":37219,\"start\":37218},{\"end\":37833,\"start\":37827},{\"end\":37844,\"start\":37840},{\"end\":37856,\"start\":37851},{\"end\":37871,\"start\":37863},{\"end\":37877,\"start\":37876},{\"end\":37895,\"start\":37889},{\"end\":37897,\"start\":37896},{\"end\":38120,\"start\":38112},{\"end\":38135,\"start\":38127},{\"end\":38148,\"start\":38141},{\"end\":38161,\"start\":38154}]", "bib_author_last_name": "[{\"end\":31781,\"start\":31774},{\"end\":31797,\"start\":31790},{\"end\":31812,\"start\":31804},{\"end\":31824,\"start\":31819},{\"end\":32160,\"start\":32158},{\"end\":32169,\"start\":32166},{\"end\":32181,\"start\":32179},{\"end\":32191,\"start\":32189},{\"end\":32605,\"start\":32597},{\"end\":32895,\"start\":32888},{\"end\":32907,\"start\":32902},{\"end\":33225,\"start\":33222},{\"end\":33451,\"start\":33446},{\"end\":33463,\"start\":33458},{\"end\":33479,\"start\":33471},{\"end\":33492,\"start\":33487},{\"end\":33508,\"start\":33500},{\"end\":33526,\"start\":33517},{\"end\":33540,\"start\":33535},{\"end\":33555,\"start\":33549},{\"end\":33571,\"start\":33565},{\"end\":33979,\"start\":33976},{\"end\":33990,\"start\":33988},{\"end\":34000,\"start\":33997},{\"end\":34010,\"start\":34006},{\"end\":34188,\"start\":34186},{\"end\":34206,\"start\":34199},{\"end\":34221,\"start\":34215},{\"end\":34234,\"start\":34231},{\"end\":34247,\"start\":34242},{\"end\":34258,\"start\":34254},{\"end\":34658,\"start\":34651},{\"end\":34674,\"start\":34665},{\"end\":34684,\"start\":34680},{\"end\":34698,\"start\":34691},{\"end\":34712,\"start\":34708},{\"end\":35094,\"start\":35089},{\"end\":35376,\"start\":35359},{\"end\":35393,\"start\":35387},{\"end\":35409,\"start\":35401},{\"end\":35419,\"start\":35411},{\"end\":35700,\"start\":35696},{\"end\":35716,\"start\":35711},{\"end\":36202,\"start\":36192},{\"end\":36216,\"start\":36210},{\"end\":36228,\"start\":36222},{\"end\":36454,\"start\":36444},{\"end\":36600,\"start\":36597},{\"end\":36616,\"start\":36610},{\"end\":36639,\"start\":36632},{\"end\":36952,\"start\":36948},{\"end\":36962,\"start\":36959},{\"end\":36972,\"start\":36969},{\"end\":37216,\"start\":37212},{\"end\":37231,\"start\":37220},{\"end\":37240,\"start\":37233},{\"end\":37838,\"start\":37834},{\"end\":37849,\"start\":37845},{\"end\":37861,\"start\":37857},{\"end\":37874,\"start\":37872},{\"end\":37887,\"start\":37878},{\"end\":37903,\"start\":37898},{\"end\":37909,\"start\":37905},{\"end\":38125,\"start\":38121},{\"end\":38139,\"start\":38136},{\"end\":38152,\"start\":38149},{\"end\":38165,\"start\":38162}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":31765,\"start\":31730},{\"attributes\":{\"doi\":\"arXiv:1606.01781\",\"id\":\"b1\"},\"end\":32091,\"start\":31767},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":46249948},\"end\":32498,\"start\":32093},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2427083},\"end\":32790,\"start\":32500},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1689250},\"end\":33156,\"start\":32792},{\"attributes\":{\"doi\":\"arXiv:1408.5882\",\"id\":\"b5\"},\"end\":33364,\"start\":33158},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2319779},\"end\":33903,\"start\":33366},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":16756501},\"end\":34176,\"start\":33905},{\"attributes\":{\"doi\":\"arXiv:1712.02768\",\"id\":\"b8\"},\"end\":34566,\"start\":34178},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":16447573},\"end\":34997,\"start\":34568},{\"attributes\":{\"id\":\"b10\"},\"end\":35272,\"start\":34999},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":13038865},\"end\":35609,\"start\":35274},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":14234178},\"end\":36152,\"start\":35611},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1399322},\"end\":36440,\"start\":36154},{\"attributes\":{\"id\":\"b14\"},\"end\":36497,\"start\":36442},{\"attributes\":{\"doi\":\"arXiv:1503.00075\",\"id\":\"b15\"},\"end\":36857,\"start\":36499},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":784094},\"end\":37133,\"start\":36859},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":217537},\"end\":37764,\"start\":37135},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6857205},\"end\":38110,\"start\":37766},{\"attributes\":{\"doi\":\"arXiv:1511.08630\",\"id\":\"b19\"},\"end\":38394,\"start\":38112}]", "bib_title": "[{\"end\":32148,\"start\":32093},{\"end\":32586,\"start\":32500},{\"end\":32882,\"start\":32792},{\"end\":33438,\"start\":33366},{\"end\":33968,\"start\":33905},{\"end\":34643,\"start\":34568},{\"end\":35347,\"start\":35274},{\"end\":35687,\"start\":35611},{\"end\":36180,\"start\":36154},{\"end\":36941,\"start\":36859},{\"end\":37205,\"start\":37135},{\"end\":37825,\"start\":37766}]", "bib_author": "[{\"end\":31738,\"start\":31732},{\"end\":31783,\"start\":31767},{\"end\":31799,\"start\":31783},{\"end\":31814,\"start\":31799},{\"end\":31826,\"start\":31814},{\"end\":32162,\"start\":32150},{\"end\":32171,\"start\":32162},{\"end\":32183,\"start\":32171},{\"end\":32193,\"start\":32183},{\"end\":32607,\"start\":32588},{\"end\":32897,\"start\":32884},{\"end\":32909,\"start\":32897},{\"end\":33227,\"start\":33217},{\"end\":33453,\"start\":33440},{\"end\":33465,\"start\":33453},{\"end\":33481,\"start\":33465},{\"end\":33494,\"start\":33481},{\"end\":33510,\"start\":33494},{\"end\":33528,\"start\":33510},{\"end\":33542,\"start\":33528},{\"end\":33557,\"start\":33542},{\"end\":33573,\"start\":33557},{\"end\":33981,\"start\":33970},{\"end\":33992,\"start\":33981},{\"end\":34002,\"start\":33992},{\"end\":34012,\"start\":34002},{\"end\":34190,\"start\":34178},{\"end\":34208,\"start\":34190},{\"end\":34223,\"start\":34208},{\"end\":34236,\"start\":34223},{\"end\":34249,\"start\":34236},{\"end\":34260,\"start\":34249},{\"end\":34660,\"start\":34645},{\"end\":34676,\"start\":34660},{\"end\":34686,\"start\":34676},{\"end\":34700,\"start\":34686},{\"end\":34714,\"start\":34700},{\"end\":35096,\"start\":35080},{\"end\":35378,\"start\":35349},{\"end\":35395,\"start\":35378},{\"end\":35411,\"start\":35395},{\"end\":35421,\"start\":35411},{\"end\":35702,\"start\":35689},{\"end\":35718,\"start\":35702},{\"end\":36204,\"start\":36182},{\"end\":36218,\"start\":36204},{\"end\":36230,\"start\":36218},{\"end\":36456,\"start\":36444},{\"end\":36602,\"start\":36587},{\"end\":36618,\"start\":36602},{\"end\":36641,\"start\":36618},{\"end\":36954,\"start\":36943},{\"end\":36964,\"start\":36954},{\"end\":36974,\"start\":36964},{\"end\":37218,\"start\":37207},{\"end\":37233,\"start\":37218},{\"end\":37242,\"start\":37233},{\"end\":37840,\"start\":37827},{\"end\":37851,\"start\":37840},{\"end\":37863,\"start\":37851},{\"end\":37876,\"start\":37863},{\"end\":37889,\"start\":37876},{\"end\":37905,\"start\":37889},{\"end\":37911,\"start\":37905},{\"end\":38127,\"start\":38112},{\"end\":38141,\"start\":38127},{\"end\":38154,\"start\":38141},{\"end\":38167,\"start\":38154}]", "bib_venue": "[{\"end\":31906,\"start\":31842},{\"end\":32269,\"start\":32193},{\"end\":32632,\"start\":32607},{\"end\":32958,\"start\":32909},{\"end\":33215,\"start\":33158},{\"end\":33617,\"start\":33573},{\"end\":34016,\"start\":34012},{\"end\":34348,\"start\":34276},{\"end\":34763,\"start\":34714},{\"end\":35078,\"start\":34999},{\"end\":35424,\"start\":35421},{\"end\":35838,\"start\":35718},{\"end\":36279,\"start\":36230},{\"end\":36585,\"start\":36499},{\"end\":36979,\"start\":36974},{\"end\":37343,\"start\":37242},{\"end\":37920,\"start\":37911},{\"end\":38230,\"start\":38183},{\"end\":35852,\"start\":35840},{\"end\":37431,\"start\":37345}]"}}}, "year": 2023, "month": 12, "day": 17}