{"id": 235658361, "updated": "2023-10-06 02:00:04.294", "metadata": {"title": "Time-Series Representation Learning via Temporal and Contextual Contrasting", "authors": "[{\"first\":\"Emadeldeen\",\"last\":\"Eldele\",\"middle\":[]},{\"first\":\"Mohamed\",\"last\":\"Ragab\",\"middle\":[]},{\"first\":\"Zhenghua\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Min\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Chee\",\"last\":\"Kwoh\",\"middle\":[\"Keong\"]},{\"first\":\"Xiaoli\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Cuntai\",\"last\":\"Guan\",\"middle\":[]}]", "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence", "journal": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence", "publication_date": {"year": 2021, "month": 6, "day": 26}, "abstract": "Learning decent representations from unlabeled time-series data with temporal dynamics is a very challenging task. In this paper, we propose an unsupervised Time-Series representation learning framework via Temporal and Contextual Contrasting (TS-TCC), to learn time-series representation from unlabeled data. First, the raw time-series data are transformed into two different yet correlated views by using weak and strong augmentations. Second, we propose a novel temporal contrasting module to learn robust temporal representations by designing a tough cross-view prediction task. Last, to further learn discriminative representations, we propose a contextual contrasting module built upon the contexts from the temporal contrasting module. It attempts to maximize the similarity among different contexts of the same sample while minimizing similarity among contexts of different samples. Experiments have been carried out on three real-world time-series datasets. The results manifest that training a linear classifier on top of the features learned by our proposed TS-TCC performs comparably with the supervised training. Additionally, our proposed TS-TCC shows high efficiency in few-labeled data and transfer learning scenarios. The code is publicly available at https://github.com/emadeldeen24/TS-TCC.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2106.14112", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ijcai/Eldele0C000G21", "doi": "10.24963/ijcai.2021/324"}}, "content": {"source": {"pdf_hash": "47e0dab08c920b589a9ddd11643c694d47ccd1c4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2106.14112v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://www.ijcai.org/proceedings/2021/0324.pdf", "status": "BRONZE"}}, "grobid": {"id": "15949f5194d5058bc2d5c0136bb4900d61be9bb0", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/47e0dab08c920b589a9ddd11643c694d47ccd1c4.txt", "contents": "\nTime-Series Representation Learning via Temporal and Contextual Contrasting\n\n\nEmadeldeen Eldele \nSchool of Computer Science and Engineering\nNanyang Technological University\nSingapore\n\nMohamed Ragab \nSchool of Computer Science and Engineering\nNanyang Technological University\nSingapore\n\nZhenghua Chen \nInstitute for Infocomm Research\nA*STARSingapore\n\nMin Wu wumin@i2r.a-star.edu.sg \nInstitute for Infocomm Research\nA*STARSingapore\n\nKeong Chee \nKwoh \nSchool of Computer Science and Engineering\nNanyang Technological University\nSingapore\n\nXiaoli Li xlli@i2r.a-star.edu.sg \nInstitute for Infocomm Research\nA*STARSingapore\n\nCuntai Guan ctguan@ntu.edu.sg \nSchool of Computer Science and Engineering\nNanyang Technological University\nSingapore\n\nTime-Series Representation Learning via Temporal and Contextual Contrasting\nThis article has been published in the International Joint Conferences on Artificial Intelligence (IJCAI-21). 1\nLearning decent representations from unlabeled time-series data with temporal dynamics is a very challenging task. In this paper, we propose an unsupervised Time-Series representation learning framework via Temporal and Contextual Contrasting (TS-TCC), to learn time-series representation from unlabeled data. First, the raw timeseries data are transformed into two different yet correlated views by using weak and strong augmentations. Second, we propose a novel temporal contrasting module to learn robust temporal representations by designing a tough cross-view prediction task. Last, to further learn discriminative representations, we propose a contextual contrasting module built upon the contexts from the temporal contrasting module. It attempts to maximize the similarity among different contexts of the same sample while minimizing similarity among contexts of different samples. Experiments have been carried out on three real-world time-series datasets. The results manifest that training a linear classifier on top of the features learned by our proposed TS-TCC performs comparably with the supervised training. Additionally, our proposed TS-TCC shows high efficiency in few-labeled data and transfer learning scenarios. The code is publicly available at https://github.com/emadeldeen24/TS-TCC.\n\nIntroduction\n\nTime-series data are being incrementally collected on daily basis from IoT and wearable devices for various applications in healthcare, manufacturing, etc. However, they generally do not have human recognizable patterns and require specialists for annotation/labeling. Therefore, it is much harder to label time-series data than images, and little time-series data have been labeled in real-world applications [Ching et al., 2018]. Given that deep learning methods usually require a massive amount of labeled data for training, it is thus very challenging to apply them on time-series data with these labeling limitations. * Corresponding Author Self-supervised learning gained more attention recently to extract effective representations from unlabeled data for downstream tasks. Compared with models trained on full labeled data (i.e., supervised models), self-supervised pretrained models can achieve comparable performance with limited labeled data . Various selfsupervised approaches relied on different pretext tasks to train the models and learn representations from unlabeled data, such as solving puzzles [Noroozi and Favaro, 2016] and predicting image rotation [Gidaris et al., 2018]. However, the pretext tasks can limit the generality of the learned representations. For example, classifying the different rotation angles of an image may deviate the model from learning features about the color or orientation of objects [Oord et al., 2018].\n\nContrastive learning has recently shown its strong ability for self-supervised representation learning in computer vision domain because of its ability to learn invariant representation from augmented data [Hjelm et al., 2019;He et al., 2020;. It explores different views of the input images by first applying data augmentation techniques and then learns the representations by maximizing the similarity of different views from the same sample and minimizing the similarity with the views from different samples. However, these image-based contrastive learning methods are not able to work well on time-series data for the following reasons. First, they may not be able to address the temporal dependencies of data, which are key characteristics of time-series [Franceschi et al., 2019]. Second, some augmentation techniques used for images such as color distortion, generally cannot fit well with time-series data. So far, few works on contrastive learning have been proposed for time-series data. For example, [Mohsenvand et al., 2020;Cheng et al., 2020] developed contrastive learning methods for bio-signals such as EEG and ECG. However, the above two methods are proposed for specific applications and they are not generalizable to other time-series data.\n\nTo address the above issues, we propose a Time-Series representation learning framework via Temporal and Contextual Contrasting (TS-TCC). Our framework employs simple yet efficient data augmentations that can fit any time-series data to create two different, but correlated views of the input data. Next, we propose a novel temporal contrasting module to learn robust representations by designing a tough cross-view prediction task, which for a certain timestep, it utilizes the past latent features of one augmentation to predict the future of another augmentation. This novel operation will force the model to learn robust representation by a harder prediction task against any perturbations introduced by different timesteps and augmentations. Furthermore, we propose a contextual contrasting module in TS-TCC to further learn discriminative representations upon the robust representations learned by the temporal contrasting module. In this contextual contrasting module, we aim to maximize the similarity among different contexts of the same sample while minimizing similarity among contexts of different samples.\n\nIn summary, the main contributions of this work are as follows.\n\n\u2022 A novel contrastive learning framework is proposed for unsupervised time-series representation learning.\n\n\u2022 Simple yet efficient augmentations are designed for time-series data in the contrastive learning framework. \u2022 We propose a novel temporal contrasting module to learn robust representations from time series data by designing a tough cross-view prediction task. In addition, we propose a contextual contrasting module to further learn discriminative representations upon the robust representations. \u2022 We perform extensive experiments on our proposed TS-TCC framework using three datasets. Experimental results show that the learned representations are effective for downstream tasks under supervised learning, semisupervised learning and transfer learning settings.\n\n2 Related Works 2.1 Self-supervised Learning\n\nThe recent advances in self-supervised learning started with applying pretext tasks on images to learn useful representations, such as solving jigsaw puzzles [Noroozi and Favaro, 2016], image colorization [Zhang et al., 2016] and predicting image rotation [Gidaris et al., 2018]. Despite the good results achieved by these pretext tasks, they relied on heuristics that might limit the generality of the learned representations. On the other hand, contrastive methods started to shine via learning invariant representations from augmented data. For instance, MoCo [He et al., 2020] utilized a momentum encoder to learn representations of negative pairs obtained from a memory bank. SimCLR  replaced the momentum encoder by using a larger batch of negative pairs. Also, BYOL [Grill et al., 2020] learned representations by bootstrapping representations even without using negative samples. Last, SimSiam [Chen and He, 2020] supported the idea of neglecting the negative samples, and relied only on a Siamese network and stop-gradient operation to achieve the state-of-the-art performance. While all these approaches have successfully improved representation learning for visual data, they may not work well on time series data that have different properties, such as temporal dependency.\n\n\nSelf-supervised Learning for Time-Series\n\nRepresentation learning for time series is becoming increasingly popular. Some approaches employed pretext tasks for   to EEG data. Existing approaches used either temporal or global features. Differently, we first construct different views for input data by designing time-series specific augmentations. Additionally, we propose a novel cross-view temporal and contextual contrasting modules to improve the learned representations for time-series data.\n\n\nMethods\n\nThis section describes our proposed TS-TCC in details. As shown in Figure 1, we first generate two different yet correlated views of the input data based on strong and weak augmentations. Then, a temporal contrasting module is proposed to explore the temporal features of the data with an autoregressive model. These models perform a tough cross-view prediction task by predicting the future of one view using the past of the other. We further maximize the agreement be-tween the contexts of the autoregressive models by a contextual contrasting module. Next, we will introduce each component in the following subsections.\n\n\nTime-Series Data Augmentation\n\nData augmentation is a key part in the success of the contrastive learning methods Grill et al., 2020]. Contrastive methods try to maximize the similarity among different views of the same sample, while minimizing its similarity with other samples. It is thus important to design proper data augmentations for contrastive learning Mohsenvand et al., 2020]. Usually, contrastive learning methods use two (random) variants of the same augmentation. Given a sample x, they produce two views x 1 and x 2 sampled from the same augmentations family T , i.e., x 1 \u223c T and x 2 \u223c T . However, we argue that using different augmentations can improve the robustness of the learned representations. Consequently, we propose applying two separate augmentations, such that one augmentation is weak and the other is strong. In this paper, weak augmentation is a jitter-and-scale strategy. Specifically, we add random variations to the signal and scale up its magnitude. For strong augmentation, we apply permutation-and-jitter strategy, where permutation includes splitting the signal into a random number of segments with a maximum of M and randomly shuffling them. Next, a random jittering is added to the permuted signal. Notably, the augmentation hyperparameters should be chosen carefully according to the nature of the time-series data. For example, the value of M in a time-series data with longer sequences should be greater than its value in those with shorter sequences when applying permutation. Similarly, the jittering ratio for normalized time-series data should be much less than the ratio for unnormalized data. For each input sample x, we denote its strongly augmented view as x s , and its weakly augmented view as x w , where x s \u223c T s and x w \u223c T w . These views are then passed to the encoder to extract their high dimensional latent representations. In particular, the encoder has a 3-block convolutional architecture as proposed in [Wang et al., 2017]. For an input x, the encoder maps x into a high-dimensional latent representation z = f enc (x). We define z = [z 1 , z 2 , . . . z T ], where T is the total timesteps, z i \u2208 R d , where d is the feature length. Thus, we get z s for the strong augmented views, and z w for the weak augmented views, which are then fed into the temporal contrasting module.\n\n\nTemporal Contrasting\n\nThe Temporal Contrasting module deploys a contrastive loss to extract temporal features in the latent space with an autoregressive model. Given the latent representations z, the autoregressive model f ar summarizes all z \u2264t into a context vector c t = f ar (z \u2264t ), c t \u2208 R h , where h is the hidden dimension of f ar . The context vector c t is then used to predict the timesteps from z t+1 until z t+k (1 < k \u2264 K). To predict future timesteps, we use log-bilinear model that would preserve the mutual information between the input x t+k and c t ,  Figure 2: Architecture of Transformer model used in Temporal Contrasting module. The token c in the output is sent next to the Contextual Contrasting module.\nsuch that f k (x t+k , c t ) = exp((W k (c t )) T z t+k ), where W k is\nIn our approach, the strong augmentation generates c s t and the weak augmentation generates c w t . We propose a tough cross-view prediction task by using the context of the strong augmentation c s t to predict the future timesteps of the weak augmentation z w t+k and vice versa. The contrastive loss tries to minimize the dot product between the predicted representation and the true one of the same sample, while maximizing the dot product with the other samples N t,k within the minibatch. Accordingly, we calculate the two losses L s T C and L w T C as follows:\nL s T C = \u2212 1 K K k=1 log exp((W k (c s t )) T z w t+k ) n\u2208N t,k exp((W k (c s t )) T z w n )\n(1)\nL w T C = \u2212 1 K K k=1 log exp((W k (c w t )) T z s t+k ) n\u2208N t,k exp((W k (c w t )) T z s n )(2)\nWe use Transformer as the autoregressive model because of its efficiency and speed [Vaswani et al., 2017]. The architecture of the Transformer model is shown in Figure 2. It mainly consists of successive blocks of multi-headed attention (MHA) followed by an MLP block. The MLP block is composed of two fully-connected layers with a non-linearity ReLU function and dropout in between. Pre-norm residual connections, which can produce more stable gradients [Wang et al., 2019], are adopted in our Transformer. We stack L identical layers to generate the final features. Inspired by BERT model [Devlin et al., 2019], we add a token c \u2208 R h to the input whose state acts as a representative context vector in the output. The operation of the Transformer starts by applying the features z \u2264t to a linear projection W T ran layer that maps the features into the hidden dimension, i.e. W T ran : R d\u2192h . The output of this linear projection is then sent to the Transformer i.e.z = W T ran (z \u2264t ),z \u2208 R h . Next, we attach the context vector into the features vectorz such that the input features become \u03c8 0 = [c;z], where the subscript 0 denotes being the input to the first layer. Next, we pass \u03c8 0 through Transformer layers as in the following equations:\n\u03c8 = MHA(Norm(\u03c8 \u22121 )) + \u03c8 \u22121 , 1 \u2264 \u2264 L; (3) \u03c8 = MLP(Norm(\u03c8 )) +\u03c8 , 1 \u2264 \u2264 L. (4)\nFinally, we re-attach the context vector from the final output such that c t = \u03c8 0 L . This context vector will be the input of the following contextual contrasting module.\n\n\nContextual Contrasting\n\nWe further propose a contextual contrasting module that aims to learn more discriminative representations. It starts with applying a non-linear transformation to the contexts using a non-linear projection head as in . The projection head maps the contexts into the space of where the contextual contrasting is applied.\n\nGiven a batch of N input samples, we will have two contexts for each sample from its two augmented views, and thus have 2N contexts. For a context c i t , we denote c i + t as the positive sample of c i t that comes from the other augmented view of the same input, and hence, (c i t , c i + t ) are considered to be a positive pair. Meanwhile, the remaining (2N \u2212 2) contexts from other inputs within the same batch are considered as the negative samples of c i t , i.e., c i t can form (2N \u2212 2) negative pairs with its negative samples. Therefore, we can derive a contextual contrasting loss to maximize the similarity between the positive pair and minimizing the similarity between negative pairs. As such, the final representations can be discriminative.\n\nEq. 5 defines the contextual contrasting loss function L CC . Given a context c i t , we divide its similarity with its positive sample c i + t by its similarity with all the other (2N \u2212 1) samples, including the positive pair and (2N \u2212 2) negative pairs, to normalize the loss.\nL CC = \u2212 N i=1 log exp sim c i t , c i + t /\u03c4 2N m=1 1 [m =i] exp sim c i t , c m t /\u03c4 ,(5)\nwhere sim(u, v) = u T v/ u v denotes the dot product between 2 normalized u and v (i.e., cosine similarity), 1 [m =i] \u2208 {0, 1} is an indicator function, evaluating to 1 iff m = i, and \u03c4 is a temperature parameter.\n\nThe overall self-supervised loss is the combination of the two temporal contrasting losses and the contextual contrasting loss as follows.\nL = \u03bb 1 \u00b7 (L s T C + L w T C ) + \u03bb 2 \u00b7 L CC ,(6)\nwhere \u03bb 1 and \u03bb 2 are fixed scalar hyperparameters denoting the relative weight of each loss.\n\n\nExperimental Setup\n\n\nDatasets\n\nTo evaluate our model, we adopted three publicly available datasets for human activity recognition, sleep stage classification and epileptic seizure prediction, respectively. Additionally, we investigated the transferability of our learned features on a fault diagnosis dataset.\n\n\nHuman Activity Recognition (HAR)\n\nWe \n\n\nSleep Stage Classification\n\nIn this problem, we aim to classify the input EEG signal into one of five classes: Wake (W), Non-rapid eye movement (N1, N2, N3) and Rapid Eye Movement (REM). We downloaded Sleep-EDF dataset from the PhysioBank [Goldberger et al., 2000]. Sleep-EDF includes whole-night PSG sleep recordings, where we used a single EEG channel (i.e., Fpz-Cz) with a sampling rate of 100 Hz, following previous studies [Eldele et al., 2021].\n\n\nEpilepsy Seizure Prediction\n\nThe Epileptic Seizure Recognition dataset [Andrzejak et al., 2001] consists of EEG recordings from 500 subjects, where the brain activity was recorded for each subject for 23.6 seconds. Note that the original dataset is labeled with five classes. As four of them do not include epileptic seizure, so we merged them into one class and treat it as a binary classification problem.\n\n\nFault Diagnosis (FD)\n\nWe conducted the transferability experiment on a real-world fault diagnosis dataset [Lessmeier et al., 2016]. This dataset was collected under four different working conditions. Each working condition can be considered as a separate domain as it has different characteristics from the other working conditions [Ragab et al., 2020]. Each domain has three classes, namely, two fault classes (i.e., inner fault and outer fault) and one healthy class. Table 1 summarizes the details of each dataset, e.g., the number of training samples (# Train) and testing samples (# Test), the length of the sample, the number of sensor channels (# Channel) and the number of classes (# Class).\n\n\nImplementation Details\n\nWe split the data into 60%, 20%, 20% for training, validation and testing, with considering subject-wise split for Sleep-EDF dataset to avoid overfitting. Experiments were repeated for 5 times with 5 different seeds, and we reported the mean and standard deviation. The pretraining and downstream tasks were done for 40 epochs, as we noticed that the performance does not improve with further training. We applied a batch size of 128 (which was reduced to 32 in few-labeled data experiments as data size may be less than 128). We used Adam optimizer with a learning rate of 3e-4, weight decay of 3e-4, \u03b2 1 = 0.9, and \u03b2 2 = 0.99. For the strong augmentation, we set M HAR = 10, M Ep = 12 and M EDF = 20, while for the weak augmentation, we set the scaling ratio to 2 for all the datasets. We set \u03bb 1 = 1, while we achieved good performance when \u03bb 2 \u2248 1. Particularly, we set it as 0.7 in our experiments on the four datasets. In  the Transformer, we set the L = 4, and the number of heads as 4. We tuned h \u2208 {32, 50, 64, 100, 128, 200, 256} and set h HAR,Ep = 100, h EDF = 64. We also set its dropout to 0.1. In contextual contrasting, we set \u03c4 = 0.2. Lastly, we built our model using PyTorch 1.7 and trained it on a NVIDIA GeForce RTX 2080 Ti GPU.\n\n\nResults\n\nTo show the efficacy of our proposed TS-TCC, we test it on three different training settings, including linear evaluation, semi-supervised training and transfer learning. We evaluate the performance using two metrics namely the accuracy and the macro-averaged F1-score (MF1) to better evaluate the imbalanced datasets.\n\n\nComparison with Baseline Approaches\n\nWe compare our proposed approach against the following baselines. SimCLR . It is worth noting that, we use time-series specific augmentations to adapt SimCLR to our application as it was originally designed for images.\n\nTo evaluate the performance of our TS-TCC model, we follow the standard linear benchmarking evaluation scheme [Oord et al., 2018;. Particularly, we train a linear classifier (single MLP layer) on top of a frozen self-supervised pretrained encoder model. Table 2 shows the linear evaluation results of our approach against the baseline methods. Overall, our proposed TS-TCC outperforms all the three state-of-the-art methods. Furthermore, TS-TCC, with only linear classifier, performs best on two out of three datasets while achieving comparable performance to the supervised approach on the third dataset. This demonstrates the powerful representation learning capability of our TS-TCC model. Notably, contrastive methods (e.g., CPC, SimCLR and our TS-TCC) generally achieve better results than the pretext-based method (i.e., SSL-ECG), which reflects the power of invariant features learned by contrastive methods. Additionally, CPC method shows better results than SimCLR, indicating that temporal features are more important than general features in time-series data. \n\n\nSemi-supervised Training\n\nWe investigate the effectiveness of our TS-TCC under the semi-supervised settings, by training the model with 1%, 5%, 10%, 50%, and 75% of randomly selected instances of the training data. Figure 3 shows the results of our TS-TCC along with the supervised training under the aforementioned settings. In particular, TS-TCC fine-tuning (i.e., red curves in Figure 3) means that we fine-tuned the pretrained encoder with few labeled samples.\n\nWe observe that supervised training performs poorly with limited labeled data, while our TS-TCC fine-tuning achieves significantly better performance than supervised training with only 1% of labeled data. For example, TS-TCC fine-tuning can still achieve around 70% and 90% for HAR and Epilepsy datasets respectively. Furthermore, our TS-TCC fine-tuning with only 10% of labeled data can achieve comparable performance with the supervised training with 100% of labeled data in the three datasets, demonstrating the effectiveness of our TS-TCC method under the semi-supervised setting.\n\n\nTransfer Learning Experiment\n\nWe further examine the transferability of the learned features by designing a transfer learning experiment. We use Fault Diagnosis (FD) dataset introduced in Table 1 for the evaluation under the transfer learning setting. Here, we train the model on one condition (i.e., source domain) and test it on another condition (i.e., target domain). In particular, we adopt two training schemes on the source domain, namely, (1) supervised training and (2)    fine-tuned our pretrained encoder using the labeled data in the source domain. Table 3 shows the performance of the two training schemes under 12 cross-domain scenarios. Clearly, our pretrained TS-TCC model with fine-tuning (FT) consistently outperforms the supervised pretraining in 8 out of 12 cross-domain scenarios. TS-TCC model can achieve at least 7% improvement in 7 out of 8 winning scenarios (except for D\u2192B scenario). Overall, our proposed approach can improve the transferability of learned representations over the supervised training by about 4% in terms of accuracy.\n\n\nAblation Study\n\nWe study the effectiveness of each component in our proposed TS-TCC model. Specifically, we derive different model variants for comparison as follows. First, we train the Temporal Contrasting module (TC) without the cross-view prediction task, where each branch predicts the future timesteps of the same augmented view. This variant is denoted as 'TC only'. Second, we train the TC module with adding the cross-view prediction task, which is denoted as 'TC + X-Aug'. Third, we train the whole proposed TS-TCC model, which is denoted as 'TC + X-Aug + CC'. We also study the effect of using a single augmentation in TS-TCC. In particular, for an input x, we generate two different views x 1 and x 2 from the same augmentation type, i.e., x 1 \u223c T w and x 2 \u223c T w when using the weak augmentation. Table 4 shows this ablation study on the three datasets. Clearly, the proposed cross-view prediction task generates robust features and thus improves the performance by more than 5% on HAR datasets, and \u223c1% on Sleep-EDF and Epilepsy datasets. Additionally, the contextual contrasting module further improves the performance, as it helps the features to be more discriminative. Studying the augmentations effect, we find that generating different views from the same augmentation type is not helpful with HAR and Sleep-EDF datasets. On the other hand, Epilepsy dataset can achieve comparable performance with only one augmentation. Overall, our proposed TS-TCC method using both types of augmentations achieves the best performance.\n\n\nSensitivity Analysis\n\nWe perform sensitivity analysis on HAR dataset to study three parameters namely, the number of predicted future timesteps K in the temporal contrasting module, besides \u03bb 1 and \u03bb 2 in Eq. 5. Figure 4a shows the effect of K on the overall performance, where x-axis is the percentage K/d, d is the length of the features. Clearly, increasing the percentage of the predicted future timesteps improves the performance. However, larger percentages can harm the performance as it reduces the amount of past data used for training the autoregressive model. We observe that predicting 40% of the total feature length performs the best, and thus we set K as d\u00d740% in our experiments. Figures 4b and 4c show the results of varying \u03bb 1 and \u03bb 2 in a range between 0.001 and 1000 respectively. We fix \u03bb 1 = 1 and change the values of \u03bb 2 in Figure 4c. We observe that our model achieves good performance when \u03bb 2 \u2248 1, where the model performs best with \u03bb 2 = 0.7. Consequently, we fix \u03bb 2 = 0.7 and tune the value of \u03bb 1 as in Figure 4b, where we find that our model achieves the best performance when \u03bb 1 = 1. We also find that as \u03bb 1 < 10, our model is less sensitive to its value, while it is more sensitive to different values of \u03bb 2 .\n\n\nConclusions\n\nWe propose a novel framework called TS-TCC for unsupervised representation learning from time-series data. The proposed TS-TCC framework first creates two views for each sample by applying strong and weak augmentations. Then the temporal contrasting module learns robust temporal features by applying a tough cross-view prediction task. We further propose a contextual contrasting module to learn discriminative features upon the learned robust representations. The experiments show that a linear classifier trained on top the features learned by our TS-TCC performs comparably with supervised training. In addition, our proposed TS-TCC shows high efficiency on few-labeled data and transfer learning scenarios, e.g., our TS-TCC by using only 10% of the labeled data can achieve close performance to the supervised training with full labeled data.\n\n( 1 )\n1Random Initialization: training a linear classifier on top of randomly initialized encoder; (2) Supervised: supervised training of both encoder and classifier model; (3) SSL-ECG [P. Sarkar, 2020]; (4) CPC [Oord et al., 2018]; (5)\n\nFigure 3 :\n3Comparison between supervised training vs. TS-TCC fine-tuning for different few-labeled data scenarios in terms of MF1.\n\nFigure 4 :\n4Three sensitivity analysis experiments on HAR dataset.\n\n\na linear function that maps c t back into the same dimension as z, i.e. W k : R h\u2192d .Nx \n\nNor m \nM ulti-Head \nAttention \nM L P \nNor m \n\nEncoder \nFeatures \n\nM odified \nFeatures \n\nlinear \nProj ection \n\n\n\n\nTable 1: Description of datasets used in our experiments. The details of FD is the same for all the 4 working conditions.use UCI HAR dataset [Anguita et al., 2013] which con-\ntains sensor readings for 30 subjects performing 6 activities \n(i.e. walking, walking upstairs, downstairs, standing, sitting, \nand lying down). They collected the data using a mounted \nSamsung Galaxy S2 device on their waist, with a sampling \nrate of 50 Hz. \n\nDataset \n# Train # Test Length # Channel # Class \n\nHAR \n7352 \n2947 \n128 \n9 \n6 \nSleep-EDF 25612 \n8910 \n3000 \n1 \n5 \nEpilepsy \n9200 \n2300 \n178 \n1 \n2 \nFD \n8184 \n2728 \n5120 \n1 \n3 \n\n\n\nTable 2 :\n2Comparison between our proposed TS-TCC model against baselines using linear classifier evaluation experiment.\n\n\nTS-TCC fine-tuning where we A\u2192B A\u2192C A\u2192D B\u2192A B\u2192C B\u2192D C\u2192A C\u2192B C\u2192D D\u2192A D\u2192B D\u2192C AVGSupervised \n34.38 44.94 34.57 52.93 63.67 99.82 52.93 84.02 83.54 53.15 99.56 62.43 63.83 \nTS-TCC (FT) 43.15 51.50 42.74 47.98 70.38 99.30 38.89 98.31 99.38 51.91 99.96 70.31 67.82 \n\n\n\nTable 3 :\n3Cross-domains transfer learning experiment applied on Fault Diagnosis dataset in terms of accuracy. (FT stands for fine-tuning) 76\u00b11.50 82.17\u00b11.64 80.55\u00b10.39 70.99\u00b10.86 94.39\u00b11.19 90.93\u00b11.41 TC + X-Aug 87.86\u00b11.33 87.91\u00b11.09 81.58\u00b11.70 71.88\u00b11.71 95.56\u00b10.24 92.57\u00b10.29 TS-TCC (TC + X-Aug + CC) 90.37\u00b10.34 90.38\u00b10.39 83.00\u00b10.71 73.57\u00b10.74 97.23\u00b10.10 95.54\u00b10.08 TS-TCC (Weak only) 76.55\u00b13.59 75.14\u00b14.66 80.90\u00b11.87 72.51\u00b11.74 97.18\u00b10.17 95.47\u00b10.31 TS-TCC (Strong only) 60.23\u00b13.31 56.15\u00b14.14 78.55\u00b12.94 68.05\u00b11.87 97.14\u00b10.23 95.39\u00b10.29HAR \nSleep-EDF \nEpilepsy \n\nComponent \nACC \nMF1 \nACC \nMF1 \nACC \nMF1 \n\nTC only \n82.\n\nTable 4 :\n4Ablation study of each component in TS-TCC model performed with linear classifier evaluation experiment.\nAcknowledgementsThis research is supported by the Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funds (Grant No. A20H6b0151) and Career Development Award (Grant No. C210112046).\nIndications of nonlinear deterministic and finite-dimensional structures in time series of brain electrical activity: Dependence on recording region and brain state. Aggarwal, Phys. Rev. E. AAAIAdversarial unsupervised representation learning for activity timeseries. A public domain dataset for human activity recognition using smartphones. In ESANNReferences [Aggarwal et al., 2019] Karan Aggarwal, Shafiq R. Joty, Luis Fern\u00e1ndez-Luque, and Jaideep Srivastava. Adversar- ial unsupervised representation learning for activity time- series. AAAI, 2019. [Andrzejak et al., 2001] Ralph G. Andrzejak, Klaus Lehn- ertz, Florian Mormann, and et al. Rieke. Indications of nonlinear deterministic and finite-dimensional structures in time series of brain electrical activity: Dependence on recording region and brain state. Phys. Rev. E, 2001. [Anguita et al., 2013] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, and Jorge Luis Reyes-Ortiz. A public domain dataset for human activity recognition using smartphones. In ESANN, 2013.\n\narXiv:2011.10566and He, 2020] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv preprintand He, 2020] Xinlei Chen and Kaiming He. Explor- ing simple siamese representation learning. arXiv preprint arXiv:2011.10566, 2020.\n\nA simple framework for contrastive learning of visual representations. arXiv:2007.04871Oncel Tuzel, and Erdrin Azemi. Subjectaware contrastive learning for biosignals. arXiv preprintICMLet al., 2020] Ting Chen, Simon Kornblith, Moham- mad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. [Cheng et al., 2020] Joseph Y Cheng, Hanlin Goh, Kaan Dogrusoz, Oncel Tuzel, and Erdrin Azemi. Subject- aware contrastive learning for biosignals. arXiv preprint arXiv:2007.04871, 2020.\n\nAn attention-based deep learning approach for sleep stage classification with single-channel eeg. Ching , IEEE Transactions on Neural Systems and Rehabilitation Engineering. Emadeldeen Eldele, Zhenghua Chen, Chengyu Liu, Min Wu, Chee-Keong Kwoh, Xiaoli Li, and Cuntai GuanACLChing et al., 2018] Travers Ching, Daniel S Himmelstein, Brett K Beaulieu-Jones, Alexandr A Kalinin, Brian T Do, Gregory P Way, Enrico Ferrero, Paul-Michael Agapow, Michael Zietz, Michael M Hoffman, et al. Opportunities and obstacles for deep learning in biology and medicine. Journal of The Royal Society Interface, 2018. [Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken- ton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understand- ing. In ACL, 2019. [Eldele et al., 2021] Emadeldeen Eldele, Zhenghua Chen, Chengyu Liu, Min Wu, Chee-Keong Kwoh, Xiaoli Li, and Cuntai Guan. An attention-based deep learning ap- proach for sleep stage classification with single-channel eeg. IEEE Transactions on Neural Systems and Rehabili- tation Engineering, 2021.\n\nUnsupervised scalable representation learning for multivariate time series. [ Franceschi, NeurIPS. [Franceschi et al., 2019] Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable representation learning for multivariate time series. In NeurIPS, 2019.\n\nUnsupervised representation learning by predicting image rotations. [ Gidaris, Spyros Gidaris, Praveer Singh, and Nikos Komodakis. ICML[Gidaris et al., 2018] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In ICML, 2018.\n\nLearning deep representations by mutual information estimation and maximization. [ Goldberger, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals. circulation. CVPR, 2020. [Hjelm et al.101ICML[Goldberger et al., 2000] Ary L Goldberger, Luis AN Ama- ral, Leon Glass, and et al. Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals. circulation, 101(23), 2000. [Grill et al., 2020] Jean-Bastien Grill, Florian Strub, and Florent Altch\u00e9 et al. Bootstrap your own latent: A new approach to self-supervised learning. In NeurIPS, 2020. [He et al., 2020] Kaiming He, Haoqi Fan, Yuxin Wu, Sain- ing Xie, and Ross Girshick. Momentum contrast for unsu- pervised visual representation learning. In CVPR, 2020. [Hjelm et al., 2019] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representa- tions by mutual information estimation and maximization. In ICML, 2019.\n\nCondition monitoring of bearing damage in electromechanical drive systems by using motor current signals of electric motors: A benchmark data set for data-driven classification. [ Lessmeier, Machine Learning for Health NeurIPS Workshop. Noroozi and FavaroECCV[Lessmeier et al., 2016] Christian Lessmeier, James Kuria Kimotho, Detmar Zimmer, and Walter Sextro. Condition monitoring of bearing damage in electromechanical drive systems by using motor current signals of electric motors: A benchmark data set for data-driven classification. In Eu- ropean conference of the prognostics and health manage- ment society, 2016. [Mohsenvand et al., 2020] Mostafa Neo Mohsenvand, Mo- hammad Rasool Izadi, and Pattie Maes. Contrastive repre- sentation learning for electroencephalogram classification. In Machine Learning for Health NeurIPS Workshop, 2020. [Noroozi and Favaro, 2016] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, 2016.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. [ Oord, arXiv:1807.03748Representation learning with contrastive predictive coding. arXiv preprint[Oord et al., 2018] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic- tive coding. arXiv preprint arXiv:1807.03748, 2018.\n\nTime series classification from scratch with deep neural networks: A strong baseline. ; A Sarkar, P Etemad, Sarkar, Ragab, IEEE Transactions on Affective Computing. Wang et al., 2017] Zhiguang Wang, Weizhong Yan, and Tim OatesACLSarkar, 2020] A. Etemad P. Sarkar. Self-supervised ecg representation learning for emotion recognition. IEEE Transactions on Affective Computing, 2020. [Ragab et al., 2020] Mohamed Ragab, Zhenghua Chen, Min Wu, Haoliang Li, Chee-Keong Kwoh, Ruqiang Yan, and Xiaoli Li. Adversarial multiple-target domain adaptation for fault classification. IEEE Transactions on Instrumen- tation and Measurement, 2020. [Saeed et al., 2019] Aaqib Saeed, Tanir Ozcelebi, and Johan Lukkien. Multi-task self-supervised learning for human activity detection. ACM Interact. Mob. Wearable Ubiqui- tous Technol., 2019. [Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. [Wang et al., 2017] Zhiguang Wang, Weizhong Yan, and Tim Oates. Time series classification from scratch with deep neural networks: A strong baseline. In IJCNN, 2017. [Wang et al., 2019] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. Learning deep transformer models for machine translation. In ACL, 2019.\n\nColorful image colorization. Zhang, ECCV. Zhang et al., 2016] Richard Zhang, Phillip Isola, and Alexei A. Efros. Colorful image colorization. In ECCV, 2016.\n", "annotations": {"author": "[{\"end\":184,\"start\":79},{\"end\":286,\"start\":185},{\"end\":350,\"start\":287},{\"end\":431,\"start\":351},{\"end\":443,\"start\":432},{\"end\":536,\"start\":444},{\"end\":619,\"start\":537},{\"end\":737,\"start\":620}]", "publisher": null, "author_last_name": "[{\"end\":96,\"start\":90},{\"end\":198,\"start\":193},{\"end\":300,\"start\":296},{\"end\":357,\"start\":355},{\"end\":442,\"start\":438},{\"end\":448,\"start\":444},{\"end\":546,\"start\":544},{\"end\":631,\"start\":627}]", "author_first_name": "[{\"end\":89,\"start\":79},{\"end\":192,\"start\":185},{\"end\":295,\"start\":287},{\"end\":354,\"start\":351},{\"end\":437,\"start\":432},{\"end\":543,\"start\":537},{\"end\":626,\"start\":620}]", "author_affiliation": "[{\"end\":183,\"start\":98},{\"end\":285,\"start\":200},{\"end\":349,\"start\":302},{\"end\":430,\"start\":383},{\"end\":535,\"start\":450},{\"end\":618,\"start\":571},{\"end\":736,\"start\":651}]", "title": "[{\"end\":76,\"start\":1},{\"end\":813,\"start\":738}]", "venue": null, "abstract": "[{\"end\":2233,\"start\":926}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2679,\"start\":2659},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3389,\"start\":3363},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3442,\"start\":3420},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3701,\"start\":3682},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3930,\"start\":3910},{\"end\":3946,\"start\":3930},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4490,\"start\":4465},{\"end\":4741,\"start\":4716},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4760,\"start\":4741},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7156,\"start\":7130},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7197,\"start\":7177},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7250,\"start\":7228},{\"end\":7893,\"start\":7874},{\"end\":9525,\"start\":9506},{\"end\":9778,\"start\":9754},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11382,\"start\":11363},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13411,\"start\":13389},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13780,\"start\":13761},{\"end\":13918,\"start\":13897},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17401,\"start\":17376},{\"end\":17586,\"start\":17565},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17685,\"start\":17661},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18130,\"start\":18106},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18352,\"start\":18332},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20692,\"start\":20673}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27647,\"start\":27410},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27780,\"start\":27648},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27848,\"start\":27781},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":28051,\"start\":27849},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":28666,\"start\":28052},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":28788,\"start\":28667},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":29053,\"start\":28789},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":29677,\"start\":29054},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":29794,\"start\":29678}]", "paragraph": "[{\"end\":3702,\"start\":2249},{\"end\":4964,\"start\":3704},{\"end\":6084,\"start\":4966},{\"end\":6149,\"start\":6086},{\"end\":6257,\"start\":6151},{\"end\":6924,\"start\":6259},{\"end\":6970,\"start\":6926},{\"end\":8257,\"start\":6972},{\"end\":8755,\"start\":8302},{\"end\":9389,\"start\":8767},{\"end\":11738,\"start\":9423},{\"end\":12470,\"start\":11763},{\"end\":13110,\"start\":12543},{\"end\":13208,\"start\":13205},{\"end\":14557,\"start\":13306},{\"end\":14809,\"start\":14637},{\"end\":15154,\"start\":14836},{\"end\":15913,\"start\":15156},{\"end\":16193,\"start\":15915},{\"end\":16499,\"start\":16286},{\"end\":16639,\"start\":16501},{\"end\":16782,\"start\":16689},{\"end\":17094,\"start\":16816},{\"end\":17134,\"start\":17131},{\"end\":17587,\"start\":17165},{\"end\":17997,\"start\":17619},{\"end\":18699,\"start\":18022},{\"end\":19973,\"start\":18726},{\"end\":20303,\"start\":19985},{\"end\":20561,\"start\":20343},{\"end\":21634,\"start\":20563},{\"end\":22101,\"start\":21663},{\"end\":22687,\"start\":22103},{\"end\":23752,\"start\":22720},{\"end\":25296,\"start\":23771},{\"end\":26546,\"start\":25321},{\"end\":27409,\"start\":26562}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12542,\"start\":12471},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13204,\"start\":13111},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13305,\"start\":13209},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14636,\"start\":14558},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16285,\"start\":16194},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16688,\"start\":16640}]", "table_ref": "[{\"end\":18477,\"start\":18470},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":20824,\"start\":20817},{\"end\":22885,\"start\":22878},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":23258,\"start\":23251},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":24572,\"start\":24565}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2247,\"start\":2235},{\"attributes\":{\"n\":\"2.2\"},\"end\":8300,\"start\":8260},{\"attributes\":{\"n\":\"3\"},\"end\":8765,\"start\":8758},{\"attributes\":{\"n\":\"3.1\"},\"end\":9421,\"start\":9392},{\"attributes\":{\"n\":\"3.2\"},\"end\":11761,\"start\":11741},{\"attributes\":{\"n\":\"3.3\"},\"end\":14834,\"start\":14812},{\"attributes\":{\"n\":\"4\"},\"end\":16803,\"start\":16785},{\"attributes\":{\"n\":\"4.1\"},\"end\":16814,\"start\":16806},{\"end\":17129,\"start\":17097},{\"end\":17163,\"start\":17137},{\"end\":17617,\"start\":17590},{\"end\":18020,\"start\":18000},{\"attributes\":{\"n\":\"4.2\"},\"end\":18724,\"start\":18702},{\"attributes\":{\"n\":\"5\"},\"end\":19983,\"start\":19976},{\"attributes\":{\"n\":\"5.1\"},\"end\":20341,\"start\":20306},{\"attributes\":{\"n\":\"5.2\"},\"end\":21661,\"start\":21637},{\"attributes\":{\"n\":\"5.3\"},\"end\":22718,\"start\":22690},{\"attributes\":{\"n\":\"5.4\"},\"end\":23769,\"start\":23755},{\"attributes\":{\"n\":\"5.5\"},\"end\":25319,\"start\":25299},{\"attributes\":{\"n\":\"6\"},\"end\":26560,\"start\":26549},{\"end\":27416,\"start\":27411},{\"end\":27659,\"start\":27649},{\"end\":27792,\"start\":27782},{\"end\":28677,\"start\":28668},{\"end\":29064,\"start\":29055},{\"end\":29688,\"start\":29679}]", "table": "[{\"end\":28051,\"start\":27936},{\"end\":28666,\"start\":28175},{\"end\":29053,\"start\":28870},{\"end\":29677,\"start\":29596}]", "figure_caption": "[{\"end\":27647,\"start\":27418},{\"end\":27780,\"start\":27661},{\"end\":27848,\"start\":27794},{\"end\":27936,\"start\":27851},{\"end\":28175,\"start\":28054},{\"end\":28788,\"start\":28679},{\"end\":28870,\"start\":28791},{\"end\":29596,\"start\":29066},{\"end\":29794,\"start\":29690}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8842,\"start\":8834},{\"end\":12321,\"start\":12313},{\"end\":13475,\"start\":13467},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21860,\"start\":21852},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22026,\"start\":22018},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25520,\"start\":25511},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26012,\"start\":25995},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26157,\"start\":26148},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26343,\"start\":26334}]", "bib_author_first_name": "[{\"end\":31946,\"start\":31941},{\"end\":33005,\"start\":33004},{\"end\":33277,\"start\":33276},{\"end\":33584,\"start\":33583},{\"end\":34726,\"start\":34725},{\"end\":35594,\"start\":35593},{\"end\":35949,\"start\":35948},{\"end\":35951,\"start\":35950},{\"end\":35961,\"start\":35960}]", "bib_author_last_name": "[{\"end\":30181,\"start\":30173},{\"end\":33016,\"start\":33006},{\"end\":33285,\"start\":33278},{\"end\":33595,\"start\":33585},{\"end\":34736,\"start\":34727},{\"end\":35599,\"start\":35595},{\"end\":35958,\"start\":35952},{\"end\":35968,\"start\":35962},{\"end\":35976,\"start\":35970},{\"end\":35983,\"start\":35978},{\"end\":37265,\"start\":37260}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":8582357},\"end\":31043,\"start\":30007},{\"attributes\":{\"doi\":\"arXiv:2011.10566\",\"id\":\"b1\"},\"end\":31299,\"start\":31045},{\"attributes\":{\"doi\":\"arXiv:2007.04871\",\"id\":\"b2\",\"matched_paper_id\":211096730},\"end\":31841,\"start\":31301},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":233448337},\"end\":32926,\"start\":31843},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":59413908},\"end\":33206,\"start\":32928},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":4009713},\"end\":33500,\"start\":33208},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52055130},\"end\":34545,\"start\":33502},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":4692476},\"end\":35542,\"start\":34547},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b8\"},\"end\":35860,\"start\":35544},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14303613},\"end\":37229,\"start\":35862},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":50698},\"end\":37387,\"start\":37231}]", "bib_title": "[{\"end\":30171,\"start\":30007},{\"end\":31370,\"start\":31301},{\"end\":31939,\"start\":31843},{\"end\":33002,\"start\":32928},{\"end\":33274,\"start\":33208},{\"end\":33581,\"start\":33502},{\"end\":34723,\"start\":34547},{\"end\":35591,\"start\":35544},{\"end\":35946,\"start\":35862},{\"end\":37258,\"start\":37231}]", "bib_author": "[{\"end\":30183,\"start\":30173},{\"end\":31949,\"start\":31941},{\"end\":33018,\"start\":33004},{\"end\":33287,\"start\":33276},{\"end\":33597,\"start\":33583},{\"end\":34738,\"start\":34725},{\"end\":35601,\"start\":35593},{\"end\":35960,\"start\":35948},{\"end\":35970,\"start\":35960},{\"end\":35978,\"start\":35970},{\"end\":35985,\"start\":35978},{\"end\":37267,\"start\":37260}]", "bib_venue": "[{\"end\":30195,\"start\":30183},{\"end\":31151,\"start\":31061},{\"end\":31467,\"start\":31388},{\"end\":32015,\"start\":31949},{\"end\":33025,\"start\":33018},{\"end\":33337,\"start\":33287},{\"end\":33709,\"start\":33597},{\"end\":34782,\"start\":34738},{\"end\":35675,\"start\":35617},{\"end\":36025,\"start\":35985},{\"end\":37271,\"start\":37267}]"}}}, "year": 2023, "month": 12, "day": 17}