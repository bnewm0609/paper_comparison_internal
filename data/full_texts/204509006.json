{"id": 204509006, "updated": "2023-10-06 22:07:42.515", "metadata": {"title": "Quantum-Inspired Classical Algorithms for Singular Value Transformation", "authors": "[{\"first\":\"Dhawal\",\"last\":\"Jethwani\",\"middle\":[]},{\"first\":\"Franccois\",\"last\":\"Gall\",\"middle\":[\"Le\"]},{\"first\":\"Sanjay\",\"last\":\"Singh\",\"middle\":[\"K.\"]}]", "venue": "Proceedings of the 45th International Symposium on Mathematical Foundations of Computer Science (MFCS 2020), 53:1-53:14, 2020", "journal": null, "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "A recent breakthrough by Tang (STOC 2019) showed how to\"dequantize\"the quantum algorithm for recommendation systems by Kerenidis and Prakash (ITCS 2017). The resulting algorithm, classical but\"quantum-inspired\", efficiently computes a low-rank approximation of the users' preference matrix. Subsequent works have shown how to construct efficient quantum-inspired algorithms for approximating the pseudo-inverse of a low-rank matrix as well, which can be used to (approximately) solve low-rank linear systems of equations. In the present paper, we pursue this line of research and develop quantum-inspired algorithms for a large class of matrix transformations that are defined via the singular value decomposition of the matrix. In particular, we obtain classical algorithms with complexity polynomially related (in most parameters) to the complexity of the best quantum algorithms for singular value transformation recently developed by Chakraborty, Gily\\'{e}n and Jeffery (ICALP 2019) and Gily\\'{e}n, Su, Low and Wiebe (STOC19).", "fields_of_study": "[\"Computer Science\",\"Physics\"]", "external_ids": {"arxiv": "1910.05699", "mag": "3081652313", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/mfcs/JethwaniGS20", "doi": "10.4230/lipics.mfcs.2020.53"}}, "content": {"source": {"pdf_hash": "40d83d4d165d43e31f936496cd316dd31689452d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1910.05699v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "fa11f9ad36edd95d776fdf7a25e255a859c9ef5e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/40d83d4d165d43e31f936496cd316dd31689452d.txt", "contents": "\nQuantum-Inspired Classical Algorithms for Singular Value Transformation\n\n\nDhawal Jethwani dhawal.jethwani.cse15@iitbhu.ac.in \nFran\u00e7ois Le Gall legall@math.nagoya-u.ac.jp \nSanjay K Singh \n\nIndian Institute of Technology (BHU)\nVaranasiIndia\n\n\nIndian Institute of Technology (BHU)\nNagoya University\nVaranasiJapan, India\n\nQuantum-Inspired Classical Algorithms for Singular Value Transformation\nand phrases Sampling algorithmsquantum-inspired algorithmslinear algebra\nA recent breakthrough by Tang (STOC 2019) showed how to \"dequantize\" the quantum algorithm for recommendation systems by Kerenidis and Prakash (ITCS 2017). The resulting algorithm, classical but \"quantum-inspired\", efficiently computes a low-rank approximation of the users' preference matrix. Subsequent works have shown how to construct efficient quantum-inspired algorithms for approximating the pseudo-inverse of a low-rank matrix as well, which can be used to (approximately) solve low-rank linear systems of equations. In the present paper, we pursue this line of research and develop quantum-inspired algorithms for a large class of matrix transformations that are defined via the singular value decomposition of the matrix. In particular, we obtain classical algorithms with complexity polynomially related (in most parameters) to the complexity of the best quantum algorithms for singular value transformation recently developed by Chakraborty, Gily\u00e9n and Jeffery (ICALP 2019) and Gily\u00e9n, Su, Low and Wiebe (STOC 2019).ACM Subject ClassificationTheory of computation \u2192 Design and analysis of algorithms algorithm does not explicitly solve the system of equations, but instead enables sampling from the solution, in a very efficient way.There have been several proposals to apply the HHL algorithm (and one of its core components, phase estimation) to linear-algebra based machine learning tasks, leading for instance to the discovery of quantum algorithms for principal component analysis (PCA)[15]and quantum support vector machine[16]. We refer to [3] for a recent survey on this field called quantum machine learning. One of the most convincing applications of quantum algorithms to machine learning has been speeding up recommendation systems[14]. In machine learning, recommendations systems are used to predict the preferences of users. From a mathematical perspective, the core task in recommendation systems can be modeled as follows: given an m \u00d7 n matrix A (representing the preferences of m users) and an index i \u2208 [m] (representing one specific user), sample from the i-th row of a low-rank approximation of A. Kerenidis and Prakash [14]  showed how to adapt the HHL algorithm to solve this problem in time polynomial in log(mn), which was exponentially better than the best known classical algorithms for recommendation systems.Similarly to the HHL algorithm, the quantum algorithm from[14]works only under the assumption that the input is stored in an appropriate structure (called \"Quantum Random-Access Memory\", or \"QRAM\") that allows specific quantum access. Very recently, Tang [18] has shown that assuming that the input is stored in a classical data structure that allows 2 -norm sampling access (i.e., allows sampling rows with probability proportional to their 2 -norm), polylog(mn)-time classical algorithms for recommendation systems can be designed as well. This results eliminates one of the best examples of quantum speedup for machine learning. The paper [18] also introduced the term \"quantum-inspired algorithms\" to refer to such classical algorithms obtained by \"dequantizing\" quantum algorithms.More quantum-inspired algorithms have soon been developed: Tang [17] first showed how to construct classical algorithms for PCA that essentially match the complexity of the quantum algorithm for PCA from [15] mentioned above. Gily\u00e9n, Lloyd and Tang[11] and,independently, Chia, Lin and Wang [6]  have shown how to obtain new classical algorithms for solving linear systems of equations, which also essentially match the complexity of the quantum algorithms when the input matrix has low-rank (see below for details). We also refer to [2] for a discussion of the performance of these quantum-inspired algorithms in practice.Singular value transformation.The Singular Value Decomposition (SVD) of a matrix M \u2208 C m\u00d7n is a factorization of the form M = U \u03a3V * where U \u2208 C m\u00d7m and V \u2208 C n\u00d7n are unitary matrices and \u03a3 is a m \u00d7 n diagonal matrix with min(m, n) non-negative real numbers on the diagonal, where V * denotes the complex-conjugate transpose of V. A crucial property is that this decomposition exists for any complex matrix. Given a function f : R \u22650 \u2192 R \u22650 , the singular value transformation associated with f , denoted \u03a6 f , is the function that maps the matrix M = U \u03a3V * to the matrix \u03a6 f (M ) = U \u03a3 f V * where \u03a3 f is the diagonal matrix obtained from \u03a3 by replacing each diagonal entry \u03c3 by f (\u03c3). We refer to Definition 4 in Section 2 for more details.An important example is obtained by taking the \"pseudo-inverse\" function inv : R \u22650 \u2192 R \u22650 such that inv(x) = 1/x if x > 0 and inv(0) = 0. Solving a linear system of equations Ax = b corresponds 1 to calculating (or approximating) the vector \u03a6 inv (A * )b. If all the singular values of A are between 1/\u03ba and 1, for some value \u03ba, the quantum-inspired algorithms from[6,11]solve this task in time poly (k A , \u03ba, A F , 1/ , log(mn)), where k A denotes the 1 Indeed, one solution is given by x = A + b, where A + represents the Moore-Penrose pseudo-inverse of the matrix A (or simply the inverse when A is invertible). It is easy to check that A + = \u03a6inv(A * ).\n\nIntroduction\n\nBackground. One of the most celebrated quantum algorithms discovered so far is the HHL algorithm [13]. This quantum algorithm solves a system of linear equations of the form Ax = b, where A is an n \u00d7 n matrix and b is an n-dimensional vector, in time polynomial in log n when the matrix A is sufficiently sparse and well-conditioned. This is exponentially better that the best known classical algorithms, which run in time polynomial in n (see also [1,7,8,20] for improvements and relaxations of the assumptions). There are nevertheless two significant caveats. First, the input should be given in a way that allows very specific quantum access. In particular, the HHL algorithm requires the ability to efficiently create a quantum state proportional to b. The second, and main, caveat is that the output of the HHL algorithm is not the solution x of the linear system (which is an n-dimensional vector) but only a O(log n)-qubit quantum state proportional to this vector. While measuring this quantum state can give some meaningful statistics about the solution x, this naturally does not give enough information to obtain the whole vector x. In this perspective, the HHL rank of A, A F denotes the Frobenius norm of A and denotes the approximation error. 2 One crucial point here is that the dependence on the dimensions of the matrix is only poly-logarithmic. Another important point is that the best known quantum algorithms (see [4,11]) enable 2 -norm sampling from the output in time O (\u03ba A F polylog(mn/ )) in the QRAM input model. This means that, except for the dependence in , for low-rank matrices the classical running time is polynomially related to the quantum running time.\n\nThe core computational problem in recommendation systems can also be described as approximating the i-row of the matrix \u03a6 th (A) for the threshold function th : R \u22650 \u2192 R \u22650 such that th(x) = x if x \u2265 \u03c3 and th(x) = 0 otherwise (for some appropriate threshold value \u03c3). This corresponds to approximating the vector \u03a6 th (A * )b where b is the vector with 1 in the i-th coordinate and zero elsewhere. Ref. [18] shows how to solve this problem in time poly ( A F /\u03c3, 1/ , log(mn)). (For the value \u03c3 chosen for recommendation systems, the term A F /\u03c3 becomes an upper bound on the rank of a low-rank approximation of A.)\n\nOur results. In this paper we significantly extend the class of functions for which the singular value transformation can be efficiently computed by \"quantum-inspired\" classical algorithms. The formal and most general statements of our results are given in Section 3. For the sake of readability, in this introduction we only describe our results for a restricted (but still very general) class of \"smooth\" functions. Let R \u22650 and R >0 denote the sets of non-negative numbers and positive numbers, respectively. We say below that a function f : R \u22650 \u2192 R \u22650 is \"smooth\" if f is differentiable in R >0 and the following condition holds: for any \u03b1, \u03b2 \u2265 1, over the interval [1/\u03b1, \u03b2] the maximum values of f and its derivative f can be upper bounded by a polynomial function of \u03b1 and \u03b2. We are mostly interested in functions such that f (0) = 0 since typically we do not want the transformation to increase the rank.\n\nOur main results are the following two theorems (we refer to Section 3 for the formal versions). 3 Theorem 1 (Informal Version). Let f : R \u22650 \u2192 R \u22650 be any smooth function such that f (0) = 0. For any sufficiently small > 0, there exists a classical algorithm that has sampling access to a matrix A \u2208 C m\u00d7n with singular values in [1/\u03ba, 1] and to a non-zero vector b \u2208 C m , receives as input an index i \u2208 [n], outputs with high probability an approximation of the i-th coordinate of the vector \u03a6 f (A * )b with additive error , and has poly (\u03ba, A F , 1/ , log(mn)) time complexity. Theorem 2 (Informal Version). Let f : R \u22650 \u2192 R \u22650 be any smooth function such that f (0) = 0 and f (x) > 0 for all x > 0. For any sufficiently small > 0, there exists a classical algorithm that has sampling access to a matrix A \u2208 C m\u00d7n with singular values in [1/\u03ba, 1] and to a non-zero vector b \u2208 C m , and 2 -samples with high probability from a distribution -close in total variation distance to the distribution associated with the vector \u03a6 f (A * )b, and has poly (\u03ba, A F , 1/ , log(mn)) time complexity.\n\nNote that instead of stating our results for the transformation \u03a6 f (A) we state them for the transformation \u03a6 f (A * ) = (\u03a6 f (A)) * in Theorems 1 and 2. The reason is that this 2 The term log(mn) represents the time complexity of implementing sampling and query operations (see Proposition 6 in Section 2.3), which we also include in the complexity. 3 These informal versions can be derived from the formal versions given in Section 3 by observing that \u03ba2/ A 2 \u2264 \u03ba if all the singular values of A are between 1/\u03ba and 1. The smoothness condition implies that both \u2126 and \u03c6 are upper bounded by a polynomial of \u03ba and A F . Note that for Theorem 2 we actually need an additional smoothness condition expressing that the minimum value of f cannot be too small as well (see the term \u03c9 in the formal version of Theorem 2).\n\nsimplifies the presentation of our algorithms and makes the comparison with prior works easier. Theorems 1 and 2 show that under the same assumptions (namely, sampling access to the input) and similar requirements for the output (i.e., outputting one coordinate of \u03a6 f (A * )b or sampling from the associated distribution) as the prior works on quantum-inspired algorithms, we can efficiently compute classically the singular value transformation for any smooth enough function. This extends the results from [6,11,18] and significantly broadens the applicability of quantum-inspired algorithms.\n\nFast quantum algorithms have been constructed in recent works [4,12] for singular value transformations. For the class of smooth functions we consider, the quantum running time obtained would be O (poly (\u03ba, A F , log(mn/ ))) in the QRAM input model. Our results thus show that except possibly for the dependence on , we can again obtain classical algorithms with running time polynomially related to the quantum running time.\n\nOverview of our approach. We use the same sampling methods as in [2,6,9,11,18]: we first sample r rows from the input matrix A \u2208 C m\u00d7n according to probability proportional to the row norms, which gives (after normalization) a matrix S \u2208 C r\u00d7n . We then do the same with matrix S, this time sampling c columns, which gives (after normalization) a matrix W \u2208 C r\u00d7c . The analysis of this process, which has been done in the seminal work by Frieze, Kannan and Vempala [9], shows that with high probability we have A * A \u2248 S * S and SS * \u2248 W W * when r and c are large enough (but still much smaller than m and n). Since W is a small matrix, we can then afford to compute its SVD.\n\nThe main contribution of this paper is the next step (and its analysis). We show how to use the SVD of the matrix W in order to compute the singular value transformation \u03a6 f . Using the SVD of W , we first compute the matrices \u03a6 inv (W ), \u03a6 inv (W * ) and \u03a6 f (W ). We then compute the matrix\nP = \u03a6 inv (W )\u03a6 f (W * )\u03a6 inv (W )\u03a6 inv (W * ) \u2208 C r\u00d7r .\nThis matrix P is the output of Algorithm 1 presented in Section 3.2. Our central claim is the following:\nS * P SA * \u2248 \u03a6 f (A * ).(1)\nProving (1) and quantifying the quality of the approximation is our main technical contribution. This is done in Proposition 13 (which itself relies on several lemmas proved in Sections 3.1 and 3.2). Finally, using similar post-processing techniques as in prior works [6,18], from the output P of Algorithm 1 we can efficiently approximate coordinates of \u03a6 f (A * )b and sample from \u03a6 f (A * )b. This post-processing is described in Algorithms 2 and 3 in Section 3.3.\n\nWe now give an outline of the main ideas used to establish (1). The basic strategy is to exploit the relations A * A \u2248 S * S and SS * \u2248 W W * mentioned above. Our first insight is to define the function h :\nR \u22650 \u2192 R \u22650 such that h(x) = f ( \u221a x)/ \u221a x if x > 0 and h(0) = 0, and observe that \u03a6 f (A * ) = \u03a6 h (A * A)A * . We then prove, in Lemma 10, that A * A \u2248 S * S implies \u03a6 h (A * A) \u2248 \u03a6 h (S * S).\nThe next natural step would be to relate \u03a6 h (S * S) and \u03a6 h (W * W ), but this cannot be done directly since the only guarantee is SS * \u2248 W W * , and not S * S \u2248 W * W .\nInstead, we observe that \u03a6 h (S * S) = S * P S, where P = \u03a6 inv (S)\u03a6 f (S * )\u03a6 inv (S)\u03a6 inv (S * ). Since \u03a6 inv (S)\u03a6 f (S * ) = \u03a6 h (SS * ) and \u03a6 inv (W )\u03a6 f (W * ) = \u03a6 h (W W * )\n, and since we can show that \u03a6 h (SS * ) is close to \u03a6 h (W W * ) using Lemma 10, we are able to prove that P \u2248 P (this is proved in Lemma 12). To summarize, we have S * P SA * \u2248 S * P SA\n* = \u03a6 h (S * S)A * \u2248 \u03a6 h (A * A)A * = \u03a6 f (A * ), as needed.\nRelated independent work. Independently from our work, Chia, Gily\u00e9n, Li, Lin, Tang and Wang simultaneously derived similar results [5]. They additionally provide general matrix arithmetic primitives for adding and multiplying matrices having sample and query access, and recover known dequantized algorithms. They also show how to use these results on the singular value transformation to obtain new quantum-inspired algorithms for other applications, including Hamiltonian simulation and discriminant analysis.\n\n\nPreliminaries\n\n\nNotations and conventions\n\nGeneral notations. In this paper we use the notation [n] = {1, ....., n} for any integer n \u2265 1. For any set S we denote Conv(S) the convex hull of S. Given a matrix M \u2208 C m\u00d7n , we use M (i,.) \u2208 C 1\u00d7n , M (.,j) \u2208 C m\u00d71 and M (i,j) \u2208 C to denote its i-th row, its j-th column and its (i, j)-th element, respectively. The complexconjugate transpose or Hermitian transpose of a matrix M \u2208 C m\u00d7n (or a vector v \u2208 C n ) is denoted as M * (and v * , respectively). The notations M F and M 2 represent the Frobenius and spectral norm, respectively. Note that M 2 \u2264 M F for any M . For a vector v \u2208 C n , we denote v the 2 norm of the vector. In this paper we will use several times the following standard inequalities that hold for any vector v \u2208 C n and any matrices M \u2208 C n\u00d7m and N \u2208 C m\u00d7p :\nM v \u2264 M 2 v , M N F \u2264 M 2 N F , M N F \u2264 M F N 2 .(2)\nFor a non-zero vector v \u2208 C n , let P v denote the probability distribution on [n] where the probability of choosing i \u2208 [n] is defined as P v (i) = |vi| 2 v 2 . For two vectors v and w, the total variation distance between distributions P v and P w is defined as\nP v \u2212 P w T V = 1 2 n i=1 |P v (i) \u2212 P w (i)|.\nWe will use the following easy inequality (see for instance [6,18] for a proof): for any two vectors v, w \u2208 C n ,\nP v \u2212 P w T V \u2264 2 v \u2212 w v .(3)\nSingular Value Decomposition. The Singular Value Decomposition (SVD) of a matrix M \u2208 C m\u00d7n is a factorization of the form M = U \u03a3V * where U \u2208 C m\u00d7m and V \u2208 C n\u00d7n are unitary matrices and \u03a3 is an m \u00d7 n diagonal matrix with min(m, n) non-negative real numbers, in non-increasing order, down the diagonal. The columns of U and V represent the left and right singular vectors, respectively. Each entry of this diagonal matrix is a singular value of matrix M . A crucial property is that a SVD exists for any complex matrix. We can also write the SVD of a matrix as\nM = U \u03a3V * = min(m,n) i=1 \u03c3 i u i v * i(4)\nwhere {u i } i\u2208 [m] and {v j } j\u2208 [n] are columns of matrices U and V and thus the left and right singular vectors of matrix M , respectively, and \u03c3 i denotes the i-th singular value (the i-th entry of the diagonal matrix \u03a3) for each i \u2208 [min(m, n)]. For any matrix M \u2208 C m\u00d7n , we denote the set of all singular values of M as s(M ). We denote its i-th singular value (in non-increasing order) as \u03c3 i (M ), i.e., the value \u03c3 i in the decomposition of Equation (4). We write \u03c3 max (M ) the largest singular value (i.e., \u03c3 max (M ) = \u03c3 1 (M )), and write \u03c3 min (M ) the smallest non-zero singular value. We define the 2 condition number of M as \u03ba 2 (M ) = \u03c3 max (M )/\u03c3 min (M ) \u2265 1. Note that with this definition, \u03ba 2 is well defined even for singular matrices.\n\nIn this paper, we will use the following inequality by Weyl [19] quite often.\n\nLemma 3 (Weyl's inequality [19]). For two matrices M \u2208 C m\u00d7n , N \u2208 C m\u00d7n and any\ni \u2208 [min(m, n)], |\u03c3 i (M ) \u2212 \u03c3 i (N )| \u2264 M \u2212 N 2 .\n\nSingular Value Transformation.\n\nWe are now ready to introduce the Singular Value Transformation.\n\n\nDefinition 4 (Singular Value Transformation). For any function\nf : R \u22650 \u2192 R \u22650 such that f (0) = 0, the Singular Value Transformation associated to f is the function denoted \u03a6 f that maps any matrix M \u2208 C m\u00d7n to the matrix \u03a6 f (M ) \u2208 C m\u00d7n defined as follows: \u03a6 f (M ) = min(m,n) i=1 f (\u03c3 i )u i v * i ,\nwhere the \u03c3 i 's, the u i 's and the v i 's correspond to the SVD of M given in Eq. (4).\n\nIt is easy to check that the value \u03a6 f (M ) does not depend on the SVD of M chosen in the definition (i.e., it does not depend on which U and which V are chosen). Also note that from our requirement on the function f , the rank (i.e., the number of nonzero singular values) of \u03a6 f (M ) is never larger than the rank of M .\n\nThe \nMoore-Penrose pseudo-inverse of matrix M is the matrix M + = k i=1 \u03c3 \u22121 i v i u * i , where k is\n\n2 -norm sampling\n\nWe now present the assumptions to sample from a matrix and then introduce the technique of 2 -norm sampling that has been used in previous works [2,6,9,11,18].\n\nSample accesses to matrices. Let M \u2208 C m\u00d7n be a matrix. We say that we have sample access to M if the following conditions hold: 1. We can sample from the probability distribution\nR M : [m] \u2192 [0, 1] defined as R M (i) = M (i,.) 2 M 2 F for any i \u2208 [m]. 2. For each i \u2208 [m], we can sample from the probability distribution R i M : [n] \u2192 [0, 1] defined as R i M (j) = |M (i,j) | 2 M (i,.) 2 for any j \u2208 [n]. (Note that R i M is precisely the distribution P u introduced in Section 2, where u is the i-th row of M .)\nWe define sample access to a vector v \u2208 C m using the same definition, by taking the matrix M \u2208 C m\u00d71 that has v as unique row. Note that with this definition, the distribution R M is precisely the distribution P v introduced in Section 2.1.\n\nFor an algorithm handling matrices and vectors using sample accesses, the sample complexity of the algorithm is defined as the total number of samples used by the algorithm.\n\n\n-norm sampling.\n\nLet M \u2208 C m\u00d7n be a matrix for which we have sample access. Consider the following process. For some integer q \u2265 1, sample q row indices p 1 , p 2 , . . . , p q \u2208 [m] using the probability distribution R M and then form the matrix N \u2208 C q\u00d7n by defining\nN (i,.) = M (pi,.) M (pi,.) M F \u221a q for each i \u2208 [q]\n. Note that this corresponds to selecting the rows with indices p 1 , . . . , p q of M and re-normalizing them. We will also use the following fact which is easy to observe using the definition of matrix N :\nN F = M F(5)\nThe central insight of the 2 -norm sampling approach introduced in [9] is that the matrix N obtained by this process is in some sense close enough to M to be able to perform several interesting calculations. We will in particular use the following result that shows that when q is large enough, with high probability the matrix N * N is close to the matrix M * M .\n\nLemma 5 (Lemma 2 in [9]). For any \u03b7 \u2208 (0, 1), any \u03b2 > 0 and for q \u2265 1 \u03b7\u03b2 2 , the inequality\nM * M \u2212 N * N F \u2264 \u03b2 M 2 F holds with probability at least 1 \u2212 \u03b7.\n\nData structures for storing matrices\n\nThe following proposition shows that there exist low over-head data structures that enable sampling access to matrices.\n\n\nProposition 6 ([18]\n\n). There exists a tree-like data structure that stores a matrix M \u2208 C m\u00d7n in O(a log 2 (mn)) space, where a denotes the number of non-zero entries of M , and supports the following operations:\n1) Output M 2 F in O(1) time; 2) Read and update an entry M (i,j) in O(log 2 (mn)) time; 3) Output M (i,.) in O(log 2 (m)) time; 4) Sampling from R M in O(log 2 (mn)) time; 5) For any i \u2208 [m], sampling from R i M in O(log 2 (mn)) time.\nThe data structure of Proposition 6 can naturally be used to store vectors as well. We will need the following two technical lemma in our main algorithms. Lemma 7 shows that a vector-matrix-vector product can be efficiently approximated given sampling access. Lemma 8 states that, given sampling access to k vectors represented by a n \u00d7 k matrix, sampling from their linear combination is possible.\nLemma 7 ([6]).\nLet v \u2208 C m and w \u2208 C n be two vectors and M \u2208 C m\u00d7n be a matrix, all stored in the data structure specified in Proposition 6. Then for any > 0 and \u03b4 > 0, the value v * M w can be approximated with additive error with probability at least 1\u2212\u03b4 in sample \ncomplexity O v 2 w 2 M 2 F 2 log ( 1 \u03b4 ) and time complexity O v 2 w 2 M 2\n\n. Formal Versions and Proofs of the Main Theorems\n\nWe now give the formal versions of Theorems 1 and 2 presented in the introduction. In this section, \u03ba 2 will always denote the 2 condition number of the matrix A. We define the intervals L and Q (which depend on A) as follows:\nL = A 2 \u221a 2\u03ba 2 , A 2 \u221a 2\u03ba 2 (2\u03ba 2 2 + 1) and Q = A 2 2 2\u03ba 2 2 , A 2 2 2\u03ba 2 2 2\u03ba 2 2 + 1 .(6)\nTheorem 1 (Formal Version). Let f : R \u22650 \u2192 R \u22650 be any function such that f (0) = 0. For any \u03b7 > 0 and any sufficiently small 1 > 0, there exists a classical algorithm that has sampling access as in Proposition 6 to a matrix A \u2208 C m\u00d7n and to a non-zero vector b \u2208 C m , receives as input an index i \u2208 [n] and has the following behavior: if f is differentiable on the set L, the algorithm outputs with probability at least 1 \u2212 \u03b7 a value \u03bb such that\n|(\u03a6 f (A * )b) i \u2212 \u03bb| \u2264 1 , using O A 8 F b 4 \u03ba 4 2 4 1 \u03b7 \u03ba 2 A 2 6 \u2126 2 \u03c6 + 3 \u221a 2\u2126 \u03ba 2 A 2 2 polylog mn \u03b7 samples and O A 12 F b 6 2 \u03ba 12 2 6 1 \u03b7 3 \u03c6 + 7 \u221a 2\u2126 \u03ba 2 A 2 6 polylog(mn)\ntime complexity, where \u2126 = max \u03c3\u2208L |f (\u03c3)| and \u03c6 = max \u03c3\u2208L |f (\u03c3)|. Theorems 1 and 2 are stated for a fixed function f and their correctness is guaranteed for matrices A such that f is differentiable on L (remember that L depends on A). Another way of interpreting these theorems is as follows: for a matrix A and vector b (given as inputs), the algorithms of Theorems 1 and 2 work for any function f :\nR \u22650 \u2192 R \u22650 with f (0) = 0 (and f (x) > 0 \u2200 x > 0 for Theorem 2) that is differentiable in the set L.\nSection 3 is organized as follows. Section 3.1 presents a crucial lemma that gives an upper bound on \u03a6 g (X) \u2212 \u03a6 g (Y ) F in terms of X \u2212 Y F , the values of g and the values of its derivative g . In Section 3.2 we present our central procedure, which performs row and column sampling to compute a matrix P \u2208 C r\u00d7c , and analyze this procedure using the lemma proved in Section 3.1. Finally, in Section 3.3 we prove Theorems 1 and 2 by applying appropriate post-processing to the matrix P .\n\n\nBound on the distance between two singular value transformations\n\nThe following lemma uses a result from [10] in order to derive an upper bound on the distance between two singular value transformations of positive semi-definite matrices. The proof can be found in Appendix A.\n\nLemma 10. Let X, Y \u2208 C m\u00d7m be two m \u00d7 m positive semi-definite matrices, and write S = Conv ((s(X) \u222a s(Y )) \\ {0}). For any function g : R \u22650 \u2192 R \u22650 such that g(0) = 0 and g is differentiable in S, we have:\n\u03a6 g (X) \u2212 \u03a6 g (Y ) F \u2264 X \u2212 Y F \u00b7 max \u03c3\u2208S |g (\u03c3)| + g(\u03c3) \u03c3 .\n\nCore procedure\n\nLet us consider Algorithm 1 below. The goal of this subsection is to analyze its behavior.\n\n\nAlgorithm 1 Computing the matrix P .\n\nParameters: Three real numbers \u03b8, \u03b3 \u2208 0, \n\u2208 C r\u00d7c such that W (s,t) = S (s,q t ) S (.,q t ) S F \u221a c = S (s,q t ) S (.,q t ) A F \u221a c , for each (s, t) \u2208 [r]\u00d7[c]\n. Query all the entries of A corresponding to entries of W using operation 2) of Proposition 6. 6: Compute the singular value decomposition of matrix W . 7: Compute the matrix P = \u03a6 inv (W )\u03a6 f (W * )\u03a6 inv (W )\u03a6 inv (W * ) using the output of the SVD step.\n\nThe sampling process of Steps 3-5 is exactly the same as in prior works [2,6,9,11,18], but with different values for c and r. The following lemma analyzes the matrices S and W obtained by this process. The proof, which is the same as in these prior works (but with different values for c and r), can be found in Appendix B.\n\nLemma 11. For any input matrix A and any parameters (\u03b8, \u03b3, \u03b7) in the specified range, with probability at least 1 \u2212 2\u03b7/3 the following statements are simultaneously true for the matrices S and W computed by Algorithm 1:\nS F = A F (7) A * A \u2212 S * S F \u2264 \u03b8 A 2 F ,(8)SS * \u2212 W W * F \u2264 \u03b3 S 2 F ,(9)\u03c3 min (S) > A 2 \u221a 2\u03ba 2 , \u03c3 max (S) < A 2 \u221a 2\u03ba 2 (2\u03ba 2 2 + 1),(10)\u03c3 min (W ) > A 2 \u221a 2\u03ba 2 , \u03c3 max (W ) < A 2 \u221a 2\u03ba 2 (2\u03ba 2 2 + 1).(11)\nLemma 11 above guarantees in particular that with high probability all the nontrivial singular values of the matrix S and W are in the interval L defined in Equation (6).\n\nThe main originality of our approach is Step 7 of Algorithm 1, which we now analyze. Let us define the matrix P = \u03a6 inv (S)\u03a6 f (S * )\u03a6 inv (S)\u03a6 inv (S * ). The following lemma shows that the output P of Algorithm 1 is close to the matrix P . Due to space constraints, here we only give a sketch of the proof. A complete proof can be found in Appendix B. (7)- (11) of Lemma 11 all hold (which happens with probability at least 1 \u2212 2\u03b7/3). Assume that f is differentiable in L and f (0) = 0. Then the matrix P \u2208 C r\u00d7r obtained as the output of Algorithm 1 satisfies the following inequality, where \u2126 = max \u03c3\u2208L |f (\u03c3)| and \u03c6 = max \u03c3\u2208L |f (\u03c3)|.\n\n\nLemma 12. Assume that Statements\nP \u2212 P F \u2264 2\u03b3 A 2 F \u03ba 2 A 2 4 \u03c6 + 7 \u221a 2\u2126 \u03ba 2 A 2 ,(12)\nSketch of the proof. Let us define a function h : (10) and (11) \nR \u22650 \u2192 R \u22650 as follows. For any \u03c3 \u2208 Q we define h(\u03c3) = f ( \u221a \u03c3)inv( \u221a \u03c3) = f ( \u221a \u03c3)/ \u221a \u03c3, we define h(0) = f (0)inv(0) = 0, and we define h(\u03c3) arbitrarily when \u03c3 / \u2208 Q\u222a{0}. Since f is differentiable in L, the function h is differentiable in Q. From Equationswe know that Conv (s(SS * ) \u222a s(W W * ) \\ {0}) \u2282 Q and can write \u03a6 h (SS * ) = \u03a6 inv (S)\u03a6 f (S * ) and \u03a6 h (W W * ) = \u03a6 inv (W )\u03a6 f (W * ).\nUsing the definition of P and P , we now have\nP \u2212 P F = \u03a6 inv (W )\u03a6 f (W * )\u03a6 inv (W )\u03a6 inv (W * ) \u2212 \u03a6 inv (S)\u03a6 f (S * )\u03a6 inv (S)\u03a6 inv (S * ) F = \u03a6 h (W W * )\u03a6 inv (W W * ) \u2212 \u03a6 h (SS * )\u03a6 inv (SS * ) F = {\u03a6 h (W W * ) \u2212 \u03a6 h (SS * )} \u03a6 inv (W W * ) + \u03a6 h (SS * ) {\u03a6 inv (W W * ) \u2212 \u03a6 inv (SS * )} F \u2264 {\u03a6 h (W W * ) \u2212 \u03a6 h (SS * )} \u03a6 inv (W W * ) F + \u03a6 h (SS * ) {\u03a6 inv (W W * ) \u2212 \u03a6 inv (SS * )} F \u2264 \u03a6 inv (W W * ) 2 \u03a6 h (W W * ) \u2212 \u03a6 h (SS * ) F + \u03a6 h (SS * ) 2 \u03a6 inv (W W * ) \u2212 \u03a6 inv (SS * ) F .\nUsing Lemma 10 twice for \u03a6 h and \u03a6 inv , we obtain\nP \u2212 P F \u2264 \u03a6 inv (W W * ) 2 W W * \u2212 SS * F max \u03c3\u2208Q |h (\u03c3)| + h(\u03c3) \u03c3 + \u03a6 h (SS * ) 2 W W * \u2212 SS * F max \u03c3\u2208Q |inv (\u03c3)| + inv(\u03c3) \u03c3 .\nNow we use condition (9). Also, since the non-trivial singular values of SS * and W W * lie in the set Q, the non-trivial singular values of S and W lie in set L (i.e., if \u03c3 \u2208 Q then \u03c3 1/2 \u2208 L). Using this observation, we can then derive the claimed upper bound by routine calculations (omitted here).\n\nThe next proposition is the main result of this subsection. Due to space constrain, the proof has been deferred to Appendix B.\n\n\nProposition 13. Let b \u2208 C m be any non-zero vector and be any positive number such that\n< 1 2 A 2 b \u03c6 + 3 \u221a 2\u2126 \u03ba 2 A 2 .(13)\nLet us fix the parameters of Algorithm 1 as follows:\n\u03b8 = 2 A 2 F \u03ba 2 2 A 2 \u03c6 + 3 \u221a 2\u2126 \u03ba 2 A 2 b \u22121 ,(14)\u03b3 = 2 A 2 F \u03ba 2 2 A 2 \u03c6 + 7 \u221a 2\u2126 \u03ba 2 A 2 b \u22121 .(15)\nThen, under the assumptions of Lemma 12, the two vectors\nx = S * P SA * b and \u03a6 f (A * )b satisfy the inequality x \u2212 \u03a6 f (A * )b \u2264 .\n\nPost-processing and proofs of Theorems 1 and 2\n\nProof of Theorem 1. Let us write\n= 1 4\u2126 r (2\u03ba 2 2 + 1) A 2 \u03ba 2 2 and \u03b4 = \u03b7/3r.(16)\nThe algorithm we consider for estimating the value (\u03a6 f (A * )b) i is described below.\nAlgorithm 2 Estimating (\u03a6 f (A * )b) i 1:\nApply Algorithm 1 with matrix A as input, using the values \u03b8 and \u03b3 given by Equations (14) and (15) with = 1 /2, and using the desired \u03b7 as parameters. This returns a matrix P and a description of a matrix S. 2: Compute an estimation z of the vector SA * b \u2208 C r\u00d71 by estimating, for each j \u2208 [r], the quantity S (j,.) A * b using Lemma 7 with parameters and \u03b4 given by Equation (16). 3: Compute the row vector S * (i,.) \u2208 C 1\u00d7r by querying all the elements in the i-th row of S * (i.e., the i-th column of S). 4: Output the complex number S * (i,.) P z.\n\nWe now analyze Algorithm 2. Let us write x = S * P z \u2208 C n\u00d71 , where P and z are the matrices and the vector computed at Steps 1 and 2 of the algorithm, respectively.\nRemember that P = \u03a6 inv (W )\u03a6 f (W * )\u03a6 inv (W )\u03a6 inv (W * ),\nwhere W is the matrix computed in Algorithm 1. Note that the output of Algorithm 2 is the i-th coordinate of the vector x .\n\nLet us write x = S * P SA * b. From the analysis of Section 3.2, and especially Lemma 11 and Proposition 13, we know that Statements (10) and (11) and the inequality x\u2212\u03a6 f (A * )b \u2264 1 2 simultaneously hold with probability 1 \u2212 2\u03b7/3.\n\nThe vector x then satisfies the inequality\nx \u2212 x \u2264 S * P z \u2212 S * P SA * b \u2264 S * 2 P 2 z \u2212 SA * b \u2264 S * 2 \u03a6 inv (W ) 2 \u03a6 f (W * ) 2 \u03a6 inv (W ) 2 \u03a6 inv (W * ) 2 z \u2212 SA * b \u2264 A 2 \u221a 2\u03ba 2 2\u03ba 2 2 + 1 1/2 \u2126 \u221a 2\u03ba 2 A 2 3 z \u2212 SA * b ,\nwhere we used Statements (10) and (11) and the bound \u03a6 f (W * ) 2 \u2264 \u2126 to derive the last inequality. Lemma 7 now guarantees that with probability at least 1\u2212\u03b7/3 we have z \u2212SA * b \u2264 \u221a r, which implies:\nx \u2212 x \u2264 A 2 \u221a 2\u03ba 2 2\u03ba 2 2 + 1 1/2 \u2126 \u221a 2\u03ba 2 A 2 3 1 4\u2126 (2\u03ba 2 2 + 1) A 2 \u03ba 2 2 = 1 2 .\nIn conclusion, the inequality\nx \u2212 \u03a6 f (A * )b \u2264 x \u2212 x + x \u2212 \u03a6 f (A * )b \u2264 1(17)\nholds with overall probability at least 1 \u2212 \u03b7 for sufficiently small 1 > 0 (a precise upper bound can be derived by using Proposition 13 with = 1 /2).\n\nThis implies that Algorithm 2 outputs, with probability at least 1 \u2212 \u03b7, the i-th coordinate of a vector x that satisfies Equation (17). This proves the correctness of Algorithm 2.\n\nLet us now analyze the complexity of Algorithm 2. Algorithm 1 (and thus Step 1 of Algorithm 2) has time complexity dominated by the computation of the SVD of the matrix W , i.e.,\nO max r 2 c, rc 2 polylog(mn) = O A 12 F b 6 2 \u03ba 12 2 6 1 \u03b7 3 \u03c6 + 7 \u221a 2\u2126 \u03ba 2 A 2 6\npolylog(mn) .\n\nAlgorithm 1 uses r + c samples.\n\nObserve that S (j,.) = A F \u221a r for any j \u2208 [r] (see Step 3 of Algorithm 1).\n\nStep 2 of Algorithm 2 thus uses\nO S (j,.) 2 b 2 A * 2 F 2 polylog mn \u03b4 r = O A 8 F b 4 \u03ba 4 2 4 1 \u03b7 \u03ba 2 A 2 6 \u2126 2 \u03c6 + 3 \u221a 2\u2126 \u03ba 2 A 2 2\npolylog mn \u03b7 samples, and has the same time complexity. Finally, Step 3 of Algorithm 2 has time complexity O(r), while Step 4 has time complexity O(r 2 ). These two steps do not use any sample.\n\nIn conclusion, the time complexity of Algorithm 2 is dominated by Step 1, while the sample complexity is dominated by Step 2.\n\n\nProof sketch of Theorem 2. Let us write\n= 2 \u03c9\u03b1 b 8\u2126 r (2\u03ba 2 2 + 1) A 2 \u03ba 2 2 and \u03b4 = \u03b7/3r,(18)\nwhere \u03b1 is a constant such that the norm of the projection of b on the column space of \u03a6 f (A) is at least \u03b1 b . The algorithm we use to sample from a distribution 2 -close to P \u03a6 f (A * )b is described below.\n\nAlgorithm 3 Sample access to a distribution 2 -close to P \u03a6 f (A * )b 1: Apply Algorithm 1 with matrix A as input, using the values \u03b8 and \u03b3 given by Equations (14) and (15) with = 2\u03c9\u03b1 4 b , and using the desired \u03b7 as parameters. This returns a matrix P and a description of a matrix S. 2: Compute an estimation z of the vector SA * b \u2208 C r\u00d71 by estimating, for each j \u2208 [r], the quantity S (j,.) A * b using Lemma 7 with parameters and \u03b4 given by Equation (18). 3: Compute the vector P z. 4: Use Lemma 8 to output a sample from x = S * P z.\n\nNote that Algorithm 3 is very similar to Algorithm 2: the main modification is Step 4. Also note that we can use Lemma 8 since we have sample access to the columns of S * , from the information obtained at Step 1, and we can compute the vector P z from the information obtained at Steps 1 and 2.\n\nThe complete analyses of the correctness and the complexity of Algorithm 3, which are similar to the analyses done for Algorithm 2 in the proof of Theorem 2, can be found in Appendix C.\n\n\nA Proof of Lemma 10\n\nIn this appendix we give the proof of Lemma 10. Our proof relies on a prior work that established similar bounds for eigenvalue transformations. We first present this result in Part A.1, and then present the proof of Lemma 10 in Part A.2.\n\n\nA.1 Eigenvalue transformations\n\nLet us introduce below another transformation applicable to a diagonalizable matrix M \u2208 C m\u00d7m , i.e., a matrix than can be written as\nM = Q diag(\u03bb 1 , . . . , \u03bb m ) Q \u22121(19)\nfor some invertible matrix Q \u2208 C m\u00d7m where diag(\u03bb 1 , . . . , \u03bb m ) denotes the m\u00d7m diagonal matrix with diagonal entries as m complex numbers \u03bb 1 , . . . , \u03bb m . We write e(M ) = {\u03bb 1 , . . . , \u03bb m }, which is the set of eigenvalues of M .\n\n\nDefinition 14 (Eigenvalue Transformation).\n\nFor any function f : C \u2192 C such that f (0) = 0, the Eigenvalue Transformation associated to f is the function denoted \u03a8 f that maps any diagonalizable matrix M \u2208 C m\u00d7m to the matrix \u03a6 f (M ) \u2208 C m\u00d7m defined as follows:\n\u03a8 f (M ) = Q diag(f (\u03bb 1 ), . . . , f (\u03bb m )) Q \u22121 ,\nwhere Q and \u03bb 1 , . . . , \u03bb m correspond to the decomposition of M given in Eq. (19).\n\nSimilarly to Definition 4, due to our assumption on f the eigenvalue transformation function does not increase the rank of the input matrix.\n\nWe For any function f : C \u2192 C we have\n\u03a8 f (M ) \u2212 \u03a8 f (M ) F \u2264 \u03ba 2 (Q)\u03ba 2 (Q ) M \u2212 M F \u00b7 max j\u2208[m],k\u2208[m] f (\u03bb j ) \u2212 f (\u03bb k ) \u03bb j \u2212 \u03bb k ,\nwhere the convention\nf (\u03bbj )\u2212f (\u03bb k ) \u03bbj \u2212\u03bb k = 0 if \u03bb j = \u03bb k is used.\n\nA.2 Proof of Lemma 10\n\nProof of Lemma 10. For a positive semi-definite matrix the singular values are equal to the eigenvalues and the matrix Q in the decomposition of Equation (19) can be taken as a unitary matrix. For means that for a positive semi-definite matrix, its singular value transformation is equal to its eigenvalue transformation. Note that if Q is unitary then \u03ba 2 (Q) = 1. Using Lemma 15 we thus obtain:\n\u03a6 g (X) \u2212 \u03a6 g (Y ) F = \u03a8 g (X) \u2212 \u03a8 g (Y ) F \u2264 X \u2212 Y F \u00b7 max j\u2208[m],k\u2208[m] g(\u03c3 j ) \u2212 g(\u03c3 k ) \u03c3 j \u2212 \u03c3 k ,\nwhere we write s(X) = {\u03c3 1 , \u03c3 2 , . . . , \u03c3 m } and s(Y ) = {\u03c3 1 , \u03c3 2 , . . . , \u03c3 m }.\n\nFor conciseness, let us write \u03b4 jk =\ng(\u03c3j )\u2212g(\u03c3 k ) \u03c3j \u2212\u03c3 k for any (j, k) \u2208 [m] \u00d7 [m]\n. There are three cases: 1. For any (j, k) such that \u03c3 j = 0 and \u03c3 k = 0 we have \u03b4 jk \u2264 max \u03c3\u2208S |g (\u03c3)|. This happens because g is differentiable in S. Indeed, if we choose values a \u2208 S and b \u2208 S such that a < b, we can always find a value \u03c3 \u2208 [a, b] such that g (\u03c3) = g(b)\u2212g(a)\n\nb\u2212a by then Intermediate Value Theorem. Since this happens for all values of a, b, we obtain \u03b4 jk \u2264 max \u03c3\u2208S |g (\u03c3)|.\n\n\n2.\n\nFor any (j, k) such that \u03c3 j = 0 and \u03c3 k = 0, or \u03c3 j = 0 and \u03c3 k = 0, we have \u03b4 jk \u2264 max \u03c3\u2208S g(\u03c3) \u03c3 ; 3. For any (j, k) such that \u03c3 j = 0 and \u03c3 k = 0 we have \u03b4 jk = 0 (by convention in Lemma 15).\nThen max j\u2208[m],k\u2208[m] {\u03b4 jk } \u2264 max \u03c3\u2208S |g (\u03c3)|, g(\u03c3) \u03c3 .\nTherefore,\n\u03a6 g (X) \u2212 \u03a6 g (Y ) F \u2264 X \u2212 Y F \u00b7 max \u03c3\u2208S |g (\u03c3)| + g(\u03c3) \u03c3 ,\nas claimed.\n\n\nB Proofs of Lemma 11, Lemma 12 and Proposition 13\n\nProof of Lemma 11. Equation (5) suggests Statement (7) always holds. Hence S F = A F . Using Lemma 5 twice, the following two inequalities simultaneously hold for matrices A, S and W in Algorithm 1 with probability at least 1 \u2212 2\u03b7/3:\nA * A \u2212 S * S F \u2264 \u03b8 A 2 F , SS * \u2212 W W * F \u2264 \u03b3 S 2 F\n. Thus with probability at least 1 \u2212 2\u03b7/3, Statements (7), (8) and (9) simultaneously hold. We now show that in this case, Statements (10) and (11) always hold.\n\nUsing Weyl's inequality (Lemma 3) for k = min{rank(A * A), rank(S * S)}, and the above conditions, we have\n|\u03c3 k (S * S) \u2212 \u03c3 k (A * A)| \u2264 A * A \u2212 S * S 2 \u2264 A * A \u2212 S * S F \u2264 \u03b8 A 2 F < A 2 2 4\u03ba 2 2 . Now rank(S * S) \u2264 rank(A * A) (as we sample rows from A). Since \u03c3 min (A * A) = A 2 2 \u03ba 2 2\n(by the definition of \u03ba 2 ), we get\n\u03c3 2 min (S) = \u03c3 min (S * S) > \u03c3 k (A * A) \u2212 A 2 2 4\u03ba 2 2 > \u03c3 min (A * A) \u2212 A 2 2 4\u03ba 2 2 = 3 A 2 2 4\u03ba 2 2 > A 2 2 2\u03ba 2 2 .\nBy a similar argument we get\n\u03c3 2 max (S) = \u03c3 max (S * S) < \u03c3 max (A * A) + A 2 2 4\u03ba 2 2 = A 2 2 1 + 1 4\u03ba 2 2 < A 2 2 2\u03ba 2 2 2\u03ba 2 2 + 1 .\nUsing Weyl's inequality (Lemma 3) again for k = min{rank(SS * ), rank(W W * )} we obtain:\n|\u03c3 k (W W * ) \u2212 \u03c3 k (SS * )| \u2264 SS * \u2212 W W * F \u2264 \u03b3 S 2 F \u2264 A 2 2 4\u03ba 2 2 A 2 F A 2 F < A 2 2 4\u03ba 2 2 . Since \u03c3 min (SS * ) > 3 A 2 2 4\u03ba 2 2\nwe finally obtain the lower bound\n\u03c3 2 min (W ) = \u03c3 min (W W * ) > A 2 2 2\u03ba 2 2 .\nA similar argument gives the upper bound \u03c3 2 max (W ) < A  (10) and (11) \n\u03c3 \u2208 Q we define h(\u03c3) = f ( \u221a \u03c3)inv( \u221a \u03c3) = f ( \u221a \u03c3)/ \u221a \u03c3, we define h(0) = f (0)inv(0) = 0, and we define h(\u03c3) arbitrarily when \u03c3 / \u2208 Q\u222a{0}. Since f is differentiable in L, the function h is differentiable in Q. From Equationswe know that Conv (s(SS * ) \u222a s(W W * ) \\ {0}) \u2282 Q and can write \u03a6 h (SS * ) = \u03a6 inv (S)\u03a6 f (S * ) and \u03a6 h (W W * ) = \u03a6 inv (W )\u03a6 f (W * ).\nUsing the definition of P and P , we now have\nP \u2212 P F = \u03a6 inv (W )\u03a6 f (W * )\u03a6 inv (W )\u03a6 inv (W * ) \u2212 \u03a6 inv (S)\u03a6 f (S * )\u03a6 inv (S)\u03a6 inv (S * ) F = \u03a6 h (W W * )\u03a6 inv (W W * ) \u2212 \u03a6 h (SS * )\u03a6 inv (SS * ) F = {\u03a6 h (W W * ) \u2212 \u03a6 h (SS * )} \u03a6 inv (W W * ) + \u03a6 h (SS * ) {\u03a6 inv (W W * ) \u2212 \u03a6 inv (SS * )} F \u2264 {\u03a6 h (W W * ) \u2212 \u03a6 h (SS * )} \u03a6 inv (W W * ) F + \u03a6 h (SS * ) {\u03a6 inv (W W * ) \u2212 \u03a6 inv (SS * )} F \u2264 \u03a6 inv (W W * ) 2 \u03a6 h (W W * ) \u2212 \u03a6 h (SS * ) F + \u03a6 h (SS * ) 2 \u03a6 inv (W W * ) \u2212 \u03a6 inv (SS * ) F .\nUsing Lemma 10 twice for \u03a6 h and \u03a6 inv , we obtain\nP \u2212 P F \u2264 \u03a6 inv (W W * ) 2 W W * \u2212 SS * F max \u03c3\u2208Q |h (\u03c3)| + h(\u03c3) \u03c3 + \u03a6 h (SS * ) 2 W W * \u2212 SS * F max \u03c3\u2208Q |inv (\u03c3)| + inv(\u03c3) \u03c3 .\nNow using (9) we obtain\nP \u2212 P F \u2264 \u03b3 S 2 F \u03a6 inv (W W * ) 2 max \u03c3\u2208Q |h (\u03c3)| + h(\u03c3) \u03c3 + \u03b3 S 2 F \u03a6 h (SS * ) 2 max \u03c3\u2208Q |inv (\u03c3)| + inv(\u03c3) \u03c3 .\nSince the nontrivial singular values of SS * and W W * lie in the set Q, the nontrivial singular values of S and W lie in set L (i.e., if \u03c3 \u2208 Q then \u03c3 1/2 \u2208 L). We can thus write the above equation as:\nP \u2212 P F \u2264 \u03b3 S 2 F max \u03c3\u2208Q {|inv(\u03c3)|} max \u03c3\u2208Q |h (\u03c3)| + h(\u03c3) \u03c3 + \u03b3 S 2 F max \u03c3\u2208Q {|h(\u03c3)|} max \u03c3\u2208Q |inv (\u03c3)| + inv(\u03c3) \u03c3 .\nBy routine calculation, \u2264 \u2126 max\nmax \u03c3\u2208Q {|h (\u03c3)|} = max \u03c3\u2208Q \u221a \u03c3f ( \u221a \u03c3) \u2212 f ( \u221a \u03c3) 2( \u221a \u03c3) 3 \u2264 max \u03c3\u2208L f (\u03c3) 2\u03c3 2 + f (\u03c3) 2\u03c3 3 ,\u03c3\u2208L 2 |\u03c3| 5 \u2264 2 \u03ba 2 A 2 4 4 \u221a 2\u2126 \u03ba 2 A 2 .\nUsing these inequalities we finally obtain the upper bound\nP \u2212 P F \u2264 \u03b3 A 2 F 2 \u03ba 2 A 2 4 \u03c6 + 7 \u221a 2\u2126 \u03ba 2 A 2 = 2\u03b3 A 2 F \u03ba 2 A 2 4 \u03c6 + 7 \u221a 2\u2126 \u03ba 2 A 2 ,\nas claimed.\n\nProof of Proposition 13. Consider the same function h : R \u22650 \u2192 R \u22650 as in the proof of Lemma 12. Remember that we have \u03a6 h (S * S) = \u03a6 f (S * )\u03a6 inv (S). As discussed in Section 2, we also have \u03a6 inv (S * )S = S * \u03a6 inv (S) = \u03a0 row(S) . We can thus write\nS * P S = S * (\u03a6 inv (S)\u03a6 f (S * )\u03a6 inv (S)\u03a6 inv (S * ))S = \u03a0 row(S) \u03a6 h (S * S)\u03a0 row(S) = \u03a6 h (S * S).\nSimilarly, observe that \u03a6 h (A * A)A * = \u03a6 f (A * )\u03a0 col(A) = \u03a6 f (A * ). We can thus write:\nx \u2212 \u03a6 f (A * )b = S * P SA * b \u2212 (S * P S)A * b + \u03a6 h (S * S)A * b \u2212 \u03a6 h (A * A)A * b \u2264 S * P SA * b \u2212 S * P SA * b + \u03a6 h (S * S)A * b \u2212 \u03a6 h (A * A)A * b \u2264 ( S * 2 P \u2212 P F S 2 + \u03a6 h (A * A) \u2212 \u03a6 h (S * S) F ) A * b .\nUsing Lemma 10 and the definitions of set L and Q in Equation (6), we get\nx \u2212 \u03a6 f (A * )b \u2264 P \u2212 P F max \u03c3\u2208L |\u03c3| 2 A 2 b + \u03b8 A 2 F max \u03c3\u2208Q |h (\u03c3)| + h(\u03c3) \u03c3 A 2 b .\nNow, similarly to the proof of Lemma 12, we have\nmax \u03c3\u2208Q |h (\u03c3)| + h(\u03c3) \u03c3 \u2264 \u03ba 2 A 2 2 \u03c6 + 3 \u221a 2\u2126 \u03ba 2 A 2 .\nUsing this inequality and Equation (12), we get\nx \u2212 \u03a6 f (A * )b \u2264 2\u03b3 A 2 F \u03ba 2 A 2 4 \u03c6 + 7 \u221a 2\u2126 \u03ba 2 A 2 A 2 2 2\u03ba 2 2 A 2 b + \u03b8 A 2 F \u03ba 2 A 2 2 \u03c6 + 3 \u221a 2\u2126 \u03ba 2 A 2 A 2 b \u2264 \u03b3 A 2 F \u03ba 2 2 A 2 \u03c6 + 7 \u221a 2\u2126 \u03ba 2 A 2 + \u03b8 A 2 F \u03ba 2 2 A 2 \u03c6 + 3 \u221a 2\u2126 \u03ba 2 A 2 b = 2 + 2 = .\nThus we obtain x \u2212 \u03a6 f (A * )b \u2264 when choosing the values for \u03b3 and \u03b8 in the statement of the lemma (straightforward calculations show that for satisfying Inequality (13) these values are in the ranges allowed for the parameters \u03b3 and \u03b8 in Algorithm 1).\n\n\nC Complete analysis of Algorithm 3\n\nLet us first show the correctness of Algorithm 3. Similarly to the analysis done in Theorem 1, with probability at least 1 \u2212 \u03b7, the vector x satisfies\nx \u2212 \u03a6 f (A * )b \u2264 2 \u03c9\u03b1 2 b .\nRemember that the norm of the projection of b on the column space of \u03a6 f (A) is at least \u03b1 b . Consider b = m i=1 b i u i , where u i are the left singular vectors of matrix \u03a6 f (A) \u2208 C m\u00d7n , where k is the rank of this matrix. So the following inequality holds:\n\u03a6 f (A * )b 2 = min (m,n) i=1 |b i f (\u03c3 i (A))| 2 \u2265 k i=1 |b i f (\u03c3 i (A))| 2 \u2265 min i\u2208[k] f (\u03c3 i (A)) 2 k i=1 |b i | 2 \u2265 \u03c9 2 \u03b1 2 b 2 .(20)\nThus using Inequality (3) and (20), the following inequality is true\nP x \u2212 P \u03a6 f (A * )b T V \u2264 2 \u03c9\u03b1 b \u03a6 f (A * )b \u2264 2 .\nLet us now analyze the complexity of Algorithm 3.\n\nStep 4 in Algorithm 3 uses Lemma 8 and has O(r 2 C(S * , P z)) sample complexity and O(r 2 C(S * , P z) log 2 (nr)) time complexity, where C(S * , P z) = \n\n\nthe rank of the matrix M . Note that we only consider non-trivial singular values of the matrix. As in the introduction, we define the inverse function inv : R \u22650 \u2192 R \u22650 such that inv(0) = 0 and inv(x) = 1/x for x > 0. Then we have \u03a6 inv (M * ) = M + . Note that M M + = M \u03a6 inv (M * ) = \u03a0 col(M ) and M + M = \u03a6 inv (M * )M = \u03a0 row(M ) , where \u03a0 col(M ) denotes the orthogonal projector into the column space of M and \u03a0 row(M ) denotes the orthogonal projector into the row space of M .\n\n.\nLemma 8 ([18]). Let M \u2208 C n\u00d7k be a matrix stored in the data structure specified in Proposition 6. Let v \u2208 C k be an input vector. Then a sample from M v can be obtained in expected sample complexity O k 2 C(M, v) and expected time complexity O k 2 C(M, v) log 2 (nk) , where C(M, v) = k i=1 viM (.,i) 2 M v 2\n\nTheorem 2 (\n2Formal Version). Let f : R \u22650 \u2192 R \u22650 be any function such that f (0) = 0 and f (x) > 0 for all x > 0. For any \u03b7 > 0 and any sufficiently small 2 > 0, there exists a classical algorithm that has sampling access as in Proposition 6 to a matrix A \u2208 C m\u00d7n and to a non-zero vector b \u2208 C m and has the following behavior: if f is differentiable on the set L and the projection of b on the column space of \u03a6 f (A * ) has norm \u2126( b ), with probability at least 1 \u2212 \u03b7 the algorithm samples from a distribution which is 2 -close in total variation distance to the distribution P \u03a6 f (A * )b , using complexity, where \u2126 = max \u03c3\u2208L |f (\u03c3)|, \u03c6 = max \u03c3\u2208L |f (\u03c3)| and \u03c9 = min \u03c3\u2208L |f (\u03c3)|.\n\n\nInput:A \u2208 C m\u00d7n stored in the data structure specified in Proposition 6 1: Set r = 3/(\u03b7\u03b8 2 ) .2: Set c = 3/(\u03b7\u03b3 2 ) . 3: Sample r row indices p 1 ,...., p r using operation 4) of Proposition 6. Let S \u2208 C r\u00d7n be the matrix whose s-th row is S (s,.) = A (ps,.) A (ps,.) A F \u221a r , for each s \u2208 [r]. 4: Sample c column indices q 1 ,...., q c by repeating the following procedure c times: sample a row index s \u2208 [r] uniformly at random and then sample a column index q \u2208 [n] with probability |S (s,q) | 2 S (s,.) 2 = |A (ps,q) | 2 A (ps,.) 2 using operation 5) of Proposition 6. 5: Define the matrix W\n\n\nwill later use the following upper bound on the norm of \u03a8 f (M ) \u2212 \u03a8 f (M ) F for diagonalizable matrices M and M from [10]. Lemma 15 (Corollary 2.3 in [10]). Let M and M be m \u00d7 m diagonalizable matrices with decompositions M = Q diag(\u03bb 1 , . . . , \u03bb m ) Q \u22121 , M = Q diag(\u03bb 1 , . . . , \u03bb m ) Q \u22121 .\n\n.\nProof of Lemma 12. Let us define a function h : R \u22650 \u2192 R \u22650 as follows. For any\n\n\nr i=1 (P z) i S (.,i) 2 S * P z 2 \u2264 r i=1 |(P z) i | S (.,i) 2 S * P z 2 .\n\nVariable time amplitude amplification and quantum algorithms for linear algebra problems. Andris Ambainis, Proceedings of the 29th International Symposium on Theoretical Aspects of Computer Science. the 29th International Symposium on Theoretical Aspects of Computer ScienceAndris Ambainis. Variable time amplitude amplification and quantum algorithms for linear algebra problems. In Proceedings of the 29th International Symposium on Theoretical Aspects of Computer Science, pages 636-647, 2012.\n\nJuan Miguel Arrazola, Alain Delgado, Seth Bhaskar Roy Bardhan, Lloyd, arXiv:1905.10415Quantuminspired algorithms in practice. Juan Miguel Arrazola, Alain Delgado, Bhaskar Roy Bardhan, and Seth Lloyd. Quantum- inspired algorithms in practice. arXiv:1905.10415, 2019.\n\nQuantum machine learning. Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, Seth Lloyd, Nature. 549Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd. Quantum machine learning. Nature, 549:195-202, 2017.\n\nThe power of block-encoded matrix powers: Improved regression techniques via faster Hamiltonian simulation. Shantanav Chakraborty, Andr\u00e1s Gily\u00e9n, Stacey Jeffery, Proceeding of the 46th International Colloquium on Automata, Languages, and Programming. eeding of the 46th International Colloquium on Automata, Languages, and Programming33Shantanav Chakraborty, Andr\u00e1s Gily\u00e9n, and Stacey Jeffery. The power of block-encoded matrix powers: Improved regression techniques via faster Hamiltonian simulation. In Proceeding of the 46th International Colloquium on Automata, Languages, and Programming, pages 33:1-33:14, 2019.\n\nSampling-based sublinear low-rank matrix arithmetic framework for dequantizing quantum machine learning. Nai-Hui Chia, Andr\u00e1s Gily\u00e9n, Tongyang Li, Han-Hsuan Lin, Ewin Tang, Chunhao Wang, arXiv:1910.06151Proceedings of the 52nd Annual ACM Symposium on Theory of Computing. the 52nd Annual ACM Symposium on Theory of Computingto appearNai-Hui Chia, Andr\u00e1s Gily\u00e9n, Tongyang Li, Han-Hsuan Lin, Ewin Tang, and Chunhao Wang. Sampling-based sublinear low-rank matrix arithmetic framework for dequantizing quantum machine learning. In Proceedings of the 52nd Annual ACM Symposium on Theory of Computing, to appear, 2020. arXiv:1910.06151.\n\nQuantum-inspired sublinear classical algorithms for solving low-rank linear systems. Nai-Hui Chia, Han-Hsuan Lin, Chunhao Wang, arXiv:1811.04852Nai-Hui Chia, Han-Hsuan Lin, and Chunhao Wang. Quantum-inspired sublinear classical algorithms for solving low-rank linear systems. arXiv:1811.04852, 2018.\n\nQuantum algorithm for systems of linear equations with exponentially improved dependence on precision. Andrew M Childs, Robin Kothari, Rolando D Somma, SIAM Journal on Computing. 466Andrew M. Childs, Robin Kothari, and Rolando D. Somma. Quantum algorithm for systems of linear equations with exponentially improved dependence on precision. SIAM Journal on Computing, 46(6):1920-1950, 2017.\n\nPreconditioned quantum linear system algorithm. B , David Clader, Bryan C Jacobs, Chad R Sprouse, Physical Review Letters. 110250504B. David Clader, Bryan C. Jacobs, and Chad R. Sprouse. Preconditioned quantum linear system algorithm. Physical Review Letters, 110:250504, 2013.\n\nFast Monte-Carlo algorithms for finding low-rank approximations. Alan Frieze, Ravi Kannan, Santosh Vempala, Journal of the ACM. 516Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast Monte-Carlo algorithms for finding low-rank approximations. Journal of the ACM, 51(6):1025-1041, 2004.\n\nPerturbations of functions of diagonalizable matrices. Michael I Gil, Electronic Journal of Linear Algebra. 271645Michael I. Gil. Perturbations of functions of diagonalizable matrices. Electronic Journal of Linear Algebra, 27(1):645, 2014.\n\nQuantum-inspired low-rank stochastic regression with logarithmic dependence on the dimension. Andr\u00e1s Gily\u00e9n, Seth Lloyd, Ewin Tang, arXiv:1811.04909Andr\u00e1s Gily\u00e9n, Seth Lloyd, and Ewin Tang. Quantum-inspired low-rank stochastic regression with logarithmic dependence on the dimension. arXiv:1811.04909, 2018.\n\nQuantum singular value transformation and beyond: exponential improvements for quantum matrix arithmetics. Andr\u00e1s Gily\u00e9n, Yuan Su, Guang Hao Low, Nathan Wiebe, Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing. the 51st Annual ACM SIGACT Symposium on Theory of ComputingAndr\u00e1s Gily\u00e9n, Yuan Su, Guang Hao Low, and Nathan Wiebe. Quantum singular value transformation and beyond: exponential improvements for quantum matrix arithmetics. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 193-204, 2019.\n\nQuantum algorithm for solving linear systems of equations. Aram W Harrow, Avinatan Hassidim, Seth Lloyd, Physical Review Letters. 15103150502Aram W. Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum algorithm for solving linear systems of equations. Physical Review Letters, 15(103):150502, 2009.\n\nQuantum recommendation systems. Iordanis Kerenidis, Anupam Prakash, Proceedings of the 8th Innovations in Theoretical Computer Science Conference. the 8th Innovations in Theoretical Computer Science Conference49Iordanis Kerenidis and Anupam Prakash. Quantum recommendation systems. In Proceedings of the 8th Innovations in Theoretical Computer Science Conference, pages 49:1-49:21, 2017.\n\nQuantum principal component analysis. Seth Lloyd, Masoud Mohseni, Patrick Rebentrost, Nature Physics. 10Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quantum principal component analysis. Nature Physics, 10:631-633, 2014.\n\nQuantum support vector machine for big data classification. Patrick Rebentrost, Masoud Mohseni, Seth Lloyd, Physical review letters. 11313130503Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd. Quantum support vector machine for big data classification. Physical review letters, 113(13):130503, 2014.\n\nQuantum-inspired classical algorithms for principal component analysis and supervised clustering. Ewin Tang, arXiv:1811.00414Ewin Tang. Quantum-inspired classical algorithms for principal component analysis and supervised clustering. arXiv:1811.00414, 2018.\n\nA quantum-inspired classical algorithm for recommendation systems. Ewin Tang, Proceedings of the 51st Annual Symposium on Theory of Computing. the 51st Annual Symposium on Theory of ComputingEwin Tang. A quantum-inspired classical algorithm for recommendation systems. In Proceedings of the 51st Annual Symposium on Theory of Computing, pages 217-228, 2019.\n\nDas asymptotische verteilungsgesetz der eigenwerte linearer partieller differentialgleichungen (mit einer anwendung auf die theorie der hohlraumstrahlung). Hermann Weyl, Mathematische Annalen. 714Hermann Weyl. Das asymptotische verteilungsgesetz der eigenwerte linearer partieller differen- tialgleichungen (mit einer anwendung auf die theorie der hohlraumstrahlung). Mathematische Annalen, 71(4):441-479, 1912.\n\nQuantum linear system algorithm for dense matrices. Leonard Wossnig, Zhikuan Zhao, Anupam Prakash, Physical Review Letters. 12050502Leonard Wossnig, Zhikuan Zhao, and Anupam Prakash. Quantum linear system algorithm for dense matrices. Physical Review Letters, 120:050502, 2018.\n", "annotations": {"author": "[{\"end\":126,\"start\":75},{\"end\":171,\"start\":127},{\"end\":187,\"start\":172},{\"end\":240,\"start\":188},{\"end\":318,\"start\":241}]", "publisher": null, "author_last_name": "[{\"end\":90,\"start\":82},{\"end\":143,\"start\":136},{\"end\":186,\"start\":181}]", "author_first_name": "[{\"end\":81,\"start\":75},{\"end\":135,\"start\":127},{\"end\":178,\"start\":172},{\"end\":180,\"start\":179}]", "author_affiliation": "[{\"end\":239,\"start\":189},{\"end\":317,\"start\":242}]", "title": "[{\"end\":72,\"start\":1},{\"end\":390,\"start\":319}]", "venue": null, "abstract": "[{\"end\":5624,\"start\":464}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5741,\"start\":5737},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6092,\"start\":6089},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6094,\"start\":6092},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6096,\"start\":6094},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6099,\"start\":6096},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6898,\"start\":6897},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7077,\"start\":7074},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7080,\"start\":7077},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7737,\"start\":7733},{\"end\":8626,\"start\":8618},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8959,\"start\":8958},{\"end\":9200,\"start\":9192},{\"end\":9712,\"start\":9704},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10135,\"start\":10134},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10308,\"start\":10307},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11286,\"start\":11283},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11289,\"start\":11286},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11292,\"start\":11289},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11436,\"start\":11433},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11439,\"start\":11436},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11866,\"start\":11863},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11868,\"start\":11866},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11870,\"start\":11868},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11873,\"start\":11870},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11876,\"start\":11873},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12267,\"start\":12264},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12971,\"start\":12968},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13231,\"start\":13228},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13234,\"start\":13231},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13491,\"start\":13488},{\"end\":14324,\"start\":14315},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14565,\"start\":14562},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16201,\"start\":16198},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16204,\"start\":16201},{\"end\":16907,\"start\":16904},{\"end\":16925,\"start\":16922},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17714,\"start\":17710},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17760,\"start\":17756},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18948,\"start\":18945},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18950,\"start\":18948},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18952,\"start\":18950},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18955,\"start\":18952},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18958,\"start\":18955},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20507,\"start\":20504},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20826,\"start\":20823},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24422,\"start\":24418},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25499,\"start\":25496},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25501,\"start\":25499},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25503,\"start\":25501},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25506,\"start\":25503},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25509,\"start\":25506},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26710,\"start\":26706},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28236,\"start\":28233},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29409,\"start\":29405},{\"end\":29530,\"start\":29528},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":29702,\"start\":29698},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29831,\"start\":29830},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30375,\"start\":30371},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30727,\"start\":30723},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31341,\"start\":31337},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32701,\"start\":32697},{\"end\":32826,\"start\":32824},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32998,\"start\":32994},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":33028,\"start\":33027},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34676,\"start\":34672},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":35211,\"start\":35207},{\"end\":36326,\"start\":36317},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":36870,\"start\":36867}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":42657,\"start\":42169},{\"attributes\":{\"id\":\"fig_1\"},\"end\":42970,\"start\":42658},{\"attributes\":{\"id\":\"fig_2\"},\"end\":43658,\"start\":42971},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44256,\"start\":43659},{\"attributes\":{\"id\":\"fig_4\"},\"end\":44558,\"start\":44257},{\"attributes\":{\"id\":\"fig_5\"},\"end\":44641,\"start\":44559},{\"attributes\":{\"id\":\"fig_6\"},\"end\":44718,\"start\":44642}]", "paragraph": "[{\"end\":7328,\"start\":5640},{\"end\":7945,\"start\":7330},{\"end\":8859,\"start\":7947},{\"end\":9953,\"start\":8861},{\"end\":10772,\"start\":9955},{\"end\":11369,\"start\":10774},{\"end\":11796,\"start\":11371},{\"end\":12475,\"start\":11798},{\"end\":12769,\"start\":12477},{\"end\":12931,\"start\":12827},{\"end\":13427,\"start\":12960},{\"end\":13635,\"start\":13429},{\"end\":14001,\"start\":13831},{\"end\":14369,\"start\":14182},{\"end\":14942,\"start\":14431},{\"end\":15773,\"start\":14988},{\"end\":16090,\"start\":15827},{\"end\":16251,\"start\":16138},{\"end\":16844,\"start\":16283},{\"end\":17648,\"start\":16888},{\"end\":17727,\"start\":17650},{\"end\":17809,\"start\":17729},{\"end\":17958,\"start\":17894},{\"end\":18353,\"start\":18265},{\"end\":18677,\"start\":18355},{\"end\":18683,\"start\":18679},{\"end\":18959,\"start\":18800},{\"end\":19140,\"start\":18961},{\"end\":19716,\"start\":19475},{\"end\":19891,\"start\":19718},{\"end\":20162,\"start\":19911},{\"end\":20423,\"start\":20216},{\"end\":20801,\"start\":20437},{\"end\":20894,\"start\":20803},{\"end\":21118,\"start\":20999},{\"end\":21334,\"start\":21142},{\"end\":21969,\"start\":21571},{\"end\":22238,\"start\":21985},{\"end\":22592,\"start\":22366},{\"end\":23133,\"start\":22686},{\"end\":23717,\"start\":23315},{\"end\":24310,\"start\":23820},{\"end\":24589,\"start\":24379},{\"end\":24797,\"start\":24591},{\"end\":24965,\"start\":24875},{\"end\":25047,\"start\":25006},{\"end\":25422,\"start\":25166},{\"end\":25747,\"start\":25424},{\"end\":25968,\"start\":25749},{\"end\":26345,\"start\":26175},{\"end\":26986,\"start\":26347},{\"end\":27140,\"start\":27076},{\"end\":27584,\"start\":27539},{\"end\":28082,\"start\":28032},{\"end\":28513,\"start\":28212},{\"end\":28641,\"start\":28515},{\"end\":28821,\"start\":28769},{\"end\":28981,\"start\":28925},{\"end\":29139,\"start\":29107},{\"end\":29276,\"start\":29190},{\"end\":29873,\"start\":29319},{\"end\":30041,\"start\":29875},{\"end\":30227,\"start\":30104},{\"end\":30461,\"start\":30229},{\"end\":30505,\"start\":30463},{\"end\":30889,\"start\":30689},{\"end\":31004,\"start\":30975},{\"end\":31205,\"start\":31055},{\"end\":31386,\"start\":31207},{\"end\":31566,\"start\":31388},{\"end\":31663,\"start\":31650},{\"end\":31696,\"start\":31665},{\"end\":31773,\"start\":31698},{\"end\":31806,\"start\":31775},{\"end\":32102,\"start\":31909},{\"end\":32229,\"start\":32104},{\"end\":32536,\"start\":32327},{\"end\":33078,\"start\":32538},{\"end\":33375,\"start\":33080},{\"end\":33562,\"start\":33377},{\"end\":33824,\"start\":33586},{\"end\":33992,\"start\":33859},{\"end\":34273,\"start\":34033},{\"end\":34538,\"start\":34320},{\"end\":34677,\"start\":34592},{\"end\":34819,\"start\":34679},{\"end\":34858,\"start\":34821},{\"end\":34977,\"start\":34957},{\"end\":35449,\"start\":35053},{\"end\":35640,\"start\":35552},{\"end\":35678,\"start\":35642},{\"end\":36007,\"start\":35729},{\"end\":36125,\"start\":36009},{\"end\":36327,\"start\":36132},{\"end\":36395,\"start\":36385},{\"end\":36467,\"start\":36456},{\"end\":36754,\"start\":36521},{\"end\":36968,\"start\":36808},{\"end\":37076,\"start\":36970},{\"end\":37295,\"start\":37260},{\"end\":37446,\"start\":37418},{\"end\":37644,\"start\":37555},{\"end\":37815,\"start\":37782},{\"end\":37936,\"start\":37863},{\"end\":38348,\"start\":38303},{\"end\":38846,\"start\":38796},{\"end\":38999,\"start\":38976},{\"end\":39316,\"start\":39115},{\"end\":39468,\"start\":39437},{\"end\":39666,\"start\":39608},{\"end\":39769,\"start\":39758},{\"end\":40025,\"start\":39771},{\"end\":40222,\"start\":40130},{\"end\":40512,\"start\":40439},{\"end\":40650,\"start\":40602},{\"end\":40756,\"start\":40709},{\"end\":41222,\"start\":40969},{\"end\":41411,\"start\":41261},{\"end\":41703,\"start\":41441},{\"end\":41911,\"start\":41843},{\"end\":42012,\"start\":41963},{\"end\":42168,\"start\":42014}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12826,\"start\":12770},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12959,\"start\":12932},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13830,\"start\":13636},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14181,\"start\":14002},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14430,\"start\":14370},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15826,\"start\":15774},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16137,\"start\":16091},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16282,\"start\":16252},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16887,\"start\":16845},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17860,\"start\":17810},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18264,\"start\":18024},{\"attributes\":{\"id\":\"formula_11\"},\"end\":18780,\"start\":18684},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19474,\"start\":19141},{\"attributes\":{\"id\":\"formula_13\"},\"end\":20215,\"start\":20163},{\"attributes\":{\"id\":\"formula_14\"},\"end\":20436,\"start\":20424},{\"attributes\":{\"id\":\"formula_15\"},\"end\":20959,\"start\":20895},{\"attributes\":{\"id\":\"formula_16\"},\"end\":21570,\"start\":21335},{\"attributes\":{\"id\":\"formula_17\"},\"end\":21984,\"start\":21970},{\"attributes\":{\"id\":\"formula_18\"},\"end\":22313,\"start\":22239},{\"attributes\":{\"id\":\"formula_19\"},\"end\":22685,\"start\":22593},{\"attributes\":{\"id\":\"formula_20\"},\"end\":23314,\"start\":23134},{\"attributes\":{\"id\":\"formula_21\"},\"end\":23819,\"start\":23718},{\"attributes\":{\"id\":\"formula_22\"},\"end\":24857,\"start\":24798},{\"attributes\":{\"id\":\"formula_23\"},\"end\":25165,\"start\":25048},{\"attributes\":{\"id\":\"formula_24\"},\"end\":26013,\"start\":25969},{\"attributes\":{\"id\":\"formula_25\"},\"end\":26042,\"start\":26013},{\"attributes\":{\"id\":\"formula_26\"},\"end\":26107,\"start\":26042},{\"attributes\":{\"id\":\"formula_27\"},\"end\":26174,\"start\":26107},{\"attributes\":{\"id\":\"formula_28\"},\"end\":27075,\"start\":27022},{\"attributes\":{\"id\":\"formula_29\"},\"end\":27399,\"start\":27141},{\"attributes\":{\"id\":\"formula_30\"},\"end\":27538,\"start\":27399},{\"attributes\":{\"id\":\"formula_31\"},\"end\":28031,\"start\":27585},{\"attributes\":{\"id\":\"formula_32\"},\"end\":28211,\"start\":28083},{\"attributes\":{\"id\":\"formula_33\"},\"end\":28768,\"start\":28732},{\"attributes\":{\"id\":\"formula_34\"},\"end\":28873,\"start\":28822},{\"attributes\":{\"id\":\"formula_35\"},\"end\":28924,\"start\":28873},{\"attributes\":{\"id\":\"formula_36\"},\"end\":29057,\"start\":28982},{\"attributes\":{\"id\":\"formula_37\"},\"end\":29189,\"start\":29140},{\"attributes\":{\"id\":\"formula_38\"},\"end\":29318,\"start\":29277},{\"attributes\":{\"id\":\"formula_39\"},\"end\":30103,\"start\":30042},{\"attributes\":{\"id\":\"formula_40\"},\"end\":30688,\"start\":30506},{\"attributes\":{\"id\":\"formula_41\"},\"end\":30974,\"start\":30890},{\"attributes\":{\"id\":\"formula_42\"},\"end\":31054,\"start\":31005},{\"attributes\":{\"id\":\"formula_43\"},\"end\":31649,\"start\":31567},{\"attributes\":{\"id\":\"formula_44\"},\"end\":31908,\"start\":31807},{\"attributes\":{\"id\":\"formula_45\"},\"end\":32326,\"start\":32272},{\"attributes\":{\"id\":\"formula_46\"},\"end\":34032,\"start\":33993},{\"attributes\":{\"id\":\"formula_47\"},\"end\":34591,\"start\":34539},{\"attributes\":{\"id\":\"formula_48\"},\"end\":34956,\"start\":34859},{\"attributes\":{\"id\":\"formula_49\"},\"end\":35028,\"start\":34978},{\"attributes\":{\"id\":\"formula_50\"},\"end\":35551,\"start\":35450},{\"attributes\":{\"id\":\"formula_51\"},\"end\":35728,\"start\":35679},{\"attributes\":{\"id\":\"formula_52\"},\"end\":36384,\"start\":36328},{\"attributes\":{\"id\":\"formula_53\"},\"end\":36455,\"start\":36396},{\"attributes\":{\"id\":\"formula_54\"},\"end\":36807,\"start\":36755},{\"attributes\":{\"id\":\"formula_55\"},\"end\":37259,\"start\":37077},{\"attributes\":{\"id\":\"formula_56\"},\"end\":37417,\"start\":37296},{\"attributes\":{\"id\":\"formula_57\"},\"end\":37554,\"start\":37447},{\"attributes\":{\"id\":\"formula_58\"},\"end\":37781,\"start\":37645},{\"attributes\":{\"id\":\"formula_59\"},\"end\":37862,\"start\":37816},{\"attributes\":{\"id\":\"formula_60\"},\"end\":38163,\"start\":37937},{\"attributes\":{\"id\":\"formula_61\"},\"end\":38302,\"start\":38163},{\"attributes\":{\"id\":\"formula_62\"},\"end\":38795,\"start\":38349},{\"attributes\":{\"id\":\"formula_63\"},\"end\":38975,\"start\":38847},{\"attributes\":{\"id\":\"formula_64\"},\"end\":39114,\"start\":39000},{\"attributes\":{\"id\":\"formula_65\"},\"end\":39436,\"start\":39317},{\"attributes\":{\"id\":\"formula_66\"},\"end\":39565,\"start\":39469},{\"attributes\":{\"id\":\"formula_67\"},\"end\":39607,\"start\":39565},{\"attributes\":{\"id\":\"formula_68\"},\"end\":39757,\"start\":39667},{\"attributes\":{\"id\":\"formula_69\"},\"end\":40129,\"start\":40026},{\"attributes\":{\"id\":\"formula_70\"},\"end\":40438,\"start\":40223},{\"attributes\":{\"id\":\"formula_71\"},\"end\":40601,\"start\":40513},{\"attributes\":{\"id\":\"formula_72\"},\"end\":40708,\"start\":40651},{\"attributes\":{\"id\":\"formula_73\"},\"end\":40968,\"start\":40757},{\"attributes\":{\"id\":\"formula_74\"},\"end\":41440,\"start\":41412},{\"attributes\":{\"id\":\"formula_75\"},\"end\":41842,\"start\":41704},{\"attributes\":{\"id\":\"formula_76\"},\"end\":41962,\"start\":41912}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":5638,\"start\":5626},{\"attributes\":{\"n\":\"2\"},\"end\":14958,\"start\":14945},{\"attributes\":{\"n\":\"2.1\"},\"end\":14986,\"start\":14961},{\"end\":17892,\"start\":17862},{\"end\":18023,\"start\":17961},{\"attributes\":{\"n\":\"2.2\"},\"end\":18798,\"start\":18782},{\"attributes\":{\"n\":\"2\"},\"end\":19909,\"start\":19894},{\"attributes\":{\"n\":\"2.3\"},\"end\":20997,\"start\":20961},{\"end\":21140,\"start\":21121},{\"end\":22364,\"start\":22315},{\"attributes\":{\"n\":\"3.1\"},\"end\":24377,\"start\":24313},{\"attributes\":{\"n\":\"3.2\"},\"end\":24873,\"start\":24859},{\"end\":25004,\"start\":24968},{\"end\":27021,\"start\":26989},{\"end\":28731,\"start\":28644},{\"attributes\":{\"n\":\"3.3\"},\"end\":29105,\"start\":29059},{\"end\":32271,\"start\":32232},{\"end\":33584,\"start\":33565},{\"end\":33857,\"start\":33827},{\"end\":34318,\"start\":34276},{\"end\":35051,\"start\":35030},{\"end\":36130,\"start\":36128},{\"end\":36519,\"start\":36470},{\"end\":41259,\"start\":41225},{\"end\":42660,\"start\":42659},{\"end\":42983,\"start\":42972},{\"end\":44561,\"start\":44560}]", "table": null, "figure_caption": "[{\"end\":42657,\"start\":42171},{\"end\":42970,\"start\":42661},{\"end\":43658,\"start\":42985},{\"end\":44256,\"start\":43661},{\"end\":44558,\"start\":44259},{\"end\":44641,\"start\":44562},{\"end\":44718,\"start\":44644}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":44816,\"start\":44810},{\"end\":45222,\"start\":45218},{\"end\":45245,\"start\":45240},{\"end\":45259,\"start\":45255},{\"end\":45516,\"start\":45511},{\"end\":45532,\"start\":45527},{\"end\":45547,\"start\":45541},{\"end\":45565,\"start\":45558},{\"end\":45584,\"start\":45578},{\"end\":45596,\"start\":45592},{\"end\":45883,\"start\":45874},{\"end\":45903,\"start\":45897},{\"end\":45918,\"start\":45912},{\"end\":46497,\"start\":46490},{\"end\":46510,\"start\":46504},{\"end\":46527,\"start\":46519},{\"end\":46541,\"start\":46532},{\"end\":46551,\"start\":46547},{\"end\":46565,\"start\":46558},{\"end\":47109,\"start\":47102},{\"end\":47125,\"start\":47116},{\"end\":47138,\"start\":47131},{\"end\":47427,\"start\":47421},{\"end\":47429,\"start\":47428},{\"end\":47443,\"start\":47438},{\"end\":47460,\"start\":47453},{\"end\":47462,\"start\":47461},{\"end\":47758,\"start\":47757},{\"end\":47766,\"start\":47761},{\"end\":47780,\"start\":47775},{\"end\":47782,\"start\":47781},{\"end\":47795,\"start\":47791},{\"end\":47797,\"start\":47796},{\"end\":48057,\"start\":48053},{\"end\":48070,\"start\":48066},{\"end\":48086,\"start\":48079},{\"end\":48337,\"start\":48330},{\"end\":48339,\"start\":48338},{\"end\":48616,\"start\":48610},{\"end\":48629,\"start\":48625},{\"end\":48641,\"start\":48637},{\"end\":48938,\"start\":48932},{\"end\":48951,\"start\":48947},{\"end\":48961,\"start\":48956},{\"end\":48977,\"start\":48971},{\"end\":49448,\"start\":49444},{\"end\":49450,\"start\":49449},{\"end\":49467,\"start\":49459},{\"end\":49482,\"start\":49478},{\"end\":49724,\"start\":49716},{\"end\":49742,\"start\":49736},{\"end\":50115,\"start\":50111},{\"end\":50129,\"start\":50123},{\"end\":50146,\"start\":50139},{\"end\":50369,\"start\":50362},{\"end\":50388,\"start\":50382},{\"end\":50402,\"start\":50398},{\"end\":50708,\"start\":50704},{\"end\":50936,\"start\":50932},{\"end\":51387,\"start\":51380},{\"end\":51696,\"start\":51689},{\"end\":51713,\"start\":51706},{\"end\":51726,\"start\":51720}]", "bib_author_last_name": "[{\"end\":44825,\"start\":44817},{\"end\":45238,\"start\":45223},{\"end\":45253,\"start\":45246},{\"end\":45279,\"start\":45260},{\"end\":45286,\"start\":45281},{\"end\":45525,\"start\":45517},{\"end\":45539,\"start\":45533},{\"end\":45556,\"start\":45548},{\"end\":45576,\"start\":45566},{\"end\":45590,\"start\":45585},{\"end\":45602,\"start\":45597},{\"end\":45895,\"start\":45884},{\"end\":45910,\"start\":45904},{\"end\":45926,\"start\":45919},{\"end\":46502,\"start\":46498},{\"end\":46517,\"start\":46511},{\"end\":46530,\"start\":46528},{\"end\":46545,\"start\":46542},{\"end\":46556,\"start\":46552},{\"end\":46570,\"start\":46566},{\"end\":47114,\"start\":47110},{\"end\":47129,\"start\":47126},{\"end\":47143,\"start\":47139},{\"end\":47436,\"start\":47430},{\"end\":47451,\"start\":47444},{\"end\":47468,\"start\":47463},{\"end\":47773,\"start\":47767},{\"end\":47789,\"start\":47783},{\"end\":47805,\"start\":47798},{\"end\":48064,\"start\":48058},{\"end\":48077,\"start\":48071},{\"end\":48094,\"start\":48087},{\"end\":48343,\"start\":48340},{\"end\":48623,\"start\":48617},{\"end\":48635,\"start\":48630},{\"end\":48646,\"start\":48642},{\"end\":48945,\"start\":48939},{\"end\":48954,\"start\":48952},{\"end\":48969,\"start\":48962},{\"end\":48983,\"start\":48978},{\"end\":49457,\"start\":49451},{\"end\":49476,\"start\":49468},{\"end\":49488,\"start\":49483},{\"end\":49734,\"start\":49725},{\"end\":49750,\"start\":49743},{\"end\":50121,\"start\":50116},{\"end\":50137,\"start\":50130},{\"end\":50157,\"start\":50147},{\"end\":50380,\"start\":50370},{\"end\":50396,\"start\":50389},{\"end\":50408,\"start\":50403},{\"end\":50713,\"start\":50709},{\"end\":50941,\"start\":50937},{\"end\":51392,\"start\":51388},{\"end\":51704,\"start\":51697},{\"end\":51718,\"start\":51714},{\"end\":51734,\"start\":51727}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":9864971},\"end\":45216,\"start\":44720},{\"attributes\":{\"doi\":\"arXiv:1905.10415\",\"id\":\"b1\"},\"end\":45483,\"start\":45218},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":25062002},\"end\":45764,\"start\":45485},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4614529},\"end\":46383,\"start\":45766},{\"attributes\":{\"doi\":\"arXiv:1910.06151\",\"id\":\"b4\",\"matched_paper_id\":204509632},\"end\":47015,\"start\":46385},{\"attributes\":{\"doi\":\"arXiv:1811.04852\",\"id\":\"b5\"},\"end\":47316,\"start\":47017},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3834959},\"end\":47707,\"start\":47318},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":33391978},\"end\":47986,\"start\":47709},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2483891},\"end\":48273,\"start\":47988},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":8315873},\"end\":48514,\"start\":48275},{\"attributes\":{\"doi\":\"arXiv:1811.04909\",\"id\":\"b10\"},\"end\":48823,\"start\":48516},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":46941335},\"end\":49383,\"start\":48825},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":199678359},\"end\":49682,\"start\":49385},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":579463},\"end\":50071,\"start\":49684},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":5589954},\"end\":50300,\"start\":50073},{\"attributes\":{\"id\":\"b15\"},\"end\":50604,\"start\":50302},{\"attributes\":{\"doi\":\"arXiv:1811.00414\",\"id\":\"b16\"},\"end\":50863,\"start\":50606},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":44036160},\"end\":51222,\"start\":50865},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":120278241},\"end\":51635,\"start\":51224},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3714239},\"end\":51914,\"start\":51637}]", "bib_title": "[{\"end\":44808,\"start\":44720},{\"end\":45509,\"start\":45485},{\"end\":45872,\"start\":45766},{\"end\":46488,\"start\":46385},{\"end\":47419,\"start\":47318},{\"end\":47755,\"start\":47709},{\"end\":48051,\"start\":47988},{\"end\":48328,\"start\":48275},{\"end\":48930,\"start\":48825},{\"end\":49442,\"start\":49385},{\"end\":49714,\"start\":49684},{\"end\":50109,\"start\":50073},{\"end\":50360,\"start\":50302},{\"end\":50930,\"start\":50865},{\"end\":51378,\"start\":51224},{\"end\":51687,\"start\":51637}]", "bib_author": "[{\"end\":44827,\"start\":44810},{\"end\":45240,\"start\":45218},{\"end\":45255,\"start\":45240},{\"end\":45281,\"start\":45255},{\"end\":45288,\"start\":45281},{\"end\":45527,\"start\":45511},{\"end\":45541,\"start\":45527},{\"end\":45558,\"start\":45541},{\"end\":45578,\"start\":45558},{\"end\":45592,\"start\":45578},{\"end\":45604,\"start\":45592},{\"end\":45897,\"start\":45874},{\"end\":45912,\"start\":45897},{\"end\":45928,\"start\":45912},{\"end\":46504,\"start\":46490},{\"end\":46519,\"start\":46504},{\"end\":46532,\"start\":46519},{\"end\":46547,\"start\":46532},{\"end\":46558,\"start\":46547},{\"end\":46572,\"start\":46558},{\"end\":47116,\"start\":47102},{\"end\":47131,\"start\":47116},{\"end\":47145,\"start\":47131},{\"end\":47438,\"start\":47421},{\"end\":47453,\"start\":47438},{\"end\":47470,\"start\":47453},{\"end\":47761,\"start\":47757},{\"end\":47775,\"start\":47761},{\"end\":47791,\"start\":47775},{\"end\":47807,\"start\":47791},{\"end\":48066,\"start\":48053},{\"end\":48079,\"start\":48066},{\"end\":48096,\"start\":48079},{\"end\":48345,\"start\":48330},{\"end\":48625,\"start\":48610},{\"end\":48637,\"start\":48625},{\"end\":48648,\"start\":48637},{\"end\":48947,\"start\":48932},{\"end\":48956,\"start\":48947},{\"end\":48971,\"start\":48956},{\"end\":48985,\"start\":48971},{\"end\":49459,\"start\":49444},{\"end\":49478,\"start\":49459},{\"end\":49490,\"start\":49478},{\"end\":49736,\"start\":49716},{\"end\":49752,\"start\":49736},{\"end\":50123,\"start\":50111},{\"end\":50139,\"start\":50123},{\"end\":50159,\"start\":50139},{\"end\":50382,\"start\":50362},{\"end\":50398,\"start\":50382},{\"end\":50410,\"start\":50398},{\"end\":50715,\"start\":50704},{\"end\":50943,\"start\":50932},{\"end\":51394,\"start\":51380},{\"end\":51706,\"start\":51689},{\"end\":51720,\"start\":51706},{\"end\":51736,\"start\":51720}]", "bib_venue": "[{\"end\":44994,\"start\":44919},{\"end\":46100,\"start\":46017},{\"end\":46709,\"start\":46657},{\"end\":49120,\"start\":49061},{\"end\":49893,\"start\":49831},{\"end\":51056,\"start\":51008},{\"end\":44917,\"start\":44827},{\"end\":45342,\"start\":45304},{\"end\":45610,\"start\":45604},{\"end\":46015,\"start\":45928},{\"end\":46655,\"start\":46588},{\"end\":47100,\"start\":47017},{\"end\":47495,\"start\":47470},{\"end\":47830,\"start\":47807},{\"end\":48114,\"start\":48096},{\"end\":48381,\"start\":48345},{\"end\":48608,\"start\":48516},{\"end\":49059,\"start\":48985},{\"end\":49513,\"start\":49490},{\"end\":49829,\"start\":49752},{\"end\":50173,\"start\":50159},{\"end\":50433,\"start\":50410},{\"end\":50702,\"start\":50606},{\"end\":51006,\"start\":50943},{\"end\":51415,\"start\":51394},{\"end\":51759,\"start\":51736}]"}}}, "year": 2023, "month": 12, "day": 17}