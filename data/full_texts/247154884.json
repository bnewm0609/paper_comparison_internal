{"id": 247154884, "updated": "2023-10-05 16:50:09.905", "metadata": {"title": "Reconstruction of Perceived Images from fMRI Patterns and Semantic Brain Exploration using Instance-Conditioned GANs", "authors": "[{\"first\":\"Furkan\",\"last\":\"Ozcelik\",\"middle\":[]},{\"first\":\"Bhavin\",\"last\":\"Choksi\",\"middle\":[]},{\"first\":\"Milad\",\"last\":\"Mozafari\",\"middle\":[]},{\"first\":\"Leila\",\"last\":\"Reddy\",\"middle\":[]},{\"first\":\"Rufin\",\"last\":\"VanRullen\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Reconstructing perceived natural images from fMRI signals is one of the most engaging topics of neural decoding research. Prior studies had success in reconstructing either the low-level image features or the semantic/high-level aspects, but rarely both. In this study, we utilized an Instance-Conditioned GAN (IC-GAN) model to reconstruct images from fMRI patterns with both accurate semantic attributes and preserved low-level details. The IC-GAN model takes as input a 119-dim noise vector and a 2048-dim instance feature vector extracted from a target image via a self-supervised learning model (SwAV ResNet-50); these instance features act as a conditioning for IC-GAN image generation, while the noise vector introduces variability between samples. We trained ridge regression models to predict instance features, noise vectors, and dense vectors (the output of the first dense layer of the IC-GAN generator) of stimuli from corresponding fMRI patterns. Then, we used the IC-GAN generator to reconstruct novel test images based on these fMRI-predicted variables. The generated images presented state-of-the-art results in terms of capturing the semantic attributes of the original test images while remaining relatively faithful to low-level image details. Finally, we use the learned regression model and the IC-GAN generator to systematically explore and visualize the semantic features that maximally drive each of several regions-of-interest in the human brain.", "fields_of_study": "[\"Computer Science\",\"Engineering\",\"Biology\"]", "external_ids": {"arxiv": "2202.12692", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ijcnn/OzcelikCMRV22", "doi": "10.1109/ijcnn55064.2022.9892673"}}, "content": {"source": {"pdf_hash": "feb7cf19e6493fa9516dfd601395e217aa602294", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2202.12692v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "bd2e04e5c77d3d1c29ed6af41bd90b60f8323209", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/feb7cf19e6493fa9516dfd601395e217aa602294.txt", "contents": "\nReconstruction of Perceived Images from fMRI Patterns and Semantic Brain Exploration using Instance-Conditioned GANs\n\n\nFurkan Ozcelik furkan.ozcelik@univ-tlse3.fr \nCerCo\nCNRS UMR5549\nUniversit\u00e9 de Toulouse Toulouse\nFrance\n\nBhavin Choksi bhavin.choksi@cnrs.fr \nCNRS UMR5549\nUniversit\u00e9 de Toulouse Toulouse\nFrance\n\nMilad Mozafari milad.mozafari@cnrs.fr \nUMR5549 IRIT, CNRS UMR5505 Toulouse\nCNRS\nFrance\n\nLeila Reddy leila.reddy@cnrs.fr \nUMR5549 and ANITI\nCNRS\nUniversit\u00e9 de Toulouse Toulouse\nFrance\n\nRufin Vanrullen rufin.vanrullen@cnrs.fr \nUMR5549 and ANITI\nCNRS\nUniversit\u00e9 de Toulouse Toulouse\nFrance\n\nReconstruction of Perceived Images from fMRI Patterns and Semantic Brain Exploration using Instance-Conditioned GANs\nIndex Terms-Natural Image ReconstructionfMRI DecodingIC-GANBrain-Computer Interface\nReconstructing perceived natural images from fMRI signals is one of the most engaging topics of neural decoding research. Prior studies had success in reconstructing either the low-level image features or the semantic/high-level aspects, but rarely both. In this study, we utilized an Instance-Conditioned GAN (IC-GAN) model to reconstruct images from fMRI patterns with both accurate semantic attributes and preserved low-level details. The IC-GAN model takes as input a 119-dim noise vector and a 2048-dim instance feature vector extracted from a target image via a self-supervised learning model (SwAV ResNet-50); these instance features act as a conditioning for IC-GAN image generation, while the noise vector introduces variability between samples. We trained ridge regression models to predict instance features, noise vectors, and dense vectors (the output of the first dense layer of the IC-GAN generator) of stimuli from corresponding fMRI patterns. Then, we used the IC-GAN generator to reconstruct novel test images based on these fMRIpredicted variables. The generated images presented state-of-theart results in terms of capturing the semantic attributes of the original test images while remaining relatively faithful to low-level image details. Finally, we use the learned regression model and the IC-GAN generator to systematically explore and visualize the semantic features that maximally drive each of several regionsof-interest in the human brain.\n\nI. INTRODUCTION\n\nUnderstanding the brain and cognition has always been one of the fundamental research areas of science. One of the ways researchers approach this task is by establishing neural encoding and decoding methods. New ways to decode information from brain signals have emerged with recent developments in modeling and computation. In vision research, many studies have used statistical methods and machine learning to decode specific information like position [1] or orientation [2], [3], to classify image categories [4], [5], to retrieve the closest images from a candidate set [6], or even to reconstruct images with low-complexity like basic shapes and structures [7].\n\nWith the emergence of deep learning, and in particular advanced deep generative models, reconstructing more complex images like handwritten digits [8], faces [9], and natural scenes [10] has become possible. These deep generative models include variational auto-encoders (VAEs), generative adversarial networks (GANs), and many variants and hybrids of both. Although many studies have used these models, they typically managed to reconstruct either low-level or high-level features of the images, but rarely both at the same time.\n\nHere, we propose a method to reconstruct natural images from fMRI activation patterns with both accurate semantic attributes and relatively preserved low-level details, using an Instance-Conditioned GAN (IC-GAN) -a recent generative model [11] inspired by the success of self-supervised feature learning [12]. In our framework, we first extract latent representations for a set of training images (see Figure 1): highlevel attributes of the images, called \"instance features\" in IC-GAN, are computed with a single forward pass through the SwAV ResNet-50 model; lower-level aspects of the image (e.g. reflecting the size, position or orientation of an object, details of the background, etc.) are obtained via a two-stage optimization of the IC-GAN \"noise\" and \"dense\" latent vectors (inspired by the method of Pividori et al. [13]). Next, we train three ridge regression models to predict these latent image representations from the corresponding fMRI patterns, recorded while human subjects viewed the same training images ( Figure 2, Step 1). Finally, for each image in the test set, we predict the instance feature, noise vector, and dense vector from fMRI data (using the previously learned regression models), and then reconstruct an estimate of the image using IC-GAN's generator (Figure 2, Step 2 and 3). The code of this paper can be found in the official GitHub repository 1 .\n\nOur method establishes a new state-of-the-art performance for capturing the semantic attributes of the images, while preserving a reasonable amount of low-level details. We present both qualitative and quantitative results, and a comparison with previous methods to support our claims. We also take advantage of our brain-based image reconstruction system to explore and visualize the semantic image attributes encoded in various brain regions-of-interest (ROIs), and discuss how these findings align with neuroscientific evidence.\n\nII. PREVIOUS WORKS Many methods have been proposed in the literature for reconstructing natural images from fMRI patterns using deep learning models. Shen et al. [10] optimized input images with a deep generator network using the loss provided by fMRI decoded CNN features. Seeliger et al. [14] trained a linear regression between fMRI signals and DCGAN's latent space using a feature loss from a CNN model and a pixel-space MSE loss. In addition to supervised training with {fMRI, stimulus} pairs, Beliy et al. [15] used a consistency loss for unsupervised training with test fMRI data (without corresponding stimuli) and additional image data. Later, Gaziv et al. [16] improved on this method by imposing a perceptual loss on reconstructed images, resulting in sharper reconstructions. Mozafari et al. [17] proposed the first semantically-oriented reconstruction model using BigBiGAN [18]. Finally, Ren et al. [19] developed a dual VAE-GAN model that uses a three-stage learning strategy incorporating adversarial learning and knowledge distillation. We qualitatively and quantitatively compare the results of these studies with our proposed method.\n\n\nIII. MATERIALS AND METHODS\n\n\nA. Instance-Conditioned GAN\n\nWe utilized an Instance-Conditioned GAN (IC-GAN) model, pretrained for natural image generation on the Ima-geNet dataset [20]. IC-GAN can be considered as a generic framework rather than a single model, because it can be applied to different GAN backbones, e.g. StyleGAN [21] or BigGAN [22]. In the usual conditional GAN setting [23], class labels are provided along with noise vectors sampled from a normal distribution to generate images. Images belonging to that specific class are labeled as \"real\", and generated images from the generator are labeled as \"fake\". Both the generator and discriminator are trained with these images and labels in an adversarial learning framework. In the instance-conditional setting, instead of giving a class label, instance features that capture the semantic attributes of a given image are extracted (via a pre-trained feature extractor) and provided to the generator as conditioning, alongside a sampled noise vector. For 1 https://github.com/ozcelikfu/IC-GAN_fMRI_ Reconstruction\n\nStep 1 -Compute instance features h for training images Y tr SwAV h tr\n\nStep 2 -Optimize noise vector z for training images\nh tr z i \u27f3 IC-GAN Embed Dense\u0176 z Y tr\n\nLoss\n\nStep 3 -Optimize dense vector d for training images Step 2: In addition to the instance feature vector, the IC-GAN also requires a noise vector (z i ) as input, which encodes lower-level properties of the image (e.g., pose, orientation, background etc.). While providing htr obtained from Step 1 to the IC-GAN's generator, we optimize the noise vector (z i ) to generate the closest image (\u0176z) to the groundtruth image (Ytr). The resulting optimized noise vector is ztr.\nh tr z tr d i \u27f3 IC-GAN Embed\u0176 d Y tr\nStep 3: To further improve image reconstruction so as to better match the more detailed spatial structure of the training image, we apply another optimization stage, in which we optimize the dense layer vectors of IC-GAN itself. To achieve, this, we pass the first 17 dimensions of ztr to the dense layer of the IC-GAN's generator and obtain initial dense vectors (d 0 ). While keeping both htr and the remaining 102 dimensions of ztr fixed, we optimize the dense vector d i to generate the closest image (\u0176 d ) to the groundtruth image (Ytr). dtr is the resulting optimized dense vector.\n\ntraining, IC-GAN selects k images in the neighborhood of the conditioning image (according to the feature extractor); these images are labeled as real, while generated images are considered as fake images to train both the generator and discriminator.\n\nFor the instance feature extraction, IC-GAN models use the SwAV (Swapping Assignments between Views) architecture [24] with a ResNet-50 [25] backbone. SwAV is a self-supervised learning model which means that it does not require handcrafted labels from humans. Similar to contrastive learning methods [12], SwAV minimizes the distance in feature space between representations of two transformed images (coming from the same original image).\n\nIt is possible to train the IC-GAN framework with different feature extractors, as long as they provide rich feature representations. However, using features from self-supervised learning models (e.g., SwAV) is better suited to the problem of neural decoding and natural image reconstruction. Indeed, many recent studies show that representations gathered from self-supervised learning models present more similarity to brain representations than other learning methods [26], [27].\n\nThe specific IC-GAN model we used here relies on a Big-GAN [22] architecture with 7 layers. It generates 256\u00d7256\u00d73 images from a 2048-dim (dimensional) instance feature vector extracted from SwAV ResNet-50 and a 119-dim noise vector sampled from a normal distribution. The 2048-dim instance features are given to an embedding layer and thus reduced to 512-dim embedded vectors. The 119-dim noise vector, which encodes lower-level properties of the image (e.g. pose, size, orientation of the object), is split into seven hierarchical levels, each with 17 dimensions. The first 17-dim level is directly given to the first dense layer of the IC-GAN generator. The remaining six hierarchical levels are concatenated with the embedded instance vector to be fed to the generator in each of the six BigGAN residual blocks.\n\nOverall, the purpose of IC-GAN is to generate, from one conditioning image, new and diverse image instances that share semantic attributes (as captured by SwAV instance features), but differ in low-level properties (e.g. object position, size, orientation, background details). The diversity of low-level properties is determined by randomly sampled \"noise\" vectors (and by the \"dense\" vectors directly derived from them). However, for the purpose of fMRI-based image reconstruction, both high-level and low-level properties must be specified. Therefore, rather than randomly sampling noise vectors, we computed a specific noise vector (and the associated dense vector) for each training image in the dataset, as detailed below.\n\n\nB. Extracting Latent Variables from Training Stimuli\n\nWe illustrate the computation of latent variables in Figure 1. We first extracted a 2048-dim instance feature vector for each training image in our dataset (see dataset details below) by presenting it to a SwAV ResNet-50 feature extractor. We then provided these instance features to the IC-GAN generator, and optimized the 119-dim noise vector for the same image using the covariance matrix adaptation evolution strategy (CMA-ES) [28]. We used this method because we empirically observed that global optimization strategies worked better than local optimization strategies (like gradient-based methods) for the noise vector. The loss function for this optimization was the distance between the generated image and the original training image in Layer\u22124 of SwAV ResNet-50; this representation level, hierarchically lower than the instance feature level, encodes more spatially structured information.\n\nFinally, to further match the more detailed spatial structure of the original image, we applied one more optimization stage. Inspired by the two-stage inversion method of Pividori et al. [13], we provided the first 17 dimensions of the previously optimized noise vector to the first dense layer of the IC-GAN, resulting in a 1536 \u00d7 4 \u00d7 4-dim dense vector. While the instance features and the remaining 102 dimensions of the noise vector were kept fixed, we optimized these dense vectors with the RMSProp optimizer. For this second-stage optimization, the previous loss (SwAV ResNet-50 Layer\u22124 feature distance) was combined with a Learned Perceptual Image Patch Similarity (LPIPS) [29] loss gathered from a pretrained VGG16 model [30] and a pixel (MSE) loss from 64 \u00d7 64 resized images.\n\nStep 1 -Train regression models with training fMRI data\nY tr X tr Ridge Ridge Ridge h tr z tr d tr\nStep 2 -Decode latent variables using test fMRI data\nY ts X ts Ridge Ridge Ridge\u0125 t\u015d z t\u015d d ts\nStep 3 -Reconstruct images from decoded latent variable\u015d Fig. 2. Decoding latent variables from fMRI patterns and reconstructing images from decoded variables.\nh t\u015d z t\u015d d ts IC-GAN\u0176 ts\nStep 1: Having obtained the instance features (htr), noise vectors (ztr) and dense vectors (dtr) of training images (Ytr) as described in Figure 1, we train three ridge regression models to map fMRI patterns of the training set (Xtr) to these latent variables.\n\nStep 2: Using these trained regression models, we decode latent variables of the test set (\u0125ts,\u1e91ts,dts) from test fMRI patterns (Xts).\n\nStep 3: We pass the decoded latent variables to the IC-GAN Generator to obtain reconstructed images (\u0176ts)\n\n\nC. Generic Object Decoding Dataset\n\nIn this study, we used previously published fMRI recordings of five human subjects presented with images from the ImageNet dataset [31]. The dataset contains training and testing image perception sessions where subjects looked at 1200 training samples drawn from 150 categories (8 samples each) and 50 testing samples chosen from 50 categories (1 sample each), respectively. Training and testing categories were chosen independently and were non-overlapping. Each training image was presented only once, while testing images were repeated 35 times during the whole experiment. All fMRI runs followed a similar design: fixation (33s), 50 image presentations (9s per image flashing at 2Hz), fixation (6s). Moreover, subjects were also asked to perform a one-back task by pressing a button whenever the same image was presented two times in a row (five such events occurred per run).\n\nThe fMRI data were pre-processed for each subject by three-dimensional motion correction followed by coregistration to the high-resolution anatomical image. Then, the brain representation of each image was calculated by averaging the percent signal change values of each voxel over the 9-s presentation window. Additionally, the dataset provides functional regions of interest (ROIs) that cover the entire visual cortex, including V1-V4, the fusiform face area (FFA), parahippocampal place area (PPA), and lateral occipital complex (LOC). The pre-processed data is available to download at brainliner.jp 2 .\n\n\nD. fMRI Decoding and Image Reconstruction\n\nDetails of fMRI decoding and image reconstruction are depicted in Figure 2. The procedure involves two separate stages for training and testing the brain decoding system of each subject.\n\nFirst, we trained three separate ridge regression models to predict the latent variables (instance features; noise vectors; dense vectors) for each of the 1200 training images based on the corresponding fMRI patterns. Since both the fMRI data and the latent variables are high-dimensional, we applied L 2 regularization on the regression weights during training.\n\nAt test time, we averaged the 35 repetitions of fMRI signals corresponding to each test stimulus. Next, we used the previously trained regression models to predict the instance features, noise vectors, and dense vectors from these averaged fMRI signals. Finally, we used these predicted latent variables to generate image reconstructions using the IC-GAN generator.\n\n\nIV. RESULTS AND ANALYSES A. Image Reconstruction Results\n\nExamples of image reconstructions produced by our method are displayed in Figure 3. First of all, it is important to examine IC-GAN reconstructions (second column) based on the optimized \"ground-truth\" latent vectors (derived as detailed in Figure 1): we can see that IC-GAN can successfully reconstruct the semantic attributes of the test images; however, it often misses some visual details, like parts of the vehicle (third row), liquid in the glass (fifth row), or the precise text in the gravestone (eighth row). These reconstructions help us understand how the IC-GAN generator would behave if we perfectly decoded latent variables from fMRI patterns, i.e. they serve as an upper bound on the expected reconstruction quality.\n\nWhen we inspect actual fMRI reconstructions for the five subjects (third to seventh columns), our first observation is that reconstructions look like natural images. Furthermore, they are consistent across subjects. Again, these reconstructions capture some of the semantic attributes, while also missing specific aspects of the test images. For example, the system generates images of horned animals for the goat image (second row), but their species are not clearly identifiable. For the token image (fifth row), round objects are reconstructed, but not with the right texture. For the gravestone image (last row), similar square-shaped objects with text and symbols are generated, but most of them would not qualify as a gravestone. Overall, our method appears to reconstruct semantic attributes with slight but significant variations in details.\n\nHow does it compare to previously proposed methods? In Figure 4, we present image reconstructions using alternative methods proposed in five other studies, together with our results for comparison 3 . From these reconstructions, we can see that many methods capture low-level details rather than high-level ones; as a result, many of the reconstructions do not look natural. A notable exception is the study of Mozafari et al., based on the BigBiGAN architecture [17], in which reconstructions often capture high-level properties and are more naturalistic. Even this method, however, does not correctly reconstruct semantic details for some of the images; furthermore, it misses many of the low-level details. Among the other studies, Ren et al. [19] succeed in reconstructing colors and textures better than other methods, while Gaziv et al. [16] give sharper object edges. Our method generates realistic-looking image reconstructions with appropriate semantic features, while preserving the low-level aspects to a certain degree. These qualitative observations are supported by the quantitative comparison of methods in Table I, according to both low-level measures of image quality (Pix-Comp, SSIM) and higher-level \"semantic\" measures (Inception or CLIP distance). Pix-Comp is a 2-way comparison of pixel-wise correlation measures computed over the whole test set. We used the results reported by authors in their respective papers, except for Gaziv et al. [16], who did not report Pix-Comp: we re-computed it over the reconstructed images provided in their supplementary material. All other metrics (SSIM [32], Inception-V3 [33] distance, and CLIP ViT-B/32 [34] distance), were computed over the seven common image reconstructions presented in Figure 4. Our own results are presented for three different versions of IC-GAN decoding, using different combinations of the three brain regression models in Figure 2, to evaluate the effects of each regressor on performance. First, the IC-GAN (Random) version uses brain-decoded instance features together with randomly sampled noise vectors from a normal distribution. Second, the IC-GAN (Noise) version combines brain-decoded instance features with brain-decoded noise vectors, without using the brain-decoded dense vectors (instead, the output of the first dense layer is used directly). Finally, IC-GAN (Dense) is the complete framework described  in Figure 2, which uses all the brain-decoded latent variables (thus overriding the dense vector with its brain-decoded version). The table indicates that most other methods yield better results than IC-GAN on the low-level measures (Pix-Comp, SSIM), except for Mozafari et al [17]; like ours, that study was aimed at matching higher-level \"semantic\" aspects of the input images. Importantly, IC-GAN outperforms the Mozafari et al method for both low-level measures. For the high-level measures (Inception and CLIP Distances), IC-GAN demonstrates state-of-the-art performance, surpassing all methods-including Mozafari et al.-by a significant margin. The comparison of the 3 versions of our IC-GAN method reveals that the inclusion of both the brain-decoded noise vector (IC-GAN Noise) and the brain-decoded dense vector (IC-GAN Dense) helps improve the model's ability to capture low-level details. Still, the full model remains inferior to many previous methods in this respect. Regarding high-level semantic attributes, while the full method IC-GAN (Dense) is superior to IC-GAN (Noise) for the Inception distance, the opposite is true for the CLIP distance. This could be because Inception features include more spatially structured information than CLIP features; indeed, the function of dense vectors in our method is precisely to capture the image spatial structure that is less explicitly encoded in the noise vectors.\n\n\nB. Semantic Analysis of Visual Encoding in Brain ROIs\n\nOur brain decoding model, relying on the latent space(s) of the IC-GAN network, can reconstruct the high-level content of perceived images better than all prior methods, while retaining more low-level details than at least some of these methods. From a neuroscience viewpoint, can this brain decoding model also help us understand the neural coding of visual information in the brain? Here, we use our model to explore and directly visualize the types of information that are preferentially represented in various brain regions-of-interest (ROIs).\n\nThe fMRI dataset counts seven distinct ROIs across visual cortex for each subject-in hierarchical order: V1, V2, V3, V4, LOC (Lateral Occipital Complex), FFA (Fusiform Face Area) and PPA (Parahippocampal Place Area). First, we ask whether each region carries more information about high-level latent features-as captured by the model's instance features-or about low-level properties-as captured by the model's dense vector (note that similar results, not shown here, were obtained for the noise vector instead of the dense vector). To answer this  , Gaziv et al. [16], Mozafari et al. [17], and Ren et al. [19], respectively. fMRI reconstructions by the IC-GAN method demonstrate more naturalistic-looking images with accurate semantic attributes, while preserving some low-level details (e.g. object position, size or orientation).\n\nquestion, for each brain voxel we compared the L 1 norm of the model's ridge regression weights for the instance features vs. dense vectors ( Figure 5). As expected, lower brain regions (V1-V3) were more informative about the dense vector, while higher brain regions (V4, LOC, FFA, PPA) carried more information about instance features. Next, we use our brain decoding model to visualize the \"optimal\" stimulus for each brain region. Instead of using fMRI patterns recorded from subjects, we synthesized seven patterns, one for each ROI, with a value of 1 for all voxels inside the ROI and 0 outside. We provided these synthetic patterns to the three trained ridge regression models to obtain predicted latent variables (as described in Figure 2). To mitigate the scaling problem, we normalized instance features to have unit norms. We then passed the predicted latent variables through the IC-GAN generator to generate images.\n\nPreviously, Gu et al. [35] synthesized optimal images for different ROIs using a BigGAN generator and a feature extractor. They iteratively optimized the latent variables of the generator in such a way that predicted fMRI patterns (obtained via the feature extractor) maximized activation in a specific ROI. In contrast, our method involves a single pass through our image reconstruction pipeline, and does not require iterative optimization of the latent variables. Figure 6 presents the generated images from each subject (second to sixth columns), together with reconstructions using averaged latent variables across all five subjects (first column).\n\nIn lower visual cortex (V1-V2), basic textures (foliage, trees, stones) are produced rather than (or in addition to) identifiable objects. The textures emphasize the periphery of the visual field, in line with the fact that V1-V2 have small receptive fields that can be positioned at high visual eccentricity. For V3 and V4, the generated textures present more regularity than V1 and V2, and we begin to see visuals close to objects with multiple parts, including text-like symbols, notably in V4. LOC is known for its selectivity to object shapes; when maximizing this region's response, IC-GAN generates complete objects at the center of the image, rather than extended textures. At this stage, the visual periphery appears empty or blurry, in contrast with the crisp peripheral textures produced for V1-V2. In FFA, a high-level region known for its selectivity to face images, IC-GAN generates human and animal faces. The presence of animal faces is not unexpected, since the ImageNet dataset (on which IC-GAN was trained) contains many more animal images than human images. Some previous experimental and computational work [35], [36] also suggests that fusiform regions may show a preferential response to animals, and particularly dogs. Nonetheless, the model still higher weight for instance feature higher weight for dense vector Difference in regression weights (percentile)\n-4 8 V 1 L O C 0 F F A P P A V 2 V 3 V 4\nRegions of Interest (ROI) b a frontal occipital occipital frontal 7.\n\n-7. generates human face images for two of the subjects. The last ROI from the higher visual cortex is PPA, known for its selectivity to environmental scenes like indoor and outdoor places. IC-GAN also generates indoor and outdoor places when the voxels of this region are activated. Some of the images have an object in the center of the scene; this might be caused by the training of the IC-GAN model on ImageNet-an object-centered dataset. It is worth noting that PPA-optimized images produce more details in the visual periphery than FFAoptimized images; this is compatible with the known difference in preferential eccentricity between the two regions [37]. Overall, the outcomes of this analysis are consistent with findings from the neuroscience literature, indicating that our IC-GAN-based model learned to appropriately decode visual feature selectivity in the brain. Most importantly, the method allows us to directly visualize this selectivity, rather than inferring it from extended experiments.  6. Generated images from synthetic fMRI patterns constructed by activating all voxels in a specific brain region-of-interest (ROI), and none outside of the ROI. The rows represent various brain regions: V1, V2, V3, V4, LOC, FFA, and PPA. The first column is generated after averaging the brain-predicted latent variables for all five subjects. The following columns are for individual subjects.\n\n\nV. DISCUSSION\n\nIn this paper, we presented a framework for natural image reconstruction from fMRI patterns using the IC-GAN model, pretrained on ImageNet. First, we extracted instance features, noise vectors, and dense vectors from training images, and trained ridge regression models from fMRI patterns to these latent variables. With these regression models, we decoded latent variables from the test fMRI patterns, and finally reconstructed images with the IC-GAN generator.\n\nMany previous studies implemented fMRI reconstruction frameworks with deep generative models. However, these models were able to reconstruct either low-level or high-level features of the images. Our method demonstrated state-ofthe-art performance on reconstructing semantic (high-level) attributes of the images, both qualitatively and quantitatively, while generating naturalistic-looking images. Meanwhile, compared to other semantically oriented models (e.g. Mozafari et al. [17], an approach based on BigBiGAN), it was able to maintain more low-level details. Furthermore, we could use our fMRI-based image reconstruction model to visualize images decoded from synthetic fMRI patterns, designed to maximize activations in specific brain ROIs. The results of this analysis were aligned with the existing neuroscience literature, opening a range of possibilities for future brain exploration and visualization techniques.\n\nWe acknowledge that there is still room for improving our model, especially in terms of better reproducing low-level details. This may be achieved in future work by improving our optimization of the noise and dense vectors, or by pairing IC-GAN with other generative networks more focused on lowlevel image properties.\n\n\nFunded by AI-REPS grant ANR-18-CE37-0007-01 and ANITI grant ANR-19-PI3A-0004\n\nLossFig. 1 .\n1Extraction of the latent variables (htr, ztr and dtr) for each training image (Ytr). Step 1: Instance features of training images (htr) are extracted using SwAV ResNet-50. This 2048-dim instance feature vector (htr) captures the semantic attributes of the image.\n\nFig. 3 .\n3fMRI Reconstructions by the IC-GAN model for all subjects. The first column is the groundtruth test image, whereas the second column is the reconstructed image by IC-GAN using true extracted latent variables. The following five columns demonstrate the equivalent reconstructions using fMRIdecoded latent variables for each subject. fMRI reconstructions are generally consistent with the groundtruth images in terms of semantic attributes, while they preserve the low-level details to a certain degree.\n\nFig. 4 .\n4Comparison of fMRI reconstructions for several methods. The first column is the groundtruth image, the second column is the reconstructed image with IC-GAN using true extracted latent variables. Columns three to eight present fMRI reconstructions from IC-GAN (Ours), Shen et al.[10], Beliy et al.[15]\n\nFig. 5 .\n5Mapping of instance features vs. dense vectors over brain regions. (a) Difference between the percentiles of the regression weights (L 1 norm) for the instance features vs. the dense vector, averaged over voxels in each ROI. Positive values indicate relatively higher weight for instance features compared to the dense vector, and vice versa. Error bars represent standard error of the mean across 5 subjects. (b) Voxel-by-voxel maps (left: axial; right: sagittal) of the difference between the percentiles of the regression weights (L 1 norm) for the instance features (red) vs. the dense vector (blue), averaged over the 5 subjects. Dense vector weights are higher in early visual cortex (occipital regions), while instance feature weights are larger in higher visual cortex (temporal regions).\n\nTABLE I QUANTITATIVE\nICOMPARISON OF IMAGE RECONSTRUCTIONS. FOR EACH MEASURE, THE BEST VALUE IS IN BOLD. (FOR PIX-COMP/SSIM, HIGHER IS BETTER; FOR INCEPTION/CLIP DISTANCE, LOWER IS BETTER)Method \n\nSimilarity Measure \nLow-Level \nHigh-Level \nPix-Comp \u2191 \nSSIM \u2191 \nInception \u2193 \nCLIP \u2193 \nShen et al. [10] \n79.7% \n0.582 \n0.829 \n0.358 \nBeliy et al. [15] \n85.3% \n0.597 \n0.865 \n0.424 \nGaziv et al. [16] \n91.5% \n0.601 \n0.841 \n0.387 \nRen et al. [19] \n87.3% \n0.588 \n0.847 \n0.383 \nMozafari et al. [17] \n54.3% \n0.450 \n0.818 \n0.352 \nIC-GAN (Random) \n64.1% \n0.467 \n0.761 \n0.328 \nIC-GAN (Noise) \n66.5% \n0.489 \n0.744 \n0.320 \nIC-GAN (Dense) \n67.2% \n0.491 \n0.742 \n0.330 \n\n\nhttp://brainliner.jp/data/brainliner/Generic_ Object_Decoding\nWe selected these seven images because it was the only common set of reconstruction exemplars presented across all of the considered studies.\n\nInverse retinotopy: inferring the visual content of images from brain activation patterns. B Thirion, E Duchesnay, E Hubbard, J Dubois, J.-B Poline, D Lebihan, S Dehaene, Neuroimage. 334B. Thirion, E. Duchesnay, E. Hubbard, J. Dubois, J.-B. Poline, D. Lebi- han, and S. Dehaene, \"Inverse retinotopy: inferring the visual content of images from brain activation patterns,\" Neuroimage, vol. 33, no. 4, pp. 1104-1116, 2006.\n\nDecoding the visual and subjective contents of the human brain. Y Kamitani, F Tong, Nature neuroscience. 85Y. Kamitani and F. Tong, \"Decoding the visual and subjective contents of the human brain,\" Nature neuroscience, vol. 8, no. 5, pp. 679-685, 2005.\n\nPredicting the orientation of invisible stimuli from activity in human primary visual cortex. J.-D Haynes, G Rees, Nature neuroscience. 85J.-D. Haynes and G. Rees, \"Predicting the orientation of invisible stimuli from activity in human primary visual cortex,\" Nature neuroscience, vol. 8, no. 5, pp. 686-691, 2005.\n\nDistributed and overlapping representations of faces and objects in ventral temporal cortex. J V Haxby, M I Gobbini, M L Furey, A Ishai, J L Schouten, P Pietrini, Science. 2935539J. V. Haxby, M. I. Gobbini, M. L. Furey, A. Ishai, J. L. Schouten, and P. Pietrini, \"Distributed and overlapping representations of faces and objects in ventral temporal cortex,\" Science, vol. 293, no. 5539, pp. 2425-2430, 2001.\n\nFunctional magnetic resonance imaging (fmri)\"brain reading\": detecting and classifying distributed patterns of fmri activity in human visual cortex. D D Cox, R L Savoy, Neuroimage. 192D. D. Cox and R. L. Savoy, \"Functional magnetic resonance imaging (fmri)\"brain reading\": detecting and classifying distributed patterns of fmri activity in human visual cortex,\" Neuroimage, vol. 19, no. 2, pp. 261-270, 2003.\n\nIdentifying natural images from human brain activity. K N Kay, T Naselaris, R J Prenger, J L Gallant, Nature. 4527185K. N. Kay, T. Naselaris, R. J. Prenger, and J. L. Gallant, \"Identifying natural images from human brain activity,\" Nature, vol. 452, no. 7185, pp. 352-355, 2008.\n\nVisual image reconstruction from human brain activity using a combination of multiscale local image decoders. Y Miyawaki, H Uchida, O Yamashita, M Sato, Y Morito, H C Tanabe, N Sadato, Y Kamitani, Neuron. 605Y. Miyawaki, H. Uchida, O. Yamashita, M.-a. Sato, Y. Morito, H. C. Tanabe, N. Sadato, and Y. Kamitani, \"Visual image reconstruction from human brain activity using a combination of multiscale local image decoders,\" Neuron, vol. 60, no. 5, pp. 915-929, 2008.\n\nLinear reconstruction of perceived images from human brain activity. S Schoenmakers, M Barth, T Heskes, M Van Gerven, Neu-roImage. 83S. Schoenmakers, M. Barth, T. Heskes, and M. Van Gerven, \"Linear reconstruction of perceived images from human brain activity,\" Neu- roImage, vol. 83, pp. 951-961, 2013.\n\nReconstructing faces from fmri patterns using deep generative neural networks. R Vanrullen, L Reddy, Communications biology. 21R. VanRullen and L. Reddy, \"Reconstructing faces from fmri patterns using deep generative neural networks,\" Communications biology, vol. 2, no. 1, pp. 1-10, 2019.\n\nDeep image reconstruction from human brain activity. G Shen, T Horikawa, K Majima, Y Kamitani, PLoS computational biology. 1511006633G. Shen, T. Horikawa, K. Majima, and Y. Kamitani, \"Deep image reconstruction from human brain activity,\" PLoS computational biology, vol. 15, no. 1, p. e1006633, 2019.\n\nInstance-conditioned GAN. A Casanova, M Careil, J Verbeek, M Drozdzal, A Romero, ; A Beygelzimer, Y Dauphin, P Liang, J , Advances in Neural Information Processing Systems. A. Casanova, M. Careil, J. Verbeek, M. Drozdzal, and A. Romero, \"Instance-conditioned GAN,\" in Advances in Neural Information Processing Systems, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W.\n\n. Vaughan, Vaughan, Eds., 2021. [Online]. Available: https://openreview.net/forum? id=aUuTEEcyY\n\nA survey on contrastive self-supervised learning. A Jaiswal, A R Babu, M Z Zadeh, D Banerjee, F Makedon, 9TechnologiesA. Jaiswal, A. R. Babu, M. Z. Zadeh, D. Banerjee, and F. Makedon, \"A survey on contrastive self-supervised learning,\" Technologies, vol. 9, no. 1, p. 2, 2021.\n\nExploiting gan internal capacity for high-quality reconstruction of natural images. M Pividori, G L Grinblat, L C , arXiv:1911.05630arXiv preprintM. Pividori, G. L. Grinblat, and L. C. Uzal, \"Exploiting gan internal ca- pacity for high-quality reconstruction of natural images,\" arXiv preprint arXiv:1911.05630, 2019.\n\nGenerative adversarial networks for reconstructing natural images from brain activity. K Seeliger, U G\u00fc\u00e7l\u00fc, L Ambrogioni, Y G\u00fc\u00e7l\u00fct\u00fcrk, M Van Gerven, NeuroImage. 181K. Seeliger, U. G\u00fc\u00e7l\u00fc, L. Ambrogioni, Y. G\u00fc\u00e7l\u00fct\u00fcrk, and M. van Gerven, \"Generative adversarial networks for reconstructing natural images from brain activity,\" NeuroImage, vol. 181, pp. 775-785, 2018. [Online]. Available: https://www.sciencedirect.com/science/article/pii/ S105381191830658X\n\nFrom voxels to pixels and back: Self-supervision in natural-image reconstruction from fmri. R Beliy, G Gaziv, A Hoogi, F Strappini, T Golan, M Irani, Advances in Neural Information Processing Systems. 32R. Beliy, G. Gaziv, A. Hoogi, F. Strappini, T. Golan, and M. Irani, \"From voxels to pixels and back: Self-supervision in natural-image reconstruction from fmri,\" Advances in Neural Information Processing Systems, vol. 32, 2019.\n\nSelf-supervised natural image reconstruction and rich semantic classification from brain activity. G Gaziv, R Beliy, N Granot, A Hoogi, F Strappini, T Golan, M Irani, bioRxivG. Gaziv, R. Beliy, N. Granot, A. Hoogi, F. Strappini, T. Golan, and M. Irani, \"Self-supervised natural image reconstruction and rich semantic classification from brain activity,\" bioRxiv, 2020.\n\nReconstructing natural scenes from fmri patterns using bigbigan. M Mozafari, L Reddy, R Vanrullen, 2020 International joint conference on neural networks (IJCNN). IEEEM. Mozafari, L. Reddy, and R. VanRullen, \"Reconstructing natural scenes from fmri patterns using bigbigan,\" in 2020 International joint conference on neural networks (IJCNN). IEEE, 2020, pp. 1-8.\n\nLarge scale adversarial representation learning. J Donahue, K Simonyan, Advances in Neural Information Processing Systems. 32J. Donahue and K. Simonyan, \"Large scale adversarial representation learning,\" Advances in Neural Information Processing Systems, vol. 32, 2019.\n\nReconstructing seen image from brain activity by visually-guided cognitive representation and adversarial learning. Z Ren, J Li, X Xue, X Li, F Yang, Z Jiao, X Gao, NeuroImage. 228117602Z. Ren, J. Li, X. Xue, X. Li, F. Yang, Z. Jiao, and X. Gao, \"Reconstruct- ing seen image from brain activity by visually-guided cognitive repre- sentation and adversarial learning,\" NeuroImage, vol. 228, p. 117602, 2021.\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"Imagenet: A large-scale hierarchical image database,\" in 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009, pp. 248-255.\n\nAnalyzing and improving the image quality of stylegan. T Karras, S Laine, M Aittala, J Hellsten, J Lehtinen, T Aila, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionT. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila, \"Analyzing and improving the image quality of stylegan,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 8110-8119.\n\nLarge scale GAN training for high fidelity natural image synthesis. A Brock, J Donahue, K Simonyan, International Conference on Learning Representations. A. Brock, J. Donahue, and K. Simonyan, \"Large scale GAN training for high fidelity natural image synthesis,\" in International Conference on Learning Representations, 2019. [Online]. Available: https://openreview.net/forum?id=B1xsqj09Fm\n\nConditional generative adversarial nets. M Mirza, S Osindero, arXiv:1411.1784arXiv preprintM. Mirza and S. Osindero, \"Conditional generative adversarial nets,\" arXiv preprint arXiv:1411.1784, 2014.\n\nUnsupervised learning of visual features by contrasting cluster assignments. M Caron, I Misra, J Mairal, P Goyal, P Bojanowski, A Joulin, Advances in Neural Information Processing Systems. 33M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin, \"Unsupervised learning of visual features by contrasting cluster assign- ments,\" Advances in Neural Information Processing Systems, vol. 33, pp. 9912-9924, 2020.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionK. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770-778.\n\nA self-supervised domain-general learning framework for human ventral stream representation. T Konkle, G A Alvarez, Nature Communications. 131T. Konkle and G. A. Alvarez, \"A self-supervised domain-general learning framework for human ventral stream representation,\" Nature Communications, vol. 13, no. 1, pp. 1-12, 2022.\n\nUnsupervised neural network models of the ventral visual stream. C Zhuang, S Yan, A Nayebi, M Schrimpf, M C Frank, J J Dicarlo, D L Yamins, Proceedings of the National Academy of Sciences. 1183C. Zhuang, S. Yan, A. Nayebi, M. Schrimpf, M. C. Frank, J. J. DiCarlo, and D. L. Yamins, \"Unsupervised neural network models of the ventral visual stream,\" Proceedings of the National Academy of Sciences, vol. 118, no. 3, 2021.\n\nCompletely derandomized selfadaptation in evolution strategies. N Hansen, A Ostermeier, Evolutionary Computation. 92N. Hansen and A. Ostermeier, \"Completely derandomized self- adaptation in evolution strategies,\" Evolutionary Computation, vol. 9, no. 2, pp. 159-195, 2001.\n\nThe unreasonable effectiveness of deep features as a perceptual metric. R Zhang, P Isola, A A Efros, E Shechtman, O Wang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionR. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, \"The unreasonable effectiveness of deep features as a perceptual metric,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 586-595.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556arXiv preprintK. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" arXiv preprint arXiv:1409.1556, 2014.\n\nGeneric decoding of seen and imagined objects using hierarchical visual features. T Horikawa, Y Kamitani, Nature communications. 81T. Horikawa and Y. Kamitani, \"Generic decoding of seen and imagined objects using hierarchical visual features,\" Nature communications, vol. 8, no. 1, pp. 1-15, 2017.\n\nImage quality assessment: from error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, IEEE transactions on image processing. 134Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, \"Image quality assessment: from error visibility to structural similarity,\" IEEE transactions on image processing, vol. 13, no. 4, pp. 600-612, 2004.\n\nRethinking the inception architecture for computer vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionC. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \"Rethinking the inception architecture for computer vision,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2818-2826.\n\nLearning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International Conference on Machine Learning. PMLRA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., \"Learning transferable visual models from natural language supervision,\" in International Conference on Machine Learning. PMLR, 2021, pp. 8748-8763.\n\nNeurogen: activation optimized image synthesis for discovery neuroscience. Z Gu, K W Jamison, M Khosla, E J Allen, Y Wu, T Naselaris, K Kay, M R Sabuncu, A Kuceyeski, NeuroImage. 247118812Z. Gu, K. W. Jamison, M. Khosla, E. J. Allen, Y. Wu, T. Naselaris, K. Kay, M. R. Sabuncu, and A. Kuceyeski, \"Neurogen: activation optimized image synthesis for discovery neuroscience,\" NeuroImage, vol. 247, p. 118812, 2022.\n\nRegional brain response to faces of humans and dogs. L X Blonder, C D Smith, C E Davis, M L Kesler, T F Garrity, M J Avison, A H Andersen, Cognitive Brain Research. 203L. X. Blonder, C. D. Smith, C. E. Davis, M. L. Kesler, T. F. Garrity, M. J. Avison, A. H. Andersen et al., \"Regional brain response to faces of humans and dogs,\" Cognitive Brain Research, vol. 20, no. 3, pp. 384-394, 2004.\n\nCenterperiphery organization of human object areas. I Levy, U Hasson, G Avidan, T Hendler, R Malach, Nature neuroscience. 45I. Levy, U. Hasson, G. Avidan, T. Hendler, and R. Malach, \"Center- periphery organization of human object areas,\" Nature neuroscience, vol. 4, no. 5, pp. 533-539, 2001.\n", "annotations": {"author": "[{\"end\":223,\"start\":120},{\"end\":313,\"start\":224},{\"end\":401,\"start\":314},{\"end\":497,\"start\":402},{\"end\":601,\"start\":498}]", "publisher": null, "author_last_name": "[{\"end\":134,\"start\":127},{\"end\":237,\"start\":231},{\"end\":328,\"start\":320},{\"end\":413,\"start\":408},{\"end\":513,\"start\":504}]", "author_first_name": "[{\"end\":126,\"start\":120},{\"end\":230,\"start\":224},{\"end\":319,\"start\":314},{\"end\":407,\"start\":402},{\"end\":503,\"start\":498}]", "author_affiliation": "[{\"end\":222,\"start\":165},{\"end\":312,\"start\":261},{\"end\":400,\"start\":353},{\"end\":496,\"start\":435},{\"end\":600,\"start\":539}]", "title": "[{\"end\":117,\"start\":1},{\"end\":718,\"start\":602}]", "venue": null, "abstract": "[{\"end\":2271,\"start\":803}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2747,\"start\":2744},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2766,\"start\":2763},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2771,\"start\":2768},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2805,\"start\":2802},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2810,\"start\":2807},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2867,\"start\":2864},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2955,\"start\":2952},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3108,\"start\":3105},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3119,\"start\":3116},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3144,\"start\":3140},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3733,\"start\":3729},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3798,\"start\":3794},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4320,\"start\":4316},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5576,\"start\":5572},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5704,\"start\":5700},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5926,\"start\":5922},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6080,\"start\":6076},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6218,\"start\":6214},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6300,\"start\":6296},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6326,\"start\":6322},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6747,\"start\":6743},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6897,\"start\":6893},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6912,\"start\":6908},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6955,\"start\":6951},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9282,\"start\":9278},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9304,\"start\":9300},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9469,\"start\":9465},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10080,\"start\":10076},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10086,\"start\":10082},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10152,\"start\":10148},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12126,\"start\":12122},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12784,\"start\":12780},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13278,\"start\":13274},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13327,\"start\":13323},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14438,\"start\":14434},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18598,\"start\":18597},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18867,\"start\":18863},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19150,\"start\":19146},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19247,\"start\":19243},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19865,\"start\":19861},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20014,\"start\":20010},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":20033,\"start\":20029},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":20066,\"start\":20062},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21083,\"start\":21079},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23403,\"start\":23399},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23425,\"start\":23421},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23446,\"start\":23442},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24625,\"start\":24621},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":26386,\"start\":26382},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":26392,\"start\":26388},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":27410,\"start\":27406},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":29117,\"start\":29113},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31042,\"start\":31038},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31060,\"start\":31056}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29957,\"start\":29879},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30235,\"start\":29958},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30748,\"start\":30236},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31060,\"start\":30749},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31868,\"start\":31061},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32519,\"start\":31869}]", "paragraph": "[{\"end\":2956,\"start\":2290},{\"end\":3488,\"start\":2958},{\"end\":4875,\"start\":3490},{\"end\":5408,\"start\":4877},{\"end\":6561,\"start\":5410},{\"end\":7642,\"start\":6622},{\"end\":7714,\"start\":7644},{\"end\":7767,\"start\":7716},{\"end\":8283,\"start\":7813},{\"end\":8909,\"start\":8321},{\"end\":9162,\"start\":8911},{\"end\":9604,\"start\":9164},{\"end\":10087,\"start\":9606},{\"end\":10904,\"start\":10089},{\"end\":11634,\"start\":10906},{\"end\":12591,\"start\":11691},{\"end\":13379,\"start\":12593},{\"end\":13436,\"start\":13381},{\"end\":13532,\"start\":13480},{\"end\":13734,\"start\":13575},{\"end\":14021,\"start\":13761},{\"end\":14157,\"start\":14023},{\"end\":14264,\"start\":14159},{\"end\":15183,\"start\":14303},{\"end\":15792,\"start\":15185},{\"end\":16024,\"start\":15838},{\"end\":16388,\"start\":16026},{\"end\":16755,\"start\":16390},{\"end\":17547,\"start\":16816},{\"end\":18398,\"start\":17549},{\"end\":22228,\"start\":18400},{\"end\":22833,\"start\":22286},{\"end\":23668,\"start\":22835},{\"end\":24597,\"start\":23670},{\"end\":25252,\"start\":24599},{\"end\":26637,\"start\":25254},{\"end\":26747,\"start\":26679},{\"end\":28152,\"start\":26749},{\"end\":28632,\"start\":28170},{\"end\":29558,\"start\":28634},{\"end\":29878,\"start\":29560}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7805,\"start\":7768},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8320,\"start\":8284},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13479,\"start\":13437},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13574,\"start\":13533},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13760,\"start\":13735},{\"attributes\":{\"id\":\"formula_5\"},\"end\":26678,\"start\":26638}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19529,\"start\":19522}]", "section_header": "[{\"end\":2288,\"start\":2273},{\"end\":6590,\"start\":6564},{\"end\":6620,\"start\":6593},{\"end\":7811,\"start\":7807},{\"end\":11689,\"start\":11637},{\"end\":14301,\"start\":14267},{\"end\":15836,\"start\":15795},{\"end\":16814,\"start\":16758},{\"end\":22284,\"start\":22231},{\"end\":28168,\"start\":28155},{\"end\":29971,\"start\":29959},{\"end\":30245,\"start\":30237},{\"end\":30758,\"start\":30750},{\"end\":31070,\"start\":31062},{\"end\":31890,\"start\":31870}]", "table": "[{\"end\":32519,\"start\":32057}]", "figure_caption": "[{\"end\":29957,\"start\":29881},{\"end\":30235,\"start\":29973},{\"end\":30748,\"start\":30247},{\"end\":31060,\"start\":30760},{\"end\":31868,\"start\":31072},{\"end\":32057,\"start\":31892}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3900,\"start\":3892},{\"end\":4525,\"start\":4516},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11752,\"start\":11744},{\"end\":13638,\"start\":13632},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13907,\"start\":13899},{\"end\":15912,\"start\":15904},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16898,\"start\":16890},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17065,\"start\":17057},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18463,\"start\":18455},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20157,\"start\":20149},{\"end\":20315,\"start\":20307},{\"end\":20813,\"start\":20805},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23820,\"start\":23812},{\"end\":24415,\"start\":24407},{\"end\":25074,\"start\":25066},{\"end\":27759,\"start\":27758}]", "bib_author_first_name": "[{\"end\":32817,\"start\":32816},{\"end\":32828,\"start\":32827},{\"end\":32841,\"start\":32840},{\"end\":32852,\"start\":32851},{\"end\":32865,\"start\":32861},{\"end\":32875,\"start\":32874},{\"end\":32886,\"start\":32885},{\"end\":33212,\"start\":33211},{\"end\":33224,\"start\":33223},{\"end\":33499,\"start\":33495},{\"end\":33509,\"start\":33508},{\"end\":33811,\"start\":33810},{\"end\":33813,\"start\":33812},{\"end\":33822,\"start\":33821},{\"end\":33824,\"start\":33823},{\"end\":33835,\"start\":33834},{\"end\":33837,\"start\":33836},{\"end\":33846,\"start\":33845},{\"end\":33855,\"start\":33854},{\"end\":33857,\"start\":33856},{\"end\":33869,\"start\":33868},{\"end\":34276,\"start\":34275},{\"end\":34278,\"start\":34277},{\"end\":34285,\"start\":34284},{\"end\":34287,\"start\":34286},{\"end\":34591,\"start\":34590},{\"end\":34593,\"start\":34592},{\"end\":34600,\"start\":34599},{\"end\":34613,\"start\":34612},{\"end\":34615,\"start\":34614},{\"end\":34626,\"start\":34625},{\"end\":34628,\"start\":34627},{\"end\":34927,\"start\":34926},{\"end\":34939,\"start\":34938},{\"end\":34949,\"start\":34948},{\"end\":34962,\"start\":34961},{\"end\":34970,\"start\":34969},{\"end\":34980,\"start\":34979},{\"end\":34982,\"start\":34981},{\"end\":34992,\"start\":34991},{\"end\":35002,\"start\":35001},{\"end\":35353,\"start\":35352},{\"end\":35369,\"start\":35368},{\"end\":35378,\"start\":35377},{\"end\":35388,\"start\":35387},{\"end\":35667,\"start\":35666},{\"end\":35680,\"start\":35679},{\"end\":35932,\"start\":35931},{\"end\":35940,\"start\":35939},{\"end\":35952,\"start\":35951},{\"end\":35962,\"start\":35961},{\"end\":36207,\"start\":36206},{\"end\":36219,\"start\":36218},{\"end\":36229,\"start\":36228},{\"end\":36240,\"start\":36239},{\"end\":36252,\"start\":36251},{\"end\":36264,\"start\":36261},{\"end\":36279,\"start\":36278},{\"end\":36290,\"start\":36289},{\"end\":36299,\"start\":36298},{\"end\":36696,\"start\":36695},{\"end\":36707,\"start\":36706},{\"end\":36709,\"start\":36708},{\"end\":36717,\"start\":36716},{\"end\":36719,\"start\":36718},{\"end\":36728,\"start\":36727},{\"end\":36740,\"start\":36739},{\"end\":37008,\"start\":37007},{\"end\":37020,\"start\":37019},{\"end\":37022,\"start\":37021},{\"end\":37034,\"start\":37033},{\"end\":37036,\"start\":37035},{\"end\":37330,\"start\":37329},{\"end\":37342,\"start\":37341},{\"end\":37351,\"start\":37350},{\"end\":37365,\"start\":37364},{\"end\":37378,\"start\":37377},{\"end\":37791,\"start\":37790},{\"end\":37800,\"start\":37799},{\"end\":37809,\"start\":37808},{\"end\":37818,\"start\":37817},{\"end\":37831,\"start\":37830},{\"end\":37840,\"start\":37839},{\"end\":38230,\"start\":38229},{\"end\":38239,\"start\":38238},{\"end\":38248,\"start\":38247},{\"end\":38258,\"start\":38257},{\"end\":38267,\"start\":38266},{\"end\":38280,\"start\":38279},{\"end\":38289,\"start\":38288},{\"end\":38566,\"start\":38565},{\"end\":38578,\"start\":38577},{\"end\":38587,\"start\":38586},{\"end\":38914,\"start\":38913},{\"end\":38925,\"start\":38924},{\"end\":39252,\"start\":39251},{\"end\":39259,\"start\":39258},{\"end\":39265,\"start\":39264},{\"end\":39272,\"start\":39271},{\"end\":39278,\"start\":39277},{\"end\":39286,\"start\":39285},{\"end\":39294,\"start\":39293},{\"end\":39597,\"start\":39596},{\"end\":39605,\"start\":39604},{\"end\":39613,\"start\":39612},{\"end\":39626,\"start\":39622},{\"end\":39632,\"start\":39631},{\"end\":39638,\"start\":39637},{\"end\":39984,\"start\":39983},{\"end\":39994,\"start\":39993},{\"end\":40003,\"start\":40002},{\"end\":40014,\"start\":40013},{\"end\":40026,\"start\":40025},{\"end\":40038,\"start\":40037},{\"end\":40500,\"start\":40499},{\"end\":40509,\"start\":40508},{\"end\":40520,\"start\":40519},{\"end\":40864,\"start\":40863},{\"end\":40873,\"start\":40872},{\"end\":41099,\"start\":41098},{\"end\":41108,\"start\":41107},{\"end\":41117,\"start\":41116},{\"end\":41127,\"start\":41126},{\"end\":41136,\"start\":41135},{\"end\":41150,\"start\":41149},{\"end\":41493,\"start\":41492},{\"end\":41499,\"start\":41498},{\"end\":41508,\"start\":41507},{\"end\":41515,\"start\":41514},{\"end\":41943,\"start\":41942},{\"end\":41953,\"start\":41952},{\"end\":41955,\"start\":41954},{\"end\":42237,\"start\":42236},{\"end\":42247,\"start\":42246},{\"end\":42254,\"start\":42253},{\"end\":42264,\"start\":42263},{\"end\":42276,\"start\":42275},{\"end\":42278,\"start\":42277},{\"end\":42287,\"start\":42286},{\"end\":42289,\"start\":42288},{\"end\":42300,\"start\":42299},{\"end\":42302,\"start\":42301},{\"end\":42658,\"start\":42657},{\"end\":42668,\"start\":42667},{\"end\":42940,\"start\":42939},{\"end\":42949,\"start\":42948},{\"end\":42958,\"start\":42957},{\"end\":42960,\"start\":42959},{\"end\":42969,\"start\":42968},{\"end\":42982,\"start\":42981},{\"end\":43435,\"start\":43434},{\"end\":43447,\"start\":43446},{\"end\":43710,\"start\":43709},{\"end\":43722,\"start\":43721},{\"end\":44001,\"start\":44000},{\"end\":44009,\"start\":44008},{\"end\":44011,\"start\":44010},{\"end\":44020,\"start\":44019},{\"end\":44022,\"start\":44021},{\"end\":44032,\"start\":44031},{\"end\":44034,\"start\":44033},{\"end\":44358,\"start\":44357},{\"end\":44369,\"start\":44368},{\"end\":44382,\"start\":44381},{\"end\":44391,\"start\":44390},{\"end\":44401,\"start\":44400},{\"end\":44848,\"start\":44847},{\"end\":44859,\"start\":44858},{\"end\":44861,\"start\":44860},{\"end\":44868,\"start\":44867},{\"end\":44879,\"start\":44878},{\"end\":44889,\"start\":44888},{\"end\":44896,\"start\":44895},{\"end\":44907,\"start\":44906},{\"end\":44917,\"start\":44916},{\"end\":44927,\"start\":44926},{\"end\":44938,\"start\":44937},{\"end\":45339,\"start\":45338},{\"end\":45345,\"start\":45344},{\"end\":45347,\"start\":45346},{\"end\":45358,\"start\":45357},{\"end\":45368,\"start\":45367},{\"end\":45370,\"start\":45369},{\"end\":45379,\"start\":45378},{\"end\":45385,\"start\":45384},{\"end\":45398,\"start\":45397},{\"end\":45405,\"start\":45404},{\"end\":45407,\"start\":45406},{\"end\":45418,\"start\":45417},{\"end\":45730,\"start\":45729},{\"end\":45732,\"start\":45731},{\"end\":45743,\"start\":45742},{\"end\":45745,\"start\":45744},{\"end\":45754,\"start\":45753},{\"end\":45756,\"start\":45755},{\"end\":45765,\"start\":45764},{\"end\":45767,\"start\":45766},{\"end\":45777,\"start\":45776},{\"end\":45779,\"start\":45778},{\"end\":45790,\"start\":45789},{\"end\":45792,\"start\":45791},{\"end\":45802,\"start\":45801},{\"end\":45804,\"start\":45803},{\"end\":46121,\"start\":46120},{\"end\":46129,\"start\":46128},{\"end\":46139,\"start\":46138},{\"end\":46149,\"start\":46148},{\"end\":46160,\"start\":46159}]", "bib_author_last_name": "[{\"end\":32825,\"start\":32818},{\"end\":32838,\"start\":32829},{\"end\":32849,\"start\":32842},{\"end\":32859,\"start\":32853},{\"end\":32872,\"start\":32866},{\"end\":32883,\"start\":32876},{\"end\":32894,\"start\":32887},{\"end\":33221,\"start\":33213},{\"end\":33229,\"start\":33225},{\"end\":33506,\"start\":33500},{\"end\":33514,\"start\":33510},{\"end\":33819,\"start\":33814},{\"end\":33832,\"start\":33825},{\"end\":33843,\"start\":33838},{\"end\":33852,\"start\":33847},{\"end\":33866,\"start\":33858},{\"end\":33878,\"start\":33870},{\"end\":34282,\"start\":34279},{\"end\":34293,\"start\":34288},{\"end\":34597,\"start\":34594},{\"end\":34610,\"start\":34601},{\"end\":34623,\"start\":34616},{\"end\":34636,\"start\":34629},{\"end\":34936,\"start\":34928},{\"end\":34946,\"start\":34940},{\"end\":34959,\"start\":34950},{\"end\":34967,\"start\":34963},{\"end\":34977,\"start\":34971},{\"end\":34989,\"start\":34983},{\"end\":34999,\"start\":34993},{\"end\":35011,\"start\":35003},{\"end\":35366,\"start\":35354},{\"end\":35375,\"start\":35370},{\"end\":35385,\"start\":35379},{\"end\":35399,\"start\":35389},{\"end\":35677,\"start\":35668},{\"end\":35686,\"start\":35681},{\"end\":35937,\"start\":35933},{\"end\":35949,\"start\":35941},{\"end\":35959,\"start\":35953},{\"end\":35971,\"start\":35963},{\"end\":36216,\"start\":36208},{\"end\":36226,\"start\":36220},{\"end\":36237,\"start\":36230},{\"end\":36249,\"start\":36241},{\"end\":36259,\"start\":36253},{\"end\":36276,\"start\":36265},{\"end\":36287,\"start\":36280},{\"end\":36296,\"start\":36291},{\"end\":36557,\"start\":36550},{\"end\":36704,\"start\":36697},{\"end\":36714,\"start\":36710},{\"end\":36725,\"start\":36720},{\"end\":36737,\"start\":36729},{\"end\":36748,\"start\":36741},{\"end\":37017,\"start\":37009},{\"end\":37031,\"start\":37023},{\"end\":37339,\"start\":37331},{\"end\":37348,\"start\":37343},{\"end\":37362,\"start\":37352},{\"end\":37375,\"start\":37366},{\"end\":37389,\"start\":37379},{\"end\":37797,\"start\":37792},{\"end\":37806,\"start\":37801},{\"end\":37815,\"start\":37810},{\"end\":37828,\"start\":37819},{\"end\":37837,\"start\":37832},{\"end\":37846,\"start\":37841},{\"end\":38236,\"start\":38231},{\"end\":38245,\"start\":38240},{\"end\":38255,\"start\":38249},{\"end\":38264,\"start\":38259},{\"end\":38277,\"start\":38268},{\"end\":38286,\"start\":38281},{\"end\":38295,\"start\":38290},{\"end\":38575,\"start\":38567},{\"end\":38584,\"start\":38579},{\"end\":38597,\"start\":38588},{\"end\":38922,\"start\":38915},{\"end\":38934,\"start\":38926},{\"end\":39256,\"start\":39253},{\"end\":39262,\"start\":39260},{\"end\":39269,\"start\":39266},{\"end\":39275,\"start\":39273},{\"end\":39283,\"start\":39279},{\"end\":39291,\"start\":39287},{\"end\":39298,\"start\":39295},{\"end\":39602,\"start\":39598},{\"end\":39610,\"start\":39606},{\"end\":39620,\"start\":39614},{\"end\":39629,\"start\":39627},{\"end\":39635,\"start\":39633},{\"end\":39646,\"start\":39639},{\"end\":39991,\"start\":39985},{\"end\":40000,\"start\":39995},{\"end\":40011,\"start\":40004},{\"end\":40023,\"start\":40015},{\"end\":40035,\"start\":40027},{\"end\":40043,\"start\":40039},{\"end\":40506,\"start\":40501},{\"end\":40517,\"start\":40510},{\"end\":40529,\"start\":40521},{\"end\":40870,\"start\":40865},{\"end\":40882,\"start\":40874},{\"end\":41105,\"start\":41100},{\"end\":41114,\"start\":41109},{\"end\":41124,\"start\":41118},{\"end\":41133,\"start\":41128},{\"end\":41147,\"start\":41137},{\"end\":41157,\"start\":41151},{\"end\":41496,\"start\":41494},{\"end\":41505,\"start\":41500},{\"end\":41512,\"start\":41509},{\"end\":41519,\"start\":41516},{\"end\":41950,\"start\":41944},{\"end\":41963,\"start\":41956},{\"end\":42244,\"start\":42238},{\"end\":42251,\"start\":42248},{\"end\":42261,\"start\":42255},{\"end\":42273,\"start\":42265},{\"end\":42284,\"start\":42279},{\"end\":42297,\"start\":42290},{\"end\":42309,\"start\":42303},{\"end\":42665,\"start\":42659},{\"end\":42679,\"start\":42669},{\"end\":42946,\"start\":42941},{\"end\":42955,\"start\":42950},{\"end\":42966,\"start\":42961},{\"end\":42979,\"start\":42970},{\"end\":42987,\"start\":42983},{\"end\":43444,\"start\":43436},{\"end\":43457,\"start\":43448},{\"end\":43719,\"start\":43711},{\"end\":43731,\"start\":43723},{\"end\":44006,\"start\":44002},{\"end\":44017,\"start\":44012},{\"end\":44029,\"start\":44023},{\"end\":44045,\"start\":44035},{\"end\":44366,\"start\":44359},{\"end\":44379,\"start\":44370},{\"end\":44388,\"start\":44383},{\"end\":44398,\"start\":44392},{\"end\":44407,\"start\":44402},{\"end\":44856,\"start\":44849},{\"end\":44865,\"start\":44862},{\"end\":44876,\"start\":44869},{\"end\":44886,\"start\":44880},{\"end\":44893,\"start\":44890},{\"end\":44904,\"start\":44897},{\"end\":44914,\"start\":44908},{\"end\":44924,\"start\":44918},{\"end\":44935,\"start\":44928},{\"end\":44944,\"start\":44939},{\"end\":45342,\"start\":45340},{\"end\":45355,\"start\":45348},{\"end\":45365,\"start\":45359},{\"end\":45376,\"start\":45371},{\"end\":45382,\"start\":45380},{\"end\":45395,\"start\":45386},{\"end\":45402,\"start\":45399},{\"end\":45415,\"start\":45408},{\"end\":45428,\"start\":45419},{\"end\":45740,\"start\":45733},{\"end\":45751,\"start\":45746},{\"end\":45762,\"start\":45757},{\"end\":45774,\"start\":45768},{\"end\":45787,\"start\":45780},{\"end\":45799,\"start\":45793},{\"end\":45813,\"start\":45805},{\"end\":46126,\"start\":46122},{\"end\":46136,\"start\":46130},{\"end\":46146,\"start\":46140},{\"end\":46157,\"start\":46150},{\"end\":46167,\"start\":46161}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":13361917},\"end\":33145,\"start\":32725},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":122956},\"end\":33399,\"start\":33147},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":11455850},\"end\":33715,\"start\":33401},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":6403660},\"end\":34124,\"start\":33717},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":17638165},\"end\":34534,\"start\":34126},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":54469209},\"end\":34814,\"start\":34536},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":17327816},\"end\":35281,\"start\":34816},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1738661},\"end\":35585,\"start\":35283},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":52945847},\"end\":35876,\"start\":35587},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":58667578},\"end\":36178,\"start\":35878},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":237491885},\"end\":36546,\"start\":36180},{\"attributes\":{\"id\":\"b11\"},\"end\":36643,\"start\":36548},{\"attributes\":{\"id\":\"b12\"},\"end\":36921,\"start\":36645},{\"attributes\":{\"doi\":\"arXiv:1911.05630\",\"id\":\"b13\"},\"end\":37240,\"start\":36923},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":51709940},\"end\":37696,\"start\":37242},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":195798779},\"end\":38128,\"start\":37698},{\"attributes\":{\"id\":\"b16\"},\"end\":38498,\"start\":38130},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":211003765},\"end\":38862,\"start\":38500},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":195820291},\"end\":39133,\"start\":38864},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":229935148},\"end\":39541,\"start\":39135},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":57246310},\"end\":39926,\"start\":39543},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":209202273},\"end\":40429,\"start\":39928},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":52889459},\"end\":40820,\"start\":40431},{\"attributes\":{\"doi\":\"arXiv:1411.1784\",\"id\":\"b23\"},\"end\":41019,\"start\":40822},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":219721240},\"end\":41444,\"start\":41021},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206594692},\"end\":41847,\"start\":41446},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":246247143},\"end\":42169,\"start\":41849},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":219947382},\"end\":42591,\"start\":42171},{\"attributes\":{\"id\":\"b28\"},\"end\":42865,\"start\":42593},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":4766599},\"end\":43364,\"start\":42867},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b30\"},\"end\":43625,\"start\":43366},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2354567},\"end\":43924,\"start\":43627},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":207761262},\"end\":44296,\"start\":43926},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":206593880},\"end\":44774,\"start\":44298},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":231591445},\"end\":45261,\"start\":44776},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":234741984},\"end\":45674,\"start\":45263},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":26730494},\"end\":46066,\"start\":45676},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":6441175},\"end\":46360,\"start\":46068}]", "bib_title": "[{\"end\":32814,\"start\":32725},{\"end\":33209,\"start\":33147},{\"end\":33493,\"start\":33401},{\"end\":33808,\"start\":33717},{\"end\":34273,\"start\":34126},{\"end\":34588,\"start\":34536},{\"end\":34924,\"start\":34816},{\"end\":35350,\"start\":35283},{\"end\":35664,\"start\":35587},{\"end\":35929,\"start\":35878},{\"end\":36204,\"start\":36180},{\"end\":37327,\"start\":37242},{\"end\":37788,\"start\":37698},{\"end\":38563,\"start\":38500},{\"end\":38911,\"start\":38864},{\"end\":39249,\"start\":39135},{\"end\":39594,\"start\":39543},{\"end\":39981,\"start\":39928},{\"end\":40497,\"start\":40431},{\"end\":41096,\"start\":41021},{\"end\":41490,\"start\":41446},{\"end\":41940,\"start\":41849},{\"end\":42234,\"start\":42171},{\"end\":42655,\"start\":42593},{\"end\":42937,\"start\":42867},{\"end\":43707,\"start\":43627},{\"end\":43998,\"start\":43926},{\"end\":44355,\"start\":44298},{\"end\":44845,\"start\":44776},{\"end\":45336,\"start\":45263},{\"end\":45727,\"start\":45676},{\"end\":46118,\"start\":46068}]", "bib_author": "[{\"end\":32827,\"start\":32816},{\"end\":32840,\"start\":32827},{\"end\":32851,\"start\":32840},{\"end\":32861,\"start\":32851},{\"end\":32874,\"start\":32861},{\"end\":32885,\"start\":32874},{\"end\":32896,\"start\":32885},{\"end\":33223,\"start\":33211},{\"end\":33231,\"start\":33223},{\"end\":33508,\"start\":33495},{\"end\":33516,\"start\":33508},{\"end\":33821,\"start\":33810},{\"end\":33834,\"start\":33821},{\"end\":33845,\"start\":33834},{\"end\":33854,\"start\":33845},{\"end\":33868,\"start\":33854},{\"end\":33880,\"start\":33868},{\"end\":34284,\"start\":34275},{\"end\":34295,\"start\":34284},{\"end\":34599,\"start\":34590},{\"end\":34612,\"start\":34599},{\"end\":34625,\"start\":34612},{\"end\":34638,\"start\":34625},{\"end\":34938,\"start\":34926},{\"end\":34948,\"start\":34938},{\"end\":34961,\"start\":34948},{\"end\":34969,\"start\":34961},{\"end\":34979,\"start\":34969},{\"end\":34991,\"start\":34979},{\"end\":35001,\"start\":34991},{\"end\":35013,\"start\":35001},{\"end\":35368,\"start\":35352},{\"end\":35377,\"start\":35368},{\"end\":35387,\"start\":35377},{\"end\":35401,\"start\":35387},{\"end\":35679,\"start\":35666},{\"end\":35688,\"start\":35679},{\"end\":35939,\"start\":35931},{\"end\":35951,\"start\":35939},{\"end\":35961,\"start\":35951},{\"end\":35973,\"start\":35961},{\"end\":36218,\"start\":36206},{\"end\":36228,\"start\":36218},{\"end\":36239,\"start\":36228},{\"end\":36251,\"start\":36239},{\"end\":36261,\"start\":36251},{\"end\":36278,\"start\":36261},{\"end\":36289,\"start\":36278},{\"end\":36298,\"start\":36289},{\"end\":36302,\"start\":36298},{\"end\":36559,\"start\":36550},{\"end\":36706,\"start\":36695},{\"end\":36716,\"start\":36706},{\"end\":36727,\"start\":36716},{\"end\":36739,\"start\":36727},{\"end\":36750,\"start\":36739},{\"end\":37019,\"start\":37007},{\"end\":37033,\"start\":37019},{\"end\":37039,\"start\":37033},{\"end\":37341,\"start\":37329},{\"end\":37350,\"start\":37341},{\"end\":37364,\"start\":37350},{\"end\":37377,\"start\":37364},{\"end\":37391,\"start\":37377},{\"end\":37799,\"start\":37790},{\"end\":37808,\"start\":37799},{\"end\":37817,\"start\":37808},{\"end\":37830,\"start\":37817},{\"end\":37839,\"start\":37830},{\"end\":37848,\"start\":37839},{\"end\":38238,\"start\":38229},{\"end\":38247,\"start\":38238},{\"end\":38257,\"start\":38247},{\"end\":38266,\"start\":38257},{\"end\":38279,\"start\":38266},{\"end\":38288,\"start\":38279},{\"end\":38297,\"start\":38288},{\"end\":38577,\"start\":38565},{\"end\":38586,\"start\":38577},{\"end\":38599,\"start\":38586},{\"end\":38924,\"start\":38913},{\"end\":38936,\"start\":38924},{\"end\":39258,\"start\":39251},{\"end\":39264,\"start\":39258},{\"end\":39271,\"start\":39264},{\"end\":39277,\"start\":39271},{\"end\":39285,\"start\":39277},{\"end\":39293,\"start\":39285},{\"end\":39300,\"start\":39293},{\"end\":39604,\"start\":39596},{\"end\":39612,\"start\":39604},{\"end\":39622,\"start\":39612},{\"end\":39631,\"start\":39622},{\"end\":39637,\"start\":39631},{\"end\":39648,\"start\":39637},{\"end\":39993,\"start\":39983},{\"end\":40002,\"start\":39993},{\"end\":40013,\"start\":40002},{\"end\":40025,\"start\":40013},{\"end\":40037,\"start\":40025},{\"end\":40045,\"start\":40037},{\"end\":40508,\"start\":40499},{\"end\":40519,\"start\":40508},{\"end\":40531,\"start\":40519},{\"end\":40872,\"start\":40863},{\"end\":40884,\"start\":40872},{\"end\":41107,\"start\":41098},{\"end\":41116,\"start\":41107},{\"end\":41126,\"start\":41116},{\"end\":41135,\"start\":41126},{\"end\":41149,\"start\":41135},{\"end\":41159,\"start\":41149},{\"end\":41498,\"start\":41492},{\"end\":41507,\"start\":41498},{\"end\":41514,\"start\":41507},{\"end\":41521,\"start\":41514},{\"end\":41952,\"start\":41942},{\"end\":41965,\"start\":41952},{\"end\":42246,\"start\":42236},{\"end\":42253,\"start\":42246},{\"end\":42263,\"start\":42253},{\"end\":42275,\"start\":42263},{\"end\":42286,\"start\":42275},{\"end\":42299,\"start\":42286},{\"end\":42311,\"start\":42299},{\"end\":42667,\"start\":42657},{\"end\":42681,\"start\":42667},{\"end\":42948,\"start\":42939},{\"end\":42957,\"start\":42948},{\"end\":42968,\"start\":42957},{\"end\":42981,\"start\":42968},{\"end\":42989,\"start\":42981},{\"end\":43446,\"start\":43434},{\"end\":43459,\"start\":43446},{\"end\":43721,\"start\":43709},{\"end\":43733,\"start\":43721},{\"end\":44008,\"start\":44000},{\"end\":44019,\"start\":44008},{\"end\":44031,\"start\":44019},{\"end\":44047,\"start\":44031},{\"end\":44368,\"start\":44357},{\"end\":44381,\"start\":44368},{\"end\":44390,\"start\":44381},{\"end\":44400,\"start\":44390},{\"end\":44409,\"start\":44400},{\"end\":44858,\"start\":44847},{\"end\":44867,\"start\":44858},{\"end\":44878,\"start\":44867},{\"end\":44888,\"start\":44878},{\"end\":44895,\"start\":44888},{\"end\":44906,\"start\":44895},{\"end\":44916,\"start\":44906},{\"end\":44926,\"start\":44916},{\"end\":44937,\"start\":44926},{\"end\":44946,\"start\":44937},{\"end\":45344,\"start\":45338},{\"end\":45357,\"start\":45344},{\"end\":45367,\"start\":45357},{\"end\":45378,\"start\":45367},{\"end\":45384,\"start\":45378},{\"end\":45397,\"start\":45384},{\"end\":45404,\"start\":45397},{\"end\":45417,\"start\":45404},{\"end\":45430,\"start\":45417},{\"end\":45742,\"start\":45729},{\"end\":45753,\"start\":45742},{\"end\":45764,\"start\":45753},{\"end\":45776,\"start\":45764},{\"end\":45789,\"start\":45776},{\"end\":45801,\"start\":45789},{\"end\":45815,\"start\":45801},{\"end\":46128,\"start\":46120},{\"end\":46138,\"start\":46128},{\"end\":46148,\"start\":46138},{\"end\":46159,\"start\":46148},{\"end\":46169,\"start\":46159}]", "bib_venue": "[{\"end\":40194,\"start\":40128},{\"end\":41662,\"start\":41600},{\"end\":43130,\"start\":43068},{\"end\":44550,\"start\":44488},{\"end\":32906,\"start\":32896},{\"end\":33250,\"start\":33231},{\"end\":33535,\"start\":33516},{\"end\":33887,\"start\":33880},{\"end\":34305,\"start\":34295},{\"end\":34644,\"start\":34638},{\"end\":35019,\"start\":35013},{\"end\":35412,\"start\":35401},{\"end\":35710,\"start\":35688},{\"end\":35999,\"start\":35973},{\"end\":36351,\"start\":36302},{\"end\":36693,\"start\":36645},{\"end\":37005,\"start\":36923},{\"end\":37401,\"start\":37391},{\"end\":37897,\"start\":37848},{\"end\":38227,\"start\":38130},{\"end\":38661,\"start\":38599},{\"end\":38985,\"start\":38936},{\"end\":39310,\"start\":39300},{\"end\":39711,\"start\":39648},{\"end\":40126,\"start\":40045},{\"end\":40583,\"start\":40531},{\"end\":40861,\"start\":40822},{\"end\":41208,\"start\":41159},{\"end\":41598,\"start\":41521},{\"end\":41986,\"start\":41965},{\"end\":42358,\"start\":42311},{\"end\":42705,\"start\":42681},{\"end\":43066,\"start\":42989},{\"end\":43432,\"start\":43366},{\"end\":43754,\"start\":43733},{\"end\":44084,\"start\":44047},{\"end\":44486,\"start\":44409},{\"end\":44990,\"start\":44946},{\"end\":45440,\"start\":45430},{\"end\":45839,\"start\":45815},{\"end\":46188,\"start\":46169}]"}}}, "year": 2023, "month": 12, "day": 17}