{"id": 257912849, "updated": "2023-10-05 02:49:38.649", "metadata": {"title": "LG-BPN: Local and Global Blind-Patch Network for Self-Supervised Real-World Denoising", "authors": "[{\"first\":\"Zichun\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Ying\",\"last\":\"Fu\",\"middle\":[]},{\"first\":\"Ji\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Yulun\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Despite the significant results on synthetic noise under simplified assumptions, most self-supervised denoising methods fail under real noise due to the strong spatial noise correlation, including the advanced self-supervised blind-spot networks (BSNs). For recent methods targeting real-world denoising, they either suffer from ignoring this spatial correlation, or are limited by the destruction of fine textures for under-considering the correlation. In this paper, we present a novel method called LG-BPN for self-supervised real-world denoising, which takes the spatial correlation statistic into our network design for local detail restoration, and also brings the long-range dependencies modeling ability to previously CNN-based BSN methods. First, based on the correlation statistic, we propose a densely-sampled patch-masked convolution module. By taking more neighbor pixels with low noise correlation into account, we enable a denser local receptive field, preserving more useful information for enhanced fine structure recovery. Second, we propose a dilated Transformer block to allow distant context exploitation in BSN. This global perception addresses the intrinsic deficiency of BSN, whose receptive field is constrained by the blind spot requirement, which can not be fully resolved by the previous CNN-based BSNs. These two designs enable LG-BPN to fully exploit both the detailed structure and the global interaction in a blind manner. Extensive results on real-world datasets demonstrate the superior performance of our method. https://github.com/Wang-XIaoDingdd/LGBPN", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2304.00534", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/WangFLZ23", "doi": "10.1109/cvpr52729.2023.01741"}}, "content": {"source": {"pdf_hash": "35124a6d02c876903f3861f3fb339aac19059f82", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2304.00534v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "9c820ad3d6b6c20c9a320cc7ba60176373a52580", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/35124a6d02c876903f3861f3fb339aac19059f82.txt", "contents": "\nLG-BPN: Local and Global Blind-Patch Network for Self-Supervised Real-World Denoising\n\n\nZichun Wang \nBeijing Institute of Technology\n\n\nYing Fu fuying@bit.edu.cn \nBeijing Institute of Technology\n\n\nJi Liu \nBaidu Inc\nBeijingChina\n\nYulun Zhang yulun100@gmail.com \nETH Z\u00fcrich\n\n\nLG-BPN: Local and Global Blind-Patch Network for Self-Supervised Real-World Denoising\n\nDespite the significant results on synthetic noise under simplified assumptions, most self-supervised denoising methods fail under real noise due to the strong spatial noise correlation, including the advanced self-supervised blindspot networks (BSNs). For recent methods targeting realworld denoising, they either suffer from ignoring this spatial correlation, or are limited by the destruction of fine textures for under-considering the correlation. In this paper, we present a novel method called LG-BPN for self-supervised real-world denoising, which takes the spatial correlation statistic into our network design for local detail restoration, and also brings the long-range dependencies modeling ability to previously CNN-based BSN methods. First, based on the correlation statistic, we propose a denselysampled patch-masked convolution module. By taking more neighbor pixels with low noise correlation into account, we enable a denser local receptive field, preserving more useful information for enhanced fine structure recovery. Second, we propose a dilated Transformer block to allow distant context exploitation in BSN. This global perception addresses the intrinsic deficiency of BSN, whose receptive field is constrained by the blind spot requirement, which can not be fully resolved by the previous CNN-based BSNs. These two designs enable LG-BPN to fully exploit both the detailed structure and the global interaction in a blind manner. Extensive results on real-world datasets demonstrate the superior performance of our method. https: //github.com/Wang-XIaoDingdd/LGBPN * Corresponding Author SIDD Validation: 0018-0031 DnCNN [36] C2N [15] R2R [26] 31.17/0.778 28.09/0.706 30.37/0.770 CVF-SID [25] AP-BSN [20] LG-BPN (Ours) 28.56/0.792 31.92/0.826 32.76/0.897\n\nIntroduction\n\nImage denoising is a fundamental research topic for lowlevel vision [7,36]. Noise can greatly degrade the quality of the captured images, thus bringing adverse impacts on the subsequent downstream tasks [22,32]. Recently, with the rapid development of neural networks, learning-based methods have shown significant advances compared with traditional model-based algorithms [5,8,10,11]. Figure 1. Visual comparison of various methods on the SIDD validation [1] dataset. Compared with DnCNN [36], C2N [15] and R2R [15], LG-BPN can be trained in a self-supervised manner without extra data. CVF-SID [25] still contains noise in the output, and AP-BSN [20] suffers from the loss of details.\n\nUnfortunately, learning-based methods often rely on massive labeled image pairs for training [2,34,35]. This can not be simply addressed by synthesizing additive white Gaussian noise (AWGN) pairs, since the gap between AWGN and real noise distribution severely degrades their performance in the real world [2,12]. To this end, several attempts have been made for collecting real-world datasets [1,4]. Nonetheless, its application is still hindered by the rigorously-controlled and labor-intensive collection procedure. For instance, capturing ground truth images requires long exposure or multiple shots, which is unavailable in complex situations, e.g., dynamic scenes with motion.\n\nTo alleviate the constraint of the large-scale paired dataset, methods without the need for ground truth have attracted increasing attention. The pioneer work Noise2Noise (N2N) [21] uses paired noisy observations for training, which can be applied when clean images are not available. Still, obtaining such noisy pairs under the same scene is less feasible. To make self-supervised methods more practical, researchers seek to learn from one, instead of pairs of observations. Among these methods, blind-spot networks (BSNs) [3,17,19,30] show significant advances to restore clean pixels by utilizing neighbor pixels, with a special blind spot receptive field requirement. Despite their promising results on simple noise such as AWGN, these methods usually work under simplified assumptions, e.g., the noise is pixel-wise independent. This obviously does not hold for real noise, where the distribution can be extremely complex and present a strong spatial correlation.\n\nAccordingly, a few methods have been proposed for selfsupervised real noise removal. Recorrupted-to-Recorrupted (R2R) [26] tries to construct noisy-noisy pairs, while it can not be directly applied without extra information, which is not practical in real situations. CVF-SID [25] disentangles the noise components from noisy images, but it assumes the real noise is spatially invariant and ignores the spatial correlation, which contradicts real noise distribution.\n\nRecently, AP-BSN [20] combines pixel-shuffle downsampling (PD) with the blind spot network (BSN). Though PD can be utilized to meet the noise assumption of BSN, simply combining PD with CNN-based BSN is sub-optimal for dealing with spatially-correlated real noise. It causes damage to local details, thus bringing artifacts to the subsampled images, e.g., aliasing artifact, especially for large PD stride factors [20,38]. Also, though more advanced designs of BSNs have been proposed [18,19,31], CNN-based BSNs fail to capture long-range interactions due to their convolution operator, which is further bounded by the limited receptive field under the blind spot requirement.\n\nIn this paper, we present a novel method, called LG-BPN, to address these issues on self-supervised real image denoising, including the reliance on extra information, the loss of local structures by noise correlation, and also the lacking of modeling distant pixel interaction. LG-BPN can be directly trained without external information. Furthermore, we ease the destruction of fine textures by carefully considering the spatial correlation in real noise, at the same time injecting long-range interaction by tailoring Transformers to the blind spot network. First, for local information, we introduce a densely-sampled patch-masked convolution (DSPMC) module. Based on the prior statistic of real noise spatial correlation, we take more neighbor pixels into account with a denser receptive field, allowing the network to recover more detailed structures. Second, for global information, we introduce a dilated Transformer block (DTB). Under the special blind spot requirement, this greatly enlarges the receptive field compared with previous CNN-based BSNs, permitting more neighbors to be utilized when predicting the central blind spot pixel. These two designs enable us to fully exploit local and global information, respectively. Extensive studies demonstrate that LG-BPN outperforms other state-of-the-art un-/self-supervised methods on real image denoising, as shown in Figure 1. We summarize our contributions as follows:\n\n\u2022 We present a novel self-supervised method called LG-BPN for real-world image denoising, which can effectively encode both the local detailed structure and the capture of global representation.\n\n\u2022 Based on the analysis of real noise spatial correlation, we propose DSPMC module, which takes advantage of the higher sampling density on the neighbor pixels, enabling a denser receptive field for improved local texture recovery.\n\n\u2022 To establish long-distance dependencies in previous CNN-based BSN methods, we introduce DTB, which aggregates global context while complying with the constraint of blind spot receptive field.\n\n\nRelated Work\n\n\nSupervised Image Denoising\n\nDnCNN [36] is the first attempt to apply deep learning techniques to the image denoising task, where the training pairs are synthesized by additive white Gaussian noise (AWGN). Following DnCNN, several methods have been proposed for AWGN noise removal. For instance, FFD-Net [37] advances it by taking the noise map as additional input. While achieving superior performance on AWGN removal, recent studies [2,12] reveal the poor generalization ability of these models when applied to real noise, due to the gap between the noise distribution. The primary obstruction of real image denoising lies in the deficiency of real noisy-clean pairs. To this end, some real-world denoising datasets are collected under carefully considered conditions [1,4]. Based on these datasets, several methods [7,12] train the network directly on the real image pairs. Despite the decent performance, data collection can be extremely expensive and labor-intensive. Also, it is infeasible to collect clean images under complex scenes containing motion.\n\n\nUnsupervised Image Denoising\n\nAnother line of research focuses on the situation where paired data is unavailable, including i) generating pseudo noisy-clean image pairs, ii) generating pseudo noisy-noisy image pairs, and iii) training directly on noisy images. Generating pseudo noisy-clean image pairs. In situations where unpaired noisy-clean data is available, generationbased methods seek to synthesize real noise on clean images for aligned training data, which can be used for supervised methods. Inspired by the generative adversarial network (GAN), GCBD [6] synthesizes realistic noisy images to train the denoising network, while its performance is limited by the inaccurate consideration of noise components. UIDNet [31] takes a step further by combining the distilled knowledge of the self-supervised denoising network and extra information from synthetic pairs. C2N [15] considers various noise components in real-world scenarios for more accurate noise synthesis. However, dealing with the gap between unpaired data is still challenging. The mismatch in scene distribution can result in inaccurate generation, thus degrading the quality of synthesized data.    Figure 2. The overall architecture of LG-BPN. Our method is composed of two branches, aiming at extracting local textures and global interactions, respectively. For each branch, the input first goes through a DSPMC module, then further processed for the deep feature. Finally, the output of two branches is fused for the denoised result. 1 Generating pseudo noisy-noisy image pairs. The semisupervised method Noise2Noise [21] uses multiple noisy images for training, which can be applied without clean images. However, the acquisition of multiple independent observations under the same scene is still less practical. Therefore, several methods seek to construct noisy-noisy pairs from a single noisy image. Neighbor2Neighbor [14] generates two sub-sampled images under simplified noise assumptions. To handle complex noise distribution in real images, several methods have been proposed, including Nois-ier2Noise [24], NAC [33] and R2R [26]. Still, these methods either require prior knowledge or are limited by specific constraints, which can be impractical in real situations. Specifically, Noisier2Noise [24] requires noise distribution when synthesizing noisy/noisy pairs. NAC [33] works under the assumption that the noise level is relatively weak. R2R [26] also uses additional information, e.g., noise level function (NLF) and image signal processing (ISP) function. Training directly on noisy images. Another type of method follows a self-supervised manner, which can be directly trained on the noisy images and free of synthesizing pseudo image pairs. Noise2Void [17] and Noise2Self [3] propose the self-supervised blind-spot strategy by masking the corresponding central pixel. Laine19 [19] and D-BSN [31] are further proposed for advanced BSN designs, while the convolution-based architecture limits their exploitation for long-range dependencies. To ease the information loss by the blind spot, Blind2Unblind [30] introduces a novel re-visible loss term. Unfortunately, the above-mentioned methods work under the assumption that noise is pixel-wise independent, thus inevitably learning identity mapping under spatially-correlated real noise. Towards real image denoising, CVF-SID [25] disentangles the noise components from the clean images, but it assumes the noise is spatiallyuncorrelated, which does not match the real noise distribution. Asymmetric pixel shuffle downsampling BSN (AP-BSN) [20] combines the pixel shuffle downsampling (PD) with a CNN-based BSN [31]. While achieving promising results, local structures are damaged by directly applying the PD operation to the image. Sub-sampled images are corrupted by various artifacts, e.g., aliasing artifacts, which are more pronounced under a large PD stride factor [20,38]. Also, adopting the CNN-based BSN leads to a limited receptive field. Since BSN recovers the central pixels based on its neighbors, fewer available neighbor pixels inevitably lead to performance loss. In summary, this results in the inadequate utilization of information with respect to both local and global contexts. Instead, our method benefits from a denser sampling density for improved local detail extraction, and also enjoys the distant pixel modeling ability for the enlarged receptive field.\n\n\nMethod\n\nWe first illustrate the overall architecture of LG-BPN in Figure 2, then elaborate on our motivation, and demonstrate our two core designs: DSPMC and DTB.\n\n\n3*3 Central\n\nMasked Convolution Input\n\nSub-sampled Input  Figure 3. Comparison of receptive field for the central pixel of AP-BSN and our method. Green pixels contribute to the restoration of central pixel. Blue pixels represent convolution kernel. We realize a denser receptive field by utilizing more neighbor pixels. 2 \n\n\nAP-BSN\n\n\nMotivation and Modeling\n\nDespite the decent results on simple synthetic noise removal, the performance of self-supervised denoising methods declines significantly when dealing with real noise, due to its strong spatial noise correlation. This easily breaks the assumption on which most state-of-the-art methods are based, i.e., noise is pixel-wise independent. These methods assume the clean signal of the central pixel is dependent on neighbors, while the noise is independent instead. Thus under real scenarios, they inevitably misinterpret the spatiallydependent noise as clean signals, and fail to recover the underlying clean images. Consequently, careful consideration of the spatial correlation is a must for self-supervised real noise removal. While the existing methods either struggle with poor results when completely ignoring this correlation, or suffer details loss from ill consideration. For example, AP-BSN [20] meets the assumption of the powerful BSN by adopting PD on the input image. As shown in Figure  3, though this breaks the spatial correlation, the sampling density is dramatically decreased by the PD. This severely degrades the extraction of fine details based on the Nyquist-Shannon sampling theorem, i.e., the fidelity of the results shows a positive correlation to sampling density.\n\nBesides, though BSN is already adapted for state-of-theart performance [20], its potential is still heavily hampered by their inherent shortage, i.e., the limit on the receptive field by the masked pixels for avoiding identity mapping. Despite the recent advanced BSN designs [19,31], CNNbased BSNs are still unable to fully address this issue due to their local convolution operator and fail to model the longdistance dependencies. This adversely affects the perfor- 2 We adopt 9\u00d79 DSPMC in the local branch. Here, the 11\u00d711 size is shown for illustration purposes to better compare the receptive field. mance of BSN, as the number of neighbors around the blind spot used for inferring is dramatically reduced. We aim to tackle both of these challenges in our methods. First, we realize better extraction for detailed textures from the perspective of sampling density. As shown in Figure 3, DSPMC enables a denser sampling rate by leveraging more neighbor pixels, which raises the upper limit of reconstruction quality. Second, by tailoring normal Transformers to a blind fashion, DTB is introduced for its powerful global modeling ability to compensate for the limited receptive field of CNN-based BSNs.\n\nBased on these two modules, we now introduce the overall architecture of our method. As shown in Figure 2, LG-BPN is mainly composed of two branches in parallel, aiming at local and global contexts reconstruction respectively. For the local feature extraction branch, we first apply the 9 \u00d7 9 DSPMC module. The densely extracted features are then down-sampled to break the spatial correlation. Then, the feature maps go through dilated convolution with a dilation of 2. For the global branch, the input image first goes through a 21 \u00d7 21 DMPMC module with a larger receptive field, which is then processed by DTBs. Finally, the local and global information from the two branches is fused together for the final output.\n\n\nDensely-Sampled Patch-Masked Convolution\n\nNeither adopting a low sample rate nor sampling all neighbor pixels can be an optimal choice for BSN when tacking real noise. In DSPMC, we aim to extract as much local information as possible, at the same time avoiding misinterpretation by these strongly-correlated neighbor pixels. To this end, we start by presenting the relationship between spatial correlation and the relative position, as shown in Figure 4. Following previous works [20,38], we use Pearson's correlation coefficient to depict the relationship. Specifically, we first obtain the noise map by subtracting the clean images from the noisy images in SIDD medium [1] (a) Kernel location during training (b) Shifted kernel location during testing (c) Shifted kernel location during testing with dilation Figure 5. Illustration of shifting kernel strategy and dilation of DSPMC. The blue and green points are the kernel locations during training and testing respectively. Yellow arrows indicate the shift direction. This avoids the strongly correlated pixels when training, sampling pixels closer to the center when testing for more details.\n\ndataset. Then, correlation coefficient can be calculated by:\n\u03c1 Ncen,Nnei = cov(N cen , N nei ) \u03c3 Ncen \u03c3 Nnei ,(1)\nwhere N cen and N cen represent the noise of the central pixel and neighbor pixels respectively. In Figure 4(a), we find that there exist more neighbor pixels which can also be leveraged for prediction. These pixels are not strongly correlated to the center pixel, thus bringing useful information, instead of misinterpretation to BSN. Then, the DSPMC kernel can be calculated as:\nK DSP M C = K n \u2299 Mask cor ,(2)\nwhere K DSP M C is the kernel of DSPMC, K n is the kernel of the normal convolution, and Mask cor is the mask for filtering out highly-correlated pixels shown in Figure 4(b). By integrating this noise distribution prior to sampling locations, our module can effectively take more neighbor pixels while avoiding the strongly correlated pixels. This enables the extracted feature to contain more fine details, and a denser receptive field as well. Also, as shown in Figure 3, since the extracted high-dimension feature already gathers the rich local details, the subsequent feature-level PD can save more useful information compared with the previous image-level PD.\n\nHowever, directly applying this module is not an optimal choice: i) the inference stage requires more details compared with training, so directly using the same architecture can damage high-frequency details [20], ii) a large kernel can cause computational inefficiency. Kernel shift strategy. As shown in Figure 5(b), for the first concern, we need to obtain more details while testing, focusing on local detailed information closer to the center pixel. Inspired by the deformable convolution [9], we apply a set of fixed offsets on the kernel for each location, which enforce that the kernels are more gathered in the center: where K is the kernel sampling locations, w k is the kernel weight, x(p) and y(p) denote the features at p in input feature x and output feature y, and \u2206p is the applied kernel offset. Ratio is the extent we shift the kernel while testing. By adding offsets to the kernel, we can shrink the kernel and capture finer details while testing. Dilation in DSPMC. As shown in Figure 5(c), for the second concern, we further decrease the computational cost by adding dilation to the convolution kernel. This imposed sparsity makes our DSPMC computationally efficient especially for large kernel size, at the same time maintaining the dense receptive field for capturing detailed structures. Visualization of the extracted feature map. To validate that our DSPMC module achieves a denser receptive field and is thus better at extracting local high-frequency structures, we present the visualization of the feature map. We select the output feature of the local extraction branch, and the corresponding location in AP-BSN. All channels are averaged and normalized for visualization. As shown in Figure 6, in AP-BSN [20], the local fine texture is damaged due to the insufficient use of neighbor signals. Instead, by leveraging more neighbor pixels, our feature map shows shaper edges and preserves more details.\ny(p 0 ) = K k=1 w k \u00b7 x (p 0 + p k + \u2206p k ) , \u2206p k = Ratio * (p k \u2212 p 0 ),(3)\n\nDilated Transformer Block\n\nThe receptive field of BSN is restricted by the imposed blind spots, while the local operator in CNN-based BSNs further prevents it from gathering global interaction. However, under the special blind spot constraint on the receptive field, it is non-trivial to directly introduce normal Transformer blocks to the BSN. Inspired by the D-BSN [31], we aim to design the Transformer block without information exchange between spatially-adjacent pixels, which satisfies the blind spot requirement when combined with DSPMC. Under these requirements, we carefully consider the design of two core components in the Transformer: the selfattention calculation and the feed-forward layer.\n\nFirst, for the self-attention layer, spatial-wise attention enables spatial information exchange and thus does not meet our receptive field requirement. Recently, a grid-like self-attention can meet our requirement [28], while the grid pattern further narrows the receptive field that the blind spot (a) AP-BSN [20] (b) LG-BPN (Ours) Figure 7. The comparison of the receptive field between AP-BSN [20] and our method. We calculate the gradient response to the central pixel. The brighter color represents the higher contribution for recovering the central pixel.\n\nhas reduced. Instead, we adopt channel-wise attention [35] for its unawareness of spatial location and global perception as well. Furthermore, to enhance the local context while preventing information of adjacent pixels, we introduce dilated depth-wise convolution before computing feature similarity. For the input feature X, the Query (Q), Key (K) and Value (V) matrix is thus calculated by Q = g Q (X), K = g K (X), V = g V (X), where g Q (\u00b7), g K (\u00b7) and g V (\u00b7) denote the dilated 3 \u00d7 3 depth-wise convolution. Given the Q, K and V matrix, the channel interaction can be obtained by the dot-product, where the attention map is of size R C\u00d7C , and C is the number of channels. The overall selfattention layer is represented as:\nAttention(Q, K, V) =V Softmax(KQ), X = Attention(Q, K, V) + X,(4)\nwhere X andX denote the input and output features. Second, for the feed-forward layer, adjacent information exchange can be simply avoided by adopting 1 \u00d7 1 convolutions only. Nonetheless, this fails to capture local context, which can be critical for restoring high-frequency details. We address this issue by also introducing dilation into the normal 3 \u00d7 3 convolution in the feed-forward layer. Then, features extracted by dilated depth-wise convolution go through a gating unit for the non-linearity. This gating unit is the element-wise product of two parallel paths, with one of them activated by the GELU unit. The overall process of the feed-forward layer is formulated as: (LN(X)),\nG 1 = g 1G 2 = g 2 (LN(X)), X = GELU (G 1 ) \u2299 G 2 + X,(5)\nwhere \u2299 is element-wise multiplication, LN denotes layer normalization, g 1 (\u00b7) and g 2 (\u00b7) represent the 3 \u00d7 3 dilated depth-wise convolution. An additional benefit is that, compared to the normal 3 \u00d7 3 convolution, the introduced dilation can also enlarge the receptive field.\n\nTo prove the effectiveness of the introduced global dependencies, we also plot the receptive field for recovering the central pixel of our method and the CNN-based BSNs in Figure 7. By injecting the long-range interaction into the blind spot network, more neighbor pixels are activated for restoring the central pixels in our method, offering a broader receptive field compared to the previous CNN-based BSNs.\n\n\nExperiments\n\n\nDataset and Setup Details\n\nWe train and evaluate our method on two real-world datasets, i.e., SIDD [1] and DND [27]. Note that for the SIDD benchmark dataset and DND benchmark dataset, we submit the output to the website for online evaluation. Smartphone Image Denoising Dataset (SIDD) [1] contains paired images for real-world denoising by five smartphone cameras. For training, we use the sRGB images from SIDD-Medium including 320 pairs. For validation and evaluation, we use the sRGB images from the SIDD validation set and benchmark set respectively. Each includes 1280 patches of size 256 \u00d7 256, where the ground truth images are also provided for the validation set. Darmstadt Noise Dataset (DND) [27] contains 50 noisy images for benchmarking without the ground truth provided, and the results can only be obtained via the online submission system. Therefore, we enjoy a fully selfsupervised manner and directly train our method on the test set without extra eternal data.\n\n\nTraining Details\n\nDuring training, we keep the same setting as the previous work [20]. Specifically, a batch size of 8 is used in the experiment. We adopt L 1 loss between ground truth and output for training. The learning rate starts with 1e-4, where Adam optimizer is adopted. The network is trained with 20 epochs until it fully converges. We implement the method in PyTorch 1.8.0, and train our model on the Nvidia RTX 3090. Two metrics are utilized to evaluate the performance of methods, including peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) [29]. The larger value of PSNR and SSIM implies better fidelity.\n\n\nEvaluation of Real-world Denoising\n\nWe validate the effectiveness of our method for realworld image denoising on the commonly-used SIDD benchmark dataset and DND benchmark dataset. Table 1 shows the comparison of various methods on SIDD and DND benchmark datasets. Visualization results of several methods addressed in Table 1 on SIDD and DND datasets can be found in Figure 8 and Figure 9. We achieve better results in quantitive and qualitative metrics than previous un-/selfsupervised methods. Compared with unsupervised methods trained on unpaired clean-noisy data, LG-BPN does not rely on extra data for synthesizing training pairs, also avoiding the misalignment between the scene distribution.  Table 1. Quantitative comparison of various methods on SIDD and DND benchmark datasets. Though several supervised methods achieve better results using noisy/clean image pairs, our methods use noisy RGB images only. Results with \u22c6 mean these are reproduced and evaluated by ourselves, since they are not evaluated on the dataset we use in their original paper. The results marked with \u2020 are reported from R2R [26]. Otherwise, we report the official results from SIDD and DND benchmark websites.\n\nself-supervised methods, NAC [33] works under the weak noise level assumption, while R2R [26] can not be directly applied to sRGB images without extra NLF and ISP functions, both of which harm their performance in real-world situations. In contrast, LG-BPN can be directly applied and not restricted by these assumptions. For methods leveraging single noisy observations, CVF-SID [25] does not consider the strong spatial correlation property in real noise, thus real noise can not be fully removed as shown in Figure 8. AP-BSN [20] suffers from inadequate sample locations with a limited receptive field, so details are blurred as shown in Figure 9. Instead, LG-BPN carefully integrates the spatial correlation into the network design, simultaneously modeling distant context dependencies.\n\n\nAnalysis of the Proposed Method\n\nDilation factor in densely-sampled convolution. Directly introducing densely-sample convolution can be computationally expensive. To balance the efficiency and the performance, we further introduce the sparsity to the convolution kernel by adding dilation to the original DSPMC. Figure  5(c) shows the illustration of the dilation.\n\nTo explore the better trade-off between performance and efficiency, we provide ablation studies on the dilation rate for our DSPMC in both local and global extraction branches. As shown in Table 2(a), a dilation of 1 for 9 \u00d7 9 DSPMC and 2 for 21\u00d721 DSPMC achieve a better balance.\n\nWe claim its reason is that the difference in kernel size results in focusing on varied scales of information. This imposes different sensitivity to the dilation, i.e., sampling density. For relatively small 9 \u00d7 9 kernels, it focuses more on the local textures, thus adding dilation can notably lower its ability when reconstructing detailed structures. While for the 21 \u00d7 21 kernel, the larger kernel size makes it aim at the global context more. Thus, the introduced dilation does not severely harm its global extraction ability, at the same time reducing the computational cost. The exploitation of local and global information.\n\nLG-BPN consists of two branches in parallel. Specifically, we combine the small 9 \u00d7 9 DSPMC with dilated convolution and the large 21 \u00d7 21 DSPMC with DTB, focusing on local and global context processing respectively. To validate the reasonableness of our network architecture, we conduct ablation studies on these components.   DSPMC in local and global feature extraction branches make them focus on various scales of features. Such discrepancy across the scale demands us to treat scale-specific characteristics in a different way. When replacing the dilated convolution in the local branch with our DTB, the lack of local connectivity brings inferior performance. Similarly, when replacing the DTB in the global branch with the dilated convolution, the network is built by convolutions only. It induces a lack of long-range interaction, which greatly limits their recovery quality as well. This proves the superior design of our architecture. By exploiting the locality of convolution with a small DSPMC, also the global dependencies of DTB with a large DSPMC, our architecture enjoys more reasonable exploitation for multi-scale context.\n\nThe effect of kernel shift strategy. Since the requirement for pixel-wise independent noise is different in training and testing, directly applying the same kernel of the training phase while testing will lose image details [20]. We proposed a kernel shift strategy, as illustrated in Figure 5(b). In Table 2(b), the lack of kernel shift causes 1.68 dB drops, proving the effectiveness of our shifting paradigm.\n\n\nConclusion\n\nIn this paper, we propose LG-BPN for self-supervised real image denoising, aiming to address the details lost by the coarse consideration for real noise correlation, and the lack of global interaction by the inherent constraint on the receptive field for BSN. First, we propose DSPMC to fully preserve the local structures. Owing to a denser receptive field, we ease the destruction of fine textures and can thus better reconstruct details. Second, we propose DTB, injecting distant interactions into the previously CNN-based blind spot networks. Since blind spot networks rely on neighbor signals for predicting, more clues can be provided by activating more neighbor pixels. Extensive results on realworld datasets reveal the superior performance of LG-BPN.\n\nFigure 4 .\n4Visualization of spatial correlation in real noise. (a) Bar plot of spatial correlation calculated by the correlation coefficient. Note that we scale the height by log norm for better visualization. The higher bar indicates a stronger correlation. (b) The mask of high-correlation pixels. Locations with 0 mean this pixel is strongly correlated to the central pixel, while 1 means not.\n\nFigure 6 .\n6The comparison of the feature map visualization of our method and AP-BSN[20] in the training phase. Our feature map shows clearer edges, validating the superiority of local details extraction of DSPMC by imposing denser sampling locations.\n\nFigure 9 .\n9Visual quality comparison on DND benchmark dataset.\n\nTable 2 (\n2b) shows the results of different combinations.It can be seen that removing the DSPMC of size 9\u00d79 and 21 \u00d7 21 in either branch severely degrades the performance. It is on account of the insufficient sampling density, which limits the reconstruction quality according to the Nyquist-Shannon sampling theorem. In this situation, the sampling density is made severely sparse, which causes insufficient utilization of input signals. This performance drop demonstrates the effectiveness of our DSPMC module.Furthermore, we validate that the different sizes ofFigure 8. Visual quality comparison on SIDD benchmark dataset. Note that the quantitative results are not available. (a) Ablation studies on dilation rates. Different dilation rate combinations are explored on the DSPMC in the local branch and global branch.(b) Ablation studies on our proposed method. Improvements can be found with our proposed modules and network design.SIDD benchmark input \n\nN2V [17] \nDnCNN [36] \nAWGN-M [38] \nC2N [15] \n\nR2R [26] \nCVF-SID [25] \nAP-BSN [20] \nLG-BPN (Ours) \n\n9x9 DSPMC \ndilation \n\n21x21 DSPMC \ndilation \nPSNR SSIM FLOPS (G) Params (M) \n\n1 \n1 \n37.23 0.885 \n88.6 \n1.35 \n1 \n2 \n37.32 0.886 \n29.8 \n0.45 \n1 \n3 \n37.23 0.883 \n17.1 \n0.26 \n2 \n1 \n36.85 0.879 \n84.6 \n1.23 \n2 \n2 \n36.84 0.875 \n25.8 \n0.39 \n2 \n3 \n36.99 0.873 \n13.1 \n0.20 \n\nMethod \nPSNR \nSSIM \n\nw/o 9\u00d79 DSPMC \n35.82 \n0.855 \nw/o 21\u00d721 DSPMC \n36.21 \n0.869 \nReplacing Conv with DTB \n36.90 \n0.880 \nReplacing DTB with Conv \n36.82 \n0.875 \nw/o kernel shift \n35.64 \n0.857 \nLG-BPN (Ours) \n37.32 \n0.886 \n\n\n\nTable 2 .\n2The analysis of our method on the SIDD validation dataset. Experimental results prove the effectiveness of our method design. CVF-SID[25] AP-BSN[20] LG-BPN (Ours)DND benchmark input \n\nNAC [36] \nC2N [15] \nAWGN-M [38] \n28.50/0.777 \n29.20/0.793 \n29.26/0.797 \n\n30.22/0.855 \n32.94/0.889 \n33.37/0.897 \n\n\nWe use 'global' to differentiate from our 'local' branch. Though a more accurate term is 'non-local', we follow the usage of 'global' as[35].\n1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 (b) High-correlation pixels mask\n\nA high-quality denoising dataset for smartphone cameras. Abdelrahman Abdelhamed, Stephen Lin, Michael S Brown, CVPR. 6Abdelrahman Abdelhamed, Stephen Lin, and Michael S Brown. A high-quality denoising dataset for smartphone cameras. In CVPR, pages 1692-1700, 2018. 1, 2, 4, 6\n\nReal image denoising with feature attention. Saeed Anwar, Nick Barnes, ICCV. Saeed Anwar and Nick Barnes. Real image denoising with feature attention. In ICCV, pages 3155-3164, 2019. 1, 2, 7\n\nNoise2self: Blind denoising by self-supervision. Joshua Batson, Loic Royer, ICML. 17Joshua Batson and Loic Royer. Noise2self: Blind denoising by self-supervision. In ICML, pages 524-533, 2019. 1, 3, 7\n\nNatural image noise dataset. Benoit Brummer, Christophe De Vleeschouwer, CVPR Workshops. 1Benoit Brummer and Christophe De Vleeschouwer. Natural image noise dataset. In CVPR Workshops, pages 0-0, 2019. 1, 2\n\nA non-local algorithm for image denoising. Antoni Buades, Bartomeu Coll, J-M Morel, CVPR. Antoni Buades, Bartomeu Coll, and J-M Morel. A non-local algorithm for image denoising. In CVPR, pages 60-65, 2005. 1\n\nImage blind denoising with generative adversarial network based noise modeling. Jingwen Chen, Jiawei Chen, Hongyang Chao, Ming Yang, CVPR. 27Jingwen Chen, Jiawei Chen, Hongyang Chao, and Ming Yang. Image blind denoising with generative adversarial net- work based noise modeling. In CVPR, pages 3155-3164, 2018. 2, 7\n\nNbnet: Noise basis learning for image denoising with subspace projection. Shen Cheng, Yuzhi Wang, Haibin Huang, Donghao Liu, Haoqiang Fan, Shuaicheng Liu, CVPR. 1Shen Cheng, Yuzhi Wang, Haibin Huang, Donghao Liu, Haoqiang Fan, and Shuaicheng Liu. Nbnet: Noise basis learning for image denoising with subspace projection. In CVPR, pages 4896-4906, 2021. 1, 2\n\nImage denoising by sparse 3-d transformdomain collaborative filtering. Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, Karen Egiazarian, IEEE TIP. 1687Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising by sparse 3-d transform- domain collaborative filtering. IEEE TIP, 16(8):2080-2095, 2007. 1, 7\n\nDeformable convolutional networks. Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, Yichen Wei, ICCV. Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In ICCV, pages 764-773, 2017. 5\n\nLowlight raw video denoising with a high-quality realistic motion dataset. Ying Fu, Zichun Wang, Tao Zhang, Jun Zhang, IEEE TMM, 2022. Early access. 1Ying Fu, Zichun Wang, Tao Zhang, and Jun Zhang. Low- light raw video denoising with a high-quality realistic motion dataset. IEEE TMM, 2022. Early access. 1\n\nWeighted nuclear norm minimization with application to image denoising. Shuhang Gu, Lei Zhang, Wangmeng Zuo, Xiangchu Feng, CVPR. 17Shuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu Feng. Weighted nuclear norm minimization with applica- tion to image denoising. In CVPR, pages 2862-2869, 2014. 1, 7\n\nToward convolutional blind denoising of real photographs. Zifei Shi Guo, Kai Yan, Wangmeng Zhang, Lei Zuo, Zhang, CVPR. Shi Guo, Zifei Yan, Kai Zhang, Wangmeng Zuo, and Lei Zhang. Toward convolutional blind denoising of real pho- tographs. In CVPR, pages 1712-1722, 2019. 1, 2, 7\n\nEnd-to-end unpaired image denoising with conditional adversarial networks. Zhiwei Hong, Xiaocheng Fan, Tao Jiang, Jianxing Feng, AAAI. Zhiwei Hong, Xiaocheng Fan, Tao Jiang, and Jianxing Feng. End-to-end unpaired image denoising with conditional ad- versarial networks. In AAAI, pages 4140-4149, 2020. 7\n\nNeighbor2neighbor: Self-supervised denoising from single noisy images. Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu, Jianzhuang Liu, CVPR. Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu, and Jianzhuang Liu. Neighbor2neighbor: Self-supervised de- noising from single noisy images. In CVPR, pages 14781- 14790, 2021. 3\n\nC2n: Practical generative noise modeling for real-world denoising. Geonwoon Jang, Wooseok Lee, Sanghyun Son, Kyoung Mu Lee, ICCV. Geonwoon Jang, Wooseok Lee, Sanghyun Son, and Ky- oung Mu Lee. C2n: Practical generative noise modeling for real-world denoising. In ICCV, pages 2350-2359, 2021. 1, 2, 7, 8\n\nTransfer learning from synthetic to real-noise denoising with adaptive instance normalization. Yoonsik Kim, Jae Woong Soh, Yong Gu, Nam Ik Park, Cho, CVPR. Yoonsik Kim, Jae Woong Soh, Gu Yong Park, and Nam Ik Cho. Transfer learning from synthetic to real-noise denoising with adaptive instance normalization. In CVPR, pages 3482- 3492, 2020. 7\n\nNoise2void-learning denoising from single noisy images. Alexander Krull, Tim-Oliver Buchholz, Florian Jug, CVPR. Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. Noise2void-learning denoising from single noisy images. In CVPR, pages 2129-2137, 2019. 1, 3, 7, 8\n\nProbabilistic noise2void: Unsupervised content-aware denoising. Alexander Krull, Tom\u00e1\u0161 Vi\u010dar, Mangal Prakash, Manan Lalit, Florian Jug, Frontiers in Computer Science. 25Alexander Krull, Tom\u00e1\u0161 Vi\u010dar, Mangal Prakash, Manan Lalit, and Florian Jug. Probabilistic noise2void: Unsuper- vised content-aware denoising. Frontiers in Computer Sci- ence, 2:5, 2020. 2\n\nHigh-quality self-supervised deep image denoising. Samuli Laine, Tero Karras, Jaakko Lehtinen, Timo Aila, NIPS. Samuli Laine, Tero Karras, Jaakko Lehtinen, and Timo Aila. High-quality self-supervised deep image denoising. In NIPS, pages 6970-6980, 2019. 1, 2, 3, 4\n\nAp-bsn: Self-supervised denoising for real-world images via asymmetric pd and blind-spot network. Wooseok Lee, Sanghyun Son, Kyoung Mu Lee, CVPR. 7Wooseok Lee, Sanghyun Son, and Kyoung Mu Lee. Ap-bsn: Self-supervised denoising for real-world images via asym- metric pd and blind-spot network. In CVPR, pages 17725- 17734, 2022. 1, 2, 3, 4, 5, 6, 7, 8\n\nNoise2noise: Learning image restoration without clean data. Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, Timo Aila, ICML. 13Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and Timo Aila. Noise2noise: Learning image restoration without clean data. In ICML, pages 2965-2974, 2018. 1, 3\n\nConnecting image denoising and high-level vision tasks via deep learning. Ding Liu, Bihan Wen, Jianbo Jiao, Xianming Liu, Zhangyang Wang, Thomas S Huang, IEEE TIP. 291Ding Liu, Bihan Wen, Jianbo Jiao, Xianming Liu, Zhangyang Wang, and Thomas S Huang. Connecting im- age denoising and high-level vision tasks via deep learning. IEEE TIP, 29:3695-3706, 2020. 1\n\nMulti-level wavelet-cnn for image restoration. Pengju Liu, Hongzhi Zhang, Kai Zhang, Liang Lin, Wangmeng Zuo, CVPR workshops. Pengju Liu, Hongzhi Zhang, Kai Zhang, Liang Lin, and Wangmeng Zuo. Multi-level wavelet-cnn for image restora- tion. In CVPR workshops, pages 773-782, 2018. 7\n\nNoisier2noise: Learning to denoise from unpaired noisy data. Nick Moran, Dan Schmidt, Yu Zhong, Patrick Coady, CVPR. 2020Nick Moran, Dan Schmidt, Yu Zhong, and Patrick Coady. Noisier2noise: Learning to denoise from unpaired noisy data. In CVPR, pages 12064-12072, 2020. 3\n\nCvf-sid: Cyclic multi-variate function for self-supervised image denoising by disentangling noise from image. Reyhaneh Neshatavar, Mohsen Yavartanoo, Sanghyun Son, Kyoung Mu Lee, CVPR. Reyhaneh Neshatavar, Mohsen Yavartanoo, Sanghyun Son, and Kyoung Mu Lee. Cvf-sid: Cyclic multi-variate function for self-supervised image denoising by disentangling noise from image. In CVPR, pages 17583-17591, 2022. 1, 2, 3, 7, 8\n\nRecorrupted-to-recorrupted: unsupervised deep learning for image denoising. Tongyao Pang, Huan Zheng, Yuhui Quan, Hui Ji, CVPR. Tongyao Pang, Huan Zheng, Yuhui Quan, and Hui Ji. Recorrupted-to-recorrupted: unsupervised deep learning for image denoising. In CVPR, pages 2043-2052, 2021. 1, 2, 3, 7, 8\n\nBenchmarking denoising algorithms with real photographs. Tobias Plotz, Stefan Roth, CVPR. Tobias Plotz and Stefan Roth. Benchmarking denoising al- gorithms with real photographs. In CVPR, pages 1586-1595, 2017. 6\n\nMaxvit: Multi-axis vision transformer. Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li, 2022. 5Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li. Maxvit: Multi-axis vision transformer. ECCV, 2022. 5\n\nImage quality assessment: from error visibility to structural similarity. Zhou Wang, Alan C Bovik, R Hamid, Eero P Sheikh, Simoncelli, IEEE TIP. 134Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE TIP, 13(4):600-612, 2004. 6\n\nBlind2unblind: Self-supervised image denoising with visible blind spots. Zejin Wang, Jiazheng Liu, Guoqing Li, Hua Han, CVPR. 13Zejin Wang, Jiazheng Liu, Guoqing Li, and Hua Han. Blind2unblind: Self-supervised image denoising with visi- ble blind spots. In CVPR, pages 2027-2036, 2022. 1, 3\n\nUnpaired learning of deep image denoising. Xiaohe Wu, Ming Liu, Yue Cao, Dongwei Ren, Wangmeng Zuo, ECCV. 57Xiaohe Wu, Ming Liu, Yue Cao, Dongwei Ren, and Wang- meng Zuo. Unpaired learning of deep image denoising. In ECCV, pages 352-368, 2020. 2, 3, 4, 5, 7\n\nJoint super resolution and denoising from a single depth image. Jun Xie, Rogerio Schmidt Feris, Shiaw-Shian Yu, Ming-Ting Sun, IEEE TMM. 179Jun Xie, Rogerio Schmidt Feris, Shiaw-Shian Yu, and Ming- Ting Sun. Joint super resolution and denoising from a single depth image. IEEE TMM, 17(9):1525-1537, 2015. 1\n\nNoisy-as-clean: Learning selfsupervised denoising from corrupted image. Jun Xu, Yuan Huang, Ming-Ming Cheng, Li Liu, Fan Zhu, Zhou Xu, Ling Shao, IEEE TIP. 297Jun Xu, Yuan Huang, Ming-Ming Cheng, Li Liu, Fan Zhu, Zhou Xu, and Ling Shao. Noisy-as-clean: Learning self- supervised denoising from corrupted image. IEEE TIP, 29:9316-9329, 2020. 3, 7\n\nDeep iterative down-up cnn for image denoising. Songhyun Yu, Bumjun Park, Jechang Jeong, CVPR Workshops. 17Songhyun Yu, Bumjun Park, and Jechang Jeong. Deep itera- tive down-up cnn for image denoising. In CVPR Workshops, pages 0-0, 2019. 1, 7\n\nRestormer: Efficient transformer for high-resolution image restoration. Aditya Syed Waqas Zamir, Salman Arora, Munawar Khan, Hayat, Ming-Hsuan Fahad Shahbaz Khan, Yang, CVPR. 16Syed Waqas Zamir, Aditya Arora, Salman Khan, Mu- nawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In CVPR, pages 5728-5739, 2022. 1, 3, 6\n\nBeyond a gaussian denoiser: Residual learning of deep cnn for image denoising. Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang, IEEE TIP. 267Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE TIP, 26(7):3142-3155, 2017. 1, 2, 7, 8\n\nFfdnet: Toward a fast and flexible solution for cnn-based image denoising. Kai Zhang, Wangmeng Zuo, Lei Zhang, IEEE TIP. 279Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward a fast and flexible solution for cnn-based image denoising. IEEE TIP, 27(9):4608-4622, 2018. 2\n\nWhen awgn-based denoiser meets real noises. Yuqian Zhou, Jianbo Jiao, Haibin Huang, Yang Wang, Jue Wang, Honghui Shi, Thomas Huang, AAAI. Yuqian Zhou, Jianbo Jiao, Haibin Huang, Yang Wang, Jue Wang, Honghui Shi, and Thomas Huang. When awgn-based denoiser meets real noises. In AAAI, pages 13074-13081, 2020. 2, 3, 4, 7, 8\n", "annotations": {"author": "[{\"end\":135,\"start\":89},{\"end\":196,\"start\":136},{\"end\":228,\"start\":197},{\"end\":273,\"start\":229}]", "publisher": null, "author_last_name": "[{\"end\":100,\"start\":96},{\"end\":143,\"start\":141},{\"end\":203,\"start\":200},{\"end\":240,\"start\":235}]", "author_first_name": "[{\"end\":95,\"start\":89},{\"end\":140,\"start\":136},{\"end\":199,\"start\":197},{\"end\":234,\"start\":229}]", "author_affiliation": "[{\"end\":134,\"start\":102},{\"end\":195,\"start\":163},{\"end\":227,\"start\":205},{\"end\":272,\"start\":261}]", "title": "[{\"end\":86,\"start\":1},{\"end\":359,\"start\":274}]", "venue": null, "abstract": "[{\"end\":2137,\"start\":361}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2224,\"start\":2221},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2227,\"start\":2224},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2360,\"start\":2356},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2363,\"start\":2360},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2529,\"start\":2526},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2531,\"start\":2529},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2534,\"start\":2531},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2537,\"start\":2534},{\"end\":2547,\"start\":2539},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2612,\"start\":2609},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2646,\"start\":2642},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2656,\"start\":2652},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2669,\"start\":2665},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2753,\"start\":2749},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2805,\"start\":2801},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2937,\"start\":2934},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2940,\"start\":2937},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2943,\"start\":2940},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3150,\"start\":3147},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3153,\"start\":3150},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3238,\"start\":3235},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3240,\"start\":3238},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3706,\"start\":3702},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4052,\"start\":4049},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4055,\"start\":4052},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4058,\"start\":4055},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4061,\"start\":4058},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4617,\"start\":4613},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4775,\"start\":4771},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4984,\"start\":4980},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5381,\"start\":5377},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5384,\"start\":5381},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5452,\"start\":5448},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5455,\"start\":5452},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5458,\"start\":5455},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7751,\"start\":7747},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8020,\"start\":8016},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8150,\"start\":8147},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8153,\"start\":8150},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8485,\"start\":8482},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8487,\"start\":8485},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8533,\"start\":8530},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8536,\"start\":8533},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9339,\"start\":9336},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9504,\"start\":9500},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9656,\"start\":9652},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10287,\"start\":10286},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10373,\"start\":10369},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10678,\"start\":10674},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10866,\"start\":10862},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10876,\"start\":10872},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10889,\"start\":10885},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11060,\"start\":11056},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11134,\"start\":11130},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11211,\"start\":11207},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11525,\"start\":11521},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11544,\"start\":11541},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11649,\"start\":11645},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11664,\"start\":11660},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11874,\"start\":11870},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12146,\"start\":12142},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12360,\"start\":12356},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12431,\"start\":12427},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12691,\"start\":12687},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12694,\"start\":12691},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13685,\"start\":13684},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14625,\"start\":14621},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15088,\"start\":15084},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":15293,\"start\":15289},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15296,\"start\":15293},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15482,\"start\":15481},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17425,\"start\":17421},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17428,\"start\":17425},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17615,\"start\":17612},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19495,\"start\":19491},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19780,\"start\":19777},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21021,\"start\":21017},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21664,\"start\":21660},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22218,\"start\":22214},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22314,\"start\":22310},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22400,\"start\":22396},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":22621,\"start\":22617},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24918,\"start\":24915},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24931,\"start\":24927},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25105,\"start\":25102},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25524,\"start\":25520},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25884,\"start\":25880},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26374,\"start\":26370},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27551,\"start\":27547},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27667,\"start\":27663},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27727,\"start\":27723},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28018,\"start\":28014},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28166,\"start\":28162},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31079,\"start\":31075},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":32525,\"start\":32521},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":34452,\"start\":34448},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":34463,\"start\":34459},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":34753,\"start\":34749}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":32435,\"start\":32037},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32688,\"start\":32436},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32753,\"start\":32689},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34302,\"start\":32754},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":34612,\"start\":34303}]", "paragraph": "[{\"end\":2839,\"start\":2153},{\"end\":3523,\"start\":2841},{\"end\":4493,\"start\":3525},{\"end\":4961,\"start\":4495},{\"end\":5639,\"start\":4963},{\"end\":7071,\"start\":5641},{\"end\":7267,\"start\":7073},{\"end\":7500,\"start\":7269},{\"end\":7695,\"start\":7502},{\"end\":8771,\"start\":7741},{\"end\":13196,\"start\":8804},{\"end\":13361,\"start\":13207},{\"end\":13401,\"start\":13377},{\"end\":13686,\"start\":13403},{\"end\":15011,\"start\":13723},{\"end\":16218,\"start\":15013},{\"end\":16938,\"start\":16220},{\"end\":18088,\"start\":16983},{\"end\":18150,\"start\":18090},{\"end\":18584,\"start\":18204},{\"end\":19281,\"start\":18617},{\"end\":21213,\"start\":19283},{\"end\":21997,\"start\":21320},{\"end\":22561,\"start\":21999},{\"end\":23294,\"start\":22563},{\"end\":24051,\"start\":23361},{\"end\":24388,\"start\":24110},{\"end\":24799,\"start\":24390},{\"end\":25796,\"start\":24843},{\"end\":26434,\"start\":25817},{\"end\":27632,\"start\":26473},{\"end\":28424,\"start\":27634},{\"end\":28791,\"start\":28460},{\"end\":29073,\"start\":28793},{\"end\":29706,\"start\":29075},{\"end\":30849,\"start\":29708},{\"end\":31262,\"start\":30851},{\"end\":32036,\"start\":31277}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18203,\"start\":18151},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18616,\"start\":18585},{\"attributes\":{\"id\":\"formula_2\"},\"end\":21291,\"start\":21214},{\"attributes\":{\"id\":\"formula_3\"},\"end\":23360,\"start\":23295},{\"attributes\":{\"id\":\"formula_4\"},\"end\":24061,\"start\":24052},{\"attributes\":{\"id\":\"formula_5\"},\"end\":24109,\"start\":24061}]", "table_ref": "[{\"end\":26625,\"start\":26618},{\"end\":26763,\"start\":26756},{\"end\":27146,\"start\":27139},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":28989,\"start\":28982},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":31159,\"start\":31152}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2151,\"start\":2139},{\"attributes\":{\"n\":\"2.\"},\"end\":7710,\"start\":7698},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7739,\"start\":7713},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8802,\"start\":8774},{\"attributes\":{\"n\":\"3.\"},\"end\":13205,\"start\":13199},{\"end\":13375,\"start\":13364},{\"end\":13695,\"start\":13689},{\"attributes\":{\"n\":\"3.1.\"},\"end\":13721,\"start\":13698},{\"attributes\":{\"n\":\"3.2.\"},\"end\":16981,\"start\":16941},{\"attributes\":{\"n\":\"3.3.\"},\"end\":21318,\"start\":21293},{\"attributes\":{\"n\":\"4.\"},\"end\":24813,\"start\":24802},{\"attributes\":{\"n\":\"4.1.\"},\"end\":24841,\"start\":24816},{\"attributes\":{\"n\":\"4.2.\"},\"end\":25815,\"start\":25799},{\"attributes\":{\"n\":\"4.3.\"},\"end\":26471,\"start\":26437},{\"attributes\":{\"n\":\"4.4.\"},\"end\":28458,\"start\":28427},{\"attributes\":{\"n\":\"5.\"},\"end\":31275,\"start\":31265},{\"end\":32048,\"start\":32038},{\"end\":32447,\"start\":32437},{\"end\":32700,\"start\":32690},{\"end\":32764,\"start\":32755},{\"end\":34313,\"start\":34304}]", "table": "[{\"end\":34302,\"start\":33694},{\"end\":34612,\"start\":34477}]", "figure_caption": "[{\"end\":32435,\"start\":32050},{\"end\":32688,\"start\":32449},{\"end\":32753,\"start\":32702},{\"end\":33694,\"start\":32766},{\"end\":34477,\"start\":34315}]", "figure_ref": "[{\"end\":7027,\"start\":7019},{\"end\":9956,\"start\":9948},{\"end\":13273,\"start\":13265},{\"end\":13430,\"start\":13422},{\"end\":14723,\"start\":14714},{\"end\":15903,\"start\":15895},{\"end\":16326,\"start\":16317},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17394,\"start\":17386},{\"end\":17760,\"start\":17752},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18312,\"start\":18304},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18787,\"start\":18779},{\"end\":19089,\"start\":19081},{\"end\":19597,\"start\":19589},{\"end\":20289,\"start\":20281},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21005,\"start\":20997},{\"end\":22341,\"start\":22333},{\"end\":24049,\"start\":24043},{\"end\":24570,\"start\":24562},{\"end\":26813,\"start\":26805},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26826,\"start\":26818},{\"end\":28153,\"start\":28145},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28283,\"start\":28275},{\"end\":28748,\"start\":28739},{\"end\":31144,\"start\":31136}]", "bib_author_first_name": "[{\"end\":35097,\"start\":35086},{\"end\":35117,\"start\":35110},{\"end\":35132,\"start\":35123},{\"end\":35356,\"start\":35351},{\"end\":35368,\"start\":35364},{\"end\":35553,\"start\":35547},{\"end\":35566,\"start\":35562},{\"end\":35735,\"start\":35729},{\"end\":35755,\"start\":35745},{\"end\":35758,\"start\":35756},{\"end\":35957,\"start\":35951},{\"end\":35974,\"start\":35966},{\"end\":35984,\"start\":35981},{\"end\":36204,\"start\":36197},{\"end\":36217,\"start\":36211},{\"end\":36232,\"start\":36224},{\"end\":36243,\"start\":36239},{\"end\":36513,\"start\":36509},{\"end\":36526,\"start\":36521},{\"end\":36539,\"start\":36533},{\"end\":36554,\"start\":36547},{\"end\":36568,\"start\":36560},{\"end\":36584,\"start\":36574},{\"end\":36873,\"start\":36865},{\"end\":36891,\"start\":36881},{\"end\":36905,\"start\":36897},{\"end\":36922,\"start\":36917},{\"end\":37176,\"start\":37170},{\"end\":37188,\"start\":37182},{\"end\":37198,\"start\":37193},{\"end\":37208,\"start\":37206},{\"end\":37220,\"start\":37213},{\"end\":37231,\"start\":37228},{\"end\":37242,\"start\":37236},{\"end\":37483,\"start\":37479},{\"end\":37494,\"start\":37488},{\"end\":37504,\"start\":37501},{\"end\":37515,\"start\":37512},{\"end\":37791,\"start\":37784},{\"end\":37799,\"start\":37796},{\"end\":37815,\"start\":37807},{\"end\":37829,\"start\":37821},{\"end\":38075,\"start\":38070},{\"end\":38088,\"start\":38085},{\"end\":38102,\"start\":38094},{\"end\":38113,\"start\":38110},{\"end\":38374,\"start\":38368},{\"end\":38390,\"start\":38381},{\"end\":38399,\"start\":38396},{\"end\":38415,\"start\":38407},{\"end\":38672,\"start\":38669},{\"end\":38689,\"start\":38680},{\"end\":38696,\"start\":38694},{\"end\":38709,\"start\":38702},{\"end\":38724,\"start\":38714},{\"end\":38987,\"start\":38979},{\"end\":39001,\"start\":38994},{\"end\":39015,\"start\":39007},{\"end\":39030,\"start\":39021},{\"end\":39318,\"start\":39311},{\"end\":39327,\"start\":39324},{\"end\":39333,\"start\":39328},{\"end\":39343,\"start\":39339},{\"end\":39351,\"start\":39348},{\"end\":39354,\"start\":39352},{\"end\":39626,\"start\":39617},{\"end\":39644,\"start\":39634},{\"end\":39662,\"start\":39655},{\"end\":39902,\"start\":39893},{\"end\":39915,\"start\":39910},{\"end\":39929,\"start\":39923},{\"end\":39944,\"start\":39939},{\"end\":39959,\"start\":39952},{\"end\":40244,\"start\":40238},{\"end\":40256,\"start\":40252},{\"end\":40271,\"start\":40265},{\"end\":40286,\"start\":40282},{\"end\":40558,\"start\":40551},{\"end\":40572,\"start\":40564},{\"end\":40587,\"start\":40578},{\"end\":40871,\"start\":40865},{\"end\":40887,\"start\":40882},{\"end\":40901,\"start\":40898},{\"end\":40920,\"start\":40914},{\"end\":40932,\"start\":40928},{\"end\":40946,\"start\":40941},{\"end\":40960,\"start\":40956},{\"end\":41257,\"start\":41253},{\"end\":41268,\"start\":41263},{\"end\":41280,\"start\":41274},{\"end\":41295,\"start\":41287},{\"end\":41310,\"start\":41301},{\"end\":41325,\"start\":41317},{\"end\":41592,\"start\":41586},{\"end\":41605,\"start\":41598},{\"end\":41616,\"start\":41613},{\"end\":41629,\"start\":41624},{\"end\":41643,\"start\":41635},{\"end\":41889,\"start\":41885},{\"end\":41900,\"start\":41897},{\"end\":41912,\"start\":41910},{\"end\":41927,\"start\":41920},{\"end\":42215,\"start\":42207},{\"end\":42234,\"start\":42228},{\"end\":42255,\"start\":42247},{\"end\":42270,\"start\":42261},{\"end\":42597,\"start\":42590},{\"end\":42608,\"start\":42604},{\"end\":42621,\"start\":42616},{\"end\":42631,\"start\":42628},{\"end\":42878,\"start\":42872},{\"end\":42892,\"start\":42886},{\"end\":43078,\"start\":43068},{\"end\":43090,\"start\":43083},{\"end\":43102,\"start\":43099},{\"end\":43114,\"start\":43110},{\"end\":43127,\"start\":43121},{\"end\":43142,\"start\":43138},{\"end\":43157,\"start\":43150},{\"end\":43399,\"start\":43395},{\"end\":43410,\"start\":43406},{\"end\":43412,\"start\":43411},{\"end\":43421,\"start\":43420},{\"end\":43435,\"start\":43429},{\"end\":43719,\"start\":43714},{\"end\":43734,\"start\":43726},{\"end\":43747,\"start\":43740},{\"end\":43755,\"start\":43752},{\"end\":43982,\"start\":43976},{\"end\":43991,\"start\":43987},{\"end\":44000,\"start\":43997},{\"end\":44013,\"start\":44006},{\"end\":44027,\"start\":44019},{\"end\":44259,\"start\":44256},{\"end\":44272,\"start\":44265},{\"end\":44299,\"start\":44288},{\"end\":44313,\"start\":44304},{\"end\":44575,\"start\":44572},{\"end\":44584,\"start\":44580},{\"end\":44601,\"start\":44592},{\"end\":44611,\"start\":44609},{\"end\":44620,\"start\":44617},{\"end\":44630,\"start\":44626},{\"end\":44639,\"start\":44635},{\"end\":44903,\"start\":44895},{\"end\":44914,\"start\":44908},{\"end\":44928,\"start\":44921},{\"end\":45169,\"start\":45163},{\"end\":45194,\"start\":45188},{\"end\":45209,\"start\":45202},{\"end\":45233,\"start\":45223},{\"end\":45566,\"start\":45563},{\"end\":45582,\"start\":45574},{\"end\":45594,\"start\":45588},{\"end\":45605,\"start\":45601},{\"end\":45615,\"start\":45612},{\"end\":45902,\"start\":45899},{\"end\":45918,\"start\":45910},{\"end\":45927,\"start\":45924},{\"end\":46149,\"start\":46143},{\"end\":46162,\"start\":46156},{\"end\":46175,\"start\":46169},{\"end\":46187,\"start\":46183},{\"end\":46197,\"start\":46194},{\"end\":46211,\"start\":46204},{\"end\":46223,\"start\":46217}]", "bib_author_last_name": "[{\"end\":35108,\"start\":35098},{\"end\":35121,\"start\":35118},{\"end\":35138,\"start\":35133},{\"end\":35362,\"start\":35357},{\"end\":35375,\"start\":35369},{\"end\":35560,\"start\":35554},{\"end\":35572,\"start\":35567},{\"end\":35743,\"start\":35736},{\"end\":35771,\"start\":35759},{\"end\":35964,\"start\":35958},{\"end\":35979,\"start\":35975},{\"end\":35990,\"start\":35985},{\"end\":36209,\"start\":36205},{\"end\":36222,\"start\":36218},{\"end\":36237,\"start\":36233},{\"end\":36248,\"start\":36244},{\"end\":36519,\"start\":36514},{\"end\":36531,\"start\":36527},{\"end\":36545,\"start\":36540},{\"end\":36558,\"start\":36555},{\"end\":36572,\"start\":36569},{\"end\":36588,\"start\":36585},{\"end\":36879,\"start\":36874},{\"end\":36895,\"start\":36892},{\"end\":36915,\"start\":36906},{\"end\":36933,\"start\":36923},{\"end\":37180,\"start\":37177},{\"end\":37191,\"start\":37189},{\"end\":37204,\"start\":37199},{\"end\":37211,\"start\":37209},{\"end\":37226,\"start\":37221},{\"end\":37234,\"start\":37232},{\"end\":37246,\"start\":37243},{\"end\":37486,\"start\":37484},{\"end\":37499,\"start\":37495},{\"end\":37510,\"start\":37505},{\"end\":37521,\"start\":37516},{\"end\":37794,\"start\":37792},{\"end\":37805,\"start\":37800},{\"end\":37819,\"start\":37816},{\"end\":37834,\"start\":37830},{\"end\":38083,\"start\":38076},{\"end\":38092,\"start\":38089},{\"end\":38108,\"start\":38103},{\"end\":38117,\"start\":38114},{\"end\":38124,\"start\":38119},{\"end\":38379,\"start\":38375},{\"end\":38394,\"start\":38391},{\"end\":38405,\"start\":38400},{\"end\":38420,\"start\":38416},{\"end\":38678,\"start\":38673},{\"end\":38692,\"start\":38690},{\"end\":38700,\"start\":38697},{\"end\":38712,\"start\":38710},{\"end\":38728,\"start\":38725},{\"end\":38992,\"start\":38988},{\"end\":39005,\"start\":39002},{\"end\":39019,\"start\":39016},{\"end\":39034,\"start\":39031},{\"end\":39322,\"start\":39319},{\"end\":39337,\"start\":39334},{\"end\":39346,\"start\":39344},{\"end\":39359,\"start\":39355},{\"end\":39364,\"start\":39361},{\"end\":39632,\"start\":39627},{\"end\":39653,\"start\":39645},{\"end\":39666,\"start\":39663},{\"end\":39908,\"start\":39903},{\"end\":39921,\"start\":39916},{\"end\":39937,\"start\":39930},{\"end\":39950,\"start\":39945},{\"end\":39963,\"start\":39960},{\"end\":40250,\"start\":40245},{\"end\":40263,\"start\":40257},{\"end\":40280,\"start\":40272},{\"end\":40291,\"start\":40287},{\"end\":40562,\"start\":40559},{\"end\":40576,\"start\":40573},{\"end\":40591,\"start\":40588},{\"end\":40880,\"start\":40872},{\"end\":40896,\"start\":40888},{\"end\":40912,\"start\":40902},{\"end\":40926,\"start\":40921},{\"end\":40939,\"start\":40933},{\"end\":40954,\"start\":40947},{\"end\":40965,\"start\":40961},{\"end\":41261,\"start\":41258},{\"end\":41272,\"start\":41269},{\"end\":41285,\"start\":41281},{\"end\":41299,\"start\":41296},{\"end\":41315,\"start\":41311},{\"end\":41331,\"start\":41326},{\"end\":41596,\"start\":41593},{\"end\":41611,\"start\":41606},{\"end\":41622,\"start\":41617},{\"end\":41633,\"start\":41630},{\"end\":41647,\"start\":41644},{\"end\":41895,\"start\":41890},{\"end\":41908,\"start\":41901},{\"end\":41918,\"start\":41913},{\"end\":41933,\"start\":41928},{\"end\":42226,\"start\":42216},{\"end\":42245,\"start\":42235},{\"end\":42259,\"start\":42256},{\"end\":42274,\"start\":42271},{\"end\":42602,\"start\":42598},{\"end\":42614,\"start\":42609},{\"end\":42626,\"start\":42622},{\"end\":42634,\"start\":42632},{\"end\":42884,\"start\":42879},{\"end\":42897,\"start\":42893},{\"end\":43081,\"start\":43079},{\"end\":43097,\"start\":43091},{\"end\":43108,\"start\":43103},{\"end\":43119,\"start\":43115},{\"end\":43136,\"start\":43128},{\"end\":43148,\"start\":43143},{\"end\":43160,\"start\":43158},{\"end\":43404,\"start\":43400},{\"end\":43418,\"start\":43413},{\"end\":43427,\"start\":43422},{\"end\":43442,\"start\":43436},{\"end\":43454,\"start\":43444},{\"end\":43724,\"start\":43720},{\"end\":43738,\"start\":43735},{\"end\":43750,\"start\":43748},{\"end\":43759,\"start\":43756},{\"end\":43985,\"start\":43983},{\"end\":43995,\"start\":43992},{\"end\":44004,\"start\":44001},{\"end\":44017,\"start\":44014},{\"end\":44031,\"start\":44028},{\"end\":44263,\"start\":44260},{\"end\":44286,\"start\":44273},{\"end\":44302,\"start\":44300},{\"end\":44317,\"start\":44314},{\"end\":44578,\"start\":44576},{\"end\":44590,\"start\":44585},{\"end\":44607,\"start\":44602},{\"end\":44615,\"start\":44612},{\"end\":44624,\"start\":44621},{\"end\":44633,\"start\":44631},{\"end\":44644,\"start\":44640},{\"end\":44906,\"start\":44904},{\"end\":44919,\"start\":44915},{\"end\":44934,\"start\":44929},{\"end\":45186,\"start\":45170},{\"end\":45200,\"start\":45195},{\"end\":45214,\"start\":45210},{\"end\":45221,\"start\":45216},{\"end\":45252,\"start\":45234},{\"end\":45258,\"start\":45254},{\"end\":45572,\"start\":45567},{\"end\":45586,\"start\":45583},{\"end\":45599,\"start\":45595},{\"end\":45610,\"start\":45606},{\"end\":45621,\"start\":45616},{\"end\":45908,\"start\":45903},{\"end\":45922,\"start\":45919},{\"end\":45933,\"start\":45928},{\"end\":46154,\"start\":46150},{\"end\":46167,\"start\":46163},{\"end\":46181,\"start\":46176},{\"end\":46192,\"start\":46188},{\"end\":46202,\"start\":46198},{\"end\":46215,\"start\":46212},{\"end\":46229,\"start\":46224}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":52059988},\"end\":35304,\"start\":35029},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":118713138},\"end\":35496,\"start\":35306},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":59523708},\"end\":35698,\"start\":35498},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":173990552},\"end\":35906,\"start\":35700},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":11206708},\"end\":36115,\"start\":35908},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":51989956},\"end\":36433,\"start\":36117},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":229923112},\"end\":36792,\"start\":36435},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1475121},\"end\":37133,\"start\":36794},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4028864},\"end\":37402,\"start\":37135},{\"attributes\":{\"id\":\"b9\"},\"end\":37710,\"start\":37404},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1663191},\"end\":38010,\"start\":37712},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":49672261},\"end\":38291,\"start\":38012},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":213655883},\"end\":38596,\"start\":38293},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":231419143},\"end\":38910,\"start\":38598},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":244426949},\"end\":39214,\"start\":38912},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":211506288},\"end\":39559,\"start\":39216},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":53751136},\"end\":39827,\"start\":39561},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":173990717},\"end\":40185,\"start\":39829},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":173990648},\"end\":40451,\"start\":40187},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":247596985},\"end\":40803,\"start\":40453},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3846544},\"end\":41177,\"start\":40805},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":52168355},\"end\":41537,\"start\":41179},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":29151865},\"end\":41822,\"start\":41539},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":204904999},\"end\":42095,\"start\":41824},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":247627958},\"end\":42512,\"start\":42097},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":235719899},\"end\":42813,\"start\":42514},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":9715523},\"end\":43027,\"start\":42815},{\"attributes\":{\"doi\":\"2022. 5\",\"id\":\"b27\"},\"end\":43319,\"start\":43029},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":207761262},\"end\":43639,\"start\":43321},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":247447122},\"end\":43931,\"start\":43641},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":221376798},\"end\":44190,\"start\":43933},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":12473749},\"end\":44498,\"start\":44192},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":222154570},\"end\":44845,\"start\":44500},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":198166487},\"end\":45089,\"start\":44847},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":244346144},\"end\":45482,\"start\":45091},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":996788},\"end\":45822,\"start\":45484},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":10514149},\"end\":46097,\"start\":45824},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":102351079},\"end\":46420,\"start\":46099}]", "bib_title": "[{\"end\":35084,\"start\":35029},{\"end\":35349,\"start\":35306},{\"end\":35545,\"start\":35498},{\"end\":35727,\"start\":35700},{\"end\":35949,\"start\":35908},{\"end\":36195,\"start\":36117},{\"end\":36507,\"start\":36435},{\"end\":36863,\"start\":36794},{\"end\":37168,\"start\":37135},{\"end\":37782,\"start\":37712},{\"end\":38068,\"start\":38012},{\"end\":38366,\"start\":38293},{\"end\":38667,\"start\":38598},{\"end\":38977,\"start\":38912},{\"end\":39309,\"start\":39216},{\"end\":39615,\"start\":39561},{\"end\":39891,\"start\":39829},{\"end\":40236,\"start\":40187},{\"end\":40549,\"start\":40453},{\"end\":40863,\"start\":40805},{\"end\":41251,\"start\":41179},{\"end\":41584,\"start\":41539},{\"end\":41883,\"start\":41824},{\"end\":42205,\"start\":42097},{\"end\":42588,\"start\":42514},{\"end\":42870,\"start\":42815},{\"end\":43393,\"start\":43321},{\"end\":43712,\"start\":43641},{\"end\":43974,\"start\":43933},{\"end\":44254,\"start\":44192},{\"end\":44570,\"start\":44500},{\"end\":44893,\"start\":44847},{\"end\":45161,\"start\":45091},{\"end\":45561,\"start\":45484},{\"end\":45897,\"start\":45824},{\"end\":46141,\"start\":46099}]", "bib_author": "[{\"end\":35110,\"start\":35086},{\"end\":35123,\"start\":35110},{\"end\":35140,\"start\":35123},{\"end\":35364,\"start\":35351},{\"end\":35377,\"start\":35364},{\"end\":35562,\"start\":35547},{\"end\":35574,\"start\":35562},{\"end\":35745,\"start\":35729},{\"end\":35773,\"start\":35745},{\"end\":35966,\"start\":35951},{\"end\":35981,\"start\":35966},{\"end\":35992,\"start\":35981},{\"end\":36211,\"start\":36197},{\"end\":36224,\"start\":36211},{\"end\":36239,\"start\":36224},{\"end\":36250,\"start\":36239},{\"end\":36521,\"start\":36509},{\"end\":36533,\"start\":36521},{\"end\":36547,\"start\":36533},{\"end\":36560,\"start\":36547},{\"end\":36574,\"start\":36560},{\"end\":36590,\"start\":36574},{\"end\":36881,\"start\":36865},{\"end\":36897,\"start\":36881},{\"end\":36917,\"start\":36897},{\"end\":36935,\"start\":36917},{\"end\":37182,\"start\":37170},{\"end\":37193,\"start\":37182},{\"end\":37206,\"start\":37193},{\"end\":37213,\"start\":37206},{\"end\":37228,\"start\":37213},{\"end\":37236,\"start\":37228},{\"end\":37248,\"start\":37236},{\"end\":37488,\"start\":37479},{\"end\":37501,\"start\":37488},{\"end\":37512,\"start\":37501},{\"end\":37523,\"start\":37512},{\"end\":37796,\"start\":37784},{\"end\":37807,\"start\":37796},{\"end\":37821,\"start\":37807},{\"end\":37836,\"start\":37821},{\"end\":38085,\"start\":38070},{\"end\":38094,\"start\":38085},{\"end\":38110,\"start\":38094},{\"end\":38119,\"start\":38110},{\"end\":38126,\"start\":38119},{\"end\":38381,\"start\":38368},{\"end\":38396,\"start\":38381},{\"end\":38407,\"start\":38396},{\"end\":38422,\"start\":38407},{\"end\":38680,\"start\":38669},{\"end\":38694,\"start\":38680},{\"end\":38702,\"start\":38694},{\"end\":38714,\"start\":38702},{\"end\":38730,\"start\":38714},{\"end\":38994,\"start\":38979},{\"end\":39007,\"start\":38994},{\"end\":39021,\"start\":39007},{\"end\":39036,\"start\":39021},{\"end\":39324,\"start\":39311},{\"end\":39339,\"start\":39324},{\"end\":39348,\"start\":39339},{\"end\":39361,\"start\":39348},{\"end\":39366,\"start\":39361},{\"end\":39634,\"start\":39617},{\"end\":39655,\"start\":39634},{\"end\":39668,\"start\":39655},{\"end\":39910,\"start\":39893},{\"end\":39923,\"start\":39910},{\"end\":39939,\"start\":39923},{\"end\":39952,\"start\":39939},{\"end\":39965,\"start\":39952},{\"end\":40252,\"start\":40238},{\"end\":40265,\"start\":40252},{\"end\":40282,\"start\":40265},{\"end\":40293,\"start\":40282},{\"end\":40564,\"start\":40551},{\"end\":40578,\"start\":40564},{\"end\":40593,\"start\":40578},{\"end\":40882,\"start\":40865},{\"end\":40898,\"start\":40882},{\"end\":40914,\"start\":40898},{\"end\":40928,\"start\":40914},{\"end\":40941,\"start\":40928},{\"end\":40956,\"start\":40941},{\"end\":40967,\"start\":40956},{\"end\":41263,\"start\":41253},{\"end\":41274,\"start\":41263},{\"end\":41287,\"start\":41274},{\"end\":41301,\"start\":41287},{\"end\":41317,\"start\":41301},{\"end\":41333,\"start\":41317},{\"end\":41598,\"start\":41586},{\"end\":41613,\"start\":41598},{\"end\":41624,\"start\":41613},{\"end\":41635,\"start\":41624},{\"end\":41649,\"start\":41635},{\"end\":41897,\"start\":41885},{\"end\":41910,\"start\":41897},{\"end\":41920,\"start\":41910},{\"end\":41935,\"start\":41920},{\"end\":42228,\"start\":42207},{\"end\":42247,\"start\":42228},{\"end\":42261,\"start\":42247},{\"end\":42276,\"start\":42261},{\"end\":42604,\"start\":42590},{\"end\":42616,\"start\":42604},{\"end\":42628,\"start\":42616},{\"end\":42636,\"start\":42628},{\"end\":42886,\"start\":42872},{\"end\":42899,\"start\":42886},{\"end\":43083,\"start\":43068},{\"end\":43099,\"start\":43083},{\"end\":43110,\"start\":43099},{\"end\":43121,\"start\":43110},{\"end\":43138,\"start\":43121},{\"end\":43150,\"start\":43138},{\"end\":43162,\"start\":43150},{\"end\":43406,\"start\":43395},{\"end\":43420,\"start\":43406},{\"end\":43429,\"start\":43420},{\"end\":43444,\"start\":43429},{\"end\":43456,\"start\":43444},{\"end\":43726,\"start\":43714},{\"end\":43740,\"start\":43726},{\"end\":43752,\"start\":43740},{\"end\":43761,\"start\":43752},{\"end\":43987,\"start\":43976},{\"end\":43997,\"start\":43987},{\"end\":44006,\"start\":43997},{\"end\":44019,\"start\":44006},{\"end\":44033,\"start\":44019},{\"end\":44265,\"start\":44256},{\"end\":44288,\"start\":44265},{\"end\":44304,\"start\":44288},{\"end\":44319,\"start\":44304},{\"end\":44580,\"start\":44572},{\"end\":44592,\"start\":44580},{\"end\":44609,\"start\":44592},{\"end\":44617,\"start\":44609},{\"end\":44626,\"start\":44617},{\"end\":44635,\"start\":44626},{\"end\":44646,\"start\":44635},{\"end\":44908,\"start\":44895},{\"end\":44921,\"start\":44908},{\"end\":44936,\"start\":44921},{\"end\":45188,\"start\":45163},{\"end\":45202,\"start\":45188},{\"end\":45216,\"start\":45202},{\"end\":45223,\"start\":45216},{\"end\":45254,\"start\":45223},{\"end\":45260,\"start\":45254},{\"end\":45574,\"start\":45563},{\"end\":45588,\"start\":45574},{\"end\":45601,\"start\":45588},{\"end\":45612,\"start\":45601},{\"end\":45623,\"start\":45612},{\"end\":45910,\"start\":45899},{\"end\":45924,\"start\":45910},{\"end\":45935,\"start\":45924},{\"end\":46156,\"start\":46143},{\"end\":46169,\"start\":46156},{\"end\":46183,\"start\":46169},{\"end\":46194,\"start\":46183},{\"end\":46204,\"start\":46194},{\"end\":46217,\"start\":46204},{\"end\":46231,\"start\":46217}]", "bib_venue": "[{\"end\":35144,\"start\":35140},{\"end\":35381,\"start\":35377},{\"end\":35578,\"start\":35574},{\"end\":35787,\"start\":35773},{\"end\":35996,\"start\":35992},{\"end\":36254,\"start\":36250},{\"end\":36594,\"start\":36590},{\"end\":36943,\"start\":36935},{\"end\":37252,\"start\":37248},{\"end\":37477,\"start\":37404},{\"end\":37840,\"start\":37836},{\"end\":38130,\"start\":38126},{\"end\":38426,\"start\":38422},{\"end\":38734,\"start\":38730},{\"end\":39040,\"start\":39036},{\"end\":39370,\"start\":39366},{\"end\":39672,\"start\":39668},{\"end\":39994,\"start\":39965},{\"end\":40297,\"start\":40293},{\"end\":40597,\"start\":40593},{\"end\":40971,\"start\":40967},{\"end\":41341,\"start\":41333},{\"end\":41663,\"start\":41649},{\"end\":41939,\"start\":41935},{\"end\":42280,\"start\":42276},{\"end\":42640,\"start\":42636},{\"end\":42903,\"start\":42899},{\"end\":43066,\"start\":43029},{\"end\":43464,\"start\":43456},{\"end\":43765,\"start\":43761},{\"end\":44037,\"start\":44033},{\"end\":44327,\"start\":44319},{\"end\":44654,\"start\":44646},{\"end\":44950,\"start\":44936},{\"end\":45264,\"start\":45260},{\"end\":45631,\"start\":45623},{\"end\":45943,\"start\":45935},{\"end\":46235,\"start\":46231}]"}}}, "year": 2023, "month": 12, "day": 17}