{"id": 257834090, "updated": "2023-10-05 02:36:31.647", "metadata": {"title": "WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research", "authors": "[{\"first\":\"Xinhao\",\"last\":\"Mei\",\"middle\":[]},{\"first\":\"Chutong\",\"last\":\"Meng\",\"middle\":[]},{\"first\":\"Haohe\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Qiuqiang\",\"last\":\"Kong\",\"middle\":[]},{\"first\":\"Tom\",\"last\":\"Ko\",\"middle\":[]},{\"first\":\"Chengqi\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Mark\",\"last\":\"Plumbley\",\"middle\":[\"D.\"]},{\"first\":\"Yuexian\",\"last\":\"Zou\",\"middle\":[]},{\"first\":\"Wenwu\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "The advancement of audio-language (AL) multimodal learning tasks has been significant in recent years. However, researchers face challenges due to the costly and time-consuming collection process of existing audio-language datasets, which are limited in size. To address this data scarcity issue, we introduce WavCaps, the first large-scale weakly-labelled audio captioning dataset, comprising approximately 400k audio clips with paired captions. We sourced audio clips and their raw descriptions from web sources and a sound event detection dataset. However, the online-harvested raw descriptions are highly noisy and unsuitable for direct use in tasks such as automated audio captioning. To overcome this issue, we propose a three-stage processing pipeline for filtering noisy data and generating high-quality captions, where ChatGPT, a large language model, is leveraged to filter and transform raw descriptions automatically. We conduct a comprehensive analysis of the characteristics of WavCaps dataset and evaluate it on multiple downstream audio-language multimodal learning tasks. The systems trained on WavCaps outperform previous state-of-the-art (SOTA) models by a significant margin. Our aspiration is for the WavCaps dataset we have proposed to facilitate research in audio-language multimodal learning and demonstrate the potential of utilizing ChatGPT to enhance academic research. Our dataset and codes are available at https://github.com/XinhaoMei/WavCaps.", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": "2303.17395", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2303-17395", "doi": "10.48550/arxiv.2303.17395"}}, "content": {"source": {"pdf_hash": "3dfed62c61f650eb114f0f0aa26b4e7d37b963a6", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2303.17395v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1b50f6260b05e4a176326a1b8bae0964e53e4b74", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3dfed62c61f650eb114f0f0aa26b4e7d37b963a6.txt", "contents": "\nWavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research\n\n\nXinhao Mei \nChutong Meng \nHaohe Liu \nQiuqiang Kong \nTom Ko \nChengqi Zhao \nMark D Plumbley \nYuexian Zou \nWenwu Wang \nWavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research\n1Index Terms-Audio-language datasetmultimodal learningChatGPTdeep learning\nThe advancement of audio-language (AL) multimodal learning tasks has been significant in recent years. However, researchers face challenges due to the costly and timeconsuming collection process of existing audio-language datasets, which are limited in size. To address this data scarcity issue, we introduce WavCaps, the first large-scale weakly-labelled audio captioning dataset, comprising approximately 400k audio clips with paired captions. We sourced audio clips and their raw descriptions from web sources and a sound event detection dataset. However, the online-harvested raw descriptions are highly noisy and unsuitable for direct use in tasks such as automated audio captioning. To overcome this issue, we propose a three-stage processing pipeline for filtering noisy data and generating highquality captions, where ChatGPT, a large language model, is leveraged to filter and transform raw descriptions automatically. We conduct a comprehensive analysis of the characteristics of WavCaps dataset and evaluate it on multiple downstream audiolanguage multimodal learning tasks. The systems trained on WavCaps outperform previous state-of-the-art (SOTA) models by a significant margin. Our aspiration is for the WavCaps dataset we have proposed to facilitate research in audio-language multimodal learning and demonstrate the potential of utilizing ChatGPT to enhance academic research. Our dataset and codes are available at https://github.com/XinhaoMei/WavCaps.\n\nI. INTRODUCTION\n\nO VER the past decade, the field of machine listening has achieved notable progress, with the aid of deep learning techniques and the availability of audio event datasets [1]- [3]. This has resulted in the development of algorithms that can detect and identify sound events and acoustic scenes [4]- [7]. More recently, there has been a surge of interest in establishing a more profound comprehension of audio content by connecting audio and language. A number of audio-language (AL) multimodal learning tasks have been introduced, such as textto-audio retrieval [8], [9], automated audio captioning [10]- [12], audio question answering [13], [14], text-based sound generation [15]- [18], and text-based sound separation [19]. Research on AL takes a stride in the direction of empowering machines to comprehend audio signals at a human-like level.\n\nWhile AL research is relatively young, vision-language (VL) multimodal learning [20], the counterpart of AL multimodal learning, has been studied for decades and has contributed to the success of many vision-language applications, such as cross-modal search [21], [22], image and video captioning [23], [24], text-to-image generation [25], [26], and visual question answering [27], [28]. Two main factors have contributed to the significant progress in VL tasks. First, advances in model architectures, especially self-attention-based Transformer models [29], have shown superior performance compared to convolutional neural networks (CNNs) [30] and recurrent neural networks (RNNs) [31], both in computer vision and natural language processing tasks. Second, largescale VL multimodal pretraining enables models to learn transferable and robust multimodal representations that benefit VL downstream tasks [24], [32], [33]. Pre-training on largescale (even noisy) datasets and fine-tuning on specific datasets for downstream tasks has been a prevalent methodology in VL multimodal learning tasks [34], [35].\n\nInspired by the progress made in the domains of vision and language, Transformer-based models and training strategies have been successfully adapted in modeling audio signals [36], [37]. This has led to substantial improvements in the recognition and detection of sound events. However, the scale of audio-related datasets is still limited, which severely hinders the research in AL multimodal learning. Till now, the largest audio event dataset, AudioSet [1], has about 2M audio clips, and the largest audio captioning dataset, AudioCaps [38], contains only about 50k audio clips. Both of them are orders of magnitude smaller than their counterparts in the vision domain such as ImageNet [39], and MS COCO [40]. The reasons for this are twofold: first, the research has received less attention in the audio community, as compared to the vision community. Second, the collection of audio datasets is a more laborious, costly, and time-consuming process compared to that of visual datasets.\n\nTo alleviate the data scarcity problem and advance AL research, we gathered audio clips and their corresponding raw descriptions from multiple web sources, drawing inspiration from the collection process of the Conceptual Captions datasets [41], [42]. Nevertheless, the raw descriptions obtained through web crawling exhibit a high degree of noise. For instance, some descriptions do not present the audio content, while some are not sentences but only words or phrases. Furthermore, certain descriptions contain extraneous information unrelated to the audio, such as recording devices, time, and locations. As a result, these unrefined raw descriptions hinder the learning of robust audio-language representations and are ill-suited for direct application in tasks like automated audio captioning. To process harvested data, Conceptual Captions 3M (CC3M) [41] applied a series of complex rules to filter and transform candidate image-caption pairs and it only keeps around 0.2% of originally harvested data. As compared to image-description pairs, the limited number of audiodescription pairs available on the web makes it impractical to accept such a high rate of discarding.\n\nTo overcome this problem, we propose a three-stage processing pipeline for filtering noisy data and generating highquality captions. Initially, a pre-filtering stage is implemented to exclude irrelevant data based on text frequency. Subsequently, we employ ChatGPT 1 , a robust large language model (LLM), to further process the obtained raw descriptions. This involves content-based filtering and transforming the raw descriptions into sentences resembling captions. Lastly, we refine undesirable outputs from the second stage in a post-processing stage. Ultimately, we present the first large-scale, weaklylabelled audio captioning dataset, WavCaps, which contains about 400k audio clips with paired captions. Because these captions are augmented from raw descriptions using ChatGPT, we refer to these captions as weakly-labelled. In comparison to existing audio captioning datasets [38], [43], [44], WavCaps is not only an order of magnitude larger, but also encompasses a wider range of content. We conducted experiments on multiple audio-language multimodal learning tasks to evaluate the impact of the proposed WavCaps dataset and achieved new state-of-the-art results on most tasks, surpassing previous benchmarks by significant margins.\n\nIn summary, our work offers three main contributions: (1) the introduction of a large-scale, weakly-labeled audio captioning dataset, WavCaps, for audio-language multimodal learning tasks; (2) the use of ChatGPT to automatically filter and rewrite harvested raw descriptions into caption-like sentences, showcasing its powerful data augmentation capabilities; and (3) extensive experiments conducted on downstream tasks, demonstrating the effectiveness of our proposed WavCaps dataset. Our expectation is that WavCaps will aid in advancing research in audio-language multimodal learning and also serve as a demonstration of how ChatGPT can be utilized to enrich academic research.\n\nThe remainder of this paper is organized as follows. Section II introduces related works in vision-language and audiolanguage areas. Details of the dataset collection and processing steps are described in Section III. Section IV introduces experiments on audio-language multimodal tasks and present the results and analysis. Finally, we conclude this work in SectionV. \n\n\nII. RELATED WORKS\n\n\nA. Vision-Language Datasets\n\nIn recent years, significant progress has been made in VL learning thanks to the release of large-scale VL datasets. The images or videos in these datasets are typically sourced from online platforms. Based on the annotation methods employed, VL datasets can be classified into two categories: automatically-annotated and human-annotated. Automaticallyannotated datasets, such as CC3M [41], Conceptual Captions 12M (CC12M) [42], and ALIGN [34], have millions of imagecaption pairs, where image-description pairs are first harvested from the web and processed automatically according to a series of predefined rules, without human intervention. These largescale datasets are usually used as pre-training datasets to learn multimodal representations for downstream tasks. In contrast, human-annotated datasets, such as COCO Captions [40], NO-Caps [45], and Flickr [46], employ humans to annotate images or videos. The annotation process is expensive and timeconsuming. Therefore, human-annotated datasets are limited in size and are generally used as fine-tuning datasets for performance evaluation. In summary, automatically-annotated datasets are larger but often noisy, while human-annotated datasets are smaller but of better quality. Pre-training on largescale (even noisy) datasets and fine-tuning on small taskspecific datasets has been a prevalent methodology in VL multimodal learning [32]- [34].\n\nSimilar to automatically-annotated VL datasets [34], [41], [42], we also harvest audio-description pairs from the web, and design a pipeline to process data automatically. However, taking into account the differences in quantity and data characteristics, we did not use complex pre-defined rules as in CC3M [41] to process the harvested data, instead employ ChatGPT to filter and rewrite raw descriptions into caption-like sentences.\n\n\nB. Audio-Language Datasets\n\nCompared with flourishing research on VL multimodal learning, research on audio-language multimodal learning is limited due to the lack of AL datasets. Almost all AL tasks, such as automated audio captioning [10], [47], [48], languagebased audio retrieval [8], [9], text-to-audio generation [15], [16] and language-queried sound separation [19], rely on two main audio captioning datasets, AudioCaps [38] and Clotho [43]. AudioCaps contains about 50k audio clips sourced from AudioSet [1], the largest audio event dataset, and is annotated by humans. Clotho contains around 6k audio clips sourced from the FreeSound 2 platform, each audio clip has five humanannotated captions. Although other audio captioning datasets have been proposed, such as MACS [44] and AudioCaption [49], they are still limited in size and not widely used due to their inferior quality compared to AudioCaps and Clotho. The scale of existing audio-language datasets severely hinders AL research.\n\nOther audio researchers have also tried to harvest audio clips and their descriptions from the web. Koepke et al. [8] introduced SoundDescs dataset, which is sourced from BBC\n\n\nPre-Filtering\n\n\nAudio Duration Filtering\n\nHigh-frequency Text Filtering\n\n\nChatGPT-based Transformation\n\nFailure.\n\nA 2-ton diesel truck is passing by.\n\nAn engine starts and a man speaks.\n\n\nPost-processing\n\nA soft sound is being played on a Yamaha.\n\nA truck is passing by.\n\nAn engine starts and a man speaks. Similarly to LAION-Audio-630K, most of data in our WavCaps dataset are also from Freesound. There are two main differences between our work and other online-harvested AL datasets. First, raw descriptions harvested from the web are very noisy. Existing harvested AL datasets did not filter or process these raw descriptions. We instead employed ChatGPT to filter and rewrite the raw descriptions to caption-like sentences, making our WavCaps dataset suitable for all kinds of audio-language tasks including automated audio captioning. Second, classes in AudioSet are very unbalanced and most of the audio clips only have a single label [53], therefore, using T5 to transform tags to captions makes the training data unbalanced and noisy. We propose to generate captions for the AudioSet strongly-labelled subset [54] which has accurate human-labelled timing information of sound events. The resulting captions are more accurate than those generated using T5.\n\nIII. WAVCAPS DATASET In this section, we introduce the collection and processing steps of the WavCaps dataset. The data sources and their characteristics are first introduced. We then describe our proposed three-stage processing pipeline, including pre-filtering, ChatGPT-based transformation, and post-processing. Finally, we present a detailed analysis of the WavCaps dataset. Fig. 1 shows the overview of the three-stage data processing pipeline.\n\n\nA. Data Sources\n\nFreeSound [55] is an online collaborative sound sharing site started in 2005. The initial goal of FreeSound is to give support to sound researchers and sound artists who usually have trouble in finding royalty-free sound samples. After more than 10 years of development, there are about 560 000 audio clips 3 uploaded by registered users, and these audio clips cover diverse contents such as music, environmental sounds, synthesized sound effects and even noises. When uploading an audio clip, each user is asked to give a short description about the uploaded audio clip. Ideally, we would like to use useruploaded descriptions directly as audio captions. However, these raw descriptions are extremely noisy. For example, some are not related to the audio content (such as the recording device or personal feeling), some are not sentences but only nouns or verbs, and some are too detailed and include many specific information such as place names and people names. In addition, there are a large number of repetitive descriptions uploaded by the same user when they upload many audio clips at the same time. These different characteristics make processing descriptions from FreeSound extremely hard. BBC Sound Effects 4 contains over 33k audio clips recorded around the world over the past 100 years. These audio clips contain extensive contents from the BBC Radiophonic workshop, the Blitz in London, BBC TV and Radio productions, and the BBC Natural History Unit archive. The raw descriptions in BBC Sound Effects begin with nouns describing the sounding objects or acoustic scenes of the sound content, followed by detailed descriptions. However, most of the detailed descriptions are also not complete sentences, and some include specific information such as recording equipment, time and places. Although these raw descriptions are noisy and cannot  I  EXAMPLE PROMPTS TO CHATGPT FOR FREESOUND AND AUDIOSET STRONGLY-LABELLED SUBSET. TRANSFORMATION EXAMPLES ARE ARE IGNORED. 'SL' REFERS TO 'STRONGLY-LABELLED'.\n\n\nData sources Prompts\n\nFreeSound BBC Sound Effects SoundBible I will give you a list of descriptions of sounds. Process each individually. Extract the type of the sound and generate an audio caption describing the sound events. The audio caption should be less than 20 words. Delete the author of the sound. Delete locations, city names, country names. Delete the time. Delete device names. Delete the proper noun modifiers, number modifiers, and unit modifiers. Summarize each output into one sentence. Replace all named entities with their hypernyms. Replace people names with \"someone\". Do not write introductions or explanations. Only describe the sound events and do not use \"heard\", \"recorded\". Start each output sentence with its index. Make sure you are using grammatical subject-verb-object sentences. Output \"Failure.\" if the description is not related to sound.\n\nAudioSet SL I will give you a number of lists containing sound events occurred sequentially in time. Process each individually. Write an one-sentence audio caption to describe these sounds. Make sure you are using grammatical subject-verb-object sentences. Directly describe the sounds and avoid using the word \"heard\". The caption should be less than 20 words. be directly used as audio captions, these raw descriptions all describe the content of the audio clips.\n\nSoundBible 5 is a website for sharing free and royalty free sound effects and audio clips. We harvested about 1500 royalty free sound effects with their raw descriptions from SoundBible, and these audio clips also cover a wide range of contents. Most of the raw descriptions in SoundBible are well-written sentences describing the audio content, but some of them still contain redundant information unrelated to the audio content. Therefore, those raw descriptions still cannot be directly used as audio captions.\n\nAudioSet Strongly-Labelled Subset [54], a sound event detection dataset is included to increase the size of our proposed WavCaps dataset. The original AudioSet dataset [1] contains about 2M audio clips with unbalanced, weaklylabelled clip-level tags, and most of these audio clips only has one single tag. To investigate whether accuracy of the classifier trained on AudioSet is impaired by the weak labels, Shawn et al. [54] further released a strongly-labelled subset, containing about 100k audio clips,from a portion of AudioSet dataset, where each audio clip is annotated by humans and has precise timing information for the sound events occurred in it.\n\nWith these strongly-labelled temporal information, templatebased methods or large language models can be employed to generate captions for audio clips in AudioSet strongly-labelled subset. 5 https://soundbible.com/\n\n\nB. Data Processing\n\nOnline-harvested raw descriptions are very noisy and thus cannot be directly used as captions. To address this issue, image captioning datasets, such as CC3M, and CC12M, have applied a series of complex filtering and transformation steps to process raw image-description pairs, including image-based filtering, text-based filtering, image and text-based filtering and text transformation. These processing steps are strict, and lead to a high discard rate that we cannot afford. In order to retain as much data as possible, we propose a threestage processing pipeline by simplifying the filtering and transformation steps according to the characteristics of the harvested audio-descriptions pairs, as showing in Figure 1. Pre-Filtering We first apply minimal pre-filtering to remove undesirable data, such as descriptions do not present audio content. The pre-filtering consists of audio-duration filtering and high-frequency text filtering. For audio-duration filtering, audio clips with a duration of less than one second are removed. This is mainly because short audio clips might not contain enough meaningful content and require extremely long padding during training. We only apply high-frequency text filtering to FreeSound data only. Because raw descriptions from BBC Sound Effects and SoundBible can be considered consistent with their corresponding audio content, and sound events labels in AudioSet strongly-labelled subset are annotated manually. For FreeSound data, we observe those highfrequency descriptions shared across audio clips are usually uploaded by the same user when uploading multiple audio clips at the same time, and are more likely to be unrelated to the audio content. Therefore, we apply high-frequency text filtering to exclude descriptions that are shared by more than 5 audio clips. These two filtering steps have removed about 265 000 data samples from FreeSound.\n\nChatGPT-based Transformation To transform the raw descriptions into audio captions, we propose that a well-formed audio caption should possess the following characteristics:\n\n\u2022 Be a single, accurate description of the audio content using concise syntax; \u2022 Avoid the use of named entities such as people's names, locations, and recording devices that cannot be inferred from the audio signal alone; \u2022 Exclude any subjective sound-unrelated information such as personal feelings or opinions; However, online-harvested descriptions are extremely noisy and most of them fail to meet above requirements, particularly those from FreeSound. Due to the varying characteristics of raw descriptions, it is challenging to design rules that accurately convert them into captions, and doing so would result in a high discard rate similar to what was observed in CC3M.\n\nTo tackle the challenge of converting raw descriptions into captions, we propose using ChatGPT, a powerful large language model trained by OpenAI 6 to perform this task automatically. Trained using Reinforcement Learning from Human Feedback (RLHF) [56], ChatGPT has been shown to excel at generating human-like responses to natural language prompts, and has garnered widespread attention for its powerful understanding, reasoning, and dialogue abilities. By designing prompts that account for the characteristics of different data sources, ChatGPT can effectively filter out sound-unrelated information and rewrite raw descriptions in to audio caption-like sentences that meet the requirements we proposed in prompts. This approach has the potential to significantly reduce the discard rate of raw descriptions and improve the quality of converted captions. Prompts we used are shown in Table I. In order to make use of ChatGPT's in-context learning ability, several transformation examples are also included in the prompts and they are different for each data sources (ignored in Table I). These examples can significantly improve the caption quality. Table II presents examples of the raw descriptions and final processed captions. It can be observed that ChatGPT can transform non-sentence descriptions (i.e., nouns and phrases) into sentences, remove redundant information that is too specific or is not related to sound, and summarize long sentences into one-sentence high-level audio captions. In addition, it can also discriminate 6 https://openai.com/ descriptions that are not related to audio contents and output \"Failure\" for those descriptions.\n\nWhile ChatGPT has shown promising results in converting raw descriptions into captions, in some cases, it may still fail to produce captions that meet our requirements. For example, ChatGPT may sometimes struggle to remove numbers, people's names, and place names from raw descriptions. Additionally, a small number of descriptions that are not related to the audio content may not be filtered out.\n\nPost-Processing To address the incorrect processing cases mentioned above, we further apply post-processing steps to refine the generated captions. We employ a pre-trained named entity recognition model to identify bad outputs that still contain numbers, place names, and people's names. Any captions with named entity information are then processed again by ChatGPT to remove these details using the same prompts but different examples. In most cases, this second round of processing successfully removes the named entity information. However, if post-processing still results in captions with named entity information, we discard these captions to ensure the quality of the final dataset. Finally, we exclude captions that are too short by setting a minimum length threshold of three words. This ensures that the captions are informative and descriptive enough to provide a meaningful description of the audio content. \n\n\nC. Dataset Analysis\n\nTable III provides statistics for the raw data collected from four different sources before and after processing. It can be observed that more than half of the data samples from FreeSound were filtered out after processing, with the majority being removed by the high-frequency text filter. Since audio clips with a duration less than one second were excluded, the average duration of samples from FreeSound significantly increased. Conversely, only a small number of samples from BBC Sound Effects and SoundBible were removed. Notably, the average text length decreased significantly due to Chat-GPT's successful removal of words unrelated to sound and named entity information. Fig. 2 displays the top 100 word clouds in the WavCaps dataset and the entire harvested raw data, respectively. The top word clouds in the WavCaps dataset contain meaningful sound-related words such as sounding objects and sound events, whereas the top word clouds in the raw data are largely devoid of meaning. This also demonstrates that ChatGPT successfully extracted sound-related information and removed information unrelated to sound. Table IV provides a comparison of key statistics between WavCaps and other audio-language datasets. Human-labeled datasets are generally limited in size, but the captions are in high quality. SoundDescs [8] is sourced from the BBC Sound Effects with no processing applied to the raw descriptions. LAION-Audio-630K has the highest number of audio clips. A majority of the audio clips in LAION-Audio-630K (around 420k) are sourced from FreeSound, resulting ina substantial overlap between their dataset and ours. To preserve the diversity of clip durations, we did not exclude any long audio clips, leading to a longer total duration in our dataset compared to theirs. Fig. 3 illustrates the distribution of audio durations in the WavCaps dataset. The AudioSet Strongly-labeled subset is not considered in Fig. 3, as all audio clips in AudioSet have a consistent duration of 10 seconds. Overall, in comparison to human-labelled audio captioning datasets, WavCaps is an order of magnitude larger and encompasses a greater diversity of content. When compared to online-harvested datasets, such as SoundDescs and LAION-Audio-630k, our approach involves transforming raw descriptions into captions and filtering out noisy or sound-unrelated descriptions. This process ultimately results in the creation of the largest weakly-labeled audio captioning dataset available.\n\n\nIV. EXPERIMENTS\n\nTo evaluate the impact of the proposed WavCaps dataset, we conducted experiments on several audio-language multimodal learning tasks, including audio-language retrieval, automated audio captioning, zero-shot audio classification, and text-tosound generation. In this section, we provide a description of the AL tasks we considered, along with the corresponding experimental settings, results and analysis. For all the experiments excluding text-to-sound generation, audio clips are sampled with a 32k sampling rate, and we use 64-dimensional log mel-spectrograms extracted by a 1024-point Hanning window with a hop size of 320 as input audio features.\n\n\nA. Audio-Language Retrieval\n\nAudio-language retrieval involves searching for an audio clip or a caption in a database based on a query from another modality. To perform this task, the model learns Acoustic Semantic Embeddings (ASE) [9] that map paired audio clips and captions closer in the embedding space, while keeping embeddings for non-paired audio clips and captions far apart. We evaluate audio-language retrieval under three training settings: zero-shot, pretraining, and fine-tuning. 1) Models: Following previous works [9], [51], we build an ASE model based on a two-tower architecture, where an audio encoder is employed to encode audio representations while an language encoder is used to encode captions. Two types of audio encoders, a CNN14 from pretrained audio neural networks (PANNs) [53] and a Transformer network-HTSAT [58] are considered, where both are pretrained on AudioSet with an audio tagging task. A pretrained BERT base network [59] is employed as the language encoder. A 2-layer multilayer perceptron with a ReLU activation in between is appended after these two encoders to project respective features into the shared embedding space. Cosine similarity is used to denote the similarity between audio embeddings and language embeddings, and can be formulated as\ns ij = f (a i ) \u00b7 g(t j ) ||f (a i )|| 2 ||g(t j )|| 2(1)\nwhere f (\u00b7) is the audio encoder and g(\u00b7) is the language decoder, a i is the audio clip indexed with i in a batch, t j is the caption indexed with j in a batch, and s ij is the similarity AUDIOCAPS AND CLOTHO. \"AC\" REFERS TO \"AUDIOCAPS\", \"LA\" REFERS TO \"LAION-AUDIO-630K\", \"ZS\" REFERS TO \"ZERO-SHOT\", \"PT\" REFERS TO \"PRETRAINING\", AND \"FT\" REFERS TO \"FINE-TUNING\".\n\nHIGHER SCORE MEANS BETTER PERFORMANCE. score. The model is trained with a normalized temperaturescaled cross entropy loss (NT-Xent) [60] in a bi-directional manner, and can be formulated as:\n\n\nModel\nL = \u2212 1 2B B i=1 log exp(s ii /\u03c4 ) B j=1 exp (s ij /\u03c4 ) + log exp(s ii /\u03c4 ) B j=1 exp (s ji /\u03c4 )(2)\nwhere B is the batch size, and \u03c4 is a temperature hyperparameter. This training strategy is also known as contrastive language-audio pretraining (CLAP).\n\n2) Experimental Setup: We first train our two models on the merged training sets of AudioCaps and Clotho as baselines. For zero-shot setting, we exclude all overlapping samples in AudioCaps and Clotho from the WavCaps dataset. Baseline and zero-shot models are trained for 15 epochs with a batch size of 128 and a learning rate of 5 \u00d7 10 \u22125 using the Adam [61] optimizer. For the supervised setting, we merge WavCaps, AudioCaps and Clotho together as a large training set (validation and test sets of AudioCaps and Clotho are not included). The models are trained for 40 epochs and other hyperparameters are the same with zero-shot settings. For the fine-tuned setting, we aim to study the impact of pretraining on WavCaps dataset. We further fine-tune the models trained under pretraining setting on AudioCaps and Clotho for 20 epochs, respectively. The temperature hyperparameter \u03c4 is set to 0.07 for all settings. For the HTSAT audio encoder, all the audio clips are randomly cropped or padded to 10 seconds, because HTSAT requires fixed-sized inputs. For the CNN14 audio encoder that can receive variable length inputs, we set the maximum input duration as 30 seconds and audio clips longer than 30 seconds are randomly cropped. During training, audio clips with similar duration are grouped within a batch. Model checkpoints are selected based on their performance on validation sets after each epoch and the final model performance are evaluated on test sets of AudioCaps and Clotho using recall at rank k (R@k). For a query, R@k is 1 if at the positive item appears in the top k retrieved items, otherwise 0. The final R@k is averaged across the dataset. Table V presents the audiolanguage retrieval results on the AudioCaps and Clotho datasets, where top half of the table shows the results of existing methods and bottom half of the table shows our results. In the zero-shot setting, both of our models show strong zero-shot retrieval ability. When compared with previous SOTA models that were trained only on respective datasets, both models outperform the previous SOTA models on the Clotho dataset and achieve comparable results on the AudioCaps dataset. This demonstrates that the models trained WavCaps in zero-shot setting generalize well on both AudioCaps and Clotho. In the pretraining setting, the performance are improved compared to baselines and zero-shot models, and also achieved SOTA results on both datasets. It can be observed that using more training data leads to greater improvement on audio-to-text metrics compared to text-to-audio metrics. With fine-tuning, both of our models further improve the performance, and outperform all existing methods by a significant margin on both audio-to-text retrieval (16.7% improvement on R@1 on AudioCaps and 5.4% improvement on R@1 on Clotho) and text-to-audio retrieval (15.0% improvement on R@1 on AudioCaps and 18.1% improvement on R@1 on Clotho).\n\n\n3) Results and Analysis:\n\nIn order to demonstrate the effectiveness of our proposed WavCaps dataset, we primarily compare our results with models trained on the LAION-Audio-630K dataset [51]. Our models, trained under the pretraining setting, present a fair comparison to LAION's models trained on LAION-Audio-630K, as both training sets included respective proposed datasets as well as AudioCaps and Clotho. For LAION's models, initially, we observe a notable performance decline in the AudioCaps dataset when the LAION-Audio-630K dataset is introduced into the training while the results on the Clotho dataset show improvement. Upon incorporating AudioSet into the training, performance of LAION's model on AudioCaps enhances, but remains close to their baseline. In our case, our models surpass LAION's in most metrics for both datasets, despite utilizing less data. When incorporating WavCaps into the training, our models exhibit significant improvements on both datasets. It is important to highlight that LAION's train- ing set, which includes AudioSet, is approximately six times larger than ours, totaling 2.63 million in size. Our experimental results reveal that the quality of the audio-language dataset significantly influences the model performance. By adopting our proposed three-stage processing pipeline to filter out noisy data and rewrite raw descriptions into captions, we achieve state-of-the-art performance using a smaller quantity of higherquality data. Finally, our two models show varying performance levels on the AudioCaps and Clotho datasets. The CNN14-based model underperforms the HTSAT-based model on the AudioCaps dataset, yet it surpasses the HTSAT-based model on the Clotho dataset. A potential explanation for this could be the variable duration of audio clips in Clotho, which range from 15 to 30 seconds. Randomly cropping these clips into 10 seconds in the HTSAT model may result in information loss. As CNN14 is capable of handling variable durations, it outperforms HTSAT on the Clotho dataset. We believe that employing feature fusion methods, as seen in [51], to process variableduration audio clips could further enhance performance. This exploration will be reserved for future work.\n\n\nB. Automated Audio Captioning\n\nAutomated audio captioning is the task of generating a natural language sentence to describe the content of an audio clip, which mainly concerns environmental sounds and ignores possible voice content [10]. We still evaluate audio captioning on AudioCaps and Clotho datasets.\n\n1) Models: Audio captioning is generally solved by an encoder-decoder model, where an encoder is leveraged to extract audio features and a decoder is employed to generate captions based on audio features extracted from the encoder. We build our model based on the baseline of Detection and Classification of Acoustic Scenes and Events (DCASE) 2022 challenge task 6 [11]. Similarly to audio-language retrieval described above, two types of audio encoders, a CNN14 and an HTSAT, are investigated. The language decoder is a pretrained language model, BART base network [68]. BART is a sequence-to-sequence language model consisting of both Transformer encoder and decoder blocks, pretrained on large text corpora. The model is trained with a cross-entropy loss and can be formulated as:\nL CE (\u03b8) = \u2212 1 T T t=1 log p(y t |y 1:t\u22121 , x, \u03b8)(3)\nwhere x is an input audio clip, y t is the t-th ground truth token in a sentence whose length is T , and \u03b8 are the parameters of the audio captioning model.\n\n\n2) Experimental Setup:\n\nWe adopt a two-stage training paradigm similar to the \"fine-tuning\" setting in audio-language retrieval. The whole model is first pretrained on WavCaps plus training sets from Clotho and AudioCaps using a learning rate of 5 \u00d7 10 \u22125 and a batch size of 48 for 15 epochs. The pretrained model is further fine-tuned on AudioCaps and Clotho for 20 epochs with a learning rate of 5 \u00d7 10 \u22126 , respectively. During the whole training process, we ensure no data from validation or test sets of Clotho and AudioCaps are used for training. To assess the impact of pretraining using WavCaps dataset, we train the two models exclusively on AudioCaps and Clotho as baselines. The performance are evaluated using conventional metrics including BLEU n [69], ROGUE l [70], METEOR [71], CIDEr [72], SPICE [73], and SPIDEr [74], where SPIDEr is generally employed as the main metric in the literature.\n\n3) Results and Analysis: Results are presented in Table VI. The results demonstrate that our models surpass existing methods, achieving new SOTA performance on the Clotho and AudioCaps datasets. Notably, our model outperforms even those methods that incorporate the validation set into their training for the Clotho dataset. Compared to the baseline systems, pretraining on WavCaps leads to a significant enhancement in the final performance on both datasets.\n\nIn alignment with findings from audio-language retrieval above, the CNN14 audio encoder outperforms the HTSAT audio encoder on the Clotho dataset but exhibits inferior performance on the AudioCaps dataset. These outcomes suggest that ChatGPT effectively transforms raw descriptions into caption-like sentences, thereby boosting audio captioning performance.\n\n\nC. Zero-Shot Audio Classification\n\nAudio classification aims at classifying the class of the sound presented in an audio clip. We carry out zero-shot audio classification on three popular audio event datasets to evaluate the generalization and robustness of models trained on our WavCaps dataset.\n\n1) Models: We formulate the audio classification task as an audio-language retrieval problem, following the approach used as in CLIP [35]. Initially, we encode all class labels as class embeddings using a text encoder without using any prompts. Each audio clip will be encoded by the audio encoder and then compared with the class embeddings to get a similarity score for each class. Those similarity scores will be normalized to get a final probability distribution over the classes. We use 'CNN14-BERT-PT' and 'HTSAT-BERT-PT' models described in Section IV-A.\n\n2) Experimental Setup: WavCaps, training sets of Au-dioCaps and Clotho are merged as a training set, where overlapping samples co-occurred in the evaluated datasets are excluded. The training settings are same with these in audiolanguage retrieval. Three audio event datasets, ESC-50 [2], UrbanSound8K [75], and VGGSound [76] are employed. ESC-50 is an environmental sound classification dataset consisting of 2000 5-seconds audio clips annotated with 50 classes. UrbanSound8K contains 8732 audio clips less or equal to 4 seconds of urban sounds from 10 classes. VGGSound contains about 200k audio clips for 310 classes sourced from YouTube videos. ESC-50 and UrbanSound8K are officially split into 5folds and 10-folds for cross-validation. Therefore, We use all audio clips in ESC-50 and UrbanSound8K, and the test set of VGGSound for evaluation. Top-1 accuracy is leveraged as the evaluation metric.\n\n3) Results and Analysis: Table VII presents the results, with the top row displaying the supervised SOTA performance on each dataset, while the remaining rows show zero-shot outcomes. Compared to our model, LAION and BLAT [82] both use more data to train their models. Our models achieved SOTA zero-shot results on all three datasets, significantly outperforming other models on ESC-50 and UrbanSound8K. In the case of VGGSound, Wu et al. [51] (LAION) reported a 46.2% accuracy, but their use of AudioSet for model training without excluding overlapping samples between AudioSet and VGGSound led to a data leakage issue. Overall, in comparison with LAION and BLAT, our approach yielded superior results using less data, which highlights the effectiveness of our proposed WavCaps dataset. The zero-shot results we obtained were close to the supervised SOTA on ESC-50 and UrbanSound8K, but exhibited a considerable margin on the VGGSound dataset. ESC-50 and UrbanSound8K consist of 50 and 10 classes, respectively, while VGGSound comprises 310 classes. The 310 classes may be too fine-grained, resulting in models trained on WavCaps struggling to generalize well across all of these classes.\n\n\nD. Text-based Sound Generation\n\nText-based sound generation is a task that generates sound, including speech, music, and sound effect, with textual information [16]. We follow previous studies [15]- [17] and perform training and evaluation on the AudioCaps [38] dataset.\n\n1) Models: AudioLDM [16] is a text-to-sound generation model that builds upon contrastive language-audio pretrained (CLAP) encoders. The well-trained CLAP encoders can capture the relations across modalities and relax the training difficulties of the audio generative model. The original AudioLDM, denoted by AudioLDM LAION , adopts a CLAP model developed by [51] with a dataset of around 2.6 million audio-text pairs. We re-implement AudioLDM with a CLAP model trained with our proposed WavCaps, denoted by AudioLDM WavCaps . We further finetuned our CLAP model on AudioCaps training set for text-to-sound generation, denoted by AudioLDM WavCaps-FT . Specifically, we use the pretrained Variational Autoencoder (VAE) and vocoder in the opensource implementation of AudioLDM 7 and trained a new latent diffusion model (LDM). The LDM in AudioLDM is trained with the re-weighted training objective [83], given by\nL n (\u03b8) = E z0, ,n \u2212 \u03b8 (z n , n, E x ) 2 2 , (4) q(z n |z 0 ) = N (z n ; \u221a\u1fb1 n z 0 , (1 \u2212\u1fb1 n ) ),(5)\nwhere \u223c N (0, I) denotes the standard gaussian noise,\u1fb1 n is the noise schedule [16], z 0 is the original VAE latent extracted from audio, and z n is the output of the n-th forward diffusion step. To benchmark our models, we also included two state-ofthe-art audio generation models, namely DiffSound [17] and AudioGen [15]. 2) Experimental Setup: The training and testing set split of AudioCaps are the same as the Audio Captioning experiments in Section IV-B. The LDM is optimized on the AudioCaps training set by an Adam optimizer with a learning rate of 3 \u00d7 10 \u22125 . We adopt a batch size of 8 and train LDM for a total of 400k steps. We perform the evaluations on the AudioCaps test set every 50k steps of training and select the result with the best Frechet Audio Distance (FAD) to report. The training data from AudioCaps are resampled to a 16kHz sampling rate before the model training. The setting on spectrogram calculation follows exactly the setting of [16]. For the AudioLDM-based models, we experiment with two different modalities as training conditions, namely audio embedding and text embedding from CLAP. The model conditioned on Audio leads to a self-supervised audio generative model training while the model conditioned on text is supervised with audiotext data pair.\n\nIn accordance with [16] and their methodology, we assess the performance of our models using a range of metrics, including the FAD, Inception Score (IS), KL divergence (KL), and PANNS-based Frechet Distance (FD). The FAD and FD metrics evaluate the similarity between two audio data distributions, while IS measures the diversity of the generated audio data and its similarity to the target audio data distribution. Additionally, KL provides a sample-level measure of similarity between generated and target samples.\n\n3) Results and Analysis: Table VIII shows the evaluation result of AudioLDM paired with different CLAP models as well as the baseline methods. Even with a much smaller dataset size (15%) compared with AudioLDM LAION , our AudioLDM WavCaps still achieves a comparable performance. AudioLDM WavCaps even performs better on FAD and the inception score when we use text as training embedding, indicating our dataset has better text labelling quality. When finetuned our CLAP model on AudioCaps, AudioLDM WavCaps-FT improve significantly on KL and FD, which even outperform the KL and FD of AudioLDM LAION . Nevertheless, our model does not perform well on IS when conditioned on audio embedding. This might attribute to the reason that our training data is on a smaller scale and limit the generalization ability, which can be mitigated by adding label-to-caption augmented audioset data [51] into CLAP training. Compared with the baseline methods, all our implementation of AudioLDM outperforms DiffSound and FAD of AudioGen by a large margin.\n\n\nE. Ablation Study\n\nWe further perform two ablation studies to investigate the impact of our processing steps for raw descriptions on learning audio-language multimodal representations and the performance contribution of main data sources in WavCaps dataset. We evaluate the zero-shot audio-language retrieval performance on AudioCaps and Clotho datasets by training the 'HTSAT-BERT' model on WavCaps (excluding AudioSet strongly-labelled subset) with raw descriptions and processed captions, and AudioSet strongly-labelled subset, respectively. The training settings are same with zero-shot setting in Section IV-A. Table IX presents the experimental results. The first two rows of the table compare the performance of models trained on WavCaps, excluding AudioSet strongly-labelled subset, using raw descriptions and processed captions. Our results demonstrate that the model trained with our processed captions on both datasets achieves significantly better performance compared to the model trained on raw descriptions. This highlights the effectiveness of utilizing ChatGPT to process raw descriptions, which leads to improved model performance.\n\nThe final three rows of the table illustrate the performance of models trained using different data sources in WavCaps. Models trained on the strongly-labeled subset of AudioSet exhibit superior performance on AudioCaps but underperform on Clotho. A possible explanation for this observation is that AudioCaps are derived from AudioSet, resulting in a better distribution match and consequently improved outcomes on AudioCaps. By integrating the four data sources, the comprehensive WavCaps dataset encompasses a greater volume of data and a wider range of content, which contributes to enhanced performance on both datasets, as demonstrated in the last row of the table.\n\n\nV. CONCLUSION\n\nData scarcity presents a significant challenge in audiolanguage multimodal learning research. In this study, we introduce WavCaps, a large-scale weakly-labelled audio captioning dataset, created by collecting audio clips and their corresponding raw descriptions from the web. A three-stage processing pipeline is proposed to filter and transform crawled raw descriptions into captions using ChatGPT. Our evaluation of the WavCaps dataset on multiple audio-language multimodal learning tasks resulted in the achievement of new stateof-the-art performance across all tasks. We hope that WavCaps will not only facilitate advancements in audio-language multimodal learning research, but also demonstrate the potential of leveraging ChatGPT to enrich academic research.\n\n\nACKNOWLEDGMENTS\n\nThis work is supported partly by a Newton Institutional Links Award from the British Council, titled \"Automated Captioning of Image and Audio for Visually and Hearing Impaired\" (Grant number 623805725), and a grant EP/T019751/1 from the Engineering and Physical Sciences Research Council (EPSRC). For the purpose of open access, the authors have applied a Creative Commons Attribution (CC BY) licence to any Author Accepted Manuscript version arising.\n\n\nX.Mei, H. Liu, M. D. Plumbley, and W. Wang are with the Centre for Vision, Speech, and Signal Processing, University of Surrey, Guildford, GU2 7XH, U.K. (E-mail: [x.mei, haohe.liu, m.plumbley, w.wang]@surrey.ac.uk) C. Meng is with Johns Hopkins University, U.S.A. (E-mail: cmeng9@jhu.edu) Q, Kong, T. Ko, and C. Zhao are with ByteDance, China. (E-mail: [kongqiuqiang, tom.ko, zhaochengqi.d]@bytedance.com) Y. Zou is with the School of Electronic and Computer Engineering, Peking University, Shenzhen Graduate School, Shenzhen, 518055, China. (E-mail: zouyx@pku.edu.cn)\n\nFig. 2 .\n2Word clouds of top 100 words in WavCaps dataset (top) and the entire harvested raw descriptions (bottom), stop words are ignored.\n\nFig. 3 .\n3Distribution of audio duration in WavCaps dataset, audio clips from AudioSet are ignored.\n\n\n1 https://openai.com/blog/chatgpt/\n\nTABLE\n\n\nTABLE II EXAMPLES\nIIOF RAW DESCRIPTIONS AND CHATGPT'S OUTPUTS.Large Tibetan Bells ringing in a temple. Could also use for Monastery or Monks.Bells are ringing. Female speech, woman speaking', 'Whoosh, swoosh, swish'] A woman is speaking while something whooshes.Data sources \nraw descriptions \nChatGPT's output \n\nFreeSound \n\nThis sound is of a book falling down the staircase in the Stanford University \nlibrary west stacks. \nA book is falling down a staircase. \n\nAfter 2h and 15min. Excerpt. \nFailure. \n\nBBC Sound Effects \nTimber & Wood -Rip saw, carpenters' workshop. \nSomeone is using a rip saw in a carpenter's workshop. \nMotor Car: Ford Ecsort 1300 (Automatic) -Ford Escort 1300, exterior, passing \nwith horn. (1300cc engine, automatic transmission.) \nA car is passing with its horn. \n\nSoundBible \n\nTasmanian Devil growling screaming hissing. Warning sounds from a Tasmanian \nDevil in Zoo. \nAn animal is growling, screaming, and hissing. \n\nAudioSet SL \n['Accelerating, revving, vroom', 'Race car, auto racing'] \nA race car is accelerating and revving. \n['\n\nTABLE III STATISTICS\nIIIOF HARVESTED RAW DATA AND WAVCAPS DATASET. of audio avg. audio duration (s) avg. text length num. of audio avg. audio duration (s) avg. text lengthData Sources \nBefore Processing \nAfter Processing \nnum. FreeSound \n567078 \n56.87 \n17.74 \n262300 \n85.98 \n6.77 \nBBC Sound Effects \n33064 \n115.75 \n15.91 \n31201 \n115.04 \n9.67 \nSoundBible \n1576 \n11.20 \n17.90 \n1232 \n13.12 \n5.87 \nAudioSet SL subset \n108317 \n10.00 \n-\n108317 \n10.00 \n9.79 \nWavCaps \n-\n-\n-\n403050 \n67.59 \n7.80 \n\n\n\nTABLE IV COMPARATIVE\nIVOVERVIEW OF MAIN AUDIO-LANGUAGE DATASETS BETWEEN OUR PROPOSED WAVCAPS DATASET.Dataset \nNum. audios Duration (h) \nText source \nAudioCaps [38] \n52904 \n144.94 \nHuman \nClotho [43] \n5929 \n37.00 \nHuman \nMACS [44] \n3537 \n9.83 \nHuman \nWavText5K [50] \n4072 \n23.20 \nOnline raw-data \nSoundDescs [8] \n32979 \n1060.4 \nOnline raw-data \nLAION-Audio-630K [51] \n633526 \n4325.39 \nOnline raw-data \nWavCaps \n403050 \n7567.92 \nChatGPT \n\n\n\nTABLE V EXPERIMENTAL\nVRESULTS OF AUDIO-LANGUAGE RETRIEVAL ON TEST SETS OF\n\nTABLE VI AUTOMATED\nVIAUDIO CAPTIONING RESULTS ON TEST SETS OF AUDIOCAPS AND CLOTHO. HIGHER SCORE MEANS BETTER PERFORMANCE. * INDICATES THE VALIDATION SET OF CLOTHO IS USED TO TRAIN THE MODEL.Dataset \nModel \nBLEU 1 \nBLEU 4 \nROUGE l \nMETEOR CIDER SPICE SPIDEr \n\nClotho \n\nMAAC [62] \n57.5 \n17.4 \n37.7 \n17.4 \n41.9 \n11.9 \n26.9 \nNetease [63] \n58.3 \n17.7 \n38.8 \n17.9 \n45.6 \n12.8 \n29.2 \nFeatureCut [64] \n60.1 \n17.9 \n38.9 \n17.6 \n43.6 \n12.2 \n27.9 \nCLIP-AAC* [65] \n57.2 \n16.9 \n37.9 \n17.1 \n40.7 \n11.9 \n26.3 \nNetease* [63] \n58.5 \n18.2 \n40.0 \n18.4 \n47.4 \n13.5 \n30.2 \nCNN14-BART (baseline) \n56.0 \n16.0 \n37.0 \n17.1 \n39.3 \n11.7 \n25.5 \nCNN14-BART \n60.1 \n18.0 \n40.0 \n18.5 \n48.8 \n13.3 \n31.0 \nHTSAT-BART (baseline) \n57.6 \n16.4 \n38.2 \n17.5 \n41.5 \n11.9 \n26.7 \nHTSAT-BART \n58.5 \n16.8 \n38.3 \n18.4 \n46.2 \n13.3 \n29.7 \n\nAudioCaps \n\nACT [47] \n64.7 \n25.2 \n46.8 \n22.2 \n67.9 \n16.0 \n42.0 \nV-ACT [66] \n69.8 \n28.1 \n49.4 \n23.7 \n71.1 \n17.2 \n44.2 \nBART-tags [11] \n69.9 \n26.6 \n49.3 \n24.1 \n75.3 \n17.6 \n46.5 \nAL-MixGEN [67] \n70.0 \n28.9 \n50.2 \n24.2 \n76.9 \n18.1 \n47.5 \nCNN14-BART (baseline) \n67.0 \n26.1 \n48.3 \n23.1 \n72.1 \n16.9 \n44.5 \nCNN14-BART \n69.3 \n27.2 \n49.9 \n24.7 \n75.6 \n17.9 \n46.8 \nHTSAT-BART (baseline) \n67.5 \n27.2 \n48.3 \n23.7 \n71.1 \n17.7 \n44.4 \nHTSAT-BART \n70.7 \n28.3 \n50.7 \n25.0 \n78.7 \n18.2 \n48.5 \n\n\n\nTABLE VII RESULTS\nVIIOF THE TOP-1 ACCURACY ON ZERO-SHOT AUDIO CLASSIFICATION.Model \nESC-50 \nUrbanSound8K \nVGGSound \nSupervised SOTA \n98.1 [77] \n90.0 [78] \n75.4 [51] \nWav2CLIP [79] \n41.4 \n40.4 \n10.0 \nAudioCLIP [80] \n69.4 \n65.3 \n-\nCLAP [81] \n82.6 \n73.2 \n-\nBLAT [82] \n80.6 \n77.3 \n14.9 \nLAION [51] \n91.0 \n77.0 \n29.1 (46.2) \nOurs \n94.8 \n80.6 \n29.6 \n\n\n\nTABLE VIII PERFORMANCE\nVIIICOMPARISON ON TEXT-BASED SOUND GENERATION. AUDIOGEN IS MARKED WITH \u2020 BECAUSE THE PRETRAINED MODEL AND THE EVALUATION DATA OF AUDIOGEN ARE NOT OPEN-SOURCED, WHICH MAY LEAD TO UNRELIABLE COMPARISON.Model \nTrain Condition FAD \u2193 \nIS \u2191 KL \u2193 \nFD \u2193 \nDiffSound [17] \n-\n7.75 \n4.01 \n2.52 \n47.68 \nAudioGen-base  \u2020 [15] \n-\n3.13 \n-\n2.09 \n-\n\nAudioLDM LAION [16] \nAudio \n2.98 \n7.12 \n2.2 \n24.04 \nText \n2.47 \n6.91 \n2.25 \n24.84 \n\nAudioLDM WavCaps \nAudio \n2.85 \n5.42 \n2.31 \n24.89 \nText \n2.29 \n7.05 \n2.3 \n26.3 \n\nAudioLDM WavCaps-FT \nAudio \n2.98 \n5.79 \n2.17 \n21.9 \nText \n2.58 \n6.38 \n2.23 \n25.27 \n\n\n\nTABLE IX EXPERIMENTAL\nIXRESULTS OF ZERO-SHOT AUDIO-LANGUAGE RETRIEVAL ON TEST SETS OF AUDIOCAPS AND CLOTHO. HIGHER SCORE MEANS BETTER PERFORMANCE.Training Data \n\nAudioCaps \nClotho \nText-to-Audio \nAudio-to-Text \nText-to-Audio \nAudio-to-Text \nR@1 \nR@5 \nR@10 \nR@1 \nR@5 \nR@10 \nR@1 \nR@5 R@10 \nR@1 \nR@5 R@10 \nWavCaps (raw) w/o AudioSet SL \n15.8 \n41.6 \n57.0 \n17.5 \n46.9 \n62.4 \n14.6 \n36.9 \n49.9 \n17.9 \n40.7 \n53.8 \nWavCaps w/o AudioSet SL \n18.1 \n47.5 \n62.2 \n22.2 \n50.8 \n66.8 \n15.2 \n38.1 \n51.2 \n18.5 \n41.7 \n54.6 \nAudioSet SL \n27.9 \n59.1 \n73.1 \n37.2 \n69.3 \n81.0 \n9.7 \n25.6 \n35.6 \n9.9 \n25.5 \n35.4 \nWavCaps \n28.6 \n61.1 \n75.8 \n40.2 \n69.4 \n80.3 \n16.5 \n38.8 \n50.9 \n20.0 \n43.3 \n56.6 \n\n\nhttps://freesound.org/\nUp to Dec 2022. 4 https://sound-effects.bbcrewind.co.uk/\nhttps://github.com/haoheliu/AudioLDM\n\nAudio Set: An ontology and human-labeled dataset for audio events. J F Gemmeke, D P W Ellis, D Freedman, A Jansen, W Lawrence, R C Moore, M Plakal, M Ritter, IEEE International Conference on Acoustics, Speech and Signal Processing. New Orleans, LAJ. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, \"Audio Set: An ontology and human-labeled dataset for audio events,\" in IEEE International Conference on Acoustics, Speech and Signal Processing, New Orleans, LA, 2017.\n\nESC: Dataset for Environmental Sound Classification. K J Piczak, Proceedings of the 23rd Annual ACM Conference on Multimedia. the 23rd Annual ACM Conference on MultimediaACM PressK. J. Piczak, \"ESC: Dataset for Environmental Sound Classification,\" in Proceedings of the 23rd Annual ACM Conference on Multimedia. ACM Press, pp. 1015-1018.\n\nFSD50K: An open dataset of human-labeled sound events. E Fonseca, X Favory, J Pons, F Font, X Serra, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 30E. Fonseca, X. Favory, J. Pons, F. Font, and X. Serra, \"FSD50K: An open dataset of human-labeled sound events,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, p. 829-852, 2022.\n\nWeakly labelled AudioSet tagging with attention neural networks. Q Kong, C Yu, Y Xu, T Iqbal, W Wang, M D Plumbley, Speech, and Language Processing. 27Q. Kong, C. Yu, Y. Xu, T. Iqbal, W. Wang, and M. D. Plumbley, \"Weakly labelled AudioSet tagging with attention neural networks,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 27, no. 11, pp. 1791-1802, 2019.\n\nSound event detection: A tutorial. A Mesaros, T Heittola, T Virtanen, M D Plumbley, IEEE Signal Processing Magazine. 385A. Mesaros, T. Heittola, T. Virtanen, and M. D. Plumbley, \"Sound event detection: A tutorial,\" IEEE Signal Processing Magazine, vol. 38, no. 5, pp. 67-83, 2021.\n\nUnsupervised feature learning based on deep models for environmental audio tagging. Y Xu, Q Huang, W Wang, P Foster, S Sigtia, P J B Jackson, M D Plumbley, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 256Y. Xu, Q. Huang, W. Wang, P. Foster, S. Sigtia, P. J. B. Jackson, and M. D. Plumbley, \"Unsupervised feature learning based on deep models for environmental audio tagging,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 25, no. 6, pp. 1230-1241, 2017.\n\nAcoustic scene classification: Classifying environments from the sounds they produce. D Barchiesi, D Giannoulis, D Stowell, M D Plumbley, IEEE Signal Processing Magazine. 323D. Barchiesi, D. Giannoulis, D. Stowell, and M. D. Plumbley, \"Acoustic scene classification: Classifying environments from the sounds they produce,\" IEEE Signal Processing Magazine, vol. 32, no. 3, pp. 16- 34, 2015.\n\nAudio retrieval with natural language queries: A benchmark study. A S Koepke, A.-M Oncescu, J Henriques, Z Akata, S Albanie, IEEE Transactions on Multimedia. A. S. Koepke, A.-M. Oncescu, J. Henriques, Z. Akata, and S. Albanie, \"Audio retrieval with natural language queries: A benchmark study,\" IEEE Transactions on Multimedia, 2022.\n\nOn metric learning for audio-text cross-modal retrieval. X Mei, X Liu, J Sun, M D Plumbley, W Wang, Proc. Interspeech. ISCA. Interspeech. ISCAX. Mei, X. Liu, J. Sun, M. D. Plumbley, and W. Wang, \"On metric learning for audio-text cross-modal retrieval,\" in Proc. Interspeech. ISCA, 2022.\n\nAutomated audio captioning: An overview of recent progress and new challenges. X Mei, X Liu, M D Plumbley, W Wang, Journal on Audio, Speech, and Music Processing. 26X. Mei, X. Liu, M. D. Plumbley, and W. Wang, \"Automated audio captioning: An overview of recent progress and new challenges,\" Journal on Audio, Speech, and Music Processing, no. 26, 2022.\n\nAutomated audio captioning by fine-tuning BART with AudioSet tags. F Gontier, R Serizel, C Cerisara, Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop. the 6th Detection and Classification of Acoustic Scenes and Events 2021 WorkshopBarcelona, SpainF. Gontier, R. Serizel, and C. Cerisara, \"Automated audio captioning by fine-tuning BART with AudioSet tags,\" in Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop, Barcelona, Spain, November 2021, pp. 170-174.\n\nA Transformer-based audio captioning model with keyword estimation. Y Koizumi, R Masumura, K Nishida, M Yasuda, S Saito, Proc. Interspeech. ISCA. Interspeech. ISCAY. Koizumi, R. Masumura, K. Nishida, M. Yasuda, and S. Saito, \"A Transformer-based audio captioning model with keyword estimation,\" in Proc. Interspeech. ISCA, 2020, pp. 1977-1981.\n\nClotho-AQA: A crowdsourced dataset for audio question answering. S Lipping, P Sudarsanam, K Drossos, T Virtanen, 2022 30th European Signal Processing Conference. S. Lipping, P. Sudarsanam, K. Drossos, and T. Virtanen, \"Clotho-AQA: A crowdsourced dataset for audio question answering,\" in 2022 30th European Signal Processing Conference, 2022, pp. 1140-1144.\n\nTemporal reasoning via audio question answering. H M Fayek, J Johnson, Speech, and Language Processing. 28H. M. Fayek and J. Johnson, \"Temporal reasoning via audio question answering,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, p. 2283-2294, Aug 2020.\n\nAudioGen: Textually guided audio generation. F Kreuk, G Synnaeve, A Polyak, U Singer, A D\u00e9fossez, J Copet, D Parikh, Y Taigman, Y Adi, arXiv:2209.15352arXiv preprintF. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. D\u00e9fossez, J. Copet, D. Parikh, Y. Taigman, and Y. Adi, \"AudioGen: Textually guided audio generation,\" arXiv preprint arXiv:2209.15352, 2022.\n\nAudioLDM: Text-to-audio generation with latent diffusion models. H Liu, Z Chen, Y Yuan, X Mei, X Liu, D Mandic, W Wang, M D Plumbley, arXiv:2301.12503arXiv preprintH. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang, and M. D. Plumbley, \"AudioLDM: Text-to-audio generation with latent diffusion models,\" arXiv preprint arXiv:2301.12503, 2023.\n\nDiffsound: Discrete diffusion model for text-to-sound generation. D Yang, J Yu, H Wang, W Wang, C Weng, Y Zou, D Yu, arXiv preprint:2207.09983D. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y. Zou, and D. Yu, \"Diffsound: Discrete diffusion model for text-to-sound generation,\" arXiv preprint:2207.09983, 2022.\n\nA Agostinelli, T I Denk, Z Borsos, J Engel, M Verzetti, A Caillon, Q Huang, A Jansen, A Roberts, M Tagliasacchi, arXiv:2301.11325MusicLM: Generating music from text. arXiv preprintA. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi et al., \"MusicLM: Generating music from text,\" arXiv preprint arXiv:2301.11325, 2023.\n\nSeparate what you describe: Language-queried audio source separation. X Liu, H Liu, Q Kong, X Mei, J Zhao, Q Huang, M D Plumbley, W Wang, Proc. Interspeech. ISCA. Interspeech. ISCAX. Liu, H. Liu, Q. Kong, X. Mei, J. Zhao, Q. Huang, M. D. Plumbley, and W. Wang, \"Separate what you describe: Language-queried audio source separation,\" in Proc. Interspeech. ISCA, 2022.\n\nMultimodal research in vision and language: A review of current and emerging trends. S Uppal, S Bhagat, D Hazarika, N Majumder, S Poria, R Zimmermann, A Zadeh, Information Fusion. 77S. Uppal, S. Bhagat, D. Hazarika, N. Majumder, S. Poria, R. Zimmer- mann, and A. Zadeh, \"Multimodal research in vision and language: A review of current and emerging trends,\" Information Fusion, vol. 77, pp. 149-171, 2022.\n\nVSE++: Improving visual-semantic embeddings with hard negatives. F Faghri, D J Fleet, J R Kiros, S Fidler, Proceedings of the British Machine Vision Conference. the British Machine Vision ConferenceF. Faghri, D. J. Fleet, J. R. Kiros, and S. Fidler, \"VSE++: Improving visual-semantic embeddings with hard negatives,\" in Proceedings of the British Machine Vision Conference, 2018.\n\nImageBERT: Cross-modal pre-training with large-scale weak-supervised image-text data. D Qi, L Su, J Song, E Cui, T Bharti, A Sacheti, arXiv:2001.07966arXiv preprintD. Qi, L. Su, J. Song, E. Cui, T. Bharti, and A. Sacheti, \"ImageBERT: Cross-modal pre-training with large-scale weak-supervised image-text data,\" arXiv preprint arXiv:2001.07966, 2020.\n\nUnified vision-language pre-training for image captioning and VQA. L Zhou, H Palangi, L Zhang, H Hu, J Corso, J Gao, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34L. Zhou, H. Palangi, L. Zhang, H. Hu, J. Corso, and J. Gao, \"Uni- fied vision-language pre-training for image captioning and VQA,\" in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 07, 2020, pp. 13 041-13 049.\n\nOSCAR: Object-semantics aligned pre-training for vision-language tasks. X Li, X Yin, C Li, X Hu, P Zhang, L Zhang, L Wang, H Hu, L Dong, F Wei, Y Choi, J Gao, European Conference on Computer Vision. X. Li, X. Yin, C. Li, X. Hu, P. Zhang, L. Zhang, L. Wang, H. Hu, L. Dong, F. Wei, Y. Choi, and J. Gao, \"OSCAR: Object-semantics aligned pre-training for vision-language tasks,\" European Conference on Computer Vision, 2020.\n\nHierarchical text-conditional image generation with CLIP latents. A Ramesh, P Dhariwal, A Nichol, C Chu, M Chen, arXiv:2204.06125arXiv preprintA. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, \"Hierarchical text-conditional image generation with CLIP latents,\" arXiv preprint arXiv:2204.06125, 2022.\n\nHighresolution image synthesis with latent diffusion models. R Rombach, A Blattmann, D Lorenz, P Esser, B Ommer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition10R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, \"High- resolution image synthesis with latent diffusion models,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni- tion, June 2022, pp. 10 684-10 695.\n\nVQA: Visual question answering. S Antol, A Agrawal, J Lu, M Mitchell, D Batra, C L Zitnick, D Parikh, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionS. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh, \"VQA: Visual question answering,\" in Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 2425- 2433.\n\nDeep modular coattention networks for visual question answering. Z Yu, J Yu, Y Cui, D Tao, Q Tian, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionZ. Yu, J. Yu, Y. Cui, D. Tao, and Q. Tian, \"Deep modular co- attention networks for visual question answering,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 6281-6290.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I Polosukhin, Advances in Neural Information Processing Systems. 30A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \"Attention is all you need,\" Advances in Neural Information Processing Systems, vol. 30, 2017.\n\nDeep learning. Y Lecun, Y Bengio, G Hinton, Nature. 5217553Y. LeCun, Y. Bengio, and G. Hinton, \"Deep learning,\" Nature, vol. 521, no. 7553, pp. 436-444, 2015.\n\nLearning representations by back-propagating errors. D E Rumelhart, G E Hinton, R J Williams, Nature. 3236088D. E. Rumelhart, G. E. Hinton, and R. J. Williams, \"Learning repre- sentations by back-propagating errors,\" Nature, vol. 323, no. 6088, pp. 533-536, 1986.\n\nAlign before fuse: Vision and language representation learning with momentum distillation. J Li, R Selvaraju, A Gotmare, S Joty, C Xiong, S C H Hoi, Advances in Neural Information Processing Systems. 34J. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and S. C. H. Hoi, \"Align before fuse: Vision and language representation learning with momentum distillation,\" Advances in Neural Information Processing Systems, vol. 34, pp. 9694-9705, 2021.\n\nBLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. J Li, D Li, C Xiong, S Hoi, International Conference on Machine Learning. PMLR, 2022. 900J. Li, D. Li, C. Xiong, and S. Hoi, \"BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation,\" in International Conference on Machine Learning. PMLR, 2022, pp. 12 888-12 900.\n\nScaling up visual and vision-language representation learning with noisy text supervision. C Jia, Y Yang, Y Xia, Y.-T Chen, Z Parekh, H Pham, Q Le, Y.-H Sung, Z Li, T Duerig, International Conference on Machine Learning. PMLR, 2021. C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig, \"Scaling up visual and vision-language representation learning with noisy text supervision,\" in International Conference on Machine Learning. PMLR, 2021, pp. 4904-4916.\n\nLearning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International Conference on Machine Learning. PMLRA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., \"Learning transferable visual models from natural language supervision,\" in International Conference on Machine Learning. PMLR, 2021, pp. 8748-8763.\n\nAST: Audio Spectrogram Transformer. Y Gong, Y.-A Chung, J Glass, Proc. Interspeech. InterspeechY. Gong, Y.-A. Chung, and J. Glass, \"AST: Audio Spectrogram Trans- former,\" in Proc. Interspeech, 2021, pp. 571-575.\n\nASiT: Audio Spectrogram vIsion Transformer for general audio representation. S Atito, M Awais, W Wang, M D Plumbley, J Kittler, arXiv:2211.13189arXiv preprintS. Atito, M. Awais, W. Wang, M. D. Plumbley, and J. Kittler, \"ASiT: Au- dio Spectrogram vIsion Transformer for general audio representation,\" arXiv preprint arXiv:2211.13189, 2022.\n\nAudioCaps: Generating captions for audios in the wild. C D Kim, B Kim, H Lee, G Kim, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesC. D. Kim, B. Kim, H. Lee, and G. Kim, \"AudioCaps: Generating captions for audios in the wild,\" in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019, pp. 119-132.\n\nIm-ageNet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"Im- ageNet: A large-scale hierarchical image database,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2009.\n\nMicrosoft COCO captions: Data collection and evaluation server. X Chen, H Fang, T.-Y Lin, R Vedantam, S Gupta, P Doll\u00e1r, C L Zitnick, arXiv:1504.00325arXiv preprintX. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Doll\u00e1r, and C. L. Zitnick, \"Microsoft COCO captions: Data collection and evaluation server,\" arXiv preprint arXiv:1504.00325, 2015.\n\nConceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. P Sharma, N Ding, S Goodman, R Soricut, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational LinguisticsP. Sharma, N. Ding, S. Goodman, and R. Soricut, \"Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image cap- tioning,\" in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Melbourne, Australia: Association for Computational Linguistics, Jul. 2018, pp. 2556-2565.\n\nConceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. S Changpinyo, P Sharma, N Ding, R Soricut, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionS. Changpinyo, P. Sharma, N. Ding, and R. Soricut, \"Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 3558-3568.\n\nClotho: An audio captioning dataset. K Drossos, S Lipping, T Virtanen, IEEE International Conference on Acoustics, Speech and Signal Processing. K. Drossos, S. Lipping, and T. Virtanen, \"Clotho: An audio captioning dataset,\" in IEEE International Conference on Acoustics, Speech and Signal Processing, 2020, pp. 736-740.\n\nDiversity and bias in audio captioning datasets. I Martin, A Mesaros, Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop. the 6th Detection and Classification of Acoustic Scenes and Events 2021 WorkshopBarcelona, SpainI. Martin and A. Mesaros, \"Diversity and bias in audio captioning datasets,\" in Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop, Barcelona, Spain, Novem- ber 2021, pp. 90-94.\n\nNoCaps: Novel object captioning at scale. H Agrawal, K Desai, Y Wang, X Chen, R Jain, M Johnson, D Batra, D Parikh, S Lee, P Anderson, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionH. Agrawal, K. Desai, Y. Wang, X. Chen, R. Jain, M. Johnson, D. Batra, D. Parikh, S. Lee, and P. Anderson, \"NoCaps: Novel object captioning at scale,\" in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 8948-8957.\n\nFrom image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. P Young, A Lai, M Hodosh, J Hockenmaier, Transactions of the Association for Computational Linguistics. 2P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, \"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions,\" Transactions of the Association for Computational Linguistics, vol. 2, pp. 67-78, 02 2014.\n\nAudio captioning Transformer. X Mei, X Liu, Q Huang, M D Plumbley, W Wang, Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop. the 6th Detection and Classification of Acoustic Scenes and Events 2021 WorkshopBarcelona, SpainX. Mei, X. Liu, Q. Huang, M. D. Plumbley, and W. Wang, \"Audio captioning Transformer,\" in Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop, Barcelona, Spain, November 2021, pp. 211-215.\n\nLeveraging pre-trained BERT for audio captioning. X Liu, X Mei, Q Huang, J Sun, J Zhao, H Liu, M D Plumbley, V K\u0131l\u0131\u00e7, W Wang, arXiv:2203.02838arXiv preprintX. Liu, X. Mei, Q. Huang, J. Sun, J. Zhao, H. Liu, M. D. Plumbley, V. K\u0131l\u0131\u00e7, and W. Wang, \"Leveraging pre-trained BERT for audio cap- tioning,\" arXiv preprint arXiv:2203.02838, 2022.\n\nAudio caption: Listen and tell. M Wu, H Dinkel, K Yu, IEEE International Conference on Acoustics, Speech and Signal Processing. M. Wu, H. Dinkel, and K. Yu, \"Audio caption: Listen and tell,\" in IEEE International Conference on Acoustics, Speech and Signal Processing, 2019, pp. 830-834.\n\nAudio retrieval with Wav-Text5K and CLAP training. S Deshmukh, B Elizalde, H Wang, arXiv:2209.14275arXiv preprintS. Deshmukh, B. Elizalde, and H. Wang, \"Audio retrieval with Wav- Text5K and CLAP training,\" arXiv preprint arXiv:2209.14275, 2022.\n\nLarge-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. Y Wu, * , K Chen, * , T Zhang, * , Y Hui, * , T Berg-Kirkpatrick, S Dubnov, IEEE International Conference on Acoustics, Speech and Signal Processing. Y. Wu*, K. Chen*, T. Zhang*, Y. Hui*, T. Berg-Kirkpatrick, and S. Dub- nov, \"Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation,\" in IEEE International Conference on Acoustics, Speech and Signal Processing, 2023.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, Journal of Machine Learning Research. 21140C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, \"Exploring the limits of transfer learning with a unified text-to-text transformer,\" Journal of Machine Learning Research, vol. 21, no. 140, pp. 1-67, 2020.\n\nPANNs: Large-scale pretrained audio neural networks for audio pattern recognition. Q Kong, Y Cao, T Iqbal, Y Wang, W Wang, M D Plumbley, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 28Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley, \"PANNs: Large-scale pretrained audio neural networks for audio pattern recognition,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 2880-2894, 2020.\n\nThe benefit of temporally-strong labels in audio event classification. S Hershey, D P W Ellis, E Fonseca, A Jansen, C Liu, R Moore, M Plakal, IEEE International Conference on Acoustics, Speech and Signal Processing. S. Hershey, D. P. W. Ellis, E. Fonseca, A. Jansen, C. Liu, R. Chan- ning Moore, and M. Plakal, \"The benefit of temporally-strong labels in audio event classification,\" in IEEE International Conference on Acoustics, Speech and Signal Processing, 2021, pp. 366-370.\n\nFreesound technical demo. F Font, G Roma, X Serra, Proceedings of the 21st ACM International Conference on Multimedia, ser. MM '13. the 21st ACM International Conference on Multimedia, ser. MM '13New York, NY, USAAssociation for Computing MachineryF. Font, G. Roma, and X. Serra, \"Freesound technical demo,\" in Proceedings of the 21st ACM International Conference on Multimedia, ser. MM '13. New York, NY, USA: Association for Computing Machinery, 2013, p. 411-412.\n\nDeep reinforcement learning from human preferences. P F Christiano, J Leike, T Brown, M Martic, S Legg, D Amodei, Advances in Neural Information Processing Systems. 30P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei, \"Deep reinforcement learning from human preferences,\" Advances in Neural Information Processing Systems, vol. 30, 2017.\n\nImproving text-audio retrieval by textaware attention pooling and prior matrix revised loss. Y Xin, D Yang, Y Zou, arXiv:2303.05681arXiv preprintY. Xin, D. Yang, and Y. Zou, \"Improving text-audio retrieval by text- aware attention pooling and prior matrix revised loss,\" arXiv preprint arXiv:2303.05681, 2023.\n\nHTS-AT: A hierarchical token-semantic audio transformer for sound classification and detection. K Chen, X Du, B Zhu, Z Ma, T Berg-Kirkpatrick, S Dubnov, IEEE International Conference on Acoustics, Speech and Signal Processing. K. Chen, X. Du, B. Zhu, Z. Ma, T. Berg-Kirkpatrick, and S. Dubnov, \"HTS-AT: A hierarchical token-semantic audio transformer for sound classification and detection,\" in IEEE International Conference on Acoustics, Speech and Signal Processing, 2022, pp. 646-650.\n\nBERT: Pretraining of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"BERT: Pre- training of deep bidirectional transformers for language understanding,\" in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1, 2019, pp. 4171-4186.\n\nA simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, G Hinton, International Conference on Machine Learning. PMLR, 2020. T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, \"A simple framework for contrastive learning of visual representations,\" in International Conference on Machine Learning. PMLR, 2020, pp. 1597-1607.\n\nADAM: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintD. P. Kingma and J. Ba, \"ADAM: A method for stochastic optimization,\" arXiv preprint arXiv:1412.6980, 2014.\n\nImproving the performance of automated audio captioning via integrating the acoustic and semantic information. Z Ye, H Wang, D Yang, Y Zou, Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop. the 6th Detection and Classification of Acoustic Scenes and Events 2021 WorkshopBarcelona, SpainZ. Ye, H. Wang, D. Yang, and Y. Zou, \"Improving the performance of automated audio captioning via integrating the acoustic and semantic information,\" in Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop, Barcelona, Spain, November 2021, pp. 40-44.\n\nAutomated audio captioning with weakly supervised pre-training and word selection methods. Q Han, W Yuan, D Liu, X Li, Z Yang, Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop. the 6th Detection and Classification of Acoustic Scenes and Events 2021 WorkshopBarcelona, SpainQ. Han, W. Yuan, D. Liu, X. Li, and Z. Yang, \"Automated audio caption- ing with weakly supervised pre-training and word selection methods,\" in Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop, Barcelona, Spain, November 2021, pp. 6- 10.\n\nFeatureCut: An adaptive data augmentation for automated audio captioning. Z Ye, Y Wang, H Wang, D Yang, Y Zou, 2022Z. Ye, Y. Wang, H. Wang, D. Yang, and Y. Zou, \"FeatureCut: An adaptive data augmentation for automated audio captioning,\" in 2022\n\nAsia-Pacific , Signal and Information Processing Association Annual Summit and Conference. Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, 2022, pp. 313-318.\n\nInteractive audio-text representation for automated audio captioning with contrastive learning. C Chen, N Hou, Y Hu, H Zou, X Qi, E S Chng, arXiv:2203.15526arXiv preprintC. Chen, N. Hou, Y. Hu, H. Zou, X. Qi, and E. S. Chng, \"Inter- active audio-text representation for automated audio captioning with contrastive learning,\" arXiv preprint arXiv:2203.15526, 2022.\n\nVisually-aware audio captioning with adaptive audio-visual attention. X Liu, Q Huang, X Mei, H Liu, Q Kong, J Sun, S Li, T Ko, Y Zhang, L H Tang, arXiv:2210.16428arXiv preprintX. Liu, Q. Huang, X. Mei, H. Liu, Q. Kong, J. Sun, S. Li, T. Ko, Y. Zhang, L. H. Tang et al., \"Visually-aware audio captioning with adaptive audio-visual attention,\" arXiv preprint arXiv:2210.16428, 2022.\n\nImproving audio-language learning with MixGen and multi-level testtime augmentation. E Kim, J Kim, Y Oh, K Kim, M Park, J Sim, J Lee, K Lee, arXiv:2210.17143arXiv preprintE. Kim, J. Kim, Y. Oh, K. Kim, M. Park, J. Sim, J. Lee, and K. Lee, \"Improving audio-language learning with MixGen and multi-level test- time augmentation,\" arXiv preprint arXiv:2210.17143, 2022.\n\nBART: Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsM. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, \"BART: Denoising sequence-to- sequence pre-training for natural language generation, translation, and comprehension,\" in Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics. Association for Computational Linguistics, Jul. 2020, pp. 7871-7880.\n\nBLEU: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsK. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, \"BLEU: a method for automatic evaluation of machine translation,\" in Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, 2002, pp. 311-318.\n\nROUGE: A package for automatic evaluation of summaries. C.-Y. Lin, Text Summarization Branches Out. Association for Computational LinguisticsC.-Y. Lin, \"ROUGE: A package for automatic evaluation of summaries,\" in Text Summarization Branches Out. Association for Computational Linguistics, 2004, pp. 74-81.\n\nMETEOR: An automatic metric for MT evaluation with improved correlation with human judgments. S Banerjee, A Lavie, Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or SummarizationS. Banerjee and A. Lavie, \"METEOR: An automatic metric for MT evaluation with improved correlation with human judgments,\" Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pp. 65-72, 2005.\n\nCIDEr: Consensusbased image description evaluation. R Vedantam, C Lawrence Zitnick, D Parikh, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionR. Vedantam, C. Lawrence Zitnick, and D. Parikh, \"CIDEr: Consensus- based image description evaluation,\" in Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition, 2015, pp. 4566-4575.\n\nSPICE: Semantic propositional image caption evaluation. P Anderson, B Fernando, M Johnson, S Gould, European Conference on Computer Vision. SpringerP. Anderson, B. Fernando, M. Johnson, and S. Gould, \"SPICE: Semantic propositional image caption evaluation,\" in European Conference on Computer Vision. Springer, 2016, pp. 382-398.\n\nImproved image captioning via policy gradient optimization of SPIDEr. S Liu, Z Zhu, N Ye, S Guadarrama, K Murphy, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionS. Liu, Z. Zhu, N. Ye, S. Guadarrama, and K. Murphy, \"Improved image captioning via policy gradient optimization of SPIDEr,\" in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 873-881.\n\nA dataset and taxonomy for urban sound research. J Salamon, C Jacoby, J P Bello, 22nd ACM International Conference on Multimedia. Orlando, FL, USAJ. Salamon, C. Jacoby, and J. P. Bello, \"A dataset and taxonomy for urban sound research,\" in 22nd ACM International Conference on Multimedia, Orlando, FL, USA, Nov. 2014, pp. 1041-1044.\n\nVGGSound: A largescale audio-visual dataset. H Chen, W Xie, A Vedaldi, A Zisserman, International Conference on Acoustics, Speech, and Signal Processing. H. Chen, W. Xie, A. Vedaldi, and A. Zisserman, \"VGGSound: A large- scale audio-visual dataset,\" in International Conference on Acoustics, Speech, and Signal Processing, 2020.\n\nBEATs: Audio pre-training with acoustic tokenizers. S Chen, Y Wu, C Wang, S Liu, D Tompkins, Z Chen, F Wei, arXiv:2212.09058arXiv preprintS. Chen, Y. Wu, C. Wang, S. Liu, D. Tompkins, Z. Chen, and F. Wei, \"BEATs: Audio pre-training with acoustic tokenizers,\" arXiv preprint arXiv:2212.09058, 2022.\n\nEnd-to-end audio strikes back: Boosting augmentations towards an efficient audio classification network. A Gazneli, G Zimerman, T Ridnik, G Sharir, A Noy, arXiv:2204.11479arXiv preprintA. Gazneli, G. Zimerman, T. Ridnik, G. Sharir, and A. Noy, \"End-to-end audio strikes back: Boosting augmentations towards an efficient audio classification network,\" arXiv preprint arXiv:2204.11479, 2022.\n\nWav2CLIP: Learning robust audio representations from CLIP. H.-H Wu, P Seetharaman, K Kumar, J P Bello, IEEE International Conference on Acoustics, Speech and Signal Processing. H.-H. Wu, P. Seetharaman, K. Kumar, and J. P. Bello, \"Wav2CLIP: Learning robust audio representations from CLIP,\" in IEEE International Conference on Acoustics, Speech and Signal Processing, 2022, pp. 4563-4567.\n\nAudioCLIP: Extending CLIP to image, text and audio. A Guzhov, F Raue, J Hees, A Dengel, IEEE International Conference on Acoustics, Speech and Signal Processing. A. Guzhov, F. Raue, J. Hees, and A. Dengel, \"AudioCLIP: Extending CLIP to image, text and audio,\" in IEEE International Conference on Acoustics, Speech and Signal Processing, 2022, pp. 976-980.\n\nCLAP: Learning audio concepts from natural language supervision. B Elizalde, S Deshmukh, M A Ismail, H Wang, arXiv:2206.04769arXiv preprintB. Elizalde, S. Deshmukh, M. A. Ismail, and H. Wang, \"CLAP: Learning audio concepts from natural language supervision,\" arXiv preprint arXiv:2206.04769, 2022.\n\nBLAT: Bootstrapping language-audio pre-training based on AudioSet tag-guided synthetic data. X Xu, Z Zhang, Z Zhou, P Zhang, Z Xie, M Wu, K Q Zhu, arXiv:2303.07902arXiv preprintX. Xu, Z. Zhang, Z. Zhou, P. Zhang, Z. Xie, M. Wu, and K. Q. Zhu, \"BLAT: Bootstrapping language-audio pre-training based on AudioSet tag-guided synthetic data,\" arXiv preprint arXiv:2303.07902, 2023.\n\nDenoising diffusion probabilistic models. J Ho, A Jain, P Abbeel, Advances in Neural Information Processing Systems. 33J. Ho, A. Jain, and P. Abbeel, \"Denoising diffusion probabilistic models,\" Advances in Neural Information Processing Systems, vol. 33, pp. 6840- 6851, 2020.\n", "annotations": {"author": "[{\"end\":122,\"start\":111},{\"end\":136,\"start\":123},{\"end\":147,\"start\":137},{\"end\":162,\"start\":148},{\"end\":170,\"start\":163},{\"end\":184,\"start\":171},{\"end\":201,\"start\":185},{\"end\":214,\"start\":202},{\"end\":226,\"start\":215}]", "publisher": null, "author_last_name": "[{\"end\":121,\"start\":118},{\"end\":135,\"start\":131},{\"end\":146,\"start\":143},{\"end\":161,\"start\":157},{\"end\":169,\"start\":167},{\"end\":183,\"start\":179},{\"end\":200,\"start\":192},{\"end\":213,\"start\":210},{\"end\":225,\"start\":221}]", "author_first_name": "[{\"end\":117,\"start\":111},{\"end\":130,\"start\":123},{\"end\":142,\"start\":137},{\"end\":156,\"start\":148},{\"end\":166,\"start\":163},{\"end\":178,\"start\":171},{\"end\":189,\"start\":185},{\"end\":191,\"start\":190},{\"end\":209,\"start\":202},{\"end\":220,\"start\":215}]", "author_affiliation": null, "title": "[{\"end\":108,\"start\":1},{\"end\":334,\"start\":227}]", "venue": null, "abstract": "[{\"end\":1880,\"start\":410}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2073,\"start\":2070},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2078,\"start\":2075},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2196,\"start\":2193},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2201,\"start\":2198},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2464,\"start\":2461},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2469,\"start\":2466},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2502,\"start\":2498},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2508,\"start\":2504},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2539,\"start\":2535},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2545,\"start\":2541},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2579,\"start\":2575},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2585,\"start\":2581},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2623,\"start\":2619},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2831,\"start\":2827},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3009,\"start\":3005},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3015,\"start\":3011},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3048,\"start\":3044},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3054,\"start\":3050},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3085,\"start\":3081},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3091,\"start\":3087},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3127,\"start\":3123},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3133,\"start\":3129},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3305,\"start\":3301},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3392,\"start\":3388},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3434,\"start\":3430},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3656,\"start\":3652},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3662,\"start\":3658},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3668,\"start\":3664},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3846,\"start\":3842},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3852,\"start\":3848},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4034,\"start\":4030},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4040,\"start\":4036},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4314,\"start\":4311},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4398,\"start\":4394},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4548,\"start\":4544},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4566,\"start\":4562},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5090,\"start\":5086},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5096,\"start\":5092},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5706,\"start\":5702},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6914,\"start\":6910},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6920,\"start\":6916},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6926,\"start\":6922},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8763,\"start\":8759},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8801,\"start\":8797},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8817,\"start\":8813},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9209,\"start\":9205},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9223,\"start\":9219},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9240,\"start\":9236},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9770,\"start\":9766},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9776,\"start\":9772},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9830,\"start\":9826},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9836,\"start\":9832},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9842,\"start\":9838},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10090,\"start\":10086},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10455,\"start\":10451},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10461,\"start\":10457},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":10467,\"start\":10463},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10502,\"start\":10499},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10507,\"start\":10504},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10538,\"start\":10534},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10544,\"start\":10540},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10587,\"start\":10583},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10647,\"start\":10643},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10663,\"start\":10659},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10731,\"start\":10728},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10999,\"start\":10995},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":11021,\"start\":11017},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11332,\"start\":11329},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":12338,\"start\":12334},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":12514,\"start\":12510},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":13141,\"start\":13137},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":17038,\"start\":17034},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17171,\"start\":17168},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":17425,\"start\":17421},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17849,\"start\":17848},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":20904,\"start\":20900},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24982,\"start\":24979},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27047,\"start\":27044},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27344,\"start\":27341},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":27350,\"start\":27346},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":27617,\"start\":27613},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":27654,\"start\":27650},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":27772,\"start\":27768},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":28664,\"start\":28660},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":29341,\"start\":29337},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":32093,\"start\":32089},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":34004,\"start\":34000},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":34370,\"start\":34366},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":34811,\"start\":34807},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":35012,\"start\":35008},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":36203,\"start\":36199},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":36217,\"start\":36213},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":36230,\"start\":36226},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":36242,\"start\":36238},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":36254,\"start\":36250},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":36271,\"start\":36267},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":37603,\"start\":37599},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":38316,\"start\":38313},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":38335,\"start\":38331},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":38354,\"start\":38350},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":39158,\"start\":39154},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":39375,\"start\":39371},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40288,\"start\":40284},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":40321,\"start\":40317},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":40327,\"start\":40323},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":40385,\"start\":40381},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40420,\"start\":40416},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":40759,\"start\":40755},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":41296,\"start\":41292},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":41490,\"start\":41486},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":41711,\"start\":41707},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":41729,\"start\":41725},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":42374,\"start\":42370},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":42718,\"start\":42714},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":44101,\"start\":44097}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":47902,\"start\":47332},{\"attributes\":{\"id\":\"fig_1\"},\"end\":48043,\"start\":47903},{\"attributes\":{\"id\":\"fig_2\"},\"end\":48144,\"start\":48044},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":48181,\"start\":48145},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":48189,\"start\":48182},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":49251,\"start\":48190},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":49742,\"start\":49252},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":50181,\"start\":49743},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":50256,\"start\":50182},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":51522,\"start\":50257},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":51869,\"start\":51523},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":52474,\"start\":51870},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":53144,\"start\":52475}]", "paragraph": "[{\"end\":2745,\"start\":1899},{\"end\":3853,\"start\":2747},{\"end\":4844,\"start\":3855},{\"end\":6023,\"start\":4846},{\"end\":7269,\"start\":6025},{\"end\":7951,\"start\":7271},{\"end\":8322,\"start\":7953},{\"end\":9777,\"start\":8374},{\"end\":10212,\"start\":9779},{\"end\":11213,\"start\":10243},{\"end\":11389,\"start\":11215},{\"end\":11463,\"start\":11434},{\"end\":11504,\"start\":11496},{\"end\":11541,\"start\":11506},{\"end\":11577,\"start\":11543},{\"end\":11638,\"start\":11597},{\"end\":11662,\"start\":11640},{\"end\":12656,\"start\":11664},{\"end\":13107,\"start\":12658},{\"end\":15142,\"start\":13127},{\"end\":16016,\"start\":15167},{\"end\":16483,\"start\":16018},{\"end\":16998,\"start\":16485},{\"end\":17657,\"start\":17000},{\"end\":17873,\"start\":17659},{\"end\":19794,\"start\":17896},{\"end\":19969,\"start\":19796},{\"end\":20650,\"start\":19971},{\"end\":22308,\"start\":20652},{\"end\":22708,\"start\":22310},{\"end\":23631,\"start\":22710},{\"end\":26138,\"start\":23655},{\"end\":26809,\"start\":26158},{\"end\":28102,\"start\":26841},{\"end\":28526,\"start\":28161},{\"end\":28718,\"start\":28528},{\"end\":28979,\"start\":28827},{\"end\":31900,\"start\":28981},{\"end\":34131,\"start\":31929},{\"end\":34440,\"start\":34165},{\"end\":35225,\"start\":34442},{\"end\":35435,\"start\":35279},{\"end\":36345,\"start\":35462},{\"end\":36806,\"start\":36347},{\"end\":37165,\"start\":36808},{\"end\":37464,\"start\":37203},{\"end\":38027,\"start\":37466},{\"end\":38930,\"start\":38029},{\"end\":40121,\"start\":38932},{\"end\":40394,\"start\":40156},{\"end\":41306,\"start\":40396},{\"end\":42693,\"start\":41407},{\"end\":43211,\"start\":42695},{\"end\":44253,\"start\":43213},{\"end\":45405,\"start\":44275},{\"end\":46078,\"start\":45407},{\"end\":46860,\"start\":46096},{\"end\":47331,\"start\":46880}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":28160,\"start\":28103},{\"attributes\":{\"id\":\"formula_1\"},\"end\":28826,\"start\":28727},{\"attributes\":{\"id\":\"formula_2\"},\"end\":35278,\"start\":35226},{\"attributes\":{\"id\":\"formula_3\"},\"end\":41406,\"start\":41307}]", "table_ref": "[{\"end\":15106,\"start\":14983},{\"end\":21546,\"start\":21539},{\"end\":21741,\"start\":21733},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":21813,\"start\":21805},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":24784,\"start\":24776},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":30650,\"start\":30643},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":36405,\"start\":36397},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":38966,\"start\":38957},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":43248,\"start\":43238},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":44880,\"start\":44872}]", "section_header": "[{\"end\":1897,\"start\":1882},{\"end\":8342,\"start\":8325},{\"end\":8372,\"start\":8345},{\"end\":10241,\"start\":10215},{\"end\":11405,\"start\":11392},{\"end\":11432,\"start\":11408},{\"end\":11494,\"start\":11466},{\"end\":11595,\"start\":11580},{\"end\":13125,\"start\":13110},{\"end\":15165,\"start\":15145},{\"end\":17894,\"start\":17876},{\"end\":23653,\"start\":23634},{\"end\":26156,\"start\":26141},{\"end\":26839,\"start\":26812},{\"end\":28726,\"start\":28721},{\"end\":31927,\"start\":31903},{\"end\":34163,\"start\":34134},{\"end\":35460,\"start\":35438},{\"end\":37201,\"start\":37168},{\"end\":40154,\"start\":40124},{\"end\":44273,\"start\":44256},{\"end\":46094,\"start\":46081},{\"end\":46878,\"start\":46863},{\"end\":47912,\"start\":47904},{\"end\":48053,\"start\":48045},{\"end\":48188,\"start\":48183},{\"end\":48208,\"start\":48191},{\"end\":49273,\"start\":49253},{\"end\":49764,\"start\":49744},{\"end\":50203,\"start\":50183},{\"end\":50276,\"start\":50258},{\"end\":51541,\"start\":51524},{\"end\":51893,\"start\":51871},{\"end\":52497,\"start\":52476}]", "table": "[{\"end\":49251,\"start\":48453},{\"end\":49742,\"start\":49424},{\"end\":50181,\"start\":49845},{\"end\":51522,\"start\":50449},{\"end\":51869,\"start\":51601},{\"end\":52474,\"start\":52094},{\"end\":53144,\"start\":52622}]", "figure_caption": "[{\"end\":47902,\"start\":47334},{\"end\":48043,\"start\":47914},{\"end\":48144,\"start\":48055},{\"end\":48181,\"start\":48147},{\"end\":48453,\"start\":48211},{\"end\":49424,\"start\":49277},{\"end\":49845,\"start\":49767},{\"end\":50256,\"start\":50205},{\"end\":50449,\"start\":50279},{\"end\":51601,\"start\":51545},{\"end\":52094,\"start\":51898},{\"end\":52622,\"start\":52500}]", "figure_ref": "[{\"end\":13043,\"start\":13037},{\"end\":18616,\"start\":18608},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24341,\"start\":24335},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25449,\"start\":25443},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25586,\"start\":25580}]", "bib_author_first_name": "[{\"end\":53331,\"start\":53330},{\"end\":53333,\"start\":53332},{\"end\":53344,\"start\":53343},{\"end\":53348,\"start\":53345},{\"end\":53357,\"start\":53356},{\"end\":53369,\"start\":53368},{\"end\":53379,\"start\":53378},{\"end\":53391,\"start\":53390},{\"end\":53393,\"start\":53392},{\"end\":53402,\"start\":53401},{\"end\":53412,\"start\":53411},{\"end\":53841,\"start\":53840},{\"end\":53843,\"start\":53842},{\"end\":54182,\"start\":54181},{\"end\":54193,\"start\":54192},{\"end\":54203,\"start\":54202},{\"end\":54211,\"start\":54210},{\"end\":54219,\"start\":54218},{\"end\":54565,\"start\":54564},{\"end\":54573,\"start\":54572},{\"end\":54579,\"start\":54578},{\"end\":54585,\"start\":54584},{\"end\":54594,\"start\":54593},{\"end\":54602,\"start\":54601},{\"end\":54604,\"start\":54603},{\"end\":54919,\"start\":54918},{\"end\":54930,\"start\":54929},{\"end\":54942,\"start\":54941},{\"end\":54954,\"start\":54953},{\"end\":54956,\"start\":54955},{\"end\":55250,\"start\":55249},{\"end\":55256,\"start\":55255},{\"end\":55265,\"start\":55264},{\"end\":55273,\"start\":55272},{\"end\":55283,\"start\":55282},{\"end\":55293,\"start\":55292},{\"end\":55297,\"start\":55294},{\"end\":55308,\"start\":55307},{\"end\":55310,\"start\":55309},{\"end\":55751,\"start\":55750},{\"end\":55764,\"start\":55763},{\"end\":55778,\"start\":55777},{\"end\":55789,\"start\":55788},{\"end\":55791,\"start\":55790},{\"end\":56122,\"start\":56121},{\"end\":56124,\"start\":56123},{\"end\":56137,\"start\":56133},{\"end\":56148,\"start\":56147},{\"end\":56161,\"start\":56160},{\"end\":56170,\"start\":56169},{\"end\":56448,\"start\":56447},{\"end\":56455,\"start\":56454},{\"end\":56462,\"start\":56461},{\"end\":56469,\"start\":56468},{\"end\":56471,\"start\":56470},{\"end\":56483,\"start\":56482},{\"end\":56759,\"start\":56758},{\"end\":56766,\"start\":56765},{\"end\":56773,\"start\":56772},{\"end\":56775,\"start\":56774},{\"end\":56787,\"start\":56786},{\"end\":57101,\"start\":57100},{\"end\":57112,\"start\":57111},{\"end\":57123,\"start\":57122},{\"end\":57653,\"start\":57652},{\"end\":57664,\"start\":57663},{\"end\":57676,\"start\":57675},{\"end\":57687,\"start\":57686},{\"end\":57697,\"start\":57696},{\"end\":57995,\"start\":57994},{\"end\":58006,\"start\":58005},{\"end\":58020,\"start\":58019},{\"end\":58031,\"start\":58030},{\"end\":58338,\"start\":58337},{\"end\":58340,\"start\":58339},{\"end\":58349,\"start\":58348},{\"end\":58618,\"start\":58617},{\"end\":58627,\"start\":58626},{\"end\":58639,\"start\":58638},{\"end\":58649,\"start\":58648},{\"end\":58659,\"start\":58658},{\"end\":58671,\"start\":58670},{\"end\":58680,\"start\":58679},{\"end\":58690,\"start\":58689},{\"end\":58701,\"start\":58700},{\"end\":58993,\"start\":58992},{\"end\":59000,\"start\":58999},{\"end\":59008,\"start\":59007},{\"end\":59016,\"start\":59015},{\"end\":59023,\"start\":59022},{\"end\":59030,\"start\":59029},{\"end\":59040,\"start\":59039},{\"end\":59048,\"start\":59047},{\"end\":59050,\"start\":59049},{\"end\":59347,\"start\":59346},{\"end\":59355,\"start\":59354},{\"end\":59361,\"start\":59360},{\"end\":59369,\"start\":59368},{\"end\":59377,\"start\":59376},{\"end\":59385,\"start\":59384},{\"end\":59392,\"start\":59391},{\"end\":59587,\"start\":59586},{\"end\":59602,\"start\":59601},{\"end\":59604,\"start\":59603},{\"end\":59612,\"start\":59611},{\"end\":59622,\"start\":59621},{\"end\":59631,\"start\":59630},{\"end\":59643,\"start\":59642},{\"end\":59654,\"start\":59653},{\"end\":59663,\"start\":59662},{\"end\":59673,\"start\":59672},{\"end\":59684,\"start\":59683},{\"end\":60047,\"start\":60046},{\"end\":60054,\"start\":60053},{\"end\":60061,\"start\":60060},{\"end\":60069,\"start\":60068},{\"end\":60076,\"start\":60075},{\"end\":60084,\"start\":60083},{\"end\":60093,\"start\":60092},{\"end\":60095,\"start\":60094},{\"end\":60107,\"start\":60106},{\"end\":60430,\"start\":60429},{\"end\":60439,\"start\":60438},{\"end\":60449,\"start\":60448},{\"end\":60461,\"start\":60460},{\"end\":60473,\"start\":60472},{\"end\":60482,\"start\":60481},{\"end\":60496,\"start\":60495},{\"end\":60816,\"start\":60815},{\"end\":60826,\"start\":60825},{\"end\":60828,\"start\":60827},{\"end\":60837,\"start\":60836},{\"end\":60839,\"start\":60838},{\"end\":60848,\"start\":60847},{\"end\":61218,\"start\":61217},{\"end\":61224,\"start\":61223},{\"end\":61230,\"start\":61229},{\"end\":61238,\"start\":61237},{\"end\":61245,\"start\":61244},{\"end\":61255,\"start\":61254},{\"end\":61549,\"start\":61548},{\"end\":61557,\"start\":61556},{\"end\":61568,\"start\":61567},{\"end\":61577,\"start\":61576},{\"end\":61583,\"start\":61582},{\"end\":61592,\"start\":61591},{\"end\":62022,\"start\":62021},{\"end\":62028,\"start\":62027},{\"end\":62035,\"start\":62034},{\"end\":62041,\"start\":62040},{\"end\":62047,\"start\":62046},{\"end\":62056,\"start\":62055},{\"end\":62065,\"start\":62064},{\"end\":62073,\"start\":62072},{\"end\":62079,\"start\":62078},{\"end\":62087,\"start\":62086},{\"end\":62094,\"start\":62093},{\"end\":62102,\"start\":62101},{\"end\":62439,\"start\":62438},{\"end\":62449,\"start\":62448},{\"end\":62461,\"start\":62460},{\"end\":62471,\"start\":62470},{\"end\":62478,\"start\":62477},{\"end\":62741,\"start\":62740},{\"end\":62752,\"start\":62751},{\"end\":62765,\"start\":62764},{\"end\":62775,\"start\":62774},{\"end\":62784,\"start\":62783},{\"end\":63221,\"start\":63220},{\"end\":63230,\"start\":63229},{\"end\":63241,\"start\":63240},{\"end\":63247,\"start\":63246},{\"end\":63259,\"start\":63258},{\"end\":63268,\"start\":63267},{\"end\":63270,\"start\":63269},{\"end\":63281,\"start\":63280},{\"end\":63688,\"start\":63687},{\"end\":63694,\"start\":63693},{\"end\":63700,\"start\":63699},{\"end\":63707,\"start\":63706},{\"end\":63714,\"start\":63713},{\"end\":64118,\"start\":64117},{\"end\":64129,\"start\":64128},{\"end\":64140,\"start\":64139},{\"end\":64150,\"start\":64149},{\"end\":64163,\"start\":64162},{\"end\":64172,\"start\":64171},{\"end\":64174,\"start\":64173},{\"end\":64183,\"start\":64182},{\"end\":64193,\"start\":64192},{\"end\":64473,\"start\":64472},{\"end\":64482,\"start\":64481},{\"end\":64492,\"start\":64491},{\"end\":64671,\"start\":64670},{\"end\":64673,\"start\":64672},{\"end\":64686,\"start\":64685},{\"end\":64688,\"start\":64687},{\"end\":64698,\"start\":64697},{\"end\":64700,\"start\":64699},{\"end\":64974,\"start\":64973},{\"end\":64980,\"start\":64979},{\"end\":64993,\"start\":64992},{\"end\":65004,\"start\":65003},{\"end\":65012,\"start\":65011},{\"end\":65021,\"start\":65020},{\"end\":65025,\"start\":65022},{\"end\":65436,\"start\":65435},{\"end\":65442,\"start\":65441},{\"end\":65448,\"start\":65447},{\"end\":65457,\"start\":65456},{\"end\":65841,\"start\":65840},{\"end\":65848,\"start\":65847},{\"end\":65856,\"start\":65855},{\"end\":65866,\"start\":65862},{\"end\":65874,\"start\":65873},{\"end\":65884,\"start\":65883},{\"end\":65892,\"start\":65891},{\"end\":65901,\"start\":65897},{\"end\":65909,\"start\":65908},{\"end\":65915,\"start\":65914},{\"end\":66322,\"start\":66321},{\"end\":66333,\"start\":66332},{\"end\":66335,\"start\":66334},{\"end\":66342,\"start\":66341},{\"end\":66353,\"start\":66352},{\"end\":66363,\"start\":66362},{\"end\":66370,\"start\":66369},{\"end\":66381,\"start\":66380},{\"end\":66391,\"start\":66390},{\"end\":66401,\"start\":66400},{\"end\":66412,\"start\":66411},{\"end\":66774,\"start\":66773},{\"end\":66785,\"start\":66781},{\"end\":66794,\"start\":66793},{\"end\":67028,\"start\":67027},{\"end\":67037,\"start\":67036},{\"end\":67046,\"start\":67045},{\"end\":67054,\"start\":67053},{\"end\":67056,\"start\":67055},{\"end\":67068,\"start\":67067},{\"end\":67346,\"start\":67345},{\"end\":67348,\"start\":67347},{\"end\":67355,\"start\":67354},{\"end\":67362,\"start\":67361},{\"end\":67369,\"start\":67368},{\"end\":67964,\"start\":67963},{\"end\":67972,\"start\":67971},{\"end\":67980,\"start\":67979},{\"end\":67993,\"start\":67989},{\"end\":67999,\"start\":67998},{\"end\":68005,\"start\":68004},{\"end\":68441,\"start\":68440},{\"end\":68449,\"start\":68448},{\"end\":68460,\"start\":68456},{\"end\":68467,\"start\":68466},{\"end\":68479,\"start\":68478},{\"end\":68488,\"start\":68487},{\"end\":68498,\"start\":68497},{\"end\":68500,\"start\":68499},{\"end\":68828,\"start\":68827},{\"end\":68838,\"start\":68837},{\"end\":68846,\"start\":68845},{\"end\":68857,\"start\":68856},{\"end\":69523,\"start\":69522},{\"end\":69537,\"start\":69536},{\"end\":69547,\"start\":69546},{\"end\":69555,\"start\":69554},{\"end\":70011,\"start\":70010},{\"end\":70022,\"start\":70021},{\"end\":70033,\"start\":70032},{\"end\":70345,\"start\":70344},{\"end\":70355,\"start\":70354},{\"end\":70825,\"start\":70824},{\"end\":70836,\"start\":70835},{\"end\":70845,\"start\":70844},{\"end\":70853,\"start\":70852},{\"end\":70861,\"start\":70860},{\"end\":70869,\"start\":70868},{\"end\":70880,\"start\":70879},{\"end\":70889,\"start\":70888},{\"end\":70899,\"start\":70898},{\"end\":70906,\"start\":70905},{\"end\":71402,\"start\":71401},{\"end\":71411,\"start\":71410},{\"end\":71418,\"start\":71417},{\"end\":71428,\"start\":71427},{\"end\":71798,\"start\":71797},{\"end\":71805,\"start\":71804},{\"end\":71812,\"start\":71811},{\"end\":71821,\"start\":71820},{\"end\":71823,\"start\":71822},{\"end\":71835,\"start\":71834},{\"end\":72320,\"start\":72319},{\"end\":72327,\"start\":72326},{\"end\":72334,\"start\":72333},{\"end\":72343,\"start\":72342},{\"end\":72350,\"start\":72349},{\"end\":72358,\"start\":72357},{\"end\":72365,\"start\":72364},{\"end\":72367,\"start\":72366},{\"end\":72379,\"start\":72378},{\"end\":72388,\"start\":72387},{\"end\":72642,\"start\":72641},{\"end\":72648,\"start\":72647},{\"end\":72658,\"start\":72657},{\"end\":72949,\"start\":72948},{\"end\":72961,\"start\":72960},{\"end\":72973,\"start\":72972},{\"end\":73252,\"start\":73251},{\"end\":73258,\"start\":73257},{\"end\":73262,\"start\":73261},{\"end\":73270,\"start\":73269},{\"end\":73274,\"start\":73273},{\"end\":73283,\"start\":73282},{\"end\":73287,\"start\":73286},{\"end\":73294,\"start\":73293},{\"end\":73298,\"start\":73297},{\"end\":73318,\"start\":73317},{\"end\":73755,\"start\":73754},{\"end\":73765,\"start\":73764},{\"end\":73776,\"start\":73775},{\"end\":73787,\"start\":73786},{\"end\":73794,\"start\":73793},{\"end\":73804,\"start\":73803},{\"end\":73814,\"start\":73813},{\"end\":73822,\"start\":73821},{\"end\":73828,\"start\":73827},{\"end\":73830,\"start\":73829},{\"end\":74217,\"start\":74216},{\"end\":74225,\"start\":74224},{\"end\":74232,\"start\":74231},{\"end\":74241,\"start\":74240},{\"end\":74249,\"start\":74248},{\"end\":74257,\"start\":74256},{\"end\":74259,\"start\":74258},{\"end\":74655,\"start\":74654},{\"end\":74666,\"start\":74665},{\"end\":74670,\"start\":74667},{\"end\":74679,\"start\":74678},{\"end\":74690,\"start\":74689},{\"end\":74700,\"start\":74699},{\"end\":74707,\"start\":74706},{\"end\":74716,\"start\":74715},{\"end\":75091,\"start\":75090},{\"end\":75099,\"start\":75098},{\"end\":75107,\"start\":75106},{\"end\":75584,\"start\":75583},{\"end\":75586,\"start\":75585},{\"end\":75600,\"start\":75599},{\"end\":75609,\"start\":75608},{\"end\":75618,\"start\":75617},{\"end\":75628,\"start\":75627},{\"end\":75636,\"start\":75635},{\"end\":75986,\"start\":75985},{\"end\":75993,\"start\":75992},{\"end\":76001,\"start\":76000},{\"end\":76300,\"start\":76299},{\"end\":76308,\"start\":76307},{\"end\":76314,\"start\":76313},{\"end\":76321,\"start\":76320},{\"end\":76327,\"start\":76326},{\"end\":76347,\"start\":76346},{\"end\":76774,\"start\":76773},{\"end\":76787,\"start\":76783},{\"end\":76796,\"start\":76795},{\"end\":76803,\"start\":76802},{\"end\":77473,\"start\":77472},{\"end\":77481,\"start\":77480},{\"end\":77494,\"start\":77493},{\"end\":77505,\"start\":77504},{\"end\":77817,\"start\":77816},{\"end\":77819,\"start\":77818},{\"end\":77829,\"start\":77828},{\"end\":78084,\"start\":78083},{\"end\":78090,\"start\":78089},{\"end\":78098,\"start\":78097},{\"end\":78106,\"start\":78105},{\"end\":78692,\"start\":78691},{\"end\":78699,\"start\":78698},{\"end\":78707,\"start\":78706},{\"end\":78714,\"start\":78713},{\"end\":78720,\"start\":78719},{\"end\":79280,\"start\":79279},{\"end\":79286,\"start\":79285},{\"end\":79294,\"start\":79293},{\"end\":79302,\"start\":79301},{\"end\":79310,\"start\":79309},{\"end\":79463,\"start\":79451},{\"end\":79748,\"start\":79747},{\"end\":79756,\"start\":79755},{\"end\":79763,\"start\":79762},{\"end\":79769,\"start\":79768},{\"end\":79776,\"start\":79775},{\"end\":79782,\"start\":79781},{\"end\":79784,\"start\":79783},{\"end\":80087,\"start\":80086},{\"end\":80094,\"start\":80093},{\"end\":80103,\"start\":80102},{\"end\":80110,\"start\":80109},{\"end\":80117,\"start\":80116},{\"end\":80125,\"start\":80124},{\"end\":80132,\"start\":80131},{\"end\":80138,\"start\":80137},{\"end\":80144,\"start\":80143},{\"end\":80153,\"start\":80152},{\"end\":80155,\"start\":80154},{\"end\":80484,\"start\":80483},{\"end\":80491,\"start\":80490},{\"end\":80498,\"start\":80497},{\"end\":80504,\"start\":80503},{\"end\":80511,\"start\":80510},{\"end\":80519,\"start\":80518},{\"end\":80526,\"start\":80525},{\"end\":80533,\"start\":80532},{\"end\":80881,\"start\":80880},{\"end\":80890,\"start\":80889},{\"end\":80897,\"start\":80896},{\"end\":80906,\"start\":80905},{\"end\":80923,\"start\":80922},{\"end\":80934,\"start\":80933},{\"end\":80942,\"start\":80941},{\"end\":80954,\"start\":80953},{\"end\":81617,\"start\":81616},{\"end\":81629,\"start\":81628},{\"end\":81639,\"start\":81638},{\"end\":81650,\"start\":81646},{\"end\":82104,\"start\":82099},{\"end\":82445,\"start\":82444},{\"end\":82457,\"start\":82456},{\"end\":82836,\"start\":82835},{\"end\":82848,\"start\":82847},{\"end\":82857,\"start\":82849},{\"end\":82868,\"start\":82867},{\"end\":83286,\"start\":83285},{\"end\":83298,\"start\":83297},{\"end\":83310,\"start\":83309},{\"end\":83321,\"start\":83320},{\"end\":83631,\"start\":83630},{\"end\":83638,\"start\":83637},{\"end\":83645,\"start\":83644},{\"end\":83651,\"start\":83650},{\"end\":83665,\"start\":83664},{\"end\":84062,\"start\":84061},{\"end\":84073,\"start\":84072},{\"end\":84083,\"start\":84082},{\"end\":84085,\"start\":84084},{\"end\":84392,\"start\":84391},{\"end\":84400,\"start\":84399},{\"end\":84407,\"start\":84406},{\"end\":84418,\"start\":84417},{\"end\":84729,\"start\":84728},{\"end\":84737,\"start\":84736},{\"end\":84743,\"start\":84742},{\"end\":84751,\"start\":84750},{\"end\":84758,\"start\":84757},{\"end\":84770,\"start\":84769},{\"end\":84778,\"start\":84777},{\"end\":85081,\"start\":85080},{\"end\":85092,\"start\":85091},{\"end\":85104,\"start\":85103},{\"end\":85114,\"start\":85113},{\"end\":85124,\"start\":85123},{\"end\":85429,\"start\":85425},{\"end\":85435,\"start\":85434},{\"end\":85450,\"start\":85449},{\"end\":85459,\"start\":85458},{\"end\":85461,\"start\":85460},{\"end\":85809,\"start\":85808},{\"end\":85819,\"start\":85818},{\"end\":85827,\"start\":85826},{\"end\":85835,\"start\":85834},{\"end\":86179,\"start\":86178},{\"end\":86191,\"start\":86190},{\"end\":86203,\"start\":86202},{\"end\":86205,\"start\":86204},{\"end\":86215,\"start\":86214},{\"end\":86506,\"start\":86505},{\"end\":86512,\"start\":86511},{\"end\":86521,\"start\":86520},{\"end\":86529,\"start\":86528},{\"end\":86538,\"start\":86537},{\"end\":86545,\"start\":86544},{\"end\":86551,\"start\":86550},{\"end\":86553,\"start\":86552},{\"end\":86833,\"start\":86832},{\"end\":86839,\"start\":86838},{\"end\":86847,\"start\":86846}]", "bib_author_last_name": "[{\"end\":53341,\"start\":53334},{\"end\":53354,\"start\":53349},{\"end\":53366,\"start\":53358},{\"end\":53376,\"start\":53370},{\"end\":53388,\"start\":53380},{\"end\":53399,\"start\":53394},{\"end\":53409,\"start\":53403},{\"end\":53419,\"start\":53413},{\"end\":53850,\"start\":53844},{\"end\":54190,\"start\":54183},{\"end\":54200,\"start\":54194},{\"end\":54208,\"start\":54204},{\"end\":54216,\"start\":54212},{\"end\":54225,\"start\":54220},{\"end\":54570,\"start\":54566},{\"end\":54576,\"start\":54574},{\"end\":54582,\"start\":54580},{\"end\":54591,\"start\":54586},{\"end\":54599,\"start\":54595},{\"end\":54613,\"start\":54605},{\"end\":54927,\"start\":54920},{\"end\":54939,\"start\":54931},{\"end\":54951,\"start\":54943},{\"end\":54965,\"start\":54957},{\"end\":55253,\"start\":55251},{\"end\":55262,\"start\":55257},{\"end\":55270,\"start\":55266},{\"end\":55280,\"start\":55274},{\"end\":55290,\"start\":55284},{\"end\":55305,\"start\":55298},{\"end\":55319,\"start\":55311},{\"end\":55761,\"start\":55752},{\"end\":55775,\"start\":55765},{\"end\":55786,\"start\":55779},{\"end\":55800,\"start\":55792},{\"end\":56131,\"start\":56125},{\"end\":56145,\"start\":56138},{\"end\":56158,\"start\":56149},{\"end\":56167,\"start\":56162},{\"end\":56178,\"start\":56171},{\"end\":56452,\"start\":56449},{\"end\":56459,\"start\":56456},{\"end\":56466,\"start\":56463},{\"end\":56480,\"start\":56472},{\"end\":56488,\"start\":56484},{\"end\":56763,\"start\":56760},{\"end\":56770,\"start\":56767},{\"end\":56784,\"start\":56776},{\"end\":56792,\"start\":56788},{\"end\":57109,\"start\":57102},{\"end\":57120,\"start\":57113},{\"end\":57132,\"start\":57124},{\"end\":57661,\"start\":57654},{\"end\":57673,\"start\":57665},{\"end\":57684,\"start\":57677},{\"end\":57694,\"start\":57688},{\"end\":57703,\"start\":57698},{\"end\":58003,\"start\":57996},{\"end\":58017,\"start\":58007},{\"end\":58028,\"start\":58021},{\"end\":58040,\"start\":58032},{\"end\":58346,\"start\":58341},{\"end\":58357,\"start\":58350},{\"end\":58624,\"start\":58619},{\"end\":58636,\"start\":58628},{\"end\":58646,\"start\":58640},{\"end\":58656,\"start\":58650},{\"end\":58668,\"start\":58660},{\"end\":58677,\"start\":58672},{\"end\":58687,\"start\":58681},{\"end\":58698,\"start\":58691},{\"end\":58705,\"start\":58702},{\"end\":58997,\"start\":58994},{\"end\":59005,\"start\":59001},{\"end\":59013,\"start\":59009},{\"end\":59020,\"start\":59017},{\"end\":59027,\"start\":59024},{\"end\":59037,\"start\":59031},{\"end\":59045,\"start\":59041},{\"end\":59059,\"start\":59051},{\"end\":59352,\"start\":59348},{\"end\":59358,\"start\":59356},{\"end\":59366,\"start\":59362},{\"end\":59374,\"start\":59370},{\"end\":59382,\"start\":59378},{\"end\":59389,\"start\":59386},{\"end\":59395,\"start\":59393},{\"end\":59599,\"start\":59588},{\"end\":59609,\"start\":59605},{\"end\":59619,\"start\":59613},{\"end\":59628,\"start\":59623},{\"end\":59640,\"start\":59632},{\"end\":59651,\"start\":59644},{\"end\":59660,\"start\":59655},{\"end\":59670,\"start\":59664},{\"end\":59681,\"start\":59674},{\"end\":59697,\"start\":59685},{\"end\":60051,\"start\":60048},{\"end\":60058,\"start\":60055},{\"end\":60066,\"start\":60062},{\"end\":60073,\"start\":60070},{\"end\":60081,\"start\":60077},{\"end\":60090,\"start\":60085},{\"end\":60104,\"start\":60096},{\"end\":60112,\"start\":60108},{\"end\":60436,\"start\":60431},{\"end\":60446,\"start\":60440},{\"end\":60458,\"start\":60450},{\"end\":60470,\"start\":60462},{\"end\":60479,\"start\":60474},{\"end\":60493,\"start\":60483},{\"end\":60502,\"start\":60497},{\"end\":60823,\"start\":60817},{\"end\":60834,\"start\":60829},{\"end\":60845,\"start\":60840},{\"end\":60855,\"start\":60849},{\"end\":61221,\"start\":61219},{\"end\":61227,\"start\":61225},{\"end\":61235,\"start\":61231},{\"end\":61242,\"start\":61239},{\"end\":61252,\"start\":61246},{\"end\":61263,\"start\":61256},{\"end\":61554,\"start\":61550},{\"end\":61565,\"start\":61558},{\"end\":61574,\"start\":61569},{\"end\":61580,\"start\":61578},{\"end\":61589,\"start\":61584},{\"end\":61596,\"start\":61593},{\"end\":62025,\"start\":62023},{\"end\":62032,\"start\":62029},{\"end\":62038,\"start\":62036},{\"end\":62044,\"start\":62042},{\"end\":62053,\"start\":62048},{\"end\":62062,\"start\":62057},{\"end\":62070,\"start\":62066},{\"end\":62076,\"start\":62074},{\"end\":62084,\"start\":62080},{\"end\":62091,\"start\":62088},{\"end\":62099,\"start\":62095},{\"end\":62106,\"start\":62103},{\"end\":62446,\"start\":62440},{\"end\":62458,\"start\":62450},{\"end\":62468,\"start\":62462},{\"end\":62475,\"start\":62472},{\"end\":62483,\"start\":62479},{\"end\":62749,\"start\":62742},{\"end\":62762,\"start\":62753},{\"end\":62772,\"start\":62766},{\"end\":62781,\"start\":62776},{\"end\":62790,\"start\":62785},{\"end\":63227,\"start\":63222},{\"end\":63238,\"start\":63231},{\"end\":63244,\"start\":63242},{\"end\":63256,\"start\":63248},{\"end\":63265,\"start\":63260},{\"end\":63278,\"start\":63271},{\"end\":63288,\"start\":63282},{\"end\":63691,\"start\":63689},{\"end\":63697,\"start\":63695},{\"end\":63704,\"start\":63701},{\"end\":63711,\"start\":63708},{\"end\":63719,\"start\":63715},{\"end\":64126,\"start\":64119},{\"end\":64137,\"start\":64130},{\"end\":64147,\"start\":64141},{\"end\":64160,\"start\":64151},{\"end\":64169,\"start\":64164},{\"end\":64180,\"start\":64175},{\"end\":64190,\"start\":64184},{\"end\":64204,\"start\":64194},{\"end\":64479,\"start\":64474},{\"end\":64489,\"start\":64483},{\"end\":64499,\"start\":64493},{\"end\":64683,\"start\":64674},{\"end\":64695,\"start\":64689},{\"end\":64709,\"start\":64701},{\"end\":64977,\"start\":64975},{\"end\":64990,\"start\":64981},{\"end\":65001,\"start\":64994},{\"end\":65009,\"start\":65005},{\"end\":65018,\"start\":65013},{\"end\":65029,\"start\":65026},{\"end\":65439,\"start\":65437},{\"end\":65445,\"start\":65443},{\"end\":65454,\"start\":65449},{\"end\":65461,\"start\":65458},{\"end\":65845,\"start\":65842},{\"end\":65853,\"start\":65849},{\"end\":65860,\"start\":65857},{\"end\":65871,\"start\":65867},{\"end\":65881,\"start\":65875},{\"end\":65889,\"start\":65885},{\"end\":65895,\"start\":65893},{\"end\":65906,\"start\":65902},{\"end\":65912,\"start\":65910},{\"end\":65922,\"start\":65916},{\"end\":66330,\"start\":66323},{\"end\":66339,\"start\":66336},{\"end\":66350,\"start\":66343},{\"end\":66360,\"start\":66354},{\"end\":66367,\"start\":66364},{\"end\":66378,\"start\":66371},{\"end\":66388,\"start\":66382},{\"end\":66398,\"start\":66392},{\"end\":66409,\"start\":66402},{\"end\":66418,\"start\":66413},{\"end\":66779,\"start\":66775},{\"end\":66791,\"start\":66786},{\"end\":66800,\"start\":66795},{\"end\":67034,\"start\":67029},{\"end\":67043,\"start\":67038},{\"end\":67051,\"start\":67047},{\"end\":67065,\"start\":67057},{\"end\":67076,\"start\":67069},{\"end\":67352,\"start\":67349},{\"end\":67359,\"start\":67356},{\"end\":67366,\"start\":67363},{\"end\":67373,\"start\":67370},{\"end\":67969,\"start\":67965},{\"end\":67977,\"start\":67973},{\"end\":67987,\"start\":67981},{\"end\":67996,\"start\":67994},{\"end\":68002,\"start\":68000},{\"end\":68013,\"start\":68006},{\"end\":68446,\"start\":68442},{\"end\":68454,\"start\":68450},{\"end\":68464,\"start\":68461},{\"end\":68476,\"start\":68468},{\"end\":68485,\"start\":68480},{\"end\":68495,\"start\":68489},{\"end\":68508,\"start\":68501},{\"end\":68835,\"start\":68829},{\"end\":68843,\"start\":68839},{\"end\":68854,\"start\":68847},{\"end\":68865,\"start\":68858},{\"end\":69534,\"start\":69524},{\"end\":69544,\"start\":69538},{\"end\":69552,\"start\":69548},{\"end\":69563,\"start\":69556},{\"end\":70019,\"start\":70012},{\"end\":70030,\"start\":70023},{\"end\":70042,\"start\":70034},{\"end\":70352,\"start\":70346},{\"end\":70363,\"start\":70356},{\"end\":70833,\"start\":70826},{\"end\":70842,\"start\":70837},{\"end\":70850,\"start\":70846},{\"end\":70858,\"start\":70854},{\"end\":70866,\"start\":70862},{\"end\":70877,\"start\":70870},{\"end\":70886,\"start\":70881},{\"end\":70896,\"start\":70890},{\"end\":70903,\"start\":70900},{\"end\":70915,\"start\":70907},{\"end\":71408,\"start\":71403},{\"end\":71415,\"start\":71412},{\"end\":71425,\"start\":71419},{\"end\":71440,\"start\":71429},{\"end\":71802,\"start\":71799},{\"end\":71809,\"start\":71806},{\"end\":71818,\"start\":71813},{\"end\":71832,\"start\":71824},{\"end\":71840,\"start\":71836},{\"end\":72324,\"start\":72321},{\"end\":72331,\"start\":72328},{\"end\":72340,\"start\":72335},{\"end\":72347,\"start\":72344},{\"end\":72355,\"start\":72351},{\"end\":72362,\"start\":72359},{\"end\":72376,\"start\":72368},{\"end\":72385,\"start\":72380},{\"end\":72393,\"start\":72389},{\"end\":72645,\"start\":72643},{\"end\":72655,\"start\":72649},{\"end\":72661,\"start\":72659},{\"end\":72958,\"start\":72950},{\"end\":72970,\"start\":72962},{\"end\":72978,\"start\":72974},{\"end\":73255,\"start\":73253},{\"end\":73267,\"start\":73263},{\"end\":73280,\"start\":73275},{\"end\":73291,\"start\":73288},{\"end\":73315,\"start\":73299},{\"end\":73325,\"start\":73319},{\"end\":73762,\"start\":73756},{\"end\":73773,\"start\":73766},{\"end\":73784,\"start\":73777},{\"end\":73791,\"start\":73788},{\"end\":73801,\"start\":73795},{\"end\":73811,\"start\":73805},{\"end\":73819,\"start\":73815},{\"end\":73825,\"start\":73823},{\"end\":73834,\"start\":73831},{\"end\":74222,\"start\":74218},{\"end\":74229,\"start\":74226},{\"end\":74238,\"start\":74233},{\"end\":74246,\"start\":74242},{\"end\":74254,\"start\":74250},{\"end\":74268,\"start\":74260},{\"end\":74663,\"start\":74656},{\"end\":74676,\"start\":74671},{\"end\":74687,\"start\":74680},{\"end\":74697,\"start\":74691},{\"end\":74704,\"start\":74701},{\"end\":74713,\"start\":74708},{\"end\":74723,\"start\":74717},{\"end\":75096,\"start\":75092},{\"end\":75104,\"start\":75100},{\"end\":75113,\"start\":75108},{\"end\":75597,\"start\":75587},{\"end\":75606,\"start\":75601},{\"end\":75615,\"start\":75610},{\"end\":75625,\"start\":75619},{\"end\":75633,\"start\":75629},{\"end\":75643,\"start\":75637},{\"end\":75990,\"start\":75987},{\"end\":75998,\"start\":75994},{\"end\":76005,\"start\":76002},{\"end\":76305,\"start\":76301},{\"end\":76311,\"start\":76309},{\"end\":76318,\"start\":76315},{\"end\":76324,\"start\":76322},{\"end\":76344,\"start\":76328},{\"end\":76354,\"start\":76348},{\"end\":76781,\"start\":76775},{\"end\":76793,\"start\":76788},{\"end\":76800,\"start\":76797},{\"end\":76813,\"start\":76804},{\"end\":77478,\"start\":77474},{\"end\":77491,\"start\":77482},{\"end\":77502,\"start\":77495},{\"end\":77512,\"start\":77506},{\"end\":77826,\"start\":77820},{\"end\":77832,\"start\":77830},{\"end\":78087,\"start\":78085},{\"end\":78095,\"start\":78091},{\"end\":78103,\"start\":78099},{\"end\":78110,\"start\":78107},{\"end\":78696,\"start\":78693},{\"end\":78704,\"start\":78700},{\"end\":78711,\"start\":78708},{\"end\":78717,\"start\":78715},{\"end\":78725,\"start\":78721},{\"end\":79283,\"start\":79281},{\"end\":79291,\"start\":79287},{\"end\":79299,\"start\":79295},{\"end\":79307,\"start\":79303},{\"end\":79314,\"start\":79311},{\"end\":79753,\"start\":79749},{\"end\":79760,\"start\":79757},{\"end\":79766,\"start\":79764},{\"end\":79773,\"start\":79770},{\"end\":79779,\"start\":79777},{\"end\":79789,\"start\":79785},{\"end\":80091,\"start\":80088},{\"end\":80100,\"start\":80095},{\"end\":80107,\"start\":80104},{\"end\":80114,\"start\":80111},{\"end\":80122,\"start\":80118},{\"end\":80129,\"start\":80126},{\"end\":80135,\"start\":80133},{\"end\":80141,\"start\":80139},{\"end\":80150,\"start\":80145},{\"end\":80160,\"start\":80156},{\"end\":80488,\"start\":80485},{\"end\":80495,\"start\":80492},{\"end\":80501,\"start\":80499},{\"end\":80508,\"start\":80505},{\"end\":80516,\"start\":80512},{\"end\":80523,\"start\":80520},{\"end\":80530,\"start\":80527},{\"end\":80537,\"start\":80534},{\"end\":80887,\"start\":80882},{\"end\":80894,\"start\":80891},{\"end\":80903,\"start\":80898},{\"end\":80920,\"start\":80907},{\"end\":80931,\"start\":80924},{\"end\":80939,\"start\":80935},{\"end\":80951,\"start\":80943},{\"end\":80966,\"start\":80955},{\"end\":81626,\"start\":81618},{\"end\":81636,\"start\":81630},{\"end\":81644,\"start\":81640},{\"end\":81654,\"start\":81651},{\"end\":82108,\"start\":82105},{\"end\":82454,\"start\":82446},{\"end\":82463,\"start\":82458},{\"end\":82845,\"start\":82837},{\"end\":82865,\"start\":82858},{\"end\":82875,\"start\":82869},{\"end\":83295,\"start\":83287},{\"end\":83307,\"start\":83299},{\"end\":83318,\"start\":83311},{\"end\":83327,\"start\":83322},{\"end\":83635,\"start\":83632},{\"end\":83642,\"start\":83639},{\"end\":83648,\"start\":83646},{\"end\":83662,\"start\":83652},{\"end\":83672,\"start\":83666},{\"end\":84070,\"start\":84063},{\"end\":84080,\"start\":84074},{\"end\":84091,\"start\":84086},{\"end\":84397,\"start\":84393},{\"end\":84404,\"start\":84401},{\"end\":84415,\"start\":84408},{\"end\":84428,\"start\":84419},{\"end\":84734,\"start\":84730},{\"end\":84740,\"start\":84738},{\"end\":84748,\"start\":84744},{\"end\":84755,\"start\":84752},{\"end\":84767,\"start\":84759},{\"end\":84775,\"start\":84771},{\"end\":84782,\"start\":84779},{\"end\":85089,\"start\":85082},{\"end\":85101,\"start\":85093},{\"end\":85111,\"start\":85105},{\"end\":85121,\"start\":85115},{\"end\":85128,\"start\":85125},{\"end\":85432,\"start\":85430},{\"end\":85447,\"start\":85436},{\"end\":85456,\"start\":85451},{\"end\":85467,\"start\":85462},{\"end\":85816,\"start\":85810},{\"end\":85824,\"start\":85820},{\"end\":85832,\"start\":85828},{\"end\":85842,\"start\":85836},{\"end\":86188,\"start\":86180},{\"end\":86200,\"start\":86192},{\"end\":86212,\"start\":86206},{\"end\":86220,\"start\":86216},{\"end\":86509,\"start\":86507},{\"end\":86518,\"start\":86513},{\"end\":86526,\"start\":86522},{\"end\":86535,\"start\":86530},{\"end\":86542,\"start\":86539},{\"end\":86548,\"start\":86546},{\"end\":86557,\"start\":86554},{\"end\":86836,\"start\":86834},{\"end\":86844,\"start\":86840},{\"end\":86854,\"start\":86848}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":21519176},\"end\":53785,\"start\":53263},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":17567398},\"end\":54124,\"start\":53787},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":222090007},\"end\":54497,\"start\":54126},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":67855512},\"end\":54881,\"start\":54499},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":235795366},\"end\":55163,\"start\":54883},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":18665692},\"end\":55662,\"start\":55165},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206486170},\"end\":56053,\"start\":55664},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":245329572},\"end\":56388,\"start\":56055},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":247779240},\"end\":56677,\"start\":56390},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":248722072},\"end\":57031,\"start\":56679},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":245355790},\"end\":57582,\"start\":57033},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":220280692},\"end\":57927,\"start\":57584},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":248266787},\"end\":58286,\"start\":57929},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":208202191},\"end\":58570,\"start\":58288},{\"attributes\":{\"doi\":\"arXiv:2209.15352\",\"id\":\"b14\"},\"end\":58925,\"start\":58572},{\"attributes\":{\"doi\":\"arXiv:2301.12503\",\"id\":\"b15\"},\"end\":59278,\"start\":58927},{\"attributes\":{\"doi\":\"arXiv preprint:2207.09983\",\"id\":\"b16\"},\"end\":59584,\"start\":59280},{\"attributes\":{\"doi\":\"arXiv:2301.11325\",\"id\":\"b17\"},\"end\":59974,\"start\":59586},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":247778595},\"end\":60342,\"start\":59976},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":238639167},\"end\":60748,\"start\":60344},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6095318},\"end\":61129,\"start\":60750},{\"attributes\":{\"doi\":\"arXiv:2001.07966\",\"id\":\"b21\"},\"end\":61479,\"start\":61131},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":202734445},\"end\":61947,\"start\":61481},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":215754208},\"end\":62370,\"start\":61949},{\"attributes\":{\"doi\":\"arXiv:2204.06125\",\"id\":\"b24\"},\"end\":62677,\"start\":62372},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":245335280},\"end\":63186,\"start\":62679},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":3180429},\"end\":63620,\"start\":63188},{\"attributes\":{\"id\":\"b27\"},\"end\":64088,\"start\":63622},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":13756489},\"end\":64455,\"start\":64090},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1779661},\"end\":64615,\"start\":64457},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":205001834},\"end\":64880,\"start\":64617},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":236034189},\"end\":65327,\"start\":64882},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":246411402},\"end\":65747,\"start\":65329},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":231879586},\"end\":66248,\"start\":65749},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":231591445},\"end\":66735,\"start\":66250},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":233024831},\"end\":66948,\"start\":66737},{\"attributes\":{\"doi\":\"arXiv:2211.13189\",\"id\":\"b36\"},\"end\":67288,\"start\":66950},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":174799768},\"end\":67907,\"start\":67290},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":57246310},\"end\":68374,\"start\":67909},{\"attributes\":{\"doi\":\"arXiv:1504.00325\",\"id\":\"b39\"},\"end\":68726,\"start\":68376},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":51876975},\"end\":69422,\"start\":68728},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":231951742},\"end\":69971,\"start\":69424},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":204800739},\"end\":70293,\"start\":69973},{\"attributes\":{\"id\":\"b43\"},\"end\":70780,\"start\":70295},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":56517630},\"end\":71281,\"start\":70782},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":3104920},\"end\":71765,\"start\":71283},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":236154948},\"end\":72267,\"start\":71767},{\"attributes\":{\"doi\":\"arXiv:2203.02838\",\"id\":\"b47\"},\"end\":72607,\"start\":72269},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":67855420},\"end\":72895,\"start\":72609},{\"attributes\":{\"doi\":\"arXiv:2209.14275\",\"id\":\"b49\"},\"end\":73141,\"start\":72897},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":253510826},\"end\":73669,\"start\":73143},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":204838007},\"end\":74131,\"start\":73671},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":209444382},\"end\":74581,\"start\":74133},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":234742594},\"end\":75062,\"start\":74583},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":28550242},\"end\":75529,\"start\":75064},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":4787508},\"end\":75890,\"start\":75531},{\"attributes\":{\"doi\":\"arXiv:2303.05681\",\"id\":\"b56\"},\"end\":76201,\"start\":75892},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":246473350},\"end\":76690,\"start\":76203},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":52967399},\"end\":77399,\"start\":76692},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":211096730},\"end\":77770,\"start\":77401},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b60\"},\"end\":77970,\"start\":77772},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":238634813},\"end\":78598,\"start\":77972},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":245426648},\"end\":79203,\"start\":78600},{\"attributes\":{\"id\":\"b63\"},\"end\":79449,\"start\":79205},{\"attributes\":{\"id\":\"b64\"},\"end\":79649,\"start\":79451},{\"attributes\":{\"doi\":\"arXiv:2203.15526\",\"id\":\"b65\"},\"end\":80014,\"start\":79651},{\"attributes\":{\"doi\":\"arXiv:2210.16428\",\"id\":\"b66\"},\"end\":80396,\"start\":80016},{\"attributes\":{\"doi\":\"arXiv:2210.17143\",\"id\":\"b67\"},\"end\":80764,\"start\":80398},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":204960716},\"end\":81550,\"start\":80766},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":11080756},\"end\":82041,\"start\":81552},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":964287},\"end\":82348,\"start\":82043},{\"attributes\":{\"id\":\"b71\"},\"end\":82781,\"start\":82350},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":9026666},\"end\":83227,\"start\":82783},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":11933981},\"end\":83558,\"start\":83229},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":3873857},\"end\":84010,\"start\":83560},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":207217115},\"end\":84344,\"start\":84012},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":216522760},\"end\":84674,\"start\":84346},{\"attributes\":{\"doi\":\"arXiv:2212.09058\",\"id\":\"b77\"},\"end\":84973,\"start\":84676},{\"attributes\":{\"doi\":\"arXiv:2204.11479\",\"id\":\"b78\"},\"end\":85364,\"start\":84975},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":239616434},\"end\":85754,\"start\":85366},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":235624127},\"end\":86111,\"start\":85756},{\"attributes\":{\"doi\":\"arXiv:2206.04769\",\"id\":\"b81\"},\"end\":86410,\"start\":86113},{\"attributes\":{\"doi\":\"arXiv:2303.07902\",\"id\":\"b82\"},\"end\":86788,\"start\":86412},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":219955663},\"end\":87065,\"start\":86790}]", "bib_title": "[{\"end\":53328,\"start\":53263},{\"end\":53838,\"start\":53787},{\"end\":54179,\"start\":54126},{\"end\":54562,\"start\":54499},{\"end\":54916,\"start\":54883},{\"end\":55247,\"start\":55165},{\"end\":55748,\"start\":55664},{\"end\":56119,\"start\":56055},{\"end\":56445,\"start\":56390},{\"end\":56756,\"start\":56679},{\"end\":57098,\"start\":57033},{\"end\":57650,\"start\":57584},{\"end\":57992,\"start\":57929},{\"end\":58335,\"start\":58288},{\"end\":60044,\"start\":59976},{\"end\":60427,\"start\":60344},{\"end\":60813,\"start\":60750},{\"end\":61546,\"start\":61481},{\"end\":62019,\"start\":61949},{\"end\":62738,\"start\":62679},{\"end\":63218,\"start\":63188},{\"end\":63685,\"start\":63622},{\"end\":64115,\"start\":64090},{\"end\":64470,\"start\":64457},{\"end\":64668,\"start\":64617},{\"end\":64971,\"start\":64882},{\"end\":65433,\"start\":65329},{\"end\":65838,\"start\":65749},{\"end\":66319,\"start\":66250},{\"end\":66771,\"start\":66737},{\"end\":67343,\"start\":67290},{\"end\":67961,\"start\":67909},{\"end\":68825,\"start\":68728},{\"end\":69520,\"start\":69424},{\"end\":70008,\"start\":69973},{\"end\":70342,\"start\":70295},{\"end\":70822,\"start\":70782},{\"end\":71399,\"start\":71283},{\"end\":71795,\"start\":71767},{\"end\":72639,\"start\":72609},{\"end\":73249,\"start\":73143},{\"end\":73752,\"start\":73671},{\"end\":74214,\"start\":74133},{\"end\":74652,\"start\":74583},{\"end\":75088,\"start\":75064},{\"end\":75581,\"start\":75531},{\"end\":76297,\"start\":76203},{\"end\":76771,\"start\":76692},{\"end\":77470,\"start\":77401},{\"end\":78081,\"start\":77972},{\"end\":78689,\"start\":78600},{\"end\":80878,\"start\":80766},{\"end\":81614,\"start\":81552},{\"end\":82097,\"start\":82043},{\"end\":82833,\"start\":82783},{\"end\":83283,\"start\":83229},{\"end\":83628,\"start\":83560},{\"end\":84059,\"start\":84012},{\"end\":84389,\"start\":84346},{\"end\":85423,\"start\":85366},{\"end\":85806,\"start\":85756},{\"end\":86830,\"start\":86790}]", "bib_author": "[{\"end\":53343,\"start\":53330},{\"end\":53356,\"start\":53343},{\"end\":53368,\"start\":53356},{\"end\":53378,\"start\":53368},{\"end\":53390,\"start\":53378},{\"end\":53401,\"start\":53390},{\"end\":53411,\"start\":53401},{\"end\":53421,\"start\":53411},{\"end\":53852,\"start\":53840},{\"end\":54192,\"start\":54181},{\"end\":54202,\"start\":54192},{\"end\":54210,\"start\":54202},{\"end\":54218,\"start\":54210},{\"end\":54227,\"start\":54218},{\"end\":54572,\"start\":54564},{\"end\":54578,\"start\":54572},{\"end\":54584,\"start\":54578},{\"end\":54593,\"start\":54584},{\"end\":54601,\"start\":54593},{\"end\":54615,\"start\":54601},{\"end\":54929,\"start\":54918},{\"end\":54941,\"start\":54929},{\"end\":54953,\"start\":54941},{\"end\":54967,\"start\":54953},{\"end\":55255,\"start\":55249},{\"end\":55264,\"start\":55255},{\"end\":55272,\"start\":55264},{\"end\":55282,\"start\":55272},{\"end\":55292,\"start\":55282},{\"end\":55307,\"start\":55292},{\"end\":55321,\"start\":55307},{\"end\":55763,\"start\":55750},{\"end\":55777,\"start\":55763},{\"end\":55788,\"start\":55777},{\"end\":55802,\"start\":55788},{\"end\":56133,\"start\":56121},{\"end\":56147,\"start\":56133},{\"end\":56160,\"start\":56147},{\"end\":56169,\"start\":56160},{\"end\":56180,\"start\":56169},{\"end\":56454,\"start\":56447},{\"end\":56461,\"start\":56454},{\"end\":56468,\"start\":56461},{\"end\":56482,\"start\":56468},{\"end\":56490,\"start\":56482},{\"end\":56765,\"start\":56758},{\"end\":56772,\"start\":56765},{\"end\":56786,\"start\":56772},{\"end\":56794,\"start\":56786},{\"end\":57111,\"start\":57100},{\"end\":57122,\"start\":57111},{\"end\":57134,\"start\":57122},{\"end\":57663,\"start\":57652},{\"end\":57675,\"start\":57663},{\"end\":57686,\"start\":57675},{\"end\":57696,\"start\":57686},{\"end\":57705,\"start\":57696},{\"end\":58005,\"start\":57994},{\"end\":58019,\"start\":58005},{\"end\":58030,\"start\":58019},{\"end\":58042,\"start\":58030},{\"end\":58348,\"start\":58337},{\"end\":58359,\"start\":58348},{\"end\":58626,\"start\":58617},{\"end\":58638,\"start\":58626},{\"end\":58648,\"start\":58638},{\"end\":58658,\"start\":58648},{\"end\":58670,\"start\":58658},{\"end\":58679,\"start\":58670},{\"end\":58689,\"start\":58679},{\"end\":58700,\"start\":58689},{\"end\":58707,\"start\":58700},{\"end\":58999,\"start\":58992},{\"end\":59007,\"start\":58999},{\"end\":59015,\"start\":59007},{\"end\":59022,\"start\":59015},{\"end\":59029,\"start\":59022},{\"end\":59039,\"start\":59029},{\"end\":59047,\"start\":59039},{\"end\":59061,\"start\":59047},{\"end\":59354,\"start\":59346},{\"end\":59360,\"start\":59354},{\"end\":59368,\"start\":59360},{\"end\":59376,\"start\":59368},{\"end\":59384,\"start\":59376},{\"end\":59391,\"start\":59384},{\"end\":59397,\"start\":59391},{\"end\":59601,\"start\":59586},{\"end\":59611,\"start\":59601},{\"end\":59621,\"start\":59611},{\"end\":59630,\"start\":59621},{\"end\":59642,\"start\":59630},{\"end\":59653,\"start\":59642},{\"end\":59662,\"start\":59653},{\"end\":59672,\"start\":59662},{\"end\":59683,\"start\":59672},{\"end\":59699,\"start\":59683},{\"end\":60053,\"start\":60046},{\"end\":60060,\"start\":60053},{\"end\":60068,\"start\":60060},{\"end\":60075,\"start\":60068},{\"end\":60083,\"start\":60075},{\"end\":60092,\"start\":60083},{\"end\":60106,\"start\":60092},{\"end\":60114,\"start\":60106},{\"end\":60438,\"start\":60429},{\"end\":60448,\"start\":60438},{\"end\":60460,\"start\":60448},{\"end\":60472,\"start\":60460},{\"end\":60481,\"start\":60472},{\"end\":60495,\"start\":60481},{\"end\":60504,\"start\":60495},{\"end\":60825,\"start\":60815},{\"end\":60836,\"start\":60825},{\"end\":60847,\"start\":60836},{\"end\":60857,\"start\":60847},{\"end\":61223,\"start\":61217},{\"end\":61229,\"start\":61223},{\"end\":61237,\"start\":61229},{\"end\":61244,\"start\":61237},{\"end\":61254,\"start\":61244},{\"end\":61265,\"start\":61254},{\"end\":61556,\"start\":61548},{\"end\":61567,\"start\":61556},{\"end\":61576,\"start\":61567},{\"end\":61582,\"start\":61576},{\"end\":61591,\"start\":61582},{\"end\":61598,\"start\":61591},{\"end\":62027,\"start\":62021},{\"end\":62034,\"start\":62027},{\"end\":62040,\"start\":62034},{\"end\":62046,\"start\":62040},{\"end\":62055,\"start\":62046},{\"end\":62064,\"start\":62055},{\"end\":62072,\"start\":62064},{\"end\":62078,\"start\":62072},{\"end\":62086,\"start\":62078},{\"end\":62093,\"start\":62086},{\"end\":62101,\"start\":62093},{\"end\":62108,\"start\":62101},{\"end\":62448,\"start\":62438},{\"end\":62460,\"start\":62448},{\"end\":62470,\"start\":62460},{\"end\":62477,\"start\":62470},{\"end\":62485,\"start\":62477},{\"end\":62751,\"start\":62740},{\"end\":62764,\"start\":62751},{\"end\":62774,\"start\":62764},{\"end\":62783,\"start\":62774},{\"end\":62792,\"start\":62783},{\"end\":63229,\"start\":63220},{\"end\":63240,\"start\":63229},{\"end\":63246,\"start\":63240},{\"end\":63258,\"start\":63246},{\"end\":63267,\"start\":63258},{\"end\":63280,\"start\":63267},{\"end\":63290,\"start\":63280},{\"end\":63693,\"start\":63687},{\"end\":63699,\"start\":63693},{\"end\":63706,\"start\":63699},{\"end\":63713,\"start\":63706},{\"end\":63721,\"start\":63713},{\"end\":64128,\"start\":64117},{\"end\":64139,\"start\":64128},{\"end\":64149,\"start\":64139},{\"end\":64162,\"start\":64149},{\"end\":64171,\"start\":64162},{\"end\":64182,\"start\":64171},{\"end\":64192,\"start\":64182},{\"end\":64206,\"start\":64192},{\"end\":64481,\"start\":64472},{\"end\":64491,\"start\":64481},{\"end\":64501,\"start\":64491},{\"end\":64685,\"start\":64670},{\"end\":64697,\"start\":64685},{\"end\":64711,\"start\":64697},{\"end\":64979,\"start\":64973},{\"end\":64992,\"start\":64979},{\"end\":65003,\"start\":64992},{\"end\":65011,\"start\":65003},{\"end\":65020,\"start\":65011},{\"end\":65031,\"start\":65020},{\"end\":65441,\"start\":65435},{\"end\":65447,\"start\":65441},{\"end\":65456,\"start\":65447},{\"end\":65463,\"start\":65456},{\"end\":65847,\"start\":65840},{\"end\":65855,\"start\":65847},{\"end\":65862,\"start\":65855},{\"end\":65873,\"start\":65862},{\"end\":65883,\"start\":65873},{\"end\":65891,\"start\":65883},{\"end\":65897,\"start\":65891},{\"end\":65908,\"start\":65897},{\"end\":65914,\"start\":65908},{\"end\":65924,\"start\":65914},{\"end\":66332,\"start\":66321},{\"end\":66341,\"start\":66332},{\"end\":66352,\"start\":66341},{\"end\":66362,\"start\":66352},{\"end\":66369,\"start\":66362},{\"end\":66380,\"start\":66369},{\"end\":66390,\"start\":66380},{\"end\":66400,\"start\":66390},{\"end\":66411,\"start\":66400},{\"end\":66420,\"start\":66411},{\"end\":66781,\"start\":66773},{\"end\":66793,\"start\":66781},{\"end\":66802,\"start\":66793},{\"end\":67036,\"start\":67027},{\"end\":67045,\"start\":67036},{\"end\":67053,\"start\":67045},{\"end\":67067,\"start\":67053},{\"end\":67078,\"start\":67067},{\"end\":67354,\"start\":67345},{\"end\":67361,\"start\":67354},{\"end\":67368,\"start\":67361},{\"end\":67375,\"start\":67368},{\"end\":67971,\"start\":67963},{\"end\":67979,\"start\":67971},{\"end\":67989,\"start\":67979},{\"end\":67998,\"start\":67989},{\"end\":68004,\"start\":67998},{\"end\":68015,\"start\":68004},{\"end\":68448,\"start\":68440},{\"end\":68456,\"start\":68448},{\"end\":68466,\"start\":68456},{\"end\":68478,\"start\":68466},{\"end\":68487,\"start\":68478},{\"end\":68497,\"start\":68487},{\"end\":68510,\"start\":68497},{\"end\":68837,\"start\":68827},{\"end\":68845,\"start\":68837},{\"end\":68856,\"start\":68845},{\"end\":68867,\"start\":68856},{\"end\":69536,\"start\":69522},{\"end\":69546,\"start\":69536},{\"end\":69554,\"start\":69546},{\"end\":69565,\"start\":69554},{\"end\":70021,\"start\":70010},{\"end\":70032,\"start\":70021},{\"end\":70044,\"start\":70032},{\"end\":70354,\"start\":70344},{\"end\":70365,\"start\":70354},{\"end\":70835,\"start\":70824},{\"end\":70844,\"start\":70835},{\"end\":70852,\"start\":70844},{\"end\":70860,\"start\":70852},{\"end\":70868,\"start\":70860},{\"end\":70879,\"start\":70868},{\"end\":70888,\"start\":70879},{\"end\":70898,\"start\":70888},{\"end\":70905,\"start\":70898},{\"end\":70917,\"start\":70905},{\"end\":71410,\"start\":71401},{\"end\":71417,\"start\":71410},{\"end\":71427,\"start\":71417},{\"end\":71442,\"start\":71427},{\"end\":71804,\"start\":71797},{\"end\":71811,\"start\":71804},{\"end\":71820,\"start\":71811},{\"end\":71834,\"start\":71820},{\"end\":71842,\"start\":71834},{\"end\":72326,\"start\":72319},{\"end\":72333,\"start\":72326},{\"end\":72342,\"start\":72333},{\"end\":72349,\"start\":72342},{\"end\":72357,\"start\":72349},{\"end\":72364,\"start\":72357},{\"end\":72378,\"start\":72364},{\"end\":72387,\"start\":72378},{\"end\":72395,\"start\":72387},{\"end\":72647,\"start\":72641},{\"end\":72657,\"start\":72647},{\"end\":72663,\"start\":72657},{\"end\":72960,\"start\":72948},{\"end\":72972,\"start\":72960},{\"end\":72980,\"start\":72972},{\"end\":73257,\"start\":73251},{\"end\":73261,\"start\":73257},{\"end\":73269,\"start\":73261},{\"end\":73273,\"start\":73269},{\"end\":73282,\"start\":73273},{\"end\":73286,\"start\":73282},{\"end\":73293,\"start\":73286},{\"end\":73297,\"start\":73293},{\"end\":73317,\"start\":73297},{\"end\":73327,\"start\":73317},{\"end\":73764,\"start\":73754},{\"end\":73775,\"start\":73764},{\"end\":73786,\"start\":73775},{\"end\":73793,\"start\":73786},{\"end\":73803,\"start\":73793},{\"end\":73813,\"start\":73803},{\"end\":73821,\"start\":73813},{\"end\":73827,\"start\":73821},{\"end\":73836,\"start\":73827},{\"end\":74224,\"start\":74216},{\"end\":74231,\"start\":74224},{\"end\":74240,\"start\":74231},{\"end\":74248,\"start\":74240},{\"end\":74256,\"start\":74248},{\"end\":74270,\"start\":74256},{\"end\":74665,\"start\":74654},{\"end\":74678,\"start\":74665},{\"end\":74689,\"start\":74678},{\"end\":74699,\"start\":74689},{\"end\":74706,\"start\":74699},{\"end\":74715,\"start\":74706},{\"end\":74725,\"start\":74715},{\"end\":75098,\"start\":75090},{\"end\":75106,\"start\":75098},{\"end\":75115,\"start\":75106},{\"end\":75599,\"start\":75583},{\"end\":75608,\"start\":75599},{\"end\":75617,\"start\":75608},{\"end\":75627,\"start\":75617},{\"end\":75635,\"start\":75627},{\"end\":75645,\"start\":75635},{\"end\":75992,\"start\":75985},{\"end\":76000,\"start\":75992},{\"end\":76007,\"start\":76000},{\"end\":76307,\"start\":76299},{\"end\":76313,\"start\":76307},{\"end\":76320,\"start\":76313},{\"end\":76326,\"start\":76320},{\"end\":76346,\"start\":76326},{\"end\":76356,\"start\":76346},{\"end\":76783,\"start\":76773},{\"end\":76795,\"start\":76783},{\"end\":76802,\"start\":76795},{\"end\":76815,\"start\":76802},{\"end\":77480,\"start\":77472},{\"end\":77493,\"start\":77480},{\"end\":77504,\"start\":77493},{\"end\":77514,\"start\":77504},{\"end\":77828,\"start\":77816},{\"end\":77834,\"start\":77828},{\"end\":78089,\"start\":78083},{\"end\":78097,\"start\":78089},{\"end\":78105,\"start\":78097},{\"end\":78112,\"start\":78105},{\"end\":78698,\"start\":78691},{\"end\":78706,\"start\":78698},{\"end\":78713,\"start\":78706},{\"end\":78719,\"start\":78713},{\"end\":78727,\"start\":78719},{\"end\":79285,\"start\":79279},{\"end\":79293,\"start\":79285},{\"end\":79301,\"start\":79293},{\"end\":79309,\"start\":79301},{\"end\":79316,\"start\":79309},{\"end\":79466,\"start\":79451},{\"end\":79755,\"start\":79747},{\"end\":79762,\"start\":79755},{\"end\":79768,\"start\":79762},{\"end\":79775,\"start\":79768},{\"end\":79781,\"start\":79775},{\"end\":79791,\"start\":79781},{\"end\":80093,\"start\":80086},{\"end\":80102,\"start\":80093},{\"end\":80109,\"start\":80102},{\"end\":80116,\"start\":80109},{\"end\":80124,\"start\":80116},{\"end\":80131,\"start\":80124},{\"end\":80137,\"start\":80131},{\"end\":80143,\"start\":80137},{\"end\":80152,\"start\":80143},{\"end\":80162,\"start\":80152},{\"end\":80490,\"start\":80483},{\"end\":80497,\"start\":80490},{\"end\":80503,\"start\":80497},{\"end\":80510,\"start\":80503},{\"end\":80518,\"start\":80510},{\"end\":80525,\"start\":80518},{\"end\":80532,\"start\":80525},{\"end\":80539,\"start\":80532},{\"end\":80889,\"start\":80880},{\"end\":80896,\"start\":80889},{\"end\":80905,\"start\":80896},{\"end\":80922,\"start\":80905},{\"end\":80933,\"start\":80922},{\"end\":80941,\"start\":80933},{\"end\":80953,\"start\":80941},{\"end\":80968,\"start\":80953},{\"end\":81628,\"start\":81616},{\"end\":81638,\"start\":81628},{\"end\":81646,\"start\":81638},{\"end\":81656,\"start\":81646},{\"end\":82110,\"start\":82099},{\"end\":82456,\"start\":82444},{\"end\":82465,\"start\":82456},{\"end\":82847,\"start\":82835},{\"end\":82867,\"start\":82847},{\"end\":82877,\"start\":82867},{\"end\":83297,\"start\":83285},{\"end\":83309,\"start\":83297},{\"end\":83320,\"start\":83309},{\"end\":83329,\"start\":83320},{\"end\":83637,\"start\":83630},{\"end\":83644,\"start\":83637},{\"end\":83650,\"start\":83644},{\"end\":83664,\"start\":83650},{\"end\":83674,\"start\":83664},{\"end\":84072,\"start\":84061},{\"end\":84082,\"start\":84072},{\"end\":84093,\"start\":84082},{\"end\":84399,\"start\":84391},{\"end\":84406,\"start\":84399},{\"end\":84417,\"start\":84406},{\"end\":84430,\"start\":84417},{\"end\":84736,\"start\":84728},{\"end\":84742,\"start\":84736},{\"end\":84750,\"start\":84742},{\"end\":84757,\"start\":84750},{\"end\":84769,\"start\":84757},{\"end\":84777,\"start\":84769},{\"end\":84784,\"start\":84777},{\"end\":85091,\"start\":85080},{\"end\":85103,\"start\":85091},{\"end\":85113,\"start\":85103},{\"end\":85123,\"start\":85113},{\"end\":85130,\"start\":85123},{\"end\":85434,\"start\":85425},{\"end\":85449,\"start\":85434},{\"end\":85458,\"start\":85449},{\"end\":85469,\"start\":85458},{\"end\":85818,\"start\":85808},{\"end\":85826,\"start\":85818},{\"end\":85834,\"start\":85826},{\"end\":85844,\"start\":85834},{\"end\":86190,\"start\":86178},{\"end\":86202,\"start\":86190},{\"end\":86214,\"start\":86202},{\"end\":86222,\"start\":86214},{\"end\":86511,\"start\":86505},{\"end\":86520,\"start\":86511},{\"end\":86528,\"start\":86520},{\"end\":86537,\"start\":86528},{\"end\":86544,\"start\":86537},{\"end\":86550,\"start\":86544},{\"end\":86559,\"start\":86550},{\"end\":86838,\"start\":86832},{\"end\":86846,\"start\":86838},{\"end\":86856,\"start\":86846}]", "bib_venue": "[{\"end\":53510,\"start\":53495},{\"end\":53957,\"start\":53913},{\"end\":56532,\"start\":56515},{\"end\":57327,\"start\":57231},{\"end\":57747,\"start\":57730},{\"end\":60156,\"start\":60139},{\"end\":60948,\"start\":60911},{\"end\":61707,\"start\":61661},{\"end\":62941,\"start\":62875},{\"end\":63411,\"start\":63359},{\"end\":63870,\"start\":63804},{\"end\":66832,\"start\":66821},{\"end\":67646,\"start\":67519},{\"end\":68164,\"start\":68098},{\"end\":69048,\"start\":68956},{\"end\":69714,\"start\":69648},{\"end\":70558,\"start\":70462},{\"end\":71038,\"start\":70986},{\"end\":72035,\"start\":71939},{\"end\":75277,\"start\":75196},{\"end\":77086,\"start\":76959},{\"end\":78305,\"start\":78209},{\"end\":78920,\"start\":78824},{\"end\":81129,\"start\":81057},{\"end\":81817,\"start\":81745},{\"end\":83018,\"start\":82956},{\"end\":83795,\"start\":83743},{\"end\":84158,\"start\":84142},{\"end\":53493,\"start\":53421},{\"end\":53911,\"start\":53852},{\"end\":54290,\"start\":54227},{\"end\":54646,\"start\":54615},{\"end\":54998,\"start\":54967},{\"end\":55384,\"start\":55321},{\"end\":55833,\"start\":55802},{\"end\":56211,\"start\":56180},{\"end\":56513,\"start\":56490},{\"end\":56840,\"start\":56794},{\"end\":57229,\"start\":57134},{\"end\":57728,\"start\":57705},{\"end\":58089,\"start\":58042},{\"end\":58390,\"start\":58359},{\"end\":58615,\"start\":58572},{\"end\":58990,\"start\":58927},{\"end\":59344,\"start\":59280},{\"end\":59750,\"start\":59715},{\"end\":60137,\"start\":60114},{\"end\":60522,\"start\":60504},{\"end\":60909,\"start\":60857},{\"end\":61215,\"start\":61131},{\"end\":61659,\"start\":61598},{\"end\":62146,\"start\":62108},{\"end\":62436,\"start\":62372},{\"end\":62873,\"start\":62792},{\"end\":63357,\"start\":63290},{\"end\":63802,\"start\":63721},{\"end\":64255,\"start\":64206},{\"end\":64507,\"start\":64501},{\"end\":64717,\"start\":64711},{\"end\":65080,\"start\":65031},{\"end\":65519,\"start\":65463},{\"end\":65980,\"start\":65924},{\"end\":66464,\"start\":66420},{\"end\":66819,\"start\":66802},{\"end\":67025,\"start\":66950},{\"end\":67517,\"start\":67375},{\"end\":68096,\"start\":68015},{\"end\":68438,\"start\":68376},{\"end\":68954,\"start\":68867},{\"end\":69646,\"start\":69565},{\"end\":70116,\"start\":70044},{\"end\":70460,\"start\":70365},{\"end\":70984,\"start\":70917},{\"end\":71503,\"start\":71442},{\"end\":71937,\"start\":71842},{\"end\":72317,\"start\":72269},{\"end\":72735,\"start\":72663},{\"end\":72946,\"start\":72897},{\"end\":73399,\"start\":73327},{\"end\":73872,\"start\":73836},{\"end\":74333,\"start\":74270},{\"end\":74797,\"start\":74725},{\"end\":75194,\"start\":75115},{\"end\":75694,\"start\":75645},{\"end\":75983,\"start\":75892},{\"end\":76428,\"start\":76356},{\"end\":76957,\"start\":76815},{\"end\":77570,\"start\":77514},{\"end\":77814,\"start\":77772},{\"end\":78207,\"start\":78112},{\"end\":78822,\"start\":78727},{\"end\":79277,\"start\":79205},{\"end\":79540,\"start\":79466},{\"end\":79745,\"start\":79651},{\"end\":80084,\"start\":80016},{\"end\":80481,\"start\":80398},{\"end\":81055,\"start\":80968},{\"end\":81743,\"start\":81656},{\"end\":82141,\"start\":82110},{\"end\":82442,\"start\":82350},{\"end\":82954,\"start\":82877},{\"end\":83367,\"start\":83329},{\"end\":83741,\"start\":83674},{\"end\":84140,\"start\":84093},{\"end\":84498,\"start\":84430},{\"end\":84726,\"start\":84676},{\"end\":85078,\"start\":84975},{\"end\":85541,\"start\":85469},{\"end\":85916,\"start\":85844},{\"end\":86176,\"start\":86113},{\"end\":86503,\"start\":86412},{\"end\":86905,\"start\":86856}]"}}}, "year": 2023, "month": 12, "day": 17}