{"id": 246015975, "updated": "2023-10-05 18:02:31.835", "metadata": {"title": "A Novel Multi-Task Learning Method for Symbolic Music Emotion Recognition", "authors": "[{\"first\":\"Jibao\",\"last\":\"Qiu\",\"middle\":[]},{\"first\":\"C.\",\"last\":\"Chen\",\"middle\":[\"L.\",\"Philip\"]},{\"first\":\"Tong\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Symbolic Music Emotion Recognition(SMER) is to predict music emotion from symbolic data, such as MIDI and MusicXML. Previous work mainly focused on learning better representation via (mask) language model pre-training but ignored the intrinsic structure of the music, which is extremely important to the emotional expression of music. In this paper, we present a simple multi-task framework for SMER, which incorporates the emotion recognition task with other emotion-related auxiliary tasks derived from the intrinsic structure of the music. The results show that our multi-task framework can be adapted to different models. Moreover, the labels of auxiliary tasks are easy to be obtained, which means our multi-task methods do not require manually annotated labels other than emotion. Conducting on two publicly available datasets (EMOPIA and VGMIDI), the experiments show that our methods perform better in SMER task. Specifically, accuracy has been increased by 4.17 absolute point to 67.58 in EMOPIA dataset, and 1.97 absolute point to 55.85 in VGMIDI dataset. Ablation studies also show the effectiveness of multi-task methods designed in this paper.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2201.05782", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2201-05782", "doi": null}}, "content": {"source": {"pdf_hash": "9d0cb790952e76e1973e553b82f294888d15d97f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2201.05782v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "919c12564b4017d8ac566c76f878f58866840670", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9d0cb790952e76e1973e553b82f294888d15d97f.txt", "contents": "\nA Novel Multi-Task Learning Method for Symbolic Music Emotion Recognition\n\n\nJibao Qiu \nSouth China University of Technology\n\n\nC L Philip Chen \nSouth China University of Technology\n\n\nTong Zhang \nSouth China University of Technology\n\n\nA Novel Multi-Task Learning Method for Symbolic Music Emotion Recognition\n\nSymbolic Music Emotion Recognition(SMER) is to predict music emotion from symbolic data, such as MIDI and MusicXML. Previous work mainly focused on learning better representation via (mask) language model pre-training but ignored the intrinsic structure of the music, which is extremely important to the emotional expression of music. In this paper, we present a simple multi-task framework for SMER, which incorporates the emotion recognition task with other emotion-related auxiliary tasks derived from the intrinsic structure of the music. The results show that our multi-task framework can be adapted to different models. Moreover, the labels of auxiliary tasks are easy to be obtained, which means our multi-task methods do not require manually annotated labels other than emotion. Conducting on two publicly available datasets (EMOPIA and VGMIDI), the experiments show that our methods perform better in SMER task. Specifically, accuracy has been increased by 4.17 absolute point to 67.58 in EMOPIA dataset, and 1.97 absolute point to 55.85 in VGMIDI dataset. Ablation studies also show the effectiveness of multi-task methods designed in this paper.\u2022 We present a novel multi-task framework called MT-arXiv:2201.05782v1 [cs.SD]\n\nIntroduction\n\nEmotion recognition of music has attracted lots of attention in the field of music information retrieval(MIR). For a long time, the research on music emotion recognition has been mainly carried out in the audio domain [Baume et al., 2014;Liu et al., 2018;Panda et al., 2018;Panda et al., 2020]. However, emotion recognition is less explored for music from symbolic data, such as MIDI and MusicXML formats. Thanks to the rapid development of the symbolic music generation [Yang et al., 2017;Huang et al., 2019;Huang and Yang, 2020], more and more research focuses on symbolic music understanding [Zeng et al., 2021;Chou et al., 2021], including symbolic music emotion recognition(SMER). * Contact Author Recently, researches in SMER mainly focused on learning better representation from large-scale unlabeled music pieces via pre-training model by masked or non-masked language model borrowed from NLP and then fine-tuning the pre-trained model directly for emotion recognition on a small dataset. However, simply employing such techniques from NLP may lack the understanding of music structure which is critical to emotion classification for symbolic data [Zeng et al., 2021].\n\nThe existing psychology and music theory literature have revealed the relationship between music structure and emotion. Kaster [1990] has demonstrated that positive emotion is related to listened music in major keys, while negative emotion is related to minor keys. Similar results can be found in [Gerardi and Gerken, 1995;Gregory et al., 1996;Dalla Bella et al., 2001]. Livingstone [2010] found that the loudness of music can greatly affect the expression of emotion. However, the loudness is measured in the audio domain and is still an open problem to measure it in the symbolic domain. Adli[2007] has demonstrated that there is a linear relationship between the velocity in the symbolic domain and the loudness in the audio domain, which means that there is a connection between the velocity of music and emotion.\n\nRecognizing the importance of musical structure for emotion recognition, we present a simple framework called MT-SMNN that incorporates the emotion recognition task with other emotion-related auxiliary tasks derived from the intrinsic structure of the music. By combining the key classification and velocity classification tasks, MT-SMNN based models can better understand emotion classification. Although MT-SMNN is a multi-task framework, we only need the manually annotated emotion label because the velocity label can be extracted directly from symbolic data, and the key label can be obtained by the well-received Krumhansl-Kessler algorithm [2001], which means the proposed framework can be applied to all emotion-labeled symbolic music datasets.\n\nWe combine the MT-SMNN framework with existing models and evaluate them in both EMOPIA and VG-MIDI datasets. Results demonstrate that our proposed MT-SMNN based models achieve the new state-of-the-art on both datasets.\n\nThe chief contributions of this paper can be summarized as following aspects: SMNN, mainly focusing on emotion recognition for symbolic music. In addition to emotion recognition, a better understanding of the structure of music is also taken into account in this framework.\n\n\u2022 We propose two types of auxiliary tasks for SMER. Results show that both tasks can improve the performance of SMER, especially in the valence dimension.\n\n\u2022 MT-SMNN based models achieve new state-of-theart results due to the powerful ability to learn better emotion-based knowledge from auxiliary tasks.\n\n\u2022 We have reproduced most previous work for symbolic music emotion recognition on both exiting public available datasets which is helpful to building benchmarks.\n\n\nRelated Work\n\nWe divide previous work on symbolic music emotion recognition into the following two categories. \n\n\nProposed Method\n\nIn this section, we introduce the Multi-Task Symbolic Music Neural Network(MT-SMNN), a multi-task framework for symbolic music emotion recognition, as illustrated in Figure  1. Below, we describe the structure of MT-SMNN in detail. Figure 1: The framework of proposed MT-SMNN. The text inside parentheses indicates the information processed in the corresponding module. Two sequence-level tasks(Emotion and Key classification)and one note-level task(Velocity classification) are adopt in this framework. The given music is first encoded as a sequence of musical representations and then passed to the embedding layer to get the input representation l1 for each token. Next, the feature extractor captures the high-level information from each token's input representation and generates contextual representation. Then, according to whether the downstream task is sequence-level or note-level, the pooler generates the sentence representation l3 or keeps origin representation l2. Finally, the task-specific classifiers predict the label for each task.\n\n\nSymbolic Music Encoder\n\nA piece of music from symbolic data, such as MIDI and Mu-sicXML, can be encoded as a sequence of musical events, which are so called tokens in the previous literature. Existing method to encode symbloic music can be devided to single-word representation and compound-word(CP) [Hsiao et al., 2021] representation. The MT-SMNN framework can use either single-word representation or compound-word representation. Without losing generality, we show a singleword representation method (Ferreira's[2020] method) and a compound-word representation method [Chou et al., 2021] as example in this part. We simply describe these two types of symbolic music encoding method below. As illustrated in Figure 2(b), the CP representation method encodes given piece of music to a sequence of super tokens. Each super token consists of four sub-tokens: Bar, Sub-beat, Pitch and Duration. The Ferreira's method, shown in Figure 2(c), encodes given music as token sequence X = The CP representation encodes a piece of music to a sequence of super tokens. Each super token consists of four sub-tokens: Bar, Sub-beat, Pitch and Duration. The text inside parentheses means the value of the corresponding sub-token. (c) The Ferreira's method encodes a piece of music to a sequence of tokens. Token started with \"v \", \"d \" and \"n \" denotes the velocity, duration and pitch of the corresponding note. The \".\" token indicates the time shift event.\n\n{x\nv 1 , x d 1 , x p 1 , ..., x v S , x d S , x p S }, where x v i , x d\ni and x p i denotes the velocity, duration and pitch for the i-th note respectively and S denotes the length of this sequence.\n\n\nEmbedding Layer\n\nAccording to the symbolic music encoding method, the embedding layer of MT-SMNN can be divided into two types. The input token is directly mapped to an embedding space for the single-word representation method. Following [Chou et al., 2021], for the CP representation method, the embeddings of all sub-tokens inside the super token are concatenated, then fed to a linear layer to get a complete token embedding. The token embedding is added with the corresponding position embedding to capture the position information.\n\n\nTransformer-based Feature Extractor\n\nThe MT-SMNN employs a transformer-based model as a feature extractor. Given input representation l 1 = {x 0 , ..., x S }, the feature extractor generates output representation l 2 = {h 0 , ..., h S }, where S means the number of input tokens, x i \u2208 R E , h i \u2208 R E denotes the embedding and contextual representation of i-th input token respectively, and E represents the dimension of embedding space and hidden state.\n\n\nPooler\n\nSince both sequence-level and note-level tasks are employed in the MT-SMNN framework, we use a pooler to aggregate information from the entire contextual representation sequence for the sequence-level classification task. At the same time, the pooler keeps a series of contextual representations for the note-level classification task.\n\nFor simplicity, the pooler applies identical mapping for the note-level classification task. For sequence-level classification tasks, the pooler can be designed by one of the following strategies: taking the first contextual representation(like BERT [Devlin et al., 2019]), taking the last(like MIDIGPT[Ferreira et al., 2020]), or attention-based weighting average(like MIDIBERT-Piano[Chou et al., 2021]). In MT-SMNN, the emotion and key classification task share the same sentence representation l 3 because both are sequencelevel tasks.\n\n\nTask-specific Classification\n\nIn this part, we first introduce the auxiliary tasks employed by MT-SMNN. Then, we describe more details about these classification outputs. Finally, we show the multi-task loss function used by MT-SMNN.\n\n\nAuxiliary Tasks\n\nKey Classification: This is a sequence-level classification task. The target is to predict the musical key for the given sequence of musical representation tokens collected from a piece of music. There are 24 possible keys: 12 major keys and 12 minor keys [Thompson and Cuddy, 1997]. Velocity Classification: This is a note-level classification task [Chou et al., 2021]. The target is to predict velocity for each individual note for the given sequence of notes collected from a piece of music. \n\n\nClassification Ouputs\n\nLet H = {h 0 , ..., h S } be the contextual representation(l 2 ) of given piece of music, Z be the sentence representation(l 3 ), where h i \u2208 R E , Z \u2208 R K , E is the dimension of hidden state , and K is the dimension of sentence representation generated by the pooler. For sequence-level tasks(emotion and key classification), the probability that given a piece of music is predicted as class c by a classifier with softmax can be formalized as:\nP t (c t |Z) = softmax(\u03c6 t (Z))(1)\nwhere \u03c6 is the mapping function of classifier, t is to distinguish between different tasks. For the note-level task(velocity classification), the probability that the i-th note in a piece of music is predicted as class c can be formalized as:\nP t i (c t |H) = softmax(\u03c6 t (h i ))(2)\nwhere \u03c6 is the mapping function of classifier, i means the i-th item in coresponding sequence. For more detail, the classifiers consist of two fully connected layers with the ReLU activation function in the middle.\n\n\nMulti-task loss\n\nFor each task, we use cross-entropy loss as its objective. Let L 1 , L 2 , and L 3 be the loss of emotion recognition, key classification, and velocity classification, respectively. We employ the adaptive loss function proposed by Liebel[2018]. The multi-task loss is formalized following:\nL = t 1 2 \u00b7 \u03c3 t L t + ln 1 + \u03c3 2 t(3)\nAlgorithm 1 Training a MT-SMNN-based model. Check for early-stopping; 18: end for where L t indicates the loss of task t, \u03c3 t a learnable parameter which controls the contribution of t-th task, and the second term is a regularizer.\n\n\nExperiments\n\nIn this section, we evaluate the proposed MT-SMNN based models on EMOPIA [Hung et al., 2021] and VGMIDI [Ferreira and Whitehead, 2019;Ferreira et al., 2020] datasets. We first overview the datasets and processing procedure. Then, we describe the baselines and our proposed models(MT-MIDIBERT and MT-MIDIGPT). Finally, we show the results and analysis. \n\n\nDatasets and Preprocess\n\nThe information of the EMOPIA and VGMIDI datasets is summarized in Table 1. The EMOPIA dataset 1 is a dataset of pop piano music for symbolic music emotion recognition. The clips is labeled to 4 class accoding to Russell's 4Q [Russell, 1980]. The VGMIDI dataset 2 is a dataset of video game sound-tracks formatted in MIDI. Each clip in the VGMIDI dataset is labeled as valence-arousal pair, also according to the Russell's model. For experimental consistency, we transfer the valencearousal pair in VGMIDI to the taxonomy of Russell's 4Q as EMOPIA. The initial VGMIDI dataset has been split into a training set and a testing set. We divide a portion(about 15%) of the original training set into the validation set. In this procedure, we ensure that the clips of the validation set and the training set will not come from the same song.\n\nThe MT-SMNN need two additional labels(key and velocity) besides emotion. The velocity for each note can directly derived from symbolic data. We extract the key label via the well-received Krumhansl-Kessler algorithm [2001] provided by the Music21 library [Cuthbert and Ariza, 2010].\n\n\nImplementation details\n\nFor the sake of fair comparison, the vast majority of previous work mentioned in Section 2 is reproduced. Our implementation is based on the PyTorch code open-sourced by HugginFace[Wolf et al., 2019]. Below, we describe the reproduced models in detail. The checkpoint achieving the best metric in the validation set during the training procedure is saved and evaluated in the testing set. (e) All experiments are repeated ten times with different random seeds (from 0 to 9). Specific Settings: Following [Chou et al., 2021], the max sequence length of MIDIBERT-Piano is set as 512. The inside BERT model adopt BERT base . We start fine-tuning the MIDIBERT-Piano model from the released pre-trained checkpoint 3 . The max sequence length of MIDIGPT is set as 1024 and 2048 to cover the entire input sequence of tokens as much as possible when experimenting with VGMIDI and EMOPIA datasets, respectively. To accommodate different max sequence lengths, we pre-trained the MIDIGPT model according to [Ferreira et al., 2020], with remaining other settings unchanged except the max sequence length. We finetune the models mentioned above at most 30 epochs in VGMIDI, and 100 epochs in EMOPIA with early-stopping discussed above. \n\n\nConfiguration of Machine Learning based Methods\n\n\nConfiguration of the proposed MT-SMNN based models\n\nWe apply the proposed MT-SMNN framework to existing deep learning based methods. For the MIDIBERT-Piano model, we extend it by combining both key classification and velocity classification with the original emotion recognition task. However, we only incorporate the key classification with the original emotion recognition task for MIDIGPT because its representation method has already leaked the velocity information. We coin the model that combines the proposed MT-SMNN with MIDIBERT-Piano and MIDIGPT as \"MT-MIDIBERT\" and \"MT-MIDIGPT\" respectively.\n\n\nThe Training Procedure of MT-SMNN\n\nThe training procedure of MT-SMNN is shown in Algorithm 1. We start our training from pre-trained checkpoints, and then we finetune the MT-SMNN based model using multitask loss. After every training epoch, we evaluate the model and check whether early-stopping.\n\n\nComparison of state-of-the-art Methods\n\nWe compare MT-SMNN based models with previous stateof-the-art models. The result of symbolic music emotion recognition(SMER) is shown in Table 2. We have reproduced all these baselines in Table 2 and described them in detail in 4.2. Table 2 shows that the deep learning based models outperform the traditional machine learning based models. In addition, models that work with the proposed MT-SMNN framework perform better than the counterpart for singletask and achieve new state-of-the-art results. Specifically, Compared with the MIDIBERT-Piano model, the proposed MT-MIDIBERT model pushes the accuracy to 67.58% and 49.8%, which amounts 4.2% and 2.5% absolution improvement on the EMOPIA and VGMIDI dataset, respectively. The proposed MT-MIDIGPT model also improves the accuracy by 3.8% and 2.0% to 62.50% and 55.85% for these two datasets, respectively.\n\nSince the MT-SMNN based models have no difference except for multiple classifiers, which have a minimal amount of parameters, are employed for different tasks compared with its single-task counterpart, the improvement of the above results is attributed to our proposed MT-SMNN framework.\n\n\nAblation Studies\n\nIn this section, we conduct experiments on the EMOPIA dataset to study auxiliary tasks' impact. The results are summarized in Table 3.  Table 3 shows that both key and velocity classification auxiliary tasks effectively affect emotion recognition. Moreover, the model taken in both auxiliary tasks outperforms models only taken in a single. The accuracy is increased by 3.6% and 1.3% to 67.03% and 64.73% after combing the SMER task with the key and velocity classification task, respectively, which means that the key classification task is a more critical auxiliary task than the velocity classification task.\n\nWe also plot the confusion matrices of these experiments, as shown in Figure 3. In this figure, Q1, Q2, Q3 and Q4 denotes HVHA(high valence high arousal), LVHA(low valence high arousal), LVLA(low valence low arousal) and HVLA(high valence low arousal) respectively which also socalled Happy, Angry, Sad and Calm in some literatures. Compared Figure 3(b) with Figure 3(a), we have found that the key classification task can greatly improve the performance of emotion recognition in the class of Q1 and Q4. Similar results can be found in Figure 3(c) and Figure3(d). Since Q1 and Q4 are both in the high valence region, we finally conclude that our proposed MT-SMNN framework can improve the performance of music recognition, especially in the valence dimension.\n\n\nConclusion\n\nIn this paper, we present MT-SMNN, a multi-task framework that mainly focus on emotion recognition for symbolic music. The MT-SMNN framework combines emotion recognition with key classification and velocity classification tasks and conducts a multi-task training procedure in a single dataset. MT-SMNN based models obtain new state-of-the-art results in both EMOPIA and VGMIDI datasets. Further analysis also verifies the effectiveness of both auxiliary tasks.\n\nWe would like to apply the MT-SMNN framework to other areas for future work. For example, the MT-SMNN based models can be employed to build a metric for evaluating the performance of emotion conditioned symbolic music generation models.\n\nFigure 2 :\n2(a) An example of symbolic music. (b)\n\n\nFollowing [Chou et al., 2021], we quantize 128 possible MIDI velocity values(0-127) into six classes: pp (0-31), p (32-47), mp (48-63), mf (64-79), f (80-95), and ff (96-127).\n\n\nIn this paper, we have reproduced the machine learning based models proposed in[Lin et al., 2013] and[Panda et al., 2013]. After taking the best subset of features selected in[Lin et al., 2013] and[Panda et al., 2013], the dimension of features for Lin's method and Panda's method is 521 and 135 respectively. Both methods use the SVM classifier that works with the RBF kernel.Configuration of Deep Learning based MethodsGlobal Settings: The reproduced MIDIBERT-Piano[Chou et al., 2021], MIDIGPT[Ferreira et al., 2020] and our proposed MT-SMNN based methods all share the following global configuration: (a) The AdamW[Loshchilov and Hutter, 2019] optimizer is adopt in this paper. The \u03b2 1 , \u03b2 2 and weight decay rate is set as 0.9, 0.999 and 0.01 repectively. (b) The batch size is set as 16. (c) The learning rate is set as 3e-5 with a linear scheduler. The other trick is setting warmup steps as 500. (d) We evaluate the models every training epoch in the validation set. The model is early stopping when the macro-F1 for emotion recognition have no improvement for T consecutive epochs, where T = 0.3 * N , N denotes the number of max training epochs.\n\nFigure 3 :\n3The result of baseline model(MIDIBERT-Piano, abbreviated to MIDIBERT) is shown in (a). The performance of models incorporate with addtional Key Classifier(KC) or Velocity Classifier(VC) is shown in (b) (c) and (d).\n\n1 :\n1Load model parameters \u0398 from the pre-trained checkpoint; 2: Set the max number of epoch: epoch max ; 3: Prepare dataset D; 4: for epoch in 1, ..., epoch max do for b i in D do //b i is the i-th mini-batch of dataset ;5: \n\n6: \n\n7: \n\n1. Predict \n\n8: \n\nPredict emotion and key for music using Eq. 1; \n\n9: \n\nPredict velocity for each note using Eq. 2; \n\n10: \n\n2. Compute loss \n\n11: \n\nCompute loss for each task using cross-entropy; \n\n12: \n\nCompute loss L (\u0398) for multi-task using Eq. 3; \n\n13: \n\n4. Compute gradient: \u2207(\u0398) ; \n\n14: \n\n5. Update parameters: \u0398 = \u0398 \u2212 \u03bb\u2207(\u0398); \n\n15: \n\nend for \n\n16: \n\nEvaluate the model in validation set; \n\n17: \n\n\n\nTable 1 :\n1Summary of the two datasets: EMOPIA and VGMIDI.Datasets #Train #Valid #Test #Label \n\nEMOPIA \n869 \n114 \n88 \n4 \nVGMIDI 4,876 \n879 \n1,436 \n4 \n\n\n\nTable 2 :\n2Main result of SMER in the EMOPIA and VGMIDI dataset.Model \n\nEMOPIA \nVGMIDI \n\nAccuracy(%) \nmacro-F1 \nAccuracy(%) \nmacro-F1 \n\nSVM([Lin et al., 2013]) \n47.72 \n0.4763 \n45.12 \n0.3779 \nSVM([Panda et al., 2013]) \n39.77 \n0.3624 \n36.93 \n0.2146 \nMIDIGPT[Ferreira et al., 2020] \n58.75\u00b13.13 0.572\u00b10.029 53.88\u00b13.48 0.505\u00b10.041 \nMIDIBERT-Piano[Chou et al., 2021] 63.41\u00b13.52 0.628\u00b10.033 47.30\u00b12.81 0.432\u00b10.021 \n\nMT-MIDIGPT(proposed) \n62.50\u00b14.45 0.611\u00b10.047 55.85\u00b11.97 0.509\u00b10.017 \nMT-MIDIBERT(proposed) \n67.58\u00b12.39 0.664\u00b10.027 49.81\u00b12.52 0.453\u00b10.019 \n\n\n\nTable 3 :\n3The contribution of different tasks on EMOPIA dataset using MIDIBERT backbone. Results are evaluated by the accurary of music emotion recognition task. Key Classification Velocity Classificaiton Accuracy 63.41\u00b13.52 67.03\u00b12.54 64.73\u00b15.47 67.58\u00b12.39\nhttps://zenodo.org/record/5257995 2 https://github.com/lucasnfe/bardocomposer/tree/master/data/vgmidi\nhttps://github.com/wazenmai/MIDI-BERT\n\nPiano sound characteristics: a study on some factors affecting loudness in digital and acoustic pianos. [ References, Adli, Second International Conference on Innovative Computing, Informatio and Control (ICICIC 2007). IEEEReferences [Adli et al., 2007] Alexander Adli, Zensho Nakao, Toshiaki Yokoda, and Yasunori Nagata. Piano sound characteristics: a study on some factors affecting loudness in digital and acoustic pianos. In Second International Conference on Innovative Computing, Informatio and Control (ICICIC 2007), pages 34-34. IEEE, 2007.\n\nSelection of audio features for music emotion recognition using production music. abs/2107.05223Audio Engineering Society Conference: 53rd International Conference: Semantic Audio. Audio Engineering Society. J. Stephen Downie and Remco CCoRRCuthbert and ArizaMusic21: A toolkit for computer-aided musicology and symbolic music dataet al., 2014] Chris Baume, Gy\u00f6rgy Fazekas, Mathieu Bar- thet, David Marston, and Mark Sandler. Selection of audio fea- tures for music emotion recognition using production music. In Audio Engineering Society Conference: 53rd International Con- ference: Semantic Audio. Audio Engineering Society, 2014. [Chou et al., 2021] Yi-Hui Chou, I-Chun Chen, Chin-Jui Chang, Joann Ching, and Yi-Hsuan Yang. Midibert-piano: Large- scale pre-training for symbolic music understanding. CoRR, abs/2107.05223, 2021. [Cuthbert and Ariza, 2010] Michael Scott Cuthbert and Christo- pher Ariza. Music21: A toolkit for computer-aided musicology and symbolic music data. In J. Stephen Downie and Remco C.\n\nA developmental study of the affective value of tempo and mode in music. Veltkamp ; Dalla, Bella, Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010. the 11th International Society for Music Information Retrieval Conference, ISMIR 2010Utrecht, Netherlands80International Society for Music Information RetrievalVeltkamp, editors, Proceedings of the 11th International Soci- ety for Music Information Retrieval Conference, ISMIR 2010, Utrecht, Netherlands, August 9-13, 2010, pages 637-642. Inter- national Society for Music Information Retrieval, 2010. [Dalla Bella et al., 2001] Simone Dalla Bella, Isabelle Peretz, Luc Rousseau, and Nathalie Gosselin. A developmental study of the affective value of tempo and mode in music. Cognition, 80(3):B1-B10, 2001.\n\nBERT: pre-training of deep bidirectional transformers for language understanding. Devlin, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Arthur Flexer, Geoffroy Peeters, Juli\u00e1n Urbano, and Anja Volkthe 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USA; Delft, The Netherlands; Levi Lelis, and Jim WhiteheadLucas Ferreira1Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital EntertainmentDevlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Associa- tion for Computational Linguistics: Human Language Technolo- gies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171-4186, 2019. [Ferreira and Whitehead, 2019] Lucas Ferreira and Jim Whitehead. Learning to generate music with sentiment. In Arthur Flexer, Ge- offroy Peeters, Juli\u00e1n Urbano, and Anja Volk, editors, Proceed- ings of the 20th International Society for Music Information Re- trieval Conference, ISMIR 2019, Delft, The Netherlands, Novem- ber 4-8, 2019, pages 384-390, 2019. [Ferreira et al., 2020] Lucas Ferreira, Levi Lelis, and Jim White- head. Computer-generated music for tabletop role-playing games. In Proceedings of the AAAI Conference on Artificial Intel- ligence and Interactive Digital Entertainment, volume 16, pages 59-65, 2020.\n\nThe development of affective responses to modality and melodic contour. ; Gerken, M Gina, Louann Gerardi, Gerken, Music Perception. 123and Gerken, 1995] Gina M Gerardi and Louann Gerken. The development of affective responses to modality and melodic contour. Music Perception, 12(3):279-290, 1995.\n\nEMOPIA: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation. Gregory, ICLR 2019Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. Marianna Pinchot Kastner and Robert G. CrowderPrague, Czech Republic; Seattle, WA, USA; New Orleans, LA, USA; OnlineKastner and Crowder20Music PerceptionGregory et al., 1996] Andrew H Gregory, Lisa Worrall, and Ann Sarge. The development of emotional responses to music in young children. Motivation and Emotion, 20(4):341-348, 1996. [Grekow and Ras, 2009] Jacek Grekow and Zbigniew W. Ras. De- tecting emotions in classical music from MIDI files. In Jan Rauch, Zbigniew W. Ras, Petr Berka, and Tapio Elomaa, edi- tors, Foundations of Intelligent Systems, 18th International Sym- posium, ISMIS 2009, Prague, Czech Republic, September 14-17, 2009. Proceedings, volume 5722 of Lecture Notes in Computer Science, pages 261-270. Springer, 2009. [Hsiao et al., 2021] Wen-Yi Hsiao, Jen-Yu Liu, Yin-Cheng Yeh, and Yi-Hsuan Yang. Compound word transformer: Learning to compose full-song music over dynamic directed hypergraphs. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Ar- tificial Intelligence, IAAI 2021, The Eleventh Symposium on Ed- ucational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 178-186. AAAI Press, 2021. [Huang and Yang, 2020] Yu-Siang Huang and Yi-Hsuan Yang. Pop music transformer: Beat-based modeling and generation of ex- pressive pop piano compositions. In Chang Wen Chen, Rita Cucchiara, Xian-Sheng Hua, Guo-Jun Qi, Elisa Ricci, Zhengyou Zhang, and Roger Zimmermann, editors, MM '20: The 28th ACM International Conference on Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020, pages 1180-1188. ACM, 2020. [Huang et al., 2019] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam Shazeer, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and Douglas Eck. Music transformer: Generating music with long- term structure. In 7th International Conference on Learning Rep- resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. [Hung et al., 2021] Hsiao-Tzu Hung, Joann Ching, Seungheon Doh, Nabin Kim, Juhan Nam, and Yi-Hsuan Yang. EMOPIA: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation. In Jin Ha Lee, Alexander Lerch, Zhiyao Duan, Juhan Nam, Preeti Rao, Peter van Kra- nenburg, and Ajay Srinivasamurthy, editors, Proceedings of the 22nd International Society for Music Information Retrieval Con- ference, ISMIR 2021, Online, November 7-12, 2021, pages 318- 325, 2021. [Kastner and Crowder, 1990] Marianna Pinchot Kastner and Robert G. Crowder. Perception of the Major/Minor Distinc- tion: IV. Emotional Connotations in Young Children. Music Perception, 8(2):189-201, 12 1990.\n\nChanging musical emotion: A computational rule system for modifying score and performance. ; Krumhansl ; Carol L Krumhansl, K\u00f6rner ; Lukas Liebel, Marco Liebel, ; K\u00f6rner, Lin, arXiv:1805.06334Proceedings of the 14th International Society for Music Information Retrieval Conference. Alceu de Souza Britto Jr., Fabien Gouyon, and Simon Dixonthe 14th International Society for Music Information Retrieval ConferenceCuritiba, Brazil; New Orleans, LA, USA; New Orleans, Louisiana, USAMichigan Publishing1967arXiv preprintProceedings of the 2006 International Computer Music ConferenceKrumhansl, 2001] Carol L Krumhansl. Cognitive foundations of musical pitch. Oxford University Press, 2001. [Liebel and K\u00f6rner, 2018] Lukas Liebel and Marco K\u00f6rner. Auxiliary tasks in multi-task learning. arXiv preprint arXiv:1805.06334, 2018. [Lin et al., 2013] Yi Lin, Xiaoou Chen, and Deshun Yang. Explo- ration of music emotion recognition based on MIDI. In Alceu de Souza Britto Jr., Fabien Gouyon, and Simon Dixon, editors, Proceedings of the 14th International Society for Music Informa- tion Retrieval Conference, ISMIR 2013, Curitiba, Brazil, Novem- ber 4-8, 2013, pages 221-226, 2013. [Liu et al., 2018] Tong Liu, Li Han, Liangkai Ma, and Dongwei Guo. Audio-based deep music emotion recognition. In AIP Con- ference Proceedings, volume 1967, page 040021. AIP Publishing LLC, 2018. [Livingstone et al., 2010] Steven R Livingstone, Ralf Muhlberger, Andrew R Brown, and William F Thompson. Changing musical emotion: A computational rule system for modifying score and performance. Computer Music Journal, 34(1):41-64, 2010. [Loshchilov and Hutter, 2019] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Or- leans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. [McKay and Fujinaga, 2006] Cory McKay and Ichiro Fujinaga. jsymbolic: A feature extractor for MIDI files. In Proceedings of the 2006 International Computer Music Conference, ICMC 2006, New Orleans, Louisiana, USA, November 6-11, 2006. Michigan Publishing, 2006.\n\nAnt\u00f3nio Pedro Oliveira, and Rui Pedro Paiva. Multi-modal music emotion recognition: A new dataset, methodology and comparative analysis. 10th International Symposium on Computer Music Multidisciplinary Research (CMMR 2013). 329Ilya Sutskever. et al. Language models are unsupervised multitask learners. OpenAI bloget al., 2020] Sageev Oore, Ian Simon, Sander Dieleman, Douglas Eck, and Karen Simonyan. This time with feeling: learning expressive musical performance. Neural Comput. Appl., 32(4):955-967, 2020. [Panda et al., 2013] Renato Eduardo Silva Panda, Ricardo Mal- heiro, Bruno Rocha, Ant\u00f3nio Pedro Oliveira, and Rui Pe- dro Paiva. Multi-modal music emotion recognition: A new dataset, methodology and comparative analysis. In 10th Inter- national Symposium on Computer Music Multidisciplinary Re- search (CMMR 2013), pages 570-582, 2013. [Panda et al., 2018] Renato Panda, Ricardo Malheiro, and Rui Pe- dro Paiva. Novel audio features for music emotion recogni- tion. IEEE Transactions on Affective Computing, 11(4):614-626, 2018. [Panda et al., 2020] Renato Panda, Ricardo Manuel Malheiro, and Rui Pedro Paiva. Audio features for music emotion recognition: a survey. IEEE Transactions on Affective Computing, 2020. [Radford et al., 2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nMidinet: A convolutional generative adversarial network for symbolic-domain music generation. Russell ; James A Russell ; William Forde Thompson, Lola L Cuddy ; Wolf, arXiv:1910.03771Proceedings of the 18th International Society for Music Information Retrieval Conference. Sally Jo Cunningham, Zhiyao Duan, Xiao Hu, and Douglas Turnbullthe 18th International Society for Music Information Retrieval ConferenceSuzhou, China39arXiv preprintHuggingface's transformers: State-of-the-art natural language processingRussell, 1980] James A Russell. A circumplex model of affect. Journal of personality and social psychology, 39(6):1161, 1980. [Thompson and Cuddy, 1997] William Forde Thompson and Lola L Cuddy. Music performance and the perception of key. Journal of Experimental Psychology: Human Perception and Performance, 23(1):116, 1997. [Wolf et al., 2019] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cis- tac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Hugging- face's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. [Yang et al., 2017] Li-Chia Yang, Szu-Yu Chou, and Yi-Hsuan Yang. Midinet: A convolutional generative adversarial network for symbolic-domain music generation. In Sally Jo Cunningham, Zhiyao Duan, Xiao Hu, and Douglas Turnbull, editors, Proceed- ings of the 18th International Society for Music Information Re- trieval Conference, ISMIR 2017, Suzhou, China, October 23-27, 2017, pages 324-331, 2017.\n\nMusicbert: Symbolic music understanding with large-scale pre-training. Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021. Chengqing Zong, Fei Xia, Wenjie Li, and Roberto NavigliAssociation for Computational Linguisticsvolume ACL/IJCNLP 2021 of Findings of ACLet al., 2021] Mingliang Zeng, Xu Tan, Rui Wang, Zeqian Ju, Tao Qin, and Tie-Yan Liu. Musicbert: Symbolic music under- standing with large-scale pre-training. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pages 791-800. Association for Computational Linguistics, 2021.\n", "annotations": {"author": "[{\"end\":126,\"start\":77},{\"end\":182,\"start\":127},{\"end\":233,\"start\":183}]", "publisher": null, "author_last_name": "[{\"end\":86,\"start\":83},{\"end\":142,\"start\":138},{\"end\":193,\"start\":188}]", "author_first_name": "[{\"end\":82,\"start\":77},{\"end\":128,\"start\":127},{\"end\":137,\"start\":129},{\"end\":187,\"start\":183}]", "author_affiliation": "[{\"end\":125,\"start\":88},{\"end\":181,\"start\":144},{\"end\":232,\"start\":195}]", "title": "[{\"end\":74,\"start\":1},{\"end\":307,\"start\":234}]", "venue": null, "abstract": "[{\"end\":1543,\"start\":309}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1797,\"start\":1777},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1814,\"start\":1797},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1833,\"start\":1814},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1852,\"start\":1833},{\"end\":2049,\"start\":2030},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2068,\"start\":2049},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2089,\"start\":2068},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2173,\"start\":2154},{\"end\":2191,\"start\":2173},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2734,\"start\":2715},{\"end\":2870,\"start\":2857},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3061,\"start\":3035},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3082,\"start\":3061},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3107,\"start\":3082},{\"end\":3127,\"start\":3121},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3338,\"start\":3328},{\"end\":4210,\"start\":4204},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6780,\"start\":6760},{\"end\":7051,\"start\":7032},{\"end\":8365,\"start\":8346},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9721,\"start\":9700},{\"end\":9775,\"start\":9745},{\"end\":9853,\"start\":9820},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10526,\"start\":10500},{\"end\":10613,\"start\":10594},{\"end\":12431,\"start\":12412},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12473,\"start\":12443},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12495,\"start\":12473},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12960,\"start\":12945},{\"end\":13779,\"start\":13773},{\"end\":14065,\"start\":14036},{\"end\":14389,\"start\":14370},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14885,\"start\":14862},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19673,\"start\":19651},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19693,\"start\":19673},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19765,\"start\":19747},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19789,\"start\":19769}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":19391,\"start\":19341},{\"attributes\":{\"id\":\"fig_1\"},\"end\":19569,\"start\":19392},{\"attributes\":{\"id\":\"fig_2\"},\"end\":20726,\"start\":19570},{\"attributes\":{\"id\":\"fig_3\"},\"end\":20954,\"start\":20727},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":21595,\"start\":20955},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":21748,\"start\":21596},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":22299,\"start\":21749},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":22559,\"start\":22300}]", "paragraph": "[{\"end\":2735,\"start\":1559},{\"end\":3555,\"start\":2737},{\"end\":4309,\"start\":3557},{\"end\":4529,\"start\":4311},{\"end\":4804,\"start\":4531},{\"end\":4960,\"start\":4806},{\"end\":5110,\"start\":4962},{\"end\":5273,\"start\":5112},{\"end\":5387,\"start\":5290},{\"end\":6457,\"start\":5407},{\"end\":7904,\"start\":6484},{\"end\":7908,\"start\":7906},{\"end\":8105,\"start\":7979},{\"end\":8644,\"start\":8125},{\"end\":9102,\"start\":8684},{\"end\":9448,\"start\":9113},{\"end\":9988,\"start\":9450},{\"end\":10224,\"start\":10021},{\"end\":10739,\"start\":10244},{\"end\":11211,\"start\":10765},{\"end\":11489,\"start\":11247},{\"end\":11744,\"start\":11530},{\"end\":12053,\"start\":11764},{\"end\":12323,\"start\":12092},{\"end\":12691,\"start\":12339},{\"end\":13554,\"start\":12719},{\"end\":13839,\"start\":13556},{\"end\":15089,\"start\":13866},{\"end\":15745,\"start\":15194},{\"end\":16044,\"start\":15783},{\"end\":16944,\"start\":16087},{\"end\":17233,\"start\":16946},{\"end\":17865,\"start\":17254},{\"end\":18627,\"start\":17867},{\"end\":19102,\"start\":18642},{\"end\":19340,\"start\":19104}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7978,\"start\":7909},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11246,\"start\":11212},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11529,\"start\":11490},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12091,\"start\":12054}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":12793,\"start\":12786},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":16231,\"start\":16224},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":16282,\"start\":16275},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":16327,\"start\":16320},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":17387,\"start\":17380},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":17397,\"start\":17390}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1557,\"start\":1545},{\"attributes\":{\"n\":\"2\"},\"end\":5288,\"start\":5276},{\"attributes\":{\"n\":\"3\"},\"end\":5405,\"start\":5390},{\"attributes\":{\"n\":\"3.1\"},\"end\":6482,\"start\":6460},{\"attributes\":{\"n\":\"3.2\"},\"end\":8123,\"start\":8108},{\"attributes\":{\"n\":\"3.3\"},\"end\":8682,\"start\":8647},{\"attributes\":{\"n\":\"3.4\"},\"end\":9111,\"start\":9105},{\"attributes\":{\"n\":\"3.5\"},\"end\":10019,\"start\":9991},{\"end\":10242,\"start\":10227},{\"end\":10763,\"start\":10742},{\"end\":11762,\"start\":11747},{\"attributes\":{\"n\":\"4\"},\"end\":12337,\"start\":12326},{\"attributes\":{\"n\":\"4.1\"},\"end\":12717,\"start\":12694},{\"attributes\":{\"n\":\"4.2\"},\"end\":13864,\"start\":13842},{\"end\":15139,\"start\":15092},{\"end\":15192,\"start\":15142},{\"attributes\":{\"n\":\"4.3\"},\"end\":15781,\"start\":15748},{\"attributes\":{\"n\":\"4.4\"},\"end\":16085,\"start\":16047},{\"attributes\":{\"n\":\"4.5\"},\"end\":17252,\"start\":17236},{\"attributes\":{\"n\":\"5\"},\"end\":18640,\"start\":18630},{\"end\":19352,\"start\":19342},{\"end\":20738,\"start\":20728},{\"end\":20959,\"start\":20956},{\"end\":21606,\"start\":21597},{\"end\":21759,\"start\":21750},{\"end\":22310,\"start\":22301}]", "table": "[{\"end\":21595,\"start\":21178},{\"end\":21748,\"start\":21655},{\"end\":22299,\"start\":21814}]", "figure_caption": "[{\"end\":19391,\"start\":19354},{\"end\":19569,\"start\":19394},{\"end\":20726,\"start\":19572},{\"end\":20954,\"start\":20740},{\"end\":21178,\"start\":20961},{\"end\":21655,\"start\":21608},{\"end\":21814,\"start\":21761},{\"end\":22559,\"start\":22312}]", "figure_ref": "[{\"end\":5582,\"start\":5573},{\"end\":5647,\"start\":5639},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7179,\"start\":7171},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7394,\"start\":7386},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17945,\"start\":17937},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18217,\"start\":18209},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18234,\"start\":18226},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18412,\"start\":18404}]", "bib_author_first_name": "[{\"end\":22806,\"start\":22805},{\"end\":26847,\"start\":26846},{\"end\":26857,\"start\":26856},{\"end\":26870,\"start\":26864},{\"end\":30285,\"start\":30284},{\"end\":30331,\"start\":30317},{\"end\":30345,\"start\":30340},{\"end\":30355,\"start\":30354},{\"end\":33828,\"start\":33811},{\"end\":33877,\"start\":33863}]", "bib_author_last_name": "[{\"end\":22817,\"start\":22807},{\"end\":22823,\"start\":22819},{\"end\":24355,\"start\":24339},{\"end\":24362,\"start\":24357},{\"end\":25162,\"start\":25156},{\"end\":26854,\"start\":26848},{\"end\":26862,\"start\":26858},{\"end\":26878,\"start\":26871},{\"end\":26886,\"start\":26880},{\"end\":27180,\"start\":27173},{\"end\":30315,\"start\":30286},{\"end\":30338,\"start\":30332},{\"end\":30352,\"start\":30346},{\"end\":30362,\"start\":30356},{\"end\":30367,\"start\":30364},{\"end\":33861,\"start\":33829},{\"end\":33882,\"start\":33878}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":43584588},\"end\":23249,\"start\":22701},{\"attributes\":{\"doi\":\"abs/2107.05223\",\"id\":\"b1\",\"matched_paper_id\":45252592},\"end\":24264,\"start\":23251},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":15598315},\"end\":25072,\"start\":24266},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":52967399},\"end\":26772,\"start\":25074},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":145497204},\"end\":27071,\"start\":26774},{\"attributes\":{\"doi\":\"ICLR 2019\",\"id\":\"b5\",\"matched_paper_id\":236881050},\"end\":30191,\"start\":27073},{\"attributes\":{\"doi\":\"arXiv:1805.06334\",\"id\":\"b6\",\"matched_paper_id\":17591825},\"end\":32299,\"start\":30193},{\"attributes\":{\"id\":\"b7\"},\"end\":33715,\"start\":32301},{\"attributes\":{\"doi\":\"arXiv:1910.03771\",\"id\":\"b8\",\"matched_paper_id\":2002865},\"end\":35242,\"start\":33717},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":235391043},\"end\":35969,\"start\":35244}]", "bib_title": "[{\"end\":22803,\"start\":22701},{\"end\":23331,\"start\":23251},{\"end\":24337,\"start\":24266},{\"end\":25154,\"start\":25074},{\"end\":26844,\"start\":26774},{\"end\":27171,\"start\":27073},{\"end\":30282,\"start\":30193},{\"end\":32436,\"start\":32301},{\"end\":33809,\"start\":33717},{\"end\":35313,\"start\":35244}]", "bib_author": "[{\"end\":22819,\"start\":22805},{\"end\":22825,\"start\":22819},{\"end\":24357,\"start\":24339},{\"end\":24364,\"start\":24357},{\"end\":25164,\"start\":25156},{\"end\":26856,\"start\":26846},{\"end\":26864,\"start\":26856},{\"end\":26880,\"start\":26864},{\"end\":26888,\"start\":26880},{\"end\":27182,\"start\":27173},{\"end\":30317,\"start\":30284},{\"end\":30340,\"start\":30317},{\"end\":30354,\"start\":30340},{\"end\":30364,\"start\":30354},{\"end\":30369,\"start\":30364},{\"end\":33863,\"start\":33811},{\"end\":33884,\"start\":33863}]", "bib_venue": "[{\"end\":24571,\"start\":24466},{\"end\":25603,\"start\":25385},{\"end\":27566,\"start\":27496},{\"end\":30672,\"start\":30532},{\"end\":34139,\"start\":34053},{\"end\":22918,\"start\":22825},{\"end\":23457,\"start\":23347},{\"end\":24464,\"start\":24364},{\"end\":25322,\"start\":25164},{\"end\":26904,\"start\":26888},{\"end\":27448,\"start\":27191},{\"end\":30473,\"start\":30385},{\"end\":32523,\"start\":32438},{\"end\":33988,\"start\":33900},{\"end\":35389,\"start\":35315}]"}}}, "year": 2023, "month": 12, "day": 17}