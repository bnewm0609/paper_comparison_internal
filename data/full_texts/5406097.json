{"id": 5406097, "updated": "2022-03-24 23:22:45.738", "metadata": {"title": "LogMine: Fast Pattern Recognition for Log Analytics", "authors": "[{\"first\":\"Hossein\",\"last\":\"Hamooni\",\"middle\":[]},{\"first\":\"Biplob\",\"last\":\"Debnath\",\"middle\":[]},{\"first\":\"Jianwu\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Hui\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Guofei\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Abdullah\",\"last\":\"Mueen\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 25th ACM International on Conference on Information and Knowledge Management", "publication_date": {"year": 2016, "month": null, "day": null}, "abstract": "Modern engineering incorporates smart technologies in all aspects of our lives. Smart technologies are generating terabytes of log messages every day to report their status. It is crucial to analyze these log messages and present usable information (e.g. patterns) to administrators, so that they can manage and monitor these technologies. Patterns minimally represent large groups of log messages and enable the administrators to do further analysis, such as anomaly detection and event prediction. Although patterns exist commonly in automated log messages, recognizing them in massive set of log messages from heterogeneous sources without any prior information is a significant undertaking. We propose a method, named LogMine, that extracts high quality patterns for a given set of log messages. Our method is fast, memory efficient, accurate, and scalable. LogMine is implemented in map-reduce framework for distributed platforms to process millions of log messages in seconds. LogMine is a robust method that works for heterogeneous log messages generated in a wide variety of systems. Our method exploits algorithmic techniques to minimize the computational overhead based on the fact that log messages are always automatically generated. We evaluate the performance of LogMine on massive sets of log messages generated in industrial applications. LogMine has successfully generated patterns which are as good as the patterns generated by exact and unscalable method, while achieving a 500\u00d7 speedup. Finally, we describe three applications of the patterns generated by LogMine in monitoring large scale industrial systems.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2536393303", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cikm/HamooniDXZJM16", "doi": "10.1145/2983323.2983358"}}, "content": {"source": {"pdf_hash": "fc023354345bc603b5f965b46253c26dddeb0f1b", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "fe609ef949cdf907bbedc7094f1ee100079157c0", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fc023354345bc603b5f965b46253c26dddeb0f1b.txt", "contents": "\nLogMine: Fast Pattern Recognition for Log Analytics\nOctober 24 -28, 2016\n\nHossein Hamooni hamooni@unm.edu \nUniversity of New\nMexico\n\nBiplob Debnath biplob@nec-labs.com \nUniversity Blvd Albuquerque\nNEC\nLaboratories America 4 Independence Way Princeton87131, 08540NM, NJ\n\nJianwu Xu jianwu@nec-labs.com \nNEC\nLaboratories America 4 Independence Way Princeton08540NJ\n\nHui Zhang huizhang@nec-labs.com \nNEC\nLaboratories America 4 Independence Way Princeton08540NJ\n\nGuofei Jiang \nNEC\nLaboratories America 4 Independence Way Princeton08540NJ\n\nAbdullah Mueen mueen@unm.edu \nUniversity of New Mexico\n1 University Blvd Albuquerque87131NM\n\nLogMine: Fast Pattern Recognition for Log Analytics\n\nCIKM'16\nIndianapolis, IN, USAOctober 24 -28, 201610.1145/2983323.2983358CCS Concepts \u2022Computing methodologies \u2192 MapReduce algorithms\u2022Information systems \u2192 ClusteringKeywords Log analysisPattern recognitionMap-reduce\nModern engineering incorporates smart technologies in all aspects of our lives. Smart technologies are generating terabytes of log messages every day to report their status. It is crucial to analyze these log messages and present usable information (e.g. patterns) to administrators, so that they can manage and monitor these technologies. Patterns minimally represent large groups of log messages and enable the administrators to do further analysis, such as anomaly detection and event prediction. Although patterns exist commonly in automated log messages, recognizing them in massive set of log messages from heterogeneous sources without any prior information is a significant undertaking. We propose a method, named LogMine, that extracts high quality patterns for a given set of log messages. Our method is fast, memory efficient, accurate, and scalable. LogMine is implemented in map-reduce framework for distributed platforms to process millions of log messages in seconds. LogMine is a robust method that works for heterogeneous log messages generated in a wide variety of systems. Our method exploits algorithmic techniques to minimize the computational overhead based on the fact that log messages are always automatically generated. We evaluate the performance of Log-Mine on massive sets of log messages generated in industrial applications. LogMine has successfully generated patterns which are as good as the patterns generated by exact and unscalable method, while achieving a 500\u00d7 speedup. Finally, we describe three applications of the patterns generated by LogMine in monitoring large scale industrial systems.\n\nINTRODUCTION\n\nThe Internet of Things (IoT) enables advanced connectivity of computing and embedded devices through internet infrastructure. Although computers and smartphones are the most common devices in IoT, the number of \"things\" is expected to grow to 50 billion by 2020 [5]. IoT involves machine-to-machine communications (M2M), where it is important to continuously monitor connected machines to detect any anomaly or bug, and resolve them quickly to minimize the downtime. Logging is a commonly used mechanism to record machines' behaviors and various states for maintenance and troubleshooting. An acceptable logging standard is yet to be developed for IoT, most commonly due to the enormous varieties of \"things\" and their fast evolution over time. Thus, it is extremely challenging to parse and analyze log messages from systems like IoT.\n\nAn automated log analyzer must have one component to recognize patterns from log messages, and another component to match these patterns with the inflow of log messages to identify events and anomalies. Such a log message analyzer must have the following desirable properties:\n\n\u2022 No-supervision: The pattern recognizer needs to be working from the scratch without any prior knowledge or human supervision. For a new log message format, the pattern recognizer should not require an input from the administrator. \u2022 Heterogeneity: There can be log messages generated from different applications and systems. Each system may generate log messages in multiple formats. An automated recognizer must find all formats of the log messages irrespective of their origins. \u2022 Efficiency: IoT-like systems generate millions of log messages every day. The log processing should be done so efficiently that the processing rate is always faster than the log generation rate. \u2022 Scalability: Pattern recognizer must be able to process massive batches of log messages to maintain a current set of patterns without incurring CPU and memory bottlenecks. Many companies such as Splunk [11], Sumo Logic [12], Loggly [6], LogEntries [7], etc. offer log analysis tools. Open  source packages such as ElasticSearch [2], Graylog [4] and OS-SIM [9] have also been developed to analyze logs. Most of these tools and packages use regular expressions (regex) to match with log messages. These tools assume that the administrators know how to work with regex, and there are plenty of tools and libraries that support regex. However, these tools do not have the desirable properties mentioned earlier. By definition, these tools support only supervised matching. Human involvement is clearly non-scalable for heterogeneous and continuously evolving log message formats in systems such as IoT, and it is humanly impossible to parse the sheer number of log entries generated in an hour, let alone days and weeks. On top of that, writing regex rules is long, frustrating, error-prone, and regex rules may conflict with each other especially for IoT-like systems. Even if a set of regex rules is written, the rate of processing log messages can be slow due to overgeneralized regexes.\n\nA recent work on automated pattern recognition has shown a methodology, called HLAer, for automatically parsing heterogenous log messages [22]. Although HLAer is unsupervised and robust to heterogeneity, it is not efficient and scalable because of massive memory requirement and communication overhead in parallel implementation.\n\nIn this paper, we present an end-to-end framework, Log-Mine, that addresses all of the discussed problems with the existing tools and packages. LogMine is an unsupervised framework that scans log messages only once and, therefore, can quickly process hundreds of millions of log messages with a very small amount of memory. LogMine works in iterative manner that generates a hierarchy of patterns (regexes), one level at every iteration. The hierarchy provides flexibility to the users to select the right set of patterns that satisfies their specific needs. We implement a map-reduce version of LogMine to deploy in a massively parallel data processing system, and achieve an impressive 500\u00d7 speedup over a naive exact method.\n\nThe rest of this paper is organized to discuss related work in Section 1, and background in Section 2. We describe our proposed method in Section 3. Section 4 and 5 discuss the experimental findings and case studies on the related problems respectively. Finally, we conclude in Section 6.\n\n\nRELATED WORK AND BACKGROUND\n\nAuthors of [29] have proposed a method to cluster the web logs without any need to user-defined parameters. Their method is not scalable to large datasets because the time complexity is O(n 3 ) where n is the number of the logs. [16] introduces a method to create the search index of a website based on the users' search logs. [25] discusses a preprocessing algorithm that extracts a set of fields such as IP, date, URL, etc. from a given dataset of web logs. Authors of [17] have proposed a method to help website admins by extracting useful information from users' navigation logs.\n\nIn [15], the authors have clearly reasoned why map-reduce is the choice for log processing rather than RDBMS. Authors have showed various join processing techniques for log data in map-reduce framework. This work, along with [20], greatly inspired us to attempt clustering on massive log data. In [19], the authors describe a unified logging infrastructure for heterogeneous applications. Our framework is well suited to work on top of both of these infrastructures with minimal modification. In HPC (High Performance Computing), logs have been used to identify failures, and troubleshoot the failures in large scale systems [23]. Such tools majorly focus on categorizing archived log messages into sequence of failure events, and use the sequence to identify root cause of a problem.\n\n\nMotivating Examples\n\nIn Figure 1, we show examples of log patterns that our method generates. Our method clusters the messages into coherent groups, and identifies the most specific log pattern to represent each clusters. In Figure 1, the input log segment has four different clusters. The messages within each cluster can be merged to produce the log patterns shown in red. These four patterns can again be clustered and merged to form two more general patterns (in green). Same process can be repeated till we get to the root of the hierarchy which Clearly the most generic pattern is a set of wildcards, and the most specific pattern is a set of fixed attributes. None of these patterns are useful for the administrators. Our method produces a hierarchy of patterns: specific patterns are children of general patterns. Such hierarchy is useful for the system administrators to pick the right level of detail they want to track in the log messages as opposed to write regular expression manually.\n\n\nPattern Recognition Framework\n\nWith the above motivation, we design a novel framework for LogMine as shown in the Figure 2. LogMine takes a large batch of logs as input and clusters them very quickly with a restrictive constraints using the clustering module. A pattern is then generated for each cluster by our pattern recognition module. The sets of patterns form the leaf level of the pattern hierarchy. The method will continue to generate clusters with relaxed constraints in subsequent iterations and merge the clusters to form more general patterns, which will constitute a new parent level in the hierarchy. LogMine continues to iterate until the most general pattern has been achieved and/or the hierarchy of patterns is completely formed.\n\nThe framework meets all the criteria of a good log analytics tool. It is an unsupervised framework that does not assume any input from the administrator. The framework produces a hierarchy of patterns which is interpretable to the administrator. The framework does not assume any property in sources of the log messages. The recognizer can work on daily or hourly batches if the following challenges can be tackled.\n\n\nChallenges\n\nThis framework for log pattern recognition is unsupervised and suitable for heterogeneous logs. However, the scalability of the framework depends on the two major parts of the framework: log clustering and pattern recognition. Standard clustering and recognition methods do not scale well, and it is non-trivial to design scalable versions of the clustering and recognition modules for massive sets of log messages. Since the two modules work in a closed loop, we must speedup both of them to scale the framework for large datasets.\n\nTo quantify the significance of the challenge, let us imagine an average website that receives about 2 million visitors a day (much less compared to 500 million tweets a day in Twitter, or 3 billion searches a day in Google). Even if we assume that each visit results in only one log message, 2 million log messages per day is a reasonable number. Clustering such a large number of log messages in only one iteration is extremely time consuming. A standard DBSCAN [28] algorithm takes about two days to process a dataset of this size with state-of-the-art optimization techniques [1]. Similar amount of time would be needed by k-means algorithm although no one would know the best value for the parameter k. Clearly, the framework cannot work on daily batches using standard clustering algorithms and, therefore, we need an optimized clustering algorithm developed for log analysis. A similar argument can be made for the recognition module where a standard multiple sequence alignment operation on a reasonable sized cluster may need more than a day to recognize the patterns.\n\n\nFAST LOG-PATTERN MINING\n\nThe key intuition behind our log mining approach is that logs are automatically generated messages unlike sentences from a story book. There are some specific lines in an application source code that produce the logs, therefore, all log messages from the same application are generated by a finite set of formats. Standard clustering and merging methods do not consider any dependency among the objects in the dataset. In logs, the dependency among the messages is natural, and as we show in this paper, the dependency is useful to speedup the clustering and the merging process.\n\n\nTokenization and Type Detection\n\nWe assume all the logs are stored in a text file and each line contains a log message. We do a simple pre-processing on each log. We tokenize every log message by using whitespace separation. We then detect a set of pre-defined types such as date, time, IP and number, and replace the real value of each field with the name of the field. For instance, we replace 2015-07-09 with date, or 192.168.10.15 with IP. This set of pre-defined types can be configured by the user based on his interest in the content over the type of a field. Figure 3 shows an example of tokenization and type replacements in a log message. Tokenization and type detection is embedded in the clustering algorithm without adding any overhead due to the one-pass nature of the clustering algorithm described in the next section. Although this step is not mandatory, we use it to make the similarity between two logs meaningful. If no type detection is done, two logs generated by the same pattern can have a low similarity, just because they have different values for the same field. Therefore, we may end up generating huge number of unnecessary patterns if we do not tokenize.\n\n\nFast and Memory Efficient Clustering\n\nOur clustering algorithm is simply a one-pass version of the friends-of-friend clustering for the log messages. Our algorithm exploits several optimization techniques to improve the clustering performance. \n\n\nDistance Function\n\nWe first define the distance between two log messages by the following equations:\nDist(P, Q) = 1 \u2212 M in(len(P ),len(Q)) i=1 Score(P i ,Q i ) M ax(len(P ),len(Q)) Score(x, y) = k1 if x=y 0 otherwise\nPi is the ith field of log P , and len(P ) is the number of fields of log P . k1 is a tunable parameters. We set k1 = 1 in our default log distance function, but this parameter can be changed to put more or less weight on the matched fields in two log messages.\n\nSince we want to cluster patterns in the subsequent iterations of our framework, we also need a distance function for patterns. The distance between two patterns is defined very similarly as the distance between two log messages, just with a new score function. We again set k1 = k2 = 1 in our default pattern distance function. Our distance function is non-negative, symmetric, reflexive, and it satisfies the triangular inequality. Therefore, it is a metric. Log messages generated by the same format have a very small distance (zero in most cases), and log messages generated by different formats have larger distances. This is a desirable property for fast and memory-efficient log clustering algorithm. In the high dimensional space, the log messages form completely separated and highly dense regions. Therefore, finding the clusters using the above distance function is a straightforward task.\n\n\nFinding Clusters\n\nIn this section, we explain how to find the dense clusters out of the input logs in a sequential fashion. The same approach will also be used when we create the hierarchy of log patterns by several iterations of clustering. First, we define an internal parameter named MaxDist, which represents the maximum distance between any log entry/message in a cluster and the cluster representative. Therefore, the maximum distance between any two logs in a cluster is 2\u00d7MaxDist. We start from the first log message and process all the log messages one by one until we reach the last message. Each cluster has a representative log message, which is also the first member of the cluster. For any new log message, we insert the message in one of the existing clusters only if the distance between the log and the representative is less than the MaxDist. Otherwise, when the message is not similar to any representative, we create a new cluster and put the log message as the representative of that new cluster.\n\nThe above process can be implemented in a very small memory footprint. We need to keep just one representative log for each cluster in the memory, and output the subsequent log messages without saving in the memory. This allows our algorithm to process massive number of logs with a small amount of memory. In fact, the memory usage of our clustering algorithm is O(number of clusters). Ignoring large number of log messages when deciding about cluster membership and using only one representative log message do not reduce the quality of the clusters. The major reason is that all the log messages in any given cluster are almost identical because they are most likely generated by the same code segment of the same application. Therefore, the above one-pass clustering algorithm with a very small MaxDist in the beginning can generate highly dense (i.e., consistent) clusters of log messages, where keeping one representative message is both sufficient and efficient.\n\nWe finally have a set of dense clusters with one representative each. As mentioned before, this algorithm is also used to cluster and merge patterns. In case of clustering the patterns, unlike the above approach, we keep all the patterns in each cluster because we will use them in the pattern recognition component. In most systems, the set of patterns generated after the first iteration fits in the memory. The speed and efficiency of this algorithms comes from the fact that the number of dense clusters does not scale with the number of log messages, because it is not possible for an application to generate huge number of unique patterns. In other words, finding a million unique patterns is impossible even in a dataset of hundreds of millions of log messages.\n\nThe one-pass clustering algorithm has a strong dependency on the order of the messages, which is typically the temporal order. The pathological worst case happens when one message from every pattern in every cluster appears very early in the log, and all of the remaining messages will have to be compared with all of the unique representatives. In practice, log messages from the same application show temporal co-location which makes them more favorable for the clustering algorithm.\n\n\nEarly Abandoning Technique\n\nEarly abandoning is a useful technique to speedup similarity search under Euclidean distance. Some of the initial mentions of early abandoning were in [18] [13]. It has been extensively used later by many researchers for problems such as time series motif discovery [21], and similarity search under dynamic time warping (DTW) [24]. We adopt the technique for log analytics.\n\nWhen comparing a new log message with a cluster representative, if the distance is smaller than MaxDist, we can add the new log to that cluster. Since the distance between two logs is calculated in one scan of the logs by summing up only non-negative numbers, early abandoning techniques can be applied. As we compare two logs field by field, we may discover that the accumulated distance has already exceeded the threshold, MaxDist, even though many of the fields are yet to be compared. In this case, we don't need to continue calculating the distance completely, because we are certain that these two logs are not in MaxDist radius of each other. Since the number of fields in a log can be large, this technique helps us skip a significant amount of calculation, especially when MaxDist is small.\n\n\nScaling via Map-Reduce Implementation\n\nWe mentioned earlier that the memory usage of our onepass clustering algorithm is O(number of clusters). The onepass clustering algorithm is very amenable to parallel execution via map-reduce approach. For each log in our dataset, we create a key-value pair. The key is a fixed number (in our case 1), and the value is a singleton list containing the given log. We also add the length based index to the value of each tuple. In the reduce function, we can merge every pair of lists. Specifically, we always keep the bigger list as the base list, and update this base list by adding all elements of the smaller list to it (if needed). This makes the merging process faster. While adding the elements of the smaller list to the base set, we add only the elements which do not have any similar representative in the base set. If a very close representative already exists in the base list, we ignore the log. We also update the length based index of the base list meanwhile. Finally, the base list will be the result of the merging of two given lists. The pseudo code of the reduce function can be found in Algorithm 1. Since we use the same key for all the logs, we will get one tuple as the final output which contains all the log representative (dense clusters). As we need to create a key-value tuple for each log, the memory usage of the map-reduce implementation is no longer O(number of dense clusters), in fact it is O(number of log entries). This is not a problem because each worker in map-reduce platform loads a chunk of the logs. Even if a chunk of the data does not fit in memory, new map-reduce frameworks like Spark [30] can handle that with a small overhead. We compare the running time of both sequential and parallel implementations in Section 4.\n\n\nLog Pattern Recognition\n\nAfter we cluster the logs, we need to find a pattern for each cluster. Since we keep one representative for each dense cluster in the first round, the representative itself is the pattern  of its cluster, but in the subsequent rounds, after we cluster the patterns, we need an algorithm that can generate one pattern for a set of logs/patterns in a cluster. We start with the pattern generation process for a pair of patterns and then generalize to a set of patterns.\n\n\nPattern Generation from Pairs\n\nIrrespective of the pattern recognition algorithm, we always need to merge two logs at some point in the algorithm. Therefore, we shortly discuss our Merge algorithm here. Given two logs to be merged, we first need to find their best alignment. The best alignment of two logs is the one that generates the minimum number of wildcards and variables after merging. In the alignment process, some gaps may be inserted between the fields of each log. The alignment algorithm ensures that the length of two logs are equal after inserting the gaps. Once we have two logs with the same length, we process them field by field and generate the output. An example is shown in Figure 4. Note that the align step introduces gaps in the second message. The field detection step requires a straightforward scan of the two logs. A detailed pseudocode can be found in Algorithm 2. There are different algorithms for aligning two sequences. We use Smith-Waterman algorithm which can align two sequences of length l1 and l2 in O(l1.l2) time steps [26]. Therefore, the time complexity of our Merge function is also O(l1.l2). We use the same score function as in [22] for the Smith-Waterman algorithm.\n\n\nAlgorithm 2 Merge\n\nInput: Two logs (Loga,\nLog b ) Output: A merged log Log a , Log b \u2190 Align(Loga, Log b ) for i , i = 2, 3, . . . , |Log a | do x \u2190 F ieldi(Log a ) and y \u2190 F ieldi(Log b ) if x = y then F ieldi(Lognew) \u2190 x else if T ype(x) = T ype(y) then F ieldi(Lognew) \u2190 V ariable T ype(x) else F ieldi(Lognew) \u2190 W ildcard return Lognew\n\nSequential Pattern Generation\n\nTo generate the pattern for a set of patterns, we start from the first log message, merge it with the second log, then merge the result with the third log and we go on until we get to the last one. Clearly, the success of this approach largely depends on the ordering of the patterns in the set.\n\nHowever, as described before, the logs inside each of the dense clusters are almost identical. This is why, in practice, the merge ordering does not associate with the quality of the final pattern. In other words, we will get the same results if we do the merging in reverse or any arbitrary order. If the logs to be merged are not similar, sequential merging may end up producing a pattern with many wildcards which is not desirable. There exists techniques to find the optimal merge ordering for a set of patterns. We provide detailed experiments in the Section 4 to empirically show that sequential merging does not lose quality.\n\n\nScaling via Map-Reduce Implementation\n\nAs discussed in Section 3.3.2, the order of merging the logs in a cluster to create the final pattern has no effect on the output. Such sequential pattern generation can be parallelize very easily. An efficient way to implement sequential merging is using map-reduce framework. This framework can be useful whenever the order of the operation does not matter, and that is true for our case. Since the pattern recognition is done after clustering the logs, we know the exact cluster for each log. In the map function, we create a key-value pair for each log. The key is the cluster number of the log and the value is the log itself. The map-reduce framework will reduce all the key-value pairs with the same key. In the reduce function, two logs from the same cluster are merged. The final output of the reduce phase is one pattern for each cluster which is exactly what we want. If we ignore the map-reduce framework overhead, in a full parallel running of this algorithm on m machines, its time complexity is O( n m .l 2 ), where n is the number of the logs, and l is the average number of fields in each log.\n\n\nHierarchy of Patterns\n\nIn Sections 3.2.2 and 3.3, we explain how to find dense clusters of logs, and how to find one pattern that covers all the log messages in each cluster. These two modules constitute an iteration in our pattern recognition framework. We also motivate that one set of patterns generated in one of the iterations can be too specific or general, and may not satisfy the administrator. In contrast, a hierarchy of patterns can provide an holistic view of the log messages, and the administrator can pick a level with the right specificity in the hierarchy to monitor for anomalies.\n\nIn order to create the hierarchy, we use both clustering and pattern recognition algorithms iteratively as shown in Figure 2, and produce the hierarchy in bottom-up manner. In the first iteration, we run the clustering algorithm with a very small MaxDist on the given set of logs. This is our most restrictive clustering condition. The output of the clustering is a (possibly large) set of dense clusters each of which has a representative log. The representative log is trivially assigned as the pattern for a dense cluster without calling the pattern recognition module. We treat these patterns as the leaves (lowest level) of the pattern hierarchy. To generate the other levels of the hierarchy, we increase the MaxDist parameter of the clustering algorithm by a factor of \u03b1 (M axDistnew = \u03b1M axDist old ) and run the clustering algorithm on the generated patterns. In other words, we run a more relaxed version of the clustering algorithm on the patterns which will produce new clusters. We then run the pattern recognition module on all the patterns that are clustered together to find more general patterns. These set of new patterns will be added to the hierarchy as a new level. In each iteration of this method, a new level is added to the hierarchy. As we go higher in the hierarchy, we add less number of patterns, which are more general than the patterns in the lower levels. This structure gives us the flexibility to choose whatever level of the hierarchy as the desired set of patterns.\n\n\nHybrid Pattern Recognition\n\nWhen we explain our sequential pattern recognition is Section 3.3.2, we assume that the logs/patterns inside a cluster are very close together. In order to generate the hierarchy of patterns, we start from the leaf level which has the most specific patterns, and the clusters are also dense. As we go up in the hierarchy and merge patterns, the clusters become less dense. Since the MaxDist parameter has been relaxed, we allow patterns with larger distances to group together. This creates a chance of being incorrect at the higher levels of the hierarchy if we use the sequential pattern recognition algorithm. Fortunately, the number of patterns inside each cluster at the higher levels of the hierarchy is much less than the lower levels, and we can use a selective merge order, instead of the sequential order, found by the classic UPGMA (Unweighted Pair Group Method with Arithmetic Mean) method. UPGMA, which is simply the hierarchical clustering with average linkage [27], is optimal when the clusters are spherical in shape in the high dimensional space, and we conjecture that this is asymptotically true for log patterns when clusters contain large number of messages.\n\nOur final pattern recognition algorithm is a hybrid of UP-GMA, the sequential recognition and the map-reduce implementation. Algorithm 3 shows how We pick the best choice for each cluster of logs. th1 and th2 are the thresholds for density and the number of patterns in a cluster respectively. \n\n\nAlgorithm 3 HybridPatternRecognition\n\n\nCost of a Level\n\nGiven a hierarchy of patterns for a set of logs, the user may be interested in a level with specific properties. Some users may prefer to get the minimum number of patterns while the others may be interested to get very specific patterns and not care about the number of patterns. There are many different criteria one can say a level is satisfying or not. We introduce an intuitive cost function to pick the best descriptive level of the hierarchy. We also propose a cost function that suits our datasets. This cost function can easily serve as a general template to calculate the cost of a level of the hierarchy.\nCost = # of clusters i=1\nSizei \u00d7 (a1W Ci + a2V ari + a3F Vi) where Sizei is the number of logs in cluster i and W Ci, V ari and F Vi are the number of wildcards, variable fields and fixed value fields in the pattern of cluster i respectively. a1, a2 and a3 are tunable parameters that can be set in such a way that satisfies user's requirements.\n\nIf a user has no preference, we can set a1 = 1, a2 = 0, and a3 = 0 in the cost function, and select the level having no wildcards with minimum number of patterns as the final set of patterns. For example, in Figure 5 we find that Level 2 generates two patterns with no wildcards, so we select these two patterns from Level 2 as the final set of patterns. In our experiments, we assume that a user will not provide any preferences. A user can also provide preferences by specifying the maximum number of expected patterns. For example, a user may want to generate at most 4 patterns. In this case, we select two patterns from the Level 2 in Figure 5 because it will generate minimum number of wildcards while not exceeding the user given maximum of 4 patterns.\n\n\nEVALUATION\n\nWe describe the baseline method that we compare Log-Mine against first. We then discuss our datasets and provide detailed experimental results.\n\n\nHLAer: The Baseline\n\nWe pick the method HLAer [22] as a baseline algorithm, that is similar to our method in being unsupervised and supporting heterogeneous logs. HLAer finds very good sets of patterns, if not the optimal patterns, under reasonable assumptions. HLAer uses a highly accurate and robust clustering algorithm, OPTICS (Ordering Points To Identify the Clustering Structure), and the average linkage technique (UPGMA) instead of sequential merging for pattern recognition. We describe these two modules next.\n\n\nClustering using OPTICS\n\nOPTICS (Ordering Points To Identify the Clustering Structure) is a famous hierarchical density-based clustering algorithm [14] used in HLAer. In OPTICS, a priority queue of the objects (e.g. using an indexed heap) is generated [28]. The priority queue can be used to cluster the objects at many different levels without redoing the bulk of the computation. OPTICS has two parameters: and MinPts. It ensures that the final clusters have at least MinPts objects, and the maximum distance between any two objects in a cluster is less than or equal to .\n\nOPTICS is an expensive clustering algorithm for large datasets because it needs to calculate the MinPts-nearest neighbors for each object, which requires O(n 2 ) pair-wise distance calculations for n objects. Typical improvement  2,000  65  proprietary  D2  8,028  200  proprietary  D3  10,000,000  90  proprietary  D4  10,000  37  public  D5  10,000  45  public  D6 10,000 25 public strategies include parallel computation and indexing techniques. However, these techniques become invalid in the iterative framework and for near-online batch processing. One may also think of pre-computing the pairwise distances, which requires loading all the computed distances in memory in O(n 2 ) space for random accesses during clustering. Irrespective of non-scalability, OPTICS is a good baseline to compare accuracy with. The algorithm can find clusters of arbitrary shapes and densities in the high dimensional space. The algorithm is easy to reconfigure without recalculating the pair-wise distances.\n\n\nLog pattern recognition via UPGMA\n\nOnce the HLAer finds all the clusters, it needs to find a pattern for each cluster that covers all the log entries in it.\n\nHLAer considers a log entry as a sequence of fields. It finds the best way to align multiple log entries together based on a cost function. After alignment, the algorithm merges each field by selecting a representative field to output. Unweighted Pair Group Method with Arithmetic Mean (UP-GMA) is one of the most commonly used multiple sequence alignment (MSA) algorithms. It is a simple bottom-up hierarchical clustering algorithm [27] which can produce a tree of input objects on the basis of their pairwise similarities. HLAer runs UPGMA on the set of logs and finds the best order of merging the log entries. The Merge function that HLAer uses is identical to the one in Section 3.3.\n\nIf there are n log entries with l fields (on average), the time complexity of running UPGMA is O(n 2 l 2 ) which takes 300 years for 10 million logs. However, UPGMA can be used for smaller number of log entries, and generate valuable ground truth to evaluate the performance of our pattern recognition algorithms.\n\n\nDatasets\n\nFor evaluation, we use six different datasets as summarized in Table 1. D1 and D2: Small proprietary datasets. D3: An industrial proprietary dataset of size 10,000,000 generated by an application during 30 days. The size of D3 on disk is 10 GB. D4 and D5: Random log entries from two traces which contain a day's worth of all HTTP requests to the EPA webserver located at Research Triangle Park, North Carolina [3], and the San Diego Supercomputer Center in San Diego, California [10] respectively. D6: A synthetic dataset generated from 10 pre-defined log patterns. We fix the type of each field in the patterns, and generate random values for that field. This dataset is used as a ground truth to evaluate the accuracy of our method.\n\nWe set M axDist = 0.01 and \u03b1 = 1.3 in all the experiments unless otherwise stated. Since HLAer takes and MinPts as the input parameters, an expert set them for each dataset. As HLAer is a single CPU algorithm, for fair com-parison, we run the sequential version of our algorithm on a single machine for all experiments unless otherwise stated.\n\n\nAccuracy\n\nSince our algorithm is made up of two main components, we test the accuracy of each component separately.\n\n\nAccuracy of Clustering\n\nWe use the clusters generated by OPTICS as a baseline. We define an agreement metric to calculate how close the output of our fast clustering algorithm is to the output of OPTICS. Given a set of n log entries S = {l1, l2, . . . , ln}, we run OPTICS to get the set of clusters X = {X1, X2, . . . , Xr} and we run our clustering algorithm to get the set of clusters Y = {Y1, Y2, . . . , Ys}. The agreement score is a b , where a is the number of pairs of logs in S that are in the same set in Y and in the same set in X, and b is the number of pairs of logs in S that are in the same set in Y . This metric takes a value between 0 (the worst) and 1 (the best). Note that splitting a cluster of OPTICS into multiple clusters does not harm us because it leads to more accurate patterns and we can merge the sub-clusters in higher levels of the hierarchy. On the other hand, having a cluster which has log entries from multiple OPTICS's clusters can generate a meaningless pattern.\n\nAs shown in Table 2, in all datasets except D2, we capture the OPTICS's clusters. The problem with D2 is that the log entries do not have a clear underlying structure. Logs in D2 have many strings and commas, and they are very similar to each other. In addition, OPTICS throws away 38% of the entries as outliers because they do not fall in a cluster with at least MinPts entries. Therefore, a set of separable clusters may not exist in this dataset failing both LogMine and OPTICS.\n\n\nAccuracy of Pattern Recognition\n\nAs discussed in Section 4.1.2, UPGMA finds the best order to merge the log entries, and it produces the best possible pattern for a given cluster. We use the results of UPGMA as a ground truth to evaluate the accuracy of our pattern recognition algorithm. We cluster each dataset by both OP-TICS and our clustering algorithm, and then give each cluster to both UPGMA and our pattern recognition algorithm to produce one pattern. We compare the patterns generated by the two algorithms, field by field. The accuracy of a given pattern compared to the ground truth is simply the number of matched fields over the number of all fields. The accuracy of pattern recognition for each dataset is:\nTotal Accuracy = # of clusters i=1 (Acci\u00d7Sizei) \u00f7 # of clusters i=1\nSizei where Acci is the accuracy of pattern recognition on cluster i and Sizei is the number of log entries in cluster i. As Table  2 shows we can get almost same patterns as UPGMA except in D2. Since the quality of clustering on D2 is low, the logs inside each cluster are not very similar, and the order of merging can change the structure of the final pattern. The fact that our patterns for D2 are 73% similar to UPGMA patterns does not mean that ours are not accurate, because the patterns generated by UPGMA are also low quality. All the other results support the fact that the order of merging the logs has no effect on the final generated pattern in a cluster with similar logs. \n\n\nMemory Usage\n\nHLAer calculates all the logs pairwise distances and use them while running OPTICS. This needs either O(n 2 ) memory space or multiple disk accesses in case all the distances are stored back in the disk for n logs. Conversely, LogMine is very memory efficient with a space complexity of O(number of clusters) in sequential fashion. We run our sequential implementation and measure the amount of memory used by both HLAer and LogMine. Results are shown in Table 2.\n\n\nRunning Time\n\nHLAer has maximum processing capacity of 10,000 log entries because of the quadratic memory requirement. For the rest of the datasets, results are shown in Figure 6(right). LogMine is up to 500\u00d7 faster than HLAer . It takes 1,524 seconds for LogMine to cluster the logs, and find all the patterns in dataset D3.\n\nIt is worth to mention that LogMine has an advantage over HLAer in terms of running time of pattern recognition. Pattern recognition component of LogMine has a fixed running time for datasets of the same size, because it just scans the data once no matter how many cluster exists in the dataset. In contrast, HLAer depends on domain and data properties\n\n\nMap-Reduce vs. Sequential\n\nWe discussed the way we find the dense clusters in Section 3.2.2 both in sequential and map-reduce fashion. In this experiment we compare them. We generate synthetic data by changing number of log entries (10 million default) and number of patterns (1500 default). We change the number of map-reduce workers (8 default) to understand scalability. Each worker has 1 GB of memory and a single-core CPU.\n\nAs shown in Figure 6(left), the execution time of the mapreduce implementation grows slowly compared to the growth of the sequential implementation. Map-reduce implementation reaches up to 5\u00d7 speed-up by using 8 workers compared to the sequential implementation. Note that we have a fixed number of patterns in this experiment. Our mapreduce implementation can handle millions of logs in few minutes, because the number of patterns does not grow at the same rate as the number of logs grows in real world applications. Figure 6(second-left) shows that with increasing number of patterns, the execution time of both sequential and map-reduce implementation consistently grows. In Figure 6(second-right), we show that doubling the number of workers reduces the running time by 40%. The reason is that as we add more workers, the algorithm needs to perform more merges (see Algorithm 1), and this adds more overhead to the algorithm.  \n\n\nParameter Sensitivity\n\nWe have two parameters in our algorithm, and both of them are used in the clustering phase. The first one is the M axDist0 which is the value we use to run the clustering algorithm at the lowest level of the hierarchy. After we find the leaves, we relax the clustering algorithm condition by increasing the M axDist by a factor of \u03b1. We run an experiment 10 times on a dataset of 10,000 log entries picked randomly from dataset D3, and report the average of different measurements change by increasing M axDist0(0.01 default) and \u03b1 (1.3 default). Results are shown in Figure 7. We make the following observations:\n\n\u2022 As we increase M axDist0 we find fewer number of leaves in shorter time. Since the leaves are the basis of our hierarchy, we don't want to lose many of them due to a large M axDist0. On the other hand, small M axDist0 usually (not always) yields to longer running time. Although the best value for M axDist0 is to some extent depends on the dataset, we recommend to set it to 0.01. \u2022 As the M axDist0 grows, we may end up extracting fewer number of patterns, but the plot shows that the algorithm can capture almost the same set of patterns even if we change M axDist0 in a wide range [0.001,0.02]. Thus, our algorithm is not very sensitive to this parameter in terms of the final set of patterns.\n\n\u2022 Obviously if we pick smaller values for M axDist0 and \u03b1, the final hierarchy will have more levels and we have more options to choose a satisfactory level in the hierarchy. However, it takes longer to produce such a hierarchy. \u2022 Number of patterns does not change drastically with changes in \u03b1. We vary this parameter from 1.1 to 1.5 and the number of final patterns stays within the range [966,1127]. We find \u03b1 = 1.3 is the best performing value.\n\n\nCASE STUDY\n\nWe use LogMine to analyze logs collected from the Open-Stack framework, which an open-source platform for cloud computing [8]. We have collected logs from the execution of nova-boot commands to demonstrate how LogMine can be used to build a log analytics solution. LogMine enable us to reveal various insights on logs that helps us to quickly diagnose the cause of failures. Dataset: We have collected logs generated by 200 successful execution of the nova-boot commands. This is our training dataset. Using LogMine we have found 28 patterns which can correctly identify all training logs. These patterns serve as a basis for analyzing OpenStack logs. Next, we have collected logs from six failed executions (i.e., abnormal) of nova-boot commands. This is our testing dataset. Detecting New Logs: To identify the cause of a failure, we need to detect logs which are not seen during the successful (i.e., normal) executions of the nova-boot commands. We use the 28 patterns generated by LogMine to detect those unseen logs. If a log in the testing dataset does not match with any of the 28 patterns, then we conclude that it is a new log. Using LogMine we have correctly identify those new logs, and report them to the system administrators for further analysis. Typically, administrators run adhoc keyword-based search on these new logs using their domain knowledge. In this case, searching few keywords, they have identified a subset of new logs, which help them to quickly localize the cause of the failed executions. Detecting Logs with New Content: To analyze a failure, we are interested to find out whether or not there is any content-wise anomaly among the failure logs. To this end, we have structured 28 patterns generated by LogMine into various fields, and built a content-profile map for each field using the training dataset. During testing, if an incoming log matches with any of the 28 patterns, we identify its fields from the content. Now, if the content of any field value is not present in our training content-profile map, then we report corresponding log to the system administrators for detailed analysis. Using this content analysis, they have correctly identified new contents in the testing logs, which help them to diagnose failure scenarios quickly. Detecting Logs with Abnormal Execution Sequence: The execution of OpenStack could result abnormal log pattern sequence if any failure happens. Therefore, we can detect system failure by discovery of anomalous log sequences. In order to achieve this functionality, we built log sequence order for any pair of 28 patterns and modeled their statistics such as the maximal elapse time, maximal concurrency of log pattern during training stage. During testing, we detect if the incoming log violates any of the following rules: log sequence reversal, exceeding the maximal elapse time or concurrency number, or missing the matching log for the pair. System administrators find these violations very useful to debug failures. Detecting Log Rate Fluctuations: In order to analyze logs, it is helpful to find out whether or not there is any fluctuation in the log rates compared to the normal working scenarios. To detect fluctuations, we keep track of the range (i.e., minimum and maximum counts) that we have observed in a fixed interval of the training dataset for all 28 patterns generated by LogMine. During testing, if the matched logs count of any pattern falls out its training range in an interval, we report corresponding time range and pattern information to the system administrators for the further analysis, and they find it very useful to diagnose failure scenarios.\n\n\nCONCLUSION\n\nWe have proposed an end-to-end framework, LogMine, to identify patterns in massive heterogeneous logs. LogMine is the first such framework that is 1 unsupervised, 2 scalable and 3 robust to heterogeneity. LogMine can process millions of logs in a matter of seconds on a distributed platform. It is a one-pass framework with very low memory footprint, which is useful to scale the framework up to hundreds of millions of logs.\n\nFigure 1 :\n1Extracting log patterns for a given set of logs. Hierarchy of patterns gives user the flexibility to choose a level based on his needs.\n\nFigure 3 :\n3Two examples of how pre-processing works on the input log messages.\n\n\nx=y and both are fixed value k2 if x=y and both are variable 0 otherwise\n\n\nTwo tuples A = (1, List1), B = (1, List2) Output: A tuple if size(List1) >= size(List2) then Base list \u2190 List1 Small list \u2190 List2 else if size(List1) < size(List2) then Base list \u2190 List2 Small list \u2190 List1 for i = 1, . . . , size(Small list) do Found=False for j = 1, . . . , size(Base list) do if d(Small list(i), Base list(j)) \u2264 M axDist then Found=True break if \u00ac Found then Append Small list(i) in the Base list return (1,Base list)\n\nFigure 4 :\n4An example of how Algorithm 2 works.\n\n\nInput: A cluster of logs (Logs) Output: A pattern if density(Logs) \u2264 th1 then pattern \u2190 U P GM A(Logs) else if Size(Logs) \u2264 th2 then pattern \u2190 SequentialP atternGeneration(Logs) else pattern \u2190 M apReduceP atternGeneration(Logs) return pattern\n\nFigure 5 :\n5Pattern selection illustration with a1 = 1, a2 = 0, and a3 = 0 in the cost function.\n\nFigure 6 :\n6Comparison of running times of the sequential and the map-reduce clustering. (right) Comparing the running time of LogMine with HLAer. The y-axis is in log scale.\n\nFigure 7 :\n7Sensitivity of the algorithm to the parameters M axDist0 and \u03b1.\n\n\nWARNING action=remove home=\"/users/tom\" 9. date time,number INFO action=insert user=david id=455095 record=XXX 10. date time,number XXX action=XXX user=tom id=201923 record=XXX 11. date time,number INFO action=set XXX=XXX 12. date time,number WARNING action=remove home=XXX 13. date time,number XXX action=XXX user=XXX id=XXX record=XXX 14. date time,number XXX action=XXX XXX=XXX 15. date time,number XXX action=XXX XXX=XXX XXX*=XXX* XXX*=XXX*1. 2015-07-09 10:22:12,235 INFO action=set root=\"/\" \n2. 2015-07-09 12:32:46,806 INFO action=insert user=tom id=201923 record=abf343rf \n3. 2015-07-09 14:24:16,247 WARNING action=remove home=\"/users/david\" \n4. 2015-07-09 20:09:11,909 INFO action=insert user=david id=455095 record=efrdf4w2 \n5. 2015-07-09 21:56:01,728 INFO action=set home=\"/users\" \n6. 2015-07-09 22:11:56,434 WARNING action=delete user=tom id=201923 record=asepg9e \n7. 2015-07-09 22:32:46,657 INFO action=insert user=david id=455095 record=3jnsg67 \n8. 2015-07-09 22:34:12,724 \n\nTable 1 :\n1A summary of all datasets.Dataset \n# Logs \n# Fields Availability \nD1 \n\n\nTable 2 :\n2The accuracy of pattern recognition and the agreement score for different datasets. We do not report these measurements on D3 because HLAer cannot handle it.Accu-\nracy \n\nAgree-\nment \nScore \n\nHLAer \nMemory \n(MB) \n\nLogMine \nMemory \n(MB) \nD1 \n100% \n86% \n32 \n2 \nD2 \n73% \n48% \n520 \n11 \nD4 \n96% \n95% \n801 \n15 \nD5 \n98% \n100% \n802 \n14 \nD6 \n100% \n100% \n795 \n13 \n\n\n\n. Benchmarking, Optics, Benchmarking for DBSCAN and OPTICS. http://elki.dbs.ifi.lmu.de/wiki/Benchmarking.\n\n. Elasticsearch: Store, Analyze Search, Elasticsearch: Store, Search, and Analyze. https://www.elastic.co/guide/index.html.\n\n. Epa, EPA dataset. http://ita.ee.lbl.gov/html/contrib/EPA-HTTP.html.\n\n. Graylog, GrayLog. https://www.graylog.org.\n\n. Things Internet Of, IoTInternet of Things (IoT). http://www.cisco.com/web/ solutions/trends/iot/overview.html.\n\n. Log Management Explained. Log Management Explained. https://www.loggly.com/log-management-explained/.\n\n. Logentries, LogEntries. https://logentries.com/doc/.\n\n. Openstack, OpenStack. https://en.wikipedia.org/wiki/OpenStack.\n\nOSSIM (Open Source Security Information Management. OSSIM (Open Source Security Information Management). https://en.wikipedia.org/wiki/OSSIM.\n\nSDSC dataset. SDSC dataset. http://ita.ee.lbl.gov/html/contrib/SDSC-HTTP.html.\n\nSplunk. Splunk. http://www.splunk.com/en us/solutions/ solution-areas/internet-of-things.html.\n\nOn the need for time series data mining benchmarks: a survey and empirical demonstration. E , S Kasetty, Data Mining and knowledge discovery. 74E. and S. Kasetty. On the need for time series data mining benchmarks: a survey and empirical demonstration. Data Mining and knowledge discovery, 7(4):349-371, 2003.\n\nOptics: ordering points to identify the clustering structure. M Ankerst, M M Breunig, H.-P Kriegel, J Sander, ACM Sigmod Record. ACM28M. Ankerst, M. M. Breunig, H.-P. Kriegel, and J. Sander. Optics: ordering points to identify the clustering structure. In ACM Sigmod Record, volume 28, pages 49-60. ACM, 1999.\n\nA comparison of join algorithms for log processing in mapreduce. S Blanas, J M Patel, V Ercegovac, J Rao, E J Shekita, Y Tian, SIGMOD. ACMS. Blanas, J. M. Patel, V. Ercegovac, J. Rao, E. J. Shekita, and Y. Tian. A comparison of join algorithms for log processing in mapreduce. In SIGMOD, pages 975-986. ACM, 2010.\n\nLog-based indexing to improve web site search. C Ding, J Zhou, SAC. ACMC. Ding and J. Zhou. Log-based indexing to improve web site search. In SAC, pages 829-833. ACM, 2007.\n\nExtracting knowledge from web server logs using web usage mining. M Eltahir, A Dafa-Alla, Computing, Electrical and Electronics Engineering (ICCEEE). M. Eltahir and A. Dafa-Alla. Extracting knowledge from web server logs using web usage mining. In Computing, Electrical and Electronics Engineering (ICCEEE), pages 413-417, Aug 2013.\n\nFast subsequence matching in time-series databases. C Faloutsos, M Ranganathan, Y Manolopoulos, ACM23C. Faloutsos, M. Ranganathan, and Y. Manolopoulos. Fast subsequence matching in time-series databases, volume 23. ACM, 1994.\n\nThe unified logging infrastructure for data analytics at twitter. G Lee, J Lin, C Liu, A Lorek, D Ryaboy, Proceedings of the VLDB Endowment. the VLDB Endowment5G. Lee, J. Lin, C. Liu, A. Lorek, and D. Ryaboy. The unified logging infrastructure for data analytics at twitter. Proceedings of the VLDB Endowment, 5(12):1771-1780, 2012.\n\nParallel data processing with mapreduce: a survey. K.-H Lee, Y.-J Lee, H Choi, Y D Chung, B Moon, ACM SIGMOD Record. 404K.-H. Lee, Y.-J. Lee, H. Choi, Y. D. Chung, and B. Moon. Parallel data processing with mapreduce: a survey. ACM SIGMOD Record, 40(4):11-20, 2012.\n\nExact discovery of time series motifs. A Mueen, E J Keogh, Q Zhu, S Cash, M B Westover, SDM. SIAMA. Mueen, E. J. Keogh, Q. Zhu, S. Cash, and M. B. Westover. Exact discovery of time series motifs. In SDM, pages 473-484. SIAM, 2009.\n\nHLAer: A system for heterogeneous log analysis. X Ning, G Jiang, SDM Workshop on Heterogeneous Learning. X. Ning and G. Jiang. HLAer: A system for heterogeneous log analysis, 2014. SDM Workshop on Heterogeneous Learning.\n\nMonitoring and predicting hardware failures in hpc clusters with ftb-ipmi. R Rajachandrasekar, X Besseron, D K Panda, IPDPSW Workshops. IEEER. Rajachandrasekar, X. Besseron, and D. K. Panda. Monitoring and predicting hardware failures in hpc clusters with ftb-ipmi. In IPDPSW Workshops, pages 1136-1143. IEEE, 2012.\n\nSearching and mining trillions of time series subsequences under dynamic time warping. T Rakthanmanon, B Campana, A Mueen, G Batista, B Westover, Q Zhu, J Zakaria, E Keogh, SIGKDD. ACMT. Rakthanmanon, B. Campana, A. Mueen, G. Batista, B. Westover, Q. Zhu, J. Zakaria, and E. Keogh. Searching and mining trillions of time series subsequences under dynamic time warping. In SIGKDD, pages 262-270. ACM, 2012.\n\nPreprocessing the web server logs: An illustrative approach for effective usage mining. K S Reddy, G P S Varma, I R Babu, SIGSOFT Softw. Eng. Notes. 373K. S. Reddy, G. P. S. Varma, and I. R. Babu. Preprocessing the web server logs: An illustrative approach for effective usage mining. SIGSOFT Softw. Eng. Notes, 37(3):1-5, May 2012.\n\nIdentification of common molecular subsequences. T F Smith, M S Waterman, Journal of molecular biology. 1471T. F. Smith and M. S. Waterman. Identification of common molecular subsequences. Journal of molecular biology, 147(1):195-197, 1981.\n\nUnweighted pair group method with arithmetic mean. P Sneath, R Sokal, Numerical Taxonomy. P. Sneath and R. Sokal. Unweighted pair group method with arithmetic mean. Numerical Taxonomy, pages 230-234, 1973.\n\nDbscan -wikipedia, the free encyclopedia. Wikipedia, Wikipedia. Dbscan -wikipedia, the free encyclopedia. https://en.wikipedia.org/w/index.php? title=DBSCAN&oldid=672504091, 2015.\n\nNetwork user interest pattern mining based on entropy clustering algorithm. C Xu, S Chen, J Cheng, Cyber-Enabled Distributed Computing and Knowledge Discovery. C. Xu, S. Chen, and J. Cheng. Network user interest pattern mining based on entropy clustering algorithm. In Cyber-Enabled Distributed Computing and Knowledge Discovery, pages 200-204, Sept 2015.\n\nSpark: cluster computing with working sets. M Zaharia, M Chowdhury, M J Franklin, S Shenker, I Stoica, Proceedings of the HotCloud'10. the HotCloud'10M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Stoica. Spark: cluster computing with working sets. In Proceedings of the HotCloud'10.\n", "annotations": {"author": "[{\"end\":133,\"start\":75},{\"end\":270,\"start\":134},{\"end\":363,\"start\":271},{\"end\":458,\"start\":364},{\"end\":534,\"start\":459},{\"end\":627,\"start\":535}]", "publisher": null, "author_last_name": "[{\"end\":90,\"start\":83},{\"end\":148,\"start\":141},{\"end\":280,\"start\":278},{\"end\":373,\"start\":368},{\"end\":471,\"start\":466},{\"end\":549,\"start\":544}]", "author_first_name": "[{\"end\":82,\"start\":75},{\"end\":140,\"start\":134},{\"end\":277,\"start\":271},{\"end\":367,\"start\":364},{\"end\":465,\"start\":459},{\"end\":543,\"start\":535}]", "author_affiliation": "[{\"end\":132,\"start\":108},{\"end\":269,\"start\":170},{\"end\":362,\"start\":302},{\"end\":457,\"start\":397},{\"end\":533,\"start\":473},{\"end\":626,\"start\":565}]", "title": "[{\"end\":52,\"start\":1},{\"end\":679,\"start\":628}]", "venue": "[{\"end\":688,\"start\":681}]", "abstract": "[{\"end\":2527,\"start\":897}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2808,\"start\":2805},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4546,\"start\":4542},{\"end\":4563,\"start\":4559},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4575,\"start\":4572},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4591,\"start\":4588},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4671,\"start\":4668},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4684,\"start\":4681},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4699,\"start\":4696},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5770,\"start\":5766},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7023,\"start\":7019},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7241,\"start\":7237},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7339,\"start\":7335},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7483,\"start\":7479},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7600,\"start\":7596},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7822,\"start\":7818},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7894,\"start\":7890},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8222,\"start\":8218},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11563,\"start\":11559},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11678,\"start\":11675},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19030,\"start\":19026},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19035,\"start\":19031},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19145,\"start\":19141},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19206,\"start\":19202},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21725,\"start\":21721},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23416,\"start\":23412},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23530,\"start\":23526},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":29134,\"start\":29130},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31622,\"start\":31618},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32245,\"start\":32241},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32350,\"start\":32346},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":34264,\"start\":34260},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":35257,\"start\":35254},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":35327,\"start\":35323},{\"end\":41486,\"start\":41474},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":43466,\"start\":43463}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":47580,\"start\":47432},{\"attributes\":{\"id\":\"fig_1\"},\"end\":47661,\"start\":47581},{\"attributes\":{\"id\":\"fig_2\"},\"end\":47736,\"start\":47662},{\"attributes\":{\"id\":\"fig_3\"},\"end\":48175,\"start\":47737},{\"attributes\":{\"id\":\"fig_4\"},\"end\":48225,\"start\":48176},{\"attributes\":{\"id\":\"fig_5\"},\"end\":48470,\"start\":48226},{\"attributes\":{\"id\":\"fig_6\"},\"end\":48568,\"start\":48471},{\"attributes\":{\"id\":\"fig_7\"},\"end\":48744,\"start\":48569},{\"attributes\":{\"id\":\"fig_8\"},\"end\":48821,\"start\":48745},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":49809,\"start\":48822},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":49892,\"start\":49810},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":50259,\"start\":49893}]", "paragraph": "[{\"end\":3378,\"start\":2543},{\"end\":3656,\"start\":3380},{\"end\":5626,\"start\":3658},{\"end\":5957,\"start\":5628},{\"end\":6686,\"start\":5959},{\"end\":6976,\"start\":6688},{\"end\":7591,\"start\":7008},{\"end\":8377,\"start\":7593},{\"end\":9378,\"start\":8401},{\"end\":10129,\"start\":9412},{\"end\":10546,\"start\":10131},{\"end\":11093,\"start\":10561},{\"end\":12172,\"start\":11095},{\"end\":12779,\"start\":12200},{\"end\":13966,\"start\":12815},{\"end\":14213,\"start\":14007},{\"end\":14316,\"start\":14235},{\"end\":14694,\"start\":14433},{\"end\":15596,\"start\":14696},{\"end\":16616,\"start\":15617},{\"end\":17587,\"start\":16618},{\"end\":18357,\"start\":17589},{\"end\":18844,\"start\":18359},{\"end\":19249,\"start\":18875},{\"end\":20050,\"start\":19251},{\"end\":21854,\"start\":20092},{\"end\":22349,\"start\":21882},{\"end\":23564,\"start\":22383},{\"end\":23608,\"start\":23586},{\"end\":24234,\"start\":23939},{\"end\":24868,\"start\":24236},{\"end\":26020,\"start\":24910},{\"end\":26621,\"start\":26046},{\"end\":28124,\"start\":26623},{\"end\":29334,\"start\":28155},{\"end\":29630,\"start\":29336},{\"end\":30304,\"start\":29689},{\"end\":30650,\"start\":30330},{\"end\":31411,\"start\":30652},{\"end\":31569,\"start\":31426},{\"end\":32091,\"start\":31593},{\"end\":32668,\"start\":32119},{\"end\":33666,\"start\":32670},{\"end\":33825,\"start\":33704},{\"end\":34515,\"start\":33827},{\"end\":34830,\"start\":34517},{\"end\":35578,\"start\":34843},{\"end\":35923,\"start\":35580},{\"end\":36041,\"start\":35936},{\"end\":37044,\"start\":36068},{\"end\":37528,\"start\":37046},{\"end\":38253,\"start\":37564},{\"end\":39009,\"start\":38322},{\"end\":39489,\"start\":39026},{\"end\":39817,\"start\":39506},{\"end\":40171,\"start\":39819},{\"end\":40601,\"start\":40201},{\"end\":41535,\"start\":40603},{\"end\":42174,\"start\":41561},{\"end\":42875,\"start\":42176},{\"end\":43326,\"start\":42877},{\"end\":46991,\"start\":43341},{\"end\":47431,\"start\":47006}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14432,\"start\":14317},{\"attributes\":{\"id\":\"formula_1\"},\"end\":23906,\"start\":23609},{\"attributes\":{\"id\":\"formula_2\"},\"end\":30329,\"start\":30305},{\"attributes\":{\"id\":\"formula_3\"},\"end\":38321,\"start\":38254}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":33036,\"start\":32900},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":34913,\"start\":34906},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":37065,\"start\":37058},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":38455,\"start\":38447},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":39488,\"start\":39481}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2541,\"start\":2529},{\"attributes\":{\"n\":\"2.\"},\"end\":7006,\"start\":6979},{\"attributes\":{\"n\":\"2.1\"},\"end\":8399,\"start\":8380},{\"attributes\":{\"n\":\"2.2\"},\"end\":9410,\"start\":9381},{\"attributes\":{\"n\":\"2.3\"},\"end\":10559,\"start\":10549},{\"attributes\":{\"n\":\"3.\"},\"end\":12198,\"start\":12175},{\"attributes\":{\"n\":\"3.1\"},\"end\":12813,\"start\":12782},{\"attributes\":{\"n\":\"3.2\"},\"end\":14005,\"start\":13969},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":14233,\"start\":14216},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":15615,\"start\":15599},{\"attributes\":{\"n\":\"3.2.3\"},\"end\":18873,\"start\":18847},{\"attributes\":{\"n\":\"3.2.4\"},\"end\":20090,\"start\":20053},{\"attributes\":{\"n\":\"3.3\"},\"end\":21880,\"start\":21857},{\"attributes\":{\"n\":\"3.3.1\"},\"end\":22381,\"start\":22352},{\"end\":23584,\"start\":23567},{\"attributes\":{\"n\":\"3.3.2\"},\"end\":23937,\"start\":23908},{\"attributes\":{\"n\":\"3.3.3\"},\"end\":24908,\"start\":24871},{\"attributes\":{\"n\":\"3.4\"},\"end\":26044,\"start\":26023},{\"attributes\":{\"n\":\"3.4.1\"},\"end\":28153,\"start\":28127},{\"end\":29669,\"start\":29633},{\"attributes\":{\"n\":\"3.4.2\"},\"end\":29687,\"start\":29672},{\"attributes\":{\"n\":\"4.\"},\"end\":31424,\"start\":31414},{\"attributes\":{\"n\":\"4.1\"},\"end\":31591,\"start\":31572},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":32117,\"start\":32094},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":33702,\"start\":33669},{\"attributes\":{\"n\":\"4.2\"},\"end\":34841,\"start\":34833},{\"attributes\":{\"n\":\"4.3\"},\"end\":35934,\"start\":35926},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":36066,\"start\":36044},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":37562,\"start\":37531},{\"attributes\":{\"n\":\"4.4\"},\"end\":39024,\"start\":39012},{\"attributes\":{\"n\":\"4.5\"},\"end\":39504,\"start\":39492},{\"attributes\":{\"n\":\"4.6\"},\"end\":40199,\"start\":40174},{\"attributes\":{\"n\":\"4.7\"},\"end\":41559,\"start\":41538},{\"attributes\":{\"n\":\"5.\"},\"end\":43339,\"start\":43329},{\"attributes\":{\"n\":\"6.\"},\"end\":47004,\"start\":46994},{\"end\":47443,\"start\":47433},{\"end\":47592,\"start\":47582},{\"end\":48187,\"start\":48177},{\"end\":48482,\"start\":48472},{\"end\":48580,\"start\":48570},{\"end\":48756,\"start\":48746},{\"end\":49820,\"start\":49811},{\"end\":49903,\"start\":49894}]", "table": "[{\"end\":49809,\"start\":49268},{\"end\":49892,\"start\":49848},{\"end\":50259,\"start\":50062}]", "figure_caption": "[{\"end\":47580,\"start\":47445},{\"end\":47661,\"start\":47594},{\"end\":47736,\"start\":47664},{\"end\":48175,\"start\":47739},{\"end\":48225,\"start\":48189},{\"end\":48470,\"start\":48228},{\"end\":48568,\"start\":48484},{\"end\":48744,\"start\":48582},{\"end\":48821,\"start\":48758},{\"end\":49268,\"start\":48824},{\"end\":49848,\"start\":49822},{\"end\":50062,\"start\":49905}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8412,\"start\":8404},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8613,\"start\":8605},{\"end\":9503,\"start\":9495},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13357,\"start\":13349},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23057,\"start\":23049},{\"end\":26747,\"start\":26739},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":30868,\"start\":30860},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":31300,\"start\":31292},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":39670,\"start\":39662},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":40623,\"start\":40615},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":41130,\"start\":41122},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":41304,\"start\":41282},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":42137,\"start\":42129}]", "bib_author_first_name": "[{\"end\":50399,\"start\":50392},{\"end\":50618,\"start\":50612},{\"end\":51361,\"start\":51360},{\"end\":51365,\"start\":51364},{\"end\":51644,\"start\":51643},{\"end\":51655,\"start\":51654},{\"end\":51657,\"start\":51656},{\"end\":51671,\"start\":51667},{\"end\":51682,\"start\":51681},{\"end\":51958,\"start\":51957},{\"end\":51968,\"start\":51967},{\"end\":51970,\"start\":51969},{\"end\":51979,\"start\":51978},{\"end\":51992,\"start\":51991},{\"end\":51999,\"start\":51998},{\"end\":52001,\"start\":52000},{\"end\":52012,\"start\":52011},{\"end\":52255,\"start\":52254},{\"end\":52263,\"start\":52262},{\"end\":52448,\"start\":52447},{\"end\":52459,\"start\":52458},{\"end\":52768,\"start\":52767},{\"end\":52781,\"start\":52780},{\"end\":52796,\"start\":52795},{\"end\":53009,\"start\":53008},{\"end\":53016,\"start\":53015},{\"end\":53023,\"start\":53022},{\"end\":53030,\"start\":53029},{\"end\":53039,\"start\":53038},{\"end\":53331,\"start\":53327},{\"end\":53341,\"start\":53337},{\"end\":53348,\"start\":53347},{\"end\":53356,\"start\":53355},{\"end\":53358,\"start\":53357},{\"end\":53367,\"start\":53366},{\"end\":53583,\"start\":53582},{\"end\":53592,\"start\":53591},{\"end\":53594,\"start\":53593},{\"end\":53603,\"start\":53602},{\"end\":53610,\"start\":53609},{\"end\":53618,\"start\":53617},{\"end\":53620,\"start\":53619},{\"end\":53824,\"start\":53823},{\"end\":53832,\"start\":53831},{\"end\":54073,\"start\":54072},{\"end\":54093,\"start\":54092},{\"end\":54105,\"start\":54104},{\"end\":54107,\"start\":54106},{\"end\":54402,\"start\":54401},{\"end\":54418,\"start\":54417},{\"end\":54429,\"start\":54428},{\"end\":54438,\"start\":54437},{\"end\":54449,\"start\":54448},{\"end\":54461,\"start\":54460},{\"end\":54468,\"start\":54467},{\"end\":54479,\"start\":54478},{\"end\":54810,\"start\":54809},{\"end\":54812,\"start\":54811},{\"end\":54821,\"start\":54820},{\"end\":54825,\"start\":54822},{\"end\":54834,\"start\":54833},{\"end\":54836,\"start\":54835},{\"end\":55105,\"start\":55104},{\"end\":55107,\"start\":55106},{\"end\":55116,\"start\":55115},{\"end\":55118,\"start\":55117},{\"end\":55349,\"start\":55348},{\"end\":55359,\"start\":55358},{\"end\":55762,\"start\":55761},{\"end\":55768,\"start\":55767},{\"end\":55776,\"start\":55775},{\"end\":56087,\"start\":56086},{\"end\":56098,\"start\":56097},{\"end\":56111,\"start\":56110},{\"end\":56113,\"start\":56112},{\"end\":56125,\"start\":56124},{\"end\":56136,\"start\":56135}]", "bib_author_last_name": "[{\"end\":50275,\"start\":50263},{\"end\":50283,\"start\":50277},{\"end\":50390,\"start\":50370},{\"end\":50406,\"start\":50400},{\"end\":50498,\"start\":50495},{\"end\":50573,\"start\":50566},{\"end\":50630,\"start\":50619},{\"end\":50841,\"start\":50831},{\"end\":50896,\"start\":50887},{\"end\":51373,\"start\":51366},{\"end\":51652,\"start\":51645},{\"end\":51665,\"start\":51658},{\"end\":51679,\"start\":51672},{\"end\":51689,\"start\":51683},{\"end\":51965,\"start\":51959},{\"end\":51976,\"start\":51971},{\"end\":51989,\"start\":51980},{\"end\":51996,\"start\":51993},{\"end\":52009,\"start\":52002},{\"end\":52017,\"start\":52013},{\"end\":52260,\"start\":52256},{\"end\":52268,\"start\":52264},{\"end\":52456,\"start\":52449},{\"end\":52469,\"start\":52460},{\"end\":52778,\"start\":52769},{\"end\":52793,\"start\":52782},{\"end\":52809,\"start\":52797},{\"end\":53013,\"start\":53010},{\"end\":53020,\"start\":53017},{\"end\":53027,\"start\":53024},{\"end\":53036,\"start\":53031},{\"end\":53046,\"start\":53040},{\"end\":53335,\"start\":53332},{\"end\":53345,\"start\":53342},{\"end\":53353,\"start\":53349},{\"end\":53364,\"start\":53359},{\"end\":53372,\"start\":53368},{\"end\":53589,\"start\":53584},{\"end\":53600,\"start\":53595},{\"end\":53607,\"start\":53604},{\"end\":53615,\"start\":53611},{\"end\":53629,\"start\":53621},{\"end\":53829,\"start\":53825},{\"end\":53838,\"start\":53833},{\"end\":54090,\"start\":54074},{\"end\":54102,\"start\":54094},{\"end\":54113,\"start\":54108},{\"end\":54415,\"start\":54403},{\"end\":54426,\"start\":54419},{\"end\":54435,\"start\":54430},{\"end\":54446,\"start\":54439},{\"end\":54458,\"start\":54450},{\"end\":54465,\"start\":54462},{\"end\":54476,\"start\":54469},{\"end\":54485,\"start\":54480},{\"end\":54818,\"start\":54813},{\"end\":54831,\"start\":54826},{\"end\":54841,\"start\":54837},{\"end\":55113,\"start\":55108},{\"end\":55127,\"start\":55119},{\"end\":55356,\"start\":55350},{\"end\":55365,\"start\":55360},{\"end\":55555,\"start\":55546},{\"end\":55765,\"start\":55763},{\"end\":55773,\"start\":55769},{\"end\":55782,\"start\":55777},{\"end\":56095,\"start\":56088},{\"end\":56108,\"start\":56099},{\"end\":56122,\"start\":56114},{\"end\":56133,\"start\":56126},{\"end\":56143,\"start\":56137}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":50366,\"start\":50261},{\"attributes\":{\"id\":\"b1\"},\"end\":50491,\"start\":50368},{\"attributes\":{\"id\":\"b2\"},\"end\":50562,\"start\":50493},{\"attributes\":{\"id\":\"b3\"},\"end\":50608,\"start\":50564},{\"attributes\":{\"id\":\"b4\"},\"end\":50722,\"start\":50610},{\"attributes\":{\"id\":\"b5\"},\"end\":50827,\"start\":50724},{\"attributes\":{\"id\":\"b6\"},\"end\":50883,\"start\":50829},{\"attributes\":{\"id\":\"b7\"},\"end\":50949,\"start\":50885},{\"attributes\":{\"id\":\"b8\"},\"end\":51092,\"start\":50951},{\"attributes\":{\"id\":\"b9\"},\"end\":51172,\"start\":51094},{\"attributes\":{\"id\":\"b10\"},\"end\":51268,\"start\":51174},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6072819},\"end\":51579,\"start\":51270},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":9378040},\"end\":51890,\"start\":51581},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":7526304},\"end\":52205,\"start\":51892},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":17307101},\"end\":52379,\"start\":52207},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":25133725},\"end\":52713,\"start\":52381},{\"attributes\":{\"id\":\"b16\"},\"end\":52940,\"start\":52715},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":16254261},\"end\":53274,\"start\":52942},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1878208},\"end\":53541,\"start\":53276},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":8262995},\"end\":53773,\"start\":53543},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":16970889},\"end\":53995,\"start\":53775},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":12804904},\"end\":54312,\"start\":53997},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":3018103},\"end\":54719,\"start\":54314},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":12894003},\"end\":55053,\"start\":54721},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":20031248},\"end\":55295,\"start\":55055},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":89906163},\"end\":55502,\"start\":55297},{\"attributes\":{\"id\":\"b26\"},\"end\":55683,\"start\":55504},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":18200158},\"end\":56040,\"start\":55685},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":11818928},\"end\":56339,\"start\":56042}]", "bib_title": "[{\"end\":51358,\"start\":51270},{\"end\":51641,\"start\":51581},{\"end\":51955,\"start\":51892},{\"end\":52252,\"start\":52207},{\"end\":52445,\"start\":52381},{\"end\":53006,\"start\":52942},{\"end\":53325,\"start\":53276},{\"end\":53580,\"start\":53543},{\"end\":53821,\"start\":53775},{\"end\":54070,\"start\":53997},{\"end\":54399,\"start\":54314},{\"end\":54807,\"start\":54721},{\"end\":55102,\"start\":55055},{\"end\":55346,\"start\":55297},{\"end\":55759,\"start\":55685},{\"end\":56084,\"start\":56042}]", "bib_author": "[{\"end\":50277,\"start\":50263},{\"end\":50285,\"start\":50277},{\"end\":50392,\"start\":50370},{\"end\":50408,\"start\":50392},{\"end\":50500,\"start\":50495},{\"end\":50575,\"start\":50566},{\"end\":50632,\"start\":50612},{\"end\":50843,\"start\":50831},{\"end\":50898,\"start\":50887},{\"end\":51364,\"start\":51360},{\"end\":51375,\"start\":51364},{\"end\":51654,\"start\":51643},{\"end\":51667,\"start\":51654},{\"end\":51681,\"start\":51667},{\"end\":51691,\"start\":51681},{\"end\":51967,\"start\":51957},{\"end\":51978,\"start\":51967},{\"end\":51991,\"start\":51978},{\"end\":51998,\"start\":51991},{\"end\":52011,\"start\":51998},{\"end\":52019,\"start\":52011},{\"end\":52262,\"start\":52254},{\"end\":52270,\"start\":52262},{\"end\":52458,\"start\":52447},{\"end\":52471,\"start\":52458},{\"end\":52780,\"start\":52767},{\"end\":52795,\"start\":52780},{\"end\":52811,\"start\":52795},{\"end\":53015,\"start\":53008},{\"end\":53022,\"start\":53015},{\"end\":53029,\"start\":53022},{\"end\":53038,\"start\":53029},{\"end\":53048,\"start\":53038},{\"end\":53337,\"start\":53327},{\"end\":53347,\"start\":53337},{\"end\":53355,\"start\":53347},{\"end\":53366,\"start\":53355},{\"end\":53374,\"start\":53366},{\"end\":53591,\"start\":53582},{\"end\":53602,\"start\":53591},{\"end\":53609,\"start\":53602},{\"end\":53617,\"start\":53609},{\"end\":53631,\"start\":53617},{\"end\":53831,\"start\":53823},{\"end\":53840,\"start\":53831},{\"end\":54092,\"start\":54072},{\"end\":54104,\"start\":54092},{\"end\":54115,\"start\":54104},{\"end\":54417,\"start\":54401},{\"end\":54428,\"start\":54417},{\"end\":54437,\"start\":54428},{\"end\":54448,\"start\":54437},{\"end\":54460,\"start\":54448},{\"end\":54467,\"start\":54460},{\"end\":54478,\"start\":54467},{\"end\":54487,\"start\":54478},{\"end\":54820,\"start\":54809},{\"end\":54833,\"start\":54820},{\"end\":54843,\"start\":54833},{\"end\":55115,\"start\":55104},{\"end\":55129,\"start\":55115},{\"end\":55358,\"start\":55348},{\"end\":55367,\"start\":55358},{\"end\":55557,\"start\":55546},{\"end\":55767,\"start\":55761},{\"end\":55775,\"start\":55767},{\"end\":55784,\"start\":55775},{\"end\":56097,\"start\":56086},{\"end\":56110,\"start\":56097},{\"end\":56124,\"start\":56110},{\"end\":56135,\"start\":56124},{\"end\":56145,\"start\":56135}]", "bib_venue": "[{\"end\":53101,\"start\":53083},{\"end\":56192,\"start\":56177},{\"end\":50750,\"start\":50726},{\"end\":51001,\"start\":50951},{\"end\":51106,\"start\":51094},{\"end\":51180,\"start\":51174},{\"end\":51410,\"start\":51375},{\"end\":51708,\"start\":51691},{\"end\":52025,\"start\":52019},{\"end\":52273,\"start\":52270},{\"end\":52529,\"start\":52471},{\"end\":52765,\"start\":52715},{\"end\":53081,\"start\":53048},{\"end\":53391,\"start\":53374},{\"end\":53634,\"start\":53631},{\"end\":53878,\"start\":53840},{\"end\":54131,\"start\":54115},{\"end\":54493,\"start\":54487},{\"end\":54868,\"start\":54843},{\"end\":55157,\"start\":55129},{\"end\":55385,\"start\":55367},{\"end\":55544,\"start\":55504},{\"end\":55843,\"start\":55784},{\"end\":56175,\"start\":56145}]"}}}, "year": 2023, "month": 12, "day": 17}