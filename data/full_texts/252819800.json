{"id": 252819800, "updated": "2023-04-05 01:43:42.016", "metadata": {"title": "Serving Heterogeneous Machine Learning Models on Multi-GPU Servers with Spatio-Temporal Sharing", "authors": "[{\"first\":\"Seungbeom\",\"last\":\"Choi\",\"middle\":[]},{\"first\":\"Sunho\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Yeonjae\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Jongse\",\"last\":\"Park\",\"middle\":[]},{\"first\":\"Youngjin\",\"last\":\"Kwon\",\"middle\":[]},{\"first\":\"Jaehyuk\",\"last\":\"Huh\",\"middle\":[]}]", "venue": "USENIX Annual Technical Conference", "journal": "199-216", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "As machine learning (ML) techniques are applied to a widening range of applications, high throughput ML inference serving has become critical for online services. Such ML inference servers with multiple GPUs pose new challenges in the scheduler design. First, they must provide a bounded latency for each request to support a consistent service-level objective (SLO). Second, they must be able to serve multiple heterogeneous ML models in a system, as cloud-based consolidation improves system utilization. To address the two requirements of ML inference servers, this paper proposes a new inference scheduling framework for multi-model ML inference servers. The paper shows that with SLO constraints, GPUs with growing parallelism are not fully utilized for ML inference tasks. To maximize the resource ef\ufb01ciency of GPUs, a key mechanism proposed in this paper is to exploit hardware support for spatial partitioning of GPU resources. With spatio-temporal sharing, a new abstraction layer of GPU resources is created with con\ufb01gurable GPU resources. The scheduler assigns requests to virtual GPUs, called gpulets , with the most effective amount of resources. The scheduler explores the three-dimensional search space with different batch sizes, temporal sharing, and spatial sharing ef\ufb01ciently. To minimize the cost for cloud-based inference servers, the framework auto-scales the required number of GPUs for a given workload. To consider the potential interference overheads when two ML tasks are running concurrently by spatially sharing a GPU, the scheduling decision is made with an interference prediction model. Our prototype implementation proves that the proposed spatio-temporal scheduling enhances throughput by 61.7% on average compared to the prior temporal scheduler, Abstract To maximize the resource ef\ufb01ciency of inference servers, we proposed a key mechanism to exploit hardware support for spatial partitioning of GPU resources. With the partitioning mechanism, a new abstraction layer of GPU resources is created with con\ufb01gurable GPU resources. The scheduler assigns requests to virtual GPUs, called gpulets, with the most effective amount of resources. The prototype framework auto-scales the required number of GPUs for a given workloads, minimizing the cost for cloud-based inference servers. The prototype framework also deploys a remedy for potential interference effects when two ML tasks are running concurrently in a GPU.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/usenix/ChoiLKPKH22", "doi": null}}, "content": {"source": {"pdf_hash": "556e7d4e591c5e3c9790c5e3b6b4810d225468a4", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "2e1aef7e5ef14c1e3fde9b7e71c6aa3211b33dd2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/556e7d4e591c5e3c9790c5e3b6b4810d225468a4.txt", "contents": "\nServing Heterogeneous Machine Learning Models on Multi-GPU Servers with Spatio-Temporal Sharing\n\n\nSeungbeom Choi \nSunho Lee \nYeonjae Kim \nJongse Park \nYoungjin Kwon \nJaehyuk Huh \nServing Heterogeneous Machine Learning Models on Multi-GPU Servers with Spatio-Temporal Sharing\n\n\n\n\u25aa GPUs are widely adopted as inference accelerator \u25aa Following requirements must be satisfied: Serve queries in a bounded time, service-level objective (SLO) Serve multiple-heterogeneous ML models \u25aa Improves throughput and utilization of GPU \u25aa Batch size could not be huge due to SLO \n\n\nFor each partition size\n\nStep \u2460 Get maximum batch size b with Latency(b) < SLO\n\nStep   \n\u2461 Get throughput = b / Latency(b) Resource (Partition size\n\u25aa\nChallenge: Large overhead exists for preparing a new Gpulet \u25aa Overhead: Loading kernels, warming up models \u25aa Solution: Hide overhead by shadowing in the\n\n\nPrior Approach: Time-Sharing \u25aa Time-sharing: Round-based interleaved execution of batches[1]    \u25aa Need an abstraction of spatial/temporal resource \u25aa Gpulet: A share of spatial/temporal partition of GPU resourcePrior Approach: Batching \n\n3/23 \n\nWaiting time + execution time < SLO \n\nInputs waiting in queue \n\n[1] Clipper [ATC'17] \n[2] Clockwork [OSDI'20] \n[3] Nexus [SOSP'19] \n\nExecute \nBatched \ninput \n\nPrior Approach: Time-Sharing \n\n\u25aa Time-sharing: Round-based interleaved execution of batches [1] \n\n\u25aa Increase utilization by reducing idle time on GPU \n\u25aa Guarantee 2 rounds < SLO \n\n4/23 \n\n[1] Nexus [SOSP'19] \n\nWaiting batch A1,B1 \n\nTime \n\nRound 1 \n\nExec batch A1 Exec batch B1 \n\nRound 2 \nModel A \n\nModel B \n\nWaiting batch A2,B2 \n\n< SLO \n\n\u25aa Increase utilization by reducing idle time on GPU \n\u25aa Guarantee 2 rounds < SLO \n\n5/23 \n\n[1] Nexus [SOSP'19] \n\nWaiting batch A1,B1 \n\nTime \n\nRound 1 \n\nExec batch A1 Exec batch B1 \n\nRound 2 \nModel A \n\nModel B \n\nWaiting batch A2,B2 \n\n< SLO \n\nBatching and time-sharing inference, \nunderutilize GPUs \n\nProblem with prior approaches \n\n\u25aa Measured latency vs. computing resources w/ varying batch size \n\nUnderutilized Resources \n\n6/23 \n\nDiminishing return \nbeyond 40% \n\n1.3x \n\n1.7x \n\n2.2x \n\n1.4x \n\nOpportunities for improving performance \nwith better resource utilization \n\nLittle improvement in \nsmaller batch sizes \n\nNew Opportunity: Spatio-temporal Scheduling \n\n7/23 \n\n\u25aa Spatio-temporal scheduling: \n\n\u25aa Schedule tasks with batching, time-sharing, and spatial sharing \n\nBetter utilization \u2794 Improved throughput \n\nBatching \n\nTime-sharing \n\nSpatial sharing \n\nPrior approach \nSpatio-temporal \nscheduling \n\nResource \nutilization \n\nTime \n\nResource \nutilization \n\nTime \n\nScheduling \nsearch space \n\n8/23 \n\nGpulet 1 \n\nNew Abstraction: Gpulet \n\nTime \n\nGpulet 2 \n\n[1] Nexus [SOSP'19] \n\nPartition size \n\nDuration of Gpulet decided \nby squishy bin-packing [1] \n\nReserved computational \nresource \n\nGpulet 1 \nGpulet 2 \nGpulet 1 \nGpulet 2 \n\nBackend \nServer \n\nOverview of Gpulet Scheduling Framework \n\n9/23 \n\nFrontend Server \nBackend Servers \n\nGpulet \nScheduler \n\nRequest \nQueues \n\nProfiled latency, SLO \n\nSpatio-temporal \nScheduling \n\nGpulet \nGpulet \nGpulet \nGpulet \n\nGPU 1 \nGPU 2 \n\nExecute \n\nDesign Overview of Gpulet Scheduler \n\n10/23 \n\nCost-effective \nscheduling \n\nDynamic \nreorganization \n\nScheduling event \nMaximize \nPerformance \n\nMinimize \nResource Usage \n\nReorganizing Resources \n\nInterference \nprediction \n\nModel \nA \n\nShared Resources \n\nModel \nB \n\nPlease refer to the paper! \n\nDesign Overview of Gpulet Scheduler \n\n11/23 \n\nCost-effective \nscheduling \n\nDynamic \nreorganization \n\nScheduling event \nMaximize \nPerformance \n\nMinimize \nResource Usage \n\nReorganizing Resources \n\nInterference \nprediction \n\nModel \nA \n\nShared Resources \n\nModel \nB \n\nPlease refer to the paper! \n\n\u25aa Challenge: Large search space for spatial scheduling \n\n\u25aa P spatial partitioning choices for N GPUs: \ncases to search exhaustively \n\n\u25aa Main idea: Allocate partitions to GPUs incrementally \n\nScheduling Gpulets \n\n12/23 \n\nGpulet \nGpulet \n\nGpulet \nGpulet \n\nQ) How to find cost-effective partitions? \n\nGpulet \n\nGpulet \n\nGpulet \nGpulet \n\nGpulet \n\ncost-effective partition \n\nTry \n\nGpulet \n\nGpulet \nGpulet \n\nAllocate \n\n\u2026 \n\nGpulet \nGpulet \n\nGpulet \n\nEfficient than \nexhaustive search \n\nCase 1 \nCase 2 \nCase \n\n\u25aa Cost-effective: Maximum performance / resource \n\n\u25aa Cost-effective partition size (resource) = starting point of diminishing return \n\u25aa Performance is not linearly proportional to resource \n\u25aa Example) GoogLeNet \n\nCost-effective Partition \n\n13/23 \n\n\n\nEvaluated Benchmarks \u25aa\nBenchmarksTwo multi-model applications \u25aa game: image/digit recognition \u25aa traffic: camera footage analysis \u25aa Five multi-model scenarios\u25aa Composed 5 group of models \nby memory footprint size \n\n18/23 \n\nName \nNumber of models by size \n(small : medium : large) \n\nscen1 \n2 : 2 : 0 \n\nscen2 \n0 : 1 : 1 \n\nscen3 \n1 : 1 : 1 \n\nscen4 \n1 : 2 : 0 \n\nscen5 \n1 : 2: 1 \n\n", "annotations": {"author": "[{\"end\":114,\"start\":99},{\"end\":125,\"start\":115},{\"end\":138,\"start\":126},{\"end\":151,\"start\":139},{\"end\":166,\"start\":152},{\"end\":179,\"start\":167}]", "publisher": null, "author_last_name": "[{\"end\":113,\"start\":109},{\"end\":124,\"start\":121},{\"end\":137,\"start\":134},{\"end\":150,\"start\":146},{\"end\":165,\"start\":161},{\"end\":178,\"start\":175}]", "author_first_name": "[{\"end\":108,\"start\":99},{\"end\":120,\"start\":115},{\"end\":133,\"start\":126},{\"end\":145,\"start\":139},{\"end\":160,\"start\":152},{\"end\":174,\"start\":167}]", "author_affiliation": null, "title": "[{\"end\":96,\"start\":1},{\"end\":275,\"start\":180}]", "venue": null, "abstract": null, "bib_ref": null, "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":867,\"start\":712},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":4415,\"start\":868},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":4791,\"start\":4416}]", "paragraph": "[{\"end\":563,\"start\":279},{\"end\":644,\"start\":591},{\"end\":653,\"start\":646}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":712,\"start\":654}]", "table_ref": null, "section_header": "[{\"end\":589,\"start\":566},{\"end\":714,\"start\":713},{\"end\":4439,\"start\":4417}]", "table": "[{\"end\":4415,\"start\":1080},{\"end\":4791,\"start\":4574}]", "figure_caption": "[{\"end\":867,\"start\":715},{\"end\":1080,\"start\":870},{\"end\":4574,\"start\":4450}]", "figure_ref": null, "bib_author_first_name": null, "bib_author_last_name": null, "bib_entry": null, "bib_title": null, "bib_author": null, "bib_venue": null}}}, "year": 2023, "month": 12, "day": 17}