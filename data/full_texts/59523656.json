{"id": 59523656, "updated": "2023-10-02 17:02:49.645", "metadata": {"title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks", "authors": "[{\"first\":\"Jason\",\"last\":\"Wei\",\"middle\":[]},{\"first\":\"Kai\",\"last\":\"Zou\",\"middle\":[]}]", "venue": "EMNLP", "journal": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1901.11196", "mag": "2971296908", "acl": "D19-1670", "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/WeiZ19", "doi": "10.18653/v1/d19-1670"}}, "content": {"source": {"pdf_hash": "1bc383650c164c688920cd529fbdd05d6abfaf56", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/D19-1670.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/D19-1670.pdf", "status": "HYBRID"}}, "grobid": {"id": "7ec004d73d8f04df8fd62ca2064526c085aac940", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1bc383650c164c688920cd529fbdd05d6abfaf56.txt", "contents": "\nEDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks\nNovember 3-7, 2019\n\nJason Wei jason.20@dartmouth.edu \nProtago Labs Research\nTysons Corner\nVirginiaUSA\n\nDepartment of Computer Science\nDartmouth College\n\n\nKai Zou \nDepartment of Mathematics and Statistics\nGeorgetown University\n\n\nEDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks\n\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing\nthe 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaNovember 3-7, 20196382\nWe present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.\n\nIntroduction\n\nText classification is a fundamental task in natural language processing (NLP). Machine learning and deep learning have achieved high accuracy on tasks ranging from sentiment analysis (Tang et al., 2015) to topic classification (Tong and Koller, 2002), but high performance often depends on the size and quality of training data, which is often tedious to collect. Automatic data augmentation is commonly used in computer vision (Simard et al., 1998;Szegedy et al., 2014;Krizhevsky et al., 2017) and speech (Cui et al., 2015;Ko et al., 2015) and can help train more robust models, particularly when using smaller datasets. However, because it is challenging to come up with generalized rules for language transformation, universal data augmentation techniques in NLP have not been thoroughly explored.\n\nPrevious work has proposed some techniques for data augmentation in NLP. One popular study generated new data by translating sentences into French and back into English (Yu et al., 2018). Other work has used data noising as smoothing   (Xie et al., 2017) and predictive language models for synonym replacement (Kobayashi, 2018). Although these techniques are valid, they are not often used in practice because they have a high cost of implementation relative to performance gain.\n\nIn this paper, we present a simple set of universal data augmentation techniques for NLP called EDA (easy data augmentation). To the best of our knowledge, we are the first to comprehensively explore text editing techniques for data augmentation. We systematically evaluate EDA on five benchmark classification tasks, showing that EDA provides substantial improvements on all five tasks and is particularly helpful for smaller datasets. Code is publicly available at http://github. com/jasonwei20/eda_nlp.\n\n\nEDA\n\nFrustrated by the measly performance of text classifiers trained on small datasets, we tested a number of augmentation operations loosely inspired by those used in computer vision and found that they helped train more robust models. Here, we present the full details of EDA. For a given sentence in the training set, we randomly choose and perform one of the following operations:\n\n1. Synonym Replacement (SR): Randomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random.\n\n\nRandom Insertion (RI):\n\nFind a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random position in the sentence. Do this n times. Since long sentences have more words than short ones, they can absorb more noise while maintaining their original class label. To compensate, we vary the number of words changed, n, for SR, RI, and RS based on the sentence length l with the formula n=\u03b1 l, where \u03b1 is a parameter that indicates the percent of the words in a sentence are changed (we use p=\u03b1 for RD). Furthermore, for each original sentence, we generate n aug augmented sentences. Examples of augmented sentences are shown in Table 1. We note that synonym replacement has been used previously (Kolomiyets et al., 2011;Wang and Yang, 2015), but to our knowledge, random insertions, swaps, and deletions have not been extensively studied.\n\n\nExperimental Setup\n\nWe choose five benchmark text classification tasks and two network architectures to evaluate EDA.\n\n\nBenchmark Datasets\n\nWe conduct experiments on five benchmark text classification tasks: (1) SST-2: Stanford Sentiment Treebank (Socher et al., 2013), (2) CR: customer reviews (Hu and Liu, 2004;, (3) SUBJ: subjectivity/objectivity dataset (Pang and Lee, 2004), (4) TREC: question type dataset (Li and Roth, 2002), and (5) PC: Pro-Con dataset (Ganapathibhotla and Liu, 2008). Summary statistics are shown in Table 5 in Supplemental Materials. Furthermore, we hypothesize that EDA is more helpful for smaller datasets, so we delegate the following sized datasets by selecting a random subset of the full training set with N train ={500, 2,000, 5,000, all available data}.\n\n\nText Classification Models\n\nWe run experiments for two popular models in text classification.\n\n(1) Recurrent neural networks (RNNs) are suitable for sequential data. We use a LSTM-RNN (Liu et al., 2016).\n\n(2) Convolutional neural networks (CNNs) have also achieved high performance for text classification. We implement them as described in (Kim, 2014). Details are in Section 9.1 in Supplementary Materials.\n\n\nResults\n\nIn this section, we test EDA on five NLP tasks with CNNs and RNNs. For all experiments, we average results from five different random seeds.\n\n\nEDA Makes Gains\n\nWe run both CNN and RNN models with and without EDA across all five datasets for varying training set sizes. Average performances (%) are shown in Table 2. Of note, average improvement was 0.8% for full datasets and 3.0% for N train =500.  \n\n\nTraining Set Sizing\n\nOverfitting tends to be more severe when training on smaller datasets. By conducting experiments using a restricted fraction of the available training data, we show that EDA has more significant improvements for smaller training sets. We run both normal training and EDA training for the following training set fractions (%): {1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100}. Figure 1(a)-(e) shows performance with and without EDA for each dataset, and 1(f) shows the averaged performance across all datasets. The best average accuracy without augmentation, 88.3%, was achieved using 100% of the training data. Models trained using EDA surpassed this number by achieving an average accuracy of 88.6% while only using 50% of the available training data.\n\n\nDoes EDA conserve true labels?\n\nIn data augmentation, input data is altered while class labels are maintained. If sentences are significantly changed, however, then original class labels may no longer be valid. We take a visualization approach to examine whether EDA operations significantly change the meanings of augmented sentences. First, we train an RNN on the pro-con classification task (PC) without augmentation. Then, we apply EDA to the test set by generating nine augmented sentences per original sentence. These are fed into the RNN along with the original sentences, and we extract the outputs from the last dense layer. We apply t-SNE (Van Der Maaten, 2014) to these vectors and plot their 2-D representations (Figure 2). We found that the resulting latent space representations for augmented sentences closely surrounded those of the original sentences, which suggests that for the most part, sentences augmented with EDA conserved the labels of their original sentences.\n\n\nAblation Study: EDA Decomposed\n\nSo far, we have seen encouraging empirical results. In this section, we perform an ablation study Pro (original) Pro (EDA) Con (original) Con (EDA) Figure 2: Latent space visualization of original and augmented sentences in the Pro-Con dataset. Augmented sentences (small triangles and circles) closely surround original sentences (big triangles and circles) of the same color, suggesting that augmented sentences maintianed their true class labels.\n\nto explore the effects of each operation in EDA. Synonym replacement has been previously used (Kolomiyets et al., 2011;Wang and Yang, 2015), but the other three EDA operations have not yet been explored. One could hypothesize that the bulk of EDA's performance gain is from synonym replacement, so we isolate each of the EDA operations to determine their individual ability to boost performance. For all four operations, we ran models using a single oper- It turns out that all four EDA operations contribute to performance gain. For SR, improvement was good for small \u03b1, but high \u03b1 hurt performance, likely because replacing too many words in a sentence changed the identity of the sentence. For RI, performance gains were more stable for different \u03b1 values, possibly because the original words in the sentence and their relative order were maintained in this operation. RS yielded high performance gains at \u03b1\u22640.2, but declined at \u03b1\u22650.3 since performing too many swaps is equivalent to shuffling the entire order of the sentence. RD had the highest gains for low \u03b1 but severely hurt performance at high \u03b1, as sentences are likely unintelligible if up to half the words are removed. Improvements were more substantial on smaller datasets for all operations, and \u03b1=0.1 appeared to be a \"sweet spot\" across the board.\n\n\nHow much augmentation?\n\nThe natural next step is to determine how the number of generated augmented sentences per original sentence, n aug , affects performance. In Figure 4, we show average performances over all datasets for n aug ={1, 2, 4, 8, 16, 32}. For smaller train- ing sets, overfitting was more likely, so generating many augmented sentences yielded large performance boosts. For larger training sets, adding more than four augmented sentences per original sentence was unhelpful since models tend to generalize properly when large quantities of real data are available. Based on these results, we recommend usage parameters in Table 3.  \n\n\nComparison with Related Work\n\nRelated work is creative but often complex. Backtranslation (Sennrich et al., 2016), translational data augmentation (Fadaee et al., 2017), and noising (Xie et al., 2017) have shown improvements in BLEU measure for machine translation. For other tasks, previous approaches include task-specific heuristics (Kafle et al., 2017) and back-translation (Silfverberg et al., 2017;Yu et al., 2018). Regarding synonym replacement (SR), one study showed a 1.4% F1-score boost for tweet classification by finding synonyms with k-nearest neighbors using word embeddings (Wang and Yang, 2015).\n\nAnother study found no improvement in temporal analysis when replacing headwords with synonyms (Kolomiyets et al., 2011), and mixed results were reported for using SR in character-level text classification ; however, neither work conducted extensive ablation studies.\n\nMost studies explore data augmentation as a complementary result for translation or in a taskspecific context, so it is hard to directly compare EDA with previous literature. But there are two studies similar to ours that evaluate augmentation techniques on multiple datasets. Hu (2017) proposed a generative model that combines a variational auto-encoder (VAE) and attribute discriminator to generate fake data, demonstrating a 3% gain in accuracy on two datasets. Kobayashi (2018) showed that replacing words with other words that were predicted from the sentence context using a bi-directional language model yielded a 0.5% gain on five datasets. However, training a variational auto-encoder or bidirectional LSTM language model is a lot of work. EDA yields results on the same order of magnitude but is much easier to use because it does not require training a language model and does not use external datasets. In Table 4 (Kolomiyets et al., 2011) for temporal analysis 7 (Kobayashi, 2018) for text classification 8 (Wang and Yang, 2015) for tweet classification 9 EDA does use a synonym dictionary, WordNet, but the cost of downloading it is far less than training a model on an external dataset, so we don't count it as an \"external dataset.\" recent years, we suspect that researchers will soon find higher-performing augmentation techniques that will also be easy to use.\n\nNotably, much of the recent work in NLP focuses on making neural models larger or more complex. Our work, however, takes the opposite approach. We introduce simple operations, the result of asking the fundamental question, how can we generate sentences for augmentation without changing their true labels? We do not expect EDA to be the go-to augmentation method for NLP, either now or in the future. Rather, we hope that our line of thought might inspire new approaches for universal or task-specific data augmentation. Now, let's note many of EDA's limitations. Foremost, performance gain can be marginal when data is sufficient; for our five classification tasks, the average performance gain for was less than 1% when training with full datasets. And while performance gains seem clear for small datasets, EDA might not yield substantial improvements when using pre-trained models. One study found that EDA's improvement was negligible when using ULMFit (Shleifer, 2019), and we expect similar results for ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018). Finally, although we evaluate on five benchmark datasets, other studies on data augmentation in NLP use different models and datasets, and so fair comparison with related work is highly non-trivial.\n\n\nConclusions\n\nWe have shown that simple data augmentation operations can boost performance on text classification tasks. Although improvement is at times marginal, EDA substantially boosts performance and reduces overfitting when training on smaller datasets. Continued work on this topic could explore the theoretical underpinning of the EDA operations. We hope that EDA's simplicity makes a compelling case for further thought.\n\n3 .\n3Random Swap (RS): Randomly choose two words in the sentence and swap their positions. Do this n times. 4. Random Deletion (RD): Randomly remove each word in the sentence with probability p.\n\nFigure 1 :\n1Performance on benchmark text classification tasks with and without EDA, for various dataset sizes used for training. For reference, the dotted grey line indicates best performances fromKim (2014) forSST-2, CR,  SUBJ, and TREC, and Ganapathibhotla (2008)  for PC.\n\nFigure 3 :\n3Average performance gain of EDA operations over five text classification tasks for different training set sizes. The \u03b1 parameter roughly means \"percent of words in sentence changed by each augmentation.\" SR: synonym replacement. RI: random insertion. RS: random swap. RD: random deletion. ation while varying the augmentation parameter \u03b1={0.05, 0.1, 0.2, 0.3, 0.4, 0.5}(Figure 3).\n\nFigure 4 :\n4Average performance gain of EDA across five text classification tasks for various training set sizes. n aug is the number of generated augmented sentences per original sentence.\n\nTable 1 :\n1Sentences generated using EDA. SR: synonym replacement. RI: random insertion. RS: random swap. RD: random deletion.\n\nTable 2 :\n2Average performances (%) across five text classification tasks for models with and without EDA on different training set sizes.\n\nTable 3 :\n3Recommended usage parameters.\n\n\n, we show EDA's ease of use compared with other techniques.Technique (#datasets) LM Ex Dat \nTrans. data aug. 1 (1) \nyes \nyes \nBack-translation 2 (1) yes \nyes \nVAE + discrim. 3 (2) \nyes \nyes \nNoising 4 (1) \nyes \nno \nBack-translation 5 (2) yes \nno \nLM + SR 6 (2) \nyes \nno \nContextual aug. 7 (5) \nyes \nno \nSR -kNN 8 (1) \nno \nno \nEDA (5) \nno \nno \n\n\n\nTable 4 :\n4Related work in data augmentation. #datasets: number of datasets used for evaluation. Gain: reported performance gain on all evaluation datasets. LM: requires training a language model or deep learning. Ex Dat: requires an external dataset. 9Our paper aimed to address the lack of standardized data augmentation in NLP (compared to vision) by introducing a set of simple operations that might serve as a baseline for future investigation. With the rate that NLP research has progressed in6 Discussion and Limitations \n\n1 (Fadaee et al., 2017) for translation \n2 (Yu et al., 2018) for comprehension \n3 (Hu et al., 2017) for text classification \n4 (Xie et al., 2017) for translation \n5 (Sennrich et al., 2016) for translation \n6 \nAcknowledgementsWe thank Chengyu Huang, Fei Xing, and Yifang Wei for help with study design and paper revisions, and Chunxiao Zhou for insightful feedback. Jason Wei thanks Eugene Santos for inspiration.\nData augmentation for deep neural network acoustic modeling. Xiaodong Cui, Vaibhava Goel, Brian Kingsbury, 10.1109/TASLP.2015.2438544IEEE/ACM Trans. Audio, Speech and Lang. Proc. 239Xiaodong Cui, Vaibhava Goel, and Brian Kingsbury. 2015. Data augmentation for deep neural net- work acoustic modeling. IEEE/ACM Trans. Audio, Speech and Lang. Proc., 23(9):1469-1477.\n\nBERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, abs/1810.04805CoRRJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirectional transformers for language under- standing. CoRR, abs/1810.04805.\n\nData augmentation for low-resource neural machine translation. Marzieh Fadaee, Arianna Bisazza, Christof Monz, 10.18653/v1/P17-2090Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2Short Papers)Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017. Data augmentation for low-resource neural machine translation. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 567- 573. Association for Computational Linguistics.\n\nMining opinions in comparative sentences. Murthy Ganapathibhotla, Bing Liu, Proceedings of the 22Nd International Conference on Computational Linguistics. the 22Nd International Conference on Computational LinguisticsStroudsburg, PA, USAAssociation for Computational Linguistics1COLING '08Murthy Ganapathibhotla and Bing Liu. 2008. Mining opinions in comparative sentences. In Proceedings of the 22Nd International Conference on Computa- tional Linguistics -Volume 1, COLING '08, pages 241-248, Stroudsburg, PA, USA. Association for Computational Linguistics.\n\nMining and summarizing customer reviews. Minqing Hu, Bing Liu, 10.1145/1014052.1014073Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '04. the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '04New York, NY, USAACMMinqing Hu and Bing Liu. 2004. Mining and summa- rizing customer reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowl- edge Discovery and Data Mining, KDD '04, pages 168-177, New York, NY, USA. ACM.\n\nToward controlled generation of text. Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric P Xing, ICML. Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P. Xing. 2017. Toward con- trolled generation of text. In ICML.\n\nData augmentation for visual question answering. Kushal Kafle, Mohammed Yousefhussien, Christopher Kanan, 10.18653/v1/W17-3529Proceedings of the 10th International Conference on Natural Language Generation. the 10th International Conference on Natural Language GenerationAssociation for Computational LinguisticsKushal Kafle, Mohammed Yousefhussien, and Christo- pher Kanan. 2017. Data augmentation for visual question answering. In Proceedings of the 10th In- ternational Conference on Natural Language Gen- eration, pages 198-202. Association for Computa- tional Linguistics.\n\nConvolutional neural networks for sentence classification. Yoon Kim, abs/1408.5882CoRRYoon Kim. 2014. Convolutional neural networks for sentence classification. CoRR, abs/1408.5882.\n\nAudio augmentation for speech recognition. Tom Ko, Vijayaditya Peddinti, Daniel Povey, Sanjeev Khudanpur, INTERSPEECH. Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San- jeev Khudanpur. 2015. Audio augmentation for speech recognition. In INTERSPEECH.\n\nContextual augmentation: Data augmentation by words with paradigmatic relations. Sosuke Kobayashi, NAACL-HLT. Sosuke Kobayashi. 2018. Contextual augmentation: Data augmentation by words with paradigmatic re- lations. In NAACL-HLT.\n\nModel-portability experiments for textual temporal analysis. Oleksandr Kolomiyets, Steven Bethard, Marie-Francine Moens, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers. the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short PapersStroudsburg, PA, USAAssociation for Computational Linguistics2Oleksandr Kolomiyets, Steven Bethard, and Marie- Francine Moens. 2011. Model-portability experi- ments for textual temporal analysis. In Proceed- ings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers -Volume 2, HLT '11, pages 271-276, Stroudsburg, PA, USA. Association for Computational Linguistics.\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E , 10.1145/3065386Commun. ACM. 606HintonAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin- ton. 2017. Imagenet classification with deep convo- lutional neural networks. Commun. ACM, 60(6):84- 90.\n\nLearning question classifiers. Xin Li, Dan Roth, 10.3115/1072228.1072378Proceedings of the 19th International Conference on Computational Linguistics. the 19th International Conference on Computational LinguisticsStroudsburg, PA, USAAssociation for Computational Linguistics1COLING '02Xin Li and Dan Roth. 2002. Learning question classi- fiers. In Proceedings of the 19th International Con- ference on Computational Linguistics -Volume 1, COLING '02, pages 1-7, Stroudsburg, PA, USA. Association for Computational Linguistics.\n\nRecurrent neural network for text classification with multi-task learning. Pengfei Liu, Xipeng Qiu, Xuanjing Huang, Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI'16. the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI'16AAAI PressPengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016. Recurrent neural network for text classification with multi-task learning. In Proceedings of the Twenty- Fifth International Joint Conference on Artificial Intelligence, IJCAI'16, pages 2873-2879. AAAI Press.\n\nAutomated rule selection for aspect extraction in opinion mining. Qian Liu, Zhiqiang Gao, Bing Liu, Yuanlin Zhang, Proceedings of the 24th International Conference on Artificial Intelligence, IJCAI'15. the 24th International Conference on Artificial Intelligence, IJCAI'15AAAI PressQian Liu, Zhiqiang Gao, Bing Liu, and Yuanlin Zhang. 2015. Automated rule selection for aspect extrac- tion in opinion mining. In Proceedings of the 24th International Conference on Artificial Intelligence, IJCAI'15, pages 1291-1297. AAAI Press.\n\nWordnet: A lexical database for english. A George, Miller, 10.1145/219717.219748Commun. ACM. 3811George A. Miller. 1995. Wordnet: A lexical database for english. Commun. ACM, 38(11):39-41.\n\nA sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. Bo Pang, Lillian Lee, 10.3115/1218955.1218990Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics, ACL '04. the 42Nd Annual Meeting on Association for Computational Linguistics, ACL '04Stroudsburg, PA, USAAssociation for Computational LinguisticsBo Pang and Lillian Lee. 2004. A sentimental educa- tion: Sentiment analysis using subjectivity summa- rization based on minimum cuts. In Proceedings of the 42Nd Annual Meeting on Association for Com- putational Linguistics, ACL '04, Stroudsburg, PA, USA. Association for Computational Linguistics.\n\nGlove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, Empirical Methods in Natural Language Processing (EMNLP). Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Nat- ural Language Processing (EMNLP), pages 1532- 1543.\n\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, abs/1802.05365Deep contextualized word representations. CoRR. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. CoRR, abs/1802.05365.\n\nDeep learning is robust to massive label noise. David Rolnick, Andreas Veit, Serge J Belongie, Nir Shavit, abs/1705.10694CoRRDavid Rolnick, Andreas Veit, Serge J. Belongie, and Nir Shavit. 2017. Deep learning is robust to massive label noise. CoRR, abs/1705.10694.\n\nImproving neural machine translation models with monolingual data. Rico Sennrich, Barry Haddow, Alexandra Birch, 10.18653/v1/P16-1009Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics1Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural machine translation mod- els with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 86-96. Association for Computational Linguistics.\n\nLow resource text classification with ulmfit and backtranslation. Sam Shleifer, abs/1903.09244CoRRSam Shleifer. 2019. Low resource text classifi- cation with ulmfit and backtranslation. CoRR, abs/1903.09244.\n\nData augmentation for morphological reinflection. Miikka Silfverberg, Adam Wiemerslage, Ling Liu, Lingshuang Jack Mao, 10.18653/v1/K17-2010Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection. the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological ReinflectionAssociation for Computational LinguisticsMiikka Silfverberg, Adam Wiemerslage, Ling Liu, and Lingshuang Jack Mao. 2017. Data augmentation for morphological reinflection. In Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Univer- sal Morphological Reinflection, pages 90-99. Asso- ciation for Computational Linguistics.\n\nTransformation invariance in pattern recognition-tangent distance and tangent propagation. Patrice Simard, Yann Lecun, John S Denker, Bernard Victorri, Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop. London, UK, UKSpringer-VerlagPatrice Simard, Yann LeCun, John S. Denker, and Bernard Victorri. 1998. Transformation invari- ance in pattern recognition-tangent distance and tan- gent propagation. In Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop, pages 239-27, London, UK, UK. Springer-Verlag.\n\nParsing With Compositional Vector Grammars. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, Christopher Potts, EMNLP. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, and Christopher Potts. 2013. Parsing With Composi- tional Vector Grammars. In EMNLP.\n\nDumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E Reed, Dragomir Anguelov, Going deeper with convolutions. CoRR, abs/1409.4842Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser- manet, Scott E. Reed, Dragomir Anguelov, Du- mitru Erhan, Vincent Vanhoucke, and Andrew Ra- binovich. 2014. Going deeper with convolutions. CoRR, abs/1409.4842.\n\nDocument modeling with gated recurrent neural network for sentiment classification. Duyu Tang, Bing Qin, Ting Liu, 10.18653/v1/D15-1167Duyu Tang, Bing Qin, and Ting Liu. 2015. Document modeling with gated recurrent neural network for sentiment classification. pages 1422-1432.\n\nSupport vector machine active learning with applications to text classification. Simon Tong, Daphne Koller, 10.1162/153244302760185243J. Mach. Learn. Res. 2Simon Tong and Daphne Koller. 2002. Support vec- tor machine active learning with applications to text classification. J. Mach. Learn. Res., 2:45-66.\n\nAccelerating t-sne using tree-based algorithms. Laurens Van Der Maaten, J. Mach. Learn. Res. 151Laurens Van Der Maaten. 2014. Accelerating t-sne using tree-based algorithms. J. Mach. Learn. Res., 15(1):3221-3245.\n\nThat's so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using #petpeeve tweets. Yang William, Diyi Wang, Yang, 10.18653/v1/D15-1306Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsWilliam Yang Wang and Diyi Yang. 2015. That's so annoying!!!: A lexical and frame-semantic em- bedding based data augmentation approach to au- tomatic categorization of annoying behaviors using #petpeeve tweets. In Proceedings of the 2015 Con- ference on Empirical Methods in Natural Language Processing, pages 2557-2563. Association for Com- putational Linguistics.\n\nZiang Xie, Sida I Wang, Jiwei Li, Daniel Levy, Aiming Nie, Dan Jurafsky, Andrew Y Ng, Data noising as smoothing in neural network language models. Ziang Xie, Sida I. Wang, Jiwei Li, Daniel Levy, Aim- ing Nie, Dan Jurafsky, and Andrew Y. Ng. 2017. Data noising as smoothing in neural network lan- guage models.\n\n. Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, V Quoc, Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V.\n\nQanet: Combining local convolution with global self-attention for reading comprehension. Le, abs/1804.09541CoRRLe. 2018. Qanet: Combining local convolution with global self-attention for reading comprehen- sion. CoRR, abs/1804.09541.\n\nCharacter-level convolutional networks for text classification. Xiang Zhang, Junbo Zhao, Yann Lecun, Proceedings of the 28th International Conference on Neural Information Processing Systems. the 28th International Conference on Neural Information Processing SystemsCambridge, MA, USAMIT Press1NIPS'15Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text clas- sification. In Proceedings of the 28th International Conference on Neural Information Processing Sys- tems -Volume 1, NIPS'15, pages 649-657, Cam- bridge, MA, USA. MIT Press.\n", "annotations": {"author": "[{\"end\":247,\"start\":114},{\"end\":321,\"start\":248}]", "publisher": null, "author_last_name": "[{\"end\":123,\"start\":120},{\"end\":255,\"start\":252}]", "author_first_name": "[{\"end\":119,\"start\":114},{\"end\":251,\"start\":248}]", "author_affiliation": "[{\"end\":195,\"start\":148},{\"end\":246,\"start\":197},{\"end\":320,\"start\":257}]", "title": "[{\"end\":93,\"start\":1},{\"end\":414,\"start\":322}]", "venue": "[{\"end\":576,\"start\":416}]", "abstract": "[{\"end\":1446,\"start\":761}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1665,\"start\":1646},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":1713,\"start\":1690},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1912,\"start\":1891},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1933,\"start\":1912},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1957,\"start\":1933},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1987,\"start\":1969},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2003,\"start\":1987},{\"end\":2451,\"start\":2434},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2519,\"start\":2501},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2592,\"start\":2575},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4574,\"start\":4549},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4594,\"start\":4574},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4963,\"start\":4942},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5008,\"start\":4990},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5072,\"start\":5053},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5126,\"start\":5107},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5187,\"start\":5156},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5688,\"start\":5670},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5838,\"start\":5827},{\"end\":6703,\"start\":6656},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8675,\"start\":8650},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8695,\"start\":8675},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10638,\"start\":10615},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10693,\"start\":10672},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10725,\"start\":10707},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10881,\"start\":10861},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10929,\"start\":10903},{\"end\":10945,\"start\":10929},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11135,\"start\":11114},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11258,\"start\":11233},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11693,\"start\":11684},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11889,\"start\":11873},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12359,\"start\":12334},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12401,\"start\":12384},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13762,\"start\":13746},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13824,\"start\":13803},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13855,\"start\":13834},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14892,\"start\":14882}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":14682,\"start\":14487},{\"attributes\":{\"id\":\"fig_1\"},\"end\":14959,\"start\":14683},{\"attributes\":{\"id\":\"fig_2\"},\"end\":15353,\"start\":14960},{\"attributes\":{\"id\":\"fig_3\"},\"end\":15544,\"start\":15354},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":15672,\"start\":15545},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":15812,\"start\":15673},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":15854,\"start\":15813},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":16201,\"start\":15855},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":16941,\"start\":16202}]", "paragraph": "[{\"end\":2263,\"start\":1462},{\"end\":2744,\"start\":2265},{\"end\":3251,\"start\":2746},{\"end\":3639,\"start\":3259},{\"end\":3807,\"start\":3641},{\"end\":4692,\"start\":3834},{\"end\":4812,\"start\":4715},{\"end\":5483,\"start\":4835},{\"end\":5579,\"start\":5514},{\"end\":5689,\"start\":5581},{\"end\":5894,\"start\":5691},{\"end\":6046,\"start\":5906},{\"end\":6306,\"start\":6066},{\"end\":7081,\"start\":6330},{\"end\":8070,\"start\":7116},{\"end\":8554,\"start\":8105},{\"end\":9871,\"start\":8556},{\"end\":10522,\"start\":9898},{\"end\":11136,\"start\":10555},{\"end\":11405,\"start\":11138},{\"end\":12786,\"start\":11407},{\"end\":14055,\"start\":12788},{\"end\":14486,\"start\":14071}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":4489,\"start\":4482},{\"end\":5228,\"start\":5221},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":6220,\"start\":6213},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":10519,\"start\":10512},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":12333,\"start\":12326}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1460,\"start\":1448},{\"attributes\":{\"n\":\"2\"},\"end\":3257,\"start\":3254},{\"attributes\":{\"n\":\"2.\"},\"end\":3832,\"start\":3810},{\"attributes\":{\"n\":\"3\"},\"end\":4713,\"start\":4695},{\"attributes\":{\"n\":\"3.1\"},\"end\":4833,\"start\":4815},{\"attributes\":{\"n\":\"3.2\"},\"end\":5512,\"start\":5486},{\"attributes\":{\"n\":\"4\"},\"end\":5904,\"start\":5897},{\"attributes\":{\"n\":\"4.1\"},\"end\":6064,\"start\":6049},{\"attributes\":{\"n\":\"4.2\"},\"end\":6328,\"start\":6309},{\"attributes\":{\"n\":\"4.3\"},\"end\":7114,\"start\":7084},{\"attributes\":{\"n\":\"4.4\"},\"end\":8103,\"start\":8073},{\"attributes\":{\"n\":\"4.5\"},\"end\":9896,\"start\":9874},{\"attributes\":{\"n\":\"5\"},\"end\":10553,\"start\":10525},{\"attributes\":{\"n\":\"7\"},\"end\":14069,\"start\":14058},{\"end\":14491,\"start\":14488},{\"end\":14694,\"start\":14684},{\"end\":14971,\"start\":14961},{\"end\":15365,\"start\":15355},{\"end\":15555,\"start\":15546},{\"end\":15683,\"start\":15674},{\"end\":15823,\"start\":15814},{\"end\":16212,\"start\":16203}]", "table": "[{\"end\":16201,\"start\":15916},{\"end\":16941,\"start\":16702}]", "figure_caption": "[{\"end\":14682,\"start\":14493},{\"end\":14959,\"start\":14696},{\"end\":15353,\"start\":14973},{\"end\":15544,\"start\":15367},{\"end\":15672,\"start\":15557},{\"end\":15812,\"start\":15685},{\"end\":15854,\"start\":15825},{\"end\":15916,\"start\":15857},{\"end\":16702,\"start\":16214}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6713,\"start\":6705},{\"end\":7817,\"start\":7808},{\"end\":8261,\"start\":8253},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":10047,\"start\":10039}]", "bib_author_first_name": "[{\"end\":17215,\"start\":17207},{\"end\":17229,\"start\":17221},{\"end\":17241,\"start\":17236},{\"end\":17599,\"start\":17594},{\"end\":17616,\"start\":17608},{\"end\":17630,\"start\":17624},{\"end\":17644,\"start\":17636},{\"end\":17923,\"start\":17916},{\"end\":17939,\"start\":17932},{\"end\":17957,\"start\":17949},{\"end\":18548,\"start\":18542},{\"end\":18570,\"start\":18566},{\"end\":19109,\"start\":19102},{\"end\":19118,\"start\":19114},{\"end\":19644,\"start\":19637},{\"end\":19655,\"start\":19649},{\"end\":19669,\"start\":19662},{\"end\":19683,\"start\":19677},{\"end\":19703,\"start\":19699},{\"end\":19705,\"start\":19704},{\"end\":19909,\"start\":19903},{\"end\":19925,\"start\":19917},{\"end\":19952,\"start\":19941},{\"end\":20496,\"start\":20492},{\"end\":20662,\"start\":20659},{\"end\":20678,\"start\":20667},{\"end\":20695,\"start\":20689},{\"end\":20710,\"start\":20703},{\"end\":20957,\"start\":20951},{\"end\":21172,\"start\":21163},{\"end\":21191,\"start\":21185},{\"end\":21215,\"start\":21201},{\"end\":21972,\"start\":21968},{\"end\":21989,\"start\":21985},{\"end\":22009,\"start\":22001},{\"end\":22011,\"start\":22010},{\"end\":22245,\"start\":22242},{\"end\":22253,\"start\":22250},{\"end\":22821,\"start\":22814},{\"end\":22833,\"start\":22827},{\"end\":22847,\"start\":22839},{\"end\":23381,\"start\":23377},{\"end\":23395,\"start\":23387},{\"end\":23405,\"start\":23401},{\"end\":23418,\"start\":23411},{\"end\":23882,\"start\":23881},{\"end\":24132,\"start\":24130},{\"end\":24146,\"start\":24139},{\"end\":24761,\"start\":24754},{\"end\":24781,\"start\":24774},{\"end\":24801,\"start\":24790},{\"end\":24803,\"start\":24802},{\"end\":25079,\"start\":25072},{\"end\":25081,\"start\":25080},{\"end\":25094,\"start\":25090},{\"end\":25109,\"start\":25104},{\"end\":25121,\"start\":25117},{\"end\":25142,\"start\":25131},{\"end\":25156,\"start\":25150},{\"end\":25166,\"start\":25162},{\"end\":25481,\"start\":25476},{\"end\":25498,\"start\":25491},{\"end\":25510,\"start\":25505},{\"end\":25512,\"start\":25511},{\"end\":25526,\"start\":25523},{\"end\":25765,\"start\":25761},{\"end\":25781,\"start\":25776},{\"end\":25799,\"start\":25790},{\"end\":26399,\"start\":26396},{\"end\":26595,\"start\":26589},{\"end\":26613,\"start\":26609},{\"end\":26631,\"start\":26627},{\"end\":26652,\"start\":26637},{\"end\":27269,\"start\":27262},{\"end\":27282,\"start\":27278},{\"end\":27294,\"start\":27290},{\"end\":27296,\"start\":27295},{\"end\":27312,\"start\":27305},{\"end\":27799,\"start\":27792},{\"end\":27812,\"start\":27808},{\"end\":27828,\"start\":27824},{\"end\":27838,\"start\":27833},{\"end\":27858,\"start\":27847},{\"end\":27874,\"start\":27868},{\"end\":27890,\"start\":27879},{\"end\":28144,\"start\":28135},{\"end\":28157,\"start\":28154},{\"end\":28171,\"start\":28163},{\"end\":28183,\"start\":28177},{\"end\":28199,\"start\":28194},{\"end\":28201,\"start\":28200},{\"end\":28216,\"start\":28208},{\"end\":28582,\"start\":28578},{\"end\":28593,\"start\":28589},{\"end\":28603,\"start\":28599},{\"end\":28858,\"start\":28853},{\"end\":28871,\"start\":28865},{\"end\":29134,\"start\":29127},{\"end\":29466,\"start\":29462},{\"end\":29480,\"start\":29476},{\"end\":30086,\"start\":30081},{\"end\":30096,\"start\":30092},{\"end\":30098,\"start\":30097},{\"end\":30110,\"start\":30105},{\"end\":30121,\"start\":30115},{\"end\":30134,\"start\":30128},{\"end\":30143,\"start\":30140},{\"end\":30160,\"start\":30154},{\"end\":30162,\"start\":30161},{\"end\":30399,\"start\":30394},{\"end\":30403,\"start\":30400},{\"end\":30413,\"start\":30408},{\"end\":30431,\"start\":30421},{\"end\":30442,\"start\":30439},{\"end\":30452,\"start\":30449},{\"end\":30467,\"start\":30459},{\"end\":30478,\"start\":30477},{\"end\":30885,\"start\":30880},{\"end\":30898,\"start\":30893},{\"end\":30909,\"start\":30905}]", "bib_author_last_name": "[{\"end\":17219,\"start\":17216},{\"end\":17234,\"start\":17230},{\"end\":17251,\"start\":17242},{\"end\":17606,\"start\":17600},{\"end\":17622,\"start\":17617},{\"end\":17634,\"start\":17631},{\"end\":17654,\"start\":17645},{\"end\":17930,\"start\":17924},{\"end\":17947,\"start\":17940},{\"end\":17962,\"start\":17958},{\"end\":18564,\"start\":18549},{\"end\":18574,\"start\":18571},{\"end\":19112,\"start\":19110},{\"end\":19122,\"start\":19119},{\"end\":19647,\"start\":19645},{\"end\":19660,\"start\":19656},{\"end\":19675,\"start\":19670},{\"end\":19697,\"start\":19684},{\"end\":19710,\"start\":19706},{\"end\":19915,\"start\":19910},{\"end\":19939,\"start\":19926},{\"end\":19958,\"start\":19953},{\"end\":20500,\"start\":20497},{\"end\":20665,\"start\":20663},{\"end\":20687,\"start\":20679},{\"end\":20701,\"start\":20696},{\"end\":20720,\"start\":20711},{\"end\":20967,\"start\":20958},{\"end\":21183,\"start\":21173},{\"end\":21199,\"start\":21192},{\"end\":21221,\"start\":21216},{\"end\":21983,\"start\":21973},{\"end\":21999,\"start\":21990},{\"end\":22248,\"start\":22246},{\"end\":22258,\"start\":22254},{\"end\":22825,\"start\":22822},{\"end\":22837,\"start\":22834},{\"end\":22853,\"start\":22848},{\"end\":23385,\"start\":23382},{\"end\":23399,\"start\":23396},{\"end\":23409,\"start\":23406},{\"end\":23424,\"start\":23419},{\"end\":23889,\"start\":23883},{\"end\":23897,\"start\":23891},{\"end\":24137,\"start\":24133},{\"end\":24150,\"start\":24147},{\"end\":24772,\"start\":24762},{\"end\":24788,\"start\":24782},{\"end\":24811,\"start\":24804},{\"end\":25088,\"start\":25082},{\"end\":25102,\"start\":25095},{\"end\":25115,\"start\":25110},{\"end\":25129,\"start\":25122},{\"end\":25148,\"start\":25143},{\"end\":25160,\"start\":25157},{\"end\":25178,\"start\":25167},{\"end\":25489,\"start\":25482},{\"end\":25503,\"start\":25499},{\"end\":25521,\"start\":25513},{\"end\":25533,\"start\":25527},{\"end\":25774,\"start\":25766},{\"end\":25788,\"start\":25782},{\"end\":25805,\"start\":25800},{\"end\":26408,\"start\":26400},{\"end\":26607,\"start\":26596},{\"end\":26625,\"start\":26614},{\"end\":26635,\"start\":26632},{\"end\":26656,\"start\":26653},{\"end\":27276,\"start\":27270},{\"end\":27288,\"start\":27283},{\"end\":27303,\"start\":27297},{\"end\":27321,\"start\":27313},{\"end\":27806,\"start\":27800},{\"end\":27822,\"start\":27813},{\"end\":27831,\"start\":27829},{\"end\":27845,\"start\":27839},{\"end\":27866,\"start\":27859},{\"end\":27877,\"start\":27875},{\"end\":27896,\"start\":27891},{\"end\":28152,\"start\":28145},{\"end\":28161,\"start\":28158},{\"end\":28175,\"start\":28172},{\"end\":28192,\"start\":28184},{\"end\":28206,\"start\":28202},{\"end\":28225,\"start\":28217},{\"end\":28587,\"start\":28583},{\"end\":28597,\"start\":28594},{\"end\":28607,\"start\":28604},{\"end\":28863,\"start\":28859},{\"end\":28878,\"start\":28872},{\"end\":29149,\"start\":29135},{\"end\":29474,\"start\":29467},{\"end\":29485,\"start\":29481},{\"end\":29491,\"start\":29487},{\"end\":30090,\"start\":30087},{\"end\":30103,\"start\":30099},{\"end\":30113,\"start\":30111},{\"end\":30126,\"start\":30122},{\"end\":30138,\"start\":30135},{\"end\":30152,\"start\":30144},{\"end\":30165,\"start\":30163},{\"end\":30406,\"start\":30404},{\"end\":30419,\"start\":30414},{\"end\":30437,\"start\":30432},{\"end\":30447,\"start\":30443},{\"end\":30457,\"start\":30453},{\"end\":30475,\"start\":30468},{\"end\":30483,\"start\":30479},{\"end\":30672,\"start\":30670},{\"end\":30891,\"start\":30886},{\"end\":30903,\"start\":30899},{\"end\":30915,\"start\":30910}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1109/TASLP.2015.2438544\",\"id\":\"b0\",\"matched_paper_id\":3355461},\"end\":17510,\"start\":17146},{\"attributes\":{\"doi\":\"abs/1810.04805\",\"id\":\"b1\"},\"end\":17851,\"start\":17512},{\"attributes\":{\"doi\":\"10.18653/v1/P17-2090\",\"id\":\"b2\",\"matched_paper_id\":3291104},\"end\":18498,\"start\":17853},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":8985962},\"end\":19059,\"start\":18500},{\"attributes\":{\"doi\":\"10.1145/1014052.1014073\",\"id\":\"b4\",\"matched_paper_id\":207155218},\"end\":19597,\"start\":19061},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":20981275},\"end\":19852,\"start\":19599},{\"attributes\":{\"doi\":\"10.18653/v1/W17-3529\",\"id\":\"b6\",\"matched_paper_id\":22162396},\"end\":20431,\"start\":19854},{\"attributes\":{\"doi\":\"abs/1408.5882\",\"id\":\"b7\"},\"end\":20614,\"start\":20433},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":7360763},\"end\":20868,\"start\":20616},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":21725995},\"end\":21100,\"start\":20870},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7186165},\"end\":21901,\"start\":21102},{\"attributes\":{\"doi\":\"10.1145/3065386\",\"id\":\"b11\",\"matched_paper_id\":195908774},\"end\":22209,\"start\":21903},{\"attributes\":{\"doi\":\"10.3115/1072228.1072378\",\"id\":\"b12\",\"matched_paper_id\":11039301},\"end\":22737,\"start\":22211},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":16017905},\"end\":23309,\"start\":22739},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":17762145},\"end\":23838,\"start\":23311},{\"attributes\":{\"doi\":\"10.1145/219717.219748\",\"id\":\"b15\",\"matched_paper_id\":1671874},\"end\":24028,\"start\":23840},{\"attributes\":{\"doi\":\"10.3115/1218955.1218990\",\"id\":\"b16\",\"matched_paper_id\":388},\"end\":24705,\"start\":24030},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1957433},\"end\":25070,\"start\":24707},{\"attributes\":{\"doi\":\"abs/1802.05365\",\"id\":\"b18\"},\"end\":25426,\"start\":25072},{\"attributes\":{\"doi\":\"abs/1705.10694\",\"id\":\"b19\"},\"end\":25692,\"start\":25428},{\"attributes\":{\"doi\":\"10.18653/v1/P16-1009\",\"id\":\"b20\",\"matched_paper_id\":15600925},\"end\":26328,\"start\":25694},{\"attributes\":{\"doi\":\"abs/1903.09244\",\"id\":\"b21\"},\"end\":26537,\"start\":26330},{\"attributes\":{\"doi\":\"10.18653/v1/K17-2010\",\"id\":\"b22\",\"matched_paper_id\":19265222},\"end\":27169,\"start\":26539},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":16067356},\"end\":27746,\"start\":27171},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":14687186},\"end\":28076,\"start\":27748},{\"attributes\":{\"id\":\"b25\"},\"end\":28492,\"start\":28078},{\"attributes\":{\"doi\":\"10.18653/v1/D15-1167\",\"id\":\"b26\"},\"end\":28770,\"start\":28494},{\"attributes\":{\"doi\":\"10.1162/153244302760185243\",\"id\":\"b27\",\"matched_paper_id\":7806109},\"end\":29077,\"start\":28772},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":14618636},\"end\":29291,\"start\":29079},{\"attributes\":{\"doi\":\"10.18653/v1/D15-1306\",\"id\":\"b29\",\"matched_paper_id\":3257353},\"end\":30079,\"start\":29293},{\"attributes\":{\"id\":\"b30\"},\"end\":30390,\"start\":30081},{\"attributes\":{\"id\":\"b31\"},\"end\":30579,\"start\":30392},{\"attributes\":{\"doi\":\"abs/1804.09541\",\"id\":\"b32\"},\"end\":30814,\"start\":30581},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":368182},\"end\":31392,\"start\":30816}]", "bib_title": "[{\"end\":17205,\"start\":17146},{\"end\":17914,\"start\":17853},{\"end\":18540,\"start\":18500},{\"end\":19100,\"start\":19061},{\"end\":19635,\"start\":19599},{\"end\":19901,\"start\":19854},{\"end\":20657,\"start\":20616},{\"end\":20949,\"start\":20870},{\"end\":21161,\"start\":21102},{\"end\":21966,\"start\":21903},{\"end\":22240,\"start\":22211},{\"end\":22812,\"start\":22739},{\"end\":23375,\"start\":23311},{\"end\":23879,\"start\":23840},{\"end\":24128,\"start\":24030},{\"end\":24752,\"start\":24707},{\"end\":25759,\"start\":25694},{\"end\":26587,\"start\":26539},{\"end\":27260,\"start\":27171},{\"end\":27790,\"start\":27748},{\"end\":28851,\"start\":28772},{\"end\":29125,\"start\":29079},{\"end\":29460,\"start\":29293},{\"end\":30878,\"start\":30816}]", "bib_author": "[{\"end\":17221,\"start\":17207},{\"end\":17236,\"start\":17221},{\"end\":17253,\"start\":17236},{\"end\":17608,\"start\":17594},{\"end\":17624,\"start\":17608},{\"end\":17636,\"start\":17624},{\"end\":17656,\"start\":17636},{\"end\":17932,\"start\":17916},{\"end\":17949,\"start\":17932},{\"end\":17964,\"start\":17949},{\"end\":18566,\"start\":18542},{\"end\":18576,\"start\":18566},{\"end\":19114,\"start\":19102},{\"end\":19124,\"start\":19114},{\"end\":19649,\"start\":19637},{\"end\":19662,\"start\":19649},{\"end\":19677,\"start\":19662},{\"end\":19699,\"start\":19677},{\"end\":19712,\"start\":19699},{\"end\":19917,\"start\":19903},{\"end\":19941,\"start\":19917},{\"end\":19960,\"start\":19941},{\"end\":20502,\"start\":20492},{\"end\":20667,\"start\":20659},{\"end\":20689,\"start\":20667},{\"end\":20703,\"start\":20689},{\"end\":20722,\"start\":20703},{\"end\":20969,\"start\":20951},{\"end\":21185,\"start\":21163},{\"end\":21201,\"start\":21185},{\"end\":21223,\"start\":21201},{\"end\":21985,\"start\":21968},{\"end\":22001,\"start\":21985},{\"end\":22014,\"start\":22001},{\"end\":22250,\"start\":22242},{\"end\":22260,\"start\":22250},{\"end\":22827,\"start\":22814},{\"end\":22839,\"start\":22827},{\"end\":22855,\"start\":22839},{\"end\":23387,\"start\":23377},{\"end\":23401,\"start\":23387},{\"end\":23411,\"start\":23401},{\"end\":23426,\"start\":23411},{\"end\":23891,\"start\":23881},{\"end\":23899,\"start\":23891},{\"end\":24139,\"start\":24130},{\"end\":24152,\"start\":24139},{\"end\":24774,\"start\":24754},{\"end\":24790,\"start\":24774},{\"end\":24813,\"start\":24790},{\"end\":25090,\"start\":25072},{\"end\":25104,\"start\":25090},{\"end\":25117,\"start\":25104},{\"end\":25131,\"start\":25117},{\"end\":25150,\"start\":25131},{\"end\":25162,\"start\":25150},{\"end\":25180,\"start\":25162},{\"end\":25491,\"start\":25476},{\"end\":25505,\"start\":25491},{\"end\":25523,\"start\":25505},{\"end\":25535,\"start\":25523},{\"end\":25776,\"start\":25761},{\"end\":25790,\"start\":25776},{\"end\":25807,\"start\":25790},{\"end\":26410,\"start\":26396},{\"end\":26609,\"start\":26589},{\"end\":26627,\"start\":26609},{\"end\":26637,\"start\":26627},{\"end\":26658,\"start\":26637},{\"end\":27278,\"start\":27262},{\"end\":27290,\"start\":27278},{\"end\":27305,\"start\":27290},{\"end\":27323,\"start\":27305},{\"end\":27808,\"start\":27792},{\"end\":27824,\"start\":27808},{\"end\":27833,\"start\":27824},{\"end\":27847,\"start\":27833},{\"end\":27868,\"start\":27847},{\"end\":27879,\"start\":27868},{\"end\":27898,\"start\":27879},{\"end\":28154,\"start\":28135},{\"end\":28163,\"start\":28154},{\"end\":28177,\"start\":28163},{\"end\":28194,\"start\":28177},{\"end\":28208,\"start\":28194},{\"end\":28227,\"start\":28208},{\"end\":28589,\"start\":28578},{\"end\":28599,\"start\":28589},{\"end\":28609,\"start\":28599},{\"end\":28865,\"start\":28853},{\"end\":28880,\"start\":28865},{\"end\":29151,\"start\":29127},{\"end\":29476,\"start\":29462},{\"end\":29487,\"start\":29476},{\"end\":29493,\"start\":29487},{\"end\":30092,\"start\":30081},{\"end\":30105,\"start\":30092},{\"end\":30115,\"start\":30105},{\"end\":30128,\"start\":30115},{\"end\":30140,\"start\":30128},{\"end\":30154,\"start\":30140},{\"end\":30167,\"start\":30154},{\"end\":30408,\"start\":30394},{\"end\":30421,\"start\":30408},{\"end\":30439,\"start\":30421},{\"end\":30449,\"start\":30439},{\"end\":30459,\"start\":30449},{\"end\":30477,\"start\":30459},{\"end\":30485,\"start\":30477},{\"end\":30674,\"start\":30670},{\"end\":30893,\"start\":30880},{\"end\":30905,\"start\":30893},{\"end\":30917,\"start\":30905}]", "bib_venue": "[{\"end\":18145,\"start\":18073},{\"end\":18737,\"start\":18655},{\"end\":19367,\"start\":19257},{\"end\":20125,\"start\":20061},{\"end\":21490,\"start\":21355},{\"end\":22444,\"start\":22362},{\"end\":23040,\"start\":22956},{\"end\":23583,\"start\":23513},{\"end\":24366,\"start\":24269},{\"end\":25988,\"start\":25916},{\"end\":26845,\"start\":26770},{\"end\":27426,\"start\":27412},{\"end\":29672,\"start\":29601},{\"end\":31100,\"start\":31008},{\"end\":17323,\"start\":17279},{\"end\":17592,\"start\":17512},{\"end\":18071,\"start\":17984},{\"end\":18653,\"start\":18576},{\"end\":19255,\"start\":19147},{\"end\":19716,\"start\":19712},{\"end\":20059,\"start\":19980},{\"end\":20490,\"start\":20433},{\"end\":20733,\"start\":20722},{\"end\":20978,\"start\":20969},{\"end\":21353,\"start\":21223},{\"end\":22040,\"start\":22029},{\"end\":22360,\"start\":22283},{\"end\":22954,\"start\":22855},{\"end\":23511,\"start\":23426},{\"end\":23931,\"start\":23920},{\"end\":24267,\"start\":24175},{\"end\":24869,\"start\":24813},{\"end\":25240,\"start\":25194},{\"end\":25474,\"start\":25428},{\"end\":25914,\"start\":25827},{\"end\":26394,\"start\":26330},{\"end\":26768,\"start\":26678},{\"end\":27410,\"start\":27323},{\"end\":27903,\"start\":27898},{\"end\":28133,\"start\":28078},{\"end\":28576,\"start\":28494},{\"end\":28925,\"start\":28906},{\"end\":29170,\"start\":29151},{\"end\":29599,\"start\":29513},{\"end\":30226,\"start\":30167},{\"end\":30668,\"start\":30581},{\"end\":31006,\"start\":30917}]"}}}, "year": 2023, "month": 12, "day": 17}