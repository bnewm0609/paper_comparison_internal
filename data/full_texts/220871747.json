{"id": 220871747, "updated": "2023-10-06 12:34:49.697", "metadata": {"title": "SynergicLearning: Neural Network-Based Feature Extraction for Highly-Accurate Hyperdimensional Learning", "authors": "[{\"first\":\"Mahdi\",\"last\":\"Nazemi\",\"middle\":[]},{\"first\":\"Amirhossein\",\"last\":\"Esmaili\",\"middle\":[]},{\"first\":\"Arash\",\"last\":\"Fayyazi\",\"middle\":[]},{\"first\":\"Massoud\",\"last\":\"Pedram\",\"middle\":[]}]", "venue": "2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "journal": "2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "publication_date": {"year": 2020, "month": 7, "day": 30}, "abstract": "Machine learning models differ in terms of accuracy, computational/memory complexity, training time, and adaptability among other characteristics. For example, neural networks (NNs) are well-known for their high accuracy due to the quality of their automatic feature extraction while brain-inspired hyperdimensional (HD) learning models are famous for their quick training, computational efficiency, and adaptability. This work presents a hybrid, synergic machine learning model that excels at all the said characteristics and is suitable for incremental, on-line learning on a chip. The proposed model comprises an NN and a classifier. The NN acts as a feature extractor and is specifically trained to work well with the classifier that employs the HD computing framework. This work also presents a parameterized hardware implementation of the said feature extraction and classification components while introducing a compiler that maps any arbitrary NN and/or classifier to the aforementioned hardware. The proposed hybrid machine learning model has the same level of accuracy (i.e. $\\pm$1%) as NNs while achieving at least 10% improvement in accuracy compared to HD learning models. Additionally, the end-to-end hardware realization of the hybrid model improves power efficiency by 1.60x compared to state-of-the-art, high-performance HD learning implementations while improving latency by 2.13x. These results have profound implications for the application of such synergic models in challenging cognitive tasks.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2007.15222", "mag": "3112653938", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccad/NazemiEFP20", "doi": "10.1145/3400302.3415696"}}, "content": {"source": {"pdf_hash": "11a8256580e1ede03e69b650c63875b84800f55d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2007.15222v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": null, "status": "CLOSED"}}, "grobid": {"id": "e9048eae4f14b108b2d12d22faeaed9201bfe976", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/11a8256580e1ede03e69b650c63875b84800f55d.txt", "contents": "\nSynergicLearning: Neural Network-Based Feature Extraction for Highly-Accurate Hyperdimensional Learning\n\n\nMahdi Nazemi mnazemi@usc.edu \nAmirhossein Esmaili esmailid@usc.edu \nArash Fayyazi fayyazi@usc.edu \nMassoud Pedram pedram@usc.edu \n\nUniversity of Southern California\nUniversity of Southern\nCalifornia\n\n\nUniversity of Southern\nCalifornia\n\n\nUniversity of Southern\nCalifornia\n\nSynergicLearning: Neural Network-Based Feature Extraction for Highly-Accurate Hyperdimensional Learning\n\nMachine learning models differ in terms of accuracy, computational/memory complexity, training time, and adaptability among other characteristics. For example, neural networks (NNs) are wellknown for their high accuracy due to the quality of their automatic feature extraction while brain-inspired hyperdimensional (HD) learning models are famous for their quick training, computational efficiency, and adaptability. This work presents a hybrid, synergic machine learning model that excels at all the said characteristics and is suitable for incremental, on-line learning on a chip. The proposed model comprises an NN and a classifier. The NN acts as a feature extractor and is specifically trained to work well with the classifier that employs the HD computing framework. This work also presents a parameterized hardware implementation of the said feature extraction and classification components while introducing a compiler that maps any arbitrary NN and/or classifier to the aforementioned hardware. The proposed hybrid machine learning model has the same level of accuracy (i.e. \u00b11%) as NNs while achieving at least 10% improvement in accuracy compared to HD learning models. Additionally, the end-to-end hardware realization of the hybrid model improves power efficiency by 1.60x compared to state-of-the-art, high-performance HD learning implementations while improving latency by 2.13x. These results have profound implications for the application of such synergic models in challenging cognitive tasks.\n\nINTRODUCTION\n\nMachine learning models have proven successful in solving a wide variety of challenging problems such as computer vision and speech recognition. They are commonly characterized by their level of accuracy, computational/memory complexity, training time, and adaptability among other features. One can categorize machine learning models according to the aforesaid characteristics. For example, neural networks (NNs) typically achieve high accuracy [1], are computationally expensive [2], have long training times [3], and tend to forget previously learned information upon learning new information (aka catastrophic forgetting) [4][5][6]. A machine learning model is more viable for on-chip learning (also called learning on-a-chip which refers to designing a custom chip that can be used for both training and inference) when it has low computational/memory complexity and supports one-pass training/finetuning while maintaining a high level of accuracy.\n\nThe main reason behind the high accuracy of NNs is their ability to automatically extract high-quality, high-level features from labeled data. AlexNet [7] is an outstanding example that clearly demonstrates the gap between the quality of features extracted by NNs compared to handcrafted features extracted by experts in the domain (in the ImageNet Large Scale Visual Recognition Challenge [8], AlexNet was able to achieve 10.8% higher accuracy compared to the runner up, which used handcrafted features). Unfortunately, the high accuracy of NNs is accompanied by an enormous computational/memory cost during training and inference. Training an NN is a time-consuming, iterative process where in each iteration, all training data is applied to the model and the parameters of the model are updated according to stochastic gradient descent.\n\nAs another example, hyperdimensional (HD) learning models train quickly, are highly adaptable and computationally efficient (compared to NNs), but suffer from lower levels of accuracy compared to NNs [9]. HD learning uses randomly generated, highdimensional vectors to project training data into HD space such that samples belonging to the same class are placed in close proximity of each other, forming a cluster in the HD space. It then defines HD centroids that represent different classes. This relatively simple training process only requires one pass over the training data. It also enables efficient incremental, lifelong learning because updating the model with new training data is as simple as updating the cluster centroids. The major disadvantage of HD learning is that it works with raw or handcrafted input features, which are inferior to the ones extracted by NNs.\n\nThe complementary characteristics of NNs and HD models encourage the introduction of a hybrid, synergic machine learning model that builds on their strengths while avoiding their shortcomings. However, simply employing NNs for feature extraction and HD models for classification so as to enable on-chip learning has the following challenges. Not only is the training of NNs for feature extraction an iterative, energy-consuming process but also it requires access to both previous training data and newly provided data to avoid catastrophic forgetting. Therefore, frequent weight updates of NNs can be extremely costly in the context of learning on-a-chip. Additionally, the HD learning models that work well for solving cognitive tasks have a huge number of dimensions, e.g., 10,000, which requires their hardware implementation to timeshare resources and therefore, have a relatively high latency. This prevents real-time fine-tuning of the model when new training data becomes available. Moreover, training NNs for feature extraction separately from the design of the HD learning model produces suboptimal results because it does not account for the effect of HD classification layers on the NN feature extraction layers and vice versa. This means that the prediction/classification accuracy of the overall hybrid solution will suffer. This work presents SynergicLearning, a hybrid learning framework for incremental, on-line learning on a chip. SynergicLearning is comprised of three components which enable end-to-end learning:\n\n(1) A Two-step Training Approach: This training approach first trains an NN while including some components of the HD learning system in the NN's training loop to learn highquality, high-level features that are specifically tailored for the HD learning system. It then passes training data (including the initial data as well as the ones that are generated during the lifetime of the model) through the feature extraction layers of the NN to provide features for training/fine-tuning of the HD classifier (the neural network parameters are fixed at this step). Such a two-level training approach enables automatic feature extraction while reducing the number of dimensions in the HD classifier by two to three orders of magnitude 1 . (2) An On-chip Learning Module: This module is comprised of parameterized NN and HD processing modules, which respectively execute operations required by the NN feature extraction layers and operations required by the HD classifier. The NN processing module includes a systolic array which performs vector-matrix multiplications and an ALU which supports operations such as batch normalization, pooling, and ReLU. The HD processing module supports the arithmetic operations defined in the HD computing including binding, bundling, and distance calculation (Section 2 details these operations). The parameterized hardware implementation enables efficient exploration of the design space to find configurations that satisfy the design constraints such as energy and resource utilization. (3) A Compiler: The custom compiler performs code optimizations and generates instructions that efficiently schedule different operations required by the NN feature extraction and HD classification steps (e.g., vector-matrix multiplications and data movement) on the target platform. Table 1 compares different characteristics of NNs, HD learning systems (HDL), and the proposed SynergicLearning approach. It is observed that SynergicLearning enjoys automatic feature extraction and high accuracy because it employs an NN that is tailored for HDL. Furthermore, it only requires one pass to train/fine-tune its HD classifier and last but not least, it does not require accessing previous training samples to update the model when new data becomes available. The remainder of this paper is organized as follows. Section 2 explains the preliminaries on HD computing, discusses some of its shortcomings, and motivates the presented solution. Next, Section 3 details the proposed learning framework while Section 4 explains the proposed hardware architecture and compiler for inference.\n\n\nPRELIMINARIES & MOTIVATION\n\nHD computing defines a new computation framework that relies on high-dimensional random vectors (aka hypervectors) and the arithmetic operations that manipulate such large random patterns. An HD system starts by randomly generating d h -dimensional, holistic seed hypervectors with independent and identically distributed (i.i.d) elements. This means that the information encoded into each hypervector is uniformly distributed over all its elements. Therefore, unlike the conventional computing framework, elements in different bit positions in hypervectors are equally significant. The seed hypervectors are typically stored in a memory called the cleanup memory. The arithmetic operations defined on the seed hypervectors, e.g. binding and bundling, enable meaningful computations in the corresponding hyperspace. The focus of this paper is on binary hypervectors where each element is equally likely to be a zero or one. Binary hypervectors enjoy simplified, hardware-friendly arithmetic operations.\n\nThe distance between two binary hypervectors is measured in normalized Hamming distance, i.e. the number of bit positions where the values of hypervectors differ, divided by d h . Consequently, the distance is always in the range zero to one inclusive. However, because the distance between two randomly generated hypervectors follows a binomial distribution, most hypervectors are about 0.5 apart from one another (when d h is large) and therefore, are nearly orthogonal (aka unrelated). Additionally, flipping the values of a relatively large portion of elements in a hypervector, e.g. one-third of all elements, results in a hypervector that is closer to the original hypervector compared to its unrelated hypervectors. This results in considerable tolerance to noise and approximation. When the cleanup memory is queried with a noisy hypervector, it returns the seed hypervector that is closest to the input query, hence the name cleanup.\n\nTwo of the commonly used arithmetic operations in HD computing are binding and bundling. The binding operation is used for variable-value association. Assume variable z and its corresponding value z 0 are represented with unrelated hypervectors and 0 , respectively. Then, the bound pair z = z 0 can be represented by * 0 , where element-wise multiplication ( * ) is replaced with elementwise XOR for binary hepervectors. The resulting hypervector is unrelated to both and 0 . However, each original hypervector can be recovered from the resulting hypervector given the other, e.g. 0 = S(( * 0 ) * ), where S(.) looks up the cleanup memory. This process is called unbinding. The bundling operation condenses a list of hypervectors into a single representative hypervector that is similar to all its constituents. This is achieved by summing up all hypervectors, followed by the comparison of each element in the resulting (summation) hypervector with half the number of original hypervectors to create a binary hypervector. If the original hypervectors are bound, their variables and/or values can be found through unbinding the bundled hypervector.\n\nThe HD computing framework can be used to solve cognitive tasks such as speech recognition and activity recognition [9][10][11].  ), and the number of quantization levels used to discretize the input values while the outputs are the HD centroids representing each class. The training starts with the generation of seed hypervectors for all d l features as well as the quantized values they can assume. While the seed hypervectors for features are generated randomly, the ones for quantized values are found by randomly flipping a specific number of bits of a seed hypervector to ensure the similarity of the hypervectors representing nearby values. Next, each feature and its value are bound and the set of all bound hypervectors are bundled into a single hypervector (aka encoding). Finally, the encoded hypervectors are categorized according to their labels and the set of hypervectors belonging to a class are bundled to find a representative centroid. During inference, the closest centroid to an encoded test sample (in terms of normalized Hamming distance) determines the model's prediction. By fixing d h and increasing q, the hypervectors representing different quantization levels become more similar because fewer bits are flipped across consecutive hypervectors. This, in turn, complicates the unbinding process because the cleanup memory may return the wrong values. Fig. 1 clearly illustrates this phenomenon by depicting the mean and standard deviation of the normalized absolute error between the input features and the decoded features of their encoded hypervectors. Ideally, decoding encoded hypervectors should return the exact same low-dimensional features as the original inputs (i.e., zero error), but this does not happen in practice. We believe this phenomenon is the main reason for the relatively poor performance of HD models compared to some other machine learning models such as NNs. Therefore, creating input features that are aware of the error due to very similar quantization levels can improve classification accuracy significantly, especially at lower d h . Fig. 2 demonstrates a high-level overview of the proposed hybrid learning framework. The proposed framework comprises two major components: an encoder-aware NN for high-quality feature extraction and an HD classifier.\n\n\nPROPOSED METHOD\n\nThe NN includes feature extraction layers, an HD encoder-decoder pair (i.e. HD codec), and classifier layer(s). It takes the input features, passes them through the said components (aka forward propagation), and calculates a loss value by comparing the predicted labels with the expected ones. It then updates the model parameters, i.e. weights and biases, by backpropagating the loss value using the derivative of the operations defined in the NN. Because the operations defined in the codec are not differentiable and the fact that an ideal codec should behave like the identity function, the codec's\n\n\nAlgorithm 1 Training an HD Model\n\nInput: \nn\u00d7d l = 1..n , i \u2208 R d l //the low-dimensional input features = y 1..n , 1 \u2264 y i \u2264 c //X i = \u2205 11:\nfor j in 1..d l do //bind feature-value pairs 12: Pre-processing input features with an NN has numerous important advantages. First, including the codec in the training loop encourages the NN to adjust its parameters such that it minimizes the impact of the codec's error on classification accuracy. Training two identically initialized NNs, one including a codec and the other without a codec, would result in a completely different set of parameters. Second, the number of features extracted by the NN (d N N ) can be much lower than the number of low-dimensional input features (d l ), which in turn reduces the complexity of the HD classifier. Third, because the NN extracts encoder-aware features, d h can be reduced by two to three orders of magnitude compared to the existing HD systems. In other words, because the degree of similarity of hypervectors representing quantization levels is less concerning, lower d h values can work equally well. Fourth, there is a large body of work on reducing the complexity of NNs through quantization [12,13], pruning [14], and knowledge distillation [15], to name but a few. This allows training NNs that are lightweight, thereby adding little overhead to the overall hardware cost.\nX i = X i \u222a bind( j , x q i j\nThe HD classifier is very similar to the one described in Alg. 1. The only difference is that it takes the output of NN's feature extraction layers ( N N i ) instead of the original input features ( i ). Therefore, it not only benefits from the inherent strength of NNs in feature extraction but also enjoys features that are specifically tailored for the HD encoder.\n\n\nPROPOSED HARDWARE ARCHITECTURE & COMPILER\n\nThe proposed hardware architecture implements an end-to-end, fully-parameterized implementation of SynergicLearning for inference. It consists of two major hardware components: an NN processing module, which includes a systolic array and an ALU, and a fully-parallel HD processing module which supports various operations such as binding, bundling, and distance calculation. Fig. 3 demonstrates a high-level overview of the NN processing module, which comprises the following components:\n\n\nNN Processing Module\n\n\u2022 the systolic array, which consists of a two-dimensional array of processing elements, \u2022 on-chip memories (i.e. weight, input, and output buffers), which act as an intermediate storage between the DRAM and the systolic array, \u2022 tree adders, each of which performs a summation over a row of the systolic array, and \u2022 ALUs, which support activation functions, batch normalization, pooling, etc. Processing each layer of the NN requires the following operations. First, the weights are read from the external memory (DRAM) and stored in the weight buffer while inputs are read either from the DRAM or the output buffer. Next, the systolic array and tree adders calculate the neurons' pre-activation values by implementing vector-matrix multiplications. Then, ALUs apply batch normalization, activation function, pooling, etc. to pre-activations to generate the output features. Finally, the output features are either rerouted to the input buffers or written back to the DRAM.\n\nThe systolic array implements a weight-stationary dataflow [2], which reuses each weight value in different computations involved in vector-matrix multiplication and therefore, reduces the overhead associated with data movement. In this dataflow, the number of cycles it takes to process each layer of the NN is approximated by\n( d l i w sys + log 2 w sys ) \u00d7 d l i +1 h sys ,\nwhere d l i is number of neurons in the i th layer and w sys (h sys ) is the number of columns (rows) in the systolic array. log 2 w sys represents the depth of each tree adder.\n\nFor mapping a neural network presented in high-level languages to the target FPGA, we developed an in-house compiler called Syn-ergicCompiler. Since all hardware designs presented in this paper perform the same computation, i.e. a three-level nested loop for fully connected layers, the space explorations are defined by transformations (e.g., block, reorder, and parallelize) on the nested loop. Therefore, the compiler tries various choices of loop ordering and hardware parallelism for computing these nested loops of NNs and finds the most efficient one in terms of latency. The Synergic-Compiler also generates a static schedule for the data movements between hierarchies of memories, e.g., between external memories and buffers, and buffers and registers within PEs. Static scheduling mitigates the need for complex handshaking and improves the scalability and performance of the processing modules. Finally, the compiler delivers a set of instructions that efficiently schedule different operations such as vector-matrix multiplications and data movement on the target platform. More details about the compiler are not included in this paper for brevity. Fig. 4 demonstrates a high-level overview of the pipelined HD processing module, which comprises the following components:\n\n\nHD Processing Module\n\n\u2022 lookup tables (LUTs) that store hypervectors representing quantized levels, \u2022 binding/unbinding units, which perform parallel XOR operations, \u2022 majority counters, which compute the population count of a bit vector by incrementing a (log 2 (d l + 1) + 1)-bit counter when a set bit is encountered and decrementing it when a reset bit is seen, \u2022 comparators, which produce binary hypervectors from integer hypervectors, and \u2022 tree adders and tree comparators, which implement a fullyparallel Hamming distance calculation and therefore, produce outputs in constant time.\n\nThe architecture of the proposed HD processing module has the lowest achievable latency but suffers from high resource consumption at large d h values compared to other possible architectures   such as the ones explained in [16]. However, because SynergicLearning allows the utilization of HD learning systems with extremely low d h values, the resource usage of the HD processing module will be negligible. Furthermore, because all the aforementioned components produce their results in constant time, the final output of the HD processing module will be produced in constant time too. Additionally, because of the pipelined implementation of the HD processing module, it can produce an output every cycle, hence very high throughput.\n\n\nRESULTS & DISCUSSION 5.1 Experimental Setup\n\n\nDatasets.\n\nTo study the effectiveness of SynergicLearning, we use two publicly available datasets: Human Activity Recognition (HAR) [17] and ISOLET [18]. HAR includes 10,299 samples, each of which contains 561 handcrafted features and a label that corresponds to one of six possible activities. ISOLET, on the other hand, contains 7,797 samples, each of which includes 617 handcrafted features and a label that corresponds to one of the 26 characters in the English alphabet. The goal is to take the input features and their labels and train classifiers that predict labels of unseen samples accurately.\n\n\nTraining Framework.\n\nWe implement a PyTorch-compatible [19] HD computing library that includes operations such as binding/unbinding, bundling, encoding, and decoding. Because of the compatibility with PyTorch, the operations can be mapped efficiently to either CPUs or GPUs. Additionally, they can be easily integrated into existing PyTorch designs such as NNs.\n\nWe also implement a training ecosystem that takes a user-defined (possibly existing) NN architecture and the parameters of the HD learning system (e.g. d h and q) and automatically glues different components together to enable encoder-aware training of the neural network. Similarly, it includes easy-to-use HD training modules. This training ecosystem allows us to quickly explore different designs and compare their accuracy.\n\n\nNeural Network Training.\n\nWe train all NNs by minimizing a cross-entropy loss function for 120 epochs, with a batch size of 256, and an l 2 regularizer. Additionally, we use a learning rate scheduler similar to the one described in [20] where the maximum  Figure 4: Architectural overview of the HD processing module which includes lookup tables that store hypervectors representing quantized levels, binding/unbinding units, majority counters, comparators, tree adders, and tree comparators.\n\nlearning rate is set to 0.01 while the number of steps per epoch is 25.\n\n\nHardware Emulation Framework.\n\nTo implement the NN and HD processing modules, we use the Xilinx SDAccel which provides a toolchain for programming and optimizing different applications on Xilinx FPGAs using a high-level language (C, C++ or OpenCL) and/or hardware description languages (VHDL, Verilog and SystemVerilog), as well as a runtime based on the OpenCL APIs that can be used by the host-side software to interact with the accelerator. We evaluate our proposed architecture using SDAccel on the ISOLET dataset targeting the Xilinx UltraScale+ VU9P FPGA on AWS EC2 F1 instances. We also use the Vivado power report provided by Xilinx to assess the power consumption of each design.\n\n\nThe Impact of NNs on the Quality of HD Features\n\nIn this section, we study the impact of NNs on the quality of encoded HD features by visualizing different samples of the HAR dataset in two-dimensional (2D) space. The feature extraction layers of the NNs consist of two fully-connected layers, each of which has 561 neurons. We deliberately keep the number of neurons in the final feature extraction layer the same as the one for the input features (i.e. d N N = d l ) to ensure the difference across the results of various experiments is only due to the introduction of NNs. We use ReLU and PACT [12] for the activation functions of the first and second layer, respectively. Fig. 5 shows the 2D representation of the encoded hypervectors of the test set for three different designs: HDL, NN followed by HDL, and encoder-aware NN followed by HDL (i.e. the proposed flow). To obtain the 2D representation, we employ t-distributed stochastic neighbor embedding (t-SNE) [21], which is a technique used for visualizing high-dimensional data. t-SNE tends to provide good visualizations because it tries to keep the similarities in HD space in the 2D representation as well. The 2D representations of hypervectors belonging to different classes are shown using different colors. For figures 5a-5c, we use d h = 16, and for figures 5d-5f, we use d h = 10, 240. For all experiments, q = 4.\n\nFor small values of d h (e.g. 16), it is observed that HDL performs poorly in the separation of points in the HD space (Fig. 5a). On the other hand, the addition of an NN to the flow helps with more proper separation of data points (Fig. 5b) while introducing an encoder-aware NN leads to a near-perfect clustering of data (Fig. 5c).\n\nThe accuracy values reported in Fig. 5a-5c further support this observation. For large values of d h (e.g. 10,240), it is observed that HDL performs relatively well while models that include NNs still outperform the HDL model by a large margin (Fig. 5d-5f). In this configuration, the model that includes an NN and the one that has an encoder-aware NN perform almost equally well. Table 2 compares the highest values of accuracy reported for NNs and HD learning systems with the proposed SynergicLearning approach on the HAR and ISOLET datasets. It is observed that on these datasets, the proposed hybrid model outperforms both NNs and HD learning systems used in the prior work. Fig. 6 compares classification accuracy of three different models (HDL, NN followed by HDL, and encoder-aware NN followed by HDL) for different values of d h and q on HAR and ISOLET datasets. It is observed that the model that includes an NN consistently outperforms HDL while the model that has an encoder-aware NN outperforms the other two in almost all experiments. On the HAR dataset, the difference between the model with an encoder-aware NN and the HDL model is as large as about 63% at d h = 16 while it decreases to about 14% at d h = 10, 240. Similarly, On the ISOLET dataset, the difference between the model with an encoder-aware NN and the HDL model is as large as about 83% at d h = 16 while it decreases to about 10% at d h = 10, 240.\n\n\nComparison of Classification Accuracy\n\nAnother key observation is that the model with an encoderaware NN achieves almost the same level of accuracy at different values of d h . This is particularly interesting from a hardware cost perspective, because we can pick the lowest value of d h (16 in these experiments) and achieve significant reduction in resource utilization while maintaining high accuracy.  Figure 5: Two-dimensional (t-SNE) representation of the encoded hypervectors of the HAR dataset for three different designs: HDL, NN followed by HDL, and encoder-aware NN followed by HDL. We also study the effect of different random seeds for initialization of NN weights and randomly generated seed hypervectors on classification accuracy. Based on our experiments, the difference between the lowest and highest values of classification accuracy across designs that use different seeds is at most 1%. We believe such variation in classification accuracy is acceptable. Table 3 compares the accuracy of HD learning models and Syner-gicLearning when a portion of data is initially used for training while the remaining data is used for fine-tuning the model on a chip. Because on-chip-learning is extremely costly for NNs, we do not consider them in this comparison. As expected, the HDL model is insensitive to whether the training data is provided incrementally or all at once and therefore, its accuracy remains constant and relatively low. For the SynergicLearning model, on the other hand, the accuracy keeps increasing when more data is provided to the NN in the initial training phase because it allows the NN to find higher quality features. This encourages less frequent, off-line updates to the NN for increasing the accuracy of the model.  Table. 4). While our parameterized architecture has a capability to generate both parallel and sequential-vector for HD processing module of SynergicLearning approach, we report the results for parallel implementation which delivers higher performance. Thanks to exteremly low d h value in SynergicHD, the hardware overhead of parallel implementation is minimal. Table 4 compares area utilization, latency, and power consumption of SynergicLearning at d h = 16 with pure HD processing module at d h = 10, 240. SynergicLearning outperforms the fullyparallel pure HD processing module in terms of latency by a factor of 2.13x while yielding 1.60x lower power consumption. Compared to the vector-sequential implementation of HD processing module, SynergicLearning achieves 33.89x improvement in latency while yielding 1.45x lower power consumption.\n\n\nIncremental Learning\n\nIt is worth mentioning that our designs are capable of achieving high clock rates (i.e. 344 MHz). The breakdown of different metrics between the NN and HD processing modules is as follows. The NN processing module consumes 93%, 100%, 87%, and 71% of the total consumed BRAMs-18K, DSPs-48E, FFs, and LUTs, respectively. The latency of the NN processing module is 23.12\u00b5s and the power consumption of the HD processing module is negligible compared to the NN processing module (i.e. less than 4% of total power consumption).\n\n\nRELATED WORK\n\nKanerva [9] explains the advantages and mathematical properties of HD computing, and how data patterns should correspond in a systematic way to the entities they represent in the real world for achieving brain-like computing. Some of the prior work attempt to improve the performance of HD computing, either by increasing the obtained accuracy for some complex tasks, or enabling it to maintain the accuracy for lower dimensions. Authors in [10] propose a hierarchical HD computing framework, which enables HD to improve its performance using multiple encoders without increasing the cost of classification. In [11], authors utilize the mathematics of hyperdimensional spaces, and split each class hypervector into separate components and combine them into a reduced dimensional model. However, these works have not explored the effect of the feature extraction for low-dimensional input features. Several studies in the literature explore hardware optimizations for implementing HD computing for different application domains. Authors in [26] propose a memory-centric architecture for the HD classifier with modular and scalable components, and demonstrate its performance on a language identification task. In [27], authors develop a programmable and scalable architecture for energyefficient supervised classification using HD computing, and compare it with traditional architectures for a few conventional machine learning algorithms. The work in [28] explores architectural designs for the cleanup memory to facilitate energy-efficient, fast, and scalable search operation, and the proposed designs are evaluated for a language recognition application.\n\n\nCONCLUSIONS\n\nIn this paper, we proposed SynergicLearning, in which by designing NNs that include some components of the HD models in their training loop, we trained high-quality feature extraction layers tailored to the HD learning model. By passing the input low-dimensional features through these layers before encoding them into the HD space, the number of dimensions of the HD space was reduced by two to three orders of magnitude, while maintaining the high classification accuracy, which led to less complex HD classifier. We also proposed and implemented an end-to-end fully-parametrized implementation of SynergicLearning for inference. Following the proposed hardware architecture, we achieved 2.13x improvement in terms of latency, while yielding 1.60x lower power consumption compared to pure HD computing. Pure HD [16] Parallel 0 (N/A) 0 (N/A) 11.0 (93%) 15.0 (66%) 49.5 (53%) 8.5 (38%) Sequential 0 (N/A) 0 (N/A) 11.0 (93%) 9.0 (43%) 788.7 (97%) 7.7 (31%) NN [24,25] Systolic Array 1.7 (-6%) 15.0 (0%) 0.7 (-14%) 3.6 (-42%) 835.9 (97%) 5.1 (-4%)\n\n=\nbundle(X i ) //bundle bound hypervectors 15: end for 16: T 1 = T 2 = ... = T c = \u2205 17: for each enc i do //group encoded inputs by labels 18: T y i = T y i \u222a enc i 19: end for 20: for each T k do //bundle all members of each class 21: k = bundle(T k ) 22: end for 23: return derivative is approximated with that of the identity function during backpropagation.\n\nFigure 1 :\n1The mean and standard deviation of the normalized absolute error between the input features and the decoded features of their encoded hypervectors for different values of d h and q. Ideally, this error should be zero everywhere. However, the error has a non-zero value even at extremely high dimensions (d h \u2243 10, 000).\n\nFigure 3 :\n3Architectural view of the NN processing module which includes a systolic array, on-chip memories, tree adders, and ALUs.\n\nFigure 6 :\n6Classification accuracy of different models on HAR and ISOLET datasets for different values of d h and q.\n\nFigure 7 :\n7the LUT utilization and latency of HD processing modules for different values of d h .\n\nTable 1 :\n1Comparison of different characteristics of NNs, HD learning systems, and SynergicLearning.Machine Learning \nAutomatic \nHigh \nOne-pass \nAdaptable w/o Accessing \nModel \nFeature Extraction \nAccuracy \nTraining/Fine-tuning \nPrevious Training Samples \nNN \n\u2713 \n\u2713 \n\u2717 \n\u2717 \nHDL \n\u2717 \n\u2717 \n\u2713 \n\u2713 \nSynergicLearning \n\u2713 \n\u2713 \n\u2713 \n\u2713 \n\nAlg. 1 summarizes different steps of training an HD model. The \ninputs to the algorithm are d l -dimensional input features, their \ncorresponding labels/classes, the dimension of hypervectors (d h \n\n\nc\u00d7d h = 1..c //the HD centroids 1: generate d l \u00d7d h = 1..d l //seed hypervectors for features for i = 2..q do 5: i = randomly pick p unflipped bits and flip them in i\u22121 6: end for 7: q\u00d7d h = 1..q //seed hypervectors for levels 8: for each i do //encode all samplesthe target labels/classes \nd h \n//the number of hyperspace dimensions \nq \n//the number of quantization levels \nOutput: \n\n2: p = \u230a d h \nq \u230b \n//number of bits to flip \n3: generate 1 randomly \n4: 9: \nq \n\ni = quantize( i , q) \n//quantize real values to integers \n\n10: \n\n\n\nTable 2 :\n2Top accuracy reported for NNs, HD learning systems, and SynergicLearning on HAR and ISOLET datasets. Uses a convolutional neural network.* Uses a fully-connected network with 48 hidden layers.Dataset Machine Learning Model \nAccuracy (%) \n\nHAR \nNN [22]  \u20212 \n95.31 % \nHDL [23] \n93.4% \nSynergicLearning \n96.44 % \nISOLET \nNN [24, 25]  *  \n95.9 % \nHDL [25] \n93.8 % \nSynergicLearning \n96.67 % \n \u2021 \n\nTable 3 :\n3Comparison of the effect of incremental learning on the accuracy of different models on the ISOLET dataset. The Hardware Cost of NN & HD Processing Modules Fig. 7 shows the LUT utilization and latency of HD processing modules for different values of d h while limiting the number of adders in each stage of tree adders to 16. It is observed that the latency grows very rapidly when increasing d h to values required for meeting accuracy requirements. Additionally, to reduce the resource utilization for large values of d h , we can change the architecture from a fully-parallel architecture to a vector-sequential architecture where all adders and counters operate in a sequential manner (compare Sequential Implementation with Parallel Implementation entries inMachine Learning \nAccuracy \nModel \n(Ratio of the Initial Training Data) \n\nHDL \n85.76% 85.76% 85.76% 85.76% \n(0.25) \n(0.5) \n(0.75) \n(1) \n\nSynergicLearning \n86.21% 91.21% 94.03% 95.77% \n(0.25) \n(0.5) \n(0.75) \n(1) \n\n5.5 \n\nTable 4 :\n4Comparison between the hardware metrics of SynergicLearning (d h = 16) with pure HD (d h = 10, 240) over the ISOLET dataset on Xilinx UltraScale+ VU9P FPGA. The improvements of our approach compared to other approaches are shown in parantheses.Approach \nImplementation BRAMs-18K (%) DSPs-48E (%) \nFFs (%) \nLUTs (%) Latency (\u00b5s) Power (W) \n\nSynergicLearning \nNN+HD \n1.8 \n15.0 \n0.8 \n5.1 \n23.3 \n5.3 \n\n\nWhile the term hyperdimensional learning is no longer applicable to such a classifier, we keep using the same term to highlight the fact that the operations used in the classifier are based on those defined in the hyperdimensional computing framework.After that, Section 5 presents the experimental results while Section 6 briefly reviews the related work on HD computing. Finally, Section 7 concludes the paper.\nThey also reported higher accuracy of 97.6 % when they added statistical features and data centering methods to their convolutional neural network.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, IEEE conference on computer vision and pattern recognition (CVPR). Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learn- ing for image recognition. In IEEE conference on computer vision and pattern recognition (CVPR), 2016.\n\nEfficient processing of deep neural networks: A tutorial and survey. Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, Joel S Emer, Proceedings of the IEEE. the IEEEVivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. Efficient processing of deep neural networks: A tutorial and survey. Proceedings of the IEEE, 2017.\n\nOn the computational efficiency of training neural networks. Roi Livni, Shai Shalev-Shwartz, Ohad Shamir, Advances in neural information processing systems. Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. In Advances in neural information processing systems, 2014.\n\nCatastrophic interference in connectionist networks: The sequential learning problem. Michael Mccloskey, J Neal, Cohen, Psychology of learning and motivation. ElsevierMichael McCloskey and Neal J Cohen. Catastrophic interference in connection- ist networks: The sequential learning problem. In Psychology of learning and motivation. Elsevier, 1989.\n\nDoyen Sahoo, Quang Pham, Jing Lu, C H Steven, Hoi, arXiv:1711.03705Online deep learning: Learning deep neural networks on the fly. arXiv preprintDoyen Sahoo, Quang Pham, Jing Lu, and Steven CH Hoi. Online deep learning: Learning deep neural networks on the fly. arXiv preprint arXiv:1711.03705, 2017.\n\nIncremental on-line learning: A review and comparison of state of the art algorithms. Viktor Losing, Barbara Hammer, Heiko Wersing, Neurocomputing. Viktor Losing, Barbara Hammer, and Heiko Wersing. Incremental on-line learn- ing: A review and comparison of state of the art algorithms. Neurocomputing, 2018.\n\nImageNet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in neural information processing systems. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep convolutional neural networks. In Advances in neural information processing systems, 2012.\n\nIma-geNet large scale visual recognition challenge. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, International journal of computer vision. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Ima- geNet large scale visual recognition challenge. International journal of computer vision, 2015.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Pentti Kanerva, Cognitive computation. Pentti Kanerva. Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Cognitive computation, 2009.\n\nHierarchical hyperdimensional computing for energy efficient classification. Mohsen Imani, Chenyu Huang, Deqian Kong, Tajana Rosing, ACM/ESDA/IEEE Design Automation Conference (DAC). Mohsen Imani, Chenyu Huang, Deqian Kong, and Tajana Rosing. Hierarchical hy- perdimensional computing for energy efficient classification. In ACM/ESDA/IEEE Design Automation Conference (DAC), 2018.\n\nCompHD: Efficient hyperdimensional computing using model compression. Justin Morris, Mohsen Imani, Samuel Bosch, Anthony Thomas, Helen Shu, Tajana Rosing, IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED). Justin Morris, Mohsen Imani, Samuel Bosch, Anthony Thomas, Helen Shu, and Tajana Rosing. CompHD: Efficient hyperdimensional computing using model compression. In IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED), 2019.\n\nJungwook Choi, Zhuo Wang, Swagath Venkataramani, I-Jen Pierce, Vijayalakshmi Chuang, Kailash Srinivasan, Gopalakrishnan, arXiv:1805.06085PACT: Parameterized clipping activation for quantized neural networks. arXiv preprintJungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. PACT: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018.\n\nShuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, Yuheng Zou, arXiv:1606.06160DoReFa-Net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprintShuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. DoReFa-Net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016.\n\nA systematic DNN weight pruning framework using alternating direction method of multipliers. Tianyun Zhang, Shaokai Ye, Kaiqi Zhang, Jian Tang, Wujie Wen, Makan Fardad, Yanzhi Wang, European Conference on Computer Vision (ECCV). Tianyun Zhang, Shaokai Ye, Kaiqi Zhang, Jian Tang, Wujie Wen, Makan Fardad, and Yanzhi Wang. A systematic DNN weight pruning framework using alternat- ing direction method of multipliers. In European Conference on Computer Vision (ECCV), 2018.\n\nDistilling the knowledge in a neural network. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531arXiv preprintGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\n\nHardware optimizations of dense binary hyperdimensional computing: Rematerialization of hypervectors, binarized bundling, and combinational associative memory. Manuel Schmuck, Luca Benini, Abbas Rahimi, ACM Journal on Emerging Technologies in Computing Systems. Manuel Schmuck, Luca Benini, and Abbas Rahimi. Hardware optimizations of dense binary hyperdimensional computing: Rematerialization of hypervectors, binarized bundling, and combinational associative memory. ACM Journal on Emerging Technologies in Computing Systems (JETC), 2019.\n\nA public domain dataset for human activity recognition using smartphones. Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, Jorge Luis Reyes-Ortiz , EsannDavide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, and Jorge Luis Reyes-Ortiz. A public domain dataset for human activity recognition using smartphones. In Esann, 2013.\n\nThe ISOLET spoken letter database. Oregon Graduate Institute of Science and Technology. Ron Cole, Yeshwant Muthusamy, Mark Fanty, Department of Computer \u00e2\u0102\u0119Ron Cole, Yeshwant Muthusamy, and Mark Fanty. The ISOLET spoken letter database. Oregon Graduate Institute of Science and Technology, Department of Computer \u00e2\u0102\u0119, 1990.\n\nPy-Torch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in Neural Information Processing Systems. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Py- Torch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, 2019.\n\nSuper-convergence: Very fast training of neural networks using large learning rates. N Leslie, Nicholay Smith, Topin, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications. International Society for Optics and Photonics. Leslie N Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using large learning rates. In Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications. International Society for Optics and Photonics, 2019.\n\nVisualizing data using t-SNE. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of machine learning research, 2008.\n\nReal-time human activity recognition from accelerometer data using convolutional neural networks. Andrey Ignatov, Applied Soft Computing. Andrey Ignatov. Real-time human activity recognition from accelerometer data using convolutional neural networks. Applied Soft Computing, 2018.\n\nFach: Fpga-based acceleration of hyperdimensional computing by reducing computational complexity. Mohsen Imani, Sahand Salamat, Saransh Gupta, Jiani Huang, Tajana Rosing, Proceedings of the 24th Asia and South Pacific Design Automation Conference. the 24th Asia and South Pacific Design Automation ConferenceMohsen Imani, Sahand Salamat, Saransh Gupta, Jiani Huang, and Tajana Rosing. Fach: Fpga-based acceleration of hyperdimensional computing by reducing com- putational complexity. In Proceedings of the 24th Asia and South Pacific Design Automation Conference, 2019.\n\nAn online algorithm for large scale image similarity learning. Gal Chechik, Uri Shalit, Varun Sharma, Samy Bengio, Advances in Neural Information Processing Systems. Gal Chechik, Uri Shalit, Varun Sharma, and Samy Bengio. An online algorithm for large scale image similarity learning. In Advances in Neural Information Processing Systems, 2009.\n\nVoicehd: Hyperdimensional computing for efficient speech recognition. Mohsen Imani, Deqian Kong, Abbas Rahimi, Tajana Rosing, 2017 IEEE International Conference on Rebooting Computing (ICRC). IEEEMohsen Imani, Deqian Kong, Abbas Rahimi, and Tajana Rosing. Voicehd: Hyper- dimensional computing for efficient speech recognition. In 2017 IEEE International Conference on Rebooting Computing (ICRC). IEEE, 2017.\n\nA robust and energy-efficient classifier using brain-inspired hyperdimensional computing. Abbas Rahimi, Pentti Kanerva, Jan M Rabaey, Proceedings of the 2016 International Symposium on Low Power Electronics and Design (ISLPED). the 2016 International Symposium on Low Power Electronics and Design (ISLPED)Abbas Rahimi, Pentti Kanerva, and Jan M Rabaey. A robust and energy-efficient classifier using brain-inspired hyperdimensional computing. In Proceedings of the 2016 International Symposium on Low Power Electronics and Design (ISLPED), 2016.\n\nA programmable hyper-dimensional processor architecture for human-centric IoT. Sohum Datta, A G Ryan, Antonio, R S Aldrin, Jan M Ison, Rabaey, IEEE Journal on Emerging and Selected Topics in Circuits and Systems. Sohum Datta, Ryan AG Antonio, Aldrin RS Ison, and Jan M Rabaey. A pro- grammable hyper-dimensional processor architecture for human-centric IoT. IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 2019.\n\nExploring hyperdimensional associative memory. Mohsen Imani, Abbas Rahimi, Deqian Kong, Tajana Rosing, Jan M Rabaey, 2017 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEEMohsen Imani, Abbas Rahimi, Deqian Kong, Tajana Rosing, and Jan M Rabaey. Exploring hyperdimensional associative memory. In 2017 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE, 2017.\n", "annotations": {"author": "[{\"end\":136,\"start\":107},{\"end\":174,\"start\":137},{\"end\":205,\"start\":175},{\"end\":236,\"start\":206},{\"end\":306,\"start\":237},{\"end\":342,\"start\":307},{\"end\":378,\"start\":343}]", "publisher": null, "author_last_name": "[{\"end\":119,\"start\":113},{\"end\":156,\"start\":149},{\"end\":188,\"start\":181},{\"end\":220,\"start\":214}]", "author_first_name": "[{\"end\":112,\"start\":107},{\"end\":148,\"start\":137},{\"end\":180,\"start\":175},{\"end\":213,\"start\":206}]", "author_affiliation": "[{\"end\":305,\"start\":238},{\"end\":341,\"start\":308},{\"end\":377,\"start\":344}]", "title": "[{\"end\":104,\"start\":1},{\"end\":482,\"start\":379}]", "venue": null, "abstract": "[{\"end\":1995,\"start\":484}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2460,\"start\":2457},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2495,\"start\":2492},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2525,\"start\":2522},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2640,\"start\":2637},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2643,\"start\":2640},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2646,\"start\":2643},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3120,\"start\":3117},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3359,\"start\":3356},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4010,\"start\":4007},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12072,\"start\":12069},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12076,\"start\":12072},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12080,\"start\":12076},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16078,\"start\":16074},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16081,\"start\":16078},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16095,\"start\":16091},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16128,\"start\":16124},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18250,\"start\":18247},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20852,\"start\":20848},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21544,\"start\":21540},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21560,\"start\":21556},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22073,\"start\":22069},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23043,\"start\":23039},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24667,\"start\":24663},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25037,\"start\":25033},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25482,\"start\":25479},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30391,\"start\":30388},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30825,\"start\":30821},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30995,\"start\":30991},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":31423,\"start\":31419},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31596,\"start\":31592},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":31835,\"start\":31831},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":32870,\"start\":32866},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":33016,\"start\":33012},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":33019,\"start\":33016},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33047,\"start\":33045}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33462,\"start\":33099},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33795,\"start\":33463},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33929,\"start\":33796},{\"attributes\":{\"id\":\"fig_4\"},\"end\":34048,\"start\":33930},{\"attributes\":{\"id\":\"fig_5\"},\"end\":34148,\"start\":34049},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":34668,\"start\":34149},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":35202,\"start\":34669},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":35606,\"start\":35203},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":36599,\"start\":35607},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":37010,\"start\":36600}]", "paragraph": "[{\"end\":2964,\"start\":2011},{\"end\":3805,\"start\":2966},{\"end\":4686,\"start\":3807},{\"end\":6220,\"start\":4688},{\"end\":8823,\"start\":6222},{\"end\":9856,\"start\":8854},{\"end\":10800,\"start\":9858},{\"end\":11951,\"start\":10802},{\"end\":14262,\"start\":11953},{\"end\":14884,\"start\":14282},{\"end\":14928,\"start\":14921},{\"end\":16256,\"start\":15028},{\"end\":16654,\"start\":16287},{\"end\":17187,\"start\":16700},{\"end\":18186,\"start\":17212},{\"end\":18515,\"start\":18188},{\"end\":18742,\"start\":18565},{\"end\":20028,\"start\":18744},{\"end\":20622,\"start\":20053},{\"end\":21359,\"start\":20624},{\"end\":22011,\"start\":21419},{\"end\":22375,\"start\":22035},{\"end\":22804,\"start\":22377},{\"end\":23299,\"start\":22833},{\"end\":23372,\"start\":23301},{\"end\":24063,\"start\":23406},{\"end\":25447,\"start\":24115},{\"end\":25782,\"start\":25449},{\"end\":27212,\"start\":25784},{\"end\":29816,\"start\":27254},{\"end\":30363,\"start\":29841},{\"end\":32037,\"start\":30380},{\"end\":33098,\"start\":32053}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15016,\"start\":14929},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15027,\"start\":15016},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16286,\"start\":16257},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18564,\"start\":18516}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":8033,\"start\":8026},{\"end\":12084,\"start\":12083},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":26172,\"start\":26165},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":28198,\"start\":28191},{\"end\":28977,\"start\":28971},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":29341,\"start\":29334}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2009,\"start\":1997},{\"attributes\":{\"n\":\"2\"},\"end\":8852,\"start\":8826},{\"attributes\":{\"n\":\"3\"},\"end\":14280,\"start\":14265},{\"end\":14919,\"start\":14887},{\"attributes\":{\"n\":\"4\"},\"end\":16698,\"start\":16657},{\"attributes\":{\"n\":\"4.1\"},\"end\":17210,\"start\":17190},{\"attributes\":{\"n\":\"4.2\"},\"end\":20051,\"start\":20031},{\"attributes\":{\"n\":\"5\"},\"end\":21405,\"start\":21362},{\"attributes\":{\"n\":\"5.1.1\"},\"end\":21417,\"start\":21408},{\"attributes\":{\"n\":\"5.1.2\"},\"end\":22033,\"start\":22014},{\"attributes\":{\"n\":\"5.1.3\"},\"end\":22831,\"start\":22807},{\"attributes\":{\"n\":\"5.1.4\"},\"end\":23404,\"start\":23375},{\"attributes\":{\"n\":\"5.2\"},\"end\":24113,\"start\":24066},{\"attributes\":{\"n\":\"5.3\"},\"end\":27252,\"start\":27215},{\"attributes\":{\"n\":\"5.4\"},\"end\":29839,\"start\":29819},{\"attributes\":{\"n\":\"6\"},\"end\":30378,\"start\":30366},{\"attributes\":{\"n\":\"7\"},\"end\":32051,\"start\":32040},{\"end\":33101,\"start\":33100},{\"end\":33474,\"start\":33464},{\"end\":33807,\"start\":33797},{\"end\":33941,\"start\":33931},{\"end\":34060,\"start\":34050},{\"end\":34159,\"start\":34150},{\"end\":35213,\"start\":35204},{\"end\":35617,\"start\":35608},{\"end\":36610,\"start\":36601}]", "table": "[{\"end\":34668,\"start\":34251},{\"end\":35202,\"start\":34936},{\"end\":35606,\"start\":35407},{\"end\":36599,\"start\":36382},{\"end\":37010,\"start\":36856}]", "figure_caption": "[{\"end\":33462,\"start\":33102},{\"end\":33795,\"start\":33476},{\"end\":33929,\"start\":33809},{\"end\":34048,\"start\":33943},{\"end\":34148,\"start\":34062},{\"end\":34251,\"start\":34161},{\"end\":34936,\"start\":34671},{\"end\":35407,\"start\":35215},{\"end\":36382,\"start\":35619},{\"end\":36856,\"start\":36612}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13338,\"start\":13332},{\"end\":14051,\"start\":14045},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17081,\"start\":17075},{\"end\":19912,\"start\":19906},{\"end\":23071,\"start\":23063},{\"end\":24748,\"start\":24742},{\"end\":25577,\"start\":25568},{\"end\":25690,\"start\":25681},{\"end\":25781,\"start\":25772},{\"end\":25823,\"start\":25816},{\"end\":26039,\"start\":26028},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26470,\"start\":26464},{\"end\":27629,\"start\":27621}]", "bib_author_first_name": "[{\"end\":37626,\"start\":37619},{\"end\":37638,\"start\":37631},{\"end\":37654,\"start\":37646},{\"end\":37664,\"start\":37660},{\"end\":37994,\"start\":37986},{\"end\":38007,\"start\":38000},{\"end\":38021,\"start\":38014},{\"end\":38032,\"start\":38028},{\"end\":38034,\"start\":38033},{\"end\":38298,\"start\":38295},{\"end\":38310,\"start\":38306},{\"end\":38331,\"start\":38327},{\"end\":38655,\"start\":38648},{\"end\":38668,\"start\":38667},{\"end\":38917,\"start\":38912},{\"end\":38930,\"start\":38925},{\"end\":38941,\"start\":38937},{\"end\":38947,\"start\":38946},{\"end\":38949,\"start\":38948},{\"end\":39306,\"start\":39300},{\"end\":39322,\"start\":39315},{\"end\":39336,\"start\":39331},{\"end\":39592,\"start\":39588},{\"end\":39609,\"start\":39605},{\"end\":39629,\"start\":39621},{\"end\":39631,\"start\":39630},{\"end\":39929,\"start\":39925},{\"end\":39946,\"start\":39943},{\"end\":39956,\"start\":39953},{\"end\":39969,\"start\":39961},{\"end\":39985,\"start\":39978},{\"end\":40000,\"start\":39996},{\"end\":40012,\"start\":40005},{\"end\":40026,\"start\":40020},{\"end\":40043,\"start\":40037},{\"end\":40059,\"start\":40052},{\"end\":40499,\"start\":40493},{\"end\":40786,\"start\":40780},{\"end\":40800,\"start\":40794},{\"end\":40814,\"start\":40808},{\"end\":40827,\"start\":40821},{\"end\":41161,\"start\":41155},{\"end\":41176,\"start\":41170},{\"end\":41190,\"start\":41184},{\"end\":41205,\"start\":41198},{\"end\":41219,\"start\":41214},{\"end\":41231,\"start\":41225},{\"end\":41575,\"start\":41567},{\"end\":41586,\"start\":41582},{\"end\":41600,\"start\":41593},{\"end\":41621,\"start\":41616},{\"end\":41643,\"start\":41630},{\"end\":41659,\"start\":41652},{\"end\":42032,\"start\":42024},{\"end\":42044,\"start\":42039},{\"end\":42054,\"start\":42049},{\"end\":42064,\"start\":42059},{\"end\":42073,\"start\":42071},{\"end\":42085,\"start\":42079},{\"end\":42518,\"start\":42511},{\"end\":42533,\"start\":42526},{\"end\":42543,\"start\":42538},{\"end\":42555,\"start\":42551},{\"end\":42567,\"start\":42562},{\"end\":42578,\"start\":42573},{\"end\":42593,\"start\":42587},{\"end\":42946,\"start\":42938},{\"end\":42960,\"start\":42955},{\"end\":42974,\"start\":42970},{\"end\":43310,\"start\":43304},{\"end\":43324,\"start\":43320},{\"end\":43338,\"start\":43333},{\"end\":43766,\"start\":43760},{\"end\":43786,\"start\":43776},{\"end\":43797,\"start\":43793},{\"end\":43811,\"start\":43805},{\"end\":43841,\"start\":43819},{\"end\":44118,\"start\":44115},{\"end\":44133,\"start\":44125},{\"end\":44149,\"start\":44145},{\"end\":44427,\"start\":44423},{\"end\":44439,\"start\":44436},{\"end\":44456,\"start\":44447},{\"end\":44468,\"start\":44464},{\"end\":44481,\"start\":44476},{\"end\":44499,\"start\":44492},{\"end\":44514,\"start\":44508},{\"end\":44530,\"start\":44524},{\"end\":44543,\"start\":44536},{\"end\":44560,\"start\":44556},{\"end\":44992,\"start\":44991},{\"end\":45009,\"start\":45001},{\"end\":45461,\"start\":45454},{\"end\":45486,\"start\":45478},{\"end\":45756,\"start\":45750},{\"end\":46039,\"start\":46033},{\"end\":46053,\"start\":46047},{\"end\":46070,\"start\":46063},{\"end\":46083,\"start\":46078},{\"end\":46097,\"start\":46091},{\"end\":46573,\"start\":46570},{\"end\":46586,\"start\":46583},{\"end\":46600,\"start\":46595},{\"end\":46613,\"start\":46609},{\"end\":46929,\"start\":46923},{\"end\":46943,\"start\":46937},{\"end\":46955,\"start\":46950},{\"end\":46970,\"start\":46964},{\"end\":47358,\"start\":47353},{\"end\":47373,\"start\":47367},{\"end\":47386,\"start\":47383},{\"end\":47388,\"start\":47387},{\"end\":47894,\"start\":47889},{\"end\":47903,\"start\":47902},{\"end\":47905,\"start\":47904},{\"end\":47922,\"start\":47921},{\"end\":47924,\"start\":47923},{\"end\":47936,\"start\":47933},{\"end\":47938,\"start\":47937},{\"end\":48298,\"start\":48292},{\"end\":48311,\"start\":48306},{\"end\":48326,\"start\":48320},{\"end\":48339,\"start\":48333},{\"end\":48351,\"start\":48348},{\"end\":48353,\"start\":48352}]", "bib_author_last_name": "[{\"end\":37629,\"start\":37627},{\"end\":37644,\"start\":37639},{\"end\":37658,\"start\":37655},{\"end\":37668,\"start\":37665},{\"end\":37998,\"start\":37995},{\"end\":38012,\"start\":38008},{\"end\":38026,\"start\":38022},{\"end\":38039,\"start\":38035},{\"end\":38304,\"start\":38299},{\"end\":38325,\"start\":38311},{\"end\":38338,\"start\":38332},{\"end\":38665,\"start\":38656},{\"end\":38673,\"start\":38669},{\"end\":38680,\"start\":38675},{\"end\":38923,\"start\":38918},{\"end\":38935,\"start\":38931},{\"end\":38944,\"start\":38942},{\"end\":38956,\"start\":38950},{\"end\":38961,\"start\":38958},{\"end\":39313,\"start\":39307},{\"end\":39329,\"start\":39323},{\"end\":39344,\"start\":39337},{\"end\":39603,\"start\":39593},{\"end\":39619,\"start\":39610},{\"end\":39638,\"start\":39632},{\"end\":39941,\"start\":39930},{\"end\":39951,\"start\":39947},{\"end\":39959,\"start\":39957},{\"end\":39976,\"start\":39970},{\"end\":39994,\"start\":39986},{\"end\":40003,\"start\":40001},{\"end\":40018,\"start\":40013},{\"end\":40035,\"start\":40027},{\"end\":40050,\"start\":40044},{\"end\":40069,\"start\":40060},{\"end\":40507,\"start\":40500},{\"end\":40792,\"start\":40787},{\"end\":40806,\"start\":40801},{\"end\":40819,\"start\":40815},{\"end\":40834,\"start\":40828},{\"end\":41168,\"start\":41162},{\"end\":41182,\"start\":41177},{\"end\":41196,\"start\":41191},{\"end\":41212,\"start\":41206},{\"end\":41223,\"start\":41220},{\"end\":41238,\"start\":41232},{\"end\":41580,\"start\":41576},{\"end\":41591,\"start\":41587},{\"end\":41614,\"start\":41601},{\"end\":41628,\"start\":41622},{\"end\":41650,\"start\":41644},{\"end\":41670,\"start\":41660},{\"end\":41686,\"start\":41672},{\"end\":42037,\"start\":42033},{\"end\":42047,\"start\":42045},{\"end\":42057,\"start\":42055},{\"end\":42069,\"start\":42065},{\"end\":42077,\"start\":42074},{\"end\":42089,\"start\":42086},{\"end\":42524,\"start\":42519},{\"end\":42536,\"start\":42534},{\"end\":42549,\"start\":42544},{\"end\":42560,\"start\":42556},{\"end\":42571,\"start\":42568},{\"end\":42585,\"start\":42579},{\"end\":42598,\"start\":42594},{\"end\":42953,\"start\":42947},{\"end\":42968,\"start\":42961},{\"end\":42979,\"start\":42975},{\"end\":43318,\"start\":43311},{\"end\":43331,\"start\":43325},{\"end\":43345,\"start\":43339},{\"end\":43774,\"start\":43767},{\"end\":43791,\"start\":43787},{\"end\":43803,\"start\":43798},{\"end\":43817,\"start\":43812},{\"end\":44123,\"start\":44119},{\"end\":44143,\"start\":44134},{\"end\":44155,\"start\":44150},{\"end\":44434,\"start\":44428},{\"end\":44445,\"start\":44440},{\"end\":44462,\"start\":44457},{\"end\":44474,\"start\":44469},{\"end\":44490,\"start\":44482},{\"end\":44506,\"start\":44500},{\"end\":44522,\"start\":44515},{\"end\":44534,\"start\":44531},{\"end\":44554,\"start\":44544},{\"end\":44567,\"start\":44561},{\"end\":44999,\"start\":44993},{\"end\":45015,\"start\":45010},{\"end\":45022,\"start\":45017},{\"end\":45476,\"start\":45462},{\"end\":45493,\"start\":45487},{\"end\":45764,\"start\":45757},{\"end\":46045,\"start\":46040},{\"end\":46061,\"start\":46054},{\"end\":46076,\"start\":46071},{\"end\":46089,\"start\":46084},{\"end\":46104,\"start\":46098},{\"end\":46581,\"start\":46574},{\"end\":46593,\"start\":46587},{\"end\":46607,\"start\":46601},{\"end\":46620,\"start\":46614},{\"end\":46935,\"start\":46930},{\"end\":46948,\"start\":46944},{\"end\":46962,\"start\":46956},{\"end\":46977,\"start\":46971},{\"end\":47365,\"start\":47359},{\"end\":47381,\"start\":47374},{\"end\":47395,\"start\":47389},{\"end\":47900,\"start\":47895},{\"end\":47910,\"start\":47906},{\"end\":47919,\"start\":47912},{\"end\":47931,\"start\":47925},{\"end\":47943,\"start\":47939},{\"end\":47951,\"start\":47945},{\"end\":48304,\"start\":48299},{\"end\":48318,\"start\":48312},{\"end\":48331,\"start\":48327},{\"end\":48346,\"start\":48340},{\"end\":48360,\"start\":48354}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":206594692},\"end\":37915,\"start\":37573},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3273340},\"end\":38232,\"start\":37917},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2133959},\"end\":38560,\"start\":38234},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":61019113},\"end\":38910,\"start\":38562},{\"attributes\":{\"doi\":\"arXiv:1711.03705\",\"id\":\"b4\"},\"end\":39212,\"start\":38912},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":30444761},\"end\":39521,\"start\":39214},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":195908774},\"end\":39871,\"start\":39523},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":2930547},\"end\":40366,\"start\":39873},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":733980},\"end\":40701,\"start\":40368},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":49301394},\"end\":41083,\"start\":40703},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":197618493},\"end\":41565,\"start\":41085},{\"attributes\":{\"doi\":\"arXiv:1805.06085\",\"id\":\"b11\"},\"end\":42022,\"start\":41567},{\"attributes\":{\"doi\":\"arXiv:1606.06160\",\"id\":\"b12\"},\"end\":42416,\"start\":42024},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":4752389},\"end\":42890,\"start\":42418},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b14\"},\"end\":43142,\"start\":42892},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":49907924},\"end\":43684,\"start\":43144},{\"attributes\":{\"id\":\"b16\"},\"end\":44025,\"start\":43686},{\"attributes\":{\"id\":\"b17\"},\"end\":44350,\"start\":44027},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":202786778},\"end\":44904,\"start\":44352},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":23376859},\"end\":45422,\"start\":44906},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":5855042},\"end\":45650,\"start\":45424},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":25293504},\"end\":45933,\"start\":45652},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":58027670},\"end\":46505,\"start\":45935},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":7861073},\"end\":46851,\"start\":46507},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":21351739},\"end\":47261,\"start\":46853},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":9812826},\"end\":47808,\"start\":47263},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":201900134},\"end\":48243,\"start\":47810},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1677864},\"end\":48669,\"start\":48245}]", "bib_title": "[{\"end\":37617,\"start\":37573},{\"end\":37984,\"start\":37917},{\"end\":38293,\"start\":38234},{\"end\":38646,\"start\":38562},{\"end\":39298,\"start\":39214},{\"end\":39586,\"start\":39523},{\"end\":39923,\"start\":39873},{\"end\":40491,\"start\":40368},{\"end\":40778,\"start\":40703},{\"end\":41153,\"start\":41085},{\"end\":42509,\"start\":42418},{\"end\":43302,\"start\":43144},{\"end\":44421,\"start\":44352},{\"end\":44989,\"start\":44906},{\"end\":45452,\"start\":45424},{\"end\":45748,\"start\":45652},{\"end\":46031,\"start\":45935},{\"end\":46568,\"start\":46507},{\"end\":46921,\"start\":46853},{\"end\":47351,\"start\":47263},{\"end\":47887,\"start\":47810},{\"end\":48290,\"start\":48245}]", "bib_author": "[{\"end\":37631,\"start\":37619},{\"end\":37646,\"start\":37631},{\"end\":37660,\"start\":37646},{\"end\":37670,\"start\":37660},{\"end\":38000,\"start\":37986},{\"end\":38014,\"start\":38000},{\"end\":38028,\"start\":38014},{\"end\":38041,\"start\":38028},{\"end\":38306,\"start\":38295},{\"end\":38327,\"start\":38306},{\"end\":38340,\"start\":38327},{\"end\":38667,\"start\":38648},{\"end\":38675,\"start\":38667},{\"end\":38682,\"start\":38675},{\"end\":38925,\"start\":38912},{\"end\":38937,\"start\":38925},{\"end\":38946,\"start\":38937},{\"end\":38958,\"start\":38946},{\"end\":38963,\"start\":38958},{\"end\":39315,\"start\":39300},{\"end\":39331,\"start\":39315},{\"end\":39346,\"start\":39331},{\"end\":39605,\"start\":39588},{\"end\":39621,\"start\":39605},{\"end\":39640,\"start\":39621},{\"end\":39943,\"start\":39925},{\"end\":39953,\"start\":39943},{\"end\":39961,\"start\":39953},{\"end\":39978,\"start\":39961},{\"end\":39996,\"start\":39978},{\"end\":40005,\"start\":39996},{\"end\":40020,\"start\":40005},{\"end\":40037,\"start\":40020},{\"end\":40052,\"start\":40037},{\"end\":40071,\"start\":40052},{\"end\":40509,\"start\":40493},{\"end\":40794,\"start\":40780},{\"end\":40808,\"start\":40794},{\"end\":40821,\"start\":40808},{\"end\":40836,\"start\":40821},{\"end\":41170,\"start\":41155},{\"end\":41184,\"start\":41170},{\"end\":41198,\"start\":41184},{\"end\":41214,\"start\":41198},{\"end\":41225,\"start\":41214},{\"end\":41240,\"start\":41225},{\"end\":41582,\"start\":41567},{\"end\":41593,\"start\":41582},{\"end\":41616,\"start\":41593},{\"end\":41630,\"start\":41616},{\"end\":41652,\"start\":41630},{\"end\":41672,\"start\":41652},{\"end\":41688,\"start\":41672},{\"end\":42039,\"start\":42024},{\"end\":42049,\"start\":42039},{\"end\":42059,\"start\":42049},{\"end\":42071,\"start\":42059},{\"end\":42079,\"start\":42071},{\"end\":42091,\"start\":42079},{\"end\":42526,\"start\":42511},{\"end\":42538,\"start\":42526},{\"end\":42551,\"start\":42538},{\"end\":42562,\"start\":42551},{\"end\":42573,\"start\":42562},{\"end\":42587,\"start\":42573},{\"end\":42600,\"start\":42587},{\"end\":42955,\"start\":42938},{\"end\":42970,\"start\":42955},{\"end\":42981,\"start\":42970},{\"end\":43320,\"start\":43304},{\"end\":43333,\"start\":43320},{\"end\":43347,\"start\":43333},{\"end\":43776,\"start\":43760},{\"end\":43793,\"start\":43776},{\"end\":43805,\"start\":43793},{\"end\":43819,\"start\":43805},{\"end\":43844,\"start\":43819},{\"end\":44125,\"start\":44115},{\"end\":44145,\"start\":44125},{\"end\":44157,\"start\":44145},{\"end\":44436,\"start\":44423},{\"end\":44447,\"start\":44436},{\"end\":44464,\"start\":44447},{\"end\":44476,\"start\":44464},{\"end\":44492,\"start\":44476},{\"end\":44508,\"start\":44492},{\"end\":44524,\"start\":44508},{\"end\":44536,\"start\":44524},{\"end\":44556,\"start\":44536},{\"end\":44569,\"start\":44556},{\"end\":45001,\"start\":44991},{\"end\":45017,\"start\":45001},{\"end\":45024,\"start\":45017},{\"end\":45478,\"start\":45454},{\"end\":45495,\"start\":45478},{\"end\":45766,\"start\":45750},{\"end\":46047,\"start\":46033},{\"end\":46063,\"start\":46047},{\"end\":46078,\"start\":46063},{\"end\":46091,\"start\":46078},{\"end\":46106,\"start\":46091},{\"end\":46583,\"start\":46570},{\"end\":46595,\"start\":46583},{\"end\":46609,\"start\":46595},{\"end\":46622,\"start\":46609},{\"end\":46937,\"start\":46923},{\"end\":46950,\"start\":46937},{\"end\":46964,\"start\":46950},{\"end\":46979,\"start\":46964},{\"end\":47367,\"start\":47353},{\"end\":47383,\"start\":47367},{\"end\":47397,\"start\":47383},{\"end\":47902,\"start\":47889},{\"end\":47912,\"start\":47902},{\"end\":47921,\"start\":47912},{\"end\":47933,\"start\":47921},{\"end\":47945,\"start\":47933},{\"end\":47953,\"start\":47945},{\"end\":48306,\"start\":48292},{\"end\":48320,\"start\":48306},{\"end\":48333,\"start\":48320},{\"end\":48348,\"start\":48333},{\"end\":48362,\"start\":48348}]", "bib_venue": "[{\"end\":37735,\"start\":37670},{\"end\":38064,\"start\":38041},{\"end\":38389,\"start\":38340},{\"end\":38719,\"start\":38682},{\"end\":39041,\"start\":38979},{\"end\":39360,\"start\":39346},{\"end\":39689,\"start\":39640},{\"end\":40111,\"start\":40071},{\"end\":40530,\"start\":40509},{\"end\":40884,\"start\":40836},{\"end\":41317,\"start\":41240},{\"end\":41773,\"start\":41704},{\"end\":42198,\"start\":42107},{\"end\":42645,\"start\":42600},{\"end\":42936,\"start\":42892},{\"end\":43404,\"start\":43347},{\"end\":43758,\"start\":43686},{\"end\":44113,\"start\":44027},{\"end\":44618,\"start\":44569},{\"end\":45157,\"start\":45024},{\"end\":45531,\"start\":45495},{\"end\":45788,\"start\":45766},{\"end\":46181,\"start\":46106},{\"end\":46671,\"start\":46622},{\"end\":47043,\"start\":46979},{\"end\":47489,\"start\":47397},{\"end\":48021,\"start\":47953},{\"end\":48444,\"start\":48362},{\"end\":38074,\"start\":38066},{\"end\":46243,\"start\":46183},{\"end\":47568,\"start\":47491}]"}}}, "year": 2023, "month": 12, "day": 17}