{"id": 235446706, "updated": "2023-10-06 08:03:37.033", "metadata": {"title": "Ditto: Fair and Robust Federated Learning Through Personalization", "authors": "[{\"first\":\"Tian\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Shengyuan\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Ahmad\",\"last\":\"Beirami\",\"middle\":[]},{\"first\":\"Virginia\",\"last\":\"Smith\",\"middle\":[]}]", "venue": "ICML", "journal": "6357-6368", "publication_date": {"year": 2020, "month": 12, "day": 8}, "abstract": "Fairness and robustness are two important concerns for federated learning systems. In this work, we identify that robustness to data and model poisoning attacks and fairness, measured as the uniformity of performance across devices, are competing constraints in statistically heterogeneous networks. To address these constraints, we propose employing a simple, general framework for personalized federated learning, Ditto, that can inherently provide fairness and robustness benefits, and develop a scalable solver for it. Theoretically, we analyze the ability of Ditto to achieve fairness and robustness simultaneously on a class of linear problems. Empirically, across a suite of federated datasets, we show that Ditto not only achieves competitive performance relative to recent personalization methods, but also enables more accurate, robust, and fair models relative to state-of-the-art fair or robust baselines.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2012.04221", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/00050BS21", "doi": null}}, "content": {"source": {"pdf_hash": "31949039a48961f939ac50440c3b4b8504fccceb", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2012.04221v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "0780d49b79131ff3307693639334dc9e5df2406c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/31949039a48961f939ac50440c3b4b8504fccceb.txt", "contents": "\nDitto: Fair and Robust Federated Learning Through Personalization\n\n\nTian Li \nShengyuan Hu \nAhmad Beirami \nVirginia Smith \nDitto: Fair and Robust Federated Learning Through Personalization\n\nFairness and robustness are two important concerns for federated learning systems. In this work, we identify that robustness to data and model poisoning attacks and fairness, measured as the uniformity of performance across devices, are competing constraints in statistically heterogeneous networks. To address these constraints, we propose employing a simple, general framework for personalized federated learning, Ditto, that can inherently provide fairness and robustness benefits, and develop a scalable solver for it. Theoretically, we analyze the ability of Ditto to achieve fairness and robustness simultaneously on a class of linear problems. Empirically, across a suite of federated datasets, we show that Ditto not only achieves competitive performance relative to recent personalization methods, but also enables more accurate, robust, and fair models relative to state-of-the-art fair or robust baselines.\n\nIntroduction\n\nFederated learning (FL) aims to collaboratively learn from data that has been generated by, and resides on, a number of remote devices or servers (McMahan et al., 2017). FL stands to produce highly accurate statistical models by aggregating knowledge from disparate data sources. However, to deploy FL in practice, it is necessary for the resulting systems to be not only accurate, but to also satisfy a number of pragmatic constraints regarding issues such as fairness, robustness, and privacy. Simultaneously satisfying these varied constraints can be exceptionally difficult (Kairouz et al., 2019).\n\nWe focus in this work specifically on issues of accuracy, fairness (i.e., limiting performance disparities across the network (Mohri et al., 2019)), and robustness (against trainingtime data and model poisoning attacks). Many prior efforts have separately considered fairness or robustness in federated learning. For instance, fairness strategies include using Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). minimax optimization to focus on the worst-performing devices (Mohri et al., 2019;Hu et al., 2020) or reweighting the devices to allow for a flexible fairness/accuracy tradeoff (Li et al., 2020e;2021). Robust methods commonly use techniques such as gradient clipping (Sun et al., 2019) or robust aggregation (Blanchard et al., 2017;Yin et al., 2018).\n\nWhile these approaches may be effective at either promoting fairness or defending against training-time attacks in isolation, we show that the constraints of fairness and robustness can directly compete with one another when training a single global model, and that simultaneously optimizing for accuracy, fairness, and robustness requires careful consideration. For example, as we empirically demonstrate (Section 4), current fairness approaches can render FL systems highly susceptible to training time attacks from malicious devices. On the other hand, robust methods may filter out rare but informative updates, causing unfairness (Wang et al., 2020).\n\nIn this work, we investigate a simple, scalable technique to simultaneously improve accuracy, fairness, and robustness in federated learning. While addressing the competing constraints of FL may seem like an insurmountable problem, we identify that statistical heterogeneity (i.e., non-identically distributed data) is a root cause for tension between these constraints, and is key in paving a path forward. In particular, we suggest that methods for personalized FL-which model and adapt to the heterogeneity in federated settings by learning distinct models for each device-may provide inherent benefits in terms of fairness and robustness.\n\nTo explore this idea, we propose Ditto, a scalable federated multi-task learning framework. Ditto can be seen as a lightweight personalization add-on for standard global FL. It is applicable to both convex and non-convex objectives, and inherits similar privacy and efficiency properties as traditional FL. We evaluate Ditto on a suite of federated benchmarks and show that, surprisingly, this simple form of personalization can in fact deliver better accuracy, robustness, and fairness benefits than state-of-the-art, problemspecific objectives that consider these constraints separately. We summarize our contributions below:\n\n\u2022 We propose Ditto, a multi-task learning objective for federated learning that provides personalization while retaining similar efficiency and privacy benefits as traditional FL. We provide convergence guarantees for our arXiv:2012.04221v3 [cs.LG] 15 Jun 2021\n\nproposed Ditto solver, which incorporate common practices in cross-device federated learning such as limited device participation and local updating. Despite its simplicity, we show that Ditto can deliver similar or superior accuracy relative to other common methods for personalized federated learning.\n\n\u2022 Next, we demonstrate that the benefits of Ditto go beyond accuracy-showing that the personalized objective can inherently offer robustness superior to that of common robust FL methods across a diverse set of data and model poisoning attacks. On average across all datasets and attacks, Ditto improves test accuracy by \u223c6% (absolute) over the strongest robust baseline.\n\n\u2022 Similarly, we show that Ditto can naturally increase fairness-reducing variance of the test accuracy across devices by \u223c10% while maintaining similar or superior accuracy relative to state-of-the-art methods for fair FL.\n\n\u2022 Finally, we highlight that Ditto is particularly useful for practical applications where we simultaneously care about multiple constraints (accuracy, fairness, and robustness).\n\nWe motivate this through analysis on a toy example in Section 3, as well as experiments across a suite of federated datasets in Section 4.\n\n\nBackground & Related Work\n\nRobustness and fairness are two broad areas of research that extend well beyond the application of federated learning. In this section we provide precise definitions of the notions of robustness/fairness considered in this work, and give an overview of prior work in robustness, fairness, and personalization in the context of federated learning.\n\nRobustness in Federated Learning. Training-time attacks (including data poisoning and model poisoning) have been extensively studied in prior work (Biggio et al., 2012;Gu et al., 2017;Chen et al., 2017;Shafahi et al., 2018;Liu et al., 2018;Huang et al., 2020;Xie et al., 2020;Wang et al., 2020;Dumford & Scheirer, 2018;Huang et al., 2020).\n\nIn federated settings, a number of strong attack methods have been explored, including scaling malicious model updates (Bagdasaryan et al., 2020), collaborative attacking (Sun et al., 2020), defense-aware attacks (Bhagoji et al., 2019;Fang et al., 2020), and adding edge-case adversarial training samples (Wang et al., 2020). Our work aims to investigate common attacks related to Byzantine robustness (Lamport et al., 2019), as formally described below.\n\nDefinition 1 (Robustness). We are conceptually interested in Byzantine robustness (Lamport et al., 2019), where the malicious devices can send arbitrary updates to the server to compromise training. To measure robustness, we assess the mean test performance on benign devices, i.e., we consider model w 1 to be more robust than w 2 to a specific attack if the mean test performance across the benign devices is higher for model w 1 than w 2 after training with the attack. We examine three widely-used attacks in our threat model:\n\n\u2022 (A1) Label poisoning: Corrupted devices do not have access to the training APIs and training samples are poisoned with flipped (if binary) or uniformly random noisy labels (Bhagoji et al., 2019;Biggio et al., 2011).\n\n\u2022 (A2) Random updates: Malicious devices send random zero-mean Gaussian parameters (Xu & Lyu, 2020).\n\n\u2022 (A3) Model replacement: Malicious devices scale their adversarial updates to make them dominate the aggregate updates (Bagdasaryan et al., 2020).\n\nWhile non-exhaustive, these attacks have been commonly studied in distributed and federated settings, and explore corruption at various points (the underlying data, labels, or model). In terms of defenses, robust aggregation is a common strategy to mitigate the effect of malicious updates (Blanchard et al., 2017;Pillutla et al., 2019;Sun et al., 2019;Li et al., 2019;He et al., 2020). Other defenses include gradient clipping (Sun et al., 2019) or normalization (Hu et al., 2020). While these strategies can improve robustness, they may also produce unfair models by filtering out informative updates, especially in heterogeneous settings (Wang et al., 2020). In our experiments (Section 4), we compare Ditto with several strong defenses (median, gradient clipping (Sun et al., 2019), Krum, Multi-Krum (Blanchard et al., 2017), gradient-norm based anomaly detector (Bagdasaryan et al., 2020), and a new defense proposed herein) and show that Ditto can improve both robustness and fairness compared with these methods.\n\nFairness in Federated Learning. Due to the heterogeneity of the data in federated networks, it is possible that the performance of a model will vary significantly across the devices. This concern, also known as representation disparity (Hashimoto et al., 2018), is a major challenge in FL, as it can potentially result in uneven outcomes for the devices. Following Li et al. (2020e), we provide a more formal definition of this fairness in the context of FL below:\n\nDefinition 2 (Fairness). We say that a model w 1 is more fair than w 2 if the test performance distribution of w 1 across the network is more uniform than that of w 2 , i.e., std {F k (w 1 )} k\u2208 [K] < std {F k (w 2 )} k\u2208 [K] where F k (\u00b7) denotes the test loss on device k\u2208 [K], and std{\u00b7} denotes the standard deviation. In the presence of adversaries, we measure fairness only on benign devices.\n\nWe note that there exists a tension between variance and utility in the definition above; in general, a common goal is to lower the variance while maintaining a reasonable average performance (e.g., average test accuracy). To address representation disparity, it is common to use minimax optimization (Mohri et al., 2019;Deng et al., 2020) or flexible sample reweighting approaches (Li et al., 2020e;2021) to encourage a more uniform quality of service. In all cases, by up-weighting the importance of rare devices or data, fair methods may not be robust in that they can easily overfit to corrupted devices (see Section 4.3). The tension between fairness and robustness has been studied in previous works, though for different notions of fairness (equalized odds) or robustness (backdoor attacks) (Wang et al., 2020), or in centralized settings (Chang et al., 2020). Recently, Hu et al. (2020) proposed FedMGDA+, a method targeting fair and robust FL; however, this work combines classical fairness (minimax optimization) and robustness (gradient normalization) techniques, in contrast to the multi-task framework proposed herein, which we show can inherently provide benefits with respect to both constraints simultaneously.\n\nPersonalized Federated Learning. Given the variability of data in federated networks, personalization is a natural approach used to improve accuracy. Numerous works have proposed techniques for personalized federated learning. Smith et al. (2017) first explore personalized FL via a primal-dual MTL framework, which applies to convex settings. Personalized FL has also been explored through clustering (e.g., Ghosh et al., 2020;Sattler et al., 2020;Muhammad et al., 2020), finetuning/transfer learning (Zhao et al., 2018;Yu et al., 2020), meta-learning (Jiang et al., 2019Chen et al., 2018;Khodak et al., 2019;Fallah et al., 2020;Li et al., 2020a;Singhal et al., 2021), and other forms of MTL, such as hard model parameter sharing (Agarwal et al., 2020;Liang et al., 2020) or the weighted combination method in Zhang et al. (2021). Our work differs from these approaches by simultaneously learning local and global models via a global-regularized MTL framework, which applies to non-convex ML objectives.\n\nSimilar in spirit to our approach are works that interpolate between global and local models (Mansour et al., 2020;Deng et al., 2021). However, as discussed in Deng et al. (2021), these approaches can effectively reduce to local minimizers without additional constraints. The most closely related works are those that regularize personalized models towards their average (Hanzely & Richt\u00e1rik, 2020;Hanzely et al., 2020;Dinh et al., 2020), which can be seen as a form of classical mean-regularized MTL (Evgeniou & Pontil, 2004). Our objective is similarly inspired by meanregularized MTL, although we regularize towards a global model rather than the average personalized model. As we discuss in Section 3, one advantage of this is that it allows for methods designed for the global federated learning problem (e.g., optimization methods, privacy/security mechanisms) to be easily re-used in our framework, with the benefit of additional personalization. We compare against a range of personalized methods empirically in Section 4.4, showing that Ditto achieves similar or superior performance across a number of common FL benchmarks.\n\nFinally, a key contribution of our work is jointly exploring the robustness and fairness benefits of personalized FL. The benefits of personalization for fairness alone have been demonstrated empirically in prior work (Wang et al., 2019;Hao et al., 2020). Connections between personalization and robustness have also been explored in Yu et al. (2020), although the authors propose using personalization methods on top of robust mechanisms. Our work differs from these works by arguing that MTL itself offers inherent robustness and fairness benefits, and exploring the challenges that exist when attempting to satisfy both constraints simultaneously.\n\n\nDitto: Global-Regularized Federated Multi-Task Learning\n\nIn order to explore the possible fairness/robustness benefits of personalized FL, we first propose a simple and scalable framework for federated multi-task learning. As we will see, this lightweight personalization framework is amenable to analyses while also having strong empirical performance. We explain our proposed objective, Ditto, in Section 3.1 and then present a scalable algorithm to solve it in federated settings (Section 3.2). We provide convergence guarantees for our solver, and explain several practical benefits of our modular approach in terms of privacy and efficiency. Finally, in Section 3.3, we characterize the benefits of Ditto in terms of fairness and robustness on a class of linear problems. We empirically explore the fairness and robustness properties against state-of-the-art baselines in Section 4.\n\n\nDitto Objective\n\nTraditionally, federated learning objectives consider fitting a single global model, w, across all local data in the network. The aim is to solve:\nmin w G(F 1 (w), . . . F K (w)) , (Global Obj)\nwhere F k (w) is the local objective for device k, and G(\u00b7) is a function that aggregates the local objectives {F k (w)} k\u2208 [K] from each device. For example, in FedAvg (McMahan et al., 2017), G(\u00b7) is typically set to be a weighted average of local losses, i.e., K k=1 p k F k (w), where p k is a pre-defined non-negative weight such that k p k = 1.\n\nHowever, in general, each device may generate data x k via a distinct distribution D k , i.e., F k (w) :\n= E x k \u223cD k [f k (w; x k )].\nTo better account for this heterogeneity, it is common to consider techniques that learn personalized, device-specific models, {v k } k\u2208 [K] across the network. In this work we explore personalization through a simple framework for federated multi-task learning. We consider two 'tasks': the global objective (Global Obj), and the local objective F k (v k ), which aims to learn a model using only the data of device k. To relate these tasks, we incorporate a regularization term that encourages the personalized models to be close to the optimal global model. The resulting bi-level optimization problem for each device k \u2208 [K] is given by:\nmin v k h k (v k ; w * ) := F k (v k ) + \u03bb 2 v k \u2212 w * 2 s.t. w * \u2208 arg min w G(F 1 (w), . . . F K (w))) .(Ditto)\nHere the hyperparameter \u03bb controls the interpolation between local and global models. When \u03bb is set to 0, Ditto is reduced to training local models; as \u03bb grows large, it recovers global model objective (Global Obj) (\u03bb \u2192 +\u221e).\n\nIntuition for Fairness/Robustness Benefits. In addition to improving accuracy via personalization, we argue that Ditto can offer fairness and robustness benefits. To reason about this, consider a simple case where data are homogeneous across devices. Without adversaries, learning a single global model is optimal for generalization. However, in the presence of adversaries, learning globally might introduce corruption, while learning local models may not generalize well due to limited sample size. Ditto with an appropriate value of \u03bb offers a tradeoff between these two extremes: the smaller \u03bb, the more the personalized models v k can deviate from the (corrupted) global model w, potentially providing robustness at the expense of generalization. In the heterogeneous case (which can lead to issues of unfairness as described in Section 2), a finite \u03bb exists to offer robustness and fairness jointly. We explore these ideas more rigorously in Section 3.3 by analyzing the tradeoffs between accuracy, fairness, and robustness in terms of \u03bb for a class of linear regression problems, and demonstrate fairness/robustness benefits of Ditto empirically in Section 4.\n\nOther Personalization Schemes. As discussed in Section 2, personalization is a widely-studied topic in FL. Our intuition in Ditto is that personalization, by reducing reliance on the global model, can reduce representation disparity (i.e., unfairness) and potentially improve robustness. It is possible that other personalization techniques beyond Ditto offer similar benefits: We provide some initial, encouraging results on this in Section 4.4. However, we specifically explore Ditto due to its simple nature, scalability, and strong empirical performance. Ditto is closely related to works that regularize personalized models towards their average (Hanzely & Richt\u00e1rik, 2020;Hanzely et al., 2020;Dinh et al., 2020), similar to classical mean-regularized MTL (Evgeniou & Pontil, 2004); Ditto differs by regularizing towards a global model rather than the average personalized model. We find that this provides benefits in terms of analysis (Section 3.3), as we can easily reason about Ditto relative to the global (\u03bb \u2192 \u221e) vs. local (\u03bb \u2192 0) baselines; empirically, in terms of accuracy, fairness, and robustness (Section 4); and practically, in terms of the modularity it affords our corresponding solver (Section 3.2).\n\nOther Regularizers. To encourage the personalized models v k to be close to the optimal global model w * , there are choices beyond the L 2 norm that could be considered, e.g., using a Bregman divergence-based regularizer or reshaping the L 2 ball using the Fisher information matrix. Under the logistic loss (used in our experiments), the Bregman divergence will reduce to KL divergence, and its second-order Taylor expansion will result in an L 2 ball reshaped with the Fisher information matrix. Remark (Relation to FedProx). We note that the L 2 term in Ditto bears resemblance to FedProx, a method which was developed to address heterogeneity in federated optimization (Li et al., 2020d). However, Ditto fundamentally differs from FedProx in that the goal is to learn personalized models v k , while FedProx produces a single global model w. For instance, when the regularization hyperparameter is zero, Ditto reduces to learning separate local models, whereas FedProx would reduce to FedAvg. In fact, Ditto is significantly more general than FedProx in that FedProx could be used as the global model solver in Ditto to optimize G(\u00b7). As discussed above, other regularizers beyond the L 2 norm may also be used in practice.\n\n\nDitto Solver\n\nTo solve Ditto, we propose jointly solving for the global model w * and personalized models {v k } k\u2208 [K] in an alternating fashion, as summarized in Algorithm 1. Optimization proceeds in two phases: (i) updates to the global model, w * , are computed across the network, and then (ii) the personalized models v k are fit on each local device. The process of optimizing w * is exactly the same as optimizing for any objective G(\u00b7) in federated settings: If we use iterative solvers, then at each communication round, each selected device can solve the local subproblem of G(\u00b7) approximately (Line 5). For personalization, device k solves the global-regularized local objective min v k h k (v k ; w t ) inexactly at each round (Line 6). Due to this alternating scheme, our solver can scale well to large networks, as it does not introduce additional communication or privacy overheads compared with existing solvers for G(\u00b7). In our experiments (all except Table 3), we use FedAvg as the objective and solver for G(\u00b7), under which we simply let device k run local SGD on F k (Line 5). We provide a simplified algorithm definition using FedAvg for the w * update in Algorithm 2 in the appendix.\n\nAlgorithm 1: Ditto for Personalized FL 1 Input: K, T , s, \u03bb, \u03b7, w 0 , {v 0 k } k\u2208[K] 2 for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do 3 Server randomly selects a subset of devices S t , and sends w t to them 4 for device k \u2208 S t in parallel do 5 Solve the local sub-problem of G(\u00b7) inexactly starting from w t to obtain w t k :\nw t k \u2190 UPDATE GLOBAL(w t , \u2207F k (w t )) / * Solve h k (v k ; w t ) * / 6 Update v k for s local iterations: v k = v k \u2212 \u03b7(\u2207F k (v k ) + \u03bb(v k \u2212 w t )) Send \u2206 t k := w t k \u2212 w t back 7 Server aggregates {\u2206 t k }: w t+1 \u2190 AGGREGATE w t , {\u2206 t k } k\u2208{St} 8 return {v k } k\u2208[K] (personalized), w T (global)\nWe note that another natural choice to solve the Ditto objective is to first obtain w * , and then for each device k, perform finetuning on the local objective min v k h k (v k ; w * ).\n\nThese two approaches will arrive at the same solutions in strongly convex cases. In non-convex settings, we observe that there may be additional benefits of joint optimization: Empirically, we find that the updating scheme tends to guide the optimization trajectory towards a better solution compared with finetuning starting from w * , particularly when w * is corrupted by adversarial attacks (Section 4.4). Intuitively, under training-time attacks, the global model may start from a random one, get optimized, and gradually become corrupted as training proceeds (Li et al., 2020b). In these cases, feeding in early global information (i.e., before the global model converges to w * ) may be helpful under strong attacks.\n\nWe note that Ditto with joint optimization requires the devices to maintain local states (i.e., personalized models) and carry these local states to the next communication round where they are selected. Solving Ditto with finetuning does not need devices to be stateful, while losing the benefits of alternate updating discussed above.\n\nModularity of Ditto. From the Ditto objective and Alg 1, we see that a key advantage of Ditto is its modularity, i.e., that we can readily use prior art developed for the Global Obj along with the personalization add-on of h k (v k ; w * ), as highlighted in red. This has several benefits:\n\n\u2022 Optimization: It is possible to plug in other methods beyond FedAvg (e.g., Li et al., 2020c;Karimireddy et al., 2020;Reddi et al., 2021) in Algorithm 1 to update the global model, and inherit the convergence benefits, if any (we make this more precise in Theorem 1). \u2022 Privacy: Ditto communicates the same information over the network as typical FL solvers for the global objective, thus preserving whatever privacy or communication benefits exist for the global objective and its respective solver. This is different from most other personalization methods where global model updates depend on local parameters, which may raise privacy concerns (London, 2020). \u2022 Robustness: Beyond the inherent robustness benefits of personalization, robust global methods can be used with Ditto to further improve performance (see Section 4.4).\n\nIn particular, while not the main focus of our work, we note that Ditto may offer a better privacy-utility tradeoff than training a global model. For instance, when training Ditto, if we fix the number of communication rounds and add the same amount of noise per round to satisfy differential privacy, Ditto consumes exactly the same privacy budget as normal global training, while yielding higher accuracy via personalization (Section 4). Similar benefits have been studied, e.g., via finetuning strategies (Yu et al., 2020).\n\nConvergence of Algorithm 1. Note that optimizing the global model w t does not depend on any personalized models {v k } k\u2208 [K] . Therefore, w enjoys the same global convergence rates with the solver we use for G. Under this observation, we present the local convergence of Algorithm 1. Theorem 1 (Local Convergence of Alg. 1; formal statement and proof in Theorem 10). Assume for k \u2208 [K], F k is strongly convex and smooth, under common assumptions, if w t converges to w * with rate g(t), then there exists a constant C<\u221e such that for \u03bb \u2208 R, and for k \u2208 [K], v t k converges to v * k := arg min v k h k (v k ; w * ) with rate Cg(t). Using Theorem 1, we can directly plug in previous convergence analyses for any G(\u00b7). For instance, when the global objective and its solver are those of FedAvg, we can obtain an O(1/t) convergence rate for Ditto under suitable conditions (Corollary 1). We provide a full theorem statement and proof of convergence in Appendix B.\n\n\nAnalyzing the Fairness/Robustness Benefits of Ditto in Simplified Settings\n\nIn this section, we more rigorously explore the fairness/robustness benefits of Ditto on a class of linear problems. Throughout our analysis, we assume G(\u00b7) is the standard objective in FedAvg (McMahan et al., 2017).\n\n\nPoint Estimation.\n\nTo provide intuition, we first examine a toy one-dimensional point estimation problem. Denote the underlying models for the devices as {v k } k\u2208 [K] , v k \u2208 R, and let the points on device k, {x k,1 , . . . , x k,n } 1 , be observations of v k with random perturbation, i.e., x k,i = v k +z k,i , where z k,i \u223c N (0, \u03c3 2 ) and are IID. Assume v k \u223c N (\u03b8, \u03c4 2 ), where \u03b8 is drawn from the uniform uninformative prior on R, and \u03c4 is a known constant. Here, \u03c4 controls the degree of relatedness of the data on different devices: \u03c4 =0 captures the case where the data on all devices are identically distributed while \u03c4 \u2192 \u221e results in the scenario where the data on different devices are completely unrelated. The local objective is\nmin v k F k (v k ) = 1 2 (v k \u2212 1 n k n k i=1 x k,i ) 2 .\nIn the presence of adversaries, we look at a specific type of label poisoning attack. Let K a denote the number of malicious devices, and the 'capability' of an adversary is modeled by \u03c4 a , i.e., the underlying model of an adversary follows N (\u03b8, \u03c4 2 a ) where \u03c4 2 a > \u03c4 2 . We first derive the Bayes estimator (which will be the most accurate and robust) for the real model distribution by observing a finite number of training points. Then, we show that by solving Ditto, we are able to recover the Bayes estimator with a proper \u03bb * (with the knowledge of \u03c4 ). In addition, the same \u03bb * results in the most fair solution among the set of solutions of Ditto parameterized by \u03bb. This shows that Ditto with a proper choice of \u03bb is Bayes optimal for this particular problem instance. In general, in Theorem 8 (appendix), we prove that\n\u03bb * = \u03c3 2 n K K\u03c4 2 + Ka K\u22121 (\u03c4 2 a \u2212 \u03c4 2 )\n.\n\nWe see that \u03bb * decreases when (i) there are more local samples n, (ii) the devices are less related (larger \u03c4 ), or (iii) the attacks are stronger (larger number of attackers, K a , and more powerful adversaries, \u03c4 a ). Related theorems (Theorem 6-9) are presented in Appendix A.3. Figure 1. Empirically, the \u03bb * given by Theorem 6-9 results in the most accurate, fair, and robust solution within Ditto's solution space. \u03bb * is also optimal in terms of accuracy and robustness among any possible federated estimation algorithms.\n\nIn Figure 1, we plot average test error, fairness (standard 1 For ease of notation, we assume each device has the same number of training samples. It is straightforward to extend the current analysis to allow for varying number of samples per device. Figure 2. Impact of data relatedness across all devices. When 1/\u03c4 is small (less related), local outperforms global; when 1/\u03c4 is large (more related), global is better than local. Ditto (\u03bb * ) achieves the lowest test error and variance (measured across benign devices). deviation shown as error bars), and robustness (test error in the adversarial case) across a set of \u03bb's for both clean and adversarial cases. We see that in the solution space of Ditto, there exists a specific \u03bb which minimizes the average test error and standard deviation across all devices at the same time, which is equal to the optimal \u03bb * given by our theory. Figure 2 shows (i) Ditto with \u03bb * is superior than learning local or global models, and (ii) \u03bb * should increase as the relatedness between devices (1/\u03c4 ) increases.\n\nLinear Regression. All results discussed above can be generalized to establish the optimality of Ditto on a class of linear regression problems (with additional assumptions on feature covariance). We defer readers to Appendix A.2 for full statements and proofs. While our analyses here are limited to a simplified set of attacks and problem settings, we build on this intuition in Section 4-empirically demonstrating the accuracy, robustness, and fairness benefits of Ditto using both convex and non-convex models, across a range of federated learning benchmarks, and under a diverse set of attacks.\n\n\nExperiments\n\nIn this section, we first demonstrate that Ditto can inherently offer similar or superior robustness relative to strong robust baselines (Section 4.1). We then show it results more fair performance than recent fair methods (Section 4.2). Ditto is particularly well-suited for mitigating the tension between these constraints and achieving both fairness and robustness simultaneously (Section 4.3). We explore additional beneficial properties of Ditto in Section 4.4.\n\nSetup. For all experiments, we measure robustness via test accuracy, and fairness via test accuracy variance (or standard deviation), both across benign devices (see Def. 1, 2). We use datasets from common FL benchmarks (Caldas et al., 2018;Smith et al., 2017;TFF), which cover both vision and language tasks, and convex and non-convex models. Detailed datasets and models are provided in Table 4 in Appendix C. We split local data on each device into train/test/validation sets randomly, and measure performance on the test data. For each device, we select \u03bb locally based on its local validation data. We further assume the devices can make a binary decision on whether the attack is strong or not. For devices with very few validation samples (less than 4), we use a fixed small \u03bb (\u03bb=0.1) for strong attacks, and use a fixed relatively large \u03bb (\u03bb=1) for all other attacks. For devices with more than 5 validation data points, we let each select \u03bb from {0.05, 0.1, 0.2} for strong attacks, and select \u03bb from {0.1, 1, 2} for all other attacks. See Appendix D.2 for details. More advanced tuning methods are left for future work. Our code, data, and experiments are publicly available at github.com/litian96/ditto.\n\n\nRobustness of Ditto\n\nFollowing our threat model described in Definition 1, we apply three attacks to corrupt a random subset of devices. We pick corruption levels until a point where there is a significant performance drop when training a global model. We compare robustness (Def. 1) of Ditto with various defense baselines, presenting the results of three strongest defenses in Figure 3. Execution details and full results are reported in Appendix D.4. As shown in Figure 3, Ditto achieves the highest accuracy under most attacks, particularly those with a large fraction of malicious devices. On average across all datasets and attacks, Ditto results in \u223c6% absolute accuracy improvement compared with the strongest robust baseline (Appendix D.4). In scenarios where a robust baseline outperforms Ditto, we have also found that replacing the global objective and its solver (FedAvg) with a robust version (e.g., using robust aggregators) can further improve Ditto, yielding superior performance (Section 4.4).\n\n\nFairness of Ditto\n\nTo explore the fairness of Ditto, we compare against TERM (Li et al., 2021) as a baseline. It is an improved version of the q-FFL (Li et al., 2020e) objective, which has been recently proposed for fair federated learning. TERM also recovers AFL (Mohri et al., 2019), another fair FL objective, as a special case. TERM uses a parameter t to offer flexible tradeoffs between fairness and accuracy. In Table 1, we compare the proposed objective with global, local, and fair methods (TERM) in terms of test accuracies and standard deviation. When the corruption level is high, 'global' or 'fair' will even fail to converge. Ditto results in more accurate and fair solutions both with and without attacks. On average across all datasets, Ditto reduces variance across devices by \u223c10% while improving absolute test accuracy by 5% compared with TERM (on clean data).\n\n\nAddressing Competing Constraints\n\nIn this section, we examine the competing constraints between robustness and fairness. When training a single global model, fair methods aim to encourage a more uniform performance distribution, but may be highly susceptible to training-time attacks in statistically heterogeneous environments. We investigate the test accuracy on benign devices when learning global, local, and fair models. In the TERM objective, we set t = 1, 2, 5 to achieve different levels of fairness (the higher, the fairer). We perform the data poisoning attack (A1 in Def. 1). The results are plotted in Figure 4. As the corruption level increases, we see that fitting a global model becomes less robust. Using fair methods will be more susceptible to attacks. When t gets larger, the test accuracy gets lower, an indication that the fair method is overfitting to the corrupted devices relative to the global baseline.\n\nNext, we apply various strong robust methods under the same attack, and explore the robustness/accuracy and fairness performance. The robust approaches include: Krum, multi-Krum (Blanchard et al., 2017), taking the coordinatewise median of gradients ('median'), gradient clipping ('clipping'), filtering out the gradients with largest norms ('k-norm'), and taking the gradient of the k-th largest loss where k is the number of malicious devices ('k-loss'). For Krum, multi-Krum, k-norm, and k-loss, we assume that Table 1. Average (standard deviation) test accuracy to benchmark performance and fairness (Definition 2) on Fashion MNIST and FEMNIST. Ditto is either (i) more fair compared with the baselines of training a global model, or (ii) more accurate than the fair baseline under a set of attacks. We bold the method with highest average minus standard deviation across all methods.\n\n\nFashion\n\nA1 (ratio of adversaries) A2 (ratio of adversaries) A3 (ratio of adversaries)\nMethods clean 20% 50% 80% 20% 50% 80% 10% 20% 50%\nglobal .911 (.08) .897 (.08) .855 (.10) .753 (.13) .900 (.08) .882 (.09) .857 (.10) .753 (.10) .551 (.13) .275 (.12) local\n\n.876 (.10) .874 (.10) .876 (.11) .879 (.10) .874 (.10) .876 (.11) .879 (.10) .877 (.10) .874 (.10) .876 (.11) fair (TERM, t=1) .909 (.07) .751 (.12) .637 (.13) .547 (.11) .731 (.13) .637 (.14) .635 (.14) .653 (.13) .601 (.12) .131 (.16) Ditto\n\n.943 (.06) .944 (.07) .937 (.07) .907 (.10) .938 (.07) .930 (.08) .913 (.09) .921 (.09) .902 (.09) .873 (.11) FEMNIST A1 (ratio of adversaries) A2 (ratio of adversaries) A3 (ratio of adversaries)\nMethods clean 20% 50% 80% 20% 50% 80% 10% 15% 20%\nglobal .804 (.11) .773 (.11) .727 (.12) .574 (.15) .774 (.11) .703 (.14) .636 (.15) .517 (.14) .487 (.14) .314 (.13) local\n\n.628 (.15) .620 (.14) .627 (.14) .607 (.14) .620 (.14) .627 (.14) .607 (.14) .622 (.14) .621 (.14) .620 (.14) fair (TERM, t=1) .809 (.11) .636 (.15) .562 (.13) .478 (.12) .440 (.15) .336 (.12) .363 (.12) .353 (.12) .316 (.12) .299 (.11) Ditto\n\n.834 (.09) .802 (.10) .762 (.11) .672 (.13) .801 (.09) .700 (.15) .675 (.14) .685 (.15) .650 (.14) .613 (.13) Figure 4. Fair methods can overfit to corrupted devices (possibly with large training losses) by imposing more weights on them, thus being particularly susceptible to attacks. Figure 5. Compared with learning a global model, robust baselines (i.e., the methods listed in the figure excluding 'global' and 'Ditto') are either robust but not fair (with higher accuracy, larger variance), or not even robust (with lower accuracy). Ditto lies at the lower right corner, which is our preferred region.\n\nthe server knows the expected number of malicious devices that are selected each round, and can set k accordingly for k-norm and k-loss. From Figure 5, we see that robust baselines are either (i) more robust than global but less fair, or (ii) fail to provide robustness due to heterogeneity. Ditto is more robust, accurate, and fair.\n\n\nAdditional Properties of Ditto\n\nPersonalization. We additionally explore the performance of other personalized FL methods in terms of accuracy and fairness, on both clean and adversarial cases. In particular, we consider objectives that ( We compare Ditto with the above alternatives, using the same learning rate tuned on FedAvg on clean data for all methods except Per-FedAvg, which requires additional tuning to prevent divergence. For finetuning methods (EWC and SKL), we finetune on each local device for 50 epochs starting from the converged global model. We report results of baseline methods using their best hyperparameters. Despite Ditto's simplicity, in Table 2 below, we see that Ditto achieves similar or superier test accuracy with slightly lower standard deviation compared with these recent personalization methods.\n\nWe also evaluate the performance of MOCHA with a convex SVM model in Table 7 in the appendix. MOCHA is more robust and fair than most baselines, which is in line with our reasoning that personalization can provide benefits for these constraints. Further understanding the robustness/fairness benefits of other personalized approaches would be an interesting direction of future work. .804 (.11) .911 (.19) .727 (.12) .538 (.28) local\n\n.628 (.15) .692 (.27) .627 (.14) .682 (.27) plain finetuning .815 (.09) .912 (.18) .734 (.12) .721 (.28) L2SGD\n\n.817 (.10) .899 (.18) .732 (.15) .725 (.25) EWC\n\n.810 (.11) .910 (.18) .756 (.12) .642 (.26) SKL\n\n.820 (.10) .915 (.16) .752 (.12) .708 (.27) Per-FedAvg (HF) .827 (.09) .907 (.17) .604 (.14) .756 (.26) mapper .792 (.12) .773 (.25) .726 (.13) .704 (.27) APFL\n\n.811 (.11) .911 (.17) .750 (.11) .710 (.27) Ditto\n\n.836 (.10) .914 (.18) .767 (.10) .721 (.27) Augmenting with Robust Baselines. Ditto allows the flexibility of learning robust w * leveraging any previous robust aggregation techniques, which could further improve the performance of personalized models. For instance, in the aggregation step at the server side (Line 7 in Algorithm 1), instead of simply averaging the global model updates as in FedAvg, we can aggregate them via multi-Krum, or after gradient clipping. As is shown in Table 3, Ditto combined with clipping yields improvements compared with vanilla Ditto. We present full results on different datasets trying varying robust methods in Table 6 in the appendix.  Comparing Two Solvers. As mentioned in Section 3.2, another way to solve Ditto is to finetune on min v k h k (v k ; w * ) for each k \u2208 [K] after obtaining w * . We examine the performance of two solvers under the model replacement attack (A3) with 20% adversaries. In realistic federated networks, it may be challenging to determine how many iterations to finetune for, particularly over a heterogeneous network of devices. To obtain the best performance of finetuning, we solve min v k h k (v k ; w * ) on each device by running different iterations of mini-batch SGD and pick the best one. As shown in Figure 6, the finetuning solver improves the performance compared with learning a global model, while Ditto combined with joint optimization performs the best. One can also perform finetuning after early stopping; however, it is essentially solving a different objective and it is difficult to determine the stopping criteria. We discuss this in more detail in Appendix D.1.\n\n\nConclusion and Future Work\n\nWe propose Ditto, a simple MTL framework, to address the competing constraints of accuracy, fairness, and robustness in federated learning. Ditto can be thought of as a lightweight personalization add-on for any global federated objective, which maintains the privacy and communication efficiency of the global solver. We theoretically analyze the ability of Ditto to mitigate the tension between fairness and robustness on a class of linear problems. Our empirical results demonstrate that Ditto can result in both more robust and fairer models compared with strong baselines across a diverse set of attacks. Our work suggests several interesting directions of future study, such as exploring the applicability of Ditto to other attacks such as backdoor attacks (e.g., Sun et al., 2019); understanding the fairness/robustness properties of other personalized methods; and considering additional constraints, such as privacy.\n\nlearning. arXiv preprint arXiv:2003.12880, 2020. Chang, H., Nguyen, T. D., Murakonda, S. K., Kazemi, E., and Shokri, R. On adversarial bias and the robustness of fair machine learning. arXiv preprint arXiv:2006.08669, 2020.\n\nChen, F., Luo, M., Dong, Z., Li, Z., and He, X. Federated meta-learning with fast convergence and efficient communication. arXiv preprint arXiv:1802.07876, 2018.\n\nChen, X., Liu, C., Li, B., Lu, K., and Song, D. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.\n\nCohen, G., Afshar, S., Tapson, J., and van Schaik, A. Emnist: an extension of mnist to handwritten letters. arXiv preprint arXiv:1702.05373, 2017. Fallah, A., Mokhtari, A., and Ozdaglar, A. Personalized federated learning: A meta-learning approach. In Advances in Neural Information Processing Systems, 2020.\n\nFang, M., Cao, X., Jia, J., and Gong, N. Local model poisoning attacks to byzantine-robust federated learning.\n\nIn U SEN IX Security Symposium, 2020.\n\nFinn, C., Abbeel, P., and Levine, S. Model-agnostic metalearning for fast adaptation of deep networks. In International Conference on Machine Learning, 2017.\n\nGhosh, A., Chung, J., Yin, D., and Ramchandran, K. An efficient framework for clustered federated learning. In Advances in Neural Information Processing Systems, 2020.\n\nGu, T., Dolan-Gavitt, B., and Garg, S. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.\n\nHanzely, F. and Richt\u00e1rik, P. Federated learning of a mixture of global and local models. arXiv preprint arXiv:2002.05516, 2020.\n\nHanzely, F., Hanzely, S., Horv\u00e1th, S., and Richt\u00e1rik, P. Lower bounds and optimal algorithms for personalized federated learning. Advances in Neural Information Processing Systems, 2020.\n\nHao, W., Mehta, N., Liang, K. J., Cheng, P., El-Khamy, M., and Carin, L. Waffle: Weight anonymized factorization for federated learning. arXiv preprint arXiv:2008.05687, 2020.\n\nHashimoto, T., Srivastava, M., Namkoong, H., and Liang, P. Fairness without demographics in repeated loss minimization. In International Conference on Machine Learning, 2018.\n\nHe, L., Karimireddy, S. P., and Jaggi, M. Byzantine-robust learning on heterogeneous datasets via resampling. In NeurIPS Workshop on Scalability, Privacy, and Security in Federated Learning, 2020.\n\nHu, Z., Shaloudegi, K., Zhang, G., and Yu, Y. FedMGDA+: Federated learning meets multi-objective optimization. arXiv preprint arXiv:2006.11489, 2020.\n\nHuang, W. R., Geiping, J., Fowl, L., Taylor, G., and Goldstein, T. Metapoison: Practical general-purpose cleanlabel data poisoning. In Advances in Neural Information Processing Systems, 2020.\n\nJiang, H., He, P., Chen, W., Liu, X., Gao, J., and Zhao, T. SMART: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020.\n\nJiang, Y., Kone\u010dn\u1ef3, J., Rush, K., and Kannan, S. Improving federated learning personalization via model agnostic meta learning. arXiv preprint arXiv:  Xu, X. and Lyu, L. Towards building a robust and fair federated learning system. arXiv preprint arXiv:2011.10464, 2020.\n\nYin, D., Chen, Y., Kannan, R., and Bartlett, P. Byzantinerobust distributed learning: Towards optimal statistical rates. In International Conference on Machine Learning, 2018.\n\nYu, T., Bagdasaryan, E., and Shmatikov, V. Salvaging federated learning by local adaptation. arXiv preprint arXiv:2002.04758, 2020.\n\nZhang, M., Sapra, K., Fidler, S., Yeung, S., and Alvarez, J. M. Personalized federated learning with first order model optimization. In International Conference on Learning Representations, 2021.\n\nZhao, Y., Li, M., Lai, L., Suda, N., Civin, D., and Chandra, V. Federated learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018.\n\nWe provide a simple table of contents below for easier navigation of the appendix. \n\n\nCONTENTS Section A: Analysis of the Federated Multi-Task Learning Objective Ditto\n\n\nA. Analysis of the Federated Multi-Task Learning Objective Ditto\n\nHere, we provide theoretical analyses of Ditto, mainly on a class of linear models. In this linear setting, we investigate accuracy, fairness, and robustness of Ditto. We first discuss some general properties of Ditto for strongly convex functions in terms of the training performance in Section A.1. We next present our main results on characterizing the benefits (accuracy, fairness, and robustness) of Ditto on linear regression in Section A.2. Finally, we present results on a special case of linear regression (federated point estimation problem examined in Section 3.3) in Section A.3.\n\n\nA.1. Properties of Ditto for Strongly Convex Functions\n\nLet the Ditto objective on device k be\nh k (w) = F k (w) + \u03bb\u03c8(w),(1)\nwhere F k is strongly convex, and\n\u03c8(w) := 1 2 w \u2212 w * 2 ,(2)w * := arg min w \uf8f1 \uf8f2 \uf8f3 1 K k\u2208[K] F k (w) \uf8fc \uf8fd \uf8fe .(3)\nLet\nw k (\u03bb) = arg min w h k (w).(4)\nWithout any distributional assumptions on the tasks, we first characterize the solutions of the objective h k (w).\n\nLemma 1. For all \u03bb \u2265 0,\n\u2202 \u2202\u03bb F k ( w k (\u03bb)) \u2265 0,(5)\n\u2202 \u2202\u03bb \u03c8( w k (\u03bb)) \u2264 0.\n\nIn addition, for all k, if F k (w * ) is finite, then lim \u03bb\u2192\u221e w k (\u03bb) = w * .\n\nProof. The proof here directly follows the proof in Hanzely & Richt\u00e1rik (Theorem 3.1, 2020).\n\nAs \u03bb increases, the local empirical training loss F k ( w k (\u03bb)) will also increase, and the resulting personalized models will be closer to the global model. Therefore, \u03bb effectively controls how much personalization we impose. Since for any device k \u2208 [K], training loss is minimized when \u03bb = 0, training separate local models is the most robust and fair in terms of training performance when we do not consider generalization.\n\nHowever, in order to obtain the guarantees on the test performance, we need to explicitly model the joint distribution of data on all devices. In the next section, we explore a Bayesian framework on a class of linear problems to examine the generalization, fairness, and robustness of the Ditto objective, all on the underlying test data.\n\n\nA.2. Federated Linear Regression\n\nWe first examine the case without corrupted devices in Section A.2.1. We prove that there exists a \u03bb that results in an optimal average test performance among all possible federated learning algorithms, which coincides with the optimal \u03bb in Ditto's solution space in terms of fairness. When there are adversaries, we analyze the robustness benefits of Ditto in Section A.2.2. In particular, we show there exists a \u03bb which leads to the highest test accuracy across benign devices (i.e., the most robust) and minimizes the variance of the test error across benign devices (i.e., the most fair) jointly.\n\nBefore we proceed, we first state a technical lemma that will be used throughout the analyses. Lemma 2. Let \u03b8 be drawn from the non-informative uniform prior on R d . Further, let {\u03c6 k } k\u2208[K] denote noisy observations of \u03b8 with additive zero-mean independent Gaussian noises with covariance matrices {\u03a3 k } k\u2208 [K] . Let\n\u03a3 \u03b8 := \uf8eb \uf8ed k\u2208[K] \u03a3 \u22121 k \uf8f6 \uf8f8 \u22121 .(8)\nThen, conditioned on {\u03c6 k } k\u2208[K] , we can write \u03b8 as \n\u03b8 = \u03a3 \u03b8 k\u2208[K] \u03a3 \u22121 k \u03c6 k + z, where z is N (0, \u03a3 \u03b8 ) which is independent of {\u03c6 k } k\u2208[K] .\nThen, conditioned on {\u03c6 k } k\u2208[K] , we can write \u03b8 as\n\u03b8 = \u03c3 2 \u03b8 k\u2208[K] \u03c6 k \u03c3 2 k + z, where z is N (0, \u03c3 2 \u03b8 ) which is independent of {\u03c6 k } k\u2208[K] .\n\nA.2.1. NO ADVERSARIES: DI T T O FOR ACCURACY AND FAIRNESS\n\nWe consider a Bayesian framework. Let \u03b8 be drawn from the non-informative prior on R d , i.e., uniformly distributed on R d . We assume that K devices have their data distributed with parameters {w k } k\u2208 [K] :\nw k = \u03b8 + \u03b6 k ,(10)\nwhere \u03b6 k \u223c N (0, \u03c4 2 I d ) are I.I.D, and I d denotes the d \u00d7 d identity matrix. \u03c4 controls the degree of dependence between the tasks on different devices. If \u03c4 = 0, then the data on all devices is distributed according to parameter \u03b8, i.e., the tasks are the same, and if \u03c4 \u2192 \u221e, the tasks on different devices become completely unrelated.\n\nWe first derive optimal estimators {w k } k\u2208 [K] for each device w k given observations {X i , y i } i\u2208 [K] . Lemma 4. Assume that we have y = Xw + z\n\nwhere y \u2208 R n , X \u2208 R n\u00d7d , and w \u2208 R d , and z \u2208 R n . Further assume that z \u223c N (0, \u03c3 2 I d ) and w follows the non-informative uniform prior on R d . Let w = (X T X) \u22121 X T y.\n\nThen, we have w follows a multi-variate normal distribution as follows:\n\nw \u223c N (X T X) \u22121 X T y, \u03c3 2 (X T X) \u22121 .\n\nLemma 5. Let\nw i := (X T i X i ) \u22121 X T i y i .(14)\nLet\n\u03a3 i := \u03c3 2 (X T i X i ) \u22121 + \u03c4 2 I d .(15)\nFurther, let\n\u03a3 \\k \u03b8 := \uf8eb \uf8ed i\u2208[K],i =k \u03a3 \u22121 i \uf8f6 \uf8f8 \u22121 .(16)\nFurther let \u00b5\n\\k \u03b8 := \u03a3 \\k \u03b8 i\u2208[K],i =k \u03a3 \u22121 i w i(17)\nThen, conditioned on {X i , y i } i\u2208[K],i =k , we can write \u03b8 as\n\u03b8 = \u00b5 \\k \u03b8 + \u03b7, where \u03b7 is N (0, \u03a3 \\k \u03b8 ) which is independent of {X i , y i } i\u2208[K],i =k .\nProof. From Lemma 4, we know w i is a noisy observation of the underlying w i with additive covariance \u03c3 2 (X T i X i ) \u22121 . For {w k } k\u2208[K] defined in our setup, w i is a noisy observation of \u03b8 with additive zero mean and covariance \u03a3 i :=\n\u03c4 2 I d + \u03c3 2 (X T i X i ) \u22121 . The proof completes by applying Lemma 2 to { w i } i\u2208[K],i =k . Lemma 6. Let \u03a3 \\k w k := \u03a3 \\k \u03b8 + \u03c4 2 I d .(18)\nFurther, let\n\u03a3 w k := (\u03a3 \\k w k ) \u22121 + (\u03a3 k \u2212 \u03c4 2 I d ) \u22121 \u22121 .(19)\nConditioned on {X i , y i } i\u2208[K] , we have\nw k = \u03a3 w k (\u03a3 k \u2212 \u03c4 2 I d ) \u22121 w k + \u03a3 w k (\u03a3 \\k w k ) \u22121 \u00b5 \\k \u03b8 + \u03b6 k ,(20)\nwhere \u03b6 k \u223c N (0, \u03a3 w k ).\n\nProof. w k is a noisy observation of w k with additive noise with zero mean and covariance \u03c3 2 (X T k X k ) \u22121 (which is \u03a3 k \u2212 \u03c4 2 I d ). From Lemma 5, we know conditioned on {X i , y i } i\u2208[K],i =k , \u00b5 \\k \u03b8 is a noisy observation of \u03b8 with covariance \u03a3 \\k \u03b8 . Hence, with respect to w k , the covariance is \u03a3 \\k \u03b8 + \u03c4 2 I d := \u03a3 \\k w k . The conclusion follows by applying Lemma 2 to w k and \u00b5 \\k \u03b8 .\n\nLet the empirical loss function of the linear regression problem on device k be\nF k (w) = 1 n X k w \u2212 y k 2 .(21)\nThen the estimator w k is (X T k X k ) \u22121 X T y k . Applying the previous lemmas, we obtain an optimal estimator w k given all training samples from K devices (see (20)). w k is Bayes optimal among all solutions that can be achieved by any learning method. Next, we examine the Ditto objective and its solution space parameterized by \u03bb.\n\nLet each device solve the following objective\nmin w h k (w) = F k (w) + \u03bb 2 w \u2212 w * 2 , s.t. w * = 1 K arg min w K k=1 F k (w).(22)\nThe local empirical risk minimizer for each device k is\nw k (\u03bb) = 1 n X k X k + \u03bbI \u22121 1 n X k Y k + \u03bbw * (23) = 1 n X k X k + \u03bbI \u22121 1 n X k X k w k + \u03bb K k=1 (X X) \u22121 X k X k w k(24)\nWe next prove that for any k \u2208 [K], w k (\u03bb) with a specific \u03bb can achieve the optimal w k .\n\nTheorem 2. Assume for any 1 \u2264 i \u2264 K, X T i X i = \u03b2I d for some constant \u03b2. Let \u03bb * be the optimal \u03bb that minimizes the test performance on device k, i.e.,\n\u03bb * = arg min \u03bb E F k ( w k (\u03bb))| w k , \u00b5 \\k \u03b8 .(25)\nThen,\n\u03bb * = \u03c3 2 n\u03c4 2 .(26)\nProof. Notice that\narg min \u03bb E F k ( w k (\u03bb))| w k , \u00b5 \\k \u03b8 = arg min \u03bb E X k w k (\u03bb) \u2212 (X k w k + z k ) 2 | w k , \u00b5 \\k \u03b8 (27) = arg min \u03bb E X k ( w k (\u03bb) \u2212 w k ) 2 | w k , \u00b5 \\k \u03b8 (28) = arg min \u03bb E w k \u2212 w k (\u03bb) 2 | w k , \u00b5 \\k \u03b8 .(29)\nPlug in X T k X k = \u03b2I into (20) and (24) respectively, we have the optimal estimator w k is\nw k = K \u2212 1 \u03c3 2 \u03b2 + K\u03c4 2 + \u03b2 \u03c3 2 \u22121 \u03b2 \u03c3 2 w k + K \u2212 1 \u03c3 2 \u03b2 + K\u03c4 2 + \u03b2 \u03c3 2 \u22121 \u03b2 \u03c3 2 + K\u03c4 2 \u03b2 i\u2208[K],i =k w i + \u03b6 k ,(30)\nand w k (\u03bb) is\nw k (\u03bb) = n \u03b2 + n\u03bb \uf8eb \uf8ed \u03b2 n + \u03bb K w k + \u03bb K i\u2208[K],i =k w i \uf8f6 \uf8f8 .(31)\nTaking w k and w k (\u03bb) into\n\u03bb * = arg min \u03bb E w k \u2212 w k (\u03bb) 2 2 |\u00b5 \\k \u03b8 , w k(32)\ngives \u03bb * = \u03c3 2 n\u03c4 2 , as w k (\u03bb * ) is the MMSE estimator of w k given the observations. Remark 1. We note that by using \u03bb * in Ditto, we not only achieve the most accurate solution for the objective, but also we achieve the most accurate solution of any possible federated linear regression algorithm in this problem, as Ditto with \u03bb * realizes the MMSE estimator for w k .\n\nWe have derived an optimal \u03bb * = \u03c3 2 n\u03c4 2 for Ditto in terms of generalization. Recall that we define fairness as the variance of the performance across all devices (Hashimoto et al., 2018;Li et al., 2020e). Next, we prove that the same \u03bb * that minimizes the expected MSE also achieves the optimal fairness among all Ditto solutions.\n\nTheorem 3. Assume for any 1 \u2264 i \u2264 K, X T i X i = \u03b2I d for some constant \u03b2. Among all possible solutions Ditto parameterized by \u03bb, \u03bb * results in the most fair performance across all devices when there are no adversaries, i.e., it minimizes the variance of test performance (test loss) across all devices.\n\n\nProof. Denote the variance of test performance (loss) across K devices as var\nK X k w k (\u03bb) \u2212 y k 2 2 . Let E k {a k } := 1 K k\u2208[K] a k .(33)\nThen arg min\n\u03bb var K X k w k (\u03bb) \u2212 y k 2 2 = arg min \u03bb var K X k w k (\u03bb) \u2212 (X k w k + z k ) 2 2 (34) = arg min \u03bb var K X k ( w k (\u03bb) \u2212 w k ) 2 2 (35) = arg min \u03bb var K w k (\u03bb) \u2212 w k 2 2 = E k {a 2 ki a 2 kj } \u2212 E k {a 2 ki } E k {a 2 kj },(52)\nwhere we have used the fact that \u03a3 w k is a diagonal matrix.\n\nPlugging (50) and (52) into (44) and (45) yields\nE var K w k (\u03bb) \u2212 w k 2 2 \u00b5 \\k \u03b8 , w k (53) = 2d\u03c3 4 w + i 4\u03c3 2 w E k {a 2 ki } + i E k {a 4 ki } \u2212 i E k {a 2 ki } 2 + 2 i =j E k {a 2 ki a 2 kj } \u2212 E k {a 2 ki } E k {a 2 kj } (54) = 2d\u03c3 4 w + i 4\u03c3 2 w E k {a 2 ki } + i E k {a 4 ki } + 2 i =j E k {a 2 ki a 2 kj } \u2212 ( i E k {a 2 ki } 2 + 2 i =j E k {a 2 ki } E k {a 2 kj )}) (55) = 2d\u03c3 4 w + i 4\u03c3 2 w E k {a 2 ki } + E k {( i a 2 ki ) 2 } \u2212 ( i E k {a 2 ki }) 2 (56) = 2d\u03c3 4 w + i 4\u03c3 2 w E k {a 2 ki } + 1 K k ( i a 2 ki ) 2 \u2212 ( 1 K k i a 2 ki ) 2 \u2265 2d\u03c3 2 w ,(57)\nwhere setting {a ki } 1\u2264k\u2264K,1\u2264i\u2264d = 0 achieves the minimum.\n\nObservations. From the optimal \u03bb * = \u03c3 2 n\u03c4 2 for mean test accuracy and variance of the test accuracy, we have the following observations.\n\n\u2022 Test error and variance can be jointly minimized with one \u03bb.\n\n\u2022 As n \u2192 \u221e, \u03bb * \u2192 0, i.e., when each local device has an infinite number of samples, there is no need for federated learning, and training local models is optimal in terms of generalization and fairness.\n\n\u2022 As \u03c4 \u2192 \u221e, \u03bb * \u2192 0, i.e., if the data on different devices (the tasks) are unrelated, then training local models is optimal; On the other hand, as \u03c4 \u2192 0, \u03bb * \u2192 \u221e, i.e., if the data across all devices are identically distributed, or equivalently if the tasks are the same, then training a global model is the best we can achieve.\n\nSo far we have proved that the same \u03bb * achieves the best performance (expected mean square error) for any device k and fairness (variance of mean square error) without considering adversaries. In Section A.2.2 below, we analyze the benefits of Ditto for fairness and robustness in the presence of adversaries.\n\n\nA.2.2. WITH ADVERSARIES: DI T T O FOR ACCURACY, FAIRNESS, AND ROBUSTNESS\n\nAs a special case of data poisoning attacks defined in our threat model (Definition 1), we make the following assumptions on the adversaries.\n\nLet K a and K b \u2265 1 denote the number of malicious and benign devices, respectively, such that K = K a + K b . Definition 3. We say that a device k is a benign device if w k \u223c \u03b8 + N (0, \u03c4 2 I d ); and we say a device k is a malicious device (or an adversary) if w k \u223c \u03b8 + N (0, \u03c4 2 a I d ) where \u03c4 a > \u03c4 .\n\nAs mentioned in Definition 2 and 1, in the presence of adversaries, we measure fairness as the performance variance on benign devices, and robustness as the average performance across benign devices. We next characterize the benefits of Ditto under such metrics. Lemma 7. Let w k be the underlying model parameter of a benign device k. Let\nw i := (X T i X i ) \u22121 X T i y i , i \u2208 [K].(58)\nLet\n\u03a3 \\k w = 1 (K \u2212 1) 2 \uf8eb \uf8ed i\u2208[K b ],i =k \u03c3 2 (X T i X i ) \u22121 + \u03c4 2 I d + i\u2208[Ka],i =k \u03c3 2 (X T i X i ) \u22121 + \u03c4 2 a I d \uf8f6 \uf8f8 ,(59)\nand\n\u03a3 \u22121 w,a = (\u03c3 2 (X T k X k ) \u22121 ) \u22121 + (\u03a3 \\k w + \u03c4 2 I d ) \u22121 .(60)\nConditioned on observations w k and w K\\k := 1\nK\u22121 i =k,i\u2208[K] w i , we have w k = \u03a3 w,a (\u03c3 2 (X T k X k ) \u22121 ) \u22121 w k + \u03a3 w,a (\u03a3 \\k w + \u03c4 2 I d ) \u22121 w K\\k + \u03b6 k ,(61)\nwhere \u03b6 k \u223c N (0, \u03a3 w,a ).\n\nProof. For malicious devices i \u2208 [K a ] and i = k, the additive covariance of w i with respect to \u03b8 is \u03c3 2 (X T i X i ) \u22121 + \u03c4 2 a I d . For benign devices i \u2208 [K b ] and i = K, the covariance is \u03c3 2 (X T i X i ) \u22121 + \u03c4 2 I d . Therefore, the covariance of w K\\k is \u03a3 \\k w . Hence given w K\\k , w k is Gaussian with covariance \u03a3 \\k w + \u03c4 2 I d . w K\\k can be viewed as a noisy observation of w k with covariance \u03a3 \\k w + \u03c4 2 I d . w k is a noisy observation of w k with covariance \u03c3 2 (X T k X k ) \u22121 . The proof follows by applying Lemma 2 to w k and w K\\k .\n\nTheorem 4. Assume for any 1 \u2264 i \u2264 K, X T i X I = \u03b2I d for some constant \u03b2. Let k be a benign device. Let \u03bb * a be the optimal \u03bb that minimizes the test performance on device k, i.e.,\n\u03bb * = arg min \u03bb E F k ( w k (\u03bb))| w k , w K\\k .(62)\nThen,\n\u03bb * a = \u03c3 2 n K K\u03c4 2 + Ka K\u22121 (\u03c4 2 a \u2212 \u03c4 2 ) .(63)\nProof. We obtain \u03bb * a following the proof of Theorem 2. Theorem 5. Among all Ditto solutions parameterized by \u03bb, \u03bb * a results in the most fair performance across all benign devices, i.e., it minimizes the variance of test performance (test mean square error) on benign devices.\n\nProof. Similarly, we look at the variance of the test loss across benign devices:\narg min \u03bb E var K b X k w k (\u03bb) \u2212 y k 2 2 = arg min \u03bb E var K b w k (\u03bb) \u2212 w k 2 2 (64) = arg min \u03bb E K b w k \u2212 w k 2 2 2 \u2212 E K b w k \u2212 w k (\u03bb) 2 2 2 .(65)\nThe rest of the proof is the same as the proof of Theorem 3, except that we set a k = w k (\u03bb) \u2212 w k (\u03bb * a ).\n\nRemark 2. For any benign device k, the solution we obtain by solving Ditto with \u03bb * a is the most robust solution one could obtain among any federated point estimation method given observations w k and w K\\k . \u03bb * a also results in a most fair model in the solution space of Ditto parameterized by \u03bb.\n\nLemma 8. The expected test error minimized at \u03bb * a is d\u03c3 2 w,a ; and the variance of the test loss minimized at \u03bb * a is 2d\u03c3 4 w,a , where \u03c3 w,a denotes the diagonal element of \u03a3 w,a .\n\nProof. For the expected test performance, we note that\nE w k \u2212 w k (\u03bb * a ) 2 w K\\k , w k = E[ diag(\u03a3 w,k ) 2 ] = d\u03c3 2 w,k .(66)\nFor variance, as a k = 0 if \u03bb = \u03bb * a , from (57), we get\nvar K b w k \u2212 w k (\u03bb * a ) 2 = 2d\u03c3 4 w,k .(67)\nObservations. From \u03bb * a , we have the following interesting observations.\n\n\u2022 Mean test error on benign devices (robustness) and variance of the performance across benign devices (fairness) can still be minimized with the same \u03bb a in the presence of adversaries.\n\n\u2022 As \u03c4 a \u2192 \u221e, \u03bb * a \u2192 0, i.e., training local models is optimal in terms of robustness and fairness when adversary's task may be arbitrarily far from the the task in the benign devices.\n\n\u2022 As \u03c4 \u2192 0, if \u03c4 a > 0, \u03bb * a < \u221e, which means that learning a global model is not optimal even with homogeneous data in the presence of adversaries.\n\n\u2022 \u03bb * a is a decreasing function of the number (K a ) and the capability (\u03c4 a ) of the corrupted devices. In other words, as the attacks become more adversarial, we need more personalization.\n\n\u2022 The smallest test error is \u03c3 2 w,a , and the optimal variance is 2\u03c3 4 w,a , which are both increasing with K a (number of adversarial devices) or \u03c4 a (the power of adversary) by inspecting (59) and (60). This reveals a fundamental tradeoff between fairness and robustness.\n\nDiscussion. Through our analysis, we prove that Ditto with an appropriate \u03bb is more accurate, robust, and fair compared with training global or local models on the problem described in A.2. We provide closed-form solutions for \u03bb * across different settings (with and without adversaries), and show that Ditto can achieve fairness and robustness jointly. In the future, we plan to generalize the current theoretical framework to more general models. In the next section, we present a special case of the current analysis, a federated point estimation problem, which is also studied in Section 3.3 as a motivating example.\n\n\nA.3. The Case of Federated Point Estimation\n\nWe consider the one-dimensional federated point estimation problem, which is a special case of linear regression. Similarly, Let \u03b8 be drawn from the non-informative prior on R. We assume that K devices have their data distributed with parameters {w k } k\u2208 [K] .\nw k = \u03b8 + \u03b6 k ,(68)\nwhere \u03b6 k \u223c N (0, \u03c4 2 ) are IID.\n\nLet each device have n data points denoted by x k = {x k,1 , . . . , x k,n }, such that\nx k,i = w k + z k,i ,(69)\nwhere z k,i \u223c N (0, \u03c3 2 ) and are IID.\n\nAssume that\nF k (w) = 1 2 \uf8eb \uf8ed w \u2212 1 n i\u2208[n] x k,i \uf8f6 \uf8f8 2 ,(70)\nand denote by w k the minimizer of the empirical loss F k . It is clear that\nw k = 1 n i\u2208[n]\nx k,i .\n\nFurther, let\nw * := arg min w \uf8f1 \uf8f2 \uf8f3 1 K k\u2208[K] F k (w) \uf8fc \uf8fd \uf8fe .(72)\nIt is straightforward calculation to verify that\nw * = 1 nK i\u2208[n] k\u2208[K] x k,i = 1 K k\u2208[K] w k .(73)\nLemma 9. Denote by w k (\u03bb) the minimizer of h k . Then,\nw k (\u03bb) = \u03bb 1 + \u03bb w * + 1 1 + \u03bb w k (74) = \u03bb (1 + \u03bb)K j =k w j + K + \u03bb (1 + \u03bb)K w k .(75)\nLet \u03c3 2 n :=\n\u03c3 2 n ,(76)\nand\nw K\\k := 1 K \u2212 1 j =k w j .(77)\nLemma 10. Given observations w K\\k and w k , w k is Gaussian distributed and given by\nw k = \u03c3 2 w \u03c3 2 n w k + (K \u2212 1)\u03c3 2 w K\u03c4 2 + \u03c3 2 n w K\\k + \u03be,(78)\nwhere\n1 \u03c3 2 w = 1 \u03c3 2 n + K \u2212 1 K\u03c4 2 + \u03c3 2 n ,(79)\nand\n\u03be \u223c N 0, \u03c3 2 w .(80)\nProof. The proof follows by setting X k = 1 n\u00d71 (k \u2208 [K]) in Lemma 6.\n\nTheorem 6. Let \u03bb * be the optimal \u03bb that minimizes the test performance, i.e.,\n\u03bb * = arg min \u03bb E (w k \u2212 w k (\u03bb)) 2 w K\\k , w k .(81)\nThen,\n\u03bb * = \u03c3 2 n \u03c4 2 = \u03c3 2 n\u03c4 2 .(82)\nProof. The proof follows by setting X k = 1 n\u00d71 (k \u2208 [K]) in Theorem 2.\n\nTheorem 7. Among all Ditto's solutions, \u03bb * results in the most fair performance across all devices when there are no adversaries, i.e., it minimizes the variance of test performance (test mean square error).\n\nProof. The proof follows by setting X k = 1 n\u00d71 (k \u2208 [K]) in Theorem 3.\n\nSimilarly, the adversarial case presented below (including setups, lemmas, and theorems) is also a special case of the adversarial scenarios for linear regression.\n\nLet K a and K b \u2265 1 denote the number of adversarial and benign devices, respectively, such that K = K a + K b .\n\nDefinition 4. We say that a device k is a benign device if w k \u223c \u03b8 + N (0, \u03c4 2 ); and we say a device k is a malicious device (or an adversary) if w k \u223c \u03b8 + N (0, \u03c4 2 a ) where \u03c4 a \u2265 \u03c4 . Lemma 11. Let w k be the parameter associated with a benign device. Given observations w K\\k := 1 K\u22121 j =k w j and w k , w k is Gaussian distributed and given by\nw k = \u03c3 2 w,a \u03c3 2 n w k + (K \u2212 1)\u03c3 2 w,a K\u03c4 2 + \u03c3 2 n + Ka K\u22121 (\u03c4 2 a \u2212 \u03c4 2 ) w K\\k + \u03be a ,(83)\nwhere\n1 \u03c3 2 w,a = 1 \u03c3 2 n + K \u2212 1 K\u03c4 2 + \u03c3 2 n + Ka K\u22121 (\u03c4 2 a \u2212 \u03c4 2 ) ,(84)\nand\n\u03be a \u223c N 0, \u03c3 2 w,a .(85)\nProof. The proof follows by setting X k = 1 n\u00d71 (k \u2208 [K]) in Lemma 7.\n\nTheorem 8. Let w k be a benign device. Let \u03bb * a be the optimal \u03bb that minimizes the test performance, i.e.,\n\u03bb * a = arg min \u03bb E (w k \u2212 w k (\u03bb)) 2 w K\\k , w k .(86)\nThen,\n\u03bb * a = \u03c3 2 n K K\u03c4 2 + Ka K\u22121 (\u03c4 2 a \u2212 \u03c4 2 ) .(87)\nProof. The proof follows by setting X k = 1 n\u00d71 (k \u2208 [K]) in Theorem 4.\n\nTheorem 9. Among all solutions of Objective (Ditto) parameterized by \u03bb, \u03bb * a results in the most fair performance across all benign devices, i.e., it minimizes the variance of test performance (test mean square error) on benign devices.\n\nProof. The proof follows by setting X k = 1 n\u00d71 (k \u2208 [K]) in Theorem 5.\n\nLemma 12. The expected test error minimized at \u03bb * a is \u03c3 2 w,a ; and the variance of the test performance minimized at \u03bb * a is 2\u03c3 4 w,a .\n\n\nB. Algorithm and Convergence Analysis\n\nIn this section, we first present the specific algorithm (Algorithm 2) that we use for most of our experiments (all except for Table 3 and 6). Algorithm 2 is a special case of the more general Ditto solver (Algorithm 1), where we use min w k\u2208 [K] p k F k (w) as the global objective and FedAvg as its solver. As before, the Ditto personalization add-on is highlighted in red. In addition, we prove that personalized models can inherit the convergence rates of the optimal global model for any G(\u00b7) (Theorem 10), and provide convergence guarantees for the special case of Algorithm 2 (Corollary 1).\n\nAlgorithm 2: Ditto for Personalized FL in the case of G(\u00b7) being FedAvg (McMahan et al., 2017) 1 Input: K, T , s, \u03bb, \u03b7 g , \u03b7 l , w 0 , p k , {v 0 k } k\u2208[K] 2 for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do 3 Server randomly selects a subset of devices S t , and sends w t to them 4 for device k \u2208 S t in parallel do 5 Sets w t k to w t and updates w t k for r local iterations on F k :\nw t k = w t k \u2212 \u03b7 g \u2207F k (w t k ) 6\nUpdates v k for s local iterations:\nv k = v k \u2212 \u03b7 l (\u2207F k (v k ) + \u03bb(v k \u2212 w t ) 7 Sends \u2206 t k := w t k \u2212 w t back 8\nServer updating w t+1 as\nw t+1 \u2190 w t + 1 |S t | k\u2208St \u2206 t k 9 return {v k } k\u2208[K] (personalized), w T (global)\nTo analyze the convergence behavior of Algorithm 1 and 2, we first state a list of assumptions below.\n\n\u2022 The global model converges with rate g(t), i.e., there exists g(t) such that lim t\u2192\u221e g(t) = 0, E[ w t \u2212 w * 2 ] \u2264 g(t).\n\n\u2022 For k \u2208 [K], F k is \u00b5-strongly convex.\n\n\u2022 The expectation of stochastic gradients is uniformly bounded at all devices and all iterations, i.e.,\nE[ \u2207F k (w t , \u03be t ) 2 ] \u2264 G 2 1 .(88)\nLet w * be defined as\nw * := min w G(F 1 (w), . . . F K (w))(89)\ni.e., w * is the empirically optimal global model for G(\u00b7). Let u * k denote the empirically optimal local model on device k, i.e.,\nu * k = arg min u F k (u).(90)\nWe introduce an additional assumption on the distance between optimal local models {u * k } k\u2208 [K] and the optimal global model w * below.\n\n\u2022 The L 2 distance between the optimal local models and the optimal global model is bounded, i.e., for k \u2208 [K],\nu * k \u2212 w * \u2264 M.(91)\nThis assumption sets an upper bound on the deviation of the local model on device k, with the global model. It can in turn be viewed as boundedness of heterogeneity of the training data across devices. When local data are farther from being IID, M tends to be larger. Recall that in the fairness/robustness analysis of Ditto (Appendix A), we model the relatedness of underlying models via \u03c4 , and E[ w k \u2212 \u03b8 2 ] = d\u03c4 2 where w k is the underlying model for device k and d is the model dimension. M is related to \u03c4 2 as\nE[ u * k \u2212 w * 2 ] \u2264 2E[ \u00b5 * k \u2212 w k 2 ] + 4E[ w k \u2212 \u03b8 2 ] + 4E[ \u03b8 \u2212 w * 2 ] (92) \u2192 4d\u03c4 2 .(93)\nwhen n k and the total number of samples across all devices are sufficiently large, considering the linear problems we studied. We later show that for convergence, \u03bb scales with 1/M , which is consistent with \u03bb * (for fairness/robustness) scaled with 1/\u03c4 2 .\n\n\nFurther let\nv * k = arg min v h k (v; w * ),(94)\ni.e., v * k is the optimal personalized model for device k. We are interested in the convergence of v k to v * k . We first characterize the progress of updating personalized models for one step under a general G(\u00b7). Lemma 13 (Progress of one step). Under assumptions above, let device k get selected with probability p k at each communication round, with decaying local step-size 2 (t+1)(\u00b5+\u03bb)p k , at each communication round t, we have\nE[ v t+1 k \u2212 v * k 2 ] \u2264 1 \u2212 2 t + 1 E[ v t \u2212 v * 2 ] + 4(G 1 + \u03bb(M + G1 \u00b5 )) 2 (t + 1) 2 (\u00b5 + \u03bb) 2 p 2 k + 4\u03bb 2 (t + 1) 2 (\u00b5 + \u03bb) 2 p 2 k E[ w t \u2212 w * 2 ] + 8\u03bb(G 1 + \u03bb(M + G1 \u00b5 )) (t + 1) 2 (\u00b5 + \u03bb) 2 p 2 k E[ w t \u2212 w * 2 ] + 4\u03bb (t + 1)(\u00b5 + \u03bb)p k E[ v t k \u2212 v * k 2 ]E[ w t \u2212 w * 2 ].\n(95)\nProof. Denote g(v t k ; w t ) as the stochastic gradient of h k (v t k ; w t ). Let I t indicate if device k is selected at the t-th round, and E[I t ] = p k . E[ v t+1 k \u2212 v * k 2 ] = E[ v t k \u2212 \u03b7I t g(v t k ; w t ) \u2212 v * k 2 ] (96) = E[ v t k \u2212 v * k 2 ] + \u03b7 2 E[ I t g(v t k ; w t ) 2 ] + 2\u03b7E I t g(v t k ; w t ), v * k \u2212 v t k (97) \u2264 (1 \u2212 (\u00b5 + \u03bb)\u03b7p k )E[ v t k \u2212 v * k 2 ] + \u03b7 2 E[ g(v t k ; w t ) 2 ] + 2\u03b7p k E[h(v * k ; w t ) \u2212 h(v t k ; w t )](98)\u2264 (1 \u2212 (\u00b5 + \u03bb)\u03b7p k )E[ v t k \u2212 v * k 2 ] + \u03b7 2 E[ g(v t k ; w * ) 2 ] + \u03b7 2 \u03bb 2 E[ w t \u2212 w * 2 ] + 2\u03b7 2 \u03bbE[ g(v t k ; w * ) w t \u2212 w * ] + 2\u03b7p k (h(v * k ; w * ) \u2212 E[h(v t k ; w * )]) + 2\u03b7p k \u03bbE[ v t k \u2212 v * k w t \u2212 w * ].(99)\nFurther, note that\nE[ v t k \u2212 u * k 2 ] \u2264 1 \u00b5 2 E[ \u2207F k (v t k ) 2 ] \u2264 G 2 1 \u00b5 2 ,(100)E[ v t k \u2212 w * 2 ] = E[ v t k \u2212 u * k + u * k \u2212 w * 2 ](101)\u2264 E[ v t k \u2212 u * k 2 ] + E[ u * k \u2212 w * 2 ] + 2E[ v t k \u2212 u * k u * k \u2212 w * ](102)\u2264 G 2 1 \u00b5 2 + M 2 + 2M G 1 \u00b5 ,(103)E[ g(v t k ; w * ) 2 ] = E[ \u2207F k (v t k ) + \u03bb(v t k \u2212 w * ) 2 ](104)\u2264 G 2 1 + \u03bb 2 ( G 1 \u00b5 + M ) 2 + 2G 1 \u03bb( G 1 \u00b5 + M ).(105)\nPlug it into (99),\nE[ v t+1 k \u2212 v * k 2 ] \u2264 (1 \u2212 (\u00b5 + \u03bb)\u03b7p k )E[ v t k \u2212 v * k 2 ] + \u03b7 2 (G 1 + \u03bb(M + G 1 \u00b5 )) 2 + \u03b7 2 \u03bb 2 E[ w t \u2212 w * 2 ] + 2\u03b7 2 \u03bb(G 1 + \u03bb(M + G 1 \u00b5 )) E[ w t \u2212 w * 2 ] + 2\u03b7p k \u03bb E[ v t k \u2212 v * k 2 ]E[ w t \u2212 w * 2 ].(106)\nwhere the last step is due to\nE[XY ] \u2264 E[X 2 ]E[Y 2 ]\n. The Lemma then holds by taking \u03b7 = 2 (t+1)(\u00b5+\u03bb)p k .\nLemma 13 relates E[ v t+1 k \u2212 v * k 2 ] with E[ v t k \u2212 v * k 2 ] and E[ w t k \u2212 w * 2 ].\nBased on this, we prove that personalized models can inherit the convergence rate of the global model w t for any G(\u00b7).\n\nTheorem 10 (Relations between convergence of global and personalized models). Under the assumptions above, if there exists a constant A such that g(t+1)\ng(t) \u2265 1 \u2212 g(t)\nA , then there exists C < \u221e such that for any device k \u2208\n[K], E[ v t k \u2212 v * k 2 ] \u2264 Cg(t) with a local learning rate \u03b7 = 2g(t) A(\u00b5+\u03bb)p k .\nProof. We proceed the proof by induction. First, for any constant C >\nE[ v 0 k \u2212v * k 2 ] g(0) , E[ v 0 k \u2212 v * k 2 ] \u2264 Cg(0). If E[ v t k \u2212 v * k 2 ] \u2264 Cg(t)\nholds, then for t + 1, from Lemma 13,\nE[ v t k+1 \u2212 v * k 2 ] \u2264 1 \u2212 2g(t) A Cg(t) + g(t) 2 A 4 Ap 2 k (G 1 + \u03bb(M + G1 \u00b5 )) 2 (\u00b5 + \u03bb) 2 + g(t) + 2(G 1 + \u03bb(M + G1 \u00b5 )) g(t) \u00b5 + \u03bb + g(t) 2 4\u03bb \u221a C (\u00b5 + \u03bb) (107) \u2264 1 \u2212 2g(t) A Cg(t) + Cg(t) 2 A(108)\nholds for some C < \u221e. Hence,\nE[ v t k+1 \u2212 v * k 2 ] \u2264 1 \u2212 2g(t) A Cg(t) + Cg(t) 2 A (109) = 1 \u2212 g(t) A Cg(t) (110) \u2264 Cg(t + 1),(111)\ncompleting the proof.\n\nDiscussions. Theorem 10 also suggests how the percentage/power of malicious devices can affect convergence rates. The percentage/power of adversaries impacts both the optimal global solution w * , and the convergence rate of the global model g(t). (i) For w * , it affects M in Eq (91)-the distance between the local model on a benign device and the global model. This in turn affects \u03bb in Eq (95) and (107), and the constant C. \u03bb can scale inversely proportional to M , which is consistent with our fairness/robustness analysis where \u03bb * should decrease as the increase of \u03c4 2 . (ii) For g(t), the modularity of Ditto allows for decoupling the convergence of personalized models and the global model (as demonstrated by this theorem), and we can plug in any previous algorithms and their analysis on the convergence rate g(t) as a function of malicious devices.\n\nAs a direct result of Theorem 10, we could state a result for Ditto when the global objective is FedAvg.\n\nCorollary 1 (Convergence of personalized models). Under the assumptions above, if the global objective G(\u00b7) is FedAvg, then under Algorithm 2, for k \u2208 [K], \nE[ v t k \u2212 v * k 2 ] = O(1/t).(112E[ w t \u2212 w * 2 ] \u2264 D t + B w 1 \u2212 w * 2 \u2264 D t + 1 ,(113)\nwhere D, D , B are constants. Setting g(t) = D t+1 and A = D in Theorem 10, it follows that\nE[ v t k \u2212 v * k 2 ] = O(1/t).\n\nC. Experimental Details\n\n\nC.1. Datasets and Models\n\nWe summarize the datasets, corresponding models, and tasks in Table 4 below. We evaluate the performance of Ditto with both convex and non-convex models across a set of FL benchmarks. In our datasets, we have both image data (FEMNIST, CelebA, Fashion MNIST), and text data (StackOverflow).  (Caldas et al., 2018). We have two versions of FEMNIST in this work under different partitions with different levels of statistical heterogeneity. The manually-partitioned version is more heterogeneous than the naturally-partitioned one, as we assign 5 classes to each device. We show that the benefits of Ditto can be more significant on the skewed FEMNIST data (Table 10). All results shown in the main text are based on the natural partition. We downsample the number of data points on each device (following the power law) for Vehicle. For FEMNIST, CelebA, and StackOverflow, we randomly sample devices (users) from the entire dataset. We use the full version of Fashion MNIST (which has been used in previous FL works (Bhagoji et al., 2019)), and assign 5 classes to each device.\n\n\nC.2. Personalization Baselines\n\nWe elaborate on the personalization baselines used in our experiments (Table 2) which allow for partial device participation and local updating. We consider:\n\n\u2022 MOCHA (Smith et al., 2017), a primal-dual framework for multi-task learning. It jointly learns the model parameters and a device relation matrix, and applicable to convex problems.\n\n\u2022 APFL (Deng et al., 2021), which proposes to interpolate between local and global models for personalization. While it can reduce to solving local problems (without constraints on the solution space) as pointed out in (Deng et al., 2021), we find that in neural network applications, it has some personalization benefits, possibly due to the joint optimization solver.\n\n\u2022 Elastic Weight Consolidation (EWC), which takes into account the Fisher information when finetuning from the optimal global model (Kirkpatrick et al., 2017;Yu et al., 2020). The local objective is\nmin w F k (w) + \u03bb 2 i F ii \u00b7 (w[i] \u2212 w * [i]) 2 where [i]\ndenotes the index of parameters and F ii denotes the i-th diagonal of the empirical Fisher matrix F estimated using a data batch.\n\n\u2022 L2SGD, which regularizes personalized models towards their mean (Hanzely & Richt\u00e1rik, 2020). The proposed method requires full device participation once in a while. However, to remain consistent with the other solvers, we use their objective but adopt a different solver with partial device participation-each selected local device solving min w F k (w) + \u03bb 2 w \u2212w 2 wherew is the current mean of all personalized modelsw = 1 N N k=1 w k . \u2022 Mapper, which is one of the three personalization methods proposed in Mansour et al. (2020) that needs the minimal amount of meta-information. Similar to APFL, it is also motivated by model interpolation.\n\n\u2022 Per-FedAvg (HF) (Fallah et al., 2020) which applies MAML (Finn et al., 2017) to personalize federated models with an Hessian-product approximation to approximate the second-order gradients.\n\n\u2022 Symmetrized KL constrains the symmetrized KL divergence between the prediction of finetuned models and that of the initialization. Specifically, in our setting, the local objective is min w F k (w) + \u03bb 2 (D KL (f (w)||f (w * )) + D KL (f (w * )||f (w))) where D KL (P ||Q) is the KL-divergence between P and Q, and f (\u00b7) denotes the softmax probability for classification.\n\n\nD. Additional and Complete Experiment Results\n\n\nD.1. Comparing with Finetuning\n\nAs discussed in Section 3.2, finetuning on h k for each device k is a possible solver for Ditto. In non-convex cases, however, starting from a corrupted w * may result in inferior performance compared with Algorithm 1. We provide a simple example to illustrate this point. To perform finetuning, we run different numbers of epochs of mini-batch SGD on the Ditto objective for each device in the network, and pick the best one. As shown in Figure 7 below, finetuning at round 5,000 will not result in a good final accuracy. We observe that one could also stop at early iterations and then finetune. However, it is difficult to do so in practice based on the training or validation data alone, as shown in Figure 9.  \n\n\nD.2. Tuning \u03bb\n\nWe assume that the server does not have knowledge of which devices are benign vs. malicious, and we have each device locally select and apply a best \u03bb from a candidate set of three values based on their validation data. For benign devices, this means they will pick a \u03bb based on their clean validation signal. For malicious devices, how they perform personalization (i.e., selecting \u03bb) does not affect the corrupted global model updates they send, which are independent of \u03bb. We further assume the devices have some knowledge of how 'strong' the attack is. We define strong attacks as (i) all of model replacement attacks (A3) where the magnitude of the model updates from malicious devices can scale by > 10\u00d7, and (ii) other attacks where more than half of the devices are corrupted. In particular, for devices with very few validation samples (less than 4), we use a fixed small \u03bb (\u03bb=0.1) for strong attacks, and use a fixed relatively large \u03bb (\u03bb=1) for all other attacks. For devices with more than 5 validation data points, we let each select \u03bb from {0.05, 0.  Table 5 below-showing that this dynamic tuning heuristic works well relative to an ideal, but more unrealistic strategy that picks the best \u03bb based on knowledge of which devices are benign vs. malicious (i.e., by only using the validation data of the benign devices). \n\n\nD.3. Ditto Augmented with Robust Baselines\n\nIn Section 4.4, we demonstrate that the performance of Ditto can be further improved when it is combined with robust baselines (e.g., learning a robust w * via robust aggregation). Here, we report full results validating this claim in Table 6 below. \n\n\nD.4. Ditto Complete Results\n\nIn Section 4.1, we present partial results on three strong attacks on two datasets. Here, we provide full results showing the robustness and fairness of Ditto on all attacks and all datasets compared with all defense baselines. We randomly split local data on each device into 72% train, 8% validation, and 20% test sets, and report all results on test data. We use a learning rate of 0.01 for StackOverflow, 0.05 for Fashion MNIST and 0.1 for all other datasets; and batch size 16 for CelebA and Fashion MNIST, 32 for FEMNIST and Vehicle, and 100 for StackOverflow. For every dataset, we first run FedAvg on clean data to determine the number of communication rounds. Then we run the same number of rounds for all attacks on that dataset. For our robust baselines, 'median' means coordinate-wise median. For Krum, multi-Krum, k-norm, and k-loss, we assume the server knows the expected number of malicious devices when aggregation. In other words, for k-norm, we filter out the updates with the k largest norms where k is set to the expected number of malicious devices. Similarly, for k-loss, we only use the model update with the k+1-th largest training loss. For gradient clipping, we set the threshold to be the median of the gradient norms coming from all selected devices at each round. FedMGDA+ has an additional \u03b5 hyperparameter which we select from {0, 0.1, 0.5, 1} based on the validation performance on benign devices. For the finetuning (only on neural network models) baseline, we run 50 epochs of mini-batch SGD on each device on the local objective F k starting from w * . We see that Ditto can achieve better fairness and robustness in most cases. In particular, on average of all datasets and all attack scenarios, Ditto (with dynamic \u03bb's) achieves 6% absolute accuracy improvement compared with the strongest robust baseline. In terms of fairness, Ditto is able to reduce the variance of test accuracy by 10% while improving the average accuracy by 5% relative to state-of-the-art methods for fair FL (without attacks).   0.628 (.15) 0.620 (.14) 0.627 (.14) 0.607 (.13) 0.620 (.14) 0.627 (.14) 0.607 (.13) 0.622 (.14) 0.621 (.14) 0.620 (.14) fair 0.809 ( \n\nFigure 3 .\n3Robustness, i.e., average test accuracy on benign devices (Definition 1), on Fashion MNIST and FEMNIST. We compare Ditto with learning a global model and three strong defense mechanisms (see Appendix D for results on all defense baselines), and find that Ditto is the most robust under almost all attacks.\n\n\ni) regularize with the average (L2SGD (Hanzely & Richt\u00e1rik, 2020)) or the learnt device relationship matrix (MOCHA (Smith et al., 2017)), (ii) encourage closeness to the global model in terms of some specific function behavior (EWC (Kirkpatrick et al., 2017; Yu et al., 2020) and Symmetrized KL (SKL)), (iii) interpolate between local and global models (APFL (Deng et al., 2021) and mapper (Mansour et al., 2020)), and (iv) have been motivated by meta-learning (Per-FedAvg (HF) (Fallah et al., 2020)). We provide a detailed description in Appendix C.\n\nFigure 6 .\n6Ditto with joint optimization (Algorithm 1) outperforms the alternative local finetuning solver under the strong model replacement attack.\n\n\n1909.12488, 2019. Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019. Karimireddy, S. P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh, A. T. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, 2020. Khodak, M., Balcan, M.-F. F., and Talwalkar, A. S. Adaptive gradient-based meta-learning methods. In Advances in Neural Information Processing Systems, 2019. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 2017. Lamport, L., Shostak, R., and Pease, M. The byzantine generals problem. In Concurrency: the Works of Leslie Lamport. 2019. Li, J., Khodak, M., Caldas, S., and Talwalkar, A.\n\n\nLemma 2 is a generalization of Lemma 11 presented in Mahdavifar et al. (2018) (restated in Lemma 3 below) to the multivariate Gaussian case. The proof also follows from the proof in Mahdavifar et al. (2018). Lemma 3 (Lemma 11 in Mahdavifar et al. (2018)). Let \u03b8 be drawn from the non-informative uniform prior on R. Further, let {\u03c6 k } k\u2208[K] denote noisy observations of \u03b8 with additive zero-mean independent Gaussian noises with variances {\u03c3 2 k } k\u2208[K]\n\nFigure 7 .\n7'Ditto, joint' achieves high test accuracy on benign devices. The performance can also be good if we first early stop at some specific points and then finetune.\n\nFigure 9 .\n9Finetuning is not very practical as it is difficult to determine when to stop training the global model by looking at the training loss (left) or validation accuracy (right) on all devices (without knowing which are benign).\n\n) 0 .\n0867 (.18) 0.839 (.20) 0.220 (.32) 0.867 (.18) 0.839 (.22) 0.770 (.31) 0.868 (.17) 0.836 (.08) 0.406 (.15) clipping 0.864 (.16) 0.865 (.17) 0.678 (.34) 0.234 (.30) 0.865 (.18) 0.839 (.22) 0.764 (.27) 0.868 (.17) 0.789 (.07) 0.315 (.17) k-norm 0.866 (.16) 0.867 (.17) 0.838 (.21) 0.222 (.32) 0.867 (.18) 0.839 (.22) 0.778 (.31) 0.867 (.17) 0.844 (.09) 0.458 (.16) k-loss 0.850 (.05) 0.755 (.03) 0.732 (.09) 0.217 (.31) 0.852 (.06) 0.840 (.07) 0.825 (.09) 0.866 (.17) 0.692 (.08) 0.328 (.16) FedMGDA+ 0.860 (.16) 0.835 (.09) 0.674 (.14) 0.270 (.26) 0.860 (.18) 0.843 (.22) 0.794 (.26) 0.836 (.17) 0.757 (.07) 0.676 (.17) MOCHA 0.880 (.04) 0.848 (.07) 0.832 (.08) 0.829 (.10) 0.846 (.06) 0.843 (.07) 0.833 (.10) 0.862 (.06) 0.844 (.07) 0.834 (.07) Ditto, \u03bb=0.1 0.845 (.07) 0.841 (.08) 0.841 (.09) 0.851 (.06) 0.844 (.07) 0.848 (.08) 0.866 (.05) 0.838 (.07) 0.829 (.08) 0.827 (.08) Ditto, \u03bb=1 0.875 (.05) 0.859 (.06) 0.821 (.07) 0.776 (.08) 0.875 (.06) 0.870 (.07) 0.879 (.04) 0.860 (.07) 0.813 (.07) 0.757 (.08) Ditto, \u03bb=2 0.882 (.05) 0.862 (.05) 0.800 (.07) 0.709 (.12) 0.884 (.05) 0.872 (.06) 0.869 (.04) 0.872 (.06) 0.791 (.06) 0.690 (.09)\n\n\n(.12) 0.794 (.12) 0.730 (.12) 0.057 (.04) 0.793 (.12) 0.753 (.12) 0.671 (.14) 0.798 (.11) 0.794 (.12) 0.791 (.11) finetuning 0.815 (.09) 0.778 (.11) 0.734 (.12) 0.671 (.13) 0.764 (.11) 0.695 (.18) 0.646 (.14) 0.688 (.13) 0.671 (.14) 0.655 (.13) Ditto, \u03bb=0.01 0.800 (.15) 0.709 (.15) 0.683 (.17) 0.642 (.13) 0.701 (.14) 0.684 (.14) 0.645 (.14) 0.650 (.14) 0.628 (.14) 0.650 (.14) Ditto, \u03bb=0.1 0.827 (.10) 0.794 (.11) 0.755 (.13) 0.666 (.14) 0.786 (.13) 0.743 (.14) 0.674 (.14) 0.691 (.15) 0.664 (.14) 0.640 (.14) Ditto, \u03bb=1 0.836 (.10) 0.803 (.10) 0.767 (.10) 0.672 (.14) 0.792 (.11) 0.691 (.17) 0.575 (.17) 0.642 (.12) 0.595 (.14) 0.554 (.15)\n\n\nglobal 0.911 (.08) 0.897 (.08) 0.855 (.10) 0.753 (.13) 0.900 (.08) 0.882 (.09) 0.857 (.10) 0.753 (.10) 0.551 (.13) 0.275 (.12) local 0.876 (.10) 0.874 (.10) 0.876 (.11) 0.879 (.10) 0.874 (.10) 0.876 (.11) 0.879 (.10) 0.877 (.10) 0.874 (.10) 0.876 (.11) fair 0.909 (.07) 0.751 (.12) 0.637 (.13) 0.547 (.11) 0.731 (.13) 0.637 (.14) 0.635 (.14) 0.653 (.13) 0.601 (.12) 0.131 (.16) median 0.884 (.09) 0.853 (.10) 0.818 (.12) 0.606 (.17) 0.885 (.09) 0.883 (.09) 0.864 (.10) 0.856 (.09) 0.829 (.11) 0.725 (.15) Krum 0.838 (.13) 0.864 (.11) 0.818 (.13) 0.768 (.15) 0.847 (.12) 0.870 (.11) 0.805 (.13) 0.868 (.11) 0.866 (.11) 0.640 (.18) multi-Krum 0.911 (.08) 0.907 (.08) 0.889 (.10) 0.793 (.12) 0.849 (.10) 0.827 (.12) 0.095 (.12) 0.804 (.11) 0.860 (.09) 0.823 (.13) clipping 0.913 (.07) 0.905 (.08) 0.875 (.10) 0.753 (.12) 0.904 (.08) 0.886 (.09) 0.856 (.11) 0.901 (.08) 0.844 (.11) 0.477 (.13) k-norm 0.911 (.08) 0.908 (.08) 0.888 (.10) 0.118 (.08) 0.906 (.08) 0.893 (.09) 0.096 (.07) 0.765 (.14) 0.854 (.10) 0.828 (.12) k-loss 0.898 (.08) 0.856 (.09) 0.861 (.10) 0.851 (.31) 0.876 (.09) 0.866 (.11) 0.870 (.10) 0.538 (.14) 0.257 (.13) 0.092 (.13) FedMGDA+ 0.915 (.08) 0.907 (.08) 0.874 (.10) 0.753 (.13) 0.911 (.08) 0.900 (.09) 0.873 (.10) 0.914 (.08) 0.904 (.08) 0.869 (.10) finetuning 0.945 (.06) 0.946 (.07) 0.935 (.07) 0.922 (.08) 0.945 (.07) 0.930 (.08) 0.923 (.08) 0.915 (.08) 0.871 (.11) 0.764 (.15) Ditto, \u03bb=0.1 0.929 (.09) 0.920 (.09) 0.909 (.10) 0.897 (.10) 0.921 (.09) 0.914 (.09) 0.905 (.08) 0.914 (.09) 0.903 (.09) 0.873 (.09) Ditto, \u03bb=1 0.946 (.06) 0.944 (.08) 0.935 (.07) 0.925 (.07) 0.943 (.08) 0.930 (.07) 0.912 (.08) 0.887 (.09) 0.831 (.10) 0.740 (.12) Ditto, \u03bb=2 0.945 (.06) 0.942 (.06) 0.935 (.07) 0.917 (.07) 0.936 (.07) 0.923 (.08) 0.906 (.08) 0.871 (.09) 0.785 (.11) 0.606 (.14)\n\n\n(.21) 0.526 (.29) 0.419 (.36) 0.127 (.27) 0.555 (.23) 0.550 (.26) 0.093 (.16) 0.003 (.08) 0.009 (.07) 0.006 (.05) finetuning 0.948 (.11) 0.942 (.13) 0.959 (.10) 0.946 (.10) 0.949 (.16) 0.918 (.21) 0.621 (.11) 0.788 (.25) 0.740 (.27) 0.751 (.26) Ditto, \u03bb=0.01 0.947 (.15) 0.945 (.18) 0.955 (.20) 0.946 (.13) 0.942 (.18) 0.949 (.15) 0.944 (.14) 0.902 (.20) 0.895 (.23) 0.888 (.20) Ditto, \u03bb=0.1 0.948 (.10) 0.945 (.14) 0.959 (.12) 0.936 (.09) 0.945 (.13) 0.948 (.10) 0.888 (.18) 0.936 (.16) 0.827 (.23) 0.812 (.24) Ditto, \u03bb=1 0.902 (.15) 0.899 (.15) 0.907 (.15) 0.861 (.14) 0.899 (.18) 0.818 (.22) 0.423 (.41) 0.880 (.15) 0.730 (.28) 0.736 (.28) Ditto: Fair and Robust Federated Learning Through Personalization\n\n\n(.19) 0.584 (.28) 0.550 (.31) 0.169 (.21) 0.595 (.28) 0.654 (.28) 0.683 (.26) 0.543 (.33) 0.458 (.33) 0.455 (.34) FedMGDA+ 0.909 (.19) 0.853 (.21) 0.508 (.28) 0.473 (.34) 0.907 (.19) 0.889 (.21) 0.782 (.26) 0.865 (.23) 0.805 (.26) 0.847 (.21) finetuning 0.912 (.18) 0.814 (.24) 0.721 (.28) 0.691 (.29) 0.850 (.24) 0.800 (.25) 0.747 (.24) 0.665 (.28) 0.668 (.27) 0.673 (.28) Ditto, \u03bb=0.1 0.884 (.24) 0.716 (.27) 0.721 (.27) 0.724 (.28) 0.727 (.26) 0.708 (.28) 0.706 (.28) 0.699 (.28) 0.694 (.27) 0.689 (.28) Ditto, \u03bb=1 0.911 (.16) 0.820 (.26) 0.714 (.28) 0.675 (.29) 0.872 (.22) 0.826 (.26) 0.708 (.29) 0.629 (.29) 0.667 (.28) 0.685 (.28) Ditto, \u03bb=2 0.914 (.18) 0.828 (.22) 0.698 (.27) 0.654 (.28) 0.862 (.21) 0.791 (.26) 0.623 (.31) 0.585 (.29) 0.647 (.27) 0.655 (.29)\n\n\nglobal 0.155 (.13) 0.153 (.13) 0.156 (.16) 0.169 (.18) 0.147 (.12) 0.009 (.03) 0.013 (.01) 0.000 (.00) 0.000 (.00) 0.000 (.00) local 0.311 (.15) 0.311 (.15) 0.313 (.15) 0.319 (.15) 0.311 (.15) 0.313 (.15) 0.319 (.15) 0.311 (.15) 0.313 (.15) 0.319 (.15) fair 0.154 (.13) 0.155 (.14) 0.153 (.13) 0.141 (.10) 0.000 (.00) 0.000 (.00) 0.000 (.00) 0.148 (.12) 0.152 (.13) 0.167 (.11) median 0.002 (.00) 0.001 (.00) 0.000 (.00) 0.000 (.00) 0.000 (.00) 0.001 (.00) 0.000 (.00) 0.000 (.00) 0.000 (.00) 0.000 (.00) Krum 0.154 (.13) 0.150 (.13) 0.041 (.04) 0.002 (.00) 0.158 (.13) 0.151 (.13) 0.167 (.12) 0.153 (.13) 0.154 (.14) 0.138 (.15) clipping 0.154 (.13) 0.157 (.13) 0.149 (.13) 0.163 (.17) 0.152 (.13) 0.001 (.01) 0.001 (.01) 0.155 (.12) 0.161 (.14) 0.120 (.16) k-norm 0.154 (.13) 0.156 (.12) 0.100 (.08) 0.002 (.00) 0.086 (.11) 0.042 (.03) 0.001 (.00) 0.149 (.15) 0.144 (.15) 0.155 (.13) k-loss 0.155 (.13) 0.160 (.12) 0.164 (.13) 0.129 (.14) 0.136 (.11) 0.145 (.11) 0.156 (.14) 0.148 (.14) 0.159 (.13) 0.156 (.13) FedMGDA+ 0.155 (.12) 0.154 (.13) 0.152 (.13) 0.165 (.13) 0.147 (.13) 0.160 (.14) 0.101 (.09) 0.155 (.13) 0.158 (.12) 0.154 (.13)\n\nTable 2 .\n2Ditto is competitive with or outperforms other recent personalization methods. We report the average (standard deviation) of test accuracies across all devices to capture performance and fairness (Definition 2), respectively.Clean \n50% Adversaries (A1) \n\nMethods \nFEMNIST CelebA FEMNIST CelebA \nglobal \n\n\nTable 3 .\n3Augmenting Ditto with robust baselines can further improve performance. Ditto + clipping .810 .645 .808 .684 .813 .672FEMNIST \nA1 \nA2 \nA3 \n\nMethods \n20% 80% 20% 80% 10% 20% \n\nglobal \n.773 .574 .774 .636 .517 .364 \nclipping \n.791 .408 .791 .656 .795 .061 \nDitto \n.803 .669 .792 .681 .695 .650 \n\n\n\nBagdasaryan, E.,Veit, A., Hua, Y., Estrin, D., and  Shmatikov, V.  How to backdoor federated learning. In International Conference on Artificial Intelligence and Statistics, 2020. Bhagoji, A. N., Chakraborty, S., Mittal, P., and Calo, S. Analyzing federated learning through an adversarial lens. In International Conference on Machine Learning, 2019. Biggio, B., Nelson, B., and Laskov, P. Support vector machines under adversarial label noise. In Asian Conference on Machine Learning, 2011. Biggio, B., Nelson, B., and Laskov, P. Poisoning attacks against support vector machines. In International Conference on Machine Learning, 2012. Blanchard, P., Mhamdi, E. M. E., Guerraoui, R., and Stainer, J. Machine learning with adversaries: Byzantine tolerant gradient descent. In Advances in Neural Information Processing Systems, 2017. Caldas, S., Wu, P., Li, T., Kone\u010dn\u1ef3, J., McMahan, H. B., Smith, V., and Talwalkar, A. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097, 2018.\n\n\nDeng, Y., Kamani, M. M., and Mahdavi, M. Distributionally robust federated averaging. Advances in Neural Information Processing Systems, 2020. Deng, Y., Kamani, M. M., and Mahdavi, M. Adaptive personalized federated learning, 2021. URL https: //openreview.net/forum?id=g0a-XYjpQ7r. Dinh, C. T., Tran, N. H., and Nguyen, T. D. Personalized federated learning with moreau envelopes. In Advances in Neural Information Processing Systems, 2020. Duarte, M. F. and Hu, Y. H. Vehicle classification in distributed sensor networks. Journal of Parallel and Distributed Computing, 2004. Dumford, J. and Scheirer, W. Backdooring convolutional neural networks via targeted weight perturbations. arXiv preprint arXiv:1812.03128, 2018. Evgeniou, T. and Pontil, M. Regularized multi-task learning. In International Conference on Knowledge Discovery and Data Mining, 2004.\n\n\nSmith, V., Chiang, C.-K., Sanjabi, M., and Talwalkar, A. S.Federated multi-task learning. In Advances in Neural Information Processing Systems, 2017.Differen-\ntially private meta-learning. In International Conference \non Learning Representations, 2020a. \n\nLi, L., Xu, W., Chen, T., Giannakis, G. B., and Ling, Q. \nRsa: Byzantine-robust stochastic aggregation methods \nfor distributed learning from heterogeneous datasets. In \nAAAI Conference on Artificial Intelligence, 2019. \n\nLi, M., Soltanolkotabi, M., and Oymak, S. Gradient descent \nwith early stopping is provably robust to label noise for \noverparameterized neural networks. In International Con-\nference on Artificial Intelligence and Statistics, 2020b. \n\nLi, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, \nA., and Smith, V. Federated optimization in heteroge-\nneous networks. In Conference on Machine Learning and \nSystems, 2020c. \n\nLi, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., \nand Smith, V. Federated optimization in heterogeneous \nnetworks. Proceedings of Machine Learning and Systems, \n2020d. \n\nLi, T., Sanjabi, M., Beirami, A., and Smith, V. Fair re-\nsource allocation in federated learning. In International \nConference on Learning Representations, 2020e. \n\nLi, T., Beirami, A., Sanjabi, M., and Smith, V. Tilted em-\npirical risk minimization. In International Conference on \nLearning Representations, 2021. \n\nLi, X., Huang, K., Yang, W., Wang, S., and Zhang, Z. On the \nconvergence of fedavg on non-iid data. In International \nConference on Learning Representations, 2020f. \n\nLiang, P. P., Liu, T., Ziyin, L., Salakhutdinov, R., and \nMorency, L.-P. Think locally, act globally: Federated \nlearning with local and global representations. arXiv \npreprint arXiv:2001.01523, 2020. \n\nLiu, Y., Ma, S., Aafer, Y., Lee, W., Zhai, J., Wang, W., \nand Zhang, X. Trojaning attack on neural networks. In \nNetwork and Distributed System Security Symposium, \n2018. \n\nLiu, Z., Luo, P., Wang, X., and Tang, X. Deep learning \nface attributes in the wild. In International Conference \non Computer Vision, 2015. \n\nLondon, B. PAC identifiability in federated personalization. \nIn NeurIPS 2020 Workshop on Scalability, Privacy, and \nSecurity in Federated Learning, 2020. \n\nMahdavifar, H., Beirami, A., Touri, B., and Shamma, J. S. \nGlobal games with noisy information sharing. IEEE \nTransactions on Signal and Information Processing over \nNetworks, 2018. \n\nMansour, Y., Mohri, M., Ro, J., and Suresh, A. T. Three \napproaches for personalization with applications to feder-\nated learning. arXiv preprint arXiv:2002.10619, 2020. \n\nMcMahan, B., Moore, E., Ramage, D., Hampson, S., and \ny Arcas, B. A. Communication-efficient learning of deep \nnetworks from decentralized data. In International Con-\nference on Artificial Intelligence and Statistics, 2017. \n\nMohri, M., Sivek, G., and Suresh, A. T. Agnostic feder-\nated learning. In International Conference on Machine \nLearning, 2019. \n\nMuhammad, K., Wang, Q., O'Reilly-Morgan, D., Tragos, E., \nSmyth, B., Hurley, N., Geraci, J., and Lawlor, A. Fedfast: \nGoing beyond average for faster training of federated \nrecommender systems. In International Conference on \nKnowledge Discovery & Data Mining, 2020. \n\nPillutla, K., Kakade, S. M., and Harchaoui, Z. Ro-\nbust aggregation for federated learning. arXiv preprint \narXiv:1912.13445, 2019. \nReddi, S., Charles, Z., Zaheer, M., Garrett, Z., Rush, K., \nKone\u010dn\u1ef3, J., Kumar, S., and McMahan, H. B. Adaptive \nfederated optimization. In International Conference on \nLearning Representations, 2021. \n\nSattler, F., M\u00fcller, K.-R., and Samek, W. Clustered federated \nlearning: Model-agnostic distributed multitask optimiza-\ntion under privacy constraints. IEEE Transactions on \nNeural Networks and Learning Systems, 2020. \n\nSchwarz, J., Czarnecki, W., Luketina, J., Grabska-\nBarwinska, A., Teh, Y. W., Pascanu, R., and Hadsell, \nR. Progress & compress: A scalable framework for con-\ntinual learning. In International Conference on Machine \nLearning, 2018. \n\nShafahi, A., Huang, W. R., Najibi, M., Suciu, O., Studer, \nC., Dumitras, T., and Goldstein, T. Poison frogs! tar-\ngeted clean-label poisoning attacks on neural networks. \nIn Advances in Neural Information Processing Systems, \n2018. \n\nSinghal, K., Sidahmed, H., Garrett, Z., Wu, S., Rush, K., \nand Prakash, S. Federated reconstruction: Partially local \nfederated learning. arXiv preprint arXiv:2102.03448, \n2021. \n\nSun, G., Cong, Y., Dong, J., Wang, Q., and Liu, J. Data \npoisoning attacks on federated machine learning. arXiv \npreprint arXiv:2004.10020, 2020. \n\nSun, Z., Kairouz, P., Suresh, A. T., and McMahan, H. Can \nyou really backdoor federated learning? arXiv preprint \narXiv:1911.07963, 2019. \n\nWang, H., Sreenivasan, K., Rajput, S., Vishwakarma, H., \nAgarwal, S., Sohn, J.-y., Lee, K., and Papailiopoulos, D. \nAttack of the tails: Yes, you really can backdoor federated \nlearning. In Advances in Neural Information Processing \nSystems, 2020. \n\nWang, K., Mathews, R., Kiddon, C., Eichner, H., Beaufays, \nF., and Ramage, D. Federated evaluation of on-device \npersonalization. arXiv preprint arXiv:1910.10252, 2019. \n\nXiao, H., Rasul, K., and Vollgraf, R. Fashion-mnist: a \nnovel image dataset for benchmarking machine learning \nalgorithms. arXiv preprint arXiv:1708.07747, 2017. \n\nXie, C., Huang, K., Chen, P.-Y., and Li, B. DBA: Dis-\ntributed backdoor attacks against federated learning. In \nInternational Conference on Learning Representations, \n2020. \n\n\n\n\nSection A.1: Properties of Ditto for Strongly Convex FunctionsSection A.2: Federated Linear Regression \n\nSection A.3: The Case of Federated Point Estimation \n\nSection B: Algorithm and Convergence Analysis \n\nSection C: Experimental Details \n\nSection C.1: Datasets and Models \n\nSection C.2: Personalization Baselines \n\nSection D: Additional and Complete Experiment Results \n\nSection D.1: Comparing with Finetuning \n\nSection D.2: Tuning \u03bb \n\nSection D.3: Ditto Augmented with Robust Baselines \n\nSection D.4: Ditto Complete Results \n\n\nTable 4 .\n4Summary of datasets.Datasets \n# Devices Data Partitions \nModels \nTasks \nVehicle (Duarte & Hu, 2004) 2 \n23 \nnatural (each device is a vehicle) \nlinear SVM \nbinary classification \nFEMNIST (Cohen et al., 2017) \n205 \nnatural (each device is a writer) \nCNN \n62-class classification \nCelebA (Liu et al., 2015) \n515 \nnatural (each device is a celebrity) \nCNN \nbinary classification \nFashion MNIST (Xiao et al., 2017) \n500 \nsynthetic (assign 5 classes to each device) CNN \n10-class classification \nStackOverflow (TFF) 3 \n400 \nnatural (each device is a user) \nlogistic regression 500-class tag prediction \nFEMNIST (skewed) (Cohen et al., 2017) 100 \nsynthetic (assign 5 classes to each device) CNN \n62-class classification \n\nFEMNIST is Federated EMNIST, which is EMNIST (Cohen et al., 2017) partitioned by the writers of digits/characters \ncreated by a previous federated learning benchmark \n\n\n1, 0.2} for strong attacks, and select \u03bb from {0.1, 1, 2} for all other attacks. For the StackOverflow dataset, we tune \u03bb from {0.01, 0.05, 0.1} for strong attacks, and {0.05, 0.1, 0.3} for all other attacks. We directly evaluate our hyperparameter tuning strategy in\n\nTable 5 .\n5Results (test accuracy and standard deviation) of using dynamic \u03bb's. 'Best \u03bb' refers to the results of selecting the best (fixed) \u03bb based on average validation performance on benign devices (assuming the server knows which devices are malicious). 315(.16)0.325 (.16) 0.315 (.17) 0.313 (.15) 0.314 (.16) 0.350 (.16) 0.312 (.14) 0.316 (.17) 0.321 (.17) 0.327 (.17) dynamic \u03bb's 0.317 (.17) 0.323 (.18) 0.314 (.16) 0.359 (.16) 0.326 (.17) 0.317 (.17) 0.301 (.17) 0.318 (.17) 0.319 (.17) 0.311 (.17)FEMNIST \nA1 (ratio of adversaries) \nA2 (ratio of adversaries) \nA3 (ratio of adversaries) \n\nMethods \nclean \n20% \n50% \n80% \n20% \n50% \n80% \n10% \n15% \n20% \n\nbest \u03bb \n0.836 (.10) 0.803 (.10) 0.767 (.10) 0.672 (.14) 0.792 (.11) 0.743 (.14) 0.674 (.14) 0.691 (.15) 0.664 (.14) 0.650 (.14) \ndynamic \u03bb's \n0.834 (.09) 0.802 (.10) 0.762 (.11) 0.672 (.13) 0.801 (.09) 0.700 (.15) 0.675 (.14) 0.685 (.15) 0.650 (.14) 0.613 (.13) \nFashion \nA1 (ratio of adversaries) \nA2 (ratio of adversaries) \nA3 (ratio of adversaries) \n\nMethods \nclean \n20% \n50% \n80% \n20% \n50% \n80% \n10% \n20% \n50% \n\nbest \u03bb \n0.946 (.06) 0.944 (.08) 0.935 (.07) 0.925 (.07) 0.943 (.08) 0.930 (.07) 0.912 (.08) 0.914 (.09) 0.903 (.09) 0.873 (.09) \ndynamic \u03bb's \n0.943 (.06) 0.944 (.07) 0.937 (.07) 0.907 (.10) 0.938 (.07) 0.930 (.08) 0.913 (.09) 0.921 (.09) 0.902 (.09) 0.872 (.11) \nCelebA \nA1 (ratio of adversaries) \nA2 (ratio of adversaries) \nA3 (ratio of adversaries) \n\nMethods \nclean \n20% \n50% \n80% \n20% \n50% \n80% \n10% \n15% \n20% \n\nbest \u03bb \n0.914 (.18) 0.828 (.22) 0.721 (.27) 0.724 (.28) 0.872 (.22) 0.826 (.26) 0 708 (.29) 0.699 (.28) 0.694 (.27) 0.689 (.28) \ndynamic \u03bb's \n0.911 (.16) 0.820 (.26) 0.714 (.28) 0.724 (.28) 0.872 (.22) 0.826 (.26) 0.706 (.28) 0.699 (.28) 0.694 (.27) 0.689 (.28) \nVehicle \nA1 (ratio of adversaries) \nA2 (ratio of adversaries) \nA3 (ratio of adversaries) \n\nMethods \nclean \n20% \n50% \n80% \n20% \n50% \n80% \n10% \n20% \n50% \n\nbest \u03bb \n0.882 (.05) 0.862 (.05) 0.841 (.09) 0.851 (.06) 0.884 (.05) 0.872 (.06) 0.879 (.04) 0.872 (.06) 0.829 (.08) 0.827 (.08) \ndynamic \u03bb's \n0.872 (.05) 0.857 (.06) 0.827 (.08) 0.834 (.05) 0.872 (.06) 0.867 (.07) 0.848 (.04) 0.839 (.08) 0.824 (.08) 0.822 (.09) \nStackOverflow \nA1 (ratio of adversaries) \nA2 (ratio of adversaries) \nA3 (ratio of adversaries) \n\nMethods \nclean \n20% \n50% \n80% \n20% \n50% \n80% \n10% \n20% \n50% \n\nbest \u03bb \n0.\n\nTable 6 .\n6Ditto augmented with robust baselines (full results). Ditto + multi-Krum 0.875 (.20) 0.722 (.26) 0.733 (.27) 0.903 (.20) 0.902 (.21) 0.885 (.23) 0.713 (.28) 0.709 (.28) 0.713 (.28)FEMNIST \nA1 (ratio of adversaries) \nA2 (ratio of adversaries) \nA3 (ratio of adversaries) \n\nMethods \n20% \n50% \n80% \n20% \n50% \n80% \n10% \n15% \n20% \n\nglobal \n0.773 (.11) 0.727 (.12) 0.574 (.15) 0.774 (.11) 0.703 (.14) 0.636 (.15) 0.517 (.14) 0.487 (.14) 0.364 (.13) \nclipping \n0.791 (.11) 0.736 (.11) 0.408 (.14) 0.791 (.11) 0.736 (.13) 0.656 (.13) 0.795 (.11) 0.060 (.05) 0.061 (.05) \nDitto \n0.803 (.10) 0.767 (.10) 0.672 (.14) 0.792 (.11) 0.743 (.14) 0.674 (.14) 0.691 (.15) 0.664 (.14) 0.650 (.14) \nDitto + clipping \n0.810 (.11) 0.762 (.11) 0.645 (.13) 0.808 (.11) 0.757 (.11) 0.684 (.13) 0.813 (.13) 0.707 (.15) 0.672 (.14) \nCelebA \nA1 (ratio of adversaries) \nA2 (ratio of adversaries) \nA3 (ratio of adversaries) \n\nMethods \n20% \n50% \n80% \n20% \n50% \n80% \n10% \n15% \n20% \n\nglobal \n0.810 (.22) 0.535 (.26) 0.228 (.21) 0.869 (.22) 0.823 (.23) 0.656 (.26) 0.451 (.27) 0.460 (.29) 0.515 (.31) \nmulti-Krum \n0.882 (.22) 0.564 (.26) 0.107 (.19) 0.887 (.21) 0.891 (.20) 0.617 (.30) 0.512 (.27) 0.529 (.27) 0.430 (.26) \nDitto \n0.828 (.22) 0.721 (.27) 0.724 (.28) 0.872 (.22) 0.826 (.26) 0.708 (.29) 0.699 (.28) 0.694 (.27) 0.689 (.28) \n\n\nTable 7 .\n7Full results (average and standard deviation of test accuracy across all devices) on the Vehicle dataset with linear SVM. On this convex problem, we additionally compare with another primal-dual MTL method MOCHA (Smith et al., 2017), which suggests the fairness/robustness benefits of other MTL approaches.Vehicle \nA1 (ratio of adversaries) \nA2 (ratio of adversaries) \nA3 (ratio of adversaries) \n\nMethods \nclean \n20% \n50% \n80% \n20% \n50% \n80% \n10% \n20% \n50% \n\nglobal \n0.866 (.16) 0.847 (.08) 0.643 (.10) 0.260 (.27) 0.866 (.18) 0.840 (.21) 0.762 (.27) 0.854 (.17) 0.606 (.08) 0.350 (.19) \nlocal \n0.836 (.07) 0.835 (.08) 0.840 (.09) 0.857 (.09) 0.835 (.08) 0.840 (.09) 0.857 (.09) 0.840 (.07) 0.835 (.08) 0.840 (.09) \nfair \n0.870 (.08) 0.721 (.06) 0.572 (.08) 0.404 (.13) 0.746 (.12) 0.704 (.15) 0.706 (.20) 0.775 (.13) 0.628 (.25) 0.448 (.11) \nmedian \n0.863 (.16) 0.861 (.18) 0.676 (.11) 0.229 (.31) 0.864 (.18) 0.838 (.21) 0.774 (.28) 0.867 (.17) 0.797 (.07) 0.319 (.17) \nKrum \n0.852 (.17) 0.853 (.19) 0.830 (.22) 0.221 (.32) 0.851 (.19) 0.828 (.22) 0.780 (.31) 0.867 (.17) 0.866 (.18) 0.588 (.14) \nmulti-Krum \n0.866 (.16\n\nTable 8 .\n8Full results (average and standard deviation of test accuracy across all devices) on FEMNIST.FEMNIST \nA1 (ratio of adversaries) \nA2 (ratio of adversaries) \nA3 (ratio of adversaries) \n\nMethods \nclean \n20% \n50% \n80% \n20% \n50% \n80% \n10% \n15% \n20% \n\nglobal \n0.804 (.11) 0.773 (.11) 0.727 (.12) 0.574 (.15) 0.774 (.11) 0.703 (.14) 0.636 (.15) 0.517 (.14) 0.487 (.14) 0.364 (.13) \nlocal \n\n\nTable 9 .\n9Full results (average and standard deviation of test accuracy across all devices) on Fashion MNIST.Fashion MNIST \nA1 (ratio of adversaries) \nA2 (ratio of adversaries) \nA3 (ratio of adversaries) \n\nMethods \nclean \n20% \n50% \n80% \n20% \n50% \n80% \n10% \n20% \n50% \n\n\n\nTable 10 .\n10Full results (average and standard deviation of test accuracy across all devices) on FEMNIST (skewed).FEMNIST (skewed) \nA1 (ratio of adversaries) \nA2 (ratio of adversaries) \nA3 (ratio of adversaries) \n\nMethods \nclean \n20% \n50% \n80% \n20% \n50% \n80% \n10% \n15% \n20% \n\nglobal \n0.720 (.24) 0.657 (.28) 0.585 (.30) 0.435 (.23) 0.688 (.26) 0.631 (.24) 0.589 (.26) 0.023 (.11) 0.038 (.18) 0.039 (.18) \nlocal \n0.915 (.18) 0.903 (.21) 0.937 (.18) 0.902 (.19) 0.903 (.21) 0.937 (.18) 0.902 (.19) 0.881 (.21) 0.912 (.18) 0.903 (.21) \n\n\nTable 11 .\n11Full results (average and standard deviation of test accuracy across all devices) on CelebA.CelebA \nA1 (ratio of adversaries) \nA2 (ratio of adversaries) \nA3 (ratio of adversaries) \n\nMethods \nclean \n20% \n50% \n80% \n20% \n50% \n80% \n10% \n15% \n20% \n\n\n\nTable 12 .\n12Full results (average and standard deviation of test accuracy across all devices) on StackOverflow.StackOverflow \nA1 (ratio of adversaries) \nA2 (ratio of adversaries) \nA3 (ratio of adversaries) \n\nMethods \nclean \n20% \n50% \n80% \n20% \n50% \n80% \n10% \n15% \n20% \n\n\nCarnegie Mellon University 2 Facebook AI. Correspondence to: Tian Li <tianli@cmu.edu>.\nProof. The proof follows by setting X k = 1 n\u00d71 (k \u2208 [K]) in Lemma 8.\nhttp://www.ecs.umass.edu/\u02dcmduarte/Software.html 3 https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/ stackoverflow/load_data.\nDitto, \u03bb=0.05 0.315 (.16) 0.325 (.16) 0.315 (.17) 0.313 (.15) 0.314 (.16) 0.350 (.16) 0.312 (.14) 0.316 (.17) 0.321 (.17) 0.327 (.17) Ditto, \u03bb=0.1 0.309 (.17) 0.318 (.17) 0.315 (.17) 0.293 (.13) 0.309 (.17) 0.316 (.16) 0.307 (.14) 0.319 (.17) 0.302 (.17) 0.305 (.17) Ditto, \u03bb=0.3 0.255 (.18) 0.298 (.18) 0.288 (.17) 0.304 (.16) 0.283 (.17) 0.233 (.18) 0.321 (.20) 0.252 (.17) 0.261 (.19) 0.269 (.17)\nAcknowledgementsThe work of TL, SH, and VS was supported in part by the National Science Foundation Grant IIS1838017, a Google Faculty Award, a Facebook Faculty Award, and the CONIX Research Center. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the National Science Foundation or any other funding agency.Appendix\nTensorflow federated: Machine learning on decentralized data. Tensorflow federated: Machine learning on decentral- ized data. URL https://www.tensorflow.org/ federated.\n\n. A Agarwal, J Langford, C.-Y Wei, Federated residualAgarwal, A., Langford, J., and Wei, C.-Y. Federated residual\n", "annotations": {"author": "[{\"end\":77,\"start\":69},{\"end\":91,\"start\":78},{\"end\":106,\"start\":92},{\"end\":122,\"start\":107}]", "publisher": null, "author_last_name": "[{\"end\":76,\"start\":74},{\"end\":90,\"start\":88},{\"end\":105,\"start\":98},{\"end\":121,\"start\":116}]", "author_first_name": "[{\"end\":73,\"start\":69},{\"end\":87,\"start\":78},{\"end\":97,\"start\":92},{\"end\":115,\"start\":107}]", "author_affiliation": null, "title": "[{\"end\":66,\"start\":1},{\"end\":188,\"start\":123}]", "venue": null, "abstract": "[{\"end\":1107,\"start\":190}]", "bib_ref": "[{\"end\":1291,\"start\":1269},{\"end\":1723,\"start\":1701},{\"end\":2289,\"start\":2261},{\"end\":2305,\"start\":2289},{\"end\":2402,\"start\":2384},{\"end\":2407,\"start\":2402},{\"end\":2492,\"start\":2474},{\"end\":2539,\"start\":2515},{\"end\":2556,\"start\":2539},{\"end\":3213,\"start\":3194},{\"end\":4734,\"start\":4730},{\"end\":6516,\"start\":6495},{\"end\":6532,\"start\":6516},{\"end\":6550,\"start\":6532},{\"end\":6571,\"start\":6550},{\"end\":6588,\"start\":6571},{\"end\":6607,\"start\":6588},{\"end\":6624,\"start\":6607},{\"end\":6642,\"start\":6624},{\"end\":6667,\"start\":6642},{\"end\":6686,\"start\":6667},{\"end\":6834,\"start\":6808},{\"end\":6878,\"start\":6860},{\"end\":6924,\"start\":6902},{\"end\":6942,\"start\":6924},{\"end\":7013,\"start\":6994},{\"end\":7249,\"start\":7227},{\"end\":7873,\"start\":7851},{\"end\":7893,\"start\":7873},{\"end\":7995,\"start\":7979},{\"end\":8144,\"start\":8118},{\"end\":8461,\"start\":8437},{\"end\":8483,\"start\":8461},{\"end\":8500,\"start\":8483},{\"end\":8516,\"start\":8500},{\"end\":8532,\"start\":8516},{\"end\":8593,\"start\":8575},{\"end\":8628,\"start\":8611},{\"end\":8807,\"start\":8788},{\"end\":8932,\"start\":8914},{\"end\":8975,\"start\":8934},{\"end\":9040,\"start\":9014},{\"end\":9428,\"start\":9404},{\"end\":9550,\"start\":9514},{\"end\":9832,\"start\":9829},{\"end\":9858,\"start\":9855},{\"end\":9911,\"start\":9908},{\"end\":10354,\"start\":10334},{\"end\":10372,\"start\":10354},{\"end\":10433,\"start\":10415},{\"end\":10438,\"start\":10433},{\"end\":10850,\"start\":10831},{\"end\":10899,\"start\":10879},{\"end\":10927,\"start\":10901},{\"end\":11689,\"start\":11670},{\"end\":11710,\"start\":11689},{\"end\":11732,\"start\":11710},{\"end\":11782,\"start\":11763},{\"end\":11797,\"start\":11782},{\"end\":11833,\"start\":11797},{\"end\":11851,\"start\":11833},{\"end\":11871,\"start\":11851},{\"end\":11891,\"start\":11871},{\"end\":11908,\"start\":11891},{\"end\":11929,\"start\":11908},{\"end\":12014,\"start\":11992},{\"end\":12033,\"start\":12014},{\"end\":12091,\"start\":12072},{\"end\":12382,\"start\":12360},{\"end\":12400,\"start\":12382},{\"end\":12445,\"start\":12427},{\"end\":12665,\"start\":12638},{\"end\":12686,\"start\":12665},{\"end\":12704,\"start\":12686},{\"end\":12792,\"start\":12768},{\"end\":13639,\"start\":13620},{\"end\":13656,\"start\":13639},{\"end\":13752,\"start\":13736},{\"end\":15283,\"start\":15280},{\"end\":15347,\"start\":15318},{\"end\":15782,\"start\":15779},{\"end\":18470,\"start\":18443},{\"end\":18491,\"start\":18470},{\"end\":18509,\"start\":18491},{\"end\":18578,\"start\":18553},{\"end\":19706,\"start\":19688},{\"end\":20364,\"start\":20361},{\"end\":22836,\"start\":22818},{\"end\":23701,\"start\":23684},{\"end\":23726,\"start\":23701},{\"end\":23745,\"start\":23726},{\"end\":25095,\"start\":25092},{\"end\":26226,\"start\":26204},{\"end\":26397,\"start\":26394},{\"end\":30825,\"start\":30804},{\"end\":30844,\"start\":30825},{\"end\":30848,\"start\":30844},{\"end\":34828,\"start\":34804},{\"end\":35682,\"start\":35677},{\"end\":35715,\"start\":35710},{\"end\":35726,\"start\":35721},{\"end\":35737,\"start\":35732},{\"end\":35748,\"start\":35743},{\"end\":35759,\"start\":35754},{\"end\":35788,\"start\":35783},{\"end\":35799,\"start\":35794},{\"end\":35810,\"start\":35805},{\"end\":35821,\"start\":35816},{\"end\":35832,\"start\":35827},{\"end\":35843,\"start\":35838},{\"end\":35854,\"start\":35849},{\"end\":35865,\"start\":35860},{\"end\":35876,\"start\":35871},{\"end\":35915,\"start\":35910},{\"end\":35926,\"start\":35921},{\"end\":35937,\"start\":35932},{\"end\":35948,\"start\":35943},{\"end\":35959,\"start\":35954},{\"end\":35970,\"start\":35965},{\"end\":35981,\"start\":35976},{\"end\":35992,\"start\":35987},{\"end\":36003,\"start\":35998},{\"end\":36014,\"start\":36009},{\"end\":36032,\"start\":36027},{\"end\":36043,\"start\":36038},{\"end\":36054,\"start\":36049},{\"end\":36065,\"start\":36060},{\"end\":36076,\"start\":36071},{\"end\":36087,\"start\":36082},{\"end\":36098,\"start\":36093},{\"end\":36109,\"start\":36104},{\"end\":36120,\"start\":36115},{\"end\":36285,\"start\":36280},{\"end\":36296,\"start\":36291},{\"end\":36307,\"start\":36302},{\"end\":36318,\"start\":36313},{\"end\":36329,\"start\":36324},{\"end\":36340,\"start\":36335},{\"end\":36351,\"start\":36346},{\"end\":36362,\"start\":36357},{\"end\":36373,\"start\":36368},{\"end\":36402,\"start\":36397},{\"end\":36413,\"start\":36408},{\"end\":36424,\"start\":36419},{\"end\":36435,\"start\":36430},{\"end\":36446,\"start\":36441},{\"end\":36457,\"start\":36452},{\"end\":36468,\"start\":36463},{\"end\":36479,\"start\":36474},{\"end\":36490,\"start\":36485},{\"end\":36529,\"start\":36524},{\"end\":36540,\"start\":36535},{\"end\":36551,\"start\":36546},{\"end\":36562,\"start\":36557},{\"end\":36573,\"start\":36568},{\"end\":36584,\"start\":36579},{\"end\":36595,\"start\":36590},{\"end\":36606,\"start\":36601},{\"end\":36617,\"start\":36612},{\"end\":36628,\"start\":36623},{\"end\":36646,\"start\":36641},{\"end\":36657,\"start\":36652},{\"end\":36668,\"start\":36663},{\"end\":36679,\"start\":36674},{\"end\":36690,\"start\":36685},{\"end\":36701,\"start\":36696},{\"end\":36712,\"start\":36707},{\"end\":36723,\"start\":36718},{\"end\":36734,\"start\":36729},{\"end\":38807,\"start\":38802},{\"end\":38818,\"start\":38813},{\"end\":38829,\"start\":38824},{\"end\":38858,\"start\":38853},{\"end\":38869,\"start\":38864},{\"end\":38880,\"start\":38875},{\"end\":38970,\"start\":38965},{\"end\":38981,\"start\":38976},{\"end\":38992,\"start\":38987},{\"end\":39019,\"start\":39014},{\"end\":39030,\"start\":39025},{\"end\":39041,\"start\":39036},{\"end\":39068,\"start\":39063},{\"end\":39079,\"start\":39074},{\"end\":39090,\"start\":39085},{\"end\":39128,\"start\":39123},{\"end\":39139,\"start\":39134},{\"end\":39150,\"start\":39145},{\"end\":39161,\"start\":39156},{\"end\":39229,\"start\":39224},{\"end\":39240,\"start\":39235},{\"end\":39251,\"start\":39246},{\"end\":39280,\"start\":39275},{\"end\":39291,\"start\":39286},{\"end\":39302,\"start\":39297},{\"end\":39313,\"start\":39308},{\"end\":48986,\"start\":48983},{\"end\":49593,\"start\":49590},{\"end\":50007,\"start\":50004},{\"end\":50066,\"start\":50063},{\"end\":54056,\"start\":54032},{\"end\":54073,\"start\":54056},{\"end\":62148,\"start\":62145},{\"end\":65657,\"start\":65654},{\"end\":66104,\"start\":66082},{\"end\":67376,\"start\":67373},{\"end\":73351,\"start\":73319},{\"end\":73406,\"start\":73385},{\"end\":74573,\"start\":74554},{\"end\":74785,\"start\":74766},{\"end\":75076,\"start\":75050},{\"end\":75092,\"start\":75076},{\"end\":75399,\"start\":75372},{\"end\":75841,\"start\":75820},{\"end\":75995,\"start\":75974},{\"end\":76034,\"start\":76010}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":81494,\"start\":81176},{\"attributes\":{\"id\":\"fig_1\"},\"end\":82047,\"start\":81495},{\"attributes\":{\"id\":\"fig_2\"},\"end\":82199,\"start\":82048},{\"attributes\":{\"id\":\"fig_3\"},\"end\":83243,\"start\":82200},{\"attributes\":{\"id\":\"fig_4\"},\"end\":83700,\"start\":83244},{\"attributes\":{\"id\":\"fig_5\"},\"end\":83874,\"start\":83701},{\"attributes\":{\"id\":\"fig_6\"},\"end\":84112,\"start\":83875},{\"attributes\":{\"id\":\"fig_7\"},\"end\":85259,\"start\":84113},{\"attributes\":{\"id\":\"fig_8\"},\"end\":85904,\"start\":85260},{\"attributes\":{\"id\":\"fig_9\"},\"end\":87705,\"start\":85905},{\"attributes\":{\"id\":\"fig_10\"},\"end\":88416,\"start\":87706},{\"attributes\":{\"id\":\"fig_11\"},\"end\":89187,\"start\":88417},{\"attributes\":{\"id\":\"fig_12\"},\"end\":90331,\"start\":89188},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":90647,\"start\":90332},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":90953,\"start\":90648},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":91955,\"start\":90954},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":92814,\"start\":91956},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":98289,\"start\":92815},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":98820,\"start\":98290},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":99714,\"start\":98821},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":99984,\"start\":99715},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":102323,\"start\":99985},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":103640,\"start\":102324},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":104774,\"start\":103641},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":105169,\"start\":104775},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":105440,\"start\":105170},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":105976,\"start\":105441},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":106235,\"start\":105977},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":106508,\"start\":106236}]", "paragraph": "[{\"end\":1724,\"start\":1123},{\"end\":2557,\"start\":1726},{\"end\":3214,\"start\":2559},{\"end\":3858,\"start\":3216},{\"end\":4487,\"start\":3860},{\"end\":4749,\"start\":4489},{\"end\":5054,\"start\":4751},{\"end\":5426,\"start\":5056},{\"end\":5650,\"start\":5428},{\"end\":5830,\"start\":5652},{\"end\":5970,\"start\":5832},{\"end\":6346,\"start\":6000},{\"end\":6687,\"start\":6348},{\"end\":7143,\"start\":6689},{\"end\":7675,\"start\":7145},{\"end\":7894,\"start\":7677},{\"end\":7996,\"start\":7896},{\"end\":8145,\"start\":7998},{\"end\":9166,\"start\":8147},{\"end\":9632,\"start\":9168},{\"end\":10031,\"start\":9634},{\"end\":11259,\"start\":10033},{\"end\":12265,\"start\":11261},{\"end\":13400,\"start\":12267},{\"end\":14052,\"start\":13402},{\"end\":14942,\"start\":14112},{\"end\":15108,\"start\":14962},{\"end\":15505,\"start\":15156},{\"end\":15611,\"start\":15507},{\"end\":16283,\"start\":15642},{\"end\":16622,\"start\":16398},{\"end\":17790,\"start\":16624},{\"end\":19012,\"start\":17792},{\"end\":20242,\"start\":19014},{\"end\":21451,\"start\":20259},{\"end\":21761,\"start\":21453},{\"end\":22251,\"start\":22066},{\"end\":22976,\"start\":22253},{\"end\":23313,\"start\":22978},{\"end\":23605,\"start\":23315},{\"end\":24439,\"start\":23607},{\"end\":24967,\"start\":24441},{\"end\":25932,\"start\":24969},{\"end\":26227,\"start\":26011},{\"end\":26976,\"start\":26249},{\"end\":27868,\"start\":27035},{\"end\":27913,\"start\":27912},{\"end\":28444,\"start\":27915},{\"end\":29499,\"start\":28446},{\"end\":30100,\"start\":29501},{\"end\":30582,\"start\":30116},{\"end\":31798,\"start\":30584},{\"end\":32812,\"start\":31822},{\"end\":33693,\"start\":32834},{\"end\":34624,\"start\":33730},{\"end\":35514,\"start\":34626},{\"end\":35603,\"start\":35526},{\"end\":35776,\"start\":35654},{\"end\":36020,\"start\":35778},{\"end\":36217,\"start\":36022},{\"end\":36390,\"start\":36268},{\"end\":36634,\"start\":36392},{\"end\":37242,\"start\":36636},{\"end\":37577,\"start\":37244},{\"end\":38411,\"start\":37612},{\"end\":38846,\"start\":38413},{\"end\":38958,\"start\":38848},{\"end\":39007,\"start\":38960},{\"end\":39056,\"start\":39009},{\"end\":39217,\"start\":39058},{\"end\":39268,\"start\":39219},{\"end\":40923,\"start\":39270},{\"end\":41879,\"start\":40954},{\"end\":42104,\"start\":41881},{\"end\":42267,\"start\":42106},{\"end\":42428,\"start\":42269},{\"end\":42738,\"start\":42430},{\"end\":42850,\"start\":42740},{\"end\":42889,\"start\":42852},{\"end\":43048,\"start\":42891},{\"end\":43217,\"start\":43050},{\"end\":43377,\"start\":43219},{\"end\":43507,\"start\":43379},{\"end\":43695,\"start\":43509},{\"end\":43872,\"start\":43697},{\"end\":44048,\"start\":43874},{\"end\":44246,\"start\":44050},{\"end\":44397,\"start\":44248},{\"end\":44590,\"start\":44399},{\"end\":44874,\"start\":44592},{\"end\":45146,\"start\":44876},{\"end\":45323,\"start\":45148},{\"end\":45456,\"start\":45325},{\"end\":45653,\"start\":45458},{\"end\":45795,\"start\":45655},{\"end\":45880,\"start\":45797},{\"end\":46624,\"start\":46033},{\"end\":46721,\"start\":46683},{\"end\":46785,\"start\":46752},{\"end\":46867,\"start\":46864},{\"end\":47014,\"start\":46900},{\"end\":47039,\"start\":47016},{\"end\":47089,\"start\":47068},{\"end\":47168,\"start\":47091},{\"end\":47262,\"start\":47170},{\"end\":47693,\"start\":47264},{\"end\":48033,\"start\":47695},{\"end\":48670,\"start\":48070},{\"end\":48992,\"start\":48672},{\"end\":49083,\"start\":49029},{\"end\":49229,\"start\":49176},{\"end\":49595,\"start\":49385},{\"end\":49957,\"start\":49616},{\"end\":50108,\"start\":49959},{\"end\":50288,\"start\":50110},{\"end\":50361,\"start\":50290},{\"end\":50403,\"start\":50363},{\"end\":50417,\"start\":50405},{\"end\":50460,\"start\":50457},{\"end\":50516,\"start\":50504},{\"end\":50575,\"start\":50562},{\"end\":50681,\"start\":50617},{\"end\":51015,\"start\":50774},{\"end\":51172,\"start\":51160},{\"end\":51271,\"start\":51228},{\"end\":51376,\"start\":51350},{\"end\":51779,\"start\":51378},{\"end\":51860,\"start\":51781},{\"end\":52231,\"start\":51895},{\"end\":52278,\"start\":52233},{\"end\":52420,\"start\":52365},{\"end\":52639,\"start\":52548},{\"end\":52795,\"start\":52641},{\"end\":52854,\"start\":52849},{\"end\":52894,\"start\":52876},{\"end\":53204,\"start\":53112},{\"end\":53339,\"start\":53325},{\"end\":53435,\"start\":53408},{\"end\":53865,\"start\":53490},{\"end\":54201,\"start\":53867},{\"end\":54507,\"start\":54203},{\"end\":54664,\"start\":54652},{\"end\":54956,\"start\":54896},{\"end\":55006,\"start\":54958},{\"end\":55581,\"start\":55522},{\"end\":55722,\"start\":55583},{\"end\":55786,\"start\":55724},{\"end\":55991,\"start\":55788},{\"end\":56322,\"start\":55993},{\"end\":56634,\"start\":56324},{\"end\":56852,\"start\":56711},{\"end\":57159,\"start\":56854},{\"end\":57500,\"start\":57161},{\"end\":57552,\"start\":57549},{\"end\":57681,\"start\":57678},{\"end\":57796,\"start\":57750},{\"end\":57943,\"start\":57917},{\"end\":58504,\"start\":57945},{\"end\":58688,\"start\":58506},{\"end\":58746,\"start\":58741},{\"end\":59077,\"start\":58798},{\"end\":59160,\"start\":59079},{\"end\":59425,\"start\":59316},{\"end\":59727,\"start\":59427},{\"end\":59914,\"start\":59729},{\"end\":59970,\"start\":59916},{\"end\":60102,\"start\":60045},{\"end\":60224,\"start\":60150},{\"end\":60412,\"start\":60226},{\"end\":60599,\"start\":60414},{\"end\":60750,\"start\":60601},{\"end\":60943,\"start\":60752},{\"end\":61219,\"start\":60945},{\"end\":61841,\"start\":61221},{\"end\":62150,\"start\":61889},{\"end\":62203,\"start\":62171},{\"end\":62292,\"start\":62205},{\"end\":62357,\"start\":62319},{\"end\":62370,\"start\":62359},{\"end\":62497,\"start\":62421},{\"end\":62521,\"start\":62514},{\"end\":62535,\"start\":62523},{\"end\":62637,\"start\":62589},{\"end\":62744,\"start\":62689},{\"end\":62847,\"start\":62835},{\"end\":62863,\"start\":62860},{\"end\":62981,\"start\":62896},{\"end\":63052,\"start\":63047},{\"end\":63101,\"start\":63098},{\"end\":63192,\"start\":63123},{\"end\":63272,\"start\":63194},{\"end\":63332,\"start\":63327},{\"end\":63437,\"start\":63366},{\"end\":63647,\"start\":63439},{\"end\":63720,\"start\":63649},{\"end\":63885,\"start\":63722},{\"end\":63999,\"start\":63887},{\"end\":64349,\"start\":64001},{\"end\":64451,\"start\":64446},{\"end\":64526,\"start\":64523},{\"end\":64621,\"start\":64552},{\"end\":64731,\"start\":64623},{\"end\":64793,\"start\":64788},{\"end\":64916,\"start\":64845},{\"end\":65155,\"start\":64918},{\"end\":65228,\"start\":65157},{\"end\":65369,\"start\":65230},{\"end\":66008,\"start\":65411},{\"end\":66375,\"start\":66010},{\"end\":66447,\"start\":66412},{\"end\":66553,\"start\":66529},{\"end\":66740,\"start\":66639},{\"end\":66863,\"start\":66742},{\"end\":66905,\"start\":66865},{\"end\":67010,\"start\":66907},{\"end\":67071,\"start\":67050},{\"end\":67246,\"start\":67115},{\"end\":67416,\"start\":67278},{\"end\":67529,\"start\":67418},{\"end\":68069,\"start\":67551},{\"end\":68424,\"start\":68166},{\"end\":68913,\"start\":68476},{\"end\":69203,\"start\":69199},{\"end\":69902,\"start\":69884},{\"end\":70292,\"start\":70274},{\"end\":70543,\"start\":70514},{\"end\":70622,\"start\":70568},{\"end\":70832,\"start\":70713},{\"end\":70986,\"start\":70834},{\"end\":71059,\"start\":71003},{\"end\":71212,\"start\":71143},{\"end\":71339,\"start\":71302},{\"end\":71573,\"start\":71545},{\"end\":71699,\"start\":71678},{\"end\":72563,\"start\":71701},{\"end\":72669,\"start\":72565},{\"end\":72827,\"start\":72671},{\"end\":73009,\"start\":72918},{\"end\":74169,\"start\":73094},{\"end\":74361,\"start\":74204},{\"end\":74545,\"start\":74363},{\"end\":74916,\"start\":74547},{\"end\":75116,\"start\":74918},{\"end\":75304,\"start\":75175},{\"end\":75954,\"start\":75306},{\"end\":76147,\"start\":75956},{\"end\":76523,\"start\":76149},{\"end\":77321,\"start\":76606},{\"end\":78672,\"start\":77339},{\"end\":78969,\"start\":78719},{\"end\":81175,\"start\":79001}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15155,\"start\":15109},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15641,\"start\":15612},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16397,\"start\":16284},{\"attributes\":{\"id\":\"formula_3\"},\"end\":22065,\"start\":21762},{\"attributes\":{\"id\":\"formula_4\"},\"end\":27034,\"start\":26977},{\"attributes\":{\"id\":\"formula_5\"},\"end\":27911,\"start\":27869},{\"attributes\":{\"id\":\"formula_6\"},\"end\":35653,\"start\":35604},{\"attributes\":{\"id\":\"formula_7\"},\"end\":36267,\"start\":36218},{\"attributes\":{\"id\":\"formula_8\"},\"end\":46751,\"start\":46722},{\"attributes\":{\"id\":\"formula_9\"},\"end\":46812,\"start\":46786},{\"attributes\":{\"id\":\"formula_10\"},\"end\":46863,\"start\":46812},{\"attributes\":{\"id\":\"formula_11\"},\"end\":46899,\"start\":46868},{\"attributes\":{\"id\":\"formula_12\"},\"end\":47067,\"start\":47040},{\"attributes\":{\"id\":\"formula_15\"},\"end\":49028,\"start\":48993},{\"attributes\":{\"id\":\"formula_16\"},\"end\":49175,\"start\":49084},{\"attributes\":{\"id\":\"formula_18\"},\"end\":49324,\"start\":49230},{\"attributes\":{\"id\":\"formula_19\"},\"end\":49615,\"start\":49596},{\"attributes\":{\"id\":\"formula_23\"},\"end\":50456,\"start\":50418},{\"attributes\":{\"id\":\"formula_24\"},\"end\":50503,\"start\":50461},{\"attributes\":{\"id\":\"formula_25\"},\"end\":50561,\"start\":50517},{\"attributes\":{\"id\":\"formula_26\"},\"end\":50616,\"start\":50576},{\"attributes\":{\"id\":\"formula_27\"},\"end\":50773,\"start\":50682},{\"attributes\":{\"id\":\"formula_28\"},\"end\":51159,\"start\":51016},{\"attributes\":{\"id\":\"formula_29\"},\"end\":51227,\"start\":51173},{\"attributes\":{\"id\":\"formula_30\"},\"end\":51349,\"start\":51272},{\"attributes\":{\"id\":\"formula_31\"},\"end\":51894,\"start\":51861},{\"attributes\":{\"id\":\"formula_32\"},\"end\":52364,\"start\":52279},{\"attributes\":{\"id\":\"formula_33\"},\"end\":52547,\"start\":52421},{\"attributes\":{\"id\":\"formula_34\"},\"end\":52848,\"start\":52796},{\"attributes\":{\"id\":\"formula_35\"},\"end\":52875,\"start\":52855},{\"attributes\":{\"id\":\"formula_36\"},\"end\":53111,\"start\":52895},{\"attributes\":{\"id\":\"formula_37\"},\"end\":53324,\"start\":53205},{\"attributes\":{\"id\":\"formula_38\"},\"end\":53407,\"start\":53340},{\"attributes\":{\"id\":\"formula_39\"},\"end\":53489,\"start\":53436},{\"attributes\":{\"id\":\"formula_40\"},\"end\":54651,\"start\":54588},{\"attributes\":{\"id\":\"formula_41\"},\"end\":54895,\"start\":54665},{\"attributes\":{\"id\":\"formula_42\"},\"end\":55521,\"start\":55007},{\"attributes\":{\"id\":\"formula_43\"},\"end\":57548,\"start\":57501},{\"attributes\":{\"id\":\"formula_44\"},\"end\":57677,\"start\":57553},{\"attributes\":{\"id\":\"formula_45\"},\"end\":57749,\"start\":57682},{\"attributes\":{\"id\":\"formula_46\"},\"end\":57916,\"start\":57797},{\"attributes\":{\"id\":\"formula_47\"},\"end\":58740,\"start\":58689},{\"attributes\":{\"id\":\"formula_48\"},\"end\":58797,\"start\":58747},{\"attributes\":{\"id\":\"formula_49\"},\"end\":59315,\"start\":59161},{\"attributes\":{\"id\":\"formula_50\"},\"end\":60044,\"start\":59971},{\"attributes\":{\"id\":\"formula_51\"},\"end\":60149,\"start\":60103},{\"attributes\":{\"id\":\"formula_52\"},\"end\":62170,\"start\":62151},{\"attributes\":{\"id\":\"formula_53\"},\"end\":62318,\"start\":62293},{\"attributes\":{\"id\":\"formula_54\"},\"end\":62420,\"start\":62371},{\"attributes\":{\"id\":\"formula_55\"},\"end\":62513,\"start\":62498},{\"attributes\":{\"id\":\"formula_57\"},\"end\":62588,\"start\":62536},{\"attributes\":{\"id\":\"formula_58\"},\"end\":62688,\"start\":62638},{\"attributes\":{\"id\":\"formula_59\"},\"end\":62834,\"start\":62745},{\"attributes\":{\"id\":\"formula_60\"},\"end\":62859,\"start\":62848},{\"attributes\":{\"id\":\"formula_61\"},\"end\":62895,\"start\":62864},{\"attributes\":{\"id\":\"formula_62\"},\"end\":63046,\"start\":62982},{\"attributes\":{\"id\":\"formula_63\"},\"end\":63097,\"start\":63053},{\"attributes\":{\"id\":\"formula_64\"},\"end\":63122,\"start\":63102},{\"attributes\":{\"id\":\"formula_65\"},\"end\":63326,\"start\":63273},{\"attributes\":{\"id\":\"formula_66\"},\"end\":63365,\"start\":63333},{\"attributes\":{\"id\":\"formula_67\"},\"end\":64445,\"start\":64350},{\"attributes\":{\"id\":\"formula_68\"},\"end\":64522,\"start\":64452},{\"attributes\":{\"id\":\"formula_69\"},\"end\":64551,\"start\":64527},{\"attributes\":{\"id\":\"formula_70\"},\"end\":64787,\"start\":64732},{\"attributes\":{\"id\":\"formula_71\"},\"end\":64844,\"start\":64794},{\"attributes\":{\"id\":\"formula_72\"},\"end\":66411,\"start\":66376},{\"attributes\":{\"id\":\"formula_73\"},\"end\":66528,\"start\":66448},{\"attributes\":{\"id\":\"formula_74\"},\"end\":66638,\"start\":66554},{\"attributes\":{\"id\":\"formula_75\"},\"end\":67049,\"start\":67011},{\"attributes\":{\"id\":\"formula_76\"},\"end\":67114,\"start\":67072},{\"attributes\":{\"id\":\"formula_77\"},\"end\":67277,\"start\":67247},{\"attributes\":{\"id\":\"formula_78\"},\"end\":67550,\"start\":67530},{\"attributes\":{\"id\":\"formula_79\"},\"end\":68165,\"start\":68070},{\"attributes\":{\"id\":\"formula_80\"},\"end\":68475,\"start\":68439},{\"attributes\":{\"id\":\"formula_81\"},\"end\":69198,\"start\":68914},{\"attributes\":{\"id\":\"formula_82\"},\"end\":69658,\"start\":69204},{\"attributes\":{\"id\":\"formula_83\"},\"end\":69883,\"start\":69658},{\"attributes\":{\"id\":\"formula_84\"},\"end\":69971,\"start\":69903},{\"attributes\":{\"id\":\"formula_85\"},\"end\":70031,\"start\":69971},{\"attributes\":{\"id\":\"formula_86\"},\"end\":70113,\"start\":70031},{\"attributes\":{\"id\":\"formula_87\"},\"end\":70148,\"start\":70113},{\"attributes\":{\"id\":\"formula_88\"},\"end\":70216,\"start\":70148},{\"attributes\":{\"id\":\"formula_89\"},\"end\":70273,\"start\":70216},{\"attributes\":{\"id\":\"formula_90\"},\"end\":70513,\"start\":70293},{\"attributes\":{\"id\":\"formula_91\"},\"end\":70567,\"start\":70544},{\"attributes\":{\"id\":\"formula_92\"},\"end\":70712,\"start\":70623},{\"attributes\":{\"id\":\"formula_93\"},\"end\":71002,\"start\":70987},{\"attributes\":{\"id\":\"formula_94\"},\"end\":71142,\"start\":71060},{\"attributes\":{\"id\":\"formula_95\"},\"end\":71301,\"start\":71213},{\"attributes\":{\"id\":\"formula_96\"},\"end\":71544,\"start\":71340},{\"attributes\":{\"id\":\"formula_97\"},\"end\":71677,\"start\":71574},{\"attributes\":{\"id\":\"formula_98\"},\"end\":72862,\"start\":72828},{\"attributes\":{\"id\":\"formula_99\"},\"end\":72917,\"start\":72862},{\"attributes\":{\"id\":\"formula_100\"},\"end\":73040,\"start\":73010},{\"attributes\":{\"id\":\"formula_101\"},\"end\":75174,\"start\":75117}]", "table_ref": "[{\"end\":33240,\"start\":33233},{\"end\":35147,\"start\":35140},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":38252,\"start\":38245},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":38489,\"start\":38482},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":39760,\"start\":39753},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":39926,\"start\":39919},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":65545,\"start\":65538},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":73163,\"start\":73156},{\"attributes\":{\"ref_id\":\"tab_15\"},\"end\":73757,\"start\":73748},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":74283,\"start\":74274},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":78411,\"start\":78404},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":78961,\"start\":78954},{\"end\":81174,\"start\":81173}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1121,\"start\":1109},{\"attributes\":{\"n\":\"2.\"},\"end\":5998,\"start\":5973},{\"attributes\":{\"n\":\"3.\"},\"end\":14110,\"start\":14055},{\"attributes\":{\"n\":\"3.1.\"},\"end\":14960,\"start\":14945},{\"attributes\":{\"n\":\"3.2.\"},\"end\":20257,\"start\":20245},{\"attributes\":{\"n\":\"3.3.\"},\"end\":26009,\"start\":25935},{\"end\":26247,\"start\":26230},{\"attributes\":{\"n\":\"4.\"},\"end\":30114,\"start\":30103},{\"attributes\":{\"n\":\"4.1.\"},\"end\":31820,\"start\":31801},{\"attributes\":{\"n\":\"4.2.\"},\"end\":32832,\"start\":32815},{\"attributes\":{\"n\":\"4.3.\"},\"end\":33728,\"start\":33696},{\"end\":35524,\"start\":35517},{\"attributes\":{\"n\":\"4.4.\"},\"end\":37610,\"start\":37580},{\"attributes\":{\"n\":\"5.\"},\"end\":40952,\"start\":40926},{\"end\":45964,\"start\":45883},{\"end\":46031,\"start\":45967},{\"end\":46681,\"start\":46627},{\"end\":48068,\"start\":48036},{\"end\":49383,\"start\":49326},{\"end\":54587,\"start\":54510},{\"end\":56709,\"start\":56637},{\"end\":61887,\"start\":61844},{\"end\":65409,\"start\":65372},{\"end\":68438,\"start\":68427},{\"end\":73065,\"start\":73042},{\"end\":73092,\"start\":73068},{\"end\":74202,\"start\":74172},{\"end\":76571,\"start\":76526},{\"end\":76604,\"start\":76574},{\"end\":77337,\"start\":77324},{\"end\":78717,\"start\":78675},{\"end\":78999,\"start\":78972},{\"end\":81187,\"start\":81177},{\"end\":82059,\"start\":82049},{\"end\":83712,\"start\":83702},{\"end\":83886,\"start\":83876},{\"end\":84119,\"start\":84114},{\"end\":90342,\"start\":90333},{\"end\":90658,\"start\":90649},{\"end\":98831,\"start\":98822},{\"end\":99995,\"start\":99986},{\"end\":102334,\"start\":102325},{\"end\":103651,\"start\":103642},{\"end\":104785,\"start\":104776},{\"end\":105180,\"start\":105171},{\"end\":105452,\"start\":105442},{\"end\":105988,\"start\":105978},{\"end\":106247,\"start\":106237}]", "table": "[{\"end\":90647,\"start\":90569},{\"end\":90953,\"start\":90778},{\"end\":98289,\"start\":92966},{\"end\":98820,\"start\":98354},{\"end\":99714,\"start\":98853},{\"end\":102323,\"start\":100491},{\"end\":103640,\"start\":102516},{\"end\":104774,\"start\":103959},{\"end\":105169,\"start\":104880},{\"end\":105440,\"start\":105281},{\"end\":105976,\"start\":105557},{\"end\":106235,\"start\":106083},{\"end\":106508,\"start\":106349}]", "figure_caption": "[{\"end\":81494,\"start\":81189},{\"end\":82047,\"start\":81497},{\"end\":82199,\"start\":82061},{\"end\":83243,\"start\":82202},{\"end\":83700,\"start\":83246},{\"end\":83874,\"start\":83714},{\"end\":84112,\"start\":83888},{\"end\":85259,\"start\":84121},{\"end\":85904,\"start\":85262},{\"end\":87705,\"start\":85907},{\"end\":88416,\"start\":87708},{\"end\":89187,\"start\":88419},{\"end\":90331,\"start\":89190},{\"end\":90569,\"start\":90344},{\"end\":90778,\"start\":90660},{\"end\":91955,\"start\":90956},{\"end\":92814,\"start\":91958},{\"end\":92966,\"start\":92817},{\"end\":98354,\"start\":98292},{\"end\":98853,\"start\":98833},{\"end\":99984,\"start\":99717},{\"end\":100491,\"start\":99997},{\"end\":102516,\"start\":102336},{\"end\":103959,\"start\":103653},{\"end\":104880,\"start\":104787},{\"end\":105281,\"start\":105182},{\"end\":105557,\"start\":105455},{\"end\":106083,\"start\":105991},{\"end\":106349,\"start\":106250}]", "figure_ref": "[{\"end\":28206,\"start\":28198},{\"end\":28457,\"start\":28449},{\"end\":28705,\"start\":28697},{\"end\":29342,\"start\":29334},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32188,\"start\":32180},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32275,\"start\":32267},{\"end\":34318,\"start\":34310},{\"end\":36754,\"start\":36746},{\"end\":36930,\"start\":36922},{\"end\":37394,\"start\":37386},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":40557,\"start\":40549},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":77053,\"start\":77045},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":77318,\"start\":77310}]", "bib_author_first_name": "[{\"end\":107804,\"start\":107803},{\"end\":107815,\"start\":107814},{\"end\":107830,\"start\":107826}]", "bib_author_last_name": "[{\"end\":107812,\"start\":107805},{\"end\":107824,\"start\":107816},{\"end\":107834,\"start\":107831}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":107799,\"start\":107631},{\"attributes\":{\"id\":\"b1\"},\"end\":107914,\"start\":107801}]", "bib_title": null, "bib_author": "[{\"end\":107814,\"start\":107803},{\"end\":107826,\"start\":107814},{\"end\":107836,\"start\":107826}]", "bib_venue": "[{\"end\":107691,\"start\":107631}]"}}}, "year": 2023, "month": 12, "day": 17}