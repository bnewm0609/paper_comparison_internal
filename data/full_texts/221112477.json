{"id": 221112477, "updated": "2023-10-06 12:38:48.116", "metadata": {"title": "DSDNet: Deep Structured self-Driving Network", "authors": "[{\"first\":\"Wenyuan\",\"last\":\"Zeng\",\"middle\":[]},{\"first\":\"Shenlong\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Renjie\",\"last\":\"Liao\",\"middle\":[]},{\"first\":\"Yun\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Bin\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Raquel\",\"last\":\"Urtasun\",\"middle\":[]}]", "venue": "Computer Vision \u2013 ECCV 2020", "journal": "Computer Vision \u2013 ECCV 2020", "publication_date": {"year": 2020, "month": 8, "day": 13}, "abstract": "In this paper, we propose the Deep Structured self-Driving Network (DSDNet), which performs object detection, motion prediction, and motion planning with a single neural network. Towards this goal, we develop a deep structured energy based model which considers the interactions between actors and produces socially consistent multimodal future predictions. Furthermore, DSDNet explicitly exploits the predicted future distributions of actors to plan a safe maneuver by using a structured planning cost. Our sample-based formulation allows us to overcome the difficulty in probabilistic inference of continuous random variables. Experiments on a number of large-scale self driving datasets demonstrate that our model significantly outperforms the state-of-the-art.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2008.06041", "mag": "3110109289", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/ZengWLCYU20", "doi": "10.1007/978-3-030-58589-1_10"}}, "content": {"source": {"pdf_hash": "adc1c96768995c49cccdbf75e775d0bcfbf318ee", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2008.06041v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2008.06041", "status": "GREEN"}}, "grobid": {"id": "b1bb5ed1622d522b3e6f4896e7eb001904b0abf4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/adc1c96768995c49cccdbf75e775d0bcfbf318ee.txt", "contents": "\nDSDNet: Deep Structured self-Driving Network\n\n\nWenyuan Zeng wenyuan@uber.com \nUber ATG\n\n\nUniversity of Toronto\n\n\nShenlong Wang slwang@uber.com \nUber ATG\n\n\nUniversity of Toronto\n\n\nRenjie Liao rjliao@uber.com \nUber ATG\n\n\nUniversity of Toronto\n\n\nYun Chen yun.chen@uber.com \nUber ATG\n\n\nBin Yang \nUber ATG\n\n\nUniversity of Toronto\n\n\nRaquel Urtasun urtasun@uber.com \nUber ATG\n\n\nUniversity of Toronto\n\n\nDSDNet: Deep Structured self-Driving Network\nAutonomous drivingmotion predictionmotion planning\nIn this paper, we propose the Deep Structured self-Driving Network (DSDNet), which performs object detection, motion prediction, and motion planning with a single neural network. Towards this goal, we develop a deep structured energy based model which considers the interactions between actors and produces socially consistent multimodal future predictions. Furthermore, DSDNet explicitly exploits the predicted future distributions of actors to plan a safe maneuver by using a structured planning cost. Our sample-based formulation allows us to overcome the difficulty in probabilistic inference of continuous random variables. Experiments on a number of large-scale self driving datasets demonstrate that our model significantly outperforms the state-of-the-art.\n\nIntroduction\n\nThe self-driving problem can be described as safely, comfortably and efficiently maneuvering a vehicle from point A to point B. This task is very complex; Even the most intelligent agents to date (i.e., humans) are very frequently involved in traffic accidents. Despite the development of Advanced Driver-Assistance Systems (ADAS), 1.3 million people die every year on the road, and 20 to 50 million are severely injured.\n\nAvoiding collisions in complicated traffic scenarios is not easy, primarily due to the fact that there are other traffic participants, whose future behaviors are unknown and very hard to predict. A vehicle that is next to our lane and blocked by its leading vehicle might decide to stay in its lane or cut in front of us. A pedestrian waiting on the edge of the road might decide to cross the road at any time. Moreover, the behavior of each actor depends on the actions taken by other actors, making the prediction task even harder. Thus, it is extremely important to model the future motions of actors with multi-modal distributions that also consider the interactions between actors.\n\nTo safely drive on the road, a self-driving vehicle (SDV) needs to detect surrounding actors, predict their future behaviors, and plan safe maneuvers. Despite the recent success of deep learning for perception, the prediction task, due to the aforementioned challenges, remains an open problem. Furthermore, there arXiv:2008.06041v1 [cs.CV] 13 Aug 2020 is also a need to develop motion planners that can take the uncertainty of the predictions into account. Previous works have utilized parametric distributions to model multimodality of motion prediction. Mixture of Gaussians [11,20] are a natural approach due to their close-form inference. However, it is hard to decide the number of modes in advance. Furthermore, these approaches suffer from mode collapse during training [22,20,41]. An alternative is to learn a model distribution from data using, e.g., neural networks. As shown in [42,25,49], a CVAE [47] can be applied to capture multi-modality, and the interactions between actors can be modeled through latent variables. However, it is typically hard/slow to do probabilistic inference and the interaction mechanism does not explicitly model collision which humans want to avoid at all causes. Besides, none of these works have shown the effects upon planning on real-world datasets.\n\nIn this paper we propose the Deep Structured self-Driving Network (DSD-Net), a single neural network that takes raw sensor data as input to jointly detect actors in the scene, predict a multimodal distribution over their future behaviors, and produce safe plans for the SDV. This paper has three key contributions:\n\n-Our prediction module uses an energy-based formulation to explicitly capture the interactions among actors and predict multiple future outcomes with calibrated uncertainty. -Our planning module considers multiple possibilities of how the future might unroll, and outputs a safe trajectory for the self-driving car that respects the laws of traffic and is compliant with other actors. -We address the costly probabilistic inference with a sample-based framework.\n\nDSDNet conducts efficient inference based on message passing over a sampled set of continuous trajectories to obtain the future motion predictions. It then employs a structured motion planning cost function, which combines a cost learned in a data-driven manner and a cost inspired by human prior knowledge on driving (e.g., traffic rules, collision avoidance) to ensure that the SDVs planned path is safe. We refer the reader to Fig. 1 for an overview of our full model.\n\nWe demonstrate the effectiveness of our model on two large-scale real-world datasets: nuScenes [6] and ATG4D, as well as one simulated dataset CARLA-Precog [15,42]. Our method significantly outperforms previous state-of-the-art results on both prediction and planning tasks.\n\n\nRelated Work\n\nMotion Prediction: Two of the main challenges of prediction are modeling interactions among actors and making accurate multi-modal predictions. To address these, [1,18,25,42,49,60,7,26,31] learn per-actor latent representations and model interactions by communicating those latent representations among actors. These methods can naturally work with VAE [23] and produce multi-modal predictions. However, they typically lack interpretability and it is hard to encode prior knowledge, such as the traffic participants' desire to avoid collisions. Different from building implicit distributions with VAE, [11,27] build explicit distributions using mixture of modes (e.g., GMM) where it is easier to perform efficient Fig. 1: DSDNet overview: The model takes LiDAR and map as inputs, processes them with a CNN backbone, and jointly performs object detection, multimodal socially-consistent prediction, and safe planning under uncertainty. Rainbow patterns mean highly likely actors' future positions predicted by our model. probabilistic inference. In this work, we further enhance the model capacity with a non-parametric explicit distribution constructed over a dense set of trajectory samples. In concurrent work [39] use similar representation to ours, but they do not model social interactions and do not demonstrate how such a representation can benefit planning.\n\nRecently, a new prediction paradigm of performing joint detection and prediction has been proposed [57,28,50,30,10,8,9], in which actors' location information is not known a-priori, and needs to be inferred from the sensors. In this work, we will demonstrate our approach in both settings: using sensor data or history of actors' locations as input.\n\nMotion Planning: Provided with perception and prediction results, planing is usually formulated as a cost minimization problem over trajectories. The cost function can be either manually engineered to guarantee certain properties [5,16,36,64], or learned from data through imitation learning or inverse reinforcement learning [44,52,57,63]. However, most of these planners assume detection and prediction to be accurate and certain, which is not true in practice. Thus, [2,19,43,59] consider uncertainties in other actors' behaviors, and formulate collision avoidance in a probabilistic manner. Following this line of work, we also conduct uncertainty-aware motion planning. End-to-end self-driving methods try to fully utilize the power of data-driven approaches and enjoy simple inference. They typically use a neural network to directly map from raw sensor data to planning outputs, and are learned through imitation learning [4,40], or reinforcement learning [13,37] when a simulator [15,33] is available. However, most of them lack interpretability and do not explicitly ensure safety. While our method also benefits from the power of deep learning, in contrast to the aforementioned approaches, we explicitly model interactions between the SDV and the other dynamic agents, achieving safer planning. Furthermore, safety is explicitly accounted for in our planning cost functions.  \n= \" > A A A B 7 3 i c d V D L S g N B E J z 1 G e M r 6 t H L Y B A 8 h Z k g J g E P A R E 8 R j A P S J Y w O 5 l N x s w + n O k V w p K f 8 O J B E a / + j j f / x t k k g o o W N B R V 3 X R 3 e b G S B g j 5 c J a W V 1 b X 1 n M b + c 2 t 7 Z 3 d w t 5 + y 0 S J 5 q L J I x X p j s e M U D I U T Z C g R C f W g g W e E m 1 v f J H 5 7 X u h j Y z C G 5 j E w g 3 Y M J S + 5 A y s 1 L n s p 6 D Z 7 b R f K J I S I Y R S i j N C K 2 f E k l q t W q Z V T D P L o o g W a P Q L 7 7 1 B x J N A h M A V M 6 Z L S Q x u y j R I r s Q 0 3 0 u M i B k f s 6 H o W h q y Q B g 3 n d 0 7 x c d W G W A / 0 r Z C w D P 1 + 0 T K A m M m g W c 7 A w Y j 8 9 v L x L + 8 b g J + 1 U 1 l G C c g Q j 5 f 5 C c K Q 4 S z 5 / F A a s F B T S x h X E t 7 K + Y j p h k H G 1 H e h v D 1 K f 6 f t M o l S k r 0 + r R Y P 1 / E k U O H 6 A i d I I o q q I 6 u U A M 1 E U c K P a A n 9 O z c O Y / O i / M 6 b 1 1 y F j M H 6\nA e c t 0 + J y 5 B H < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" G 5 l a W g w O z r R I M y v a F I l j Q N C U r M 0\n= \" > A A A B 7 3 i c d V D L S g N B E J z 1 G e M r 6 t H L Y B A 8 h Z k g J g E P A R E 8 R j A P S J Y w O 5 l N x s w + n O k V w p K f 8 O J B E a / + j j f / x t k k g o o W N B R V 3 X R 3 e b G S B g j 5 c J a W V 1 b X 1 n M b + c 2 t 7 Z 3 d w t 5 + y 0 S J 5 q L J I x X p j s e M U D I U T Z C g R C f W g g W e E m 1 v f J H 5 7 X u h j Y z C G 5 j E w g 3 Y M J S + 5 A y s 1 L n s p 6 D Z 7 b R f K J I S I Y R S i j N C K 2 f E k l q t W q Z V T D P L o o g W a P Q L 7 7 1 B x J N A h M A V M 6 Z L S Q x u y j R I r s Q 0 3 0 u M i B k f s 6 H o W h q y Q B g 3 n d 0 7 x c d W G W A / 0 r Z C w D P 1 + 0 T K A m M m g W c 7 A w Y j 8 9 v L x L + 8 b g J + 1 U 1 l G C c g Q j 5 f 5 C c K Q 4 S z 5 / F A a s F B T S x h X E t 7 K + Y j p h k H G 1 H e h v D 1 K f 6 f t M o l S k r 0 + r R Y P 1 / E k U O H 6 A i d I I o q q I 6 u U A M 1 E U c K P a A n 9 O z c O Y / O i / M 6 b 1 1 y F j M H 6\nA e c t 0 + J y 5 B H < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" G 5 l a W g w O z r R I M y v a F I l j Q N C U r M 0\n= \" > A A A B 7 3 i c d V D L S g N B E J z 1 G e M r 6 t H L Y B A 8 h Z k g J g E P A R E 8 R j A P S J Y w O 5 l N x s w + n O k V w p K f 8 O J B E a / + j j f / x t k k g o o W N B R V 3 X R 3 e b G S B g j 5 c J a W V 1 b X 1 n M b + c 2 t 7 Z 3 d w t 5 + y 0 S J 5 q L J I x X p j s e M U D I U T Z C g R C f W g g W e E m 1 v f J H 5 7 X u h j Y z C G 5 j E w g 3 Y M J S + 5 A y s 1 L n s p 6 D Z 7 b R f K J I S I Y R S i j N C K 2 f E k l q t W q Z V T D P L o o g W a P Q L 7 7 1 B x J N A h M A V M 6 Z L S Q x u y j R I r s Q 0 3 0 u M i B k f s 6 H o W h q y Q B g 3 n d 0 7 x c d W G W A / 0 r Z C w D P 1 + 0 T K A m M m g W c 7 A w Y j 8 9 v L x L + 8 b g J + 1 U 1 l G C c g Q j 5 f 5 C c K Q 4 S z 5 / F A a s F B T S x h X E t 7 K + Y j p h k H G 1 H e h v D 1 K f 6 f t M o l S k r 0 + r R Y P 1 / E k U O H 6 A i d I I o q q I 6 u U A M 1 E U c K P a A n 9 O z c O Y / O i / M 6 b 1 1 y F j M H 6\nA e c t 0 + J y 5 B H < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" G 5 l a W g w O z r R I M y v a F I l j Q N C U r M 0      Fig. 2: Details of the multimodal social prediction module: For each actor, we sample a set of physically valid trajectories, and use a neural network E traj to assign energies (probabilities) to them. To make different actors' behaviors socially consistent, we employ message passing steps which explicitly model interactions and can encode human prior knowledge (collision avoidance). The final predicted socially-consistent distribution is shown on top right.\n= \" > A A A B 7 3 i c d V D L S g N B E J z 1 G e M r 6 t H L Y B A 8 h Z k g J g E P A R E 8 R j A P S J Y w O 5 l N x s w + n O k V w p K f 8 O J B E a / + j j f / x t k k g o o W N B R V 3 X R 3 e b G S B g j 5 c J a W V 1 b X 1 n M b + c 2 t 7 Z 3 d w t 5 + y 0 S J 5 q L J I x X p j s e M U D I U T Z C g R C f W g g W e E m 1 v f J H 5 7 X u h j Y z C G 5 j E w g 3 Y M J S + 5 A y s 1 L n s p 6 D Z 7 b R f K J I S I Y R S i j N C K 2 f E k l q t W q Z V T D P L o o g W a P Q L 7 7 1 B x J N A h M A V M 6 Z L S Q x u y j R I r s Q 0 3 0 u M i B k f s 6 H o W h q y Q B g 3 n d 0 7 x c d W G W A / 0 r Z C w D P 1 + 0 T K A m M m g W c 7 A w Y j 8 9 v L x L + 8 b g J + 1 U 1 l G C c g Q j 5 f 5 C c K Q 4 S z 5 / F A a s F B T S x h X E t 7 K + Y j p h k H G 1 H e h v D 1 K f 6 f t M o l S k r 0 + r R Y P 1 / E k U O H 6 A i d I I o q q I 6 u U A M 1 E U c K P a A n 9 O z c O Y / O i / M 6 b 1 1 y F j M H 6 A e c t 0 + J y 5 B H < / l a t e x i t > E coll < l a t e x i t s h a 1 _ b a s e 6 4 = \" y y E h F 0 y x r E A a E B f M a j P V O C E o n / o = \" > A A A B 7 3 i c d V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q i R F b A s u C i K 4 r G A f 0 A 4 l k 2 b a 0 M z D J C O U o T / h x o U i b v 0 d d / 6 N m b a C i h 4 I H M 6 5 9 + b e 4 8 V S a I P x h 5 N b W V 1 b 3 8 h v F r a 2 d 3 b 3 i v s H b R 0 l i v E W i 2 S k u h 7 V X I q Q t 4 w w k n d j x W n g S d 7 x J p e Z 3 7 n n S o s o v D X T m L s B H Y X C F 4 w a K 3 W v B q m d I W e D Y g m X M c a E E J Q R U j 3 H l t T r t Q q p I Z J Z F i V Y o j k o v v e H E U s C H h o m q d Y 9 g m P j p l Q Z w S S f F f q J 5 j F l E z r i P U t D G n D t p v N 9 Z + j E K k P k R 8 q + 0 K C 5 + r 0 j p Y H W 0 8 C z l Q E 1 Y / 3 b y 8 S / v F 5 i / J q b i j B O D A / Z 4 i M / k c h E K D s e D Y X i z M i p J Z Q p Y X d F b E w V Z c Z G V L A h f F 2 K / i f t S p n g M r k 5 K z U u l n H k 4 Q i O 4 R Q I V K E B 1 9 C E F j C Q 8 A B P 8 O z c O Y / O i / O 6 K Mv D X T m L s B H Y X C F 4 w a K 3 W v B q m d I W e D Y g m X M c a E E J Q R U j 3 H l t T r t Q q p I Z J Z F i V Y o j k o v v e H E U s C H h o m q d Y 9 g m P j p l Q Z w S S f F f q J 5 j F l E z r i P U t D G n D t p v N 9 Z + j E K k P k R 8 q + 0 K C 5 + r 0 j p Y H W 0 8 C z l Q E 1 Y / 3 b y 8 S / v F 5 i / J q b i j B O D A / Z 4 i M / k c h E K D s e D Y X i z M i p J Z Q p Y X d F b E w V Z c Z G V L A h f F 2 K / i f t S p n g M r k 5 K z U u l n H k 4 Q i O 4 R Q I V K E B 1 9 C E F j C Q 8 A B P 8 O z c O Y / O i / O 6 K Mv D X T m L s B H Y X C F 4 w a K 3 W v B q m d I W e D Y g m X M c a E E J Q R U j 3 H l t T r t Q q p I Z J Z F i V Y o j k o v v e H E U s C H h o m q d Y 9 g m P j p l Q Z w S S f F f q J 5 j F l E z r i P U t D G n D t p v N 9 Z + j E K k P k R 8 q + 0 K C 5 + r 0 j p Y H W 0 8 C z l Q E 1 Y / 3 b y 8 S / v F 5 i / J q b i j B O D A / Z 4 i M / k c h E K D s e D Y X i z M i p J Z Q p Y X d F b E w V Z c Z G V L A h f F 2 K / i f t S p n g M r k 5 K z U u l n H k 4 Q i O 4 R Q I V K E B 1 9 C E F j C Q 8 A B P 8 O z c O Y / O i / O 6 K M\" > A A A B 5 H i c d V B N S w M x E J 2 t X 7 V W r V 6 9 B I v g q S Q 9 2 P Y m i O C x g v 2 A d i n Z N N u G Z r N r k h X K 0 j / h x Y M i / i Z v / h u z b Q U V f R B 4 v DI = \" > A A A B 7 3 i c d V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B U 0 l 6 s C 1 4 K I j g s Y K t h X Y p 2 T R t Q 7 P Z N c k K Z e m f 8 O J B E a / + H W / + G 7 N t B R V 9 E H i 8 N z O Z e U E s h b E Y f 3 i 5 l d W 1 9 Y 3 8 Z m F r e 2 d 3 r 7 h / 0 D Z R o h l v s U h G u h N Q w 6 V Q v G W F l b w T a 0 7 D Q P L b Y H K R + b f 3 X B s R q R s 7 j b k f 0 p E S Q 8 G o d V L n s p + 6 G X L W L 5 Z w G W N M C E E Z I d U z 7 E i 9 X q u Q G i K Z 5 V C C J Z r 9 4 n t v E L E k 5 M o y S Y 3 p E h x b P 6 X a Cx r E A a E B f M a j P V O C E o n / o = \" > A A A B 7 3 i c d V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q i R F b A s u C i K 4 r G A f 0 A 4 l k 2 b a 0 M z D J C O U o T / h x o U i b v 0 d d / 6 N m b a C i h 4 I H M 6 5 9 + b e 4 8 V S a I P x h 5 N b W V 1 b 3 8 h v F r a 2 d 3 b 3 i v s H b R 0 l i v E W i 2 S k u h 7 V X I q Q t 4 w w k n d j x W n g S d 7 x J p e Z 3 7 n n S o s o v D X T m L s B H Y X C F 4 w a K 3 W v B q m d I W e D Y g m X M c a E E J Q R U j 3 H l t T r t Q q p I Z J Z F i V Y o j k o v v e H E U s C H h o m q d Y 9 g m P j p l Q Z w S S f F f q J 5 j F l E z r i P U t D G n D t p v N 9 Z + j E K k P k R 8 q + 0 K C 5 + r 0 j p Y H W 0 8 C z l Q E 1 Y / 3 b y 8 S / v F 5 i / J q b i j B O D A / Z 4 i M / k c h E K D s e D Y X i z M i p J Z Q p Y X d F b E w V Z c Z G V L A h f F 2 K / i f t S p n g M r k 5 K z U u l n H k 4 Q i O 4 R Q I V K E B 1 9 C E F j C Q 8 A B P 8 O z c O Y / O i / O 6 K M 0 5 y 5 5 D + A H n 7 R N + + p B A < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" y y E h F 0 y x r E A a E B f M a j P V O C E o n / o = \" > A A A B 7 3 i c d V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q i R F b A s u C i K 4 r G A f 0 A 4 l k 2 b a 0 M z D J C O U o T / h x o U i b v 0 d d / 6 N m b a C i h 4 I H M 6 5 9 + b e 4 8 V S a I P x h 5 N b W V 1 b 3 8 h v F r a 2 d 3 b 3 i v s H b R 0 l i v E W i 2 S k u h 7 V X I q Q t 4 w w k n d j x W n g S d 7 x J p e Z 3 7 n n S o s o v D X T m L s B H Y X C F 4 w a K 3 W v B q m d I W e D Y g m X M c a E E J Q R U j 3 H l t T r t Q q p I Z J Z F i V Y o j k o v v e H E U s C H h o m q d Y 9 g m P j p l Q Z w S S f F f q J 5 j F l E z r i P U t D G n D t p v N 9 Z + j E K k P k R 8 q + 0 K C 5 + r 0 j p Y H W 0 8 C z l Q E 1 Y / 3 b y 8 S / v F 5 i / J q b i j B O D A / Z 4 i M / k c h E K D s e D Y X i z M i p J Z Q p Y X d F b E w V Z c Z G V L A h f F 2 K / i f t S p n g M r k 5 K z U u l n H k 4 Q i O 4 R Q I V K E B 1 9 C E F j C Q 8 A B P 8 O z c O Y / O i / O 6 K M 0 5 y 5 5 D + A H n 7 R N + + p B A < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" y y E h F 0 y x r E A a E B f M a j P V O C E o n / o = \" > A A A B 7 3 i c d V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q i R F b A s u C i K 4 r G A f 0 A 4 l k 2 b a 0 M z D J C O U o T / h x o U i b v 0 d d / 6 N m b a C i h 4 I H M 6 5 9 + b e 4 8 V S a I P x h 5 N b W V 1 b 3 8 h v F r a 2 d 3 b 3 i v s H b R 0 l i v E W i 2 S k u h 7 V X I q Q t 4 w w k n d j x W n g S d 7 x J p e Z 3 7 n n S o s o v D X T m L s B H Y X C F 4 w a K 3 W v B q m d I W e D Y g m X M c a E E J Q R U j 3 H l t T r t Q q p I Z J Z F i V Y o j k o v v e H E U s C H h o m q d Y 9 g m P j p l Q Z w S S f F f q J 5 j F l E z r i P U t D G n D t p v N 9 Z + j E K k P k R 8 q + 0 K C 5 + r 0 j p Y H W 0 8 C z l Q E 1 Y / 3 b y 8 S / v F 5 i / J q b i j B O D A / Z 4 i M / k c h E K D s e D Y X i z M i p J Z Q p Y X d F b E w V Z c Z G V L A h f F 2 K / i f t S p n g M r k 5 K z U u l n H k 4 Q i O 4 R Q I V K E B 1 9 C E F j C Q 8 A B P 8 O z c O Y / O i / O 6 K M 0 5 y 5 5 D + A H n 7 R N + + p B A < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" y y E h F 0 y x r E A a E B f M a j P V O C E o n / o = \" > A A A B 7 3 i c d V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q i R F b A s u C i K 4 r G A f 0 A 4 l k 2 b a 0 M z D J C O U o T / h x o U i b v 0 d d / 6 N m b a C i h 4 I H M 6 5 9 + b e 4 8 V S a I P x h 5 N b W V 1 b 3 8 h v F r a 2 d 3 b 3 i v s H b R 0 l i v E W i 2 S k u h 7 V X I q Q t 4 w w k n d j x W n g S d 7 x J p e Z 3 7 n n S o s o v D X T m L s B H Y X C F 4 w a K 3 W v B q m d I W e D Y g m X M c a E E J Q R U j 3 H l t T r t Q q p I Z J Z F i V Y o j k o v v e H E U s C H h o m q d Y 9 g m P j p l Q Z w S S f F f q J 5 j F l E z r i P U t D G n D t p v N 9 Z + j E K k P k R 8 q + 0 K C 5 + r 0 j p Y H W 0 8 C z l Q E 1 Y / 3 b y 8 S / v F 5 i / J q b i j B O D A / Z 4 i M / k c h E K D s e D Y X i z M i p J Z Q p Y X d F b E w V Z c Z G V L A h f F 2 K / i f t S p n g M r k 5 K z U u l n H k 4 Q i O 4 R Q I V K E B 1 9 C E F j C Q 8 A B P 8 O z c O Y / O i / O 6 K M 0 5 y 5 5 D + A H n 7 R N + + p B A < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" y y E h F 0 y x r E A a E B f M a j P V O C E o n / o = \" > A A A B 7 3 i c d V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q i R F b A s u C i K 4 r G A f 0 A 4 l k 2 b a 0 M z D J C O U o T / h x o U i b v 0 d d / 6 N m b a C i h 4 I H M 6 5 9 + b e 4 8 V S a I P x h 5 N b W V 1 b 3 8 h v F r a 2 d 3 b 3 i v s H b R 0 l i v E W i 2 S k u h 7 V X I q Q t 4 w w k n d j x W n g S d 7 x J p e Z 3 7 n n S o s o v D X T m L s B H Y X C F 4 w a K 3 W v B q m d I W e D Y g m X M c a E E J Q R U j 3 H l t T r t Q q p I Z J Z F i V Y o j k o v v e H E U s C H h o m q d Y 9 g m P j p l Q Z w S S f F f q J 5 j F l E z r i P U t D G n D t p v N 9 Z + j E K k P k R 8 q + 0 K C 5 + r 0 j p Y H W 0 8 C z l Q E 1 Y / 3 b y 8 S / v F 5 i / J q b i j B O D A / Z 4 i M / k c h E K D s e D Y X i z M i p J Z Q p Y X d F b E w V Z c Z G V L A h f F 2 K / i f t S p n g M r k 5 K z U u l n H k 4 Q i O 4 R Q I V K E B 1 9 C E F j C Q 8 A B P 8 O z c O Y / O i / O 6 K M 0 5 y 5 5 D + A H n 7 R N + + p B A < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" y y E h F 0 y x r E A a E B f M a j P V O C E o n / o = \" > A A A B 7 3 i c d V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q i R F b A s u C i K 4 r G A f 0 A 4 l k 2 b a 0 M z D J C O U o T / h x o U i b v 0 d d / 6 N m b a C i h 4 I H M 6 5 9 + b e 4 8 V S a I P x h 5 N b W V 1 b 3 8 h v F r a 2 d 3 b 3 i v s H b R 0 l i v E W i 2 S k u h 7 V X I q Q t 4 w w k n d j x W n g S d 7 x J p e Z 3 7 n n S o s o v D X T m L s B H Y X C F 4 w a K 3 W v B q m d I W e D Y g m X M c a E E J Q R U j 3 H l t T r t Q q p I Z J Z F i V Y o j k o v v e H E U s C H h o m q d Y 9 g m P j p l Q Z w S S f F f q J 5 j F l E z r i P U t D G n D t p v N 9 Z + j E K k P k R 8 q + 0 K C 5 + r 0 j p Y H W 0 8 C z l Q E 1 Y / 3 b y 8 S / v F 5 i / J q b i j B O D A / Z 4 i M / k c h E K D s e D Y X i z M i p J Z Q p Y X d F b E w V Z c Z G V L A h f F 2 K / i f t S p n g M r k 5 K z U u l n H k 4 Q i O 4 R Q I V K E B 1 9 C E F j C Q 8 A B P 8 O z c O Y / O i / O 6 K M 0 5 y 5 5 D + A H n 7 R N + + p B A < / l a t e x i t >\nStructured models and Belief Propagation: To encode prior knowledge, there is a recent surge of deep structured models [3,12,17,46,32], which use deep neural networks (DNNs) to provide the energy terms of a probabilistic graphical models (PGMs). Combining the powerful learning capacity of DNNs and the task-specific structure imposed by PGMs, deep structured models have been successfully applied to various computer vision problems, e.g., semantic segmentation [46], anomaly detection [58], contour segmentation [34]. However, for continuous random variables, inference is very challenging. Sample-based belief propagation (BP) [51,56,21,48,54,53], address this issue by first constructing the approximation of the continuous distribution via Markov Chain Monte Carlo (MCMC) samples and then performing inference via BP. Inspired by these works, we design a deep structured model that can learn complex human behaviors from large data while incorporating our prior knowledge. We also bypass the difficulty in continuous variable inference using a physically valid sampling procedure.\n\n\nDeep Structured self-Driving Network\n\nGiven sensor measurements and a map of the environment, the objective of a self-driving vehicle (SDV) is to select a trajectory to execute (amongst all feasible ones) that is safe, comfortable, and allows the SDV to reach its destination. In order to plan a safe maneuver, a self-driving vehicle has to first understand its surroundings as well as predict how the future might evolve. It should then plan its motion by considering all possibilities of the future weighting them properly. This is not trivial as the future is very multi-modal and actors interact with each other. Moreover, the inference procedure needs to be performed in a fraction of a second in order to have practical value.\n\nIn this paper we propose DSDNet, a single neural network that jointly detects actors in the scene, predicts a socially consistent multimodal distribution over their future behaviors, and produces safe motion plans for the SDV. Fig. 1 gives an overview of our proposed approach. We first utilize a backbone network to compute the intermediate feature-maps, which are then used for detection, prediction and planning. After detecting actors with a detection header, a deep structured probabilistic inference module computes the distributions of actors' future trajectories, taking into account the interactions between them. Finally, our planning module outputs the planned trajectory by considering both the contextual information encoded in the feature-maps as well as possible futures predicted from the model.\n\nIn the following, we first briefly explain the input representation, backbone network and detection module. We then introduce our novel probabilistic prediction and motion planning framework in sections 3.2 and 3.3 respectively. Finally, we illustrate how to train our model end-to-end in section 3.4.\n\n\nBackbone Feature Network and Object Detection\n\nLet X be the LiDAR point clouds and the HD map given as input to our system. Since LiDAR point clouds can be very sparse and the actors' motion is an important cue for detection and prediction, we use the past 10 LiDAR sweeps (e.g., 1s of measurements) and voxelize them into a 3D tensor [30,55,61,57]. We utilize HD maps as they provide a strong prior about the scene. Following [57], we rasterize the lanes with different semantics (e.g., straight, turning, blocked by traffic light) into different channels and concatenate them with the 3D LiDAR tensor to form our input representation. We then process this 3D tensor with a deep convolutional network backbone and compute a backbone feature map F \u2208 R H\u00d7W \u00d7C , where H, W correspond to the spatial resolution after downsampling (backbone) and C is the channel number. We then employ a single-shot detection header on this feature map to output detection bounding boxes for the actors in the scene. We apply two Conv2D layers separately on F, one for classifying if a location is occupied by an actor, the other for regressing the position offset, size, orientation and speed of each actor. Our prediction and planning modules will then take these detections and the feature map as input to produce both a distribution over the actors' behaviors and a safe planning maneuver. For more details on our detector and backbone network please refer to the supplementary material.\n\n\nProbabilistic Multimodal Social Prediction\n\nIn order to plan a safe maneuver, we need to predict how other actors could potentially behave in the next few seconds. As actors move on the ground, we represent their possible future behavior using a trajectory defined as a sequence of 2D waypoints on birds eye view (BEV) sampled at T discrete timestamps. Note that T is the same duration as our planning horizon, and we compute the motion prediction distribution and a motion plan each time a new sensor measurement arises (i.e., every 100ms).\n\nOutput Parameterization: Let s i \u2208 R T \u00d72 be the future trajectory of the i-th actor. We are interested in modeling the joint distribution of all actors condition on the input, that is p(s 1 , \u00b7 \u00b7 \u00b7 , s N |X). Modeling this joint distribution and performing efficient inference is challenging, as each actor has a highdimensional continuous action space. Here, we propose to approximate this highdimensional continuous space with a finite number of samples, and construct a non-parametric distribution over the sampled space. Specifically, for each actor, we randomly sample K possible future trajectory {\u015d 1 i , \u00b7 \u00b7 \u00b7 ,\u015d K i } from the original continuous trajectory space R T \u00d72 . We then constrain the possible future state of each actor to be one of those K samples. To ensure samples are always diverse, dense 3 and physically plausible, we follow the Neural Motion Planner (NMP) [57] and use a combination of straight, circle, and clothoid curves. More details and analysis of the sampler can be found in the supplementary material.\n\nModeling Future Behavior of All Actors: We employ an energy formulation to measure the probability of each possible future configuration of all actors in the scene: a configuration (s 1 , \u00b7 \u00b7 \u00b7 , s N ) has low energy if it is likely to happen. We can then compute the joint distribution of all actors' future behaviors as\np(s 1 , \u00b7 \u00b7 \u00b7 , s N |X, w) = 1 Z exp (\u2212E(s 1 , \u00b7 \u00b7 \u00b7 , s N |X, w)) ,(1)\nwhere w are learnable parameters, X is the raw sensor data and Z is the partition function Z = exp(\u2212E(\u015d k1 1 , \u00b7 \u00b7 \u00b7 ,\u015d k N N )) summing over all actors' possible states. We construct the energy E(s 1 , \u00b7 \u00b7 \u00b7 , s N |X, w) inspired by how humans drive, e.g., following common sense as well as traffic rules. For example, humans drive smoothly along the road and avoid collisions with each other. Therefore, we decompose the energy E into two terms. The first term encodes the goodness of a future trajectory (independent of other actors) while the second term explicitly encodes the fact that pairs of actors should not collide in the future.\nE(s 1 , \u00b7 \u00b7 \u00b7 , s N |X, w) = N i=1 E traj (s i |X, w traj ) + N i=1 N i =j E coll (s i , s j |X, w coll ) (2)\nwhere N is the number of detected actors and w traj and w coll are the parameters.\n\nSince the goodness E traj (s i |X, w traj ) is hard to define manually, we use a neural network to learn it from data (see Fig. 3). Given the sensor data X and a proposed trajectory s i , the network will output a scalar value. Towards this goal, we first use the detected bounding box of the i-th actor and apply ROIAlign to the backbone feature map, followed by several convolution layers to compute the actor's feature. Note that the backbone feature map is expected to encode rich information about both the environment and the actor.  Fig. 3: Neural header for evaluating E traj and C traj .\n\n\nt s c t P X B w O O 9 G W b m B Y k U B l 3 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o l T z X i T x T L W n Y A a L o X i T R Q o e S f R n E a B 5 O 1 g X J / 5 7 S e u j Y j V P U 4 S 7 k d 0 q E Q o G E U r d e r 9 D D V 9 m P b L F b f q z k F W i Z e T C u R o 9 M t f v U H M 0 o g r Z J I a 0 / X c B P 2 M a h R M 8 m m p l x q e U D a m Q 9 6 1 V N G I G z + b 3 z s l Z 1 Y Z k D D W t h S S u f p 7 I q O R M Z M o s J 0 R x Z F Z 9 m b i f 1 4 3 x f D a z 4 R K U u S K L R a F q S Q Y k 9 n z Z C A 0 Z y g n l l C m h b 2 V s B H V l K G N q G R D 8 J Z f X i W t i 6 r n V r 2 7 y 0 r t J o + j C C d w C u f g w R X U 4 B Y a 0 A Q G E p 7 h F d 6 c R + f F e X c + F q 0 F J 5 8 5 h j 9 w P n 8 A O l m Q E Q = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" E m t + B w x Y R h 1 z S s j I 4 f T E f d G a y 0 g = \" > A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 J M U e v F Y w X 5 A G 8 p m u 2 n X b j Z x d y K U 0 D / h x Y M i X v 0 7 3 v w 3 b t s c t P X B w O O 9 G W b m B Y k U B l 3 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o l T z X i T x T L W n Y A a L o X i T R Q o e S f R n E a B 5 O 1 g X J / 5 7 S e u j Y j V P U 4 S 7 k d 0 q E Q o G E U r d e r 9 D D V 9 m P b L F b f q z k F W i Z e T C u R o 9 M t f v U H M 0 o g r Z J I a 0 / X c B P 2 M a h R M 8 m m p l x q e U D a m Q 9 6 1 V N G I G z + b 3 z s l Z 1 Y Z k D D W t h S S u f p 7 I q O R M Z M o s J 0 R x Z F Z 9 m b i f 1 4 3 x f D a z 4 R K U u S K L R a F q S Q Y k 9 n z Z C A 0 Z y g n l l C m h b 2 V s B H V l K G N q G R D 8 J Z f X i W t i 6 r n V r 2 7 y 0 r t J o + j C C d w C u f g w R X U 4 B Y a 0 A Q G E p 7 h F d 6 c R + f F e X c + F q 0 F J 5 8 5 h j 9 w P n 8 A O l m Q E Q = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" E m t + B w x Y R h 1 z S s j I 4 f T E f d G a y 0 g = \" > A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 J M U e v F Y w X 5 A G 8 p m u 2 n X b j Z x d y K U 0 D / h x Y M i X v 0 7 3 v w 3 b t s c t P X B w O O 9 G W b m B Y k U B l 3 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o l T z X i T x T L W n Y A a L o X i T R Q o e S f R n E a B 5 O 1 g X J / 5 7 S e u j Y j V P U 4 S 7 k d 0 q E Q o G E U r d e r 9 D D V 9 m P b L F b f q z k F W i Z e T C u R o 9 M t f v U H M 0 o g r Z J I a 0 / X c B P 2 M a h R M 8 m m p l x q e U D a m Q 9 6 1 V N G I G z + b 3 z s l Z 1 Y Z k D D W t h S S u f p 7 I q O R M Z M o s J 0 R x Z F Z 9 m b i f 1 4 3 x f D a z 4 R K U u S K L R a F q S Q Y k 9 n z Z C A 0 Z y g n l l C m h b 2 V s B H V l K G N q G R D 8 J Z f X i W t i 6 r n V r 2 7 y 0 r t J o + j C C d w C u f g w R X U 4 B Y a 0 A Q G E p 7 h F d 6 c R + f F e X c + F q 0 F J 5 8 5 h j 9 w P n 8 A O l m Q E Q = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" E m t + B w x Y R h 1 z S s j I 4 f T E f d G a y 0 g = \" > A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 J M U e v F Y w X 5 A G 8 p m u 2 n X b j Z x d y K U 0 D / h x Y M i X v 0 7 3 v w 3 b t s c t P X B w O O 9 G W b m B Y k U B l 3 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o l T z X i T x T L W n Y A a L o X i T R Q o e S f R n E a B 5 O 1 g X J / 5 7 S e u j Y j V P U 4 S 7 k d 0 q E Q o G E U r d e r 9 D D V 9 m P b L F b f q z k F W i Z e T C u R o 9 M t f v U H M 0 o g r Z J I a 0 / X c B P 2 M a h R M 8 m m p l x q e U D a m Q 9 6 1 V N G I G z + b 3 z s l Z 1 Y Z k D D W t h S S u f p 7 I q O R M Z M o s J 0 R x Z F Z 9 m b i f 1 4 3 x f D a z 4 R K U u S K L R a F q S Q Y k 9 n z Z C A 0 Z y g n l l C m h b 2 V s B H V l K G N q G R D 8 J Z f X i W t i 6 r n V r 2 7 y 0 r t J o + j C C d w C u f g w R X U 4 B Y a 0 A Q G E p 7 h F d 6 c R + f F e X c + F q 0 F J 5 8 5 h j 9 w P n 8 A O l m Q E Q = = < / l a t e x i t >\n\n\n/ < l a t e x i t s h a 1 _ b a s e 6 4 = \" 4 m U h t D y 5 T t R h s t h k q F U L h 5 2 8 D n o = \" > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 1 E 0 J M U v H h s w X 5 A G 8 p m O 2 n X b j Z h d y O U 0 F / g x Y M i X v 1 J 3 v w 3 b t s c t P X B w O O 9 G W b m B Y n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L R 2 n i m G T x S J W n Y B q F F x i 0 3 A j s J M o p F E g s B 2 M 7 2 Z + + w m V 5 r F 8 M J M E / Y g O J Q 8 5 o 8 Z K j Y t + u e J W 3 T n I K v F y U o E c 9 X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 8 T M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l r c u q 5 1 a 9 x l W l d p v H U Y Q T O I V z 8 O A a a n A P d W g C A 4 R n e I U 3 5 9 F 5 c d 6 d j 0 V r w c l n j u E P n M 8 f d m G M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 4 m U h t D y 5 T t R h s t h k q F U L h 5 2 8 D n o = \" > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 1 E 0 J M U v H h s w X 5 A G 8 p m O 2 n X b j Z h d y O U 0 F / g x Y M i X v 1 J 3 v w 3 b t s c t P X B w O O 9 G W b m B Y n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L R 2 n i m G T x S J W n Y B q F F x i 0 3 A j s J M o p F E g s B 2 M 7 2 Z + + w m V 5 r F 8 M J M E / Y g O J Q 8 5 o 8 Z K j Y t + u e J W 3 T n I K v F y U o E c 9 X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 8 T M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l r c u q 5 1 a 9 x l W l d p v H U Y Q T O I V z 8 O A a a n A P d W g C A 4 R n e I U 3 5 9 F 5 c d 6 d j 0 V r w c l n j u E P n M 8 f d m G M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 4 m U h t D y 5 T t R h s t h k q F U L h 5 2 8 D n o = \" > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 1 E 0 J M U v H h s w X 5 A G 8 p m O 2 n X b j Z h d y O U 0 F / g x Y M i X v 1 J 3 v w 3 b t s c t P X B w O O 9 G W b m B Y n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L R 2 n i m G T x S J W n Y B q F F x i 0 3 A j s J M o p F E g s B 2 M 7 2 Z + + w m V 5 r F 8 M J M E / Y g O J Q 8 5 o 8 Z K j Y t + u e J W 3 T n I K v F y U o E c 9 X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 8 T M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l r c u q 5 1 a 9 x l W l d p v H U Y Q T O I V z 8 O A a a n A P d W g C A 4 R n e I U 3 5 9 F 5 c d 6 d j 0 V r w c l n j u E P n M 8 f d m G M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 4 m U h t D y 5 T t R h s t h k q F U L h 5 2 8 D n o = \" > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 1 E 0 J M U v H h s w X 5 A G 8 p m O 2 n X b j Z h d y O U 0 F / g x Y M i X v 1 J 3 v w 3 b t s c t P X B w O O 9 G W b m B Y n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L R 2 n i m G T x S J W n Y B q F F x i 0 3 A j s J M o p F E g s B 2 M 7 2 Z + + w m V 5 r F 8 M J M E / Y g O J Q 8 5 o 8 Z K j Y t + u e J W 3 T n I K v F y U o E c 9 X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z / N A p O b P K g I S x s i U N m a u / J z I a a T 2 J A t s Z U T P S y 9 5 M / M / r p i a 8 8 T M u k 9 S g Z I t F Y S q I i c n s a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z k Q / C W X 1 4 l r c u q 5 1 a 9 x l W l d p v H U Y Q T O I V z 8 O A a a n A P d W g C A 4 R n e I U 3 5 9 F 5 c d 6 d j 0 V r w c l n j u E P n M 8 f d m G M r w = = < / l a t e x i t >\n\nWe then index (bilinear interpolation) T features on the backbone feature map at the positions of trajectory's waypoints, and concatenate them together with (x t , y t , cos(\u03b8 t ), sin(\u03b8 t ), distance t ) to form the trajectory feature of s i . Finally, we feed both actor and trajectory features into an MLP which outputs E traj (s i |X, w traj ). Fig.. 3 shows an illustration of the architecture. We use a simple yet effective collision energy: E(s i , s j ) = \u03b3 if s i collides with s j , and E(s i , s j ) = 0 otherwise, to explicitly model the collision avoidance interaction between actors as explained in the next paragraph. We found this simple pairwise energy empirically works well, the exploration of other learnable pairwise energy is thus left as future work.\n\nMessage Passing Inference: For safety, our motion planner needs to take all possible actor's future into consideration. Therefore, motion forecasting needs to infer the probability of each actor taking a particular future trajectory: p(s i =\u015d k i |X, w). We thus conduct marginal inference over the joint distribution. Note that the joint probability defined in Eq. (2) represents a deep structured model (i.e., a Markov random field with potentials computed with deep neural networks). We utilize sum-product message passing [56] to estimate the marginal distribution per actor, taking into account the effects of all other actors by marginalization. The marginal p(s i |X, w) reflects the uncertainty and multimodality in an actor's future behavior and will be leveraged by our planner. We use the following update rule in an iterative manner for each actor (s i ):\nm ij (s j ) \u221d si\u2208{s k i } e \u2212Etraj (si)\u2212E coll (si,sj ) n =i,j m ni (s i )(3)\nwhere m ij is the message sent from actor i to actor j and \u221d means equal up to a normalization constant. Through this message passing procedure, actors communicate with each others their future intentions s i and how probable those intentions are E traj (s i ). The collision energy E coll helps to coordinate intentions from different actors such that the behaviors are compliant and do not result in collision. After messages have been passed for a fixed number of iterations, we compute the approximated marginal as\np(s i =\u015d k i |X, w) \u221d e \u2212Etraj(\u015d k i ) j =i m ji (\u015d k i ).(4)\nSince we typically have a small graph (less than 100 actors in a scene) and each s i only has K possible values {\u015d 1 i , \u00b7 \u00b7 \u00b7 ,\u015d K i }, we can efficiently evaluate Eq. (3) and Eq. (4) via matrix multiplication on GPUs. In practice we find that our energy in Eq. (2) usually results in sparse graphs: most actor will only interact with nearby actors, especially the actors in the front and in the back. As a result, the message passing converges within 5 iterations 4 . With our non-highly-optimized implementation, the prediction module takes less than 100 ms on average, and thus it satisfies our real-time requirements.\n\n\nSafe Motion Planning under Uncertain Future\n\nThe motion planning module fulfills our final goal, that is, navigating towards a destination while avoiding collision and obeying traffic rules. Towards this goal, we build a cost function C, which assigns lower cost values to \"good\" trajectory proposals and higher values to \"bad\" ones. Planning is then performed by finding the optimal trajectory with the minimum cost \u03c4 \u03c4 \u03c4 * = arg min\n\u03c4 \u03c4 \u03c4 \u2208P C (\u03c4 \u03c4 \u03c4 |p(s 1 , \u00b7 \u00b7 \u00b7 , s N ), X, w) ,(5)\nwith \u03c4 \u03c4 \u03c4 * the planned optimal trajectory and P the set of physically realizable trajectories that do not violate the SDV's dynamics. In practice, we sample a large number of future trajectories for the SDV conditioned on its current dynamic state (e.g., velocity and acceleration) to form P, which gives us a finite set of feasible trajectories P = {\u03c4 \u03c4 \u03c4 1 , \u00b7 \u00b7 \u00b7 ,\u03c4 \u03c4 \u03c4 K }. We use the same sampler as described in section 3.2 to ensure we get a wide variety of physically possible trajectories.\n\nPlanning Cost: Given a SDV trajectory \u03c4 \u03c4 \u03c4 , we compute the cost based on how good \u03c4 \u03c4 \u03c4 is 1) conditioned on the scene, (e.g., traffic lights and road topology); 2) considering all other actors' future behaviors (i.e., marginal distribution estimated from the prediction module). We thus define our cost as C(\u03c4 \u03c4 \u03c4 |p(s1, \u00b7 \u00b7 \u00b7 , sN ), X, w) = Ctraj(\u03c4 \u03c4 \u03c4 |X, w) (6) where C traj models the goodness of a SDV trajectory using a neural network. Similar to E traj in the prediction module, we use the trajectory feature and ROIAlign extracted from the backbone feature map to compute this scalar cost value. The collision cost C coll is designed for guaranteeing safety and avoid collision: i.e., C coll (\u03c4 \u03c4 \u03c4 , s i ) = \u03bb if \u03c4 \u03c4 \u03c4 and s i colide, and 0 otherwise. This ensures a dangerous trajectory \u03c4 \u03c4 \u03c4 will incur a very high cost and will be rejected by our cost minimization inference process. Furthermore, Eq. (6) evaluates the expected collision cost E p(si|X,w) [C coll ]. Such a formulation is helpful for safe motion planning since the future is uncertain and we need to consider all possibilities, properly weighted by how likely they are to happen.\n+ N i=1 E p(s i |X,w) [C coll (\u03c4 \u03c4 \u03c4 , si|X, w)] ,\nInference: We conduct exact minimization over P. C traj is a neural network based cost and we can evaluate all K possible trajectories with a single batch forward pass. C coll can be computed with a GPU based collision checker. As a consequence, we can efficiently evaluate C(\u03c4 \u03c4 \u03c4 ) for all K samples and select the trajectory with minimum-cost as our final planning result.\n\n\nLearning\n\nWe train the full model (backbone, detection, prediction and planning) jointly with a multi-class loss defined as follows\nL = L planning + \u03b1L prediction + \u03b2L detection .(7)\nwhere \u03b1, \u03b2 are constant hyper-parameters. Such a multi-task loss can fully exploit the supervision for each task and help the training 5 .\n\nDetection Loss: We employ a standard detection loss L detection , which is a sum of classification and regression loss. We use a cross-entropy classification loss and assign an anchor's label based on its IoU with any actor. The regression loss is a smooth 1 between our model regression outputs and the ground-truth targets. Those targets include position, size, orientation and velocity. We refer the reader to the supplementary material for more details.\n\nPrediction Loss: As our prediction module outputs a discrete distribution for each actor's behavior, we employ cross-entropy between our discrete distribution and the true target. as our prediction loss. Once we sampled K trajectories per actor, this loss can be regarded as a standard classification loss over K classes (one for each trajectory sample). The target class is set to be the closest trajectory sample to the ground-truth future trajectory (in 2 distance).\n\nPlanning Loss: We expect our model to assign lower planning costs to better trajectories (e.g., collision free, towards the goal), and higher costs to bad ones. However, we do not have direct supervision over the cost. Instead, we utilize a max-margin loss, using expert behavior as positive examples and randomly sampled trajectories as negative ones. We set large margins for dangerous behaviors such as trajectories with collisions. This allows our model to penalize dangerous behaviors more severely. More formally, our planning loss is defined as\nL planning = data max k C \u03c4 \u03c4 \u03c4 gt |X \u2212 C \u03c4 \u03c4 \u03c4 k |X + d k + \u03b3 k + ,\nwhere [\u00b7] + is a ReLU function, \u03c4 \u03c4 \u03c4 gt is the expert trajectory and\u03c4 \u03c4 \u03c4 k is the k-th trajectory sample. We also define d k as the 2 distance between\u03c4 \u03c4 \u03c4 k and \u03c4 \u03c4 \u03c4 gt , and \u03b3 k is a constant positive penalty if\u03c4 \u03c4 \u03c4 k behave dangerously, e.g., \u03b3 collision if \u03c4 \u03c4 \u03c4 k collides with another actor and 0 otherwise.\nnuScenes L2 (m) Col (\u2030) ATG4D L2 (m) Col (\u2030) Method\n1s 2s 3s 1s 2s 3s Method 1s 2s 3s 1s 2s 3s Social-LSTM [1] 0.71 -1.85 0. 8 -9.6 FaF [30] 0.60 1.11 1.82 ---CSP [14] 0   \n\n\nExperimental Evaluation\n\nWe evaluate our model on all three tasks: detection, prediction, and planning. We show results on two large scale real-world self-driving datasets: nuScenes [6] and our in-house dataset ATG4D, as well as the CARLA simulated dataset [15,42]. We show that 1) our prediction module largely outperforms the stateof-the-art on public benchmarks and we demonstrate the benefits of explicitly modeling the interactions between actors. 2) Our planning module achieves the safest planning results and largely decreases the collision and lane violation rate, compared to competing methods. 3) Although sharing a single backbone to speedup inference, our model does not sacrifice detection performance compared to the state-of-the-art. We provide datasets' details and implementation details in the supplementary material.\n\n\nMulti-modal Interactive Prediction\n\nBaselines: On CARLA, we compare with the state-of-the-art reproduced and reported from [11,42,49]. On nuScenes 6 , we compare our method against several powerful multi-agent prediction approaches reproduced and reported from [7] 7 : Social-LSTM [1], Convolutional Social Pooling (CSP) [14] and CAR-Net [45]. On ATG4D, we compare with LiDAR-based joint detection and prediction models, including FaF [30] and IntentNet [10]. We also compare with NMP [57] on both datasets.  Table 2: Motion planning performance on ATG4D. All metrics are computed in a cumulative manner across time, lower the better.\n\nMetrics: Following previous works [1,18,25,30,57], we report L2 Error between our prediction (most likely) and the ground-truth at different future timestamps. We also report Collision Rate, defined as the percentage of actors that will collide with others if they follow the predictions. We argue that a socially consistent prediction model should achieve low collision rate, as avoiding collision is always one of the highest priorities for a human driver. On CARLA, we follow [42] and use minMSD as our metric, which is the minimal mean squared distance between the top 12 predictions and the ground-truth.\n\nQuantitative Results: As shown in Table 1a, our method achieves the best results on both datasets. This is impressive as most baselines use 2 as training objective, and thus are directly favored by the 2 error metric, while our approaches uses cross-entropy loss to learn proper distributions and capture multi-modality. Note that multimodal techniques are thought to score worst in this metric (see e.g., [11]). Here, we show that it is possible to model multimodality while achieving lower 2 error, as the model can better understand actors' behavior. Our approach also significantly reduces the collisions between the actors' predicted trajectories, which justifies the benefit of our multi-agent interaction modeling. We further evaluate our prediction performance when assuming ground-truth perception / history are known, instead of predicting using noisy detections from the model. We conduct this evaluation on CARLA where all previous methods use this settings. As shown in Table 1b, our method again significantly outperforms previous best results.\n\n\nMotion Planning\n\nBaselines: We implement multiple baselines for comparison, including both neural-based and classical planners: Ego-motion takes past 1 second positions of the ego-car and use a 4-layer MLP to predict the future locations, as the ego-motion usually providse strong cues of how the SDV will move in the future. Imitation Learning (IL) uses the same backbone network as our model but directly regresses an output trajectory for the SDV. We train such a regression model with 2 loss w.r.t. the ground-truth planning trajectory. Manual Cost  Table 3: Detection performance: higher is better. Note that although our method uses single backbone for multiple challenging tasks, our detection module can achieve on-par performance with the state-of-the-art.\n\nis a classical sampling based motion planner based on a manually designed cost function encoding collision avoidance and route following. The planned trajectory is chosen by finding the trajectory sample with minimal cost. We also include previously published learnable motion planning methods: Learnable-PLT [44] and Neural Motion Planner (NMP) [57]. These two method utilize a similar max-margin planning loss as ours. However, Learnable-PLT only consider the most probable future prediction, while NMP assumes planning is independent of prediction given the features.\n\nMetrics: We exploit three metrics to evaluate motion planning performance. Collision Rate and Lane Violation rate are the ratios of frames at which our planned trajectory either collides with other actors' ground-truth future behaviors, or touches / crosses a lane boundary, up to a specific future timestamp. Those are important safety metrics (lower is better). L2 to expert path is the average 2 distance between the planning trajectory and the expert driving path. Note that the expert driving path is just one among many possibilities, thus rendering this metric not perfect.\n\nQuantitative Results: The planning results are shown in Table 2. We can observe that: 1) our proposed method provides the safest plans, as we achieve much lower collision and lane violation rates compared to all other methods.\n\n2) Ego-motion and IL achieves the best 2 metric, as they employ the power of neural networks and directly optimize the 2 loss. However, they have high collision rate, which indicates directly mimicking expert demonstrations is still insufficient to learn a safety-aware self-driving stack. In contrast, by learning interpretable intermediate results (detection and prediction) and by incorporating prior knowledge (collision cost), our model can achieve much better results. The later point is further validated by comparing to NMP, which, despite learning detection and prediction, does not explicitly condition on them during planning.\n\n3) Manual-Cost and Learnable-PLT explicitly consider collision avoidance and traffic rules. However, unlike our approach, they only take the most likely motion forecast into consideration. Consequently, these methods have a higher collision rate than our approach.  \n\n\nObject Detection Results\n\nWe show our object detection results on nuScenes and ATG4D. Although we use a single backbone for all three challenging tasks, we show in Table 3 that our model can achieve similar or better results than state-of-the-art LiDAR detectors on both datasets. On nuScenes 8 , we follow the official benchmark and use detection average precision (AP) at different distance (in meters) thresholds as our metric. Here Megvii [62] is the leading method on the leaderboard at the time of our submission. We use a smaller resolution (Megvii has 1000 pixels on each side while ours is 500) for faster online inference, yet achieve on-par performance. We also conduct experiments on ATG4D. Since our model uses the same backbone as Pixor [55], which only focuses on detection, we demonstrate that a multi-task formulation does not sacrifice detection performance. Table 4a compares different prediction modules with the same backbone network.\n\n\nAblation Study and Qualitative Results\n\nWe can see that explicitly modeling a multi-modal future significantly boosts the prediction performance in terms of both collision rate and 2 error, comparing to a deterministic (unimodal) prediction. The performance is further boosted if the prediction module explicitly models the future interaction between multiple actors, particularly in collision rate. Table 4 compares motion planners that consider different prediction results. We can see that explicitly incorporating future prediction, even only the most likely prediction, will boost the motion planning performance, especially the collision rate. Furthermore, if the motion planner takes multi-modal futures into consideration, it achieves the best performance among the three. This further justifies our model design. We show qualitative results in Fig. 4, where we visualize our detections, predictions, motion planning trajectories, and the predicted uncertainties. We use different colors for different future timestamps to visualize high-probability actors' future positions estimated from our prediction module. Thus larger 'rainbow' areas mean more uncertain. On the first row, we can see the predictions are certain when vehicles drive along the lanes (left), while we see multi-modal predictions when vehicles approach an intersection (middle, right). On the second row, we can see our planning can nicely follow the lane (left), make a smooth left turn (middle), and take a nudge when an obstacle is blocking our path (right).\n\n\nConclusion\n\nIn this paper, we propose DSDNet, which is built on top of a single backbone network that takes as input raw sensor data and an HD map and performs perception, prediction, and planning under a unified framework. In particular, we build a deep energy based model to parameterize the joint distribution of future trajectories of all traffic participants. We resort to a sample based formulation, which enables efficient inference of the marginal distribution via belief propagation. We design a structured planning cost which encourages traffic-rule following and collision avoidance for the SDV. We show that our model outperforms the state-of-the-art on several challenging datasets. In the future, we plan to explore more types of structured energies and cost functions.\n\n\nSupplementary Materials\n\n\nA Datasets\n\nCARLA: This is a public available multi-agent trajectory prediction dataset, collected by [42] using CARLA simulator [15]. It contains over 60k training sequences, 7k testing sequences collected from Town01, and 17k testing sequences from Town02. Each sequence is composed of 2 seconds of history, and 4 seconds future information.\n\nnuScenes: It contains 1000 driving snippets of length 20 seconds each. LiDAR point clouds are collected at 20Hz, and labels 3D bounding boxes are provided at 2Hz. To augment the labels, we generate bounding boxes for non-labeled frames using linear interpolation from 2 consecutive labeled frames. Since nuScenes dataset currently does not provide routing information for motion planning, we only conduct detection and prediction studies. We follow the official data split and compare against other methods on the \"car\" class.\n\nATG4D: We collected a challenging self-driving datasets over multiple cities across North America. It contains \u223c 5,000 snippets collected from 1000 different trips, with a 64-beam LiDAR running at 10 Hz and HD maps. We also labeled the data at 10 Hz, with maximum labeling range of 100 meters. We ignore parking areas far from the roads, as they will not interact with the SDV. We split out 500 snippets for testing and evaluate the full autonomy stack including motion planning, prediction as well as detection performance.\n\n\nB Network Architecture Details\n\nIn the following, we first describe our backbone network, the detection header, as well as the header for computing prediction (E traj ) and planning C traj . Note we use the same architecture for nuScenes and ATG4D, but a slightly different one for CARLA as the setting there is different, which we will explain in section D.\n\nBackbone: Our backbone is adapted from the detection network of [55,57], which has 5 blocks of layers in total. There are {2, 2, 3, 6, 5} Conv2D layers with {32, 64, 128, 256, 256} number of filters in those 5 blocks respectively. All Conv2D kernels are 3x3 and have stride 1. For the first three blocks, we use a max-pooling layer after each block to downsample the feature map by 2. After the 4-th block, we construct a multi-scale feature map by resizing the feature maps after each block to be of the same size (4 times smaller than the input) and then concatenate them together. This multi-scale feature map is then fed to the 5-th block. The final feature map computed by the 5-th block has a downsample rate of 4, and is shared for detection, prediction, and motion planning modules.\n\n\nC Trajectory Sampler Details\n\nFollowing NMP [57], we assume a bicycle dynamic model for vehicles, and we use a combination of straight line, circle arcs, and clothoid curves to sample possible trajectories. More specifically, to sample a trajectory for a given detected actor, we first estimate its initial position and speed as well as heading angle from our detection output. We then sample the mode of this trajectory, i.e., straight, circle, clothoid proportional to (0.3, 0.2, 0.5) probability. Next, we uniformly sample values of control parameters for the chosen mode, e.g., radius for circle mode, radius and canonical heading angle of clothoid. These sampled parameters determine the shape of this trajectory. We then sample an acceleration value, and compute the velocity values for the next 3 seconds based on this acceleration and the initial speed. Finally, we go along our sampled trajectory with our sampled velocity, to determine the waypoints along this trajectory for the next 3 seconds.\n\nIn our experiments, we notice that different numbers of samples used for inference will affect the final performance. For a metric only cares about precision, e.g., L2, which we used on nuScenes and ATG4D, more samples generally produces better performance. For instance, increasing number of trajectory samples (for inference) from 100 to 200 will decrease final timestep L2 error from 1.29m to 1.22m on ATG4D, but further increase number of samples only brings marginal improvements. Due to the consideration of memory and speed, we use 200 trajectory samples during inference and 100 trajectory samples for training. However, for a metric that considers both diversity and precision, e.g., minMSD (CARLA), there is a sweet point of number of samples. On CARLA, we found that sample 100 trajectories during inference performs worse than sampling 50 samples, which corresponds to 0.24 and 0.18 minMSD on validation set respectively, and sampling 1000 trajectories performs the worst, producing a minMSD of 0.30. This is because when presented with a set of dense samples, trajectories are spatially very close to each other. As a result, the highest-scored samples and its nearby samples will have very similar scores, and thus selected by the top-K evaluation procedure, which loses some diversity. We also found on CARLA, it's helpful to regress a future trajectory from the backbone and add it to the trajectory samples set before our prediction module, in order to augment the sampled set and alleviate any potential gaps between our dynamic model assumption and the dynamic model used in CARLA.  3,5] meters centered at the ego car. We aggregate the current LiDAR sweep with past 9 sweeps (0.5s period), and voxelize the space with 0.2 \u00d7 0.2 \u00d7 0.25 meter per voxel resolution. This gives us a 496 \u00d7 496 \u00d7 320 input LiDAR tensor. We further rasterize the map information with the same resolution. The map information includes road mask, lane boundary and road boundary, which provide critical information for predicting the behavior of a vehicle. We train our model for the car class, using Adam optimizer with initial learning rate of 0.001. We decay the learning rate by 10 at the 6-th and 7-th epoch respectively, and stop training at 8-th epoch. The training batch size is 80 and we use 16 GPUs in total. For detection, we treat anchors larger than 0.7 IoU with labels as positive examples, and smaller than 0.5 IoU as negative, and treat others as ignore. We adopt hard-mining to get good detection performance. For training the prediction, we treat a detection as positive if it has larger than 0.1 IoU with labels, and only apply prediction loss on those positive examples. Besides, we apply data augmentation [62] during training: randomly translating a frame (-1 to 1 meters in XY and -0.2 to 0.2 for Z), random rotating along the Z axis (-45 degree to 45 degree), randomly scaling the whole frame (0.95 to 1.05), and randomly flipping over the X and Y axes.\n\n\nD Implementation Details\n\n\nCARLA-PRECOG:\n\nWe train the model with Adam optimizer, using a learning rate of 0.0001, and decay by 10 at 20 epochs and 30 epochs, and finish training at 40 epochs. We use batch size of 160 on 4 GPUs. No other data augmentation is applied on this dataset.\n\nSince the dataset and experiment setup is different from nuScenes (Carla has no map and only 4 height channels for LiDAR; the prediction task is also provided with ground-truth actors' locations), we use a slightly different architecture. We first rasterize the LiDAR and the past positions of all actors similar to PRECOG [42], and feed them into a shallower backbone network with 5 blocks of Conv2D layers, each has {2,2,2,2,2} layers respectively. We then compute the feature of each actor by concatenating the actor features extracted from the backbone (ROIAlign followed by a number of convolutional layers) and a motion feature. Such a motion feature is computed by applying several Conv1D layers on top of the past locations of an actor, which is a T \u00d7 3 tensor. Those 3 dimensions are x, y coordinates and timestamp indexes. Finally, we compute the E traj and prediction results using the same architecture we have described in Section. B.\n\n\nE More Qualitative Results\n\nAdditionally we provide more qualitative results showing our inference results on ATG4D. In Figure. 5 we show the inputs and outputs of our model, and in Figure. 6 we explain how we visualize the prediction uncertainty estimated by our model. We can clearly see that our approach produces multimodal estimates when an actor is approaching an intersection. We provide more cases showing prediction results in Figure. 7. Finally, we show our motion planner can handle various situations including lane following, turning and nudging around other vehicles to avoid collision in Figure. 8. Our detections and predictions are shown in cyan, and the ego-car as well as motion planning results are shown in red.   6: We visualize the prediction uncertainty estimated by our model. We highlight the high-probability regions that actors might go to at different future timestamps using different colors, and overlay them together to have a better visualization. From left to right: high-probability region at 1 to 3 seconds into the future. We can observe clear multi-modality for the actor near an intersection (going straight / turning left / turning right). Fig. 7: More Prediction Visualizations. We show our predicted uncertainty: 1) aligns with lanes when an actor is driving on a straight road (meaning that we are certain about future direction but not certain about future speed in this case). 2) shows multi-modality when an actor approaches an intersection (either turning or going straight). \n\n\nfrom raw sensor data: 2 and Col (collision rate), lower the better.\n\nFig. 4 :\n4Qualitative results on ATG4D. Our prediction module can capture multimodalities when vehicles approach intersections, while being certain and accurate when vehicles drive along a single lane (top). Our model can produce smooth trajectories which follow the lane and are compliant with other actors (bottom). Cyan boxes: detection. Cyan trajectory: prediction. Red box: ego-car. Red trajectory: our planning. We overlay the predicted marginal distribution for different timestamps with different colors and only show high-probability regions.\n\nnuScenes\nWe follow the dataset range defined by the creators of nuScenes, and use an input region of [\u221249.6, 49.6] \u00d7 [\u221249.6, 49.6] \u00d7 [\u2212\n\nFig. 5 :\n5Left: Input to our model. Middle: Detection outputs (shown in cyan). Right: Socially consistent prediction (shown in cyan) and safe motion planning (shown in red).\n\nFig.\nFig. 6: We visualize the prediction uncertainty estimated by our model. We highlight the high-probability regions that actors might go to at different future timestamps using different colors, and overlay them together to have a better visualization. From left to right: high-probability region at 1 to 3 seconds into the future. We can observe clear multi-modality for the actor near an intersection (going straight / turning left / turning right).\n\nFig. 8 :\n8More Planning Visualizations: We show our motion planner can nicely handle lane following (row 1), turning (row 2 & 3) and nudging to avoid collision (row 4).\n\nTable 1 :\n1Prediction performance on nuScenes, ATG4D and CARLA\n\nTable 4 :\n4Ablation Study for prediction and planning modules\nWe would like the samples to cover the original continuous space and have high recall wrt the ground-truth future trajectories.\nAlthough the sum-product algorithm is only exact for tree structures, it is shown to work well in practice for graphs with cycles[38,51].\nWe find that using only L planning without the other two terms prevents the model from learning reasonable detection and prediction.\nNumbers are reported on official validation split, since there is no joint detection and prediction benchmark. 7[7] replaced the original encoder (taking the ground-truth detection and tracking as input) with a learned CNN that takes LiDAR as input for a fair comparison.\nWe conduct the comparison on the official validation split, as our model currently only focuses on vehicles while the testing benchmark is built for multi-class detection.\nAcknowledgementWe would like to thank Ming Liang, Kelvin Wong, Jerry Liu, Min Bai, Katie Luo and Shivam Duggal for their helpful comments on the paper.Detection Header: We use feature maps from the backbone and apply a single-shot detection header, similar to SSD[29], to predict the location, shape, orientation and velocity of each actor. More specifically, the detection header contains two Conv2D layers with 1 \u00d7 1 kernel, one for classification and the other one for regression. We apply the two Conv2D on the backbone feature map separately. To reduce the variance of regression outputs, we follow SSD[29]and use a set of predefined anchor boxes: each pixel at the backbone feature map is associated with 12 anchors, with different sizes and aspect ratios. We predict a classification score p k i,j for each pixel (i, j) and anchor k on the feature map, which indicates how likely it is for an actor to be presented at this location. For the regression layer, the header outputs the offset values at each location. These offset values include position offset l x , l y , size s w , s h , heading angle a sin , a cos , and velocity v x , v y . Their corresponding ground-truth target values can be computed using the labeled bounding box, namely,y , where subscript l means label value, and a means anchor value. Finally, we combine these two outputs and apply an NMS operation to determine the bounding boxes for all actors and their initial speeds.Prediction (E traj ) / Planning (C traj ) Headers: In addition to the detection header, our model has two headers: one outputs prediction energy E traj , the other outputs motion planning costs C traj . Note that these two headers have the same architecture, but different learnable parameters. After computing the backbone feature map, we apply four Conv2D layers with 128 filters. This increases the model capacity to better handle multiple tasks. Then, to extract the actor features, we perform ROI align based on the actor's detection bounding box, which output a 16 \u00d7 16 \u00d7 128 feature tensor for this actor. We then apply another four Conv2D layers, each with a downsample rate of 2 and filter size {256, 512, 512, 512} respectively. This gives us a 512 dimensional feature vector for each actor. Note that we parameterized trajectories with 7 waypoint (2Hz for 3 second, including the inital waypoint at 0 second). To extract trajectory features, we first index the feature on our header feature map at those waypoints with bilinear interpolation. This gives us a 7 \u00d7 128 feature. We then concatenate this feature with the (x t , y t , cos\u03b8 t , sin\u03b8 t , distance t ) of those 7 waypoints, where (x t , y t ) is the coordinate of that waypoint, (cos\u03b8 t , sin\u03b8 t ) is the direction, and distance t is the traveled distance along the trajectory up to that waypoint. Finally, we feed the actor and trajectory features to a 5 layer MLP to compute the E traj / C traj value for this trajectory. The MLP has (1024, 1024, 512, 256, 1) neurons for each layer.\nSocial lstm: Human trajectory prediction in crowded spaces. A Alahi, K Goel, V Ramanathan, A Robicquet, L Fei-Fei, S Savarese, CVPRAlahi, A., Goel, K., Ramanathan, V., Robicquet, A., Fei-Fei, L., Savarese, S.: Social lstm: Human trajectory prediction in crowded spaces. In: CVPR (2016)\n\nIntentionaware motion planning. T Bandyopadhyay, K S Won, E Frazzoli, D Hsu, W S Lee, D Rus, Algorithmic foundations of robotics X. Bandyopadhyay, T., Won, K.S., Frazzoli, E., Hsu, D., Lee, W.S., Rus, D.: Intention- aware motion planning. In: Algorithmic foundations of robotics X (2013)\n\nStructured prediction energy networks. D Belanger, A Mccallum, ICML. Belanger, D., McCallum, A.: Structured prediction energy networks. In: ICML (2016)\n\n. M Bojarski, D Del Testa, D Dworakowski, B Firner, B Flepp, P Goyal, L D Jackel, M Monfort, U Muller, J Zhang, End to end learning for self-driving cars. arXivBojarski, M., Del Testa, D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., Jackel, L.D., Monfort, M., Muller, U., Zhang, J., et al.: End to end learning for self-driving cars. arXiv (2016)\n\nThe DARPA urban challenge: autonomous vehicles in city traffic. M Buehler, K Iagnemma, S Singh, Buehler, M., Iagnemma, K., Singh, S.: The DARPA urban challenge: autonomous vehicles in city traffic (2009)\n\nH Caesar, V Bankiti, A H Lang, S Vora, V E Liong, Q Xu, A Krishnan, Y Pan, G Baldan, O Beijbom, nuscenes: A multimodal dataset for autonomous driving. arXiv. Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan, A., Pan, Y., Baldan, G., Beijbom, O.: nuscenes: A multimodal dataset for autonomous driving. arXiv (2019)\n\nS Casas, C Gulino, R Liao, R Urtasun, Spatially-aware graph neural networks for relational behavior forecasting from sensor data. arXiv. Casas, S., Gulino, C., Liao, R., Urtasun, R.: Spatially-aware graph neural networks for relational behavior forecasting from sensor data. arXiv (2019)\n\nImplicit latent variable model for scene-consistent motion forecasting. S Casas, C Gulino, S Suo, K Luo, R Liao, R Urtasun, ECCVCasas, S., Gulino, C., Suo, S., Luo, K., Liao, R., Urtasun, R.: Implicit latent variable model for scene-consistent motion forecasting. In: ECCV (2020)\n\nThe importance of prior knowledge in precise multimodal prediction. S Casas, C Gulino, S Suo, R Urtasun, IROSCasas, S., Gulino, C., Suo, S., Urtasun, R.: The importance of prior knowledge in precise multimodal prediction. In: IROS (2020)\n\nIntentnet: Learning to predict intention from raw sensor data. S Casas, W Luo, R Urtasun, Proceedings of The 2nd Conference on Robot Learning. The 2nd Conference on Robot LearningCasas, S., Luo, W., Urtasun, R.: Intentnet: Learning to predict intention from raw sensor data. In: Proceedings of The 2nd Conference on Robot Learning (2018)\n\nY Chai, B Sapp, M Bansal, D Anguelov, Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction. arXiv. Chai, Y., Sapp, B., Bansal, M., Anguelov, D.: Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction. arXiv (2019)\n\nLearning deep structured models. L C Chen, A Schwing, A Yuille, R Urtasun, ICMLChen, L.C., Schwing, A., Yuille, A., Urtasun, R.: Learning deep structured models. In: ICML (2015)\n\nEnd-to-end driving via conditional imitation learning. F Codevilla, M Miiller, A L\u00f3pez, V Koltun, A Dosovitskiy, ICRACodevilla, F., Miiller, M., L\u00f3pez, A., Koltun, V., Dosovitskiy, A.: End-to-end driv- ing via conditional imitation learning. In: ICRA (2018)\n\nConvolutional social pooling for vehicle trajectory prediction. N Deo, M M Trivedi, CVPRDeo, N., Trivedi, M.M.: Convolutional social pooling for vehicle trajectory predic- tion. In: CVPR (2018)\n\nA Dosovitskiy, G Ros, F Codevilla, A Lopez, V Koltun, Carla: An open urban driving simulator. Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., Koltun, V.: Carla: An open urban driving simulator. arXiv (2017)\n\nH Fan, F Zhu, C Liu, L Zhang, L Zhuang, D Li, W Zhu, J Hu, H Li, Q Kong, Baidu apollo em motion planner. arXiv. Fan, H., Zhu, F., Liu, C., Zhang, L., Zhuang, L., Li, D., Zhu, W., Hu, J., Li, H., Kong, Q.: Baidu apollo em motion planner. arXiv (2018)\n\nDeep structured prediction with nonlinear output transformations. C Graber, O Meshi, A Schwing, NeurIPSGraber, C., Meshi, O., Schwing, A.: Deep structured prediction with nonlinear output transformations. In: NeurIPS (2018)\n\nSocial gan: Socially acceptable trajectories with generative adversarial networks. A Gupta, J Johnson, L Fei-Fei, S Savarese, A Alahi, CVPRGupta, A., Johnson, J., Fei-Fei, L., Savarese, S., Alahi, A.: Social gan: Socially acceptable trajectories with generative adversarial networks. In: CVPR (2018)\n\nContingency planning over probabilistic obstacle predictions for autonomous road vehicles. J Hardy, M Campbell, IEEE Transactions on Robotics. Hardy, J., Campbell, M.: Contingency planning over probabilistic obstacle predic- tions for autonomous road vehicles. IEEE Transactions on Robotics (2013)\n\nRules of the road: Predicting driving behavior with a convolutional model of semantic interactions. J Hong, B Sapp, J Philbin, CVPRHong, J., Sapp, B., Philbin, J.: Rules of the road: Predicting driving behavior with a convolutional model of semantic interactions. In: CVPR (2019)\n\nParticle belief propagation. A Ihler, D Mcallester, Artificial Intelligence and Statistics. Ihler, A., McAllester, D.: Particle belief propagation. In: Artificial Intelligence and Statistics (2009)\n\nA Jain, S Casas, R Liao, Y Xiong, S Feng, S Segal, R Urtasun, Discrete residual flow for probabilistic pedestrian behavior prediction. arXiv. Jain, A., Casas, S., Liao, R., Xiong, Y., Feng, S., Segal, S., Urtasun, R.: Discrete residual flow for probabilistic pedestrian behavior prediction. arXiv (2019)\n\nD P Kingma, M Welling, Auto-encoding variational bayes. arXiv. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv (2013)\n\nPointpillars: Fast encoders for object detection from point clouds. A H Lang, S Vora, H Caesar, L Zhou, J Yang, O Beijbom, CVPRLang, A.H., Vora, S., Caesar, H., Zhou, L., Yang, J., Beijbom, O.: Pointpillars: Fast encoders for object detection from point clouds. In: CVPR (2019)\n\nDesire: Distant future prediction in dynamic scenes with interacting agents. N Lee, W Choi, P Vernaza, C B Choy, P H Torr, M Chandraker, CVPRLee, N., Choi, W., Vernaza, P., Choy, C.B., Torr, P.H., Chandraker, M.: Desire: Distant future prediction in dynamic scenes with interacting agents. In: CVPR (2017)\n\nEnd-toend contextual perception and prediction with interaction transformer. L Li, B Yang, M Liang, W Zeng, M Ren, S Segal, R Urtasun, IROSLi, L., Yang, B., Liang, M., Zeng, W., Ren, M., Segal, S., Urtasun, R.: End-to- end contextual perception and prediction with interaction transformer. In: IROS (2020)\n\nLearning lane graph representations for motion forecasting. M Liang, B Yang, R Hu, Y Chen, R Liao, S Feng, R Urtasun, ECCVLiang, M., Yang, B., Hu, R., Chen, Y., Liao, R., Feng, S., Urtasun, R.: Learning lane graph representations for motion forecasting. In: ECCV (2020)\n\nPnpnet: End-to-end perception and prediction with tracking in the loop. M Liang, B Yang, W Zeng, Y Chen, R Hu, S Casas, R Urtasun, CVPRLiang, M., Yang, B., Zeng, W., Chen, Y., Hu, R., Casas, S., Urtasun, R.: Pnpnet: End-to-end perception and prediction with tracking in the loop. In: CVPR (2020)\n\nSsd: Single shot multibox detector. W Liu, D Anguelov, D Erhan, C Szegedy, S Reed, C Y Fu, A C Berg, ECCVLiu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.: Ssd: Single shot multibox detector. In: ECCV (2016)\n\nFast and furious: Real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net. W Luo, B Yang, R Urtasun, Luo, W., Yang, B., Urtasun, R.: Fast and furious: Real time end-to-end 3d detec- tion, tracking and motion forecasting with a single convolutional net\n\nForecasting interactive dynamics of pedestrians with fictitious play. W C Ma, D A Huang, N Lee, K M Kitani, CVPR. Ma, W.C., Huang, D.A., Lee, N., Kitani, K.M.: Forecasting interactive dynamics of pedestrians with fictitious play. In: CVPR. pp. 774-782 (2017)\n\nDeep rigid instance scene flow. W C Ma, S Wang, R Hu, Y Xiong, R Urtasun, CVPR. Ma, W.C., Wang, S., Hu, R., Xiong, Y., Urtasun, R.: Deep rigid instance scene flow. In: CVPR. pp. 3614-3622 (2019)\n\nLidarsim: Realistic lidar simulation by leveraging the real world. S Manivasagam, S Wang, K Wong, W Zeng, M Sazanovich, S Tan, B Yang, W C Ma, R Urtasun, CVPRManivasagam, S., Wang, S., Wong, K., Zeng, W., Sazanovich, M., Tan, S., Yang, B., Ma, W.C., Urtasun, R.: Lidarsim: Realistic lidar simulation by leveraging the real world. In: CVPR (2020)\n\nLearning deep structured active contours end-to-end. D Marcos, D Tuia, B Kellenberger, L Zhang, M Bai, R Liao, R Urtasun, CVPRMarcos, D., Tuia, D., Kellenberger, B., Zhang, L., Bai, M., Liao, R., Urtasun, R.: Learning deep structured active contours end-to-end. In: CVPR (2018)\n\nMulti-view reprojection architecture for orientation estimation. Min Choi, H Kang, H Hyun, Y , ICCVMin Choi, H., Kang, H., Hyun, Y.: Multi-view reprojection architecture for orien- tation estimation. In: ICCV (2019)\n\nJunior: The stanford entry in the urban challenge. M Montemerlo, J Becker, S Bhat, H Dahlkamp, D Dolgov, S Ettinger, D Haehnel, T Hilden, G Hoffmann, B Huhnke, Journal of field Robotics. Montemerlo, M., Becker, J., Bhat, S., Dahlkamp, H., Dolgov, D., Ettinger, S., Haehnel, D., Hilden, T., Hoffmann, G., Huhnke, B., et al.: Junior: The stanford entry in the urban challenge. Journal of field Robotics (2008)\n\nM M\u00fcller, A Dosovitskiy, B Ghanem, V Koltun, Driving policy transfer via modularity and abstraction. arXiv. M\u00fcller, M., Dosovitskiy, A., Ghanem, B., Koltun, V.: Driving policy transfer via modularity and abstraction. arXiv (2018)\n\nLoopy belief propagation for approximate inference: An empirical study. K P Murphy, Y Weiss, M I Jordan, Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence. the Fifteenth conference on Uncertainty in artificial intelligenceMurphy, K.P., Weiss, Y., Jordan, M.I.: Loopy belief propagation for approximate inference: An empirical study. In: Proceedings of the Fifteenth conference on Un- certainty in artificial intelligence (1999)\n\nCovernet: Multimodal behavior prediction using trajectory sets. T Phan-Minh, E C Grigore, F A Boulton, O Beijbom, E M Wolff, CVPRPhan-Minh, T., Grigore, E.C., Boulton, F.A., Beijbom, O., Wolff, E.M.: Covernet: Multimodal behavior prediction using trajectory sets. In: CVPR (2020)\n\nAlvinn: An autonomous land vehicle in a neural network. D A Pomerleau, NeurIPSPomerleau, D.A.: Alvinn: An autonomous land vehicle in a neural network. In: NeurIPS (1989)\n\nR2p2: A reparameterized pushforward policy for diverse, precise generative path forecasting. N Rhinehart, K M Kitani, P Vernaza, ECCVRhinehart, N., Kitani, K.M., Vernaza, P.: R2p2: A reparameterized pushforward policy for diverse, precise generative path forecasting. In: ECCV (2018)\n\nN Rhinehart, R Mcallister, K Kitani, S Levine, Precog: Prediction conditioned on goals in visual multi-agent settings. arXiv. Rhinehart, N., McAllister, R., Kitani, K., Levine, S.: Precog: Prediction condi- tioned on goals in visual multi-agent settings. arXiv (2019)\n\nPerceive, predict, and plan: Safe motion planning through interpretable semantic representations. A Sadat, S Casas, M Ren, X Wu, P Dhawan, R Urtasun, ECCVSadat, A., Casas, S., Ren, M., Wu, X., Dhawan, P., Urtasun, R.: Perceive, predict, and plan: Safe motion planning through interpretable semantic representations. In: ECCV (2020)\n\nA Sadat, M Ren, A Pokrovsky, Y C Lin, E Yumer, R Urtasun, Jointly learnable behavior and trajectory planning for self-driving vehicles. arXiv. Sadat, A., Ren, M., Pokrovsky, A., Lin, Y.C., Yumer, E., Urtasun, R.: Jointly learnable behavior and trajectory planning for self-driving vehicles. arXiv (2019)\n\nCar-net: Clairvoyant attentive recurrent network. A Sadeghian, F Legros, M Voisin, R Vesel, A Alahi, S Savarese, ECCVSadeghian, A., Legros, F., Voisin, M., Vesel, R., Alahi, A., Savarese, S.: Car-net: Clairvoyant attentive recurrent network. In: ECCV (2018)\n\nA G Schwing, R Urtasun, Fully connected deep structured networks. arXiv. Schwing, A.G., Urtasun, R.: Fully connected deep structured networks. arXiv (2015)\n\nLearning structured output representation using deep conditional generative models. K Sohn, H Lee, X Yan, NeurIPSSohn, K., Lee, H., Yan, X.: Learning structured output representation using deep conditional generative models. In: NeurIPS (2015)\n\nNonparametric belief propagation. E B Sudderth, A T Ihler, M Isard, W T Freeman, A S Willsky, Communications of the ACM. Sudderth, E.B., Ihler, A.T., Isard, M., Freeman, W.T., Willsky, A.S.: Nonpara- metric belief propagation. Communications of the ACM (2010)\n\nY C Tang, R Salakhutdinov, Multiple futures prediction. arXiv. Tang, Y.C., Salakhutdinov, R.: Multiple futures prediction. arXiv (2019)\n\nV2vnet: Vehicle-to-vehicle communication for joint perception and prediction. T H Wang, S Manivasagam, M Liang, B Yang, W Zeng, U Raquel, ECCVWang, T.H., Manivasagam, S., Liang, M., Yang, B., Zeng, W., Raquel, U.: V2vnet: Vehicle-to-vehicle communication for joint perception and prediction. In: ECCV (2020)\n\nBelief propagation: technical perspective. Y Weiss, J Pearl, Communications of the ACM. Weiss, Y., Pearl, J.: Belief propagation: technical perspective. Communications of the ACM (2010)\n\nM Wulfmeier, P Ondruska, I Posner, Maximum entropy deep inverse reinforcement learning. arXiv. Wulfmeier, M., Ondruska, P., Posner, I.: Maximum entropy deep inverse reinforce- ment learning. arXiv (2015)\n\nContinuous markov random fields for robust stereo estimation. K Yamaguchi, T Hazan, D Mcallester, R Urtasun, ECCVYamaguchi, K., Hazan, T., McAllester, D., Urtasun, R.: Continuous markov ran- dom fields for robust stereo estimation. In: ECCV (2012)\n\nEfficient joint segmentation, occlusion labeling, stereo and flow estimation. K Yamaguchi, D Mcallester, R Urtasun, ECCVYamaguchi, K., McAllester, D., Urtasun, R.: Efficient joint segmentation, occlusion labeling, stereo and flow estimation. In: ECCV (2014)\n\nPixor: Real-time 3d object detection from point clouds. B Yang, W Luo, R Urtasun, Yang, B., Luo, W., Urtasun, R.: Pixor: Real-time 3d object detection from point clouds\n\nUnderstanding belief propagation and its generalizations. Exploring artificial intelligence in the new millennium. J S Yedidia, W T Freeman, Y Weiss, Yedidia, J.S., Freeman, W.T., Weiss, Y.: Understanding belief propagation and its generalizations. Exploring artificial intelligence in the new millennium (2003)\n\nEnd-to-end interpretable neural motion planner. W Zeng, W Luo, S Suo, A Sadat, B Yang, S Casas, R Urtasun, CVPRZeng, W., Luo, W., Suo, S., Sadat, A., Yang, B., Casas, S., Urtasun, R.: End-to-end interpretable neural motion planner. In: CVPR (2019)\n\nDeep structured energy based models for anomaly detection. S Zhai, Y Cheng, W Lu, Z Zhang, ICMLZhai, S., Cheng, Y., Lu, W., Zhang, Z.: Deep structured energy based models for anomaly detection. In: ICML (2016)\n\nA non-conservatively defensive strategy for urban autonomous driving. W Zhan, C Liu, C Y Chan, M Tomizuka, Intelligent Transportation Systems (ITSC). IEEE 19th International Conference onZhan, W., Liu, C., Chan, C.Y., Tomizuka, M.: A non-conservatively defensive strat- egy for urban autonomous driving. In: Intelligent Transportation Systems (ITSC), 2016 IEEE 19th International Conference on (2016)\n\nMulti-agent tensor fusion for contextual trajectory prediction. T Zhao, Y Xu, M Monfort, W Choi, C Baker, Y Zhao, Y Wang, Y N Wu, CVPRZhao, T., Xu, Y., Monfort, M., Choi, W., Baker, C., Zhao, Y., Wang, Y., Wu, Y.N.: Multi-agent tensor fusion for contextual trajectory prediction. In: CVPR (2019)\n\nVoxelnet: End-to-end learning for point cloud based 3d object detection. Y Zhou, O Tuzel, CVPRZhou, Y., Tuzel, O.: Voxelnet: End-to-end learning for point cloud based 3d object detection. In: CVPR (2018)\n\nB Zhu, Z Jiang, X Zhou, Z Li, G Yu, Class-balanced grouping and sampling for point cloud 3d object detection. arXiv. Zhu, B., Jiang, Z., Zhou, X., Li, Z., Yu, G.: Class-balanced grouping and sampling for point cloud 3d object detection. arXiv (2019)\n\nMaximum entropy inverse reinforcement learning. B D Ziebart, A L Maas, J A Bagnell, A K Dey, AAAIZiebart, B.D., Maas, A.L., Bagnell, J.A., Dey, A.K.: Maximum entropy inverse reinforcement learning. In: AAAI (2008)\n\nTrajectory planning for berthaa local, continuous method. J Ziegler, P Bender, T Dang, C Stiller, Intelligent Vehicles Symposium Proceedings. IEEEZiegler, J., Bender, P., Dang, T., Stiller, C.: Trajectory planning for berthaa local, continuous method. In: Intelligent Vehicles Symposium Proceedings, 2014 IEEE (2014)\n", "annotations": {"author": "[{\"end\":113,\"start\":48},{\"end\":179,\"start\":114},{\"end\":243,\"start\":180},{\"end\":282,\"start\":244},{\"end\":327,\"start\":283},{\"end\":395,\"start\":328}]", "publisher": null, "author_last_name": "[{\"end\":60,\"start\":56},{\"end\":127,\"start\":123},{\"end\":191,\"start\":187},{\"end\":252,\"start\":248},{\"end\":291,\"start\":287},{\"end\":342,\"start\":335}]", "author_first_name": "[{\"end\":55,\"start\":48},{\"end\":122,\"start\":114},{\"end\":186,\"start\":180},{\"end\":247,\"start\":244},{\"end\":286,\"start\":283},{\"end\":334,\"start\":328}]", "author_affiliation": "[{\"end\":88,\"start\":79},{\"end\":112,\"start\":90},{\"end\":154,\"start\":145},{\"end\":178,\"start\":156},{\"end\":218,\"start\":209},{\"end\":242,\"start\":220},{\"end\":281,\"start\":272},{\"end\":302,\"start\":293},{\"end\":326,\"start\":304},{\"end\":370,\"start\":361},{\"end\":394,\"start\":372}]", "title": "[{\"end\":45,\"start\":1},{\"end\":440,\"start\":396}]", "venue": null, "abstract": "[{\"end\":1256,\"start\":492}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2965,\"start\":2961},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2968,\"start\":2965},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3165,\"start\":3161},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3168,\"start\":3165},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3170,\"start\":3168},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3277,\"start\":3273},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3280,\"start\":3277},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3283,\"start\":3280},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3296,\"start\":3292},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5031,\"start\":5028},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5093,\"start\":5089},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5096,\"start\":5093},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5389,\"start\":5386},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5392,\"start\":5389},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5395,\"start\":5392},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5398,\"start\":5395},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":5401,\"start\":5398},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":5404,\"start\":5401},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5406,\"start\":5404},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5409,\"start\":5406},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5412,\"start\":5409},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5581,\"start\":5577},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5830,\"start\":5826},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5833,\"start\":5830},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6440,\"start\":6436},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":6694,\"start\":6690},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6697,\"start\":6694},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6700,\"start\":6697},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6703,\"start\":6700},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6706,\"start\":6703},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6708,\"start\":6706},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6710,\"start\":6708},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7175,\"start\":7172},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7178,\"start\":7175},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7181,\"start\":7178},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":7184,\"start\":7181},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7272,\"start\":7268},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7275,\"start\":7272},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":7278,\"start\":7275},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":7281,\"start\":7278},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7415,\"start\":7412},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7418,\"start\":7415},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7421,\"start\":7418},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":7424,\"start\":7421},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7874,\"start\":7871},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7877,\"start\":7874},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7909,\"start\":7905},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7912,\"start\":7909},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7934,\"start\":7930},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7937,\"start\":7934},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22170,\"start\":22167},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22173,\"start\":22170},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22176,\"start\":22173},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":22179,\"start\":22176},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22182,\"start\":22179},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":22515,\"start\":22511},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":22539,\"start\":22535},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22566,\"start\":22562},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":22682,\"start\":22678},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":22685,\"start\":22682},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22688,\"start\":22685},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":22691,\"start\":22688},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":22694,\"start\":22691},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":22697,\"start\":22694},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25326,\"start\":25322},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":25329,\"start\":25326},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":25332,\"start\":25329},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":25335,\"start\":25332},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":25418,\"start\":25414},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":27894,\"start\":27890},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":39064,\"start\":39060},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":42045,\"start\":42042},{\"end\":45591,\"start\":45585},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":45600,\"start\":45596},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":45627,\"start\":45623},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":45820,\"start\":45817},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":45896,\"start\":45892},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":45899,\"start\":45896},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":46601,\"start\":46597},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":46604,\"start\":46601},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":46607,\"start\":46604},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":46758,\"start\":46755},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":46799,\"start\":46795},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":46816,\"start\":46812},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":46913,\"start\":46909},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":46932,\"start\":46928},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":47147,\"start\":47144},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":47150,\"start\":47147},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":47153,\"start\":47150},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":47156,\"start\":47153},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":47159,\"start\":47156},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":47593,\"start\":47589},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":48131,\"start\":48127},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":49862,\"start\":49858},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":49899,\"start\":49895},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":52286,\"start\":52282},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":52594,\"start\":52590},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":55257,\"start\":55253},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":55284,\"start\":55280},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":56979,\"start\":56975},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":56982,\"start\":56979},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":57752,\"start\":57748},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":60315,\"start\":60313},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":60317,\"start\":60315},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":61437,\"start\":61433},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":62298,\"start\":62294},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":66394,\"start\":66390},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":66397,\"start\":66394},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":66647,\"start\":66644}]", "figure": "[{\"attributes\":{\"id\":\"fig_7\"},\"end\":64514,\"start\":64445},{\"attributes\":{\"id\":\"fig_8\"},\"end\":65067,\"start\":64515},{\"attributes\":{\"id\":\"fig_9\"},\"end\":65204,\"start\":65068},{\"attributes\":{\"id\":\"fig_10\"},\"end\":65379,\"start\":65205},{\"attributes\":{\"id\":\"fig_11\"},\"end\":65835,\"start\":65380},{\"attributes\":{\"id\":\"fig_12\"},\"end\":66005,\"start\":65836},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":66069,\"start\":66006},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":66132,\"start\":66070}]", "paragraph": "[{\"end\":1693,\"start\":1272},{\"end\":2381,\"start\":1695},{\"end\":3678,\"start\":2383},{\"end\":3994,\"start\":3680},{\"end\":4458,\"start\":3996},{\"end\":4931,\"start\":4460},{\"end\":5207,\"start\":4933},{\"end\":6589,\"start\":5224},{\"end\":6940,\"start\":6591},{\"end\":8329,\"start\":6942},{\"end\":9387,\"start\":9250},{\"end\":10445,\"start\":10308},{\"end\":11971,\"start\":11366},{\"end\":23133,\"start\":22048},{\"end\":23868,\"start\":23174},{\"end\":24681,\"start\":23870},{\"end\":24984,\"start\":24683},{\"end\":26459,\"start\":25034},{\"end\":27003,\"start\":26506},{\"end\":28043,\"start\":27005},{\"end\":28366,\"start\":28045},{\"end\":29080,\"start\":28439},{\"end\":29273,\"start\":29191},{\"end\":29871,\"start\":29275},{\"end\":38532,\"start\":37759},{\"end\":39401,\"start\":38534},{\"end\":39998,\"start\":39480},{\"end\":40683,\"start\":40061},{\"end\":41120,\"start\":40731},{\"end\":41675,\"start\":41174},{\"end\":42838,\"start\":41677},{\"end\":43265,\"start\":42890},{\"end\":43399,\"start\":43278},{\"end\":43589,\"start\":43451},{\"end\":44048,\"start\":43591},{\"end\":44519,\"start\":44050},{\"end\":45072,\"start\":44521},{\"end\":45459,\"start\":45142},{\"end\":45632,\"start\":45512},{\"end\":46471,\"start\":45660},{\"end\":47108,\"start\":46510},{\"end\":47719,\"start\":47110},{\"end\":48779,\"start\":47721},{\"end\":49547,\"start\":48799},{\"end\":50119,\"start\":49549},{\"end\":50701,\"start\":50121},{\"end\":50929,\"start\":50703},{\"end\":51568,\"start\":50931},{\"end\":51836,\"start\":51570},{\"end\":52794,\"start\":51865},{\"end\":54336,\"start\":52837},{\"end\":55122,\"start\":54351},{\"end\":55494,\"start\":55163},{\"end\":56022,\"start\":55496},{\"end\":56548,\"start\":56024},{\"end\":56909,\"start\":56583},{\"end\":57701,\"start\":56911},{\"end\":58709,\"start\":57734},{\"end\":61683,\"start\":58711},{\"end\":61969,\"start\":61728},{\"end\":62918,\"start\":61971},{\"end\":64444,\"start\":62949}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9249,\"start\":8330},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10307,\"start\":9388},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11365,\"start\":10446},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13942,\"start\":11972},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14489,\"start\":13942},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15036,\"start\":14489},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15219,\"start\":15036},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15756,\"start\":15219},{\"attributes\":{\"id\":\"formula_8\"},\"end\":22047,\"start\":15756},{\"attributes\":{\"id\":\"formula_9\"},\"end\":28438,\"start\":28367},{\"attributes\":{\"id\":\"formula_10\"},\"end\":29190,\"start\":29081},{\"attributes\":{\"id\":\"formula_11\"},\"end\":39479,\"start\":39402},{\"attributes\":{\"id\":\"formula_12\"},\"end\":40060,\"start\":39999},{\"attributes\":{\"id\":\"formula_13\"},\"end\":41173,\"start\":41121},{\"attributes\":{\"id\":\"formula_14\"},\"end\":42889,\"start\":42839},{\"attributes\":{\"id\":\"formula_15\"},\"end\":43450,\"start\":43400},{\"attributes\":{\"id\":\"formula_16\"},\"end\":45141,\"start\":45073},{\"attributes\":{\"id\":\"formula_17\"},\"end\":45511,\"start\":45460}]", "table_ref": "[{\"end\":46990,\"start\":46983},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":47763,\"start\":47755},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":48712,\"start\":48704},{\"end\":49343,\"start\":49336},{\"end\":50766,\"start\":50759},{\"end\":52010,\"start\":52003},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":52724,\"start\":52716},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":53204,\"start\":53197}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1270,\"start\":1258},{\"attributes\":{\"n\":\"2\"},\"end\":5222,\"start\":5210},{\"attributes\":{\"n\":\"3\"},\"end\":23172,\"start\":23136},{\"attributes\":{\"n\":\"3.1\"},\"end\":25032,\"start\":24987},{\"attributes\":{\"n\":\"3.2\"},\"end\":26504,\"start\":26462},{\"end\":33713,\"start\":29874},{\"end\":37757,\"start\":33716},{\"attributes\":{\"n\":\"3.3\"},\"end\":40729,\"start\":40686},{\"attributes\":{\"n\":\"3.4\"},\"end\":43276,\"start\":43268},{\"attributes\":{\"n\":\"4\"},\"end\":45658,\"start\":45635},{\"attributes\":{\"n\":\"4.1\"},\"end\":46508,\"start\":46474},{\"attributes\":{\"n\":\"4.2\"},\"end\":48797,\"start\":48782},{\"attributes\":{\"n\":\"4.3\"},\"end\":51863,\"start\":51839},{\"attributes\":{\"n\":\"4.4\"},\"end\":52835,\"start\":52797},{\"attributes\":{\"n\":\"5\"},\"end\":54349,\"start\":54339},{\"end\":55148,\"start\":55125},{\"end\":55161,\"start\":55151},{\"end\":56581,\"start\":56551},{\"end\":57732,\"start\":57704},{\"end\":61710,\"start\":61686},{\"end\":61726,\"start\":61713},{\"end\":62947,\"start\":62921},{\"end\":64524,\"start\":64516},{\"end\":65077,\"start\":65069},{\"end\":65214,\"start\":65206},{\"end\":65385,\"start\":65381},{\"end\":65845,\"start\":65837},{\"end\":66016,\"start\":66007},{\"end\":66080,\"start\":66071}]", "table": null, "figure_caption": "[{\"end\":64514,\"start\":64447},{\"end\":65067,\"start\":64526},{\"end\":65204,\"start\":65078},{\"end\":65379,\"start\":65216},{\"end\":65835,\"start\":65386},{\"end\":66005,\"start\":65847},{\"end\":66069,\"start\":66018},{\"end\":66132,\"start\":66082}]", "figure_ref": "[{\"end\":4896,\"start\":4890},{\"end\":5944,\"start\":5938},{\"end\":11515,\"start\":11509},{\"end\":24103,\"start\":24097},{\"end\":29404,\"start\":29398},{\"end\":29821,\"start\":29815},{\"end\":38115,\"start\":38108},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":53656,\"start\":53650},{\"end\":63048,\"start\":63041},{\"end\":63110,\"start\":63103},{\"end\":63364,\"start\":63357},{\"end\":63531,\"start\":63524},{\"end\":63657,\"start\":63656},{\"end\":64107,\"start\":64101}]", "bib_author_first_name": "[{\"end\":70049,\"start\":70048},{\"end\":70058,\"start\":70057},{\"end\":70066,\"start\":70065},{\"end\":70080,\"start\":70079},{\"end\":70093,\"start\":70092},{\"end\":70104,\"start\":70103},{\"end\":70308,\"start\":70307},{\"end\":70325,\"start\":70324},{\"end\":70327,\"start\":70326},{\"end\":70334,\"start\":70333},{\"end\":70346,\"start\":70345},{\"end\":70353,\"start\":70352},{\"end\":70355,\"start\":70354},{\"end\":70362,\"start\":70361},{\"end\":70604,\"start\":70603},{\"end\":70616,\"start\":70615},{\"end\":70720,\"start\":70719},{\"end\":70732,\"start\":70731},{\"end\":70745,\"start\":70744},{\"end\":70760,\"start\":70759},{\"end\":70770,\"start\":70769},{\"end\":70779,\"start\":70778},{\"end\":70788,\"start\":70787},{\"end\":70790,\"start\":70789},{\"end\":70800,\"start\":70799},{\"end\":70811,\"start\":70810},{\"end\":70821,\"start\":70820},{\"end\":71137,\"start\":71136},{\"end\":71148,\"start\":71147},{\"end\":71160,\"start\":71159},{\"end\":71278,\"start\":71277},{\"end\":71288,\"start\":71287},{\"end\":71299,\"start\":71298},{\"end\":71301,\"start\":71300},{\"end\":71309,\"start\":71308},{\"end\":71317,\"start\":71316},{\"end\":71319,\"start\":71318},{\"end\":71328,\"start\":71327},{\"end\":71334,\"start\":71333},{\"end\":71346,\"start\":71345},{\"end\":71353,\"start\":71352},{\"end\":71363,\"start\":71362},{\"end\":71621,\"start\":71620},{\"end\":71630,\"start\":71629},{\"end\":71640,\"start\":71639},{\"end\":71648,\"start\":71647},{\"end\":71982,\"start\":71981},{\"end\":71991,\"start\":71990},{\"end\":72001,\"start\":72000},{\"end\":72008,\"start\":72007},{\"end\":72015,\"start\":72014},{\"end\":72023,\"start\":72022},{\"end\":72259,\"start\":72258},{\"end\":72268,\"start\":72267},{\"end\":72278,\"start\":72277},{\"end\":72285,\"start\":72284},{\"end\":72493,\"start\":72492},{\"end\":72502,\"start\":72501},{\"end\":72509,\"start\":72508},{\"end\":72769,\"start\":72768},{\"end\":72777,\"start\":72776},{\"end\":72785,\"start\":72784},{\"end\":72795,\"start\":72794},{\"end\":73083,\"start\":73082},{\"end\":73085,\"start\":73084},{\"end\":73093,\"start\":73092},{\"end\":73104,\"start\":73103},{\"end\":73114,\"start\":73113},{\"end\":73284,\"start\":73283},{\"end\":73297,\"start\":73296},{\"end\":73308,\"start\":73307},{\"end\":73317,\"start\":73316},{\"end\":73327,\"start\":73326},{\"end\":73552,\"start\":73551},{\"end\":73559,\"start\":73558},{\"end\":73561,\"start\":73560},{\"end\":73683,\"start\":73682},{\"end\":73698,\"start\":73697},{\"end\":73705,\"start\":73704},{\"end\":73718,\"start\":73717},{\"end\":73727,\"start\":73726},{\"end\":73895,\"start\":73894},{\"end\":73902,\"start\":73901},{\"end\":73909,\"start\":73908},{\"end\":73916,\"start\":73915},{\"end\":73925,\"start\":73924},{\"end\":73935,\"start\":73934},{\"end\":73941,\"start\":73940},{\"end\":73948,\"start\":73947},{\"end\":73954,\"start\":73953},{\"end\":73960,\"start\":73959},{\"end\":74212,\"start\":74211},{\"end\":74222,\"start\":74221},{\"end\":74231,\"start\":74230},{\"end\":74454,\"start\":74453},{\"end\":74463,\"start\":74462},{\"end\":74474,\"start\":74473},{\"end\":74485,\"start\":74484},{\"end\":74497,\"start\":74496},{\"end\":74763,\"start\":74762},{\"end\":74772,\"start\":74771},{\"end\":75071,\"start\":75070},{\"end\":75079,\"start\":75078},{\"end\":75087,\"start\":75086},{\"end\":75281,\"start\":75280},{\"end\":75290,\"start\":75289},{\"end\":75451,\"start\":75450},{\"end\":75459,\"start\":75458},{\"end\":75468,\"start\":75467},{\"end\":75476,\"start\":75475},{\"end\":75485,\"start\":75484},{\"end\":75493,\"start\":75492},{\"end\":75502,\"start\":75501},{\"end\":75756,\"start\":75755},{\"end\":75758,\"start\":75757},{\"end\":75768,\"start\":75767},{\"end\":75961,\"start\":75960},{\"end\":75963,\"start\":75962},{\"end\":75971,\"start\":75970},{\"end\":75979,\"start\":75978},{\"end\":75989,\"start\":75988},{\"end\":75997,\"start\":75996},{\"end\":76005,\"start\":76004},{\"end\":76249,\"start\":76248},{\"end\":76256,\"start\":76255},{\"end\":76264,\"start\":76263},{\"end\":76275,\"start\":76274},{\"end\":76277,\"start\":76276},{\"end\":76285,\"start\":76284},{\"end\":76287,\"start\":76286},{\"end\":76295,\"start\":76294},{\"end\":76556,\"start\":76555},{\"end\":76562,\"start\":76561},{\"end\":76570,\"start\":76569},{\"end\":76579,\"start\":76578},{\"end\":76587,\"start\":76586},{\"end\":76594,\"start\":76593},{\"end\":76603,\"start\":76602},{\"end\":76846,\"start\":76845},{\"end\":76855,\"start\":76854},{\"end\":76863,\"start\":76862},{\"end\":76869,\"start\":76868},{\"end\":76877,\"start\":76876},{\"end\":76885,\"start\":76884},{\"end\":76893,\"start\":76892},{\"end\":77129,\"start\":77128},{\"end\":77138,\"start\":77137},{\"end\":77146,\"start\":77145},{\"end\":77154,\"start\":77153},{\"end\":77162,\"start\":77161},{\"end\":77168,\"start\":77167},{\"end\":77177,\"start\":77176},{\"end\":77390,\"start\":77389},{\"end\":77397,\"start\":77396},{\"end\":77409,\"start\":77408},{\"end\":77418,\"start\":77417},{\"end\":77429,\"start\":77428},{\"end\":77437,\"start\":77436},{\"end\":77439,\"start\":77438},{\"end\":77445,\"start\":77444},{\"end\":77447,\"start\":77446},{\"end\":77709,\"start\":77708},{\"end\":77716,\"start\":77715},{\"end\":77724,\"start\":77723},{\"end\":77957,\"start\":77956},{\"end\":77959,\"start\":77958},{\"end\":77965,\"start\":77964},{\"end\":77967,\"start\":77966},{\"end\":77976,\"start\":77975},{\"end\":77983,\"start\":77982},{\"end\":77985,\"start\":77984},{\"end\":78179,\"start\":78178},{\"end\":78181,\"start\":78180},{\"end\":78187,\"start\":78186},{\"end\":78195,\"start\":78194},{\"end\":78201,\"start\":78200},{\"end\":78210,\"start\":78209},{\"end\":78410,\"start\":78409},{\"end\":78425,\"start\":78424},{\"end\":78433,\"start\":78432},{\"end\":78441,\"start\":78440},{\"end\":78449,\"start\":78448},{\"end\":78463,\"start\":78462},{\"end\":78470,\"start\":78469},{\"end\":78478,\"start\":78477},{\"end\":78480,\"start\":78479},{\"end\":78486,\"start\":78485},{\"end\":78743,\"start\":78742},{\"end\":78753,\"start\":78752},{\"end\":78761,\"start\":78760},{\"end\":78777,\"start\":78776},{\"end\":78786,\"start\":78785},{\"end\":78793,\"start\":78792},{\"end\":78801,\"start\":78800},{\"end\":79036,\"start\":79033},{\"end\":79044,\"start\":79043},{\"end\":79052,\"start\":79051},{\"end\":79060,\"start\":79059},{\"end\":79237,\"start\":79236},{\"end\":79251,\"start\":79250},{\"end\":79261,\"start\":79260},{\"end\":79269,\"start\":79268},{\"end\":79281,\"start\":79280},{\"end\":79291,\"start\":79290},{\"end\":79303,\"start\":79302},{\"end\":79314,\"start\":79313},{\"end\":79324,\"start\":79323},{\"end\":79336,\"start\":79335},{\"end\":79595,\"start\":79594},{\"end\":79605,\"start\":79604},{\"end\":79620,\"start\":79619},{\"end\":79630,\"start\":79629},{\"end\":79898,\"start\":79897},{\"end\":79900,\"start\":79899},{\"end\":79910,\"start\":79909},{\"end\":79919,\"start\":79918},{\"end\":79921,\"start\":79920},{\"end\":80351,\"start\":80350},{\"end\":80364,\"start\":80363},{\"end\":80366,\"start\":80365},{\"end\":80377,\"start\":80376},{\"end\":80379,\"start\":80378},{\"end\":80390,\"start\":80389},{\"end\":80401,\"start\":80400},{\"end\":80403,\"start\":80402},{\"end\":80624,\"start\":80623},{\"end\":80626,\"start\":80625},{\"end\":80832,\"start\":80831},{\"end\":80845,\"start\":80844},{\"end\":80847,\"start\":80846},{\"end\":80857,\"start\":80856},{\"end\":81024,\"start\":81023},{\"end\":81037,\"start\":81036},{\"end\":81051,\"start\":81050},{\"end\":81061,\"start\":81060},{\"end\":81391,\"start\":81390},{\"end\":81400,\"start\":81399},{\"end\":81409,\"start\":81408},{\"end\":81416,\"start\":81415},{\"end\":81422,\"start\":81421},{\"end\":81432,\"start\":81431},{\"end\":81626,\"start\":81625},{\"end\":81635,\"start\":81634},{\"end\":81642,\"start\":81641},{\"end\":81655,\"start\":81654},{\"end\":81657,\"start\":81656},{\"end\":81664,\"start\":81663},{\"end\":81673,\"start\":81672},{\"end\":81981,\"start\":81980},{\"end\":81994,\"start\":81993},{\"end\":82004,\"start\":82003},{\"end\":82014,\"start\":82013},{\"end\":82023,\"start\":82022},{\"end\":82032,\"start\":82031},{\"end\":82190,\"start\":82189},{\"end\":82192,\"start\":82191},{\"end\":82203,\"start\":82202},{\"end\":82431,\"start\":82430},{\"end\":82439,\"start\":82438},{\"end\":82446,\"start\":82445},{\"end\":82626,\"start\":82625},{\"end\":82628,\"start\":82627},{\"end\":82640,\"start\":82639},{\"end\":82642,\"start\":82641},{\"end\":82651,\"start\":82650},{\"end\":82660,\"start\":82659},{\"end\":82662,\"start\":82661},{\"end\":82673,\"start\":82672},{\"end\":82675,\"start\":82674},{\"end\":82853,\"start\":82852},{\"end\":82855,\"start\":82854},{\"end\":82863,\"start\":82862},{\"end\":83068,\"start\":83067},{\"end\":83070,\"start\":83069},{\"end\":83078,\"start\":83077},{\"end\":83093,\"start\":83092},{\"end\":83102,\"start\":83101},{\"end\":83110,\"start\":83109},{\"end\":83118,\"start\":83117},{\"end\":83342,\"start\":83341},{\"end\":83351,\"start\":83350},{\"end\":83486,\"start\":83485},{\"end\":83499,\"start\":83498},{\"end\":83511,\"start\":83510},{\"end\":83753,\"start\":83752},{\"end\":83766,\"start\":83765},{\"end\":83775,\"start\":83774},{\"end\":83789,\"start\":83788},{\"end\":84018,\"start\":84017},{\"end\":84031,\"start\":84030},{\"end\":84045,\"start\":84044},{\"end\":84255,\"start\":84254},{\"end\":84263,\"start\":84262},{\"end\":84270,\"start\":84269},{\"end\":84484,\"start\":84483},{\"end\":84486,\"start\":84485},{\"end\":84497,\"start\":84496},{\"end\":84499,\"start\":84498},{\"end\":84510,\"start\":84509},{\"end\":84730,\"start\":84729},{\"end\":84738,\"start\":84737},{\"end\":84745,\"start\":84744},{\"end\":84752,\"start\":84751},{\"end\":84761,\"start\":84760},{\"end\":84769,\"start\":84768},{\"end\":84778,\"start\":84777},{\"end\":84990,\"start\":84989},{\"end\":84998,\"start\":84997},{\"end\":85007,\"start\":85006},{\"end\":85013,\"start\":85012},{\"end\":85212,\"start\":85211},{\"end\":85220,\"start\":85219},{\"end\":85227,\"start\":85226},{\"end\":85229,\"start\":85228},{\"end\":85237,\"start\":85236},{\"end\":85608,\"start\":85607},{\"end\":85616,\"start\":85615},{\"end\":85622,\"start\":85621},{\"end\":85633,\"start\":85632},{\"end\":85641,\"start\":85640},{\"end\":85650,\"start\":85649},{\"end\":85658,\"start\":85657},{\"end\":85666,\"start\":85665},{\"end\":85668,\"start\":85667},{\"end\":85914,\"start\":85913},{\"end\":85922,\"start\":85921},{\"end\":86046,\"start\":86045},{\"end\":86053,\"start\":86052},{\"end\":86062,\"start\":86061},{\"end\":86070,\"start\":86069},{\"end\":86076,\"start\":86075},{\"end\":86345,\"start\":86344},{\"end\":86347,\"start\":86346},{\"end\":86358,\"start\":86357},{\"end\":86360,\"start\":86359},{\"end\":86368,\"start\":86367},{\"end\":86370,\"start\":86369},{\"end\":86381,\"start\":86380},{\"end\":86383,\"start\":86382},{\"end\":86570,\"start\":86569},{\"end\":86581,\"start\":86580},{\"end\":86591,\"start\":86590},{\"end\":86599,\"start\":86598}]", "bib_author_last_name": "[{\"end\":70055,\"start\":70050},{\"end\":70063,\"start\":70059},{\"end\":70077,\"start\":70067},{\"end\":70090,\"start\":70081},{\"end\":70101,\"start\":70094},{\"end\":70113,\"start\":70105},{\"end\":70322,\"start\":70309},{\"end\":70331,\"start\":70328},{\"end\":70343,\"start\":70335},{\"end\":70350,\"start\":70347},{\"end\":70359,\"start\":70356},{\"end\":70366,\"start\":70363},{\"end\":70613,\"start\":70605},{\"end\":70625,\"start\":70617},{\"end\":70729,\"start\":70721},{\"end\":70742,\"start\":70733},{\"end\":70757,\"start\":70746},{\"end\":70767,\"start\":70761},{\"end\":70776,\"start\":70771},{\"end\":70785,\"start\":70780},{\"end\":70797,\"start\":70791},{\"end\":70808,\"start\":70801},{\"end\":70818,\"start\":70812},{\"end\":70827,\"start\":70822},{\"end\":71145,\"start\":71138},{\"end\":71157,\"start\":71149},{\"end\":71166,\"start\":71161},{\"end\":71285,\"start\":71279},{\"end\":71296,\"start\":71289},{\"end\":71306,\"start\":71302},{\"end\":71314,\"start\":71310},{\"end\":71325,\"start\":71320},{\"end\":71331,\"start\":71329},{\"end\":71343,\"start\":71335},{\"end\":71350,\"start\":71347},{\"end\":71360,\"start\":71354},{\"end\":71371,\"start\":71364},{\"end\":71627,\"start\":71622},{\"end\":71637,\"start\":71631},{\"end\":71645,\"start\":71641},{\"end\":71656,\"start\":71649},{\"end\":71988,\"start\":71983},{\"end\":71998,\"start\":71992},{\"end\":72005,\"start\":72002},{\"end\":72012,\"start\":72009},{\"end\":72020,\"start\":72016},{\"end\":72031,\"start\":72024},{\"end\":72265,\"start\":72260},{\"end\":72275,\"start\":72269},{\"end\":72282,\"start\":72279},{\"end\":72293,\"start\":72286},{\"end\":72499,\"start\":72494},{\"end\":72506,\"start\":72503},{\"end\":72517,\"start\":72510},{\"end\":72774,\"start\":72770},{\"end\":72782,\"start\":72778},{\"end\":72792,\"start\":72786},{\"end\":72804,\"start\":72796},{\"end\":73090,\"start\":73086},{\"end\":73101,\"start\":73094},{\"end\":73111,\"start\":73105},{\"end\":73122,\"start\":73115},{\"end\":73294,\"start\":73285},{\"end\":73305,\"start\":73298},{\"end\":73314,\"start\":73309},{\"end\":73324,\"start\":73318},{\"end\":73339,\"start\":73328},{\"end\":73556,\"start\":73553},{\"end\":73569,\"start\":73562},{\"end\":73695,\"start\":73684},{\"end\":73702,\"start\":73699},{\"end\":73715,\"start\":73706},{\"end\":73724,\"start\":73719},{\"end\":73734,\"start\":73728},{\"end\":73899,\"start\":73896},{\"end\":73906,\"start\":73903},{\"end\":73913,\"start\":73910},{\"end\":73922,\"start\":73917},{\"end\":73932,\"start\":73926},{\"end\":73938,\"start\":73936},{\"end\":73945,\"start\":73942},{\"end\":73951,\"start\":73949},{\"end\":73957,\"start\":73955},{\"end\":73965,\"start\":73961},{\"end\":74219,\"start\":74213},{\"end\":74228,\"start\":74223},{\"end\":74239,\"start\":74232},{\"end\":74460,\"start\":74455},{\"end\":74471,\"start\":74464},{\"end\":74482,\"start\":74475},{\"end\":74494,\"start\":74486},{\"end\":74503,\"start\":74498},{\"end\":74769,\"start\":74764},{\"end\":74781,\"start\":74773},{\"end\":75076,\"start\":75072},{\"end\":75084,\"start\":75080},{\"end\":75095,\"start\":75088},{\"end\":75287,\"start\":75282},{\"end\":75301,\"start\":75291},{\"end\":75456,\"start\":75452},{\"end\":75465,\"start\":75460},{\"end\":75473,\"start\":75469},{\"end\":75482,\"start\":75477},{\"end\":75490,\"start\":75486},{\"end\":75499,\"start\":75494},{\"end\":75510,\"start\":75503},{\"end\":75765,\"start\":75759},{\"end\":75776,\"start\":75769},{\"end\":75968,\"start\":75964},{\"end\":75976,\"start\":75972},{\"end\":75986,\"start\":75980},{\"end\":75994,\"start\":75990},{\"end\":76002,\"start\":75998},{\"end\":76013,\"start\":76006},{\"end\":76253,\"start\":76250},{\"end\":76261,\"start\":76257},{\"end\":76272,\"start\":76265},{\"end\":76282,\"start\":76278},{\"end\":76292,\"start\":76288},{\"end\":76306,\"start\":76296},{\"end\":76559,\"start\":76557},{\"end\":76567,\"start\":76563},{\"end\":76576,\"start\":76571},{\"end\":76584,\"start\":76580},{\"end\":76591,\"start\":76588},{\"end\":76600,\"start\":76595},{\"end\":76611,\"start\":76604},{\"end\":76852,\"start\":76847},{\"end\":76860,\"start\":76856},{\"end\":76866,\"start\":76864},{\"end\":76874,\"start\":76870},{\"end\":76882,\"start\":76878},{\"end\":76890,\"start\":76886},{\"end\":76901,\"start\":76894},{\"end\":77135,\"start\":77130},{\"end\":77143,\"start\":77139},{\"end\":77151,\"start\":77147},{\"end\":77159,\"start\":77155},{\"end\":77165,\"start\":77163},{\"end\":77174,\"start\":77169},{\"end\":77185,\"start\":77178},{\"end\":77394,\"start\":77391},{\"end\":77406,\"start\":77398},{\"end\":77415,\"start\":77410},{\"end\":77426,\"start\":77419},{\"end\":77434,\"start\":77430},{\"end\":77442,\"start\":77440},{\"end\":77452,\"start\":77448},{\"end\":77713,\"start\":77710},{\"end\":77721,\"start\":77717},{\"end\":77732,\"start\":77725},{\"end\":77962,\"start\":77960},{\"end\":77973,\"start\":77968},{\"end\":77980,\"start\":77977},{\"end\":77992,\"start\":77986},{\"end\":78184,\"start\":78182},{\"end\":78192,\"start\":78188},{\"end\":78198,\"start\":78196},{\"end\":78207,\"start\":78202},{\"end\":78218,\"start\":78211},{\"end\":78422,\"start\":78411},{\"end\":78430,\"start\":78426},{\"end\":78438,\"start\":78434},{\"end\":78446,\"start\":78442},{\"end\":78460,\"start\":78450},{\"end\":78467,\"start\":78464},{\"end\":78475,\"start\":78471},{\"end\":78483,\"start\":78481},{\"end\":78494,\"start\":78487},{\"end\":78750,\"start\":78744},{\"end\":78758,\"start\":78754},{\"end\":78774,\"start\":78762},{\"end\":78783,\"start\":78778},{\"end\":78790,\"start\":78787},{\"end\":78798,\"start\":78794},{\"end\":78809,\"start\":78802},{\"end\":79041,\"start\":79037},{\"end\":79049,\"start\":79045},{\"end\":79057,\"start\":79053},{\"end\":79248,\"start\":79238},{\"end\":79258,\"start\":79252},{\"end\":79266,\"start\":79262},{\"end\":79278,\"start\":79270},{\"end\":79288,\"start\":79282},{\"end\":79300,\"start\":79292},{\"end\":79311,\"start\":79304},{\"end\":79321,\"start\":79315},{\"end\":79333,\"start\":79325},{\"end\":79343,\"start\":79337},{\"end\":79602,\"start\":79596},{\"end\":79617,\"start\":79606},{\"end\":79627,\"start\":79621},{\"end\":79637,\"start\":79631},{\"end\":79907,\"start\":79901},{\"end\":79916,\"start\":79911},{\"end\":79928,\"start\":79922},{\"end\":80361,\"start\":80352},{\"end\":80374,\"start\":80367},{\"end\":80387,\"start\":80380},{\"end\":80398,\"start\":80391},{\"end\":80409,\"start\":80404},{\"end\":80636,\"start\":80627},{\"end\":80842,\"start\":80833},{\"end\":80854,\"start\":80848},{\"end\":80865,\"start\":80858},{\"end\":81034,\"start\":81025},{\"end\":81048,\"start\":81038},{\"end\":81058,\"start\":81052},{\"end\":81068,\"start\":81062},{\"end\":81397,\"start\":81392},{\"end\":81406,\"start\":81401},{\"end\":81413,\"start\":81410},{\"end\":81419,\"start\":81417},{\"end\":81429,\"start\":81423},{\"end\":81440,\"start\":81433},{\"end\":81632,\"start\":81627},{\"end\":81639,\"start\":81636},{\"end\":81652,\"start\":81643},{\"end\":81661,\"start\":81658},{\"end\":81670,\"start\":81665},{\"end\":81681,\"start\":81674},{\"end\":81991,\"start\":81982},{\"end\":82001,\"start\":81995},{\"end\":82011,\"start\":82005},{\"end\":82020,\"start\":82015},{\"end\":82029,\"start\":82024},{\"end\":82041,\"start\":82033},{\"end\":82200,\"start\":82193},{\"end\":82211,\"start\":82204},{\"end\":82436,\"start\":82432},{\"end\":82443,\"start\":82440},{\"end\":82450,\"start\":82447},{\"end\":82637,\"start\":82629},{\"end\":82648,\"start\":82643},{\"end\":82657,\"start\":82652},{\"end\":82670,\"start\":82663},{\"end\":82683,\"start\":82676},{\"end\":82860,\"start\":82856},{\"end\":82877,\"start\":82864},{\"end\":83075,\"start\":83071},{\"end\":83090,\"start\":83079},{\"end\":83099,\"start\":83094},{\"end\":83107,\"start\":83103},{\"end\":83115,\"start\":83111},{\"end\":83125,\"start\":83119},{\"end\":83348,\"start\":83343},{\"end\":83357,\"start\":83352},{\"end\":83496,\"start\":83487},{\"end\":83508,\"start\":83500},{\"end\":83518,\"start\":83512},{\"end\":83763,\"start\":83754},{\"end\":83772,\"start\":83767},{\"end\":83786,\"start\":83776},{\"end\":83797,\"start\":83790},{\"end\":84028,\"start\":84019},{\"end\":84042,\"start\":84032},{\"end\":84053,\"start\":84046},{\"end\":84260,\"start\":84256},{\"end\":84267,\"start\":84264},{\"end\":84278,\"start\":84271},{\"end\":84494,\"start\":84487},{\"end\":84507,\"start\":84500},{\"end\":84516,\"start\":84511},{\"end\":84735,\"start\":84731},{\"end\":84742,\"start\":84739},{\"end\":84749,\"start\":84746},{\"end\":84758,\"start\":84753},{\"end\":84766,\"start\":84762},{\"end\":84775,\"start\":84770},{\"end\":84786,\"start\":84779},{\"end\":84995,\"start\":84991},{\"end\":85004,\"start\":84999},{\"end\":85010,\"start\":85008},{\"end\":85019,\"start\":85014},{\"end\":85217,\"start\":85213},{\"end\":85224,\"start\":85221},{\"end\":85234,\"start\":85230},{\"end\":85246,\"start\":85238},{\"end\":85613,\"start\":85609},{\"end\":85619,\"start\":85617},{\"end\":85630,\"start\":85623},{\"end\":85638,\"start\":85634},{\"end\":85647,\"start\":85642},{\"end\":85655,\"start\":85651},{\"end\":85663,\"start\":85659},{\"end\":85671,\"start\":85669},{\"end\":85919,\"start\":85915},{\"end\":85928,\"start\":85923},{\"end\":86050,\"start\":86047},{\"end\":86059,\"start\":86054},{\"end\":86067,\"start\":86063},{\"end\":86073,\"start\":86071},{\"end\":86079,\"start\":86077},{\"end\":86355,\"start\":86348},{\"end\":86365,\"start\":86361},{\"end\":86378,\"start\":86371},{\"end\":86387,\"start\":86384},{\"end\":86578,\"start\":86571},{\"end\":86588,\"start\":86582},{\"end\":86596,\"start\":86592},{\"end\":86607,\"start\":86600}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":70273,\"start\":69988},{\"attributes\":{\"id\":\"b1\"},\"end\":70562,\"start\":70275},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6366436},\"end\":70715,\"start\":70564},{\"attributes\":{\"id\":\"b3\"},\"end\":71070,\"start\":70717},{\"attributes\":{\"id\":\"b4\"},\"end\":71275,\"start\":71072},{\"attributes\":{\"id\":\"b5\"},\"end\":71618,\"start\":71277},{\"attributes\":{\"id\":\"b6\"},\"end\":71907,\"start\":71620},{\"attributes\":{\"id\":\"b7\"},\"end\":72188,\"start\":71909},{\"attributes\":{\"id\":\"b8\"},\"end\":72427,\"start\":72190},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":53109814},\"end\":72766,\"start\":72429},{\"attributes\":{\"id\":\"b10\"},\"end\":73047,\"start\":72768},{\"attributes\":{\"id\":\"b11\"},\"end\":73226,\"start\":73049},{\"attributes\":{\"id\":\"b12\"},\"end\":73485,\"start\":73228},{\"attributes\":{\"id\":\"b13\"},\"end\":73680,\"start\":73487},{\"attributes\":{\"id\":\"b14\"},\"end\":73892,\"start\":73682},{\"attributes\":{\"id\":\"b15\"},\"end\":74143,\"start\":73894},{\"attributes\":{\"id\":\"b16\"},\"end\":74368,\"start\":74145},{\"attributes\":{\"id\":\"b17\"},\"end\":74669,\"start\":74370},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":13441162},\"end\":74968,\"start\":74671},{\"attributes\":{\"id\":\"b19\"},\"end\":75249,\"start\":74970},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":976263},\"end\":75448,\"start\":75251},{\"attributes\":{\"id\":\"b21\"},\"end\":75753,\"start\":75450},{\"attributes\":{\"id\":\"b22\"},\"end\":75890,\"start\":75755},{\"attributes\":{\"id\":\"b23\"},\"end\":76169,\"start\":75892},{\"attributes\":{\"id\":\"b24\"},\"end\":76476,\"start\":76171},{\"attributes\":{\"id\":\"b25\"},\"end\":76783,\"start\":76478},{\"attributes\":{\"id\":\"b26\"},\"end\":77054,\"start\":76785},{\"attributes\":{\"id\":\"b27\"},\"end\":77351,\"start\":77056},{\"attributes\":{\"id\":\"b28\"},\"end\":77588,\"start\":77353},{\"attributes\":{\"id\":\"b29\"},\"end\":77884,\"start\":77590},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":13972155},\"end\":78144,\"start\":77886},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":85464424},\"end\":78340,\"start\":78146},{\"attributes\":{\"id\":\"b32\"},\"end\":78687,\"start\":78342},{\"attributes\":{\"id\":\"b33\"},\"end\":78966,\"start\":78689},{\"attributes\":{\"id\":\"b34\"},\"end\":79183,\"start\":78968},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1534183},\"end\":79592,\"start\":79185},{\"attributes\":{\"id\":\"b36\"},\"end\":79823,\"start\":79594},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":16462148},\"end\":80284,\"start\":79825},{\"attributes\":{\"id\":\"b38\"},\"end\":80565,\"start\":80286},{\"attributes\":{\"id\":\"b39\"},\"end\":80736,\"start\":80567},{\"attributes\":{\"id\":\"b40\"},\"end\":81021,\"start\":80738},{\"attributes\":{\"id\":\"b41\"},\"end\":81290,\"start\":81023},{\"attributes\":{\"id\":\"b42\"},\"end\":81623,\"start\":81292},{\"attributes\":{\"id\":\"b43\"},\"end\":81928,\"start\":81625},{\"attributes\":{\"id\":\"b44\"},\"end\":82187,\"start\":81930},{\"attributes\":{\"id\":\"b45\"},\"end\":82344,\"start\":82189},{\"attributes\":{\"id\":\"b46\"},\"end\":82589,\"start\":82346},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":39032320},\"end\":82850,\"start\":82591},{\"attributes\":{\"id\":\"b48\"},\"end\":82987,\"start\":82852},{\"attributes\":{\"id\":\"b49\"},\"end\":83296,\"start\":82989},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":20087915},\"end\":83483,\"start\":83298},{\"attributes\":{\"id\":\"b51\"},\"end\":83688,\"start\":83485},{\"attributes\":{\"id\":\"b52\"},\"end\":83937,\"start\":83690},{\"attributes\":{\"id\":\"b53\"},\"end\":84196,\"start\":83939},{\"attributes\":{\"id\":\"b54\"},\"end\":84366,\"start\":84198},{\"attributes\":{\"id\":\"b55\"},\"end\":84679,\"start\":84368},{\"attributes\":{\"id\":\"b56\"},\"end\":84928,\"start\":84681},{\"attributes\":{\"id\":\"b57\"},\"end\":85139,\"start\":84930},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":14222084},\"end\":85541,\"start\":85141},{\"attributes\":{\"id\":\"b59\"},\"end\":85838,\"start\":85543},{\"attributes\":{\"id\":\"b60\"},\"end\":86043,\"start\":85840},{\"attributes\":{\"id\":\"b61\"},\"end\":86294,\"start\":86045},{\"attributes\":{\"id\":\"b62\"},\"end\":86509,\"start\":86296},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":181980},\"end\":86827,\"start\":86511}]", "bib_title": "[{\"end\":70305,\"start\":70275},{\"end\":70601,\"start\":70564},{\"end\":72490,\"start\":72429},{\"end\":74760,\"start\":74671},{\"end\":75278,\"start\":75251},{\"end\":77954,\"start\":77886},{\"end\":78176,\"start\":78146},{\"end\":79234,\"start\":79185},{\"end\":79895,\"start\":79825},{\"end\":82623,\"start\":82591},{\"end\":83339,\"start\":83298},{\"end\":85209,\"start\":85141},{\"end\":86567,\"start\":86511}]", "bib_author": "[{\"end\":70057,\"start\":70048},{\"end\":70065,\"start\":70057},{\"end\":70079,\"start\":70065},{\"end\":70092,\"start\":70079},{\"end\":70103,\"start\":70092},{\"end\":70115,\"start\":70103},{\"end\":70324,\"start\":70307},{\"end\":70333,\"start\":70324},{\"end\":70345,\"start\":70333},{\"end\":70352,\"start\":70345},{\"end\":70361,\"start\":70352},{\"end\":70368,\"start\":70361},{\"end\":70615,\"start\":70603},{\"end\":70627,\"start\":70615},{\"end\":70731,\"start\":70719},{\"end\":70744,\"start\":70731},{\"end\":70759,\"start\":70744},{\"end\":70769,\"start\":70759},{\"end\":70778,\"start\":70769},{\"end\":70787,\"start\":70778},{\"end\":70799,\"start\":70787},{\"end\":70810,\"start\":70799},{\"end\":70820,\"start\":70810},{\"end\":70829,\"start\":70820},{\"end\":71147,\"start\":71136},{\"end\":71159,\"start\":71147},{\"end\":71168,\"start\":71159},{\"end\":71287,\"start\":71277},{\"end\":71298,\"start\":71287},{\"end\":71308,\"start\":71298},{\"end\":71316,\"start\":71308},{\"end\":71327,\"start\":71316},{\"end\":71333,\"start\":71327},{\"end\":71345,\"start\":71333},{\"end\":71352,\"start\":71345},{\"end\":71362,\"start\":71352},{\"end\":71373,\"start\":71362},{\"end\":71629,\"start\":71620},{\"end\":71639,\"start\":71629},{\"end\":71647,\"start\":71639},{\"end\":71658,\"start\":71647},{\"end\":71990,\"start\":71981},{\"end\":72000,\"start\":71990},{\"end\":72007,\"start\":72000},{\"end\":72014,\"start\":72007},{\"end\":72022,\"start\":72014},{\"end\":72033,\"start\":72022},{\"end\":72267,\"start\":72258},{\"end\":72277,\"start\":72267},{\"end\":72284,\"start\":72277},{\"end\":72295,\"start\":72284},{\"end\":72501,\"start\":72492},{\"end\":72508,\"start\":72501},{\"end\":72519,\"start\":72508},{\"end\":72776,\"start\":72768},{\"end\":72784,\"start\":72776},{\"end\":72794,\"start\":72784},{\"end\":72806,\"start\":72794},{\"end\":73092,\"start\":73082},{\"end\":73103,\"start\":73092},{\"end\":73113,\"start\":73103},{\"end\":73124,\"start\":73113},{\"end\":73296,\"start\":73283},{\"end\":73307,\"start\":73296},{\"end\":73316,\"start\":73307},{\"end\":73326,\"start\":73316},{\"end\":73341,\"start\":73326},{\"end\":73558,\"start\":73551},{\"end\":73571,\"start\":73558},{\"end\":73697,\"start\":73682},{\"end\":73704,\"start\":73697},{\"end\":73717,\"start\":73704},{\"end\":73726,\"start\":73717},{\"end\":73736,\"start\":73726},{\"end\":73901,\"start\":73894},{\"end\":73908,\"start\":73901},{\"end\":73915,\"start\":73908},{\"end\":73924,\"start\":73915},{\"end\":73934,\"start\":73924},{\"end\":73940,\"start\":73934},{\"end\":73947,\"start\":73940},{\"end\":73953,\"start\":73947},{\"end\":73959,\"start\":73953},{\"end\":73967,\"start\":73959},{\"end\":74221,\"start\":74211},{\"end\":74230,\"start\":74221},{\"end\":74241,\"start\":74230},{\"end\":74462,\"start\":74453},{\"end\":74473,\"start\":74462},{\"end\":74484,\"start\":74473},{\"end\":74496,\"start\":74484},{\"end\":74505,\"start\":74496},{\"end\":74771,\"start\":74762},{\"end\":74783,\"start\":74771},{\"end\":75078,\"start\":75070},{\"end\":75086,\"start\":75078},{\"end\":75097,\"start\":75086},{\"end\":75289,\"start\":75280},{\"end\":75303,\"start\":75289},{\"end\":75458,\"start\":75450},{\"end\":75467,\"start\":75458},{\"end\":75475,\"start\":75467},{\"end\":75484,\"start\":75475},{\"end\":75492,\"start\":75484},{\"end\":75501,\"start\":75492},{\"end\":75512,\"start\":75501},{\"end\":75767,\"start\":75755},{\"end\":75778,\"start\":75767},{\"end\":75970,\"start\":75960},{\"end\":75978,\"start\":75970},{\"end\":75988,\"start\":75978},{\"end\":75996,\"start\":75988},{\"end\":76004,\"start\":75996},{\"end\":76015,\"start\":76004},{\"end\":76255,\"start\":76248},{\"end\":76263,\"start\":76255},{\"end\":76274,\"start\":76263},{\"end\":76284,\"start\":76274},{\"end\":76294,\"start\":76284},{\"end\":76308,\"start\":76294},{\"end\":76561,\"start\":76555},{\"end\":76569,\"start\":76561},{\"end\":76578,\"start\":76569},{\"end\":76586,\"start\":76578},{\"end\":76593,\"start\":76586},{\"end\":76602,\"start\":76593},{\"end\":76613,\"start\":76602},{\"end\":76854,\"start\":76845},{\"end\":76862,\"start\":76854},{\"end\":76868,\"start\":76862},{\"end\":76876,\"start\":76868},{\"end\":76884,\"start\":76876},{\"end\":76892,\"start\":76884},{\"end\":76903,\"start\":76892},{\"end\":77137,\"start\":77128},{\"end\":77145,\"start\":77137},{\"end\":77153,\"start\":77145},{\"end\":77161,\"start\":77153},{\"end\":77167,\"start\":77161},{\"end\":77176,\"start\":77167},{\"end\":77187,\"start\":77176},{\"end\":77396,\"start\":77389},{\"end\":77408,\"start\":77396},{\"end\":77417,\"start\":77408},{\"end\":77428,\"start\":77417},{\"end\":77436,\"start\":77428},{\"end\":77444,\"start\":77436},{\"end\":77454,\"start\":77444},{\"end\":77715,\"start\":77708},{\"end\":77723,\"start\":77715},{\"end\":77734,\"start\":77723},{\"end\":77964,\"start\":77956},{\"end\":77975,\"start\":77964},{\"end\":77982,\"start\":77975},{\"end\":77994,\"start\":77982},{\"end\":78186,\"start\":78178},{\"end\":78194,\"start\":78186},{\"end\":78200,\"start\":78194},{\"end\":78209,\"start\":78200},{\"end\":78220,\"start\":78209},{\"end\":78424,\"start\":78409},{\"end\":78432,\"start\":78424},{\"end\":78440,\"start\":78432},{\"end\":78448,\"start\":78440},{\"end\":78462,\"start\":78448},{\"end\":78469,\"start\":78462},{\"end\":78477,\"start\":78469},{\"end\":78485,\"start\":78477},{\"end\":78496,\"start\":78485},{\"end\":78752,\"start\":78742},{\"end\":78760,\"start\":78752},{\"end\":78776,\"start\":78760},{\"end\":78785,\"start\":78776},{\"end\":78792,\"start\":78785},{\"end\":78800,\"start\":78792},{\"end\":78811,\"start\":78800},{\"end\":79043,\"start\":79033},{\"end\":79051,\"start\":79043},{\"end\":79059,\"start\":79051},{\"end\":79063,\"start\":79059},{\"end\":79250,\"start\":79236},{\"end\":79260,\"start\":79250},{\"end\":79268,\"start\":79260},{\"end\":79280,\"start\":79268},{\"end\":79290,\"start\":79280},{\"end\":79302,\"start\":79290},{\"end\":79313,\"start\":79302},{\"end\":79323,\"start\":79313},{\"end\":79335,\"start\":79323},{\"end\":79345,\"start\":79335},{\"end\":79604,\"start\":79594},{\"end\":79619,\"start\":79604},{\"end\":79629,\"start\":79619},{\"end\":79639,\"start\":79629},{\"end\":79909,\"start\":79897},{\"end\":79918,\"start\":79909},{\"end\":79930,\"start\":79918},{\"end\":80363,\"start\":80350},{\"end\":80376,\"start\":80363},{\"end\":80389,\"start\":80376},{\"end\":80400,\"start\":80389},{\"end\":80411,\"start\":80400},{\"end\":80638,\"start\":80623},{\"end\":80844,\"start\":80831},{\"end\":80856,\"start\":80844},{\"end\":80867,\"start\":80856},{\"end\":81036,\"start\":81023},{\"end\":81050,\"start\":81036},{\"end\":81060,\"start\":81050},{\"end\":81070,\"start\":81060},{\"end\":81399,\"start\":81390},{\"end\":81408,\"start\":81399},{\"end\":81415,\"start\":81408},{\"end\":81421,\"start\":81415},{\"end\":81431,\"start\":81421},{\"end\":81442,\"start\":81431},{\"end\":81634,\"start\":81625},{\"end\":81641,\"start\":81634},{\"end\":81654,\"start\":81641},{\"end\":81663,\"start\":81654},{\"end\":81672,\"start\":81663},{\"end\":81683,\"start\":81672},{\"end\":81993,\"start\":81980},{\"end\":82003,\"start\":81993},{\"end\":82013,\"start\":82003},{\"end\":82022,\"start\":82013},{\"end\":82031,\"start\":82022},{\"end\":82043,\"start\":82031},{\"end\":82202,\"start\":82189},{\"end\":82213,\"start\":82202},{\"end\":82438,\"start\":82430},{\"end\":82445,\"start\":82438},{\"end\":82452,\"start\":82445},{\"end\":82639,\"start\":82625},{\"end\":82650,\"start\":82639},{\"end\":82659,\"start\":82650},{\"end\":82672,\"start\":82659},{\"end\":82685,\"start\":82672},{\"end\":82862,\"start\":82852},{\"end\":82879,\"start\":82862},{\"end\":83077,\"start\":83067},{\"end\":83092,\"start\":83077},{\"end\":83101,\"start\":83092},{\"end\":83109,\"start\":83101},{\"end\":83117,\"start\":83109},{\"end\":83127,\"start\":83117},{\"end\":83350,\"start\":83341},{\"end\":83359,\"start\":83350},{\"end\":83498,\"start\":83485},{\"end\":83510,\"start\":83498},{\"end\":83520,\"start\":83510},{\"end\":83765,\"start\":83752},{\"end\":83774,\"start\":83765},{\"end\":83788,\"start\":83774},{\"end\":83799,\"start\":83788},{\"end\":84030,\"start\":84017},{\"end\":84044,\"start\":84030},{\"end\":84055,\"start\":84044},{\"end\":84262,\"start\":84254},{\"end\":84269,\"start\":84262},{\"end\":84280,\"start\":84269},{\"end\":84496,\"start\":84483},{\"end\":84509,\"start\":84496},{\"end\":84518,\"start\":84509},{\"end\":84737,\"start\":84729},{\"end\":84744,\"start\":84737},{\"end\":84751,\"start\":84744},{\"end\":84760,\"start\":84751},{\"end\":84768,\"start\":84760},{\"end\":84777,\"start\":84768},{\"end\":84788,\"start\":84777},{\"end\":84997,\"start\":84989},{\"end\":85006,\"start\":84997},{\"end\":85012,\"start\":85006},{\"end\":85021,\"start\":85012},{\"end\":85219,\"start\":85211},{\"end\":85226,\"start\":85219},{\"end\":85236,\"start\":85226},{\"end\":85248,\"start\":85236},{\"end\":85615,\"start\":85607},{\"end\":85621,\"start\":85615},{\"end\":85632,\"start\":85621},{\"end\":85640,\"start\":85632},{\"end\":85649,\"start\":85640},{\"end\":85657,\"start\":85649},{\"end\":85665,\"start\":85657},{\"end\":85673,\"start\":85665},{\"end\":85921,\"start\":85913},{\"end\":85930,\"start\":85921},{\"end\":86052,\"start\":86045},{\"end\":86061,\"start\":86052},{\"end\":86069,\"start\":86061},{\"end\":86075,\"start\":86069},{\"end\":86081,\"start\":86075},{\"end\":86357,\"start\":86344},{\"end\":86367,\"start\":86357},{\"end\":86380,\"start\":86367},{\"end\":86389,\"start\":86380},{\"end\":86580,\"start\":86569},{\"end\":86590,\"start\":86580},{\"end\":86598,\"start\":86590},{\"end\":86609,\"start\":86598}]", "bib_venue": "[{\"end\":70046,\"start\":69988},{\"end\":70405,\"start\":70368},{\"end\":70631,\"start\":70627},{\"end\":71134,\"start\":71072},{\"end\":71433,\"start\":71373},{\"end\":71755,\"start\":71658},{\"end\":71979,\"start\":71909},{\"end\":72256,\"start\":72190},{\"end\":72570,\"start\":72519},{\"end\":72899,\"start\":72806},{\"end\":73080,\"start\":73049},{\"end\":73281,\"start\":73228},{\"end\":73549,\"start\":73487},{\"end\":73774,\"start\":73736},{\"end\":74004,\"start\":73967},{\"end\":74209,\"start\":74145},{\"end\":74451,\"start\":74370},{\"end\":74812,\"start\":74783},{\"end\":75068,\"start\":74970},{\"end\":75341,\"start\":75303},{\"end\":75590,\"start\":75512},{\"end\":75816,\"start\":75778},{\"end\":75958,\"start\":75892},{\"end\":76246,\"start\":76171},{\"end\":76553,\"start\":76478},{\"end\":76843,\"start\":76785},{\"end\":77126,\"start\":77056},{\"end\":77387,\"start\":77353},{\"end\":77706,\"start\":77590},{\"end\":77998,\"start\":77994},{\"end\":78224,\"start\":78220},{\"end\":78407,\"start\":78342},{\"end\":78740,\"start\":78689},{\"end\":79031,\"start\":78968},{\"end\":79370,\"start\":79345},{\"end\":79700,\"start\":79639},{\"end\":80011,\"start\":79930},{\"end\":80348,\"start\":80286},{\"end\":80621,\"start\":80567},{\"end\":80829,\"start\":80738},{\"end\":81147,\"start\":81070},{\"end\":81388,\"start\":81292},{\"end\":81766,\"start\":81683},{\"end\":81978,\"start\":81930},{\"end\":82260,\"start\":82213},{\"end\":82428,\"start\":82346},{\"end\":82710,\"start\":82685},{\"end\":82913,\"start\":82879},{\"end\":83065,\"start\":82989},{\"end\":83384,\"start\":83359},{\"end\":83578,\"start\":83520},{\"end\":83750,\"start\":83690},{\"end\":84015,\"start\":83939},{\"end\":84252,\"start\":84198},{\"end\":84481,\"start\":84368},{\"end\":84727,\"start\":84681},{\"end\":84987,\"start\":84930},{\"end\":85289,\"start\":85248},{\"end\":85605,\"start\":85543},{\"end\":85911,\"start\":85840},{\"end\":86160,\"start\":86081},{\"end\":86342,\"start\":86296},{\"end\":86651,\"start\":86609},{\"end\":72608,\"start\":72572},{\"end\":80079,\"start\":80013}]"}}}, "year": 2023, "month": 12, "day": 17}