{"id": 50778760, "updated": "2023-10-03 10:35:06.622", "metadata": {"title": "Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System", "authors": "[{\"first\":\"Jiaxi\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Ke\",\"last\":\"Wang\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining", "publication_date": {"year": 2018, "month": 9, "day": 19}, "abstract": "We propose a novel way to train ranking models, such as recommender systems, that are both effective and efficient. Knowledge distillation (KD) was shown to be successful in image recognition to achieve both effectiveness and efficiency. We propose a KD technique for learning to rank problems, called \\emph{ranking distillation (RD)}. Specifically, we train a smaller student model to learn to rank documents/items from both the training data and the supervision of a larger teacher model. The student model achieves a similar ranking performance to that of the large teacher model, but its smaller model size makes the online inference more efficient. RD is flexible because it is orthogonal to the choices of ranking models for the teacher and student. We address the challenges of RD for ranking problems. The experiments on public data sets and state-of-the-art recommendation models showed that RD achieves its design purposes: the student model learnt with RD has a model size less than half of the teacher model while achieving a ranking performance similar to the teacher model and much better than the student model learnt without RD.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1809.07428", "mag": "2951780535", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1809-07428", "doi": "10.1145/3219819.3220021"}}, "content": {"source": {"pdf_hash": "b49c100cc8c288cb9c3a772a1addda78c08cd5c0", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1809.07428v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "807d0a343a15c5b1a602e7321e7bbe5bd4f14610", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b49c100cc8c288cb9c3a772a1addda78c08cd5c0.txt", "contents": "\nRanking Distillation: Learning Compact Ranking Models With High Performance for Recommender System\n\n\nJiaxi Tang jiaxit@sfu.ca \nSchool of Computing Science\nSchool of Computing Science\nSimon Fraser University British Columbia\nCanada\n\nKe Wang wangk@cs.sfu.ca \nSimon Fraser University British Columbia\nCanada\n\nRanking Distillation: Learning Compact Ranking Models With High Performance for Recommender System\n10.1145/3219819.3220021ACM Reference Format: Jiaxi Tang and Ke Wang. 2018. Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System. In Proceedings of The 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD '18). ACM, New York, NY, USA, 10 pages.CCS CONCEPTS \u2022 Information systems \u2192 Retrieval models and rankingRec- ommender systemsRetrieval efficiencyKEYWORDS Recommender SystemLearning to RankKnowledge TransferModel Compression\nWe propose a novel way to train ranking models, such as recommender systems, that are both effective and efficient. Knowledge distillation (KD) was shown to be successful in image recognition to achieve both effectiveness and efficiency. We propose a KD technique for learning to rank problems, called ranking distillation (RD). Specifically, we train a smaller student model to learn to rank documents/items from both the training data and the supervision of a larger teacher model. The student model achieves a similar ranking performance to that of the large teacher model, but its smaller model size makes the online inference more efficient. RD is flexible because it is orthogonal to the choices of ranking models for the teacher and student. We address the challenges of RD for ranking problems. The experiments on public data sets and state-of-the-art recommendation models showed that RD achieves its design purposes: the student model learnt with RD has a model size less than half of the teacher model while achieving a ranking performance similar to the teacher model and much better than the student model learnt without RD.\n\nINTRODUCTION\n\nGoogle and Yahoo; personalized item retrieval a.k.a recommender systems for Amazon and Netflix. The core in such systems is a ranking model for computing the relevance score of each (q, d) pair for future use, where q is the query (e.g., keywords for web page retrieval and user profile for recommender systems) and d is a document (e.g., a web page or item). The effectiveness of IR systems largely depends on how well the ranking model performs, whereas the efficiency determines how fast the systems will respond to user queries, a.k.a online inferences.\n\nSince the 2009 Netflix Prize competition, it is increasingly realized that a simple linear model with few parameters cannot model the complex query-document (user-item) interaction properly, and latent factor models with numerous parameters are shown to have better effectiveness and good representation power [21]. Recently, with the great impact of neural networks on computer vision [17,22] and natural language processing [19,25], a new branch of IR using neural networks has shown strong performances. As neural networks have incredible power to automatically capture features, recent works use neural networks to capture semantic representations for both queries and documents, relieving the manual feature engineering work required by other approaches. Several successful ranking models with neural networks have been investigated [14,33,35,37]. However, the size of such models (in terms of the number of model parameters) increases by an order of magnitude or more than previous methods. While such models have better ranking performance by capturing more query-document interactions, they incur a larger latency at online inference phase when responding to user requests due to the larger model size.\n\nBalancing effectiveness and efficiency has been a line of recent research [23,34,39,41,42]. Discrete hashing techniques [39,40,42] and binary coding of model parameters are suggested to speed up the calculation of the relevance score for a given (q,d) pair. Other works focus on database-related methods, such as pruning and indexing to speed-up retrieval of related items [23,34], using fast models for candidate generation and applying time-consuming models to the candidates for online inferences [9,24]. These methods either lose much of effectiveness, due to the introduced model constraints, or cannot be easily extended to other models in most cases, due to the model-dependency nature. given an query, student model learns to give higher rank for it's teacher model's top-K ranking of documents.\n\nfrom the training set, and a smaller student model is then trained by minimizing two deviations: the deviation from the training set's ground-truth label distribution, and the deviation from the label distribution generated by the teacher model. Then the student model is used for making online inferences. Intuitively, the larger teacher model helps capture more information of the label distribution (for example, outputting a high probability for \"tiger\" images for a \"cat\" image query due to correlation), which is used as an additional supervision to the training of the student model. The student model trained with KD has an effectiveness comparable to that of the teacher model [1,15,20] and can make more efficient online inference due to its small model size.\n\nDespite of this breakthrough in image recognition, it is not straightforward to apply KD to ranking models and ranking problems (e.g., recommendation). First, the existing KD is designed for classification problems, not for ranking problems. In ranking problems, the focus is on predicting the relative order of documents or items, instead of predicting a label or class as in classification. Second, KD requires computing the label distribution of documents for each query using both teacher and student models, which is feasible for image classification where there are a small number of labels, for example, no more than 1000 for the ImageNet data set; however, for ranking and recommendation problems, the total number of documents or items could be several orders of magnitudes larger, say millions, and computing the distribution for all documents or items for each training instance makes little sense, especially only the highly ranked documents or items near the top of the ranking will matter. We also want to point out that, for context sensitive recommendation, such as sequential recommendation, the items to be recommended usually depend on the user behaviors prior to the recommendation point (e.g., what she has viewed or purchased), and the set of contexts of a user is only known at the recommendation time. This feature requires recommender system to be strictly responsive and makes online inference efficiency particularly important.\n\n\nContributions\n\nIn this work, we study knowledge distillation for the learning to rank problem that is the core in recommender systems and many other IR systems. Our objective is achieving the ranking performance of a large model with the online inference efficiency of a small model. In particular, by fusing the idea of knowledge transfer and learning to rank, we propose a technique called ranking distillation (RD) to learn a compact ranking model that remains effective. The idea is shown in Figure 1b where a small student model is trained to learn to rank from two sources of information, i.e., the training set and the top-K documents for each query generated by a large well-trained teacher ranking model. With the large model size, the teacher model captures more ranking patterns from the training set and provides top-K ranked unlabeled documents as an extra training data for the student model. This makes RD differ from KD, as teacher model in KD only generate additional labels on existing data, while RD generate additional training data and labels from unlabeled data set. The student model benefits from the extra training data generated from the teacher, in addition to the data from usual training set, thus, inherits the strong ranking performance of the teacher, but is more efficient for online inferences thanks to its small model size.\n\nWe will examine several key issues of RD, i.e., the problem formulation, the representation of teacher's supervision, and the balance between the trust on the data from the training set and the data generated by the teacher model, and present our solutions. Extensive experiments on recommendation problems and real-world datasets show that the student model achieves a similar or better ranking performance compared to the teacher model while using less than half of model parameters. While the design goal of RD is retaining the teacher's effectiveness (while achieving the student's online inference efficiency), RD exceeds this expectation that the student sometime has a even better ranking performance than the teacher. Similar to KD, RD is orthogonal to the choices of student and teacher models by treating them as black-boxes. To our knowledge, this is the first attempt to adopt the idea of knowledge distillation to large-scale ranking problems.\n\nIn the rest of the paper, we introduce background materials in Section 2, propose ranking distillation for ranking problems in Section 3, present experimental studies in Section 4, and discuss related work in Section 5. We finally conclude with a summary of this work and in Section ??.\n\n\nBACKGROUNDS\n\nWe first review the learning to rank problem, then revisit the issues of effectiveness and efficiency in the problem, which serves to motivate our ranking distillation. Without loss of generality, we use the IR terms \"query\" q and \"document\" d in our discussion, but these terms can be replaced with \"user profile\" and \"item\" when applied to recommender systems.\n\n\nRanking from scratch\n\nThe learning to rank problem can be summarized as follows: Given a set of queries Q={q 1 ,\u00b7 \u00b7 \u00b7 ,q | Q | } and a set of documents D={d 1 ,\u00b7 \u00b7 \u00b7 ,d | D | }, we want to retrieve documents that are most relevant to a certain query. The degree of relevance for a query-document pair (q, d) is determined by a relevance score. Sometimes, for a single (q, d) pair, a relevance score y (q) d is labeled by human (or statistical results) as ground-truth, but the number of labeled (q, d) pairs is much smaller compared to the pairs with unknown labels. Such labels can be binary (i.e., relevant/non-relevant) or ordinal (i.e., very relevant/relevant/non-relevant). In order to rank documents for future queries with unknown relevance scores, we need a ranking model to predict their relevance scores. A ranking model M(q, d; \u03b8 ) =\u0177 (q) d is defined by a set of model parameters \u03b8 and computes a relevance score\u0177 (q) d given the query q and document d. The model predicted document ranking is supervised by the human-labeled ground truth ranking. The optimal model parameter set \u03b8 * is obtained by minimizing a ranking-based loss function:\n\u03b8 * = argmin \u03b8 q \u2208 Q L R (y (q) ,\u0177 (q) ).(1)\nFor simplicity, we focus on a single query q and omit the superscripts related to queries (i.e., y (q) d will becomes y d ). The ranking-based loss could be categorized as point-wise, pairwise, and list-wise. Since the first two are more widely adopted, we don't discuss list-wise loss in this work. The point-wise loss is widely used when relevance labels are binary [14,33]. One typical point-wise loss is taking the negative logarithmic of the likelihood function:\nL R (y,\u0177) = \u2212( d \u2208y d + log(P(rel = 1|\u0177 d )) + d \u2208y d \u2212 log(1 \u2212 P(rel = 1|\u0177 d ))),(2)\nwhere y d + and y d \u2212 are the sets of relevant and non-relevant documents, respectively. We could use the logistic function \u03c3 (x) = 1/(1+ e \u2212x ) and P(rel = 1|\u0177 d ) = \u03c3 (\u0177 d ) to transform a real-valued relevance score to the probability of a document being relevant (rel = 1). For ordinal relevance labels, pair-wise loss better models the partial order information:\nL R (y,\u0177) = \u2212 d i ,d j \u2208C log(P(d i \u227b d j |\u0177 i ,\u0177 j )),(3)\nwhere C is the set of document pairs {(d i , d j ) : y i \u227b y j } and the probability P(d i \u227b d j ) can be modeled using the logistic function P(d i \u227b d j |y i , y j ) = \u03c3 (y i \u2212 y j ).\n\n\nRethinking Effectiveness and Efficiency\n\nWe consider ranking models with latent factors or neural networks (a.k.a neural ranking models) instead of traditional models (e.g., SVM, tree-based models) for the following reasons. First, these models are well-studied recently for its capability to capture features from a latent space and are shown to be highly effective; indeed, neural ranking models are powerful for capturing rich semantics for queries and documents, which eliminates the tedious and ad-hoc feature extraction and engineering normally required in traditional models. Second, these models usually require many parameters and suffer from efficiency issue when making online inferences. Third, traditional models like SVM usually has convex guarantees and are trained through convex optimization. The objectives of latent factor models and neural networks are usually non-convex [8,18], which means that their training processes are more challenging and need more attentions. The goal of ranking models is predicting the rank of documents as accurately as possible near the top positions, through learning from human-labeled ground-truth document ranking. Typically, there are two ways to make a ranking model perform better at top positions: (1) By having a large model size, as long as it doesn't overfit the data, the model could better capture complex querydocument interaction patterns and has more predictive capability. Figure 2a shows that, when a ranking model has more parameters, it acquires more flexibility to fit the data and has a higher MAP, where the mean average precision (MAP) is more sensitive to the precision at top positions. (2) By having more training data, side information, human-defined rules etc., the model can be trained with more guidance and has less variance in gradients [15]. Figure 2b shows that, when more training instances are sampled from the underlying data distribution, a ranking model could achieve a better performance. However, each method has its limitations: method (1) surrenders efficiency for effectiveness whereas method (2) requires additional informative data, which is not always available or is expensive to obtain in practice.\n\n\nRANKING DISTILLATION\n\nIn this section, we propose ranking distillation (RD) to address the dual goals of effectiveness and efficiency for ranking problems. To address the efficiency of online inference, we use a smaller ranking model so that we can rank documents for a given query more efficiently. To address the effectiveness issue without requiring more\n\n\nGround-truth Document Labels:\n\nModel Predicted Top-Ranking:\n\n.. = ( ) , + \u2026 -)  Figure 3: The learning paradigm with ranking distillation. We first train a teacher model and let it predict a top-K ranked list of unlabeled (unobserved) documents for a given query q. The student model is then supervised by both ground-truth ranking from the training data set and teacher model's top-K ranking on unlabeled documents.\n\ntraining data, we introduce extra information generated from a well-trained teacher model and make the student model as effective as the teacher. Figure 3 shows the overview of ranking distillation. In the offline training phase (prior to any user query), similar to KD, first we train a large teacher model with a strong ranking performance on the training set. Then for each query, we use the well-trained teacher model to make predictions on unlabeled documents (green part in Figure 3) and use this extra information for learning the smaller student model. Since the teacher model is allowed to have many parameters, it captures more complex features for ranking and is much powerful, thus, its predictions on unlabeled documents could be used to provide extra information for the student model's training. The student model with fewer parameters is more efficient for online inference, and because of the extra information provided by the teacher model, the student model inherits the high ranking performance of the teacher model. Specifically, the offline training for student model with ranking distillation consists of two steps. First, we train a larger teacher model M T by minimizing a ranking-based loss with the groundtruth ranking from the training data set, as showed in Eqn (1). With much more parameters in this model, it captures more patterns from data and thus has a strong performance. We compute the predicted relevance scores of the teacher model M T for unlabeled documents O = {d : y d = \u2205} and get a top-K unlabeled document ranking \u03c0 1..K = (\u03c0 1 , . . . , \u03c0 K ), where \u03c0 r \u2208 D is the r -th document in this ranking. Then, we train a smaller ranking model M S to minimize a ranking loss from the ground-truth ranking in the training data set, as well as a distillation loss with the exemplary top-K ranking on unlabeled document set \u03c0 1..k offered by its teacher M T . The overall loss to be minimized is as follows:\n\n\nOverview\nL(\u03b8 S ) = (1 \u2212 \u03b1)L R (y,\u0177) + \u03b1 L D (\u03c0 1..K ,\u0177).(4)\nHere\u0177 is the student model's predicted scores 1 . L R (, ) stands for the ranking-based objective as in Eqn (1). The distillation loss, denoted by L D (, ), uses teacher model's top-K ranking on unlabeled documents to guide the student model learning. \u03b1 is the hyperparameter used for balancing these two losses. For a given query, the top documents ranked by the well-trained teacher can be regarded to have a strong correlation to this query, although they are not labeled in the training set. For example, if a user watches many action movies, the teacher's top-ranked documents may contain some other action movies as well as some adventure movies because they are correlated. In this sense, the proposed RD lets the teacher model teach its student to find the correlations and capture their patterns, thus, makes the student more generalizable and perform well on unseen data in the future. We use the top-K ranking from the teacher instead of the whole ranked list because the noisy ranking at lower positions tends to cause the student model to overfit its teacher and lose generalizability. Besides, only top positions are considered important for ranking problems. K is a hyper-parameter that represents the trust level on the teacher during teaching, i.e., how much we adopt from teacher.\n\nThe choice of the ranking loss L R (, ) follows from different models' preferences and we only focus on the second term L D (, ) in Eqn (4). A question is how much we should trust teacher's top-K ranking, especially for a larger K. In the rest of the section, we consider this issue.\n\n\nIncorporating Distillation Loss\n\nWe consider the point-wise ranking loss for binary relevance labels for performing distillation, we also tried the pair-wise loss and will discuss their pros and cons later. Similar to Eqn (2), we formalize distillation loss as:\nL D (\u03c0 1..K ,\u0177) = \u2212 K r =1 w r \u00b7 log(P(rel = 1|\u0177 \u03c0 r )) = \u2212 K r =1 w r \u00b7 log(\u03c3 (\u0177 \u03c0 r )),(5)\nwhere \u03c3 (\u00b7) is the sigmoid function and w r is the weight to be discussed later. There are several differences compared to Eqn (2). First, in Eqn (5), we treat the top-K ranked documents from the teacher model as positive instances and there is no negative instance. Recall that KD causes the student model to output a higher probability for the label \"tiger\" when the ground-truth label is \"cat\" because their features captured by the teacher model are correlated. Along this line, we want the student model to rank higher for teacher's top-K ranked documents. As we mentioned above, for the given query, besides the ground-truth positive documents y + , teacher's top-K ranked unlabeled documents are also strongly correlated to this query. These correlations are captured by the well-trained powerful teacher model in the latent space when using latent factor model or neural networks.\n\nHowever, as K increases, the relevance of the top-K ranked unlabeld documents becomes weaker. Following the work of learning Algorithm 1 Estimate Student's Ranking for \u03c0 r Require: Student Model M S (q, d; \u03b8 S ), unlabeled document set\u014c for a given query q and the hyper-parameter \u03b5 y \u03c0 r \u2190 M S (q, \u03c0 r ; \u03b8 S ) Initialize n = 0 for t = 1, 2, ...\u03f5 do Sample a document d from\u014c without replacement y d \u2190 M S (q, d; \u03b8 S ) if\u0177 d >\u0177 \u03c0 r then n \u2190 n + 1 end if end for r \u03c0 r \u2190 n\u00d7( |\u014c |\u22121) \u03f5 + 1 returnr \u03c0 r from noise labels [26], we use a weighted sum over the loss on documents from \u03c0 1..K with weight w r on each position r from 1 to K. There are two straightforward choices for w r : w r = 1/r puts more emphasis on the top positions, whereas w r = 1/K weights each position equally. Such weightings are heuristic and pre-determined, may not be flexible enough to deal with general cases. Instead, we introduce two flexible weighting schemes, which were shown to be superior in our experimental studies.\n\n\nWeighting by Position Importance.\n\nIn this weighting scheme, we assume that the teacher predicted unlabeled documents at top positions are more correlated to the query and are more likely to the positive ground-truth documents, therefore, this weight w a should be inversely proportional to the rank:\nw a r \u221d r \u22121 and r \u2208 [1, K],(6)\nwhere r is the rank range from 1 to K. As pointed out above, this scheme pre-determines the weight. Rendle et al [29] proposed an empirical weight for sampling a single position from a top-K ranking, following a geometric distribution:\n\nw a r = \u03c1(1 \u2212 \u03c1) r and \u03c1 \u2208 (0, 1).\n\nFollowing their work, we use a parametrized geometric distribution for weighting the position importance: w a r \u221d e \u2212r /\u03bb and \u03bb \u2208 R + ,\n\nwhere \u03bb is the hyperparameter that controls the sharpness of the distribution, and is searched through the validation set. When \u03bb is small, this scheme puts more emphasis on top positions, and when \u03bb is large enough, the distribution becomes the uniform distribution. This parametrization is easy to implement and configurable to each kind of situation.\n\n\n3.2.2\n\nWeighting by Ranking Discrepancy. The weighting by position importance is static, meaning that the weight at the same position is fixed during training process. Our second scheme is dynamic that considers the discrepancy between the student-predicted rank and the teacher-predicted rank for a given unlabeled document, and uses it as another weight w b . This weighting scheme allows the training to gradually concentrate on the documents in teacher's top-K ranking that are not well-predicted by the student. The details are as follows.  For the r -th document \u03c0 r (r \u2208 [1, K]) in teacher model's top-K ranking, the teacher-predicted ranking (i.e., r ) is known for us. But we know only the student predicted relevant score\u0177 \u03c0 r instead of its rank without computing relevance scores for all documents. To get the student predicted rank for this document, we apply Weston et al [36]'s sequential sampling, and do it in a parallel manner [16]. As described in Algorithm 1, for the r -th document \u03c0 r , if we want to know its rank in a list of N documents without computing the scores for all documents, we can randomly sample \u03f5 \u2208 [1, N \u2212 1] documents in this list and estimate the relative rank by n/\u03f5, where n is the number of documents whose (student) scores are greater than\u0177 \u03c0 r . Then the estimated rank in the whole list i\u015d r \u03c0 r = n\u00d7(N \u22121) \u03f5 + 1. When \u03f5 goes larger, the estimated rank is more close to the actual rank.\n\nAfter getting the estimated student's rankr \u03c0 r for the r -th document \u03c0 r in teacher's top-K ranking, the discrepancy between r andr is computed by\nw b r = tanh(max(\u00b5 \u00b7 (r \u03c0 r \u2212 r ), 0)),(9)\nwhere tanh(\u00b7) is a rescaled logistic function tanh(x) = 2\u03c3 (2x)\u22121 that rescale the output range to [0, 1] when x > 0. The hyper-parameter \u00b5 \u2208 R + is used to control the sharpness of the tanh function. Eqn (9) gives a dynamic weight: when the student predicted-rank of a document is close to its teacher, we think this document has been well-predicted and impose little loss on it (i.e., w b r \u2248 0); the rest concentrates on the documents (i.e., w b r \u2248 1) whose student predicted-rank is far from the teacher's rank. Note that the ranking discrepancy weight w b is computed for each document in \u03c0 1..K during training. So in practice, we choose \u03f5 \u226a |\u00d4| for training efficiency. While extra computation used to compute relevance scores for sampled \u03f5 documents, we still boost the whole offline training process. Because the dynamic weight allows the training to focus on the erroneous parts in the distillation loss.\n\n\nHybrid Weighting Scheme.\n\nThe hybrid weighting combines the weight w a by position importance, and the weight w b by ranking discrepancy:  illustrates the advantages of hybrid weighting over weighting only by position importance. Our experiments show that this hybrid weighting gives better results in general. In the actual implementation, since the estimated student ranking ofr \u03c0 r is not accurate during the first few iterations, we use only w a during the first m iterations to warm up the model, and then use the hybrid weighting to make training focus on the erroneous parts in distillation loss. m should be determined via the validation set. In our experiments, m is usually set to more than half of the total training iterations.\nw r = (w a r \u00b7 w b r )/( K i=1 w a i \u00b7 w b i ).\n\nDiscussion\n\nUnder the paradigm of ranking distillation, for a certain query q, besides the labeled documents, we use a top-K ranking for unlabeled documents generated by a well-trained teacher ranking model M T as extra information to guide the training of the student ranking model M S with less parameters. During the student model training, we use a weighted point-wise ranking loss as the distillation loss and propose two types of flexible weighting schemes, i.e., w a and w b and propose an effective way to fusion them. For the hyperparameters (\u03b1, \u03bb, \u00b5, \u03f5, K, m), they are dataset-dependent and are determined for each data set through the validation set. Two key factors for the success of ranking distillation are: (1) larger models are capable to capture the complex interaction patterns between queries and documents, thus, their predicted unlabeled documents at top positions are also strongly correlated with the given query and (2) student models with less parameters can learn from the extra-provided helpful information (top-K unlabeled documents in teacher's ranking) and boost their performances. We also tried to use a pair-wise distillation loss when learning from teacher's top-K ranking. Specifically, we use Eqn (3) for the distillation loss by taking the partial order in teacher's top-K ranking as objective. However, the results were disappointing. We found that if we use pair-wise distillation loss to place much focus on the partial order within teacher's ranking, it will produce both upward and downward gradients, making the training unstable and sometimes even fail to converge. However, our weighted point-wise distillation loss that only contains upward gradients doesn't suffer from this issue.\n\n\nEXPERIMENTAL STUDIES\n\nWe evaluate the performance of ranking distillation on two realworld data sets. The source code and processed data sets are publicly available online 2 .\n\n\nExperimental Setup\n\nTask Description. We use recommendation as our task for evaluating the performance of ranking distillation. In this problem, we 2 https://github.com/graytowne/rank_distill have a set of users U = {u 1 , u 2 , \u00b7 \u00b7 \u00b7 , u |U | } and a universe of items I = {i 1 , i 2 , \u00b7 \u00b7 \u00b7 , i |I | }. For recommendation without context information, we can cache the recommendation list for each user 3 . However, for context-aware recommendation, we have to re-compute the recommendation list each time a user comes with a new context, so the online inference efficiency becomes important. The following sequential recommendation is one case of context-aware recommendation. Given a users u with her/his history sequence (i.e., past L interacted items) at time t, S (u,t ) = (S u t \u22121 , .., S u t \u2212L ), where S u i \u2208 I, the goal is to retrieve a list of items for this user that meets her/his future needs. In IR's terms, the query is the user profile (u, S (u,t ) ) at time t, and the document is the item. Note that whenever the user has a new behavior (e.g., watch a video/listen to a music), we have to re-compute the recommendation list as her/his context changes. We also wish to point out that, in general, ranking distillation can be applied to other learning to rank tasks, not just to recommendation. Datasets. We choose two real-world data sets in this work, as they contain numerous sequential signals and thus suitable for sequential recommendation [33]. Their statistics are described in Table 1. Gowalla 4 was constructed by [7] and Foursquare was obtained from [38]. These data sets contain sequences of implicit feedbacks through user-venue check-ins. During the offline training phase, for a user u, we extract every 5 successive items (L = 5) from her sequence as S (u,t ) , and the immediately next item as the groundtruth. Following [33,38], we hold the first 70% of actions in each user's sequence as the training set and use the next 10% of actions as the validation set to search the optimal hyperparameter settings for all models. The remaining 20% actions in each user's sequence are used as the test set for evaluating a model's performance. Evaluation Metrics. As in [10,27,28,37], three different evaluation metrics used are Precision@n (Prec@n), nDCG@n, and Mean Average Precision (MAP). We set n \u2208 {3, 5, 10}, as recommendations are top positions of rank lists are more important. To measure the online inference efficiency, we count the number of parameters in each model and report the wall time for making a recommendation list to every user based on her/his last 5 actions in the training data set. While training models efficiently is also important, training is done offline before the recommendation phase starts, and our focus is the online inference efficiency where the user is waiting for the responses from the system. Teacher/Student Models. We apply the proposed ranking distillation to two sequential recommendation models that have been shown to have strong performances:\n\n\u2022 Fossil. Factorized Sequential Prediction with Item Similarity ModeLs (Fossil) [13] models sequential patterns and user  (1) The performance of the models with ranking distillation, Fossil-RD and Caser-RD, always has statistically significant improvements over the student-only models, Fossil-S and Caser-S. (2) The performance of the models with ranking distillation, Fossil-RD and Caser-RD, has no significant degradation from that of the teacher models, Fossil-T and Caser-T. We use the one-tail t-test with significance level at 0.05.  [33] incorporates the Convolutional Neural Network and latent factor model to learn sequential patterns as well as user preferences. It uses a point-wise ranking loss. To apply ranking distillation, we adopt as many parameters as possible for the teacher model to achieve a good performance on each data set. These well-trained teacher models are denoted by Fossil-T and Caser-T. We then use these models to teach smaller student models denoted by Fossil-RD and Caser-RD by minimizing the ranking distillation loss in Eqn (4). The model sizes of the student models are gradually increased until the models reach a comparable performance to their teachers. Fossil-S and Caser-S denote the student models trained with only ranking loss, i.e., without the help from the teacher. Note that the increasing in model sizes is achieved by using larger dimensions for embeddings, without any changes to the model structure.\n\n\nGowalla\n\n\nOverall Results\n\nThe results of each method are summarized in Table 2. We also included three non-sequential recommendation baselines: the popularity (in all users' sequences) based item recommendation (POP), the item based Collaborative Filtering 5 (ItemCF) [32], and the Bayesian personalized ranking (BPR) [30]. Clearly, the performance of these non-sequential baselines is worse than that of the sequential recommenders, i.e., Fossil and Caser. The teacher models, i.e., Fossil-T and Caser-T, have a better performance than the student-only models, i.e., Fossil-S and Caser-S, indicating that a larger model size provides more flexibility to fit the complex data with more predictive power. The effectiveness of ranking distillation is manifested by the significantly better performance of Fossil-RD and Caser-RD compared to Fossil-S and Caser-S, and by the similar performance of Fossil-RD and Caser-RD compared to Fossil-T and Caser-T. In other words, thanks to the knowledge transfer of ranking distillation, we are able to learn a student model that has fewer parameters but similar performance as the teacher model. Surprisingly, student models with ranking distillation often have even better performance than their teachers. This finding is consistent with [20] and we will explain possible reasons in Section 4.3.\n\nThe online inference efficiency is measured by the model size ( number of model parameters) and is shown in Table 3. Note that Fossil-S and Caser-S have the same model size as Fossil-RD and Caser-RD. All inferences were implemented using PyTorch with CUDA from GTX1070 GPU and Intel i7-6700K CPU. Fossil-RD and Caser-RD nearly half down the model size compared to their teacher models, Fossil-T and Caser-T. This reduction in model size is translated into a similar reduction in online inference time. In many practical applications, the data set is much larger than the data sets considered here in terms of the numbers of users and items; for example, Youtube could have 30 million active users per day and 1.3 billion of items 6 . For such large data sets, online inference could be more time-consuming and the reduction in model size has more privileges. Also, for models that are much more complicated than Fossil and Caser, the reduction in model size could yield a larger reduction in online inference time than reported here.\n\nIn conclusion, the findings in Table 2 and 3 together confirm that ranking distillation helps generate compact models with no or little compromise on effectiveness, and these advantages are independent of the choices of models.\n\n\nEffects of Model Size and Distillation Loss\n\nIn this experiment, we study the impact of model size on the student model's performance (i.e., MAP). We consider only Caser because the results for Fossil are similar. Figure 5a shows the results. Caser-S and Caser-RD perform better when the model size goes up, but there is always a gap. Caser-RD reaches a similar performance to its teacher with the medium model size, which is about 50% of the teacher model size. Figure 5b shows the impact of ranking distillation on the student model's MAP iteration by iteration. We compare four models: Caser-T performs well at first but tends to get overfitted after about 60 iterations due to its large model size and the sparse recommendation data sets. In contrast, Caser-RD and Caser-S-RD, which have smaller model sizes, are more robust to overfitting issue, though their training is partially supervised by the teacher. This finding reveals another advantage of ranking distillation. Figure 6 shows the MAP for various balancing parameter \u03b1 to explore models' performance when balancing ranking loss and distillation loss. For Gowalla data, the best performance is achieved when \u03b1 is around 0.5. But for Foursquare data, the best performance is achieved when \u03b1 is around 0.3, indicating too much concentrate on distillation loss leads to a bad performance. On both data sets, either discarding ranking loss or discarding distillation loss gives poor results. Table 4 shows the effects of the proposed weighting schemes in our ranking distillation. For the weight w r for r -th document in the teacher's top-K ranking, we used the equal weight (w r = 1/K) as the baseline and considered the weighting by position importance (w r = w a r ), the weighting by ranking discrepancy (w r = w b r ), and the hybrid weighting (w r \u221d w a r \u00b7 w b r ). The equal weight performs the worst. The position importance weighting is much better, suggesting that within the teacher's top-K ranking, documents at top positions are more related to the positive ground truth. The ranking discrepancy weighting only doesn't give impressive results, but when used with the position importance weighting, the hybrid weighting yields the best results on both data sets.\n\n\nEffects of Weighting Schemes\n\n\nRELATED WORK\n\nIn this section, we compared our works with several related research areas. Knowledge Distillation Knowledge distillation has been used in image recognition [3,15,31] and neural machine translation [20] as a way to generate compact models. As pointed out in Introduction, it is not straightforward to apply KD to ranking models and new issues must be addressed. In the context of ranking problems, the most relevant work is [6], which uses knowledge distillation for image retrieval. This method applies the sampling technique to rank a sample of the image from all data each time. In general, training on a sample works if the sample shares similar patterns with the rest of data through some content information, such as image contents in the case of [6]. But this technique is not applicable to training a recommender model when items and users are represented by IDs with no content information, as in the case of collaborative filtering. In this case, the recommender model training cannot be easily generalize to all users and items. Semi-Supervised Learning Another related research area is semisupervised learning [4,43]. Unlike the teacher-student model learning paradigm in knowledge distillation and in our work, semisupervised learning usually trains a single model and utilizes weaklabeled or unlabeled data as well as the labeled data to gain a better performance. Several works in information retrieval followed this direction, using weak-labeled or unlabeled data to construct test collections [2], to provide extra features [11] and labels [10] for ranking model training. The basic idea of ranking distillation and semi-supervised learning is similar as they both utilize unlabeled data while with different purpose. Transfer Learning for Recommender System Transfer learning has been widely used in the field of recommender systems [5,12]. These methods mainly focus on how to transfer knowledge (e.g., user rating patterns) from a source domain (e.g., movies) to a target domain (e.g., musics) for improving the recommendation performance. If we consider the student as a target model and the teacher as a source model, our teacher-student learning can be seen as a special transfer learning. However, unlike transfer learning, our teacher-student learning does not require two domains because the teacher and student models are learned from the same domain. Having a compact student model to enhance online inference efficiency is another purpose of our teacher-student learning.\n\n\nCONCLUSION\n\nThe proposed ranking distillation enables generating compact ranking models for better online inference efficiency without scarifying the ranking performance. The idea is training a teacher model with more parameters to teach a student model with fewer parameters to rank unlabeled documents. While the student model is compact, its training benefits from the extra supervision of the teacher, in addition to the usual ground truth from the training data, making the student model comparable with the teacher model in the ranking performance. This paper focused on several key issues of ranking distillation, i.e., the problem formulation, the representation of teacher's supervision, and the balance between the trust on the training data and the trust on the teacher, and presented our solutions. The evaluation on real data sets supported our claims.\n\nFigure 1 :\n1(a) Knowledge Distillation: given an input, student model learns to minimize the KL Divergence of its label distribution and teacher model's. (b) Ranking Distillation:\n\nFigure 2 :\n2Two ways of boosting mean average precision (MAP) on Gowalla data for recommendation. (a) shows that a larger model size in number of parameters, indicated by the bars, leads to a higher MAP. (b) shows that a larger sample size of training instances leads to a higher MAP.\n\nFigure 4 :\n4An illustration of hybrid weighting scheme. We use K = 3 in this example.\n\nFigure 4\n4\n\nFigure 5 :\n5Mean average precision vs. (a) model size and (b) the choice of distillation loss.\n\nFigure 6 :\n6MAP vs. balancing parameter \u03b1 Case-S, Caser-T, Caser-RD, and Caser-S-RD. The last model minimizes ranking loss during the first 30 iterations and adds distillation loss after that. Caser-RD outperforms Caser-S all the time. Caser-S reaches its limit after 50 iterations on Gowalla and 70 iterations on Foursquare. Caser-S-RD performs similarly to Caser-S during the first 30 iterations, but catches up with the Caser-RD at the end, indicating the impressive effectiveness of ranking distillation.\n\nTable 1 :\n1Statistics of the data setsDatasets \n#users \n#items \navg. actions \n(u, S (u,t ) ) \nSparsity \nper user \npairs \nGowalla \n13.1k \n14.0k \n40.74 \n367.6k \n99.71% \nFoursquare \n10.1k \n23.4k \n30.16 \n198.9k \n99.87% \n\n\n\nTable 2 :\n2Performance comparison.\n\nTable 3 :\n3Model compactness and online inference efficiency. Time (seconds) indicates the wall time used for generating a recommendation list for every user. Ratio is the student model's parameter size relative to the teacher model's parameter size.Datasets Model \nTime \nTime #Params Ratio \n(CPU) (GPU) \n\nGowalla \n\nFossil-T \n9.32s \n3.72s \n1.48M \n100% \nFossil-RD \n4.99s \n2.11s \n0.64M \n43.2% \nCaser-T \n38.58s \n4.52s \n5.58M \n100% \nCaser-RD 18.63s \n2.99s \n2.79M \n50.0% \n\nFoursquare \n\nFossil-T \n6.35s \n2.47s \n1.01M \n100% \nFossil-RD \n3.86s \n2.01s \n0.54M \n53.5% \nCaser-T \n23.89s \n2.95s \n4.06M \n100% \nCaser-RD 11.65s \n1.96s \n1.64M \n40.4% \n\n\n\nTable 4 :\n4Performance of Caser-RD with different choices of weighting scheme on two data sets.Datasets Weighting P@10 nDCG@10 \nMAP \n\nGowalla \n\nw r = 1/K \n0.0843 \n0.1198 \n0.0925 \nw r = w a \n\nr \n\n0.0850 \n0.1230 \n0.0945 \nw r = w b \n\nr \n\n0.0851 \n0.1227 \n0.0937 \nhybrid \n0.0878 \n0.1283 \n0.0969 \n\nFoursquare \n\nw r = 1/K \n0.0424 \n0.1046 \n0.0914 \nw r = w a \n\nr \n\n0.0423 \n0.1052 \n0.0929 \nw r = w b \n\nr \n\n0.0429 \n0.1035 \n0.0912 \nhybrid \n0.0444 \n0.1076 \n0.0952 \n\n\nWhen using point-wise and pair-wise losses, we only need to compute the student's predictions for a subset of documents, instead of all documents, for a given query.\nWe suppose the recommendation model doesn't change immediately whenever new observed data come, which is common in real-world cases. 4 https://snap.stanford.edu/data/loc-gowalla.html\nWe use Jaccard similarity measure and set the number of nearest neighbor to 20.\nACKNOWLEDGEMENTThe work of the second author is partially supported by a Discovery Grant from Natural Sciences and Engineering Research Council of Canada.\nLarge scale distributed neural network training through online distillation. Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Orm\u00e1ndi, George E Dahl, Geoffrey E Hinton, arXiv:1804.03235arXiv preprintRohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Orm\u00e1ndi, George E. Dahl, and Geoffrey E. Hinton. 2018. Large scale distributed neural network training through online distillation. arXiv preprint arXiv:1804.03235 (2018).\n\nPseudo test collections for learning web search ranking functions. Nima Asadi, Donald Metzler, Tamer Elsayed, Jimmy Lin, International Conference on Research and development in Information Retrieval. ACMNima Asadi, Donald Metzler, Tamer Elsayed, and Jimmy Lin. 2011. Pseudo test collections for learning web search ranking functions. In International Conference on Research and development in Information Retrieval. ACM, 1073-1082.\n\nDo deep nets really need to be deep. Jimmy Ba, Rich Caruana, Advances in neural information processing systems. Jimmy Ba and Rich Caruana. 2014. Do deep nets really need to be deep?. In Advances in neural information processing systems. 2654-2662.\n\nSemi-supervised learning (chapelle, o. Olivier Chapelle, Bernhard Scholkopf, Alexander Zien, . et al.book reviewsOlivier Chapelle, Bernhard Scholkopf, and Alexander Zien. 2009. Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews].\n\n. IEEE Transactions on Neural Networks. 20IEEE Transactions on Neural Networks 20, 3 (2009), 542-542.\n\nLearning to rank features for recommendation over multiple categories. Xu Chen, Zheng Qin, Yongfeng Zhang, Tao Xu, International ACM SIGIR conference on Research and Development in Information Retrieval. Xu Chen, Zheng Qin, Yongfeng Zhang, and Tao Xu. 2016. Learning to rank features for recommendation over multiple categories. In International ACM SIGIR conference on Research and Development in Information Retrieval. 305-314.\n\nYuntao Chen, Naiyan Wang, Zhaoxiang Zhang, arXiv:1707.01220DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer. arXiv preprintYuntao Chen, Naiyan Wang, and Zhaoxiang Zhang. 2017. DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer. arXiv preprint arXiv:1707.01220 (2017).\n\nFriendship and mobility: user movement in location-based social networks. Eunjoon Cho, A Seth, Jure Myers, Leskovec, International Conference on Knowledge Discovery and Data Mining. ACMEunjoon Cho, Seth A Myers, and Jure Leskovec. 2011. Friendship and mobility: user movement in location-based social networks. In International Conference on Knowledge Discovery and Data Mining. ACM, 1082-1090.\n\nThe loss surfaces of multilayer networks. Anna Choromanska, Mikael Henaff, Michael Mathieu, Artificial Intelligence and Statistics. G\u00e9rard Ben Arous, and Yann LeCunAnna Choromanska, Mikael Henaff, Michael Mathieu, G\u00e9rard Ben Arous, and Yann LeCun. 2015. The loss surfaces of multilayer networks. In Artificial Intelli- gence and Statistics. 192-204.\n\nDeep neural networks for youtube recommendations. Paul Covington, Jay Adams, Emre Sargin, ACM Conference on Recommender systems. Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In ACM Conference on Recommender systems. 191-198.\n\nMostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, W Bruce Croft, arXiv:1704.08803Neural Ranking Models with Weak Supervision. arXiv preprintMostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W Bruce Croft. 2017. Neural Ranking Models with Weak Supervision. arXiv preprint arXiv:1704.08803 (2017).\n\nLearning to Rank with Labeled Features. Fernando Diaz, International Conference on the Theory of Information Retrieval. ACMFernando Diaz. 2016. Learning to Rank with Labeled Features. In International Conference on the Theory of Information Retrieval. ACM, 41-44.\n\nCross-domain recommender systems: A survey of the state of the art. Ignacio Fern\u00e1ndez-Tob\u00edas, Iv\u00e1n Cantador, Marius Kaminskas, Francesco Ricci, Spanish Conference on Information Retrieval. sn. 24Ignacio Fern\u00e1ndez-Tob\u00edas, Iv\u00e1n Cantador, Marius Kaminskas, and Francesco Ricci. 2012. Cross-domain recommender systems: A survey of the state of the art. In Spanish Conference on Information Retrieval. sn, 24.\n\nFusing Similarity Models with Markov Chains for Sparse Sequential Recommendation. Ruining He, Julian Mcauley, International Conference on Data Mining. IEEERuining He and Julian McAuley. 2016. Fusing Similarity Models with Markov Chains for Sparse Sequential Recommendation. In International Conference on Data Mining. IEEE.\n\nNeural collaborative filtering. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua, International Conference on World Wide Web. ACMXiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In International Conference on World Wide Web. ACM, 173-182.\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. arXiv preprintGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015).\n\nCollaborative metric learning. Cheng-Kang Hsieh, Longqi Yang, Yin Cui, Tsung-Yi Lin, Serge Belongie, Deborah Estrin, International Conference on World Wide Web. Cheng-Kang Hsieh, Longqi Yang, Yin Cui, Tsung-Yi Lin, Serge Belongie, and Deborah Estrin. 2017. Collaborative metric learning. In International Conference on World Wide Web. 193-201.\n\nLarge-scale video classification with convolutional neural networks. Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, Li Fei-Fei, IEEE conference on Computer Vision and Pattern Recognition. Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Suk- thankar, and Li Fei-Fei. 2014. Large-scale video classification with convolutional neural networks. In IEEE conference on Computer Vision and Pattern Recognition. 1725-1732.\n\nDeep learning without poor local minima. Kenji Kawaguchi, Advances in Neural Information Processing Systems. Kenji Kawaguchi. 2016. Deep learning without poor local minima. In Advances in Neural Information Processing Systems. 586-594.\n\nConvolutional Neural Networks for Sentence Classification. Yoon Kim, Conference on Empirical Methods on Natural Language Processing. ACL. Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. In Conference on Empirical Methods on Natural Language Processing. ACL, 1756- 1751.\n\nYoon Kim, Alexander M Rush, arXiv:1606.07947Sequence-level knowledge distillation. arXiv preprintYoon Kim and Alexander M Rush. 2016. Sequence-level knowledge distillation. arXiv preprint arXiv:1606.07947 (2016).\n\nMatrix factorization techniques for recommender systems. Yehuda Koren, Robert Bell, Chris Volinsky, Computer. 42Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech- niques for recommender systems. Computer 42, 8 (2009).\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in Neural Information Processing Systems. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifica- tion with deep convolutional neural networks. In Advances in Neural Information Processing Systems. 1097-1105.\n\nFEXIPRO: Fast and Exact Inner Product Retrieval in Recommender Systems. Hui Li, Nam Tsz, Man Lung Chan, Nikos Yiu, Mamoulis, International Conference on Management of Data. ACMHui Li, Tsz Nam Chan, Man Lung Yiu, and Nikos Mamoulis. 2017. FEXIPRO: Fast and Exact Inner Product Retrieval in Recommender Systems. In International Conference on Management of Data. ACM, 835-850.\n\nRelated pins at pinterest: The evolution of a real-world recommender system. C David, Stephanie Liu, Raymond Rogers, Dmitry Shiau, Kislyuk, C Kevin, Zhigang Ma, Jenny Zhong, Yushi Liu, Jing, International Conference on World Wide Web. David C Liu, Stephanie Rogers, Raymond Shiau, Dmitry Kislyuk, Kevin C Ma, Zhigang Zhong, Jenny Liu, and Yushi Jing. 2017. Related pins at pinterest: The evolution of a real-world recommender system. In International Conference on World Wide Web. 583-592.\n\nRecurrent neural network based language model. Tomas Mikolov, Martin Karafi\u00e1t, Lukas Burget, Jan Cernock\u1ef3, Sanjeev Khudanpur, Interspeech. Tomas Mikolov, Martin Karafi\u00e1t, Lukas Burget, Jan Cernock\u1ef3, and Sanjeev Khu- danpur. 2010. Recurrent neural network based language model.. In Interspeech.\n\nLearning with noisy labels. Nagarajan Natarajan, S Inderjit, Dhillon, K Pradeep, Ambuj Ravikumar, Tewari, Advances in neural information processing systems. Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. 2013. Learning with noisy labels. In Advances in neural information processing systems. 1196-1204.\n\nText Matching As Image Recognition. Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan, Xueqi Cheng, AAAI Conference on Artificial Intelligence. AAAI PressLiang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan, and Xueqi Cheng. 2016. Text Matching As Image Recognition. In AAAI Conference on Artificial Intelligence. AAAI Press, 2793-2799.\n\nDeepRank: A New Deep Architecture for Relevance Ranking in Information Retrieval. Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Jingfang Xu, Xueqi Cheng, arXiv:1710.05649arXiv preprintLiang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Jingfang Xu, and Xueqi Cheng. 2017. DeepRank: A New Deep Architecture for Relevance Ranking in Information Retrieval. arXiv preprint arXiv:1710.05649 (2017).\n\nImproving pairwise learning for item recommendation from implicit feedback. Steffen Rendle, Christoph Freudenthaler, International Conference on Web Search and Data Mining. ACMSteffen Rendle and Christoph Freudenthaler. 2014. Improving pairwise learning for item recommendation from implicit feedback. In International Conference on Web Search and Data Mining. ACM, 273-282.\n\nBPR: Bayesian personalized ranking from implicit feedback. Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme, Conference on Uncertainty in Artificial Intelligence. AUAI PressSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian personalized ranking from implicit feedback. In Conference on Uncertainty in Artificial Intelligence. AUAI Press, 452-461.\n\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, Yoshua Bengio, arXiv:1412.6550Fitnets: Hints for thin deep nets. arXiv preprintAdriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. 2014. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550 (2014).\n\nItem-based collaborative filtering recommendation algorithms. Badrul Sarwar, George Karypis, Joseph Konstan, John Riedl, International Conference on World Wide Web. ACMBadrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-based collaborative filtering recommendation algorithms. In International Conference on World Wide Web. ACM, 285-295.\n\nPersonalized Top-N Sequential Recommendation via Convolutional Sequence Embedding. Jiaxi Tang, Ke Wang, ACM International Conference on Web Search and Data Mining. Jiaxi Tang and Ke Wang. 2018. Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding. In ACM International Conference on Web Search and Data Mining.\n\nLemp: Fast retrieval of large entries in a matrix product. Christina Teflioudi, Rainer Gemulla, Olga Mykytiuk, International Conference on Management of Data. ACMChristina Teflioudi, Rainer Gemulla, and Olga Mykytiuk. 2015. Lemp: Fast retrieval of large entries in a matrix product. In International Conference on Management of Data. ACM, 107-122.\n\nCollaborative deep learning for recommender systems. Hao Wang, Naiyan Wang, Dit-Yan Yeung, International Conference on Knowledge Discovery and Data Mining. ACMHao Wang, Naiyan Wang, and Dit-Yan Yeung. 2015. Collaborative deep learning for recommender systems. In International Conference on Knowledge Discovery and Data Mining. ACM, 1235-1244.\n\nLarge scale image annotation: learning to rank with joint word-image embeddings. Jason Weston, Samy Bengio, Nicolas Usunier, Machine learning. 81Jason Weston, Samy Bengio, and Nicolas Usunier. 2010. Large scale image annotation: learning to rank with joint word-image embeddings. Machine learning 81, 1 (2010), 21-35.\n\nEnd-to-End Neural Ad-hoc Ranking with Kernel Pooling. Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, Russell Power, International ACM SIGIR conference on Research and Development in Information Retrieval. Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-End Neural Ad-hoc Ranking with Kernel Pooling. In International ACM SIGIR conference on Research and Development in Information Retrieval. 55-64.\n\nGraph-based point-of-interest recommendation with geographical and temporal influences. Quan Yuan, Gao Cong, Aixin Sun, International Conference on Information and Knowledge Management. ACMQuan Yuan, Gao Cong, and Aixin Sun. 2014. Graph-based point-of-interest recommendation with geographical and temporal influences. In International Conference on Information and Knowledge Management. ACM, 659-668.\n\nDiscrete collaborative filtering. Hanwang Zhang, Fumin Shen, Wei Liu, Xiangnan He, Huanbo Luan, Tat-Seng Chua, International Conference on Research and Development in Information Retrieval. ACMHanwang Zhang, Fumin Shen, Wei Liu, Xiangnan He, Huanbo Luan, and Tat- Seng Chua. 2016. Discrete collaborative filtering. In International Conference on Research and Development in Information Retrieval. ACM, 325-334.\n\nDiscrete Personalized Ranking for Fast Collaborative Filtering from Implicit Feedback. Yan Zhang, Defu Lian, Guowu Yang, AAAI Conference on Artificial Intelligence. AAAI PressYan Zhang, Defu Lian, and Guowu Yang. 2017. Discrete Personalized Ranking for Fast Collaborative Filtering from Implicit Feedback.. In AAAI Conference on Artificial Intelligence. AAAI Press, 1669-1675.\n\nPreference preserving hashing for efficient recommendation. Zhiwei Zhang, Qifan Wang, Lingyun Ruan, Luo Si, International Conference on Research and Development in Information Retrieval. ACMZhiwei Zhang, Qifan Wang, Lingyun Ruan, and Luo Si. 2014. Preference preserv- ing hashing for efficient recommendation. In International Conference on Research and Development in Information Retrieval. ACM, 183-192.\n\nLearning binary codes for collaborative filtering. Ke Zhou, Hongyuan Zha, International Conference on Knowledge Discovery and Data Mining. ACMKe Zhou and Hongyuan Zha. 2012. Learning binary codes for collaborative filtering. In International Conference on Knowledge Discovery and Data Mining. ACM, 498-506.\n\nSemi-supervised learning literature survey. Xiaojin Zhu, Xiaojin Zhu. 2005. Semi-supervised learning literature survey. (2005).\n", "annotations": {"author": "[{\"end\":232,\"start\":102},{\"end\":306,\"start\":233}]", "publisher": null, "author_last_name": "[{\"end\":112,\"start\":108},{\"end\":240,\"start\":236}]", "author_first_name": "[{\"end\":107,\"start\":102},{\"end\":235,\"start\":233}]", "author_affiliation": "[{\"end\":231,\"start\":128},{\"end\":305,\"start\":258}]", "title": "[{\"end\":99,\"start\":1},{\"end\":405,\"start\":307}]", "venue": null, "abstract": "[{\"end\":2047,\"start\":910}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2936,\"start\":2932},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3012,\"start\":3008},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3015,\"start\":3012},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3052,\"start\":3048},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3055,\"start\":3052},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3464,\"start\":3460},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3467,\"start\":3464},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3470,\"start\":3467},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3473,\"start\":3470},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3912,\"start\":3908},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3915,\"start\":3912},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3918,\"start\":3915},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3921,\"start\":3918},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3924,\"start\":3921},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3958,\"start\":3954},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3961,\"start\":3958},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3964,\"start\":3961},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4211,\"start\":4207},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4214,\"start\":4211},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4337,\"start\":4334},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4340,\"start\":4337},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5328,\"start\":5325},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5331,\"start\":5328},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5334,\"start\":5331},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11423,\"start\":11419},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11426,\"start\":11423},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13114,\"start\":13111},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13117,\"start\":13114},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14043,\"start\":14039},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18643,\"start\":18640},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19275,\"start\":19272},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20557,\"start\":20553},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21488,\"start\":21484},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23027,\"start\":23023},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23086,\"start\":23082},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":28852,\"start\":28848},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28929,\"start\":28926},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":28967,\"start\":28963},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29244,\"start\":29240},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":29247,\"start\":29244},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29585,\"start\":29581},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29588,\"start\":29585},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":29591,\"start\":29588},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":29594,\"start\":29591},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":30490,\"start\":30486},{\"end\":30716,\"start\":30715},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":30951,\"start\":30947},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":32137,\"start\":32133},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":32187,\"start\":32183},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":33146,\"start\":33142},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":33932,\"start\":33931},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":36910,\"start\":36907},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":36913,\"start\":36910},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":36916,\"start\":36913},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":36952,\"start\":36948},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":37177,\"start\":37174},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":37506,\"start\":37503},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":37875,\"start\":37872},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":37878,\"start\":37875},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":38263,\"start\":38260},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":38295,\"start\":38291},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":38311,\"start\":38307},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":38604,\"start\":38601},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":38607,\"start\":38604}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":40299,\"start\":40119},{\"attributes\":{\"id\":\"fig_1\"},\"end\":40585,\"start\":40300},{\"attributes\":{\"id\":\"fig_2\"},\"end\":40672,\"start\":40586},{\"attributes\":{\"id\":\"fig_3\"},\"end\":40684,\"start\":40673},{\"attributes\":{\"id\":\"fig_4\"},\"end\":40780,\"start\":40685},{\"attributes\":{\"id\":\"fig_5\"},\"end\":41290,\"start\":40781},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":41509,\"start\":41291},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":41545,\"start\":41510},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":42180,\"start\":41546},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":42635,\"start\":42181}]", "paragraph": "[{\"end\":2620,\"start\":2063},{\"end\":3832,\"start\":2622},{\"end\":4637,\"start\":3834},{\"end\":5408,\"start\":4639},{\"end\":6864,\"start\":5410},{\"end\":8226,\"start\":6882},{\"end\":9184,\"start\":8228},{\"end\":9472,\"start\":9186},{\"end\":9850,\"start\":9488},{\"end\":11005,\"start\":9875},{\"end\":11518,\"start\":11051},{\"end\":11972,\"start\":11605},{\"end\":12216,\"start\":12032},{\"end\":14417,\"start\":12260},{\"end\":14777,\"start\":14442},{\"end\":14839,\"start\":14811},{\"end\":15196,\"start\":14841},{\"end\":17141,\"start\":15198},{\"end\":18502,\"start\":17204},{\"end\":18787,\"start\":18504},{\"end\":19051,\"start\":18823},{\"end\":20033,\"start\":19145},{\"end\":21035,\"start\":20035},{\"end\":21338,\"start\":21073},{\"end\":21606,\"start\":21371},{\"end\":21642,\"start\":21608},{\"end\":21779,\"start\":21644},{\"end\":22134,\"start\":21781},{\"end\":23570,\"start\":22144},{\"end\":23720,\"start\":23572},{\"end\":24679,\"start\":23764},{\"end\":25421,\"start\":24708},{\"end\":27201,\"start\":25483},{\"end\":27379,\"start\":27226},{\"end\":30404,\"start\":27402},{\"end\":31861,\"start\":30406},{\"end\":33199,\"start\":31891},{\"end\":34234,\"start\":33201},{\"end\":34463,\"start\":34236},{\"end\":36702,\"start\":34511},{\"end\":39250,\"start\":36750},{\"end\":40118,\"start\":39265}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11050,\"start\":11006},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11604,\"start\":11519},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12031,\"start\":11973},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17203,\"start\":17153},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19144,\"start\":19052},{\"attributes\":{\"id\":\"formula_5\"},\"end\":21370,\"start\":21339},{\"attributes\":{\"id\":\"formula_8\"},\"end\":23763,\"start\":23721},{\"attributes\":{\"id\":\"formula_9\"},\"end\":25469,\"start\":25422}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28895,\"start\":28888},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31943,\"start\":31936},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":33316,\"start\":33309},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":34274,\"start\":34267},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":35925,\"start\":35918}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2061,\"start\":2049},{\"attributes\":{\"n\":\"1.2\"},\"end\":6880,\"start\":6867},{\"attributes\":{\"n\":\"2\"},\"end\":9486,\"start\":9475},{\"attributes\":{\"n\":\"2.1\"},\"end\":9873,\"start\":9853},{\"attributes\":{\"n\":\"2.2\"},\"end\":12258,\"start\":12219},{\"attributes\":{\"n\":\"3\"},\"end\":14440,\"start\":14420},{\"end\":14809,\"start\":14780},{\"attributes\":{\"n\":\"3.1\"},\"end\":17152,\"start\":17144},{\"attributes\":{\"n\":\"3.2\"},\"end\":18821,\"start\":18790},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":21071,\"start\":21038},{\"end\":22142,\"start\":22137},{\"attributes\":{\"n\":\"3.2.3\"},\"end\":24706,\"start\":24682},{\"attributes\":{\"n\":\"3.3\"},\"end\":25481,\"start\":25471},{\"attributes\":{\"n\":\"4\"},\"end\":27224,\"start\":27204},{\"attributes\":{\"n\":\"4.1\"},\"end\":27400,\"start\":27382},{\"end\":31871,\"start\":31864},{\"attributes\":{\"n\":\"4.2\"},\"end\":31889,\"start\":31874},{\"attributes\":{\"n\":\"4.3\"},\"end\":34509,\"start\":34466},{\"attributes\":{\"n\":\"4.4\"},\"end\":36733,\"start\":36705},{\"attributes\":{\"n\":\"5\"},\"end\":36748,\"start\":36736},{\"attributes\":{\"n\":\"6\"},\"end\":39263,\"start\":39253},{\"end\":40130,\"start\":40120},{\"end\":40311,\"start\":40301},{\"end\":40597,\"start\":40587},{\"end\":40682,\"start\":40674},{\"end\":40696,\"start\":40686},{\"end\":40792,\"start\":40782},{\"end\":41301,\"start\":41292},{\"end\":41520,\"start\":41511},{\"end\":41556,\"start\":41547},{\"end\":42191,\"start\":42182}]", "table": "[{\"end\":41509,\"start\":41330},{\"end\":42180,\"start\":41797},{\"end\":42635,\"start\":42277}]", "figure_caption": "[{\"end\":40299,\"start\":40132},{\"end\":40585,\"start\":40313},{\"end\":40672,\"start\":40599},{\"end\":40780,\"start\":40698},{\"end\":41290,\"start\":40794},{\"end\":41330,\"start\":41303},{\"end\":41545,\"start\":41522},{\"end\":41797,\"start\":41558},{\"end\":42277,\"start\":42193}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7372,\"start\":7363},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13668,\"start\":13659},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14054,\"start\":14045},{\"end\":14868,\"start\":14860},{\"end\":15352,\"start\":15344},{\"end\":15686,\"start\":15678},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":34689,\"start\":34680},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":34938,\"start\":34929},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":35451,\"start\":35443}]", "bib_author_first_name": "[{\"end\":43302,\"start\":43297},{\"end\":43316,\"start\":43309},{\"end\":43335,\"start\":43326},{\"end\":43350,\"start\":43344},{\"end\":43366,\"start\":43360},{\"end\":43368,\"start\":43367},{\"end\":43383,\"start\":43375},{\"end\":43385,\"start\":43384},{\"end\":43722,\"start\":43718},{\"end\":43736,\"start\":43730},{\"end\":43751,\"start\":43746},{\"end\":43766,\"start\":43761},{\"end\":44126,\"start\":44121},{\"end\":44135,\"start\":44131},{\"end\":44379,\"start\":44372},{\"end\":44398,\"start\":44390},{\"end\":44419,\"start\":44410},{\"end\":44761,\"start\":44759},{\"end\":44773,\"start\":44768},{\"end\":44787,\"start\":44779},{\"end\":44798,\"start\":44795},{\"end\":45125,\"start\":45119},{\"end\":45138,\"start\":45132},{\"end\":45154,\"start\":45145},{\"end\":45535,\"start\":45528},{\"end\":45542,\"start\":45541},{\"end\":45553,\"start\":45549},{\"end\":45896,\"start\":45892},{\"end\":45916,\"start\":45910},{\"end\":45932,\"start\":45925},{\"end\":46255,\"start\":46251},{\"end\":46270,\"start\":46267},{\"end\":46282,\"start\":46278},{\"end\":46489,\"start\":46482},{\"end\":46505,\"start\":46500},{\"end\":46522,\"start\":46514},{\"end\":46536,\"start\":46532},{\"end\":46551,\"start\":46544},{\"end\":46855,\"start\":46847},{\"end\":47147,\"start\":47140},{\"end\":47170,\"start\":47166},{\"end\":47187,\"start\":47181},{\"end\":47208,\"start\":47199},{\"end\":47567,\"start\":47560},{\"end\":47578,\"start\":47572},{\"end\":47843,\"start\":47835},{\"end\":47852,\"start\":47848},{\"end\":47866,\"start\":47859},{\"end\":47881,\"start\":47874},{\"end\":47890,\"start\":47887},{\"end\":47903,\"start\":47895},{\"end\":48144,\"start\":48136},{\"end\":48158,\"start\":48153},{\"end\":48172,\"start\":48168},{\"end\":48436,\"start\":48426},{\"end\":48450,\"start\":48444},{\"end\":48460,\"start\":48457},{\"end\":48474,\"start\":48466},{\"end\":48485,\"start\":48480},{\"end\":48503,\"start\":48496},{\"end\":48815,\"start\":48809},{\"end\":48832,\"start\":48826},{\"end\":48850,\"start\":48843},{\"end\":48865,\"start\":48859},{\"end\":48878,\"start\":48873},{\"end\":48893,\"start\":48891},{\"end\":49259,\"start\":49254},{\"end\":49513,\"start\":49509},{\"end\":49752,\"start\":49748},{\"end\":49769,\"start\":49758},{\"end\":50025,\"start\":50019},{\"end\":50039,\"start\":50033},{\"end\":50051,\"start\":50046},{\"end\":50279,\"start\":50275},{\"end\":50296,\"start\":50292},{\"end\":50316,\"start\":50308},{\"end\":50318,\"start\":50317},{\"end\":50648,\"start\":50645},{\"end\":50656,\"start\":50653},{\"end\":50665,\"start\":50662},{\"end\":50670,\"start\":50666},{\"end\":50682,\"start\":50677},{\"end\":51027,\"start\":51026},{\"end\":51044,\"start\":51035},{\"end\":51057,\"start\":51050},{\"end\":51072,\"start\":51066},{\"end\":51090,\"start\":51089},{\"end\":51105,\"start\":51098},{\"end\":51115,\"start\":51110},{\"end\":51128,\"start\":51123},{\"end\":51492,\"start\":51487},{\"end\":51508,\"start\":51502},{\"end\":51524,\"start\":51519},{\"end\":51536,\"start\":51533},{\"end\":51554,\"start\":51547},{\"end\":51772,\"start\":51763},{\"end\":51785,\"start\":51784},{\"end\":51806,\"start\":51805},{\"end\":51821,\"start\":51816},{\"end\":52113,\"start\":52108},{\"end\":52126,\"start\":52120},{\"end\":52139,\"start\":52132},{\"end\":52148,\"start\":52145},{\"end\":52162,\"start\":52153},{\"end\":52173,\"start\":52168},{\"end\":52512,\"start\":52507},{\"end\":52525,\"start\":52519},{\"end\":52538,\"start\":52531},{\"end\":52547,\"start\":52544},{\"end\":52560,\"start\":52552},{\"end\":52570,\"start\":52565},{\"end\":52895,\"start\":52888},{\"end\":52913,\"start\":52904},{\"end\":53254,\"start\":53247},{\"end\":53272,\"start\":53263},{\"end\":53292,\"start\":53288},{\"end\":53306,\"start\":53302},{\"end\":53618,\"start\":53611},{\"end\":53634,\"start\":53627},{\"end\":53649,\"start\":53643},{\"end\":53658,\"start\":53650},{\"end\":53673,\"start\":53666},{\"end\":53689,\"start\":53684},{\"end\":53703,\"start\":53697},{\"end\":54030,\"start\":54024},{\"end\":54045,\"start\":54039},{\"end\":54061,\"start\":54055},{\"end\":54075,\"start\":54071},{\"end\":54411,\"start\":54406},{\"end\":54420,\"start\":54418},{\"end\":54732,\"start\":54723},{\"end\":54750,\"start\":54744},{\"end\":54764,\"start\":54760},{\"end\":55069,\"start\":55066},{\"end\":55082,\"start\":55076},{\"end\":55096,\"start\":55089},{\"end\":55444,\"start\":55439},{\"end\":55457,\"start\":55453},{\"end\":55473,\"start\":55466},{\"end\":55738,\"start\":55731},{\"end\":55752,\"start\":55746},{\"end\":55763,\"start\":55758},{\"end\":55779,\"start\":55772},{\"end\":55792,\"start\":55785},{\"end\":56214,\"start\":56210},{\"end\":56224,\"start\":56221},{\"end\":56236,\"start\":56231},{\"end\":56566,\"start\":56559},{\"end\":56579,\"start\":56574},{\"end\":56589,\"start\":56586},{\"end\":56603,\"start\":56595},{\"end\":56614,\"start\":56608},{\"end\":56629,\"start\":56621},{\"end\":57027,\"start\":57024},{\"end\":57039,\"start\":57035},{\"end\":57051,\"start\":57046},{\"end\":57381,\"start\":57375},{\"end\":57394,\"start\":57389},{\"end\":57408,\"start\":57401},{\"end\":57418,\"start\":57415},{\"end\":57775,\"start\":57773},{\"end\":57790,\"start\":57782},{\"end\":58081,\"start\":58074}]", "bib_author_last_name": "[{\"end\":43307,\"start\":43303},{\"end\":43324,\"start\":43317},{\"end\":43342,\"start\":43336},{\"end\":43358,\"start\":43351},{\"end\":43373,\"start\":43369},{\"end\":43392,\"start\":43386},{\"end\":43728,\"start\":43723},{\"end\":43744,\"start\":43737},{\"end\":43759,\"start\":43752},{\"end\":43770,\"start\":43767},{\"end\":44129,\"start\":44127},{\"end\":44143,\"start\":44136},{\"end\":44388,\"start\":44380},{\"end\":44408,\"start\":44399},{\"end\":44424,\"start\":44420},{\"end\":44766,\"start\":44762},{\"end\":44777,\"start\":44774},{\"end\":44793,\"start\":44788},{\"end\":44801,\"start\":44799},{\"end\":45130,\"start\":45126},{\"end\":45143,\"start\":45139},{\"end\":45160,\"start\":45155},{\"end\":45539,\"start\":45536},{\"end\":45547,\"start\":45543},{\"end\":45559,\"start\":45554},{\"end\":45569,\"start\":45561},{\"end\":45908,\"start\":45897},{\"end\":45923,\"start\":45917},{\"end\":45940,\"start\":45933},{\"end\":46265,\"start\":46256},{\"end\":46276,\"start\":46271},{\"end\":46289,\"start\":46283},{\"end\":46498,\"start\":46490},{\"end\":46512,\"start\":46506},{\"end\":46530,\"start\":46523},{\"end\":46542,\"start\":46537},{\"end\":46557,\"start\":46552},{\"end\":46860,\"start\":46856},{\"end\":47164,\"start\":47148},{\"end\":47179,\"start\":47171},{\"end\":47197,\"start\":47188},{\"end\":47214,\"start\":47209},{\"end\":47570,\"start\":47568},{\"end\":47586,\"start\":47579},{\"end\":47846,\"start\":47844},{\"end\":47857,\"start\":47853},{\"end\":47872,\"start\":47867},{\"end\":47885,\"start\":47882},{\"end\":47893,\"start\":47891},{\"end\":47908,\"start\":47904},{\"end\":48151,\"start\":48145},{\"end\":48166,\"start\":48159},{\"end\":48177,\"start\":48173},{\"end\":48442,\"start\":48437},{\"end\":48455,\"start\":48451},{\"end\":48464,\"start\":48461},{\"end\":48478,\"start\":48475},{\"end\":48494,\"start\":48486},{\"end\":48510,\"start\":48504},{\"end\":48824,\"start\":48816},{\"end\":48841,\"start\":48833},{\"end\":48857,\"start\":48851},{\"end\":48871,\"start\":48866},{\"end\":48889,\"start\":48879},{\"end\":48901,\"start\":48894},{\"end\":49269,\"start\":49260},{\"end\":49517,\"start\":49514},{\"end\":49756,\"start\":49753},{\"end\":49774,\"start\":49770},{\"end\":50031,\"start\":50026},{\"end\":50044,\"start\":50040},{\"end\":50060,\"start\":50052},{\"end\":50290,\"start\":50280},{\"end\":50306,\"start\":50297},{\"end\":50325,\"start\":50319},{\"end\":50651,\"start\":50649},{\"end\":50660,\"start\":50657},{\"end\":50675,\"start\":50671},{\"end\":50686,\"start\":50683},{\"end\":50696,\"start\":50688},{\"end\":51033,\"start\":51028},{\"end\":51048,\"start\":51045},{\"end\":51064,\"start\":51058},{\"end\":51078,\"start\":51073},{\"end\":51087,\"start\":51080},{\"end\":51096,\"start\":51091},{\"end\":51108,\"start\":51106},{\"end\":51121,\"start\":51116},{\"end\":51132,\"start\":51129},{\"end\":51138,\"start\":51134},{\"end\":51500,\"start\":51493},{\"end\":51517,\"start\":51509},{\"end\":51531,\"start\":51525},{\"end\":51545,\"start\":51537},{\"end\":51564,\"start\":51555},{\"end\":51782,\"start\":51773},{\"end\":51794,\"start\":51786},{\"end\":51803,\"start\":51796},{\"end\":51814,\"start\":51807},{\"end\":51831,\"start\":51822},{\"end\":51839,\"start\":51833},{\"end\":52118,\"start\":52114},{\"end\":52130,\"start\":52127},{\"end\":52143,\"start\":52140},{\"end\":52151,\"start\":52149},{\"end\":52166,\"start\":52163},{\"end\":52179,\"start\":52174},{\"end\":52517,\"start\":52513},{\"end\":52529,\"start\":52526},{\"end\":52542,\"start\":52539},{\"end\":52550,\"start\":52548},{\"end\":52563,\"start\":52561},{\"end\":52576,\"start\":52571},{\"end\":52902,\"start\":52896},{\"end\":52927,\"start\":52914},{\"end\":53261,\"start\":53255},{\"end\":53286,\"start\":53273},{\"end\":53300,\"start\":53293},{\"end\":53321,\"start\":53307},{\"end\":53625,\"start\":53619},{\"end\":53641,\"start\":53635},{\"end\":53664,\"start\":53659},{\"end\":53682,\"start\":53674},{\"end\":53695,\"start\":53690},{\"end\":53710,\"start\":53704},{\"end\":54037,\"start\":54031},{\"end\":54053,\"start\":54046},{\"end\":54069,\"start\":54062},{\"end\":54081,\"start\":54076},{\"end\":54416,\"start\":54412},{\"end\":54425,\"start\":54421},{\"end\":54742,\"start\":54733},{\"end\":54758,\"start\":54751},{\"end\":54773,\"start\":54765},{\"end\":55074,\"start\":55070},{\"end\":55087,\"start\":55083},{\"end\":55102,\"start\":55097},{\"end\":55451,\"start\":55445},{\"end\":55464,\"start\":55458},{\"end\":55481,\"start\":55474},{\"end\":55744,\"start\":55739},{\"end\":55756,\"start\":55753},{\"end\":55770,\"start\":55764},{\"end\":55783,\"start\":55780},{\"end\":55798,\"start\":55793},{\"end\":56219,\"start\":56215},{\"end\":56229,\"start\":56225},{\"end\":56240,\"start\":56237},{\"end\":56572,\"start\":56567},{\"end\":56584,\"start\":56580},{\"end\":56593,\"start\":56590},{\"end\":56606,\"start\":56604},{\"end\":56619,\"start\":56615},{\"end\":56634,\"start\":56630},{\"end\":57033,\"start\":57028},{\"end\":57044,\"start\":57040},{\"end\":57056,\"start\":57052},{\"end\":57387,\"start\":57382},{\"end\":57399,\"start\":57395},{\"end\":57413,\"start\":57409},{\"end\":57421,\"start\":57419},{\"end\":57780,\"start\":57776},{\"end\":57794,\"start\":57791},{\"end\":58085,\"start\":58082}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1804.03235\",\"id\":\"b0\"},\"end\":43649,\"start\":43220},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2998607},\"end\":44082,\"start\":43651},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":11536917},\"end\":44331,\"start\":44084},{\"attributes\":{\"id\":\"b3\"},\"end\":44583,\"start\":44333},{\"attributes\":{\"id\":\"b4\"},\"end\":44686,\"start\":44585},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":8400616},\"end\":45117,\"start\":44688},{\"attributes\":{\"doi\":\"arXiv:1707.01220\",\"id\":\"b6\"},\"end\":45452,\"start\":45119},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1480192},\"end\":45848,\"start\":45454},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2266226},\"end\":46199,\"start\":45850},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":207240067},\"end\":46480,\"start\":46201},{\"attributes\":{\"doi\":\"arXiv:1704.08803\",\"id\":\"b10\"},\"end\":46805,\"start\":46482},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1198417},\"end\":47070,\"start\":46807},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2652645},\"end\":47476,\"start\":47072},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":9124261},\"end\":47801,\"start\":47478},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":13907106},\"end\":48134,\"start\":47803},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b15\"},\"end\":48393,\"start\":48136},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":11129957},\"end\":48738,\"start\":48395},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":206592218},\"end\":49211,\"start\":48740},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1605269},\"end\":49448,\"start\":49213},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":9672033},\"end\":49746,\"start\":49450},{\"attributes\":{\"doi\":\"arXiv:1606.07947\",\"id\":\"b20\"},\"end\":49960,\"start\":49748},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":58370896},\"end\":50208,\"start\":49962},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":195908774},\"end\":50571,\"start\":50210},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3514119},\"end\":50947,\"start\":50573},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6206080},\"end\":51438,\"start\":50949},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":17048224},\"end\":51733,\"start\":51440},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":423350},\"end\":52070,\"start\":51735},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3993933},\"end\":52423,\"start\":52072},{\"attributes\":{\"doi\":\"arXiv:1710.05649\",\"id\":\"b28\"},\"end\":52810,\"start\":52425},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":8750954},\"end\":53186,\"start\":52812},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":10795036},\"end\":53609,\"start\":53188},{\"attributes\":{\"doi\":\"arXiv:1412.6550\",\"id\":\"b31\"},\"end\":53960,\"start\":53611},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":8047550},\"end\":54321,\"start\":53962},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":39847715},\"end\":54662,\"start\":54323},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":17999283},\"end\":55011,\"start\":54664},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":4833213},\"end\":55356,\"start\":55013},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":7587705},\"end\":55675,\"start\":55358},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":5878197},\"end\":56120,\"start\":55677},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":17476857},\"end\":56523,\"start\":56122},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":13124023},\"end\":56935,\"start\":56525},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":315444},\"end\":57313,\"start\":56937},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":7990142},\"end\":57720,\"start\":57315},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":16572331},\"end\":58028,\"start\":57722},{\"attributes\":{\"id\":\"b43\"},\"end\":58157,\"start\":58030}]", "bib_title": "[{\"end\":43716,\"start\":43651},{\"end\":44119,\"start\":44084},{\"end\":44757,\"start\":44688},{\"end\":45526,\"start\":45454},{\"end\":45890,\"start\":45850},{\"end\":46249,\"start\":46201},{\"end\":46845,\"start\":46807},{\"end\":47138,\"start\":47072},{\"end\":47558,\"start\":47478},{\"end\":47833,\"start\":47803},{\"end\":48424,\"start\":48395},{\"end\":48807,\"start\":48740},{\"end\":49252,\"start\":49213},{\"end\":49507,\"start\":49450},{\"end\":50017,\"start\":49962},{\"end\":50273,\"start\":50210},{\"end\":50643,\"start\":50573},{\"end\":51024,\"start\":50949},{\"end\":51485,\"start\":51440},{\"end\":51761,\"start\":51735},{\"end\":52106,\"start\":52072},{\"end\":52886,\"start\":52812},{\"end\":53245,\"start\":53188},{\"end\":54022,\"start\":53962},{\"end\":54404,\"start\":54323},{\"end\":54721,\"start\":54664},{\"end\":55064,\"start\":55013},{\"end\":55437,\"start\":55358},{\"end\":55729,\"start\":55677},{\"end\":56208,\"start\":56122},{\"end\":56557,\"start\":56525},{\"end\":57022,\"start\":56937},{\"end\":57373,\"start\":57315},{\"end\":57771,\"start\":57722}]", "bib_author": "[{\"end\":43309,\"start\":43297},{\"end\":43326,\"start\":43309},{\"end\":43344,\"start\":43326},{\"end\":43360,\"start\":43344},{\"end\":43375,\"start\":43360},{\"end\":43394,\"start\":43375},{\"end\":43730,\"start\":43718},{\"end\":43746,\"start\":43730},{\"end\":43761,\"start\":43746},{\"end\":43772,\"start\":43761},{\"end\":44131,\"start\":44121},{\"end\":44145,\"start\":44131},{\"end\":44390,\"start\":44372},{\"end\":44410,\"start\":44390},{\"end\":44426,\"start\":44410},{\"end\":44768,\"start\":44759},{\"end\":44779,\"start\":44768},{\"end\":44795,\"start\":44779},{\"end\":44803,\"start\":44795},{\"end\":45132,\"start\":45119},{\"end\":45145,\"start\":45132},{\"end\":45162,\"start\":45145},{\"end\":45541,\"start\":45528},{\"end\":45549,\"start\":45541},{\"end\":45561,\"start\":45549},{\"end\":45571,\"start\":45561},{\"end\":45910,\"start\":45892},{\"end\":45925,\"start\":45910},{\"end\":45942,\"start\":45925},{\"end\":46267,\"start\":46251},{\"end\":46278,\"start\":46267},{\"end\":46291,\"start\":46278},{\"end\":46500,\"start\":46482},{\"end\":46514,\"start\":46500},{\"end\":46532,\"start\":46514},{\"end\":46544,\"start\":46532},{\"end\":46559,\"start\":46544},{\"end\":46862,\"start\":46847},{\"end\":47166,\"start\":47140},{\"end\":47181,\"start\":47166},{\"end\":47199,\"start\":47181},{\"end\":47216,\"start\":47199},{\"end\":47572,\"start\":47560},{\"end\":47588,\"start\":47572},{\"end\":47848,\"start\":47835},{\"end\":47859,\"start\":47848},{\"end\":47874,\"start\":47859},{\"end\":47887,\"start\":47874},{\"end\":47895,\"start\":47887},{\"end\":47910,\"start\":47895},{\"end\":48153,\"start\":48136},{\"end\":48168,\"start\":48153},{\"end\":48179,\"start\":48168},{\"end\":48444,\"start\":48426},{\"end\":48457,\"start\":48444},{\"end\":48466,\"start\":48457},{\"end\":48480,\"start\":48466},{\"end\":48496,\"start\":48480},{\"end\":48512,\"start\":48496},{\"end\":48826,\"start\":48809},{\"end\":48843,\"start\":48826},{\"end\":48859,\"start\":48843},{\"end\":48873,\"start\":48859},{\"end\":48891,\"start\":48873},{\"end\":48903,\"start\":48891},{\"end\":49271,\"start\":49254},{\"end\":49519,\"start\":49509},{\"end\":49758,\"start\":49748},{\"end\":49776,\"start\":49758},{\"end\":50033,\"start\":50019},{\"end\":50046,\"start\":50033},{\"end\":50062,\"start\":50046},{\"end\":50292,\"start\":50275},{\"end\":50308,\"start\":50292},{\"end\":50327,\"start\":50308},{\"end\":50653,\"start\":50645},{\"end\":50662,\"start\":50653},{\"end\":50677,\"start\":50662},{\"end\":50688,\"start\":50677},{\"end\":50698,\"start\":50688},{\"end\":51035,\"start\":51026},{\"end\":51050,\"start\":51035},{\"end\":51066,\"start\":51050},{\"end\":51080,\"start\":51066},{\"end\":51089,\"start\":51080},{\"end\":51098,\"start\":51089},{\"end\":51110,\"start\":51098},{\"end\":51123,\"start\":51110},{\"end\":51134,\"start\":51123},{\"end\":51140,\"start\":51134},{\"end\":51502,\"start\":51487},{\"end\":51519,\"start\":51502},{\"end\":51533,\"start\":51519},{\"end\":51547,\"start\":51533},{\"end\":51566,\"start\":51547},{\"end\":51784,\"start\":51763},{\"end\":51796,\"start\":51784},{\"end\":51805,\"start\":51796},{\"end\":51816,\"start\":51805},{\"end\":51833,\"start\":51816},{\"end\":51841,\"start\":51833},{\"end\":52120,\"start\":52108},{\"end\":52132,\"start\":52120},{\"end\":52145,\"start\":52132},{\"end\":52153,\"start\":52145},{\"end\":52168,\"start\":52153},{\"end\":52181,\"start\":52168},{\"end\":52519,\"start\":52507},{\"end\":52531,\"start\":52519},{\"end\":52544,\"start\":52531},{\"end\":52552,\"start\":52544},{\"end\":52565,\"start\":52552},{\"end\":52578,\"start\":52565},{\"end\":52904,\"start\":52888},{\"end\":52929,\"start\":52904},{\"end\":53263,\"start\":53247},{\"end\":53288,\"start\":53263},{\"end\":53302,\"start\":53288},{\"end\":53323,\"start\":53302},{\"end\":53627,\"start\":53611},{\"end\":53643,\"start\":53627},{\"end\":53666,\"start\":53643},{\"end\":53684,\"start\":53666},{\"end\":53697,\"start\":53684},{\"end\":53712,\"start\":53697},{\"end\":54039,\"start\":54024},{\"end\":54055,\"start\":54039},{\"end\":54071,\"start\":54055},{\"end\":54083,\"start\":54071},{\"end\":54418,\"start\":54406},{\"end\":54427,\"start\":54418},{\"end\":54744,\"start\":54723},{\"end\":54760,\"start\":54744},{\"end\":54775,\"start\":54760},{\"end\":55076,\"start\":55066},{\"end\":55089,\"start\":55076},{\"end\":55104,\"start\":55089},{\"end\":55453,\"start\":55439},{\"end\":55466,\"start\":55453},{\"end\":55483,\"start\":55466},{\"end\":55746,\"start\":55731},{\"end\":55758,\"start\":55746},{\"end\":55772,\"start\":55758},{\"end\":55785,\"start\":55772},{\"end\":55800,\"start\":55785},{\"end\":56221,\"start\":56210},{\"end\":56231,\"start\":56221},{\"end\":56242,\"start\":56231},{\"end\":56574,\"start\":56559},{\"end\":56586,\"start\":56574},{\"end\":56595,\"start\":56586},{\"end\":56608,\"start\":56595},{\"end\":56621,\"start\":56608},{\"end\":56636,\"start\":56621},{\"end\":57035,\"start\":57024},{\"end\":57046,\"start\":57035},{\"end\":57058,\"start\":57046},{\"end\":57389,\"start\":57375},{\"end\":57401,\"start\":57389},{\"end\":57415,\"start\":57401},{\"end\":57423,\"start\":57415},{\"end\":57782,\"start\":57773},{\"end\":57796,\"start\":57782},{\"end\":58087,\"start\":58074}]", "bib_venue": "[{\"end\":43295,\"start\":43220},{\"end\":43849,\"start\":43772},{\"end\":44194,\"start\":44145},{\"end\":44370,\"start\":44333},{\"end\":44623,\"start\":44587},{\"end\":44890,\"start\":44803},{\"end\":45260,\"start\":45178},{\"end\":45634,\"start\":45571},{\"end\":45980,\"start\":45942},{\"end\":46328,\"start\":46291},{\"end\":46618,\"start\":46575},{\"end\":46925,\"start\":46862},{\"end\":47263,\"start\":47216},{\"end\":47627,\"start\":47588},{\"end\":47952,\"start\":47910},{\"end\":48239,\"start\":48195},{\"end\":48554,\"start\":48512},{\"end\":48961,\"start\":48903},{\"end\":49320,\"start\":49271},{\"end\":49586,\"start\":49519},{\"end\":49829,\"start\":49792},{\"end\":50070,\"start\":50062},{\"end\":50376,\"start\":50327},{\"end\":50744,\"start\":50698},{\"end\":51182,\"start\":51140},{\"end\":51577,\"start\":51566},{\"end\":51890,\"start\":51841},{\"end\":52223,\"start\":52181},{\"end\":52505,\"start\":52425},{\"end\":52983,\"start\":52929},{\"end\":53375,\"start\":53323},{\"end\":53760,\"start\":53727},{\"end\":54125,\"start\":54083},{\"end\":54485,\"start\":54427},{\"end\":54821,\"start\":54775},{\"end\":55167,\"start\":55104},{\"end\":55499,\"start\":55483},{\"end\":55887,\"start\":55800},{\"end\":56306,\"start\":56242},{\"end\":56713,\"start\":56636},{\"end\":57100,\"start\":57058},{\"end\":57500,\"start\":57423},{\"end\":57859,\"start\":57796},{\"end\":58072,\"start\":58030}]"}}}, "year": 2023, "month": 12, "day": 17}