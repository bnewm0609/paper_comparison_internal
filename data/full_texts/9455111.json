{"id": 9455111, "updated": "2023-04-05 09:45:56.589", "metadata": {"title": "Vision meets robotics: The KITTI dataset", "authors": "[{\"first\":\"A\",\"last\":\"Geiger\",\"middle\":[]},{\"first\":\"P\",\"last\":\"Lenz\",\"middle\":[]},{\"first\":\"C\",\"last\":\"Stiller\",\"middle\":[]},{\"first\":\"R\",\"last\":\"Urtasun\",\"middle\":[]}]", "venue": null, "journal": "The International Journal of Robotics Research", "publication_date": {"year": 2013, "month": null, "day": null}, "abstract": "We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10\u2013100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2115579991", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/ijrr/GeigerLSU13", "doi": "10.1177/0278364913491297"}}, "content": {"source": {"pdf_hash": "026e3363b7f76b51cc711886597a44d5f1fd1de2", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://ttic.uchicago.edu/~rurtasun/publications/geiger_et_al_ijrr13.pdf", "status": "GREEN"}}, "grobid": {"id": "ae0b0f3df160336a81eed499c5fee8a47645d954", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/026e3363b7f76b51cc711886597a44d5f1fd1de2.txt", "contents": "\nVision meets Robotics: The KITTI Dataset\n\n\nAndreas Geiger \nPhilip Lenz \nChristoph Stiller \nRaquel Urtasun \nVision meets Robotics: The KITTI Dataset\n1Index Terms-datasetautonomous drivingmobile roboticsfield roboticscomputer visioncameraslaserGPSbenchmarksstereooptical flowSLAMobject detectiontrackingKITTI\nWe present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as highresolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations and range from freeways over rural areas to innercity scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide.\n\nI. INTRODUCTION\n\nThe KITTI dataset has been recorded from a moving platform ( Fig. 1) while driving in and around Karlsruhe, Germany (Fig. 2). It includes camera images, laser scans, high-precision GPS measurements and IMU accelerations from a combined GPS/IMU system. The main purpose of this dataset is to push forward the development of computer vision and robotic algorithms targeted to autonomous driving [1]- [7]. While our introductory paper [8] mainly focuses on the benchmarks, their creation and use for evaluating state-of-the-art computer vision methods, here we complement this information by providing technical details on the raw data itself. We give precise instructions on how to access the data and comment on sensor limitations and common pitfalls. The dataset can be downloaded from http://www.cvlibs.net/datasets/kitti. For a review on related work, we refer the reader to [8].\n\n\nII. SENSOR SETUP\n\nOur sensor setup is illustrated in Fig. 3:  Our VW Passat station wagon is equipped with four video cameras (two color and two grayscale cameras), a rotating 3D laser scanner and a combined GPS/IMU inertial navigation system. \u2022 1 \u00d7 OXTS RT3003 inertial and GPS navigation system, 6 axis, 100 Hz, L1/L2 RTK, resolution: 0.02m / 0.1 \u2022 Note that the color cameras lack in terms of resolution due to the Bayer pattern interpolation process and are less sensitive to light. This is the reason why we use two stereo camera rigs, one for grayscale and one for color. The baseline of both stereo camera rigs is approximately 54 cm. The trunk of our vehicle houses a PC with two six-core Intel XEON X5650 processors and a shock-absorbed RAID 5 hard disk storage with a capacity of 4 Terabytes. Our computer runs Ubuntu Linux (64 bit) and a real-time database [9] to store the incoming data streams.\n\u2022 2 \u00d7\n\nIII. DATASET\n\nThe raw data described in this paper can be accessed from http://www.cvlibs.net/datasets/kitti and contains \u223c 25% of our overall recordings. The reason for this is that primarily data with 3D tracklet annotations has been put online, though we will make more data available upon request. Furthermore, we have removed all sequences which are part of our benchmark test sets. The raw data set is divided into the categories 'Road', 'City', 'Residential', 'Campus' and 'Person'. Example frames are illustrated in Fig. 5. For each sequence, we provide the raw data, object annotations in form of 3D bounding box tracklets and a calibration file, as illustrated in Fig. 4. Our recordings have taken place on the 26th, 28th, 29th, 30th of September and on the 3rd of October 2011 during daytime. The total size of the provided data is 180 GB. Recording Zone. This figure shows the GPS traces of our recordings in the metropolitan area of Karlsruhe, Germany. Colors encode the GPS signal quality: Red tracks have been recorded with highest precision using RTK corrections, blue denotes the absence of correction signals. The black runs have been excluded from our data set as no GPS signal has been available.\n\n\nA. Data Description\n\nAll sensor readings of a sequence are zipped into a single file named date_drive.zip, where date and drive are placeholders for the recording date and the sequence number. The directory structure is shown in Fig. 4. Besides the raw recordings ('raw data'), we also provide post-processed data ('synced data'), i.e., rectified and synchronized video streams, on the dataset website.\n\nTimestamps are stored in timestamps.txt and perframe sensor readings are provided in the corresponding data sub-folders. Each line in timestamps.txt is composed of the date and time in hours, minutes and seconds. As the Velodyne laser scanner has a 'rolling shutter', three timestamp files are provided for this sensor, one for the start position (timestamps_start.txt) of a spin, one for the end position (timestamps_end.txt) of a spin, and one for the time, where the laser scanner is facing forward and triggering the cameras (timestamps.txt). The data format in which each sensor stream is stored is as follows: a) Images: Both, color and grayscale images are stored with loss-less compression using 8-bit PNG files. The engine hood and the sky region have been cropped. To simplify working with the data, we also provide rectified images. The size of the images after rectification depends on the calibration parameters and is \u223c 0.5 Mpx on average. The original images before rectification are available as well.\n\nb) OXTS (GPS/IMU): For each frame, we store 30 different GPS/IMU values in a text file: The geographic coordinates including altitude, global orientation, velocities, accelerations, angular rates, accuracies and satellite information. Accelera-  tions and angular rates are both specified using two coordinate systems, one which is attached to the vehicle body (x, y, z) and one that is mapped to the tangent plane of the earth surface at that location (f, l, u). From time to time we encountered short (\u223c 1 second) communication outages with the OXTS device for which we interpolated all values linearly and set the last 3 entries to '-1' to indicate the missing information. More details are provided in dataformat.txt. Conversion utilities are provided in the development kit. c) Velodyne: For efficiency, the Velodyne scans are stored as floating point binaries that are easy to parse using the C++ or MATLAB code provided. Each point is stored with its (x, y, z) coordinate and an additional reflectance value (r). While the number of points per scan is not constant, on average each file/frame has a size of \u223c 1.9 MB which corresponds to \u223c 120, 000 3D points and reflectance values. Note that the Velodyne laser scanner rotates continuously around its vertical axis (counter-clockwise), which can be taken into account using the timestamp files.\n\n\nB. Annotations\n\nFor each dynamic object within the reference camera's field of view, we provide annotations in the form of 3D bounding box tracklets, represented in Velodyne coordinates. We define the classes 'Car', 'Van', 'Truck', 'Pedestrian', 'Person (sitting)', 'Cyclist', 'Tram' and 'Misc' (e.g., Trailers, Segways). The tracklets are stored in date_drive_tracklets.xml. Each object is assigned a class and its 3D size (height, width, length). For each frame, we provide the object's translation and rotation in 3D, as illustrated in Fig. 7. Note that we only provide the yaw angle, while the other two angles are assumed to be close to zero. Furthermore, the level of occlusion and truncation is specified. The development kit contains C++/MATLAB code for reading and writing tracklets using the boost::serialization 1 library.\n\nTo give further insights into the properties of our dataset, we provide statistics for all sequences that contain annotated objects. The total number of objects and the object orientations for the two predominant classes 'Car' and 'Pedestrian' are shown in Fig. 8. For each object class, the number of object labels per image and the length of the captured sequences is shown in Fig. 9. The egomotion of our platform recorded by the GPS/IMU system as well as statistics about the sequence length and the number of objects are shown in Fig. 10 for the whole dataset and in Fig. 11 per street category.\n\n\nC. Development Kit\n\nThe raw data development kit provided on the KITTI website 2 contains MATLAB demonstration code with C++ wrappers and a readme.txt file which gives further details. Here, we will briefly discuss the most important features. Before running the scripts, the mex wrapper readTrackletsMex.cpp for reading tracklets into MAT-LAB structures and cell arrays needs to be built using the script make.m. It wraps the file tracklets.h from the Fig. 7. Object Coordinates. This figure illustrates the coordinate system of the annotated 3D bounding boxes with respect to the coordinate system of the 3D Velodyne laser scanner. In z-direction, the object coordinate system is located at the bottom of the object (contact point with the supporting surface).\n\ncpp folder which holds the tracklet object for serialization. This file can also be directly interfaced with when working in a C++ environment.\n\nThe script run_demoTracklets.m demonstrates how 3D bounding box tracklets can be read from the XML files and projected onto the image plane of the cameras. The projection of 3D Velodyne point clouds into the image plane is demonstrated in run_demoVelodyne.m. See Fig. 6 for an illustration.\n\nThe script run_demoVehiclePath.m shows how to read and display the 3D vehicle trajectory using the GPS/IMU data. It makes use of convertOxtsToPose(), which takes as input GPS/IMU measurements and outputs the 6D pose of the vehicle in Euclidean space. For this conversion we make use of the Mercator projection [10] x = s \u00d7 r \u00d7 \u03c0 lon 180 (1)\ny = s \u00d7 r \u00d7 log tan \u03c0(90 + lat) 360(2)\nwith earth radius r \u2248 6378137 meters, scale s = cos lat0\u00d7\u03c0 180 , and (lat, lon) the geographic coordinates. lat 0 denotes the latitude of the first frame's coordinates and uniquely determines the Mercator scale.\n\nThe function loadCalibrationCamToCam() can be used to read the intrinsic and extrinsic calibration parameters of the four video sensors. The other 3D rigid body transformations can be parsed with loadCalibrationRigid().\n\n\nD. Benchmarks\n\nIn addition to the raw data, our KITTI website hosts evaluation benchmarks for several computer vision and robotic tasks such as stereo, optical flow, visual odometry, SLAM, 3D object detection and 3D object tracking. For details about the benchmarks and evaluation metrics we refer the reader to [8].\n\n\nIV. SENSOR CALIBRATION\n\nWe took care that all sensors are carefully synchronized and calibrated. To avoid drift over time, we calibrated the sensors every day after our recordings. Note that even though the sensor setup hasn't been altered in between, numerical differences are possible. The coordinate systems are defined as illustrated in Fig. 1 and Fig. 3, i.e.:\n\u2022 Camera: x = right, y = down, z = forward \u2022 Velodyne: x = forward, y = left, z = up City\nResidential Road Campus Person \u2022 GPS/IMU: x = forward, y = left, z = up Notation: In the following, we write scalars in lower-case letters (a), vectors in bold lower-case (a) and matrices using bold-face capitals (A). 3D rigid-body transformations which take points from coordinate system a to coordinate system b will be denoted by T b a , with T for 'transformation'.\n\n\nA. Synchronization\n\nIn order to synchronize the sensors, we use the timestamps of the Velodyne 3D laser scanner as a reference and consider each spin as a frame. We mounted a reed contact at the bottom of the continuously rotating scanner, triggering the cameras when facing forward. This minimizes the differences in the range and image observations caused by dynamic objects. Unfortunately, the GPS/IMU system cannot be synchronized that way. Instead, as it provides updates at 100 Hz, we collect the information with the closest timestamp to the laser scanner timestamp for a particular frame, resulting in a worstcase time difference of 5 ms between a GPS/IMU and a camera/Velodyne data package. Note that all timestamps are provided such that positioning information at any time can be easily obtained via interpolation. All timestamps have been recorded on our host computer using the system clock.\n\n\nB. Camera Calibration\n\nFor calibrating the cameras intrinsically and extrinsically, we use the approach proposed in [11]. Note that all camera centers are aligned, i.e., they lie on the same x/y-plane. This is important as it allows us to rectify all images jointly.\n\nThe calibration parameters for each day are stored in row-major order in calib_cam_to_cam.txt using the following notation: Here, i \u2208 {0, 1, 2, 3} is the camera index, where 0 represents the left grayscale, 1 the right grayscale, 2 the left color and 3 the right color camera. Note that the variable definitions are compliant with the OpenCV library, which we used for warping the images. When working with the synchronized and rectified datasets only the variables with rect-subscript are relevant. Note that due to the pincushion distortion effect the images have been cropped such that the size of the rectified images is smaller than the original size of 1392 \u00d7 512 pixels.\n\nThe projection of a 3D point x = (x, y, z, 1) T in rectified (rotated) camera coordinates to a point y = (u, v, 1) T in the i'th camera image is given as\ny = P (i) rect x (3) with P (i) rect = \uf8eb \uf8ec \uf8ed f (i) u 0 c (i) u \u2212f (i) u b (i) x 0 f (i) v c (i) v 0 0 0 1 0 \uf8f6 \uf8f7 \uf8f8 (4) the i'th projection matrix. Here, b (i)\nx denotes the baseline (in meters) with respect to reference camera 0. Note that in order to project a 3D point x in reference camera coordinates to a point y on the i'th image plane, the rectifying rotation matrix of the reference camera R (0) rect must be considered as well:\ny = P (i) rect R (0) rect x(5)\nHere, R\n\nrect has been expanded into a 4\u00d74 matrix by appending a fourth zero-row and column, and setting R \n\n\nC. Velodyne and IMU Calibration\n\nWe have registered the Velodyne laser scanner with respect to the reference camera coordinate system (camera 0) by initializing the rigid body transformation using [11]. Next, we optimized an error criterion based on the Euclidean distance of 50 manually selected correspondences and a robust measure on the disparity error with respect to the 3 top performing stereo methods in the KITTI stereo benchmark [8]. The optimization was carried out using Metropolis-Hastings sampling.\n\nThe rigid body transformation from Velodyne coordinates to camera coordinates is given in calib_velo_to_cam.txt: a 3D point x in Velodyne coordinates gets projected to a point y in the i'th camera image as\n\u2022 R cam velo \u2208 R 3\u00d73 . . . .y = P (i) rect R (0) rect T cam velo x(7)\nFor registering the IMU/GPS with respect to the Velodyne laser scanner, we first recorded a sequence with an '\u221e'-loop and registered the (untwisted) point clouds using the Pointto-Plane ICP algorithm. Given two trajectories this problem corresponds to the well-known hand-eye calibration problem which can be solved using standard tools [12]. The rotation matrix R velo imu and the translation vector t velo imu are stored in calib_imu_to_velo.txt. A 3D point x in IMU/GPS coordinates gets projected to a point y in the i'th image as\ny = P (i) rect R (0) rect T cam velo T velo imu x(8)\n\nV. SUMMARY AND FUTURE WORK\n\nIn this paper, we have presented a calibrated, synchronized and rectified autonomous driving dataset capturing a wide range of interesting scenarios. We believe that this dataset will be highly useful in many areas of robotics and computer vision. In the future we plan on expanding the set of available sequences by adding additional 3D object labels for currently unlabeled sequences and recording new sequences, for example in difficult lighting situations such as at night, in tunnels, or in the presence of fog or rain. Furthermore, we plan on extending our benchmark suite by novel challenges. In particular, we will provide pixel-accurate semantic labels for many of the sequences.  Fig. 10. Egomotion, Sequence Count and Length. This figure show (from-left-to-right) the egomotion (velocity and acceleration) of our recording platform for the whole dataset. Note that we excluded sequences with a purely static observer from these statistics. The length of the available sequences is shown as a histogram counting the number of frames per sequence. The rightmost figure shows the number of frames/images per scene category.  \n\n\nCity\n\nResidential Road Campus Person Fig. 11. Velocities, Accelerations and Number of Objects per Object Class. For each scene category we show the acceleration and velocity of the mobile platform as well as the number of labels and objects per class. Note that sequences with a purely static observer have been excluded from the velocity and acceleration histograms as they don't represent natural driving behavior. The category 'Person' has been recorded from a static observer.\n\nFig. 1 .\n1Recording Platform.\n\n\nFig. 2. Recording Zone. This figure shows the GPS traces of our recordings in the metropolitan area of Karlsruhe, Germany. Colors encode the GPS signal quality: Red tracks have been recorded with highest precision using RTK corrections, blue denotes the absence of correction signals. The black runs have been excluded from our data set as no GPS signal has been available.\n\nFig. 3 .\n3Sensor Setup. This figure illustrates the dimensions and mounting positions of the sensors (red) with respect to the vehicle body. Heights above ground are marked in green and measured with respect to the road surface. Transformations between sensors are shown in blue.\n\nFig. 4 .\n4Structure of the provided Zip-Files and their location within a global file structure that stores all KITTI sequences. Here, 'date' and 'drive' are placeholders, and 'image 0x' refers to the 4 video camera streams.\n\n3 Fig. 6 .\n36Development kit. Working with tracklets (top), Velodyne point clouds (bottom) and their projections onto the image plane is demonstrated in the MATLAB development kit which is available from the KITTI website.\n\nFig. 5 .\n5Examples from the KITTI dataset. This figure demonstrates the diversity in our dataset. The left color camera image is shown.\n\n\n\u2022 s (i) \u2208 N 2 . . . . . . . . . . . . original image size (1392 \u00d7 512) \u2022 K (i) \u2208 R 3\u00d73 . . . . . . . . . calibration matrices (unrectified) \u2022 d (i) \u2208 R 5 . . . . . . . . . . distortion coefficients (unrectified) \u2022 R (i) \u2208 R 3\u00d73 . . . . . . rotation from camera 0 to camera i \u2022 t (i) \u2208 R 1\u00d73 . . . . . translation from camera 0 to camera i \u2022 s (i) rect \u2208 N 2 . . . . . . . . . . . . . . . image size after rectification \u2022 R (i) rect \u2208 R 3\u00d73 . . . . . . . . . . . . . . . rectifying rotation matrix \u2022 P (i) rect \u2208 R 3\u00d74 . . . . . . projection matrix after rectification\n\nFig. 8 .\n8Object Occurrence and Orientation Statistics of our Dataset. This figure shows the different types of objects occurring in our sequences (top) and the orientation histograms (bottom) for the two most predominant categories 'Car' and 'Pedestrian'.\n\nFig. 9 .\n9Number of Object Labels per Class and Image. This figure shows how often an object occurs in an image. Since our labeling efforts focused on cars and pedestrians, these are the most predominant classes here.\n\n\nA. Geiger, P.Lenz  and C. Stiller are with the Department of Measurement and Control Systems, Karlsruhe Institute of Technology, Germany. Email: {geiger,lenz,stiller}@kit.edu R. Urtasun is with the Toyota Technological Institute at Chicago, USA. Email: rurtasun@ttic.eduPointGray Flea2 grayscale cameras (FL2-14S3M-C), \n1.4 Megapixels, 1/2\" Sony ICX267 CCD, global shutter \n\u2022 2 \u00d7 PointGray Flea2 color cameras (FL2-14S3C-C), 1.4 \nMegapixels, 1/2\" Sony ICX267 CCD, global shutter \n\u2022 4 \u00d7 Edmund Optics lenses, 4mm, opening angle \u223c 90 \u2022 , \nvertical opening angle of region of interest (ROI) \u223c 35 \u2022 \n\u2022 1 \u00d7 Velodyne HDL-64E rotating 3D laser scanner, 10 Hz, \n64 beams, 0.09 degree angular resolution, 2 cm distance \naccuracy, collecting \u223c 1.3 million points/second, field of \nview: 360 \u2022 horizontal, 26.8 \u2022 vertical, range: 120 m \n\n\n\n\nrotation matrix: velodyne \u2192 camera \u2022 t cam velo \u2208 R 1\u00d73 . . . translation vector: velodyne \u2192 camera UsingT cam \nvelo = \n\n\nR cam \n\nvelo \n\nt cam \n\nvelo \n\n0 \n1 \n\n\n(6) \n\n\nhttp://www.boost.org 2 http://www.cvlibs.net/datasets/kitti/raw data.php\n\nAcquiring semantics induced topology in urban environments. G Singh, J Kosecka, ICRA. G. Singh and J. Kosecka, \"Acquiring semantics induced topology in urban environments,\" in ICRA, 2012.\n\nFAB-MAP 3D: Topological mapping with spatial and visual appearance. R Paul, P Newman, ICRAR. Paul and P. Newman, \"FAB-MAP 3D: Topological mapping with spatial and visual appearance,\" in ICRA, 2010.\n\nMonocular visual scene understanding: Understanding multi-object traffic scenes. C Wojek, S Walk, S Roth, K Schindler, B Schiele, PAMIC. Wojek, S. Walk, S. Roth, K. Schindler, and B. Schiele, \"Monocular visual scene understanding: Understanding multi-object traffic scenes,\" PAMI, 2012.\n\nEfficient representation of traffic scenes by means of dynamic stixels. D Pfeiffer, U Franke, IVD. Pfeiffer and U. Franke, \"Efficient representation of traffic scenes by means of dynamic stixels,\" in IV, 2010.\n\nA generative model for 3d urban scene understanding from movable platforms. A Geiger, M Lauer, R Urtasun, CVPR. A. Geiger, M. Lauer, and R. Urtasun, \"A generative model for 3d urban scene understanding from movable platforms,\" in CVPR, 2011.\n\nJoint 3d estimation of objects and scene layout. A Geiger, C Wojek, R Urtasun, NIPS. A. Geiger, C. Wojek, and R. Urtasun, \"Joint 3d estimation of objects and scene layout,\" in NIPS, 2011.\n\nLost! leveraging the crowd for probabilistic visual self-localization. M A Brubaker, A Geiger, R Urtasun, CVPR. M. A. Brubaker, A. Geiger, and R. Urtasun, \"Lost! leveraging the crowd for probabilistic visual self-localization.\" in CVPR, 2013.\n\nAre we ready for autonomous driving? The KITTI vision benchmark suite. A Geiger, P Lenz, R Urtasun, CVPR. A. Geiger, P. Lenz, and R. Urtasun, \"Are we ready for autonomous driving? The KITTI vision benchmark suite,\" in CVPR, 2012.\n\nA real-time-capable hard-and software architecture for joint image and knowledge processing in cognitive automobiles. M Goebl, G Faerber, IVM. Goebl and G. Faerber, \"A real-time-capable hard-and software architecture for joint image and knowledge processing in cognitive automobiles,\" in IV, 2007.\n\nThe mercator projections. P Osborne, P. Osborne, \"The mercator projections,\" 2008. [Online]. Available: http://mercator.myzen.co.uk/mercator.pdf\n\nA toolbox for automatic calibration of range and camera sensors using a single shot. A Geiger, F Moosmann, O Car, B Schuster, ICRAA. Geiger, F. Moosmann, O. Car, and B. Schuster, \"A toolbox for automatic calibration of range and camera sensors using a single shot,\" in ICRA, 2012.\n\nHand-eye calibration. R Horaud, F Dornaika, IJRR. 143R. Horaud and F. Dornaika, \"Hand-eye calibration,\" IJRR, vol. 14, no. 3, pp. 195-210, 1995.\n", "annotations": {"author": "[{\"end\":59,\"start\":44},{\"end\":72,\"start\":60},{\"end\":91,\"start\":73},{\"end\":107,\"start\":92}]", "publisher": null, "author_last_name": "[{\"end\":58,\"start\":52},{\"end\":71,\"start\":67},{\"end\":90,\"start\":83},{\"end\":106,\"start\":99}]", "author_first_name": "[{\"end\":51,\"start\":44},{\"end\":66,\"start\":60},{\"end\":82,\"start\":73},{\"end\":98,\"start\":92}]", "author_affiliation": null, "title": "[{\"end\":41,\"start\":1},{\"end\":148,\"start\":108}]", "venue": null, "abstract": "[{\"end\":1203,\"start\":308}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1618,\"start\":1615},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1623,\"start\":1620},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1657,\"start\":1654},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2102,\"start\":2099},{\"end\":2351,\"start\":2350},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2977,\"start\":2974},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9970,\"start\":9966},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10786,\"start\":10783},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12645,\"start\":12641},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14404,\"start\":14400},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14645,\"start\":14642},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15334,\"start\":15330}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":17256,\"start\":17226},{\"attributes\":{\"id\":\"fig_1\"},\"end\":17632,\"start\":17257},{\"attributes\":{\"id\":\"fig_2\"},\"end\":17913,\"start\":17633},{\"attributes\":{\"id\":\"fig_3\"},\"end\":18139,\"start\":17914},{\"attributes\":{\"id\":\"fig_4\"},\"end\":18363,\"start\":18140},{\"attributes\":{\"id\":\"fig_5\"},\"end\":18500,\"start\":18364},{\"attributes\":{\"id\":\"fig_6\"},\"end\":19070,\"start\":18501},{\"attributes\":{\"id\":\"fig_8\"},\"end\":19328,\"start\":19071},{\"attributes\":{\"id\":\"fig_9\"},\"end\":19547,\"start\":19329},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":20377,\"start\":19548},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":20546,\"start\":20378}]", "paragraph": "[{\"end\":2103,\"start\":1222},{\"end\":3013,\"start\":2124},{\"end\":4237,\"start\":3035},{\"end\":4642,\"start\":4261},{\"end\":5661,\"start\":4644},{\"end\":7014,\"start\":5663},{\"end\":7850,\"start\":7033},{\"end\":8452,\"start\":7852},{\"end\":9217,\"start\":8475},{\"end\":9362,\"start\":9219},{\"end\":9654,\"start\":9364},{\"end\":9996,\"start\":9656},{\"end\":10247,\"start\":10036},{\"end\":10468,\"start\":10249},{\"end\":10787,\"start\":10486},{\"end\":11155,\"start\":10814},{\"end\":11615,\"start\":11246},{\"end\":12522,\"start\":11638},{\"end\":12791,\"start\":12548},{\"end\":13470,\"start\":12793},{\"end\":13625,\"start\":13472},{\"end\":14061,\"start\":13784},{\"end\":14100,\"start\":14093},{\"end\":14200,\"start\":14102},{\"end\":14715,\"start\":14236},{\"end\":14922,\"start\":14717},{\"end\":15526,\"start\":14993},{\"end\":16742,\"start\":15609},{\"end\":17225,\"start\":16751}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3019,\"start\":3014},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10035,\"start\":9997},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11245,\"start\":11156},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13783,\"start\":13626},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14092,\"start\":14062},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14951,\"start\":14923},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14992,\"start\":14951},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15579,\"start\":15527}]", "table_ref": null, "section_header": "[{\"end\":1220,\"start\":1205},{\"end\":2122,\"start\":2106},{\"end\":3033,\"start\":3021},{\"end\":4259,\"start\":4240},{\"end\":7031,\"start\":7017},{\"end\":8473,\"start\":8455},{\"end\":10484,\"start\":10471},{\"end\":10812,\"start\":10790},{\"end\":11636,\"start\":11618},{\"end\":12546,\"start\":12525},{\"end\":14234,\"start\":14203},{\"end\":15607,\"start\":15581},{\"end\":16749,\"start\":16745},{\"end\":17235,\"start\":17227},{\"end\":17642,\"start\":17634},{\"end\":17923,\"start\":17915},{\"end\":18151,\"start\":18141},{\"end\":18373,\"start\":18365},{\"end\":19080,\"start\":19072},{\"end\":19338,\"start\":19330}]", "table": "[{\"end\":20377,\"start\":19820},{\"end\":20546,\"start\":20485}]", "figure_caption": "[{\"end\":17256,\"start\":17237},{\"end\":17632,\"start\":17259},{\"end\":17913,\"start\":17644},{\"end\":18139,\"start\":17925},{\"end\":18363,\"start\":18154},{\"end\":18500,\"start\":18375},{\"end\":19070,\"start\":18503},{\"end\":19328,\"start\":19082},{\"end\":19547,\"start\":19340},{\"end\":19820,\"start\":19550},{\"end\":20485,\"start\":20380}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":1290,\"start\":1283},{\"end\":1346,\"start\":1338},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":2165,\"start\":2159},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":3551,\"start\":3545},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":3701,\"start\":3695},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":4475,\"start\":4469},{\"end\":7562,\"start\":7556},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":8115,\"start\":8109},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":8237,\"start\":8231},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8394,\"start\":8387},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8431,\"start\":8424},{\"end\":8914,\"start\":8908},{\"end\":9633,\"start\":9627},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11148,\"start\":11131},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16306,\"start\":16299},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16789,\"start\":16782}]", "bib_author_first_name": "[{\"end\":20682,\"start\":20681},{\"end\":20691,\"start\":20690},{\"end\":20879,\"start\":20878},{\"end\":20887,\"start\":20886},{\"end\":21091,\"start\":21090},{\"end\":21100,\"start\":21099},{\"end\":21108,\"start\":21107},{\"end\":21116,\"start\":21115},{\"end\":21129,\"start\":21128},{\"end\":21370,\"start\":21369},{\"end\":21382,\"start\":21381},{\"end\":21585,\"start\":21584},{\"end\":21595,\"start\":21594},{\"end\":21604,\"start\":21603},{\"end\":21801,\"start\":21800},{\"end\":21811,\"start\":21810},{\"end\":21820,\"start\":21819},{\"end\":22012,\"start\":22011},{\"end\":22014,\"start\":22013},{\"end\":22026,\"start\":22025},{\"end\":22036,\"start\":22035},{\"end\":22256,\"start\":22255},{\"end\":22266,\"start\":22265},{\"end\":22274,\"start\":22273},{\"end\":22534,\"start\":22533},{\"end\":22543,\"start\":22542},{\"end\":22741,\"start\":22740},{\"end\":22946,\"start\":22945},{\"end\":22956,\"start\":22955},{\"end\":22968,\"start\":22967},{\"end\":22975,\"start\":22974},{\"end\":23165,\"start\":23164},{\"end\":23175,\"start\":23174}]", "bib_author_last_name": "[{\"end\":20688,\"start\":20683},{\"end\":20699,\"start\":20692},{\"end\":20884,\"start\":20880},{\"end\":20894,\"start\":20888},{\"end\":21097,\"start\":21092},{\"end\":21105,\"start\":21101},{\"end\":21113,\"start\":21109},{\"end\":21126,\"start\":21117},{\"end\":21137,\"start\":21130},{\"end\":21379,\"start\":21371},{\"end\":21389,\"start\":21383},{\"end\":21592,\"start\":21586},{\"end\":21601,\"start\":21596},{\"end\":21612,\"start\":21605},{\"end\":21808,\"start\":21802},{\"end\":21817,\"start\":21812},{\"end\":21828,\"start\":21821},{\"end\":22023,\"start\":22015},{\"end\":22033,\"start\":22027},{\"end\":22044,\"start\":22037},{\"end\":22263,\"start\":22257},{\"end\":22271,\"start\":22267},{\"end\":22282,\"start\":22275},{\"end\":22540,\"start\":22535},{\"end\":22551,\"start\":22544},{\"end\":22749,\"start\":22742},{\"end\":22953,\"start\":22947},{\"end\":22965,\"start\":22957},{\"end\":22972,\"start\":22969},{\"end\":22984,\"start\":22976},{\"end\":23172,\"start\":23166},{\"end\":23184,\"start\":23176}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1061956},\"end\":20808,\"start\":20621},{\"attributes\":{\"id\":\"b1\"},\"end\":21007,\"start\":20810},{\"attributes\":{\"id\":\"b2\"},\"end\":21295,\"start\":21009},{\"attributes\":{\"id\":\"b3\"},\"end\":21506,\"start\":21297},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1640943},\"end\":21749,\"start\":21508},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":15249982},\"end\":21938,\"start\":21751},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":9082669},\"end\":22182,\"start\":21940},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6724907},\"end\":22413,\"start\":22184},{\"attributes\":{\"id\":\"b8\"},\"end\":22712,\"start\":22415},{\"attributes\":{\"id\":\"b9\"},\"end\":22858,\"start\":22714},{\"attributes\":{\"id\":\"b10\"},\"end\":23140,\"start\":22860},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2348246},\"end\":23286,\"start\":23142}]", "bib_title": "[{\"end\":20679,\"start\":20621},{\"end\":21582,\"start\":21508},{\"end\":21798,\"start\":21751},{\"end\":22009,\"start\":21940},{\"end\":22253,\"start\":22184},{\"end\":23162,\"start\":23142}]", "bib_author": "[{\"end\":20690,\"start\":20681},{\"end\":20701,\"start\":20690},{\"end\":20886,\"start\":20878},{\"end\":20896,\"start\":20886},{\"end\":21099,\"start\":21090},{\"end\":21107,\"start\":21099},{\"end\":21115,\"start\":21107},{\"end\":21128,\"start\":21115},{\"end\":21139,\"start\":21128},{\"end\":21381,\"start\":21369},{\"end\":21391,\"start\":21381},{\"end\":21594,\"start\":21584},{\"end\":21603,\"start\":21594},{\"end\":21614,\"start\":21603},{\"end\":21810,\"start\":21800},{\"end\":21819,\"start\":21810},{\"end\":21830,\"start\":21819},{\"end\":22025,\"start\":22011},{\"end\":22035,\"start\":22025},{\"end\":22046,\"start\":22035},{\"end\":22265,\"start\":22255},{\"end\":22273,\"start\":22265},{\"end\":22284,\"start\":22273},{\"end\":22542,\"start\":22533},{\"end\":22553,\"start\":22542},{\"end\":22751,\"start\":22740},{\"end\":22955,\"start\":22945},{\"end\":22967,\"start\":22955},{\"end\":22974,\"start\":22967},{\"end\":22986,\"start\":22974},{\"end\":23174,\"start\":23164},{\"end\":23186,\"start\":23174}]", "bib_venue": "[{\"end\":20705,\"start\":20701},{\"end\":20876,\"start\":20810},{\"end\":21088,\"start\":21009},{\"end\":21367,\"start\":21297},{\"end\":21618,\"start\":21614},{\"end\":21834,\"start\":21830},{\"end\":22050,\"start\":22046},{\"end\":22288,\"start\":22284},{\"end\":22531,\"start\":22415},{\"end\":22738,\"start\":22714},{\"end\":22943,\"start\":22860},{\"end\":23190,\"start\":23186}]"}}}, "year": 2023, "month": 12, "day": 17}