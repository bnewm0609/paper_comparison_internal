{"id": 237157766, "updated": "2023-10-06 00:07:25.573", "metadata": {"title": "Stochastic Scene-Aware Motion Prediction", "authors": "[{\"first\":\"Mohamed\",\"last\":\"Hassan\",\"middle\":[]},{\"first\":\"Duygu\",\"last\":\"Ceylan\",\"middle\":[]},{\"first\":\"Ruben\",\"last\":\"Villegas\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Saito\",\"middle\":[]},{\"first\":\"Jimei\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Yi\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Black\",\"middle\":[]}]", "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2021, "month": 8, "day": 18}, "abstract": "A long-standing goal in computer vision is to capture, model, and realistically synthesize human behavior. Specifically, by learning from data, our goal is to enable virtual humans to navigate within cluttered indoor scenes and naturally interact with objects. Such embodied behavior has applications in virtual reality, computer games, and robotics, while synthesized behavior can be used as a source of training data. This is challenging because real human motion is diverse and adapts to the scene. For example, a person can sit or lie on a sofa in many places and with varying styles. It is necessary to model this diversity when synthesizing virtual humans that realistically perform human-scene interactions. We present a novel data-driven, stochastic motion synthesis method that models different styles of performing a given action with a target object. Our method, called SAMP, for Scene-Aware Motion Prediction, generalizes to target objects of various geometries while enabling the character to navigate in cluttered scenes. To train our method, we collected MoCap data covering various sitting, lying down, walking, and running styles. We demonstrate our method on complex indoor scenes and achieve superior performance compared to existing solutions. Our code and data are available for research at https://samp.is.tue.mpg.de.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2108.08284", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/HassanCVSYZB21", "doi": "10.1109/iccv48922.2021.01118"}}, "content": {"source": {"pdf_hash": "28165bbb209630f0bba9c6af7f2abbf572161b0a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2108.08284v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "dfcd03b1b5d61fbd3f8e2baa499c06c8d78f1a42", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/28165bbb209630f0bba9c6af7f2abbf572161b0a.txt", "contents": "\nStochastic Scene-Aware Motion Prediction\n\n\nMohamed Hassan mhassan@tue.mpg.de \nMax Planck Institute for Intelligent Systems\nT\u00fcbingenGermany\n\nDuygu Ceylan ceylan@adobe.com \nAdobe Research\n\n\nRuben Villegas villegas@adobe.com \nAdobe Research\n\n\nJun Saito jsaito@adobe.com \nAdobe Research\n\n\nJimei Yang jimyang@adobe.com \nAdobe Research\n\n\nYi Zhou \nAdobe Research\n\n\nMichael Black black@tue.mpg.de \nMax Planck Institute for Intelligent Systems\nT\u00fcbingenGermany\n\nStochastic Scene-Aware Motion Prediction\n\nFigure 1: SAMP synthesizes virtual humans navigating complex scenes with realistic and diverse human-scene interactions.AbstractA long-standing goal in computer vision is to capture, model, and realistically synthesize human behavior. Specifically, by learning from data, our goal is to enable virtual humans to navigate within cluttered indoor scenes and naturally interact with objects. Such embodied behavior has applications in virtual reality, computer games, and robotics, while synthesized behavior can be used as training data. The problem is challenging because real human motion is diverse and adapts to the scene. For example, a person can sit or lie on a sofa in many places and with varying styles. We must model this diversity to synthesize virtual humans that realistically perform human-scene interactions. We present a novel data-driven, stochastic motion synthesis method that models different styles of performing a given action with a target object. Our Scene-Aware Motion Prediction method (SAMP) generalizes to target objects of various geometries while enabling the character to navigate in cluttered scenes. To train SAMP, we collected MoCap data covering various sitting, lying down, walking, and running styles. We demonstrate SAMP on complex indoor scenes and achieve superior performance than existing solutions. Code and data are available for research at https://samp.is.tue.mpg.de.\n\nIntroduction\n\nThe computer vision community has made substantial progress on 3D scene understanding and on capturing 3D human motion, but less work has focused on synthesizing 3D people in 3D scenes. The advances in these two subfields, however, have provided tools for, and have created interest in, embodied agents for virtual worlds (e.g. [35,42,55,56]) and in placing humans into scenes (e.g. [6,21]). Creating virtual humans that move and act like real people, however, is challenging and requires tackling many smaller but difficult problems such as perception of unseen environments, plausible human motion modeling, and embodied interaction with complex scenes. While advances have been made in human locomotion modeling [23,32] thanks to the availability of large scale datasets [7,33,38,45,50], realistically synthesizing virtual humans moving and interacting with 3D scenes, remains largely unsolved.\n\nImagine instructing a virtual human to \"sit on a couch\" in a cluttered scene, as illustrated in Fig. 1. To achieve this goal, the character needs to perform a series of complex actions. First, it should navigate through the scene to reach the target object while avoiding collisions with other objects in the scene. Next, the character needs to choose a contact point on the couch that will result in a plausible sitting action facing the right direction. Finally, if the character performs this action multiple times, there should be natural variations in the motion, mimicking real-world human-scene interactions; e.g., sitting on different parts of the couch with different styles such as with crossed legs, arms in different poses, etc. Achieving these goals requires a system to jointly reason about the scene geometry, smoothly transition between cyclic (e.g., walking) and acyclic (e.g., sitting) motions, and to model the diversity of human-scene interactions.\n\nTo this end, we propose SAMP for Scene-Aware Mo-tion Prediction. SAMP is a stochastic model that takes a 3D scene as input, samples valid interaction goals, and generates goal-conditioned and scene-aware motion sequences of a character depicting realistic dynamic character-scene interactions. At the core of SAMP is a novel autoregressive conditional variational autoencoder (cVAE) called Mo-tionNet. Given a target object and an action, MotionNet samples a random latent vector at each frame to condition the next pose both on the previous pose of the character as well as the random vector. This enables MotionNet to model a wide range of styles while performing the target action. Given the geometry of the target object, SAMP further uses another novel neural network called GoalNet to generate multiple plausible contact points and orientations on the target object (e.g., different positions and sitting orientations on the cushions of a sofa). This component enables SAMP to generalize across objects with diverse geometry. Finally, to ensure the character avoids obstacles while reaching the goal in a cluttered scene, we use an explicit path planning algorithm (A* search) to pre-compute an obstacle-free path between the starting location of the character and the goal. This piecewise linear path consists of multiple way-points, which SAMP treats as intermediate goals to drive the character around the scene. SAMP runs in real-time at 30 fps. To the best of our knowledge, these individual components make SAMP the first system that addresses the problem of generating diverse dynamic motion sequences that depict realistic human-scene interactions in cluttered environments. Training SAMP requires a dataset of rich and diverse character scene interactions. Existing large-scale MoCap datasets are largely dominated by locomotion and the few interaction examples lack diversity. Additionally, traditional MoCap focuses on the body and rarely captures the scene. Hence, we capture a new dataset covering various human-scene interactions with multiple objects. In each motion sequence, we track both the body motion and the object using a high resolution optical marker MoCap system. The dataset is available for research purposes.\n\nOur contributions are: (1) A novel stochastic model for synthesizing varied goal-driven character-scene interactions in real-time. (2) A new method for modeling plausible action-dependent goal locations and orientations of the body given the target object geometry. (3) Incorporating explicit path planning into a variational motion synthesis network enabling navigation in cluttered scenes. (4) A new MoCap dataset with diverse human-scene interactions.\n\n\nRelated Work\n\nInteraction Synthesis: Analyzing and synthesizing plausible human-scene interactions have received a lot of attention from the computer vision and graphics communities. Various algorithms have been proposed for pre-dicting object functionalities [16,66], affordance analysis [18,53], and synthesizing static human-scene interactions [16,18,21,27,41,62,64].\n\nA less explored area involves generating dynamic human-scene interactions. While earlier work [28] focuses on synthesizing motions of a character in the same environment in which the motion was captured, follow up work [2,26,29,43] assembles motion sequences from a large database to synthesize interactions with new environments or characters. Such methods, however, require large databases and expensive nearest neighbor matching.\n\nAn important sub-category of human-scene interaction involves locomotion, where the character must respond to changes in terrain with appropriate foot placement. Phasefunctioned neural networks [23] have shown impressive results by using a guiding signal representing the state of the motion cycle (i.e., phase). Zhang et al. [61] extend this idea to use a mixture of experts [13,25,60] as the motion prediction network. An additional gating network is used to predict the expert blending weights at run time. More closely related to our work is the Neural State Machine (NSM) [47], which extends the ideas of phase labels and expert networks to model human-scene interactions such as sit, carry, and open. While NSM is a powerful method, it does not generate variations in such interactions, which is one of our key contributions. Our experiments also demonstrate that NSM often fails to avoid intersections between the 3D character and objects in cluttered scenes (Sec. 5.2). Furthermore, training NSM requires time-consuming manual, and often ambiguous, labeling of phases for non-periodic actions. Starke et al. [48] propose a method to automatically extract local phase variables for each body part in the context of a two-player basketball game. Extending local phases to non-periodic actions is not trivial, however. We find that using scheduled sampling [5] provides an alternative to generate smooth transitions without phase labels. More recently, Wang et al. [52] introduce a hierarchical framework for synthesizing human-scene interactions. They generate sub-goal positions in the scene, predict the pose at each of these sub-goals, and synthesize the motion between such poses. This method requires a postoptimization framework to ensure smoothness and robust foot contact and to discourage penetration with the scene. Corona et al. [11] use a semantic graph to model humanobject relationships followed by an RNN to predict human and object movements.\n\nAn alternative approach uses reinforcement learning (RL) to build a control policy that models interactions. Merel et al. [37] and Eom et al. [14] focus on ball catching from egocentric vision. Chao et al. [10] train sub-task controllers and a meta controller to execute the sub-tasks to complete a sitting task. However, in contrast to SAMP, their approach does not enable variations in the goal posi-tions and directions. In addition, as with many RL-based approaches, generalizing the learned policies to new environments or actions is often challenging.\n\nMotion Synthesis: Neural networks (feed-forward networks, LSTMs, or RNNs) have been extensively applied to the motion synthesis problem [1,15,19,24,36,49,51]. A typical approach predicts the future motion of a character based on previous frame(s). While showing impressive results when generating short sequences, many of these methods either converge to the mean pose or diverge when tested on long sequences. A common solution is to employ scheduled sampling [5] to ensure stable predictions at test time to generate long locomotion and dancing sequences [32,65].\n\nSeveral works have focused on modeling the stochastic nature of human motion, with a specific emphasis on trajectory prediction. Given the past trajectory of a character, they model multiple plausible future trajectories [4,6,8,17,34,40,44]. Recently, Cao et al. [6] sample multiple future goals and then use them to generate different future skeletal motions. This is similar in spirit to our use of GoalNet. The difference is that our goal is to predict various trajectories that always lead to the same target object (instead of predicting any plausible future trajectory).\n\nModeling the stochasticity of the full human motion is a less explored area [54,58,59]. Motion VAE [32] predicts a distribution of the next poses instead of one pose using the latent space of a conditional variational autoencoder. MoGlow is a controllable probabilistic generative model based on normalizing flows [22]. Generating diverse dance motions from music has also been recently explored [30,31]. Xu et al. [57] generate diverse motions by blending short sequences from a database. To the best of our knowledge, no previous work has tackled the problem of generating diverse human-scene interactions.\n\n\nMethod\n\nGenerating dynamic human scene interactions in cluttered environments requires solutions to several subproblems. First and foremost, the synthesized motion of the character should be realistic and capture natural variations. Given a target object, it is important to sample plausible contact points and orientations for performing a specific action (e.g., where to sit on a chair and which direction to face). Finally, the motion needs to be synthesized such that it navigates to the goal location while avoiding penetrating objects in the scene. Our system consists of three main components that address each of these sub-problems: a Mo-tionNet, GoalNet, and a Path Planning Module. At the core of our method is the MotionNet which predicts the pose of the character based on the previous pose as well as other factors such as the interaction object geometry and the target goal position and orientation. GoalNet predicts the goal position and orientation for the interaction on the desired object. The Path Planning Module computes an obstaclefree path between the starting location of the character and the goal location. The full pipeline is illustrated in Fig. 2. \n\n\nMotionNet\n\nMotionNet is an autoregressive conditional variational autoencoder (cVAE) [12,46] that generates the pose of the character conditioned on its previous state (e.g., pose, trajectory, goal) as well as the geometry of the interaction object. MotionNet has two components: an encoder and a decoder. The encoder encodes the previous and current states of the character and the interaction object to a latent vector Z. The decoder takes this latent vector, the character's previous state, and the interaction object to predict the character's next state. The pipeline is shown in Fig. 3. Note that, at test time, we only utilize the decoder of MotionNet and sample Z from a standard normal distribution.\n\nEncoder: The encoder consists of two sub-encoders: State Encoder and Interaction Encoder. The State Encoder encodes the previous and current state of the character into a low-dimensional vector. Similarly, the Interaction Encoder encodes the object geometry into a different lowdimensional vector. Next, the two vectors are concatenated and passed through two identical fully connected layers to predict the mean \u00b5 and standard deviation \u03c3 of a Gaussian distribution representing a latent embedding space. We then sample a random latent code Z, which is provided to the decoder when predicting the next state of the character.\n\nState Representation: We use a representation similar to Starke et al. [47] to encode the state of the character. Specifically, the state at frame i is defined as\nX i = j p i , j r i , j v i ,j p i , t p i , t d i ,t p i ,t d i , t a i , g p i , g d i , g a i , c i , (1) where j p i \u2208 R 3j , j r i \u2208 R 6j , j v i \u2208 R 3j\nare the position, rotation, and velocity of each joint relative to the root. j is the number of joints in the skeleton which is 22 in our data.j p i \u2208 R 3j are the joint positions relative to future root 1 second ahead. t p i \u2208 R 2t and t d i \u2208 R 2t are the root positions and forward directions relative to the root of frame\ni \u2212 1.t p i \u2208 R 2t andt d i \u2208 R 2t\nare the root positions and forward directions relative to the goal of frame i \u2212 1. We define these inputs for t time steps sampled uniformly in a 2 second window between [\u22121, 1] seconds. t a i \u2208 R nat is a vector of continuous action labels on each of the t samples. In our experiments, n a is 5, which is the total number of actions we model (i.e., idle, walk, run, sit, lie down). g p i \u2208 R 3t , g d i \u2208 R 3t are the goal positions and directions, and g a i \u2208 R nat is a one-hot action label describing the action to be performed at each of the t samples. c i \u2208 R 5 are contact labels for pelvis, feet, and hands.\n\nState Encoder: The State Encoder takes the current X i and previous state X i\u22121 and encodes them into a lowdimensional vector using three fully connected layers.  Interaction Encoder: The Interaction Encoder takes a voxel representation of the interaction object I and encodes it into a low-dimensional vector. We use a voxel grid of size 8 \u00d7 8 \u00d7 8. Each voxel stores a 4\u2212dimensional vector. The first three components refer to the position of the voxel center relative to the root of the character. The fourth element stores the real-valued occupancy (between 0 and 1) of the voxel. The architecture consists of three fully connected layers.\n\nDecoder: The decoder takes the random latent code Z, the interaction object representation I, and the previous state X i\u22121 , and predicts the next stateX i . Similar to recent work [32,47], our decoder is built as a mixture-of-experts with two components: the Prediction Network and Gating Network.\n\nThe Prediction Network is responsible for predicting the next stateX i . The weights of the Prediction Network \u03b1 are computed by blending K expert weights:\n\u03b1 = K i=1 \u03c9 i \u03b1 i ,(2)\nwhere the blending weights \u03c9 i are predicted by the Gating Network. Each expert is a three-layer fully connected network. The Gating Network is also a three-layer fully connected network, which takes as input Z and X i\u22121 .\n\nMotionNet is trained end-to-end to minimize the loss\nL motion = ||X i \u2212 X i || 2 2 + \u03b2 1 KL(Q(Z|X i , X i\u22121 , I)||p(Z)),(3)\nwhere the first term minimizes the difference between the ground truth and predicted states of the character and KL denotes the Kullback-Leibler divergence.\n\n\nGoalNet\n\nGiven a target interaction object (which can be interactively defined by a user at test time or randomly sampled among the objects in the scene), the character is driven by the goal position g p \u2208 R 3 and direction g d \u2208 R 3 sampled on the object's surface. In order to perform realistic interactions; the character requires the ability to predict these goal positions and directions from the object geometry. For example, while a regular chair allows variation in terms of sitting direction, the direction of sitting on an armchair is restricted (see Fig. 7). We use GoalNet to model objectspecific goal positions and directions. GoalNet is a conditional variational autoencoder (cVAE) that predicts plausible goal positions and directions given the voxel representation of the target interaction object I as shown in Fig. 4. The encoder encodes the interaction object I, goal position g p , and direction g d , into a latent code Z goal . The decoder reconstructs the goal position\u011d p , and direction\u011d d from Z goal and I. We represent the object using a voxel representation similar to the one used in MotionNet (Sec. 3.1). The only difference is that we compute the voxel position relative to the object center instead of the character root. In the encoder, we use an Interaction Encoder similar to the one used in MotionNet (see Sec. 3.1) to encode the object representation I to a low dimension vector. This vector is then concatenated with g p and g d and encoded further to the latent vector Z goal . The decoder has the same architecture as the encoder as shown in Fig. 4. The network is trained to minimize the loss: At test time, given a target object I, we randomly sample Z goal \u223c N (0, I) and use the decoder to generate various goal positions g p and directions g d .\nL goal =||\u011d p \u2212 g p || 2 2 + ||\u011d d \u2212 g d || 2 2 + \u03b2 2 KL(Q(Z goal |g p , g d , I)||p(Z goal )).(4)\n\nPath Planning\n\nTo ensure the character can navigate inside cluttered environments while avoiding obstacles, we employ an explicit A* path planning algorithm [20]. Given the desired goal location, we use A* to compute an obstacle-free path from the starting position of the character to the goal. The path is defined as a series of waypoints w i = {w 0 , w 1 , w 2 , ...} that define the locations where the path changes direction. We break the task of performing the final desired action into sub-tasks in which each sub-task requires the character to walk to the next waypoint. The final sub-task requires the character to perform the desired action at the final waypoint.\n\n\nTraining Strategy\n\nTraining MotionNet using standard supervised training produces poor quality predictions at run time (see Sup. Mat.). This is due to the accumulation of error at run time when the output of the network is fed back as input in the next step. To account for this, we train the network using scheduled sampling [5], which has been shown to result in long stable motion predictions [32]. During training, the current network prediction is used as input in the next training step with a probability 1 \u2212 P . P is (see Sup. Mat.):\nP = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 epoch \u2264 C 1 , 1 \u2212 epoch\u2212C1 C2\u2212C1 C1 < epoch \u2264 C 2 , 0 epoch > C2.(5)\n\nData Preparation\n\n\nMotion Data\n\nTo model variations in human-scene interactions, we capture a new dataset using an optical MoCap system with 54 Vicon cameras. We place seven different objects in the center of the MoCap area, namely two sofas, an armchair, a chair, a high bar chair, a low chair and a table. We record multiple clips of each interaction with different styles. In each sequence, the subject starts from an A-Pose in a random location in the MoCap space, walks towards the object, and performs the action for 20 \u2212 40 seconds. Finally, the subject gets up from the object and walks away. Our goal is to capture various styles of performing the same action, thus we ask the subject to change the style in each sequence. In addition to the subject, we also capture the object pose using attached markers. We also have the CAD model for each object. Finally, we capture running, walking, and idle sequences where the subject walks and runs in different directions with different speeds and stands in an idle state. Our dataset consists of \u223c100 minutes of motion data recorded at 30 fps from a single subject, resulting in \u223c185K frames. We use MoSh++ [33] to fit the SMPL-X [39] body model to the optical markers. More details about the data are available in the Sup. Mat.\n\n\nMotion Data Augmentation\n\nWith only seven captured objects, MotionNet will fail to adapt to new unseen objects. Capturing MoCap with a wide range of objects requires a significant amount of effort and time. We address this issue by augmenting our data using an efficient augmentation pipeline similar to [3,47]. Since we capture both the body motion as well as the object pose, we compute the contact between the body and the object. We detect the contacts of five key joints of the character skeleton. Namely, pelvis, hands, and feet. We then augment our data by randomly switching or scaling the object at each frame. When switching, we replace the original object with a random object of a similar size selected from ShapeNet [9]. For each new object (scaled or switched), we project the contacts detected from the ground truth data to the new object. Finally, we use an IK solver to recompute the full pose such that the contacts are maintained. Please refer to the Sup. Mat. for more details.\n\n\nGoal Data\n\nTo train GoalNet, we label various goal positions g p and directions g d for different objects from ShapeNet [9]. These goals represent the position on the object surface where a character could sit and the forward direction of the character when sitting. We select 5 categories from ShapeNet namely, sofas, L-shaped sofas, chairs, armchairs, and tables. From each category, we select 15 \u2212 20 instances and we manually label 1 \u2212 5 goals for each instance. The number of goals labeled per instance depends on how many different goals an object can afford. For example, an L-shaped sofa offers more places to sit than a chair. In total, we use 80 objects as our training data. We augment our data by randomly scaling the objects across the xyz axes leading to \u223c13K training samples.\n\n\nExperiments & Evaluation\n\n\nQualitative Evaluation\n\nIn this section, we provide qualitative results and discuss the main points. We refer to the Sup. Mat. and the accompanying video for more results.\n\nGenerating Diverse Motion: In contrast to previous deterministic methods [47], SAMP generates a wide range of diverse styles of an action while ensuring realism. Several different sitting and lying down styles generated by SAMP are shown in Fig. 5. The use of the Interaction Encoder 3.1 and the data augmentation (Sec. 4.2) further ensures SAMP can adapt to different objects with varying geometry. Notice how the character naturally leans its head back on the sofa. The style of the action is also conditioned on the interacting object. The character lifts its legs when sitting on a high chair/table but extends its legs when sitting on a very low table. We observe that lying down is a harder task and several of baseline methods fail to execute this task (see Sec. 5.2). While SAMP synthesizes reasonable sequences, our results are not always perfect. The generated motion might involve some penetration with the object.\n\nGoal Generation: When presented with a new object, the character needs to predict where and in which direction the action should be executed. In [47], the goal is computed as the object center. However, this heuristic fails for objects with complex geometries. In Fig. 6 we show that using the object center results in invalid actions whereas GoalNet allows our method to reason about where the action should be executed. As shown in Fig. 7, by sampling different latent codes Z goal , GoalNet generates multiple goal positions and directions for various objects. Notice how GoalNet captures that, while a person can sit sideways on a regular chair, this is not valid for an armchair. Figure 8 shows how the different goals generated by GoalNet guide the motion of the character. Starting from the same position, direction, and initial pose, the virtual human follows two different paths to reach different goal positions when performing the \"sit on the couch\" action. The final pose of the character is also different in the two cases due to the stochastic nature of MotionNet.\n\nPath Planning: When navigating to a particular goal location in a cluttered scene, it is critical to avoid obstacles. Our Path Planning Module achieves this goal by predicting the shortest obstacle-free path between the starting character position and the goal using a navigation mesh computed based on the 3D scene. The navigation mesh defines the walk-able areas in the scene and is computed once offline. In Fig. 9, we show an example path computed by the Path Planning Module. Without this module, the character often walks through objects in the scene. We observe a similar behaviour in the previous work of NSM [47], even though NSM uses a volumetric representation of the environment to help the character navigate.\n\n\nQuantitative Evaluation\n\nDeterministic vs. Stochastic: To quantify the diversity of the generated motion, we put the character in a fixed starting position and direction and we run our method ten times with the same goal. For example, we instruct the character to sit/lie down on the same object multiple times starting from the same initial state/position/direction. For walking and running, we instruct the character to run in each of the four directions for 15 seconds. We record the character motion for each run and then compute the Average Pairwise Distance (APD) [58,63] as shown in Table. 1. The APD is defined as:\nAP D = 1 N (N \u2212 1) N i=0 N j=0 j =i ||X i \u2212 X j || 2 2 .(6)\nX i represents the character's local pose features at frame i.\nX i = {j p i , j r i , j v i }.\nN is the total number of frames for all sequences. For comparison we also report the APD for the ground truth (GT) data in Table. 1.\n\nGoalNet: Given 150 unseen goals sampled on test objects, we measure the average position and orientation reconstruction error of GoalNet to be 6.04 cm and 2.29 deg (we note that the objects have real-life measurements). To measure the diversity of the generated goals, we compute the Average Pairwise Distance (APD) among the generated goal positions g p and directions g d :\nAPD-Pos = 1 LN (N \u2212 1) L k=0 N i=0 N j=0 j =i |g p i \u2212 g p j |(7)APD-Rot = 1 LN (N \u2212 1) L k=0 N i=0 N j=0 j =i arccos(g d i .g d j ). (8)\nL = 150 is the number of objects and N = 10 is the number of goals generated for each object. We find APD-Pos and APD-Rot for our generated goals to be 16.42 cm and 41.27 deg compared to 16.18 cm and 90.23 deg for the ground truth (GT) data. Path Planning Module: To quantitatively evaluate the effectiveness of our Path Planning Module, we test our method in a cluttered scene. We put the character in a random initial position and orientation and select a random   goal. We repeat this 10 times. We find the percentage of frames where a penetration happens is 3.8%, 11.2%, and 8.11% for SAMP with Path Planning Module, without Path Planning Module, and NSM [47], respectively. While NSM uses a volumetric sensor to detect collisions with the environment, it is not as effective as explicit path planning.\n\nComparison  (FD) between the distribution of the generated motion and ground truth. Execution time is the time required to transition to the target action label from an idle state. Precision is the positional (PE) and rotational (RE) error at the goal. We measure FD on a subset of the state features which we call X:X = j p , j r , j v ,t p ,t d .\n\nAs our baselines, we choose a feedforward network (MLP) as the motion prediction network, Mixture of Experts (MoE) [61], and NSM [47] (see Sup. Mat. for details). SAMP vs. MLP vs. MoE: We re-trained the MLP and MoE using the same training strategy and data we used for SAMP. Both MLP and MoE take a longer time to execute the task and often fail to execute the \"lie down\" action (denoted \u221e) as evidenced by the execution time in Table. 2 and precision in Table. 3. These architectures sometimes generate implausible poses as shown in Sup. Mat., which is reflected by the lower FD in Table. 4 SAMP vs. NSM: For NSM, we used the publicly available pre-trained model since retraining NSM on our data is infeasible due to the missing phase labels. We trained SAMP on the same data on which NSM was trained. In      Limitations and Future Work: We observe that sometimes slight penetrations between the character and the interacting object can occur. A potential solution is to incorporate a post-processing step to optimize the pose of the character to avoid such intersections. In order to generalize SAMP to interacting objects that have significantly differ-  ent geometry than those seen in training, in future work, we would like to explore methods to encode local object geometries.\n\n\nConclusion\n\nHere we have described SAMP, which makes several important steps toward creating lifelike avatars that move and act like real people in previously unseen and complex environments. Critically, we introduce three elements that must be part of a solution. First, characters must be able to navigate the world and avoid obstacles. For this, we use an existing path planning method. Second, characters can interact with objects in different ways. To address this, we train GoalNet to take an object and stochastically produce an interaction location and direction. Third, the character should produce motions achieving the goal that vary naturally. To that end, we train a novel MotionNet that incrementally generates body poses based on the past motion and the goal. We train SAMP using a novel dataset of motion capture data involving human-object interaction.    \n\n\nA.2. Goal Data\n\nWe select 5 categories from ShapeNet namely, sofas, Lshaped sofas, chairs, armchairs, and tables. From each cate-  gory, we select 15\u221220 instances and we manually label 1\u22125 goals for each instance. Table. S.2 shows the number of instances for each category. We manually label 1 \u2212 5 goals for each instance. The number of goals labelled per instance depends on how many different goals an object can afford. For example, we label 5 different goals for the L-shaped sofa compared to 3 for the chair as shown in Fig. S.3.\n\n\nB. Training Details\n\n\nB.1. MotionNet\n\nThe character state X is of size 647. The State Encoder, Interaction Encoder, Gating Network, and Prediction Network are all three-layer fully connected networks with rectified linear function ELU. The dimensions of each network are in Table S.3. The encoder latent code Z is of size 64 and we set the number of experts K to 12. We use a learning rate of 5e \u2212 5 and train our network for 100 epochs. We use the Adam optimizer with linear weight decay. The weight of the Kullback-Leibler divergence \u03b2 1 is 0.1.\n\n\nB.2. GoalNet\n\nThe Interaction Encoder of GoalNet is a three-layer fully connected network of shape {512, 512, 64}. The latent vec-  tor Z goal is of size 3. The weight of the Kullback-Leibler divergence \u03b2 2 is 0.5. We use the Adam optimizer with a learning rate of 1e \u2212 3 and train GoalNet for 100 epochs.\n\n\nB.3. Schedule Sampling\n\nFor the schedule sampling training strategy, we set C 1 = 30 and C 2 = 60. We define a roll-out window of size L where we set L = 60 in our experiments. For each roll-out, we feed the ground truth first frame as input to the network and then sequentially predict the subsequent frames while using the scheduled sampling strategy. We divide our training data to equal-length clips of size L.\n\n\nC. Baselines\n\nAs our baselines, we choose a feedforward network (MLP) and a Mixture of Experts (MoE). The architecture of the MLP is shown in Fig. S.4. We use the same Interaction Encoder used for our MotionNet followed by four fully connected layers of size 512. The architecture of the MoE is shown in Fig. S.5. The Interaction Encoder, Gating Network, and Prediction Network are all the same as the one used in MotionNet.\n\n\nD. Schedule Sampling\n\nWe found that using Schedule Sampling is essential to enable the character to successfully reach the goal and execute the action. Without it, we found the model to often diverge, get stuck, or take very long time to reach the goal as we show in Fig. S.6. \n\n\nE. Path Planning Formulation\n\nIn order to use the Path Planning Module, we first compute the surface area where the character could stand or move. We call this the navigation mesh. This is computed from the character cylinder collider and the scene geometry. The navigation mesh is stored as convex polygons. To find a path between given start and end points, we first map these points to the closest polygons and then use A* to find the shortest path between the polygons 1 .\n\n\nF. Data Augmentation Details\n\nWhen the object is transformed, the contacts follow the same transformation. When the object is replaced by a new one, we project the original contact by finding the closest points on the surface of the new object. The new motion curve is computed by interpolation and the whole full body pose is computed using CCD IK solver. This does not guarantee smoothness but we found it to be stable in practice. More details are in [47].\n\n\nG. Interaction Encoder Ablation:\n\nTo quantify the importance of the Interaction Encoder, we trained SAMP without the Interaction Encoder. We found that the precision of reaching the goal deteriorates to 14.82 cm and 3.65 deg compared to 6.09 cm and 3.55 deg when the Interaction Encoder was used.\n\n\nH. Comparison to Cao et al.:\n\nWhile relevant, the formulation of Cao [6] et al. is significantly different than our method making a direct comparison difficult. Given a target interaction object and action (e.g. \"sit on the couch\"), SAMP samples a goal location and orientation on the object, computes an obstaclefree path towards the object, and synthesizes diverse motion sequences that are of arbitrary length until the goal is executed. We assume that the character starts the action from an idle position without any knowledge of the past. In contrast, Cao et al. sample a goal location in the image space given a one-second-long history of motion. Based on this trajectory, a deterministic motion sequence of fixed length (two-seconds) is synthesized. The action executed in this trajectory is not controllable.\n\n\nI. Failure Cases\n\nWe observe that SAMP might not adapt well to objects with significantly different geometry than those seen in training as shown in Fig. S.7. Future work might explore different methods of encoding the object geometry. \n\nFigure 2 :\n2Our system consists of three main components. GoalNet predicts oriented goal locations (green sphere and blue arrow on the chair) given an interaction object. The Path Planning Module predicts an obstacle-free path from the starting position to the goal. MotionNet sequentially predicts the next character state until the desired action is executed.\n\nFigure 3 :\n3MotionNet consists of an encoder and a decoder. The encoder consists of two sub-encoders: State Encoder and Interaction Encoder. The decoder consists of a Prediction Network to predict the next character state and a gating network that predicts the blending weights of the Prediction Network. See Sec. 3.1.\n\nFigure 4 :\n4GoalNet generates multiple valid goal position\u015d g p and directions\u011d d given an object representation I. FC(N) denotes a fully connected layer of size N.\n\nFigure 5 :\n5SAMP generates plausible and diverse action styles and adapts to different object geometries.\n\nFigure 6 :\n6Without GoalNet (left), SAMP fails to sit on a valid place. SAMP with GoalNet is shown on the right.\n\nFigure 7 :\n7GoalNet generates diverse valid goals on different objects. Spheres indicate goal positions, and blue arrows indicate goal directions.\n\nFigure 8 :\n8Goals generated by GoalNet (mesh spheres) are used by MotionNet to guide the motion of virtual characters.\n\nFigure 9 :\n9Our Path Planning Module helps SAMP to successfully navigate cluttered scenes (left). NSM[47] fails in such scenes (right).\n\nFig\n. S.1 shows examples of different sitting and lying down styles from our MoCap. A breakdown of the dataset in terms of different actions is shown in Table S.1. The objects used during the MoCap are shown in Fig. S.2.\n\nFigure S. 1 :\n1Examples of action styles in our motion capture data.\n\nFigure S. 2 :\n2Objects used during motion capture.\n\nFigure\nFigure S.3: Goal Labelling.\n\nFigure S. 4 :\n4MLP Architecture.\n\nFigure S. 5 :\n5MoE Architecture.\n\nFigure S. 6 :\n6SAMP With Schedule Sampling (Top) and without (bottom). The black line shows the root projection on the xz plane. The blue and green circles denote the root at the first and last frame respectively. The red circle denotes the goal position. Note how SAMP fails to reach the goal without the use of Schedule Sampling.\n\nFigure S. 7 :\n7SAMP with significantly different geometry.\n\nTable 2 :\n2Average execution Time in seconds. \u221e means the method failed to reach the goal within 3 minutes.\n\nTable 3 :\n3Average precision in terms of positional and rota-\ntional errors (PE and RE). \u221e means the method failed to \nreach the goal within 3 minutes. \n\nIdle \nWalk \nRun \nSit \nLiedown \nMLP \n102.85 \n121.18 \n150.56 \n105.87 \n36.85 \nMoE \n102.91 \n114.17 \n151.14 \n105.10 \n35.79 \nSAMP 102.72 111.09 141.11 104.68 \n17.30 \n\n\n\nTable 4 :\n4Fr\u00e8chet distance.\n\nTable 5\n5we observe that our model is on par with NSM in \nterms of achieving goals without the need for phase labels, \nwhich are cumbersome and often ambiguous to annotate. \nIn addition, our main focus is to model diverse motions via \na stochastic model while NSM is deterministic. Our Path \nPlanning Module module helps SAMP to safely navigate \ncomplex scenes where NSM fails as shown by the penetra-\ntion amounts. \nFor all evaluations, all test objects are randomly selected \nfrom ShapeNet and none is part of our training set. \n\n\nTable 5 :\n5SAMP vs. NSM.\n\n\nTable S.1: Motion capture data breakdown with respect to actions.Labels \nMinutes Percentage % \nIdle \n18.3 \n17.7 \nWalk \n42.3 \n41.0 \nRun \n5.1 \n4.9 \nSit \n27.3 \n26.4 \nLie down \n10.1 \n9.7 \nTotal \n103.3 \n\n\n\nTable S .\nS2: GoalNet data breakdown with respect to object categories. Interaction Encoder {256, 256, 256} Gating Network {512, 256, 12} Prediction Network {512, 512, 647} Table S.3: Architecture details. All networks are all threelayer fully connected networks with ELU.Network \nArchitecture \nState Encoder \n{512, 256, 256} \n\nhttps://docs.unity3d.com/Manual/ nav-InnerWorkings.html\nAcknowledgement This work was initiated while MH was an intern at Adobe. We are grateful to Sebastian Starke for inspiring work, helpful discussion, and making his code open-source. We thank Joachim Tesch for feedback on Unity and rendering, Nima Ghorbani for MoSH++, and Meshcapade for the character texture. For helping with the data collection, we are grateful to Tsvetelina Alexiadis, Galina Henz, Markus H\u00f6schle and Tobias Bauch. Disclosure: MJB has received research funds from Adobe, Intel, Nvidia, Facebook, and Amazon. While MJB is a parttime employee of Amazon, his research was performed solely at, and funded solely by, Max Planck. MJB has financial interests in Amazon, Datagen Technologies, and Meshcapade GmbH.\nTripod: Human trajectory and pose dynamics forecasting in the wild. Vida Adeli, Mahsa Ehsanpour, Ian Reid, Juan Carlos Niebles, Silvio Savarese, Ehsan Adeli, Hamid Rezatofighi, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)Vida Adeli, Mahsa Ehsanpour, Ian Reid, Juan Car- los Niebles, Silvio Savarese, Ehsan Adeli, and Hamid Rezatofighi. Tripod: Human trajectory and pose dynamics forecasting in the wild. In Proceedings of the IEEE Interna- tional Conference on Computer Vision (ICCV), 2021. 3\n\nTask-based locomotion. Shailen Agrawal, Michiel Van De Panne, ACM Trans. Graph. 354Shailen Agrawal and Michiel van de Panne. Task-based lo- comotion. ACM Trans. Graph., 35(4), 2016. 2\n\nRelationship descriptors for interactive motion adaptation. Rami Ali Al-Asqhar , Taku Komura, Myung Geol Choi, Proceedings of the 12th ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '13. the 12th ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '13New York, NY, USAAssociation for Computing MachineryRami Ali Al-Asqhar, Taku Komura, and Myung Geol Choi. Relationship descriptors for interactive motion adaptation. In Proceedings of the 12th ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '13, page 45-53, New York, NY, USA, 2013. Association for Computing Ma- chinery. 5\n\nSocial lstm: Human trajectory prediction in crowded spaces. Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, Silvio Savarese, Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. So- cial lstm: Human trajectory prediction in crowded spaces. In Proceedings IEEE/CVF Conf. on Computer Vision and Pat- tern Recognition (CVPR), pages 961-971, 2016. 3\n\nScheduled sampling for sequence prediction with recurrent neural networks. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer, Proceedings of the 28th International Conference on Neural Information Processing Systems. the 28th International Conference on Neural Information Processing SystemsCambridge, MA, USAMIT Press15NIPS'15Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Proceedings of the 28th In- ternational Conference on Neural Information Processing Systems -Volume 1, NIPS'15, page 1171-1179, Cambridge, MA, USA, 2015. MIT Press. 2, 3, 5\n\nLong-term human motion prediction with scene context. Zhe Cao, Hang Gao, Karttikeya Mangalam, Qi-Zhi Cai, Minh Vo, Jitendra Malik, European Conference on Computer Vision (ECCV). Springer114Zhe Cao, Hang Gao, Karttikeya Mangalam, Qi-Zhi Cai, Minh Vo, and Jitendra Malik. Long-term human motion pre- diction with scene context. In European Conference on Com- puter Vision (ECCV), pages 387-404. Springer, 2020. 1, 3, 14\n\n. CMU MoCap Dataset. 1Carnegie Mellon UniversityCarnegie Mellon University. CMU MoCap Dataset. 1\n\nMultipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction. Yuning Chai, Benjamin Sapp, Mayank Bansal, Dragomir Anguelov, PMLRProceedings of the Conference on Robot Learning. the Conference on Robot Learning100Yuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir Anguelov. Multipath: Multiple probabilistic anchor trajec- tory hypotheses for behavior prediction. In Proceedings of the Conference on Robot Learning, volume 100 of Proceed- ings of Machine Learning Research, pages 86-99. PMLR, 30 Oct-01 Nov 2020. 3\n\nShapeNet: An Information-Rich 3D Model Repository. Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu, arXiv:1512.03012Stanford University -Princeton University -Toyota Technological Institute at ChicagoTechnical Reportcs.GRAngel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Mano- lis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University -Princeton University -Toyota Tech- nological Institute at Chicago, 2015. 5\n\nLearning to sit: Synthesizing human-chair interactions via hierarchical control. Yu-Wei Chao, Jimei Yang, Weifeng Chen, Jia Deng, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceYu-Wei Chao, Jimei Yang, Weifeng Chen, and Jia Deng. Learning to sit: Synthesizing human-chair interactions via hierarchical control. Proceedings of the AAAI Conference on Artificial Intelligence, 2019. 2\n\nContext-aware human motion prediction. Enric Corona, Albert Pumarola, Guillem Alenya, Francesc Moreno-Noguer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionEnric Corona, Albert Pumarola, Guillem Alenya, and Francesc Moreno-Noguer. Context-aware human motion prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6992- 7001, 2020. 2\n\nAuto-encoding variational bayes. Max P Kingma Diederik, Welling, International Conference on Learning Representations ICLR. P Kingma Diederik and Max Welling. Auto-encoding varia- tional bayes. In International Conference on Learning Rep- resentations ICLR, 2014. 3\n\nLearning factored representations in a deep mixture of experts. David Eigen, Marc&apos;aurelio Ranzato, Ilya Sutskever, International Conference on Learning Representations ICLR. David Eigen, Marc'Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep mixture of ex- perts. In International Conference on Learning Representa- tions ICLR, 2014. 2\n\nModel predictive control with a visuomotor system for physics-based character animation. Haegwang Eom, Daseong Han, Joseph S Shin, Junyong Noh, ACM Trans. Graph. 391Haegwang Eom, Daseong Han, Joseph S. Shin, and Junyong Noh. Model predictive control with a visuomotor system for physics-based character animation. ACM Trans. Graph., 39(1), Oct. 2019. 2\n\nRecurrent network models for human dynamics. Katerina Fragkiadaki, Sergey Levine, Panna Felsen, Jitendra Malik, Proceedings of the IEEE International Conference on Computer Vision (ICCV), ICCV '15. the IEEE International Conference on Computer Vision (ICCV), ICCV '15USAIEEE Computer SocietyKaterina Fragkiadaki, Sergey Levine, Panna Felsen, and Ji- tendra Malik. Recurrent network models for human dynam- ics. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), ICCV '15, page 4346-4354, USA, 2015. IEEE Computer Society. 3\n\nWhat makes a chair a chair?. Helmut Grabner, Juergen Gall, Luc Van Gool, Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)Helmut Grabner, Juergen Gall, and Luc Van Gool. What makes a chair a chair? In Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), pages 1529-1536, 2011. 2\n\nSocial gan: Socially acceptable trajectories with generative adversarial networks. Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, Alexandre Alahi, Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social gan: Socially acceptable trajec- tories with generative adversarial networks. In Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recogni- tion (CVPR), pages 2255-2264, 2018. 3\n\nFrom 3D scene geometry to human workspace. A Gupta, S Satkin, A A Efros, M Hebert, Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)A. Gupta, S. Satkin, A. A. Efros, and M. Hebert. From 3D scene geometry to human workspace. In Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recogni- tion (CVPR), pages 1961-1968, 2011. 2\n\nA recurrent variational autoencoder for human motion synthesis. I Habibie, Daniel Holden, Jonathan Schwarz, Joe Yearsley, T Komura, BMVC. I. Habibie, Daniel Holden, Jonathan Schwarz, Joe Yearsley, and T. Komura. A recurrent variational autoencoder for hu- man motion synthesis. In BMVC, 2017. 3\n\nA formal basis for the heuristic determination of minimum cost paths. P E Hart, N J Nilsson, B Raphael, IEEE Transactions on Systems Science and Cybernetics. 42P. E. Hart, N. J. Nilsson, and B. Raphael. A formal basis for the heuristic determination of minimum cost paths. IEEE Transactions on Systems Science and Cybernetics, 4(2):100- 107, 1968. 5\n\nPopulating 3D scenes by learning human-scene interaction. Mohamed Hassan, Partha Ghosh, Joachim Tesch, Dimitrios Tzionas, Michael J Black, Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)1Mohamed Hassan, Partha Ghosh, Joachim Tesch, Dim- itrios Tzionas, and Michael J. Black. Populating 3D scenes by learning human-scene interaction. In Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recogni- tion (CVPR), June 2021. 1, 2\n\nMoGlow: Probabilistic and controllable motion synthesis using normalising flows. Gustav Eje Henter, Simon Alexanderson, Jonas Beskow, ACM Trans. Graph. 396Gustav Eje Henter, Simon Alexanderson, and Jonas Beskow. MoGlow: Probabilistic and controllable motion synthesis using normalising flows. ACM Trans. Graph., 39(6), Nov. 2020. 3\n\nPhasefunctioned neural networks for character control. Daniel Holden, Taku Komura, Jun Saito, ACM Trans. Graph. 364Daniel Holden, Taku Komura, and Jun Saito. Phase- functioned neural networks for character control. ACM Trans. Graph., 36(4), July 2017. 1, 2\n\nA deep learning framework for character motion synthesis and editing. Daniel Holden, Jun Saito, Taku Komura, ACM Trans. Graph. 354Daniel Holden, Jun Saito, and Taku Komura. A deep learning framework for character motion synthesis and editing. ACM Trans. Graph., 35(4):1-11, 2016. 3\n\nHinton. Adaptive mixtures of local experts. R A Jacobs, M I Jordan, S J Nowlan, G E , Neural Computation. 31R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hin- ton. Adaptive mixtures of local experts. Neural Computa- tion, 3(1):79-87, 1991. 2\n\nPrecision: Precomputing environment semantics for contact-rich character animation. Mubbasir Kapadia, Xu Xianghao, Maurizio Nitti, Marcelo Kallmann, Stelian Coros, Robert W Sumner, Markus Gross, Proceedings of the 20th ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D '16. the 20th ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D '16New York, NY, USAAssociation for Computing MachineryMubbasir Kapadia, Xu Xianghao, Maurizio Nitti, Marcelo Kallmann, Stelian Coros, Robert W. Sumner, and Markus Gross. Precision: Precomputing environment semantics for contact-rich character animation. In Proceedings of the 20th ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D '16, page 29-37, New York, NY, USA, 2016. Association for Computing Machinery. 2\n\nShape2Pose: Human-centric shape analysis. Vladimir G Kim, Siddhartha Chaudhuri, Leonidas Guibas, Thomas Funkhouser, ACM Trans. Graph. Vladimir G. Kim, Siddhartha Chaudhuri, Leonidas Guibas, and Thomas Funkhouser. Shape2Pose: Human-centric shape analysis. ACM Trans. Graph., Aug. 2014. 2\n\nInteractive control of avatars animated with human motion data. Jehee Lee, Jinxiang Chai, Paul S A Reitsma, Jessica K Hodgins, Nancy S Pollard, ACM Trans. Graph. 213Jehee Lee, Jinxiang Chai, Paul S. A. Reitsma, Jessica K. Hodgins, and Nancy S. Pollard. Interactive control of avatars animated with human motion data. ACM Trans. Graph., 21(3):491-500, July 2002. 2\n\nMotion patches: Building blocks for virtual environments annotated with motion data. Myung Geol Kang Hoon Lee, Jehee Choi, Lee, ACM SIGGRAPH 2006 Papers, SIGGRAPH '06. New York, NY, USAAssociation for Computing MachineryKang Hoon Lee, Myung Geol Choi, and Jehee Lee. Mo- tion patches: Building blocks for virtual environments anno- tated with motion data. In ACM SIGGRAPH 2006 Papers, SIGGRAPH '06, page 898-906, New York, NY, USA, 2006. Association for Computing Machinery. 2\n\nLearning to generate diverse dance motions with transformer. Jiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler, Hao Li, arXiv:2008.08171arXiv preprintJiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler, and Hao Li. Learning to generate diverse dance motions with transformer. arXiv preprint arXiv:2008.08171, 2020. 3\n\nLearn to dance with AIST++: Music conditioned 3D dance generation. Ruilong Li, Shan Yang, A David, Angjoo Ross, Kanazawa, 2021. 3Proceedings of the IEEE International Conference on Computer Vision (ICCV. the IEEE International Conference on Computer Vision (ICCVRuilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa. Learn to dance with AIST++: Music conditioned 3D dance generation. In Proceedings of the IEEE Interna- tional Conference on Computer Vision (ICCV), 2021. 3\n\nCharacter controllers using motion vaes. Hung Yu Ling, Fabio Zinno, George Cheng, Michiel Van De Panne, ACM Trans. Graph. 394Hung Yu Ling, Fabio Zinno, George Cheng, and Michiel Van De Panne. Character controllers using motion vaes. ACM Trans. Graph., 39(4), July 2020. 1, 3, 4, 5\n\nAMASS: Archive of motion capture as surface shapes. Naureen Mahmood, Nima Ghorbani, F Nikolaus, Gerard Troje, Michael J Pons-Moll, Black, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger- ard Pons-Moll, and Michael J. Black. AMASS: Archive of motion capture as surface shapes. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 5442-5451, Oct. 2019. 1, 5\n\nOvercoming limitations of mixture density networks: A sampling and fitting framework for multimodal future prediction. O Makansi, E Ilg, \u00d6 , T Brox, Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)O. Makansi, E. Ilg,\u00d6. \u00c7 i\u00e7ek, and T. Brox. Overcoming lim- itations of mixture density networks: A sampling and fitting framework for multimodal future prediction. In Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recogni- tion (CVPR), 2019. 3\n\nHabitat: A Platform for Embodied AI Research. Manolis Savva, * , Abhishek Kadian, * , Oleksandr Maksymets, * , Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, Dhruv Batra, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)Manolis Savva*, Abhishek Kadian*, Oleksandr Maksymets*, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A Platform for Embodied AI Research. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2019. 1\n\nOn human motion prediction using recurrent neural networks. Julieta Martinez, J Michael, Javier Black, Romero, Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)Julieta Martinez, Michael J Black, and Javier Romero. On human motion prediction using recurrent neural networks. In Proceedings IEEE/CVF Conf. on Computer Vision and Pat- tern Recognition (CVPR), pages 2891-2900, 2017. 3\n\nCatch & carry: Reusable neural controllers for vision-guided whole-body tasks. Josh Merel, Saran Tunyasuvunakool, Arun Ahuja, Yuval Tassa, Leonard Hasenclever, Vu Pham, Tom Erez, Greg Wayne, Nicolas Heess, ACM Trans. Graph. 394Josh Merel, Saran Tunyasuvunakool, Arun Ahuja, Yuval Tassa, Leonard Hasenclever, Vu Pham, Tom Erez, Greg Wayne, and Nicolas Heess. Catch & carry: Reusable neural controllers for vision-guided whole-body tasks. ACM Trans. Graph., 39(4), July 2020. 2\n\nDocumentation mocap database HDM05. M M\u00fcller, T R\u00f6der, M Clausen, B Eberhardt, B Kr\u00fcger, A Weber, CG-2007-2Universit\u00e4t BonnTechnical ReportM. M\u00fcller, T. R\u00f6der, M. Clausen, B. Eberhardt, B. Kr\u00fcger, and A. Weber. Documentation mocap database HDM05. Technical Report CG-2007-2, Universit\u00e4t Bonn, June 2007. 1\n\nExpressive body capture: 3D hands, face, and body from a single image. Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, A A Ahmed, Dimitrios Osman, Michael J Tzionas, Black, Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3D hands, face, and body from a single image. In Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2019. 5\n\nSophie: An attentive gan for predicting paths compliant to social and physical constraints. Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki Hirose, Hamid Rezatofighi, Silvio Savarese, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionAmir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki Hirose, Hamid Rezatofighi, and Silvio Savarese. Sophie: An attentive gan for predicting paths compliant to social and physical constraints. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 1349-1358, 2019. 3\n\nPiGraphs: Learning interaction snapshots from observations. Manolis Savva, X Angel, Pat Chang, Matthew Hanrahan, Matthias Fisher, Nie\u00dfner, ACM Trans. Graph. 354Manolis Savva, Angel X Chang, Pat Hanrahan, Matthew Fisher, and Matthias Nie\u00dfner. PiGraphs: Learning inter- action snapshots from observations. ACM Trans. Graph., 35(4):1-12, 2016. 2\n\niGibson 1.0: a simulation environment for interactive tasks in large realistic scenes. Bokui Shen, Fei Xia, Chengshu Li, Roberto Mart\u00edn-Mart\u00edn, Linxi Fan, Guanzhi Wang, Claudia P\u00e9rez-D&apos;arpino, Shyamal Buch, Sanjana Srivastava, Lyne P Tchapmi, Micael E Tchapmi, Kent Vainio, Josiah Wong, Li Fei-Fei, Silvio Savarese, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems. Bokui Shen, Fei Xia, Chengshu Li, Roberto Mart\u00edn-Mart\u00edn, Linxi Fan, Guanzhi Wang, Claudia P\u00e9rez-D'Arpino, Shya- mal Buch, Sanjana Srivastava, Lyne P. Tchapmi, Micael E. Tchapmi, Kent Vainio, Josiah Wong, Li Fei-Fei, and Silvio Savarese. iGibson 1.0: a simulation environment for interac- tive tasks in large realistic scenes. In 2021 IEEE/RSJ Inter- national Conference on Intelligent Robots and Systems, page accepted. IEEE, 2021. 1\n\nInteraction patches for multi-character animation. P H Hubert, Taku Shum, Masashi Komura, Shuntaro Shiraishi, Yamazaki, ACM Trans. Graph. 275Hubert P. H. Shum, Taku Komura, Masashi Shiraishi, and Shuntaro Yamazaki. Interaction patches for multi-character animation. ACM Trans. Graph., 27(5), Dec. 2008. 2\n\nImplicit probabilistic models of human motion for synthesis and tracking. H Sidenbladh, M J Black, L Sigal, European Conf. on Computer Vision (ECCV). 1H. Sidenbladh, M. J. Black, and L. Sigal. Implicit proba- bilistic models of human motion for synthesis and tracking. In European Conf. on Computer Vision (ECCV), volume 1, pages 784-800, 2002. 3\n\nHumanEva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion. L Sigal, A Balan, M J Black, International Journal of Computer Vision. 874L. Sigal, A. Balan, and M. J. Black. HumanEva: Synchro- nized video and motion capture dataset and baseline algo- rithm for evaluation of articulated human motion. Interna- tional Journal of Computer Vision, 87(4):4-27, Mar. 2010. 1\n\nLearning structured output representation using deep conditional generative models. Kihyuk Sohn, Honglak Lee, Xinchen Yan, Advances in Neural Information Processing Systems. Curran Associates, Inc283Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional gen- erative models. In Advances in Neural Information Process- ing Systems 28, pages 3483-3491. Curran Associates, Inc., 2015. 3\n\nNeural state machine for character-scene interactions. Sebastian Starke, He Zhang, Taku Komura, Jun Saito, ACM Trans. Graph. 38613Sebastian Starke, He Zhang, Taku Komura, and Jun Saito. Neural state machine for character-scene interactions. ACM Trans. Graph., 38(6), Nov. 2019. 2, 3, 4, 5, 6, 7, 8, 13\n\nTaku Komura, and Kazi Zaman. Local motion phases for learning multi-contact character movements. Sebastian Starke, Yiwei Zhao, ACM Trans. Graph. 394Sebastian Starke, Yiwei Zhao, Taku Komura, and Kazi Za- man. Local motion phases for learning multi-contact charac- ter movements. ACM Trans. Graph., 39(4), July 2020. 2\n\nFactored conditional restricted boltzmann machines for modeling motion style. W Graham, Geoffrey E Taylor, Hinton, chinery. 3Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09. the 26th Annual International Conference on Machine Learning, ICML '09New York, NY, USAAssociation for Computing MaGraham W. Taylor and Geoffrey E. Hinton. Factored con- ditional restricted boltzmann machines for modeling motion style. In Proceedings of the 26th Annual International Con- ference on Machine Learning, ICML '09, page 1025-1032, New York, NY, USA, 2009. Association for Computing Ma- chinery. 3\n\nTotal capture: 3D human pose estimation fusing video and inertial sensors. Matthew Trumble, Andrew Gilbert, Charles Malleson, Adrian Hilton, John Collomosse, Proceedings of the British Machine Vision Conference (BMVC). the British Machine Vision Conference (BMVC)13Matthew Trumble, Andrew Gilbert, Charles Malleson, Adrian Hilton, and John Collomosse. Total capture: 3D human pose estimation fusing video and inertial sensors. In Proceedings of the British Machine Vision Conference (BMVC), pages 14.1-14.13, Sept. 2017. 1\n\nLearning to generate longterm future via hierarchical prediction. Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin, Honglak Lee, PMLR, 2017. 3international conference on machine learning. Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin, and Honglak Lee. Learning to generate long- term future via hierarchical prediction. In international conference on machine learning, pages 3560-3569. PMLR, 2017. 3\n\nSynthesizing long-term 3D human motion and interaction in 3D scenes. Jiashun Wang, Huazhe Xu, Jingwei Xu, Sifei Liu, Xiaolong Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionJiashun Wang, Huazhe Xu, Jingwei Xu, Sifei Liu, and Xiao- long Wang. Synthesizing long-term 3D human motion and interaction in 3D scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9401-9411, 2021. 2\n\nBinge watching: Scaling affordance learning from sitcoms. Xiaolong Wang, Rohit Girdhar, Abhinav Gupta, Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)Xiaolong Wang, Rohit Girdhar, and Abhinav Gupta. Binge watching: Scaling affordance learning from sitcoms. In Pro- ceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2017. 2\n\nCombining recurrent neural networks and adversarial training for human motion synthesis and control. Z Wang, J Chai, S Xia, IEEE Transactions on Visualization and Computer Graphics. 271Z. Wang, J. Chai, and S. Xia. Combining recurrent neu- ral networks and adversarial training for human motion syn- thesis and control. IEEE Transactions on Visualization and Computer Graphics, 27(1):14-28, 2021. 3\n\nGibson env: real-world perception for embodied agents. Fei Xia, R Amir, Zhi-Yang Zamir, Alexander He, Jitendra Sax, Silvio Malik, Savarese, Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)IEEEFei Xia, Amir R. Zamir, Zhi-Yang He, Alexander Sax, Ji- tendra Malik, and Silvio Savarese. Gibson env: real-world perception for embodied agents. In Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE, 2018. 1\n\nSAPIEN: A simulated part-based interactive environment. Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, Li Yi, Angel X Chang, Leonidas J Guibas, Hao Su, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognitio). the IEEE/CVF Conference on Computer Vision and Pattern Recognitio)Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, Li Yi, Angel X. Chang, Leonidas J. Guibas, and Hao Su. SAPIEN: A simulated part-based interactive envi- ronment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognitio), June 2020. 1\n\nHierarchical style-based networks for motion synthesis. Jingwei Xu, Huazhe Xu, Bingbing Ni, Xiaokang Yang, Xiaolong Wang, Trevor Darrell, European Conference on Computer Vision (ECCV). ChamSpringer International PublishingJingwei Xu, Huazhe Xu, Bingbing Ni, Xiaokang Yang, Xi- aolong Wang, and Trevor Darrell. Hierarchical style-based networks for motion synthesis. In European Conference on Computer Vision (ECCV), pages 178-194, Cham, 2020. Springer International Publishing. 3\n\nDlow: Diversifying latent flows for diverse human motion prediction. Ye Yuan, Kris Kitani, European Conference on Computer Vision (ECCV). 36Ye Yuan and Kris Kitani. Dlow: Diversifying latent flows for diverse human motion prediction. In European Conference on Computer Vision (ECCV), 2020. 3, 6\n\nDiverse trajectory forecasting with determinantal point processes. Ye Yuan, Kris M Kitani, International Conference on Learning Representations. 2020Ye Yuan and Kris M. Kitani. Diverse trajectory forecasting with determinantal point processes. In International Confer- ence on Learning Representations, 2020. 3\n\nTwenty years of mixture of experts. S E Yuksel, J N Wilson, P D Gader, IEEE Transactions on Neural Networks and Learning Systems. 238S. E. Yuksel, J. N. Wilson, and P. D. Gader. Twenty years of mixture of experts. IEEE Transactions on Neural Networks and Learning Systems, 23(8):1177-1193, 2012. 2\n\nMode-adaptive neural networks for quadruped motion control. He Zhang, Sebastian Starke, Taku Komura, Jun Saito, ACM Trans. Graph. 3747He Zhang, Sebastian Starke, Taku Komura, and Jun Saito. Mode-adaptive neural networks for quadruped motion con- trol. ACM Trans. Graph., 37(4), July 2018. 2, 7\n\nPlace: Proximity learning of articulation and contact in 3D environments. Siwei Zhang, Yan Zhang, Qianli Ma, J Michael, Siyu Black, Tang, International Conference on 3D Vision (3DV). 2020Siwei Zhang, Yan Zhang, Qianli Ma, Michael J Black, and Siyu Tang. Place: Proximity learning of articulation and con- tact in 3D environments. In International Conference on 3D Vision (3DV), 2020. 2\n\nWe are more than our joints: Predicting how 3D bodies move. Yan Zhang, Michael J Black, Siyu Tang, Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)Yan Zhang, Michael J. Black, and Siyu Tang. We are more than our joints: Predicting how 3D bodies move. In Pro- ceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), page 3372-3382, June 2021. 6\n\nGenerating 3D people in scenes without people. Yan Zhang, Mohamed Hassan, Heiko Neumann, J Michael, Siyu Black, Tang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionYan Zhang, Mohamed Hassan, Heiko Neumann, Michael J Black, and Siyu Tang. Generating 3D people in scenes with- out people. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6194- 6204, 2020. 2\n\nAuto-conditioned recurrent networks for extended complex human motion synthesis. Yi Zhou, Zimo Li, Shuangjiu Xiao, Chong He, Zeng Huang, Hao Li, International Conference on Learning Representations ICLR. Yi Zhou, Zimo Li, Shuangjiu Xiao, Chong He, Zeng Huang, and Hao Li. Auto-conditioned recurrent networks for ex- tended complex human motion synthesis. In International Conference on Learning Representations ICLR, 2018. 3\n\nUnderstanding tools: Task-oriented object modeling, learning and recognition. Yixin Zhu, Yibiao Zhao, Song-Chun Zhu, Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)Yixin Zhu, Yibiao Zhao, and Song-Chun Zhu. Under- standing tools: Task-oriented object modeling, learning and recognition. In Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2015. 2\n", "annotations": {"author": "[{\"end\":140,\"start\":44},{\"end\":188,\"start\":141},{\"end\":240,\"start\":189},{\"end\":285,\"start\":241},{\"end\":332,\"start\":286},{\"end\":358,\"start\":333},{\"end\":452,\"start\":359}]", "publisher": null, "author_last_name": "[{\"end\":58,\"start\":52},{\"end\":153,\"start\":147},{\"end\":203,\"start\":195},{\"end\":250,\"start\":245},{\"end\":296,\"start\":292},{\"end\":340,\"start\":336},{\"end\":372,\"start\":367}]", "author_first_name": "[{\"end\":51,\"start\":44},{\"end\":146,\"start\":141},{\"end\":194,\"start\":189},{\"end\":244,\"start\":241},{\"end\":291,\"start\":286},{\"end\":335,\"start\":333},{\"end\":366,\"start\":359}]", "author_affiliation": "[{\"end\":139,\"start\":79},{\"end\":187,\"start\":172},{\"end\":239,\"start\":224},{\"end\":284,\"start\":269},{\"end\":331,\"start\":316},{\"end\":357,\"start\":342},{\"end\":451,\"start\":391}]", "title": "[{\"end\":41,\"start\":1},{\"end\":493,\"start\":453}]", "venue": null, "abstract": "[{\"end\":1907,\"start\":495}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2255,\"start\":2251},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2258,\"start\":2255},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":2261,\"start\":2258},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":2264,\"start\":2261},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2309,\"start\":2306},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2312,\"start\":2309},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2642,\"start\":2638},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2645,\"start\":2642},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2700,\"start\":2697},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2703,\"start\":2700},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2706,\"start\":2703},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2709,\"start\":2706},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2712,\"start\":2709},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6758,\"start\":6754},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":6761,\"start\":6758},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6787,\"start\":6783},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":6790,\"start\":6787},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6845,\"start\":6841},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6848,\"start\":6845},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6851,\"start\":6848},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6854,\"start\":6851},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6857,\"start\":6854},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":6860,\"start\":6857},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":6863,\"start\":6860},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6964,\"start\":6960},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7088,\"start\":7085},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7091,\"start\":7088},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7094,\"start\":7091},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7097,\"start\":7094},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7498,\"start\":7494},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7630,\"start\":7626},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7680,\"start\":7676},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7683,\"start\":7680},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":7686,\"start\":7683},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7881,\"start\":7877},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8420,\"start\":8416},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8665,\"start\":8662},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":8774,\"start\":8770},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9150,\"start\":9146},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9392,\"start\":9388},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9412,\"start\":9408},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9476,\"start\":9472},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9964,\"start\":9961},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9967,\"start\":9964},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9970,\"start\":9967},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9973,\"start\":9970},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9976,\"start\":9973},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9979,\"start\":9976},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9982,\"start\":9979},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10289,\"start\":10286},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10386,\"start\":10382},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":10389,\"start\":10386},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10616,\"start\":10613},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10618,\"start\":10616},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10620,\"start\":10618},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10623,\"start\":10620},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10626,\"start\":10623},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10629,\"start\":10626},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10632,\"start\":10629},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10658,\"start\":10655},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":11050,\"start\":11046},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":11053,\"start\":11050},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":11056,\"start\":11053},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11073,\"start\":11069},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11288,\"start\":11284},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11370,\"start\":11366},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11373,\"start\":11370},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":11389,\"start\":11385},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12850,\"start\":12846},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":12853,\"start\":12850},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":14174,\"start\":14170},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":16227,\"start\":16223},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":16230,\"start\":16227},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19081,\"start\":19077},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19925,\"start\":19922},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19996,\"start\":19992},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21388,\"start\":21384},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21411,\"start\":21407},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21815,\"start\":21812},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":21818,\"start\":21815},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22240,\"start\":22237},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22631,\"start\":22628},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":23579,\"start\":23575},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":24578,\"start\":24574},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":26130,\"start\":26126},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":26808,\"start\":26804},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":26811,\"start\":26808},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":28323,\"start\":28319},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":28937,\"start\":28933},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":28951,\"start\":28947},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":34437,\"start\":34433},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":34812,\"start\":34809},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":37241,\"start\":37237}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36159,\"start\":35797},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36479,\"start\":36160},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36645,\"start\":36480},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36752,\"start\":36646},{\"attributes\":{\"id\":\"fig_4\"},\"end\":36866,\"start\":36753},{\"attributes\":{\"id\":\"fig_5\"},\"end\":37014,\"start\":36867},{\"attributes\":{\"id\":\"fig_6\"},\"end\":37134,\"start\":37015},{\"attributes\":{\"id\":\"fig_7\"},\"end\":37271,\"start\":37135},{\"attributes\":{\"id\":\"fig_8\"},\"end\":37493,\"start\":37272},{\"attributes\":{\"id\":\"fig_9\"},\"end\":37563,\"start\":37494},{\"attributes\":{\"id\":\"fig_10\"},\"end\":37615,\"start\":37564},{\"attributes\":{\"id\":\"fig_11\"},\"end\":37651,\"start\":37616},{\"attributes\":{\"id\":\"fig_12\"},\"end\":37685,\"start\":37652},{\"attributes\":{\"id\":\"fig_13\"},\"end\":37719,\"start\":37686},{\"attributes\":{\"id\":\"fig_14\"},\"end\":38052,\"start\":37720},{\"attributes\":{\"id\":\"fig_15\"},\"end\":38112,\"start\":38053},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":38221,\"start\":38113},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":38538,\"start\":38222},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":38568,\"start\":38539},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":39101,\"start\":38569},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":39127,\"start\":39102},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":39329,\"start\":39128},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":39658,\"start\":39330}]", "paragraph": "[{\"end\":2820,\"start\":1923},{\"end\":3790,\"start\":2822},{\"end\":6035,\"start\":3792},{\"end\":6491,\"start\":6037},{\"end\":6864,\"start\":6508},{\"end\":7298,\"start\":6866},{\"end\":9264,\"start\":7300},{\"end\":9823,\"start\":9266},{\"end\":10390,\"start\":9825},{\"end\":10968,\"start\":10392},{\"end\":11578,\"start\":10970},{\"end\":12758,\"start\":11589},{\"end\":13469,\"start\":12772},{\"end\":14097,\"start\":13471},{\"end\":14261,\"start\":14099},{\"end\":14745,\"start\":14420},{\"end\":15396,\"start\":14781},{\"end\":16040,\"start\":15398},{\"end\":16340,\"start\":16042},{\"end\":16497,\"start\":16342},{\"end\":16743,\"start\":16521},{\"end\":16797,\"start\":16745},{\"end\":17025,\"start\":16869},{\"end\":18819,\"start\":17037},{\"end\":19593,\"start\":18935},{\"end\":20137,\"start\":19615},{\"end\":21505,\"start\":20256},{\"end\":22505,\"start\":21534},{\"end\":23299,\"start\":22519},{\"end\":23500,\"start\":23353},{\"end\":24427,\"start\":23502},{\"end\":25507,\"start\":24429},{\"end\":26231,\"start\":25509},{\"end\":26856,\"start\":26259},{\"end\":26979,\"start\":26917},{\"end\":27144,\"start\":27012},{\"end\":27521,\"start\":27146},{\"end\":28466,\"start\":27660},{\"end\":28816,\"start\":28468},{\"end\":30102,\"start\":28818},{\"end\":30978,\"start\":30117},{\"end\":31515,\"start\":30997},{\"end\":32065,\"start\":31556},{\"end\":32373,\"start\":32082},{\"end\":32790,\"start\":32400},{\"end\":33217,\"start\":32807},{\"end\":33497,\"start\":33242},{\"end\":33976,\"start\":33530},{\"end\":34438,\"start\":34009},{\"end\":34737,\"start\":34475},{\"end\":35557,\"start\":34770},{\"end\":35796,\"start\":35578}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14419,\"start\":14262},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14780,\"start\":14746},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16520,\"start\":16498},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16868,\"start\":16798},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18918,\"start\":18820},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20222,\"start\":20138},{\"attributes\":{\"id\":\"formula_6\"},\"end\":26916,\"start\":26857},{\"attributes\":{\"id\":\"formula_7\"},\"end\":27011,\"start\":26980},{\"attributes\":{\"id\":\"formula_8\"},\"end\":27587,\"start\":27522},{\"attributes\":{\"id\":\"formula_9\"},\"end\":27659,\"start\":27587}]", "table_ref": "[{\"end\":26830,\"start\":26824},{\"end\":27141,\"start\":27135},{\"end\":29253,\"start\":29247},{\"end\":29279,\"start\":29273},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29409,\"start\":29401},{\"end\":31201,\"start\":31195},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":31800,\"start\":31792}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1921,\"start\":1909},{\"attributes\":{\"n\":\"2.\"},\"end\":6506,\"start\":6494},{\"attributes\":{\"n\":\"3.\"},\"end\":11587,\"start\":11581},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12770,\"start\":12761},{\"attributes\":{\"n\":\"3.2.\"},\"end\":17035,\"start\":17028},{\"attributes\":{\"n\":\"3.3.\"},\"end\":18933,\"start\":18920},{\"attributes\":{\"n\":\"3.4.\"},\"end\":19613,\"start\":19596},{\"attributes\":{\"n\":\"4.\"},\"end\":20240,\"start\":20224},{\"attributes\":{\"n\":\"4.1.\"},\"end\":20254,\"start\":20243},{\"attributes\":{\"n\":\"4.2.\"},\"end\":21532,\"start\":21508},{\"attributes\":{\"n\":\"4.3.\"},\"end\":22517,\"start\":22508},{\"attributes\":{\"n\":\"5.\"},\"end\":23326,\"start\":23302},{\"attributes\":{\"n\":\"5.1.\"},\"end\":23351,\"start\":23329},{\"attributes\":{\"n\":\"5.2.\"},\"end\":26257,\"start\":26234},{\"attributes\":{\"n\":\"6.\"},\"end\":30115,\"start\":30105},{\"end\":30995,\"start\":30981},{\"end\":31537,\"start\":31518},{\"end\":31554,\"start\":31540},{\"end\":32080,\"start\":32068},{\"end\":32398,\"start\":32376},{\"end\":32805,\"start\":32793},{\"end\":33240,\"start\":33220},{\"end\":33528,\"start\":33500},{\"end\":34007,\"start\":33979},{\"end\":34473,\"start\":34441},{\"end\":34768,\"start\":34740},{\"end\":35576,\"start\":35560},{\"end\":35808,\"start\":35798},{\"end\":36171,\"start\":36161},{\"end\":36491,\"start\":36481},{\"end\":36657,\"start\":36647},{\"end\":36764,\"start\":36754},{\"end\":36878,\"start\":36868},{\"end\":37026,\"start\":37016},{\"end\":37146,\"start\":37136},{\"end\":37276,\"start\":37273},{\"end\":37508,\"start\":37495},{\"end\":37578,\"start\":37565},{\"end\":37623,\"start\":37617},{\"end\":37666,\"start\":37653},{\"end\":37700,\"start\":37687},{\"end\":37734,\"start\":37721},{\"end\":38067,\"start\":38054},{\"end\":38123,\"start\":38114},{\"end\":38232,\"start\":38223},{\"end\":38549,\"start\":38540},{\"end\":38577,\"start\":38570},{\"end\":39112,\"start\":39103},{\"end\":39340,\"start\":39331}]", "table": "[{\"end\":38538,\"start\":38234},{\"end\":39101,\"start\":38579},{\"end\":39329,\"start\":39195},{\"end\":39658,\"start\":39603}]", "figure_caption": "[{\"end\":36159,\"start\":35810},{\"end\":36479,\"start\":36173},{\"end\":36645,\"start\":36493},{\"end\":36752,\"start\":36659},{\"end\":36866,\"start\":36766},{\"end\":37014,\"start\":36880},{\"end\":37134,\"start\":37028},{\"end\":37271,\"start\":37148},{\"end\":37493,\"start\":37277},{\"end\":37563,\"start\":37510},{\"end\":37615,\"start\":37580},{\"end\":37651,\"start\":37624},{\"end\":37685,\"start\":37668},{\"end\":37719,\"start\":37702},{\"end\":38052,\"start\":37736},{\"end\":38112,\"start\":38069},{\"end\":38221,\"start\":38125},{\"end\":38568,\"start\":38551},{\"end\":39127,\"start\":39114},{\"end\":39195,\"start\":39130},{\"end\":39603,\"start\":39342}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":2924,\"start\":2918},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12757,\"start\":12750},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13352,\"start\":13346},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":17595,\"start\":17589},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17862,\"start\":17856},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18617,\"start\":18611},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23749,\"start\":23743},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24699,\"start\":24693},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24869,\"start\":24863},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":25122,\"start\":25114},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":25926,\"start\":25920},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":31514,\"start\":31506},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":32943,\"start\":32935},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":33105,\"start\":33097},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":33495,\"start\":33487},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":35717,\"start\":35709}]", "bib_author_first_name": "[{\"end\":40513,\"start\":40509},{\"end\":40526,\"start\":40521},{\"end\":40541,\"start\":40538},{\"end\":40552,\"start\":40548},{\"end\":40559,\"start\":40553},{\"end\":40575,\"start\":40569},{\"end\":40591,\"start\":40586},{\"end\":40604,\"start\":40599},{\"end\":41056,\"start\":41049},{\"end\":41289,\"start\":41271},{\"end\":41296,\"start\":41292},{\"end\":41310,\"start\":41305},{\"end\":41315,\"start\":41311},{\"end\":41898,\"start\":41889},{\"end\":41914,\"start\":41906},{\"end\":41928,\"start\":41921},{\"end\":41950,\"start\":41941},{\"end\":41964,\"start\":41962},{\"end\":41980,\"start\":41974},{\"end\":42487,\"start\":42483},{\"end\":42501,\"start\":42496},{\"end\":42518,\"start\":42511},{\"end\":42531,\"start\":42527},{\"end\":43113,\"start\":43110},{\"end\":43123,\"start\":43119},{\"end\":43139,\"start\":43129},{\"end\":43156,\"start\":43150},{\"end\":43166,\"start\":43162},{\"end\":43179,\"start\":43171},{\"end\":43667,\"start\":43661},{\"end\":43682,\"start\":43674},{\"end\":43695,\"start\":43689},{\"end\":43712,\"start\":43704},{\"end\":44175,\"start\":44170},{\"end\":44177,\"start\":44176},{\"end\":44191,\"start\":44185},{\"end\":44212,\"start\":44204},{\"end\":44224,\"start\":44221},{\"end\":44241,\"start\":44235},{\"end\":44253,\"start\":44249},{\"end\":44264,\"start\":44258},{\"end\":44282,\"start\":44275},{\"end\":44296,\"start\":44290},{\"end\":44306,\"start\":44303},{\"end\":44320,\"start\":44311},{\"end\":44329,\"start\":44327},{\"end\":44340,\"start\":44334},{\"end\":44926,\"start\":44920},{\"end\":44938,\"start\":44933},{\"end\":44952,\"start\":44945},{\"end\":44962,\"start\":44959},{\"end\":45328,\"start\":45323},{\"end\":45343,\"start\":45337},{\"end\":45361,\"start\":45354},{\"end\":45378,\"start\":45370},{\"end\":45806,\"start\":45803},{\"end\":46106,\"start\":46101},{\"end\":46131,\"start\":46114},{\"end\":46145,\"start\":46141},{\"end\":46507,\"start\":46499},{\"end\":46520,\"start\":46513},{\"end\":46532,\"start\":46526},{\"end\":46534,\"start\":46533},{\"end\":46548,\"start\":46541},{\"end\":46817,\"start\":46809},{\"end\":46837,\"start\":46831},{\"end\":46851,\"start\":46846},{\"end\":46868,\"start\":46860},{\"end\":47352,\"start\":47346},{\"end\":47369,\"start\":47362},{\"end\":47379,\"start\":47376},{\"end\":47803,\"start\":47798},{\"end\":47817,\"start\":47811},{\"end\":47829,\"start\":47827},{\"end\":47845,\"start\":47839},{\"end\":47865,\"start\":47856},{\"end\":48332,\"start\":48331},{\"end\":48341,\"start\":48340},{\"end\":48351,\"start\":48350},{\"end\":48353,\"start\":48352},{\"end\":48362,\"start\":48361},{\"end\":48779,\"start\":48778},{\"end\":48795,\"start\":48789},{\"end\":48812,\"start\":48804},{\"end\":48825,\"start\":48822},{\"end\":48837,\"start\":48836},{\"end\":49081,\"start\":49080},{\"end\":49083,\"start\":49082},{\"end\":49091,\"start\":49090},{\"end\":49093,\"start\":49092},{\"end\":49104,\"start\":49103},{\"end\":49426,\"start\":49419},{\"end\":49441,\"start\":49435},{\"end\":49456,\"start\":49449},{\"end\":49473,\"start\":49464},{\"end\":49490,\"start\":49483},{\"end\":49492,\"start\":49491},{\"end\":49976,\"start\":49970},{\"end\":49980,\"start\":49977},{\"end\":49994,\"start\":49989},{\"end\":50014,\"start\":50009},{\"end\":50283,\"start\":50277},{\"end\":50296,\"start\":50292},{\"end\":50308,\"start\":50305},{\"end\":50556,\"start\":50550},{\"end\":50568,\"start\":50565},{\"end\":50580,\"start\":50576},{\"end\":50808,\"start\":50807},{\"end\":50810,\"start\":50809},{\"end\":50820,\"start\":50819},{\"end\":50822,\"start\":50821},{\"end\":50832,\"start\":50831},{\"end\":50834,\"start\":50833},{\"end\":50844,\"start\":50843},{\"end\":50846,\"start\":50845},{\"end\":51104,\"start\":51096},{\"end\":51116,\"start\":51114},{\"end\":51135,\"start\":51127},{\"end\":51150,\"start\":51143},{\"end\":51168,\"start\":51161},{\"end\":51182,\"start\":51176},{\"end\":51184,\"start\":51183},{\"end\":51199,\"start\":51193},{\"end\":51854,\"start\":51846},{\"end\":51856,\"start\":51855},{\"end\":51872,\"start\":51862},{\"end\":51892,\"start\":51884},{\"end\":51907,\"start\":51901},{\"end\":52161,\"start\":52156},{\"end\":52175,\"start\":52167},{\"end\":52186,\"start\":52182},{\"end\":52190,\"start\":52187},{\"end\":52207,\"start\":52200},{\"end\":52209,\"start\":52208},{\"end\":52224,\"start\":52219},{\"end\":52226,\"start\":52225},{\"end\":52547,\"start\":52542},{\"end\":52552,\"start\":52548},{\"end\":52573,\"start\":52568},{\"end\":53002,\"start\":52996},{\"end\":53013,\"start\":53007},{\"end\":53023,\"start\":53019},{\"end\":53031,\"start\":53029},{\"end\":53044,\"start\":53038},{\"end\":53056,\"start\":53051},{\"end\":53068,\"start\":53065},{\"end\":53361,\"start\":53354},{\"end\":53370,\"start\":53366},{\"end\":53378,\"start\":53377},{\"end\":53392,\"start\":53386},{\"end\":53809,\"start\":53805},{\"end\":53812,\"start\":53810},{\"end\":53824,\"start\":53819},{\"end\":53838,\"start\":53832},{\"end\":53853,\"start\":53846},{\"end\":54105,\"start\":54098},{\"end\":54119,\"start\":54115},{\"end\":54131,\"start\":54130},{\"end\":54148,\"start\":54142},{\"end\":54163,\"start\":54156},{\"end\":54165,\"start\":54164},{\"end\":54697,\"start\":54696},{\"end\":54708,\"start\":54707},{\"end\":54715,\"start\":54714},{\"end\":54719,\"start\":54718},{\"end\":55177,\"start\":55170},{\"end\":55186,\"start\":55185},{\"end\":55197,\"start\":55189},{\"end\":55207,\"start\":55206},{\"end\":55219,\"start\":55210},{\"end\":55232,\"start\":55231},{\"end\":55239,\"start\":55235},{\"end\":55250,\"start\":55246},{\"end\":55267,\"start\":55260},{\"end\":55280,\"start\":55274},{\"end\":55292,\"start\":55289},{\"end\":55305,\"start\":55298},{\"end\":55322,\"start\":55314},{\"end\":55334,\"start\":55330},{\"end\":55348,\"start\":55343},{\"end\":55873,\"start\":55866},{\"end\":55885,\"start\":55884},{\"end\":55901,\"start\":55895},{\"end\":56365,\"start\":56361},{\"end\":56378,\"start\":56373},{\"end\":56400,\"start\":56396},{\"end\":56413,\"start\":56408},{\"end\":56428,\"start\":56421},{\"end\":56444,\"start\":56442},{\"end\":56454,\"start\":56451},{\"end\":56465,\"start\":56461},{\"end\":56480,\"start\":56473},{\"end\":56796,\"start\":56795},{\"end\":56806,\"start\":56805},{\"end\":56815,\"start\":56814},{\"end\":56826,\"start\":56825},{\"end\":56839,\"start\":56838},{\"end\":56849,\"start\":56848},{\"end\":57145,\"start\":57137},{\"end\":57165,\"start\":57156},{\"end\":57179,\"start\":57175},{\"end\":57194,\"start\":57190},{\"end\":57205,\"start\":57204},{\"end\":57207,\"start\":57206},{\"end\":57224,\"start\":57215},{\"end\":57239,\"start\":57232},{\"end\":57241,\"start\":57240},{\"end\":57784,\"start\":57780},{\"end\":57802,\"start\":57796},{\"end\":57816,\"start\":57813},{\"end\":57835,\"start\":57828},{\"end\":57849,\"start\":57844},{\"end\":57869,\"start\":57863},{\"end\":58406,\"start\":58399},{\"end\":58415,\"start\":58414},{\"end\":58426,\"start\":58423},{\"end\":58441,\"start\":58434},{\"end\":58460,\"start\":58452},{\"end\":58775,\"start\":58770},{\"end\":58785,\"start\":58782},{\"end\":58799,\"start\":58791},{\"end\":58811,\"start\":58804},{\"end\":58832,\"start\":58827},{\"end\":58845,\"start\":58838},{\"end\":58859,\"start\":58852},{\"end\":58888,\"start\":58881},{\"end\":58902,\"start\":58895},{\"end\":58919,\"start\":58915},{\"end\":58921,\"start\":58920},{\"end\":58937,\"start\":58931},{\"end\":58939,\"start\":58938},{\"end\":58953,\"start\":58949},{\"end\":58968,\"start\":58962},{\"end\":58977,\"start\":58975},{\"end\":58993,\"start\":58987},{\"end\":59565,\"start\":59564},{\"end\":59567,\"start\":59566},{\"end\":59580,\"start\":59576},{\"end\":59594,\"start\":59587},{\"end\":59611,\"start\":59603},{\"end\":59894,\"start\":59893},{\"end\":59908,\"start\":59907},{\"end\":59910,\"start\":59909},{\"end\":59919,\"start\":59918},{\"end\":60291,\"start\":60290},{\"end\":60300,\"start\":60299},{\"end\":60309,\"start\":60308},{\"end\":60311,\"start\":60310},{\"end\":60688,\"start\":60682},{\"end\":60702,\"start\":60695},{\"end\":60715,\"start\":60708},{\"end\":61100,\"start\":61091},{\"end\":61111,\"start\":61109},{\"end\":61123,\"start\":61119},{\"end\":61135,\"start\":61132},{\"end\":61445,\"start\":61436},{\"end\":61459,\"start\":61454},{\"end\":61737,\"start\":61736},{\"end\":61754,\"start\":61746},{\"end\":61756,\"start\":61755},{\"end\":62363,\"start\":62356},{\"end\":62379,\"start\":62373},{\"end\":62396,\"start\":62389},{\"end\":62413,\"start\":62407},{\"end\":62426,\"start\":62422},{\"end\":62876,\"start\":62871},{\"end\":62892,\"start\":62887},{\"end\":62906,\"start\":62899},{\"end\":62921,\"start\":62912},{\"end\":62933,\"start\":62928},{\"end\":62946,\"start\":62939},{\"end\":63321,\"start\":63314},{\"end\":63334,\"start\":63328},{\"end\":63346,\"start\":63339},{\"end\":63356,\"start\":63351},{\"end\":63370,\"start\":63362},{\"end\":63842,\"start\":63834},{\"end\":63854,\"start\":63849},{\"end\":63871,\"start\":63864},{\"end\":64322,\"start\":64321},{\"end\":64330,\"start\":64329},{\"end\":64338,\"start\":64337},{\"end\":64678,\"start\":64675},{\"end\":64685,\"start\":64684},{\"end\":64700,\"start\":64692},{\"end\":64717,\"start\":64708},{\"end\":64730,\"start\":64722},{\"end\":64742,\"start\":64736},{\"end\":65209,\"start\":65204},{\"end\":65222,\"start\":65217},{\"end\":65235,\"start\":65228},{\"end\":65246,\"start\":65240},{\"end\":65255,\"start\":65252},{\"end\":65269,\"start\":65261},{\"end\":65282,\"start\":65275},{\"end\":65295,\"start\":65288},{\"end\":65307,\"start\":65303},{\"end\":65316,\"start\":65314},{\"end\":65325,\"start\":65323},{\"end\":65335,\"start\":65330},{\"end\":65337,\"start\":65336},{\"end\":65353,\"start\":65345},{\"end\":65355,\"start\":65354},{\"end\":65367,\"start\":65364},{\"end\":65916,\"start\":65909},{\"end\":65927,\"start\":65921},{\"end\":65940,\"start\":65932},{\"end\":65953,\"start\":65945},{\"end\":65968,\"start\":65960},{\"end\":65981,\"start\":65975},{\"end\":66405,\"start\":66403},{\"end\":66416,\"start\":66412},{\"end\":66699,\"start\":66697},{\"end\":66710,\"start\":66706},{\"end\":66712,\"start\":66711},{\"end\":66979,\"start\":66978},{\"end\":66981,\"start\":66980},{\"end\":66991,\"start\":66990},{\"end\":66993,\"start\":66992},{\"end\":67003,\"start\":67002},{\"end\":67005,\"start\":67004},{\"end\":67303,\"start\":67301},{\"end\":67320,\"start\":67311},{\"end\":67333,\"start\":67329},{\"end\":67345,\"start\":67342},{\"end\":67615,\"start\":67610},{\"end\":67626,\"start\":67623},{\"end\":67640,\"start\":67634},{\"end\":67646,\"start\":67645},{\"end\":67660,\"start\":67656},{\"end\":67986,\"start\":67983},{\"end\":68001,\"start\":67994},{\"end\":68003,\"start\":68002},{\"end\":68015,\"start\":68011},{\"end\":68431,\"start\":68428},{\"end\":68446,\"start\":68439},{\"end\":68460,\"start\":68455},{\"end\":68471,\"start\":68470},{\"end\":68485,\"start\":68481},{\"end\":68967,\"start\":68965},{\"end\":68978,\"start\":68974},{\"end\":68992,\"start\":68983},{\"end\":69004,\"start\":68999},{\"end\":69013,\"start\":69009},{\"end\":69024,\"start\":69021},{\"end\":69393,\"start\":69388},{\"end\":69405,\"start\":69399},{\"end\":69421,\"start\":69412}]", "bib_author_last_name": "[{\"end\":40519,\"start\":40514},{\"end\":40536,\"start\":40527},{\"end\":40546,\"start\":40542},{\"end\":40567,\"start\":40560},{\"end\":40584,\"start\":40576},{\"end\":40597,\"start\":40592},{\"end\":40616,\"start\":40605},{\"end\":41064,\"start\":41057},{\"end\":41086,\"start\":41066},{\"end\":41303,\"start\":41297},{\"end\":41320,\"start\":41316},{\"end\":41904,\"start\":41899},{\"end\":41919,\"start\":41915},{\"end\":41939,\"start\":41929},{\"end\":41960,\"start\":41951},{\"end\":41972,\"start\":41965},{\"end\":41989,\"start\":41981},{\"end\":42494,\"start\":42488},{\"end\":42509,\"start\":42502},{\"end\":42525,\"start\":42519},{\"end\":42539,\"start\":42532},{\"end\":43117,\"start\":43114},{\"end\":43127,\"start\":43124},{\"end\":43148,\"start\":43140},{\"end\":43160,\"start\":43157},{\"end\":43169,\"start\":43167},{\"end\":43185,\"start\":43180},{\"end\":43672,\"start\":43668},{\"end\":43687,\"start\":43683},{\"end\":43702,\"start\":43696},{\"end\":43721,\"start\":43713},{\"end\":44183,\"start\":44178},{\"end\":44202,\"start\":44192},{\"end\":44219,\"start\":44213},{\"end\":44233,\"start\":44225},{\"end\":44247,\"start\":44242},{\"end\":44256,\"start\":44254},{\"end\":44273,\"start\":44265},{\"end\":44288,\"start\":44283},{\"end\":44301,\"start\":44297},{\"end\":44309,\"start\":44307},{\"end\":44325,\"start\":44321},{\"end\":44332,\"start\":44330},{\"end\":44343,\"start\":44341},{\"end\":44931,\"start\":44927},{\"end\":44943,\"start\":44939},{\"end\":44957,\"start\":44953},{\"end\":44967,\"start\":44963},{\"end\":45335,\"start\":45329},{\"end\":45352,\"start\":45344},{\"end\":45368,\"start\":45362},{\"end\":45392,\"start\":45379},{\"end\":45824,\"start\":45807},{\"end\":45833,\"start\":45826},{\"end\":46112,\"start\":46107},{\"end\":46139,\"start\":46132},{\"end\":46155,\"start\":46146},{\"end\":46511,\"start\":46508},{\"end\":46524,\"start\":46521},{\"end\":46539,\"start\":46535},{\"end\":46552,\"start\":46549},{\"end\":46829,\"start\":46818},{\"end\":46844,\"start\":46838},{\"end\":46858,\"start\":46852},{\"end\":46874,\"start\":46869},{\"end\":47360,\"start\":47353},{\"end\":47374,\"start\":47370},{\"end\":47388,\"start\":47380},{\"end\":47809,\"start\":47804},{\"end\":47825,\"start\":47818},{\"end\":47837,\"start\":47830},{\"end\":47854,\"start\":47846},{\"end\":47871,\"start\":47866},{\"end\":48338,\"start\":48333},{\"end\":48348,\"start\":48342},{\"end\":48359,\"start\":48354},{\"end\":48369,\"start\":48363},{\"end\":48787,\"start\":48780},{\"end\":48802,\"start\":48796},{\"end\":48820,\"start\":48813},{\"end\":48834,\"start\":48826},{\"end\":48844,\"start\":48838},{\"end\":49088,\"start\":49084},{\"end\":49101,\"start\":49094},{\"end\":49112,\"start\":49105},{\"end\":49433,\"start\":49427},{\"end\":49447,\"start\":49442},{\"end\":49462,\"start\":49457},{\"end\":49481,\"start\":49474},{\"end\":49498,\"start\":49493},{\"end\":49987,\"start\":49981},{\"end\":50007,\"start\":49995},{\"end\":50021,\"start\":50015},{\"end\":50290,\"start\":50284},{\"end\":50303,\"start\":50297},{\"end\":50314,\"start\":50309},{\"end\":50563,\"start\":50557},{\"end\":50574,\"start\":50569},{\"end\":50587,\"start\":50581},{\"end\":50817,\"start\":50811},{\"end\":50829,\"start\":50823},{\"end\":50841,\"start\":50835},{\"end\":51112,\"start\":51105},{\"end\":51125,\"start\":51117},{\"end\":51141,\"start\":51136},{\"end\":51159,\"start\":51151},{\"end\":51174,\"start\":51169},{\"end\":51191,\"start\":51185},{\"end\":51205,\"start\":51200},{\"end\":51860,\"start\":51857},{\"end\":51882,\"start\":51873},{\"end\":51899,\"start\":51893},{\"end\":51918,\"start\":51908},{\"end\":52165,\"start\":52162},{\"end\":52180,\"start\":52176},{\"end\":52198,\"start\":52191},{\"end\":52217,\"start\":52210},{\"end\":52234,\"start\":52227},{\"end\":52566,\"start\":52553},{\"end\":52578,\"start\":52574},{\"end\":52583,\"start\":52580},{\"end\":53005,\"start\":53003},{\"end\":53017,\"start\":53014},{\"end\":53027,\"start\":53024},{\"end\":53036,\"start\":53032},{\"end\":53049,\"start\":53045},{\"end\":53063,\"start\":53057},{\"end\":53071,\"start\":53069},{\"end\":53364,\"start\":53362},{\"end\":53375,\"start\":53371},{\"end\":53384,\"start\":53379},{\"end\":53397,\"start\":53393},{\"end\":53407,\"start\":53399},{\"end\":53817,\"start\":53813},{\"end\":53830,\"start\":53825},{\"end\":53844,\"start\":53839},{\"end\":53866,\"start\":53854},{\"end\":54113,\"start\":54106},{\"end\":54128,\"start\":54120},{\"end\":54140,\"start\":54132},{\"end\":54154,\"start\":54149},{\"end\":54175,\"start\":54166},{\"end\":54182,\"start\":54177},{\"end\":54705,\"start\":54698},{\"end\":54712,\"start\":54709},{\"end\":54724,\"start\":54720},{\"end\":55183,\"start\":55178},{\"end\":55204,\"start\":55198},{\"end\":55229,\"start\":55220},{\"end\":55244,\"start\":55240},{\"end\":55258,\"start\":55251},{\"end\":55272,\"start\":55268},{\"end\":55287,\"start\":55281},{\"end\":55296,\"start\":55293},{\"end\":55312,\"start\":55306},{\"end\":55328,\"start\":55323},{\"end\":55341,\"start\":55335},{\"end\":55354,\"start\":55349},{\"end\":55882,\"start\":55874},{\"end\":55893,\"start\":55886},{\"end\":55907,\"start\":55902},{\"end\":55915,\"start\":55909},{\"end\":56371,\"start\":56366},{\"end\":56394,\"start\":56379},{\"end\":56406,\"start\":56401},{\"end\":56419,\"start\":56414},{\"end\":56440,\"start\":56429},{\"end\":56449,\"start\":56445},{\"end\":56459,\"start\":56455},{\"end\":56471,\"start\":56466},{\"end\":56486,\"start\":56481},{\"end\":56803,\"start\":56797},{\"end\":56812,\"start\":56807},{\"end\":56823,\"start\":56816},{\"end\":56836,\"start\":56827},{\"end\":56846,\"start\":56840},{\"end\":56855,\"start\":56850},{\"end\":57154,\"start\":57146},{\"end\":57173,\"start\":57166},{\"end\":57188,\"start\":57180},{\"end\":57202,\"start\":57195},{\"end\":57213,\"start\":57208},{\"end\":57230,\"start\":57225},{\"end\":57249,\"start\":57242},{\"end\":57256,\"start\":57251},{\"end\":57794,\"start\":57785},{\"end\":57811,\"start\":57803},{\"end\":57826,\"start\":57817},{\"end\":57842,\"start\":57836},{\"end\":57861,\"start\":57850},{\"end\":57878,\"start\":57870},{\"end\":58412,\"start\":58407},{\"end\":58421,\"start\":58416},{\"end\":58432,\"start\":58427},{\"end\":58450,\"start\":58442},{\"end\":58467,\"start\":58461},{\"end\":58476,\"start\":58469},{\"end\":58780,\"start\":58776},{\"end\":58789,\"start\":58786},{\"end\":58802,\"start\":58800},{\"end\":58825,\"start\":58812},{\"end\":58836,\"start\":58833},{\"end\":58850,\"start\":58846},{\"end\":58879,\"start\":58860},{\"end\":58893,\"start\":58889},{\"end\":58913,\"start\":58903},{\"end\":58929,\"start\":58922},{\"end\":58947,\"start\":58940},{\"end\":58960,\"start\":58954},{\"end\":58973,\"start\":58969},{\"end\":58985,\"start\":58978},{\"end\":59002,\"start\":58994},{\"end\":59574,\"start\":59568},{\"end\":59585,\"start\":59581},{\"end\":59601,\"start\":59595},{\"end\":59621,\"start\":59612},{\"end\":59631,\"start\":59623},{\"end\":59905,\"start\":59895},{\"end\":59916,\"start\":59911},{\"end\":59925,\"start\":59920},{\"end\":60297,\"start\":60292},{\"end\":60306,\"start\":60301},{\"end\":60317,\"start\":60312},{\"end\":60693,\"start\":60689},{\"end\":60706,\"start\":60703},{\"end\":60719,\"start\":60716},{\"end\":61107,\"start\":61101},{\"end\":61117,\"start\":61112},{\"end\":61130,\"start\":61124},{\"end\":61141,\"start\":61136},{\"end\":61452,\"start\":61446},{\"end\":61464,\"start\":61460},{\"end\":61744,\"start\":61738},{\"end\":61763,\"start\":61757},{\"end\":61771,\"start\":61765},{\"end\":62371,\"start\":62364},{\"end\":62387,\"start\":62380},{\"end\":62405,\"start\":62397},{\"end\":62420,\"start\":62414},{\"end\":62437,\"start\":62427},{\"end\":62885,\"start\":62877},{\"end\":62897,\"start\":62893},{\"end\":62910,\"start\":62907},{\"end\":62926,\"start\":62922},{\"end\":62937,\"start\":62934},{\"end\":62950,\"start\":62947},{\"end\":63326,\"start\":63322},{\"end\":63337,\"start\":63335},{\"end\":63349,\"start\":63347},{\"end\":63360,\"start\":63357},{\"end\":63375,\"start\":63371},{\"end\":63847,\"start\":63843},{\"end\":63862,\"start\":63855},{\"end\":63877,\"start\":63872},{\"end\":64327,\"start\":64323},{\"end\":64335,\"start\":64331},{\"end\":64342,\"start\":64339},{\"end\":64682,\"start\":64679},{\"end\":64690,\"start\":64686},{\"end\":64706,\"start\":64701},{\"end\":64720,\"start\":64718},{\"end\":64734,\"start\":64731},{\"end\":64748,\"start\":64743},{\"end\":64758,\"start\":64750},{\"end\":65215,\"start\":65210},{\"end\":65226,\"start\":65223},{\"end\":65238,\"start\":65236},{\"end\":65250,\"start\":65247},{\"end\":65259,\"start\":65256},{\"end\":65273,\"start\":65270},{\"end\":65286,\"start\":65283},{\"end\":65301,\"start\":65296},{\"end\":65312,\"start\":65308},{\"end\":65321,\"start\":65317},{\"end\":65328,\"start\":65326},{\"end\":65343,\"start\":65338},{\"end\":65362,\"start\":65356},{\"end\":65370,\"start\":65368},{\"end\":65919,\"start\":65917},{\"end\":65930,\"start\":65928},{\"end\":65943,\"start\":65941},{\"end\":65958,\"start\":65954},{\"end\":65973,\"start\":65969},{\"end\":65989,\"start\":65982},{\"end\":66410,\"start\":66406},{\"end\":66423,\"start\":66417},{\"end\":66704,\"start\":66700},{\"end\":66719,\"start\":66713},{\"end\":66988,\"start\":66982},{\"end\":67000,\"start\":66994},{\"end\":67011,\"start\":67006},{\"end\":67309,\"start\":67304},{\"end\":67327,\"start\":67321},{\"end\":67340,\"start\":67334},{\"end\":67351,\"start\":67346},{\"end\":67621,\"start\":67616},{\"end\":67632,\"start\":67627},{\"end\":67643,\"start\":67641},{\"end\":67654,\"start\":67647},{\"end\":67666,\"start\":67661},{\"end\":67672,\"start\":67668},{\"end\":67992,\"start\":67987},{\"end\":68009,\"start\":68004},{\"end\":68020,\"start\":68016},{\"end\":68437,\"start\":68432},{\"end\":68453,\"start\":68447},{\"end\":68468,\"start\":68461},{\"end\":68479,\"start\":68472},{\"end\":68491,\"start\":68486},{\"end\":68497,\"start\":68493},{\"end\":68972,\"start\":68968},{\"end\":68981,\"start\":68979},{\"end\":68997,\"start\":68993},{\"end\":69007,\"start\":69005},{\"end\":69019,\"start\":69014},{\"end\":69027,\"start\":69025},{\"end\":69397,\"start\":69394},{\"end\":69410,\"start\":69406},{\"end\":69425,\"start\":69422}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":233204613},\"end\":41024,\"start\":40441},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14614315},\"end\":41209,\"start\":41026},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7621556},\"end\":41827,\"start\":41211},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":9854676},\"end\":42406,\"start\":41829},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1820089},\"end\":43054,\"start\":42408},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":215810902},\"end\":43473,\"start\":43056},{\"attributes\":{\"id\":\"b6\"},\"end\":43571,\"start\":43475},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b7\",\"matched_paper_id\":204509212},\"end\":44117,\"start\":43573},{\"attributes\":{\"doi\":\"arXiv:1512.03012\",\"id\":\"b8\"},\"end\":44837,\"start\":44119},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":201103634},\"end\":45282,\"start\":44839},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":102352622},\"end\":45768,\"start\":45284},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":216078090},\"end\":46035,\"start\":45770},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":11492613},\"end\":46408,\"start\":46037},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":204731777},\"end\":46762,\"start\":46410},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":128024},\"end\":47315,\"start\":46764},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1201984},\"end\":47713,\"start\":47317},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4461350},\"end\":48286,\"start\":47715},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6106986},\"end\":48712,\"start\":48288},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":9123693},\"end\":49008,\"start\":48714},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":206799161},\"end\":49359,\"start\":49010},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":229340301},\"end\":49887,\"start\":49361},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":155100335},\"end\":50220,\"start\":49889},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7261259},\"end\":50478,\"start\":50222},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":18149328},\"end\":50761,\"start\":50480},{\"attributes\":{\"id\":\"b24\"},\"end\":51010,\"start\":50763},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3661442},\"end\":51802,\"start\":51012},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":15138051},\"end\":52090,\"start\":51804},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":376779},\"end\":52455,\"start\":52092},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":13955492},\"end\":52933,\"start\":52457},{\"attributes\":{\"doi\":\"arXiv:2008.08171\",\"id\":\"b29\"},\"end\":53285,\"start\":52935},{\"attributes\":{\"doi\":\"2021. 3\",\"id\":\"b30\",\"matched_paper_id\":231662465},\"end\":53762,\"start\":53287},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":221105236},\"end\":54044,\"start\":53764},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":102351100},\"end\":54575,\"start\":54046},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":182952939},\"end\":55122,\"start\":54577},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":91184540},\"end\":55804,\"start\":55124},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":645845},\"end\":56280,\"start\":55806},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":219708692},\"end\":56757,\"start\":56282},{\"attributes\":{\"doi\":\"CG-2007-2\",\"id\":\"b37\"},\"end\":57064,\"start\":56759},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":109932872},\"end\":57686,\"start\":57066},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":46938830},\"end\":58337,\"start\":57688},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":7509088},\"end\":58681,\"start\":58339},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":227347434},\"end\":59511,\"start\":58683},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":13555112},\"end\":59817,\"start\":59513},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":6017383},\"end\":60165,\"start\":59819},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":11279201},\"end\":60596,\"start\":60167},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":13936837},\"end\":61034,\"start\":60598},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":207984929},\"end\":61337,\"start\":61036},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":221105756},\"end\":61656,\"start\":61339},{\"attributes\":{\"doi\":\"chinery. 3\",\"id\":\"b48\",\"matched_paper_id\":718390},\"end\":62279,\"start\":61658},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":52271809},\"end\":62803,\"start\":62281},{\"attributes\":{\"doi\":\"PMLR, 2017. 3\",\"id\":\"b50\",\"matched_paper_id\":15117981},\"end\":63243,\"start\":62805},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":228083961},\"end\":63774,\"start\":63245},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":4709722},\"end\":64218,\"start\":63776},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":49395735},\"end\":64618,\"start\":64220},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":49358881},\"end\":65146,\"start\":64620},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":213175506},\"end\":65851,\"start\":65148},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":221266014},\"end\":66332,\"start\":65853},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":212747811},\"end\":66628,\"start\":66334},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":195886377},\"end\":66940,\"start\":66630},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":9922492},\"end\":67239,\"start\":66942},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":51692385},\"end\":67534,\"start\":67241},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":224413564},\"end\":67921,\"start\":67536},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":227238680},\"end\":68379,\"start\":67923},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":208857374},\"end\":68882,\"start\":68381},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":49669430},\"end\":69308,\"start\":68884},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":457232},\"end\":69780,\"start\":69310}]", "bib_title": "[{\"end\":40507,\"start\":40441},{\"end\":41047,\"start\":41026},{\"end\":41269,\"start\":41211},{\"end\":41887,\"start\":41829},{\"end\":42481,\"start\":42408},{\"end\":43108,\"start\":43056},{\"end\":43659,\"start\":43573},{\"end\":44918,\"start\":44839},{\"end\":45321,\"start\":45284},{\"end\":45801,\"start\":45770},{\"end\":46099,\"start\":46037},{\"end\":46497,\"start\":46410},{\"end\":46807,\"start\":46764},{\"end\":47344,\"start\":47317},{\"end\":47796,\"start\":47715},{\"end\":48329,\"start\":48288},{\"end\":48776,\"start\":48714},{\"end\":49078,\"start\":49010},{\"end\":49417,\"start\":49361},{\"end\":49968,\"start\":49889},{\"end\":50275,\"start\":50222},{\"end\":50548,\"start\":50480},{\"end\":50805,\"start\":50763},{\"end\":51094,\"start\":51012},{\"end\":51844,\"start\":51804},{\"end\":52154,\"start\":52092},{\"end\":52540,\"start\":52457},{\"end\":53352,\"start\":53287},{\"end\":53803,\"start\":53764},{\"end\":54096,\"start\":54046},{\"end\":54694,\"start\":54577},{\"end\":55168,\"start\":55124},{\"end\":55864,\"start\":55806},{\"end\":56359,\"start\":56282},{\"end\":57135,\"start\":57066},{\"end\":57778,\"start\":57688},{\"end\":58397,\"start\":58339},{\"end\":58768,\"start\":58683},{\"end\":59562,\"start\":59513},{\"end\":59891,\"start\":59819},{\"end\":60288,\"start\":60167},{\"end\":60680,\"start\":60598},{\"end\":61089,\"start\":61036},{\"end\":61434,\"start\":61339},{\"end\":61734,\"start\":61658},{\"end\":62354,\"start\":62281},{\"end\":62869,\"start\":62805},{\"end\":63312,\"start\":63245},{\"end\":63832,\"start\":63776},{\"end\":64319,\"start\":64220},{\"end\":64673,\"start\":64620},{\"end\":65202,\"start\":65148},{\"end\":65907,\"start\":65853},{\"end\":66401,\"start\":66334},{\"end\":66695,\"start\":66630},{\"end\":66976,\"start\":66942},{\"end\":67299,\"start\":67241},{\"end\":67608,\"start\":67536},{\"end\":67981,\"start\":67923},{\"end\":68426,\"start\":68381},{\"end\":68963,\"start\":68884},{\"end\":69386,\"start\":69310}]", "bib_author": "[{\"end\":40521,\"start\":40509},{\"end\":40538,\"start\":40521},{\"end\":40548,\"start\":40538},{\"end\":40569,\"start\":40548},{\"end\":40586,\"start\":40569},{\"end\":40599,\"start\":40586},{\"end\":40618,\"start\":40599},{\"end\":41066,\"start\":41049},{\"end\":41088,\"start\":41066},{\"end\":41292,\"start\":41271},{\"end\":41305,\"start\":41292},{\"end\":41322,\"start\":41305},{\"end\":41906,\"start\":41889},{\"end\":41921,\"start\":41906},{\"end\":41941,\"start\":41921},{\"end\":41962,\"start\":41941},{\"end\":41974,\"start\":41962},{\"end\":41991,\"start\":41974},{\"end\":42496,\"start\":42483},{\"end\":42511,\"start\":42496},{\"end\":42527,\"start\":42511},{\"end\":42541,\"start\":42527},{\"end\":43119,\"start\":43110},{\"end\":43129,\"start\":43119},{\"end\":43150,\"start\":43129},{\"end\":43162,\"start\":43150},{\"end\":43171,\"start\":43162},{\"end\":43187,\"start\":43171},{\"end\":43674,\"start\":43661},{\"end\":43689,\"start\":43674},{\"end\":43704,\"start\":43689},{\"end\":43723,\"start\":43704},{\"end\":44185,\"start\":44170},{\"end\":44204,\"start\":44185},{\"end\":44221,\"start\":44204},{\"end\":44235,\"start\":44221},{\"end\":44249,\"start\":44235},{\"end\":44258,\"start\":44249},{\"end\":44275,\"start\":44258},{\"end\":44290,\"start\":44275},{\"end\":44303,\"start\":44290},{\"end\":44311,\"start\":44303},{\"end\":44327,\"start\":44311},{\"end\":44334,\"start\":44327},{\"end\":44345,\"start\":44334},{\"end\":44933,\"start\":44920},{\"end\":44945,\"start\":44933},{\"end\":44959,\"start\":44945},{\"end\":44969,\"start\":44959},{\"end\":45337,\"start\":45323},{\"end\":45354,\"start\":45337},{\"end\":45370,\"start\":45354},{\"end\":45394,\"start\":45370},{\"end\":45826,\"start\":45803},{\"end\":45835,\"start\":45826},{\"end\":46114,\"start\":46101},{\"end\":46141,\"start\":46114},{\"end\":46157,\"start\":46141},{\"end\":46513,\"start\":46499},{\"end\":46526,\"start\":46513},{\"end\":46541,\"start\":46526},{\"end\":46554,\"start\":46541},{\"end\":46831,\"start\":46809},{\"end\":46846,\"start\":46831},{\"end\":46860,\"start\":46846},{\"end\":46876,\"start\":46860},{\"end\":47362,\"start\":47346},{\"end\":47376,\"start\":47362},{\"end\":47390,\"start\":47376},{\"end\":47811,\"start\":47798},{\"end\":47827,\"start\":47811},{\"end\":47839,\"start\":47827},{\"end\":47856,\"start\":47839},{\"end\":47873,\"start\":47856},{\"end\":48340,\"start\":48331},{\"end\":48350,\"start\":48340},{\"end\":48361,\"start\":48350},{\"end\":48371,\"start\":48361},{\"end\":48789,\"start\":48778},{\"end\":48804,\"start\":48789},{\"end\":48822,\"start\":48804},{\"end\":48836,\"start\":48822},{\"end\":48846,\"start\":48836},{\"end\":49090,\"start\":49080},{\"end\":49103,\"start\":49090},{\"end\":49114,\"start\":49103},{\"end\":49435,\"start\":49419},{\"end\":49449,\"start\":49435},{\"end\":49464,\"start\":49449},{\"end\":49483,\"start\":49464},{\"end\":49500,\"start\":49483},{\"end\":49989,\"start\":49970},{\"end\":50009,\"start\":49989},{\"end\":50023,\"start\":50009},{\"end\":50292,\"start\":50277},{\"end\":50305,\"start\":50292},{\"end\":50316,\"start\":50305},{\"end\":50565,\"start\":50550},{\"end\":50576,\"start\":50565},{\"end\":50589,\"start\":50576},{\"end\":50819,\"start\":50807},{\"end\":50831,\"start\":50819},{\"end\":50843,\"start\":50831},{\"end\":50849,\"start\":50843},{\"end\":51114,\"start\":51096},{\"end\":51127,\"start\":51114},{\"end\":51143,\"start\":51127},{\"end\":51161,\"start\":51143},{\"end\":51176,\"start\":51161},{\"end\":51193,\"start\":51176},{\"end\":51207,\"start\":51193},{\"end\":51862,\"start\":51846},{\"end\":51884,\"start\":51862},{\"end\":51901,\"start\":51884},{\"end\":51920,\"start\":51901},{\"end\":52167,\"start\":52156},{\"end\":52182,\"start\":52167},{\"end\":52200,\"start\":52182},{\"end\":52219,\"start\":52200},{\"end\":52236,\"start\":52219},{\"end\":52568,\"start\":52542},{\"end\":52580,\"start\":52568},{\"end\":52585,\"start\":52580},{\"end\":53007,\"start\":52996},{\"end\":53019,\"start\":53007},{\"end\":53029,\"start\":53019},{\"end\":53038,\"start\":53029},{\"end\":53051,\"start\":53038},{\"end\":53065,\"start\":53051},{\"end\":53073,\"start\":53065},{\"end\":53366,\"start\":53354},{\"end\":53377,\"start\":53366},{\"end\":53386,\"start\":53377},{\"end\":53399,\"start\":53386},{\"end\":53409,\"start\":53399},{\"end\":53819,\"start\":53805},{\"end\":53832,\"start\":53819},{\"end\":53846,\"start\":53832},{\"end\":53868,\"start\":53846},{\"end\":54115,\"start\":54098},{\"end\":54130,\"start\":54115},{\"end\":54142,\"start\":54130},{\"end\":54156,\"start\":54142},{\"end\":54177,\"start\":54156},{\"end\":54184,\"start\":54177},{\"end\":54707,\"start\":54696},{\"end\":54714,\"start\":54707},{\"end\":54718,\"start\":54714},{\"end\":54726,\"start\":54718},{\"end\":55185,\"start\":55170},{\"end\":55189,\"start\":55185},{\"end\":55206,\"start\":55189},{\"end\":55210,\"start\":55206},{\"end\":55231,\"start\":55210},{\"end\":55235,\"start\":55231},{\"end\":55246,\"start\":55235},{\"end\":55260,\"start\":55246},{\"end\":55274,\"start\":55260},{\"end\":55289,\"start\":55274},{\"end\":55298,\"start\":55289},{\"end\":55314,\"start\":55298},{\"end\":55330,\"start\":55314},{\"end\":55343,\"start\":55330},{\"end\":55356,\"start\":55343},{\"end\":55884,\"start\":55866},{\"end\":55895,\"start\":55884},{\"end\":55909,\"start\":55895},{\"end\":55917,\"start\":55909},{\"end\":56373,\"start\":56361},{\"end\":56396,\"start\":56373},{\"end\":56408,\"start\":56396},{\"end\":56421,\"start\":56408},{\"end\":56442,\"start\":56421},{\"end\":56451,\"start\":56442},{\"end\":56461,\"start\":56451},{\"end\":56473,\"start\":56461},{\"end\":56488,\"start\":56473},{\"end\":56805,\"start\":56795},{\"end\":56814,\"start\":56805},{\"end\":56825,\"start\":56814},{\"end\":56838,\"start\":56825},{\"end\":56848,\"start\":56838},{\"end\":56857,\"start\":56848},{\"end\":57156,\"start\":57137},{\"end\":57175,\"start\":57156},{\"end\":57190,\"start\":57175},{\"end\":57204,\"start\":57190},{\"end\":57215,\"start\":57204},{\"end\":57232,\"start\":57215},{\"end\":57251,\"start\":57232},{\"end\":57258,\"start\":57251},{\"end\":57796,\"start\":57780},{\"end\":57813,\"start\":57796},{\"end\":57828,\"start\":57813},{\"end\":57844,\"start\":57828},{\"end\":57863,\"start\":57844},{\"end\":57880,\"start\":57863},{\"end\":58414,\"start\":58399},{\"end\":58423,\"start\":58414},{\"end\":58434,\"start\":58423},{\"end\":58452,\"start\":58434},{\"end\":58469,\"start\":58452},{\"end\":58478,\"start\":58469},{\"end\":58782,\"start\":58770},{\"end\":58791,\"start\":58782},{\"end\":58804,\"start\":58791},{\"end\":58827,\"start\":58804},{\"end\":58838,\"start\":58827},{\"end\":58852,\"start\":58838},{\"end\":58881,\"start\":58852},{\"end\":58895,\"start\":58881},{\"end\":58915,\"start\":58895},{\"end\":58931,\"start\":58915},{\"end\":58949,\"start\":58931},{\"end\":58962,\"start\":58949},{\"end\":58975,\"start\":58962},{\"end\":58987,\"start\":58975},{\"end\":59004,\"start\":58987},{\"end\":59576,\"start\":59564},{\"end\":59587,\"start\":59576},{\"end\":59603,\"start\":59587},{\"end\":59623,\"start\":59603},{\"end\":59633,\"start\":59623},{\"end\":59907,\"start\":59893},{\"end\":59918,\"start\":59907},{\"end\":59927,\"start\":59918},{\"end\":60299,\"start\":60290},{\"end\":60308,\"start\":60299},{\"end\":60319,\"start\":60308},{\"end\":60695,\"start\":60682},{\"end\":60708,\"start\":60695},{\"end\":60721,\"start\":60708},{\"end\":61109,\"start\":61091},{\"end\":61119,\"start\":61109},{\"end\":61132,\"start\":61119},{\"end\":61143,\"start\":61132},{\"end\":61454,\"start\":61436},{\"end\":61466,\"start\":61454},{\"end\":61746,\"start\":61736},{\"end\":61765,\"start\":61746},{\"end\":61773,\"start\":61765},{\"end\":62373,\"start\":62356},{\"end\":62389,\"start\":62373},{\"end\":62407,\"start\":62389},{\"end\":62422,\"start\":62407},{\"end\":62439,\"start\":62422},{\"end\":62887,\"start\":62871},{\"end\":62899,\"start\":62887},{\"end\":62912,\"start\":62899},{\"end\":62928,\"start\":62912},{\"end\":62939,\"start\":62928},{\"end\":62952,\"start\":62939},{\"end\":63328,\"start\":63314},{\"end\":63339,\"start\":63328},{\"end\":63351,\"start\":63339},{\"end\":63362,\"start\":63351},{\"end\":63377,\"start\":63362},{\"end\":63849,\"start\":63834},{\"end\":63864,\"start\":63849},{\"end\":63879,\"start\":63864},{\"end\":64329,\"start\":64321},{\"end\":64337,\"start\":64329},{\"end\":64344,\"start\":64337},{\"end\":64684,\"start\":64675},{\"end\":64692,\"start\":64684},{\"end\":64708,\"start\":64692},{\"end\":64722,\"start\":64708},{\"end\":64736,\"start\":64722},{\"end\":64750,\"start\":64736},{\"end\":64760,\"start\":64750},{\"end\":65217,\"start\":65204},{\"end\":65228,\"start\":65217},{\"end\":65240,\"start\":65228},{\"end\":65252,\"start\":65240},{\"end\":65261,\"start\":65252},{\"end\":65275,\"start\":65261},{\"end\":65288,\"start\":65275},{\"end\":65303,\"start\":65288},{\"end\":65314,\"start\":65303},{\"end\":65323,\"start\":65314},{\"end\":65330,\"start\":65323},{\"end\":65345,\"start\":65330},{\"end\":65364,\"start\":65345},{\"end\":65372,\"start\":65364},{\"end\":65921,\"start\":65909},{\"end\":65932,\"start\":65921},{\"end\":65945,\"start\":65932},{\"end\":65960,\"start\":65945},{\"end\":65975,\"start\":65960},{\"end\":65991,\"start\":65975},{\"end\":66412,\"start\":66403},{\"end\":66425,\"start\":66412},{\"end\":66706,\"start\":66697},{\"end\":66721,\"start\":66706},{\"end\":66990,\"start\":66978},{\"end\":67002,\"start\":66990},{\"end\":67013,\"start\":67002},{\"end\":67311,\"start\":67301},{\"end\":67329,\"start\":67311},{\"end\":67342,\"start\":67329},{\"end\":67353,\"start\":67342},{\"end\":67623,\"start\":67610},{\"end\":67634,\"start\":67623},{\"end\":67645,\"start\":67634},{\"end\":67656,\"start\":67645},{\"end\":67668,\"start\":67656},{\"end\":67674,\"start\":67668},{\"end\":67994,\"start\":67983},{\"end\":68011,\"start\":67994},{\"end\":68022,\"start\":68011},{\"end\":68439,\"start\":68428},{\"end\":68455,\"start\":68439},{\"end\":68470,\"start\":68455},{\"end\":68481,\"start\":68470},{\"end\":68493,\"start\":68481},{\"end\":68499,\"start\":68493},{\"end\":68974,\"start\":68965},{\"end\":68983,\"start\":68974},{\"end\":68999,\"start\":68983},{\"end\":69009,\"start\":68999},{\"end\":69021,\"start\":69009},{\"end\":69029,\"start\":69021},{\"end\":69399,\"start\":69388},{\"end\":69412,\"start\":69399},{\"end\":69427,\"start\":69412}]", "bib_venue": "[{\"end\":40692,\"start\":40618},{\"end\":41104,\"start\":41088},{\"end\":41412,\"start\":41322},{\"end\":42067,\"start\":41991},{\"end\":42630,\"start\":42541},{\"end\":43232,\"start\":43187},{\"end\":43494,\"start\":43477},{\"end\":43774,\"start\":43727},{\"end\":44168,\"start\":44119},{\"end\":45030,\"start\":44969},{\"end\":45475,\"start\":45394},{\"end\":45892,\"start\":45835},{\"end\":46214,\"start\":46157},{\"end\":46570,\"start\":46554},{\"end\":46960,\"start\":46876},{\"end\":47466,\"start\":47390},{\"end\":47949,\"start\":47873},{\"end\":48447,\"start\":48371},{\"end\":48850,\"start\":48846},{\"end\":49166,\"start\":49114},{\"end\":49576,\"start\":49500},{\"end\":50039,\"start\":50023},{\"end\":50332,\"start\":50316},{\"end\":50605,\"start\":50589},{\"end\":50867,\"start\":50849},{\"end\":51299,\"start\":51207},{\"end\":51936,\"start\":51920},{\"end\":52252,\"start\":52236},{\"end\":52623,\"start\":52585},{\"end\":52994,\"start\":52935},{\"end\":53489,\"start\":53416},{\"end\":53884,\"start\":53868},{\"end\":54258,\"start\":54184},{\"end\":54802,\"start\":54726},{\"end\":55430,\"start\":55356},{\"end\":55993,\"start\":55917},{\"end\":56504,\"start\":56488},{\"end\":56793,\"start\":56759},{\"end\":57334,\"start\":57258},{\"end\":57961,\"start\":57880},{\"end\":58494,\"start\":58478},{\"end\":59076,\"start\":59004},{\"end\":59649,\"start\":59633},{\"end\":59967,\"start\":59927},{\"end\":60359,\"start\":60319},{\"end\":60770,\"start\":60721},{\"end\":61159,\"start\":61143},{\"end\":61482,\"start\":61466},{\"end\":61868,\"start\":61783},{\"end\":62498,\"start\":62439},{\"end\":63009,\"start\":62965},{\"end\":63458,\"start\":63377},{\"end\":63955,\"start\":63879},{\"end\":64400,\"start\":64344},{\"end\":64836,\"start\":64760},{\"end\":65453,\"start\":65372},{\"end\":66036,\"start\":65991},{\"end\":66470,\"start\":66425},{\"end\":66773,\"start\":66721},{\"end\":67070,\"start\":67013},{\"end\":67369,\"start\":67353},{\"end\":67717,\"start\":67674},{\"end\":68098,\"start\":68022},{\"end\":68580,\"start\":68499},{\"end\":69086,\"start\":69029},{\"end\":69503,\"start\":69427},{\"end\":40753,\"start\":40694},{\"end\":41506,\"start\":41414},{\"end\":42133,\"start\":42069},{\"end\":42724,\"start\":42632},{\"end\":43808,\"start\":43776},{\"end\":45078,\"start\":45032},{\"end\":45543,\"start\":45477},{\"end\":47034,\"start\":46962},{\"end\":47532,\"start\":47468},{\"end\":48015,\"start\":47951},{\"end\":48513,\"start\":48449},{\"end\":49642,\"start\":49578},{\"end\":51395,\"start\":51301},{\"end\":52642,\"start\":52625},{\"end\":53549,\"start\":53491},{\"end\":54319,\"start\":54260},{\"end\":54868,\"start\":54804},{\"end\":55491,\"start\":55432},{\"end\":56059,\"start\":55995},{\"end\":57400,\"start\":57336},{\"end\":58029,\"start\":57963},{\"end\":61957,\"start\":61870},{\"end\":62544,\"start\":62500},{\"end\":63526,\"start\":63460},{\"end\":64021,\"start\":63957},{\"end\":64902,\"start\":64838},{\"end\":65521,\"start\":65455},{\"end\":66042,\"start\":66038},{\"end\":68164,\"start\":68100},{\"end\":68648,\"start\":68582},{\"end\":69569,\"start\":69505}]"}}}, "year": 2023, "month": 12, "day": 17}