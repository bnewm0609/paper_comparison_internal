{"id": 220903131, "updated": "2022-10-06 14:10:34.211", "metadata": {"title": "Optimizing Federated Learning on Non-IID Data with Reinforcement Learning", "authors": "[{\"first\":\"Hao\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Zakhary\",\"last\":\"Kaplan\",\"middle\":[]},{\"first\":\"Di\",\"last\":\"Niu\",\"middle\":[]},{\"first\":\"Baochun\",\"last\":\"Li\",\"middle\":[]}]", "venue": "IEEE INFOCOM 2020 - IEEE Conference on Computer Communications", "journal": "IEEE INFOCOM 2020 - IEEE Conference on Computer Communications", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "The widespread deployment of machine learning applications in ubiquitous environments has sparked interests in exploiting the vast amount of data stored on mobile devices. To preserve data privacy, Federated Learning has been proposed to learn a shared model by performing distributed training locally on participating devices and aggregating the local models into a global one. However, due to the limited network connectivity of mobile devices, it is not practical for federated learning to perform model updates and aggregation on all participating devices in parallel. Besides, data samples across all devices are usually not independent and identically distributed (IID), posing additional challenges to the convergence and speed of federated learning. In this paper, we propose Favor, an experience-driven control framework that intelligently chooses the client devices to participate in each round of federated learning to counterbalance the bias introduced by non-IID data and to speed up convergence. Through both empirical and mathematical analysis, we observe an implicit connection between the distribution of training data on a device and the model weights trained based on those data, which enables us to profile the data distribution on that device based on its uploaded model weights. We then propose a mechanism based on deep Q-learning that learns to select a subset of devices in each communication round to maximize a reward that encourages the increase of validation accuracy and penalizes the use of more communication rounds. With extensive experiments performed in PyTorch, we show that the number of communication rounds required in federated learning can be reduced by up to 49% on the MNIST dataset, 23% on FashionMNIST, and 42% on CIFAR-10, as compared to the Federated Averaging algorithm.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3047304572", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/infocom/WangKNL20", "doi": "10.1109/infocom41043.2020.9155494"}}, "content": {"source": {"pdf_hash": "84c5c8d24802f63d2764743df6627210aecfe4da", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "c84bfd2b465044e44898609a1acf8e40442ebf0c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/84c5c8d24802f63d2764743df6627210aecfe4da.txt", "contents": "\nOptimizing Federated Learning on Non-IID Data with Reinforcement Learning\n\n\nHao Wang haowang@ece.utoronto.ca \nUniversity of Toronto\n\n\nZakhary Kaplan zakhary.kaplan@mail.utoronto.ca \nUniversity of Toronto\n\n\nDi Niu dniu@ualberta.ca \nUniversity of Alberta\n\n\nBaochun Li \nUniversity of Toronto\n\n\nOptimizing Federated Learning on Non-IID Data with Reinforcement Learning\n\naccelerate and stabilize the federated learning process by learning to actively select the best subset of devices in each communication round that can counterbalance the bias introduced by non-IID data.Since privacy concerns have ruled out any possibility of accessing the raw data on each device, it is impossible to profile the data distribution on each device directly. However, we observe that there is an implicit connection between the distribution of the training samples on a device and the model weights trained based on those samples. Therefore, our main intuition is to use the local model weights and the shared global model as states to judiciously select devices that may contribute to global model improvement. We propose a reinforcement learning agent based on a Deep Q-Network (DQN), which is trained through a Double DQN for increased efficiency and robustness. We carefully design the reward signal in order to aggressively improve the global model accuracy per round as well as to reduce the total number of communication rounds required. We also propose a practical scheme that compresses model weights to reduce the high dimensionality of the state space, especially for big models, e.g., deep neural network models.We have implemented FAVOR in a federated learning simulator developed from scratch using PyTorch 1 , and evaluated it under a variety of federated learning tasks. Our experimental results on the MNIST, FashionMNIST, and CIFAR-10 datasets have shown that FAVOR can reduce the required number of communication rounds in federated learning by up to 49% on the MNIST, by up to 23% on FashionMNIST, and by up to 42% on CIFAR-10, as compared to the FEDAVG algorithm.II. BACKGROUND AND MOTIVATION\n\nAbstract-The widespread deployment of machine learning applications in ubiquitous environments has sparked interests in exploiting the vast amount of data stored on mobile devices. To preserve data privacy, Federated Learning has been proposed to learn a shared model by performing distributed training locally on participating devices and aggregating the local models into a global one. However, due to the limited network connectivity of mobile devices, it is not practical for federated learning to perform model updates and aggregation on all participating devices in parallel. Besides, data samples across all devices are usually not independent and identically distributed (IID), posing additional challenges to the convergence and speed of federated learning.\n\nIn this paper, we propose FAVOR, an experience-driven control framework that intelligently chooses the client devices to participate in each round of federated learning to counterbalance the bias introduced by non-IID data and to speed up convergence. Through both empirical and mathematical analysis, we observe an implicit connection between the distribution of training data on a device and the model weights trained based on those data, which enables us to profile the data distribution on that device based on its uploaded model weights. We then propose a mechanism based on deep Q-learning that learns to select a subset of devices in each communication round to maximize a reward that encourages the increase of validation accuracy and penalizes the use of more communication rounds. With extensive experiments performed in PyTorch, we show that the number of communication rounds required in federated learning can be reduced by up to 49% on the MNIST dataset, 23% on FashionMNIST, and 42% on CIFAR-10, as compared to the Federated Averaging algorithm.\n\n\nI. INTRODUCTION\n\nAs the primary computing resource for billions of users, mobile devices are constantly generating massive volumes of data, such as photos, voices, and keystrokes, which are of great value for training machine learning models. However, due to privacy concerns, collecting private data from mobile devices to the cloud for centralized model training is not always possible. To efficiently utilize end-user data, Federated Learning (FL) has emerged as a new paradigm of distributed machine learning that orchestrates model training across mobile devices [1]. With federated learning, locally trained models are communicated to a server for aggregation, without collecting any raw data from users. Federated learning has enabled joint model training over privacy-sensitive data in a wide range of applications, including natural language processing, computer vision, and speech recognition. This work is supported by a research contract with Huawei Technologies Co. Ltd. and a NSERC Discovery Research Program.\n\nHowever, unlike in a server-based environment, distributed machine learning on mobile devices is faced with a few fundamental challenges, such as the limited connectivity of wireless networks, unstable availability of mobile devices, and the non-IID distributions of local datasets which are hard to characterize statistically [1]- [4]. Due to these reasons, it is not as practical to perform machine learning on all participating devices simultaneously in a synchronous or semi-synchronous manner as is in a parameter-server cluster. Therefore, as a common practice in federated learning, only a subset of devices are randomly selected to participate in each round of model training to avoid long-tailed waiting times due to unstable network conditions and straggler devices [1,5].\n\nSince the completion time of federated learning is dominated by the communication time, Kone\u010dn\u1ef3 et al. [6,7] proposed structured and sketched updates to decrease the time it takes to complete a communication round. McMahan et al. [1] presented the Federated Averaging (FEDAVG) algorithm, which aims to reduce the number communication rounds required by averaging model weights from client devices instead of applying traditional gradient descent updates. FEDAVG has also been deployed in a large-scale system [4] to test its effectiveness.\n\nHowever, existing federated learning methods have not solved the statistical challenges posed by heterogeneous local datasets. Since different users have different device usage patterns, the data samples and labels located on any individual device may follow a different distribution, which cannot represent the global data distribution. It has been recently pointed out that the performance of federated learning, especially FEDAVG, may significantly degrade in the presence of non-IID data, in terms of the model accuracy and the communication rounds required for convergence [8]- [10]. In particular, FEDAVG randomly selects a subset of devices in each round and averages their local model weights to update the global model. The randomly selected local datasets may not reflect the true data distribution in a global view, which inevitably incurs biases to global model updates. Furthermore, the models locally trained on non-IID data can be significantly different from each other. Aggregating these divergent models can slow down convergence and substantially reduce the model accuracy [8].\n\nIn this paper, we present the design and implementation of FAVOR, a control framework that aims to improve the performance of federated learning through intelligent device selection. Based on reinforcement learning, FAVOR aims to In this section, we briefly introduce how federated learning works in general and show how non-IID data poses challenges to existing federated learning algorithms. We demonstrate how to properly select client devices at each round to improve the performance of federated learning on non-IID data.\n\n\nA. Federated Learning\n\nFederated learning (FL) trains a shared global model by iteratively aggregating model updates from multiple client devices, which may have slow and unstable network connections. Initially, eligible client devices first check-in with a remote server. The remote server then proceeds federated learning synchronously in rounds. In each round, the server randomly selects a subset of available client devices to participate in training. The selected devices first download the latest global model from the server, train the model on their local datasets, and report their respective model updates to the server for aggregation.\n\nWe formally introduce FL in the context of a C-class classification problem, which is defined over a compact feature space X and a label space Y = [C], where [C] = 1, \u00b7 \u00b7 \u00b7 , C. Let (x, y) denote a particular labeled sample. Let f :\nX \u2192 S denote the prediction function, where S = {z| C i=1 z i = 1, z i 0, \u2200i \u2208 [C]}.\nThat is, the vector valued function f yields a probability vector z for each sample x, where f i predicts the probability that the sample belongs to the ith class.\n\nLet the vector w denote model weights. For classification, the commonly used training loss is cross entropy, defined as\n(w) := E x,y\u223cp C i=1 1 y=i log f i (x, w) = C i=1 p(y = i)E x|y=i [log f i (x, w)],\nThe learning problem is to solve the following optimization problem:\nminimize w C i=1 p(y = i)E x|y=i [log f i (x, w)].(1)\nIn federated learning, suppose there are N client devices in total. The kth device has m (k) data samples following the data distribution p (k) , which is a joint distribution of the samples {x, y} on this device. In each round t, K devices are selected (at random in the original FL), each of which downloads the current global model weights w t\u22121 from the server and performs the following stochastic gradient descent (SGD) training locally:\nw (k) t = w t\u22121 \u2212 \u03b7\u2207 (w t\u22121 ) (2) = w t\u22121 \u2212 \u03b7 C i=1 p (k) (y = i)\u2207 w E x|y=i [log f i (x, w t\u22121 )],\nwhere \u03b7 is the learning rate. Note that in such notation with expectations, w\n(k) t\ncan be a result one or multiple epochs of local SGD training [1].\n\nOnce w (k) t is obtained, each participating device k reports a model weight difference \u2206 (k) t to the FL server [1], which is defined as \u2206\n(k) t := w (k) t \u2212 w (k) t\u22121 .\nDevices can also upload their local models w (k) t directly, but transferring \u2206 (k) t is more amenable to compression and communication reduction [4]. After the FL server has collected updates from all K participating devices in round t, it performs the federated averaging (FEDAVG) algorithm to update the global model:\n\u2206 t = K k=1 m (k) \u2206 (k) t / K k=1 m (k) , w t \u2190 w t\u22121 + \u2206 t .\n\nB. The Challenges of Non-IID Data Distribution\n\nFEDAVG has been shown to work well approximating the model trained on centrally collected data, given that the data and label distributions on different devices are IID [1]. However, in reality, data owned by each device are typically non-IID, i.e., p (k) are different on different devices, due to different user preferences and usage patterns. When data distributions are non-IID, FEDAVG is unstable and may even diverge [8].\n\nThis is due to the inconsistency between the locally performed SGD algorithm, which aims to minimize the loss value on m (k) local samples on each device and the global objective of minimizing the overall loss on all K k=1 m (k) data samples. As we keep fitting models on different devices to heterogeneous local data, the divergence among the weights w (k) of these local models will be accumulated and eventually degrades the performance of learning [8,10], leading to more communication rounds before training converges. Reducing the number of communication rounds in federated learning is crucially essential for mobile devices, which have a limited computation capacity and communication bandwidth.\n\nWe use an experiment to demonstrate that non-IID data may slow down the convergence of federated learning if devices are randomly selected in each round for model aggregation. Instead of random selection, we show that selecting devices with a clustering algorithm can help to even out data distribution and speed up convergence.\n\nIn our experiment, we train a two-layer CNN model with PyTorch on the MNIST dataset (containing 60,000 samples) until the model achieves a 99% test accuracy. We consider 100 devices in total, each running on a different thread, and we train the CNN model under the IID and non-IID settings, respectively. For the IID setting, the 60,000 samples are randomly distributed among 100 devices, such that each device owns 600 samples. For the non-IID setting, each device still owns 600 samples, yet 80% of which come from a dominant class and the remaining 20% belong to other classes. For example, a \"0\"-dominated device has 480 data samples with the label \"0\", while the remaining 120 data samples have labels evenly distributed among \"1\" to \"9\". In each round, ten devices are selected to participate in federated learning. Note that FL selects only a subset of all devices at each round to balance computation complexity and convergence rate, and to avoid excessively long waiting times for model aggregation [1,4], which is also discussed in Sec. IV-D.\n\nFEDAVG randomly selects ten devices to participate in each round of training. As shown in Fig. 1, FEDAVG takes 47 rounds to achieve the target accuracy in the IID setting 2 . However, FEDAVG takes 163 rounds to achieve the 99% target accuracy on non-IID data, which more than triples the number of communication rounds.\n\nTo reduce the required communication rounds under the non-IID setting, we then tuned the device selection strategy. While in FL, it is impossible to peek into the data distribution of each device, we observe that there is an implicit connection between the data distribution on a device and the model weights trained on that device. Therefore, we may profile each device by analyzing the local model weights after the first round.\n\nWe provide some rationale for this observation. According to (2), given the initial global model weights w init , the local weights on device k after one epoch of SGD can be represented by\nw (k) 1 = w init \u2212 \u03b7 C i=1 p (k) (y = i)\u2207 w E x|y=i [log f i (x, w init )].\nThe weight divergence between device k and device k can be measured as w\n(k ) 1 \u2212 w (k) 1\n, where k, k \u2208 [K]. Then, by using a similar bounding technique adopted in [8], we can derive the following bound for weight divergence in the first round:\nw (k ) 1 \u2212 w (k) 1 \u03b7g max (w init ) C i=1 p (k ) (y = i) \u2212 p (k) (y = i) , where g max (w) := max C i=1 \u2207 w E x|y=i [log f i (x, w)] .\nThis implies that even if local models are trained with the same initial weights w init , there is an implicit connection between the discrepancy of data distributions on device k and k , which is the term C i=1 p (k ) (y = i) \u2212 p (k) (y = i) , and their weight divergence.\n\nBased on the sights above, we first let each of the 100 devices download the same initial global weights (randomly generated) and perform one epoch of SGD based on local data to get w (k) 1 , k = 1, . . . , 100. We then apply the K-Center clustering algorithm [11]  } to cluster the 100 devices into 10 groups. In each round, we randomly select one device from each group to participate in FL training, which still results in 10 simultaneously participating devices per round. As shown in Fig. 1, the K-Center algorithm takes 131 rounds to reach the target accuracy under the non-IID setting, which is 32 rounds less than the vanilla FEDAVG algorithm. This example implies that it is possible to improve the performance of federated learning by carefully selecting the set of participating devices in each round, especially under the non-IID data setting.\n\n\nC. Deep Reinforcement Learning (DRL)\n\nReinforcement Learning (RL) is the learning process of an agent that acts in corresponding to the environment to maximize its rewards. The recent success of RL [12,13] comes from the capability that RL agents learn from the interaction with a dynamic environment. Specifically, at each time step t, the RL agent observes states s t and performs an action a t . The state s t of the environment then transits to s t+1 , and the agent will receive reward r t . The state transitions and rewards follow a Markov Decision Process (MDP) [14], which is a discrete time stochastic control process, denoted by a sequence s 1 , a 1 , r 1 , s 2 , . . . , r t\u22121 , s t , a t , . . . . The objective of RL is to maximize the expectation of the cumulative discounted return R = T t=1 \u03b3 t\u22121 r t , where \u03b3 \u2208 (0, 1] is a factor discounting future rewards.\n\nThe RL agent seeks to learn a cheat sheet that points out the actions leading to the maximum cumulative return under a particular state. Value-based RL algorithms formally uses an action-value function Q(s t , a) to estimate an expected return starting from state s t :\nQ \u03c0 (s t , a) : = E \u03c0 [ \u221e k=1 \u03b3 k\u22121 r t+k\u22121 |s t , a] = E st+1,a [r t + \u03b3Q \u03c0 (s t+1 , a)|s t , a t ],\nwhere \u03c0 is the policy mapping from states to probabilities of selecting possible actions. The optimal action-value function Q * (s t , a) is the cheat sheet sought by the RL agent, which is defined as the maximum expectation of the cumulative discounted return starting from s t :\nQ * (s t , a) := E st+1 [r t + \u03b3 max a Q * (s t+1 , a)|s t , a]. (3)\nThen, we could apply function approximation techniques to learn a parameterized value function Q(s t , a; \u03b8 t ) approximating the optimal value function Q * (s t , a). The one-step lookahead r t + \u03b3 max a Q(s t+1 , a; \u03b8 t ) becomes the target that Q(s t , a; \u03b8 t ) learns to be. Typically, a deep neural network (DNN) is used to represent the function approximator [15]. The RL learning problem becomes minimizing the MSE loss between the target and the approximator, which is defined as:\nt (\u03b8 t ) := (r t + \u03b3 max a Q(s t+1 , a; \u03b8 t ) \u2212 Q(s t , a; \u03b8 t )) 2 (4)\n\nIII. DRL FOR CLIENT SELECTION\n\nWe formulate device selection for federated learning as a deep reinforcement learning (DRL) problem. Based on the formulation, we present a federated learning workflow driven by DRL in Sec. III-B. We then introduce the challenges of dealing with high-dimensional models in federated learning. Finally, we describe training the DRL agent with the double Deep Q-learning Network (DDQN) [16].\n\nThe per-round FL process can be modeled as a Markov Decision Process (MDP), with state s represented by the global model weights and the model weights of each client device in each round. Given a state, the DRL agent takes an action a to select a subset of devices that perform local training and update the global model. Then a reward signal r is observed, which is a function of the test accuracy achieved by the global model so far on a held-out validation set. The objective is to train the DRL agent to converge to the target accuracy for federated learning as quickly as possible.\n\nIn the proposed framework, the agent does not need to collect or check any data samples from mobile devicesonly model weights are communicated-thus preserving the same level of privacy as the original FL does. It solely relies on model weight information to determine which device may improve the global model the most, since there is an implicit connection between the data distribution on a device and its local model weights obtained by performing SGD on those data. In fact, in Sec. III-C we will show that even after dimension reduction, the divergence between local model weights is still obvious and contains information to guide device selection.\n\n\nA. The Agent based on Deep Q-Network\n\nSuppose there is a federated learning job on N available devices with a target accuracy \u2126. In each round, using a Deep Q-Network (DQN) [15], K devices are selected to participate in the training. Considering limited available traces from federated learning jobs, DQN can be more efficiently trained and can reuse data more effectively than policy gradient methods and actor-critic methods.\n\nState: Let the state of round t be represented by a vector\ns t = (w t , w (1) t , . . . , w (N ) t ),\nwhere w t denotes the weights of the global model after round t, and w\n(1) t , . . . , w (N ) t are model weights of the N devices, respectively.\nThe agent collocates with the FL server and maintains a list of model weights {w (k) |k \u2208 [N ]}; a particular w (k) is updated in round t only if device k is selected for training and the resulting \u2206 (k) t is received by the FL server. Therefore, there is no extra communication overhead introduced for the devices.\n\nThe resulting state space can be huge, e.g., a CNN model can contain millions of weights. It is challenging to train a DQN with such a large state space. In practice, we propose to apply an effective and lightweight dimension reduction technique on the state space, i.e., on model weights, which will be described in detail in Sec. III-C.\n\nAction: At the beginning of each round t, the agent needs to decide to select which subset of K devices from the N devices. This selection would have resulted in a large action space of size N K , which complicates RL training. We propose a trick to keep the action space small while still leveraging the intelligent control provided by the DRL agent.\n\nIn particular, the agent is trained by selecting only one out of N devices to participate in FL per round based on DQN, while in testing and application the agent will sample a batch of top-K clients to participate in FL. That is, the DQN agent learns an approximator of the optimal action-value function Q * (s t , a) through a neural network, which estimates the action that maximizes the expected return starting from s t . The action space is thus reduced to {1, 2, . . . , N }, where a = i means that device i is selected to participate in FL.\n\nOnce DQN has been trained to approximate Q * (s, a), during testing, in round t the DQN agent will compute {Q * (s t , a)|a \u2208 [N ]} for all N actions. Each action-value indicates the maximum expected return that the agent can get by selecting a particular action a at state s t . Then we select the K devices, each corresponding to a different action a, that lead to the top-K values of Q * (s t , a).\n\nReward: We set the reward observed at the end of each round t to be r t = \u039e (\u03c9t\u2212\u2126) \u2212 1, t = 1, . . . , T , where \u03c9 t is the testing accuracy achieved by the global model on the held-out validation set after round t, \u2126 is the target accuracy, and \u039e is a positive constant that ensures that r t grows exponentially with the testing accuracy \u03c9 t . We have r t \u2208 (\u22121, 0], since 0 \u03c9 t \u2126 1. The federated learning stops when \u03c9 t = \u2126, at which point r t reaches its maximum value of 0.\n\nThe DQN agent is trained to maximize the expectation of the cumulative discounted reward given by\nR = T t=1 \u03b3 t\u22121 r t = T t=1 \u03b3 t\u22121 (\u039e (\u03c9t\u2212\u2126) \u2212 1),\nwhere \u03b3 \u2208 (0, 1] is a factor discounting future rewards.\n\nWe now explain the motivations behind the two terms \u039e (\u03c9t\u2212\u2126) and \u22121 in r t . The first term, \u039e (\u03c9t\u2212\u2126) , incentivizes the agent to select devices that achieve a higher test accuracy \u03c9 t . \u039e controls how fast reward r t grows with \u03c9 t . In general, as ML training proceeds, the model accuracy will increase at a slower pace, which means |\u03c9 t \u2212 \u03c9 t\u22121 | decreases as round t increases. Therefore, we use an exponential term to amplify the marginal accuracy increase as FL progresses into later stages. \u039e is set to 64 in our experiments.\n\nThe second term, \u22121, encourages the agent to complete training in fewer rounds, because the more rounds it takes, the less cumulative reward the agent will receive.\n\n\nB. Workflow\n\nFig. 2 shows how our system FAVOR performs federated learning with a DRL agent selecting devices in each round, following the steps below:\n\nStep 1: All N eligible devices check in with the FL server.\n\nStep 2: Each device downloads the initial random model weights w init from the server, performs local SGD training for one epoch, and returns the resulting model weights {w Steps 3-5 will be repeated until completion, e.g., until a target accuracy is reached or after a certain number of rounds.\n(k) 1 , k \u2208 [N ]} to the FL server.\n\nCheck-in Initialize \u2026\n\nSelection time \u2026\n\nStep 1\n\nStep 2\n\nStep 3\n\nStep 4\n\n\nReporting\n\n\nDRL Agent\n\nStep 5 \u2026 w init < l a t e x i t s h a 1 _ b a s e 6 4 = \" k p 7     \nB 4 4 q Y G a A u Y O Q k T U / b W 3 q w I G k = \" > A A A B / 3 i c b V D L S s N A F J 3 U V 6 2 v q O D G z W A R X J W k C r o s u H F Z w T 6 g D W E y m b R D J z N h Z q K U m I W / 4 s a F I m 7 9 D X f + j Z M 2 C 2 2 9 M M z h n H u 5 5 5 4 g Y V R p x / m 2 K i u r a + s b 1 c 3 a 1 v b O 7 p 6 9 f 9 B V I p W Y d L B g Q v Y D p A i j n H Q 0 1 Y z 0 E 0 l Q H D D S C y b X h d 6 7 J 1 J R w e / 0 N C F e j E a c R h Q j b S j f P h o G g o V q G p s v e 8 j 9 j H K q 8 5 p v 1 5 2 G M y u 4 D N w S 1 E F Z b d / + G o Y C p z H h G j O k 1 M B 1 E u 1 l S G q K G c l r w 1 S R B O E J G p G B g R z F R H n Z z H 8 O T w 0 T w k h I 8 7 i G M / b 3 R I Z i V V g 0 n T H S Y 7 W o F e R / 2 i D V 0 Z V n T k p S T T i e L 4 p S B r W A R R g w p J J g z a Y G I C y p 8 Q r x G E m E t Y m s C M F d P H k Z d J s N 9 7 z R v L 2 o t 5 p l H F V w D E 7 A G X D B J W i B G 9 A G H Y D B I 3 g G r + D N e r J e r H f r Y 9 5 a s c q Z Q / C n r M 8 f 1 x m W k g = = < / l a t e x i t > w (k) 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" E p q x c O C y X i + H C L i 1 o B R t P 8 6 Z J k 4 = \" > A A A C A n i c b V D L S g M x F M 3 4 r P U 1 6 k r c B I t Q N 2 W m C r o s u H F Z w T 6 g H Y d M J m 1 D M 8 m Q Z J Q y D G 7 8 F T c u F H H r V 7 j z b 8 y 0 s 9 D W A y G H c + 7 l 3 n u C m F G l H e f b W l p e W V 1 b L 2 2 U N 7 e 2 d 3 b t v f 2 2 E o n E p I U F E 7 I b I E U Y 5 a S l q W a k G 0 u C o o C R T j C + y v 3 O P Z G K C n 6 r J z H x I j T k d E A x 0 k b y 7 c N + I F i o J p H 5 0 o f M T 9 3 s L q 2 O T 7 O y b 1 e c m j M F X C R u Q S q g Q N O 3 v / q h w E l E u M Y M K d V z n V h 7 K Z K a Y k a y c j 9 R J E Z 4 j I a k Z y h H E V F e O j 0 h g y d G C e F A S P O 4 h l P 1 d 0 e K I p V v a S o j p E d q 3 s v F / 7 x e o g e X X k p 5 n G j C 8 W z Q I G F Q C 5 j n A U M q C d Z s Y g j C k p p d I R 4 h i b A 2 q e U h u P M n L 5 J 2 v e a e 1 e o 3 5 5 V G v Y i j B I 7 A M a g C F 1 y A B r g G T d A C G D y C Z / A K 3 q w n 6 8 V 6 t z 5 m p U t W 0 X M A / s D 6 / A E 5 7 Z c / < / l a t e x i t > \u2026 {w (k) 2 |k 2 [K]} < l a t e x i t s h a 1 _ b a s e 6 4 = \" g a q 2 J V H u C q E O 1 R 5 c W 2 0 4 L F c O o c I = \" > A A A C D H i c b V D L S g M x F M 3 U V 6 2 v q k s 3 w S L U T Z m p g i 4 L b g Q 3 F e w D Z s a S S d M 2 N J M M S U Y p 4 3 y A G 3 / F j Q t F 3 P o B 7 v w b M + 0 s t P V C y O G c c 7 n 3 n i B i V G n b / r Y K S 8 s r q 2 v F 9 d L G 5 t b 2 T n l 3 r 6 1 E L D F p Y c G E 7 A Z I E U Y 5 a W m q G e l G k q A w Y K Q T j C 8 y v X N H p K K C 3 + h J R P wv 8 f z p W x Y d 7 L U X A Q q O f 2 I Y = \" > A A A C K H i c b V D L S s N A F J 3 U d 3 1 V X b o Z L E I F K U k V F F x Y c O N K L N g q N L F M p t N 2 y G Q S Z m 6 U E v M 5 b v w V N y K K d O u X O G 2 z 8 H V g m\n\n= \" o V 5 h + Y S O U e E d 8 a s u a F / C + s H A t O I = \" > A A A C D H i c b V C 7 T s M w F H X K q 5 R X g Z H F o k I q S 5 U U J B g r s T C h I t G H l I T K c Z z W q u N E t g O q Q j 6 A h V 9 h Y Q A h V j 6 A j b / B a T N\nN z g Y E a o = \" > A A A C K H i c b V D L S s N A F J 3 U d 3 1 V X b o Z L E I F K U k U F F x Y c O N K L N g q N L F M p t N 2 y G Q S Z m 6 U E v M 5 b v w V N y K K d O u X O H 0 s f B 0 Y 5 n D O v d x 7 T 5 A I r s G 2 h 1 Z h Z n Z u f m F x q b i 8 s r q 2 X t r Y b O o 4 V Z Q 1 a C x i d R M Q z Q S X r A E c B L tx C R Q Q b R u O X Y C f k Y U c C p Y X v R S z R J C Q 9 J j L U M l i Z j 2 s / G h O d 4 1 S g d 3 Y 2 W e B D x W v 3 d k J N K j Z U 1 l R K C v f 3 s j 8 T + v l U L 3 2 M + 4 T F J g k k 4 G d V O B I c a j 1 H C H K 0 Z B D A w h V H G z K 6 Z 9 o g g F k 2 3 R h O D 8 P v k v\n\n< l a t e x i t s h a 1 _ b a s e 6 4 = \" S A J 8 p i p Y 4 r h T g + T U E 4 P v 0 o g s 3 C E = \" > A A A B / 3 i c b V D N S 8 M w H E 3 n 1 5 x f V c G L l + I Q 5 m W 0 m 6 D H g R e P E 9 w H b L W k a b q F p U l J U m X U H v x X v H h Q x K v / h j f / G 9 O t B 9 1 8 E P J 4 7 / c j L 8 + P K Z H K t r + N 0 s r q 2 v p G e b O y t b 2 z u 2 f u H 3 Q l T w T C H c Q p F 3 0 f S k w J w x 1 F F M X 9 W G A Y + R T 3 / M l V 7 v f u s Z C E s 1 s 1 j b E b w R E j I U F Q a c k z j 4 Y + p 4 G c R v p K H 7 K 7 t D Y 5 y 7 y m Z 1 b t u j 2 D t U y c g l R B g b Z n f g 0 D j p I I M 4 U o l H L g 2 L F y U y g U Q R R n l W E i c Q z R B I 7 w Q F M G I y z d d J Y / s 0 6 1 E l g h F / o w Z c 3 U 3 x s p j G Q e U U 9 G U I 3 l o p e L / 3 m D R I W X b k p Y n C j M 0 P y h M K G W 4 l Z e h h U Q g Z G i U 0 0 g E k R n t d A Y C o i U r q y i S 3 A W v 7 x M u o 2 6 0 6 w 3 b s 6 r r U Z R R x k c g x N Q A w 6 4 A C 1 w D d q g A x B 4 B M / g F b w Z T 8 a L 8 W 5 8 z E d L R r F z C P 7 A + P w B K b G W I Q = = < / l a t e x i t >\n\n\nSelection\n\nReporting\n{w (k) 3 |k 2 [K]} <\n\nl a t e x i t s h a 1 _ b a s e 6 4 = \" o S L j V M i 6 m Y I J 3 l L O a 0 C / I X 6 A U T 0 = \" > A A A C D H i c b V D L S s N A F J 3 4 r P V V d e l m s A h 1 U 5 J W 0 G X B j e C m g n 1 A E s t k M m m H T i Z h Z q K U m A 9 w 4 6 + 4 c a G I W z / A n X / j p M 1 C W y 8 M c z j n X O 6 9 x 4 s Z l c o 0 v 4 2 l 5 Z X V t f X S R n l z a 3 t n t 7 K 3 3 5 V R I j D p 4 I h F o u 8 h S R j l p K O o Y q Q f C 4 J C j 5 G e N 7 7 I 9 d 4 d E Z J G / E Z N Y u K G a M h p Q D F S m h p U q k 7 q e B H z 5 S T U X 3 q f D Z q 3 a W 1 8 k j 2 M H c q h f e U 6 m X a Z d X N a c B F Y B a i C o t q D y p f j R z g J C V e Y I S l t y 4 y V m y K h K G Y k K z u J J D H C Y z Q k t o Y c h U S 6 6 f S Y D B 5 r x o d B J P T j C k 7 Z 3 x 0 p C m W + r X a G S I 3 k v J a T / 2 l 2 o o J z N 6 U 8 T h T h e D Y o S B h U E c y T g T 4 V B C s 2 0 Q B h Q f W u E I + Q Q F j p / M o 6 B G v + 5 E X Q b d S t Z r 1 x f V p t N Y o 4 S u A Q H I E a s M A Z a I F L 0 A Y d g M E j e A a v 4 M 1 4 M l 6 M d + N j Z l 0 y i p 4 D 8 K e M z x 8 U z p u Q < / l a t e x i t > \u2026\n\nStep 3\n\nStep 4 Fig. 2: The federated learning workflow with FAVOR.\n\nNote that the above workflow can be easily tweaked to let each selected client upload the model difference \u2206 (as compared to the last version of the local model) instead of the new local model itself, without changing the underlying logic. The FL server can always use the \u2206 to reconstruct a copy of the corresponding local model stored on it.\n\nWhen a device k has new training data, it will request the global model weights w t and perform local SGD training for one epoch as Step 2, which generates new local model weights w (k) t . Then the device pushes w (k) t to the FL server, and the FL server updates the state s t to make the DRL agent aware of the device with updated data. It should be noted that data on each device remain unchanged during the DRL training to ensure that the training follows the Markov Decision Process.\n\nThe overhead introduced is minimum since no extra computation/communication overhead is added to devices. The only overhead is that now the FL server needs to store a copy of the latest local model weights from each device in order to form the state s t . However, the FL server is deployed in cloud, where provisions sufficient on-demand computational and storage capacity .\n\n\nC. Dimension Reduction\n\nOne issue of the proposed DQN agent is that it uses the weights of the global model and all local models as the state, which leads to a large state space. Many deep neural networks (e.g., CNN) have millions of weights, making it challenging to train the DQN agent with such a high dimensional state space. To reduce the dimension of the state space, we propose t , k \u2208 [N ]}, through linear transformation, without fitting the PCA model again. Therefore, the overhead to compress model weights in subsequent rounds is negligibly small.\n\nWe demonstrate the effectiveness of using PCA-compressed model weights to differentiate between data distributions through a simple experiment. Consider a two-layer CNN model (with 431,080 model weights in total) to be trained with federated learning on 100 devices, running PyTorch, on the MNIST dataset. Data samples are distributed in the same way as the experiment in Sec. II: each client has 80% of its data samples belonging to a dominant class, while the remaining 20% of its samples have random labels. After five epochs of local SGD training in Round 1, we project the model weight vectors of the 100 devices, {w (k) 1 , k \u2208 [N ]}, onto a two-dimensional space of the first and second principal components.\n\nAs shown in Fig. 3, different shapes (or colors) indicate local models trained on devices with different dominant labels. For example, all the yellow \"+\" signs denote the compressed local models in Round 1 from those devices with a dominant label \"6\". Even reduced from 431,080 dimensions to only two dimensions and even at Round 1, we can observe a clustering effecting of local models according to their dominant labels. Therefore, in the evaluation in Sec. IV, we use PCAcompressed model weights as states to enable efficient training of the DQN agent.\n\n\nD. Training the Agent with Double DQN\n\nWe propose to use the double deep Q-learning network (DDQN) to learn the function Q * (s t , a). Q-learning provides a value estimation for each potential action a at state s t , based on which devices are selected. However, the original Q-learning algorithms can be unstable since they indirectly n f e e L a i E Q 9 4 j T l Q U x H S k S C U b T S g x 7 g o F p z 6 + 4 C Z J 1 4 B a l B g d a g + t U f J i y L u U I m q T E 9 z 0 0 x y K l G w S S f V f q Z 4 S l l E z r i P U s V j b k J 8 s W p M 3 J h l S G J E m 1 L I V m o v y d y G h s z j U P b G V M c m 1 V v L v 7 n 9 T K M b o J c q D R D r t h y U Z R J g g m Z / 0 2 G Q n O G c m o J Z V r Y W w k b U 0 0 Z 2 n Q q N g R v 9 e V 1 0 m 7 U P b f u 3 V / V m o 0 i j j K c w T l c g g f X 0 I Q 7 a I E P D E b w D K / w 5 k j n x X l 3 P p a t J a e Y O Y U / c D 5 / A G N I j c 0 = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" a j x j 6 o u H R h v i v I 0 u K k Y + 9 / o e r d w = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o M e C F 4 8 V T V t o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J X C o O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S N k m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l 7 4 S T 2 7 n f e e L a i E Q 9 4 j T l Q U x H S k S C U b T S g x 7 g o F p z 6 + 4 C Z J 1 4 B a l B g d a g + t U f J i y L u U I m q T E 9 z 0 0 x y K l G w S S f V f q Z 4 S l l E z r i P U s V j b k J 8 s W p M 3 J h l S G J E m 1 L I V m o v y d y G h s z j U P b G V M c m 1 V v L v 7 n 9 T K M b o J c q D R D r t h y U Z R J g g m Z / 0 2 G Q n O G c m o J Z V r Y W w k b U 0 0 Z 2 n Q q N g R v 9 e V 1 0 m 7 U P b f u 3 V / V m o 0 i j j K c w T l c g g f X 0 I Q 7 a I E P D E b w D K / w 5 k j n x X l 3 P p a t J a e Y O Y U / c D 5 / A G N I j c 0 = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" a j x j 6 o u H R h v i v I 0 u K k Y + 9 / o e r d w = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o M e C F 4 8 V T V t o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J X C o O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S N k m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l 7 4 S T 2 7 n f e e L a i E Q 9 4 j T l Q U x H S k S C U b T S g x 7 g o F p z 6 + 4 C Z J 1 4 B a l B g d a g + t U f J i y L u U I m q T E 9 z 0 0 x y K l G w S S f V f q Z 4 S l l E z r i P U s V j b k J 8 s W p M 3 J h l S G J E m 1 L I V m o v y d y G h s z j U P b G V M c m 1 V v L v 7 n 9 T K M b o J c q D R D r t h y U Z R J g g m Z / 0 2 G Q n O G c m o J Z V r Y W w k b U 0 0 Z 2 n Q q N g R v 9 e V 1 0 m 7 U P b f u 3 V / V m o 0 i j j K c w T l c g g f X 0 I Q 7 a I E P D E b w D K / w 5 k j n x X l 3 P p a t J a e Y O Y U / c D 5 / A G N I j c 0 = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" a j x j 6 o u H R h v i v I 0 u K k Y + 9 / o e r d w = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o M e C F 4 8 V T V t o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J X C o O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S N k m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l 7 4 S T 2 7 n f e e L a i E Q 9 4 j T l Q U x H S k S C U b T S g x 7 g o F p z 6 + 4 C Z J 1 4 B a l B g d a g + t U f J i y L u U I m q T E 9 z 0 0 x y K l G w S S f V f q Z 4 S l l E z r i P U s V j b k J 8 s W p M 3 J h l S G J E m 1 L I V m o v y d y G h s z j U P b G V M c m 1 V v L v 7 n 9 T K M b o J c q D R D r t h y U Z R J g g m Z / 0 2 G Q n O G c m o J Z V r Y W w k b U 0 0 Z 2 n Q q N g R v 9 e V 1 0 m 7 U P b f u 3 V / V m o 0 i j j K c w T l c g g f X 0 I Q 7 a I E P D E b w D K / w 5 k j n x X l 3 P p a t J a e Y O Y U / c D 5 / A G N I j c 0 = < / l a t e x i t > Action a t\n\n\n< l a t e x i t s h a 1 _ b a s e 6 4 = \" J i B K S x Y y y y X 1 X 4 O 2 4 i Y U G K 1 + p D 4 = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l q Q Y 8 F L x 4 r 2 g 9 o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I J H C o O t + O 4 W N z a 3 t n e J u a W / / 4 P C o f H z S N n G q G W + x W M a 6 G 1 D D p V C 8 h Q I l 7 y a a 0 y i Q v B N M b u d + 5 4 l r I 2 L 1 i N O E + x E d K R E K R t F K D 3 S A g 3 L F r b o L k H X i 5 a Q C O Z q D 8 l d / G L M 0 4 g q Z p M b 0 P D d B P 6 M a B Z N 8 V u q n h i e U T e i I 9 y x V N O L G z x a n z s i F V Y Y k j L U t h W S h / p 7 I a G T M N A p s Z 0 R x b F a 9\n\nu f i f 1 0 s x v P E z o Z I U u W L L R W E q C c Z k / j c Z C s 0 Z y q k l l G l h b y V s T D V l a N M p 2 R C 8 1 Z f X S b t W 9 a 6 q t f t 6 p V H L 4 y j C G Z z D J X h w D Q 2 4 g y a 0 g M E I n u E V 3 h z p v D j v z s e y t e D k M 6 f w B 8 7 n D 0 q w j c A = < / l a t e x i t > Q \u2713 optimize the agent performance by learning an approximator Q(s, a; \u03b8 t ) to the optimal action-value function Q * (s, a). DDQN adds another value function Q(s, a; \u03b8 t ) to stabilize the action-value function estimation. The idea behind DDQN is that the network is frozen every M updates. DDQN adds stability to the action-value evaluation, which is less prone to \"jittering.\" To train the DRL agent, the FL server first performs random device selection to initialize the states. As shown in Fig. 4, the states are fed into the one of the double DQNs Q(s t , a; \u03b8 t ). The DQN generates an action a to select a device for the FL server. After several rounds of FL training, the DRL agent has sampled a few action-state pairs, with which the agent learns to solve the (4) as:\n\n\n< l a t e x i t s h a 1 _ b a s e 6 4 = \" E T 4 t c d 8 Q M 9 N i 9 I t 4 g U 8 e E c P L o G 8 = \" > A A A B 8 X i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e C F 4 8 t 2 A 9 s Q 9 l s J + 3 S z S b s T o Q S + i + 8 e F D E q / / G m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I J H C o O t + O 4 W N z a 3 t n e J u a W / / 4 P C o f H z S N n G q O b R 4 L G P d D Z g B K R S 0 U K C E b q K B R Y G E T j C 5 m / u d J 9 B G x O o B p w n 4 E R s p E Q r O 0 E q P z U H W x z E g m w 3 K F b f q L k D X i Z e T C s n R G J S / + s O Y p x E o 5 J I Z 0 / P c B P 2 M a R R c w q z U T w 0 k j E / Y C H q W K h a B 8 b P F x T N 6 Y Z U h D W N t S y F d q L 8 n M h Y Z M 4 0 C 2 x k x H J t V b y 7 + 5 / V S D G / 9 T K g k R V B 8 u S h M J c W Y z t + n\nt (\u03b8 t ) = (Y DoubleQ t \u2212 Q(s t , a; \u03b8 t )) 2 ,\nwhere Y DoubleQ t is the target at round t defined as\nY DoubleQ t : = r t + \u03b3 max a Q(s t+1 , a; \u03b8 t )(5)\n= r t + \u03b3Q(s t , argmax a Q(s t , a; \u03b8 t ); \u03b8 t ). (6) (6) uses two acton-value functions to update Y DoubleQ t , in which \u03b8 t is the online parameters updated per time step, and the \u03b8 t is the frozen parameters to add stability to actionvalue estimation. The action-value function Q(s t+1 , a; \u03b8 t ) is updated to minimize t (\u03b8 t ) by gradient descent, i.e.,\n\u03b8 t+1 = \u03b8 t + \u03b1(Y DoubleQ t \u2212 Q(s t , a; \u03b8 t ))\u2207 \u03b8t Q(s t , a; \u03b8 t ),\nwhere \u03b1 is a scalar step size.\n\n\nIV. EVALUATION\n\nWe have implemented FAVOR with PyTorch in around 2000 lines of code, which we have released as an opensource project. With the Python threading library, FAVOR can simulate a large number of devices with lightweight threads, each running real-world PyTorch models.\n\nWe evaluated FAVOR by training popular CNN models on three benchmark datasets: MNIST, FashionMNIST, and CIFAR-10, with FEDAVG and K-Center as the groups of comparison. We evaluated the accuracy of the trained models using the testing set from each dataset. Our experimental results show that FAVOR can reduce the communication rounds by up to 49% on the MNIST, up to 23% on FashionMNIST,  and up to 42% on CIFAR-10, compared to the FEDAVG algorithm. We briefly describe our methodology and settings as follows.\n\nDatasets and models: We explored different combinations of hyper-parameters for the CNN models on different datasets and chose the hyper-parameters leading to the best performance of FEDAVG.\n\n\u2022 MNIST. We train a CNN model with two 5\u00d75 convolution layers. The first layer has 20 output channels and the second has 50, with each layer followed by 2\u00d72 max pooling. On each device, the batch size is ten and the epoch number is five. \u2022 FashionMNIST. We train a CNN model with two 5\u00d75 convolution layers. The first layer has 16 output channels and the second has 32, with each layer followed by 2\u00d72 max pooling. On each device, the batch size is 100 and the epoch number is five. \u2022 CIFAR-10. We train a CNN model with two 5\u00d75 convolution layers. The first layer has six output channels and the second having 16, with each layer followed by 2\u00d72 max pooling. On each device, the batch size is 50 and the epoch number is five. Performance metrics: In federated learning, due to the limited computation capacity and network bandwidth of mobile devices, reducing the number of communication rounds is crucially important. Thus, we use the number of communication rounds as the performance metric of FAVOR.\n\n\nA. Training the DRL agent\n\nWe train the DRL agent on different datasets with 100 available devices. The DDQN model in the DRL agent consists of two two-layer MLP networks, with 512 hidden states. The input size is 10,100, where we have 101 model weights (i.e., the global weights w and the local weights {w (k) |k \u2208 [100]} from 100 devices) reduced to 100 dimensions. The output size of the second layer is 100. Each output passing through a softmax layer becomes the probability of selecting the particular device. We train the DRL agent on an AWS EC2 instance p2.2xlarge with a K80 GPU. The DDQN model is lightweight, and each training iteration takes seconds on GPU. Reducing model weights from 431,080 to 100 dimensions takes minutes by sklearn.decomposition.PCA. accuracy \u2126. The total return is the cumulative discounted reward R obtained in one episode. The target accuracy \u2126 is set to 99% for training on the MNIST, 85% for training on FashionMNIST, and 55% for training on CIFAR-10.\n\n\nB. Different Levels of Non-IID Data\n\nWe investigate different levels of non-IID data to testify the efficiency of FAVOR. We include the performance of FEDAVG and K-Center as a comparison. At each round, the number of selected devices K is set to 10, as in [1].\n\nWe use \u03c3 to denote the four different levels of non-IID data: \u03c3 = 1.0 indicating that data on each device only belong to one label, \u03c3 = 0.8 indicating that 80% of the data belong to one label and the remaining 20% data belong to other labels, \u03c3 = 0.5 indicating that 50% of the data belong to one label and the remaining 50% data belong to other labels, \u03c3 = 0 indicating the data one each device is IID and \u03c3 = H indicating that data on each device evenly belong to two labels. We plot the test-set accuracy v.s. the communication rounds of federated learning on the three benchmarks in Fig. 6, Fig. 7, and Fig. 8. Each entry in Table I shows the number of communication rounds necessary to achieve a test-set accuracy of 99% for the CNN on MNIST, 85% for FashionMNIST, and 55% for CIFAR-10. It should be noted that the italic numbers indicate that the model converges to a lower accuracy than the testset accuracy. The model on MNIST with data distribution of \u03c3 = 1.0 converges to a test-set accuracy of 96%. The models trained on FashionMNIST with data distribution of \u03c3 = 1.0 and \u03c3 = H both converge to a test-set accuracy of 80%. The model trained on CIFAR-10 with data distribution of \u03c3 = 1.0 converges to a test-set accuracy of 50%.\n\nOur experimental results show there is no guarantee that K-Center can always outperform FEDAVG. Devices selected from   the K-Center clusters may introduce more bias to federated learning than devices selected randomly by FEDAVG. However, FAVOR has reduced the number of communication rounds by up to 49% on the MNIST, up to 23% on FashionMNIST, and up to 42% on CIFAR-10, compared to the FEDAVG algorithm.\n\n\nC. Device Selection and Weight Updates\n\nWe save the global model weights and local model weights per round when we train the two-layer CNN on MNIST with \u03c3 = 0.8. The saved model weights are reduced to two- dimension vectors by PCA and plotted in Fig. 9. By examining the model weights trained with FEDAVG and FAVOR, respectively, Fig. 9 shows that FAVOR updates the global model with a larger weight update \u2206 than FEDAVG in early rounds, which leads to a faster convergence speed.\n\n\nD. Increasing Parallelism\n\nWe also studied the performance of FAVOR on different numbers of selected devices. The CIFAR-10 dataset is distributed to 100 devices with the non-IID level at \u03c3 = 0.8. We apply FAVOR, FEDAVG, and K-Center to train the same CNN on CIFAR-10 separately. The number of selected devices K is set to 10, 50, and 100 to study the performance of federated learning with different parallelism. Fig. 10 shows that increasing the parallelism does not reduce the number of communication rounds and even increases the number of communication rounds.\n\nV. RELATED WORK Federated learning allows machine learning models to be trained on mobile devices in a distributed fashion without violating user privacy. Existing studies on federated learning have mostly focused on improving its efficiency. We classify the existing literature into the following two categories:\n\nCommunication efficiency. Mobile devices typically have unstable and expensive connections, and existing works have attempted to improve the communication efficiency of federated learning. Kone\u010dn\u1ef3 et al. [6,7] proposed structured and sketched updates to decrease the completion time of each communication round. Bonawitz et al. [3] developed a secure aggregation protocol that enables a server to perform the computation of high-dimensional data from the devices, which is widely deployed in production environments [4,17]. McMahan et al. [1] presented the Federated Averaging (FEDAVG) algorithm that allows devices to perform local training of multiple epochs, which further reduces the number of communication rounds by averaging model weights from the client devices. Nishio et al. [18] proposed a resource-aware selection algorithm that maximizes the number of participating devices in each round. Sattler et al. [9] proposed a compression framework, Sparse Ternary Compression (STC), that reduces the communication costs and remains robust to non-IID data. FAVOR applies DRL to select the best subset of participating devices to minimize the number of communication rounds, which is orthogonal to these studies on communication efficiency.\n\nSample efficiency. Unlike centralized machine learning, federated learning performs training on non-IID data on devices. Zhao et al. [8] presented a mathematical demonstration to show that non-IID data reduces the accuracy of federated learning by a substantial margin, and proposed to push a small set of uniform distributed data to participating devices. Downloading extra data further increases communication cost and computation workload for the devices. Mehryar et al. [10] proposed an agnostic federated learning framework for fairness to avoid biases due to non-IID data from the devices. In contrast, FAVOR is the first work that counterbalances the bias from different non-IID data by dynamically constructing the subset of participating devices with DRL techniques.\n\n\nVI. CONCLUDING REMARKS\n\nIn this paper, we presented our design and implementation of FAVOR, an experience-driven federated learning framework that performs active device selection to minimize the communication rounds of FL training. We argue that non-IID data exacerbates the divergence of model weights on participating devices, and increases the number of communication rounds of federated learning by a substantial margin. With both mathematical demonstrations and empirical studies, we found the implicit connection between model weights and the distribution of data that the model is trained on. We proposed to actively select a specific subset of devices to participate in training at each round, in order to counterbalance the bias introduced by non-IID data on each device and to speedup FL training by minimizing the number of communication rounds. In particular, we designed a DRL-based agent that applies the DDQN algorithm to select the best subset of devices to achieve our objectives. We have implemented an open-source prototype of FAVOR with PyTorch in more than 2K lines of code and evaluated it with a variety of ML models. An extensive comparison with FL training jobs by FEDAVG has shown that FL training with FAVOR has reduced the number of communication rounds by up to 49% on the MNIST dataset, up to 23% on FashionMNIST, and up to 42% on CIFAR-10.\n\nFig. 1 :\n1Training a CNN model on non-IID MNIST data.\n\nStep 3 :\n3In round t, where t = 1, 2, . . ., upon receiving the uploaded local weights, the corresponding copies of local model weights stored on the server are updated. The DQN agent computes Q(s t , a; \u03b8) for all devices a = 1, . . . , N .Step 4: The DQN agent selects K devices corresponding to the top-K values of Q(s t , a; \u03b8), a = 1, . . . , N . The selected K devices download the latest global model weights w t and perform one epoch of SGD locally to obtain {w |k \u2208 [K]} are reported (uploaded) to the server to compute w t+1 based on FEDAVG. Move into round t + 1 and repeat Steps 3-5.\n\nFig. 3 :\n3PCA of CNN weights on the MNIST dataset. to apply principle component analysis (PCA) to model weights and use the compressed model weights to represent states instead. Specifically, we compute the PCA loading vectors of different principal components only based on local model weights for t = 1, i.e., {w (k) 1 , k \u2208 [N ]}, obtained in Step 2. In the subsequent rounds t = 2, 3, . . ., such loading vectors are reused to obtain first several principal components of {w (k)\n\nFig. 4 :\n4The DDQN Agent interacting with the FL server.\n\nFig. 5 :\n5Training the DRL agent.\n\nFig. 5 FLFig. 6 :\n56plots the training progress of the DRL agent on three FL tasks. An episode starts at the initialization of a federated learning job and ends when the job converges to the target Accuracy v.s. communication rounds on different levels of non-IID MNIST data.\n\nFig. 7 :\n7Accuracy v.s. communication rounds on different levels of non-IID FashionMNIST data.\n\nFig. 8 :\n8Accuracy v.s. communication rounds on different levels of non-IID CIFAR-10 data. Training on MNIST with FAVOR.\n\nFig. 9 :\n9PCA on model weights of FL training with MNIST. w 1 , w 2 . . . , w 5 are the global model weights at Round 1-5.\n\nFig. 10 :\n10FL training on CIFAR-10 with different levels of parallelism.\n\nTABLE I :\nIThe number of communication rounds to reach a target accuracy for FAVOR v.s. FEDAVG and K-Center.\nOur implementation is available as an open-source GitHub repository, located at https://github.com/iqua/flsim.\nThe same setting costs FEDAVG 50 rounds on TensorFlow in[1].\n\nCommunication-Efficient Learning of Deep Networks from Decentralized Data. H B Mcmahan, E Moore, D Ramage, S Hampson, B A Y Arcas, Proc. the Artificial Intelligence and Statistics Conference. the Artificial Intelligence and Statistics ConferenceAISTATSH. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y. Arcas, \"Communication-Efficient Learning of Deep Networks from Decentralized Data,\" in Proc. the Artificial Intelligence and Statistics Conference (AISTATS), 2017.\n\nFederated Multi-Task Learning. V Smith, C.-K Chiang, M Sanjabi, A S Talwalkar, Proc. the Advances in Neural Information Processing Systems (NeurIPS). the Advances in Neural Information essing Systems (NeurIPS)V. Smith, C.-K. Chiang, M. Sanjabi, and A. S. Talwalkar, \"Federated Multi-Task Learning,\" in Proc. the Advances in Neural Information Processing Systems (NeurIPS), 2017.\n\nPractical Secure Aggregation for Privacy-Preserving Machine Learning. K Bonawitz, V Ivanov, B Kreuter, A Marcedone, H B Mcmahan, S Patel, D Ramage, A Segal, K Seth, Proc. the ACM SIGSAC Conference on Computer and Communications Security (CCS. the ACM SIGSAC Conference on Computer and Communications Security (CCSK. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel, D. Ramage, A. Segal, and K. Seth, \"Practical Secure Aggre- gation for Privacy-Preserving Machine Learning,\" in Proc. the ACM SIGSAC Conference on Computer and Communications Security (CCS), 2017.\n\nTowards Federated Learning at Scale: System Design. K Bonawitz, H Eichner, W Grieskamp, D Huba, Proc. the Conference on Systems and Machine Learning (SysML). the Conference on Systems and Machine Learning (SysML)K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba et al., \"Towards Federated Learning at Scale: System Design,\" in Proc. the Conference on Systems and Machine Learning (SysML), 2019.\n\nDeep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training. Y Lin, S Han, H Mao, Y Wang, W J Dally, Proc. the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)Y. Lin, S. Han, H. Mao, Y. Wang, and W. J. Dally, \"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training,\" in Proc. the International Conference on Learning Represen- tations (ICLR), 2018.\n\nFederated optimization: Distributed optimization beyond the datacenter. J Kone\u010dn\u1ef3, B Mcmahan, D Ramage, Proc. the NIPS Workshop on Optimization for Machine Learning (OPT). the NIPS Workshop on Optimization for Machine Learning (OPT)J. Kone\u010dn\u1ef3, B. McMahan, and D. Ramage, \"Federated optimization: Distributed optimization beyond the datacenter,\" in Proc. the NIPS Workshop on Optimization for Machine Learning (OPT), 2015.\n\nFederated Learning: Strategies for Improving Communication Efficiency. J Kone\u010dn\u00fd, H B Mcmahan, F X Yu, P Richtarik, A T Suresh, D Bacon, Proc. the NIPS Workshop on Private Multi-Party Machine Learning. the NIPS Workshop on Private Multi-Party Machine LearningJ. Kone\u010dn\u00fd, H. B. McMahan, F. X. Yu, P. Richtarik, A. T. Suresh, and D. Bacon, \"Federated Learning: Strategies for Improving Communica- tion Efficiency,\" in Proc. the NIPS Workshop on Private Multi-Party Machine Learning, 2016.\n\nFederated Learning with Non-IID Data. Y Zhao, M Li, L Lai, N Suda, D Civin, V Chandra, arXiv:1806.00582arXiv preprintY. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, \"Federated Learning with Non-IID Data,\" arXiv preprint arXiv:1806.00582, 2018.\n\nRobust and Communication-Efficient Federated Learning from Non-IID Data. F Sattler, S Wiedemann, K.-R M\u00fcller, W Samek, arXiv:1903.02891arXiv preprintF. Sattler, S. Wiedemann, K.-R. M\u00fcller, and W. Samek, \"Robust and Communication-Efficient Federated Learning from Non-IID Data,\" arXiv preprint arXiv:1903.02891, 2019.\n\nAgnostic Federated Learning. M Mohri, G Sivek, A T Suresh, International Conference on Machine Learning (ICML). M. Mohri, G. Sivek, and A. T. Suresh, \"Agnostic Federated Learning,\" in International Conference on Machine Learning (ICML), 2019.\n\nActive Learning for Convolutional Neural Networks: A Core-Set Approach. O Sener, S Savarese, Proc. the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)O. Sener and S. Savarese, \"Active Learning for Convolutional Neural Networks: A Core-Set Approach,\" in Proc. the International Conference on Learning Representations (ICLR), 2018.\n\nResource Management with Deep Reinforcement Learning. H Mao, M Alizadeh, I Menache, S Kandula, Proc. the 15th ACM Workshop on Hot Topics in Networks. the 15th ACM Workshop on Hot Topics in NetworksACMH. Mao, M. Alizadeh, I. Menache, and S. Kandula, \"Resource Man- agement with Deep Reinforcement Learning,\" in Proc. the 15th ACM Workshop on Hot Topics in Networks. ACM, 2016.\n\nDevice Placement Optimization with Reinforcement Learning. A Mirhoseini, H Pham, Q Le, M Norouzi, S Bengio, B Steiner, Y Zhou, N Kumar, R Larsen, J Dean, International Conference on Machine Learning (ICML). A. Mirhoseini, H. Pham, Q. Le, M. Norouzi, S. Bengio, B. Steiner, Y. Zhou, N. Kumar, R. Larsen, and J. Dean, \"Device Placement Opti- mization with Reinforcement Learning,\" in International Conference on Machine Learning (ICML), 2017.\n\nReinforcement learning: An introduction. R S Sutton, A G Barto, MIT press CambridgeR. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press Cambridge, 1998.\n\nPlaying Atari with Deep Reinforcement Learning. M Volodymyr, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, Proc. NIPS Deep Learning Workshop. NIPS Deep Learning WorkshopM. Volodymyr, K. Kavukcuoglu, D. Silver, A. Graves, and I. Antonoglou, \"Playing Atari with Deep Reinforcement Learning,\" in Proc. NIPS Deep Learning Workshop, 2013.\n\nDeep Reinforcement Learning with Double Q-Learning. H Van Hasselt, A Guez, D Silver, Proc. the Thirtieth AAAI Conference on Artificial Intelligence (AAAI). the Thirtieth AAAI Conference on Artificial Intelligence (AAAI)H. Van Hasselt, A. Guez, and D. Silver, \"Deep Reinforcement Learning with Double Q-Learning,\" in Proc. the Thirtieth AAAI Conference on Artificial Intelligence (AAAI), 2016.\n\nT Yang, G Andrew, H Eichner, H Sun, W Li, N Kong, D Ramage, F Beaufays, arXiv:1812.02903Applied Federated Learning: Improving Google Keyboard Query Suggestions. arXiv preprintT. Yang, G. Andrew, H. Eichner, H. Sun, W. Li, N. Kong, D. Ram- age, and F. Beaufays, \"Applied Federated Learning: Improving Google Keyboard Query Suggestions,\" arXiv preprint arXiv:1812.02903, 2018.\n\nClient Selection for Federated Learning with Heterogeneous Resources in Mobile Edge. T Nishio, R Yonetani, Proc. the IEEE International Conference on Communications (ICC). the IEEE International Conference on Communications (ICC)T. Nishio and R. Yonetani, \"Client Selection for Federated Learning with Heterogeneous Resources in Mobile Edge,\" in Proc. the IEEE International Conference on Communications (ICC), 2019.\n", "annotations": {"author": "[{\"end\":134,\"start\":77},{\"end\":206,\"start\":135},{\"end\":255,\"start\":207},{\"end\":291,\"start\":256}]", "publisher": null, "author_last_name": "[{\"end\":85,\"start\":81},{\"end\":149,\"start\":143},{\"end\":213,\"start\":210},{\"end\":266,\"start\":264}]", "author_first_name": "[{\"end\":80,\"start\":77},{\"end\":142,\"start\":135},{\"end\":209,\"start\":207},{\"end\":263,\"start\":256}]", "author_affiliation": "[{\"end\":133,\"start\":111},{\"end\":205,\"start\":183},{\"end\":254,\"start\":232},{\"end\":290,\"start\":268}]", "title": "[{\"end\":74,\"start\":1},{\"end\":365,\"start\":292}]", "venue": null, "abstract": "[{\"end\":2094,\"start\":367}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4498,\"start\":4495},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5282,\"start\":5279},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5287,\"start\":5284},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5731,\"start\":5728},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5733,\"start\":5731},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5842,\"start\":5839},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5844,\"start\":5842},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5969,\"start\":5966},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6248,\"start\":6245},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6858,\"start\":6855},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6864,\"start\":6860},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7372,\"start\":7369},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10055,\"start\":10052},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10174,\"start\":10171},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10378,\"start\":10375},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10833,\"start\":10830},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11087,\"start\":11084},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11545,\"start\":11542},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11548,\"start\":11545},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13136,\"start\":13133},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13138,\"start\":13136},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13996,\"start\":13993},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14365,\"start\":14362},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15117,\"start\":15113},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15913,\"start\":15909},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15916,\"start\":15913},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16285,\"start\":16281},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17680,\"start\":17676},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18292,\"start\":18288},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19717,\"start\":19713},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":43652,\"start\":43649},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":46875,\"start\":46872},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":46877,\"start\":46875},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":46999,\"start\":46996},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":47187,\"start\":47184},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":47190,\"start\":47187},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":47210,\"start\":47207},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":47457,\"start\":47453},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":47588,\"start\":47585},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":48050,\"start\":48047},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":48392,\"start\":48388},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":52267,\"start\":52264}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":50118,\"start\":50064},{\"attributes\":{\"id\":\"fig_1\"},\"end\":50715,\"start\":50119},{\"attributes\":{\"id\":\"fig_7\"},\"end\":51199,\"start\":50716},{\"attributes\":{\"id\":\"fig_9\"},\"end\":51257,\"start\":51200},{\"attributes\":{\"id\":\"fig_11\"},\"end\":51292,\"start\":51258},{\"attributes\":{\"id\":\"fig_12\"},\"end\":51569,\"start\":51293},{\"attributes\":{\"id\":\"fig_13\"},\"end\":51665,\"start\":51570},{\"attributes\":{\"id\":\"fig_14\"},\"end\":51787,\"start\":51666},{\"attributes\":{\"id\":\"fig_15\"},\"end\":51911,\"start\":51788},{\"attributes\":{\"id\":\"fig_16\"},\"end\":51986,\"start\":51912},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":52096,\"start\":51987}]", "paragraph": "[{\"end\":2862,\"start\":2096},{\"end\":3924,\"start\":2864},{\"end\":4950,\"start\":3944},{\"end\":5734,\"start\":4952},{\"end\":6275,\"start\":5736},{\"end\":7373,\"start\":6277},{\"end\":7901,\"start\":7375},{\"end\":8551,\"start\":7927},{\"end\":8785,\"start\":8553},{\"end\":9034,\"start\":8871},{\"end\":9155,\"start\":9036},{\"end\":9308,\"start\":9240},{\"end\":9806,\"start\":9363},{\"end\":9984,\"start\":9907},{\"end\":10056,\"start\":9991},{\"end\":10197,\"start\":10058},{\"end\":10549,\"start\":10229},{\"end\":11088,\"start\":10661},{\"end\":11793,\"start\":11090},{\"end\":12123,\"start\":11795},{\"end\":13177,\"start\":12125},{\"end\":13498,\"start\":13179},{\"end\":13930,\"start\":13500},{\"end\":14120,\"start\":13932},{\"end\":14269,\"start\":14197},{\"end\":14442,\"start\":14287},{\"end\":14851,\"start\":14578},{\"end\":15708,\"start\":14853},{\"end\":16587,\"start\":15749},{\"end\":16858,\"start\":16589},{\"end\":17241,\"start\":16961},{\"end\":17799,\"start\":17311},{\"end\":18293,\"start\":17904},{\"end\":18881,\"start\":18295},{\"end\":19537,\"start\":18883},{\"end\":19967,\"start\":19578},{\"end\":20027,\"start\":19969},{\"end\":20141,\"start\":20071},{\"end\":20532,\"start\":20217},{\"end\":20872,\"start\":20534},{\"end\":21225,\"start\":20874},{\"end\":21775,\"start\":21227},{\"end\":22178,\"start\":21777},{\"end\":22658,\"start\":22180},{\"end\":22757,\"start\":22660},{\"end\":22864,\"start\":22808},{\"end\":23398,\"start\":22866},{\"end\":23564,\"start\":23400},{\"end\":23718,\"start\":23580},{\"end\":23779,\"start\":23720},{\"end\":24076,\"start\":23781},{\"end\":24153,\"start\":24137},{\"end\":24161,\"start\":24155},{\"end\":24169,\"start\":24163},{\"end\":24177,\"start\":24171},{\"end\":24185,\"start\":24179},{\"end\":24279,\"start\":24211},{\"end\":29031,\"start\":29022},{\"end\":30151,\"start\":30145},{\"end\":30211,\"start\":30153},{\"end\":30556,\"start\":30213},{\"end\":31047,\"start\":30558},{\"end\":31424,\"start\":31049},{\"end\":31986,\"start\":31451},{\"end\":32703,\"start\":31988},{\"end\":33260,\"start\":32705},{\"end\":37216,\"start\":33302},{\"end\":39007,\"start\":37930},{\"end\":39893,\"start\":39840},{\"end\":40305,\"start\":39946},{\"end\":40406,\"start\":40376},{\"end\":40688,\"start\":40425},{\"end\":41200,\"start\":40690},{\"end\":41392,\"start\":41202},{\"end\":42397,\"start\":41394},{\"end\":43390,\"start\":42427},{\"end\":43653,\"start\":43430},{\"end\":44893,\"start\":43655},{\"end\":45301,\"start\":44895},{\"end\":45784,\"start\":45344},{\"end\":46351,\"start\":45814},{\"end\":46666,\"start\":46353},{\"end\":47912,\"start\":46668},{\"end\":48689,\"start\":47914},{\"end\":50063,\"start\":48716}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8870,\"start\":8786},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9239,\"start\":9156},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9362,\"start\":9309},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9906,\"start\":9807},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9990,\"start\":9985},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10228,\"start\":10198},{\"attributes\":{\"id\":\"formula_6\"},\"end\":10611,\"start\":10550},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14196,\"start\":14121},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14286,\"start\":14270},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14577,\"start\":14443},{\"attributes\":{\"id\":\"formula_10\"},\"end\":16960,\"start\":16859},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17310,\"start\":17242},{\"attributes\":{\"id\":\"formula_12\"},\"end\":17871,\"start\":17800},{\"attributes\":{\"id\":\"formula_13\"},\"end\":20070,\"start\":20028},{\"attributes\":{\"id\":\"formula_14\"},\"end\":20216,\"start\":20142},{\"attributes\":{\"id\":\"formula_15\"},\"end\":22807,\"start\":22758},{\"attributes\":{\"id\":\"formula_16\"},\"end\":24112,\"start\":24077},{\"attributes\":{\"id\":\"formula_17\"},\"end\":26860,\"start\":24280},{\"attributes\":{\"id\":\"formula_18\"},\"end\":27085,\"start\":26860},{\"attributes\":{\"id\":\"formula_19\"},\"end\":27654,\"start\":27325},{\"attributes\":{\"id\":\"formula_20\"},\"end\":27941,\"start\":27654},{\"attributes\":{\"id\":\"formula_21\"},\"end\":29052,\"start\":29032},{\"attributes\":{\"id\":\"formula_22\"},\"end\":39839,\"start\":39792},{\"attributes\":{\"id\":\"formula_23\"},\"end\":39945,\"start\":39894},{\"attributes\":{\"id\":\"formula_24\"},\"end\":40375,\"start\":40306}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":44291,\"start\":44284}]", "section_header": "[{\"end\":3942,\"start\":3927},{\"end\":7925,\"start\":7904},{\"end\":10659,\"start\":10613},{\"end\":15747,\"start\":15711},{\"end\":17902,\"start\":17873},{\"end\":19576,\"start\":19540},{\"end\":23578,\"start\":23567},{\"end\":24135,\"start\":24114},{\"end\":24197,\"start\":24188},{\"end\":24209,\"start\":24200},{\"end\":27324,\"start\":27087},{\"end\":29008,\"start\":27943},{\"end\":29020,\"start\":29011},{\"end\":30143,\"start\":29054},{\"end\":31449,\"start\":31427},{\"end\":33300,\"start\":33263},{\"end\":37928,\"start\":37219},{\"end\":39791,\"start\":39010},{\"end\":40423,\"start\":40409},{\"end\":42425,\"start\":42400},{\"end\":43428,\"start\":43393},{\"end\":45342,\"start\":45304},{\"end\":45812,\"start\":45787},{\"end\":48714,\"start\":48692},{\"end\":50073,\"start\":50065},{\"end\":50128,\"start\":50120},{\"end\":50725,\"start\":50717},{\"end\":51209,\"start\":51201},{\"end\":51267,\"start\":51259},{\"end\":51311,\"start\":51294},{\"end\":51579,\"start\":51571},{\"end\":51675,\"start\":51667},{\"end\":51797,\"start\":51789},{\"end\":51922,\"start\":51913},{\"end\":51997,\"start\":51988}]", "table": null, "figure_caption": "[{\"end\":50118,\"start\":50075},{\"end\":50715,\"start\":50130},{\"end\":51199,\"start\":50727},{\"end\":51257,\"start\":51211},{\"end\":51292,\"start\":51269},{\"end\":51569,\"start\":51314},{\"end\":51665,\"start\":51581},{\"end\":51787,\"start\":51677},{\"end\":51911,\"start\":51799},{\"end\":51986,\"start\":51925},{\"end\":52096,\"start\":51999}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13275,\"start\":13269},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15348,\"start\":15342},{\"end\":30166,\"start\":30160},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":32723,\"start\":32717},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":38731,\"start\":38725},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":44268,\"start\":44242},{\"attributes\":{\"ref_id\":\"fig_15\"},\"end\":45556,\"start\":45550},{\"attributes\":{\"ref_id\":\"fig_15\"},\"end\":45640,\"start\":45634},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46207,\"start\":46200}]", "bib_author_first_name": "[{\"end\":52346,\"start\":52345},{\"end\":52348,\"start\":52347},{\"end\":52359,\"start\":52358},{\"end\":52368,\"start\":52367},{\"end\":52378,\"start\":52377},{\"end\":52389,\"start\":52388},{\"end\":52393,\"start\":52390},{\"end\":52780,\"start\":52779},{\"end\":52792,\"start\":52788},{\"end\":52802,\"start\":52801},{\"end\":52813,\"start\":52812},{\"end\":52815,\"start\":52814},{\"end\":53199,\"start\":53198},{\"end\":53211,\"start\":53210},{\"end\":53221,\"start\":53220},{\"end\":53232,\"start\":53231},{\"end\":53245,\"start\":53244},{\"end\":53247,\"start\":53246},{\"end\":53258,\"start\":53257},{\"end\":53267,\"start\":53266},{\"end\":53277,\"start\":53276},{\"end\":53286,\"start\":53285},{\"end\":53766,\"start\":53765},{\"end\":53778,\"start\":53777},{\"end\":53789,\"start\":53788},{\"end\":53802,\"start\":53801},{\"end\":54197,\"start\":54196},{\"end\":54204,\"start\":54203},{\"end\":54211,\"start\":54210},{\"end\":54218,\"start\":54217},{\"end\":54226,\"start\":54225},{\"end\":54228,\"start\":54227},{\"end\":54668,\"start\":54667},{\"end\":54679,\"start\":54678},{\"end\":54690,\"start\":54689},{\"end\":55090,\"start\":55089},{\"end\":55101,\"start\":55100},{\"end\":55103,\"start\":55102},{\"end\":55114,\"start\":55113},{\"end\":55116,\"start\":55115},{\"end\":55122,\"start\":55121},{\"end\":55135,\"start\":55134},{\"end\":55137,\"start\":55136},{\"end\":55147,\"start\":55146},{\"end\":55545,\"start\":55544},{\"end\":55553,\"start\":55552},{\"end\":55559,\"start\":55558},{\"end\":55566,\"start\":55565},{\"end\":55574,\"start\":55573},{\"end\":55583,\"start\":55582},{\"end\":55836,\"start\":55835},{\"end\":55847,\"start\":55846},{\"end\":55863,\"start\":55859},{\"end\":55873,\"start\":55872},{\"end\":56110,\"start\":56109},{\"end\":56119,\"start\":56118},{\"end\":56128,\"start\":56127},{\"end\":56130,\"start\":56129},{\"end\":56397,\"start\":56396},{\"end\":56406,\"start\":56405},{\"end\":56787,\"start\":56786},{\"end\":56794,\"start\":56793},{\"end\":56806,\"start\":56805},{\"end\":56817,\"start\":56816},{\"end\":57169,\"start\":57168},{\"end\":57183,\"start\":57182},{\"end\":57191,\"start\":57190},{\"end\":57197,\"start\":57196},{\"end\":57208,\"start\":57207},{\"end\":57218,\"start\":57217},{\"end\":57229,\"start\":57228},{\"end\":57237,\"start\":57236},{\"end\":57246,\"start\":57245},{\"end\":57256,\"start\":57255},{\"end\":57593,\"start\":57592},{\"end\":57595,\"start\":57594},{\"end\":57605,\"start\":57604},{\"end\":57607,\"start\":57606},{\"end\":57782,\"start\":57781},{\"end\":57795,\"start\":57794},{\"end\":57810,\"start\":57809},{\"end\":57820,\"start\":57819},{\"end\":57830,\"start\":57829},{\"end\":58124,\"start\":58123},{\"end\":58139,\"start\":58138},{\"end\":58147,\"start\":58146},{\"end\":58466,\"start\":58465},{\"end\":58474,\"start\":58473},{\"end\":58484,\"start\":58483},{\"end\":58495,\"start\":58494},{\"end\":58502,\"start\":58501},{\"end\":58508,\"start\":58507},{\"end\":58516,\"start\":58515},{\"end\":58526,\"start\":58525},{\"end\":58927,\"start\":58926},{\"end\":58937,\"start\":58936}]", "bib_author_last_name": "[{\"end\":52356,\"start\":52349},{\"end\":52365,\"start\":52360},{\"end\":52375,\"start\":52369},{\"end\":52386,\"start\":52379},{\"end\":52399,\"start\":52394},{\"end\":52786,\"start\":52781},{\"end\":52799,\"start\":52793},{\"end\":52810,\"start\":52803},{\"end\":52825,\"start\":52816},{\"end\":53208,\"start\":53200},{\"end\":53218,\"start\":53212},{\"end\":53229,\"start\":53222},{\"end\":53242,\"start\":53233},{\"end\":53255,\"start\":53248},{\"end\":53264,\"start\":53259},{\"end\":53274,\"start\":53268},{\"end\":53283,\"start\":53278},{\"end\":53291,\"start\":53287},{\"end\":53775,\"start\":53767},{\"end\":53786,\"start\":53779},{\"end\":53799,\"start\":53790},{\"end\":53807,\"start\":53803},{\"end\":54201,\"start\":54198},{\"end\":54208,\"start\":54205},{\"end\":54215,\"start\":54212},{\"end\":54223,\"start\":54219},{\"end\":54234,\"start\":54229},{\"end\":54676,\"start\":54669},{\"end\":54687,\"start\":54680},{\"end\":54697,\"start\":54691},{\"end\":55098,\"start\":55091},{\"end\":55111,\"start\":55104},{\"end\":55119,\"start\":55117},{\"end\":55132,\"start\":55123},{\"end\":55144,\"start\":55138},{\"end\":55153,\"start\":55148},{\"end\":55550,\"start\":55546},{\"end\":55556,\"start\":55554},{\"end\":55563,\"start\":55560},{\"end\":55571,\"start\":55567},{\"end\":55580,\"start\":55575},{\"end\":55591,\"start\":55584},{\"end\":55844,\"start\":55837},{\"end\":55857,\"start\":55848},{\"end\":55870,\"start\":55864},{\"end\":55879,\"start\":55874},{\"end\":56116,\"start\":56111},{\"end\":56125,\"start\":56120},{\"end\":56137,\"start\":56131},{\"end\":56403,\"start\":56398},{\"end\":56415,\"start\":56407},{\"end\":56791,\"start\":56788},{\"end\":56803,\"start\":56795},{\"end\":56814,\"start\":56807},{\"end\":56825,\"start\":56818},{\"end\":57180,\"start\":57170},{\"end\":57188,\"start\":57184},{\"end\":57194,\"start\":57192},{\"end\":57205,\"start\":57198},{\"end\":57215,\"start\":57209},{\"end\":57226,\"start\":57219},{\"end\":57234,\"start\":57230},{\"end\":57243,\"start\":57238},{\"end\":57253,\"start\":57247},{\"end\":57261,\"start\":57257},{\"end\":57602,\"start\":57596},{\"end\":57613,\"start\":57608},{\"end\":57792,\"start\":57783},{\"end\":57807,\"start\":57796},{\"end\":57817,\"start\":57811},{\"end\":57827,\"start\":57821},{\"end\":57841,\"start\":57831},{\"end\":58136,\"start\":58125},{\"end\":58144,\"start\":58140},{\"end\":58154,\"start\":58148},{\"end\":58471,\"start\":58467},{\"end\":58481,\"start\":58475},{\"end\":58492,\"start\":58485},{\"end\":58499,\"start\":58496},{\"end\":58505,\"start\":58503},{\"end\":58513,\"start\":58509},{\"end\":58523,\"start\":58517},{\"end\":58535,\"start\":58527},{\"end\":58934,\"start\":58928},{\"end\":58946,\"start\":58938}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":14955348},\"end\":52746,\"start\":52270},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3586416},\"end\":53126,\"start\":52748},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3833774},\"end\":53711,\"start\":53128},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":59599820},\"end\":54104,\"start\":53713},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":38796293},\"end\":54593,\"start\":54106},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":848589},\"end\":55016,\"start\":54595},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14999259},\"end\":55504,\"start\":55018},{\"attributes\":{\"doi\":\"arXiv:1806.00582\",\"id\":\"b7\"},\"end\":55760,\"start\":55506},{\"attributes\":{\"doi\":\"arXiv:1903.02891\",\"id\":\"b8\"},\"end\":56078,\"start\":55762},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":59553531},\"end\":56322,\"start\":56080},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3383786},\"end\":56730,\"start\":56324},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":207244918},\"end\":57107,\"start\":56732},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3635282},\"end\":57549,\"start\":57109},{\"attributes\":{\"id\":\"b13\"},\"end\":57731,\"start\":57551},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15238391},\"end\":58069,\"start\":57733},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6208256},\"end\":58463,\"start\":58071},{\"attributes\":{\"doi\":\"arXiv:1812.02903\",\"id\":\"b16\"},\"end\":58839,\"start\":58465},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":5062760},\"end\":59257,\"start\":58841}]", "bib_title": "[{\"end\":52343,\"start\":52270},{\"end\":52777,\"start\":52748},{\"end\":53196,\"start\":53128},{\"end\":53763,\"start\":53713},{\"end\":54194,\"start\":54106},{\"end\":54665,\"start\":54595},{\"end\":55087,\"start\":55018},{\"end\":56107,\"start\":56080},{\"end\":56394,\"start\":56324},{\"end\":56784,\"start\":56732},{\"end\":57166,\"start\":57109},{\"end\":57779,\"start\":57733},{\"end\":58121,\"start\":58071},{\"end\":58924,\"start\":58841}]", "bib_author": "[{\"end\":52358,\"start\":52345},{\"end\":52367,\"start\":52358},{\"end\":52377,\"start\":52367},{\"end\":52388,\"start\":52377},{\"end\":52401,\"start\":52388},{\"end\":52788,\"start\":52779},{\"end\":52801,\"start\":52788},{\"end\":52812,\"start\":52801},{\"end\":52827,\"start\":52812},{\"end\":53210,\"start\":53198},{\"end\":53220,\"start\":53210},{\"end\":53231,\"start\":53220},{\"end\":53244,\"start\":53231},{\"end\":53257,\"start\":53244},{\"end\":53266,\"start\":53257},{\"end\":53276,\"start\":53266},{\"end\":53285,\"start\":53276},{\"end\":53293,\"start\":53285},{\"end\":53777,\"start\":53765},{\"end\":53788,\"start\":53777},{\"end\":53801,\"start\":53788},{\"end\":53809,\"start\":53801},{\"end\":54203,\"start\":54196},{\"end\":54210,\"start\":54203},{\"end\":54217,\"start\":54210},{\"end\":54225,\"start\":54217},{\"end\":54236,\"start\":54225},{\"end\":54678,\"start\":54667},{\"end\":54689,\"start\":54678},{\"end\":54699,\"start\":54689},{\"end\":55100,\"start\":55089},{\"end\":55113,\"start\":55100},{\"end\":55121,\"start\":55113},{\"end\":55134,\"start\":55121},{\"end\":55146,\"start\":55134},{\"end\":55155,\"start\":55146},{\"end\":55552,\"start\":55544},{\"end\":55558,\"start\":55552},{\"end\":55565,\"start\":55558},{\"end\":55573,\"start\":55565},{\"end\":55582,\"start\":55573},{\"end\":55593,\"start\":55582},{\"end\":55846,\"start\":55835},{\"end\":55859,\"start\":55846},{\"end\":55872,\"start\":55859},{\"end\":55881,\"start\":55872},{\"end\":56118,\"start\":56109},{\"end\":56127,\"start\":56118},{\"end\":56139,\"start\":56127},{\"end\":56405,\"start\":56396},{\"end\":56417,\"start\":56405},{\"end\":56793,\"start\":56786},{\"end\":56805,\"start\":56793},{\"end\":56816,\"start\":56805},{\"end\":56827,\"start\":56816},{\"end\":57182,\"start\":57168},{\"end\":57190,\"start\":57182},{\"end\":57196,\"start\":57190},{\"end\":57207,\"start\":57196},{\"end\":57217,\"start\":57207},{\"end\":57228,\"start\":57217},{\"end\":57236,\"start\":57228},{\"end\":57245,\"start\":57236},{\"end\":57255,\"start\":57245},{\"end\":57263,\"start\":57255},{\"end\":57604,\"start\":57592},{\"end\":57615,\"start\":57604},{\"end\":57794,\"start\":57781},{\"end\":57809,\"start\":57794},{\"end\":57819,\"start\":57809},{\"end\":57829,\"start\":57819},{\"end\":57843,\"start\":57829},{\"end\":58138,\"start\":58123},{\"end\":58146,\"start\":58138},{\"end\":58156,\"start\":58146},{\"end\":58473,\"start\":58465},{\"end\":58483,\"start\":58473},{\"end\":58494,\"start\":58483},{\"end\":58501,\"start\":58494},{\"end\":58507,\"start\":58501},{\"end\":58515,\"start\":58507},{\"end\":58525,\"start\":58515},{\"end\":58537,\"start\":58525},{\"end\":58936,\"start\":58926},{\"end\":58948,\"start\":58936}]", "bib_venue": "[{\"end\":52460,\"start\":52401},{\"end\":52896,\"start\":52827},{\"end\":53369,\"start\":53293},{\"end\":53869,\"start\":53809},{\"end\":54305,\"start\":54236},{\"end\":54765,\"start\":54699},{\"end\":55218,\"start\":55155},{\"end\":55542,\"start\":55506},{\"end\":55833,\"start\":55762},{\"end\":56190,\"start\":56139},{\"end\":56486,\"start\":56417},{\"end\":56880,\"start\":56827},{\"end\":57314,\"start\":57263},{\"end\":57590,\"start\":57551},{\"end\":57876,\"start\":57843},{\"end\":58225,\"start\":58156},{\"end\":58624,\"start\":58553},{\"end\":59011,\"start\":58948},{\"end\":52515,\"start\":52462},{\"end\":52957,\"start\":52898},{\"end\":53441,\"start\":53371},{\"end\":53925,\"start\":53871},{\"end\":54370,\"start\":54307},{\"end\":54827,\"start\":54767},{\"end\":55277,\"start\":55220},{\"end\":56551,\"start\":56488},{\"end\":56929,\"start\":56882},{\"end\":57905,\"start\":57878},{\"end\":58290,\"start\":58227},{\"end\":59070,\"start\":59013}]"}}}, "year": 2023, "month": 12, "day": 17}