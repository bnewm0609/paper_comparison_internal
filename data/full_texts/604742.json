{"id": 604742, "updated": "2023-09-29 18:00:44.53", "metadata": {"title": "Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser", "authors": "[{\"first\":\"Fangzhou\",\"last\":\"Liao\",\"middle\":[]},{\"first\":\"Ming\",\"last\":\"Liang\",\"middle\":[]},{\"first\":\"Yinpeng\",\"last\":\"Dong\",\"middle\":[]},{\"first\":\"Tianyu\",\"last\":\"Pang\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Xiaolin\",\"last\":\"Hu\",\"middle\":[]}]", "venue": "CVPR 2018", "journal": null, "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "Neural networks are vulnerable to adversarial examples. This phenomenon poses a threat to their applications in security-sensitive systems. It is thus important to develop effective defending methods to strengthen the robustness of neural networks to adversarial attacks. Many techniques have been proposed, but only a few of them are validated on large datasets like the ImageNet dataset. We propose high-level representation guided denoiser (HGD) as a defense for image classification. HGD uses a U-net structure to capture multi-scale information. It serves as a preprocessing step to remove the adversarial noise from the input, and feeds its output to the target model. To train the HGD, we define the loss function as the difference of the target model's outputs activated by the clean image and denoised image. Compared with the traditional denoiser that imposes loss function at the pixel-level, HGD is better at suppressing the influence of adversarial noise. Compared with ensemble adversarial training which is the state-of-the-art defending method, HGD has three advantages. First, with HGD as a defense, the target model is more robust to either white-box or black-box adversarial attacks. Second, HGD can be trained on a small subset of the images and generalizes well to other images, which makes the training much easier on large-scale datasets. Third, HGD can be transferred to defend models other than the one guiding it. We further validated the proposed method in NIPS adversarial examples dataset and achieved state-of-the-art result.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1712.02976", "mag": "2952678275", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/LiaoLDPH018", "doi": "10.1109/cvpr.2018.00191"}}, "content": {"source": {"pdf_hash": "1cdfca2da2b6d0d14dd2df1ab4bfdc0ebc0e3480", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1712.02976v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1712.02976", "status": "GREEN"}}, "grobid": {"id": "7b9d0b1fc91ae172a8a22e755e2f52c850613875", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1cdfca2da2b6d0d14dd2df1ab4bfdc0ebc0e3480.txt", "contents": "\nDefense against Adversarial Attacks Using High-Level Representation Guided Denoiser\n\n\nFangzhou Liao liaofangzhou@gmail.com \nTsinghua National Lab for Information Science and Technology State Key Lab of Intelligent Technology and Systems Center for Bio-Inspired Computing Research Department of Computer Science and Technology\nTsinghua University\n\n\nMing Liang \nTsinghua National Lab for Information Science and Technology State Key Lab of Intelligent Technology and Systems Center for Bio-Inspired Computing Research Department of Computer Science and Technology\nTsinghua University\n\n\nYinpeng Dong \nTsinghua National Lab for Information Science and Technology State Key Lab of Intelligent Technology and Systems Center for Bio-Inspired Computing Research Department of Computer Science and Technology\nTsinghua University\n\n\nTianyu Pang \nTsinghua National Lab for Information Science and Technology State Key Lab of Intelligent Technology and Systems Center for Bio-Inspired Computing Research Department of Computer Science and Technology\nTsinghua University\n\n\nJun Zhu \nTsinghua National Lab for Information Science and Technology State Key Lab of Intelligent Technology and Systems Center for Bio-Inspired Computing Research Department of Computer Science and Technology\nTsinghua University\n\n\nXiaolin Hu xlhu@mail.tsinghua.edu.cn \nTsinghua National Lab for Information Science and Technology State Key Lab of Intelligent Technology and Systems Center for Bio-Inspired Computing Research Department of Computer Science and Technology\nTsinghua University\n\n\nDefense against Adversarial Attacks Using High-Level Representation Guided Denoiser\n\nNeural networks are vulnerable to adversarial examples. This phenomenon poses a threat to their applications in security-sensitive systems. It is thus important to develop effective defending methods to strengthen the robustness of neural networks to adversarial attacks. Many techniques have been proposed, but only a few of them are validated on large datasets like the ImageNet dataset. We propose high-level representation guided denoiser (HGD) as a defense for image classification. HGD uses a U-net structure to capture multi-scale information. It servers as a preprocessing step to remove the adversarial noise from the input, and feeds its output to the target model. To train the HGD, we define the loss function as the difference of the target model's outputs activated by the clean image and denoised image. Compared with the traditional denoiser that imposes loss function at the pixel-level, HGD is better at suppressing the influence of adversarial noise. Compared with ensemble adversarial training which is the state-of-the-art defending method, HGD has three advantages. First, with HGD as a defense, the target model is more robust to either whitebox or black-box adversarial attacks. Second, HGD can be trained on a small subset of the images and generalizes well to other images, which makes the training much easier on large-scale datasets. Third, HGD can be transferred to defend models other than the one guiding it. We further validated the proposed method in NIPS adversarial examples dataset and achieved state-of-the-art result.\n\nIntroduction\n\nAs many other machine learning models [2], neural networks are known to be vulnerable to adversarial examples [22,5]. Adversarial examples are maliciously designed inputs which have small perturbations on original inputs, but can mislead the target model. An even worse fact is that adversarial examples can be transferred across different models [22,15]. This transferability enables black-box adversarial attacks without knowing the weights and structures of the target model. That is, adversarial examples are trained on a different model and then used to attack the target model. Black-box attacks have been shown to be feasible in real-world scenarios [16], which poses a potential threat to security-sensitive deep learning applications, such as identity authentication and autonomous driving. It is thus important to find effective defensive methods against adversarial attacks.\n\nSince adversarial examples are constructed by adding noises to original images, a natural idea is to denoise the adversarial examples before sending them to a recognition model ( Figure 1). Compared with adversarial training method, this method is more interpretable, which is beneficial for the understanding of adversarial examples as well as deep neural networks.\n\nTwo models are explored for denoising adversarial examples, and it is found that the noise level indeed can be reduced, which proves that the idea is feasible. However, none of the models can remove all perturbations introduced by the attacks, and small perturbation in the image could be amplified to large perturbation in higher-level representation (called \"error amplification effect\"), which leads to a wrong prediction. To solve this problem, instead of imposing the loss function at the pixel-level, we set the loss function of the denoiser as minimizing the difference of toplevel outputs of the target model induced by the perturbed and clean images ( Figure 1). We name the denoiser trained by this loss function \"high-level representation guided denoiser\" (HGD).\n\nCompared with ensemble adversarial training [23], Figure 1: The idea of high-level representation guided denoising. The difference between the original image and adversarial image is tiny, but the difference is amplified in high-level representation (logits for example) of a CNN. We use the distance in high-level representation to guide the training of an image denoiser to suppress the influence of adversarial perturbation.\n\nwhich is the current state-of-the-art method, the proposed method has the following advantages: first, it achieves much higher accuracy when defending both white-box and black-box attacks with only a slight decrease in accuracy on clean images. Second, it requires much less training data and training time. Third, it is more interpretable and can be transferred across different target models.\n\nWe further validate the performance of HGD on NIPS adversarial examples dataset. Our HGD method significantly outperforms other methods by a significant margin.\n\n\nBackground and Related Work\n\nIn this section, we first specify some of the notations used in this paper. Let x denote the clean image from a given dataset, and y denote the class. The ground truth label is denoted by y true . A neural network f : x \u2192 y is called the target model. Given an input x, its feature vector at layer l is f l (x), and its predicted probability of class y is p(y|x). y x = arg max y p(y|x) is the predicted class of x. J(x, y) denotes the loss function of the classifier given the input x and its target class y. For image classification, J(x, y) is often chosen to be the cross-entropy loss. We use x * to denote the adversarial example generated from x. is the magnitude of adversarial perturbation, measured by some distance metric.\n\n\nExisting methods for adversarial attacks\n\nAdversarial examples [22] are maliciously designed inputs which have a negligible difference from clean images but cause the classifier to give wrong classifications. That is, for x * with a sufficiently small perturbation magnitude , y x * = y x .\n\nSzegedy et al. [22] used a box-constrained L-BFGS algorithm to generate a targeted adversarial example. More specifically, they minimize the weighted sum of and J(x * , y target ) while constraining the elements of x * to be within the range of pixel value.\n\nGoodfellow et al. [5] suggest that adversarial examples can be caused by the summed effects of high dimensional model weights, and they further propose a simple adversarial attack algorithm, called Fast Gradient Sign Method (FGSM):\nx * = x + \u00b7 sign(\u2207 x J(x, y)).(1)\nFGSM only computes the gradients once, and thus is much more efficient than L-BFGS. In early practices, FGSM uses the true label y = y true to compute the gradients. This approach is suggested to have the label leaking [12] effect. A better alternative is to replace y true with the model predicted class y x . FGSM is untargeted and aims to increase the overall loss. The targeted FGSM can be obtained by modifying FGSM to maximize the predicted probability of a specific target y target :\nx * = x \u2212 \u00b7 sign(\u2207 x J(x, y target ))(2)\ny target can be chosen as the least likely class predicted by the model or a random class. Kurakin et al. [12] propose an iterative FGSM (IFGSM) attack by repeating FGSM for n steps (IFGSMn). IFSGM usually results in higher classification error than FGSM. The model used to generate adversarial attacks is called the attacking model, which can be a single model or an ensemble of models [23]. When the attacking model is the target model itself or contains the target model, the resulting attacks are white-box. An intriguing property of adversarial examples is that they can be transferred to different models and datasets [22,5]. This property enables black-box attacks. Practical black-box attacks have been demonstrated in some real-world scenarios [16,15]. As white-box attacks are less likely to happen in real systems, defense against black-box attacks has more practical meaning.\n\n\nExisting methods for defenses\n\nAdversarial training [5,12,23] is one of the most extensively investigated defenses against adversarial attacks. It aims to train a robust model from scratch on a training set augmented with adversarially perturbed data [5,12,23]. Adversarial training can improve the classification accuracy of the target model on adversarial examples [22,5,12,23].\n\nIt even improves the accuracy of clean images on some small datasets [22,5], although this effect is not found on the large ImageNet dataset. However, adversarial training is more time consuming than training on clean images only, because online adversarial example generation costs extra computation, and it is harder to fit both the adversarial and clean examples. These limitations hinder the usage of newer and harder attacks in adversarial training, and practical adversarial training on the ImageNet dataset only adopts FGSM.\n\nAnother family of adversarial defenses is based on the so-called gradient masking effect [16,17,23]. These defenses apply some regularizers or smooth labels into the training process so that the output of learned models are less sensitive to the perturbation of input. Gu and Rigazio [6] propose the deep contrastive network, whose loss function includes a layer-wise contrastive penalty term. Such penalty forces the output of each layer invariant to the perturbation of its input. Papernot et al. [18] adapts knowledge distillation [9] to adversarial defense. The output of another model is used as soft labels to train the target model. Nayebi and Surya [14] use saturating networks for adversarial robustness. A penalty term in the loss function encourages the activations to be in their saturating regime.\n\nThe basic problem with these gradient masking approaches is that they do not solve the vulnerability of the models to adversarial attacks, but makes the construction of white-box adversarial examples more difficult. These defenses suffer from black-box attacks [16,23] generated on some other models. Besides, as far as we know, most of them have not been validated in ImageNet-level tasks.\n\n\nMethods\n\n\nPixel guided denoiser\n\nDuring the adversarial training process, the two tasks of classifying clean images and defending against adversarial noises are coupled together, which is more difficult to learn than each task alone. This coupling may be unnecessary. A more straightforward way is learning to predict the adversarial noise only. The predicted noise can then be removed from the input before it is fed to the target model. This denoising approach is conducted as a preprocessing step, and the target model is not altered.\n\nIn this section, we introduce a set of denoising networks and their motivations. These denoisers are designed in the context of image classification on the ImageNet [4]. They are used in conjunction with a pretrained classifier f (By default Inception V3 [21] in this study). The denoising function is denoted as D : x * \u2192x, wherex denotes the denoised image. The loss function is: where | \u00b7 | stands for the L 1 norm. Since the loss function is defined at the pixel-level, we name this kind of denoiser pixel guided denoiser (PGD).\nL = |x \u2212x|,(3)\n\nDenoising U-net\n\nDenoising autoencoder (DAE) [24] is a potential choice of the denoising network. In previous work [6], DAE in the form of a multi-layer perceptron is used to defend images against adversarial attacks. However, their experiments were conducted over the relatively simple MNIST [13] dataset. To better represent the high-resolution images in the ImageNet, we use a convolutional version of DAE. DAE has a bottleneck for the transmission of fine-scale information between the encoder and decoder. This bottleneck structure may not be capable of carrying the multiscale information contained in the images. In other tasks requiring full resolution output, the network often has a multiscale architecture, such as U-net [19] used for medical image segmentation. Here we use a denoising U-net.\n\nSo we designed a denoising U-net (DUNET). Compared with DAE, the DUNET adds some lateral connections from encoder layers to their corresponding decoder layers of the same resolution ( Figure 2). These lateral connections serve as shortcuts to transmit fine-scale information. Thus, along with the top-down processing of the decoder, the fine-scale (i.e., low-level) information is gradually integrated with the coarse-scale (i.e., high-level) information. Note that there is also a shortcut from input to output to additively combine them. In this way, the network is learning to predict adversarial noise only (dx in Figure 2), which is more relevant to denosing and easier than reconstructing the whole image [26]. The clean image can be readily obtained by substracting the noise (adding -dx)from the corrupted input.\n\n\nNetwork structure\n\nWe use DUNET as an example to illustrate the architecture ( Figure 3). DAE can be obtained simply by removing the lateral connections from DUNET. C is defined as a stack of a 3 \u00d7 3 convolutional layer, a batch normalization layer [10] and a rectified linear unit, and Ck is defined as k consecutive C. The network is composed of a feedforward path and a feedback path. The feedforward path is composed of five blocks, corresponding to one C2 and four C3, respectively. The stride of the first convolutional layer of each C3 is set to 2 \u00d7 2 for downsampling. The feedforward path receives the image as input, and generates a set of feature maps of increasingly lower resolutions (See the top pathway of Figure  3).\n\nThe feedback path is composed of four blocks and a 1\u00d71 convolution. Each block receives a feedback input from the feedback path and a lateral input from the feedforward path. It first upsamples the feedback input to the same size as the lateral input using bilinear interpolation, and then processes the concatenated feedback and lateral inputs with a Ck. From top to bottom, three C3 and one C2 are used by the four blocks, respectively. Along the feedback path, the resolution of feature maps is progressively larger. The output of the last block is transformed to a predicted negative noise map \u2212dx by the 1 \u00d7 1 convolution. (See the bottom pathway of Figure 3)The final output is the sum of the negative noise map and the input image:\nx = x * \u2212 dx.(4)\n\nHigh-level representation guided denoiser\n\nA potential problem with PGD is the amplifying effect of adversarial noise. Adversarial examples have negligible differences from the clean images. However, this small perturbation is progressively amplified by deep neural networks and yields a wrong prediction. Even if the denoiser can significantly suppress the pixel-level noise, the remaining noise may still distort the high-level responses of the target model. Refer to Section 5.1 for details.\n\nTo overcome this problem, we replace the pixel-level loss function with the reconstruction loss of the target model's outputs. More specifically, given a target neural network, we extract its representation at l-th layer for x and x, and calculate the loss function as:\nL = |f l (x) \u2212 f l (x)|.(5)\nThe corresponding models are called HGD, in that the supervised signal comes from certain high-level layers of the classifier and carries guidance information related to image classification. HGD uses the same U-net structure as DUNET. They only differ in their loss functions. We propose two HGDs with different choices of l. For the first HGD, we define l = \u22122 as the index of the topmost convolutional layer. The activations of this layer are fed to the linear classification layer after global average pooling, so it is more related to the classification objective than lower convolutional layers. This denoiser is called feature guided denoiser (FGD) (see Figure 4a). For the second HGD, we define l = \u22121 as the index of the layer before the final softmax function, which is called the logits. So it is called logits guided denoiser (LGD). In this case, the loss function is the difference between the two logits activated byx and x (see Figure 4b). We consider both FGD and LGD for the following reason. The convolutional feature maps provide richer supervising information, while the logits directly represent the classification results.\n\nAll PGD and these HGDs are unsupervised models, in that the ground truth labels are not needed in their training process. The denoiser just learns to remove the perturbation in the input or the feature maps of the target model. An alternative is to use the classification loss of the target model \n\n\nExperimental settings\n\nThroughout experiments, the pre-trained Inception v3 (IncV3) [21] is assumed to be the target model that attacking models attempt to fool and our denoisers attempt to defend. Therefore this model is used for training the three HGDs illustrated in Figure 4. However, it will be seen that the HGDs trained with this target model can also defend other models (see Section 5.3).\n\n\nDataset\n\nFor both training and testing of the proposed method, adversarial images are needed. To prepare the training set, we first extract 30K images from the ImageNet training set (30 images per class). Then we use a bunch of adversarial attacking models to distort these images and form a training set by collecting the outputs of all models. Different attacking methods including FGSM and IFGSM are applied to the following models: Pre-trained IncV3, InceptionResnet v2 (IncResV2) [20], ResNet50 v2 (Res) [8] individually or in combinations (the same model ensemble as the work of Tramer et al. [23] ). For each training sample, the perturbation level is uniformly sampled from integers in [1,16]. See Table 1 for details. As a consequence, we gather 210K images in the training set (TrainSet).\n\nTo prepare the validation set, we first extract 10K images from the ImageNet training set (10 images per class), then apply the same method as described above. Therefore the size of the validation set (ValSet) is 70K.\n\nTwo different test sets are needed, one for white-box attack (WhiteTestSet) 1 and the other for black-box attack (BlackTestSet). They are obtained from the same clean 10K images from the ImageNet validation set (10 im- 1 The white-box attacks defined in this paper should be called oblivious attacks according to Carlini and Wagner's definition [3]  ages per class) but using different attacking methods. The WhiteTestSet uses two attacks targeting at IncV3, which are also used for generating training images, and the BlackTest-Set uses two attacks based on a holdout model Pre-trained Inception V4 (IncV4) [20], which is not used for generating training images. Every attacking method is conducted on two perturbation levels \u2208 {4, 16}. So both WhiteTestSet and BlackTestSet have 40k images (see Table 1 for details).\n\n\nImplementation details\n\nThe denoisers are optimized using Adam [11]. The learning rate is initially set to 0.001, and decayed to 0.0001 when the training loss converges. The model is trained on six GPUs and the batch size is 60. The number of training epochs ranges from 20 to 30, depends on the convergence speed of the model. The model with the lowest validation loss is used for testing.\n\n\nResults\n\n\nPGD and the error amplication effect\n\nThe results of DAE and DUNET on the test sets are shown in Table 2. The original IncV3 without any defense  is used as a baseline, denoted as NA. For all types of attacks, DUNET has much lower denoising loss than DAE and NA, which demonstrates the structural advantage of DUNET. DAE does not perform well on encoding the highresolution images, as its accuracy on clean images significantly drops. DUNET slightly decreases the accuracy of clean images, but significantly improves the robustness of the target model to black-box attacks. In the following content, DUNET is used as the default PGD method. A notable result is that the denoising loss and classification accuracy of PGD are not so consistent. For white-box attacks, DUNET has much lower denoising loss than DAE, but its classification accuracy is significantly worse. To investigate this inconsistency, we analyze the layer-wise perturbations of the target model activated by PGD denoised images. Let x p denote a perturbed image. The perturbation level at layer l is computed as: The E l for PGD denoised images, adversarial images, and Gaussian noise perturbed images are shown in Figure 5. The latter two are used as baselines. For convenience, they are abbreviated as PGD perturbation, adversarial perturbation, and random perturbation. 30 images generated by the attack \"IFGSM4 x IncV3/IncResV2/Res ( = 16)\", which shows a high inconsistency, are used in this experiment. Although the pixel-level PGD perturbation significantly suppressed, the remaining perturbation is progressively amplified along the layer hierarchy. At the top layer, PGD perturbation is much higher than random perturbation and close to adversarial perturbation. Because the classification result is closely related to the top-level features, this large perturbation well explains the inconsistency between the denoising performance and classification accuracy of PGD.\nE l (x p , x) = |f l (x p ) \u2212 f l (x)|/|f l (x)|.(6)\n\nEvaluation results of HGD\n\nCompared to PGD, LGD strongly suppressed the error amplification effect (the black curve in Figure 5). LGD perturbation at the final layer is much lower than PGD and adversarial perturbations and close to random perturbation.\n\nHGD is more robust to white-box and black-box adversarial attacks than PGD and ensV3 (Table 8). All three HGD methods significantly outperform PGD and ensV3 for all types of attacks. The accuracy of clean images only slightly decreases (by 0.5% for LGD). The difference between these HGD methods is insignificant. In later sections, LGD is chosen as our default HGD method for it achieves a good balance between accuracy on clean and adversarial images.\n\nCompared to adversarial training, HGD only uses a small fraction of training images and is efficient to train. Only 30K clean images are used to construct our training set, while all 120K clean images of the ImageNet dataset are used for training ensV3. HGD is trained for less than 30 epochs on 21K images, while ensV3 is trained for about 200 epochs on 120K images [23].\n\nTo summary, with less training data and time, HGD significantly outperforms adversarial training on defense against adversarial attacks. These results suggest that learning to denoise only is much easier than learning the coupled  task of classification and defense.\n\n\nTransferability of HGD\n\nUnlike adversarial training, HGD uncouples defense from classification, and its learning objective is the adversarial noise itself. As adversarial attacks can be transferred on different models and datasets [22], HGD is expected to have similar transferabilities.\n\nTo evaluate the transferability of HGD over different models, we use the IncV3 guided LGD to defend Resnet [7]. As expected, this LGD significantly improve the robustness of Resnet to all attacks. Furthermore, it achieves very close defending performance as the Resnet guided LGD. (Table  5) To evaluate the transferability of HGD over different classes, we build another dataset. Its key difference from the original dataset is that there are only 750 classes in the TrainSet, and the other 250 classes are put in ValSet and TestSets. The number of original images in each class in all datasets are changed to 40 to keep the size of dataset unchanged. It is found that although the 250 classes in the test set are never trained, the LGD still learns to defend against the attacks targeting at them (Table 4).\n\n\nQualitative analysis on denoised images\n\nHGD is much more robust to adversarial attacks than PGD. However, HGD denoised images have higher pixellevel noise than adversarial images (see Figure 5), indicating HGD even increases the \"noise\" on images. In this section, we qualitatively analyze this phenomenon. The images used in the experiments are the same as those in Section 5.1.\n\nAn example image, its adversarial image, and its denoised outputs by PGD and LGD are shown in Figure 6. It can be observed that LGD does not suppress the overall pixel-level perturbation like what PGD does. Instead, it even increases the perturbation level. To investigate this issue further, We plot the 2D histogram of the adversarial perturbation (dx * = x * \u2212 x) and the predicted perturbation (dx = x * \u2212x) given by the PGD and LGD ( Figure  7). The ideal result should be dx = dx * , which means the adversarial perturbations are completely removed. Two lines dx = kdx * are fit for PGD and LGD, respectively (the red lines in Figure 7). The slope of PGD's line is lower than 1, indicating that PGD only removes a portion of the adversarial noises. On contrary, the slope of LGD's line is even larger than 1. Moreover, the estimation is very noisy, which leads to high noise in the pixel level. These results suggest that LGD may denoise the images in a relatively aggressive way.\n\n\nThe solution in NIPS dataset\n\nThe NIPS adversarial examples dataset provided 5000 ImageNet-compatible original images, on which 91 nontargeted attacks and 65 targeted attacks are conducted. All these images and attacks are unknown to users, and the evaluation is conducted on the cloud by organizers. Each defending method is evaluated on all the attacks, and a normalized score is calculated based on the accuracy of all attacks.\n\nTo achieve higher performance, we gathered 14 powerful attacks, all with = 16. Most of them are iterative attacks on an ensemble of many models (for details, please refer to supplementary file). We choose four pre-trained models (ensV3 [23], ensIncResV2 [23], Resnet152 [7], ResNext101 [25]) and trained a FGD for each one. The logit output of the four defended models are averaged, and the class with the highest score is chosen as the prediction.\n\nWithout elaborately parameter tuning, our solution achieved the best result with a significantly higher score and shorter evaluating time than other top methods (Table 6).\n\n\nConclusion\n\nIn this study, we discovered the error amplification effect of adversarial examples in neural networks and proposed to LGD dx = 1. 05dx * Figure 7: The relationship between dx * and dx in PGD and HGD.\n\nuse the error in the top layers of the neural network as loss functions to guide the training of an image denoiser. This method turned to be very robust against both white-box and black-box attacks. The proposed HGD has simple training procedure, good generalization, and high flexibility.\n\nIn future work, we aim to build an optimal set of training attacks. The denoising ability of HGD depends on the representability of the training set. In current experiments, we used FGSM and iterative attacks. Incorporating other different attacks, such as the attacks generated by adversarial transformation networks [1], probably improves the performance of HGD. It is also possible to explore an end-to-end training approach, in which the attacks are generated online by another neural network. \n\n\nSupplementary materials\n\nA. Combination of HGD and adversarial trained model\n\nAs two different approaches, denoising networks and adversarial training may have complementary effects, use a HGD to process the distorted images before they are inputted to an adversarially trained model may further improve the performance. To test this idea, we train an LGD with ensemble adversarially trained Inception V3 (ensV3) [23] as the target model. For fair comparison, we replace the attack methods targeting at IncV3 (and other models) with attack methods targeting at ensV3 (and other models) and make a new dataset correspondingly. The\n\nLGD+ensV3 model is trained and tested on this new dataset.\n\nBecause ensV3 is more robust than IncV3, we expect to see higher robustness in LGD+ensV3. However, the results show that although LGD helps the ensv3 to improve the robustness, their combination is not significantly better than the LGD+IncV3 (Table 7) (The results of LGD+IncV3 are copied from Table 3.).\n\n\nB. Remedy for the low slope of PGD\n\nIn Section 5.4 of the paper, we show the different denoising properties of PGD and LGD (dx = kdx * ), and it may be inferred that the low k value of PGD is an important factor for PGD's poor performance. To validate this assumption, we replace the output of PGD withx = x * \u2212 2dx (i.e. replace \u2212dx in Fig. 2 by \u22122dx), so that its k is close to 1. The results (denoted by PGD x2 in Table 8) are significantly higher than those of the original PGD but still not as high as those of LGD. Besides, this change also significantly decreases the accuracy on clean images. Therefore the low k value of PGD is indeed a reason for its poor robustness, but it cannot explain all difference between PGD and LGD.\n\n\nC. Ensembles of HGD protected models\n\nEnsemble is an effective method to boost the performance of classifiers. We explore the ensemble of several HGDs and target models. Specifically, we train two\n\nLGDs. The first one is denoted by LGD1, which is trained with IncV3 as the target model and the second one is denoted by LGD2, which is trained with ensV3 as the target model 2 . Different combinations of the two denoisers and various target models are tested:\n\n\u2022 IncV3&ensV3. The adversarial images (x * ) are fed directly to the two models without protection, and the output logits of IncV3 and ensV3 are averaged as the result. The symbol & indicate an ensemble.\n\nMotivation: This method serves as the baseline ensemble.\n\n\u2022 LGD1\u2192 IncV3&ensV3. x * is firstly fed to the LGD1, resulting in a denoised imagex, which is then fed to the ensemble of IncV3&ensV3. The \u2192 indicates the flow of data.\n\nMotivation: LGD shows certain transferability across different models (Section 5.3 in main paper). So it is possible to use an LGD to protect multiple models.\n\n\u2022 (LGD1\u2192IncV3&ensV3)&(LGD2\u2192IncV3&ensV3).\n\nLGD1 and LGD2 give two denoised imagesx 1 andx 2 , which are then fed to IncV3&ensV3 independently. The four output logits are averaged as the result.\n\nMotivation: This method is similar with the last one, but make use of LGD2.\n\n\u2022\n\nLGD1&LGD2\u2192IncV3&ensV3. The output of LGD1 and LGD2 are averaged (x = (x 1 +x 2 )/2). The averaged denoised image is then fed to IncV3&ensV3.\n\n\nMotivation: Each\n\nLGD give an independent estimation of the adversarial perturbation, so averaging the outputs of two LGD may result in a better estimation of perturbation.\n\n\u2022 (LGD1\u2192IncV3)&(LGD2\u2192ensV3).x 1 andx 2 are fed to IncV3 and ensV3 respectively. The logits of the two models are averaged as result.\n\nMotivation: The most straightforward way of ensemble.\n\nExcept for these ensembles, two single model baselines\n\nLGD1\u2192IncV3 and\n\nLGD2\u2192ensV3 are also tested.\n\nThe results of these methods are shown in Table 9. (LGD1\u2192IncV3)&(LGD2\u2192ensV3) performs the best and shows consistent improvement comparing to baselines. Other ensemble methods achieve little improvements comparing with the single models. Some of them even have degraded performance.\n\n\nD. The details of NIPS 2017 solution\n\nIn this section, we list the attacks we used to generate the training and validation sets for training the denoiser. is the L \u221e constraint of the adversarial perturbation. In our experiment, = 16 is fixed.\n\n\u2022 No operation. The clean images are directly adopted.\n\n\u2022 Images withs random noise. Each pixel of an original image is randomly perturbed with or \u2212 with equal probability.\n\n\u2022 FGSM x IncV3. The word before \"x\" indicates the attacking method, and the word after it indicates the target model. FGSM meas fast gradient sign method (defined in the main paper).\n\n\u2022 FGSM x ensV3&advV3. advV3 is an adversarially trained Inception V3 model [12].\n\n\u2022 FGSM x IncV3&IncV4&ensIncResV2. IncV4 is Inception V4 [20]. EnsIncResV2 is an ensemble adversarially trained InceptionResnet V2 [20,23].\n\n\u2022 IFGSM 2 x 7 models. IFGSM k means k step iterative FGSM. The step size of each step is /k. 7 models means the ensemble of IncV3, advV3, ensV3, ens4V3 [23], IncV4, IncResV2 and ensIn-cResV2. ens4V3 is another ensemble adversarially trained Inception V3 [23]. IncResV2 is the Inception-Resnet V2 [20].\n\n\u2022 IFGSM 4 x 7 models.\n\n\u2022 IFGSM 8 x 7 models.\n\n\u2022 IFGSM 2 x 8 models. 8 models means the ensemble of the 7 models mentioned above and Resnet101 [7].\n\n\u2022 IFGSM 8 x 8 models.\n\n\u2022 dIFGSM 4 x 8 models. dIFGSM means IFGSM with decayed step size, which is set as 0.4 , 0.3 , 0.2 , 0.1 for dIFGSM 4 .\n\n\u2022 adaIFGSM 1 x 8 models. adaIFGSM means adaptive IFGSM. In adaIFGSM, there is a list of IFGSM attacks with increasing power and running time (i.e. more iterations). An original image is firstly perturbed by the simplest attack. If this adversarial image successfully fools all the target models, it is used as output and the procedure ends, else the original image would be perturbed by the attack in the next level. This procedure continues until the adversarial image fools all the target models or the last attack in the list is used.\n\nIn adaIFGSM 1 , the attack list is (dIFGSM 4 x 8 models, IFGSM 20 x 8 models).\n\n\u2022 adaIFGSM 2 x 8 models. In adaIFGSM 2 , the attack list is (dIFGSM 3 x 8 models, dIFGSM 4 x 8 models, IFGSM 20 x 8 models). The step size is set as 1 2 , 1 3 , 1 6 for dIFGSM 3 .\n\n\u2022 adaIFGSM 3 x 8 models. In adaIFGSM 3 , the attack list is (FGSM x 8 models, IFGSM 2 x 8 models, IFGSM 4 x 8 models, IFGSM 8 x 8 modelS, IFGSM 20 x 8 models).\n\nThese attacks are applied to 15,000 original images, resulting in a training set of 210,000 adversarial images. And they are applied to other 5,000 original images, resulting in a validation set of 70,000 adversarial images. \n\nFigure 2 :\n2Diagrams of DAE (left) and DUNET (right).\n\nFigure 3 :\n3The detail of DUNET. The numbers inside each cube stand for width \u00d7 height, and the number outside the cube stands for the number of channels. In all the C3 of the feedforward path, the stride of the first C is 2 \u00d7 2.\n\nFigure 4 :\n4Three different training methods for HGD. The square boxes stand for data blobs, the circles and ovals stand for networks. D stands for denoiser. CNN is the model to be defended. The parameters of the CNN are shared and fixed.as the denoising loss function, which is supervised learning as ground truth labels are needed. This model is called class label guided denoiser (CGD) (seeFigure4c).\n\nFigure 5 :\n5Layerwise perturbation levels of the target model. Adversarial, Random noise, PGD and LGD correspond to the E l for adverarial images, Gaussian noise perturbed images, PGD denoised images, and LGD denoised images, respectively.\n\nFigure 6 :\n6The first row is an original image (1st column), adversarial image (2nd column), denoised adversarial image generated by PGD (3rd column) and LGD (4th column). The second row shows zoomed in images. The third row visualizes the L 1 norm of differences between the original image and the last three images in the second row, relatively.\n\nTable 1 :\n1Adversarial images generated by different models for training and testing.Attacking \nmethod \nAttacked model \n\nTrainSet \nand ValSet \n\nFGSM \nIncV3 \n\n[1,16] \n\nFGSM \nIncResV2 \nFGSM \nRes \nFGSM \nIncV3/IncResV2/Res \nIFGSM2 \nIncV3/IncResV2/Res \nIFGSM4 \nIncV3/IncResV2/Res \nIFGSM8 \nIncV3/IncResV2/Res \n\nWhiteTestSet \nFGSM \nIncV3 \n{4,16} \nIFGSM4 \nIncV3/IncResV2/Res \n\nBlackTestSet \nFGSM \nIncV4 \n{4,16} \nIFGSM4 \nIncV4 \n\n\n\nTable 2 :\n2Denosing loss and classification accuracy of different PGD methods on the test sets. Denosing loss is the L 1 distance between the input image and the denoised image. NA means no defense. Clean stands for original images.Defense \nClean \nWhiteTestSet \nBlackTestSet \n= 4 \n= 16 \n= 4 \n= 16 \nNA \n0.0000 0.0177 0.0437 0.0176 0.0451 \nDAE \n0.0360 0.0359 0.0360 0.0360 0.0369 \nDUNET 0.0150 0.0140 0.0164 0.0140 0.0181 \nNA \n76.7% 14.5% \n14.4% \n61.2% \n41.0% \nDAE \n58.3% 51.4% 36.7% \n55.9% \n48.8% \nDUNET 75.3% 20.0% \n13.8% 67.5% 55.7% \n\n\n\nTable 3 :\n3The classification accuracy on test sets obtained by different defenses. NA means no defense.Defense \nClean \nWhiteTestSet \nBlackTestSet \n= 4 \n= 16 \n= 4 \n= 16 \nNA \n76.7% 14.5% \n14.4% \n61.2% \n41.0% \nPGD \n75.3% \n20.0% \n13.8% \n67.5% \n55.7% \nensV3 [23] 76.9% 69.8% \n58.0% \n72.4% \n62.0% \nFGD \n76.1% \n73.7% \n67.4% \n74.3% \n71.8% \nLGD \n76.2% 75.2% \n69.2% 75.1% 72.2% \nCGD \n74.9% 75.8% 73.2% 74.5% \n71.1% \n\n\n\nTable 4 :\n4The transferability of HGD to different classes. The 1000 ImageNet classes are separated in training and test test.Defense Clean \nWhiteTestSet \nBlackTestSet \n= 4 \n= 16 \n= 4 \n= 16 \nNA \n76.6% 14.5% \n14.4% \n61.2% \n41.0% \nLGD \n76.3% 73.9% 65.7% 74.8% 72.2% \n\n\n\nTable 5 :\n5The transferability of HGD to different model. Resnet is used as the target model in this table.Denoiser for \nResnet \nClean \nWhiteTestSet \nBlackTestSet \n= 4 \n= 16 \n= 4 \n= 16 \nNA \n78.5% 63.3% \n38.4% \n67.8% \n48.6% \nIncV3 guided \nLGD \n77.4% 75.8% \n71.7% \n76.1% \n72.7% \n\nResnet guided \nLGD \n78.4% 76.1% 72.9% 76.5% 74.6% \n\n\n\nTable 6 :\n6The results on the NIPS dataset. Time stands for average evaluating time.Team/Method Normalized Score Time(s) \n\niyswim \n0.9235 \n121.8 \nAnil Thomas \n0.9148 \n95.3 \nerko \n0.9120 \n86.4 \nFGD(ours) \n0.9532 \n50.2 \n\n\n\nTable 7 :\n7The influence of adding a LGD before ensV3. For fair comparison, the white-box attacks (WhiteTestSet) used in the two blocks are targeting at IncV3 and ensV3 respectively.Defense \nClean \nWhiteTestSet \nBlackTestSet \n= 4 \n= 16 \n= 4 \n= 16 \n\nIncV3 \n76.7% 14.5% 14.4% \n61.2% \n41.0% \nLGD + IncV3 76.2% 75.2% 69.2% \n75.1% 72.2% \nensV3 \n76.9% 71.4% \n61.7% \n72.4% \n62.0% \nLGD + ensV3 76.9% 75.0% \n72.5% 74.7% \n72.1% \n\n\n\nTable 8 :\n8The classification accuracy on test sets obtained by different defenses. NA means no defense.Defense \nClean \nWhiteTestSet \nBlackTestSet \n= 4 \n= 16 \n= 4 \n= 16 \nNA \n76.7% 14.5% \n14.4% \n61.2% \n41.0% \nPGD \n75.3% 20.0% \n13.8% \n67.5% \n55.7% \nPGD x2 73.7% 40.7% \n50.8% \n70.9% \n67.4% \nLGD \n76.2% 75.2% 69.2% 75.1% 72.2% \n\n\n\nTable 9 :\n9The classification accuracy on the test set obtained by different methods.Defense \nClean \nWhiteTestSet \nBlackTestSet \n= 4 \n= 16 \n= 4 \n= 16 \nLGD1\u2192IncV3 \n76.2% 75.2% \n69.2% \n75.1% 72.2% \nLGD2\u2192ensV3 \nDifferent from Section A, the LGD2 used in this section is trained and evaluated on the default dataset.\n\nAdversarial transformation networks: Learning to generate adversarial examples. Shumeet Baluja, Ian Fischer, arXiv:1703.09387arXiv preprintShumeet Baluja and Ian Fischer. Adversarial trans- formation networks: Learning to generate adversarial examples. arXiv preprint arXiv:1703.09387, 2017.\n\nEvasion attacks against machine learning at test time. Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Pavel Nedim\u0161rndi\u0107, Giorgio Laskov, Fabio Giacinto, Roli, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim\u0160rndi\u0107, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against ma- chine learning at test time. In Joint European Confer- ence on Machine Learning and Knowledge Discovery in Databases, pages 387-402, 2013.\n\nAdversarial examples are not easily detected: Bypassing ten detection methods. Nicholas Carlini, David Wagner, arXiv:1705.07263arXiv preprintNicholas Carlini and David Wagner. Adversarial ex- amples are not easily detected: Bypassing ten detec- tion methods. arXiv preprint arXiv:1705.07263, 2017.\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, IEEE Conference on Computer Vision and Pattern Recognition. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchi- cal image database. In IEEE Conference on Computer Vision and Pattern Recognition, pages 248-255, 2009.\n\nJ Ian, Goodfellow, arXiv:1412.6572Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprintIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial ex- amples. arXiv preprint arXiv:1412.6572, 2014.\n\nShixiang Gu, Luca Rigazio, arXiv:1412.5068Towards deep neural network architectures robust to adversarial examples. arXiv preprintShixiang Gu and Luca Rigazio. Towards deep neural network architectures robust to adversarial examples. arXiv preprint arXiv:1412.5068, 2014.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, IEEE conference on computer vision and pattern recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.\n\nIdentity mappings in deep residual networks. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, European Conference on Computer Vision. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision, pages 630-645, 2016.\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. arXiv preprintGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill- ing the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, International Conference on Machine Learning. Sergey Ioffe and Christian Szegedy. Batch normaliza- tion: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pages 448-456, 2015.\n\nAdam: A method for stochastic optimization. Diederik Kingma, Jimmy Ba, arXiv:1412.6980arXiv preprintDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nAdversarial machine learning at scale. Alexey Kurakin, Ian Goodfellow, Samy Bengio, arXiv:1611.01236arXiv preprintAlexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236, 2016.\n\nGradient-based learning applied to document recognition. Yann Lecun, L\u00e9on Bottou, Yoshua Bengio, Patrick Haffner, Proceedings of the IEEE. 8611Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.\n\nBiologically inspired protection of deep networks from adversarial attacks. Aran Nayebi, Surya Ganguli, arXiv:1703.09202arXiv preprintAran Nayebi and Surya Ganguli. Biologically inspired protection of deep networks from adversarial attacks. arXiv preprint arXiv:1703.09202, 2017.\n\nNicolas Papernot, Patrick Mcdaniel, Ian Goodfellow, arXiv:1605.07277Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. arXiv preprintNicolas Papernot, Patrick McDaniel, and Ian Good- fellow. Transferability in machine learning: from phe- nomena to black-box attacks using adversarial sam- ples. arXiv preprint arXiv:1605.07277, 2016.\n\nPractical black-box attacks against machine learning. Nicolas Papernot, Patrick Mcdaniel, Ian Goodfellow, Somesh Jha, Ananthram Berkay Celik, Swami, Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning.\n\n. ACM Asia Conference on Computer and Communications Security. In ACM Asia Conference on Computer and Communi- cations Security, pages 506-519, 2017.\n\nTowards the science of security and privacy in machine learning. Nicolas Papernot, Patrick Mcdaniel, Arunesh Sinha, Michael Wellman, arXiv:1611.03814arXiv preprintNicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman. Towards the science of secu- rity and privacy in machine learning. arXiv preprint arXiv:1611.03814, 2016.\n\nDistillation as a defense to adversarial perturbations against deep neural networks. Nicolas Papernot, Patrick Mcdaniel, Xi Wu, Somesh Jha, Ananthram Swami, IEEE Symposium on Security and Privacy (SP). Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural net- works. In IEEE Symposium on Security and Privacy (SP), pages 582-597, 2016.\n\nU-net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, International Conference on Medical Image Computing and Computer-Assisted Intervention. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Med- ical Image Computing and Computer-Assisted Inter- vention, pages 234-241, 2015.\n\nInception-v4, inceptionresnet and the impact of residual connections on learning. Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alexander A Alemi, AAAI. Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception- resnet and the impact of residual connections on learn- ing. In AAAI, pages 4278-4284, 2017.\n\nRethinking the inception architecture for computer vision. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna, IEEE Conference on Computer Vision and Pattern Recognition. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the in- ception architecture for computer vision. In IEEE Conference on Computer Vision and Pattern Recog- nition, pages 2818-2826, 2016.\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, arXiv:1312.6199Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprintChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\n\nFlorian Tram\u00e8r, Alexey Kurakin, Nicolas Papernot, Dan Boneh, Patrick Mcdaniel, arXiv:1705.07204Ensemble adversarial training: Attacks and defenses. arXiv preprintFlorian Tram\u00e8r, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and Patrick McDaniel. Ensemble adver- sarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204, 2017.\n\nExtracting and composing robust features with denoising autoencoders. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, Pierre-Antoine Manzagol, International Conference on Machine learning. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In In- ternational Conference on Machine learning, pages 1096-1103, 2008.\n\nAggregated residual transformations for deep neural networks. Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He, arXiv:1611.05431arXiv preprintSaining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transfor- mations for deep neural networks. arXiv preprint arXiv:1611.05431, 2016.\n\nBeyond a gaussian denoiser: Residual learning of deep cnn for image denoising. Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang, IEEE Transactions on Image Processing. Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE Transactions on Image Processing, 2017.\n", "annotations": {"author": "[{\"end\":348,\"start\":87},{\"end\":584,\"start\":349},{\"end\":822,\"start\":585},{\"end\":1059,\"start\":823},{\"end\":1292,\"start\":1060},{\"end\":1554,\"start\":1293}]", "publisher": null, "author_last_name": "[{\"end\":100,\"start\":96},{\"end\":359,\"start\":354},{\"end\":597,\"start\":593},{\"end\":834,\"start\":830},{\"end\":1067,\"start\":1064},{\"end\":1303,\"start\":1301}]", "author_first_name": "[{\"end\":95,\"start\":87},{\"end\":353,\"start\":349},{\"end\":592,\"start\":585},{\"end\":829,\"start\":823},{\"end\":1063,\"start\":1060},{\"end\":1300,\"start\":1293}]", "author_affiliation": "[{\"end\":347,\"start\":125},{\"end\":583,\"start\":361},{\"end\":821,\"start\":599},{\"end\":1058,\"start\":836},{\"end\":1291,\"start\":1069},{\"end\":1553,\"start\":1331}]", "title": "[{\"end\":84,\"start\":1},{\"end\":1638,\"start\":1555}]", "venue": null, "abstract": "[{\"end\":3195,\"start\":1640}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3252,\"start\":3249},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3325,\"start\":3321},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3327,\"start\":3325},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3562,\"start\":3558},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3565,\"start\":3562},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3872,\"start\":3868},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5289,\"start\":5285},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7060,\"start\":7056},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7304,\"start\":7300},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7565,\"start\":7562},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8033,\"start\":8029},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8452,\"start\":8448},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8733,\"start\":8729},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8970,\"start\":8966},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8972,\"start\":8970},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9099,\"start\":9095},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9102,\"start\":9099},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9287,\"start\":9284},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9290,\"start\":9287},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9293,\"start\":9290},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9486,\"start\":9483},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9489,\"start\":9486},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9492,\"start\":9489},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9603,\"start\":9599},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9605,\"start\":9603},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9608,\"start\":9605},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9611,\"start\":9608},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9687,\"start\":9683},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9689,\"start\":9687},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10240,\"start\":10236},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10243,\"start\":10240},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10246,\"start\":10243},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10434,\"start\":10431},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10650,\"start\":10646},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10684,\"start\":10681},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10808,\"start\":10804},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11224,\"start\":11220},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11227,\"start\":11224},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12059,\"start\":12056},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12150,\"start\":12146},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12489,\"start\":12485},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12558,\"start\":12555},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12737,\"start\":12733},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13176,\"start\":13172},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13961,\"start\":13957},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14322,\"start\":14318},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17888,\"start\":17884},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18689,\"start\":18685},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18712,\"start\":18709},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18803,\"start\":18799},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18897,\"start\":18894},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18900,\"start\":18897},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19439,\"start\":19438},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19567,\"start\":19564},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19831,\"start\":19827},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20107,\"start\":20103},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23523,\"start\":23519},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24030,\"start\":24026},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24194,\"start\":24191},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26939,\"start\":26935},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26957,\"start\":26953},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26972,\"start\":26969},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26989,\"start\":26985},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28149,\"start\":28146},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":28746,\"start\":28742},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":32965,\"start\":32961},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":33028,\"start\":33024},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":33102,\"start\":33098},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":33105,\"start\":33102},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":33264,\"start\":33260},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":33366,\"start\":33362},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":33408,\"start\":33404},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":33556,\"start\":33553}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34943,\"start\":34889},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35174,\"start\":34944},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35579,\"start\":35175},{\"attributes\":{\"id\":\"fig_3\"},\"end\":35820,\"start\":35580},{\"attributes\":{\"id\":\"fig_4\"},\"end\":36169,\"start\":35821},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36591,\"start\":36170},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":37129,\"start\":36592},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37539,\"start\":37130},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":37807,\"start\":37540},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":38139,\"start\":37808},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":38360,\"start\":38140},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":38782,\"start\":38361},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":39109,\"start\":38783},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":39318,\"start\":39110}]", "paragraph": "[{\"end\":4096,\"start\":3211},{\"end\":4464,\"start\":4098},{\"end\":5239,\"start\":4466},{\"end\":5668,\"start\":5241},{\"end\":6064,\"start\":5670},{\"end\":6226,\"start\":6066},{\"end\":6990,\"start\":6258},{\"end\":7283,\"start\":7035},{\"end\":7542,\"start\":7285},{\"end\":7775,\"start\":7544},{\"end\":8300,\"start\":7810},{\"end\":9229,\"start\":8342},{\"end\":9612,\"start\":9263},{\"end\":10145,\"start\":9614},{\"end\":10957,\"start\":10147},{\"end\":11349,\"start\":10959},{\"end\":11889,\"start\":11385},{\"end\":12423,\"start\":11891},{\"end\":13244,\"start\":12457},{\"end\":14066,\"start\":13246},{\"end\":14801,\"start\":14088},{\"end\":15541,\"start\":14803},{\"end\":16054,\"start\":15603},{\"end\":16325,\"start\":16056},{\"end\":17498,\"start\":16354},{\"end\":17797,\"start\":17500},{\"end\":18197,\"start\":17823},{\"end\":18998,\"start\":18209},{\"end\":19217,\"start\":19000},{\"end\":20037,\"start\":19219},{\"end\":20430,\"start\":20064},{\"end\":22388,\"start\":20481},{\"end\":22695,\"start\":22470},{\"end\":23150,\"start\":22697},{\"end\":23524,\"start\":23152},{\"end\":23792,\"start\":23526},{\"end\":24082,\"start\":23819},{\"end\":24893,\"start\":24084},{\"end\":25276,\"start\":24937},{\"end\":26264,\"start\":25278},{\"end\":26697,\"start\":26297},{\"end\":27147,\"start\":26699},{\"end\":27320,\"start\":27149},{\"end\":27535,\"start\":27335},{\"end\":27826,\"start\":27537},{\"end\":28326,\"start\":27828},{\"end\":28405,\"start\":28354},{\"end\":28958,\"start\":28407},{\"end\":29018,\"start\":28960},{\"end\":29324,\"start\":29020},{\"end\":30062,\"start\":29363},{\"end\":30261,\"start\":30103},{\"end\":30523,\"start\":30263},{\"end\":30728,\"start\":30525},{\"end\":30786,\"start\":30730},{\"end\":30956,\"start\":30788},{\"end\":31116,\"start\":30958},{\"end\":31158,\"start\":31118},{\"end\":31310,\"start\":31160},{\"end\":31387,\"start\":31312},{\"end\":31390,\"start\":31389},{\"end\":31532,\"start\":31392},{\"end\":31707,\"start\":31553},{\"end\":31841,\"start\":31709},{\"end\":31896,\"start\":31843},{\"end\":31952,\"start\":31898},{\"end\":31968,\"start\":31954},{\"end\":31997,\"start\":31970},{\"end\":32280,\"start\":31999},{\"end\":32526,\"start\":32321},{\"end\":32582,\"start\":32528},{\"end\":32700,\"start\":32584},{\"end\":32884,\"start\":32702},{\"end\":32966,\"start\":32886},{\"end\":33106,\"start\":32968},{\"end\":33409,\"start\":33108},{\"end\":33432,\"start\":33411},{\"end\":33455,\"start\":33434},{\"end\":33557,\"start\":33457},{\"end\":33580,\"start\":33559},{\"end\":33700,\"start\":33582},{\"end\":34239,\"start\":33702},{\"end\":34319,\"start\":34241},{\"end\":34500,\"start\":34321},{\"end\":34661,\"start\":34502},{\"end\":34888,\"start\":34663}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7809,\"start\":7776},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8341,\"start\":8301},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12438,\"start\":12424},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15558,\"start\":15542},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16353,\"start\":16326},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22441,\"start\":22389}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":18913,\"start\":18906},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20023,\"start\":20016},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20547,\"start\":20540},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":22790,\"start\":22782},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":24375,\"start\":24365},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24891,\"start\":24883},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27318,\"start\":27310},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":29270,\"start\":29262},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":29321,\"start\":29314},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":29751,\"start\":29744},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":32048,\"start\":32041}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":3209,\"start\":3197},{\"attributes\":{\"n\":\"2.\"},\"end\":6256,\"start\":6229},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7033,\"start\":6993},{\"attributes\":{\"n\":\"2.2.\"},\"end\":9261,\"start\":9232},{\"attributes\":{\"n\":\"3.\"},\"end\":11359,\"start\":11352},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11383,\"start\":11362},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":12455,\"start\":12440},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":14086,\"start\":14069},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15601,\"start\":15560},{\"attributes\":{\"n\":\"4.\"},\"end\":17821,\"start\":17800},{\"attributes\":{\"n\":\"4.1.\"},\"end\":18207,\"start\":18200},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20062,\"start\":20040},{\"attributes\":{\"n\":\"5.\"},\"end\":20440,\"start\":20433},{\"attributes\":{\"n\":\"5.1.\"},\"end\":20479,\"start\":20443},{\"attributes\":{\"n\":\"5.2.\"},\"end\":22468,\"start\":22443},{\"attributes\":{\"n\":\"5.3.\"},\"end\":23817,\"start\":23795},{\"attributes\":{\"n\":\"5.4.\"},\"end\":24935,\"start\":24896},{\"attributes\":{\"n\":\"5.5.\"},\"end\":26295,\"start\":26267},{\"attributes\":{\"n\":\"6.\"},\"end\":27333,\"start\":27323},{\"end\":28352,\"start\":28329},{\"end\":29361,\"start\":29327},{\"end\":30101,\"start\":30065},{\"end\":31551,\"start\":31535},{\"end\":32319,\"start\":32283},{\"end\":34900,\"start\":34890},{\"end\":34955,\"start\":34945},{\"end\":35186,\"start\":35176},{\"end\":35591,\"start\":35581},{\"end\":35832,\"start\":35822},{\"end\":36180,\"start\":36171},{\"end\":36602,\"start\":36593},{\"end\":37140,\"start\":37131},{\"end\":37550,\"start\":37541},{\"end\":37818,\"start\":37809},{\"end\":38150,\"start\":38141},{\"end\":38371,\"start\":38362},{\"end\":38793,\"start\":38784},{\"end\":39120,\"start\":39111}]", "table": "[{\"end\":36591,\"start\":36256},{\"end\":37129,\"start\":36825},{\"end\":37539,\"start\":37235},{\"end\":37807,\"start\":37667},{\"end\":38139,\"start\":37916},{\"end\":38360,\"start\":38225},{\"end\":38782,\"start\":38544},{\"end\":39109,\"start\":38888},{\"end\":39318,\"start\":39196}]", "figure_caption": "[{\"end\":34943,\"start\":34902},{\"end\":35174,\"start\":34957},{\"end\":35579,\"start\":35188},{\"end\":35820,\"start\":35593},{\"end\":36169,\"start\":35834},{\"end\":36256,\"start\":36182},{\"end\":36825,\"start\":36604},{\"end\":37235,\"start\":37142},{\"end\":37667,\"start\":37552},{\"end\":37916,\"start\":37820},{\"end\":38225,\"start\":38152},{\"end\":38544,\"start\":38373},{\"end\":38888,\"start\":38795},{\"end\":39196,\"start\":39122}]", "figure_ref": "[{\"end\":4285,\"start\":4277},{\"end\":5135,\"start\":5127},{\"end\":5299,\"start\":5291},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13438,\"start\":13430},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13872,\"start\":13864},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14156,\"start\":14148},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14799,\"start\":14790},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15466,\"start\":15458},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17024,\"start\":17015},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17306,\"start\":17297},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18078,\"start\":18070},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21634,\"start\":21626},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22570,\"start\":22562},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25089,\"start\":25081},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":25380,\"start\":25372},{\"end\":25726,\"start\":25717},{\"end\":25919,\"start\":25911},{\"end\":27481,\"start\":27473},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29679,\"start\":29664}]", "bib_author_first_name": "[{\"end\":39512,\"start\":39505},{\"end\":39524,\"start\":39521},{\"end\":39781,\"start\":39773},{\"end\":39795,\"start\":39790},{\"end\":39810,\"start\":39804},{\"end\":39826,\"start\":39820},{\"end\":39840,\"start\":39835},{\"end\":39861,\"start\":39854},{\"end\":39875,\"start\":39870},{\"end\":40354,\"start\":40346},{\"end\":40369,\"start\":40364},{\"end\":40622,\"start\":40619},{\"end\":40632,\"start\":40629},{\"end\":40646,\"start\":40639},{\"end\":40661,\"start\":40655},{\"end\":40669,\"start\":40666},{\"end\":40676,\"start\":40674},{\"end\":40958,\"start\":40957},{\"end\":41248,\"start\":41240},{\"end\":41257,\"start\":41253},{\"end\":41566,\"start\":41559},{\"end\":41578,\"start\":41571},{\"end\":41594,\"start\":41586},{\"end\":41604,\"start\":41600},{\"end\":41908,\"start\":41901},{\"end\":41920,\"start\":41913},{\"end\":41936,\"start\":41928},{\"end\":41946,\"start\":41942},{\"end\":42165,\"start\":42157},{\"end\":42179,\"start\":42174},{\"end\":42193,\"start\":42189},{\"end\":42511,\"start\":42505},{\"end\":42528,\"start\":42519},{\"end\":42839,\"start\":42831},{\"end\":42853,\"start\":42848},{\"end\":43045,\"start\":43039},{\"end\":43058,\"start\":43055},{\"end\":43075,\"start\":43071},{\"end\":43303,\"start\":43299},{\"end\":43315,\"start\":43311},{\"end\":43330,\"start\":43324},{\"end\":43346,\"start\":43339},{\"end\":43633,\"start\":43629},{\"end\":43647,\"start\":43642},{\"end\":43841,\"start\":43834},{\"end\":43859,\"start\":43852},{\"end\":43873,\"start\":43870},{\"end\":44279,\"start\":44272},{\"end\":44297,\"start\":44290},{\"end\":44311,\"start\":44308},{\"end\":44330,\"start\":44324},{\"end\":44345,\"start\":44336},{\"end\":44746,\"start\":44739},{\"end\":44764,\"start\":44757},{\"end\":44782,\"start\":44775},{\"end\":44797,\"start\":44790},{\"end\":45108,\"start\":45101},{\"end\":45126,\"start\":45119},{\"end\":45139,\"start\":45137},{\"end\":45150,\"start\":45144},{\"end\":45165,\"start\":45156},{\"end\":45520,\"start\":45516},{\"end\":45541,\"start\":45534},{\"end\":45557,\"start\":45551},{\"end\":45977,\"start\":45968},{\"end\":45993,\"start\":45987},{\"end\":46008,\"start\":46001},{\"end\":46029,\"start\":46020},{\"end\":46031,\"start\":46030},{\"end\":46307,\"start\":46298},{\"end\":46324,\"start\":46317},{\"end\":46342,\"start\":46336},{\"end\":46353,\"start\":46350},{\"end\":46370,\"start\":46362},{\"end\":46681,\"start\":46672},{\"end\":46699,\"start\":46691},{\"end\":46713,\"start\":46709},{\"end\":46729,\"start\":46725},{\"end\":47055,\"start\":47048},{\"end\":47070,\"start\":47064},{\"end\":47087,\"start\":47080},{\"end\":47101,\"start\":47098},{\"end\":47116,\"start\":47109},{\"end\":47464,\"start\":47458},{\"end\":47478,\"start\":47474},{\"end\":47497,\"start\":47491},{\"end\":47520,\"start\":47506},{\"end\":47868,\"start\":47861},{\"end\":47878,\"start\":47874},{\"end\":47894,\"start\":47889},{\"end\":47910,\"start\":47903},{\"end\":47922,\"start\":47915},{\"end\":48213,\"start\":48210},{\"end\":48229,\"start\":48221},{\"end\":48241,\"start\":48235},{\"end\":48252,\"start\":48248},{\"end\":48262,\"start\":48259}]", "bib_author_last_name": "[{\"end\":39519,\"start\":39513},{\"end\":39532,\"start\":39525},{\"end\":39788,\"start\":39782},{\"end\":39802,\"start\":39796},{\"end\":39818,\"start\":39811},{\"end\":39833,\"start\":39827},{\"end\":39852,\"start\":39841},{\"end\":39868,\"start\":39862},{\"end\":39884,\"start\":39876},{\"end\":39890,\"start\":39886},{\"end\":40362,\"start\":40355},{\"end\":40376,\"start\":40370},{\"end\":40627,\"start\":40623},{\"end\":40637,\"start\":40633},{\"end\":40653,\"start\":40647},{\"end\":40664,\"start\":40662},{\"end\":40672,\"start\":40670},{\"end\":40684,\"start\":40677},{\"end\":40962,\"start\":40959},{\"end\":40974,\"start\":40964},{\"end\":41251,\"start\":41249},{\"end\":41265,\"start\":41258},{\"end\":41569,\"start\":41567},{\"end\":41584,\"start\":41579},{\"end\":41598,\"start\":41595},{\"end\":41608,\"start\":41605},{\"end\":41911,\"start\":41909},{\"end\":41926,\"start\":41921},{\"end\":41940,\"start\":41937},{\"end\":41950,\"start\":41947},{\"end\":42172,\"start\":42166},{\"end\":42187,\"start\":42180},{\"end\":42198,\"start\":42194},{\"end\":42517,\"start\":42512},{\"end\":42536,\"start\":42529},{\"end\":42846,\"start\":42840},{\"end\":42856,\"start\":42854},{\"end\":43053,\"start\":43046},{\"end\":43069,\"start\":43059},{\"end\":43082,\"start\":43076},{\"end\":43309,\"start\":43304},{\"end\":43322,\"start\":43316},{\"end\":43337,\"start\":43331},{\"end\":43354,\"start\":43347},{\"end\":43640,\"start\":43634},{\"end\":43655,\"start\":43648},{\"end\":43850,\"start\":43842},{\"end\":43868,\"start\":43860},{\"end\":43884,\"start\":43874},{\"end\":44288,\"start\":44280},{\"end\":44306,\"start\":44298},{\"end\":44322,\"start\":44312},{\"end\":44334,\"start\":44331},{\"end\":44358,\"start\":44346},{\"end\":44365,\"start\":44360},{\"end\":44755,\"start\":44747},{\"end\":44773,\"start\":44765},{\"end\":44788,\"start\":44783},{\"end\":44805,\"start\":44798},{\"end\":45117,\"start\":45109},{\"end\":45135,\"start\":45127},{\"end\":45142,\"start\":45140},{\"end\":45154,\"start\":45151},{\"end\":45171,\"start\":45166},{\"end\":45532,\"start\":45521},{\"end\":45549,\"start\":45542},{\"end\":45562,\"start\":45558},{\"end\":45985,\"start\":45978},{\"end\":45999,\"start\":45994},{\"end\":46018,\"start\":46009},{\"end\":46037,\"start\":46032},{\"end\":46315,\"start\":46308},{\"end\":46334,\"start\":46325},{\"end\":46348,\"start\":46343},{\"end\":46360,\"start\":46354},{\"end\":46376,\"start\":46371},{\"end\":46689,\"start\":46682},{\"end\":46707,\"start\":46700},{\"end\":46723,\"start\":46714},{\"end\":46735,\"start\":46730},{\"end\":47062,\"start\":47056},{\"end\":47078,\"start\":47071},{\"end\":47096,\"start\":47088},{\"end\":47107,\"start\":47102},{\"end\":47125,\"start\":47117},{\"end\":47472,\"start\":47465},{\"end\":47489,\"start\":47479},{\"end\":47504,\"start\":47498},{\"end\":47529,\"start\":47521},{\"end\":47872,\"start\":47869},{\"end\":47887,\"start\":47879},{\"end\":47901,\"start\":47895},{\"end\":47913,\"start\":47911},{\"end\":47925,\"start\":47923},{\"end\":48219,\"start\":48214},{\"end\":48233,\"start\":48230},{\"end\":48246,\"start\":48242},{\"end\":48257,\"start\":48253},{\"end\":48268,\"start\":48263}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1703.09387\",\"id\":\"b0\"},\"end\":39716,\"start\":39425},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":18716873},\"end\":40265,\"start\":39718},{\"attributes\":{\"doi\":\"arXiv:1705.07263\",\"id\":\"b2\"},\"end\":40564,\"start\":40267},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":57246310},\"end\":40955,\"start\":40566},{\"attributes\":{\"doi\":\"arXiv:1412.6572\",\"id\":\"b4\"},\"end\":41238,\"start\":40957},{\"attributes\":{\"doi\":\"arXiv:1412.5068\",\"id\":\"b5\"},\"end\":41511,\"start\":41240},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206594692},\"end\":41854,\"start\":41513},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6447277},\"end\":42155,\"start\":41856},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b8\"},\"end\":42409,\"start\":42157},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5808102},\"end\":42785,\"start\":42411},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b10\"},\"end\":42998,\"start\":42787},{\"attributes\":{\"doi\":\"arXiv:1611.01236\",\"id\":\"b11\"},\"end\":43240,\"start\":43000},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":14542261},\"end\":43551,\"start\":43242},{\"attributes\":{\"doi\":\"arXiv:1703.09202\",\"id\":\"b13\"},\"end\":43832,\"start\":43553},{\"attributes\":{\"doi\":\"arXiv:1605.07277\",\"id\":\"b14\"},\"end\":44216,\"start\":43834},{\"attributes\":{\"id\":\"b15\"},\"end\":44521,\"start\":44218},{\"attributes\":{\"id\":\"b16\"},\"end\":44672,\"start\":44523},{\"attributes\":{\"doi\":\"arXiv:1611.03814\",\"id\":\"b17\"},\"end\":45014,\"start\":44674},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":2672720},\"end\":45449,\"start\":45016},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3719281},\"end\":45884,\"start\":45451},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1023605},\"end\":46237,\"start\":45886},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":206593880},\"end\":46670,\"start\":46239},{\"attributes\":{\"doi\":\"arXiv:1312.6199\",\"id\":\"b22\"},\"end\":47046,\"start\":46672},{\"attributes\":{\"doi\":\"arXiv:1705.07204\",\"id\":\"b23\"},\"end\":47386,\"start\":47048},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":207168299},\"end\":47797,\"start\":47388},{\"attributes\":{\"doi\":\"arXiv:1611.05431\",\"id\":\"b25\"},\"end\":48129,\"start\":47799},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":996788},\"end\":48496,\"start\":48131}]", "bib_title": "[{\"end\":39771,\"start\":39718},{\"end\":40617,\"start\":40566},{\"end\":41557,\"start\":41513},{\"end\":41899,\"start\":41856},{\"end\":42503,\"start\":42411},{\"end\":43297,\"start\":43242},{\"end\":45099,\"start\":45016},{\"end\":45514,\"start\":45451},{\"end\":45966,\"start\":45886},{\"end\":46296,\"start\":46239},{\"end\":47456,\"start\":47388},{\"end\":48208,\"start\":48131}]", "bib_author": "[{\"end\":39521,\"start\":39505},{\"end\":39534,\"start\":39521},{\"end\":39790,\"start\":39773},{\"end\":39804,\"start\":39790},{\"end\":39820,\"start\":39804},{\"end\":39835,\"start\":39820},{\"end\":39854,\"start\":39835},{\"end\":39870,\"start\":39854},{\"end\":39886,\"start\":39870},{\"end\":39892,\"start\":39886},{\"end\":40364,\"start\":40346},{\"end\":40378,\"start\":40364},{\"end\":40629,\"start\":40619},{\"end\":40639,\"start\":40629},{\"end\":40655,\"start\":40639},{\"end\":40666,\"start\":40655},{\"end\":40674,\"start\":40666},{\"end\":40686,\"start\":40674},{\"end\":40964,\"start\":40957},{\"end\":40976,\"start\":40964},{\"end\":41253,\"start\":41240},{\"end\":41267,\"start\":41253},{\"end\":41571,\"start\":41559},{\"end\":41586,\"start\":41571},{\"end\":41600,\"start\":41586},{\"end\":41610,\"start\":41600},{\"end\":41913,\"start\":41901},{\"end\":41928,\"start\":41913},{\"end\":41942,\"start\":41928},{\"end\":41952,\"start\":41942},{\"end\":42174,\"start\":42157},{\"end\":42189,\"start\":42174},{\"end\":42200,\"start\":42189},{\"end\":42519,\"start\":42505},{\"end\":42538,\"start\":42519},{\"end\":42848,\"start\":42831},{\"end\":42858,\"start\":42848},{\"end\":43055,\"start\":43039},{\"end\":43071,\"start\":43055},{\"end\":43084,\"start\":43071},{\"end\":43311,\"start\":43299},{\"end\":43324,\"start\":43311},{\"end\":43339,\"start\":43324},{\"end\":43356,\"start\":43339},{\"end\":43642,\"start\":43629},{\"end\":43657,\"start\":43642},{\"end\":43852,\"start\":43834},{\"end\":43870,\"start\":43852},{\"end\":43886,\"start\":43870},{\"end\":44290,\"start\":44272},{\"end\":44308,\"start\":44290},{\"end\":44324,\"start\":44308},{\"end\":44336,\"start\":44324},{\"end\":44360,\"start\":44336},{\"end\":44367,\"start\":44360},{\"end\":44757,\"start\":44739},{\"end\":44775,\"start\":44757},{\"end\":44790,\"start\":44775},{\"end\":44807,\"start\":44790},{\"end\":45119,\"start\":45101},{\"end\":45137,\"start\":45119},{\"end\":45144,\"start\":45137},{\"end\":45156,\"start\":45144},{\"end\":45173,\"start\":45156},{\"end\":45534,\"start\":45516},{\"end\":45551,\"start\":45534},{\"end\":45564,\"start\":45551},{\"end\":45987,\"start\":45968},{\"end\":46001,\"start\":45987},{\"end\":46020,\"start\":46001},{\"end\":46039,\"start\":46020},{\"end\":46317,\"start\":46298},{\"end\":46336,\"start\":46317},{\"end\":46350,\"start\":46336},{\"end\":46362,\"start\":46350},{\"end\":46378,\"start\":46362},{\"end\":46691,\"start\":46672},{\"end\":46709,\"start\":46691},{\"end\":46725,\"start\":46709},{\"end\":46737,\"start\":46725},{\"end\":47064,\"start\":47048},{\"end\":47080,\"start\":47064},{\"end\":47098,\"start\":47080},{\"end\":47109,\"start\":47098},{\"end\":47127,\"start\":47109},{\"end\":47474,\"start\":47458},{\"end\":47491,\"start\":47474},{\"end\":47506,\"start\":47491},{\"end\":47531,\"start\":47506},{\"end\":47874,\"start\":47861},{\"end\":47889,\"start\":47874},{\"end\":47903,\"start\":47889},{\"end\":47915,\"start\":47903},{\"end\":47927,\"start\":47915},{\"end\":48221,\"start\":48210},{\"end\":48235,\"start\":48221},{\"end\":48248,\"start\":48235},{\"end\":48259,\"start\":48248},{\"end\":48270,\"start\":48259}]", "bib_venue": "[{\"end\":39503,\"start\":39425},{\"end\":39974,\"start\":39892},{\"end\":40344,\"start\":40267},{\"end\":40744,\"start\":40686},{\"end\":41077,\"start\":40991},{\"end\":41354,\"start\":41282},{\"end\":41668,\"start\":41610},{\"end\":41990,\"start\":41952},{\"end\":42260,\"start\":42216},{\"end\":42582,\"start\":42538},{\"end\":42829,\"start\":42787},{\"end\":43037,\"start\":43000},{\"end\":43379,\"start\":43356},{\"end\":43627,\"start\":43553},{\"end\":44000,\"start\":43902},{\"end\":44270,\"start\":44218},{\"end\":44584,\"start\":44525},{\"end\":44737,\"start\":44674},{\"end\":45216,\"start\":45173},{\"end\":45650,\"start\":45564},{\"end\":46043,\"start\":46039},{\"end\":46436,\"start\":46378},{\"end\":46839,\"start\":46752},{\"end\":47194,\"start\":47143},{\"end\":47575,\"start\":47531},{\"end\":47859,\"start\":47799},{\"end\":48307,\"start\":48270}]"}}}, "year": 2023, "month": 12, "day": 17}