{"id": 252898601, "updated": "2022-10-18 18:22:25.573", "metadata": {"title": "RF-URL: unsupervised representation learning for RF sensing", "authors": "[{\"first\":\"Ruiyuan\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Dongheng\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Zhi\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Cong\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Chunyang\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Shuai\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Yang\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Yan\",\"last\":\"Chen\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 28th Annual International Conference on Mobile Computing And Networking", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "The major obstacle for learning-based RF sensing is to obtain a high-quality large-scale annotated dataset. However, unlike visual datasets that can be easily annotated by human workers, RF signal is non-intuitive and non-interpretable, which causes the annotation of RF signals time-consuming and laborious. To resolve the rapacious appetite of annotated data, we propose a novel unsupervised representation learning (URL) framework for RF sensing, RF-URL, to learn a pre-training model on large-scale unannotated RF datasets that can be easily collected. RF-URL utilizes a contrastive framework to mind the gap between signal-processing-based RF sensing and learning-based RF sensing. By constructing positive and negative pairs through different signal processing representations, RF-URL seamlessly integrates the existing RF signal processing algorithms into the learning-based networks. Moreover, the RF-URL is carefully designed to take into account the asymmetric characteristics of different RF signal processing representations. We show that RF-URL is universal to a variety of RF sensing tasks by evaluating RF-URL in three typical RF sensing tasks (human gesture recognition, 3D pose estimation and silhouette generation) based on two general RF devices (WiFi and radar). All experimental results strongly demonstrate that RF-URL takes an important step towards learning-based solutions for large-scale RF sensing applications.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/mobicom/SongZW0XY0022", "doi": "10.1145/3495243.3560529"}}, "content": {"source": {"pdf_hash": "a35f245bb7a2c02a2b3a7c54fe239cd3eb8d9055", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "dde6848e0ff5848a95bcc7d707c8c050c9332894", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a35f245bb7a2c02a2b3a7c54fe239cd3eb8d9055.txt", "contents": "\nUnsupervised Representation Learning for RF Sensing\nACMCopyright ACMOctober 17-21, 2022. October 17-21, 2022\n\nRuiyuan Song \nSchool of Cyber Science and Technology\nResearch Center from Data to Cyberspace\nCommunication and Detection\nMinistry of Culture and Tourism\nACM Reference Format\nUniversity of Science and Technology of China\nUniversity of Science and Technology of China Key Lab of Cyberspace Cultural Content Cognition\n\n\nDongheng Zhang \nSchool of Cyber Science and Technology\nResearch Center from Data to Cyberspace\nCommunication and Detection\nMinistry of Culture and Tourism\nACM Reference Format\nUniversity of Science and Technology of China\nUniversity of Science and Technology of China Key Lab of Cyberspace Cultural Content Cognition\n\n\nZhi Wu \nSchool of Cyber Science and Technology\nResearch Center from Data to Cyberspace\nCommunication and Detection\nMinistry of Culture and Tourism\nACM Reference Format\nUniversity of Science and Technology of China\nUniversity of Science and Technology of China Key Lab of Cyberspace Cultural Content Cognition\n\n\nCong Yu \nSchool of Cyber Science and Technology\nResearch Center from Data to Cyberspace\nCommunication and Detection\nMinistry of Culture and Tourism\nACM Reference Format\nUniversity of Science and Technology of China\nUniversity of Science and Technology of China Key Lab of Cyberspace Cultural Content Cognition\n\n\nChunyang Xie \nSchool of Cyber Science and Technology\nResearch Center from Data to Cyberspace\nCommunication and Detection\nMinistry of Culture and Tourism\nACM Reference Format\nUniversity of Science and Technology of China\nUniversity of Science and Technology of China Key Lab of Cyberspace Cultural Content Cognition\n\n\nShuai Yang \nSchool of Cyber Science and Technology\nResearch Center from Data to Cyberspace\nCommunication and Detection\nMinistry of Culture and Tourism\nACM Reference Format\nUniversity of Science and Technology of China\nUniversity of Science and Technology of China Key Lab of Cyberspace Cultural Content Cognition\n\n\nYang Hu \nSchool of Cyber Science and Technology\nResearch Center from Data to Cyberspace\nCommunication and Detection\nMinistry of Culture and Tourism\nACM Reference Format\nUniversity of Science and Technology of China\nUniversity of Science and Technology of China Key Lab of Cyberspace Cultural Content Cognition\n\n\nYan Chen \nSchool of Cyber Science and Technology\nResearch Center from Data to Cyberspace\nCommunication and Detection\nMinistry of Culture and Tourism\nACM Reference Format\nUniversity of Science and Technology of China\nUniversity of Science and Technology of China Key Lab of Cyberspace Cultural Content Cognition\n\n\nRuiyuan Song \nSchool of Cyber Science and Technology\nResearch Center from Data to Cyberspace\nCommunication and Detection\nMinistry of Culture and Tourism\nACM Reference Format\nUniversity of Science and Technology of China\nUniversity of Science and Technology of China Key Lab of Cyberspace Cultural Content Cognition\n\n\nDongheng Zhang \nSchool of Cyber Science and Technology\nResearch Center from Data to Cyberspace\nCommunication and Detection\nMinistry of Culture and Tourism\nACM Reference Format\nUniversity of Science and Technology of China\nUniversity of Science and Technology of China Key Lab of Cyberspace Cultural Content Cognition\n\n\nZhi Wu \nSchool of Cyber Science and Technology\nResearch Center from Data to Cyberspace\nCommunication and Detection\nMinistry of Culture and Tourism\nACM Reference Format\nUniversity of Science and Technology of China\nUniversity of Science and Technology of China Key Lab of Cyberspace Cultural Content Cognition\n\n\nCong Yu \nSchool of Cyber Science and Technology\nResearch Center from Data to Cyberspace\nCommunication and Detection\nMinistry of Culture and Tourism\nACM Reference Format\nUniversity of Science and Technology of China\nUniversity of Science and Technology of China Key Lab of Cyberspace Cultural Content Cognition\n\n\nChunyang Xie \nSchool of Cyber Science and Technology\nResearch Center from Data to Cyberspace\nCommunication and Detection\nMinistry of Culture and Tourism\nACM Reference Format\nUniversity of Science and Technology of China\nUniversity of Science and Technology of China Key Lab of Cyberspace Cultural Content Cognition\n\n\nShuai Yang \nSchool of Cyber Science and Technology\nResearch Center from Data to Cyberspace\nCommunication and Detection\nMinistry of Culture and Tourism\nACM Reference Format\nUniversity of Science and Technology of China\nUniversity of Science and Technology of China Key Lab of Cyberspace Cultural Content Cognition\n\n\nYang Hu \nSchool of Cyber Science and Technology\nResearch Center from Data to Cyberspace\nCommunication and Detection\nMinistry of Culture and Tourism\nACM Reference Format\nUniversity of Science and Technology of China\nUniversity of Science and Technology of China Key Lab of Cyberspace Cultural Content Cognition\n\n\nYan Chen \nSchool of Cyber Science and Technology\nResearch Center from Data to Cyberspace\nCommunication and Detection\nMinistry of Culture and Tourism\nACM Reference Format\nUniversity of Science and Technology of China\nUniversity of Science and Technology of China Key Lab of Cyberspace Cultural Content Cognition\n\n\nYan Chen \nSchool of Cyber Science and Technology\nResearch Center from Data to Cyberspace\nCommunication and Detection\nMinistry of Culture and Tourism\nACM Reference Format\nUniversity of Science and Technology of China\nUniversity of Science and Technology of China Key Lab of Cyberspace Cultural Content Cognition\n\n\nUnsupervised Representation Learning for RF Sensing\n\nThe 28th Annual International Conference On Mobile Computing And Networking (ACM MobiCom '22)\nSydney, NSW, Australia; New York, NY, USA; Sydney, NSW, AustraliaACM14October 17-21, 2022. October 17-21, 202210.1145/3495243.3560529* Corresponding author:RF sensingunsupervised representation learningcontrastive learn- ingpre-training model\nThe major obstacle for learning-based RF sensing is to obtain a high-quality large-scale annotated dataset. However, unlike visual datasets that can be easily annotated by human workers, RF signal is non-intuitive and non-interpretable, which causes the annotation of RF signals time-consuming and laborious. To resolve the rapacious appetite of annotated data, we propose a novel unsupervised representation learning (URL) framework for RF sensing, RF-URL, to learn a pre-training model on large-scale unannotated RF datasets that can be easily collected. RF-URL utilizes a contrastive framework to mind the gap between signal-processing-based RF sensing and learning-based RF sensing. By constructing positive and negative pairs through different signal processing representations, RF-URL seamlessly integrates the existing RF signal processing algorithms into the learning-based networks. Moreover, the RF-URL is carefully designed to take into account the asymmetric characteristics of different RF signal processing representations. We show that RF-URL is universal to a variety of RF sensing tasks by evaluating RF-URL in three typical RF sensing tasks (human gesture recognition, 3D pose estimation and silhouette generation) based on two general RF devices (WiFi and radar). All experimental results strongly demonstrate that RF-URL takes an important step towards learning-based solutions for large-scale RF sensing applications.CCS CONCEPTS\u2022 Human-centered computing \u2192 Ubiquitous and mobile computing systems and tools; \u2022 Computing methodologies \u2192 Machine learning.\n\nINTRODUCTION\n\nThe past decade has witnessed much progress in RF sensing. Researchers have utilized advanced signal processing technologies to build explicit models between signal variations and human behaviors, which have enabled applications including vital sign estimation [1] and location tracking [2]. Nevertheless, when the sensing tasks become more complicated, the explicit signal processing models become intractable. Hence, learning-based RF sensing powered by deep neural networks has been an emerging field, which has enabled various applications including gesture recognition [51], human pose estimation [54] and identification [16], etc. While promising results have been achieved under specific conditions, it is still difficult to scale the data-driven system to large-scale RF sensing applications due to the dataset limitation.\n\nChallenge: Limitation on Annotated RF Datasets. The datadriven RF sensing methods are typically achieved in a supervised learning manner. However, unlike RGB data which can be annotated manually, annotating RF data is time-consuming and laborious since RF signals are not human interpretable. Moreover, the captured RF signal is highly relevant to the environment for signal propagation, which forces researcher to collect a large-scale annotated dataset with various environments. To resolve such a problem, supervised RF data augmented with other modalities including RGB-cameras [52,54] and accelerometers [27] has been proposed. However, the overhead introduced by synchronization and calibration between different modalities still limit the realworld deployments of these systems.\n\nOpportunity: Unsupervised Representation Learning. The limitation on annotated RF datasets encourages us to exploit unannotated data for model training. URL [3] has attracted much interest in computer vision and natural language processing [14,23]. Contrastive learning is one kind of URLs that have been growing rapidly recently in computer vision community, which repulses different images (negative pairs) while attracting the same image's different views (positive pairs). In such a way, the general semantic information cross different views corresponding to the same image is retained, while the rest information (noise) is thrown away, resulting in a useful representation. However, prior work has shown that contrastive learning tends to learn shortcut rather than meaningful information for RF signals [32]. To this end, we have noted that the core of contrastive framework lies in building positive and negative pairs to learn their inherent consistencies and discrepancies, while existing methods generally utilize data augmentation to construct the positive and negative pairs, which is designed for visual images but are not able to avoid shortcuts for RF signals.\n\nInsight: Minding the Gap Between Signal Processing and Neural Network. To design an effective contrastive framework for RF signals, we find that the signal processing technologies, which could achieve diverse representations with theoretic signal models, have been underexplored in the existing learning-based frameworks. For instance, given a series of RF signals, we can derive their Angle of Arrival (AoA)-Time of Flight (ToF) [48,54], Doppler-Frequency-Spectrum (DFS) [51], etc. These different signal processing representations, together with the raw signal samples, are actually corresponding to the same semantic information, which can naturally form the positive pairs for the contrastive framework. On the other hand, the signal processing representations of different RF signals also naturally form the negative pairs.\n\nIn this paper, we introduce a new URL framework, RF-URL, for RF sensing. RF-URL utilizes contrastive learning by introducing different signal processing methods to replace data augmentation which is the common practice for computer vision. In such a way, we can seamlessly integrate the well-developed RF signal processing algorithms into the learning-based RF-URL networks. However, different signal representations have different characteristics (like dimensions, feature, etc), which is referred to \"asymmetric characteristics\". For instance, DFS is a 2D tensor (Frequency-Time) while AoA-ToF is a 3D tensor (AoA-ToF-Time). Such an asymmetric characteristic would make the contrastive learning framework difficult to converge. To this end, we design a series of modules to transform knowledge between different representations as follows.\n\n\u2022 A multi-branch structure is designed to involve different signal processing representations obtained from the well-developed RF signal processing algorithms. \u2022 A translator is utilized as a mediator to embed different signal representations of RF signals into a unified metric space to avoid convergence problem, and a predictor with stop-gradient operation is proposed to improve the performance of RF-URL. \u2022 A memory bank stores all representations of the training dataset and can effectively sample a large-scale negative pairs.\n\nContribution: This paper takes an important step towards learning-based solutions for RF sensing applications by extending URL to solve the appetite for large-scale annotated RF data. The main contributions are summarized as follows:\n\n\u2022 The paper introduces a novel URL framework, RF-URL, for RF sensing. To the best of our knowledge, this is the first work to utilize a contrastive framework to mind the gap between signalprocessing-based RF sensing and learning-based RF sensing. By learning a general semantic information for various sensing tasks from different signal representations, the proposed RF-URL could enhance the sensing performance in an unsupervised manner. \u2022 The paper presents an architecture for unsupervised RF sensing that leverages a multi-branch design, translator, memory bank and predictor with stop-gradient operation to achieve balance among simplicity, scalability and performance. \u2022 We show that RF-URL is a universal framework to a variety of RF sensing tasks by evaluating it on three basic tasks with two kinds of RF signals (WiFi and radar), including (1) single label prediction task: human gesture recognition with WiFi signals; (2) structured prediction task: 3D pose estimation with millimeter wave radar signals; and (3) dense prediction: human silhouette generation with millimeter wave radar signals. Experimental results show that RF-URL pre-training model improves the performance of all three tasks: 8.98% accuracy improvement for human gesture recognition, 38.23% 2 distance reduction for 3D pose estimation, and 11.33% IoU improvement for human silhouette generation.\n\n\nMETHOD\n\nRF-URL is a URL framework for RF-based sensing based on contrastive learning. It utilizes different signal representations to construct positive and negative pairs for contrastive learning, which allows us to seamlessly integrate the well-developed RF signal processing techniques with URL framework. As shown in Figure 1, the architecture of RF-URL mainly contains four components: signal representation, feature extraction with translation, predictor and memory bank sampling. Specifically, RF signal is firstly processed by different signal processing techniques to obtain different representations ( 1 , ..., ). Then, for each , a backbone network is utilized to extract the corresponding feature . Since different representations of RF signals are with different characteristics and dimensions, e.g., DFS is a 2D tensor (Frequency-Time) and AoA-ToF is a 3D tensor (AoA-ToF-Time), a translator network is adopted to map the RF signal features into a unified metric space with = ( ). Then, ( 1 , ..., ) together with the representations sampled from memory bank are utilized to construct positive pairs (different signal representations of the same RF signal) and negative pairs (the signal representations of different RF signals) to minimize an InfoNCE loss, which is supposed to be small for positive pair and large for negative pairs. In addition, a shared-weight predictor \u210e is applied on one branch to predict the output of another branch to further improve the representation quality. We will discuss these four components in detail as follows.\n\n\nSignal Representation\n\nIn the field of RF sensing, various signal processing techniques have been developed to obtain the representations of the RF signals, channel state information (CSI), DFS, AoA-ToF, etc. However, since each of these signal representations only provides a certain perspective of the RF signals, directly utilizing these representations for learning-based RF sensing may introduce inductive bias and lead to unsatisfied solution [19]. For example, DFS mainly embodies the velocity information, due to which the inductive bias might enforce the network to over-weight the feature of DFS while ignoring other information, leading to over-fitting and low generalization performance.\n\nRF-URL aims to exploit general semantic information for various sensing tasks from different signal representations. It is achieved by the contrastive learning theory through attracting different signal representations of the same RF signals while repelling others.\n\n\nFeature Extraction and Translation\n\nBackbone Encoder: The different signal representations of RF signals have different dimensions and characteristics, e.g., DFS is a 2D tensor (Frequency-Time) and AoA-ToF is a 3D tensor (AoA-ToF-Time). Thus, a customized multi-branch backbone network is adopt to process different signal representations.\n\nTranslator: The different signal representations may cause convergence issue of the RF-URL. Thus, a small MLP neural network, named as translator, is adopted as a mediator to transform the different representations of RF signals into a unified latent space  with = ( ). As shown in Figure. 2, we illustrate the training processing of RF-URL with/without translator, from which we can see that translator plays a crucial role on enforcing convergence. Overall, backbone is adopted to extract features of different signal  representations generated by different RF signal processing algorithms. Translator is utilized as a mediator to embed different signal representations of RF signals into a unified metric space to avoid convergence problem.\n\n\nInteract with memory bank\n\n\nPredictor and Loss Function\n\nPredictor is a small shared-weight neural network \u210e with stop-grad (stop gradient) operation, which is applied on one branch to predict the output of another branch to further improve the representation quality. Stop-grad could avoid a direct interaction between two branches, which prevent training collapsing. The loss is calculated as\nL = 1 2( \u2212 1) \u22121 \u2211\ufe01 =1 D (\u210e ( ), ( +1 ) ) + D ( ( ), \u210e ( +1 ) ) ,(1)\nwhere D (.) indicates a distance metric and (.) stands for stop gradient operation.\n\nRF-URL is a contrastive-learning-based framework that learns features of RF signals from the positive and negative pairs created through different signal processing methods. A contrastive loss [22] is low when is similar to +1 for positive pairs { , +1 } and dissimilar for negative pairs { , +1 }( \u2260 ). With similarity function (u, v) = u \u22a4 v/\u2225u\u2225\u2225v\u2225, a contrastive loss InfoNCE [40] with negative pairs { , +1 } =1 is written as\nL ( , +1 ) = \u2212 log exp( ( , +1 )/ ) exp( ( , +1 )/ ) + =1 exp( ( , +1 )/ ) ,(2)\nwhere denotes temperature parameter [44]. However, L ( , +1 ) defines an asymmetric loss by fixing , i.e., L ( +1 , ) \u2260 L ( , +1 ). Similarly, we define L ( +1 , ) by fixing +1 and obtain a symmetrical contrastive loss as follows\nL = 1 2( \u2212 1) \u22121 \u2211\ufe01 =1 L ( , +1 ) + L ( +1 , ) .(3)\nThe final loss function for RF-URL is\nL \u2212 = \u2211\ufe01 L + \u2211\ufe01 L ,(4)\nwhere is the scale factor. It is noticed that both L and L only calculate the distances between consecutive processed signals ( and +1 ) rather than all pairs. Compared with directly calculating distances between all pairs, calculating distance between consecutive signals could maintain the consistency between all pairs while reducing the computational complexity.\n\n\nMemory Bank\n\nMemory bank stores all representations of the training dataset [44]. Therefore, we can effectively sample a large-scale negative samples. As shown in Figure. 1, the memory bank mainly contains two operations: sampling representations from buffers to calculate contrastive loss L and updating the representations in buffers. Sampling: We adopt a cross sampling strategy. For representation , a mini-batch of samples { +1 } =1 are randomly sampled from buffer + 1 to form the negative pairs {( , +1 )} =1 and positive pair ( , +1 ). The same operation is executed for representation +1 to obtain negative pairs {( +1 , )} =1 and positive pair ( +1 , ). Then, the contrastive loss L in Eqn.(3) can be calculated to update parameters of neural networks by back-propagation algorithm.\n\nUpdate buffer: The representations in memory bank could not be updated by back-propagation process, and a dynamically update strategy is adopted to update the parameters of memory bank with a momentum mechanism \u2190 \u00b7 + (1 \u2212 ) ,\n\nwhere \u2208 (0, 1) is a momentum coefficient, is the output of the translator, and comes from the memory bank.\n\n\nRF SENSING TASKS\n\nAs shown in Figure. 3, in this paper, we demonstrate the universality of RF-URL framework through three different RF sensing tasks including human gesture recognition with WiFi signals, 3D pose estimation with millimeter wave radar signals, and human silhouette generation with millimeter wave radar signals.\n\n\u2022 Human gesture recognition is a single label prediction task which utilizes a classifier after the pre-trained backbone of RF-URL to classify different gestures. \u2022 3D pose estimation is a structured prediction task that estimates human skeletons by adding a regression module after the pretrained backbone of RF-URL. \u2022 Human silhouette generation is a dense prediction task that generates a semantic segmentation of human by adding a decoder module to the back-end of the pre-trained backbone of RF-URL. Since almost all RF sensing tasks can be seen as a combination of above three tasks, and two most widely used RF signals are WiFi and radar signals, with the above three RF sensing tasks, it is sufficient to demonstrate the universality of the RF-URL framework.\n\n\nSignal Processing\n\nWhile there are many different signal processing algorithms which can be utilized to produce different signal representations, without loss of generality, we mainly utilize AoA-ToF and DFS in this paper. Specifically, as shown in Figure. 3, for human gesture recognition, we adopt two different signal processing algorithms (AoA-ToF and DFS) to produce two signal representations as the input of neural network. For human 3D pose estimation and silhouette generation, two perpendicular radars have been deployed for data collection, which have captured human information from two different views. In such a case, we can simply adopt one signal processing algorithm (AoA-ToF) to naturally generate two signal representations from the captured data of two radars.\n\n3.1.1 AoA-ToF. Considering the signal transmitted and reflected from AoA and ToF , the relative phase shift of this signal on adjacent antennas is \u03a6( ) = {\u2212 2 cos }, where , and denote the space interval between two adjacent antennas, signal frequency and the speed of light. The phase shift on adjacent\nfrequencies is \u03a6( ) = {\u2212 2 \u0394 },\nwhere \u0394 denotes the difference between adjacent frequencies. By compensating the phase shift and adding the signals on different antennas and frequencies, the signals from AoA and ToF would superimpose coherently while the signals from other locations would be suppressed. Hence, the signals from that AoA-ToF could be separated, and the extracted signal [48,49] can be expressed as\n( , ) = \u2211\ufe01 =0 \u2211\ufe01 =0 , 2 cos 2 \u0394 ,(6)\nwhere , denotes the received signal, and denote the index of receiver antenna and signal frequency.\n\nSince typical frequency modulated continuous wave (FMCW) radars could perform signal transceiving over large bandwidth with multiple input multiple output (MIMO) antenna array, AoA-ToF representations from radar could achieve much higher spatial resolution with larger data size compared with that of WiFi. By contrast, WiFi devices do not need to perform frequency sweeping, which leads to higher frame rate compared with radar. The output of the algorithm is a matrix of dimension 1 \u00d7 2 . For FMCW radar, 1 = 160 and 2 = 200. For WiFi, 1 = 96 and 2 = 96, which is smaller due to its lower spatial resolution.\n\n\nDFS.\n\nFollowing the literature [51], the signal processing pipeline of DFS includes three steps: (1) We first perform conjugate multiplication on CSI of two antennas to remove random offsets;\n\n(2) We perform Principal Component Analysis (PCA) algorithm on the CSI stream to extract human reflections and reduce the data dimension; (3) We perform short-time Fourier transform (STFT) on the processed data to extract Doppler information. The DFS output is a frequency-time matrix of dimension (121, 1024).\n\n\nBasic settings\n\nUnless specified, the following settings are used for RF-URL.\n\n\u2022 Translator has batch normalization (BN) applied to each fullyconnected (FC) layer with ReLU activation function, and the output FC has no ReLU with 128-dimension (128-d). This MLP has 3 layers with hidden size 1024-d. \u2022 Predictor has BN applied to each FC layer with ReLU activation function. This MLP has 2 layers with hidden size 1024-d. The cosine distance is used for the distance metric in Eqn.(1) as follows\nD ( 1 , 2 ) = \u2212 1 \u2225 1 \u2225 2 \u00b7 2 \u2225 2 \u2225 2 ,(7)\nwhere \u2225\u00b7\u2225 2 is L 2 normal. The predictor works only in first five epochs and is closed afterwards during pre-training stage. The in Eqn. (4) is 1.0 by default. \u2022 Optimizer: We use stochastic gradient descent (SGD) optimizer with a cosine decay schedule and a warm up of 10 epochs. We also find a large initial learning rate, e.g., 0.6, can work well and produce better results. This is because RF-URL is based on memory bank, which requires a large learning rate to ensure the backbone to be adapted to the stored representations. The weight decay is 0.0001 and the SGD momentum is 0.9. \u2022 Device: All experiments runs on 4\u00d7 NVIDIA Tesla V100 GPU(32GB)\n\nwith PyTorch [34] library. All BN layer are replaced by Sync BN. \u2022 Hyperparameters: The temperature parameter in Eqn. (2) is set as 0.07, and the momentum coefficient for memory bank update in Eqn. (5) is set as 0.5. The negative pairs for calculating InfoNCE loss in Eqn. 2 is 4096. The batch size of the pre-training stage is 256 by default. \u2022 Evaluation protocol: We evaluate the RF-URL performance under both feature frozen setting and fine-tuning setting by freezing/ fine-tuning the backbone (initialized from RF-URL) and train a sub network from scratch. The evaluation metrics are classification accuracy, average 2 distance between predicted keypoints and their ground-truth, and the average intersection-over-union (IoU) between the generating silhouette and ground-truth.\n\n\nHUMAN GESTURE RECOGNITION 4.1 Dataset\n\nRF-URL is designed to learn a general feature representation in an unsupervised manner using pre-training dataset without labels. Then training dataset with labels is utilized to evaluate the effectiveness of RF-URL. In practical deployment, it requires less overhead to collect unannotated data, and it is more common that the pre-training dataset is much larger than the annotated training dataset. Thus, we use a public dataset Widar3.0 [51] to evaluate the RF-URL framework for human gesture recognition, where two types of human gesture dataset are collected. The first dataset (non number dataset) collects the widely used hand gestures for human-computer interaction, which contains 38687 samples. Due to the difficulty in constructing large-scale annotated RF dataset, it is unpractical to pre-train the model in a supervised manner. Thus, although the Widar3.0 is annotated, to simulate the real scenario, we remove the labels of first dataset to create the pre-training dataset for RF-URL to extract the general representation.\n\nThe second dataset (number dataset) collects some complex and semantic gestures that draw number 0-9 in the horizontal plane with a total of 5000 samples.\n\nWe perform a dynamic link selection (DLS) algorithm [51] for Widar3.0 to prune those WiFi receivers that may potentially be blocked by human torso and use the rest of the devices for human gesture recognition. Each sample in dataset 1 and dataset 2 contains 6 links, which means that we can get 6 \u00d7 38687 = 232002 samples for dataset 1 and 6 \u00d7 5000 = 30000 samples for dataset 2. After performing the DLS algorithm, we obtain 143255 samples for dataset 1 and 23574 samples for dataset 2. The dataset 2 are randomly split into training dataset (21335 samples) and validation dataset (2239 samples) with a ratio of 0.9:0.1.  [25], as an alternative version of ResNet-18/34. Given that the backbones of RF-URL for human gesture recognition are dual-branch, we split ResNet into two part by halving channels, e.g, a convolution layer with 256 3 \u00d7 3 filters is split into two convolution layers with 128 3 \u00d7 3 filters. \n\n\nBaseline\n\n\nExperimental Results\n\n\nClassification Accuracy.\n\nComparison study: The performance of different models on the human gesture recognition accuracy is illustrated in Table. 1. We can see that with the RF-URL pre-training, the accuracy of all backbones are all improved, and 94.060% accuracy can be achieved for ResNet-152 with fine-tuned backbone. The highest accuracy for training from scratch is 89.326% obtained by ResNet-152, which is however even lower than the lowest accuracy of RF-URL pretraining method, i.e., 91.201% obtained by fine-tuned ResNet-17.\n\nFrom Table. 1, we can also see that with the ResNet-50 backbone, a carefully designed RF-URL can improve the performance with 7.995%, achieving 97.008% accuracy, which is 4.108% higher than Widar3.0 [51]. These results show that our RF-URL pre-training model can extract general information for gesture recognition from dataset 1 and apply it to dataset 2. Representation quality: Table. 2 shows the results of frozen feature setting with backbone initialized randomly (denoted as \"Random init\") or from RF-URL (denoted as \"RF-URL(Frozen)\"). We can see that pre-training using RF-URL learns better representations, which deliver a maximum improvement of 70.334% for ResNet-101 and a minimum improvement of 44.975% for ResNet-17.\n\nAccuracy vs parameters: As shown in Figure. 4, as the increase of neural network parameters, both fine-tuning and learning from scratch methods achieve better performance. However, the accuracy of learning from scratch method increases slowly and then tends to be stable. The fine-tuning method is more benefit from the larger models with a rapid and continuous increasing trend.   Overall, as the annotated samples increase from 0% to 100%, the performance improves but the improvement decreases. Compared with training from scratch, RF-URL can achieve significant performance improvement with fewer annotated samples. For example, the performance of RF-URL pre-training models trained on 50% of training dataset is higher than the model trained form scratch using 100% of training dataset. Even though only using 10% of training dataset, RF-URL pre-training model also achieves a remarkable performance of 82.314%, 83.519% and 84.323%, which demonstrates the effectiveness of RF-URL. We should notice that RF-URL pretraining model only shows the effectiveness when combined with a fine-tune process. Without the training dataset, i.e., 0% training dataset, RF-URL only obtains about 10% accuracy, which behaves like a random classifier. This is because the RF-URL only extracts the general feature representations in an unsupervised learning manner, which does not include the training process of classifier. The classifier works well only when it is fine-tuned using annotated dataset. Thus, we use 10% labels as the smallest annotated samples in the following human 3D pose estimation and silhouette generation task.\n\nPre-training vs Scratch: From Table. 3, we can observe that as the number of annotated training data decreases, the accuracy of all methods decreases, e.g. removing 90% of training dataset, ResNet-50 drops 24.565% (Scratch), 8.888% (Frozen) and 21.528% (Fine-tune). Compared with learning from scratch, the RF-URL pretraining models (both Frozen and Fine-tune) maintain relatively high accuracy, which demonstrates that RF sensing task could be benefit from the RF-URL pre-training models, even in the condition of limited annotated samples.\n\nFrozen vs Fine-tune: We also observe that RF-URL with finetuned backbone achieves the best performance located at the lower left corner of the Table. 3. Thus, RF-URL with fine-tuned backbone is benefit from a larger model associated with large-scale annotated dataset. Nevertheless, frozen RF-URL is more stable w.r.t the change of models and annotated samples, which indicates that frozen RF-URL can be a general scheme with some performance loss, e.g., drops 1.965% for ResNet152 combined with 100% labels. Table. 4 reports the performance of ResNet-50 with different size of pre-training dataset. Overall, a smaller pre-training dataset has worse performance, which may be due to the over-fitting effect on the smaller pretraining dataset. \n\n\nDifferent Size of Pre-training Dataset.\n\n\nAblation.\n\nIn this subsection, we conduct ablation studies to evaluate some important components of our RF-URL framework for gesture recognition.\n\nPredictor: We ablate the predictor of RF-URL by using ResNet-17 with a total pre-training number of 100 epochs and a bigger batchsize 512. We adopt frozen RF-URL method to train the classifier. The results are shown in Table. 5, where epochs indicates that the predictor participates in training during epoch 0 to and is discarded afterwards. We get the highest accuracy 88.566% when predictor is only trained 5 epochs, which improves the accuracy of 1.027% compared to that without predictor. However, a longer trained predictor reduces the performance about 0.402% (epoch 50) compared to that without predictor. The results indicate that RF-URL could be benefit from a short trained predictor. Backbone and representation extracted layer: We ablate the dual branch of RF-URL with symmetrical backbone where both DFS and AoA-ToF use ResNet-50, and asymmetrical backbone where DFS and AoA-ToF use ResNet-50 and 3D-ResNet-50, respectively. Note that AoA-ToF is a 3D tensor (AoA-ToF-time) rather than a 2D tensor. Thus, in the asymmetrical backbone, ResNet-50 is replaced by 3D-ResNet-50 [18]. For the symmetrical backbone, a 3D tensor can be treated as multiple 2D tensors stacked over the channel dimension. Hence, we adopt the same ResNet-50 with different input channels numbers for 2D and 3D tensor. Since the translator plays the role of unifying the representation of ResNet-50 and 3D-ResNet-50 by transforming the feature of 2D tensor and 3D tensor into a vector, we also take the representation extracted layer in translator into consideration. As shown in Table. 6, the asymmetrical backbone (ResNet-50 + 3D-ResNet-50) with the representation in layer-2 obtains the highest accuracy in both frozen (94.239%) and fine-tuning (96.784%) strategies. This result shows that RF-URL is benefit from a customized backbone for input RF signals. Table.6 also reports that the asymmetrical backbone significantly decreases the accuracy in layer-0, compared with symmetrical backbone, which decreases 7.504% and 8.308% for frozen and fine-tune strategy respectively. This is because it is difficult for the classifier to handle the simply stacked 2D spatial and 3D spatio-temporal features.\n\nBy comparing layer-2 with layer-0, there is a great gain +9.336% (frozen) and +12.461% (fine-tune) in ResNet-50 + 3D-ResNet-50, which supports our hypothesis that translator plays the role of transforming information. It is also worth noting that when using a symmetrical backbone, representation in a deeper extracted layer (frozen with layer-1 or layer-2) might has a negative impact on performance. Therefore, a symmetrical backbone should use layer-0 as representation layer for frozen strategy and layer-2 as representation layer for fine-tuning strategy.\n\nShuffle BN vs Sync BN: As similarly reported in [23], our empirical studies show that using Sync BN has a negative impact on performance. This is possibly because the intra-batch communication among samples leaks information. We solve this problem by shuffling BN [23] that trains with multiple GPUs and performs BN on the samples independently for each GPU. The results are shown in Table. 1, from which we can see that a shuffle BN mitigates the leaking information of BN, and improves the performance by 0.224% for a fine-tuned backbone.\n\n\n3D HUMAN POSE ESTIMATION 5.1 Dataset\n\nWe collect a multi-modal dataset, RFP3D (RFPose3D), to evaluate the RF-URL for 3D human pose estimation. As shown in Figure.  The multi-camera system senses the target from different views to generate 3D keypoints using AlphaPose [17] associated with a triangulation process. The hardware system and data processing methods are similar to literature [54]. We collect data under 11 different conditions, including random walk without occlusion, random walk under occlusion (styrofoam, carton, yoga mat and dark) and random action (stand, walk, squat and sit). The RFP3D dataset contains three parts: pre-training dataset, training dataset and validation dataset. Pre-training dataset includes 149506 samples. Both training dataset and validation dataset include 25842 annnotated samples. We feed 10 frames of RF signal into neural network to generate the 3D keypoint of last frame.\n\n\nBaseline\n\n\nNetwork Structure.\n\nOur pose estimation network follows the design of RF-Extractor in RFGAN [47], which utilizes two RF encoding networks to extract human pose information from vertical and horizontal RF signal with a fusion module to combine the extracted information.\n\nBackbone network: The RF encoding network utilizes 6 layers of 5 \u00d7 5 convolutions with strides 2 and padding 2. The channels of 6 convolution layers are [10 , 5 , 16 , 32 , 128 , 128 ], where = 4, = 0.5, 1, 2, denoted as RFPose-Tiny (RFP-T), RFPose-Base (RFP-B) and RFPose-Large (RFP-L), respectively. Each convolution layer is followed by a BN layer and ReLU activation function.\n\nCross spatial attention (CSA) module is an information aggregation module that makes vertical and horizontal RF signals interact with each other. The CSA is calculated as\n( , ) = ( ) \u00b7 ( ) / \u221a , , \u2208 [0, \u00b7 \u210e] , [1, \u00b7\u210e, \u00b7\u210e] = Conv [ , \u00b7\u210e, \u00b7\u210e] ,(8)\nwhere , (with a reshape operation [ , , \u210e] \u2192 [ , / , \u00b7\u210e]) are feature maps of vertical and horizontal RF signals that are extracted by backbone, , \u210e, , are channel, height, width of feature maps and scale factor, and Conv is a 5 \u00d7 5 convolution layer with strides 2 and padding 2 to process CSA information.\n\nPose estimation network (PEN): The PEN network receives the representations from CSA to estimate 3D keypoints, which is composed of 2 FC layers with hidden size 256. Each FC layer is followed by a BN layer and ReLU activation function. The output layer size is 14 \u00d7 3 without activation function followed. The loss function is\n= 1 \u2211\ufe01 =1 \u2225 \u2212 \u2225 2 + 1 \u2211\ufe01 =1 ( \u2212 ) 2 ,(9)\nwhere indicates the number of human keypoints, and are prediction and ground-truth of 3D coordinates for -th keypoint. It is noted that the proposed PEN model works only for single-user case since only single-user dataset has been accessible. A multi-user case could be supported by adding some additional modules like region proposal network (RPN) and ROI Pooling as [53,54].\n\n\nTraining Details.\n\n(1) Pre-training: We perform pre-training on the pre-training dataset with a total training number of 50 epochs.\n\n(2) Fine-tune: The PEN module combined with a frozen or finetuned backbone is trained on the training dataset with 50 epochs and evaluated on the validation dataset. The batch size is 128. The learning rate is 0.03 for PEN and 0.003 for fine-tuning backbone.\n\n\nExperimental Results\n\n\n3D Pose Estimation Performance.\n\nComparison study: The performance of 3D pose estimation with different methods are shown in Table. 7 and Figure. 6. We can see that the RF-URL pre-training method achieves centimeter accuracy for RFP-T (63 ), RFP-B (64 ) and RFP-L (64 ), while the trained model from scratch achieves decimeter accuracy for RFP-T (102 ), RFP-B (114 ) and RFP-L (262 ). These results demonstrate that 3D pose estimation task is benefit from the RF-URL pre-training model. From Table. 7, we can also see that with RFP-T, an elaborated designed RF-URL can improve the performance by 39 . The result shows that our RF-URL pre-training model can generalize well to 3D pose estimation task.\n\nThe performance of RFP-T trained from scratch (102 ) is higher than that of RF-Pose3D (112. 7 ) although the network architecture of RFP-T is simple. This is due to the fact that the training dataset of RFP3D is relatively small which may not be adequate to tune the parameters in RF-Pose3D. A similar phenomenon occurs for RFP-B (114 ) and RFP-L (262 ), which have more parameters but achieve poorer performance.  Representation quality: Table. 8 reveals that pre-training using RF-URL learns high-quality representations, which delivers a maximum improvement of 135 for RFP-B and a minimum improvement of 123 for RFP-L compared with random initialization backbone. Does RFP-B/L exist over-fitting? Table. 7 shows that a larger model gets a worse accuracy, which might exist over-fitting due to the limited dataset. Therefore, an early terminated training experiment for RFP-B is conducted to verify our conjecture with RF-URL (Frozen), and the results are shown in Table. 9. When only training 30 epochs, RFP-B obtains the highest accuracy 69 that is almost the same accuracy as RFP-T. This result suggests that pre-training dataset should be as rich as possible and an early terminated training can relieve the over-fitting problem.  Table. 10 shows the performance of RFP-T/B/L with different annotated samples. The training dataset are randomly sampled % of samples, e.g., 100%labels (25842 samples), 50% (12921samples) and 10% (2584 samples). All results are evaluated on RFP3D validation dataset. From Table. 10, we can see that all RF-URL pre-training methods outperform the corresponding scratch methods. Even only with 10% of training data, the RF-URL pre-training methods still maintain relatively high accuracy for RFP-T (103 ), RFP-B (109 ) and RFP-L (119 ). Fine-tune RF-URL method is higher than frozen method about 4.1 mm. These results show that the RF-URL pre-training models are scalable to the limited data condition. \n\n\nDifferent Size of Pre-training Dataset.\n\nWe evaluate the performance of RFP-T under different size of pre-training dataset. Table. 11 shows that as the size of pre-training dataset reduces from 100% to 0%, the performance decreases from 63 to 86 . The decreased performance is expected since the pre-training model gradually suffers from the over-fitting problem with the size reduction of pre-training dataset. \n\n\nAblation.\n\nIn this subsection, we conduct ablation studies to evaluate some important components of our RF-URL framework for 3D pose estimation. Predictor: We ablate the predictor of RF-URL by using RFP-T (Frozen) with different training duration. The results are shown in Table. 12, from which we can see that the highest accuracy 68 is obtained when predictor is only trained 5 epochs. However, a longer trained predictor (epoch 50) reduces the performance by 4 compared that without predictor. The results indicate that RF-URL could be benefit from a short trained predictor. Information aggregation module (IAM): We ablate IAM for RF-URL 3D pose estimation using RFP-T network. The candidate modules include CSA, channel shuffle (CS) [33], layer-3 (feature extracted from layer 3 of translator) and without IAM (just stack the extracted feature maps). The results are illustrated in Table. 13. CSA obtains the highest accuracy of 68\n\n. Compared without IAM, CSA, CS and layer-3 improves the accuracy by 11 , 4 and 8\n\n. Note that CSA reduces the error by 5 when training from scratch, according to Table. 7 where RFP-T with CSA gets 102 accuracy and RFP-T without IAM gets 97 accuracy. These results demonstrate that CSA is more suitable for RF-URL pre-training model, but has a negative impact for training from scratch.  Table. 7. We can see that a shuffle BN improves 1 for fine-tuned backbone. Although only a little performance improvement, Sync BN also has a positive impact on RF-URL for 3D pose estimation.\n\n\nHUMAN SILHOUETTE GENERATION 6.1 Dataset\n\nUsing the same hardware system as in Section 5, we collect a multimodal dataset to evaluate the RF-URL framework for human silhouette generation task. The multi-camera system captures images to generate human silhouette ground-truth using Mask R-CNN [24]. We collect data in four different environments under 11 different conditions, including random walk with no occlusion, random walk under occlusion (styrofoam, carton, yoga mat and dark) and random action (stand, walk, squat and sit) for both single-person and multiperson scenarios. We use three environments data as pre-training dataset (119280 samples), and one environments data excluding occlusion parts as training dataset (16272 samples) and validate dataset (3312 samples). We feed 12 frames of RF signals to generate 6 frames of human silhouette segmentation.\n\n\nBaseline\n\n6.2.1 Network Structure. For fair comparison with the existing methods, we do not adopt the same backbone in Section 5. Instead, our human silhouette generation network, named as RFSG, follows the design of RF-Pose [52], which uses two RF encoding networks to extract features from vertical and horizontal RF signals. Then, the outputs of encoding networks are concatenated and fed into generation network to generate human silhouette segmentation.\n\nBackbone network: The RF encoding network uses 10 9 \u00d7 5 \u00d7 5 3D convolutions layers with 1 \u00d7 2 \u00d7 2 strides and 16 channels followed by a BN layer and a ReLU activation function, where = 1 for the last layer. The channels of backbone network = 0.5, 1, 2 are named as RFSG-T, RFSG-B and RFSG-L.\n\nSilhouette generation network (SGN): The generation network is composed of 4 deconvolution layers, where the first three layers are equipped with the kernel of size 3 \u00d7 6 \u00d7 6 and stride 1 \u00d7 2 \u00d7 2, while the last one has the kernel of size 3 \u00d7 6 \u00d7 6 and stride 1\u00d74\u00d74. The number of channels at different layers are [64, 32, 16, 1], respectively. RF-Pose [52] uses Parametric ReLU (PReLU) activation function without BN layer. However, RFSG uses Leaky ReLU with slope of 0.02 and BN layer.\n\n\nTraining Details.\n\n(1) Pre-training: We perform pre-training on pre-training dataset with a total training number of 50 epochs.\n\n(2)Fine-tune: The SGN module combined with a frozen or finetuned backbone is trained on training dataset with 50 epochs and evaluated on validation dataset. The batch size is 64. The learning rate of SGN and backbone is 1.0.\n\n\nExperimental Results\n\n\nHuman Silhouette Generation Performance.\n\nComparison study: The performance of human silhouette generation with different methods are shown in Table. 14 and Figure.  Representation quality: Table. 15 reveals that pre-training using RF-URL learns high quality representations, which delivers an IoU improvement of 0.327, 0.317, 0.288 for RFSG-T, RFSG-B and RFSG-L, respectively, compared with random initialization backbone. Table. 16 shows the performance of RFSG-T/B/L with different annotated samples. The training dataset are randomly sampled % of samples, e.g., 100%labels (16272 samples), 50% (8136 samples) and 10% (1627 samples). All results are evaluated on the validation dataset. From Table. 16, we can see that all RF-URL pre-training methods outperform the corresponding scratch methods. Even only with 10% of training data, the RF-URL pre-training methods still maintain relatively high IoU for RFSG-T (0.581), RFSG-B (0.586) and RFSG-L (0.565). However, the performance without pre-training decreases rapidly, i.e., decrease     Table.18, from which we can see that the highest IoU 0.619 is obtained when predictor is only trained 5 epochs. The results indicate that RF-URL could be benefit from a short trained predictor.  Table. 14, shuffle BN decreases the performance of RF-URL pre-training model, which is different from those in Section 4.3.4 and 5.3.4. This may be reason that 3D full convolution network RFSG is hard to train. As shown in Figure. 8, compared with the negative impact of leaking information, the poor convergence brought by Shuffle BN is more harmful.\n\n\nDifferent Annotated Samples.\n\n\nRELATED WORK\n\nRF sensing: Learning-based RF sensing has recently gained attentions in health care and smart homes, including human gesture recognition [26,31,41,42,51], activity recognition [11,15,29,43,45,46], human pose estimation [30,[52][53][54], person re-identification [16,32], fall detection [39], vital sign monitoring [12,13,[48][49][50]55], and so on. These existing works mainly rely on supervised learning which requires large-scale annotated RF datasets, while the proposed framework exploits unannotated data for model training.\n\nMasking and predicting model: The pre-training method has achieved unprecedented success in NLP community, e.g. GPT [4,35,36] and BERT [14]. These methods mask a portion of the input sequence and try to predict the missing content, which have been shown with excellent scalability and generalization. Although language and RF signals have similar sequential structure, relevant information in radio signals are typically very sparse while language signals are highly information-dense, causing a big gap between RF sensing and NLP community.\n\nContrastive learning: Recently, contrastive learning has became popular for learning effective representations. The learned representations make downstream tasks solved easier, and the performance even surpasses the supervised methods [8,23]. The core idea of contrastive learning is to attract the positive sample pairs and repulse the negative sample pairs. The commonly used contrastive learning frameworks include memory bank method (e.g., InsDis [44], MoCo [8,23] and contrastive multiview coding (CMC) [38]), big batchsize (e.g., SimCLR [7]), clustering (e.g., SwAV [5]), transformer (e.g., MoCov3 [10] and DINO [6]) and negative-pairs-free methods (e.g., BYOL [20] and SimSiam [9]). However, these methods strongly depend on data augmentation [7,9,20], which always tends to learn shortcut rather than meaningful information for RF signals [32].\n\nContrastive multiview coding (CMC) [38]: Our RF-URL is a form of CMC, but different from the classical CMC in following ways. Firstly, existing investigations have demonstrated that traditional data augmentation methods are inefficient for RF data. To resolve this problem, we have noted that different signal processing methods could naturally generate different representations of the same signal. Inspired by this phenomenon, RF-URL utilizes RF signal processing methods to construct positive and negative pairs rather than data augmentation. Secondly, the inputs of CMC are symmetric but RF-URL adopts an asymmetric signal representations as input(e.g., DFS and AoA-ToF). The asymmetric signal representations make the CMC-based method not converge, while RF-URL adopts a translator to solve this problem. In addition, RF-URL adopts a predictor with stop-grad operation to improve the quality of learned representations through a short-term training.\n\nSynthetic data: Synthetic dataset can be generated by a RF ray-tracing simulator to solve the data-hungry problem [21,37]. Although synthetic RF signals share some similar properties with real RF data, the simulators may not capture all physical RF phenomena such as multi-path, reflections, diffraction and polarization effects causing a gap between synthetic and real RF signals. Nevertheless, these techniques can also be integrated with the proposed framework to generate more data for general semantic feature extraction.\n\n\nDISCUSSIONS\n\nWhy contrastive learning is a reasonable choice? Different from visual images or natural languages, annotating RF signals is much more costly since they are non-intuitive and non-interpretable. Contrastive learning is an unsupervised learning method that could learn a general semantic representation from unannotated dataset. The main challenge of contrastive learning lies in the design of the principle to construct positive and negative pairs. To this end, we have noted that different signal processing methods could naturally generate different representations of the same signal, which could be utilized to construct positive/negative pairs. Since various signal processing methods have been developed for RF sensing, we could utilize contrastive learning to seamlessly integrate these welldeveloped signal processing techniques. In this sense, contrastive learning is a good choice.\n\nSignal representation: In this paper, the backbone of RF-URL is based on convolutional neural networks (CNNs), which could extract features of the signals based on their spatial distributions. Although linear transformations are utilized to generate different signal representations which wouldn't change the information of signal itself, they could rearrange the spatial characteristics of the input signal to yield different spatial information. Thus, different linear transformations of the same RF signal could expose different spatial features to CNNs, which is helpful for training.\n\nThe role of predictor and InfoNCE: The functionality of InfoNCE is to attract positive pairs and repulse negative pairs, where both the positive and negative pairs are composed by the features from memory bank and the output of neural network. However, since the memory bank is randomly initialized, at the early stages of the training processes, random noise may be sampled to form positive pairs with the network output, leading to fluctuations of training. On the other hand, predictor directly shortens the distance between the output of multi-branch neural network, which can weaken the negative impact of noise in memory bank and smooth the training processes. Thus, in this paper, we enable the predictor in the first 5 epochs of the training processes, which lead to slightly performance gain. Note that the further training of predictor tends to prevent the backbone from aligning the features of the memory bank, which may lead to performance degradation.\n\n\nCONCLUSION\n\nThis paper presented a novel URL framework, RF-URL, for RF-based sensing through contrastive learning. The positive and negative pairs were constructed with different signal processing technologies, which achieved effective contrastive learning in an unsupervised manner by minding the gap between signal processing and neural network. All experimental results strongly demonstrated that RF-URL took an important step towards deploying learningbased solutions for large-scale RF-based sensing applications.\n\nFigure 1 :\n1An illustration of RF-URL pre-training mode for RF sensing.\n\nFigure 2 :\n2Training RF-URL with/without translator.\n\nFigure 3 :\n3An illustration of RF-URL for gesture recognition, 3D pose estimation and silhouette generation.\n\n\n4.2.2 Training Details.(1) Pre-training: We perform pre-training on Widar3.0 pre-training dataset with a total training number of 150 epochs. (2) Fine-tune: The linear classifier combined with a frozen or fine-tuned backbone is trained on training dataset with 50 epochs and evaluated on validation dataset. The batch size is 128. For frozen setting, the initial learning rate is 30. For fine-tune setting, the learning rates of backbone and classifier are 0.003 and 0.03, respectively.\n\nFigure 4 :\n4Evaluation of RF-URL for human gesture recognition under different parameters and settings. 4.3.2 Different Annotated Samples.\n\n\n5, RFP3D synchronously captures the millimeter wave radar signals by dual perpendicular TI MMWCAS-RF-EVM FMCW radars with an antenna array of 12 transmitters and 16 receivers, and the optical images by a 13-view camera system. The sweep ranges of dual FMCW radar are 77-78.23 GHz and 79-80.23 GHz respectively.\n\nFigure 5 :\n5RF signals and RGB image synchronous record.\n\nFigure 6 :\n6Pose estimation under different environments, where RF-URL adopts fine-tuning strategy and GT stands for ground-truth.\n\n\n7.The RF-URL pre-training method achieves IoU of 0.610, 0.619, 0.613 for RFSG-T, RFSG-B and RFSG-L, while the trained model from scratch only achieves IoU of 0.539, 0.556, 0.571 for RFSG-T, RFSG-B and RFSG-L, respectively. Compared to RF-Pose[52], with a similar number of parameters, RFSG-B improves the performance of IoU about 0.036. The results show that our RF-URL pre-training models can generalize well to human silhouette generation.\n\n\n4.2.1 Backbone Network. We adopt ResNet [25] as the backbone followed with translator and predictor. Since there are two different basic block of ResNet, e.g., BasicBlock for ResNet-18/34 and Bottleneck for ResNet-50/101/152, to avoid potential inconsistency, we unify the basic block with Bottleneck and propose a new structure ResNet-17/35, i.e., ResNet-17: [1, 1, 2, 1] and ResNet-35: [2, 3, 3, 3] for Bottleneck [conv2_x, conv3_x, conv4_x, conv5_x] in\n\nTable 1 :\n1The performance of different models on the human gesture recognition accuracy.Pre-training \nMethod \nAccuracy \n\nEI[28] \n80.0 \n-\nWidar3.0[51] \n92.9 \n\nResNet-17 \n86.780 \nResNet-35 \n88.656 \n-\nResNet-50 \n89.013 \nResNet-101 \n89.058 \nResNet-152 \n89.326 \n\nResNet-17 \n91.201 (+4.421) \nResNet-35 \n92.363 (+3.707) \nRF-URL \nResNet-50 \n92.631 (+3.618) \n(Fine-tune) \nResNet-101 \n93.301 (+4.243) \nResNet-152 \n94.060 (+4.734) \n\nResNet-50 (baseline) \n89.013 \n+ RF-URL(frozen) \n92.229 (+3.216) \n+ Predictor \n92.407 (+0.178) \nRF-URL \n+ Fine-tune \n92.631 (+0.224) \n(Details) \n+ 3D CNN \n84.323 (-8.308) \n+ feature in translator \n96.784 (+12.461) \n+ Shuffle BN \n97.008 (+0.224) \n\n\n\nTable 2 :\n2Evaluate the accuracy of RF-URL for human gesture recognition under frozen feature setting with random initialization or RF-URL.Model \nParameters \nRandom init \nRF-URL (Frozen) \n\nResNet-17 \n11.18M \n46.539 \n91.514 \nResNet-35 \n21.85M \n34.971 \n91.603 \nResNet-50 \n25.55M \n28.093 \n92.407 \nResNet-101 \n44.54M \n21.617 \n91.961 \nResNet-152 \n60.19M \n22.778 \n92.095 \n\n\n\nTable .\n.3 shows the performance \nof ResNet-17/50/152 with different annotated samples. The train-\ning dataset of Widar3.0 are randomly sampled % of samples, e.g., \n100%labels (21335 samples), 50% (10667samples) and 10% (2133 sam-\nples). All results are evaluated on Widar3.0 validation dataset. \n\n\n\nTable 3 :\n3Evaluate the accuracy of RF-URL with different annotated samples, Frozen and Fine-tune are backbone initialization from RF-URL.Model \nPre-training 100%labels 50%labels 10%labels 0%labels \n\n-\n86.780 \n82.269 \n65.699 \n10.540 \nResNet-17 Frozen \n91.514 \n89.549 \n82.314 \n10.808 \nFine-tune \n91.201 \n84.591 \n63.510 \n-\n\n-\n89.013 \n84.815 \n64.448 \n11.121 \nResNet-50 Frozen \n92.407 \n90.621 \n83.519 \n10.630 \nFine-tune \n92.631 \n90.174 \n71.103 \n-\n\n-\n89.326 \n84.323 \n61.411 \n10.585 \nResNet-152 Frozen \n92.095 \n90.889 \n84.323 \n9.558 \nFine-tune \n94.060 \n91.157 \n72.086 \n-\n\n\n\nTable 4 :\n4Evaluate the accuracy of ResNet-50 under different size of pre-training dataset.Size \n100% \n80% \n60% \n40% \n20% \n0% \n\nFrozen \n92.407 89.192 82.448 76.061 65.386 28.093 \nFine-tune 92.631 92.586 89.951 84.949 84.055 84.011 \n\n\n\nTable 5 :\n5Predictor with different training epochs.Epochs 0 (w/o pred.) \n5 \n10 \n25 \n50 \n100 \n\nAcc. \n87.539 \n88.566 87.673 87.271 87.137 88.164 \n\n\n\nTable 6 :\n6Evaluate the accuracy of RF-URL with different backbone and the representation extracted layer.Models \nRep. in \nFrozen \nFine-tune \n\nlayer-0 \n92.407 \n92.631 \nResNet-50 + \nlayer-1 \n90.531 \n92.720 \nResNet-50 \nlayer-2 \n90.621 \n95.489 \n\nlayer-0 \n84.903 \n84.323 \nResNet-50 + \nlayer-1 \n87.628 \n85.753 \n3D-ResNet-50 \nlayer-2 \n94.239 \n96.784 \n\n\n\nTable\n\n\nTable 8 :\n8Evaluate the performance (Pose Err.(mm)) of RF-URL for 3D pose estimation by RFP under frozen feature setting with different backbone initialization strategy.Model \nParameters \nRandom init \nRF-URL (Frozen) \n\nRFP-T \n2.66M \n198 \n68 \nRFP-B \n3.87M \n206 \n71 \nRFP-L \n7.09M \n200 \n77 \n\n\n\nTable 9 :\n9Early terminated training of RFP-BEpoch \n10 \n20 \n30 \n40 \n50 \n\nPose Err.(mm) \n89 \n73 \n69 \n71 \n71 \n\n5.3.2 Different Annotated Samples. \n\nTable 10 :\n10Evaluate the performance (Pose Err.(mm)) of RFP with different annotated samples, Frozen and Fine-tune are backbone initialized from RF-URL.Model Pre-training 100% labels 50% labels 10% labels \n\n-\n102 \n304 \n305 \nRFP-T Frozen \n68 \n72 \n104 \nFine-tune \n63 \n68 \n103 \n\n-\n114 \n288 \n305 \nRFP-B Frozen \n71 \n76 \n111 \nFine-tune \n64 \n70 \n109 \n\n-\n262 \n303 \n305 \nRFP-L \nFrozen \n77 \n82 \n119 \nFine-tune \n64 \n71 \n122 \n\n\n\nTable 11 :\n11Evaluate the performance (Pose Err.(mm)) of RFP-T under different size of pre-training dataset.Size \n100% \n80% \n60% \n40% \n20% \n0% \n\nFrozen \n68 \n79 \n88 \n101 \n109 \n198 \nFine-tune \n63 \n67 \n72 \n78 \n83 \n86 \n\n\nTable 12 :\n12Predictor with different training epochs.Epochs \n0 (w/o predictor) \n5 \n15 \n30 \n50 \n\nPose Err.(mm) \n70 \n68 \n69 \n69 \n74 \n\n\n\nTable 13 :\n13Different information aggregation module. Shuffle BN vs Sync BN: As reported in Section. 4.3.4, BN might leak information that prevents RF-URL from learning good representations. Thus, we ablate Shuffle BN and Sync BN for RF-URL in RFP-T network. The results are illustrated inIAM \nCSA \nCS \nw/o IAM \nlayer-3 \n\nPose Err.(mm) \n68 \n75 \n79 \n71 \n\n\n\nTable 14 :\n14The performance of different models on the human silhouette generation.Figure 7: Human silhouette generation under different environments, where RF-URL adopts fine-tuning strategy and GT stands for ground-truth.Pre-training \nMethod \nIoU \n\n-\nRF-Pose[52] \n0.583 \n\nRFSG-T \n0.539 \n-\nRFSG-B \n0.556 \nRFSG-L \n0.571 \n\nRFSG-T \n0.610 (+0.071) \nRF-URL \nRFSG-B \n0.619 (+0.063) \n(Fine-tune) \nRFSG-L \n0.613 (+0.042) \n\nRFSG-B (baseline) \n0.556 \n+ RF-URL(frozen) \n0.557 (+0.001) \nRF-URL \n+ Fine-tune \n0.611 (+0.054) \n(Details) \n+ Predictor \n0.619 (+0.008) \n+ Shuffle BN \n0.614 (-0.005) \n\nRF-URL \n\nScratch \n\nGT \n\nNone \n\nDark \n\n\n\nTable 15 :\n15Evaluate the performance (IoU) of RF-URL for human silhouette generation under frozen feature setting with different backbone initialization strategy.IoU of 0.082, 0.075 and 0.069. These results show that a small annotated dataset could be benefit from RF-URL pre-training method.6.3.3 Different Size of Pre-training Dataset.Table.17 shows the performance of RFSG-B with different size of pre-training dataset. It illustrates that RFSG-B works well in a larger pre-training dataset for both forzen and fine-tune strategy.Model \nParameters \nRandom init \nRF-URL (Frozen) \n\nRFSG-T \n0.39M \n0.225 \n0.552 \nRFSG-B \n0.76M \n0.239 \n0.556 \nRFSG-L \n2.09M \n0.248 \n0.536 \n\n\n\nTable 16 :\n16Evaluate the performance (IoU) of RFSG with different annotated samples, Frozen and Fine-tune are backbone initialization from RF-URL.Model \nPre-training 100% labels 50% labels 10% labels \n\n-\n0.539 \n0.539 \n0.457 \nRFSG-T Frozen \n0.552 \n0.553 \n0.532 \nFine-tune \n0.610 \n0.611 \n0.581 \n\n-\n0.556 \n0.550 \n0.481 \nRFSG-B Frozen \n0.557 \n0.552 \n0.537 \nFine-tune \n0.619 \n0.614 \n0.586 \n\n-\n0.571 \n0.591 \n0.502 \nRFSG-L Frozen \n0.536 \n0.529 \n0.506 \nFine-tune \n0.613 \n0.612 \n0.565 \n\n\n\nTable 17 :\n17Evaluate the performance (IoU) of RFSG-B under different size of pre-training dataset. Ablation. In this subsection, we conduct ablation studies to evaluate some important components of our RF-URL framework for human silhouette generation.Predictor: We ablate the predictor of RF-URL by using RFSG-B (Fine-tune) with different training duration. The results are shown inSize \n100% \n80% \n60% \n40% \n20% \n0% \n\nFrozen \n0.557 \n0.531 \n0.529 \n0.489 \n0.426 \n0.239 \nFine-tune \n0.619 \n0.602 \n0.585 \n0.573 \n0.562 \n0.556 \n\n6.3.4 \n\nTable 18 :\n18Predictor with different training epochs.Figure 8: Training RF-URL with Shuffle BN or Sync BN. Shuffle BN vs Sync BN: We ablate Shuffle BN and Sync BN for RF-URL based on RFSG-B. As shown inEpochs \n0 (w/o predictor) \n5 \n15 \n30 \n50 \n\nIoU \n0.611 \n0.619 \n0.615 \n0.617 \n0.615 \n\n0 \n10 \n20 \n30 \n40 \n50 \n\nEpoch \n\n2 \n\n4 \n\n6 \n\n8 \n\n10 \n\nLoss \n\nShuffle BN \nSync BN \n\n\n\n3D Tracking via Body Radio Reflections. Zachary Fadel Adib, Dina Kabelac, Robert C Katabi, Miller, Proc. of the 11th USENIX (NSDI'14). of the 11th USENIX (NSDI'14)Fadel Adib, Zachary Kabelac, Dina Katabi, and Robert C. Miller. 2014. 3D Tracking via Body Radio Reflections. In Proc. of the 11th USENIX (NSDI'14). 317-329.\n\nSmart Homes That Monitor Breathing and Heart Rate. Hongzi Fadel Adib, Zachary Mao, Dina Kabelac, Robert C Katabi, Miller, Proc. of the 33rd ACM CHI (CHI '15. of the 33rd ACM CHI (CHI '15Fadel Adib, Hongzi Mao, Zachary Kabelac, Dina Katabi, and Robert C. Miller. 2015. Smart Homes That Monitor Breathing and Heart Rate. In Proc. of the 33rd ACM CHI (CHI '15). 837-846.\n\nRepresentation Learning: A Review and New Perspectives. Yoshua Bengio, Aaron Courville, Pascal Vincent, IEEE Transactions on Pattern Analysis and Machine Intelligence. 35Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation Learning: A Review and New Perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence 35, 8 (2013), 1798-1828.\n\nLanguage Models are Few-Shot Learners. Tom Brown, Benjamin Mann, Nick Ryder, Proc. of the 34th NeurIPS (NIPS'20. of the 34th NeurIPS (NIPS'2033Tom Brown, Benjamin Mann, Nick Ryder, and et. al. 2020. Language Models are Few-Shot Learners. In Proc. of the 34th NeurIPS (NIPS'20, Vol. 33). 1877-1901.\n\nUnsupervised Learning of Visual Features by Contrasting Cluster Assignments. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin, Proc. of the 34th NeurIPS (NIPS'20). of the 34th NeurIPS (NIPS'20)831Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. 2020. Unsupervised Learning of Visual Features by Contrasting Cluster Assignments. In Proc. of the 34th NeurIPS (NIPS'20). Article 831, 13 pages.\n\nEmerging Properties in Self-Supervised Vision Transformers. Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 Jegou, Julien Mairal, Piotr Bojanowski, Armand Joulin, Proc. of the IEEE/CVF ICCV. of the IEEE/CVF ICCVMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. 2021. Emerging Properties in Self-Supervised Vision Transformers. In Proc. of the IEEE/CVF ICCV. 9630-9640.\n\nA Simple Framework for Contrastive Learning of Visual Representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, Proc. of the 37th ICML (ICML'20). of the 37th ICML (ICML'20)Article 149, 11 pagesTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A Simple Framework for Contrastive Learning of Visual Representations. In Proc. of the 37th ICML (ICML'20). Article 149, 11 pages.\n\nXinlei Chen, Haoqi Fan, arXiv:2003.04297Ross Girshick, and Kaiming He. 2020. Improved baselines with momentum contrastive learning. arXiv preprintXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. 2020. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297 (2020).\n\nExploring Simple Siamese Representation Learning. Xinlei Chen, Kaiming He, Proc. of the IEEE/CVF CVPR. of the IEEE/CVF CVPRXinlei Chen and Kaiming He. 2021. Exploring Simple Siamese Representation Learning. In Proc. of the IEEE/CVF CVPR. 15745-15753.\n\nAn Empirical Study of Training Self-Supervised Vision Transformers. Xinlei Chen, Saining Xie, Kaiming He, Proc. of the IEEE/CVF ICCV. of the IEEE/CVF ICCVXinlei Chen, Saining Xie, and Kaiming He. 2021. An Empirical Study of Training Self-Supervised Vision Transformers. In Proc. of the IEEE/CVF ICCV. 9620-9629.\n\nSpeedNet: Indoor Speed Estimation With Radio Signals. Yan Chen, Hongyu Deng, Dongheng Zhang, Yang Hu, IEEE Internet of Things Journal. 8Yan Chen, Hongyu Deng, Dongheng Zhang, and Yang Hu. 2021. SpeedNet: Indoor Speed Estimation With Radio Signals. IEEE Internet of Things Journal 8, 4 (2021), 2762-2774.\n\nResidual Carrier Frequency Offset Estimation and Compensation for Commodity WiFi. Yan Chen, Xiang Su, Yang Hu, Bing Zeng, IEEE Transactions on Mobile Computing. 19Yan Chen, Xiang Su, Yang Hu, and Bing Zeng. 2020. Residual Carrier Frequency Offset Estimation and Compensation for Commodity WiFi. IEEE Transactions on Mobile Computing 19, 12 (2020), 2891-2902.\n\nMoVi-Fi: Motion-Robust Vital Signs Waveform Recovery via Deep Interpreted RF Sensing. Zhe Chen, Tianyue Zheng, Chao Cai, Proc. of the 27th ACM MobiCom (MobiCom '21. of the 27th ACM MobiCom (MobiCom '21Zhe Chen, Tianyue Zheng, Chao Cai, and Jun Luo. 2021. MoVi-Fi: Motion-Robust Vital Signs Waveform Recovery via Deep Interpreted RF Sensing. In Proc. of the 27th ACM MobiCom (MobiCom '21). 392-405.\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proc. of the ACL. of the ACLJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proc. of the ACL. 4171-4186.\n\nRF-Net: A Unified Meta-Learning Framework for RF-Enabled One-Shot Human Activity Recognition. Shuya Ding, Zhe Chen, Tianyue Zheng, Proc. of the 18th ACM SenSys. of the 18th ACM SenSysShuya Ding, Zhe Chen, Tianyue Zheng, and Jun Luo. 2020. RF-Net: A Unified Meta-Learning Framework for RF-Enabled One-Shot Human Activity Recogni- tion. In Proc. of the 18th ACM SenSys. 517-530.\n\nLearning Longterm Representations for Person Re-Identification Using Radio Signals. Lijie Fan, Tianhong Li, Rongyao Fang, Rumen Hristov, Yuan Yuan, Dina Katabi, Proc. of the IEEE/CVF CVPR. of the IEEE/CVF CVPRLijie Fan, Tianhong Li, Rongyao Fang, Rumen Hristov, Yuan Yuan, and Dina Katabi. 2020. Learning Longterm Representations for Person Re-Identification Using Radio Signals. In Proc. of the IEEE/CVF CVPR. 10696-10706.\n\nRMPE: Regional Multi-person Pose Estimation. Shuqin Hao-Shu Fang, Yu-Wing Xie, Cewu Tai, Lu, Proc. of the IEEE/CVF ICCV. of the IEEE/CVF ICCVHao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu. 2017. RMPE: Regional Multi-person Pose Estimation. In Proc. of the IEEE/CVF ICCV. 2353-2362.\n\nSlow-Fast Networks for Video Recognition. Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He, Proc. of the IEEE/CVF ICCV. of the IEEE/CVF ICCVChristoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. 2019. Slow- Fast Networks for Video Recognition. In Proc. of the IEEE/CVF ICCV. 6201-6210.\n\nInductive biases for deep learning of higher-level cognition. Anirudh Goyal, Yoshua Bengio, arXiv:2011.15091arXiv preprintAnirudh Goyal and Yoshua Bengio. 2020. Inductive biases for deep learning of higher-level cognition. arXiv preprint arXiv:2011.15091 (2020).\n\nBootstrap Your Own Latent a New Approach to Self-Supervised Learning. Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, Michal Valko, Proc. of the 34th NeurIPS (NIPS'20. of the 34th NeurIPS (NIPS'20Article 1786, 14 pagesJean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhao- han Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, and Michal Valko. 2020. Bootstrap Your Own Latent a New Ap- proach to Self-Supervised Learning. In Proc. of the 34th NeurIPS (NIPS'20). Article 1786, 14 pages.\n\nThrough Fog High-Resolution Imaging Using Millimeter Wave Radar. Junfeng Guan, Sohrab Madani, Suraj Jog, Saurabh Gupta, Haitham Hassanieh, Proceedings of the IEEE/CVF CVPR. the IEEE/CVF CVPRJunfeng Guan, Sohrab Madani, Suraj Jog, Saurabh Gupta, and Haitham Hassanieh. 2020. Through Fog High-Resolution Imaging Using Millimeter Wave Radar. In Proceedings of the IEEE/CVF CVPR. 11461-11470.\n\nDimensionality Reduction by Learning an Invariant Mapping. R Hadsell, S Chopra, Y Lecun, Proc. of the IEEE/CVF CVPR ). of the IEEE/CVF CVPR )2R. Hadsell, S. Chopra, and Y. LeCun. 2006. Dimensionality Reduction by Learning an Invariant Mapping. In Proc. of the IEEE/CVF CVPR ), Vol. 2. 1735-1742.\n\nMomentum Contrast for Unsupervised Visual Representation Learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Proc. of the IEEE/CVF CVPR. of the IEEE/CVF CVPRKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Mo- mentum Contrast for Unsupervised Visual Representation Learning. In Proc. of the IEEE/CVF CVPR. 9726-9735.\n\nMask R-CNN. Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick, Proc. of the IEEE/CVF ICCV. of the IEEE/CVF ICCVKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. 2017. Mask R-CNN. In Proc. of the IEEE/CVF ICCV. 2980-2988.\n\nDeep Residual Learning for Image Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proc. of the IEEE/CVF CVPR. of the IEEE/CVF CVPRKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In Proc. of the IEEE/CVF CVPR. 770-778.\n\nWiFi Vision: Sensing, Recognition, and Detection With Commodity MIMO-OFDM WiFi. Ying He, Yan Chen, Yang Hu, Bing Zeng, IEEE Internet of Things Journal. 7Ying He, Yan Chen, Yang Hu, and Bing Zeng. 2020. WiFi Vision: Sensing, Recog- nition, and Detection With Commodity MIMO-OFDM WiFi. IEEE Internet of Things Journal 7, 9 (2020), 8296-8317.\n\nEnabling Identification and Behavioral Sensing in Homes Using Radio Reflections. Chen-Yu Hsu, Rumen Hristov, Guang-He Lee, Mingmin Zhao, Dina Katabi, Proc. of the 2019 ACM CHI (CHI '19. of the 2019 ACM CHI (CHI '19Chen-Yu Hsu, Rumen Hristov, Guang-He Lee, Mingmin Zhao, and Dina Katabi. 2019. Enabling Identification and Behavioral Sensing in Homes Using Radio Reflections. In Proc. of the 2019 ACM CHI (CHI '19). 1-13.\n\nTowards Environment Independent Device Free Human Activity Recognition. Wenjun Jiang, Chenglin Miao, Fenglong Ma, Shuochao Yao, Yaqing Wang, Ye Yuan, Hongfei Xue, Chen Song, Xin Ma, Dimitrios Koutsonikolas, Wenyao Xu, Lu Su, Proc. of the 24th ACM MobiCom (MobiCom '18. of the 24th ACM MobiCom (MobiCom '18Wenjun Jiang, Chenglin Miao, Fenglong Ma, Shuochao Yao, Yaqing Wang, Ye Yuan, Hongfei Xue, Chen Song, Xin Ma, Dimitrios Koutsonikolas, Wenyao Xu, and Lu Su. 2018. Towards Environment Independent Device Free Human Activity Recognition. In Proc. of the 24th ACM MobiCom (MobiCom '18). 289-304.\n\nTowards Environment Independent Device Free Human Activity Recognition. Wenjun Jiang, Chenglin Miao, Fenglong Ma, Shuochao Yao, Yaqing Wang, Ye Yuan, Hongfei Xue, Chen Song, Xin Ma, Dimitrios Koutsonikolas, Wenyao Xu, Lu Su, Proc. of the 24th ACM MobiCom (MobiCom '18. of the 24th ACM MobiCom (MobiCom '18Wenjun Jiang, Chenglin Miao, Fenglong Ma, Shuochao Yao, Yaqing Wang, Ye Yuan, Hongfei Xue, Chen Song, Xin Ma, Dimitrios Koutsonikolas, Wenyao Xu, and Lu Su. 2018. Towards Environment Independent Device Free Human Activity Recognition. In Proc. of the 24th ACM MobiCom (MobiCom '18). 289-304.\n\nTowards 3D Human Pose Construction Using Wifi. Wenjun Jiang, Hongfei Xue, Chenglin Miao, Shiyang Wang, Sen Lin, Chong Tian, Srinivasan Murali, Haochen Hu, Zhi Sun, Lu Su, Proc. of the 26th ACM MobiCom. of the 26th ACM MobiComArticle 23, 14 pagesWenjun Jiang, Hongfei Xue, Chenglin Miao, Shiyang Wang, Sen Lin, Chong Tian, Srinivasan Murali, Haochen Hu, Zhi Sun, and Lu Su. 2020. Towards 3D Human Pose Construction Using Wifi. In Proc. of the 26th ACM MobiCom. Article 23, 14 pages.\n\nWiFinger: Talk to Your Smart Devices with Finger-Grained Gesture. Hong Li, Wei Yang, Jianxin Wang, Yang Xu, Liusheng Huang, Proc. of the ACM UbiComp (UbiComp '16. of the ACM UbiComp (UbiComp '16Hong Li, Wei Yang, Jianxin Wang, Yang Xu, and Liusheng Huang. 2016. WiFinger: Talk to Your Smart Devices with Finger-Grained Gesture. In Proc. of the ACM UbiComp (UbiComp '16). 250-261.\n\nUnsupervised Learning for Human Sensing Using Radio Signals. Tianhong Li, Lijie Fan, Yuan Yuan, Dina Katabi, Proc. of the IEEE/CVF WACV. of the IEEE/CVF WACVTianhong Li, Lijie Fan, Yuan Yuan, and Dina Katabi. 2022. Unsupervised Learning for Human Sensing Using Radio Signals. In Proc. of the IEEE/CVF WACV. 1091- 1100.\n\nShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design. Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, Jian Sun, Proc. of the ECCV. Cham. of the ECCV. ChamNingning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. 2018. ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design. In Proc. of the ECCV. Cham, 122-138.\n\nPyTorch: An Imperative Style, High-Performance Deep Learning Library. Adam Paszke, Sam Gross, Francisco Massa, Proc. of the 33rd NeurIPS. Article 721. of the 33rd NeurIPS. Article 72112Adam Paszke, Sam Gross, Francisco Massa, and et. al. 2019. PyTorch: An Impera- tive Style, High-Performance Deep Learning Library. In Proc. of the 33rd NeurIPS. Article 721, 12 pages.\n\nImproving Language Understanding by Generative Pre-Training. Alec Radford, Karthik Narasimhan, Alec Radford and Karthik Narasimhan. 2018. Improving Language Understanding by Generative Pre-Training.\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog (2019).\n\nSquig-gleMilli: Approximating SAR Imaging on Mobile Millimeter-Wave Devices. Hem Regmi, Sanjib Moh Sabbir Saadat, Srihari Sur, Nelakuditi, Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 526 pagesHem Regmi, Moh Sabbir Saadat, Sanjib Sur, and Srihari Nelakuditi. 2021. Squig- gleMilli: Approximating SAR Imaging on Mobile Millimeter-Wave Devices. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 5, 3, Article 125 (sep 2021), 26 pages.\n\nContrastive Multiview Coding. Yonglong Tian, Dilip Krishnan, Phillip Isola, Proc. of the 16th ECCV. of the 16th ECCVYonglong Tian, Dilip Krishnan, and Phillip Isola. 2020. Contrastive Multiview Coding. In Proc. of the 16th ECCV. 776-794.\n\nRF-Based Fall Monitoring Using Convolutional Neural Networks. Yonglong Tian, Guang-He Lee, Hao He, Chen-Yu Hsu, Dina Katabi, Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 224 pagesYonglong Tian, Guang-He Lee, Hao He, Chen-Yu Hsu, and Dina Katabi. 2018. RF-Based Fall Monitoring Using Convolutional Neural Networks. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 2, 3, Article 137 (sep 2018), 24 pages.\n\nAaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.03748Representation learning with contrastive predictive coding. arXiv preprintAaron Van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).\n\nWiFi based Multi-User Gesture Recognition. H Raghav, Shakir Venkatnarayan, Muhammad Mahmood, Shahzad, IEEE Transactions on Mobile Computing. 20Raghav H. Venkatnarayan, Shakir Mahmood, and Muhammad Shahzad. 2021. WiFi based Multi-User Gesture Recognition. IEEE Transactions on Mobile Com- puting 20, 3 (2021), 1242-1256.\n\nPosition and Orientation Agnostic Gesture Recognition Using WiFi. Aditya Virmani, Muhammad Shahzad, Proc. of the 15th ACM MobiSys (MobiSys '17. of the 15th ACM MobiSys (MobiSys '17Aditya Virmani and Muhammad Shahzad. 2017. Position and Orientation Agnos- tic Gesture Recognition Using WiFi. In Proc. of the 15th ACM MobiSys (MobiSys '17). 252-264.\n\nE-Eyes: Device-Free Location-Oriented Activity Identification Using Fine-Grained WiFi Signatures. Yan Wang, Jian Liu, Yingying Chen, Marco Gruteser, Jie Yang, Hongbo Liu, Proc. of the 20th ACM MobiCom (MobiCom '14. of the 20th ACM MobiCom (MobiCom '14Yan Wang, Jian Liu, Yingying Chen, Marco Gruteser, Jie Yang, and Hongbo Liu. 2014. E-Eyes: Device-Free Location-Oriented Activity Identification Using Fine-Grained WiFi Signatures. In Proc. of the 20th ACM MobiCom (MobiCom '14). 617-628.\n\nUnsupervised Feature Learning via Non-parametric Instance Discrimination. Zhirong Wu, Yuanjun Xiong, Stella X Yu, Dahua Lin, Proc. of the IEEE/CVF CVPR. of the IEEE/CVF CVPRZhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. 2018. Unsupervised Feature Learning via Non-parametric Instance Discrimination. In Proc. of the IEEE/CVF CVPR. 3733-3742.\n\nRadio Biometrics: Human Recognition Through a Wall. Qinyi Xu, Yan Chen, Beibei Wang, K J Ray, Liu, IEEE Transactions on Information Forensics and Security. 12Qinyi Xu, Yan Chen, BeiBei Wang, and K. J. Ray Liu. 2017. Radio Biometrics: Human Recognition Through a Wall. IEEE Transactions on Information Forensics and Security 12, 5 (2017), 1141-1155.\n\nTRIEDS: Wireless Events Detection Through the Wall. Qinyi Xu, Yan Chen, Beibei Wang, K J Ray, Liu, IEEE Internet of Things Journal. 4Qinyi Xu, Yan Chen, Beibei Wang, and K. J. Ray Liu. 2017. TRIEDS: Wireless Events Detection Through the Wall. IEEE Internet of Things Journal 4, 3 (2017), 723-735.\n\nRFGAN: RF-Based Human Synthesis. Cong Yu, Zhi Wu, Dongheng Zhang, Zhi Lu, Yang Hu, Yan Chen, IEEE Transactions on Multimedia. Cong Yu, Zhi Wu, Dongheng Zhang, Zhi Lu, Yang Hu, and Yan Chen. 2022. RFGAN: RF-Based Human Synthesis. IEEE Transactions on Multimedia (2022), 1-1.\n\nMTrack: Tracking Multiperson Moving Trajectories and Vital Signs With Radio Signals. Dongheng Zhang, Yang Hu, Yan Chen, IEEE Internet of Things Journal. 8Dongheng Zhang, Yang Hu, and Yan Chen. 2021. MTrack: Tracking Multiperson Moving Trajectories and Vital Signs With Radio Signals. IEEE Internet of Things Journal 8, 5 (2021), 3904-3914.\n\nBreathTrack: Tracking Indoor Human Breath Status via Commodity WiFi. Dongheng Zhang, Yang Hu, Yan Chen, Bing Zeng, IEEE Internet of Things Journal. 6Dongheng Zhang, Yang Hu, Yan Chen, and Bing Zeng. 2019. BreathTrack: Tracking Indoor Human Breath Status via Commodity WiFi. IEEE Internet of Things Journal 6, 2 (2019), 3899-3911.\n\nCalibrating Phase Offsets for Commodity WiFi. Dongheng Zhang, Yang Hu, Yan Chen, Bing Zeng, IEEE Systems Journal. 14Dongheng Zhang, Yang Hu, Yan Chen, and Bing Zeng. 2020. Calibrating Phase Offsets for Commodity WiFi. IEEE Systems Journal 14, 1 (2020), 661-664.\n\nYi Zhang, Yue Zheng, Kun Qian, Guidong Zhang, Yunhao Liu, Chenshu Wu, and Zheng Yang. 2021. Widar3.0: Zero-Effort Cross-Domain Gesture Recognition with Wi-Fi. IEEE Transactions on Pattern Analysis and Machine Intelligence. Yi Zhang, Yue Zheng, Kun Qian, Guidong Zhang, Yunhao Liu, Chenshu Wu, and Zheng Yang. 2021. Widar3.0: Zero-Effort Cross-Domain Gesture Recognition with Wi-Fi. IEEE Transactions on Pattern Analysis and Machine Intelligence (2021), 1-1.\n\nThrough-Wall Human Pose Estimation Using Radio Signals. Mingmin Zhao, Tianhong Li, Mohammad Abu Alsheikh, Yonglong Tian, Hang Zhao, Antonio Torralba, Dina Katabi, Proc. of the IEEE/CVF CVPR. of the IEEE/CVF CVPRMingmin Zhao, Tianhong Li, Mohammad Abu Alsheikh, Yonglong Tian, Hang Zhao, Antonio Torralba, and Dina Katabi. 2018. Through-Wall Human Pose Estimation Using Radio Signals. In Proc. of the IEEE/CVF CVPR. 7356-7365.\n\nThrough-Wall Human Mesh Recovery Using Radio Signals. Mingmin Zhao, Yingcheng Liu, Aniruddh Raghu, Hang Zhao, Tianhong Li, Antonio Torralba, Dina Katabi, Proc. of the IEEE/CVF ICCV. of the IEEE/CVF ICCVMingmin Zhao, Yingcheng Liu, Aniruddh Raghu, Hang Zhao, Tianhong Li, An- tonio Torralba, and Dina Katabi. 2019. Through-Wall Human Mesh Recovery Using Radio Signals. In Proc. of the IEEE/CVF ICCV. 10112-10121.\n\nRF-Based 3D Skeletons. Mingmin Zhao, Yonglong Tian, Hang Zhao, Mohammad Abu Alsheikh, Tianhong Li, Rumen Hristov, Zachary Kabelac, Dina Katabi, Antonio Torralba, Proc. of the ACM SIGCOMM (SIGCOMM '18. of the ACM SIGCOMM (SIGCOMM '18Mingmin Zhao, Yonglong Tian, Hang Zhao, Mohammad Abu Alsheikh, Tianhong Li, Rumen Hristov, Zachary Kabelac, Dina Katabi, and Antonio Torralba. 2018. RF-Based 3D Skeletons. In Proc. of the ACM SIGCOMM (SIGCOMM '18). 267-281.\n\nV2iFi: In-Vehicle Vital Sign Monitoring via Compact RF Sensing. Tianyue Zheng, Zhe Chen, Chao Cai, Jun Luo, Xu Zhang, Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 427 pagesTianyue Zheng, Zhe Chen, Chao Cai, Jun Luo, and Xu Zhang. 2020. V2iFi: In- Vehicle Vital Sign Monitoring via Compact RF Sensing. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 4, 2, Article 70 (jun 2020), 27 pages.\n", "annotations": {"author": "[{\"end\":427,\"start\":111},{\"end\":746,\"start\":428},{\"end\":1057,\"start\":747},{\"end\":1369,\"start\":1058},{\"end\":1686,\"start\":1370},{\"end\":2001,\"start\":1687},{\"end\":2313,\"start\":2002},{\"end\":2626,\"start\":2314},{\"end\":2943,\"start\":2627},{\"end\":3262,\"start\":2944},{\"end\":3573,\"start\":3263},{\"end\":3885,\"start\":3574},{\"end\":4202,\"start\":3886},{\"end\":4517,\"start\":4203},{\"end\":4829,\"start\":4518},{\"end\":5142,\"start\":4830},{\"end\":5455,\"start\":5143}]", "publisher": "[{\"end\":56,\"start\":53},{\"end\":5671,\"start\":5668}]", "author_last_name": "[{\"end\":123,\"start\":119},{\"end\":442,\"start\":437},{\"end\":753,\"start\":751},{\"end\":1065,\"start\":1063},{\"end\":1382,\"start\":1379},{\"end\":1697,\"start\":1693},{\"end\":2009,\"start\":2007},{\"end\":2322,\"start\":2318},{\"end\":2639,\"start\":2635},{\"end\":2958,\"start\":2953},{\"end\":3269,\"start\":3267},{\"end\":3581,\"start\":3579},{\"end\":3898,\"start\":3895},{\"end\":4213,\"start\":4209},{\"end\":4525,\"start\":4523},{\"end\":5151,\"start\":5147}]", "author_first_name": "[{\"end\":118,\"start\":111},{\"end\":436,\"start\":428},{\"end\":750,\"start\":747},{\"end\":1062,\"start\":1058},{\"end\":1378,\"start\":1370},{\"end\":1692,\"start\":1687},{\"end\":2006,\"start\":2002},{\"end\":2317,\"start\":2314},{\"end\":2634,\"start\":2627},{\"end\":2952,\"start\":2944},{\"end\":3266,\"start\":3263},{\"end\":3578,\"start\":3574},{\"end\":3894,\"start\":3886},{\"end\":4208,\"start\":4203},{\"end\":4522,\"start\":4518},{\"end\":4833,\"start\":4830},{\"end\":4838,\"start\":4834},{\"end\":5146,\"start\":5143}]", "author_affiliation": "[{\"end\":426,\"start\":125},{\"end\":745,\"start\":444},{\"end\":1056,\"start\":755},{\"end\":1368,\"start\":1067},{\"end\":1685,\"start\":1384},{\"end\":2000,\"start\":1699},{\"end\":2312,\"start\":2011},{\"end\":2625,\"start\":2324},{\"end\":2942,\"start\":2641},{\"end\":3261,\"start\":2960},{\"end\":3572,\"start\":3271},{\"end\":3884,\"start\":3583},{\"end\":4201,\"start\":3900},{\"end\":4516,\"start\":4215},{\"end\":4828,\"start\":4527},{\"end\":5141,\"start\":4840},{\"end\":5454,\"start\":5153}]", "title": "[{\"end\":52,\"start\":1},{\"end\":5507,\"start\":5456}]", "venue": "[{\"end\":5602,\"start\":5509}]", "abstract": "[{\"end\":7421,\"start\":5846}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7701,\"start\":7698},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7727,\"start\":7724},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8015,\"start\":8011},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8043,\"start\":8039},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8067,\"start\":8063},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":8855,\"start\":8851},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8858,\"start\":8855},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8882,\"start\":8878},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9216,\"start\":9213},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9300,\"start\":9296},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9303,\"start\":9300},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9871,\"start\":9867},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":10669,\"start\":10665},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":10672,\"start\":10669},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10711,\"start\":10707},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16077,\"start\":16073},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18426,\"start\":18422},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":18612,\"start\":18608},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":18779,\"start\":18775},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19531,\"start\":19527},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":23155,\"start\":23151},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":23158,\"start\":23155},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":23965,\"start\":23961},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25644,\"start\":25640},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":26895,\"start\":26891},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":27702,\"start\":27698},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28273,\"start\":28269},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":29336,\"start\":29332},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":34053,\"start\":34049},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":35765,\"start\":35761},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":35981,\"start\":35977},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":36528,\"start\":36524},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":36648,\"start\":36644},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":37284,\"start\":37280},{\"end\":37643,\"start\":37612},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":39136,\"start\":39132},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":39139,\"start\":39136},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":40158,\"start\":40156},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":40355,\"start\":40354},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":43081,\"start\":43080},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":43360,\"start\":43356},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":43627,\"start\":43625},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":44433,\"start\":44429},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":45234,\"start\":45230},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":46115,\"start\":46111},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":48405,\"start\":48401},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":48408,\"start\":48405},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":48411,\"start\":48408},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":48414,\"start\":48411},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":48417,\"start\":48414},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":48444,\"start\":48440},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":48447,\"start\":48444},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":48450,\"start\":48447},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":48453,\"start\":48450},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":48456,\"start\":48453},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":48459,\"start\":48456},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":48487,\"start\":48483},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":48491,\"start\":48487},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":48495,\"start\":48491},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":48499,\"start\":48495},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":48530,\"start\":48526},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":48533,\"start\":48530},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":48554,\"start\":48550},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":48582,\"start\":48578},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":48585,\"start\":48582},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":48589,\"start\":48585},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":48593,\"start\":48589},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":48597,\"start\":48593},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":48600,\"start\":48597},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":48914,\"start\":48911},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":48917,\"start\":48914},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":48920,\"start\":48917},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":48934,\"start\":48930},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":49576,\"start\":49573},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":49579,\"start\":49576},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":49793,\"start\":49789},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":49803,\"start\":49800},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":49806,\"start\":49803},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":49850,\"start\":49846},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":49884,\"start\":49881},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":49913,\"start\":49910},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":49946,\"start\":49942},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":49959,\"start\":49956},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":50009,\"start\":50005},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":50025,\"start\":50022},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":50091,\"start\":50088},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":50093,\"start\":50091},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":50096,\"start\":50093},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":50189,\"start\":50185},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":50231,\"start\":50227},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":51266,\"start\":51262},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":51269,\"start\":51266},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":56276,\"start\":56272}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":54731,\"start\":54659},{\"attributes\":{\"id\":\"fig_2\"},\"end\":54785,\"start\":54732},{\"attributes\":{\"id\":\"fig_3\"},\"end\":54895,\"start\":54786},{\"attributes\":{\"id\":\"fig_4\"},\"end\":55384,\"start\":54896},{\"attributes\":{\"id\":\"fig_5\"},\"end\":55524,\"start\":55385},{\"attributes\":{\"id\":\"fig_6\"},\"end\":55837,\"start\":55525},{\"attributes\":{\"id\":\"fig_7\"},\"end\":55895,\"start\":55838},{\"attributes\":{\"id\":\"fig_8\"},\"end\":56027,\"start\":55896},{\"attributes\":{\"id\":\"fig_9\"},\"end\":56471,\"start\":56028},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":56929,\"start\":56472},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":57600,\"start\":56930},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":57969,\"start\":57601},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":58269,\"start\":57970},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":58837,\"start\":58270},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":59072,\"start\":58838},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":59220,\"start\":59073},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":59568,\"start\":59221},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":59576,\"start\":59569},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":59867,\"start\":59577},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":60013,\"start\":59868},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":60431,\"start\":60014},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":60648,\"start\":60432},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":60783,\"start\":60649},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":61140,\"start\":60784},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":61765,\"start\":61141},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":62439,\"start\":61766},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":62920,\"start\":62440},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":63452,\"start\":62921},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":63823,\"start\":63453}]", "paragraph": "[{\"end\":8267,\"start\":7437},{\"end\":9054,\"start\":8269},{\"end\":10233,\"start\":9056},{\"end\":11063,\"start\":10235},{\"end\":11906,\"start\":11065},{\"end\":12441,\"start\":11908},{\"end\":12676,\"start\":12443},{\"end\":14056,\"start\":12678},{\"end\":15621,\"start\":14067},{\"end\":16323,\"start\":15647},{\"end\":16590,\"start\":16325},{\"end\":16932,\"start\":16629},{\"end\":17677,\"start\":16934},{\"end\":18074,\"start\":17737},{\"end\":18227,\"start\":18144},{\"end\":18658,\"start\":18229},{\"end\":18968,\"start\":18739},{\"end\":19058,\"start\":19021},{\"end\":19448,\"start\":19082},{\"end\":20243,\"start\":19464},{\"end\":20470,\"start\":20245},{\"end\":20578,\"start\":20472},{\"end\":20907,\"start\":20599},{\"end\":21675,\"start\":20909},{\"end\":22458,\"start\":21697},{\"end\":22763,\"start\":22460},{\"end\":23178,\"start\":22796},{\"end\":23315,\"start\":23216},{\"end\":23927,\"start\":23317},{\"end\":24121,\"start\":23936},{\"end\":24433,\"start\":24123},{\"end\":24513,\"start\":24452},{\"end\":24930,\"start\":24515},{\"end\":25625,\"start\":24974},{\"end\":26409,\"start\":25627},{\"end\":27488,\"start\":26451},{\"end\":27644,\"start\":27490},{\"end\":28560,\"start\":27646},{\"end\":29131,\"start\":28623},{\"end\":29861,\"start\":29133},{\"end\":31483,\"start\":29863},{\"end\":32026,\"start\":31485},{\"end\":32771,\"start\":32028},{\"end\":32961,\"start\":32827},{\"end\":35149,\"start\":32963},{\"end\":35711,\"start\":35151},{\"end\":36253,\"start\":35713},{\"end\":37174,\"start\":36294},{\"end\":37457,\"start\":37208},{\"end\":37839,\"start\":37459},{\"end\":38011,\"start\":37841},{\"end\":38394,\"start\":38087},{\"end\":38722,\"start\":38396},{\"end\":39140,\"start\":38764},{\"end\":39274,\"start\":39162},{\"end\":39534,\"start\":39276},{\"end\":40260,\"start\":39593},{\"end\":42200,\"start\":40262},{\"end\":42615,\"start\":42244},{\"end\":43554,\"start\":42629},{\"end\":43637,\"start\":43556},{\"end\":44135,\"start\":43639},{\"end\":45002,\"start\":44179},{\"end\":45463,\"start\":45015},{\"end\":45756,\"start\":45465},{\"end\":46245,\"start\":45758},{\"end\":46375,\"start\":46267},{\"end\":46601,\"start\":46377},{\"end\":48216,\"start\":46669},{\"end\":48793,\"start\":48264},{\"end\":49336,\"start\":48795},{\"end\":50190,\"start\":49338},{\"end\":51146,\"start\":50192},{\"end\":51674,\"start\":51148},{\"end\":52580,\"start\":51690},{\"end\":53170,\"start\":52582},{\"end\":54137,\"start\":53172},{\"end\":54658,\"start\":54152}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18143,\"start\":18075},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18738,\"start\":18659},{\"attributes\":{\"id\":\"formula_2\"},\"end\":19020,\"start\":18969},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19081,\"start\":19059},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22795,\"start\":22764},{\"attributes\":{\"id\":\"formula_6\"},\"end\":23215,\"start\":23179},{\"attributes\":{\"id\":\"formula_7\"},\"end\":24973,\"start\":24931},{\"attributes\":{\"id\":\"formula_8\"},\"end\":38086,\"start\":38012},{\"attributes\":{\"id\":\"formula_9\"},\"end\":38763,\"start\":38723}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":28743,\"start\":28737},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":29144,\"start\":29138},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":29520,\"start\":29514},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":31521,\"start\":31515},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":32177,\"start\":32171},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":32545,\"start\":32537},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":33188,\"start\":33182},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":34533,\"start\":34527},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":34813,\"start\":34807},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":36103,\"start\":36097},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":39691,\"start\":39685},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":40058,\"start\":40052},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":40707,\"start\":40701},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":40970,\"start\":40962},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":41235,\"start\":41229},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":41505,\"start\":41499},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":41777,\"start\":41771},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":42336,\"start\":42327},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":42897,\"start\":42891},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":43511,\"start\":43505},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":43727,\"start\":43719},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":43950,\"start\":43944},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":46776,\"start\":46770},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":46823,\"start\":46817},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":47057,\"start\":47051},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":47328,\"start\":47322},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":47676,\"start\":47670},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":47871,\"start\":47865}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":7435,\"start\":7423},{\"attributes\":{\"n\":\"2\"},\"end\":14065,\"start\":14059},{\"attributes\":{\"n\":\"2.1\"},\"end\":15645,\"start\":15624},{\"attributes\":{\"n\":\"2.2\"},\"end\":16627,\"start\":16593},{\"end\":17705,\"start\":17680},{\"attributes\":{\"n\":\"2.3\"},\"end\":17735,\"start\":17708},{\"attributes\":{\"n\":\"2.4\"},\"end\":19462,\"start\":19451},{\"attributes\":{\"n\":\"3\"},\"end\":20597,\"start\":20581},{\"attributes\":{\"n\":\"3.1\"},\"end\":21695,\"start\":21678},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":23934,\"start\":23930},{\"attributes\":{\"n\":\"3.2\"},\"end\":24450,\"start\":24436},{\"attributes\":{\"n\":\"4\"},\"end\":26449,\"start\":26412},{\"attributes\":{\"n\":\"4.2\"},\"end\":28571,\"start\":28563},{\"attributes\":{\"n\":\"4.3\"},\"end\":28594,\"start\":28574},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":28621,\"start\":28597},{\"attributes\":{\"n\":\"4.3.3\"},\"end\":32813,\"start\":32774},{\"attributes\":{\"n\":\"4.3.4\"},\"end\":32825,\"start\":32816},{\"attributes\":{\"n\":\"5\"},\"end\":36292,\"start\":36256},{\"attributes\":{\"n\":\"5.2\"},\"end\":37185,\"start\":37177},{\"attributes\":{\"n\":\"5.2.1\"},\"end\":37206,\"start\":37188},{\"attributes\":{\"n\":\"5.2.2\"},\"end\":39160,\"start\":39143},{\"attributes\":{\"n\":\"5.3\"},\"end\":39557,\"start\":39537},{\"attributes\":{\"n\":\"5.3.1\"},\"end\":39591,\"start\":39560},{\"attributes\":{\"n\":\"5.3.3\"},\"end\":42242,\"start\":42203},{\"attributes\":{\"n\":\"5.3.4\"},\"end\":42627,\"start\":42618},{\"attributes\":{\"n\":\"6\"},\"end\":44177,\"start\":44138},{\"attributes\":{\"n\":\"6.2\"},\"end\":45013,\"start\":45005},{\"attributes\":{\"n\":\"6.2.2\"},\"end\":46265,\"start\":46248},{\"attributes\":{\"n\":\"6.3\"},\"end\":46624,\"start\":46604},{\"attributes\":{\"n\":\"6.3.1\"},\"end\":46667,\"start\":46627},{\"attributes\":{\"n\":\"6.3.2\"},\"end\":48247,\"start\":48219},{\"attributes\":{\"n\":\"7\"},\"end\":48262,\"start\":48250},{\"attributes\":{\"n\":\"8\"},\"end\":51688,\"start\":51677},{\"attributes\":{\"n\":\"9\"},\"end\":54150,\"start\":54140},{\"end\":54670,\"start\":54660},{\"end\":54743,\"start\":54733},{\"end\":54797,\"start\":54787},{\"end\":55396,\"start\":55386},{\"end\":55849,\"start\":55839},{\"end\":55907,\"start\":55897},{\"end\":56940,\"start\":56931},{\"end\":57611,\"start\":57602},{\"end\":57978,\"start\":57971},{\"end\":58280,\"start\":58271},{\"end\":58848,\"start\":58839},{\"end\":59083,\"start\":59074},{\"end\":59231,\"start\":59222},{\"end\":59575,\"start\":59570},{\"end\":59587,\"start\":59578},{\"end\":59878,\"start\":59869},{\"end\":60025,\"start\":60015},{\"end\":60443,\"start\":60433},{\"end\":60660,\"start\":60650},{\"end\":60795,\"start\":60785},{\"end\":61152,\"start\":61142},{\"end\":61777,\"start\":61767},{\"end\":62451,\"start\":62441},{\"end\":62932,\"start\":62922},{\"end\":63464,\"start\":63454}]", "table": "[{\"end\":57600,\"start\":57020},{\"end\":57969,\"start\":57741},{\"end\":58269,\"start\":57980},{\"end\":58837,\"start\":58409},{\"end\":59072,\"start\":58930},{\"end\":59220,\"start\":59126},{\"end\":59568,\"start\":59328},{\"end\":59867,\"start\":59747},{\"end\":60013,\"start\":59914},{\"end\":60431,\"start\":60168},{\"end\":60648,\"start\":60541},{\"end\":60783,\"start\":60704},{\"end\":61140,\"start\":61075},{\"end\":61765,\"start\":61366},{\"end\":62439,\"start\":62301},{\"end\":62920,\"start\":62588},{\"end\":63452,\"start\":63305},{\"end\":63823,\"start\":63657}]", "figure_caption": "[{\"end\":54731,\"start\":54672},{\"end\":54785,\"start\":54745},{\"end\":54895,\"start\":54799},{\"end\":55384,\"start\":54898},{\"end\":55524,\"start\":55398},{\"end\":55837,\"start\":55527},{\"end\":55895,\"start\":55851},{\"end\":56027,\"start\":55909},{\"end\":56471,\"start\":56030},{\"end\":56929,\"start\":56474},{\"end\":57020,\"start\":56942},{\"end\":57741,\"start\":57613},{\"end\":58409,\"start\":58282},{\"end\":58930,\"start\":58850},{\"end\":59126,\"start\":59085},{\"end\":59328,\"start\":59233},{\"end\":59747,\"start\":59589},{\"end\":59914,\"start\":59880},{\"end\":60168,\"start\":60028},{\"end\":60541,\"start\":60446},{\"end\":60704,\"start\":60663},{\"end\":61075,\"start\":60798},{\"end\":61366,\"start\":61155},{\"end\":62301,\"start\":61780},{\"end\":62588,\"start\":62454},{\"end\":63305,\"start\":62935},{\"end\":63657,\"start\":63467}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14388,\"start\":14380},{\"end\":17223,\"start\":17216},{\"end\":19621,\"start\":19614},{\"end\":20618,\"start\":20611},{\"end\":21934,\"start\":21927},{\"end\":29906,\"start\":29899},{\"end\":36418,\"start\":36411},{\"end\":39705,\"start\":39698},{\"end\":46791,\"start\":46784},{\"end\":48095,\"start\":48088}]", "bib_author_first_name": "[{\"end\":63872,\"start\":63865},{\"end\":63889,\"start\":63885},{\"end\":63905,\"start\":63899},{\"end\":63907,\"start\":63906},{\"end\":64204,\"start\":64198},{\"end\":64224,\"start\":64217},{\"end\":64234,\"start\":64230},{\"end\":64250,\"start\":64244},{\"end\":64252,\"start\":64251},{\"end\":64578,\"start\":64572},{\"end\":64592,\"start\":64587},{\"end\":64610,\"start\":64604},{\"end\":64931,\"start\":64928},{\"end\":64947,\"start\":64939},{\"end\":64958,\"start\":64954},{\"end\":65273,\"start\":65265},{\"end\":65286,\"start\":65281},{\"end\":65300,\"start\":65294},{\"end\":65314,\"start\":65309},{\"end\":65327,\"start\":65322},{\"end\":65346,\"start\":65340},{\"end\":65733,\"start\":65725},{\"end\":65745,\"start\":65741},{\"end\":65760,\"start\":65755},{\"end\":65773,\"start\":65768},{\"end\":65787,\"start\":65781},{\"end\":65801,\"start\":65796},{\"end\":65820,\"start\":65814},{\"end\":66169,\"start\":66165},{\"end\":66181,\"start\":66176},{\"end\":66201,\"start\":66193},{\"end\":66219,\"start\":66211},{\"end\":66520,\"start\":66514},{\"end\":66532,\"start\":66527},{\"end\":66873,\"start\":66867},{\"end\":66887,\"start\":66880},{\"end\":67143,\"start\":67137},{\"end\":67157,\"start\":67150},{\"end\":67170,\"start\":67163},{\"end\":67439,\"start\":67436},{\"end\":67452,\"start\":67446},{\"end\":67467,\"start\":67459},{\"end\":67479,\"start\":67475},{\"end\":67772,\"start\":67769},{\"end\":67784,\"start\":67779},{\"end\":67793,\"start\":67789},{\"end\":67802,\"start\":67798},{\"end\":68136,\"start\":68133},{\"end\":68150,\"start\":68143},{\"end\":68162,\"start\":68158},{\"end\":68533,\"start\":68528},{\"end\":68550,\"start\":68542},{\"end\":68564,\"start\":68558},{\"end\":68578,\"start\":68570},{\"end\":68904,\"start\":68899},{\"end\":68914,\"start\":68911},{\"end\":68928,\"start\":68921},{\"end\":69272,\"start\":69267},{\"end\":69286,\"start\":69278},{\"end\":69298,\"start\":69291},{\"end\":69310,\"start\":69305},{\"end\":69324,\"start\":69320},{\"end\":69335,\"start\":69331},{\"end\":69659,\"start\":69653},{\"end\":69681,\"start\":69674},{\"end\":69691,\"start\":69687},{\"end\":69946,\"start\":69937},{\"end\":69967,\"start\":69962},{\"end\":69981,\"start\":69973},{\"end\":69996,\"start\":69989},{\"end\":70278,\"start\":70271},{\"end\":70292,\"start\":70286},{\"end\":70555,\"start\":70543},{\"end\":70570,\"start\":70563},{\"end\":70585,\"start\":70578},{\"end\":70602,\"start\":70594},{\"end\":70617,\"start\":70611},{\"end\":70619,\"start\":70618},{\"end\":70636,\"start\":70631},{\"end\":70654,\"start\":70650},{\"end\":70672,\"start\":70664},{\"end\":70693,\"start\":70686},{\"end\":70700,\"start\":70694},{\"end\":70714,\"start\":70706},{\"end\":70725,\"start\":70715},{\"end\":70737,\"start\":70732},{\"end\":70749,\"start\":70744},{\"end\":70767,\"start\":70763},{\"end\":70781,\"start\":70775},{\"end\":71343,\"start\":71336},{\"end\":71356,\"start\":71350},{\"end\":71370,\"start\":71365},{\"end\":71383,\"start\":71376},{\"end\":71398,\"start\":71391},{\"end\":71721,\"start\":71720},{\"end\":71732,\"start\":71731},{\"end\":71742,\"start\":71741},{\"end\":72032,\"start\":72025},{\"end\":72042,\"start\":72037},{\"end\":72053,\"start\":72048},{\"end\":72065,\"start\":72058},{\"end\":72075,\"start\":72071},{\"end\":72336,\"start\":72329},{\"end\":72348,\"start\":72341},{\"end\":72364,\"start\":72359},{\"end\":72377,\"start\":72373},{\"end\":72613,\"start\":72606},{\"end\":72625,\"start\":72618},{\"end\":72641,\"start\":72633},{\"end\":72651,\"start\":72647},{\"end\":72937,\"start\":72933},{\"end\":72945,\"start\":72942},{\"end\":72956,\"start\":72952},{\"end\":72965,\"start\":72961},{\"end\":73282,\"start\":73275},{\"end\":73293,\"start\":73288},{\"end\":73311,\"start\":73303},{\"end\":73324,\"start\":73317},{\"end\":73335,\"start\":73331},{\"end\":73693,\"start\":73687},{\"end\":73709,\"start\":73701},{\"end\":73724,\"start\":73716},{\"end\":73737,\"start\":73729},{\"end\":73749,\"start\":73743},{\"end\":73758,\"start\":73756},{\"end\":73772,\"start\":73765},{\"end\":73782,\"start\":73778},{\"end\":73792,\"start\":73789},{\"end\":73806,\"start\":73797},{\"end\":73828,\"start\":73822},{\"end\":73835,\"start\":73833},{\"end\":74291,\"start\":74285},{\"end\":74307,\"start\":74299},{\"end\":74322,\"start\":74314},{\"end\":74335,\"start\":74327},{\"end\":74347,\"start\":74341},{\"end\":74356,\"start\":74354},{\"end\":74370,\"start\":74363},{\"end\":74380,\"start\":74376},{\"end\":74390,\"start\":74387},{\"end\":74404,\"start\":74395},{\"end\":74426,\"start\":74420},{\"end\":74433,\"start\":74431},{\"end\":74864,\"start\":74858},{\"end\":74879,\"start\":74872},{\"end\":74893,\"start\":74885},{\"end\":74907,\"start\":74900},{\"end\":74917,\"start\":74914},{\"end\":74928,\"start\":74923},{\"end\":74945,\"start\":74935},{\"end\":74961,\"start\":74954},{\"end\":74969,\"start\":74966},{\"end\":74977,\"start\":74975},{\"end\":75364,\"start\":75360},{\"end\":75372,\"start\":75369},{\"end\":75386,\"start\":75379},{\"end\":75397,\"start\":75393},{\"end\":75410,\"start\":75402},{\"end\":75744,\"start\":75736},{\"end\":75754,\"start\":75749},{\"end\":75764,\"start\":75760},{\"end\":75775,\"start\":75771},{\"end\":76078,\"start\":76070},{\"end\":76090,\"start\":76083},{\"end\":76105,\"start\":76098},{\"end\":76117,\"start\":76113},{\"end\":76415,\"start\":76411},{\"end\":76427,\"start\":76424},{\"end\":76444,\"start\":76435},{\"end\":76776,\"start\":76772},{\"end\":76793,\"start\":76786},{\"end\":76968,\"start\":76964},{\"end\":76985,\"start\":76978},{\"end\":76995,\"start\":76990},{\"end\":77008,\"start\":77003},{\"end\":77020,\"start\":77015},{\"end\":77033,\"start\":77029},{\"end\":77306,\"start\":77303},{\"end\":77320,\"start\":77314},{\"end\":77347,\"start\":77340},{\"end\":77711,\"start\":77703},{\"end\":77723,\"start\":77718},{\"end\":77741,\"start\":77734},{\"end\":77982,\"start\":77974},{\"end\":77997,\"start\":77989},{\"end\":78006,\"start\":78003},{\"end\":78018,\"start\":78011},{\"end\":78028,\"start\":78024},{\"end\":78335,\"start\":78330},{\"end\":78355,\"start\":78350},{\"end\":78365,\"start\":78360},{\"end\":78665,\"start\":78664},{\"end\":78680,\"start\":78674},{\"end\":78704,\"start\":78696},{\"end\":79014,\"start\":79008},{\"end\":79032,\"start\":79024},{\"end\":79392,\"start\":79389},{\"end\":79403,\"start\":79399},{\"end\":79417,\"start\":79409},{\"end\":79429,\"start\":79424},{\"end\":79443,\"start\":79440},{\"end\":79456,\"start\":79450},{\"end\":79862,\"start\":79855},{\"end\":79874,\"start\":79867},{\"end\":79888,\"start\":79882},{\"end\":79890,\"start\":79889},{\"end\":79900,\"start\":79895},{\"end\":80190,\"start\":80185},{\"end\":80198,\"start\":80195},{\"end\":80211,\"start\":80205},{\"end\":80219,\"start\":80218},{\"end\":80221,\"start\":80220},{\"end\":80540,\"start\":80535},{\"end\":80548,\"start\":80545},{\"end\":80561,\"start\":80555},{\"end\":80569,\"start\":80568},{\"end\":80571,\"start\":80570},{\"end\":80818,\"start\":80814},{\"end\":80826,\"start\":80823},{\"end\":80839,\"start\":80831},{\"end\":80850,\"start\":80847},{\"end\":80859,\"start\":80855},{\"end\":80867,\"start\":80864},{\"end\":81149,\"start\":81141},{\"end\":81161,\"start\":81157},{\"end\":81169,\"start\":81166},{\"end\":81474,\"start\":81466},{\"end\":81486,\"start\":81482},{\"end\":81494,\"start\":81491},{\"end\":81505,\"start\":81501},{\"end\":81782,\"start\":81774},{\"end\":81794,\"start\":81790},{\"end\":81802,\"start\":81799},{\"end\":81813,\"start\":81809},{\"end\":81993,\"start\":81991},{\"end\":82004,\"start\":82001},{\"end\":82015,\"start\":82012},{\"end\":82029,\"start\":82022},{\"end\":82043,\"start\":82037},{\"end\":82513,\"start\":82506},{\"end\":82528,\"start\":82520},{\"end\":82541,\"start\":82533},{\"end\":82564,\"start\":82556},{\"end\":82575,\"start\":82571},{\"end\":82589,\"start\":82582},{\"end\":82604,\"start\":82600},{\"end\":82938,\"start\":82931},{\"end\":82954,\"start\":82945},{\"end\":82968,\"start\":82960},{\"end\":82980,\"start\":82976},{\"end\":82995,\"start\":82987},{\"end\":83007,\"start\":83000},{\"end\":83022,\"start\":83018},{\"end\":83320,\"start\":83313},{\"end\":83335,\"start\":83327},{\"end\":83346,\"start\":83342},{\"end\":83361,\"start\":83353},{\"end\":83384,\"start\":83376},{\"end\":83394,\"start\":83389},{\"end\":83411,\"start\":83404},{\"end\":83425,\"start\":83421},{\"end\":83441,\"start\":83434},{\"end\":83818,\"start\":83811},{\"end\":83829,\"start\":83826},{\"end\":83840,\"start\":83836},{\"end\":83849,\"start\":83846},{\"end\":83857,\"start\":83855}]", "bib_author_last_name": "[{\"end\":63883,\"start\":63873},{\"end\":63897,\"start\":63890},{\"end\":63914,\"start\":63908},{\"end\":63922,\"start\":63916},{\"end\":64215,\"start\":64205},{\"end\":64228,\"start\":64225},{\"end\":64242,\"start\":64235},{\"end\":64259,\"start\":64253},{\"end\":64267,\"start\":64261},{\"end\":64585,\"start\":64579},{\"end\":64602,\"start\":64593},{\"end\":64618,\"start\":64611},{\"end\":64937,\"start\":64932},{\"end\":64952,\"start\":64948},{\"end\":64964,\"start\":64959},{\"end\":65279,\"start\":65274},{\"end\":65292,\"start\":65287},{\"end\":65307,\"start\":65301},{\"end\":65320,\"start\":65315},{\"end\":65338,\"start\":65328},{\"end\":65353,\"start\":65347},{\"end\":65739,\"start\":65734},{\"end\":65753,\"start\":65746},{\"end\":65766,\"start\":65761},{\"end\":65779,\"start\":65774},{\"end\":65794,\"start\":65788},{\"end\":65812,\"start\":65802},{\"end\":65827,\"start\":65821},{\"end\":66174,\"start\":66170},{\"end\":66191,\"start\":66182},{\"end\":66209,\"start\":66202},{\"end\":66226,\"start\":66220},{\"end\":66525,\"start\":66521},{\"end\":66536,\"start\":66533},{\"end\":66878,\"start\":66874},{\"end\":66890,\"start\":66888},{\"end\":67148,\"start\":67144},{\"end\":67161,\"start\":67158},{\"end\":67173,\"start\":67171},{\"end\":67444,\"start\":67440},{\"end\":67457,\"start\":67453},{\"end\":67473,\"start\":67468},{\"end\":67482,\"start\":67480},{\"end\":67777,\"start\":67773},{\"end\":67787,\"start\":67785},{\"end\":67796,\"start\":67794},{\"end\":67807,\"start\":67803},{\"end\":68141,\"start\":68137},{\"end\":68156,\"start\":68151},{\"end\":68166,\"start\":68163},{\"end\":68540,\"start\":68534},{\"end\":68556,\"start\":68551},{\"end\":68568,\"start\":68565},{\"end\":68588,\"start\":68579},{\"end\":68909,\"start\":68905},{\"end\":68919,\"start\":68915},{\"end\":68934,\"start\":68929},{\"end\":69276,\"start\":69273},{\"end\":69289,\"start\":69287},{\"end\":69303,\"start\":69299},{\"end\":69318,\"start\":69311},{\"end\":69329,\"start\":69325},{\"end\":69342,\"start\":69336},{\"end\":69672,\"start\":69660},{\"end\":69685,\"start\":69682},{\"end\":69695,\"start\":69692},{\"end\":69699,\"start\":69697},{\"end\":69960,\"start\":69947},{\"end\":69971,\"start\":69968},{\"end\":69987,\"start\":69982},{\"end\":69999,\"start\":69997},{\"end\":70284,\"start\":70279},{\"end\":70299,\"start\":70293},{\"end\":70561,\"start\":70556},{\"end\":70576,\"start\":70571},{\"end\":70592,\"start\":70586},{\"end\":70609,\"start\":70603},{\"end\":70629,\"start\":70620},{\"end\":70648,\"start\":70637},{\"end\":70662,\"start\":70655},{\"end\":70684,\"start\":70673},{\"end\":70704,\"start\":70701},{\"end\":70730,\"start\":70726},{\"end\":70742,\"start\":70738},{\"end\":70761,\"start\":70750},{\"end\":70773,\"start\":70768},{\"end\":70787,\"start\":70782},{\"end\":71348,\"start\":71344},{\"end\":71363,\"start\":71357},{\"end\":71374,\"start\":71371},{\"end\":71389,\"start\":71384},{\"end\":71408,\"start\":71399},{\"end\":71729,\"start\":71722},{\"end\":71739,\"start\":71733},{\"end\":71748,\"start\":71743},{\"end\":72035,\"start\":72033},{\"end\":72046,\"start\":72043},{\"end\":72056,\"start\":72054},{\"end\":72069,\"start\":72066},{\"end\":72084,\"start\":72076},{\"end\":72339,\"start\":72337},{\"end\":72357,\"start\":72349},{\"end\":72371,\"start\":72365},{\"end\":72386,\"start\":72378},{\"end\":72616,\"start\":72614},{\"end\":72631,\"start\":72626},{\"end\":72645,\"start\":72642},{\"end\":72655,\"start\":72652},{\"end\":72940,\"start\":72938},{\"end\":72950,\"start\":72946},{\"end\":72959,\"start\":72957},{\"end\":72970,\"start\":72966},{\"end\":73286,\"start\":73283},{\"end\":73301,\"start\":73294},{\"end\":73315,\"start\":73312},{\"end\":73329,\"start\":73325},{\"end\":73342,\"start\":73336},{\"end\":73699,\"start\":73694},{\"end\":73714,\"start\":73710},{\"end\":73727,\"start\":73725},{\"end\":73741,\"start\":73738},{\"end\":73754,\"start\":73750},{\"end\":73763,\"start\":73759},{\"end\":73776,\"start\":73773},{\"end\":73787,\"start\":73783},{\"end\":73795,\"start\":73793},{\"end\":73820,\"start\":73807},{\"end\":73831,\"start\":73829},{\"end\":73838,\"start\":73836},{\"end\":74297,\"start\":74292},{\"end\":74312,\"start\":74308},{\"end\":74325,\"start\":74323},{\"end\":74339,\"start\":74336},{\"end\":74352,\"start\":74348},{\"end\":74361,\"start\":74357},{\"end\":74374,\"start\":74371},{\"end\":74385,\"start\":74381},{\"end\":74393,\"start\":74391},{\"end\":74418,\"start\":74405},{\"end\":74429,\"start\":74427},{\"end\":74436,\"start\":74434},{\"end\":74870,\"start\":74865},{\"end\":74883,\"start\":74880},{\"end\":74898,\"start\":74894},{\"end\":74912,\"start\":74908},{\"end\":74921,\"start\":74918},{\"end\":74933,\"start\":74929},{\"end\":74952,\"start\":74946},{\"end\":74964,\"start\":74962},{\"end\":74973,\"start\":74970},{\"end\":74980,\"start\":74978},{\"end\":75367,\"start\":75365},{\"end\":75377,\"start\":75373},{\"end\":75391,\"start\":75387},{\"end\":75400,\"start\":75398},{\"end\":75416,\"start\":75411},{\"end\":75747,\"start\":75745},{\"end\":75758,\"start\":75755},{\"end\":75769,\"start\":75765},{\"end\":75782,\"start\":75776},{\"end\":76081,\"start\":76079},{\"end\":76096,\"start\":76091},{\"end\":76111,\"start\":76106},{\"end\":76121,\"start\":76118},{\"end\":76422,\"start\":76416},{\"end\":76433,\"start\":76428},{\"end\":76450,\"start\":76445},{\"end\":76784,\"start\":76777},{\"end\":76804,\"start\":76794},{\"end\":76976,\"start\":76969},{\"end\":76988,\"start\":76986},{\"end\":77001,\"start\":76996},{\"end\":77013,\"start\":77009},{\"end\":77027,\"start\":77021},{\"end\":77043,\"start\":77034},{\"end\":77312,\"start\":77307},{\"end\":77338,\"start\":77321},{\"end\":77351,\"start\":77348},{\"end\":77363,\"start\":77353},{\"end\":77716,\"start\":77712},{\"end\":77732,\"start\":77724},{\"end\":77747,\"start\":77742},{\"end\":77987,\"start\":77983},{\"end\":78001,\"start\":77998},{\"end\":78009,\"start\":78007},{\"end\":78022,\"start\":78019},{\"end\":78035,\"start\":78029},{\"end\":78348,\"start\":78336},{\"end\":78358,\"start\":78356},{\"end\":78373,\"start\":78366},{\"end\":78672,\"start\":78666},{\"end\":78694,\"start\":78681},{\"end\":78712,\"start\":78705},{\"end\":78721,\"start\":78714},{\"end\":79022,\"start\":79015},{\"end\":79040,\"start\":79033},{\"end\":79397,\"start\":79393},{\"end\":79407,\"start\":79404},{\"end\":79422,\"start\":79418},{\"end\":79438,\"start\":79430},{\"end\":79448,\"start\":79444},{\"end\":79460,\"start\":79457},{\"end\":79865,\"start\":79863},{\"end\":79880,\"start\":79875},{\"end\":79893,\"start\":79891},{\"end\":79904,\"start\":79901},{\"end\":80193,\"start\":80191},{\"end\":80203,\"start\":80199},{\"end\":80216,\"start\":80212},{\"end\":80225,\"start\":80222},{\"end\":80230,\"start\":80227},{\"end\":80543,\"start\":80541},{\"end\":80553,\"start\":80549},{\"end\":80566,\"start\":80562},{\"end\":80575,\"start\":80572},{\"end\":80580,\"start\":80577},{\"end\":80821,\"start\":80819},{\"end\":80829,\"start\":80827},{\"end\":80845,\"start\":80840},{\"end\":80853,\"start\":80851},{\"end\":80862,\"start\":80860},{\"end\":80872,\"start\":80868},{\"end\":81155,\"start\":81150},{\"end\":81164,\"start\":81162},{\"end\":81174,\"start\":81170},{\"end\":81480,\"start\":81475},{\"end\":81489,\"start\":81487},{\"end\":81499,\"start\":81495},{\"end\":81510,\"start\":81506},{\"end\":81788,\"start\":81783},{\"end\":81797,\"start\":81795},{\"end\":81807,\"start\":81803},{\"end\":81818,\"start\":81814},{\"end\":81999,\"start\":81994},{\"end\":82010,\"start\":82005},{\"end\":82020,\"start\":82016},{\"end\":82035,\"start\":82030},{\"end\":82047,\"start\":82044},{\"end\":82518,\"start\":82514},{\"end\":82531,\"start\":82529},{\"end\":82554,\"start\":82542},{\"end\":82569,\"start\":82565},{\"end\":82580,\"start\":82576},{\"end\":82598,\"start\":82590},{\"end\":82611,\"start\":82605},{\"end\":82943,\"start\":82939},{\"end\":82958,\"start\":82955},{\"end\":82974,\"start\":82969},{\"end\":82985,\"start\":82981},{\"end\":82998,\"start\":82996},{\"end\":83016,\"start\":83008},{\"end\":83029,\"start\":83023},{\"end\":83325,\"start\":83321},{\"end\":83340,\"start\":83336},{\"end\":83351,\"start\":83347},{\"end\":83374,\"start\":83362},{\"end\":83387,\"start\":83385},{\"end\":83402,\"start\":83395},{\"end\":83419,\"start\":83412},{\"end\":83432,\"start\":83426},{\"end\":83450,\"start\":83442},{\"end\":83824,\"start\":83819},{\"end\":83834,\"start\":83830},{\"end\":83844,\"start\":83841},{\"end\":83853,\"start\":83850},{\"end\":83863,\"start\":83858}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2467259},\"end\":64145,\"start\":63825},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3341082},\"end\":64514,\"start\":64147},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":393948},\"end\":64887,\"start\":64516},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218971783},\"end\":65186,\"start\":64889},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":219721240},\"end\":65663,\"start\":65188},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":233444273},\"end\":66092,\"start\":65665},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":211096730},\"end\":66512,\"start\":66094},{\"attributes\":{\"doi\":\"arXiv:2003.04297\",\"id\":\"b7\"},\"end\":66815,\"start\":66514},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":227118869},\"end\":67067,\"start\":66817},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":233024948},\"end\":67380,\"start\":67069},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":226621490},\"end\":67685,\"start\":67382},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":201886205},\"end\":68045,\"start\":67687},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":239029012},\"end\":68444,\"start\":68047},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":52967399},\"end\":68803,\"start\":68446},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":227154342},\"end\":69181,\"start\":68805},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":214775195},\"end\":69606,\"start\":69183},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6529517},\"end\":69893,\"start\":69608},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":54463801},\"end\":70207,\"start\":69895},{\"attributes\":{\"doi\":\"arXiv:2011.15091\",\"id\":\"b18\"},\"end\":70471,\"start\":70209},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":219687798},\"end\":71269,\"start\":70473},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":215762013},\"end\":71659,\"start\":71271},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":8281592},\"end\":71956,\"start\":71661},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":207930212},\"end\":72315,\"start\":71958},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":54465873},\"end\":72558,\"start\":72317},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":206594692},\"end\":72851,\"start\":72560},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":219048999},\"end\":73192,\"start\":72853},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":140218609},\"end\":73613,\"start\":73194},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":51919046},\"end\":74211,\"start\":73615},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":51919046},\"end\":74809,\"start\":74213},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":214796512},\"end\":75292,\"start\":74811},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":15799692},\"end\":75673,\"start\":75294},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":246869540},\"end\":75993,\"start\":75675},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":51880435},\"end\":76339,\"start\":75995},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":202786778},\"end\":76709,\"start\":76341},{\"attributes\":{\"id\":\"b34\"},\"end\":76909,\"start\":76711},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":160025533},\"end\":77224,\"start\":76911},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":236941952},\"end\":77671,\"start\":77226},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":189762205},\"end\":77910,\"start\":77673},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":52295205},\"end\":78328,\"start\":77912},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b39\"},\"end\":78619,\"start\":78330},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":212982067},\"end\":78940,\"start\":78621},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":8946663},\"end\":79289,\"start\":78942},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":1955977},\"end\":79779,\"start\":79291},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":4591284},\"end\":80131,\"start\":79781},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":10800497},\"end\":80481,\"start\":80133},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":10939538},\"end\":80779,\"start\":80483},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":244920930},\"end\":81054,\"start\":80781},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":226567335},\"end\":81395,\"start\":81056},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":68026981},\"end\":81726,\"start\":81397},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":132867757},\"end\":81989,\"start\":81728},{\"attributes\":{\"id\":\"b50\"},\"end\":82448,\"start\":81991},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":47020925},\"end\":82875,\"start\":82450},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":207993979},\"end\":83288,\"start\":82877},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":51927460},\"end\":83745,\"start\":83290},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":219690735},\"end\":84149,\"start\":83747}]", "bib_title": "[{\"end\":63863,\"start\":63825},{\"end\":64196,\"start\":64147},{\"end\":64570,\"start\":64516},{\"end\":64926,\"start\":64889},{\"end\":65263,\"start\":65188},{\"end\":65723,\"start\":65665},{\"end\":66163,\"start\":66094},{\"end\":66865,\"start\":66817},{\"end\":67135,\"start\":67069},{\"end\":67434,\"start\":67382},{\"end\":67767,\"start\":67687},{\"end\":68131,\"start\":68047},{\"end\":68526,\"start\":68446},{\"end\":68897,\"start\":68805},{\"end\":69265,\"start\":69183},{\"end\":69651,\"start\":69608},{\"end\":69935,\"start\":69895},{\"end\":70541,\"start\":70473},{\"end\":71334,\"start\":71271},{\"end\":71718,\"start\":71661},{\"end\":72023,\"start\":71958},{\"end\":72327,\"start\":72317},{\"end\":72604,\"start\":72560},{\"end\":72931,\"start\":72853},{\"end\":73273,\"start\":73194},{\"end\":73685,\"start\":73615},{\"end\":74283,\"start\":74213},{\"end\":74856,\"start\":74811},{\"end\":75358,\"start\":75294},{\"end\":75734,\"start\":75675},{\"end\":76068,\"start\":75995},{\"end\":76409,\"start\":76341},{\"end\":76962,\"start\":76911},{\"end\":77301,\"start\":77226},{\"end\":77701,\"start\":77673},{\"end\":77972,\"start\":77912},{\"end\":78662,\"start\":78621},{\"end\":79006,\"start\":78942},{\"end\":79387,\"start\":79291},{\"end\":79853,\"start\":79781},{\"end\":80183,\"start\":80133},{\"end\":80533,\"start\":80483},{\"end\":80812,\"start\":80781},{\"end\":81139,\"start\":81056},{\"end\":81464,\"start\":81397},{\"end\":81772,\"start\":81728},{\"end\":82504,\"start\":82450},{\"end\":82929,\"start\":82877},{\"end\":83311,\"start\":83290},{\"end\":83809,\"start\":83747}]", "bib_author": "[{\"end\":63885,\"start\":63865},{\"end\":63899,\"start\":63885},{\"end\":63916,\"start\":63899},{\"end\":63924,\"start\":63916},{\"end\":64217,\"start\":64198},{\"end\":64230,\"start\":64217},{\"end\":64244,\"start\":64230},{\"end\":64261,\"start\":64244},{\"end\":64269,\"start\":64261},{\"end\":64587,\"start\":64572},{\"end\":64604,\"start\":64587},{\"end\":64620,\"start\":64604},{\"end\":64939,\"start\":64928},{\"end\":64954,\"start\":64939},{\"end\":64966,\"start\":64954},{\"end\":65281,\"start\":65265},{\"end\":65294,\"start\":65281},{\"end\":65309,\"start\":65294},{\"end\":65322,\"start\":65309},{\"end\":65340,\"start\":65322},{\"end\":65355,\"start\":65340},{\"end\":65741,\"start\":65725},{\"end\":65755,\"start\":65741},{\"end\":65768,\"start\":65755},{\"end\":65781,\"start\":65768},{\"end\":65796,\"start\":65781},{\"end\":65814,\"start\":65796},{\"end\":65829,\"start\":65814},{\"end\":66176,\"start\":66165},{\"end\":66193,\"start\":66176},{\"end\":66211,\"start\":66193},{\"end\":66228,\"start\":66211},{\"end\":66527,\"start\":66514},{\"end\":66538,\"start\":66527},{\"end\":66880,\"start\":66867},{\"end\":66892,\"start\":66880},{\"end\":67150,\"start\":67137},{\"end\":67163,\"start\":67150},{\"end\":67175,\"start\":67163},{\"end\":67446,\"start\":67436},{\"end\":67459,\"start\":67446},{\"end\":67475,\"start\":67459},{\"end\":67484,\"start\":67475},{\"end\":67779,\"start\":67769},{\"end\":67789,\"start\":67779},{\"end\":67798,\"start\":67789},{\"end\":67809,\"start\":67798},{\"end\":68143,\"start\":68133},{\"end\":68158,\"start\":68143},{\"end\":68168,\"start\":68158},{\"end\":68542,\"start\":68528},{\"end\":68558,\"start\":68542},{\"end\":68570,\"start\":68558},{\"end\":68590,\"start\":68570},{\"end\":68911,\"start\":68899},{\"end\":68921,\"start\":68911},{\"end\":68936,\"start\":68921},{\"end\":69278,\"start\":69267},{\"end\":69291,\"start\":69278},{\"end\":69305,\"start\":69291},{\"end\":69320,\"start\":69305},{\"end\":69331,\"start\":69320},{\"end\":69344,\"start\":69331},{\"end\":69674,\"start\":69653},{\"end\":69687,\"start\":69674},{\"end\":69697,\"start\":69687},{\"end\":69701,\"start\":69697},{\"end\":69962,\"start\":69937},{\"end\":69973,\"start\":69962},{\"end\":69989,\"start\":69973},{\"end\":70001,\"start\":69989},{\"end\":70286,\"start\":70271},{\"end\":70301,\"start\":70286},{\"end\":70563,\"start\":70543},{\"end\":70578,\"start\":70563},{\"end\":70594,\"start\":70578},{\"end\":70611,\"start\":70594},{\"end\":70631,\"start\":70611},{\"end\":70650,\"start\":70631},{\"end\":70664,\"start\":70650},{\"end\":70686,\"start\":70664},{\"end\":70706,\"start\":70686},{\"end\":70732,\"start\":70706},{\"end\":70744,\"start\":70732},{\"end\":70763,\"start\":70744},{\"end\":70775,\"start\":70763},{\"end\":70789,\"start\":70775},{\"end\":71350,\"start\":71336},{\"end\":71365,\"start\":71350},{\"end\":71376,\"start\":71365},{\"end\":71391,\"start\":71376},{\"end\":71410,\"start\":71391},{\"end\":71731,\"start\":71720},{\"end\":71741,\"start\":71731},{\"end\":71750,\"start\":71741},{\"end\":72037,\"start\":72025},{\"end\":72048,\"start\":72037},{\"end\":72058,\"start\":72048},{\"end\":72071,\"start\":72058},{\"end\":72086,\"start\":72071},{\"end\":72341,\"start\":72329},{\"end\":72359,\"start\":72341},{\"end\":72373,\"start\":72359},{\"end\":72388,\"start\":72373},{\"end\":72618,\"start\":72606},{\"end\":72633,\"start\":72618},{\"end\":72647,\"start\":72633},{\"end\":72657,\"start\":72647},{\"end\":72942,\"start\":72933},{\"end\":72952,\"start\":72942},{\"end\":72961,\"start\":72952},{\"end\":72972,\"start\":72961},{\"end\":73288,\"start\":73275},{\"end\":73303,\"start\":73288},{\"end\":73317,\"start\":73303},{\"end\":73331,\"start\":73317},{\"end\":73344,\"start\":73331},{\"end\":73701,\"start\":73687},{\"end\":73716,\"start\":73701},{\"end\":73729,\"start\":73716},{\"end\":73743,\"start\":73729},{\"end\":73756,\"start\":73743},{\"end\":73765,\"start\":73756},{\"end\":73778,\"start\":73765},{\"end\":73789,\"start\":73778},{\"end\":73797,\"start\":73789},{\"end\":73822,\"start\":73797},{\"end\":73833,\"start\":73822},{\"end\":73840,\"start\":73833},{\"end\":74299,\"start\":74285},{\"end\":74314,\"start\":74299},{\"end\":74327,\"start\":74314},{\"end\":74341,\"start\":74327},{\"end\":74354,\"start\":74341},{\"end\":74363,\"start\":74354},{\"end\":74376,\"start\":74363},{\"end\":74387,\"start\":74376},{\"end\":74395,\"start\":74387},{\"end\":74420,\"start\":74395},{\"end\":74431,\"start\":74420},{\"end\":74438,\"start\":74431},{\"end\":74872,\"start\":74858},{\"end\":74885,\"start\":74872},{\"end\":74900,\"start\":74885},{\"end\":74914,\"start\":74900},{\"end\":74923,\"start\":74914},{\"end\":74935,\"start\":74923},{\"end\":74954,\"start\":74935},{\"end\":74966,\"start\":74954},{\"end\":74975,\"start\":74966},{\"end\":74982,\"start\":74975},{\"end\":75369,\"start\":75360},{\"end\":75379,\"start\":75369},{\"end\":75393,\"start\":75379},{\"end\":75402,\"start\":75393},{\"end\":75418,\"start\":75402},{\"end\":75749,\"start\":75736},{\"end\":75760,\"start\":75749},{\"end\":75771,\"start\":75760},{\"end\":75784,\"start\":75771},{\"end\":76083,\"start\":76070},{\"end\":76098,\"start\":76083},{\"end\":76113,\"start\":76098},{\"end\":76123,\"start\":76113},{\"end\":76424,\"start\":76411},{\"end\":76435,\"start\":76424},{\"end\":76452,\"start\":76435},{\"end\":76786,\"start\":76772},{\"end\":76806,\"start\":76786},{\"end\":76978,\"start\":76964},{\"end\":76990,\"start\":76978},{\"end\":77003,\"start\":76990},{\"end\":77015,\"start\":77003},{\"end\":77029,\"start\":77015},{\"end\":77045,\"start\":77029},{\"end\":77314,\"start\":77303},{\"end\":77340,\"start\":77314},{\"end\":77353,\"start\":77340},{\"end\":77365,\"start\":77353},{\"end\":77718,\"start\":77703},{\"end\":77734,\"start\":77718},{\"end\":77749,\"start\":77734},{\"end\":77989,\"start\":77974},{\"end\":78003,\"start\":77989},{\"end\":78011,\"start\":78003},{\"end\":78024,\"start\":78011},{\"end\":78037,\"start\":78024},{\"end\":78350,\"start\":78330},{\"end\":78360,\"start\":78350},{\"end\":78375,\"start\":78360},{\"end\":78674,\"start\":78664},{\"end\":78696,\"start\":78674},{\"end\":78714,\"start\":78696},{\"end\":78723,\"start\":78714},{\"end\":79024,\"start\":79008},{\"end\":79042,\"start\":79024},{\"end\":79399,\"start\":79389},{\"end\":79409,\"start\":79399},{\"end\":79424,\"start\":79409},{\"end\":79440,\"start\":79424},{\"end\":79450,\"start\":79440},{\"end\":79462,\"start\":79450},{\"end\":79867,\"start\":79855},{\"end\":79882,\"start\":79867},{\"end\":79895,\"start\":79882},{\"end\":79906,\"start\":79895},{\"end\":80195,\"start\":80185},{\"end\":80205,\"start\":80195},{\"end\":80218,\"start\":80205},{\"end\":80227,\"start\":80218},{\"end\":80232,\"start\":80227},{\"end\":80545,\"start\":80535},{\"end\":80555,\"start\":80545},{\"end\":80568,\"start\":80555},{\"end\":80577,\"start\":80568},{\"end\":80582,\"start\":80577},{\"end\":80823,\"start\":80814},{\"end\":80831,\"start\":80823},{\"end\":80847,\"start\":80831},{\"end\":80855,\"start\":80847},{\"end\":80864,\"start\":80855},{\"end\":80874,\"start\":80864},{\"end\":81157,\"start\":81141},{\"end\":81166,\"start\":81157},{\"end\":81176,\"start\":81166},{\"end\":81482,\"start\":81466},{\"end\":81491,\"start\":81482},{\"end\":81501,\"start\":81491},{\"end\":81512,\"start\":81501},{\"end\":81790,\"start\":81774},{\"end\":81799,\"start\":81790},{\"end\":81809,\"start\":81799},{\"end\":81820,\"start\":81809},{\"end\":82001,\"start\":81991},{\"end\":82012,\"start\":82001},{\"end\":82022,\"start\":82012},{\"end\":82037,\"start\":82022},{\"end\":82049,\"start\":82037},{\"end\":82520,\"start\":82506},{\"end\":82533,\"start\":82520},{\"end\":82556,\"start\":82533},{\"end\":82571,\"start\":82556},{\"end\":82582,\"start\":82571},{\"end\":82600,\"start\":82582},{\"end\":82613,\"start\":82600},{\"end\":82945,\"start\":82931},{\"end\":82960,\"start\":82945},{\"end\":82976,\"start\":82960},{\"end\":82987,\"start\":82976},{\"end\":83000,\"start\":82987},{\"end\":83018,\"start\":83000},{\"end\":83031,\"start\":83018},{\"end\":83327,\"start\":83313},{\"end\":83342,\"start\":83327},{\"end\":83353,\"start\":83342},{\"end\":83376,\"start\":83353},{\"end\":83389,\"start\":83376},{\"end\":83404,\"start\":83389},{\"end\":83421,\"start\":83404},{\"end\":83434,\"start\":83421},{\"end\":83452,\"start\":83434},{\"end\":83826,\"start\":83811},{\"end\":83836,\"start\":83826},{\"end\":83846,\"start\":83836},{\"end\":83855,\"start\":83846},{\"end\":83865,\"start\":83855}]", "bib_venue": "[{\"end\":63988,\"start\":63960},{\"end\":64333,\"start\":64305},{\"end\":65030,\"start\":65002},{\"end\":65421,\"start\":65392},{\"end\":65877,\"start\":65857},{\"end\":66288,\"start\":66262},{\"end\":66940,\"start\":66920},{\"end\":67223,\"start\":67203},{\"end\":68248,\"start\":68212},{\"end\":68618,\"start\":68608},{\"end\":68988,\"start\":68966},{\"end\":69392,\"start\":69372},{\"end\":69749,\"start\":69729},{\"end\":70049,\"start\":70029},{\"end\":70853,\"start\":70825},{\"end\":71461,\"start\":71444},{\"end\":71802,\"start\":71780},{\"end\":72134,\"start\":72114},{\"end\":72436,\"start\":72416},{\"end\":72705,\"start\":72685},{\"end\":73408,\"start\":73380},{\"end\":73920,\"start\":73884},{\"end\":74518,\"start\":74482},{\"end\":75036,\"start\":75013},{\"end\":75488,\"start\":75457},{\"end\":75832,\"start\":75812},{\"end\":76165,\"start\":76148},{\"end\":76524,\"start\":76492},{\"end\":77789,\"start\":77773},{\"end\":79122,\"start\":79086},{\"end\":79542,\"start\":79506},{\"end\":79954,\"start\":79934},{\"end\":82661,\"start\":82641},{\"end\":83079,\"start\":83059},{\"end\":83522,\"start\":83491},{\"end\":63958,\"start\":63924},{\"end\":64303,\"start\":64269},{\"end\":64682,\"start\":64620},{\"end\":65000,\"start\":64966},{\"end\":65390,\"start\":65355},{\"end\":65855,\"start\":65829},{\"end\":66260,\"start\":66228},{\"end\":66644,\"start\":66554},{\"end\":66918,\"start\":66892},{\"end\":67201,\"start\":67175},{\"end\":67515,\"start\":67484},{\"end\":67846,\"start\":67809},{\"end\":68210,\"start\":68168},{\"end\":68606,\"start\":68590},{\"end\":68964,\"start\":68936},{\"end\":69370,\"start\":69344},{\"end\":69727,\"start\":69701},{\"end\":70027,\"start\":70001},{\"end\":70269,\"start\":70209},{\"end\":70823,\"start\":70789},{\"end\":71442,\"start\":71410},{\"end\":71778,\"start\":71750},{\"end\":72112,\"start\":72086},{\"end\":72414,\"start\":72388},{\"end\":72683,\"start\":72657},{\"end\":73003,\"start\":72972},{\"end\":73378,\"start\":73344},{\"end\":73882,\"start\":73840},{\"end\":74480,\"start\":74438},{\"end\":75011,\"start\":74982},{\"end\":75455,\"start\":75418},{\"end\":75810,\"start\":75784},{\"end\":76146,\"start\":76123},{\"end\":76490,\"start\":76452},{\"end\":76770,\"start\":76711},{\"end\":77056,\"start\":77045},{\"end\":77417,\"start\":77365},{\"end\":77771,\"start\":77749},{\"end\":78089,\"start\":78037},{\"end\":78449,\"start\":78391},{\"end\":78760,\"start\":78723},{\"end\":79084,\"start\":79042},{\"end\":79504,\"start\":79462},{\"end\":79932,\"start\":79906},{\"end\":80287,\"start\":80232},{\"end\":80613,\"start\":80582},{\"end\":80905,\"start\":80874},{\"end\":81207,\"start\":81176},{\"end\":81543,\"start\":81512},{\"end\":81840,\"start\":81820},{\"end\":82212,\"start\":82049},{\"end\":82639,\"start\":82613},{\"end\":83057,\"start\":83031},{\"end\":83489,\"start\":83452},{\"end\":83917,\"start\":83865}]"}}}, "year": 2023, "month": 12, "day": 17}