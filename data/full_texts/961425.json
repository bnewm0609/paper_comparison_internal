{"id": 961425, "updated": "2023-04-05 03:14:31.784", "metadata": {"title": "LIBSVM: A library for support vector machines", "authors": "[{\"first\":\"Chih-Chung\",\"last\":\"Chang\",\"middle\":[]},{\"first\":\"Chih-Jen\",\"last\":\"Lin\",\"middle\":[]}]", "venue": "TIST", "journal": null, "publication_date": {"year": 2011, "month": null, "day": null}, "abstract": "LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "332049209", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tist/ChangL11", "doi": "10.1145/1961189.1961199"}}, "content": {"source": {"pdf_hash": "c3e5fbc19199dfa9cadc623bc5bec0fd5c185832", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4af6f7beea63daaaf3b3255a03aee955b8fc5fc3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c3e5fbc19199dfa9cadc623bc5bec0fd5c185832.txt", "contents": "\nLIBSVM: A Library for Support Vector Machines\nInitial version: 2001 Last updated: August 23, 2022\n\nChih-Chung Chang \nDepartment of Computer Science\nNational Taiwan University\nTaipeiTaiwan\n\nChih-Jen Lin cjlin@csie.ntu.edu.tw \nDepartment of Computer Science\nNational Taiwan University\nTaipeiTaiwan\n\nLIBSVM: A Library for Support Vector Machines\nInitial version: 2001 Last updated: August 23, 2022ClassificationLIBSVMoptimizationregressionsupport vector ma- chinesSVM\nLIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems, theoretical convergence, multi-class classification, probability estimates, and parameter selection are discussed in detail.\n\nIntroduction\n\nSupport Vector Machines (SVMs) are a popular machine learning method for classification, regression, and other learning tasks. Since the year 2000, we have been developing the package LIBSVM as a library for support vector machines. The Web address of the package is at http://www.csie.ntu.edu.tw/~cjlin/libsvm. LIBSVM is currently one of the most widely used SVM software. In this article, 1 we present all implementation details of LIBSVM. However, this article does not intend to teach the practical use of LIBSVM. For instructions of using LIBSVM, see the README file included in the package, the LIBSVM FAQ, 2 and the practical guide by Hsu et al. (2003). An earlier version of this article was published in Chang and Lin (2011).\n\nLIBSVM supports the following learning tasks.\n\nsupported in LIBSVM are quadratic minimization problems. We discuss the optimization algorithm in Section 4. Section 5 describes two implementation techniques to reduce the running time for minimizing SVM quadratic problems: shrinking and caching. LIBSVM provides some special settings for unbalanced data; details are in Section 6. Section 7 discusses our implementation for multi-class classification.\n\nSection 8 presents how to transform SVM decision values into probability values. Parameter selection is important for obtaining good SVM models. Section 9 presents a simple and useful parameter selection tool in LIBSVM. Finally, Section 10 concludes this work.\n\n\nSVM Formulations\n\nLIBSVM supports various SVM formulations for classification, regression, and distribution estimation. In this section, we present these formulations and give corresponding references. We also show performance measures used in LIBSVM.\n\n\nC-Support Vector Classification\n\nGiven training vectors x i \u2208 R n , i = 1, . . . , l, in two classes, and an indicator vector y \u2208 R l such that y i \u2208 {1, \u22121}, C-SVC (Boser et al., 1992;Cortes and Vapnik, 1995) solves the following primal optimization problem. min w,b,\u03be\n1 2 w T w + C l i=1 \u03be i (1) subject to y i (w T \u03d5(x i ) + b) \u2265 1 \u2212 \u03be i , \u03be i \u2265 0, i = 1, . . . , l,\nwhere \u03d5(x i ) maps x i into a higher-dimensional space and C > 0 is the regularization parameter. Due to the possible high dimensionality of the vector variable w, usually we solve the following dual problem.\nmin \u03b1 1 2 \u03b1 T Q\u03b1 \u2212 e T \u03b1 subject to y T \u03b1 = 0,(2)\n0 \u2264 \u03b1 i \u2264 C, i = 1, . . . , l,\n\nwhere e = [1, . . . , 1] T is the vector of all ones, Q is an l by l positive semidefinite matrix, Q ij \u2261 y i y j K(x i , x j ), and K(x i , x j ) \u2261 \u03d5(x i ) T \u03d5(x j ) is the kernel function.\n\nAfter problem (2) is solved, using the primal-dual relationship, the optimal w\nsatisfies w = l i=1 y i \u03b1 i \u03d5(x i )(3)\nand the decision function is\nsgn w T \u03d5(x) + b = sgn l i=1 y i \u03b1 i K(x i , x) + b .\nWe store y i \u03b1 i \u2200i, b, label names, 4 support vectors, and other information such as kernel parameters in the model for prediction.\n\n\n\u03bd-Support Vector Classification\n\nThe \u03bd-support vector classification (Sch\u00f6lkopf et al., 2000) introduces a new parameter \u03bd \u2208 (0, 1]. It is proved that \u03bd an upper bound on the fraction of training errors and a lower bound of the fraction of support vectors.\n\nGiven training vectors x i \u2208 R n , i = 1, . . . , l, in two classes, and a vector y \u2208 R l such that y i \u2208 {1, \u22121}, the primal optimization problem is min w,b,\u03be,\u03c1\n1 2 w T w \u2212 \u03bd\u03c1 + 1 l l i=1 \u03be i subject to y i (w T \u03d5(x i ) + b) \u2265 \u03c1 \u2212 \u03be i ,(4)\n\u03be i \u2265 0, i = 1, . . . , l, \u03c1 \u2265 0.\n\nThe dual problem is min \u03b1 1 2 \u03b1 T Q\u03b1 subject to 0 \u2264 \u03b1 i \u2264 1/l, i = 1, . . . , l,\ne T \u03b1 \u2265 \u03bd, y T \u03b1 = 0,(5)\nwhere Q ij = y i y j K(x i , x j ). Chang and Lin (2001) show that problem (5) is feasible if and only if \u03bd \u2264 2 min(#y i = +1, #y i = \u22121) l \u2264 1, so the usable range of \u03bd is smaller than (0, 1].\n\nThe decision function is\nsgn l i=1 y i \u03b1 i K(x i , x) + b .\nIt is shown that e T \u03b1 \u2265 \u03bd can be replaced by e T \u03b1 = \u03bd (Crisp and Burges, 2000;Chang and Lin, 2001). In LIBSVM, we solve a scaled version of problem (5) because numerically \u03b1 i may be too small due to the constraint \u03b1 i \u2264 1/l. min \u03b1 1 2\u1fb1 T Q\u1fb1 subject to 0 \u2264\u1fb1 i \u2264 1, i = 1, . . . , l,\n\ne T\u1fb1 = \u03bdl, y T\u1fb1 = 0.\n\nIf \u03b1 is optimal for the dual problem (5) and \u03c1 is optimal for the primal problem (4), Chang and Lin (2001) show that \u03b1/\u03c1 is an optimal solution of C-SVM with C = 1/(\u03c1l). Thus, in LIBSVM, we output (\u03b1/\u03c1, b/\u03c1) in the model. 5\n\n\nDistribution Estimation (One-class SVM)\n\nOne-class SVM was proposed by Sch\u00f6lkopf et al. (2001) for estimating the support of a high-dimensional distribution. Given training vectors x i \u2208 R n , i = 1, . . . , l without any class information, the primal problem of one-class SVM is min w,\u03be,\u03c1\n1 2 w T w \u2212 \u03c1 + 1 \u03bdl l i=1 \u03be i (7) subject to w T \u03d5(x i ) \u2265 \u03c1 \u2212 \u03be i ,(8)\u03be i \u2265 0, i = 1, . . . , l.\nThe dual problem is\nmin \u03b1 1 2 \u03b1 T Q\u03b1 subject to 0 \u2264 \u03b1 i \u2264 1/(\u03bdl), i = 1, . . . , l,(9)e T \u03b1 = 1, where Q ij = K(x i , x j ) = \u03d5(x i ) T \u03d5(x j ). The decision function is sgn l i=1 \u03b1 i K(x i , x) \u2212 \u03c1 .\nSimilar to the case of \u03bd-SVC, in LIBSVM, we solve a scaled version of (9).\nmin \u03b1 1 2 \u03b1 T Q\u03b1 subject to 0 \u2264 \u03b1 i \u2264 1, i = 1, . . . , l,(10)\ne T \u03b1 = \u03bdl.\n\n\n\u03f5-Support Vector Regression (\u03f5-SVR)\n\nConsider a set of training points, {(x 1 , z 1 ), . . . , (x l , z l )}, where x i \u2208 R n is a feature vector and z i \u2208 R 1 is the target output. Under given parameters C > 0 and \u03f5 > 0, the standard form of support vector regression (Vapnik, 1998) is\nmin w,b,\u03be,\u03be * 1 2 w T w + C l i=1 \u03be i + C l i=1 \u03be * i subject to w T \u03d5(x i ) + b \u2212 z i \u2264 \u03f5 + \u03be i , z i \u2212 w T \u03d5(x i ) \u2212 b \u2264 \u03f5 + \u03be * i , \u03be i , \u03be * i \u2265 0, i = 1, . . . , l.\nThe dual problem is\nmin \u03b1,\u03b1 * 1 2 (\u03b1 \u2212 \u03b1 * ) T Q(\u03b1 \u2212 \u03b1 * ) + \u03f5 l i=1 (\u03b1 i + \u03b1 * i ) + l i=1 z i (\u03b1 i \u2212 \u03b1 * i ) subject to e T (\u03b1 \u2212 \u03b1 * ) = 0,(11)0 \u2264 \u03b1 i , \u03b1 * i \u2264 C, i = 1, . . . , l, where Q ij = K(x i , x j ) \u2261 \u03d5(x i ) T \u03d5(x j ).\nAfter solving problem (11), the approximate function is\nl i=1 (\u2212\u03b1 i + \u03b1 * i )K(x i , x) + b.\nIn LIBSVM, we output \u03b1 * \u2212 \u03b1 in the model.\n\n\n\u03bd-Support Vector Regression (\u03bd-SVR)\n\nSimilar to \u03bd-SVC, for regression, Sch\u00f6lkopf et al. (2000) use a parameter \u03bd \u2208 (0, 1] to control the number of support vectors. The parameter \u03f5 in \u03f5-SVR becomes a parameter here. With (C, \u03bd) as parameters, \u03bd-SVR solves min w,b,\u03be,\u03be * ,\u03f5\n1 2 w T w + C(\u03bd\u03f5 + 1 l l i=1 (\u03be i + \u03be * i )) subject to (w T \u03d5(x i ) + b) \u2212 z i \u2264 \u03f5 + \u03be i , z i \u2212 (w T \u03d5(x i ) + b) \u2264 \u03f5 + \u03be * i , \u03be i , \u03be * i \u2265 0, i = 1, . . . , l, \u03f5 \u2265 0.\nThe dual problem is\nmin \u03b1,\u03b1 * 1 2 (\u03b1 \u2212 \u03b1 * ) T Q(\u03b1 \u2212 \u03b1 * ) + z T (\u03b1 \u2212 \u03b1 * ) subject to e T (\u03b1 \u2212 \u03b1 * ) = 0, e T (\u03b1 + \u03b1 * ) \u2264 C\u03bd,(12)0 \u2264 \u03b1 i , \u03b1 * i \u2264 C/l, i = 1, . . . , l.\nThe approximate function is\nl i=1 (\u2212\u03b1 i + \u03b1 * i )K(x i , x) + b.\nSimilar to \u03bd-SVC, Chang and Lin (2002) show that the inequality e T (\u03b1+\u03b1 * ) \u2264 C\u03bd can be replaced by an equality. Moreover, C/l may be too small because users often choose C to be a small constant like one. Thus, in LIBSVM, we treat the userspecified regularization parameter as C/l. That is,C = C/l is what users specified and LIBSVM solves the following problem.\nmin \u03b1,\u03b1 * 1 2 (\u03b1 \u2212 \u03b1 * ) T Q(\u03b1 \u2212 \u03b1 * ) + z T (\u03b1 \u2212 \u03b1 * ) subject to e T (\u03b1 \u2212 \u03b1 * ) = 0, e T (\u03b1 + \u03b1 * ) =Cl\u03bd, 0 \u2264 \u03b1 i , \u03b1 * i \u2264C, i = 1, . . . , l.\nChang and Lin (2002) prove that \u03f5-SVR with parameters (C, \u03f5) has the same solution as \u03bd-SVR with parameters (lC, \u03bd).\n\n\nPerformance Measures, Basic Usage, and Code Organization\n\nThis section describes LIBSVM's evaluation measures, shows some simple examples of running LIBSVM, and presents the code structure.\n\n\nPerformance Measures\n\nAfter solving optimization problems listed in previous sections, users can apply decision functions to predict labels (target values) of testing data. Let x 1 , . . . , xl be the testing data and f (x 1 ), . . . , f (xl) be decision values (target values for regression) predicted by LIBSVM. If the true labels (true target values) of testing data are known and denoted as y i , . . . , yl, we evaluate the prediction results by the following measures.\n\n\nClassification\n\nAccuracy = # correctly predicted data # total testing data \u00d7 100%\n\n\nRegression\n\nLIBSVM outputs MSE (mean squared error) and r 2 (squared correlation coefficient).\nMSE = 1 ll i=1 (f (x i ) \u2212 y i ) 2 , r 2 = l l i=1 f (x i )y i \u2212 l i=1 f (x i ) l i=1 y i 2 l l i=1 f (x i ) 2 \u2212 l i=1 f (x i ) 2 l l i=1 y 2 i \u2212 l i=1 y i 2 .\n\nA Simple Example of Running LIBSVM\n\nWhile detailed instructions of using LIBSVM are available in the README file of the package and the practical guide by Hsu et al. (2003), here we give a simple example. LIBSVM includes a sample data set heart scale of 270 instances. We split the data to a training set heart scale.tr (170 instances) and a testing set heart scale.te.\n\n$ python tools/subset.py heart_scale 170 heart_scale.tr heart_scale.te\n\nThe command svm-train solves an SVM optimization problem to produce a model.  Next, the command svm-predict uses the obtained model to classify the testing set.\n\n$ ./svm-predict heart_scale.te heart_scale.tr.model output Accuracy = 83% (83/100) (classification)\n\nThe file output contains predicted class labels.\n\n\nCode Organization\n\nAll LIBSVM's training and testing algorithms are implemented in the file svm.cpp.\n\nThe two main sub-routines are svm train and svm predict. The training procedure is more sophisticated, so we give the code organization in Figure 1.\n\nFrom Figure 1, for classification, svm train decouples a multi-class problem to two-class problems (see Section 7) and calls svm train one several times. For regression and one-class SVM, it directly calls svm train one. The probability outputs for classification and regression are also handled in svm train. Then, according to the SVM formulation, svm train one calls a corresponding sub-routine such as solve c svc for C-SVC and solve nu svc for \u03bd-SVC. All solve * sub-routines call the solver Solve after preparing suitable input values. The sub-routine Solve minimizes a general form of SVM optimization problems; see (13) and (24). Details of the sub-routine Solve are described in Sections 4-6.\n\n\nSolving the Quadratic Problems\n\nThis section discusses algorithms used in LIBSVM to solve dual quadratic problems listed in Section 2. We split the discussion to two parts. The first part considers optimization problems with one linear constraint, while the second part checks those with two linear constraints. 4.1 Quadratic Problems with One Linear Constraint: C-SVC, \u03f5-SVR, and One-class SVM\n\nWe consider the following general form of C-SVC, \u03f5-SVR, and one-class SVM.\nmin \u03b1 f (\u03b1) subject to y T \u03b1 = \u2206,(13)0 \u2264 \u03b1 t \u2264 C, t = 1, . . . , l, where f (\u03b1) \u2261 1 2 \u03b1 T Q\u03b1 + p T \u03b1\nand y t = \u00b11, t = 1, . . . , l. The constraint y T \u03b1 = 0 is called a linear constraint. It can be clearly seen that C-SVC and one-class SVM are already in the form of problem (13). For \u03f5-SVR, we use the following reformulation of Eq. (11).\nmin \u03b1,\u03b1 * 1 2 (\u03b1 * ) T , \u03b1 T Q \u2212Q \u2212Q Q \u03b1 * \u03b1 + \u03f5e T \u2212 z T , \u03f5e T + z T \u03b1 * \u03b1 subject to y T \u03b1 * \u03b1 = 0, 0 \u2264 \u03b1 t , \u03b1 * t \u2264 C, t = 1, . . . , l, where y = [1, . . . , 1 l , \u22121, . . . , \u22121 l ] T .\nWe do not assume that Q is positive semi-definite (PSD) because sometimes non-PSD kernel matrices are used.\n\n\nDecomposition Method for Dual Problems\n\nThe main difficulty for solving problem (13) is that Q is a dense matrix and may be too large to be stored. In LIBSVM, we consider a decomposition method to conquer this difficulty. Some earlier works on decomposition methods for SVM include, for example, Osuna et al. (1997a); Joachims (1998); Platt (1998); Keerthi et al. (2001); Hsu and Lin (2002b). Subsequent developments include, for example, Fan et al. (2005); Palagi and Sciandrone (2005); Glasmachers and Igel (2006). A decomposition method modifies only a subset of \u03b1 per iteration, so only some columns of Q are needed. This subset of variables, denoted as the working set B, leads to a smaller optimization sub-problem. An extreme case of the decomposition methods is the Sequential Minimal Optimization (SMO) (Platt, 1998), which restricts B to have only two elements. Then, at each iteration, we solve a simple two-variable problem without needing any optimization software. LIBSVM considers an SMO-type decomposition method proposed in Fan et al. (2005).\n\nAlgorithm 1 (An SMO-type decomposition method in Fan et al., 2005) 1. Find \u03b1 1 as the initial feasible solution. Set k = 1.\n\n2. If \u03b1 k is a stationary point of problem (2), stop. Otherwise, find a two-element working set B = {i, j} by WSS 1 (described in Section 4.1.2). Define N \u2261 {1, . . . , l}\\B. Let \u03b1 k B and \u03b1 k N be sub-vectors of \u03b1 k corresponding to B and N , respectively.\n\n3. If a ij \u2261 K ii + K jj \u2212 2K ij > 0, 7 Solve the following sub-problem with the variable\n\u03b1 B = [\u03b1 i \u03b1 j ] T . min \u03b1 i ,\u03b1 j 1 2 \u03b1 i \u03b1 j Q ii Q ij Q ij Q jj \u03b1 i \u03b1 j + (p B + Q BN \u03b1 k N ) T \u03b1 i \u03b1 j subject to 0 \u2264 \u03b1 i , \u03b1 j \u2264 C,(14)y i \u03b1 i + y j \u03b1 j = \u2206 \u2212 y T N \u03b1 k N , else 7 We abbreviate K(x i , x j ) to K ij .\nLet \u03c4 be a small positive constant and solve\nmin \u03b1 i ,\u03b1 j 1 2 \u03b1 i \u03b1 j Q ii Q ij Q ij Q jj \u03b1 i \u03b1 j + (p B + Q BN \u03b1 k N ) T \u03b1 i \u03b1 j + \u03c4 \u2212 a ij 4 ((\u03b1 i \u2212 \u03b1 k i ) 2 + (\u03b1 j \u2212 \u03b1 k j ) 2 )(15)\nsubject to constraints of problem (14).\n4. Set \u03b1 k+1 B\nto be the optimal solution of sub-problem (14) or (15), and \u03b1 k+1 N \u2261 \u03b1 k N . Set k \u2190 k + 1 and go to Step 2.\n\nNote that B is updated at each iteration, but for simplicity, we use B instead of (15) is used only to handle the situation where Q is non-PSD.\nB k . If Q is PSD, then a ij > 0. Thus sub-problem\n\nStopping Criteria and Working Set Selection\n\nThe Karush-Kuhn-Tucker (KKT) optimality condition of problem (13) implies that a feasible \u03b1 is a stationary point of (13) if and only if there exists a number b and two nonnegative vectors \u03bb and \u03be such that\n\u2207f (\u03b1) + by = \u03bb \u2212 \u03be, \u03bb i \u03b1 i = 0, \u03be i (C \u2212 \u03b1 i ) = 0, \u03bb i \u2265 0, \u03be i \u2265 0, i = 1, . . . , l,(16)\nwhere \u2207f (\u03b1) \u2261 Q\u03b1 + p is the gradient of f (\u03b1). Note that if Q is PSD, from the primal-dual relationship, \u03be, b, and w generated by Eq. (3) form an optimal solution of the primal problem. The condition (16) can be rewritten as\n\u2207 i f (\u03b1) + by i \u2265 0 if \u03b1 i < C, \u2264 0 if \u03b1 i > 0.(17)\nSince\ny i = \u00b11, condition (17) is equivalent to that there exists b such that m(\u03b1) \u2264 b \u2264 M (\u03b1), where m(\u03b1) \u2261 max i\u2208Iup(\u03b1) \u2212y i \u2207 i f (\u03b1) and M (\u03b1) \u2261 min i\u2208I low (\u03b1) \u2212y i \u2207 i f (\u03b1),\nand I up (\u03b1) \u2261 {t | \u03b1 t < C, y t = 1 or \u03b1 t > 0, y t = \u22121}, and\nI low (\u03b1) \u2261 {t | \u03b1 t < C, y t = \u22121 or \u03b1 t > 0, y t = 1}.\nThat is, a feasible \u03b1 is a stationary point of problem (13) if and only if\nm(\u03b1) \u2264 M (\u03b1).(18)\nFrom (18), a suitable stopping condition is\nm(\u03b1 k ) \u2212 M (\u03b1 k ) \u2264 \u03f5,(19)\nwhere \u03f5 is the tolerance. For the selection of the working set B, we use the following procedure from Section II of Fan et al. (2005).\n\n\nWSS 1\n\n1. For all t, s, define\na ts \u2261 K tt + K ss \u2212 2K ts , b ts \u2261 \u2212y t \u2207 t f (\u03b1 k ) + y s \u2207 s f (\u03b1 k ) > 0,(20)and\u0101 ts \u2261 a ts if a ts > 0, \u03c4 otherwise. Select i \u2208 arg max t {\u2212y t \u2207 t f (\u03b1 k ) | t \u2208 I up (\u03b1 k )}, j \u2208 arg min t \u2212 b 2 it a it | t \u2208 I low (\u03b1 k ), \u2212y t \u2207 t f (\u03b1 k ) < \u2212y i \u2207 i f (\u03b1 k ) . (21) 2. Return B = {i, j}.\nThe procedure selects a pair {i, j} approximately minimizing the function value; see\nthe term \u2212b 2 it /\u0101 it in Eq. (21).\n\nSolving the Two-variable Sub-problem\n\nDetails of solving the two-variable sub-problem in Eqs. (14) and (15) are deferred to Section 6, where a more general sub-problem is discussed.\n\n\nMaintaining the Gradient\n\nFrom the discussion in Sections 4.1.1 and 4.1.2, the main operations per iteration are on finding Q BN \u03b1 k N + p B for constructing the sub-problem (14), and calculating \u2207f (\u03b1 k ) for the working set selection and the stopping condition. These two operations can be considered together because\nQ BN \u03b1 k N + p B = \u2207 B f (\u03b1 k ) \u2212 Q BB \u03b1 k B(22)\nand\n\u2207f (\u03b1 k+1 ) = \u2207f (\u03b1 k ) + Q :,B (\u03b1 k+1 B \u2212 \u03b1 k B ),(23)\nwhere |B| \u226a |N | and Q :,B is the sub-matrix of Q including columns in B. If at the kth iteration we already have \u2207f (\u03b1 k ), then Eq. (22) can be used to construct the sub-problem. After the sub-problem is solved, Eq. (23) is employed to have the next \u2207f (\u03b1 k+1 ). Therefore, LIBSVM maintains the gradient throughout the decomposition method.\n\n\nThe Calculation of b or \u03c1\n\nAfter the solution \u03b1 of the dual optimization problem is obtained, the variables b or \u03c1 must be calculated as they are used in the decision function.\n\nNote that b of C-SVC and \u03f5-SVR plays the same role as \u2212\u03c1 in one-class SVM, so we define \u03c1 = \u2212b and discuss how to find \u03c1. If there exists \u03b1 i such that 0 < \u03b1 i < C, then from the KKT condition (18), \u03c1 = y i \u2207 i f (\u03b1). In LIBSVM, for numerical stability, we average all these values.\n\u03c1 = i:0<\u03b1 i <C y i \u2207 i f (\u03b1) |{i | 0 < \u03b1 i < C}| .\nFor the situation that no \u03b1 i satisfying 0 < \u03b1 i < C, the KKT condition (18) becomes\n\u2212M (\u03b1) = max{y i \u2207 i f (\u03b1) | \u03b1 i = 0, y i = \u22121 or \u03b1 i = C, y i = 1} \u2264 \u03c1 \u2264 \u2212m(\u03b1) = min{y i \u2207 i f (\u03b1) | \u03b1 i = 0, y i = 1 or \u03b1 i = C, y i = \u22121}.\nWe take \u03c1 the midpoint of the preceding range.\n\n\nInitial Values\n\nAlgorithm 1 requires an initial feasible \u03b1. For C-SVC and \u03f5-SVR, because the zero vector is feasible, we select it as the initial \u03b1.\n\nFor one-class SVM, the scaled form (10) requires that\n0 \u2264 \u03b1 i \u2264 1, and l i=1 \u03b1 i = \u03bdl.\nWe let the first \u230a\u03bdl\u230b elements have \u03b1 i = 1 and the (\u230a\u03bdl\u230b + 1)st element have \u03b1 i = \u03bdl \u2212 \u230a\u03bdl\u230b. \n\n\nConvergence of the Decomposition Method\n\n\nQuadratic Problems with Two Linear Constraints: \u03bd-SVC and \u03bd-SVR\n\nFrom problems (6) and (12), both \u03bd-SVC and \u03bd-SVR can be written as the following general form.\nmin \u03b1 1 2 \u03b1 T Q\u03b1 + p T \u03b1 subject to y T \u03b1 = \u2206 1 ,(24)e T \u03b1 = \u2206 2 , 0 \u2264 \u03b1 t \u2264 C, t = 1, . . . , l.\nThe main difference between problems (13) and (24) is that (24) has two linear constraints y T \u03b1 = \u2206 1 and e T \u03b1 = \u2206 2 . The optimization algorithm is very similar to that for (13), so we describe only differences.\n\n\nStopping Criteria and Working Set Selection\n\nLet f (\u03b1) be the objective function of problem (24). By the same derivation in Section 4.1.2, The KKT condition of problem (24) implies that there exist b and \u03c1 such\n\nthat\n\u2207 i f (\u03b1) \u2212 \u03c1 + by i \u2265 0 if \u03b1 i < C, \u2264 0 if \u03b1 i > 0.(25)\nDefine\nr 1 \u2261 \u03c1 \u2212 b and r 2 \u2261 \u03c1 + b.(26)\nIf y i = 1, (25) becomes\n\u2207 i f (\u03b1) \u2212 r 1 \u2265 0 if \u03b1 i < C, \u2264 0 if \u03b1 i > 0.(27)\nif y i = \u22121, (25) becomes\n\u2207 i f (\u03b1) \u2212 r 2 \u2265 0 if \u03b1 i < C, \u2264 0 if \u03b1 i > 0.(28)\nHence, given a tolerance \u03f5 > 0, the stopping condition is\nmax (m p (\u03b1) \u2212 M p (\u03b1), m n (\u03b1) \u2212 M n (\u03b1)) < \u03f5,(29)\nwhere\nm p (\u03b1) \u2261 max i\u2208Iup(\u03b1),y i =1 \u2212y i \u2207 i f (\u03b1), M p (\u03b1) \u2261 min i\u2208I low (\u03b1),y i =1 \u2212y i \u2207 i f (\u03b1), and m n (\u03b1) \u2261 max i\u2208Iup(\u03b1),y i =\u22121 \u2212y i \u2207 i f (\u03b1), M n (\u03b1) \u2261 min i\u2208I low (\u03b1),y i =\u22121 \u2212y i \u2207 i f (\u03b1).\nThe following working set selection is extended from WSS 1.\n\nWSS 2 (Extension of WSS 1 for \u03bd-SVM)\n1. Find i p \u2208 arg m p (\u03b1 k ), j p \u2208 arg min t \u2212 b 2 ipt a ipt | y t = 1, \u03b1 t \u2208 I low (\u03b1 k ), \u2212y t \u2207 t f (\u03b1 k ) < \u2212y ip \u2207 ip f (\u03b1 k ) .\n\nFind\ni n \u2208 arg m n (\u03b1 k ), j n \u2208 arg min t \u2212 b 2 int a int | y t = \u22121, \u03b1 t \u2208 I low (\u03b1 k ), \u2212y t \u2207 t f (\u03b1 k ) < \u2212y in \u2207 in f (\u03b1 k ) .\n3. Return {i p , j p } or {i n , j n } depending on which one gives smaller \u2212b 2 ij /\u0101 ij .\n\n\nThe Calculation of b and \u03c1\n\nWe have shown that the KKT condition of problem (24) implies Eqs. (27) and (28) according to y i = 1 and \u22121, respectively. Now we consider the case of y i = 1. If there exists \u03b1 i such that 0 < \u03b1 i < C, then we obtain r 1 = \u2207 i f (\u03b1). In LIBSVM, for numerical stability, we average these values.\nr 1 = i:0<\u03b1 i <C,y i =1 \u2207 i f (\u03b1) |{i | 0 < \u03b1 i < C, y i = 1}| .\nIf there is no \u03b1 i such that 0 < \u03b1 i < C, then r 1 satisfies\nmax \u03b1 i =C,y i =1 \u2207 i f (\u03b1) \u2264 r 1 \u2264 min \u03b1 i =0,y i =1 \u2207 i f (\u03b1).\nWe take r 1 the midpoint of the previous range.\n\nFor the case of y i = \u22121, we can calculate r 2 in a similar way. After r 1 and r 2 are obtained, from Eq. (26),\n\u03c1 = r 1 + r 2 2 and \u2212 b = r 1 \u2212 r 2 2 .\n\nInitial Values\n\nFor \u03bd-SVC, the scaled form (6) requires that\n0 \u2264 \u03b1 i \u2264 1, i:y i =1 \u03b1 i = \u03bdl 2 , and i:y i =\u22121 \u03b1 i = \u03bdl 2 .\nWe let the first \u03bdl/2 elements of \u03b1 i with y i = 1 to have the value one. 8 The situation\n\nfor y i = \u22121 is similar. The same setting is applied to \u03bd-SVR.\n\n\nShrinking and Caching\n\nThis section discusses two implementation tricks (shrinking and caching) for the decomposition method and investigates the computational complexity of Algorithm 1.\n\n\nShrinking\n\nAn optimal solution \u03b1 of the SVM dual problem may contain some bounded elements (i.e., \u03b1 i = 0 or C). These elements may have already been bounded in the middle of the decomposition iterations. To save the training time, the shrinking technique tries to identify and remove some bounded elements, so a smaller optimization problem is solved (Joachims, 1998). The following theorem theoretically supports the shrinking technique by showing that at the final iterations of Algorithm 1 in Section 4.1.2, only a small set of variables is still changed.\n\nTheorem 5.1 (Theorem IV in Fan et al., 2005) Consider problem (13) and assume Q is positive semi-definite.\n\n1. The following set is independent of any optimal solution\u1fb1.\nI \u2261 {i | \u2212y i \u2207 i f (\u1fb1) > M (\u1fb1) or \u2212 y i \u2207 i f (\u1fb1) < m(\u1fb1)}.\nFurther, for every i \u2208 I, problem (13) has a unique and bounded optimal solution at \u03b1 i .\n\n2. Assume Algorithm 1 generates an infinite sequence {\u03b1 k }. There existsk such that after k \u2265k, every \u03b1 k i , i \u2208 I has reached the unique and bounded optimal solution. That is, \u03b1 k i remains the same in all subsequent iterations. In addition, \u2200k \u2265k:\ni \u0338 \u2208 {t | M (\u03b1 k ) \u2264 \u2212y t \u2207 t f (\u03b1 k ) \u2264 m(\u03b1 k )}.\nIf we denote A as the set containing elements not shrunk at the kth iteration, then instead of solving problem (13), the decomposition method works on a smaller problem. After solving problem (30), we may find that some elements are wrongly shrunk. When that happens, the original problem (13) is reoptimized from a starting point\nmin \u03b1 A 1 2 \u03b1 T A Q AA \u03b1 A + (p A + Q AN \u03b1 k N ) T \u03b1 A subject to 0 \u2264 \u03b1 i \u2264 C, \u2200i \u2208 A,(30)y T A \u03b1 A = \u2206 \u2212 y T N \u03b1 k N , where N = {1, . . . ,\u03b1 = [ \u03b1 A \u03b1 N ],\nwhere \u03b1 A is optimal for problem (30) and \u03b1 N corresponds to shrunk bounded variables.\n\nIn LIBSVM, we start the shrinking procedure in an early stage. The procedure is as follows.\n\n1. After every min(l, 1000) iterations, we try to shrink some variables. Note that throughout the iterative process, we have\nm(\u03b1 k ) > M (\u03b1 k )(31)\nbecause the condition (19) is not satisfied yet. Following Theorem 5.1, we conjecture that variables in the following set can be shrunk.\n{t | \u2212y t \u2207 t f (\u03b1 k ) > m(\u03b1 k ), t \u2208 I low (\u03b1 k ), \u03b1 k t is bounded}\u222a {t | \u2212y t \u2207 t f (\u03b1 k ) < M (\u03b1 k ), t \u2208 I up (\u03b1 k ), \u03b1 k t is bounded} = {t | \u2212y t \u2207 t f (\u03b1 k ) > m(\u03b1 k ), \u03b1 k t = C, y t = 1 or \u03b1 k t = 0, y t = \u22121}\u222a {t | \u2212y t \u2207 t f (\u03b1 k ) < M (\u03b1 k ), \u03b1 k t = 0, y t = 1 or \u03b1 k t = C, y t = \u22121}.(32)\nThus, the size of the set A is gradually reduced in every min(l, 1000) iterations.\n\nThe problem (30), and the way of calculating m(\u03b1 k ) and M (\u03b1 k ) are adjusted accordingly.\n\n2. The preceding shrinking strategy is sometimes too aggressive. Hence, when the decomposition method achieves the following condition for the first time.\nm(\u03b1 k ) \u2264 M (\u03b1 k ) + 10\u03f5,(33)\nwhere \u03f5 is the specified stopping tolerance, we reconstruct the gradient (details in Section 5.3). Then, the shrinking procedure can be performed based on more accurate information.\n\n3. Once the stopping condition\nm(\u03b1 k ) \u2264 M (\u03b1 k ) + \u03f5(34)\nof the smaller problem (30)  Note that in solving the shrunk problem (30), we only maintain its gradient  (13), we must reconstruct the whole gradient \u2207f (\u03b1). Details are discussed in Section 5.3.\nQ AA \u03b1 A + Q AN \u03b1 N + p A (see also\nFor \u03bd-SVC and \u03bd-SVR, because the stopping condition (29) is different from (19), variables being shrunk are different from those in (32). For y t = 1, we shrink elements in the following set\n{t | \u2212y t \u2207 t f (\u03b1 k ) > m p (\u03b1 k ), \u03b1 t = C, y t = 1}\u222a {t | \u2212y t \u2207 t f (\u03b1 k ) < M p (\u03b1 k ), \u03b1 t = 0, y t = 1}.\nFor y t = \u22121, we consider the following set.\n{t | \u2212y t \u2207 t f (\u03b1 k ) > m n (\u03b1 k ), \u03b1 t = 0, y t = \u22121}\u222a {t | \u2212y t \u2207 t f (\u03b1 k ) < M n (\u03b1 k ), \u03b1 t = C, y t = \u22121}.\n\nCaching\n\nCaching is an effective technique for reducing the computational time of the decomposition method. Because Q may be too large to be stored in the computer memory, Q ij elements are calculated as needed. We can use available memory (called kernel cache)\n\nto store some recently used Q ij (Joachims, 1998 A structure stores the first len elements of a kernel column. Using pointers prev and next, it is easy to insert or delete a column. The circular list is maintained so that structures are ordered from the least-recent-used one to the most-recent-used one.\n\nBecause of shrinking, columns cached in the computer memory may be in different length. Assume the ith column is needed and Q 1:t,i have been cached. If t \u2264 |A|, we calculate Q t+1:|A|,i and store Q 1:|A|,i in the cache. If t > |A|, the desired Q 1:|A|,i are already in the cache. In this situation, we do not change the cached contents of the ith column.\n\n\nReconstructing the Gradient\n\nIf condition (33) or (34) is satisfied, LIBSVM reconstructs the gradient. Because \u2207 i f (\u03b1), i = 1, . . . , |A| have been maintained in solving the smaller problem (30), what we need is to calculate \u2207 i f (\u03b1), i = |A| + 1, . . . , l. To decrease the cost of this reconstruction, throughout iterations we maintain a vector\u1e20 \u2208 R l .\nG i = C j:\u03b1 j =C Q ij , i = 1, . . . , l.(35)\nThen, for i / \u2208 A,\n\u2207 i f (\u03b1) = l j=1 Q ij \u03b1 j + p i =\u1e20 i + p i + j:j\u2208A 0<\u03b1 j <C Q ij \u03b1 j .(36)\nNote that we use the fact that if j / \u2208 A, then \u03b1 j = 0 or C.\n\n\nThe calculation of \u2207f (\u03b1) via Eq. (36) involves a two-level loop over i and j.\n\nUsing i or j first may result in a very different number of Q ij evaluations. We discuss the differences next. 2. j first: let F \u2261 {j | 1 \u2264 j \u2264 |A| and 0 < \u03b1 j < C}.\n\nFor each j \u2208 F , calculate Q 1:l,j . Though only Q |A|+1:l,j is needed in calculating \u2207 i f (\u03b1), i = |A| + 1, . . . , l, we must get the whole column because of our cache implementation. 9 Thus, this strategy needs no more than\nl \u00b7 |F |(38)\nkernel evaluations. This is an upper bound because certain kernel columns (e.g., Q 1:|A|,j , j \u2208 A) may be already in the cache and do not need to be recalculated.\n\nWe may choose a method by comparing (37) and (38). However, the decision depends on whether Q's elements have been cached. If the cache is large enough, then elements of Q's first |A| columns tend to be in the cache because they have been used recently. In contrast, Q i,1:|A| , i / \u2208 A needed by method 1 may be less likely in the cache because columns not in A are not used to solve problem (30). In such a situation, method 1 may require almost (l \u2212 |A|) \u00b7 |A| kernel valuations, while method 2 needs much fewer evaluations than l \u00b7 |F |. Because method 2 takes an advantage of the cache implementation, we slightly lower the estimate in Eq. (38) and use the following rule to decide the method of calculating Eq. (36): If (l/2) \u00b7 |F | > (l \u2212 |A|) \u00b7 |A| use method 1 Else use method 2\n\nThis rule may not give the optimal choice because we do not take the cache contents into account. However, we argue that in the worst scenario, the selected method by the preceding rule is only slightly slower than the other method. This result can be proved by making the following assumptions.\n\nA LIBSVM training procedure involves only two gradient reconstructions. The first is performed when the 10\u03f5 tolerance is achieved; see Eq. (33). The second is in the end of the training procedure.\n\nOur rule assigns the same method to perform the two gradient reconstructions.\n\nMoreover, these two reconstructions cost a similar amount of time.\n\nWe refer to \"total training time of method x\" as the whole LIBSVM training time (where method x is used for reconstructing gradients), and \"reconstruction time of method x\" as the time of one single gradient reconstruction via method x. We then consider two situations.\n\n1. Method 1 is chosen, but method 2 is better.\n\n\nWe have\n\nTotal time of method 1 \u2264 (Total time of method 2) + 2 \u00b7 (Reconstruction time of method 1) \u2264 2 \u00b7 (Total time of method 2).\n\nWe explain the second inequality in detail. Method 2 for gradient reconstruction requires l \u00b7 |F | kernel elements; however, the number of kernel evaluations may be smaller because some elements have been cached. Therefore, l \u00b7 |F | \u2264 Total time of method 2.\n\nBecause method 1 is chosen and Eq. (37) is an upper bound, 2 \u00b7 (Reconstruction time of method 1) \u2264 2 \u00b7 (l \u2212 |A|) \u00b7 |A| < l \u00b7 |F |.\n\nCombining inequalities (40) and (41) leads to (39).\n\n2. Method 2 is chosen, but method 1 is better.\n\nWe consider the worst situation where Q's first |A| columns are not in the cache.\n\nAs |A|+1, . . . , l are indices of shrunk variables, most likely the remaining l \u2212|A|    We consider problems a7a and ijcnn1. 10 Clearly, the proposed rule selects the better method for both problems. We implement this technique after version 2.88 of LIBSVM.\nx i , x j ) = exp(\u2212\u03b3\u2225x i \u2212 x j \u2225 2 ).(\n10 Available at http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets.\n\n\nIndex Rearrangement\n\nIn solving the smaller problem (30), we need only indices in A (e.g., \u03b1 i , y i , and x i , where i \u2208 A). Thus, a naive implementation does not access array contents in a continuous manner. Alternatively, we can maintain A = {1, . . . , |A|} by rearranging array contents. This approach allows a continuous access of array contents, but requires costs for the rearrangement. We decide to rearrange elements in arrays because throughout the discussion in Sections 5.2-5.3, we assume that a cached ith kernel column contains elements from the first to the tth (i.e., Q 1:t,i ), where t \u2264 l. If we do not rearrange indices so that A = {1, . . . , |A|}, then the whole column Q 1:l,i must be cached because l may be an element in A.\n\nWe rearrange indices by sequentially swapping pairs of indices. If t 1 is going to be shrunk, we find an index t 2 that should stay and then swap them. Swapping two elements in a vector \u03b1 or y is easy, but swapping kernel elements in the cache is more expensive. That is, we must swap (Q t 1 ,i , Q t 2 ,i ) for every cached kernel column i. To make the number of swapping operations small, we use the following implementation. Starting from the first and the last indices, we identify the smallest t 1 that should leave the largest t 2 that should stay. Then, (t 1 , t 2 ) are swapped and we continue the same procedure to identify the next pair.\n\n\nA Summary of the Shrinking Procedure\n\nWe summarize the shrinking procedure in Algorithm 2.\n\nAlgorithm 2 (Extending Algorithm 1 to include the shrinking procedure) Initialization 1. Let \u03b1 1 be an initial feasible solution.\n\n2. Calculate the initial \u2207f (\u03b1 1 ) and\u1e20 in Eq. (35). \n\n\nInitialize a counter\n\n\nIs Shrinking Always Better?\n\nWe found that if the number of iterations is large, then shrinking can shorten the training time. However, if we loosely solve the optimization problem (e.g., by using a large stopping tolerance \u03f5), the code without using shrinking may be much faster.\n\nIn this situation, because of the small number of iterations, the time spent on all decomposition iterations can be even less than one single gradient reconstruction. Table 2 compares the total training time with/without shrinking. For a7a, we use the default \u03f5 = 0.001. Under the parameters C = 1 and \u03b3 = 4, the number of iterations is more than 30,000. Then shrinking is useful. However, for ijcnn1, we deliberately use a loose tolerance \u03f5 = 0.5, so the number of iterations is only around 4,000. Because our shrinking strategy is quite aggressive, before the first gradient reconstruction, only Q A,A is in the cache. Then, we need many kernel evaluations for reconstructing the gradient, so the implementation with shrinking is slower. If enough iterations have been run, most elements in A correspond to free \u03b1 i (0 < \u03b1 i < C); i.e., A \u2248 F . In contrast, if the number of iterations is small (e.g., ijcnn1 in Table 2), many bounded elements have not been shrunk and |F | \u226a |A|.\n\nTherefore, we can check the relation between |F | and |A| to conjecture if shrinking 11 That is, shrinking is performed at the next iteration.\n\nis useful. In LIBSVM, if shrinking is enabled and 2 \u00b7 |F | < |A| in reconstructing the gradient, we issue a warning message to indicate that the code may be faster without shrinking.\n\n\nComputational Complexity\n\nWhile Section 4.1.7 has discussed the asymptotic convergence and the local convergence rate of the decomposition method, in this section, we investigate the computational complexity. Several works have studied the number of iterations of decomposition methods; see, for example, List and Simon (2007). However, algorithms studied in these works are slightly different from LIBSVM, so there is no theoretical result yet on LIBSVM's number of iterations. Empirically, it is known that the number of iterations may be higher than linear to the number of training data. Thus, LIBSVM may take considerable training time for huge data sets. Many techniques, for example, Fine and Scheinberg (2001); Lee and Mangasarian (2001); Keerthi et al. (2006); Segata and Blanzieri (2010), have been developed to obtain an approximate model, but these are beyond the scope of our discussion. In LIBSVM, we provide a simple sub-sampling tool, so users can quickly train a small subset.\n\n\nUnbalanced Data and Solving the Two-variable Sub-problem\n\nFor some classification problems, numbers of data in different classes are unbalanced.\n\nSome researchers (e.g., Osuna et al., 1997b, Section 2.5;Vapnik, 1998, Chapter 10.9) have proposed using different penalty parameters in the SVM formulation. For example, the C-SVM problem becomes min w,b,\u03be\n1 2 w T w + C + y i =1 \u03be i + C \u2212 y i =\u22121 \u03be i subject to y i (w T \u03d5(x i ) + b) \u2265 1 \u2212 \u03be i ,(42)\u03be i \u2265 0, i = 1, . . . , l,\nwhere C + and C \u2212 are regularization parameters for positive and negative classes, respectively. LIBSVM supports this setting, so users can choose weights for classes.\n\nThe dual problem of problem (42) is\nmin \u03b1 1 2 \u03b1 T Q\u03b1 \u2212 e T \u03b1 subject to 0 \u2264 \u03b1 i \u2264 C + , if y i = 1, 0 \u2264 \u03b1 i \u2264 C \u2212 , if y i = \u22121, y T \u03b1 = 0.\nA more general setting is to assign each instance x i a regularization parameter C i .\n\nIf C is replaced by C i , i = 1, . . . , l in problem (13), most results discussed in earlier sections can be extended without problems. 13 The major change of Algorithm 1 is on solving the sub-problem (14), which now becomes\nmin \u03b1 i ,\u03b1 j 1 2 \u03b1 i \u03b1 j Q ii Q ij Q ji Q jj \u03b1 i \u03b1 j + (Q i,N \u03b1 N + p i )\u03b1 i + (Q j,N \u03b1 N + p j )\u03b1 j subject to y i \u03b1 i + y j \u03b1 j = \u2206 \u2212 y T N \u03b1 k N ,(43)0 \u2264 \u03b1 i \u2264 C i , 0 \u2264 \u03b1 j \u2264 C j .\nLet \u03b1 i = \u03b1 k i + d i and \u03b1 j = \u03b1 k j + d j . The sub-problem (43) can be written as\nmin d i ,d j 1 2 d i d j Q ii Q ij Q ij Q jj d i d j + \u2207 i f (\u03b1 k ) \u2207 j f (\u03b1 k ) d i d j subject to y i d i + y j d j = 0, \u2212\u03b1 k i \u2264 d i \u2264 C i \u2212 \u03b1 k i , \u2212\u03b1 k j \u2264 d j \u2264 C j \u2212 \u03b1 k j .\nDefine a ij , b ij , and\u0101 ij as in Eq. (20), andd i \u2261 y i d i ,d j \u2261 y j d j . Usingd i = \u2212d j , the objective function can be written as 1 2\u0101\nijd 2 j + b ijdj .\nMinimizing the previous quadratic function leads to\n\u03b1 new i = \u03b1 k i + y i b ij /\u0101 ij , \u03b1 new j = \u03b1 k j \u2212 y j b ij /\u0101 ij .(44)\nThese two values may need to be modified because of bound constraints. We first consider the case of y i \u0338 = y j and re-write Eq. (44) as\n\u03b1 new i = \u03b1 k i + (\u2212\u2207 i f (\u03b1 k ) \u2212 \u2207 j f (\u03b1 k ))/\u0101 ij , \u03b1 new j = \u03b1 k j + (\u2212\u2207 i f (\u03b1 k ) \u2212 \u2207 j f (\u03b1 k ))/\u0101 ij .\nIn the following figure, a box is generated according to bound constraints. An infeasible (\u03b1 new i , \u03b1 new j ) must be in one of the four regions outside the following box. \n\u03b1 i \u03b1 j \u03b1 i \u2212 \u03b1 j = C i \u2212 C j \u03b1 i \u2212 \u03b1 j = 0 C i C\u03b1 new i \u2212 \u03b1 new j = \u03b1 k i \u2212 \u03b1 k j . If (\u03b1 new i , \u03b1 new j ) is in region I, we set \u03b1 k+1 i = C i and \u03b1 k+1 j = C i \u2212 (\u03b1 k i \u2212 \u03b1 k j ).\nOf course, we must identify the region that (\u03b1 new i , \u03b1 new j ) resides. For region I, we have 7 Multi-class classification LIBSVM implements the \"one-against-one\" approach (Knerr et al., 1990) for multiclass classification. Some early works of applying this strategy to SVM include, for example, Kressel (1998). If k is the number of classes, then k(k \u2212 1)/2 classifiers are constructed and each one trains data from two classes. For training data from the ith and the jth classes, we solve the following two-class classification problem.\n\u03b1 k i \u2212 \u03b1 k j > C i \u2212 C j and \u03b1 new i \u2265 C i .min w ij ,b ij ,\u03be ij 1 2 (w ij ) T w ij + C t (\u03be ij ) t subject to (w ij ) T \u03d5(x t ) + b ij \u2265 1 \u2212 \u03be ij t , if x t in the ith class, (w ij ) T \u03d5(x t ) + b ij \u2264 \u22121 + \u03be ij t , if x t in the jth class, \u03be ij t \u2265 0.\nIn classification we use a voting strategy: each binary classification is considered to be a voting where votes can be cast for all data points x -in the end a point is designated to be in a class with the maximum number of votes.\n\nIn case that two classes have identical votes, though it may not be a good strategy, now we simply choose the class appearing first in the array of storing class names.\n\nMany other methods are available for multi-class SVM classification. Hsu and Lin (2002a) give a detailed comparison and conclude that \"one-against-one\" is a competitive approach.\n\n\nProbability Estimates\n\nSVM predicts only class label (target value for regression) without probability information. This section discusses the LIBSVM implementation for extending SVM to give probability estimates. More details are in Wu et al. (2004) for classification, in  for regression, and in Que and Lin (2022) for one-class SVM.\n\n\nProbabilistic Outputs for Classification\n\nGiven k classes of data, for any x, the goal is to estimate\np i = P (y = i | x), i = 1, . . . , k.\nFollowing the setting of the one-against-one (i.e., pairwise) approach for multi-class classification, we first estimate pairwise class probabilities\nr ij \u2248 P (y = i | y = i or j, x)\nusing an improved implementation (Lin et al., 2007) of Platt (2000). Iff is the decision value at x, then we assume\nr ij \u2248 1 1 + e Af +B ,(45)\nwhere A and B are estimated by minimizing the negative log likelihood of training data (using their labels and decision values). It has been observed that decision values from training may overfit the model (45), so we conduct five-fold cross-validation to obtain decision values before minimizing the negative log likelihood. Note that if some classes contain five or even fewer data points, the resulting model may not be good. You can duplicate the data set so that each fold of cross-validation gets more data.\n\nAfter collecting all r ij values, Wu et al. (2004) propose several approaches to obtain p i , \u2200i. In LIBSVM, we consider their second approach and solve the following optimization problem.\nmin p 1 2 k i=1 j:j\u0338 =i (r ji p i \u2212 r ij p j ) 2 subject to p i \u2265 0, \u2200i, k i=1 p i = 1.(46)\nThe objective function in problem (46) comes from the equality\nP (y = j | y = i or j, x) \u00b7 P (y = i | x) = P (y = i | y = i or j, x) \u00b7 P (y = j | x)\nand can be reformulated as Wu et al. (2004) prove that the non-negativity constraints p i \u2265 0, \u2200i in problem (46) are redundant. After removing these constraints, the optimality condition implies that there exists a scalar b (the Lagrange multiplier of the equality constraint k i=1 p i = 1) such that Q e e T 0\nmin p 1 2 p T Qp, where Q ij = s:s\u0338 =i r 2 si if i = j, \u2212r ji r ij if i \u0338 = j.p b = 0 1 ,(47)\nwhere e is the k \u00d7 1 vector of all ones and 0 is the k \u00d7 1 vector of all zeros.\n\nInstead of solving the linear system (47) by a direct method such as Gaussian elimination, Wu et al. (2004) derive a simple iterative method. Because\n\u2212p T Qp = \u2212p T Q(\u2212be) = bp T e = b,\nthe optimal solution p satisfies\n(Qp) t \u2212 p T Qp = Q tt p t + j:j\u0338 =t Q tj p j \u2212 p T Qp = 0, \u2200t.(48)\nUsing Eq. (48), we consider Algorithm 3.\n\n\nAlgorithm 3\n\n1. Start with an initial p satisfying p i \u2265 0, \u2200i and k i=1 p i = 1.\n\n2. Repeat (t = 1, . . . , k, 1, . . .)\np t \u2190 1 Q tt [\u2212 j:j\u0338 =t Q tj p j + p T Qp](49)\nnormalize p until Eq. (47) is satisfied.\n\nEq. (49) can be simplified to\np t \u2190 p t + 1 Q tt [\u2212(Qp) t + p T Qp].\nAlgorithm 3 guarantees to converge globally to the unique optimum of problem (46).\n\nUsing some tricks, we do not need to recalculate p T Qp at each iteration. More implementation details are in Appendix C of Wu et al. (2004). We consider a relative stopping condition for Algorithm 3.\n\u2225Qp \u2212 p T Qpe\u2225 \u221e = max t |(Qp) t \u2212 p T Qp| < 0.005/k.\nWhen k (the number of classes) is large, some elements of p may be very close to zero. Thus, we use a more strict stopping condition by decreasing the tolerance by a factor of k.\n\n\nProbabilistic Inference for Regression\n\nFor a given set of training data D = {(x i , y i ) | x i \u2208 R n , y i \u2208 R, i = 1, . . . , l}, we assume that the data are collected from the model\ny i = f (x i ) + \u03b4 i ,\nwhere f (x) is the underlying function and \u03b4 i 's are independent and identically distributed random noises. Given a test data x, the distribution of y given x and D, P (y | x, D), allows us to draw probabilistic inferences about y; for example, we can estimate the probability that y is in an interval such as [f (x)\u2212\u2206, f (x)+\u2206]. Denotin\u011d f as the estimated function based on D using SVR, then \u03b6 = \u03b6(x) \u2261 y \u2212f (x) is the out-of-sample residual (or prediction error). We propose modeling the distribution of \u03b6 based on cross-validation residuals {\u03b6 i } l i=1 . The \u03b6 i 's are generated by first conducting a five-fold cross-validation to getf j , j = 1, . . . , 5, and then setting \u03b6 i \u2261 y i \u2212f j (x i ) for (x i , y i ) in the jth fold. It is conceptually clear that the distribution of \u03b6 i 's may resemble that of the prediction error \u03b6. Figure 2 illustrates \u03b6 i 's from a data set. Basically, a discretized distribution like histogram can be used to model the data; however, it is complex because all \u03b6 i 's must be retained. On the contrary, distributions like Gaussian and Laplace, commonly used as noise models, require only location and scale parameters. In Figure 2, we plot the fitted curves using these two families and the histogram of \u03b6 i 's. The figure\n\nshows that the distribution of \u03b6 i 's seems symmetric about zero and that both Gaussian and Laplace reasonably capture the shape of \u03b6 i 's. Thus, we propose to model \u03b6 i by zero-mean Gaussian and Laplace, or equivalently, model the conditional distribution of y givenf (x) by Gaussian and Laplace with meanf (x).  discuss a method to judge whether a Laplace and Gaussian distribution should be used. Moreover, they experimentally show that in all cases they have tried, Laplace is better. Thus, in LIBSVM, we consider the zero-mean Laplace with a density function.\np(z) = 1 2\u03c3 e \u2212 |z| \u03c3 .\nAssuming that \u03b6 i 's are independent, we can estimate the scale parameter \u03c3 by maximizing the likelihood. For Laplace, the maximum likelihood estimate is\n\u03c3 = l i=1 |\u03b6 i | l .\nLin and Weng (2004) point out that some \"very extreme\" \u03b6 i 's may cause inaccurate estimation of \u03c3. Thus, they propose estimating the scale parameter by discarding \u03b6 i 's which exceed \u00b15 \u00b7 (standard deviation of the Laplace distribution). For any new data x, we consider that\ny =f (x) + z,\nwhere z is a random variable following the Laplace distribution with parameter \u03c3.\n\nIn theory, the distribution of \u03b6 may depend on the input x, but here we assume that it is free of x. Such an assumption works well in practice and leads to a simple model. Figure 2: Histogram of \u03b6 i 's and the models via Laplace and Gaussian distributions. The x-axis is \u03b6 i using five-fold cross-validation and the y-axis is the normalized number of data in each bin of width 1.\n\n\nProbabilistic Outputs for One-class SVM\n\nWithout the label information, it is difficult to obtain probabilistic outputs for oneclass SVM. Que and Lin (2022) broadly checked existing approaches for two-class classification, but showed that almost none of them are suitable for one-class SVM. They suggest that a feasible setting is to have probabilities mimic to the decision values of training data. The idea is to group instances with negative values to five bins and instances with positive values to five bins. Then we obtain 11 marks corresponding to probability values 0 0.1 . . . 1.\n\nIn the prediction phase, we check that the decision valuef of a test instance x is the closest to which mark. The corresponding value is our estimate of the probability of being a normal instance (i.e., not an outlier). Specifically, after solving (8), we sort decision values of training data in ascending order and obtain the following points. In our implementation,f is sequentially compared with m i to locate the interval and thus the corresponding probability value.\n\n\nParameter Selection\n\nTo train SVM problems, users must specify some parameters. LIBSVM provides a simple tool to check a grid of parameters. For each parameter setting, LIBSVM obtains cross-validation (CV) accuracy. Finally, the parameters with the highest CV accuracy are returned. The parameter selection tool assumes that the RBF (Gaussian) kernel is used although extensions to other kernels and SVR can be easily made. The RBF kernel takes the form\nK(x i , x j ) = e \u2212\u03b3\u2225x i \u2212x j \u2225 2 ,(50)\nso (C, \u03b3) are parameters to be decided. Users can provide a possible interval of C (or \u03b3) with the grid space. Then, all grid points of (C, \u03b3) are tried to find the one giving the highest CV accuracy. Users then use the best parameters to train the whole training set and generate the final model. We do not consider more advanced parameter selection methods because for only two parameters (C and \u03b3), the number of grid points is not too large. Further, because SVM problems under different (C, \u03b3) parameters are independent, LIBSVM provides a simple tool so that jobs can be run in a parallel (multi-core, shared memory, or distributed) environment. Figure 3: Contour plot of running the parameter selection tool in LIBSVM. The data set heart scale (included in the package) is used. The x-axis is log 2 C and the y-axis is log 2 \u03b3.\n\nFor multi-class classification, under a given (C, \u03b3), LIBSVM uses the one-againstone method to obtain the CV accuracy. Hence, the parameter selection tool suggests the same (C, \u03b3) for all k(k \u2212 1)/2 decision functions. Chen et al. (2005, Section 8) discuss issues of using the same or different parameters for the k(k \u2212 1)/2 two-class problems.\n\nLIBSVM outputs the contour plot of cross-validation accuracy. An example is in Figure 3.\n\n\nConclusions\n\nWhen we released the first version of LIBSVM in 2000, only two-class C-SVC was supported. Gradually, we added other SVM variants, and supported functions such as multi-class classification and probability estimates. Then, LIBSVM becomes a complete SVM package. We add a function only if it is needed by enough users. By keeping the system simple, we strive to ensure good system reliability.\n\nIn summary, this article gives implementation details of LIBSVM. We are still actively updating and maintaining this package. We hope the community will benefit more from our continuing development of LIBSVM.\n\n6\nThe default solver is C-SVC using the RBF kernel (50) with C = 1 and \u03b3 = 1/n.\n\nFigure 1 :\n1LIBSVM's code organization for training. All sub-routines are in svm.cpp.\n\n\nFan et al. (2005, Section III)  andChen et al. (2006) discuss the convergence of Algorithm 1 in detail. For the rate of linear convergence,List and Simon (2009) prove a result without making the assumption used inChen et al. (2006).\n\n\nl}\\A is the set of shrunk variables. Note that in LIBSVM, we always rearrange elements of \u03b1, y, and p to maintain that A = {1, . . . , |A|}. Details of the index rearrangement are in Section 5.4.\n\n\nis reached, we must check if the stopping condition of the original problem (13) has been satisfied. If not, then we reactivate all variables by setting A = {1, . . . , l} and start the same shrinking procedure on the problem (30).\n\n\u2264\ncolumns of Q are not in the cache either and (l \u2212 |A|) \u00b7 |A| kernel evaluations are needed for method 1. Because l \u00b7 |F | \u2264 2 \u00b7 (l \u2212 |A|) \u00b7 |A|, (Reconstruction time of method 2) \u2264 2 \u00b7 (Reconstruction time of method 1(Total time of method 1) + 2 \u00b7 (Reconstruction time of method 1) \u2264 2 \u00b7 (Total time of method 1).\n\n\nso shrinking is conducted every min(l, 1000) iterations 4. Let A = {1, . . . , l} For k = 1, 2, . . . 1. Decrease the shrinking counter 2. If the counter is zero, then shrinking is conducted. (a) If condition (33) is satisfied for the first time, reconstruct the gradient (b) Shrink A by removing elements in the set (32). The implementation described in Section 5.4 ensures that A = {1, . . . , |A|}. A = {1, . . . , l} and set the counter to one 11 4. Find a two-element working set B = {i, j} by WSS 1 5. Obtain Q 1:|A|,i and Q 1:|A|,j from cache or by calculation 6. Solve sub-problem (14) or (15) by procedures in Section 6. Update \u03b1 k to \u03b1 k+1 7. Update the gradient by Eq. (23) and update the vector\u1e20\n\nFrom Section 4 ,\n4two places consume most operations at each iteration: finding the working set B by WSS 1 and calculating Q :,B (\u03b1 k+1 B \u2212 \u03b1 k B ) in Eq. (23). 12 Each place requires O(l) operations. However, if Q :,B is not available in the cache and assume each kernel evaluation costs O(n), the cost becomes O(ln) for calculating a column of kernel elements. Therefore, the complexity of Algorithm 1 is 1. #Iterations \u00d7 O(l) if most columns of Q are cached throughout iterations. 2. #Iterations \u00d7 O(nl) if columns of Q are not cached and each kernel evaluation costs O(n).\n\n\nm 0m1 . . .m 5 = 0 . . .m 10 wherem i , i = 0, . . . , 4 are the 20 \u00d7 i percentile of sorted negative decision values and m i , i = 6, . . . , 10 are the 20 \u00d7 (i \u2212 5) percentile of sorted positive decision values. The setting ofm 5 = 0 follows from the assumption that P (normal|f = 0) = 0.5. By defining m i =m i +m i+1 2 , i = 0, . . . , 9, identifying the closestm i is equivalent to checking which of the following intervals the decision value falls into. f (\u2212\u221e, m 0 ) [m 0 , m 1 ) [m 1 , m 2 ) \u00b7 \u00b7 \u00b7 [m 10 , \u221e)\n\n\nSection 4.1.4). Hence, when we reactivate all variables to reoptimize the problem\n\n\n). Then, some kernel elements may not need to be recalculated. Theorem 5.1 also supports the use of caching because in final iterations, only certain columns of the matrix Q are still needed. If the cache already contains these columns, we can save kernel evaluations in final iterations.In LIBSVM, we consider a simple least-recent-use caching strategy. We use a circular list of structures, where each structure is defined as follows.struct head_t \n\n{ \n\nhead_t *prev, *next; \n// a circular list \n\nQfloat *data; \nint len; \n// data[0,len) is cached in this entry \n\n}; \n\n\n\n\n1. i first: for |A| + 1 \u2264 i \u2264 l, calculate Q i,1:|A| . Although from Eq. (36), only {Q ij | 0 < \u03b1 j < C, j \u2208 A} are needed, our implementation obtains all Q i,1:|A| (i.e., {Q ij | j \u2208 A}). Hence, this case needs at most(l \u2212 |A|) \u00b7 |A| \n(37) \n\nkernel evaluations. Note that LIBSVM uses a column-based caching implemen-\n\ntation. Due to the symmetry of Q, Q i,1:|A| is part of Q's ith column and may \n\nhave been cached. Thus, Eq. (37) is only an upper bound. \n\n\n\nTable 2 :\n2A comparison between two gradient reconstruction methods. The decomposition method reconstructs the gradient twice after satisfying conditions (33) and (34). We show in each row the number of kernel evaluations of a reconstruction. We check two cache sizes to reflect the situations with/without enough cache. The last two rows give the total training time (gradient reconstructions and other operations) in seconds. We use the RBF kernel K(\n\nTable 2\n2compares the number of kernel evaluations in reconstructing the gradient.\n\n\nIf y i = y j , the derivation is the same.Other cases are similar. We have the following pseudo code to identify which region \n\n(\u03b1 new \n\ni \n\n, \u03b1 new \nj ) is in and modify (\u03b1 new \n\ni \n\n, \u03b1 new \nj ) to satisfy bound constraints. \n\nif(y[i]!=y[j]) \n{ \ndouble quad_coef = Q_i[i]+Q_j[j]+2*Q_i[j]; \nif (quad_coef <= 0) \nquad_coef = TAU; \ndouble delta = (-G[i]-G[j])/quad_coef; \ndouble diff = alpha[i] -alpha[j]; \nalpha[i] += delta; \nalpha[j] += delta; \n\nif(diff > 0) \n{ \nif(alpha[j] < 0) // in region III \n{ \nalpha[j] = 0; \nalpha[i] = diff; \n} \n} \nelse \n{ \nif(alpha[i] < 0) // in region IV \n{ \nalpha[i] = 0; \nalpha[j] = -diff; \n} \n} \nif(diff > C_i -C_j) \n{ \nif(alpha[i] > C_i) // in region I \n{ \nalpha[i] = C_i; \nalpha[j] = C_i -diff; \n} \n} \nelse \n{ \nif(alpha[j] > C_j) // in region II \n{ \nalpha[j] = C_j; \nalpha[i] = C_j + diff; \n} \n} \n} \n\n\nThis LIBSVM implementation document was created in 2001 and has been maintained at http: //www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf. 2 LIBSVM FAQ: http://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html.\nIn LIBSVM, any integer can be a label name, so we map label names to \u00b11 by assigning the first training instance to have y 1 = +1.\nMore precisely, solving (6) obtains\u03c1 = \u03c1l. Because\u1fb1 = l\u03b1, we have \u03b1/\u03c1 =\u1fb1/\u03c1. Hence, in LIBSVM, we calculate\u1fb1/\u03c1.\nSpecial care must be made as \u03bdl/2 may not be an integer. See also Section 4.1.6.\nWe always store the first |A| elements of a column.\nNote that because |B| = 2, once the sub-problem has been constructed, solving it takes only a constant number of operations (see details in Section 6).\nThis feature of using C i , \u2200i is not included in LIBSVM, but is available as an extension at libsvmtools.\nAcknowledgmentsThis work was supported in part by the National Science Council of Taiwan via the grants NSC 89-2213-E-002-013 and NSC 89-2213-E-002-106. The authors thank their group members and users for many helpful comments. A list of acknowledgments is at http://www.csie.ntu.edu.tw/~cjlin/libsvm/acknowledgements.\nA training algorithm for optimal margin classifiers. B E Boser, I Guyon, V Vapnik, Proceedings of the Fifth Annual Workshop on Computational Learning Theory. the Fifth Annual Workshop on Computational Learning TheoryACM PressB. E. Boser, I. Guyon, and V. Vapnik. A training algorithm for optimal margin classifiers. In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, pages 144-152. ACM Press, 1992.\n\nTraining \u03bd-support vector classifiers: Theory and algorithms. C.-C Chang, C.-J Lin, Neural Computation. 139C.-C. Chang and C.-J. Lin. Training \u03bd-support vector classifiers: Theory and algo- rithms. Neural Computation, 13(9):2119-2147, 2001.\n\nTraining \u03bd-support vector regression: Theory and algorithms. C.-C Chang, C.-J Lin, Neural Computation. 148C.-C. Chang and C.-J. Lin. Training \u03bd-support vector regression: Theory and algo- rithms. Neural Computation, 14(8):1959-1977, 2002.\n\nLIBSVM: a library for support vector machines. C.-C Chang, C.-J Lin, ACM Transactions on Intelligent Systems and Technology. 23C.-C. Chang and C.-J. Lin. LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(3):27:1-27:27, 2011. Soft- ware available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.\n\nA tutorial on \u03bd-support vector machines. P.-H Chen, C.-J Lin, B Sch\u00f6lkopf, P.-H. Chen, C.-J. Lin, and B. Sch\u00f6lkopf. A tutorial on \u03bd-support vector machines.\n\n. Applied Stochastic Models in Business and Industry. 21Applied Stochastic Models in Business and Industry, 21:111-136, 2005. URL http: //www.csie.ntu.edu.tw/~cjlin/papers/nusvmtoturial.pdf.\n\nA study on SMO-type decomposition methods for support vector machines. P.-H Chen, R.-E Fan, C.-J Lin, IEEE Transactions on Neural Networks. 17P.-H. Chen, R.-E. Fan, and C.-J. Lin. A study on SMO-type decomposition methods for support vector machines. IEEE Transactions on Neural Networks, 17:893-908, July 2006. URL http://www.csie.ntu.edu.tw/~cjlin/papers/generalSMO. pdf.\n\nSupport-vector network. C Cortes, V Vapnik, Machine Learning. 20C. Cortes and V. Vapnik. Support-vector network. Machine Learning, 20:273-297, 1995.\n\nA geometric interpretation of \u03bd-SVM classifiers. D J Crisp, C J C Burges, D. J. Crisp and C. J. C. Burges. A geometric interpretation of \u03bd-SVM classifiers.\n\nIn S Solla, T Leen, K.-R M\u00fcller, Advances in Neural Information Processing Systems. Cambridge, MAMIT Press12In S. Solla, T. Leen, and K.-R. M\u00fcller, editors, Advances in Neural Information Processing Systems, volume 12, Cambridge, MA, 2000. MIT Press.\n\nBDVal: reproducible large-scale predictive model development and validation in high-throughput datasets. K C Dorff, N Chambwe, M Srdanovic, F Campagne, Bioinformatics. 2619K. C. Dorff, N. Chambwe, M. Srdanovic, and F. Campagne. BDVal: repro- ducible large-scale predictive model development and validation in high-throughput datasets. Bioinformatics, 26(19):2472-2473, 2010.\n\nWorking set selection using second order information for training SVM. R.-E Fan, P.-H Chen, C.-J Lin, Journal of Machine Learning Research. 6R.-E. Fan, P.-H. Chen, and C.-J. Lin. Working set selection using second order information for training SVM. Journal of Machine Learning Research, 6:1889-1918, 2005. URL http://www.csie.ntu.edu.tw/~cjlin/papers/quadworkset.pdf.\n\nEfficient SVM training using low-rank kernel representations. S Fine, K Scheinberg, Journal of Machine Learning Research. 2S. Fine and K. Scheinberg. Efficient SVM training using low-rank kernel representa- tions. Journal of Machine Learning Research, 2:243-264, 2001.\n\nMaximum-gain working set selection for support vector machines. T Glasmachers, C Igel, Journal of Machine Learning Research. 7T. Glasmachers and C. Igel. Maximum-gain working set selection for support vector machines. Journal of Machine Learning Research, 7:1437-1466, 2006.\n\nThe pyramid match kernel: Discriminative classification with sets of image features. K Grauman, T Darrell, Proceedings of IEEE International Conference on Computer Vision. IEEE International Conference on Computer VisionK. Grauman and T. Darrell. The pyramid match kernel: Discriminative classification with sets of image features. In Proceedings of IEEE International Conference on Computer Vision, 2005.\n\nPyMVPA: A Python toolbox for multivariate pattern analysis of fMRI data. M Hanke, Y O Halchenko, P B Sederberg, S J Hanson, J V Haxby, S Pollmann, 1539-2791Neuroinformatics. 71M. Hanke, Y. O. Halchenko, P. B. Sederberg, S. J. Hanson, J. V. Haxby, and S. Poll- mann. PyMVPA: A Python toolbox for multivariate pattern analysis of fMRI data. Neuroinformatics, 7(1):37-53, 2009. ISSN 1539-2791.\n\nA comparison of methods for multi-class support vector machines. C.-W Hsu, C.-J Lin, IEEE Transactions on Neural Networks. 132C.-W. Hsu and C.-J. Lin. A comparison of methods for multi-class support vector machines. IEEE Transactions on Neural Networks, 13(2):415-425, 2002a.\n\nA simple decomposition method for support vector machines. C.-W Hsu, C.-J Lin, Machine Learning. 46C.-W. Hsu and C.-J. Lin. A simple decomposition method for support vector ma- chines. Machine Learning, 46:291-314, 2002b.\n\nA practical guide to support vector classification. C.-W Hsu, C.-C Chang, C.-J Lin, Department of Computer Science, National Taiwan UniversityTechnical reportC.-W. Hsu, C.-C. Chang, and C.-J. Lin. A practical guide to support vector classifica- tion. Technical report, Department of Computer Science, National Taiwan Univer- sity, 2003. URL http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide. pdf.\n\nMaking large-scale SVM learning practical. T Joachims, Advances in Kernel Methods -Support Vector Learning. B. Sch\u00f6lkopf, C. J. C. Burges, and A. J. SmolaCambridge, MAMIT PressT. Joachims. Making large-scale SVM learning practical. In B. Sch\u00f6lkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods -Support Vector Learning, pages 169-184, Cambridge, MA, 1998. MIT Press.\n\nImprovements to Platt's SMO algorithm for SVM classifier design. S S Keerthi, S K Shevade, C Bhattacharyya, K R K Murthy, Neural Computation. 13S. S. Keerthi, S. K. Shevade, C. Bhattacharyya, and K. R. K. Murthy. Improvements to Platt's SMO algorithm for SVM classifier design. Neural Computation, 13:637- 649, 2001.\n\nBuilding support vector machines with reduced classifier complexity. S S Keerthi, O Chapelle, D Decoste, Journal of Machine Learning Research. 7S. S. Keerthi, O. Chapelle, and D. DeCoste. Building support vector machines with reduced classifier complexity. Journal of Machine Learning Research, 7:1493-1515, 2006.\n\nSingle-layer learning revisited: a stepwise procedure for building and training a neural network. S Knerr, L Personnaz, G Dreyfus, Neurocomputing: Algorithms, Architectures and Applications. J. FogelmanSpringer-VerlagS. Knerr, L. Personnaz, and G. Dreyfus. Single-layer learning revisited: a stepwise procedure for building and training a neural network. In J. Fogelman, editor, Neu- rocomputing: Algorithms, Architectures and Applications. Springer-Verlag, 1990.\n\nPairwise classification and support vector machines. U H , -G Kressel, Advances in Kernel Methods -Support Vector Learning. B. Sch\u00f6lkopf, C. J. C. Burges, and A. J. SmolaCambridge, MAMIT PressU. H.-G. Kressel. Pairwise classification and support vector machines. In B. Sch\u00f6lkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods -Support Vector Learning, pages 255-268, Cambridge, MA, 1998. MIT Press.\n\nRSVM: Reduced support vector machines. Y.-J Lee, O L Mangasarian, Proceedings of the First SIAM International Conference on Data Mining. the First SIAM International Conference on Data MiningY.-J. Lee and O. L. Mangasarian. RSVM: Reduced support vector machines. In Proceedings of the First SIAM International Conference on Data Mining, 2001.\n\nSimple probabilistic predictions for support vector regression. C.-J Lin, R C Weng, Department of Computer Science, National Taiwan UniversityTechnical reportC.-J. Lin and R. C. Weng. Simple probabilistic predictions for support vector regres- sion. Technical report, Department of Computer Science, National Taiwan Univer- sity, 2004. URL http://www.csie.ntu.edu.tw/~cjlin/papers/svrprob.pdf.\n\nA note on Platt's probabilistic outputs for support vector machines. H.-T Lin, C.-J Lin, R C Weng, Machine Learning. 68H.-T. Lin, C.-J. Lin, and R. C. Weng. A note on Platt's probabilistic outputs for support vector machines. Machine Learning, 68:267-276, 2007. URL http://www. csie.ntu.edu.tw/~cjlin/papers/plattprob.pdf.\n\nGeneral polynomial time decomposition algorithms. N List, H U Simon, Journal of Machine Learning Research. 8N. List and H. U. Simon. General polynomial time decomposition algorithms. Journal of Machine Learning Research, 8:303-321, 2007.\n\nSVM-optimization and steepest-descent line search. N List, H U Simon, Proceedings of the 22nd Annual Conference on Computational Learning Theory. the 22nd Annual Conference on Computational Learning TheoryN. List and H. U. Simon. SVM-optimization and steepest-descent line search. In Proceedings of the 22nd Annual Conference on Computational Learning Theory, 2009.\n\nMaltParser: A language-independent system for data-driven dependency parsing. J Nivre, J Hall, J Nilsson, A Chanev, G Eryigit, S Kubler, S Marinov, E Marsi, Natural Language Engineering. 132J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. Kubler, S. Marinov, and E. Marsi. MaltParser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95-135, 2007.\n\nTraining support vector machines: An application to face detection. E Osuna, R Freund, F Girosi, Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)E. Osuna, R. Freund, and F. Girosi. Training support vector machines: An appli- cation to face detection. In Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), pages 130-136, 1997a.\n\nSupport vector machines: Training and applications. AI Memo 1602. E Osuna, R Freund, F Girosi, Massachusetts Institute of TechnologyE. Osuna, R. Freund, and F. Girosi. Support vector machines: Training and appli- cations. AI Memo 1602, Massachusetts Institute of Technology, 1997b.\n\nOn the convergence of a modified version of SVM light algorithm. L Palagi, M Sciandrone, Optimization Methods and Software. 202-3L. Palagi and M. Sciandrone. On the convergence of a modified version of SVM light algorithm. Optimization Methods and Software, 20(2-3):315-332, 2005.\n\nFast training of support vector machines using sequential minimal optimization. J C Platt, Advances in Kernel Methods -Support Vector Learning. B. Sch\u00f6lkopf, C. J. C. Burges, and A. J. SmolaCambridge, MAMIT PressJ. C. Platt. Fast training of support vector machines using sequential minimal opti- mization. In B. Sch\u00f6lkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods -Support Vector Learning, Cambridge, MA, 1998. MIT Press.\n\nProbabilistic outputs for support vector machines and comparison to regularized likelihood methods. J C Platt, Advances in Large Margin Classifiers. A. Smola, P. Bartlett, B. Sch\u00f6lkopf, and D. SchuurmansCambridge, MAMIT PressJ. C. Platt. Probabilistic outputs for support vector machines and comparison to reg- ularized likelihood methods. In A. Smola, P. Bartlett, B. Sch\u00f6lkopf, and D. Schuur- mans, editors, Advances in Large Margin Classifiers, Cambridge, MA, 2000. MIT Press.\n\nOne-class svm probabilistic outputs. Z Que, C.-J Lin, 2022Technical reportZ. Que and C.-J. Lin. One-class svm probabilistic outputs. Technical report, 2022.\n\nNew support vector algorithms. B Sch\u00f6lkopf, A Smola, R C Williamson, P L Bartlett, Neural Computation. 12B. Sch\u00f6lkopf, A. Smola, R. C. Williamson, and P. L. Bartlett. New support vector algorithms. Neural Computation, 12:1207-1245, 2000.\n\nEstimating the support of a high-dimensional distribution. B Sch\u00f6lkopf, J C Platt, J Shawe-Taylor, A J Smola, R C Williamson, Neural Computation. 137B. Sch\u00f6lkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson. Estimating the support of a high-dimensional distribution. Neural Computation, 13(7):1443-1471, 2001.\n\nFast and scalable local kernel machines. N Segata, E Blanzieri, Journal of Machine Learning Research. 11N. Segata and E. Blanzieri. Fast and scalable local kernel machines. Journal of Machine Learning Research, 11:1883-1926, 2010.\n\nV Vapnik, Statistical Learning Theory. New York, NYWileyV. Vapnik. Statistical Learning Theory. Wiley, New York, NY, 1998.\n\nProbability estimates for multi-class classification by pairwise coupling. T.-F Wu, C.-J Lin, R C Weng, Journal of Machine Learning Research. 5T.-F. Wu, C.-J. Lin, and R. C. Weng. Probability estimates for multi-class classifica- tion by pairwise coupling. Journal of Machine Learning Research, 5:975-1005, 2004. URL http://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf.\n", "annotations": {"author": "[{\"end\":189,\"start\":100},{\"end\":297,\"start\":190},{\"end\":189,\"start\":100},{\"end\":297,\"start\":190}]", "publisher": null, "author_last_name": "[{\"end\":116,\"start\":111},{\"end\":202,\"start\":199},{\"end\":116,\"start\":111},{\"end\":202,\"start\":199}]", "author_first_name": "[{\"end\":110,\"start\":100},{\"end\":198,\"start\":190},{\"end\":110,\"start\":100},{\"end\":198,\"start\":190}]", "author_affiliation": "[{\"end\":188,\"start\":118},{\"end\":296,\"start\":226},{\"end\":188,\"start\":118},{\"end\":296,\"start\":226}]", "title": "[{\"end\":46,\"start\":1},{\"end\":343,\"start\":298},{\"end\":46,\"start\":1},{\"end\":343,\"start\":298}]", "venue": null, "abstract": "[{\"end\":974,\"start\":466},{\"end\":974,\"start\":466}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b18\"},\"end\":1649,\"start\":1632},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1723,\"start\":1703},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2880,\"start\":2860},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2904,\"start\":2880},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3977,\"start\":3953},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4580,\"start\":4560},{\"end\":4859,\"start\":4835},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4879,\"start\":4859},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5193,\"start\":5173},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5407,\"start\":5384},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6338,\"start\":6324},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6976,\"start\":6953},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7601,\"start\":7581},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8094,\"start\":8084},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9374,\"start\":9357},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12346,\"start\":12326},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12363,\"start\":12348},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12377,\"start\":12365},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12400,\"start\":12379},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12421,\"start\":12402},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12486,\"start\":12469},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12516,\"start\":12488},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12545,\"start\":12518},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12855,\"start\":12842},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13088,\"start\":13071},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13157,\"start\":13140},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15560,\"start\":15543},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21407,\"start\":21391},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21644,\"start\":21627},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25094,\"start\":25079},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":33383,\"start\":33362},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":33802,\"start\":33776},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":33825,\"start\":33804},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":33854,\"start\":33827},{\"end\":34256,\"start\":34223},{\"end\":34283,\"start\":34256},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":36690,\"start\":36670},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":36808,\"start\":36794},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":37782,\"start\":37763},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":38125,\"start\":38109},{\"end\":38191,\"start\":38173},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":38588,\"start\":38570},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":38604,\"start\":38592},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":39246,\"start\":39230},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":39669,\"start\":39653},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":40220,\"start\":40204},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":40947,\"start\":40931},{\"end\":44394,\"start\":44376},{\"end\":46881,\"start\":46852},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":47908,\"start\":47890},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":48015,\"start\":47994},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":48086,\"start\":48068},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":1649,\"start\":1632},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1723,\"start\":1703},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2880,\"start\":2860},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2904,\"start\":2880},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3977,\"start\":3953},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4580,\"start\":4560},{\"end\":4859,\"start\":4835},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4879,\"start\":4859},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5193,\"start\":5173},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5407,\"start\":5384},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6338,\"start\":6324},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6976,\"start\":6953},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7601,\"start\":7581},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8094,\"start\":8084},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9374,\"start\":9357},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12346,\"start\":12326},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12363,\"start\":12348},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12377,\"start\":12365},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12400,\"start\":12379},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12421,\"start\":12402},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12486,\"start\":12469},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12516,\"start\":12488},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12545,\"start\":12518},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12855,\"start\":12842},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13088,\"start\":13071},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13157,\"start\":13140},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15560,\"start\":15543},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21407,\"start\":21391},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21644,\"start\":21627},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25094,\"start\":25079},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":33383,\"start\":33362},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":33802,\"start\":33776},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":33825,\"start\":33804},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":33854,\"start\":33827},{\"end\":34256,\"start\":34223},{\"end\":34283,\"start\":34256},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":36690,\"start\":36670},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":36808,\"start\":36794},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":37782,\"start\":37763},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":38125,\"start\":38109},{\"end\":38191,\"start\":38173},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":38588,\"start\":38570},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":38604,\"start\":38592},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":39246,\"start\":39230},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":39669,\"start\":39653},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":40220,\"start\":40204},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":40947,\"start\":40931},{\"end\":44394,\"start\":44376},{\"end\":46881,\"start\":46852},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":47908,\"start\":47890},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":48015,\"start\":47994},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":48086,\"start\":48068}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":47765,\"start\":47685},{\"attributes\":{\"id\":\"fig_2\"},\"end\":47852,\"start\":47766},{\"attributes\":{\"id\":\"fig_3\"},\"end\":48087,\"start\":47853},{\"attributes\":{\"id\":\"fig_4\"},\"end\":48285,\"start\":48088},{\"attributes\":{\"id\":\"fig_5\"},\"end\":48519,\"start\":48286},{\"attributes\":{\"id\":\"fig_6\"},\"end\":48836,\"start\":48520},{\"attributes\":{\"id\":\"fig_7\"},\"end\":49546,\"start\":48837},{\"attributes\":{\"id\":\"fig_8\"},\"end\":50124,\"start\":49547},{\"attributes\":{\"id\":\"fig_9\"},\"end\":50642,\"start\":50125},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":50726,\"start\":50643},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":51299,\"start\":50727},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":51760,\"start\":51300},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":52214,\"start\":51761},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":52298,\"start\":52215},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":53135,\"start\":52299},{\"attributes\":{\"id\":\"fig_0\"},\"end\":47765,\"start\":47685},{\"attributes\":{\"id\":\"fig_2\"},\"end\":47852,\"start\":47766},{\"attributes\":{\"id\":\"fig_3\"},\"end\":48087,\"start\":47853},{\"attributes\":{\"id\":\"fig_4\"},\"end\":48285,\"start\":48088},{\"attributes\":{\"id\":\"fig_5\"},\"end\":48519,\"start\":48286},{\"attributes\":{\"id\":\"fig_6\"},\"end\":48836,\"start\":48520},{\"attributes\":{\"id\":\"fig_7\"},\"end\":49546,\"start\":48837},{\"attributes\":{\"id\":\"fig_8\"},\"end\":50124,\"start\":49547},{\"attributes\":{\"id\":\"fig_9\"},\"end\":50642,\"start\":50125},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":50726,\"start\":50643},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":51299,\"start\":50727},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":51760,\"start\":51300},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":52214,\"start\":51761},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":52298,\"start\":52215},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":53135,\"start\":52299}]", "paragraph": "[{\"end\":1724,\"start\":990},{\"end\":1771,\"start\":1726},{\"end\":2176,\"start\":1773},{\"end\":2438,\"start\":2178},{\"end\":2692,\"start\":2459},{\"end\":2964,\"start\":2728},{\"end\":3273,\"start\":3065},{\"end\":3354,\"start\":3324},{\"end\":3546,\"start\":3356},{\"end\":3626,\"start\":3548},{\"end\":3694,\"start\":3666},{\"end\":3881,\"start\":3749},{\"end\":4140,\"start\":3917},{\"end\":4303,\"start\":4142},{\"end\":4416,\"start\":4383},{\"end\":4498,\"start\":4418},{\"end\":4717,\"start\":4524},{\"end\":4743,\"start\":4719},{\"end\":5063,\"start\":4779},{\"end\":5085,\"start\":5065},{\"end\":5310,\"start\":5087},{\"end\":5602,\"start\":5354},{\"end\":5721,\"start\":5702},{\"end\":5977,\"start\":5903},{\"end\":6052,\"start\":6041},{\"end\":6341,\"start\":6092},{\"end\":6531,\"start\":6512},{\"end\":6799,\"start\":6744},{\"end\":6879,\"start\":6837},{\"end\":7153,\"start\":6919},{\"end\":7345,\"start\":7326},{\"end\":7525,\"start\":7498},{\"end\":7927,\"start\":7563},{\"end\":8190,\"start\":8074},{\"end\":8382,\"start\":8251},{\"end\":8859,\"start\":8407},{\"end\":8943,\"start\":8878},{\"end\":9040,\"start\":8958},{\"end\":9571,\"start\":9238},{\"end\":9643,\"start\":9573},{\"end\":9805,\"start\":9645},{\"end\":9906,\"start\":9807},{\"end\":9956,\"start\":9908},{\"end\":10059,\"start\":9978},{\"end\":10209,\"start\":10061},{\"end\":10912,\"start\":10211},{\"end\":11309,\"start\":10947},{\"end\":11385,\"start\":11311},{\"end\":11726,\"start\":11487},{\"end\":12027,\"start\":11920},{\"end\":13089,\"start\":12070},{\"end\":13214,\"start\":13091},{\"end\":13473,\"start\":13216},{\"end\":13564,\"start\":13475},{\"end\":13831,\"start\":13787},{\"end\":14012,\"start\":13973},{\"end\":14137,\"start\":14028},{\"end\":14282,\"start\":14139},{\"end\":14586,\"start\":14380},{\"end\":14906,\"start\":14681},{\"end\":14965,\"start\":14960},{\"end\":15204,\"start\":15141},{\"end\":15336,\"start\":15262},{\"end\":15398,\"start\":15355},{\"end\":15561,\"start\":15427},{\"end\":15594,\"start\":15571},{\"end\":15976,\"start\":15892},{\"end\":16195,\"start\":16052},{\"end\":16517,\"start\":16224},{\"end\":16570,\"start\":16567},{\"end\":16969,\"start\":16627},{\"end\":17148,\"start\":16999},{\"end\":17432,\"start\":17150},{\"end\":17568,\"start\":17484},{\"end\":17757,\"start\":17711},{\"end\":17908,\"start\":17776},{\"end\":17963,\"start\":17910},{\"end\":18092,\"start\":17997},{\"end\":18296,\"start\":18202},{\"end\":18609,\"start\":18395},{\"end\":18822,\"start\":18657},{\"end\":18828,\"start\":18824},{\"end\":18892,\"start\":18886},{\"end\":18950,\"start\":18926},{\"end\":19028,\"start\":19003},{\"end\":19138,\"start\":19081},{\"end\":19196,\"start\":19191},{\"end\":19452,\"start\":19393},{\"end\":19490,\"start\":19454},{\"end\":19851,\"start\":19760},{\"end\":20177,\"start\":19882},{\"end\":20303,\"start\":20243},{\"end\":20416,\"start\":20369},{\"end\":20529,\"start\":20418},{\"end\":20631,\"start\":20587},{\"end\":20783,\"start\":20694},{\"end\":20847,\"start\":20785},{\"end\":21036,\"start\":20873},{\"end\":21598,\"start\":21050},{\"end\":21706,\"start\":21600},{\"end\":21769,\"start\":21708},{\"end\":21919,\"start\":21830},{\"end\":22172,\"start\":21921},{\"end\":22555,\"start\":22225},{\"end\":22800,\"start\":22714},{\"end\":22893,\"start\":22802},{\"end\":23019,\"start\":22895},{\"end\":23179,\"start\":23043},{\"end\":23566,\"start\":23484},{\"end\":23659,\"start\":23568},{\"end\":23815,\"start\":23661},{\"end\":24027,\"start\":23846},{\"end\":24059,\"start\":24029},{\"end\":24283,\"start\":24087},{\"end\":24510,\"start\":24320},{\"end\":24667,\"start\":24623},{\"end\":25044,\"start\":24792},{\"end\":25350,\"start\":25046},{\"end\":25707,\"start\":25352},{\"end\":26069,\"start\":25739},{\"end\":26134,\"start\":26116},{\"end\":26272,\"start\":26211},{\"end\":26520,\"start\":26355},{\"end\":26749,\"start\":26522},{\"end\":26926,\"start\":26763},{\"end\":27715,\"start\":26928},{\"end\":28012,\"start\":27717},{\"end\":28210,\"start\":28014},{\"end\":28289,\"start\":28212},{\"end\":28357,\"start\":28291},{\"end\":28628,\"start\":28359},{\"end\":28676,\"start\":28630},{\"end\":28809,\"start\":28688},{\"end\":29069,\"start\":28811},{\"end\":29201,\"start\":29071},{\"end\":29254,\"start\":29203},{\"end\":29302,\"start\":29256},{\"end\":29385,\"start\":29304},{\"end\":29645,\"start\":29387},{\"end\":29756,\"start\":29685},{\"end\":30508,\"start\":29780},{\"end\":31157,\"start\":30510},{\"end\":31250,\"start\":31198},{\"end\":31381,\"start\":31252},{\"end\":31436,\"start\":31383},{\"end\":31742,\"start\":31491},{\"end\":32726,\"start\":31744},{\"end\":32870,\"start\":32728},{\"end\":33054,\"start\":32872},{\"end\":34050,\"start\":33083},{\"end\":34197,\"start\":34111},{\"end\":34405,\"start\":34199},{\"end\":34693,\"start\":34526},{\"end\":34730,\"start\":34695},{\"end\":34921,\"start\":34835},{\"end\":35148,\"start\":34923},{\"end\":35418,\"start\":35334},{\"end\":35742,\"start\":35600},{\"end\":35813,\"start\":35762},{\"end\":36025,\"start\":35888},{\"end\":36311,\"start\":36138},{\"end\":37036,\"start\":36496},{\"end\":37522,\"start\":37292},{\"end\":37692,\"start\":37524},{\"end\":37872,\"start\":37694},{\"end\":38210,\"start\":37898},{\"end\":38314,\"start\":38255},{\"end\":38503,\"start\":38354},{\"end\":38652,\"start\":38537},{\"end\":39194,\"start\":38680},{\"end\":39384,\"start\":39196},{\"end\":39539,\"start\":39477},{\"end\":39937,\"start\":39626},{\"end\":40111,\"start\":40032},{\"end\":40262,\"start\":40113},{\"end\":40331,\"start\":40299},{\"end\":40440,\"start\":40400},{\"end\":40524,\"start\":40456},{\"end\":40564,\"start\":40526},{\"end\":40652,\"start\":40612},{\"end\":40683,\"start\":40654},{\"end\":40805,\"start\":40723},{\"end\":41007,\"start\":40807},{\"end\":41240,\"start\":41062},{\"end\":41428,\"start\":41283},{\"end\":42717,\"start\":41452},{\"end\":43283,\"start\":42719},{\"end\":43461,\"start\":43308},{\"end\":43758,\"start\":43483},{\"end\":43854,\"start\":43773},{\"end\":44235,\"start\":43856},{\"end\":44826,\"start\":44279},{\"end\":45300,\"start\":44828},{\"end\":45756,\"start\":45324},{\"end\":46631,\"start\":45797},{\"end\":46977,\"start\":46633},{\"end\":47067,\"start\":46979},{\"end\":47474,\"start\":47083},{\"end\":47684,\"start\":47476},{\"end\":1724,\"start\":990},{\"end\":1771,\"start\":1726},{\"end\":2176,\"start\":1773},{\"end\":2438,\"start\":2178},{\"end\":2692,\"start\":2459},{\"end\":2964,\"start\":2728},{\"end\":3273,\"start\":3065},{\"end\":3354,\"start\":3324},{\"end\":3546,\"start\":3356},{\"end\":3626,\"start\":3548},{\"end\":3694,\"start\":3666},{\"end\":3881,\"start\":3749},{\"end\":4140,\"start\":3917},{\"end\":4303,\"start\":4142},{\"end\":4416,\"start\":4383},{\"end\":4498,\"start\":4418},{\"end\":4717,\"start\":4524},{\"end\":4743,\"start\":4719},{\"end\":5063,\"start\":4779},{\"end\":5085,\"start\":5065},{\"end\":5310,\"start\":5087},{\"end\":5602,\"start\":5354},{\"end\":5721,\"start\":5702},{\"end\":5977,\"start\":5903},{\"end\":6052,\"start\":6041},{\"end\":6341,\"start\":6092},{\"end\":6531,\"start\":6512},{\"end\":6799,\"start\":6744},{\"end\":6879,\"start\":6837},{\"end\":7153,\"start\":6919},{\"end\":7345,\"start\":7326},{\"end\":7525,\"start\":7498},{\"end\":7927,\"start\":7563},{\"end\":8190,\"start\":8074},{\"end\":8382,\"start\":8251},{\"end\":8859,\"start\":8407},{\"end\":8943,\"start\":8878},{\"end\":9040,\"start\":8958},{\"end\":9571,\"start\":9238},{\"end\":9643,\"start\":9573},{\"end\":9805,\"start\":9645},{\"end\":9906,\"start\":9807},{\"end\":9956,\"start\":9908},{\"end\":10059,\"start\":9978},{\"end\":10209,\"start\":10061},{\"end\":10912,\"start\":10211},{\"end\":11309,\"start\":10947},{\"end\":11385,\"start\":11311},{\"end\":11726,\"start\":11487},{\"end\":12027,\"start\":11920},{\"end\":13089,\"start\":12070},{\"end\":13214,\"start\":13091},{\"end\":13473,\"start\":13216},{\"end\":13564,\"start\":13475},{\"end\":13831,\"start\":13787},{\"end\":14012,\"start\":13973},{\"end\":14137,\"start\":14028},{\"end\":14282,\"start\":14139},{\"end\":14586,\"start\":14380},{\"end\":14906,\"start\":14681},{\"end\":14965,\"start\":14960},{\"end\":15204,\"start\":15141},{\"end\":15336,\"start\":15262},{\"end\":15398,\"start\":15355},{\"end\":15561,\"start\":15427},{\"end\":15594,\"start\":15571},{\"end\":15976,\"start\":15892},{\"end\":16195,\"start\":16052},{\"end\":16517,\"start\":16224},{\"end\":16570,\"start\":16567},{\"end\":16969,\"start\":16627},{\"end\":17148,\"start\":16999},{\"end\":17432,\"start\":17150},{\"end\":17568,\"start\":17484},{\"end\":17757,\"start\":17711},{\"end\":17908,\"start\":17776},{\"end\":17963,\"start\":17910},{\"end\":18092,\"start\":17997},{\"end\":18296,\"start\":18202},{\"end\":18609,\"start\":18395},{\"end\":18822,\"start\":18657},{\"end\":18828,\"start\":18824},{\"end\":18892,\"start\":18886},{\"end\":18950,\"start\":18926},{\"end\":19028,\"start\":19003},{\"end\":19138,\"start\":19081},{\"end\":19196,\"start\":19191},{\"end\":19452,\"start\":19393},{\"end\":19490,\"start\":19454},{\"end\":19851,\"start\":19760},{\"end\":20177,\"start\":19882},{\"end\":20303,\"start\":20243},{\"end\":20416,\"start\":20369},{\"end\":20529,\"start\":20418},{\"end\":20631,\"start\":20587},{\"end\":20783,\"start\":20694},{\"end\":20847,\"start\":20785},{\"end\":21036,\"start\":20873},{\"end\":21598,\"start\":21050},{\"end\":21706,\"start\":21600},{\"end\":21769,\"start\":21708},{\"end\":21919,\"start\":21830},{\"end\":22172,\"start\":21921},{\"end\":22555,\"start\":22225},{\"end\":22800,\"start\":22714},{\"end\":22893,\"start\":22802},{\"end\":23019,\"start\":22895},{\"end\":23179,\"start\":23043},{\"end\":23566,\"start\":23484},{\"end\":23659,\"start\":23568},{\"end\":23815,\"start\":23661},{\"end\":24027,\"start\":23846},{\"end\":24059,\"start\":24029},{\"end\":24283,\"start\":24087},{\"end\":24510,\"start\":24320},{\"end\":24667,\"start\":24623},{\"end\":25044,\"start\":24792},{\"end\":25350,\"start\":25046},{\"end\":25707,\"start\":25352},{\"end\":26069,\"start\":25739},{\"end\":26134,\"start\":26116},{\"end\":26272,\"start\":26211},{\"end\":26520,\"start\":26355},{\"end\":26749,\"start\":26522},{\"end\":26926,\"start\":26763},{\"end\":27715,\"start\":26928},{\"end\":28012,\"start\":27717},{\"end\":28210,\"start\":28014},{\"end\":28289,\"start\":28212},{\"end\":28357,\"start\":28291},{\"end\":28628,\"start\":28359},{\"end\":28676,\"start\":28630},{\"end\":28809,\"start\":28688},{\"end\":29069,\"start\":28811},{\"end\":29201,\"start\":29071},{\"end\":29254,\"start\":29203},{\"end\":29302,\"start\":29256},{\"end\":29385,\"start\":29304},{\"end\":29645,\"start\":29387},{\"end\":29756,\"start\":29685},{\"end\":30508,\"start\":29780},{\"end\":31157,\"start\":30510},{\"end\":31250,\"start\":31198},{\"end\":31381,\"start\":31252},{\"end\":31436,\"start\":31383},{\"end\":31742,\"start\":31491},{\"end\":32726,\"start\":31744},{\"end\":32870,\"start\":32728},{\"end\":33054,\"start\":32872},{\"end\":34050,\"start\":33083},{\"end\":34197,\"start\":34111},{\"end\":34405,\"start\":34199},{\"end\":34693,\"start\":34526},{\"end\":34730,\"start\":34695},{\"end\":34921,\"start\":34835},{\"end\":35148,\"start\":34923},{\"end\":35418,\"start\":35334},{\"end\":35742,\"start\":35600},{\"end\":35813,\"start\":35762},{\"end\":36025,\"start\":35888},{\"end\":36311,\"start\":36138},{\"end\":37036,\"start\":36496},{\"end\":37522,\"start\":37292},{\"end\":37692,\"start\":37524},{\"end\":37872,\"start\":37694},{\"end\":38210,\"start\":37898},{\"end\":38314,\"start\":38255},{\"end\":38503,\"start\":38354},{\"end\":38652,\"start\":38537},{\"end\":39194,\"start\":38680},{\"end\":39384,\"start\":39196},{\"end\":39539,\"start\":39477},{\"end\":39937,\"start\":39626},{\"end\":40111,\"start\":40032},{\"end\":40262,\"start\":40113},{\"end\":40331,\"start\":40299},{\"end\":40440,\"start\":40400},{\"end\":40524,\"start\":40456},{\"end\":40564,\"start\":40526},{\"end\":40652,\"start\":40612},{\"end\":40683,\"start\":40654},{\"end\":40805,\"start\":40723},{\"end\":41007,\"start\":40807},{\"end\":41240,\"start\":41062},{\"end\":41428,\"start\":41283},{\"end\":42717,\"start\":41452},{\"end\":43283,\"start\":42719},{\"end\":43461,\"start\":43308},{\"end\":43758,\"start\":43483},{\"end\":43854,\"start\":43773},{\"end\":44235,\"start\":43856},{\"end\":44826,\"start\":44279},{\"end\":45300,\"start\":44828},{\"end\":45756,\"start\":45324},{\"end\":46631,\"start\":45797},{\"end\":46977,\"start\":46633},{\"end\":47067,\"start\":46979},{\"end\":47474,\"start\":47083},{\"end\":47684,\"start\":47476}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3064,\"start\":2965},{\"attributes\":{\"id\":\"formula_1\"},\"end\":3323,\"start\":3274},{\"attributes\":{\"id\":\"formula_2\"},\"end\":3665,\"start\":3627},{\"attributes\":{\"id\":\"formula_3\"},\"end\":3748,\"start\":3695},{\"attributes\":{\"id\":\"formula_4\"},\"end\":4382,\"start\":4304},{\"attributes\":{\"id\":\"formula_5\"},\"end\":4523,\"start\":4499},{\"attributes\":{\"id\":\"formula_6\"},\"end\":4778,\"start\":4744},{\"attributes\":{\"id\":\"formula_8\"},\"end\":5675,\"start\":5603},{\"attributes\":{\"id\":\"formula_9\"},\"end\":5701,\"start\":5675},{\"attributes\":{\"id\":\"formula_10\"},\"end\":5788,\"start\":5722},{\"attributes\":{\"id\":\"formula_11\"},\"end\":5902,\"start\":5788},{\"attributes\":{\"id\":\"formula_12\"},\"end\":6040,\"start\":5978},{\"attributes\":{\"id\":\"formula_13\"},\"end\":6511,\"start\":6342},{\"attributes\":{\"id\":\"formula_14\"},\"end\":6657,\"start\":6532},{\"attributes\":{\"id\":\"formula_15\"},\"end\":6743,\"start\":6657},{\"attributes\":{\"id\":\"formula_16\"},\"end\":6836,\"start\":6800},{\"attributes\":{\"id\":\"formula_17\"},\"end\":7325,\"start\":7154},{\"attributes\":{\"id\":\"formula_18\"},\"end\":7457,\"start\":7346},{\"attributes\":{\"id\":\"formula_19\"},\"end\":7497,\"start\":7457},{\"attributes\":{\"id\":\"formula_20\"},\"end\":7562,\"start\":7526},{\"attributes\":{\"id\":\"formula_21\"},\"end\":8073,\"start\":7928},{\"attributes\":{\"id\":\"formula_22\"},\"end\":9200,\"start\":9041},{\"attributes\":{\"id\":\"formula_23\"},\"end\":11423,\"start\":11386},{\"attributes\":{\"id\":\"formula_24\"},\"end\":11486,\"start\":11423},{\"attributes\":{\"id\":\"formula_25\"},\"end\":11919,\"start\":11727},{\"attributes\":{\"id\":\"formula_26\"},\"end\":13704,\"start\":13565},{\"attributes\":{\"id\":\"formula_27\"},\"end\":13786,\"start\":13704},{\"attributes\":{\"id\":\"formula_28\"},\"end\":13972,\"start\":13832},{\"attributes\":{\"id\":\"formula_29\"},\"end\":14027,\"start\":14013},{\"attributes\":{\"id\":\"formula_30\"},\"end\":14333,\"start\":14283},{\"attributes\":{\"id\":\"formula_31\"},\"end\":14680,\"start\":14587},{\"attributes\":{\"id\":\"formula_32\"},\"end\":14959,\"start\":14907},{\"attributes\":{\"id\":\"formula_33\"},\"end\":15140,\"start\":14966},{\"attributes\":{\"id\":\"formula_34\"},\"end\":15261,\"start\":15205},{\"attributes\":{\"id\":\"formula_35\"},\"end\":15354,\"start\":15337},{\"attributes\":{\"id\":\"formula_36\"},\"end\":15426,\"start\":15399},{\"attributes\":{\"id\":\"formula_37\"},\"end\":15676,\"start\":15595},{\"attributes\":{\"id\":\"formula_38\"},\"end\":15891,\"start\":15676},{\"attributes\":{\"id\":\"formula_39\"},\"end\":16012,\"start\":15977},{\"attributes\":{\"id\":\"formula_40\"},\"end\":16566,\"start\":16518},{\"attributes\":{\"id\":\"formula_41\"},\"end\":16626,\"start\":16571},{\"attributes\":{\"id\":\"formula_42\"},\"end\":17483,\"start\":17433},{\"attributes\":{\"id\":\"formula_43\"},\"end\":17710,\"start\":17569},{\"attributes\":{\"id\":\"formula_44\"},\"end\":17996,\"start\":17964},{\"attributes\":{\"id\":\"formula_45\"},\"end\":18350,\"start\":18297},{\"attributes\":{\"id\":\"formula_46\"},\"end\":18394,\"start\":18350},{\"attributes\":{\"id\":\"formula_47\"},\"end\":18885,\"start\":18829},{\"attributes\":{\"id\":\"formula_48\"},\"end\":18925,\"start\":18893},{\"attributes\":{\"id\":\"formula_49\"},\"end\":19002,\"start\":18951},{\"attributes\":{\"id\":\"formula_50\"},\"end\":19080,\"start\":19029},{\"attributes\":{\"id\":\"formula_51\"},\"end\":19190,\"start\":19139},{\"attributes\":{\"id\":\"formula_52\"},\"end\":19392,\"start\":19197},{\"attributes\":{\"id\":\"formula_53\"},\"end\":19625,\"start\":19491},{\"attributes\":{\"id\":\"formula_54\"},\"end\":19759,\"start\":19632},{\"attributes\":{\"id\":\"formula_55\"},\"end\":20242,\"start\":20178},{\"attributes\":{\"id\":\"formula_56\"},\"end\":20368,\"start\":20304},{\"attributes\":{\"id\":\"formula_57\"},\"end\":20569,\"start\":20530},{\"attributes\":{\"id\":\"formula_58\"},\"end\":20693,\"start\":20632},{\"attributes\":{\"id\":\"formula_59\"},\"end\":21829,\"start\":21770},{\"attributes\":{\"id\":\"formula_60\"},\"end\":22224,\"start\":22173},{\"attributes\":{\"id\":\"formula_61\"},\"end\":22646,\"start\":22556},{\"attributes\":{\"id\":\"formula_62\"},\"end\":22697,\"start\":22646},{\"attributes\":{\"id\":\"formula_63\"},\"end\":22713,\"start\":22697},{\"attributes\":{\"id\":\"formula_64\"},\"end\":23042,\"start\":23020},{\"attributes\":{\"id\":\"formula_65\"},\"end\":23483,\"start\":23180},{\"attributes\":{\"id\":\"formula_66\"},\"end\":23845,\"start\":23816},{\"attributes\":{\"id\":\"formula_67\"},\"end\":24086,\"start\":24060},{\"attributes\":{\"id\":\"formula_68\"},\"end\":24319,\"start\":24284},{\"attributes\":{\"id\":\"formula_69\"},\"end\":24622,\"start\":24511},{\"attributes\":{\"id\":\"formula_70\"},\"end\":24781,\"start\":24668},{\"attributes\":{\"id\":\"formula_71\"},\"end\":26115,\"start\":26070},{\"attributes\":{\"id\":\"formula_72\"},\"end\":26210,\"start\":26135},{\"attributes\":{\"id\":\"formula_73\"},\"end\":26762,\"start\":26750},{\"attributes\":{\"id\":\"formula_77\"},\"end\":29684,\"start\":29646},{\"attributes\":{\"id\":\"formula_78\"},\"end\":34499,\"start\":34406},{\"attributes\":{\"id\":\"formula_79\"},\"end\":34525,\"start\":34499},{\"attributes\":{\"id\":\"formula_80\"},\"end\":34834,\"start\":34731},{\"attributes\":{\"id\":\"formula_81\"},\"end\":35302,\"start\":35149},{\"attributes\":{\"id\":\"formula_82\"},\"end\":35333,\"start\":35302},{\"attributes\":{\"id\":\"formula_83\"},\"end\":35599,\"start\":35419},{\"attributes\":{\"id\":\"formula_84\"},\"end\":35761,\"start\":35743},{\"attributes\":{\"id\":\"formula_85\"},\"end\":35887,\"start\":35814},{\"attributes\":{\"id\":\"formula_86\"},\"end\":36137,\"start\":36026},{\"attributes\":{\"id\":\"formula_87\"},\"end\":36361,\"start\":36312},{\"attributes\":{\"id\":\"formula_88\"},\"end\":36495,\"start\":36361},{\"attributes\":{\"id\":\"formula_89\"},\"end\":37082,\"start\":37037},{\"attributes\":{\"id\":\"formula_90\"},\"end\":37291,\"start\":37082},{\"attributes\":{\"id\":\"formula_91\"},\"end\":38353,\"start\":38315},{\"attributes\":{\"id\":\"formula_92\"},\"end\":38536,\"start\":38504},{\"attributes\":{\"id\":\"formula_93\"},\"end\":38679,\"start\":38653},{\"attributes\":{\"id\":\"formula_94\"},\"end\":39476,\"start\":39385},{\"attributes\":{\"id\":\"formula_95\"},\"end\":39625,\"start\":39540},{\"attributes\":{\"id\":\"formula_96\"},\"end\":40016,\"start\":39938},{\"attributes\":{\"id\":\"formula_97\"},\"end\":40031,\"start\":40016},{\"attributes\":{\"id\":\"formula_98\"},\"end\":40298,\"start\":40263},{\"attributes\":{\"id\":\"formula_99\"},\"end\":40399,\"start\":40332},{\"attributes\":{\"id\":\"formula_100\"},\"end\":40611,\"start\":40565},{\"attributes\":{\"id\":\"formula_101\"},\"end\":40722,\"start\":40684},{\"attributes\":{\"id\":\"formula_102\"},\"end\":41061,\"start\":41008},{\"attributes\":{\"id\":\"formula_103\"},\"end\":41451,\"start\":41429},{\"attributes\":{\"id\":\"formula_104\"},\"end\":43307,\"start\":43284},{\"attributes\":{\"id\":\"formula_105\"},\"end\":43482,\"start\":43462},{\"attributes\":{\"id\":\"formula_106\"},\"end\":43772,\"start\":43759},{\"attributes\":{\"id\":\"formula_107\"},\"end\":45796,\"start\":45757},{\"attributes\":{\"id\":\"formula_0\"},\"end\":3064,\"start\":2965},{\"attributes\":{\"id\":\"formula_1\"},\"end\":3323,\"start\":3274},{\"attributes\":{\"id\":\"formula_2\"},\"end\":3665,\"start\":3627},{\"attributes\":{\"id\":\"formula_3\"},\"end\":3748,\"start\":3695},{\"attributes\":{\"id\":\"formula_4\"},\"end\":4382,\"start\":4304},{\"attributes\":{\"id\":\"formula_5\"},\"end\":4523,\"start\":4499},{\"attributes\":{\"id\":\"formula_6\"},\"end\":4778,\"start\":4744},{\"attributes\":{\"id\":\"formula_8\"},\"end\":5675,\"start\":5603},{\"attributes\":{\"id\":\"formula_9\"},\"end\":5701,\"start\":5675},{\"attributes\":{\"id\":\"formula_10\"},\"end\":5788,\"start\":5722},{\"attributes\":{\"id\":\"formula_11\"},\"end\":5902,\"start\":5788},{\"attributes\":{\"id\":\"formula_12\"},\"end\":6040,\"start\":5978},{\"attributes\":{\"id\":\"formula_13\"},\"end\":6511,\"start\":6342},{\"attributes\":{\"id\":\"formula_14\"},\"end\":6657,\"start\":6532},{\"attributes\":{\"id\":\"formula_15\"},\"end\":6743,\"start\":6657},{\"attributes\":{\"id\":\"formula_16\"},\"end\":6836,\"start\":6800},{\"attributes\":{\"id\":\"formula_17\"},\"end\":7325,\"start\":7154},{\"attributes\":{\"id\":\"formula_18\"},\"end\":7457,\"start\":7346},{\"attributes\":{\"id\":\"formula_19\"},\"end\":7497,\"start\":7457},{\"attributes\":{\"id\":\"formula_20\"},\"end\":7562,\"start\":7526},{\"attributes\":{\"id\":\"formula_21\"},\"end\":8073,\"start\":7928},{\"attributes\":{\"id\":\"formula_22\"},\"end\":9200,\"start\":9041},{\"attributes\":{\"id\":\"formula_23\"},\"end\":11423,\"start\":11386},{\"attributes\":{\"id\":\"formula_24\"},\"end\":11486,\"start\":11423},{\"attributes\":{\"id\":\"formula_25\"},\"end\":11919,\"start\":11727},{\"attributes\":{\"id\":\"formula_26\"},\"end\":13704,\"start\":13565},{\"attributes\":{\"id\":\"formula_27\"},\"end\":13786,\"start\":13704},{\"attributes\":{\"id\":\"formula_28\"},\"end\":13972,\"start\":13832},{\"attributes\":{\"id\":\"formula_29\"},\"end\":14027,\"start\":14013},{\"attributes\":{\"id\":\"formula_30\"},\"end\":14333,\"start\":14283},{\"attributes\":{\"id\":\"formula_31\"},\"end\":14680,\"start\":14587},{\"attributes\":{\"id\":\"formula_32\"},\"end\":14959,\"start\":14907},{\"attributes\":{\"id\":\"formula_33\"},\"end\":15140,\"start\":14966},{\"attributes\":{\"id\":\"formula_34\"},\"end\":15261,\"start\":15205},{\"attributes\":{\"id\":\"formula_35\"},\"end\":15354,\"start\":15337},{\"attributes\":{\"id\":\"formula_36\"},\"end\":15426,\"start\":15399},{\"attributes\":{\"id\":\"formula_37\"},\"end\":15676,\"start\":15595},{\"attributes\":{\"id\":\"formula_38\"},\"end\":15891,\"start\":15676},{\"attributes\":{\"id\":\"formula_39\"},\"end\":16012,\"start\":15977},{\"attributes\":{\"id\":\"formula_40\"},\"end\":16566,\"start\":16518},{\"attributes\":{\"id\":\"formula_41\"},\"end\":16626,\"start\":16571},{\"attributes\":{\"id\":\"formula_42\"},\"end\":17483,\"start\":17433},{\"attributes\":{\"id\":\"formula_43\"},\"end\":17710,\"start\":17569},{\"attributes\":{\"id\":\"formula_44\"},\"end\":17996,\"start\":17964},{\"attributes\":{\"id\":\"formula_45\"},\"end\":18350,\"start\":18297},{\"attributes\":{\"id\":\"formula_46\"},\"end\":18394,\"start\":18350},{\"attributes\":{\"id\":\"formula_47\"},\"end\":18885,\"start\":18829},{\"attributes\":{\"id\":\"formula_48\"},\"end\":18925,\"start\":18893},{\"attributes\":{\"id\":\"formula_49\"},\"end\":19002,\"start\":18951},{\"attributes\":{\"id\":\"formula_50\"},\"end\":19080,\"start\":19029},{\"attributes\":{\"id\":\"formula_51\"},\"end\":19190,\"start\":19139},{\"attributes\":{\"id\":\"formula_52\"},\"end\":19392,\"start\":19197},{\"attributes\":{\"id\":\"formula_53\"},\"end\":19625,\"start\":19491},{\"attributes\":{\"id\":\"formula_54\"},\"end\":19759,\"start\":19632},{\"attributes\":{\"id\":\"formula_55\"},\"end\":20242,\"start\":20178},{\"attributes\":{\"id\":\"formula_56\"},\"end\":20368,\"start\":20304},{\"attributes\":{\"id\":\"formula_57\"},\"end\":20569,\"start\":20530},{\"attributes\":{\"id\":\"formula_58\"},\"end\":20693,\"start\":20632},{\"attributes\":{\"id\":\"formula_59\"},\"end\":21829,\"start\":21770},{\"attributes\":{\"id\":\"formula_60\"},\"end\":22224,\"start\":22173},{\"attributes\":{\"id\":\"formula_61\"},\"end\":22646,\"start\":22556},{\"attributes\":{\"id\":\"formula_62\"},\"end\":22697,\"start\":22646},{\"attributes\":{\"id\":\"formula_63\"},\"end\":22713,\"start\":22697},{\"attributes\":{\"id\":\"formula_64\"},\"end\":23042,\"start\":23020},{\"attributes\":{\"id\":\"formula_65\"},\"end\":23483,\"start\":23180},{\"attributes\":{\"id\":\"formula_66\"},\"end\":23845,\"start\":23816},{\"attributes\":{\"id\":\"formula_67\"},\"end\":24086,\"start\":24060},{\"attributes\":{\"id\":\"formula_68\"},\"end\":24319,\"start\":24284},{\"attributes\":{\"id\":\"formula_69\"},\"end\":24622,\"start\":24511},{\"attributes\":{\"id\":\"formula_70\"},\"end\":24781,\"start\":24668},{\"attributes\":{\"id\":\"formula_71\"},\"end\":26115,\"start\":26070},{\"attributes\":{\"id\":\"formula_72\"},\"end\":26210,\"start\":26135},{\"attributes\":{\"id\":\"formula_73\"},\"end\":26762,\"start\":26750},{\"attributes\":{\"id\":\"formula_77\"},\"end\":29684,\"start\":29646},{\"attributes\":{\"id\":\"formula_78\"},\"end\":34499,\"start\":34406},{\"attributes\":{\"id\":\"formula_79\"},\"end\":34525,\"start\":34499},{\"attributes\":{\"id\":\"formula_80\"},\"end\":34834,\"start\":34731},{\"attributes\":{\"id\":\"formula_81\"},\"end\":35302,\"start\":35149},{\"attributes\":{\"id\":\"formula_82\"},\"end\":35333,\"start\":35302},{\"attributes\":{\"id\":\"formula_83\"},\"end\":35599,\"start\":35419},{\"attributes\":{\"id\":\"formula_84\"},\"end\":35761,\"start\":35743},{\"attributes\":{\"id\":\"formula_85\"},\"end\":35887,\"start\":35814},{\"attributes\":{\"id\":\"formula_86\"},\"end\":36137,\"start\":36026},{\"attributes\":{\"id\":\"formula_87\"},\"end\":36361,\"start\":36312},{\"attributes\":{\"id\":\"formula_88\"},\"end\":36495,\"start\":36361},{\"attributes\":{\"id\":\"formula_89\"},\"end\":37082,\"start\":37037},{\"attributes\":{\"id\":\"formula_90\"},\"end\":37291,\"start\":37082},{\"attributes\":{\"id\":\"formula_91\"},\"end\":38353,\"start\":38315},{\"attributes\":{\"id\":\"formula_92\"},\"end\":38536,\"start\":38504},{\"attributes\":{\"id\":\"formula_93\"},\"end\":38679,\"start\":38653},{\"attributes\":{\"id\":\"formula_94\"},\"end\":39476,\"start\":39385},{\"attributes\":{\"id\":\"formula_95\"},\"end\":39625,\"start\":39540},{\"attributes\":{\"id\":\"formula_96\"},\"end\":40016,\"start\":39938},{\"attributes\":{\"id\":\"formula_97\"},\"end\":40031,\"start\":40016},{\"attributes\":{\"id\":\"formula_98\"},\"end\":40298,\"start\":40263},{\"attributes\":{\"id\":\"formula_99\"},\"end\":40399,\"start\":40332},{\"attributes\":{\"id\":\"formula_100\"},\"end\":40611,\"start\":40565},{\"attributes\":{\"id\":\"formula_101\"},\"end\":40722,\"start\":40684},{\"attributes\":{\"id\":\"formula_102\"},\"end\":41061,\"start\":41008},{\"attributes\":{\"id\":\"formula_103\"},\"end\":41451,\"start\":41429},{\"attributes\":{\"id\":\"formula_104\"},\"end\":43307,\"start\":43284},{\"attributes\":{\"id\":\"formula_105\"},\"end\":43482,\"start\":43462},{\"attributes\":{\"id\":\"formula_106\"},\"end\":43772,\"start\":43759},{\"attributes\":{\"id\":\"formula_107\"},\"end\":45796,\"start\":45757}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31918,\"start\":31911},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32665,\"start\":32658},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31918,\"start\":31911},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32665,\"start\":32658}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":988,\"start\":976},{\"attributes\":{\"n\":\"2\"},\"end\":2457,\"start\":2441},{\"attributes\":{\"n\":\"2.1\"},\"end\":2726,\"start\":2695},{\"attributes\":{\"n\":\"2.2\"},\"end\":3915,\"start\":3884},{\"attributes\":{\"n\":\"2.3\"},\"end\":5352,\"start\":5313},{\"attributes\":{\"n\":\"2.4\"},\"end\":6090,\"start\":6055},{\"attributes\":{\"n\":\"2.5\"},\"end\":6917,\"start\":6882},{\"attributes\":{\"n\":\"3\"},\"end\":8249,\"start\":8193},{\"attributes\":{\"n\":\"3.1\"},\"end\":8405,\"start\":8385},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":8876,\"start\":8862},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":8956,\"start\":8946},{\"attributes\":{\"n\":\"3.2\"},\"end\":9236,\"start\":9202},{\"attributes\":{\"n\":\"3.3\"},\"end\":9976,\"start\":9959},{\"attributes\":{\"n\":\"4\"},\"end\":10945,\"start\":10915},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":12068,\"start\":12030},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":14378,\"start\":14335},{\"end\":15569,\"start\":15564},{\"attributes\":{\"n\":\"4.1.3\"},\"end\":16050,\"start\":16014},{\"attributes\":{\"n\":\"4.1.4\"},\"end\":16222,\"start\":16198},{\"attributes\":{\"n\":\"4.1.5\"},\"end\":16997,\"start\":16972},{\"attributes\":{\"n\":\"4.1.6\"},\"end\":17774,\"start\":17760},{\"attributes\":{\"n\":\"4.1.7\"},\"end\":18134,\"start\":18095},{\"attributes\":{\"n\":\"4.2\"},\"end\":18200,\"start\":18137},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":18655,\"start\":18612},{\"attributes\":{\"n\":\"2.\"},\"end\":19631,\"start\":19627},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":19880,\"start\":19854},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":20585,\"start\":20571},{\"attributes\":{\"n\":\"5\"},\"end\":20871,\"start\":20850},{\"attributes\":{\"n\":\"5.1\"},\"end\":21048,\"start\":21039},{\"attributes\":{\"n\":\"5.2\"},\"end\":24790,\"start\":24783},{\"attributes\":{\"n\":\"5.3\"},\"end\":25737,\"start\":25710},{\"end\":26353,\"start\":26275},{\"end\":28686,\"start\":28679},{\"attributes\":{\"n\":\"5.4\"},\"end\":29778,\"start\":29759},{\"attributes\":{\"n\":\"5.5\"},\"end\":31196,\"start\":31160},{\"attributes\":{\"n\":\"3.\"},\"end\":31459,\"start\":31439},{\"attributes\":{\"n\":\"5.6\"},\"end\":31489,\"start\":31462},{\"attributes\":{\"n\":\"5.7\"},\"end\":33081,\"start\":33057},{\"attributes\":{\"n\":\"6\"},\"end\":34109,\"start\":34053},{\"attributes\":{\"n\":\"8\"},\"end\":37896,\"start\":37875},{\"attributes\":{\"n\":\"8.1\"},\"end\":38253,\"start\":38213},{\"end\":40454,\"start\":40443},{\"attributes\":{\"n\":\"8.2\"},\"end\":41281,\"start\":41243},{\"attributes\":{\"n\":\"8.3\"},\"end\":44277,\"start\":44238},{\"attributes\":{\"n\":\"9\"},\"end\":45322,\"start\":45303},{\"attributes\":{\"n\":\"10\"},\"end\":47081,\"start\":47070},{\"end\":47687,\"start\":47686},{\"end\":47777,\"start\":47767},{\"end\":48522,\"start\":48521},{\"end\":49564,\"start\":49548},{\"end\":51771,\"start\":51762},{\"end\":52223,\"start\":52216},{\"attributes\":{\"n\":\"1\"},\"end\":988,\"start\":976},{\"attributes\":{\"n\":\"2\"},\"end\":2457,\"start\":2441},{\"attributes\":{\"n\":\"2.1\"},\"end\":2726,\"start\":2695},{\"attributes\":{\"n\":\"2.2\"},\"end\":3915,\"start\":3884},{\"attributes\":{\"n\":\"2.3\"},\"end\":5352,\"start\":5313},{\"attributes\":{\"n\":\"2.4\"},\"end\":6090,\"start\":6055},{\"attributes\":{\"n\":\"2.5\"},\"end\":6917,\"start\":6882},{\"attributes\":{\"n\":\"3\"},\"end\":8249,\"start\":8193},{\"attributes\":{\"n\":\"3.1\"},\"end\":8405,\"start\":8385},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":8876,\"start\":8862},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":8956,\"start\":8946},{\"attributes\":{\"n\":\"3.2\"},\"end\":9236,\"start\":9202},{\"attributes\":{\"n\":\"3.3\"},\"end\":9976,\"start\":9959},{\"attributes\":{\"n\":\"4\"},\"end\":10945,\"start\":10915},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":12068,\"start\":12030},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":14378,\"start\":14335},{\"end\":15569,\"start\":15564},{\"attributes\":{\"n\":\"4.1.3\"},\"end\":16050,\"start\":16014},{\"attributes\":{\"n\":\"4.1.4\"},\"end\":16222,\"start\":16198},{\"attributes\":{\"n\":\"4.1.5\"},\"end\":16997,\"start\":16972},{\"attributes\":{\"n\":\"4.1.6\"},\"end\":17774,\"start\":17760},{\"attributes\":{\"n\":\"4.1.7\"},\"end\":18134,\"start\":18095},{\"attributes\":{\"n\":\"4.2\"},\"end\":18200,\"start\":18137},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":18655,\"start\":18612},{\"attributes\":{\"n\":\"2.\"},\"end\":19631,\"start\":19627},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":19880,\"start\":19854},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":20585,\"start\":20571},{\"attributes\":{\"n\":\"5\"},\"end\":20871,\"start\":20850},{\"attributes\":{\"n\":\"5.1\"},\"end\":21048,\"start\":21039},{\"attributes\":{\"n\":\"5.2\"},\"end\":24790,\"start\":24783},{\"attributes\":{\"n\":\"5.3\"},\"end\":25737,\"start\":25710},{\"end\":26353,\"start\":26275},{\"end\":28686,\"start\":28679},{\"attributes\":{\"n\":\"5.4\"},\"end\":29778,\"start\":29759},{\"attributes\":{\"n\":\"5.5\"},\"end\":31196,\"start\":31160},{\"attributes\":{\"n\":\"3.\"},\"end\":31459,\"start\":31439},{\"attributes\":{\"n\":\"5.6\"},\"end\":31489,\"start\":31462},{\"attributes\":{\"n\":\"5.7\"},\"end\":33081,\"start\":33057},{\"attributes\":{\"n\":\"6\"},\"end\":34109,\"start\":34053},{\"attributes\":{\"n\":\"8\"},\"end\":37896,\"start\":37875},{\"attributes\":{\"n\":\"8.1\"},\"end\":38253,\"start\":38213},{\"end\":40454,\"start\":40443},{\"attributes\":{\"n\":\"8.2\"},\"end\":41281,\"start\":41243},{\"attributes\":{\"n\":\"8.3\"},\"end\":44277,\"start\":44238},{\"attributes\":{\"n\":\"9\"},\"end\":45322,\"start\":45303},{\"attributes\":{\"n\":\"10\"},\"end\":47081,\"start\":47070},{\"end\":47687,\"start\":47686},{\"end\":47777,\"start\":47767},{\"end\":48522,\"start\":48521},{\"end\":49564,\"start\":49548},{\"end\":51771,\"start\":51762},{\"end\":52223,\"start\":52216}]", "table": "[{\"end\":51299,\"start\":51165},{\"end\":51760,\"start\":51521},{\"end\":53135,\"start\":52343},{\"end\":51299,\"start\":51165},{\"end\":51760,\"start\":51521},{\"end\":53135,\"start\":52343}]", "figure_caption": "[{\"end\":47765,\"start\":47688},{\"end\":47852,\"start\":47779},{\"end\":48087,\"start\":47855},{\"end\":48285,\"start\":48090},{\"end\":48519,\"start\":48288},{\"end\":48836,\"start\":48523},{\"end\":49546,\"start\":48839},{\"end\":50124,\"start\":49566},{\"end\":50642,\"start\":50127},{\"end\":50726,\"start\":50645},{\"end\":51165,\"start\":50729},{\"end\":51521,\"start\":51302},{\"end\":52214,\"start\":51773},{\"end\":52298,\"start\":52225},{\"end\":52343,\"start\":52301},{\"end\":47765,\"start\":47688},{\"end\":47852,\"start\":47779},{\"end\":48087,\"start\":47855},{\"end\":48285,\"start\":48090},{\"end\":48519,\"start\":48288},{\"end\":48836,\"start\":48523},{\"end\":49546,\"start\":48839},{\"end\":50124,\"start\":49566},{\"end\":50642,\"start\":50127},{\"end\":50726,\"start\":50645},{\"end\":51165,\"start\":50729},{\"end\":51521,\"start\":51302},{\"end\":52214,\"start\":51773},{\"end\":52298,\"start\":52225},{\"end\":52343,\"start\":52301}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10208,\"start\":10200},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10224,\"start\":10216},{\"end\":42300,\"start\":42292},{\"end\":42625,\"start\":42617},{\"end\":44036,\"start\":44028},{\"end\":46457,\"start\":46449},{\"end\":47066,\"start\":47058},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10208,\"start\":10200},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10224,\"start\":10216},{\"end\":42300,\"start\":42292},{\"end\":42625,\"start\":42617},{\"end\":44036,\"start\":44028},{\"end\":46457,\"start\":46449},{\"end\":47066,\"start\":47058}]", "bib_author_first_name": "[{\"end\":54345,\"start\":54344},{\"end\":54347,\"start\":54346},{\"end\":54356,\"start\":54355},{\"end\":54365,\"start\":54364},{\"end\":54784,\"start\":54780},{\"end\":54796,\"start\":54792},{\"end\":55025,\"start\":55021},{\"end\":55037,\"start\":55033},{\"end\":55251,\"start\":55247},{\"end\":55263,\"start\":55259},{\"end\":55592,\"start\":55588},{\"end\":55603,\"start\":55599},{\"end\":55610,\"start\":55609},{\"end\":55972,\"start\":55968},{\"end\":55983,\"start\":55979},{\"end\":55993,\"start\":55989},{\"end\":56297,\"start\":56296},{\"end\":56307,\"start\":56306},{\"end\":56472,\"start\":56471},{\"end\":56474,\"start\":56473},{\"end\":56483,\"start\":56482},{\"end\":56487,\"start\":56484},{\"end\":56581,\"start\":56579},{\"end\":56583,\"start\":56582},{\"end\":56592,\"start\":56591},{\"end\":56603,\"start\":56599},{\"end\":56937,\"start\":56936},{\"end\":56939,\"start\":56938},{\"end\":56948,\"start\":56947},{\"end\":56959,\"start\":56958},{\"end\":56972,\"start\":56971},{\"end\":57282,\"start\":57278},{\"end\":57292,\"start\":57288},{\"end\":57303,\"start\":57299},{\"end\":57640,\"start\":57639},{\"end\":57648,\"start\":57647},{\"end\":57912,\"start\":57911},{\"end\":57927,\"start\":57926},{\"end\":58209,\"start\":58208},{\"end\":58220,\"start\":58219},{\"end\":58604,\"start\":58603},{\"end\":58613,\"start\":58612},{\"end\":58615,\"start\":58614},{\"end\":58628,\"start\":58627},{\"end\":58630,\"start\":58629},{\"end\":58643,\"start\":58642},{\"end\":58645,\"start\":58644},{\"end\":58655,\"start\":58654},{\"end\":58657,\"start\":58656},{\"end\":58666,\"start\":58665},{\"end\":58991,\"start\":58987},{\"end\":59001,\"start\":58997},{\"end\":59262,\"start\":59258},{\"end\":59272,\"start\":59268},{\"end\":59478,\"start\":59474},{\"end\":59488,\"start\":59484},{\"end\":59500,\"start\":59496},{\"end\":59867,\"start\":59866},{\"end\":60282,\"start\":60281},{\"end\":60284,\"start\":60283},{\"end\":60295,\"start\":60294},{\"end\":60297,\"start\":60296},{\"end\":60308,\"start\":60307},{\"end\":60325,\"start\":60324},{\"end\":60329,\"start\":60326},{\"end\":60604,\"start\":60603},{\"end\":60606,\"start\":60605},{\"end\":60617,\"start\":60616},{\"end\":60629,\"start\":60628},{\"end\":60948,\"start\":60947},{\"end\":60957,\"start\":60956},{\"end\":60970,\"start\":60969},{\"end\":61368,\"start\":61367},{\"end\":61370,\"start\":61369},{\"end\":61375,\"start\":61373},{\"end\":61781,\"start\":61777},{\"end\":61788,\"start\":61787},{\"end\":61790,\"start\":61789},{\"end\":62150,\"start\":62146},{\"end\":62157,\"start\":62156},{\"end\":62159,\"start\":62158},{\"end\":62550,\"start\":62546},{\"end\":62560,\"start\":62556},{\"end\":62567,\"start\":62566},{\"end\":62569,\"start\":62568},{\"end\":62852,\"start\":62851},{\"end\":62860,\"start\":62859},{\"end\":62862,\"start\":62861},{\"end\":63092,\"start\":63091},{\"end\":63100,\"start\":63099},{\"end\":63102,\"start\":63101},{\"end\":63486,\"start\":63485},{\"end\":63495,\"start\":63494},{\"end\":63503,\"start\":63502},{\"end\":63514,\"start\":63513},{\"end\":63524,\"start\":63523},{\"end\":63535,\"start\":63534},{\"end\":63545,\"start\":63544},{\"end\":63556,\"start\":63555},{\"end\":63886,\"start\":63885},{\"end\":63895,\"start\":63894},{\"end\":63905,\"start\":63904},{\"end\":64393,\"start\":64392},{\"end\":64402,\"start\":64401},{\"end\":64412,\"start\":64411},{\"end\":64675,\"start\":64674},{\"end\":64685,\"start\":64684},{\"end\":64972,\"start\":64971},{\"end\":64974,\"start\":64973},{\"end\":65445,\"start\":65444},{\"end\":65447,\"start\":65446},{\"end\":65863,\"start\":65862},{\"end\":65873,\"start\":65869},{\"end\":66015,\"start\":66014},{\"end\":66028,\"start\":66027},{\"end\":66037,\"start\":66036},{\"end\":66039,\"start\":66038},{\"end\":66053,\"start\":66052},{\"end\":66055,\"start\":66054},{\"end\":66282,\"start\":66281},{\"end\":66295,\"start\":66294},{\"end\":66297,\"start\":66296},{\"end\":66306,\"start\":66305},{\"end\":66322,\"start\":66321},{\"end\":66324,\"start\":66323},{\"end\":66333,\"start\":66332},{\"end\":66335,\"start\":66334},{\"end\":66595,\"start\":66594},{\"end\":66605,\"start\":66604},{\"end\":66786,\"start\":66785},{\"end\":66988,\"start\":66984},{\"end\":66997,\"start\":66993},{\"end\":67004,\"start\":67003},{\"end\":67006,\"start\":67005},{\"end\":54345,\"start\":54344},{\"end\":54347,\"start\":54346},{\"end\":54356,\"start\":54355},{\"end\":54365,\"start\":54364},{\"end\":54784,\"start\":54780},{\"end\":54796,\"start\":54792},{\"end\":55025,\"start\":55021},{\"end\":55037,\"start\":55033},{\"end\":55251,\"start\":55247},{\"end\":55263,\"start\":55259},{\"end\":55592,\"start\":55588},{\"end\":55603,\"start\":55599},{\"end\":55610,\"start\":55609},{\"end\":55972,\"start\":55968},{\"end\":55983,\"start\":55979},{\"end\":55993,\"start\":55989},{\"end\":56297,\"start\":56296},{\"end\":56307,\"start\":56306},{\"end\":56472,\"start\":56471},{\"end\":56474,\"start\":56473},{\"end\":56483,\"start\":56482},{\"end\":56487,\"start\":56484},{\"end\":56581,\"start\":56579},{\"end\":56583,\"start\":56582},{\"end\":56592,\"start\":56591},{\"end\":56603,\"start\":56599},{\"end\":56937,\"start\":56936},{\"end\":56939,\"start\":56938},{\"end\":56948,\"start\":56947},{\"end\":56959,\"start\":56958},{\"end\":56972,\"start\":56971},{\"end\":57282,\"start\":57278},{\"end\":57292,\"start\":57288},{\"end\":57303,\"start\":57299},{\"end\":57640,\"start\":57639},{\"end\":57648,\"start\":57647},{\"end\":57912,\"start\":57911},{\"end\":57927,\"start\":57926},{\"end\":58209,\"start\":58208},{\"end\":58220,\"start\":58219},{\"end\":58604,\"start\":58603},{\"end\":58613,\"start\":58612},{\"end\":58615,\"start\":58614},{\"end\":58628,\"start\":58627},{\"end\":58630,\"start\":58629},{\"end\":58643,\"start\":58642},{\"end\":58645,\"start\":58644},{\"end\":58655,\"start\":58654},{\"end\":58657,\"start\":58656},{\"end\":58666,\"start\":58665},{\"end\":58991,\"start\":58987},{\"end\":59001,\"start\":58997},{\"end\":59262,\"start\":59258},{\"end\":59272,\"start\":59268},{\"end\":59478,\"start\":59474},{\"end\":59488,\"start\":59484},{\"end\":59500,\"start\":59496},{\"end\":59867,\"start\":59866},{\"end\":60282,\"start\":60281},{\"end\":60284,\"start\":60283},{\"end\":60295,\"start\":60294},{\"end\":60297,\"start\":60296},{\"end\":60308,\"start\":60307},{\"end\":60325,\"start\":60324},{\"end\":60329,\"start\":60326},{\"end\":60604,\"start\":60603},{\"end\":60606,\"start\":60605},{\"end\":60617,\"start\":60616},{\"end\":60629,\"start\":60628},{\"end\":60948,\"start\":60947},{\"end\":60957,\"start\":60956},{\"end\":60970,\"start\":60969},{\"end\":61368,\"start\":61367},{\"end\":61370,\"start\":61369},{\"end\":61375,\"start\":61373},{\"end\":61781,\"start\":61777},{\"end\":61788,\"start\":61787},{\"end\":61790,\"start\":61789},{\"end\":62150,\"start\":62146},{\"end\":62157,\"start\":62156},{\"end\":62159,\"start\":62158},{\"end\":62550,\"start\":62546},{\"end\":62560,\"start\":62556},{\"end\":62567,\"start\":62566},{\"end\":62569,\"start\":62568},{\"end\":62852,\"start\":62851},{\"end\":62860,\"start\":62859},{\"end\":62862,\"start\":62861},{\"end\":63092,\"start\":63091},{\"end\":63100,\"start\":63099},{\"end\":63102,\"start\":63101},{\"end\":63486,\"start\":63485},{\"end\":63495,\"start\":63494},{\"end\":63503,\"start\":63502},{\"end\":63514,\"start\":63513},{\"end\":63524,\"start\":63523},{\"end\":63535,\"start\":63534},{\"end\":63545,\"start\":63544},{\"end\":63556,\"start\":63555},{\"end\":63886,\"start\":63885},{\"end\":63895,\"start\":63894},{\"end\":63905,\"start\":63904},{\"end\":64393,\"start\":64392},{\"end\":64402,\"start\":64401},{\"end\":64412,\"start\":64411},{\"end\":64675,\"start\":64674},{\"end\":64685,\"start\":64684},{\"end\":64972,\"start\":64971},{\"end\":64974,\"start\":64973},{\"end\":65445,\"start\":65444},{\"end\":65447,\"start\":65446},{\"end\":65863,\"start\":65862},{\"end\":65873,\"start\":65869},{\"end\":66015,\"start\":66014},{\"end\":66028,\"start\":66027},{\"end\":66037,\"start\":66036},{\"end\":66039,\"start\":66038},{\"end\":66053,\"start\":66052},{\"end\":66055,\"start\":66054},{\"end\":66282,\"start\":66281},{\"end\":66295,\"start\":66294},{\"end\":66297,\"start\":66296},{\"end\":66306,\"start\":66305},{\"end\":66322,\"start\":66321},{\"end\":66324,\"start\":66323},{\"end\":66333,\"start\":66332},{\"end\":66335,\"start\":66334},{\"end\":66595,\"start\":66594},{\"end\":66605,\"start\":66604},{\"end\":66786,\"start\":66785},{\"end\":66988,\"start\":66984},{\"end\":66997,\"start\":66993},{\"end\":67004,\"start\":67003},{\"end\":67006,\"start\":67005}]", "bib_author_last_name": "[{\"end\":54353,\"start\":54348},{\"end\":54362,\"start\":54357},{\"end\":54372,\"start\":54366},{\"end\":54790,\"start\":54785},{\"end\":54800,\"start\":54797},{\"end\":55031,\"start\":55026},{\"end\":55041,\"start\":55038},{\"end\":55257,\"start\":55252},{\"end\":55267,\"start\":55264},{\"end\":55597,\"start\":55593},{\"end\":55607,\"start\":55604},{\"end\":55620,\"start\":55611},{\"end\":55977,\"start\":55973},{\"end\":55987,\"start\":55984},{\"end\":55997,\"start\":55994},{\"end\":56304,\"start\":56298},{\"end\":56314,\"start\":56308},{\"end\":56480,\"start\":56475},{\"end\":56494,\"start\":56488},{\"end\":56589,\"start\":56584},{\"end\":56597,\"start\":56593},{\"end\":56610,\"start\":56604},{\"end\":56945,\"start\":56940},{\"end\":56956,\"start\":56949},{\"end\":56969,\"start\":56960},{\"end\":56981,\"start\":56973},{\"end\":57286,\"start\":57283},{\"end\":57297,\"start\":57293},{\"end\":57307,\"start\":57304},{\"end\":57645,\"start\":57641},{\"end\":57659,\"start\":57649},{\"end\":57924,\"start\":57913},{\"end\":57932,\"start\":57928},{\"end\":58217,\"start\":58210},{\"end\":58228,\"start\":58221},{\"end\":58610,\"start\":58605},{\"end\":58625,\"start\":58616},{\"end\":58640,\"start\":58631},{\"end\":58652,\"start\":58646},{\"end\":58663,\"start\":58658},{\"end\":58675,\"start\":58667},{\"end\":58995,\"start\":58992},{\"end\":59005,\"start\":59002},{\"end\":59266,\"start\":59263},{\"end\":59276,\"start\":59273},{\"end\":59482,\"start\":59479},{\"end\":59494,\"start\":59489},{\"end\":59504,\"start\":59501},{\"end\":59876,\"start\":59868},{\"end\":60292,\"start\":60285},{\"end\":60305,\"start\":60298},{\"end\":60322,\"start\":60309},{\"end\":60336,\"start\":60330},{\"end\":60614,\"start\":60607},{\"end\":60626,\"start\":60618},{\"end\":60637,\"start\":60630},{\"end\":60954,\"start\":60949},{\"end\":60967,\"start\":60958},{\"end\":60978,\"start\":60971},{\"end\":61383,\"start\":61376},{\"end\":61785,\"start\":61782},{\"end\":61802,\"start\":61791},{\"end\":62154,\"start\":62151},{\"end\":62164,\"start\":62160},{\"end\":62554,\"start\":62551},{\"end\":62564,\"start\":62561},{\"end\":62574,\"start\":62570},{\"end\":62857,\"start\":62853},{\"end\":62868,\"start\":62863},{\"end\":63097,\"start\":63093},{\"end\":63108,\"start\":63103},{\"end\":63492,\"start\":63487},{\"end\":63500,\"start\":63496},{\"end\":63511,\"start\":63504},{\"end\":63521,\"start\":63515},{\"end\":63532,\"start\":63525},{\"end\":63542,\"start\":63536},{\"end\":63553,\"start\":63546},{\"end\":63562,\"start\":63557},{\"end\":63892,\"start\":63887},{\"end\":63902,\"start\":63896},{\"end\":63912,\"start\":63906},{\"end\":64399,\"start\":64394},{\"end\":64409,\"start\":64403},{\"end\":64419,\"start\":64413},{\"end\":64682,\"start\":64676},{\"end\":64696,\"start\":64686},{\"end\":64980,\"start\":64975},{\"end\":65453,\"start\":65448},{\"end\":65867,\"start\":65864},{\"end\":65877,\"start\":65874},{\"end\":66025,\"start\":66016},{\"end\":66034,\"start\":66029},{\"end\":66050,\"start\":66040},{\"end\":66064,\"start\":66056},{\"end\":66292,\"start\":66283},{\"end\":66303,\"start\":66298},{\"end\":66319,\"start\":66307},{\"end\":66330,\"start\":66325},{\"end\":66346,\"start\":66336},{\"end\":66602,\"start\":66596},{\"end\":66615,\"start\":66606},{\"end\":66793,\"start\":66787},{\"end\":66991,\"start\":66989},{\"end\":67001,\"start\":66998},{\"end\":67011,\"start\":67007},{\"end\":54353,\"start\":54348},{\"end\":54362,\"start\":54357},{\"end\":54372,\"start\":54366},{\"end\":54790,\"start\":54785},{\"end\":54800,\"start\":54797},{\"end\":55031,\"start\":55026},{\"end\":55041,\"start\":55038},{\"end\":55257,\"start\":55252},{\"end\":55267,\"start\":55264},{\"end\":55597,\"start\":55593},{\"end\":55607,\"start\":55604},{\"end\":55620,\"start\":55611},{\"end\":55977,\"start\":55973},{\"end\":55987,\"start\":55984},{\"end\":55997,\"start\":55994},{\"end\":56304,\"start\":56298},{\"end\":56314,\"start\":56308},{\"end\":56480,\"start\":56475},{\"end\":56494,\"start\":56488},{\"end\":56589,\"start\":56584},{\"end\":56597,\"start\":56593},{\"end\":56610,\"start\":56604},{\"end\":56945,\"start\":56940},{\"end\":56956,\"start\":56949},{\"end\":56969,\"start\":56960},{\"end\":56981,\"start\":56973},{\"end\":57286,\"start\":57283},{\"end\":57297,\"start\":57293},{\"end\":57307,\"start\":57304},{\"end\":57645,\"start\":57641},{\"end\":57659,\"start\":57649},{\"end\":57924,\"start\":57913},{\"end\":57932,\"start\":57928},{\"end\":58217,\"start\":58210},{\"end\":58228,\"start\":58221},{\"end\":58610,\"start\":58605},{\"end\":58625,\"start\":58616},{\"end\":58640,\"start\":58631},{\"end\":58652,\"start\":58646},{\"end\":58663,\"start\":58658},{\"end\":58675,\"start\":58667},{\"end\":58995,\"start\":58992},{\"end\":59005,\"start\":59002},{\"end\":59266,\"start\":59263},{\"end\":59276,\"start\":59273},{\"end\":59482,\"start\":59479},{\"end\":59494,\"start\":59489},{\"end\":59504,\"start\":59501},{\"end\":59876,\"start\":59868},{\"end\":60292,\"start\":60285},{\"end\":60305,\"start\":60298},{\"end\":60322,\"start\":60309},{\"end\":60336,\"start\":60330},{\"end\":60614,\"start\":60607},{\"end\":60626,\"start\":60618},{\"end\":60637,\"start\":60630},{\"end\":60954,\"start\":60949},{\"end\":60967,\"start\":60958},{\"end\":60978,\"start\":60971},{\"end\":61383,\"start\":61376},{\"end\":61785,\"start\":61782},{\"end\":61802,\"start\":61791},{\"end\":62154,\"start\":62151},{\"end\":62164,\"start\":62160},{\"end\":62554,\"start\":62551},{\"end\":62564,\"start\":62561},{\"end\":62574,\"start\":62570},{\"end\":62857,\"start\":62853},{\"end\":62868,\"start\":62863},{\"end\":63097,\"start\":63093},{\"end\":63108,\"start\":63103},{\"end\":63492,\"start\":63487},{\"end\":63500,\"start\":63496},{\"end\":63511,\"start\":63504},{\"end\":63521,\"start\":63515},{\"end\":63532,\"start\":63525},{\"end\":63542,\"start\":63536},{\"end\":63553,\"start\":63546},{\"end\":63562,\"start\":63557},{\"end\":63892,\"start\":63887},{\"end\":63902,\"start\":63896},{\"end\":63912,\"start\":63906},{\"end\":64399,\"start\":64394},{\"end\":64409,\"start\":64403},{\"end\":64419,\"start\":64413},{\"end\":64682,\"start\":64676},{\"end\":64696,\"start\":64686},{\"end\":64980,\"start\":64975},{\"end\":65453,\"start\":65448},{\"end\":65867,\"start\":65864},{\"end\":65877,\"start\":65874},{\"end\":66025,\"start\":66016},{\"end\":66034,\"start\":66029},{\"end\":66050,\"start\":66040},{\"end\":66064,\"start\":66056},{\"end\":66292,\"start\":66283},{\"end\":66303,\"start\":66298},{\"end\":66319,\"start\":66307},{\"end\":66330,\"start\":66325},{\"end\":66346,\"start\":66336},{\"end\":66602,\"start\":66596},{\"end\":66615,\"start\":66606},{\"end\":66793,\"start\":66787},{\"end\":66991,\"start\":66989},{\"end\":67001,\"start\":66998},{\"end\":67011,\"start\":67007}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":207165665},\"end\":54716,\"start\":54291},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1627812},\"end\":54958,\"start\":54718},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":236168999},\"end\":55198,\"start\":54960},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":961425},\"end\":55545,\"start\":55200},{\"attributes\":{\"id\":\"b4\"},\"end\":55703,\"start\":55547},{\"attributes\":{\"id\":\"b5\"},\"end\":55895,\"start\":55705},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":9417532},\"end\":56270,\"start\":55897},{\"attributes\":{\"id\":\"b7\"},\"end\":56420,\"start\":56272},{\"attributes\":{\"id\":\"b8\"},\"end\":56577,\"start\":56422},{\"attributes\":{\"id\":\"b9\"},\"end\":56829,\"start\":56579},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":10212158},\"end\":57205,\"start\":56831},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":16237677},\"end\":57575,\"start\":57207},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":13899309},\"end\":57845,\"start\":57577},{\"attributes\":{\"id\":\"b13\"},\"end\":58121,\"start\":57847},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":13036203},\"end\":58528,\"start\":58123},{\"attributes\":{\"doi\":\"1539-2791\",\"id\":\"b15\",\"matched_paper_id\":2845142},\"end\":58920,\"start\":58530},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14561315},\"end\":59197,\"start\":58922},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":36777484},\"end\":59420,\"start\":59199},{\"attributes\":{\"id\":\"b18\"},\"end\":59821,\"start\":59422},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":61116019},\"end\":60214,\"start\":59823},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1536643},\"end\":60532,\"start\":60216},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":13375961},\"end\":60847,\"start\":60534},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":59718844},\"end\":61312,\"start\":60849},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":57961414},\"end\":61736,\"start\":61314},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":14152802},\"end\":62080,\"start\":61738},{\"attributes\":{\"id\":\"b25\"},\"end\":62475,\"start\":62082},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6445796},\"end\":62799,\"start\":62477},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":9982978},\"end\":63038,\"start\":62801},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":13581795},\"end\":63405,\"start\":63040},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":7833213},\"end\":63815,\"start\":63407},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2845602},\"end\":64324,\"start\":63817},{\"attributes\":{\"id\":\"b31\"},\"end\":64607,\"start\":64326},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":8826126},\"end\":64889,\"start\":64609},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1099857},\"end\":65342,\"start\":64891},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":56563878},\"end\":65823,\"start\":65344},{\"attributes\":{\"id\":\"b35\"},\"end\":65981,\"start\":65825},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":207673395},\"end\":66220,\"start\":65983},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":2110475},\"end\":66551,\"start\":66222},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":14323205},\"end\":66783,\"start\":66553},{\"attributes\":{\"id\":\"b39\"},\"end\":66907,\"start\":66785},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":7664224},\"end\":67287,\"start\":66909},{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":207165665},\"end\":54716,\"start\":54291},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1627812},\"end\":54958,\"start\":54718},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":236168999},\"end\":55198,\"start\":54960},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":961425},\"end\":55545,\"start\":55200},{\"attributes\":{\"id\":\"b4\"},\"end\":55703,\"start\":55547},{\"attributes\":{\"id\":\"b5\"},\"end\":55895,\"start\":55705},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":9417532},\"end\":56270,\"start\":55897},{\"attributes\":{\"id\":\"b7\"},\"end\":56420,\"start\":56272},{\"attributes\":{\"id\":\"b8\"},\"end\":56577,\"start\":56422},{\"attributes\":{\"id\":\"b9\"},\"end\":56829,\"start\":56579},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":10212158},\"end\":57205,\"start\":56831},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":16237677},\"end\":57575,\"start\":57207},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":13899309},\"end\":57845,\"start\":57577},{\"attributes\":{\"id\":\"b13\"},\"end\":58121,\"start\":57847},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":13036203},\"end\":58528,\"start\":58123},{\"attributes\":{\"doi\":\"1539-2791\",\"id\":\"b15\",\"matched_paper_id\":2845142},\"end\":58920,\"start\":58530},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14561315},\"end\":59197,\"start\":58922},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":36777484},\"end\":59420,\"start\":59199},{\"attributes\":{\"id\":\"b18\"},\"end\":59821,\"start\":59422},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":61116019},\"end\":60214,\"start\":59823},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1536643},\"end\":60532,\"start\":60216},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":13375961},\"end\":60847,\"start\":60534},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":59718844},\"end\":61312,\"start\":60849},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":57961414},\"end\":61736,\"start\":61314},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":14152802},\"end\":62080,\"start\":61738},{\"attributes\":{\"id\":\"b25\"},\"end\":62475,\"start\":62082},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6445796},\"end\":62799,\"start\":62477},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":9982978},\"end\":63038,\"start\":62801},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":13581795},\"end\":63405,\"start\":63040},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":7833213},\"end\":63815,\"start\":63407},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2845602},\"end\":64324,\"start\":63817},{\"attributes\":{\"id\":\"b31\"},\"end\":64607,\"start\":64326},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":8826126},\"end\":64889,\"start\":64609},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1099857},\"end\":65342,\"start\":64891},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":56563878},\"end\":65823,\"start\":65344},{\"attributes\":{\"id\":\"b35\"},\"end\":65981,\"start\":65825},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":207673395},\"end\":66220,\"start\":65983},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":2110475},\"end\":66551,\"start\":66222},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":14323205},\"end\":66783,\"start\":66553},{\"attributes\":{\"id\":\"b39\"},\"end\":66907,\"start\":66785},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":7664224},\"end\":67287,\"start\":66909}]", "bib_title": "[{\"end\":54342,\"start\":54291},{\"end\":54778,\"start\":54718},{\"end\":55019,\"start\":54960},{\"end\":55245,\"start\":55200},{\"end\":55966,\"start\":55897},{\"end\":56294,\"start\":56272},{\"end\":56934,\"start\":56831},{\"end\":57276,\"start\":57207},{\"end\":57637,\"start\":57577},{\"end\":57909,\"start\":57847},{\"end\":58206,\"start\":58123},{\"end\":58601,\"start\":58530},{\"end\":58985,\"start\":58922},{\"end\":59256,\"start\":59199},{\"end\":59864,\"start\":59823},{\"end\":60279,\"start\":60216},{\"end\":60601,\"start\":60534},{\"end\":60945,\"start\":60849},{\"end\":61365,\"start\":61314},{\"end\":61775,\"start\":61738},{\"end\":62544,\"start\":62477},{\"end\":62849,\"start\":62801},{\"end\":63089,\"start\":63040},{\"end\":63483,\"start\":63407},{\"end\":63883,\"start\":63817},{\"end\":64672,\"start\":64609},{\"end\":64969,\"start\":64891},{\"end\":65442,\"start\":65344},{\"end\":66012,\"start\":65983},{\"end\":66279,\"start\":66222},{\"end\":66592,\"start\":66553},{\"end\":66982,\"start\":66909},{\"end\":54342,\"start\":54291},{\"end\":54778,\"start\":54718},{\"end\":55019,\"start\":54960},{\"end\":55245,\"start\":55200},{\"end\":55966,\"start\":55897},{\"end\":56294,\"start\":56272},{\"end\":56934,\"start\":56831},{\"end\":57276,\"start\":57207},{\"end\":57637,\"start\":57577},{\"end\":57909,\"start\":57847},{\"end\":58206,\"start\":58123},{\"end\":58601,\"start\":58530},{\"end\":58985,\"start\":58922},{\"end\":59256,\"start\":59199},{\"end\":59864,\"start\":59823},{\"end\":60279,\"start\":60216},{\"end\":60601,\"start\":60534},{\"end\":60945,\"start\":60849},{\"end\":61365,\"start\":61314},{\"end\":61775,\"start\":61738},{\"end\":62544,\"start\":62477},{\"end\":62849,\"start\":62801},{\"end\":63089,\"start\":63040},{\"end\":63483,\"start\":63407},{\"end\":63883,\"start\":63817},{\"end\":64672,\"start\":64609},{\"end\":64969,\"start\":64891},{\"end\":65442,\"start\":65344},{\"end\":66012,\"start\":65983},{\"end\":66279,\"start\":66222},{\"end\":66592,\"start\":66553},{\"end\":66982,\"start\":66909}]", "bib_author": "[{\"end\":54355,\"start\":54344},{\"end\":54364,\"start\":54355},{\"end\":54374,\"start\":54364},{\"end\":54792,\"start\":54780},{\"end\":54802,\"start\":54792},{\"end\":55033,\"start\":55021},{\"end\":55043,\"start\":55033},{\"end\":55259,\"start\":55247},{\"end\":55269,\"start\":55259},{\"end\":55599,\"start\":55588},{\"end\":55609,\"start\":55599},{\"end\":55622,\"start\":55609},{\"end\":55979,\"start\":55968},{\"end\":55989,\"start\":55979},{\"end\":55999,\"start\":55989},{\"end\":56306,\"start\":56296},{\"end\":56316,\"start\":56306},{\"end\":56482,\"start\":56471},{\"end\":56496,\"start\":56482},{\"end\":56591,\"start\":56579},{\"end\":56599,\"start\":56591},{\"end\":56612,\"start\":56599},{\"end\":56947,\"start\":56936},{\"end\":56958,\"start\":56947},{\"end\":56971,\"start\":56958},{\"end\":56983,\"start\":56971},{\"end\":57288,\"start\":57278},{\"end\":57299,\"start\":57288},{\"end\":57309,\"start\":57299},{\"end\":57647,\"start\":57639},{\"end\":57661,\"start\":57647},{\"end\":57926,\"start\":57911},{\"end\":57934,\"start\":57926},{\"end\":58219,\"start\":58208},{\"end\":58230,\"start\":58219},{\"end\":58612,\"start\":58603},{\"end\":58627,\"start\":58612},{\"end\":58642,\"start\":58627},{\"end\":58654,\"start\":58642},{\"end\":58665,\"start\":58654},{\"end\":58677,\"start\":58665},{\"end\":58997,\"start\":58987},{\"end\":59007,\"start\":58997},{\"end\":59268,\"start\":59258},{\"end\":59278,\"start\":59268},{\"end\":59484,\"start\":59474},{\"end\":59496,\"start\":59484},{\"end\":59506,\"start\":59496},{\"end\":59878,\"start\":59866},{\"end\":60294,\"start\":60281},{\"end\":60307,\"start\":60294},{\"end\":60324,\"start\":60307},{\"end\":60338,\"start\":60324},{\"end\":60616,\"start\":60603},{\"end\":60628,\"start\":60616},{\"end\":60639,\"start\":60628},{\"end\":60956,\"start\":60947},{\"end\":60969,\"start\":60956},{\"end\":60980,\"start\":60969},{\"end\":61373,\"start\":61367},{\"end\":61385,\"start\":61373},{\"end\":61787,\"start\":61777},{\"end\":61804,\"start\":61787},{\"end\":62156,\"start\":62146},{\"end\":62166,\"start\":62156},{\"end\":62556,\"start\":62546},{\"end\":62566,\"start\":62556},{\"end\":62576,\"start\":62566},{\"end\":62859,\"start\":62851},{\"end\":62870,\"start\":62859},{\"end\":63099,\"start\":63091},{\"end\":63110,\"start\":63099},{\"end\":63494,\"start\":63485},{\"end\":63502,\"start\":63494},{\"end\":63513,\"start\":63502},{\"end\":63523,\"start\":63513},{\"end\":63534,\"start\":63523},{\"end\":63544,\"start\":63534},{\"end\":63555,\"start\":63544},{\"end\":63564,\"start\":63555},{\"end\":63894,\"start\":63885},{\"end\":63904,\"start\":63894},{\"end\":63914,\"start\":63904},{\"end\":64401,\"start\":64392},{\"end\":64411,\"start\":64401},{\"end\":64421,\"start\":64411},{\"end\":64684,\"start\":64674},{\"end\":64698,\"start\":64684},{\"end\":64982,\"start\":64971},{\"end\":65455,\"start\":65444},{\"end\":65869,\"start\":65862},{\"end\":65879,\"start\":65869},{\"end\":66027,\"start\":66014},{\"end\":66036,\"start\":66027},{\"end\":66052,\"start\":66036},{\"end\":66066,\"start\":66052},{\"end\":66294,\"start\":66281},{\"end\":66305,\"start\":66294},{\"end\":66321,\"start\":66305},{\"end\":66332,\"start\":66321},{\"end\":66348,\"start\":66332},{\"end\":66604,\"start\":66594},{\"end\":66617,\"start\":66604},{\"end\":66795,\"start\":66785},{\"end\":66993,\"start\":66984},{\"end\":67003,\"start\":66993},{\"end\":67013,\"start\":67003},{\"end\":54355,\"start\":54344},{\"end\":54364,\"start\":54355},{\"end\":54374,\"start\":54364},{\"end\":54792,\"start\":54780},{\"end\":54802,\"start\":54792},{\"end\":55033,\"start\":55021},{\"end\":55043,\"start\":55033},{\"end\":55259,\"start\":55247},{\"end\":55269,\"start\":55259},{\"end\":55599,\"start\":55588},{\"end\":55609,\"start\":55599},{\"end\":55622,\"start\":55609},{\"end\":55979,\"start\":55968},{\"end\":55989,\"start\":55979},{\"end\":55999,\"start\":55989},{\"end\":56306,\"start\":56296},{\"end\":56316,\"start\":56306},{\"end\":56482,\"start\":56471},{\"end\":56496,\"start\":56482},{\"end\":56591,\"start\":56579},{\"end\":56599,\"start\":56591},{\"end\":56612,\"start\":56599},{\"end\":56947,\"start\":56936},{\"end\":56958,\"start\":56947},{\"end\":56971,\"start\":56958},{\"end\":56983,\"start\":56971},{\"end\":57288,\"start\":57278},{\"end\":57299,\"start\":57288},{\"end\":57309,\"start\":57299},{\"end\":57647,\"start\":57639},{\"end\":57661,\"start\":57647},{\"end\":57926,\"start\":57911},{\"end\":57934,\"start\":57926},{\"end\":58219,\"start\":58208},{\"end\":58230,\"start\":58219},{\"end\":58612,\"start\":58603},{\"end\":58627,\"start\":58612},{\"end\":58642,\"start\":58627},{\"end\":58654,\"start\":58642},{\"end\":58665,\"start\":58654},{\"end\":58677,\"start\":58665},{\"end\":58997,\"start\":58987},{\"end\":59007,\"start\":58997},{\"end\":59268,\"start\":59258},{\"end\":59278,\"start\":59268},{\"end\":59484,\"start\":59474},{\"end\":59496,\"start\":59484},{\"end\":59506,\"start\":59496},{\"end\":59878,\"start\":59866},{\"end\":60294,\"start\":60281},{\"end\":60307,\"start\":60294},{\"end\":60324,\"start\":60307},{\"end\":60338,\"start\":60324},{\"end\":60616,\"start\":60603},{\"end\":60628,\"start\":60616},{\"end\":60639,\"start\":60628},{\"end\":60956,\"start\":60947},{\"end\":60969,\"start\":60956},{\"end\":60980,\"start\":60969},{\"end\":61373,\"start\":61367},{\"end\":61385,\"start\":61373},{\"end\":61787,\"start\":61777},{\"end\":61804,\"start\":61787},{\"end\":62156,\"start\":62146},{\"end\":62166,\"start\":62156},{\"end\":62556,\"start\":62546},{\"end\":62566,\"start\":62556},{\"end\":62576,\"start\":62566},{\"end\":62859,\"start\":62851},{\"end\":62870,\"start\":62859},{\"end\":63099,\"start\":63091},{\"end\":63110,\"start\":63099},{\"end\":63494,\"start\":63485},{\"end\":63502,\"start\":63494},{\"end\":63513,\"start\":63502},{\"end\":63523,\"start\":63513},{\"end\":63534,\"start\":63523},{\"end\":63544,\"start\":63534},{\"end\":63555,\"start\":63544},{\"end\":63564,\"start\":63555},{\"end\":63894,\"start\":63885},{\"end\":63904,\"start\":63894},{\"end\":63914,\"start\":63904},{\"end\":64401,\"start\":64392},{\"end\":64411,\"start\":64401},{\"end\":64421,\"start\":64411},{\"end\":64684,\"start\":64674},{\"end\":64698,\"start\":64684},{\"end\":64982,\"start\":64971},{\"end\":65455,\"start\":65444},{\"end\":65869,\"start\":65862},{\"end\":65879,\"start\":65869},{\"end\":66027,\"start\":66014},{\"end\":66036,\"start\":66027},{\"end\":66052,\"start\":66036},{\"end\":66066,\"start\":66052},{\"end\":66294,\"start\":66281},{\"end\":66305,\"start\":66294},{\"end\":66321,\"start\":66305},{\"end\":66332,\"start\":66321},{\"end\":66348,\"start\":66332},{\"end\":66604,\"start\":66594},{\"end\":66617,\"start\":66604},{\"end\":66795,\"start\":66785},{\"end\":66993,\"start\":66984},{\"end\":67003,\"start\":66993},{\"end\":67013,\"start\":67003}]", "bib_venue": "[{\"end\":54447,\"start\":54374},{\"end\":54820,\"start\":54802},{\"end\":55061,\"start\":55043},{\"end\":55323,\"start\":55269},{\"end\":55586,\"start\":55547},{\"end\":55757,\"start\":55707},{\"end\":56035,\"start\":55999},{\"end\":56332,\"start\":56316},{\"end\":56469,\"start\":56422},{\"end\":56661,\"start\":56612},{\"end\":56997,\"start\":56983},{\"end\":57345,\"start\":57309},{\"end\":57697,\"start\":57661},{\"end\":57970,\"start\":57934},{\"end\":58293,\"start\":58230},{\"end\":58702,\"start\":58686},{\"end\":59043,\"start\":59007},{\"end\":59294,\"start\":59278},{\"end\":59472,\"start\":59422},{\"end\":59929,\"start\":59878},{\"end\":60356,\"start\":60338},{\"end\":60675,\"start\":60639},{\"end\":61038,\"start\":60980},{\"end\":61436,\"start\":61385},{\"end\":61873,\"start\":61804},{\"end\":62144,\"start\":62082},{\"end\":62592,\"start\":62576},{\"end\":62906,\"start\":62870},{\"end\":63184,\"start\":63110},{\"end\":63592,\"start\":63564},{\"end\":64011,\"start\":63914},{\"end\":64390,\"start\":64326},{\"end\":64731,\"start\":64698},{\"end\":65033,\"start\":64982},{\"end\":65491,\"start\":65455},{\"end\":65860,\"start\":65825},{\"end\":66084,\"start\":66066},{\"end\":66366,\"start\":66348},{\"end\":66653,\"start\":66617},{\"end\":66822,\"start\":66795},{\"end\":67049,\"start\":67013},{\"end\":54447,\"start\":54374},{\"end\":54820,\"start\":54802},{\"end\":55061,\"start\":55043},{\"end\":55323,\"start\":55269},{\"end\":55586,\"start\":55547},{\"end\":55757,\"start\":55707},{\"end\":56035,\"start\":55999},{\"end\":56332,\"start\":56316},{\"end\":56469,\"start\":56422},{\"end\":56661,\"start\":56612},{\"end\":56997,\"start\":56983},{\"end\":57345,\"start\":57309},{\"end\":57697,\"start\":57661},{\"end\":57970,\"start\":57934},{\"end\":58293,\"start\":58230},{\"end\":58702,\"start\":58686},{\"end\":59043,\"start\":59007},{\"end\":59294,\"start\":59278},{\"end\":59472,\"start\":59422},{\"end\":59929,\"start\":59878},{\"end\":60356,\"start\":60338},{\"end\":60675,\"start\":60639},{\"end\":61038,\"start\":60980},{\"end\":61436,\"start\":61385},{\"end\":61873,\"start\":61804},{\"end\":62144,\"start\":62082},{\"end\":62592,\"start\":62576},{\"end\":62906,\"start\":62870},{\"end\":63184,\"start\":63110},{\"end\":63592,\"start\":63564},{\"end\":64011,\"start\":63914},{\"end\":64390,\"start\":64326},{\"end\":64731,\"start\":64698},{\"end\":65033,\"start\":64982},{\"end\":65491,\"start\":65455},{\"end\":65860,\"start\":65825},{\"end\":66084,\"start\":66066},{\"end\":66366,\"start\":66348},{\"end\":66653,\"start\":66617},{\"end\":66822,\"start\":66795},{\"end\":67049,\"start\":67013},{\"end\":54507,\"start\":54449},{\"end\":56676,\"start\":56663},{\"end\":58343,\"start\":58295},{\"end\":59990,\"start\":59977},{\"end\":61497,\"start\":61484},{\"end\":61929,\"start\":61875},{\"end\":63245,\"start\":63186},{\"end\":64095,\"start\":64013},{\"end\":65094,\"start\":65081},{\"end\":65560,\"start\":65547},{\"end\":66836,\"start\":66824},{\"end\":54507,\"start\":54449},{\"end\":56676,\"start\":56663},{\"end\":58343,\"start\":58295},{\"end\":59990,\"start\":59977},{\"end\":61497,\"start\":61484},{\"end\":61929,\"start\":61875},{\"end\":63245,\"start\":63186},{\"end\":64095,\"start\":64013},{\"end\":65094,\"start\":65081},{\"end\":65560,\"start\":65547},{\"end\":66836,\"start\":66824}]"}}}, "year": 2023, "month": 12, "day": 17}