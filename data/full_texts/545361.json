{"id": 545361, "updated": "2023-07-28 15:40:23.486", "metadata": {"title": "Indoor Segmentation and Support Inference from RGBD Images", "authors": "[{\"first\":\"Nathan\",\"last\":\"Silberman\",\"middle\":[]},{\"first\":\"Derek\",\"last\":\"Hoiem\",\"middle\":[]},{\"first\":\"Pushmeet\",\"last\":\"Kohli\",\"middle\":[]},{\"first\":\"Rob\",\"last\":\"Fergus\",\"middle\":[]}]", "venue": "ECCV", "journal": "746-760", "publication_date": {"year": 2012, "month": null, "day": null}, "abstract": ". We present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into \ufb02oor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute a novel integer programming formulation to infer physical support relations. We o\ufb00er a new dataset of 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues and inferred support lead to better object segmentation.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "125693051", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/SilbermanHKF12", "doi": "10.1007/978-3-642-33715-4_54"}}, "content": {"source": {"pdf_hash": "e2588375e2a88990080be5564ca22114049e5319", "pdf_src": "Adhoc", "pdf_uri": "[\"https://web.archive.org/web/20200214050636/https:/link.springer.com/content/pdf/10.1007/978-3-642-33715-4_54.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7aa2d9a487cc26c9a4bfcac9884afaad62ebed5b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e2588375e2a88990080be5564ca22114049e5319.txt", "contents": "\nIndoor Segmentation and Support Inference from RGBD Images\n\n\nNathan Silberman \nCourant Institute\nNew York University\n\n\nDerek Hoiem \nDepartment of Computer Science\nUniversity of Illinois at Urbana-Champaign\n\n\nPushmeet Kohli \nMicrosoft Research\nCambridge\n\nRob Fergus \nCourant Institute\nNew York University\n\n\nIndoor Segmentation and Support Inference from RGBD Images\n\nWe present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into floor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute a novel integer programming formulation to infer physical support relations. We offer a new dataset of 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues and inferred support lead to better object segmentation.\n\nIntroduction\n\nTraditional approaches to scene understanding aim to provide labels for each object in the image. However, this is an impoverished description since labels tell us little about the physical relationships between objects, possible actions that can be performed, or the geometric structure of the scene.\n\nMany robotics and scene understanding applications require a physical parse of the scene into objects, surfaces, and their relations. A person walking into a room, for example, might want to find his coffee cup and favorite book, grab them, find a place to sit down, walk over, and sit down. These tasks require parsing the scene into different objects and surfaces -the coffee cup must be distinguished from surrounding objects and the supporting surface for example. Some tasks also require understanding the interactions of scene elements: if the coffee cup is supported by the book, then the cup must be lifted first.\n\nIn this paper, our goal is to provide such a physical scene parse: to segment visible regions into surfaces and objects and to infer their support relations. In particular, we are interested in indoor scenes that reflect typical living conditions. Challenges include the well-known difficulty of object segmentation, prevalence of small objects, and heavy occlusion, which are all compounded by the mess and disorder that are common in lived-in rooms. What makes interpretation possible at all is the rich geometric structure: most rooms are composed of large planar surfaces, such as the floor, walls, and table tops, and objects can often be interpreted in relation to those surfaces. We can better interpret the room by rectifying our visual data with the room's geometric structure.\n\nOur approach, illustrated in Fig. 1, is to first infer the overall 3D structure of the scene and then jointly parse the image into separate objects and estimate their support relations. Some tasks, such as estimating the floor orientation or finding large planar surfaces are much easier with depth information, which is easy to acquire indoors. But other tasks, such as segmenting and classifying objects require appearance based cues. Thus, we use depth cues to sidestep the common geometric challenges that bog down single-view image-based approaches, enabling a more detailed and accurate geometric structure. We are then able to focus on properly leveraging this structure to jointly segment the objects and infer support relations, using both image and depth cues. One of our innovations is to classify objects into structural classes that reflect their physical role in the scene: \"ground\"; \"permanent structures\" such as walls, ceilings, and columns; large \"furniture\" such as tables, dressers, and counters; and \"props\" which are easily movable objects. We show that these structural classes aid both segmentation and support estimation.\n\nTo reason about support, we introduce a principled approach that integrates physical constraints (e.g. is the object close to its putative supporting object?) and statistical priors on support relationships (e.g. mugs are often supported by tables, but rarely by walls). Our method is designed for real-world scenes that contain tens or hundred of objects with heavy occlusion and clutter. In this setting, interfaces between objects are often not visible and thus must be inferred. Even without occlusion, limited image resolution can make support ambiguous, necessitating global reasoning between image regions. Real-world images also contain significant variation in focal length. While wide-angle shots contain many objects, narrow-angle views can also be challenging as important structural elements of the scene, such as the floor, are not observed. Our scheme is able to handle these situations by inferring the location of invisible elements and how they interact with the visible components of the scene.\n\n\nRelated Work\n\nOur overall approach of incorporating geometric priors to improve scene interpretation is most related to a set of image-based single-view methods (e.g. [1][2][3][4][5][6][7]). Our use of \"structural classes\", such as \"furniture\" and \"prop\", to improve segmentation and support inference relates to the use of \"geometric classes\" [1] to segment objects [8] or volumetric scene parses [3,[5][6][7]. Our goal of inferring support relations is most closely related to Gupta et al. [6], who apply heuristics inspired by physical reasoning to infer volumetric shapes, occlusion, and support in outdoor scenes. Our 3D cues provide a much stronger basis for inference of support, and our dataset enables us to train and evaluate support predictors that can cope with scene clutter and invisible supporting regions. Russell and Torralba [9] show how a dataset of user-annotated scenes can be used to infer 3D structure and support; our approach, in contrast, is fully automatic.  Given an input image with raw and inpainted depth maps, we compute surface normals and align them to the room by finding three dominant orthogonal directions. We then fit planes to the points using RANSAC and segment them based on depth and color gradients. Given the 3D scene structure and initial estimates of physical support, we then create a hierarchical segmentation and infer the support structure. In the surface normal images, the absolute value of the three normal directions is stored in the R, G, and B channels. The 3D planes are indicated by separate colors. Segmentation is indicated by red boundaries. Arrows point from the supported object to the surface that supports it.\n\nOur approach to estimate geometric structure from depth cues is most closely related to Zhang et al. [10]. After estimating depth from a camera on a moving vehicle, Zhang et al. use RANSAC to fit a ground plane and represent 3D scene points relative to the ground and direction of the moving vehicle. We use RANSAC on 3D points to initialize plane fitting but also infer a segmentation and improved plane parameters using a graph cut segmentation that accounts for 3D position, 3D normal, and intensity gradients. Their application is pixel labeling, but ours is parsing into regions and support relations. Others, such as Silberman et al. [11] and Karayev et al. [12] use RGBD images from the Kinect for object recognition, but do not consider tasks beyond category labeling.\n\nTo summarize, the most original of our contributions is the inference of support relations in complex indoor scenes. We incorporate geometric structure inferred from depth, object properties encoded in our structural classes, and data-driven scene priors, and our approach is robust to clutter, stacked objects, and invisible supporting surfaces. We also contribute ideas for interpreting geometric structure from a depth image, such as graph cut segmentation of planar surfaces and ways to use the structure to improve segmentation. Finally, we offer a new large dataset with registered RGBD images, detailed object labels, and annotated physical relations.\n\n\nDataset for Indoor Scene Understanding\n\nSeveral Kinect scene datasets have recently been introduced. However, the NYU indoor scene dataset [11] has limited diversity (only 67 scenes); in the Berkeley Scenes dataset [12] only a few objects per scene are labeled; and others such as [13,14] are designed for robotics applications. We therefore introduce a new Kinect dataset 1 , significantly larger and more diverse than existing ones.\n\nThe dataset consists of 1449 RGBD images 2 , gathered from a wide range of commercial and residential buildings in three different US cities, comprising 464 different indoor scenes across 26 scene classes.A dense per-pixel labeling was obtained for each image using Amazon Mechanical Turk. If a scene contained multiple instances of an object class, each instance received a unique instance label, e.g. two different cups in the same image would be labeled: cup 1 and cup 2, to uniquely identify them. The dataset contains 35,064 distinct objects, spanning 894 different classes. For each of the 1449 images, support annotations were manually added. Each image's support annotations consists of a set of 3tuples:\n[R i , R j , type]\nwhere R i is the region ID of the supported object, R j is the region ID of the supporting object and type indicates whether the support is from below (e.g. cup on a table) or from behind (e.g. picture on a wall). Examples of the dataset are found in Fig 7 (object category labels not shown).\n\n\nModeling the Structure of Indoor Scenes\n\nIndoor scenes are usually arranged with respect to the orthogonal orientations of the floor and walls and the major planar surfaces such as supporting surfaces, floor, walls, and blocky furnishings. We treat initial inference of scene surfaces as an alignment and segmentation problem. We first compute surface normals from the depth image. Then, based on surface normals and straight lines, we find three dominant and orthogonal scene directions and rotate the 3D coordinates to be axis aligned with the principal directions. Finally, we propose 3D planes using RANSAC on the 3D points and segment the visible regions into one of these planes or background using graph cuts based on surface normals, 3D points, and RGB gradients. Several examples are shown in Fig. 2. We now describe each stage of this procedure in more detail.\n\n\nAligning to Room Coordinates\n\nWe are provided with registered RGB and depth images, with in-painted depth pixels [15]. We compute 3D surface normals at each pixel by sampling surrounding pixels within a depth threshold and fitting a least squares plane. For each pixel we have an image coordinate (u, v), 3D coordinate (X, Y , Z), and surface normal (N X , N Y , N Z ). Our first step is to align our 3D measurements to room coordinates, so that the floor points upwards (N Y =1) and each wall's normal is in the X or Z direction. Our alignment is based on the Manhattan world assumption [16], that many visible surfaces will be along one of three orthogonal directions. To obtain candidates for the principal directions, we extract straight lines from the image and compute mean-shift modes of surface normals. Straight line segments are extracted from the RGB image using the method described by Kosecka et al. [17] and the 3D coordinates along each line are recorded. We compute the 3D direction of each line using SVD to find the direction of maximum variance. Typically, we have 100-200 candidates of principal directions. For each candidate that is approximately in the Y direction, we sample two orthogonal candidates and compute the score of the triple as follows:\n(a) Input RGB (b) Normals (c) Aligned (d) RANSAC Fit (e) SegmentedS(v 1 , v 2 , v 3 ) = 3 j=1 [ w N N N NN i exp( \u2212(N i \u00b7 v j ) 2 \u03c3 2 ) + w L N L NL i exp(\u2212 (L i \u00b7 v j ) 2 \u03c3 2 )] (1)\nwhere v 1 , v 2 , v 3 are the three principal directions, N i is the surface normal of a pixel, L i is the direction of a straight line, N N and N L are the number of surface points and lines, and w N and w L are weights of the 3D normal and line scores. In experiments, we set w N = 0.7, w L = 0.3, and \u03c3 = 0.01. We choose the set of candidates that has the largest score, and denote them by v X , v Y , and v Z , where v Y is chosen to be the direction closest to the original Y direction. We can then align the 3D points, normals, and planes of the scene using the ro-\ntation matrix R = [v X v Y v Z ].\nAs shown in Fig. 3, the alignment procedure brings 80% of scene floors within < 5 \u2022 of vertical, compared to 5% beforehand.\n\n\nProposing and Segmenting Planes\n\nWe generate potential wall, floor, support, and ceiling planes using a RANSAC procedure. Several hundred points along the grid of pixel coordinates are sampled, together with nearby points at a fixed distance (e.g., 20 pixels) in the horizontal and vertical directions. While thousands of planes are proposed, only planes above a threshold (2500) of inlier pixels after RANSAC and non-maximal suppression are retained.\n\nTo determine which image pixels correspond to each plane, we solve a segmentation using graph cuts with alpha expansion based on the 3D points X, the surface normals N and the RGB intensities I of each pixel. Each pixel i is assigned a plane label y i = 0..N p for N p planes (y i = 0 signifies no plane) to minimize the following energy:\nE(data, y) = \u03b1 i i f 3d (X i , y i ) + f norm (N i , y i ) + i,j\u2208N8 f pair (y i , y j , I) (2)\nThe unary terms f 3d and f norm encode whether the 3D values and normals at a pixel match those of the plane. Each term is defined as log P r(dist|inlier) P r(dist|outlier) , the log ratio of the probability of the distance between the pixel's 3D position or normal compared to that of the plane, given that the pixel is an inlier or outlier. The probabilities are computed using histograms with 100 bins using the RANSAC inlier/outlier estimates to initialize. The unary terms are weighted by \u03b1 i , according to whether we have directly recorded depth measurements (\u03b1 i = 1), inpainted depth measurements (\u03b1 i = 0.25), or no depth measurements (\u03b1 i = 0) at each pixel. 1(.) is an indicator function. The pairwise term f pair (y i , y j , I) = \u03b2 1 + \u03b2 2 ||I i \u2212 I j || 2 enforces gradient-sensitive smoothing. In our experiments, \u03b2 1 = 1 and \u03b2 2 = 45/\u03bc g , where \u03bc g is the average squared difference of intensity values for pixels connected within N 8 , the 8-connected neighborhood.\n\n\nSegmentation\n\nIn order to classify objects and interpret their relations, we must first segment the image into regions that correspond to individual object or surface instances. Starting from an oversegmentation, pairs of regions are iteratively merged based on learned similarities. The key element is a set of classifiers trained to predict whether two regions correspond to the same object instance based on cues from the RGB image, the depth image, and the estimated scene structure (Sec. 3).\n\nTo create an initial set of regions, we use the watershed algorithm applied to Pb boundaries, as first suggested by Arbeleaz [18]. We force this oversegmentation to be consistent with the 3D plane regions described in Sec. 3, which primarily helps to avoid regions that span wall boundaries with faint intensity edges. We also experimented with incorporating edges from depth or surface orientation maps, but found them unhelpful, mostly because discontinuities in depth or surface orientation are usually manifest as intensity discontinuities. Our oversegmentation typically provides 1000-2000 regions, such that very few regions overlap more than one object instance.\n\nFor hierarchical segmentation, we adapt the algorithm and code of Hoiem et al. [8]. Regions with minimum boundary strength are iteratively merged until the minimum cost reaches a given threshold. Boundary strengths are predicted by a trained boosted decision tree classifier as P(y i = y j |x s ij ), where y i is the instance label of the i th region and x s ij are paired region features. The classifier is trained using similar RGB and position features 3 to Hoiem et al. [8], but the \"geometric context\" features are replaced with ones using more reliable depthbased cues. These proposed 3D features encode regions corresponding to different planes or having different surface orientations or depth differences are likely to belong to different objects. Both types of features are important: 3D features help differentiate between texture and objects edges, and standard 2D features are crucial for nearby or touching objects.\n\n\nModeling Support Relationships\n\n\nThe Model\n\nGiven an image split into R regions, we denote by S i : i = 1..R the hidden variable representing a region's physical support relation. The basic assumption made by our model is that every region is either (a) supported by a region visible in the image plane, in which case S i \u2208 {1..R}, (b) supported by an object not visible in the image plane, S i = h, or (c) requires no support indicating that the region is the ground itself, S i = g. Additionally, let T i encode whether region i is supported from below (T i = 0) or supported from behind (T i = 1).\n\nWhen inferring support, prior knowledge of object types can be reliable predictors of the likelihoods of support relations. For example, it is unlikely that a piece of fruit is supporting a couch. However, rather that attempt to model support in terms of object classes, we model each region's structure class M i , where M i can take on one of the following values: Ground (M i = 1), Furniture (M i = 2), Prop (M i = 3) or Structure (M i = 4). We map each object in our densely labeled dataset to one of these four structure classes. Props are small objects that can be easily carried; furniture are large objects that cannot. Structure refers to non-floor parts of a room (walls, ceiling, columns). We map each object in our labeled dataset to one of these structure classes.\n\nWe want to infer the most probable joint assignment of support regions S = \n\nwhere E(S, T, M|I) = \u2212 log P (S, T, M|I) is the energy of the labeling. The posterior distribution of our model factorizes into likelihood and prior terms as\nP (S, T, M|I) \u221d R i=1 P (I|S i , T i )P (I|M i ) P (S, T, M)( 4 )\nto give the energy The prior E P is composed of a number of different terms, and is formally defined as:\nE(S, T, M) = \u2212 R i=1 log(D s (F s i,Si |S i , T i )+log(D m (F m i |M i ))+E P (S,E P (S, T, M) = R i=1 \u03c8 T C (M i , M Si , T i ) + \u03c8 SC (S i , T i ) + \u03c8 GC (S i , M i ) + \u03c8 GGC (M). (6)\nThe transition prior, \u03c8 T C , encodes the probability of regions belonging to different structure classes supporting each other. It takes the following form:\n\u03c8 T C (M i , M Si , T i ) \u221d \u2212 log z\u2208supportLabels \u00bd[z = [M i , M Si , T i ]] z\u2208supportLabels \u00bd[z = [M i , * , T i ]](7)\nThe support consistency term, \u03c8 SC (S i , T i ), ensures that the supported and supporting regions are close to each other. Formally, it is defined as:\n\u03c8 SC (S i , T i ) = (H b i \u2212 H t Si ) 2 if T i = 0, V (i, S i ) 2 if T i = 1,(8)\nwhere H b i and H t Si are the lowest and highest points in 3D of region i and S i respectively, as measured from the ground, and V (i, S i ) is the minimum horizontal distance between regions i and S i .\n\nThe ground consistency term \u03c8 GC (S i , M i ) has infinite cost if S i = g \u2227 M i = 1 and 0 cost otherwise, enforcing that all non-ground regions must be supported.\n\nThe global ground consistency term \u03c8 GGC (M) ensures that the region taking the floor label is lower than other regions in the scene. Formally, it is defined as:\n\u03c8 GGC (M) = R i=1 R j=1 \u03ba if M i = 1 \u2227 H b i > H b j 0 o t h e r w i s e ,(9)\n\nInteger Program Formulation\n\nThe maximum a posteriori (MAP) inference problem defined in equation (3) can be formulated in terms of an integer program. This requires the introduction of boolean indicator variables to represent the different configurations of the unobserved variables S, M and T. Let R = R + 1 be the total number of regions in the image plus a hidden region assignment. For each region i, let boolean variables s i,j : 1 \u2264 j \u2264 2R + 1 represent both S i and T i as follows: s i,j : 1 < j \u2264 R indicate that region i is supported from below (T i = 0) by regions {1, ..., R, h}. Next, s i,j : R < j \u2264 2R indicate that region i is supported from behind (T i = 1) by regions {1, ..., R, h}. Finally, variable s i,2R +1 indicates whether or not region i is the ground (S i = g).\n\nFurther, we will use boolean variables m i,u = 1 to indicate that region i belongs to structure class u, and indicator variables w u,v i,j to represent s i,j = 1, m i,u = 1 and m j,v = 1. Using this over-complete representation we can formulate the MAP inference problem as an Integer Program using equations 10-16. arg min\ns,m,w i,j \u03b8 s i,j s i,j + i,u \u03b8 m i,u m i,u + i,j,u,v \u03b8 w i,j,u,v w u,v i,j(10)s.t. j s i,j = 1, u m i,u = 1 \u2200i (11) j,u,v w u,v i,j = 1, \u2200i(12)s i,2R +1 = m i,1 , \u2200i (13) u,v w u,v i,j = s i,j , \u2200u, v (14) j,v w u,v i,j \u2264 m i,u , \u2200i, u(15)s i,j , m i,u , w u,v i,j \u2208 {0, 1}, \u2200i, j, u, v(16)\nThe support likelihood D s (eq. 5) and the support consistency \u03c8 SC (eq. 8) terms of the energy are encoded in the IP objective though coefficients \u03b8 s i,j . The structure class likelihood D m (eq. 5) and the global ground consistency \u03c8 GGC (eq. 9) terms are encoded in the objective through coefficients \u03b8 m i,u . The transition prior \u03c8 T C (eq. 7) is encoded using the parameters \u03b8 w i,j,u,v . Constraints 11 and 12 ensure that each region is assigned a single support, type and structure label. Constraint 13 satisfies the Ground Consistency \u03c6 GC term. Constraints 14 and 15 are marginalization and consistency constraints.\n\nFinally, constraint 16 ensure that all indicator variables take integral values. It is NP-hard to solve the integer program defined in equations 10-16. We reformulate the constraints as a linear program, which we solve using Gurobi's LP solver, by relaxing the integrality constraints 16 to:\ns i,j , m i,u , w u,v i,j \u2208 [0, 1], \u2200i, j, u, v.(17)\nFractional solutions are resolved by setting the most likely support, type and structure class to 1 and the remaining values to zero. In our experiments, we found this relation to be tight in that the duality gap was 0 in 1394/1449 images.\n\n\nSupport Features and Local Classification\n\nOur support features capture individual and pairwise characteristics of regions. Such characteristics are not symmetric: feature vector F s i,j would be used to determine whether i supports j but not vice versa. Geometrical features encode proximity and containment, e.g. whether one region contains another when projected onto the ground plane. Shape features are important for capturing characteristics of different supporting objects: objects that support others from below have large horizontal components and those that support from behind have large vertical components. Finally, location features capture the absolute 3d locations of the candidate objects. 4 To train D s , a logistic regression classifier, each feature vector F S i,j is paired with a label Y S \u2208 {1..4} which indicates whether (1) i is supported from below by j, (2) i is supported from behind by j, (3) j represents the ground or (4) no relationship exists between the two regions. Predicting whether j is the ground is necessary for computing D s (S i = g, T i = 0; F S i,g ) such that Si,Ti D s (S i , T i ; F S i,Si ) is a proper probability distribution.\n\n\nStructure Class Features and Local Classification\n\nOur structure class features are similar to those that have been used for object classification in previous works [14]. They include SIFT features, histograms of surface normals, 2D and 3D bounding box dimensions, color histograms [19] and relative depth [11] 4 . A logistic regression classifier is trained to predict the correct structure class for each region of the image, and the output of the classifier is interpreted as probability for the likelihood term D m .\n\n\nExperiments\n\n\nEvaluating Segmentation\n\nTo evaluate our segmentation algorithm, we use the overlap criteria from [8]. As shown in Table 1, the combination of RGB and Depth features outperform each set of features individually by margins of 10% and 7%, respectively, using the area-weighted score. We also performed two additional segmentation experiments in which at each stage of the segmentation, we extracted and classified support and structure class features from the intermediate segmentations and used the support and structure classifier output as features for boundary classification. The addition of these features both improve segmentation performance with Support providing a slightly larger gain.\n\n\nEvaluating Support\n\nBecause the support labels are defined in terms of ground truth regions, we must map the relationships onto the segmented regions. To avoid penalizing the support inference for errors in the bottom up segmentation, the mapping is performed as follows: each support label from the ground truth region\n[R GT i , R GT j , T ] is replaced with a set of labels [R S a1 , R S b1 , T ]...[R S aw , R S bw , T ]\nwhere the overlap between supported regions (R GT i ,R S aw ) and supporting regions, (R GT j ,R S bw ) exceeds a threshold (.25).\n\nWe evaluate our support inference model against several baselines:\n\n-Image Plane Rules: A Floor Classifier is trained in order to assign S i = g properly. For the remaining regions: if a region is completely surrounded by another region in the image plane, then a support-from-behind relationship is assigned to the pair with the smaller region as the supported region. Otherwise, for each candidate region, choose the region directly below it as its support from below. -Structure Class Rules: A classifier is trained to predict each region's structure class. If a region is predicted to be a floor, S i = g is assigned.\n\nRegions predicted to be of Structure class Furniture or Structure are assigned the support of the nearest floor region. Finally, Props are assigned support from below by the region directly beneath them in the image plane. -Support Classifier: For each region in the image, we infer the likelihood of support between it and every other region in the image using D s and assign each region the most likely support relation indicated by the support classifier score.\n\nThe metric used for evaluation is the number of regions for which we predict a correct support divided by the total number of regions which have a support label. We also differentiate between Type Agnostic accuracy, in which we consider a predicted support relation correct regardless of whether the support type (below or from behind) matched the label and Type Aware accuracy in which only a prediction of the correct type is considered a correct support prediction. We also evaluate each method on both the ground truth regions and regions generated by the bottom up segmentation. Results for support classification are listed in Table 2. When using the ground truth regions, the Image Plane Rules and Structure Class Rules perform well  Fig. 6. Accuracy of the structure class recognition Fig. 7. Examples of support and structure class inference with the LP solution. \u2192 : support from below, : support from behind, + : support from hidden region. Correct support predictions in green, incorrect in red. Ground in pink, Furniture in Purple, Props in Blue, Structure in Yellow, Grey indicates missing structure class label. Incorrect structure predictions are striped.\n\n\nGround Truth Regions\n\n\nSegmented Regions\n\ngiven their simplicity. Indeed, when using ground truth regions, the Structure Class Rules prove superior to the support classifier alone, demonstrating the usefulness of the Structure categories. However, both rule-based approaches cannot handle occlusion well nor are they particularly good at inferring the type of support involved. When considering the support type, our energy based model improves on the Structure Class Rules by 9% and 17% when using the ground truth and segmented regions, respectively, demonstrating the need to take into account a combination of global reasoning and discriminative inference.\n\nVisual examples are shown in Fig 7. They demonstrate that many objects, such as the right dresser in the row3, column 3 and the chairs in row 5, column 1, are supported by regions that are far from them in the image plane, necessitating non-local inference. One of the main stumbling blocks of the algorithm is incorrect floor classification as show in the 3rd image of the last row. Incorrectly labeling the rug as the floor creates a cascade of errors since the walls and bed rely on this as support rather than using the true floor. Additionally, incorrect structure class prediction can lead to incorrect support inference, such as the objects on the table in row 4, column 1.\n\n\nEvaluating Structure Class Prediction\n\nTo evaluate the structure class prediction, we calculate both the overall accuracy and the mean diagonal of the confusion matrix. As 6 indicates, the LP solution makes a small improvement over the local structure class prediction. Structure class accuracy often struggles when the depth values are noisy or when the segmentation incorrectly merges two regions of different structure class.\n\n\nConclusion\n\nWe have introduced a new dataset useful for various tasks including recognition, segmentation and inference of physical support relationships. Our dataset is unique in the diversity and complexity of depicted indoor scenes, and we provide an approach to parse such complex environments through appearance cues, room-aligned 3D cues, surface fitting, and scene priors. Our experiments show that we can reliably infer the supporting region and the type of support, especially when segmentations are accurate. We also show that initial estimates of support and major surfaces lead to better segmentation. Future work could include inferring the full extent of objects and surfaces and categorizing objects.\n\nFig. 1 .\n1Overview of algorithm. Our algorithm flows from left to right.\n\nFig. 2 .\n2Scene Structure Estimation. Given an input image (a), we compute surface normals (b) and align the normals (c) to the room. We then use RANSAC to generate several plane candidates which are sorted by number of inliers (d). Finally, we segment the visible portions of the planes using graph cuts (e). Top row: a typical indoor scene with a rectangular layout. Bottom row: an scene with many oblique angles; floor orientation is correctly recovered.\n\nFig. 3 .\n3Alignment of Floors\n\nFig. 4 .\n4Segmentation Examples. We show two examples of hierarchical segmentation. Starting with roughly 1500 superpixels (not shown), our algorithm iteratively merges regions based on the likelihood of two regions belonging to the same object instance. For the final segmentation, no two regions have greater than 50% chance of being part of the same object.\n\n\n{S 1 , ...S R }, support types T \u2208 {0, 1} R and structure classes M \u2208 {1..4} R . More formally, {S * , T * , M * } = arg max S,T,M P (S, T, M|I) = arg min S,T,M E(S, T, M|I),\n\n\nT, M). (5) where F S i,Si are the support features for regions i and S i , and D s is a Support Relation classifier trained to maximize P (F S i,Si |S i , T i ). F M i are the structure features for region i and D m is a Structure classifier trained to maximize P (F M i |M i ). The specifics regarding training and choice of features for both classifiers are found in sections 5.3 and 5.4, respectively.\n\nTable 1 .\n1Accuracy of hierarchical segmentation, measured as average overlap over \nground truth regions for best-matching segmented region, either weighted by pixel \narea or unweighted \n\nFeatures \nWeighted Score Unweighted Score \nRGB Only \n52.5 \n48.7 \nDepth Only \n55.9 \n47.3 \nRGBD \n62.7 \n52.7 \nRGBD + Support \n63.4 \n53.7 \nRGBD + Support + Structure classes \n63.9 \n54.1 \n\n\n\nTable 2 .\n2Results of the various approaches to support inference. Accuracy is measured by total regions whose support is correctly inferred divided by the number of labeled regions. Type Aware accuracy penalized incorrect support type and Type Agnostic does not.Fig. 5.Comparison of support algorithms. Image Plane Rules incorrectly assigns many support relationships. Structure Class Rules corrects several support relationships for Furniture objects but struggles with Props. The Support classifier corrects several of the Props but infers an implausible Furniture support. Finally, our LP solution correctly assigns most of the support relationships. (\u2192 : support from below, : support from behind, + : support from hidden region. Correct support predictions in green, incorrect in red. Ground in pink, Furniture in Purple, Props in Blue, Structure in Yellow, Grey indicates missing structure class label. Incorrect structure predictions are striped.)Predicting Support Relationships \nRegion Source \nGround Truth \nSegmentation \nAlgorithm \nType Agnostic Type Aware Type Agnostic Type Aware \nImage Plane Rules \n63.9 \n50.7 \n22.1 \n19.4 \nStructure Class Rules \n72.0 \n57.7 \n45.8 \n41.4 \nSupport Classifier \n70.1 \n63.4 \n45.8 \n37.1 \nEnergy Min (LP) \n75.9 \n72.6 \n55.1 \n54.5 \n\n\nhttp://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html 2 640 \u00d7 480 resolution. The images were hand selected from 435, 103 video frames, to ensure diverse scene content and lack of similarity to other frames.\nA full list of features can be found in the supplementary material.\nA full list of features can be found in the supplementary material.\nAcknowledgements. This work was supported in part by NSF Awards 09-04209, 09-16014 and IIS-1116923. The authors would also like to thank Microsoft for their support. Part of this work was conducted while Rob Fergus and Derek Hoiem were visiting researchers at Microsoft Research Cambridge.\nGeometric context from a single image. D Hoiem, A A Efros, M Hebert, ICCVHoiem, D., Efros, A.A., Hebert, M.: Geometric context from a single image. In: ICCV (2005)\n\nRecovering the spatial layout of cluttered rooms. V Hedau, D Hoiem, D Forsyth, ICCVHedau, V., Hoiem, D., Forsyth, D.: Recovering the spatial layout of cluttered rooms. In: ICCV (2009)\n\nThinking Inside the Box: Using Appearance Models and Context Based on Room Geometry. V Hedau, D Hoiem, D Forsyth, ECCV 2010, Part VI. Daniilidis, K., Maragos, P., Paragios, N.HeidelbergSpringer6316Hedau, V., Hoiem, D., Forsyth, D.: Thinking Inside the Box: Using Appearance Models and Context Based on Room Geometry. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010, Part VI. LNCS, vol. 6316, pp. 224-237. Springer, Heidelberg (2010)\n\nGeometric reasoning for single image structure recovery. D C Lee, M Hebert, T Kanade, CVPRLee, D.C., Hebert, M., Kanade, T.: Geometric reasoning for single image structure recovery. In: CVPR (2009)\n\nEstimating spatial layout of rooms using volumetric reasoning about objects and surfaces. D C Lee, A Gupta, M Hebert, T Kanade, NIPSLee, D.C., Gupta, A., Hebert, M., Kanade, T.: Estimating spatial layout of rooms using volumetric reasoning about objects and surfaces. In: NIPS (2010)\n\nBlocks World Revisited: Image Understanding Using Qualitative Geometry and Mechanics. A Gupta, A A Efros, M Hebert, ECCV 2010, Part IV. Daniilidis, K., Maragos, P., Paragios, N.HeidelbergSpringer6314Gupta, A., Efros, A.A., Hebert, M.: Blocks World Revisited: Image Understand- ing Using Qualitative Geometry and Mechanics. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010, Part IV. LNCS, vol. 6314, pp. 482-496. Springer, Heidelberg (2010)\n\nFrom 3d scene geometry to human workspace. A Gupta, S Satkin, A A Efros, M Hebert, CVPRGupta, A., Satkin, S., Efros, A.A., Hebert, M.: From 3d scene geometry to human workspace. In: CVPR (2011)\n\nRecovering occlusion boundaries from an image. D Hoiem, A A Efros, M Hebert, Int. J. Comput. Vision. 91Hoiem, D., Efros, A.A., Hebert, M.: Recovering occlusion boundaries from an im- age. Int. J. Comput. Vision 91, 328-346 (2011)\n\nBuilding a database of 3d scenes from user annotations. B C Russell, A Torralba, CVPRRussell, B.C., Torralba, A.: Building a database of 3d scenes from user annotations. In: CVPR (2009)\n\nSemantic Segmentation of Urban Scenes Using Dense Depth Maps. C Zhang, L Wang, R Yang, ECCV 2010, Part IV. Daniilidis, K., Maragos, P., Paragios, N.HeidelbergSpringer6314Zhang, C., Wang, L., Yang, R.: Semantic Segmentation of Urban Scenes Using Dense Depth Maps. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010, Part IV. LNCS, vol. 6314, pp. 708-721. Springer, Heidelberg (2010)\n\nIndoor scene segmentation using a structured light sensor. N Silberman, R Fergus, ICCV Workshop on 3D Representation and Recognition. Silberman, N., Fergus, R.: Indoor scene segmentation using a structured light sen- sor. In: ICCV Workshop on 3D Representation and Recognition (2011)\n\nA category-level 3-d database: Putting the kinect to work. S Karayev, A Janoch, Y Jia, J Barron, M Fritz, K Saenko, T Darrell, ICCV Workshop on Consumer Depth Cameras for Computer Vision. Karayev, S., Janoch, A., Jia, Y., Barron, J., Fritz, M., Saenko, K., Darrell, T.: A category-level 3-d database: Putting the kinect to work. In: ICCV Workshop on Consumer Depth Cameras for Computer Vision (2011)\n\nA large-scale hierarchical multi-view rgb-d object dataset. K Lai, L Bo, X Ren, D Fox, ICRALai, K., Bo, L., Ren, X., Fox, D.: A large-scale hierarchical multi-view rgb-d object dataset. In: ICRA (2011)\n\nSemantic labeling of 3d point clouds for indoor scenes. H Koppula, A Anand, T Joachims, A Saxena, NIPSKoppula, H., Anand, A., Joachims, T., Saxena, A.: Semantic labeling of 3d point clouds for indoor scenes. In: NIPS (2011)\n\nColorization using optimization. A Levin, D Lischinski, Y Weiss, SIG-GRAPHLevin, A., Lischinski, D., Weiss, Y.: Colorization using optimization. In: SIG- GRAPH (2004)\n\nManhattan world: orientation and outlier detection by Bayesian inference. J Coughlan, A Yuille, Neural Computation. 15Coughlan, J., Yuille, A.: Manhattan world: orientation and outlier detection by Bayesian inference. Neural Computation 15 (2003)\n\nVideo Compass. J Kosecka, W Zhang, A Heyden, G Sparr, M Nielsen, Part IV. Johansen, P.HeidelbergSpringer2353Kosecka, J., Zhang, W.: Video Compass. In: Heyden, A., Sparr, G., Nielsen, M., Johansen, P. (eds.) ECCV 2002, Part IV. LNCS, vol. 2353, pp. 476-490. Springer, Heidelberg (2002)\n\nBoundary extraction in natural images using ultrametric contour maps. P Arbelaez, Proc. POCV. POCVArbelaez, P.: Boundary extraction in natural images using ultrametric contour maps. In: Proc. POCV (2006)\n\nSuperParsing: Scalable Nonparametric Image Parsing with Superpixels. J Tighe, S Lazebnik, ECCV 2010, Part V. LNCS. Daniilidis, K., Maragos, P., Paragios, N.HeidelbergSpringer6315Tighe, J., Lazebnik, S.: SuperParsing: Scalable Nonparametric Image Parsing with Superpixels. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010, Part V. LNCS, vol. 6315, pp. 352-365. Springer, Heidelberg (2010)\n", "annotations": {"author": "[{\"end\":119,\"start\":62},{\"end\":208,\"start\":120},{\"end\":254,\"start\":209},{\"end\":306,\"start\":255}]", "publisher": null, "author_last_name": "[{\"end\":78,\"start\":69},{\"end\":131,\"start\":126},{\"end\":223,\"start\":218},{\"end\":265,\"start\":259}]", "author_first_name": "[{\"end\":68,\"start\":62},{\"end\":125,\"start\":120},{\"end\":217,\"start\":209},{\"end\":258,\"start\":255}]", "author_affiliation": "[{\"end\":118,\"start\":80},{\"end\":207,\"start\":133},{\"end\":253,\"start\":225},{\"end\":305,\"start\":267}]", "title": "[{\"end\":59,\"start\":1},{\"end\":365,\"start\":307}]", "venue": null, "abstract": "[{\"end\":1233,\"start\":367}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5297,\"start\":5294},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5300,\"start\":5297},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5303,\"start\":5300},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5306,\"start\":5303},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5309,\"start\":5306},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5312,\"start\":5309},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5315,\"start\":5312},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5474,\"start\":5471},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5497,\"start\":5494},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5528,\"start\":5525},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5531,\"start\":5528},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5534,\"start\":5531},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5537,\"start\":5534},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5622,\"start\":5619},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5973,\"start\":5970},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6909,\"start\":6905},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7448,\"start\":7444},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7472,\"start\":7468},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8386,\"start\":8382},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8462,\"start\":8458},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8528,\"start\":8524},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8531,\"start\":8528},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10696,\"start\":10692},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11171,\"start\":11167},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11496,\"start\":11492},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15268,\"start\":15264},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15892,\"start\":15889},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16288,\"start\":16285},{\"end\":20945,\"start\":20939},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23169,\"start\":23168},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23811,\"start\":23807},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23928,\"start\":23924},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23952,\"start\":23948},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24280,\"start\":24277}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30260,\"start\":30187},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30719,\"start\":30261},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30750,\"start\":30720},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31112,\"start\":30751},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31289,\"start\":31113},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31696,\"start\":31290},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32070,\"start\":31697},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33342,\"start\":32071}]", "paragraph": "[{\"end\":1550,\"start\":1249},{\"end\":2173,\"start\":1552},{\"end\":2961,\"start\":2175},{\"end\":4109,\"start\":2963},{\"end\":5124,\"start\":4111},{\"end\":6802,\"start\":5141},{\"end\":7580,\"start\":6804},{\"end\":8240,\"start\":7582},{\"end\":8677,\"start\":8283},{\"end\":9391,\"start\":8679},{\"end\":9703,\"start\":9411},{\"end\":10576,\"start\":9747},{\"end\":11851,\"start\":10609},{\"end\":12606,\"start\":12035},{\"end\":12764,\"start\":12641},{\"end\":13218,\"start\":12800},{\"end\":13558,\"start\":13220},{\"end\":14638,\"start\":13654},{\"end\":15137,\"start\":14655},{\"end\":15808,\"start\":15139},{\"end\":16740,\"start\":15810},{\"end\":17343,\"start\":16787},{\"end\":18122,\"start\":17345},{\"end\":18199,\"start\":18124},{\"end\":18358,\"start\":18201},{\"end\":18529,\"start\":18425},{\"end\":18874,\"start\":18717},{\"end\":19146,\"start\":18995},{\"end\":19432,\"start\":19228},{\"end\":19597,\"start\":19434},{\"end\":19760,\"start\":19599},{\"end\":20628,\"start\":19869},{\"end\":20953,\"start\":20630},{\"end\":21872,\"start\":21246},{\"end\":22165,\"start\":21874},{\"end\":22458,\"start\":22219},{\"end\":23639,\"start\":22504},{\"end\":24162,\"start\":23693},{\"end\":24873,\"start\":24204},{\"end\":25195,\"start\":24896},{\"end\":25430,\"start\":25300},{\"end\":25498,\"start\":25432},{\"end\":26053,\"start\":25500},{\"end\":26519,\"start\":26055},{\"end\":27692,\"start\":26521},{\"end\":28355,\"start\":27737},{\"end\":29037,\"start\":28357},{\"end\":29468,\"start\":29079},{\"end\":30186,\"start\":29483}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9410,\"start\":9392},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11918,\"start\":11852},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12034,\"start\":11918},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12640,\"start\":12607},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13653,\"start\":13559},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18424,\"start\":18359},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18612,\"start\":18530},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18716,\"start\":18612},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18994,\"start\":18875},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19227,\"start\":19147},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19838,\"start\":19761},{\"attributes\":{\"id\":\"formula_12\"},\"end\":21033,\"start\":20954},{\"attributes\":{\"id\":\"formula_13\"},\"end\":21098,\"start\":21033},{\"attributes\":{\"id\":\"formula_14\"},\"end\":21194,\"start\":21098},{\"attributes\":{\"id\":\"formula_15\"},\"end\":21245,\"start\":21194},{\"attributes\":{\"id\":\"formula_16\"},\"end\":22218,\"start\":22166},{\"attributes\":{\"id\":\"formula_17\"},\"end\":25299,\"start\":25196}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24301,\"start\":24294},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27161,\"start\":27154}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1247,\"start\":1235},{\"attributes\":{\"n\":\"1.1\"},\"end\":5139,\"start\":5127},{\"attributes\":{\"n\":\"2\"},\"end\":8281,\"start\":8243},{\"attributes\":{\"n\":\"3\"},\"end\":9745,\"start\":9706},{\"attributes\":{\"n\":\"3.1\"},\"end\":10607,\"start\":10579},{\"attributes\":{\"n\":\"3.2\"},\"end\":12798,\"start\":12767},{\"attributes\":{\"n\":\"4\"},\"end\":14653,\"start\":14641},{\"attributes\":{\"n\":\"5\"},\"end\":16773,\"start\":16743},{\"attributes\":{\"n\":\"5.1\"},\"end\":16785,\"start\":16776},{\"attributes\":{\"n\":\"5.2\"},\"end\":19867,\"start\":19840},{\"attributes\":{\"n\":\"5.3\"},\"end\":22502,\"start\":22461},{\"attributes\":{\"n\":\"5.4\"},\"end\":23691,\"start\":23642},{\"attributes\":{\"n\":\"6\"},\"end\":24176,\"start\":24165},{\"attributes\":{\"n\":\"6.1\"},\"end\":24202,\"start\":24179},{\"attributes\":{\"n\":\"6.2\"},\"end\":24894,\"start\":24876},{\"end\":27715,\"start\":27695},{\"end\":27735,\"start\":27718},{\"attributes\":{\"n\":\"6.3\"},\"end\":29077,\"start\":29040},{\"attributes\":{\"n\":\"7\"},\"end\":29481,\"start\":29471},{\"end\":30196,\"start\":30188},{\"end\":30270,\"start\":30262},{\"end\":30729,\"start\":30721},{\"end\":30760,\"start\":30752},{\"end\":31707,\"start\":31698},{\"end\":32081,\"start\":32072}]", "table": "[{\"end\":32070,\"start\":31709},{\"end\":33342,\"start\":33027}]", "figure_caption": "[{\"end\":30260,\"start\":30198},{\"end\":30719,\"start\":30272},{\"end\":30750,\"start\":30731},{\"end\":31112,\"start\":30762},{\"end\":31289,\"start\":31115},{\"end\":31696,\"start\":31292},{\"end\":33027,\"start\":32083}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2998,\"start\":2992},{\"end\":9669,\"start\":9662},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10514,\"start\":10508},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12659,\"start\":12653},{\"end\":27268,\"start\":27262},{\"end\":27320,\"start\":27314},{\"end\":28392,\"start\":28386}]", "bib_author_first_name": "[{\"end\":34019,\"start\":34018},{\"end\":34028,\"start\":34027},{\"end\":34030,\"start\":34029},{\"end\":34039,\"start\":34038},{\"end\":34195,\"start\":34194},{\"end\":34204,\"start\":34203},{\"end\":34213,\"start\":34212},{\"end\":34415,\"start\":34414},{\"end\":34424,\"start\":34423},{\"end\":34433,\"start\":34432},{\"end\":34836,\"start\":34835},{\"end\":34838,\"start\":34837},{\"end\":34845,\"start\":34844},{\"end\":34855,\"start\":34854},{\"end\":35068,\"start\":35067},{\"end\":35070,\"start\":35069},{\"end\":35077,\"start\":35076},{\"end\":35086,\"start\":35085},{\"end\":35096,\"start\":35095},{\"end\":35349,\"start\":35348},{\"end\":35358,\"start\":35357},{\"end\":35360,\"start\":35359},{\"end\":35369,\"start\":35368},{\"end\":35761,\"start\":35760},{\"end\":35770,\"start\":35769},{\"end\":35780,\"start\":35779},{\"end\":35782,\"start\":35781},{\"end\":35791,\"start\":35790},{\"end\":35960,\"start\":35959},{\"end\":35969,\"start\":35968},{\"end\":35971,\"start\":35970},{\"end\":35980,\"start\":35979},{\"end\":36200,\"start\":36199},{\"end\":36202,\"start\":36201},{\"end\":36213,\"start\":36212},{\"end\":36393,\"start\":36392},{\"end\":36402,\"start\":36401},{\"end\":36410,\"start\":36409},{\"end\":36785,\"start\":36784},{\"end\":36798,\"start\":36797},{\"end\":37070,\"start\":37069},{\"end\":37081,\"start\":37080},{\"end\":37091,\"start\":37090},{\"end\":37098,\"start\":37097},{\"end\":37108,\"start\":37107},{\"end\":37117,\"start\":37116},{\"end\":37127,\"start\":37126},{\"end\":37472,\"start\":37471},{\"end\":37479,\"start\":37478},{\"end\":37485,\"start\":37484},{\"end\":37492,\"start\":37491},{\"end\":37671,\"start\":37670},{\"end\":37682,\"start\":37681},{\"end\":37691,\"start\":37690},{\"end\":37703,\"start\":37702},{\"end\":37873,\"start\":37872},{\"end\":37882,\"start\":37881},{\"end\":37896,\"start\":37895},{\"end\":38082,\"start\":38081},{\"end\":38094,\"start\":38093},{\"end\":38271,\"start\":38270},{\"end\":38282,\"start\":38281},{\"end\":38291,\"start\":38290},{\"end\":38301,\"start\":38300},{\"end\":38310,\"start\":38309},{\"end\":38612,\"start\":38611},{\"end\":38816,\"start\":38815},{\"end\":38825,\"start\":38824}]", "bib_author_last_name": "[{\"end\":34025,\"start\":34020},{\"end\":34036,\"start\":34031},{\"end\":34046,\"start\":34040},{\"end\":34201,\"start\":34196},{\"end\":34210,\"start\":34205},{\"end\":34221,\"start\":34214},{\"end\":34421,\"start\":34416},{\"end\":34430,\"start\":34425},{\"end\":34441,\"start\":34434},{\"end\":34842,\"start\":34839},{\"end\":34852,\"start\":34846},{\"end\":34862,\"start\":34856},{\"end\":35074,\"start\":35071},{\"end\":35083,\"start\":35078},{\"end\":35093,\"start\":35087},{\"end\":35103,\"start\":35097},{\"end\":35355,\"start\":35350},{\"end\":35366,\"start\":35361},{\"end\":35376,\"start\":35370},{\"end\":35767,\"start\":35762},{\"end\":35777,\"start\":35771},{\"end\":35788,\"start\":35783},{\"end\":35798,\"start\":35792},{\"end\":35966,\"start\":35961},{\"end\":35977,\"start\":35972},{\"end\":35987,\"start\":35981},{\"end\":36210,\"start\":36203},{\"end\":36222,\"start\":36214},{\"end\":36399,\"start\":36394},{\"end\":36407,\"start\":36403},{\"end\":36415,\"start\":36411},{\"end\":36795,\"start\":36786},{\"end\":36805,\"start\":36799},{\"end\":37078,\"start\":37071},{\"end\":37088,\"start\":37082},{\"end\":37095,\"start\":37092},{\"end\":37105,\"start\":37099},{\"end\":37114,\"start\":37109},{\"end\":37124,\"start\":37118},{\"end\":37135,\"start\":37128},{\"end\":37476,\"start\":37473},{\"end\":37482,\"start\":37480},{\"end\":37489,\"start\":37486},{\"end\":37496,\"start\":37493},{\"end\":37679,\"start\":37672},{\"end\":37688,\"start\":37683},{\"end\":37700,\"start\":37692},{\"end\":37710,\"start\":37704},{\"end\":37879,\"start\":37874},{\"end\":37893,\"start\":37883},{\"end\":37902,\"start\":37897},{\"end\":38091,\"start\":38083},{\"end\":38101,\"start\":38095},{\"end\":38279,\"start\":38272},{\"end\":38288,\"start\":38283},{\"end\":38298,\"start\":38292},{\"end\":38307,\"start\":38302},{\"end\":38318,\"start\":38311},{\"end\":38621,\"start\":38613},{\"end\":38822,\"start\":38817},{\"end\":38834,\"start\":38826}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":34142,\"start\":33979},{\"attributes\":{\"id\":\"b1\"},\"end\":34327,\"start\":34144},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2046975},\"end\":34776,\"start\":34329},{\"attributes\":{\"id\":\"b3\"},\"end\":34975,\"start\":34778},{\"attributes\":{\"id\":\"b4\"},\"end\":35260,\"start\":34977},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":16466083},\"end\":35715,\"start\":35262},{\"attributes\":{\"id\":\"b6\"},\"end\":35910,\"start\":35717},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":9573412},\"end\":36141,\"start\":35912},{\"attributes\":{\"id\":\"b8\"},\"end\":36328,\"start\":36143},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":17886940},\"end\":36723,\"start\":36330},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":13993169},\"end\":37008,\"start\":36725},{\"attributes\":{\"id\":\"b11\"},\"end\":37409,\"start\":37010},{\"attributes\":{\"id\":\"b12\"},\"end\":37612,\"start\":37411},{\"attributes\":{\"id\":\"b13\"},\"end\":37837,\"start\":37614},{\"attributes\":{\"id\":\"b14\"},\"end\":38005,\"start\":37839},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":207585013},\"end\":38253,\"start\":38007},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1413778},\"end\":38539,\"start\":38255},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4474066},\"end\":38744,\"start\":38541},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":51694943},\"end\":39147,\"start\":38746}]", "bib_title": "[{\"end\":34412,\"start\":34329},{\"end\":35346,\"start\":35262},{\"end\":35957,\"start\":35912},{\"end\":36390,\"start\":36330},{\"end\":36782,\"start\":36725},{\"end\":37067,\"start\":37010},{\"end\":38079,\"start\":38007},{\"end\":38268,\"start\":38255},{\"end\":38609,\"start\":38541},{\"end\":38813,\"start\":38746}]", "bib_author": "[{\"end\":34027,\"start\":34018},{\"end\":34038,\"start\":34027},{\"end\":34048,\"start\":34038},{\"end\":34203,\"start\":34194},{\"end\":34212,\"start\":34203},{\"end\":34223,\"start\":34212},{\"end\":34423,\"start\":34414},{\"end\":34432,\"start\":34423},{\"end\":34443,\"start\":34432},{\"end\":34844,\"start\":34835},{\"end\":34854,\"start\":34844},{\"end\":34864,\"start\":34854},{\"end\":35076,\"start\":35067},{\"end\":35085,\"start\":35076},{\"end\":35095,\"start\":35085},{\"end\":35105,\"start\":35095},{\"end\":35357,\"start\":35348},{\"end\":35368,\"start\":35357},{\"end\":35378,\"start\":35368},{\"end\":35769,\"start\":35760},{\"end\":35779,\"start\":35769},{\"end\":35790,\"start\":35779},{\"end\":35800,\"start\":35790},{\"end\":35968,\"start\":35959},{\"end\":35979,\"start\":35968},{\"end\":35989,\"start\":35979},{\"end\":36212,\"start\":36199},{\"end\":36224,\"start\":36212},{\"end\":36401,\"start\":36392},{\"end\":36409,\"start\":36401},{\"end\":36417,\"start\":36409},{\"end\":36797,\"start\":36784},{\"end\":36807,\"start\":36797},{\"end\":37080,\"start\":37069},{\"end\":37090,\"start\":37080},{\"end\":37097,\"start\":37090},{\"end\":37107,\"start\":37097},{\"end\":37116,\"start\":37107},{\"end\":37126,\"start\":37116},{\"end\":37137,\"start\":37126},{\"end\":37478,\"start\":37471},{\"end\":37484,\"start\":37478},{\"end\":37491,\"start\":37484},{\"end\":37498,\"start\":37491},{\"end\":37681,\"start\":37670},{\"end\":37690,\"start\":37681},{\"end\":37702,\"start\":37690},{\"end\":37712,\"start\":37702},{\"end\":37881,\"start\":37872},{\"end\":37895,\"start\":37881},{\"end\":37904,\"start\":37895},{\"end\":38093,\"start\":38081},{\"end\":38103,\"start\":38093},{\"end\":38281,\"start\":38270},{\"end\":38290,\"start\":38281},{\"end\":38300,\"start\":38290},{\"end\":38309,\"start\":38300},{\"end\":38320,\"start\":38309},{\"end\":38623,\"start\":38611},{\"end\":38824,\"start\":38815},{\"end\":38836,\"start\":38824}]", "bib_venue": "[{\"end\":34514,\"start\":34504},{\"end\":35449,\"start\":35439},{\"end\":36488,\"start\":36478},{\"end\":38351,\"start\":38341},{\"end\":38639,\"start\":38635},{\"end\":38912,\"start\":38902},{\"end\":34016,\"start\":33979},{\"end\":34192,\"start\":34144},{\"end\":34461,\"start\":34443},{\"end\":34833,\"start\":34778},{\"end\":35065,\"start\":34977},{\"end\":35396,\"start\":35378},{\"end\":35758,\"start\":35717},{\"end\":36011,\"start\":35989},{\"end\":36197,\"start\":36143},{\"end\":36435,\"start\":36417},{\"end\":36857,\"start\":36807},{\"end\":37196,\"start\":37137},{\"end\":37469,\"start\":37411},{\"end\":37668,\"start\":37614},{\"end\":37870,\"start\":37839},{\"end\":38121,\"start\":38103},{\"end\":38327,\"start\":38320},{\"end\":38633,\"start\":38623},{\"end\":38859,\"start\":38836}]"}}}, "year": 2023, "month": 12, "day": 17}