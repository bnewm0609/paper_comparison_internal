{"id": 252368023, "updated": "2023-11-18 17:39:55.043", "metadata": {"title": "Dataset Inference for Self-Supervised Models", "authors": "[{\"first\":\"Adam\",\"last\":\"Dziedzic\",\"middle\":[]},{\"first\":\"Haonan\",\"last\":\"Duan\",\"middle\":[]},{\"first\":\"Muhammad\",\"last\":\"Kaleem\",\"middle\":[\"Ahmad\"]},{\"first\":\"Nikita\",\"last\":\"Dhawan\",\"middle\":[]},{\"first\":\"Jonas\",\"last\":\"Guan\",\"middle\":[]},{\"first\":\"Yannis\",\"last\":\"Cattan\",\"middle\":[]},{\"first\":\"Franziska\",\"last\":\"Boenisch\",\"middle\":[]},{\"first\":\"Nicolas\",\"last\":\"Papernot\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Self-supervised models are increasingly prevalent in machine learning (ML) since they reduce the need for expensively labeled data. Because of their versatility in downstream applications, they are increasingly used as a service exposed via public APIs. At the same time, these encoder models are particularly vulnerable to model stealing attacks due to the high dimensionality of vector representations they output. Yet, encoders remain undefended: existing mitigation strategies for stealing attacks focus on supervised learning. We introduce a new dataset inference defense, which uses the private training set of the victim encoder model to attribute its ownership in the event of stealing. The intuition is that the log-likelihood of an encoder's output representations is higher on the victim's training data than on test data if it is stolen from the victim, but not if it is independently trained. We compute this log-likelihood using density estimation models. As part of our evaluation, we also propose measuring the fidelity of stolen encoders and quantifying the effectiveness of the theft detection without involving downstream tasks; instead, we leverage mutual information and distance measurements. Our extensive empirical results in the vision domain demonstrate that dataset inference is a promising direction for defending self-supervised models against model stealing.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/DziedzicDKDGCBP22", "doi": "10.48550/arxiv.2209.09024"}}, "content": {"source": {"pdf_hash": "274dbb98c63cdd282eb86b0338bdc3c5dfd9b904", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2209.09024v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b3e809286e40502f6327a0d54cef4767e22a9dbe", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/274dbb98c63cdd282eb86b0338bdc3c5dfd9b904.txt", "contents": "\nDataset Inference for Self-Supervised Models\n\n\nAdam Dziedzic \nUniversity of Toronto and Vector Institute\n\n\nHaonan Duan \nUniversity of Toronto and Vector Institute\n\n\nMuhammad Ahmad Kaleem \nUniversity of Toronto and Vector Institute\n\n\nNikita Dhawan \nUniversity of Toronto and Vector Institute\n\n\nJonas Guan \nUniversity of Toronto and Vector Institute\n\n\nYannis Cattan \nUniversity of Toronto and Vector Institute\n\n\nFranziska Boenisch \nUniversity of Toronto and Vector Institute\n\n\nNicolas Papernot \nUniversity of Toronto and Vector Institute\n\n\nDataset Inference for Self-Supervised Models\n\nSelf-supervised models are increasingly prevalent in machine learning (ML) since they reduce the need for expensively labeled data. Because of their versatility in downstream applications, they are increasingly used as a service exposed via public APIs. At the same time, these encoder models are particularly vulnerable to model stealing attacks due to the high dimensionality of vector representations they output. Yet, encoders remain undefended: existing mitigation strategies for stealing attacks focus on supervised learning. We introduce a new dataset inference defense, which uses the private training set of the victim encoder model to attribute its ownership in the event of stealing. The intuition is that the log-likelihood of an encoder's output representations is higher on the victim's training data than on test data if it is stolen from the victim, but not if it is independently trained. We compute this log-likelihood using density estimation models. As part of our evaluation, we also propose measuring the fidelity of stolen encoders and quantifying the effectiveness of the theft detection without involving downstream tasks; instead, we leverage mutual information and distance measurements. Our extensive empirical results in the vision domain demonstrate that dataset inference is a promising direction for defending self-supervised models against model stealing. * Corresponding and leading author: adam.dziedzic@utoronto.ca \u2020\n\nIntroduction\n\nThe self-supervised learning (SSL) paradigm enables pre-training models with unlabeled data to learn generally useful domain knowledge and then transfer the knowledge to solve specific downstream tasks. The ability to learn from unlabeled data alleviates the high costs of labeling large datasets [24], and the transfer learning setup reduces the computational costs of retraining. These advantages have made SSL increasingly popular [20] in domains like vision [5], language [12], and bioinformatics [29].\n\nRecently, commercial service providers like Cohere [1] and OpenAI [2] began offering paid query access to trained SSL encoders over public APIs. This exposes the encoders to black-box extraction attacks, i.e., model stealing. In a model stealing attack, an attacker aims to train an approximate copy of a victim model by submitting carefully chosen queries and observing the victim's outputs. The high costs of data collection, preprocessing and model training make encoders valuable targets for stealing. For example, the training data of CLIP includes 400 million image and text pairs [37], while computation costs of training a large language model can exceed one million USD [41]. The threat of model stealing in SSL is real: researchers have demonstrated that encoders can be stolen at a fraction of the victim's training cost [14,39]. Yet, most current defenses are designed for supervised models [15,23,35] and cannot be directly applied to encoders [14].\n\nDataset inference [31] is a state-of-the-art defense against model stealing in the supervised learning setting. The defense provides ownership resolution: it enables the model owner to make a strong statistical claim that a given model is a stolen copy of their own model by showing that this model is derivative of their own private training data. Dataset inference does not require retraining or overfitting the model to any form of explicit watermark [4] and has been shown to resist attacks from Training Inference 2 \u210e 2 (\u210e 1 ) \u2248 (\u210e ) Independent Figure 1: Ownership Resolution for Encoders. First, an arbitrator trains density estimator E: divide DP into non-overlapping partitions DP 1 and DP 2, and train density estimator E using the representations hD P 2 of f on DP 2. Next, the arbitrator performs dataset inference: apply E on the representations of DP 1 and DN of the encoder f . For a stolen encoder, the log-likelihood of the representations hD P 1 is significantly higher than hD N , while, for an independent encoder, the log-likelihoods of the representations are not significantly different.\n\nadaptive adversaries [31]. These properties make the defense particularly attractive for SSL, as large encoders can be expensive to retrain, and performance is paramount because it carries over to all downstream tasks. However, the original dataset inference algorithm from [31] cannot be applied to encoders, because it relies on computing distances between data points and decision boundaries. These decision boundaries do not exist in SSL encoders since they are trained on unlabeled data.\n\nWe introduce a new dataset inference method ( Figure 1) to defend against model stealing for encoders. Our algorithm is suitable for the high-dimensional outputs of SSL encoders and does not rely on labeled data or decision boundaries. Instead, it relies solely on the private training data of the victim encoder as a signature. Moreover, our algorithm retains the advantages of dataset inference for supervised models [31], namely, it does not require retraining or overfitting the SSL encoder.\n\nOur key intuition is to identify stolen encoders by characterizing differences between an encoder's representations on its training data vs on unseen test data. The victim encoder and its derivatives, such as stolen copies, exhibit different behavior on the victim's private training data than on test data; independently trained encoders do not. These differences exist because encoders overfit to training data [18,30]. Although for well-trained encoders the effect is minimal on any given data point, we show that when aggregated over many training points it provides a statistically strong signal. To identify the differences, we train a Gaussian Mixture Model (GMM), as an efficient general approximation, to model the distribution of an encoder's data representations from its training domain. We then use the GMMs to predict the log-likelihood of the encoders' representations of the victim's training set and a test set; derivatives of the victim encoder will have a higher log-likelihood on the training set than on the test set. We perform experiments on five datasets from the vision domain and show that we are able to distinguish between stolen and independent encoders even in cases when adversaries obfuscate the representations from the stolen encoders to hide the theft (e.g., by shuffling the elements in the representation vectors or applying to them some form of a linear transformation).\n\nAs part of our evaluation, we also introduce new metrics to measure the fidelity of stolen encoders without involving downstream tasks and to quantify the effectiveness of theft detection. We compute scores directly on the representations using tools from information theory and distance metrics. These methods work well because losses used for stealing encoders directly minimize distances between representations of victim and stolen encoders. Our mutual information score to assess the quality of the stolen encoders is robust against obfuscations that an adversary might apply to the representations returned by a stolen encoder. Without any obfuscation, our cosine similarity score shows a clearer distinction between stolen and independent encoders. Finally, using these metrics, we observe that the higher the quality of the stolen encoders, the more confident our dataset inference defense becomes.\n\nOur main contributions are as follows:\n\n\u2022 We propose a new defense against model stealing attacks on encoders, by combining dataset inference with density estimation models for ownership resolution on unlabeled data.\n\n\u2022 We are the first to design new metrics that quantify the quality of stolen encoders, which are derived from the mutual information and distances between representations.\n\n\u2022 We evaluate our defense using five datasets from the computer vision domain and show that our defense can successfully identify stolen encoders with a strong statistical significance.\n\n\nRelated Work\n\nIn model stealing, an adversary queries the victim model, obtains outputs, and uses them to recreate a copy of the victim [42]. This is most commonly performed with black-box access, e.g., via a public API. When stealing encoders in SSL, the goal of an adversary is to extract high-quality embeddings either to train a stolen copy that achieves high performance on downstream tasks, or to obtain faithful replicas of the victim's embeddings on the same inputs. Stolen encoders might be further used for model reselling, backdoor attacks, or membership inference [9].\n\nWhile most past research on model stealing and defenses focuses on classifiers trained via supervised learning, recent work constructed new attacks that target encoders [14,39]. The main differences between the attacks in these settings are that the outputs of encoders leak more information due to their higher dimensionalities [39], and the attacks require different loss functions. Inspired by contrastive learning, Cont-Steal [39] provides a method of stealing encoders using a loss function based on InfoNCE [5]. SSL extraction [14]-a general Siamese-network-based framework for stealing encoders-leverages losses including mean squared error, InfoNCE, Soft Nearest Neighbor, and Wasserstein distance. The authors empirically show that an adversary can steal an ImageNet victim encoder in less than a fifth of the queries required for training.\n\nProof of Learning (PoL) [22] is a reactive defense that involves the defender claiming ownership of a model by showing incremental updates of the model training. It is a complementary method to dataset inference, which instead identifies a stolen model. PoL could be applied directly to SSL encoders, however, it requires an expensive verification process, where the verifier needs to perform model updates and the prover needs to save intermediate weights of the model, which is more expensive than dataset inference with GMMs. Unfortunately, other current defenses against model stealing for supervised learning are inadequate for defending encoders, and adjusting them to the specificities of encoders is non-trivial [14]. One line of approach is watermarking [9,14], where the defender embeds a secret trigger into the victim encoder during training to determine ownership at test time. However, watermarking-based defenses have two significant disadvantages. First, researchers have repeatedly shown that adaptive attackers can remove watermarks without severely affecting model performance [7,21,40,44], e.g., through pruning, fine-tuning, rounding or performing backdoor removal [4]. Second, the watermark must be embedded during training; if a model is already trained, or if a watermark defense needs to be updated, the model must be retrained. This is not practical for large encoders.\n\nAnother state-of-the-art defense against model stealing in the supervised setting is dataset inference [31] which addresses these disadvantages. However, the adaptation of dataset inference to encoders is difficult, because (1) the algorithm [31] relies on decision boundaries, which do not exist for encoders; and (2) encoders are less prone to overfitting, which provides the signal for dataset inference [14,31]. Therefore, naive approaches like computing the loss of representations or distances between train and test sets are ineffective [14]. To overcome these issues, we extract more signals from the representations by estimating their densities for train and test sets, as described in Section 3.\n\nWhen it comes to comparing signals within representations, prior work has considered measuring the similarity between different representations. This has led to the proposal of various similarity metrics including canonical correlation analysis (CCA) [33], centered kernel alignment (CKA) [25], and the orthogonal Procrustes distance [13], which use methods from linear regression, principal component analysis (PCA), and singular value decomposition (SVD). However, these metrics are very general and complex. They have also been shown to disagree in some cases [13]. Since we can only access the final embeddings from encoders, we design metrics more closely related to our setting.\n\n\nDefense Method\n\nDataset inference serves as a defense against model stealing. It enables the model owner or a third-party arbitrator to attribute the ownership of a model in the event that it is stolen. The idea is to take advantage of the effects of knowledge from the victim's training set and to use that as a signature for attributing ownership. Given a well-trained encoder, the effects are small on any single data point; however, when aggregated over many points in the training set, they collectively provide a strong statistical signal for dataset inference. As depicted in Figure 2, for a victim that leverages the private data D P during training or a stolen copy, we can identify a difference between Stolen / Victim \u210e \u210e \u210e \u210e Independent Figure 2: Dataset Inference -Intuition. D P and D N come from the same distribution. For independent encoders, their representations h D N and h D P are i.i.d. while for victim/stolen encoders, h D N and h D P induce different distributions.\n\ndistributions of the train data's representations h D P and the test data's representations h D N while the distributions of these representations from an independent encoder cannot be distinguished. We use this signal to determine whether a model is a derivative of the victim's training data, i.e., either directly trained on the data or stolen from the victim. To capture the signal, we first partition the victim encoder's training data into two subsets and use one to train Gaussian Mixture Models (GMMs) with the aim of modeling the distribution of representations of data from the encoder's domain. Then, we apply the GMMs to perform dataset inference by measuring the log-likelihood of the encoder's representations of the remaining training data vs some test data (see Figure 1). For the victim encoder or its stolen copies, the log-likelihood of training data representations is significantly higher than the one of test data. We use this to construct statistical t-tests to determine whether a model is stolen, and present our empirical results in Section 5.\n\n\nThreat Model\n\nAs described in Figure 7, we consider a victim encoder f v trained on a private training dataset D P . An adversary with black-box access to f v trains a stolen encoder f s by querying the victim with data points from a dataset D S to obtain representations. These representations are then used as part of the training objective for the stolen encoder. During the dataset inference, we assume the presence of a third-party arbitrator, such as law enforcement, with white-box access to the victim's training data, as well as all encoders. Additionally, the arbitrator requires the test dataset D N from the same distribution as D P to perform dataset inference. An independent encoder f i is trained with no access to the victim's private training set D P and without any queries to the victim's encoder. It is used as a baseline for the ownership resolution in the dataset inference.\n\n\nDensity Estimation of Representations\n\nRepresentations from encoders contain rich features for given inputs. We analyze the inputs that come from the training and test sets through their representations. For the training inputs, we compute their representations and model their densities [16]. To this end, we leverage GMMs as universal approximators of densities. We give representations a probabilistic interpretation such that they have a smooth enough density which can be approximated by any specific nonzero amount of error using a GMM with enough components. Each component has a separately parameterized mean \u00b5 and covariance \u03a3. In some cases, we observe that GMMs can overfit to their training data when no constraints are applied to the covariance matrix, hence we limit the covariance matrix for each component to be diagonal. Moreover, this constraint makes training more computationally efficient since it avoids storing and inverting full high-dimensional covariance matrices.\n\n\nData Flow\n\nThe full flow of our dataset inference for encoders consists of the following four main steps (which are also visualized in Figure 7 in Appendix):\n\n1. Victim Training. The victim's encoder f v is trained using the whole private training dataset D P . 2. Encoder Stealing. To steal the victim encoder f v , an adversary queries f v with data points from D S to obtain representations h D S \u2208 R n : h D S = f v (D S ). With these representations, the adversary trains the stolen encoder f s in a contrastive manner. 3. Training Estimators. To perform ownership resolution, an arbitrator trains three density estimators E v , E s and E i for the victim f v , stolen f s , and independent encoder f i as follows: a) D P (where D P is not necessarily the whole private training dataset) is divided into two non-overlapping subsets D P 1 and D P 2 . While D P 2 serves as the base for training the density estimators, D P 1 is used to evaluate density estimates of the private training data vs part of the test data D N . b) For a given encoder f \u2208 {f v , f s , f i }, the arbitrator generates representations h \u2208 R n : h D P 2 = f (D P 2 ) on dataset D P 2 . Training the density estimators on the respective representations yields the final density estimators E v , E s , and E i . \n\n\nOwnership Resolution\n\nFor an encoder f , we compute the log-likelihood on D P 1 and D N as: u P := 1\n|D P 1 | x\u2208D P 1 E(f (x)) and u N := 1 |D N | x\u2208D N E(f (x)\n). The density estimator E measures the similarity between the distributions over the victim's representations of the training D P vs test data D N . The intuition behind the setup is that if an encoder was trained on D P , representations of D P 1 are much more similar to representations of D P 2 , because the whole dataset D P was used for training the encoder, however, representations of D N differ from representation of D P 2 since D N was not used to train the encoder. For a victim f v and a stolen encoder f s , u P is significantly larger than u N , whereas, for an independent encoder f i , the values do not differ significantly. Finally, we carry out a hypothesis test with the null hypothesis being: H 0 := u P \u2264 u N . If the null-hypothesis can be rejected (p-value < 0.05), i.e., when the log-likelihood for the training set D P 1 is higher than that for the test set D N , we can conclude that the tested model was stolen. On the other hand, if the null hypothesis cannot be rejected then the test is inconclusive and we cannot determine if a tested encoder was stolen or not.\n\n\nEncoder Similarity Scores\n\nMeasuring the quality of stolen encoders allows us to assess attacks and defenses. In standard supervised learning, the quality of a stolen model is evaluated using two main objectives, namely task accuracy, which is the model's performance on the test set, and fidelity, which is the agreement in the predictions for a given task between the stolen and the victim model [19]. One of the approaches to measure the quality of an extracted encoder is to use its outputs to train a downstream task and compute the accuracy of that task or fidelity (with respect to the outputs of the downstream task trained on the victim encoder). However, a single downstream task cannot adequately reflect the degree of similarity between encoders since it reduces their high dimensional embeddings to single label representations, which are confounded by choices of downstream data and training protocol. Instead, we propose two new metrics. Our first metric is an information-theoretic score based on mutual information [10,27]. Our second metric is a cosine similarity score based on the representations returned by different encoders. These metrics correspond to the fidelity metric in supervised learning. The behavior of the two metrics differs in certain cases, for example, when used on obfuscated representations (e.g., with shuffled elements) or with independent models, however, we find that the overall trend is similar. Moreover, the mutual information score is based on an approximation while the cosine similarity score is calculated exactly given representation vectors. Often the effectiveness of defenses may be underestimated against low-quality stolen copies that haven't successfully stolen victim behavior. Our metrics help disentangle such effects and enable faithful evaluation of defenses.\n\n\nMutual Information Score\n\nOur first approach to assessing the quality of a stolen encoder uses a score based on mutual information. We sample N data points from the victim's private training dataset D P and pass them through the encoders f v , f s , and f i to generate the respective representations. Per standard practice, we recenter and normalize the representations [13]. We denote the entropy by H and compute it according to Algorithm 2 which takes f v , D P , and N as input. For the joint entropy H(f v , f s |D), we generate representations from the two encoders (in this case victim f v and stolen f s ) and concatenate them, which increases the dimensionality of the final representation to 2d, while other steps remain unchanged. A detailed algorithm for computing the joint entropy can be found in the Appendix as Algorithm 3. We compute an approximate score that is based on the definition of mutual information I(f v , f s |D P ) between the victim encoder f v and the stolen copy f s as well as the analogous mutual information I(f v , f i |D P ) between the victim encoder f v and the independently trained encoder f i . We rely on approximations since we measure mutual information using finite data. Yet, in practice, such approximations have proven useful [32]. We define our mutual information score as follows:\nI(f v , f s |D P ) = H(f v |D P ) + H(f s |D P ) \u2212 H(f v , f s |D P ).(1)\nA higher value of the mutual information I(f v , f s |D P ) indicates a higher information leakage incurred by the stolen encoder. Expectedly, mutual information is higher between the victim and the stolen encoder than between the victim and independent encoders\nI(f v , f s |D P ) >> I(f v , f i |D P ).\nWe can normalize mutual information into a score (between 0 and 1) by setting the lower bound as the mutual information between the victim f v and a randomly initialized model f r :\nI min = I(f v , f r |D P )\nand the upper bound as the mutual information between the victim and itself:\nI max = I(f v , f v |D P )\nFor the current mutual information score I c , the normalized score is defined as S := Ic\u2212Imin Imax\u2212Imin .\n\n\nCosine Similarity Score\n\nThe second score we use to assess the quality of a stolen encoder is based on the cosine similarity between its representations and the victim's representations. More specifically, we first compute representations for the two encoders on a set of N randomly selected data points from the dataset D P . Again as per standard practice [13], we recenter and normalize these representations. For each of the N inputs, we then compute the cosine similarity between the corresponding representations from both encoders where the cosine similarity sim(a, b) = a T b ||a||2||b||2 for representation vectors a and b. We show (in Section 4.3) that the loss functions, which are used for stealing encoders, directly maximize the cosine similarity between representations from victim and stolen encoders. We thus propose to use the cosine similarity score C as a metric, which we define as:\nC = |sim(a, b)| (2).\nThe score yields values in the range [0, 1], with a higher score indicating closer representations. To calculate a per-encoder cosine similarity score, we average the cosine similarity scores over all inputs. We find that the cosine similarity score is well-calibrated across encoders. Namely, an independent encoder, expected to have representations unrelated to the victim encoder, has an average cosine similarity concentrated around 0 [9], while a stolen encoder exhibits significantly higher scores. The cosine similarity score is also easy to compute since it only requires the corresponding representations of the two models and their dot product.\n\n\nAnalysis\n\nThere are various ways in which an attacker may steal an encoder. To simplify our analysis of the cosine similarity score, we consider the two best-performing loss functions used for stealing [14]: the first where the attacker minimizes the non-contrastive MSE (Mean Squared Error) loss between its representations and the victim encoder's representations to train the stolen encoder, and the second where the attacker uses a contrastive loss function, such as the InfoNCE loss [43] which is used in SimCLR [5].\n\nStealing with MSE loss. In the case where the MSE loss is used, let x i be a query made by an attacker and let f v (x i ) = h vi , f s (x i ) = h si \u2208 R n be the corresponding representations of the victim and stolen encoders, respectively. The MSE loss between these two representations is\n1 n n j=1 (h vi j \u2212 h si j ) 2 = 1 n ||h vi \u2212 h si || 2 2 .\nIt follows directly that minimizing the MSE loss also minimizes the 2 distance between representations and equivalently maximizes the cosine similarity between representations:\nTheorem 1 ||a \u2212 b|| 2 = 2(1 \u2212 sim(a, b)), ||a|| 2 = ||b|| 2 = 1 (see G.1).\nStealing with a contrastive loss. When an attacker uses a contrastive loss function for stealing, minimizing the loss corresponds to maximizing the sum of the cosine similarities between positive pairs, i.e., m c=1 (sim(h sc , h vc )/\u03c4 ). The InfoNCE loss, or contrastive losses in general, also increase the mutual information score [3,43]. We therefore expect that stolen encoders will have larger similarity scores w.r.t. the victim encoder than independent encoders. We refer the reader to Appendix G.1 for a more detailed discussion of the loss functions and their relationship with the similarity scores.\n\n\nEmpirical Evaluation\n\nWe evaluate our defense against encoder extraction attacks using five different vision datasets (CIFAR10, CIFAR100 [28], SVHN [34], STL10 [8], and ImageNet [11]). Table 1 shows that our dataset inference method is able to differentiate between the stolen copies of the victim encoder and independently trained encoders by using the victim's private training data as the signature. We also show that our defense works in the scenario where the adversary modifies the representations to render them inconspicuous, e.g., by shuffling the order of elements in the representation vectors. To assess the quality of the stolen encoders and the performance of our defense, we measure the mutual information and cosine similarity scores between encoders and present our results in Tables 2 and 3.\n\n\nTraining Victim, Stolen and Independent Encoders\n\nVictim. We use victim encoders trained on the ImageNet, CIFAR10, and SVHN datatsets. For the ImageNet victim encoder , we use a model released by the authors of SimSiam [6]. To train CIFAR10 and SVHN victim encoders, we use an open-source PyTorch implementation of SimCLR 3 . For SVHN, we merge the original training and test splits, and use the randomly-selected 80% as the training set and the rest 20% as the test set. This is necessary because the original training and test splits for SVHN are not i.i.d [36], which violates the assumption for dataset inference (see Section B). The ImageNet victim has an output representation dimension of 2048, while the CIFAR10 and SVHN victim encoders have 512-dimensional representations.\n\nStolen. When stealing from the victim encoders, we evaluate different numbers of queries from various datasets, including CIFAR10, SVHN, ImageNet, and STL10. Stolen encoders are trained in a similar contrastive way as the victim and use the InfoNCE loss, where the positive pairs consist of representations from the victim and stolen encoder for a given input. Algorithm 1 summarizes the stealing approach used by an adversary.\n\nIndependent. For each victim encoder, we train independent encoders using datasets different from the victim's private training dataset D P . The encoders are trained with the SimCLR approach, similar to the way the victim encoders were trained. In the case where the dataset used to train the independent model had different image dimensions from the victim's training dataset, the dataset was resized to be of the same size.\n\nMore details on the training and stealing of encoders can be found in Section D.3 of the Appendix.\n\n\nDataset Inference on Encoders\n\nSetup. We train GMMs with 10 components for SVHN and CIFAR10, and 50 components for ImageNet. In general, we observe that the larger number of components for GMMs, the better the defense is. For ImageNet, we restrict the covariance matrix to be diagonal for efficiency. For CIFAR10 and SVHN, we use the full covariance matrix. For SVHN and CIFAR10, we use 50% of the training set to train GMMs, and the remaining for evaluation. For ImageNet, we use 100K images from the training set to train GMMs, and another 100K of the training set as an evaluation set. We normalize representations by l 2 norm for training GMM. For ImageNet, we also standardize representations (subtract mean and divide by standard deviation) before normalization. We do not use augmentations in dataset inference. For each setting, the hyperparameters are tuned on the victim model and a randomly-initialized model.\n\nEvaluation of our Defense. The empirical results in Table 1 demonstrate that we are able to differentiate between stolen and independent encoders from the difference in log-likelihoods. We observe that the stolen encoders have significantly larger \u2206\u00b5 than the independent encoders. The p-values further show that for stolen encoders the null hypothesis is rejected while for independent encoders, the test is inconclusive. Similar to dataset inference for supervised learning [31], the victim model typically has the largest \u2206\u00b5 and the smallest p-values. We also observe that our method is better at detecting encoders that are stolen using queries from the victim's training set.\n\nNumber of Stolen Queries. Table 3 shows that as the attacker steals with more queries, the p-value from our defense becomes lower. This is consistent with the finding in [31] that dataset inference works better with stronger stolen encoders. We also find that our defense is able to detect stolen Table 1: Dataset inference via density estimation of representations. We detect if a given encoder was stolen. f v denotes the victim encoder trained on data D, f s is the stolen encoder extracted using queries from a given stealing dataset D, and f i is an independent encoder trained on data D (different than the victim's private training data). Each value is an average of 3 trials. \u2206\u00b5 is the effect size from the statistical t-test. Obfuscations: the representation can be modified by an attacker in the following ways: (1) Shuffle the elements in the representation vectors, (2) Pad with zeros or add zeros at random positions, and (3) apply a linear Transform. The first row below denotes the victim's private data D P .\n\nVictim's private data:  Table 3, we are able to claim ownership when only 50K -100k queries are used for stealing ImageNet victims (around 4% of its training set).\nCIFAR10 SVHN ImageNet Encoder Obfuscate D p-value \u2206\u00b5 D p-value \u2206\u00b5 D p-value \u2206\u00b5 fv N/A CIFAR10 5.\nRobustness of Dataset Inference to Obfuscations. The attacker can obfuscate the stolen encoder representations by, for instance, applying shuffling (changing the order of elements), padding (adding zeros), or linear transformations (e.g., scaling or adding a constant). These obfuscations have little impact on the downstream performance [17] but may pose challenges to the defenses of the victim.\n\nThe results in Table 1 show that the p-values for the stolen encoders after attackers' obfuscations remain low, which implies that our method is robust to these types of obfuscations. Evaluation of Metrics. To evaluate the mutual information and the cosine similarity scores, we conduct two sets of experiments to verify if: (1) the scores are higher for stolen than independent encoders, and (2) the scores increase as more queries are used to steal encoders, which suggests a higher quality of the stolen copies [14]. In Table 2, we observe that both our scores assign higher values to the stolen encoders than the independent encoders. Table 3 shows that our mutual information and cosine similarity scores generally increase while the p-values from our dataset inference decrease with respect to the number of queries used to steal an encoder. This implies that the performance of our defense is consistent with the similarity metrics and becomes more effective as the quality of the stolen encoder improves. We also plot a histogram of the cosine similarity scores for the stolen and independent encoders in Figure 3 for an SVHN victim encoder, a stolen encoder from it (using CIFAR10 training data for queries), and an independent encoder (trained on CIFAR100). There is a pronounced difference between the two distributions with the Table 2: Encoder similarity scores. We compare encoders via the encoder quality metrics using the same setting as in Table 1. We compute the score S(\u00b7, f v ) based on the mutual information between a given encoder (in a row) and the victim encoder f v . Analogously, we compute the cosine similarity score C(\u00b7, f v ).\n\nVictim's private data: CIFAR10 SVHN ImageNet  Table 3: Encoder similarity scores and p-values from dataset inference vs the number of queries. The quality of the stolen encoders increases with more stealing queries, which is reflected by the rise in the mutual information and cosine similarity scores as well as the better performance of our defense as indicated by the decreasing p-values. D P is the private dataset used to train the victim and D S is the dataset used for stealing. cosine similarity scores for the independent encoder being close to 0 and the scores for the stolen encoder being much higher than 0.\nEncoder Obfuscate D S(\u00b7, fv ) C(\u00b7, fv ) D S(\u00b7, fv ) C(\u00b7, fv ) D S(\u00b7, fv ) C(\u00b7, fv ) fv N/ACIFAR10\nRobustness of Metrics to Obfuscations. We also consider the effect of obfuscations on these metrics. Without any obfuscation of the representations from stolen encoders, the cosine similarity score shows a clearer distinction between stolen and independent encoders than the mutual information score: in Table 2, the cosine similarity scores for all independent encoders are close to zero, but the mutual information scores can be quite high (such as 0.9 for the independent encoders of SVHN, which is likely because of the mutual information score being based on an approximation). However, the mutual information score is robust to the obfuscations of the attackers while cosine similarity is not: in Table 2, the cosine similarity score for the stolen encoders after shuffling and padding drops close to zero. Mutual information, as a more general metric based on the information measurement instead of the brittle structure of the representation vectors, performs better and is oblivious to the obfuscations that attackers might introduce.\n\n\nLimitations\n\nIf the t-test run as part of dataset inference is inconclusive for an extracted encoder, we cannot state whether the encoder was stolen. Similarly, for an independent encoder, there is the possibility of it being incorrectly classified as stolen. Previous work [26,38] has shown that self-supervised encoders trained using heavy augmentations and contrastive learning generalize better than their supervised counterparts, which makes it harder for the dataset inference to differentiate between train and test representations in SSL than in the SL setting [14]. The loss values of projected individual representations are insufficient for dataset inference [14]. We build on top of this observation to enable dataset inference for encoders and use GMMs to distinguish between train and test representations.\n\n\nConclusions\n\nNew public APIs expose self-supervised encoder models which return high-dimensional embeddings for provided inputs. Adversaries can use these embeddings to steal the encoders. We present a novel method based on dataset inference for defending against such stealing attacks along with metrics to assess the quality of the stolen encoders and to quantify the effectiveness of our defense. We observe that knowledge contained in the private training set is transferred from the victim encoder to its stolen copy. Thus, the private data acts as a signature of the victim encoder. By leveraging density estimation on the respective encoders' representations, we obtain a signal allowing us to differentiate between the encoder's training and test data. This difference is detectable in both the victim encoder and its stolen copy but not in independent encoders which are legitimately trained on different data than the victim's private training data. Thus, we are able to flag the stolen copy of the victim encoder while not accusing creators of legitimately trained encoders of theft. We show the high effectiveness of our defense on vision encoders. Future work may explore additional applications of our proposed defense and metrics beyond model stealing and ownership verification, as well as their use in other domains such as natural language processing (NLP). In particular, our method may help enforce the ethical usage of sensitive online data, such as images on social media, in accordance with privacy regulations by auditing if a given provider's encoder contains knowledge of these sensitive data.\n\n[41] Or Sharir, Barak Peleg, and Yoav Shoham.  \n\n\nA Negative Societal Impacts\n\nOur work aims to defend self-supervised models against model stealing attacks. Since we are directly defending models and aim to provide attribution, any negative societal impacts of our work are minimal. One potentially negative impact could be if the t-test result is inconclusive about a stolen model being stolen or if it incorrectly identifies an independent model as stolen. However, as shown from our results, we are consistently able to differentiate correctly between stolen and independent models and can use our metrics to further reinforce the results from dataset inference. In terms of data and model access, we assume that the victim or a trusted third party, such as law enforcement, is responsible for running the dataset inference so that there are no privacy-related concerns.\n\n\nB Protocol for Dataset Inference\n\nWe design the following protocol for Dataset Inference:\n\n1. Select a third-trusted party as an arbitrator for the ownership resolution.\n\n2. Arbitrator ensures that the train and validation sets are IID (from the same distribution) by combining the train and test sets followed by a random split into the training set for the defended model and the private validation set used for dataset inference.\n\n3. Specification of the number of data points used for dataset inference: use all the data points from the validation set and the equivalent number of data points from the train set.\n\n\nC More Related Work C.1 Membership Inference\n\nEncoderMI [30] leverages the finding that an image encoder overfits to its pre-training dataset and returns more similar embeddings for pairs of augmented pre-training data points than for points not in the pre-training set. EncoderMI assumes some data points to be assessed as members and a shadow dataset as inputs. The first step is to create n augmentations of a point from the shadow dataset and compute for it n 2 (pair-wise) similarity scores using the embeddings extracted from a shadow encoder, which is trained on the shadow dataset. The scores form the membership feature vector for a given shadow data point. After labeling each such point as member or non-member, the membership features and the corresponding labels are used to train an inference classifier to infer if a given data point was a member or non-member of a target encoder. The early stopping is investigated as a mitigation defense against EncoderMI. The defense can reduce the effectiveness of the attack, however, at the cost of the lower performance of the defended encoder on downstream tasks.\n\n\nC.2 Non-Transferable Learning\n\nNon-Transferable Learning (NTL) [45] achieves ownership resolution and usage authorization by discouraging the model to generalize to data domains outside of its training data. To perform ownership resolution, NTL incentives the model to generalize poorly to a specific target domain, making it more likely to mis-classify on data from that domain. The authors argue that the model's unexpectedly poor behavior on the target domain can then be used like a watermark to claim ownership, and unlike many previous watermarking defenses, is harder to remove because the misclassification behavior is embedded in the model. To control usage authorization, NTL intentionally degrades the model performance on the target domain. The authors argue that this prevents users from applying the model on unauthorized data.\n\n\nC.3 Metrics to Compare Encoders\n\nThe desired metric for comparison between two representations should evaluate whether two representations are essentially similar or importantly different [13]. CIFAR100 [28]: The CIFAR100 dataset consists of 32x32 coloured images with 100 classes. There are 50000 training images and 10000 test images.\n\nSVHN [34]: The SVHN dataset contains 32x32 coloured images with 10 classes. There are roughly 73000 training images, 26000 test images and 530000 \"extra\" images.\n\nImageNet [11]: Larger sized coloured images with 1000 classes. As is commonly done, we resize all images to be of size 224x224. There are approximately 1 million training images and 50000 test images.\n\nSTL10 [8]: The STL10 dataset contains 96x96 coloured images with 10 classes. There are 5000 training images, 8000 test images, and 100000 unlabeled images.\n\n\nD.2 Encoder similarity scores and p-values\n\nWe present additional results for the encoder similarity scores and p-values from dataset inference vs the number of queries in Table ??.\n\n\nD.3 Details on Experimental Setup\n\nWe show a summary of the encoders used in our experiments in Table 4.\n\nThe ResNet18/ResNet34 architectures used for the CIFAR10 and SVHN victim encoders and the related stolen and independent encoders used a 3x3 Conv layer of stride 1 instead of the default 7x7 Conv layer and did not use a max pooling layer. When stealing from the ImageNet victim encoder, the images used for queries were resized to be of size 224x224. Similarly when training independent ResNet50 encoders to be used with the ImageNet victim encoder, the images in the respective datasets were resized to a size of 224x224.\n\nFor the results in Tables 1 and 2, encoders with the highest numbers of queries for each case were used. In other words, for encoders stolen from the CIFAR10 or SVHN victim encoders, 50K queries were used while for encoders stolen from the ImageNet victim encoder, 250K queries were used. Note that since CIFAR10 does not have 250K different examples, 60K queries from the aggregated training and test set were used when stealing with the CIFAR10 dataset. Encoders with a smaller number of queries were also stolen with the numbers of queries ranging from 500 -50K for the CIFAR10 and SVHN victim encoders, and queries ranging from 5K -250K for the ImageNet victim encoder.\n\nWe train the SVHN and CIFAR10 victim models for 200 epochs. To train the independent and stolen encoders, we used 100 epochs. When stealing from the ImageNet victim encoder, the SGD/LARS optimizer was used while for other models, the Adam optimizer was used. The initial learning rate Algorithm 1 Stealing an Encoder [14]. Input: Querying Dataset D, access to a victim encoder f v (w; \u03b8 v ).\n\nOutput: Stolen representation model f s (w; \u03b8 a ) 1: Initialize f s with a similar architecture as f v . 2: for sampled queries {x k } N k=1 \u2208 D do 3:\n\nQuery victim encoder to obtain representations:\ny v = f v (x k ) 4:\nGenerate representations from stolen encoder:\ny s = f s (x k ) 5:\nCompute loss L {y v , y s }. Sample an augmentation t.\n\n\n4:\n\nGenerate view w k = t(x k ).\n\n\n5:\n\nQuery the encoder f to generate the representation: y k = f (w k ). 6: end for 7: for each representation {y k } N k=1 \u2208 R d do 8:\n\nFind nearest neighbor distance: R k = ||y k \u2212 y i || 2 , where i = k.\n\n\n9:\n\nCompute transformation: z k = (N \u2212 1) \u00b7 (R k ) d . 10: end for 11: Compute the volume of the unit ball in R d :\nB d = \u03c0 d/2\n\u0393(1+d/2) . 12: Compute Entropy:\nH = 1 N N i=1 log z i + log B d + \u03b3\nwas kept constant in all cases and was adjusted with the Cosine Annealing scheduler. A batch size of 256 or 512 was used for training the models. The temperature parameters used varied between 0.1, 0.15, 0.2, and 0.25 with a larger temperature used for models with a higher number of queries. For all queries under 50K, the temperature was set to be 0.1.\n\nWe ran all experiments on machines equipped with an Intel\u00ae Xeon\u00ae Silver 4210 processor, 128 GB of RAM, and four NVIDIA GeForce RTX 2080 graphics cards, running Ubuntu 18.04.\n\n\nE Entropy Estimation\n\nWe present the entropy estimator in Algorithm 2 and the joint entropy estimator in Algorithm 3.\n\n\nF Linear Evaluation of Encoders\n\nIn Table 5, we show results for the downstream accuracies of models stolen from the encoder pretrained on the ImageNet dataset. The extraction of the representation model is possible at a fraction of the cost with a smaller number of queries (less than one-fifth) required to train the victim model. In general, the performance of the stolen encoders increases with the number of queries. We also perform a similar evaluation for victim encoders trained on the CIFAR10 and SVHN datasets and models stolen from these encoders in Tables 6 and 8, respectively.\n\n\nAlgorithm 3 Kozachenko-Leonenko Joint Entropy Estimator for Encoders.\n\nInput: Dataset D, number of data points N \u2265 2 to be sampled from D, an access to an encoder f (\u00b7), an access to an encoder g(\u00b7), the Euler-Mascheroni constant \u03b3 \u2248 0.577, and the gamma function \u0393. Output: Joint Entropy Estimation H.\n\n1: Sample N data points from D:\nx 1 , ..., x N . 2: for each sampled data point {x k } N k=1 \u2208 D do 3:\nSample an augmentation t.\n\n\n4:\n\nGenerate view w k = t(x k ).\n\n\n5:\n\nQuery the encoder f to generate the representation:\u0177 k = f (w k ).\n\n\n6:\n\nQuery the encoder g to generate the representation:\u0233 k = g(w k ).\n\n\n7:\n\nConcatenate the representations: y k =\u0177 k \u0233 k . 8: end for 9: for each representation {y k } N k=1 \u2208 R d do 10:\n\nFind nearest neighbor distance: R k = ||y k \u2212 y i || 2 , where i = k.\n\n\n11:\n\nCompute transformation: z k = (N \u2212 1) \u00b7 (R k ) 2d . 12: end for 13: Compute the volume of the unit ball in R d :\nB d = \u03c0 d/2\n\u0393(1+d/2) . 14: Compute Entropy: \nH = 1 N N i=1 log z i + log B 2d + \u03b3\n\nG Metrics for Measuring the Quality of Stolen Encoders\n\nThis section considers additional metrics for measuring the quality of stolen encoders. As in Section 5.3, we select a random sample of N inputs from the training set and find the representations of the encoders on each of the inputs. The representations for each input are then centered by subtracting the mean and normalized to be unit vectors. For each of the inputs, the p norm of the difference in representations by the encoders is computed where p = 1, 2, \u221e. The final value used as the metric is then the mean of these norms over all of the inputs. Tables 9, 10, and 11 show the results obtained for the 1 , 2 , and \u221e norms respectively. In a similar way, we find the cosine similarity between representations (closely related to the 2 norm) and present results in Table 12. All norms and the cosine similarity are able to differentiate between stolen and independent encoders, however the 1 , 2 norms and cosine similarity have a more clear difference.\n\n\nG.1 Analysis of Distance and Cosine Similarity Based Metrics\n\nWe use the 2 score based on the 2 norm of the distance between representations, and the cosine similarity score between representations as ways to measure the quality of stolen encoders and differentiate between stolen and independent encoders.\n\nWe create two score metrics, namely the cosine similarity score C:  Table 7: Linear evaluation accuracy, mutual information score, cosine similarity and p-values on a victim and stolen encoders. The victim encoder is pre-trained on the SVHN dataset. We observe higher performance on downstream tasks with more queries and similarly observe higher similarity scores for encoders stolen with more queries (see Table 3). where sim(a, b) = a T b ||a||2||b||2 is the cosine similarity between representation vectors a and b, and the 2 distance score which transforms the 2 norm of the difference as:\nC = |sim(a, b)|(2)Score 2 = 1 \u2212 1 2 a a 2 \u2212 b b 2 2(3)\nWe first note that there are various ways by which an attacker may steal an encoder. To simplify the analysis, we consider two main cases, one where the attacker minimizes the mean squared error between its representations and the representations returned by the victim and the other where the attacker minimizes the InfoNCE contrastive loss (other contrastive loss functions are similar).\n\nWith the MSE loss, the attacker directly minimizes the mean squared error between its representations and the representations of the victim encoder on the queries it makes. Let x i be a query made by an attacker and f v (x i ) = h vi , f s (x i ) = h si \u2208 R n be the representations of the victim and stolen encoders respectively for this query. The MSE loss between these two representations is then\n1 n n j=1 (h vi j \u2212 h si j ) 2 .\nComparatively, the 2 distance ( 2 norm of the difference) between these two representations is ||h vi \u2212 h si || 2 = n j=1 (h vi j \u2212 h si j ) 2 . From these two expressions, it follows directly that minimizing the MSE loss, which is equivalent to minimizing n j=1 (h vi j \u2212 h si j ) 2 , also minimizes the 2 distance.\n\nWe now consider the case where an attacker uses a contrastive loss function such as the InfoNCE loss [43], specifically as used in [5]. The InfoNCE loss consists of positive and negative pairs and encourages positive pairs to have similar representations and negative pairs to have dissimilar representations. When stealing from a victim encoder, the attacker uses its own representation and the representation from the victim encoder for a single query as a positive pairs while the other inputs in the batch are considered negative pairs. Given an input batch of queries {x 1 , x 2 , . . . , x m } Table 8: Linear evaluation accuracy, mutual information score, cosine similarity and p-values on a victim and stolen encoders. The victim encoder is pre-trained on the CIFAR10 dataset. We observe higher performance on downstream tasks with more queries and similarly observe higher similarity scores for encoders stolen with more queries (see Table 3   . Note that exp(r) > 0 \u2200r so that the loss L is always positive (the denominator contains the terms in the numerator and each term is positive so the fraction is < 1 and has a negative log).\n\nSimplifying L by combining the exponents in the numerator and using log a b = log a \u2212 log b gives:    Figure 4: Distribution of the normalized L2 score for the representations of an SVHN victim encoder, a stolen encoder from it (using CIFAR10 training data) and a random encoder (trained on CIFAR100). There is a pronounced difference in the distribution of the distances between stolen and independent encoders. This histogram relates to the values presented in Table 10.\nL = 1 2m (log m c=1 (2m\nWe now note that the cosine similarity is such that \u22121 \u2264 sim(a, b) \u2264 1. Therefore minimizing the loss corresponds to maximizing the sum of the cosine similarities between positive pairs m c=1 (sim(h sc , h vc )/\u03c4 )) (as this term is subtracted and the the overall loss is positive). This then corresponds to maximizing the individual cosine similarities sim(h sc , h vc ). In other words, the similarity between the representation of the victim and stolen encoders on the query samples x i is Table 12: Cosine similarity between normalized and centered representations from Encoders. We compute sim(\u00b7, f v ) between the representations of a given encoder (in a row) and the Victim encoder f v where sim is the cosine similarity.  Figure 5: Distribution of the normalized L2 score for the representations of a CIFAR10 victim encoder, a stolen encoder from it (using SVHN training data) and a random encoder (trained on CIFAR100). There is a pronounced difference in the distribution of the distances between stolen and independent encoders. This histogram relates to the values presented in Table 10.\n\nmaximized through the loss function. Note that the first term of the loss corresponds to minimizing the similarity between negative pairs, however, we do not focus on that aspect of the loss as part of this analysis. We now consider the relationship between the 2 norm of the difference of two unit vectors a and b and the cosine similarity sim(a, b) through the following theorem: sim(a, b)) for unit vectors a, b.\nTheorem 1 ||a \u2212 b|| 2 = 2(1 \u2212\nProof: sim(a, b) = a T b ||a||2||b||2 = a T b, since a and b are unit vectors. sim(a, b)) Therefore maximizing the cosine similarity sim(h si , h vi ) through the InfoNCE loss means minimizing the 2 norm of the difference between the normalized representations h si , h vi . Similarly, minimizing the 2 norm through the MSE loss corresponds to maximizing the cosine similarity. It also follows from this theorem that the 2 distance ||a \u2212 b|| 2 is such that 0 \u2264 ||a \u2212 b|| 2 \u2264 2. We therefore divide the 2 distance by 2 to get the distance to be between 0 and 1. Transforming the  Figure 6: Distribution of the cosine similarity scores for the representations of a CIFAR10 victim encoder, a stolen encoder from it (using SVHN training data) and a random encoder (trained on CIFAR100). There is a pronounced difference in the distribution of the scores between stolen and independent encoders. This histogram relates to the values presented in Table 12. 2 distance into the 2 score, allows us to relate it more closely to the cosine similarity so that an increase in the cosine similarity corresponds to an increase in the 2 score. The metrics are also made to have ranges between 0 and 1 through these scores.\n||a\u2212b|| 2 2 = (a\u2212b) T (a\u2212b) = (a T \u2212b T )(a\u2212b) = a T a\u2212a T b\u2212b T a+b T b = ||a|| 2 \u22122a T b+||b|| 2 = 1 \u2212 2a T b + 1 = 2 \u2212 2a T b = 2(1 \u2212 sim(a, b)) \u2234 ||a \u2212 b|| 2 = 2(1 \u2212\nTheorem 1 allows us to relate our 2 score and cosine similarity score. We have: sim(a, b))\nScore 2 = 1 \u2212 1 2 a a 2 \u2212 b b 2 2 = 1 \u2212 1 2 2(1 \u2212\nThis can equivalently be written as:  sim(a, b) = 1 \u2212 2(1 \u2212 Score 2 ) 2 C = |1 \u2212 2(1 \u2212 Score 2 ) 2 | When computing the distances and similarity, centering and normalizing the representations before computing the scores is important to get useful metrics. Centering (i.e. subtracting the mean of the elements in each representation from each element) allows for the values in the representations to be distributed in a similar way about the mean so that values above the mean are positive while values below the mean are negative. Normalizing the representations scales the values in the representations from the two encoders being compared to a similar range which then makes the metrics consistent across different encoders where representations may have different ranges of values. Moreover, normalizing the representations allows for Theorem 1 to be applied which then gives us bounded score metrics and a relationship between the 2 score and cosine similarity score.\n\n\nH Additional Figures\n\nIn this section, we present additional figures. Figure 7 presents the full overview of our dataset inference method for the self-supervised models. Figure 1 presents the resolution of the encoder ownership (this is a simplified version of Figure 7).\n\n\nI Number of Queries For Dataset Inference\n\nIn Table 15, we check how the dataset inference performs after fine-tuning the stolen model with a different number of samples.\n\nIn Table 16, we check how the dataset inference performs after fine-tuning the stolen model with a different number of epochs. 2 Adversary steals fv: submit queries from dataset DS and obtain representations hD S to train the stolen encoder fs. 3 Arbitrator trains density estimators: divide DP into non-overlapping partitions DP 1 and DP 2, and train density estimators Ev, Es and Ei using the representations of fv, fs, fi on DP 2, respectively. 4 Arbitrator performs dataset inference: apply Ev, Es and Ei on the representations of DP 1 and DN of each encoder. For the victim and stolen encoders, the log-likelihood of the representations of DP 1 is significantly higher than DN , whereas, for an independent encoder, the log-likelihoods of the representations are not significantly different. \n\n4 .\n4Estimating Densities. The arbitrator generates representations of D P 1 and (a subset of) D N with each encoder f \u2208 {f v , f s , f i }. Applying the respective density estimator E \u2208 {E v , E s , E i } on the representations yields the log-likelihood of each data point x in the respective dataset: \u2200x \u2208 D : p(x) = E(f (x)).\n\nFigure 3 :\n3Distribution of cosine similarity scores.Setup. To measure the quality of stolen encoders, we select a random subset of N = 20K unaugmented images from the private training dataset D P and compute their representations from stolen and victim encoders. We then centralize (subtract the mean for each dimension) and normalize the representations (divide by the 2 norm). For the mutual information score, we first estimate the entropies H(f v ), H(f s ), H(f v , f s ), which are then added and normalized as in Section 4.1. The score is capped to be in the range [0, 1]. To compute the cosine similarity score, we find the absolute value of the dot product of corresponding representations for the two encoders (Equation 2). These dot products are then averaged over all representations.\n\nChecklist 1 .\n1For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Appendix Section 5.4. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Appendix Section A. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] (b) Did you include complete proofs of all theoretical results? [Yes] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Provided with the supplementary materials. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Appendix D. (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix D. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] See Section 2 (b) Did you mention the license of the assets? [No] All licences MIT. (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [No] Used datasets are either synthetic or popular standard datasets. (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [No] Used datasets are either synthetic or popular standard datasets. 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\n\n\nencoder parameters \u03b8 s := \u03b8 s \u2212 \u03b7\u2207 \u03b8s L. 7: end for Algorithm 2 Kozachenko-Leonenko Entropy Estimator for Encoders. Input: Dataset D, number of data points N \u2265 2 to be sampled from D, access to an encoder f (\u00b7), the Euler-Mascheroni constant \u03b3 \u2248 0.577, and the gamma function \u0393. Output: Entropy Estimation H. 1: Sample N data points from D: x 1 , ..., x N . 2: for each sampled data point {x k } N k=1 \u2208 D do 3:\n\n\nsent by an attacker, the representations for each query from both the victim and stolen encoders are concatenated as {h s1 , h s2 , . . . , h sm , h v1 , h v2 , . . . , h vm } where h si = f s (x i ) and h vi = f v (x i ) are the representations by the stolen and victim encoders respectively. The positive pairs are (h s1 , h v1 ), . . . , (h sm , h vm ) and the loss between a positive pair of samples(h si , h vi ) is defined as l(s i , v i ) = \u2212 log exp (sim(hs i ,hv i )/\u03c4 ) 2m k=1 1 [k =s i ] exp (sim(hs i ,h k )/\u03c4 ) . Here sim is the cosine similarity function (sim(u, v) = u T v ||u||2||v||2 ),\u03c4 is the temperature parameter, and 1 [k =si] is an indicator function equal to 1 iff k = s i and 0 otherwise. The overall loss for the batch is the sum of the losses over each positive pair i.e. L = 1 2m m c=1 [l(s c , v c ) + l(v c , s c )]. We can combine the log terms to rewrite this loss function as L = \u2212 1 2m log m c=1 (exp (sim(hs c ,hv c )/\u03c4 )) 2 m c=1 ( 2m k=1 1 [k =sc ] exp (sim(hs c ,h k )/\u03c4 ))( 2m k=1 1 [k =vc ] exp (sim(hv c ,h k )/\u03c4 ))\n\nFigure 7 :\n7Dataset Inference on Encoders. 1 Victim trains encoder fv using private training data DP .\n\n\nencoders even if the attacker only steals from a small number of queries. For example, in61e-82 \n18.92 \nSVHN \n2.75e-125 \n23.88 \nImageNet \n6.23e-14 \n7.09 \n\nfs \nN/A \n\nSVHN \n3.97e-2 \n3.04 \nSVHN \n6.35e-41 \n13.36 \nSVHN \n3.33e-4 \n4.04 \nCIFAR10 \n8.73e-7 \n5.09 \nCIFAR10 \n2.38e-4 \n4.61 \nCIFAR10 \n1.47e-4 \n6.21 \nSTL10 \n1.04e-2 \n3.42 \nSTL10 \n1.23e-5 \n5.22 \nSTL10 \n1.09e-4 \n5.87 \nImageNet \n6.34e-3 \n3.47 \nImageNet \n9.81e-3 \n3.74 \nImageNet \n3.14e-5 \n7.32 \n\nfs \n\nShuffle \nCIFAR10 \n1.72e-6 \n4.98 \nCIFAR10 \n7.32e-4 \n4.77 \nCIFAR10 \n6.72e-4 \n5.21 \nPad \nCIFAR10 \n3.44e-6 \n4.84 \nCIFAR10 \n2.51e-3 \n3.08 \nCIFAR10 \n2.31e-3 \n4.23 \nTransform \nCIFAR10 \n6.81e-7 \n5.11 \nCIFAR10 \n6.45e-3 \n3.32 \nCIFAR10 \n8.45e-3 \n3.98 \n\nfi \nN/A \nCIFAR100 \n3.67e-1 \n-0.37 \nCIFAR100 \n6.21e-1 \n0.52 \nCIFAR100 \n7.53e-2 \n1.63 \nSVHN \n2.96e-1 \n0.98 \nCIFAR10 \n4.82e-1 \n0.56 \nSVHN \n5.42e-1 \n0.69 \n\n\n\n\nThe cost of training NLP models: A concise overview. CoRR, abs/2004.08900, 2020. URL https://arxiv.org/abs/2004.08900. [42] Florian Tram\u00e8r, F. Zhang, A. Juels, M. Reiter, and T. Ristenpart. Stealing machine learning models via prediction apis. USENIX Security Symposium, 2016.[43] A\u00e4ron van den Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive \ncoding. ArXiv, abs/1807.03748, 2018. \n\n[44] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and \nBen Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. \nIn 2019 IEEE Symposium on Security and Privacy (SP), pages 707-723. IEEE, 2019. \n\n[45] Lixu Wang, Shichao Xu, Ruiqi Xu, Xiao Wang, and Qi Zhu. Non-transferable learning: A new \napproach for model ownership verification and applicability authorization. In International \nConference on Learning Representations, 2022. URL https://openreview.net/forum? \nid=tYRrOdSnVUy. \n\n\nTable 4 :\n4Summary of our encoders. We show all possible combinations of victim, stolen, and independent encoders along with their architectures. *, ** denotes that the models listed are equivalent.VICTIM ENCODER \nSTOLEN ENCODER \nINDEPENDENT ENCODER \nD \nARCHITECTURE \nD \nARCHITECTURE \n# QUERIES \nD \nARCHITECTURE \nCIFAR10 \nRESNET34 \nCIFAR10 \nRESNET34 \n500 -50K \nCIFAR100* \nRESNET18 \nCIFAR10 \nRESNET34 \nSVHN \nRESNET34 \n500 -50K \nCIFAR100* \nRESNET18 \nCIFAR10 \nRESNET34 \nSTL10 \nRESNET18 \n500 -50K \nSVHN \nRESNET34 \n\nSVHN \nRESNET34 \nCIFAR10 \nRESNET34 \n500 -50K \nCIFAR100* \nRESNET18 \nSVHN \nRESNET34 \nSVHN \nRESNET34 \n500 -50K \nCIFAR100* \nRESNET18 \nSVHN \nRESNET34 \nSTL10 \nRESNET34 \n500 -50K \nCIFAR10 \nRESNET34 \n\nIMAGENET \nRESNET50 \nCIFAR10 \nRESNET50 \n5K -50K \nCIFAR100** \nRESNET50 \nIMAGENET \nRESNET50 \nSVHN \nRESNET50 \n5K -250K \nCIFAR100** \nRESNET50 \nIMAGENET \nRESNET50 \nIMAGENET \nRESNET50 \n5K -250K \nSVHN \nRESNET50 \n\nD Additional Details on Experiments \n\nD.1 Datasets Used \n\nCIFAR10 [28]: The CIFAR10 dataset consists of 32x32 colored images with 10 classes. There are \n50000 training images and 10000 test images. \n\n\n\nTable 5 :\n5Linear Evaluation Accuracy on a victim and stolen encoders. The victim encoder is pre-trained on the ImageNet dataset.# OF QUERIES \nDATASET \nDATA TYPE \nCIFAR10 CIFAR100 STL10 SVHN F-MNIST \n\nVictim Encoder \nN/A \nN/A \n90.33 \n71.45 \n94.9 \n79.39 \n91.9 \n\n60K \nCIFAR10 \nTRAIN/TEST \n83.3 \n57.0 \n71.2 \n73.8 \n90.7 \n250K \nIMAGENET \nTRAIN \n80.0 \n57.0 \n85.8 \n71.5 \n90.2 \n5K \nSVHN \nEXTRA \n42.0 \n16.2 \n34.4 \n26.9 \n81.3 \n10K \nSVHN \nEXTRA \n60.8 \n33.0 \n50.5 \n71.7 \n87.5 \n50K \nSVHN \nEXTRA \n73.3 \n47.1 \n58.2 \n78.8 \n90.4 \n100K \nSVHN \nEXTRA \n76.3 \n50.2 \n61.1 \n78.2 \n90.8 \n200K \nSVHN \nEXTRA \n76.9 \n52.0 \n62.1 \n78.3 \n90.8 \n250K \nSVHN \nEXTRA \n77.1 \n52.6 \n61.9 \n80.2 \n91.4 \n\n\n\nTable 6 :\n6Linear Evaluation Accuracy on a victim and stolen encoders. The victim encoder is pre-trained on the CIFAR10 dataset.# OF QUERIES \nDATASET \nCIFAR10 STL10 SVHN \n\nVictim Encoder \nN/A \n87.4 \n73.4 \n49.5 \n\n50K \nSVHN \n61.2 \n51.7 \n54.8 \n50K \nCIFAR10 \n84.8 \n70.7 \n52.4 \n50K \nSTL10 \n86.8 \n73.0 \n49.8 \n\n\n\nTable 9 :\n91 distance between normalized and centered representations from Encoders. We compute d(\u00b7, f v ) between the representations of a given encoder (in a row) and the Victim encoder f v where d is the 1 norm.CIFAR10 \nSVHN \nIMAGENET \nENCODER \nDATASET \nd(\u00b7, fv) \nDATASET \nd(\u00b7, fv) \nDATASET \nd(\u00b7, fv) \n\nfs \n\nSVHN \n14.74 \u00b1 0.22 \nSVHN \n6.92 \u00b1 0.10 \nSVHN \n25.78 \u00b1 0.26 \nCIFAR10 \n5.19 \u00b1 0.05 \nCIFAR10 \n11.92 \u00b1 0.21 \nCIFAR10 \n24.89 \u00b1 0.28 \nSTL10 \n6.24 \u00b1 0.08 \nSTL10 \n7.22 \u00b1 0.14 \nIMAGENET 16.68 \u00b1 0.21 \n\nfi \n\nSVHN \n22.20 \u00b1 0.07 \nCIFAR10 \n22.17 \u00b1 0.06 \nSVHN \n35.45 \u00b1 0.21 \nCIFAR100 23.65 \u00b1 0.07 CIFAR100 23.13 \u00b1 0.07 CIFAR100 38.96 \u00b1 0.25 \n\n\n\n\nk=1 1 [k =sc] exp (sim(h sc , h k )/\u03c4 ))( k=1 1 [k =vc] exp (sim(h vc , h k )/\u03c4 )) \u22122m \n\n1 \n\n2m (log exp( \n\nm \n\nc=1 (2 \u00b7 sim(h sc , h vc )/\u03c4 ))) \nL = 1 \n2m (log \n\nm \n\nc=1 ( \n\n2m \n\nk=1 1 [k =sc] exp (sim(h sc , h k )/\u03c4 ))( \n\n2m \n\nk=1 1 [k =vc] exp (sim(h vc , h k )/\u03c4 )) \u2212 \n\n1 \n\nm ( \n\nm \n\nc=1 (sim(h sc , h vc )/\u03c4 )) \n\n\nTable 10 :\n102 distance between normalized and centered representations from Encoders. We compute d(\u00b7, f v ) between the representations of a given encoder (in a row) and the Victim encoder f v where d is the 2 norm divided by 2.CIFAR10 \nSVHN \nIMAGENET \nENCODER \nDATASET \nd(\u00b7, fv) \nDATASET \nd(\u00b7, fv) \nDATASET \nd(\u00b7, fv) \n\nfs \n\nSVHN \n0.49 \u00b1 0.007 \nSVHN \n0.21 \u00b1 0.003 \nSVHN \n0.55 \u00b1 0.04 \nCIFAR10 \n0.16 \u00b1 0.02 \nCIFAR10 \n0.39 \u00b1 0.007 \nCIFAR10 \n0.53 \u00b1 0.004 \nSTL10 \n0.19 \u00b1 0.003 \nSTL10 \n0.23 \u00b1 0.005 IMAGENET 0.33 \u00b1 0.004 \n\nfi \n\nSVHN \n0.71 \u00b1 0.001 \nCIFAR10 \n0.70 \u00b1 0.01 \nSVHN \n0.71 \u00b1 0.007 \nCIFAR100 0.71 \u00b1 0.001 CIFAR100 \n0.71 \u00b1 0.01 \nCIFAR100 0.71 \u00b1 0.007 \n\n\n\nTable 11 :\n11\u221e distance between normalized and centered representations from Encoders. We compute d(\u00b7, f v ) between the representations of a given encoder (in a row) and the Victim encoder f v where d is the \u221e norm. CIFAR100 0.263 \u00b1 0.003 CIFAR100 0.31 \u00b1 0.006 CIFAR100 0.27 \u00b1 0.006CIFAR10 \nSVHN \nIMAGENET \nENCODER \nDATASET \nd(\u00b7, fv) \nDATASET \nd(\u00b7, fv) \nDATASET \nd(\u00b7, fv) \n\nfs \n\nSVHN \n0.13 \u00b1 0.003 \nSVHN \n0.10 \u00b1 0.002 \nSVHN \n0.27 \u00b1 0.005 \nCIFAR10 \n0.06 \u00b1 0.001 \nCIFAR10 \n0.19 \u00b1 0.004 \nCIFAR10 \n0.26 \u00b1 0.005 \nSTL10 \n0.08 \u00b1 0.002 \nSTL10 \n0.12 \u00b1 0.003 IMAGENET 0.15 \u00b1 0.004 \n\nfi \n\nSVHN \n0.256 \u00b1 0.003 \nCIFAR10 \n0.31 \u00b1 0.006 \nSVHN \n0.33 \u00b1 0.004 \n\n\nTable 13 :\n132 score vs the number of queries. The quality of the stolen encoder should increase with respect to the number of queries. Therefore, we should be able to observe an increasing trend for the 2 score.Victim Encoder \nStolen dataset \nNumber of Queries \n\nCIFAR10 \nSVHN \n500 \n5K \n10K \n20K \n30K \n50K \n\n0.322 \u00b1 0.005 \n0.372 \u00b1 0.005 \n0.411 \u00b1 0.006 \n0.421 \u00b1 0.006 \n0.475 \u00b1 0.007 \n0.511 \u00b1 0.007 \n\nCIFAR10 \nSTL10 \n500 \n5K \n10K \n20K \n30K \n50K \n\n0.564 \u00b1 0.007 \n0.749 \u00b1 0.004 \n0.781 \u00b1 0.003 \n0.795 \u00b1 0.003 \n0.811 \u00b1 0.002 \n0.807 \u00b1 0.003 \n\nSVHN \nCIFAR10 \n500 \n5K \n10K \n20K \n30K \n50K \n\n0.386 \u00b1 0.006 \n0.459 \u00b1 0.007 \n0.516 \u00b1 0.007 \n0.551 \u00b1 0.007 \n0.580 \u00b1 0.007 \n0.614 \u00b1 0.007 \n\nSVHN \nSTL10 \n500 \n5K \n10K \n20K \n30K \n50K \n\n0.458 \u00b1 0.007 \n0.611 \u00b1 0.007 \n0.667 \u00b1 0.006 \n0.736 \u00b1 0.005 \n0.738 \u00b1 0.005 \n0.769 \u00b1 0.005 \n\nImageNet \nSVHN \n5k \n10k \n50k \n100k \n200k \n250k \n\n0.390 \u00b1 0.001 \n0.418 \u00b1 0.001 \n0.444 \u00b1 0.001 \n0.446 \u00b1 0.001 \n0.452 \u00b1 0.001 \n0.450 \u00b1 0.001 \n\nImageNet \nImageNet \n5k \n10k \n50k \n100k \n200k \n250k \n\n0.407 \u00b1 0.001 \n0.493 \u00b1 0.001 \n0.448 \u00b1 0.001 \n0.515 \u00b1 0.001 \n0.661 \u00b1 0.001 \n0.674 \u00b1 0.001 \n\n\n\nTable 14 :\n14Cosine similarity score vs the number of queries. The quality of the stolen encoder should increase with respect to the number of queries. Therefore, we should be able to observe a similar increasing trend for the cosine similarity score.Victim Encoder \nStolen dataset \nNumber of Queries \n\nCIFAR10 \nSVHN \n500 \n5K \n10K \n20K \n30K \n50K \n\n0.073 \u00b1 0.012 \n0.205 \u00b1 0.012 \n0.294 \u00b1 0.014 \n0.318 \u00b1 0.014 \n0.433 \u00b1 0.015 \n0.508 \u00b1 0.013 \n\nCIFAR10 \nSTL10 \n500 \n5K \n10K \n20K \n30K \n50K \n\n0.606 \u00b1 0.012 \n0.869 \u00b1 0.004 \n0.901 \u00b1 0.003 \n0.913 \u00b1 0.003 \n0.927 \u00b1 0.002 \n0.923 \u00b1 0.002 \n\nSVHN \nCIFAR10 \n500 \n5K \n10K \n20K \n30K \n50K \n\n0.235 \u00b1 0.014 \n0.400 \u00b1 0.015 \n0.518 \u00b1 0.013 \n0.582 \u00b1 0.013 \n0.632 \u00b1 0.012 \n0.689 \u00b1 0.010 \n\nSVHN \nSTL10 \n500 \n5K \n10K \n20K \n30K \n50K \n\n0.396 \u00b1 0.015 \n0.682 \u00b1 0.012 \n0.767 \u00b1 0.009 \n0.852 \u00b1 0.006 \n0.856 \u00b1 0.006 \n0.887 \u00b1 0.005 \n\nImageNet \nSVHN \n5k \n10k \n50k \n100k \n200k \n250k \n\n0.254 \u00b1 0.001 \n0.320 \u00b1 0.001 \n0.377 \u00b1 0.002 \n0.383 \u00b1 0.002 \n0.395 \u00b1 0.002 \n0.391 \u00b1 0.002 \n\nImageNet \nImageNet \n5k \n10k \n50k \n100k \n200k \n250k \n\n0.295 \u00b1 0.001 \n0.480 \u00b1 0.002 \n0.386 \u00b1 0.002 \n0.526 \u00b1 0.002 \n0.766 \u00b1 0.001 \n0.783 \u00b1 0.001 \n\n\n\nTable 15 :\n15Dataset Inference for fine-tuning with a different number of samples. We detect if a given encoder was stolen after fine-tuning with a different number of samples. We use a stolen encoder from the SVHN victim model and then retrain it with standard contrastive training using 5 epochs and data from CIFAR10.# of points \np-value \n\u2206\u00b5 \n\n5K \n3.24e-16 \n7.03 \n10K \n1.82e-16 \n7.14 \n20K \n6.91e-14 \n6.09 \n50K \n5.28e-12 \n5.53 \n\n\n\nTable 16 :\n16Dataset Inference for fine-tuning with a different number of epochs. We detect if a given encoder was stolen after fine-tuning with a different number of epochs. We use a stolen encoder from the SVHN victim model and then retrain it with standard contrastive learning using 50K data points from CIFAR10.# of epochs \np-value \n\u2206\u00b5 \n\n5 \n5.28e-12 \n5.53 \n10 \n8.73e-6 \n4.62 \n25 \n6.81e-1 \n1.34 \n50 \n1.73e-1 \n0.92 \n100 \n8.53e-1 \n-0.53 \n\nhttps://github.com/kuangliu/pytorch-cifar\nAcknowledgmentsWe would like to acknowledge our sponsors, who support our research with financial and in-kind contributions: CIFAR through the Canada CIFAR AI Chair program, DARPA through the GARD program, Intel, Meta, NFRF through an Exploration grant, and NSERC through the Discovery Grant and COHESA Strategic Alliance. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute. We would like to thank members of the CleverHans Lab for their feedback.\n. Cohere, Cohere, https://cohere.ai. URL https://cohere.ai/.\n\n. Openai, Openai, https://openai.com. URL https://openai.com/.\n\nLearning representations by maximizing mutual information across views. Philip Bachman, Devon Hjelm, William Buchwalter, Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views, 2019. URL https://arxiv.org/abs/1906. 00910.\n\nA systematic review on model watermarking for neural networks. Franziska Boenisch, 10.3389/fdata.2021.729663Frontiers in Big Data. 4Franziska Boenisch. A systematic review on model watermarking for neural networks. Frontiers in Big Data, 4, nov 2021. doi: 10.3389/fdata.2021.729663.\n\nA simple framework for contrastive learning of visual representations. S Ting Chen, M Kornblith, G Norouzi, Hinton, International Conference on Machine Learning. Ting Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual representations. International Conference on Machine Learning, 2020.\n\nExploring simple siamese representation learning. Xinlei Chen, Kaiming He, Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. 2020.\n\nRefit: a unified watermark removal framework for deep learning systems with limited data. Xinyun Chen, Wenxiao Wang, Chris Bender, Yiming Ding, Ruoxi Jia, Bo Li, Dawn Song, Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security. the 2021 ACM Asia Conference on Computer and Communications SecurityXinyun Chen, Wenxiao Wang, Chris Bender, Yiming Ding, Ruoxi Jia, Bo Li, and Dawn Song. Refit: a unified watermark removal framework for deep learning systems with limited data. In Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security, pages 321-335, 2021.\n\nAn Analysis of Single Layer Networks in Unsupervised Feature Learning. Adam Coates, Andrew Ng, Honglak Lee, AISTATS. Adam Coates, Andrew Ng, and Honglak Lee. An Analysis of Single Layer Networks in Unsupervised Feature Learning. In AISTATS, 2011. https://cs.stanford.edu/~acoates/ papers/coatesleeng_aistats_2011.pdf.\n\nSslguard: A watermarking scheme for selfsupervised learning pre-trained encoders. CoRR, abs/2201.11692, 2022. Tianshuo Cong, Xinlei He, Yang Zhang, Tianshuo Cong, Xinlei He, and Yang Zhang. Sslguard: A watermarking scheme for self- supervised learning pre-trained encoders. CoRR, abs/2201.11692, 2022. URL https://arxiv. org/abs/2201.11692.\n\nElements of Information Theory. M Thomas, Joy A Cover, Thomas, Telecommunications and Signal Processing. Wiley-Interscience, USAWileyISBN 0471241954Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecom- munications and Signal Processing). Wiley-Interscience, USA, 2006. ISBN 0471241954.\n\nImagenet: A largescale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large- scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nGrounding representation similarity through statistical testing. Frances Ding, Jean-Stanislas Denain, Jacob Steinhardt, Advances in Neural Information Processing Systems. A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman VaughanFrances Ding, Jean-Stanislas Denain, and Jacob Steinhardt. Grounding representation sim- ilarity through statistical testing. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wort- man Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=_kwj6V53ZqB.\n\nOn the difficulty of defending self-supervised learning against model extraction. Adam Dziedzic, Nikita Dhawan, Muhammad Ahmad Kaleem, Jonas Guan, Nicolas Papernot, International Conference on Machine Learning. Adam Dziedzic, Nikita Dhawan, Muhammad Ahmad Kaleem, Jonas Guan, and Nicolas Pa- pernot. On the difficulty of defending self-supervised learning against model extraction. In International Conference on Machine Learning, 2022.\n\nIncreasing the cost of model extraction with calibrated proof of work. Adam Dziedzic, Muhammad Ahmad Kaleem, Yu Shen Lu, Nicolas Papernot, International Conference on Learning Representations. Adam Dziedzic, Muhammad Ahmad Kaleem, Yu Shen Lu, and Nicolas Papernot. Increasing the cost of model extraction with calibrated proof of work. In International Conference on Learning Representations, 2022. URL https://arxiv.org/abs/2201.09243.\n\nDeep Learning. Ian Goodfellow, Yoshua Bengio, Aaron Courville, MIT PressIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.\n\nProvable guarantees for selfsupervised deep learning with spectral contrastive loss. Colin Jeff Z Haochen, Adrien Wei, Tengyu Gaidon, Ma, Advances in Neural Information Processing Systems. 342021Jeff Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for self- supervised deep learning with spectral contrastive loss. Advances in Neural Information Processing Systems, 34, 2021.\n\nQuantifying and mitigating privacy risks of contrastive learning. Xinlei He, Yang Zhang, 10.1145/3460120.3484571Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security, CCS '21. the 2021 ACM SIGSAC Conference on Computer and Communications Security, CCS '21New York, NY, USAAssociation for Computing MachineryXinlei He and Yang Zhang. Quantifying and mitigating privacy risks of contrastive learning. In Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security, CCS '21, page 845-863, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450384544. doi: 10.1145/3460120.3484571. URL https://doi.org/10.1145/ 3460120.3484571.\n\nHigh accuracy and high fidelity extraction of neural networks. Matthew Jagielski, N Carlini, D Berthelot, A Kurakin, N Papernot, USENIX Security Symposium. Matthew Jagielski, N. Carlini, D. Berthelot, A. Kurakin, and N. Papernot. High accuracy and high fidelity extraction of neural networks. USENIX Security Symposium, 2020.\n\nA survey on contrastive self-supervised learning. Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, Fillia Makedon, 10.3390/technologies901000292021TechnologiesAshish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A survey on contrastive self-supervised learning. Technologies, 9(1), 2021. ISSN 2227-7080. doi: 10.3390/technologies9010002. URL https://www.mdpi.com/2227-7080/ 9/1/2.\n\nEntangled watermarks as a defense against model extraction. C A Hengrui Jia, V Choquette-Choo, N Chandrasekaran, Papernot, USENIX Security Symposium. Hengrui Jia, C. A. Choquette-Choo, V. Chandrasekaran, and N. Papernot. Entangled watermarks as a defense against model extraction. USENIX Security Symposium, 2021.\n\nHengrui Jia, Mohammad Yaghini, A Christopher, Natalie Choquette-Choo, Dullerud, arXiv:2103.05633Anvith Thudi, Varun Chandrasekaran, and Nicolas Papernot. Proof-of-learning: Definitions and practice. arXiv preprintHengrui Jia, Mohammad Yaghini, Christopher A Choquette-Choo, Natalie Dullerud, Anvith Thudi, Varun Chandrasekaran, and Nicolas Papernot. Proof-of-learning: Definitions and practice. arXiv preprint arXiv:2103.05633, 2021.\n\nPrada: protecting against dnn model stealing attacks. Mika Juuti, Sebastian Szyller, Samuel Marchal, N Asokan, 2019 IEEE European Symposium on Security and Privacy (EuroS&P). IEEEMika Juuti, Sebastian Szyller, Samuel Marchal, and N Asokan. Prada: protecting against dnn model stealing attacks. In 2019 IEEE European Symposium on Security and Privacy (EuroS&P), pages 512-527. IEEE, 2019.\n\nSelective supervision: Guiding supervised learning with decision-theoretic active learning. Ashish Kapoor, Eric Horvitz, Sumit Basu, IJCAI. 7Ashish Kapoor, Eric Horvitz, and Sumit Basu. Selective supervision: Guiding supervised learning with decision-theoretic active learning. In IJCAI, volume 7, pages 877-882, 2007.\n\nSimilarity of neural network representations revisited. Simon Kornblith, Mohammad Norouzi, Honglak Lee, Geoffrey Hinton, Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited, 2019. URL https://arxiv.org/abs/1905.00414.\n\nContrasting contrastive self-supervised representation learning pipelines. Klemen Kotar, Gabriel Ilharco, Ludwig Schmidt, Kiana Ehsani, Roozbeh Mottaghi, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Klemen Kotar, Gabriel Ilharco, Ludwig Schmidt, Kiana Ehsani, and Roozbeh Mottaghi. Con- trasting contrastive self-supervised representation learning pipelines. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 9949-9959, October 2021.\n\nSample estimate of the entropy of a random vector. L F Kozachenko, N N Leonenko, Probl. Peredachi Inf. 23L. F. Kozachenko and N. N. Leonenko. Sample estimate of the entropy of a random vector. Probl. Peredachi Inf., 23:9-16, 1987. URL http://www.mathnet.ru/links/ 5e0609d41b53e39bccea1f8711152ecd/ppi797.pdf.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Technical reportAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\n\nBioBERT: a pre-trained biomedical language representation model for biomedical text mining. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, 10.1093/bioinformatics/btz6821367-4803. doi: 10. 1093/bioinformatics/btz682Bioinformatics. 364Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234-1240, 09 2019. ISSN 1367-4803. doi: 10. 1093/bioinformatics/btz682. URL https://doi.org/10.1093/bioinformatics/btz682.\n\nEncodermi: Membership inference against pre-trained encoders in contrastive learning. Hongbin Liu, Jinyuan Jia, Wenjie Qu, Neil Zhenqiang Gong, Proceedings of the 2021. the 2021Hongbin Liu, Jinyuan Jia, Wenjie Qu, and Neil Zhenqiang Gong. Encodermi: Membership inference against pre-trained encoders in contrastive learning. In Proceedings of the 2021\n\nAssociation for Computing Machinery. 10.1145/3460120.3484749ACM SIGSAC Conference on Computer and Communications Security, CCS '21. New York, NY, USA, 2021ACM SIGSAC Conference on Computer and Communications Security, CCS '21, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450384544. URL https: //doi.org/10.1145/3460120.3484749.\n\nDataset inference: Ownership resolution in machine learning. Pratyush Maini, Mohammad Yaghini, Nicolas Papernot, Proceedings of ICLR 2021: 9th International Conference on Learning Representationsn. ICLR 2021: 9th International Conference on Learning RepresentationsnPratyush Maini, Mohammad Yaghini, and Nicolas Papernot. Dataset inference: Ownership resolution in machine learning. In Proceedings of ICLR 2021: 9th International Conference on Learning Representationsn, 2021.\n\nFormal limitations on the measurement of mutual information. David Mcallester, Karl Stratos, PMLRProceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics. Silvia Chiappa and Roberto Calandrathe Twenty Third International Conference on Artificial Intelligence and Statistics108David McAllester and Karl Stratos. Formal limitations on the measurement of mutual in- formation. In Silvia Chiappa and Roberto Calandra, editors, Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Pro- ceedings of Machine Learning Research, pages 875-884. PMLR, 26-28 Aug 2020. URL https://proceedings.mlr.press/v108/mcallester20a.html.\n\nInsights on representational similarity in neural networks with canonical correlation. Ari S Morcos, Maithra Raghu, Samy Bengio, Ari S. Morcos, Maithra Raghu, and Samy Bengio. Insights on representational similarity in neural networks with canonical correlation, 2018.\n\nReading digits in natural images with unsupervised feature learning. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y Ng, NIPS Workshop on Deep Learning and Unsupervised Feature Learning. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011, 2011. URL http://ufldl.stanford. edu/housenumbers/nips2011_housenumbers.pdf.\n\nPrediction poisoning: Towards defenses against dnn model stealing attacks. Tribhuvanesh Orekondy, Bernt Schiele, Mario Fritz, International Conference on Learning Representations. Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Prediction poisoning: Towards defenses against dnn model stealing attacks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SyevYxHtDB.\n\nFailing loudly: An empirical study of methods for detecting dataset shift. Stephan Rabanser, Stephan G\u00fcnnemann, Zachary Lipton, Advances in Neural Information Processing Systems. 32Stephan Rabanser, Stephan G\u00fcnnemann, and Zachary Lipton. Failing loudly: An empirical study of methods for detecting dataset shift. Advances in Neural Information Processing Systems, 32, 2019.\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, G Krueger, I Sutskever, abs/2103.00020Arxiv. Alec Radford, Jong Wook Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision. Arxiv, abs/2103.00020, 2021.\n\nConcept generalization in visual representation learning. Yannis Mert Bulent Sariyildiz, Diane Kalantidis, Karteek Larlus, Alahari, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Mert Bulent Sariyildiz, Yannis Kalantidis, Diane Larlus, and Karteek Alahari. Concept gen- eralization in visual representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 9629-9639, October 2021.\n\nCan't steal? cont-steal! contrastive stealing attacks against image encoders. Zeyang Sha, Xinlei He, Ning Yu, Michael Backes, Yang Zhang, Zeyang Sha, Xinlei He, Ning Yu, Michael Backes, and Yang Zhang. Can't steal? cont-steal! contrastive stealing attacks against image encoders. 2022. URL https://arxiv.org/abs/ 2201.07513.\n\nOn the robustness of backdoor-based watermarking in deep neural networks. Masoumeh Shafieinejad, Nils Lukas, Jiaqi Wang, Xinda Li, Florian Kerschbaum, Proceedings of the 2021 ACM Workshop on Information Hiding and Multimedia Security. the 2021 ACM Workshop on Information Hiding and Multimedia SecurityMasoumeh Shafieinejad, Nils Lukas, Jiaqi Wang, Xinda Li, and Florian Kerschbaum. On the robustness of backdoor-based watermarking in deep neural networks. In Proceedings of the 2021 ACM Workshop on Information Hiding and Multimedia Security, pages 177-188, 2021.\n", "annotations": {"author": "[{\"end\":107,\"start\":48},{\"end\":165,\"start\":108},{\"end\":233,\"start\":166},{\"end\":293,\"start\":234},{\"end\":350,\"start\":294},{\"end\":410,\"start\":351},{\"end\":475,\"start\":411},{\"end\":538,\"start\":476}]", "publisher": null, "author_last_name": "[{\"end\":61,\"start\":53},{\"end\":119,\"start\":115},{\"end\":187,\"start\":181},{\"end\":247,\"start\":241},{\"end\":304,\"start\":300},{\"end\":364,\"start\":358},{\"end\":429,\"start\":421},{\"end\":492,\"start\":484}]", "author_first_name": "[{\"end\":52,\"start\":48},{\"end\":114,\"start\":108},{\"end\":174,\"start\":166},{\"end\":180,\"start\":175},{\"end\":240,\"start\":234},{\"end\":299,\"start\":294},{\"end\":357,\"start\":351},{\"end\":420,\"start\":411},{\"end\":483,\"start\":476}]", "author_affiliation": "[{\"end\":106,\"start\":63},{\"end\":164,\"start\":121},{\"end\":232,\"start\":189},{\"end\":292,\"start\":249},{\"end\":349,\"start\":306},{\"end\":409,\"start\":366},{\"end\":474,\"start\":431},{\"end\":537,\"start\":494}]", "title": "[{\"end\":45,\"start\":1},{\"end\":583,\"start\":539}]", "venue": null, "abstract": "[{\"end\":2037,\"start\":585}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2354,\"start\":2350},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2491,\"start\":2487},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2518,\"start\":2515},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2533,\"start\":2529},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2558,\"start\":2554},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2615,\"start\":2612},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2630,\"start\":2627},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3152,\"start\":3148},{\"end\":3244,\"start\":3240},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3397,\"start\":3393},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3400,\"start\":3397},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3468,\"start\":3464},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3471,\"start\":3468},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3474,\"start\":3471},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3522,\"start\":3518},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3547,\"start\":3543},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3982,\"start\":3979},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4662,\"start\":4658},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4915,\"start\":4911},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5554,\"start\":5550},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6045,\"start\":6041},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6048,\"start\":6045},{\"end\":8665,\"start\":8661},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9104,\"start\":9101},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9280,\"start\":9276},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9283,\"start\":9280},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9440,\"start\":9436},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9541,\"start\":9537},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9623,\"start\":9620},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9644,\"start\":9640},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9986,\"start\":9982},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10682,\"start\":10678},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10724,\"start\":10721},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10727,\"start\":10724},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11057,\"start\":11054},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11060,\"start\":11057},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11063,\"start\":11060},{\"end\":11066,\"start\":11063},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11147,\"start\":11144},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11462,\"start\":11458},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11601,\"start\":11597},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11766,\"start\":11762},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11769,\"start\":11766},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11903,\"start\":11899},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12318,\"start\":12314},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12356,\"start\":12352},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12401,\"start\":12397},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12630,\"start\":12626},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16006,\"start\":16002},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19660,\"start\":19656},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20294,\"start\":20290},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20297,\"start\":20294},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21460,\"start\":21456},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":22366,\"start\":22362},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23582,\"start\":23578},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24587,\"start\":24584},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25008,\"start\":25004},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25322,\"start\":25319},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26265,\"start\":26262},{\"end\":26268,\"start\":26265},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26682,\"start\":26678},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":26693,\"start\":26689},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26704,\"start\":26701},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26723,\"start\":26719},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27575,\"start\":27572},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":27916,\"start\":27912},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":30497,\"start\":30493},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":30873,\"start\":30869},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":32328,\"start\":32324},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32903,\"start\":32899},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":36086,\"start\":36082},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":36089,\"start\":36086},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":36381,\"start\":36377},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":36482,\"start\":36478},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":39808,\"start\":39804},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":41908,\"start\":41904},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":41923,\"start\":41919},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":42063,\"start\":42059},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":42230,\"start\":42226},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":42428,\"start\":42425},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":44387,\"start\":44383},{\"end\":44566,\"start\":44564},{\"end\":50717,\"start\":50713},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":50746,\"start\":50743}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":57867,\"start\":57538},{\"attributes\":{\"id\":\"fig_1\"},\"end\":58666,\"start\":57868},{\"attributes\":{\"id\":\"fig_2\"},\"end\":61105,\"start\":58667},{\"attributes\":{\"id\":\"fig_3\"},\"end\":61519,\"start\":61106},{\"attributes\":{\"id\":\"fig_4\"},\"end\":62578,\"start\":61520},{\"attributes\":{\"id\":\"fig_6\"},\"end\":62682,\"start\":62579},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":63528,\"start\":62683},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":64499,\"start\":63529},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":65609,\"start\":64500},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":66272,\"start\":65610},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":66578,\"start\":66273},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":67218,\"start\":66579},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":67538,\"start\":67219},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":68194,\"start\":67539},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":68839,\"start\":68195},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":69932,\"start\":68840},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":71064,\"start\":69933},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":71497,\"start\":71065},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":71939,\"start\":71498}]", "paragraph": "[{\"end\":2559,\"start\":2053},{\"end\":3523,\"start\":2561},{\"end\":4635,\"start\":3525},{\"end\":5129,\"start\":4637},{\"end\":5626,\"start\":5131},{\"end\":7036,\"start\":5628},{\"end\":7944,\"start\":7038},{\"end\":7984,\"start\":7946},{\"end\":8162,\"start\":7986},{\"end\":8335,\"start\":8164},{\"end\":8522,\"start\":8337},{\"end\":9105,\"start\":8539},{\"end\":9956,\"start\":9107},{\"end\":11353,\"start\":9958},{\"end\":12061,\"start\":11355},{\"end\":12747,\"start\":12063},{\"end\":13740,\"start\":12766},{\"end\":14811,\"start\":13742},{\"end\":15711,\"start\":14828},{\"end\":16704,\"start\":15753},{\"end\":16864,\"start\":16718},{\"end\":17996,\"start\":16866},{\"end\":18099,\"start\":18021},{\"end\":19255,\"start\":18160},{\"end\":21082,\"start\":19285},{\"end\":22418,\"start\":21111},{\"end\":22755,\"start\":22493},{\"end\":22979,\"start\":22798},{\"end\":23083,\"start\":23007},{\"end\":23217,\"start\":23111},{\"end\":24123,\"start\":23245},{\"end\":24799,\"start\":24145},{\"end\":25323,\"start\":24812},{\"end\":25615,\"start\":25325},{\"end\":25852,\"start\":25676},{\"end\":26538,\"start\":25928},{\"end\":27350,\"start\":26563},{\"end\":28135,\"start\":27403},{\"end\":28564,\"start\":28137},{\"end\":28992,\"start\":28566},{\"end\":29092,\"start\":28994},{\"end\":30015,\"start\":29126},{\"end\":30697,\"start\":30017},{\"end\":31723,\"start\":30699},{\"end\":31888,\"start\":31725},{\"end\":32383,\"start\":31986},{\"end\":34042,\"start\":32385},{\"end\":34663,\"start\":34044},{\"end\":35805,\"start\":34762},{\"end\":36628,\"start\":35821},{\"end\":38250,\"start\":36644},{\"end\":38299,\"start\":38252},{\"end\":39126,\"start\":38331},{\"end\":39218,\"start\":39163},{\"end\":39298,\"start\":39220},{\"end\":39561,\"start\":39300},{\"end\":39745,\"start\":39563},{\"end\":40869,\"start\":39794},{\"end\":41713,\"start\":40903},{\"end\":42052,\"start\":41749},{\"end\":42215,\"start\":42054},{\"end\":42417,\"start\":42217},{\"end\":42574,\"start\":42419},{\"end\":42758,\"start\":42621},{\"end\":42865,\"start\":42796},{\"end\":43389,\"start\":42867},{\"end\":44064,\"start\":43391},{\"end\":44457,\"start\":44066},{\"end\":44609,\"start\":44459},{\"end\":44658,\"start\":44611},{\"end\":44724,\"start\":44679},{\"end\":44799,\"start\":44745},{\"end\":44834,\"start\":44806},{\"end\":44971,\"start\":44841},{\"end\":45042,\"start\":44973},{\"end\":45160,\"start\":45049},{\"end\":45204,\"start\":45173},{\"end\":45595,\"start\":45241},{\"end\":45770,\"start\":45597},{\"end\":45890,\"start\":45795},{\"end\":46483,\"start\":45926},{\"end\":46788,\"start\":46557},{\"end\":46821,\"start\":46790},{\"end\":46918,\"start\":46893},{\"end\":46953,\"start\":46925},{\"end\":47026,\"start\":46960},{\"end\":47098,\"start\":47033},{\"end\":47216,\"start\":47105},{\"end\":47287,\"start\":47218},{\"end\":47407,\"start\":47295},{\"end\":47452,\"start\":47420},{\"end\":48508,\"start\":47547},{\"end\":48817,\"start\":48573},{\"end\":49413,\"start\":48819},{\"end\":49858,\"start\":49469},{\"end\":50260,\"start\":49860},{\"end\":50610,\"start\":50294},{\"end\":51755,\"start\":50612},{\"end\":52229,\"start\":51757},{\"end\":53353,\"start\":52254},{\"end\":53770,\"start\":53355},{\"end\":55008,\"start\":53801},{\"end\":55269,\"start\":55179},{\"end\":56291,\"start\":55320},{\"end\":56565,\"start\":56316},{\"end\":56738,\"start\":56611},{\"end\":57537,\"start\":56740}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18159,\"start\":18100},{\"attributes\":{\"id\":\"formula_1\"},\"end\":22492,\"start\":22419},{\"attributes\":{\"id\":\"formula_2\"},\"end\":22797,\"start\":22756},{\"attributes\":{\"id\":\"formula_3\"},\"end\":23006,\"start\":22980},{\"attributes\":{\"id\":\"formula_4\"},\"end\":23110,\"start\":23084},{\"attributes\":{\"id\":\"formula_5\"},\"end\":24144,\"start\":24124},{\"attributes\":{\"id\":\"formula_6\"},\"end\":25675,\"start\":25616},{\"attributes\":{\"id\":\"formula_7\"},\"end\":25927,\"start\":25853},{\"attributes\":{\"id\":\"formula_8\"},\"end\":31985,\"start\":31889},{\"attributes\":{\"id\":\"formula_9\"},\"end\":34761,\"start\":34664},{\"attributes\":{\"id\":\"formula_10\"},\"end\":44678,\"start\":44659},{\"attributes\":{\"id\":\"formula_11\"},\"end\":44744,\"start\":44725},{\"attributes\":{\"id\":\"formula_12\"},\"end\":45172,\"start\":45161},{\"attributes\":{\"id\":\"formula_13\"},\"end\":45240,\"start\":45205},{\"attributes\":{\"id\":\"formula_14\"},\"end\":46892,\"start\":46822},{\"attributes\":{\"id\":\"formula_15\"},\"end\":47419,\"start\":47408},{\"attributes\":{\"id\":\"formula_16\"},\"end\":47489,\"start\":47453},{\"attributes\":{\"id\":\"formula_17\"},\"end\":49432,\"start\":49414},{\"attributes\":{\"id\":\"formula_18\"},\"end\":49468,\"start\":49432},{\"attributes\":{\"id\":\"formula_19\"},\"end\":50293,\"start\":50261},{\"attributes\":{\"id\":\"formula_20\"},\"end\":52253,\"start\":52230},{\"attributes\":{\"id\":\"formula_21\"},\"end\":53800,\"start\":53771},{\"attributes\":{\"id\":\"formula_22\"},\"end\":55178,\"start\":55009},{\"attributes\":{\"id\":\"formula_23\"},\"end\":55319,\"start\":55270}]", "table_ref": "[{\"end\":26733,\"start\":26726},{\"end\":27349,\"start\":27335},{\"end\":30076,\"start\":30069},{\"end\":30732,\"start\":30725},{\"end\":31003,\"start\":30996},{\"end\":31756,\"start\":31749},{\"end\":32407,\"start\":32400},{\"end\":32915,\"start\":32908},{\"end\":33031,\"start\":33024},{\"end\":33732,\"start\":33725},{\"end\":33849,\"start\":33842},{\"end\":34097,\"start\":34090},{\"end\":35073,\"start\":35066},{\"end\":35472,\"start\":35465},{\"end\":42756,\"start\":42749},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":42864,\"start\":42857},{\"end\":43424,\"start\":43410},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":45936,\"start\":45929},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":46468,\"start\":46454},{\"end\":48328,\"start\":48320},{\"end\":48894,\"start\":48887},{\"end\":49234,\"start\":49227},{\"end\":51219,\"start\":51212},{\"end\":51562,\"start\":51555},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":52228,\"start\":52220},{\"end\":52755,\"start\":52747},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":53352,\"start\":53344},{\"end\":54750,\"start\":54742},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":56622,\"start\":56614},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":56751,\"start\":56743}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2051,\"start\":2039},{\"attributes\":{\"n\":\"2\"},\"end\":8537,\"start\":8525},{\"attributes\":{\"n\":\"3\"},\"end\":12764,\"start\":12750},{\"attributes\":{\"n\":\"3.1\"},\"end\":14826,\"start\":14814},{\"attributes\":{\"n\":\"3.2\"},\"end\":15751,\"start\":15714},{\"attributes\":{\"n\":\"3.3\"},\"end\":16716,\"start\":16707},{\"attributes\":{\"n\":\"3.4\"},\"end\":18019,\"start\":17999},{\"attributes\":{\"n\":\"4\"},\"end\":19283,\"start\":19258},{\"attributes\":{\"n\":\"4.1\"},\"end\":21109,\"start\":21085},{\"attributes\":{\"n\":\"4.2\"},\"end\":23243,\"start\":23220},{\"attributes\":{\"n\":\"4.3\"},\"end\":24810,\"start\":24802},{\"attributes\":{\"n\":\"5\"},\"end\":26561,\"start\":26541},{\"attributes\":{\"n\":\"5.1\"},\"end\":27401,\"start\":27353},{\"attributes\":{\"n\":\"5.2\"},\"end\":29124,\"start\":29095},{\"attributes\":{\"n\":\"5.4\"},\"end\":35819,\"start\":35808},{\"attributes\":{\"n\":\"6\"},\"end\":36642,\"start\":36631},{\"end\":38329,\"start\":38302},{\"end\":39161,\"start\":39129},{\"end\":39792,\"start\":39748},{\"end\":40901,\"start\":40872},{\"end\":41747,\"start\":41716},{\"end\":42619,\"start\":42577},{\"end\":42794,\"start\":42761},{\"end\":44804,\"start\":44802},{\"end\":44839,\"start\":44837},{\"end\":45047,\"start\":45045},{\"end\":45793,\"start\":45773},{\"end\":45924,\"start\":45893},{\"end\":46555,\"start\":46486},{\"end\":46923,\"start\":46921},{\"end\":46958,\"start\":46956},{\"end\":47031,\"start\":47029},{\"end\":47103,\"start\":47101},{\"end\":47293,\"start\":47290},{\"end\":47545,\"start\":47491},{\"end\":48571,\"start\":48511},{\"end\":56314,\"start\":56294},{\"end\":56609,\"start\":56568},{\"end\":57542,\"start\":57539},{\"end\":57879,\"start\":57869},{\"end\":58681,\"start\":58668},{\"end\":62590,\"start\":62580},{\"end\":64510,\"start\":64501},{\"end\":65620,\"start\":65611},{\"end\":66283,\"start\":66274},{\"end\":66589,\"start\":66580},{\"end\":67550,\"start\":67540},{\"end\":68206,\"start\":68196},{\"end\":68851,\"start\":68841},{\"end\":69944,\"start\":69934},{\"end\":71076,\"start\":71066},{\"end\":71509,\"start\":71499}]", "table": "[{\"end\":63528,\"start\":62774},{\"end\":64499,\"start\":63807},{\"end\":65609,\"start\":64699},{\"end\":66272,\"start\":65740},{\"end\":66578,\"start\":66402},{\"end\":67218,\"start\":66794},{\"end\":67538,\"start\":67305},{\"end\":68194,\"start\":67769},{\"end\":68839,\"start\":68479},{\"end\":69932,\"start\":69053},{\"end\":71064,\"start\":70185},{\"end\":71497,\"start\":71386},{\"end\":71939,\"start\":71815}]", "figure_caption": "[{\"end\":57867,\"start\":57544},{\"end\":58666,\"start\":57881},{\"end\":61105,\"start\":58683},{\"end\":61519,\"start\":61108},{\"end\":62578,\"start\":61522},{\"end\":62682,\"start\":62592},{\"end\":62774,\"start\":62685},{\"end\":63807,\"start\":63531},{\"end\":64699,\"start\":64512},{\"end\":65740,\"start\":65622},{\"end\":66402,\"start\":66285},{\"end\":66794,\"start\":66591},{\"end\":67305,\"start\":67221},{\"end\":67769,\"start\":67553},{\"end\":68479,\"start\":68209},{\"end\":69053,\"start\":68854},{\"end\":70185,\"start\":69947},{\"end\":71386,\"start\":71079},{\"end\":71815,\"start\":71512}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":4084,\"start\":4076},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":5185,\"start\":5177},{\"end\":13341,\"start\":13333},{\"end\":13507,\"start\":13499},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14528,\"start\":14520},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":14852,\"start\":14844},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":16850,\"start\":16842},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33506,\"start\":33498},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":51867,\"start\":51859},{\"end\":52992,\"start\":52984},{\"end\":53746,\"start\":53737},{\"end\":53889,\"start\":53880},{\"end\":54388,\"start\":54380},{\"end\":55268,\"start\":55259},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":56372,\"start\":56364},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":56472,\"start\":56464},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":56563,\"start\":56555}]", "bib_author_first_name": "[{\"end\":72763,\"start\":72757},{\"end\":72778,\"start\":72773},{\"end\":72793,\"start\":72786},{\"end\":73051,\"start\":73042},{\"end\":73335,\"start\":73334},{\"end\":73348,\"start\":73347},{\"end\":73361,\"start\":73360},{\"end\":73657,\"start\":73651},{\"end\":73671,\"start\":73664},{\"end\":73857,\"start\":73851},{\"end\":73871,\"start\":73864},{\"end\":73883,\"start\":73878},{\"end\":73898,\"start\":73892},{\"end\":73910,\"start\":73905},{\"end\":73918,\"start\":73916},{\"end\":73927,\"start\":73923},{\"end\":74449,\"start\":74445},{\"end\":74464,\"start\":74458},{\"end\":74476,\"start\":74469},{\"end\":74811,\"start\":74803},{\"end\":74824,\"start\":74818},{\"end\":74833,\"start\":74829},{\"end\":75068,\"start\":75067},{\"end\":75080,\"start\":75077},{\"end\":75082,\"start\":75081},{\"end\":75415,\"start\":75412},{\"end\":75425,\"start\":75422},{\"end\":75439,\"start\":75432},{\"end\":75454,\"start\":75448},{\"end\":75462,\"start\":75459},{\"end\":75469,\"start\":75467},{\"end\":75774,\"start\":75769},{\"end\":75791,\"start\":75783},{\"end\":75805,\"start\":75799},{\"end\":75819,\"start\":75811},{\"end\":75829,\"start\":75820},{\"end\":76202,\"start\":76195},{\"end\":76223,\"start\":76209},{\"end\":76237,\"start\":76232},{\"end\":76756,\"start\":76752},{\"end\":76773,\"start\":76767},{\"end\":76790,\"start\":76782},{\"end\":76796,\"start\":76791},{\"end\":76810,\"start\":76805},{\"end\":76824,\"start\":76817},{\"end\":77183,\"start\":77179},{\"end\":77202,\"start\":77194},{\"end\":77208,\"start\":77203},{\"end\":77219,\"start\":77217},{\"end\":77224,\"start\":77220},{\"end\":77236,\"start\":77229},{\"end\":77564,\"start\":77561},{\"end\":77583,\"start\":77577},{\"end\":77597,\"start\":77592},{\"end\":77826,\"start\":77821},{\"end\":77849,\"start\":77843},{\"end\":77861,\"start\":77855},{\"end\":78209,\"start\":78203},{\"end\":78218,\"start\":78214},{\"end\":78910,\"start\":78903},{\"end\":78923,\"start\":78922},{\"end\":78934,\"start\":78933},{\"end\":78947,\"start\":78946},{\"end\":78958,\"start\":78957},{\"end\":79223,\"start\":79217},{\"end\":79239,\"start\":79233},{\"end\":79246,\"start\":79240},{\"end\":79261,\"start\":79253},{\"end\":79266,\"start\":79262},{\"end\":79283,\"start\":79274},{\"end\":79300,\"start\":79294},{\"end\":79682,\"start\":79681},{\"end\":79684,\"start\":79683},{\"end\":79699,\"start\":79698},{\"end\":79717,\"start\":79716},{\"end\":79943,\"start\":79936},{\"end\":79957,\"start\":79949},{\"end\":79968,\"start\":79967},{\"end\":79989,\"start\":79982},{\"end\":80429,\"start\":80425},{\"end\":80446,\"start\":80437},{\"end\":80462,\"start\":80456},{\"end\":80473,\"start\":80472},{\"end\":80858,\"start\":80852},{\"end\":80871,\"start\":80867},{\"end\":80886,\"start\":80881},{\"end\":81141,\"start\":81136},{\"end\":81161,\"start\":81153},{\"end\":81178,\"start\":81171},{\"end\":81192,\"start\":81184},{\"end\":81452,\"start\":81446},{\"end\":81467,\"start\":81460},{\"end\":81483,\"start\":81477},{\"end\":81498,\"start\":81493},{\"end\":81514,\"start\":81507},{\"end\":81995,\"start\":81994},{\"end\":81997,\"start\":81996},{\"end\":82011,\"start\":82010},{\"end\":82013,\"start\":82012},{\"end\":82312,\"start\":82308},{\"end\":82537,\"start\":82530},{\"end\":82549,\"start\":82543},{\"end\":82564,\"start\":82556},{\"end\":82579,\"start\":82570},{\"end\":82591,\"start\":82585},{\"end\":82601,\"start\":82597},{\"end\":82615,\"start\":82609},{\"end\":83144,\"start\":83137},{\"end\":83157,\"start\":83150},{\"end\":83169,\"start\":83163},{\"end\":83188,\"start\":83174},{\"end\":83829,\"start\":83821},{\"end\":83845,\"start\":83837},{\"end\":83862,\"start\":83855},{\"end\":84304,\"start\":84299},{\"end\":84321,\"start\":84317},{\"end\":85048,\"start\":85045},{\"end\":85050,\"start\":85049},{\"end\":85066,\"start\":85059},{\"end\":85078,\"start\":85074},{\"end\":85302,\"start\":85297},{\"end\":85314,\"start\":85311},{\"end\":85325,\"start\":85321},{\"end\":85344,\"start\":85334},{\"end\":85357,\"start\":85355},{\"end\":85368,\"start\":85362},{\"end\":85370,\"start\":85369},{\"end\":85832,\"start\":85820},{\"end\":85848,\"start\":85843},{\"end\":85863,\"start\":85858},{\"end\":86249,\"start\":86242},{\"end\":86267,\"start\":86260},{\"end\":86286,\"start\":86279},{\"end\":86617,\"start\":86613},{\"end\":86631,\"start\":86627},{\"end\":86636,\"start\":86632},{\"end\":86643,\"start\":86642},{\"end\":86654,\"start\":86653},{\"end\":86664,\"start\":86663},{\"end\":86671,\"start\":86670},{\"end\":86682,\"start\":86681},{\"end\":86692,\"start\":86691},{\"end\":86702,\"start\":86701},{\"end\":86713,\"start\":86712},{\"end\":86722,\"start\":86721},{\"end\":86733,\"start\":86732},{\"end\":87077,\"start\":87071},{\"end\":87107,\"start\":87102},{\"end\":87127,\"start\":87120},{\"end\":87625,\"start\":87619},{\"end\":87637,\"start\":87631},{\"end\":87646,\"start\":87642},{\"end\":87658,\"start\":87651},{\"end\":87671,\"start\":87667},{\"end\":87949,\"start\":87941},{\"end\":87968,\"start\":87964},{\"end\":87981,\"start\":87976},{\"end\":87993,\"start\":87988},{\"end\":88005,\"start\":87998}]", "bib_author_last_name": "[{\"end\":72567,\"start\":72561},{\"end\":72629,\"start\":72623},{\"end\":72771,\"start\":72764},{\"end\":72784,\"start\":72779},{\"end\":72804,\"start\":72794},{\"end\":73060,\"start\":73052},{\"end\":73345,\"start\":73336},{\"end\":73358,\"start\":73349},{\"end\":73369,\"start\":73362},{\"end\":73377,\"start\":73371},{\"end\":73662,\"start\":73658},{\"end\":73674,\"start\":73672},{\"end\":73862,\"start\":73858},{\"end\":73876,\"start\":73872},{\"end\":73890,\"start\":73884},{\"end\":73903,\"start\":73899},{\"end\":73914,\"start\":73911},{\"end\":73921,\"start\":73919},{\"end\":73932,\"start\":73928},{\"end\":74456,\"start\":74450},{\"end\":74467,\"start\":74465},{\"end\":74480,\"start\":74477},{\"end\":74816,\"start\":74812},{\"end\":74827,\"start\":74825},{\"end\":74839,\"start\":74834},{\"end\":75075,\"start\":75069},{\"end\":75088,\"start\":75083},{\"end\":75096,\"start\":75090},{\"end\":75420,\"start\":75416},{\"end\":75430,\"start\":75426},{\"end\":75446,\"start\":75440},{\"end\":75457,\"start\":75455},{\"end\":75465,\"start\":75463},{\"end\":75477,\"start\":75470},{\"end\":75781,\"start\":75775},{\"end\":75797,\"start\":75792},{\"end\":75809,\"start\":75806},{\"end\":75834,\"start\":75830},{\"end\":76207,\"start\":76203},{\"end\":76230,\"start\":76224},{\"end\":76248,\"start\":76238},{\"end\":76765,\"start\":76757},{\"end\":76780,\"start\":76774},{\"end\":76803,\"start\":76797},{\"end\":76815,\"start\":76811},{\"end\":76833,\"start\":76825},{\"end\":77192,\"start\":77184},{\"end\":77215,\"start\":77209},{\"end\":77227,\"start\":77225},{\"end\":77245,\"start\":77237},{\"end\":77575,\"start\":77565},{\"end\":77590,\"start\":77584},{\"end\":77607,\"start\":77598},{\"end\":77841,\"start\":77827},{\"end\":77853,\"start\":77850},{\"end\":77868,\"start\":77862},{\"end\":77872,\"start\":77870},{\"end\":78212,\"start\":78210},{\"end\":78224,\"start\":78219},{\"end\":78920,\"start\":78911},{\"end\":78931,\"start\":78924},{\"end\":78944,\"start\":78935},{\"end\":78955,\"start\":78948},{\"end\":78967,\"start\":78959},{\"end\":79231,\"start\":79224},{\"end\":79251,\"start\":79247},{\"end\":79272,\"start\":79267},{\"end\":79292,\"start\":79284},{\"end\":79308,\"start\":79301},{\"end\":79696,\"start\":79685},{\"end\":79714,\"start\":79700},{\"end\":79732,\"start\":79718},{\"end\":79742,\"start\":79734},{\"end\":79947,\"start\":79944},{\"end\":79965,\"start\":79958},{\"end\":79980,\"start\":79969},{\"end\":80004,\"start\":79990},{\"end\":80014,\"start\":80006},{\"end\":80435,\"start\":80430},{\"end\":80454,\"start\":80447},{\"end\":80470,\"start\":80463},{\"end\":80480,\"start\":80474},{\"end\":80865,\"start\":80859},{\"end\":80879,\"start\":80872},{\"end\":80891,\"start\":80887},{\"end\":81151,\"start\":81142},{\"end\":81169,\"start\":81162},{\"end\":81182,\"start\":81179},{\"end\":81199,\"start\":81193},{\"end\":81458,\"start\":81453},{\"end\":81475,\"start\":81468},{\"end\":81491,\"start\":81484},{\"end\":81505,\"start\":81499},{\"end\":81523,\"start\":81515},{\"end\":82008,\"start\":81998},{\"end\":82022,\"start\":82014},{\"end\":82323,\"start\":82313},{\"end\":82541,\"start\":82538},{\"end\":82554,\"start\":82550},{\"end\":82568,\"start\":82565},{\"end\":82583,\"start\":82580},{\"end\":82595,\"start\":82592},{\"end\":82607,\"start\":82602},{\"end\":82620,\"start\":82616},{\"end\":83148,\"start\":83145},{\"end\":83161,\"start\":83158},{\"end\":83172,\"start\":83170},{\"end\":83193,\"start\":83189},{\"end\":83835,\"start\":83830},{\"end\":83853,\"start\":83846},{\"end\":83871,\"start\":83863},{\"end\":84315,\"start\":84305},{\"end\":84329,\"start\":84322},{\"end\":85057,\"start\":85051},{\"end\":85072,\"start\":85067},{\"end\":85085,\"start\":85079},{\"end\":85309,\"start\":85303},{\"end\":85319,\"start\":85315},{\"end\":85332,\"start\":85326},{\"end\":85353,\"start\":85345},{\"end\":85360,\"start\":85358},{\"end\":85373,\"start\":85371},{\"end\":85841,\"start\":85833},{\"end\":85856,\"start\":85849},{\"end\":85869,\"start\":85864},{\"end\":86258,\"start\":86250},{\"end\":86277,\"start\":86268},{\"end\":86293,\"start\":86287},{\"end\":86625,\"start\":86618},{\"end\":86640,\"start\":86637},{\"end\":86651,\"start\":86644},{\"end\":86661,\"start\":86655},{\"end\":86668,\"start\":86665},{\"end\":86679,\"start\":86672},{\"end\":86689,\"start\":86683},{\"end\":86699,\"start\":86693},{\"end\":86710,\"start\":86703},{\"end\":86719,\"start\":86714},{\"end\":86730,\"start\":86723},{\"end\":86743,\"start\":86734},{\"end\":87100,\"start\":87078},{\"end\":87118,\"start\":87108},{\"end\":87134,\"start\":87128},{\"end\":87143,\"start\":87136},{\"end\":87629,\"start\":87626},{\"end\":87640,\"start\":87638},{\"end\":87649,\"start\":87647},{\"end\":87665,\"start\":87659},{\"end\":87677,\"start\":87672},{\"end\":87962,\"start\":87950},{\"end\":87974,\"start\":87969},{\"end\":87986,\"start\":87982},{\"end\":87996,\"start\":87994},{\"end\":88016,\"start\":88006}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":72619,\"start\":72559},{\"attributes\":{\"id\":\"b1\"},\"end\":72683,\"start\":72621},{\"attributes\":{\"id\":\"b2\"},\"end\":72977,\"start\":72685},{\"attributes\":{\"doi\":\"10.3389/fdata.2021.729663\",\"id\":\"b3\",\"matched_paper_id\":244826151},\"end\":73261,\"start\":72979},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":211096730},\"end\":73599,\"start\":73263},{\"attributes\":{\"id\":\"b5\"},\"end\":73759,\"start\":73601},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":208139043},\"end\":74372,\"start\":73761},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":308212},\"end\":74691,\"start\":74374},{\"attributes\":{\"id\":\"b8\"},\"end\":75033,\"start\":74693},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":190432},\"end\":75358,\"start\":75035},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":57246310},\"end\":75767,\"start\":75360},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b11\"},\"end\":76128,\"start\":75769},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":248498316},\"end\":76668,\"start\":76130},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":248834354},\"end\":77106,\"start\":76670},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":246240998},\"end\":77544,\"start\":77108},{\"attributes\":{\"id\":\"b15\"},\"end\":77734,\"start\":77546},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":235367888},\"end\":78135,\"start\":77736},{\"attributes\":{\"doi\":\"10.1145/3460120.3484571\",\"id\":\"b17\",\"matched_paper_id\":231846491},\"end\":78838,\"start\":78137},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":211858541},\"end\":79165,\"start\":78840},{\"attributes\":{\"doi\":\"10.3390/technologies9010002\",\"id\":\"b19\"},\"end\":79619,\"start\":79167},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":211532649},\"end\":79934,\"start\":79621},{\"attributes\":{\"doi\":\"arXiv:2103.05633\",\"id\":\"b21\"},\"end\":80369,\"start\":79936},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":21670607},\"end\":80758,\"start\":80371},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":843188},\"end\":81078,\"start\":80760},{\"attributes\":{\"id\":\"b24\"},\"end\":81369,\"start\":81080},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":237213752},\"end\":81941,\"start\":81371},{\"attributes\":{\"id\":\"b26\"},\"end\":82251,\"start\":81943},{\"attributes\":{\"id\":\"b27\"},\"end\":82436,\"start\":82253},{\"attributes\":{\"doi\":\"10.1093/bioinformatics/btz682\",\"id\":\"b28\",\"matched_paper_id\":59291975},\"end\":83049,\"start\":82438},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":237290083},\"end\":83402,\"start\":83051},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":889967},\"end\":83758,\"start\":83404},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":231609191},\"end\":84236,\"start\":83760},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":53278187},\"end\":84956,\"start\":84238},{\"attributes\":{\"id\":\"b33\"},\"end\":85226,\"start\":84958},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":16852518},\"end\":85743,\"start\":85228},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":209314546},\"end\":86165,\"start\":85745},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":53096511},\"end\":86540,\"start\":86167},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":231591445},\"end\":87011,\"start\":86542},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":228083964},\"end\":87539,\"start\":87013},{\"attributes\":{\"id\":\"b39\"},\"end\":87865,\"start\":87541},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":195068886},\"end\":88431,\"start\":87867}]", "bib_title": "[{\"end\":73040,\"start\":72979},{\"end\":73332,\"start\":73263},{\"end\":73849,\"start\":73761},{\"end\":74443,\"start\":74374},{\"end\":75065,\"start\":75035},{\"end\":75410,\"start\":75360},{\"end\":76193,\"start\":76130},{\"end\":76750,\"start\":76670},{\"end\":77177,\"start\":77108},{\"end\":77819,\"start\":77736},{\"end\":78201,\"start\":78137},{\"end\":78901,\"start\":78840},{\"end\":79679,\"start\":79621},{\"end\":80423,\"start\":80371},{\"end\":80850,\"start\":80760},{\"end\":81444,\"start\":81371},{\"end\":81992,\"start\":81943},{\"end\":82528,\"start\":82438},{\"end\":83135,\"start\":83051},{\"end\":83439,\"start\":83404},{\"end\":83819,\"start\":83760},{\"end\":84297,\"start\":84238},{\"end\":85295,\"start\":85228},{\"end\":85818,\"start\":85745},{\"end\":86240,\"start\":86167},{\"end\":86611,\"start\":86542},{\"end\":87069,\"start\":87013},{\"end\":87939,\"start\":87867}]", "bib_author": "[{\"end\":72569,\"start\":72561},{\"end\":72631,\"start\":72623},{\"end\":72773,\"start\":72757},{\"end\":72786,\"start\":72773},{\"end\":72806,\"start\":72786},{\"end\":73062,\"start\":73042},{\"end\":73347,\"start\":73334},{\"end\":73360,\"start\":73347},{\"end\":73371,\"start\":73360},{\"end\":73379,\"start\":73371},{\"end\":73664,\"start\":73651},{\"end\":73676,\"start\":73664},{\"end\":73864,\"start\":73851},{\"end\":73878,\"start\":73864},{\"end\":73892,\"start\":73878},{\"end\":73905,\"start\":73892},{\"end\":73916,\"start\":73905},{\"end\":73923,\"start\":73916},{\"end\":73934,\"start\":73923},{\"end\":74458,\"start\":74445},{\"end\":74469,\"start\":74458},{\"end\":74482,\"start\":74469},{\"end\":74818,\"start\":74803},{\"end\":74829,\"start\":74818},{\"end\":74841,\"start\":74829},{\"end\":75077,\"start\":75067},{\"end\":75090,\"start\":75077},{\"end\":75098,\"start\":75090},{\"end\":75422,\"start\":75412},{\"end\":75432,\"start\":75422},{\"end\":75448,\"start\":75432},{\"end\":75459,\"start\":75448},{\"end\":75467,\"start\":75459},{\"end\":75479,\"start\":75467},{\"end\":75783,\"start\":75769},{\"end\":75799,\"start\":75783},{\"end\":75811,\"start\":75799},{\"end\":75836,\"start\":75811},{\"end\":76209,\"start\":76195},{\"end\":76232,\"start\":76209},{\"end\":76250,\"start\":76232},{\"end\":76767,\"start\":76752},{\"end\":76782,\"start\":76767},{\"end\":76805,\"start\":76782},{\"end\":76817,\"start\":76805},{\"end\":76835,\"start\":76817},{\"end\":77194,\"start\":77179},{\"end\":77217,\"start\":77194},{\"end\":77229,\"start\":77217},{\"end\":77247,\"start\":77229},{\"end\":77577,\"start\":77561},{\"end\":77592,\"start\":77577},{\"end\":77609,\"start\":77592},{\"end\":77843,\"start\":77821},{\"end\":77855,\"start\":77843},{\"end\":77870,\"start\":77855},{\"end\":77874,\"start\":77870},{\"end\":78214,\"start\":78203},{\"end\":78226,\"start\":78214},{\"end\":78922,\"start\":78903},{\"end\":78933,\"start\":78922},{\"end\":78946,\"start\":78933},{\"end\":78957,\"start\":78946},{\"end\":78969,\"start\":78957},{\"end\":79233,\"start\":79217},{\"end\":79253,\"start\":79233},{\"end\":79274,\"start\":79253},{\"end\":79294,\"start\":79274},{\"end\":79310,\"start\":79294},{\"end\":79698,\"start\":79681},{\"end\":79716,\"start\":79698},{\"end\":79734,\"start\":79716},{\"end\":79744,\"start\":79734},{\"end\":79949,\"start\":79936},{\"end\":79967,\"start\":79949},{\"end\":79982,\"start\":79967},{\"end\":80006,\"start\":79982},{\"end\":80016,\"start\":80006},{\"end\":80437,\"start\":80425},{\"end\":80456,\"start\":80437},{\"end\":80472,\"start\":80456},{\"end\":80482,\"start\":80472},{\"end\":80867,\"start\":80852},{\"end\":80881,\"start\":80867},{\"end\":80893,\"start\":80881},{\"end\":81153,\"start\":81136},{\"end\":81171,\"start\":81153},{\"end\":81184,\"start\":81171},{\"end\":81201,\"start\":81184},{\"end\":81460,\"start\":81446},{\"end\":81477,\"start\":81460},{\"end\":81493,\"start\":81477},{\"end\":81507,\"start\":81493},{\"end\":81525,\"start\":81507},{\"end\":82010,\"start\":81994},{\"end\":82024,\"start\":82010},{\"end\":82325,\"start\":82308},{\"end\":82543,\"start\":82530},{\"end\":82556,\"start\":82543},{\"end\":82570,\"start\":82556},{\"end\":82585,\"start\":82570},{\"end\":82597,\"start\":82585},{\"end\":82609,\"start\":82597},{\"end\":82622,\"start\":82609},{\"end\":83150,\"start\":83137},{\"end\":83163,\"start\":83150},{\"end\":83174,\"start\":83163},{\"end\":83195,\"start\":83174},{\"end\":83837,\"start\":83821},{\"end\":83855,\"start\":83837},{\"end\":83873,\"start\":83855},{\"end\":84317,\"start\":84299},{\"end\":84331,\"start\":84317},{\"end\":85059,\"start\":85045},{\"end\":85074,\"start\":85059},{\"end\":85087,\"start\":85074},{\"end\":85311,\"start\":85297},{\"end\":85321,\"start\":85311},{\"end\":85334,\"start\":85321},{\"end\":85355,\"start\":85334},{\"end\":85362,\"start\":85355},{\"end\":85375,\"start\":85362},{\"end\":85843,\"start\":85820},{\"end\":85858,\"start\":85843},{\"end\":85871,\"start\":85858},{\"end\":86260,\"start\":86242},{\"end\":86279,\"start\":86260},{\"end\":86295,\"start\":86279},{\"end\":86627,\"start\":86613},{\"end\":86642,\"start\":86627},{\"end\":86653,\"start\":86642},{\"end\":86663,\"start\":86653},{\"end\":86670,\"start\":86663},{\"end\":86681,\"start\":86670},{\"end\":86691,\"start\":86681},{\"end\":86701,\"start\":86691},{\"end\":86712,\"start\":86701},{\"end\":86721,\"start\":86712},{\"end\":86732,\"start\":86721},{\"end\":86745,\"start\":86732},{\"end\":87102,\"start\":87071},{\"end\":87120,\"start\":87102},{\"end\":87136,\"start\":87120},{\"end\":87145,\"start\":87136},{\"end\":87631,\"start\":87619},{\"end\":87642,\"start\":87631},{\"end\":87651,\"start\":87642},{\"end\":87667,\"start\":87651},{\"end\":87679,\"start\":87667},{\"end\":87964,\"start\":87941},{\"end\":87976,\"start\":87964},{\"end\":87988,\"start\":87976},{\"end\":87998,\"start\":87988},{\"end\":88018,\"start\":87998}]", "bib_venue": "[{\"end\":74087,\"start\":74019},{\"end\":75163,\"start\":75140},{\"end\":78441,\"start\":78345},{\"end\":81668,\"start\":81605},{\"end\":83228,\"start\":83220},{\"end\":83559,\"start\":83536},{\"end\":84026,\"start\":83958},{\"end\":84553,\"start\":84470},{\"end\":87288,\"start\":87225},{\"end\":88169,\"start\":88102},{\"end\":72755,\"start\":72685},{\"end\":73108,\"start\":73087},{\"end\":73423,\"start\":73379},{\"end\":73649,\"start\":73601},{\"end\":74017,\"start\":73934},{\"end\":74489,\"start\":74482},{\"end\":74801,\"start\":74693},{\"end\":75138,\"start\":75098},{\"end\":75542,\"start\":75479},{\"end\":75926,\"start\":75852},{\"end\":76299,\"start\":76250},{\"end\":76879,\"start\":76835},{\"end\":77299,\"start\":77247},{\"end\":77559,\"start\":77546},{\"end\":77923,\"start\":77874},{\"end\":78343,\"start\":78249},{\"end\":78994,\"start\":78969},{\"end\":79215,\"start\":79167},{\"end\":79769,\"start\":79744},{\"end\":80133,\"start\":80032},{\"end\":80544,\"start\":80482},{\"end\":80898,\"start\":80893},{\"end\":81134,\"start\":81080},{\"end\":81603,\"start\":81525},{\"end\":82044,\"start\":82024},{\"end\":82306,\"start\":82253},{\"end\":82711,\"start\":82697},{\"end\":83218,\"start\":83195},{\"end\":83534,\"start\":83464},{\"end\":83956,\"start\":83873},{\"end\":84433,\"start\":84335},{\"end\":85043,\"start\":84958},{\"end\":85439,\"start\":85375},{\"end\":85923,\"start\":85871},{\"end\":86344,\"start\":86295},{\"end\":86764,\"start\":86759},{\"end\":87223,\"start\":87145},{\"end\":87617,\"start\":87541},{\"end\":88100,\"start\":88018}]"}}}, "year": 2023, "month": 12, "day": 17}