{"id": 247037290, "updated": "2022-10-31 16:19:48.712", "metadata": {"title": "XCelHD: An Efficient GPU-Powered Hyperdimensional Computing with Parallelized Training", "authors": "[{\"first\":\"Jaeyoung\",\"last\":\"Kang\",\"middle\":[]},{\"first\":\"Behnam\",\"last\":\"Khaleghi\",\"middle\":[]},{\"first\":\"Yeseong\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Tajana\",\"last\":\"Rosing\",\"middle\":[]}]", "venue": "2022 27th Asia and South Pacific Design Automation Conference (ASP-DAC)", "journal": "2022 27th Asia and South Pacific Design Automation Conference (ASP-DAC)", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Hyperdimensional Computing (HDC) is an emerging lightweight machine learning method alternative to deep learning. One of its key strengths is the ability to accelerate it in hardware, as it offers massive parallelisms. Prior work primarily focused on FPGA and ASIC, which do not provide the seamless flexibility required for HDC applications. Few studies that attempted GPU designs are inefficient, partly due to the complexity of accelerating HDC on GPUs because of the bit-level operations of HDC. Besides, HDC training exhibited low hardware utilization due to sequential operations. In this paper, we present XCelHD, a high-performance GPU-powered framework for HDC. XCelHD uses a novel training method to maximize the training speed of the HDC model while fully utilizing hardware. We propose memory optimization strategies specialized for GPU-based HDC, minimizing the access time to different memory subsystems and redundant operations. We show that the proposed training method reduces the required number of training epochs by four-fold to achieve comparable accuracy. Our evaluation results on NVIDIA Jetson TX2 show that XCelHD is up to $35\\times$ faster than the state-of-the-art TensorFlow-based HDC implementation.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aspdac/0001KKR22", "doi": "10.1109/asp-dac52403.2022.9712549"}}, "content": {"source": {"pdf_hash": "c9bfa14184e1d771afa8710d066e5a64e467de15", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "58ef7bc572a498192401add619582867eb2e5b50", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c9bfa14184e1d771afa8710d066e5a64e467de15.txt", "contents": "\nXCelHD: An Efficient GPU-Powered Hyperdimensional Computing with Parallelized Training\n\n\nJaeyoung Kang j5kang@ucsd.edu \nBehnam Khaleghi bkhaleghi@ucsd.edu \nYeseong Kim yeseongkim@dgist.ac.kr \nTajana Rosing tajana@ucsd.edu \n\nDepartment of Electrical and Computer Engineering\nDepartment of Computer Science and Engineering\nUniversity of California\nSan Diego La JollaCAUSA\n\n\nDepartment of Information and Communication Engineering DGIST Daegu\nDepartment of Computer Science and Engineering\nUniversity of California\nSan Diego La JollaCAUSA, Republic of Korea\n\n\nUniversity of California\nSan Diego La JollaCAUSA\n\nXCelHD: An Efficient GPU-Powered Hyperdimensional Computing with Parallelized Training\n10.1109/ASP-DAC52403.2022.9712549\nHyperdimensional Computing (HDC) is an emerging lightweight machine learning method alternative to deep learning. One of its key strengths is the ability to accelerate it in hardware, as it offers massive parallelisms. Prior work primarily focused on FPGA and ASIC, which do not provide the seamless flexibility required for HDC applications. Few studies that attempted GPU designs are inefficient, partly due to the complexity of accelerating HDC on GPUs because of the bit-level operations of HDC. Besides, HDC training exhibited low hardware utilization due to sequential operations. In this paper, we present XCelHD, a high-performance GPU-powered framework for HDC. XCelHD uses a novel training method to maximize the training speed of the HDC model while fully utilizing hardware. We propose memory optimization strategies specialized for GPU-based HDC, minimizing the access time to different memory subsystems and redundant operations. We show that the proposed training method reduces the required number of training epochs by four-fold to achieve comparable accuracy. Our evaluation results on NVIDIA Jetson TX2 show that XCelHD is up to 35\u00d7 faster than the state-of-the-art TensorFlow-based HDC implementation.\n\nI. INTRODUCTION\n\nMachine learning on edge devices has become more prevalent as it enables real-time and in-place analysis. Techniques based on deep learning (DL) are popular for processing complex tasks by extracting high-level features. However, running such algorithms on conventional systems results in high energy consumption and slow processing speed. Braininspired Hyperdimensional Computing (HDC) is an alternative solution based on a long-term memory model, which emulates human cognition with operations on high-dimensional vectors, called hypervectors [1]. HDC first encodes data into hypervectors to perform the rest of the training process in a lightweight way. Many different learning applications have been shown to provide high accuracy and efficiency when implemented using HDC, e.g., language recognition [2], robotics [3], activity identification and voice recognition [4], recommendation system [5] and multimodal sensor fusion [6]. Several companies are also using HDC to implement more generalizeable learning, e.g., Google [7], IBM [8], and Numenta [9].\n\nHDC has a number of advantages as compared to the conventional DL: (1) it is suitable for on-device learning based on hardware acceleration due to its highly parallel nature [10], (2) hidden features of information can be wellexposed, thereby empowering both training and inference with the light-weight computation and a small number of iterations, and (3) the hypervector representation inherently exhibits strong robustness against the noise and corrupted data [11]. A few hardware accelerators have been proposed for HDC, such as ASIC [2], [12], [13], and in-memory computing accelerator [14]. These works show nearly remarkable speedup and energy efficiency improvements compared to the CPUbased implementation, primarily due to their ability to leverage high parallelism of operations on hypervectors. However, ASIC and PIM have long design times and are expensive to manufacture, so they are not commonly available. More importantly, HDC applications require flexibility in terms of, e.g., hypervector lengths, encoding algorithm, and precision of operations that such platforms cannot readily offer. An excellent commercial off-the-shelf platform is GPU. Recently there has been a proliferation of GPU-enabled embedded devices, such as NVIDIA Jetson.\n\nUnfortunately, recent work that accelerated HDC on embedded GPU [15] showed a comparable performance over CPUbased HDC. Our recent experiments with TensorFlow XLAbased HDC also gained a marginal performance improvement (1.74\u00d7) over the CPU, showing the GPU utilization of 40% and 10% for the encoding and the training, respectively. There are several reasons why a library such as TensorFlow is not suitable for running HDC on the GPU. First, HDC training by default requires sequential data inputs to refine the trained model iteratively for higher accuracy. It creates a lot of HDC data transfer between CPU and GPU, which causes a memory bottleneck and impedes full exploitation of GPU parallelism. Furthermore, TensorFlow-based HDC cannot carefully leverage the mapping data to the memory hierarchy on the GPU since HDC data are large in size and have specific memory access patterns. For the off-the-shelf embedded GPUs, these characteristics lead to underutilized hardware resources, impeding the high throughput and perfor- mance of HDC applications.\n\nIn this paper, we present a GPU-powered HDC framework called XCelHD. XCelHD effectively mitigates the low parallelization and memory access issues of the existing works. Furthermore, we propose a highly parallel HDC training, called PARTRAIN that elevates the GPU parallelism by redesigning the HDC training algorithm. It converts the sequential HD training process into multiple simultaneous subtasks; each task learns partial models with different training samples while updating the final model based on the partial models. Our contributions are summarized as follows: \u2022 We present XCelHD for GPUs, which supports various classification tasks based on HDC. Unlike existing DL framework-based implementations, XCelHD parallelizes the HDC learning procedure in a GPU-friendly way while intelligently optimizing memory allocation/access patterns.\n\nTo the best of our knowledge, this is the first work that designs GPU-centric optimization strategies for HDC. \u2022 We propose a novel HDC training method for high parallelism. The proposed training method enables full utilization of hardware resources and reduces the number of required training epochs by 4\u00d7 on top of the GPU-accelerated HDC. \u2022 Our strategies specially designed for GPU maximize data reuse and enhance cache utilization, additionally accelerating the prediction (up to 12\u00d7) and the encoding step (up to 48%) in HDC with simple parallelization, respectively. We implemented and evaluated XCelHD with various datasets using NVIDIA Jetson TX2, which aims to low-power edge devices. Our experimental results show that the proposed parallelized training method can save the number of training iterations required to achieve the same accuracy by 4\u00d7. Furthermore, our design is up to 35\u00d7 faster than TensorFlow 2 XLA-based HDC classification implementation.\n\nII. XCELHD OVERVIEW Classification is a representative cognitive task that HDC can be used [4], [16]. Figure 1 illustrates the XCelHD learning procedure, which consists of the encoding, training, and inference stages. Encoding maps (encode) real-world data into the HD space. There are various encoding methods. One of the most popular ones is the ID-level method [4], [14], [17]. First, the encoding module randomly generates orthogonal unique ID hypervectors,\n\n\u2212 \u2192 ID f \u2208 {\u22121, 1} D assigned to an individual f th index of feature. Next, to capture an arbitrary value m in f th index of feature, we create level hypervectors, \u2212\u2192 LV m for using Fig. 2: Overview of the proposed parallel training method the method shown in [4]. Once ID hypervectors and level hypervectors are generated, we can encode an input data to an input hypervector \u2212 \u2192 I as follows:\n\u2212 \u2192 C1 \u2212 \u2192 C2 \u2212 \u2192 CN \u2212 \u2192 C1 \u2212 \u2192 C2 \u2212 \u2192 CN \u2212 \u2192 C1 \u2212 \u2192 C2 \u2212 \u2192 CN \u2212 \u2192 C1 \u2212 \u2192 C2 \u2212 \u2192 CN\u2212 \u2192 I = f \u2212 \u2192 ID f \u2212\u2192 LV m(1)\nwhere indicates an element-wise multiplication. The IDlevel method is more complex but includes all the steps of other algorithms, such as random projection. Thus, any implementation that can carry out ID-level can also implement the random projection [4]. Initialization and training stage builds class hypervectors. During the initialization, also known as single-pass training, it combines all hypervectors \u2212 \u2192 H k i belonging each class k using the element-wise addition, i.e.,\n\u2212 \u2192 C k = i \u2212 \u2192 H k i where \u2212 \u2192 C k indicates class hypervector for class k.\nIterative training is used to achieve higher accuracy. HDC calibrates class hypervectors (HDC model) based on the prediction using the current state of class hypervectors. First, it compares the ground truth label to the predicted class. We select a class with a maximum similarity between input hypervector and class hypervectors. For binarized hypervectors, we use the Hamming distance, while cosine similarity is utilized for another type of data. If the prediction is incorrect, the training module updates the HDC model: the input hypervector is subtracted from the class hypervector of the incorrectly predicted class and added to the class hypervector of the correct answer. We can apply the learning rate (\u03b7 \u2208 (0, 1]) for scaling the input hypervector during the update [18]. We define epoch as the number of training iterations. Inference or prediction finds the most similar class hypervector to the test data. We first encode test queries (data) into hypervectors, called query hypervectors using the same encoding method used for training. For each query hypervector \u2212 \u2192 q , the inference module calculates its similarity against the class hypervectors. Next, it selects a class with the highest similarity using either Hamming distance (binary hypervectors) or cosine similarity (non-binary data).\n\n\nIII. XCELHD OPTIMIZATION TECHNIQUES A. PARTRAIN: Parallel Training for HDC Classification\n\nThe iterative training process is essential to achieve higher accuracy than the single-pass (shot) training (see Fig. 5.) However, the training stage processes data sequentially, thus it hinders from data-parallel benefits of the GPU. It can lead to the bottleneck of the GPU-based HDC. We propose a method to refine the HDC model in parallel, called PARTRAIN. It can unleash parallelism, maximize hardware utilization and achieve the target accuracy with small iterations. 1) ENC-Streamer: ENC-STREAMER is designed to allocate data to the appropriate memory hierarchy to achieve high efficiency during data-intensive encoding and inference stages. These two stages map training and testing data to the HD space, respectively. Encoding often takes as much as 70% of the overall runtime of HDC on CPU [19], which is the main bottleneck of HDC application. Here we focus on one of the popular HDC encoding techniques, the ID-Level encoding. Same strategy work for the simpler random projection method as well. A naive way to implement the ID-Level encoding on GPU is to parallelize over training data and each dimension of input hypervectors. A thread accumulates \u2212 \u2192 ID i \u2212\u2192 LV i over features where i is the index of an element in a hypervector that a thread is computing.\n3C-2 \u2212 \u2192 I \u2212 \u2212 \u2192 ID f \u2212 \u2212 \u2192 ID 1 \u2212 \u2212 \u2192 ID 2 \u2212 \u2212\u2212\u2212 \u2192 LV base\nENC-STREAMER enhances memory transactions on top of the parallelization. Fig. 3 shows the design of the encoding module with ENC-STREAMER. The GPGPU memory hierarchy includes several memories, e.g., global memory, shared memory, and constant memory, each having different characteristics such as size and latency. The execution time diverges depending on a data allocation strategy of an algorithm. For instance, global memory offers the largest capacity with long latency. Constant memory has a small size and is read-only but can be accessed with a few clock cycles.\n\nWe use two streams to hide the memory copy time between the host and the GPU. The streamed data is encoded to the hypervector using the aforementioned method and stored in the global memory. On top of that, ENC-STREAMER applies an additional caching strategy for ID hypervectors and level hypervectors, which is a frequently used term as shown in Eq. 1. We cache \u2212 \u2192 ID in the shared memory. This encoded data is used for the rest of the runtime, thus completely eliminating the data movement costs. At the same time, the same strategy is applied to the other stream synchronously. It runs until all datapoints are covered in an interleaved fashion. For the GPUs with unified virtual addressing, we use a single stream and managed memory. Raw data requires one read transaction and does not have to be cached. This mechanism helps to alleviate the memory capacity limitation that is especially critical in embedded GPUs.\n\nFurthermore, ENC-STREAMER optimizes access to level hypervectors. Since the feature values are random, the access to their associated level hypervector is irregular. In other words, it entails non-coalesced access to memory, leading to increased latency. Based on the fact that the level hypervector generation method uses flips according to quantized levels, we generate only one level hypervector to use it as a base, called base-level hypervector, \u2212 \u2212\u2212\u2212 \u2192 LV base . Then, according to the value of each index, the corresponding level hypervector is generated in-situ. Nevertheless, this approach can still cause contention during memory access if it is placed on the global memory. We bit-pack the base-level hypervector and store it in constant memory. Since hypervectors require a large size array, packing is essential to satisfy the constant memory constraint. Empirically, packing eight bits showed the best performance. Each thread encodes the eight components of the input hypervector.\n\n2) L2-Recycle: Pairwise similarity computation is the core component of HDC training and inference. The cosine similarity \u03b4(x, y) between vector x and y is defined by\n\u03b4(x, y) = x \u00b7 y x y .(2)\nTo accommodate higher parallelism, we compute the numerator and the denominator separately. Then, operations, including the L2 norm, can leverage the parallel reduction technique because of the large dimensionality. For HDC vectors, decomposing product-and-sum computations into multiple threads can lead to higher performance. However, this strategy showed a marginal improvement compared to the CPU-based HDC in our experiments due to a lack of data-parallel benefits. Fig. 4: Design of L2-RECYCLE-applied training module in HDC-based classification Instead, we devise the L2-RECYCLE module, which optimizes the memory access time in HDC by identifying invariant calculation results in the similarity computation. It computes the two L2 norm components only when required. Fig. 4 shows the design of L2-RECYCLE-applied training module in HDC-based classification. Since the training procedure calculates the similarity (s[k]) between the k th class hypervector and input hypervectors, we pre-compute their L2 norms when updating them in the training loop. Also, L2-RECYCLE separately manages the array of the \u2212 \u2192 C k . This strategy significantly minimizes the memory access time to the large size of hypervectors. In addition, it removes unnecessary computation during the HDC application execution.\n3C-2 \u2212 \u2192 I \u2212 \u2192 C1 \u2212 \u2192 C2 \u2212 \u2192 CN \u2212 \u2192 I \u2212 \u2192 I \u2212 \u2192 C1 \u2212 \u2192 C2 \u2212 \u2192 CN \u2212 \u2192 I\n\nIV. EVALUATION A. Experimental Setup\n\nIn this section, we test XCelHD on HDC-based classification [4]. We evaluated the implementation on the CPU and GPU in the NVIDIA Jetson TX2 device with Jetpack 4.4.1. We set the dynamic voltage and frequency scaling governor.\n\nWe compare the performance of our XCelHD-based implementation with the state-of-the-art TensorFlow 2 XLA-based HDC, which implements the design in [15]. Note that we did not adopt TensorRT/TFLite-based inference implementation, as end-to-end HDC execution includes the model training. Next, to show the energy efficiency of XCelHD, we compare the energy consumption with the state-of-the-art CPU-based HDC classification [18] implementation, which uses Python with a C++ backend to take full advantage of the parallel capabilities of SIMD. We included the communication time, e.g., memory copy time, between the GPU and the host. For the power consumption measurement, tegrastats utility is used. We set fixed quantization level Q to 100, and epochs to 20. The hypervector dimensionality, D, is set to 10, 000 and 4, 000. Also, PARTRAIN-powered XCelHD is configured with 10 local trainers. Datasets To verify the actual use-case of HDC-based classification, we evaluated the implementation on a wide range of following benchmark datasets [20], [21] including UCIHAR: detecting human activity, FACE: classifying images with faces and non-faces, each sampled from Caltech's 10, 000 web faces dataset, CIFAR-100, and Pascal VOS 2012 datasets, ISOLET: recognizing audio of the English alphabet, and PAMAP2: classifying human activities based on data from a heart rate monitor and inertial measurement units.  -20). Also, we measure the accuracy of the first (PARTRAIN-1) and the fifth epoch (PARTRAIN-5), for PAR-TRAIN-powered XCelHD. As shown in Fig. 5, XCelHD shows comparable accuracies compared to the baseline. The peak performance of the baseline was challenging to obtain with a single epoch of PARTRAIN. Nevertheless, after five iterations, the PARTRAIN-powered HDC model offered a similar accuracy compared to the original HDC. Considering that the conventional training method requires 20 epochs to reach the target accuracy, the proposed strategy can quickly reach the target accuracy. It reduces required epochs by 4\u00d7 while enhancing the data-parallel benefits and hardware utilization on GPU. Note that the overhead of gathering local trainer information and distributing the global HDC model was less than 5% of the total execution time. C. Performance Improvements Fig. 6 describes the performance improvement of the XCelHD-powered HDC as compared to the TensorFlow GPUbased implementation. We measured the execution time per stage since the optimization technique that dominates each stage is different.\n\nThe encoding module would be mainly optimized by ENC-STREAMER. Multiple accesses to the dataset, \u2212\u2192 LV m , \u2212 \u2192 ID f occurs, so caching on constant memory improves the performance. The ENC-STREAMER module flexibly maps the data on the proper CUDA memory hierarchy. Also, accumulating through for loop is used due to the reduction operation. In the GPGPU architecture, the size of the register assigned to each thread is limited. If the program exceeds the constraint, it utilizes global memory. As shown in Fig. 6(a), the scenarios with relatively small features are accelerated more compared to the others. When data is fittable on the on-chip memory constraint, ENC-STREAMER in XCelHD automatically utilizes it. The result shows that XCelHD enables the encoding module on average 20\u00d7 faster, with a max speedup of 84\u00d7, compared to the TensorFlow-based HDC.\n\nIterative training makes predictions based on the current class hypervectors and updates class hypervectors as a result. In particular, the prediction is primarily benefitted by L2-RECYCLE. Fig. 6(b) illustrates the speedup of the training stage. Although the training module does not offer substantial data-level parallelism compared to the encoding module, XCelHD improves the speed by reusing computation results. Furthermore, since PARTRAIN can reduce the number of The inference stage improvement originates from the combination of optimization for the encoding module and the prediction, i.e., from ENC-STREAMER and L2-RECYCLE. Unlike the training stage, which processes data sequentially to update the class hypervectors, we can make a prediction on multiple test data in parallel. The inference module benefits from both simultaneous data processing and parallel reduction. As shown in Fig. 6(c), XCelHD yields performance gain up to 83\u00d7, and 20\u00d7 on average.\n\nUltimately, XCelHD offers significant speedup in all stages of the HDC classification compared to the existing TensorFlow-based solution. XCelHD most efficiently handles the encoding stage, which is the bottleneck of CPU-based HDC. PARTRAIN helps to overcome limited parallelism and underutilization during the training process in GPU-based HDC. For the end-to-end execution of HDC process, XCelHDpowered implementation gains up to 35\u00d7, with an average speedup of 23\u00d7 as illustrated in Fig. 6(d). Fig. 7 shows a comparison of the energy consumption of XCelHD against the low-power CPU. Here, we disabled PARTRAIN for a fair comparison. In terms of power consumption, XCelHD showed different power consumption trends for each stage. Since XCelHD maximizes parallelism in the encoding module, it showed the highest power consumption. Nevertheless, the execution time reduced significantly, leading to 41\u00d7 (25\u00d7) energy improvement at maximum (on average). The training stage without PARTRAIN involves the serialized data feed, causing limited thread utilization. Even if this can impede energy efficiency improvement, we successfully reduced the computation and the execution time using L2-RECYCLE, leading to a significant energy efficiency improve- ment: up to 400\u00d7 and 163\u00d7 on average. Enabling PARTRAIN increases power consumption up to 7\u00d7 due to higher hardware utilization. Nevertheless, the proposed framework offers a significant amount of energy improvement in the case of XCelHD without PARTRAIN. Therefore, increased power consumption can also be compensated by reduced epochs.\n\n\nD. Energy Efficiency Improvements\n\nOverall, XCelHD results in up to 172\u00d7 improved energy efficiency over CPU. The energy efficiency improvements of XCelHD versus TensorFlow GPU are directly proportional to the performance improvements shown in Fig. 6, as both run on the same hardware platform.\n\n\nE. Effectiveness of ENC-STREAMER and L2-RECYCLE\n\nTo show the effectiveness of our memory access optimization strategies, ENC-STREAMER and L2-RECYCLE, we compare XCelHD with and without these modules. The effectiveness of our optimization strategies varies according to the dataset characteristics. The performance of the encoding module is affected by the dimension size D and the feature size F . The speed (per epoch) of the training stage is affected by D and the number of classes C. While fixing the dataset size to 10, 000, we set representative scenario combinations.\n\nFor ENC-STREAMER, we evaluated on four scenarios, (F , D): Case A) (500, 10000), case B) (500, 4000), case C) (250, 10000) and case D) (250, 4000), with fixed C = 10. As shown in Fig. 8(a), ENC-STREAMER improves the speed by 48%, 14%, 25% and 22% in case A, case B, case C, and case D, respectively. As the dimensionality increases, the benefit of 3C-2 ENC-STREAMER increases. Also, we observed that the power consumption between the two variants is similar, leading to energy efficiency improvement.\n\nWhen evaluating L2-RECYCLE, we fixed F = 500 and changed the C. We experimented on four scenarios, (C, D): Case A) (10, 10000), case B) (10, 4000), case C) (20, 10000) and case D) (20,4000). In the case of D = 10000, it leads to around 12\u00d7 performance improvement, while case B and case D (D=4000) shows 8\u00d7 and 5\u00d7 speedup, respectively. As shown in Fig. 8(b), the training without L2-RECYCLE is heavily affected by the dimensionality. In contrast, our optimization strategy guarantees almost consistent execution time regardless of the D and C, with only a minimal amount of additional memory usage (e.g., 80 Byte in case C). Reducing the redundant computation plays a significant role in accelerating the training with minimal overhead.\n\nV. RELATED WORK HDC consists of many bit-level arithmetic operations that can be effectively parallelized. The ASIC designs are proposed, e.g., similarity computation circuits [11] and text classification ASIC design [22] based on advanced memory technology. The work in [14] also shows how to accelerate the HD encoding scheme using PIM techniques. In [23], the authors present a hybrid ASIC architecture that runs both DL and HDC. In [17], the authors propose the HDCbased classification running on the FPGA. Although FPGA, ASIC, and PIM implementations can offer superior efficiency, their design and synthesis time impede rapid development. The work in [15] shows TensorFlow for HDC classification acceleration on GPU. Hypervectors can be treated as a tensor data type. Hence, HDC operations can be implemented and accelerated with the GPU with tensor operations in TensorFlow. The results show that TensorFlow-based HDC running on the NVIDIA Jetson Nano consumes more energy and runs just as slow as HDC on CPU, thus illustrating the need for a tool such as XCelHD that can automatically create an efficient mapping of HDC applications to GPUs. XCelHD maximizes parallelism specific to HDC applications while effectively leveraging the memory hierarchy and minimizing the memory accesses.\n\nVI. CONCLUSION In this paper, we presented a GPU-based HDC acceleration framework called XCelHD. We address the memory access challenges in the current HDC application running on the GPU, with efficient cache utilization and data reuse. The proposed optimization module benefits the encoding and the prediction of the HDC, which affects the end-to-end pipeline of HDC applications. Furthermore, we propose a parallelized training method, called PARTRAIN, to enhance hardware utilization and data-parallel benefits on the GPU. The proposed PARTRAIN reduces the required epochs to obtain comparable accuracy by 4\u00d7. Also, our evaluation results with the lowpowered embedded GPU show that XCelHD is 23\u00d7 faster execution time on average as compared to the existing Ten-sorFlow GPU-based HDC classification implementation.\n\nFig. 1 :\n1Overview of HDC-based classification.\n\nFig. 3 :\n3Overview of ENC-STREAMER-applied HDC encodingFig. 2 describes the overview of the proposed training. The main principle of the proposed method uses multiple local trainers to generate a local HDC model and creates the global HDC model G, which is used for the inference. A local trainer has two sets of hypervectors, each with C hypervectors, where C is the number of classes. One set stores a local HDC model, and the other accumulates the changes of the local HDC model. We initialize the latter hypervector set, D with zeros. The local HDC models and global HDC models are initialized with the results of HDC single-pass training. We shard the training data by identical numbers and feed them to each local trainer. (Fig. 2-\u2022 1 ) It trains its local HDC model following the conventional HDC training process (Fig. 2-\u2022 2 ). It is updated when the predicted values differ from the ground truth. Additionally, changes of the local HDC model are aggregated to D (Fig. 2-\u2022 3 ). After training with assigned datapoints, we update the global HDC model by calculating G = i D i , where i indicates ith local trainer (Fig. 2-\u2022 4 ). Finally, the global model replaces the local HDC model(Fig. 2-\u2022 5). Note that every accumulating operation is parallelized over the hypervector dimension. We define this whole process as one epoch. The original HDC training processes the same amount of data points in a single epoch. Using multiple local trainers in a single GPU device can enable a higher level of data parallelism. Also, the number of local trainers can be dynamically adjusted based on available hardware resources. B. HDC-BASED MEMORY OPTIMIZATION XCelHD encompasses two key optimization techniques to reduce the memory access times due to frequent accesses to large size hypervectors: ENC-STREAMER and L2-RECYCLE. They are used during the encoding and the training of HDC.\n\nFig. 5 :\n5Accuracy comparison between PARTRAIN and the conventional HDC training method. B. Accuracy of PARTRAIN If XCelHD with PARTRAIN refines an HDC model effectively, it should show comparable accuracies to the XCelHD without PARTRAIN, i.e., conventional HDC training method. As a baseline, we compared the HDC model, which trained with a single epoch (Single Shot), 20 epochs of training (Retraining\n\nFig. 7 :\n7Energy consumption comparison of XCelHD versus HDC running on the embedded CPU required epochs to achieve the target accuracy, XCelHD can lead to significant speedup in the actual deployment of GPUpowered HDC applications. XCelHD provides up to 34\u00d7 speedup, with 24\u00d7 speedup on average in the inference module compared to the TensorFlow GPU-based HDC.\n\nFig. 8 :\n8Execution time with and without proposed optimization techniques\nACKNOWLEDGMENTSThis work was supported in part by CRISP, one of six centers in JUMP, an SRC program sponsored by DARPA, in part by SRC-Global Research Collaboration grants, and also NSF grants # 2100237, #1730158, #1826967, and #1911095.\nP Kanerva, Sparse distributed memory. MIT pressP. Kanerva, Sparse distributed memory. MIT press, 1988.\n\nA robust and energyefficient classifier using brain-inspired hyperdimensional computing. A Rahimi, P Kanerva, J M Rabaey, International Symposium on Low Power Electronics and Design. Association for Computing MachineryA. Rahimi, P. Kanerva, and J. M. Rabaey, \"A robust and energy- efficient classifier using brain-inspired hyperdimensional computing,\" in International Symposium on Low Power Electronics and Design. Association for Computing Machinery, 2016, p. 64-69.\n\nLearning sensorimotor control with neuromorphic sensors: Toward hyperdimensional active perception. A Mitrokhin, P Sutor, C Ferm\u00fcller, Y Aloimonos, Science Robotics. 430A. Mitrokhin, P. Sutor, C. Ferm\u00fcller, and Y. Aloimonos, \"Learning sen- sorimotor control with neuromorphic sensors: Toward hyperdimensional active perception,\" Science Robotics, vol. 4, no. 30, May 2019.\n\nVoicehd: Hyperdimensional computing for efficient speech recognition. M Imani, D Kong, A Rahimi, T Rosing, International Conference on Rebooting Computing (ICRC). IEEEM. Imani, D. Kong, A. Rahimi, and T. Rosing, \"Voicehd: Hyperdi- mensional computing for efficient speech recognition,\" in International Conference on Rebooting Computing (ICRC). IEEE, 2017, pp. 1-8.\n\nHyperrec: Efficient recommender systems with hyperdimensional computing. Y Guo, M Imani, J Kang, S Salamat, J Morris, B Aksanli, Y Kim, T Rosing, 10.1145/3394885.3431553Proceedings of the 26th Asia and South Pacific Design Automation Conference, ser. ASPDAC '21. the 26th Asia and South Pacific Design Automation Conference, ser. ASPDAC '21New York, NY, USAAssociation for Computing MachineryY. Guo, M. Imani, J. Kang, S. Salamat, J. Morris, B. Aksanli, Y. Kim, and T. Rosing, \"Hyperrec: Efficient recommender systems with hyperdimensional computing,\" in Proceedings of the 26th Asia and South Pacific Design Automation Conference, ser. ASPDAC '21. New York, NY, USA: Association for Computing Machinery, 2021, p. 384-389. [Online]. Available: https://doi.org/10.1145/3394885.3431553\n\nModeling dependencies in multiple parallel data streams with hyperdimensional computing. O R\u00e4s\u00e4nen, S Kakouros, IEEE Signal Processing Letters. 217O. R\u00e4s\u00e4nen and S. Kakouros, \"Modeling dependencies in multiple parallel data streams with hyperdimensional computing,\" IEEE Signal Processing Letters, vol. 21, no. 7, pp. 899-903, 2014.\n\nThe kanerva machine: A generative distributed memory. Y Wu, G Wayne, A Graves, T Lillicrap, arXiv:1804.01756arXiv preprintY. Wu, G. Wayne, A. Graves, and T. Lillicrap, \"The kanerva machine: A generative distributed memory,\" arXiv preprint arXiv:1804.01756, 2018.\n\nG Karunaratne, M L Gallo, G Cherubini, L Benini, A Rahimi, A Sebastian, arXiv:1906.01548-memory hyperdimensional computing. arXiv preprintG. Karunaratne, M. L. Gallo, G. Cherubini, L. Benini, A. Rahimi, and A. Sebastian, \"In-memory hyperdimensional computing,\" arXiv preprint arXiv:1906.01548, 2019.\n\nProperties of sparse distributed representations and their application to hierarchical temporal memory. S Ahmad, J Hawkins, arXiv:1503.07469arXiv preprintS. Ahmad and J. Hawkins, \"Properties of sparse distributed represen- tations and their application to hierarchical temporal memory,\" arXiv preprint arXiv:1503.07469, 2015.\n\nA programmable hyper-dimensional processor architecture for human-centric iot. S Datta, R Antonio, IEEE Journal on Emerging and Selected Topics in Circuits and Systems. 93S. Datta, R. Antonio et al., \"A programmable hyper-dimensional pro- cessor architecture for human-centric iot,\" IEEE Journal on Emerging and Selected Topics in Circuits and Systems, vol. 9, no. 3, pp. 439-452, 2019.\n\nExploring hyperdimensional associative memory. M Imani, A Rahimi, D Kong, T Rosing, J M Rabaey, IEEE International Symposium on High Performance Computer Architecture (HPCA). M. Imani, A. Rahimi, D. Kong, T. Rosing, and J. M. Rabaey, \"Ex- ploring hyperdimensional associative memory,\" in IEEE International Symposium on High Performance Computer Architecture (HPCA), Feb. 2017.\n\nBraininspired computing exploiting carbon nanotube fets and resistive ram: Hyperdimensional computing case study. T F Wu, H Li, P.-C Huang, A Rahimi, J M Rabaey, IEEE International Solid-State Circuits Conference-(ISSCC). IEEET. F. Wu, H. Li, P.-C. Huang, A. Rahimi, J. M. Rabaey et al., \"Brain- inspired computing exploiting carbon nanotube fets and resistive ram: Hyperdimensional computing case study,\" in IEEE International Solid- State Circuits Conference-(ISSCC). IEEE, 2018, pp. 492-494.\n\nHierarchical hyperdimensional computing for energy efficient classification. M Imani, C Huang, D Kong, T Rosing, Proceedings of the 55th Annual Design Automation Conference. the 55th Annual Design Automation ConferenceM. Imani, C. Huang, D. Kong, and T. Rosing, \"Hierarchical hyperdi- mensional computing for energy efficient classification,\" in Proceedings of the 55th Annual Design Automation Conference, 2018.\n\nFelix: Fast and energy-efficient logic in memory. S Gupta, M Imani, T Rosing, Proceedings of the International Conference on Computer-Aided Design. the International Conference on Computer-Aided DesignS. Gupta, M. Imani, and T. Rosing, \"Felix: Fast and energy-efficient logic in memory,\" in Proceedings of the International Conference on Computer-Aided Design, 2018.\n\nA programmable hyper-dimensional processor architecture for humancentric IoT. S Datta, R A G Antonio, A R S Ison, J M Rabaey, IEEE Journal on Emerging and Selected Topics in Circuits and Systems. 93S. Datta, R. A. G. Antonio, A. R. S. Ison, and J. M. Rabaey, \"A programmable hyper-dimensional processor architecture for human- centric IoT,\" IEEE Journal on Emerging and Selected Topics in Circuits and Systems, vol. 9, no. 3, pp. 439-452, Sep. 2019.\n\nLanguage recognition using random indexing. A Joshi, J Halseth, P Kanerva, A. Joshi, J. Halseth, and P. Kanerva, \"Language recognition using random indexing,\" 2014.\n\nF5-hd: Fast flexible fpga-based framework for refreshing hyperdimensional computing. S Salamat, M Imani, B Khaleghi, T Rosing, International Symposium on FPGAs. ACMS. Salamat, M. Imani, B. Khaleghi, and T. Rosing, \"F5-hd: Fast flexible fpga-based framework for refreshing hyperdimensional computing,\" in International Symposium on FPGAs. ACM, 2019, pp. 53-62.\n\nAdapthd: Adaptive efficient training for brain-inspired hyperdimensional computing. M Imani, J Morris, S Bosch, H Shu, G De Micheli, T Rosing, IEEE BioCAS. M. Imani, J. Morris, S. Bosch, H. Shu, G. De Micheli, and T. Rosing, \"Adapthd: Adaptive efficient training for brain-inspired hyperdimen- sional computing,\" in IEEE BioCAS, 2019, pp. 1-4.\n\nBric: Locality-based encoding for energyefficient brain-inspired hyperdimensional computing. M Imani, J Morris, 56th Annual Design Automation Conference. M. Imani, J. Morris et al., \"Bric: Locality-based encoding for energy- efficient brain-inspired hyperdimensional computing,\" in 56th Annual Design Automation Conference, 2019, pp. 1-6.\n\nUCI machine learning repository. D Dua, C Graff, D. Dua and C. Graff, \"UCI machine learning repository,\" 2017. [Online]. Available: http://archive.ics.uci.edu/ml\n\nIntroducing a new benchmarked dataset for activity monitoring. A Reiss, D Stricker, 2012 16th International Symposium on Wearable Computers. A. Reiss and D. Stricker, \"Introducing a new benchmarked dataset for activity monitoring,\" in 2012 16th International Symposium on Wearable Computers, 2012, pp. 108-109.\n\nHyperdimensional computing with 3d vrram in-memory kernels: Device-architecture co-design for energy-efficient, error-resilient language recognition. H Li, T F Wu, A Rahimi, IEEE IEDM. 16.1.1-16.1.4.H. Li, T. F. Wu, A. Rahimi et al., \"Hyperdimensional computing with 3d vrram in-memory kernels: Device-architecture co-design for energy-efficient, error-resilient language recognition,\" in IEEE IEDM, pp. 16.1.1-16.1.4.\n\nSynergiclearning: neural network-based feature extraction for highly-accurate hyperdimensional learning. M Nazemi, A Esmaili, A Fayyazi, M Pedram, 2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD). IEEEM. Nazemi, A. Esmaili, A. Fayyazi, and M. Pedram, \"Synergiclearning: neural network-based feature extraction for highly-accurate hyperdi- mensional learning,\" in 2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD). IEEE, 2020, pp. 1-9.\n", "annotations": {"author": "[{\"end\":120,\"start\":90},{\"end\":156,\"start\":121},{\"end\":192,\"start\":157},{\"end\":223,\"start\":193},{\"end\":371,\"start\":224},{\"end\":556,\"start\":372},{\"end\":607,\"start\":557}]", "publisher": null, "author_last_name": "[{\"end\":103,\"start\":99},{\"end\":136,\"start\":128},{\"end\":168,\"start\":165},{\"end\":206,\"start\":200}]", "author_first_name": "[{\"end\":98,\"start\":90},{\"end\":127,\"start\":121},{\"end\":164,\"start\":157},{\"end\":199,\"start\":193}]", "author_affiliation": "[{\"end\":370,\"start\":225},{\"end\":555,\"start\":373},{\"end\":606,\"start\":558}]", "title": "[{\"end\":87,\"start\":1},{\"end\":694,\"start\":608}]", "venue": null, "abstract": "[{\"end\":1950,\"start\":729}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2517,\"start\":2514},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2777,\"start\":2774},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2791,\"start\":2788},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2842,\"start\":2839},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2869,\"start\":2866},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2902,\"start\":2899},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3000,\"start\":2997},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3009,\"start\":3006},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3026,\"start\":3023},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3099,\"start\":3096},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3207,\"start\":3203},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3212,\"start\":3209},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3497,\"start\":3493},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3571,\"start\":3568},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3577,\"start\":3573},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3583,\"start\":3579},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3625,\"start\":3621},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4357,\"start\":4353},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7258,\"start\":7255},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7264,\"start\":7260},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7531,\"start\":7528},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7537,\"start\":7533},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7543,\"start\":7539},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7890,\"start\":7887},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8389,\"start\":8386},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9475,\"start\":9471},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10901,\"start\":10897},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15587,\"start\":15584},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15903,\"start\":15899},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16177,\"start\":16173},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16794,\"start\":16790},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16800,\"start\":16796},{\"end\":17161,\"start\":17157},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23243,\"start\":23239},{\"end\":23248,\"start\":23243},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23978,\"start\":23974},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24019,\"start\":24015},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24073,\"start\":24069},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24155,\"start\":24151},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24238,\"start\":24234},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24459,\"start\":24455}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25958,\"start\":25910},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27840,\"start\":25959},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28246,\"start\":27841},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28609,\"start\":28247},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28685,\"start\":28610}]", "paragraph": "[{\"end\":3027,\"start\":1969},{\"end\":4287,\"start\":3029},{\"end\":5346,\"start\":4289},{\"end\":6194,\"start\":5348},{\"end\":7162,\"start\":6196},{\"end\":7625,\"start\":7164},{\"end\":8020,\"start\":7627},{\"end\":8615,\"start\":8134},{\"end\":10003,\"start\":8693},{\"end\":11369,\"start\":10097},{\"end\":11998,\"start\":11430},{\"end\":12920,\"start\":12000},{\"end\":13917,\"start\":12922},{\"end\":14085,\"start\":13919},{\"end\":15413,\"start\":14111},{\"end\":15750,\"start\":15524},{\"end\":18267,\"start\":15752},{\"end\":19126,\"start\":18269},{\"end\":20094,\"start\":19128},{\"end\":21681,\"start\":20096},{\"end\":21978,\"start\":21719},{\"end\":22555,\"start\":22030},{\"end\":23057,\"start\":22557},{\"end\":23796,\"start\":23059},{\"end\":25091,\"start\":23798},{\"end\":25909,\"start\":25093}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8104,\"start\":8021},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8133,\"start\":8104},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8692,\"start\":8616},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11429,\"start\":11370},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14110,\"start\":14086},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15484,\"start\":15414}]", "table_ref": null, "section_header": "[{\"end\":1967,\"start\":1952},{\"end\":10095,\"start\":10006},{\"end\":15522,\"start\":15486},{\"end\":21717,\"start\":21684},{\"end\":22028,\"start\":21981},{\"end\":25919,\"start\":25911},{\"end\":25968,\"start\":25960},{\"end\":27850,\"start\":27842},{\"end\":28256,\"start\":28248},{\"end\":28619,\"start\":28611}]", "table": null, "figure_caption": "[{\"end\":25958,\"start\":25921},{\"end\":27840,\"start\":25970},{\"end\":28246,\"start\":27852},{\"end\":28609,\"start\":28258},{\"end\":28685,\"start\":28621}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7274,\"start\":7266},{\"end\":7815,\"start\":7809},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10216,\"start\":10210},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11509,\"start\":11503},{\"end\":14588,\"start\":14582},{\"end\":14892,\"start\":14886},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17251,\"start\":17239},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17301,\"start\":17295},{\"end\":18034,\"start\":18028},{\"end\":18784,\"start\":18775},{\"end\":19327,\"start\":19318},{\"end\":20031,\"start\":20022},{\"end\":20591,\"start\":20582},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20599,\"start\":20593},{\"end\":21934,\"start\":21928},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22745,\"start\":22736},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23417,\"start\":23408}]", "bib_author_first_name": "[{\"end\":28925,\"start\":28924},{\"end\":29118,\"start\":29117},{\"end\":29128,\"start\":29127},{\"end\":29139,\"start\":29138},{\"end\":29141,\"start\":29140},{\"end\":29599,\"start\":29598},{\"end\":29612,\"start\":29611},{\"end\":29621,\"start\":29620},{\"end\":29634,\"start\":29633},{\"end\":29943,\"start\":29942},{\"end\":29952,\"start\":29951},{\"end\":29960,\"start\":29959},{\"end\":29970,\"start\":29969},{\"end\":30313,\"start\":30312},{\"end\":30320,\"start\":30319},{\"end\":30329,\"start\":30328},{\"end\":30337,\"start\":30336},{\"end\":30348,\"start\":30347},{\"end\":30358,\"start\":30357},{\"end\":30369,\"start\":30368},{\"end\":30376,\"start\":30375},{\"end\":31114,\"start\":31113},{\"end\":31125,\"start\":31124},{\"end\":31413,\"start\":31412},{\"end\":31419,\"start\":31418},{\"end\":31428,\"start\":31427},{\"end\":31438,\"start\":31437},{\"end\":31623,\"start\":31622},{\"end\":31638,\"start\":31637},{\"end\":31640,\"start\":31639},{\"end\":31649,\"start\":31648},{\"end\":31662,\"start\":31661},{\"end\":31672,\"start\":31671},{\"end\":31682,\"start\":31681},{\"end\":32028,\"start\":32027},{\"end\":32037,\"start\":32036},{\"end\":32330,\"start\":32329},{\"end\":32339,\"start\":32338},{\"end\":32686,\"start\":32685},{\"end\":32695,\"start\":32694},{\"end\":32705,\"start\":32704},{\"end\":32713,\"start\":32712},{\"end\":32723,\"start\":32722},{\"end\":32725,\"start\":32724},{\"end\":33132,\"start\":33131},{\"end\":33134,\"start\":33133},{\"end\":33140,\"start\":33139},{\"end\":33149,\"start\":33145},{\"end\":33158,\"start\":33157},{\"end\":33168,\"start\":33167},{\"end\":33170,\"start\":33169},{\"end\":33591,\"start\":33590},{\"end\":33600,\"start\":33599},{\"end\":33609,\"start\":33608},{\"end\":33617,\"start\":33616},{\"end\":33978,\"start\":33977},{\"end\":33987,\"start\":33986},{\"end\":33996,\"start\":33995},{\"end\":34374,\"start\":34373},{\"end\":34383,\"start\":34382},{\"end\":34387,\"start\":34384},{\"end\":34398,\"start\":34397},{\"end\":34402,\"start\":34399},{\"end\":34410,\"start\":34409},{\"end\":34412,\"start\":34411},{\"end\":34791,\"start\":34790},{\"end\":34800,\"start\":34799},{\"end\":34811,\"start\":34810},{\"end\":34998,\"start\":34997},{\"end\":35009,\"start\":35008},{\"end\":35018,\"start\":35017},{\"end\":35030,\"start\":35029},{\"end\":35358,\"start\":35357},{\"end\":35367,\"start\":35366},{\"end\":35377,\"start\":35376},{\"end\":35386,\"start\":35385},{\"end\":35393,\"start\":35392},{\"end\":35396,\"start\":35394},{\"end\":35407,\"start\":35406},{\"end\":35712,\"start\":35711},{\"end\":35721,\"start\":35720},{\"end\":35992,\"start\":35991},{\"end\":35999,\"start\":35998},{\"end\":36185,\"start\":36184},{\"end\":36194,\"start\":36193},{\"end\":36584,\"start\":36583},{\"end\":36590,\"start\":36589},{\"end\":36592,\"start\":36591},{\"end\":36598,\"start\":36597},{\"end\":36959,\"start\":36958},{\"end\":36969,\"start\":36968},{\"end\":36980,\"start\":36979},{\"end\":36991,\"start\":36990}]", "bib_author_last_name": "[{\"end\":28933,\"start\":28926},{\"end\":29125,\"start\":29119},{\"end\":29136,\"start\":29129},{\"end\":29148,\"start\":29142},{\"end\":29609,\"start\":29600},{\"end\":29618,\"start\":29613},{\"end\":29631,\"start\":29622},{\"end\":29644,\"start\":29635},{\"end\":29949,\"start\":29944},{\"end\":29957,\"start\":29953},{\"end\":29967,\"start\":29961},{\"end\":29977,\"start\":29971},{\"end\":30317,\"start\":30314},{\"end\":30326,\"start\":30321},{\"end\":30334,\"start\":30330},{\"end\":30345,\"start\":30338},{\"end\":30355,\"start\":30349},{\"end\":30366,\"start\":30359},{\"end\":30373,\"start\":30370},{\"end\":30383,\"start\":30377},{\"end\":31122,\"start\":31115},{\"end\":31134,\"start\":31126},{\"end\":31416,\"start\":31414},{\"end\":31425,\"start\":31420},{\"end\":31435,\"start\":31429},{\"end\":31448,\"start\":31439},{\"end\":31635,\"start\":31624},{\"end\":31646,\"start\":31641},{\"end\":31659,\"start\":31650},{\"end\":31669,\"start\":31663},{\"end\":31679,\"start\":31673},{\"end\":31692,\"start\":31683},{\"end\":32034,\"start\":32029},{\"end\":32045,\"start\":32038},{\"end\":32336,\"start\":32331},{\"end\":32347,\"start\":32340},{\"end\":32692,\"start\":32687},{\"end\":32702,\"start\":32696},{\"end\":32710,\"start\":32706},{\"end\":32720,\"start\":32714},{\"end\":32732,\"start\":32726},{\"end\":33137,\"start\":33135},{\"end\":33143,\"start\":33141},{\"end\":33155,\"start\":33150},{\"end\":33165,\"start\":33159},{\"end\":33177,\"start\":33171},{\"end\":33597,\"start\":33592},{\"end\":33606,\"start\":33601},{\"end\":33614,\"start\":33610},{\"end\":33624,\"start\":33618},{\"end\":33984,\"start\":33979},{\"end\":33993,\"start\":33988},{\"end\":34003,\"start\":33997},{\"end\":34380,\"start\":34375},{\"end\":34395,\"start\":34388},{\"end\":34407,\"start\":34403},{\"end\":34419,\"start\":34413},{\"end\":34797,\"start\":34792},{\"end\":34808,\"start\":34801},{\"end\":34819,\"start\":34812},{\"end\":35006,\"start\":34999},{\"end\":35015,\"start\":35010},{\"end\":35027,\"start\":35019},{\"end\":35037,\"start\":35031},{\"end\":35364,\"start\":35359},{\"end\":35374,\"start\":35368},{\"end\":35383,\"start\":35378},{\"end\":35390,\"start\":35387},{\"end\":35404,\"start\":35397},{\"end\":35414,\"start\":35408},{\"end\":35718,\"start\":35713},{\"end\":35728,\"start\":35722},{\"end\":35996,\"start\":35993},{\"end\":36005,\"start\":36000},{\"end\":36191,\"start\":36186},{\"end\":36203,\"start\":36195},{\"end\":36587,\"start\":36585},{\"end\":36595,\"start\":36593},{\"end\":36605,\"start\":36599},{\"end\":36966,\"start\":36960},{\"end\":36977,\"start\":36970},{\"end\":36988,\"start\":36981},{\"end\":36998,\"start\":36992}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":29026,\"start\":28924},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9812826},\"end\":29496,\"start\":29028},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":182118830},\"end\":29870,\"start\":29498},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":21351739},\"end\":30237,\"start\":29872},{\"attributes\":{\"doi\":\"10.1145/3394885.3431553\",\"id\":\"b4\",\"matched_paper_id\":231730928},\"end\":31022,\"start\":30239},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1690456},\"end\":31356,\"start\":31024},{\"attributes\":{\"doi\":\"arXiv:1804.01756\",\"id\":\"b6\"},\"end\":31620,\"start\":31358},{\"attributes\":{\"doi\":\"arXiv:1906.01548\",\"id\":\"b7\"},\"end\":31921,\"start\":31622},{\"attributes\":{\"doi\":\"arXiv:1503.07469\",\"id\":\"b8\"},\"end\":32248,\"start\":31923},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":201900134},\"end\":32636,\"start\":32250},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1677864},\"end\":33015,\"start\":32638},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3869844},\"end\":33511,\"start\":33017},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":49301394},\"end\":33925,\"start\":33513},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":53235957},\"end\":34293,\"start\":33927},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":201900134},\"end\":34744,\"start\":34295},{\"attributes\":{\"id\":\"b15\"},\"end\":34910,\"start\":34746},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":67872077},\"end\":35271,\"start\":34912},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":203651142},\"end\":35616,\"start\":35273},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":163164623},\"end\":35956,\"start\":35618},{\"attributes\":{\"id\":\"b19\"},\"end\":36119,\"start\":35958},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":10337279},\"end\":36431,\"start\":36121},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":25209638},\"end\":36851,\"start\":36433},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":220871747},\"end\":37332,\"start\":36853}]", "bib_title": "[{\"end\":29115,\"start\":29028},{\"end\":29596,\"start\":29498},{\"end\":29940,\"start\":29872},{\"end\":30310,\"start\":30239},{\"end\":31111,\"start\":31024},{\"end\":32327,\"start\":32250},{\"end\":32683,\"start\":32638},{\"end\":33129,\"start\":33017},{\"end\":33588,\"start\":33513},{\"end\":33975,\"start\":33927},{\"end\":34371,\"start\":34295},{\"end\":34995,\"start\":34912},{\"end\":35355,\"start\":35273},{\"end\":35709,\"start\":35618},{\"end\":36182,\"start\":36121},{\"end\":36581,\"start\":36433},{\"end\":36956,\"start\":36853}]", "bib_author": "[{\"end\":28935,\"start\":28924},{\"end\":29127,\"start\":29117},{\"end\":29138,\"start\":29127},{\"end\":29150,\"start\":29138},{\"end\":29611,\"start\":29598},{\"end\":29620,\"start\":29611},{\"end\":29633,\"start\":29620},{\"end\":29646,\"start\":29633},{\"end\":29951,\"start\":29942},{\"end\":29959,\"start\":29951},{\"end\":29969,\"start\":29959},{\"end\":29979,\"start\":29969},{\"end\":30319,\"start\":30312},{\"end\":30328,\"start\":30319},{\"end\":30336,\"start\":30328},{\"end\":30347,\"start\":30336},{\"end\":30357,\"start\":30347},{\"end\":30368,\"start\":30357},{\"end\":30375,\"start\":30368},{\"end\":30385,\"start\":30375},{\"end\":31124,\"start\":31113},{\"end\":31136,\"start\":31124},{\"end\":31418,\"start\":31412},{\"end\":31427,\"start\":31418},{\"end\":31437,\"start\":31427},{\"end\":31450,\"start\":31437},{\"end\":31637,\"start\":31622},{\"end\":31648,\"start\":31637},{\"end\":31661,\"start\":31648},{\"end\":31671,\"start\":31661},{\"end\":31681,\"start\":31671},{\"end\":31694,\"start\":31681},{\"end\":32036,\"start\":32027},{\"end\":32047,\"start\":32036},{\"end\":32338,\"start\":32329},{\"end\":32349,\"start\":32338},{\"end\":32694,\"start\":32685},{\"end\":32704,\"start\":32694},{\"end\":32712,\"start\":32704},{\"end\":32722,\"start\":32712},{\"end\":32734,\"start\":32722},{\"end\":33139,\"start\":33131},{\"end\":33145,\"start\":33139},{\"end\":33157,\"start\":33145},{\"end\":33167,\"start\":33157},{\"end\":33179,\"start\":33167},{\"end\":33599,\"start\":33590},{\"end\":33608,\"start\":33599},{\"end\":33616,\"start\":33608},{\"end\":33626,\"start\":33616},{\"end\":33986,\"start\":33977},{\"end\":33995,\"start\":33986},{\"end\":34005,\"start\":33995},{\"end\":34382,\"start\":34373},{\"end\":34397,\"start\":34382},{\"end\":34409,\"start\":34397},{\"end\":34421,\"start\":34409},{\"end\":34799,\"start\":34790},{\"end\":34810,\"start\":34799},{\"end\":34821,\"start\":34810},{\"end\":35008,\"start\":34997},{\"end\":35017,\"start\":35008},{\"end\":35029,\"start\":35017},{\"end\":35039,\"start\":35029},{\"end\":35366,\"start\":35357},{\"end\":35376,\"start\":35366},{\"end\":35385,\"start\":35376},{\"end\":35392,\"start\":35385},{\"end\":35406,\"start\":35392},{\"end\":35416,\"start\":35406},{\"end\":35720,\"start\":35711},{\"end\":35730,\"start\":35720},{\"end\":35998,\"start\":35991},{\"end\":36007,\"start\":35998},{\"end\":36193,\"start\":36184},{\"end\":36205,\"start\":36193},{\"end\":36589,\"start\":36583},{\"end\":36597,\"start\":36589},{\"end\":36607,\"start\":36597},{\"end\":36968,\"start\":36958},{\"end\":36979,\"start\":36968},{\"end\":36990,\"start\":36979},{\"end\":37000,\"start\":36990}]", "bib_venue": "[{\"end\":28960,\"start\":28935},{\"end\":29209,\"start\":29150},{\"end\":29662,\"start\":29646},{\"end\":30033,\"start\":29979},{\"end\":30500,\"start\":30408},{\"end\":31166,\"start\":31136},{\"end\":31410,\"start\":31358},{\"end\":31744,\"start\":31710},{\"end\":32025,\"start\":31923},{\"end\":32417,\"start\":32349},{\"end\":32811,\"start\":32734},{\"end\":33237,\"start\":33179},{\"end\":33685,\"start\":33626},{\"end\":34073,\"start\":34005},{\"end\":34489,\"start\":34421},{\"end\":34788,\"start\":34746},{\"end\":35071,\"start\":35039},{\"end\":35427,\"start\":35416},{\"end\":35770,\"start\":35730},{\"end\":35989,\"start\":35958},{\"end\":36260,\"start\":36205},{\"end\":36616,\"start\":36607},{\"end\":37071,\"start\":37000},{\"end\":30596,\"start\":30502},{\"end\":33731,\"start\":33687},{\"end\":34128,\"start\":34075}]"}}}, "year": 2023, "month": 12, "day": 17}