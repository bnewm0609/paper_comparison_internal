{"id": 231855665, "updated": "2023-10-06 06:05:34.081", "metadata": {"title": "SLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks", "authors": "[{\"first\":\"Bahare\",\"last\":\"Fatemi\",\"middle\":[]},{\"first\":\"Layla\",\"last\":\"Asri\",\"middle\":[\"El\"]},{\"first\":\"Seyed\",\"last\":\"Kazemi\",\"middle\":[\"Mehran\"]}]", "venue": "NeurIPS", "journal": "22667-22681", "publication_date": {"year": 2021, "month": 2, "day": 9}, "abstract": "Graph neural networks (GNNs) work well when the graph structure is provided. However, this structure may not always be available in real-world applications. One solution to this problem is to infer a task-specific latent structure and then apply a GNN to the inferred graph. Unfortunately, the space of possible graph structures grows super-exponentially with the number of nodes and so the task-specific supervision may be insufficient for learning both the structure and the GNN parameters. In this work, we propose the Simultaneous Learning of Adjacency and GNN Parameters with Self-supervision, or SLAPS, a method that provides more supervision for inferring a graph structure through self-supervision. A comprehensive experimental study demonstrates that SLAPS scales to large graphs with hundreds of thousands of nodes and outperforms several models that have been proposed to learn a task-specific graph structure on established benchmarks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2102.05034", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/FatemiAK21", "doi": null}}, "content": {"source": {"pdf_hash": "4dc3c61426a3332238ea0feb23f2113a96aef0d4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2102.05034v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "0a3d1a39246e188562b3ff55315d60464a3aaa65", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4dc3c61426a3332238ea0feb23f2113a96aef0d4.txt", "contents": "\nSLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks\n\n\nBahare Fatemi bfatemi@cs.ubc.ca \nGoogle Research\nUniversity of British Columbia\nBorealisAI\n\nLayla El Asri layla.elasri@borealisai.com \nGoogle Research\nUniversity of British Columbia\nBorealisAI\n\nSeyed Mehran Kazemi mehrankazemi@google.com \nGoogle Research\nUniversity of British Columbia\nBorealisAI\n\nSLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks\n\nGraph neural networks (GNNs) work well when the graph structure is provided. However, this structure may not always be available in real-world applications. One solution to this problem is to infer a task-specific latent structure and then apply a GNN to the inferred graph. Unfortunately, the space of possible graph structures grows super-exponentially with the number of nodes and so the taskspecific supervision may be insufficient for learning both the structure and the GNN parameters. In this work, we propose the Simultaneous Learning of Adjacency and GNN Parameters with Self-supervision, or SLAPS, a method that provides more supervision for inferring a graph structure through self-supervision. A comprehensive experimental study demonstrates that SLAPS scales to large graphs with hundreds of thousands of nodes and outperforms several models that have been proposed to learn a task-specific graph structure on established benchmarks.\n\nIntroduction\n\nGraph representation learning has grown rapidly and found applications in domains where a natural graph of the data points is available [4,26]. Graph neural networks (GNNs) [45] have been a key component to the success of the research in this area. Specifically, GNNs have shown promising results for semi-supervised classification when the available graph structure exhibits a high degree of homophily (i.e. connected nodes often belong to the same class) [63].\n\nWe study the applicability of GNNs to (semi-supervised) classification problems where a graph structure is not readily available. The existing approaches for this problem either fix a similarity graph between the nodes or learn the GNN parameters and a graph structure simultaneously (see Related Work). In both cases, one main goal is to construct or learn a graph structure with a high degree of homophily with respect to the labels to aid the GNN classification. The latter approach is sometimes called latent graph learning and often results in higher predictive performance compared to the former approach (see, e.g., [13]).\n\nWe identify a supervision starvation problem in latent graph learning approaches in which the edges between pairs of nodes that are far from labeled nodes receive insufficient supervision; this results in learning poor structures away from labeled nodes and hence poor generalization. We propose a solution for this problem by adopting a multi-task learning framework in which we supplement the classification task with a self-supervised task. The self-supervised task is based on the hypothesis that a graph structure that is suitable for predicting the node features is also suitable for predicting the node labels. It works by masking some input features (or adding noise to them) and training a separate GNN aiming at updating the adjacency matrix in such a way that it can recover the masked (or noisy) features. The task is generic and can be combined with several existing latent graph learning approaches.\n\nWe develop a latent graph learning model, dubbed SLAPS, that adopts the proposed self-supervised task. We provide a comprehensive experimental study on nine datasets (thirteen variations) of various sizes and from various domains and perform thorough analyses to show the merit of SLAPS.\n\nOur main contributions include: 1) identifying a supervision starvation problem for latent graph learning, 2) proposing a solution for the identified problem through self-supervision, 3) developing SLAPS, a latent graph learning model that adopts the self-supervised solution, 4) providing comprehensive experimental results showing SLAPS substantially outperforms existing latent graph learning baselines from various categories on various benchmarks, and 5) providing an implementation for latent graph learning that scales to graphs with hundreds of thousands of nodes.\n\n\nRelated work\n\nExisting methods that relate to this work can be grouped into the following categories. We discuss selected work from each category and refer the reader to [67] for a full survey.\n\nSimilarity graph: One approach for inferring a graph structure is to select a similarity metric and set the edge weight between two nodes to be their similarity [44,49,3]. To obtain a sparse structure, one may create a kNN similarity graph, only connect pairs of nodes whose similarity surpasses some predefined threshold, or do sampling. As an example, in [15] a (fixed) kNN graph using the cosine similarity of the node features is created. In [53], this idea is extended by creating a fresh graph in each layer of the GNN based on the node embedding similarities in that layer. Instead of choosing a single similarity metric, in [16] several (potentially weak) measures of similarity are fused. The quality of the predictions of these methods depends heavily on the choice of the similarity metric(s).\n\nFully connected graph: Another approach is to start with a fully connected graph and assign edge weights using the available meta-data or employ the GNN variants that provide weights for each edge via an attention mechanism [50,59]. This approach has been used in computer vision [e.g., 48], natural language processing [e.g., 62], and few-shot learning [e.g., 14]. The complexity of this approach grows rapidly making it applicable only to small-sized graphs. Zhang et al. [60] propose to define local neighborhoods for each node and only assume that these local neighborhoods are fully connected. Their approach relies on an initial graph structure to define the local neighborhoods.\n\nLatent graph learning: Instead of a similarity graph based on the initial features, one may use a graph generator with learnable parameters. In [34], a fully connected graph is created based on a bilinear similarity function with learnable parameters. In [13], a Bernoulli distribution is learned for each possible edge and graph structures are created through sampling from these distributions. In [55], the input structure is updated to increase homophily based on the labels and model predictions. In [6], an iterative approach is proposed that iterates over projecting the nodes to a latent space and constructing an adjacency matrix from the latent representations multiple times. A common approach in this category is to learn a projection of the nodes to a latent space where node similarities correspond to edge weights or edge probabilities. In [54], the nodes are projected to a latent space by learning weights for each of the input features. In [43,22,9], a multi-layer perceptron is used for projection. In [58,61], a GNN is used for projection; it uses the node features and an initial graph structure. In [27], different graph structures are created in different layers by using separate GNN projectors, where the input to the GNN projector in a layer is the projected values and the generated graph structure from the previous layer. In our experiments, we compare with several approaches from this category.\n\nLeveraging domain knowledge: In some applications, one may leverage domain knowledge to guide the model toward learning specific structures. For example, in [25], abstract syntax trees and regular languages are leveraged in learning graph structures of Python programs that aid reasoning for downstream tasks. In [24], the structure learning is guided for robustness to adversarial attacks through the domain knowledge that clean adjacency matrices are often sparse and low-rank and exhibit feature smoothness along the connected nodes. Other examples in this category include [20,43]. In our paper, we experiment with general-purpose datasets without access to domain knowledge.\n\nProposed method: Our model falls within the latent graph learning category. We supplement the training with a self-supervised objective to increase the amount of supervision in learning a structure. Our self-supervised task is inspired by, and similar to, the pre-training strategies for GNNs [18,19,23,57,64] (specifically, we adopt the multi-task learning framework of You et al. [57]), but it differs from this line of work as we use self-supervision for learning a graph structure whereas the above methods use it to learn better (and, in some cases, transferable) GNN parameters.\n\n\nBackground and notation\n\nWe use lowercase letters to denote scalars, bold lowercase letters to denote vectors and bold uppercase letters to denote matrices. I represents an identity matrix. For a vector v, we represent its i th element as v i . For a matrix M , we represent the i th row as M i and the element at the i th row and j th column as M ij . For an attributed graph, we use n, m and f to represent the number of nodes, edges, and features respectively, and denote the graph as G = {V, A, X} where V = {v 1 , . . . , v n } is a set of nodes, A \u2208 R n\u00d7n is an adjacency matrix with A ij indicating the weight of the edge from v i to v j (A ij = 0 implies no edge), and X \u2208 R n\u00d7f is a matrix whose rows correspond to node features.\n\nGraph convolutional networks (GCNs) [29] are a powerful variant of GNNs. For a graph G = {V, A, X} with a degree matrix D, layer l of the GCN architecture can be defined as H (l) = \u03c3(\u00c2H (l\u22121) W (l) ) where\u00c2 represents a normalized adjacency matrix, H (l\u22121) \u2208 R n\u00d7d l\u22121 represents the node representations in layer l-1 (H (0) = X), W (l) \u2208 R d l\u22121 \u00d7d l is a weight matrix, \u03c3 is an activation function such as ReLU [38], and H (l) \u2208 R n\u00d7d l is the updated node embeddings. For undirected graphs where the adjacency is symmetric,\u00c2 = D \u2212 1 2 (A + I)D \u2212 1 2 corresponds to a row-and-column normalized adjacency with self-loops, and for directed graphs where the adjacency is not necessarily symmetric,\u00c2 = D \u22121 (A + I) corresponds to a row normalized adjacency matrix with self-loops. Here, D is a (diagonal) degree matrix for (A + I) defined as D ii = 1 + j A ij .\n\n\nProposed method: SLAPS\n\nSLAPS consists of four components: 1) generator, 2) adjacency processor, 3) classifier, and 4) self-supervision. Figure 1 illustrates these components. In the next three subsections, we explain the first three components. Then, we point out a supervision starvation problem for a model based only on these components. Then we describe the self-supervision component as a solution to the supervision starvation problem and the full SLAPS model.\n\n\nGenerator\n\nThe generator is a function G : R n\u00d7f \u2192 R n\u00d7n with parameters \u03b8 G which takes the node features X \u2208 R n\u00d7f as input and produces a matrix\u00c3 \u2208 R n\u00d7n as output. We consider the following two generators and leave experimenting with more sophisticated graph generators (e.g., [56,36,35]) and models with tractable adjacency computations (e.g., [7]) as future work.\n\nFull parameterization (FP): For this generator, \u03b8 G \u2208 R n\u00d7n and the generator function is defined as\u00c3 = G F P (X; \u03b8 G ) = \u03b8 G . That is, the generator ignores the input node features and directly optimizes the adjacency matrix. FP is similar to the generator in [13] except that they treat each element of\u00c3 as the parameter of a Bernoulli distribution and sample graph structures from these distributions. FP is simple and flexible for learning any adjacency matrix but adds n 2 parameters which limits scalability and makes the model susceptible to overfitting.\n\n\nMLP-kNN:\n\nHere, \u03b8 G corresponds to the weights of a multi-layer perceptron (MLP) and\u00c3 = G MLP (X; \u03b8 G ) = kNN(MLP(X)), where MLP : R n\u00d7f \u2192 R n\u00d7f is an MLP that produces a matrix with updated node representations X ; kNN : R n\u00d7f \u2192 R n\u00d7n produces a sparse matrix. The implementation details for the kNN operation is provided in the supplementary material.\n\nInitialization and variants of MLP-kNN: Let A kN N represent an adjacency matrix created by applying a kNN function on the initial node features. One smart initialization for \u03b8 G is to initialize it in a way that the generator initially generates A kN N (i.e.\u00c3 = A kN N before training starts). This can be trivially done for the FP generator by initializing \u03b8 G to A kN N . For MLP-kNN, we consider two variants. In one, hereafter referred to simply as MLP, we keep the input dimension the same throughout the layers. In the other, hereafter referred to as MLP-D, we consider MLPs with diagonal weight matrices (i.e., except the main diagonal, all other parameters in the weight matrices are zero). For both variants, we initialize the weight matrices in \u03b8 G with the identity matrix to ensure that the output of the MLP is initially the same as its input and the kNN graph created on these outputs is equivalent to A kN N (Alternatively, one may use other MLP variants but pre-train the weights to output A kN N before the main training starts.). MLP-D can be thought of as assigning different weights to different features and then computing node similarities.\n\n\nAdjacency processor\n\nThe output\u00c3 of the generator may have both positive and negative values, may be non-symmetric and non-normalized. We let A = 1 2 D \u2212 1 2 (P(\u00c3) + P(\u00c3) T )D \u2212 1 2 . Here P is a function with a non-negative range applied element-wise on its input -see supplementary material for details. The sub-expression 1 2 (P(\u00c3) + P(\u00c3) T ) makes the resulting matrix P(\u00c3) symmetric. To understand the reason for taking the mean of P(\u00c3) and P(\u00c3) T , assume\u00c3 is generated by G MLP . If v j is among the k most similar nodes to v i and vice versa, then the strength of the connection between v i and v j will remain the same. However, if, say, v j is among the k most similar nodes to v i but v i is not among the top k for v j , then taking the average of the similarities reduces the strength of the connection between v i and v j . Finally, once we have a symmetric adjacency with non-negative values, we normalize \n\n\nClassifier\n\nThe classifier is a function GNN C : R n\u00d7f \u00d7 R n\u00d7n \u2192 R n\u00d7|C| with parameters \u03b8 GNN C . It takes the node features X and the generated adjacency A as input and provides for each node the logits for each class. C corresponds to the classes and |C| corresponds to the number of classes. We use a twolayer GCN for which \u03b8 GNN C = {W (1) , W (2) } and define our classifier as GNN C (A, X; \u03b8 GNN C ) = AReLU(AXW (1) )W (2) but other GNN variants can be used as well (recall that A is normalized). The training loss L C for the classification task is computed by taking the softmax of the logits to produce a probability distribution for each node and then computing the cross-entropy loss.\n\n\nUsing only the first three components leads to supervision starvation\n\nOne may create a model using only the three components described so far corresponding to the top part of Figure 1. As we will explain here, however, this model may suffer severely from supervision starvation. The same problem also applies to many existing approaches for latent graph learning, as they can be formulated as a combination of variants of these three components.\n\nConsider a scenario during training where two unlabeled nodes v i and v j are not directly connected to any labeled nodes according to the generated structure. Then, since a two-layer GCN makes predictions for the nodes based on their two-hop neighbors, the classification loss (i.e. L C ) is not affected by the edge between v i and v j and this edge receives no supervision 2 . Figure 2 provides an example of such a scenario. Let us call the edges that do not affect the loss function L C (and consequently do not receive supervision) as starved edges. These edges are problematic because although they may not affect the training loss, the predictions at the test time depend on these edges and if their values are learned without enough supervision, the model may make poor predictions at the test time. A natural question concerning the extent of the problem caused by such edges is the proportion of starved edges. The following theorem formally establishes the extent of the problem for Erd\u0151s-R\u00e9nyi graphs [11]; in the supplementary, we extend this result to the Barab\u00e1si-Albert model [1] and scale-free networks [2]. An Erd\u0151s-R\u00e9nyi graph with n nodes and m edges is a graph chosen uniformly at random from the collection of all graphs which have n nodes and m edges.\n\nTheorem 1 Let G(n, m) be an Erd\u0151s-R\u00e9nyi graph with n nodes and m edges. Assume we have labels for q nodes selected uniformly at random. The probability of an edge being a starved edge with a two-layer GCN is equal to (\n1 \u2212 q n )(1 \u2212 q n\u22121 ) 2q i=1 (1 \u2212 m\u22121 ( n 2 )\u2212i ).\nWe defer the proof to the supplementary material. To put the numbers from the theorem in perspective, let us consider three established benchmarks for semi-supervised node classification namely Cora, Citeseer, and Pubmed (the statistics for these datasets can be found in the Appendix). For an Erd\u0151s-R\u00e9nyi graph with similar statistics as the Cora dataset (n = 2708, m = 5429, q = 140), the probability of an edge being a starved edge is 59.4% according to the above theorem. For Citeseer and Pubmed, this number is 75.7% and 96.7% respectively. While Theorem 1 is stated for Erd\u0151s-R\u00e9nyi graphs, the identified problem also applies to natural graphs. For the original structures of Cora, Citeseer, and Pubmed, for example, 48.8%, 65.2%, and 91.6% of the edges are starved edges.\n\n\nSelf-supervision\n\nOne possible solution to the supervision starvation problem is to define a prior graph structure and regularize the learned structure toward it. This leads the starved edges toward the prior structure as opposed to neglecting them. The choice of the prior is important as it determines the inductive bias incorporated into the model. We define a prior structure based on the following hypothesis:\n\nHypothesis 1 A graph structure that is suitable for predicting the node features is also suitable for predicting the node labels.\n\nWe first explain why the above hypothesis is reasonable for an extreme case that is easy to understand and then extend the explanation to the general case. Consider an extreme scenario where one of the node features is the same as the node labels. A graph structure that is suitable for predicting this feature exhibits homophily for it. Because of the equivalence between this feature and the labels, the graph structure also exhibits homophily for the labels, so it is also suitable for predicting the labels.\n\nIn the general (non-extreme) case, there may not be a single feature that is equivalent to the labels but a subset of the features may be highly predictive of the labels. A graph structure that is suitable for predicting this subset exhibits homophily for the features in the subset. Because this subset is highly predictive of the labels, the structure also exhibits a high degree of homophily for the labels, so it is also suitable for predicting the node labels.\n\nNext, we explain how to design a suitable graph structure for predicting the features and how to regularize toward it. One could design such a structure manually (e.g., by handcrafting a graph that connects nodes based on the collective homophily between their individual features) and then penalize the difference between this prior graph and the learned graph. Alternatively, in this paper, we take a learning-based approach based on self-supervision where we not only use the learned graph structure for the classification task, but also for denoising the node features. The self-supervised task encourages the model to learn a structure that is suitable for predicting the node features. We describe this approach below and provide comparisons to the manual approach in the supplementary material.\n\nOur self-supervised task is based on denoising autoencoders [51]. Let GNN DAE : R n\u00d7f \u00d7 R n\u00d7n \u2192 R n\u00d7f be a GNN with parameters \u03b8 GNN DAE that takes node features and a generated adjacency as input and provides updated node features with the same dimension as output. We train GNN DAE such that it receives a noisy versionX of the features X as input and produces the denoised features X as output. Let idx represent the indices corresponding to the elements of X to which we have added noise, and X idx represent the values at these indices. During training, we minimize:\nL DAE = L(X idx , GNN DAE (X, A; \u03b8 GNN DAE ) idx )(1)\nwhere A is the generated adjacency matrix and L is a loss function. For datasets where the features consist of binary vectors, idx consists of r percent of the indices of X whose values are 1 and r\u03b7 percent of the indices whose values are 0, both selected uniformly at random in each epoch. Both r and \u03b7 (corresponding to the negative ratio) are hyperparameters. In this case, we add noise by setting the 1s in the selected mask to 0s and L is the binary cross-entropy loss. For datasets where the input features are continuous numbers, idx consists of r percent of the indices of X selected uniformly at random in each epoch. We add noise by either replacing the values at idx with 0 or by adding independent Gaussian noises to each of the features. In this case, L is the mean-squared error loss.\n\nNote that the self-supervised task in equation 1 is generic and can be added to different GNNs as well as latent graph learning models. It can be also combined with other techniques in the literature that encourage learning more homophilous structures or increase the amount of supervision. In our experiments, we test the combination of our self-supervised task with two such techniques namely self-training [33] and AdaEdge [5]. Self-training helps the model \"see\" more labeled nodes and AdaEdge helps iteratively create graph structure with higher degrees of homophily. We refer the reader to the supplementary material for descriptions of self-training and AdaEdge.\n\n\nSLAPS\n\nOur final model is trained to minimize L = L C + \u03bbL DAE where L C is the classification loss, L DAE is the denoising autoencoder loss (see Equation 1), and \u03bb is a hyperparameter controlling the relative importance of the two losses.\n\n\nExperiments\n\nIn this section, we report our key results. More empirical comparisons, experimental analyses, and ablation studies are presented in the supplementary material.\n\nBaselines: We compare our proposal to several baselines with different properties. The first baseline is a multi-layer perceptron (MLP) which does not take the graph structure into account. We also compare against MLP-GAM* [47] which learns a fully connected graph structure and uses this structure to supplement the loss function of the MLP toward predicting similar labels for neighboring nodes. Our third baseline is label propagation (LP) [65], a well-known model for semi-supervised learning. Similar to [13], we also consider a baseline named kNN-GCN where we create a kNN graph based on the node feature similarities and feed this graph to a GCN; the graph structure remains fixed in this approach. We also compare with prominent existing latent graph learning models including LDS [13], GRCN [58], DGCNN [53], and IDGL [6]. In [6], another variant named IDGL-ANCH is also proposed that reduces time complexity through anchor-based approximation [37]. We compare against the base IDGL model because it does not sacrifice accuracy for time complexity, and because anchor-based approximation is model-agnostic and could be combined with other models too. We Table 1: Results of SLAPS and the baselines on established node classification benchmarks. \u2020 indicates results have been taken from Franceschi et al. [13]. \u2021 indicates results have been taken from Stretcu et al. [47]. Bold and underlined values indicate best and second-best mean performances respectively. OOM indicates out of memory. OOT indicates out of time (we allowed 24h for each run). NA indicates not applicable. feed a kNN graph to the models requiring an initial graph structure. We also explore how adding self-training and AdaEdge impact the performance of kNN-GCN as well as SLAPS.\nModel Cora Citeseer Cora390 Citeseer370 Pubmed ogbn-arxiv MLP 56.1 \u00b1 1.6 \u2020 56.7 \u00b1 1.7 \u2020 65.8 \u00b1 0.4 67.1 \u00b1 0.5 71.4 \u00b1 0.0 54.7 \u00b1 0.1 MLP-GAM* 70.7 \u2021 70.3 \u2021 \u2212 \u2212 71.9 \u2021 \u2212 LP 37.6 \u00b1 0.0 23.2 \u00b1 0.0 36.2 \u00b1 0.0 29.1 \u00b1 0.0 41.3 \u00b1 0.0 OOM kNN-GCN 66.5 \u00b1 0.4 \u2020 68.3 \u00b1 1.3 \u2020 72.5 \u00b1 0.5 71.8 \u00b1 0.8 70.4 \u00b1 0.4 49.1 \u00b1 0.3 LDS \u2212 \u2212 71.5 \u00b1 0.8 \u2020 71.5 \u00b1 1.1 \u2020 OOM OOM GRCN 67.4 \u00b1 0.3 67.3 \u00b1 0.\nDatasets: We use three established benchmarks in the GNN literature namely Cora, Citeseer, and Pubmed [46] as well as the ogbn-arxiv dataset [17] that is orders of magnitude larger than the other three datasets and is more challenging due to the more realistic split of the data into train, validation, and test sets. For these datasets, we only feed the node features to the models and not their original graph structure. Following [13,6], we also experiment with several classification (non-graph) datasets available in scikit-learn [41] including Wine, Cancer, Digits, and 20News. Furthermore, following [21], we also provide results on MNIST [31]. The dataset statistics can be found in the supplementary. For Cora and Citeseer, the LDS model uses the train data for learning the parameters of the classification GCN, half of the validation for learning the parameters of the adjacency matrix (in their bi-level optimization setup, these are considered as hyperparameters), and the other half of the validation set for early stopping and tuning the other hyperparameters. Besides experimenting with the original setups of these two datasets, we also consider a setup that is closer to that of LDS: we use the train set and half of the validation set for training and the other half of validation for early stopping and hyperparameter tuning. We name the modified versions Cora390 and Citeseer370 respectively where the number proceeding the dataset name shows the number of labels from which gradients are computed. We follow a similar procedure for the scikit-learn datasets.\n\nImplementation: We defer the implementation details and the best hyperparameter settings for our model on all the datasets to the supplementary material. Code and data is available at https://github.com/BorealisAI/SLAPS-GNN.\n\n\nComparative results\n\nThe results of SLAPS and the baselines on our benchmarks are reported in Tables 1 and 2. We start by analyzing the results in Table 1 first. Starting with the baselines, we see that learning a fully connected graph in MLP-GAM* makes it outperform MLP. kNN-GCN significantly outperforms MLP on Cora and Citeseer but underperforms on Pubmed and ogbn-arxiv. Furthermore, both self-training and AdaEdge improve the performance of kNN-GCN. This shows the importance of the similarity metric and the graph structure that is fed into GCN; a low-quality structure can harm model performance. LDS outperforms MLP but the fully parameterized adjacency matrix of LDS results in memory issues for Pubmed and ogbn-arxiv. As for GRCN, it was shown in the original paper that GRCN can revise a good initial adjacency matrix and provide a substantial boost in performance. However, as evidenced by the results, if the initial graph structure is somewhat poor, GRCN's performance becomes on par with kNN-GCN. IDGL is the best performing baseline. In addition to the aforementioned baselines, we also experimented with GCN, GAT, and Transformer (encoder only) architectures applied on fully connected graphs. GCN always learned to predict the majority class. This is because after one fully connected GCN layer, all nodes will have the same embedding and become indistinguishable. GAT also showed similar behavior. We believe this is because the attention weights are (almost) random at the beginning (due to random initialization of the model parameters) resulting in nodes becoming indistinguishable and GAT cannot escape from that state. The skip connections of Transformer helped avoid the problem observed for GCN and GAT and we were able to achieve better results (\u223c 40% accuracy on Cora). However, we observed severe overfitting (even with very small models and with high dropout probabilities).\n\nSLAPS consistently outperforms the baselines in some cases by large margins. Among the generators, the winner is dataset-dependent with MLP-D mostly outperforming MLP on datasets with many features and MLP outperforming on datasets with small numbers of features. Using the software that was publicly released by the authors, the baselines that learn a graph structure fail on ogbn-arxiv 3 ; our implementation, on the other hand, scales to such large graphs. Adding self-training helps further improve the results of SLAPS. Adding AdaEdge, however, does not seem effective, probably because the graph structure learned by SLAPS already exhibits a high degree of homophily (see Section 5.4).\n\nIn Table 2, we only compared SLAPS with the best performing baselines from Table 1 (kNN-GCN, LDS and IDGL). We also included an MLP baseline for comparison. On three out of four datasets, SLAPS outperforms the LDS and IDGL baselines. For the Digits dataset, interestingly kNN-GCN outperforms the learning-based models. This could be because the initial kNN structure for this dataset is already a good structure. Among the datasets on which we can train SLAPS with the FP generator, 20news has the largest number of nodes (9,607 nodes). On this dataset, we observed that an FP generator suffers from overfitting and produces weaker results compared to other generators due to its large number of parameters.\n\nJiang et al. [22] show that learning a latent graph structure of the input examples can help with Here, we conduct an experiment to measure the performance of SLAPS on these variants of the MNIST dataset. We compare against GLCN [22] as well as the baselines in the GLCN paper including manifold regularization [3], label propagation, deep walk [42], graph convolutional networks (GCN), and graph attention networks (GAT).\n\nThe results are reported in Table 3. From the results, it can be viewed that SLAPS outperforms GLCN and all the other baselines on the 3 variants. Compared to GLCN, on the three variants SLAPS reduces the error by 7%, 5%, and 2% respectively, showing that SLAPS can be more effective when the labeled set is small and providing more empirical evidence for Theorem 1.  Learning a structure only using self-supervision: To provide more insight into the value provided by the self-supervision task and the generalizability of the adjacency learned through this task, we conduct experiments with a variant of SLAPS named SLAP S 2s that is trained in two stages. We first train the GNN DAE model by minimizing L DAE described in in Equation 1. Recall that L DAE depends on the parameters \u03b8 G of the generator and the parameters \u03b8 GNN DAE of the denoising autoencoder. After every t epochs of training, we fix the adjacency matrix, train a classifier with the fixed adjacency matrix, and measure classification accuracy on the validation set. We select the epoch that produces the adjacency providing the best validation accuracy for the classifier. Note that in SLAP S 2s , the adjacency matrix only receives gradients from the self-supervised task in Equation 1. Figure 3 shows the performance of SLAPS and SLAPS 2s on Cora and compares them with kNN-GCN. Although SLAPS 2s does not use the node labels in learning an adjacency matrix, it outperforms kNN-GCN (8.4% improvement when using an FP generator). With an FP generator, SLAPS 2s even achieves competitive performance with SLAPS; this is mainly because FP does not leverage the supervision provided by GCN C toward learning generalizable patterns that can be used for nodes other than those in the training set. These results corroborate the effectiveness of the self-supervision task for learning an adjacency matrix. Besides, the results show that learning the adjacency using both self-supervision and the task-specific node labels results in higher predictive accuracy.  The value of \u03bb: Figure 4 shows the performance of SLAPS 4 on Cora and Citeseer with different values of \u03bb. When \u03bb = 0, corresponding to removing self-supervision, the model performance is somewhat poor. As soon as \u03bb becomes positive, both models see a large boost in performance showing that self-supervision is crucial to the high performance of SLAPS. Increasing \u03bb further provides larger boosts until it becomes so large that the self-supervision loss dominates the classification loss and the performance deteriorates. Note that with \u03bb = 0, SLAPS with the MLP generator becomes a variant of the model proposed in [9], but with a different similarity function.\n\n\nThe effectiveness of self-supervision\n\nIs self-supervision actually solving the supervision starvation problem? In Fig 4, we showed that self-supervision is key to the high performance of SLAPS. Here, we examine if this is because self-supervision indeed addresses the supervision starvation problem. For this purpose, we compared SLAPS with and without self-supervision on two groups of test nodes on Cora: 1) those that are not connected to any labeled nodes after training, and 2) those that are connected to at least one labeled node after training. The nodes in group one have a high chance of having starved edges. We observed that adding self-supervision provides 38.0% improvement for the first group and only 8.9% improvement for the latter. Since self-supervision mainly helps with nodes in group 1, this provides evidence that self-supervision is an effective solution to the supervision starvation problem.\n\nThe effect of the training set size: According to Theorem 1, a smaller q (corresponding to the training set size) results in more starved edges in each epoch. To explore the effect of self-supervision as a function of q, we compared SLAPS with and without supervision on Cora and Citeseer while reducing the number of labeled nodes per class from 20 to 5. We used the FP generator for this experiment. With 5 labeled nodes per class, adding self-supervision provides 16.7% and 22.0% improvements on Cora and Citeseer respectively, which is substantially higher than the corresponding numbers when using 20 labeled nodes per class (10.0% and 7.0% respectively). This provides empirical evidence for Theorem 1. Note that the results on Cora390 and Citeseer 370 datasets provide evidence that the self-supervised task is effective even when the label rate is high. The performance of GNNs highly depends on the quality of the input graph structure and deteriorates when the graph structure is noisy [see 68, 10,12]. Here, we verify whether selfsupervision is also helpful when a noisy structure is provided as input. Toward this goal, we experiment with Cora and Citeseer and provide noisy versions of the input graph as input. The provided noisy graph structure is used only for initialization; it is then further optimized by SLAPS. We perturb the graph structure by replacing \u03c1 percent of the edges in the original structure (selected uniformly at random) with random edges. Figure 5 shows the performance of SLAPS with and without self-supervision (\u03bb = 0 corresponds to no supervision). We also report the results of vanilla GCN on these perturbed graphs for comparison. It can be viewed that self-supervision consistently provides a boost in performance especially for higher values of \u03c1.\n\n\nExperiments with noisy graphs\n\n\nAnalyses of the learned adjacency\n\nNoisy graphs: Following the experiment in Section 5.3, we compared the learned and original structures by measuring the number of random edges added during perturbation but removed by the model and the number of edges removed during the perturbation but recovered by the model. For Cora, SLAPS removed 76.2% and 70.4% of the noisy edges and recovered 58.3% and 44.5% of the removed edges for \u03c1 = 25% and \u03c1 = 50% respectively  Homophily: As explained earlier, a properly learned graph for semi-supervised classification with GNNs exhibits high homophily. To verify the quality of the learned adjacency with respect to homophily, for every pair of nodes in the test set, we compute the odds of the two nodes sharing the same label as a function of the normalized weight of the edge connecting them. Figure 6 represents the odds for different weight intervals (recall that A is row and column normalized). For both Cora and Citeseer, nodes' connected with higher edge weights are more likely to share the same label compared to nodes with lower or zero edge weights. Specifically, when A ij \u2265 0.1, v i and v j are almost 2.5 and 2.0 times more likely to share the same label on Cora and Citeseer respectively.\n\n\nConclusion\n\nWe proposed SLAPS: a model for learning the parameters of a graph neural network and a graph structure of the nodes connectivities simultaneously from data. We identified a supervision starvation problem that emerges for graph structure learning, especially when training data is scarce. We proposed a solution to the supervision starvation problem by supplementing the training objective with a well-motivated self-supervised task. We showed the effectiveness of our model through a comprehensive set of experiments and analyses.\n\n\nFunding Transparency Statement\n\nThis work was fully funded by Borealis AI.\n\n[68] Daniel Z\u00fcgner, Amir Akbarnejad, and Stephan G\u00fcnnemann. Adversarial attacks on neural networks for graph data. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2847-2856, 2018. Importance of k in kNN: Figure 7 shows the performance of SLAPS on Cora for three graph generators as a function of k in kNN. For all three cases, the value of k plays a major role in model performance. The FP generator is the least sensitive because, in FP, k only affects the initialization of the adjacency matrix but then the model can change the number of neighbors of each node. For MLP and MLP-D, however, the number of neighbors of each node remains close to k (but not necessarily equal as the adjacency processor can add or remove some edges) and the two generators become more sensitive to k. For larger values of k, the extra flexibility of the MLP generator enables removing some of the unwanted edges through the function P or reducing the weights of the unwanted edges resulting in MLP being less sensitive to large values of k compared to MLP-D.\n\n\nA More Experiments and Analyses\n\nIncreasing the number of layers: In the main text, we described how some edges may receive no supervision during latent graph learning. We pointed out that while increasing the number of layers of the GCN may alleviate the problem to some extent, deeper GCNs typically provide inferior results due to issues such as oversmoothing [see, e.g., 33,39]. We empirically tested deeper GCNs for latent graph learning to see if simply using more layers can obviate the need for the proposed self-supervision. Specifically, we tested SLAPS without self-supervision (i.e. \u03bb = 0) with 2, 4, and 6 layers on Cora. We also added residual connections that have been shown to help train deeper GCNs [32]. The accuracies for 2, 4, and 6-layer models are 66.2%, 67.1%, and 55.8% respectively. It can be viewed that increasing the number of layers from 2 to 4 provides an improvement. This might be because the benefit provided by a 4-layer model in terms of alleviating the starved edge problem outweighs the increase in oversmoothing. However, when the number of layers increases to 6, the oversmoothing problem outweighs and the performance drops significantly. Further increasing the number of layers resulted in even lower accuracies.\n\nSymmetrization: In the adjacency processor, we used the following equation:  which symmetrized the adjacency matrix by taking the average of P(\u00c3) and P(\u00c3) T . Here we also consider two other choices: 1) max(P(\u00c3), P(\u00c3) T ), and 2) not symmetrizing the adjacency (i.e. using P(\u00c3)). Figure 8 compares these three choices on Cora and Citeseer with an MLP generator (other generators produced similar results). On both datasets, symmetrizing the adjacency provides a performance boost. Compared to mean symmetrization, max symmetrization performs slightly worse. This may be because max symmetrization does not distinguish between the case where both v i and v j are among the k most similar nodes of each other and the case where only one of them is among the k most similar nodes of the other.\nA = D \u2212 1 2 P(\u00c3) + P(\u00c3) T 2 D \u2212\nFixing a prior graph manually instead of using selfsupervision: In the main text, we validated Hypothesis 1 by adding a self-supervised task to encourage learning a graph structure that is appropriate for predicting the node features, and showing in our experiments how this additional task helps improve the results. Here, we provide more evidence for the validity of Hypothesis 1 by showing that we can obtain good results even when regularizing the learned graph structure toward a manually fixed structure that is appropriate for predicting the node features.  Toward this goal, we experimented with Cora and Citeseer and created a cosine similarity graph as our prior graph A prior where the edge weights correspond to the cosine similarity of the nodes. We sparsified A prior by connecting each node only to the k most similar nodes. Then, we added a term \u03bb||A \u2212 A prior || F to the loss function where \u03bb is a hyperparameter, A is the learned graph structure (i.e. the output of the graph generator), and ||.|| F shows the Frobenius norm. Note that A prior exhibits homophily with respect to the node features because the node features in Cora and Citeseer are binary, so two nodes that share the same values for more features have a higher similarity and are more likely to be connected.\n\nThe results can be viewed in Figure 9. According to the results, we can see that regularizing toward a manually designed A prior also provides good results but falls short of SLAPS with self-supervision. The superiority of the self-supervised approach compared to the manual design could be due to two reasons.\n\n\u2022 Some of the node features may be redundant (e.g., they may be derived from other features) or highly correlated. These features can negatively affect the similarity computations for the prior graph in A prior . As an example, consider three nodes with seven binary features [0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0] and [1, 1, 1, 1, 1, 1, 1] respectively and assume the last two features for each node are always equivalent and are computed based on a logical and of the 4th and 5th features 5 . Without these two features, the first node is more similar to the second than the third node, but when considering these derived features, it becomes more similar to the third node. This change in node similarities affects the construction of A prior which can deteriorate the overall performance of the model. The version of SLAPS with the self-supervised task, on the other hand, is not affected by this problem as much because the model can learn to predict the derived node features based on other features and without heavily relying on the graph structure.\n\n\u2022 While many graph structures may be appropriate for predicting the node features, in the manual approach we only regularize toward one particular such structure. Using the selfsupervised task, however, SLAPS can learn any of those structures; ideally, it learns the one that is more suited for the downstream task due to the extra supervision coming from the downstream task.\n\n\nB Implementation Details\n\nWe implemented our model in PyTorch [40], used deep graph library (DGL) [52] for the sparse operations, and used Adam [28] as the optimizer. We performed early stopping and hyperparameter tuning based on the accuracy on the validation set for all datasets except Wine and Cancer. For these two datasets, the validation accuracy reached 100 percent with many hyperparameter settings, making it difficult to select the best set of hyperparameters. Instead, we used the validation cross-entropy loss for these two datasets.\n\nWe fixed the maximum number of epochs to 2000. We use two-layer GCNs for both GNN C and GNN DAE as well as for baselines and two-layer MLPs throughout the paper (for experiments on ogbn-arxiv, although the original paper uses models with three layers and with batch normalization after each layer, to be consistent with our other experiments we used two layers and removed the normalization). We used two learning rates, one for GCN C as lr C and one for the other parameters of the models as lr DAE . We tuned the two learning rates from the set {0.01, 0.001}. We added dropout layers with dropout probabilities of 0.5 after the first layer of the GNNs. We also added dropout to the adjacency matrix for both GNN C and GNN DAE as dropout C dropout DAE respectively and tuned the values from the set {0.25, 0.5}. We set the hidden dimension of GNN C to 32 for all datasets except for ogbn-arxiv for which we set it to 256. We used cosine similarity for building the kNN graphs and tuned the value of k from the set {10, 15, 20, 30}. We tuned \u03bb (\u03bb controls the relative importance of the two losses) from the set {0.1, 1, 10, 100, 500}. We tuned r and \u03b7 from the sets {1, 5, 10} and {1, 5} respectively. The best set of hyperparameters for each dataset chosen on the validation set is in table 4. The code of our experiments will be available upon acceptance of the paper.\n\nFor GRCN [58], DGCNN [53], and IDGL [6], we used the code released by the authors and tuned the hyperparameters as suggested in the original papers. The results of LDS [13] are directly taken from the original paper. For LP [66], we used scikit-learn python package [41].\n\nAll the results for our model and the baselines are averaged over 10 runs. We report the mean and standard deviation. We ran all the experiments on a single GPU (NVIDIA GeForce GTX 1080 Ti).\n\nSelf-training and AdaEdge: We combined SLAPS (and kNN-GCN) with two techniques from the literature namely self-training and AdaEdge. For completeness sake, we provide a brief description of these approaches and refer the reader to the original papers for detailed descriptions.\n\nFor self-training, we first trained a model using the existing labels in the training set. Then we used this model to make predictions for the unlabeled nodes that were not in the train, validation, or test sets. We considered the label predictions for the top \u03b6 most confident unlabeled nodes as ground truth labels and added them to the training labels. Finally, we trained a model from scratch on the expanded set of labels. Here, \u03b6 is a hyperparameter. We tuned its value from the set {50, 100, 200, 300, 400, 500}.\n\nFor AdaEdge, in the case of kNN-GCN, we first trained a kNN-GCN model. Then we changed the structure of the graph from the kNN graph to a new graph by following these steps: 1) add edges between nodes with the same class predictions if both prediction confidences surpass a threshold, 2) remove edge between nodes with different class predictions if both prediction confidences surpass a threshold. Then, we trained a GCN model on the new structure and repeated the aforementioned steps to generate a new structure. We did this iteratively until generating a new structure did not provide a boost in performance on the validation set. For SLAPS, we followed a similar approach except that the initial model was a SLAPS model instead of a kNN-GCN model.\n\nkNN Implementation: For our MLP generator, we used a kNN operation to sparsify the generated graph. Here, we explain how we implemented the kNN operation to avoid blocking the gradient flow. Let M \u2208 R n\u00d7n with M ij = 1 if v j is among the top k similar nodes to v i and 0 otherwise, and let S \u2208 R n\u00d7n with S ij = Sim(X i , X j ) for some differentiable similarity function Sim (we used cosine). Then\u00c3 = kNN(X ) = M S where represents the Hadamard (element-wise) product. With this formulation, in the forward phase of the network, one can first compute the matrix M using an off-the-shelf k-nearest neighbors algorithm and then compute the similarities in S only for pairs of nodes where M ij = 1. In our experiments, we compute exact k-nearest neighbors; one can approximate it using locality-sensitive hashing approaches for larger graphs (see, e.g., [16,30]).\n\nIn the backward phase of our model, we compute the gradients only with respect to those elements in S whose corresponding value in M is 1 (i.e. those elements S ij such that M ij = 1); the gradient with respect to the other elements is 0. Since S is computed based on X , the gradients flow to the elements in X (and consequently to the weights of the MLP) through S.\n\nAdjacency processor: We used a function P in our adjacency processor to make the values of the\u00c3 positive. In our experiments, when using an MLP generator, we let P be the ReLU function applied element-wise on the elements of\u00c3. When using the fully-parameterized (FP) generator, applying ReLU results in a gradient flow problem as any edge whose corresponding value in\u00c3 becomes less than or equal to zero stops receiving gradient updates. For this reason, for FP we apply the ELU [8] function to the elements of\u00c3 and then add a value of 1.\n\n\nC Dataset statistics\n\nThe statistics of the datasets used in the experiments can be found in Table 5.\n\n\nD Supervision starvation in Erd\u0151s-R\u00e9nyi and scale-free networks\n\nWe start by defining some new notation that helps simplify the proofs and analysis in this section. We let l v be a random variable indicating that v is a labeled node, with l v indicating that its negation, c v,u be a random variable indicating that v is connected to u with an edge, with c v,u indicating its negation, and cl v be random variable indicating that v is connected to at least one labeled node with cl v indicating its negation (i.e. it indicates that v is connected to no labeled nodes).\n\nTheorem 2 Let G(n, m) be an Erd\u0151s-R\u00e9nyi graph with n nodes and m edges. Assume we have labels for q nodes selected uniformly at random. The probability of an edge being a starved edge with a two-layer GCN is equal to\n(1 \u2212 q n )(1 \u2212 q n\u22121 ) 2q i=1 (1 \u2212 m\u22121 ( n 2 )\u2212i ).\nProof 1 To compute the probability of an edge being a starved edge, we first compute the probability of the two nodes of the edge being unlabeled themselves and then the probability of the two nodes not being connected to any labeled nodes. Let v and u represent two nodes connected by an edge.\n\nWith n nodes and q labels, the probability of a node being labeled is q n . Therefore, P r(l v ) = (1 \u2212 q n ) and P r(l u | l v ) = (1 \u2212 q n\u22121 ). Therefore, P r(l v \u2227 l u ) = (1 \u2212 q n )(1 \u2212 q n\u22121 ). Since there is an edge between v and v, there are m \u2212 1 edges remaining. Also, there are n 2 \u2212 1 pairs of nodes that can potentially have an edge between them. Therefore, the probability of v being disconnected from the first labeled node is 1 \u2212 m\u22121 ( n 2 )\u22121 .\n\nIf v is disconnected from the first labeled node, there are still m \u2212 1 edges remaining and there are now n 2 \u2212 2 pairs of nodes that can potentially have an edge between them. So the probability of v being disconnected from the second node given that it is disconnected from the first labeled node is 1 \u2212 m\u22121 ( n 2 )\u22122\n\n. With similar reasoning, we can see that the probability of v being disconnected from the i-th labeled node given that it is disconnected from the first i \u2212 1 labeled nodes is 1 \u2212 m\u22121 ( n 2 )\u2212i . We can follow similar reasoning for u. The probability of u being disconnected from the first labeled node given that v is disconnected from all q labeled nodes is 1 \u2212 m\u22121 ( n 2 )\u2212q\u22121 . That is because there are still m \u2212 1 edges remaining and n 2 \u2212 q \u2212 1 pairs of nodes that can potentially be connected with an edge. We can also see that the probability of u being disconnected from the i-th labeled node given that it is disconnected from the first i \u2212 1 labeled nodes and that v is disconnected from all q labeled nodes is 1 \u2212 m\u22121 ( n 2 )\u2212q\u2212i .\n\nAs the probability of the two nodes being unlabeled and not being connected to any labeled nodes in the graph are independent, their joint probability is the multiplication of their probabilities computed above and it is equal to\n(1 \u2212 q n )(1 \u2212 q n\u22121 ) 2q i=1 (1 \u2212 m\u22121 ( n 2 )\u2212i ).\nBarab\u00e1si-Albert and scale-free networks: We also extend the above result for Erd\u0151s-R\u00e9nyi graphs to the Barab\u00e1si-Albert [2] model. Since Barab\u00e1si-Albert graph generation results in scale-free networks with a scale parameter \u03b3 = \u22123, we present results for the general case of scale-free networks as it makes the analysis simpler and more general. In what follows, we compute the probability of an edge being a starved edge in a scale-free network. Let G be a scale-free network with n nodes, q labels (selected uniformly at random), and scale parameter \u03b3. Then, if we select a random edge between two nodes v and u, the probability of the edge between them being a starved edge is:\n\nP r(l v ) * P r(l u |l v ) * P r(cl v |c v,u , l v , l u ) * P r(cl u |c v,u , l v , l u , cl v ).\n\nEach of these terms can be computed as follows ( a b represents the number of combinations of selecting b items from a set with a items):\n\n\u2022 P r(l v ) = (1 \u2212 q n ) \u2022 P r(l u |l v ) = (1 \u2212 q n\u22121 )\n\n\u2022 P r(cl v |c v,u , l u , l v ) = n\u22121 k=1 k \u03b3 ( n\u2212q\u22122 k\u22121 ) ( n\u22122 k\u22121 ) n\u22121 k=1 k \u03b3 For a large enough network, P r(cl u |c v,u , l v , l u , cl v ) can be approximated as P r(cl u |c v,u , l v , l u ) and it can be computed similarly as the previous case.\n\nWith the derivation above, for a scale-free network with n = 2708 and q = 140 (corresponding to the stats from Cora), the probability of an edge being a starved edge for \u03b3 = \u22123 is 0.87 and for \u03b3 = \u22122 is 0.76 . E Why not compare the learned graph structures with the original ones?\n\nA comparison between the learned graph structures using SLAPS (or other baselines) and the original graph structure of the datasets we used may not be sensible. We explain this using an example. Before getting into the example, we remind the reader that the goal of structure learning for semi-supervised classification with graph neural networks is to learn a structure with a high degree of homophily. Following [63], we define the edge homophily ratio as the fraction of edges in the graph that connect nodes that have the same class label. Figure 10 demonstrates an example where two graph structures for the same set of nodes have the same edge homophily ratio (0.8 for both) but have no edges in common. For our task, it is possible that the original graph structure (e.g., the citation graph in Cora) corresponds to the structure on the left but SLAPS (or any other model) learns the graph on the right, or vice versa. While both these structures may be equally good 6 , they do not share any edges. Therefore, measuring the quality of the learned graph using SLAPS by comparing it to the original graph of the datasets may not be sensible. However, if a noisy version of the initial structure is provided as input for SLAPS, then one may expect that SLAPS recovers a structure similar to the cleaned original graph and this is indeed what we demonstrate in the main text. \n\n\nF Limitations\n\nIn this section, we discuss some of the limitations of the proposed model. Firstly, in cases where nodes do not have input features but an initial noisy structure of the nodes is available, our self-supervised task cannot be readily applied. One possible solution is to first run an unsupervised node embedding model such as DeepWalk [42] to obtain node embeddings, then treat these embeddings as node features and run SLAPS. Secondly, the FP graph generator is not applicable in the inductive setting; this is because FP directly optimizes the adjacency matrix. However, our other two graph generators (MLP and MLP-D) can be applied in the inductive setting.\n\nFigure 1 :\n1Overview of SLAPS. At the top, a generator receives the node features and produces a non-symmetric, non-normalized adjacency having (possibly) both positive and negative values (Section 4.1). The adjacency processor makes the values positive, symmetrizes and normalizes the adjacency (Section 4.2). The resulting adjacency and the node features go into GNN C which predicts the node classes (Section 4.3). At the bottom, some noise is added to the node features. The resulting noisy features and the generated adjacency go into GNN DAE which then denoises the features (Section 4.5).\n\n1 2\n1(P(\u00c3) + P(\u00c3) T ) by computing its degree matrix D and multiplying it from left and right to D \u2212 1 2 .\n\nFigure 2 :\n2Using a two-layer GCN, the predictions made for the labeled nodes are not affected by the dashed (starved) edge.\n\nFigure 3 :\n3SLAPS vs SLAPS 2s on Cora with different generators.\n\nFigure 4 :\n4The performance of SLAPS with MLP graph generator as a function of \u03bb.\n\nFigure 5 :\n5Performance comparison when noisy graphs are provided as input (\u03c1 indicates the percentage of perturbations).\n\nFigure 6 :\n6The odds of two nodes in the test set sharing the same label as a function of the edge weights learned by SLAPS.while SLAPS with \u03bb = 0 only removed 62.8% and 54.9% of the noisy edges and recovered 51.4% and 35.8% of the removed edges. This provides evidence on self-supervision being helpful for structure learning.\n\nFigure 7 :\n7The performance of SLAPS on Cora as a function of k in kNN.\n\nFigure 8 :\n8The performance of SLAPS on Cora and Citeseer with different adjacency symmetrizations.\n\nFigure 9 :\n9The performance of SLAPS and regularization toward a manually defined prior structure on Cora and Citeseer when using the MLP generator.\n\nFigure 10 :\n10Two example graph structures. Node colors indicates the class labels. Solid lines indicate homophilous edges and dashed lines indicate non-homophilous edges. The two graphs exhibit the same degree of homophily yet there is not overlap between their edges.\n\n\nSLAPS (MLP) + AdaEdge 72.8 \u00b1 0.7 70.6 \u00b1 1.5 75.2 \u00b1 0.6 72.6 \u00b1 1.4 OOT OOT SLAPS (MLP) + self-training 74.2 \u00b1 0.5 73.1 \u00b1 1.0 75.5 \u00b1 0.7 73.3 \u00b1 0.6 74.3 \u00b1 1.4 NA8 71.3 \u00b1 0.9 70.9 \u00b1 0.7 67.3 \u00b1 0.3 \nOOM \nDGCNN \n56.5 \u00b1 1.2 55.1 \u00b1 1.4 67.3 \u00b1 0.7 66.6 \u00b1 0.8 70.1 \u00b1 1.3 \nOOM \nIDGL \n70.9 \u00b1 0.6 68.2 \u00b1 0.6 73.4 \u00b1 0.5 72.7 \u00b1 0.4 72.3 \u00b1 0.4 \nOOM \nkNN-GCN + AdaEdge \n67.7 \u00b1 1.0 68.8 \u00b1 1.0 72.2 \u00b1 0.4 71.8 \u00b1 0.6 \nOOT \nOOT \nkNN-GCN + self-training \n67.3 \u00b1 0.3 69.8 \u00b1 1.0 71.1 \u00b1 0.3 72.4 \u00b1 0.2 72.7 \u00b1 0.1 \nNA \nSLAPS (FP) \n72.4 \u00b1 0.4 70.7 \u00b1 0.4 76.6 \u00b1 0.4 73.1 \u00b1 0.6 \nOOM \nOOM \nSLAPS (MLP) \n72.8 \u00b1 0.8 70.5 \u00b1 1.1 75.3 \u00b1 1.0 73.0 \u00b1 0.9 74.4 \u00b1 0.6 56.6 \u00b1 0.1 \nSLAPS (MLP-D) \n73.4 \u00b1 0.3 72.6 \u00b1 0.6 75.1 \u00b1 0.5 73.9 \u00b1 0.4 73.1 \u00b1 0.7 52.9 \u00b1 0.1 \n\n\nTable 2 :\n2Results on classification datasets. \u2020 indicates results have been taken from Franceschi et al.[13]. Bold and underlined values indicate best and second-best mean performances respectively.Model \nWine \nCancer \nDigits \n20news \nMLP \n96.1 \u00b1 1.0 \n95.3 \u00b1 0.9 \n81.9 \u00b1 1.0 \n30.4 \u00b1 0.1 \nkNN-GCN \n93.5 \u00b1 0.7 \n95.3 \u00b1 0.4 \n95.4 \u00b1 0.4 \n46.3 \u00b1 0.3 \nLDS \n97.3 \u00b1 0.4  \u2020 \n94.4 \u00b1 1.9  \u2020 \n92.5 \u00b1 0.7  \u2020 \n46.4 \u00b1 1.6  \u2020 \nIDGL \n97.0 \u00b1 0.7 \n94.2 \u00b1 2.3 \n92.5 \u00b1 1.3 \n48.5 \u00b1 0.6 \nSLAPS (FP) \n96.6 \u00b1 0.4 \n94.6 \u00b1 0.3 \n94.4 \u00b1 0.7 \n44.4 \u00b1 0.8 \nSLAPS (MLP) \n96.3 \u00b1 1.0 \n96.0 \u00b1 0.8 \n92.5 \u00b1 0.7 \n50.4 \u00b1 0.7 \nSLAPS (MLP-D) \n96.5 \u00b1 0.8 \n96.6 \u00b1 0.2 \n94.2 \u00b1 0.1 \n49.8 \u00b1 0.9 \n\n\n\nTable 3 :\n3Results on the MNIST dataset. semi-supervised image classification. In particular, they create three versions of the MNIST dataset each consisting of a randomly selected subset with 10,000 examples in total. The first version contains 1000 labels for training, the second contains 2000, and the third version contains 3000 labels for training. All three variants use an extra 1000 labels for validation. The other examples are used as test examples.Bold values \n\n\nTable 4 :\n4Best set of hyperparameters for different datasets chosen on validation set.Dataset \nGenerator \nlrC \nlrDAE dropoutc dropoutDAE \nk \n\u03bb \nr \n\u03b7 \nCora \nFP \n0.001 \n0.01 \n0.5 \n0.25 \n30 \n10 \n10 5 \nCora \nMLP \n0.01 \n0.001 \n0.25 \n0.5 \n20 \n10 \n10 5 \nCora \nMLP-D \n0.01 \n0.001 \n0.25 \n0.5 \n15 \n10 \n10 5 \nCiteseer \nFP \n0.01 \n0.01 \n0.5 \n0.5 \n30 \n1 \n10 1 \nCiteseer \nMLP \n0.01 \n0.001 \n0.25 \n0.5 \n30 \n10 \n10 5 \nCiteseer \nMLP-D \n0.001 \n0.01 \n0.5 \n0.5 \n20 \n10 \n10 5 \nCora390 \nFP \n0.01 \n0.01 \n0.25 \n0.5 \n20 100 10 5 \nCora390 \nMLP \n0.01 \n0.001 \n0.25 \n0.5 \n20 \n10 \n10 5 \nCora390 \nMLP-D \n0.001 \n0.001 \n0.25 \n0.5 \n20 \n10 \n10 5 \nCiteseer370 \nFP \n0.01 \n0.01 \n0.5 \n0.5 \n30 \n1 \n10 1 \nCiteseer370 \nMLP \n0.01 \n0.001 \n0.25 \n0.5 \n30 \n10 \n10 5 \nCiteseer370 \nMLP-D \n0.01 \n0.01 \n0.25 \n0.5 \n20 \n10 \n10 5 \nPubmed \nMLP \n0.01 \n0.01 \n0.5 \n0.5 \n15 \n10 \n10 5 \nPubmed \nMLP-D \n0.01 \n0.01 \n0.25 \n0.25 \n15 100 \n5 \n5 \nogbn-arxiv \nMLP \n0.01 \n0.001 \n0.25 \n0.5 \n15 \n10 \n1 \n5 \nogbn-arxiv \nMLP-D \n0.01 \n0.001 \n0.5 \n0.25 \n15 \n10 \n1 \n5 \nWine \nFP \n0.01 \n0.001 \n0.5 \n0.5 \n20 0.1 \n5 \n5 \nWine \nMLP \n0.01 \n0.001 \n0.5 \n0.25 \n20 0.1 \n5 \n5 \nWine \nMLP-D \n0.01 \n0.01 \n0.25 \n0.5 \n10 \n1 \n5 \n5 \nCancer \nFP \n0.01 \n0.001 \n0.5 \n0.25 \n20 0.1 \n5 \n5 \nCancer \nMLP \n0.01 \n0.001 \n0.5 \n0.5 \n20 1.0 \n5 \n5 \nCancer \nMLP-D \n0.01 \n0.01 \n0.5 \n0.5 \n20 0.1 \n5 \n5 \nDigits \nFP \n0.01 \n0.001 \n0.25 \n0.5 \n20 0.1 \n5 \n5 \nDigits \nMLP \n0.01 \n0.001 \n0.25 \n0.5 \n20 \n10 \n5 \n5 \nDigits \nMLP-D \n0.01 \n0.001 \n0.5 \n0.25 \n15 0.1 \n5 \n5 \n20news \nFP \n0.01 \n0.01 \n0.5 \n0.5 \n20 500 \n5 \n5 \n20news \nMLP \n0.001 \n0.001 \n0.25 \n0.5 \n20 500 \n5 \n5 \n20news \nMLP-D \n0.01 \n0.01 \n0.25 \n0.25 \n20 100 \n5 \n5 \nMNIST (1000) \nMLP \n0.01 \n0.01 \n0.5 \n0.5 \n15 \n10 \n10 5 \nMNIST (2000) \nMLP-D \n0.01 \n0.001 \n0.5 \n0.5 \n15 100 10 5 \nMNIST (3000) \nMLP \n0.01 \n0.01 \n0.5 \n0.5 \n15 \n10 \n5 \n5 \n\n\n\nTable 5 :\n5Dataset statistics.Dataset \nNodes \nEdges \nClasses Features \nLabel rate \nCora \n2,708 \n5,429 \n7 \n1,433 \n0.052 \nCiteseer \n3,327 \n4,732 \n6 \n3,703 \n0.036 \nPubmed \n19,717 \n44,338 \n3 \n500 \n0.003 \nogbn-arxiv 169,343 1,166,243 \n40 \n128 \n0.537 \nWine \n178 \n0 \n3 \n13 \n0.112 \nCancer \n569 \n0 \n2 \n30 \n0.035 \nDigits \n1,797 \n0 \n10 \n64 \n0.056 \n20news \n9,607 \n0 \n10 \n236 \n0.021 \nMNIST \n10,000 \n0 \n10 \n784 \n0.1, 0.2 and 0.3 \n\n\nWhile using more layers may somewhat alleviate this problem, deeper GCNs typically produce inferior results, e.g., due to oversmoothing[33,39] -see the supplementary material for empirical evidence.\nWe note that IDGL-ANCH also scales to ogbn-arxiv.\nThe generator used in this experiment is MLP; other generators produced similar results.\nFor the first node in the example, the 4th and 5th features are both 1 so their logical and is also 1 and so the last two features for this node are both 1. The computation for the other two nodes is similar.\nWe are disregarding the features for simplicity sake.\n\nStatistical mechanics of complex networks. R\u00e9ka Albert, Albert-L\u00e1szl\u00f3 Barab\u00e1si, Reviews of modern physics. 74147R\u00e9ka Albert and Albert-L\u00e1szl\u00f3 Barab\u00e1si. Statistical mechanics of complex networks. Reviews of modern physics, 74(1):47, 2002.\n\nEmergence of scaling in random networks. Albert-L\u00e1szl\u00f3 Barab\u00e1si, R\u00e9ka Albert, science. 2865439Albert-L\u00e1szl\u00f3 Barab\u00e1si and R\u00e9ka Albert. Emergence of scaling in random networks. science, 286(5439):509-512, 1999.\n\nManifold regularization: A geometric framework for learning from labeled and unlabeled examples. Mikhail Belkin, Partha Niyogi, Vikas Sindhwani, JMLRR. 7Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. JMLRR, 7(Nov):2399-2434, 2006.\n\nInes Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher R\u00e9, Kevin Murphy, arXiv:2005.03675Machine learning on graphs: A model and comprehensive taxonomy. arXiv preprintInes Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher R\u00e9, and Kevin Murphy. Machine learning on graphs: A model and comprehensive taxonomy. arXiv preprint arXiv:2005.03675, 2020.\n\nMeasuring and relieving the over-smoothing problem for graph neural networks from the topological view. Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, Xu Sun, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-smoothing problem for graph neural networks from the topological view. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 3438-3445, 2020.\n\nDeep iterative and adaptive learning for graph neural networks. Yu Chen, Lingfei Wu, Mohammed J Zaki, Neural Information Processing Systems (NeurIPS). 2020Yu Chen, Lingfei Wu, and Mohammed J. Zaki. Deep iterative and adaptive learning for graph neural networks. In Neural Information Processing Systems (NeurIPS), 2020.\n\nRethinking attention with performers. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, arXiv:2009.14794arXiv preprintKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020.\n\nFast and accurate deep network learning by exponential linear units (elus). Djork-Arn\u00e9 Clevert, Thomas Unterthiner, Sepp Hochreiter, arXiv:1511.07289arXiv preprintDjork-Arn\u00e9 Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.\n\nLatent patient network learning for automatic diagnosis. Luca Cosmo, Anees Kazi, Seyed-Ahmad Ahmadi, Nassir Navab, Michael Bronstein, arXiv:2003.13620arXiv preprintLuca Cosmo, Anees Kazi, Seyed-Ahmad Ahmadi, Nassir Navab, and Michael Bronstein. Latent patient network learning for automatic diagnosis. arXiv preprint arXiv:2003.13620, 2020.\n\nHanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, Le Song, arXiv:1806.02371Adversarial attack on graph structured data. arXiv preprintHanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial attack on graph structured data. arXiv preprint arXiv:1806.02371, 2018.\n\nOn random graphs. Paul Erd\u0151s, Alfred R\u00e9nyi, Publicationes Mathematicae Debrecen. 6Paul Erd\u0151s and Alfred R\u00e9nyi. On random graphs. Publicationes Mathematicae Debrecen, 6: 290-297, 1959.\n\nHow robust are graph neural networks to structural noise?. James Fox, Sivasankaran Rajamanickam, arXiv:1912.10206arXiv preprintJames Fox and Sivasankaran Rajamanickam. How robust are graph neural networks to structural noise? arXiv preprint arXiv:1912.10206, 2019.\n\nLearning discrete structures for graph neural networks. Luca Franceschi, Mathias Niepert, Massimiliano Pontil, Xiao He, ICML. Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. Learning discrete structures for graph neural networks. In ICML, 2019.\n\nFew-shot learning with graph neural networks. Victor Garcia, Joan Bruna, arXiv:1711.04043arXiv preprintVictor Garcia and Joan Bruna. Few-shot learning with graph neural networks. arXiv preprint arXiv:1711.04043, 2017.\n\nGenerating classification weights with gnn denoising autoencoders for few-shot learning. Spyros Gidaris, Nikos Komodakis, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSpyros Gidaris and Nikos Komodakis. Generating classification weights with gnn denoising autoencoders for few-shot learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 21-30, 2019.\n\nGrale: Designing networks for graph learning. Jonathan Halcrow, Alexandru Mosoi, Sam Ruth, Bryan Perozzi, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningJonathan Halcrow, Alexandru Mosoi, Sam Ruth, and Bryan Perozzi. Grale: Designing networks for graph learning. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2523-2532, 2020.\n\nOpen graph benchmark: Datasets for machine learning on graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, arXiv:2005.00687arXiv preprintWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020.\n\nStrategies for pre-training graph neural networks. Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, Jure Leskovec, ICLR. Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. In ICLR, 2020.\n\nGpt-gnn: Generative pre-training of graph neural networks. Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, Yizhou Sun, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningZiniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. Gpt-gnn: Generative pre-training of graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1857-1867, 2020.\n\nSoobeom Jang, Seong-Eun Moon, Jong-Seok Lee, arXiv:1905.11678Brain signal classification via learning connectivity structure. arXiv preprintSoobeom Jang, Seong-Eun Moon, and Jong-Seok Lee. Brain signal classification via learning connectivity structure. arXiv preprint arXiv:1905.11678, 2019.\n\nSemi-supervised learning with graph learning-convolutional networks. Bo Jiang, Ziyan Zhang, Doudou Lin, Jin Tang, Bin Luo, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionBo Jiang, Ziyan Zhang, Doudou Lin, Jin Tang, and Bin Luo. Semi-supervised learning with graph learning-convolutional networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11313-11320, 2019.\n\nSemi-supervised learning with graph learning-convolutional networks. Bo Jiang, Ziyan Zhang, Doudou Lin, Jin Tang, Bin Luo, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionBo Jiang, Ziyan Zhang, Doudou Lin, Jin Tang, and Bin Luo. Semi-supervised learning with graph learning-convolutional networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11313-11320, 2019.\n\nSelf-supervised learning on graphs. Wei Jin, Tyler Derr, Haochen Liu, Yiqi Wang, Suhang Wang, Zitao Liu, Jiliang Tang, arXiv:2006.10141Deep insights and new direction. arXiv preprintWei Jin, Tyler Derr, Haochen Liu, Yiqi Wang, Suhang Wang, Zitao Liu, and Jiliang Tang. Self-supervised learning on graphs: Deep insights and new direction. arXiv preprint arXiv:2006.10141, 2020.\n\nWei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, Jiliang Tang, arXiv:2005.10203Graph structure learning for robust graph neural networks. arXiv preprintWei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang. Graph structure learning for robust graph neural networks. arXiv preprint arXiv:2005.10203, 2020.\n\nLearning graph structure with a finite-state automaton layer. D Daniel, Hugo Johnson, Daniel Larochelle, Tarlow, arXiv:2007.04929arXiv preprintDaniel D Johnson, Hugo Larochelle, and Daniel Tarlow. Learning graph structure with a finite-state automaton layer. arXiv preprint arXiv:2007.04929, 2020.\n\nRepresentation learning for dynamic graphs: A survey. Rishab Seyed Mehran Kazemi, Kshitij Goel, Ivan Jain, Akshay Kobyzev, Peter Sethi, Pascal Forsyth, Poupart, JMLRSeyed Mehran Kazemi, Rishab Goel, Kshitij Jain, Ivan Kobyzev, Akshay Sethi, Peter Forsyth, and Pascal Poupart. Representation learning for dynamic graphs: A survey. JMLR, 2020.\n\nDifferentiable graph module (dgm) for graph convolutional networks. Anees Kazi, Luca Cosmo, Nassir Navab, Michael Bronstein, arXiv:2002.04999arXiv preprintAnees Kazi, Luca Cosmo, Nassir Navab, and Michael Bronstein. Differentiable graph module (dgm) for graph convolutional networks. arXiv preprint arXiv:2002.04999, 2020.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nSemi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, ICLR. Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2017.\n\nReformer: The efficient transformer. Nikita Kitaev, \u0141ukasz Kaiser, Anselm Levskaya, arXiv:2001.04451arXiv preprintNikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.\n\nMnist handwritten digit database. Yann Lecun, Corinna Cortes, C J Burges, 2ATT LabsYann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.\n\nDeepgcns: Can gcns go as deep as cnns?. Guohao Li, Matthias Muller, Ali Thabet, Bernard Ghanem, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionGuohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9267-9276, 2019.\n\nDeeper insights into graph convolutional networks for semi-supervised learning. Qimai Li, Zhichao Han, Xiao-Ming Wu, In AAAI. Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI, 2018.\n\nRuoyu Li, Sheng Wang, Feiyun Zhu, Junzhou Huang, arXiv:1801.03226Adaptive graph convolutional neural networks. arXiv preprintRuoyu Li, Sheng Wang, Feiyun Zhu, and Junzhou Huang. Adaptive graph convolutional neural networks. arXiv preprint arXiv:1801.03226, 2018.\n\nEfficient graph generation with graph recurrent attention networks. Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Charlie Nash, L William, David Hamilton, Raquel Duvenaud, Richard S Urtasun, Zemel, arXiv:1910.00760arXiv preprintRenjie Liao, Yujia Li, Yang Song, Shenlong Wang, Charlie Nash, William L Hamilton, David Duvenaud, Raquel Urtasun, and Richard S Zemel. Efficient graph generation with graph recurrent attention networks. arXiv preprint arXiv:1910.00760, 2019.\n\nGraph normalizing flows. Jenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, Kevin Swersky, NeurIPS. Jenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, and Kevin Swersky. Graph normalizing flows. In NeurIPS, pages 13556-13566, 2019.\n\nLarge graph construction for scalable semi-supervised learning. Wei Liu, Junfeng He, Shih-Fu Chang, ICML. Wei Liu, Junfeng He, and Shih-Fu Chang. Large graph construction for scalable semi-supervised learning. In ICML, 2010.\n\nRectified linear units improve restricted boltzmann machines. Vinod Nair, Geoffrey E Hinton, Icml. Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Icml, 2010.\n\nGraph neural networks exponentially lose expressive power for node classification. Kenta Oono, Taiji Suzuki, ICLR. Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classification. In ICLR, 2020.\n\nAutomatic differentiation in pytorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.\n\nScikitlearn: Machine learning in python. Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, JMLR. 12Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit- learn: Machine learning in python. JMLR, 12:2825-2830, 2011.\n\nDeepwalk: Online learning of social representations. Bryan Perozzi, Rami Al-Rfou, Steven Skiena, Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. the 20th ACM SIGKDD international conference on Knowledge discovery and data miningBryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repre- sentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701-710, 2014.\n\nLearning representations of irregular particle-detector geometry with distance-weighted graph networks. Jan Shah Rukh Qasim, Yutaro Kieseler, Maurizio Iiyama, Pierini, The European Physical Journal C. 797Shah Rukh Qasim, Jan Kieseler, Yutaro Iiyama, and Maurizio Pierini. Learning representations of irregular particle-detector geometry with distance-weighted graph networks. The European Physical Journal C, 79(7):1-11, 2019.\n\nNonlinear dimensionality reduction by locally linear embedding. science. T Sam, Lawrence K Roweis, Saul, 290Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding. science, 290(5500):2323-2326, 2000.\n\nThe graph neural network model. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, Gabriele Monfardini, IEEE Transactions on Neural Networks. 201Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2008.\n\nCollective classification in network data. Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, Tina Eliassi-Rad, AI magazine. 293Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi- Rad. Collective classification in network data. AI magazine, 29(3):93-93, 2008.\n\nGraph agreement models for semi-supervised learning. Otilia Stretcu, Krishnamurthy Viswanathan, Dana Movshovitz-Attias, Emmanouil Platanios, Sujith Ravi, Andrew Tomkins, NeurIPS. Otilia Stretcu, Krishnamurthy Viswanathan, Dana Movshovitz-Attias, Emmanouil Platanios, Sujith Ravi, and Andrew Tomkins. Graph agreement models for semi-supervised learning. In NeurIPS, pages 8713-8723, 2019.\n\nMixture-kernel graph attention network for situation recognition. Mohammed Suhail, Leonid Sigal, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionMohammed Suhail and Leonid Sigal. Mixture-kernel graph attention network for situation recognition. In Proceedings of the IEEE International Conference on Computer Vision, pages 10363-10372, 2019.\n\nA global geometric framework for nonlinear dimensionality reduction. science. Vin De Joshua B Tenenbaum, John C Silva, Langford, 290Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for nonlinear dimensionality reduction. science, 290(5500):2319-2323, 2000.\n\nGraph attention networks. Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, In ICLR. Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In ICLR, 2018.\n\nExtracting and composing robust features with denoising autoencoders. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, Pierre-Antoine Manzagol, ICML. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, pages 1096-1103, 2008.\n\nMinjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao Ye, Mufei Li, Jinjing Zhou, Qi Huang, Chao Ma, arXiv:1909.01315Deep graph library: Towards efficient and scalable deep learning on graphs. arXiv preprintMinjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao Ye, Mufei Li, Jinjing Zhou, Qi Huang, Chao Ma, et al. Deep graph library: Towards efficient and scalable deep learning on graphs. arXiv preprint arXiv:1909.01315, 2019.\n\nDynamic graph cnn for learning on point clouds. Yue Wang, Yongbin Sun, Ziwei Liu, E Sanjay, Sarma, Justin M Michael M Bronstein, Solomon, Acm Transactions On Graphics (tog). 385Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. Acm Transactions On Graphics (tog), 38(5):1-12, 2019.\n\nA quest for structure: Jointly learning the graph structure and semi-supervised classification. Xuan Wu, Lingxiao Zhao, Leman Akoglu, CIKM. Xuan Wu, Lingxiao Zhao, and Leman Akoglu. A quest for structure: Jointly learning the graph structure and semi-supervised classification. In CIKM, pages 87-96, 2018.\n\nTopology optimization based graph convolutional network. Liang Yang, Zesheng Kang, Xiaochun Cao, Di Jin, Bo Yang, Yuanfang Guo, IJCAI. Liang Yang, Zesheng Kang, Xiaochun Cao, Di Jin, Bo Yang, and Yuanfang Guo. Topology optimization based graph convolutional network. In IJCAI, pages 4054-4061, 2019.\n\nJiaxuan You, Rex Ying, Xiang Ren, L William, Jure Hamilton, Leskovec, Graphrnn, arXiv:1802.08773Generating realistic graphs with deep auto-regressive models. arXiv preprintJiaxuan You, Rex Ying, Xiang Ren, William L Hamilton, and Jure Leskovec. Graphrnn: Generating realistic graphs with deep auto-regressive models. arXiv preprint arXiv:1802.08773, 2018.\n\nWhen does self-supervision help graph convolutional networks?. Yuning You, Tianlong Chen, Zhangyang Wang, Yang Shen, arXiv:2006.09136arXiv preprintYuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. When does self-supervision help graph convolutional networks? arXiv preprint arXiv:2006.09136, 2020.\n\nGraph-revised convolutional network. Donghan Yu, Ruohong Zhang, Zhengbao Jiang, Yuexin Wu, Yiming Yang, ECML PKDD. Donghan Yu, Ruohong Zhang, Zhengbao Jiang, Yuexin Wu, and Yiming Yang. Graph-revised convolutional network. In ECML PKDD, 2020.\n\nJiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, Dit-Yan Yeung, arXiv:1803.07294Gaan: Gated attention networks for learning on large and spatiotemporal graphs. arXiv preprintJiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: Gated attention networks for learning on large and spatiotemporal graphs. arXiv preprint arXiv:1803.07294, 2018.\n\nGraph-bert: Only attention is needed for learning graph representations. Jiawei Zhang, Haopeng Zhang, Congying Xia, Li Sun, arXiv:2001.05140arXiv preprintJiawei Zhang, Haopeng Zhang, Congying Xia, and Li Sun. Graph-bert: Only attention is needed for learning graph representations. arXiv preprint arXiv:2001.05140, 2020.\n\nTong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, Neil Shah, arXiv:2006.06830Data augmentation for graph neural networks. arXiv preprintTong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. arXiv preprint arXiv:2006.06830, 2020.\n\nGraph neural networks with generated parameters for relation extraction. Hao Zhu, Yankai Lin, Zhiyuan Liu, Jie Fu, arXiv:1902.00756arXiv preprintTat-seng Chua, and Maosong SunHao Zhu, Yankai Lin, Zhiyuan Liu, Jie Fu, Tat-seng Chua, and Maosong Sun. Graph neural networks with generated parameters for relation extraction. arXiv preprint arXiv:1902.00756, 2019.\n\nBeyond homophily in graph neural networks: Current limitations and effective designs. Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, Danai Koutra, Advances in Neural Information Processing Systems. 33Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Be- yond homophily in graph neural networks: Current limitations and effective designs. Advances in Neural Information Processing Systems, 33, 2020.\n\nSelf-supervised training of graph convolutional networks. Qikui Zhu, Bo Du, Pingkun Yan, arXiv:2006.02380arXiv preprintQikui Zhu, Bo Du, and Pingkun Yan. Self-supervised training of graph convolutional networks. arXiv preprint arXiv:2006.02380, 2020.\n\nLearning from labeled and unlabeled data with label propagation. Xiaojin Zhu, Zoubin Ghahramani, Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. 2002.\n\nSemi-supervised learning using gaussian fields and harmonic functions. Xiaojin Zhu, Zoubin Ghahramani, John D Lafferty, Proceedings of the 20th International conference on Machine learning (ICML-03). the 20th International conference on Machine learning (ICML-03)Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In Proceedings of the 20th International conference on Machine learning (ICML-03), pages 912-919, 2003.\n\nYanqiao Zhu, Weizhi Xu, Jinghao Zhang, Qiang Liu, Shu Wu, Liang Wang, arXiv:2103.03036Deep graph structure learning for robust representations: A survey. arXiv preprintYanqiao Zhu, Weizhi Xu, Jinghao Zhang, Qiang Liu, Shu Wu, and Liang Wang. Deep graph structure learning for robust representations: A survey. arXiv preprint arXiv:2103.03036, 2021.\n", "annotations": {"author": "[{\"end\":172,\"start\":81},{\"end\":274,\"start\":173},{\"end\":378,\"start\":275}]", "publisher": null, "author_last_name": "[{\"end\":94,\"start\":88},{\"end\":186,\"start\":182},{\"end\":294,\"start\":288}]", "author_first_name": "[{\"end\":87,\"start\":81},{\"end\":178,\"start\":173},{\"end\":181,\"start\":179},{\"end\":280,\"start\":275},{\"end\":287,\"start\":281}]", "author_affiliation": "[{\"end\":171,\"start\":114},{\"end\":273,\"start\":216},{\"end\":377,\"start\":320}]", "title": "[{\"end\":78,\"start\":1},{\"end\":456,\"start\":379}]", "venue": null, "abstract": "[{\"end\":1404,\"start\":458}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1559,\"start\":1556},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1562,\"start\":1559},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":1597,\"start\":1593},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":1881,\"start\":1877},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2511,\"start\":2507},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":4468,\"start\":4464},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4654,\"start\":4650},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4657,\"start\":4654},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4659,\"start\":4657},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4850,\"start\":4846},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":4939,\"start\":4935},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5125,\"start\":5121},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":5523,\"start\":5519},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":5526,\"start\":5523},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":5585,\"start\":5582},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":5625,\"start\":5622},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5659,\"start\":5656},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":5773,\"start\":5769},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6130,\"start\":6126},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6241,\"start\":6237},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":6385,\"start\":6381},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6489,\"start\":6486},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":6840,\"start\":6836},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6943,\"start\":6939},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6946,\"start\":6943},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6948,\"start\":6946},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":7006,\"start\":7002},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7009,\"start\":7006},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7106,\"start\":7102},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7569,\"start\":7565},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7725,\"start\":7721},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7989,\"start\":7985},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7992,\"start\":7989},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8386,\"start\":8382},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8389,\"start\":8386},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8392,\"start\":8389},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":8395,\"start\":8392},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":8398,\"start\":8395},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":8475,\"start\":8471},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9456,\"start\":9452},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9833,\"start\":9829},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":11033,\"start\":11029},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11036,\"start\":11033},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11039,\"start\":11036},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11100,\"start\":11097},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11385,\"start\":11381},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14473,\"start\":14470},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14481,\"start\":14478},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14558,\"start\":14555},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16294,\"start\":16290},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16372,\"start\":16369},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16400,\"start\":16397},{\"end\":16771,\"start\":16770},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":19998,\"start\":19994},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21773,\"start\":21769},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21789,\"start\":21786},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":22676,\"start\":22672},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":22896,\"start\":22892},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22962,\"start\":22958},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23242,\"start\":23238},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":23253,\"start\":23249},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":23265,\"start\":23261},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23279,\"start\":23276},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23287,\"start\":23284},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23406,\"start\":23402},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23766,\"start\":23762},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":23828,\"start\":23824},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":24690,\"start\":24686},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24729,\"start\":24725},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25021,\"start\":25017},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25023,\"start\":25021},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25123,\"start\":25119},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25195,\"start\":25191},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25234,\"start\":25230},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29719,\"start\":29715},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29935,\"start\":29931},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30016,\"start\":30013},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":30051,\"start\":30047},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":32774,\"start\":32771},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":34748,\"start\":34745},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":34751,\"start\":34748},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":38911,\"start\":38908},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":38914,\"start\":38911},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":39254,\"start\":39250},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":43730,\"start\":43726},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":43766,\"start\":43762},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":43812,\"start\":43808},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":45598,\"start\":45594},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":45610,\"start\":45606},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":45624,\"start\":45621},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":45757,\"start\":45753},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":45813,\"start\":45809},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":45855,\"start\":45851},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":48461,\"start\":48457},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":48464,\"start\":48461},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":49319,\"start\":49316},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":52551,\"start\":52548},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":54365,\"start\":54361},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":55683,\"start\":55679},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":58868,\"start\":58864},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":62206,\"start\":62202},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":62209,\"start\":62206}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":56601,\"start\":56005},{\"attributes\":{\"id\":\"fig_1\"},\"end\":56709,\"start\":56602},{\"attributes\":{\"id\":\"fig_2\"},\"end\":56835,\"start\":56710},{\"attributes\":{\"id\":\"fig_4\"},\"end\":56901,\"start\":56836},{\"attributes\":{\"id\":\"fig_6\"},\"end\":56984,\"start\":56902},{\"attributes\":{\"id\":\"fig_7\"},\"end\":57107,\"start\":56985},{\"attributes\":{\"id\":\"fig_9\"},\"end\":57436,\"start\":57108},{\"attributes\":{\"id\":\"fig_10\"},\"end\":57509,\"start\":57437},{\"attributes\":{\"id\":\"fig_12\"},\"end\":57610,\"start\":57510},{\"attributes\":{\"id\":\"fig_14\"},\"end\":57760,\"start\":57611},{\"attributes\":{\"id\":\"fig_15\"},\"end\":58031,\"start\":57761},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":58757,\"start\":58032},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":59409,\"start\":58758},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":59884,\"start\":59410},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":61647,\"start\":59885},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":62066,\"start\":61648}]", "paragraph": "[{\"end\":1882,\"start\":1420},{\"end\":2513,\"start\":1884},{\"end\":3428,\"start\":2515},{\"end\":3717,\"start\":3430},{\"end\":4291,\"start\":3719},{\"end\":4487,\"start\":4308},{\"end\":5293,\"start\":4489},{\"end\":5980,\"start\":5295},{\"end\":7406,\"start\":5982},{\"end\":8087,\"start\":7408},{\"end\":8673,\"start\":8089},{\"end\":9414,\"start\":8701},{\"end\":10275,\"start\":9416},{\"end\":10745,\"start\":10302},{\"end\":11117,\"start\":10759},{\"end\":11681,\"start\":11119},{\"end\":12037,\"start\":11694},{\"end\":13202,\"start\":12039},{\"end\":14126,\"start\":13226},{\"end\":14825,\"start\":14141},{\"end\":15274,\"start\":14899},{\"end\":16551,\"start\":15276},{\"end\":16771,\"start\":16553},{\"end\":17601,\"start\":16823},{\"end\":18018,\"start\":17622},{\"end\":18149,\"start\":18020},{\"end\":18662,\"start\":18151},{\"end\":19129,\"start\":18664},{\"end\":19932,\"start\":19131},{\"end\":20505,\"start\":19934},{\"end\":21358,\"start\":20560},{\"end\":22029,\"start\":21360},{\"end\":22271,\"start\":22039},{\"end\":22447,\"start\":22287},{\"end\":24207,\"start\":22449},{\"end\":26164,\"start\":24584},{\"end\":26390,\"start\":26166},{\"end\":28298,\"start\":26414},{\"end\":28991,\"start\":28300},{\"end\":29700,\"start\":28993},{\"end\":30124,\"start\":29702},{\"end\":32817,\"start\":30126},{\"end\":33738,\"start\":32859},{\"end\":35530,\"start\":33740},{\"end\":36806,\"start\":35600},{\"end\":37351,\"start\":36821},{\"end\":37428,\"start\":37386},{\"end\":38530,\"start\":37430},{\"end\":39787,\"start\":38566},{\"end\":40579,\"start\":39789},{\"end\":41906,\"start\":40612},{\"end\":42218,\"start\":41908},{\"end\":43283,\"start\":42220},{\"end\":43661,\"start\":43285},{\"end\":44210,\"start\":43690},{\"end\":45583,\"start\":44212},{\"end\":45856,\"start\":45585},{\"end\":46048,\"start\":45858},{\"end\":46327,\"start\":46050},{\"end\":46848,\"start\":46329},{\"end\":47602,\"start\":46850},{\"end\":48466,\"start\":47604},{\"end\":48835,\"start\":48468},{\"end\":49375,\"start\":48837},{\"end\":49479,\"start\":49400},{\"end\":50050,\"start\":49547},{\"end\":50268,\"start\":50052},{\"end\":50615,\"start\":50321},{\"end\":51077,\"start\":50617},{\"end\":51398,\"start\":51079},{\"end\":52145,\"start\":51400},{\"end\":52376,\"start\":52147},{\"end\":53108,\"start\":52429},{\"end\":53208,\"start\":53110},{\"end\":53347,\"start\":53210},{\"end\":53405,\"start\":53349},{\"end\":53663,\"start\":53407},{\"end\":53945,\"start\":53665},{\"end\":55327,\"start\":53947},{\"end\":56004,\"start\":55345}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16822,\"start\":16772},{\"attributes\":{\"id\":\"formula_1\"},\"end\":20559,\"start\":20506},{\"attributes\":{\"id\":\"formula_2\"},\"end\":24583,\"start\":24208},{\"attributes\":{\"id\":\"formula_3\"},\"end\":40611,\"start\":40580},{\"attributes\":{\"id\":\"formula_4\"},\"end\":50320,\"start\":50269},{\"attributes\":{\"id\":\"formula_5\"},\"end\":52428,\"start\":52377}]", "table_ref": "[{\"end\":23619,\"start\":23612},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26501,\"start\":26487},{\"end\":26547,\"start\":26540},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29003,\"start\":28996},{\"end\":29075,\"start\":29068},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":30161,\"start\":30154},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":49478,\"start\":49471}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1418,\"start\":1406},{\"attributes\":{\"n\":\"2\"},\"end\":4306,\"start\":4294},{\"attributes\":{\"n\":\"3\"},\"end\":8699,\"start\":8676},{\"attributes\":{\"n\":\"4\"},\"end\":10300,\"start\":10278},{\"attributes\":{\"n\":\"4.1\"},\"end\":10757,\"start\":10748},{\"end\":11692,\"start\":11684},{\"attributes\":{\"n\":\"4.2\"},\"end\":13224,\"start\":13205},{\"attributes\":{\"n\":\"4.3\"},\"end\":14139,\"start\":14129},{\"attributes\":{\"n\":\"4.4\"},\"end\":14897,\"start\":14828},{\"attributes\":{\"n\":\"4.5\"},\"end\":17620,\"start\":17604},{\"attributes\":{\"n\":\"4.6\"},\"end\":22037,\"start\":22032},{\"attributes\":{\"n\":\"5\"},\"end\":22285,\"start\":22274},{\"attributes\":{\"n\":\"5.1\"},\"end\":26412,\"start\":26393},{\"attributes\":{\"n\":\"5.2\"},\"end\":32857,\"start\":32820},{\"attributes\":{\"n\":\"5.3\"},\"end\":35562,\"start\":35533},{\"attributes\":{\"n\":\"5.4\"},\"end\":35598,\"start\":35565},{\"attributes\":{\"n\":\"6\"},\"end\":36819,\"start\":36809},{\"attributes\":{\"n\":\"7\"},\"end\":37384,\"start\":37354},{\"end\":38564,\"start\":38533},{\"end\":43688,\"start\":43664},{\"end\":49398,\"start\":49378},{\"end\":49545,\"start\":49482},{\"end\":55343,\"start\":55330},{\"end\":56016,\"start\":56006},{\"end\":56606,\"start\":56603},{\"end\":56721,\"start\":56711},{\"end\":56847,\"start\":56837},{\"end\":56913,\"start\":56903},{\"end\":56996,\"start\":56986},{\"end\":57119,\"start\":57109},{\"end\":57448,\"start\":57438},{\"end\":57521,\"start\":57511},{\"end\":57622,\"start\":57612},{\"end\":57773,\"start\":57762},{\"end\":58768,\"start\":58759},{\"end\":59420,\"start\":59411},{\"end\":59895,\"start\":59886},{\"end\":61658,\"start\":61649}]", "table": "[{\"end\":58757,\"start\":58193},{\"end\":59409,\"start\":58958},{\"end\":59884,\"start\":59871},{\"end\":61647,\"start\":59973},{\"end\":62066,\"start\":61679}]", "figure_caption": "[{\"end\":56601,\"start\":56018},{\"end\":56709,\"start\":56608},{\"end\":56835,\"start\":56723},{\"end\":56901,\"start\":56849},{\"end\":56984,\"start\":56915},{\"end\":57107,\"start\":56998},{\"end\":57436,\"start\":57121},{\"end\":57509,\"start\":57450},{\"end\":57610,\"start\":57523},{\"end\":57760,\"start\":57624},{\"end\":58031,\"start\":57776},{\"end\":58193,\"start\":58034},{\"end\":58958,\"start\":58770},{\"end\":59871,\"start\":59422},{\"end\":59973,\"start\":59897},{\"end\":61679,\"start\":61660}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10423,\"start\":10415},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15012,\"start\":15004},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15664,\"start\":15656},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31393,\"start\":31385},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":32178,\"start\":32170},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":32941,\"start\":32932},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":35223,\"start\":35215},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":36405,\"start\":36397},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":37701,\"start\":37693},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":40077,\"start\":40069},{\"attributes\":{\"ref_id\":\"fig_14\"},\"end\":41945,\"start\":41937},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":54500,\"start\":54491}]", "bib_author_first_name": "[{\"end\":62716,\"start\":62712},{\"end\":62738,\"start\":62725},{\"end\":62962,\"start\":62949},{\"end\":62977,\"start\":62973},{\"end\":63222,\"start\":63215},{\"end\":63237,\"start\":63231},{\"end\":63251,\"start\":63246},{\"end\":63456,\"start\":63452},{\"end\":63468,\"start\":63464},{\"end\":63488,\"start\":63483},{\"end\":63509,\"start\":63498},{\"end\":63519,\"start\":63514},{\"end\":63914,\"start\":63910},{\"end\":63927,\"start\":63921},{\"end\":63936,\"start\":63933},{\"end\":63945,\"start\":63941},{\"end\":63953,\"start\":63950},{\"end\":63962,\"start\":63960},{\"end\":64412,\"start\":64410},{\"end\":64426,\"start\":64419},{\"end\":64439,\"start\":64431},{\"end\":64441,\"start\":64440},{\"end\":64714,\"start\":64705},{\"end\":64735,\"start\":64728},{\"end\":64756,\"start\":64751},{\"end\":64771,\"start\":64764},{\"end\":64785,\"start\":64778},{\"end\":64797,\"start\":64792},{\"end\":64811,\"start\":64806},{\"end\":64826,\"start\":64821},{\"end\":64839,\"start\":64834},{\"end\":64857,\"start\":64851},{\"end\":65228,\"start\":65218},{\"end\":65244,\"start\":65238},{\"end\":65262,\"start\":65258},{\"end\":65543,\"start\":65539},{\"end\":65556,\"start\":65551},{\"end\":65574,\"start\":65563},{\"end\":65589,\"start\":65583},{\"end\":65604,\"start\":65597},{\"end\":65830,\"start\":65824},{\"end\":65839,\"start\":65836},{\"end\":65848,\"start\":65844},{\"end\":65858,\"start\":65855},{\"end\":65869,\"start\":65866},{\"end\":65879,\"start\":65876},{\"end\":65887,\"start\":65885},{\"end\":66150,\"start\":66146},{\"end\":66164,\"start\":66158},{\"end\":66377,\"start\":66372},{\"end\":66395,\"start\":66383},{\"end\":66639,\"start\":66635},{\"end\":66659,\"start\":66652},{\"end\":66681,\"start\":66669},{\"end\":66694,\"start\":66690},{\"end\":66897,\"start\":66891},{\"end\":66910,\"start\":66906},{\"end\":67159,\"start\":67153},{\"end\":67174,\"start\":67169},{\"end\":67608,\"start\":67600},{\"end\":67627,\"start\":67618},{\"end\":67638,\"start\":67635},{\"end\":67650,\"start\":67645},{\"end\":68143,\"start\":68137},{\"end\":68156,\"start\":68148},{\"end\":68169,\"start\":68162},{\"end\":68184,\"start\":68178},{\"end\":68197,\"start\":68191},{\"end\":68208,\"start\":68203},{\"end\":68221,\"start\":68214},{\"end\":68235,\"start\":68231},{\"end\":68549,\"start\":68543},{\"end\":68559,\"start\":68554},{\"end\":68571,\"start\":68565},{\"end\":68586,\"start\":68579},{\"end\":68600,\"start\":68595},{\"end\":68613,\"start\":68608},{\"end\":68625,\"start\":68621},{\"end\":68870,\"start\":68865},{\"end\":68881,\"start\":68875},{\"end\":68895,\"start\":68888},{\"end\":68909,\"start\":68902},{\"end\":68923,\"start\":68917},{\"end\":69367,\"start\":69360},{\"end\":69383,\"start\":69374},{\"end\":69399,\"start\":69390},{\"end\":69725,\"start\":69723},{\"end\":69738,\"start\":69733},{\"end\":69752,\"start\":69746},{\"end\":69761,\"start\":69758},{\"end\":69771,\"start\":69768},{\"end\":70236,\"start\":70234},{\"end\":70249,\"start\":70244},{\"end\":70263,\"start\":70257},{\"end\":70272,\"start\":70269},{\"end\":70282,\"start\":70279},{\"end\":70715,\"start\":70712},{\"end\":70726,\"start\":70721},{\"end\":70740,\"start\":70733},{\"end\":70750,\"start\":70746},{\"end\":70763,\"start\":70757},{\"end\":70775,\"start\":70770},{\"end\":70788,\"start\":70781},{\"end\":71057,\"start\":71054},{\"end\":71066,\"start\":71063},{\"end\":71078,\"start\":71071},{\"end\":71092,\"start\":71084},{\"end\":71105,\"start\":71099},{\"end\":71119,\"start\":71112},{\"end\":71453,\"start\":71452},{\"end\":71466,\"start\":71462},{\"end\":71482,\"start\":71476},{\"end\":71749,\"start\":71743},{\"end\":71778,\"start\":71771},{\"end\":71789,\"start\":71785},{\"end\":71802,\"start\":71796},{\"end\":71817,\"start\":71812},{\"end\":71831,\"start\":71825},{\"end\":72105,\"start\":72100},{\"end\":72116,\"start\":72112},{\"end\":72130,\"start\":72124},{\"end\":72145,\"start\":72138},{\"end\":72401,\"start\":72400},{\"end\":72417,\"start\":72412},{\"end\":72641,\"start\":72640},{\"end\":72653,\"start\":72650},{\"end\":72832,\"start\":72826},{\"end\":72847,\"start\":72841},{\"end\":72862,\"start\":72856},{\"end\":73069,\"start\":73065},{\"end\":73084,\"start\":73077},{\"end\":73094,\"start\":73093},{\"end\":73096,\"start\":73095},{\"end\":73311,\"start\":73305},{\"end\":73324,\"start\":73316},{\"end\":73336,\"start\":73333},{\"end\":73352,\"start\":73345},{\"end\":73774,\"start\":73769},{\"end\":73786,\"start\":73779},{\"end\":73801,\"start\":73792},{\"end\":73957,\"start\":73952},{\"end\":73967,\"start\":73962},{\"end\":73980,\"start\":73974},{\"end\":73993,\"start\":73986},{\"end\":74290,\"start\":74284},{\"end\":74302,\"start\":74297},{\"end\":74311,\"start\":74307},{\"end\":74326,\"start\":74318},{\"end\":74340,\"start\":74333},{\"end\":74348,\"start\":74347},{\"end\":74363,\"start\":74358},{\"end\":74380,\"start\":74374},{\"end\":74398,\"start\":74391},{\"end\":74400,\"start\":74399},{\"end\":74721,\"start\":74716},{\"end\":74733,\"start\":74727},{\"end\":74746,\"start\":74741},{\"end\":74756,\"start\":74751},{\"end\":74769,\"start\":74764},{\"end\":74985,\"start\":74982},{\"end\":74998,\"start\":74991},{\"end\":75010,\"start\":75003},{\"end\":75211,\"start\":75206},{\"end\":75226,\"start\":75218},{\"end\":75228,\"start\":75227},{\"end\":75443,\"start\":75438},{\"end\":75455,\"start\":75450},{\"end\":75640,\"start\":75636},{\"end\":75652,\"start\":75649},{\"end\":75667,\"start\":75660},{\"end\":75685,\"start\":75678},{\"end\":75700,\"start\":75694},{\"end\":75714,\"start\":75707},{\"end\":75729,\"start\":75723},{\"end\":75740,\"start\":75735},{\"end\":75756,\"start\":75752},{\"end\":75769,\"start\":75765},{\"end\":76025,\"start\":76019},{\"end\":76041,\"start\":76037},{\"end\":76062,\"start\":76053},{\"end\":76080,\"start\":76073},{\"end\":76097,\"start\":76089},{\"end\":76114,\"start\":76107},{\"end\":76130,\"start\":76123},{\"end\":76145,\"start\":76140},{\"end\":76163,\"start\":76160},{\"end\":76178,\"start\":76171},{\"end\":76500,\"start\":76495},{\"end\":76514,\"start\":76510},{\"end\":76530,\"start\":76524},{\"end\":77057,\"start\":77054},{\"end\":77081,\"start\":77075},{\"end\":77100,\"start\":77092},{\"end\":77452,\"start\":77451},{\"end\":77468,\"start\":77458},{\"end\":77659,\"start\":77653},{\"end\":77676,\"start\":77671},{\"end\":77685,\"start\":77683},{\"end\":77704,\"start\":77698},{\"end\":77727,\"start\":77719},{\"end\":78015,\"start\":78005},{\"end\":78028,\"start\":78021},{\"end\":78044,\"start\":78037},{\"end\":78057,\"start\":78053},{\"end\":78071,\"start\":78066},{\"end\":78087,\"start\":78083},{\"end\":78353,\"start\":78347},{\"end\":78376,\"start\":78363},{\"end\":78394,\"start\":78390},{\"end\":78423,\"start\":78414},{\"end\":78441,\"start\":78435},{\"end\":78454,\"start\":78448},{\"end\":78757,\"start\":78749},{\"end\":78772,\"start\":78766},{\"end\":79180,\"start\":79177},{\"end\":79183,\"start\":79181},{\"end\":79210,\"start\":79204},{\"end\":79423,\"start\":79418},{\"end\":79443,\"start\":79436},{\"end\":79461,\"start\":79454},{\"end\":79479,\"start\":79472},{\"end\":79494,\"start\":79488},{\"end\":79506,\"start\":79500},{\"end\":79743,\"start\":79737},{\"end\":79757,\"start\":79753},{\"end\":79776,\"start\":79770},{\"end\":79799,\"start\":79785},{\"end\":80002,\"start\":79996},{\"end\":80016,\"start\":80009},{\"end\":80023,\"start\":80021},{\"end\":80035,\"start\":80031},{\"end\":80043,\"start\":80041},{\"end\":80054,\"start\":80049},{\"end\":80064,\"start\":80059},{\"end\":80076,\"start\":80069},{\"end\":80085,\"start\":80083},{\"end\":80097,\"start\":80093},{\"end\":80488,\"start\":80485},{\"end\":80502,\"start\":80495},{\"end\":80513,\"start\":80508},{\"end\":80520,\"start\":80519},{\"end\":80542,\"start\":80536},{\"end\":80544,\"start\":80543},{\"end\":80910,\"start\":80906},{\"end\":80923,\"start\":80915},{\"end\":80935,\"start\":80930},{\"end\":81179,\"start\":81174},{\"end\":81193,\"start\":81186},{\"end\":81208,\"start\":81200},{\"end\":81216,\"start\":81214},{\"end\":81224,\"start\":81222},{\"end\":81239,\"start\":81231},{\"end\":81425,\"start\":81418},{\"end\":81434,\"start\":81431},{\"end\":81446,\"start\":81441},{\"end\":81453,\"start\":81452},{\"end\":81467,\"start\":81463},{\"end\":81844,\"start\":81838},{\"end\":81858,\"start\":81850},{\"end\":81874,\"start\":81865},{\"end\":81885,\"start\":81881},{\"end\":82126,\"start\":82119},{\"end\":82138,\"start\":82131},{\"end\":82154,\"start\":82146},{\"end\":82168,\"start\":82162},{\"end\":82179,\"start\":82173},{\"end\":82331,\"start\":82326},{\"end\":82347,\"start\":82339},{\"end\":82360,\"start\":82353},{\"end\":82369,\"start\":82366},{\"end\":82379,\"start\":82374},{\"end\":82393,\"start\":82386},{\"end\":82789,\"start\":82783},{\"end\":82804,\"start\":82797},{\"end\":82820,\"start\":82812},{\"end\":82828,\"start\":82826},{\"end\":83036,\"start\":83032},{\"end\":83048,\"start\":83043},{\"end\":83062,\"start\":83054},{\"end\":83076,\"start\":83070},{\"end\":83091,\"start\":83087},{\"end\":83103,\"start\":83099},{\"end\":83428,\"start\":83425},{\"end\":83440,\"start\":83434},{\"end\":83453,\"start\":83446},{\"end\":83462,\"start\":83459},{\"end\":83805,\"start\":83800},{\"end\":83816,\"start\":83811},{\"end\":83830,\"start\":83822},{\"end\":83841,\"start\":83837},{\"end\":83856,\"start\":83851},{\"end\":83870,\"start\":83865},{\"end\":84228,\"start\":84223},{\"end\":84236,\"start\":84234},{\"end\":84248,\"start\":84241},{\"end\":84489,\"start\":84482},{\"end\":84501,\"start\":84495},{\"end\":84699,\"start\":84692},{\"end\":84711,\"start\":84705},{\"end\":84730,\"start\":84724},{\"end\":85120,\"start\":85113},{\"end\":85132,\"start\":85126},{\"end\":85144,\"start\":85137},{\"end\":85157,\"start\":85152},{\"end\":85166,\"start\":85163},{\"end\":85176,\"start\":85171}]", "bib_author_last_name": "[{\"end\":62723,\"start\":62717},{\"end\":62747,\"start\":62739},{\"end\":62971,\"start\":62963},{\"end\":62984,\"start\":62978},{\"end\":63229,\"start\":63223},{\"end\":63244,\"start\":63238},{\"end\":63261,\"start\":63252},{\"end\":63462,\"start\":63457},{\"end\":63481,\"start\":63469},{\"end\":63496,\"start\":63489},{\"end\":63512,\"start\":63510},{\"end\":63526,\"start\":63520},{\"end\":63919,\"start\":63915},{\"end\":63931,\"start\":63928},{\"end\":63939,\"start\":63937},{\"end\":63948,\"start\":63946},{\"end\":63958,\"start\":63954},{\"end\":63966,\"start\":63963},{\"end\":64417,\"start\":64413},{\"end\":64429,\"start\":64427},{\"end\":64446,\"start\":64442},{\"end\":64726,\"start\":64715},{\"end\":64749,\"start\":64736},{\"end\":64762,\"start\":64757},{\"end\":64776,\"start\":64772},{\"end\":64790,\"start\":64786},{\"end\":64804,\"start\":64798},{\"end\":64819,\"start\":64812},{\"end\":64832,\"start\":64827},{\"end\":64849,\"start\":64840},{\"end\":64864,\"start\":64858},{\"end\":65236,\"start\":65229},{\"end\":65256,\"start\":65245},{\"end\":65273,\"start\":65263},{\"end\":65549,\"start\":65544},{\"end\":65561,\"start\":65557},{\"end\":65581,\"start\":65575},{\"end\":65595,\"start\":65590},{\"end\":65614,\"start\":65605},{\"end\":65834,\"start\":65831},{\"end\":65842,\"start\":65840},{\"end\":65853,\"start\":65849},{\"end\":65864,\"start\":65859},{\"end\":65874,\"start\":65870},{\"end\":65883,\"start\":65880},{\"end\":65892,\"start\":65888},{\"end\":66156,\"start\":66151},{\"end\":66170,\"start\":66165},{\"end\":66381,\"start\":66378},{\"end\":66408,\"start\":66396},{\"end\":66650,\"start\":66640},{\"end\":66667,\"start\":66660},{\"end\":66688,\"start\":66682},{\"end\":66697,\"start\":66695},{\"end\":66904,\"start\":66898},{\"end\":66916,\"start\":66911},{\"end\":67167,\"start\":67160},{\"end\":67184,\"start\":67175},{\"end\":67616,\"start\":67609},{\"end\":67633,\"start\":67628},{\"end\":67643,\"start\":67639},{\"end\":67658,\"start\":67651},{\"end\":68146,\"start\":68144},{\"end\":68160,\"start\":68157},{\"end\":68176,\"start\":68170},{\"end\":68189,\"start\":68185},{\"end\":68201,\"start\":68198},{\"end\":68212,\"start\":68209},{\"end\":68229,\"start\":68222},{\"end\":68244,\"start\":68236},{\"end\":68552,\"start\":68550},{\"end\":68563,\"start\":68560},{\"end\":68577,\"start\":68572},{\"end\":68593,\"start\":68587},{\"end\":68606,\"start\":68601},{\"end\":68619,\"start\":68614},{\"end\":68634,\"start\":68626},{\"end\":68873,\"start\":68871},{\"end\":68886,\"start\":68882},{\"end\":68900,\"start\":68896},{\"end\":68915,\"start\":68910},{\"end\":68927,\"start\":68924},{\"end\":69372,\"start\":69368},{\"end\":69388,\"start\":69384},{\"end\":69403,\"start\":69400},{\"end\":69731,\"start\":69726},{\"end\":69744,\"start\":69739},{\"end\":69756,\"start\":69753},{\"end\":69766,\"start\":69762},{\"end\":69775,\"start\":69772},{\"end\":70242,\"start\":70237},{\"end\":70255,\"start\":70250},{\"end\":70267,\"start\":70264},{\"end\":70277,\"start\":70273},{\"end\":70286,\"start\":70283},{\"end\":70719,\"start\":70716},{\"end\":70731,\"start\":70727},{\"end\":70744,\"start\":70741},{\"end\":70755,\"start\":70751},{\"end\":70768,\"start\":70764},{\"end\":70779,\"start\":70776},{\"end\":70793,\"start\":70789},{\"end\":71061,\"start\":71058},{\"end\":71069,\"start\":71067},{\"end\":71082,\"start\":71079},{\"end\":71097,\"start\":71093},{\"end\":71110,\"start\":71106},{\"end\":71124,\"start\":71120},{\"end\":71460,\"start\":71454},{\"end\":71474,\"start\":71467},{\"end\":71493,\"start\":71483},{\"end\":71501,\"start\":71495},{\"end\":71769,\"start\":71750},{\"end\":71783,\"start\":71779},{\"end\":71794,\"start\":71790},{\"end\":71810,\"start\":71803},{\"end\":71823,\"start\":71818},{\"end\":71839,\"start\":71832},{\"end\":71848,\"start\":71841},{\"end\":72110,\"start\":72106},{\"end\":72122,\"start\":72117},{\"end\":72136,\"start\":72131},{\"end\":72155,\"start\":72146},{\"end\":72410,\"start\":72402},{\"end\":72424,\"start\":72418},{\"end\":72428,\"start\":72426},{\"end\":72648,\"start\":72642},{\"end\":72658,\"start\":72654},{\"end\":72667,\"start\":72660},{\"end\":72839,\"start\":72833},{\"end\":72854,\"start\":72848},{\"end\":72871,\"start\":72863},{\"end\":73075,\"start\":73070},{\"end\":73091,\"start\":73085},{\"end\":73103,\"start\":73097},{\"end\":73314,\"start\":73312},{\"end\":73331,\"start\":73325},{\"end\":73343,\"start\":73337},{\"end\":73359,\"start\":73353},{\"end\":73777,\"start\":73775},{\"end\":73790,\"start\":73787},{\"end\":73804,\"start\":73802},{\"end\":73960,\"start\":73958},{\"end\":73972,\"start\":73968},{\"end\":73984,\"start\":73981},{\"end\":73999,\"start\":73994},{\"end\":74295,\"start\":74291},{\"end\":74305,\"start\":74303},{\"end\":74316,\"start\":74312},{\"end\":74331,\"start\":74327},{\"end\":74345,\"start\":74341},{\"end\":74356,\"start\":74349},{\"end\":74372,\"start\":74364},{\"end\":74389,\"start\":74381},{\"end\":74408,\"start\":74401},{\"end\":74415,\"start\":74410},{\"end\":74725,\"start\":74722},{\"end\":74739,\"start\":74734},{\"end\":74749,\"start\":74747},{\"end\":74762,\"start\":74757},{\"end\":74777,\"start\":74770},{\"end\":74989,\"start\":74986},{\"end\":75001,\"start\":74999},{\"end\":75016,\"start\":75011},{\"end\":75216,\"start\":75212},{\"end\":75235,\"start\":75229},{\"end\":75448,\"start\":75444},{\"end\":75462,\"start\":75456},{\"end\":75647,\"start\":75641},{\"end\":75658,\"start\":75653},{\"end\":75676,\"start\":75668},{\"end\":75692,\"start\":75686},{\"end\":75705,\"start\":75701},{\"end\":75721,\"start\":75715},{\"end\":75733,\"start\":75730},{\"end\":75750,\"start\":75741},{\"end\":75763,\"start\":75757},{\"end\":75775,\"start\":75770},{\"end\":76035,\"start\":76026},{\"end\":76051,\"start\":76042},{\"end\":76071,\"start\":76063},{\"end\":76087,\"start\":76081},{\"end\":76105,\"start\":76098},{\"end\":76121,\"start\":76115},{\"end\":76138,\"start\":76131},{\"end\":76158,\"start\":76146},{\"end\":76169,\"start\":76164},{\"end\":76186,\"start\":76179},{\"end\":76508,\"start\":76501},{\"end\":76522,\"start\":76515},{\"end\":76537,\"start\":76531},{\"end\":77073,\"start\":77058},{\"end\":77090,\"start\":77082},{\"end\":77107,\"start\":77101},{\"end\":77116,\"start\":77109},{\"end\":77456,\"start\":77453},{\"end\":77475,\"start\":77469},{\"end\":77481,\"start\":77477},{\"end\":77669,\"start\":77660},{\"end\":77681,\"start\":77677},{\"end\":77696,\"start\":77686},{\"end\":77717,\"start\":77705},{\"end\":77738,\"start\":77728},{\"end\":78019,\"start\":78016},{\"end\":78035,\"start\":78029},{\"end\":78051,\"start\":78045},{\"end\":78064,\"start\":78058},{\"end\":78081,\"start\":78072},{\"end\":78099,\"start\":78088},{\"end\":78361,\"start\":78354},{\"end\":78388,\"start\":78377},{\"end\":78412,\"start\":78395},{\"end\":78433,\"start\":78424},{\"end\":78446,\"start\":78442},{\"end\":78462,\"start\":78455},{\"end\":78764,\"start\":78758},{\"end\":78778,\"start\":78773},{\"end\":79202,\"start\":79184},{\"end\":79216,\"start\":79211},{\"end\":79226,\"start\":79218},{\"end\":79434,\"start\":79424},{\"end\":79452,\"start\":79444},{\"end\":79470,\"start\":79462},{\"end\":79486,\"start\":79480},{\"end\":79498,\"start\":79495},{\"end\":79513,\"start\":79507},{\"end\":79751,\"start\":79744},{\"end\":79768,\"start\":79758},{\"end\":79783,\"start\":79777},{\"end\":79808,\"start\":79800},{\"end\":80007,\"start\":80003},{\"end\":80019,\"start\":80017},{\"end\":80029,\"start\":80024},{\"end\":80039,\"start\":80036},{\"end\":80047,\"start\":80044},{\"end\":80057,\"start\":80055},{\"end\":80067,\"start\":80065},{\"end\":80081,\"start\":80077},{\"end\":80091,\"start\":80086},{\"end\":80100,\"start\":80098},{\"end\":80493,\"start\":80489},{\"end\":80506,\"start\":80503},{\"end\":80517,\"start\":80514},{\"end\":80527,\"start\":80521},{\"end\":80534,\"start\":80529},{\"end\":80564,\"start\":80545},{\"end\":80573,\"start\":80566},{\"end\":80913,\"start\":80911},{\"end\":80928,\"start\":80924},{\"end\":80942,\"start\":80936},{\"end\":81184,\"start\":81180},{\"end\":81198,\"start\":81194},{\"end\":81212,\"start\":81209},{\"end\":81220,\"start\":81217},{\"end\":81229,\"start\":81225},{\"end\":81243,\"start\":81240},{\"end\":81429,\"start\":81426},{\"end\":81439,\"start\":81435},{\"end\":81450,\"start\":81447},{\"end\":81461,\"start\":81454},{\"end\":81476,\"start\":81468},{\"end\":81486,\"start\":81478},{\"end\":81496,\"start\":81488},{\"end\":81848,\"start\":81845},{\"end\":81863,\"start\":81859},{\"end\":81879,\"start\":81875},{\"end\":81890,\"start\":81886},{\"end\":82129,\"start\":82127},{\"end\":82144,\"start\":82139},{\"end\":82160,\"start\":82155},{\"end\":82171,\"start\":82169},{\"end\":82184,\"start\":82180},{\"end\":82337,\"start\":82332},{\"end\":82351,\"start\":82348},{\"end\":82364,\"start\":82361},{\"end\":82372,\"start\":82370},{\"end\":82384,\"start\":82380},{\"end\":82399,\"start\":82394},{\"end\":82795,\"start\":82790},{\"end\":82810,\"start\":82805},{\"end\":82824,\"start\":82821},{\"end\":82832,\"start\":82829},{\"end\":83041,\"start\":83037},{\"end\":83052,\"start\":83049},{\"end\":83068,\"start\":83063},{\"end\":83085,\"start\":83077},{\"end\":83097,\"start\":83092},{\"end\":83108,\"start\":83104},{\"end\":83432,\"start\":83429},{\"end\":83444,\"start\":83441},{\"end\":83457,\"start\":83454},{\"end\":83465,\"start\":83463},{\"end\":83809,\"start\":83806},{\"end\":83820,\"start\":83817},{\"end\":83835,\"start\":83831},{\"end\":83849,\"start\":83842},{\"end\":83863,\"start\":83857},{\"end\":83877,\"start\":83871},{\"end\":84232,\"start\":84229},{\"end\":84239,\"start\":84237},{\"end\":84252,\"start\":84249},{\"end\":84493,\"start\":84490},{\"end\":84512,\"start\":84502},{\"end\":84703,\"start\":84700},{\"end\":84722,\"start\":84712},{\"end\":84739,\"start\":84731},{\"end\":85124,\"start\":85121},{\"end\":85135,\"start\":85133},{\"end\":85150,\"start\":85145},{\"end\":85161,\"start\":85158},{\"end\":85169,\"start\":85167},{\"end\":85181,\"start\":85177}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":60545},\"end\":62906,\"start\":62669},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":524106},\"end\":63116,\"start\":62908},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":16902615},\"end\":63450,\"start\":63118},{\"attributes\":{\"doi\":\"arXiv:2005.03675\",\"id\":\"b3\"},\"end\":63804,\"start\":63452},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":202539008},\"end\":64344,\"start\":63806},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":209386433},\"end\":64665,\"start\":64346},{\"attributes\":{\"doi\":\"arXiv:2009.14794\",\"id\":\"b6\"},\"end\":65140,\"start\":64667},{\"attributes\":{\"doi\":\"arXiv:1511.07289\",\"id\":\"b7\"},\"end\":65480,\"start\":65142},{\"attributes\":{\"doi\":\"arXiv:2003.13620\",\"id\":\"b8\"},\"end\":65822,\"start\":65482},{\"attributes\":{\"doi\":\"arXiv:1806.02371\",\"id\":\"b9\"},\"end\":66126,\"start\":65824},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":4506156},\"end\":66311,\"start\":66128},{\"attributes\":{\"doi\":\"arXiv:1912.10206\",\"id\":\"b11\"},\"end\":66577,\"start\":66313},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":85543335},\"end\":66843,\"start\":66579},{\"attributes\":{\"doi\":\"arXiv:1711.04043\",\"id\":\"b13\"},\"end\":67062,\"start\":66845},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":145053868},\"end\":67552,\"start\":67064},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":220713864},\"end\":68072,\"start\":67554},{\"attributes\":{\"doi\":\"arXiv:2005.00687\",\"id\":\"b16\"},\"end\":68490,\"start\":68074},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":213085920},\"end\":68804,\"start\":68492},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":220250007},\"end\":69358,\"start\":68806},{\"attributes\":{\"doi\":\"arXiv:1905.11678\",\"id\":\"b19\"},\"end\":69652,\"start\":69360},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":195497272},\"end\":70163,\"start\":69654},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":195497272},\"end\":70674,\"start\":70165},{\"attributes\":{\"doi\":\"arXiv:2006.10141\",\"id\":\"b22\"},\"end\":71052,\"start\":70676},{\"attributes\":{\"doi\":\"arXiv:2005.10203\",\"id\":\"b23\"},\"end\":71388,\"start\":71054},{\"attributes\":{\"doi\":\"arXiv:2007.04929\",\"id\":\"b24\"},\"end\":71687,\"start\":71390},{\"attributes\":{\"id\":\"b25\"},\"end\":72030,\"start\":71689},{\"attributes\":{\"doi\":\"arXiv:2002.04999\",\"id\":\"b26\"},\"end\":72354,\"start\":72032},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b27\"},\"end\":72572,\"start\":72356},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3144218},\"end\":72787,\"start\":72574},{\"attributes\":{\"doi\":\"arXiv:2001.04451\",\"id\":\"b29\"},\"end\":73029,\"start\":72789},{\"attributes\":{\"id\":\"b30\"},\"end\":73263,\"start\":73031},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":201070021},\"end\":73687,\"start\":73265},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":11118105},\"end\":73950,\"start\":73689},{\"attributes\":{\"doi\":\"arXiv:1801.03226\",\"id\":\"b33\"},\"end\":74214,\"start\":73952},{\"attributes\":{\"doi\":\"arXiv:1910.00760\",\"id\":\"b34\"},\"end\":74689,\"start\":74216},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":170078827},\"end\":74916,\"start\":74691},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":14830880},\"end\":75142,\"start\":74918},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":15539264},\"end\":75353,\"start\":75144},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":209994765},\"end\":75596,\"start\":75355},{\"attributes\":{\"id\":\"b39\"},\"end\":75976,\"start\":75598},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":10659969},\"end\":76440,\"start\":75978},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":3051291},\"end\":76948,\"start\":76442},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":88518244},\"end\":77376,\"start\":76950},{\"attributes\":{\"id\":\"b43\"},\"end\":77619,\"start\":77378},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":206756462},\"end\":77960,\"start\":77621},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":62016134},\"end\":78292,\"start\":77962},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":202770431},\"end\":78681,\"start\":78294},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":208001406},\"end\":79097,\"start\":78683},{\"attributes\":{\"id\":\"b48\"},\"end\":79390,\"start\":79099},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":3292002},\"end\":79665,\"start\":79392},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":207168299},\"end\":79994,\"start\":79667},{\"attributes\":{\"doi\":\"arXiv:1909.01315\",\"id\":\"b51\"},\"end\":80435,\"start\":79996},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":94822},\"end\":80808,\"start\":80437},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":53035645},\"end\":81115,\"start\":80810},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":199466397},\"end\":81416,\"start\":81117},{\"attributes\":{\"doi\":\"arXiv:1802.08773\",\"id\":\"b55\"},\"end\":81773,\"start\":81418},{\"attributes\":{\"doi\":\"arXiv:2006.09136\",\"id\":\"b56\"},\"end\":82080,\"start\":81775},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":208139383},\"end\":82324,\"start\":82082},{\"attributes\":{\"doi\":\"arXiv:1803.07294\",\"id\":\"b58\"},\"end\":82708,\"start\":82326},{\"attributes\":{\"doi\":\"arXiv:2001.05140\",\"id\":\"b59\"},\"end\":83030,\"start\":82710},{\"attributes\":{\"doi\":\"arXiv:2006.06830\",\"id\":\"b60\"},\"end\":83350,\"start\":83032},{\"attributes\":{\"doi\":\"arXiv:1902.00756\",\"id\":\"b61\"},\"end\":83712,\"start\":83352},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":225062001},\"end\":84163,\"start\":83714},{\"attributes\":{\"doi\":\"arXiv:2006.02380\",\"id\":\"b63\"},\"end\":84415,\"start\":84165},{\"attributes\":{\"id\":\"b64\"},\"end\":84619,\"start\":84417},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":1052837},\"end\":85111,\"start\":84621},{\"attributes\":{\"doi\":\"arXiv:2103.03036\",\"id\":\"b66\"},\"end\":85461,\"start\":85113}]", "bib_title": "[{\"end\":62710,\"start\":62669},{\"end\":62947,\"start\":62908},{\"end\":63213,\"start\":63118},{\"end\":63908,\"start\":63806},{\"end\":64408,\"start\":64346},{\"end\":66144,\"start\":66128},{\"end\":66633,\"start\":66579},{\"end\":67151,\"start\":67064},{\"end\":67598,\"start\":67554},{\"end\":68541,\"start\":68492},{\"end\":68863,\"start\":68806},{\"end\":69721,\"start\":69654},{\"end\":70232,\"start\":70165},{\"end\":70710,\"start\":70676},{\"end\":72638,\"start\":72574},{\"end\":73303,\"start\":73265},{\"end\":73767,\"start\":73689},{\"end\":74714,\"start\":74691},{\"end\":74980,\"start\":74918},{\"end\":75204,\"start\":75144},{\"end\":75436,\"start\":75355},{\"end\":76017,\"start\":75978},{\"end\":76493,\"start\":76442},{\"end\":77052,\"start\":76950},{\"end\":77651,\"start\":77621},{\"end\":78003,\"start\":77962},{\"end\":78345,\"start\":78294},{\"end\":78747,\"start\":78683},{\"end\":79416,\"start\":79392},{\"end\":79735,\"start\":79667},{\"end\":80483,\"start\":80437},{\"end\":80904,\"start\":80810},{\"end\":81172,\"start\":81117},{\"end\":82117,\"start\":82082},{\"end\":83798,\"start\":83714},{\"end\":84690,\"start\":84621}]", "bib_author": "[{\"end\":62725,\"start\":62712},{\"end\":62749,\"start\":62725},{\"end\":62973,\"start\":62949},{\"end\":62986,\"start\":62973},{\"end\":63231,\"start\":63215},{\"end\":63246,\"start\":63231},{\"end\":63263,\"start\":63246},{\"end\":63464,\"start\":63452},{\"end\":63483,\"start\":63464},{\"end\":63498,\"start\":63483},{\"end\":63514,\"start\":63498},{\"end\":63528,\"start\":63514},{\"end\":63921,\"start\":63910},{\"end\":63933,\"start\":63921},{\"end\":63941,\"start\":63933},{\"end\":63950,\"start\":63941},{\"end\":63960,\"start\":63950},{\"end\":63968,\"start\":63960},{\"end\":64419,\"start\":64410},{\"end\":64431,\"start\":64419},{\"end\":64448,\"start\":64431},{\"end\":64728,\"start\":64705},{\"end\":64751,\"start\":64728},{\"end\":64764,\"start\":64751},{\"end\":64778,\"start\":64764},{\"end\":64792,\"start\":64778},{\"end\":64806,\"start\":64792},{\"end\":64821,\"start\":64806},{\"end\":64834,\"start\":64821},{\"end\":64851,\"start\":64834},{\"end\":64866,\"start\":64851},{\"end\":65238,\"start\":65218},{\"end\":65258,\"start\":65238},{\"end\":65275,\"start\":65258},{\"end\":65551,\"start\":65539},{\"end\":65563,\"start\":65551},{\"end\":65583,\"start\":65563},{\"end\":65597,\"start\":65583},{\"end\":65616,\"start\":65597},{\"end\":65836,\"start\":65824},{\"end\":65844,\"start\":65836},{\"end\":65855,\"start\":65844},{\"end\":65866,\"start\":65855},{\"end\":65876,\"start\":65866},{\"end\":65885,\"start\":65876},{\"end\":65894,\"start\":65885},{\"end\":66158,\"start\":66146},{\"end\":66172,\"start\":66158},{\"end\":66383,\"start\":66372},{\"end\":66410,\"start\":66383},{\"end\":66652,\"start\":66635},{\"end\":66669,\"start\":66652},{\"end\":66690,\"start\":66669},{\"end\":66699,\"start\":66690},{\"end\":66906,\"start\":66891},{\"end\":66918,\"start\":66906},{\"end\":67169,\"start\":67153},{\"end\":67186,\"start\":67169},{\"end\":67618,\"start\":67600},{\"end\":67635,\"start\":67618},{\"end\":67645,\"start\":67635},{\"end\":67660,\"start\":67645},{\"end\":68148,\"start\":68137},{\"end\":68162,\"start\":68148},{\"end\":68178,\"start\":68162},{\"end\":68191,\"start\":68178},{\"end\":68203,\"start\":68191},{\"end\":68214,\"start\":68203},{\"end\":68231,\"start\":68214},{\"end\":68246,\"start\":68231},{\"end\":68554,\"start\":68543},{\"end\":68565,\"start\":68554},{\"end\":68579,\"start\":68565},{\"end\":68595,\"start\":68579},{\"end\":68608,\"start\":68595},{\"end\":68621,\"start\":68608},{\"end\":68636,\"start\":68621},{\"end\":68875,\"start\":68865},{\"end\":68888,\"start\":68875},{\"end\":68902,\"start\":68888},{\"end\":68917,\"start\":68902},{\"end\":68929,\"start\":68917},{\"end\":69374,\"start\":69360},{\"end\":69390,\"start\":69374},{\"end\":69405,\"start\":69390},{\"end\":69733,\"start\":69723},{\"end\":69746,\"start\":69733},{\"end\":69758,\"start\":69746},{\"end\":69768,\"start\":69758},{\"end\":69777,\"start\":69768},{\"end\":70244,\"start\":70234},{\"end\":70257,\"start\":70244},{\"end\":70269,\"start\":70257},{\"end\":70279,\"start\":70269},{\"end\":70288,\"start\":70279},{\"end\":70721,\"start\":70712},{\"end\":70733,\"start\":70721},{\"end\":70746,\"start\":70733},{\"end\":70757,\"start\":70746},{\"end\":70770,\"start\":70757},{\"end\":70781,\"start\":70770},{\"end\":70795,\"start\":70781},{\"end\":71063,\"start\":71054},{\"end\":71071,\"start\":71063},{\"end\":71084,\"start\":71071},{\"end\":71099,\"start\":71084},{\"end\":71112,\"start\":71099},{\"end\":71126,\"start\":71112},{\"end\":71462,\"start\":71452},{\"end\":71476,\"start\":71462},{\"end\":71495,\"start\":71476},{\"end\":71503,\"start\":71495},{\"end\":71771,\"start\":71743},{\"end\":71785,\"start\":71771},{\"end\":71796,\"start\":71785},{\"end\":71812,\"start\":71796},{\"end\":71825,\"start\":71812},{\"end\":71841,\"start\":71825},{\"end\":71850,\"start\":71841},{\"end\":72112,\"start\":72100},{\"end\":72124,\"start\":72112},{\"end\":72138,\"start\":72124},{\"end\":72157,\"start\":72138},{\"end\":72412,\"start\":72400},{\"end\":72426,\"start\":72412},{\"end\":72430,\"start\":72426},{\"end\":72650,\"start\":72640},{\"end\":72660,\"start\":72650},{\"end\":72669,\"start\":72660},{\"end\":72841,\"start\":72826},{\"end\":72856,\"start\":72841},{\"end\":72873,\"start\":72856},{\"end\":73077,\"start\":73065},{\"end\":73093,\"start\":73077},{\"end\":73105,\"start\":73093},{\"end\":73316,\"start\":73305},{\"end\":73333,\"start\":73316},{\"end\":73345,\"start\":73333},{\"end\":73361,\"start\":73345},{\"end\":73779,\"start\":73769},{\"end\":73792,\"start\":73779},{\"end\":73806,\"start\":73792},{\"end\":73962,\"start\":73952},{\"end\":73974,\"start\":73962},{\"end\":73986,\"start\":73974},{\"end\":74001,\"start\":73986},{\"end\":74297,\"start\":74284},{\"end\":74307,\"start\":74297},{\"end\":74318,\"start\":74307},{\"end\":74333,\"start\":74318},{\"end\":74347,\"start\":74333},{\"end\":74358,\"start\":74347},{\"end\":74374,\"start\":74358},{\"end\":74391,\"start\":74374},{\"end\":74410,\"start\":74391},{\"end\":74417,\"start\":74410},{\"end\":74727,\"start\":74716},{\"end\":74741,\"start\":74727},{\"end\":74751,\"start\":74741},{\"end\":74764,\"start\":74751},{\"end\":74779,\"start\":74764},{\"end\":74991,\"start\":74982},{\"end\":75003,\"start\":74991},{\"end\":75018,\"start\":75003},{\"end\":75218,\"start\":75206},{\"end\":75237,\"start\":75218},{\"end\":75450,\"start\":75438},{\"end\":75464,\"start\":75450},{\"end\":75649,\"start\":75636},{\"end\":75660,\"start\":75649},{\"end\":75678,\"start\":75660},{\"end\":75694,\"start\":75678},{\"end\":75707,\"start\":75694},{\"end\":75723,\"start\":75707},{\"end\":75735,\"start\":75723},{\"end\":75752,\"start\":75735},{\"end\":75765,\"start\":75752},{\"end\":75777,\"start\":75765},{\"end\":76037,\"start\":76019},{\"end\":76053,\"start\":76037},{\"end\":76073,\"start\":76053},{\"end\":76089,\"start\":76073},{\"end\":76107,\"start\":76089},{\"end\":76123,\"start\":76107},{\"end\":76140,\"start\":76123},{\"end\":76160,\"start\":76140},{\"end\":76171,\"start\":76160},{\"end\":76188,\"start\":76171},{\"end\":76510,\"start\":76495},{\"end\":76524,\"start\":76510},{\"end\":76539,\"start\":76524},{\"end\":77075,\"start\":77054},{\"end\":77092,\"start\":77075},{\"end\":77109,\"start\":77092},{\"end\":77118,\"start\":77109},{\"end\":77458,\"start\":77451},{\"end\":77477,\"start\":77458},{\"end\":77483,\"start\":77477},{\"end\":77671,\"start\":77653},{\"end\":77683,\"start\":77671},{\"end\":77698,\"start\":77683},{\"end\":77719,\"start\":77698},{\"end\":77740,\"start\":77719},{\"end\":78021,\"start\":78005},{\"end\":78037,\"start\":78021},{\"end\":78053,\"start\":78037},{\"end\":78066,\"start\":78053},{\"end\":78083,\"start\":78066},{\"end\":78101,\"start\":78083},{\"end\":78363,\"start\":78347},{\"end\":78390,\"start\":78363},{\"end\":78414,\"start\":78390},{\"end\":78435,\"start\":78414},{\"end\":78448,\"start\":78435},{\"end\":78464,\"start\":78448},{\"end\":78766,\"start\":78749},{\"end\":78780,\"start\":78766},{\"end\":79204,\"start\":79177},{\"end\":79218,\"start\":79204},{\"end\":79228,\"start\":79218},{\"end\":79436,\"start\":79418},{\"end\":79454,\"start\":79436},{\"end\":79472,\"start\":79454},{\"end\":79488,\"start\":79472},{\"end\":79500,\"start\":79488},{\"end\":79515,\"start\":79500},{\"end\":79753,\"start\":79737},{\"end\":79770,\"start\":79753},{\"end\":79785,\"start\":79770},{\"end\":79810,\"start\":79785},{\"end\":80009,\"start\":79996},{\"end\":80021,\"start\":80009},{\"end\":80031,\"start\":80021},{\"end\":80041,\"start\":80031},{\"end\":80049,\"start\":80041},{\"end\":80059,\"start\":80049},{\"end\":80069,\"start\":80059},{\"end\":80083,\"start\":80069},{\"end\":80093,\"start\":80083},{\"end\":80102,\"start\":80093},{\"end\":80495,\"start\":80485},{\"end\":80508,\"start\":80495},{\"end\":80519,\"start\":80508},{\"end\":80529,\"start\":80519},{\"end\":80536,\"start\":80529},{\"end\":80566,\"start\":80536},{\"end\":80575,\"start\":80566},{\"end\":80915,\"start\":80906},{\"end\":80930,\"start\":80915},{\"end\":80944,\"start\":80930},{\"end\":81186,\"start\":81174},{\"end\":81200,\"start\":81186},{\"end\":81214,\"start\":81200},{\"end\":81222,\"start\":81214},{\"end\":81231,\"start\":81222},{\"end\":81245,\"start\":81231},{\"end\":81431,\"start\":81418},{\"end\":81441,\"start\":81431},{\"end\":81452,\"start\":81441},{\"end\":81463,\"start\":81452},{\"end\":81478,\"start\":81463},{\"end\":81488,\"start\":81478},{\"end\":81498,\"start\":81488},{\"end\":81850,\"start\":81838},{\"end\":81865,\"start\":81850},{\"end\":81881,\"start\":81865},{\"end\":81892,\"start\":81881},{\"end\":82131,\"start\":82119},{\"end\":82146,\"start\":82131},{\"end\":82162,\"start\":82146},{\"end\":82173,\"start\":82162},{\"end\":82186,\"start\":82173},{\"end\":82339,\"start\":82326},{\"end\":82353,\"start\":82339},{\"end\":82366,\"start\":82353},{\"end\":82374,\"start\":82366},{\"end\":82386,\"start\":82374},{\"end\":82401,\"start\":82386},{\"end\":82797,\"start\":82783},{\"end\":82812,\"start\":82797},{\"end\":82826,\"start\":82812},{\"end\":82834,\"start\":82826},{\"end\":83043,\"start\":83032},{\"end\":83054,\"start\":83043},{\"end\":83070,\"start\":83054},{\"end\":83087,\"start\":83070},{\"end\":83099,\"start\":83087},{\"end\":83110,\"start\":83099},{\"end\":83434,\"start\":83425},{\"end\":83446,\"start\":83434},{\"end\":83459,\"start\":83446},{\"end\":83467,\"start\":83459},{\"end\":83811,\"start\":83800},{\"end\":83822,\"start\":83811},{\"end\":83837,\"start\":83822},{\"end\":83851,\"start\":83837},{\"end\":83865,\"start\":83851},{\"end\":83879,\"start\":83865},{\"end\":84234,\"start\":84223},{\"end\":84241,\"start\":84234},{\"end\":84254,\"start\":84241},{\"end\":84495,\"start\":84482},{\"end\":84514,\"start\":84495},{\"end\":84705,\"start\":84692},{\"end\":84724,\"start\":84705},{\"end\":84741,\"start\":84724},{\"end\":85126,\"start\":85113},{\"end\":85137,\"start\":85126},{\"end\":85152,\"start\":85137},{\"end\":85163,\"start\":85152},{\"end\":85171,\"start\":85163},{\"end\":85183,\"start\":85171}]", "bib_venue": "[{\"end\":64077,\"start\":64031},{\"end\":67327,\"start\":67265},{\"end\":67839,\"start\":67758},{\"end\":69108,\"start\":69027},{\"end\":69926,\"start\":69860},{\"end\":70437,\"start\":70371},{\"end\":73490,\"start\":73434},{\"end\":76722,\"start\":76639},{\"end\":78901,\"start\":78849},{\"end\":84884,\"start\":84821},{\"end\":62774,\"start\":62749},{\"end\":62993,\"start\":62986},{\"end\":63268,\"start\":63263},{\"end\":63606,\"start\":63544},{\"end\":64029,\"start\":63968},{\"end\":64495,\"start\":64448},{\"end\":64703,\"start\":64667},{\"end\":65216,\"start\":65142},{\"end\":65537,\"start\":65482},{\"end\":65953,\"start\":65910},{\"end\":66207,\"start\":66172},{\"end\":66370,\"start\":66313},{\"end\":66703,\"start\":66699},{\"end\":66889,\"start\":66845},{\"end\":67263,\"start\":67186},{\"end\":67756,\"start\":67660},{\"end\":68135,\"start\":68074},{\"end\":68640,\"start\":68636},{\"end\":69025,\"start\":68929},{\"end\":69484,\"start\":69421},{\"end\":69858,\"start\":69777},{\"end\":70369,\"start\":70288},{\"end\":70842,\"start\":70811},{\"end\":71199,\"start\":71142},{\"end\":71450,\"start\":71390},{\"end\":71741,\"start\":71689},{\"end\":72098,\"start\":72032},{\"end\":72398,\"start\":72356},{\"end\":72673,\"start\":72669},{\"end\":72824,\"start\":72789},{\"end\":73063,\"start\":73031},{\"end\":73432,\"start\":73361},{\"end\":73813,\"start\":73806},{\"end\":74061,\"start\":74017},{\"end\":74282,\"start\":74216},{\"end\":74786,\"start\":74779},{\"end\":75022,\"start\":75018},{\"end\":75241,\"start\":75237},{\"end\":75468,\"start\":75464},{\"end\":75634,\"start\":75598},{\"end\":76192,\"start\":76188},{\"end\":76637,\"start\":76539},{\"end\":77149,\"start\":77118},{\"end\":77449,\"start\":77378},{\"end\":77776,\"start\":77740},{\"end\":78112,\"start\":78101},{\"end\":78471,\"start\":78464},{\"end\":78847,\"start\":78780},{\"end\":79175,\"start\":79099},{\"end\":79522,\"start\":79515},{\"end\":79814,\"start\":79810},{\"end\":80192,\"start\":80118},{\"end\":80609,\"start\":80575},{\"end\":80948,\"start\":80944},{\"end\":81250,\"start\":81245},{\"end\":81574,\"start\":81514},{\"end\":81836,\"start\":81775},{\"end\":82195,\"start\":82186},{\"end\":82495,\"start\":82417},{\"end\":82781,\"start\":82710},{\"end\":83169,\"start\":83126},{\"end\":83423,\"start\":83352},{\"end\":83928,\"start\":83879},{\"end\":84221,\"start\":84165},{\"end\":84480,\"start\":84417},{\"end\":84819,\"start\":84741},{\"end\":85265,\"start\":85199}]"}}}, "year": 2023, "month": 12, "day": 17}