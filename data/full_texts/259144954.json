{"id": 259144954, "updated": "2023-10-09 14:16:48.0", "metadata": {"title": "SqueezeLLM: Dense-and-Sparse Quantization", "authors": "[{\"first\":\"Sehoon\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Coleman\",\"last\":\"Hooper\",\"middle\":[]},{\"first\":\"Amir\",\"last\":\"Gholami\",\"middle\":[]},{\"first\":\"Zhen\",\"last\":\"Dong\",\"middle\":[]},{\"first\":\"Xiuyu\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Sheng\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Mahoney\",\"middle\":[\"W.\"]},{\"first\":\"Kurt\",\"last\":\"Keutzer\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing model weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3x speedup compared to the baseline. Our code is open-sourced and available online.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2306.07629", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2306-07629", "doi": "10.48550/arxiv.2306.07629"}}, "content": {"source": {"pdf_hash": "3b7ef6f9f27e33e6a4e3bfac90dcb01ab09718bc", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2306.07629v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "94935f0d22d5a147876727b6debe89e5d1b1a80b", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/3b7ef6f9f27e33e6a4e3bfac90dcb01ab09718bc.txt", "contents": "\nSQUEEZELLM: DENSE-AND-SPARSE QUANTIZATION\n4 Oct 2023\n\nSehoon Kim sehoonkim@berkeley.edu \nhttps://github\n\n\nColeman Hooper chooper@berkeley.edu \nhttps://github\n\n\nAmir Gholami amirgh@berkeley.edu \nZhen Dong zhendong@berkeley.edu \nhttps://github\n\n\nXiuyu Li \nhttps://github\n\n\nSheng Shen sheng.s@berkeley.edu \nhttps://github\n\n\nMichael W Mahoney mahoneymw@berkeley.edu \nKurt Keutzer keutzer@berkeley.edu \nhttps://github\n\n\nU C Berkeley \nhttps://github\n\n\nIcsi \nLbnl \n\nSqueezeAILab\nSqueezeLLM\n\n\nRTN SqueezeLLM 28.26 18.08 7.75 7.67 7.56\n\nSQUEEZELLM: DENSE-AND-SPARSE QUANTIZATION\n4 Oct 2023070012AFFEA789B1E536D6C3CB443344arXiv:2306.07629v2[cs.CL]\nGenerative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks.However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements.This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models.In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference.While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation.To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint.Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format.When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1\u00d7 as compared to the state-of-the-art methods with the same memory requirement.Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3\u00d7 speedup compared to the baseline.Our code is open-sourced and available online 1 .\n\nINTRODUCTION\n\nRecent advances in Large Language Models (LLMs) trained on massive text corpora, with up to hundreds of billions of parameters, have showcased their remarkable problem-solving capabilities across various domains Brown et al. (2020); Raffel et al. (2020); Scao et al. (2022); Du et al. (2022); Hoffmann et al. (2022); Chowdhery et al. (2022); Smith et al. (2022); Zhang et al. (2022); Thoppilan et al. (2022); Touvron et al. (2023a).However, deploying these models for inference has been a significant challenge due to their demanding resource requirements.For instance, the LLaMA-65B Touvron et al. (2021) model requires at least 130GB of RAM to deploy in FP16, which exceeds current GPU capacity.Even storing such large-sized models has become costly and complex.\n\nAs will be discussed in Sec. 3, the main performance bottleneck in LLM inference for generative tasks is memory bandwidth rather than compute.This means that the speed at which we can load and store parameters becomes the primary latency bottleneck for memory-bound problems, rather than arithmetic computations.However, recent advancements in memory bandwidth technology have been significantly slow compared to the improvements in computes, leading to the phenomenon known as the Memory Wall Patterson (2004).Consequently, researchers have turned their attention to exploring algorithmic methods to overcome this challenge.\n\nFigure 1: (Left) SqueezeLLM incorporates two key approaches: (i) sensitivity-based non-uniform quantization (Sec.4.1), where quantization bins are allocated closer to sensitive values, and (ii) the Dense-and-Sparse decomposition (Sec.4.2), which retains both sensitive values and outlier values as full-precision sparse format.When applied to LLaMA-7B with 3-bit quantization, our method outperforms the state-of-the-art methods Frantar et al. (2022); Lin et al. (2023) by a large perplexity margin of over 0.3 on the C4 benchmark.(Right) By applying our methods to LLaMA models of varying sizes, we can achieve improved trade-offs between perplexity and model size.\n\nOne promising approach is quantization, where model parameters are stored at lower precision, instead of the typical 16 or 32-bit precision used for training.For instance, it has been demonstrated that LLM models can be stored in 8-bit precision without performance degradation Yao et al. (2022), where 8-bit quantization not only improves the storage requirements by half but also has the potential to improve inference latency and throughput.As a result, there has been significant research interest in quantizing models to even lower precisions.A pioneering approach is GPTQ Frantar et al. (2022) which uses a training-free quantization technique that achieves near-lossless 4-bit quantization for large LLM models with over tens of billions of parameters.However, achieving high quantization performance remains challenging, particularly with lower bit precision and for relatively smaller models (e.g., < 50B parameters) such as the recent LLaMA Touvron et al. (2023a).\n\nIn this paper, we conduct an extensive study of low-bit precision quantization and identify limitations in existing approaches.Building upon these insights, we propose a novel solution that achieves lossless compression and improved quantization performance even at precisions as low as 3 bits.\n\nContributions.We first present performance modeling results demonstrating that the memory, rather than the compute, is the primary bottleneck in LLM inference with generative tasks (Sec.3 and Fig. 2).Building on this insight, we introduce SqueezeLLM, a post-training quantization framework with a novel sensitivity-based non-uniform quantization and Dense-and-Sparse decomposition.These techniques enable ultra-low-bit precision with reduced model sizes and faster inference without compromising model performance.Our detailed contributions include:\n\n\u2022 Sensitivity-based Non-Uniform Quantization: We demonstrate that uniform quantization, as commonly adopted in prior works, is sub-optimal for LLM inference for two reasons.First, the weight distributions in LLMs exhibit clear non-uniform patterns (Fig. 3).Second, the inference computation in prior works does not benefit from uniform quantization as the arithmetic is performed in FP16 precision, not in reduced precision.To address these, we propose a novel sensitivity-based non-uniform quantization method to achieve a more optimal quantization scheme for LLMs.Our approach significantly improves the perplexity of the LLaMA-7B model at 3-bit precision from 28.26 of uniform quantization to 7.75 on the C4 dataset (Sec.4.1).\n\n\u2022 Dense-and-Sparse Quantization: We observe that weights in LLMs contain significant outliers, making low-bit quantization extremely challenging.To address this, we propose a simple solution that decomposes weights into dense and sparse components.The sparse part holds outlier values in full precision using efficient sparse storage methods and leverages an efficient sparse kernel for minimal inference overhead.This allows the dense part to have a more compact range (up to 10\u00d7) and aids quantization.By extracting only 0.45% of the weight values as the sparse component, we further improve the perplexity of LLaMA-7B from 7.75 to 7.58 on the C4 dataset (Sec.4.2).\n\n\u2022 Evaluation: We extensively test SqueezeLLM on various models on language modeling tasks using the C4 and WikiText2 benchmarks, where we find that SqueezeLLM consistently outperforms existing quantization methods by a large margin across different bit precisions (Sec.5.2).We also demonstrate the potential of SqueezeLLM in quantizing instruction following models by applying it to the Vicuna models Chiang et al. (2023) on the MMLU benchmark Hendrycks et al. (2021) and the Vicuna benchmark Chiang et al. (2023) (Sec. 5.3).Furthermore, our deployed models on A6000 GPUs also exhibit significant gains in latency of up to 2.4\u00d7 compared to the FP16 baseline, showcasing the effectiveness of our method in terms of both quantization performance and inference efficiency (Sec.5.4).\n\n\nRELATED WORK\n\nIn Sec.A.1, we offer an overview and related works of Transformer quantization, with a particular emphasis on Post-Training Quantization (PTQ) and non-uniform quantization, which are the primary focus of our work.Among the various challenges in low-bit Transformer quantization, one key issue is the presence of outliers Kovaleva et al. (2021), which can unnecessarily increase the quantization range.2023) also explores a method for extracting outliers in the context of quantization.However, SpQR employs a different sensitivity metric based on the Optimal Brain Surgeon (OBS) framework Hassibi et al. (1993); Hassibi & Stork (1993), where the weights are quantized in a way that the output activations of each layer are not perturbed.In contrast, our approach is based on the Optimal Brain Damage (OBD) framework LeCun et al. (1990) where the weights are quantized to preserve the final output of the model.While both approaches show promise, we have observed that the OBD method yields better quantization performance since it is a direct measure of the end-to-end performance degradation after quantization (Sec.A.4.4).\n\nMore importantly, SqueezeLLM avoids techniques that can introduce high overhead and complexity when implementing lossless quantization.First, SqueezeLLM does not incorporate grouping.\n\nOur Dense-and-Sparse scheme provides a direct solution to prevent outlier values from negatively impacting quantization performance, eliminating the need for using the grouping strategy as an indirect solution (Sec.A.4.3).In contrast, SpQR requires fine-grained grouping (e.g., group size 16) which increases the model size and complicates the quantization pipeline by necessitating the bilevel quantization scheme.Second, the sensitivity-based non-uniform quantization in SqueezeLLM allows for much smaller (e.g., 0.05%) or even zero sparsity levels to achieve accurate quantization.This is crucial for reducing the model size as well as inference speed since higher sparsity levels can degrade inference latency.By avoiding grouping and utilizing smaller or zero sparsity levels, SqueezeLLM achieves accurate and fast quantization while pushing the average bit precision down to 3-bit, all while employing a simpler quantization pipeline and implementation.\n\nAnother concurrent work is AWQ Lin et al. (2023) which improves the weight-only quantization scheme for LLMs by introducing scaling factors to reduce the quantization error of a few important weights.However, their approach is also based on the OBS framework, where sensitivity is determined by the magnitude of activations.In Sec. 5, we demonstrate that our method consistently outperforms AWQ in terms of quantization performance across various models and applications.\n\n\nMEMORY WALL\n\nInference behavior broadly falls into two categories: compute-bound inference that is limited by computational throughput, and memory-bound inference that is bottlenecked by the rate at which data can be fed into the processing cores from memory.Arithmetic intensity, the ratio of compute to memory operations, is a typical metric used to assess this behavior.High and low arithmetic intensity indicates a compute-bound and memory-bound problem, respectively.For memory-bound problems, the speedup can be achieved by reducing the memory traffic rather than compute since the compute units in hardware are often underutilized waiting to receive data from memory.Generative LLM inference exhibits extremely low arithmetic intensity compared to other workloads2 Kim et al. (2023).This is because it consists almost entirely of matrix-vector operations, which limits the achievable data reuse as each weight load can only process a single vector for a single token, and cannot be amortized across the multiple vectors for different tokens.This low arithmetic intensity needs to be contrasted with the compute operations on a typical GPU which is orders of magnitude higher than the memory operations3 .The disparity between compute and memory bandwidth, along with the growing memory requirements of deep learning, has been termed the Memory Wall problem Gholami et al. (2021b).To further illustrate this problem in generative LLMs, we used a simple roofline-based performance modeling approach Kim et al. (2023) to study LLaMA-7B's runtime on an A5000 GPU with different bit precisions (Fig. 2).Here, we assume that all computations are kept at FP16.Despite this, we can clearly see that the latency decreases linearly as we reduce the bit precision, indicating that the main bottleneck is memory, not compute.In summary, in generative LLM inference, loading weight matrices into memory is the primary bottleneck, while the cost of dequantization and FP16 computation is relatively insignificant.Thus, by quantizing just the weights to lower precision, while leaving the activations in full precision, we can attain significant speedup as well as reduced model size.Given this insight, the appropriate strategy is to minimize the memory size even if it may add overhead to arithmetic operations.\n\n\nMETHODOLOGY\n\n4.1 SENSITIVITY-BASED NON-UNIFORM QUANTIZATION In Fig. 3 (Left), we plot an exemplary weight distribution in LLaMA-7B that demonstrates a nonuniform pattern.The main task for quantization is to find an optimal way to allocate distinct quantized values (e.g., 8 for 3 bits) in a way that preserves model performance.As discussed, a widely used approach in the recent LLM quantization works Frantar et al. (2022); Dettmers et al. (2023); Lin et al. (2023) is uniform quantization where the weight range is evenly divided into bins, and each bin is represented by a single integer number.This has two main issues.First, uniformly distributing quantized values is sub-optimal as weight distributions are typically non-uniform.Second, while the main advantage of uniform quantization is efficient integer computation, this does not lead to end-to-end latency improvement in memory-bound LLM inference.Therefore, we have chosen non-uniform quantization, which allows for a more flexible allocation of the representative values.\n\nFinding an optimal non-uniform quantization configuration translates into solving a k-means problem.Given a weight distribution, the goal is to determine k centroids that best represent the values (e.g., k=8 for 3-bit).This optimization problem for non-uniform quantization can be formulated as where W denotes the weights and W Q is the corresponding quantized weights (i.e., [Q(w) for\nQ(w) * = arg min Q \u2225W \u2212 W Q \u2225 2 2 ,(1)w \u2208 W ]), represented by k distinct values {q 1 , \u2022 \u2022 \u2022 , q k }.\nHere, the optimal solution Q(w) * can be obtained by 1-dimensional k-means clustering, which clusters the parameters into k clusters and assign the centroid of each cluster as q j 's.While this already outperforms uniform quantization, we propose an improved sensitivity-based clustering algorithm.\n\nSensitivity-Based K-means Clustering.The quantization objective is to represent the model weights with low-bit precision with minimal perturbation in the model output Dong et al. (2019).\n\nWhile quantization introduces perturbations in each layer, we need to minimize the overall perturbation with respect to the final loss term, rather than focusing on individual layers, as it provides a more direct measure of the end-to-end performance degradation after quantization LeCun et al. (1990).To achieve this, we need to place the k-means centroids closer to the values that are more sensitive with respect to the final loss, rather than treating all weight values equally as in Eq. 1.To determine more sensitive values, we perform Taylor expansion to analyze how the loss changes in response to perturbations in the weights W :\nL(W Q ) \u2243 L(W ) \u2212 g \u22a4 (W \u2212 W Q ) + 1 2 (W \u2212 W Q ) \u22a4 H(W \u2212 W Q ) (2)\nwhere g is the gradient and\nH = E[ \u2202 2 \u2202W 2 L(W )]\nis the Hessian of the loss at W . Assuming that the model has converged to a local minimum, the gradient g can be approximated as zero which gives us the following formula for computing how much the model gets perturbed after quantization:\nQ(w) * = arg min Q (W \u2212 W Q ) \u22a4 H(W \u2212 W Q ).\n(3)\n\nIn the new optimization target, as compared to Eq. 1, the perturbation of each weight after quantization, i.e., W \u2212 W Q , is weighted by the scaling factor introduced by the second-order derivative, H.This highlights the importance of minimizing perturbations for weights that have large Hessian values, as they have a greater impact on the overall perturbation of the final output.In other words, the second-order derivative serves as a measure of importance for each weight value.\n\nDue to the cost of computing the Hessian, we use an approximation to the Hessian based on the Fisher information matrix F, which can be calculated over a sample dataset D as\nH \u2243 F = 1 |D| d\u2208D g d g d \u22a4 .\nThis only requires computing gradient for a set of samples, which can be calculated efficiently with existing frameworks.To make the optimization objective in Eq. 3 more feasible, we further approximate the Fisher information matrix as a diagonal matrix by assuming that the cross-weight interactions are negligible.This simplifies our objective target as follows:\nQ(w) * \u2243 arg min Q (W \u2212 W Q ) \u22a4 diag(F)(W \u2212 W Q ) = arg min Q N i=1 F ii w i \u2212 Q(w i ) 2 . (4)\nAn important consequence of Eq. 4 is the weighted k-means clustering setting, where the centroids will be pulled closer to these sensitive weight values.In , which demonstrate that \u223c99.9% of the weights are concentrated in a narrow range of \u223c10% of the entire distribution.Naively quantizing the weights with such large range, will significantly degrade performance, especially at low precisions such as 3-bits.However, the observation in Fig. 4 also implies opportunity.The range of the weight values can be contracted by a factor of 10\u00d7 simply by removing a small number of outlier values (e.g., 0.1%), yielding a significant improvement in quantization resolution.This will then help the sensitivity-based kmeans centroids to focus more on the sensitive values rather than a few outliers.\n\nMotivated by this, we introduce a method to filter out outliers from the weight matrix W by performing a simple yet effective decomposition into a sparse matrix (S) containing the outliers and the remaining dense matrix (D) that can be quantized much more effectively thanks to its significantly reduced range of values.That is,\nW = D + S where D = W [T min \u2264 w \u2264 T max ] and S = W [w < T min or w > T max ].\nHere, T min/max are thresholds that define outliers based on the percentile of the distribution.\n\nImportantly, the overhead of this decomposition is minimal, since the number of outlier values is small.Even in the most aggressive quantization experiments, we did not find it necessary to use > 0.5% of sparsity.Therefore, the sparse matrix can be stored efficiently using methods like compressed sparse row (CSR) format.Inference is also straightforward with the decomposition as in W X = DX + SX, two kernels for dense and sparse multiplication can be overlapped, and the sparse part (SX) can benefit from sparse kernels (Sec.4.3).\n\nSensitivity-Based Sparse Matrix.Beyond extracting outliers as a sparse matrix, we have also found it beneficial to extract a few highly sensitive values in weight matrices to make sure those values are represented exactly without any error.These values can be easily identified based on the Fisher information (Sec.4.1).This offers two benefits.First, maintaining these sensitive values with FP16 precision minimizes their impact on the final output.Second, it prevents the centroids of Eq.4 from skewing towards the sensitive values, thus enhancing quantization for less sensitive weights.\n\nWe have observed that extracting only 0.05% of these sensitive values across layers substantially enhances quantization performance (Sec.A.4). Altogether, with 3-bit LLaMA-7B, extracting 0.45% of outlier and sensitive values further reduces the perplexity from 7.67 to 7.56 (Fig. 1 and Sec.5.2).\n\n\nDENSE-AND-SPARSE KERNEL IMPLEMENTATION\n\nWhile a natural question to consider is the impact of both the non-uniform and Dense-and-Sparse quantization on latency, we find it straightforward to implement them efficiently.We implement 3/4-bit LUT-based kernels for matrix-vector multiplication between compressed weight matrices and uncompressed activation vectors.These kernels load the compressed weights and dequantize them piece-by-piece to minimize memory bandwidth utilization.The compressed matrices store 3/4-bit indices, which correspond to LUT entries containing FP16 values associated with the bins obtained from non-uniform quantization.After dequantization, all arithmetic is performed in FP16.\n\nTo efficiently process our Dense-and-Sparse representation, we also develop CUDA kernels for sparse matrix-vector multiplication that load a matrix in CSR format and a dense activation vector, inspired by Evtushenko (2019).Since the non-zero entry distributions are highly skewed across rows (Sec.A.3), assigning a single thread per row can be inefficient due to the unbalanced amount of work assigned to different threads.Thus, we implement balanced hybrid kernels based on Flegar & Quintana-Ort\u00ed (2017) by assigning an equal number of nonzeros per thread; this leads to additional synchronization across threads since one row may be divided across several threads, but leads to a balanced work assignment.We set the number of threads such that there were 10 nonzero values assigned to each thread.The dense non-uniform kernel and balanced sparse kernels are launched in one call to avoid overhead from summing the output vectors from these separate operations.\n\n\nEVALUATIONS\n\n\nEXPERIMENT SETUP\n\nIn this section, we describe our experiment setup.More details can be found in Sec.A.2.\n\nModels and Datasets.We have conducted comprehensive evaluations of SqueezeLLM using various models on different tasks.First, in the language modeling evaluation, we apply SqueezeLLM to LLaMA, LLaMA2 training set for the others.While grouping can also be incorporated with our method, we found it sub-optimal as compared to extracting sensitive/outlier values with sparsity (Sec.A.4.3).Latency Profiling.We measure the latency and peak memory usage for generating 128 and 1024 tokens on an A6000 machine using the Torch CUDA profiler.As an official implementation of GPTQ (in particular, the grouped version) is not available, we implement an optimized kernel for single-batch inference based on the most active open-source codebase ( GPTQ-For-LLaMA).\n\n\nMAIN RESULTS\n\nTable 1 shows quantization results for LLaMA along with comparison with RTN, GPTQ and AWQ.\n\nThe models are grouped based on their average bitwidth (i.e., model size) for a better comparison of size-perplexity trade-offs.See Fig. 5 for a visual illustration.Below we use LLaMA-7B as the main example for the discussions for the impact of dense-only and Dense-and-Sparse quantization, and subsequently discuss how these trends extend to larger models.We provide the full evaluation result on all LLaMA models in Tab.A.4.\n\nDense-only Quantization.In Tab. 1 (Top), we compare dense-only SqueezeLLM with 0% sparsity level and GPTQ without grouping.With 4-bit quantization, our method exhibits minimal degradation compared to the FP16 baseline, with only \u223c0.1 perplexity degradation on C4 and WikiText2, while reducing the model size by 3.95\u00d7.Moreover, when compared to non-grouped GPTQ our method shows significant perplexity improvement of up to 0.22.\u2020 Since SpQR does not release their kernel implementation, we conduct our best-effort comparison using their reported speedup numbers.See Sec.A.2 for details.\n\n\u2021 GPTQ with activation ordering incurs a significant latency penalty as elements in the same channel are associated with different scaling factors, resulting in distributed memory accesses (Sec.5.4).While GPTQ without activation ordering alleviates the latency issue, comes at the cost of a substantial perplexity degradation.\n\nTable 1: Perplexity comparison of LLaMA models quantized into 3 and 4 bits using different methods including RTN, GPTQ, AWQ and SpQR on C4 and WikiText-2.We compare the performance of GPTQ, AWQ, and SqueezeLLM in groups based on similar model sizes.In the first group, we compare dense-only SqueezeLLM with non-grouped GPTQ.In the second group, we compare SqueezeLLM with a sparsity level of 0.45% to GPTQ and AWQ with a group size of 128.We add speedup and peak memory usage numbers for comparison.Further results for LLaMA-30/65B can be found in Tab.A.4. Detailed latency evaluation can be found in Tab. 3. LLaMA-65B 3bit The performance gap between the two methods becomes more pronounced with 3-bit quantization.SqueezeLLM outperforms GPTQ by a substantial margin of 1.80/1.22points on C4/WikiText2 with a 5.29\u00d7 compression rate.This is only 0.67/0.55 points off from the FP16 baseline.This demonstrates the effectiveness of the sensitivity-based non-uniform method for ultra-low-bit quantization.\n\nDense-and-Sparse Quantization.By leveraging the Dense-and-Sparse quantization, we achieve a further reduction in the perplexity gap between the FP16 baseline and quantized models, as shown in Tab. 1.This improvement is particularly significant with 3-bit quantization, where extracting just 0.45% of the values yields around 0.2 perplexity improvement.This enables nearly lossless compression with less than 0.1/0.5 perplexity deviation from the FP16 baseline for 4/3-bit, respectively.\n\nBoth GPTQ and AWQ use a grouping strategy to enhance performance with a slight overhead in model size.However, we demonstrate that SqueezeLLM with a sparsity level of 0.45% consistently outperforms both GPTQ and AWQ with a group size of 128 in all scenarios with comparable model sizes.This is more pronounced for 3-bit quantization, where SqueezeLLM with a 0.45% sparsity level outperforms both GPTQ and AWQ with a group size of 128 by up to more than 0.3 perplexity.Results on Larger Models.In Tab. 1 (13B) and Tab.A.4 (30/65B), we observe that the trend in LLaMA-7B extends to larger models, where SqueezeLLM consistently outperforms other PTQ methods across all model sizes and bit widths.Such a trend is also visually illustrated in Fig. 5 for 3-bit quantization across all model sizes.Notably, even the dense-only version of SqueezeLLM achieves perplexity comparable to the grouped GPTQ and AWQ.With sparsity, we achieve further perplexity improvements, reducing the gap from the FP16 baseline to less than 0.1/0.4perplexity points for 4/3-bit quantization.Notably, with 3-bit quantization, our approach achieves up to a 2.1\u00d7 reduction in perplexity gap from the FP16 baseline compared to existing methods.Further ablation studies on our design choices, including sensitivity metrics, sparsity levels, and grouping, are provided in Sec.A.4, and additional results on LLaMA2 and OPT are in Sec.A.6.1.\n\n\nQUANTIZATION OF INSTRUCTION FOLLOWING MODELS\n\nInstruction tuning has emerged as a method for improving the model's ability to respond to user commands.We explore the quantization of instruction-following models to demonstrate the benefits of SqueezeLLM in terms of accuracy preservation by applying it to the Vicuna models, and evaluating the performance with the following approaches.\n\nZero-shot MMLU Evaluation.We first compare the baseline and quantized model on the zero-shot multitask problem-solving benchmark of MMLU.The weighted accuracy across all tasks is provided in Tab. 2 for Vicuna v1.1, including its quantized models using AWQ and SqueezeLLM.As we can see, SqueezeLLM achieves higher accuracy for both Vicuna-7B and 13B as compared to AWQ and also preserves the FP16 baseline accuracy with 4-bit quantization.It is also noteworthy that the 4-bit Vicuna-13B of SqueezeLLM has 2\u00d7 smaller memory footprint than the 7B FP16 model, while still achieving a 2% higher accuracy.Additional results on Vicuna v1.3 are provided in Sec A.6.2.\n\nInstruction-Following Ability.Another approach for evaluating instruction-following ability is to ask GPT-4 to rank the generated responses which is the method used by Chiang et al. (2023).The results are shown in Fig. 6.SqueezeLLM without sparsity achieves near-perfect performance (i.e., 50/50 split) with 4-bit quantization for both Vicuna-7B and 13B, outperforming GPTQ with the same model size.In the case of 3-bit quantization, SqueezeLLM outperforms both GPTQ and AWQ with comparable model sizes.In the case of the Vicuna-13B model, achieving a near-perfect 50/50 split for 3-bit quantization.\n\n\nHARDWARE DEPLOYMENT AND PROFILING\n\nWhile grouping with permutation is an effective way to confine the quantization range, our Denseand-Sparse scheme can achieve higher accuracy with simpler kernels.We show the latency and peak GPU memory usage of SqueezeLLM in Tab. 3 on an A6000 GPU for different configurations when generating 128 tokens.We observe that the LUT-based non-uniform approach in SqueezeLLM (3rd row) shows up to 2.4\u00d7 speedup compared to the FP16 baseline, and exhibits comparable latency and peak memory usage to the uniform quantization of non-grouped GPTQ (2nd row).This indicates that the overhead associated with LUT-based dequantization is small, especially considering the considerable perplexity gains it enables.\n\nAdditionally, when incorporating sparsity, we still observed latency gains relative to the FP16 baseline.As shown in Tab. 3, keeping 0.45% of parameters in FP16 (4th row) only adds around 10% latency overhead relative to the dense-only implementation, while still resulting in up to 2.2\u00d7 speed up compared to the FP16 baseline.In contrast, when accounting for permutation, the GPTQ runtime is degraded heavily (5th row).This latency penalty is due to permutation, which means that elements in the same channel need to be scaled using different scaling factors (which are accessed using group indices); it is challenging for these distributed memory accesses to be performed efficiently, as GPUs rely heavily on coalesced memory accesses in order to optimally utilize memory bandwidth.This shows how our Dense-and-Sparse quantization methodology allows for both higher accuracy as well as better performance relative to GPTQ.Additional evaluation results on generating 1024 tokens are provided in Tab.A.3, where we can observe a similar trend.\n\n\nCONCLUSION\n\nWe have presented SqueezeLLM which attempts to address the Memory Wall problem associated with generative LLM inference that is memory-bound.SqueezeLLM incorporates two novel ideas that allow ultra-low precision quantization of LLMs with negligible degradation in generation performance: the sensitivity-based non-uniform quantization method; and the Dense-and-Sparse decomposition that resolves the outlier issue.We have evaluated SqueezeLLM on a wide range of models and datasets that assess language modeling, problem-solving, and instruction-following capabilities of quantized models, where we have demonstrated that our quantization method can consistently outperform the previous state-of-the-art methodologies.\n\n\nA APPENDIX\n\nA  2023), which uniformly divides weight ranges into bins, has gained popularity since it allows faster computation by using quantized precision arithmetic.However, recent hardware trends indicate that faster computation does not necessarily translate to improved end-to-end latency or throughput Gholami et al. (2021b), particularly in memory-bound tasks like generative LLM inference (Sec.3).Furthermore, uniform quantization can be sub-optimal when the weight distribution is non-uniform, as in LLMs (Fig. 3).\n\nHence, we focus on non-uniform quantization, which non-uniformly allocates quantization bins without constraints for a more accurate representation of weights and smaller quantization errors.While it does not support integer arithmetic for computational acceleration, this drawback is not significant for memory-bound problems as in our focus, where the primary bottleneck lies in memory bandwidth rather than computation.Among non-uniform quantization methods Jeon et al. (2022); Chung et al. (2020), the most similar work to ours is GOBO Zadeh et al. ( 2020), which introduces a k-means clustering-based look-up table approach.Our work introduces two novel methods as compared to GOBO: (i) sensitivity-aware and (ii) Dense-and-Sparse quantization methodologies, which yield substantial improvements within the k-means-based non-uniform quantization framework.\n\n\nA.2 EXPERIMENT SETUP (DETAILS)\n\nModels and Datasets.We have conducted comprehensive evaluations of SqueezeLLM using various models on different tasks.First, in the language modeling evaluation, we apply SqueezeLLM to the LLaMA Touvron et al. (2023a), LLaMA2 Touvron et al. (2023b) and OPT Zhang et al. (2022) models and measure the perplexity of the quantized models on the C4 Raffel et al. (2020) and Wiki-Text2 Merity et al. (2016) datasets with a chunk size of 2048.We also evaluate the domain-specific knowledge and problem-solving ability through zero-shot MMLU Hendrycks et al. (2021) using the instruction-tuned Vicuna (v1.1 and v1.3) models.We used the Language Model Evaluation Harness to run zero-shot evaluation across all tasks Gao et al. (2021).Finally, we evaluate the instruction following ability following the methodology presented in Chiang et al. (2023).To do so, we generate answers for 80 sample questions and compared them to the answers generated by the FP16 counterpart using the GPT-4 score.To minimize the ordering effect, we provide the answers to GPT-4 in both orders, resulting in a total of 160 queries.\n\nBaseline Methods.We compare SqueezeLLM against PTQ methods for LLMs including RTN as well as state-of-the-art methods including GPTQ Frantar et al. (2022), AWQ Lin et al. (2023) and SpQR Dettmers et al. (2023).To ensure a fair comparison, we use GPTQ with activation ordering throughout all experiments unless specified, which addresses the significant performance drop that would otherwise occur.For AWQ, we use official quantized models or reproduce using their official code if they are not available except for LLaMA 65B with group size 256 which ran into OOM even on A100-80G.Evaluations are then conducted based on our perplexity method.For SpQR, we rely on the paper's reported numbers since their perplexity evaluation methodology is identical to ours.SpQR aims to enhance 3-bit and 4-bit models by introducing grouping, bi-level quantization, and sparsity, making them approximately 4 and 4.6 bits on average for LLaMA.In contrast, SqueezeLLM aims to preserve 3 and 4-bit as closely as possible, minimizing any extra model size overhead.Therefore, we present our best-effort comparison of SpQR and SqueezeLLM by compar-ing 3-bit SpQR models, which average around 4 bits, and our 4-bit models, both of which possess similar model sizes.\n\nLatency Profiling.We measure the latency and peak memory usage for generating 128 and 1024 tokens on an A6000 machine using the Torch CUDA profiler.As an official implementation of GPTQ (in particular, the grouped version) is not available, we implement an optimized kernel for single-batch inference based on the most active open-source codebase ( GPTQ-For-LLaMA).\n\nTo compare latency with SpQR, we rely on their reported speedup numbers to make our best-effort comparison as their kernel implementation is not publicly available.Regarding AWQ, we utilize the GPTQ kernel without activation ordering since they exhibit identical behavior during inference.\n\nAlthough AWQ has released their own kernel implementation, their 3-bit kernels are not publicly available.Furthermore, they have incorporated optimizations that are unrelated to quantization, such as LayerNorm and positional embedding, which are universally applicable to all quantization methods.To ensure a fair comparison with other methodologies, we refrained from using their released kernels.\n\n\nA.3 DATA SKEW IN PER-CHANNEL SPARSITY PATTERN\n\nFigure A.1: Histograms of the number of non-zero entries per output channel in 7 different linear layers in the first LLaMA-7B block.The histograms reveal the presence of a few channels that contain significantly more non-zero entries than others, highlighting the skew in the sparsity patterns across different channels within the linear layers.\n\nTable A.1: Hardware profiling of latency and memory usage for LLaMA 7B, 13B, 30B, and 65B quantized into 3-bit when generating 128 tokens on an A6000 GPU.The first row shows the performance of SqueezeLLM without sparsity.The second row shows the performance of SqueezeLLM with a sparsity level of 0.45% using a standard kernel for processing a CSR matrix.The third row shows the performance of SqueezeLLM with a sparsity level of 0.45% using a balanced sparse kernel that allocates 10 nonzeros per thread, thereby more efficiently handling skewed sparse matrices.This plot shows that the nonzero distribution is heavily skewed, with a few channels containing a much larger proportion of nonzero values.This skewed distribution makes it challenging to efficiently perform computations using the sparse matrix, as it is difficult to distribute the nonzero elements evenly across parallel processing units.This motivates our modified kernel for handling channels with a large number of outliers in order to reduce the runtime overhead of the sparse matrices.As outlined in Tab.A.1, we observed over 100% added runtime overhead when employing a standard CSR-based kernel.However, if we allocate each thread to process a fixed number of nonzeros (rather than having each thread process an entire row) we were able to drastically reduce the runtime overhead to 10-20% with both sensitive values and outliers.\n\nA.4 ABLATION STUDIES A.4.1 SENSITIVITY-BASED QUANTIZATION.In our ablation study, we investigate the impact of sensitivity-aware weighted clustering on the performance of non-uniform quantization.In Tab.A.2, we compared the performance of sensitivityaware and sensitivity-agnostic approaches in the context of 3-bit quantization of the LLaMA-7B model.For sensitivity-agnostic quantization, we apply non-weighted k-means clustering at sparsity levels of 0%, 0.05%, and 0.45%.Here, we compare SqueezeLLM with (i) grouping using group sizes of 1024 and 512 (green), (ii) a hybrid approach that combines a group size of 1024 with a sparsity level of 0.05% (blue), and (iii) the Dense-and-Sparse decomposition approach with varying sparsity levels (violet).The pure Dense-and-Sparse decomposition achieves better size-perplexity trade-offs than both grouping and the hybrid approach.\n\nIn Fig.\n\nA.4, we explore the effectiveness of incorporating grouping into SqueezeLLM as an alternative approach to improve quantization performance.We compare three configurations: SqueezeLLM with (i) grouping using group sizes of 1024 and 512 (green), (ii) a hybrid approach that combines a group size of 1024 with a sparsity level of 0.05% (blue), and (iii) the Dense-and-Sparse decomposition approach with varying sparsity levels (violet), where 0.05% of sensitive values are kept and the percentage of outlier values is adjusted.The results clearly demonstrate that both grouping and the hybrid approach result in suboptimal trade-offs compared to the pure Dense-and-Sparse decomposition approach.\n\nThis can be attributed to two factors.First, the Dense-and-Sparse decomposition is a direct solution to the outlier issue.In contrast, while grouping can mitigate the impact of outliers to some extent by isolating them within individual groups, it does not provide a direct solution to this issue.In addition, grouping can introduce significant overhead in terms of storage requirements when combined with non-uniform quantization, since it needs to store one LUT per group.This can be a considerable overhead compared to the uniform quantization approach where only a scaling and zero point value per group need to be stored.Across all sparsity levels, the OBD framework, which is the foundation for SqueezeLLM, consistently outperforms the OBS framework as an alternative approach.\n\nUnder the OBD framework, the optimization objective for determining the non-uniform quantization configuration can be reformulated as arg min\nQ \u2225W X \u2212 W Q X\u2225 2 2\n, where X denotes a batch of input activations.This object can be approximated as a weighted k-means clustering problem, where each weight is weighted by the square of the corresponding input activation size.This indeed results in the activation-based sensitivity/importance metric as in the AWQ framework Lin et al. (2023). In Fig. A.4.4, we compare the perplexity on the C4 dataset for 3-bit quantization of the LLaMA-7B model using the OBS framework versus the OBD framework.Across all sparsity levels obtained by adjusting the number of outliers being extracted, SqueezeLLM based on the OBD framework outperforms the alternative of using the OBS framework by a large margin of up to around 0.3 perplexity points.\n\n\nA.5 ADDITIONAL HARDWARE PROFILING RESULTS\n\nTable A.3: Latency (s) and peak memory usage (GB) of 3-bit LLaMA when generating 1024 tokens on an A6000 GPU.The table compares the FP16 baseline, non-grouped and grouped GPTQ with activation ordering, and SqueezeLLM with different sparsity levels.For comparison, we include bitwidth and perplexity on the C4 benchmark.In Tab.A.3, we provide additional hardware profiling results using a sequence length of 1024.All the experimental setups and details are identical to Sec. 5.4 and Tab. 3.  A.7 LIMITATIONS While our empirical results primarily focus on generation tasks, the proposed ideas in this work are not inherently limited to decoder architectures.However, we have not yet conducted thorough assessments of our framework's effectiveness on encoder-only or encoder-decoder architectures, as well as other neural network architectures.Additionally, it is important to note that our hardware performance modeling approach relies on a simulation-based method using a roofline model, which entails making simplified assumptions about the hardware's inference pipeline.\n\nTable A.6: Perplexity comparison of OPT models quantized into 4 and 3 bits using different methods including RTN, GPTQ, AWQ and SpQR on C4 and WikiText-2.We compare the performance of GPTQ, AWQ, and SqueezeLLM (SQLLM) in groups based on similar model sizes.In the first group, we compare dense-only SqueezeLLM with non-grouped GPTQ.In the subsequent groups, we compare SqueezeLLM with different levels of sparsity to GPTQ and AWQ with different group sizes.Note that all GPTQ results are with activation reordering.\"div\" means that the perplexity is diverged.\n\nFigure 2 :\n2\nFigure2: Normalized runtime for LLaMA-7B when reducing the bit precision for the weights with sequence lengths of 128 (left) and 2048 (right).Results were obtained using a roofline-based performance model for an A5000 GPU.Reducing only the precision of the weights (and not the activations) is sufficient to obtain significant latency reductions.\n\n\nFigure 3 :\n3\nFigure 3: (Left) The weight distribution of one output channel in LLaMA-7B.The top-20 most sensitive values are marked in red.(Right) Weight distributions after 3-bit quantization using uniform and sensitivity-based non-uniform quantization.In the latter case, the quantized values are more clustered around the sensitive values.\n\n\nFig. 3 ,Figure 4 :\n34\nFigure 4: The distributions of the (normalized) absolute weight values, for the output layers in MHA and the down layers in FFN across different layers in LLaMA-7B.Note that the distributions exhibit outlier patterns across all layers, with 99% of the values clustered within \u223c10% of the entire range.\n\n\nFigure 5 :\n5\nFigure5: Perplexity comparison PTQ methods for 3-bit LLaMA quantization, evaluated on C4.The x-axes are the relative model sizes with respect to the model size in FP16.Different size-perplexity trade-offs are achieved by adjusting the group size for GPTQ and AWQ and the sparsity level for ours.Our quantization method consistently and significantly outperforms GPTQ and AWQ across all model size regimes, with a more pronounced gap in lower-bit and smaller model sizes.\n\n\nFigure 6 :\n6\nFigure6: Comparison of PTQ methods applied to Vicuna v1.1.Blue / yello / red represent the number of times that the quantized model won / tied / lost against the baseline FP16 model.This evaluation was performed using the methodology from Vicuna.\n\n\n\n\n.1 RELATED WORKS ON QUANTIZATION OF TRANSFORMER-BASED MODELS Quantization methods can be broadly categorized based whether retraining is required or not Gholami et al. (2021a).Quantization-Aware Training (QAT) requires retraining the model to adapt its weights to help recover accuracy after quantization Zafrir et al. (2019); Shen et al. (2020); Kim et al. (2021); Zhang et al. (2023; 2020); Bai et al. (2020), whereas Post-Training Quantization (PTQ) does not involve retraining Zhao et al. (2019); Cai et al. (2020); Shomron et al. (2021); Oh et al. (2022); Li et al. (2023).While QAT often results in better accuracy, it is often infeasible for LLMs due to the expensive retraining cost and/or lack of access to the training data and infrastructure.As such, most works on LLM quantization have focused on PTQ Yao et al. (2022); Dettmers et al.; Frantar et al. (2022); Yuan et al. (2023); Lin et al. (2023).Our work also focuses on the PTQ approach.Quantization methods can be also classified as uniform or non-uniform Gholami et al. (2021a).Uniform quantization Frantar et al. (2022); Lin et al. (2023); Dettmers et al. (2023); Zafrir et al. (2019); Shen et al. (2020); Kim et al. (2021); Huang et al. (2023); Liu et al. (\n\n\nFig\n\nFig. A.1 provides the distribution of nonzero entries per output channel across different linear layers in the first LLaMA-7B block.This plot shows that the nonzero distribution is heavily skewed, with\n\n\nFigure A. 3 :\n3\nFigure A.3: Model size (normalized by the size of the FP16 model) and perplexity trade-offs of grouping and the Dense-and-Sparse decomposition on 3-bit quantization of the LLaMA-7B model.Here, we compare SqueezeLLM with (i) grouping using group sizes of 1024 and 512 (green), (ii) a hybrid approach that combines a group size of 1024 with a sparsity level of 0.05% (blue), and (iii) the Dense-and-Sparse decomposition approach with varying sparsity levels (violet).The pure Dense-and-Sparse decomposition achieves better size-perplexity trade-offs than both grouping and the hybrid approach.\n\n\nA. 6\n6\nADDITIONAL EXPERIMENT RESULTS A.6.1 PERPLEXITY EVALUATION In Tab.A.4, we provide the full experimental results on LLaMA Touvron et al. (2023a).Furthermore, in Tab.A.5 and A.6, we provide additional experimental results on LLaMA2 Touvron et al. (2023b) and OPT Zhang et al. (2022) models.A.6.2 MMLU EVALUATION In Tab.A.7, we provide additional experimental results for Vicuna v1.3 on MMLU.\n\n\n\n\nTo address this issue, outlier-aware quantization methods have been investigated Bondarenko et al. (2021); Dettmers et al.; Wei et al. (2022; 2023).Notably, Dettmers et al. keeps outlier activations in floating-point, while Wei et al. (2022) transfers outlier factors to later lay-\n\ners without affecting functionality.These focus on activations, which is not a concern in our work where all activations are maintained in floating-point.Our Dense-and-Sparse quantization instead tackles weight outliers for low-bit LLM quantization.Concurrently to our work, SpQRDettmers et al. (\n\n\nTable 2 :\n2\nComparison of PTQ methods on zero-shot MMLU accuracy applied to Vicuna v1.1.We add speedup and peak memory usage for comparison.\nMethodAvg. bit7B Acc (\u2191) Speedup (\u2191) Mem (GB, \u2193) Acc (\u2191) Speedup (\u2191) Mem (GB, \u2193) 13BBaseline1639.1%1\u00d712.741.2%1\u00d724.6AWQ (g128)4.2538.0%1.6\u00d73.840.4%1.9\u00d77.2SqLLM4.0538.8%1.8\u00d73.839.2%2.0\u00d76.9SqLLM (0.45%)4.2639.4%1.7\u00d74.041.0%1.9\u00d77.3AWQ (g128)3.2536.5%2.0\u00d73.037.6%2.2\u00d75.7SqLLM3.0236.0%2.1\u00d72.937.2%2.4\u00d75.4SqLLM (0.45%)3.2437.7%1.9\u00d73.139.4%2.2\u00d75.84-bitGPTQSqueezeLLM04080120160\n\nTable 3 :\n3\nLatency (s)and peak memory usage (GB) of 3-bit LLaMA when generating 128 tokens on an A6000 GPU.The table compares the FP16 baseline, non-grouped and grouped GPTQ with activation ordering, and SqueezeLLM with different sparsity levels.For comparison, we include bitwidth and perpelxity on the C4 benchmark.\nMethodBit width PPL (C4) Lat (s) Mem (G) PPL (C4) Lat (s) Mem (G) PPL (C4) Lat (s) Mem (G) PPL (C4) Lat (s) Mem (G) 7B 13B 30B 65BBaseline167.083.212.76.615.624.65.98OOMOOM5.62OOMOOMGPTQ39.551.42.98.222.15.37.314.012.36.706.724.0SqueezeLLM3.027.751.52.97.082.45.46.374.012.55.997.624.5GPTQ (g128)3.257.8913.73.07.1224.25.66.4761.912.96.01117.825.1SqueezeLLM (0.45%) 3.247.561.73.16.922.55.86.234.414.75.848.828.0\n\n\n\nTableA.2: Ablation study comparing sensitivity-agnostic and sensitivity-based non-uniform quantization on the LLaMA-7B model with 3-bit quantization, measured by perplexity on the C4 benchmark.The baseline model in FP16 achieves a perplexity of 7.08.\nMethodSensitivity-Agnostic (\u2193)Sensitivity-Based (\u2193)SqueezeLLM18.087.75SqueezeLLM (0.05%)8.107.67SqueezeLLM (0.45%)7.617.56\n\n\n\nThe results demonstrate that while non-uniform quantization alone can reduce the perplexity from 28.26 (of RTN uniform quantization) to 18.08 without considering sensitivity, incorporating sensitivity-aware clustering is critical in reducing the perplexity to 7.75.(Left)Modelsize(normalized by the size of the FP16 model) and perplexity trade-off with different percentages of sensitive values included in the sparse matrix.Here, no outlier values are included in the sparse matrix.(Right)Comparison of the performance when the sensitive values are not removed as the sparse matrix (only outlier values are removed) to the case where 0.05% of the sensitive values are removed.In both cases, the trade-offs are obtained by controlling the percentage of outlier values included in the sparse matrix.In Fig.A.2 (Left), we present the perplexity results of the 3-bit quantized LLaMA-7B model on the C4 benchmarks, with varying percentages of sensitive values extracted as the sparse matrix, ranging from 0% to 0.2%.The plot demonstrates that the perplexity gain diminishes as the sparsity level of the sensitive values exceeds 0.05%.Therefore, we maintain a fixed sparsity level of 0.05% for the sensitive values throughout all experiments.Furthermore, in Figure A.2 (Right), we compare the performance when the sensitive values are not removed as the sparse matrix (only outlier values are removed) to the case where 0.05% of the sensitive values are removed.In both scenarios, we control the sparsity level by increasing the percentage of outlier values included in the sparse matrix to obtain the trade-off curves.The results indicate that the sparsity configuration with both sensitive values and outlier values consistently outperforms the configuration with only outlier values.\nThis improvement is consistent across all sparsity levels. A.4.2 IMPACT OF SPARSITY LEVELS ON SQUEEZELLM 0.02% 0.05% 0.1% 0.2% 0% 0.190 0.192 0.194 0.196 0.198 0.200 0.202 Model Size 7.60 7.61 7.62 7.63 7.64 7.65 Perplexity on C4 Figure A.2: A.4.3 IMPACT OF GROUPING ON SQUEEZELLM 7.66 LLaMA-7B 3bit 7.67 Grouping Dense-and-Sparse + Grouping Dense-and-Sparse\n\n\n\nModel size (normalized by the size of the FP16 model) and perplexity trade-offs for 3bit quantization of the LLaMA-7B model using the Optimal Brain Surgeon (OBS) framework versus the Optimal Brain Damage (OBD) framework for determining the non-uniform quantization configuration.The trade-off is obtained by adjusting the sparsity level of the outliers being extracted.\nPerplexity on C47.7 7.8 7.9 8.0 8.1LLaMA-7B 3bitOBS OBD (Ours)7.60.190 0.192 0.194 0.196 0.198 0.200 0.202 Model SizeFigure A.4:A.4.4 COMPARISON OF THE OBD FRAMEWORK VERSUS THE OBS FRAMEWORK FORNON-UNIFORM QUANTIZATIONWhile our method adopts the Optimal Brain Damage (OBD) framework to minimize the perturba-\nDettmers et al. (2023)ut of the model during quantization, it is worth noting that the Optimal Brain Surgeon (OBS) framework can also be considered as an alternative.Most existing solutions for LLM quantization includingGPTQ Frantar et al. (2022),AWQ Lin et al. (2023), and SpQRDettmers et al. (2023)have utilized the OBS framework, which aims to minimize the perturbation of output activations in individual layers.In this ablation study, we demonstrate that the OBD framework is superior to the OBS framework.\n\n\nTable A .\nA\n4: Perplexity comparison of LLaMA-30B and 65B models quantized into 4 and 3 bits using different methods including RTN, GPTQ, AWQ and SpQR on C4 and WikiText-2.We compare the performance of GPTQ, AWQ, and SqueezeLLM (SQLLM) in groups based on similar model sizes.In the first group, we compare dense-only SqueezeLLM with non-grouped GPTQ.In the subsequent groups, we compare SqueezeLLM with different levels of sparsity to GPTQ and AWQ with different group sizes.\nLLaMA-30B3-bit4-bitLLaMA-65B3-bit4-bitMethodAvg. Bits (comp. rate)PPL (\u2193) C4 Wiki (comp. rate) C4 Wiki Avg. Bits PPL (\u2193)MethodAvg. Bits (comp. rate)PPL (\u2193) C4 Wiki (comp. rate) C4 Wiki Avg. Bits PPL (\u2193)Baseline165.98 4.10165.98 4.10Baseline165.62 3.53165.62 3.53RTN3 (5.33)28.53 14.894 (4.00)6.33 4.54RTN3 (5.33)12.77 10.594 (4.00)5.86 3.92GPTQ3 (5.33)7.31 5.764 (4.00)6.20 4.43GPTQ3 (5.33)6.70 5.584 (4.00)5.81 4.11SpQR---3.89 (4.11) 6.08 4.25SpQR3 (5.33)-4.2  \u2020 3.90 (4.10) 5.70 3.68SQLLM3.02 (5.31) 6.37 4.66 4.03 (3.97) 6.06 4.22SQLLM3.02 (5.30) 5.99 4.05 4.04 (3.96) 5.69 3.76GPTQ (g128)3.25 (4.92) 6.47 4.83 4.25 (3.77) 6.07 4.24GPTQ (g128)3.25 (4.92) 6.01 4.55 4.25 (3.77) 5.69 3.76AWQ (g128)3.25 (4.92) 6.38 4.63 4.25 (3.77) 6.05 4.21AWQ (g128)3.25 (4.92) 5.94 4.00 4.25 (3.77) 5.68 3.67SQLLM (0.45%) 3.25 (4.92) 6.23 4.44 4.25 (3.77) 6.04 4.18SQLLM (0.45%) 3.24 (4.94) 5.84 3.88 4.26 (3.76) 5.67 3.63\n\nTable A . 5 :\nA5\nPerplexity comparison of LLaMA2 models quantized into 4 and 3 bits using different methods including RTN, GPTQ, AWQ and SpQR on C4 and WikiText-2.We compare the performance of GPTQ, AWQ, and SqueezeLLM (SQLLM) in groups based on similar model sizes.In the first group, we compare dense-only SqueezeLLM with non-grouped GPTQ.In the subsequent groups, we compare SqueezeLLM with different levels of sparsity to GPTQ and AWQ with different group sizes.Note that all GPTQ results are with activation reordering.\nLLaMA2-7B3-bit4-bitLLaMA2-13B3-bit4-bitMethodAvg. Bits (comp. rate)PPL (\u2193) C4 Wiki (comp. rate) C4 Wiki Avg. Bits PPL (\u2193)MethodAvg. Bits (comp. rate) C4 Wiki (comp. rate) C4 Wiki PPL (\u2193) Avg. Bits PPL (\u2193)Baseline166.975.47166.97 5.47Baseline166.47 4.88166.47 4.88RTN3 (5.33)404.45 542.864 (4.00)7.72 6.12RTN3 (5.33)12.50 10.684 (4.00)6.83 5.20GPTQ3 (5.33)10.458.974 (4.00)7.42 5.90GPTQ3 (5.33)8.27 6.174 (4.00)6.74 5.08SQLLM3.02 (5.29)7.726.184.05 (3.95) 7.12 5.62SQLLM3.02 (5.30) 6.97 5.36 4.04 (3.96) 6.57 4.99GPTQ (g128)3.24 (4.93)7.976.254.24 (3.77) 7.23 5.72GPTQ (g128)3.25 (4.92) 7.06 5.31 4.25 (3.77) 6.57 4.96AWQ (g128)3.24 (4.93)7.846.244.24 (3.77) 7.13 5.72AWQ (g128)3.25 (4.92) 6.94 5.32 4.25 (3.77) 6.56 4.97SQLLM (0.45%) 3.24 (4.93)7.515.964.27 (3.75) 7.08 5.57SQLLM (0.45%) 3.24 (4.94) 6.82 5.23 4.26 (3.76) 6.54 4.96\n\n\n\n.92) 12.30 11.41 4.25 (3.77) 11.86 10.93 SQLLM (0.5%) 3.26 (4.90) 12.18 11.31 4.28 (3.73) 11.83 10.92 .92)12.61 10.67 4.25 (3.77) 11.28 10.22 SQLLM (0.5%) 3.26 (4.90) 11.5710.54 4.28 (3.73) 11.26 10.22 5%) 3.26 (4.90) 10.93 9.77 4.28 (3.73) 10.72 9.61 Table A.7: Comparison of PTQ methods on zero-shot MMLU accuracy applied to Vicuna v1.3.\nOPT-1.3B3-bit4-bitOPT-2.7B3-bit4-bitMethodAvg. Bits (comp. rate) C4 Wiki (comp. rate) C4 Wiki PPL (\u2193) Avg. Bits PPL (\u2193)MethodAvg. Bits (comp. rate) C4 Wiki (comp. rate) C4 Wiki PPL (\u2193) Avg. Bits PPL (\u2193)Baseline1614.72 14.621614.72 14.62Baseline1613.17 12.471613.17 12.47RTN3 (5.43)div.div.4 (4)24.68 48.19RTN3 (5.33)div.div.4 (4)17.52 16.92SQLLM3.04 (5.26) 16.42 16.30 4.09 (3.91) 15.01 14.94SQLLM3.04 (5.26) 14.45 13.85 4.07 (3.93) 13.38 12.80AWQ (g128)3.25 (4.93) 16.28 16.32 4.25 (3.77) 15.04 14.95AWQ (g128)3.25 (4.93) 16.28 16.32 4.25 (3.77) 13.39 12.73SQLLM (0.5%) 3.25 (4.92) 15.84 15.76 4.30 (3.72) 14.94 14.83SQLLM (0.5%) 3.25 (4.92) 13.88 13.43 4.29 (3.73) 13.30 12.60OPT-6.7B Method Baseline RTN SpQR SQLLM SpQR AWQ (g128)3-bit (comp. rate) C4 Wiki (comp. rate) C4 Wiki 4-bit Avg. Bits PPL (\u2193) Avg. Bits PPL (\u2193) 16 11.74 10.86 16 11.74 10.86 3 (5.33) div. div. 4 (4) 13.38 12.10 ---3.94 (4.06) 11.98 11.04 3.02 (5.29) 12.44 11.70 4.05 (3.96) 11.85 11.03 ---4.27 (3.74) 11.88 10.91 3.25 (4OPT-13B Method Baseline RTN SpQR SQLLM SpQR AWQ (g128) 3.25 (4OPT-30B Avg. Bits (comp. rate) C4 Wiki (comp. rate) C4 Wiki 3-bit 4-bit PPL (\u2193) Avg. Bits PPL (\u2193) 16 11.20 10.12 16 11.20 10.12 3 (5.33) div. div. 4 (4) 12.35 11.32 ---3.93 (4.07) 11.34 10.28 3.02 (5.29) 12.69 11.76 4.05 (3.96) 11.29 10.24 ---4.27 (3.74) 11.27 10.22 3-bit 4-bitMethodAvg. Bits (comp. rate) C4 Wiki (comp. rate) C4 Wiki PPL (\u2193) Avg. Bits PPL (\u2193)Baseline1610.69 9.561610.69 9.56RTN3 (5.33)div.div.4 (4)11.90 10.98SpQR---3.94 (4.06) 10.78 9.54SQLLM3.01 (5.31) 11.10 10.17 4.03 (3.97) 10.75 9.65SpQR---4.26 (3.76) 10.73 9.50AWQ (g128)3.25 (4.92) 10.96 9.85 4.25 (3.77) 10.75 9.59SQLLM (0.MethodAvg. Bit7B (\u2191)13B (\u2191)33B (\u2191)Baseline1640.2%43.3%50.4%AWQ (g128)4.2539.6%42.2%49.5%SqueezeLLM4.0539.3%44.1%48.0%SqueezeLLM (0.45%)4.2639.5%43.8%49.9%AWQ (g128)3.2537.4%40.7%46.4%SqueezeLLM3.0235.1%40.5%46.2%SqueezeLLM (0.45%)3.2437.6%40.8%47.7%\nTo be precise, we limit this discussion to single batch inference where the arithmetic involves matrix-vector operations. For large batch inference or different model architectures, compute can become important.\nFor instance, A5000 GPU has peak computational throughput of 222 TeraFLOPs per second, which is 290\u00d7 higher than the peak memory bandwidth of 768 GigaBytes per second.\n\u2020 SpQR does not report their near-3-bit performance. However, in the case of 65B model, its 3-bit perplexity on Wikitext-2 can be inferred from the trade-off curve in Figure8of their paper. This comparison indicates that the gap between SpQR and SqueezeLLM can be larger in the lower-bitwidth regimes.\nACKNOWLEDGEMENTSThe authors would like to acknowledge Karttikeya Mangalam, Nicholas Lee, and Thanakul Wattanawong for helpful discussions and brainstorming.We acknowledge gracious support from Google Cloud, Google TRC team, and specifically Jonathan Caton, Jing Li, Jiayu Ye, and Prof. David Patterson.Prof. Keutzer's lab is sponsored by Intel corporation, Intel VLAB team, Intel One-API center of excellence, as well as gracious funding from Furiosa, Berkeley Deep Drive, and BAIR.Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred.\nBinaryBERT: Pushing the limit of BERT quantization. Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, Irwin King, arXiv:2012.157012020arXiv preprint\n\nUnderstanding and overcoming the challenges of efficient Transformer quantization. Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort, arXiv:2109.129482021arXiv preprint\n\nLanguage models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, arXiv:2005.141652020arXiv preprint\n\nZeroQ: A novel zero shot quantization framework. Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, Kurt Keutzer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020\n\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, March 2023\n\nPALM: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.023112022arXiv preprint\n\nExtremely low bit transformer quantization for on-device neural machine translation. Insoo Chung, Byeongwook Kim, Yoonjung Choi, Se Jung Kwon, Yongkweon Jeon, Baeseong Park, Sangha Kim, Dongsoo Lee, arXiv:2009.074532020arXiv preprint\n\n8-bit matrix multiplication for transformers at scale. Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, Advances in Neural Information Processing Systems. int8 (\n\nSpQR: A sparse-quantized representation for near-lossless LLM weight compression. Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Alexander Saleh Ashkboos, Torsten Borzunov, Dan Hoefler, Alistarh, arXiv:2306.030782023arXiv preprint\n\nHAWQ-V2: Hessian Aware trace-Weighted Quantization of neural networks. NeurIPS'19 workshop on Beyond First-Order Optimization Methods in Machine Learning. Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney, Kurt Keutzer, 2019\n\nGLAM: Efficient scaling of language models with mixture-of-experts. Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, International Conference on Machine Learning. PMLR2022\n\nSparse Matrix-Vector Multiplication with CUDA. Georgii Evtushenko, 2019\n\nBalanced csr sparse matrix-vector product on graphics processors. Goran Flegar, Enrique S Quintana-Ort\u00ed, Euro-Par 2017: Parallel Processing: 23rd International Conference on Parallel and Distributed Computing. Santiago de Compostela, SpainSpringerAugust 28-September 1, 2017. 201723\n\nGPTQ: Accurate post-training quantization for generative pre-trained transformers. Elias Frantar, Torsten Saleh Ashkboos, Dan Hoefler, Alistarh, arXiv:2210.173232022arXiv preprint\n\nA framework for few-shot language model evaluation. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony Dipofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle Mcdonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, Andy Zou, 10.5281/zenodo.5371628September 2021\n\nAmir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, Kurt Keutzer, arXiv:2103.13630A survey of quantization methods for efficient neural network inference. 2021aarXiv preprint\n\n. Amir Gholami, Zhewei Yao, Sehoon Kim, Michael W Mahoney, Kurt Keutzer, Memory Ai, Wall, 2021b\n\n. Gptq-For-Llama, \n\nSecond order derivatives for network pruning: Optimal brain surgeon. Babak Hassibi, David G Stork, Advances in neural information processing systems. 1993\n\nOptimal brain surgeon and general network pruning. Babak Hassibi, David G Stork, Gregory J Wolff, IEEE international conference on neural networks. IEEE1993\n\nMeasuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021\n\nTraining compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.155562022arXiv preprint\n\nOutput sensitivity-aware detr quantization. Yafeng Huang, Huanrui Yang, Zhen Dong, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Yuan Du, Shanghang Zhang, Kurt Keutzer, 2023\n\nBiQ: Post-training non-uniform quantization based on minimizing the reconstruction error. Yongkweon Jeon, Chungman Lee, Eulrang Cho, Yeonju Ro, Mr, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022\n\nSehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, Kurt Keutzer, I-Bert, arXiv:2101.01321Integeronly bert quantization. 2021arXiv preprint\n\nFull stack optimization of transformer inference: a survey. Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney, Sophia Shao, Amir Gholami, arXiv:2302.140172023arXiv preprint\n\n. Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, Anna Rumshisky, arXiv:2105.069902021Bert busters: Outlier dimensions that disrupt transformers. arXiv preprint\n\nOptimal brain damage. Yann Lecun, John S Denker, Sara A Solla, Advances in neural information processing systems. 1990\n\nXiuyu Li, Long Lian, Yijiang Liu, Huanrui Yang, Zhen Dong, Daniel Kang, arXiv:2302.04304Shanghang Zhang, and Kurt Keutzer. Q-diffusion: Quantizing diffusion models. 2023arXiv preprint\n\nAwq: Activationaware weight quantization for llm compression and acceleration. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Song Han, 2023\n\nNoisyQuant: Noisy bias-enhanced post-training activation quantization for vision transformers. Yijiang Liu, Huanrui Yang, Zhen Dong, Kurt Keutzer, Li Du, Shanghang Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023\n\nPointer sentinel mixture models. Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, 2016\n\nNon-uniform step size quantization for accurate post-training quantization. Sangyun Oh, Hyeonuk Sim, Jounghyun Kim, Jongeun Lee, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringerOctober 23-27, 2022. 2022Proceedings, Part XI\n\nLatency lags bandwith. David A Patterson, Communications of the ACM. 47102004\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020\n\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Matthias Franc \u00b8ois Yvon, Gall\u00e9, arXiv:2211.05100A 176b-parameter open-access multilingual language model. 2022arXiv preprint\n\nQ-BERT: Hessian based ultra low precision quantization of bert. Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, Kurt Keutzer, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034\n\nPost-training sparsity-aware quantization. Gil Shomron, Freddy Gabbay, Samer Kurzum, Uri Weiser, Advances in Neural Information Processing Systems. 202134\n\nUsing deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick Legresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, arXiv:2201.119902022arXiv preprint\n\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Language models for dialog applications. 2022arXiv preprint\n\nTraining data-efficient image transformers & distillation through attention. Matthieu Hugo Touvron, Matthijs Cord, Francisco Douze, Alexandre Massa, Herv\u00e9 Sablayrolles, J\u00e9gou, International Conference on Machine Learning. PMLR2021\n\nLLaMA: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timoth\u00e9e Lachaux, Baptiste Lacroix, Naman Rozi\u00e8re, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.139712023aarXiv preprint\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Open foundation and fine-tuned chat models. 2023b2arXiv preprint\n\nOutlier suppression: Pushing the limit of low-bit transformer language models. Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, Xianglong Liu, arXiv:2209.133252022arXiv preprint\n\nOutlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, Xianglong Liu, arXiv:2304.091452023arXiv preprint\n\nZeroQuant: Efficient and affordable post-training quantization for large-scale transformers. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He, arXiv:2206.018612022arXiv preprint\n\nRPTQ: Reorder-based post-training quantization for large language models. Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, Bingzhe Wu, arXiv:2304.010892023arXiv preprint\n\nGOBO: Quantizing attention-based nlp models for low latency and energy efficient inference. Ali Hadi Zadeh, Isak Edo, 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEE2020Omar Mohamed Awad, and Andreas Moshovos\n\nOfir Zafrir, Guy Boudoukh, Peter Izsak, Moshe Wasserblat, arXiv:1910.06188Q8BERT: Quantized 8bit bert. 2019arXiv preprint\n\nOPT: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.010682022arXiv preprint\n\nWei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, Qun Liu, arXiv:2009.12812TernaryBERT: Distillation-aware ultra-low bit bert. 2020arXiv preprint\n\nQd-bev: Quantization-aware view-guided distillation for multi-view 3d object detection. Yifan Zhang, Zhen Dong, Huanrui Yang, Ming Lu, Cheng-Ching Tseng, Yandong Guo, Kurt Keutzer, Li Du, Shanghang Zhang, 2023\n\nImproving neural network quantization without retraining using outlier channel splitting. Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, Zhiru Zhang, International conference on machine learning. PMLR2019\n", "annotations": {"author": "[{\"end\":106,\"start\":55},{\"end\":160,\"start\":107},{\"end\":194,\"start\":161},{\"end\":244,\"start\":195},{\"end\":271,\"start\":245},{\"end\":321,\"start\":272},{\"end\":363,\"start\":322},{\"end\":415,\"start\":364},{\"end\":446,\"start\":416},{\"end\":452,\"start\":447},{\"end\":458,\"start\":453},{\"end\":484,\"start\":459},{\"end\":528,\"start\":485}]", "publisher": null, "author_last_name": "[{\"end\":65,\"start\":62},{\"end\":121,\"start\":115},{\"end\":173,\"start\":166},{\"end\":204,\"start\":200},{\"end\":253,\"start\":251},{\"end\":282,\"start\":278},{\"end\":339,\"start\":332},{\"end\":376,\"start\":369},{\"end\":428,\"start\":420},{\"end\":451,\"start\":447},{\"end\":457,\"start\":453}]", "author_first_name": "[{\"end\":61,\"start\":55},{\"end\":114,\"start\":107},{\"end\":165,\"start\":161},{\"end\":199,\"start\":195},{\"end\":250,\"start\":245},{\"end\":277,\"start\":272},{\"end\":329,\"start\":322},{\"end\":331,\"start\":330},{\"end\":368,\"start\":364},{\"end\":417,\"start\":416},{\"end\":419,\"start\":418}]", "author_affiliation": "[{\"end\":105,\"start\":90},{\"end\":159,\"start\":144},{\"end\":243,\"start\":228},{\"end\":270,\"start\":255},{\"end\":320,\"start\":305},{\"end\":414,\"start\":399},{\"end\":445,\"start\":430},{\"end\":483,\"start\":460},{\"end\":527,\"start\":486}]", "title": "[{\"end\":42,\"start\":1},{\"end\":570,\"start\":529}]", "venue": null, "abstract": "[{\"end\":2313,\"start\":639}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2560,\"start\":2541},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2582,\"start\":2562},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2602,\"start\":2584},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2620,\"start\":2604},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2644,\"start\":2622},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2669,\"start\":2646},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2690,\"start\":2671},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2711,\"start\":2692},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2736,\"start\":2713},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2760,\"start\":2738},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2934,\"start\":2913},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3605,\"start\":3589},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4172,\"start\":4151},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4191,\"start\":4174},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":4685,\"start\":4668},{\"end\":4989,\"start\":4963},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5363,\"start\":5341},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8034,\"start\":8014},{\"end\":8080,\"start\":8057},{\"end\":8137,\"start\":8106},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8752,\"start\":8730},{\"end\":9019,\"start\":8998},{\"end\":9043,\"start\":9021},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9244,\"start\":9225},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10729,\"start\":10712},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11944,\"start\":11927},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12541,\"start\":12519},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12676,\"start\":12659},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13886,\"start\":13865},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13910,\"start\":13888},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13929,\"start\":13912},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15474,\"start\":15456},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15778,\"start\":15759},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21325,\"start\":21308},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28526,\"start\":28506},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31787,\"start\":31765},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":32461,\"start\":32443},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":32482,\"start\":32463},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":33094,\"start\":33073},{\"end\":33126,\"start\":33094},{\"end\":33154,\"start\":33131},{\"end\":33243,\"start\":33220},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":33279,\"start\":33259},{\"end\":33436,\"start\":33408},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33603,\"start\":33586},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":33718,\"start\":33698},{\"end\":34135,\"start\":34109},{\"end\":34158,\"start\":34137},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":34190,\"start\":34168},{\"end\":40952,\"start\":40920}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":43370,\"start\":43009},{\"attributes\":{\"id\":\"fig_1\"},\"end\":43715,\"start\":43371},{\"attributes\":{\"id\":\"fig_2\"},\"end\":44041,\"start\":43716},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44527,\"start\":44042},{\"attributes\":{\"id\":\"fig_4\"},\"end\":44789,\"start\":44528},{\"attributes\":{\"id\":\"fig_5\"},\"end\":46020,\"start\":44790},{\"attributes\":{\"id\":\"fig_6\"},\"end\":46229,\"start\":46021},{\"attributes\":{\"id\":\"fig_7\"},\"end\":46839,\"start\":46230},{\"attributes\":{\"id\":\"fig_8\"},\"end\":47237,\"start\":46840},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":47821,\"start\":47238},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":48334,\"start\":47822},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":49067,\"start\":48335},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":49444,\"start\":49068},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":51588,\"start\":49445},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":52783,\"start\":51589},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":54170,\"start\":52784},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":55528,\"start\":54171},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":57783,\"start\":55529}]", "paragraph": "[{\"end\":3093,\"start\":2329},{\"end\":3720,\"start\":3095},{\"end\":4388,\"start\":3722},{\"end\":5364,\"start\":4390},{\"end\":5660,\"start\":5366},{\"end\":6211,\"start\":5662},{\"end\":6942,\"start\":6213},{\"end\":7611,\"start\":6944},{\"end\":8392,\"start\":7613},{\"end\":9533,\"start\":8409},{\"end\":9718,\"start\":9535},{\"end\":10679,\"start\":9720},{\"end\":11152,\"start\":10681},{\"end\":13460,\"start\":11168},{\"end\":14497,\"start\":13476},{\"end\":14885,\"start\":14499},{\"end\":15287,\"start\":14989},{\"end\":15475,\"start\":15289},{\"end\":16114,\"start\":15477},{\"end\":16210,\"start\":16183},{\"end\":16473,\"start\":16234},{\"end\":16522,\"start\":16519},{\"end\":17006,\"start\":16524},{\"end\":17181,\"start\":17008},{\"end\":17576,\"start\":17212},{\"end\":18463,\"start\":17672},{\"end\":18793,\"start\":18465},{\"end\":18970,\"start\":18874},{\"end\":19506,\"start\":18972},{\"end\":20098,\"start\":19508},{\"end\":20395,\"start\":20100},{\"end\":21101,\"start\":20438},{\"end\":22065,\"start\":21103},{\"end\":22187,\"start\":22100},{\"end\":22939,\"start\":22189},{\"end\":23046,\"start\":22956},{\"end\":23474,\"start\":23048},{\"end\":24061,\"start\":23476},{\"end\":24389,\"start\":24063},{\"end\":25392,\"start\":24391},{\"end\":25880,\"start\":25394},{\"end\":27287,\"start\":25882},{\"end\":27675,\"start\":27336},{\"end\":28336,\"start\":27677},{\"end\":28938,\"start\":28338},{\"end\":29676,\"start\":28976},{\"end\":30720,\"start\":29678},{\"end\":31453,\"start\":30735},{\"end\":31980,\"start\":31468},{\"end\":32843,\"start\":31982},{\"end\":33979,\"start\":32878},{\"end\":35225,\"start\":33981},{\"end\":35592,\"start\":35227},{\"end\":35883,\"start\":35594},{\"end\":36283,\"start\":35885},{\"end\":36679,\"start\":36333},{\"end\":38083,\"start\":36681},{\"end\":38962,\"start\":38085},{\"end\":38971,\"start\":38964},{\"end\":39665,\"start\":38973},{\"end\":40450,\"start\":39667},{\"end\":40593,\"start\":40452},{\"end\":41330,\"start\":40614},{\"end\":42447,\"start\":41376},{\"end\":43008,\"start\":42449},{\"end\":43369,\"start\":43023},{\"end\":43714,\"start\":43385},{\"end\":44040,\"start\":43739},{\"end\":44526,\"start\":44056},{\"end\":44788,\"start\":44542},{\"end\":46019,\"start\":44793},{\"end\":46228,\"start\":46027},{\"end\":46838,\"start\":46247},{\"end\":47236,\"start\":46848},{\"end\":47522,\"start\":47241},{\"end\":47820,\"start\":47524},{\"end\":47963,\"start\":47835},{\"end\":48654,\"start\":48348},{\"end\":49321,\"start\":49071},{\"end\":51229,\"start\":49448},{\"end\":51961,\"start\":51592},{\"end\":52782,\"start\":52271},{\"end\":53260,\"start\":52797},{\"end\":54696,\"start\":54189},{\"end\":55871,\"start\":55532}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14924,\"start\":14886},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14988,\"start\":14924},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16182,\"start\":16115},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16233,\"start\":16211},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16518,\"start\":16474},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17211,\"start\":17182},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17670,\"start\":17577},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17671,\"start\":17670},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18873,\"start\":18794},{\"attributes\":{\"id\":\"formula_9\"},\"end\":40613,\"start\":40594}]", "table_ref": "[{\"end\":22963,\"start\":22962},{\"end\":24398,\"start\":24397},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":36688,\"start\":36687}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2327,\"start\":2315},{\"attributes\":{\"n\":\"2\"},\"end\":8407,\"start\":8395},{\"attributes\":{\"n\":\"3\"},\"end\":11166,\"start\":11155},{\"attributes\":{\"n\":\"4\"},\"end\":13474,\"start\":13463},{\"attributes\":{\"n\":\"4.3\"},\"end\":20436,\"start\":20398},{\"attributes\":{\"n\":\"5\"},\"end\":22079,\"start\":22068},{\"attributes\":{\"n\":\"5.1\"},\"end\":22098,\"start\":22082},{\"attributes\":{\"n\":\"5.2\"},\"end\":22954,\"start\":22942},{\"attributes\":{\"n\":\"5.3\"},\"end\":27334,\"start\":27290},{\"attributes\":{\"n\":\"5.4\"},\"end\":28974,\"start\":28941},{\"attributes\":{\"n\":\"6\"},\"end\":30733,\"start\":30723},{\"end\":31466,\"start\":31456},{\"end\":32876,\"start\":32846},{\"end\":36331,\"start\":36286},{\"end\":41374,\"start\":41333},{\"end\":43020,\"start\":43010},{\"end\":43382,\"start\":43372},{\"end\":43735,\"start\":43717},{\"end\":44053,\"start\":44043},{\"end\":44539,\"start\":44529},{\"end\":46025,\"start\":46022},{\"end\":46244,\"start\":46231},{\"end\":46845,\"start\":46841},{\"end\":47832,\"start\":47823},{\"end\":48345,\"start\":48336},{\"end\":52794,\"start\":52785},{\"end\":54185,\"start\":54172}]", "table": "[{\"end\":48334,\"start\":47964},{\"end\":49067,\"start\":48655},{\"end\":49444,\"start\":49322},{\"end\":51588,\"start\":51230},{\"end\":52270,\"start\":51962},{\"end\":54170,\"start\":53261},{\"end\":55528,\"start\":54697},{\"end\":57783,\"start\":55872}]", "figure_caption": "[{\"end\":43370,\"start\":43022},{\"end\":43715,\"start\":43384},{\"end\":44041,\"start\":43738},{\"end\":44527,\"start\":44055},{\"end\":44789,\"start\":44541},{\"end\":46020,\"start\":44792},{\"end\":46229,\"start\":46026},{\"end\":46839,\"start\":46246},{\"end\":47237,\"start\":46847},{\"end\":47523,\"start\":47240},{\"end\":47964,\"start\":47834},{\"end\":48655,\"start\":48347},{\"end\":49322,\"start\":49070},{\"end\":51230,\"start\":49447},{\"end\":51962,\"start\":51591},{\"end\":53261,\"start\":52796},{\"end\":54697,\"start\":54188},{\"end\":55872,\"start\":55531}]", "figure_ref": "[{\"end\":3730,\"start\":3729},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5860,\"start\":5859},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6468,\"start\":6467},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12758,\"start\":12757},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13532,\"start\":13531},{\"end\":18117,\"start\":18116},{\"end\":20381,\"start\":20380},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23186,\"start\":23185},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26626,\"start\":26625},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28558,\"start\":28557},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":31978,\"start\":31977}]", "bib_author_first_name": "[{\"end\":59140,\"start\":59135},{\"end\":59149,\"start\":59146},{\"end\":59159,\"start\":59157},{\"end\":59171,\"start\":59165},{\"end\":59183,\"start\":59179},{\"end\":59192,\"start\":59189},{\"end\":59203,\"start\":59200},{\"end\":59216,\"start\":59209},{\"end\":59227,\"start\":59222},{\"end\":59360,\"start\":59353},{\"end\":59379,\"start\":59373},{\"end\":59393,\"start\":59387},{\"end\":59490,\"start\":59482},{\"end\":59508,\"start\":59504},{\"end\":59522,\"start\":59515},{\"end\":59535,\"start\":59530},{\"end\":59553,\"start\":59545},{\"end\":59568,\"start\":59562},{\"end\":59585,\"start\":59579},{\"end\":59605,\"start\":59599},{\"end\":59619,\"start\":59613},{\"end\":59727,\"start\":59721},{\"end\":59739,\"start\":59733},{\"end\":59749,\"start\":59745},{\"end\":59760,\"start\":59756},{\"end\":59777,\"start\":59770},{\"end\":59779,\"start\":59778},{\"end\":59793,\"start\":59789},{\"end\":60040,\"start\":60033},{\"end\":60056,\"start\":60049},{\"end\":60063,\"start\":60061},{\"end\":60073,\"start\":60069},{\"end\":60089,\"start\":60081},{\"end\":60097,\"start\":60094},{\"end\":60112,\"start\":60105},{\"end\":60126,\"start\":60120},{\"end\":60142,\"start\":60135},{\"end\":60157,\"start\":60151},{\"end\":60159,\"start\":60158},{\"end\":60173,\"start\":60170},{\"end\":60186,\"start\":60182},{\"end\":60188,\"start\":60187},{\"end\":60263,\"start\":60254},{\"end\":60281,\"start\":60275},{\"end\":60295,\"start\":60290},{\"end\":60311,\"start\":60304},{\"end\":60325,\"start\":60319},{\"end\":60338,\"start\":60334},{\"end\":60352,\"start\":60348},{\"end\":60366,\"start\":60361},{\"end\":60370,\"start\":60367},{\"end\":60385,\"start\":60378},{\"end\":60403,\"start\":60394},{\"end\":60540,\"start\":60535},{\"end\":60558,\"start\":60548},{\"end\":60572,\"start\":60564},{\"end\":60581,\"start\":60579},{\"end\":60602,\"start\":60593},{\"end\":60617,\"start\":60609},{\"end\":60630,\"start\":60624},{\"end\":60643,\"start\":60636},{\"end\":60743,\"start\":60740},{\"end\":60758,\"start\":60754},{\"end\":60772,\"start\":60766},{\"end\":60786,\"start\":60782},{\"end\":60944,\"start\":60941},{\"end\":60961,\"start\":60955},{\"end\":60980,\"start\":60976},{\"end\":60998,\"start\":60993},{\"end\":61016,\"start\":61011},{\"end\":61035,\"start\":61026},{\"end\":61059,\"start\":61052},{\"end\":61073,\"start\":61070},{\"end\":61288,\"start\":61284},{\"end\":61301,\"start\":61295},{\"end\":61314,\"start\":61307},{\"end\":61327,\"start\":61323},{\"end\":61344,\"start\":61337},{\"end\":61346,\"start\":61345},{\"end\":61360,\"start\":61356},{\"end\":61447,\"start\":61444},{\"end\":61459,\"start\":61452},{\"end\":61473,\"start\":61467},{\"end\":61475,\"start\":61474},{\"end\":61486,\"start\":61481},{\"end\":61499,\"start\":61493},{\"end\":61519,\"start\":61510},{\"end\":61529,\"start\":61524},{\"end\":61543,\"start\":61538},{\"end\":61555,\"start\":61550},{\"end\":61559,\"start\":61556},{\"end\":61569,\"start\":61564},{\"end\":61687,\"start\":61680},{\"end\":61777,\"start\":61772},{\"end\":61793,\"start\":61786},{\"end\":61795,\"start\":61794},{\"end\":62078,\"start\":62073},{\"end\":62095,\"start\":62088},{\"end\":62115,\"start\":62112},{\"end\":62226,\"start\":62223},{\"end\":62240,\"start\":62232},{\"end\":62252,\"start\":62246},{\"end\":62266,\"start\":62263},{\"end\":62281,\"start\":62274},{\"end\":62297,\"start\":62290},{\"end\":62314,\"start\":62306},{\"end\":62331,\"start\":62324},{\"end\":62341,\"start\":62337},{\"end\":62358,\"start\":62352},{\"end\":62377,\"start\":62372},{\"end\":62390,\"start\":62385},{\"end\":62405,\"start\":62401},{\"end\":62417,\"start\":62412},{\"end\":62428,\"start\":62425},{\"end\":62440,\"start\":62435},{\"end\":62451,\"start\":62447},{\"end\":62499,\"start\":62495},{\"end\":62515,\"start\":62509},{\"end\":62525,\"start\":62521},{\"end\":62538,\"start\":62532},{\"end\":62551,\"start\":62544},{\"end\":62553,\"start\":62552},{\"end\":62567,\"start\":62563},{\"end\":62693,\"start\":62689},{\"end\":62709,\"start\":62703},{\"end\":62721,\"start\":62715},{\"end\":62734,\"start\":62727},{\"end\":62736,\"start\":62735},{\"end\":62750,\"start\":62746},{\"end\":62766,\"start\":62760},{\"end\":62878,\"start\":62873},{\"end\":62893,\"start\":62888},{\"end\":62895,\"start\":62894},{\"end\":63016,\"start\":63011},{\"end\":63031,\"start\":63026},{\"end\":63033,\"start\":63032},{\"end\":63048,\"start\":63041},{\"end\":63050,\"start\":63049},{\"end\":63173,\"start\":63170},{\"end\":63191,\"start\":63185},{\"end\":63205,\"start\":63199},{\"end\":63218,\"start\":63214},{\"end\":63230,\"start\":63224},{\"end\":63244,\"start\":63240},{\"end\":63256,\"start\":63251},{\"end\":63472,\"start\":63466},{\"end\":63492,\"start\":63483},{\"end\":63509,\"start\":63503},{\"end\":63523,\"start\":63518},{\"end\":63543,\"start\":63537},{\"end\":63554,\"start\":63549},{\"end\":63572,\"start\":63567},{\"end\":63585,\"start\":63581},{\"end\":63590,\"start\":63586},{\"end\":63606,\"start\":63598},{\"end\":63623,\"start\":63618},{\"end\":63724,\"start\":63718},{\"end\":63739,\"start\":63732},{\"end\":63750,\"start\":63746},{\"end\":63762,\"start\":63757},{\"end\":63782,\"start\":63774},{\"end\":63795,\"start\":63790},{\"end\":63808,\"start\":63804},{\"end\":63822,\"start\":63813},{\"end\":63834,\"start\":63830},{\"end\":63949,\"start\":63940},{\"end\":63964,\"start\":63956},{\"end\":63977,\"start\":63970},{\"end\":63989,\"start\":63983},{\"end\":64159,\"start\":64153},{\"end\":64169,\"start\":64165},{\"end\":64185,\"start\":64179},{\"end\":64198,\"start\":64191},{\"end\":64200,\"start\":64199},{\"end\":64214,\"start\":64210},{\"end\":64365,\"start\":64359},{\"end\":64378,\"start\":64371},{\"end\":64395,\"start\":64387},{\"end\":64415,\"start\":64409},{\"end\":64428,\"start\":64422},{\"end\":64439,\"start\":64434},{\"end\":64451,\"start\":64446},{\"end\":64464,\"start\":64458},{\"end\":64476,\"start\":64472},{\"end\":64493,\"start\":64486},{\"end\":64495,\"start\":64494},{\"end\":64511,\"start\":64505},{\"end\":64522,\"start\":64518},{\"end\":64574,\"start\":64570},{\"end\":64592,\"start\":64585},{\"end\":64611,\"start\":64607},{\"end\":64624,\"start\":64620},{\"end\":64758,\"start\":64754},{\"end\":64770,\"start\":64766},{\"end\":64772,\"start\":64771},{\"end\":64785,\"start\":64781},{\"end\":64787,\"start\":64786},{\"end\":64857,\"start\":64852},{\"end\":64866,\"start\":64862},{\"end\":64880,\"start\":64873},{\"end\":64893,\"start\":64886},{\"end\":64904,\"start\":64900},{\"end\":64917,\"start\":64911},{\"end\":65118,\"start\":65116},{\"end\":65131,\"start\":65124},{\"end\":65145,\"start\":65138},{\"end\":65157,\"start\":65152},{\"end\":65170,\"start\":65164},{\"end\":65181,\"start\":65177},{\"end\":65295,\"start\":65288},{\"end\":65308,\"start\":65301},{\"end\":65319,\"start\":65315},{\"end\":65330,\"start\":65326},{\"end\":65342,\"start\":65340},{\"end\":65356,\"start\":65347},{\"end\":65559,\"start\":65552},{\"end\":65575,\"start\":65568},{\"end\":65588,\"start\":65583},{\"end\":65606,\"start\":65599},{\"end\":65704,\"start\":65697},{\"end\":65716,\"start\":65709},{\"end\":65731,\"start\":65722},{\"end\":65744,\"start\":65737},{\"end\":66041,\"start\":66036},{\"end\":66054,\"start\":66050},{\"end\":66068,\"start\":66064},{\"end\":66087,\"start\":66078},{\"end\":66099,\"start\":66093},{\"end\":66115,\"start\":66108},{\"end\":66129,\"start\":66124},{\"end\":66139,\"start\":66136},{\"end\":66149,\"start\":66144},{\"end\":66151,\"start\":66150},{\"end\":66213,\"start\":66208},{\"end\":66229,\"start\":66223},{\"end\":66246,\"start\":66235},{\"end\":66259,\"start\":66254},{\"end\":66275,\"start\":66269},{\"end\":66288,\"start\":66282},{\"end\":66303,\"start\":66298},{\"end\":66323,\"start\":66314},{\"end\":66329,\"start\":66324},{\"end\":66348,\"start\":66340},{\"end\":66536,\"start\":66531},{\"end\":66547,\"start\":66543},{\"end\":66559,\"start\":66554},{\"end\":66571,\"start\":66564},{\"end\":66582,\"start\":66576},{\"end\":66592,\"start\":66588},{\"end\":66609,\"start\":66602},{\"end\":66611,\"start\":66610},{\"end\":66625,\"start\":66621},{\"end\":66798,\"start\":66795},{\"end\":66814,\"start\":66808},{\"end\":66828,\"start\":66823},{\"end\":66840,\"start\":66837},{\"end\":67019,\"start\":67013},{\"end\":67034,\"start\":67027},{\"end\":67051,\"start\":67044},{\"end\":67067,\"start\":67060},{\"end\":67085,\"start\":67079},{\"end\":67104,\"start\":67099},{\"end\":67117,\"start\":67113},{\"end\":67130,\"start\":67123},{\"end\":67149,\"start\":67143},{\"end\":67164,\"start\":67159},{\"end\":67219,\"start\":67214},{\"end\":67237,\"start\":67231},{\"end\":67240,\"start\":67238},{\"end\":67255,\"start\":67250},{\"end\":67266,\"start\":67262},{\"end\":67282,\"start\":67276},{\"end\":67313,\"start\":67307},{\"end\":67327,\"start\":67321},{\"end\":67339,\"start\":67333},{\"end\":67347,\"start\":67345},{\"end\":67521,\"start\":67513},{\"end\":67544,\"start\":67536},{\"end\":67560,\"start\":67551},{\"end\":67577,\"start\":67568},{\"end\":67590,\"start\":67585},{\"end\":67729,\"start\":67722},{\"end\":67751,\"start\":67744},{\"end\":67766,\"start\":67760},{\"end\":67786,\"start\":67776},{\"end\":67805,\"start\":67797},{\"end\":67823,\"start\":67815},{\"end\":67838,\"start\":67833},{\"end\":67852,\"start\":67848},{\"end\":67866,\"start\":67860},{\"end\":67923,\"start\":67919},{\"end\":67938,\"start\":67933},{\"end\":67952,\"start\":67947},{\"end\":67965,\"start\":67960},{\"end\":67979,\"start\":67974},{\"end\":67998,\"start\":67991},{\"end\":68014,\"start\":68007},{\"end\":68032,\"start\":68026},{\"end\":68048,\"start\":68040},{\"end\":68065,\"start\":68059},{\"end\":68243,\"start\":68236},{\"end\":68256,\"start\":68249},{\"end\":68272,\"start\":68264},{\"end\":68286,\"start\":68280},{\"end\":68302,\"start\":68293},{\"end\":68312,\"start\":68310},{\"end\":68327,\"start\":68320},{\"end\":68341,\"start\":68332},{\"end\":68507,\"start\":68500},{\"end\":68520,\"start\":68513},{\"end\":68534,\"start\":68528},{\"end\":68547,\"start\":68539},{\"end\":68561,\"start\":68555},{\"end\":68575,\"start\":68568},{\"end\":68590,\"start\":68581},{\"end\":68731,\"start\":68725},{\"end\":68741,\"start\":68737},{\"end\":68767,\"start\":68761},{\"end\":68782,\"start\":68775},{\"end\":68795,\"start\":68787},{\"end\":68807,\"start\":68800},{\"end\":68929,\"start\":68922},{\"end\":68939,\"start\":68936},{\"end\":68951,\"start\":68945},{\"end\":68962,\"start\":68957},{\"end\":68976,\"start\":68968},{\"end\":68990,\"start\":68983},{\"end\":69005,\"start\":68998},{\"end\":69016,\"start\":69011},{\"end\":69029,\"start\":69021},{\"end\":69041,\"start\":69034},{\"end\":69177,\"start\":69174},{\"end\":69194,\"start\":69190},{\"end\":69333,\"start\":69329},{\"end\":69345,\"start\":69342},{\"end\":69361,\"start\":69356},{\"end\":69374,\"start\":69369},{\"end\":69508,\"start\":69503},{\"end\":69523,\"start\":69516},{\"end\":69537,\"start\":69532},{\"end\":69550,\"start\":69545},{\"end\":69564,\"start\":69560},{\"end\":69578,\"start\":69571},{\"end\":69596,\"start\":69585},{\"end\":69608,\"start\":69604},{\"end\":69619,\"start\":69615},{\"end\":69626,\"start\":69624},{\"end\":69680,\"start\":69677},{\"end\":69690,\"start\":69688},{\"end\":69702,\"start\":69696},{\"end\":69714,\"start\":69708},{\"end\":69726,\"start\":69722},{\"end\":69736,\"start\":69733},{\"end\":69747,\"start\":69744},{\"end\":69934,\"start\":69929},{\"end\":69946,\"start\":69942},{\"end\":69960,\"start\":69953},{\"end\":69971,\"start\":69967},{\"end\":69987,\"start\":69976},{\"end\":70002,\"start\":69995},{\"end\":70012,\"start\":70008},{\"end\":70024,\"start\":70022},{\"end\":70038,\"start\":70029},{\"end\":70149,\"start\":70142},{\"end\":70161,\"start\":70156},{\"end\":70172,\"start\":70166},{\"end\":70186,\"start\":70181},{\"end\":70189,\"start\":70187},{\"end\":70199,\"start\":70194}]", "bib_author_last_name": "[{\"end\":59144,\"start\":59141},{\"end\":59155,\"start\":59150},{\"end\":59163,\"start\":59160},{\"end\":59177,\"start\":59172},{\"end\":59187,\"start\":59184},{\"end\":59198,\"start\":59193},{\"end\":59207,\"start\":59204},{\"end\":59220,\"start\":59217},{\"end\":59232,\"start\":59228},{\"end\":59371,\"start\":59361},{\"end\":59385,\"start\":59380},{\"end\":59405,\"start\":59394},{\"end\":59502,\"start\":59491},{\"end\":59513,\"start\":59509},{\"end\":59528,\"start\":59523},{\"end\":59543,\"start\":59536},{\"end\":59560,\"start\":59554},{\"end\":59577,\"start\":59569},{\"end\":59597,\"start\":59586},{\"end\":59611,\"start\":59606},{\"end\":59626,\"start\":59620},{\"end\":59634,\"start\":59628},{\"end\":59731,\"start\":59728},{\"end\":59743,\"start\":59740},{\"end\":59754,\"start\":59750},{\"end\":59768,\"start\":59761},{\"end\":59787,\"start\":59780},{\"end\":59801,\"start\":59794},{\"end\":60047,\"start\":60041},{\"end\":60059,\"start\":60057},{\"end\":60067,\"start\":60064},{\"end\":60079,\"start\":60074},{\"end\":60092,\"start\":60090},{\"end\":60103,\"start\":60098},{\"end\":60118,\"start\":60113},{\"end\":60133,\"start\":60127},{\"end\":60149,\"start\":60143},{\"end\":60168,\"start\":60160},{\"end\":60180,\"start\":60174},{\"end\":60193,\"start\":60189},{\"end\":60273,\"start\":60264},{\"end\":60288,\"start\":60282},{\"end\":60302,\"start\":60296},{\"end\":60317,\"start\":60312},{\"end\":60332,\"start\":60326},{\"end\":60346,\"start\":60339},{\"end\":60359,\"start\":60353},{\"end\":60376,\"start\":60371},{\"end\":60392,\"start\":60386},{\"end\":60412,\"start\":60404},{\"end\":60546,\"start\":60541},{\"end\":60562,\"start\":60559},{\"end\":60577,\"start\":60573},{\"end\":60591,\"start\":60582},{\"end\":60607,\"start\":60603},{\"end\":60622,\"start\":60618},{\"end\":60634,\"start\":60631},{\"end\":60647,\"start\":60644},{\"end\":60752,\"start\":60744},{\"end\":60764,\"start\":60759},{\"end\":60780,\"start\":60773},{\"end\":60798,\"start\":60787},{\"end\":60953,\"start\":60945},{\"end\":60974,\"start\":60962},{\"end\":60991,\"start\":60981},{\"end\":61009,\"start\":60999},{\"end\":61024,\"start\":61017},{\"end\":61050,\"start\":61036},{\"end\":61068,\"start\":61060},{\"end\":61081,\"start\":61074},{\"end\":61091,\"start\":61083},{\"end\":61293,\"start\":61289},{\"end\":61305,\"start\":61302},{\"end\":61321,\"start\":61315},{\"end\":61335,\"start\":61328},{\"end\":61354,\"start\":61347},{\"end\":61368,\"start\":61361},{\"end\":61450,\"start\":61448},{\"end\":61465,\"start\":61460},{\"end\":61479,\"start\":61476},{\"end\":61491,\"start\":61487},{\"end\":61508,\"start\":61500},{\"end\":61522,\"start\":61520},{\"end\":61536,\"start\":61530},{\"end\":61548,\"start\":61544},{\"end\":61562,\"start\":61560},{\"end\":61575,\"start\":61570},{\"end\":61698,\"start\":61688},{\"end\":61784,\"start\":61778},{\"end\":61809,\"start\":61796},{\"end\":62086,\"start\":62079},{\"end\":62110,\"start\":62096},{\"end\":62123,\"start\":62116},{\"end\":62133,\"start\":62125},{\"end\":62230,\"start\":62227},{\"end\":62244,\"start\":62241},{\"end\":62261,\"start\":62253},{\"end\":62272,\"start\":62267},{\"end\":62288,\"start\":62282},{\"end\":62304,\"start\":62298},{\"end\":62322,\"start\":62315},{\"end\":62335,\"start\":62332},{\"end\":62350,\"start\":62342},{\"end\":62370,\"start\":62359},{\"end\":62383,\"start\":62378},{\"end\":62399,\"start\":62391},{\"end\":62410,\"start\":62406},{\"end\":62423,\"start\":62418},{\"end\":62433,\"start\":62429},{\"end\":62445,\"start\":62441},{\"end\":62455,\"start\":62452},{\"end\":62507,\"start\":62500},{\"end\":62519,\"start\":62516},{\"end\":62530,\"start\":62526},{\"end\":62542,\"start\":62539},{\"end\":62561,\"start\":62554},{\"end\":62575,\"start\":62568},{\"end\":62701,\"start\":62694},{\"end\":62713,\"start\":62710},{\"end\":62725,\"start\":62722},{\"end\":62744,\"start\":62737},{\"end\":62758,\"start\":62751},{\"end\":62769,\"start\":62767},{\"end\":62775,\"start\":62771},{\"end\":62800,\"start\":62786},{\"end\":62886,\"start\":62879},{\"end\":62901,\"start\":62896},{\"end\":63024,\"start\":63017},{\"end\":63039,\"start\":63034},{\"end\":63056,\"start\":63051},{\"end\":63183,\"start\":63174},{\"end\":63197,\"start\":63192},{\"end\":63212,\"start\":63206},{\"end\":63222,\"start\":63219},{\"end\":63238,\"start\":63231},{\"end\":63249,\"start\":63245},{\"end\":63267,\"start\":63257},{\"end\":63481,\"start\":63473},{\"end\":63501,\"start\":63493},{\"end\":63516,\"start\":63510},{\"end\":63535,\"start\":63524},{\"end\":63547,\"start\":63544},{\"end\":63565,\"start\":63555},{\"end\":63579,\"start\":63573},{\"end\":63596,\"start\":63591},{\"end\":63616,\"start\":63607},{\"end\":63629,\"start\":63624},{\"end\":63636,\"start\":63631},{\"end\":63730,\"start\":63725},{\"end\":63744,\"start\":63740},{\"end\":63755,\"start\":63751},{\"end\":63772,\"start\":63763},{\"end\":63788,\"start\":63783},{\"end\":63802,\"start\":63796},{\"end\":63811,\"start\":63809},{\"end\":63828,\"start\":63823},{\"end\":63842,\"start\":63835},{\"end\":63954,\"start\":63950},{\"end\":63968,\"start\":63965},{\"end\":63981,\"start\":63978},{\"end\":63992,\"start\":63990},{\"end\":63996,\"start\":63994},{\"end\":64163,\"start\":64160},{\"end\":64177,\"start\":64170},{\"end\":64189,\"start\":64186},{\"end\":64208,\"start\":64201},{\"end\":64222,\"start\":64215},{\"end\":64230,\"start\":64224},{\"end\":64369,\"start\":64366},{\"end\":64385,\"start\":64379},{\"end\":64407,\"start\":64396},{\"end\":64420,\"start\":64416},{\"end\":64432,\"start\":64429},{\"end\":64444,\"start\":64440},{\"end\":64456,\"start\":64452},{\"end\":64470,\"start\":64465},{\"end\":64484,\"start\":64477},{\"end\":64503,\"start\":64496},{\"end\":64516,\"start\":64512},{\"end\":64530,\"start\":64523},{\"end\":64583,\"start\":64575},{\"end\":64605,\"start\":64593},{\"end\":64618,\"start\":64612},{\"end\":64634,\"start\":64625},{\"end\":64764,\"start\":64759},{\"end\":64779,\"start\":64773},{\"end\":64793,\"start\":64788},{\"end\":64860,\"start\":64858},{\"end\":64871,\"start\":64867},{\"end\":64884,\"start\":64881},{\"end\":64898,\"start\":64894},{\"end\":64909,\"start\":64905},{\"end\":64922,\"start\":64918},{\"end\":65122,\"start\":65119},{\"end\":65136,\"start\":65132},{\"end\":65150,\"start\":65146},{\"end\":65162,\"start\":65158},{\"end\":65175,\"start\":65171},{\"end\":65185,\"start\":65182},{\"end\":65299,\"start\":65296},{\"end\":65313,\"start\":65309},{\"end\":65324,\"start\":65320},{\"end\":65338,\"start\":65331},{\"end\":65345,\"start\":65343},{\"end\":65362,\"start\":65357},{\"end\":65566,\"start\":65560},{\"end\":65581,\"start\":65576},{\"end\":65597,\"start\":65589},{\"end\":65613,\"start\":65607},{\"end\":65707,\"start\":65705},{\"end\":65720,\"start\":65717},{\"end\":65735,\"start\":65732},{\"end\":65748,\"start\":65745},{\"end\":65914,\"start\":65897},{\"end\":66048,\"start\":66042},{\"end\":66062,\"start\":66055},{\"end\":66076,\"start\":66069},{\"end\":66091,\"start\":66088},{\"end\":66106,\"start\":66100},{\"end\":66122,\"start\":66116},{\"end\":66134,\"start\":66130},{\"end\":66142,\"start\":66140},{\"end\":66155,\"start\":66152},{\"end\":66221,\"start\":66214},{\"end\":66233,\"start\":66230},{\"end\":66252,\"start\":66247},{\"end\":66267,\"start\":66260},{\"end\":66280,\"start\":66276},{\"end\":66296,\"start\":66289},{\"end\":66312,\"start\":66304},{\"end\":66338,\"start\":66330},{\"end\":66364,\"start\":66349},{\"end\":66371,\"start\":66366},{\"end\":66541,\"start\":66537},{\"end\":66552,\"start\":66548},{\"end\":66562,\"start\":66560},{\"end\":66574,\"start\":66572},{\"end\":66586,\"start\":66583},{\"end\":66600,\"start\":66593},{\"end\":66619,\"start\":66612},{\"end\":66633,\"start\":66626},{\"end\":66806,\"start\":66799},{\"end\":66821,\"start\":66815},{\"end\":66835,\"start\":66829},{\"end\":66847,\"start\":66841},{\"end\":67025,\"start\":67020},{\"end\":67042,\"start\":67035},{\"end\":67058,\"start\":67052},{\"end\":67077,\"start\":67068},{\"end\":67097,\"start\":67086},{\"end\":67111,\"start\":67105},{\"end\":67121,\"start\":67118},{\"end\":67141,\"start\":67131},{\"end\":67157,\"start\":67150},{\"end\":67176,\"start\":67165},{\"end\":67229,\"start\":67220},{\"end\":67248,\"start\":67241},{\"end\":67260,\"start\":67256},{\"end\":67274,\"start\":67267},{\"end\":67295,\"start\":67283},{\"end\":67305,\"start\":67297},{\"end\":67319,\"start\":67314},{\"end\":67331,\"start\":67328},{\"end\":67343,\"start\":67340},{\"end\":67353,\"start\":67348},{\"end\":67357,\"start\":67355},{\"end\":67534,\"start\":67522},{\"end\":67549,\"start\":67545},{\"end\":67566,\"start\":67561},{\"end\":67583,\"start\":67578},{\"end\":67603,\"start\":67591},{\"end\":67610,\"start\":67605},{\"end\":67742,\"start\":67730},{\"end\":67758,\"start\":67752},{\"end\":67774,\"start\":67767},{\"end\":67795,\"start\":67787},{\"end\":67813,\"start\":67806},{\"end\":67831,\"start\":67824},{\"end\":67846,\"start\":67839},{\"end\":67858,\"start\":67853},{\"end\":67873,\"start\":67867},{\"end\":67880,\"start\":67875},{\"end\":67931,\"start\":67924},{\"end\":67945,\"start\":67939},{\"end\":67958,\"start\":67953},{\"end\":67972,\"start\":67966},{\"end\":67989,\"start\":67980},{\"end\":68005,\"start\":67999},{\"end\":68024,\"start\":68015},{\"end\":68038,\"start\":68033},{\"end\":68057,\"start\":68049},{\"end\":68073,\"start\":68066},{\"end\":68247,\"start\":68244},{\"end\":68262,\"start\":68257},{\"end\":68278,\"start\":68273},{\"end\":68291,\"start\":68287},{\"end\":68308,\"start\":68303},{\"end\":68318,\"start\":68313},{\"end\":68330,\"start\":68328},{\"end\":68345,\"start\":68342},{\"end\":68511,\"start\":68508},{\"end\":68526,\"start\":68521},{\"end\":68537,\"start\":68535},{\"end\":68553,\"start\":68548},{\"end\":68566,\"start\":68562},{\"end\":68579,\"start\":68576},{\"end\":68594,\"start\":68591},{\"end\":68735,\"start\":68732},{\"end\":68759,\"start\":68742},{\"end\":68773,\"start\":68768},{\"end\":68785,\"start\":68783},{\"end\":68798,\"start\":68796},{\"end\":68810,\"start\":68808},{\"end\":68934,\"start\":68930},{\"end\":68943,\"start\":68940},{\"end\":68955,\"start\":68952},{\"end\":68966,\"start\":68963},{\"end\":68981,\"start\":68977},{\"end\":68996,\"start\":68991},{\"end\":69009,\"start\":69006},{\"end\":69019,\"start\":69017},{\"end\":69032,\"start\":69030},{\"end\":69044,\"start\":69042},{\"end\":69188,\"start\":69178},{\"end\":69198,\"start\":69195},{\"end\":69340,\"start\":69334},{\"end\":69354,\"start\":69346},{\"end\":69367,\"start\":69362},{\"end\":69385,\"start\":69375},{\"end\":69514,\"start\":69509},{\"end\":69530,\"start\":69524},{\"end\":69543,\"start\":69538},{\"end\":69558,\"start\":69551},{\"end\":69569,\"start\":69565},{\"end\":69583,\"start\":69579},{\"end\":69602,\"start\":69597},{\"end\":69613,\"start\":69609},{\"end\":69622,\"start\":69620},{\"end\":69639,\"start\":69627},{\"end\":69686,\"start\":69681},{\"end\":69694,\"start\":69691},{\"end\":69706,\"start\":69703},{\"end\":69720,\"start\":69715},{\"end\":69731,\"start\":69727},{\"end\":69742,\"start\":69737},{\"end\":69751,\"start\":69748},{\"end\":69940,\"start\":69935},{\"end\":69951,\"start\":69947},{\"end\":69965,\"start\":69961},{\"end\":69974,\"start\":69972},{\"end\":69993,\"start\":69988},{\"end\":70006,\"start\":70003},{\"end\":70020,\"start\":70013},{\"end\":70027,\"start\":70025},{\"end\":70044,\"start\":70039},{\"end\":70154,\"start\":70150},{\"end\":70164,\"start\":70162},{\"end\":70179,\"start\":70173},{\"end\":70192,\"start\":70190},{\"end\":70205,\"start\":70200}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2012.15701\",\"id\":\"b0\"},\"end\":59268,\"start\":59083},{\"attributes\":{\"doi\":\"arXiv:2109.12948\",\"id\":\"b1\"},\"end\":59441,\"start\":59270},{\"attributes\":{\"doi\":\"arXiv:2005.14165\",\"id\":\"b2\"},\"end\":59670,\"start\":59443},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":209531713},\"end\":59956,\"start\":59672},{\"attributes\":{\"id\":\"b4\"},\"end\":60205,\"start\":59958},{\"attributes\":{\"doi\":\"arXiv:2204.02311\",\"id\":\"b5\"},\"end\":60448,\"start\":60207},{\"attributes\":{\"doi\":\"arXiv:2009.07453\",\"id\":\"b6\"},\"end\":60683,\"start\":60450},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":251564521},\"end\":60857,\"start\":60685},{\"attributes\":{\"doi\":\"arXiv:2306.03078\",\"id\":\"b8\"},\"end\":61127,\"start\":60859},{\"attributes\":{\"id\":\"b9\"},\"end\":61374,\"start\":61129},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":245124124},\"end\":61631,\"start\":61376},{\"attributes\":{\"id\":\"b11\"},\"end\":61704,\"start\":61633},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":29872031},\"end\":61988,\"start\":61706},{\"attributes\":{\"doi\":\"arXiv:2210.17323\",\"id\":\"b13\"},\"end\":62169,\"start\":61990},{\"attributes\":{\"doi\":\"10.5281/zenodo.5371628\",\"id\":\"b14\"},\"end\":62493,\"start\":62171},{\"attributes\":{\"doi\":\"arXiv:2103.13630\",\"id\":\"b15\"},\"end\":62685,\"start\":62495},{\"attributes\":{\"id\":\"b16\"},\"end\":62782,\"start\":62687},{\"attributes\":{\"id\":\"b17\"},\"end\":62802,\"start\":62784},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7057040},\"end\":62958,\"start\":62804},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":61815367},\"end\":63116,\"start\":62960},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":221516475},\"end\":63416,\"start\":63118},{\"attributes\":{\"doi\":\"arXiv:2203.15556\",\"id\":\"b21\"},\"end\":63672,\"start\":63418},{\"attributes\":{\"id\":\"b22\"},\"end\":63848,\"start\":63674},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":252577582},\"end\":64151,\"start\":63850},{\"attributes\":{\"doi\":\"arXiv:2101.01321\",\"id\":\"b24\"},\"end\":64297,\"start\":64153},{\"attributes\":{\"doi\":\"arXiv:2302.14017\",\"id\":\"b25\"},\"end\":64566,\"start\":64299},{\"attributes\":{\"doi\":\"arXiv:2105.06990\",\"id\":\"b26\"},\"end\":64730,\"start\":64568},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":7785881},\"end\":64850,\"start\":64732},{\"attributes\":{\"doi\":\"arXiv:2302.04304\",\"id\":\"b28\"},\"end\":65035,\"start\":64852},{\"attributes\":{\"id\":\"b29\"},\"end\":65191,\"start\":65037},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":254069623},\"end\":65517,\"start\":65193},{\"attributes\":{\"id\":\"b31\"},\"end\":65619,\"start\":65519},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":253448704},\"end\":65872,\"start\":65621},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":30274066},\"end\":65951,\"start\":65874},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":204838007},\"end\":66206,\"start\":65953},{\"attributes\":{\"doi\":\"arXiv:2211.05100\",\"id\":\"b35\"},\"end\":66465,\"start\":66208},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":202565587},\"end\":66750,\"start\":66467},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":235166247},\"end\":66906,\"start\":66752},{\"attributes\":{\"doi\":\"arXiv:2201.11990\",\"id\":\"b38\"},\"end\":67212,\"start\":66908},{\"attributes\":{\"doi\":\"arXiv:2201.08239\",\"id\":\"b39\"},\"end\":67434,\"start\":67214},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":229363322},\"end\":67666,\"start\":67436},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b41\"},\"end\":67917,\"start\":67668},{\"attributes\":{\"doi\":\"arXiv:2307.09288\",\"id\":\"b42\"},\"end\":68155,\"start\":67919},{\"attributes\":{\"doi\":\"arXiv:2209.13325\",\"id\":\"b43\"},\"end\":68381,\"start\":68157},{\"attributes\":{\"doi\":\"arXiv:2304.09145\",\"id\":\"b44\"},\"end\":68630,\"start\":68383},{\"attributes\":{\"doi\":\"arXiv:2206.01861\",\"id\":\"b45\"},\"end\":68846,\"start\":68632},{\"attributes\":{\"doi\":\"arXiv:2304.01089\",\"id\":\"b46\"},\"end\":69080,\"start\":68848},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":218571099},\"end\":69327,\"start\":69082},{\"attributes\":{\"doi\":\"arXiv:1910.06188\",\"id\":\"b48\"},\"end\":69450,\"start\":69329},{\"attributes\":{\"doi\":\"arXiv:2205.01068\",\"id\":\"b49\"},\"end\":69675,\"start\":69452},{\"attributes\":{\"doi\":\"arXiv:2009.12812\",\"id\":\"b50\"},\"end\":69839,\"start\":69677},{\"attributes\":{\"id\":\"b51\"},\"end\":70050,\"start\":69841},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":59413897},\"end\":70261,\"start\":70052}]", "bib_title": "[{\"end\":59719,\"start\":59672},{\"end\":60738,\"start\":60685},{\"end\":61442,\"start\":61376},{\"end\":61770,\"start\":61706},{\"end\":62871,\"start\":62804},{\"end\":63009,\"start\":62960},{\"end\":63168,\"start\":63118},{\"end\":63938,\"start\":63850},{\"end\":64752,\"start\":64732},{\"end\":65286,\"start\":65193},{\"end\":65695,\"start\":65621},{\"end\":65895,\"start\":65874},{\"end\":66034,\"start\":65953},{\"end\":66529,\"start\":66467},{\"end\":66793,\"start\":66752},{\"end\":67511,\"start\":67436},{\"end\":69172,\"start\":69082},{\"end\":70140,\"start\":70052}]", "bib_author": "[{\"end\":59146,\"start\":59135},{\"end\":59157,\"start\":59146},{\"end\":59165,\"start\":59157},{\"end\":59179,\"start\":59165},{\"end\":59189,\"start\":59179},{\"end\":59200,\"start\":59189},{\"end\":59209,\"start\":59200},{\"end\":59222,\"start\":59209},{\"end\":59234,\"start\":59222},{\"end\":59373,\"start\":59353},{\"end\":59387,\"start\":59373},{\"end\":59407,\"start\":59387},{\"end\":59504,\"start\":59482},{\"end\":59515,\"start\":59504},{\"end\":59530,\"start\":59515},{\"end\":59545,\"start\":59530},{\"end\":59562,\"start\":59545},{\"end\":59579,\"start\":59562},{\"end\":59599,\"start\":59579},{\"end\":59613,\"start\":59599},{\"end\":59628,\"start\":59613},{\"end\":59636,\"start\":59628},{\"end\":59733,\"start\":59721},{\"end\":59745,\"start\":59733},{\"end\":59756,\"start\":59745},{\"end\":59770,\"start\":59756},{\"end\":59789,\"start\":59770},{\"end\":59803,\"start\":59789},{\"end\":60049,\"start\":60033},{\"end\":60061,\"start\":60049},{\"end\":60069,\"start\":60061},{\"end\":60081,\"start\":60069},{\"end\":60094,\"start\":60081},{\"end\":60105,\"start\":60094},{\"end\":60120,\"start\":60105},{\"end\":60135,\"start\":60120},{\"end\":60151,\"start\":60135},{\"end\":60170,\"start\":60151},{\"end\":60182,\"start\":60170},{\"end\":60195,\"start\":60182},{\"end\":60275,\"start\":60254},{\"end\":60290,\"start\":60275},{\"end\":60304,\"start\":60290},{\"end\":60319,\"start\":60304},{\"end\":60334,\"start\":60319},{\"end\":60348,\"start\":60334},{\"end\":60361,\"start\":60348},{\"end\":60378,\"start\":60361},{\"end\":60394,\"start\":60378},{\"end\":60414,\"start\":60394},{\"end\":60548,\"start\":60535},{\"end\":60564,\"start\":60548},{\"end\":60579,\"start\":60564},{\"end\":60593,\"start\":60579},{\"end\":60609,\"start\":60593},{\"end\":60624,\"start\":60609},{\"end\":60636,\"start\":60624},{\"end\":60649,\"start\":60636},{\"end\":60754,\"start\":60740},{\"end\":60766,\"start\":60754},{\"end\":60782,\"start\":60766},{\"end\":60800,\"start\":60782},{\"end\":60955,\"start\":60941},{\"end\":60976,\"start\":60955},{\"end\":60993,\"start\":60976},{\"end\":61011,\"start\":60993},{\"end\":61026,\"start\":61011},{\"end\":61052,\"start\":61026},{\"end\":61070,\"start\":61052},{\"end\":61083,\"start\":61070},{\"end\":61093,\"start\":61083},{\"end\":61295,\"start\":61284},{\"end\":61307,\"start\":61295},{\"end\":61323,\"start\":61307},{\"end\":61337,\"start\":61323},{\"end\":61356,\"start\":61337},{\"end\":61370,\"start\":61356},{\"end\":61452,\"start\":61444},{\"end\":61467,\"start\":61452},{\"end\":61481,\"start\":61467},{\"end\":61493,\"start\":61481},{\"end\":61510,\"start\":61493},{\"end\":61524,\"start\":61510},{\"end\":61538,\"start\":61524},{\"end\":61550,\"start\":61538},{\"end\":61564,\"start\":61550},{\"end\":61577,\"start\":61564},{\"end\":61700,\"start\":61680},{\"end\":61786,\"start\":61772},{\"end\":61811,\"start\":61786},{\"end\":62088,\"start\":62073},{\"end\":62112,\"start\":62088},{\"end\":62125,\"start\":62112},{\"end\":62135,\"start\":62125},{\"end\":62232,\"start\":62223},{\"end\":62246,\"start\":62232},{\"end\":62263,\"start\":62246},{\"end\":62274,\"start\":62263},{\"end\":62290,\"start\":62274},{\"end\":62306,\"start\":62290},{\"end\":62324,\"start\":62306},{\"end\":62337,\"start\":62324},{\"end\":62352,\"start\":62337},{\"end\":62372,\"start\":62352},{\"end\":62385,\"start\":62372},{\"end\":62401,\"start\":62385},{\"end\":62412,\"start\":62401},{\"end\":62425,\"start\":62412},{\"end\":62435,\"start\":62425},{\"end\":62447,\"start\":62435},{\"end\":62457,\"start\":62447},{\"end\":62509,\"start\":62495},{\"end\":62521,\"start\":62509},{\"end\":62532,\"start\":62521},{\"end\":62544,\"start\":62532},{\"end\":62563,\"start\":62544},{\"end\":62577,\"start\":62563},{\"end\":62703,\"start\":62689},{\"end\":62715,\"start\":62703},{\"end\":62727,\"start\":62715},{\"end\":62746,\"start\":62727},{\"end\":62760,\"start\":62746},{\"end\":62771,\"start\":62760},{\"end\":62777,\"start\":62771},{\"end\":62802,\"start\":62786},{\"end\":62888,\"start\":62873},{\"end\":62903,\"start\":62888},{\"end\":63026,\"start\":63011},{\"end\":63041,\"start\":63026},{\"end\":63058,\"start\":63041},{\"end\":63185,\"start\":63170},{\"end\":63199,\"start\":63185},{\"end\":63214,\"start\":63199},{\"end\":63224,\"start\":63214},{\"end\":63240,\"start\":63224},{\"end\":63251,\"start\":63240},{\"end\":63269,\"start\":63251},{\"end\":63483,\"start\":63466},{\"end\":63503,\"start\":63483},{\"end\":63518,\"start\":63503},{\"end\":63537,\"start\":63518},{\"end\":63549,\"start\":63537},{\"end\":63567,\"start\":63549},{\"end\":63581,\"start\":63567},{\"end\":63598,\"start\":63581},{\"end\":63618,\"start\":63598},{\"end\":63631,\"start\":63618},{\"end\":63638,\"start\":63631},{\"end\":63732,\"start\":63718},{\"end\":63746,\"start\":63732},{\"end\":63757,\"start\":63746},{\"end\":63774,\"start\":63757},{\"end\":63790,\"start\":63774},{\"end\":63804,\"start\":63790},{\"end\":63813,\"start\":63804},{\"end\":63830,\"start\":63813},{\"end\":63844,\"start\":63830},{\"end\":63956,\"start\":63940},{\"end\":63970,\"start\":63956},{\"end\":63983,\"start\":63970},{\"end\":63994,\"start\":63983},{\"end\":63998,\"start\":63994},{\"end\":64165,\"start\":64153},{\"end\":64179,\"start\":64165},{\"end\":64191,\"start\":64179},{\"end\":64210,\"start\":64191},{\"end\":64224,\"start\":64210},{\"end\":64232,\"start\":64224},{\"end\":64371,\"start\":64359},{\"end\":64387,\"start\":64371},{\"end\":64409,\"start\":64387},{\"end\":64422,\"start\":64409},{\"end\":64434,\"start\":64422},{\"end\":64446,\"start\":64434},{\"end\":64458,\"start\":64446},{\"end\":64472,\"start\":64458},{\"end\":64486,\"start\":64472},{\"end\":64505,\"start\":64486},{\"end\":64518,\"start\":64505},{\"end\":64532,\"start\":64518},{\"end\":64585,\"start\":64570},{\"end\":64607,\"start\":64585},{\"end\":64620,\"start\":64607},{\"end\":64636,\"start\":64620},{\"end\":64766,\"start\":64754},{\"end\":64781,\"start\":64766},{\"end\":64795,\"start\":64781},{\"end\":64862,\"start\":64852},{\"end\":64873,\"start\":64862},{\"end\":64886,\"start\":64873},{\"end\":64900,\"start\":64886},{\"end\":64911,\"start\":64900},{\"end\":64924,\"start\":64911},{\"end\":65124,\"start\":65116},{\"end\":65138,\"start\":65124},{\"end\":65152,\"start\":65138},{\"end\":65164,\"start\":65152},{\"end\":65177,\"start\":65164},{\"end\":65187,\"start\":65177},{\"end\":65301,\"start\":65288},{\"end\":65315,\"start\":65301},{\"end\":65326,\"start\":65315},{\"end\":65340,\"start\":65326},{\"end\":65347,\"start\":65340},{\"end\":65364,\"start\":65347},{\"end\":65568,\"start\":65552},{\"end\":65583,\"start\":65568},{\"end\":65599,\"start\":65583},{\"end\":65615,\"start\":65599},{\"end\":65709,\"start\":65697},{\"end\":65722,\"start\":65709},{\"end\":65737,\"start\":65722},{\"end\":65750,\"start\":65737},{\"end\":65916,\"start\":65897},{\"end\":66050,\"start\":66036},{\"end\":66064,\"start\":66050},{\"end\":66078,\"start\":66064},{\"end\":66093,\"start\":66078},{\"end\":66108,\"start\":66093},{\"end\":66124,\"start\":66108},{\"end\":66136,\"start\":66124},{\"end\":66144,\"start\":66136},{\"end\":66157,\"start\":66144},{\"end\":66223,\"start\":66208},{\"end\":66235,\"start\":66223},{\"end\":66254,\"start\":66235},{\"end\":66269,\"start\":66254},{\"end\":66282,\"start\":66269},{\"end\":66298,\"start\":66282},{\"end\":66314,\"start\":66298},{\"end\":66340,\"start\":66314},{\"end\":66366,\"start\":66340},{\"end\":66373,\"start\":66366},{\"end\":66543,\"start\":66531},{\"end\":66554,\"start\":66543},{\"end\":66564,\"start\":66554},{\"end\":66576,\"start\":66564},{\"end\":66588,\"start\":66576},{\"end\":66602,\"start\":66588},{\"end\":66621,\"start\":66602},{\"end\":66635,\"start\":66621},{\"end\":66808,\"start\":66795},{\"end\":66823,\"start\":66808},{\"end\":66837,\"start\":66823},{\"end\":66849,\"start\":66837},{\"end\":67027,\"start\":67013},{\"end\":67044,\"start\":67027},{\"end\":67060,\"start\":67044},{\"end\":67079,\"start\":67060},{\"end\":67099,\"start\":67079},{\"end\":67113,\"start\":67099},{\"end\":67123,\"start\":67113},{\"end\":67143,\"start\":67123},{\"end\":67159,\"start\":67143},{\"end\":67178,\"start\":67159},{\"end\":67231,\"start\":67214},{\"end\":67250,\"start\":67231},{\"end\":67262,\"start\":67250},{\"end\":67276,\"start\":67262},{\"end\":67297,\"start\":67276},{\"end\":67307,\"start\":67297},{\"end\":67321,\"start\":67307},{\"end\":67333,\"start\":67321},{\"end\":67345,\"start\":67333},{\"end\":67355,\"start\":67345},{\"end\":67359,\"start\":67355},{\"end\":67536,\"start\":67513},{\"end\":67551,\"start\":67536},{\"end\":67568,\"start\":67551},{\"end\":67585,\"start\":67568},{\"end\":67605,\"start\":67585},{\"end\":67612,\"start\":67605},{\"end\":67744,\"start\":67722},{\"end\":67760,\"start\":67744},{\"end\":67776,\"start\":67760},{\"end\":67797,\"start\":67776},{\"end\":67815,\"start\":67797},{\"end\":67833,\"start\":67815},{\"end\":67848,\"start\":67833},{\"end\":67860,\"start\":67848},{\"end\":67875,\"start\":67860},{\"end\":67882,\"start\":67875},{\"end\":67933,\"start\":67919},{\"end\":67947,\"start\":67933},{\"end\":67960,\"start\":67947},{\"end\":67974,\"start\":67960},{\"end\":67991,\"start\":67974},{\"end\":68007,\"start\":67991},{\"end\":68026,\"start\":68007},{\"end\":68040,\"start\":68026},{\"end\":68059,\"start\":68040},{\"end\":68075,\"start\":68059},{\"end\":68249,\"start\":68236},{\"end\":68264,\"start\":68249},{\"end\":68280,\"start\":68264},{\"end\":68293,\"start\":68280},{\"end\":68310,\"start\":68293},{\"end\":68320,\"start\":68310},{\"end\":68332,\"start\":68320},{\"end\":68347,\"start\":68332},{\"end\":68513,\"start\":68500},{\"end\":68528,\"start\":68513},{\"end\":68539,\"start\":68528},{\"end\":68555,\"start\":68539},{\"end\":68568,\"start\":68555},{\"end\":68581,\"start\":68568},{\"end\":68596,\"start\":68581},{\"end\":68737,\"start\":68725},{\"end\":68761,\"start\":68737},{\"end\":68775,\"start\":68761},{\"end\":68787,\"start\":68775},{\"end\":68800,\"start\":68787},{\"end\":68812,\"start\":68800},{\"end\":68936,\"start\":68922},{\"end\":68945,\"start\":68936},{\"end\":68957,\"start\":68945},{\"end\":68968,\"start\":68957},{\"end\":68983,\"start\":68968},{\"end\":68998,\"start\":68983},{\"end\":69011,\"start\":68998},{\"end\":69021,\"start\":69011},{\"end\":69034,\"start\":69021},{\"end\":69046,\"start\":69034},{\"end\":69190,\"start\":69174},{\"end\":69200,\"start\":69190},{\"end\":69342,\"start\":69329},{\"end\":69356,\"start\":69342},{\"end\":69369,\"start\":69356},{\"end\":69387,\"start\":69369},{\"end\":69516,\"start\":69503},{\"end\":69532,\"start\":69516},{\"end\":69545,\"start\":69532},{\"end\":69560,\"start\":69545},{\"end\":69571,\"start\":69560},{\"end\":69585,\"start\":69571},{\"end\":69604,\"start\":69585},{\"end\":69615,\"start\":69604},{\"end\":69624,\"start\":69615},{\"end\":69641,\"start\":69624},{\"end\":69688,\"start\":69677},{\"end\":69696,\"start\":69688},{\"end\":69708,\"start\":69696},{\"end\":69722,\"start\":69708},{\"end\":69733,\"start\":69722},{\"end\":69744,\"start\":69733},{\"end\":69753,\"start\":69744},{\"end\":69942,\"start\":69929},{\"end\":69953,\"start\":69942},{\"end\":69967,\"start\":69953},{\"end\":69976,\"start\":69967},{\"end\":69995,\"start\":69976},{\"end\":70008,\"start\":69995},{\"end\":70022,\"start\":70008},{\"end\":70029,\"start\":70022},{\"end\":70046,\"start\":70029},{\"end\":70156,\"start\":70142},{\"end\":70166,\"start\":70156},{\"end\":70181,\"start\":70166},{\"end\":70194,\"start\":70181},{\"end\":70207,\"start\":70194}]", "bib_venue": "[{\"end\":59952,\"start\":59886},{\"end\":61945,\"start\":61916},{\"end\":63412,\"start\":63349},{\"end\":64147,\"start\":64081},{\"end\":65513,\"start\":65447},{\"end\":65819,\"start\":65803},{\"end\":66744,\"start\":66698},{\"end\":59133,\"start\":59083},{\"end\":59351,\"start\":59270},{\"end\":59480,\"start\":59443},{\"end\":59884,\"start\":59803},{\"end\":60031,\"start\":59958},{\"end\":60252,\"start\":60207},{\"end\":60533,\"start\":60450},{\"end\":60849,\"start\":60800},{\"end\":60939,\"start\":60859},{\"end\":61282,\"start\":61129},{\"end\":61621,\"start\":61577},{\"end\":61678,\"start\":61633},{\"end\":61914,\"start\":61811},{\"end\":62071,\"start\":61990},{\"end\":62221,\"start\":62171},{\"end\":62664,\"start\":62593},{\"end\":62952,\"start\":62903},{\"end\":63106,\"start\":63058},{\"end\":63347,\"start\":63269},{\"end\":63464,\"start\":63418},{\"end\":63716,\"start\":63674},{\"end\":64079,\"start\":63998},{\"end\":64277,\"start\":64248},{\"end\":64357,\"start\":64299},{\"end\":64844,\"start\":64795},{\"end\":65015,\"start\":64940},{\"end\":65114,\"start\":65037},{\"end\":65445,\"start\":65364},{\"end\":65550,\"start\":65519},{\"end\":65801,\"start\":65750},{\"end\":65941,\"start\":65916},{\"end\":66197,\"start\":66157},{\"end\":66445,\"start\":66389},{\"end\":66696,\"start\":66635},{\"end\":66898,\"start\":66849},{\"end\":67011,\"start\":66908},{\"end\":67414,\"start\":67375},{\"end\":67656,\"start\":67612},{\"end\":67720,\"start\":67668},{\"end\":68133,\"start\":68091},{\"end\":68234,\"start\":68157},{\"end\":68498,\"start\":68383},{\"end\":68723,\"start\":68632},{\"end\":68920,\"start\":68848},{\"end\":69278,\"start\":69200},{\"end\":69430,\"start\":69403},{\"end\":69501,\"start\":69452},{\"end\":69819,\"start\":69769},{\"end\":69927,\"start\":69841},{\"end\":70251,\"start\":70207}]"}}}, "year": 2023, "month": 12, "day": 17}