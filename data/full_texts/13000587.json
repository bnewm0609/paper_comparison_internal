{"id": 13000587, "updated": "2023-11-25 15:39:36.812", "metadata": {"title": "Towards Understanding Action Recognition", "authors": "[{\"first\":\"Hueihan\",\"last\":\"Jhuang\",\"middle\":[]},{\"first\":\"Juergen\",\"last\":\"Gall\",\"middle\":[]},{\"first\":\"Silvia\",\"last\":\"Zuffi\",\"middle\":[]},{\"first\":\"Cordelia\",\"last\":\"Schmid\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Black\",\"middle\":[\"J.\"]}]", "venue": "2013 IEEE International Conference on Computer Vision", "journal": "2013 IEEE International Conference on Computer Vision", "publication_date": {"year": 2013, "month": null, "day": null}, "abstract": "Although action recognition in videos is widely studied, current methods often fail on real-world datasets. Many recent approaches improve accuracy and robustness to cope with challenging video sequences, but it is often unclear what affects the results most. This paper attempts to provide insights based on a systematic performance evaluation using thoroughly-annotated data of human actions. We annotate human Joints for the HMDB dataset (J-HMDB). This annotation can be used to derive ground truth optical flow and segmentation. We evaluate current methods using this dataset and systematically replace the output of various algorithms with ground truth. This enables us to discover what is important - for example, should we work on improving flow algorithms, estimating human bounding boxes, or enabling pose estimation? In summary, we find that high-level pose features greatly outperform low/mid level features, in particular, pose over time is critical, but current pose estimation algorithms are not yet reliable enough to provide this information. We also find that the accuracy of a top-performing action recognition framework can be greatly increased by refining the underlying low/mid level features, this suggests it is important to improve optical flow and human detection algorithms. Our analysis and J-HMDB dataset should facilitate a deeper understanding of action recognition algorithms.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2034014085", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/JhuangGZSB13", "doi": "10.1109/iccv.2013.396"}}, "content": {"source": {"pdf_hash": "f062696805240a163fa70ad7ce87ac1782e8b6f3", "pdf_src": "Anansi", "pdf_uri": "[\"https://inria.hal.science/hal-00906902/file/jhuangICCV2013.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://hal.inria.fr/hal-00906902/file/jhuangICCV2013.pdf", "status": "GREEN"}}, "grobid": {"id": "bbfb3ea2d9ba132c8be2345fef55fd07fd427c60", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f062696805240a163fa70ad7ce87ac1782e8b6f3.txt", "contents": "\nTowards understanding action recognition\nDec 2013\n\nHueihan Jhuang \nJurgen Gall \nSilvia Zuffi \nCordelia Schmid \nMichael J Black \nHueihan Jhuang \nJurgen Gall \nSilvia Zuffi \nCordelia Schmid \nMichael J \nHueihan Jhuang \nMPI for Intelligent Systems\nGermany\n\nJuergen Gall \nUniversity of Bonn\nGermany\n\nSilvia Zuffi \nBrown University\nUSA\n\nCordelia Schmid \nLEAR\nINRIA\nFrance\n\nMichael J Black \nMPI for Intelligent Systems\nGermany\n\nTowards understanding action recognition\n\nICCV -IEEE International Conference on Computer Vision\nSydney, AustraliaDec 201310.1109/ICCV.2013.396HAL Id: hal-00906902 https://inria.hal.science/hal-00906902 HAL is a multi-disciplinary open access archive for the deposit and dissemination of sci-entific research documents, whether they are pub-lished or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L'archive ouverte pluridisciplinaire HAL, est destin\u00e9e au d\u00e9p\u00f4t et \u00e0 la diffusion de documents scientifiques de niveau recherche, publi\u00e9s ou non, \u00e9manant des \u00e9tablissements d'enseignement et de recherche fran\u00e7ais ou \u00e9trangers, des laboratoires publics ou priv\u00e9s. To cite this version:\nAlthough action recognition in videos is widely studied, current methods often fail on real-world datasets. Many recent approaches improve accuracy and robustness to cope with challenging video sequences, but it is often unclear what affects the results most. This paper attempts to provide insights based on a systematic performance evaluation using thoroughly-annotated data of human actions. We annotate human Joints for the HMDB dataset (J-HMDB). This annotation can be used to derive ground truth optical flow and segmentation. We evaluate current methods using this dataset and systematically replace the output of various algorithms with ground truth. This enables us to discover what is important -for example, should we work on improving flow algorithms, estimating human bounding boxes, or enabling pose estimation? In summary, we find that highlevel pose features greatly outperform low/mid level features; in particular, pose over time is critical. While current pose estimation algorithms are far from perfect, features extracted from estimated pose on a subset of J-HMDB, in which the full body is visible, outperform low/mid-level features. We also find that the accuracy of the action recognition framework can be greatly increased by refining the underlying low/mid level features; this suggests it is important to improve optical flow and human detection algorithms. Our analysis and J-HMDB dataset should facilitate a deeper understanding of action recognition algorithms.\n\nIntroduction\n\nCurrent computer vision algorithms fall far below human performance on activity recognition tasks. While most computer vision algorithms perform very well on simple lab-recorded datasets [31], state-of-the-art approaches still struggle to recognize actions in more complex videos taken from public sources like movies [14,17]. According to [30], the HMDB51 dataset [14] is the most challenging dataset for vision algorithms, with the best method achieving only 48% accuracy. Many things might be limiting current meth-ods: weak visual cues or lack of high-level cues for example. Without a clear understanding of what makes a method perform well, it is difficult for the field to make progress.\n\nOur goal is twofold. First, towards understanding algorithms for human action recognition, we systematically analyze a recognition algorithm to better understand the limitations and to identify components where an algorithmic improvement would most likely increase the overall accuracy. Second, towards understanding intermediate data that would support recognition, we present insights on how much low-to high-level reasoning about the human is needed to recognize actions.\n\nSuch an analysis requires ground truth for a challenging dataset. We focus on one of the most challenging datasets for action recognition (HMDB51 [14]) and on the approach that achieves the best performance on this dataset (Dense Trajectories [30]). From HMDB51, we extract 928 clips comprising 21 action categories and annotate each frame using a 2D articulated human puppet model [36] that provides scale, pose, segmentation, coarse viewpoint, and dense optical flow for the humans in action. An example annotation is shown in Fig. 1 (a-d). We refer to this dataset as J-HMDB for \"joint-annotated HMDB\".\n\nJ-HMDB is valuable in terms of linking low-to-midlevel features with high-level poses; see Fig. 1 (e-h) for an illustration. Holistic approaches like [30] rely on low-level cues that are sampled from the entire video (e). Dense optical flow within the mask of the person (f) provides more detailed low-level information. Also, by identifying the person in action and their size, the sampling of the features can be concentrated on the region of interest (g). Higher-level pose features require the knowledge of joints (h) but can be semantically interpreted. Relations between joints (h) provide richer information and enable more complex models.\n\nPose has been used in early work on action recognition [3,32]. For a complex dataset such as ours however, typically low-to mid-level features are used instead of pose because pose estimation is hard. Recently, human pose as a feature for action recognition has been revisited [10,22,26,29,34]. In [34], it is shown that current ap- proaches for human pose estimation from multiple camera views are accurate enough for reliable action recognition. For monocular videos, several works show that current pose estimation algorithms are reliable enough to recognize actions on relatively simple datasets [10,26,29], however [22] shows that they are not good enough to classify fine-grained activities. Using J-HMDB, we show that ground truth pose information enables action recognition performance beyond current state-of-the-art methods. While our main focus is to analyze the potential impact of different cues, the dataset is also valuable for evaluating human pose estimation and human detection in videos.\n1 low level (e) baseline\nOur preliminary results show that pose features estimated from [33] perform much worse than the ground truth pose features, but they outperform low/mid level features for action recognition on clips where the full body is visible. We also show that human bounding boxes estimated by [2] and optical flow estimated by [27] do not improve the performance of current action recognition algorithms.\n\n\nRelated Studies and Datasets\n\nPrevious work has analyzed data in detail to understand algorithm performance in the context of object detection and image classification. In [20], a human study of visual recognition tasks is performed to identify the role of algorithms, data, and features. In [11], issues like occlusion, object size, or aspect ratio are examined for two classes of object detectors. Our work shares with these studies the idea that analyzing and understanding data is important to advance the state-of-the-art.\n\nPrevious datasets used to benchmark pose estimation or action recognition algorithms are summarized in Tab. 1. Existing datasets that contain action labels and pose annotations are typically recorded in a laboratory or static environment with actors performing specific actions. These are often unrealistic, resulting in lower intra-class variation than in real-world videos. While marker-based motion capture systems provide accurate 3D ground-truth pose data [12,15,19,25], they are impractical for recording realistic video data. Other datasets focus on narrow scenarios [22,28]. More realistic datasets for pose estimation and action recognition have been collected from TV or movie footage. Commonly considered sources for action recognition are sport activities [18], YouTube videos [21], or movie scenes [14,16]. In comparison to sport videos, actions annotated from movies are much more challenging as they present real-world background variation, exhibit more intraclass variation, and have more appearance variation due to viewpoint, scale, and occlusion. Since HMDB51 [14] is the most challenging dataset among the current movie datasets [30], we build on it to create J-HMDB.\n\nJ-HMDB is, however, more than a dataset of human actions; it could also serve as a benchmark for pose estimation and human detection. Buffy stickman [10] y y ETHZ PASCAL [8] y y estimation H3D [2] y y Leeds Sports [13] y y VideoPose [24] y y y action UCF50 [21] y y y HMDB51 [14] y y y recognition Hollywood2 [17] y y y Olympics [18] y y y pose HumanEvaII [25] y y y CMU-MMAC [15] y y y and Human 3.6M [12] y y y Berkeley MHAD [19] y y y action MPII Cooking [22] y y y TUM kitchen [28] y y y J-HMDB y y y y the approximate scale of the person is known [8,10,13]. These image-based datasets constitute a very small subset of all the possible variations of human poses and sizes because the subjects are not performing actions, with the exception of the Leeds Sports Pose Dataset [13]. The Video-Pose2 dataset [24] contains a number of annotated video clips taken from two TV series in order to evaluate pose estimation approaches on realistic data. The dataset is, however, limited to upper body pose estimation and contains very few clips. Our dataset presents a new challenge to the field of human pose estimation and tracking since it contains more variation in poses, humans sizes, camera motions, motion blur, and partial-or full-body visibility.\n\n\nThe Dataset\n\n\nSelection\n\nThe HMDB51 database [14] contains more than 5,100 clips of 51 different human actions collected from movies or the Internet. Annotating this entire dataset is impractical so J-HMDB is a subset with fewer categories. We excluded categories that contain mainly facial expressions like smiling, interactions with others such as shaking hands, and actions that can only be done in a specific way such as a cartwheel. The result contains 21 categories involving a single person in action: brush hair, catch, clap, climb stairs, golf, jump, kick ball, pick, pour, pull-up, push, run, shoot ball, shoot bow, shoot gun, sit, stand, swing baseball, throw, walk, wave. Since we focus on and annotate the person in action in each clip, we remove clips in which the actor is not obvious. For the remaining clips, we further crop them in time such that the first and last frame roughly correspond to the beginning and end of an action. This selection-andcleaning process results in 36-55 clips per action class with each clip containing 15-40 frames. In summary, there are 31,838 annotated frames in total. J-HMDB is available at http://jhmdb.is.tue.mpg.de.\n\n\nAnnotation\n\nFor annotation, we use a 2D puppet model [36] in which the human body is represented as a set of 10 body parts connected by 13 joints (shoulder, elbow, wrist, hip, knee, ankle, neck) and two landmarks (face and belly). We construct puppets in 16 viewpoints across the 360 degree radial space in the transverse plane. We built a graphical user interface to control the viewpoint and scale and in which the joints can be selected and moved in the image plane. The annotation involves adjusting the joint position so that the contours of the puppet align with image information [36]. In contrast to simple joint or limb annotations, the puppet model guarantees realistic limb size proportions, in particular in the context of occlusions, and also provides an approximate 2D shape of the human body. The annotated shapes are then used to compute the 2D optical flow corresponding to the human motion, which we call \"puppet flow\" [35]. The puppet mask (i.e. the region contained within the puppet) is also used to initialize GrabCut [23] to obtain a segmentation mask. Fig. 1 (b-d) shows a sample annotation.\n\nThe annotation is done using Amazon Mechanical Turk. To aid annotators, we provide the posed puppet on the first frame of each video clip. For each subsequent frame the interface initializes the joint positions and the scale with those of the previous frame. We manually correct annotation errors during a post-annotation screening process.\n\nIn summary, the person performing the action in each frame is annotated with his/her 2D joint positions, scale, viewpoint, segmentation, puppet mask and puppet flow. Details about the annotation interface and the distribution of joint locations, viewpoints, and scales of the annotations are provided on the website.\n\n\nTraining and testing set generation\n\nTraining and testing splits are generated as in [14]. For each action category, clips are randomly grouped into two sets with the constraint that the clips from the same video belong to the same set. We iterate the grouping until the ratio of the number of clips in the two sets and the ratio of the number of distinct video sources in the two sets are both close to 7:3. The 70% set is used for training and the 30% set for testing. Three splits are randomly generated and the performance reported here is the average of the three splits. Note that the number of training/testing clips is similar across categories and we report the per-video accuracy, which does not differ much from the per-class accuracy.\n\n\nStudy of low-level features\n\nWe focus our evaluation on the Dense Trajectories (DT) algorithm [30] since it is currently the best performing method on the HMDB51 database [14] and because it relies on video feature descriptors that are also used by other methods. We first review DT in Sec. 4.1, and then we replace pieces of the algorithm with the ground truth data to provide low, mid, and high level information in Sec. 4.2, Sec. 5 and Sec. 6.2 respectively.\n\n\nDT features\n\nThe DT algorithm [30] represents video data by dense trajectories along with motion and shape features around the trajectories. The feature points are densely sampled on each frame using a grid with a spacing of 5 pixels and at each of the 8 spatial scales which increase by a factor of 1 \u221a 2 . Feature points are further pruned to keep the ones whose eigenvalues of the auto-correlation matrix are larger than some threshold. For each frame, a dense optical flow field is computed w.r.t. the next frame using the OpenCV implementation of Gunnar Farneb\u00e4ck's algorithm [9]. A 3 \u00d7 3 median filter is applied to the flow field and this denoised flow is used to compute the trajectories of selected points through the 15 frames of the clip.\n\nFor each trajectory, L = 5 types of descriptors are computed, where each descriptor is normalized to have unit L 2 norm: Traj: Given a trajectory of length T = 15, the shape of the trajectory is described by a sequence of displacement vectors, corresponding to the translation along the xand y-coordinate across the trajectory. It is further normalized by the sum of displacement vector magnitudes, i.e. (\u2206Pt,...,\u2206P t+T \u22121 )\nt+T \u22121 j=t ||\u2206Pj || , where \u2206P t = (x t+1 \u2212 x t , y t+1 \u2212 y t ).\nHOG: Histograms of oriented gradients [5] of 8 bins are computed in a 32-pixels \u00d7 32-pixels \u00d7 15-frames spatiotemporal volume surrounding the trajectory. The volume is further subdivided into a spatio-temporal grid of size 2pixels \u00d7 2-pixels \u00d7 3-frames. HOF: Histograms of optical flow [16] are computed similarly as HOG except that there are 9 bins with the additional one corresponding to pixels with optical flow magnitude lower than a threshold. MBH: Motion boundary histograms [6] are computed separately for the horizontal and vertical gradients of the optical flow (giving two descriptors).\n\nFor each descriptor type, a codebook of size N = 4, 000 is formed by running k-means 8 times on a random selection of M = 100, 000 descriptors and taking the codebook with the lowest error. The features are computed using the pub-licly available source code of Dense Trajectories [30] with one modification. While in the original implementation, optical flow is computed for each scale of the spatial pyramid, we compute the flow at the full resolution and build a spatial pyramid of the flow. While this decreases the performance on our dataset by less than 1%, it is necessary to fairly evaluate the impact of the flow accuracy using the puppet flow, which is generated at the original video scale.\n\nFor classification, a non-linear SVM with RBF-\u03c7 2 kernel, k(x, y), is used and L types of descriptors are combined in a multi-channel setup as\nK(i, j) = exp \u2212 1 L L c=1 k(x c i ,x c j ) A c .\nHere, x c i is the c-th descriptor for the i-th video, A c is the mean of the \u03c7 2 distance between the training examples for the c-th channel. The multi-class classification is done by LIBSVM [4] using a one-vs-all approach. The performance is denoted as \"baseline\" in Tab. 2 (1), and the flow is shown in Fig. 2 (1).\n\n\nDT given puppet flow\n\nWe can not evaluate the gain of having perfect dense optical flow, and therefore perfect trajectories. Instead, we use the puppet flow as the ground truth motion in the foreground, i.e. within the puppet mask (pmask). When the body parts move only slightly from one frame to the next, the puppets do not always move correspondingly because small translations are not easily observed and annotated. To address this, we replace the puppet flow for each body part that does not move with the flow from the baseline.\n\nTo evaluate the quality of the foreground flow, we set the flow outside pmask to zero to disable tracks outside the foreground. We compare optical flow (of ) computed by Farneb\u00e4ck's method and puppet flow (pf ), as shown in Fig. 2 (2-3). Masking optical flow results in a 4 percentage points (pp) gain over the baseline, and masking puppet flow gives a 6 pp gain (Tab. 2 (2-3)). The gain mainly comes from HOF and MBH.\n\nWe dilate the puppet mask to include the narrow strip surrounding the person's contour, called Dmask. The width is scale dependent, ranging from 1 to 10 pixels with an average width of 6 pixels. Since the puppet flow is not defined outside the puppet mask, of is used on the narrow strip, as shown in Fig. 2 (4). Using Dmask increases the performance of (3) by 2.3 pp (Tab. 2 (4) vs. (3)). Comparing Fig. 2 (3) and (4) by the difference of the motion around the person's contour and that of the surrounding background, suggesting that the motion boundary might be important for action recognition. We further use of on the whole region outside pmask and pf within pmask, as shown in Fig. 2 (5), and use features within a bounding box that is 20% larger in the x and y directions relative to the tightest bounding box enclosing the puppet mask (bbox). This does not bring much overall gain over (4) but increases the performance of Traj, HOG and MBH (Tab. 2 (5) vs. (4)). We use features within bbox so that the result is comparable to Tab. 2 (2-4); i.e. only consider tracks/features in a subregion surrounding the foreground person. We also try to compute (5) with features from the whole frame. This results in a 5 pp gain over the baseline, with the main improvement coming from MBH.\n\nCombing the kernel of Tab. 2 (4) and (5) results in a further boost of overall gain as well as a gain for each individual descriptor over both (4) and (5) (Tab. 2 (6)). It is now clear that the flow-related descriptors, Traj, HOF and MBH have a large gain (6.2-16 pp) over the baseline. This shows that the DT descriptors can indeed be improved with the ground-truth puppet flow.\n\nAt last, we replace the Farneb\u00e4ck's flow with Clas-sic+NL flow [27]. The flow is visually smoother than the baseline flow, as shown in Fig. 2 (7), but it is not clear whether this explains the slight drop of performance over the baseline (Tab. 2 (7)).\n\n\nStudy of mid-level features\n\nEstimating the location and size of the human in action might be an easier task than estimating accurate pixel-wise flow. We therefore ask, without using the puppet flow, how helpful it is to know the region of interest, i.e. the image region in which the human in action occupies, and its size? In the section below, we only use Farneb\u00e4ck's flow (of ).\n\n\nDT given foreground mask\n\nWe consider two types of regions of interest: the dilated puppet mask Dmask and bbox described above. We consider two ways of masking, one is in the feature space (F); i.e. compute flow/descriptors on the whole frame then only use those from within the mask. The other is to mask in the image space (Im) by setting the pixel values outside the mask to zero at every frame and then compute flow/descriptors, as shown in Fig. 2 (10). Masking features results in a slight 1.9 pp gain over the baseline for bbox (Tab. 2 (8)); using Dmask instead of bbox results in similar performance. Masking images results in a much higher gain: 5.6 and 8 pp for bbox and Dmask respectively (Tab. 2 (9-10)); in particular, it results in a much higher gain for HOG than masking features (Tab. 2 (9) vs. (8)). The reason that masking images performs better than masking features could be that the boundary of the image mask guides the optical flow algorithm to be more accurate around the contour of the person (Fig. 2 (10) vs. (1)). Not surprisingly, applying masks in all the cases boosts the performance of HOG because it only represents the texture of the foreground person. Note that when masking frames with bbox, flow has artifacts around boundaries, but this does not seem to decrease the performance much compared to masking with Dmask.\n\nWe also consider bbox from a human detector [1]. In 50% of the images, the overlap between the predicted box and the ground truth box exceeds 50%. Using the predicted boxes as above for the features does not improve the baseline (Tab. 2 (12)) and masking frames gives much worse results (34.7%). This suggests that the human detector in [1] is not accurate enough to help action recognition.\n\n\nDT given scale\n\nWe resize all the frames as well as the corresponding Dmask such that all persons are around 200 pixels in height, and repeat the analysis in (10). This causes a slight 1.4 pp gain over (10), and the HOG alone has a 5 pp gain, suggesting that DT features are not perfectly scale invariant (Tab. 2 (11) vs. (10)). Finally, combining kernels of features relying on different low/mid level features results in a 12.4 pp gain over the baseline (Tab. 2 (13)).\n\nIt is interesting to see that for many paired comparisons, such as (5) vs. 6. Study of high-level features 6\n\n\n.1. Pose features\n\nFor action recognition with pose features, we use various types of descriptors derived from joint annotations.\n\nNTraj: For each frame, we have the xand ycoordinates of 15 joints. We first normalize joint positions w.r.t the scale of the underlying puppet. We then use as features the translation of the normalized joint positions along the x and y-coordinates (dx, dy), the direction of the translational vector (arctan( dy dx )), and the relative positions of normalized joint positions w.r.t the puppet center in a sequence of T frames. Here T is the trajectory length described in Sec. 4.1. Note that due to the nature of the puppet annotation tool, all 15 joint positions are available even if they are not annotated when they are occluded or outside the frame. In this case, the joints are in the neutral puppet positions. Unless otherwise specified, we use all 15 joints regardless of their visibility. There are totally 75 descriptor types (30 for positions, 30 for translations, and 15 for directions). Note that unlike Traj in Sec. 4.1, we consider features along the xand ycoordinate as separate descriptors, and this results in better performance than treating them as one descriptor. For Traj, translation is considered as the difference of positions between two adjacent frames along the trajectory. Here we use the differences between frame t and t + s; i.e. the feature of type f is a sequence (f t+s \u2212 f t , .., .f t+ks \u2212 f t+(k\u22121)s ), k = T \u2212t s . The idea is that, for a small s, the trajectories might have jitter caused by imperfect annotation, and a larger s would reveal \"true\" motions; we compare s = 1 and 3.\n\nNTraj+: Since it has been shown in [34] that relational features describing geometric relations between joints perform better than using normalized joint positions, we also extract a set of relational features: C 15 2 = 105 distances between all the pairs of joints, 105 orientations of the vector connecting two joints, and 3 \u00d7 C 15 3 = 1365 inner angles spanned by two vectors connecting all the triples of joints, as shown in Fig. 1 (d). All possible relational features are computed for each frame, yielding 1575 descriptor types. In addition to using relational features, we also use the differences of relations between frame t and t + s as described in NTraj. There are in total 3225 descriptor types (75 for NTraj, 1575 for relations, and 1575 for their difference).\n\nFor each descriptor type, all the training samples are used to generate a codebook. We compare several small codebook sizes, N = 10, 20 and 50, because each descriptor has a small dimensionality. The performance is similar and, hereafter, we report results of N = 20.   Figure 3 shows the performance of the position-based NTraj and the position-and-relation-based NTraj+ with respect to the trajectory length T and the frame step size s. It shows that a large step size (s = 3) results in higher accuracy and that having temporal information (T > 1) is very important although the trajectory length is not critical beyond T = 7 frames. It also shows that using relation features in addition to position-based features is key to increasing accuracy. Hereafter we report the performance of s = 3 and T = 7 for NTraj+; i.e. 76.0%.\n\nTo evaluate the sensitivity of the performance to the variance of joint positions, we add Gaussian noise to every joint in every frame; the noise has zero mean and the variance is x \u00d7 (the distance to the closest joint), with x \u2264 1. The rationale is that a joint, even not perfectly estimated or annotated, is unlikely to be confused with its nearby joints because of the limbs and torso connecting them. With the noise, the performance drop is less than 2 pp.\n\n\nDT given joints\n\nWe also consider a sparse version of DT that tracks the 15 joint positions instead of tracking dense points. We use a smaller codebook size (N = 100) because here there are only 15 trajectories per frame. The trajectories are ordered to encode high-level pose information; i.e. there are 75 descriptor types (15 joints \u00d7 5 types in Sec. 4.1).\n\nSince not all the joints are visible within a frame, we use a subset of J-HMDB that has all the joints inside the frame, denoted as sub-J-HMDB. The subset contains 316 clips distributed over 12 categories. The baseline performance on the subset is 10.6 pp lower than on the full set although the chance level of the former is lower (Tab. 3 (1) vs. Tab. 2 (1)). This suggests that the subset is more challenging, which could be because it contains only full body actions (e.g. kicking); these might exhibit richer variation in terms of appearance and optical flow than partial body actions (e.g. pour). Note that here we combine the texture features HOG, HOF and MBH into HOX. We also evaluate  Table 3. The impact of high-level feature modifications on sub-J-HMDB. ALL is the combination of HOX/Traj. ALL+ is the combination of ALL/NTraj+, see Sec. 6.2 for details.\n\nDT given low and low/mid-level information as in Tab. 2 (6) and (13) respectively. The gain over the baseline is 8.0 and 17.2 pp respectively (Tab. 3 (2) and (3)). We then compute the sparse version of DT with given joint positions. We firstly recognize that the overall accuracy is the same as DT given low/mid level information (ALL in Tab. 3 (4) vs. (3)). A closer look at the performance of individual descriptors reveals that the texture-based HOX benefits more given low/mid-level than high-level information, while the position-based Traj shows the opposite. This is consistent with the intuition that HOX relates more to low/mid level cues while Traj to high-level cues. We also observe that, using the same flow setting, the sparse HOX performs better than the dense HOX by 5 pp (Tab. 3 (4) vs.\n\n(2)). This suggests that representing texture around joints is not only more effective but also more discriminative than representing the texture in the whole frame.\n\nWe then evaluate the position-and-relation-based NTraj+ on this subset (Tab. 3 (4)), the performance is similar to that on the full set (75.1% vs. 76.0%). It dramatically outperforms Traj by 24.1 pp, as well as the combination of HOX and Traj (i.e. ALL), showing that the high-level pose feature derived from normalized joints positions and their relations is the best feature for action recognition. While combining HOX and Traj improves performance, combining them with NTraj+ does not increase the performance of the latter (NTraj+ vs. ALL+ in Tab. 3 (4)), suggesting that texture features do not add much additional information when the pose features are already thoroughly extracted.\n\nThe subset sub-J-HMDB also allows us to evaluate the pose estimation algorithm from [33], which assumes the full body is visible. Using the error measurement in [7] with threshold 0.15, the pose estimation accuracy is 22.4%. There is no strong correlation between scale and accuracy but the correctly detected images mostly have people with non-occluded frontal views of upright poses. Dense Trajectories given estimated joints results in a 3.8 pp gain over the baseline, and NTraj+ computed from the 15 estimated joint positions results in a 8.1 pp gain over the baseline (Tab. 3 (5)). This suggests that while the estimated joint positions are not accurate compared to the ground truth, the derived pose features already outperform low/mid level features for action recognition.  Table 4. Overview of the recognition rate for both datasets. Table 4 summarizes the improvements to Dense Trajectories realized by providing low/mid-level and high-level features on the full dataset J-HMDB and the subset sub-J-HMDB. Overall, the two sets show a 12-17 pp improvement over the baseline with ground truth low/mid features and a 19-29 pp improvement with high-level features.\n\n\nSummary\n\n\nDiscussion\n\nWe have presented a complex, annotated, video dataset in order to analyze action recognition algorithms. Starting with a state-of-the-art method [30], we supply the algorithm with a range of low-to-high-level ground truth information. Our experiments show that there are several ways to improve action recognition without changing the existing framework. This includes improving low-level flow to improve the motion-based HOF and MBH and integrating mid-level information such a bounding box surrounding the person to improve the frame-based HOG. A surprising result is that the motion boundaries around a person's body contour seem to contain information for action recognition that is as important as the optical flow within the region of the body. It is also surprising that, with a good bounding box, which is probably easier to achieve than estimating accurate flow, one can obtain a large improvement over the baseline. Unfortunately, the human detector we evaluated is not accurate enough to predict such bounding boxes.\n\nDespite all the modifications to the Dense Trajectories algorithm using low-to-mid ground truth data, we find that the best features for action recognition (of those tested) are high-level pose features. While this might not be surprising, our contribution here is threefold. First, we point out that pose over time is the best representation for action recognition; we also point out several factors that are important to make good pose features, such as the use of relations, the number of frames, and the step size between frames in a trajectory. Second, the sparse version of Dense Trajectories as well as sub-J-HMDB allows a fair comparison between joint-wise low/mid-level texture features and high-level pose features. We observe that the texture around joints is more discriminative and effective than dense texture on the whole frame, but the low-level texture around joints performs worse than the high-level positionand-relation-based features derived directly from joint positions. Third, for sub-J-HMDB, where the full body is visible, a recent pose estimation algorithm computes poses that are more reliable than low/mid level features for action recognition of complex actions in realistic videos.\n\nBeyond understanding algorithms for action recognition, J-HMDB can serve as a challenge to the fields of pose estimation, flow estimation, and human detection.\n\nFigure 1 .\n1Overview of our annotation and evaluation. (a-d) A video frame annotated by a puppet model [36]. (a) image frame, (b) puppet flow [35], (c) puppet mask, (d) joint positions and relations. Three types of joint relations are used: 1) distance and 2) orientation of the vector connecting pairs of joints; i.e. the magnitude and the direction of the vector u. 3) Inner angle spanned by two vectors connecting triples of joints; i.e. the angle between the two vectors u and v. (e-h) From left to right, we gradually provide the baseline algorithm (e) with different levels of ground truth from (b) to (d). The trajectories are displayed in green.\n\n\nMost pose datasets contain images of a single non-occluded person in the center of the image\n\nFigure 2 .\n2Comparison of various flow settings. The flow is numbered according to Tab. 2. See Sec. 4.2 and Sec. 5 for details.\n\n\n(6),(1) vs.(7),(10) vs.(11), the amount of performance change for an individual descriptor does not always result in a similar amount of overall performance change, indicating that the features are not very complimentary, but have different error characteristics.\n\nFigure 3 .\n3Performance of pose features as a function of the trajectory length T and the frame step size s, see Sec. 6.1.\n\nTable 1 .\n1Related datasets.\n\n\n, the latter has clear flow discontinuities caused DT given low level features in Sec. 4Table 2. The impact of low and mid level feature modifications on J-HMDB. of and pf denote the optical flow computed by Farneb\u00e4ck's method and puppet flow, respectively. pmask denotes the puppet mask and Dmask the dilated pmask. F and Im corresponds to masking in the feature space and in the image space, respectively. bbox is 20% larger in the x and y dimensions than the tightest box enclosing pmask.Traj HOG HOF MBH ALL \n1) baseline \n40.0 32.9 \n40.1 \n51.1 \n56.6 \n2) of pmask \n38.5 31.9 \n46.0 \n58.7 \n60.4 \n3) pf pmask \n36.4 32.8 \n48.0 \n58.3 \n62.4 \n4) pf Dmask \n38.0 32.2 \n46.4 \n60.8 \n64.7 \n5) pf pmask \n43.0 36.1 \n44.1 \n63.6 \n65.3 \nof outside pmask \n6) 4) + 5) \n46.2 35.2 \n51.7 \n67.0 \n67.2 \n7) 1) w. [27] \n32.8 30.4 \n36.1 \n47.8 \n54.7 \nDT given mid level features in Sec. 5 \n8) bbox F \n38.5 34.9 \n42.2 \n51.1 \n58.5 \n9) bbox Im \n42.7 46.9 \n44.5 \n57.0 \n62.2 \n10) Dmask Im \n41.4 47.0 \n45.6 \n58.3 \n64.6 \n11) unit scale \n45.3 52.1 \n48.2 \n60.9 \n66.0 \n+Dmask Im \n12) 8) w. [1] \n37.7 33.9 \n39.0 \n52.2 \n56.7 \nDT given low + mid level features in Sec. 5 \n13) 4) + 5) + 11) \n51.3 49.4 \n54.4 \n68.7 \n69.0 \n\n\nAcknowledgements: JG was supported in part by the DFG Emmy Noether program (GA 1927/1-1) and CS by the ERC advanced grant Allegro.\nDetecting people using mutually consistent poselet activations. ECCV. L Bourdev, S Maji, T Brox, J Malik, L. Bourdev, S. Maji, T. Brox, and J. Malik. Detecting peo- ple using mutually consistent poselet activations. ECCV, pp. 168-181, 2010. 5\n\nPoselets: Body part detectors trained using 3D human pose annotations. ICCV. L Bourdev, J Malik, 23L. Bourdev and J. Malik. Poselets: Body part detectors trained using 3D human pose annotations. ICCV, pp. 1365- 1372, 2009. 2, 3\n\nRecognition of human body motion using phase space constraints. ICCV. L Campbell, A Bobick, L. Campbell and A. Bobick. Recognition of human body motion using phase space constraints. ICCV, pp. 624 -630, 1995. 1\n\nLIBSVM: A library for support vector machines. C.-C Chang, C.-J Lin, ACM TIST. 2C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM TIST, 2:27:1-27:27, 2011. 4\n\nHistograms of oriented gradients for human detection. CVPR. N Dalal, B Triggs, N. Dalal, , and B. Triggs. Histograms of oriented gradients for human detection. CVPR, pp. 886-893, 2005. 4\n\nHuman detection using oriented histograms of flow and appearance. ECCV. N Dalal, B Triggs, C Schmid, N. Dalal, , B. Triggs, and C. Schmid. Human detection using oriented histograms of flow and appearance. ECCV, pp. 428- 441, 2006. 4\n\nHuman pose estimation using body parts dependent joint regressors. CVPR. M Dantone, J Gall, C Leistner, L Van Gool, M. Dantone, J. Gall, C. Leistner, and L. Van Gool. Human pose estimation using body parts dependent joint regressors. CVPR, pp. 3041-3048, 2013. 7\n\nBetter appearance models for pictorial structures. M Eichner, V Ferrari, BMVC. 11M. Eichner and V. Ferrari. Better appearance models for pictorial structures. BMVC, pp. 3.1-3.11, 2009. 3\n\nTwo-frame motion estimation based on polynomial expansion. G Farneback, Image Analysis. 4G. Farneback. Two-frame motion estimation based on poly- nomial expansion. Image Analysis, pp. 363-370, 2003. 4\n\nProgressive search space reduction for human pose estimation. CVPR. V Ferrari, M Marin-Jimenez, A Zisserman, 13V. Ferrari, M. Marin-Jimenez, and A. Zisserman. Progressive search space reduction for human pose estimation. CVPR, pp. 1-8, 2008. 1, 2, 3\n\nDiagnosing error in object detectors. ECCV. D Hoiem, Y Chodpathumwan, Q Dai, D. Hoiem, Y. Chodpathumwan, and Q. Dai. Diagnosing error in object detectors. ECCV, pp. 340-353, 2012. 2\n\nHuman 3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments. C Ionescu, D Papava, V Olaru, C Sminchisescu, PAMI. 23C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu. Hu- man 3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments. PAMI, 2014. 2, 3\n\nClustered pose and nonlinear appearance models for human pose estimation. BMVC. S Johnson, M Everingham, 113S. Johnson and M. Everingham. Clustered pose and nonlin- ear appearance models for human pose estimation. BMVC, pp. 12.1-12.11, 2010. 3\n\nHMDB: A large video database for human motion recognition. ICCV. H Kuehne, H Jhuang, E Garrote, T Poggio, T Serre, H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. HMDB: A large video database for human motion recogni- tion. ICCV, pp. 2556 -2563, 2011. 1, 2, 3, 4\n\nGuide to the Carnegie Mellon University multimodal activity (CMU-MMAC) database. F D La Torre, J Hodgins, J Montano, S Valcarcel, R Forcada, J Macey, CMU-RI-TR-08-2223Tech. ReportF. D. la Torre, J. Hodgins, J. Montano, S. Valcarcel, R. For- cada, and J. Macey. Guide to the Carnegie Mellon University multimodal activity (CMU-MMAC) database. Tech. Report CMU-RI-TR-08-22, July 2009. 2, 3\n\nLearning realistic human actions from movies. CVPR. I Laptev, M Marszalek, C Schmid, B Rozenfeld, 24I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld. Learning realistic human actions from movies. CVPR, pp. 1- 8, 2008. 2, 4\n\nM Marszalek, I Laptev, C Schmid, Actions in context. CVPR. 13M. Marszalek, I. Laptev, and C. Schmid. Actions in context. CVPR, pp. 2929-2936, 2009. 1, 3\n\nModeling temporal structure of decomposable motion segments for activity classification. ECCV. J Niebles, C Chen, L Fei-Fei, 23J. Niebles, C. Chen, and L. Fei-Fei. Modeling temporal structure of decomposable motion segments for activity clas- sification. ECCV, pp. 392-405, 2010. 2, 3\n\nBerkeley MHAD: A comprehensive multimodal human action database. WACV. F Ofli, R Chaudhry, G Kurillo, R Vidal, R Bajcsy, 23F. Ofli, R. Chaudhry, G. Kurillo, R. Vidal, and R. Bajcsy. Berkeley MHAD: A comprehensive multimodal human ac- tion database. WACV, pp. 53-60, 2013. 2, 3\n\nThe role of features, algorithms and data in visual recognition. CVPR. D Parikh, C L Zitnick, D. Parikh and C. L. Zitnick. The role of features, algorithms and data in visual recognition. CVPR, pp. 2328-2335, 2010. 2\n\nRecognizing 50 human action categories of web videos. K Reddy, M Shah, MVA. 2453K. Reddy and M. Shah. Recognizing 50 human action cate- gories of web videos. MVA, 24(5):971-981, 2013. 2, 3\n\nA database for fine grained activity detection of cooking activities. CVPR. M Rohrbach, S Amin, M Andriluka, B Schiele, 13M. Rohrbach, S. Amin, M. Andriluka, and B. Schiele. A database for fine grained activity detection of cooking activ- ities. CVPR, pp. 1194-1201, 2012. 1, 2, 3\n\nGrabCut: Interactive foreground extraction using iterated graph cuts. C Rother, V Kolmogorov, A Blake, SIG-GRAPHC. Rother, V. Kolmogorov, and A. Blake. GrabCut: Inter- active foreground extraction using iterated graph cuts. SIG- GRAPH, pp. 309-314, 2004. 3\n\nParsing human motion with stretchable models. CVPR. B Sapp, D Weiss, B Taskar, B. Sapp, D. Weiss, and B. Taskar. Parsing human motion with stretchable models. CVPR, pp. 1281-1288, 2011. 3\n\nHumanEva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion. L Sigal, A Balan, M Black, IJCV. 8713L. Sigal, A. Balan, and M. Black. HumanEva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion. IJCV, 87(1):4-27, 2010. 2, 3\n\nAction recognition in cluttered dynamic scenes using pose-specific part models. ICCV. V K Singh, R Nevatia, 1V. K. Singh and R. Nevatia. Action recognition in clut- tered dynamic scenes using pose-specific part models. ICCV, pp. 113-120, 2011. 1, 2\n\nA quantitative analysis of current practices in optical flow estimation and the principles behind them. D Sun, S Roth, M Black, 5IJCV, to appearD. Sun, S. Roth, and M. Black. A quantitative analysis of current practices in optical flow estimation and the principles behind them. IJCV, to appear, 2013. 2, 5\n\nThe TUM kitchen data set of everyday manipulation activities for motion tracking and action recognition. THEMIS. M Tenorth, J Bandouch, M Beetz, 23M. Tenorth, J. Bandouch, and M. Beetz. The TUM kitchen data set of everyday manipulation activities for motion track- ing and action recognition. THEMIS, pp. 1089-1096, 2009. 2, 3\n\nModeling motion of body parts for action recognition. BMVC. K Tran, I Kakadiaris, S Shah, 12K. Tran, I. Kakadiaris, and S. Shah. Modeling motion of body parts for action recognition. BMVC, pp. 64.1-64.12, 2011. 1, 2\n\nDense trajectories and motion boundary descriptors for action recognition. H Wang, A Kl\u00e4ser, C Schmid, C.-L Liu, IJCV. 1031H. Wang, A. Kl\u00e4ser, C. Schmid, and C.-L. Liu. Dense tra- jectories and motion boundary descriptors for action recog- nition. IJCV, 103(1):60-79, 2013. 1, 2, 3, 4, 7\n\nA survey of visionbased methods for action representation, segmentation and recognition. D Weinland, R Ronfard, E Boyer, CVIU. 11521D. Weinland, R. Ronfard, and E. Boyer. A survey of vision- based methods for action representation, segmentation and recognition. CVIU, 115(2):224-241, 2010. 1\n\nParameterized modeling and recognition of activities. Y Yacoob, M Black, CVIU. 732Y. Yacoob and M. Black. Parameterized modeling and recog- nition of activities. CVIU, 73(2):232-247, 1999. 1\n\nArticulated human detection with flexible mixtures of parts. PAMI, to appear. Y Yang, D Ramanan, 27Y. Yang and D. Ramanan. Articulated human detection with flexible mixtures of parts. PAMI, to appear. 2, 7\n\nCoupled action recognition and pose estimation from multiple views. A Yao, J Gall, L Van Gool, IJCV. 10016A. Yao, J. Gall, and L. Van Gool. Coupled action recognition and pose estimation from multiple views. IJCV, 100(1):16- 37, 2012. 1, 6\n\nPuppet flow. S Zuffi, M Black, TR- IS-MPI-00723MPI for Intelligent SystemsTechnical ReportS. Zuffi and M. Black. Puppet flow. Technical Report TR- IS-MPI-007, MPI for Intelligent Systems, 2013. 2, 3\n\nS Zuffi, O Freifeld, M Black, From pictorial structures to deformable structures. CVPR. 13S. Zuffi, O. Freifeld, and M. Black. From pictorial structures to deformable structures. CVPR, pp. 3546-3553, 2012. 1, 2, 3\n", "annotations": {"author": "[{\"end\":67,\"start\":52},{\"end\":80,\"start\":68},{\"end\":94,\"start\":81},{\"end\":111,\"start\":95},{\"end\":128,\"start\":112},{\"end\":144,\"start\":129},{\"end\":157,\"start\":145},{\"end\":171,\"start\":158},{\"end\":188,\"start\":172},{\"end\":199,\"start\":189},{\"end\":252,\"start\":200},{\"end\":294,\"start\":253},{\"end\":330,\"start\":295},{\"end\":366,\"start\":331},{\"end\":420,\"start\":367}]", "publisher": null, "author_last_name": "[{\"end\":66,\"start\":60},{\"end\":79,\"start\":75},{\"end\":93,\"start\":88},{\"end\":110,\"start\":104},{\"end\":127,\"start\":122},{\"end\":143,\"start\":137},{\"end\":156,\"start\":152},{\"end\":170,\"start\":165},{\"end\":187,\"start\":181},{\"end\":214,\"start\":208},{\"end\":265,\"start\":261},{\"end\":307,\"start\":302},{\"end\":346,\"start\":340},{\"end\":382,\"start\":377}]", "author_first_name": "[{\"end\":59,\"start\":52},{\"end\":74,\"start\":68},{\"end\":87,\"start\":81},{\"end\":103,\"start\":95},{\"end\":119,\"start\":112},{\"end\":121,\"start\":120},{\"end\":136,\"start\":129},{\"end\":151,\"start\":145},{\"end\":164,\"start\":158},{\"end\":180,\"start\":172},{\"end\":196,\"start\":189},{\"end\":198,\"start\":197},{\"end\":207,\"start\":200},{\"end\":260,\"start\":253},{\"end\":301,\"start\":295},{\"end\":339,\"start\":331},{\"end\":374,\"start\":367},{\"end\":376,\"start\":375}]", "author_affiliation": "[{\"end\":251,\"start\":216},{\"end\":293,\"start\":267},{\"end\":329,\"start\":309},{\"end\":365,\"start\":348},{\"end\":419,\"start\":384}]", "title": "[{\"end\":41,\"start\":1},{\"end\":461,\"start\":421}]", "venue": "[{\"end\":517,\"start\":463}]", "abstract": "[{\"end\":2681,\"start\":1190}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2888,\"start\":2884},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3019,\"start\":3015},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3022,\"start\":3019},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3041,\"start\":3037},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3066,\"start\":3062},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4019,\"start\":4015},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4116,\"start\":4112},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4255,\"start\":4251},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4630,\"start\":4626},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5182,\"start\":5179},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5185,\"start\":5182},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5405,\"start\":5401},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5408,\"start\":5405},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5411,\"start\":5408},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5414,\"start\":5411},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5417,\"start\":5414},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5426,\"start\":5422},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5728,\"start\":5724},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5731,\"start\":5728},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5734,\"start\":5731},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5748,\"start\":5744},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6223,\"start\":6219},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6442,\"start\":6439},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6477,\"start\":6473},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6729,\"start\":6725},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6849,\"start\":6845},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7547,\"start\":7543},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7550,\"start\":7547},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7553,\"start\":7550},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7556,\"start\":7553},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7660,\"start\":7656},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7663,\"start\":7660},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7854,\"start\":7850},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7875,\"start\":7871},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7897,\"start\":7893},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7900,\"start\":7897},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8165,\"start\":8161},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8235,\"start\":8231},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8424,\"start\":8420},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8444,\"start\":8441},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8467,\"start\":8464},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8489,\"start\":8485},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8508,\"start\":8504},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8532,\"start\":8528},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8550,\"start\":8546},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8584,\"start\":8580},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8604,\"start\":8600},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8631,\"start\":8627},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8651,\"start\":8647},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8677,\"start\":8673},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8702,\"start\":8698},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8733,\"start\":8729},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8756,\"start\":8752},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8826,\"start\":8823},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8829,\"start\":8826},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8832,\"start\":8829},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9053,\"start\":9049},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9083,\"start\":9079},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9573,\"start\":9569},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10753,\"start\":10749},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11287,\"start\":11283},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11637,\"start\":11633},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11740,\"start\":11736},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12563,\"start\":12559},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13321,\"start\":13317},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13398,\"start\":13394},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13721,\"start\":13717},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14271,\"start\":14268},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14969,\"start\":14966},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15218,\"start\":15214},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15413,\"start\":15410},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":15811,\"start\":15807},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16616,\"start\":16613},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19433,\"start\":19429},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20461,\"start\":20457},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21405,\"start\":21402},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21599,\"start\":21595},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21698,\"start\":21695},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21914,\"start\":21910},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21958,\"start\":21954},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22332,\"start\":22331},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24027,\"start\":24023},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27344,\"start\":27341},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":29035,\"start\":29031},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":29111,\"start\":29108},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29531,\"start\":29528},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":30291,\"start\":30287},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33429,\"start\":33426},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":33433,\"start\":33430},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":33440,\"start\":33437},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":33445,\"start\":33441},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33453,\"start\":33449}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33199,\"start\":32545},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33294,\"start\":33200},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33423,\"start\":33295},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33689,\"start\":33424},{\"attributes\":{\"id\":\"fig_5\"},\"end\":33813,\"start\":33690},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33843,\"start\":33814},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":35029,\"start\":33844}]", "paragraph": "[{\"end\":3391,\"start\":2697},{\"end\":3867,\"start\":3393},{\"end\":4474,\"start\":3869},{\"end\":5122,\"start\":4476},{\"end\":6130,\"start\":5124},{\"end\":6550,\"start\":6156},{\"end\":7080,\"start\":6583},{\"end\":8269,\"start\":7082},{\"end\":9521,\"start\":8271},{\"end\":10693,\"start\":9549},{\"end\":11811,\"start\":10708},{\"end\":12153,\"start\":11813},{\"end\":12471,\"start\":12155},{\"end\":13220,\"start\":12511},{\"end\":13684,\"start\":13252},{\"end\":14436,\"start\":13700},{\"end\":14862,\"start\":14438},{\"end\":15525,\"start\":14928},{\"end\":16227,\"start\":15527},{\"end\":16371,\"start\":16229},{\"end\":16738,\"start\":16421},{\"end\":17275,\"start\":16763},{\"end\":17695,\"start\":17277},{\"end\":18983,\"start\":17697},{\"end\":19364,\"start\":18985},{\"end\":19617,\"start\":19366},{\"end\":20002,\"start\":19649},{\"end\":21356,\"start\":20031},{\"end\":21749,\"start\":21358},{\"end\":22222,\"start\":21768},{\"end\":22332,\"start\":22224},{\"end\":22464,\"start\":22354},{\"end\":23986,\"start\":22466},{\"end\":24762,\"start\":23988},{\"end\":25592,\"start\":24764},{\"end\":26054,\"start\":25594},{\"end\":26416,\"start\":26074},{\"end\":27283,\"start\":26418},{\"end\":28088,\"start\":27285},{\"end\":28255,\"start\":28090},{\"end\":28945,\"start\":28257},{\"end\":30117,\"start\":28947},{\"end\":31169,\"start\":30142},{\"end\":32383,\"start\":31171},{\"end\":32544,\"start\":32385}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6155,\"start\":6131},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14927,\"start\":14863},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16420,\"start\":16372}]", "table_ref": "[{\"end\":27119,\"start\":27112},{\"end\":29736,\"start\":29729},{\"end\":29797,\"start\":29790}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2695,\"start\":2683},{\"attributes\":{\"n\":\"2.\"},\"end\":6581,\"start\":6553},{\"attributes\":{\"n\":\"3.\"},\"end\":9535,\"start\":9524},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9547,\"start\":9538},{\"attributes\":{\"n\":\"3.2.\"},\"end\":10706,\"start\":10696},{\"attributes\":{\"n\":\"3.3.\"},\"end\":12509,\"start\":12474},{\"attributes\":{\"n\":\"4.\"},\"end\":13250,\"start\":13223},{\"attributes\":{\"n\":\"4.1.\"},\"end\":13698,\"start\":13687},{\"attributes\":{\"n\":\"4.2.\"},\"end\":16761,\"start\":16741},{\"attributes\":{\"n\":\"5.\"},\"end\":19647,\"start\":19620},{\"attributes\":{\"n\":\"5.1.\"},\"end\":20029,\"start\":20005},{\"attributes\":{\"n\":\"5.2.\"},\"end\":21766,\"start\":21752},{\"end\":22352,\"start\":22335},{\"attributes\":{\"n\":\"6.2.\"},\"end\":26072,\"start\":26057},{\"attributes\":{\"n\":\"6.3.\"},\"end\":30127,\"start\":30120},{\"attributes\":{\"n\":\"7.\"},\"end\":30140,\"start\":30130},{\"end\":32556,\"start\":32546},{\"end\":33306,\"start\":33296},{\"end\":33701,\"start\":33691},{\"end\":33824,\"start\":33815}]", "table": "[{\"end\":35029,\"start\":34337}]", "figure_caption": "[{\"end\":33199,\"start\":32558},{\"end\":33294,\"start\":33202},{\"end\":33423,\"start\":33308},{\"end\":33689,\"start\":33426},{\"end\":33813,\"start\":33703},{\"end\":33843,\"start\":33826},{\"end\":34337,\"start\":33846}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4410,\"start\":4398},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4573,\"start\":4567},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11778,\"start\":11772},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16733,\"start\":16727},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17513,\"start\":17501},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17653,\"start\":17646},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18008,\"start\":17998},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18103,\"start\":18097},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18390,\"start\":18380},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19511,\"start\":19501},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20456,\"start\":20450},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":21042,\"start\":21022},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24427,\"start\":24417},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":25042,\"start\":25034},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26776,\"start\":26766}]", "bib_author_first_name": "[{\"end\":35232,\"start\":35231},{\"end\":35243,\"start\":35242},{\"end\":35251,\"start\":35250},{\"end\":35259,\"start\":35258},{\"end\":35483,\"start\":35482},{\"end\":35494,\"start\":35493},{\"end\":35705,\"start\":35704},{\"end\":35717,\"start\":35716},{\"end\":35897,\"start\":35893},{\"end\":35909,\"start\":35905},{\"end\":36094,\"start\":36093},{\"end\":36103,\"start\":36102},{\"end\":36294,\"start\":36293},{\"end\":36303,\"start\":36302},{\"end\":36313,\"start\":36312},{\"end\":36529,\"start\":36528},{\"end\":36540,\"start\":36539},{\"end\":36548,\"start\":36547},{\"end\":36560,\"start\":36559},{\"end\":36771,\"start\":36770},{\"end\":36782,\"start\":36781},{\"end\":36967,\"start\":36966},{\"end\":37178,\"start\":37177},{\"end\":37189,\"start\":37188},{\"end\":37206,\"start\":37205},{\"end\":37405,\"start\":37404},{\"end\":37414,\"start\":37413},{\"end\":37431,\"start\":37430},{\"end\":37646,\"start\":37645},{\"end\":37657,\"start\":37656},{\"end\":37667,\"start\":37666},{\"end\":37676,\"start\":37675},{\"end\":37956,\"start\":37955},{\"end\":37967,\"start\":37966},{\"end\":38186,\"start\":38185},{\"end\":38196,\"start\":38195},{\"end\":38206,\"start\":38205},{\"end\":38217,\"start\":38216},{\"end\":38227,\"start\":38226},{\"end\":38477,\"start\":38476},{\"end\":38479,\"start\":38478},{\"end\":38491,\"start\":38490},{\"end\":38502,\"start\":38501},{\"end\":38513,\"start\":38512},{\"end\":38526,\"start\":38525},{\"end\":38537,\"start\":38536},{\"end\":38837,\"start\":38836},{\"end\":38847,\"start\":38846},{\"end\":38860,\"start\":38859},{\"end\":38870,\"start\":38869},{\"end\":39013,\"start\":39012},{\"end\":39026,\"start\":39025},{\"end\":39036,\"start\":39035},{\"end\":39262,\"start\":39261},{\"end\":39273,\"start\":39272},{\"end\":39281,\"start\":39280},{\"end\":39524,\"start\":39523},{\"end\":39532,\"start\":39531},{\"end\":39544,\"start\":39543},{\"end\":39555,\"start\":39554},{\"end\":39564,\"start\":39563},{\"end\":39802,\"start\":39801},{\"end\":39812,\"start\":39811},{\"end\":39814,\"start\":39813},{\"end\":40003,\"start\":40002},{\"end\":40012,\"start\":40011},{\"end\":40215,\"start\":40214},{\"end\":40227,\"start\":40226},{\"end\":40235,\"start\":40234},{\"end\":40248,\"start\":40247},{\"end\":40491,\"start\":40490},{\"end\":40501,\"start\":40500},{\"end\":40515,\"start\":40514},{\"end\":40731,\"start\":40730},{\"end\":40739,\"start\":40738},{\"end\":40748,\"start\":40747},{\"end\":40991,\"start\":40990},{\"end\":41000,\"start\":40999},{\"end\":41009,\"start\":41008},{\"end\":41301,\"start\":41300},{\"end\":41303,\"start\":41302},{\"end\":41312,\"start\":41311},{\"end\":41569,\"start\":41568},{\"end\":41576,\"start\":41575},{\"end\":41584,\"start\":41583},{\"end\":41886,\"start\":41885},{\"end\":41897,\"start\":41896},{\"end\":41909,\"start\":41908},{\"end\":42161,\"start\":42160},{\"end\":42169,\"start\":42168},{\"end\":42183,\"start\":42182},{\"end\":42393,\"start\":42392},{\"end\":42401,\"start\":42400},{\"end\":42411,\"start\":42410},{\"end\":42424,\"start\":42420},{\"end\":42696,\"start\":42695},{\"end\":42708,\"start\":42707},{\"end\":42719,\"start\":42718},{\"end\":42954,\"start\":42953},{\"end\":42964,\"start\":42963},{\"end\":43170,\"start\":43169},{\"end\":43178,\"start\":43177},{\"end\":43367,\"start\":43366},{\"end\":43374,\"start\":43373},{\"end\":43382,\"start\":43381},{\"end\":43553,\"start\":43552},{\"end\":43562,\"start\":43561},{\"end\":43740,\"start\":43739},{\"end\":43749,\"start\":43748},{\"end\":43761,\"start\":43760}]", "bib_author_last_name": "[{\"end\":35240,\"start\":35233},{\"end\":35248,\"start\":35244},{\"end\":35256,\"start\":35252},{\"end\":35265,\"start\":35260},{\"end\":35491,\"start\":35484},{\"end\":35500,\"start\":35495},{\"end\":35714,\"start\":35706},{\"end\":35724,\"start\":35718},{\"end\":35903,\"start\":35898},{\"end\":35913,\"start\":35910},{\"end\":36100,\"start\":36095},{\"end\":36110,\"start\":36104},{\"end\":36300,\"start\":36295},{\"end\":36310,\"start\":36304},{\"end\":36320,\"start\":36314},{\"end\":36537,\"start\":36530},{\"end\":36545,\"start\":36541},{\"end\":36557,\"start\":36549},{\"end\":36569,\"start\":36561},{\"end\":36779,\"start\":36772},{\"end\":36790,\"start\":36783},{\"end\":36977,\"start\":36968},{\"end\":37186,\"start\":37179},{\"end\":37203,\"start\":37190},{\"end\":37216,\"start\":37207},{\"end\":37411,\"start\":37406},{\"end\":37428,\"start\":37415},{\"end\":37435,\"start\":37432},{\"end\":37654,\"start\":37647},{\"end\":37664,\"start\":37658},{\"end\":37673,\"start\":37668},{\"end\":37689,\"start\":37677},{\"end\":37964,\"start\":37957},{\"end\":37978,\"start\":37968},{\"end\":38193,\"start\":38187},{\"end\":38203,\"start\":38197},{\"end\":38214,\"start\":38207},{\"end\":38224,\"start\":38218},{\"end\":38233,\"start\":38228},{\"end\":38488,\"start\":38480},{\"end\":38499,\"start\":38492},{\"end\":38510,\"start\":38503},{\"end\":38523,\"start\":38514},{\"end\":38534,\"start\":38527},{\"end\":38543,\"start\":38538},{\"end\":38844,\"start\":38838},{\"end\":38857,\"start\":38848},{\"end\":38867,\"start\":38861},{\"end\":38880,\"start\":38871},{\"end\":39023,\"start\":39014},{\"end\":39033,\"start\":39027},{\"end\":39043,\"start\":39037},{\"end\":39270,\"start\":39263},{\"end\":39278,\"start\":39274},{\"end\":39289,\"start\":39282},{\"end\":39529,\"start\":39525},{\"end\":39541,\"start\":39533},{\"end\":39552,\"start\":39545},{\"end\":39561,\"start\":39556},{\"end\":39571,\"start\":39565},{\"end\":39809,\"start\":39803},{\"end\":39822,\"start\":39815},{\"end\":40009,\"start\":40004},{\"end\":40017,\"start\":40013},{\"end\":40224,\"start\":40216},{\"end\":40232,\"start\":40228},{\"end\":40245,\"start\":40236},{\"end\":40256,\"start\":40249},{\"end\":40498,\"start\":40492},{\"end\":40512,\"start\":40502},{\"end\":40521,\"start\":40516},{\"end\":40736,\"start\":40732},{\"end\":40745,\"start\":40740},{\"end\":40755,\"start\":40749},{\"end\":40997,\"start\":40992},{\"end\":41006,\"start\":41001},{\"end\":41015,\"start\":41010},{\"end\":41309,\"start\":41304},{\"end\":41320,\"start\":41313},{\"end\":41573,\"start\":41570},{\"end\":41581,\"start\":41577},{\"end\":41590,\"start\":41585},{\"end\":41894,\"start\":41887},{\"end\":41906,\"start\":41898},{\"end\":41915,\"start\":41910},{\"end\":42166,\"start\":42162},{\"end\":42180,\"start\":42170},{\"end\":42188,\"start\":42184},{\"end\":42398,\"start\":42394},{\"end\":42408,\"start\":42402},{\"end\":42418,\"start\":42412},{\"end\":42428,\"start\":42425},{\"end\":42705,\"start\":42697},{\"end\":42716,\"start\":42709},{\"end\":42725,\"start\":42720},{\"end\":42961,\"start\":42955},{\"end\":42970,\"start\":42965},{\"end\":43175,\"start\":43171},{\"end\":43186,\"start\":43179},{\"end\":43371,\"start\":43368},{\"end\":43379,\"start\":43375},{\"end\":43391,\"start\":43383},{\"end\":43559,\"start\":43554},{\"end\":43568,\"start\":43563},{\"end\":43746,\"start\":43741},{\"end\":43758,\"start\":43750},{\"end\":43767,\"start\":43762}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":35403,\"start\":35161},{\"attributes\":{\"id\":\"b1\"},\"end\":35632,\"start\":35405},{\"attributes\":{\"id\":\"b2\"},\"end\":35844,\"start\":35634},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":961425},\"end\":36031,\"start\":35846},{\"attributes\":{\"id\":\"b4\"},\"end\":36219,\"start\":36033},{\"attributes\":{\"id\":\"b5\"},\"end\":36453,\"start\":36221},{\"attributes\":{\"id\":\"b6\"},\"end\":36717,\"start\":36455},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":2437110},\"end\":36905,\"start\":36719},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":15601477},\"end\":37107,\"start\":36907},{\"attributes\":{\"id\":\"b9\"},\"end\":37358,\"start\":37109},{\"attributes\":{\"id\":\"b10\"},\"end\":37541,\"start\":37360},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":4244548},\"end\":37873,\"start\":37543},{\"attributes\":{\"id\":\"b12\"},\"end\":38118,\"start\":37875},{\"attributes\":{\"id\":\"b13\"},\"end\":38393,\"start\":38120},{\"attributes\":{\"doi\":\"CMU-RI-TR-08-22\",\"id\":\"b14\"},\"end\":38782,\"start\":38395},{\"attributes\":{\"id\":\"b15\"},\"end\":39010,\"start\":38784},{\"attributes\":{\"id\":\"b16\"},\"end\":39164,\"start\":39012},{\"attributes\":{\"id\":\"b17\"},\"end\":39450,\"start\":39166},{\"attributes\":{\"id\":\"b18\"},\"end\":39728,\"start\":39452},{\"attributes\":{\"id\":\"b19\"},\"end\":39946,\"start\":39730},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":10875786},\"end\":40136,\"start\":39948},{\"attributes\":{\"id\":\"b21\"},\"end\":40418,\"start\":40138},{\"attributes\":{\"id\":\"b22\"},\"end\":40676,\"start\":40420},{\"attributes\":{\"id\":\"b23\"},\"end\":40865,\"start\":40678},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":11279201},\"end\":41212,\"start\":40867},{\"attributes\":{\"id\":\"b25\"},\"end\":41462,\"start\":41214},{\"attributes\":{\"id\":\"b26\"},\"end\":41770,\"start\":41464},{\"attributes\":{\"id\":\"b27\"},\"end\":42098,\"start\":41772},{\"attributes\":{\"id\":\"b28\"},\"end\":42315,\"start\":42100},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":5401052},\"end\":42604,\"start\":42317},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":3467933},\"end\":42897,\"start\":42606},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":438781},\"end\":43089,\"start\":42899},{\"attributes\":{\"id\":\"b32\"},\"end\":43296,\"start\":43091},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":13609995},\"end\":43537,\"start\":43298},{\"attributes\":{\"doi\":\"TR- IS-MPI-007\",\"id\":\"b34\"},\"end\":43737,\"start\":43539},{\"attributes\":{\"id\":\"b35\"},\"end\":43952,\"start\":43739}]", "bib_title": "[{\"end\":35891,\"start\":35846},{\"end\":36768,\"start\":36719},{\"end\":36964,\"start\":36907},{\"end\":37643,\"start\":37543},{\"end\":40000,\"start\":39948},{\"end\":40988,\"start\":40867},{\"end\":42390,\"start\":42317},{\"end\":42693,\"start\":42606},{\"end\":42951,\"start\":42899},{\"end\":43364,\"start\":43298}]", "bib_author": "[{\"end\":35242,\"start\":35231},{\"end\":35250,\"start\":35242},{\"end\":35258,\"start\":35250},{\"end\":35267,\"start\":35258},{\"end\":35493,\"start\":35482},{\"end\":35502,\"start\":35493},{\"end\":35716,\"start\":35704},{\"end\":35726,\"start\":35716},{\"end\":35905,\"start\":35893},{\"end\":35915,\"start\":35905},{\"end\":36102,\"start\":36093},{\"end\":36112,\"start\":36102},{\"end\":36302,\"start\":36293},{\"end\":36312,\"start\":36302},{\"end\":36322,\"start\":36312},{\"end\":36539,\"start\":36528},{\"end\":36547,\"start\":36539},{\"end\":36559,\"start\":36547},{\"end\":36571,\"start\":36559},{\"end\":36781,\"start\":36770},{\"end\":36792,\"start\":36781},{\"end\":36979,\"start\":36966},{\"end\":37188,\"start\":37177},{\"end\":37205,\"start\":37188},{\"end\":37218,\"start\":37205},{\"end\":37413,\"start\":37404},{\"end\":37430,\"start\":37413},{\"end\":37437,\"start\":37430},{\"end\":37656,\"start\":37645},{\"end\":37666,\"start\":37656},{\"end\":37675,\"start\":37666},{\"end\":37691,\"start\":37675},{\"end\":37966,\"start\":37955},{\"end\":37980,\"start\":37966},{\"end\":38195,\"start\":38185},{\"end\":38205,\"start\":38195},{\"end\":38216,\"start\":38205},{\"end\":38226,\"start\":38216},{\"end\":38235,\"start\":38226},{\"end\":38490,\"start\":38476},{\"end\":38501,\"start\":38490},{\"end\":38512,\"start\":38501},{\"end\":38525,\"start\":38512},{\"end\":38536,\"start\":38525},{\"end\":38545,\"start\":38536},{\"end\":38846,\"start\":38836},{\"end\":38859,\"start\":38846},{\"end\":38869,\"start\":38859},{\"end\":38882,\"start\":38869},{\"end\":39025,\"start\":39012},{\"end\":39035,\"start\":39025},{\"end\":39045,\"start\":39035},{\"end\":39272,\"start\":39261},{\"end\":39280,\"start\":39272},{\"end\":39291,\"start\":39280},{\"end\":39531,\"start\":39523},{\"end\":39543,\"start\":39531},{\"end\":39554,\"start\":39543},{\"end\":39563,\"start\":39554},{\"end\":39573,\"start\":39563},{\"end\":39811,\"start\":39801},{\"end\":39824,\"start\":39811},{\"end\":40011,\"start\":40002},{\"end\":40019,\"start\":40011},{\"end\":40226,\"start\":40214},{\"end\":40234,\"start\":40226},{\"end\":40247,\"start\":40234},{\"end\":40258,\"start\":40247},{\"end\":40500,\"start\":40490},{\"end\":40514,\"start\":40500},{\"end\":40523,\"start\":40514},{\"end\":40738,\"start\":40730},{\"end\":40747,\"start\":40738},{\"end\":40757,\"start\":40747},{\"end\":40999,\"start\":40990},{\"end\":41008,\"start\":40999},{\"end\":41017,\"start\":41008},{\"end\":41311,\"start\":41300},{\"end\":41322,\"start\":41311},{\"end\":41575,\"start\":41568},{\"end\":41583,\"start\":41575},{\"end\":41592,\"start\":41583},{\"end\":41896,\"start\":41885},{\"end\":41908,\"start\":41896},{\"end\":41917,\"start\":41908},{\"end\":42168,\"start\":42160},{\"end\":42182,\"start\":42168},{\"end\":42190,\"start\":42182},{\"end\":42400,\"start\":42392},{\"end\":42410,\"start\":42400},{\"end\":42420,\"start\":42410},{\"end\":42430,\"start\":42420},{\"end\":42707,\"start\":42695},{\"end\":42718,\"start\":42707},{\"end\":42727,\"start\":42718},{\"end\":42963,\"start\":42953},{\"end\":42972,\"start\":42963},{\"end\":43177,\"start\":43169},{\"end\":43188,\"start\":43177},{\"end\":43373,\"start\":43366},{\"end\":43381,\"start\":43373},{\"end\":43393,\"start\":43381},{\"end\":43561,\"start\":43552},{\"end\":43570,\"start\":43561},{\"end\":43748,\"start\":43739},{\"end\":43760,\"start\":43748},{\"end\":43769,\"start\":43760}]", "bib_venue": "[{\"end\":35229,\"start\":35161},{\"end\":35480,\"start\":35405},{\"end\":35702,\"start\":35634},{\"end\":35923,\"start\":35915},{\"end\":36091,\"start\":36033},{\"end\":36291,\"start\":36221},{\"end\":36526,\"start\":36455},{\"end\":36796,\"start\":36792},{\"end\":36993,\"start\":36979},{\"end\":37175,\"start\":37109},{\"end\":37402,\"start\":37360},{\"end\":37695,\"start\":37691},{\"end\":37953,\"start\":37875},{\"end\":38183,\"start\":38120},{\"end\":38474,\"start\":38395},{\"end\":38834,\"start\":38784},{\"end\":39069,\"start\":39045},{\"end\":39259,\"start\":39166},{\"end\":39521,\"start\":39452},{\"end\":39799,\"start\":39730},{\"end\":40022,\"start\":40019},{\"end\":40212,\"start\":40138},{\"end\":40488,\"start\":40420},{\"end\":40728,\"start\":40678},{\"end\":41021,\"start\":41017},{\"end\":41298,\"start\":41214},{\"end\":41566,\"start\":41464},{\"end\":41883,\"start\":41772},{\"end\":42158,\"start\":42100},{\"end\":42434,\"start\":42430},{\"end\":42731,\"start\":42727},{\"end\":42976,\"start\":42972},{\"end\":43167,\"start\":43091},{\"end\":43397,\"start\":43393},{\"end\":43550,\"start\":43539},{\"end\":43825,\"start\":43769}]"}}}, "year": 2023, "month": 12, "day": 17}