{"id": 254685949, "updated": "2023-10-05 06:42:31.799", "metadata": {"title": "QueryPose: Sparse Multi-Person Pose Regression via Spatial-Aware Part-Level Query", "authors": "[{\"first\":\"Yabo\",\"last\":\"Xiao\",\"middle\":[]},{\"first\":\"Kai\",\"last\":\"Su\",\"middle\":[]},{\"first\":\"Xiaojuan\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Dongdong\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Lei\",\"last\":\"Jin\",\"middle\":[]},{\"first\":\"Mingshu\",\"last\":\"He\",\"middle\":[]},{\"first\":\"Zehuan\",\"last\":\"Yuan\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "We propose a sparse end-to-end multi-person pose regression framework, termed QueryPose, which can directly predict multi-person keypoint sequences from the input image. The existing end-to-end methods rely on dense representations to preserve the spatial detail and structure for precise keypoint localization. However, the dense paradigm introduces complex and redundant post-processes during inference. In our framework, each human instance is encoded by several learnable spatial-aware part-level queries associated with an instance-level query. First, we propose the Spatial Part Embedding Generation Module (SPEGM) that considers the local spatial attention mechanism to generate several spatial-sensitive part embeddings, which contain spatial details and structural information for enhancing the part-level queries. Second, we introduce the Selective Iteration Module (SIM) to adaptively update the sparse part-level queries via the generated spatial-sensitive part embeddings stage-by-stage. Based on the two proposed modules, the part-level queries are able to fully encode the spatial details and structural information for precise keypoint regression. With the bipartite matching, QueryPose avoids the hand-designed post-processes and surpasses the existing dense end-to-end methods with 73.6 AP on MS COCO mini-val set and 72.7 AP on CrowdPose test set. Code is available at https://github.com/buptxyb666/QueryPose.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2212.07855", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/XiaoSWYJHY22", "doi": "10.48550/arxiv.2212.07855"}}, "content": {"source": {"pdf_hash": "27946d98a88348a3fd7175c15dfeb003335cff1f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2212.07855v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ed70db92b6386f3ee8e8ea757e1d21c521da4a4e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/27946d98a88348a3fd7175c15dfeb003335cff1f.txt", "contents": "\nQueryPose: Sparse Multi-Person Pose Regression via Spatial-Aware Part-Level Query\n\n\nYabo Xiao xiaoyabo@bupt.edu.cn \nKai Su sukai@bytedance.com \nXioajuan Wang \nDongdong Yu yudongdong@oppo.com \nLei Jin jinlei@bupt.edu.cn \nZehuan Yuan yuanzehuan@bytedance.com \n\nBUPT\nBeijingChina\n\n\nByteDance Inc. Shanghai\nChina\n\n\nBUPT\nBeijingChina\n\n\nOPPO Research Institute\nBeijingChina\n\n\nMingshu He BUPT\nBUPT\nBeijing, BeijingChina, China\n\n\nByteDance Inc\nBeijingChina\n\nQueryPose: Sparse Multi-Person Pose Regression via Spatial-Aware Part-Level Query\n\nWe propose a sparse end-to-end multi-person pose regression framework, termed QueryPose, which can directly predict multi-person keypoint sequences from the input image. The existing end-to-end methods rely on dense representations to preserve the spatial detail and structure for precise keypoint localization. However, the dense paradigm introduces complex and redundant post-processes during inference. In our framework, each human instance is encoded by several learnable spatial-aware part-level queries associated with an instance-level query. First, we propose the Spatial Part Embedding Generation Module (SPEGM) that considers the local spatial attention mechanism to generate several spatial-sensitive part embeddings, which contain spatial details and structural information for enhancing the part-level queries. Second, we introduce the Selective Iteration Module (SIM) to adaptively update the sparse part-level queries via the generated spatial-sensitive part embeddings stage-by-stage. Based on the two proposed modules, the part-level queries are able to fully encode the spatial details and structural information for precise keypoint regression. With the bipartite matching, QueryPose avoids the hand-designed post-processes and surpasses the existing dense end-to-end methods with 73.6 AP on MS COCO mini-val set and 72.7 AP on CrowdPose test set. Code is available at https://github.com/buptxyb666/QueryPose. * Corresponding author. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2212.07855v1 [cs.CV] 15 Dec 2022 keypoint NMS Grouping Center NMS keypoint heatmap center heatmap per-pixel offset multi-person keypoints multi-person keypoints multi-person keypoints instance-level query spatial-aware part-level queries Dense End-to-End MPPE Sparse End-to-End MPPE (a) bottom-up (b) pixel-wise regression (c) QueryPose\n\nIntroduction\n\nMulti-person pose estimation (MPPE) aims to locate all person keypoints from the input image, which is a fundamental yet challenging task in computer vision. With the prevalence of deep learning techniques He et al. (2016) ;Newell, Yang, and Deng (2016); Deng et al. (2009), MPPE has achieved remarkable progress and played an important role in many other vision tasks, such as activity recognition Li et al. (2019b); Shi et al. (2019); Yan, Xiong, and Lin (2018); Duan et al. (2021) and pose tracking Xiao, Wu, and Wei (2018); Yu et al. (2018).\n\nIn general, the existing multi-person pose estimation solutions can be summarized as top-down and bottom-up paradigms. The top-down strategy Chen et al. (2018); Fang et al. (2017); Su et al. (2019); Sun et al. (2019); Li et al. (2021b,c) uses the human detector to predict person boxes and then leverages the single-person pose estimation model to localize the keypoints on each cropped human image. The two independent models lead to the non-end-to-end pipeline, or called two-stage pipeline. Moreover, the human detector involves extra memory as well as computational cost. The bottom-up strategy Cheng et al. (2020); Papandreou et al. (2017Papandreou et al. ( , 2018; Newell, Huang, and Deng (2017) uses the keypoint heatmap to locate all person keypoints at first and then assigns them to individuals via heuristic grouping process, as shown in Figure 1(a). Some researches attempt to leverage densely pixel-wise regression Xiao et al. (2022a); Zhou, Wang, and Kr\u00e4henb\u00fchl (2019); Nie et al. (2019); Tian, Chen, and Shen (2019); Geng et al. (2021), as shown in Figure 1(b), which predicts the center heatmap and pixel-wise keypoint offset in parallel. It can be regarded as a special case of the bottom-up paradigm. Both of them can achieve end-to-end optimization thus are more efficient than the two-stage pipeline. However, in order to maintain the local details and spatial information, ones leverage the dense representations (e.g., keypoint or center heatmap). The dense manner requires the hand-crafted post-processes to suppress duplicate predictions or perform keypoint grouping during inference. The post-processes are non-differentiable and always involve many hand-designed parameters to tune. In this paper, we aim to build a sparse end-to-end MPPE solution to eliminate the complex and redundant post-processes.\n\nRecently, query-based paradigm Zhu et al. (2020); Carion et al. (2020); Sun et al. (2021); Cheng, Schwing, and Kirillov (2021); Zhang et al. (2021) has attracted much attention due to its purely sparse and end-to-end differentiable training/inference pipeline. The variants Zhang et al. (2021); Cheng et al. (2022); Cheng, Schwing, and Kirillov (2021) are employed for several computer vision tasks and achieve promising performance. The success inspires us to construct a sparse end-to-end MPPE method via the learnable query. Nevertheless, how to effectively apply the query-based paradigm to the MPPE task is rarely explored. Intuitively, keypoints are similar to the corner points of the bounding box, and the instance-level object query can be used to regress the keypoint coordinates. However, we observe that it only achieves inferior performance. We argue that the instance-level query loses the local details and destroys the spatial structure, which are both critical for pose estimation task.\n\nTowards the aforementioned issue, we introduce the learnable part-level queries to learn spatialaware features and further construct a sparse end-to-end multi-person pose regression framework, termed QueryPose. First, we divide the human region into several local parts and present the Spatial Part Embedding Generation Module (SPEGM), which uses the local spatial attention mechanism to generate the spatial-sensitive part embeddings for enhancing part-level queries. Second, we introduce the Selective Iteration Module (SIM) to adaptively update the part-level queries via the generated spatial-sensitive part embeddings stage-by-stage. The local details and structural features in the learnable part-level queries will be enhanced, and the noise will be filtered. Based on the bipartite matching, we eliminate all hand-crafted post-processes and directly output the multi-person keypoint coordinate sequences for the input image. Without bells and whistles, our sparse framework outperforms all dense end-to-end methods on MS COCO Lin et al. (2014) and CrowdPose Li et al. (2019a).\n\nThe main contributions can be summarized into three aspects as follows:\n\n\u2022 We propose to leverage the sparse learnable spatial-aware part queries to encode local spatial features. Specifically, we present the Spatial Part Embedding Generation Module (SPEGM) to generate the spatial-sensitive part embeddings via the local spatial attention mechanism, which preserves the local spatial features for enhancing the part-level queries.\n\n\u2022 We further introduce the Selective Iteration Module (SIM) to adaptively update the learnable part-level queries by using the newly generated spatial-sensitive part embedding stage-bystage. Accordingly, the encoded local details and spatial structure are strengthened, and the distractive information is ignored.\n\n\u2022 With two proposed modules and one-to-one bipartite matching, we construct a purely sparse query-based MPPE framework, termed QueryPose, which removes the complex postprocesses and directly outputs the multi-person keypoint coordinates. QueryPose achieves state-of-the-art performance and surpasses the most existing end-to-end MPPE methods on both MS COCO and CrowdPose.\n\n\nRelated Work\n\nIn this section, we review three relevant parts to our method including top-down paradigm, bottom-up paradigm and query-based paradigm.\n\nTop-down paradigm.  2021)). However, rare research employs the query-based paradigm for MPPE in end-to-end manner. PRTR Li et al. (2021b) trivially uses the keypoint query with multi-head self-attention to probe the image feature, while losing the spatial local structural information, resulting in inferior performance. In our framework, we propose to leverage several spatial-aware part-level queries to learn local details and spatial structures.\n\n\nMethod\n\n\nOverall Architecture\n\nThe overall framework is shown in Figure 2. The network is divided into three parts, including backbone, box decoder and keypoint decoder. The keypoint decoder consists of the proposed Spatial  Backbone. Given an input image I , we extract the multiple level features P 2 \u223c P 5 via the feature pyramid network built upon the backbone, where P l indicates the feature with 1 / 2 l resolution of input and 256 channels. We feed multi-level features P 2 \u223c P 5 to box decoder and only high-resolution P 2 to keypoint decoder.\n\nBox decoder. The design follows Sparse R-CNN Sun et al. (2021), we adopt a small set of learnable proposal boxes B \u2208 R N \u00d74 as dynamic anchors to provide the prior information of object position. Each proposal box corresponds to a learnable instance-level query. Concretely, we utilize the 7\u00d77 RoIAlign operation to extract features ROI box according to the learnable boxes B. The extract features are one-to-one correspondence with learnable instance-level queries Q I \u2208 R N \u00d7d . After exploring the self-attention across Q I by multi-head self-attention (MHSA) layer, we aggregate the extracted feature ROI box into instance-level query via the dynamic channel MLP (DyMLP channel in Eq. 1), and feed the enhanced instance-level query into regression head ( Head box in Eq. 1) to predict the offset for refining the input proposal box. The above process is summarized as follows:\nROI s box = RoIAlign(P 2 \u223c P 5 , B s\u22121 ), Q s I = MHSA(Q s\u22121 I ), Q s I = DyMLP channel (Q s I , ROI s box ), B s = \u2206(Head box (Q s I ), B s\u22121 ).\n(1) Keypoint decoder. After the above process, the instance-level query Q I has encoded the information of the human instance, thus can be used to regress the keypoints. However, it only obtains unsatisfactory performance. We argue that the keypoint localization requires more inherent spatial structure and local details than box regression, while Q I loses both spatial structure and local details.\n\nIn the light of this, we present to utilize several learnable spatial-aware part-level queries Q P to encode the human pose with spatial structure and local details maintained. Q P = {Q Pn |Q Pn \u2208 R M \u00d7dp } N n=1 , where n refers to n-th instance and M is the number of part-level query for each instance. First, by using the refined boxes, we perform the 14\u00d714 RoIAlign operation only on high-resolution feature P 2 to extract the more detailed features ROI pose , which are more suitable for pose estimation. Second, the proposed Spatial Part Embedding Generation Module (SPEGM) is applied for producing the spatial-sensitive part embeddings E P = {E Pn |E Pn \u2208 R M \u00d7dp } N n=1 via the local spatial attention mechanism. Each spatial-sensitive part embedding focuses on the specific local part. Thus the local spatial details and structural information can be preserved. Third, we utilize the Selective Iteration Module (SIM) to adaptively update the learnable spatial-aware part-level queries via the spatial-sensitive part embeddings generated in the current stage. Consequently, the local details and spatial structure information of part-level queries are enhanced, and the noise is filtered. Then, the spatial-aware part queries are sent into the multi-head self-attention (denoted as MHSA) layer to explore the spatial relationships across the different local parts. Accordingly, the global structure of each person instance is also encoded into the part-level queries. Finally, we leverage the non-shared linear layers (denoted as Linear in Eq. 2) to regress the keypoint coordinates respectively. The computing procedure of keypoint decoder can be formulated as follows:\nROI s pose = RoIAlign(P 2 , B s ), E s P = SPEGM(ROI s pose ), Q s P = SIM(Q s\u22121 P , E s P ), Q s P = MHSA(Q s P ), P ose s = Linear(Q s P ).\n(2)\n\nAs shown in Figure 3(b), the instance-level query Q I is iterated across the box decoder and keypoint decoder serially, and the part-level queries Q p are iterated across keypoint decoder. The box decoder of next stage adopts the predicted box of current stage as proposal box. The keypoint coordinates are predicted independently for each stage.\n\n\nSpatial Part Embedding Generation Module\n\nConsidering that the instance-level query Q I loses the local details and spatial structure information compared with dense representations, we propose the Spatial Part Embedding Generation Module to generate the spatial-sensitive part embeddings E p , which are able to maintain more spatial local features conducive to keypoint localization. The spatial-sensitive part embeddings E p are used to enhance the learnable spatial-aware part-level queries.\n\nGiven the single ROI feature ROI pose \u2208 R d\u00d7H\u00d7W as example, We firstly use the stacked 3\u00d73 convolutional layer and a deconvolutional layer to recover the higher resolution representation ROI pose \u2208 R d\u00d72H\u00d72W . Afterward, we take the spatial attention into consideration on the extracted ROI features ROI pose . In particular, we utilize a 3\u00d73 convolutional layer to squeeze the features along the channel dimension and produce the M -channels attention map A \u2208 R M \u00d72H\u00d72W . Each channel corresponds to a spatial attention map for specific local parts, M refers to the number of local parts. We flatten the attention map A along the spatial dimension and normalize the sum of attention weight to 1.0 via the softmax function. Then, by manipulating the spatial attention maps of the specific local parts, we assemble the spatial feature to generate M spatial-sensitive part embeddings. The process is formulated as:\nE P = Linear(A \u2297 ROI T pose ) \u2208 R M \u00d7dp , where \u2297 indicates dot product.\nThe spatial-sensitive part embeddings focus on the different local parts, as shown in Figure  5. Thus the local details and structural information are sufficiently preserved compared with only a single instance-level query.\n\n\nSelective Iteration Module\n\nFor enabling the learnable part-level queries Q P to learn the spatial-aware features, we propose the Spatial Part Embedding Generation Module (SPEGM) to generate several spatial-sensitive part embeddings E p . However, we consider that due to the inaccurate bounding boxes and inconsistent optimization goals in the early stage, the spatial-sensitive part embeddings contain disturbance information. Motivated by this, we present the Selective Iteration Module to adaptively update and refine the part-level queries Q P via the spatial-sensitive part embeddings E p .\n\nFirst, we directly sum the learnable part-level queries Q s\u22121 P \u2208 R M \u00d7dp output from previous stage and the spatial-sensitive part embeddings E s P \u2208 R M \u00d7dp generated in current stage. Second, we utilize a Multi-Layer Perceptron together with sigmoid operation to output two weight vectors W s E P and W s\u22121 Q P , which serve as two gates to control the contributions of the E s P and Q s\u22121 P . Finally, We leverage the weighted summation to fuse the E s P and Q s\u22121 P via W s E P and W s\u22121 Q P . The above process is formulated as: \nW s E P , W s\u22121 Q P = Sigmoid(MLP(E s P + Q s\u22121 P )), Q s P = W s E P * E s P + W s\u22121 Q P * Q s\u22121 P .(3)\ufffd \ufffd , \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd\u22122 \ufffd \ufffd\u22121 \ufffd \ufffd , \ufffd \ufffd \ufffd \ufffd \ufffd\u22122 , \ufffd \ufffd \ufffd\u22122 \ufffd \ufffd\u22122 (a)(b)\nBox Decoder s Figure 3: The iteration pipeline of the instance-level query Q I and part-level queries Q p . (a) The instance-level query Q I is only iterated across the box decoder. (b) The instance-level query Q I is iterated across the box decoder and keypoint decoder serially.\n\nBy using the Selective Iteration Module, the informative local details and spatial structure in part-level queries Q P will be refined, and the noise will be ignored.\n\n\nTraining and Testing Details\n\nTraining. In our framework, we leverage the instance-level query Q I to conduct the binary classification and bounding box regression, then use several spatial-aware part-level queries Q p to regress corresponding keypoints. The ground truth of each human instance contains the class label, box corners and keypoints coordinates, which are denoted as C , {(x box 1 ,y box 1 ), (x box 2 ,y box 2 )}, and {(x kp k ,y kp k )} K k=1 respectively. Following previous methods Carion et al. (2020); Sun et al. (2021), we adopt bipartite matching to perform one-to-one label assignment. The set-based loss function for instance-wise query is formulated as: L Q I = \u03bb cls * L cls + \u03bb L1 * L L1 + \u03bb giou * L giou , where L cls refers to focal loss to perform binary classfication, L L1 and L giou denote the L1 loss and generalized IoU loss between the predicted box and the corresponding label. the loss weight \u03bb cls , \u03bb L1 , \u03bb giou are set to 2.0, 5.0 and 2.0 respectively. For keypoint regression, we leverage normalizing flow to capture the latent keypoint distribution P \u03b8,\u03c6 (x|ROI pose ), and model the keypoint regression loss from the perspective of maximum likelihood estimation following RLE Li et al. (2021a). The density distribution P \u03b8,\u03c6 (x|ROI pose ) reflects the probability of the keypoint annotation at the position x in ROI pose , where \u03b8 and \u03c6 are the trainable parameters of our regression network and flow model respectively. For easier optimization, the flow model F \u03c6 is used to map the simple distribution to deformed one P \u03c6 (x). Our regression network outputs two values\u03bc and\u03c3 for shifting and rescaling the distribution P \u03c6 (x), which is formulated as x =x * \u03c3 +\u03bc. The process enables the network to learn how the output deviates from annotations.\n\nThe different spatial-aware part queries are sent into the non-shared fully-connected layers to regres\u015d \u00b5 \u2208 R K\u00d72 and\u03c3 \u2208 R K\u00d72 , where\u03bc is the corresponding prediction of \u00b5 g = {(x kp k ,y kp k )} K k=1 . Based on the matched pairs, the set-based keypoint regression loss for the spatial-aware part-level queries is formulated as:\nL Q P = \u2212log P \u03b8,\u03c6 (x|ROI pose )| x=\u00b5g .(4)\nIt is noteworthy that \u00b5 g is normalized by the height and width of the predicted bounding box and shifted to (-0.5, 0.5) for stabilizing the training process. The keypoint out of the predicted bounding box will be masked. The total loss of our framework can be written as: L total = L Q I + L Q P .\n\nTesting.\u03c3 can be regarded as the uncertainty score of keypoint localization, thus we define the final pose score as 1 K K k=1 (1 \u2212\u03c3 k ) * C, whereC is the predicted instance score. During inference, our  \n\n\nExperiments and Analysis\n\nIn this section, we briefly illustrate the dataset, evaluation metric as well as implementation details in subsection 4.1. Next, we delve into the proposed modules and conduct the comprehensive ablation experiments to verify the effectiveness in subsection 4.2. Finally, we compare QueryPose with the previous methods on both MS COCO and CrowdPose in subsection 4.3.\n\n\nExperimental Setup\n\nDataset. MS COCO Lin et al. (2014) is a large-scale 2D human pose benchmark, which is split into train2017, mini-val, test-dev2017 sets. We train our model on COCO train2017 set with 57k images, conduct the evaluation on the mini-val set with 5k images and test-dev2017 set with 20K images respectively. CrowdPose Li et al. (2019a) contains 20,000 images with 80,000 annotated human instances. The train, validation and test set are partitioned in proportional to 5:1:4.\n\nEvaluation Metric. We leverage average precision and average recall based on different Object Keypoint Similarity (OKS) Lin et al. (2014) thresholds to evaluate performance. For COCO dataset, AP M and AP L indicate AP over medium and large-sized instances respectively. For CrowdPose, AP E , AP M , AP H refer to AP scores over easy, medium and hard instances according to dataset annotations.\n\nImplementation Details. During training, we use random horizontal flip and random crop with probability 0.5 to augment the training samples. The input images are randomly resized, and the short side is randomly sampled from 480 to 800 pixels with the aspect ratio kept. The number of the sparse instance-level query is set to 100 by default, it can even be reduced to 50 without significant performance degradation due to the foreground only containing the person. The model is trained by using AdamW optimizer with a mini-batch size of 16 (2 per GPU) on eight Tesla A100 GPUs. The initial learning rate is linearly scaled to 2.5e-5 and dropped 10\u00d7 after 220k, 260k iterations, and terminated at 280k iterations (2\u00d7 training schedule). The learning rate of the flow model is 10\u00d7 of the regression model. All codes are implemented based on Detectron2 Wu et al. (2019). During inference, we keep the aspect ratio and resize the short side of the images to 800 pixels.\n\n\nAblation Study\n\nWe conduct the ablative studies on MS COCO mini-val set to explore the contributions of the proposed components and delve into the superiority of its design. All experiments adopt ResNet-50-FPN and 1\u00d7 training scheme. We first define the baseline as employing the stacked 3\u00d73 conv-relu + global average pooling on ROI pose to directly regress keypoint coordinates. Analysis of the part-level query. As shown in Table 1, to verify the superiority of the part-level query over the instance-level query, we leverage four different interaction methods with image feature ROI pose : (1) MHSA refers to multi-head self-attention layer.\n\n(2) DyMLP channel indicates the dynamic MLP acting on the channel dimension, where the weight of MLP is generated by the corresponding query. (3) DyMLP spatial refers to the dynamic MLP acting on the spatial dimension. Table 3: Ablation studies for the iteration pipeline of the instance-level query.\n\nIter. pipeline AP box AP kps Figure 3(a) 55.3 64.1 Figure 3(b) 56.9 64.9\n\n(4) Spatial Attn. denotes using global spatial attention to generate the instance embedding to enhance the instancelevel query. We observe that part-level query can consistently achieve the notable improvements than instance-level query in case of utilizing the different ways to interact with the image features ROI pose . This phenomenon proves that part-level query is able to preserve more details of local area than instance-level query.\n\nAnalysis of Spatial Part Embedding Generation Module. To study the superiority of the Spatial Part Embedding Generation Module (SPEGM), we compare the performance with different interaction methods in Table 1. Notably, using the SPEGM to generate spatial-sensitive part embeddings for boosting the learnable part-level queries achieves the superior performance (63.8 AP) than the other interaction ways. Furthermore, SPEGM improves the baseline by 4.6 AP as reported in Table 2(b). We suggest that the local spatial attention is able to provide the spatial prior information and focus on the informative local regions, as shown in Figure 5. Furthermore, compared with dynamic MLP with over 10M parameters, SPEGM is more effective and efficient with only 1.1M parameters. adaptively update the learnable spatialaware part queries output from the previous stage. We conduct the controlled experiments to investigate the superiority of the proposed SIM compared with the other two iteration methods, including summation and concatenation. As reported in Table 2(a), the Selective Iteration Module achieves 64.9 AP and improves the non-iteration structure by 1.1 AP. Moreover, SIM outperforms the summation and concatenation by 0.7 and 0.8 AP, respectively. We consider that the Selective Iteration Module can adaptively boost the informative spatial details and filter the noise. Analysis of the dimension for the part-level query. The part-level queries are used to encode the local spatial detail and structure. Based on the overall network, we explore the dimension of the spatial-aware part query. As reported in Table 2(c), we observed that setting the dimension to 128 is sufficient, and the larger dimension can not bring additional gains.\n\n\nAnalysis of Selective Iteration Module. Selective Iteration Module (SIM) is presented to\n(d) (c) (b) (a)\nAnalysis of the part division scheme. As shown in Figure 4 and Table 2(d), we probe four different part division schemes to design an efficient and effective partition strategy. The scheme (c) achieves the optimal performance. Compared with the scheme (a) and (b), the part-level query in scheme (c) is capable of obtaining local structural information of adjacent keypoints. Moreover, each part in scheme (c) is rigid without articulated structure, while the part in scheme (d) is deformable, thus obtaining a slightly worse result.\n\nIteration pipeline of the instance-level query. As shown in Figure 3 and Table 3, we explore the two different iteration pipeline of the instance-level query Q I . We observe that the instance-level query iterated across box decoder and keypoint decoder serially can improve 1.6 AP box and 0.8 AP kps than only iterated across box decoder.\n\n\nResults\n\nCOCO Mini-val set.  PRTR Li et al. (2021b). Finally, we use the transformer-based backbone Swin-Large Liu et al. (2021) and also achieve 73.3 AP, which proves that our method is compatible for various network architecture. QueryPose eliminates the time-consuming post-processes and achieves the faster inference speed compared with the above state-of-the-art dense competitors.\n\nCOCO test-dev2017 set. As shown in  Geng et al. (2021). We list the comparisons in Table 6. QueryPose achieves 72.1 AP with  HRNet-W48 and 72.7 AP with Swin-large on CrowdPose test set. The results outperform the most existing MPPE methods.\n\nDiscussion. We leverage the sparse spatial-aware part-level queries to encode the local detail and spatial structure instead of dense feature representation. Our sparse method achieves the better performance, proving that dense heatmap is not all you need for pose estimation. In crowded scenes, the sparse paradigm probably avoids the issues caused by the NMS process (e.g., the heavily overlapped instances or keypoints may be removed after center or keypoint NMS). Accordingly, the sparse paradigm may be more compatible for crowded scenes.\n\nFurthermore, we observe that the typical single-stage regression-based methods (without ROI) require the online auxiliary keypoint heatmap learning during training stage, which can bring 1.5-2.5 AP improvements for different approaches (e.g., AdaptivePoseXiao et al. (2022a), PointSetNetWei et al. (2020). In contrast, The online auxiliary keypoint heatmap learning is useless for QueryPose, while the heatmap pretrain is more suitable for QueryPose. We only leverage HRNet to verify the phenomenons and bring the 1.5 AP improvements.\n\n\nConclusion\n\nIn this paper, we present a sparse end-to-end multi-person pose regression framework termed as QueryPose. With two proposed modules, i.e., Spatial Part Embedding Generation Module and Selective Iteration Module, QueryPose is capable of utilizing the sparse spatial-aware part-level queries to capture the local spatial features instead of previous dense representations and achieving the better performance than most dense MPPE methods. Our method proves the potential of the sparse regression method for the multi-person pose estimation task. We believe that our core insight is able to benefit some other methods and inspire future research.   \n\nFigure 1 :\n1Comparison with the dense end-to-end solutions, including (a) bottom-up and (b) pixel-wise regression paradigms. (c) QueryPose is a sparse end-to-end solution.\n\n\nThe top-down paradigm conducts single-person pose estimation on each cropped human image. Most top-down methods Sun et al. (2019); Su et al. (2019); Chen et al. (2018); Xiao, Wu, and Wei (2018) concentrate on learning the reliable high-quality feature representations for predicting keypoint heatmap. A few of researches Huang et al. (2020); Zhang et al. (2020) try to improve the post-process and some others Sun et al. (2018); Li et al. (2021a) attempt to exploit regression methods to bypass the keypoint heatmap. However, top-down methods consist of the independent human detector and single-person pose estimation model, leading to the non-end-to-end optimization pipeline. Bottom-up paradigm. The bottom-up paradigm Kreiss, Bertoni, and Alahi (2019); Papandreou et al. (2018); Cao et al. (2017); Cheng et al. (2020); Luo et al. (2021) formulates this task as per-pixel keypoint positioning and grouping process. Moreover, densely pixel-wise regression Xiao et al. (2022a); Zhou, Wang, and Kr\u00e4henb\u00fchl (2019); Nie et al. (2019); Tian, Chen, and Shen (2019); Geng et al. (2021); Xiao et al. (2022b); Wei et al. (2020) can be considered a special case of the bottom-up paradigm. One decomposes the MPPE into pixel-wise center localization and keypoint offset regression. However, both of them leverage the dense representations, thus introducing the non-differentiable post-processes during inference. Query-based paradigm. The query-based paradigm Carion et al. (2020); Sun et al. (2021); Zhu et al. (2020); Cheng, Schwing, and Kirillov (2021); Cheng et al. (2022); Zhang et al. (2021); Li et al. (2021b) uses the learnable object query and one-to-one label assignment to achieve the purely sparse pipeline, which is applied to several vision tasks (e.g., object detection Carion et al. (2020); Sun et al. (2021); Zhu et al. (2020) and segmentation Cheng, Schwing, and Kirillov (2021); Cheng et al. (2022); Zhang et al. (\n\nFigure 2 :\n2Overview of QueryPose. (a) Backbone for extracting multi-level features. (b) Box decoder. (c) Spatial Part Embedding Generation Module (SPEGM) employs the local spatial attention mechanism to produce spatial-sensitive part embeddings E P . (d) Selective Iteration Module (SIM) adaptively updates the learnable part-level queries Q P by using the spatial-sensitive part embeddings. (e) The spatial-aware part-level quires are used to regress the keypoint coordinates via residual log-likelihood estimation(RLE) Li et al. (2021a). The box decoder is following Sparse RCNN. The keypoint decoder is composed of the proposed SPEGM and SIM.Part Embedding Generation Module and Selective Information Module. The box decoder and keypoint decoder are repeated S times to build the cascade framework. S is set to 6 by default, following previous methodsCarion et al. (2020);Sun et al. (2021). We only take stage s as example for illustration.\n\n\nhand-crafted post-procedures (e.g., keypoint NMS, center NMS and grouping) and only need to select the pose candidates with high pose score.\n\nFigure 5 :\n5The visualizations of the local spatial attention maps A, which correspond to the local parts of each person. The local salient region is visualized by bright color. The backgrounds of person regions are rendered to red. Best viewed after zooming in.\n\nFigure 4 :\n4The different part division schemes. (a) Each keypoint is corresponding to a part-level query. (b) All keypoints on head are corresponding to a part-level query. (c) Each divided part contains rigid structure and corresponds to a part-level query. (d) The head and four limbs are corresponding to a part-level query respectively.\n\nFigure 6 :\n6The visualizations of local spatial attention maps. Each local spatial attention focuses on the specific human part. The human backgrounds are visualized by red. The salient local regions are visualized by bright color.\n\nFigure 7 :\n7Examples of the predicted multi-person pose on MS COCO. QueryPose deals well with pose deformation and scale variation.\n\nFigure 8 :\n8Examples of the predicted multi-person pose on CrowPose. QueryPose is robust for the challenging crowded multi-person scenarios, including occlusions, blurred and twisted limbs.\n\nTable 1 :\n1Ablation studies for exploring the query type and feature interaction method. IQ and PQ indicate the instance-level query and part-level query respectively.Query \nInteraction \nAP \nAPM APL \n\nIQ \nMHSA \n60.7 \n56.4 \n68.1 \nPQ \nMHSA \n62.6 \n58.2 \n69.9 \nIQ \nDyMLP channel 60.4 \n56.3 \n67.4 \nPQ \nDyMLP channel 63.0 \n58.7 \n70.4 \nIQ \nDyMLP spatial \n60.5 \n56.5 \n67.5 \nPQ \nDyMLP spatial \n62.8 \n58.6 \n70.3 \nIQ \nSpatial Attn. \n60.2 \n56.0 \n67.1 \nPQ \nSPEGM \n63.8 \n59.4 \n71.3 \n\n\n\nTable 2 :\n2Ablation experiments.(a) Ablative studies for the iteration methods \nof learnable part-level queries. \nIteration \nAP \nAPM APL \nNone \n63.8 \n59.4 \n71.3 \nSummation \n64.2 \n60.1 \n71.7 \nConcatenation 64.1 \n59.6 \n71.6 \nSelective Iter. \n64.9 \n60.4 \n72.3 \n\n(b) The contributions of two proposed \nmodules in overall framework. \nMethod \nAP \nAPM APL \nBaseline \n59.2 \n55.1 \n66.0 \n+ SPEGM 63.8 \n59.4 \n71.3 \n+ SIM \n64.9 \n60.4 \n72.3 \n\n(c) The influence for the dimension of \nspatial-sensitive part-level query. \nDimension AP \nAPM APL \n32 \n64.1 \n59.8 \n71.3 \n64 \n64.4 \n60.0 \n71.8 \n128 \n64.9 \n60.4 \n72.3 \n256 \n64.7 \n60.5 \n72.1 \n\n\n\nTable 4 :\n4Comparisons with the end-to-end MPPE methods on the COCO mini-val set. Note that all results are reported for single-scale testing. Flip refers to using the flip testing to boost performance.Methods \nBackbone \nAP \nAP50 AP75 APM \nAPL Time [ms] \n\nDense End-to-End MPPE \n\nMask R-CNN He et al. (2017) \nResNet101 \n66.1 \n87.7 \n71.7 \n60.5 \n75.0 \n128 \nHrHRNet-W48 Cheng et al. (2020) \nHrHRNet-W48 66.6 \n85.3 \n72.8 \n61.7 \n74.4 \n233 \nHrHRNet-W48 (Flip) Cheng et al. (2020) \nHrHRNet-W48 69.8 \n87.2 \n76.1 \n65.4 \n76.4 \n317 \nSWAHR-W48 Luo et al. (2021) \nHrHRNet-W48 67.3 \n87.1 \n72.9 \n62.1 \n75.0 \n242 \nSWAHR-W48 (Flip) Luo et al. (2021) \nHrHRNet-W48 70.8 \n88.5 \n76.8 \n66.3 \n77.4 \n339 \nCenterGroup-W48 Bras\u00f3, Kister, and Leal-Taix\u00e9 (2021) \nHrHRNet-W48 69.1 \n-\n-\n-\n-\n-\nCenterNet-HG (Flip) Zhou, Wang, and Kr\u00e4henb\u00fchl (2019) \nHG-104 \n64.0 \n-\n-\n-\n-\n135 \nMask R-CNN + RLE Li et al. (2021a) \nResNet101 \n66.7 \n86.7 \n72.6 \n-\n-\n-\nDEKR-W48 Geng et al. (2021) \nHRNet-W48 \n67.1 \n87.7 \n73.9 \n61.5 \n77.1 \n197 \nDEKR-W48 (Flip) Geng et al. (2021) \nHRNet-W48 \n71.0 \n88.3 \n77.4 \n66.7 \n78.5 \n284 \nAdaptivePose Xiao et al. (2022a) \nHRNet-W48 \n70.0 \n87.5 \n76.1 \n65.4 \n77.1 \n110 \nLQR (Flip) Xiao et al. (2022b) \nHRNet-W48 \n72.4 \n89.1 \n79.0 \n67.3 \n80.4 \n297 \n\nSparse End-to-End MPPE \n\nPRTR Li et al. (2021b) \nHRNet-W48 \n66.2 \n85.9 \n72.1 \n61.3 \n74.4 \n-\nQueryPose \nResNet50 \n68.7 \n88.6 \n74.4 \n63.8 \n76.5 \n70 \nQueryPose \nSwin-L \n73.3 \n91.3 \n79.5 \n68.5 \n81.2 \n117 \nQueryPose \nHRNet-W32 \n72.4 \n89.8 \n78.6 \n67.9 \n79.7 \n100 \nQueryPose \nHRNet-W48 \n73.6 \n90.3 \n79.7 \n69.3 \n80.8 \n105 \n\n\n\nTable 4\n4reports the results of end-to-end methods on COCO mini-val set. AP respectively. Furthermore, QueryPose outperforms the regression-based AdaptivePose-W48Xiao et al. (2022a) and DEKR-W48(Flip)Geng et al. (2021) by 3.6 AP and 2.6 AP. QueryPose obtains 7.4 AP improvements over the existing sparse end-to-end methodCompared with dense end-to-end methods, QueryPose with ResNet-50 outperforms Mask R-CNN \nHe et al. (2017) with ResNet-101 by 2.6 AP. By only using HRNet-W32, we achieve 72.4 AP and \nmarkedly outperform all dense counterparts with larger backbone. QueryPose with HRNet-W48 \nobtains 73.6 AP and significantly surpasses the previous most competitive HrHRNet-W48 (Flip) \nCheng et al. (2020), SWAHR-W48 (Flip) Luo et al. (2021), CenterGroup-W48Bras\u00f3, Kister, and \nLeal-Taix\u00e9 (2021) by 3.8 AP, 2.8 AP, 4.5 \n\nTable 5 ,\n5we achieve 72.3 AP with HRNet-W48 and 72.2 AP with Swin-Large without flip and multi-scale testing. QueryPose exceeds all end-to-end methods and narrows the gap with two-stage methods. The results verify the superior keypoint positioning ability of QueryPose in single-forward pass.CrowdPose test set. We further evaluate the QueryPose on CrowdPose. The model is trained on the train and val set, then evaluated on test set as previous methodsCheng et al. (2020);Luo et al. (2021);\n\nTable 5 :\n5Comprehensive comparisons on COCO test-dev set. * indicates the refinement by a well-trained single-person pose estimation model. \u2020 refers to test-time augmentation (e.g., flip or multi-scale testing).Methods \nAP \nAP50 AP75 APM APL \n\nTwo-Stage Methods \n\nG-RMI Papandreou et al. (2017) \n64.9 \n85.5 \n71.3 \n62.3 \n70.0 \nIntegral Pose Sun et al. (2018) \n67.8 \n88.2 \n74.8 \n63.9 \n74.0 \nCPN  \u2020 Chen et al. (2018) \n72.1 \n91.4 \n80.0 \n68.7 \n77.2 \nSimpleBaseline  \u2020 Xiao, Wu, and Wei (2018) \n73.7 \n91.9 \n81.1 \n70.3 \n80.0 \nHRNet  \u2020 Sun et al. (2019) \n75.5 \n92.5 \n83.3 \n71.9 \n81.5 \n\nEnd-to-End Methods \n\nCMU-Pose  *  \u2020 Cao et al. (2017) \n61.8 \n84.9 \n67.5 \n57.1 \n68.2 \nAE  *  \u2020 Newell, Huang, and Deng (2017) \n65.5 \n86.8 \n72.3 \n60.6 \n72.6 \nCenterNet-HG Zhou, Wang, and Kr\u00e4henb\u00fchl (2019) 63.0 \n86.8 \n69.6 \n58.9 \n70.4 \nSPM  *  \u2020 Nie et al. (2019) \n66.9 \n88.5 \n72.9 \n62.6 \n73.1 \nPointSetNet  \u2020 Wei et al. (2020) \n68.7 \n89.9 \n76.3 \n64.8 \n75.3 \nHrHRNet  \u2020 Cheng et al. (2020) \n70.5 \n89.3 \n77.2 \n66.6 \n75.8 \nDEKR  \u2020 Geng et al. (2021) \n71.0 \n89.2 \n78.0 \n67.1 \n76.9 \nCenterGroup  \u2020 Bras\u00f3, Kister, and Leal-Taix\u00e9 (2021) \n71.1 \n90.5 \n77.5 \n66.9 \n76.7 \nLQR  \u2020 Xiao et al. (2022b) \n71.7 \n90.4 \n78.7 \n67.3 \n78.5 \nAdaptivePose  \u2020 Xiao et al. (2022a) \n71.3 \n90.0 \n78.3 \n67.1 \n77.2 \nQueryPose-W48 \n72.3 \n91.5 \n78.7 \n67.8 \n79.0 \nQueryPose-SwinL \n72.2 \n92.0 \n78.8 \n67.3 \n79.4 \n\n\n\nTable 6 :\n6Comparisons between the previous two-stage and end-to-end MPPE methods on CrowdPose test set. \u2020 indicates multi-scale testing. AdaptivePose-W48 \u2020 Xiao et al. (2022a) CenterGroup-W48 \u2020 Bras\u00f3, Kister, and Leal-Taix\u00e9 (2021) 70.0Methods \nAP \nAP50 AP75 APE APM APH \n\nTwo-Stage MPPE \n\nRmpe Fang et al. (2017) \n61.0 \n81.3 \n66.0 \n71.2 \n61.4 \n51.1 \nSimpleBaseline Xiao, Wu, and Wei (2018) \n60.8 \n84.2 \n71.5 \n71.4 \n61.2 \n51.2 \nCrowdPose Li et al. (2019a) \n66.0 \n84.2 \n71.5 \n75.5 \n66.3 \n57.4 \n\nEnd-to-End MPPE \n\nCMU-Pose Cao et al. (2017) \n-\n-\n-\n62.7 \n48.7 \n32.3 \nMask-RCNN He et al. (2017) \n57.2 \n83.5 \n60.3 \n69.4 \n57.9 \n45.8 \nSWAHR-W48 Luo et al. (2021) \n71.6 \n88.5 \n77.6 \n78.9 \n72.4 \n63.0 \nDEKR-W48  \u2020 Geng et al. (2021) \n68.0 \n85.5 \n73.4 \n76.6 \n68.8 \n58.4 \n69.2 \n87.3 \n75.0 \n76.7 \n70.0 \n60.9 \nHigherHRNet-W48  \u2020 Cheng et al. (2020) \n67.6 \n87.4 \n72.6 \n75.8 \n68.1 \n58.9 \n88.9 \n75.1 \n76.8 \n70.7 \n62.2 \n\nQueryPose-W48 \n72.1 \n89.4 \n78.0 \n79.8 \n73.3 \n63.0 \nQueryPose-SwinL \n72.7 \n91.7 \n78.1 \n79.5 \n73.4 \n65.4 \n\n\nAcknowledgementsThis work is supported by the National Natural Science Foundation of China No.62071056 and No.62102039.A AppendixCrowdPose test set.Table 6reports the comparisons on CrowdPose. Generally, the two-stage paradigm always obtains the better results than end-to-end paradigm. However, QueryPose surpasses the two-stage methods by a large margin. We consider the reasons can be summarized into two aspects: 1) The reported two-stage methods generally use the dense human detector, which require the NMS to suppress the duplicates. However, in crowd scenes, the heavily overlapped instances may be removed after NMS, thus leading to inferior results. 2) The single-person pose estimator leverages the heatmap to independently represent the keypoint position, while losing the keypoint relations. In crowded scenes, the cropped single person image always contains the limbs of other persons. Learning the keypoint associations can avoid confusion. QueryPose leverages sparse paradigm to solve the MPPE and employ the multi-head self-attention to capture the relationships across different part queries, thus is more robust for crowded scenes. Qualitative results. First, As shown inFigure 6, we present more visualizations of local attention maps for specific parts. We observe that the spatial attention can precisely focus on the corresponding local part. Moreover, in case of occluded or invisible keypoints, the corresponding spatial attention will concentrate on the region with more semantic context to infer the occluded keypoints. Second, we visualize the predicted multi-person pose on MS COCO, as shown inFigure 7. QueryPose can precisely estimate multi-person pose in various complex scenarios. Finally, we visualize the predicted multi-person pose on CrowdPose, as shown inFigure 8. QueryPose is robust for the crowded multi-person scenarios, including numerous occluded and overlapped human bodies, blurred and twisted limbs.\nThe Center of Attention: Center-Keypoint Grouping via Attention for Multi-Person Pose Estimation. G Bras\u00f3, N Kister, L Leal-Taix\u00e9, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionBras\u00f3, G.; Kister, N.; and Leal-Taix\u00e9, L. 2021. The Center of Attention: Center-Keypoint Grouping via Attention for Multi-Person Pose Estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 11853-11863.\n\nRealtime multi-person 2d pose estimation using part affinity fields. Z Cao, T Simon, S.-E Wei, Y Sheikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionCao, Z.; Simon, T.; Wei, S.-E.; and Sheikh, Y. 2017. Realtime multi-person 2d pose estimation using part affinity fields. In Proceedings of the IEEE conference on computer vision and pattern recognition, 7291-7299.\n\nEnd-to-end object detection with transformers. N Carion, F Massa, G Synnaeve, N Usunier, A Kirillov, S Zagoruyko, European conference on computer vision. SpringerCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov, A.; and Zagoruyko, S. 2020. End-to-end object detection with transformers. In European conference on computer vision, 213-229. Springer.\n\nCascaded pyramid network for multiperson pose estimation. Y Chen, Z Wang, Y Peng, Z Zhang, G Yu, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionChen, Y.; Wang, Z.; Peng, Y.; Zhang, Z.; Yu, G.; and Sun, J. 2018. Cascaded pyramid network for multi- person pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, 7103-7112.\n\nMasked-attention mask transformer for universal image segmentation. B Cheng, I Misra, A G Schwing, A Kirillov, R Girdhar, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionCheng, B.; Misra, I.; Schwing, A. G.; Kirillov, A.; and Girdhar, R. 2022. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1290-1299.\n\nPer-pixel classification is not all you need for semantic segmentation. B Cheng, A Schwing, A Kirillov, Advances in Neural Information Processing Systems. 34Cheng, B.; Schwing, A.; and Kirillov, A. 2021. Per-pixel classification is not all you need for semantic segmentation. Advances in Neural Information Processing Systems, 34.\n\nHigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation. B Cheng, B Xiao, J Wang, H Shi, T S Huang, L Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionCheng, B.; Xiao, B.; Wang, J.; Shi, H.; Huang, T. S.; and Zhang, L. 2020. HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 5386-5395.\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-Fei, L. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, 248-255. Ieee.\n\nRevisiting skeleton-based action recognition. H Duan, Y Zhao, K Chen, D Shao, D Lin, B Dai, arXiv:2104.13586arXiv preprintDuan, H.; Zhao, Y.; Chen, K.; Shao, D.; Lin, D.; and Dai, B. 2021. Revisiting skeleton-based action recognition. arXiv preprint arXiv:2104.13586.\n\nRmpe: Regional multi-person pose estimation. H.-S Fang, S Xie, Y.-W Tai, C Lu, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionFang, H.-S.; Xie, S.; Tai, Y.-W.; and Lu, C. 2017. Rmpe: Regional multi-person pose estimation. In Proceedings of the IEEE International Conference on Computer Vision, 2334-2343.\n\nBottom-up human pose estimation via disentangled keypoint regression. Z Geng, K Sun, B Xiao, Z Zhang, J Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionGeng, Z.; Sun, K.; Xiao, B.; Zhang, Z.; and Wang, J. 2021. Bottom-up human pose estimation via disentangled keypoint regression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 14676-14686.\n\nMask r-cnn. K He, G Gkioxari, P Doll\u00e1r, R Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionHe, K.; Gkioxari, G.; Doll\u00e1r, P.; and Girshick, R. 2017. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, 2961-2969.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 770-778.\n\nThe devil is in the details: Delving into unbiased data processing for human pose estimation. J Huang, Z Zhu, F Guo, G Huang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionHuang, J.; Zhu, Z.; Guo, F.; and Huang, G. 2020. The devil is in the details: Delving into unbiased data processing for human pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 5700-5709.\n\nPifpaf: Composite fields for human pose estimation. S Kreiss, L Bertoni, A Alahi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionKreiss, S.; Bertoni, L.; and Alahi, A. 2019. Pifpaf: Composite fields for human pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 11977-11986.\n\nHuman pose regression with residual log-likelihood estimation. J Li, S Bian, A Zeng, C Wang, B Pang, W Liu, C Lu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionLi, J.; Bian, S.; Zeng, A.; Wang, C.; Pang, B.; Liu, W.; and Lu, C. 2021a. Human pose regression with residual log-likelihood estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 11025-11034.\n\nCrowdpose: Efficient crowded scenes pose estimation and a new benchmark. J Li, C Wang, H Zhu, Y Mao, H.-S Fang, C Lu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionLi, J.; Wang, C.; Zhu, H.; Mao, Y.; Fang, H.-S.; and Lu, C. 2019a. Crowdpose: Efficient crowded scenes pose estimation and a new benchmark. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 10863-10872.\n\nPose recognition with cascade transformers. K Li, S Wang, X Zhang, Y Xu, W Xu, Z Tu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionLi, K.; Wang, S.; Zhang, X.; Xu, Y.; Xu, W.; and Tu, Z. 2021b. Pose recognition with cascade transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1944-1953.\n\nActional-structural graph convolutional networks for skeleton-based action recognition. M Li, S Chen, X Chen, Y Zhang, Y Wang, Q Tian, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLi, M.; Chen, S.; Chen, X.; Zhang, Y.; Wang, Y.; and Tian, Q. 2019b. Actional-structural graph convolutional networks for skeleton-based action recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3595-3603.\n\nTokenpose: Learning keypoint tokens for human pose estimation. Y Li, S Zhang, Z Wang, S Yang, W Yang, S.-T Xia, E Zhou, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionLi, Y.; Zhang, S.; Wang, Z.; Yang, S.; Yang, W.; Xia, S.-T.; and Zhou, E. 2021c. Tokenpose: Learning keypoint tokens for human pose estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 11313-11322.\n\nMicrosoft coco: Common objects in context. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, European conference on computer vision. SpringerLin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; Doll\u00e1r, P.; and Zitnick, C. L. 2014. Microsoft coco: Common objects in context. In European conference on computer vision, 740-755. Springer.\n\nSwin transformer: Hierarchical vision transformer using shifted windows. Z Liu, Y Lin, Y Cao, H Hu, Y Wei, Z Zhang, S Lin, B Guo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionLiu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin, S.; and Guo, B. 2021. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 10012-10022.\n\nRethinking the Heatmap Regression for Bottom-up Human Pose Estimation. Z Luo, Z Wang, Y Huang, L Wang, T Tan, E Zhou, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionLuo, Z.; Wang, Z.; Huang, Y.; Wang, L.; Tan, T.; and Zhou, E. 2021. Rethinking the Heatmap Regression for Bottom-up Human Pose Estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 13264-13273.\n\nAssociative embedding: End-to-end learning for joint detection and grouping. A Newell, Z Huang, J Deng, Advances in neural information processing systems. Newell, A.; Huang, Z.; and Deng, J. 2017. Associative embedding: End-to-end learning for joint detection and grouping. In Advances in neural information processing systems, 2277-2287.\n\nStacked hourglass networks for human pose estimation. A Newell, K Yang, J Deng, European conference on computer vision. SpringerNewell, A.; Yang, K.; and Deng, J. 2016. Stacked hourglass networks for human pose estimation. In European conference on computer vision, 483-499. Springer.\n\nSingle-stage multi-person pose machines. X Nie, J Feng, J Zhang, Yan , S , Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionNie, X.; Feng, J.; Zhang, J.; and Yan, S. 2019. Single-stage multi-person pose machines. In Proceedings of the IEEE International Conference on Computer Vision, 6951-6960.\n\nPersonlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model. G Papandreou, T Zhu, L.-C Chen, S Gidaris, J Tompson, K Murphy, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Papandreou, G.; Zhu, T.; Chen, L.-C.; Gidaris, S.; Tompson, J.; and Murphy, K. 2018. Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model. In Proceedings of the European Conference on Computer Vision (ECCV), 269-286.\n\nTowards accurate multi-person pose estimation in the wild. G Papandreou, T Zhu, N Kanazawa, A Toshev, J Tompson, C Bregler, K Murphy, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionPapandreou, G.; Zhu, T.; Kanazawa, N.; Toshev, A.; Tompson, J.; Bregler, C.; and Murphy, K. 2017. Towards accurate multi-person pose estimation in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 4903-4911.\n\nTwo-stream adaptive graph convolutional networks for skeletonbased action recognition. L Shi, Y Zhang, J Cheng, H Lu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionShi, L.; Zhang, Y.; Cheng, J.; and Lu, H. 2019. Two-stream adaptive graph convolutional networks for skeleton- based action recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 12026-12035.\n\nMulti-person pose estimation with enhanced channel-wise and spatial information. K Su, D Yu, Z Xu, X Geng, C Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSu, K.; Yu, D.; Xu, Z.; Geng, X.; and Wang, C. 2019. Multi-person pose estimation with enhanced channel-wise and spatial information. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 5674-5682.\n\nDeep high-resolution representation learning for human pose estimation. K Sun, B Xiao, D Liu, J Wang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSun, K.; Xiao, B.; Liu, D.; and Wang, J. 2019. Deep high-resolution representation learning for human pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, 5693-5703.\n\nSparse r-cnn: End-to-end object detection with learnable proposals. P Sun, R Zhang, Y Jiang, T Kong, C Xu, W Zhan, M Tomizuka, L Li, Z Yuan, C Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSun, P.; Zhang, R.; Jiang, Y.; Kong, T.; Xu, C.; Zhan, W.; Tomizuka, M.; Li, L.; Yuan, Z.; Wang, C.; et al. 2021. Sparse r-cnn: End-to-end object detection with learnable proposals. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 14454-14463.\n\nIntegral human pose regression. X Sun, B Xiao, F Wei, S Liang, Y Wei, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Sun, X.; Xiao, B.; Wei, F.; Liang, S.; and Wei, Y. 2018. Integral human pose regression. In Proceedings of the European Conference on Computer Vision (ECCV), 529-545.\n\nDirectPose: Direct End-to-End Multi-Person Pose Estimation. Z Tian, H Chen, C Shen, arXiv:1911.07451arXiv preprintTian, Z.; Chen, H.; and Shen, C. 2019. DirectPose: Direct End-to-End Multi-Person Pose Estimation. arXiv preprint arXiv:1911.07451.\n\nPoint-set anchors for object detection, instance segmentation and pose estimation. F Wei, X Sun, H Li, J Wang, S Lin, European Conference on Computer Vision. SpringerWei, F.; Sun, X.; Li, H.; Wang, J.; and Lin, S. 2020. Point-set anchors for object detection, instance segmentation and pose estimation. In European Conference on Computer Vision, 527-544. Springer.\n\n. Y Wu, A Kirillov, F Massa, W.-Y Lo, R Girshick, Wu, Y.; Kirillov, A.; Massa, F.; Lo, W.-Y.; and Girshick, R. 2019. Detectron2. https://github.com/ facebookresearch/detectron2.\n\nSimple baselines for human pose estimation and tracking. B Xiao, H Wu, Y Wei, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Xiao, B.; Wu, H.; and Wei, Y. 2018. Simple baselines for human pose estimation and tracking. In Proceedings of the European conference on computer vision (ECCV), 466-481.\n\nAdaptivepose: Human parts as adaptive points. Y Xiao, X J Wang, D Yu, G Wang, Q Zhang, H Mingshu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceXiao, Y.; Wang, X. J.; Yu, D.; Wang, G.; Zhang, Q.; and Mingshu, H. 2022a. Adaptivepose: Human parts as adaptive points. In Proceedings of the AAAI Conference on Artificial Intelligence.\n\nLearning quality-aware representation for multi-person pose regression. Y Xiao, D Yu, X J Wang, L Jin, G Wang, Q Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceXiao, Y.; Yu, D.; Wang, X. J.; Jin, L.; Wang, G.; and Zhang, Q. 2022b. Learning quality-aware representation for multi-person pose regression. In Proceedings of the AAAI Conference on Artificial Intelligence.\n\nSpatial temporal graph convolutional networks for skeleton-based action recognition. S Yan, Y Xiong, D Lin, Thirty-second AAAI conference on artificial intelligence. Yan, S.; Xiong, Y.; and Lin, D. 2018. Spatial temporal graph convolutional networks for skeleton-based action recognition. In Thirty-second AAAI conference on artificial intelligence.\n\nMulti-person Pose Estimation for Pose Tracking with Enhanced Cascaded Pyramid Network. D Yu, K Su, J Sun, C Wang, European Conference on Computer Vision. Yu, D.; Su, K.; Sun, J.; and Wang, C. 2018. Multi-person Pose Estimation for Pose Tracking with Enhanced Cascaded Pyramid Network. In European Conference on Computer Vision.\n\nDistribution-aware coordinate representation for human pose estimation. F Zhang, X Zhu, H Dai, M Ye, C Zhu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionZhang, F.; Zhu, X.; Dai, H.; Ye, M.; and Zhu, C. 2020. Distribution-aware coordinate representation for human pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 7093-7102.\n\nK-net: Towards unified image segmentation. W Zhang, J Pang, K Chen, C C Loy, Advances in Neural Information Processing Systems. 34Zhang, W.; Pang, J.; Chen, K.; and Loy, C. C. 2021. K-net: Towards unified image segmentation. Advances in Neural Information Processing Systems, 34.\n\nX Zhou, D Wang, P Kr\u00e4henb\u00fchl, arXiv:1904.07850Objects as points. arXiv preprintZhou, X.; Wang, D.; and Kr\u00e4henb\u00fchl, P. 2019. Objects as points. arXiv preprint arXiv:1904.07850.\n\nDeformable detr: Deformable transformers for end-to-end object detection. X Zhu, W Su, L Lu, B Li, X Wang, J Dai, arXiv:2010.04159arXiv preprintZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2020. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159.\n", "annotations": {"author": "[{\"end\":116,\"start\":85},{\"end\":144,\"start\":117},{\"end\":159,\"start\":145},{\"end\":192,\"start\":160},{\"end\":220,\"start\":193},{\"end\":258,\"start\":221},{\"end\":278,\"start\":259},{\"end\":310,\"start\":279},{\"end\":330,\"start\":311},{\"end\":369,\"start\":331},{\"end\":421,\"start\":370},{\"end\":450,\"start\":422}]", "publisher": null, "author_last_name": "[{\"end\":94,\"start\":90},{\"end\":123,\"start\":121},{\"end\":158,\"start\":154},{\"end\":171,\"start\":169},{\"end\":200,\"start\":197},{\"end\":232,\"start\":228}]", "author_first_name": "[{\"end\":89,\"start\":85},{\"end\":120,\"start\":117},{\"end\":153,\"start\":145},{\"end\":168,\"start\":160},{\"end\":196,\"start\":193},{\"end\":227,\"start\":221}]", "author_affiliation": "[{\"end\":277,\"start\":260},{\"end\":309,\"start\":280},{\"end\":329,\"start\":312},{\"end\":368,\"start\":332},{\"end\":420,\"start\":371},{\"end\":449,\"start\":423}]", "title": "[{\"end\":82,\"start\":1},{\"end\":532,\"start\":451}]", "venue": null, "abstract": "[{\"end\":2402,\"start\":534}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2640,\"start\":2624},{\"end\":2642,\"start\":2641},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2671,\"start\":2642},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2691,\"start\":2673},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2834,\"start\":2817},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2853,\"start\":2836},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2881,\"start\":2855},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2901,\"start\":2883},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2944,\"start\":2920},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2962,\"start\":2946},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3124,\"start\":3106},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3144,\"start\":3126},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3162,\"start\":3146},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3181,\"start\":3164},{\"end\":3202,\"start\":3183},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3583,\"start\":3564},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3608,\"start\":3585},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3634,\"start\":3608},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3666,\"start\":3636},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3912,\"start\":3893},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3947,\"start\":3914},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3966,\"start\":3949},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3995,\"start\":3968},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4015,\"start\":3997},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4843,\"start\":4826},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4865,\"start\":4845},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4884,\"start\":4867},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4921,\"start\":4886},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4942,\"start\":4923},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5088,\"start\":5069},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5109,\"start\":5090},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5146,\"start\":5111},{\"end\":6851,\"start\":6829},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6883,\"start\":6866},{\"end\":8297,\"start\":8275},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16768,\"start\":16748},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":16787,\"start\":16770},{\"end\":17487,\"start\":17466},{\"end\":19376,\"start\":19354},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19673,\"start\":19656},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19951,\"start\":19934},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":21075,\"start\":21059},{\"end\":25423,\"start\":25401},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25500,\"start\":25483},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25814,\"start\":25796},{\"end\":26820,\"start\":26790},{\"end\":26850,\"start\":26820},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30719,\"start\":30699},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":30737,\"start\":30720},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":34923,\"start\":34904},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":34960,\"start\":34942},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":36038,\"start\":36019},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":36056,\"start\":36039}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27915,\"start\":27743},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29842,\"start\":27916},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30788,\"start\":29843},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30931,\"start\":30789},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31195,\"start\":30932},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31538,\"start\":31196},{\"attributes\":{\"id\":\"fig_6\"},\"end\":31771,\"start\":31539},{\"attributes\":{\"id\":\"fig_7\"},\"end\":31904,\"start\":31772},{\"attributes\":{\"id\":\"fig_8\"},\"end\":32095,\"start\":31905},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":32567,\"start\":32096},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33190,\"start\":32568},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34740,\"start\":33191},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":35563,\"start\":34741},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":36057,\"start\":35564},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":37416,\"start\":36058},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":38427,\"start\":37417}]", "paragraph": "[{\"end\":2963,\"start\":2418},{\"end\":4793,\"start\":2965},{\"end\":5798,\"start\":4795},{\"end\":6884,\"start\":5800},{\"end\":6957,\"start\":6886},{\"end\":7317,\"start\":6959},{\"end\":7632,\"start\":7319},{\"end\":8006,\"start\":7634},{\"end\":8158,\"start\":8023},{\"end\":8609,\"start\":8160},{\"end\":9164,\"start\":8643},{\"end\":10046,\"start\":9166},{\"end\":10593,\"start\":10193},{\"end\":12275,\"start\":10595},{\"end\":12421,\"start\":12418},{\"end\":12769,\"start\":12423},{\"end\":13267,\"start\":12814},{\"end\":14182,\"start\":13269},{\"end\":14479,\"start\":14256},{\"end\":15078,\"start\":14510},{\"end\":15615,\"start\":15080},{\"end\":16077,\"start\":15797},{\"end\":16245,\"start\":16079},{\"end\":18043,\"start\":16278},{\"end\":18375,\"start\":18045},{\"end\":18718,\"start\":18420},{\"end\":18924,\"start\":18720},{\"end\":19319,\"start\":18953},{\"end\":19812,\"start\":19342},{\"end\":20207,\"start\":19814},{\"end\":21174,\"start\":20209},{\"end\":21822,\"start\":21193},{\"end\":22124,\"start\":21824},{\"end\":22198,\"start\":22126},{\"end\":22642,\"start\":22200},{\"end\":24387,\"start\":22644},{\"end\":25028,\"start\":24495},{\"end\":25369,\"start\":25030},{\"end\":25758,\"start\":25381},{\"end\":26000,\"start\":25760},{\"end\":26545,\"start\":26002},{\"end\":27081,\"start\":26547},{\"end\":27742,\"start\":27096}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10192,\"start\":10047},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12417,\"start\":12276},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14255,\"start\":14183},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15720,\"start\":15616},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15796,\"start\":15720},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18419,\"start\":18376},{\"attributes\":{\"id\":\"formula_6\"},\"end\":24494,\"start\":24479}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21611,\"start\":21604},{\"end\":22050,\"start\":22043},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":22852,\"start\":22845},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23121,\"start\":23114},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23702,\"start\":23695},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24265,\"start\":24258},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24565,\"start\":24558},{\"end\":25110,\"start\":25103},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":25850,\"start\":25843}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2416,\"start\":2404},{\"attributes\":{\"n\":\"2\"},\"end\":8021,\"start\":8009},{\"attributes\":{\"n\":\"3\"},\"end\":8618,\"start\":8612},{\"attributes\":{\"n\":\"3.1\"},\"end\":8641,\"start\":8621},{\"attributes\":{\"n\":\"3.2\"},\"end\":12812,\"start\":12772},{\"attributes\":{\"n\":\"3.3\"},\"end\":14508,\"start\":14482},{\"attributes\":{\"n\":\"3.4\"},\"end\":16276,\"start\":16248},{\"attributes\":{\"n\":\"4\"},\"end\":18951,\"start\":18927},{\"attributes\":{\"n\":\"4.1\"},\"end\":19340,\"start\":19322},{\"attributes\":{\"n\":\"4.2\"},\"end\":21191,\"start\":21177},{\"end\":24478,\"start\":24390},{\"attributes\":{\"n\":\"4.3\"},\"end\":25379,\"start\":25372},{\"attributes\":{\"n\":\"5\"},\"end\":27094,\"start\":27084},{\"end\":27754,\"start\":27744},{\"end\":29854,\"start\":29844},{\"end\":30943,\"start\":30933},{\"end\":31207,\"start\":31197},{\"end\":31550,\"start\":31540},{\"end\":31783,\"start\":31773},{\"end\":31916,\"start\":31906},{\"end\":32106,\"start\":32097},{\"end\":32578,\"start\":32569},{\"end\":33201,\"start\":33192},{\"end\":34749,\"start\":34742},{\"end\":35574,\"start\":35565},{\"end\":36068,\"start\":36059},{\"end\":37427,\"start\":37418}]", "table": "[{\"end\":32567,\"start\":32264},{\"end\":33190,\"start\":32601},{\"end\":34740,\"start\":33394},{\"end\":35563,\"start\":35063},{\"end\":37416,\"start\":36271},{\"end\":38427,\"start\":37654}]", "figure_caption": "[{\"end\":27915,\"start\":27756},{\"end\":29842,\"start\":27918},{\"end\":30788,\"start\":29856},{\"end\":30931,\"start\":30791},{\"end\":31195,\"start\":30945},{\"end\":31538,\"start\":31209},{\"end\":31771,\"start\":31552},{\"end\":31904,\"start\":31785},{\"end\":32095,\"start\":31918},{\"end\":32264,\"start\":32108},{\"end\":32601,\"start\":32580},{\"end\":33394,\"start\":33203},{\"end\":35063,\"start\":34751},{\"end\":36057,\"start\":35576},{\"end\":36271,\"start\":36070},{\"end\":37654,\"start\":37429}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3822,\"start\":3814},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4037,\"start\":4029},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":8685,\"start\":8677},{\"end\":12443,\"start\":12435},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":14351,\"start\":14342},{\"end\":15819,\"start\":15811},{\"end\":22166,\"start\":22155},{\"end\":22188,\"start\":22177},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23283,\"start\":23275},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24553,\"start\":24545},{\"end\":25098,\"start\":25090}]", "bib_author_first_name": "[{\"end\":40474,\"start\":40473},{\"end\":40483,\"start\":40482},{\"end\":40493,\"start\":40492},{\"end\":40941,\"start\":40940},{\"end\":40948,\"start\":40947},{\"end\":40960,\"start\":40956},{\"end\":40967,\"start\":40966},{\"end\":41381,\"start\":41380},{\"end\":41391,\"start\":41390},{\"end\":41400,\"start\":41399},{\"end\":41412,\"start\":41411},{\"end\":41423,\"start\":41422},{\"end\":41435,\"start\":41434},{\"end\":41752,\"start\":41751},{\"end\":41760,\"start\":41759},{\"end\":41768,\"start\":41767},{\"end\":41776,\"start\":41775},{\"end\":41785,\"start\":41784},{\"end\":41791,\"start\":41790},{\"end\":42228,\"start\":42227},{\"end\":42237,\"start\":42236},{\"end\":42246,\"start\":42245},{\"end\":42248,\"start\":42247},{\"end\":42259,\"start\":42258},{\"end\":42271,\"start\":42270},{\"end\":42743,\"start\":42742},{\"end\":42752,\"start\":42751},{\"end\":42763,\"start\":42762},{\"end\":43089,\"start\":43088},{\"end\":43098,\"start\":43097},{\"end\":43106,\"start\":43105},{\"end\":43114,\"start\":43113},{\"end\":43121,\"start\":43120},{\"end\":43123,\"start\":43122},{\"end\":43132,\"start\":43131},{\"end\":43601,\"start\":43600},{\"end\":43609,\"start\":43608},{\"end\":43617,\"start\":43616},{\"end\":43630,\"start\":43626},{\"end\":43636,\"start\":43635},{\"end\":43642,\"start\":43641},{\"end\":43978,\"start\":43977},{\"end\":43986,\"start\":43985},{\"end\":43994,\"start\":43993},{\"end\":44002,\"start\":44001},{\"end\":44010,\"start\":44009},{\"end\":44017,\"start\":44016},{\"end\":44249,\"start\":44245},{\"end\":44257,\"start\":44256},{\"end\":44267,\"start\":44263},{\"end\":44274,\"start\":44273},{\"end\":44651,\"start\":44650},{\"end\":44659,\"start\":44658},{\"end\":44666,\"start\":44665},{\"end\":44674,\"start\":44673},{\"end\":44683,\"start\":44682},{\"end\":45081,\"start\":45080},{\"end\":45087,\"start\":45086},{\"end\":45099,\"start\":45098},{\"end\":45109,\"start\":45108},{\"end\":45441,\"start\":45440},{\"end\":45447,\"start\":45446},{\"end\":45456,\"start\":45455},{\"end\":45463,\"start\":45462},{\"end\":45889,\"start\":45888},{\"end\":45898,\"start\":45897},{\"end\":45905,\"start\":45904},{\"end\":45912,\"start\":45911},{\"end\":46363,\"start\":46362},{\"end\":46373,\"start\":46372},{\"end\":46384,\"start\":46383},{\"end\":46790,\"start\":46789},{\"end\":46796,\"start\":46795},{\"end\":46804,\"start\":46803},{\"end\":46812,\"start\":46811},{\"end\":46820,\"start\":46819},{\"end\":46828,\"start\":46827},{\"end\":46835,\"start\":46834},{\"end\":47271,\"start\":47270},{\"end\":47277,\"start\":47276},{\"end\":47285,\"start\":47284},{\"end\":47292,\"start\":47291},{\"end\":47302,\"start\":47298},{\"end\":47310,\"start\":47309},{\"end\":47749,\"start\":47748},{\"end\":47755,\"start\":47754},{\"end\":47763,\"start\":47762},{\"end\":47772,\"start\":47771},{\"end\":47778,\"start\":47777},{\"end\":47784,\"start\":47783},{\"end\":48232,\"start\":48231},{\"end\":48238,\"start\":48237},{\"end\":48246,\"start\":48245},{\"end\":48254,\"start\":48253},{\"end\":48263,\"start\":48262},{\"end\":48271,\"start\":48270},{\"end\":48734,\"start\":48733},{\"end\":48740,\"start\":48739},{\"end\":48749,\"start\":48748},{\"end\":48757,\"start\":48756},{\"end\":48765,\"start\":48764},{\"end\":48776,\"start\":48772},{\"end\":48783,\"start\":48782},{\"end\":49200,\"start\":49196},{\"end\":49207,\"start\":49206},{\"end\":49216,\"start\":49215},{\"end\":49228,\"start\":49227},{\"end\":49236,\"start\":49235},{\"end\":49246,\"start\":49245},{\"end\":49257,\"start\":49256},{\"end\":49267,\"start\":49266},{\"end\":49269,\"start\":49268},{\"end\":49616,\"start\":49615},{\"end\":49623,\"start\":49622},{\"end\":49630,\"start\":49629},{\"end\":49637,\"start\":49636},{\"end\":49643,\"start\":49642},{\"end\":49650,\"start\":49649},{\"end\":49659,\"start\":49658},{\"end\":49666,\"start\":49665},{\"end\":50118,\"start\":50117},{\"end\":50125,\"start\":50124},{\"end\":50133,\"start\":50132},{\"end\":50142,\"start\":50141},{\"end\":50150,\"start\":50149},{\"end\":50157,\"start\":50156},{\"end\":50630,\"start\":50629},{\"end\":50640,\"start\":50639},{\"end\":50649,\"start\":50648},{\"end\":50947,\"start\":50946},{\"end\":50957,\"start\":50956},{\"end\":50965,\"start\":50964},{\"end\":51220,\"start\":51219},{\"end\":51227,\"start\":51226},{\"end\":51235,\"start\":51234},{\"end\":51246,\"start\":51243},{\"end\":51250,\"start\":51249},{\"end\":51665,\"start\":51664},{\"end\":51679,\"start\":51678},{\"end\":51689,\"start\":51685},{\"end\":51697,\"start\":51696},{\"end\":51708,\"start\":51707},{\"end\":51719,\"start\":51718},{\"end\":52184,\"start\":52183},{\"end\":52198,\"start\":52197},{\"end\":52205,\"start\":52204},{\"end\":52217,\"start\":52216},{\"end\":52227,\"start\":52226},{\"end\":52238,\"start\":52237},{\"end\":52249,\"start\":52248},{\"end\":52738,\"start\":52737},{\"end\":52745,\"start\":52744},{\"end\":52754,\"start\":52753},{\"end\":52763,\"start\":52762},{\"end\":53224,\"start\":53223},{\"end\":53230,\"start\":53229},{\"end\":53236,\"start\":53235},{\"end\":53242,\"start\":53241},{\"end\":53250,\"start\":53249},{\"end\":53699,\"start\":53698},{\"end\":53706,\"start\":53705},{\"end\":53714,\"start\":53713},{\"end\":53721,\"start\":53720},{\"end\":54151,\"start\":54150},{\"end\":54158,\"start\":54157},{\"end\":54167,\"start\":54166},{\"end\":54176,\"start\":54175},{\"end\":54184,\"start\":54183},{\"end\":54190,\"start\":54189},{\"end\":54198,\"start\":54197},{\"end\":54210,\"start\":54209},{\"end\":54216,\"start\":54215},{\"end\":54224,\"start\":54223},{\"end\":54695,\"start\":54694},{\"end\":54702,\"start\":54701},{\"end\":54710,\"start\":54709},{\"end\":54717,\"start\":54716},{\"end\":54726,\"start\":54725},{\"end\":55076,\"start\":55075},{\"end\":55084,\"start\":55083},{\"end\":55092,\"start\":55091},{\"end\":55346,\"start\":55345},{\"end\":55353,\"start\":55352},{\"end\":55360,\"start\":55359},{\"end\":55366,\"start\":55365},{\"end\":55374,\"start\":55373},{\"end\":55631,\"start\":55630},{\"end\":55637,\"start\":55636},{\"end\":55649,\"start\":55648},{\"end\":55661,\"start\":55657},{\"end\":55667,\"start\":55666},{\"end\":55865,\"start\":55864},{\"end\":55873,\"start\":55872},{\"end\":55879,\"start\":55878},{\"end\":56219,\"start\":56218},{\"end\":56227,\"start\":56226},{\"end\":56229,\"start\":56228},{\"end\":56237,\"start\":56236},{\"end\":56243,\"start\":56242},{\"end\":56251,\"start\":56250},{\"end\":56260,\"start\":56259},{\"end\":56640,\"start\":56639},{\"end\":56648,\"start\":56647},{\"end\":56654,\"start\":56653},{\"end\":56656,\"start\":56655},{\"end\":56664,\"start\":56663},{\"end\":56671,\"start\":56670},{\"end\":56679,\"start\":56678},{\"end\":57092,\"start\":57091},{\"end\":57099,\"start\":57098},{\"end\":57108,\"start\":57107},{\"end\":57445,\"start\":57444},{\"end\":57451,\"start\":57450},{\"end\":57457,\"start\":57456},{\"end\":57464,\"start\":57463},{\"end\":57759,\"start\":57758},{\"end\":57768,\"start\":57767},{\"end\":57775,\"start\":57774},{\"end\":57782,\"start\":57781},{\"end\":57788,\"start\":57787},{\"end\":58212,\"start\":58211},{\"end\":58221,\"start\":58220},{\"end\":58229,\"start\":58228},{\"end\":58237,\"start\":58236},{\"end\":58239,\"start\":58238},{\"end\":58450,\"start\":58449},{\"end\":58458,\"start\":58457},{\"end\":58466,\"start\":58465},{\"end\":58701,\"start\":58700},{\"end\":58708,\"start\":58707},{\"end\":58714,\"start\":58713},{\"end\":58720,\"start\":58719},{\"end\":58726,\"start\":58725},{\"end\":58734,\"start\":58733}]", "bib_author_last_name": "[{\"end\":40480,\"start\":40475},{\"end\":40490,\"start\":40484},{\"end\":40504,\"start\":40494},{\"end\":40945,\"start\":40942},{\"end\":40954,\"start\":40949},{\"end\":40964,\"start\":40961},{\"end\":40974,\"start\":40968},{\"end\":41388,\"start\":41382},{\"end\":41397,\"start\":41392},{\"end\":41409,\"start\":41401},{\"end\":41420,\"start\":41413},{\"end\":41432,\"start\":41424},{\"end\":41445,\"start\":41436},{\"end\":41757,\"start\":41753},{\"end\":41765,\"start\":41761},{\"end\":41773,\"start\":41769},{\"end\":41782,\"start\":41777},{\"end\":41788,\"start\":41786},{\"end\":41795,\"start\":41792},{\"end\":42234,\"start\":42229},{\"end\":42243,\"start\":42238},{\"end\":42256,\"start\":42249},{\"end\":42268,\"start\":42260},{\"end\":42279,\"start\":42272},{\"end\":42749,\"start\":42744},{\"end\":42760,\"start\":42753},{\"end\":42772,\"start\":42764},{\"end\":43095,\"start\":43090},{\"end\":43103,\"start\":43099},{\"end\":43111,\"start\":43107},{\"end\":43118,\"start\":43115},{\"end\":43129,\"start\":43124},{\"end\":43138,\"start\":43133},{\"end\":43606,\"start\":43602},{\"end\":43614,\"start\":43610},{\"end\":43624,\"start\":43618},{\"end\":43633,\"start\":43631},{\"end\":43639,\"start\":43637},{\"end\":43650,\"start\":43643},{\"end\":43983,\"start\":43979},{\"end\":43991,\"start\":43987},{\"end\":43999,\"start\":43995},{\"end\":44007,\"start\":44003},{\"end\":44014,\"start\":44011},{\"end\":44021,\"start\":44018},{\"end\":44254,\"start\":44250},{\"end\":44261,\"start\":44258},{\"end\":44271,\"start\":44268},{\"end\":44277,\"start\":44275},{\"end\":44656,\"start\":44652},{\"end\":44663,\"start\":44660},{\"end\":44671,\"start\":44667},{\"end\":44680,\"start\":44675},{\"end\":44688,\"start\":44684},{\"end\":45084,\"start\":45082},{\"end\":45096,\"start\":45088},{\"end\":45106,\"start\":45100},{\"end\":45118,\"start\":45110},{\"end\":45444,\"start\":45442},{\"end\":45453,\"start\":45448},{\"end\":45460,\"start\":45457},{\"end\":45467,\"start\":45464},{\"end\":45895,\"start\":45890},{\"end\":45902,\"start\":45899},{\"end\":45909,\"start\":45906},{\"end\":45918,\"start\":45913},{\"end\":46370,\"start\":46364},{\"end\":46381,\"start\":46374},{\"end\":46390,\"start\":46385},{\"end\":46793,\"start\":46791},{\"end\":46801,\"start\":46797},{\"end\":46809,\"start\":46805},{\"end\":46817,\"start\":46813},{\"end\":46825,\"start\":46821},{\"end\":46832,\"start\":46829},{\"end\":46838,\"start\":46836},{\"end\":47274,\"start\":47272},{\"end\":47282,\"start\":47278},{\"end\":47289,\"start\":47286},{\"end\":47296,\"start\":47293},{\"end\":47307,\"start\":47303},{\"end\":47313,\"start\":47311},{\"end\":47752,\"start\":47750},{\"end\":47760,\"start\":47756},{\"end\":47769,\"start\":47764},{\"end\":47775,\"start\":47773},{\"end\":47781,\"start\":47779},{\"end\":47787,\"start\":47785},{\"end\":48235,\"start\":48233},{\"end\":48243,\"start\":48239},{\"end\":48251,\"start\":48247},{\"end\":48260,\"start\":48255},{\"end\":48268,\"start\":48264},{\"end\":48276,\"start\":48272},{\"end\":48737,\"start\":48735},{\"end\":48746,\"start\":48741},{\"end\":48754,\"start\":48750},{\"end\":48762,\"start\":48758},{\"end\":48770,\"start\":48766},{\"end\":48780,\"start\":48777},{\"end\":48788,\"start\":48784},{\"end\":49204,\"start\":49201},{\"end\":49213,\"start\":49208},{\"end\":49225,\"start\":49217},{\"end\":49233,\"start\":49229},{\"end\":49243,\"start\":49237},{\"end\":49254,\"start\":49247},{\"end\":49264,\"start\":49258},{\"end\":49277,\"start\":49270},{\"end\":49620,\"start\":49617},{\"end\":49627,\"start\":49624},{\"end\":49634,\"start\":49631},{\"end\":49640,\"start\":49638},{\"end\":49647,\"start\":49644},{\"end\":49656,\"start\":49651},{\"end\":49663,\"start\":49660},{\"end\":49670,\"start\":49667},{\"end\":50122,\"start\":50119},{\"end\":50130,\"start\":50126},{\"end\":50139,\"start\":50134},{\"end\":50147,\"start\":50143},{\"end\":50154,\"start\":50151},{\"end\":50162,\"start\":50158},{\"end\":50637,\"start\":50631},{\"end\":50646,\"start\":50641},{\"end\":50654,\"start\":50650},{\"end\":50954,\"start\":50948},{\"end\":50962,\"start\":50958},{\"end\":50970,\"start\":50966},{\"end\":51224,\"start\":51221},{\"end\":51232,\"start\":51228},{\"end\":51241,\"start\":51236},{\"end\":51676,\"start\":51666},{\"end\":51683,\"start\":51680},{\"end\":51694,\"start\":51690},{\"end\":51705,\"start\":51698},{\"end\":51716,\"start\":51709},{\"end\":51726,\"start\":51720},{\"end\":52195,\"start\":52185},{\"end\":52202,\"start\":52199},{\"end\":52214,\"start\":52206},{\"end\":52224,\"start\":52218},{\"end\":52235,\"start\":52228},{\"end\":52246,\"start\":52239},{\"end\":52256,\"start\":52250},{\"end\":52742,\"start\":52739},{\"end\":52751,\"start\":52746},{\"end\":52760,\"start\":52755},{\"end\":52766,\"start\":52764},{\"end\":53227,\"start\":53225},{\"end\":53233,\"start\":53231},{\"end\":53239,\"start\":53237},{\"end\":53247,\"start\":53243},{\"end\":53255,\"start\":53251},{\"end\":53703,\"start\":53700},{\"end\":53711,\"start\":53707},{\"end\":53718,\"start\":53715},{\"end\":53726,\"start\":53722},{\"end\":54155,\"start\":54152},{\"end\":54164,\"start\":54159},{\"end\":54173,\"start\":54168},{\"end\":54181,\"start\":54177},{\"end\":54187,\"start\":54185},{\"end\":54195,\"start\":54191},{\"end\":54207,\"start\":54199},{\"end\":54213,\"start\":54211},{\"end\":54221,\"start\":54217},{\"end\":54229,\"start\":54225},{\"end\":54699,\"start\":54696},{\"end\":54707,\"start\":54703},{\"end\":54714,\"start\":54711},{\"end\":54723,\"start\":54718},{\"end\":54730,\"start\":54727},{\"end\":55081,\"start\":55077},{\"end\":55089,\"start\":55085},{\"end\":55097,\"start\":55093},{\"end\":55350,\"start\":55347},{\"end\":55357,\"start\":55354},{\"end\":55363,\"start\":55361},{\"end\":55371,\"start\":55367},{\"end\":55378,\"start\":55375},{\"end\":55634,\"start\":55632},{\"end\":55646,\"start\":55638},{\"end\":55655,\"start\":55650},{\"end\":55664,\"start\":55662},{\"end\":55676,\"start\":55668},{\"end\":55870,\"start\":55866},{\"end\":55876,\"start\":55874},{\"end\":55883,\"start\":55880},{\"end\":56224,\"start\":56220},{\"end\":56234,\"start\":56230},{\"end\":56240,\"start\":56238},{\"end\":56248,\"start\":56244},{\"end\":56257,\"start\":56252},{\"end\":56268,\"start\":56261},{\"end\":56645,\"start\":56641},{\"end\":56651,\"start\":56649},{\"end\":56661,\"start\":56657},{\"end\":56668,\"start\":56665},{\"end\":56676,\"start\":56672},{\"end\":56685,\"start\":56680},{\"end\":57096,\"start\":57093},{\"end\":57105,\"start\":57100},{\"end\":57112,\"start\":57109},{\"end\":57448,\"start\":57446},{\"end\":57454,\"start\":57452},{\"end\":57461,\"start\":57458},{\"end\":57469,\"start\":57465},{\"end\":57765,\"start\":57760},{\"end\":57772,\"start\":57769},{\"end\":57779,\"start\":57776},{\"end\":57785,\"start\":57783},{\"end\":57792,\"start\":57789},{\"end\":58218,\"start\":58213},{\"end\":58226,\"start\":58222},{\"end\":58234,\"start\":58230},{\"end\":58243,\"start\":58240},{\"end\":58455,\"start\":58451},{\"end\":58463,\"start\":58459},{\"end\":58477,\"start\":58467},{\"end\":58705,\"start\":58702},{\"end\":58711,\"start\":58709},{\"end\":58717,\"start\":58715},{\"end\":58723,\"start\":58721},{\"end\":58731,\"start\":58727},{\"end\":58738,\"start\":58735}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":238583209},\"end\":40869,\"start\":40375},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":16224674},\"end\":41331,\"start\":40871},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":218889832},\"end\":41691,\"start\":41333},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4703058},\"end\":42157,\"start\":41693},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":244799297},\"end\":42668,\"start\":42159},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":235829267},\"end\":43000,\"start\":42670},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":212675120},\"end\":43545,\"start\":43002},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":57246310},\"end\":43929,\"start\":43547},{\"attributes\":{\"doi\":\"arXiv:2104.13586\",\"id\":\"b8\"},\"end\":44198,\"start\":43931},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6529517},\"end\":44578,\"start\":44200},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":233033921},\"end\":45066,\"start\":44580},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":54465873},\"end\":45392,\"start\":45068},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206594692},\"end\":45792,\"start\":45394},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":208138553},\"end\":46308,\"start\":45794},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":80628362},\"end\":46724,\"start\":46310},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":236318539},\"end\":47195,\"start\":46726},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":54443684},\"end\":47702,\"start\":47197},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":233231207},\"end\":48141,\"start\":47704},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":139101198},\"end\":48668,\"start\":48143},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":233181621},\"end\":49151,\"start\":48670},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14113767},\"end\":49540,\"start\":49153},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":232352874},\"end\":50044,\"start\":49542},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":229923779},\"end\":50550,\"start\":50046},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":340420},\"end\":50890,\"start\":50552},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":13613792},\"end\":51176,\"start\":50892},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":201668824},\"end\":51545,\"start\":51178},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":4031629},\"end\":52122,\"start\":51547},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":5267028},\"end\":52648,\"start\":52124},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":195440283},\"end\":53140,\"start\":52650},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":148571756},\"end\":53624,\"start\":53142},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":67856425},\"end\":54080,\"start\":53626},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":227162412},\"end\":54660,\"start\":54082},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":4055834},\"end\":55013,\"start\":54662},{\"attributes\":{\"doi\":\"arXiv:1911.07451\",\"id\":\"b33\"},\"end\":55260,\"start\":55015},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":220364346},\"end\":55626,\"start\":55262},{\"attributes\":{\"id\":\"b35\"},\"end\":55805,\"start\":55628},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":4934594},\"end\":56170,\"start\":55807},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":245501898},\"end\":56565,\"start\":56172},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":245668699},\"end\":57004,\"start\":56567},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":19167105},\"end\":57355,\"start\":57006},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":59336928},\"end\":57684,\"start\":57357},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":204509549},\"end\":58166,\"start\":57686},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":235658076},\"end\":58447,\"start\":58168},{\"attributes\":{\"doi\":\"arXiv:1904.07850\",\"id\":\"b43\"},\"end\":58624,\"start\":58449},{\"attributes\":{\"doi\":\"arXiv:2010.04159\",\"id\":\"b44\"},\"end\":58937,\"start\":58626}]", "bib_title": "[{\"end\":40471,\"start\":40375},{\"end\":40938,\"start\":40871},{\"end\":41378,\"start\":41333},{\"end\":41749,\"start\":41693},{\"end\":42225,\"start\":42159},{\"end\":42740,\"start\":42670},{\"end\":43086,\"start\":43002},{\"end\":43598,\"start\":43547},{\"end\":44243,\"start\":44200},{\"end\":44648,\"start\":44580},{\"end\":45078,\"start\":45068},{\"end\":45438,\"start\":45394},{\"end\":45886,\"start\":45794},{\"end\":46360,\"start\":46310},{\"end\":46787,\"start\":46726},{\"end\":47268,\"start\":47197},{\"end\":47746,\"start\":47704},{\"end\":48229,\"start\":48143},{\"end\":48731,\"start\":48670},{\"end\":49194,\"start\":49153},{\"end\":49613,\"start\":49542},{\"end\":50115,\"start\":50046},{\"end\":50627,\"start\":50552},{\"end\":50944,\"start\":50892},{\"end\":51217,\"start\":51178},{\"end\":51662,\"start\":51547},{\"end\":52181,\"start\":52124},{\"end\":52735,\"start\":52650},{\"end\":53221,\"start\":53142},{\"end\":53696,\"start\":53626},{\"end\":54148,\"start\":54082},{\"end\":54692,\"start\":54662},{\"end\":55343,\"start\":55262},{\"end\":55862,\"start\":55807},{\"end\":56216,\"start\":56172},{\"end\":56637,\"start\":56567},{\"end\":57089,\"start\":57006},{\"end\":57442,\"start\":57357},{\"end\":57756,\"start\":57686},{\"end\":58209,\"start\":58168}]", "bib_author": "[{\"end\":40482,\"start\":40473},{\"end\":40492,\"start\":40482},{\"end\":40506,\"start\":40492},{\"end\":40947,\"start\":40940},{\"end\":40956,\"start\":40947},{\"end\":40966,\"start\":40956},{\"end\":40976,\"start\":40966},{\"end\":41390,\"start\":41380},{\"end\":41399,\"start\":41390},{\"end\":41411,\"start\":41399},{\"end\":41422,\"start\":41411},{\"end\":41434,\"start\":41422},{\"end\":41447,\"start\":41434},{\"end\":41759,\"start\":41751},{\"end\":41767,\"start\":41759},{\"end\":41775,\"start\":41767},{\"end\":41784,\"start\":41775},{\"end\":41790,\"start\":41784},{\"end\":41797,\"start\":41790},{\"end\":42236,\"start\":42227},{\"end\":42245,\"start\":42236},{\"end\":42258,\"start\":42245},{\"end\":42270,\"start\":42258},{\"end\":42281,\"start\":42270},{\"end\":42751,\"start\":42742},{\"end\":42762,\"start\":42751},{\"end\":42774,\"start\":42762},{\"end\":43097,\"start\":43088},{\"end\":43105,\"start\":43097},{\"end\":43113,\"start\":43105},{\"end\":43120,\"start\":43113},{\"end\":43131,\"start\":43120},{\"end\":43140,\"start\":43131},{\"end\":43608,\"start\":43600},{\"end\":43616,\"start\":43608},{\"end\":43626,\"start\":43616},{\"end\":43635,\"start\":43626},{\"end\":43641,\"start\":43635},{\"end\":43652,\"start\":43641},{\"end\":43985,\"start\":43977},{\"end\":43993,\"start\":43985},{\"end\":44001,\"start\":43993},{\"end\":44009,\"start\":44001},{\"end\":44016,\"start\":44009},{\"end\":44023,\"start\":44016},{\"end\":44256,\"start\":44245},{\"end\":44263,\"start\":44256},{\"end\":44273,\"start\":44263},{\"end\":44279,\"start\":44273},{\"end\":44658,\"start\":44650},{\"end\":44665,\"start\":44658},{\"end\":44673,\"start\":44665},{\"end\":44682,\"start\":44673},{\"end\":44690,\"start\":44682},{\"end\":45086,\"start\":45080},{\"end\":45098,\"start\":45086},{\"end\":45108,\"start\":45098},{\"end\":45120,\"start\":45108},{\"end\":45446,\"start\":45440},{\"end\":45455,\"start\":45446},{\"end\":45462,\"start\":45455},{\"end\":45469,\"start\":45462},{\"end\":45897,\"start\":45888},{\"end\":45904,\"start\":45897},{\"end\":45911,\"start\":45904},{\"end\":45920,\"start\":45911},{\"end\":46372,\"start\":46362},{\"end\":46383,\"start\":46372},{\"end\":46392,\"start\":46383},{\"end\":46795,\"start\":46789},{\"end\":46803,\"start\":46795},{\"end\":46811,\"start\":46803},{\"end\":46819,\"start\":46811},{\"end\":46827,\"start\":46819},{\"end\":46834,\"start\":46827},{\"end\":46840,\"start\":46834},{\"end\":47276,\"start\":47270},{\"end\":47284,\"start\":47276},{\"end\":47291,\"start\":47284},{\"end\":47298,\"start\":47291},{\"end\":47309,\"start\":47298},{\"end\":47315,\"start\":47309},{\"end\":47754,\"start\":47748},{\"end\":47762,\"start\":47754},{\"end\":47771,\"start\":47762},{\"end\":47777,\"start\":47771},{\"end\":47783,\"start\":47777},{\"end\":47789,\"start\":47783},{\"end\":48237,\"start\":48231},{\"end\":48245,\"start\":48237},{\"end\":48253,\"start\":48245},{\"end\":48262,\"start\":48253},{\"end\":48270,\"start\":48262},{\"end\":48278,\"start\":48270},{\"end\":48739,\"start\":48733},{\"end\":48748,\"start\":48739},{\"end\":48756,\"start\":48748},{\"end\":48764,\"start\":48756},{\"end\":48772,\"start\":48764},{\"end\":48782,\"start\":48772},{\"end\":48790,\"start\":48782},{\"end\":49206,\"start\":49196},{\"end\":49215,\"start\":49206},{\"end\":49227,\"start\":49215},{\"end\":49235,\"start\":49227},{\"end\":49245,\"start\":49235},{\"end\":49256,\"start\":49245},{\"end\":49266,\"start\":49256},{\"end\":49279,\"start\":49266},{\"end\":49622,\"start\":49615},{\"end\":49629,\"start\":49622},{\"end\":49636,\"start\":49629},{\"end\":49642,\"start\":49636},{\"end\":49649,\"start\":49642},{\"end\":49658,\"start\":49649},{\"end\":49665,\"start\":49658},{\"end\":49672,\"start\":49665},{\"end\":50124,\"start\":50117},{\"end\":50132,\"start\":50124},{\"end\":50141,\"start\":50132},{\"end\":50149,\"start\":50141},{\"end\":50156,\"start\":50149},{\"end\":50164,\"start\":50156},{\"end\":50639,\"start\":50629},{\"end\":50648,\"start\":50639},{\"end\":50656,\"start\":50648},{\"end\":50956,\"start\":50946},{\"end\":50964,\"start\":50956},{\"end\":50972,\"start\":50964},{\"end\":51226,\"start\":51219},{\"end\":51234,\"start\":51226},{\"end\":51243,\"start\":51234},{\"end\":51249,\"start\":51243},{\"end\":51253,\"start\":51249},{\"end\":51678,\"start\":51664},{\"end\":51685,\"start\":51678},{\"end\":51696,\"start\":51685},{\"end\":51707,\"start\":51696},{\"end\":51718,\"start\":51707},{\"end\":51728,\"start\":51718},{\"end\":52197,\"start\":52183},{\"end\":52204,\"start\":52197},{\"end\":52216,\"start\":52204},{\"end\":52226,\"start\":52216},{\"end\":52237,\"start\":52226},{\"end\":52248,\"start\":52237},{\"end\":52258,\"start\":52248},{\"end\":52744,\"start\":52737},{\"end\":52753,\"start\":52744},{\"end\":52762,\"start\":52753},{\"end\":52768,\"start\":52762},{\"end\":53229,\"start\":53223},{\"end\":53235,\"start\":53229},{\"end\":53241,\"start\":53235},{\"end\":53249,\"start\":53241},{\"end\":53257,\"start\":53249},{\"end\":53705,\"start\":53698},{\"end\":53713,\"start\":53705},{\"end\":53720,\"start\":53713},{\"end\":53728,\"start\":53720},{\"end\":54157,\"start\":54150},{\"end\":54166,\"start\":54157},{\"end\":54175,\"start\":54166},{\"end\":54183,\"start\":54175},{\"end\":54189,\"start\":54183},{\"end\":54197,\"start\":54189},{\"end\":54209,\"start\":54197},{\"end\":54215,\"start\":54209},{\"end\":54223,\"start\":54215},{\"end\":54231,\"start\":54223},{\"end\":54701,\"start\":54694},{\"end\":54709,\"start\":54701},{\"end\":54716,\"start\":54709},{\"end\":54725,\"start\":54716},{\"end\":54732,\"start\":54725},{\"end\":55083,\"start\":55075},{\"end\":55091,\"start\":55083},{\"end\":55099,\"start\":55091},{\"end\":55352,\"start\":55345},{\"end\":55359,\"start\":55352},{\"end\":55365,\"start\":55359},{\"end\":55373,\"start\":55365},{\"end\":55380,\"start\":55373},{\"end\":55636,\"start\":55630},{\"end\":55648,\"start\":55636},{\"end\":55657,\"start\":55648},{\"end\":55666,\"start\":55657},{\"end\":55678,\"start\":55666},{\"end\":55872,\"start\":55864},{\"end\":55878,\"start\":55872},{\"end\":55885,\"start\":55878},{\"end\":56226,\"start\":56218},{\"end\":56236,\"start\":56226},{\"end\":56242,\"start\":56236},{\"end\":56250,\"start\":56242},{\"end\":56259,\"start\":56250},{\"end\":56270,\"start\":56259},{\"end\":56647,\"start\":56639},{\"end\":56653,\"start\":56647},{\"end\":56663,\"start\":56653},{\"end\":56670,\"start\":56663},{\"end\":56678,\"start\":56670},{\"end\":56687,\"start\":56678},{\"end\":57098,\"start\":57091},{\"end\":57107,\"start\":57098},{\"end\":57114,\"start\":57107},{\"end\":57450,\"start\":57444},{\"end\":57456,\"start\":57450},{\"end\":57463,\"start\":57456},{\"end\":57471,\"start\":57463},{\"end\":57767,\"start\":57758},{\"end\":57774,\"start\":57767},{\"end\":57781,\"start\":57774},{\"end\":57787,\"start\":57781},{\"end\":57794,\"start\":57787},{\"end\":58220,\"start\":58211},{\"end\":58228,\"start\":58220},{\"end\":58236,\"start\":58228},{\"end\":58245,\"start\":58236},{\"end\":58457,\"start\":58449},{\"end\":58465,\"start\":58457},{\"end\":58479,\"start\":58465},{\"end\":58707,\"start\":58700},{\"end\":58713,\"start\":58707},{\"end\":58719,\"start\":58713},{\"end\":58725,\"start\":58719},{\"end\":58733,\"start\":58725},{\"end\":58740,\"start\":58733}]", "bib_venue": "[{\"end\":40577,\"start\":40506},{\"end\":41053,\"start\":40976},{\"end\":41485,\"start\":41447},{\"end\":41874,\"start\":41797},{\"end\":42362,\"start\":42281},{\"end\":42823,\"start\":42774},{\"end\":43221,\"start\":43140},{\"end\":43715,\"start\":43652},{\"end\":43975,\"start\":43931},{\"end\":44346,\"start\":44279},{\"end\":44771,\"start\":44690},{\"end\":45187,\"start\":45120},{\"end\":45546,\"start\":45469},{\"end\":46001,\"start\":45920},{\"end\":46469,\"start\":46392},{\"end\":46911,\"start\":46840},{\"end\":47396,\"start\":47315},{\"end\":47870,\"start\":47789},{\"end\":48355,\"start\":48278},{\"end\":48861,\"start\":48790},{\"end\":49317,\"start\":49279},{\"end\":49743,\"start\":49672},{\"end\":50245,\"start\":50164},{\"end\":50705,\"start\":50656},{\"end\":51010,\"start\":50972},{\"end\":51320,\"start\":51253},{\"end\":51792,\"start\":51728},{\"end\":52335,\"start\":52258},{\"end\":52845,\"start\":52768},{\"end\":53334,\"start\":53257},{\"end\":53805,\"start\":53728},{\"end\":54312,\"start\":54231},{\"end\":54796,\"start\":54732},{\"end\":55073,\"start\":55015},{\"end\":55418,\"start\":55380},{\"end\":55949,\"start\":55885},{\"end\":56331,\"start\":56270},{\"end\":56748,\"start\":56687},{\"end\":57170,\"start\":57114},{\"end\":57509,\"start\":57471},{\"end\":57875,\"start\":57794},{\"end\":58294,\"start\":58245},{\"end\":58512,\"start\":58495},{\"end\":58698,\"start\":58626},{\"end\":40635,\"start\":40579},{\"end\":41117,\"start\":41055},{\"end\":41938,\"start\":41876},{\"end\":42430,\"start\":42364},{\"end\":43289,\"start\":43223},{\"end\":44400,\"start\":44348},{\"end\":44839,\"start\":44773},{\"end\":45241,\"start\":45189},{\"end\":45610,\"start\":45548},{\"end\":46069,\"start\":46003},{\"end\":46533,\"start\":46471},{\"end\":46969,\"start\":46913},{\"end\":47464,\"start\":47398},{\"end\":47938,\"start\":47872},{\"end\":48419,\"start\":48357},{\"end\":48919,\"start\":48863},{\"end\":49801,\"start\":49745},{\"end\":50313,\"start\":50247},{\"end\":51374,\"start\":51322},{\"end\":51843,\"start\":51794},{\"end\":52399,\"start\":52337},{\"end\":52909,\"start\":52847},{\"end\":53398,\"start\":53336},{\"end\":53869,\"start\":53807},{\"end\":54380,\"start\":54314},{\"end\":54847,\"start\":54798},{\"end\":56000,\"start\":55951},{\"end\":56379,\"start\":56333},{\"end\":56796,\"start\":56750},{\"end\":57943,\"start\":57877}]"}}}, "year": 2023, "month": 12, "day": 17}