{"id": 202773833, "updated": "2023-10-06 22:24:26.939", "metadata": {"title": "Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with Dirichlet calibration", "authors": "[{\"first\":\"Meelis\",\"last\":\"Kull\",\"middle\":[]},{\"first\":\"Miquel\",\"last\":\"Perello-Nieto\",\"middle\":[]},{\"first\":\"Markus\",\"last\":\"Kangsepp\",\"middle\":[]},{\"first\":\"Telmo\",\"last\":\"Filho\",\"middle\":[\"Silva\"]},{\"first\":\"Hao\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Peter\",\"last\":\"Flach\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": 10, "day": 28}, "abstract": "Class probabilities predicted by most multiclass classifiers are uncalibrated, often tending towards over-confidence. With neural networks, calibration can be improved by temperature scaling, a method to learn a single corrective multiplicative factor for inputs to the last softmax layer. On non-neural models the existing methods apply binary calibration in a pairwise or one-vs-rest fashion. We propose a natively multiclass calibration method applicable to classifiers from any model class, derived from Dirichlet distributions and generalising the beta calibration method from binary classification. It is easily implemented with neural nets since it is equivalent to log-transforming the uncalibrated probabilities, followed by one linear layer and softmax. Experiments demonstrate improved probabilistic predictions according to multiple measures (confidence-ECE, classwise-ECE, log-loss, Brier score) across a wide range of datasets and classifiers. Parameters of the learned Dirichlet calibration map provide insights to the biases in the uncalibrated model.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1910.12656", "mag": "2981528437", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/KullPKFSF19", "doi": null}}, "content": {"source": {"pdf_hash": "50ae5574c61fedd2c0ea2000fa26f5861bc54490", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1910.12656v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4fff7cbcea60db8495ddec7b20446495709e6d75", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/50ae5574c61fedd2c0ea2000fa26f5861bc54490.txt", "contents": "\nBeyond temperature scaling: Obtaining well-calibrated multiclass probabilities with Dirichlet calibration\n\n\nMeelis Kull meelis.kull@ut.ee \nDepartment of Computer Science\nDepartment of Computer Science\nDepartment of Computer Science\nDepartment of Statistics\nDepartment of Computer Science\nDepartment of Computer Science\nUniversity of Tartu\nUniversity of Bristol\nUniversity of Tartu\nUniversidade Federal da Para\u00edba\nUniversity of Bristol\nUniversity of Bristol\nThe Alan Turing Institute\n\n\nMiquel Perello-Nieto miquel.perellonieto@bris.ac.uk \nDepartment of Computer Science\nDepartment of Computer Science\nDepartment of Computer Science\nDepartment of Statistics\nDepartment of Computer Science\nDepartment of Computer Science\nUniversity of Tartu\nUniversity of Bristol\nUniversity of Tartu\nUniversidade Federal da Para\u00edba\nUniversity of Bristol\nUniversity of Bristol\nThe Alan Turing Institute\n\n\nMarkus K\u00e4ngsepp markus.kangsepp@ut.ee \nDepartment of Computer Science\nDepartment of Computer Science\nDepartment of Computer Science\nDepartment of Statistics\nDepartment of Computer Science\nDepartment of Computer Science\nUniversity of Tartu\nUniversity of Bristol\nUniversity of Tartu\nUniversidade Federal da Para\u00edba\nUniversity of Bristol\nUniversity of Bristol\nThe Alan Turing Institute\n\n\nTelmo Silva Filho \nDepartment of Computer Science\nDepartment of Computer Science\nDepartment of Computer Science\nDepartment of Statistics\nDepartment of Computer Science\nDepartment of Computer Science\nUniversity of Tartu\nUniversity of Bristol\nUniversity of Tartu\nUniversidade Federal da Para\u00edba\nUniversity of Bristol\nUniversity of Bristol\nThe Alan Turing Institute\n\n\nHao Song hao.song@bristol.ac.uk \nDepartment of Computer Science\nDepartment of Computer Science\nDepartment of Computer Science\nDepartment of Statistics\nDepartment of Computer Science\nDepartment of Computer Science\nUniversity of Tartu\nUniversity of Bristol\nUniversity of Tartu\nUniversidade Federal da Para\u00edba\nUniversity of Bristol\nUniversity of Bristol\nThe Alan Turing Institute\n\n\nPeter Flach peter.flach@bristol.ac.uk \nDepartment of Computer Science\nDepartment of Computer Science\nDepartment of Computer Science\nDepartment of Statistics\nDepartment of Computer Science\nDepartment of Computer Science\nUniversity of Tartu\nUniversity of Bristol\nUniversity of Tartu\nUniversidade Federal da Para\u00edba\nUniversity of Bristol\nUniversity of Bristol\nThe Alan Turing Institute\n\n\nBeyond temperature scaling: Obtaining well-calibrated multiclass probabilities with Dirichlet calibration\n\nClass probabilities predicted by most multiclass classifiers are uncalibrated, often tending towards over-confidence. With neural networks, calibration can be improved by temperature scaling, a method to learn a single corrective multiplicative factor for inputs to the last softmax layer. On non-neural models the existing methods apply binary calibration in a pairwise or one-vs-rest fashion. We propose a natively multiclass calibration method applicable to classifiers from any model class, derived from Dirichlet distributions and generalising the beta calibration method from binary classification. It is easily implemented with neural nets since it is equivalent to log-transforming the uncalibrated probabilities, followed by one linear layer and softmax. Experiments demonstrate improved probabilistic predictions according to multiple measures (confidence-ECE, classwise-ECE, log-loss, Brier score) across a wide range of datasets and classifiers. Parameters of the learned Dirichlet calibration map provide insights to the biases in the uncalibrated model.\n\nto visualise calibration (reliability) errors; and [6] discussing proper scoring rules for forecaster evaluation and the decomposition of these loss measures into calibration and refinement losses. Calibration methods for binary classifiers have been well studied and include: logistic calibration, also known as 'Platt scaling' [24]; binning calibration [26] with either equal-width or equal-frequency bins; isotonic calibration [27]; and beta calibration [15]. Extensions of the above approaches include: [22] which performs Bayesian averaging of multiple calibration maps obtained with equal-frequency binning; [23] which uses near-isotonic regression to allow for some non-monotonic segments in the calibration maps; and [1] which introduces a non-parametric Bayesian isotonic calibration method.\n\nCalibration in multiclass scenarios has been approached by decomposing the problem into k onevs-rest binary calibration tasks [27], one for each class. The predictions of these k calibration models form unnormalised probability vectors, which, after normalisation, are not guaranteed to be calibrated. Native multiclass calibration methods were introduced recently with a focus on neural networks, including: matrix scaling, vector scaling and temperature scaling [9], which can all be seen as multiclass extensions of Platt scaling and have been proposed as calibration layers which should be applied to the logits of a neural network, replacing the softmax layer. An alternative to post-hoc calibration is to modify the classifier learning algorithm itself: MMCE [17] trains neural networks by optimising the combination of log-loss with a kernel-based measure of calibration loss; SWAG [19] models the posterior distribution over the weights of the neural network and then samples from this distribution to perform Bayesian model averaging; [20] proposed a method to transform the classification task into regression and to learn a Gaussian Process model. Calibration methods have been proposed for the regression task as well, including a method by [13] which adopts isotonic regression to calibrate the predicted quantiles. The theory of calibration functions and empirical calibration evaluation in classification was studied by [25], also proposing a statistical test of calibration.\n\nWhile there are several calibration methods tailored for deep neural networks, we propose a generalpurpose, natively multiclass calibration method called Dirichlet calibration, applicable for calibrating any probabilistic classifier. We also demonstrate that the multiclass setting introduces numerous subtleties that have not always been recognised or correctly dealt with by other authors. For example, some authors use the weaker notion of confidence calibration (our term), which requires only that the classifier's predicted probability for what it considers the most likely class is calibrated. There are also variations in the evaluation metric used and in the way calibrated probabilities are visualised. Consequently, Section 2 is concerned with clarifying such fundamental issues. We then propose the approach of Dirichlet calibration in Section 3, present and discuss experimental results in Section 4, and conclude in Section 5.\n\n\nEvaluation of calibration and temperature scaling\n\nConsider a probabilistic classifierp : X \u2192 \u2206 k that outputs class probabilities for k classes 1, . . . , k. For any given instance x in the feature space X it would output some probability vectorp(x) = (p 1 (x), . . . ,p k (x)) belonging to \u2206 k = {(q 1 , . . . , q k ) \u2208 [0, 1] k | \u2211 k i=1 q i = 1} which is the (k \u2212 1)-dimensional probability simplex over k classes. Definition 1. A probabilistic classifierp : X \u2192 \u2206 k is multiclass-calibrated, or simply calibrated, if for any prediction vector q = (q 1 , . . . , q k ) \u2208 \u2206 k , the proportions of classes among all possible instances x getting the same predictionp(x) = q are equal to the prediction vector q:\nP(Y = i |p(X) = q) = q i for i = 1, . . . , k.(1)\nOne can define several weaker notions of calibration [25] which provide necessary conditions for the model to be fully calibrated. One of these weaker notions was originally proposed by [27], requiring that all one-vs-rest probability estimators obtained from the original multiclass model are calibrated. Definition 2. A probabilistic classifierp : X \u2192 \u2206 k is classwise-calibrated, if for any class i and any predicted probability q i for this class:\nP(Y = i |p i (X) = q i ) = q i .(2)\nAnother weaker notion of calibration was used by [9], requiring that among all instances where the probability of the most likely class is predicted to be c (the confidence), the expected accuracy is c.  Figure 1: Reliability diagrams of c10_resnet_wide32 on CIFAR-10: (a) confidence-reliability before calibration; (b) confidence-reliability after temperature scaling; (c) classwise-reliability for class 2 after temperature scaling; (d) classwise-reliability for class 2 after Dirichlet calibration.\n\nDefinition 3. A probabilistic classifierp : X \u2192 \u2206 k is confidence-calibrated, if for any c \u2208 [0, 1]:\nP Y = argmax p(X) max p(X) = c = c.(3)\nFor practical evaluation purposes these idealistic definitions need to be relaxed. A common approach for checking confidence-calibration is to do equal-width binning of predictions according to confidence level and check if Eq. (3) is approximately satisfied within each bin. This can be visualised using the reliability diagram (which we will call the confidence-reliability diagram), see With accuracy below the average confidence in most bins, this figure about a wide ResNet trained on CIFAR-10 shows over-confidence, typical for neural networks which predict probabilities through the last softmax layer and are trained by minimising cross-entropy.\n\nThe calibration method called temperature scaling was proposed by [9] and it uses a hold-out validation set to learn a single temperature-parameter t > 0 which decreases confidence (if t > 1) or increases confidence (if t < 1). This is achieved by rescaling the logit vector z (input to softmax \u03c3 ), so that instead of \u03c3 (z) the predicted class probabilities will be obtained by \u03c3 (z/t). The confidencereliability diagram in Fig. 1b shows that the same c10_resnet_wide32 model has come closer to being confidence-calibrated after temperature scaling, having smaller gaps to the accuracy-equalsconfidence diagonal. This is reflected in a lower Expected Calibration Error (confidence-ECE), defined as the average gap across bins, weighted by the number of instances in the bin. In fact, confidence-ECE is low enough that the statistical test proposed by [25] with significance level \u03b1 = 0.01 does not reject the hypothesis that the model is confidence-calibrated (p-value 0.017). The main idea behind this test is that for a perfectly calibrated model, ECE against actual labels is in expectation equal to the ECE against pseudo-labels which have been drawn from the categorical distributions corresponding to the predicted class probability vectors. The above p-value was obtained by randomly drawing 10,000 sets of pseudo-labels and finding 170 of these to have higher ECE than the actual one.\n\nWhile the above temperature-scaled model is (nearly) confidence-calibrated, it is far from being classwise-calibrated. This becomes evident in Fig 1c, demonstrating that it systematically overestimates the probability of instances to belong to class 2, with predicted probability (x-axis) smaller than the observed frequency of class 2 (y-axis) in all the equal-width bins. In contrast, the model systematically under-estimates class 4 probability ( Supplementary Fig. 12a). Having only a single tuneable parameter, temperature scaling cannot learn to act differently on different classes. We propose plots such as Fig. 1c,d across all classes to be used for evaluating classwise-calibration, and we will call these the classwise-reliability diagrams. We propose classwise-ECE as a measure of classwise-calibration, defined as the average gap across all classwise-reliability diagrams, weighted by the number of instances in each bin:\nclasswise\u2212ECE = 1 k k \u2211 j=1 m \u2211 i=1 |B i, j | n |y j (B i, j ) \u2212p j (B i, j )|(4)\nwhere k, m, n are the numbers of classes, bins and instances, respectively, |B i, j | denotes the size of the bin, andp j (B i, j ) and y j (B i, j ) denote the average prediction of class j probability and the actual proportion of class j in the bin B i, j . The contribution of a single class j to the classwise-ECE will be called classj-ECE. As seen in Fig. 1(d), the same model gets closer to being class-2-calibrated after applying our proposed Dirichlet calibration. By averaging class-j-ECE across all classes we get the overall classwise-ECE which for temperature scaling is cwECE = 0.1857 and for Dirichlet calibration cwECE = 0.1795. This small difference in classwise-ECE appears more substantial when running the statistical test of [25], rejecting the null hypothesis that temperature scaling is classwisecalibrated (p < 0.0001), while for Dirichlet calibration the decision depends on the significance level (p = 0.016). A similar measure of classwise-calibration called L 2 marginal calibration error was proposed in a concurrent work by [16].\n\nBefore explaining the Dirichlet calibration method, let us highlight the fundamental limitation of evaluation using any of the above reliability diagrams and ECE measures. Namely, it is easy to obtain almost perfectly calibrated probabilities by predicting the overall class distribution, regardless of the given instance. Therefore, it is always important to consider other evaluation measures as well. In addition to the error rate, the obvious candidates are proper losses (such as Brier score or log-loss), as they evaluate probabilistic predictions and decompose into calibration loss and refinement loss [14]. Proper losses are often used as objective functions in post-hoc calibration methods, which take an uncalibrated probabilistic classifierp and use a hold-out validation dataset to learn a calibration map\u03bc : \u2206 k \u2192 \u2206 k that can be applied as\u03bc(p(x)) on top of the uncalibrated outputs of the classifier to make them better calibrated. Every proper loss is minimised by the same calibration map, known as the canonical calibration function [25] ofp, defined as\n\u00b5(q) = (P(Y = 1 |p(X) = q), . . . , P(Y = k |p(X) = q))\nThe goal of Dirichlet calibration, as of any other post-hoc calibration method, is to estimate this canonical calibration map \u00b5 for a given probabilistic classifierp.\n\n\nDirichlet calibration\n\nA key decision in designing a calibration method is the choice of parametric family. Our choice was based on the following desiderata: (1) the family needs enough capacity to express biases of particular classes or pairs of classes; (2) the family must contain the identity map for the case where the model is already calibrated; (3) for every map in the family we must be able to provide a semi-reasonable synthetic example where it is the canonical calibration function; (4) the parameters should be interpretable to some extent at least.\n\nDirichlet calibration map family. Inspired by beta calibration for binary classifiers [15], we consider the distribution of prediction vectorsp(x) separately on instances of each class, and assume these k distributions are Dirichlet distributions with different parameters:\np(X) | Y = j \u223c Dir(\u03b1 ( j) )(5)\nwhere\n\u03b1 ( j) = (\u03b1 ( j) 1 , . . . , \u03b1( j)\nk ) \u2208 (0, \u221e) k are the Dirichlet parameters for class j. Combining likelihoods P(p(X) | Y ) with priors P(Y ) expressing the overall class distribution \u03c0 \u2208 \u2206 k , we can use Bayes' rule to express the canonical calibration function P(Y |p(X)) as follows:\n\ngenerative parametrisation:\u03bc DirGen (q; \u03b1, \u03c0) = (\u03c0 1 f 1 (q), . . . , \u03c0 k f k (q)) /z (6) where z = \u2211 k j=1 \u03c0 j f j (q) is the normaliser, and f j is the probability density function of the Dirichlet distribution with parameters \u03b1 ( j) , gathered into a matrix \u03b1. It will also be convenient to have two alternative parametrisations of the same family: a linear parametrisation for fitting purposes and a canonical parametrisation for interpretation purposes. These parametrisations are defined as follows:\nlinear parametrisation:\u03bc DirLin (q; W, b) = \u03c3 (W ln q + b)(7)\nwhere W \u2208 R k\u00d7k is a k \u00d7 k parameter matrix, ln is a vector function that calculates the natural logarithm component-wise and b \u2208 R k is a parameter vector of length k;\n\ncanonical parametrisation:\u03bc Dir (q; A, c) = \u03c3 (A ln q 1/k + ln c)\n\nwhere each column in the k-by-k matrix A \u2208 [0, \u221e) k\u00d7k with non-negative entries contains at least one value 0, division of q by 1/k is component-wise, and c \u2208 \u2206 k is a probability vector of length k. Class -18.00 -1.00 2.00 -8.00 4.00 0.00 18.00 1.00 0.00 2.00 -6.00 -16.00 11.00 1.00 1.00 0.00 3.00 3.00 1.00 2.00 -12.00-12.00 50.00 -3.00 0.00 1.00 0.00 1.00 -26.00 1.00 9.00 -4.00 3.00 -14.00 3.00 0.00 0.00 0.00 2.00 1.00 -1.00 -11.00 -6.00 -1.00 16.00 7.00 0.00 -4.00 -4.00 4.00 -3.00 -5.00 -3.00 -7.00 1.00 19.00 0.00 -7.00 1.00 4.00 -24.00 -9.00 2.00 -1.00 1.00 0.00 28.00 1.00 2.00 0.00 4.00 -10.00 0.00 -6.00 3.00 4.00 0.00 -3.00 6.00 2.00\n\n1.00 -34.00 17.00 -5.00 3.00 1.00 2.00 -2.00 19.00 -2.00 -12.00-10.00 2.00 -2.00 1.00 1.00 1.00 -7.00 6.00 20.00 Theorem 1 (Equivalence of generative, linear and canonical parametrisations). The parametric families\u03bc DirGen (q; \u03b1, \u03c0),\u03bc DirLin (q; W, b) and\u03bc Dir (q; A, c) are equal, i.e. they contain exactly the same calibration maps.\n\nProof. All proofs are given in the Supplemental Material.\n\nThe benefit of the linear parametrisation is that it can be easily implemented as (additional) layers in a neural network: a logarithmic transformation followed by a fully connected layer with softmax activation. Out of the three parametrisations only the canonical parametrisation is unique, in the sense that any function in the Dirichlet calibration map family can be represented by a single pair of matrix A and vector c satisfying the requirements set by the canonical parametrisation\u03bc Dir (q; A, c).\n\nInterpretability. In addition to providing uniqueness, the canonical parametrisation is to some extent interpretable. As demonstrated in the proof of Thm. 1 provided in the Supplemental Material, the linear parametrisation W, b obtained after fitting can be easily transformed into the canonical parametrisation by a i j = w i j \u2212 min i w i j and c = \u03c3 (W ln u + b), where u = (1/k, . . . , 1/k). In the canonical parametrisation, increasing the value of element a i j in matrix A increases the calibrated probability of class i (and decreases the probabilities of all other classes), with effect size depending on the uncalibrated probability of class j. E.g., element a 3,9 = 0.63 of Fig.2b increases class 2 probability whenever class 8 has high predicted probability, modifying decision boundaries and resulting in 26 less confusions of class 2 for 8 as seen in Fig.2c. Looking at the matrix A and vector c, it is hard to know the effect of the calibration map without performing the computations. However, at k + 1 'interpretation points' this is (approximately) possible. One of these is the centre of the probability simplex, which maps to c. The other k points are vectors where one value is (almost) zero and the other values are equal, summing up to 1. Figure 2a shows the 3+1 interpretation points in an example for k = 3, where each arrow visualises the result of calibration (end of arrow) at a particular point (beginning of arrow). The result of calibration map at the interpretation points in the centres of sides (facets) is each determined by a single column of A only. The k columns of matrix A and the vector c determine, respectively, the behaviour of the calibration map near the k + 1 points\n\u03b5, 1 \u2212 \u03b5 k \u2212 1 , . . . , 1 \u2212 \u03b5 k \u2212 1 , . . . , 1 \u2212 \u03b5 k \u2212 1 , . . . , 1 \u2212 \u03b5 k \u2212 1 , \u03b5 , and 1 k , . . . , 1 k\nThe first k points are infinitesimally close to the centres of facets of the probability simplex, and the last point is the centre of the whole simplex. For 3 classes these 4 points have been visualised on the simplex in Fig. 2a. The Dirichlet calibration map\u03bc Dir (q; A, c) transforms these k + 1 points into:\n\n(\u03b5 a 11 , . . . , \u03b5 a k1 ) /z 1 , . . . , (\u03b5 a 1k , . . . , \u03b5 a kk ) /z k , and (c 1 , . . . , c k )\n\nwhere z i are normalising constants, and a i j , c j are elements of the matrix A and vector c, respectively. However, the effect of each parameter goes beyond the interpretation points and also changes classification decision boundaries. This can be seen for the calibration map for a model SVHN_convnet in Fig. 2b where larger off-diagonal coefficients a i j often result in a bigger change in the confusion matrix as seen in Fig. 2c (particularly in the 3rd row and 9th column).\n\nRelationship to other families. For 2 classes, the Dirichlet calibration map family coincides with the beta calibration map family [15]. Although temperature scaling has been defined on logits z, it can be expressed in terms of the model outputsp = \u03c3 (z) as well. It turns out that temperature scaling maps all belong to the Dirichlet family, with\u03bc TempS (q;t) =\u03bc DirLin (q; 1 t I, 0), where I is the identity matrix and 0 is the zero vector (see Prop.1 in the Supplemental Material). The Dirichlet calibration family is also related to the matrix scaling family\u03bc MatS (z; W, b) = \u03c3 (Wz + b) proposed by [9] alongside with temperature scaling. Both families use a fully connected layer with softmax activation, but the crucial difference is in the inputs to this layer. Matrix scaling uses logits z, while the linear parametrisation of Dirichlet calibration uses log-transformed probabilities ln(p) = ln(\u03c3 (z)). As softmax followed by log-transform is losing information, matrix scaling has an informational advantage over Dirichlet calibration on deep neural networks, which we will turn back to in the experiments section.\n\nFitting and ODIR regularisation. The results of [9] showed poor performance for matrix scaling (with ECE, log-loss, error rate), leading the authors to the conclusion that \"[a]ny calibration model with tens of thousands (or more) parameters will overfit to a small validation set, even when applying regularization\". We agree that some overfitting happens, but in our experiments a simple L2 regularisation suffices on non-neural models, whereas for deep neural nets we propose a novel ODIR (Off-Diagonal and Intercept Regularisation) scheme, which is efficient enough in fighting overfitting to make both Dirichlet calibration and matrix scaling outperform temperature scaling on many occasions, including cases with 100 classes and hence 10100 parameters. Fitting of Dirichlet calibration maps is performed by minimising log-loss, and by adding ODIR regularisation terms to the loss function as follows:\nL = 1 n n \u2211 i=1 logloss \u03bc DirLin (p(x i ); W, b), y i + \u03bb \u00b7 1 k(k \u2212 1) \u2211 i = j w 2 i j + \u00b5 \u00b7 1 k \u2211 j b 2 j\nwhere (x i , y i ) are validation instances and w i j , b j are elements of W and b, respectively, and \u03bb , \u00b5 are hyper-parameters tunable with internal cross-validation on the validation data. The intuition is that the diagonal is allowed to freely follow the biases of classes, whereas the intercept is regularised separately from the off-diagonal elements due to having different scales (additive vs. multiplicative).\n\nImplementation details. Implementation of Dirichlet calibration is straightforward in standard deep neural network frameworks (we used Keras [5] in the neural experiments). Alternatively, it is also possible to use the Newton-Raphson method on the L2 regularised objective function, which is constructed by applying multinomial logistic regression with k features (log-transformed predicted class probabilities). Both the gradient and Hessian matrix can be calculated either analytically or using automatic differentiation libraries (e.g. JAX [2]). Such implementations normally yield faster convergence given the convexity of the multinomial logistic loss, which is a better choice with a small number of target classes (tractable Hessian). One can also simply adopt existing implementations of logistic regression (e.g. scikit-learn) with the log transformed predicted probabilities. If the uncalibrated model outputs zero probability for some class, then this needs to be clipped to a small positive number (we used 2.2e \u2212308 , the smallest positive usable number for the type float64 in Python).\n\n\nExperiments\n\nThe main goals of our experiments are to: (1) compare performance of Dirichlet calibration with other general-purpose calibration methods on a wide range of datasets and classifiers; (2) compare Dirichlet calibration with temperature scaling on several deep neural networks and study the effectiveness of ODIR regularisation; and (3) study whether the neural-specific calibration methods outperform general-purpose calibration methods due to the information loss going from logits to softmax outputs.  \n\n\nCalibration of non-neural models\n\nExperimental setup. Calibration methods were compared on 21 UCI datasets (abalone, balancescale, car, cleveland, dermatology, glass, iris, landsat-satellite, libras-movement, mfeat-karhunen, mfeat-morphological, mfeat-zernike, optdigits, page-blocks, pendigits, segment, shuttle, vehicle, vowel, waveform-5000, yeast) with 11 classifiers: multiclass logistic regression (logistic), naive Bayes (nbayes), random forest (forest), adaboost on trees (adas), linear discriminant analysis (lda), quadratic discriminant analysis (qda), decision tree (tree), K-nearest neighbours (knn), multilayer perceptron (mlp), support vector machine with linear (svc-linear) and RBF kernel (svc-rbf ).\n\nIn each of the 21 \u00d7 11 = 231 settings we performed nested cross-validation to evaluate 6 calibration methods: one-vs-rest isotonic calibration (OvR_Isotonic) which learns an isotonic calibration map on each class vs rest separately and renormalises the individual calibration map outputs to add up to one at test time; one-vs-rest equal-width binning (OvR_Width_Bin) where one-vs-rest calibration maps predict the empirical proportion of labels in each of the equal-width bins of the range [0, 1]; one-vs-rest equal-frequency binning (OvR_Freq_Bin) constructing bins with equal numbers of instances; onevs-rest beta calibration (OvR_Beta); temperature scaling (Temp_Scaling); and Dirichlet Calibration with L2 regularisation (Dirichlet_L2). We used 3-fold internal cross-validation to train the calibration maps within the 5 times 5-fold external cross-validation. Following [24], the 3 calibration maps learned in the internal cross-validation were all used as an ensemble by averaging their predictions. For calibration methods with hyperparameters we used the training fold of the classifier to choose the hyperparameter values with the lowest log-loss.\n\nWe used 8 evaluation measures: accuracy, log-loss, Brier score, maximum calibration error (MCE), confidence-ECE (conf-ECE), classwise-ECE (cw-ECE), as well as significance measures p-conf-ECE and p-cw-ECE evaluating how often the respective ECE measures are not significantly higher than when assuming calibration. For p-conf-ECE and p-cw-ECE we used significance level \u03b1 = 0.05 in the test of [25] as explained in Section 2, and counted the proportion of significance tests accepting the model being calibrated out of 5 \u00d7 5 cases of external cross-validation. With each of the 8 evaluation measures we ranked the methods on each of the 21 \u00d7 11 tasks and performed Friedman tests to find statistical differences [7]. When the p-value of the Friedman test was under 0.005 we performed a post-hoc one-tailed Bonferroni-Dunn test to obtain Critical Differences (CDs) which indicated the minimum ranking difference to consider the methods significantly different. Further details of the experimental setup are provided in the Supplemental Material.\n\nResults. The results showed that Dirichlet_L2 was among the best calibrators for every measure.\n\nIn particular, it was the best calibration method based on log-loss, p-cw-ECE and accuracy, and in the group of best calibrators for the other measures. The rankings have been averaged into grouping by classifier learning algorithm and shown for log-loss in Table 2, and for p-cw-ECE in Table 1. The critical difference diagram for p-cw-ECE is presented in Fig. 3a. Fig. 3b shows the average p-cw-ECE for each calibration method across all datasets and shows how frequently the statistical test accepted the null hypothesis of classifier being calibrated (higher p-cw-ECE is better). The results show that Dirichlet_L2 was considered calibrated on more than 60% of the p-cw-ECE tests. An evaluation of classwise-calibration without post-hoc calibration is given in Fig. 3c. Note that svc-linear and svc-rbf have an unfair advantage because their sklearn implementation uses Platt scaling with 3-fold internal cross-validation to provide probabilities. Supplemental material contains the final ranking tables and CD diagrams for every metric, an analysis of the best calibrator hyperparameters, and a more detailed comparison of the classwise calibration for the 11 classifiers.\n\n\nCalibration of deep neural networks\n\nExperimental setup. We used 3 datasets (CIFAR-10, CIFAR-100 and SVHN), training 11 deep convolutional neural nets with various architectures: ResNet 110 [10], ResNet 110 SD [12], ResNet 152 SD [12], DenseNet 40 [11], WideNet 32 [28], LeNet 5 [18], and acquiring 3 pretrained models from [4]. For the latter we set aside 5,000 test instances for fitting the calibration map. On other models we followed [9], setting aside 5,000 training instances (6,000 in SVHN) for calibration purposes and training the models as in the original papers. For calibration methods with hyperparameters we used 5-fold cross-validation on the validation set to find optimal regularisation parameters. We used all 5 calibration models with the optimal hyperparameter values by averaging their predictions as in [24].\n\nAmong general-purpose calibration methods we compared 2 variants of Dirichlet calibration (with L2 regularisation and with ODIR) against temperature scaling (as discussed in Section 3, it can equivalently act on probabilities instead of logits and is therefore general-purpose). Other methods from our non-neural experiment were not included, as these were outperformed by temperature scaling in the experiments of [9]. Among methods that use logits (neural-specific calibration methods) we included matrix scaling with ODIR regularisation, and vector scaling, which restricts the matrix scaling family, fixing off-diagonal elements to 0. As reported by [9], the non-regularised matrix scaling performed very poorly and was not included in our comparisons. Full details and source code for training the models are in the Supplemental Material.\n\n\nResults\n\n. Tables 3 and 4 show that the best among three general-purpose calibration methods depends heavily on the model and dataset. Both variants of Dirichlet calibration (with L2 and with ODIR) outperformed temperature scaling in most cases on CIFAR-10. On CIFAR-100, Dir-L2 is poor, but Dir-ODIR outperforms TempS in cw-ECE, showing the effectiveness of ODIR regularisation. However, this comes at the expense of minor increase in log-loss. According to the average rank across all deep net experiments, Dir-ODIR is best, but without statistical significance.\n\nThe full comparison including calibration methods that use logits confirms that information loss going from logits to softmax outputs has an effect and MS-ODIR (matrix scaling with ODIR) outperforms Dir-ODIR in 8 out of 14 cases on cw-ECE and 11 out of 14 on log-loss. However, the effect is numerically usually very small, as average relative reduction of cw-ECE and log-loss is less than 1% (compared to the average relative reduction of over 30% from the uncalibrated model). According to the average rank on cw-ECE the best method is vector scaling, but this comes at the expense of increased log-loss. According to the average rank on log-loss the best method is MS-ODIR, while its cw-ECE is on average bigger than for vector scaling by 2%.\n\nAs the difference between MS-ODIR and vector scaling was on some models quite small, we further investigated the importance of off-diagonal coefficients in MS-ODIR. For this we introduced a new model MS-ODIR-zero which was obtained from the respective MS-ODIR model by replacing the offdiagonal entries with zeroes. In 6 out of 14 cases (c10_convnet, c10_densenet40, c10_resnet110_SD, c100_convnet, c100_resnet110_SD, SVHN_resnet152_SD) MS-ODIR-zero and MS-ODIR had almost identical performance (difference in log-loss of less than 0.0001), indicating that ODIR  \n\n\nConclusion\n\nIn this paper we proposed a new parametric general-purpose multiclass calibration method called Dirichlet calibration, which is a natural extension of the two-class beta calibration method. Dirichlet calibration is easy to implement as a layer in a neural net, or as multinomial logistic regression on log-transformed class probabilities, and its parameters provide insights into the biases of the model. While derived from Dirichlet-distributed likelihoods, it does not assume that the probability vectors are actually Dirichlet-distributed within each class, similarly as logistic calibration (Platt scaling) does not assume that the scores are Gaussian-distributed, while it can be derived from Gaussian likelihoods.\n\nComparisons with other general-purpose calibration methods across 21 datasets \u00d7 11 models showed best or tied best performance for Dirichlet calibration on all 8 evaluation measures. Evaluation with our proposed classwise-ECE measures how calibrated are the predicted probabilities on all classes, not only on the most likely predicted class as with the commonly used (confidence-)ECE. On neural networks we advance the state-of-the-art by introducing the ODIR regularisation scheme for matrix scaling and Dirichlet calibration, leading these to outperform temperature scaling on many deep neural networks.\n\nInterestingly, on many deep nets Dirichlet calibration learns a map which is very close to being in a temperature scaling family. This raises a fundamental theoretical question of which neural architectures and training methods result in a classifier with its canonical calibration function contained in the temperature scaling family. But even in those cases Dirichlet calibration can become useful after any kind of dataset shift, learning an interpretable calibration map to reveal the shift and recalibrate the predictions for the new context.\n\nDeriving calibration maps from Dirichlet distributions opens up the possibility of using other distributions of the exponential family to obtain new calibration maps designed for various score types, as well as investigating scores coming from mixtures of distributions inside each class.\n\n\nA Source code\n\nThe instructions and code for the experiments can be found on https://dirichletcal.github.io/.\n\n\nB Proofs\n\nTheorem 1 (Equivalence of generative, linear and canonical parametrisations). The parametric families\u03bc DirGen (q; \u03b1, \u03c0),\u03bc DirLin (q; W, b) and\u03bc Dir (q; A, c) are equal, i.e. they contain exactly the same calibration maps.\n\nProof. We will prove that:\n\n1. every function in\u03bc DirGen (q; \u03b1, \u03c0) belongs to\u03bc DirLin (q; W, b);\n\n2. every function in\u03bc DirLin (q; W, b) belongs to\u03bc Dir (q; A, c);\n\n3. every function in\u03bc Dir (q; A, c) belongs to\u03bc DirGen (q; \u03b1, \u03c0).\n\n1. Consider a function\u03bc(q) =\u03bc DirGen (q; \u03b1, \u03c0). Let us start with an observation that any vector x = (x 1 , . . . , x k ) \u2208 (0, \u221e) k with only positive elements can be renormalised to add up to 1 using the expression \u03c3(ln(x)), since:\n\u03c3(ln(x)) = exp(ln(x))/( i exp(ln(x i ))) = x/( i x i )\nwhere exp is an operator applying exponentiation element-wise. Therefore,\n\u00b5(q) = \u03c3(ln(\u03c0 1 f 1 (q), . . . , \u03c0 k f k (q))) where f i (q) is the probability density function of the distribution Dir(\u03b1 (i) ) where \u03b1 (i) is the i-th row of matrix \u03b1. Hence, f i (q) = 1 B(\u03b1 (i) ) k j=1 q \u03b1ij \u22121 j\n, where B(\u00b7) denotes the multivariate beta function. Let us define a matrix W and vector b as follows:\nw ij = \u03b1 ij \u2212 1, b i = ln(\u03c0 i ) \u2212 ln(B(\u03b1 (i) ))\nwith w ij and \u03b1 ij denoting elements of matrices W and \u03b1, respectively, and b i , \u03c0 i denoting elements of vectors b and \u03c0. Now we can write ln(\u03c0 i f i (q)) = ln(\u03c0 i ) \u2212 ln(B(\u03b1 (i) )) + ln\nk j=1 q \u03b1ij \u22121 j = ln(\u03c0 i ) \u2212 ln(B(\u03b1 (i) )) + k j=1 (\u03b1 ij \u2212 1) ln(q j ) = b i + k j=1 w ij ln(q j )\nand substituting this back into\u03bc(q) we get:\n\u00b5(q) = \u03c3(ln(\u03c0 1 f 1 (q), . . . , \u03c0 k f k (q))) = \u03c3(b + Wln(q)) =\u03bc DirLin (q; W, b) 2.\nConsider a function\u03bc(q) =\u03bc DirLin (q; W, b). Let us define a matrix A and vector c as follows:\na ij = w ij \u2212 min i w ij , c = \u03c3(W ln u + b)\nwith a ij and w ij denoting elements of matrices A and W, respectively, and u = (1/k, . . . , 1/k) is a column vector of length k. Note that A x = W x + const 1 and ln \u03c3(x) = x + const 2 for any x where const 1 and const 2 are constant vectors (all elements are equal), but the constant depends on\n\nx. Taking into account that \u03c3(v + const) = \u03c3(v) for any vector v and constant vector const, we obtain:\n\u00b5 Dir (q; A, c) = \u03c3(A ln q 1/k + ln c) = \u03c3(W ln q 1/k + const 1 + ln c) = \u03c3(W ln q \u2212 W ln u + const 1 + ln \u03c3(W ln u + b)) = \u03c3(W ln q \u2212 W ln u + const 1 + W ln u + b + const 2 ) = \u03c3(W ln q + b + const 1 + const 2 ) = \u03c3(W ln q + b) =\u03bc DirLin (q; W, b) =\u03bc(q)\n3. Consider a function\u03bc(q) =\u03bc Dir (q; A, c). Let us define a matrix \u03b1, vector b and vector \u03c0 as follows:\n\u03b1 ij = a ij + 1, b = ln c \u2212 A ln u, \u03c0 i = exp(b i ) \u00b7 B(\u03b1 (i) )\nwith \u03b1 ij and a ij denoting elements of matrices \u03b1 and A, respectively, and u = (1/k, . . . , 1/k) is a column vector of length k. We can now write:\n\u00b5(q) =\u03bc Dir (q; A, c) = \u03c3(A ln q 1/k + ln c) = \u03c3(A ln q \u2212 A ln u + ln c) = \u03c3((\u03b1 \u2212 1)ln q + b) Element i in the vector within the softmax is equal to: k j=1 (\u03b1 ij \u2212 1) ln(q j ) + b j = k j=1 (\u03b1 ij \u2212 1) ln(q j ) + ln(\u03c0 i \u00b7 1 B(\u03b1 (i) ) ) = ln(\u03c0 i \u00b7 1 B(\u03b1 (i) ) k j=1 q \u03b1ij \u22121 j ) = ln(\u03c0 i \u00b7 f i (\u03b1 (i) ))\nand therefore: \u00b5(q) = \u03c3((\u03b1 \u2212 1)ln(q) + b) = \u03c3(ln(\u03c0 i \u00b7 f i (\u03b1 (i) ))) =\u03bc DirGen (q; \u03b1, \u03c0)\n\nThe following proposition proves that temperature scaling can be viewed as a general-purpose calibration method, being a special case within the Dirichlet calibration map family. Proposition 1. Let us denote the temperature scaling family by\u03bc T empS (z; t) = \u03c3(z/t) where z are the logits. Then for any t, temperature scaling can be expressed a\u015d\n\u00b5 T empS (z; t) =\u03bc DirLin (\u03c3(z); 1 t I, 0)\nwhere I is the identity matrix and 0 is the vector of zeros.\n\nProof. Let us first observe that for any x \u2208 R k there exists a constant vector const (all elements are equal) such that ln \u03c3(x) = x + const. Furthermore, \u03c3(v + const) = \u03c3(v) for any vector v and any constant vector const. Therefore,\n\u00b5 DirLin (\u03c3(z); 1 t I, 0) = \u03c3( 1 t I ln \u03c3(z))) = \u03c3( 1 t I (z + const)) = \u03c3( 1 t I z + 1 t I const) = \u03c3(z/t + const ) = \u03c3(z/t) =\u03bc T empS (z; t)\nwhere const = 1 t I const is a constant vector as a product of a diagonal matrix with a constant vector.\n\n\nC Dirichlet calibration\n\nIn this section we show some examples of reliability diagrams and other plots that can help to understand the representational power of Dirichlet calibration compared with other calibration methods.\n\n\nC.1 Reliability diagram examples\n\nWe will look at two examples of reliability diagrams on the original classifier and after applying 6 calibration methods. Figure 1 shows the first example for the 3 class classification dataset balance-scale and the classifier MLP. This figure shows the confidence-reliability diagram in the first column and the classwise-reliability diagrams in the other columns. Figure 1a shows how posterior probabilities from the MLP have small gaps between the true class proportions and the predicted means. This visualisation may indicate that the original classifier is already well calibrated. However, when we separate the reliability diagram per class, we notice that the predictions for the first class are underconfident; as indicated by low mean predictions containing high proportions of the true class. On the other hand, classes 2 and 3 are overconfident in the regions of posterior probabilities compressed between [0.2, 0.5] while being underconfident in higher regions. The discrepancy shown by analysing the individual reliability diagrams seems to compensate in the general picture of the aggregated one. The following subfigures show how the different calibration methods try to reduce ECE, occasionally increasing the error. As can be seen in Table 1, Dirichlet L2 and One-vs.Rest isotonic regression obtain the lowest ECE while One-vs.Rest frequency binning makes the original calibration worse. Looking at Figure 1i it is possible to see how temperature scaling manages to reduce the overall overconfidence in the higher range of probabilities for classes 2 and 3, but makes the situation worse in the interval [0.2, 0.6]. However, it manages to reduce the overall ECE.\n\nIn the second example we show 3 calibration methods for a 4 class classification problem (car dataset) applied on the scores of an Adaboost SAMME classifier. Figure 2 shows one reliability diagram per class (C 1 acceptable, C 2 good, C 3 unacceptable, and C 4 very good).\n\nFrom this Figure we can see that the uncalibrated model is underconfident for classes 1, 2 and 3, showing posterior probabilities never higher than 0.7, while having true class proportions higher than 0.7 in the mentioned interval. We can see that after applying some of the calibration models the posterior probabilities reach higher probability values. As can be seen in Table 2, Dirichlet L2  Figure 2d shows how Dirichlet calibration with L2 regularisation achieved the largest spread of probabilities, also reducing the error mean gap with the predictions and the true class proportions. On the other hand, temperature scaling reduced ECE for class 1, but hurt the overall performance for the other classes.\n\nA more detailed depiction of the previous reliability diagrams can be seen in Figure 3. In this case, the posterior probabilities are not introduced in bins, but a boxplot summarises their full distribution. The first observation here is, for the good and very good classes, the uncalibrated model tends to predict probability vectors with small variance, i.e. the outputs do not change much among different  instances. Among the calibration approaches, temperature scaling still maintains this low level of variance, while both isotonic and Dirichlet L2 manage to show a higher variance on the outputs. While this observation cannot be justified here without quantitative analysis, another observation clearly shows an advantage of using Dirichlet L2. For the acceptable class, only Dirichlet L2 is capable of providing the highest mean probability for the correct class, while the other three methods tend to put higher probability mass on the unacceptable class on average.\n\n\nD Experimental setup\n\nIn this section we provide the detailed description of the experimental setup on a variety of non-neural classifiers and datasets. While our implementation of Dirichlet calibration is based on standard Newton-Raphson with multinomial logistic loss and L2 regularisation, as mentioned at the end of Section 3, existing implementations of logistic regression (e.g. scikit-learn) with the log transformed predicted probabilities can also be easily applied.\n\n\nD.1 Datasets and performance estimation\n\nThe full list of datasets, and a brief description of each one including the number of samples, features and classes is presented in Table 3. Figure 4 shows how every dataset was divided in order to get an estimated performance for every combination of dataset, classifier and calibrator. Each dataset was divided using 5 times 5-fold-crossvalidation to create 25 test partitions. For each of the 25 partitions the corresponding training set was divided further with a 3-fold-cross-validation for wich the bigger portions were used to train the classifiers (and validate the calibratiors if they had hyperparameters), and the small portion was used to train the calibrators. The 3 calibrators trained in the inner 3-folds were used to predict the corresponding test partition, and their predictions were averaged in order to obtain better estimates of  Figure 3a it is possible to see that the first class (acceptable) is missclassified as belonging to the third class (unacceptable) with high probability values, while Dirichlet Calibration is able to alleviate that problem. Also, for the second and fourth true classes (good, and very good) the original classifier uses a reduced domain of probabilities (indicative of underconfidence), while Dirichlet calibration is able to spread these probabilities with more meaningful values (as indicated by a reduction of the calibration losses; See Figure 2). their performance with the 7 different metrics (accuracy, Brier score, log-loss, maximum calibration error, confidence-ECE, classwise-ECE and the p test statistic of the ECE metrics). Finally, the 25 resulting measures were averaged.\n\n\nD.2 Full example of statistical analysis\n\nThe following is a full example of how the final rankings and statistical tests are computed. For this example, we will focus on the metric log-loss, and we will start with the naive Bayes classifier. Table  4 shows the estimated log-loss by averaging the 5-times 5-fold cross-validation log-losses of the inner 3-fold aggregated predictions. The sub-indices are the ranking of every calibrator for each dataset (ties in the ranking share the averaged rank). The resulting table of sub-indices is used to compute the Friedman test statistic, resulting in a value of 73.8 and a p-value of 6.71e \u221214 indicating statistical difference between the calibration methods. The last row contains the average ranks of the full table, which is shown in the corresponding critical difference diagram in Figure 5a. The critical difference uses the Bonferroni-Dunn one-tailed statistical test to compute the minimum ranking distance that is shown in the Figure, indicating that for this particular classifier and metric the Dirichlet calibrator with L2 regularisation is significantly better than the other methods.\n\nThe same process is applied to each of the 11 classifiers for every metric.   Figure 4: Partitions of each dataset in order to estimate out-of-sample measures. 1.11 1 1.50 4 1.14 3 1.64 5 1.12 2 3.14 6.5 3.14 6.5  \n\n\nE Results\n\nIn this Section we present all the final results, including ranking tables for every metric, critical difference diagrams, the best hyperparameters selected for Dirichlet calibration with L2 regularisation, Frequency binning and Width binning; a comparison of how calibrated the 11 classifiers are, and additional results on deep neural networks.\n\n\nE.1 Final ranking tables for all metrics\n\nWe present here all the final ranking tables for all metrics (Tables 5, 6 , 7, 8, 9, 10, 11, and 12). For each ranking, a lower value is indicative of a better metric value (eg. a higher accuracy corresponds to a lower ranking, while a lower log-loss corresponds to a lower ranking as well). Additional details on how to interpret the tables can be found in Section D.2.      \n\n\nE.2 Final critical difference diagrams for every metric\n\nIn order to perform a final comparison between calibration methods, we considered every combination of dataset and classifier as a group n = #datasets \u00d7 #classif iers, and ranked the results of the k calibration methods. With this setting, we have performed the Friedman statistical test followed by the one-tailed Bonferroni-Dunn test to obtain critical differences (CDs) for every metric (See Figure    Figures 6a 6c, and 6h), and in the group of the best calibration methods in the rest of the metrics with statistical significance, but no difference within the group. It is worth mentioning that Figure 6c showed statistical difference between Dirichlet L2, OvR Beta, OvR width binning, and the rest of the calibrators in one group; in the mentioned order. \n\n\nE.3 Best calibrator hyperparameters\n\n\nE.4 Comparison of classifiers\n\nIn this Section we compare all the classifiers without post-hoc calibration on 17 of the datasets; from the total of 21 datasets shuttle, yeast, mfeat-karhunen and libras-movement were removed from this analysis as at least one classifier was not able to complete the experiment. Figure 9 shows the Critical Difference diagram for all the 8 metrics. In particular, the MLP and the SVC with linear kernel are always in the group with the higher rankings and never in the lowest. Similarly, random forest is consistently in the best group, but in the worst group as well in 4 of the measures. SVC with radial basis kernel is in the best group 6 times, but 3 times in the worst. On the other hand, naive Bayes and Adaboost SAMME are consistently in the worst group and never in the best one. The rest of the classifiers did not show a clear ranking position. Figures 10b and 10a show the proportion of times each classifier passed the p-conf-ECE and p-cw-ECE statistical test for all datasets and cross-validation folds.\n\n\nE.5 Deep neural networks\n\nIn this section, we provide further discussion about results from the deep networks experiments. These are given in the form of critical difference diagrams ( Figure 11) and tables (Tables 13-20) both including the following measures: error rate, log-loss, Brier score, maximum calibration error (MCE), confidence-ECE (conf-ECE), classwise-ECE (cw-ECE), as well as significance measures p-conf-ECE and p-cw-ECE.\n\nIn addition, Table 21 compares MS-ODIR and vector scaling on log-loss. On the table, we also added MS-ODIR-zero which was obtained from the respective MS-ODIR model by replacing the off-diagonal entries with zeroes. Each experiment is replicated three times with different splits on datasets. This is done to compare the stability of the methods. In each replication, the best scoring model is written in bold.    Finally, Figure 12 shows that temperature scaling systematically under-estimates class 4 probabilities on the model c10_resnet_wide32 on CIFAR-10.           \n\n\nFig. 1a, where the wide blue bars show observed accuracy within each bin (empirical version of the conditional probability in Eq.(3)), and narrow red bars show the gap between the two sides of Eq.(3).\n\nFigure 2 :\n2Interpretation of Dirichlet calibration maps: (a) calibration map for MLP on the abalone dataset, 4 interpretation points shown by black dots, and canonical parametrisation as a matrix with A, c; (b) canonical parametrisation of a map on SVHN_convnet; (c) changes to the confusion matrix after applying this calibration map.\n\nFigure 3 :\n3Summarised results for p-cw-ECE: (a) CD diagram; (b) proportion of times each calibrator was calibrated (\u03b1 = 0.05); (c) proportion of times each classifier was already calibrated (\u03b1 = 0.05).\n\nFigure 1 :\n1Confidence-reliability diagrams in the first column and classwise-reliability diagrams in the remaining columns, for a real experiment with the multilayer perceptron classifier on the balance-scale dataset and a subset of the calibrators. All the test partitions from the 5 times 5-fold-cross-validation have been aggregated to draw every plot.\n\nFigure 2 :\n2Reliability diagrams per class for a real experiment with the classifier Ada boost SAMME on the car dataset and 3 calibrators. The test partitions from the 5 times 5-fold-cross-validation have been aggregated to draw every plot.\n\nFigure 3 :\n3Effect of Dirichlet Calibration on the scores of Ada boost SAMME on the car dataset which is composed of 4 classes (acceptable, good, unacceptable, and very good). The whiskers of each box indicate the 5th and 95th percentile, the notch around the median indicates the confidence interval. The green error bar to the right of each box indicates one standard deviation on each side of the mean. In each subfigure, the first boxplot corresponds to the posterior probabilities for the samples of class 1, divided in 4 boxes representing the posterior probabilities for each class. A good classifier should have the highest posterior probabilities in the box corresponding to the true class. In\n\n\n) Average over all datasets for Naive Bayes classifier\n\nFigure 5 :\n5Critical Difference diagrams for the averaged ranking results of the metric Log-loss.\n\nFigure 6 :\n6Critical difference of the average of multiclass classifiers. 6). The results showed Dirichlet L2 as the best calibration method for the measures accuracy, log-loss and p-cw-ece with statistical significance (See\n\nFigure 8 Figure 7 :Figure 8 :\n878shows the best hyperparameters for every inner 3-fold-cross-validation. Dirichlet L2 (Figure 8a) shows a preference for regularisation hyperparameter \u03bb = 1e \u22123 and lower values. Our current minimum regularisation value of 1e \u22127 is also being selected multiple times, indicating that lower values may be optimal in several occasions. However, this fact did not seem to hurt the overall good results in our experiments. One-vs.-Rest frequency binning tends to prefer 10 bins of equal Proportion of times each calibrator passes a calibration p-test with a p-Histogram of the selected hyperparameters during the inner 3-fold-cross-validation number of samples, while One-vs.Rest width binning prefers 5 equal sized bins (SeeFigures 8b and 8c respectively).\n\nFigure 9 :Figure 10 :\n910Critical difference of uncalibrated classifiers.(a) p-conf-ece (b) p-cw-ece Proportion of times each classifier is already calibrated with different p-tests.\n\nFigure 11 :\n11Critical difference of the deep neural networks.\n\nFigure 12 :\n12Reliability diagrams of c10_resnet_wide32 on CIFAR-10: (a) classwise-reliability for class 4 after temperature scaling; (b) classwise-reliability for class 4 after Dirichlet calibration.\n\nTable 1 :\n1Ranking of calibration methods for p-cw-ECE (Friedman's test significant with p-value 7.54e \u221285 ).DirL2 Beta FreqB Isot WidB TempS Uncal \n\nadas \n2.4 \n3.2 \n4.1 \n4.2 \n3.9 \n5.0 \n5.2 \nforest \n3.5 \n2.3 \n5.7 \n3.0 \n3.6 \n5.0 \n5.0 \nknn \n2.5 \n4.0 \n4.5 \n2.1 \n3.2 \n5.8 \n6.0 \nlda \n1.9 \n3.1 \n5.8 \n3.0 \n3.5 \n5.0 \n5.8 \nlogistic \n2.2 \n2.8 \n6.4 \n3.0 \n4.2 \n3.9 \n5.5 \nmlp \n2.2 \n2.9 \n6.7 \n4.0 \n5.2 \n3.0 \n4.1 \nnbayes \n1.4 \n3.6 \n4.8 \n2.6 \n4.2 \n5.3 \n6.1 \nqda \n2.2 \n2.8 \n6.3 \n2.5 \n3.8 \n4.8 \n5.6 \nsvc-linear \n2.3 \n2.7 \n6.7 \n3.8 \n4.0 \n3.7 \n4.8 \nsvc-rbf \n2.9 \n3.0 \n6.3 \n3.5 \n4.1 \n3.9 \n4.3 \ntree \n2.4 \n4.3 \n5.9 \n4.2 \n5.2 \n3.0 \n3.0 \n\navg rank \n2.34 \n3.15 \n5.73 \n3.27 4.11 \n4.37 \n5.02 \n\n\n\nTable 2 :\n2Ranking of calibration methods for log-loss (p-value 4.39e \u221277 ).DirL2 Beta FreqB Isot WidB TempS Uncal \n\n1.4 \n3.1 \n3.2 \n4.3 \n3.5 \n5.9 \n6.6 \n4.2 \n1.9 \n4.7 \n4.1 \n2.9 \n5.2 \n5.2 \n3.8 \n4.8 \n3.0 \n1.6 \n2.0 \n6.5 \n6.5 \n1.6 \n2.2 \n5.2 \n5.2 \n3.5 \n4.6 \n5.7 \n1.3 \n2.1 \n5.8 \n6.1 \n3.5 \n3.6 \n5.6 \n2.2 \n2.3 \n6.5 \n6.2 \n4.7 \n2.9 \n3.4 \n1.1 \n3.4 \n3.4 \n4.0 \n4.4 \n5.5 \n6.3 \n1.7 \n2.7 \n5.6 \n4.6 \n3.4 \n4.2 \n5.8 \n1.3 \n2.3 \n6.1 \n6.1 \n4.3 \n3.0 \n4.8 \n2.6 \n2.2 \n4.3 \n4.8 \n4.5 \n4.0 \n5.6 \n3.9 \n5.1 \n3.4 \n2.1 \n2.4 \n5.6 \n5.6 \n\n2.25 \n2.92 \n4.66 \n4.48 \n3.54 \n4.61 \n5.54 \n\n\n\nTable 3 :\n3Scores and ranking of calibration methods for cw-ECE. general-purpose calibrators calibrators using logits Uncal TempS Dir-L2 Dir-ODIR VecS MS-ODIRc10_convnet \n0.104 6 0.044 4 0.043 2 \n0.045 5 \n0.043 1 \n0.044 3 \nc10_densenet40 \n0.114 6 0.040 5 0.034 1 \n0.037 4 \n0.036 2 \n0.037 3 \nc10_lenet5 \n0.198 6 0.171 5 0.052 1 \n0.059 4 \n0.057 2 \n0.059 3 \nc10_resnet110 \n0.098 6 0.043 5 0.032 1 \n0.039 4 \n0.037 3 \n0.036 2 \nc10_resnet110_SD \n0.086 6 0.031 4 0.031 5 \n0.029 3 \n0.027 2 \n0.027 1 \nc10_resnet_wide32 \n0.095 6 0.048 5 0.032 3 \n0.029 2 \n0.032 4 \n0.029 1 \nc100_convnet \n0.424 6 0.227 1 0.402 5 \n0.240 3 \n0.241 4 \n0.240 2 \nc100_densenet40 \n0.470 6 0.187 2 0.330 5 \n0.186 1 \n0.189 3 \n0.191 4 \nc100_lenet5 \n0.473 6 0.385 5 0.219 4 \n0.213 2 \n0.203 1 \n0.214 3 \nc100_resnet110 \n0.416 6 0.201 3 0.359 5 \n0.186 1 \n0.194 2 \n0.203 4 \nc100_resnet110_SD \n0.375 6 0.203 4 0.373 5 \n0.189 3 \n0.170 1 \n0.186 2 \nc100_resnet_wide32 0.420 6 0.186 4 0.333 5 \n0.180 2 \n0.171 1 \n0.180 3 \nSVHN_convnet \n0.159 6 0.038 4 0.043 5 \n0.026 2 \n0.025 1 \n0.027 3 \nSVHN_resnet152_SD 0.019 2 0.018 1 0.022 6 \n0.020 3 \n0.021 5 \n0.021 4 \n\nAverage rank \n5.71 \n3.71 \n3.79 \n2.79 \n2.29 \n2.71 \n\n\n\nTable 4 :\n4Scores and ranking of calibration methods for log-loss.general-purpose calibrators calibrators using logits \nUncal TempS Dir-L2 Dir-ODIR VecS \nMS-ODIR \n\n0.391 6 0.195 1 0.197 4 \n0.195 2 \n0.197 5 \n0.196 3 \n0.428 6 0.225 5 0.220 1 \n0.224 4 \n0.223 3 \n0.222 2 \n0.823 6 0.800 5 0.744 2 \n0.744 3 \n0.747 4 \n0.743 1 \n0.358 6 0.209 5 0.203 1 \n0.205 3 \n0.206 4 \n0.204 2 \n0.303 6 0.178 5 0.177 4 \n0.176 3 \n0.175 2 \n0.175 1 \n0.382 6 0.191 5 0.185 4 \n0.182 2 \n0.183 3 \n0.182 1 \n1.641 6 0.942 1 1.189 5 \n0.961 2 \n0.964 4 \n0.961 3 \n2.017 6 1.057 2 1.253 5 \n1.059 4 \n1.058 3 \n1.051 1 \n2.784 6 2.650 5 2.595 4 \n2.490 2 \n2.516 3 \n2.487 1 \n1.694 6 1.092 3 1.212 5 \n1.096 4 \n1.089 2 \n1.074 1 \n1.353 6 0.942 3 1.198 5 \n0.945 4 \n0.923 1 \n0.927 2 \n1.802 6 0.945 3 1.087 5 \n0.953 4 \n0.937 2 \n0.933 1 \n0.205 6 0.151 5 0.142 3 \n0.138 2 \n0.144 4 \n0.138 1 \n0.085 6 0.079 1 0.085 5 \n0.080 2 \n0.081 4 \n0.081 3 \n\n6.0 \n3.5 \n3.79 \n2.93 \n3.14 \n1.64 \n\n\n\nTable 1 :\n1Averaged results for the confidence-ECE and classwise-ECE metrics of 6 calibrators applied on an MLP trained in the balance-scale dataset.DirL2 Beta FreqB \nIsot \nWidB TempS Uncal \n\nconf-ECE 0.041 0.053 0.137 \n0.052 0.086 \n0.054 \n0.085 \ncw-ECE \n0.122 0.133 0.297 0.121 0.175 \n0.154 \n0.206 \n\n\n\nTable 2 :\n2Averaged results for the confidence-ECE and classwise-ECE metrics of 6 calibrators applied on an Adaboost SAMME trained in the car dataset. and One-vs.Rest Isotonic Regression obtain the lowest ECE while Temperature Scaling makes the original calibration worse.DirL2 Beta FreqB \nIsot \nWidB TempS Uncal \n\nconf-ECE 0.071 0.104 0.125 \n0.072 0.093 \n0.147 \n0.146 \ncw-ECE \n0.182 0.233 0.295 0.181 0.254 \n0.327 \n0.296 \n\n\n\nTable 6\n6shows the final average results of all classifiers. Notice that the row corresponding to naive Bayes has the rounded average rankings fromFigure 5a.n_samples \n\nn_features \nn_classes \ndataset \n\nabalone \n4177 \n8 \n3 \nbalance-scale \n625 \n4 \n3 \ncar \n1728 \n6 \n4 \ncleveland \n297 \n13 \n5 \ndermatology \n358 \n34 \n6 \nglass \n214 \n9 \n6 \niris \n150 \n4 \n3 \nlandsat-satellite \n6435 \n36 \n6 \nlibras-movement \n360 \n90 \n15 \nmfeat-karhunen \n2000 \n64 \n10 \nmfeat-morphological \n2000 \n6 \n10 \nmfeat-zernike \n2000 \n47 \n10 \noptdigits \n5620 \n64 \n10 \npage-blocks \n5473 \n10 \n5 \npendigits \n10992 \n16 \n10 \nsegment \n2310 \n19 \n7 \nshuttle \n101500 \n9 \n7 \nvehicle \n846 \n18 \n4 \nvowel \n990 \n10 \n11 \nwaveform-5000 \n5000 \n40 \n3 \nyeast \n1484 \n8 \n10 \n\n\n\nTable 3 :\n3Datasets used for the large-scale empirical comparison.\n\nTable 4 :\n4Ranking of calibration methods applied on the classifier naive Bayes with log-loss (Friedman statistic test = 73.8, p-value = 6.71E-14)DirL2 \nBeta \nFreqB \nIsot \nWidB \nTempS \nUncal \n\nabalone \n0.89 1 0.89 4 0.89 2 \n0.90 5 \n0.92 6 \n0.89 3 \n1.95 7 \nbalance-sc \n0.21 1 0.30 2 0.36 5 \n0.32 3 \n0.35 4 \n0.41 6 \n0.47 7 \ncar \n0.38 1 0.59 4 0.56 2 \n0.57 3 \n0.67 5 1.56 6.5 1.56 6.5 \ncleveland \n1.02 1 1.30 4 1.12 2 \n1.38 5 \n1.14 3 \n2.18 6 \n2.49 7 \ndermatolog \n0.20 1 0.41 5 0.23 2 \n0.39 3 \n0.40 4 \n2.51 7 \n2.51 6 \nglass \n\n\nTable 5 :\n5Rankings for AccuracyDirL2 Beta FreqB Isot WidB TempS Uncal \n\nadas \n2.5 \n4.1 \n2.9 \n4.5 \n3.1 \n5.3 \n5.5 \nforest \n4.0 \n3.0 \n5.6 \n3.2 \n4.4 \n3.9 \n3.9 \nknn \n5.0 \n3.9 \n4.8 \n3.1 \n3.1 \n4.0 \n4.0 \nlda \n3.5 \n5.1 \n4.9 \n3.7 \n5.0 \n3.0 \n2.9 \nlogistic \n2.1 \n3.7 \n5.3 \n4.0 \n3.6 \n4.6 \n4.7 \nmlp \n2.9 \n2.8 \n5.9 \n3.7 \n4.5 \n4.0 \n4.3 \nnbayes \n1.4 \n3.8 \n3.0 \n2.9 \n5.0 \n6.0 \n6.0 \nqda \n2.7 \n3.6 \n3.9 \n2.9 \n3.8 \n5.6 \n5.6 \nsvc-linear \n1.8 \n3.5 \n5.7 \n2.8 \n4.3 \n5.1 \n4.8 \nsvc-rbf \n3.3 \n3.5 \n3.8 \n3.2 \n3.6 \n5.0 \n5.5 \ntree \n3.7 \n4.8 \n4.5 \n5.0 \n4.3 \n2.8 \n2.8 \n\navg rank \n2.99 \n3.78 \n4.58 \n3.55 \n4.07 \n4.48 \n4.54 \n\n\n\nTable 6 :\n6Rankings for log-lossDirL2 Beta FreqB Isot WidB TempS Uncal \n\nadas \n1.4 \n3.1 \n3.2 \n4.3 \n3.5 \n5.9 \n6.6 \nforest \n4.2 \n1.9 \n4.7 \n4.1 \n2.9 \n5.2 \n5.2 \nknn \n3.8 \n4.8 \n3.0 \n1.6 \n2.0 \n6.5 \n6.5 \nlda \n1.6 \n2.2 \n5.2 \n5.2 \n3.5 \n4.6 \n5.7 \nlogistic \n1.3 \n2.1 \n5.8 \n6.1 \n3.5 \n3.6 \n5.6 \nmlp \n2.2 \n2.3 \n6.5 \n6.2 \n4.7 \n2.9 \n3.4 \nnbayes \n1.1 \n3.4 \n3.4 \n4.0 \n4.4 \n5.5 \n6.3 \nqda \n1.7 \n2.7 \n5.6 \n4.6 \n3.4 \n4.2 \n5.8 \nsvc-linear \n1.3 \n2.3 \n6.1 \n6.1 \n4.3 \n3.0 \n4.8 \nsvc-rbf \n2.6 \n2.2 \n4.3 \n4.8 \n4.5 \n4.0 \n5.6 \ntree \n3.9 \n5.1 \n3.4 \n2.1 \n2.4 \n5.6 \n5.6 \n\navg rank \n2.25 \n2.92 \n4.66 \n4.48 \n3.54 \n4.61 \n5.54 \n\n\n\nTable 7 :\n7Rankings for Brier scoreDirL2 Beta FreqB Isot WidB TempS Uncal \n\nadas \n1.6 \n3.0 \n3.3 \n3.6 \n3.6 \n6.3 \n6.5 \nforest \n4.4 \n1.8 \n5.4 \n1.9 \n3.9 \n5.3 \n5.3 \nknn \n3.9 \n3.5 \n5.3 \n1.9 \n3.8 \n4.8 \n4.8 \nlda \n1.8 \n3.2 \n5.3 \n2.2 \n3.9 \n6.0 \n5.8 \nlogistic \n1.6 \n2.7 \n6.1 \n2.5 \n4.3 \n4.3 \n6.4 \nmlp \n3.0 \n2.2 \n6.6 \n2.8 \n5.2 \n3.9 \n4.2 \nnbayes \n1.2 \n3.5 \n4.2 \n2.3 \n4.9 \n5.7 \n6.2 \nqda \n1.9 \n2.9 \n5.8 \n2.1 \n4.4 \n5.1 \n5.7 \nsvc-linear \n1.5 \n2.8 \n6.5 \n2.6 \n4.6 \n4.1 \n5.8 \nsvc-rbf \n3.0 \n2.5 \n4.7 \n2.8 \n4.7 \n4.5 \n5.8 \ntree \n3.4 \n4.2 \n6.5 \n4.7 \n5.4 \n1.9 \n2.0 \n\navg rank \n2.48 \n2.94 \n5.43 \n2.67 \n4.43 \n4.72 \n5.33 \n\n\n\nTable 8 :\n8Rankings for MCEDirL2 Beta FreqB Isot WidB TempS Uncal \n\nadas \n3.0 \n3.4 \n3.5 \n3.4 \n3.6 \n5.3 \n5.9 \nforest \n4.2 \n3.2 \n4.8 \n3.8 \n3.4 \n4.1 \n4.5 \nknn \n4.2 \n4.7 \n4.2 \n3.7 \n3.3 \n4.0 \n4.0 \nlda \n2.0 \n3.2 \n4.8 \n4.5 \n4.0 \n5.0 \n4.5 \nlogistic \n3.4 \n3.8 \n5.4 \n4.8 \n2.5 \n3.5 \n4.7 \nmlp \n3.2 \n4.2 \n4.7 \n4.6 \n3.0 \n3.7 \n4.5 \nnbayes \n2.6 \n3.0 \n3.6 \n3.0 \n4.2 \n5.6 \n5.9 \nqda \n3.2 \n2.2 \n5.1 \n3.7 \n4.2 \n4.2 \n5.4 \nsvc-linear \n2.8 \n4.2 \n5.5 \n3.7 \n3.5 \n4.1 \n4.2 \nsvc-rbf \n5.0 \n4.6 \n3.9 \n3.5 \n3.3 \n3.5 \n4.2 \ntree \n4.3 \n4.2 \n4.5 \n3.4 \n3.7 \n4.0 \n4.0 \n\navg rank \n3.44 \n3.73 \n4.53 \n3.83 \n3.50 \n4.27 \n4.71 \n\n\n\nTable 9 :\n9Rankings for confidence-ECEDirL2 Beta FreqB Isot WidB TempS Uncal \n\nadas \n1.7 \n2.7 \n4.3 \n2.7 \n4.2 \n6.0 \n6.4 \nforest \n4.2 \n2.2 \n5.7 \n1.4 \n4.4 \n5.1 \n5.1 \nknn \n3.0 \n3.0 \n6.1 \n3.5 \n5.8 \n3.3 \n3.3 \nlda \n2.0 \n2.9 \n5.9 \n2.1 \n4.0 \n5.7 \n5.5 \nlogistic \n2.2 \n3.0 \n6.3 \n1.9 \n4.7 \n3.8 \n6.1 \nmlp \n3.5 \n2.7 \n6.6 \n1.4 \n5.7 \n4.0 \n4.2 \nnbayes \n2.1 \n2.8 \n5.2 \n2.4 \n4.3 \n5.3 \n5.9 \nqda \n3.1 \n2.3 \n6.5 \n1.7 \n4.7 \n4.6 \n5.1 \nsvc-linear \n2.7 \n2.8 \n6.7 \n2.0 \n4.9 \n3.4 \n5.5 \nsvc-rbf \n3.7 \n3.4 \n6.5 \n2.9 \n4.5 \n2.7 \n4.3 \ntree \n2.6 \n3.6 \n6.8 \n4.8 \n5.7 \n2.2 \n2.3 \n\navg rank \n2.80 \n2.86 \n6.05 \n2.42 \n4.81 \n4.17 \n4.89 \n\n\n\nTable 10 :\n10Rankings for classwise-ECEDirL2 Beta FreqB Isot WidB TempS Uncal \n\nadas \n1.9 \n3.2 \n4.3 \n4.3 \n4.1 \n5.0 \n5.1 \nforest \n4.0 \n2.1 \n5.8 \n1.1 \n4.0 \n5.5 \n5.4 \nknn \n4.0 \n3.9 \n6.0 \n3.6 \n5.6 \n2.5 \n2.5 \nlda \n2.4 \n2.8 \n5.8 \n2.0 \n4.1 \n5.3 \n5.7 \nlogistic \n2.2 \n2.5 \n6.2 \n2.0 \n4.4 \n4.5 \n6.1 \nmlp \n3.0 \n2.3 \n6.6 \n1.7 \n5.5 \n4.3 \n4.5 \nnbayes \n1.9 \n3.5 \n5.0 \n2.5 \n4.0 \n5.4 \n5.7 \nqda \n2.7 \n2.6 \n6.4 \n1.8 \n4.6 \n5.0 \n4.9 \nsvc-linear \n2.5 \n2.6 \n6.7 \n2.5 \n4.6 \n3.6 \n5.5 \nsvc-rbf \n2.7 \n2.9 \n6.5 \n3.1 \n4.5 \n3.6 \n4.7 \ntree \n3.1 \n4.1 \n6.5 \n4.7 \n5.5 \n1.9 \n2.0 \n\navg rank \n2.76 \n2.95 \n5.97 \n2.67 \n4.65 \n4.25 \n4.75 \n\n\n\nTable 11 :\n11Rankings for p-confidence-ECEDirL2 Beta FreqB Isot WidB TempS Uncal \n\nadas \n1.8 \n2.9 \n4.3 \n3.1 \n4.4 \n5.6 \n5.7 \nforest \n3.7 \n2.2 \n6.0 \n1.9 \n4.7 \n4.7 \n4.9 \nknn \n2.6 \n2.6 \n5.7 \n2.9 \n5.1 \n4.5 \n4.6 \nlda \n1.9 \n3.0 \n6.1 \n2.2 \n3.9 \n5.3 \n5.5 \nlogistic \n2.6 \n2.9 \n6.3 \n1.8 \n4.7 \n3.7 \n6.0 \nmlp \n3.4 \n2.8 \n6.6 \n2.2 \n5.7 \n3.5 \n3.9 \nnbayes \n2.1 \n2.7 \n5.2 \n2.5 \n4.5 \n4.9 \n6.1 \nqda \n3.0 \n2.1 \n6.5 \n2.1 \n4.6 \n4.6 \n5.2 \nsvc-linear \n2.5 \n3.0 \n6.6 \n2.4 \n5.0 \n3.3 \n5.1 \nsvc-rbf \n3.4 \n3.4 \n6.3 \n3.0 \n5.0 \n2.8 \n4.2 \ntree \n2.5 \n3.7 \n6.4 \n4.3 \n5.7 \n2.6 \n2.7 \n\navg rank \n2.69 \n2.87 \n6.00 \n2.58 \n4.85 \n4.11 \n4.90 \n\n\n\nTable 12 :\n12Rankings for p-classwise-ECEDirL2 Beta FreqB Isot WidB TempS Uncal \n\nadas \n2.4 \n3.2 \n4.1 \n4.2 \n3.9 \n5.0 \n5.2 \nforest \n3.5 \n2.3 \n5.7 \n3.0 \n3.6 \n5.0 \n5.0 \nknn \n2.5 \n4.0 \n4.5 \n2.1 \n3.2 \n5.8 \n6.0 \nlda \n1.9 \n3.1 \n5.8 \n3.0 \n3.5 \n5.0 \n5.8 \nlogistic \n2.2 \n2.8 \n6.4 \n3.0 \n4.2 \n3.9 \n5.5 \nmlp \n2.2 \n2.9 \n6.7 \n4.0 \n5.2 \n3.0 \n4.1 \nnbayes \n1.4 \n3.6 \n4.8 \n2.6 \n4.2 \n5.3 \n6.1 \nqda \n2.2 \n2.8 \n6.3 \n2.5 \n3.8 \n4.8 \n5.6 \nsvc-linear \n2.3 \n2.7 \n6.7 \n3.8 \n4.0 \n3.7 \n4.8 \nsvc-rbf \n2.9 \n3.0 \n6.3 \n3.5 \n4.1 \n3.9 \n4.3 \ntree \n2.4 \n4.3 \n5.9 \n4.2 \n5.2 \n3.0 \n3.0 \n\navg rank \n2.34 \n3.15 \n5.73 \n3.27 \n4.11 \n4.37 \n5.02 \n\n\n\nTable 13 :\n13Scores and ranking of calibration methods for log-loss.general-purpose calibrators \ncalibrators using logits \nUncal \nTempS \nDir-L2 \nDir-ODIR \nVecS \nMS-ODIR \n\nc10_convnet \n0.39098 6 \n0.19497 1 \n0.19692 4 \n0.19536 2 \n0.19743 5 \n0.19634 3 \nc10_densenet40 \n0.42821 6 \n0.22509 5 \n0.22048 1 \n0.22371 4 \n0.22270 3 \n0.22240 2 \nc10_lenet5 \n0.82326 6 \n0.80031 5 \n0.74418 2 \n0.74441 3 \n0.74704 4 \n0.74262 1 \nc10_resnet110 \n0.35827 6 \n0.20926 5 \n0.20303 1 \n0.20511 3 \n0.20595 4 \n0.20375 2 \nc10_resnet110_SD \n0.30325 6 \n0.17760 5 \n0.17694 4 \n0.17608 3 \n0.17549 2 \n0.17537 1 \nc10_resnet_wide32 \n0.38170 6 \n0.19148 5 \n0.18464 4 \n0.18203 2 \n0.18276 3 \n0.18165 1 \nc100_convnet \n1.64120 6 \n0.94162 1 \n1.18945 5 \n0.96121 2 \n0.96369 4 \n0.96141 3 \nc100_densenet40 \n2.01740 6 \n1.05713 2 \n1.25293 5 \n1.05909 4 \n1.05831 3 \n1.05084 1 \nc100_lenet5 \n2.78365 6 \n2.64979 5 \n2.59482 4 \n2.48951 2 \n2.51590 3 \n2.48670 1 \nc100_resnet110 \n1.69371 6 \n1.09169 3 \n1.21239 5 \n1.09607 4 \n1.08916 2 \n1.07370 1 \nc100_resnet110_SD \n1.35250 6 \n0.94214 3 \n1.19837 5 \n0.94477 4 \n0.92341 1 \n0.92731 2 \nc100_resnet_wide32 \n1.80215 6 \n0.94453 3 \n1.08711 5 \n0.95288 4 \n0.93650 2 \n0.93273 1 \nSVHN_convnet \n0.20460 6 \n0.15142 5 \n0.14246 3 \n0.13791 2 \n0.14388 4 \n0.13760 1 \nSVHN_resnet152_SD \n0.08542 6 \n0.07861 1 \n0.08463 5 \n0.08038 2 \n0.08124 4 \n0.08100 3 \n\navg rank \n6.0 \n3.5 \n3.79 \n2.93 \n3.14 \n1.64 \n\n\n\nTable 14 :\n14Scores and ranking of calibration methods for Brier score.general-purpose calibrators \ncalibrators using logits \nUncal \nTempS \nDir-L2 \nDir-ODIR \nVecS \nMS-ODIR \n\nc10_convnet \n0.01090 6 \n0.00952 1 \n0.00969 5 \n0.00955 3 \n0.00958 4 \n0.00953 2 \nc10_densenet40 \n0.01274 6 \n0.01100 4 \n0.01102 5 \n0.01097 2 \n0.01097 3 \n0.01097 1 \nc10_lenet5 \n0.03788 6 \n0.03748 5 \n0.03510 2 \n0.03511 3 \n0.03523 4 \n0.03502 1 \nc10_resnet110 \n0.01102 6 \n0.00979 4 \n0.00979 5 \n0.00977 2 \n0.00978 3 \n0.00976 1 \nc10_resnet110_SD \n0.00981 6 \n0.00874 4 \n0.00877 5 \n0.00867 3 \n0.00867 2 \n0.00866 1 \nc10_resnet_wide32 \n0.01047 6 \n0.00924 5 \n0.00909 4 \n0.00888 1 \n0.00891 3 \n0.00889 2 \nc100_convnet \n0.00425 5 \n0.00358 1 \n0.00441 6 \n0.00358 2 \n0.00362 4 \n0.00361 3 \nc100_densenet40 \n0.00491 6 \n0.00401 3 \n0.00468 5 \n0.00400 2 \n0.00403 4 \n0.00400 1 \nc100_lenet5 \n0.00813 6 \n0.00792 5 \n0.00786 4 \n0.00760 2 \n0.00767 3 \n0.00760 1 \nc100_resnet110 \n0.00453 6 \n0.00392 3 \n0.00438 5 \n0.00391 2 \n0.00393 4 \n0.00391 1 \nc100_resnet110_SD \n0.00418 5 \n0.00367 4 \n0.00456 6 \n0.00364 3 \n0.00360 1 \n0.00361 2 \nc100_resnet_wide32 \n0.00432 6 \n0.00355 4 \n0.00401 5 \n0.00354 3 \n0.00352 2 \n0.00351 1 \nSVHN_convnet \n0.00776 6 \n0.00598 5 \n0.00555 3 \n0.00530 1 \n0.00561 4 \n0.00532 2 \nSVHN_resnet152_SD \n0.00297 3 \n0.00291 1 \n0.00305 6 \n0.00293 2 \n0.00299 5 \n0.00298 4 \n\navg rank \n5.64 \n3.5 \n4.71 \n2.21 \n3.29 \n1.64 \n\n\n\nTable 15 :\n15Scores and ranking of calibration methods for confidence-ECE.general-purpose calibrators \ncalibrators using logits \nUncal \nTempS \nDir-L2 \nDir-ODIR \nVecS \nMS-ODIR \n\nc10_convnet \n0.04760 6 \n0.01065 5 \n0.00769 2 \n0.00960 4 \n0.00740 1 \n0.00782 3 \nc10_densenet40 \n0.05500 6 \n0.00946 2 \n0.00568 1 \n0.01097 5 \n0.01018 4 \n0.00988 3 \nc10_lenet5 \n0.05180 6 \n0.01665 5 \n0.01383 3 \n0.01367 2 \n0.01310 1 \n0.01468 4 \nc10_resnet110 \n0.04750 6 \n0.01132 5 \n0.00680 1 \n0.01086 3 \n0.01130 4 \n0.01059 2 \nc10_resnet110_SD \n0.04113 6 \n0.00555 1 \n0.00646 4 \n0.00815 5 \n0.00579 3 \n0.00566 2 \nc10_resnet_wide32 \n0.04505 6 \n0.00784 4 \n0.00524 1 \n0.00837 5 \n0.00769 3 \n0.00727 2 \nc100_convnet \n0.17614 6 \n0.01367 1 \n0.14347 5 \n0.02069 3 \n0.01965 2 \n0.02660 4 \nc100_densenet40 \n0.21156 6 \n0.00902 1 \n0.12380 5 \n0.01138 2 \n0.01224 3 \n0.02197 4 \nc100_lenet5 \n0.12125 6 \n0.01499 4 \n0.01369 2 \n0.02003 5 \n0.01294 1 \n0.01407 3 \nc100_resnet110 \n0.18480 6 \n0.02380 1 \n0.14535 5 \n0.02822 4 \n0.02693 2 \n0.02735 3 \nc100_resnet110_SD \n0.15861 5 \n0.01214 1 \n0.15920 6 \n0.02283 4 \n0.01296 2 \n0.02246 3 \nc100_resnet_wide32 \n0.18784 6 \n0.01472 1 \n0.13509 5 \n0.01891 3 \n0.01718 2 \n0.02581 4 \nSVHN_convnet \n0.07755 6 \n0.01179 4 \n0.01910 5 \n0.00997 2 \n0.00934 1 \n0.01037 3 \nSVHN_resnet152_SD \n0.00862 6 \n0.00607 4 \n0.00691 5 \n0.00582 1 \n0.00595 2 \n0.00604 3 \n\navg rank \n5.93 \n2.79 \n3.57 \n3.43 \n2.21 \n3.07 \n\n\nTable 16 :\n16Scores and ranking of calibration methods for classwise-ECE.general-purpose calibrators \ncalibrators using logits \nUncal \nTempS \nDir-L2 \nDir-ODIR \nVecS \nMS-ODIR \n\nc10_convnet \n0.10375 6 \n0.04423 4 \n0.04262 2 \n0.04507 5 \n0.04259 1 \n0.04352 3 \nc10_densenet40 \n0.11430 6 \n0.03977 5 \n0.03412 1 \n0.03687 4 \n0.03609 2 \n0.03678 3 \nc10_lenet5 \n0.19849 6 \n0.17141 5 \n0.05185 1 \n0.05891 4 \n0.05705 2 \n0.05862 3 \nc10_resnet110 \n0.09846 6 \n0.04344 5 \n0.03206 1 \n0.03950 4 \n0.03653 3 \n0.03615 2 \nc10_resnet110_SD \n0.08647 6 \n0.03071 4 \n0.03148 5 \n0.02937 3 \n0.02713 2 \n0.02681 1 \nc10_resnet_wide32 \n0.09530 6 \n0.04775 5 \n0.03153 3 \n0.02947 2 \n0.03164 4 \n0.02921 1 \nc100_convnet \n0.42414 6 \n0.22683 1 \n0.40185 5 \n0.24041 3 \n0.24063 4 \n0.23958 2 \nc100_densenet40 \n0.47026 6 \n0.18664 2 \n0.32985 5 \n0.18630 1 \n0.18879 3 \n0.19112 4 \nc100_lenet5 \n0.47264 6 \n0.38481 5 \n0.21865 4 \n0.21348 2 \n0.20293 1 \n0.21379 3 \nc100_resnet110 \n0.41644 6 \n0.20095 3 \n0.35885 5 \n0.18639 1 \n0.19442 2 \n0.20270 4 \nc100_resnet110_SD \n0.37518 6 \n0.20310 4 \n0.37346 5 \n0.18895 3 \n0.17015 1 \n0.18552 2 \nc100_resnet_wide32 \n0.42027 6 \n0.18573 4 \n0.33258 5 \n0.17951 2 \n0.17082 1 \n0.17966 3 \nSVHN_convnet \n0.15935 6 \n0.03830 4 \n0.04276 5 \n0.02638 2 \n0.02480 1 \n0.02750 3 \nSVHN_resnet152_SD \n0.01940 2 \n0.01849 1 \n0.02184 6 \n0.01988 3 \n0.02120 5 \n0.02088 4 \n\navg rank \n5.71 \n3.71 \n3.79 \n2.79 \n2.29 \n2.71 \n\n\n\nTable 17 :\n17Scores and ranking of calibration methods for MCE.general-purpose calibrators \ncalibrators using logits \nUncal \nTempS \nDir-L2 \nDir-ODIR \nVecS \nMS-ODIR \n\nc10_convnet \n0.59173 6 \n0.23150 4 \n0.12432 2 \n0.24830 5 \n0.12831 3 \n0.07621 1 \nc10_densenet40 \n0.33396 6 \n0.09929 2 \n0.11679 4 \n0.07858 1 \n0.12046 5 \n0.11297 3 \nc10_lenet5 \n0.11281 6 \n0.09158 3 \n0.05112 1 \n0.09009 2 \n0.09996 4 \n0.10061 5 \nc10_resnet110 \n0.29580 6 \n0.23639 4 \n0.24405 5 \n0.08331 1 \n0.13130 2 \n0.22678 3 \nc10_resnet110_SD \n0.32484 6 \n0.07823 1 \n0.23064 5 \n0.13309 3 \n0.14276 4 \n0.08422 2 \nc10_resnet_wide32 \n0.37215 4 \n0.07060 1 \n0.49283 6 \n0.41567 5 \n0.26539 3 \n0.26372 2 \nc100_convnet \n0.36391 6 \n0.13689 4 \n0.23333 5 \n0.07235 2 \n0.07043 1 \n0.08171 3 \nc100_densenet40 \n0.45400 6 \n0.02213 1 \n0.19748 5 \n0.04074 2 \n0.04293 3 \n0.05004 4 \nc100_lenet5 \n0.20097 6 \n0.05836 3 \n0.05678 1 \n0.06774 4 \n0.05749 2 \n0.08939 5 \nc100_resnet110 \n0.39882 6 \n0.07099 2 \n0.20732 5 \n0.08026 4 \n0.07354 3 \n0.06678 1 \nc100_resnet110_SD \n0.48291 6 \n0.04099 2 \n0.24578 5 \n0.05979 3 \n0.04038 1 \n0.06612 4 \nc100_resnet_wide32 \n0.45639 6 \n0.03606 1 \n0.19370 5 \n0.05521 2 \n0.06605 4 \n0.06468 3 \nSVHN_convnet \n0.30011 5 \n0.40691 6 \n0.16154 1 \n0.18458 3 \n0.16312 2 \n0.18588 4 \nSVHN_resnet152_SD \n0.25032 5 \n0.18244 1 \n0.23895 4 \n0.19649 2 \n0.23092 3 \n0.80082 6 \n\navg rank \n5.71 \n2.5 \n3.86 \n2.79 \n2.86 \n3.29 \n\n\n\nTable 18 :\n18Scores and ranking of calibration methods for error rate (%).general-purpose calibrators \ncalibrators using logits \nUncal \nTempS \nDir-L2 \nDir-ODIR \nVecS \nMS-ODIR \n\nc10_convnet \n6.18000 2 \n6.18000 2 \n6.38000 6 \n6.12000 1 \n6.36000 5 \n6.32000 4 \nc10_densenet40 \n7.58000 5 \n7.58000 5 \n7.49000 1 \n7.53000 4 \n7.52000 3 \n7.50000 2 \nc10_lenet5 \n27.26000 5 \n27.26000 5 \n25.25000 1 \n25.44000 2 \n25.49000 3 \n25.50000 4 \nc10_resnet110 \n6.44000 1 \n6.44000 1 \n6.54000 6 \n6.49000 4 \n6.47000 3 \n6.49000 4 \nc10_resnet110_SD \n5.96000 5 \n5.96000 5 \n5.90000 4 \n5.77000 1 \n5.83000 3 \n5.81000 2 \nc10_resnet_wide32 \n6.07000 5 \n6.07000 5 \n5.94000 4 \n5.76000 2 \n5.74000 1 \n5.81000 3 \nc100_convnet \n26.12000 1 \n26.12000 1 \n30.96000 6 \n26.22000 3 \n26.56000 4 \n26.60000 5 \nc100_densenet40 \n30.00000 3 \n30.00000 3 \n33.48000 6 \n29.87000 2 \n30.16000 5 \n29.61000 1 \nc100_lenet5 \n66.41000 5 \n66.41000 5 \n65.97000 4 \n62.53000 2 \n63.59000 3 \n62.44000 1 \nc100_resnet110 \n28.52000 4 \n28.52000 4 \n30.04000 6 \n28.36000 1 \n28.40000 2 \n28.45000 3 \nc100_resnet110_SD \n27.17000 4 \n27.17000 4 \n31.43000 6 \n26.96000 3 \n26.50000 2 \n26.42000 1 \nc100_resnet_wide32 \n26.18000 4 \n26.18000 4 \n27.69000 6 \n26.07000 2 \n26.08000 3 \n26.06000 1 \nSVHN_convnet \n3.82750 5 \n3.82750 5 \n3.42811 3 \n3.34728 1 \n3.51845 4 \n3.37105 2 \nSVHN_resnet152_SD \n1.84773 2 \n1.84773 2 \n1.90535 6 \n1.80547 1 \n1.87462 4 \n1.87462 4 \n\navg rank \n4.14 \n4.14 \n4.64 \n2.11 \n3.25 \n2.71 \n\n\n\nTable 19 :\n19Scores and ranking of calibration methods for p-confidence-ECE.general-purpose calibrators \ncalibrators using logits \nUncal \nTempS \nDir-L2 \nDir-ODIR \nVecS \nMS-ODIR \n\n\n\nTable 20 :\n20Scores and ranking of calibration methods for p-classwise-ECE.general-purpose calibrators \ncalibrators using logits \nUncal \nTempS \nDir-L2 \nDir-ODIR \nVecS \nMS-ODIR \n\n\n\nTable 21 :\n21Comparison of MS-ODIR and vector scaling for log-loss.Replication 1 \nReplication 2 \nReplication 3 \nVecS \nMS-ODIR \nMS-ODIR-zero \nVecS \nMS-ODIR \nMS-ODIR-zero \nVecS \nMS-ODIR \nMS-ODIR-zero \n\nc10_convnet \n0.19774 \n0.19632 \n0.19632 \n\u2212 \n\u2212 \n\u2212 \n\u2212 \n\u2212 \n\u2212 \nc10_densenet40 \n0.22240 \n0.22240 \n0.22240 \n0.21316 \n0.21186 \n0.21366 \n0.21350 \n0.21325 \n0.21327 \nc10_lenet5 \n0.74688 \n0.74262 \n0.74830 \n0.69392 \n0.69287 \n0.69335 \n0.67955 \n0.67974 \n0.68127 \nc10_resnet110 \n0.20624 \n0.20375 \n0.20537 \n0.20064 \n0.19803 \n0.20040 \n0.19655 \n0.19536 \n0.19739 \nc10_resnet110_SD \n0.17545 \n0.17537 \n0.17539 \n0.18123 \n0.18094 \n0.18097 \n0.17799 \n0.17829 \n0.17829 \nc10_resnet_wide32 \n0.18274 \n0.18165 \n0.18302 \n0.18522 \n0.18364 \n0.18546 \n0.17431 \n0.17274 \n0.17448 \nc100_convnet \n0.96311 \n0.96141 \n0.96149 \n\u2212 \n\u2212 \n\u2212 \n\u2212 \n\u2212 \n\u2212 \nc100_densenet40 \n1.05714 \n1.05084 \n1.06804 \n1.06366 \n1.05456 \n1.07107 \n1.07704 \n1.06918 \n1.08559 \nc100_lenet5 \n2.51695 \n2.48670 \n2.57932 \n2.21546 \n2.20054 \n2.22360 \n2.28054 \n2.27887 \n2.29485 \nc100_resnet110 \n1.08824 \n1.07370 \n1.10137 \n1.09066 \n1.08267 \n1.11116 \n1.11977 \n1.10672 \n1.13900 \nc100_resnet110_SD \n0.92275 \n0.92731 \n0.92730 \n0.87758 \n0.87698 \n0.87701 \n0.88523 \n0.88731 \n0.88727 \nc100_resnet_wide32 \n0.93724 \n0.93273 \n0.94060 \n0.93291 \n0.92531 \n0.94854 \n0.93183 \n0.92439 \n0.94568 \nSVHN_convnet \n0.14392 \n0.13760 \n0.14507 \n\u2212 \n\u2212 \n\u2212 \n\u2212 \n\u2212 \n\u2212 \nSVHN_resnet152_SD \n0.08131 \n0.08100 \n0.08100 \n0.12728 \n0.12723 \n0.12639 \n0.12559 \n0.12453 \n0.12381 \n\nAcknowledgementsThe work of MKu and MK\u00e4 was supported by the Estonian Research Council under grant PUT1458. The work of MPN and HS was supported by the SPHERE Next Steps Project funded by the UK Engineering and Physical Sciences Research Council (EPSRC), Grant EP/R005273/1. The work of PF and HS was supported by The Alan Turing Institute under EPSRC, Grant EP/N510129/1.\nNon-parametric Bayesian isotonic calibration: Fighting overconfidence in binary classification. M.-L Allikivi, M Kull, Machine Learning and Knowledge Discovery in Databases (ECML-PKDD'19). SpringerM.-L. Allikivi and M. Kull. Non-parametric Bayesian isotonic calibration: Fighting over- confidence in binary classification. In Machine Learning and Knowledge Discovery in Databases (ECML-PKDD'19), pages 68-85. Springer, 2019.\n\nJ Bradbury, R Frostig, P Hawkins, M J Johnson, C Leary, D Maclaurin, S Wanderman-Milne, JAX: composable transformations of Python+NumPy programs. J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, and S. Wanderman- Milne. JAX: composable transformations of Python+NumPy programs, 2018.\n\nVerification of forecasts expressed in terms of probability. G W Brier, Monthly Weather Review. 781G. W. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather Review, 78(1):1-3, 1950.\n\nBase pretrained models and datasets in pytorch. A Cheni, A. Cheni. Base pretrained models and datasets in pytorch, 2017.\n\n. F Chollet, F. Chollet et al. Keras. https://keras.io, 2015.\n\nThe comparison and evaluation of forecasters. M H Degroot, S E Fienberg, Journal of the Royal Statistical Society. Series D (The Statistician). 321/2M. H. DeGroot and S. E. Fienberg. The comparison and evaluation of forecasters. Journal of the Royal Statistical Society. Series D (The Statistician), 32(1/2):12-22, 1983.\n\nStatistical comparisons of classifiers over multiple data sets. J Dem\u0161ar, J. Machine Learning Research. 7J. Dem\u0161ar. Statistical comparisons of classifiers over multiple data sets. J. Machine Learning Research, 7(Jan):1-30, 2006.\n\nImproving the AUC of probabilistic estimation trees. C Ferri, P A Flach, J Hern\u00e1ndez-Orallo, European Conference on Machine Learning. SpringerC. Ferri, P. A. Flach, and J. Hern\u00e1ndez-Orallo. Improving the AUC of probabilistic estimation trees. In European Conference on Machine Learning, pages 121-132. Springer, 2003.\n\nOn Calibration of Modern Neural Networks. C Guo, G Pleiss, Y Sun, K Q Weinberger, Thirty-fourth International Conference on Machine Learning. Sydney, AustraliaC. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On Calibration of Modern Neural Networks. In Thirty-fourth International Conference on Machine Learning, Sydney, Australia, jun 2017.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, abs/1512.03385CoRRK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015.\n\n. G Huang, Z Liu, K Q Weinberger, abs/1608.06993G. Huang, Z. Liu, and K. Q. Weinberger. Densely connected convolutional networks. CoRR, abs/1608.06993, 2016.\n\nDeep networks with stochastic depth. G Huang, Y Sun, Z Liu, D Sedra, K Q Weinberger, abs/1603.09382CoRRG. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger. Deep networks with stochastic depth. CoRR, abs/1603.09382, 2016.\n\nAccurate uncertainties for deep learning using calibrated regression. V Kuleshov, N Fenner, S Ermon, arXiv:1807.00263arXiv preprintV. Kuleshov, N. Fenner, and S. Ermon. Accurate uncertainties for deep learning using calibrated regression. arXiv preprint arXiv:1807.00263, 2018.\n\nNovel decompositions of proper scoring rules for classification: Score adjustment as precursor to calibration. M Kull, P Flach, Machine Learning and Knowledge Discovery in Databases (ECML-PKDD'15). SpringerM. Kull and P. Flach. Novel decompositions of proper scoring rules for classification: Score adjustment as precursor to calibration. In Machine Learning and Knowledge Discovery in Databases (ECML-PKDD'15), pages 68-85. Springer, 2015.\n\nBeyond sigmoids: How to obtain well-calibrated probabilities from binary classifiers with beta calibration. M Kull, T M Silva Filho, P Flach, Electron. J. Statist. 112M. Kull, T. M. Silva Filho, and P. Flach. Beyond sigmoids: How to obtain well-calibrated probabilities from binary classifiers with beta calibration. Electron. J. Statist., 11(2):5052-5080, 2017.\n\nVerified uncertainty calibration. A Kumar, P Liang, T Ma, Advances in Neural Information Processing Systems (NeurIPS'19). A. Kumar, P. Liang, and T. Ma. Verified uncertainty calibration. In Advances in Neural Information Processing Systems (NeurIPS'19), 2019.\n\nTrainable calibration measures for neural networks from kernel mean embeddings. A Kumar, S Sarawagi, U Jain, PMLRProceedings of the 35th International Conference on Machine Learning. J. Dy and A. Krausethe 35th International Conference on Machine LearningStockholmsm\u00e4ssan, Stockholm Sweden80A. Kumar, S. Sarawagi, and U. Jain. Trainable calibration measures for neural networks from kernel mean embeddings. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 2805-2814, Stockholmsm\u00e4ssan, Stockholm Sweden, 10-15 Jul 2018. PMLR.\n\nGradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proceedings of the IEEE. 8611Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.\n\nA simple baseline for bayesian uncertainty in deep learning. W Maddox, T Garipov, P Izmailov, D P Vetrov, A G Wilson, abs/1902.02476CoRR. W. Maddox, T. Garipov, P. Izmailov, D. P. Vetrov, and A. G. Wilson. A simple baseline for bayesian uncertainty in deep learning. CoRR, abs/1902.02476, 2019.\n\nDirichlet-based gaussian processes for large-scale calibrated classification. D Milios, R Camoriano, P Michiardi, L Rosasco, M Filippone, Advances in Neural Information Processing Systems. D. Milios, R. Camoriano, P. Michiardi, L. Rosasco, and M. Filippone. Dirichlet-based gaussian processes for large-scale calibrated classification. In Advances in Neural Information Processing Systems, pages 6005-6015, 2018.\n\nReliability of subjective probability forecasts of precipitation and temperature. A H Murphy, R L Winkler, Journal of the Royal Statistical Society. Series C (Applied Statistics). 261A. H. Murphy and R. L. Winkler. Reliability of subjective probability forecasts of precipitation and temperature. Journal of the Royal Statistical Society. Series C (Applied Statistics), 26(1):41- 47, 1977.\n\nObtaining well calibrated probabilities using bayesian binning. M P Naeini, G Cooper, M Hauskrecht, AAAI Conference on Artificial Intelligence. M. P. Naeini, G. Cooper, and M. Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. In AAAI Conference on Artificial Intelligence, 2015.\n\nBinary classifier calibration using an ensemble of near isotonic regression models. M P Naeini, G F Cooper, 2016 IEEE 16th International Conference on Data Mining (ICDM). IEEEM. P. Naeini and G. F. Cooper. Binary classifier calibration using an ensemble of near isotonic regression models. In 2016 IEEE 16th International Conference on Data Mining (ICDM), pages 360-369. IEEE, 2016.\n\nProbabilities for SV machines. J Platt, Advances in Large Margin Classifiers. A. Smola, P. Bartlett, B. Sch\u00f6lkopf, and D. SchuurmansMIT PressJ. Platt. Probabilities for SV machines. In A. Smola, P. Bartlett, B. Sch\u00f6lkopf, and D. Schuur- mans, editors, Advances in Large Margin Classifiers, pages 61-74. MIT Press, 2000.\n\nEvaluating model calibration in classification. J Vaicenavicius, D Widmann, C Andersson, F Lindsten, J Roll, T Sch\u00f6n, PMLRProceedings of Machine Learning Research. K. Chaudhuri and M. SugiyamaMachine Learning Research89J. Vaicenavicius, D. Widmann, C. Andersson, F. Lindsten, J. Roll, and T. Sch\u00f6n. Evaluating model calibration in classification. In K. Chaudhuri and M. Sugiyama, editors, Proceedings of Machine Learning Research, volume 89 of Proceedings of Machine Learning Research, pages 3459-3467. PMLR, 16-18 Apr 2019.\n\nObtaining calibrated probability estimates from decision trees and naive bayesian classifiers. B Zadrozny, C Elkan, Proc. 18th Int. Conf. on Machine Learning (ICML'01). 18th Int. Conf. on Machine Learning (ICML'01)B. Zadrozny and C. Elkan. Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers. In Proc. 18th Int. Conf. on Machine Learning (ICML'01), pages 609-616, 2001.\n\nTransforming classifier scores into accurate multiclass probability estimates. B Zadrozny, C Elkan, Proc. 8th Int. Conf. on Knowledge Discovery and Data Mining (KDD'02). 8th Int. Conf. on Knowledge Discovery and Data Mining (KDD'02)ACMB. Zadrozny and C. Elkan. Transforming classifier scores into accurate multiclass probability estimates. In Proc. 8th Int. Conf. on Knowledge Discovery and Data Mining (KDD'02), pages 694-699. ACM, 2002.\n\nS Zagoruyko, N Komodakis, Wide residual networks. CoRR, abs/1605.07146. S. Zagoruyko and N. Komodakis. Wide residual networks. CoRR, abs/1605.07146, 2016.\n", "annotations": {"author": "[{\"end\":485,\"start\":109},{\"end\":884,\"start\":486},{\"end\":1269,\"start\":885},{\"end\":1634,\"start\":1270},{\"end\":2013,\"start\":1635},{\"end\":2398,\"start\":2014}]", "publisher": null, "author_last_name": "[{\"end\":120,\"start\":116},{\"end\":506,\"start\":493},{\"end\":900,\"start\":892},{\"end\":1287,\"start\":1282},{\"end\":1643,\"start\":1639},{\"end\":2025,\"start\":2020}]", "author_first_name": "[{\"end\":115,\"start\":109},{\"end\":492,\"start\":486},{\"end\":891,\"start\":885},{\"end\":1275,\"start\":1270},{\"end\":1281,\"start\":1276},{\"end\":1638,\"start\":1635},{\"end\":2019,\"start\":2014}]", "author_affiliation": "[{\"end\":484,\"start\":140},{\"end\":883,\"start\":539},{\"end\":1268,\"start\":924},{\"end\":1633,\"start\":1289},{\"end\":2012,\"start\":1668},{\"end\":2397,\"start\":2053}]", "title": "[{\"end\":106,\"start\":1},{\"end\":2504,\"start\":2399}]", "venue": null, "abstract": "[{\"end\":3573,\"start\":2506}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3629,\"start\":3626},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3908,\"start\":3904},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3934,\"start\":3930},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4009,\"start\":4005},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4036,\"start\":4032},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4086,\"start\":4082},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4193,\"start\":4189},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4303,\"start\":4300},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4507,\"start\":4503},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4844,\"start\":4841},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5146,\"start\":5142},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5270,\"start\":5266},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5425,\"start\":5421},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5634,\"start\":5630},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5816,\"start\":5812},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7632,\"start\":7628},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7765,\"start\":7761},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8115,\"start\":8112},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9430,\"start\":9427},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10217,\"start\":10213},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12522,\"start\":12518},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12830,\"start\":12826},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13447,\"start\":13443},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13888,\"start\":13884},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14785,\"start\":14781},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15385,\"start\":15382},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20508,\"start\":20504},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20980,\"start\":20977},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21550,\"start\":21547},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23077,\"start\":23074},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23479,\"start\":23476},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26150,\"start\":26146},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26827,\"start\":26823},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27144,\"start\":27141},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28946,\"start\":28942},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28966,\"start\":28962},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28986,\"start\":28982},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29004,\"start\":29000},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29021,\"start\":29017},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":29035,\"start\":29031},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29079,\"start\":29076},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29194,\"start\":29191},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29582,\"start\":29578},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30003,\"start\":30000},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30242,\"start\":30239},{\"end\":46593,\"start\":46567}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":49995,\"start\":49793},{\"attributes\":{\"id\":\"fig_3\"},\"end\":50333,\"start\":49996},{\"attributes\":{\"id\":\"fig_4\"},\"end\":50537,\"start\":50334},{\"attributes\":{\"id\":\"fig_5\"},\"end\":50895,\"start\":50538},{\"attributes\":{\"id\":\"fig_6\"},\"end\":51137,\"start\":50896},{\"attributes\":{\"id\":\"fig_7\"},\"end\":51841,\"start\":51138},{\"attributes\":{\"id\":\"fig_8\"},\"end\":51898,\"start\":51842},{\"attributes\":{\"id\":\"fig_9\"},\"end\":51997,\"start\":51899},{\"attributes\":{\"id\":\"fig_10\"},\"end\":52223,\"start\":51998},{\"attributes\":{\"id\":\"fig_11\"},\"end\":53010,\"start\":52224},{\"attributes\":{\"id\":\"fig_12\"},\"end\":53194,\"start\":53011},{\"attributes\":{\"id\":\"fig_13\"},\"end\":53258,\"start\":53195},{\"attributes\":{\"id\":\"fig_14\"},\"end\":53460,\"start\":53259},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":54129,\"start\":53461},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":54677,\"start\":54130},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":55840,\"start\":54678},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":56770,\"start\":55841},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":57073,\"start\":56771},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":57499,\"start\":57074},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":58217,\"start\":57500},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":58285,\"start\":58218},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":58808,\"start\":58286},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":59401,\"start\":58809},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":59994,\"start\":59402},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":60590,\"start\":59995},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":61178,\"start\":60591},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":61777,\"start\":61179},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":62377,\"start\":61778},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":62980,\"start\":62378},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":63582,\"start\":62981},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":64950,\"start\":63583},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":66322,\"start\":64951},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":67697,\"start\":66323},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":69072,\"start\":67698},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":70436,\"start\":69073},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":71854,\"start\":70437},{\"attributes\":{\"id\":\"tab_24\",\"type\":\"table\"},\"end\":72035,\"start\":71855},{\"attributes\":{\"id\":\"tab_25\",\"type\":\"table\"},\"end\":72215,\"start\":72036},{\"attributes\":{\"id\":\"tab_26\",\"type\":\"table\"},\"end\":73668,\"start\":72216}]", "paragraph": "[{\"end\":4375,\"start\":3575},{\"end\":5867,\"start\":4377},{\"end\":6809,\"start\":5869},{\"end\":7524,\"start\":6863},{\"end\":8026,\"start\":7575},{\"end\":8564,\"start\":8063},{\"end\":8666,\"start\":8566},{\"end\":9359,\"start\":8706},{\"end\":10754,\"start\":9361},{\"end\":11690,\"start\":10756},{\"end\":12831,\"start\":11773},{\"end\":13904,\"start\":12833},{\"end\":14127,\"start\":13961},{\"end\":14693,\"start\":14153},{\"end\":14968,\"start\":14695},{\"end\":15005,\"start\":15000},{\"end\":15294,\"start\":15041},{\"end\":15801,\"start\":15296},{\"end\":16032,\"start\":15864},{\"end\":16099,\"start\":16034},{\"end\":16748,\"start\":16101},{\"end\":17084,\"start\":16750},{\"end\":17143,\"start\":17086},{\"end\":17650,\"start\":17145},{\"end\":19366,\"start\":17652},{\"end\":19786,\"start\":19476},{\"end\":19888,\"start\":19788},{\"end\":20371,\"start\":19890},{\"end\":21497,\"start\":20373},{\"end\":22404,\"start\":21499},{\"end\":22931,\"start\":22512},{\"end\":24032,\"start\":22933},{\"end\":24550,\"start\":24048},{\"end\":25269,\"start\":24587},{\"end\":26427,\"start\":25271},{\"end\":27473,\"start\":26429},{\"end\":27570,\"start\":27475},{\"end\":28749,\"start\":27572},{\"end\":29583,\"start\":28789},{\"end\":30428,\"start\":29585},{\"end\":30995,\"start\":30440},{\"end\":31742,\"start\":30997},{\"end\":32307,\"start\":31744},{\"end\":33041,\"start\":32322},{\"end\":33649,\"start\":33043},{\"end\":34198,\"start\":33651},{\"end\":34488,\"start\":34200},{\"end\":34600,\"start\":34506},{\"end\":34834,\"start\":34613},{\"end\":34862,\"start\":34836},{\"end\":34932,\"start\":34864},{\"end\":34999,\"start\":34934},{\"end\":35066,\"start\":35001},{\"end\":35301,\"start\":35068},{\"end\":35430,\"start\":35357},{\"end\":35749,\"start\":35647},{\"end\":35986,\"start\":35798},{\"end\":36130,\"start\":36087},{\"end\":36311,\"start\":36217},{\"end\":36654,\"start\":36357},{\"end\":36758,\"start\":36656},{\"end\":37119,\"start\":37015},{\"end\":37332,\"start\":37184},{\"end\":37724,\"start\":37635},{\"end\":38071,\"start\":37726},{\"end\":38175,\"start\":38115},{\"end\":38410,\"start\":38177},{\"end\":38658,\"start\":38554},{\"end\":38884,\"start\":38686},{\"end\":40601,\"start\":38921},{\"end\":40874,\"start\":40603},{\"end\":41588,\"start\":40876},{\"end\":42566,\"start\":41590},{\"end\":43044,\"start\":42591},{\"end\":44726,\"start\":43088},{\"end\":45872,\"start\":44771},{\"end\":46088,\"start\":45874},{\"end\":46448,\"start\":46102},{\"end\":46869,\"start\":46493},{\"end\":47690,\"start\":46929},{\"end\":48779,\"start\":47762},{\"end\":49219,\"start\":48808},{\"end\":49792,\"start\":49221}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7574,\"start\":7525},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8062,\"start\":8027},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8705,\"start\":8667},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11772,\"start\":11691},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13960,\"start\":13905},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14999,\"start\":14969},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15040,\"start\":15006},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15863,\"start\":15802},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19475,\"start\":19367},{\"attributes\":{\"id\":\"formula_10\"},\"end\":22511,\"start\":22405},{\"attributes\":{\"id\":\"formula_11\"},\"end\":35356,\"start\":35302},{\"attributes\":{\"id\":\"formula_12\"},\"end\":35646,\"start\":35431},{\"attributes\":{\"id\":\"formula_13\"},\"end\":35797,\"start\":35750},{\"attributes\":{\"id\":\"formula_14\"},\"end\":36086,\"start\":35987},{\"attributes\":{\"id\":\"formula_15\"},\"end\":36216,\"start\":36131},{\"attributes\":{\"id\":\"formula_16\"},\"end\":36356,\"start\":36312},{\"attributes\":{\"id\":\"formula_17\"},\"end\":37014,\"start\":36759},{\"attributes\":{\"id\":\"formula_18\"},\"end\":37183,\"start\":37120},{\"attributes\":{\"id\":\"formula_19\"},\"end\":37634,\"start\":37333},{\"attributes\":{\"id\":\"formula_20\"},\"end\":38114,\"start\":38072},{\"attributes\":{\"id\":\"formula_21\"},\"end\":38553,\"start\":38411}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27837,\"start\":27830},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":27866,\"start\":27859},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":30456,\"start\":30442},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":40180,\"start\":40173},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":41256,\"start\":41249},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":43228,\"start\":43221},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":44980,\"start\":44972},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":46566,\"start\":46554},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":49003,\"start\":48989},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":49242,\"start\":49234}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":6861,\"start\":6812},{\"attributes\":{\"n\":\"3\"},\"end\":14151,\"start\":14130},{\"attributes\":{\"n\":\"4\"},\"end\":24046,\"start\":24035},{\"attributes\":{\"n\":\"4.1\"},\"end\":24585,\"start\":24553},{\"attributes\":{\"n\":\"4.2\"},\"end\":28787,\"start\":28752},{\"end\":30438,\"start\":30431},{\"attributes\":{\"n\":\"5\"},\"end\":32320,\"start\":32310},{\"end\":34504,\"start\":34491},{\"end\":34611,\"start\":34603},{\"end\":38684,\"start\":38661},{\"end\":38919,\"start\":38887},{\"end\":42589,\"start\":42569},{\"end\":43086,\"start\":43047},{\"end\":44769,\"start\":44729},{\"end\":46100,\"start\":46091},{\"end\":46491,\"start\":46451},{\"end\":46927,\"start\":46872},{\"end\":47728,\"start\":47693},{\"end\":47760,\"start\":47731},{\"end\":48806,\"start\":48782},{\"end\":50007,\"start\":49997},{\"end\":50345,\"start\":50335},{\"end\":50549,\"start\":50539},{\"end\":50907,\"start\":50897},{\"end\":51149,\"start\":51139},{\"end\":51910,\"start\":51900},{\"end\":52009,\"start\":51999},{\"end\":52254,\"start\":52225},{\"end\":53033,\"start\":53012},{\"end\":53207,\"start\":53196},{\"end\":53271,\"start\":53260},{\"end\":53471,\"start\":53462},{\"end\":54140,\"start\":54131},{\"end\":54688,\"start\":54679},{\"end\":55851,\"start\":55842},{\"end\":56781,\"start\":56772},{\"end\":57084,\"start\":57075},{\"end\":57508,\"start\":57501},{\"end\":58228,\"start\":58219},{\"end\":58296,\"start\":58287},{\"end\":58819,\"start\":58810},{\"end\":59412,\"start\":59403},{\"end\":60005,\"start\":59996},{\"end\":60601,\"start\":60592},{\"end\":61189,\"start\":61180},{\"end\":61789,\"start\":61779},{\"end\":62389,\"start\":62379},{\"end\":62992,\"start\":62982},{\"end\":63594,\"start\":63584},{\"end\":64962,\"start\":64952},{\"end\":66334,\"start\":66324},{\"end\":67709,\"start\":67699},{\"end\":69084,\"start\":69074},{\"end\":70448,\"start\":70438},{\"end\":71866,\"start\":71856},{\"end\":72047,\"start\":72037},{\"end\":72227,\"start\":72217}]", "table": "[{\"end\":54129,\"start\":53571},{\"end\":54677,\"start\":54207},{\"end\":55840,\"start\":54837},{\"end\":56770,\"start\":55908},{\"end\":57073,\"start\":56921},{\"end\":57499,\"start\":57347},{\"end\":58217,\"start\":57658},{\"end\":58808,\"start\":58433},{\"end\":59401,\"start\":58842},{\"end\":59994,\"start\":59435},{\"end\":60590,\"start\":60031},{\"end\":61178,\"start\":60619},{\"end\":61777,\"start\":61218},{\"end\":62377,\"start\":61818},{\"end\":62980,\"start\":62421},{\"end\":63582,\"start\":63023},{\"end\":64950,\"start\":63652},{\"end\":66322,\"start\":65023},{\"end\":67697,\"start\":66398},{\"end\":69072,\"start\":67772},{\"end\":70436,\"start\":69137},{\"end\":71854,\"start\":70512},{\"end\":72035,\"start\":71932},{\"end\":72215,\"start\":72112},{\"end\":73668,\"start\":72284}]", "figure_caption": "[{\"end\":49995,\"start\":49795},{\"end\":50333,\"start\":50009},{\"end\":50537,\"start\":50347},{\"end\":50895,\"start\":50551},{\"end\":51137,\"start\":50909},{\"end\":51841,\"start\":51151},{\"end\":51898,\"start\":51844},{\"end\":51997,\"start\":51912},{\"end\":52223,\"start\":52011},{\"end\":53010,\"start\":52258},{\"end\":53194,\"start\":53037},{\"end\":53258,\"start\":53210},{\"end\":53460,\"start\":53274},{\"end\":53571,\"start\":53473},{\"end\":54207,\"start\":54142},{\"end\":54837,\"start\":54690},{\"end\":55908,\"start\":55853},{\"end\":56921,\"start\":56783},{\"end\":57347,\"start\":57086},{\"end\":57658,\"start\":57510},{\"end\":58285,\"start\":58230},{\"end\":58433,\"start\":58298},{\"end\":58842,\"start\":58821},{\"end\":59435,\"start\":59414},{\"end\":60031,\"start\":60007},{\"end\":60619,\"start\":60603},{\"end\":61218,\"start\":61191},{\"end\":61818,\"start\":61792},{\"end\":62421,\"start\":62392},{\"end\":63023,\"start\":62995},{\"end\":63652,\"start\":63597},{\"end\":65023,\"start\":64965},{\"end\":66398,\"start\":66337},{\"end\":67772,\"start\":67712},{\"end\":69137,\"start\":69087},{\"end\":70512,\"start\":70451},{\"end\":71932,\"start\":71869},{\"end\":72112,\"start\":72050},{\"end\":72284,\"start\":72230}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":8275,\"start\":8267},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":9793,\"start\":9786},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":10906,\"start\":10899},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":11228,\"start\":11206},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":11378,\"start\":11371},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":12138,\"start\":12129},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18344,\"start\":18338},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18524,\"start\":18518},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18924,\"start\":18915},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19704,\"start\":19697},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20205,\"start\":20198},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20325,\"start\":20318},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27945,\"start\":27929},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28344,\"start\":28337},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":39051,\"start\":39043},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":39296,\"start\":39287},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":40347,\"start\":40338},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":40769,\"start\":40761},{\"end\":40895,\"start\":40886},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":41281,\"start\":41272},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":41676,\"start\":41668},{\"end\":43238,\"start\":43230},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":43950,\"start\":43941},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":44490,\"start\":44482},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":45571,\"start\":45562},{\"end\":45718,\"start\":45711},{\"end\":45960,\"start\":45952},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":47356,\"start\":47334},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":47538,\"start\":47529},{\"end\":48050,\"start\":48042},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":48637,\"start\":48618},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":48976,\"start\":48967},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":49653,\"start\":49644}]", "bib_author_first_name": "[{\"end\":74142,\"start\":74138},{\"end\":74154,\"start\":74153},{\"end\":74469,\"start\":74468},{\"end\":74481,\"start\":74480},{\"end\":74492,\"start\":74491},{\"end\":74503,\"start\":74502},{\"end\":74505,\"start\":74504},{\"end\":74516,\"start\":74515},{\"end\":74525,\"start\":74524},{\"end\":74538,\"start\":74537},{\"end\":74842,\"start\":74841},{\"end\":74844,\"start\":74843},{\"end\":75044,\"start\":75043},{\"end\":75120,\"start\":75119},{\"end\":75227,\"start\":75226},{\"end\":75229,\"start\":75228},{\"end\":75240,\"start\":75239},{\"end\":75242,\"start\":75241},{\"end\":75567,\"start\":75566},{\"end\":75786,\"start\":75785},{\"end\":75795,\"start\":75794},{\"end\":75797,\"start\":75796},{\"end\":75806,\"start\":75805},{\"end\":76094,\"start\":76093},{\"end\":76101,\"start\":76100},{\"end\":76111,\"start\":76110},{\"end\":76118,\"start\":76117},{\"end\":76120,\"start\":76119},{\"end\":76441,\"start\":76440},{\"end\":76447,\"start\":76446},{\"end\":76456,\"start\":76455},{\"end\":76463,\"start\":76462},{\"end\":76602,\"start\":76601},{\"end\":76611,\"start\":76610},{\"end\":76618,\"start\":76617},{\"end\":76620,\"start\":76619},{\"end\":76796,\"start\":76795},{\"end\":76805,\"start\":76804},{\"end\":76812,\"start\":76811},{\"end\":76819,\"start\":76818},{\"end\":76828,\"start\":76827},{\"end\":76830,\"start\":76829},{\"end\":77056,\"start\":77055},{\"end\":77068,\"start\":77067},{\"end\":77078,\"start\":77077},{\"end\":77376,\"start\":77375},{\"end\":77384,\"start\":77383},{\"end\":77815,\"start\":77814},{\"end\":77823,\"start\":77822},{\"end\":77825,\"start\":77824},{\"end\":77840,\"start\":77839},{\"end\":78105,\"start\":78104},{\"end\":78114,\"start\":78113},{\"end\":78123,\"start\":78122},{\"end\":78412,\"start\":78411},{\"end\":78421,\"start\":78420},{\"end\":78433,\"start\":78432},{\"end\":79030,\"start\":79029},{\"end\":79039,\"start\":79038},{\"end\":79049,\"start\":79048},{\"end\":79059,\"start\":79058},{\"end\":79315,\"start\":79314},{\"end\":79325,\"start\":79324},{\"end\":79336,\"start\":79335},{\"end\":79348,\"start\":79347},{\"end\":79350,\"start\":79349},{\"end\":79360,\"start\":79359},{\"end\":79362,\"start\":79361},{\"end\":79628,\"start\":79627},{\"end\":79638,\"start\":79637},{\"end\":79651,\"start\":79650},{\"end\":79664,\"start\":79663},{\"end\":79675,\"start\":79674},{\"end\":80046,\"start\":80045},{\"end\":80048,\"start\":80047},{\"end\":80058,\"start\":80057},{\"end\":80060,\"start\":80059},{\"end\":80419,\"start\":80418},{\"end\":80421,\"start\":80420},{\"end\":80431,\"start\":80430},{\"end\":80441,\"start\":80440},{\"end\":80745,\"start\":80744},{\"end\":80747,\"start\":80746},{\"end\":80757,\"start\":80756},{\"end\":80759,\"start\":80758},{\"end\":81076,\"start\":81075},{\"end\":81414,\"start\":81413},{\"end\":81431,\"start\":81430},{\"end\":81442,\"start\":81441},{\"end\":81455,\"start\":81454},{\"end\":81467,\"start\":81466},{\"end\":81475,\"start\":81474},{\"end\":81987,\"start\":81986},{\"end\":81999,\"start\":81998},{\"end\":82384,\"start\":82383},{\"end\":82396,\"start\":82395},{\"end\":82745,\"start\":82744},{\"end\":82758,\"start\":82757}]", "bib_author_last_name": "[{\"end\":74151,\"start\":74143},{\"end\":74159,\"start\":74155},{\"end\":74478,\"start\":74470},{\"end\":74489,\"start\":74482},{\"end\":74500,\"start\":74493},{\"end\":74513,\"start\":74506},{\"end\":74522,\"start\":74517},{\"end\":74535,\"start\":74526},{\"end\":74554,\"start\":74539},{\"end\":74850,\"start\":74845},{\"end\":75050,\"start\":75045},{\"end\":75128,\"start\":75121},{\"end\":75237,\"start\":75230},{\"end\":75251,\"start\":75243},{\"end\":75574,\"start\":75568},{\"end\":75792,\"start\":75787},{\"end\":75803,\"start\":75798},{\"end\":75823,\"start\":75807},{\"end\":76098,\"start\":76095},{\"end\":76108,\"start\":76102},{\"end\":76115,\"start\":76112},{\"end\":76131,\"start\":76121},{\"end\":76444,\"start\":76442},{\"end\":76453,\"start\":76448},{\"end\":76460,\"start\":76457},{\"end\":76467,\"start\":76464},{\"end\":76608,\"start\":76603},{\"end\":76615,\"start\":76612},{\"end\":76631,\"start\":76621},{\"end\":76802,\"start\":76797},{\"end\":76809,\"start\":76806},{\"end\":76816,\"start\":76813},{\"end\":76825,\"start\":76820},{\"end\":76841,\"start\":76831},{\"end\":77065,\"start\":77057},{\"end\":77075,\"start\":77069},{\"end\":77084,\"start\":77079},{\"end\":77381,\"start\":77377},{\"end\":77390,\"start\":77385},{\"end\":77820,\"start\":77816},{\"end\":77837,\"start\":77826},{\"end\":77846,\"start\":77841},{\"end\":78111,\"start\":78106},{\"end\":78120,\"start\":78115},{\"end\":78126,\"start\":78124},{\"end\":78418,\"start\":78413},{\"end\":78430,\"start\":78422},{\"end\":78438,\"start\":78434},{\"end\":79036,\"start\":79031},{\"end\":79046,\"start\":79040},{\"end\":79056,\"start\":79050},{\"end\":79067,\"start\":79060},{\"end\":79322,\"start\":79316},{\"end\":79333,\"start\":79326},{\"end\":79345,\"start\":79337},{\"end\":79357,\"start\":79351},{\"end\":79369,\"start\":79363},{\"end\":79635,\"start\":79629},{\"end\":79648,\"start\":79639},{\"end\":79661,\"start\":79652},{\"end\":79672,\"start\":79665},{\"end\":79685,\"start\":79676},{\"end\":80055,\"start\":80049},{\"end\":80068,\"start\":80061},{\"end\":80428,\"start\":80422},{\"end\":80438,\"start\":80432},{\"end\":80452,\"start\":80442},{\"end\":80754,\"start\":80748},{\"end\":80766,\"start\":80760},{\"end\":81082,\"start\":81077},{\"end\":81428,\"start\":81415},{\"end\":81439,\"start\":81432},{\"end\":81452,\"start\":81443},{\"end\":81464,\"start\":81456},{\"end\":81472,\"start\":81468},{\"end\":81481,\"start\":81476},{\"end\":81996,\"start\":81988},{\"end\":82005,\"start\":82000},{\"end\":82393,\"start\":82385},{\"end\":82402,\"start\":82397},{\"end\":82755,\"start\":82746},{\"end\":82768,\"start\":82759}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":74466,\"start\":74042},{\"attributes\":{\"id\":\"b1\"},\"end\":74778,\"start\":74468},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":122906757},\"end\":74993,\"start\":74780},{\"attributes\":{\"id\":\"b3\"},\"end\":75115,\"start\":74995},{\"attributes\":{\"id\":\"b4\"},\"end\":75178,\"start\":75117},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":109884250},\"end\":75500,\"start\":75180},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":7553535},\"end\":75730,\"start\":75502},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":8471161},\"end\":76049,\"start\":75732},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":28671436},\"end\":76392,\"start\":76051},{\"attributes\":{\"doi\":\"abs/1512.03385\",\"id\":\"b9\"},\"end\":76597,\"start\":76394},{\"attributes\":{\"doi\":\"abs/1608.06993\",\"id\":\"b10\"},\"end\":76756,\"start\":76599},{\"attributes\":{\"doi\":\"abs/1603.09382\",\"id\":\"b11\"},\"end\":76983,\"start\":76758},{\"attributes\":{\"doi\":\"arXiv:1807.00263\",\"id\":\"b12\"},\"end\":77262,\"start\":76985},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":44300661},\"end\":77704,\"start\":77264},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":85546658},\"end\":78068,\"start\":77706},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":202718866},\"end\":78329,\"start\":78070},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b16\",\"matched_paper_id\":49314079},\"end\":78970,\"start\":78331},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":14542261},\"end\":79251,\"start\":78972},{\"attributes\":{\"doi\":\"abs/1902.02476\",\"id\":\"b18\",\"matched_paper_id\":59608823},\"end\":79547,\"start\":79253},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":44085695},\"end\":79961,\"start\":79549},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":127203550},\"end\":80352,\"start\":79963},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6292807},\"end\":80658,\"start\":80354},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":636054},\"end\":81042,\"start\":80660},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":64295966},\"end\":81363,\"start\":81044},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b24\",\"matched_paper_id\":67749814},\"end\":81889,\"start\":81365},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":9594071},\"end\":82302,\"start\":81891},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":3349576},\"end\":82742,\"start\":82304},{\"attributes\":{\"id\":\"b27\"},\"end\":82898,\"start\":82744}]", "bib_title": "[{\"end\":74136,\"start\":74042},{\"end\":74839,\"start\":74780},{\"end\":75224,\"start\":75180},{\"end\":75564,\"start\":75502},{\"end\":75783,\"start\":75732},{\"end\":76091,\"start\":76051},{\"end\":77373,\"start\":77264},{\"end\":77812,\"start\":77706},{\"end\":78102,\"start\":78070},{\"end\":78409,\"start\":78331},{\"end\":79027,\"start\":78972},{\"end\":79312,\"start\":79253},{\"end\":79625,\"start\":79549},{\"end\":80043,\"start\":79963},{\"end\":80416,\"start\":80354},{\"end\":80742,\"start\":80660},{\"end\":81073,\"start\":81044},{\"end\":81411,\"start\":81365},{\"end\":81984,\"start\":81891},{\"end\":82381,\"start\":82304}]", "bib_author": "[{\"end\":74153,\"start\":74138},{\"end\":74161,\"start\":74153},{\"end\":74480,\"start\":74468},{\"end\":74491,\"start\":74480},{\"end\":74502,\"start\":74491},{\"end\":74515,\"start\":74502},{\"end\":74524,\"start\":74515},{\"end\":74537,\"start\":74524},{\"end\":74556,\"start\":74537},{\"end\":74852,\"start\":74841},{\"end\":75052,\"start\":75043},{\"end\":75130,\"start\":75119},{\"end\":75239,\"start\":75226},{\"end\":75253,\"start\":75239},{\"end\":75576,\"start\":75566},{\"end\":75794,\"start\":75785},{\"end\":75805,\"start\":75794},{\"end\":75825,\"start\":75805},{\"end\":76100,\"start\":76093},{\"end\":76110,\"start\":76100},{\"end\":76117,\"start\":76110},{\"end\":76133,\"start\":76117},{\"end\":76446,\"start\":76440},{\"end\":76455,\"start\":76446},{\"end\":76462,\"start\":76455},{\"end\":76469,\"start\":76462},{\"end\":76610,\"start\":76601},{\"end\":76617,\"start\":76610},{\"end\":76633,\"start\":76617},{\"end\":76804,\"start\":76795},{\"end\":76811,\"start\":76804},{\"end\":76818,\"start\":76811},{\"end\":76827,\"start\":76818},{\"end\":76843,\"start\":76827},{\"end\":77067,\"start\":77055},{\"end\":77077,\"start\":77067},{\"end\":77086,\"start\":77077},{\"end\":77383,\"start\":77375},{\"end\":77392,\"start\":77383},{\"end\":77822,\"start\":77814},{\"end\":77839,\"start\":77822},{\"end\":77848,\"start\":77839},{\"end\":78113,\"start\":78104},{\"end\":78122,\"start\":78113},{\"end\":78128,\"start\":78122},{\"end\":78420,\"start\":78411},{\"end\":78432,\"start\":78420},{\"end\":78440,\"start\":78432},{\"end\":79038,\"start\":79029},{\"end\":79048,\"start\":79038},{\"end\":79058,\"start\":79048},{\"end\":79069,\"start\":79058},{\"end\":79324,\"start\":79314},{\"end\":79335,\"start\":79324},{\"end\":79347,\"start\":79335},{\"end\":79359,\"start\":79347},{\"end\":79371,\"start\":79359},{\"end\":79637,\"start\":79627},{\"end\":79650,\"start\":79637},{\"end\":79663,\"start\":79650},{\"end\":79674,\"start\":79663},{\"end\":79687,\"start\":79674},{\"end\":80057,\"start\":80045},{\"end\":80070,\"start\":80057},{\"end\":80430,\"start\":80418},{\"end\":80440,\"start\":80430},{\"end\":80454,\"start\":80440},{\"end\":80756,\"start\":80744},{\"end\":80768,\"start\":80756},{\"end\":81084,\"start\":81075},{\"end\":81430,\"start\":81413},{\"end\":81441,\"start\":81430},{\"end\":81454,\"start\":81441},{\"end\":81466,\"start\":81454},{\"end\":81474,\"start\":81466},{\"end\":81483,\"start\":81474},{\"end\":81998,\"start\":81986},{\"end\":82007,\"start\":81998},{\"end\":82395,\"start\":82383},{\"end\":82404,\"start\":82395},{\"end\":82757,\"start\":82744},{\"end\":82770,\"start\":82757}]", "bib_venue": "[{\"end\":76210,\"start\":76193},{\"end\":78620,\"start\":78533},{\"end\":81582,\"start\":81557},{\"end\":82105,\"start\":82060},{\"end\":82536,\"start\":82474},{\"end\":74229,\"start\":74161},{\"end\":74612,\"start\":74556},{\"end\":74874,\"start\":74852},{\"end\":75041,\"start\":74995},{\"end\":75322,\"start\":75253},{\"end\":75604,\"start\":75576},{\"end\":75864,\"start\":75825},{\"end\":76191,\"start\":76133},{\"end\":76438,\"start\":76394},{\"end\":76793,\"start\":76758},{\"end\":77053,\"start\":76985},{\"end\":77460,\"start\":77392},{\"end\":77868,\"start\":77848},{\"end\":78190,\"start\":78128},{\"end\":78512,\"start\":78444},{\"end\":79092,\"start\":79069},{\"end\":79389,\"start\":79385},{\"end\":79736,\"start\":79687},{\"end\":80141,\"start\":80070},{\"end\":80496,\"start\":80454},{\"end\":80829,\"start\":80768},{\"end\":81120,\"start\":81084},{\"end\":81527,\"start\":81487},{\"end\":82058,\"start\":82007},{\"end\":82472,\"start\":82404},{\"end\":82814,\"start\":82770}]"}}}, "year": 2023, "month": 12, "day": 17}