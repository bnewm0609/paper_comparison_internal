{"id": 207796611, "updated": "2022-02-02 06:09:14.753", "metadata": {"title": "Beyond the Single Neuron Convex Barrier for Neural Network Certification", "authors": "[{\"middle\":[],\"last\":\"Singh\",\"first\":\"Gagandeep\"},{\"middle\":[],\"last\":\"Ganvir\",\"first\":\"Rupanshu\"},{\"middle\":[],\"last\":\"P\u00fcschel\",\"first\":\"Markus\"},{\"middle\":[],\"last\":\"Vechev\",\"first\":\"Martin\"}]", "venue": "NeurIPS", "journal": "15072-15083", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "We propose a new parametric framework, called k-ReLU, for computing precise and scalable convex relaxations used to certify neural networks. The key idea is to approximate the output of multiple ReLUs in a layer jointly instead of separately. This joint relaxation captures dependencies between the inputs to different ReLUs in a layer and thus overcomes the convex barrier imposed by the single neuron triangle relaxation and its approximations. The framework is parametric in the number of k ReLUs it considers jointly and can be combined with existing verifiers in order to improve their precision. Our experimental results show that k-ReLU enables significantly more precise certification than existing state-of-the-art verifiers while maintaining scalability.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2991226929", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/SinghGPV19", "doi": null}}, "content": {"source": {"pdf_hash": "14b0b74582f6001489dc3ba03bc192e17a90449d", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "3d59ab04370720101e583bc217f9ed4004356baf", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/14b0b74582f6001489dc3ba03bc192e17a90449d.txt", "contents": "\nBeyond the Single Neuron Convex Barrier for Neural Network Certification\n\n\nGagandeep Singh \nDepartment of Computer Science\nETH Zurich\nSwitzerland\n\nRupanshu Ganvir \nDepartment of Computer Science\nETH Zurich\nSwitzerland\n\nMarkus P\u00fcschel \nDepartment of Computer Science\nETH Zurich\nSwitzerland\n\nMartin Vechev martin.vechev@inf.ethz.ch2rganvir@student.ethz.ch \nDepartment of Computer Science\nETH Zurich\nSwitzerland\n\nBeyond the Single Neuron Convex Barrier for Neural Network Certification\n\nWe propose a new parametric framework, called k-ReLU, for computing precise and scalable convex relaxations used to certify neural networks. The key idea is to approximate the output of multiple ReLUs in a layer jointly instead of separately. This joint relaxation captures dependencies between the inputs to different ReLUs in a layer and thus overcomes the convex barrier imposed by the single neuron triangle relaxation and its approximations. The framework is parametric in the number of k ReLUs it considers jointly and can be combined with existing verifiers in order to improve their precision. Our experimental results show that k-ReLU enables significantly more precise certification than existing state-of-the-art verifiers while maintaining scalability.\n\nIntroduction\n\nNeural networks are being increasingly used in many safety critical domains including autonomous driving, medical devices, and face recognition. Thus, it is important to ensure they are provably robust and cannot be fooled by adversarial examples [1]: small changes to a given image that can fool the network into making a wrong classification. To address this challenge, a range of verification techniques were introduced recently ranging from exact but expensive methods based on SMT solvers [2][3][4], mixed integer linear programming [5], and Lipschitz optimization [6] to approximate and incomplete, but more scalable methods based on abstract interpretation [7][8][9], duality [10,11], semi definite [12,13] and linear relaxations [14][15][16][17]. Recently, combinations of approximate methods with solvers have been used for producing more precise results than approximate methods alone while also being more scalable than exact methods [18,19].\n\nThe key challenge any verification method must address is computing the output of ReLU assignments where the input can take both positive and negative values. Exact computation must consider two paths per neuron, which quickly becomes infeasible due to a combinatorial explosion while approximate methods trade precision for scalability via a convex relaxation of ReLU outputs.\n\nThe most precise convex relaxation of ReLU output is based on the convex hull of Polyhedra [20] which is practically infeasible as it requires an exponential number of convex hull computations, each with a worst-case exponential complexity in the number of neurons. The most common convex relaxation of y 1 :=ReLU(x 1 ) used in practice [17,5] is the triangle relaxation from [3]. We note that other works such as [8,9,[14][15][16]11] approximate this relaxation. The triangle relaxation creates constraints only between y 1 and x 1 and is optimal in the x 1 y 1 -plane. Because of this optimality, recent work [17] refers to the triangle relaxation as the convex barrier, meaning the best convex approximation one can obtain when processing each ReLU separately. However, the triangle relaxation is not optimal when one considers multiple neurons at a time as it ignores all dependencies between x 1 and any other neuron x 2 in the same layer, and thus loses precision. This work: beyond the single neuron convex barrier In this work, we address this issue by proposing a novel parameterized framework, called k-ReLU, for generating convex approximations that consider multiple ReLUs jointly. Here, the parameter k determines how many ReLUs are considered jointly with large k resulting in more precise output. For example, unlike prior work, our framework can generate a convex relaxation for y 1 :=ReLU(x 1 ) and y 2 :=ReLU(x 2 ) that is optimal in the x 1 x 2 y 1 y 2 -space. We next illustrate this point with an example.\n\nPrecision gain with k-ReLU on an example Consider the input space of x 1 x 2 as defined by the blue area in Fig. 1 and the ReLU operations y 1 :=ReLU(x 1 ) and y 2 :=ReLU(x 2 ). The input space is bounded by the relational constraints\nx 2 \u2212 x 1 \u2264 2, x 1 \u2212 x 2 \u2264 2, x 1 + x 2 \u2264 2 and \u2212x 1 \u2212 x 2 \u2264 2.\nThe relaxations produced are in a four dimensional space of x 1 x 2 y 1 y 2 . For simplicity of presentation, we show the feasible shape of y 1 y 2 as a function of z = x 1 + x 2 .\n\nThe triangle relaxation from [3] is in fact a special case of our framework with k = 1, that is, 1-ReLU. 1-ReLU independently computes two relaxations -one in the x 1 y 1 space and the other in the x 2 y 2 space. The final relaxation is the cartesian product of the feasible sets of the two individually computed relaxations and is oblivious to any correlations between x 1 and x 2 . The relaxation adds triangle constraints {y 1 \u2265 0, y 1 \u2265 x 1 , y 1 \u2264 0.5 \u00b7 x 1 + 1} between x 1 and y 1 as well as {y 2 \u2265 0, y 2 \u2265 x 2 , y 2 \u2264 0.5 \u00b7 x 2 + 1} between x 2 and y 2 .\n\nIn contrast, 2-ReLU considers the two ReLU's jointly and captures the relational constraints between x 1 and x 2 . 2-ReLU computes the following relaxation:\n{y 1 \u2265 0, y 1 \u2265 x 1 , y 2 \u2265 0, y 2 \u2265 x 2 , 2 \u00b7 y 1 + 2 \u00b7 y 2 \u2212 x 1 \u2212 x 2 \u2264 2}\nThe polytope produced is shown in Fig. 1c. Note that in this case the shape of y 1 y 2 is not independent of x 1 + x 2 as opposed to the triangle relaxation. At the same time, it is more precise than Fig. 1b for all values of z.\n\nMain contributions Our main contributions are:\n\n\u2022 A novel framework, called k-ReLU, that computes optimal convex relaxations for the output of k ReLU operations jointly. k-ReLU is generic and can be combined with existing verifiers for improved precision while maintaining scalability. Further, k-ReLU is also adaptive and can be tuned to balance precision and scalability by varying k.\n\n\u2022 A method for computing approximations of the optimal relaxations for larger k, which is more precise than simply using l < k.\n\n\u2022 An instantiation of k-ReLU with the recent DeepPoly convex relaxation [9] resulting in a verifier called kPoly.\n\n\u2022 An evaluation showing kPoly is more precise and scalable than the state-of-the-art verifiers [9,19] on the task of certifying neural networks of up to 100K neurons against challenging adversarial perturbations (e.g., L \u221e balls with = 0.3).\n\nWe note that the work of [12] computes semi definite relaxations that consider multiple ReLUs jointly, however these are not optimal and do not scale to the large networks used in our experiments. Precision gain in practice Table 1 quantitatively compares the precision of kPoly instantiated with three relaxations: k = 1, k = 2, and k = 3. We measure the volume of the output bounding box computed after propagating an L \u221e -ball of radius = 0.015 through a deep, fully connected MNIST network with 9 layers containing 200 neurons each. We can observe that the volume of the output from 3-ReLU and 2-ReLU is respectively 9 and 7 orders of magnitude smaller than from 1-ReLU.\n\nWe note that the networks we consider, as for example the 9 \u00d7 200 network above, are especially challenging for state-of-the-art verifiers as these methods either lose unnecessary precision [8,9,14,15,19,16,17] or simply do not scale [5,18,12,4,13,10].\n\nFinally, we remark that while we consider robustness certification against norm based perturbations in our evaluation, our framework can also be used for precise and scalable verification of other network safety properties such as stability [21] or robustness against geometric and semantic perturbations [9,22,23].\n\n\nOverview of k-ReLU\n\nWe now show, on a simple example, that the k-ReLU concept can be used to improve the results of state-of-the-art verifiers. In particular, we illustrate how the output of our verifier kPoly instantiated with 1-ReLU is refined by instantiating it with 2-ReLU. This is possible as the 2-ReLU relaxation can capture extra relationships between neurons that 1-ReLU inherently cannot.\n\nConsider the simple feedforward neural network with ReLU activations shown in Fig. 2. The network has two inputs each taking values independently in the range [\u22121, 1], one hidden layer and one output layer each containing two neurons. For simplicity, we split each layer into two parts: one for the affine transformation and the other for the ReLU. The weights of the affine transformation are shown on the arrows and the biases are above or below the respective neuron. The goal is to verify that x 9 \u2264 4 holds for the output x 9 with respect to all inputs.\n\nWe first show that 1-ReLU instantiated with the state-of-the-art DeepPoly [9] relaxation fails to verify the property. DeepPoly, described formally in Section 4, associates two pairs of lower and upper bounds with each neuron\nx i : (a \u2264 i , a \u2265 i ) and (l i , u i ). Here, a \u2264 i and a \u2265 i have the form j a j \u00b7 x j + c where c, l i , u i , \u2208 R \u222a {\u2212\u221e,\n+\u221e} and a j \u2208 R. The bounds computed by the verifier using this instantiation are shown as annotations in Fig. 2.\n\nFirst Layer The verifier starts by computing the bounds for x 1 and x 2 which are simply taken from the input specification resulting in:\nx 1 \u2265 \u22121, x 1 \u2264 1, l 1 = \u22121, u 1 = 1, and x 2 \u2265 \u22121, x 2 \u2264 1, l 2 = \u22121, u 2 = 1.\nSecond Layer Next, the affine assignments x 3 := x 1 +x 2 and x 4 := x 1 \u2212x 2 are handled. DeepPoly handles affine transformations exactly and thus no precision is lost. The affine transformation results in the following bounds for x 3 and x 4 :\nx 3 \u2265 x 1 + x 2 , x 3 \u2264 x 1 + x 2 , l 3 = \u22122, u 3 = 2, x 4 \u2265 x 1 \u2212 x 2 , x 4 \u2264 x 1 \u2212 x 2 , l 4 = \u22122, u 4 = 2.\nDeepPoly can precisely handle ReLU assignments when the input neuron takes only positive or negative values, otherwise it loses precision. Since x 3 and x 4 can take both positive and negative values, an approximation based on the triangle relaxation is applied which for x 5 yields:\nx 5 \u2265 0, x 5 \u2264 1 + 0.5 \u00b7 x 3 .(1)\nNote that DeepPoly discards the other lower bound x 5 \u2265 x 3 from the triangle relaxation. The lower bound l 5 is set to 0 and the relation x 3 \u2264 x 1 + x 2 is substituted for x 3 in (1) for computing the upper bound which yields l 5 = 0, u 5 = 2. Analogously, for x 6 we obtain:\nx 6 \u2265 0, x 6 \u2264 1 + 0.5 \u00b7 x 4 , l 6 = 0, u 6 = 2.(2)\nx 1\n\nx 2\n\nx 3\n\nx 4\n\nx 5\n\nx 6\n\nx 7\n\nx 8\n\nx 9\n\nx 10 Third Layer Next, the affine assignments x 7 := x 5 + 2x 6 and x 8 := x 6 + 1.5 are handled. DeepPoly adds the constraints:\n[-1,1] [-1,1] 1 1 1 -1 max(0, x 3 ) max(0, x 4 ) 1 0 2 1 max(0, x 7 ) max(0, x 8 ) 0 0 0 1.5 x 1 \u2265 \u22121 x 1 \u2264 1 l 1 = \u22121 u 1 = 1 x 2 \u2265 \u22121 x 2 \u2264 1 l 2 = \u22121 u 2 = 1 x 3 \u2265 x 1 + x 2 x 3 \u2264 x 1 + x 2 l 3 = \u22122 u 3 = 2 x 4 \u2265 x 1 \u2212 x 2 x 4 \u2264 x 1 \u2212 x 2 l 4 = \u22122 u 4 = 2 x 5 \u2265 0 x 5 \u2264 1 + 0.5 \u00b7 x 3 l 5 = 0 u 5 = 2 x 6 \u2265 0 x 6 \u2264 1 + 0.5 \u00b7 x 4 l 6 = 0 u 6 = 2 x 7 \u2265 x 5 + 2 \u00b7 x 6 x 7 \u2264 x 5 + 2 \u00b7 x 6 l 7 = 0 u 7 = 5 x 8 \u2265 x 6 + 1.5 x 8 \u2264 x 6 + 1.5 l 8 = 1.5 u 8 = 3.5 x 9 \u2265 x 7 x 9 \u2264 x 7 l 9 = 0 u 9 = 5 x 10 \u2265 x 8 x 10 \u2264 x 8 l 10 = 1.5 u 10 = 3.5 1-ReLU x 3 + x 4 \u2264 2, x 3 \u2212 x 4 \u2264 2, x 4 \u2212 x 3 \u2264 2, \u2212x 3 \u2212 x 4 \u2264 2 2 \u00b7 x 5 + 2 \u00b7 x 6 \u2212 x 3 \u2212 x 4 \u2264 2 x 7 \u2264 4, x 9 \u2264 4 2-ReLUx 7 \u2265 x 5 + 2 \u00b7 x 6 , x 7 \u2264 x 5 + 2 \u00b7 x 6 , x 8 \u2265 x 6 + 1.5, x 8 \u2264 x 6 + 1.5,(3)\nTo compute the upper and lower bounds for x 7 and x 8 , DeepPoly substitutes the polyhedral constraints for x 5 and x 6 from (1) and (2) in (3). It again substitutes for the constraints for x 5 and x 6 in terms of x 3 and x 4 and iterates until it reaches the input layer where it substitutes the concrete bounds for x 1 and x 2 . Doing so yields l 7 = 0, u 7 = 5 and l 8 = 1.5, u 8 = 3.5.\n\nRefinement with 1-ReLU fails Because DeepPoly discards one of the lower bounds from the triangle relaxations for the ReLU assignments in the previous layer, it is possible to refine lower and upper bounds for x 7 and x 8 by encoding the network upto the final affine transformation using the relatively tighter ReLU relaxations based on the triangle formulation and then computing bounds for x 7 and x 8 with respect to this formulation. However, this does not improve bounds and still yields l 7 = 0, u 7 = 5, l 8 = 1.5, u 8 = 3.5.\n\nAs the lower bounds for both x 7 and x 8 are non-negative, the DeepPoly ReLU approximation simply propagates x 7 and x 8 to the output layer. The final output is thus:\nx 9 \u2265 x 7 , x 9 \u2264 x 7 , l 9 = 0, u 9 = 5, x 10 \u2265 x 8 , x 10 \u2264 x 8 , l 10 = 1.5, u 10 = 3.5.\nBecause the upper bound is u 9 = 5, the verifier fails to prove the property that x 9 \u2264 4.\n\nRefinement with 2-ReLU verifies the property Now we consider refinement with our 2-ReLU relaxation which considers the two ReLU assignments x 5 := ReLU (x 3 ) and x 6 := ReLU (x 4 ) jointly. Besides the box constraints for x 3 and x 4 , it also considers the constraints\nx 3 + x 4 \u2264 2, x 3 \u2212 x 4 \u2264 \u22122, \u2212x 3 \u2212 x 4 \u2264 2, x 4 \u2212 x 3 \u2264 2\nfor computing the output of ReLU. The ReLU output contains the extra constraint 2 \u00b7 x 5 + 2 \u00b7 x 6 \u2212 x 3 \u2212 x 4 \u2264 2 that 1-ReLU cannot capture. We again encode the network upto the final affine transformation with the tighter ReLU relaxations obtained using 2-ReLU and refine the bounds for x 7 , x 8 . Now, we obtain better upper bounds as u 7 = 4. The better bound for u 7 is then propagated to u 9 and is sufficient for proving the desired property.\n\nWe remark that while in this work we instantiate the k-ReLU concept with the DeepPoly relaxation, the idea can be applied to other relaxation [11, 7-10, 12, 14, 15, 17, 18].\n\n\nk-ReLU relaxation framework\n\nIn this section we formally describe our k-ReLU framework for generating optimal convex relaxations in the input-output space for k ReLU operations jointly. In the next section, we discuss the instantiation of our framework with existing verifiers which enables more precise results.\n\nWe consider a ReLU based feedforward, convolutional or residual neural network with h neurons from a set H (that is h = |H|) and a bounded input region I \u2286 R m where m < h is the number of neural network inputs. In our exposition, we treat the affine transformation and the ReLUs as separate layers. We consider a convex approximation method M that processes network layers in sequence from the input to the output layer passing the output of predecessor layers as input to the successor layers. Let S \u2286 R h be a convex set computed via M approximating the set of values that neurons upto layer l-1 can take with respect to I and B \u2287 S be the smallest bounding box around S. We use Conv(S 1 , S 2 ) and S 1 \u2229 S 2 to denote the convex hull and the intersection of convex sets S 1 and S 2 , respectively.\n\nLet X , Y \u2286 H be respectively the set of input and output neurons in the l-th layer consisting of n ReLU assignments of the form y i :=ReLU(x i ) where x i \u2208 X and y i \u2208 Y. In the general case, each input neuron x i takes on both positive and negative values in S. We define the polyhedra induced by the two branches of each ReLU assignment y i :=ReLU(\nx i ) as C + i = {x i \u2265 0, y i = x i } \u2286 R h and C \u2212 i = {x i \u2264 0, y i = 0} \u2286 R h . Let Q J = { i\u2208J C s(i) i | s \u2208 J \u2192 {\u2212, +}} (where J \u2286 [n]}) be the set of polyhedra Q \u2286 R h constructed by the intersection of polyhedra C i \u2286 R h for neurons x i , y i indexed by the set J such that each C i \u2208 {C + i ,C \u2212 i }.\nWe next formulate the best convex relaxation of the output after n ReLU assignments.\n\n\nBest convex relaxation\n\nThe best convex relaxation after the n ReLU assignments is given by\nS best = Conv Q\u2208Q [n] (Q \u2229 S).(4)\nS best considers all n assignments jointly. Computing it is practically infeasible as it involves computing 2 n convex hulls each of which has exponential cost in the number of neurons h [24].\n\n\n1-ReLU\n\nWe now describe the prior convex relaxation [3] through triangles (here called 1-ReLU) that handles the n ReLU assignments separately. Here, the input to the i-th assignment y i :=ReLU(x i ) is the polyhedron P 1-ReLU \u2287 S where for each x i \u2208 X , P 1-ReLU,i contains only an interval constraint [l i , u i ] that bounds x i , that is, l i \u2264 x i \u2264 u i . Here, the interval bounds are simply obtained from the bounding box B of S. The output of this method after n assignments is\nS 1-ReLU = S \u2229 n i=1 Conv(P 1-ReLU,i \u2229 C + i , P 1-ReLU,i \u2229 C \u2212 i ).(5)\nThe projection of Conv(P 1-ReLU,i \u2229 C + i , P 1-ReLU,i \u2229 C \u2212 i ) onto the x i y i -plane is a triangle minimizing the area and is the optimal convex relaxation in this plane. However, because the input polyhedron P 1-ReLU is a hyperrectangle (when projected to X ), it does not capture relational constraints between different x i 's in X (meaning it typically has to substantially over-approximate the set S). Thus, as expected, the computed result S 1-ReLU of the 1-ReLU method will incur significant imprecision when compared with the S best result.\n\n\nk-ReLU relaxations\n\nWe now describe our k-ReLU framework for computing a convex relaxation of the output of n ReLUs in one layer by considering groups of k ReLUs jointly with k > 1. For simplicity, we assume that n > k and k divides n. Let J be a partition of the set of indices [n] such that each block J i \u2208 J contains exactly k indices. Let P k-ReLU,i \u2286 R h be a polyhedron containing interval and relational constraints over the neurons from X indexed by J i . In our framework, P k-ReLU,i is derived via B and S and satisfies S \u2286 P k-ReLU,i . \n2x1 + x2 + x3 \u2212 y1 \u2264 0 y2 + x2 \u2212 x3 \u2264 \u22121 y3 \u2212 x1 + x3 \u2264 1 . . . . . . (S \u2229 n/k i=1\nKi) as per (6) Figure 3: Steps to instantiating the k-ReLU framework.\n\nOur k-ReLU framework produces the following convex relaxation of the output:\nS k-ReLU = S \u2229 n/k i=1 Conv Q\u2208Q J i (P k-ReLU,i \u2229 Q).(6)\nThe result of (6) is the optimal convex relaxation for the output of n ReLUs for the given choice of S, k, J , and P k-ReLU,i . Theorem 3.1. For k > 1 and a partition J of indices, if there exists a J i for which P k-ReLU,i u\u2208Ji P 1-ReLU,u holds, then S k-ReLU S 1-ReLU . The proof of Theorem 3.1 is given in appendix. Note that P 1-ReLU only contains interval constraints whereas P k-ReLU contains both, the same interval constraints and extra relational constraints. Thus, any convex relaxation obtained using k-ReLU is typically strictly more precise than a 1-ReLU one.\n\nPrecise and scalable relaxations for large k For each J i , the optimal convex relaxation K i = Conv Q\u2208Q J i (P k-ReLU,i \u2229 Q) from (6) requires computing the convex hull of 2 k convex sets each of which has a worst-case exponential cost in terms of k. Thus, computing K i via (6) can become computationally expensive for large values of k. We propose an efficient relaxation K i for each block J i \u2208 J (where |J i |= k as described earlier) based on computing relaxations for all subsets of J i that are of size 2 \u2264 l < k. Let R i = {{j 1 , . . . , j l } | j 1 , . . . , j l \u2208 J i } be the set containing all subsets of J i containing l indices. For each R \u2208 R i , let P l-ReLU,R \u2286 R h be a polyhedron containing interval and relational constraints between the neurons from X indexed by R with S \u2286 P l-ReLU,R .\n\nThe relaxation K i is computed by applying l-ReLU k l times as:\nK i = R\u2208Ri Conv Q\u2208Q R (P l-ReLU,R \u2229 Q).(7)\nThe layerwise convex relaxation S k- (7) is tighter than computing relaxation S l-ReLU via (6) with a partition J where for each block J i \u2208 J there exists R j corresponding to a block of J such that J i \u2208 R j and P l-ReLU,J i \u2286 P l-ReLU,J i where P l-ReLU,J i is the polyhedron in (6) for computing S l-ReLU . In our instantiations, we ensure that this condition holds for gaining precision.\nReLU = S \u2229 n/k i=1 K i via\n\nInstantiating the k-ReLU framework\n\nOur k-ReLU framework from Section 3 can be instantiated to produce different relaxations depending on the parameters S, k, J , and P k-ReLU,i . Fig. 3 shows the steps to instantiating our framework. The inputs to the framework is the convex set S and the partition J based on k. These inputs are first used to produce a set containing n/k polyhedra {P k-ReLU,i }. Each polyhedron P k-ReLU,i is then intersected with polyhedra from the set Q Ji producing 2 k polyhedra which are then combined via the convex hull (each called K i ). The K i 's are then combined with S to produce the final relaxation that captures the values which neurons can take after the ReLU assignments. This relaxation is tighter than that produced by applying M directly on the ReLU layer enabling precision gains. \n\n\nComputing key parameters\n\nWe next describe the choice of the key parameters S, k, J , P k-ReLU,i in our framework.\n\nInput convex set Examples of convex approximation method M for computing S include [11, 7-10, 12, 14, 15, 17, 18]. In this paper, we use the DeepPoly [9] relaxation for computing S which is a state-of-the-art precise and scalable verifier for neural networks.\n\nk and partition J We use (6) to compute the output relaxation when k \u2208 {2, 3}. For larger k, we compute the output based on (7). To maximize precision gain, we group those indices i together into a block where the triangle relaxation for y i :=ReLU(x i ) has the larger area in the x i y i -plane.\n\nComputing P krelu,i We note that for a fixed block J i , several polyhedron P k-ReLU,i are possible that produce convex relaxations with varying degree of precision. Ideally, one would like P k-ReLU,i to be the projection of S onto the variables in the set X indexed by the block J i . However, computing this projection exactly is expensive and therefore we compute a relaxation of it.\n\nWe use the method M to compute P k-ReLU,i by computing the upper bounds for linear relational expressions of the form k u=1 a u \u00b7 x u with respect to S. In our experiments, we found that setting a u \u2208 {\u22121, 0, 1} yields maximum precision (except the case where all possible a u are zero). Thus P k-ReLU,i \u2287 S contains 3 k \u2212 1 constraints which include the interval constraints for all x u .\n\n\nVerification with k-ReLU framework\n\nLet \u03c8 \u2286 R h be a convex set defining a safe region for the outputs with respect to the input region I and S O \u2286 R h be the output convex relaxation obtained after processing affine layers with the convex approximation method M and ReLU layers with our k-ReLU framework. \u03c8 holds if \u03c8 \u2286 S O .\n\n\nInstantiation with DeepPoly\n\nWe now show to instantiate the k-ReLU framework with DeepPoly [9]. DeepPoly is a type of a restricted Polyhedra abstraction which balances scalability and precision of the analysis. It associates four constraints per neuron h i \u2208 H: (a) a lower polyhedral constraint of the form a \u2264 i \u2264 h i , (b) an upper polyhedral constraint h i \u2264 a \u2265 i , (c) a lower bound constraint l i \u2264 h i , and (d) an upper bound constraint h i \u2264 u i . The polyhedral expressions a \u2264 i , a \u2265 i are of the form j a j \u00b7h j +c where a j , c \u2208 R and capture relational information ensuring that DeepPoly is exact for affine transformations. The analysis proceeds layer by layer and thus the polyhedral constraints for a neuron in layer l contain only the neurons upto layer l-1. S here is the set of points satisfying DeepPoly constraints for all neurons. We next discuss how the k-ReLU framework can be used for improving the precision of the ReLU transformer for DeepPoly and also that of the overall verification procedure.\n\nImproving the precision of DeepPoly ReLU relaxation DeepPoly loses precision for ReLU assignments y i :=ReLU(x i ) where x i can take both positive and negative values. It computes convex relaxations shown in Fig. 4 (a) and (b). It keeps the one with smaller area in the x i y i -plane.We note that both of these relaxations depend only on the interval bounds l i , u i for x i . DeepPoly uses backsubstitution (see [9] for details) for obtaining precise bounds l i , u i . We note that DeepPoly ReLU relaxations are weaker than the 1-ReLU relaxation (as they contain one constraint less than the triangle). This precision loss accumulates as the analysis proceeds deeper in the network. We now show how the k-ReLU framework can recover precision for DeepPoly.\n\nWe compute refined bounds l i , u i for those neurons x i that are inputs to a ReLU and can take positive values. We maximize and minimize x i with respect to the convex relaxation produced by replacing the DeepPoly ReLU relaxation ( Fig. 4 (a) and (b)) with our k-ReLU relaxations based on (6). Since the constraints from both DeepPoly and k-ReLU are linear, we use a LP solver for maximizing and minimizing. l i and u i facilitate the two DeepPoly ReLU relaxations shown in green in Fig. 4 (a) and (b). These relaxations are tighter than the original ones and improve the precision of DeepPoly.\n\nk-ReLU for improving robustness certification When DeepPoly alone cannot prove the target property, we instead check if \u03c8 holds with the tighter ReLU relaxations from k-ReLU via LP solver.\n\n\nEvaluation\n\nWe instantiated our k-ReLU framework with DeepPoly in the form of a verifier called kPoly. kPoly is written in Python and uses cdd [25,26] for computing convex hulls, and Gurobi [27] for refining DeepPoly ReLU relaxations and proving that \u03c8 holds with k-ReLU relaxations. We made kPoly publicly available as part of the ERAN [28] framework for neural network verification. We evaluated kPoly for the task of robustness certification of challenging deep neural networks. We compare kPoly against two state-of-the-art verifiers: DeepPoly [9] and RefineZono [19]. DeepPoly has the same precision as [15,16] whereas RefineZono refines the results of DeepZ [8] and is more precise than [8,11,14]. Both, DeepPoly and RefineZono are more scalable than [5,18,12,4,13,10], however we show that kPoly is more precise than DeepPoly and RefineZono while also scaling to large networks. We next describe the neural networks, benchmarks and parameters used in our experiments.\n\nNeural networks We used 9 MNIST [31] and CIFAR10 [32] fully connected (FNNs), convolutional (CNNs), and residual networks with ReLU activations shown in Table 2. The first 8 networks in Table 2 are available at https://github.com/eth-sri/eran; the residual network is taken from https://github.com/locuslab/convex_adversarial. Five of the networks do not use adversarial training while the rest use different variants of it. The MNIST ConvBig network is trained with DiffAI [29], the two CIFAR10 convolutional networks are trained with PGD [30] and the residual network is trained via [11]. The largest network in our experiments contains > 100K neurons and has 13 layers.\n\n\nRobustness property\n\nWe consider the L \u221e -norm [33] based adversarial region around a correctly classified image from the test set parameterized by the radius \u2208 R. Our goal is to certify that the network classifies all images in the adversarial region correctly.\n\nMachines The runtimes of all experiments for the MNIST FNNs were measured on a 3.3 GHz 10 Core Intel i9-7900X Skylake CPU with a main memory of 64 GB whereas the experiments for the rest were run on a 2.6 GHz 14 core Intel Xeon CPU E5-2690 with 512 GB of main memory. Benchmarks For each MNIST and CIFAR10 network, we selected the first 1000 images from the respective test set and filtered out incorrectly classified images. The number of correctly classified images by each network are shown in Table 3. We chose challenging values for defining the adversarial region for each network. We note that our benchmarks (e.g., the 9 \u00d7 200 network with = 0.015) are quite challenging to handle for state-of-the-art verifiers (as we will see below).\n\nk-ReLU parameters for the experiments We refine both the DeepPoly ReLU relaxation and the verification results for the MNIST FNNs whereas only the verification results are refined for the rest. All neurons that can take positive values after the affine transformation are selected for refinement.\n\nAs an optimization, we use the MILP ReLU encoding from [5] when refining the ReLU relaxation for the second ReLU layer. The last column of Table 2 shows the value of k for all networks. For the MNIST and CIFAR10 ConvBig networks, we encode the first 3 ReLU layers with 1-ReLU while the remaining are encoded with 5-ReLU. We use l = 3 in (7) for encoding 5-ReLU. For the remaining 3 networks, we encode the first ReLU layer with 1-ReLU while the remaining layers are encoded adaptively. Here, we choose a value of k for which the total number of calls to 3-ReLU is \u2264 500. We next discuss our experimental results shown in Table 3.\n\nkPoly vs DeepPoly and RefineZono Table 3 compares the precision in number of adversarial regions verified and the average runtime per image in seconds for kPoly, DeepPoly and RefineZono. We refine the verification results with RefineZono and kPoly only when DeepZ and DeepPoly fails to verify. kPoly is more precise than both DeepPoly and RefineZono on all networks. RefineZono is more precise than DeepPoly on the networks trained without adversarial training. On the 9 \u00d7 200 and MNIST ConvSmall networks, kPoly verifies 506 and 347 regions respectively whereas RefineZono verifies 316 and 179 regions respectively. The precision gain is less on networks with adversarial training and kPoly verifies 25, 40, 38 and 2 regions more than DeepPoly on the last 4 networks in Table 3. kPoly is faster than RefineZono on all networks and has an average runtime of < 8 minutes.\n\nThe largest runtimes are on the MNIST 9 \u00d7 200 and ConvSmall networks. These are quite small compared to the CIFAR10 ResNet network where kPoly has an average runtime of only 91 seconds.\n\n\n1-ReLU vs k-ReLU\n\nWe consider the first 100 regions for the MNIST ConvSmall network and compare the number of regions verified by kPoly when run with k-ReLU and 1-ReLU. We note that kPoly run with 1-ReLU is equivalent to [17]. kPoly with 1-ReLU verifies 20 regions whereas with k-ReLU it verifies 35. kPoly with 1-ReLU has an average runtime of 9 seconds.\n\nEffect of heuristic for J We ran kPoly based on k-ReLU with random partitioning J r using the same setup as for 1-ReLU. We observed that kPoly produced worse bounds and verified 34 regions.\n\n\nConclusion\n\nWe presented k-ReLU, a novel parametric framework which produces more precise results than the single neuron triangle convex relaxation. The key idea of k-ReLU is to consider multiple ReLUs jointly. We showed k-ReLU leads to significantly improved precision, enabling us to prove properties beyond the reach of prior work, while preserving scalability.\n\n\nA Appendix\n\nA.1 Proof of Theorem 3.1\n\nProof. Since P k-ReLU,i u\u2208Ji P 1-ReLU,u for J i , by monotonicity of intersection and convex hull,\nConv Q\u2208Q J i (P k-ReLU,i \u2229 Q) Conv Q\u2208Q J i (( u\u2208Ji P 1-ReLU,u ) \u2229 Q)(8)\nFor any Q \u2208 Q Ji , we have that either Q \u2286 C + u or Q \u2286 C \u2212 u for u \u2208 J i . Thus, we can replace all Q on the right hand side of (8) with either C + u or C \u2212 u such that for all u \u2208 J i both C + u and C \u2212 u are used at least in one substitution and obtain by monotonicity,\n\u2286 Conv u\u2208Ji (( u\u2208Ji P 1-ReLU,u ) \u2229 C + u , ( u\u2208Ji P 1-ReLU,u ) \u2229 C \u2212 u )\n\u2286 Conv u\u2208Ji (P 1-ReLU,u \u2229 C + u , P 1-ReLU,u \u2229 C \u2212 u ) ( u\u2208Ji P 1-ReLU,u \u2286 P 1-ReLU,u ).\n\nFor remaining i, similarly Conv Q\u2208Qi (P k-ReLU,i \u2229 Q) \u2286 Conv u\u2208Ji (P 1-ReLU,u \u2229 C + u , P 1-ReLU,u \u2229 C \u2212 u ) holds. Since relation holds for at least one i and \u2286 holds for others, S k-ReLU S 1-ReLU holds.\n\nFigure 1 :\n1The input space for the ReLU assignments y 1 := ReLU (x 1 ), y 2 := ReLU (x 2 ) is shown on the left in blue. Shapes of the relaxations projected to 3D are shown on the right in red.\n\nFigure 2 :\n2Verification of property x 9 \u2264 2. Refining DeepPoly with 1-ReLU fails to prove the property whereas 2-ReLU adds extra constraints (in green) that help in verifying the property.\n\nFigure 4 :\n4DeepPoly relaxations for y i :=ReLU(x i ) using the original bounds l i , u i (in blue) and the refined bounds l i , u i (in green) for x i . The refined relaxations have smaller area in the x i y i -plane.\n\nTable 1 :\n1Volume of the output bounding box computed by kPoly on a 9 \u00d7 200 network.k \n1-ReLU \n2-ReLU \n3-ReLU \n\nVolume 4.5272 \u00b7 10 14 5.1252 \u00b7 10 7 2.9679 \u00b7 10 5 \n\n\n\nTable 2 :\n2Neural network architectures and parameters used in our experiments.Dataset \nModel \nType \n#Neurons #Layers \nDefense Refine \nk \nReLU \n\nMNIST \n6 \u00d7 100 \nfully connected \n610 \n6 \nNone \n\n3 \n9 \u00d7 100 \nfully connected \n910 \n9 \nNone \n\n2 \n6 \u00d7 200 \nfully connected \n1 210 \n6 \nNone \n\n2 \n9 \u00d7 200 \nfully connected \n1 810 \n9 \nNone \n\n2 \nConvSmall \nconvolutional \n3 604 \n3 \nNone \nAdaptive \nConvBig \nconvolutional \n34 688 \n6 DiffAI [29] \n\n5 \n\nCIFAR10 ConvSmall \nconvolutional \n4 852 \n3 \nPGD [30] \nAdaptive \nConvBig \nconvolutional \n62,464 \n6 \nPGD [30] \n\n5 \nResNet \nResidual \n107,496 \n13 \nWong [11] \nAdaptive \n\n\n\nTable 3 :\n3Number of verified adversarial regions and runtime of kPoly vs. DeepPoly and RefineZono.Dataset \nModel \n#correct \nDeepPoly[9] \nRefineZono [19] \nkPoly \n\nverified(#) time(s) verified(#) time(s) verified(#) time(s) \n\nMNIST \n6 \u00d7 100 \n960 \n0.026 \n160 \n0.3 \n312 \n310 \n441 \n307 \n9 \u00d7 100 \n947 \n0.026 \n182 \n0.4 \n304 \n411 \n369 \n171 \n6 \u00d7 200 \n972 \n0.015 \n292 \n0.5 \n341 \n570 \n574 \n187 \n9 \u00d7 200 \n950 \n0.015 \n259 \n0.9 \n316 \n860 \n506 \n464 \nConvSmall 980 \n0.12 \n158 \n3 \n179 \n707 \n347 \n477 \nConvBig \n929 \n0.3 \n711 \n21 \n648 \n285 \n736 \n40 \n\nCIFAR10 ConvSmall 630 \n2/255 \n359 \n4 \n347 \n716 \n399 \n86 \nConvBig \n631 \n2/255 \n421 \n43 \n305 \n592 \n459 \n346 \nResNet \n290 \n8/255 \n243 \n12 \n243 \n27 \n245 \n91 \n\n\n\nIntriguing properties of neural networks. C Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan, I Goodfellow, R Fergus, arXiv:1312.6199arXiv preprintC. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, \"Intriguing properties of neural networks,\" arXiv preprint arXiv:1312.6199, 2013.\n\nReluplex: An efficient SMT solver for verifying deep neural networks. G Katz, C W Barrett, D L Dill, K Julian, M J Kochenderfer, Computer Aided Verification -29th International Conference. Heidelberg, GermanyG. Katz, C. W. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer, \"Reluplex: An efficient SMT solver for verifying deep neural networks,\" in Computer Aided Verification -29th Inter- national Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I, 2017.\n\nFormal verification of piece-wise linear feed-forward neural networks. R Ehlers, Automated Technology for Verification and Analysis. ATVAR. Ehlers, \"Formal verification of piece-wise linear feed-forward neural networks,\" in Automated Technology for Verification and Analysis (ATVA), 2017.\n\nA unified view of piecewise linear neural network verification. R Bunel, I Turkaslan, P H Torr, P Kohli, M P Kumar, Proc. Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information essing Systems (NeurIPS)R. Bunel, I. Turkaslan, P. H. Torr, P. Kohli, and M. P. Kumar, \"A unified view of piecewise linear neural network verification,\" in Proc. Advances in Neural Information Processing Systems (NeurIPS), 2018, pp. 4795-4804.\n\nEvaluating robustness of neural networks with mixed integer programming. V Tjeng, K Y Xiao, R Tedrake, International Conference on Learning Representations. V. Tjeng, K. Y. Xiao, and R. Tedrake, \"Evaluating robustness of neural networks with mixed integer programming,\" in International Conference on Learning Representations, (ICLR), 2019.\n\nReachability analysis of deep neural networks with provable guarantees. W Ruan, X Huang, M Kwiatkowska, Proc. International Joint Conference on Artificial Intelligence, (IJCAI). International Joint Conference on Artificial Intelligence, (IJCAI)W. Ruan, X. Huang, and M. Kwiatkowska, \"Reachability analysis of deep neural networks with provable guarantees,\" in Proc. International Joint Conference on Artificial Intelligence, (IJCAI), 2018.\n\nAI2: Safety and robustness certification of neural networks with abstract interpretation. T Gehr, M Mirman, D Drachsler-Cohen, P Tsankov, S Chaudhuri, M Vechev, Proc. IEEE Symposium on Security and Privacy (SP). IEEE Symposium on Security and Privacy (SP)00T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev, \"AI2: Safety and robustness certification of neural networks with abstract interpretation,\" in Proc. IEEE Symposium on Security and Privacy (SP), vol. 00, 2018, pp. 948-963.\n\nFast and effective robustness certification. G Singh, T Gehr, M Mirman, M P\u00fcschel, M Vechev, Proc. Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information essing Systems (NeurIPS)10G. Singh, T. Gehr, M. Mirman, M. P\u00fcschel, and M. Vechev, \"Fast and effective robustness certification,\" in Proc. Advances in Neural Information Processing Systems (NeurIPS), 2018, pp. 10 825-10 836.\n\nAn abstract domain for certifying neural networks. G Singh, T Gehr, M P\u00fcschel, M Vechev, 41:1-41:30Proc. ACM Program. Lang. 3POPLG. Singh, T. Gehr, M. P\u00fcschel, and M. Vechev, \"An abstract domain for certifying neural networks,\" Proc. ACM Program. Lang., vol. 3, no. POPL, pp. 41:1-41:30, 2019.\n\nA dual approach to scalable verification of deep networks. K Dvijotham, R Stanforth, S Gowal, T Mann, P Kohli, Proc. Uncertainty in Artificial Intelligence (UAI). Uncertainty in Artificial Intelligence (UAI)K. Dvijotham, R. Stanforth, S. Gowal, T. Mann, and P. Kohli, \"A dual approach to scalable verification of deep networks,\" in Proc. Uncertainty in Artificial Intelligence (UAI), 2018, pp. 162-171.\n\nProvable defenses against adversarial examples via the convex outer adversarial polytope. E Wong, J Z Kolter, arXiv:1711.00851arXiv preprintE. Wong and J. Z. Kolter, \"Provable defenses against adversarial examples via the convex outer adversarial polytope,\" arXiv preprint arXiv:1711.00851, 2017.\n\nSemidefinite relaxations for certifying robustness to adversarial examples. A Raghunathan, J Steinhardt, P S Liang, Advances in Neural Information Processing Systems (NeurIPS). A. Raghunathan, J. Steinhardt, and P. S. Liang, \"Semidefinite relaxations for certifying robust- ness to adversarial examples,\" in Advances in Neural Information Processing Systems (NeurIPS), 2018, pp. 10 877-10 887.\n\nEfficient neural network verification with exactness characterization. K D Dvijotham, R Stanforth, S Gowal, C Qin, S De, P Kohli, Proc. Uncertainty in Artificial Intelligence, UAI. Uncertainty in Artificial Intelligence, UAI164K. D. Dvijotham, R. Stanforth, S. Gowal, C. Qin, S. De, and P. Kohli, \"Efficient neural network verification with exactness characterization,\" in Proc. Uncertainty in Artificial Intelligence, UAI, 2019, p. 164.\n\nTowards fast computation of certified robustness for ReLU networks. L Weng, H Zhang, H Chen, Z Song, C.-J Hsieh, L Daniel, D Boning, I Dhillon, Proc. International Conference on Machine Learning (ICML). International Conference on Machine Learning (ICML)80L. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, L. Daniel, D. Boning, and I. Dhillon, \"Towards fast computation of certified robustness for ReLU networks,\" in Proc. International Conference on Machine Learning (ICML), vol. 80, 2018, pp. 5276-5285.\n\nEfficient neural network robustness certification with general activation functions. H Zhang, T.-W Weng, P.-Y Chen, C.-J Hsieh, L Daniel, Proc. Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information essing Systems (NeurIPS)H. Zhang, T.-W. Weng, P.-Y. Chen, C.-J. Hsieh, and L. Daniel, \"Efficient neural network robust- ness certification with general activation functions,\" in Proc. Advances in Neural Information Processing Systems (NeurIPS), 2018.\n\nCnn-cert: An efficient framework for certifying robustness of convolutional neural networks. A Boopathy, T.-W Weng, P.-Y Chen, S Liu, L Daniel, AAAI Conference on Artificial Intelligence (AAAI). A. Boopathy, T.-W. Weng, P.-Y. Chen, S. Liu, and L. Daniel, \"Cnn-cert: An efficient framework for certifying robustness of convolutional neural networks,\" in AAAI Conference on Artificial Intelligence (AAAI), Jan 2019.\n\nA convex relaxation barrier to tight robustness verification of neural networks. H Salman, G Yang, H Zhang, C Hsieh, P Zhang, abs/1902.08722CoRR. H. Salman, G. Yang, H. Zhang, C. Hsieh, and P. Zhang, \"A convex relaxation barrier to tight robustness verification of neural networks,\" CoRR, vol. abs/1902.08722, 2019.\n\nEfficient formal safety analysis of neural networks. S Wang, K Pei, J Whitehouse, J Yang, S Jana, Proc. Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information essing Systems (NeurIPS)S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana, \"Efficient formal safety analysis of neural networks,\" in Proc. Advances in Neural Information Processing Systems (NeurIPS), 2018, pp. 6369-6379.\n\nBoosting robustness certification of neural networks. G Singh, T Gehr, M P\u00fcschel, M Vechev, International Conference on Learning Representations. G. Singh, T. Gehr, M. P\u00fcschel, and M. Vechev, \"Boosting robustness certification of neural networks,\" in International Conference on Learning Representations, 2019.\n\nAutomatic discovery of linear restraints among variables of a program. P Cousot, N Halbwachs, Proc. Principles of Programming Languages (POPL). Principles of Programming Languages (POPL)P. Cousot and N. Halbwachs, \"Automatic discovery of linear restraints among variables of a program,\" in Proc. Principles of Programming Languages (POPL), 1978, pp. 84-96.\n\nCorrectness verification of neural networks. Y Yang, M Rinard, Y. Yang and M. Rinard, \"Correctness verification of neural networks,\" 2019.\n\nCertifying geometric robustness of neural networks. M Balunovic, M Baader, G Singh, T Gehr, M Vechev, Advances in Neural Information Processing Systems (NeurIPS). M. Balunovic, M. Baader, G. Singh, T. Gehr, and M. Vechev, \"Certifying geometric robustness of neural networks,\" in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 15 287-15 297.\n\nTowards verifying robustness of neural networks against semantic perturbations. J Mohapatra, Tsui-Wei, P.-Y Weng, S Chen, L Liu, Daniel, J. Mohapatra, Tsui-Wei, Weng, P.-Y. Chen, S. Liu, and L. Daniel, \"Towards verifying robustness of neural networks against semantic perturbations,\" 2019.\n\nFast polyhedra abstract domain. G Singh, M P\u00fcschel, M Vechev, ACM SIGPLAN Notices. ACMG. Singh, M. P\u00fcschel, and M. Vechev, \"Fast polyhedra abstract domain,\" in ACM SIGPLAN Notices. ACM, 2017.\n\nExtended convex hull. Computational Geometry. 20126] \"pycddlib,\" 2018. [Online\"Extended convex hull,\" Computational Geometry, vol. 20, no. 1, pp. 13 -23, 2001. [26] \"pycddlib,\" 2018. [Online]. Available: https://pypi.org/project/pycddlib/\n\nGurobi optimizer reference manual. Gurobi Optimization, LLCGurobi Optimization, LLC, \"Gurobi optimizer reference manual,\" 2018. [Online]. Available: http://www.gurobi.com\n\nERAN: ETH Robustness Analyzer for Neural Networks. \"ERAN: ETH Robustness Analyzer for Neural Networks,\" 2018. [Online]. Available: https://github.com/eth-sri/eran\n\nDifferentiable abstract interpretation for provably robust neural networks. M Mirman, T Gehr, M Vechev, Proc. International Conference on Machine Learning (ICML). International Conference on Machine Learning (ICML)M. Mirman, T. Gehr, and M. Vechev, \"Differentiable abstract interpretation for provably robust neural networks,\" in Proc. International Conference on Machine Learning (ICML), 2018, pp. 3575-3583.\n\nTowards deep learning models resistant to adversarial attacks. A Madry, A Makelov, L Schmidt, D Tsipras, A Vladu, Proc. International Conference on Learning Representations (ICLR). International Conference on Learning Representations (ICLR)A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, \"Towards deep learning models resistant to adversarial attacks,\" in Proc. International Conference on Learning Representations (ICLR), 2018.\n\nGradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proc. of the IEEE. of the IEEEY. Lecun, L. Bottou, Y. Bengio, and P. Haffner, \"Gradient-based learning applied to document recognition,\" in Proc. of the IEEE, 1998, pp. 2278-2324.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, Tech. Rep.A. Krizhevsky, \"Learning multiple layers of features from tiny images,\" Tech. Rep., 2009.\n\nTowards evaluating the robustness of neural networks. N Carlini, D A Wagner, Proc. IEEE Symposium on Security and Privacy (SP. IEEE Symposium on Security and Privacy (SPN. Carlini and D. A. Wagner, \"Towards evaluating the robustness of neural networks,\" in Proc. IEEE Symposium on Security and Privacy (SP), 2017, pp. 39-57.\n", "annotations": {"author": "[{\"start\":\"76\",\"end\":\"147\"},{\"start\":\"148\",\"end\":\"219\"},{\"start\":\"220\",\"end\":\"290\"},{\"start\":\"291\",\"end\":\"410\"}]", "publisher": null, "author_last_name": "[{\"start\":\"86\",\"end\":\"91\"},{\"start\":\"157\",\"end\":\"163\"},{\"start\":\"227\",\"end\":\"234\"},{\"start\":\"298\",\"end\":\"304\"}]", "author_first_name": "[{\"start\":\"76\",\"end\":\"85\"},{\"start\":\"148\",\"end\":\"156\"},{\"start\":\"220\",\"end\":\"226\"},{\"start\":\"291\",\"end\":\"297\"}]", "author_affiliation": "[{\"start\":\"93\",\"end\":\"146\"},{\"start\":\"165\",\"end\":\"218\"},{\"start\":\"236\",\"end\":\"289\"},{\"start\":\"356\",\"end\":\"409\"}]", "title": "[{\"start\":\"1\",\"end\":\"73\"},{\"start\":\"411\",\"end\":\"483\"}]", "venue": null, "abstract": "[{\"start\":\"485\",\"end\":\"1249\"}]", "bib_ref": "[{\"start\":\"1512\",\"end\":\"1515\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"1759\",\"end\":\"1762\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"1762\",\"end\":\"1765\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"1765\",\"end\":\"1768\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"1803\",\"end\":\"1806\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"1835\",\"end\":\"1838\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"1929\",\"end\":\"1932\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"1932\",\"end\":\"1935\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"1935\",\"end\":\"1938\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"1948\",\"end\":\"1952\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"1952\",\"end\":\"1955\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"1971\",\"end\":\"1975\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"1975\",\"end\":\"1978\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"2002\",\"end\":\"2006\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"2006\",\"end\":\"2010\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"2010\",\"end\":\"2014\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"2014\",\"end\":\"2018\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"2210\",\"end\":\"2214\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"2214\",\"end\":\"2217\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"2690\",\"end\":\"2694\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"2936\",\"end\":\"2940\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"2940\",\"end\":\"2942\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"2975\",\"end\":\"2978\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"3013\",\"end\":\"3016\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"3016\",\"end\":\"3018\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"3018\",\"end\":\"3022\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"3022\",\"end\":\"3026\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"3026\",\"end\":\"3030\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"3030\",\"end\":\"3033\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"3210\",\"end\":\"3214\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"4637\",\"end\":\"4640\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"6227\",\"end\":\"6230\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"6365\",\"end\":\"6368\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"6368\",\"end\":\"6371\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"6538\",\"end\":\"6542\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"7379\",\"end\":\"7382\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"7382\",\"end\":\"7384\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"7384\",\"end\":\"7387\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"7387\",\"end\":\"7390\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"7390\",\"end\":\"7393\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"7393\",\"end\":\"7396\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"7396\",\"end\":\"7399\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"7423\",\"end\":\"7426\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"7426\",\"end\":\"7429\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"7429\",\"end\":\"7432\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"7432\",\"end\":\"7434\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"7434\",\"end\":\"7437\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"7437\",\"end\":\"7440\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"7684\",\"end\":\"7688\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"7748\",\"end\":\"7751\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"7751\",\"end\":\"7754\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"7754\",\"end\":\"7756\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"8796\",\"end\":\"8799\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"13527\",\"end\":\"13557\"},{\"start\":\"15744\",\"end\":\"15748\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"15804\",\"end\":\"15807\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"20643\",\"end\":\"20673\"},{\"start\":\"20710\",\"end\":\"20713\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"20846\",\"end\":\"20849\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"22320\",\"end\":\"22323\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"23674\",\"end\":\"23677\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"24311\",\"end\":\"24314\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"24952\",\"end\":\"24956\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"24956\",\"end\":\"24959\"},{\"start\":\"24999\",\"end\":\"25003\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"25146\",\"end\":\"25150\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"25357\",\"end\":\"25360\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"25376\",\"end\":\"25380\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"25417\",\"end\":\"25421\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"25421\",\"end\":\"25424\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"25473\",\"end\":\"25476\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"25502\",\"end\":\"25505\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"25505\",\"end\":\"25508\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"25508\",\"end\":\"25511\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"25566\",\"end\":\"25569\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"25569\",\"end\":\"25572\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"25572\",\"end\":\"25575\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"25575\",\"end\":\"25577\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"25577\",\"end\":\"25580\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"25580\",\"end\":\"25583\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"25817\",\"end\":\"25821\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"25834\",\"end\":\"25838\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"26259\",\"end\":\"26263\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"26325\",\"end\":\"26329\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"26370\",\"end\":\"26374\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"26507\",\"end\":\"26511\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"27822\",\"end\":\"27825\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"29679\",\"end\":\"29683\",\"attributes\":{\"ref_id\":\"b16\"}}]", "figure": "[{\"start\":\"31224\",\"end\":\"31419\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"31420\",\"end\":\"31610\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"31611\",\"end\":\"31830\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"31831\",\"end\":\"31996\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}},{\"start\":\"31997\",\"end\":\"32600\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}},{\"start\":\"32601\",\"end\":\"33290\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"1265\",\"end\":\"2218\"},{\"start\":\"2220\",\"end\":\"2597\"},{\"start\":\"2599\",\"end\":\"4125\"},{\"start\":\"4127\",\"end\":\"4361\"},{\"start\":\"4426\",\"end\":\"4606\"},{\"start\":\"4608\",\"end\":\"5171\"},{\"start\":\"5173\",\"end\":\"5329\"},{\"start\":\"5408\",\"end\":\"5636\"},{\"start\":\"5638\",\"end\":\"5684\"},{\"start\":\"5686\",\"end\":\"6024\"},{\"start\":\"6026\",\"end\":\"6153\"},{\"start\":\"6155\",\"end\":\"6268\"},{\"start\":\"6270\",\"end\":\"6511\"},{\"start\":\"6513\",\"end\":\"7187\"},{\"start\":\"7189\",\"end\":\"7441\"},{\"start\":\"7443\",\"end\":\"7758\"},{\"start\":\"7781\",\"end\":\"8160\"},{\"start\":\"8162\",\"end\":\"8720\"},{\"start\":\"8722\",\"end\":\"8947\"},{\"start\":\"9073\",\"end\":\"9186\"},{\"start\":\"9188\",\"end\":\"9325\"},{\"start\":\"9406\",\"end\":\"9651\"},{\"start\":\"9762\",\"end\":\"10045\"},{\"start\":\"10080\",\"end\":\"10357\"},{\"start\":\"10410\",\"end\":\"10413\"},{\"start\":\"10415\",\"end\":\"10418\"},{\"start\":\"10420\",\"end\":\"10423\"},{\"start\":\"10425\",\"end\":\"10428\"},{\"start\":\"10430\",\"end\":\"10433\"},{\"start\":\"10435\",\"end\":\"10438\"},{\"start\":\"10440\",\"end\":\"10443\"},{\"start\":\"10445\",\"end\":\"10448\"},{\"start\":\"10450\",\"end\":\"10453\"},{\"start\":\"10455\",\"end\":\"10583\"},{\"start\":\"11324\",\"end\":\"11713\"},{\"start\":\"11715\",\"end\":\"12247\"},{\"start\":\"12249\",\"end\":\"12416\"},{\"start\":\"12509\",\"end\":\"12599\"},{\"start\":\"12601\",\"end\":\"12871\"},{\"start\":\"12933\",\"end\":\"13383\"},{\"start\":\"13385\",\"end\":\"13558\"},{\"start\":\"13590\",\"end\":\"13873\"},{\"start\":\"13875\",\"end\":\"14677\"},{\"start\":\"14679\",\"end\":\"15031\"},{\"start\":\"15344\",\"end\":\"15428\"},{\"start\":\"15455\",\"end\":\"15522\"},{\"start\":\"15557\",\"end\":\"15749\"},{\"start\":\"15760\",\"end\":\"16237\"},{\"start\":\"16310\",\"end\":\"16862\"},{\"start\":\"16885\",\"end\":\"17413\"},{\"start\":\"17497\",\"end\":\"17566\"},{\"start\":\"17568\",\"end\":\"17644\"},{\"start\":\"17702\",\"end\":\"18274\"},{\"start\":\"18276\",\"end\":\"19086\"},{\"start\":\"19088\",\"end\":\"19151\"},{\"start\":\"19195\",\"end\":\"19587\"},{\"start\":\"19652\",\"end\":\"20441\"},{\"start\":\"20470\",\"end\":\"20558\"},{\"start\":\"20560\",\"end\":\"20819\"},{\"start\":\"20821\",\"end\":\"21118\"},{\"start\":\"21120\",\"end\":\"21506\"},{\"start\":\"21508\",\"end\":\"21897\"},{\"start\":\"21936\",\"end\":\"22226\"},{\"start\":\"22258\",\"end\":\"23256\"},{\"start\":\"23258\",\"end\":\"24018\"},{\"start\":\"24020\",\"end\":\"24616\"},{\"start\":\"24618\",\"end\":\"24806\"},{\"start\":\"24821\",\"end\":\"25783\"},{\"start\":\"25785\",\"end\":\"26457\"},{\"start\":\"26481\",\"end\":\"26722\"},{\"start\":\"26724\",\"end\":\"27467\"},{\"start\":\"27469\",\"end\":\"27765\"},{\"start\":\"27767\",\"end\":\"28396\"},{\"start\":\"28398\",\"end\":\"29268\"},{\"start\":\"29270\",\"end\":\"29455\"},{\"start\":\"29476\",\"end\":\"29813\"},{\"start\":\"29815\",\"end\":\"30004\"},{\"start\":\"30019\",\"end\":\"30371\"},{\"start\":\"30386\",\"end\":\"30410\"},{\"start\":\"30412\",\"end\":\"30510\"},{\"start\":\"30583\",\"end\":\"30855\"},{\"start\":\"30929\",\"end\":\"31017\"},{\"start\":\"31019\",\"end\":\"31223\"}]", "formula": "[{\"start\":\"4362\",\"end\":\"4425\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"5330\",\"end\":\"5407\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"8948\",\"end\":\"9072\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"9326\",\"end\":\"9405\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"9652\",\"end\":\"9761\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"10046\",\"end\":\"10079\",\"attributes\":{\"id\":\"formula_5\"}},{\"start\":\"10358\",\"end\":\"10409\",\"attributes\":{\"id\":\"formula_6\"}},{\"start\":\"10584\",\"end\":\"11243\",\"attributes\":{\"id\":\"formula_7\"}},{\"start\":\"11243\",\"end\":\"11323\",\"attributes\":{\"id\":\"formula_8\"}},{\"start\":\"12417\",\"end\":\"12508\",\"attributes\":{\"id\":\"formula_9\"}},{\"start\":\"12872\",\"end\":\"12932\",\"attributes\":{\"id\":\"formula_10\"}},{\"start\":\"15032\",\"end\":\"15343\",\"attributes\":{\"id\":\"formula_11\"}},{\"start\":\"15523\",\"end\":\"15556\",\"attributes\":{\"id\":\"formula_12\"}},{\"start\":\"16238\",\"end\":\"16309\",\"attributes\":{\"id\":\"formula_13\"}},{\"start\":\"17414\",\"end\":\"17496\",\"attributes\":{\"id\":\"formula_14\"}},{\"start\":\"17645\",\"end\":\"17701\",\"attributes\":{\"id\":\"formula_15\"}},{\"start\":\"19152\",\"end\":\"19194\",\"attributes\":{\"id\":\"formula_16\"}},{\"start\":\"19588\",\"end\":\"19614\",\"attributes\":{\"id\":\"formula_17\"}},{\"start\":\"30511\",\"end\":\"30582\",\"attributes\":{\"id\":\"formula_18\"}},{\"start\":\"30856\",\"end\":\"30928\",\"attributes\":{\"id\":\"formula_19\"}}]", "table_ref": "[{\"start\":\"6737\",\"end\":\"6744\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"25938\",\"end\":\"25945\",\"attributes\":{\"ref_id\":\"tab_2\"}},{\"start\":\"25971\",\"end\":\"25978\",\"attributes\":{\"ref_id\":\"tab_2\"}},{\"start\":\"27221\",\"end\":\"27228\",\"attributes\":{\"ref_id\":\"tab_3\"}},{\"start\":\"27906\",\"end\":\"27913\",\"attributes\":{\"ref_id\":\"tab_2\"}},{\"start\":\"28388\",\"end\":\"28395\",\"attributes\":{\"ref_id\":\"tab_3\"}},{\"start\":\"28431\",\"end\":\"28438\",\"attributes\":{\"ref_id\":\"tab_3\"}},{\"start\":\"29169\",\"end\":\"29176\",\"attributes\":{\"ref_id\":\"tab_3\"}}]", "section_header": "[{\"start\":\"1251\",\"end\":\"1263\",\"attributes\":{\"n\":\"1\"}},{\"start\":\"7761\",\"end\":\"7779\",\"attributes\":{\"n\":\"2\"}},{\"start\":\"13561\",\"end\":\"13588\",\"attributes\":{\"n\":\"3\"}},{\"start\":\"15431\",\"end\":\"15453\",\"attributes\":{\"n\":\"3.1\"}},{\"start\":\"15752\",\"end\":\"15758\",\"attributes\":{\"n\":\"3.2\"}},{\"start\":\"16865\",\"end\":\"16883\",\"attributes\":{\"n\":\"3.3\"}},{\"start\":\"19616\",\"end\":\"19650\",\"attributes\":{\"n\":\"4\"}},{\"start\":\"20444\",\"end\":\"20468\",\"attributes\":{\"n\":\"4.1\"}},{\"start\":\"21900\",\"end\":\"21934\",\"attributes\":{\"n\":\"4.2\"}},{\"start\":\"22229\",\"end\":\"22256\",\"attributes\":{\"n\":\"4.3\"}},{\"start\":\"24809\",\"end\":\"24819\",\"attributes\":{\"n\":\"5\"}},{\"start\":\"26460\",\"end\":\"26479\"},{\"start\":\"29458\",\"end\":\"29474\"},{\"start\":\"30007\",\"end\":\"30017\",\"attributes\":{\"n\":\"6\"}},{\"start\":\"30374\",\"end\":\"30384\"},{\"start\":\"31225\",\"end\":\"31235\"},{\"start\":\"31421\",\"end\":\"31431\"},{\"start\":\"31612\",\"end\":\"31622\"},{\"start\":\"31832\",\"end\":\"31841\"},{\"start\":\"31998\",\"end\":\"32007\"},{\"start\":\"32602\",\"end\":\"32611\"}]", "table": "[{\"start\":\"31916\",\"end\":\"31996\"},{\"start\":\"32077\",\"end\":\"32600\"},{\"start\":\"32701\",\"end\":\"33290\"}]", "figure_caption": "[{\"start\":\"31237\",\"end\":\"31419\"},{\"start\":\"31433\",\"end\":\"31610\"},{\"start\":\"31624\",\"end\":\"31830\"},{\"start\":\"31843\",\"end\":\"31916\"},{\"start\":\"32009\",\"end\":\"32077\"},{\"start\":\"32613\",\"end\":\"32701\"}]", "figure_ref": "[{\"start\":\"4235\",\"end\":\"4241\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"5442\",\"end\":\"5449\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"5608\",\"end\":\"5615\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"8240\",\"end\":\"8246\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"9179\",\"end\":\"9185\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"17512\",\"end\":\"17520\"},{\"start\":\"19796\",\"end\":\"19802\"},{\"start\":\"23467\",\"end\":\"23485\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"24254\",\"end\":\"24264\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"24505\",\"end\":\"24515\",\"attributes\":{\"ref_id\":\"fig_2\"}}]", "bib_author_first_name": "[{\"start\":\"33334\",\"end\":\"33335\"},{\"start\":\"33345\",\"end\":\"33346\"},{\"start\":\"33356\",\"end\":\"33357\"},{\"start\":\"33369\",\"end\":\"33370\"},{\"start\":\"33378\",\"end\":\"33379\"},{\"start\":\"33387\",\"end\":\"33388\"},{\"start\":\"33401\",\"end\":\"33402\"},{\"start\":\"33681\",\"end\":\"33682\"},{\"start\":\"33689\",\"end\":\"33690\"},{\"start\":\"33691\",\"end\":\"33692\"},{\"start\":\"33702\",\"end\":\"33703\"},{\"start\":\"33704\",\"end\":\"33705\"},{\"start\":\"33712\",\"end\":\"33713\"},{\"start\":\"33722\",\"end\":\"33723\"},{\"start\":\"33724\",\"end\":\"33725\"},{\"start\":\"34175\",\"end\":\"34176\"},{\"start\":\"34458\",\"end\":\"34459\"},{\"start\":\"34467\",\"end\":\"34468\"},{\"start\":\"34480\",\"end\":\"34481\"},{\"start\":\"34482\",\"end\":\"34483\"},{\"start\":\"34490\",\"end\":\"34491\"},{\"start\":\"34499\",\"end\":\"34500\"},{\"start\":\"34501\",\"end\":\"34502\"},{\"start\":\"34926\",\"end\":\"34927\"},{\"start\":\"34935\",\"end\":\"34936\"},{\"start\":\"34937\",\"end\":\"34938\"},{\"start\":\"34945\",\"end\":\"34946\"},{\"start\":\"35267\",\"end\":\"35268\"},{\"start\":\"35275\",\"end\":\"35276\"},{\"start\":\"35284\",\"end\":\"35285\"},{\"start\":\"35726\",\"end\":\"35727\"},{\"start\":\"35734\",\"end\":\"35735\"},{\"start\":\"35744\",\"end\":\"35745\"},{\"start\":\"35763\",\"end\":\"35764\"},{\"start\":\"35774\",\"end\":\"35775\"},{\"start\":\"35787\",\"end\":\"35788\"},{\"start\":\"36194\",\"end\":\"36195\"},{\"start\":\"36203\",\"end\":\"36204\"},{\"start\":\"36211\",\"end\":\"36212\"},{\"start\":\"36221\",\"end\":\"36222\"},{\"start\":\"36232\",\"end\":\"36233\"},{\"start\":\"36617\",\"end\":\"36618\"},{\"start\":\"36626\",\"end\":\"36627\"},{\"start\":\"36634\",\"end\":\"36635\"},{\"start\":\"36645\",\"end\":\"36646\"},{\"start\":\"36920\",\"end\":\"36921\"},{\"start\":\"36933\",\"end\":\"36934\"},{\"start\":\"36946\",\"end\":\"36947\"},{\"start\":\"36955\",\"end\":\"36956\"},{\"start\":\"36963\",\"end\":\"36964\"},{\"start\":\"37355\",\"end\":\"37356\"},{\"start\":\"37363\",\"end\":\"37364\"},{\"start\":\"37365\",\"end\":\"37366\"},{\"start\":\"37639\",\"end\":\"37640\"},{\"start\":\"37654\",\"end\":\"37655\"},{\"start\":\"37668\",\"end\":\"37669\"},{\"start\":\"37670\",\"end\":\"37671\"},{\"start\":\"38029\",\"end\":\"38030\"},{\"start\":\"38031\",\"end\":\"38032\"},{\"start\":\"38044\",\"end\":\"38045\"},{\"start\":\"38057\",\"end\":\"38058\"},{\"start\":\"38066\",\"end\":\"38067\"},{\"start\":\"38073\",\"end\":\"38074\"},{\"start\":\"38079\",\"end\":\"38080\"},{\"start\":\"38465\",\"end\":\"38466\"},{\"start\":\"38473\",\"end\":\"38474\"},{\"start\":\"38482\",\"end\":\"38483\"},{\"start\":\"38490\",\"end\":\"38491\"},{\"start\":\"38498\",\"end\":\"38502\"},{\"start\":\"38510\",\"end\":\"38511\"},{\"start\":\"38520\",\"end\":\"38521\"},{\"start\":\"38530\",\"end\":\"38531\"},{\"start\":\"38989\",\"end\":\"38990\"},{\"start\":\"38998\",\"end\":\"39002\"},{\"start\":\"39009\",\"end\":\"39013\"},{\"start\":\"39020\",\"end\":\"39024\"},{\"start\":\"39032\",\"end\":\"39033\"},{\"start\":\"39485\",\"end\":\"39486\"},{\"start\":\"39497\",\"end\":\"39501\"},{\"start\":\"39508\",\"end\":\"39512\"},{\"start\":\"39519\",\"end\":\"39520\"},{\"start\":\"39526\",\"end\":\"39527\"},{\"start\":\"39888\",\"end\":\"39889\"},{\"start\":\"39898\",\"end\":\"39899\"},{\"start\":\"39906\",\"end\":\"39907\"},{\"start\":\"39915\",\"end\":\"39916\"},{\"start\":\"39924\",\"end\":\"39925\"},{\"start\":\"40177\",\"end\":\"40178\"},{\"start\":\"40185\",\"end\":\"40186\"},{\"start\":\"40192\",\"end\":\"40193\"},{\"start\":\"40206\",\"end\":\"40207\"},{\"start\":\"40214\",\"end\":\"40215\"},{\"start\":\"40599\",\"end\":\"40600\"},{\"start\":\"40608\",\"end\":\"40609\"},{\"start\":\"40616\",\"end\":\"40617\"},{\"start\":\"40627\",\"end\":\"40628\"},{\"start\":\"40928\",\"end\":\"40929\"},{\"start\":\"40938\",\"end\":\"40939\"},{\"start\":\"41260\",\"end\":\"41261\"},{\"start\":\"41268\",\"end\":\"41269\"},{\"start\":\"41407\",\"end\":\"41408\"},{\"start\":\"41420\",\"end\":\"41421\"},{\"start\":\"41430\",\"end\":\"41431\"},{\"start\":\"41439\",\"end\":\"41440\"},{\"start\":\"41447\",\"end\":\"41448\"},{\"start\":\"41801\",\"end\":\"41802\"},{\"start\":\"41824\",\"end\":\"41828\"},{\"start\":\"41835\",\"end\":\"41836\"},{\"start\":\"41843\",\"end\":\"41844\"},{\"start\":\"42044\",\"end\":\"42045\"},{\"start\":\"42053\",\"end\":\"42054\"},{\"start\":\"42064\",\"end\":\"42065\"},{\"start\":\"42857\",\"end\":\"42858\"},{\"start\":\"42867\",\"end\":\"42868\"},{\"start\":\"42875\",\"end\":\"42876\"},{\"start\":\"43255\",\"end\":\"43256\"},{\"start\":\"43264\",\"end\":\"43265\"},{\"start\":\"43275\",\"end\":\"43276\"},{\"start\":\"43286\",\"end\":\"43287\"},{\"start\":\"43297\",\"end\":\"43298\"},{\"start\":\"43691\",\"end\":\"43692\"},{\"start\":\"43700\",\"end\":\"43701\"},{\"start\":\"43710\",\"end\":\"43711\"},{\"start\":\"43720\",\"end\":\"43721\"},{\"start\":\"43967\",\"end\":\"43968\"},{\"start\":\"44136\",\"end\":\"44137\"},{\"start\":\"44147\",\"end\":\"44148\"},{\"start\":\"44149\",\"end\":\"44150\"}]", "bib_author_last_name": "[{\"start\":\"33336\",\"end\":\"33343\"},{\"start\":\"33347\",\"end\":\"33354\"},{\"start\":\"33358\",\"end\":\"33367\"},{\"start\":\"33371\",\"end\":\"33376\"},{\"start\":\"33380\",\"end\":\"33385\"},{\"start\":\"33389\",\"end\":\"33399\"},{\"start\":\"33403\",\"end\":\"33409\"},{\"start\":\"33683\",\"end\":\"33687\"},{\"start\":\"33693\",\"end\":\"33700\"},{\"start\":\"33706\",\"end\":\"33710\"},{\"start\":\"33714\",\"end\":\"33720\"},{\"start\":\"33726\",\"end\":\"33738\"},{\"start\":\"34177\",\"end\":\"34183\"},{\"start\":\"34460\",\"end\":\"34465\"},{\"start\":\"34469\",\"end\":\"34478\"},{\"start\":\"34484\",\"end\":\"34488\"},{\"start\":\"34492\",\"end\":\"34497\"},{\"start\":\"34503\",\"end\":\"34508\"},{\"start\":\"34928\",\"end\":\"34933\"},{\"start\":\"34939\",\"end\":\"34943\"},{\"start\":\"34947\",\"end\":\"34954\"},{\"start\":\"35269\",\"end\":\"35273\"},{\"start\":\"35277\",\"end\":\"35282\"},{\"start\":\"35286\",\"end\":\"35297\"},{\"start\":\"35728\",\"end\":\"35732\"},{\"start\":\"35736\",\"end\":\"35742\"},{\"start\":\"35746\",\"end\":\"35761\"},{\"start\":\"35765\",\"end\":\"35772\"},{\"start\":\"35776\",\"end\":\"35785\"},{\"start\":\"35789\",\"end\":\"35795\"},{\"start\":\"36196\",\"end\":\"36201\"},{\"start\":\"36205\",\"end\":\"36209\"},{\"start\":\"36213\",\"end\":\"36219\"},{\"start\":\"36223\",\"end\":\"36230\"},{\"start\":\"36234\",\"end\":\"36240\"},{\"start\":\"36619\",\"end\":\"36624\"},{\"start\":\"36628\",\"end\":\"36632\"},{\"start\":\"36636\",\"end\":\"36643\"},{\"start\":\"36647\",\"end\":\"36653\"},{\"start\":\"36922\",\"end\":\"36931\"},{\"start\":\"36935\",\"end\":\"36944\"},{\"start\":\"36948\",\"end\":\"36953\"},{\"start\":\"36957\",\"end\":\"36961\"},{\"start\":\"36965\",\"end\":\"36970\"},{\"start\":\"37357\",\"end\":\"37361\"},{\"start\":\"37367\",\"end\":\"37373\"},{\"start\":\"37641\",\"end\":\"37652\"},{\"start\":\"37656\",\"end\":\"37666\"},{\"start\":\"37672\",\"end\":\"37677\"},{\"start\":\"38033\",\"end\":\"38042\"},{\"start\":\"38046\",\"end\":\"38055\"},{\"start\":\"38059\",\"end\":\"38064\"},{\"start\":\"38068\",\"end\":\"38071\"},{\"start\":\"38075\",\"end\":\"38077\"},{\"start\":\"38081\",\"end\":\"38086\"},{\"start\":\"38467\",\"end\":\"38471\"},{\"start\":\"38475\",\"end\":\"38480\"},{\"start\":\"38484\",\"end\":\"38488\"},{\"start\":\"38492\",\"end\":\"38496\"},{\"start\":\"38503\",\"end\":\"38508\"},{\"start\":\"38512\",\"end\":\"38518\"},{\"start\":\"38522\",\"end\":\"38528\"},{\"start\":\"38532\",\"end\":\"38539\"},{\"start\":\"38991\",\"end\":\"38996\"},{\"start\":\"39003\",\"end\":\"39007\"},{\"start\":\"39014\",\"end\":\"39018\"},{\"start\":\"39025\",\"end\":\"39030\"},{\"start\":\"39034\",\"end\":\"39040\"},{\"start\":\"39487\",\"end\":\"39495\"},{\"start\":\"39502\",\"end\":\"39506\"},{\"start\":\"39513\",\"end\":\"39517\"},{\"start\":\"39521\",\"end\":\"39524\"},{\"start\":\"39528\",\"end\":\"39534\"},{\"start\":\"39890\",\"end\":\"39896\"},{\"start\":\"39900\",\"end\":\"39904\"},{\"start\":\"39908\",\"end\":\"39913\"},{\"start\":\"39917\",\"end\":\"39922\"},{\"start\":\"39926\",\"end\":\"39931\"},{\"start\":\"40179\",\"end\":\"40183\"},{\"start\":\"40187\",\"end\":\"40190\"},{\"start\":\"40194\",\"end\":\"40204\"},{\"start\":\"40208\",\"end\":\"40212\"},{\"start\":\"40216\",\"end\":\"40220\"},{\"start\":\"40601\",\"end\":\"40606\"},{\"start\":\"40610\",\"end\":\"40614\"},{\"start\":\"40618\",\"end\":\"40625\"},{\"start\":\"40629\",\"end\":\"40635\"},{\"start\":\"40930\",\"end\":\"40936\"},{\"start\":\"40940\",\"end\":\"40949\"},{\"start\":\"41262\",\"end\":\"41266\"},{\"start\":\"41270\",\"end\":\"41276\"},{\"start\":\"41409\",\"end\":\"41418\"},{\"start\":\"41422\",\"end\":\"41428\"},{\"start\":\"41432\",\"end\":\"41437\"},{\"start\":\"41441\",\"end\":\"41445\"},{\"start\":\"41449\",\"end\":\"41455\"},{\"start\":\"41803\",\"end\":\"41812\"},{\"start\":\"41814\",\"end\":\"41822\"},{\"start\":\"41829\",\"end\":\"41833\"},{\"start\":\"41837\",\"end\":\"41841\"},{\"start\":\"41845\",\"end\":\"41848\"},{\"start\":\"41850\",\"end\":\"41856\"},{\"start\":\"42046\",\"end\":\"42051\"},{\"start\":\"42055\",\"end\":\"42062\"},{\"start\":\"42066\",\"end\":\"42072\"},{\"start\":\"42859\",\"end\":\"42865\"},{\"start\":\"42869\",\"end\":\"42873\"},{\"start\":\"42877\",\"end\":\"42883\"},{\"start\":\"43257\",\"end\":\"43262\"},{\"start\":\"43266\",\"end\":\"43273\"},{\"start\":\"43277\",\"end\":\"43284\"},{\"start\":\"43288\",\"end\":\"43295\"},{\"start\":\"43299\",\"end\":\"43304\"},{\"start\":\"43693\",\"end\":\"43698\"},{\"start\":\"43702\",\"end\":\"43708\"},{\"start\":\"43712\",\"end\":\"43718\"},{\"start\":\"43722\",\"end\":\"43729\"},{\"start\":\"43969\",\"end\":\"43979\"},{\"start\":\"44138\",\"end\":\"44145\"},{\"start\":\"44151\",\"end\":\"44157\"}]", "bib_entry": "[{\"start\":\"33292\",\"end\":\"33609\",\"attributes\":{\"id\":\"b0\",\"doi\":\"arXiv:1312.6199\"}},{\"start\":\"33611\",\"end\":\"34102\",\"attributes\":{\"matched_paper_id\":\"516928\",\"id\":\"b1\"}},{\"start\":\"34104\",\"end\":\"34392\",\"attributes\":{\"matched_paper_id\":\"1931807\",\"id\":\"b2\"}},{\"start\":\"34394\",\"end\":\"34851\",\"attributes\":{\"matched_paper_id\":\"41612217\",\"id\":\"b3\"}},{\"start\":\"34853\",\"end\":\"35193\",\"attributes\":{\"matched_paper_id\":\"47016770\",\"id\":\"b4\"}},{\"start\":\"35195\",\"end\":\"35634\",\"attributes\":{\"matched_paper_id\":\"19173163\",\"id\":\"b5\"}},{\"start\":\"35636\",\"end\":\"36147\",\"attributes\":{\"matched_paper_id\":\"206579396\",\"id\":\"b6\"}},{\"start\":\"36149\",\"end\":\"36564\",\"attributes\":{\"matched_paper_id\":\"53960414\",\"id\":\"b7\"}},{\"start\":\"36566\",\"end\":\"36859\",\"attributes\":{\"matched_paper_id\":\"57757287\",\"id\":\"b8\",\"doi\":\"41:1-41:30\"}},{\"start\":\"36861\",\"end\":\"37263\",\"attributes\":{\"matched_paper_id\":\"3972365\",\"id\":\"b9\"}},{\"start\":\"37265\",\"end\":\"37561\",\"attributes\":{\"id\":\"b10\",\"doi\":\"arXiv:1711.00851\"}},{\"start\":\"37563\",\"end\":\"37956\",\"attributes\":{\"matched_paper_id\":\"53215541\",\"id\":\"b11\"}},{\"start\":\"37958\",\"end\":\"38395\",\"attributes\":{\"matched_paper_id\":\"197661813\",\"id\":\"b12\"}},{\"start\":\"38397\",\"end\":\"38902\",\"attributes\":{\"matched_paper_id\":\"13750928\",\"id\":\"b13\"}},{\"start\":\"38904\",\"end\":\"39390\",\"attributes\":{\"matched_paper_id\":\"53297058\",\"id\":\"b14\"}},{\"start\":\"39392\",\"end\":\"39805\",\"attributes\":{\"matched_paper_id\":\"54011126\",\"id\":\"b15\"}},{\"start\":\"39807\",\"end\":\"40122\",\"attributes\":{\"matched_paper_id\":\"67855530\",\"id\":\"b16\",\"doi\":\"abs/1902.08722\"}},{\"start\":\"40124\",\"end\":\"40543\",\"attributes\":{\"matched_paper_id\":\"52347370\",\"id\":\"b17\"}},{\"start\":\"40545\",\"end\":\"40855\",\"attributes\":{\"matched_paper_id\":\"196059499\",\"id\":\"b18\"}},{\"start\":\"40857\",\"end\":\"41213\",\"attributes\":{\"matched_paper_id\":\"16411662\",\"id\":\"b19\"}},{\"start\":\"41215\",\"end\":\"41353\",\"attributes\":{\"id\":\"b20\"}},{\"start\":\"41355\",\"end\":\"41719\",\"attributes\":{\"matched_paper_id\":\"202766379\",\"id\":\"b21\"}},{\"start\":\"41721\",\"end\":\"42010\",\"attributes\":{\"id\":\"b22\"}},{\"start\":\"42012\",\"end\":\"42203\",\"attributes\":{\"matched_paper_id\":\"15378666\",\"id\":\"b23\"}},{\"start\":\"42205\",\"end\":\"42443\",\"attributes\":{\"matched_paper_id\":\"6061279\",\"id\":\"b24\"}},{\"start\":\"42445\",\"end\":\"42615\",\"attributes\":{\"id\":\"b25\"}},{\"start\":\"42617\",\"end\":\"42779\",\"attributes\":{\"id\":\"b26\"}},{\"start\":\"42781\",\"end\":\"43190\",\"attributes\":{\"matched_paper_id\":\"51872670\",\"id\":\"b27\"}},{\"start\":\"43192\",\"end\":\"43632\",\"attributes\":{\"matched_paper_id\":\"3488815\",\"id\":\"b28\"}},{\"start\":\"43634\",\"end\":\"43910\",\"attributes\":{\"matched_paper_id\":\"14542261\",\"id\":\"b29\"}},{\"start\":\"43912\",\"end\":\"44080\",\"attributes\":{\"id\":\"b30\"}},{\"start\":\"44082\",\"end\":\"44406\",\"attributes\":{\"matched_paper_id\":\"2893830\",\"id\":\"b31\"}}]", "bib_title": "[{\"start\":\"33611\",\"end\":\"33679\"},{\"start\":\"34104\",\"end\":\"34173\"},{\"start\":\"34394\",\"end\":\"34456\"},{\"start\":\"34853\",\"end\":\"34924\"},{\"start\":\"35195\",\"end\":\"35265\"},{\"start\":\"35636\",\"end\":\"35724\"},{\"start\":\"36149\",\"end\":\"36192\"},{\"start\":\"36566\",\"end\":\"36615\"},{\"start\":\"36861\",\"end\":\"36918\"},{\"start\":\"37563\",\"end\":\"37637\"},{\"start\":\"37958\",\"end\":\"38027\"},{\"start\":\"38397\",\"end\":\"38463\"},{\"start\":\"38904\",\"end\":\"38987\"},{\"start\":\"39392\",\"end\":\"39483\"},{\"start\":\"39807\",\"end\":\"39886\"},{\"start\":\"40124\",\"end\":\"40175\"},{\"start\":\"40545\",\"end\":\"40597\"},{\"start\":\"40857\",\"end\":\"40926\"},{\"start\":\"41355\",\"end\":\"41405\"},{\"start\":\"42012\",\"end\":\"42042\"},{\"start\":\"42205\",\"end\":\"42225\"},{\"start\":\"42781\",\"end\":\"42855\"},{\"start\":\"43192\",\"end\":\"43253\"},{\"start\":\"43634\",\"end\":\"43689\"},{\"start\":\"44082\",\"end\":\"44134\"}]", "bib_author": "[{\"start\":\"33334\",\"end\":\"33345\"},{\"start\":\"33345\",\"end\":\"33356\"},{\"start\":\"33356\",\"end\":\"33369\"},{\"start\":\"33369\",\"end\":\"33378\"},{\"start\":\"33378\",\"end\":\"33387\"},{\"start\":\"33387\",\"end\":\"33401\"},{\"start\":\"33401\",\"end\":\"33411\"},{\"start\":\"33681\",\"end\":\"33689\"},{\"start\":\"33689\",\"end\":\"33702\"},{\"start\":\"33702\",\"end\":\"33712\"},{\"start\":\"33712\",\"end\":\"33722\"},{\"start\":\"33722\",\"end\":\"33740\"},{\"start\":\"34175\",\"end\":\"34185\"},{\"start\":\"34458\",\"end\":\"34467\"},{\"start\":\"34467\",\"end\":\"34480\"},{\"start\":\"34480\",\"end\":\"34490\"},{\"start\":\"34490\",\"end\":\"34499\"},{\"start\":\"34499\",\"end\":\"34510\"},{\"start\":\"34926\",\"end\":\"34935\"},{\"start\":\"34935\",\"end\":\"34945\"},{\"start\":\"34945\",\"end\":\"34956\"},{\"start\":\"35267\",\"end\":\"35275\"},{\"start\":\"35275\",\"end\":\"35284\"},{\"start\":\"35284\",\"end\":\"35299\"},{\"start\":\"35726\",\"end\":\"35734\"},{\"start\":\"35734\",\"end\":\"35744\"},{\"start\":\"35744\",\"end\":\"35763\"},{\"start\":\"35763\",\"end\":\"35774\"},{\"start\":\"35774\",\"end\":\"35787\"},{\"start\":\"35787\",\"end\":\"35797\"},{\"start\":\"36194\",\"end\":\"36203\"},{\"start\":\"36203\",\"end\":\"36211\"},{\"start\":\"36211\",\"end\":\"36221\"},{\"start\":\"36221\",\"end\":\"36232\"},{\"start\":\"36232\",\"end\":\"36242\"},{\"start\":\"36617\",\"end\":\"36626\"},{\"start\":\"36626\",\"end\":\"36634\"},{\"start\":\"36634\",\"end\":\"36645\"},{\"start\":\"36645\",\"end\":\"36655\"},{\"start\":\"36920\",\"end\":\"36933\"},{\"start\":\"36933\",\"end\":\"36946\"},{\"start\":\"36946\",\"end\":\"36955\"},{\"start\":\"36955\",\"end\":\"36963\"},{\"start\":\"36963\",\"end\":\"36972\"},{\"start\":\"37355\",\"end\":\"37363\"},{\"start\":\"37363\",\"end\":\"37375\"},{\"start\":\"37639\",\"end\":\"37654\"},{\"start\":\"37654\",\"end\":\"37668\"},{\"start\":\"37668\",\"end\":\"37679\"},{\"start\":\"38029\",\"end\":\"38044\"},{\"start\":\"38044\",\"end\":\"38057\"},{\"start\":\"38057\",\"end\":\"38066\"},{\"start\":\"38066\",\"end\":\"38073\"},{\"start\":\"38073\",\"end\":\"38079\"},{\"start\":\"38079\",\"end\":\"38088\"},{\"start\":\"38465\",\"end\":\"38473\"},{\"start\":\"38473\",\"end\":\"38482\"},{\"start\":\"38482\",\"end\":\"38490\"},{\"start\":\"38490\",\"end\":\"38498\"},{\"start\":\"38498\",\"end\":\"38510\"},{\"start\":\"38510\",\"end\":\"38520\"},{\"start\":\"38520\",\"end\":\"38530\"},{\"start\":\"38530\",\"end\":\"38541\"},{\"start\":\"38989\",\"end\":\"38998\"},{\"start\":\"38998\",\"end\":\"39009\"},{\"start\":\"39009\",\"end\":\"39020\"},{\"start\":\"39020\",\"end\":\"39032\"},{\"start\":\"39032\",\"end\":\"39042\"},{\"start\":\"39485\",\"end\":\"39497\"},{\"start\":\"39497\",\"end\":\"39508\"},{\"start\":\"39508\",\"end\":\"39519\"},{\"start\":\"39519\",\"end\":\"39526\"},{\"start\":\"39526\",\"end\":\"39536\"},{\"start\":\"39888\",\"end\":\"39898\"},{\"start\":\"39898\",\"end\":\"39906\"},{\"start\":\"39906\",\"end\":\"39915\"},{\"start\":\"39915\",\"end\":\"39924\"},{\"start\":\"39924\",\"end\":\"39933\"},{\"start\":\"40177\",\"end\":\"40185\"},{\"start\":\"40185\",\"end\":\"40192\"},{\"start\":\"40192\",\"end\":\"40206\"},{\"start\":\"40206\",\"end\":\"40214\"},{\"start\":\"40214\",\"end\":\"40222\"},{\"start\":\"40599\",\"end\":\"40608\"},{\"start\":\"40608\",\"end\":\"40616\"},{\"start\":\"40616\",\"end\":\"40627\"},{\"start\":\"40627\",\"end\":\"40637\"},{\"start\":\"40928\",\"end\":\"40938\"},{\"start\":\"40938\",\"end\":\"40951\"},{\"start\":\"41260\",\"end\":\"41268\"},{\"start\":\"41268\",\"end\":\"41278\"},{\"start\":\"41407\",\"end\":\"41420\"},{\"start\":\"41420\",\"end\":\"41430\"},{\"start\":\"41430\",\"end\":\"41439\"},{\"start\":\"41439\",\"end\":\"41447\"},{\"start\":\"41447\",\"end\":\"41457\"},{\"start\":\"41801\",\"end\":\"41814\"},{\"start\":\"41814\",\"end\":\"41824\"},{\"start\":\"41824\",\"end\":\"41835\"},{\"start\":\"41835\",\"end\":\"41843\"},{\"start\":\"41843\",\"end\":\"41850\"},{\"start\":\"41850\",\"end\":\"41858\"},{\"start\":\"42044\",\"end\":\"42053\"},{\"start\":\"42053\",\"end\":\"42064\"},{\"start\":\"42064\",\"end\":\"42074\"},{\"start\":\"42857\",\"end\":\"42867\"},{\"start\":\"42867\",\"end\":\"42875\"},{\"start\":\"42875\",\"end\":\"42885\"},{\"start\":\"43255\",\"end\":\"43264\"},{\"start\":\"43264\",\"end\":\"43275\"},{\"start\":\"43275\",\"end\":\"43286\"},{\"start\":\"43286\",\"end\":\"43297\"},{\"start\":\"43297\",\"end\":\"43306\"},{\"start\":\"43691\",\"end\":\"43700\"},{\"start\":\"43700\",\"end\":\"43710\"},{\"start\":\"43710\",\"end\":\"43720\"},{\"start\":\"43720\",\"end\":\"43731\"},{\"start\":\"43967\",\"end\":\"43981\"},{\"start\":\"44136\",\"end\":\"44147\"},{\"start\":\"44147\",\"end\":\"44159\"}]", "bib_venue": "[{\"start\":\"33292\",\"end\":\"33332\"},{\"start\":\"33740\",\"end\":\"33798\"},{\"start\":\"34185\",\"end\":\"34235\"},{\"start\":\"34510\",\"end\":\"34575\"},{\"start\":\"34956\",\"end\":\"35008\"},{\"start\":\"35299\",\"end\":\"35371\"},{\"start\":\"35797\",\"end\":\"35846\"},{\"start\":\"36242\",\"end\":\"36307\"},{\"start\":\"36665\",\"end\":\"36688\"},{\"start\":\"36972\",\"end\":\"37022\"},{\"start\":\"37265\",\"end\":\"37353\"},{\"start\":\"37679\",\"end\":\"37738\"},{\"start\":\"38088\",\"end\":\"38137\"},{\"start\":\"38541\",\"end\":\"38598\"},{\"start\":\"39042\",\"end\":\"39107\"},{\"start\":\"39536\",\"end\":\"39585\"},{\"start\":\"39947\",\"end\":\"39951\"},{\"start\":\"40222\",\"end\":\"40287\"},{\"start\":\"40637\",\"end\":\"40689\"},{\"start\":\"40951\",\"end\":\"40999\"},{\"start\":\"41215\",\"end\":\"41258\"},{\"start\":\"41457\",\"end\":\"41516\"},{\"start\":\"41721\",\"end\":\"41799\"},{\"start\":\"42074\",\"end\":\"42093\"},{\"start\":\"42227\",\"end\":\"42249\"},{\"start\":\"42445\",\"end\":\"42478\"},{\"start\":\"42617\",\"end\":\"42666\"},{\"start\":\"42885\",\"end\":\"42942\"},{\"start\":\"43306\",\"end\":\"43371\"},{\"start\":\"43731\",\"end\":\"43748\"},{\"start\":\"43912\",\"end\":\"43965\"},{\"start\":\"44159\",\"end\":\"44207\"},{\"start\":\"33800\",\"end\":\"33819\"},{\"start\":\"34577\",\"end\":\"34632\"},{\"start\":\"35373\",\"end\":\"35439\"},{\"start\":\"35848\",\"end\":\"35891\"},{\"start\":\"36309\",\"end\":\"36364\"},{\"start\":\"37024\",\"end\":\"37068\"},{\"start\":\"38139\",\"end\":\"38182\"},{\"start\":\"38600\",\"end\":\"38651\"},{\"start\":\"39109\",\"end\":\"39164\"},{\"start\":\"40289\",\"end\":\"40344\"},{\"start\":\"41001\",\"end\":\"41043\"},{\"start\":\"42944\",\"end\":\"42995\"},{\"start\":\"43373\",\"end\":\"43432\"},{\"start\":\"43750\",\"end\":\"43761\"},{\"start\":\"44209\",\"end\":\"44251\"}]"}}}, "year": 2023, "month": 12, "day": 17}