{"id": 204866526, "updated": "2023-03-31 13:12:14.878", "metadata": {"title": "A Multi-Organ Nucleus Segmentation Challenge", "authors": "[{\"first\":\"Neeraj\",\"last\":\"Kumar\",\"middle\":[]},{\"first\":\"Ruchika\",\"last\":\"Verma\",\"middle\":[]},{\"first\":\"Deepak\",\"last\":\"Anand\",\"middle\":[]},{\"first\":\"Yanning\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Omer\",\"last\":\"Onder\",\"middle\":[\"Fahri\"]},{\"first\":\"Efstratios\",\"last\":\"Tsougenis\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Pheng-Ann\",\"last\":\"Heng\",\"middle\":[]},{\"first\":\"Jiahui\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Zhiqiang\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Yunzhi\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Navid\",\"last\":\"Koohbanani\",\"middle\":[\"Alemi\"]},{\"first\":\"Mostafa\",\"last\":\"Jahanifar\",\"middle\":[]},{\"first\":\"Neda\",\"last\":\"Tajeddin\",\"middle\":[\"Zamani\"]},{\"first\":\"Ali\",\"last\":\"Gooya\",\"middle\":[]},{\"first\":\"Nasir\",\"last\":\"Rajpoot\",\"middle\":[]},{\"first\":\"Xuhua\",\"last\":\"Ren\",\"middle\":[]},{\"first\":\"Sihang\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Qian\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Dinggang\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Cheng-Kun\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Chi-Hung\",\"last\":\"Weng\",\"middle\":[]},{\"first\":\"Wei-Hsiang\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Chao-Yuan\",\"last\":\"Yeh\",\"middle\":[]},{\"first\":\"Shuang\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Shuoyu\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Pak\",\"last\":\"Yeung\",\"middle\":[\"Hei\"]},{\"first\":\"Peng\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Amirreza\",\"last\":\"Mahbod\",\"middle\":[]},{\"first\":\"Gerald\",\"last\":\"Schaefer\",\"middle\":[]},{\"first\":\"Isabella\",\"last\":\"Ellinger\",\"middle\":[]},{\"first\":\"Rupert\",\"last\":\"Ecker\",\"middle\":[]},{\"first\":\"Orjan\",\"last\":\"Smedby\",\"middle\":[]},{\"first\":\"Chunliang\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Benjamin\",\"last\":\"Chidester\",\"middle\":[]},{\"first\":\"That-Vinh\",\"last\":\"Ton\",\"middle\":[]},{\"first\":\"Minh-Triet\",\"last\":\"Tran\",\"middle\":[]},{\"first\":\"Jian\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Minh\",\"last\":\"Do\",\"middle\":[\"N.\"]},{\"first\":\"Simon\",\"last\":\"Graham\",\"middle\":[]},{\"first\":\"Quoc\",\"last\":\"Vu\",\"middle\":[\"Dang\"]},{\"first\":\"Jin\",\"last\":\"Kwak\",\"middle\":[\"Tae\"]},{\"first\":\"Akshaykumar\",\"last\":\"Gunda\",\"middle\":[]},{\"first\":\"Raviteja\",\"last\":\"Chunduri\",\"middle\":[]},{\"first\":\"Corey\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Xiaoyang\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Dariush\",\"last\":\"Lotfi\",\"middle\":[]},{\"first\":\"Reza\",\"last\":\"Safdari\",\"middle\":[]},{\"first\":\"Antanas\",\"last\":\"Kascenas\",\"middle\":[]},{\"first\":\"Alison\",\"last\":\"O\u2019Neil\",\"middle\":[]},{\"first\":\"Dennis\",\"last\":\"Eschweiler\",\"middle\":[]},{\"first\":\"Johannes\",\"last\":\"Stegmaier\",\"middle\":[]},{\"first\":\"Yanping\",\"last\":\"Cui\",\"middle\":[]},{\"first\":\"Baocai\",\"last\":\"Yin\",\"middle\":[]},{\"first\":\"Kailin\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Xinmei\",\"last\":\"Tian\",\"middle\":[]},{\"first\":\"Philipp\",\"last\":\"Gruening\",\"middle\":[]},{\"first\":\"Erhardt\",\"last\":\"Barth\",\"middle\":[]},{\"first\":\"Elad\",\"last\":\"Arbel\",\"middle\":[]},{\"first\":\"Itay\",\"last\":\"Remer\",\"middle\":[]},{\"first\":\"Amir\",\"last\":\"Ben-Dor\",\"middle\":[]},{\"first\":\"Ekaterina\",\"last\":\"Sirazitdinova\",\"middle\":[]},{\"first\":\"Matthias\",\"last\":\"Kohl\",\"middle\":[]},{\"first\":\"Stefan\",\"last\":\"Braunewell\",\"middle\":[]},{\"first\":\"Yuexiang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Xinpeng\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Linlin\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Krishanu\",\"last\":\"Baksi\",\"middle\":[\"Das\"]},{\"first\":\"Mohammad\",\"last\":\"Khan\",\"middle\":[\"Azam\"]},{\"first\":\"Jaegul\",\"last\":\"Choo\",\"middle\":[]},{\"first\":\"Adri\u00e1n\",\"last\":\"Colomer\",\"middle\":[]},{\"first\":\"Valery\",\"last\":\"Naranjo\",\"middle\":[]},{\"first\":\"Linmin\",\"last\":\"Pei\",\"middle\":[]},{\"first\":\"Khan\",\"last\":\"Iftekharuddin\",\"middle\":[\"M.\"]},{\"first\":\"Kaushiki\",\"last\":\"Roy\",\"middle\":[]},{\"first\":\"Debotosh\",\"last\":\"Bhattacharjee\",\"middle\":[]},{\"first\":\"Anibal\",\"last\":\"Pedraza\",\"middle\":[]},{\"first\":\"Maria\",\"last\":\"Bueno\",\"middle\":[\"Gloria\"]},{\"first\":\"Sabarinathan\",\"last\":\"Devanathan\",\"middle\":[]},{\"first\":\"Saravanan\",\"last\":\"Radhakrishnan\",\"middle\":[]},{\"first\":\"Praveen\",\"last\":\"Koduganty\",\"middle\":[]},{\"first\":\"Zihan\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Guanyu\",\"last\":\"Cai\",\"middle\":[]},{\"first\":\"Xiaojie\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Yuqin\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Amit\",\"last\":\"Sethi\",\"middle\":[]}]", "venue": "IEEE Transactions on Medical Imaging", "journal": "IEEE Transactions on Medical Imaging", "publication_date": {"year": 2020, "month": 5, "day": 1}, "abstract": "Generalized nucleus segmentation techniques can contribute greatly to reducing the time to develop and validate visual biomarkers for new digital pathology datasets. We summarize the results of MoNuSeg 2018 Challenge whose objective was to develop generalizable nuclei segmentation techniques in digital pathology. The challenge was an official satellite event of the MICCAI 2018 conference in which 32 teams with more than 80 participants from geographically diverse institutes participated. Contestants were given a training set with 30 images from seven organs with annotations of 21,623 individual nuclei. A test dataset with 14 images taken from seven organs, including two organs that did not appear in the training set was released without annotations. Entries were evaluated based on average aggregated Jaccard index (AJI) on the test set to prioritize accurate instance segmentation as opposed to mere semantic segmentation. More than half the teams that completed the challenge outperformed a previous baseline. Among the trends observed that contributed to increased accuracy were the use of color normalization as well as heavy data augmentation. Additionally, fully convolutional networks inspired by variants of U-Net, FCN, and Mask-RCNN were popularly used, typically based on ResNet or VGG base architectures. Watershed segmentation on predicted semantic segmentation maps was a popular post-processing strategy. Several of the top techniques compared favorably to an individual human annotator and can be used with confidence for nuclear morphometrics.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "2981994674", "acl": null, "pubmed": "31647422", "pubmedcentral": null, "dblp": "journals/tmi/KumarVAZOTCHLHW20", "doi": "10.1109/tmi.2019.2947628"}}, "content": {"source": {"pdf_hash": "367b159782fc5e7a72d7c949d4c93b3445aac139", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://figshare.com/articles/journal_contribution/A_multi-organ_nucleus_segmentation_challenge/14236643/files/26876531.pdf", "status": "GREEN"}}, "grobid": {"id": "012e3b33edc6f54ed0285931d2551854e0e38084", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/367b159782fc5e7a72d7c949d4c93b3445aac139.txt", "contents": "\nA Multi-Organ Nucleus Segmentation Challenge\nMAY 2020\n\nNeeraj Kumar \nRuchika Verma \nDeepak Anand \nYanning Zhou \nOmer Fahri Onder \nEfstratios Tsougenis \nHao Chen \nPheng-Ann Heng \nJiahui Li \nZhiqiang Hu \nNavidYunzhi Wang \nAlemi Koohbanani \nMostafa Jahanifar \nNeda Zamani Tajeddin \nAli Gooya \nNasir Rajpoot \nXuhua Ren \nSihang Zhou \nQian Wang \nDinggang Shen \nCheng-Kun Yang \nChi-Hung Weng \nWei-Hsiang Yu \nChao-Yuan Yeh \nShuang Yang \nPakShuoyu Xu \nHei Yeung \nPeng Sun \nAmirreza Mahbod \nGerald Schaefer \nIsabella Ellinger \nRupert Ecker \nOrjan Smedby \nChunliang Wang \nThat-Benjamin Chidester \nMinh-Triet Ton \nJian Tran \nMinh N Ma \nSimon Do \nQuocDang Graham \nJin Tae Vu \nAkshaykumar Kwak \nRaviteja Gunda \nCorey Chunduri \nXiaoyang Hu \nDariush Zhou \nReza Lotfi \nAntanas Safdari \nAlison Kascenas \nDennis O&apos;neil \nJohannes Eschweiler \nYanping Stegmaier \nBaocai Cui \nKailin Yin \nXinmei Chen \nPhilipp Tian \nErhardt Gruening \nElad Barth \nItay Arbel \nAmir Remer \nEkaterina Ben-Dor \nMatthias Sirazitdinova \nStefan Kohl \nYuexiang Braunewell \nXinpeng Li \nLinlin Xie \nJun Shen \nKrishanu Ma \nMohammad Azam Das Baksi \nJaegul Khan \nAdri\u00e1n Choo \nValery Colomer \nLinmin Naranjo \nKhan M Pei \nKaushiki Iftekharuddin \nDebotosh Roy \nAnibal Bhattacharjee \nMaria Gloria Pedraza \nSabarinathan Bueno \nSaravanan Devanathan \nPraveen Radhakrishnan \nZihan Koduganty \nGuanyu Wu \nXiaojie Cai \nYuqin Liu \nAmit Wang \nSethi \nA Multi-Organ Nucleus Segmentation Challenge\n\nIEEE TRANSACTIONS ON MEDICAL IMAGING\n395MAY 202010.1109/TMI.2019.2947628Manuscript received July 23, 2019; revised October 6, 2019; accepted October 8, 2019. Date of publication October 23, 2019; date of current version April 30, 2020.1380 (Corresponding author: Neeraj Kumar.) Please see the Acknowledgment section of this article for the author affiliations. This article has supplementary downloadable material available atIndex Terms-Multi-organnucleus segmentationdigi- tal pathologyinstance segmentationaggregated Jaccard index\nGeneralized nucleus segmentation techniques can contribute greatly to reducing the time to develop and validate visual biomarkers for new digital pathology datasets. We summarize the results of MoNuSeg 2018 Challenge whose objective was to develop generalizable nuclei segmentation techniques in digital pathology. The challenge was an official satellite event of the MICCAI 2018 conference in which 32 teams with more than 80 participants from geographically diverse institutes participated. Contestants were given a training set with 30 images from seven organs with annotations of 21,623 individual nuclei. A test dataset with 14 images taken from seven organs, including two organs that did not appear in the training set was released without annotations. Entries were evaluated based on average aggregated Jaccard index (AJI) on the test set to prioritize accurate instance segmentation as opposed to mere semantic segmentation. More than half the teams that completed the challenge outperformed a previous baseline. Among the trends observed that contributed to increased accuracy were the use of color normalization as well as heavy data augmentation. Additionally, fully convolutional networks inspired by variants of U-Net, FCN, and Mask-RCNN were popularly used, typically based on ResNet or VGG base architectures. Watershed segmentation on predicted semantic segmentation maps was a popular post-processing strategy. Several of the top techniques compared favorably to an individual human annotator and can be used with confidence for nuclear morphometrics.Index Terms-Multi-organ, nucleus segmentation, digital pathology, instance segmentation, aggregated Jaccard index.0278-0062\n\nI. INTRODUCTION\n\nE XAMINATION of H&E stained tissue under a microscope remains the mainstay of pathology. The popularity of H&E is due to its low cost and ability to reveal tissue structure and nuclear morphology, which is sufficient for primary diagnosis of several diseases including many cancers. Nuclear shapes and spatial arrangements often form the basis of the examination of H&E stained tissue sections. For example, grading of various types of cancer and risk stratification of patients is usually done by examining different types of nuclei on a tissue slide [1], [2]. Nuclear morphometric features and appearance including the color of their surrounding cytoplasm also helps in identifying various types of cells such as epithelial (glandular), stromal, or inflammatory, which in turn give an idea of the glandular structure and disease presentation at low power [1]- [4]. Segmentation of nuclei accurately in H&E images therefore has high utility in digital pathology.\n\nHowever, nucleus segmentation algorithms that work well on one dataset can perform poorly on a different dataset. There is far too much variation in the appearance of nuclei and their surroundings by organs, disease conditions, and even digital scanner brands or histology technicians. Examples of such variations are shown in Fig. 1, along with the problems of  [5] and Cell Profiler [10] gives merged nuclei (under-segmentation). Marker controlled watershed segmentation [6] and Fiji [9] produces fragmented nuclei (over-segmentation). Segmented nuclei instances are shown in different colors in rows 2-5. some common segmentation algorithms such as Otsu thresholding [5], marker controlled watershed segmentation [6]- [8] or open-source packages like Fiji [9] and Cell Profiler [10]. Segmentation based on machine learning should be able to do a better job, but that makes designing and refining nucleus segmentation algorithms for a new study a tedious task because annotations of thousands of nuclei are needed to train such segmentation models on datasets of interest. Algorithms that generalize to new datasets and organs that were not seen during training can reduce this effort substantially and contribute to rapid experimentation with new phenotypical (visual) biomarkers.\n\nUntil recently, one of the major challenges in training generalized nucleus segmentation models has been the unavailability of large multi-organ datasets with annotated nuclei. In 2017 Kumar et al. [11] released a dataset with more than 21,000 hand-annotated nuclei in H&E stained tissue images acquired at the commonly used 40\u00d7 magnification, sourced from seven organs and multiple hospitals in The Cancer Genome Atlas (TCGA) [12]. Kumar et al. also introduced a metric called Aggregated Jaccard Index (AJI) that is more appropriate to evaluate algorithms for this instance segmentation problem as opposed to other popular metrics such as Dice coefficient, which are more suited for semantic segmentation problems. This is because nucleus segmentation algorithms should not only tell the difference between nuclear and non-nuclear pixels, but they should also be able to tell pixels belonging to two nuclei apart that touch or overlap with each other. Additionally, they had released a trained model that performed reasonably well on unseen organs from the test subset of images.\n\nWe organized the Multi-organ nucleus segmentation (MoNuSeg) Challenge at MICCAI 2018 to build upon Kumar et al.'s work by enlarging the dataset and by encouraging others to introduce new techniques for generalized nucleus segmentation. The participation was wide and several of participants outperformed the previous benchmark [11] by a significant margin. In this paper we describe in detail the objectives of the competition, the released dataset, and the emerging trends of techniques that performed well on the challenge task. We hope that the algorithms described on the challenge webpage [13] will be of use to the computational pathology research community.\n\nThe rest of the paper is organized as follows. We describe the prior work on nucleus segmentation and dataset creation in Section II. We describe the dataset and competition rules in Section III. We present an organized summary of the techniques used by the challenge participants in Section IV. Finally, we discuss emerging trends in nucleus segmentation techniques in Section V.\n\n\nII. BACKGROUND AND PRIOR WORK\n\nIn this section we describe the importance of H&E stained images in histopathology and provide details of some previous notable techniques and datasets for nucleus segmentation from H&E stained images.\n\n\nA. Hematoxylin and Eosin (H&E) Stained Images\n\nPathologists usually observe tissue slides under a microscope at a specific resolution (ranging between 5\u00d7 and 40\u00d7) to report their diagnoses including tumor grade, extent of spread, surgical margin, etc. Their assessment is primarily based on the appearance, size, shape, color and crowding of various nuclei (and glands) in epithelium and stroma. Stains are used to enhance the contrast between these tissue components to help a pathologist looking for specific nuclei and gland features. The combination of hematoxylin and eosin (H&E) is a frequentlyused, universal, and inexpensive staining scheme for general contrast enhancement of histologic structures of a tissue. Hematoxylin renders the nuclei dark blueish purple and the epithelium light purple, while eosin renders the stroma pink. Compared to the general use of H&E, immunohistochemical staining is more specialized as it targets proteins specific to certain disease states for visual identification.\n\nWith the advent of high resolution cameras mounted on microscopes, and more importantly, digital whole slide scanners, it is now possible to acquire whole slide images (WSIs) of the tissue sections for computer assisted diagnosis (CAD). However, the development of CAD systems requires automated extraction of rich information encoded in the pixels of WSIs. Recently, computer based assessment of tissue images has been used for tumor molecular sub-type detection [14], mortality or recurrence prediction [3], [15], and treatment effectiveness prediction [4]. Notably, nucleus detection and segmentation is often a first step for several such CAD systems that rely on nuclear morphometrics for disease state stratification and predictive modelling. Therefore, MoNuSeg 2018 focused on crowdsourcing techniques for nucleus segmentation in H&E stained images captured at 40\u00d7 resolution.\n\n\nB. Nucleus Segmentation Techniques\n\nPrior to the advent of deep learning, approaches to segment nuclei relied on watershed segmentation, morphological operations -such as erosion, dilation, opening and closingcolor-based thresholding, and variants of active contours [6], [7], [16]- [18]. These techniques were often complemented with a collection of pre-processing methods, such as contrast enhancement and deblurring to improve the 'image quality'. Additionally, several post-processing techniques, such as hole filling, noise removal, graph-cuts, etc., were also used to refine the outputs of the segmentation algorithms. However, these approaches do not generalize well across a wide spectrum of tissue images due to reasons such as (a) variations in nuclei morphologies of various organs and tissue types, (b) inter-and intra-nuclei color variations in crowded and chromatin-sparse nuclei, and (c) diversity in the quality of tissue images owing to the differences in image acquisition equipment and slide preparation protocols across hospitals and clinics.\n\nThere have been tremendous advances in the recent years to develop learning-based nucleus segmentation methods to advance the state-of-the-art. Instead of relying on pre-determined algorithms for segmentation, machine learning methods derive data driven algorithms that are trained in a supervised manner based on annotations of nuclear and non-nuclear pixels. This allows them to concentrate on relative differences between nuclear and non-nuclear pixels and their surrounding patches and overcome the aforementioned sources of intra-class variations for better generalized segmentation. The use of learning based approaches started with the extraction of hand-crafted local features based on color and spatial filtering that were fed to traditional learning-based models such as random forests, support vector machines, etc. to segment nuclei and non-nuclei regions [1], [19], [20]. The selection of features is dependent on domain knowledge and trial-and-error for improving nucleus segmentation performance, and yet it is difficult to detect all nuclei with diverse appearances and crowding patterns.\n\nTo circumvent the constraints of hand-crafted features, representation learning algorithms, popularly known as deep learning techniques, have recently emerged. These methods -specifically the ones using convolutional neural networks (CNNs) -have outperformed previous techniques in nucleus detection and segmentation tasks by significant margins [11], [21]- [24]. To use deep learning, the problem is often cast as one of semantic segmentation wherein a two-class probability map for nuclear and non-nuclear regions is usually computed. After semantic segmentation, sophisticated post-processing methods -such as graph partitioning [21], or the computation of distance transform of the nuclear map followed by H-minima transform and region growing [22] -are often used to obtain final nuclei shapes with the desired separation of touching and overlapping nuclei. Semantic segmentation of third class of pixels -those on the nuclear boundaries including that between two touching nuclei -has also been proposed to exclusively refine the separation between the segmented touching and overlapping nuclei [11]. Deep generative models have also been used for accurate nuclei segmentation [25]. More recently, nucleus segmentation problem has been formulated as a regression task to predict a distance map with respect to centroids or boundaries of nuclei using fully convolutional networks (FCNs) to achieve both segmentation and computational performance gains over previous deep learning based approaches [24]. More comprehensive reviews of state-of-the-art nucleus segmentation algorithms can be found in [26] and [27].\n\nOne of the major barriers in out of the box (without re-training) application of state-of-the art deep learning based nucleus segmentation algorithms was the lack of publicly available source codes and trained models by previously published techniques until Kumar et al. [11] and Naylor et al. [24] released their source codes. The other major barrier was the lack of publicly available annotated datasets for benchmarking, which we address next.\n\n\nC. Nucleus Segmentation Datasets\n\nThe success of machine learning and the development of state-of-the art deep learning algorithms in computer vision can be attributed to the healthy competition enabled by publicly available consumer photography datasets such as ImageNet [28] and COCO [29] for object recognition in images. Unfortunately, we do not see similar progress in digital pathology image analysis as there is dearth of labeled and annotated datasets for solving various tasks of pathologist's interest. For example, CAMELYON dataset [30], which is one of the largest histopathology classification dataset, has 1,399 images, while ImageNet [28] has 14 million images. Similarly, CheXpert [31], which is one of the largest medical image segmentation datasets has only 224, 316 images. This is because labeling and annotating pathology images require expert knowledge and diligent work. However, there have been a few recent efforts dedicated to the release of hand-annotated H&E stained tissue slide images for nucleus segmentation as summarized in Table I. These datasets can also be downloaded from the challenge webpage [13]. Please note that we have not included datasets where the nuclei were annotated for detection alone in Table I because these cannot be used for the segmentation task. We also excluded datasets annotated for other specific objectives such as gland  I  PUBLICLY AVAILABLE H&E STAINED TISSUE IMAGE DATASETS ANNOTATED FOR NUCLEUS SEGMENTATION segmentation, mitosis detection, epithelial segmentation, and tumor type classification, as opposed to generalized nucleus segmentation. Most of the datasets listed in Table I focus on a specific organ with the exception of Kumar et al. [11] and Wienert et al. [18].\n\n\nIII. DATASET AND COMPETITION RULES\n\nThe objective of MoNuSeg 2018 was to encourage the development of learning based generalized nucleus segmentation techniques that work right out of the box (without re-training) on a diverse set of H&E-stained tissue images. The images therefore spanned a range of patients, organs, disease states, and sourcing hospitals with potentially different slide preparation and image acquisition methods. Training and testing datasets were carefully curated and the competition rules were crafted in accordance with these objectives.\n\n\nA. Training Dataset\n\nThe training data of MoNuSeg 2018 was the same as that released previously by Kumar et al. [11], which comprised 30 tissues images, each of size 1000 \u00d7 1000, containing 21, 623 hand-annotated nuclear boundaries. Each 1000 \u00d7 1000 image in this dataset was extracted from a separate whole slide image (WSI) (scanned at 40\u00d7) of an individual patient downloaded from TCGA [12]. The dataset represented 7 different organs viz., breast, liver, kidney, prostate, bladder, colon and stomach, and included both benign and diseased tissue samples to ensure diversity of nuclear appearances. Furthermore, the training images came from 18 different hospitals, which introduced another source of appearance variation due to the differences in the staining practices and image acquisition equipments (scanners) across labs. Representative 1000 \u00d7 1000 sub-images from regions dense in nuclei were extracted from patient WSIs to reduce the computational burden of processing WSIs and increase participation. Only one crop per WSI and patient was included in the dataset to ensure diversity. The distribution of training images across organs is shown in Table II while patient and hospital details are available on the challenge webpage [13].\n\nBoth epithelial and stromal nuclei were manually annotated in the 1000 \u00d7 1000 sub-images using Aperio ImageScope \u00ae . Annotations were performed on a 25\" monitor with a 200\u00d7 digital magnification such that each image pixel occupied 5\u00d75 screen pixels to ensure clear visibility for annotating 0 Only annotations verified by a pathologist were considered. nuclear boundaries with a laser mouse. For overlapping nuclei, each multi-nuclear pixel was assigned to the nucleus that appeared to be on top in the 3-D structure. The annotators were engineering students and the quality control was performed by an expert pathologist with years of experience in analyzing tissue sections. Specifically, each H&E image was included in a PowerPoint \u00ae (Microsoft, Redmond WA, USA) slide at 300 dots per inch, along with the annotated boundaries overlaid in bright green. The slides were examined by a pathologist on 25\" monitor to point out missed nuclei, false nuclei, and nuclei with wrong boundaries. For each image, the numbers of each type of error was summed up and divided by the number of annotated nuclei to assess the quality of annotations. As shown in Supplementary Table S2, the error rate for each organ was smaller than 1%. The images and XML files containing pixel coordinates of the annotated nuclear boundaries were released for public use by [11]. The reasons that make this dataset ideal for training a generalized nucleus segmentation model are as follows:\n\n1) It is the largest repository of hand annotated nuclei which aptly represents a miscellany of nuclei shapes, and sizes across multiple organs, disease states and patients. The inclusion of tissue sections from 18 hospitals further augments the richness of this dataset. From Table I, the only multi-organ alternative to it is Wienart et al. [18]. However, Wienart et al. [18] contains tissues from lesser number of organs captured in a single hospital with a single scanner. 2) It extracted only one sub-image of 1000 \u00d7 1000 pixels per patient to maximize nuclear appearance variation. Other datasets mentioned in Table I extracted multiple sub-images from each patient and are thus limited in representing nuclear appearance diversity. For example, WSIs of only 10 and 11 patients were used in Irshad et al. [33] and Naylor et al. [24], respectively. 3) It provided coordinates of annotated nuclear boundaries in an XML format instead of binary masks. This is crucial for learning to separate touching and overlapping nuclei in any automatic nucleus segmentation algorithm. This helped several participants of MoNuSeg 2018 whose nucleus segmentation algorithms explicitly learned to recognize nuclear boundaries in addition to the usual foreground (nuclei pixels) and background classes (non-nuclei pixels). 4) It publicly released the source code of their generalized nucleus segmentation algorithm to catalyze natural competition among a newer generation of automatic nucleus segmentation algorithms. \n\n\nB. Testing Dataset\n\nA new testing set comprising 14 images, each of size 1000 \u00d7 1000 pixels, spanning 7 organs (viz. kidney, lung, colon, breast, bladder, prostate, brain), several disease states (benign and tumors at different stages), and approximately 7,223 annotated nuclei was prepared in the same manner as used for preparing the training data. As shown in Table II, lung and brain tissue images were exclusive to the test set which made it more challenging. More details about the test set are available in the \"supplementary material\" tab of the challenge webpage [13]. The annotations of the test set were not released to the participants. To formally conclude the challenge, with this paper, we are releasing the test annotations on the challenge webpage [13] to facilitate future research in the development of generalized nucleus segmentation algorithms.\n\n\nC. Competition Metric and Results\n\nCompetitors were evaluated only once on the test set. Their latest submission before the deadline was considered as the final submission for evaluation. Average aggregated Jaccard Index (AJI) was used as the metric to evaluate nucleus segmentation performance of the competing algorithms because of its established advantages over other segmentation metrics [11], [24]. The value of AJI ranges between 0 to 1 (higher is better). Computing AJI involves matching every ground truth nuclei to one detected nuclei by maximizing the Jaccard index. The AJI is then equal to the ratio of the sums of the cardinals of intersection and union of these matched ground truth and predicted nuclei. Additionally, all detected components that are not matched are added to the denominator. We reproduce Algorithm 1 detailing AJI computation from [11] with permission. The code for computing AJI is available on the challenge webpage [13].\n\nParticipants were asked to submit 14 segmentation output files (one for each of the 14 test images) to the challenge organizers. For each participant's submission, the organizers then computed 14 AJIs (one for each test image) as per Algorithm 1. If a participant did not submit the results for a particular testing image then AJI value of zero was assigned for that particular image to that participant. The organizers then computed the average AJI (a-AJI) for each participant by averaging image level AJIs across 14 test images. The participants were then ranked in the descending order of a-AJI to obtain the final leaderboard shown in Table III.  Table III also includes the 95% confidence intervals (CIs) around each participant's a-AJI. It is evident that the confidence intervals of the top five techniques exclude a-AJI of the lower ranked techniques. To further assess the overall a-AJI based ranking scheme, we also computed organ level a-AJI Algorithm 1 Aggregated Jaccard Index (AJI) Input: A set of images with a combined set of annotated nuclei G i indexed by i , and a segmented set of nuclei S k indexed by k. Output: Aggregated Jaccard Index A.\n\n1: Initialize overall correct and union pixel counts: C \u2190 0; U \u2190 0 2: for Each ground truth nucleus G i do 3:\nj \u2190 arg max k (|G i \u2229 S k |/|G i \u222a S k |) 4: Update pixel counts: C \u2190 C + |G i \u2229 S j |; U \u2190 U + |G i \u222a S j | 5:\nMark S j used 6: end for 7: for Each segmented nucleus S j do 8: If S k is not used then U \u2190 U + |S k | 9: end for 10: A \u2190 C/U (and confidence intervals), for each participant, by averaging image level AJIs across the number of images that belonged to a specific organ, as shown in Supplementary Table S3. From  Supplementary Table S3, it is evident that (a) the top five techniques perform better than other techniques for each organ as well, (b) the organ is a larger contributor to the variability in performance among the top five techniques than the technique itself, and (c) techniques with a higher overall a-AJI perform better for more organs even among the top five techniques. Specifically, for instance, (a) no technique that is not among the top-five overall breaks into the top-five for more than two organs, (b) breast cancer images had AJI's that were lower by about 0.063 to 0.085 compared to those for bladder for the top-five techniques, and (c) the overall top-ranked technique is also the top-ranked one for all but one organ.\n\n\nIV. SUMMARY OF SEGMENTATION TECHNIQUES\n\nIn this section we present a summary of the techniques used by 32 teams who successfully completed the challenge. We describe the trends observed in pre-processing, data augmentation, modeling, task specification, optimization, and post-processing techniques used by the teams. Specific details of all algorithms are provided in the respective manuscripts submitted by participants as per challenge policies and are available at challenge webpage [13] under \"manuscripts\" tab.\n\n\nA. Pre-Processing and Data Augmentation\n\nPre-processing techniques reduce unwanted variations among input images -from both the training and testing sets -so that the test data distribution is not very different from the training data distribution, by projecting both to the same low-dimensional manifold. On the other hand, data augmentation techniques increase the training data set size by introducing controlled random variations with the hope of creating a training data distribution that covers most of the test data distribution. There are several ways in which the participants altered the given images and their ground truth masks before passing them to the segmentation learning systems in order to increase test accuracy. We summarize some of the interesting trends observed in this challenge. These results are also summarized in Table III. 1) Color and Intensity Normalization: Among the data pre-processing techniques, color and intensity transformations were the most common. Approximately half the teams used color normalization techniques that were specifically developed for pathology images to reduce unwanted color variations between training and testing data. Structure Preserving Color Normalization (SPCN) by Vahadane et al. [35] was used by ten teams due to its demonstrated performance and code availability. Another seven teams used Mecenko et al.'s color normalization scheme [36], out of which one used this technique in combination with another technique by Reinhard et al. [37].\n\nPixel intensity and RGB color transformations that are unspecific to pathology were also used by approximately half of the teams. Most popular among this class of techniques were channel-wise mean subtraction, variance normalization (unit variance), and pixel-value range standardization. Six teams also used either contrast enhancement (or histogram equalization), among which CLAHE [38] was the most commonly used technique.\n\nAmong the unique techniques, one team used image sharpening to remove unwanted variations between training and testing data, one team concatenated HSV and L channels (of L, a*, b* color space) to the RGB channels, and one team used only the blue channel after color normalization of the RGB images.\n\n2) Data Augmentation: Among data augmentation techniques, geometric transformations of the image grid were the most common. For example, rigid transformations of the images -such as rotation (especially, by multiples of 90 degrees) and/or flipping -were used by all but four teams to increase the size of the training data. However, as can be seen in Table III, all of the top twelve teams by a-AJI also augmented the training set using affine transformations, while only five teams below that used this type of augmentation. Another transformation used by the participants was elastic deformation, but it was not very popular among the contestants due to the marginal gain it might afford over an affine transform, while being more complicated to implement. Another geometric transformation is image scaling, which was used by nine contestants.\n\nAnother popular set of augmentation techniques involve changing the pixel values while leaving the geometric structure intact. The most popular among these techniques was the addition of white Gaussian noise, which was used by several of the top performing teams. Another popular technique is color jitter or random HSV shifts, which was used by nine of the top twelve teams. Color jitter is opposite in spirit to color normalization in that it is used to present more color variations of the same input geometric structure to the learning machine with the hope that it will learn to focus on the geometric structure as opposed to the color of nuclei, which may vary between training and testing data sets. Random intensity (brightness) shifts were used by fewer participants, as were blurring by isotropic Gaussian filters of random widths and random image sharpening.\n\nOne interesting data augmentation technique that was used by team CMU-UIUC involved extracting the nuclei, augment them in-place, filling the holes in the background, and then pasting the nuclei back on to the reconstructed background.\n\n\nB. Specification of the Learning Task\n\nThe challenge of nucleus segmentation can be split into two tasks: distinguishing between nuclear and non-nuclear pixels (semantic segmentation) and separating touching nuclei (instance segmentation). The following were three principal types of outputs that the contestants produced using deep learning to meet these two challenges:\n\n1) Binary class probability maps distinguish between pixels that belong to the core of any nucleus versus those that do not. The process of not including the outer periphery of the nuclei into the foreground class helps separate touching nuclei. The lost nuclear territory can later be gained back during post-processing. 2) Ternary class probability map distinguishes between nuclear core, non-nuclear, and nuclear boundary pixels. Nuclear pixels that are on a shared boundary of two touching nuclei are considered to belong to the third class, which has been shown to be useful in separating touching nuclei [11]. 3) Distance map estimates how far a nuclear pixel is from the centroid of a nucleus. Such a map can also distinguish between nuclear and non-nuclear pixels by assigning a fixed value to the latter, such as 0. This is a per-pixel regression problem while the previous two are classification problems. A variant of this distance map is to predict the distance from the boundary of the nucleus. Most teams trained their models to predict variants of one or more of the three types of maps described above. One interesting departure from these three tasks was by Canon Medical Research Europe who predicted a five-class probability map -one for nuclear pixels, and the other four for their probability of belonging to one of the four Cartesian quadrants of a nucleus in order to separate touching nuclei.\n\n\nC. Model Architectures\n\nAll participants used deep convolutional neural networks. Twenty one teams used variants of U-Net [39], of which the original U-Net architecture was used by 11 teams while six teams used base architectures inspired by VGGNet [40], and another 11 teams used architectures inspired by either MRCNN [41], FCN [42], DenseNet [43], or ResNet [44] with different depths. Eight teams used Mask Region with CNN features (MRCNN) [41] as the primary models (of which, two also used U-Net), and two used FCN [42] (of which one also used U-Net). Among the remaining, four teams used their own custom models and architectures, and one each used VGGNet [40], Deep Layer Aggregation [45], PANet [46], and TernausNet [47]. A few teams used multiple architectures for ensembling. Two teams used two architectures each for two different tasks, for example one for semantic segmentation (binary classification between foreground and background pixels) and another for distance map prediction to separate touching nuclei. Notable innovations in model architectures tried by some of the top teams are described in Section IV-G.\n\n\nD. Model Optimization\n\nThe choice of loss function depends on the desired output being predicted. Among various choices for the loss function, pixel-wise cross entropy was used by 28 teams for predicting binary or ternary probability maps, and it was by far the most popular loss function. Ten teams used Dice loss [48], and two teams used its variant such as smooth Jaccard index loss or IOU (intersection over union) loss [49]. For regression problems, seven teams used a smooth L 1 loss. Five teams used mean square error. In total, 16 teams used more than one loss function. Most teams trained their models end-to-end, except when an ensemble of more than one model was used, with the exception of team Yunzhi that used a cascade of two neural networks trained one after another.\n\n\nE. Post-Processing\n\nFor post-processing, watershed segmentation (WS) was used by 17 teams. The most popular way to apply WS was on the nuclear probability pixel map. Additionally, to separate touching nuclei several teams used a neural network to predict the location of a marker for each nucleus, such as by using a nuclear-core probability map, a distance map, or a vector map pointing to the nearest nuclear center. Cleaning up small or weakly detected nuclei was also a common theme. Non-maxima suppression and h-minima were commonly used along with a threshold to clean up false positives.\n\n\nF. Training and Testing Time\n\nTraining times ranged from 2 hours and 17 minutes on using a single Nvidia 1080Ti GPU for team Junma to 42 hours for team Johannes Stegmaier on a similar hardware. Testing times also had a wide range from 1 second per 1000 \u00d7 1000 image for team Unblockabulls on an Amazon Web Services GPU instance powered by an Nvidia K80 GPU to 2 minutes 58 seconds per image on an Nvidia Titan X GPU by team CVBLab. \n\n\nG. Description of the Top-Five Techniques\n\nWe now describe the top-five techniques in more detail as examples of the innovations and diligence with which the participants tried to get robust generalization. Comparative results of the top-five techniques are shown in Fig. 2. Specific details about parameter settings of each algorithm can be found in their respective manuscripts available on the challenge webpage [13] under the \"manuscripts\" tab.\n\n\n1) CUHK & IMSIGHT:\n\nExtensive data augmentation based on random affine transform, rotation, and color jitter was used. Nuclei segmentation task was split into that of nucleus and boundary segmentation. A contour information aggregation network (CIA-Net), inspired by FCN [42] and U-net [39], to simultaneously segment nuclei and boundary was developed using Resnet50 [44] as the backbone architecture. The binary cross-entropy loss function that combined nucleus and boundary annotation errors was used to train the network.\n\nThis algorithm missed some of the smaller nuclei and over-segmented (incorrectly splitting a large nuclei into multiple smaller nuclei) some larger nuclei as shown in Fig. 2.\n\n2) BUPT.J.LI: Images were color normalized and training data was augmented using random cropping, flipping, rotation, scaling, and noise addition. Deep layer aggregation [45] architecture was used to perform three tasks -(1) detect inside-nuclei pixels, (2) estimate the geometric center of the inside-nuclei pixels and (3) estimate a center vector that pointed towards the estimated nuclei center for each inside-nuclei pixel. During inference, the detected nuclei centers and center vectors were used to assign inside-nuclei pixels to one of the overlapping or touching nuclei instances. Since, nuclei boundary information was not explicitly used by the network, this technique produced overly smooth nuclei boundaries (Fig. 2), especially for nuclei with high curvature boundaries.\n\n3) pku.hzq: Extensive data augmentation was used such as flips, rotations, scaling, and noise addition. Then a U-Net [39] was used to predict a ternary class map similar to Kumar et al. [11]. Additionally, an MRCNN [41] was used for top-down instance segmentation. Predictions from the two models were combined as an ensemble for both boundary and nucleus prediction. Then the ensembled nuclei center masks were calculated using morphological eroding of the predicted nuclei pixels. A random walker was used to obtain instance segmentation masks from the ensembled semantic masks and center masks. From Fig. 2, it is evident the boundaries for touching and overlapping nuclei were sometimes unnatural (and occasionally merged) due to pixel-level (semantic) ensembling of the boundary class predictions. 4) Yunzhi: For data preparation contrast-limited adaptive histogram equalization (CLAHE) [38] was used. Data augmentation was done using mirror flipping, rotations that were multiples of 90 degrees, color jitter, Gaussian noise addition, and elastic deformation. For each pixel, the probability of it belonging to a nucleus, or a nucleus boundary and unit vector to the center of the nuclei was computed using two cascaded U-nets [39]. First U-net predicted the inside nuclei pixels and unit vector to the center of the nuclei, which were then used in the subsequent U-net to accurately predict nuclei boundaries. Delineation of touching and overlapping nuclei using this technique heavily relied on accurate estimation of the unit vector that pointed towards the center of a nuclei and due to inaccuracy in precisely estimating the unit vector, this technique produced some over-segmentation and under-segmentation (incorrectly merging two touching or overlapping nuclei) errors (see Fig. 2). 5) Navid Alemi: A neural network predicted both foreground (nuclear core) and background (nuclear boundary) markers. The neural network was a multi-scale feature-sharing network that used extensive skip connections, and was dubbed Spaghet-tiNet. For training the marker head prediction, the network used a combination of weighted Dice and binary cross entropy loss. For predicting the boundaries, it used smooth Jaccard loss and the boundary map was cleaned up using Frangi vesselness filter [50]. Finally, marker-controlled watershed segmentation using predicted markers and boundaries was employed to obtain the instance segmentation maps. Fig. 2 shows that this technique produced overly smooth boundaries with some over-segmentation and under-segmentation errors.\n\n\nH. Ensemble of Top-Five Techniques\n\nUnlike ensembling of semantic segmentation, where class probabilities or decisions can be averaged for each pixel location, ensembling of instance segmentation results is far from trivial. Hence, we developed our own approach to generate the ensemble output of instances segmented by the top-five techniques because literature on this topic is thin and unconvincing. First, we looped over instances of the top-ranked technique and identified the corresponding nuclei instances from the other four techniques on the basis of maximum overlapping pixels. Once the matched instances from all techniques were identified, the corresponding ensemble instance was computed through pixel level majority voting as would be done for semantic segmentation of a single nucleus. Once we looped over all nuclei instances predicted by rank 1 technique, to incorporate the instances missed by rank 1 technique, we looped over all those instances of rank 2 technique that did not find an overlap with those of the rank 1 technique. The process was repeated for rank 3 technique, but not for the other two remaining techniques because the extra instances detected by those two would not have a majority vote from the top three techniques. This ensembling method gave an overall a-AJI of 0.693 (95%CI: 0.682-0.703), which is only marginally better than the individual results of top-five teams.\n\n\nI. Comparison to Inter-Human Agreement\n\nWe re-annotated all 14 test images and computed their a-AJI with the previous annotations. The re-annotation protocol was identical to the one used for creating the training set of MoNuSeg 2018 and the annotator was blinded to the previous test set annotations. The a-AJI between new and old manual annotations across 14 test images was 0.653 (95%CI 0.639-0.667), to which the a-AJI of the top few techniques compares very favorably. This suggests that for nucleus segmentation in H&E images, machine performance is at par with human performance if the image quality is as good as the one used in this challenge.\n\n\nV. DISCUSSION AND CONCLUSION\n\nSome clear trends emerged from analyzing the top few techniques in Table III. While based on a prior idea that color normalization can improve performance of segmentation tasks [11], [51], it is becoming apparent that color augmentation (jitter) trains more robust segmentation models [52]. Most of the top techniques relied on heavy data augmentation including affine transformations, color jitter and noise addition. ResNet [44] seems to be an architecture of choice for several top performers irrespective of how they formulated the learning task. This is because the residual skip connections in ResNet allow backpropagation of gradient deep into the network without dilution. Most of the highly successful networks stuck to predicting pixel-wise class probabilities or using MRCNN [41] to predict instance maps. Watershed segmentation was among the most heavily utilized post-processing techniques. It was applied to the nuclear probability maps, most often coupled with a marker, where the marker was based on detecting the cores of individual nuclei. Some of the aforementioned general trends observed corroborated those found in instance segmentation challenges of general photography images such as Common Objects in Context (COCO) Challenge [29].\n\nAlthough, the participating nuclei segmentation techniques reported significant improvement over the baseline method of [11], more improvements are possible and welcome. To further improve the nuclei segmentation quality, the ambiguity at the boundaries of touching and overlapping nuclei need to be better resolved. Additionally, new techniques should also produce more accurate nuclei boundaries without smoothing out high curvature boundaries. Another direction to be investigated is that of developing techniques that are tolerant of errors in the ground truth annotation itself. The role of generatative adverserial networks (GANs) to further improve nuclei segmentation performance should also be explored [25]. Based on the fact that the top techniques submitted to the MoNuSeg challenge had a-AJIs that were at par with that of a human annotator, it seems that it is time to put some of these techniques to use in nuclear morphometry based disease assessment studies to develop morphometric biomarkers. Finally, the robustness of the dataset and the techniques that have emerged as a part of the MoNuSeg challenge should be assessed for segmenting nuclei under multi-resolution and multi-stain settings. This can be achieved by conducting future competitions on the datasets containing annotated nuclei from images obtained at multiple microscopic resolutions (e.g, 10\u00d7, 20\u00d7, 40\u00d7, etc.) and including annotated nuclei from images stained with different types of stains (e.g. multiple IHC stains).\n\nX. Zhou is with the Department of Computer Science\n\nFig. 1 .\n1Nucleus segmentation challenges: Original H&E images show crowded and chromatin-sparse nuclei with color variation across tissue slides. Otsu thresholding\n\nFig. 2 .\n2Test sub-images taken from different organs exemplifying challenges of working with varied nuclear appearances and crowding patterns are shown in columns. Original H&E images, nuclear boundary annotations and segmentation results from the top-five techniques are shown in rows.\n\nTABLE\n\n\nTABLE II\nIIMoNuSeg 2018 TRAINING AND TESTING DATSET COMPOSITION\n\nTABLE III COMPARISON\nIIIOF TECHNIQUES THAT COMPLETED THE MoNuSeg CHALLENGE\nACKNOWLEDGMENTThe authors are thankful to Gaurav Patel, Yashodhan Ghadge, and Sanjay Kumar for annotating nuclei for the testing set and to NVIDIA for donating the GPUs. N. Kumar, R. Verma, D. Anand, and A. Sethi co-organized the challenge; all others contributed the results of their algorithms.N. Kumar is with the Department of Pathology, The University of Illinois at Chicago, Chicago, IL 60607 USA (e-mail:\nInvariant delineation of nuclear architecture in glioblastoma multiforme for clinical and molecular association. H Chang, IEEE Trans. Med. Imag. 324H. Chang et al., \"Invariant delineation of nuclear architecture in glioblas- toma multiforme for clinical and molecular association,\" IEEE Trans. Med. Imag., vol. 32, no. 4, pp. 670-682, Apr. 2013.\n\nComputer-aided breast cancer diagnosis based on the analysis of cytological images of fine needle biopsies. P Filipczuk, T Fevens, A Krzyzak, R Monczak, IEEE Trans. Med. Imag. 3212P. Filipczuk, T. Fevens, A. Krzyzak, and R. Monczak, \"Computer-aided breast cancer diagnosis based on the analysis of cytological images of fine needle biopsies,\" IEEE Trans. Med. Imag., vol. 32, no. 12, pp. 2169-2178, Dec. 2013.\n\nSystematic analysis of breast cancer morphology uncovers stromal features associated with survival. A H Beck, Sci. Transl. Med. 3108A. H. Beck et al., \"Systematic analysis of breast cancer morphology uncovers stromal features associated with survival,\" Sci. Transl. Med., vol. 3, no. 108, p. 108ra113, 2011.\n\nComputer vision detects subtle histological effects of dutasteride on benign prostate. A Sethi, L Sha, N Kumar, V Macias, R J Deaton, P H Gann, BJU Int. 1221A. Sethi, L. Sha, N. Kumar, V. Macias, R. J. Deaton, and P. H. Gann, \"Computer vision detects subtle histological effects of dutasteride on benign prostate,\" BJU Int., vol. 122, no. 1, pp. 143-151, 2018.\n\nt-tests, F-tests and Otsu's methods for image thresholding. J H Xue, D M Titterington, IEEE Trans. Image Process. 208J. H. Xue and D. M. Titterington, \"t-tests, F-tests and Otsu's methods for image thresholding,\" IEEE Trans. Image Process., vol. 20, no. 8, pp. 2392-2396, Aug. 2011.\n\nNuclei segmentation using markercontrolled watershed, tracking using mean-shift, and Kalman filter in time-lapse microscopy. X Yang, H Li, X Zhou, IEEE Trans. Circuits Syst. I, Reg. Papers. 5311X. Yang, H. Li, and X. Zhou, \"Nuclei segmentation using marker- controlled watershed, tracking using mean-shift, and Kalman filter in time-lapse microscopy,\" IEEE Trans. Circuits Syst. I, Reg. Papers, vol. 53, no. 11, pp. 2405-2414, Nov. 2006.\n\nAutomatic nuclei segmentation in H&E stained breast cancer histopathology images. M Veta, P J Van Diest, R Kornegoor, A Huisman, M A Viergever, J P Pluim, PLoS ONE. 8770221M. Veta, P. J. van Diest, R. Kornegoor, A. Huisman, M. A. Viergever, and J. P. Pluim, \"Automatic nuclei segmentation in H&E stained breast cancer histopathology images,\" PLoS ONE, vol. 8, no. 7, p. e70221, 2013.\n\nTowards generalized nuclear segmentation in histological images. A Vahadane, A Sethi, Proc. IEEE 13th Int. Conf. Bioinf. Bioeng. (BIBE). IEEE 13th Int. Conf. Bioinf. Bioeng. (BIBE)A. Vahadane and A. Sethi, \"Towards generalized nuclear segmentation in histological images,\" in Proc. IEEE 13th Int. Conf. Bioinf. Bio- eng. (BIBE), Nov. 2013, pp. 1-4.\n\nFiji: An open-source platform for biologicalimage analysis. J Schindelin, 10.1038/nmeth.2019Nature Methods. 97J. Schindelin et al., \"Fiji: An open-source platform for biological- image analysis,\" Nature Methods, vol. 9, no. 7, pp. 676-682, Jul. 2012. doi: 10.1038/nmeth.2019.\n\nCellProfiler: Image analysis software for identifying and quantifying cell phenotypes. A E Carpenter, 10.1186/gb-2006-7-10-r100Genome Biol. 710100A. E. Carpenter et al., \"CellProfiler: Image analysis software for iden- tifying and quantifying cell phenotypes,\" Genome Biol., vol. 7, no. 10, p. R100, 2006. doi: 10.1186/gb-2006-7-10-r100.\n\nA dataset and a technique for generalized nuclear segmentation for computational pathology. N Kumar, R Verma, S Sharma, S Bhargava, A Vahadane, A Sethi, IEEE Trans. Med. Imag. 367N. Kumar, R. Verma, S. Sharma, S. Bhargava, A. Vahadane, and A. Sethi, \"A dataset and a technique for generalized nuclear segmen- tation for computational pathology,\" IEEE Trans. Med. Imag., vol. 36, no. 7, pp. 1550-1560, Jul. 2017.\n\nThe Cancer Genome Atlas (TCGA). AccessedThe Cancer Genome Atlas (TCGA). Accessed: Sep. 24, 2019. [Online]. Available: http://cancergenome.nih.gov/\n\nDetecting multiple sub-types of breast cancer in a single patient. R Verma, N Kumar, A Sethi, P H Gann, Proc. IEEE Int. Conf. Image Process. (ICIP). IEEE Int. Conf. Image ess. (ICIP)R. Verma, N. Kumar, A. Sethi, and P. H. Gann, \"Detecting multiple sub-types of breast cancer in a single patient,\" in Proc. IEEE Int. Conf. Image Process. (ICIP), Sep. 2016, pp. 2648-2652.\n\nConvolutional neural networks for prostate cancer recurrence prediction. N Kumar, 101400HProc. SPIE. SPIE10140N. Kumar et al., \"Convolutional neural networks for prostate can- cer recurrence prediction,\" Proc. SPIE, vol. 10140, Mar. 2017, Art. no. 101400H.\n\nAn integrated region-, boundary-, shapebased active contour for multiple object overlap resolution in histological imagery. S Ali, A Madabhushi, IEEE Trans. Med. Imag. 317S. Ali and A. Madabhushi, \"An integrated region-, boundary-, shape- based active contour for multiple object overlap resolution in histological imagery,\" IEEE Trans. Med. Imag., vol. 31, no. 7, pp. 1448-1460, Jul. 2012.\n\nImproved automatic detection and segmentation of cell nuclei in histopathology images. Y Al-Kofahi, W Lassoued, W Lee, B Roysam, IEEE Trans. Biomed. Eng. 574Y. Al-Kofahi, W. Lassoued, W. Lee, and B. Roysam, \"Improved automatic detection and segmentation of cell nuclei in histopathology images,\" IEEE Trans. Biomed. Eng., vol. 57, no. 4, pp. 841-852, Apr. 2010.\n\nDetection and segmentation of cell nuclei in virtual microscopy images: A minimum-model approach. S Wienert, Sci. Rep. 2503S. Wienert et al., \"Detection and segmentation of cell nuclei in virtual microscopy images: A minimum-model approach,\" Sci. Rep., vol. 2, Nov. 2012, Art. no. 503.\n\nPartitioning histopathological images: An integrated framework for supervised colortexture segmentation and cell splitting. H Kong, M Gurcan, K Belkacem-Boussaid, IEEE Trans. Med. Imag. 309H. Kong, M. Gurcan, and K. Belkacem-Boussaid, \"Partitioning histopathological images: An integrated framework for supervised color- texture segmentation and cell splitting,\" IEEE Trans. Med. Imag., vol. 30, no. 9, pp. 1661-1677, Sep. 2011.\n\nSmall blob identification in medical images using regional features from optimum scale. M Zhang, T Wu, K M Bennett, IEEE Trans. Biomed. Eng. 624M. Zhang, T. Wu, and K. M. Bennett, \"Small blob identification in medical images using regional features from optimum scale,\" IEEE Trans. Biomed. Eng., vol. 62, no. 4, pp. 1051-1062, Apr. 2015.\n\nAccurate segmentation of cervical cytoplasm and nuclei based on multiscale convolutional network and graph partitioning. Y Song, L Zhang, S Chen, D Ni, B Lei, T Wang, IEEE Trans. Biomed. Eng. 6210Y. Song, L. Zhang, S. Chen, D. Ni, B. Lei, and T. Wang, \"Accurate segmentation of cervical cytoplasm and nuclei based on multiscale convolutional network and graph partitioning,\" IEEE Trans. Biomed. Eng., vol. 62, no. 10, pp. 2421-2433, Oct. 2015.\n\nAn automatic learning-based framework for robust nucleus segmentation. F Xing, Y Xie, L Yang, IEEE Trans. Med. Imag. 352F. Xing, Y. Xie, and L. Yang, \"An automatic learning-based framework for robust nucleus segmentation,\" IEEE Trans. Med. Imag., vol. 35, no. 2, pp. 550-566, Feb. 2016.\n\nLocality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images. K Sirinukunwattana, S E A Raza, Y.-W Tsang, D R J Snead, I A Cree, N M Rajpoot, IEEE Trans. Med. Imag. 355K. Sirinukunwattana, S. E. A. Raza, Y.-W. Tsang, D. R. J. Snead, I. A. Cree, and N. M. Rajpoot, \"Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images,\" IEEE Trans. Med. Imag., vol. 35, no. 5, pp. 1196-1206, May 2016.\n\nSegmentation of nuclei in histopathology images by deep regression of the distance map. P Naylor, M La\u00e9, F Reyal, T Walter, IEEE Trans. Med. Imag. 382P. Naylor, M. La\u00e9, F. Reyal, and T. Walter, \"Segmentation of nuclei in histopathology images by deep regression of the distance map,\" IEEE Trans. Med. Imag., vol. 38, no. 2, pp. 448-459, Feb. 2019.\n\nDeep adversarial training for multi-organ nuclei segmentation in histopathology images. F Mahmood, arXiv:1810.00236F. Mahmood et al., \"Deep adversarial training for multi-organ nuclei seg- mentation in histopathology images,\" 2018, arXiv:1810.00236. [Online].\n\nMethods for nuclei detection, segmentation, and classification in digital histopathology: A review-Current status and future potential. H Irshad, A Veillard, L Roux, D Racoceanu, IEEE Rev. Biomed. Eng. 7H. Irshad, A. Veillard, L. Roux, and D. Racoceanu, \"Methods for nuclei detection, segmentation, and classification in digital histopathology: A review-Current status and future potential,\" IEEE Rev. Biomed. Eng., vol. 7, pp. 97-114, 2014.\n\nRobust nucleus/cell detection and segmentation in digital pathology and microscopy images: A comprehensive review. F Xing, L Yang, IEEE Rev. Biomed. Eng. 9F. Xing and L. Yang, \"Robust nucleus/cell detection and segmentation in digital pathology and microscopy images: A comprehensive review,\" IEEE Rev. Biomed. Eng., vol. 9, pp. 234-263, Jan. 2016.\n\nImageNet large scale visual recognition challenge. O Russakovsky, Int. J. Comput. Vis. 1153O. Russakovsky et al., \"ImageNet large scale visual recognition challenge,\" Int. J. Comput. Vis., vol. 115, no. 3, pp. 211-252, Dec. 2015.\n\nMicrosoft COCO: Common objects in context. T.-Y Lin, M E E Maire, Proc. Eur. Conf. Comput. Vis. Eur. Conf. Comput. VisOxford, U.K.Oxford Univ. PressT.-Y. Lin and M. E. E. Maire, \"Microsoft COCO: Common objects in context,\" in Proc. Eur. Conf. Comput. Vis. Oxford, U.K.: Oxford Univ. Press, 2014, pp. 740-755.\n\n1399 H&E-stained sentinel lymph node sections of breast cancer patients: The CAMELYON dataset. G Litjens, GigaScience. 7665G. Litjens et al., \"1399 H&E-stained sentinel lymph node sections of breast cancer patients: The CAMELYON dataset,\" GigaScience, vol. 7, no. 6, p. giy065, 2018.\n\nChexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. J Irvin, arXiv:1901.07031J. Irvin et al., \"Chexpert: A large chest radiograph dataset with uncer- tainty labels and expert comparison,\" 2019, arXiv:1901.07031. [Online]. Available: https://arxiv.org/abs/1901.07031\n\nDeep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases. A Janowczyk, A Madabhushi, J. Pathol. Inform. 729A. Janowczyk and A. Madabhushi, \"Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases,\" J. Pathol. Inform., vol. 7, p. 29, Jul. 2016.\n\nCrowdsourcing image annotation for nucleus detection and segmentation in computational pathology: Evaluating experts, automated methods, and the crowd. H Irshad, Proc. Pacific Symp. Biocomput. Pacific Symp. BiocomputH. Irshad et al., \"Crowdsourcing image annotation for nucleus detec- tion and segmentation in computational pathology: Evaluating experts, automated methods, and the crowd,\" in Proc. Pacific Symp. Biocomput., 2015, pp. 294-305.\n\nA biosegmentation benchmark for evaluation of bioimage analysis methods. E D Gelasca, B Obara, D Fedorov, K Kvilekval, B Manjunath, BMC Bioinform. 101368E. D. Gelasca, B. Obara, D. Fedorov, K. Kvilekval, and B. Manjunath, \"A biosegmentation benchmark for evaluation of bioimage analysis methods,\" BMC Bioinform., vol. 10, no. 1, p. 368, 2009.\n\nStructure-preserving color normalization and sparse stain separation for histological images. A Vahadane, IEEE Trans. Med. Imag. 358A. Vahadane et al., \"Structure-preserving color normalization and sparse stain separation for histological images,\" IEEE Trans. Med. Imag., vol. 35, no. 8, pp. 1962-1971, Aug. 2016.\n\nA method for normalizing histology slides for quantitative analysis. M Macenko, Proc. IEEE Int. Symp. Biomed. Imag., From Nano Macro. IEEE Int. Symp. Biomed. Imag., From Nano MacroM. Macenko et al., \"A method for normalizing histology slides for quantitative analysis,\" in Proc. IEEE Int. Symp. Biomed. Imag., From Nano Macro, Jun. 2009, pp. 1107-1110.\n\nColor transfer between images. E Reinhard, M Adhikhmin, B Gooch, P Shirley, IEEE Comput. Graph. Appl. 215E. Reinhard, M. Adhikhmin, B. Gooch, and P. Shirley, \"Color transfer between images,\" IEEE Comput. Graph. Appl., vol. 21, no. 5, pp. 34-41, Sep./Oct. 2001.\n\nAdaptive histogram equalization and its variations. S M Pizer, Comput. Vis., Graph., Image Process. 393S. M. Pizer et al., \"Adaptive histogram equalization and its variations,\" Comput. Vis., Graph., Image Process., vol. 39, no. 3, pp. 355-368, 1987.\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, Proc. Int. Conf. Med. Image Comput. Int. Conf. Med. Image ComputMunich, GermanySpringerO. Ronneberger, P. Fischer, and T. Brox, \"U-net: Convolutional networks for biomedical image segmentation,\" in Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent. Munich, Germany: Springer, 2015, pp. 234-241.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556K. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" 2014, arXiv:1409.1556. [Online].\n\nMask R-CNN. K He, G Gkioxari, P Doll\u00e1r, R Girshick, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisK. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick, \"Mask R-CNN,\" in Proc. IEEE Int. Conf. Comput. Vis., Oct. 2017, pp. 2961-2969.\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitJ. Long, E. Shelhamer, and T. Darrell, \"Fully convolutional networks for semantic segmentation,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2015, pp. 3431-3440.\n\nDensely connected convolutional networks. G Huang, Z Liu, L Van Der Maaten, K Q Weinberger, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger, \"Densely connected convolutional networks,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 2261-2269.\n\nInception-v4, inception-resnet and the impact of residual connections on learning. C Szegedy, S Ioffe, V Vanhoucke, A A , Proc. 31st AAAI Conf. Artif. Intell. 31st AAAI Conf. Artif. IntellC. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, \"Inception-v4, inception-resnet and the impact of residual connections on learning,\" in Proc. 31st AAAI Conf. Artif. Intell., 2017, pp. 1-7.\n\nDeep layer aggregation. F Yu, D Wang, E Shelhamer, T Darrell, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitF. Yu, D. Wang, E. Shelhamer, and T. Darrell, \"Deep layer aggrega- tion,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 2403-2412.\n\nPath aggregation network for instance segmentation. S Liu, L Qi, H Qin, J Shi, J Jia, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitS. Liu, L. Qi, H. Qin, J. Shi, and J. Jia, \"Path aggregation network for instance segmentation,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 8759-8768.\n\nTernausNet: U-net with vgg11 encoder pretrained on imagenet for image segmentation. V Iglovikov, A Shvets, arXiv:1801.05746V. Iglovikov and A. Shvets, \"TernausNet: U-net with vgg11 encoder pre- trained on imagenet for image segmentation,\" 2018, arXiv:1801.05746. [Online]. Available: https://arxiv.org/abs/1801.05746\n\nMeasures of the amount of ecologic association between species. L R Dice, Ecology. 263L. R. Dice, \"Measures of the amount of ecologic association between species,\" Ecology, vol. 26, no. 3, pp. 297-302, 1945.\n\n\u00c9tude comparative de la distribution florale dans une portion des alpes et des jura. P Jaccard, Bull Soc. Vaudoise Sci. Nat. 37142P. Jaccard, \"\u00c9tude comparative de la distribution florale dans une portion des alpes et des jura,\" Bull Soc. Vaudoise Sci. Nat., vol. 37, no. 142, pp. 547-579, Jan. 1901.\n\nMultiscale vessel enhancement filtering. A F Frangi, W J Niessen, K L Vincken, M A Viergever, Proc. Int. Conf. Med. Image Comput. Int. Conf. Med. Image ComputSpringerA. F. Frangi, W. J. Niessen, K. L. Vincken, and M. A. Viergever, \"Multiscale vessel enhancement filtering,\" in Proc. Int. Conf. Med. Image Comput. Comput. Assist. Intervent. Springer, 1998, pp. 130-137.\n\nEmpirical comparison of color normalization methods for epithelial-stromal classification in H and E images. A , J. Pathol. Inform. 717A. Sethi al., \"Empirical comparison of color normalization methods for epithelial-stromal classification in H and E images,\" J. Pathol. Inform., vol. 7, p. 17, Apr. 2016.\n\nQuantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology. D Tellez, arXiv:1902.06543D. Tellez et al., \"Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for com- putational pathology,\" 2019, arXiv:1902.06543. [Online]. Available: https://arxiv.org/abs/1902.06543\n\nOn Stain Normalization in Deep Learning. A Janowczyk, A. Janowczyk. (2018). On Stain Normalization in Deep Learn- ing. Accessed: Sep. 24, 2019. [Online]. Available: http://www. andrewjanowczyk.com/on-stain-normalization-in-deep-learning/\n", "annotations": {"author": "[{\"end\":69,\"start\":56},{\"end\":84,\"start\":70},{\"end\":98,\"start\":85},{\"end\":112,\"start\":99},{\"end\":130,\"start\":113},{\"end\":152,\"start\":131},{\"end\":162,\"start\":153},{\"end\":178,\"start\":163},{\"end\":189,\"start\":179},{\"end\":202,\"start\":190},{\"end\":220,\"start\":203},{\"end\":238,\"start\":221},{\"end\":257,\"start\":239},{\"end\":279,\"start\":258},{\"end\":290,\"start\":280},{\"end\":305,\"start\":291},{\"end\":316,\"start\":306},{\"end\":329,\"start\":317},{\"end\":340,\"start\":330},{\"end\":355,\"start\":341},{\"end\":371,\"start\":356},{\"end\":386,\"start\":372},{\"end\":401,\"start\":387},{\"end\":416,\"start\":402},{\"end\":429,\"start\":417},{\"end\":443,\"start\":430},{\"end\":454,\"start\":444},{\"end\":464,\"start\":455},{\"end\":481,\"start\":465},{\"end\":498,\"start\":482},{\"end\":517,\"start\":499},{\"end\":531,\"start\":518},{\"end\":545,\"start\":532},{\"end\":561,\"start\":546},{\"end\":586,\"start\":562},{\"end\":602,\"start\":587},{\"end\":613,\"start\":603},{\"end\":624,\"start\":614},{\"end\":634,\"start\":625},{\"end\":651,\"start\":635},{\"end\":663,\"start\":652},{\"end\":681,\"start\":664},{\"end\":697,\"start\":682},{\"end\":713,\"start\":698},{\"end\":726,\"start\":714},{\"end\":740,\"start\":727},{\"end\":752,\"start\":741},{\"end\":769,\"start\":753},{\"end\":786,\"start\":770},{\"end\":806,\"start\":787},{\"end\":827,\"start\":807},{\"end\":846,\"start\":828},{\"end\":858,\"start\":847},{\"end\":870,\"start\":859},{\"end\":883,\"start\":871},{\"end\":897,\"start\":884},{\"end\":915,\"start\":898},{\"end\":927,\"start\":916},{\"end\":939,\"start\":928},{\"end\":951,\"start\":940},{\"end\":970,\"start\":952},{\"end\":994,\"start\":971},{\"end\":1007,\"start\":995},{\"end\":1028,\"start\":1008},{\"end\":1040,\"start\":1029},{\"end\":1052,\"start\":1041},{\"end\":1062,\"start\":1053},{\"end\":1075,\"start\":1063},{\"end\":1100,\"start\":1076},{\"end\":1113,\"start\":1101},{\"end\":1126,\"start\":1114},{\"end\":1142,\"start\":1127},{\"end\":1158,\"start\":1143},{\"end\":1170,\"start\":1159},{\"end\":1194,\"start\":1171},{\"end\":1208,\"start\":1195},{\"end\":1230,\"start\":1209},{\"end\":1252,\"start\":1231},{\"end\":1272,\"start\":1253},{\"end\":1294,\"start\":1273},{\"end\":1317,\"start\":1295},{\"end\":1334,\"start\":1318},{\"end\":1345,\"start\":1335},{\"end\":1358,\"start\":1346},{\"end\":1369,\"start\":1359},{\"end\":1380,\"start\":1370},{\"end\":1387,\"start\":1381}]", "publisher": null, "author_last_name": "[{\"end\":68,\"start\":63},{\"end\":83,\"start\":78},{\"end\":97,\"start\":92},{\"end\":111,\"start\":107},{\"end\":129,\"start\":124},{\"end\":151,\"start\":142},{\"end\":161,\"start\":157},{\"end\":177,\"start\":173},{\"end\":188,\"start\":186},{\"end\":201,\"start\":199},{\"end\":219,\"start\":215},{\"end\":237,\"start\":227},{\"end\":256,\"start\":247},{\"end\":278,\"start\":270},{\"end\":289,\"start\":284},{\"end\":304,\"start\":297},{\"end\":315,\"start\":312},{\"end\":328,\"start\":324},{\"end\":339,\"start\":335},{\"end\":354,\"start\":350},{\"end\":370,\"start\":366},{\"end\":385,\"start\":381},{\"end\":400,\"start\":398},{\"end\":415,\"start\":412},{\"end\":428,\"start\":424},{\"end\":442,\"start\":440},{\"end\":453,\"start\":448},{\"end\":463,\"start\":460},{\"end\":480,\"start\":474},{\"end\":497,\"start\":489},{\"end\":516,\"start\":508},{\"end\":530,\"start\":525},{\"end\":544,\"start\":538},{\"end\":560,\"start\":556},{\"end\":585,\"start\":576},{\"end\":601,\"start\":598},{\"end\":612,\"start\":608},{\"end\":623,\"start\":621},{\"end\":633,\"start\":631},{\"end\":650,\"start\":644},{\"end\":662,\"start\":660},{\"end\":680,\"start\":676},{\"end\":696,\"start\":691},{\"end\":712,\"start\":704},{\"end\":725,\"start\":723},{\"end\":739,\"start\":735},{\"end\":751,\"start\":746},{\"end\":768,\"start\":761},{\"end\":785,\"start\":777},{\"end\":805,\"start\":794},{\"end\":826,\"start\":816},{\"end\":845,\"start\":836},{\"end\":857,\"start\":854},{\"end\":869,\"start\":866},{\"end\":882,\"start\":878},{\"end\":896,\"start\":892},{\"end\":914,\"start\":906},{\"end\":926,\"start\":921},{\"end\":938,\"start\":933},{\"end\":950,\"start\":945},{\"end\":969,\"start\":962},{\"end\":993,\"start\":980},{\"end\":1006,\"start\":1002},{\"end\":1027,\"start\":1017},{\"end\":1039,\"start\":1037},{\"end\":1051,\"start\":1048},{\"end\":1061,\"start\":1057},{\"end\":1074,\"start\":1072},{\"end\":1099,\"start\":1090},{\"end\":1112,\"start\":1108},{\"end\":1125,\"start\":1121},{\"end\":1141,\"start\":1134},{\"end\":1157,\"start\":1150},{\"end\":1169,\"start\":1166},{\"end\":1193,\"start\":1180},{\"end\":1207,\"start\":1204},{\"end\":1229,\"start\":1216},{\"end\":1251,\"start\":1244},{\"end\":1271,\"start\":1266},{\"end\":1293,\"start\":1283},{\"end\":1316,\"start\":1303},{\"end\":1333,\"start\":1324},{\"end\":1344,\"start\":1342},{\"end\":1357,\"start\":1354},{\"end\":1368,\"start\":1365},{\"end\":1379,\"start\":1375},{\"end\":1386,\"start\":1381}]", "author_first_name": "[{\"end\":62,\"start\":56},{\"end\":77,\"start\":70},{\"end\":91,\"start\":85},{\"end\":106,\"start\":99},{\"end\":117,\"start\":113},{\"end\":123,\"start\":118},{\"end\":141,\"start\":131},{\"end\":156,\"start\":153},{\"end\":172,\"start\":163},{\"end\":185,\"start\":179},{\"end\":198,\"start\":190},{\"end\":214,\"start\":208},{\"end\":226,\"start\":221},{\"end\":246,\"start\":239},{\"end\":262,\"start\":258},{\"end\":269,\"start\":263},{\"end\":283,\"start\":280},{\"end\":296,\"start\":291},{\"end\":311,\"start\":306},{\"end\":323,\"start\":317},{\"end\":334,\"start\":330},{\"end\":349,\"start\":341},{\"end\":365,\"start\":356},{\"end\":380,\"start\":372},{\"end\":397,\"start\":387},{\"end\":411,\"start\":402},{\"end\":423,\"start\":417},{\"end\":439,\"start\":433},{\"end\":447,\"start\":444},{\"end\":459,\"start\":455},{\"end\":473,\"start\":465},{\"end\":488,\"start\":482},{\"end\":507,\"start\":499},{\"end\":524,\"start\":518},{\"end\":537,\"start\":532},{\"end\":555,\"start\":546},{\"end\":575,\"start\":567},{\"end\":597,\"start\":587},{\"end\":607,\"start\":603},{\"end\":618,\"start\":614},{\"end\":620,\"start\":619},{\"end\":630,\"start\":625},{\"end\":643,\"start\":639},{\"end\":655,\"start\":652},{\"end\":659,\"start\":656},{\"end\":675,\"start\":664},{\"end\":690,\"start\":682},{\"end\":703,\"start\":698},{\"end\":722,\"start\":714},{\"end\":734,\"start\":727},{\"end\":745,\"start\":741},{\"end\":760,\"start\":753},{\"end\":776,\"start\":770},{\"end\":793,\"start\":787},{\"end\":815,\"start\":807},{\"end\":835,\"start\":828},{\"end\":853,\"start\":847},{\"end\":865,\"start\":859},{\"end\":877,\"start\":871},{\"end\":891,\"start\":884},{\"end\":905,\"start\":898},{\"end\":920,\"start\":916},{\"end\":932,\"start\":928},{\"end\":944,\"start\":940},{\"end\":961,\"start\":952},{\"end\":979,\"start\":971},{\"end\":1001,\"start\":995},{\"end\":1016,\"start\":1008},{\"end\":1036,\"start\":1029},{\"end\":1047,\"start\":1041},{\"end\":1056,\"start\":1053},{\"end\":1071,\"start\":1063},{\"end\":1084,\"start\":1076},{\"end\":1089,\"start\":1085},{\"end\":1107,\"start\":1101},{\"end\":1120,\"start\":1114},{\"end\":1133,\"start\":1127},{\"end\":1149,\"start\":1143},{\"end\":1163,\"start\":1159},{\"end\":1165,\"start\":1164},{\"end\":1179,\"start\":1171},{\"end\":1203,\"start\":1195},{\"end\":1215,\"start\":1209},{\"end\":1236,\"start\":1231},{\"end\":1243,\"start\":1237},{\"end\":1265,\"start\":1253},{\"end\":1282,\"start\":1273},{\"end\":1302,\"start\":1295},{\"end\":1323,\"start\":1318},{\"end\":1341,\"start\":1335},{\"end\":1353,\"start\":1346},{\"end\":1364,\"start\":1359},{\"end\":1374,\"start\":1370}]", "author_affiliation": null, "title": "[{\"end\":45,\"start\":1},{\"end\":1432,\"start\":1388}]", "venue": "[{\"end\":1470,\"start\":1434}]", "abstract": "[{\"end\":3660,\"start\":1968}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4234,\"start\":4231},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4239,\"start\":4236},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4539,\"start\":4536},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4544,\"start\":4541},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5010,\"start\":5007},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5033,\"start\":5029},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5120,\"start\":5117},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5133,\"start\":5130},{\"end\":5251,\"start\":5247},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5317,\"start\":5314},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5363,\"start\":5360},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5368,\"start\":5365},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5406,\"start\":5403},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5429,\"start\":5425},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6131,\"start\":6127},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6360,\"start\":6356},{\"end\":6374,\"start\":6362},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7342,\"start\":7338},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9775,\"start\":9771},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9815,\"start\":9812},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9821,\"start\":9817},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9865,\"start\":9862},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10463,\"start\":10460},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10468,\"start\":10465},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10474,\"start\":10470},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10480,\"start\":10476},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12128,\"start\":12125},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12134,\"start\":12130},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12140,\"start\":12136},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12713,\"start\":12709},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12719,\"start\":12715},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12725,\"start\":12721},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12999,\"start\":12995},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13115,\"start\":13111},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13468,\"start\":13464},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13550,\"start\":13546},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13869,\"start\":13865},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13970,\"start\":13966},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13979,\"start\":13975},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14257,\"start\":14253},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14280,\"start\":14276},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14707,\"start\":14703},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14721,\"start\":14717},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14978,\"start\":14974},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15084,\"start\":15080},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":15132,\"start\":15128},{\"end\":15566,\"start\":15562},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16147,\"start\":16143},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16171,\"start\":16167},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16856,\"start\":16852},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17133,\"start\":17129},{\"end\":17985,\"start\":17981},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19338,\"start\":19334},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19799,\"start\":19795},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19829,\"start\":19825},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20267,\"start\":20263},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20290,\"start\":20286},{\"end\":21537,\"start\":21533},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22227,\"start\":22223},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22233,\"start\":22229},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22699,\"start\":22695},{\"end\":22786,\"start\":22782},{\"end\":24202,\"start\":24200},{\"end\":24239,\"start\":24237},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26995,\"start\":26991},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":27150,\"start\":27146},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":27250,\"start\":27246},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":27641,\"start\":27637},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30924,\"start\":30920},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":31855,\"start\":31851},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":31982,\"start\":31978},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":32053,\"start\":32049},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":32063,\"start\":32059},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":32078,\"start\":32074},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":32094,\"start\":32090},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":32177,\"start\":32173},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":32254,\"start\":32250},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":32396,\"start\":32392},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":32425,\"start\":32421},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":32437,\"start\":32433},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":32458,\"start\":32454},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":33181,\"start\":33177},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":33290,\"start\":33286},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":35406,\"start\":35402},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":35421,\"start\":35417},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":35502,\"start\":35498},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":36007,\"start\":36003},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":36740,\"start\":36736},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":36809,\"start\":36805},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":36838,\"start\":36834},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":37515,\"start\":37511},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":37856,\"start\":37852},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":38912,\"start\":38908},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":41465,\"start\":41461},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":41471,\"start\":41467},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":41573,\"start\":41569},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":41714,\"start\":41710},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":42074,\"start\":42070},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":42539,\"start\":42535},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":42666,\"start\":42662},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":43258,\"start\":43254}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":44264,\"start\":44099},{\"attributes\":{\"id\":\"fig_1\"},\"end\":44553,\"start\":44265},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":44561,\"start\":44554},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":44626,\"start\":44562},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":44702,\"start\":44627}]", "paragraph": "[{\"end\":4642,\"start\":3679},{\"end\":5927,\"start\":4644},{\"end\":7009,\"start\":5929},{\"end\":7675,\"start\":7011},{\"end\":8057,\"start\":7677},{\"end\":8292,\"start\":8091},{\"end\":9305,\"start\":8342},{\"end\":10190,\"start\":9307},{\"end\":11255,\"start\":10229},{\"end\":12361,\"start\":11257},{\"end\":13980,\"start\":12363},{\"end\":14428,\"start\":13982},{\"end\":16172,\"start\":14465},{\"end\":16737,\"start\":16211},{\"end\":17986,\"start\":16761},{\"end\":19450,\"start\":17988},{\"end\":20958,\"start\":19452},{\"end\":21827,\"start\":20981},{\"end\":22787,\"start\":21865},{\"end\":23951,\"start\":22789},{\"end\":24062,\"start\":23953},{\"end\":25221,\"start\":24175},{\"end\":25740,\"start\":25264},{\"end\":27251,\"start\":25784},{\"end\":27679,\"start\":27253},{\"end\":27979,\"start\":27681},{\"end\":28826,\"start\":27981},{\"end\":29697,\"start\":28828},{\"end\":29934,\"start\":29699},{\"end\":30308,\"start\":29976},{\"end\":31726,\"start\":30310},{\"end\":32859,\"start\":31753},{\"end\":33645,\"start\":32885},{\"end\":34242,\"start\":33668},{\"end\":34677,\"start\":34275},{\"end\":35128,\"start\":34723},{\"end\":35655,\"start\":35151},{\"end\":35831,\"start\":35657},{\"end\":36617,\"start\":35833},{\"end\":39183,\"start\":36619},{\"end\":40596,\"start\":39222},{\"end\":41251,\"start\":40639},{\"end\":42540,\"start\":41284},{\"end\":44046,\"start\":42542},{\"end\":44098,\"start\":44048}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":24174,\"start\":24063}]", "table_ref": "[{\"end\":15495,\"start\":15488},{\"end\":15677,\"start\":15670},{\"end\":15905,\"start\":15815},{\"end\":16081,\"start\":16074},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17906,\"start\":17898},{\"end\":19159,\"start\":19137},{\"end\":19736,\"start\":19729},{\"end\":20094,\"start\":20068},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":21332,\"start\":21324},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23450,\"start\":23429},{\"end\":24509,\"start\":24457},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26595,\"start\":26585},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28341,\"start\":28332},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":41360,\"start\":41351}]", "section_header": "[{\"end\":3677,\"start\":3662},{\"end\":8089,\"start\":8060},{\"end\":8340,\"start\":8295},{\"end\":10227,\"start\":10193},{\"end\":14463,\"start\":14431},{\"end\":16209,\"start\":16175},{\"end\":16759,\"start\":16740},{\"end\":20979,\"start\":20961},{\"end\":21863,\"start\":21830},{\"end\":25262,\"start\":25224},{\"end\":25782,\"start\":25743},{\"end\":29974,\"start\":29937},{\"end\":31751,\"start\":31729},{\"end\":32883,\"start\":32862},{\"end\":33666,\"start\":33648},{\"end\":34273,\"start\":34245},{\"end\":34721,\"start\":34680},{\"end\":35149,\"start\":35131},{\"end\":39220,\"start\":39186},{\"end\":40637,\"start\":40599},{\"end\":41282,\"start\":41254},{\"end\":44108,\"start\":44100},{\"end\":44274,\"start\":44266},{\"end\":44560,\"start\":44555},{\"end\":44571,\"start\":44563},{\"end\":44648,\"start\":44628}]", "table": null, "figure_caption": "[{\"end\":44264,\"start\":44110},{\"end\":44553,\"start\":44276},{\"end\":44626,\"start\":44574},{\"end\":44702,\"start\":44652}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4977,\"start\":4971},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":34953,\"start\":34947},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":35830,\"start\":35824},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":36562,\"start\":36554},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":37228,\"start\":37217},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38413,\"start\":38407},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":39064,\"start\":39058}]", "bib_author_first_name": "[{\"end\":45229,\"start\":45228},{\"end\":45571,\"start\":45570},{\"end\":45584,\"start\":45583},{\"end\":45594,\"start\":45593},{\"end\":45605,\"start\":45604},{\"end\":45974,\"start\":45973},{\"end\":45976,\"start\":45975},{\"end\":46270,\"start\":46269},{\"end\":46279,\"start\":46278},{\"end\":46286,\"start\":46285},{\"end\":46295,\"start\":46294},{\"end\":46305,\"start\":46304},{\"end\":46307,\"start\":46306},{\"end\":46317,\"start\":46316},{\"end\":46319,\"start\":46318},{\"end\":46605,\"start\":46604},{\"end\":46607,\"start\":46606},{\"end\":46614,\"start\":46613},{\"end\":46616,\"start\":46615},{\"end\":46954,\"start\":46953},{\"end\":46962,\"start\":46961},{\"end\":46968,\"start\":46967},{\"end\":47350,\"start\":47349},{\"end\":47358,\"start\":47357},{\"end\":47360,\"start\":47359},{\"end\":47373,\"start\":47372},{\"end\":47386,\"start\":47385},{\"end\":47397,\"start\":47396},{\"end\":47399,\"start\":47398},{\"end\":47412,\"start\":47411},{\"end\":47414,\"start\":47413},{\"end\":47718,\"start\":47717},{\"end\":47730,\"start\":47729},{\"end\":48063,\"start\":48062},{\"end\":48367,\"start\":48366},{\"end\":48369,\"start\":48368},{\"end\":48711,\"start\":48710},{\"end\":48720,\"start\":48719},{\"end\":48729,\"start\":48728},{\"end\":48739,\"start\":48738},{\"end\":48751,\"start\":48750},{\"end\":48763,\"start\":48762},{\"end\":49247,\"start\":49246},{\"end\":49256,\"start\":49255},{\"end\":49265,\"start\":49264},{\"end\":49274,\"start\":49273},{\"end\":49276,\"start\":49275},{\"end\":49625,\"start\":49624},{\"end\":49934,\"start\":49933},{\"end\":49941,\"start\":49940},{\"end\":50289,\"start\":50288},{\"end\":50302,\"start\":50301},{\"end\":50314,\"start\":50313},{\"end\":50321,\"start\":50320},{\"end\":50663,\"start\":50662},{\"end\":50976,\"start\":50975},{\"end\":50984,\"start\":50983},{\"end\":50994,\"start\":50993},{\"end\":51370,\"start\":51369},{\"end\":51379,\"start\":51378},{\"end\":51385,\"start\":51384},{\"end\":51387,\"start\":51386},{\"end\":51742,\"start\":51741},{\"end\":51750,\"start\":51749},{\"end\":51759,\"start\":51758},{\"end\":51767,\"start\":51766},{\"end\":51773,\"start\":51772},{\"end\":51780,\"start\":51779},{\"end\":52137,\"start\":52136},{\"end\":52145,\"start\":52144},{\"end\":52152,\"start\":52151},{\"end\":52472,\"start\":52471},{\"end\":52492,\"start\":52491},{\"end\":52496,\"start\":52493},{\"end\":52507,\"start\":52503},{\"end\":52516,\"start\":52515},{\"end\":52520,\"start\":52517},{\"end\":52529,\"start\":52528},{\"end\":52531,\"start\":52530},{\"end\":52539,\"start\":52538},{\"end\":52541,\"start\":52540},{\"end\":52948,\"start\":52947},{\"end\":52958,\"start\":52957},{\"end\":52965,\"start\":52964},{\"end\":52974,\"start\":52973},{\"end\":53297,\"start\":53296},{\"end\":53606,\"start\":53605},{\"end\":53616,\"start\":53615},{\"end\":53628,\"start\":53627},{\"end\":53636,\"start\":53635},{\"end\":54028,\"start\":54027},{\"end\":54036,\"start\":54035},{\"end\":54314,\"start\":54313},{\"end\":54540,\"start\":54536},{\"end\":54547,\"start\":54546},{\"end\":54551,\"start\":54548},{\"end\":54899,\"start\":54898},{\"end\":55179,\"start\":55178},{\"end\":55496,\"start\":55495},{\"end\":55509,\"start\":55508},{\"end\":55880,\"start\":55879},{\"end\":56246,\"start\":56245},{\"end\":56248,\"start\":56247},{\"end\":56259,\"start\":56258},{\"end\":56268,\"start\":56267},{\"end\":56279,\"start\":56278},{\"end\":56292,\"start\":56291},{\"end\":56611,\"start\":56610},{\"end\":56901,\"start\":56900},{\"end\":57217,\"start\":57216},{\"end\":57229,\"start\":57228},{\"end\":57242,\"start\":57241},{\"end\":57251,\"start\":57250},{\"end\":57500,\"start\":57499},{\"end\":57502,\"start\":57501},{\"end\":57764,\"start\":57763},{\"end\":57779,\"start\":57778},{\"end\":57790,\"start\":57789},{\"end\":58174,\"start\":58173},{\"end\":58186,\"start\":58185},{\"end\":58360,\"start\":58359},{\"end\":58366,\"start\":58365},{\"end\":58378,\"start\":58377},{\"end\":58388,\"start\":58387},{\"end\":58646,\"start\":58645},{\"end\":58654,\"start\":58653},{\"end\":58667,\"start\":58666},{\"end\":58984,\"start\":58983},{\"end\":58993,\"start\":58992},{\"end\":59000,\"start\":58999},{\"end\":59018,\"start\":59017},{\"end\":59020,\"start\":59019},{\"end\":59410,\"start\":59409},{\"end\":59421,\"start\":59420},{\"end\":59430,\"start\":59429},{\"end\":59443,\"start\":59442},{\"end\":59445,\"start\":59444},{\"end\":59734,\"start\":59733},{\"end\":59740,\"start\":59739},{\"end\":59748,\"start\":59747},{\"end\":59761,\"start\":59760},{\"end\":60065,\"start\":60064},{\"end\":60072,\"start\":60071},{\"end\":60078,\"start\":60077},{\"end\":60085,\"start\":60084},{\"end\":60092,\"start\":60091},{\"end\":60447,\"start\":60446},{\"end\":60460,\"start\":60459},{\"end\":60745,\"start\":60744},{\"end\":60747,\"start\":60746},{\"end\":60975,\"start\":60974},{\"end\":61233,\"start\":61232},{\"end\":61235,\"start\":61234},{\"end\":61245,\"start\":61244},{\"end\":61247,\"start\":61246},{\"end\":61258,\"start\":61257},{\"end\":61260,\"start\":61259},{\"end\":61271,\"start\":61270},{\"end\":61273,\"start\":61272},{\"end\":61671,\"start\":61670},{\"end\":62006,\"start\":62005},{\"end\":62311,\"start\":62310}]", "bib_author_last_name": "[{\"end\":45235,\"start\":45230},{\"end\":45581,\"start\":45572},{\"end\":45591,\"start\":45585},{\"end\":45602,\"start\":45595},{\"end\":45613,\"start\":45606},{\"end\":45981,\"start\":45977},{\"end\":46276,\"start\":46271},{\"end\":46283,\"start\":46280},{\"end\":46292,\"start\":46287},{\"end\":46302,\"start\":46296},{\"end\":46314,\"start\":46308},{\"end\":46324,\"start\":46320},{\"end\":46611,\"start\":46608},{\"end\":46629,\"start\":46617},{\"end\":46959,\"start\":46955},{\"end\":46965,\"start\":46963},{\"end\":46973,\"start\":46969},{\"end\":47355,\"start\":47351},{\"end\":47370,\"start\":47361},{\"end\":47383,\"start\":47374},{\"end\":47394,\"start\":47387},{\"end\":47409,\"start\":47400},{\"end\":47420,\"start\":47415},{\"end\":47727,\"start\":47719},{\"end\":47736,\"start\":47731},{\"end\":48074,\"start\":48064},{\"end\":48379,\"start\":48370},{\"end\":48717,\"start\":48712},{\"end\":48726,\"start\":48721},{\"end\":48736,\"start\":48730},{\"end\":48748,\"start\":48740},{\"end\":48760,\"start\":48752},{\"end\":48769,\"start\":48764},{\"end\":49253,\"start\":49248},{\"end\":49262,\"start\":49257},{\"end\":49271,\"start\":49266},{\"end\":49281,\"start\":49277},{\"end\":49631,\"start\":49626},{\"end\":49938,\"start\":49935},{\"end\":49952,\"start\":49942},{\"end\":50299,\"start\":50290},{\"end\":50311,\"start\":50303},{\"end\":50318,\"start\":50315},{\"end\":50328,\"start\":50322},{\"end\":50671,\"start\":50664},{\"end\":50981,\"start\":50977},{\"end\":50991,\"start\":50985},{\"end\":51012,\"start\":50995},{\"end\":51376,\"start\":51371},{\"end\":51382,\"start\":51380},{\"end\":51395,\"start\":51388},{\"end\":51747,\"start\":51743},{\"end\":51756,\"start\":51751},{\"end\":51764,\"start\":51760},{\"end\":51770,\"start\":51768},{\"end\":51777,\"start\":51774},{\"end\":51785,\"start\":51781},{\"end\":52142,\"start\":52138},{\"end\":52149,\"start\":52146},{\"end\":52157,\"start\":52153},{\"end\":52489,\"start\":52473},{\"end\":52501,\"start\":52497},{\"end\":52513,\"start\":52508},{\"end\":52526,\"start\":52521},{\"end\":52536,\"start\":52532},{\"end\":52549,\"start\":52542},{\"end\":52955,\"start\":52949},{\"end\":52962,\"start\":52959},{\"end\":52971,\"start\":52966},{\"end\":52981,\"start\":52975},{\"end\":53305,\"start\":53298},{\"end\":53613,\"start\":53607},{\"end\":53625,\"start\":53617},{\"end\":53633,\"start\":53629},{\"end\":53646,\"start\":53637},{\"end\":54033,\"start\":54029},{\"end\":54041,\"start\":54037},{\"end\":54326,\"start\":54315},{\"end\":54544,\"start\":54541},{\"end\":54557,\"start\":54552},{\"end\":54907,\"start\":54900},{\"end\":55185,\"start\":55180},{\"end\":55506,\"start\":55497},{\"end\":55520,\"start\":55510},{\"end\":55887,\"start\":55881},{\"end\":56256,\"start\":56249},{\"end\":56265,\"start\":56260},{\"end\":56276,\"start\":56269},{\"end\":56289,\"start\":56280},{\"end\":56302,\"start\":56293},{\"end\":56620,\"start\":56612},{\"end\":56909,\"start\":56902},{\"end\":57226,\"start\":57218},{\"end\":57239,\"start\":57230},{\"end\":57248,\"start\":57243},{\"end\":57259,\"start\":57252},{\"end\":57508,\"start\":57503},{\"end\":57776,\"start\":57765},{\"end\":57787,\"start\":57780},{\"end\":57795,\"start\":57791},{\"end\":58183,\"start\":58175},{\"end\":58196,\"start\":58187},{\"end\":58363,\"start\":58361},{\"end\":58375,\"start\":58367},{\"end\":58385,\"start\":58379},{\"end\":58397,\"start\":58389},{\"end\":58651,\"start\":58647},{\"end\":58664,\"start\":58655},{\"end\":58675,\"start\":58668},{\"end\":58990,\"start\":58985},{\"end\":58997,\"start\":58994},{\"end\":59015,\"start\":59001},{\"end\":59031,\"start\":59021},{\"end\":59418,\"start\":59411},{\"end\":59427,\"start\":59422},{\"end\":59440,\"start\":59431},{\"end\":59737,\"start\":59735},{\"end\":59745,\"start\":59741},{\"end\":59758,\"start\":59749},{\"end\":59769,\"start\":59762},{\"end\":60069,\"start\":60066},{\"end\":60075,\"start\":60073},{\"end\":60082,\"start\":60079},{\"end\":60089,\"start\":60086},{\"end\":60096,\"start\":60093},{\"end\":60457,\"start\":60448},{\"end\":60467,\"start\":60461},{\"end\":60752,\"start\":60748},{\"end\":60983,\"start\":60976},{\"end\":61242,\"start\":61236},{\"end\":61255,\"start\":61248},{\"end\":61268,\"start\":61261},{\"end\":61283,\"start\":61274},{\"end\":62013,\"start\":62007},{\"end\":62321,\"start\":62312}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":7748330},\"end\":45460,\"start\":45115},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2095857},\"end\":45871,\"start\":45462},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7843488},\"end\":46180,\"start\":45873},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3419689},\"end\":46542,\"start\":46182},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":5942400},\"end\":46826,\"start\":46544},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":18653283},\"end\":47265,\"start\":46828},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2092326},\"end\":47650,\"start\":47267},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":24623678},\"end\":48000,\"start\":47652},{\"attributes\":{\"doi\":\"10.1038/nmeth.2019\",\"id\":\"b8\",\"matched_paper_id\":205420589},\"end\":48277,\"start\":48002},{\"attributes\":{\"doi\":\"10.1186/gb-2006-7-10-r100\",\"id\":\"b9\",\"matched_paper_id\":215779792},\"end\":48616,\"start\":48279},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":5162860},\"end\":49029,\"start\":48618},{\"attributes\":{\"id\":\"b11\"},\"end\":49177,\"start\":49031},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":16845163},\"end\":49549,\"start\":49179},{\"attributes\":{\"doi\":\"101400H\",\"id\":\"b13\",\"matched_paper_id\":21732359},\"end\":49807,\"start\":49551},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":7480362},\"end\":50199,\"start\":49809},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":10186071},\"end\":50562,\"start\":50201},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":9747597},\"end\":50849,\"start\":50564},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":10035420},\"end\":51279,\"start\":50851},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":39246121},\"end\":51618,\"start\":51281},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":21205798},\"end\":52063,\"start\":51620},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":21593159},\"end\":52351,\"start\":52065},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":54556210},\"end\":52857,\"start\":52353},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":59601271},\"end\":53206,\"start\":52859},{\"attributes\":{\"doi\":\"arXiv:1810.00236\",\"id\":\"b23\"},\"end\":53467,\"start\":53208},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2991432},\"end\":53910,\"start\":53469},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6538247},\"end\":54260,\"start\":53912},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":2930547},\"end\":54491,\"start\":54262},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14113767},\"end\":54801,\"start\":54493},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":44172410},\"end\":55086,\"start\":54803},{\"attributes\":{\"doi\":\"arXiv:1901.07031\",\"id\":\"b29\"},\"end\":55391,\"start\":55088},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":27391290},\"end\":55725,\"start\":55393},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":15898258},\"end\":56170,\"start\":55727},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":10156299},\"end\":56514,\"start\":56172},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":6994762},\"end\":56829,\"start\":56516},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":15008471},\"end\":57183,\"start\":56831},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":14088925},\"end\":57445,\"start\":57185},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":62771950},\"end\":57696,\"start\":57447},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3719281},\"end\":58103,\"start\":57698},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b38\"},\"end\":58345,\"start\":58105},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":54465873},\"end\":58587,\"start\":58347},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":1629541},\"end\":58939,\"start\":58589},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":9433631},\"end\":59324,\"start\":58941},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":1023605},\"end\":59707,\"start\":59326},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":30834643},\"end\":60010,\"start\":59709},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":3698141},\"end\":60360,\"start\":60012},{\"attributes\":{\"doi\":\"arXiv:1801.05746\",\"id\":\"b45\"},\"end\":60678,\"start\":60362},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":53335638},\"end\":60887,\"start\":60680},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":135345056},\"end\":61189,\"start\":60889},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":551981},\"end\":61559,\"start\":61191},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":10246983},\"end\":61866,\"start\":61561},{\"attributes\":{\"doi\":\"arXiv:1902.06543\",\"id\":\"b50\"},\"end\":62267,\"start\":61868},{\"attributes\":{\"id\":\"b51\"},\"end\":62506,\"start\":62269}]", "bib_title": "[{\"end\":45226,\"start\":45115},{\"end\":45568,\"start\":45462},{\"end\":45971,\"start\":45873},{\"end\":46267,\"start\":46182},{\"end\":46602,\"start\":46544},{\"end\":46951,\"start\":46828},{\"end\":47347,\"start\":47267},{\"end\":47715,\"start\":47652},{\"end\":48060,\"start\":48002},{\"end\":48364,\"start\":48279},{\"end\":48708,\"start\":48618},{\"end\":49244,\"start\":49179},{\"end\":49622,\"start\":49551},{\"end\":49931,\"start\":49809},{\"end\":50286,\"start\":50201},{\"end\":50660,\"start\":50564},{\"end\":50973,\"start\":50851},{\"end\":51367,\"start\":51281},{\"end\":51739,\"start\":51620},{\"end\":52134,\"start\":52065},{\"end\":52469,\"start\":52353},{\"end\":52945,\"start\":52859},{\"end\":53603,\"start\":53469},{\"end\":54025,\"start\":53912},{\"end\":54311,\"start\":54262},{\"end\":54534,\"start\":54493},{\"end\":54896,\"start\":54803},{\"end\":55493,\"start\":55393},{\"end\":55877,\"start\":55727},{\"end\":56243,\"start\":56172},{\"end\":56608,\"start\":56516},{\"end\":56898,\"start\":56831},{\"end\":57214,\"start\":57185},{\"end\":57497,\"start\":57447},{\"end\":57761,\"start\":57698},{\"end\":58357,\"start\":58347},{\"end\":58643,\"start\":58589},{\"end\":58981,\"start\":58941},{\"end\":59407,\"start\":59326},{\"end\":59731,\"start\":59709},{\"end\":60062,\"start\":60012},{\"end\":60742,\"start\":60680},{\"end\":60972,\"start\":60889},{\"end\":61230,\"start\":61191},{\"end\":61668,\"start\":61561}]", "bib_author": "[{\"end\":45237,\"start\":45228},{\"end\":45583,\"start\":45570},{\"end\":45593,\"start\":45583},{\"end\":45604,\"start\":45593},{\"end\":45615,\"start\":45604},{\"end\":45983,\"start\":45973},{\"end\":46278,\"start\":46269},{\"end\":46285,\"start\":46278},{\"end\":46294,\"start\":46285},{\"end\":46304,\"start\":46294},{\"end\":46316,\"start\":46304},{\"end\":46326,\"start\":46316},{\"end\":46613,\"start\":46604},{\"end\":46631,\"start\":46613},{\"end\":46961,\"start\":46953},{\"end\":46967,\"start\":46961},{\"end\":46975,\"start\":46967},{\"end\":47357,\"start\":47349},{\"end\":47372,\"start\":47357},{\"end\":47385,\"start\":47372},{\"end\":47396,\"start\":47385},{\"end\":47411,\"start\":47396},{\"end\":47422,\"start\":47411},{\"end\":47729,\"start\":47717},{\"end\":47738,\"start\":47729},{\"end\":48076,\"start\":48062},{\"end\":48381,\"start\":48366},{\"end\":48719,\"start\":48710},{\"end\":48728,\"start\":48719},{\"end\":48738,\"start\":48728},{\"end\":48750,\"start\":48738},{\"end\":48762,\"start\":48750},{\"end\":48771,\"start\":48762},{\"end\":49255,\"start\":49246},{\"end\":49264,\"start\":49255},{\"end\":49273,\"start\":49264},{\"end\":49283,\"start\":49273},{\"end\":49633,\"start\":49624},{\"end\":49940,\"start\":49933},{\"end\":49954,\"start\":49940},{\"end\":50301,\"start\":50288},{\"end\":50313,\"start\":50301},{\"end\":50320,\"start\":50313},{\"end\":50330,\"start\":50320},{\"end\":50673,\"start\":50662},{\"end\":50983,\"start\":50975},{\"end\":50993,\"start\":50983},{\"end\":51014,\"start\":50993},{\"end\":51378,\"start\":51369},{\"end\":51384,\"start\":51378},{\"end\":51397,\"start\":51384},{\"end\":51749,\"start\":51741},{\"end\":51758,\"start\":51749},{\"end\":51766,\"start\":51758},{\"end\":51772,\"start\":51766},{\"end\":51779,\"start\":51772},{\"end\":51787,\"start\":51779},{\"end\":52144,\"start\":52136},{\"end\":52151,\"start\":52144},{\"end\":52159,\"start\":52151},{\"end\":52491,\"start\":52471},{\"end\":52503,\"start\":52491},{\"end\":52515,\"start\":52503},{\"end\":52528,\"start\":52515},{\"end\":52538,\"start\":52528},{\"end\":52551,\"start\":52538},{\"end\":52957,\"start\":52947},{\"end\":52964,\"start\":52957},{\"end\":52973,\"start\":52964},{\"end\":52983,\"start\":52973},{\"end\":53307,\"start\":53296},{\"end\":53615,\"start\":53605},{\"end\":53627,\"start\":53615},{\"end\":53635,\"start\":53627},{\"end\":53648,\"start\":53635},{\"end\":54035,\"start\":54027},{\"end\":54043,\"start\":54035},{\"end\":54328,\"start\":54313},{\"end\":54546,\"start\":54536},{\"end\":54559,\"start\":54546},{\"end\":54909,\"start\":54898},{\"end\":55187,\"start\":55178},{\"end\":55508,\"start\":55495},{\"end\":55522,\"start\":55508},{\"end\":55889,\"start\":55879},{\"end\":56258,\"start\":56245},{\"end\":56267,\"start\":56258},{\"end\":56278,\"start\":56267},{\"end\":56291,\"start\":56278},{\"end\":56304,\"start\":56291},{\"end\":56622,\"start\":56610},{\"end\":56911,\"start\":56900},{\"end\":57228,\"start\":57216},{\"end\":57241,\"start\":57228},{\"end\":57250,\"start\":57241},{\"end\":57261,\"start\":57250},{\"end\":57510,\"start\":57499},{\"end\":57778,\"start\":57763},{\"end\":57789,\"start\":57778},{\"end\":57797,\"start\":57789},{\"end\":58185,\"start\":58173},{\"end\":58198,\"start\":58185},{\"end\":58365,\"start\":58359},{\"end\":58377,\"start\":58365},{\"end\":58387,\"start\":58377},{\"end\":58399,\"start\":58387},{\"end\":58653,\"start\":58645},{\"end\":58666,\"start\":58653},{\"end\":58677,\"start\":58666},{\"end\":58992,\"start\":58983},{\"end\":58999,\"start\":58992},{\"end\":59017,\"start\":58999},{\"end\":59033,\"start\":59017},{\"end\":59420,\"start\":59409},{\"end\":59429,\"start\":59420},{\"end\":59442,\"start\":59429},{\"end\":59448,\"start\":59442},{\"end\":59739,\"start\":59733},{\"end\":59747,\"start\":59739},{\"end\":59760,\"start\":59747},{\"end\":59771,\"start\":59760},{\"end\":60071,\"start\":60064},{\"end\":60077,\"start\":60071},{\"end\":60084,\"start\":60077},{\"end\":60091,\"start\":60084},{\"end\":60098,\"start\":60091},{\"end\":60459,\"start\":60446},{\"end\":60469,\"start\":60459},{\"end\":60754,\"start\":60744},{\"end\":60985,\"start\":60974},{\"end\":61244,\"start\":61232},{\"end\":61257,\"start\":61244},{\"end\":61270,\"start\":61257},{\"end\":61285,\"start\":61270},{\"end\":61674,\"start\":61670},{\"end\":62015,\"start\":62005},{\"end\":62323,\"start\":62310}]", "bib_venue": "[{\"end\":47832,\"start\":47789},{\"end\":49361,\"start\":49328},{\"end\":49656,\"start\":49652},{\"end\":54623,\"start\":54589},{\"end\":55943,\"start\":55920},{\"end\":57011,\"start\":56965},{\"end\":57876,\"start\":57833},{\"end\":58461,\"start\":58434},{\"end\":58765,\"start\":58725},{\"end\":59137,\"start\":59089},{\"end\":59514,\"start\":59485},{\"end\":59859,\"start\":59819},{\"end\":60186,\"start\":60146},{\"end\":61349,\"start\":61321},{\"end\":45258,\"start\":45237},{\"end\":45636,\"start\":45615},{\"end\":45999,\"start\":45983},{\"end\":46333,\"start\":46326},{\"end\":46656,\"start\":46631},{\"end\":47016,\"start\":46975},{\"end\":47430,\"start\":47422},{\"end\":47787,\"start\":47738},{\"end\":48108,\"start\":48094},{\"end\":48417,\"start\":48406},{\"end\":48792,\"start\":48771},{\"end\":49061,\"start\":49031},{\"end\":49326,\"start\":49283},{\"end\":49650,\"start\":49640},{\"end\":49975,\"start\":49954},{\"end\":50353,\"start\":50330},{\"end\":50681,\"start\":50673},{\"end\":51035,\"start\":51014},{\"end\":51420,\"start\":51397},{\"end\":51810,\"start\":51787},{\"end\":52180,\"start\":52159},{\"end\":52572,\"start\":52551},{\"end\":53004,\"start\":52983},{\"end\":53294,\"start\":53208},{\"end\":53669,\"start\":53648},{\"end\":54064,\"start\":54043},{\"end\":54347,\"start\":54328},{\"end\":54587,\"start\":54559},{\"end\":54920,\"start\":54909},{\"end\":55176,\"start\":55088},{\"end\":55539,\"start\":55522},{\"end\":55918,\"start\":55889},{\"end\":56317,\"start\":56304},{\"end\":56643,\"start\":56622},{\"end\":56963,\"start\":56911},{\"end\":57285,\"start\":57261},{\"end\":57545,\"start\":57510},{\"end\":57831,\"start\":57797},{\"end\":58171,\"start\":58105},{\"end\":58432,\"start\":58399},{\"end\":58723,\"start\":58677},{\"end\":59087,\"start\":59033},{\"end\":59483,\"start\":59448},{\"end\":59817,\"start\":59771},{\"end\":60144,\"start\":60098},{\"end\":60444,\"start\":60362},{\"end\":60761,\"start\":60754},{\"end\":61012,\"start\":60985},{\"end\":61319,\"start\":61285},{\"end\":61691,\"start\":61674},{\"end\":62003,\"start\":61868},{\"end\":62308,\"start\":62269}]"}}}, "year": 2023, "month": 12, "day": 17}