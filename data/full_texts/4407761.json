{"id": 4407761, "updated": "2023-09-30 23:28:39.805", "metadata": {"title": "Attentive Statistics Pooling for Deep Speaker Embedding", "authors": "[{\"first\":\"Koji\",\"last\":\"Okabe\",\"middle\":[]},{\"first\":\"Takafumi\",\"last\":\"Koshinaka\",\"middle\":[]},{\"first\":\"Koichi\",\"last\":\"Shinoda\",\"middle\":[]}]", "venue": "Interspeech 2018", "journal": "Interspeech 2018", "publication_date": {"year": 2018, "month": 3, "day": 29}, "abstract": "This paper proposes attentive statistics pooling for deep speaker embedding in text-independent speaker verification. In conventional speaker embedding, frame-level features are averaged over all the frames of a single utterance to form an utterance-level feature. Our method utilizes an attention mechanism to give different weights to different frames and generates not only weighted means but also weighted standard deviations. In this way, it can capture long-term variations in speaker characteristics more effectively. An evaluation on the NIST SRE 2012 and the VoxCeleb data sets shows that it reduces equal error rates (EERs) from the conventional method by 7.5% and 8.1%, respectively.", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": "1803.10963", "mag": "2794506738", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/interspeech/OkabeKS18", "doi": "10.21437/interspeech.2018-993"}}, "content": {"source": {"pdf_hash": "4a64008cd21488071ac418aad32ce7129408edb4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1803.10963v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://t2r2.star.titech.ac.jp/rrws/file/CTT100771284/ATD100000413/", "status": "GREEN"}}, "grobid": {"id": "0407490760fb31d9716697c7467a3523154ded61", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4a64008cd21488071ac418aad32ce7129408edb4.txt", "contents": "\nAttentive Statistics Pooling for Deep Speaker Embedding\n25 Feb 2019\n\nKoji Okabe k-okabe@bx.jp.nec.com \nData Science Research Laboratories\nNEC Corporation\nJapan\n\nTakafumi Koshinaka koshinak@ap.jp.nec.com \nData Science Research Laboratories\nNEC Corporation\nJapan\n\nKoichi Shinoda shinoda@c.titech.ac.jp \nDepartment of Computer Science\nTokyo Institute of Technology\nJapan\n\nAttentive Statistics Pooling for Deep Speaker Embedding\n25 Feb 2019Index Terms: speaker recognitiondeep neural networksat- tentionstatistics pooling\nThis paper proposes attentive statistics pooling for deep speaker embedding in text-independent speaker verification. In conventional speaker embedding, frame-level features are averaged over all the frames of a single utterance to form an utterance-level feature. Our method utilizes an attention mechanism to give different weights to different frames and generates not only weighted means but also weighted standard deviations. In this way, it can capture long-term variations in speaker characteristics more effectively. An evaluation on the NIST SRE 2012 and the VoxCeleb data sets shows that it reduces equal error rates (EERs) from the conventional method by 7.5% and 8.1%, respectively.\n\nIntroduction\n\nSpeaker recognition has advanced considerably in the last decade with the i-vector paradigm [1], in which a speech utterance or a speaker is represented in the form of a fixed-lowdimensional feature vector.\n\nWith the great success of deep learning over a wide range of machine learning tasks, including automatic speech recognition (ASR), an increasing number of research studies have introduced deep learning into feature extraction for speaker recognition. In early studies [2,3], deep neural networks (DNNs) derived from acoustic models for ASR have been employed as a universal background model (UBM) to provide phoneme posteriors as well as bottleneck features, which are used for, respectively, zeroth-and first-order statistics in i-vector extraction. While they have shown better performance than conventional UBMs based on Gaussian mixture models (GMMs), they have the drawback of language dependency [4] and also require expensive phonetic transcriptions for training [5].\n\nRecently, DNNs have been shown to be useful for extracting speaker-discriminative feature vectors independently from the i-vector framework. With the help of large-scale training data, such approaches lead to better results, particularly under conditions of short-duration utterances. In fixed-phrase textdependent speaker verification, an end-to-end neural networkbased method has been proposed [6] in which Long Short-Term Memory (LSTM) with a single output from the last frame is used to obtain utterance-level speaker features, and it has outperformed conventional i-vector extraction.\n\nIn text-independent speaker verification, where input utterances can have variable phrases and lengths, an average pooling layer has been introduced to aggregate frame-level speaker feature vectors to obtain an utterance-level feature vector, i.e., speaker embedding, with a fixed number of dimensions. Most recent studies have shown that DNNs achieve better accuracy than do i-vectors [7,8]. Snyder et al. [9] employed an extension of average pooling, in which what they called statistics pooling calculated not only the mean, but also the standard deviation of frame-level features. They, however, have not yet reported the effictiveness of standard deviaion pooling to accuracy improvement.\n\nOther recent studies conducted from a different perspective [10,11] have incorporated attention mechanisms [12]. It had previously produced significant improvement in machine translation. In the scenario of speaker recognition, an importance metric is computed by the small attention network that works as a part of the speaker embedding network. The importance is utilized for calculating the weighted mean of frame-level feature vectors. This mechanism enables speaker embedding to be focused on important frames and to obtain long-term speaker representation with higher discriminative power. Such previous work, however, has been evaluated only in such limited tasks as fixed-duration text-independent [10] or text-dependent speaker recognition [11].\n\nIn this paper, we propose a new pooling method, called attentive statistics pooling, that provides importance-weighted standard deviations as well as the weighted means of framelevel features, for which the importance is calculated by an attention mechanism. This enables speaker embedding to more accurately and efficiently capture speaker factors with respect to long-term variations. To the best of our knowledge, this is the first attempt reported in the literature to use attentive statistics pooling in text-independent and variable-duration scenarios. We have also experimentally shown, through comparisons of various pooling layers, the effectiveness of long-term speaker characteristics derived from standard deviations.\n\nThe remainder of this paper is organized as follows: Section 2 describes a conventional method for extracting deep speaker embedding. Section 3 reviews two extensions for the conventional method, and then introduces the proposed speaker embedding method. The experimental setup and results are presented in Section 4. Section 5 summarizes our work and notes future plans.\n\n\nDeep speaker embedding\n\nThe conventional DNN for extracting utterance-level speaker features consists of three blocks, as shown in Figure 1.\n\nThe first block is a frame-level feature extractor. The input to this block is a sequence of acoustic features, e.g., MFCCs and filter-bank coefficients. After considering relatively shortterm acoustic features, this block outputs frame-level features. Any type of neural network is applicable for the extractor, e.g., a Time-Delay Neural Network (TDNN) [9], Convolutional Neural Network (CNN) [7,8], LSTM [10,11], or Gated Recurrent Unit (GRU) [8].\n\nThe second block is a pooling layer that converts variablelength frame-level features into a fixed-dimensional vector. The\n\n\nPooling layer\nAcoustic feature \u123c \u0b35 , \u0b36 , \u2026 , \u0bcd \u123d Frame-level feature \u123c \u0b35 , \u0b36 , \u2026 , \u0bcd \u123d Speaker IDs Utterance-level feature \u22ef \u22ef Frame-level feature extractor\nUtterance-level feature extractor \u22ef \u22ef Figure 1: DNNs for extracting utterance-level speaker features most standard type of pooling layer obtains the average of all frame-level features (average pooling). The third block is an utterance-level feature extractor in which a number of fully-connected hidden layers are stacked. One of these hidden layers is often designed to have a smaller number of units (i.e., to be a bottleneck layer), which forces the information brought from the preceding layer into a lowdimensional representation. The output is a softmax layer, and each of its output nodes corresponds to one speaker ID. For training, we employ back-propagation with cross entropy loss. We can then use bottleneck features in the third block as utterance-level features. Some studies refrain from using softmax layers and achieve end-to-end neural networks by using contrastive loss [7] or triplet loss [8]. Probabilistic linear discriminant analysis (PLDA) [13,14] can also be used for measuring the distance between two utterance-level features [9,10].\n\n\nHigher-order pooling with attention\n\nThe conventional speaker embedding described in the previous section suggests the addition of two extensions of the pooling method: the use of higher-order statistics and the use of attention mechanisms. In this section we review both and then introduce our proposed pooling method, which we refer to as attentive statistics pooling.\n\n\nStatistics pooling\n\nThe statistics pooling layer [9] calculates the mean vector \u00b5 as well as the second-order statistics as the standard deviation vector \u03c3 over frame-level features ht (t = 1, \u00b7 \u00b7 \u00b7 , T ).\n\u00b5 = 1 T T t ht,(1)\u03c3 = 1 T T t ht \u2299 ht \u2212 \u00b5 \u2299 \u00b5,(2)\nwhere \u2299 represents the Hadamard product. The mean vector (1) which aggregates frame-level features can be viewed as the main body of utterance-level features. We consider that the standard deviation (2) also plays an important role since it contains other speaker characteristics in terms of temporal variability over long contexts. LSTM is capable of taking relatively long contexts into account, using its recurrent connections and gating functions. However, the scope of LSTM is actually no more than a second (\u223c100 frames) due to the vanishing gradient problem [15]. A standard deviation, which is potentially capable of revealing any distance in a context, can help speaker embedding capture long-term variability over an utterance.\n\n\nAttention mechanism\n\nIt is often the case that frame-level features of some frames are more unique and important for discriminating speakers than others in a given utterance. Recent studies [10,11] have applied attention mechanisms to speaker recognition for the purpose of frame selection by automatically calculating the importance of each frame. An attention model works in conjunction with the original DNN and calculates a scalar score et for each frame-level fea-\nture et = v T f (W ht + b) + k,(3)\nwhere f (\u00b7) is a non-linear activation function, such as a tanh or ReLU function. The score is normalized over all frames by a softmax function so as to add up to the following unity:\n\u03b1t = exp(et) T \u03c4 exp(e\u03c4 ) .(4)\nThe normalized score \u03b1t is then used as the weight in the pooling layer to calculate the weighted mean vector\n\u00b5 = T t \u03b1tht.(5)\nIn this way, an utterance-level feature extracted from a weighted mean vector focuses on important frames and hence becomes more speaker discriminative.\n\n\nAttentive statistics pooling\n\nThe authors believe that both higher-order statistics (standard deviations as utterance-level features) and attention mechanisms are effective for higher speaker discriminability. Hence, it would make sense to consider a new pooling method, attentive statistics pooling, which produces both means and standard deviations with importance weighting by means of attention, as illustrated in Figure 2. Here the weighted mean is given by (5), and the weighted standard deviation is defined as follows:\n\u03c3 = T t \u03b1tht \u2299 ht \u2212\u03bc \u2299\u03bc,(6)\nwhere the weight \u03b1t calculated by (4) is shared by both the weighted mean\u03bc and weighted standard deviation\u03c3. The weighted standard deviation is thought to take the advantage of both statistics pooling and attention, i.e., feature representation in terms of long-term variations and frame selection in accord with importance, bringing higher speaker discriminability to utterance-level features. Needless to say, as (6) is differentiable, DNNs with attentive statistics pooling can be trained on the basis of back-propagation. \n\n\nExperimental settings\n\nWe report here speaker verification accuracy w.r.t. the NIST SRE 2012 [16] Common Condition 2 (SRE12 CC2) and Vox-Celeb corpora [7]. Deep speaker embedding with our attentive statistics pooling is compared to that with conventional statistics pooling and with attentive average pooling, as well as with traditional i-vector extraction based on GMM-UBM.\n\n\ni-vector system\n\nThe baseline i-vector system uses 20-dimensional MFCCs for every 10ms. Their delta and delta-delta features were appended to form 60-dimensional acoustic features. Sliding mean normalization with a 3-second window and energy-based voice activity detection (VAD) were then applied, in that order. An i-vector of 400 dimensions was then extracted from the acoustic feature vectors, using a 2048-mixture UBM and a total variability matrix (TVM). Mean subtraction, whitening, and length normalization [17] were applied to the i-vector as pre-processing steps before sending it to the PLDA, and similarity was then evaluated using a PLDA model with a speaker space of 400 dimensions.\n\n\nDeep speaker embedding system\n\nWe used 20-dimensional MFCCs for SRE12 evaluation, and 40-dimensional MFCCs for VoxCeleb evaluation, for every 10ms. Sliding mean normalization with a 3-second window and energy-based VAD were then applied in the same way as was done with the i-vector system. The network structure, except for its input dimensions, was exactly the same as the one shown in the recipe 1 published in Kaldi's official repository [18,19]. A 5-layer TDNN with ReLU followed by batch normalization was used for extracting framelevel features. The number of hidden nodes in each hidden layer was 512. The dimension of a frame-level feature for pooling was 1500. Each frame-level feature was generated from a 15frame context of acoustic feature vectors.\n\nPooling layer aggregates frame-level features, followed by 2 fully-connected layers with ReLU activation functions, batch normalization, and a softmax output layer. The 512dimensional bottleneck features from the first fully-connected 1 egs/sre16/v2 layer were used as speaker embedding.\n\nWe tried four pooling techniques to evaluate the effectiveness of the proposed method: (i) simple average pooling to produce means only, (ii) statistics pooling to produce means and standard deviations, (iii) attentive average pooling to produce weighted means, and (iv) our proposed attentive statistics pooling. We used ReLU followed by batch normalization for activation function f (\u00b7) in (3) of the attention model. The number of hidden nodes was 64.\n\nMean subtraction, whitening, and length normalization were applied to the speaker embedding, as pre-processing steps before sending it to the PLDA, and similarity was then evaluated using a PLDA model with a speaker space of 512 dimensions.\n\n\nTraining and evaluation data\n\nIn order to avoid condition mismatch, different training data were used for each evaluation task w.r.t. SRE12 CC2 and Vox-Celeb.\n\nFor SRE12 evaluation, telephone recordings from SRE04-10, Switchboard, and Fisher English were used as training data. We also applied data augmentation to the training set in the following ways: (a) Additive noise: each segment was mixed with one of the noise samples in the PRISM corpus [20] (SNR: 8, 15, or 20dB), (b) Reverberation: each segment was convolved with one of the room impulse responses in the REVERB challenge data [21], (c) Speech encoding: each segment was encoded with AMR codec (6.7 or 4.75 kbps). The evaluation set we used was SRE12 Common Condition 2 (CC2), which is known as a typical subset of telephone conversations without added noise.\n\nFor VoxCeleb evaluation, the development and test sets defined in [7] were respectively used as training data and evaluation data. The number of speakers in the training and evaluation sets were 1,206 and 40, respectively. The number of segments in training and evaluation sets were 140,286 and 4,772, respectively. Note that these numbers are slightly smaller than those reported in [7] due to a few dead links on the official download server. We also used the data augmentation (a) and (b) mentioned above.\n\nWe report here results in terms of equal error rate (EER) and the minimum of the normalized detection cost function, for which we assume a prior target probability Ptar of 0.01 (DCF10 -2 ) or 0.001 (DCF10 -3 ), and equal weights of 1.0 between misses Cmiss and false alarms C f a . Table 1 shows the performance on NIST SRE12 CC2. In the \"Embedding\" column, average [7,8] denotes average pooling that used only means, attention [10,11] used weighted means scaled by attention (attentive average pooling), statistics [9] used both means and standard deviations (statistics pooling), and attentive statistics is the proposed method (attentive statistics pooling), which used both weighted means and weighted standard deviations scaled by attention.\n\n\nResults\n\n\nNIST SRE 2012\n\nIn comparison to average pooling, which used only means, the addition of attention was superior in terms of all evaluation measures. Surprisingly, the addition of standard deviations was even more effective than that of attention. This indicates that long-term information is quite important in textindependent speaker verification. Further, the proposed attentive statistics pooling resulted in the best EER as well as minD-CFs. In terms of EER, it was 7.5% better than statistics pooling.  [10,11] 0.228 0.399 1.99 statistics [9] 0.183 0.331 1.58 attentive statistics 0.170 0.309 1.47 This reflects the effect of using both long-context and frameimportance. The traditional i-vector systems, however, performed better than speaker embedding-based systems, except for performance w.r.t. EER. This seems to have been because the SRE12 CC2 task consisted of long-utterance trials in which durations of test utterances were from 30 seconds to 300 seconds and durations of multi-enrollment utterances were longer than 300 seconds. Table 2 shows comparisons of EERs for several durations on NIST SRE12 CC2. We can see that deep speaker embedding offered robustness in short-duration trials. Although i-vector offered the best performance under the longest-duration condition (300s), our attentive statistics pooling achieved the best under all other conditions, with better error rates than those of statistics pooling under all conditions, including Pool (overall average). Only attentive statistics pooling showed better performance than i-vectors on both 30-second trials and 100-second trials. Table 3 shows performance on the VoxCeleb test set. Here, also, the addition of both attention and of standard deviations helped improve performance. As in the SRE12 CC2 case, standard deviation addition had a larger impact than that of attention. The proposed attentive statistics pooling achieved the best performance in all evaluation measures and was 8.1% better in terms of EER than statistics pooling. This may have been because the durations were shorter than those with SRE12 CC2 (about 8 seconds on average in the evaluation), and speaker embedding outperformed i-vectors, as well. It should be noted that compared to the baseline performance shown in [7], whose best EER was 7.8%, our experimental systems achieved much better performance, even though we used slightly smaller training and evaluation sets due to lack of certain videos. \n\n\nVoxCeleb\n\n\nSummary and Future Work\n\nWe have proposed attentive statistics pooling for extracting deep speaker embedding. The proposed pooling layer calculates weighted means and weighted standard deviations over frame-level features scaled by an attention model. This enables speaker embedding to focus only on important frames. Moreover, long-term variations can be obtained as speaker characteristics in the standard deviations. Such a combination of attention and standard deviations produces a synergetic effect to give deep speaker embedding higher discriminative power. Textindependent speaker verification experiments on NIST SRE 2012 and VoxCeleb evaluation sets showed that it reduced EERs from a conventional method by, respectively, 7.5% and 8.1% for the two sets. While we have achieved considerable improvement under both short-and long-duration conditions, i-vectors are still competitive for long durations (e.g., 300s in SRE12 CC2). Pursuing even better accuracy under such conditions is an issue for our future work.\n\nFigure 2 :\n2Attentive\n\nTable 1 :\n1Performance on NIST SRE 2012 Common Condition 2.Boldface denotes the best performance for each column.Embedding \nDCF10 -2 DCF10 -3 EER (%) \n\ni-vector \n0.169 \n0.291 \n1.50 \n\naverage [7, 8] \n0.290 \n0.484 \n2.57 \nattention \n\nTable 2 :\n2EERs(%) for each duration on NIST SRE 2012 Com-\nmon Condition 2. Boldface denotes the best performance for \neach column. \n\nEmbedding \n30s \n100s 300s Pool \n\ni-vector \n2.66 1.09 0.58 1.50 \n\naverage [7, 8] \n3.58 2.07 1.86 2.57 \nattention [10, 11] \n3.00 1.58 1.27 1.99 \nstatistics [9] \n2.49 1.25 0.82 1.58 \nattentive statistics 2.46 1.07 0.80 1.47 \n\n\n\nTable 3 :\n3Performance on VoxCeleb. Boldface denotes the best performance for each column.Embedding \nDCF10 -2 DCF10 -3 EER (%) \n\ni-vector \n0.479 \n0.595 \n5.39 \n\naverage [7, 8] \n0.464 \n0.550 \n4.70 \nattention [10, 11] \n0.443 \n0.598 \n4.52 \nstatistics [9] \n0.413 \n0.530 \n4.19 \nattentive statistics \n0.406 \n0.513 \n3.85 \n\n\n\nFront-end factor analysis for speaker verification. N Dehak, P J Kenny, R Dehak, P Dumouchel, P Ouellet, IEEE Transactions on Audio, Speech, and Language Processing. 194N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet, \"Front-end factor analysis for speaker verification,\" IEEE Trans- actions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788-798, 2011.\n\nA novel scheme for speaker recognition using a phonetically-aware deep neural network. Y Lei, N Scheffer, L Ferrer, M Mclaren, Proc. ICASSP. ICASSPY. Lei, N. Scheffer, L. Ferrer, and M. McLaren, \"A novel scheme for speaker recognition using a phonetically-aware deep neural network,\" in Proc. ICASSP, 2014, pp. 1695-1699.\n\nAdvances in deep neural network approaches to speaker recognition. M Mclaren, Y Lei, L Ferrer, Proc. ICASSP. ICASSPM. McLaren, Y. Lei, and L. Ferrer, \"Advances in deep neural net- work approaches to speaker recognition,\" in Proc. ICASSP, 2015, pp. 4814-4818.\n\nExploring robustness of DNN/RNN for extracting speaker Baum-Welch statistics in mismatched conditions. H Zheng, S Zhang, W Liu, Proc. Interspeech. InterspeechH. Zheng, S. Zhang, and W. Liu, \"Exploring robustness of DNN/RNN for extracting speaker Baum-Welch statistics in mis- matched conditions,\" in Proc. Interspeech, 2015, pp. 1161-1165.\n\nImproving deep neural networks based speaker verification using unlabeled data. Y Tian, M Cai, L He, W.-Q Zhang, J Liu, Proc. Interspeech. InterspeechY. Tian, M. Cai, L. He, W.-Q. Zhang, and J. Liu, \"Improving deep neural networks based speaker verification using unlabeled data.\" in Proc. Interspeech, 2016, pp. 1863-1867.\n\nEnd-to-end text-dependent speaker verification. G Heigold, I Moreno, S Bengio, N Shazeer, Proc. ICASSP. ICASSPG. Heigold, I. Moreno, S. Bengio, and N. Shazeer, \"End-to-end text-dependent speaker verification,\" in Proc. ICASSP, 2016, pp. 5115-5119.\n\nVoxCeleb: A largescale speaker identification dataset. A Nagrani, J S Chung, A Zisserman, Proc. Interspeech. InterspeechA. Nagrani, J. S. Chung, and A. Zisserman, \"VoxCeleb: A large- scale speaker identification dataset,\" in Proc. Interspeech, 2017, pp. 2616-2620.\n\nDeep speaker: an end-to-end neural speaker embedding system. C Li, X Ma, B Jiang, X Li, X Zhang, X Liu, Y Cao, A Kannan, Z Zhu, arXiv:1705.02304arXiv preprintC. Li, X. Ma, B. Jiang, X. Li, X. Zhang, X. Liu, Y. Cao, A. Kan- nan, and Z. Zhu, \"Deep speaker: an end-to-end neural speaker embedding system,\" arXiv preprint arXiv:1705.02304, 2017.\n\nDeep neural network embeddings for text-independent speaker verification. D Snyder, D Garcia-Romero, D Povey, S Khudanpur, Proc. Interspeech. InterspeechD. Snyder, D. Garcia-Romero, D. Povey, and S. Khudanpur, \"Deep neural network embeddings for text-independent speaker verification,\" in Proc. Interspeech, 2017, pp. 999-1003.\n\nDeep speaker embeddings for short-duration speaker verification. G Bhattacharya, J Alam, P Kenny, Proc. Interspeech. InterspeechG. Bhattacharya, J. Alam, and P. Kenny, \"Deep speaker em- beddings for short-duration speaker verification,\" in Proc. Inter- speech, 2017, pp. 1517-1521.\n\nAttentionbased models for text-dependent speaker verification. F Chowdhury, Q Wang, I L Moreno, L Wan, arXiv:1710.10470arXiv preprintF. Chowdhury, Q. Wang, I. L. Moreno, and L. Wan, \"Attention- based models for text-dependent speaker verification,\" arXiv preprint arXiv:1710.10470, 2017.\n\nFeed-forward networks with attention can solve some long-term memory problems. C Raffel, D P Ellis, arXiv:1512.08756arXiv preprintC. Raffel and D. P. Ellis, \"Feed-forward networks with atten- tion can solve some long-term memory problems,\" arXiv preprint arXiv:1512.08756, 2015.\n\nProbabilistic linear discriminant analysis. S Ioffe, European Conference on Computer Vision. SpringerS. Ioffe, \"Probabilistic linear discriminant analysis,\" in European Conference on Computer Vision. Springer, 2006, pp. 531-542.\n\nProbabilistic linear discriminant analysis for inferences about identity. S J Prince, J H Elder, Proc. ICCV. ICCVS. J. Prince and J. H. Elder, \"Probabilistic linear discriminant anal- ysis for inferences about identity,\" in Proc. ICCV, 2007, pp. 1-8.\n\nDeep neural network based text-dependent speaker recognition: Preliminary results. G Bhattacharya, J Alam, T Stafylakis, P Kenny, Proc. Odyssey. OdysseyG. Bhattacharya, J. Alam, T. Stafylakis, and P. Kenny, \"Deep neu- ral network based text-dependent speaker recognition: Prelimi- nary results,\" in Proc. Odyssey, 2016, pp. 9-15.\n\nThe 2012 NIST speaker recognition evaluation. C S Greenberg, V M Stanford, A F Martin, M Yadagiri, G R Doddington, J J Godfrey, J Hernandez-Cordero, Proc. Interspeech. InterspeechC. S. Greenberg, V. M. Stanford, A. F. Martin, M. Yadagiri, G. R. Doddington, J. J. Godfrey, and J. Hernandez-Cordero, \"The 2012 NIST speaker recognition evaluation.\" in Proc. Interspeech, 2013, pp. 1971-1975.\n\nAnalysis of i-vector length normalization in speaker recognition systems. D Garcia-Romero, C Y Espy-Wilson, Proc. Interspeech. InterspeechD. Garcia-Romero and C. Y. Espy-Wilson, \"Analysis of i-vector length normalization in speaker recognition systems,\" in Proc. In- terspeech, 2011, pp. 249-252.\n\nThe Kaldi speech recognition toolkit. D Povey, A Ghoshal, G Boulianne, L Burget, O Glembek, N Goel, M Hannemann, P Motlicek, Y Qian, P Schwarz, no. EPFL-CONF-192584IEEE 2011 workshop on automatic speech recognition and understanding (ASRU). D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., \"The Kaldi speech recognition toolkit,\" in IEEE 2011 workshop on automatic speech recognition and understanding (ASRU), no. EPFL-CONF-192584. IEEE Signal Processing Society, 2011.\n\nX-vectors: Robust DNN embeddings for speaker recognition. D Snyder, D Garcia-Romero, G Sell, D Povey, S Khudanpur, Proc. ICASSP. ICASSPD. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudan- pur, \"X-vectors: Robust DNN embeddings for speaker recogni- tion,\" in Proc. ICASSP, 2018.\n\nPromoting robustness for speaker modeling in the community: the PRISM evaluation set. L Ferrer, H Bratt, L Burget, H Cernocky, O Glembek, M Graciarena, A Lawson, Y Lei, P Matejka, O Plchot, Proceedings of NIST 2011 workshop. NIST 2011 workshopL. Ferrer, H. Bratt, L. Burget, H. Cernocky, O. Glembek, M. Gra- ciarena, A. Lawson, Y. Lei, P. Matejka, O. Plchot et al., \"Pro- moting robustness for speaker modeling in the community: the PRISM evaluation set,\" in Proceedings of NIST 2011 workshop, 2011.\n\nA summary of the REVERB challenge: state-ofthe-art and remaining challenges in reverberant speech processing research. K Kinoshita, M Delcroix, S Gannot, E A Habets, R Haeb-Umbach, W Kellermann, V Leutnant, R Maas, T Nakatani, B Raj, EURASIP Journal on Advances in Signal Processing. 201617K. Kinoshita, M. Delcroix, S. Gannot, E. A. Habets, R. Haeb- Umbach, W. Kellermann, V. Leutnant, R. Maas, T. Nakatani, B. Raj et al., \"A summary of the REVERB challenge: state-of- the-art and remaining challenges in reverberant speech processing research,\" EURASIP Journal on Advances in Signal Processing, vol. 2016, no. 1, p. 7, 2016.\n", "annotations": {"author": "[{\"end\":161,\"start\":70},{\"end\":262,\"start\":162},{\"end\":369,\"start\":263},{\"end\":161,\"start\":70},{\"end\":262,\"start\":162},{\"end\":369,\"start\":263}]", "publisher": null, "author_last_name": "[{\"end\":80,\"start\":75},{\"end\":180,\"start\":171},{\"end\":277,\"start\":270},{\"end\":80,\"start\":75},{\"end\":180,\"start\":171},{\"end\":277,\"start\":270}]", "author_first_name": "[{\"end\":74,\"start\":70},{\"end\":170,\"start\":162},{\"end\":269,\"start\":263},{\"end\":74,\"start\":70},{\"end\":170,\"start\":162},{\"end\":269,\"start\":263}]", "author_affiliation": "[{\"end\":160,\"start\":104},{\"end\":261,\"start\":205},{\"end\":368,\"start\":302},{\"end\":160,\"start\":104},{\"end\":261,\"start\":205},{\"end\":368,\"start\":302}]", "title": "[{\"end\":56,\"start\":1},{\"end\":425,\"start\":370},{\"end\":56,\"start\":1},{\"end\":425,\"start\":370}]", "venue": null, "abstract": "[{\"end\":1213,\"start\":519},{\"end\":1213,\"start\":519}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1324,\"start\":1321},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1708,\"start\":1705},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1710,\"start\":1708},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2142,\"start\":2139},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2210,\"start\":2207},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2612,\"start\":2609},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3193,\"start\":3190},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3195,\"start\":3193},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3214,\"start\":3211},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3563,\"start\":3559},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3566,\"start\":3563},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3610,\"start\":3606},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4209,\"start\":4205},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4252,\"start\":4248},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5859,\"start\":5856},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5899,\"start\":5896},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5901,\"start\":5899},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5912,\"start\":5908},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5915,\"start\":5912},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5950,\"start\":5947},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7128,\"start\":7125},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7148,\"start\":7145},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7204,\"start\":7200},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7207,\"start\":7204},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7292,\"start\":7289},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7295,\"start\":7292},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7724,\"start\":7721},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8497,\"start\":8493},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8862,\"start\":8858},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8865,\"start\":8862},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10136,\"start\":10133},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10851,\"start\":10847},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10908,\"start\":10905},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11650,\"start\":11646},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12276,\"start\":12272},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12279,\"start\":12276},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14033,\"start\":14029},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14175,\"start\":14171},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14474,\"start\":14471},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14792,\"start\":14789},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15284,\"start\":15281},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15286,\"start\":15284},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15347,\"start\":15343},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15350,\"start\":15347},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15434,\"start\":15431},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16185,\"start\":16181},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16188,\"start\":16185},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16220,\"start\":16217},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17947,\"start\":17944},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1324,\"start\":1321},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1708,\"start\":1705},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1710,\"start\":1708},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2142,\"start\":2139},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2210,\"start\":2207},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2612,\"start\":2609},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3193,\"start\":3190},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3195,\"start\":3193},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3214,\"start\":3211},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3563,\"start\":3559},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3566,\"start\":3563},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3610,\"start\":3606},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4209,\"start\":4205},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4252,\"start\":4248},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5859,\"start\":5856},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5899,\"start\":5896},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5901,\"start\":5899},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5912,\"start\":5908},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5915,\"start\":5912},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5950,\"start\":5947},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7128,\"start\":7125},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7148,\"start\":7145},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7204,\"start\":7200},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7207,\"start\":7204},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7292,\"start\":7289},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7295,\"start\":7292},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7724,\"start\":7721},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8497,\"start\":8493},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8862,\"start\":8858},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8865,\"start\":8862},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10136,\"start\":10133},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10851,\"start\":10847},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10908,\"start\":10905},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11650,\"start\":11646},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12276,\"start\":12272},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12279,\"start\":12276},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14033,\"start\":14029},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14175,\"start\":14171},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14474,\"start\":14471},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14792,\"start\":14789},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15284,\"start\":15281},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15286,\"start\":15284},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15347,\"start\":15343},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15350,\"start\":15347},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15434,\"start\":15431},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16185,\"start\":16181},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16188,\"start\":16185},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16220,\"start\":16217},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17947,\"start\":17944}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":19189,\"start\":19167},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":19420,\"start\":19190},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":19779,\"start\":19421},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":20096,\"start\":19780},{\"attributes\":{\"id\":\"fig_0\"},\"end\":19189,\"start\":19167},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":19420,\"start\":19190},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":19779,\"start\":19421},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":20096,\"start\":19780}]", "paragraph": "[{\"end\":1435,\"start\":1229},{\"end\":2211,\"start\":1437},{\"end\":2802,\"start\":2213},{\"end\":3497,\"start\":2804},{\"end\":4253,\"start\":3499},{\"end\":4984,\"start\":4255},{\"end\":5357,\"start\":4986},{\"end\":5500,\"start\":5384},{\"end\":5951,\"start\":5502},{\"end\":6075,\"start\":5953},{\"end\":7296,\"start\":6235},{\"end\":7669,\"start\":7336},{\"end\":7877,\"start\":7692},{\"end\":8665,\"start\":7928},{\"end\":9137,\"start\":8689},{\"end\":9356,\"start\":9173},{\"end\":9497,\"start\":9388},{\"end\":9667,\"start\":9515},{\"end\":10196,\"start\":9700},{\"end\":10751,\"start\":10225},{\"end\":11129,\"start\":10777},{\"end\":11827,\"start\":11149},{\"end\":12591,\"start\":11861},{\"end\":12880,\"start\":12593},{\"end\":13336,\"start\":12882},{\"end\":13578,\"start\":13338},{\"end\":13739,\"start\":13611},{\"end\":14403,\"start\":13741},{\"end\":14913,\"start\":14405},{\"end\":15661,\"start\":14915},{\"end\":18130,\"start\":15689},{\"end\":19166,\"start\":18169},{\"end\":1435,\"start\":1229},{\"end\":2211,\"start\":1437},{\"end\":2802,\"start\":2213},{\"end\":3497,\"start\":2804},{\"end\":4253,\"start\":3499},{\"end\":4984,\"start\":4255},{\"end\":5357,\"start\":4986},{\"end\":5500,\"start\":5384},{\"end\":5951,\"start\":5502},{\"end\":6075,\"start\":5953},{\"end\":7296,\"start\":6235},{\"end\":7669,\"start\":7336},{\"end\":7877,\"start\":7692},{\"end\":8665,\"start\":7928},{\"end\":9137,\"start\":8689},{\"end\":9356,\"start\":9173},{\"end\":9497,\"start\":9388},{\"end\":9667,\"start\":9515},{\"end\":10196,\"start\":9700},{\"end\":10751,\"start\":10225},{\"end\":11129,\"start\":10777},{\"end\":11827,\"start\":11149},{\"end\":12591,\"start\":11861},{\"end\":12880,\"start\":12593},{\"end\":13336,\"start\":12882},{\"end\":13578,\"start\":13338},{\"end\":13739,\"start\":13611},{\"end\":14403,\"start\":13741},{\"end\":14913,\"start\":14405},{\"end\":15661,\"start\":14915},{\"end\":18130,\"start\":15689},{\"end\":19166,\"start\":18169}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6234,\"start\":6092},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7896,\"start\":7878},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7927,\"start\":7896},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9172,\"start\":9138},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9387,\"start\":9357},{\"attributes\":{\"id\":\"formula_5\"},\"end\":9514,\"start\":9498},{\"attributes\":{\"id\":\"formula_6\"},\"end\":10224,\"start\":10197},{\"attributes\":{\"id\":\"formula_0\"},\"end\":6234,\"start\":6092},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7896,\"start\":7878},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7927,\"start\":7896},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9172,\"start\":9138},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9387,\"start\":9357},{\"attributes\":{\"id\":\"formula_5\"},\"end\":9514,\"start\":9498},{\"attributes\":{\"id\":\"formula_6\"},\"end\":10224,\"start\":10197}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":15204,\"start\":15197},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":16724,\"start\":16717},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":17290,\"start\":17283},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":15204,\"start\":15197},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":16724,\"start\":16717},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":17290,\"start\":17283}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1227,\"start\":1215},{\"attributes\":{\"n\":\"2.\"},\"end\":5382,\"start\":5360},{\"end\":6091,\"start\":6078},{\"attributes\":{\"n\":\"3.\"},\"end\":7334,\"start\":7299},{\"attributes\":{\"n\":\"3.1.\"},\"end\":7690,\"start\":7672},{\"attributes\":{\"n\":\"3.2.\"},\"end\":8687,\"start\":8668},{\"attributes\":{\"n\":\"3.3.\"},\"end\":9698,\"start\":9670},{\"attributes\":{\"n\":\"4.1.\"},\"end\":10775,\"start\":10754},{\"attributes\":{\"n\":\"4.1.1.\"},\"end\":11147,\"start\":11132},{\"attributes\":{\"n\":\"4.1.2.\"},\"end\":11859,\"start\":11830},{\"attributes\":{\"n\":\"4.1.3.\"},\"end\":13609,\"start\":13581},{\"attributes\":{\"n\":\"4.2.\"},\"end\":15671,\"start\":15664},{\"attributes\":{\"n\":\"4.2.1.\"},\"end\":15687,\"start\":15674},{\"attributes\":{\"n\":\"4.2.2.\"},\"end\":18141,\"start\":18133},{\"attributes\":{\"n\":\"5.\"},\"end\":18167,\"start\":18144},{\"end\":19178,\"start\":19168},{\"end\":19200,\"start\":19191},{\"end\":19431,\"start\":19422},{\"end\":19790,\"start\":19781},{\"attributes\":{\"n\":\"1.\"},\"end\":1227,\"start\":1215},{\"attributes\":{\"n\":\"2.\"},\"end\":5382,\"start\":5360},{\"end\":6091,\"start\":6078},{\"attributes\":{\"n\":\"3.\"},\"end\":7334,\"start\":7299},{\"attributes\":{\"n\":\"3.1.\"},\"end\":7690,\"start\":7672},{\"attributes\":{\"n\":\"3.2.\"},\"end\":8687,\"start\":8668},{\"attributes\":{\"n\":\"3.3.\"},\"end\":9698,\"start\":9670},{\"attributes\":{\"n\":\"4.1.\"},\"end\":10775,\"start\":10754},{\"attributes\":{\"n\":\"4.1.1.\"},\"end\":11147,\"start\":11132},{\"attributes\":{\"n\":\"4.1.2.\"},\"end\":11859,\"start\":11830},{\"attributes\":{\"n\":\"4.1.3.\"},\"end\":13609,\"start\":13581},{\"attributes\":{\"n\":\"4.2.\"},\"end\":15671,\"start\":15664},{\"attributes\":{\"n\":\"4.2.1.\"},\"end\":15687,\"start\":15674},{\"attributes\":{\"n\":\"4.2.2.\"},\"end\":18141,\"start\":18133},{\"attributes\":{\"n\":\"5.\"},\"end\":18167,\"start\":18144},{\"end\":19178,\"start\":19168},{\"end\":19200,\"start\":19191},{\"end\":19431,\"start\":19422},{\"end\":19790,\"start\":19781}]", "table": "[{\"end\":19420,\"start\":19304},{\"end\":19779,\"start\":19437},{\"end\":20096,\"start\":19871},{\"end\":19420,\"start\":19304},{\"end\":19779,\"start\":19437},{\"end\":20096,\"start\":19871}]", "figure_caption": "[{\"end\":19189,\"start\":19180},{\"end\":19304,\"start\":19202},{\"end\":19437,\"start\":19433},{\"end\":19871,\"start\":19792},{\"end\":19189,\"start\":19180},{\"end\":19304,\"start\":19202},{\"end\":19437,\"start\":19433},{\"end\":19871,\"start\":19792}]", "figure_ref": "[{\"end\":5499,\"start\":5491},{\"end\":6281,\"start\":6273},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10096,\"start\":10088},{\"end\":5499,\"start\":5491},{\"end\":6281,\"start\":6273},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10096,\"start\":10088}]", "bib_author_first_name": "[{\"end\":20151,\"start\":20150},{\"end\":20160,\"start\":20159},{\"end\":20162,\"start\":20161},{\"end\":20171,\"start\":20170},{\"end\":20180,\"start\":20179},{\"end\":20193,\"start\":20192},{\"end\":20571,\"start\":20570},{\"end\":20578,\"start\":20577},{\"end\":20590,\"start\":20589},{\"end\":20600,\"start\":20599},{\"end\":20874,\"start\":20873},{\"end\":20885,\"start\":20884},{\"end\":20892,\"start\":20891},{\"end\":21170,\"start\":21169},{\"end\":21179,\"start\":21178},{\"end\":21188,\"start\":21187},{\"end\":21488,\"start\":21487},{\"end\":21496,\"start\":21495},{\"end\":21503,\"start\":21502},{\"end\":21512,\"start\":21508},{\"end\":21521,\"start\":21520},{\"end\":21781,\"start\":21780},{\"end\":21792,\"start\":21791},{\"end\":21802,\"start\":21801},{\"end\":21812,\"start\":21811},{\"end\":22037,\"start\":22036},{\"end\":22048,\"start\":22047},{\"end\":22050,\"start\":22049},{\"end\":22059,\"start\":22058},{\"end\":22309,\"start\":22308},{\"end\":22315,\"start\":22314},{\"end\":22321,\"start\":22320},{\"end\":22330,\"start\":22329},{\"end\":22336,\"start\":22335},{\"end\":22345,\"start\":22344},{\"end\":22352,\"start\":22351},{\"end\":22359,\"start\":22358},{\"end\":22369,\"start\":22368},{\"end\":22665,\"start\":22664},{\"end\":22675,\"start\":22674},{\"end\":22692,\"start\":22691},{\"end\":22701,\"start\":22700},{\"end\":22985,\"start\":22984},{\"end\":23001,\"start\":23000},{\"end\":23009,\"start\":23008},{\"end\":23266,\"start\":23265},{\"end\":23279,\"start\":23278},{\"end\":23287,\"start\":23286},{\"end\":23289,\"start\":23288},{\"end\":23299,\"start\":23298},{\"end\":23571,\"start\":23570},{\"end\":23581,\"start\":23580},{\"end\":23583,\"start\":23582},{\"end\":23816,\"start\":23815},{\"end\":24076,\"start\":24075},{\"end\":24078,\"start\":24077},{\"end\":24088,\"start\":24087},{\"end\":24090,\"start\":24089},{\"end\":24337,\"start\":24336},{\"end\":24353,\"start\":24352},{\"end\":24361,\"start\":24360},{\"end\":24375,\"start\":24374},{\"end\":24631,\"start\":24630},{\"end\":24633,\"start\":24632},{\"end\":24646,\"start\":24645},{\"end\":24648,\"start\":24647},{\"end\":24660,\"start\":24659},{\"end\":24662,\"start\":24661},{\"end\":24672,\"start\":24671},{\"end\":24684,\"start\":24683},{\"end\":24686,\"start\":24685},{\"end\":24700,\"start\":24699},{\"end\":24702,\"start\":24701},{\"end\":24713,\"start\":24712},{\"end\":25049,\"start\":25048},{\"end\":25066,\"start\":25065},{\"end\":25068,\"start\":25067},{\"end\":25311,\"start\":25310},{\"end\":25320,\"start\":25319},{\"end\":25331,\"start\":25330},{\"end\":25344,\"start\":25343},{\"end\":25354,\"start\":25353},{\"end\":25365,\"start\":25364},{\"end\":25373,\"start\":25372},{\"end\":25386,\"start\":25385},{\"end\":25398,\"start\":25397},{\"end\":25406,\"start\":25405},{\"end\":25876,\"start\":25875},{\"end\":25886,\"start\":25885},{\"end\":25903,\"start\":25902},{\"end\":25911,\"start\":25910},{\"end\":25920,\"start\":25919},{\"end\":26193,\"start\":26192},{\"end\":26203,\"start\":26202},{\"end\":26212,\"start\":26211},{\"end\":26222,\"start\":26221},{\"end\":26234,\"start\":26233},{\"end\":26245,\"start\":26244},{\"end\":26259,\"start\":26258},{\"end\":26269,\"start\":26268},{\"end\":26276,\"start\":26275},{\"end\":26287,\"start\":26286},{\"end\":26727,\"start\":26726},{\"end\":26740,\"start\":26739},{\"end\":26752,\"start\":26751},{\"end\":26762,\"start\":26761},{\"end\":26764,\"start\":26763},{\"end\":26774,\"start\":26773},{\"end\":26789,\"start\":26788},{\"end\":26803,\"start\":26802},{\"end\":26815,\"start\":26814},{\"end\":26823,\"start\":26822},{\"end\":26835,\"start\":26834},{\"end\":20151,\"start\":20150},{\"end\":20160,\"start\":20159},{\"end\":20162,\"start\":20161},{\"end\":20171,\"start\":20170},{\"end\":20180,\"start\":20179},{\"end\":20193,\"start\":20192},{\"end\":20571,\"start\":20570},{\"end\":20578,\"start\":20577},{\"end\":20590,\"start\":20589},{\"end\":20600,\"start\":20599},{\"end\":20874,\"start\":20873},{\"end\":20885,\"start\":20884},{\"end\":20892,\"start\":20891},{\"end\":21170,\"start\":21169},{\"end\":21179,\"start\":21178},{\"end\":21188,\"start\":21187},{\"end\":21488,\"start\":21487},{\"end\":21496,\"start\":21495},{\"end\":21503,\"start\":21502},{\"end\":21512,\"start\":21508},{\"end\":21521,\"start\":21520},{\"end\":21781,\"start\":21780},{\"end\":21792,\"start\":21791},{\"end\":21802,\"start\":21801},{\"end\":21812,\"start\":21811},{\"end\":22037,\"start\":22036},{\"end\":22048,\"start\":22047},{\"end\":22050,\"start\":22049},{\"end\":22059,\"start\":22058},{\"end\":22309,\"start\":22308},{\"end\":22315,\"start\":22314},{\"end\":22321,\"start\":22320},{\"end\":22330,\"start\":22329},{\"end\":22336,\"start\":22335},{\"end\":22345,\"start\":22344},{\"end\":22352,\"start\":22351},{\"end\":22359,\"start\":22358},{\"end\":22369,\"start\":22368},{\"end\":22665,\"start\":22664},{\"end\":22675,\"start\":22674},{\"end\":22692,\"start\":22691},{\"end\":22701,\"start\":22700},{\"end\":22985,\"start\":22984},{\"end\":23001,\"start\":23000},{\"end\":23009,\"start\":23008},{\"end\":23266,\"start\":23265},{\"end\":23279,\"start\":23278},{\"end\":23287,\"start\":23286},{\"end\":23289,\"start\":23288},{\"end\":23299,\"start\":23298},{\"end\":23571,\"start\":23570},{\"end\":23581,\"start\":23580},{\"end\":23583,\"start\":23582},{\"end\":23816,\"start\":23815},{\"end\":24076,\"start\":24075},{\"end\":24078,\"start\":24077},{\"end\":24088,\"start\":24087},{\"end\":24090,\"start\":24089},{\"end\":24337,\"start\":24336},{\"end\":24353,\"start\":24352},{\"end\":24361,\"start\":24360},{\"end\":24375,\"start\":24374},{\"end\":24631,\"start\":24630},{\"end\":24633,\"start\":24632},{\"end\":24646,\"start\":24645},{\"end\":24648,\"start\":24647},{\"end\":24660,\"start\":24659},{\"end\":24662,\"start\":24661},{\"end\":24672,\"start\":24671},{\"end\":24684,\"start\":24683},{\"end\":24686,\"start\":24685},{\"end\":24700,\"start\":24699},{\"end\":24702,\"start\":24701},{\"end\":24713,\"start\":24712},{\"end\":25049,\"start\":25048},{\"end\":25066,\"start\":25065},{\"end\":25068,\"start\":25067},{\"end\":25311,\"start\":25310},{\"end\":25320,\"start\":25319},{\"end\":25331,\"start\":25330},{\"end\":25344,\"start\":25343},{\"end\":25354,\"start\":25353},{\"end\":25365,\"start\":25364},{\"end\":25373,\"start\":25372},{\"end\":25386,\"start\":25385},{\"end\":25398,\"start\":25397},{\"end\":25406,\"start\":25405},{\"end\":25876,\"start\":25875},{\"end\":25886,\"start\":25885},{\"end\":25903,\"start\":25902},{\"end\":25911,\"start\":25910},{\"end\":25920,\"start\":25919},{\"end\":26193,\"start\":26192},{\"end\":26203,\"start\":26202},{\"end\":26212,\"start\":26211},{\"end\":26222,\"start\":26221},{\"end\":26234,\"start\":26233},{\"end\":26245,\"start\":26244},{\"end\":26259,\"start\":26258},{\"end\":26269,\"start\":26268},{\"end\":26276,\"start\":26275},{\"end\":26287,\"start\":26286},{\"end\":26727,\"start\":26726},{\"end\":26740,\"start\":26739},{\"end\":26752,\"start\":26751},{\"end\":26762,\"start\":26761},{\"end\":26764,\"start\":26763},{\"end\":26774,\"start\":26773},{\"end\":26789,\"start\":26788},{\"end\":26803,\"start\":26802},{\"end\":26815,\"start\":26814},{\"end\":26823,\"start\":26822},{\"end\":26835,\"start\":26834}]", "bib_author_last_name": "[{\"end\":20157,\"start\":20152},{\"end\":20168,\"start\":20163},{\"end\":20177,\"start\":20172},{\"end\":20190,\"start\":20181},{\"end\":20201,\"start\":20194},{\"end\":20575,\"start\":20572},{\"end\":20587,\"start\":20579},{\"end\":20597,\"start\":20591},{\"end\":20608,\"start\":20601},{\"end\":20882,\"start\":20875},{\"end\":20889,\"start\":20886},{\"end\":20899,\"start\":20893},{\"end\":21176,\"start\":21171},{\"end\":21185,\"start\":21180},{\"end\":21192,\"start\":21189},{\"end\":21493,\"start\":21489},{\"end\":21500,\"start\":21497},{\"end\":21506,\"start\":21504},{\"end\":21518,\"start\":21513},{\"end\":21525,\"start\":21522},{\"end\":21789,\"start\":21782},{\"end\":21799,\"start\":21793},{\"end\":21809,\"start\":21803},{\"end\":21820,\"start\":21813},{\"end\":22045,\"start\":22038},{\"end\":22056,\"start\":22051},{\"end\":22069,\"start\":22060},{\"end\":22312,\"start\":22310},{\"end\":22318,\"start\":22316},{\"end\":22327,\"start\":22322},{\"end\":22333,\"start\":22331},{\"end\":22342,\"start\":22337},{\"end\":22349,\"start\":22346},{\"end\":22356,\"start\":22353},{\"end\":22366,\"start\":22360},{\"end\":22373,\"start\":22370},{\"end\":22672,\"start\":22666},{\"end\":22689,\"start\":22676},{\"end\":22698,\"start\":22693},{\"end\":22711,\"start\":22702},{\"end\":22998,\"start\":22986},{\"end\":23006,\"start\":23002},{\"end\":23015,\"start\":23010},{\"end\":23276,\"start\":23267},{\"end\":23284,\"start\":23280},{\"end\":23296,\"start\":23290},{\"end\":23303,\"start\":23300},{\"end\":23578,\"start\":23572},{\"end\":23589,\"start\":23584},{\"end\":23822,\"start\":23817},{\"end\":24085,\"start\":24079},{\"end\":24096,\"start\":24091},{\"end\":24350,\"start\":24338},{\"end\":24358,\"start\":24354},{\"end\":24372,\"start\":24362},{\"end\":24381,\"start\":24376},{\"end\":24643,\"start\":24634},{\"end\":24657,\"start\":24649},{\"end\":24669,\"start\":24663},{\"end\":24681,\"start\":24673},{\"end\":24697,\"start\":24687},{\"end\":24710,\"start\":24703},{\"end\":24731,\"start\":24714},{\"end\":25063,\"start\":25050},{\"end\":25080,\"start\":25069},{\"end\":25317,\"start\":25312},{\"end\":25328,\"start\":25321},{\"end\":25341,\"start\":25332},{\"end\":25351,\"start\":25345},{\"end\":25362,\"start\":25355},{\"end\":25370,\"start\":25366},{\"end\":25383,\"start\":25374},{\"end\":25395,\"start\":25387},{\"end\":25403,\"start\":25399},{\"end\":25414,\"start\":25407},{\"end\":25883,\"start\":25877},{\"end\":25900,\"start\":25887},{\"end\":25908,\"start\":25904},{\"end\":25917,\"start\":25912},{\"end\":25930,\"start\":25921},{\"end\":26200,\"start\":26194},{\"end\":26209,\"start\":26204},{\"end\":26219,\"start\":26213},{\"end\":26231,\"start\":26223},{\"end\":26242,\"start\":26235},{\"end\":26256,\"start\":26246},{\"end\":26266,\"start\":26260},{\"end\":26273,\"start\":26270},{\"end\":26284,\"start\":26277},{\"end\":26294,\"start\":26288},{\"end\":26737,\"start\":26728},{\"end\":26749,\"start\":26741},{\"end\":26759,\"start\":26753},{\"end\":26771,\"start\":26765},{\"end\":26786,\"start\":26775},{\"end\":26800,\"start\":26790},{\"end\":26812,\"start\":26804},{\"end\":26820,\"start\":26816},{\"end\":26832,\"start\":26824},{\"end\":26839,\"start\":26836},{\"end\":20157,\"start\":20152},{\"end\":20168,\"start\":20163},{\"end\":20177,\"start\":20172},{\"end\":20190,\"start\":20181},{\"end\":20201,\"start\":20194},{\"end\":20575,\"start\":20572},{\"end\":20587,\"start\":20579},{\"end\":20597,\"start\":20591},{\"end\":20608,\"start\":20601},{\"end\":20882,\"start\":20875},{\"end\":20889,\"start\":20886},{\"end\":20899,\"start\":20893},{\"end\":21176,\"start\":21171},{\"end\":21185,\"start\":21180},{\"end\":21192,\"start\":21189},{\"end\":21493,\"start\":21489},{\"end\":21500,\"start\":21497},{\"end\":21506,\"start\":21504},{\"end\":21518,\"start\":21513},{\"end\":21525,\"start\":21522},{\"end\":21789,\"start\":21782},{\"end\":21799,\"start\":21793},{\"end\":21809,\"start\":21803},{\"end\":21820,\"start\":21813},{\"end\":22045,\"start\":22038},{\"end\":22056,\"start\":22051},{\"end\":22069,\"start\":22060},{\"end\":22312,\"start\":22310},{\"end\":22318,\"start\":22316},{\"end\":22327,\"start\":22322},{\"end\":22333,\"start\":22331},{\"end\":22342,\"start\":22337},{\"end\":22349,\"start\":22346},{\"end\":22356,\"start\":22353},{\"end\":22366,\"start\":22360},{\"end\":22373,\"start\":22370},{\"end\":22672,\"start\":22666},{\"end\":22689,\"start\":22676},{\"end\":22698,\"start\":22693},{\"end\":22711,\"start\":22702},{\"end\":22998,\"start\":22986},{\"end\":23006,\"start\":23002},{\"end\":23015,\"start\":23010},{\"end\":23276,\"start\":23267},{\"end\":23284,\"start\":23280},{\"end\":23296,\"start\":23290},{\"end\":23303,\"start\":23300},{\"end\":23578,\"start\":23572},{\"end\":23589,\"start\":23584},{\"end\":23822,\"start\":23817},{\"end\":24085,\"start\":24079},{\"end\":24096,\"start\":24091},{\"end\":24350,\"start\":24338},{\"end\":24358,\"start\":24354},{\"end\":24372,\"start\":24362},{\"end\":24381,\"start\":24376},{\"end\":24643,\"start\":24634},{\"end\":24657,\"start\":24649},{\"end\":24669,\"start\":24663},{\"end\":24681,\"start\":24673},{\"end\":24697,\"start\":24687},{\"end\":24710,\"start\":24703},{\"end\":24731,\"start\":24714},{\"end\":25063,\"start\":25050},{\"end\":25080,\"start\":25069},{\"end\":25317,\"start\":25312},{\"end\":25328,\"start\":25321},{\"end\":25341,\"start\":25332},{\"end\":25351,\"start\":25345},{\"end\":25362,\"start\":25355},{\"end\":25370,\"start\":25366},{\"end\":25383,\"start\":25374},{\"end\":25395,\"start\":25387},{\"end\":25403,\"start\":25399},{\"end\":25414,\"start\":25407},{\"end\":25883,\"start\":25877},{\"end\":25900,\"start\":25887},{\"end\":25908,\"start\":25904},{\"end\":25917,\"start\":25912},{\"end\":25930,\"start\":25921},{\"end\":26200,\"start\":26194},{\"end\":26209,\"start\":26204},{\"end\":26219,\"start\":26213},{\"end\":26231,\"start\":26223},{\"end\":26242,\"start\":26235},{\"end\":26256,\"start\":26246},{\"end\":26266,\"start\":26260},{\"end\":26273,\"start\":26270},{\"end\":26284,\"start\":26277},{\"end\":26294,\"start\":26288},{\"end\":26737,\"start\":26728},{\"end\":26749,\"start\":26741},{\"end\":26759,\"start\":26753},{\"end\":26771,\"start\":26765},{\"end\":26786,\"start\":26775},{\"end\":26800,\"start\":26790},{\"end\":26812,\"start\":26804},{\"end\":26820,\"start\":26816},{\"end\":26832,\"start\":26824},{\"end\":26839,\"start\":26836}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":41754},\"end\":20481,\"start\":20098},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":12361704},\"end\":20804,\"start\":20483},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8868753},\"end\":21064,\"start\":20806},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":5877163},\"end\":21405,\"start\":21066},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":10632806},\"end\":21730,\"start\":21407},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":276220},\"end\":21979,\"start\":21732},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":10475843},\"end\":22245,\"start\":21981},{\"attributes\":{\"doi\":\"arXiv:1705.02304\",\"id\":\"b7\"},\"end\":22588,\"start\":22247},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6206708},\"end\":22917,\"start\":22590},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":42462955},\"end\":23200,\"start\":22919},{\"attributes\":{\"doi\":\"arXiv:1710.10470\",\"id\":\"b10\"},\"end\":23489,\"start\":23202},{\"attributes\":{\"doi\":\"arXiv:1512.08756\",\"id\":\"b11\"},\"end\":23769,\"start\":23491},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":41807584},\"end\":23999,\"start\":23771},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":4253493},\"end\":24251,\"start\":24001},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":28127812},\"end\":24582,\"start\":24253},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1664446},\"end\":24972,\"start\":24584},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":8581960},\"end\":25270,\"start\":24974},{\"attributes\":{\"doi\":\"no. EPFL-CONF-192584\",\"id\":\"b17\",\"matched_paper_id\":1774023},\"end\":25815,\"start\":25272},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":46954166},\"end\":26104,\"start\":25817},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2071785},\"end\":26605,\"start\":26106},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1674006},\"end\":27233,\"start\":26607},{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":41754},\"end\":20481,\"start\":20098},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":12361704},\"end\":20804,\"start\":20483},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8868753},\"end\":21064,\"start\":20806},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":5877163},\"end\":21405,\"start\":21066},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":10632806},\"end\":21730,\"start\":21407},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":276220},\"end\":21979,\"start\":21732},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":10475843},\"end\":22245,\"start\":21981},{\"attributes\":{\"doi\":\"arXiv:1705.02304\",\"id\":\"b7\"},\"end\":22588,\"start\":22247},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6206708},\"end\":22917,\"start\":22590},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":42462955},\"end\":23200,\"start\":22919},{\"attributes\":{\"doi\":\"arXiv:1710.10470\",\"id\":\"b10\"},\"end\":23489,\"start\":23202},{\"attributes\":{\"doi\":\"arXiv:1512.08756\",\"id\":\"b11\"},\"end\":23769,\"start\":23491},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":41807584},\"end\":23999,\"start\":23771},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":4253493},\"end\":24251,\"start\":24001},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":28127812},\"end\":24582,\"start\":24253},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1664446},\"end\":24972,\"start\":24584},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":8581960},\"end\":25270,\"start\":24974},{\"attributes\":{\"doi\":\"no. EPFL-CONF-192584\",\"id\":\"b17\",\"matched_paper_id\":1774023},\"end\":25815,\"start\":25272},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":46954166},\"end\":26104,\"start\":25817},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2071785},\"end\":26605,\"start\":26106},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1674006},\"end\":27233,\"start\":26607}]", "bib_title": "[{\"end\":20148,\"start\":20098},{\"end\":20568,\"start\":20483},{\"end\":20871,\"start\":20806},{\"end\":21167,\"start\":21066},{\"end\":21485,\"start\":21407},{\"end\":21778,\"start\":21732},{\"end\":22034,\"start\":21981},{\"end\":22662,\"start\":22590},{\"end\":22982,\"start\":22919},{\"end\":23813,\"start\":23771},{\"end\":24073,\"start\":24001},{\"end\":24334,\"start\":24253},{\"end\":24628,\"start\":24584},{\"end\":25046,\"start\":24974},{\"end\":25308,\"start\":25272},{\"end\":25873,\"start\":25817},{\"end\":26190,\"start\":26106},{\"end\":26724,\"start\":26607},{\"end\":20148,\"start\":20098},{\"end\":20568,\"start\":20483},{\"end\":20871,\"start\":20806},{\"end\":21167,\"start\":21066},{\"end\":21485,\"start\":21407},{\"end\":21778,\"start\":21732},{\"end\":22034,\"start\":21981},{\"end\":22662,\"start\":22590},{\"end\":22982,\"start\":22919},{\"end\":23813,\"start\":23771},{\"end\":24073,\"start\":24001},{\"end\":24334,\"start\":24253},{\"end\":24628,\"start\":24584},{\"end\":25046,\"start\":24974},{\"end\":25308,\"start\":25272},{\"end\":25873,\"start\":25817},{\"end\":26190,\"start\":26106},{\"end\":26724,\"start\":26607}]", "bib_author": "[{\"end\":20159,\"start\":20150},{\"end\":20170,\"start\":20159},{\"end\":20179,\"start\":20170},{\"end\":20192,\"start\":20179},{\"end\":20203,\"start\":20192},{\"end\":20577,\"start\":20570},{\"end\":20589,\"start\":20577},{\"end\":20599,\"start\":20589},{\"end\":20610,\"start\":20599},{\"end\":20884,\"start\":20873},{\"end\":20891,\"start\":20884},{\"end\":20901,\"start\":20891},{\"end\":21178,\"start\":21169},{\"end\":21187,\"start\":21178},{\"end\":21194,\"start\":21187},{\"end\":21495,\"start\":21487},{\"end\":21502,\"start\":21495},{\"end\":21508,\"start\":21502},{\"end\":21520,\"start\":21508},{\"end\":21527,\"start\":21520},{\"end\":21791,\"start\":21780},{\"end\":21801,\"start\":21791},{\"end\":21811,\"start\":21801},{\"end\":21822,\"start\":21811},{\"end\":22047,\"start\":22036},{\"end\":22058,\"start\":22047},{\"end\":22071,\"start\":22058},{\"end\":22314,\"start\":22308},{\"end\":22320,\"start\":22314},{\"end\":22329,\"start\":22320},{\"end\":22335,\"start\":22329},{\"end\":22344,\"start\":22335},{\"end\":22351,\"start\":22344},{\"end\":22358,\"start\":22351},{\"end\":22368,\"start\":22358},{\"end\":22375,\"start\":22368},{\"end\":22674,\"start\":22664},{\"end\":22691,\"start\":22674},{\"end\":22700,\"start\":22691},{\"end\":22713,\"start\":22700},{\"end\":23000,\"start\":22984},{\"end\":23008,\"start\":23000},{\"end\":23017,\"start\":23008},{\"end\":23278,\"start\":23265},{\"end\":23286,\"start\":23278},{\"end\":23298,\"start\":23286},{\"end\":23305,\"start\":23298},{\"end\":23580,\"start\":23570},{\"end\":23591,\"start\":23580},{\"end\":23824,\"start\":23815},{\"end\":24087,\"start\":24075},{\"end\":24098,\"start\":24087},{\"end\":24352,\"start\":24336},{\"end\":24360,\"start\":24352},{\"end\":24374,\"start\":24360},{\"end\":24383,\"start\":24374},{\"end\":24645,\"start\":24630},{\"end\":24659,\"start\":24645},{\"end\":24671,\"start\":24659},{\"end\":24683,\"start\":24671},{\"end\":24699,\"start\":24683},{\"end\":24712,\"start\":24699},{\"end\":24733,\"start\":24712},{\"end\":25065,\"start\":25048},{\"end\":25082,\"start\":25065},{\"end\":25319,\"start\":25310},{\"end\":25330,\"start\":25319},{\"end\":25343,\"start\":25330},{\"end\":25353,\"start\":25343},{\"end\":25364,\"start\":25353},{\"end\":25372,\"start\":25364},{\"end\":25385,\"start\":25372},{\"end\":25397,\"start\":25385},{\"end\":25405,\"start\":25397},{\"end\":25416,\"start\":25405},{\"end\":25885,\"start\":25875},{\"end\":25902,\"start\":25885},{\"end\":25910,\"start\":25902},{\"end\":25919,\"start\":25910},{\"end\":25932,\"start\":25919},{\"end\":26202,\"start\":26192},{\"end\":26211,\"start\":26202},{\"end\":26221,\"start\":26211},{\"end\":26233,\"start\":26221},{\"end\":26244,\"start\":26233},{\"end\":26258,\"start\":26244},{\"end\":26268,\"start\":26258},{\"end\":26275,\"start\":26268},{\"end\":26286,\"start\":26275},{\"end\":26296,\"start\":26286},{\"end\":26739,\"start\":26726},{\"end\":26751,\"start\":26739},{\"end\":26761,\"start\":26751},{\"end\":26773,\"start\":26761},{\"end\":26788,\"start\":26773},{\"end\":26802,\"start\":26788},{\"end\":26814,\"start\":26802},{\"end\":26822,\"start\":26814},{\"end\":26834,\"start\":26822},{\"end\":26841,\"start\":26834},{\"end\":20159,\"start\":20150},{\"end\":20170,\"start\":20159},{\"end\":20179,\"start\":20170},{\"end\":20192,\"start\":20179},{\"end\":20203,\"start\":20192},{\"end\":20577,\"start\":20570},{\"end\":20589,\"start\":20577},{\"end\":20599,\"start\":20589},{\"end\":20610,\"start\":20599},{\"end\":20884,\"start\":20873},{\"end\":20891,\"start\":20884},{\"end\":20901,\"start\":20891},{\"end\":21178,\"start\":21169},{\"end\":21187,\"start\":21178},{\"end\":21194,\"start\":21187},{\"end\":21495,\"start\":21487},{\"end\":21502,\"start\":21495},{\"end\":21508,\"start\":21502},{\"end\":21520,\"start\":21508},{\"end\":21527,\"start\":21520},{\"end\":21791,\"start\":21780},{\"end\":21801,\"start\":21791},{\"end\":21811,\"start\":21801},{\"end\":21822,\"start\":21811},{\"end\":22047,\"start\":22036},{\"end\":22058,\"start\":22047},{\"end\":22071,\"start\":22058},{\"end\":22314,\"start\":22308},{\"end\":22320,\"start\":22314},{\"end\":22329,\"start\":22320},{\"end\":22335,\"start\":22329},{\"end\":22344,\"start\":22335},{\"end\":22351,\"start\":22344},{\"end\":22358,\"start\":22351},{\"end\":22368,\"start\":22358},{\"end\":22375,\"start\":22368},{\"end\":22674,\"start\":22664},{\"end\":22691,\"start\":22674},{\"end\":22700,\"start\":22691},{\"end\":22713,\"start\":22700},{\"end\":23000,\"start\":22984},{\"end\":23008,\"start\":23000},{\"end\":23017,\"start\":23008},{\"end\":23278,\"start\":23265},{\"end\":23286,\"start\":23278},{\"end\":23298,\"start\":23286},{\"end\":23305,\"start\":23298},{\"end\":23580,\"start\":23570},{\"end\":23591,\"start\":23580},{\"end\":23824,\"start\":23815},{\"end\":24087,\"start\":24075},{\"end\":24098,\"start\":24087},{\"end\":24352,\"start\":24336},{\"end\":24360,\"start\":24352},{\"end\":24374,\"start\":24360},{\"end\":24383,\"start\":24374},{\"end\":24645,\"start\":24630},{\"end\":24659,\"start\":24645},{\"end\":24671,\"start\":24659},{\"end\":24683,\"start\":24671},{\"end\":24699,\"start\":24683},{\"end\":24712,\"start\":24699},{\"end\":24733,\"start\":24712},{\"end\":25065,\"start\":25048},{\"end\":25082,\"start\":25065},{\"end\":25319,\"start\":25310},{\"end\":25330,\"start\":25319},{\"end\":25343,\"start\":25330},{\"end\":25353,\"start\":25343},{\"end\":25364,\"start\":25353},{\"end\":25372,\"start\":25364},{\"end\":25385,\"start\":25372},{\"end\":25397,\"start\":25385},{\"end\":25405,\"start\":25397},{\"end\":25416,\"start\":25405},{\"end\":25885,\"start\":25875},{\"end\":25902,\"start\":25885},{\"end\":25910,\"start\":25902},{\"end\":25919,\"start\":25910},{\"end\":25932,\"start\":25919},{\"end\":26202,\"start\":26192},{\"end\":26211,\"start\":26202},{\"end\":26221,\"start\":26211},{\"end\":26233,\"start\":26221},{\"end\":26244,\"start\":26233},{\"end\":26258,\"start\":26244},{\"end\":26268,\"start\":26258},{\"end\":26275,\"start\":26268},{\"end\":26286,\"start\":26275},{\"end\":26296,\"start\":26286},{\"end\":26739,\"start\":26726},{\"end\":26751,\"start\":26739},{\"end\":26761,\"start\":26751},{\"end\":26773,\"start\":26761},{\"end\":26788,\"start\":26773},{\"end\":26802,\"start\":26788},{\"end\":26814,\"start\":26802},{\"end\":26822,\"start\":26814},{\"end\":26834,\"start\":26822},{\"end\":26841,\"start\":26834}]", "bib_venue": "[{\"end\":20262,\"start\":20203},{\"end\":20622,\"start\":20610},{\"end\":20913,\"start\":20901},{\"end\":21211,\"start\":21194},{\"end\":21544,\"start\":21527},{\"end\":21834,\"start\":21822},{\"end\":22088,\"start\":22071},{\"end\":22306,\"start\":22247},{\"end\":22730,\"start\":22713},{\"end\":23034,\"start\":23017},{\"end\":23263,\"start\":23202},{\"end\":23568,\"start\":23491},{\"end\":23862,\"start\":23824},{\"end\":24108,\"start\":24098},{\"end\":24396,\"start\":24383},{\"end\":24750,\"start\":24733},{\"end\":25099,\"start\":25082},{\"end\":25511,\"start\":25436},{\"end\":25944,\"start\":25932},{\"end\":26329,\"start\":26296},{\"end\":26889,\"start\":26841},{\"end\":20262,\"start\":20203},{\"end\":20622,\"start\":20610},{\"end\":20913,\"start\":20901},{\"end\":21211,\"start\":21194},{\"end\":21544,\"start\":21527},{\"end\":21834,\"start\":21822},{\"end\":22088,\"start\":22071},{\"end\":22306,\"start\":22247},{\"end\":22730,\"start\":22713},{\"end\":23034,\"start\":23017},{\"end\":23263,\"start\":23202},{\"end\":23568,\"start\":23491},{\"end\":23862,\"start\":23824},{\"end\":24108,\"start\":24098},{\"end\":24396,\"start\":24383},{\"end\":24750,\"start\":24733},{\"end\":25099,\"start\":25082},{\"end\":25511,\"start\":25436},{\"end\":25944,\"start\":25932},{\"end\":26329,\"start\":26296},{\"end\":26889,\"start\":26841},{\"end\":20630,\"start\":20624},{\"end\":20921,\"start\":20915},{\"end\":21224,\"start\":21213},{\"end\":21557,\"start\":21546},{\"end\":21842,\"start\":21836},{\"end\":22101,\"start\":22090},{\"end\":22743,\"start\":22732},{\"end\":23047,\"start\":23036},{\"end\":24114,\"start\":24110},{\"end\":24405,\"start\":24398},{\"end\":24763,\"start\":24752},{\"end\":25112,\"start\":25101},{\"end\":25952,\"start\":25946},{\"end\":26349,\"start\":26331},{\"end\":20630,\"start\":20624},{\"end\":20921,\"start\":20915},{\"end\":21224,\"start\":21213},{\"end\":21557,\"start\":21546},{\"end\":21842,\"start\":21836},{\"end\":22101,\"start\":22090},{\"end\":22743,\"start\":22732},{\"end\":23047,\"start\":23036},{\"end\":24114,\"start\":24110},{\"end\":24405,\"start\":24398},{\"end\":24763,\"start\":24752},{\"end\":25112,\"start\":25101},{\"end\":25952,\"start\":25946},{\"end\":26349,\"start\":26331}]"}}}, "year": 2023, "month": 12, "day": 17}