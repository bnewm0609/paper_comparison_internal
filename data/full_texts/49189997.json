{"id": 49189997, "updated": "2023-09-30 20:56:09.896", "metadata": {"title": "BA-Net: Dense Bundle Adjustment Network", "authors": "[{\"first\":\"Chengzhou\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Ping\",\"last\":\"Tan\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2018, "month": 6, "day": 13}, "abstract": "This paper introduces a network architecture to solve the structure-from-motion (SfM) problem via feature-metric bundle adjustment (BA), which explicitly enforces multi-view geometry constraints in the form of feature-metric error. The whole pipeline is differentiable so that the network can learn suitable features that make the BA problem more tractable. Furthermore, this work introduces a novel depth parameterization to recover dense per-pixel depth. The network first generates several basis depth maps according to the input image and optimizes the final depth as a linear combination of these basis depth maps via feature-metric BA. The basis depth maps generator is also learned via end-to-end training. The whole system nicely combines domain knowledge (i.e. hard-coded multi-view geometry constraints) and deep learning (i.e. feature learning and basis depth maps learning) to address the challenging dense SfM problem. Experiments on large scale real data prove the success of the proposed method.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1806.04807", "mag": "2949115101", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/TangT19", "doi": null}}, "content": {"source": {"pdf_hash": "17caec6cd9128b0cbe0aa05c27ae18637c13389e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1806.04807v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "dda2ed1e1ea7e1aa287b32f338e36bd0763ed901", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/17caec6cd9128b0cbe0aa05c27ae18637c13389e.txt", "contents": "\nBA-NET: DENSE BUNDLE ADJUSTMENT NETWORKS\n\n\nChengzhou Tang chengzhou_tang@sfu.ca \nSchool of Computer Science\nSchool of Computer Science\nSimon Fraser University\nSimon Fraser University\n\n\nPing Tan pingtan@sfu.ca \nSchool of Computer Science\nSchool of Computer Science\nSimon Fraser University\nSimon Fraser University\n\n\nBA-NET: DENSE BUNDLE ADJUSTMENT NETWORKS\n\nThis paper introduces a network architecture to solve the structure-from-motion (SfM) problem via feature-metric bundle adjustment (BA), which explicitly enforces multi-view geometry constraints in the form of feature-metric error. The whole pipeline is differentiable, so that the network can learn suitable features that make the BA problem more tractable. Furthermore, this work introduces a novel depth parameterization to recover dense per-pixel depth. The network first generates several basis depth maps according to the input image, and optimizes the final depth as a linear combination of these basis depth maps via feature-metric BA. The basis depth maps generator is also learned via end-to-end training. The whole system nicely combines domain knowledge (i.e. hard-coded multi-view geometry constraints) and deep learning (i.e. feature learning and basis depth maps learning) to address the challenging dense SfM problem. Experiments on large scale real data prove the success of the proposed method. arXiv:1806.04807v2 [cs.CV] 20 Dec 2018 depth maps. The combination coefficients will be optimized in the BA-Layer together with camera motion. This novel parameterization guarantees a smooth depth map with good consistency with object boundaries. It also reduces the number of unknowns and makes dense BA possible in networks.Similar depth parameterization is introduced in a recent work, CodeSLAM (Bloesch et al., 2018).The major difference is that our method learns the basis depth map generator through the gradients back-propagated from the BA-Layer, while CodeSLAM learns the generator separately and uses its results for a standalone optimization component. Thus, our basis depth map generator has the chance to be better trained for the SfM problem. Furthermore, we use a different network structure to generate basis depth maps. CodeSLAM employs a variational auto-encoder (VAE), while we use a standard encoder-decoder. This design enables us to use the same backbone network for both feature learning and basis depth map learning, making joint training of the whole network possible.To demonstrate the effectiveness of our method, we evaluate on the ScanNet (Dai et al., 2017a) and KITTI (Geiger et al., 2012)  dataset. Our method outperforms DeMoN (Ummenhofer et al., 2017), LS-Net (Clark et al., 2018), as well as several conventional baselines. Due to page limit, we move the ablation studies, evaluation on DeMoN's dataset, multi-view SfM (up to 5 views), and comparison with CodeSLAM on the EuroC dataset(Burri et al., 2016)to the appendix.\n\nINTRODUCTION\n\nThe Structure-from-Motion (SfM) problem has been extensively studied in the past a few decades. Almost all conventional SfM algorithms Wu et al., 2011;Sch\u00f6nberger & Frahm, 2016;Engel et al., 2018;Delaunoy & Pollefeys, 2014) jointly optimize scene structures and camera motion via the Bundle-Adjustment (BA) algorithm (Triggs et al., 2000;Agarwal et al., 2010), which minimizes the geometric Wu et al., 2011;Sch\u00f6nberger & Frahm, 2016) or photometric (Engel et al., 2014;Delaunoy & Pollefeys, 2014) error through the Levenberg-Marquardt (LM) algorithm (Nocedal & Wright, 2006). Some recent works (Ummenhofer et al., 2017; attempt to solve SfM using deep learning techniques, but most of them do not enforce the geometric constraints between 3D structures and camera motion in their networks. For example, in the recent work DeMoN (Ummenhofer et al., 2017), the scene depths and the camera motion are estimated by two individual sub-network branches. This paper formulates BA as a differentiable layer, the BA-Layer, to bridge the gap between classic methods and recent deep learning based approaches. To this end, we learn a feed-forward multilayer perceptron (MLP) to predict the damping factor in the LM algorithm, which makes all involved computation differentiable. Furthermore, unlike conventional BA that minimizes geometric or photometric error, our BA-layer minimizes the distance between aligned CNN feature maps. Our novel feature-metric BA takes CNN features of multiple images as inputs and optimizes for the scene structures and camera motion. This feature-metric BA is desirable, because it has been observed by Engel et al. (2014; that the geometric BA does not exploit all image information, while the photometric BA is sensitive to moving objects, exposure or white balance changes, etc. Most importantly, our BA-Layer can back-propagate loss from scene structures and camera motion to learn appropriate features that are most suitable for structure-from-motion and bundle adjustment. In this way, our network hard-codes the multi-view geometry constraints in the BA-Layer and learns suitable feature representations from training data.\n\nWe strive to estimate a dense per-pixel depth, because dense depth is critical for many tasks such as object detection and robot navigation. A major challenge in solving dense per-pixel depth is to find a compact parameterization. Direct per-pixel depth is computational expensive, which makes the network training intractable. So we train a network to generate a set of basis depth maps for an arbitrary input image and represent the result depth map as a linear combination of these basis 2 RELATED WORK Monocular Depth Estimation Networks Estimating depth from a monocular image is an ill-posed problem because an infinite number of possible scenes may have produced the same image. Before the raise of deep learning based methods, some works predict depth from a single image based on MRF (Saxena et al., 2005;2009), semantic segmentation (Ladick\u00fd et al., 2014), or manually designed features (Hoiem et al., 2005). Eigen et al. (2014) propose a multi-scale approach for depth prediction with two CNNs, where a coarse-scale network first predicts the scene depth at the global level and then a fine-scale network will refine the local regions. This approach was extended in Eigen & Fergus (2015) to handle semantic segmentation and surface normal estimation as well. Recently, Laina et al. (2016) propose to use ResNet (He et al., 2016) based structure to predict depth, and Xu et al. (2017) construct multi-scale CRFs for depth prediction. In comparison, we exploit monocular image depth estimation network for depth parameterization, which only produces a set of basis depth maps and the final result will be further improved through optimization.\n\nStructure-from-Motion Networks Recently, some works exploit CNNs to resolve the SfM problem. Handa et al. (2016) solve the camera motion by a network from a pair of images with known depth.  employ two CNNs for depth and camera motion estimation respectively, where both CNNs are trained jointly by minimizing the photometric loss in an unsupervised manner.  implement the direct method (Steinbruecker et al., 2011) as a differentiable component to compute camera motion after scene depth is estimated by the method in . In Ummenhofer et al. (2017), the scene depth and the camera motion are predicted from optical flow features, which help to make it generalizing better to unseen data. However, the scene depth and the camera motion are solved by two separate network branches, multi-view geometry constraints between depth and motion are not enforced. Recently,  propose to solve nonlinear least squares in two-view SfM using a LSTM-RNN (Hochreiter et al., 2001) as the optimizer.\n\nOur method belongs to this category. Unlike all previous works, we propose the BA-Layer to simultaneously predict the scene depth and the camera motion from CNN features, which explicitly enforces multi-view geometry constraints. The hard-coded multi-view geometry constraints enable our method to reconstruct more than two images, while most deep learning methods can only handle two images. Furthermore, we propose to minimize a feature-metric error instead of the photometric error in  to enhance robustness.\n\n\nBUNDLE ADJUSTMENT REVISITED\n\nBefore introducing our BA-Net architecture, we revisit the classic BA to have a better understanding about where the difficulties are and why feature-metric BA and feature learning are desirable. We only introduce the most relevant content and refer the readers to Triggs et al. (2000) and Agarwal et al. (2010) for a comprehensive introduction. Given images I = {I i |i = 1 \u00b7 \u00b7 \u00b7 N i }, the geometric BA (Triggs et al., 2000;Agarwal et al., 2010) jointly optimizes camera poses T = {T i |i = 1 \u00b7 \u00b7 \u00b7 N i } and 3D scene point coordinates P = {p j |j = 1 \u00b7 \u00b7 \u00b7 N j } by minimizing the re-projection error:\nX = argmin Ni i=1 Nj j=1 e g i,j (X ) ,(1)\nwhere the geometric distance e g i,j (X ) = \u03c0(T i , p j ) \u2212 q i,j measures the difference between a projected scene point and its corresponding feature point. The function \u03c0 projects scene points to image space, q i,j = [x i,j , y i,j , 1] is the normalized homogeneous pixel coordinate, and X = [T 1 , T 2 \u00b7 \u00b7 \u00b7 T Ni , p 1 , p 2 \u00b7 \u00b7 \u00b7 p Nj ] contains all the points' and the cameras' parameters. The general strategy to minimize Equation (1) is the Levenberg-Marquardt (LM) (Nocedal & Wright, 2006;Lourakis & Argyros, 2005) algorithm. At each iteration, the LM algorithm solves for an optimal update \u2206X * to the solution by minimizing:\n\u2206X * = argmin J(X )\u2206X + E(X ) + \u03bb D(X )\u2206X .(2)\nHere, E(X ) = [e g 1,1 (X ), e g 1,2 (X ) \u00b7 \u00b7 \u00b7 e g Ni,Nj (X )], and J(X ) is the Jacobian matrix of E(X ) respect to X , D(X ) is a non-negative diagonal matrix, typically the square root of the diagonal of the approximated Hessian J(X ) J(X ). The non-negative value \u03bb controls the regularization strength. The special structure of J(X ) J(X ) motivates the use of Schur-Complement (Brown, 1958).\n\nThis geometric BA with re-projection error is the golden standard for structure-from-motion in the last two decades, but with two main drawbacks:\n\n\u2022 Only image information conforming to the respective feature types, typically image corners, blobs, or line segments, is utilized. \u2022 Features have to be matched to each other, which often result in a lot of outliers. Outlier rejection like RANSAC is necessary, which still cannot guarantee correct result. These two difficulties motivate the recent development of direct methods (Engel et al., 2014;Delaunoy & Pollefeys, 2014) which propose the photometric BA algorithm to eliminate feature matching and directly minimizes the photometric error (pixel intensity difference) of aligned pixels. The photometric error is defined as:\ne p i,j (X ) = I i (\u03c0(T i , d j \u00b7 q j )) \u2212 I 1 (q j ),(3)\nwhere d j \u2208 D = {d j |j = 1 \u00b7 \u00b7 \u00b7 N j } is the depth of a pixel q j at the image I 1 , and d j \u00b7 q j upgrade the pixel q j to its 3D coordinate. Thus, the optimization parameter is X = [T 1 , T 2 \u00b7 \u00b7 \u00b7 T Ni , d 1 , d 2 \u00b7 \u00b7 \u00b7 d Nj ] . The direct methods have the advantages of using all pixels with sufficient gradient magnitude. They have demonstrated superior performance, especially at less textured scenes. However, these methods also have some drawbacks:\n\n\u2022 They are sensitive to initialization as demonstrated in (Mur-Artal et al., 2015) and (Tang et al., 2017) because the photometric error increases the non-convexity (Engel et al., 2018). \u2022 They are sensitive to camera exposure and white balance changes. An automatic photometric calibration is required (Engel et al., 2018;. \u2022 They are more sensitive to outliers such as moving objects.\n\n\nTHE BA-NET ARCHITECTURE\n\nTo deal with the above challenges, we propose a feature-metric BA algorithm which estimates the same scene depth and camera motion parameters X as in photometric BA, but minimizes the feature-metric difference of aligned pixels:\ne f i,j (X ) = F i (\u03c0(T i , d j \u00b7 q j )) \u2212 F 1 (q j ),(4)\nwhere F = {F i |i = 1 \u00b7 \u00b7 \u00b7 N i } are feature pyramids of images I = {I i |i = 1 \u00b7 \u00b7 \u00b7 N i }. Similar to the photometric BA, our feature-metric BA considers more pixels than corners or blobs. It has the potential to learn more suitable features for SfM to deal with exposure changes, moving objects, etc.  Figure 1: Overview of our BA-Net structure, which consists of a DRN-54 (Yu et al., 2017) as the backbone network, a Basis Depth Maps Generator that generates a set of basis depth maps, a Feature Pyramid Constructor that constructs multi-scale feature maps, and a BA-Layer that optimizes both the depth map and the camera poses through a novel differentiable LM algorithm.\nF 1 F N i { { { { w T + Dense Structure Camera Motion D\nWe learn features suitable for SfM via back-propagation, instead of using pre-trained CNN features for image classification (Czarnowski et al., 2017). Therefore, it is crucial to design a differentiable optimization layer, our BA-Layer, to solve the optimization problem, so that the loss information can be back-propagated. The BA-Layer predicts the camera poses T and the dense depth map D during forward pass and back-propagates the loss from T and D to the feature pyramids F for training.\n\n\nOVERVIEW\n\nAs illustrated in Figure 1, our BA-Net receives multiple images and then feed them to the backbone DRN-54. We use DRN-54 (Yu et al., 2017) because it replaces max-pooling with convolution layers and generates smoother feature maps, which is desirable for BA optimization. Note the original DRN is memory inefficient due to the high resolution feature maps after dilation convolutions. We replace the dilation convolution with ordinary convolution with strides to address this issue. After DRN-54, a feature pyramid is then constructed for each input image, which are the inputs for the BA-Layer.\n\nAt the same time, the basis depth maps generator generates multiple basis depth maps for the image I 1 , and the final depth map is represented as a linear combination of these basis depth maps.\n\nFinally, the BA-Layer optimizes for the camera poses and the dense depth map jointly by minimizing the feature-metric error defined in Equation (4), which makes the whole pipeline end-to-end trainable.\n\n\nFEATURE PYRAMID\n\nThe feature pyramid learns suitable features for the BA-Layer. Similar to the feature pyramid networks (FPN) for object detection (Lin et al., 2017), we exploit the inherent multi-scale hierarchy of deep convolutional networks to construct feature pyramids. A top-down architecture with lateral connections is applied to propagate richer context information from coarser scales to finer scales. Thus, our feature-metric BA will have a larger convergence radius. As shown in Figure 2(a), we construct a feature pyramid from the backbone DRN-54. We denote the last residual blocks of conv1, conv2, conv3, conv4 in DRN-54 as {C 1 , C 2 , C 3 , C 4 }, with strides {1, 2, 4, 8} respectively. We upsample a feature map C k+1 by a factor of 2 with bilinear interpolation and concatenate the upsampled feature map with C k in the next level. This procedure is iterated until the finest level. Finally, we apply a 3 \u00d7 3 convolution on the concatenated feature maps to reduce its dimensionality to 128 to balance the expressiveness and computational complexity, which leads to the final feature pyramid\n+ + + 2X Upsampling 2X Upsampling 3X3 Convolution 128 3X3 Convolution 128 RGB F 1 F 2 F 3 C 3 C 4 C 2 C 1F i = [F 1 i , F 2 i , F 3 i ] for image I i .\nWe visualize some typical channels from the raw image I (i.e. the RGB channels), the pre-trained DRN-54 C 3 and our learned F 3 in Figure 2(b). It is evident that, after training with our BA-Layer, the feature pyramid becomes smoother and each channel correspondences to different regions in the image. Note that our feature pyramids have higher resolution than FPN to facilitate precise alignment.\n\nTo have a better intuition about how much the BA optimization benefits from our learned features, we visualize different distances in Figure 3. We evaluate the distance between a pixel marked by a yellow cross in the top image in Figure 3 (a) and all pixels in a neighbourhood of its corresponding point in the bottom image of Figure 3 (a). The distances evaluated from raw RGB values, pretrained feature C 3 , and our learned feature F 3 are visualized in (b), (c), and (d) respectively. All distances are normalized to [0, 1] and visualized as heat maps. The x-axis and y-axis are the offsets to the ground-truth corresponding point. The RGB distance in (b) (i.e. e p in Equation (3)) has no clear global minimum, which makes the photometric BA sensitive to initialization (Engel et al., 2014;. The distance measured by the pretrained feature C 3 has both global and local minimums. Finally, the distance measured by our learned feature F 3 has a clear global minimum and smooth basin, which is helpful in gradient based optimization such as the LM algorithm.\n\n\nBUNDLE ADJUSTMENT LAYER\n\nAfter building feature pyramids for all images, we optimize camera poses and a dense depth map by minimizing the feature-metric error in Equation (4). Following the conventional Bundle Adjustment principle, we optimize Equation (4) using the Levenberg-Marquardt (LM) algorithm. However, the original LM algorithm is non-differentiable because of two difficulties:\n\n\u2022 The iterative computation terminates when a specified convergence threshold is reached.\n\nThis if-else based termination strategy makes the output solution X non-differentiable with respect to the input F (Domke, 2012). \u2022 In each iteration, it updates the damping factor \u03bb based on the current value of the objective function. It raises \u03bb if a step fails to reduce the objective; otherwise it reduces \u03bb. This if-else decision also makes X non-differentiable with respect to F.\n\nWhen the solution X is non-differentiable with respect to F, feature learning by back-propagation becomes impossible. The first difficulty has been studied in Domke (2012) and the author proposes to fix the number of iterations, which is refered as 'incomplete optimization'. Besides making the optimization differentiable, this 'incomplete optimization' technique also reduces memory consumption because the number of iterations is usually fixed at a small value.\n\nThe second difficulty has never been studied. Previous works mainly focus on gradient descent (Domke, 2012) or quadratic minimization (Amos & Kolter, 2017;Schmidt & Roth, 2014). In this section, we propose a simple yet effective approach to soften the if-else decision and yields a differentiable LM algorithm. We send the current objective value to a MLP network to predict \u03bb. This technique not only makes the optimization differentiable, but also learns to predict a better damping factor \u03bb, which helps the optimization to reach a better solution within limited iterations.\n\nTo start with, we illustrate a single iteration of the LM optimization as a diagram in Figure 4 by interpreting intermediate variables as network nodes. During the forward pass, we compute the solution update \u2206X from feature pyramids F and current solution X as the following steps:\n\n\u2022 We compute the feature-metric error E(X ) = [e f 1,1 (X ), e f 1,2 (X ) \u00b7 \u00b7 \u00b7 e f Ni,Nj (X )] with Equation (4) on all N i images and N j pixels, where X is the solution from the previous iteration;\n\n\u2022 We then compute the Jacobian matrix J(X ), the Hessian matrix J(X ) J(X ) and its diagonal matrix D(X ); \u2022 To predict the damping factor \u03bb, we use global average pooling to aggregate the aboslute value of E(X ) over all pixels for each feature channel, and get a 128D feature vector. We then send it to a MLP sub-network to predict \u03bb; \u2022 Finally, the update \u2206X to the current solution is computed as a standard LM step:\n\u2206X = (J(X ) J(X ) + \u03bbD(X )) \u22121 J(X ) E(X ).(5)\nIn this way, we can consider \u03bb as an intermediate variable and denote each LM step as a function g about features pyramids F and the solution X from the previous iteration. In other words, \u2206X = g(X ; F). Therefore, the solution after the k-th iteration is:\nX k = g(X k\u22121 ; F) \u2022 X k\u22121 .(6)\nHere, \u2022 denotes parameters updating, which is addition for depth and SE(3) exponential mapping for camera poses. Equation (6) is differentiable with respect to the feature pyramids F, which makes back-propagation possible through the whole pipeline for feature learning. The MLP that predicts \u03bb is also shown in Figure 4. We stack four fully-connected layers to predict \u03bb from the input 128D vector. We use ReLU as the activation function to guarantee \u03bb is non-negative. Following the photometric BA (Engel et al., 2014;, we solve our feature-metric BA using a coarse-to-fine strategy with feature map warping at each iteration. We apply the differentiable LM algorithm for 5 iterations at each pyramid level, leading to 15 iterations in total. All the camera poses are initialized with identity rotation and zero translation, and the initialization of depth map will be introduced in Section 4.4.\n\n\nBASIS DEPTH MAPS GENERATION\n\nParameterizing a dense depth map by a per-pixel depth value is impractical under our formulation. Firstly, it introduces too many parameters for optimization. For example, an image of 320 \u00d7 240 pixels results in 76.8k parameters. Secondly, in the beginning of training, many pixels will become invisible in the other views because of the poorly predicted depth or motion. So little information can be back-propagated to improve the network, which makes training difficult.\n\nTo deal with these problems, we use the convolutional network for monocular image depth estimation as a compact parameterization, rather than using it as an initialization as in Tateno et al. (2017) and Yang et al. (2018). We use a standard encoder-decoder architecture for monocular depth learning as in Laina et al. (2016). We use DRN-54 as the encoder to share the same backbone features with our feature pyramids. For the decoder, we modify the last convolutional feature maps of Laina et al. (2016) to 128 channels and use these feature maps as the basis depth maps for optimization. The final depth map is generated as the linear combination of these basis depth maps, which is:\nD = ReLU(w B).(7)\nHere, D is the h \u00b7 w depth map that contains depth values for all pixels, B is a 128 \u00d7 h \u00b7 w matrix, representing 128 basis depth maps generated from network, w is the linear combination weights of these basis depth maps. The w will be optimized in our BA-Layer. The ReLU activation function guarantees the final depth is non-negative. Once B is generated from the network, we fix B and use w as a compact depth parameterization in BA optimization, and the feature-metric distance becomes:\ne f i,j (X ) = F i (\u03c0(T i , ReLU(w B[j]) \u00b7 q j )) \u2212 F 1 (q j ),(8)\nwhere B[j] is the j-th column of B, and ReLU(w B[j]) is the corresponding depth of q j . To further speedup convergence, we learn the initial weight w 0 as a 1D convolution filter for an arbitrary image, i.e. D 0 = ReLU(w 0 B). The B of various images are visualized in the appendix.\n\n\nTRAINING\n\nThe BA-Net learns the feature pyramid, the damping factor predictor, and the basis depth maps generator in a supervised manner. We apply the following commonly used loss for training, though more sophisticated ones might be designed.\n\n\nCamera Pose Loss\n\nThe camera rotation loss is the distance between rotation quaternion vectors L rotation = q \u2212 q * . Similarly, translation loss is the Euclidean distance between prediction and groundtruth in metric scale, L translation = t \u2212 t * .\n\nDepth Map Loss For each dense depth map we applies the berHu Loss (Zwald & Lambert-Lacroix, 2012) as in Laina et al. (2016).\n\nWe initialize the back-bone network from DRN-54 (Yu et al., 2017), and the other components are trained with ADAM (Kingma & Ba, 2015) from scratch with initial learning rate 0.001, and the learning rate is divided by two when we observe plateaus from the Tensorboard interface.\n\n\nEVALUATION\n\n\nDATASET\n\nScanNet ScanNet (Dai et al., 2017a) is a large-scale indoor dataset with 1,513 sequences in 706 different scenes. Camera poses and depth maps are not perfect, because they are estimated via BundleFusion (Dai et al., 2017b). The metric scale is known in all data from ScanNet, because the data are recorded with a depth camera which returns absolute depth values.\n\nTo sample image pairs for training, we apply a simple filtering process. We first filter out pairs with a large photo-consistency error, to avoid image pairs with large pose or depth error. We also filter out image pairs, if less than 50% of the pixels from one image are visible in the other image. In addition, we also discard a pair if their roundness score (Beder & Steffen, 2006) is less than 0.001, which avoids pairs with too narrow baselines.\n\nWe split the whole dataset into the training and the testing sets. The training set contains the first 1,413 sequences and the testing set contains the rest 100 sequences. We sample 547,991 training pairs and 2,000 testing pairs from the training and testing sequences respectively.\n\nKITTI KITTI (Geiger et al., 2012) is a widely used benchmark dataset collected by car-mounted cameras and a LIDAR sensor on streets. It contains 61 scenes belonging to the \"city\", \"residential\", or \"road\" categories. Eigen et al. (2014) select 28 scenes for testing and 28 scenes from the remaining for training. We use the same data split, to make a fair comparison with previous methods. Since ground truth pose is unavailable from the raw KITTI dataset, we compute camera poses by LibVISO2 (Geiger et al., 2011) and take them as ground truth after discarding poses with large errors.\n\n\nCOMPARISONS WITH OTHER METHODS\n\nScanNet To evaluate the results' quality, we use the depth error metrics suggested in Eigen & Fergus (2015), where RMSE (linear, log, and log, scale inv.) measure the RMSE of the raw, the logarithmical, and aligned logarithmical depth values, while the other two metrics measure the mean of the ratios that divide the absolute and square error by groundtruth depth.. The errors in camera poses are measured by the rotation error (the angle between the ground truth and the estimated camera rotations), the translation direction error (the angle between the ground truth and estimated camera translation directions) and the absolute position error (the distance between the ground truth and the estimated camera translation vectors).\n\nIn Table 1, we compare our method with DeMoN (Ummenhofer et al., 2017) and the conventional photometric and geometric BA. Note that we cannot get DeMoN trained on the ScanNet. For fair comparison, we train our network on the same training data as DeMoN and test both networks on our testing data 1 . We also show the results of our network trained on ScanNet. Our BA-Net consistently performs better than DeMoN no matter which training data is used. Since DeMoN does not recover the absolute scale, we align its depth map with the groundtruth to recover its metric scale for evaluation. We further compare with conventional geometric (Nister, 2004; and photometric (Engel et al., 2014) BA. Again, our method produces better results. The geometric BA works poorly here, because feature matching is difficult in indoor scenes. Even the RANSAC process cannot get rid of all outliers. While for photometirc BA, the highly non-convex objective function is difficult to optimize as described in Section 3.\n\nKITTI We use the same metrics as the comparisons on ScanNet for depth evaluation. To evaluate the camera poses, we follow  to use the Absolute Trajectory Error (ATE), which measures the Euclidean differences between two trajectories (Steinbruecker et al., 2011), on the 9th and 10th sequences from the KITTI odometry data. In this experiment, we create short sequences of 5 frames by first computing 5 two-view reconstructions from our BA-Net and then align the two-view reconstructions in the coordinate system anchored at the first frame. minimize the photometric error.   Godard et al., 2017) methods.  (Eigen et al., 2014) as well as recent unsupervised methods Godard et al., 2017). Our method also achieves more accurate camera trajectories than  and . We believe this is due to our feature-metric BA with features learned specifically for SfM problem, which makes the objective function closer to convex and easier to optimize as discussed in Section 4.2. In comparison,  and  minimize the photometric error.\n\nMore comparison with DeMoN, ablation studies, and multi-view SfM (up to 5 views) are reported in the appendix due to page limit.\n\n\nCONCLUSIONS AND FUTURE WORKS\n\nThis paper presents the BA-Net, a network that explicitly enforces multi-view geometry constraints in terms of feature-metric error. It optimizes scene depths and camera motion jointly via feature-metric bundle adjustment. The whole pipeline is differentiable and thus end-to-end trainable, such that the features are learned from data to facilitate structure-from-motion. The dense depth is parameterized as a linear combination of several basis depth maps generated from the network. Our BA-Net nicely combines domain knowledge (hard-coded multi-view geometry constraint) with deep learning (learned feature representation and basis depth maps generator). It outperforms conventional BA and recent deep learning based methods. \nC 1 C 2 C 3 C 4 C 5 C 6\n\n2X Stride Shortcut Convolution\n\n(a) Architecture of the DRN-54 backbone  Network Architecture Details Figure 5 illustrates the detailed network architectures for the backbone DRN-54 and the depth basis generator. The architecture of the feature pyramid has been provided in Figure 2(a). We modify the dilated convolution of the original DRN-54 to convolution with strides and discard the conv7 and conv8 as shown in Figure 5(a). C 1 to C 6 are layers with {1,2,4,8,16,32} strides and {16,32,256,512,1024,2048} channels, where C 1 and C 2 are basic convolution layers, while C 3 to C 6 are standerd bottleneck blocks as in ResNet (He et al., 2016). Figure 5(b) visualizes our depth basis generator which adopts the up-projection structure proposed in Laina et al. (2016). The depth basis generator is a stander decoder that takes the output of C 6 as input and stacks five up-projection blocks to generate 128 basis depth maps, and each of the basis depth maps is half the resolution of the input image. The up-projection block is shown on the right of Figure 5(b) which upsample the input by 2\u00d7 and then apply convolutions with projection connection.\n\nEvaluation Time To evaluate the running time of our method, we use the Tensorflow profiler tool to retrieve the time in ms for all network nodes and then summarize the results corresponding to each component in our pipeline. As shown in Table 3, our method takes 95.21 ms to reconstruct two 320 \u00d7 240 images, which is slightly faster than DeMoN that takes 110 ms for two 256 \u00d7 192 images.\n\nThe current computation bottleneck is the BA-Layer which contains a large amount of matrix operations and can be further speeded up by direct CUDA implementation. Since we explicitly hard-code the multi-view geometry constraints in the BA-Layer, it is possible to share the backbone DRN-54 with other high-level vision tasks, such as semantic segmentation and object detection, to maximize reuse of network structures and minimize extra computation cost.    Table 4, the pre-trained features (i.e. w/o Feature Learning) produce larger error. This proves the discussion in Section 4.2.\n\n\nBundle Adjustment Optimization vs SE(3) Pose Estimation\n\nOur BA-Layer optimizes depth and camera poses jointly. We compare it to the SE(3) camera pose estimation with fixed depth map (e.g. the initialized depth D 0 in Section 4.4), and similar strategy is adopted in . To make a fair comparison, we also use our learned feature pyramids for the SE(3) camera pose estimation. As shown in Table 4, without BA optimization (i.e. w/o Joint Optimization), both the depth maps and camera poses are worse, because the errors in the depth estimation will degrades the camera pose estimation.\n\nDifferentiable Levenberg-Marquardt vs Gauss-Newton To make the whole pipeline end-to-end trainable, we makes the Levenberg-Marquardt algorithm differentiable by learning the damping factor from the network. We first compare our method against vanilla Gauss-Newton without damping factor \u03bb (i.e. \u03bb = 0). Since the objective function of feature-metric BA is non-convex, the Hessian matrix J(X ) J(X ) might not be positive definite, which makes the matrix inversion by Cholesky decomposition fail.\n\nTo deal with this problem, we use QR decomposition instead for training with Gauss-Newton. As shown in Table 4, the Gauss-Newton algorithm (i.e. w/o \u03bb) generates much larger error, because the BA optimization is non-convex and the Gauss-Newton algorithm has no guaranteed convergence unless the initial solution is sufficiently close to the optimal (Nocedal & Wright, 2006). This comparison reveals that, similar to conventional BA, our differnetiable Levenberg-Marquardt algorithm is superior than the Gauss-Newton algorithm for feature-metric BA.  Predicted vs Constant \u03bb Another way to make the Levenberg-Marquardt algorithm differentiable is to fix the \u03bb during the iterations. We compare with this strategy. As shown in Figure 6(a), increasing \u03bb makes the both rotation and translation error decreases, until \u03bb = 0.5, and then increases. The reason is that a small \u03bb makes the algorithm close to the Gauss-Newton algorithm, which has convergence issues. A large \u03bb leads to a small update at each iteration, which makes it difficult to reach a good solution within limited iterations.\n\nWhile in Figure 6(b), increasing \u03bb always makes depth errors decrease, probably because a larger \u03bb leads to a small update and makes the final depth close to the initialed depth, which is better than the optimized one with small constant \u03bb.\n\nUsing constant \u03bb value consistently generates worse results than using a predicted \u03bb from the MLP network, because there is no optimal \u03bb for all data and it should be adapted to different data and different iterations. We draw the errors of our method in Figure 6(a) and Figure 6(b) as the flat dash lines for a reference.\n\nAPPENDIX C: EVALUATION ON DEMON DATASET Table 5 summarizes our results on the DeMoN dataset. For a comparison, we also cite the results from DeMoN (Ummenhofer et al., 2017) and the most recent work LS-Net . We further cite the results from some conventional approaches as reported in DeMoN, indicated as Oracle, SIFT, FF, and Matlab respectively. Here, Oracle uses ground truth camera poses to solve the multi-view stereo by SGM (Hirschmuller, 2005), while SIFT, FF, and Matlab further use sparse features, optical flow, and KLT tracking respectively for feature correspondence to solve camera poses by the 8-pt algorithm (Hartley, 1997  Our method consistently outperforms DeMoN (Ummenhofer et al., 2017) at both camera motion and scene depth, except on the 'Scenes11' data, because we enforce multi-view geometry constraint in the BA-Layer. Our results are poorer on the 'Scene11' dataset, because the images there are synthesized with random objects from the ShapeNet (Chang et al., 2015) without physically correct scale. This setting is inconsistent with real data and makes it harder for our method to learn the basis depth map generator.\n\nWhen compared with LS-Net , our method achieves similar accuracy on camera poses but better scene depth. It proves our feature-metric BA with learned feature is superior than the photometric BA in the LS-Net.\n\n\nAPPENDIX D: MULTI-VIEW STRUCTURE-FROM-MOTION\n\nOur method can be easily extended to reconstruct multiple images. We evaluate our method in the multi-view setting on the ScanNet (Dai et al., 2017a) dataset. To sample multi-view images for training, we randomly select two-view image pairs that shares a common image to construct N -view sequences. Due to the limited GPU memory (12G), we limit N to 5.\n\nAs shown in the Table 6, the accuracy is consistently improved when more views are included, which demonstrates the strength of the multi-view geometry constraints. Instead, most existing deep learning approaches can only handle two views at a time, which is sub-optimal as known in structure-from-motion literature.  \n\n\nAPPENDIX E: QUANTITATIVE COMPARISONS WITH CODESLAM\n\nWe compare our method with CodeSLAM  which adopts similar idea for depth parameterization. But the difference is that CodeSLAM learns the conditioned depth auto-encoder separately and uses the depth codes in a standalone photometric BA component, while our method learns the feature pyramid and basis depth maps generator through feature-metric BA end-to-end. Since there is no public code for CodeSLAM, we directly cite the results from their paper. 2 To get the trajectory on the EuroC MH02 sequence of our method, we select one frame every four frames and concatenate the reconstructed groups that contains every five selected frames. Then we use the same evaluation metrics as in CodeSLAM, which measures the translation errors correspond to different traveled distances. As shown in Figure 7, our method outperforms CodeSLAM. Our median error is less than the half of CodeSLAM's error, i.e. CodeSLAM exhibits an error of roughly 1 m for a traveled distance of 9 m, while our method's error is about 0.4 m. This comparison demonstrates the superiority of end-to-end learning with feature pyramid and feature-metric BA over learning depth parameterization only.\n\n\nAPPENDIX F: VISUALIZATION OF BASIS DEPTH MAPS\n\nIn Figure 8, we visualize four typical basis depth maps as heat maps for each of the four images. An interesting observation is that one basis depth map has higher responses on close objects while another oppositely has higher responses to the far background. Some other basis depth maps have smoothly varying responses and correspond to the layouts of scenes. This observation reveals that our learned basis depth maps have captured the latent structures of scenes. \n\n\nAPPENDIX G: QUALITATIVE COMPARISONS WITH OTHER METHODS\n\nFinally, we show some qualitative comparison with the previous methods. Figure 9 shows the recovered depth map by our method and DeMoN Ummenhofer et al. (2017) on the ScanNet data. As we can see from the regions highlighted with a red circle, our method recovers more shape details. This is consistent with the quantitative results in Table 1. Figure 11 shows the recovered depth maps by our method, , and Godard et al. (2017) respectively. Similarly, we observe more shape details in our results, as reflected in the quantitative results in Table 2.  \n\nFigure 2 :Figure 3 :\n23A feature pyramid and some typical channels from different feature maps. Feature distance maps defined over raw RGB values, pretrained CNN features C 3 , or our learned features F 3 . Our features produce smoother objective function to facilitate optimization.\n\nFigure 4 :\n4A single iteration of the differentiable LM.\n\nFigure 5 :\n5Network details for the (a) the DRN-54 backbone and (b) the basis depth generator.\n\n\nDiff (Constant ) Abs Rel Diff (Predicted ) RMSE Linear (Constant ) RMSE Linear (Predicted )(b) Depth errors.\n\nFigure 6 :\n6The camera pose and the depth errors correspond to different constant \u03bb values.\n\nFigure 7 :\n7Quantitative Comparisons with CodeSLAM on EuroC MH02. The error bars represent the maximum and the minimum errors. The orange and the blue boxes represent the median errors for CodeSLAM and our method.\n\nFigure 8 :\n8Visualization of different basis depth maps.\n\nFigure 9 :Figure 10 :\n910Qualitative Comparisons with DeMoN (Ummenhofer et al., 2017) on ScanNet. Qualitative Comparisons with DeMoN (Ummenhofer et al., 2017) on its dataset.\n\nFigure 11 :\n11Qualitative Comparisons with andGodard et al. (2017).\n\n\nTable 1: Quantitative comparisons with DeMoN and classic BA. The superindex * denotes that the model is trained on the trainning set described inUmmenhofer et al. (2017).Ours \n\nOurs  *  DeMoN  *  Photometric BA Geometric BA \nRotation (degree) \n1.018 \n1.587 \n3.791 \n4.409 \n8.56 \nTranslation (cm) \n3.39 \n10.81 \n15.5 \n21.40 \n36.995 \nTranslation (degree) \n20.577 31.005 \n31.626 \n34.36 \n39.392 \nabs relative difference \n0.161 \n0.238 \n0.231 \n0.268 \n0.382 \nsqr relative difference \n0.092 \n0.176 \n0.520 \n0.427 \n1.163 \nRMSE (linear) \n0.346 \n0.488 \n0.761 \n0.788 \n0.876 \nRMSE (log) \n0.214 \n0.279 \n0.289 \n0.330 \n0.366 \nRMSE (log, scale inv.) \n0.184 \n0.276 \n0.284 \n0.323 \n0.357 \n\n\n\nTable 2 :\n2Quantitative comparisons on KITTI with supervised and(Eigen et al., 2014) unsupervised  \n\nTable 2\n2summarizes our results on KITTI. Our method outperforms the supervised methods\n\nTable 3 :\n3Evaluation time for each component, which is summarized using Tensorflow profiler.APPENDIX B: ABLATION STUDIES \n\nLearned Features vs Pre-trained Features Our learned feature pyramid improves the convexity of \nthe objective function to facilitate the optimization. We compare our learned features with features \n\n\nTable 4 :\n4Ablation Study Comparisons by Disabling Different Components of BA-Net pre-trained on ImageNet for classification tasks. As shown in\n\n) .\n)DepthMotion Method L1-inv sc-inv L1-rel Rotation TranslationDepthMotion Method L1-inv sc-inv L1-rel Rotation Translation L1-inv sc-inv L1-rel Rotation TranslationDepthMotion Method L1-inv sc-inv L1-rel Rotation TranslationMVS \n\nOracle \n0.019 0.197 0.105 \n0 \n0 \nSIFT \n0.056 0.309 0.361 \n21.180 \n60.516 \nFF \n0.055 0.308 0.322 \n4.834 \n17.252 \nMatlab \n-\n-\n-\n10.843 \n32.736 \nDeMoN 0.047 0.202 0.305 \n5.156 \n14.447 \nLS-Net \n0.051 0.221 0.311 \n4.653 \n11.221 \nOurs \n0.03 \n0.15 \n0.08 \n3.499 \n11.238 \n\nScenes11 \n\nOracle \n0.023 0.618 0.349 \n0 \n0 \nSIFT \n0.051 0.900 1.027 \n6.179 \n56.650 \nFF \n0.038 0.793 0.776 \n1.309 \n19.425 \nMatlab \n-\n-\n-\n0.917 \n14.639 \nDeMoN 0.019 0.315 0.248 \n0.809 \n8.918 \nLS-Net \n0.010 0.410 0.210 \n0.910 \n8.21 \nOurs \n0.08 \n0.21 \n0.13 \n1.298 \n10.37 \nDepth \nMotion \nMethod RGB-D \n\nOracle \n0.026 0.398 0.336 \n0 \n0 \nSIFT \n0.050 0.577 0.703 \n12.010 \n56.021 \nFF \n0.045 0.548 0.613 \n4.709 \n46.058 \nMatlab \n-\n-\n-\n12.831 \n49.612 \nDeMoN 0.028 0.130 0.212 \n2.641 \n20.585 \nLS-Net \n0.019 \n0.09 \n0.301 \n1.01 \n22.1 \nOurs \n0.008 0.087 \n0.05 \n2.459 \n14.90 \n\nSun3D \n\nOracle \n0.020 0.241 0.220 \n0 \n0 \nSIFT \n0.029 0.290 0.286 \n7.702 \n41.825 \nFF \n0.029 0.284 0.297 \n3.681 \n33.301 \nMatlab \n-\n-\n-\n5.920 \n32.298 \nDeMoN 0.019 0.114 0.172 \n1.801 \n18.811 \nLS-Net \n0.015 0.189 0.650 \n1.521 \n14.347 \nOurs \n0.015 \n0.11 \n0.06 \n1.729 \n13.26 \n\n\n\nTable 5 :\n5Quantitative comparisons on the DeMoN dataset.\n\nTable 6 :\n6Quantitative comparisons on multi-view reconstruction on ScanNet.\nMore comparison with DeMoN on DeMoN's data is provided in the appendix.\nWe thank the authors of CodeSLAM to share their source file of the figure in their paper.\n\n. Sameer Agarwal, Keir Mierle, Others Ceres, Sameer Agarwal, Keir Mierle, and Others. Ceres solver. http://ceres-solver.org.\n\nBundle adjustment in the large. Sameer Agarwal, Noah Snavely, Steven M Seitz, Richard Szeliski, European Conference on Computer Vision (ECCV). Sameer Agarwal, Noah Snavely, Steven M. Seitz, and Richard Szeliski. Bundle adjustment in the large. In European Conference on Computer Vision (ECCV), pp. 29-42, 2010.\n\nBuilding rome in a day. Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M Seitz, Richard Szeliski, Commun. ACM. 54Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M. Seitz, and Richard Szeliski. Building rome in a day. Commun. ACM, 54:105-112, 2011.\n\nOptNet: Differentiable optimization as a layer in neural networks. Brandon Amos, J. Zico Kolter, International Conference on Machine Learning (ICML). 70Brandon Amos and J. Zico Kolter. OptNet: Differentiable optimization as a layer in neural networks. In International Conference on Machine Learning (ICML), volume 70, pp. 136-145, 2017.\n\nDetermining an initial image pair for fixing the scale of a 3d reconstruction from an image sequence. Christian Beder, Richard Steffen, Pattern Recognition. Christian Beder and Richard Steffen. Determining an initial image pair for fixing the scale of a 3d reconstruction from an image sequence. In Pattern Recognition, pp. 657-666, 2006.\n\nCodeslam -learning a compact, optimisable representation for dense visual slam. Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, Andrew J Davison, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, and Andrew J. Davison. Codeslam -learning a compact, optimisable representation for dense visual slam. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n\nA Solution to the General Problem of Multiple Station Analytical Stereo triangulation. D C Brown, D. Brown Associates, IncorporatedD.C. Brown. A Solution to the General Problem of Multiple Station Analytical Stereo triangulation. D. Brown Associates, Incorporated, 1958.\n\nEuroc micro aerial vehicle datasets. Michael Burri, Janosch Nikolic, Pascal Gohl, Thomas Schneider, Joern Rehder, Sammy Omari, Markus W Achtelik, Roland Siegwart, International Journal of Robotics Research. 35Michael Burri, Janosch Nikolic, Pascal Gohl, Thomas Schneider, Joern Rehder, Sammy Omari, Markus W Achtelik, and Roland Siegwart. Euroc micro aerial vehicle datasets. International Journal of Robotics Research, 35, 2016.\n\nShapenet: An information-rich 3d model repository. Angel X Chang, Thomas A Funkhouser, Leonidas J Guibas, Pat Hanrahan, Qi-Xing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu, abs/1512.03012CoRRAngel X. Chang, Thomas A. Funkhouser, Leonidas J. Guibas, Pat Hanrahan, Qi-Xing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. Shapenet: An information-rich 3d model repository. CoRR, abs/1512.03012, 2015.\n\nLearning to solve nonlinear least squares for monocular stereo. Ronald Clark, Michael Bloesch, Jan Czarnowski, Stefan Leutenegger, Andrew J Davison, European Conference on Computer Vision (ECCV). Ronald Clark, Michael Bloesch, Jan Czarnowski, Stefan Leutenegger, and Andrew J. Davison. Learning to solve nonlinear least squares for monocular stereo. In European Conference on Computer Vision (ECCV), 2018.\n\nSemantic texture for robust dense tracking. J Czarnowski, S Leutenegger, A J Davison, IEEE International Conference on Computer Vision Workshops (ICCVW). J. Czarnowski, S. Leutenegger, and A. J. Davison. Semantic texture for robust dense tracking. In IEEE International Conference on Computer Vision Workshops (ICCVW), pp. 851-859, 2017.\n\nScannet: Richlyannotated 3d reconstructions of indoor scenes. A Dai, A X Chang, M Savva, M Halber, T Funkhouser, M Nie\u00dfner, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nie\u00dfner. Scannet: Richly- annotated 3d reconstructions of indoor scenes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2432-2443, 2017a.\n\nBundlefusion: Real-time globally consistent 3d reconstruction using on-the-fly surface reintegration. Angela Dai, Matthias Niessner, Michael Zollh\u00f6fer, Shahram Izadi, Christian Theobalt, ACM Transactions on Graphics. 36Angela Dai, Matthias Niessner, Michael Zollh\u00f6fer, Shahram Izadi, and Christian Theobalt. Bundlefu- sion: Real-time globally consistent 3d reconstruction using on-the-fly surface reintegration. ACM Transactions on Graphics, 36, 2017b.\n\nPhotometric bundle adjustment for dense multi-view 3d modeling. A Delaunoy, M Pollefeys, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). A. Delaunoy and M. Pollefeys. Photometric bundle adjustment for dense multi-view 3d modeling. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1486-1493, 2014.\n\nGeneric methods for optimization-based modeling. Justin Domke, AISTATS. Justin Domke. Generic methods for optimization-based modeling. In AISTATS, 2012.\n\nPredicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. D Eigen, R Fergus, IEEE International Conference on Computer Vision (ICCV). D. Eigen and R. Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In IEEE International Conference on Computer Vision (ICCV), pp. 2650-2658, 2015.\n\nDepth map prediction from a single image using a multi-scale deep network. David Eigen, Christian Puhrsch, Rob Fergus, International Conference on Neural Information Processing Systems (NIPS). David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. In International Conference on Neural Information Processing Systems (NIPS), pp. 2366-2374, 2014.\n\nDirect sparse odometry. J Engel, V Koltun, D Cremers, IEEE Transactions on Pattern Analysis and Machine Intelligence. 40J. Engel, V. Koltun, and D. Cremers. Direct sparse odometry. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40:611-625, 2018.\n\nLsd-slam: Large-scale direct monocular slam. Jakob Engel, Thomas Sch\u00f6ps, Daniel Cremers, European Conference on Computer Vision (ECCV). Jakob Engel, Thomas Sch\u00f6ps, and Daniel Cremers. Lsd-slam: Large-scale direct monocular slam. In European Conference on Computer Vision (ECCV), 2014.\n\nA photometrically calibrated benchmark for monocular visual odometry. Jakob Engel, C Vladyslav, Daniel Usenko, Cremers, abs/1607.02555CoRRJakob Engel, Vladyslav C. Usenko, and Daniel Cremers. A photometrically calibrated benchmark for monocular visual odometry. CoRR, abs/1607.02555, 2016.\n\nStereoscan: Dense 3d reconstruction in real-time. Andreas Geiger, Julius Ziegler, Christoph Stiller, Intelligent Vehicles Symposium (IV). Andreas Geiger, Julius Ziegler, and Christoph Stiller. Stereoscan: Dense 3d reconstruction in real-time. In Intelligent Vehicles Symposium (IV), 2011.\n\nAre we ready for autonomous driving? the kitti vision benchmark suite. Andreas Geiger, Philip Lenz, Raquel Urtasun, Conference on Computer Vision and Pattern Recognition (CVPR). Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3354-3361, 2012.\n\nUnsupervised monocular depth estimation with left-right consistency. Cl\u00e9ment Godard, Oisin Mac Aodha, Gabriel J Brostow, IEEE Conference on Computer Vision and Pattern Recognition. Cl\u00e9ment Godard, Oisin Mac Aodha, and Gabriel J. Brostow. Unsupervised monocular depth estima- tion with left-right consistency. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\ngvnn: Neural network library for geometric computer vision. Ankur Handa, Michael Bloesch, Viorica P\u0203tr\u0203ucean, Simon Stent, John Mccormac, Andrew Davison, European Conference on Computer Vision Workshop (ECCVW). Ankur Handa, Michael Bloesch, Viorica P\u0203tr\u0203ucean, Simon Stent, John McCormac, and Andrew Davison. gvnn: Neural network library for geometric computer vision. In European Conference on Computer Vision Workshop (ECCVW), pp. 67-82, 2016.\n\nIn defense of the eight-point algorithm. R I Hartley, IEEE Transactions on Pattern Analysis and Machine Intelligence. 19R. I. Hartley. In defense of the eight-point algorithm. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19:580-593, 1997.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, 2016.\n\nAccurate and efficient stereo processing by semi-global matching and mutual information. H Hirschmuller, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR). 2H. Hirschmuller. Accurate and efficient stereo processing by semi-global matching and mutual information. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), volume 2, pp. 807-814, 2005.\n\nLearning to learn using gradient descent. Sepp Hochreiter, A Steven Younger, Peter R Conwell, International Conference on Artificial Neural Networks (ICANN). Sepp Hochreiter, A. Steven Younger, and Peter R. Conwell. Learning to learn using gradient descent. In International Conference on Artificial Neural Networks (ICANN), pp. 87-94, 2001.\n\nAutomatic photo pop-up. Derek Hoiem, Alexei A Efros, Martial Hebert, ACM SIGGRAPH. Derek Hoiem, Alexei A. Efros, and Martial Hebert. Automatic photo pop-up. In ACM SIGGRAPH, pp. 577-584, 2005.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, International Conference on Learning Representations (ICLR). Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.\n\nPulling things out of perspective. Jianbo L&apos;ubor Ladick\u00fd, Marc Shi, Pollefeys, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). L'ubor Ladick\u00fd, Jianbo Shi, and Marc Pollefeys. Pulling things out of perspective. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 89-96, 2014.\n\nDeeper depth prediction with fully convolutional residual networks. I Laina, C Rupprecht, V Belagiannis, F Tombari, N Navab, International Conference on 3D Vision (3DV). I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and N. Navab. Deeper depth prediction with fully convolutional residual networks. In International Conference on 3D Vision (3DV), pp. 239-248, 2016.\n\nFeature pyramid networks for object detection. T Y Lin, P Doll\u00e1r, R Girshick, K He, B Hariharan, S Belongie, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). T. Y. Lin, P. Doll\u00e1r, R. Girshick, K. He, B. Hariharan, and S. Belongie. Feature pyramid networks for object detection. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 936-944, 2017.\n\nIs levenberg-marquardt the most efficient optimization algorithm for implementing bundle adjustment. M L A Lourakis, A A Argyros, IEEE International Conference on Computer Vision (ICCV). 2M. L. A. Lourakis and A. A. Argyros. Is levenberg-marquardt the most efficient optimization algorithm for implementing bundle adjustment? In IEEE International Conference on Computer Vision (ICCV), volume 2, pp. 1526-1531, 2005.\n\nOrb-slam: a versatile and accurate monocular slam system. Ra\u00fal Mur-Artal, J M M Montiel, Juan D Tard\u00f3s, IEEE Transactions on Robotics. 31Ra\u00fal Mur-Artal, J. M. M. Montiel, and Juan D. Tard\u00f3s. Orb-slam: a versatile and accurate monocular slam system. IEEE Transactions on Robotics, 31:1147-1163, 2015.\n\nAn efficient solution to the five-point relative pose problem. D Nister, IEEE Transactions on Pattern Analysis and Machine Intelligence. 26D. Nister. An efficient solution to the five-point relative pose problem. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26:756-770, 2004.\n\nNumerical Optimization. J Nocedal, S J Wright, Springersecond editionJ. Nocedal and S. J. Wright. Numerical Optimization. Springer, second edition, 2006.\n\nMake3d: Learning 3d scene structure from a single still image. A Saxena, M Sun, A Y Ng, IEEE Transactions on Pattern Analysis and Machine Intelligence. 31A. Saxena, M. Sun, and A. Y. Ng. Make3d: Learning 3d scene structure from a single still image. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31:824-840, 2009.\n\nLearning depth from single monocular images. Ashutosh Saxena, Sung H Chung, Andrew Y Ng, International Conference on Neural Information Processing Systems (NIPS). Ashutosh Saxena, Sung H. Chung, and Andrew Y. Ng. Learning depth from single monocular images. In International Conference on Neural Information Processing Systems (NIPS), pp. 1161-1168, 2005.\n\nShrinkage fields for effective image restoration. U Schmidt, S Roth, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). U. Schmidt and S. Roth. Shrinkage fields for effective image restoration. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2774-2781, 2014.\n\nStructure-from-motion revisited. Johannes Lutz Sch\u00f6nberger, Jan-Michael Frahm, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Johannes Lutz Sch\u00f6nberger and Jan-Michael Frahm. Structure-from-motion revisited. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4104-4113, 2016.\n\nReal-time visual odometry from dense rgb-d images. F Steinbruecker, J Sturm, D Cremers, International Conference on Computer Vision Workshop on Live Dense Reconstruction with Moving Cameras(ICCVW). F. Steinbruecker, J. Sturm, and D. Cremers. Real-time visual odometry from dense rgb-d images. In International Conference on Computer Vision Workshop on Live Dense Reconstruction with Moving Cameras(ICCVW), 2011.\n\nGslam: Initialization-robust monocular visual slam via global structure-from-motion. C Tang, O Wang, P Tan, International Conference on 3D Vision (3DV). C. Tang, O. Wang, and P. Tan. Gslam: Initialization-robust monocular visual slam via global structure-from-motion. In International Conference on 3D Vision (3DV), pp. 239-248, 2017.\n\nCnn-slam: Real-time dense monocular slam with learned depth prediction. K Tateno, F Tombari, I Laina, N Navab, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). K. Tateno, F. Tombari, I. Laina, and N. Navab. Cnn-slam: Real-time dense monocular slam with learned depth prediction. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6565-6574, 2017.\n\nBundle adjustment -a modern synthesis. Bill Triggs, Philip F Mclauchlan, Richard I Hartley, Andrew W Fitzgibbon, Vision Algorithms: Theory and Practice. Bill Triggs, Philip F. McLauchlan, Richard I. Hartley, and Andrew W. Fitzgibbon. Bundle adjustment -a modern synthesis. In Vision Algorithms: Theory and Practice, pp. 298-372, 2000.\n\nDemon: Depth and motion network for learning monocular stereo. Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, Thomas Brox, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. Demon: Depth and motion network for learning monocular stereo. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5622-5631, 2017.\n\nLearning depth from monocular videos using direct methods. Chaoyang Wang, Miguel Buenaposada, Rui Jose, Simon Zhu, Lucey, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Chaoyang Wang, Buenaposada, Miguel Jose, Rui Zhu, , and Simon Lucey. Learning depth from monocular videos using direct methods. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 851-859, 2018.\n\nMulticore bundle adjustment. C Wu, S Agarwal, B Curless, S M Seitz, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). C. Wu, S. Agarwal, B. Curless, and S. M. Seitz. Multicore bundle adjustment. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3057-3064, 2011.\n\nMulti-scale continuous crfs as sequential deep networks for monocular depth estimation. D Xu, E Ricci, W Ouyang, X Wang, N Sebe, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). D. Xu, E. Ricci, W. Ouyang, X. Wang, and N. Sebe. Multi-scale continuous crfs as sequential deep networks for monocular depth estimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 161-169, 2017.\n\nDeep virtual stereo odometry: Leveraging deep depth prediction for monocular direct sparse odometry. Nan Yang, Rui Wang, Jorg Stuckler, Daniel Cremers, European Conference on Computer Vision (ECCV). Nan Yang, Rui Wang, Jorg Stuckler, and Daniel Cremers. Deep virtual stereo odometry: Leveraging deep depth prediction for monocular direct sparse odometry. In European Conference on Computer Vision (ECCV), 2018.\n\nDilated residual networks. F Yu, V Koltun, T Funkhouser, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). F. Yu, V. Koltun, and T. Funkhouser. Dilated residual networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 636-644, 2017.\n\nUnsupervised learning of depth and ego-motion from video. Tinghui Zhou, Matthew Brown, Noah Snavely, David G Lowe, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Tinghui Zhou, Matthew Brown, Noah Snavely, and David G. Lowe. Unsupervised learning of depth and ego-motion from video. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6612-6619, 2017.\n\nL Zwald, S Lambert-Lacroix, The berhu penalty and the grouped effect. CoRR, abs/1207. 6868L. Zwald and S. Lambert-Lacroix. The berhu penalty and the grouped effect. CoRR, abs/1207.6868, 2012.\n", "annotations": {"author": "[{\"end\":185,\"start\":44},{\"end\":314,\"start\":186}]", "publisher": null, "author_last_name": "[{\"end\":58,\"start\":54},{\"end\":194,\"start\":191}]", "author_first_name": "[{\"end\":53,\"start\":44},{\"end\":190,\"start\":186}]", "author_affiliation": "[{\"end\":184,\"start\":82},{\"end\":313,\"start\":211}]", "title": "[{\"end\":41,\"start\":1},{\"end\":355,\"start\":315}]", "venue": null, "abstract": "[{\"end\":2925,\"start\":357}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3092,\"start\":3076},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3118,\"start\":3092},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3137,\"start\":3118},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3164,\"start\":3137},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3279,\"start\":3258},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3300,\"start\":3279},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3348,\"start\":3332},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3374,\"start\":3348},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3410,\"start\":3390},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3437,\"start\":3410},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3515,\"start\":3491},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3560,\"start\":3535},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3794,\"start\":3769},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4584,\"start\":4565},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5908,\"start\":5887},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5913,\"start\":5908},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5959,\"start\":5937},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6011,\"start\":5991},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6032,\"start\":6013},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6292,\"start\":6271},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6393,\"start\":6374},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6433,\"start\":6416},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6488,\"start\":6472},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6860,\"start\":6841},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7163,\"start\":7135},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7296,\"start\":7272},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7713,\"start\":7688},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8561,\"start\":8541},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8587,\"start\":8566},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8702,\"start\":8681},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8723,\"start\":8702},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9423,\"start\":9399},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9448,\"start\":9423},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10005,\"start\":9992},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10555,\"start\":10535},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10582,\"start\":10555},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11386,\"start\":11362},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11410,\"start\":11391},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11489,\"start\":11469},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11627,\"start\":11607},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":12399,\"start\":12382},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12888,\"start\":12863},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":13383,\"start\":13366},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14407,\"start\":14389},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16700,\"start\":16680},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18009,\"start\":17997},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18411,\"start\":18398},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18459,\"start\":18438},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":18480,\"start\":18459},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20646,\"start\":20626},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":21727,\"start\":21707},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":21750,\"start\":21732},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21853,\"start\":21834},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22032,\"start\":22013},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":23668,\"start\":23638},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":23695,\"start\":23676},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":23763,\"start\":23746},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24034,\"start\":24016},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24222,\"start\":24203},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24748,\"start\":24725},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25132,\"start\":25112},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25336,\"start\":25317},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25614,\"start\":25593},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25828,\"start\":25807},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":26525,\"start\":26500},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":27103,\"start\":27089},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27140,\"start\":27120},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":27717,\"start\":27689},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":28051,\"start\":28031},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28082,\"start\":28062},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":28142,\"start\":28122},{\"end\":29892,\"start\":29845},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30035,\"start\":30018},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":30158,\"start\":30139},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":32973,\"start\":32949},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":34428,\"start\":34403},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":34705,\"start\":34685},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":34892,\"start\":34878},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":34961,\"start\":34936},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35247,\"start\":35227},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":35808,\"start\":35789},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":38286,\"start\":38262},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":38553,\"start\":38533},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":39839,\"start\":39819},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":40012,\"start\":39988}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38964,\"start\":38680},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39022,\"start\":38965},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39118,\"start\":39023},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39229,\"start\":39119},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39322,\"start\":39230},{\"attributes\":{\"id\":\"fig_6\"},\"end\":39537,\"start\":39323},{\"attributes\":{\"id\":\"fig_7\"},\"end\":39595,\"start\":39538},{\"attributes\":{\"id\":\"fig_8\"},\"end\":39771,\"start\":39596},{\"attributes\":{\"id\":\"fig_9\"},\"end\":39840,\"start\":39772},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":40510,\"start\":39841},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":40611,\"start\":40511},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":40700,\"start\":40612},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":41024,\"start\":40701},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":41169,\"start\":41025},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":42498,\"start\":41170},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":42557,\"start\":42499},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":42635,\"start\":42558}]", "paragraph": "[{\"end\":5092,\"start\":2941},{\"end\":6746,\"start\":5094},{\"end\":7731,\"start\":6748},{\"end\":8244,\"start\":7733},{\"end\":8880,\"start\":8276},{\"end\":9560,\"start\":8924},{\"end\":10006,\"start\":9608},{\"end\":10153,\"start\":10008},{\"end\":10785,\"start\":10155},{\"end\":11302,\"start\":10844},{\"end\":11690,\"start\":11304},{\"end\":11946,\"start\":11718},{\"end\":12682,\"start\":12005},{\"end\":13232,\"start\":12739},{\"end\":13840,\"start\":13245},{\"end\":14036,\"start\":13842},{\"end\":14239,\"start\":14038},{\"end\":15352,\"start\":14259},{\"end\":15903,\"start\":15505},{\"end\":16966,\"start\":15905},{\"end\":17357,\"start\":16994},{\"end\":17448,\"start\":17359},{\"end\":17836,\"start\":17450},{\"end\":18302,\"start\":17838},{\"end\":18881,\"start\":18304},{\"end\":19165,\"start\":18883},{\"end\":19367,\"start\":19167},{\"end\":19789,\"start\":19369},{\"end\":20093,\"start\":19837},{\"end\":21023,\"start\":20126},{\"end\":21527,\"start\":21055},{\"end\":22213,\"start\":21529},{\"end\":22721,\"start\":22232},{\"end\":23072,\"start\":22789},{\"end\":23318,\"start\":23085},{\"end\":23570,\"start\":23339},{\"end\":23696,\"start\":23572},{\"end\":23975,\"start\":23698},{\"end\":24362,\"start\":24000},{\"end\":24814,\"start\":24364},{\"end\":25098,\"start\":24816},{\"end\":25686,\"start\":25100},{\"end\":26453,\"start\":25721},{\"end\":27454,\"start\":26455},{\"end\":28471,\"start\":27456},{\"end\":28601,\"start\":28473},{\"end\":29363,\"start\":28634},{\"end\":30539,\"start\":29421},{\"end\":30929,\"start\":30541},{\"end\":31515,\"start\":30931},{\"end\":32101,\"start\":31575},{\"end\":32598,\"start\":32103},{\"end\":33688,\"start\":32600},{\"end\":33930,\"start\":33690},{\"end\":34254,\"start\":33932},{\"end\":35400,\"start\":34256},{\"end\":35610,\"start\":35402},{\"end\":36012,\"start\":35659},{\"end\":36332,\"start\":36014},{\"end\":37551,\"start\":36387},{\"end\":38068,\"start\":37601},{\"end\":38679,\"start\":38127}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8923,\"start\":8881},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9607,\"start\":9561},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10843,\"start\":10786},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12004,\"start\":11947},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12738,\"start\":12683},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15458,\"start\":15353},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15504,\"start\":15458},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19836,\"start\":19790},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20125,\"start\":20094},{\"attributes\":{\"id\":\"formula_9\"},\"end\":22231,\"start\":22214},{\"attributes\":{\"id\":\"formula_10\"},\"end\":22788,\"start\":22722},{\"attributes\":{\"id\":\"formula_11\"},\"end\":29387,\"start\":29364}]", "table_ref": "[{\"end\":26465,\"start\":26458},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":30785,\"start\":30778},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":31396,\"start\":31389},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":31912,\"start\":31905},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":32710,\"start\":32703},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":34303,\"start\":34296},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":36037,\"start\":36030},{\"end\":38469,\"start\":38462},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":38676,\"start\":38669}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2939,\"start\":2927},{\"attributes\":{\"n\":\"3\"},\"end\":8274,\"start\":8247},{\"attributes\":{\"n\":\"4\"},\"end\":11716,\"start\":11693},{\"attributes\":{\"n\":\"4.1\"},\"end\":13243,\"start\":13235},{\"attributes\":{\"n\":\"4.2\"},\"end\":14257,\"start\":14242},{\"attributes\":{\"n\":\"4.3\"},\"end\":16992,\"start\":16969},{\"attributes\":{\"n\":\"4.4\"},\"end\":21053,\"start\":21026},{\"attributes\":{\"n\":\"4.5\"},\"end\":23083,\"start\":23075},{\"end\":23337,\"start\":23321},{\"attributes\":{\"n\":\"5\"},\"end\":23988,\"start\":23978},{\"attributes\":{\"n\":\"5.1\"},\"end\":23998,\"start\":23991},{\"attributes\":{\"n\":\"5.2\"},\"end\":25719,\"start\":25689},{\"attributes\":{\"n\":\"6\"},\"end\":28632,\"start\":28604},{\"end\":29419,\"start\":29389},{\"end\":31573,\"start\":31518},{\"end\":35657,\"start\":35613},{\"end\":36385,\"start\":36335},{\"end\":37599,\"start\":37554},{\"end\":38125,\"start\":38071},{\"end\":38701,\"start\":38681},{\"end\":38976,\"start\":38966},{\"end\":39034,\"start\":39024},{\"end\":39241,\"start\":39231},{\"end\":39334,\"start\":39324},{\"end\":39549,\"start\":39539},{\"end\":39618,\"start\":39597},{\"end\":39784,\"start\":39773},{\"end\":40521,\"start\":40512},{\"end\":40620,\"start\":40613},{\"end\":40711,\"start\":40702},{\"end\":41035,\"start\":41026},{\"end\":41174,\"start\":41171},{\"end\":42509,\"start\":42500},{\"end\":42568,\"start\":42559}]", "table": "[{\"end\":40510,\"start\":40013},{\"end\":41024,\"start\":40795},{\"end\":42498,\"start\":41398}]", "figure_caption": "[{\"end\":38964,\"start\":38704},{\"end\":39022,\"start\":38978},{\"end\":39118,\"start\":39036},{\"end\":39229,\"start\":39121},{\"end\":39322,\"start\":39243},{\"end\":39537,\"start\":39336},{\"end\":39595,\"start\":39551},{\"end\":39771,\"start\":39622},{\"end\":39840,\"start\":39787},{\"end\":40013,\"start\":39843},{\"end\":40611,\"start\":40523},{\"end\":40700,\"start\":40622},{\"end\":40795,\"start\":40713},{\"end\":41169,\"start\":41037},{\"end\":41398,\"start\":41176},{\"end\":42557,\"start\":42511},{\"end\":42635,\"start\":42570}]", "figure_ref": "[{\"end\":12319,\"start\":12311},{\"end\":13271,\"start\":13263},{\"end\":14741,\"start\":14733},{\"end\":15644,\"start\":15636},{\"end\":16047,\"start\":16039},{\"end\":16143,\"start\":16135},{\"end\":16240,\"start\":16232},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18978,\"start\":18970},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20446,\"start\":20438},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29499,\"start\":29491},{\"end\":29674,\"start\":29663},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29813,\"start\":29805},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30045,\"start\":30037},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30449,\"start\":30441},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":33333,\"start\":33325},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":33707,\"start\":33699},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34195,\"start\":34187},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34211,\"start\":34203},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":37183,\"start\":37175},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":37612,\"start\":37604},{\"end\":38207,\"start\":38199},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":38480,\"start\":38471}]", "bib_author_first_name": "[{\"end\":42807,\"start\":42801},{\"end\":42821,\"start\":42817},{\"end\":42836,\"start\":42830},{\"end\":42963,\"start\":42957},{\"end\":42977,\"start\":42973},{\"end\":42993,\"start\":42987},{\"end\":42995,\"start\":42994},{\"end\":43010,\"start\":43003},{\"end\":43267,\"start\":43261},{\"end\":43285,\"start\":43277},{\"end\":43300,\"start\":43296},{\"end\":43313,\"start\":43310},{\"end\":43326,\"start\":43321},{\"end\":43342,\"start\":43336},{\"end\":43344,\"start\":43343},{\"end\":43359,\"start\":43352},{\"end\":43629,\"start\":43622},{\"end\":43643,\"start\":43636},{\"end\":44005,\"start\":43996},{\"end\":44020,\"start\":44013},{\"end\":44321,\"start\":44314},{\"end\":44334,\"start\":44331},{\"end\":44353,\"start\":44347},{\"end\":44367,\"start\":44361},{\"end\":44387,\"start\":44381},{\"end\":44389,\"start\":44388},{\"end\":44801,\"start\":44800},{\"end\":44803,\"start\":44802},{\"end\":45029,\"start\":45022},{\"end\":45044,\"start\":45037},{\"end\":45060,\"start\":45054},{\"end\":45073,\"start\":45067},{\"end\":45090,\"start\":45085},{\"end\":45104,\"start\":45099},{\"end\":45118,\"start\":45112},{\"end\":45120,\"start\":45119},{\"end\":45137,\"start\":45131},{\"end\":45472,\"start\":45467},{\"end\":45474,\"start\":45473},{\"end\":45488,\"start\":45482},{\"end\":45490,\"start\":45489},{\"end\":45511,\"start\":45503},{\"end\":45513,\"start\":45512},{\"end\":45525,\"start\":45522},{\"end\":45543,\"start\":45536},{\"end\":45555,\"start\":45551},{\"end\":45566,\"start\":45560},{\"end\":45584,\"start\":45577},{\"end\":45598,\"start\":45592},{\"end\":45608,\"start\":45605},{\"end\":45622,\"start\":45613},{\"end\":45631,\"start\":45629},{\"end\":45642,\"start\":45636},{\"end\":46002,\"start\":45996},{\"end\":46017,\"start\":46010},{\"end\":46030,\"start\":46027},{\"end\":46049,\"start\":46043},{\"end\":46069,\"start\":46063},{\"end\":46071,\"start\":46070},{\"end\":46384,\"start\":46383},{\"end\":46398,\"start\":46397},{\"end\":46413,\"start\":46412},{\"end\":46415,\"start\":46414},{\"end\":46741,\"start\":46740},{\"end\":46748,\"start\":46747},{\"end\":46750,\"start\":46749},{\"end\":46759,\"start\":46758},{\"end\":46768,\"start\":46767},{\"end\":46778,\"start\":46777},{\"end\":46792,\"start\":46791},{\"end\":47207,\"start\":47201},{\"end\":47221,\"start\":47213},{\"end\":47239,\"start\":47232},{\"end\":47258,\"start\":47251},{\"end\":47275,\"start\":47266},{\"end\":47618,\"start\":47617},{\"end\":47630,\"start\":47629},{\"end\":47950,\"start\":47944},{\"end\":48158,\"start\":48157},{\"end\":48167,\"start\":48166},{\"end\":48527,\"start\":48522},{\"end\":48544,\"start\":48535},{\"end\":48557,\"start\":48554},{\"end\":48887,\"start\":48886},{\"end\":48896,\"start\":48895},{\"end\":48906,\"start\":48905},{\"end\":49176,\"start\":49171},{\"end\":49190,\"start\":49184},{\"end\":49205,\"start\":49199},{\"end\":49487,\"start\":49482},{\"end\":49496,\"start\":49495},{\"end\":49514,\"start\":49508},{\"end\":49760,\"start\":49753},{\"end\":49775,\"start\":49769},{\"end\":49794,\"start\":49785},{\"end\":50071,\"start\":50064},{\"end\":50086,\"start\":50080},{\"end\":50099,\"start\":50093},{\"end\":50454,\"start\":50447},{\"end\":50468,\"start\":50463},{\"end\":50487,\"start\":50480},{\"end\":50489,\"start\":50488},{\"end\":50829,\"start\":50824},{\"end\":50844,\"start\":50837},{\"end\":50861,\"start\":50854},{\"end\":50879,\"start\":50874},{\"end\":50891,\"start\":50887},{\"end\":50908,\"start\":50902},{\"end\":51253,\"start\":51252},{\"end\":51255,\"start\":51254},{\"end\":51517,\"start\":51516},{\"end\":51523,\"start\":51522},{\"end\":51532,\"start\":51531},{\"end\":51539,\"start\":51538},{\"end\":51875,\"start\":51874},{\"end\":52244,\"start\":52240},{\"end\":52258,\"start\":52257},{\"end\":52265,\"start\":52259},{\"end\":52280,\"start\":52275},{\"end\":52282,\"start\":52281},{\"end\":52570,\"start\":52565},{\"end\":52584,\"start\":52578},{\"end\":52586,\"start\":52585},{\"end\":52601,\"start\":52594},{\"end\":52780,\"start\":52779},{\"end\":52796,\"start\":52791},{\"end\":53059,\"start\":53053},{\"end\":53085,\"start\":53081},{\"end\":53409,\"start\":53408},{\"end\":53418,\"start\":53417},{\"end\":53431,\"start\":53430},{\"end\":53446,\"start\":53445},{\"end\":53457,\"start\":53456},{\"end\":53760,\"start\":53759},{\"end\":53762,\"start\":53761},{\"end\":53769,\"start\":53768},{\"end\":53779,\"start\":53778},{\"end\":53791,\"start\":53790},{\"end\":53797,\"start\":53796},{\"end\":53810,\"start\":53809},{\"end\":54200,\"start\":54199},{\"end\":54204,\"start\":54201},{\"end\":54216,\"start\":54215},{\"end\":54218,\"start\":54217},{\"end\":54578,\"start\":54574},{\"end\":54591,\"start\":54590},{\"end\":54595,\"start\":54592},{\"end\":54609,\"start\":54605},{\"end\":54611,\"start\":54610},{\"end\":54881,\"start\":54880},{\"end\":55138,\"start\":55137},{\"end\":55149,\"start\":55148},{\"end\":55151,\"start\":55150},{\"end\":55332,\"start\":55331},{\"end\":55342,\"start\":55341},{\"end\":55349,\"start\":55348},{\"end\":55351,\"start\":55350},{\"end\":55654,\"start\":55646},{\"end\":55667,\"start\":55663},{\"end\":55669,\"start\":55668},{\"end\":55683,\"start\":55677},{\"end\":55685,\"start\":55684},{\"end\":56009,\"start\":56008},{\"end\":56020,\"start\":56019},{\"end\":56301,\"start\":56293},{\"end\":56331,\"start\":56320},{\"end\":56632,\"start\":56631},{\"end\":56649,\"start\":56648},{\"end\":56658,\"start\":56657},{\"end\":57079,\"start\":57078},{\"end\":57087,\"start\":57086},{\"end\":57095,\"start\":57094},{\"end\":57402,\"start\":57401},{\"end\":57412,\"start\":57411},{\"end\":57423,\"start\":57422},{\"end\":57432,\"start\":57431},{\"end\":57761,\"start\":57757},{\"end\":57776,\"start\":57770},{\"end\":57778,\"start\":57777},{\"end\":57798,\"start\":57791},{\"end\":57800,\"start\":57799},{\"end\":57816,\"start\":57810},{\"end\":57818,\"start\":57817},{\"end\":58125,\"start\":58117},{\"end\":58146,\"start\":58138},{\"end\":58158,\"start\":58153},{\"end\":58174,\"start\":58166},{\"end\":58186,\"start\":58182},{\"end\":58198,\"start\":58192},{\"end\":58218,\"start\":58212},{\"end\":58626,\"start\":58618},{\"end\":58639,\"start\":58633},{\"end\":58656,\"start\":58653},{\"end\":58668,\"start\":58663},{\"end\":58996,\"start\":58995},{\"end\":59002,\"start\":59001},{\"end\":59013,\"start\":59012},{\"end\":59024,\"start\":59023},{\"end\":59026,\"start\":59025},{\"end\":59359,\"start\":59358},{\"end\":59365,\"start\":59364},{\"end\":59374,\"start\":59373},{\"end\":59384,\"start\":59383},{\"end\":59392,\"start\":59391},{\"end\":59798,\"start\":59795},{\"end\":59808,\"start\":59805},{\"end\":59819,\"start\":59815},{\"end\":59836,\"start\":59830},{\"end\":60134,\"start\":60133},{\"end\":60140,\"start\":60139},{\"end\":60150,\"start\":60149},{\"end\":60449,\"start\":60442},{\"end\":60463,\"start\":60456},{\"end\":60475,\"start\":60471},{\"end\":60490,\"start\":60485},{\"end\":60492,\"start\":60491},{\"end\":60779,\"start\":60778},{\"end\":60788,\"start\":60787}]", "bib_author_last_name": "[{\"end\":42815,\"start\":42808},{\"end\":42828,\"start\":42822},{\"end\":42842,\"start\":42837},{\"end\":42971,\"start\":42964},{\"end\":42985,\"start\":42978},{\"end\":43001,\"start\":42996},{\"end\":43019,\"start\":43011},{\"end\":43275,\"start\":43268},{\"end\":43294,\"start\":43286},{\"end\":43308,\"start\":43301},{\"end\":43319,\"start\":43314},{\"end\":43334,\"start\":43327},{\"end\":43350,\"start\":43345},{\"end\":43368,\"start\":43360},{\"end\":43634,\"start\":43630},{\"end\":43650,\"start\":43644},{\"end\":44011,\"start\":44006},{\"end\":44028,\"start\":44021},{\"end\":44329,\"start\":44322},{\"end\":44345,\"start\":44335},{\"end\":44359,\"start\":44354},{\"end\":44379,\"start\":44368},{\"end\":44397,\"start\":44390},{\"end\":44809,\"start\":44804},{\"end\":45035,\"start\":45030},{\"end\":45052,\"start\":45045},{\"end\":45065,\"start\":45061},{\"end\":45083,\"start\":45074},{\"end\":45097,\"start\":45091},{\"end\":45110,\"start\":45105},{\"end\":45129,\"start\":45121},{\"end\":45146,\"start\":45138},{\"end\":45480,\"start\":45475},{\"end\":45501,\"start\":45491},{\"end\":45520,\"start\":45514},{\"end\":45534,\"start\":45526},{\"end\":45549,\"start\":45544},{\"end\":45558,\"start\":45556},{\"end\":45575,\"start\":45567},{\"end\":45590,\"start\":45585},{\"end\":45603,\"start\":45599},{\"end\":45611,\"start\":45609},{\"end\":45627,\"start\":45623},{\"end\":45634,\"start\":45632},{\"end\":45645,\"start\":45643},{\"end\":46008,\"start\":46003},{\"end\":46025,\"start\":46018},{\"end\":46041,\"start\":46031},{\"end\":46061,\"start\":46050},{\"end\":46079,\"start\":46072},{\"end\":46395,\"start\":46385},{\"end\":46410,\"start\":46399},{\"end\":46423,\"start\":46416},{\"end\":46745,\"start\":46742},{\"end\":46756,\"start\":46751},{\"end\":46765,\"start\":46760},{\"end\":46775,\"start\":46769},{\"end\":46789,\"start\":46779},{\"end\":46800,\"start\":46793},{\"end\":47211,\"start\":47208},{\"end\":47230,\"start\":47222},{\"end\":47249,\"start\":47240},{\"end\":47264,\"start\":47259},{\"end\":47284,\"start\":47276},{\"end\":47627,\"start\":47619},{\"end\":47640,\"start\":47631},{\"end\":47956,\"start\":47951},{\"end\":48164,\"start\":48159},{\"end\":48174,\"start\":48168},{\"end\":48533,\"start\":48528},{\"end\":48552,\"start\":48545},{\"end\":48564,\"start\":48558},{\"end\":48893,\"start\":48888},{\"end\":48903,\"start\":48897},{\"end\":48914,\"start\":48907},{\"end\":49182,\"start\":49177},{\"end\":49197,\"start\":49191},{\"end\":49213,\"start\":49206},{\"end\":49493,\"start\":49488},{\"end\":49506,\"start\":49497},{\"end\":49521,\"start\":49515},{\"end\":49530,\"start\":49523},{\"end\":49767,\"start\":49761},{\"end\":49783,\"start\":49776},{\"end\":49802,\"start\":49795},{\"end\":50078,\"start\":50072},{\"end\":50091,\"start\":50087},{\"end\":50107,\"start\":50100},{\"end\":50461,\"start\":50455},{\"end\":50478,\"start\":50469},{\"end\":50497,\"start\":50490},{\"end\":50835,\"start\":50830},{\"end\":50852,\"start\":50845},{\"end\":50872,\"start\":50862},{\"end\":50885,\"start\":50880},{\"end\":50900,\"start\":50892},{\"end\":50916,\"start\":50909},{\"end\":51263,\"start\":51256},{\"end\":51520,\"start\":51518},{\"end\":51529,\"start\":51524},{\"end\":51536,\"start\":51533},{\"end\":51543,\"start\":51540},{\"end\":51888,\"start\":51876},{\"end\":52255,\"start\":52245},{\"end\":52273,\"start\":52266},{\"end\":52290,\"start\":52283},{\"end\":52576,\"start\":52571},{\"end\":52592,\"start\":52587},{\"end\":52608,\"start\":52602},{\"end\":52789,\"start\":52781},{\"end\":52803,\"start\":52797},{\"end\":52807,\"start\":52805},{\"end\":53079,\"start\":53060},{\"end\":53089,\"start\":53086},{\"end\":53100,\"start\":53091},{\"end\":53415,\"start\":53410},{\"end\":53428,\"start\":53419},{\"end\":53443,\"start\":53432},{\"end\":53454,\"start\":53447},{\"end\":53463,\"start\":53458},{\"end\":53766,\"start\":53763},{\"end\":53776,\"start\":53770},{\"end\":53788,\"start\":53780},{\"end\":53794,\"start\":53792},{\"end\":53807,\"start\":53798},{\"end\":53819,\"start\":53811},{\"end\":54213,\"start\":54205},{\"end\":54226,\"start\":54219},{\"end\":54588,\"start\":54579},{\"end\":54603,\"start\":54596},{\"end\":54618,\"start\":54612},{\"end\":54888,\"start\":54882},{\"end\":55146,\"start\":55139},{\"end\":55158,\"start\":55152},{\"end\":55339,\"start\":55333},{\"end\":55346,\"start\":55343},{\"end\":55354,\"start\":55352},{\"end\":55661,\"start\":55655},{\"end\":55675,\"start\":55670},{\"end\":55688,\"start\":55686},{\"end\":56017,\"start\":56010},{\"end\":56025,\"start\":56021},{\"end\":56318,\"start\":56302},{\"end\":56337,\"start\":56332},{\"end\":56646,\"start\":56633},{\"end\":56655,\"start\":56650},{\"end\":56666,\"start\":56659},{\"end\":57084,\"start\":57080},{\"end\":57092,\"start\":57088},{\"end\":57099,\"start\":57096},{\"end\":57409,\"start\":57403},{\"end\":57420,\"start\":57413},{\"end\":57429,\"start\":57424},{\"end\":57438,\"start\":57433},{\"end\":57768,\"start\":57762},{\"end\":57789,\"start\":57779},{\"end\":57808,\"start\":57801},{\"end\":57829,\"start\":57819},{\"end\":58136,\"start\":58126},{\"end\":58151,\"start\":58147},{\"end\":58164,\"start\":58159},{\"end\":58180,\"start\":58175},{\"end\":58190,\"start\":58187},{\"end\":58210,\"start\":58199},{\"end\":58223,\"start\":58219},{\"end\":58631,\"start\":58627},{\"end\":58651,\"start\":58640},{\"end\":58661,\"start\":58657},{\"end\":58672,\"start\":58669},{\"end\":58679,\"start\":58674},{\"end\":58999,\"start\":58997},{\"end\":59010,\"start\":59003},{\"end\":59021,\"start\":59014},{\"end\":59032,\"start\":59027},{\"end\":59362,\"start\":59360},{\"end\":59371,\"start\":59366},{\"end\":59381,\"start\":59375},{\"end\":59389,\"start\":59385},{\"end\":59397,\"start\":59393},{\"end\":59803,\"start\":59799},{\"end\":59813,\"start\":59809},{\"end\":59828,\"start\":59820},{\"end\":59844,\"start\":59837},{\"end\":60137,\"start\":60135},{\"end\":60147,\"start\":60141},{\"end\":60161,\"start\":60151},{\"end\":60454,\"start\":60450},{\"end\":60469,\"start\":60464},{\"end\":60483,\"start\":60476},{\"end\":60497,\"start\":60493},{\"end\":60785,\"start\":60780},{\"end\":60804,\"start\":60789}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":42923,\"start\":42799},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":10389453},\"end\":43235,\"start\":42925},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7448214},\"end\":43553,\"start\":43237},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1791473},\"end\":43892,\"start\":43555},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":12469179},\"end\":44232,\"start\":43894},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":4624670},\"end\":44711,\"start\":44234},{\"attributes\":{\"id\":\"b6\"},\"end\":44983,\"start\":44713},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":9999787},\"end\":45414,\"start\":44985},{\"attributes\":{\"doi\":\"abs/1512.03012\",\"id\":\"b8\"},\"end\":45930,\"start\":45416},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":52185197},\"end\":46337,\"start\":45932},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":4755631},\"end\":46676,\"start\":46339},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":7684883},\"end\":47097,\"start\":46678},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":32286806},\"end\":47551,\"start\":47099},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":15055356},\"end\":47893,\"start\":47553},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2792062},\"end\":48047,\"start\":47895},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":102496818},\"end\":48445,\"start\":48049},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":2255738},\"end\":48860,\"start\":48447},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3299195},\"end\":49124,\"start\":48862},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14547347},\"end\":49410,\"start\":49126},{\"attributes\":{\"doi\":\"abs/1607.02555\",\"id\":\"b19\"},\"end\":49701,\"start\":49412},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":16284071},\"end\":49991,\"start\":49703},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6724907},\"end\":50376,\"start\":49993},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":206596513},\"end\":50762,\"start\":50378},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":12733543},\"end\":51209,\"start\":50764},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":16919747},\"end\":51468,\"start\":51211},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206594692},\"end\":51783,\"start\":51470},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":2850611},\"end\":52196,\"start\":51785},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":52872549},\"end\":52539,\"start\":52198},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":834882},\"end\":52733,\"start\":52541},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":6628106},\"end\":53016,\"start\":52735},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":11106957},\"end\":53338,\"start\":53018},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":11091110},\"end\":53710,\"start\":53340},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":10716717},\"end\":54096,\"start\":53712},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":16542484},\"end\":54514,\"start\":54098},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":206775100},\"end\":54815,\"start\":54516},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":886598},\"end\":55111,\"start\":54817},{\"attributes\":{\"id\":\"b36\"},\"end\":55266,\"start\":55113},{\"attributes\":{\"id\":\"b37\"},\"end\":55599,\"start\":55268},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":10748875},\"end\":55956,\"start\":55601},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":3612623},\"end\":56258,\"start\":55958},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":1728538},\"end\":56578,\"start\":56260},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":2007767},\"end\":56991,\"start\":56580},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":8125892},\"end\":57327,\"start\":56993},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":206596482},\"end\":57716,\"start\":57329},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":1354186},\"end\":58052,\"start\":57718},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":6159584},\"end\":58557,\"start\":58054},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":21352010},\"end\":58964,\"start\":58559},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":11832473},\"end\":59268,\"start\":58966},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":4396708},\"end\":59692,\"start\":59270},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":49658377},\"end\":60104,\"start\":59694},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":6592393},\"end\":60382,\"start\":60106},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":11977588},\"end\":60776,\"start\":60384},{\"attributes\":{\"id\":\"b52\"},\"end\":60969,\"start\":60778}]", "bib_title": "[{\"end\":42955,\"start\":42925},{\"end\":43259,\"start\":43237},{\"end\":43620,\"start\":43555},{\"end\":43994,\"start\":43894},{\"end\":44312,\"start\":44234},{\"end\":45020,\"start\":44985},{\"end\":45994,\"start\":45932},{\"end\":46381,\"start\":46339},{\"end\":46738,\"start\":46678},{\"end\":47199,\"start\":47099},{\"end\":47615,\"start\":47553},{\"end\":47942,\"start\":47895},{\"end\":48155,\"start\":48049},{\"end\":48520,\"start\":48447},{\"end\":48884,\"start\":48862},{\"end\":49169,\"start\":49126},{\"end\":49751,\"start\":49703},{\"end\":50062,\"start\":49993},{\"end\":50445,\"start\":50378},{\"end\":50822,\"start\":50764},{\"end\":51250,\"start\":51211},{\"end\":51514,\"start\":51470},{\"end\":51872,\"start\":51785},{\"end\":52238,\"start\":52198},{\"end\":52563,\"start\":52541},{\"end\":52777,\"start\":52735},{\"end\":53051,\"start\":53018},{\"end\":53406,\"start\":53340},{\"end\":53757,\"start\":53712},{\"end\":54197,\"start\":54098},{\"end\":54572,\"start\":54516},{\"end\":54878,\"start\":54817},{\"end\":55329,\"start\":55268},{\"end\":55644,\"start\":55601},{\"end\":56006,\"start\":55958},{\"end\":56291,\"start\":56260},{\"end\":56629,\"start\":56580},{\"end\":57076,\"start\":56993},{\"end\":57399,\"start\":57329},{\"end\":57755,\"start\":57718},{\"end\":58115,\"start\":58054},{\"end\":58616,\"start\":58559},{\"end\":58993,\"start\":58966},{\"end\":59356,\"start\":59270},{\"end\":59793,\"start\":59694},{\"end\":60131,\"start\":60106},{\"end\":60440,\"start\":60384}]", "bib_author": "[{\"end\":42817,\"start\":42801},{\"end\":42830,\"start\":42817},{\"end\":42844,\"start\":42830},{\"end\":42973,\"start\":42957},{\"end\":42987,\"start\":42973},{\"end\":43003,\"start\":42987},{\"end\":43021,\"start\":43003},{\"end\":43277,\"start\":43261},{\"end\":43296,\"start\":43277},{\"end\":43310,\"start\":43296},{\"end\":43321,\"start\":43310},{\"end\":43336,\"start\":43321},{\"end\":43352,\"start\":43336},{\"end\":43370,\"start\":43352},{\"end\":43636,\"start\":43622},{\"end\":43652,\"start\":43636},{\"end\":44013,\"start\":43996},{\"end\":44030,\"start\":44013},{\"end\":44331,\"start\":44314},{\"end\":44347,\"start\":44331},{\"end\":44361,\"start\":44347},{\"end\":44381,\"start\":44361},{\"end\":44399,\"start\":44381},{\"end\":44811,\"start\":44800},{\"end\":45037,\"start\":45022},{\"end\":45054,\"start\":45037},{\"end\":45067,\"start\":45054},{\"end\":45085,\"start\":45067},{\"end\":45099,\"start\":45085},{\"end\":45112,\"start\":45099},{\"end\":45131,\"start\":45112},{\"end\":45148,\"start\":45131},{\"end\":45482,\"start\":45467},{\"end\":45503,\"start\":45482},{\"end\":45522,\"start\":45503},{\"end\":45536,\"start\":45522},{\"end\":45551,\"start\":45536},{\"end\":45560,\"start\":45551},{\"end\":45577,\"start\":45560},{\"end\":45592,\"start\":45577},{\"end\":45605,\"start\":45592},{\"end\":45613,\"start\":45605},{\"end\":45629,\"start\":45613},{\"end\":45636,\"start\":45629},{\"end\":45647,\"start\":45636},{\"end\":46010,\"start\":45996},{\"end\":46027,\"start\":46010},{\"end\":46043,\"start\":46027},{\"end\":46063,\"start\":46043},{\"end\":46081,\"start\":46063},{\"end\":46397,\"start\":46383},{\"end\":46412,\"start\":46397},{\"end\":46425,\"start\":46412},{\"end\":46747,\"start\":46740},{\"end\":46758,\"start\":46747},{\"end\":46767,\"start\":46758},{\"end\":46777,\"start\":46767},{\"end\":46791,\"start\":46777},{\"end\":46802,\"start\":46791},{\"end\":47213,\"start\":47201},{\"end\":47232,\"start\":47213},{\"end\":47251,\"start\":47232},{\"end\":47266,\"start\":47251},{\"end\":47286,\"start\":47266},{\"end\":47629,\"start\":47617},{\"end\":47642,\"start\":47629},{\"end\":47958,\"start\":47944},{\"end\":48166,\"start\":48157},{\"end\":48176,\"start\":48166},{\"end\":48535,\"start\":48522},{\"end\":48554,\"start\":48535},{\"end\":48566,\"start\":48554},{\"end\":48895,\"start\":48886},{\"end\":48905,\"start\":48895},{\"end\":48916,\"start\":48905},{\"end\":49184,\"start\":49171},{\"end\":49199,\"start\":49184},{\"end\":49215,\"start\":49199},{\"end\":49495,\"start\":49482},{\"end\":49508,\"start\":49495},{\"end\":49523,\"start\":49508},{\"end\":49532,\"start\":49523},{\"end\":49769,\"start\":49753},{\"end\":49785,\"start\":49769},{\"end\":49804,\"start\":49785},{\"end\":50080,\"start\":50064},{\"end\":50093,\"start\":50080},{\"end\":50109,\"start\":50093},{\"end\":50463,\"start\":50447},{\"end\":50480,\"start\":50463},{\"end\":50499,\"start\":50480},{\"end\":50837,\"start\":50824},{\"end\":50854,\"start\":50837},{\"end\":50874,\"start\":50854},{\"end\":50887,\"start\":50874},{\"end\":50902,\"start\":50887},{\"end\":50918,\"start\":50902},{\"end\":51265,\"start\":51252},{\"end\":51522,\"start\":51516},{\"end\":51531,\"start\":51522},{\"end\":51538,\"start\":51531},{\"end\":51545,\"start\":51538},{\"end\":51890,\"start\":51874},{\"end\":52257,\"start\":52240},{\"end\":52275,\"start\":52257},{\"end\":52292,\"start\":52275},{\"end\":52578,\"start\":52565},{\"end\":52594,\"start\":52578},{\"end\":52610,\"start\":52594},{\"end\":52791,\"start\":52779},{\"end\":52805,\"start\":52791},{\"end\":52809,\"start\":52805},{\"end\":53081,\"start\":53053},{\"end\":53091,\"start\":53081},{\"end\":53102,\"start\":53091},{\"end\":53417,\"start\":53408},{\"end\":53430,\"start\":53417},{\"end\":53445,\"start\":53430},{\"end\":53456,\"start\":53445},{\"end\":53465,\"start\":53456},{\"end\":53768,\"start\":53759},{\"end\":53778,\"start\":53768},{\"end\":53790,\"start\":53778},{\"end\":53796,\"start\":53790},{\"end\":53809,\"start\":53796},{\"end\":53821,\"start\":53809},{\"end\":54215,\"start\":54199},{\"end\":54228,\"start\":54215},{\"end\":54590,\"start\":54574},{\"end\":54605,\"start\":54590},{\"end\":54620,\"start\":54605},{\"end\":54890,\"start\":54880},{\"end\":55148,\"start\":55137},{\"end\":55160,\"start\":55148},{\"end\":55341,\"start\":55331},{\"end\":55348,\"start\":55341},{\"end\":55356,\"start\":55348},{\"end\":55663,\"start\":55646},{\"end\":55677,\"start\":55663},{\"end\":55690,\"start\":55677},{\"end\":56019,\"start\":56008},{\"end\":56027,\"start\":56019},{\"end\":56320,\"start\":56293},{\"end\":56339,\"start\":56320},{\"end\":56648,\"start\":56631},{\"end\":56657,\"start\":56648},{\"end\":56668,\"start\":56657},{\"end\":57086,\"start\":57078},{\"end\":57094,\"start\":57086},{\"end\":57101,\"start\":57094},{\"end\":57411,\"start\":57401},{\"end\":57422,\"start\":57411},{\"end\":57431,\"start\":57422},{\"end\":57440,\"start\":57431},{\"end\":57770,\"start\":57757},{\"end\":57791,\"start\":57770},{\"end\":57810,\"start\":57791},{\"end\":57831,\"start\":57810},{\"end\":58138,\"start\":58117},{\"end\":58153,\"start\":58138},{\"end\":58166,\"start\":58153},{\"end\":58182,\"start\":58166},{\"end\":58192,\"start\":58182},{\"end\":58212,\"start\":58192},{\"end\":58225,\"start\":58212},{\"end\":58633,\"start\":58618},{\"end\":58653,\"start\":58633},{\"end\":58663,\"start\":58653},{\"end\":58674,\"start\":58663},{\"end\":58681,\"start\":58674},{\"end\":59001,\"start\":58995},{\"end\":59012,\"start\":59001},{\"end\":59023,\"start\":59012},{\"end\":59034,\"start\":59023},{\"end\":59364,\"start\":59358},{\"end\":59373,\"start\":59364},{\"end\":59383,\"start\":59373},{\"end\":59391,\"start\":59383},{\"end\":59399,\"start\":59391},{\"end\":59805,\"start\":59795},{\"end\":59815,\"start\":59805},{\"end\":59830,\"start\":59815},{\"end\":59846,\"start\":59830},{\"end\":60139,\"start\":60133},{\"end\":60149,\"start\":60139},{\"end\":60163,\"start\":60149},{\"end\":60456,\"start\":60442},{\"end\":60471,\"start\":60456},{\"end\":60485,\"start\":60471},{\"end\":60499,\"start\":60485},{\"end\":60787,\"start\":60778},{\"end\":60806,\"start\":60787}]", "bib_venue": "[{\"end\":43066,\"start\":43021},{\"end\":43381,\"start\":43370},{\"end\":43703,\"start\":43652},{\"end\":44049,\"start\":44030},{\"end\":44464,\"start\":44399},{\"end\":44798,\"start\":44713},{\"end\":45190,\"start\":45148},{\"end\":45465,\"start\":45416},{\"end\":46126,\"start\":46081},{\"end\":46491,\"start\":46425},{\"end\":46867,\"start\":46802},{\"end\":47314,\"start\":47286},{\"end\":47707,\"start\":47642},{\"end\":47965,\"start\":47958},{\"end\":48231,\"start\":48176},{\"end\":48638,\"start\":48566},{\"end\":48978,\"start\":48916},{\"end\":49260,\"start\":49215},{\"end\":49480,\"start\":49412},{\"end\":49839,\"start\":49804},{\"end\":50169,\"start\":50109},{\"end\":50557,\"start\":50499},{\"end\":50973,\"start\":50918},{\"end\":51327,\"start\":51265},{\"end\":51610,\"start\":51545},{\"end\":51972,\"start\":51890},{\"end\":52354,\"start\":52292},{\"end\":52622,\"start\":52610},{\"end\":52868,\"start\":52809},{\"end\":53167,\"start\":53102},{\"end\":53508,\"start\":53465},{\"end\":53886,\"start\":53821},{\"end\":54283,\"start\":54228},{\"end\":54649,\"start\":54620},{\"end\":54952,\"start\":54890},{\"end\":55135,\"start\":55113},{\"end\":55418,\"start\":55356},{\"end\":55762,\"start\":55690},{\"end\":56092,\"start\":56027},{\"end\":56404,\"start\":56339},{\"end\":56776,\"start\":56668},{\"end\":57144,\"start\":57101},{\"end\":57505,\"start\":57440},{\"end\":57869,\"start\":57831},{\"end\":58290,\"start\":58225},{\"end\":58746,\"start\":58681},{\"end\":59099,\"start\":59034},{\"end\":59464,\"start\":59399},{\"end\":59891,\"start\":59846},{\"end\":60228,\"start\":60163},{\"end\":60564,\"start\":60499},{\"end\":60862,\"start\":60806}]"}}}, "year": 2023, "month": 12, "day": 17}