{"id": 21553643, "updated": "2023-09-28 20:44:06.512", "metadata": {"title": "Hyper-dimensional computing for a visual question-answering system that is trainable end-to-end", "authors": "[{\"first\":\"Guglielmo\",\"last\":\"Montone\",\"middle\":[]},{\"first\":\"J.\",\"last\":\"O'Regan\",\"middle\":[\"Kevin\"]},{\"first\":\"Alexander\",\"last\":\"Terekhov\",\"middle\":[\"V.\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 11, "day": 28}, "abstract": "In this work we propose a system for visual question answering. Our architecture is composed of two parts, the first part creates the logical knowledge base given the image. The second part evaluates questions against the knowledge base. Differently from previous work, the knowledge base is represented using hyper-dimensional computing. This choice has the advantage that all the operations in the system, namely creating the knowledge base and evaluating the questions against it, are differentiable, thereby making the system easily trainable in an end-to-end fashion.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1711.10185", "mag": "2770439130", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1711-10185", "doi": null}}, "content": {"source": {"pdf_hash": "004dc8de3a6832c8d4764144570dc122b5265ec5", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1711.10185v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "3dae21b4fa9618c41842469af3817f2048aedbb9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/004dc8de3a6832c8d4764144570dc122b5265ec5.txt", "contents": "\nHyper-dimensional computing for a visual question-answering system that is trainable end-to-end\n\n\nGuglielmo Montone montone.guglielmo@gmail.com \nLaboratoire Psychologie de la Perception\nUniversit\u00e9 Paris Descartes\n75006ParisFrance\n\nJ Kevin O&apos;regan jkevin.oregan@gmail.com \nLaboratoire Psychologie de la Perception Universit\u00e9 Paris Descartes\n75006ParisFrance\n\nAlexander V Terekhov avterekhov@gmail.com \nLaboratoire Psychologie de la Perception Universit\u00e9 Paris Descartes\n75006ParisFrance\n\nHyper-dimensional computing for a visual question-answering system that is trainable end-to-end\n\nIn this work we propose a system for visual question answering. Our architecture is composed of two parts, the first part creates the logical knowledge base given the image. The second part evaluates questions against the knowledge base. Differently from previous work, the knowledge base is represented using hyper-dimensional computing. This choice has the advantage that all the operations in the system, namely creating the knowledge base and evaluating the questions against it, are differentiable, thereby making the system easily trainable in an end-to-end fashion.\n\nIntroduction\n\nVisual Question Answering (VQA) or Visual Turing Test are terms that refer to a task in which a machine is provided with a picture and a question about a picture and the machine is asked to return the answer to the question. Such tasks have become popular in the last year thanks to the emergence of several datasets containing images, with associated questions and answers [8,1]. Classical deep neural network approaches face these tasks by training architectures composed of several parts. In these architectures there are often a RNN (often an LSTM) for encoding the question and for producing the answer, and a CNN for analyzing the image [2,7]. The main idea behind such architectures is to project the question and the image into a relatively low dimensional space where the two are compared. These approaches give impressive results when the question is about simple properties of the image like \"What is the color of the bus?\". However the results become worse when the question is more complex and involves verifying one or more relations among objects in the picture (\"Is the fruit in the plate different from the one in the basket?\"). Another group of approaches builds architectures with a perceiver and an evaluator [5]. The perceiver receives the image as input and has to build the knowledge base relative to the input. The evaluator, executes the question against the knowledge base to produce an answer. The disadvantage of such approaches is that the computations performed by the different parts of the system are of very different nature, leading to cumbersome architectures that are complex to train in an end-to-end fashion. The main contribution of this work is to show that, at least in the simple case we propose, it is possible to build an architecture where the evaluator performs computations of the same kind as the perceiver. It is for this reason that the architecture can be easily trained in an end-to-end fashion. In particular 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. Figure 1: Some images in the dataset in our architecture we will exploit the properties of hyper-dimensional computing (HD-computing) [4]. It is well known in fact that, by defining vectors in a high-dimensional space (HD-vectors), it is possible to store and then retrieve information from these vectors with very simple operations [4]. We will use HD-vectors to store the values of logical constants, and also the values of relations among these constants that will constitute the knowledge base of the system. In our architecture a FFW network will be asked to associate to each image in the input the HD-vector describing the image. The HD-vector returned as output can then be queried to extract answers to given questions, with such questions consisting in sets of differentiable operations on the HD-vectors. The parameters of the network will be updated with a gradient descent procedure in order to minimize the number of wrong answers. In the present paper we prove that this architecture can be successfully trained on a simple VQA task. The paper is organized as follows, in section 2 we present the dataset, in 3 we show how it is possible to encode the knowledge base in an HD-vector and then to retrieve information from it. Finally in section 4 we describe the training procedure and comment on the results of the tests.\n\n\nDataset\n\nThe dataset is composed of 28x28 RGB images. In each image there are two geometrical figures of four possible colors. The geometrical figures can be in four different positions in the image, namely top-left, top-right, bottom-left, bottom-right. The colors and shapes used are the following:\n\n\u2022 colors: red, green, magenta, orange;\n\n\u2022 shapes: circle, square, triangle, cross.\n\nAll possible images containing two figures were created. A sample of the images in the dataset is presented in Figure 1. The number of total images was 3072. 30% of these images were used as test set, and the rest of the images were used as training set.\n\n\nHD representation of the knowledge base\n\nEach picture in the dataset can be described in terms of the following set of natural language concepts: \"position\", \"color\", \"shape\", \"top-left\", \"top-right\", \"bottom-left\", \"bottom-right\", \"red\", \"green\", \"magenta\", \"orange\", \"circle\", \"square\", \"triangle\", \"cross\". For example the first picture of Figure  1 can be described by the following sentence: \"In position top-left there is a shape of type square and color magenta; and in position top-right there is a shape of type square and color green\". In the following we show how by using a method known as HD-computing, it is possible to store and retrieve the information contained in this sentence in one vector [3,6]. We will take the HD-vectors to be 1000 dimensional vectors where each component is randomly chosen to be 1 or -1. We will label HD-vectors using letters in italics, like k, m, n. Let's define two operations over the HD-vectors: entangle and grouping. Entangle is defined as follows:\n\u2297 : k = m \u2297 n \u21d4 k i = XOR(m i , n i ) \u2200i \u2208 {1, . . . , 1000}(1)\nPlease notice that XOR = (XOR) \u22121 , so we will sometimes refer to the previous operation also as dis-entangle. Grouping is defined as follows:\n\u2295 : k = m \u2295 n \u21d4 k i = m i + n i \u2200i \u2208 {1, . . . , 1000}(2)\nAlso it will be useful to define a distance between two vectors. We choose as distance the following function:\ncos(m, n) = i (m i \u00b7 n i ) ( i m 2 i ) \u00b7 ( i n 2 i )(3)\nWe associate each of the concepts listed in the previous paragraph with a random HD-vector. The HD-vector representing a concept will have the same name as the concept but it will be written in italics. So for example to the concept \"color\" will correspond the HD-vector color.\n\nTo each image in the dataset we can associate an HD-vector in the following way that we will illustrate with an example. Consider the first picture in Figure 1. The natural language description of the picture is the following: \"In the position top-left there is a shape of type square and color magenta. And in the position top-right there is a shape of type square and color green\". To such a description we will associate the following HD-vector:\nm = top\u2212lef t \u2297 (shape \u2297 square \u2295 color \u2297 magenta) \u2295 top\u2212right \u2297 (shape \u2297 square \u2295 color \u2297 green)(4)\nIt can be shown that the information stored in the vector m can be retrieved. For example to retrieve the information about the shape in position \"top-left\" we can dis-entangle the vector m with the vector top\u2212lef t and then with the vector shape. The resulting vector will be much closer to the vector square than to any of the other three vectors: triangle, circle and cross.\n\nFollowing the previous example we associated to each image I j in the dataset an HD-vector m j , building the following dataset:\nD = {I j , m j } 3072 j=1 (5)\n\nQuerying the knowledge base\n\nThe vectors m j just defined contain information about the pictures and can be queried to retrieve some of the properties of the picture. Querying an HD-vector consists in applying a set of operations in a specific sequence. In the following we will present the queries we used to train our architecture. The queries will be presented at first in natural language followed by the corresponding set of operations in the space of HD-vectors. To do so let's first define the set Positions:\nP ositions = {top\u2212lef t, top\u2212right, bottom\u2212lef t, bottom\u2212right}(6)\nIn the following the questions. \n\nQuestion 5: \"Is the shape in position top-left the same as the one in top-right?\":\ncos( shape \u2297 top\u2212lef t \u2297 m, shape \u2297 top\u2212right \u2297 m) > 0.5.(11)\nEach of the previous questions has a positive or a negative answer for each of the pictures in the dataset. So to each picture I j we will associate five numbers, namely q j 1 , q j 2 , q j 3 , q j 4 , q j 5 . The number q j 1 will be equal to 1 if the answer of Question 1 for the j\u2212th picture is true, otherwise q 1 j will be 0. We will use these values during the training of the network as target values. So let's add these values to the dataset D previously defined:\nD = {I j , m j , q j 1 , q j 2 , q j 3 , q j 4 , q j 5 } 3072 j=1(12)\n\nTraining and Testing\n\nWe trained a FFW network with two hidden layers of 200 rectified linear units each. The network was asked to return as output the HD-vector describing the picture. For this reason the output of the network was a layer of 1000 nodes with hyperbolic tangent as activation function.\n\nThe network was trained in order to associate to each input image an HD-vector such that when the vector was queried, it returned the correct information about the image. For this reason the error function that we minimized during the training was composed of a term for each of the questions.\n\nIn particular for each equation ( 7,8,9,10,11 ) we took the left side of the equation and computed its value for each image in the dataset by substituting in the equation the term m with the output of the network net(I j ) (here net is the function implemented by the network). The result of the computation was forced to be closer to 1 if the answer to the question for the picture I j was true, otherwise the result of the computation was forced to be closer to 0. Let's consider for example Question 1 and its equation (eq. 7). Here we defined the error term E 1 as:\nE 1 = j pos cos circle , shape \u2297 pos \u2297 net(I j ) \u2212 q j 1 2(13)\nIn the same way we defined an error term for each of the questions in the previous paragraph. We called E 2 , E 3 , E 4 , E 5 the error terms relative to Question 2, Question 3, Question 4 and Question 5.\n\nThe error function that we minimized during the training was the following sum:\nE = 5 i=1 E i(14)\nWe developed two kinds of test for the network. In a first test the network was asked to answer the questions used during the training, but on new data. In the second test the network was asked to answer new questions that were not used in the training. In the first test the network was tested on 30% of the dataset, on data that were not used during the training. For each example in the test set we evaluated the answer to each of the questions by substituting the output of the net net(I j ) with the vector m in the equation representing the question, assuming the answer to be true if the inequality was respected, false otherwise. The network reached an accuracy of 100% on all the questions. On the second experiment we tested the network on new questions. In particular we asked questions similar to question 7, but relative to the three other shapes: \"square\", \"triangle\" and \"cross\". In these cases we obtained accuracy values of 72%, 69% and 60%. Interestingly the worst performance was obtained on the \"cross\" shape, which was the shape that was never used in any of the questions in the training.\n\n\nConclusions\n\nIn this paper we presented an architecture for VQA that uses HD-computing to encode the knowledge base and evaluating a query against it. This choice makes the system easy to train in an end-to-end fashion. The system proved to work very well and to generalize well on a simple task. Further experiments are needed to test the architecture on more challenging and natural benchmarks.\n\nQuestion 1 :\n1\"Is there a circle in the picture?\" pos cos(circle , shape \u2297 pos \u2297 m) > 0.5, pos \u2208 P ositions. (7) Question 2: \"Is there the color green?\" pos cos(green , color \u2297 pos \u2297 m) > 0.5, pos \u2208 P ositions. (8) Question 3: \"Is there a magenta triangle?\" pos cos(magenta , color\u2297pos\u2297m)\u00b7cos(triangle , shape\u2297pos\u2297m) > 0.25, pos \u2208 P ositions. (9) Question 4: \"Is there a square in the bottom-left?\": cos(square , shape \u2297 bottom\u2212lef t \u2297 m) > 0.5.\nAcknowledgmentsThis work was funded by ERC Advanced Grant Number 323674 \"FEEL\" and ERC Proof of Concept Grant Number 692765 \"FeelSpeech\".\nVqa: Visual question answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, Devi Parikh, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zit- nick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE International Conference on Computer Vision, pages 2425-2433, 2015.\n\nAre you talking to a machine? dataset and methods for multilingual image question. Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, Wei Xu, Advances in Neural Information Processing Systems. Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. Are you talking to a machine? dataset and methods for multilingual image question. In Advances in Neural Information Processing Systems, pages 2296-2304, 2015.\n\nAditya Joshi, Johan Halseth, Pentti Kanerva, arXiv:1412.7026Language recognition using random indexing. arXiv preprintAditya Joshi, Johan Halseth, and Pentti Kanerva. Language recognition using random indexing. arXiv preprint arXiv:1412.7026, 2014.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Pentti Kanerva, Cognitive Computation. 12Pentti Kanerva. Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Cognitive Computation, 1(2):139-159, 2009.\n\nJointly learning to parse and perceive: Connecting natural language to the physical world. Jayant Krishnamurthy, Thomas Kollar, Transactions of the Association for Computational Linguistics. 1Jayant Krishnamurthy and Thomas Kollar. Jointly learning to parse and perceive: Connecting natural language to the physical world. Transactions of the Association for Computational Linguistics, 1:193-206, 2013.\n\nA robust and energy-efficient classifier using brain-inspired hyperdimensional computing. Abbas Rahimi, Pentti Kanerva, Jan M Rabaey, Proceedings of the 2016 International Symposium on Low Power Electronics and Design. the 2016 International Symposium on Low Power Electronics and DesignACMAbbas Rahimi, Pentti Kanerva, and Jan M. Rabaey. A robust and energy-efficient classifier using brain-inspired hyperdimensional computing. In Proceedings of the 2016 International Symposium on Low Power Electronics and Design. ACM, 2016.\n\nImage question answering: A visual semantic embedding model and a new dataset. Mengye Ren, Ryan Kiros, Richard Zemel, Proc. Advances in Neural Inf. Advances in Neural Inf15Mengye Ren, Ryan Kiros, and Richard Zemel. Image question answering: A visual semantic embedding model and a new dataset. Proc. Advances in Neural Inf. Process. Syst, 1(2):5, 2015.\n\nVisual madlibs: Fill in the blank description generation and question answering. Licheng Yu, Eunbyung Park, C Alexander, Tamara L Berg, Berg, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionLicheng Yu, Eunbyung Park, Alexander C Berg, and Tamara L Berg. Visual madlibs: Fill in the blank description generation and question answering. In Proceedings of the IEEE International Conference on Computer Vision, pages 2461-2469, 2015.\n", "annotations": {"author": "[{\"end\":231,\"start\":99},{\"end\":363,\"start\":232},{\"end\":492,\"start\":364}]", "publisher": null, "author_last_name": "[{\"end\":116,\"start\":109},{\"end\":252,\"start\":234},{\"end\":384,\"start\":376}]", "author_first_name": "[{\"end\":108,\"start\":99},{\"end\":233,\"start\":232},{\"end\":373,\"start\":364},{\"end\":375,\"start\":374}]", "author_affiliation": "[{\"end\":230,\"start\":146},{\"end\":362,\"start\":278},{\"end\":491,\"start\":407}]", "title": "[{\"end\":96,\"start\":1},{\"end\":588,\"start\":493}]", "venue": null, "abstract": "[{\"end\":1162,\"start\":590}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1555,\"start\":1552},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1557,\"start\":1555},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1824,\"start\":1821},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1826,\"start\":1824},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2410,\"start\":2407},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3368,\"start\":3365},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3567,\"start\":3564},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5926,\"start\":5923},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5928,\"start\":5926},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9952,\"start\":9948},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9954,\"start\":9952},{\"end\":9956,\"start\":9954},{\"end\":9959,\"start\":9956},{\"end\":9963,\"start\":9959}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":12809,\"start\":12363}]", "paragraph": "[{\"end\":4567,\"start\":1178},{\"end\":4870,\"start\":4579},{\"end\":4910,\"start\":4872},{\"end\":4954,\"start\":4912},{\"end\":5210,\"start\":4956},{\"end\":6212,\"start\":5254},{\"end\":6419,\"start\":6277},{\"end\":6588,\"start\":6478},{\"end\":6922,\"start\":6645},{\"end\":7372,\"start\":6924},{\"end\":7851,\"start\":7474},{\"end\":7981,\"start\":7853},{\"end\":8528,\"start\":8042},{\"end\":8628,\"start\":8596},{\"end\":8712,\"start\":8630},{\"end\":9246,\"start\":8775},{\"end\":9619,\"start\":9340},{\"end\":9914,\"start\":9621},{\"end\":10485,\"start\":9916},{\"end\":10753,\"start\":10549},{\"end\":10834,\"start\":10755},{\"end\":11963,\"start\":10853},{\"end\":12362,\"start\":11979}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6276,\"start\":6213},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6477,\"start\":6420},{\"attributes\":{\"id\":\"formula_2\"},\"end\":6644,\"start\":6589},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7473,\"start\":7373},{\"attributes\":{\"id\":\"formula_4\"},\"end\":8011,\"start\":7982},{\"attributes\":{\"id\":\"formula_5\"},\"end\":8595,\"start\":8529},{\"attributes\":{\"id\":\"formula_7\"},\"end\":8774,\"start\":8713},{\"attributes\":{\"id\":\"formula_8\"},\"end\":9316,\"start\":9247},{\"attributes\":{\"id\":\"formula_9\"},\"end\":10548,\"start\":10486},{\"attributes\":{\"id\":\"formula_10\"},\"end\":10852,\"start\":10835}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1176,\"start\":1164},{\"attributes\":{\"n\":\"2\"},\"end\":4577,\"start\":4570},{\"attributes\":{\"n\":\"3\"},\"end\":5252,\"start\":5213},{\"attributes\":{\"n\":\"3.1\"},\"end\":8040,\"start\":8013},{\"attributes\":{\"n\":\"4\"},\"end\":9338,\"start\":9318},{\"attributes\":{\"n\":\"5\"},\"end\":11977,\"start\":11966},{\"end\":12376,\"start\":12364}]", "table": null, "figure_caption": "[{\"end\":12809,\"start\":12378}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3239,\"start\":3231},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5075,\"start\":5067},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5565,\"start\":5556},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7083,\"start\":7075}]", "bib_author_first_name": "[{\"end\":12989,\"start\":12980},{\"end\":13006,\"start\":12997},{\"end\":13022,\"start\":13016},{\"end\":13035,\"start\":13027},{\"end\":13051,\"start\":13046},{\"end\":13067,\"start\":13059},{\"end\":13081,\"start\":13077},{\"end\":13547,\"start\":13540},{\"end\":13559,\"start\":13553},{\"end\":13568,\"start\":13565},{\"end\":13582,\"start\":13575},{\"end\":13593,\"start\":13590},{\"end\":13603,\"start\":13600},{\"end\":13898,\"start\":13892},{\"end\":13911,\"start\":13906},{\"end\":13927,\"start\":13921},{\"end\":14273,\"start\":14267},{\"end\":14590,\"start\":14584},{\"end\":14612,\"start\":14606},{\"end\":14992,\"start\":14987},{\"end\":15007,\"start\":15001},{\"end\":15020,\"start\":15017},{\"end\":15022,\"start\":15021},{\"end\":15511,\"start\":15505},{\"end\":15521,\"start\":15517},{\"end\":15536,\"start\":15529},{\"end\":15868,\"start\":15861},{\"end\":15881,\"start\":15873},{\"end\":15889,\"start\":15888},{\"end\":15907,\"start\":15901},{\"end\":15909,\"start\":15908}]", "bib_author_last_name": "[{\"end\":12995,\"start\":12990},{\"end\":13014,\"start\":13007},{\"end\":13025,\"start\":13023},{\"end\":13044,\"start\":13036},{\"end\":13057,\"start\":13052},{\"end\":13075,\"start\":13068},{\"end\":13088,\"start\":13082},{\"end\":13551,\"start\":13548},{\"end\":13563,\"start\":13560},{\"end\":13573,\"start\":13569},{\"end\":13588,\"start\":13583},{\"end\":13598,\"start\":13594},{\"end\":13606,\"start\":13604},{\"end\":13904,\"start\":13899},{\"end\":13919,\"start\":13912},{\"end\":13935,\"start\":13928},{\"end\":14281,\"start\":14274},{\"end\":14604,\"start\":14591},{\"end\":14619,\"start\":14613},{\"end\":14999,\"start\":14993},{\"end\":15015,\"start\":15008},{\"end\":15029,\"start\":15023},{\"end\":15515,\"start\":15512},{\"end\":15527,\"start\":15522},{\"end\":15542,\"start\":15537},{\"end\":15871,\"start\":15869},{\"end\":15886,\"start\":15882},{\"end\":15899,\"start\":15890},{\"end\":15914,\"start\":15910},{\"end\":15920,\"start\":15916}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3180429},\"end\":13455,\"start\":12948},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":209217},\"end\":13890,\"start\":13457},{\"attributes\":{\"doi\":\"arXiv:1412.7026\",\"id\":\"b2\"},\"end\":14140,\"start\":13892},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":733980},\"end\":14491,\"start\":14142},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":10250712},\"end\":14895,\"start\":14493},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":9812826},\"end\":15424,\"start\":14897},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":78798},\"end\":15778,\"start\":15426},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7142199},\"end\":16282,\"start\":15780}]", "bib_title": "[{\"end\":12978,\"start\":12948},{\"end\":13538,\"start\":13457},{\"end\":14265,\"start\":14142},{\"end\":14582,\"start\":14493},{\"end\":14985,\"start\":14897},{\"end\":15503,\"start\":15426},{\"end\":15859,\"start\":15780}]", "bib_author": "[{\"end\":12997,\"start\":12980},{\"end\":13016,\"start\":12997},{\"end\":13027,\"start\":13016},{\"end\":13046,\"start\":13027},{\"end\":13059,\"start\":13046},{\"end\":13077,\"start\":13059},{\"end\":13090,\"start\":13077},{\"end\":13553,\"start\":13540},{\"end\":13565,\"start\":13553},{\"end\":13575,\"start\":13565},{\"end\":13590,\"start\":13575},{\"end\":13600,\"start\":13590},{\"end\":13608,\"start\":13600},{\"end\":13906,\"start\":13892},{\"end\":13921,\"start\":13906},{\"end\":13937,\"start\":13921},{\"end\":14283,\"start\":14267},{\"end\":14606,\"start\":14584},{\"end\":14621,\"start\":14606},{\"end\":15001,\"start\":14987},{\"end\":15017,\"start\":15001},{\"end\":15031,\"start\":15017},{\"end\":15517,\"start\":15505},{\"end\":15529,\"start\":15517},{\"end\":15544,\"start\":15529},{\"end\":15873,\"start\":15861},{\"end\":15888,\"start\":15873},{\"end\":15901,\"start\":15888},{\"end\":15916,\"start\":15901},{\"end\":15922,\"start\":15916}]", "bib_venue": "[{\"end\":13157,\"start\":13090},{\"end\":13657,\"start\":13608},{\"end\":13994,\"start\":13952},{\"end\":14304,\"start\":14283},{\"end\":14682,\"start\":14621},{\"end\":15114,\"start\":15031},{\"end\":15572,\"start\":15544},{\"end\":15989,\"start\":15922},{\"end\":13211,\"start\":13159},{\"end\":15184,\"start\":15116},{\"end\":15596,\"start\":15574},{\"end\":16043,\"start\":15991}]"}}}, "year": 2023, "month": 12, "day": 17}