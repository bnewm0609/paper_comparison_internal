{"id": 222278549, "updated": "2023-10-06 10:36:53.164", "metadata": {"title": "Reversible Watermarking in Deep Convolutional Neural Networks for Integrity Authentication", "authors": "[{\"first\":\"Xiquan\",\"last\":\"Guan\",\"middle\":[]},{\"first\":\"Huamin\",\"last\":\"Feng\",\"middle\":[]},{\"first\":\"Weiming\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Hang\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Jie\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Nenghai\",\"last\":\"Yu\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": 4, "day": 9}, "abstract": "Deep convolutional neural networks have made outstanding contributions in many fields such as computer vision in the past few years and many researchers published well-trained network for downloading. But recent studies have shown serious concerns about integrity due to model-reuse attacks and backdoor attacks. In order to protect these open-source networks, many algorithms have been proposed such as watermarking. However, these existing algorithms modify the contents of the network permanently and are not suitable for integrity authentication. In this paper, we propose a reversible watermarking algorithm for integrity authentication. Specifically, we present the reversible watermarking problem of deep convolutional neural networks and utilize the pruning theory of model compression technology to construct a host sequence used for embedding watermarking information by histogram shift. As shown in the experiments, the influence of embedding reversible watermarking on the classification performance is less than 0.5% and the parameters of the model can be fully recovered after extracting the watermarking. At the same time, the integrity of the model can be verified by applying the reversible watermarking: if the model is modified illegally, the authentication information generated by original model will be absolutely different from the extracted watermarking information.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2104.04268", "mag": "3092793669", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2104-04268", "doi": "10.1145/3394171.3413729"}}, "content": {"source": {"pdf_hash": "ab349aab7ecec6b541b366ee066022a206fe4d21", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2104.04268v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2104.04268", "status": "GREEN"}}, "grobid": {"id": "1862d6f29d082aca7830a1a4d22e98c9d3ab51d0", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ab349aab7ecec6b541b366ee066022a206fe4d21.txt", "contents": "\nReversible Watermarking in Deep Convolutional Neural Net-works for Integrity Authentication\nOctober 12-16, 2020. 2020. October 12-16, 2020\n\nXiquan Guan \nHuamin Feng fenghm@besti.edu.cn \nWeiming Zhang zhangwm@ustc.edu.cn \nHang Zhou \nJie Zhang \nNenghai Yu \nWeiming Zhang \nNenghai Yu \nXiquan Guan \nHuamin Feng \nWeiming Zhang \nHang Zhou \nJie Zhang \nNeng-Hai Yu \n\nBeijing Electronic Science and Technology Institute\nUniversity of Science and Technology of China\nUniversity of Science and Technology\nof China\n\n\nUniversity of Science and Technology\nof China\n\n\nUniversity of Science and Technology\nof China\n\n\nUniversity of Science and Technology\nof China\n\nReversible Watermarking in Deep Convolutional Neural Net-works for Integrity Authentication\n\nProceedings of the 28th ACM Interna-tional Conference on Multimedia (MM '20)\nthe 28th ACM Interna-tional Conference on Multimedia (MM '20)Seattle, WA, USA; Seattle, WAOctober 12-16, 2020. 2020. October 12-16, 2020ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00 USA. ACM, New York, NY, USA, 8 pages. https://Reversible watermarkingConvolutional neural networksSecurityIntegrity authentication\nDeep convolutional neural networks have made outstanding contributions in many fields such as computer vision in the past few years and many researchers published well-trained network for downloading. But recent studies have shown serious concerns about integrity due to model-reuse attacks and backdoor attacks. In order to protect these open-source networks, many algorithms have been proposed such as watermarking. However, these existing algorithms modify the contents of the network permanently and are not suitable for integrity authentication. In this paper, we propose a reversible watermarking algorithm for integrity authentication. Specifically, we present the reversible watermarking problem of deep convolutional neural networks and utilize the pruning theory of model compression technology to construct a host sequence used for embedding watermarking information by histogram shift. As shown in the experiments, the influence of embedding reversible watermarking on the classification performance is less than \u00b10.5% and the parameters of the model can be fully recovered after extracting the watermarking. At the same time, the integrity of the model can be verified by applying the reversible watermarking: if the model is modified illegally, the authentication information generated by original model will be absolutely different from the extracted watermarking information.CCS CONCEPTS\u2022 Security and privacy \u2192 Authentication.\n\nINTRODUCTION\n\nDeep convolutional neural networks (CNNs) have obtained significant achievements in computer vision recently such as image classification [5], target tracking [9] and automatic driving [2]. However, the structures of the models are increasingly complex and the training of deep neural network models is difficult: several weeks are essential for a deep ResNet (ResNet152) with GPUs on ImageNet [5]. As a result, a large number of trained deep learning models have been published on the website to help people reproduce the results or improve the performance of networks by fine-tuning. During the spread of these trained models, illegal tampering has become an important issue threatening the security of the shared models. A classical method is backdoor attacks on CNNs [4]. The backdoor is defined as a hidden pattern injected into a deep neural network model by modifying the parameters while training. The backdoor does not affect the model's performance on clean inputs, but forces the model to produce unexpected behavior if and only if a specific input is applied. Besides, model-reuse attacks [7] also threaten the networks. These illegal tampering will leave fatal flaws and reduce the accuracy of the trained model. Once these \"infected\" parent models are utilized for training, the flaws will spread like viruses in the child models and if these \"infected\" child models are applied in financial or security field, the flaws are likely to be exploited, which will cause destructive impact. Therefore, to ensure there is no illegal tampering on the model, that is, integrity authentication of the model, is a significant research content of model application and security.\n\nAimed at the security of models, there are two main protecting categories against illegal tampering: defense and authentication. Defense focuses on detection and erasure. In these methods, all models are assumed to have been tampered with illegally. Taking the backdoors defense as an example, Wang et al. [21] proposed Neuron Cleanse and scanned all the model output labels to infer the potential hidden triggers. Chen et al. [3] applied Activation Clustering to detect data maliciously inserted into the training set for injecting backdoors. Liu et al. [10] proposed Fine-Pruning to remove backdoor triggers by pruning redundant neurons. Most of these methods detect the backdoors passively based on the characteristics of backdoors themselves and may easily lead to missing alarm and false alarm, which will impact the performance of models after utilizing passive defense category, especially for \"clean\" models.\n\nAnother protection category is authentication which is realized by embedding some meaningful information artificially such as watermarking of CNNs. According to whether the internal details of the model are known to the public, the model watermarking can be roughly categorized into two types: white-box watermarking and black-box watermarking. White-box watermarking embeds the watermarking information in the model internals such as weights and bias, which assumes that the internal details are public. Uchida et al. [20] propose the first CNNs watermarking technique. They choose the weights of a specific layer to embed a binary watermarking in the cover model by adding a regularization term to the loss function in the training process. Besides, Rouhani et al. [13] embed the watermarking in the probability density function of the data abstraction obtained in different layers of the model. Blackbox watermarking embeds the watermarking into the model which only has application programming interface (API) access by choosing a set of key pairs to alter the decision boundary of the cover model. Yossi et al. [1] utilize the images with triggers and the corresponding key labels to retrain the cover model. Zhang et al. [24] propose three different generation methods of watermarking key images including choosing images in another unrelated dataset, superimposing some images from training data with additional meaning content and random noise images. Very recently, Zhang et al. [23] provided a watermarking framework to protect the image processing networks in black-box way.\n\nHowever, these watermarking techniques are all irreversible. In the embedding process, the irreversible watermarking can only reduce the impact on the performance of the original model as much as possible, but this kind of watermarking still permanently modifies the internal parameters and destroys the integrity of the model. Therefore, this irreversible watermarking is unacceptable for integrity authentication. In order to achieve model integrity authentication, we need to propose a method which can not only embed watermarking information in the model, but also completely recover the original model parameters after extracting the watermarking, which is much more important in the models of military domain, medical domain, law application and so on. Inspired by the digital image reversible watermarking techniques, which can recover the carrier after extracting the watermarking, we proposed the first reversible model watermarking for integrity authentication of CNNs.\n\nGenerally speaking, nearly all reversible watermarking algorithms consist of two steps. First, a host sequence with a small entropy should be generated for embedding, i.e., a sharp histogram achieved by prediction errors [14]. Second, users embed the watermarking information into the host sequence by specific coding theories such as difference expansion [19], histogram shift [12] and recursive coding [25]. With the development of the techniques, the coding theories have reached the optimal. So how to construct a host sequence with lower entropy is a significant research goal for reversible watermarking in images. At present, the main way of constructing host sequence is using the correlation of image pixels. Nevertheless, the characteristics of parameters in CNNs are totally different from pixels in images. Due to the incomprehensibility of the CNNs, the correlation of parameters can not be described. At the same time, the format of the parameters are different between CNNs and images. As a result, the traditional reversible watermarking methods for images can not be applied to the model directly and it is crucial to construct the host sequence which is suitable for CNNs.\n\nTo this end, we propose a CNNs watermarking method based on the pruning theory of model compression to construct the host sequence for reversible watermarking embedding. Besides, we propose a framework to realize the reversible watermarking embedding of CNNs by utilizing the coding theory learning from the images. In experiments, we take the classification networks as examples to show the effectiveness of reversible watermarking. The results of model integrity authentication is also shown in our paper. The contributions of this paper are summarized as follows:\n\n(1) We present a novel problem: embedding reversible watermarking into CNNs for integrity authentication. \n\n\nREVERSIBLE WATERMARKING OF CNNS 2.1 Problem Formulation\n\nFor the convenience of description, we consider the convolution layers C = {C 1 , C 2 , \u00b7 \u00b7 \u00b7 , C } of CNN model M. We use a triplet C = \u27e8L , W , * \u27e9 to define the\u210e convolution layer, where L \u2208 R \u00d7\u210e\u00d7 is the input tensor of layer and W \u2208 R \u00d7 \u00d7 \u00d7 is the weights of all filters in layer . The * denotes the convolution operation. and denote the number of input channels and output channels respectively. \u210e and denote the height and width of input and is the size of convolution kernel. The target of reversible watermarking embedding is to embed a -bit vector \u2208 {0, 1} , which is encrypted as a watermarking before, into M and obtain the marked model M \u2032 . So the task can be described as following:\nM \u2032 = (M, ) (M, ) = (M \u2032 )(1)\nwhere (\u00b7) and (\u00b7) present the embedding algorithm and extraction algorithm, which are reversible for each other.\n\n\nProposed Framework\n\nIn this part, we briefly introduce the framework of reversible watermarking of CNNs. As shown in Fig. 1, the embedding process begins from original model (at the left of Fig. 1) and mainly includes three steps: host sequence construction, data preprocessing and watermarking embedding. The extraction process starts from  watermarked model (at the right of Fig. 1) and is inverse to the embedding process. Next, we take the watermarking embedding process as an example to introduce the specific implementation of our proposed method.\n\n\nHost Sequence Construction\n\nAs mentioned before, it is much more difficult to apply the traditional image reversible data hiding methods directly into CNN, that is to say, we must construct the host sequence for models utilizing their own characteristics. Inspired by the pruning theory in [11], we adopt the entropy to rank the importance of the parameters, and select the parameters with small entropy to construct the host sequence. Notice that in irreversible watermarking method and entropy-based pruning theory, convolution layers are used as targets. Therefore, we also consider the convolution layers only and utilize the weight parameters to construct the host sequence for reversible watermarking embedding.\n\nFor the convolution layer in model M, according to the regulations of CNN, each filter in layer corresponds to a single channel of its activation tensor L +1 , which is also the input of layer + 1.\n\nIn entropy-based channel pruning theory [11], the entropy should be calculated first to measure the importance of each channel. As a result, we first select images I = 1 , 2 , \u00b7 \u00b7 \u00b7 , from validation set as the model input. For image \u2208 I input L \u2208 R \u00d7\u210e\u00d7 and the filter of this layer W \u2208 R \u00d7 \u00d7 \u00d7 , a corresponding activation tensor L +1 , which is a \u00d7\u210e \u2032 \u00d7 \u2032 tensor, will be obtained obviously.\n\nSince the output feature map reflects the weights characteristics of this layer, we use the output of the layer as the basis for weight importance measurement. Here, we utilize global average pooling to convert the tensor into a vector as \u2208 R . Therefore, each channel of layer will get a score of image in this way. In order to calculate the entropy, we input the whole images in I to calculate the channel score and obtain a matrix F \u2208 R \u00d7 as following:\nF = 1 2 .\n. .\n\u225c F :,1 , F :,2 , \u00b7 \u00b7 \u00b7 , F :,(2)\nwhere is the channel number of output. For each channel \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , }, we take the distribution vector F :, as consideration to compute the entropy value. In order to get the frequency distribution, we first divide F :, into different bins and calculate the probability of each bin. Then, the entropy value can be calculated as follows:\n= \u2212 \u2211\ufe01 =1 log(3)\nwhere , = {1, 2, \u00b7 \u00b7 \u00b7 , } is the probability of bin and is the entropy of channel . It should be noticed that there is a log (\u00b7) function in the calculation formula of entropy Eq. (3), so the requirement of \u2260 0 must be satisfied. As a result, the compromise of the number of bins has to be considered. If we divide too much bins, some will become 0 and the entropy will be meaningless. On the contrary, if is too small, the entropy of each channel will be not reflected enough. In our method, we utilize the iteration to obtain the largest number of , which will ensure that the probability of each bin satisfies \u2260 0.\n\nFor the channels of layer , we can obtain a corresponding entropy sequence = { 1 , 2 , \u00b7 \u00b7 \u00b7 , }. According to the magnitude of entropy, we can sort the and obtain the ascending sequence = 1 , 2 , \u00b7 \u00b7 \u00b7 , and obtain an index of the importance of channels as = { 1 , 2 , \u00b7 \u00b7 \u00b7 , }. Here we select an integer < and utilize the channels corresponding to the top indexes in to construct the host sequence. As analyzed above, the smaller the entropy is, the less important the parameters are.\n\nThe filter weights W \u2208 R \u00d7 \u00d7 \u00d7 of layer can be rewritten as W = {W 1 , W 2 , \u00b7 \u00b7 \u00b7 , W }, where the elements of W belong to R \u00d7 \u00d7 . We can sort the W by the first indexes in the index sequence = { 1 , 2 , \u00b7 \u00b7 \u00b7 , } and obtain the sorting sequence W = W 1 , W 2 , \u00b7 \u00b7 \u00b7 , W . For each W \u2208 W , we define \u2208 R \u00d7 as the kernel weights where \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , } and, therefore,\nW = 1 , 2 , \u00b7 \u00b7 \u00b7 ,\n. In order to construct the host sequence more similar with an image, we rearrange W as :\n= 1 1 1 2 \u00b7 \u00b7 \u00b7 1 2 1 2 2 \u00b7 \u00b7 \u00b7 2 . . . . . . . . . . . . 1 2 \u00b7 \u00b7 \u00b7 \u00d7(4)\nNote that \u2208 R \u00d7 , so the can also be written as following:\n= 1,1 1,2 \u00b7 \u00b7 \u00b7 1, \u00d7 2,1 2,2 \u00b7 \u00b7 \u00b7 2, \u00d7 . . . . . . . . . . . . \u00d7 ,1 \u00d7 ,2 \u00b7 \u00b7 \u00b7 \u00d7 , \u00d7(5)\nwhere , \u2208 R. And here is taken as the host sequence for watermarking embedding.\n\n\nData Preprocessing\n\nAs mentioned above, we obtain the host sequence utilizing the pruning theory. However, all the elements in matrix are not integer, which can not be directly applied in the traditional image reversible data hiding method. As a result, in our framework, we intercept two digits from each element of and the range of these intercepted parameters is [\u221299, 99]. Then, we add to these intercepted parameters to adjust it to the appropriate range, that is, positive integer, where \u2208 Z is an adjustable parameter.\n\nIn addition to the number of interception digits, the location of interception should be considered. We assume the element , \u2208 as following:\n, = \u00b1 0. 00 \u00b7 \u00b7 \u00b7 0 1 2 \u00b7 \u00b7 \u00b7 ,(6)\nwhere \u2a7e 0, > 0 and , \u2208 Z. In Eq. (6), 1 denotes the first non-zero digit of , , 2 denotes the second non-zero digit of , and so on. For convenience, we define the\u210e non-zero digit of , , , as the\u210e significant digit. It should be noticed that for different elements of , the value of is different, that is, the position of the first significant digit is different.\n\nDue to the modification of the first significant digit 1 will cause a great influence on the value of , , we only consider modifying the digits from second significant digit to the last significant digit, namely, 2 , 3 , \u00b7 \u00b7 \u00b7 , . In order to obtain a larger embedding capacity, the theory of Kalker and Willems is considered in our method. In [22], the upper bound of embedding capacity under a given distortion constraint \u0394 was proposed as following:\n(\u0394) = maximize{ ( )} \u2212 ( ),(7)\nwhere and denote the host sequence and the marked sequence after embedding respectively and (\u00b7) denote entropy calculation function. According to the Eq. (7), the smaller the entropy of host sequence is, the larger the embedding capacity can be obtained. Thus, we calculate the entropy of all possible host sequences constructed by intercepting TWO different significant digits. Then we decide the position of the selected significant digit according to the values of these entropy.\n\nSpecifically, we take the\u210e convolution layer as an example. For all the elements of , , , we first select the second significant digit 2 and the third significant digit 3 . After adjusting the value by mentioned before, we construct optional host sequence 2,3 . Then we count the frequency and calculate the entropy of 2,3 as 2,3 . Similarly, we can obtain the entropy values 3,4 , 4,5 , \u00b7 \u00b7 \u00b7 , \u22121, . According to the method in [22], we choose the significant digit pairs, defined as ( , +1 ), corresponding to the minimum entropy to construct the host sequence.\n\nOnce we ensure the selection digits ( , +1 ), we can get the integer * , = \u00b1 +1 . It should be noticed that the symbols of * , and , are consistent, that is, if the symbol of , is positive, the symbol of , will be positive and vice versa. Then we can obtain , = * , + and the host sequence = ( , ) \u00d7 , \u00d7 after data processing.\n\n\nEmbedding and Extracting Strategy\n\nEmbedding: The integer host sequence generated above can be considered as a traditional grayscale image and we can utilize image reversible data hiding strategy to embed watermarking. In this paper, we choose histogram shift (HS) strategy [12]. The embedding process contains the two basic steps as following:\n\n(1) Histogram generation: for , \u2208 , we generate the histogram ( ) the same as image: counting the number of different elements in a matrix .\n\n(2) Histogram modification: we define the value in corresponding to the histogram peak as \u03a9 and the histogram valley (generally speaking, 0) as \u03a9 . Without loss of generality, in our framework, \u03a9 < \u03a9 . As mentioned in [16], the HS encoding algorithm embedding one bit can be described as following:\n\u2032 , = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 , + , , = \u03a9 , + 1, , \u2208 ( \u03a9 , \u03a9 ) , , \u2209 [ \u03a9 , \u03a9 ).(8)\nAs shown in Fig. 2 and Fig. 3 \n\u2032 , = \u00b1 0. 00 \u00b7 \u00b7 \u00b7 0 1 2 \u00b7 \u00b7 \u00b7 \u2032 \u2032 +1 \u00b7 \u00b7 \u00b7 ,(9)\nthen the elements in Eq. 5 can be replaced as \u2032 , and get the modified W \u2032 . According to the parameter and index sequence ,  [12]. Here the histogram on the left is the initial histogram, the histogram in the middle is generated by shifting the bins more than \u03a9 towards right by 1 to create a vacant bin for data embedding and the histogram on the right is the histogram embedded with watermark information utilizing HS. Without loss of generality, we assume that the number of binary 0 and the number of binary 1 to be embedded are equal. Figure 3: Mapping rule of histogram bins described in [12]: the watermark information is embedded into \u03a9 and the values bigger than \u03a9 in are shifted right while the values smaller than \u03a9 in are remained unchanged.\n\nwe can replace W 1 , W 2 , \u00b7 \u00b7 \u00b7 , W in W by W \u2032 1 , W \u2032 2 , \u00b7 \u00b7 \u00b7 , W \u2032 and obtain the update filter weights W \u2032 , that is, marked model M \u2032 .\n\nIt should be noted that the additional informations , , and should also be embedded in the the filters. Here we embed these bits into the last binary bit (converting parameters to binary numbers) of W \u2032 . Similar to the previous definition, we can define a matrix by arranging the parameters of all channels in order as following:\n= 1,1 1,2 \u00b7 \u00b7 \u00b7 1, \u00d7 2,1 2,2 \u00b7 \u00b7 \u00b7 2, \u00d7 . . . . . . . . . . . . \u00d7 ,1 \u00d7 ,2 \u00b7 \u00b7 \u00b7 \u00d7 , \u00d7(10)\nwhere , \u2208 R. Then, we convert , to binary number as , and replace the last bits of , by encrypted additional informations , , and . In order to keep the reversibility, we reserve a part of space in the head of watermarking information to store the original last bits information of those replaced , above.\n\nExtraction and Restoration: We first extract the additional informations , , and from the filter in layer . Then we construct the marked sequence \u2032 using the methods in 2.3 and 2.4.\n\nThen, the same as embedding process, we generate the histogram ( \u2032 ) and extract the embedded bit according to the following:\n= 1, \u2032 , = \u03a9 + 1 0, \u2032 , = \u03a9 ,(11)\nand after extracting the embedding bits, the original element can be recovered as:\n, = \u2032 , \u2212 1, \u2032 , \u2208 ( \u03a9 , \u03a9 ] \u2032 , \u2032 , \u2209 [ \u03a9 , \u03a9 ].(12)\nAs mentioned above, we can recover the original W and update the filters in layer to obtain the original model M.\n\n\nEXPERIMENTS\n\nIn this section, we firstly introduce the experimental settings (Sec.3.1) and compare the top-5 accuracy between our proposed method and irreversible watermarking technique modifying parameters directly (Sec.3.2). We then show the multi-embedding of reversible watermarking performance (Sec.3.3). Finally we show the process of integrity certification utilizing reversible watermarking (Sec.3.4).\n\n\nSettings\n\nFor experiments, we adopt three pretrained networks AlexNet [8], VGG19 [17], ResNet152 [5], DenseNet121 [6] and MobileNet [15] as the target models M, and utilize the ImageNet validation images dataset consists of 50, 000 color images in 1000 classes with 50 images per class to calculate the entropy of channels. For the process of host sequence construction, according to the relationship between depth of layer and model performance, we decide to choose the last three layers of these models to embed the reversible watermarking and choose the first = 128 channels in VGG19 and ResNet152, the first = 32 channels in DenseNet121, the first = 48, = 64, = 96 channels for the different layers in AlexNet and the first = 320, = 960, = 960 channels for the different layers in MobileNet to rearrange the weights parameters. The reason for the difference of value is that the weight tensors of different convolutions in different models are different. Besides, we choose = 128 as the adjustable parameter and = 2 as the selected significant digit position. The implementation is based on Python3.5 and MATLAB R2018a with the NVIDIA RTX 2080 Ti GPU.\n\n\nComparison with Non-reversible Methods\n\nFirst, we organize a comparison table according to the characteristics of irreversible watermarking and reversible watermarking as shown in Fig. 1. Here we divide the irreversible watermarking into two categories, one is robust reversible watermarking, the other is non-robust reversible watermarking, similar to image steganography.  For reversible watermarking, it is fragile, reversible and the capacity is medium. It is mainly used for integrity authentication. In contrast, irreversible watermarking is irreversible. For robust irreversible watermarking, it is robust, which is utilized for intellectual property protection. For non-robust irreversible watermarking, the capacity is large. Non-robust irreversible watermarking, which is also fragile similar to reversible watermarking, is usually used for covert communication. Since we do not consider robustness and the reversible watermarking is first proposed, we only choose the two types fragile watermarking, non-robust irreversible watermarking and reversible watermarking, for comparison in the next experiment.\n\nTo illustrate the universality of our reversible watermarking method (RW), we choose a non-robust irreversible watermarking method proposed by Song et al. [18]. They embedded watermarking information by least significant bit replacement (LSBR). In our experiments, we embed the watermarking in the selected layers and calculate the top-5 accuracy of classification. For convenience, we utilize I, II, III to represent the last layer, the second to last layer and the third to last layer. In order to make our comparative experiment more convincing, we first select the last three convolution layers of AlexNet to embed different sizes of watermark information to analyze the impact of the length of watermark information on the performance of the model. Then we choose the last three convolution layers of VGG19 and ResNet152 to embed the same size of watermark information to analyze the influence of different models on the performance of the model. Table 2, with same embedding bits in the same layer, the top-5 classification accuracies before or after embedding two types watermarking are almost equal (\u22120.4% \u223c +0.1%). Besides, the accuracies between LSBR and our proposed method are almost equal (\u22120.5% \u223c +0.2%). It should be noticed that our proposed method is reversible watermarking which can be extracted and maintain the model integrity. According to the results, embedding the reversible watermarking hardly affects the classification results of the model, which is much more different from image reversible watermarking. This can be explained in two ways. On the one hand, the modification has little influence on the value of the parameters. On the other hand, the number of parameters in these models is very large and the modification of parameters in model is limited. Besides, our method achieves the reversibility without affecting the performance of the model compared with non-robust irreversible watermarking.\n\n\nAs shown in\n\n\nMulti-layered Reversible Watermarking\n\nIn this part, we compare the classification performance of the models between single-layered watermarking embedding and multilayered watermarking embedding. For the multi-layered embedding, we modify the parameters of each selected layer respectively, and then merged them into a complete modification model.\n\nFirst, we choose AlexNet, VGG19 and ResNet152 to compare the effect of embedding watermark in different layers on the performance of the model. As shown in Table 3, the accuracies between clean model and multi-layered watermarking embedded model are almost equal (\u22120.3% \u223c +0.2%). Then, we choose DenseNet121 and MobileNet to compare the effect of embedding watermark in single layer and multiple layers. As shown in Table 4, the accuracies between clean model and embedded watermarking model are almost equal (\u22120.6% \u223c \u22120.1%). As analyzed above, the embedding of single-layered watermarking has little influence on the model performance, so whether we embed multi-layered watermarking or single-layered watermarking, the performance of the models do not change much, which provides the possibility to recover the tampered model by embedding more watermarking information of model parameters' characteristic in the future. Table 3 Top-5 Classification Accuracy on ImageNet: The results of multi-layered watermarking embedding in the last three layers of three classical classification models: AlexNet, VGG19, ResNet152.\n\n\nMode\n\nClassification  At the last of this subsection, we compared the difference between the original model and the reconstructed model after the extraction for the five models mentioned above. The results are shown in Table 5. Both the experimental results and the theoretical analysis can prove that our method is completely reversible, that is, the integrity of the model is preserved.\n\n\nIntegrity Authentication\n\nIn this part, we realize the integrity authentication applied reversible watermarking. First, we utilize a Hash algorithm SHA-256 (Secure Hash Algorithm 256) to obtain the characteristic of the whole model. Then, we embed the SHA-256 value into the convolution layer by our proposed reversible watermarking algorithm. Due to the the excellent characteristics of the Hash algorithm, no matter where the attacker modifies the model, the newly generated SHA-256 value will be different from the extracted SHA-256 value.\n\nAs shown in Fig. 4, Alice is the holder of the model and she regards the SHA-256 value as the watermarking 1 and embed it into the model by our reversible watermarking algorithm. Then she uploads her watermarked model to cloud server for others to download. Bob downloads Alice's model from cloud server but he does not know whether the model is complete (Unknown Model), so he extracts the watermarking (defined as 2 ) from the unknown model and calculates the SHA-256 3 from reconstructed model. It will be two cases here comparing 2 and 3 : (1) if the model is modified illegally by Mallory, then 2 \u2260 3 (top right of Fig. 4). (2) if the model is not modified, then 2 = 3 (bottom right of Fig. 4).\n\nFor our algorithm, we give a brief security analysis as following: We begin by presenting a definition for the security: the model is Integrity if it is impossible for an attacker to modify the model without being discovered. As mentioned in above, we use the SHA-256 value as the reversible watermarking to verify integrity shown in Fig. 4. Then the security of our method is reduced to the security of a cryptographic Hash algorithm (SHA-256) which is collisionresistant. Meanwhile, a security Hash function \u210e( ), where the domain is \u210e and the range is \u210e , is collision-resistant if it is difficult to find:\n\u210e( 1 ) = \u210e( 2 ) 1 , 2 \u2208 \u210e 1 \u2260 2 .(13)\nSince the Hash function of SHA-256 is collision-resistant up to now, the method for integrity authentication is secure.\n\nIn our experiments, we choose the last convolution layer of ResNet152 to embed SHA-256 value as watermarking information. All experiments have shown that no matter where we modify or erase the parameters, our method can detect that the model has been tampered with.\n\n\nCONCLUSION\n\nIn this paper, we present a new problem: embedding reversible watermarking into deep convolutional neural networks (CNNs) for integrity authentication. Since the state-of-art model watermarking techniques are irreversible and destroy the integrity of the model permanently, these methods are not suitable for integrity authentication. Inspired by the traditional image integrity authentication, we consider the reversible watermarking and apply it into CNNs. According to the characteristics of CNNs, we propose a method to construct the host sequence of trained model and formulate a framework to embed the reversible watermarking into CNNs by histogram shift. In the experiments, we demonstrate that our reversible watermarking in CNNs is effective and we utilize the reversible watermarking for integrity authentication in whole model.  In the future work, we will study how to determine the location where the model is modified and recover the modified parameters as much as possible by the extracted watermarking information. Furthermore, we just utilize our framework on CNNs, so we will research how to extend the reversible watermarking technique to other deep neural networks for integrity authentication.\n\n( 2 )\n2We propose a method to construct the host sequence of trained model and formulate a framework to embed the reversible watermarking into CNNs by histogram shift.(3) We perform comprehensive experiments in different models to show the performance of reversible watermarking on trained models.\n\nFigure 1 :\n1Reversible watermarking framework in CNNs.\n\nFigure 2 :\n2Illustration of Ni et al.'s method\n\nFigure 4 :\n4Integrity authentication protocol utilizing reversible watermarking of CNNs.\n\nTable 1\n1Comparison of Reversible Watermarking and Irreversible Watermarking: Qualitative comparison of two different watermarks.Watermarking \n\nReversible \nIrreversible \nRobust \nNon-robust \nFragility \n\u2713 \n\u2713 \nRobustness \n\u2713 \nReversibility \n\u2713 \nCapacity \nMedium \nSmall \nLarge \n\nApplication \nIntegrity \nIntellectual \nCovert \nauthentication property protection Communication \n\n\n\nTable 2\n2Top-5 Classification Accuracy on ImageNet: The comparison between our proposed method RW and LSBR embedded in the last three layers of three classical classification models: AlexNet, VGG19, ResNet152.Network Layer Clean Model Accuracy (%) \nMarked Model Accuracy (%) Length of Watermark (bits) \nLSBR [18] \nRW (ours) \n\nAlexNet \n\nIII \n75.9 \n\n75.7 \n75.7 \n12442 \nII \n76.0 \n75.8 \n49766 \nI \n75.8 \n75.6 \n22118 \n\nVGG19 \n\nIII \n81.1 \n\n80.9 \n81.2 \n88474 \nII \n81.1 \n81.1 \n88474 \nI \n81.0 \n80.8 \n88474 \n\nResNet152 \n\nIII \n85.9 \n\n85.5 \n85.7 \n88474 \nII \n85.5 \n86.0 \n88474 \nI \n85.6 \n85.9 \n88474 \n\n\n\nTable 4\n4Top-5 Classification Accuracy on ImageNet: The comparison between our proposed method RW embedded in different layers and clean models Model reconstruction error rate: Compare the consistency between the reconstructed model and the original model. A reconstruction rate of 0 indicates that the algorithm is completely reversible..Network \nLayer \nClean RW Length of Watermark \n(%) \n(%) \n(bits) \n\nDenseNet121 \n\nI \n80.4 \n\n80.3 \n5530 \nI&II \n80.0 \n11060 \nI&II&III \n80.2 \n16590 \n\nMobileNet \n\nI \n76.8 \n\n76.6 \n46080 \nI&II \n76.6 \n47376 \nI&II&III \n76.2 \n70416 \n\nTable 5 \nModel \nReconstruction error rate (%) \nSinge layer Multiple layers \nAlexNet \n0 \n0 \nVGG19 \n0 \n0 \nResNet152 \n0 \n0 \nDenseNet121 \n0 \n0 \nMobileNet \n0 \n0 \n\n\n\n\n!$ = !#?Calculate \nSHA-256 \n! \" \n\nReversible \nwatermarking \nalgorithm \nEmbed !\" \n\nOriginal Model \nWatermarked Model \n\nAlice \n\nModify illegally \n\nMallory \n\nDownload \n\nDownload \n\nCalculate \nSHA-256 \n! # \n\nReversible \nwatermarking \nalgorithm \n\nWatermarking \nSHA-256 !$ \n\nExtract \nwatermarking \n\nUnknown Model \n\n? \n\nNO \n\nReconstructed \nModel \nBob \n\nCalculate \nSHA-256 \n!# \n\nReversible \nwatermarking \nalgorithm \n\nWatermarking \nSHA-256 !$ \n\nExtract \nwatermarking \n\nUnknown Model \n\n\nACKNOWLEDGMENTS\nTurning your weakness into a strength: Watermarking deep neural networks by backdooring. Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, Joseph Keshet, 27th {USENIX} Security Symposium ({USENIX} Security 18. Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet. 2018. Turning your weakness into a strength: Watermarking deep neural net- works by backdooring. In 27th {USENIX} Security Symposium ({USENIX} Security 18). 1615-1631.\n\nOn the iterative refinement of densely connected representation levels for semantic segmentation. Arantxa Casanova, Guillem Cucurull, Michal Drozdzal, Adriana Romero, Yoshua Bengio, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsArantxa Casanova, Guillem Cucurull, Michal Drozdzal, Adriana Romero, and Yoshua Bengio. 2018. On the iterative refinement of densely connected represen- tation levels for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. 978-987.\n\nBryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, Biplav Srivastava, arXiv:1811.03728Detecting backdoor attacks on deep neural networks by activation clustering. arXiv preprintBryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. 2018. Detecting back- door attacks on deep neural networks by activation clustering. arXiv preprint arXiv:1811.03728 (2018).\n\nBadnets: Identifying vulnerabilities in the machine learning model supply chain. Tianyu Gu, Brendan Dolan-Gavitt, Siddharth Garg, arXiv:1708.06733arXiv preprintTianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733 (2017).\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770-778.\n\nDensely connected convolutional networks. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, Kilian Q Weinberger, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. 2017. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 4700-4708.\n\nModelreuse attacks on deep learning systems. Yujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, Ting Wang, Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security. the 2018 ACM SIGSAC Conference on Computer and Communications SecurityACMYujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, and Ting Wang. 2018. Model- reuse attacks on deep learning systems. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security. ACM, 349-363.\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in neural information processing systems. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifica- tion with deep convolutional neural networks. In Advances in neural information processing systems. 1097-1105.\n\nSiamrpn++: Evolution of siamese visual tracking with very deep networks. Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, Junjie Yan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionBo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, and Junjie Yan. 2019. Siamrpn++: Evolution of siamese visual tracking with very deep networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 4282-4291.\n\nFine-pruning: Defending against backdooring attacks on deep neural networks. Kang Liu, Brendan Dolan-Gavitt, Siddharth Garg, International Symposium on Research in Attacks, Intrusions, and Defenses. SpringerKang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. 2018. Fine-pruning: De- fending against backdooring attacks on deep neural networks. In International Symposium on Research in Attacks, Intrusions, and Defenses. Springer, 273-294.\n\nAn entropy-based pruning method for cnn compression. Jian-Hao Luo, Jianxin Wu, arXiv:1706.05791arXiv preprintJian-Hao Luo and Jianxin Wu. 2017. An entropy-based pruning method for cnn compression. arXiv preprint arXiv:1706.05791 (2017).\n\nReversible data hiding. Zhicheng Ni, Yun-Qing Shi, Nirwan Ansari, Wei Su, IEEE Transactions on circuits and systems for video technology. 16Zhicheng Ni, Yun-Qing Shi, Nirwan Ansari, and Wei Su. 2006. Reversible data hiding. IEEE Transactions on circuits and systems for video technology 16, 3 (2006), 354-362.\n\nDeepsigns: A generic watermarking framework for ip protection of deep learning models. Huili Bita Darvish Rouhani, Farinaz Chen, Koushanfar, arXiv:1804.00750arXiv preprintBita Darvish Rouhani, Huili Chen, and Farinaz Koushanfar. 2018. Deepsigns: A generic watermarking framework for ip protection of deep learning models. arXiv preprint arXiv:1804.00750 (2018).\n\nReversible watermarking algorithm using sorting and prediction. Vasiliy Sachnev, Hyoung Joong Kim, Jeho Nam, Sundaram Suresh, Yun Qing Shi, IEEE Transactions on Circuits and Systems for Video Technology. 19Vasiliy Sachnev, Hyoung Joong Kim, Jeho Nam, Sundaram Suresh, and Yun Qing Shi. 2009. Reversible watermarking algorithm using sorting and prediction. IEEE Transactions on Circuits and Systems for Video Technology 19, 7 (2009), 989-999.\n\nMobilenetv2: Inverted residuals and linear bottlenecks. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang- Chieh Chen. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 4510-4520.\n\nReversible Data Hiding: Advances in the Past Two Decades. Yun Qing Shi, Xiaolong Li, Xinpeng Zhang, Haotian Wu, Bin Ma, IEEE Access. 4Yun Qing Shi, Xiaolong Li, Xinpeng Zhang, Haotian Wu, and Bin Ma. 2016. Reversible Data Hiding: Advances in the Past Two Decades. IEEE Access 4 (2016), 1-1.\n\nKaren Simonyan, Andrew Zisserman, arXiv:1409.1556Very deep convolutional networks for large-scale image recognition. arXiv preprintKaren Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).\n\nMachine Learning Models that Remember Too Much. Congzheng Song, Thomas Ristenpart, Vitaly Shmatikov, the 2017 ACM SIGSAC Conference. Congzheng Song, Thomas Ristenpart, and Vitaly Shmatikov. 2017. Machine Learning Models that Remember Too Much. In the 2017 ACM SIGSAC Conference.\n\nReversible data embedding using a difference expansion. IEEE transactions on circuits and systems for video technology. 13Jun Tian. 2003. Reversible data embedding using a difference expansion. IEEE transactions on circuits and systems for video technology 13, 8 (2003), 890-896.\n\nEmbedding watermarks into deep neural networks. Yusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, Shin&apos;ichi Satoh, Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval. the 2017 ACM on International Conference on Multimedia RetrievalACMYusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, and Shin'ichi Satoh. 2017. Embedding watermarks into deep neural networks. In Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval. ACM, 269-277.\n\nBolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, Y Ben, Zhao, Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks. 0Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. 2019. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks (2019), 0.\n\nCapacity bounds and code constructions for reversible data-hiding. F M Willems, Kalker, IS&T/SPIE Proceedings, Security and Watermarking of Multimedia 19 Contents V 5020. FM Willems and T Kalker. 2003. Capacity bounds and code constructions for reversible data-hiding. IS&T/SPIE Proceedings, Security and Watermarking of Multimedia 19 Contents V 5020 (2003).\n\nModel Watermarking for Image Processing Networks. Jie Zhang, Dongdong Chen, Jing Liao, Han Fang, Weiming Zhang, Wenbo Zhou, Hao Cui, Nenghai Yu, AAAI. Jie Zhang, Dongdong Chen, Jing Liao, Han Fang, Weiming Zhang, Wenbo Zhou, Hao Cui, and Nenghai Yu. 2020. Model Watermarking for Image Processing Networks.. In AAAI. 12805-12812.\n\nProtecting intellectual property of deep neural networks with watermarking. Jialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, Marc Ph Stoecklin, Heqing Huang, Ian Molloy, Proceedings of the 2018 on Asia Conference on Computer and Communications Security. the 2018 on Asia Conference on Computer and Communications SecurityACMJialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, Marc Ph Stoecklin, Heqing Huang, and Ian Molloy. 2018. Protecting intellectual property of deep neural networks with watermarking. In Proceedings of the 2018 on Asia Conference on Computer and Communications Security. ACM, 159-172.\n\nImproving various reversible data hiding schemes via optimal codes for binary covers. Weiming Zhang, Biao Chen, Nenghai Yu, IEEE transactions on image processing. 21Weiming Zhang, Biao Chen, and Nenghai Yu. 2012. Improving various reversible data hiding schemes via optimal codes for binary covers. IEEE transactions on image processing 21, 6 (2012), 2991-3003.\n", "annotations": {"author": "[{\"end\":153,\"start\":141},{\"end\":186,\"start\":154},{\"end\":221,\"start\":187},{\"end\":232,\"start\":222},{\"end\":243,\"start\":233},{\"end\":255,\"start\":244},{\"end\":270,\"start\":256},{\"end\":282,\"start\":271},{\"end\":295,\"start\":283},{\"end\":308,\"start\":296},{\"end\":323,\"start\":309},{\"end\":334,\"start\":324},{\"end\":345,\"start\":335},{\"end\":358,\"start\":346},{\"end\":504,\"start\":359},{\"end\":552,\"start\":505},{\"end\":600,\"start\":553},{\"end\":648,\"start\":601}]", "publisher": null, "author_last_name": "[{\"end\":152,\"start\":148},{\"end\":165,\"start\":161},{\"end\":200,\"start\":195},{\"end\":231,\"start\":227},{\"end\":242,\"start\":237},{\"end\":254,\"start\":252},{\"end\":269,\"start\":264},{\"end\":281,\"start\":279},{\"end\":294,\"start\":290},{\"end\":307,\"start\":303},{\"end\":322,\"start\":317},{\"end\":333,\"start\":329},{\"end\":344,\"start\":339},{\"end\":357,\"start\":355}]", "author_first_name": "[{\"end\":147,\"start\":141},{\"end\":160,\"start\":154},{\"end\":194,\"start\":187},{\"end\":226,\"start\":222},{\"end\":236,\"start\":233},{\"end\":251,\"start\":244},{\"end\":263,\"start\":256},{\"end\":278,\"start\":271},{\"end\":289,\"start\":283},{\"end\":302,\"start\":296},{\"end\":316,\"start\":309},{\"end\":328,\"start\":324},{\"end\":338,\"start\":335},{\"end\":354,\"start\":346}]", "author_affiliation": "[{\"end\":503,\"start\":360},{\"end\":551,\"start\":506},{\"end\":599,\"start\":554},{\"end\":647,\"start\":602}]", "title": "[{\"end\":92,\"start\":1},{\"end\":740,\"start\":649}]", "venue": "[{\"end\":818,\"start\":742}]", "abstract": "[{\"end\":2574,\"start\":1131}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2731,\"start\":2728},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2752,\"start\":2749},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2778,\"start\":2775},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2987,\"start\":2984},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3364,\"start\":3361},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3694,\"start\":3691},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4583,\"start\":4579},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4703,\"start\":4700},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4832,\"start\":4828},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5714,\"start\":5710},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5962,\"start\":5958},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6310,\"start\":6307},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6422,\"start\":6418},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6683,\"start\":6679},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7984,\"start\":7980},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8119,\"start\":8115},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8141,\"start\":8137},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8167,\"start\":8163},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11377,\"start\":11373},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12045,\"start\":12041},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16567,\"start\":16563},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17620,\"start\":17616},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18359,\"start\":18355},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18791,\"start\":18787},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19152,\"start\":19148},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19621,\"start\":19617},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21732,\"start\":21729},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21744,\"start\":21740},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21759,\"start\":21756},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21776,\"start\":21773},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21795,\"start\":21791},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24093,\"start\":24089}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31549,\"start\":31251},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31605,\"start\":31550},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31653,\"start\":31606},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31743,\"start\":31654},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":32115,\"start\":31744},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":32704,\"start\":32116},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":33425,\"start\":32705},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":33903,\"start\":33426}]", "paragraph": "[{\"end\":4271,\"start\":2590},{\"end\":5189,\"start\":4273},{\"end\":6776,\"start\":5191},{\"end\":7757,\"start\":6778},{\"end\":8949,\"start\":7759},{\"end\":9517,\"start\":8951},{\"end\":9625,\"start\":9519},{\"end\":10381,\"start\":9685},{\"end\":10524,\"start\":10412},{\"end\":11080,\"start\":10547},{\"end\":11800,\"start\":11111},{\"end\":11999,\"start\":11802},{\"end\":12394,\"start\":12001},{\"end\":12851,\"start\":12396},{\"end\":12865,\"start\":12862},{\"end\":13241,\"start\":12900},{\"end\":13877,\"start\":13259},{\"end\":14366,\"start\":13879},{\"end\":14738,\"start\":14368},{\"end\":14848,\"start\":14759},{\"end\":14980,\"start\":14922},{\"end\":15149,\"start\":15070},{\"end\":15677,\"start\":15172},{\"end\":15819,\"start\":15679},{\"end\":16217,\"start\":15855},{\"end\":16671,\"start\":16219},{\"end\":17185,\"start\":16703},{\"end\":17750,\"start\":17187},{\"end\":18078,\"start\":17752},{\"end\":18425,\"start\":18116},{\"end\":18567,\"start\":18427},{\"end\":18867,\"start\":18569},{\"end\":18971,\"start\":18941},{\"end\":19776,\"start\":19022},{\"end\":19921,\"start\":19778},{\"end\":20253,\"start\":19923},{\"end\":20649,\"start\":20344},{\"end\":20832,\"start\":20651},{\"end\":20959,\"start\":20834},{\"end\":21076,\"start\":20994},{\"end\":21244,\"start\":21131},{\"end\":21656,\"start\":21260},{\"end\":22814,\"start\":21669},{\"end\":23932,\"start\":22857},{\"end\":25865,\"start\":23934},{\"end\":26229,\"start\":25921},{\"end\":27348,\"start\":26231},{\"end\":27739,\"start\":27357},{\"end\":28284,\"start\":27768},{\"end\":28985,\"start\":28286},{\"end\":29596,\"start\":28987},{\"end\":29754,\"start\":29635},{\"end\":30021,\"start\":29756},{\"end\":31250,\"start\":30036}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10411,\"start\":10382},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12861,\"start\":12852},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12899,\"start\":12866},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13258,\"start\":13242},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14758,\"start\":14739},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14921,\"start\":14849},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15069,\"start\":14981},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15854,\"start\":15820},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16702,\"start\":16672},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18940,\"start\":18868},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19021,\"start\":18972},{\"attributes\":{\"id\":\"formula_11\"},\"end\":20343,\"start\":20254},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20993,\"start\":20960},{\"attributes\":{\"id\":\"formula_13\"},\"end\":21130,\"start\":21077},{\"attributes\":{\"id\":\"formula_14\"},\"end\":29634,\"start\":29597}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24893,\"start\":24886},{\"end\":26394,\"start\":26387},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":26654,\"start\":26647},{\"end\":27159,\"start\":27152},{\"end\":27577,\"start\":27570}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2588,\"start\":2576},{\"attributes\":{\"n\":\"2\"},\"end\":9683,\"start\":9628},{\"attributes\":{\"n\":\"2.2\"},\"end\":10545,\"start\":10527},{\"attributes\":{\"n\":\"2.3\"},\"end\":11109,\"start\":11083},{\"attributes\":{\"n\":\"2.4\"},\"end\":15170,\"start\":15152},{\"attributes\":{\"n\":\"2.5\"},\"end\":18114,\"start\":18081},{\"attributes\":{\"n\":\"3\"},\"end\":21258,\"start\":21247},{\"attributes\":{\"n\":\"3.1\"},\"end\":21667,\"start\":21659},{\"attributes\":{\"n\":\"3.2\"},\"end\":22855,\"start\":22817},{\"end\":25879,\"start\":25868},{\"attributes\":{\"n\":\"3.3\"},\"end\":25919,\"start\":25882},{\"end\":27355,\"start\":27351},{\"attributes\":{\"n\":\"3.4\"},\"end\":27766,\"start\":27742},{\"attributes\":{\"n\":\"4\"},\"end\":30034,\"start\":30024},{\"end\":31257,\"start\":31252},{\"end\":31561,\"start\":31551},{\"end\":31617,\"start\":31607},{\"end\":31665,\"start\":31655},{\"end\":31752,\"start\":31745},{\"end\":32124,\"start\":32117},{\"end\":32713,\"start\":32706}]", "table": "[{\"end\":32115,\"start\":31874},{\"end\":32704,\"start\":32326},{\"end\":33425,\"start\":33045},{\"end\":33903,\"start\":33436}]", "figure_caption": "[{\"end\":31549,\"start\":31259},{\"end\":31605,\"start\":31563},{\"end\":31653,\"start\":31619},{\"end\":31743,\"start\":31667},{\"end\":31874,\"start\":31754},{\"end\":32326,\"start\":32126},{\"end\":33045,\"start\":32715},{\"end\":33436,\"start\":33428}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10650,\"start\":10644},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10723,\"start\":10717},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10910,\"start\":10904},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18959,\"start\":18953},{\"end\":18970,\"start\":18964},{\"end\":19571,\"start\":19563},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23003,\"start\":22997},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28304,\"start\":28298},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28913,\"start\":28906},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28984,\"start\":28977},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29327,\"start\":29321}]", "bib_author_first_name": "[{\"end\":34014,\"start\":34009},{\"end\":34027,\"start\":34020},{\"end\":34043,\"start\":34034},{\"end\":34056,\"start\":34051},{\"end\":34071,\"start\":34065},{\"end\":34485,\"start\":34478},{\"end\":34503,\"start\":34496},{\"end\":34520,\"start\":34514},{\"end\":34538,\"start\":34531},{\"end\":34553,\"start\":34547},{\"end\":35025,\"start\":35019},{\"end\":35037,\"start\":35032},{\"end\":35056,\"start\":35048},{\"end\":35073,\"start\":35068},{\"end\":35090,\"start\":35082},{\"end\":35107,\"start\":35100},{\"end\":35116,\"start\":35113},{\"end\":35131,\"start\":35125},{\"end\":35593,\"start\":35587},{\"end\":35605,\"start\":35598},{\"end\":35629,\"start\":35620},{\"end\":35900,\"start\":35893},{\"end\":35912,\"start\":35905},{\"end\":35928,\"start\":35920},{\"end\":35938,\"start\":35934},{\"end\":36329,\"start\":36326},{\"end\":36343,\"start\":36337},{\"end\":36356,\"start\":36349},{\"end\":36381,\"start\":36373},{\"end\":36799,\"start\":36794},{\"end\":36811,\"start\":36804},{\"end\":36827,\"start\":36819},{\"end\":36837,\"start\":36832},{\"end\":36847,\"start\":36843},{\"end\":37305,\"start\":37301},{\"end\":37322,\"start\":37318},{\"end\":37342,\"start\":37334},{\"end\":37344,\"start\":37343},{\"end\":37674,\"start\":37672},{\"end\":37682,\"start\":37679},{\"end\":37692,\"start\":37687},{\"end\":37705,\"start\":37699},{\"end\":37721,\"start\":37713},{\"end\":37734,\"start\":37728},{\"end\":38207,\"start\":38203},{\"end\":38220,\"start\":38213},{\"end\":38244,\"start\":38235},{\"end\":38628,\"start\":38620},{\"end\":38641,\"start\":38634},{\"end\":38837,\"start\":38829},{\"end\":38850,\"start\":38842},{\"end\":38862,\"start\":38856},{\"end\":38874,\"start\":38871},{\"end\":39208,\"start\":39203},{\"end\":39238,\"start\":39231},{\"end\":39550,\"start\":39543},{\"end\":39572,\"start\":39560},{\"end\":39582,\"start\":39578},{\"end\":39596,\"start\":39588},{\"end\":39613,\"start\":39605},{\"end\":39982,\"start\":39978},{\"end\":39998,\"start\":39992},{\"end\":40015,\"start\":40007},{\"end\":40027,\"start\":40021},{\"end\":40050,\"start\":40039},{\"end\":40504,\"start\":40496},{\"end\":40518,\"start\":40510},{\"end\":40530,\"start\":40523},{\"end\":40545,\"start\":40538},{\"end\":40553,\"start\":40550},{\"end\":40735,\"start\":40730},{\"end\":40752,\"start\":40746},{\"end\":41069,\"start\":41060},{\"end\":41082,\"start\":41076},{\"end\":41101,\"start\":41095},{\"end\":41627,\"start\":41621},{\"end\":41640,\"start\":41636},{\"end\":41657,\"start\":41648},{\"end\":41682,\"start\":41668},{\"end\":42064,\"start\":42059},{\"end\":42079,\"start\":42071},{\"end\":42090,\"start\":42085},{\"end\":42104,\"start\":42097},{\"end\":42114,\"start\":42109},{\"end\":42132,\"start\":42126},{\"end\":42141,\"start\":42140},{\"end\":42656,\"start\":42655},{\"end\":42658,\"start\":42657},{\"end\":43001,\"start\":42998},{\"end\":43017,\"start\":43009},{\"end\":43028,\"start\":43024},{\"end\":43038,\"start\":43035},{\"end\":43052,\"start\":43045},{\"end\":43065,\"start\":43060},{\"end\":43075,\"start\":43072},{\"end\":43088,\"start\":43081},{\"end\":43361,\"start\":43354},{\"end\":43377,\"start\":43369},{\"end\":43388,\"start\":43382},{\"end\":43398,\"start\":43395},{\"end\":43407,\"start\":43403},{\"end\":43428,\"start\":43422},{\"end\":43439,\"start\":43436},{\"end\":43977,\"start\":43970},{\"end\":43989,\"start\":43985},{\"end\":44003,\"start\":43996}]", "bib_author_last_name": "[{\"end\":34018,\"start\":34015},{\"end\":34032,\"start\":34028},{\"end\":34049,\"start\":34044},{\"end\":34063,\"start\":34057},{\"end\":34078,\"start\":34072},{\"end\":34494,\"start\":34486},{\"end\":34512,\"start\":34504},{\"end\":34529,\"start\":34521},{\"end\":34545,\"start\":34539},{\"end\":34560,\"start\":34554},{\"end\":35030,\"start\":35026},{\"end\":35046,\"start\":35038},{\"end\":35066,\"start\":35057},{\"end\":35080,\"start\":35074},{\"end\":35098,\"start\":35091},{\"end\":35111,\"start\":35108},{\"end\":35123,\"start\":35117},{\"end\":35142,\"start\":35132},{\"end\":35596,\"start\":35594},{\"end\":35618,\"start\":35606},{\"end\":35634,\"start\":35630},{\"end\":35903,\"start\":35901},{\"end\":35918,\"start\":35913},{\"end\":35932,\"start\":35929},{\"end\":35942,\"start\":35939},{\"end\":36335,\"start\":36330},{\"end\":36347,\"start\":36344},{\"end\":36371,\"start\":36357},{\"end\":36392,\"start\":36382},{\"end\":36802,\"start\":36800},{\"end\":36817,\"start\":36812},{\"end\":36830,\"start\":36828},{\"end\":36841,\"start\":36838},{\"end\":36852,\"start\":36848},{\"end\":37316,\"start\":37306},{\"end\":37332,\"start\":37323},{\"end\":37351,\"start\":37345},{\"end\":37677,\"start\":37675},{\"end\":37685,\"start\":37683},{\"end\":37697,\"start\":37693},{\"end\":37711,\"start\":37706},{\"end\":37726,\"start\":37722},{\"end\":37738,\"start\":37735},{\"end\":38211,\"start\":38208},{\"end\":38233,\"start\":38221},{\"end\":38249,\"start\":38245},{\"end\":38632,\"start\":38629},{\"end\":38644,\"start\":38642},{\"end\":38840,\"start\":38838},{\"end\":38854,\"start\":38851},{\"end\":38869,\"start\":38863},{\"end\":38877,\"start\":38875},{\"end\":39229,\"start\":39209},{\"end\":39243,\"start\":39239},{\"end\":39255,\"start\":39245},{\"end\":39558,\"start\":39551},{\"end\":39576,\"start\":39573},{\"end\":39586,\"start\":39583},{\"end\":39603,\"start\":39597},{\"end\":39617,\"start\":39614},{\"end\":39990,\"start\":39983},{\"end\":40005,\"start\":39999},{\"end\":40019,\"start\":40016},{\"end\":40037,\"start\":40028},{\"end\":40055,\"start\":40051},{\"end\":40508,\"start\":40505},{\"end\":40521,\"start\":40519},{\"end\":40536,\"start\":40531},{\"end\":40548,\"start\":40546},{\"end\":40556,\"start\":40554},{\"end\":40744,\"start\":40736},{\"end\":40762,\"start\":40753},{\"end\":41074,\"start\":41070},{\"end\":41093,\"start\":41083},{\"end\":41111,\"start\":41102},{\"end\":41634,\"start\":41628},{\"end\":41646,\"start\":41641},{\"end\":41666,\"start\":41658},{\"end\":41688,\"start\":41683},{\"end\":42069,\"start\":42065},{\"end\":42083,\"start\":42080},{\"end\":42095,\"start\":42091},{\"end\":42107,\"start\":42105},{\"end\":42124,\"start\":42115},{\"end\":42138,\"start\":42133},{\"end\":42145,\"start\":42142},{\"end\":42151,\"start\":42147},{\"end\":42666,\"start\":42659},{\"end\":42674,\"start\":42668},{\"end\":43007,\"start\":43002},{\"end\":43022,\"start\":43018},{\"end\":43033,\"start\":43029},{\"end\":43043,\"start\":43039},{\"end\":43058,\"start\":43053},{\"end\":43070,\"start\":43066},{\"end\":43079,\"start\":43076},{\"end\":43091,\"start\":43089},{\"end\":43367,\"start\":43362},{\"end\":43380,\"start\":43378},{\"end\":43393,\"start\":43389},{\"end\":43401,\"start\":43399},{\"end\":43420,\"start\":43408},{\"end\":43434,\"start\":43429},{\"end\":43446,\"start\":43440},{\"end\":43983,\"start\":43978},{\"end\":43994,\"start\":43990},{\"end\":44006,\"start\":44004}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3322503},\"end\":34378,\"start\":33920},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":13745801},\"end\":35017,\"start\":34380},{\"attributes\":{\"doi\":\"arXiv:1811.03728\",\"id\":\"b2\"},\"end\":35504,\"start\":35019},{\"attributes\":{\"doi\":\"arXiv:1708.06733\",\"id\":\"b3\"},\"end\":35845,\"start\":35506},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":206594692},\"end\":36282,\"start\":35847},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":9433631},\"end\":36747,\"start\":36284},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":53059573},\"end\":37234,\"start\":36749},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":195908774},\"end\":37597,\"start\":37236},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":57189581},\"end\":38124,\"start\":37599},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":44096776},\"end\":38565,\"start\":38126},{\"attributes\":{\"doi\":\"arXiv:1706.05791\",\"id\":\"b10\"},\"end\":38803,\"start\":38567},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1322184},\"end\":39114,\"start\":38805},{\"attributes\":{\"doi\":\"arXiv:1804.00750\",\"id\":\"b12\"},\"end\":39477,\"start\":39116},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":951335},\"end\":39920,\"start\":39479},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":4555207},\"end\":40436,\"start\":39922},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":11644595},\"end\":40728,\"start\":40438},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b16\"},\"end\":41010,\"start\":40730},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":2904063},\"end\":41290,\"start\":41012},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6315327},\"end\":41571,\"start\":41292},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":13060737},\"end\":42057,\"start\":41573},{\"attributes\":{\"id\":\"b20\"},\"end\":42586,\"start\":42059},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":61951114},\"end\":42946,\"start\":42588},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":211296567},\"end\":43276,\"start\":42948},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":44085059},\"end\":43882,\"start\":43278},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":8316324},\"end\":44245,\"start\":43884}]", "bib_title": "[{\"end\":34007,\"start\":33920},{\"end\":34476,\"start\":34380},{\"end\":35891,\"start\":35847},{\"end\":36324,\"start\":36284},{\"end\":36792,\"start\":36749},{\"end\":37299,\"start\":37236},{\"end\":37670,\"start\":37599},{\"end\":38201,\"start\":38126},{\"end\":38827,\"start\":38805},{\"end\":39541,\"start\":39479},{\"end\":39976,\"start\":39922},{\"end\":40494,\"start\":40438},{\"end\":41058,\"start\":41012},{\"end\":41346,\"start\":41292},{\"end\":41619,\"start\":41573},{\"end\":42653,\"start\":42588},{\"end\":42996,\"start\":42948},{\"end\":43352,\"start\":43278},{\"end\":43968,\"start\":43884}]", "bib_author": "[{\"end\":34020,\"start\":34009},{\"end\":34034,\"start\":34020},{\"end\":34051,\"start\":34034},{\"end\":34065,\"start\":34051},{\"end\":34080,\"start\":34065},{\"end\":34496,\"start\":34478},{\"end\":34514,\"start\":34496},{\"end\":34531,\"start\":34514},{\"end\":34547,\"start\":34531},{\"end\":34562,\"start\":34547},{\"end\":35032,\"start\":35019},{\"end\":35048,\"start\":35032},{\"end\":35068,\"start\":35048},{\"end\":35082,\"start\":35068},{\"end\":35100,\"start\":35082},{\"end\":35113,\"start\":35100},{\"end\":35125,\"start\":35113},{\"end\":35144,\"start\":35125},{\"end\":35598,\"start\":35587},{\"end\":35620,\"start\":35598},{\"end\":35636,\"start\":35620},{\"end\":35905,\"start\":35893},{\"end\":35920,\"start\":35905},{\"end\":35934,\"start\":35920},{\"end\":35944,\"start\":35934},{\"end\":36337,\"start\":36326},{\"end\":36349,\"start\":36337},{\"end\":36373,\"start\":36349},{\"end\":36394,\"start\":36373},{\"end\":36804,\"start\":36794},{\"end\":36819,\"start\":36804},{\"end\":36832,\"start\":36819},{\"end\":36843,\"start\":36832},{\"end\":36854,\"start\":36843},{\"end\":37318,\"start\":37301},{\"end\":37334,\"start\":37318},{\"end\":37353,\"start\":37334},{\"end\":37679,\"start\":37672},{\"end\":37687,\"start\":37679},{\"end\":37699,\"start\":37687},{\"end\":37713,\"start\":37699},{\"end\":37728,\"start\":37713},{\"end\":37740,\"start\":37728},{\"end\":38213,\"start\":38203},{\"end\":38235,\"start\":38213},{\"end\":38251,\"start\":38235},{\"end\":38634,\"start\":38620},{\"end\":38646,\"start\":38634},{\"end\":38842,\"start\":38829},{\"end\":38856,\"start\":38842},{\"end\":38871,\"start\":38856},{\"end\":38879,\"start\":38871},{\"end\":39231,\"start\":39203},{\"end\":39245,\"start\":39231},{\"end\":39257,\"start\":39245},{\"end\":39560,\"start\":39543},{\"end\":39578,\"start\":39560},{\"end\":39588,\"start\":39578},{\"end\":39605,\"start\":39588},{\"end\":39619,\"start\":39605},{\"end\":39992,\"start\":39978},{\"end\":40007,\"start\":39992},{\"end\":40021,\"start\":40007},{\"end\":40039,\"start\":40021},{\"end\":40057,\"start\":40039},{\"end\":40510,\"start\":40496},{\"end\":40523,\"start\":40510},{\"end\":40538,\"start\":40523},{\"end\":40550,\"start\":40538},{\"end\":40558,\"start\":40550},{\"end\":40746,\"start\":40730},{\"end\":40764,\"start\":40746},{\"end\":41076,\"start\":41060},{\"end\":41095,\"start\":41076},{\"end\":41113,\"start\":41095},{\"end\":41636,\"start\":41621},{\"end\":41648,\"start\":41636},{\"end\":41668,\"start\":41648},{\"end\":41690,\"start\":41668},{\"end\":42071,\"start\":42059},{\"end\":42085,\"start\":42071},{\"end\":42097,\"start\":42085},{\"end\":42109,\"start\":42097},{\"end\":42126,\"start\":42109},{\"end\":42140,\"start\":42126},{\"end\":42147,\"start\":42140},{\"end\":42153,\"start\":42147},{\"end\":42668,\"start\":42655},{\"end\":42676,\"start\":42668},{\"end\":43009,\"start\":42998},{\"end\":43024,\"start\":43009},{\"end\":43035,\"start\":43024},{\"end\":43045,\"start\":43035},{\"end\":43060,\"start\":43045},{\"end\":43072,\"start\":43060},{\"end\":43081,\"start\":43072},{\"end\":43093,\"start\":43081},{\"end\":43369,\"start\":43354},{\"end\":43382,\"start\":43369},{\"end\":43395,\"start\":43382},{\"end\":43403,\"start\":43395},{\"end\":43422,\"start\":43403},{\"end\":43436,\"start\":43422},{\"end\":43448,\"start\":43436},{\"end\":43985,\"start\":43970},{\"end\":43996,\"start\":43985},{\"end\":44008,\"start\":43996}]", "bib_venue": "[{\"end\":34723,\"start\":34651},{\"end\":36085,\"start\":36023},{\"end\":36535,\"start\":36473},{\"end\":37011,\"start\":36941},{\"end\":37881,\"start\":37819},{\"end\":40198,\"start\":40136},{\"end\":41835,\"start\":41771},{\"end\":43599,\"start\":43532},{\"end\":34134,\"start\":34080},{\"end\":34649,\"start\":34562},{\"end\":35235,\"start\":35160},{\"end\":35585,\"start\":35506},{\"end\":36021,\"start\":35944},{\"end\":36471,\"start\":36394},{\"end\":36939,\"start\":36854},{\"end\":37402,\"start\":37353},{\"end\":37817,\"start\":37740},{\"end\":38323,\"start\":38251},{\"end\":38618,\"start\":38567},{\"end\":38941,\"start\":38879},{\"end\":39201,\"start\":39116},{\"end\":39681,\"start\":39619},{\"end\":40134,\"start\":40057},{\"end\":40569,\"start\":40558},{\"end\":40845,\"start\":40779},{\"end\":41143,\"start\":41113},{\"end\":41410,\"start\":41348},{\"end\":41769,\"start\":41690},{\"end\":42311,\"start\":42153},{\"end\":42757,\"start\":42676},{\"end\":43097,\"start\":43093},{\"end\":43530,\"start\":43448},{\"end\":44045,\"start\":44008}]"}}}, "year": 2023, "month": 12, "day": 17}