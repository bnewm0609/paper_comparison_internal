{"id": 254877499, "updated": "2023-10-05 06:25:09.829", "metadata": {"title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": "[{\"first\":\"Harsh\",\"last\":\"Trivedi\",\"middle\":[]},{\"first\":\"Niranjan\",\"last\":\"Balasubramanian\",\"middle\":[]},{\"first\":\"Tushar\",\"last\":\"Khot\",\"middle\":[]},{\"first\":\"Ashish\",\"last\":\"Sabharwal\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2212.10509", "mag": null, "acl": "2023.acl-long.557", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/TrivediBKS23", "doi": "10.18653/v1/2023.acl-long.557"}}, "content": {"source": {"pdf_hash": "8153ac74dae0c035c836927bd4a2e855285f017e", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2023.acl-long.557.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "521cca4c068c41c303b9cfd1ca694ce72e4685e2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8153ac74dae0c035c836927bd4a2e855285f017e.txt", "contents": "\nInterleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions\nLong PapersCopyright Long PapersJuly 9-14, 2023\n\nHarsh Trivedi hjtrivedi@cs.stonybrook.edu \nInstitute for AI Seattle\nStony Brook University Stony Brook\nU.S.A., U.S.A\n\nNiranjan Balasubramanian niranjan@cs.stonybrook.edu \nInstitute for AI Seattle\nStony Brook University Stony Brook\nU.S.A., U.S.A\n\nTushar Khot tushark@allenai.org \nInstitute for AI Seattle\nStony Brook University Stony Brook\nU.S.A., U.S.A\n\nAshish Sabharwal ashishs@allenai.org \nInstitute for AI Seattle\nStony Brook University Stony Brook\nU.S.A., U.S.A\n\n\u2021 Allen \nInstitute for AI Seattle\nStony Brook University Stony Brook\nU.S.A., U.S.A\n\nInterleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions\n\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nthe 61st Annual Meeting of the Association for Computational LinguisticsLong Papers1July 9-14, 2023\nPrompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-ofdistribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning. 1 .\n\nIntroduction\n\nLarge language models are capable of answering complex questions by generating step-bystep natural language reasoning steps-so called chains of thoughts (CoT)-when prompted appropriately (Wei et al., 2022). This approach has been successful when all information needed to answer the question is either provided as context (e.g., algebra questions) or assumed to be present in the model's parameters (e.g., commonsense reasoning). 1 Code, data, and prompts are available at https:// github.com/stonybrooknlp/ircot\n\nIn what country was Lost Gravity manufactured?\n\nThe Lost Gravity was manufactured by Mack Rides.\n\nMack Rides is a company from Germany.\n\nThe answer is Germany.\n\ncumulate docs cumulate docs cumulate docs Figure 1: IRCoT interleaves chain-of-thought (CoT) generation and knowledge retrieval steps in order to guide the retrieval by CoT and vice-versa. This interleaving allows retrieving more relevant information for later reasoning steps, compared to standard retrieval using solely the question as the query.\n\nHowever, for many open-domain questions, all required knowledge is not always available or up-todate in models' parameters and it's beneficial to retrieve knowledge from external sources (Lazaridou et al., 2022;Kasai et al., 2022). How can we augment chain-of-thought prompting for open-domain, knowledge-intensive tasks that require complex, multi-step reasoning?\n\nWhile a one-shot retrieval from a knowledge source based solely on the question can successfully augment LMs with relevant knowledge for many factoid-based tasks (Lewis et al., 2020;Guu et al., 2020;Borgeaud et al., 2022;Izacard et al., 2022), this strategy has clear limitations for more complex multi-step reasoning questions. For such questions, one often must retrieve partial knowledge, perform partial reasoning, retrieve additional information based on the outcome of the partial reasoning done so far, and iterate. As an example, consider the question illustrated in Fig. 1, \"In what country was Lost Gravity manufactured?\". The Wikipedia document retrieved using the question (in particular, the roller coaster Lost Gravity) as the query does not mention where Lost Gravity was manufactured. Instead, one must first infer that it was manufactured by a company called Mack Rides, and then perform further retrieval, guided by the inferred company name, to obtain evidence pointing to the manufacturing country.\n\nThus, the retrieval and reasoning steps must inform each other. Without retrieval, a model is likely to generate an incorrect reasoning step due to hallucination. Additionally, without generating the first reasoning step, the text supporting the second step can't be identified easily given the lack of lexical or even semantic overlap with the question. In other words, we need retrieved facts in order to generate factually correct reasoning steps and the reasoning steps to retrieve relevant facts.\n\nBased on this intuition, we propose an interleaving approach to this problem, where the idea is to use retrieval to guide the chain-of-thought (CoT) reasoning steps and use CoT reasoning to guide the retrieval. Fig. 1 shows an overview of our retrieval method, which we call IRCoT. 2 We begin by retrieving a base set of paragraphs using the question as a query. Subsequently, we alternate between the following two steps: (i) extend CoT: use the question, the paragraphs collected thus far, and the CoT sentences generated thus far to generate the next CoT sentence; (ii) expand retrieved information: use the last CoT sentence as a query to retrieve additional paragraphs to add to the collected set. We repeat these steps till the CoT reports an answer or we reach the maximum allowed number of reasoning steps. Upon termination, all collected paragraphs are returned as the retrieval outcome. Finally, we use these as the context for answering the question via direct QA prompting (Brown et al., 2020) or CoT prompting (Wei et al., 2022).\n\nWe evaluate the efficacy of our system on 4 multi-step reasoning datasets under an open-domain setting: HotpotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020), MuSiQue (Trivedi et al., 2022), and IIRC (Ferguson et al., 2020). Our experiments using OpenAI GPT3 (code-davinci-002) (Brown et al., 2020;Ouyang et al., 2022;Chen et al., 2021) demon-2 Interleaved Retrieval guided by Chain-of-Thought. strate that retrieval using IRCoT is substantially more effective than the baseline, one-step, questionbased retrieval by 11-21 recall points under a fixedbudget optimal recall setup. 3 When IRCoT is used in conjunction with a prompting-based reader, it also leads to substantial improvement (up to 15 F1 points) in downstream few-shot QA performance and reduces factual errors in generated CoT by up to 50%. Our approach also works on much smaller Flan-T5 models (11B, 3B, and 0.7B) showing similar trends. In particular, we find QA using Flan-T5-XL (3B) with IRCoT even outperforms the 58X larger GPT3 with a one-step questionbased retrieval. Furthermore, these improvements also hold up in an out-of-distribution (OOD) setting where the demonstrations from one dataset are used when testing on another dataset. Lastly, we note that our QA scores exceed those reported by recent works on few-shot prompting for open-domain QA (ODQA) (Khot et al., 2023;Press et al., 2022;Yao et al., 2022), although a fair apples-to-apples comparison with them isn't possible (cf. Appendix C).\n\nIn summary, our main contribution is a novel retrieval method, IRCoT, that leverages LMs' chainof-thought generation capabilities to guide retrieval and uses retrieval in turn to improve CoT reasoning. We demonstrate that IRCoT:\n\n1. improves both retrieval and few-shot QA performance on several multi-step open-domain QA datasets, in both IID and OOD settings; 2. reduces factual errors in generated CoTs; and 3. improves performance with both large-scale (175B models) as well as smaller-scale models (Flan-T5-*, \u226411B) without any training.\n\n\nRelated Work\n\nPrompting for Open-Domain QA. LLMs can learn various tasks by simply using a few examples as prompts (Brown et al., 2020). They've also been shown to answer complex questions by producing step-by-step reasoning (chain-ofthoughts, or CoT) when prompted with a few or zero demonstrations (Wei et al., 2022;Kojima et al., 2022). Prompting has been applied to open-domain QA (Lazaridou et al., 2022;Sun et al., 2022;Yu et al., 2023) but its value in improving retrieval and QA for multi-step open-domain questions remains relatively underexplored.\n\nRecently three approaches have been proposed for multi-step open-domain QA. SelfAsk (Press et al., 2022) prompts LLMs to decompose a question into subquestions and answers subquestions by a call to Google Search API. DecomP (Khot et al., 2023) is a general framework that decomposes a task and delegates sub-tasks to appropriate submodels. They also decompose questions but delegate retrieval to a BM25-based retriever. Both of these approaches are not developed for CoT reasoning, do not focus on the retrieval problem, and require a single-hop QA model to answer the decomposed questions. Recently proposed ReAct (Yao et al., 2022) system frames the problem as generating a sequence of reasoning and action steps. These steps are much more complex, rely on much larger models (PaLM-540B), and require fine-tuning to outperform CoT for multi-step ODQA. Furthermore, none of these works have been shown to be effective for smaller models without any training. While a direct comparison with these approaches is not straightforward (difference in knowledge corpus, LLMs, examples), we find that our ODQA performance is much higher than all their reported numbers where available ( \u00a75).\n\nSupervised Multi-Step Open-Domain QA. Prior work has explored iterative retrieval for open-domain QA in a fully supervised setting. Das et al. (2019) proposes an iterative retrieval model that retrieves using a neural query representation and then updates it based on a reading comprehension model's output. Feldman and El-Yaniv (2019) apply similar neural query reformulation idea for multihop open-domain QA. Xiong et al. (2021) extends the widely-used Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) to multihop setting, which has since been improved by Khattab et al. (2021). Asai et al. (2020) leverages the graph structure induced by the entity links present in Wikipedia paragraphs to perform iterative multi-step retrieval. GoldEn (Gold Entity) retriever (Qi et al., 2019) iteratively generates text queries based on paragraphs retrieved from an off-the-shelf retriever but requires training data for this next query generator. Nakano et al. (2021) used GPT3 to answer long-form questions by interacting with the browser but relied on human annotations of these interactions. All of these methods rely on supervised training on a large-scale dataset and can not be easily extended to a few-shot setting.\n\n\nChain-of-Thought-Guided Retrieval and Open-Domain QA\n\nOur goal is to answer a knowledge-intensive multistep reasoning question Q in a few-shot setting by using a knowledge source containing a large number of documents. To do this we follow a retrieve-and-read paradigm (Zhu et al., 2021), where the retriever first retrieves documents from the knowledge source and the QA model reads the retrieved documents and the question to generate the final answer. Our contribution is mainly in the retrieve step ( \u00a73.1), and we use standard prompting strategies for the read step ( \u00a73.2). As noted earlier, for multi-step reasoning, retrieval can help guide the next reasoning step, which in turn can inform what to retrieve next. This motivates our interleaving strategy, discussed next.\n\n\nInterleaving Retrieval with\n\nChain-of-Thought Reasoning\n\nOur proposed retriever method, IRCoT, can be instantiated from the following three ingredients: (i) a base retriever that can take a query and return a given number of paragraphs from a corpus or knowledge source; (ii) a language model with zero/few-shot Chain-of-Thought (CoT) generation capabilities; and (iii) a small number of annotated questions with reasoning steps explaining how to arrive at the answer in natural language (chain of thoughts) and a set of paragraphs from the knowledge source that collectively support the reasoning chain and the answer. The overview of IRCoT is given in Fig. 2. We first gather a base set of paragraphs by retrieving K paragraphs using the question Q as the query. Then, we interleave two steps (reason and retrieve) iteratively until the termination criterion is met.\n\nThe retrieval-guided reasoning step (\"Reason\") generates the next CoT sentence using the question, the paragraphs collected thus far, and the CoT sentences generated thus far. The prompt template for the task looks as follows:  Figure 2: IRCoT interleaves chain-of-thought (CoT) generation and retrieval steps to guide the retrieval by CoT and vice-versa. We start by retrieving K documents using the question as they query and repeat two steps alternatingly until termination. (i) reason-step generates next CoT sentence based on the question, so far retrieved paragraphs, and CoT sentences. (ii) retrieve-step retrieves K more paragraphs based on the last CoT sentence. The process terminates when the generated CoT has \"answer is\" or the number of steps exceeds a threshold. The collection of all paragraphs is returned as the retrieval result on the termination. we show the model only the CoT sentences generated thus far and let it complete the rest. Even though the model may output multiple sentences, for each reason-step, we only take the first generated sentence and discard the rest. For the paragraphs in the in-context demonstrations, we use ground-truth supporting paragraphs and M randomly sampled paragraphs shuffled and concatenated together in the above format. For a test instance, we show all the paragraphs collected thus far across all the previous retrieve-steps.\n\nIf the generated CoT sentence has the \"answer is:\" string or the maximum number of steps 4 has been reached, we terminate the process and return all collected paragraphs as the retrieval result.\n\nThe CoT-guided retrieval step (\"Retrieve\") uses the last generated CoT sentence as a query to retrieve more paragraphs and adds them to the collected paragraphs. We cap the total number of collected paragraphs 5 so as to fit in at least a few demonstrations in the model's context limit.\n\n\nQuestion Answering Reader\n\nThe QA reader answers the question using retrieved paragraphs taken from the retriever. We consider two versions of the QA reader implemented via two prompting strategies: CoT Prompting as proposed by Wei et al. (2022), Direct Prompting as proposed by Brown et al. (2020). For CoT prompting, we use the same template as shown in \u00a73.2, but at test time we ask the model to generate the full CoT from scratch. The final sentence of CoT is expected to be of the form \"answer is: ...\", so that the answer can be extracted programmatically. If it's not in that form, the full generation is returned as the answer. For Direct Prompting, we use the same template as CoT Prompting but the answer field (\"A: \") contains only the final answer instead of CoT. See App. G for details. For each of the other three datasets, which originally come in a reading comprehension or mixed setting, we used the associated contexts to construct a corpus for our open-domain setting (see App. A for details). For each dataset, we use 100 randomly sampled questions from the original development set for tuning hyperparameters, and 500 other randomly sampled questions as our test set.\n\n\nExperimental Setup\n\n\nModels\n\nRetriever. We use BM25 (Robertson et al., 2009) implemented in Elasticsearch 6 as our base retriever. We compare two retriever systems:\n\n(i) One-step Retriever (OneR) uses the question as a query to retrieve K paragraphs. We select K \u2208 {5, 7, 9, 11, 13, 15} that's best on the dev set.\n\n(ii) IRCoT Retriever is our method described in \u00a73. We use BM25 as its underlying retriever and experiment with OpenAI GPT3 (code-davinci-002) (Brown et al., 2020;Ouyang et al., 2022;Chen et al., 2021) and Flan-T5 (Chung et al., 2022) of different sizes as its CoT generator.\n\nFor demonstrating in-context examples to these LMs, we wrote CoTs for 20 questions for all the datasets (see App. \u00a7G). We then create 3 demonstration (\"training\") sets by sampling 15 questions each for each dataset. For each experiment, we search for the best hyperparameters for the dev set using the first demonstration set and evaluate each demonstration set on the test set using the selected hyperparameters. We report the mean and standard deviation of these 3 results for each experiment.\n\nAt test time, we pack as many demonstrations as possible within the model's context length limit. The context limit for GPT3 (code-davinci-002) is 8K word pieces. Flan-T5-* doesn't have any hard limit as it uses relative position embeddings. But we limit Flan-T5's context to 6K word pieces, which is the maximum we could fit in the memory of our 80G A100 GPUs.\n\nIRCoT Retriever has one key hyperparameter: K \u2208 {2, 4, 6, 8}, the number of paragraphs to retrieve at each step. Additionally, when creating \"training\" demonstrations for IRCoT's Reasoner module, we use gold paragraphs and a smaller number M \u2208 {1, 2, 3} of distractor paragraphs ( \u00a73.1).\n\nRetrieval Metric: We allow a maximum of 15 paragraphs for all retriever systems and measure the recall of the gold paragraphs among the retrieved set of paragraphs. We search for the hyperparameter K (and M for IRCoT) that maximizes the recall on the dev set and use it on the test set.\n\nThe reported metric can thus be viewed as the fixedbudget optimal recall for each system considered. 7 QA Reader. To implement the reader, we use the same LMs as used in the reason-step of IRCoT Retriever. We found that QA readers implemented with Flan-T5-* perform better with the Direct Prompting strategy and GPT3 performs better with CoT Prompting strategy (see App. E). Hence we use Direct prompting strategy for QA with Flan-T5-* and CoT with GPT3 for the experiments. 8 The QA reader has one hyperparameter M : the number of distractor paragraphs in the in-context demonstrations. We search for M in {1, 2, 3}. When used in conjunction with IRCoT retriever M is tied for the CoT generator and the reader.\n\nOpen-Domain QA (ODQA) Models. Putting retrievers and readers together, we experiment with ODQA models constructed from the various language models denoted as OneR QA and IRCoT QA. For IRCoT QA, the choice of LM for the CoT generator and the reader is kept the same. We also experiment with retriever-less QA readers NoR QA to assess how well LMs can answer the question from their parametric knowledge alone. To select the best hyperparameters for the ODQA model, we search for the hyperparameters K and M that maximize the answer F1 on the development set.\n\nIIRC is structured slightly differently from the other datasets, in that its questions are grounded in a main passage and other supporting paragraphs come from the Wikipedia pages of entities mentioned in this passage. We slightly modify the retrievers and readers to account for this (see App. B).\n\n\nResults\n\nIRCoT retrieval is better than one-step. Fig. 3 compares OneR with IRCoT retrievers made from 7 Note that our retrieved documents are not ranked, making standard information retrieval metrics such as MAP and DCG inapplicable. Further, we can only limit the number of retrieved paragraphs per step to K. Since the total number of reasoning steps varies for questions, and in some cases, we don't even obtain all K paragraphs in a given step, the total number of retrieved paragraphs also varies (even though capped at 15). This makes Recall@k, Precision@k, etc., also not applicable as metrics for any given k. 8 IRCoT, by construction, produces a CoT as a part of its retrieval process. Thus, instead of having a separate post-hoc reader, one can also just extract the answer from the CoT generated during retrieval. However, we found this to be a suboptimal choice, so we always use a separate reader (see App. F).  Flan-T5-XXL and GPT3 LMs. For both models, IRCoT significantly outperforms one-step retrieval across all datasets. For Flan-T5-XXL, IRCoT improves our recall metric relative to one-step retrieval, on HotpotQA by 7.9, on 2WikiMultihopQA by 14.3, on MuSiQue by 3.5, and on IIRC by 10.2 points. For GPT3, this improvement is by 11.3, 22.6, 12.5, and 21.2 points, respectively.\n\nIRCoT QA outperforms NoR and OneR QA. Fig. 4 compares ODQA performance using NoR, OneR and IRCoT retriever made from Flan-T5-XXL and GPT3 LMs. For Flan-T5-XXL, IRCoT QA outperforms OneR QA on HotpotQA by 9.4, on 2WikiMultihopQA by 15.3, on MuSiQue by 5.0 and IIRC by 2.5 F1 points. For GPT3, the corresponding numbers (except for IIRC) are 7.1, 13.2, and 7.1 F1 points. For GPT3, IRCoT doesn't improve the QA score on IIRC, despite significantly improved retrieval (21 points as shown in Fig. 3). This is likely because IIRC relevant knowledge may already be present in GPT3, as also evidenced by its NoR QA score being similar. For other datasets and model combinations, NoR QA is much worse than IRCoT QA, indicating the limits of the models' parametric knowledge.\n\nIRCoT is effective in OOD setting. Since CoT may not always be easy to write for new datasets, we evaluate NoR, OneR, and IRCoT on generalization to new datasets, i.e. OOD setting. To do so, we use prompt demonstrations from one dataset to evaluate on another dataset. 9 For all pairs of the datasets 10 and for both Flan-T5-XXL and GPT3, we find the same trend as in the IID setting: IRCoT retrieval outperforms OneR (Fig. 5), and IRCoT QA outperforms both OneR QA and NoR QA (Fig. 6).\n\n\nIRCoT generates CoT with fewer factual errors.\n\nTo assess whether our approach also improves the factuality of generated CoTs, we manually annotated CoTs generated by NoR QA, OneR QA, and IRCoT QA using GPT3 for 40 randomly sampled questions from each of the four datasets. We considered CoT to have a factual error if at least one   of the facts 11 is not true. 12 As Fig. 7 shows, NoR makes the most factual errors, OneR makes fewer, and IRCoT the least. In particular, IRCoT reduces the factual errors over OneR by 50% on HotpotQA and 40% on 2WikiMultihopQA. Table 2 illustrates how the CoT predictions for different methods vary qualitatively. Since NoR relies completely on parametric knowledge, it often makes a factual error in the first sentence, which derails the full CoT. OneR can retrieve relevant information closest to the question and is less likely to make such errors early on, but it still makes errors later in the CoT. IRCoT, on the other hand, is often able to prevent such errors in each step.\n\nIRCoT is also effective for smaller models. To see how effective IRCoT is at different LM sizes, we show the scaling plots in Fig. 8. 13 We compare the recall for OneR and IRCoT using Flan-T5 {base (0.2B), large (0.7B), XL (3B), XXL (11B)}, and GPT3 code-davinci-002 (175B). IRCoT with even the smallest model (0.2B) is better than  OneR, and the performance roughly improves with the model size. This shows the CoT generation capabilities of even small models can be leveraged for improving retrieval. Furthermore, we show the effect of model size on the QA score in Fig. 9. For all sizes except the smallest (0.2B), we see IRCoT QA is better than OneR QA. Moreover, IRCoT with a 3B model even outperforms OneR and NoR with a 58X larger 175B GPT3 model in all datasets.\n\nIRCoT is SOTA for few-shot multistep ODQA. 14 We compare IRCoT QA with five recent approaches to using LLMs for ODQA: Internet-Augmented QA (Lazaridou et al., 2022), RE-CITE (Sun et al., 2022) ReAct (Yao et al., 2022), SelfAsk (Press et al., 2022), and DecomP (Khot et al., 2022. Although these are not head-to-head comparisons as different methods use different APIs, knowledge sources, and even LLMs (see App. C for details), it is still informative to explore, in a leaderboard-style fashion, how IRCoT performs relative to the best numbers published for these recent systems.\n\n\nModel\n\nHpQA Br HpQA 2WikiMQA MQ 2H  As shown in Table 1, IRCoT QA significantly outperforms all of these recent systems by a large margin, setting a new state of the art in terms of what's achievable via retrieval-augmented LLMs (without supervised training).\nInterAug \u2212 | \u2212 30.3 | \u2212 \u2212 | \u2212 \u2212 | \u2212 RECITE \u2212 | \u2212 37.1 | 48.4 \u2212 | \u2212 \u2212 | \u2212 ReAct \u2212 | \u2212 35.1 | \u2212 \u2212 | \u2212 \u2212 | \u2212 SelfAsk \u2212 | \u2212 \u2212 | \u2212 40.1 | \u2212 15.2 | \u2212 DecomP \u2212 | 50.0 \u2212 | \u2212 \u2212 | 59.3 \u2212 | \u2212IRCoT\n\nConclusions\n\nChain-of-thought prompting has significantly improved LLMs' ability to perform multi-step reason-  [1984][1985][1986][1987][1988][1989] was performed by Jane Siberry. Jane Siberry was born in Toronto. The castle in Toronto is the Casa Loma. So the answer is: Casa Loma. Table 2: Example CoTs generated by GPT3 with different methods. Since NoR relies on parametric knowledge, it often makes a factual error in the first sentence derailing the full CoT. OneR can retrieve relevant information closest to the question and is less likely to make such errors early on, but it still makes errors later in the CoT. As IRCoT performs retrieval after each step, it is often able to prevent such errors in each step. More examples are in App. D.\n\ning. We leveraged this ability to improve retrieval, and in turn, improve QA performance for complex knowledge-intensive open-domain tasks in a few-shot setting. We argued that one-step questionbased retrieval is insufficient for such tasks, and introduced IRCoT, which uses interleaved CoT reasoning and retrieval steps that guide each other step-by-step. On four datasets, IRCoT significantly improves both retrieval and QA performance when compared to one-step retrieval, for both large and relatively smaller-scale LMs. Additionally, CoTs generated by IRCoT contain fewer factual errors.\n\n\nLimitations\n\nIRCoT relies on the base LM to have a zero or few-shot CoT-generation ability. While this is commonly available in large LMs (over 100B), it's not as common for small LMs (under 20B), which to some extent limits IRCoT adoptability. Given the recent surge of interest (Tay et al., 2023;Magister et al., 2022;Ho et al., 2022), however, smaller LMs will likely increasingly acquire such ability, making IRCoT compatible with many more LMs.\n\nIRCoT also relies on the base LM to support long inputs as multiple retrieved paragraphs need to fit in the LM's input, in addition to at least a few demonstrations of QA or CoT with paragraphs. This was supported by the models we used as code-davinci-002 (GPT3) allows 8K tokens and Flan-T5-* uses relative position embeddings making it as extensible as the GPU memory constraints allow. Future work can explore strategies to rerank and select the retrieved paragraphs instead of passing all of them to the LM to alleviate the need for the LM to support long input.\n\nThe performance gain of IRCoT retriever and QA (over OneR and ZeroR baselines) come with an additional computational cost. This is because IRCoT makes a separate call to an (L)LM for each sentence of CoT. Future work can focus on, for instance, dynamically deciding when to retrieve more information and when to perform additional reasoning with the current information.\n\nLastly, a portion of our experiments was carried out using a commercial LLM API from OpenAI (code-davinci-002). This model was deprecated by OpenAI after our submission making the reproduction of these experiments challenging despite our best efforts, just like any other work using such APIs. The trends discussed in the paper (IRCoT > OneR > NoR), we believe, would still hold. Additionally, all our experiments using Flan-T5-*, which exhibit similar trends as that of GPT3, will remain reproducible, thanks to its publicly available model weights.\n\n\nEthical Considerations\n\nLanguage models are known to hallucinate incorrect and potentially biased information. This is especially problematic when the questions asked to it are of a sensitive nature. While retrievalaugmented approaches such as ours are expected to alleviate this issue to some extent by grounding generation in external text, this by no means solves the problem of generating biased or offensive statements. Appropriate care should thus be taken if deploying such systems in user-facing applications.\n\nAll the datasets and models used in this work are publicly available with permissible licenses. HotpotQA has CC BY-SA 4.0 license 15 , 2Wiki-MultihopQA has Apache-2.0 license 16 \n\n\nA Constructing Retrieval Corpora\n\nHotpotQA already comes with the associated Wikipedia corpus for the open-domain setting, so we use it directly. 2WikiMultihopQA and MuSiQue, however, are originally reading comprehension datasets. Questions in 2WikiMulti-hopQA and MuSiQue are associated with 10 and 20 paragraphs respectively, 2-4 of which are supporting and others are non-supporting. To turn these datasets into an open-domain setting, we make two corpora, one for each dataset, by combining all supporting and non-supporting paragraphs for all its questions in the train, development, and test sets. IIRC is originally a mix between reading comprehension and an open-domain setting. Each question is grounded in one main paragraph, which contains links to multiple Wikipedia pages with several paragraphs each. We create a corpus out of all the paragraphs from all the Wikipedia pages present in the dataset. 18 We do assume the availability of the main passage which doesn't need to be retrieved and is always present. We don't assume the availability of Wikipedia links in the main passage, however, to keep the retrieval problem challenging. 19\n\n\nB Special Handling of Models for IIRC\n\nIIRC is slightly different from the other datasets, in that the question is grounded in the main passage and other supporting paragraphs come from the Wikipedia pages of entities mentioned in this passage. We modify the retrievers and readers to account for this difference: (i) We always keep the main passage as part of the input to the model regardless of the retrieval strategy used. (ii) For all the retrieval methods, we first prompt the model to generate a list of Wikipedia page titles using the main passage and the question. We map these generated titles to the nearest Wikipedia page titles in the corpus (found using BM25), and then the rest of the paragraph retrieval queries are scoped within only those Wikipedia pages. To prompt the model to generate Wikipedia page titles using the main passage and the question for 18 Following are the corpus sizes for the datasets: 233,329),2WikiMultihopQA (430,225),MuSiQue has (139,416),and IIRC (1,882,415) 19 IIRC corpus has a positional bias, i.e., the majority of supporting paragraphs are always within the first few positions of the Wikipedia page. To keep the retrieval problem challenging enough we shuffle the paragraphs before indexing the corpus, i.e., we don't use positional information in any way. IIRC, we use the following template. For \"training\", i.e., for demonstrations, N (\u2264 3) is the number of supporting Wikipedia page titles for the question. At test time, since the number of supporting page titles is unknown, we use a fixed value of 3. We found this trick of prompting the model to generate more titles at the test time improves its recall over letting the model decide by itself how many titles to generate.\n\n\nC Comparison with Previous Systems for ODQA with LLMs\n\nWe showed a leaderboard-style comparison with previous approaches to using large language models for open-domain QA in \u00a7 5. We noted though that the comparison is not head-to-head given various differences. We briefly describe each method and the differences in API, LLM, retrieval corpus, and other choices here. Internet-Augmented QA (Lazaridou et al., 2022) does (one-step) Google Search retrieval, performs additional LLM-based filtering on it, and then prompts an LLM to answer the question using the resulting context. It uses the Gopher 280B language model. RECITE (Sun et al., 2022) bypasses the retrieval and instead prompts an LLM to first generate (recite) one or several relevant passages from its own memory, and generate the answer conditioned on this generation. They experiment with many LLMs, the highest performing of which is code-davinci-002 which we report here. ReAct (Yao et al., 2022) prompts LLMs to produce reasoning and action traces where actions are calls to a Wikipedia API to return the summary for a given Wikipedia page title. It uses the PALM 540B model. SelfAsk (Press et al., 2022) prompts LLMs to decompose a question into subquestions and answers these subquestions by issuing separate calls to the Google Search API. It uses the GPT3 (text-davinci-002) model. Finally, DecomP (Khot et al., 2023) is a general framework that decomposes a task and delegates sub-tasks to appropriate sub-models. Similar to our system, it uses BM25 Search and the GPT3 (code-davinci-002) model. And lastly,  (Yao et al., 2022) \u2212   DSP (Khattab et al., 2023) provides a way to programmatically define interactions between LLM and retrieval for ODQA (e.g., via question decomposition), bootstrap demonstrations for such a program, and use them to make the answer prediction. It uses GPT3.5 LLM with ColBERT-based retrieval.\n\u2212 | \u2212 30.3 | \u2212 \u2212 | \u2212 \u2212 | \u2212 \u2212 | \u2212 RECITE (Sun et al., 2022) \u2212 | \u2212 37.1 | 48.4 \u2212 | \u2212 \u2212 | \u2212 \u2212 | \u2212 ReAct| \u2212 35.1 | \u2212 \u2212 | \u2212 \u2212 | \u2212 \u2212 | \u2212 SelfAsk (Press et al., 2022) \u2212 | \u2212 \u2212 | \u2212 40.1 | \u2212 15.2 | \u2212 \u2212 | \u2212 DecomP (Khot et al., 2022) \u2212 | 50.0 \u2212 | \u2212 \u2212 | 59.3 \u2212 | \u2212 \u2212 | \u2212 DecomP (Khot et al., 2023) * \u2212 | \u2212 \u2212 | 53.5 \u2212 | 70.8 \u2212 | \u2212 \u2212 | 30.9 DSP (Khattab et al., 2023) * \u2212 | \u2212 51.4 | 62.9 \u2212 | \u2212 \u2212 | \u2212 \u2212 | \u2212IRCoT\nSince most of these methods use different knowledge sources or APIs and are built using different LLMs and retrieval models, it's difficult to make a fair scientific comparison across these systems. Additionally, the evaluations in the respective papers are on different random subsets (from the same distribution) of test instances.\n\nDespite these differences, it is still informative to explore, in a leaderboard-style fashion, how IRCoT performs relative to the best numbers published for these recent systems. We speculate DecomP performs well on 2WikiMul-tihopQA because it has only a few easy-to-predict decomposition patterns, which DecomP's question decomposition can leverage. The lack of such patterns in HotpotQA and MuSiQue causes it to underperform compared to IRCoT. Lastly, it will be useful to assess whether DSP, which is hardcoded for 2-hop questions like that of HotpotQA, will work well for a dataset with a varied number of hops like that of MuSiQue. We leave this further investigation to future work. Table 5 provides illustrations, in addition to the ones provided in Table 2, for how the CoT generations for NoR QA, OneR QA, and IRCoT QA methods vary. This gives an insight into how IR-CoT improves QA performance. Since NoR relies completely on parametric knowledge, it often makes a factual error in the first sentence, which derails the full reasoning chain. Some of this factual information can be fixed by OneR, especially information closest to the question (i.e., can be retrieved using the question). This is insufficient for fixing  Table 5: Additional CoTs generated by GPT3 with different methods. ZeroR is most prone to factual errors. OneR often fixes some of the factual information which is closest to the question but doesn't always fix it all the way. Since IRCoT retrieves after each step, it can also fix the errors at each step. More examples are in Table 2. all the mistakes. Since IRCoT involves retrieval after each step, it can fix errors at each step. Table 4 compares reader choice (Direct vs CoT Prompting) for Flan-T5-XXL and GPT3. We find that Flan-T5-XXL works better with Direct Prompting as a reader and GPT3 works better with CoT Prompting as a reader. Therefore, for the experiments in the main paper, we go with this choice. Note though that the trends discussed in \u00a7 5 (IRCoT QA > OneR QA > ZeroR QA) hold regardless of the choice of the reader.\n\n\nD Additional CoT Generation Examples\n\n\nE Direct vs CoT Prompting Readers\n\n\nF Separate Reader in IRCoT QA\n\nIRCoT, by construction, produces a CoT as a part of its retrieval process. So, instead of having a separate post-hoc reader, one can also just extract the answer from the CoT generated during retrieval. As Table 6 shows the effect of such an ablation. For Flan-T5-XXL having a separate reader is significantly better. For GPT3, this is not always true, but at least a model with a separate reader is always better or close to the one without. So overall we go with the choice of using the reader for the experiments in this paper.  Table 6: Answer F1 of IRCoT QA with and without a separate reader for Flan-T5-XXL (top two rows) and GPT3 (bottom two rows). When the reader is not used, the answer is extracted from the CoT generated by IRCoT while doing the retrieval. Ablating the reader usually hurts the performance.\n\n\nG Prompts\n\nOur manually written chain-of-thought annotations for HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC are given in Listing 1, 2, 3 and 4 respectively. Our prompts for GPT3 CoT Prompting are the same as these, except they have Wikipipedia paragraphs on the top of the questions as shown in \u00a7 3.1 20 . Our prompts for GPT3 Direct Prompting are the same as that of CoT prompting, except have the answer after \"A:\" directly. Our prompts for Flan-T5-* are slightly different from that of GPT3. For CoT Prompting, we prefix the question line: \"Q: Answer the following question by reasoning step-by-step. <actual-question>\". For Direct Prompting, we prefix the question line: \"Q: Answer the following question. <actual-question>\". We did this to follow Flan-T5-*'s training format and found it to help its CoT generation.\n\nListing 1: Chain-of-Thought annotations for HotpotQA.\n\n\nWe evaluate our method on 4 multi-step QA datasets in the open-domain setting: HotpotQA (Yang et al., 2018), 2WikiMul-tihopQA (Ho et al., 2020), answerable subset of MuSiQue (Trivedi et al., 2022), and answerable subset of IIRC (Ferguson et al., 2020). For HotpotQA, we use the Wikipedia corpus that comes with it for the open-domain setting.\n\nFigure 3 :\n3Retrieval recall for one-step retriever (OneR) and IRCoT instantiated from Flan-T5-XXL (left) and GPT3 (right) models. IRCoT outperforms OneR for both models and all datasets.\n\nFigure 4 :\n4Answer F1 for ODQA model made using (i) no retriever (NoR QA) (ii) one-step retriever (OneR QA) and (iii) IRCoT QA instantiated from Flan-T5-XXL (left) and GPT3 (right) models. IRCoT QA outperforms OneR QA and NoR QA for both models on all datasets, except for GPT3 on IIRC.\n\nFigure 5 :\n5Retrieval recall for OneR and IRCoT using Flan-T5-XXL (Left) and GPT3 (Right) in out-of-distribution (OOD) setting. HQ (HotpotQA), 2W (2WikiMultihopQA), MQ (MuSiQue). The result X\u2192Y indicates prompt demonstrations are from dataset X and evaluation is on dataset Y. IRCoT outperforms OneR in such an OOD setting.\n\nFigure 6 :\n6Answer F1 for NoR QA, OneR QA and IRCoT QA using Flan-T5-XXL (Left) and GPT3 (Right) in out-of-distribution (OOD) setting. HQ (HotpotQA), 2W (2WikiMultihopQA), MQ (MuSiQue). The result X\u2192Y indicates prompt demonstrations are from dataset X and evaluation is on dataset Y. IRCoT QA outperforms OneR QA and NoR QA in such OOD setting.\n\nFigure 7 :\n7Number of questions, out of 40, where CoT generated by GPT3 using different methods has at least 1 factual error. Factual errors: IRCoT < OneR < NoR.\n\nFigure 8 :\n8Retrieval recall for OneR (bottom) and IRCoT (top) for LMs of increasing sizes: Flan-T5 {base (0.2B), large (0.7B), XL (3B), XXL (11B)} and GPT3 (175B) on HotpotQA, 2WikiMultihopQA, MuSiQue. IRCoT outperforms OneR for all model sizes, including the 0.3B model, and the difference roughly grows with model size. Note: OneR doesn't use LM in its retrieval and so has a fixed score.\n\nFigure 9 :\n9Answer F1 for ODQA models made using OneR (bottom) and IRCoT (top) for LMs of increasing sizes: Flan-T5 {base (0.2B), large (0.7B), XL (3B), XXL (11B)} and GPT3 (175B) on HotpotQA, 2WikiMultihopQA and MuSiQue. IRCoT QA outperforms OneR QA for all model sizes except for the smallest, 0.3B. IRCoT with 3B model even outperforms OneR with 58X larger GPT3 model showing the value of improved retrieval.\n\n\nFor in-context demonstrations, we use the complete CoT in the above format. For a test instance, Who wrote the 1970 international hit song that Murray Head is most recognized for?Wikipedia Title: <Page Title> \n<Paragraph Text> \n... \nWikipedia Title: <Page Title> \n<Paragraph Text> \n\nQ: <Question> \nA: <CoT-Sent-1> ... <CoT-Sent-n> \n\nThe 1970 international hit song that \nMurray Head is most recognized for \nis \"Super Star\" \n\n\"Super Star\" was written by \nAndrew Lloyd Webber and Tim Rice. \n\nSo the answer is: \nAndrew Lloyd Webber and Tim Rice. \n\nRetrieve (Q) \u2192 \n\nRetrieve (T1) \u2192 \n\nRetrieve (T2) \u2192 \n\nT1 \u2190 Reason (Q, \n, \n) \n\nT2 \u2190 Reason (Q, \n+ , T1) \n\nT3 \u2190 Reason (Q, \n+ \n+ , T1+T2) \n\nT1 \n\nQ \n\nT2 \n\nT3 \n\nStop \n\n+ \n+ \n\nIRCoT \n\nInterleaved Retrieval guided \nby Chain-of-Thought Reasoning \n\nRetrieve( \n) \n\nWikipedia Title: Mack Rides \nMack Rides GmbH & Co KG, also ... \n\nQ: In what country was \nLost Gravity manufactured? \nA: The Lost Gravity was manufactured by Mack \nRides. Mack Rides is a company from \nGermany. The answer is Germany. \n... \nWikipedia Title: Murray Head \nMurray Seafield St George Head .. \n... \nWikipedia Title: Most Beautifullest Hits \nThe Most Beautifullest Hits is ... \n\nQ: Who wrote the 1970 international hit .. \nA: The 1970 international hit song that \nMurray Head is most recognized for \nis \"Super Star\". \"Super Star\" was written \nby. Andrew Lloyd Webber and Tim Rice. \n\nq \n\nReason( \n, \n, \n) \n\nQ \nT1 \n\nQ \n\nT1 \n\nq \n\n\n\nTable 1 :\n1Comparison with other LLM-based ODQA \nsystems on EM and F1 scores. '\u2212': score is unavail-\nable. HpQA Br : Bridge questions subset of HotpotQA. \nMQ 2H : MuSiQue 2-hop questions. IRCoT QA with \nGPT3 (ours) outperforms other systems by a large mar-\ngin. Note: Comparisons aren't head-to-head as dis-\ncussed in the text. App.  \u00a7C reports updated SOTA num-\nbers, including contemporaneous and newer works. \n\n\n\n\nJane Siberry was born in Toronto. \u21d2 The castle in Toronto is the Casa Loma. The performer of A Collection 1984-1989 is The The. The The was born in London. The name of the castle in London is the Tower of London. So the answer is: the Tower of London. Jane Siberry was born in Toronto. The castle in Toronto is Peqin Castle. So the answer is: Peqin Castle.Question \n\nGold Facts \nNoR QA \nOneR QA \nIRCoT QA \n\nHotpotQA \n\nThe tap dancing \nsoloist in My \nFeet \nKeep \nDancing also \nstarred in what \n1935 \nMGM \nmusical? \n\n\u21d2 The tap dancing \nsoloist in My Feet \nKeep Dancing is Fayard \nNicholas. \n\u21d2 Fayard Nicholas \nstarred in the 1935 \nMGM musical An \nAll-Colored Vaudeville \nShow. \n\nThe tap dancing soloist \nin My Feet Keep Danc-\ning is Eleanor Pow-\nell. \nEleanor Powell \nalso starred in the 1935 \nMGM musical Broad-\nway Melody of 1936. \nSo the answer is: Broad-\nway Melody of 1936. \n\nThe tap dancing soloist \nin My Feet Keep Danc-\ning is Fayard Nicholas. \nFayard Nicholas also \nstarred in the 1935 \nMGM musical Top Hat. \nSo the answer is: Top \nHat. \n\nThe tap dancing soloist \nin My Feet Keep Danc-\ning is Fayard Nicholas. \nFayard Nicholas starred \nin the 1935 MGM mu-\nsical An All-Colored \nVaudeville Show. So \nthe answer is: An \nAll-Colored Vaudeville \nShow. \n\n2WikiMQA \n\nWho is the \nfather-in-law \nof Queen Hyo-\njeong? \n\n\u21d2 Queen Hyojeong \nis the wife of King \nHeonjong of Joseon. \n\u21d2 King Heonjong \nof Joseon is the son \nof Crown Prince Hy-\nomyeong. \n\nQueen Hyojeong has a \nhusband named King \nGojong. King Gojong \nhas a father named \nHeungseon Daewongun. \nThus, Queen Hyojeong \nhas a father-in-law \nnamed \nHeungseon \nDaewongun. So the \nanswer is: Heungseon \nDaewongun. \n\nQueen Hyojeong is the \nwife of King Heonjong \nof Joseon. King Heon-\njong of Joseon is the \nson of King Sejo of \nJoseon. Thus, King Sejo \nof Joseon is the father-\nin-law of Queen Hyo-\njeong. So the answer is: \nKing Sejo of Joseon. \n\nQueen Hyojeong is the \nwife of King Heonjong \nof Joseon. King Heon-\njong of Joseon is the son \nof Crown Prince Hy-\nomyeong. Thus, Crown \nPrince Hyomyeong is \nthe father-in-law of \nQueen Hyojeong. So \nthe answer is: Crown \nPrince Hyomyeong. \n\nMuSiQue \n\nWhat is the \nname of the \ncastle in the \ncity where the \nperformer of \nA Collection \n1984-1989 was \nborn? \n\n\u21d2 \nA \nCollection \n1984-1989 was per-\nformed by Jane Siberry. \n\u21d2 A \nCollection \n1984-1989 \nwas \nperformed by Jane \nSiberry. A \nCollection \n\n\n\n, MuSiQUe and IIRC have CC BY 4.0 license 17 , and Flan-T5-* models have Apache-2.0 license. question answering. In International Conference on Learning Representations.Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste \nLespiau, Bogdan Damoc, Aidan Clark, Diego \nDe Las Casas, Aurelia Guy, Jacob Menick, Roman \nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock, \nMichela Paganini, Geoffrey Irving, Oriol Vinyals, \nSimon Osindero, Karen Simonyan, Jack Rae, Erich \nElsen, and Laurent Sifre. 2022. Improving language \nmodels by retrieving from trillions of tokens. In \nProceedings of the 39th International Conference \non Machine Learning, volume 162 of Proceedings \nof Machine Learning Research, pages 2206-2240. \nPMLR. \n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie \nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind \nNeelakantan, Pranav Shyam, Girish Sastry, Amanda \nAskell, et al. 2020. Language models are few-shot \nlearners. Advances in neural information processing \nsystems, 33:1877-1901. \n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming \nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph, \nGreg Brockman, et al. 2021. Evaluating large \nlanguage models trained on code. arXiv preprint \narXiv:2107.03374. \n\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi \nWang, Mostafa Dehghani, Siddhartha Brahma, et al. \n2022. Scaling instruction-finetuned language models. \narXiv preprint arXiv:2210.11416. \n\nRajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, \nand Andrew McCallum. 2019. Multi-step retriever-\nreader interaction for scalable open-domain question \nanswering. In International Conference on Learning \nRepresentations. \n\nYair Feldman and Ran El-Yaniv. 2019. Multi-hop para-\ngraph retrieval for open-domain question answering. \nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 2296-\n2309, Florence, Italy. Association for Computational \nLinguistics. \n\nJames Ferguson, Matt Gardner, Hannaneh Hajishirzi, \nTushar Khot, and Pradeep Dasigi. 2020. IIRC: A \ndataset of incomplete information reading compre-\nhension questions. In EMNLP. \n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, \nand Mingwei Chang. 2020. Retrieval augmented \nlanguage model pre-training. In Proceedings of the \n37th International Conference on Machine Learning, \nvolume 119 of Proceedings of Machine Learning \nResearch, pages 3929-3938. PMLR. \nNamgyu Ho, Laura Schmid, and Se-Young Yun. 2022. \nLarge language models are reasoning teachers. arXiv \npreprint arXiv:2212.10071. \n\nXanh Ho, A. Nguyen, Saku Sugawara, and Akiko \nAizawa. 2020. Constructing a multi-hop qa dataset \nfor comprehensive evaluation of reasoning steps. In \nCOLING. \n\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas \nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard \nGrave. 2022. Atlas: Few-shot learning with re-\ntrieval augmented language models. arXiv preprint \narXiv:2208.03299. \n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick \nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and \nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the \n2020 Conference on Empirical Methods in Natural \nLanguage Processing (EMNLP), pages 6769-6781, \nOnline. Association for Computational Linguistics. \n\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, \nRonan Le Bras, Akari Asai, Xinyan Yu, Dragomir \nRadev, Noah A Smith, Yejin Choi, and Kentaro Inui. \n2022. RealTime QA: What's the answer right now? \narXiv preprint arXiv:2207.13332. \n\nOmar Khattab, Christopher Potts, and Matei Zaharia. \n2021. Baleen: Robust multi-hop reasoning at scale \nvia condensed retrieval. In Advances in Neural Infor-\nmation Processing Systems. \n\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li, \nDavid Hall, Percy Liang, Christopher Potts, and \nMatei Zaharia. 2023. Demonstrate-search-predict: \nComposing retrieval and language models for \nknowledge-intensive NLP. \n\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, \nKyle Richardson, Peter Clark, and Ashish Sabharwal. \n2022. Decomposed prompting: A modular approach \nfor solving complex tasks. \n\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao \nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\nharwal. 2023. Decomposed prompting: A modular \napproach for solving complex tasks. In The Eleventh \nInternational Conference on Learning Representa-\ntions. \n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In ICML 2022 \nWorkshop on Knowledge Retrieval and Language \nModels. \n\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech \nStokowiec, and Nikolai Grigorev. 2022. Internet-\naugmented language models through few-shot \nprompting for open-domain question answering. \narXiv preprint arXiv:2203.05115. \n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio \nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020. \nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 9459-\n9474. Curran Associates, Inc. \n\nLucie Charlotte Magister, Jonathan Mallinson, Jakub \nAdamek, Eric Malmi, and Aliaksei Severyn. 2022. \nTeaching small language models to reason. arXiv \npreprint arXiv:2212.08410. \n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, \nLong Ouyang, Christina Kim, Christopher Hesse, \nShantanu Jain, Vineet Kosaraju, William Saunders, \net al. 2021. WebGPT: Browser-assisted question-\nanswering with human feedback. arXiv preprint \narXiv:2112.09332. \n\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, \nCarroll Wainwright, Pamela Mishkin, Chong Zhang, \nSandhini Agarwal, Katarina Slama, Alex Gray, John \nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller, \nMaddie Simens, Amanda Askell, Peter Welinder, \nPaul Christiano, Jan Leike, and Ryan Lowe. 2022. \nTraining language models to follow instructions with \nhuman feedback. In Advances in Neural Information \nProcessing Systems. \n\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, \nNoah A Smith, and Mike Lewis. 2022. Measuring \nand narrowing the compositionality gap in language \nmodels. arXiv preprint arXiv:2210.03350. \n\nPeng Qi, Xiaowen Lin, Leo Mehr, Zijian Wang, and \nChristopher D. Manning. 2019. Answering complex \nopen-domain questions through iterative query gen-\neration. In Proceedings of the 2019 Conference on \nEmpirical Methods in Natural Language Processing \nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages \n2590-2602, Hong Kong, China. Association for Com-\nputational Linguistics. \n\nStephen Robertson, Hugo Zaragoza, et al. 2009. The \nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends\u00ae in Information Re-\ntrieval, 3(4):333-389. \n\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and \nDenny Zhou. 2022. Recitation-augmented language \nmodels. arXiv preprint arXiv:2210.01296. \n\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, \nJason Wei, Xuezhi Wang, Hyung Won Chung, Dara \nBahri, Tal Schuster, Steven Zheng, Denny Zhou, Neil \nHoulsby, and Donald Metzler. 2023. UL2: Unifying \nlanguage learning paradigms. In The Eleventh Inter-\nnational Conference on Learning Representations. \n\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, \nand Ashish Sabharwal. 2022. MuSiQue: Multi-\nhop questions via single-hop question composition. \nTACL, 10:539-554. \n\n\n\nWikipedia Title: <Main Page Title> <Main Paragraph Text> Q: The question is: '<Question>'. Generate titles of <N> Wikipedia pages that have relevant information to answer this question. A: [\"<Title-1>\", \"<Title-2>\", ...]\n\nTable 3 :\n3Extended comparison with published LLM-based ODQA systems (as of May 25, 2023) on EM and F1 scores (with new numbers marked with *). '\u2212': score is unavailable. HpQA Br : Bridge questions subset of HotpotQA. MQ 2H : MuSiQue 2-hop questions. IRCoT remains SOTA for MuSiQue and is close to SOTA for HotpotQA and 2WikiMultihopQA. Note the comparisons here are not head-to-head as discussed in the text.Flan-T5-XXL \nGPT3 \n\nModel \nHotpotQA 2WikiMQA MuSiQue \nIIRC \nHotpotQA 2WikiMQA MuSiQue \nIIRC \n\nZeroR QA \nDirect \n25.3\u00b1 0.3 \n32.7\u00b1 0.3 \n13.7\u00b1 0.3 28.9\u00b1 0.3 \n41.0\u00b1 1.1 \n38.5\u00b1 1.1 \n19.0\u00b1 1.2 40.9\u00b1 0.7 \nCoT \n22.9\u00b1 0.1 \n31.7\u00b1 1.5 \n10.3\u00b1 0.5 24.4\u00b1 0.1 \n47.5\u00b1 0.4 \n41.2\u00b1 1.0 \n25.2\u00b1 1.2 52.1\u00b1 0.1 \n\nOneR QA \nDirect \n49.7\u00b1 0.5 \n51.2\u00b1 0.3 \n25.8\u00b1 0.6 40.0\u00b1 1.3 \n50.7\u00b1 0.1 \n46.4\u00b1 2.9 \n20.4\u00b1 0.3 40.1\u00b1 0.9 \nCoT \n43.1\u00b1 0.7 \n47.8\u00b1 0.9 \n17.6\u00b1 0.2 34.5\u00b1 1.5 \n53.6\u00b1 0.7 \n54.8\u00b1 2.1 \n29.4\u00b1 0.8 49.8\u00b1 2.3 \n\nIRCoT QA \nDirect \n59.1\u00b1 0.9 \n66.5\u00b1 1.4 \n30.8\u00b1 0.2 42.5\u00b1 2.1 \n60.6\u00b1 1.0 \n63.5\u00b1 2.7 \n36.0\u00b1 0.5 47.9\u00b1 2.3 \nCoT \n52.0\u00b1 0.6 \n55.1\u00b1 1.0 \n24.9\u00b1 1.0 36.5\u00b1 1.3 \n60.7\u00b1 1.1 \n68.0\u00b1 1.5 \n36.5\u00b1 1.2 49.9\u00b1 1.1 \n\n\n\nTable 4 :\n4Answer F1 for different ODQA models made from NoR, One and IRCoT retrievals, and Direct and CoT prompting readers. For Flan-T5-XXL, Direct prompting is a better choice for the reader, and for GPT3, CoT prompting is a better choice for the reader. Hence, we make different reader choices for Flan-T5 and GPT3 for the experiments in the main paper. Note that IRCoT QA > OneR QA > ZeroR QA holds up regardless of this choice.\n\nTable 3\n3shows results from different systems, including contemporaneous and newer numbers. The two new systems in this table (relative to Table 1) are DecomP (newer version) and DSP. While IRCoT remains SOTA on MuSiQue, DSP outperforms it on HotpotQA by 2.0 points and the newer version of Decomp outperforms IRCoT on 2WikiMultihopQA by 2.8 points.\n\n\n\u21d2 Marinelli Glacier is located on the island of Tierra del Fuego. \u21d2 The island of Tierra del Fuego was formerly known as Isla de Xativa.Marinelli Glacier is located on the island of Graham Land. Graham Land was formerly known as Graham's Land. So the answer is: Graham's Land. Marinelli Glacier is located on Isla Grande de Tierra del Fuego. Isla Grande de Tierra del Fuego was formerly known as Tierra del Fuego. So the answer is: Tierra del Fuego. Marinelli Glacier is located on the island of Tierra del Fuego. The island of Tierra del Fuego was formerly known as Isla de Xativa. So the answer is: Isla de Xativa. \u21d2 The director of Mukhyamantri (1996 film) is Anjan Choudhury. \u21d2 Anjan Choudhury has a child named Sandip Choudhury. The film Mukhyamantri was directed by S. V. Rajendra Singh Babu. S. V. Rajendra Singh Babu has a child named S. V. Rajendra Singh Babu. So the answer is: S. V. Rajendra Singh Babu. \u21d2 Sacerdotii Nostri Primordia was written by Pope John XXIII. \u21d2 Pope John XXIII died in Vatican City. \u21d2 The Governorship of Vatican City ended in 1952. The author of Sacerdotii Nostri Primordia is Pope Pius IX. Pope Pius IX died in the city of Rome. The Governorship of Rome ended in 1870. So the answer is: 1870. Sacerdotii Nostri Primordia was written by Pope John XXIII. Pope John XXIII died in the city of Rome. The Governorship of Rome ended in 1870. So the answer is: 1870. Sacerdotii Nostri Primordia was written by Pope John XXIII. Pope John XXIII died in Vatican City. The Governorship of Vatican City ended in 1952. So the answer is: 1952.Question \n\nGold Facts \nNoR QA \nOneR QA \nIRCoT QA \n\nHotpotQA \n\nWhat was the \nisland, \non \nwhich Marinelli \nGlacier is lo-\ncated, formerly \nknown as? \n\n2WikiMQA \n\nWho is the \nchild of the \ndirector of film \nMukhyamantri \n(1996 Film)? \n\nMukhyamantri (1996 \nfilm) was directed \nby Anjan Choudhury. \nAnjan Choudhury has a \nchild named Aniruddha \nChoudhury. \nSo the \nanswer is: Aniruddha \nChoudhury. \n\nThe director of Mukhya-\nmantri (1996 film) \nis Anjan Choudhury. \nAnjan Choudhury has \na child named Sandip \nChoudhury. \nSo the \nanswer is: \nSandip \nChoudhury. \n\nMuSiQue \n\nWhen did the \nGovernorship \nend of the \ncity where the \nauthor of Sac-\nerdotii Nostri \nPrimordia died? \n\n\n\n\nModel HotpotQA 2WikiMQA MuSiQue IIRC GPT3 IRCoT QA 60.7\u00b1 1.1 68.0\u00b1 1.5 36.5\u00b1 1.2 49.9\u00b1 1.1 w/o reader 61.0\u00b1 0.7 70.4\u00b1 1.5 31.5\u00b1 0.6 48.4\u00b1 1.0Flan \nIRCoT QA 59.1\u00b1 0.9 66.5\u00b1 1.4 30.8\u00b1 0.2 42.5\u00b1 2.1 \nw/o reader 52.6\u00b1 0.3 60.9\u00b1 0.6 24.9\u00b1 0.2 40.3\u00b1 0.2 \n\n\nWe explain later (in the Metric section and Footnote 7) the appropriateness of this metric in our setting as opposed to more mainstream information recall metrics.\nset to 8 in our experiments. 5 set to 15 in our experiments.\nhttps://www.elastic.co/\nWe use the evaluation dataset's corpus for retrieval.10  We skip IIRC in this exploration as the task is structured a bit differently and requires special handling (see App. B).\nall sentences before the final \"answer is:\" sentence. 12 Note that factual error doesn't necessarily mean the predicted answer is incorrect and vice-versa. This is because the model can generate a wrong answer despite all correct facts, and vice-versa. We also account for the possibility of answer annotation errors in the original datasets.\nWe skip IIRC here as the smaller models are not good at identifying Wikipedia titles from a paragraph and a question which is necessary for IIRC (see App. B).\nApp. \u00a7C reports updated SOTA numbers, including contemporaneous and newer works.\nhttps://creativecommons.org/licenses/by-sa/4. 0/ 16 https://www.apache.org/licenses/LICENSE-2.0 17 https://creativecommons.org/licenses/by/4.0\nWe are not showing the paragraphs in the paper for brevity but they can be obtained from the released code.\nAcknowledgmentsWe thank the reviewers for their valuable feedback and suggestions. We also thank OpenAI for providing access to the code-davinci-002 API. This material is based on research supported in part by the Air Force Research Laboratory (AFRL), DARPA, for the KAIROS program under agreement number FA8750-19-2-1003, in part by the National Science Foundation under the award IIS #2007290, and in part by an award from the Stony Brook Trustees Faculty Awards Program.Listing 3: Chain-of-Thought annotations for MuSiQue. Q: When did the first large winter carnival take place in the city where CIMI\u2212FM is licensed to broadcast? A: CIMI\u2212FM is licensed to broadcast in Quebec City. The first large winter carnival in Quebec City took place in 1894. So the answer is: 1894.Q: When was Neville A. Stanton's employer founded? A: The employer of Neville A. Stanton is University of Southampton. The University of Southampton was founded in 1862. So the answer is: 1862.Q: What county is Hebron located in, in the same province the Heritage Places Protection Act applies to? A: Heritage Places Protection Act applies to the jurisdiction of Prince Edward Island. Hebron, Prince Edward Island is located in the Prince County. So the answer is: Prince County.Q: What weekly publication in the Connecticut city with the most Zagat rated restaurants is issued by university of America\u2212 Lite: How Imperial Academia Dismantled Our Culture's author? A: The author of America\u2212Lite: How Imperial Academia Dismantled Our Culture is David Gelernter. David Gelernter was educated at the Yale University. The city in Connecticut that has the highest number of Zagat\u2212rated restaurants is New Haven. The weekly publication in New Haven that is issued by Yale University is Yale Herald. So the answer is: Yale Herald. C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? Not applicable. Left blank.D Did you use human annotators (e.g., crowdworkers) or research with human participants?Left blank.D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? Not applicable. Left blank.D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? Not applicable. Left blank.D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? Not applicable. Left blank.D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? Not applicable. Left blank.D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? Not applicable. Left blank.\nLearning to retrieve reasoning paths over wikipedia graph for. Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, Caiming Xiong, Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph for\n\nChain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Advances in Neural Information Processing Systems. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of thought prompt- ing elicits reasoning in large language models. In Advances in Neural Information Processing Systems.\n\nAnswering complex open-domain questions with multi-hop dense retrieval. Wenhan Xiong, Xiang Li, Srini Iyer, Jingfei Du, Patrick Lewis, William Yang Wang, Yashar Mehdad, Scott Yih, Sebastian Riedel, Douwe Kiela, Barlas Oguz, International Conference on Learning Representations. Wenhan Xiong, Xiang Li, Srini Iyer, Jingfei Du, Patrick Lewis, William Yang Wang, Yashar Mehdad, Scott Yih, Sebastian Riedel, Douwe Kiela, and Barlas Oguz. 2021. Answering complex open-domain ques- tions with multi-hop dense retrieval. In International Conference on Learning Representations.\n\nHotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, EMNLP. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben- gio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answer- ing. In EMNLP.\n\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629ReAct: Synergizing reasoning and acting in language models. arXiv preprintShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. ReAct: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.\n\nGenerate rather than retrieve: Large language models are strong context generators. Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, Meng Jiang, The Eleventh International Conference on Learning Representations. Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate rather than retrieve: Large language models are strong context generators. In The Eleventh Inter- national Conference on Learning Representations.\n\nFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, arXiv:2101.00774Soujanya Poria, and Tat-Seng Chua. 2021. Retrieving and reading: A comprehensive survey on open-domain question answering. arXiv preprintFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. 2021. Retrieving and reading: A comprehensive survey on open-domain question answering. arXiv preprint arXiv:2101.00774.\n", "annotations": {"author": "[{\"end\":267,\"start\":150},{\"end\":395,\"start\":268},{\"end\":503,\"start\":396},{\"end\":616,\"start\":504},{\"end\":700,\"start\":617}]", "publisher": "[{\"end\":112,\"start\":101},{\"end\":973,\"start\":962}]", "author_last_name": "[{\"end\":163,\"start\":156},{\"end\":292,\"start\":277},{\"end\":407,\"start\":403},{\"end\":520,\"start\":511},{\"end\":624,\"start\":619}]", "author_first_name": "[{\"end\":155,\"start\":150},{\"end\":276,\"start\":268},{\"end\":402,\"start\":396},{\"end\":510,\"start\":504},{\"end\":618,\"start\":617}]", "author_affiliation": "[{\"end\":266,\"start\":193},{\"end\":394,\"start\":321},{\"end\":502,\"start\":429},{\"end\":615,\"start\":542},{\"end\":699,\"start\":626}]", "title": "[{\"end\":100,\"start\":1},{\"end\":800,\"start\":701}]", "venue": "[{\"end\":889,\"start\":802}]", "abstract": "[{\"end\":2258,\"start\":990}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2479,\"start\":2461},{\"end\":2705,\"start\":2704},{\"end\":3510,\"start\":3486},{\"end\":3529,\"start\":3510},{\"end\":3847,\"start\":3827},{\"end\":3864,\"start\":3847},{\"end\":3886,\"start\":3864},{\"end\":3907,\"start\":3886},{\"end\":5471,\"start\":5470},{\"end\":6193,\"start\":6173},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6229,\"start\":6211},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6364,\"start\":6345},{\"end\":6399,\"start\":6382},{\"end\":6431,\"start\":6401},{\"end\":6465,\"start\":6442},{\"end\":6540,\"start\":6520},{\"end\":6560,\"start\":6540},{\"end\":6578,\"start\":6560},{\"end\":6822,\"start\":6821},{\"end\":7591,\"start\":7572},{\"end\":7610,\"start\":7591},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7627,\"start\":7610},{\"end\":8397,\"start\":8377},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8580,\"start\":8562},{\"end\":8600,\"start\":8580},{\"end\":8671,\"start\":8647},{\"end\":8688,\"start\":8671},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8704,\"start\":8688},{\"end\":8925,\"start\":8905},{\"end\":9064,\"start\":9045},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9454,\"start\":9436},{\"end\":10156,\"start\":10139},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10437,\"start\":10418},{\"end\":10516,\"start\":10492},{\"end\":10592,\"start\":10571},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10612,\"start\":10594},{\"end\":10794,\"start\":10777},{\"end\":10970,\"start\":10950},{\"end\":11515,\"start\":11497},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15000,\"start\":14983},{\"end\":15053,\"start\":15034},{\"end\":16022,\"start\":15993},{\"end\":16425,\"start\":16405},{\"end\":16445,\"start\":16425},{\"end\":16463,\"start\":16445},{\"end\":18452,\"start\":18451},{\"end\":20169,\"start\":20168},{\"end\":23260,\"start\":23258},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24112,\"start\":24095},{\"end\":24142,\"start\":24112},{\"end\":24174,\"start\":24142},{\"end\":25043,\"start\":25037},{\"end\":25049,\"start\":25043},{\"end\":25055,\"start\":25049},{\"end\":25061,\"start\":25055},{\"end\":25067,\"start\":25061},{\"end\":25073,\"start\":25067},{\"end\":26568,\"start\":26550},{\"end\":26590,\"start\":26568},{\"end\":26606,\"start\":26590},{\"end\":28910,\"start\":28908},{\"end\":29829,\"start\":29827},{\"end\":30942,\"start\":30940},{\"end\":30996,\"start\":30992},{\"end\":31001,\"start\":30996},{\"end\":31022,\"start\":31001},{\"end\":31027,\"start\":31022},{\"end\":31044,\"start\":31027},{\"end\":31049,\"start\":31044},{\"end\":31061,\"start\":31049},{\"end\":31065,\"start\":31061},{\"end\":31069,\"start\":31065},{\"end\":32445,\"start\":32427},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32763,\"start\":32745},{\"end\":32972,\"start\":32952},{\"end\":33189,\"start\":33154},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":33400,\"start\":33382},{\"end\":33431,\"start\":33409}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38652,\"start\":38308},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38841,\"start\":38653},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39129,\"start\":38842},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39454,\"start\":39130},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39800,\"start\":39455},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39963,\"start\":39801},{\"attributes\":{\"id\":\"fig_6\"},\"end\":40356,\"start\":39964},{\"attributes\":{\"id\":\"fig_7\"},\"end\":40769,\"start\":40357},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":42207,\"start\":40770},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":42623,\"start\":42208},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":45024,\"start\":42624},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":52750,\"start\":45025},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":52973,\"start\":52751},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":54065,\"start\":52974},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":54500,\"start\":54066},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":54851,\"start\":54501},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":57090,\"start\":54852},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":57343,\"start\":57091}]", "paragraph": "[{\"end\":2786,\"start\":2274},{\"end\":2834,\"start\":2788},{\"end\":2884,\"start\":2836},{\"end\":2923,\"start\":2886},{\"end\":2947,\"start\":2925},{\"end\":3297,\"start\":2949},{\"end\":3663,\"start\":3299},{\"end\":4683,\"start\":3665},{\"end\":5186,\"start\":4685},{\"end\":6230,\"start\":5188},{\"end\":7715,\"start\":6232},{\"end\":7945,\"start\":7717},{\"end\":8259,\"start\":7947},{\"end\":8819,\"start\":8276},{\"end\":10005,\"start\":8821},{\"end\":11225,\"start\":10007},{\"end\":12007,\"start\":11282},{\"end\":12065,\"start\":12039},{\"end\":12878,\"start\":12067},{\"end\":14267,\"start\":12880},{\"end\":14463,\"start\":14269},{\"end\":14752,\"start\":14465},{\"end\":15943,\"start\":14782},{\"end\":16110,\"start\":15975},{\"end\":16260,\"start\":16112},{\"end\":16537,\"start\":16262},{\"end\":17034,\"start\":16539},{\"end\":17397,\"start\":17036},{\"end\":17686,\"start\":17399},{\"end\":17974,\"start\":17688},{\"end\":18687,\"start\":17976},{\"end\":19246,\"start\":18689},{\"end\":19546,\"start\":19248},{\"end\":20848,\"start\":19558},{\"end\":21616,\"start\":20850},{\"end\":22104,\"start\":21618},{\"end\":23122,\"start\":22155},{\"end\":23894,\"start\":23124},{\"end\":24475,\"start\":23896},{\"end\":24737,\"start\":24485},{\"end\":25674,\"start\":24938},{\"end\":26267,\"start\":25676},{\"end\":26719,\"start\":26283},{\"end\":27287,\"start\":26721},{\"end\":27659,\"start\":27289},{\"end\":28211,\"start\":27661},{\"end\":28731,\"start\":28238},{\"end\":28911,\"start\":28733},{\"end\":30065,\"start\":28948},{\"end\":31797,\"start\":30107},{\"end\":33695,\"start\":31855},{\"end\":34426,\"start\":34093},{\"end\":36499,\"start\":34428},{\"end\":37427,\"start\":36608},{\"end\":38252,\"start\":37441},{\"end\":38307,\"start\":38254}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":24923,\"start\":24738},{\"attributes\":{\"id\":\"formula_1\"},\"end\":33796,\"start\":33696},{\"attributes\":{\"id\":\"formula_2\"},\"end\":34092,\"start\":33796}]", "table_ref": "[{\"end\":22676,\"start\":22669},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24533,\"start\":24526},{\"end\":25215,\"start\":25208},{\"end\":35124,\"start\":35117},{\"end\":35192,\"start\":35185},{\"end\":35667,\"start\":35660},{\"end\":35996,\"start\":35988},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":36102,\"start\":36095},{\"end\":36821,\"start\":36814},{\"end\":37147,\"start\":37140}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2272,\"start\":2260},{\"attributes\":{\"n\":\"2\"},\"end\":8274,\"start\":8262},{\"attributes\":{\"n\":\"3\"},\"end\":11280,\"start\":11228},{\"attributes\":{\"n\":\"3.1\"},\"end\":12037,\"start\":12010},{\"attributes\":{\"n\":\"3.2\"},\"end\":14780,\"start\":14755},{\"attributes\":{\"n\":\"4\"},\"end\":15964,\"start\":15946},{\"attributes\":{\"n\":\"4.1\"},\"end\":15973,\"start\":15967},{\"attributes\":{\"n\":\"5\"},\"end\":19556,\"start\":19549},{\"end\":22153,\"start\":22107},{\"end\":24483,\"start\":24478},{\"attributes\":{\"n\":\"6\"},\"end\":24936,\"start\":24925},{\"end\":26281,\"start\":26270},{\"end\":28236,\"start\":28214},{\"end\":28946,\"start\":28914},{\"end\":30105,\"start\":30068},{\"end\":31853,\"start\":31800},{\"end\":36538,\"start\":36502},{\"end\":36574,\"start\":36541},{\"end\":36606,\"start\":36577},{\"end\":37439,\"start\":37430},{\"end\":38664,\"start\":38654},{\"end\":38853,\"start\":38843},{\"end\":39141,\"start\":39131},{\"end\":39466,\"start\":39456},{\"end\":39812,\"start\":39802},{\"end\":39975,\"start\":39965},{\"end\":40368,\"start\":40358},{\"end\":42218,\"start\":42209},{\"end\":52984,\"start\":52975},{\"end\":54076,\"start\":54067},{\"end\":54509,\"start\":54502}]", "table": "[{\"end\":42207,\"start\":40951},{\"end\":42623,\"start\":42220},{\"end\":45024,\"start\":42982},{\"end\":52750,\"start\":45196},{\"end\":54065,\"start\":53384},{\"end\":57090,\"start\":56418},{\"end\":57343,\"start\":57234}]", "figure_caption": "[{\"end\":38652,\"start\":38310},{\"end\":38841,\"start\":38666},{\"end\":39129,\"start\":38855},{\"end\":39454,\"start\":39143},{\"end\":39800,\"start\":39468},{\"end\":39963,\"start\":39814},{\"end\":40356,\"start\":39977},{\"end\":40769,\"start\":40370},{\"end\":40951,\"start\":40772},{\"end\":42982,\"start\":42626},{\"end\":45196,\"start\":45027},{\"end\":52973,\"start\":52753},{\"end\":53384,\"start\":52986},{\"end\":54500,\"start\":54078},{\"end\":54851,\"start\":54511},{\"end\":56418,\"start\":54854},{\"end\":57234,\"start\":57093}]", "figure_ref": "[{\"end\":2999,\"start\":2991},{\"end\":4246,\"start\":4240},{\"end\":5405,\"start\":5399},{\"end\":12670,\"start\":12664},{\"end\":13116,\"start\":13108},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19605,\"start\":19599},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20894,\"start\":20888},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21344,\"start\":21338},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22044,\"start\":22036},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22103,\"start\":22095},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22482,\"start\":22476},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":23256,\"start\":23250},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":23698,\"start\":23692}]", "bib_author_first_name": "[{\"end\":61784,\"start\":61779},{\"end\":61797,\"start\":61791},{\"end\":61817,\"start\":61809},{\"end\":61837,\"start\":61830},{\"end\":61853,\"start\":61846},{\"end\":62092,\"start\":62087},{\"end\":62104,\"start\":62098},{\"end\":62115,\"start\":62111},{\"end\":62135,\"start\":62128},{\"end\":62148,\"start\":62143},{\"end\":62160,\"start\":62157},{\"end\":62168,\"start\":62166},{\"end\":62170,\"start\":62169},{\"end\":62177,\"start\":62176},{\"end\":62189,\"start\":62184},{\"end\":62580,\"start\":62574},{\"end\":62593,\"start\":62588},{\"end\":62603,\"start\":62598},{\"end\":62617,\"start\":62610},{\"end\":62629,\"start\":62622},{\"end\":62644,\"start\":62637},{\"end\":62649,\"start\":62645},{\"end\":62662,\"start\":62656},{\"end\":62676,\"start\":62671},{\"end\":62691,\"start\":62682},{\"end\":62705,\"start\":62700},{\"end\":62719,\"start\":62713},{\"end\":63155,\"start\":63149},{\"end\":63166,\"start\":63162},{\"end\":63179,\"start\":63171},{\"end\":63193,\"start\":63187},{\"end\":63209,\"start\":63202},{\"end\":63211,\"start\":63210},{\"end\":63225,\"start\":63219},{\"end\":63252,\"start\":63241},{\"end\":63254,\"start\":63253},{\"end\":63494,\"start\":63488},{\"end\":63507,\"start\":63500},{\"end\":63518,\"start\":63514},{\"end\":63526,\"start\":63523},{\"end\":63536,\"start\":63531},{\"end\":63553,\"start\":63546},{\"end\":63570,\"start\":63566},{\"end\":63948,\"start\":63942},{\"end\":63956,\"start\":63953},{\"end\":63971,\"start\":63963},{\"end\":63985,\"start\":63978},{\"end\":63998,\"start\":63990},{\"end\":64009,\"start\":64003},{\"end\":64027,\"start\":64018},{\"end\":64040,\"start\":64033},{\"end\":64051,\"start\":64047},{\"end\":64417,\"start\":64410},{\"end\":64431,\"start\":64423},{\"end\":64441,\"start\":64437},{\"end\":64456,\"start\":64448}]", "bib_author_last_name": "[{\"end\":61789,\"start\":61785},{\"end\":61807,\"start\":61798},{\"end\":61828,\"start\":61818},{\"end\":61844,\"start\":61838},{\"end\":61859,\"start\":61854},{\"end\":62096,\"start\":62093},{\"end\":62109,\"start\":62105},{\"end\":62126,\"start\":62116},{\"end\":62141,\"start\":62136},{\"end\":62155,\"start\":62149},{\"end\":62164,\"start\":62161},{\"end\":62174,\"start\":62171},{\"end\":62182,\"start\":62178},{\"end\":62192,\"start\":62190},{\"end\":62198,\"start\":62194},{\"end\":62586,\"start\":62581},{\"end\":62596,\"start\":62594},{\"end\":62608,\"start\":62604},{\"end\":62620,\"start\":62618},{\"end\":62635,\"start\":62630},{\"end\":62654,\"start\":62650},{\"end\":62669,\"start\":62663},{\"end\":62680,\"start\":62677},{\"end\":62698,\"start\":62692},{\"end\":62711,\"start\":62706},{\"end\":62724,\"start\":62720},{\"end\":63160,\"start\":63156},{\"end\":63169,\"start\":63167},{\"end\":63185,\"start\":63180},{\"end\":63200,\"start\":63194},{\"end\":63217,\"start\":63212},{\"end\":63239,\"start\":63226},{\"end\":63262,\"start\":63255},{\"end\":63498,\"start\":63495},{\"end\":63512,\"start\":63508},{\"end\":63521,\"start\":63519},{\"end\":63529,\"start\":63527},{\"end\":63544,\"start\":63537},{\"end\":63564,\"start\":63554},{\"end\":63574,\"start\":63571},{\"end\":63951,\"start\":63949},{\"end\":63961,\"start\":63957},{\"end\":63976,\"start\":63972},{\"end\":63988,\"start\":63986},{\"end\":64001,\"start\":63999},{\"end\":64016,\"start\":64010},{\"end\":64031,\"start\":64028},{\"end\":64045,\"start\":64041},{\"end\":64057,\"start\":64052},{\"end\":64421,\"start\":64418},{\"end\":64435,\"start\":64432},{\"end\":64446,\"start\":64442},{\"end\":64462,\"start\":64457}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":62014,\"start\":61716},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":246411621},\"end\":62500,\"start\":62016},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":221970302},\"end\":63072,\"start\":62502},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":52822214},\"end\":63486,\"start\":63074},{\"attributes\":{\"doi\":\"arXiv:2210.03629\",\"id\":\"b4\"},\"end\":63856,\"start\":63488},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":252408513},\"end\":64408,\"start\":63858},{\"attributes\":{\"doi\":\"arXiv:2101.00774\",\"id\":\"b6\"},\"end\":64826,\"start\":64410}]", "bib_title": "[{\"end\":62085,\"start\":62016},{\"end\":62572,\"start\":62502},{\"end\":63147,\"start\":63074},{\"end\":63940,\"start\":63858}]", "bib_author": "[{\"end\":61791,\"start\":61779},{\"end\":61809,\"start\":61791},{\"end\":61830,\"start\":61809},{\"end\":61846,\"start\":61830},{\"end\":61861,\"start\":61846},{\"end\":62098,\"start\":62087},{\"end\":62111,\"start\":62098},{\"end\":62128,\"start\":62111},{\"end\":62143,\"start\":62128},{\"end\":62157,\"start\":62143},{\"end\":62166,\"start\":62157},{\"end\":62176,\"start\":62166},{\"end\":62184,\"start\":62176},{\"end\":62194,\"start\":62184},{\"end\":62200,\"start\":62194},{\"end\":62588,\"start\":62574},{\"end\":62598,\"start\":62588},{\"end\":62610,\"start\":62598},{\"end\":62622,\"start\":62610},{\"end\":62637,\"start\":62622},{\"end\":62656,\"start\":62637},{\"end\":62671,\"start\":62656},{\"end\":62682,\"start\":62671},{\"end\":62700,\"start\":62682},{\"end\":62713,\"start\":62700},{\"end\":62726,\"start\":62713},{\"end\":63162,\"start\":63149},{\"end\":63171,\"start\":63162},{\"end\":63187,\"start\":63171},{\"end\":63202,\"start\":63187},{\"end\":63219,\"start\":63202},{\"end\":63241,\"start\":63219},{\"end\":63264,\"start\":63241},{\"end\":63500,\"start\":63488},{\"end\":63514,\"start\":63500},{\"end\":63523,\"start\":63514},{\"end\":63531,\"start\":63523},{\"end\":63546,\"start\":63531},{\"end\":63566,\"start\":63546},{\"end\":63576,\"start\":63566},{\"end\":63953,\"start\":63942},{\"end\":63963,\"start\":63953},{\"end\":63978,\"start\":63963},{\"end\":63990,\"start\":63978},{\"end\":64003,\"start\":63990},{\"end\":64018,\"start\":64003},{\"end\":64033,\"start\":64018},{\"end\":64047,\"start\":64033},{\"end\":64059,\"start\":64047},{\"end\":64423,\"start\":64410},{\"end\":64437,\"start\":64423},{\"end\":64448,\"start\":64437},{\"end\":64464,\"start\":64448}]", "bib_venue": "[{\"end\":61777,\"start\":61716},{\"end\":62249,\"start\":62200},{\"end\":62778,\"start\":62726},{\"end\":63269,\"start\":63264},{\"end\":63650,\"start\":63592},{\"end\":64124,\"start\":64059},{\"end\":64601,\"start\":64480}]"}}}, "year": 2023, "month": 12, "day": 17}