{"id": 17144146, "updated": "2023-03-25 15:48:04.819", "metadata": {"title": "Event Detection in Crowded Videos", "authors": "[{\"first\":\"Yan\",\"last\":\"Ke\",\"middle\":[]},{\"first\":\"R.\",\"last\":\"Sukthankar\",\"middle\":[]},{\"first\":\"M.\",\"last\":\"Hebert\",\"middle\":[]}]", "venue": "2007 IEEE 11th International Conference on Computer Vision", "journal": "2007 IEEE 11th International Conference on Computer Vision", "publication_date": {"year": 2007, "month": null, "day": null}, "abstract": "Real-world actions occur often in crowded, dynamic environments. This poses a difficult challenge for current approaches to video event detection because it is difficult to segment the actor from the background due to distracting motion from other objects in the scene. We propose a technique for event recognition in crowded videos that reliably identifies actions in the presence of partial occlusion and background clutter. Our approach is based on three key ideas: (1) we efficiently match the volumetric representation of an event against oversegmented spatio-temporal video volumes; (2) we augment our shape-based features using flow; (3) rather than treating an event template as an atomic entity, we separately match by parts (both in space and time), enabling robustness against occlusions and actor variability. Our experiments on human actions, such as picking up a dropped object or waving in a crowd show reliable detection with few false positives.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2137981002", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/KeSH07", "doi": "10.1109/iccv.2007.4409011"}}, "content": {"source": {"pdf_hash": "afaf36f3ac048510b5bed668751c1219ae6a0b1e", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://www-2.cs.cmu.edu/~rahuls/pub/iccv2007-rahuls.pdf", "status": "GREEN"}}, "grobid": {"id": "03dcc0ce81dbb7367caf4b134aea4837bafef788", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/afaf36f3ac048510b5bed668751c1219ae6a0b1e.txt", "contents": "\nEvent Detection in Crowded Videos\n\n\nYan Ke \nSchool of Computer Science\nCarnegie Mellon\n\n\nRahul Sukthankar rahuls@cs.cmu.edu \nSchool of Computer Science\nCarnegie Mellon\n\n\nIntel Research Pittsburgh\n\n\nMartial Hebert hebert@cs.cmu.edu \nSchool of Computer Science\nCarnegie Mellon\n\n\nEvent Detection in Crowded Videos\n\nReal-world actions occur often in crowded, dynamic environments. This poses a difficult challenge for current approaches to video event detection because it is difficult to segment the actor from the background due to distracting motion from other objects in the scene. We propose a technique for event recognition in crowded videos that reliably identifies actions in the presence of partial occlusion and background clutter. Our approach is based on three key ideas: (1) we efficiently match the volumetric representation of an event against oversegmented spatio-temporal video volumes; (2) we augment our shape-based features using flow; (3) rather than treating an event template as an atomic entity, we separately match by parts (both in space and time), enabling robustness against occlusions and actor variability. Our experiments on human actions, such as picking up a dropped object or waving in a crowd show reliable detection with few false positives.\n\nIntroduction\n\nThe goal of event detection is to identify and localize specified spatio-temporal patterns in video, such as a person waving his or her hand. As observed by Ke et al. [14] and Shechtman & Irani [24], the task is similar to object detection in many respects since the pattern can be located anywhere in the scene (in both space and time) and requires reliable detection in the presence of significant background clutter. Event detection is thus distinct from the problem of human action recognition, where the primary goal is to classify a short video sequence of an actor performing an unknown action into one of several classes [2,23,32].\n\nOur goal is to perform event detection in challenging real-world conditions where the action of interest is masked by the activity of a dynamic and crowded environment.\n\nConsider the examples shown in Figure 1. In Figure 1(a), the person waving his hand to flag down a bus is partially occluded, and his arm motion occurs near pedestrians that generate optical flow in the image. The scene also contains multiple moving objects and significant clutter that make it The person picking up the dropped object is matched even though the scene is very cluttered and the dominant motion is that of the crowd in the background.\n\ndifficult to cleanly segment the actor from the background. In Figure 1(b), the goal is to detect the person picking up an object from the floor. In this case, the image flow is dominated by the motion of the crowd surrounding the actor, and the actor's clothing blends into the scene given the poor lighting conditions. Earlier work has identified several promising strategies that could be employed for event detection. These can be broadly categorized into approaches based on tracking [21,26], flow [8,14,24], spatio-temporal shapes [2,3,31,32], and interest points [7,20,23]. A more comprehensive review of historical work is presented by Aggarwal and Cai [1]. Methods based on tracking process the video frame-byframe and segment an object of interest from background clutter, typically by matching the current frame against a model. By following the object's motion through time, a trace of model parameters is generated; this trace can be compared with that of the target spatio-temporal pattern to determine whether the observed event is of interest. Tracking-based approaches can incorporate existing domain knowledge about the target event in the model (e.g., joint angle limits in human kinematic models) and the system can support online queries since the video is processed a single frame at a time. However, initializing tracking models can be difficult, particularly when the scene contains distracting objects. And while recent work has demonstrated significant progress in cluttered environments [22], tracking remains challenging in such environments, and the tracker output tends to be noisy. An alternate approach to tracking-based event detection focuses on multi-agent activities, where each actor is tracked as a blob and activities are classified based on observed locations and spatial interactions between blobs [12,13]. These models are well-suited for expressing activities such as loitering, meeting, arrival and departure; the focus of our work is on finer-grained events where the body pose of the actor is critical to recognition.\n\nFlow-based methods for event detection operate directly on the spatio-temporal sequence, attempting to recognize the specified pattern by brute-force correlation without segmentation. Efros et al. correlate flow templates with videos to recognize actions at a distance [8]. Ke et al. [14] train a cascade of boosted classifiers to process the vertical and horizontal components of flow in a video sequence using an algorithm that is similar in spirit to Viola and Jones' object detector for still images [27]. Shechtman and Irani propose an algorithm for correlating spatio-temporal event templates against videos without explicitly computing the optical flow, which can be noisy on object boundaries [24].\n\nShape-based methods treat the spatio-temporal volume of a video sequence as a 3D object. Different events in video generate distinctive shapes, and the goal of such methods is to recognize an event by recognizing its shape. Shape-based methods employ a variety of techniques to characterize the shape of an event, such as shape invariants [2,32]. For computational efficiency and greater robustness to action variations, Bobick and Davis [3] project the spatio-temporal volume down to motion-history images, which Weinland et al. extend to motion-history volumes [31]. These techniques work best when the action of interest is performed in a setting that enables reliable segmentation. In particular, for static scenes, techniques such as background subtraction can generate high-quality spatio-temporal volumes that are amenable to this analysis. Unfortunately, these conditions do not hold in typical real-world videos due to the presence of multiple moving objects and scene clutter. Similarly, the extensive research on generalizing shape matching [11,17] requires reliable figure/ground separation, which is infeasible in crowded scenes using current segmentation techniques. In this paper, we show how ideas from shapebased event detection can be extended to operate on oversegmented spatio-temporal volumes and perform well in challenging conditions.\n\nRecently, space-time interest points [16] have become popular in the action recognition community [7,20,23], with many parallels to how traditional interest points [18] have been applied for object recognition. While the sparsity of interest points and their resulting computational efficiency is appealing, space-time interest points suffer the same drawbacks as their 2D analogues, such as failure to capture smooth motions and tendency to generate spurious detections at object boundaries.\n\nWe synthesize key ideas from each of the previous approaches and propose an algorithm to enable event detection in real-world crowded videos (Section 2). This paper focuses primarily on two topics: (1) effective representations of shape and motion for event detection, and (2) efficient matching of event models to over-segmented spatiotemporal volumes. The models that we match are derived from a single example and are manually constructed; automatic generation of event models from weakly-labeled observations is a related interesting problem and is not covered in the current work. This paper is organized as follows. First, we show that spatio-temporal shapes are useful features for event detection. Where the previous work is typically limited to scenes with static backgrounds, we demonstrate shape matching in cluttered scenes with dynamic backgrounds. Second, we combine our shape descriptor with Shechtman and Irani's flow descriptor, which is a complementary feature that can be computed in cluttered environments without figure/ground separation (Section 3). Third, recognizing the value of a parts-based representation, which is explicitly modeled by the human tracking approaches, and implic-itly modeled by the interest-point approaches, we break our action templates into parts and extend the pictorial structures algorithm [9,10] to 3D parts for recognition (Section 4). Finally, we present an evaluation of event detection on crowded videos in Section 5. Figure 2 presents an overview of the approach.\n\n\nShape Matching\n\nWe briefly review the shape matching algorithm proposed in [15]. Space-time events are represented as spatiotemporal volumes in our system, as shown in Figure 2. The target events that we wish to recognize are typically one second long, and represent actions such as picking up an object from the ground, or a hand-wave. Denoting the template as T and the video volume as V , detecting the event involves sliding the template across all possible locations l in V and measuring the shape matching distance between T and V . An event is detected when the distance falls below a specified threshold. Similar to other sliding-window detection techniques, this is a rare-event detection task and therefore keeping the false-positive rate low is extremely important.\n\nThe first step is to extract spatio-temporal shape contours in the video using an unsupervised clustering technique. This enables us to ignore highly variable and potentially irrelevant features of the video such as color and texture, while preserving the object boundaries needed for shape classification. As a preprocessing step, the video is automatically segmented into regions in space-time using mean shift, with color and location as the input features [5,6,29]. This is the spatio-temporal equivalent of the concept of superpixels [19]. Figure 3 shows an example video sequence and the resulting segmentation. Note that there is no explicit figure/ground separation in the segmentation and that the objects are over-segmented. The degree to which the video is over-segmented can be adjusted by changing the kernel bandwidth. However, since finding an \"optimal\" bandwidth is difficult and not very meaningful, we use a single value of the bandwidth in all of our experiments, which errs on the side of over-rather than under-segmentation. Processing the video as a spatio-temporal volume (rather than frame-byframe) results in better segmentations by preserving temporal continuity. We have found mean shift to work well in our task, but in general, any segmentation algorithm could be used as long as it produces an over-segmentation that tends to preserve object boundaries.\n\nOur shape matching metric is based on the region intersection distance between the template volume and the set of over-segmented volumes in the video. Given two binary shapes, A and B, a natural distance metric between them is the set difference between the union and the intersection of the region, i.e., |A \u222a B \\ A \u2229 B|. Because of the over-segmentation, a video volume V at some location l = (x, y, t) is composed of a set of k regions such  Figure 4. A naive approach for computing the optimal distance between a template volume T and V is to enumerate through the 2 k subsets of V , compute the voxel distance between T and each subset, and choose the minimum. This would be prohibitively expensive even with a small number of regions. In [15], we showed that whether each region V i should belong in the optimal set can be decided independently of all other regions, and therefore the distance computation is linear in k, the number of over-segmented regions. The distance between the template T and the volume V at location l is defined as\nthat V = \u222a k i=1 V i , as shown ind(T, V ; l) = k i=1 d(T, V i ; l),(1)\nwhere\nd(T, V i ; l) = |T \u2229 V i | if |T \u2229 V i | < |V i |/2 |V i \u2212 T \u2229 V i | otherwise.\n(2) This distance metric is equivalent to choosing the optimal set of over-segmented regions and computing the region intersection distance.\n\nA consequence of employing automatic segmentation (as opposed to figure/ground separation) is that some objects will be over-segmented. Regions that are highly textured could be finely over-segmented, and therefore would result in a low matching distance to any template. To reduce this sensitivity to the segmentation granularity, we introduce a normalizing term that accounts for the flexibility of the candidate regions in matching arbitrary templates. The normalized distance is\nd N (T, V ; l) = d(T, V ; l) E T [d(\u00b7, V ; l)] ,(3)\nwhere the denominator is the expected distance of a template to the volume V , averaged over T , the set of all pos- Figure 4: Example showing how a template is matched to an over-segmented volume using our shape matching method. The template is drawn in bold, and the distance (mismatch) is the area of the shaded region.\n\nsible templates that fit within V . By enumerating through all possible templates that fit in V , we derive the expected distance to be\nk i=1 1 2 |Vi| |Vi|\u22121 j=1 |V i | j min(j, |V i | \u2212 j).(4)\nNote that the above term is a function of only |V i |. Therefore, we can pre-compute this term so that the run-time computation reduces to table look-ups.\n\n\nFlow Matching\n\nOptical flow has been shown to be a useful feature for event detection in video [8,14,24]. Similar to our shape descriptor, it is invariant to appearance and lighting changes and does not require figure/ground separation. Shechtman and Irani [24] introduced a flow-based correlation technique that has been shown to complement our shape descriptor for event detection [15]. Given two spatial-temporal patches (of size 7 \u00d7 7 \u00d7 3) centered at P 1 = T (x 1 , y 1 , t 1 ) and P 2 = V (x 2 , y 2 , t 2 ), traditional matching algorithms first compute the flow vectors, and then compute the distance between the flows. The results are often noisy or even inaccurate at object boundaries. Instead, Shechtman and Irani's algorithm computes whether the same flow could have generated the patches observed in the template and the video. The local inconsistency in motion between two patches P 1 and P 2 is given by\nm 12 = \u2206r 12 min(\u2206r 1 , \u2206r 2 ) + ,(5)\nwhere \u2206r is the rank increase between the three dimensional and the two dimensional Harris matrix of the patch from P 1 , P 2 , or the concatenation of the two patches in the case of \u2206r 12 . The flow correlation distance is therefore\nd F (T, V ; l) = i\u2208T,j\u2208(T \u2229V ) m ij |T | .(6)\nOur implementation of this algorithm generates an unusually-high number of false positives in regions with little texture. From the equations, it is obvious that uniform regions have indeterminate flow, and therefore could match any possible template. To eliminate such cases, we add a pre-filtering step to Shechtman and Irani's algorithm that discards uniform regions by thresholding on the Harris score of the region. We discuss how we combine the shape and flow distance metrics in the next section.\n\n\nRecognition\n\nThe previous section describes a method for matching volumetric shape features on automatically-segmented video. The main strength of the algorithm is that it can perform shape matching without precise object masks in the input video [2,3,32]. Further, using template-based matching enables search with only one training example. However, like all template-based matching techniques [3,24], it suffers from limited generalization power due to the variability in how different people perform the same action. A standard approach to improve generalization is to break the model into parts, allowing the parts to move independently, and to measure the joint appearance and geometric matching score of the parts. Allowing the parts to move makes the template more robust to the spatial and temporal variability of actions. This idea has been studied extensively in recognition in both images [30] and video [4,25]. Therefore, we extend our baseline matching algorithm by introducing a parts-based volumetric shape-matching model. Specifically, we extend the pictorial structures framework [9,10] to video volumes to model the geometric configuration of the parts and to find the optimal match in both appearance and configuration in the video.\n\n\nMatching Parts\n\nA key feature of our baseline algorithm is that it can perform shape matching with over-segmented regions. However, it assumes that the template consists of a single region, and that only the video is over-segmented. Given a single template, one must use prior knowledge to break the template into parts. For events that consist of human actions, these parts typically correspond to the rigid sections of the human body, and therefore the process is straightforward. We illustrate how one might manually break the handwave template into parts, as shown in Figure 5. We note that, for this action, only the upper body moves while the legs remain stationary. Therefore, a natural break should  be at the actor's waist. Such a break would allow the template parts to match people with different heights. Another natural break would be to split the top half of the action temporally, thus producing two parts that correspond to the upward and downward swing of the handwave action. This allows for some variation in the speed with which people swing their arms. It is important to note that, just like the whole template, the parts are also spatio-temporal volumes and could represent a body part in motion.\n\nWe now generalize our baseline algorithm (Section 2) and describe how we match template parts to oversegmented regions. Consider the oval template that has been split into two parts in the toy example illustrated in Figure 6. Although the whole template matches the oval (V 1 \u222a V 2 \u222a V 3 ) in the candidate volume, the parts would match poorly because the over-segmentation is inconsistent with the boundaries between the two parts. For example, our baseline algorithm would not match Part 1 to V 1 , nor Part 2 to V 3 . In general, there is no reason to believe that they should match because some of the part boundaries are artificially created (as shown by the dashed lines) and do not necessarily correspond to any real object boundaries. Our solution is to introduce additional cuts using a virtual plane that is aligned to and moves with the template part. For example, as we slide Part 1 across the video, we subdivide all the regions that intersect with the cutting plane placed on the right edge of the Part 1. V 2 is divided correctly, and Part 1 now matches the union of V 1 and the shaded region of V 2 . For convenience, we only use cutting planes that are aligned with the principal axes, but in general the plane can be oriented in any direction. By pre-computing the cuts and with judicious bookkeeping, the parts-based matching can be performed with the same computational efficiency as our baseline shape-based matching algorithm.\n\n\nMatching Part Configuration\n\nWe now describe how the framework of pictorial structures [9,10] can be extended to parts-based event detection Figure 6: Illustration of how we artificially cut the candidate volume to match how the whole template is split into its constituent parts. The candidate volume is dynamically cut as we slide the template parts along it. The cutting process is very efficient.\n\nin video. Intuitively, each part in the template should match the video well, and the relative locations of parts should be in a valid geometric configuration. More formally, consider a set of n parts that form a tree in a graph. Adopting a notation based on Felzenszwalb and Huttenlocher [9], let the part model be specified by a graph G = (P, E). Template part T i is represented as a vertex p i \u2208 P and the connection between parts p i and p j is represented as an edge (p i , p j ) \u2208 E. The configuration of the parts is specified by L = (l 1 , . . . , l n ), where l i = (x i , y i , t i ) is the location of part T i in the candidate volume V . Let a i (l i ) be the distance in appearance between the template part T i and the video at location l i . Let d ij (l i , l j ) be the distance in configuration between parts T i and T j when they are placed at locations l i and l j , respectively. The general energy function that we want to minimize for an optimal match is:\nL * = argmin L \uf8eb \uf8ed n i=1 a i (l i ) + (vi,vj )\u2208E d ij (l i , l j ) \uf8f6 \uf8f8 . (7)\nThe appearance distance a() is a linear combination of our normalized distance metric (Equation 3) and Irani & Shechtman's flow-based correlation distance:\na i (l i ) = d N (T i , V ; l i ) + \u03b1d F (T i , V ; l i ),(8)\nwhere \u03b1 = 0.2 (we use the same weight for all experiments). For matching efficiency, our parts model is organized in a tree structure and we model the relative position of each part as a Gaussian with a diagonal covariance matrix. Therefore, where s ij is the mean offset and \u03a3 ij is the diagonal covariance. \u03b2 adjusts the relative weight of the configuration vs. appearance terms and for all of our experiments we use \u03b2 = 0.02. The mean offset is taken from the location where we cut the parts, and the covariance is set manually, typically around 10% of the template size. Learning this matrix from multiple templates is part of our future work. As described by Felzenszwalb & Huttenlocher [9], the minimization can be efficiently solved using distance transforms and dynamic programming. Because we employ a sliding window approach to event detection, we also record the actual distance solved in the minimization and threshold on that distance. Only those locations with a distance below a specified threshold are considered as detections. As discussed earlier, a key feature of our algorithm is that although a segmented instance of the event template is needed, we do not assume that the input video can be reliably segmented. This makes event detection possible in challenging cases, such as crowded scenes, where reliable segmentation is difficult.\nd ij (l i , l j ) = \u03b2N (l i \u2212 l j , s ij , \u03a3 ij ),(9)\n\nResults\n\nTo evaluate the effectiveness of our algorithms, we selected events that represent real world actions such as picking up an object from the ground, waving for a bus, or pushing an elevator button (Figure 7). In our previous work [15], we evaluated our initial matching algorithms on standard datasets (e.g., the KTH datasets [23]). This data was appropriate to evaluate the basic matching capabilities, but it is too \"clean\" to evaluate the effectiveness of the techniques described in this paper. Therefore, we acquired new videos by using a hand-held camera in environments with moving people or cars in the background. This data is designed to evaluate the performance of the algorithm in crowded scenes. We study the effects of using different combinations of shape and flow descriptors, and parts-based versus whole shape models. One subject performed one instance of each action for training 1 . Between three to six other subjects performed multiple instances of the actions for testing. We collected approximately twenty minutes of video containing 110 events of interest. The videos were downscaled to 160x120 in resolution. There is high variability in both how the subjects performed the actions and in the background clutter. There are also significant spatial and temporal scale differences in the actions as well.\n\nFor each event, we create the model from a single instance by interactively segmenting the spatio-temporal volume using a graph-cut tool similar to [28]. The templates are typically 60 \u00d7 80 \u00d7 30 in size and range from 20,000-80,000 voxels. The whole template is then manually broken into parts, as shown in Figure 7. The video is automatically segmented using mean shift; the average segment size is approximately 100 voxels. We scan the event template over the videos using the shape and flow distance metrics described earlier, and combine them using pictorial structures. There are approximately 120,000 possible locations to be scanned per second of video for a typical template. In these experiments, to evaluate the robustness of our matching algorithm to variations in observed scale, we match only at a single fixed scale; in practice, one could match over multiple scales. The algorithm returns a three-dimensional distance map representing the matching distance between the model and the video at every location in the video. For efficiency, we project the map to a one-dimensional vector of scores, keeping only the best detection for each frame, as shown in Figure 8(a). Since it is rare for two instances of an action to start and end at exactly the same time instant, this is a reasonable simplification. An event is detected when the matching distance falls below a specified threshold. We vary this threshold and count the number of true positives and false positives to generate the Precision-Recall graphs. A detected event is considered a true positive if it has greater than 50% overlap (in space-time) with the labeled event.\n\nWe now analyze the performance of our algorithm and compare it to baseline methods. Figure 7 shows example detections using our algorithm with the parts-based shape and flow descriptor in crowded scenes. Note the amount of clutter and movement from other people near the event. The precision-recall graphs for all of the actions are shown in Figures 8(b)-(f). We compare our results to Shechtman and Irani's flow consistency method [24] as a baseline, labeled as Flow (Whole) in our graphs. This stateof-the-art baseline method achieves low precision and recall in nearly all actions, demonstrating the difficulty of our dataset. Our combined parts-based shape and flow descriptor is significantly better and outperforms either descriptor alone, which confirms our previous findings [15]. The parts-based shape descriptor is better than the whole shape descriptor in the hand-wave, push button, and two-handed 1 The two-handed wave template was taken from the KTH videos. wave actions, while there is little benefit to adding the parts model for the jumping-jacks and pick-up actions.\n\n\nConclusion\n\nWe present a method for detecting events in crowded videos. The video is treated as a spatio-temporal volume and events are detected using our volumetric shape descriptor in combination with Shechtman and Irani's flow descriptor. Unlike existing shape-based methods, our system does not require figure/ground separation, and is thus more applicable to real-world settings. We extend our baseline shape matching algorithm to detect event parts (sliced in both space or time), and generalize the model to recognize actions with higher actor variability. The parts are combined using pictorial structures to find the optimal configuration. Our approach detects events in difficult situations containing highly-cluttered dynamic backgrounds, and signficantly out-performs the baseline method [24]. This paper emphasizes the matching aspects of event detection and demonstrates robust performance on real-world videos. The biggest limitation of the current work is that the model is derived from a single exemplar of the event, thus limiting our ability to generalize across observed event variations. Future work will focus on the modeling aspects of the task, including the automatic selection of event parts and the aggregation of several training videos into a single model. Initial results show that greater generalization performance can be achieved by combining the matching scores from multiple event models.  Our parts-based shape and flow descriptor significantly outperforms all other descriptors. The baseline method [24], labeled as \"Flow (Whole)\", achieves low precision and recall in most actions, demonstrating the difficulty of our dataset.\n\nFigure 1 :\n1Examples of successful event detection in crowded settings. (a) The hand wave is detected despite the partial occlusion and moving objects near the actor's hand; (b)\n\nFigure 2 :\n2An overview of the proposed approach. An event model is constructed from a single training example and efficiently matched against oversegmented spatio-temporal volumes.\n\nFigure 3 :\n3Input video and corresponding spatio-temporal segmentation using mean shift. The action is composed of a set of over-segmented regions.\n\nFigure 5 :\n5To generalize the model and allow for more variability in the action, we break the action template (a) into parts (b). The model can be split in both space or time to generate the parts.\n\nFigure 7 :\n7Examples of event detection in crowded video. Training sequences and event models are shown on the left. Detections in several challenging test sequences are shown on the right. The action mask from the appropriate time in the event model is overlaid on the test sequence frame, and a bounding box marks the matched location of each part.\n\nFigure 8 :\n8(a) Projected matching distance on video with three pick-up events. A threshold of 0.6 successfully detects all of them. (b)-(f) Precision/recall curves for a variety of events.\nAcknowledgementsThis work was supported by NSF Grant IIS-0534962. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.\nHuman motion analysis: A review. J K Aggarwal, Q Cai, Computer Vision and Image Understanding. 733J. K. Aggarwal and Q. Cai. Human motion analysis: A review. Computer Vision and Image Understanding, 73(3):428-440, 1999.\n\nActions as space-time shapes. M Blank, L Gorelick, E Shechtman, M Irani, R Basri, Proc. ICCV. ICCVM. Blank, L. Gorelick, E. Shechtman, M. Irani, and R. Basri. Actions as space-time shapes. In Proc. ICCV, 2005.\n\nThe recognition of human movement using temporal templates. A F Bobick, J W Davis, PAMI23A. F. Bobick and J. W. Davis. The recognition of human movement using temporal templates. PAMI, 23(3), 2001.\n\nSimilarity by composition. O Boiman, M Irani, NIPS. O. Boiman and M. Irani. Similarity by composition. In NIPS, 2006.\n\nMean shift, mode seeking, and clustering. Y Cheng, PAMI17Y. Cheng. Mean shift, mode seeking, and clustering. PAMI, 17(8), 1995.\n\nSpatio-temporal segmentation of video by hierarchical mean shift analysis. D Dementhon, Statistical Methods in Video Processing Workshop. D. DeMenthon. Spatio-temporal segmentation of video by hierarchical mean shift analysis. In Statistical Methods in Video Processing Workshop, 2002.\n\nBehavior recognition via sparse spatio-temporal features. P Dollar, V Rabaud, G Cottrell, S Belongie, IEEE VS-PETS Workshop. P. Dollar, V. Rabaud, G. Cottrell, and S. Belongie. Behavior recognition via sparse spatio-temporal features. In IEEE VS- PETS Workshop, 2005.\n\nRecognizing action at a distance. A Efros, A Berg, G Mori, J Malik, Proc. ICCV. ICCVA. Efros, A. Berg, G. Mori, and J. Malik. Recognizing action at a distance. In Proc. ICCV, 2003.\n\nPictorial structures for object recognition. P Felzenszwalb, D Huttenlocher, IJCV. 161P. Felzenszwalb and D. Huttenlocher. Pictorial structures for object recognition. IJCV, 61(1), 2005.\n\nThe representation and matching of pictorial structures. M A Fischler, R A Elschlager, IEEE Trans. on Computers. 221M. A. Fischler and R. A. Elschlager. The representation and matching of pictorial structures. IEEE Trans. on Computers, 22(1), Jan. 1973.\n\nShape representation and classification using the poisson equation. L Gorelick, M Galun, E Sharon, R Basri, A Brandt, PAMI. 2812L. Gorelick, M. Galun, E. Sharon, R. Basri, and A. Brandt. Shape representation and classification using the poisson equation. PAMI, 28(12), 2006.\n\nDiscovery and characterization of activities from event-streams. R Hamid, S Maddi, A Johnson, A Bobick, I Essa, C Isbell, Proc. UAI. UAIR. Hamid, S. Maddi, A. Johnson, A. Bobick, I. Essa, and C. Isbell. Discovery and characterization of activities from event-streams. In Proc. UAI, 2005.\n\nMulti-agent event recognition. S Hongeng, R Nevatia, Proc. ICCV. ICCVS. Hongeng and R. Nevatia. Multi-agent event recognition. In Proc. ICCV, 2001.\n\nEfficient visual event detection using volumetric features. Y Ke, R Sukthankar, M Hebert, Proc. ICCV. ICCVY. Ke, R. Sukthankar, and M. Hebert. Efficient visual event detection using volumetric features. In Proc. ICCV, 2005.\n\nSpatio-temporal shape and flow correlation for action recognition. Y Ke, R Sukthankar, M Hebert, Workshop on Visual Surveillance. Y. Ke, R. Sukthankar, and M. Hebert. Spatio-temporal shape and flow correlation for action recognition. In Workshop on Visual Surveillance, 2007.\n\nSpace-time interest points. I Laptev, T Lindeberg, Proc. ICCV. ICCVI. Laptev and T. Lindeberg. Space-time interest points. In Proc. ICCV, 2003.\n\nShape classification using the inner-distance. H Ling, D W Jacobs, PAMI. 292H. Ling and D. W. Jacobs. Shape classification using the inner-distance. PAMI, 29(2), 2007.\n\nDistinctive image features from scale-invariant keypoints. D G Lowe, IJCV. 602D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60(2), 2004.\n\nGuiding model search using segmentation. G Mori, Proc. ICCV. ICCVG. Mori. Guiding model search using segmentation. In Proc. ICCV, 2005.\n\nUnsupervised learning of human action categories using spatial-temporal words. J C Niebles, H Wang, L Fei-Fei, Proc. BMVC. BMVCJ. C. Niebles, H. Wang, and L. Fei-Fei. Unsupervised learn- ing of human action categories using spatial-temporal words. In Proc. BMVC, 2006.\n\nAutomatic annotation of everyday movements. D Ramanan, D A Forsyth, NIPS. D. Ramanan and D. A. Forsyth. Automatic annotation of everyday movements. In NIPS, 2003.\n\nTracking people by learning their appearance. D Ramanan, D A Forsyth, A Zisserman, PAMI29D. Ramanan, D. A. Forsyth, and A. Zisserman. Tracking people by learning their appearance. PAMI, 29(1), 2007.\n\nRecognizing human actions: A local SVM approach. C Schuldt, I Laptev, B Caputo, Proc. ICPR. ICPRC. Schuldt, I. Laptev, and B. Caputo. Recognizing human actions: A local SVM approach. In Proc. ICPR, 2004.\n\nSpace-time behavior based correlation. E Shechtman, M Irani, Proc. CVPR. CVPRE. Shechtman and M. Irani. Space-time behavior based cor- relation. In Proc. CVPR, 2005.\n\nMatching local self-similarities across images and video. E Shechtman, M Irani, Proc. CVPR. CVPRE. Shechtman and M. Irani. Matching local self-similarities across images and video. In Proc. CVPR, 2007.\n\nExploring the space of a human action. Y Sheikh, M Sheikh, M Shah, Proc. ICCV. ICCVY. Sheikh, M. Sheikh, and M. Shah. Exploring the space of a human action. In Proc. ICCV, 2005.\n\nRobust real-time face detection. P Viola, M Jones, IJCV. 572P. Viola and M. Jones. Robust real-time face detection. IJCV, 57(2), 2004.\n\nInteractive video cutout. J Wang, P Bhat, A Colburn, M Agrawala, M Cohen, ACM SIGGRAPH. J. Wang, P. Bhat, A. Colburn, M. Agrawala, and M. Cohen. Interactive video cutout. In ACM SIGGRAPH, 2005.\n\nImage and video segmentation by anisotropic kernel mean shift. J Wang, B Thiesson, Y Xu, M Cohen, Proc. ECCV. ECCVJ. Wang, B. Thiesson, Y. Xu, and M. Cohen. Image and video segmentation by anisotropic kernel mean shift. In Proc. ECCV, 2004.\n\nUnsupervised learning of models for recognition. M Weber, M Welling, P Perona, Proc. ECCV. ECCVM. Weber, M. Welling, and P. Perona. Unsupervised learning of models for recognition. In Proc. ECCV, 2000.\n\nFree viewpoint action recognition using motion history volumes. D Weinland, R Ronfard, E Boyer, Computer Vision and Image Understanding. 1042D. Weinland, R. Ronfard, and E. Boyer. Free viewpoint ac- tion recognition using motion history volumes. Computer Vision and Image Understanding, 104(2), 2006.\n\nActions as objects: A novel action representation. A Yilmaz, M Shah, Proc. CVPR. CVPRA. Yilmaz and M. Shah. Actions as objects: A novel action representation. In Proc. CVPR, 2005.\n", "annotations": {"author": "[{\"end\":89,\"start\":37},{\"end\":198,\"start\":90},{\"end\":277,\"start\":199}]", "publisher": null, "author_last_name": "[{\"end\":43,\"start\":41},{\"end\":106,\"start\":96},{\"end\":213,\"start\":207}]", "author_first_name": "[{\"end\":40,\"start\":37},{\"end\":95,\"start\":90},{\"end\":206,\"start\":199}]", "author_affiliation": "[{\"end\":88,\"start\":45},{\"end\":169,\"start\":126},{\"end\":197,\"start\":171},{\"end\":276,\"start\":233}]", "title": "[{\"end\":34,\"start\":1},{\"end\":311,\"start\":278}]", "venue": null, "abstract": "[{\"end\":1275,\"start\":313}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1462,\"start\":1458},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1489,\"start\":1485},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1923,\"start\":1920},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":1926,\"start\":1923},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":1929,\"start\":1926},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3047,\"start\":3043},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3050,\"start\":3047},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3060,\"start\":3057},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3063,\"start\":3060},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3066,\"start\":3063},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3094,\"start\":3091},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3096,\"start\":3094},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3099,\"start\":3096},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3102,\"start\":3099},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3127,\"start\":3124},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3130,\"start\":3127},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3133,\"start\":3130},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3218,\"start\":3215},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4072,\"start\":4068},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4397,\"start\":4393},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4400,\"start\":4397},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4891,\"start\":4888},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4907,\"start\":4903},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5127,\"start\":5123},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5324,\"start\":5320},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5669,\"start\":5666},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5672,\"start\":5669},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5768,\"start\":5765},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5894,\"start\":5890},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6383,\"start\":6379},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6386,\"start\":6383},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6727,\"start\":6723},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6787,\"start\":6784},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6790,\"start\":6787},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6793,\"start\":6790},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6854,\"start\":6850},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8524,\"start\":8521},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8527,\"start\":8524},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8782,\"start\":8778},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9944,\"start\":9941},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9946,\"start\":9944},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9949,\"start\":9946},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10024,\"start\":10020},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11614,\"start\":11610},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13521,\"start\":13518},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13524,\"start\":13521},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13527,\"start\":13524},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13684,\"start\":13680},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13810,\"start\":13806},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15417,\"start\":15414},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15419,\"start\":15417},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":15422,\"start\":15419},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15566,\"start\":15563},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15569,\"start\":15566},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16072,\"start\":16068},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16086,\"start\":16083},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16089,\"start\":16086},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16268,\"start\":16265},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16271,\"start\":16268},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19184,\"start\":19181},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19187,\"start\":19184},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19788,\"start\":19785},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21465,\"start\":21462},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22424,\"start\":22420},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22520,\"start\":22516},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23090,\"start\":23089},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23672,\"start\":23668},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25604,\"start\":25600},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25955,\"start\":25951},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26079,\"start\":26078},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27059,\"start\":27055},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27795,\"start\":27791}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28098,\"start\":27920},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28281,\"start\":28099},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28430,\"start\":28282},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28630,\"start\":28431},{\"attributes\":{\"id\":\"fig_5\"},\"end\":28982,\"start\":28631},{\"attributes\":{\"id\":\"fig_7\"},\"end\":29173,\"start\":28983}]", "paragraph": "[{\"end\":1930,\"start\":1291},{\"end\":2100,\"start\":1932},{\"end\":2552,\"start\":2102},{\"end\":4617,\"start\":2554},{\"end\":5325,\"start\":4619},{\"end\":6684,\"start\":5327},{\"end\":7178,\"start\":6686},{\"end\":8700,\"start\":7180},{\"end\":9479,\"start\":8719},{\"end\":10864,\"start\":9481},{\"end\":11912,\"start\":10866},{\"end\":11990,\"start\":11985},{\"end\":12211,\"start\":12071},{\"end\":12695,\"start\":12213},{\"end\":13070,\"start\":12748},{\"end\":13207,\"start\":13072},{\"end\":13420,\"start\":13266},{\"end\":14342,\"start\":13438},{\"end\":14614,\"start\":14381},{\"end\":15164,\"start\":14661},{\"end\":16419,\"start\":15180},{\"end\":17641,\"start\":16438},{\"end\":19091,\"start\":17643},{\"end\":19494,\"start\":19123},{\"end\":20474,\"start\":19496},{\"end\":20707,\"start\":20552},{\"end\":22126,\"start\":20770},{\"end\":23518,\"start\":22191},{\"end\":25166,\"start\":23520},{\"end\":26252,\"start\":25168},{\"end\":27919,\"start\":26267}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11947,\"start\":11913},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11984,\"start\":11947},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12070,\"start\":11991},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12747,\"start\":12696},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13265,\"start\":13208},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14380,\"start\":14343},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14660,\"start\":14615},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20551,\"start\":20475},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20769,\"start\":20708},{\"attributes\":{\"id\":\"formula_9\"},\"end\":22180,\"start\":22127}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1289,\"start\":1277},{\"attributes\":{\"n\":\"2.\"},\"end\":8717,\"start\":8703},{\"attributes\":{\"n\":\"3.\"},\"end\":13436,\"start\":13423},{\"attributes\":{\"n\":\"4.\"},\"end\":15178,\"start\":15167},{\"attributes\":{\"n\":\"4.1.\"},\"end\":16436,\"start\":16422},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19121,\"start\":19094},{\"attributes\":{\"n\":\"5.\"},\"end\":22189,\"start\":22182},{\"attributes\":{\"n\":\"6.\"},\"end\":26265,\"start\":26255},{\"end\":27931,\"start\":27921},{\"end\":28110,\"start\":28100},{\"end\":28293,\"start\":28283},{\"end\":28442,\"start\":28432},{\"end\":28642,\"start\":28632},{\"end\":28994,\"start\":28984}]", "table": null, "figure_caption": "[{\"end\":28098,\"start\":27933},{\"end\":28281,\"start\":28112},{\"end\":28430,\"start\":28295},{\"end\":28630,\"start\":28444},{\"end\":28982,\"start\":28644},{\"end\":29173,\"start\":28996}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2141,\"start\":2133},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2154,\"start\":2146},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2625,\"start\":2617},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8662,\"start\":8654},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8879,\"start\":8871},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10034,\"start\":10026},{\"end\":11319,\"start\":11311},{\"end\":12873,\"start\":12865},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":17002,\"start\":16994},{\"end\":17867,\"start\":17859},{\"end\":19243,\"start\":19235},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22397,\"start\":22387},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23835,\"start\":23827},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":24698,\"start\":24690},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":25260,\"start\":25252},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":25526,\"start\":25510}]", "bib_author_first_name": "[{\"end\":29464,\"start\":29463},{\"end\":29466,\"start\":29465},{\"end\":29478,\"start\":29477},{\"end\":29682,\"start\":29681},{\"end\":29691,\"start\":29690},{\"end\":29703,\"start\":29702},{\"end\":29716,\"start\":29715},{\"end\":29725,\"start\":29724},{\"end\":29923,\"start\":29922},{\"end\":29925,\"start\":29924},{\"end\":29935,\"start\":29934},{\"end\":29937,\"start\":29936},{\"end\":30089,\"start\":30088},{\"end\":30099,\"start\":30098},{\"end\":30223,\"start\":30222},{\"end\":30385,\"start\":30384},{\"end\":30655,\"start\":30654},{\"end\":30665,\"start\":30664},{\"end\":30675,\"start\":30674},{\"end\":30687,\"start\":30686},{\"end\":30900,\"start\":30899},{\"end\":30909,\"start\":30908},{\"end\":30917,\"start\":30916},{\"end\":30925,\"start\":30924},{\"end\":31093,\"start\":31092},{\"end\":31109,\"start\":31108},{\"end\":31293,\"start\":31292},{\"end\":31295,\"start\":31294},{\"end\":31307,\"start\":31306},{\"end\":31309,\"start\":31308},{\"end\":31559,\"start\":31558},{\"end\":31571,\"start\":31570},{\"end\":31580,\"start\":31579},{\"end\":31590,\"start\":31589},{\"end\":31599,\"start\":31598},{\"end\":31832,\"start\":31831},{\"end\":31841,\"start\":31840},{\"end\":31850,\"start\":31849},{\"end\":31861,\"start\":31860},{\"end\":31871,\"start\":31870},{\"end\":31879,\"start\":31878},{\"end\":32087,\"start\":32086},{\"end\":32098,\"start\":32097},{\"end\":32265,\"start\":32264},{\"end\":32271,\"start\":32270},{\"end\":32285,\"start\":32284},{\"end\":32497,\"start\":32496},{\"end\":32503,\"start\":32502},{\"end\":32517,\"start\":32516},{\"end\":32735,\"start\":32734},{\"end\":32745,\"start\":32744},{\"end\":32899,\"start\":32898},{\"end\":32907,\"start\":32906},{\"end\":32909,\"start\":32908},{\"end\":33080,\"start\":33079},{\"end\":33082,\"start\":33081},{\"end\":33231,\"start\":33230},{\"end\":33406,\"start\":33405},{\"end\":33408,\"start\":33407},{\"end\":33419,\"start\":33418},{\"end\":33427,\"start\":33426},{\"end\":33641,\"start\":33640},{\"end\":33652,\"start\":33651},{\"end\":33654,\"start\":33653},{\"end\":33807,\"start\":33806},{\"end\":33818,\"start\":33817},{\"end\":33820,\"start\":33819},{\"end\":33831,\"start\":33830},{\"end\":34010,\"start\":34009},{\"end\":34021,\"start\":34020},{\"end\":34031,\"start\":34030},{\"end\":34205,\"start\":34204},{\"end\":34218,\"start\":34217},{\"end\":34391,\"start\":34390},{\"end\":34404,\"start\":34403},{\"end\":34575,\"start\":34574},{\"end\":34585,\"start\":34584},{\"end\":34595,\"start\":34594},{\"end\":34748,\"start\":34747},{\"end\":34757,\"start\":34756},{\"end\":34877,\"start\":34876},{\"end\":34885,\"start\":34884},{\"end\":34893,\"start\":34892},{\"end\":34904,\"start\":34903},{\"end\":34916,\"start\":34915},{\"end\":35109,\"start\":35108},{\"end\":35117,\"start\":35116},{\"end\":35129,\"start\":35128},{\"end\":35135,\"start\":35134},{\"end\":35337,\"start\":35336},{\"end\":35346,\"start\":35345},{\"end\":35357,\"start\":35356},{\"end\":35555,\"start\":35554},{\"end\":35567,\"start\":35566},{\"end\":35578,\"start\":35577},{\"end\":35844,\"start\":35843},{\"end\":35854,\"start\":35853}]", "bib_author_last_name": "[{\"end\":29475,\"start\":29467},{\"end\":29482,\"start\":29479},{\"end\":29688,\"start\":29683},{\"end\":29700,\"start\":29692},{\"end\":29713,\"start\":29704},{\"end\":29722,\"start\":29717},{\"end\":29731,\"start\":29726},{\"end\":29932,\"start\":29926},{\"end\":29943,\"start\":29938},{\"end\":30096,\"start\":30090},{\"end\":30105,\"start\":30100},{\"end\":30229,\"start\":30224},{\"end\":30395,\"start\":30386},{\"end\":30662,\"start\":30656},{\"end\":30672,\"start\":30666},{\"end\":30684,\"start\":30676},{\"end\":30696,\"start\":30688},{\"end\":30906,\"start\":30901},{\"end\":30914,\"start\":30910},{\"end\":30922,\"start\":30918},{\"end\":30931,\"start\":30926},{\"end\":31106,\"start\":31094},{\"end\":31122,\"start\":31110},{\"end\":31304,\"start\":31296},{\"end\":31320,\"start\":31310},{\"end\":31568,\"start\":31560},{\"end\":31577,\"start\":31572},{\"end\":31587,\"start\":31581},{\"end\":31596,\"start\":31591},{\"end\":31606,\"start\":31600},{\"end\":31838,\"start\":31833},{\"end\":31847,\"start\":31842},{\"end\":31858,\"start\":31851},{\"end\":31868,\"start\":31862},{\"end\":31876,\"start\":31872},{\"end\":31886,\"start\":31880},{\"end\":32095,\"start\":32088},{\"end\":32106,\"start\":32099},{\"end\":32268,\"start\":32266},{\"end\":32282,\"start\":32272},{\"end\":32292,\"start\":32286},{\"end\":32500,\"start\":32498},{\"end\":32514,\"start\":32504},{\"end\":32524,\"start\":32518},{\"end\":32742,\"start\":32736},{\"end\":32755,\"start\":32746},{\"end\":32904,\"start\":32900},{\"end\":32916,\"start\":32910},{\"end\":33087,\"start\":33083},{\"end\":33236,\"start\":33232},{\"end\":33416,\"start\":33409},{\"end\":33424,\"start\":33420},{\"end\":33435,\"start\":33428},{\"end\":33649,\"start\":33642},{\"end\":33662,\"start\":33655},{\"end\":33815,\"start\":33808},{\"end\":33828,\"start\":33821},{\"end\":33841,\"start\":33832},{\"end\":34018,\"start\":34011},{\"end\":34028,\"start\":34022},{\"end\":34038,\"start\":34032},{\"end\":34215,\"start\":34206},{\"end\":34224,\"start\":34219},{\"end\":34401,\"start\":34392},{\"end\":34410,\"start\":34405},{\"end\":34582,\"start\":34576},{\"end\":34592,\"start\":34586},{\"end\":34600,\"start\":34596},{\"end\":34754,\"start\":34749},{\"end\":34763,\"start\":34758},{\"end\":34882,\"start\":34878},{\"end\":34890,\"start\":34886},{\"end\":34901,\"start\":34894},{\"end\":34913,\"start\":34905},{\"end\":34922,\"start\":34917},{\"end\":35114,\"start\":35110},{\"end\":35126,\"start\":35118},{\"end\":35132,\"start\":35130},{\"end\":35141,\"start\":35136},{\"end\":35343,\"start\":35338},{\"end\":35354,\"start\":35347},{\"end\":35364,\"start\":35358},{\"end\":35564,\"start\":35556},{\"end\":35575,\"start\":35568},{\"end\":35584,\"start\":35579},{\"end\":35851,\"start\":35845},{\"end\":35859,\"start\":35855}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2857532},\"end\":29649,\"start\":29430},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":175905},\"end\":29860,\"start\":29651},{\"attributes\":{\"id\":\"b2\"},\"end\":30059,\"start\":29862},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":6561482},\"end\":30178,\"start\":30061},{\"attributes\":{\"id\":\"b4\"},\"end\":30307,\"start\":30180},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14031741},\"end\":30594,\"start\":30309},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1956774},\"end\":30863,\"start\":30596},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1350374},\"end\":31045,\"start\":30865},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2277383},\"end\":31233,\"start\":31047},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14554383},\"end\":31488,\"start\":31235},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":110745},\"end\":31764,\"start\":31490},{\"attributes\":{\"id\":\"b11\"},\"end\":32053,\"start\":31766},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":17405672},\"end\":32202,\"start\":32055},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":7138891},\"end\":32427,\"start\":32204},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":8041859},\"end\":32704,\"start\":32429},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":2619278},\"end\":32849,\"start\":32706},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":17848433},\"end\":33018,\"start\":32851},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":174065},\"end\":33187,\"start\":33020},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":2983119},\"end\":33324,\"start\":33189},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":40046466},\"end\":33594,\"start\":33326},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":702738},\"end\":33758,\"start\":33596},{\"attributes\":{\"id\":\"b21\"},\"end\":33958,\"start\":33760},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":8777811},\"end\":34163,\"start\":33960},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6891864},\"end\":34330,\"start\":34165},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2341530},\"end\":34533,\"start\":34332},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":7283643},\"end\":34712,\"start\":34535},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":2796017},\"end\":34848,\"start\":34714},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1942508},\"end\":35043,\"start\":34850},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":2749844},\"end\":35285,\"start\":35045},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":8970876},\"end\":35488,\"start\":35287},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":6087524},\"end\":35790,\"start\":35490},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":17214584},\"end\":35971,\"start\":35792}]", "bib_title": "[{\"end\":29461,\"start\":29430},{\"end\":29679,\"start\":29651},{\"end\":30086,\"start\":30061},{\"end\":30382,\"start\":30309},{\"end\":30652,\"start\":30596},{\"end\":30897,\"start\":30865},{\"end\":31090,\"start\":31047},{\"end\":31290,\"start\":31235},{\"end\":31556,\"start\":31490},{\"end\":31829,\"start\":31766},{\"end\":32084,\"start\":32055},{\"end\":32262,\"start\":32204},{\"end\":32494,\"start\":32429},{\"end\":32732,\"start\":32706},{\"end\":32896,\"start\":32851},{\"end\":33077,\"start\":33020},{\"end\":33228,\"start\":33189},{\"end\":33403,\"start\":33326},{\"end\":33638,\"start\":33596},{\"end\":34007,\"start\":33960},{\"end\":34202,\"start\":34165},{\"end\":34388,\"start\":34332},{\"end\":34572,\"start\":34535},{\"end\":34745,\"start\":34714},{\"end\":34874,\"start\":34850},{\"end\":35106,\"start\":35045},{\"end\":35334,\"start\":35287},{\"end\":35552,\"start\":35490},{\"end\":35841,\"start\":35792}]", "bib_author": "[{\"end\":29477,\"start\":29463},{\"end\":29484,\"start\":29477},{\"end\":29690,\"start\":29681},{\"end\":29702,\"start\":29690},{\"end\":29715,\"start\":29702},{\"end\":29724,\"start\":29715},{\"end\":29733,\"start\":29724},{\"end\":29934,\"start\":29922},{\"end\":29945,\"start\":29934},{\"end\":30098,\"start\":30088},{\"end\":30107,\"start\":30098},{\"end\":30231,\"start\":30222},{\"end\":30397,\"start\":30384},{\"end\":30664,\"start\":30654},{\"end\":30674,\"start\":30664},{\"end\":30686,\"start\":30674},{\"end\":30698,\"start\":30686},{\"end\":30908,\"start\":30899},{\"end\":30916,\"start\":30908},{\"end\":30924,\"start\":30916},{\"end\":30933,\"start\":30924},{\"end\":31108,\"start\":31092},{\"end\":31124,\"start\":31108},{\"end\":31306,\"start\":31292},{\"end\":31322,\"start\":31306},{\"end\":31570,\"start\":31558},{\"end\":31579,\"start\":31570},{\"end\":31589,\"start\":31579},{\"end\":31598,\"start\":31589},{\"end\":31608,\"start\":31598},{\"end\":31840,\"start\":31831},{\"end\":31849,\"start\":31840},{\"end\":31860,\"start\":31849},{\"end\":31870,\"start\":31860},{\"end\":31878,\"start\":31870},{\"end\":31888,\"start\":31878},{\"end\":32097,\"start\":32086},{\"end\":32108,\"start\":32097},{\"end\":32270,\"start\":32264},{\"end\":32284,\"start\":32270},{\"end\":32294,\"start\":32284},{\"end\":32502,\"start\":32496},{\"end\":32516,\"start\":32502},{\"end\":32526,\"start\":32516},{\"end\":32744,\"start\":32734},{\"end\":32757,\"start\":32744},{\"end\":32906,\"start\":32898},{\"end\":32918,\"start\":32906},{\"end\":33089,\"start\":33079},{\"end\":33238,\"start\":33230},{\"end\":33418,\"start\":33405},{\"end\":33426,\"start\":33418},{\"end\":33437,\"start\":33426},{\"end\":33651,\"start\":33640},{\"end\":33664,\"start\":33651},{\"end\":33817,\"start\":33806},{\"end\":33830,\"start\":33817},{\"end\":33843,\"start\":33830},{\"end\":34020,\"start\":34009},{\"end\":34030,\"start\":34020},{\"end\":34040,\"start\":34030},{\"end\":34217,\"start\":34204},{\"end\":34226,\"start\":34217},{\"end\":34403,\"start\":34390},{\"end\":34412,\"start\":34403},{\"end\":34584,\"start\":34574},{\"end\":34594,\"start\":34584},{\"end\":34602,\"start\":34594},{\"end\":34756,\"start\":34747},{\"end\":34765,\"start\":34756},{\"end\":34884,\"start\":34876},{\"end\":34892,\"start\":34884},{\"end\":34903,\"start\":34892},{\"end\":34915,\"start\":34903},{\"end\":34924,\"start\":34915},{\"end\":35116,\"start\":35108},{\"end\":35128,\"start\":35116},{\"end\":35134,\"start\":35128},{\"end\":35143,\"start\":35134},{\"end\":35345,\"start\":35336},{\"end\":35356,\"start\":35345},{\"end\":35366,\"start\":35356},{\"end\":35566,\"start\":35554},{\"end\":35577,\"start\":35566},{\"end\":35586,\"start\":35577},{\"end\":35853,\"start\":35843},{\"end\":35861,\"start\":35853}]", "bib_venue": "[{\"end\":29749,\"start\":29745},{\"end\":30949,\"start\":30945},{\"end\":31902,\"start\":31899},{\"end\":32124,\"start\":32120},{\"end\":32310,\"start\":32306},{\"end\":32773,\"start\":32769},{\"end\":33254,\"start\":33250},{\"end\":33453,\"start\":33449},{\"end\":34056,\"start\":34052},{\"end\":34242,\"start\":34238},{\"end\":34428,\"start\":34424},{\"end\":34618,\"start\":34614},{\"end\":35159,\"start\":35155},{\"end\":35382,\"start\":35378},{\"end\":35877,\"start\":35873},{\"end\":29523,\"start\":29484},{\"end\":29743,\"start\":29733},{\"end\":29920,\"start\":29862},{\"end\":30111,\"start\":30107},{\"end\":30220,\"start\":30180},{\"end\":30445,\"start\":30397},{\"end\":30719,\"start\":30698},{\"end\":30943,\"start\":30933},{\"end\":31128,\"start\":31124},{\"end\":31346,\"start\":31322},{\"end\":31612,\"start\":31608},{\"end\":31897,\"start\":31888},{\"end\":32118,\"start\":32108},{\"end\":32304,\"start\":32294},{\"end\":32557,\"start\":32526},{\"end\":32767,\"start\":32757},{\"end\":32922,\"start\":32918},{\"end\":33093,\"start\":33089},{\"end\":33248,\"start\":33238},{\"end\":33447,\"start\":33437},{\"end\":33668,\"start\":33664},{\"end\":33804,\"start\":33760},{\"end\":34050,\"start\":34040},{\"end\":34236,\"start\":34226},{\"end\":34422,\"start\":34412},{\"end\":34612,\"start\":34602},{\"end\":34769,\"start\":34765},{\"end\":34936,\"start\":34924},{\"end\":35153,\"start\":35143},{\"end\":35376,\"start\":35366},{\"end\":35625,\"start\":35586},{\"end\":35871,\"start\":35861}]"}}}, "year": 2023, "month": 12, "day": 17}