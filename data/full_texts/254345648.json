{"id": 254345648, "updated": "2022-12-20 17:11:34.742", "metadata": {"title": "SuperFusion: A Versatile Image Registration and Fusion Network with Semantic Awareness", "authors": "[{\"first\":\"Linfeng\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Yuxin\",\"last\":\"Deng\",\"middle\":[]},{\"first\":\"Yong\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Jiayi\",\"last\":\"Ma\",\"middle\":[]}]", "venue": "IEEE/CAA Journal of Automatica Sinica", "journal": "IEEE/CAA Journal of Automatica Sinica", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Image fusion aims to integrate complementary information in source images to synthesize a fused image comprehensively characterizing the imaging scene. However, existing image fusion algorithms are only applicable to strictly aligned source images and cause severe artifacts in the fusion results when input images have slight shifts or deformations. In addition, the fusion results typically only have good visual effect, but neglect the semantic requirements of high-level vision tasks. This study incorporates image registration, image fusion, and semantic requirements of high-level vision tasks into a single framework and proposes a novel image registration and fusion method, named SuperFusion. Specifically, we design a registration network to estimate bidirectional deformation fields to rectify geometric distortions of input images under the supervision of both photometric and end-point constraints. The registration and fusion are combined in a symmetric scheme, in which while mutual promotion can be achieved by optimizing the naive fusion loss, it is further enhanced by the mono-modal consistent constraint on symmetric fusion outputs. In addition, the image fusion network is equipped with the global spatial attention mechanism to achieve adaptive feature integration. Moreover, the semantic constraint based on the pre-trained segmentation model and Lovasz-Softmax loss is deployed to guide the fusion network to focus more on the semantic requirements of high-level vision tasks. Extensive experiments on image registration, image fusion, and semantic segmentation tasks demonstrate the superiority of our SuperFusion compared to the state-of-the-art alternatives. The source code and pre-trained model are publicly available at https://github.com/Linfeng-Tang/SuperFusion.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/ieeejas/TangDMHM22", "doi": "10.1109/jas.2022.106082"}}, "content": {"source": {"pdf_hash": "2c4eaa4ae17b9f34cb5de8ae1a780447b0b16ea9", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1a8557a9588a33323ffe7e1fc73a68a50be413e6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2c4eaa4ae17b9f34cb5de8ae1a780447b0b16ea9.txt", "contents": "\nSuperFusion: A Versatile Image Registration and Fusion Network with Semantic Awareness\nDECEMBER 2022\n\nIeee/Caa \nOf \nSinica \nSuperFusion: A Versatile Image Registration and Fusion Network with Semantic Awareness\n912DECEMBER 202210.1109/JAS.2022.1060822121Index Terms-Global spatial attentionimage fusionimage registrationmutual promotionsemantic awareness\nImage fusion aims to integrate complementary information in source images to synthesize a fused image comprehensively characterizing the imaging scene. However, existing image fusion algorithms are only applicable to strictly aligned source images and cause severe artifacts in the fusion results when input images have slight shifts or deformations. In addition, the fusion results typically only have good visual effect, but neglect the semantic requirements of high-level vision tasks. This study incorporates image registration, image fusion, and semantic requirements of high-level vision tasks into a single framework and proposes a novel image registration and fusion method, named SuperFusion. Specifically, we design a registration network to estimate bidirectional deformation fields to rectify geometric distortions of input images under the supervision of both photometric and end-point constraints. The registration and fusion are combined in a symmetric scheme, in which while mutual promotion can be achieved by optimizing the naive fusion loss, it is further enhanced by the mono-modal consistent constraint on symmetric fusion outputs. In addition, the image fusion network is equipped with the global spatial attention mechanism to achieve adaptive feature integration. Moreover, the semantic constraint based on the pre-trained segmentation model and Lovasz-Softmax loss is deployed to guide the fusion network to focus more on the semantic requirements of high-level vision tasks. Extensive experiments on image registration, image fusion, and semantic segmentation tasks demonstrate the superiority of our SuperFusion compared to the state-of-the-art alternatives. The source code and pre-trained model are publicly available at https://github.com/Linfeng-Tang/SuperFusion.\n\nI. INTRODUCTION\n\nA single-type device usually fails to offer a comprehensive description of the entire imaging scenario due to the limitations of the shooting environment or hardware [1]. To assist humans and machines in comprehensively perceiving the imaging scene, image fusion, an essential image enhancement technique, extracts complementary and useful information from multiple source images and then synthesizes a fused image [2], [3], [4], [5]. Among all image fusion tasks, infrared and visible image fusion has been extensively investigated and has promising prospects due to their unique characteristics. More specifically, the visible camera yields digital images with abundant texture detail via capturing the reflected light from the surface of objects. However, the visible camera fails to effectively capture significant objectives (e.g., pedestrians and vehicles) under complex circumstances such as night, occlusion, camouflage, and smoke. Fortunately, the infrared sensor generates images by collecting thermal radiation information from objects, which can defend against interference from extreme environments. Therefore, infrared images are capable of presenting significant targets with high contrast but usually fail to characterize the texture information effectively. The complementary nature of visible and infrared images encourages researchers to integrate useful information from multi-modal images into a single fused image to facilitate human perception and subsequent visual applications, such as object detection, object tracking, semantic segmentation, security surveillance, and military reconnaissance [2], [6], [7].\n\nIn the past decades, numerous infrared and visible image fusion algorithms that focus on boosting visual quality have been proposed. Typically, these fusion approaches can be divided into traditional fusion frameworks and deep learning-based frameworks. More specifically, traditional fusion frameworks involve broadly five categories, i.e., multi-scale transformation (MST)based [8], [9], [10], sparsity representation (SR)-based [11], subspace-based [12], saliency-based [13], and optimizationbased [14], [15] fusion methods. Similarly, the deep learningbased frameworks can be classified into four categories, i.e., Autoencoder (AE)-based [16], [17], [18], Convolutional Neural Network (CNN)-based [19], [20], [21], [22], Generative Adversarial Network (GAN)-based [23], [24], [25], [26], and Transformer-based [27], [28], [29] methods. In particular, with the rapid boom of deep learning techniques, deep learningbased algorithms dominate the development of image fusion.\n\nAlthough the existing infrared and visible image fusion methods have achieved satisfactory results, they seldom take into account the following two inevitable issues in the realworld scenario:\n\n1) The existing fusion algorithms, both traditional and deep learning-based ones, are sensitive to misalignment in source images. Infrared and visible images generally suffer from varying degrees of misalignment in practice due to differences in imaging principles. Once there are offsets or deformations in source images, the fusion results inevitably suffer from artifacts, Source images & GT UMF-CMGR SeAFusion SuperFusion (Ours) TarDAL Fig. 1. An illustration of infrared and visible image registration, fusion, and segmentation. The first row is the source images and fused images, and the second row is the segmentation results. The existing methods can satisfy only one of the registration or semantic requirement, which fail to facilitate high-level vision tasks in the presence of misalignment in the source images. In contrast, the proposed method can eliminate the impact of misalignment and effectively boost the performance of high-level vision tasks. as presented in Fig. 1. The latest work, i.e., UMF-CMGR [30], attempts to eliminate the effect of minor deformations from source images on the fusion results, which implements multimodel image registration by transferring visible images into the infrared domain. However, its registration performance is limited, as shown in Fig. 1, because the supervision of registration is built on the quality of the transfer. Therefore, we directly construct photometric constraints between registered images and their ground truth. Moreover, for better robustness to the noise, we further employ end-point loss to constrain the estimated deformation field in a bidirectional flow estimation scheme. In this way, image fusion can be promoted by satisfactory registration results. In turn, it is possible for the fusion to benefit the registration during joint training. Because the fusion loss, which always requires the general similarity between the fusion results and the input images, especially in salient structure, implicitly forces the inputs to be similar, i.e., aligned. To further enhance the positive feedback of fusion, we propose a symmetric joint registration and fusion scheme, in which, two fused images are expected to be the same, if the registration is well finished. Consequently, we simply employ mono-modal similarity measurement to constrain the symmetric fusion outputs to fulfill the mutual promotion between registration and fusion.\n\n2) The existing fusion approaches barely consider how to facilitate high-level vision tasks. Most fusion approaches focus only on the visual quality of fused images while ignoring the contribution of fusion results to high-level vision tasks, e.g., semantic segmentation, and object detection. Some researchers have noticed this drawback and proposed practical solutions, such as SeAFusion [22] and TarDAL [25]. However, as a preliminary exploration, the aforementioned schemes only design simple loss functions to guide the training of fusion networks, which reserves space for further improvements. From Fig. 1, it can be observed that the segmentation model fails to yield accurate segmentation results from the fused images generated by SeAFusion and TarDAL. In particular, while focusing on increasing the contrast of fused objects to facilitate automatic machine detection, TarDAL neglects the whole visual quality of fused images. To address the limitations of the prior works, we design a more elaborated semantic constraint to enforce the fusion network to preserve semantic information as much as possible during the fusion process to facilitate subsequent high-level vision tasks.\n\nFurthermore, we introduce a global spatial attention module (GSAM) to improve the perceptual effect of fusion results. GSAM can adaptively estimate appropriate weights for infrared and visible features by perceiving global structural information from four directions. It is worth emphasizing that we take registration, fusion, and semantic requirements into a single framework to meet the need of practical fusion. As shown in Fig. 1, the proposed method is capable of obtaining satisfactory fusion results and segmentation results even if there is a severe misalignment in source images.\n\nTo be concrete, this work has the following contributions:\n\n\u2022 We model image registration, image fusion, and highlevel semantic requirements uniformly into a single framework. To the best of our knowledge, this is the first practical image fusion method that adequately considers the prerequisites of image fusion (i.e., image registration) and the subsequent applications of image fusion. \u2022 We design a symmetric bi-directional image registration module to effectively perform multi-modal image alignment. Specifically, the property of symmetry allows our approach to achieve the mutual promotion of image fusion and image registration. \u2022 A semantic constraint based on semantic segmentation is introduced to prompt the fusion network to respond to the demands of high-level vision tasks. Moreover, a global spatial attention module is embedded into the fusion network to achieve adaptive feature integration. \u2022 Extensive experiments demonstrate the superiority of our method compared to state-of-the-art alternatives. In particular, our approach can accomplish unaligned image fusion while facilitating performance improvement for high-level vision tasks. The rest of the paper is organized as follows. Section II summarizes some relevant research to the proposed method.\n\nSection III provides a detailed discussion of our SuperFusion. In Section IV, we present some qualitative and quantitative results on the image registration, image fusion, and semantic segmentation tasks, as well as perform the ablation study to verify the effectiveness of specific designs. Some concluding remarks are presented in Section V.\n\nII. RELATED WORK Image fusion, cross-modal image registration, and recurrent neural network are three of the most relevant techniques to our work. We review some representative researches to introduce their developments in this section.\n\n\nA. Typical Infrared and Visible Image Fusion\n\nIn recent years, image fusion, especially typical image fusion aimed at improving image perception, continues to attract increasing attention. Typical image fusion methods involve the following four categories.\n\n1) AE-based Image Fusion Methods: Deep learning dominates the image fusion field owing to its robust feature extraction and information integration capabilities. The autoencoder (AE)-based image fusion scheme is a vital branch of applying deep learning to image fusion, which employs the encoder and decoder to implement two key processes in image fusion, i.e., feature extraction and image reconstruction. Then, feature fusion is usually accomplished using hand-crafted fusion strategies, such as element-wise average, element-wise addition, and element-wise weighting. DenseFuse [16] is the pioneer of AE-based fusion approaches, which introduces dense blocks in the encoder to achieve effective feature extraction and information utilization. Subsequently, Li et al. further introduced nest connection and attention mechanism to improve the fusion performance [31], [32]. In addition, Xu et al. [33], [34] and Zhao et al. [35] promoted the interpretability of AEbased fusion methods via devising multiple encoders to extract specific features. It is worth noting that the hand-crafted fusion strategies hinder the further improvement of AE-based image fusion approaches. For this purpose, Xu et al. designed a classification saliency-based fusion rule to selectively integrate the features extracted by the encoder from infrared and visible images [17].\n\n2) CNN-based Image Fusion Methods: As another branch of deep learning-based methods, the convolutional neural network (CNN)-based approach relies on elaborate network structures and loss functions to implement feature extraction, fusion, and image reconstruction in an end-to-end manner. Zhao et al. devised a multi-realm image fusion method using CNNbased realm-specific and realm-general feature representation learning [36]. Incorporating the characteristics of infrared and visible image fusion, Tang et al. designed an illumination-aware loss to guide the fusion model to implement round-the-clock image fusion according to the lighting conditions [37]. The focus of the above methods is on the design of the learning paradigms or loss functions. In terms of the network structure, RXDNFuse develops an aggregated residual dense network to achieve more effective feature extraction and information preservation [38]. Moreover, Liu et al. proposed a flexible fusion method that can search effective architecture dynamically according to various modality principles and fusion mechanisms to avoid target blurring and texture detail loss [39]. It is instructive to note that different image fusion tasks usually have certain commonalities and complementarities, which enables the possibility of solving multiple image fusion tasks in a unified framework. Zhang et al. defined the image fusion problem as proportional maintenance of gradient and intensity. They first manually assigned appropriate weights for sub-losses according to the characteristics of different fusion missions [40], and further developed an adaptive decision block to dynamically assign weights for sub-loss terms to cope with the complex scenario [41]. In addition, Xu et al. developed a unified model for multi-fusion missions, which can achieve crossfertilization between different fusion tasks [42], [43]. Similarly, researchers further developed a great number of general image fusion models (e.g., IFCNN [44], SGFusion [45]) to fully take advantage of their convenient potential.\n\n3) Transformer-based Image Fusion Methods: It is worth mentioning that the receptive field of the convolutional neural network is limited, which cannot effectively exploit the global context in the source images to assist information aggregation. Therefore, researchers attempted to leverage Transformer to model long-range dependencies to sufficiently merge complementary information from input images. Fu et al. first attempted to leverage a pure Transformer architecture for the image fusion tasks [46]. They developed a Patch Pyramid Transformer (PPT) to capture local correlation information of neighboring pixels and non-local information from the entire image. Considering the advantages of CNNs in modeling local dependencies, most Transformer-based fusion methods (e.g., CGTF [29], SwinFusion [27], and YDTR [28]) first utilize CNN to extract shallow features and then employ Transformer to mine global interactions to facilitate adequate information integration. In addition, some works such as IFT [47] and DNDT [48] leverage the intrinsic of Transformer, i.e., the selfattention and cross-attention mechanism, to design novel fusion rules, thus taking advantage of the global interaction during the feature fusion phase.\n\n\n4) GAN-based Image Fusion Methods:\n\nConsidering the lack of ground truth as a reference for infrared and visible image fusion, researchers also introduced generative adversarial mechanisms to impose stronger constraints for fusion networks from the perspective of probability distributions. FusionGAN first models the image fusion task as the adversarial game between a generator and a discriminator, where the discriminator forces the generator to retain more texture details from visible images [23], [49]. However, a single discriminator is prone to modal imbalance. Thus, subsequent works, such as DDcGAN [50], AttentionFGAN [51], SDDGAN [52], and GAN-FM [53] design dual discriminators to maintain modal balance.\n\n\nB. Practice-oriented Image Fusion\n\nAlthough the above typical approaches can generate satisfactory fusion results, they ignore some real-world challenges, such as unaligned image fusion and the requirements of high-level vision tasks. Fortunately, some researches provide preliminary explorations for the aforementioned challenges. Wang et al. are the first to realize that slight misalignment in source images may introduce severe ghosts in fusion results [30]. Thus, they proposed a cross-modality perceptual style transfer module to transfer visible images into the infrared domain. Then, a multi-level refinement registration is deployed to align infrared images to pseudo-infrared images. Finally, aligned infrared images and visible images are fed to the dual-path interaction fusion module to complete the complementary information integration. Their method (i.e., UMF-CMGR) can mitigate the effects of minor deformations in input images. However, the fusion results synthesized by UMF-CMGR still suffer from artifacts when the unaligned degree of source images is obvious. In particular, UMF-CMGR only considers unaligned image fusion but ignores the semantic requirements of subsequent applications.\n\nOf course, there are some works (e.g., SeAFusion [22] and TarDAL [25]) that take into account the demands of high-level vision tasks and design semantic constraints to guide the fusion network to inject more semantic information into the fused images. More specifically, they interface a high-level model following the fusion network to measure semantic information contained in fusion results, and losses of the high-level model are utilized to guide the training of the fusion network through gradient back-propagation. SeAFusion and TarDAL perform semantic measurements using the semantic segmentation model and the target detection model, respectively. In addition, Liu et al. injected the semanticdriven idea into the GAN-based framework and employed a segmentation network as the discriminator to synthesize more meaningful fusion results from the perspective of the segmentation task [54]. They also adopted image fusion as an auxiliary task to assist the semantic segmentation task, where image fusion is deployed as an additional regularization for feature learning [55]. The above semantic-driven approaches can improve the performance of high-level tasks on fusion results yet fail to implement unaligned image fusion. Therefore, these methods still have limitations in practical applications. For this purpose, we adequately consider the prerequisites and application requirements of image fusion in this work and incorporate image registration, image fusion, and semantic requirements of high-level vision tasks into a single framework.\n\n\nC. Cross Modal Image Registration\n\nCompared to mono-modal image registration, cross-modal image registration algorithms, including both traditional and deep learning ones, develop slowly due to the severe appearance variance in different modalities [56]. Old fashions began with presupposing a transform model, e.g., affine transformation [57], and free-form deformation [58], and then obtaining the model parameters by minimizing metrics that measure the similarity between the target image and the moved source image, e.g., normalized correlation coefficient (NCC) [59] and mutual information (MI) [60]. However, those traditional dense matching methods are sensitive to noise. To tackle this problem, some cross-modal sparse features, including MFP [61], DASC [62], and RIFT [63] focus on extracting interest points and encoding the local information into features, which significantly improve the robustness of matching. Nevertheless, those sparse features could not handle the free-form deformation. Therefore, dense matching methods, i.e., flow estimation, still attract many researchers.\n\nRecently, VoxelMorph [64] introduces the deep neural network to estimate the cross-modal flow with no supervision, which only employs NCC as the loss function. Nemar [65] proposes an unsupervised joint translation and registration framework, in which the supervision is constructed on the transfer domain. CrossRAFT [66] generalizes the single modal flow estimator into cross-modal tasks with data augmentation and knowledge distillation. However, none of those unsupervised methods really face the open problem in cross-modal image registration-how to build the supervision on the severe appearance variance in different modalities. By contrast, those supervised methods can address the problem in an end-to-end manner. Wang et al. [67] developed the traditional matching method with neural networks in a supervised manner. The registration module of UMF-CMGR [30] is only supervised by similarity constraints between registered infrared images and transferred images, which is sensitive to noise. To tackle those problems, we introduce both supervised flow and photometric constraints for robust registration training.\n\n\nD. Recurrent Neural Network in Computer Vision\n\nRecurrent Neural Networks (RNN) that specialize in modeling long-range dependencies and are easy to train, have made a splash in the field of natural language processing [68]. When it is employed in the field of computer vision, two sequential RNN layers can guarantee that information can be efficiently propagated across the whole feature maps, thus fully integrating the global contexts. Bell et al. [69] firstly designed a two-round four-directional RNN architecture to exploit global context information to improve the performance of small object detection. Given the feature h i,j at pixel (i, j), one round of data translations in four-directional RNN can be formulated as:\nh i,j = max(\u03b1 dir h i,j\u22121 + h i,j , 0),(1)\nwhere \u03b1 dir indicates the weight parameter in the recurrent translation layer for each direction. Specifically, all weights are first initialized as an identity matrix, and then adaptively updated during the training process. Moreover, the procedure of two-round four-directional RRN can be summarized in Fig. 2 Fig. 2. Illustration of how two-round four-directional RNN architecture integrates global contextual information in two stages. In the first round, four-directional recurrent convolution are employed to collect horizontal and vertical neighborhood information for each position in the input feature maps.\n\nIn the second round, the contextual information from the entire feature maps is gathered by repeating the previous operations.\n\ninternal structure of source images in an explicit manner. In this work, four-directional RNN is incorporated into the global spatial attention module to adaptively allocate fusion weights for infrared and visible features.\n\n\nIII. METHODOLOGY\n\n\nA. Overall Framework\n\nGiven a strictly registered pair of visible image I vi \u2208 R H\u00d7W \u00d73 and infrared image I ir \u2208 R H\u00d7W \u00d71 , image fusion aims to integrate complementary information of both into a single fused image I f \u2208 R H\u00d7W \u00d73 . However, it is impossible to directly capture aligned images for fusion, because of the different extrinsic and intrinsic parameters of cameras. Additionally, the cross-modality binocular photograph may suffer from shakes, latency, and special noise. Especially, the infrared camera may be severely interfered by internal temperature and external hot airflow. Thereby, it is urgent to consider the misalignment between input infrared and visible images in real-world fusion. Taking the visible image I vi and moved infrared image I ir as inputs, our method first yields the registered infrared image I reg ir with a registration network N R , as illustrated in Fig. 3. More specifically, a novel dense mathcer (DM ) is devised to estimate the infraredto-visible deformation field \u03c6 ir \u2192vi \u2208 R H\u00d7W \u00d72 , which is formulated as:\n\u03c6 ir \u2192vi = DM (I vi , I ir ).(2)\nIn particular, each element \u03c6 ir \u2192vi [i, j] = (\u2206x, \u2206y) \u2208 R 2 represents the deformation offset for the pixel of the unaligned infrared image. The registered infrared image can be achieved by re-sampling the unaligned infrared image with the estimated deformation field, which can be expressed as:\nI reg ir = R(I ir , \u03c6 ir \u2192vi ),(3)\nwhere R indicates the re-sampler. The resampling process can be briefly described as:\nI reg ir [i, j] = I ir [i + \u03c6 ir \u2192vi (i, j, 1), j + \u03c6 ir \u2192vi (i, j, 2)]. (4)\nAfter that, the registered infrared image I reg ir and visible image I vi are fed into the fusion network N F to synthesize the fused image I 1 f as:\nI 1 f = N F (I vi , I reg ir ).(5)\nIt is instructive to note that we design a global spatial attention module (GSAM) in the fusion network to implement adaptive feature fusion. GSAM leverages two-round four-directional RNN that fully exploits the global context in features and assigns appropriate fusion weights for features, thus integrating more meaningful information into the fusion results. Finally, the fusion result is fed into the segmentation network N S , whose output is the predicted class probability y 1 seg , to implement the semantic information measurement, which is formulated as:\ny 1 seg = N S (I 1 f ).(6)\nIt is worth noting that infrared and visible image registration is a challenging mission due to the vast modal variance. Fortunately, two symmetric fused images could eliminate the modal variance and thus provide appropriate pixel-level supervision for multi-modal image registration. In particular, the fused image is a public domain of infrared and visible images, which not only eliminates modal variance but also contains more information. Therefore, we further develop a symmetric image registration and fusion framework to obtain symmetric fused images, as presented in Fig. 3. More specifically, the infrared image I ir and moved visible image I vi are used as the inputs for the other branch. The specific registration and fusion processes are shown in (7) and (8):\n\u03c6 vi \u2192ir = DM (I ir , I vi ), I reg vi = R(I vi , \u03c6 vi \u2192ir ),(7)I 2 f = N F (I reg vi , I ir ).(8)\nMoreover, the fusion result I 2 f is also fed into the segmentation network to perform the semantic measurement, which is presented as:\ny 2 seg = N S (I 2 f ).(9)\nIt is notable that the color visible image is first converted into the YCbCr space from the RGB space. Then, the Y (luminance) channel of the visible image and the grayscale infrared image are fed into the fusion model. The output of the fusion network is the Y channel of the fused image, which is mapped back to the RGB space along with Cb and Cr (chrominance) channels of the visible images to obtain the color fused image.\n\n\nB. Loss Function\n\nOur SuperFusion incorporates image registration, image fusion, and semantic requirements into a unified framework. In order to implement each component more effectively, we elaborate losses to guide the training of related networks.\n\n1) Loss Function of Image Registration: Photometric error and end-point error, which are the most common losses for deformation field estimation, both can benefit the performance. While the end-point loss forces the estimator to regress the flow in the smooth areas, the photometric loss improves the accuracy of both quantitative and qualitative results in the textured areas. In the case of cross-modality image registration, there are always severe appearance variances requesting flow supervision, for example, scattered lights in visible images, lightened people in infrared images, and so on. However, discriminative information provided by these peculiar areas  Fig. 3. The overall framework of the proposed SuperFusion for cross-modal image registration and fusion. I ir and I vi indicate the moved infrared and visible image, respectively; \u03c6 ir \u2192vi and \u03c6 vi \u2192ir denote the deformation field from the moved infrared to the fixed visible image and the deformation field from the moved visible image to the fixed infrared image, respectively; R indicates the re-sample operation; I reg ir and I reg ir represent the registered infrared and visible image, respectively; y 1 seg and y 2 seg mean the segmentation results.\n\nconfuses the flow estimator and makes the loss larger, so the flow estimator is enforced to focus on those areas and neglect the common areas that are rich in texture. Therefore, for keeping the precision of textured areas, it is also necessary to supervise the flow learning with photometric constraints. Specifically, given a pair of aligned infrared and visible images I ir , I vi , we conduct a synthetic transform \u03c6 gt on I ir to generate a misaligned infrared image I ir . To register I ir to I vi , we estimate the flow \u03c6 ir\u2192vi with the dense matcher and conduct it onto I ir to generate the registered infrared image I reg ir as shown in (2) and (3). Then, the photometric loss is constructed between I reg ir and its ground truth I ir as:\nL P H (I reg ir , I ir ) = 1 HW H,W i,j I reg ir (i, j) \u2212 I ir (i, j) 1 ,(10)\nwhere \u00b7 1 denotes l 1 -norm.\n\nTo construct the end-point loss, we have to invert \u03c6 gt . However, obtaining the inverse flow requires a complex resampling strategy [72]. Moreover, the resampling strategy cannot handle all situations and might introduce noise according to Nyquist Sampling Theorem. Therefore, instead of computing the inverse flow of ground truth, we believe a dense matcher should be able to estimate the bidirectional flows if the input features are adaptive to modalities and force the dense matcher to estimate the inverse flow \u03c6 vi\u2192ir . Then the end-point loss can be constructed between \u03c6 vi\u2192ir and its ground truth \u03c6 gt as:\nL EP (\u03c6 vi\u2192ir , \u03c6 gt ) = 1 HW H,W i,j \u03c6 vi\u2192ir (i, j) \u2212 \u03c6 gt (i, j) 2 ,(11)\nwhere \u00b7 2 denotes l 2 -norm. Moreover, to better constrain \u03c6 vi\u2192ir , we warp I ir with \u03c6 vi\u2192ir to generate I ir , and then construct a photometric loss L P H (I ir , I ir ) as (10).\n\nAfter registration, the registered image pair I reg ir and I vi would be fed into the fusion network. However, if I reg ir is not registered well, the fusion loss (detailed in the subsequent subsection) that is constructed between I reg ir and I f would be hard to optimize and the fusion module would prefer to minimize the loss between I vi and I f . Consequently, the loss would converge into an unpredictable situation and mutual promotion would not be achieved. That is one reason why we propose a symmetrical architecture as shown in Fig. 3, which yields symmetrical outputs I reg vi , \u03c6 ir\u2192vi , I vi and losses L P H (I reg vi , I vi ), L EP (\u03c6 ir\u2192vi , \u03c6 gt ), L P H (I vi , I vi ) as mentioned above. In this symmetrical scheme, the optimization of registration and fusion would be balanced.\n\nTo sum up, the final photometric loss and end-point loss are formulated in (12) and (13) \nL EP = L EP (\u03c6 vi\u2192ir , \u03c6 gt ) + L EP (\u03c6 ir\u2192vi , \u03c6 gt ).(12)\nFurthermore, our symmetric branches output two fusion results I 1 f , I 2 f , which are considered to be the same if the registration is well finished. Therefore, we develop a consistency constraint loss L CC as:\nL CC = 1 HW H,W i,j I 1 f (i, j) \u2212 I 2 f (i, j) 1 .(14)\nSuch a constraint is believed to make the fusion and registration promoted mutually. That is another reason why the symmetric scheme is designed. Finally, the full objective function of the image registration network is a weighted sum of the photometric loss, end-point loss, and consistency constraint loss, which is defined as:\nL Reg = L P H + \u03b1 1 \u00b7 L EP + \u03b1 2 \u00b7 L CC ,(15)\nwhere \u03b1 1 and \u03b1 2 are hyper-parameters to trade off each component of registration loss.\n\n\n2) Loss Function of Image Fusion:\n\nThe fusion model is expected to preserve the structures and abundant texture details of source images. Thus, we design the SSIM loss L SSIM and texture loss L T ext to guide the fusion network N F to achieve the above purposes. The SSIM loss is presented as:\nL SSIM = 4 \u2212 (SSIM (I 1 f , I vi ) + SSIM (I 1 f , I reg ir )) \u2212 (SSIM (I 2 f , I reg vi ) + SSIM (I 2 f , I ir )),(16)\nwhere SSIM (\u00b7, \u00b7) indicates the structural similarity measurement, which could measure image distortion from three perspectives, i.e., light, contrast, and structure [73]. The texture details of an image can be characterized by its gradient. Therefore, we calculate the error between the gradient of the fused image and the maximum gradient aggregation of the source image to construct the texture loss as:\nL T ext = 1 HW |\u2207I 1 f | \u2212 max(|\u2207I vi |, |\u2207I reg ir |) 1 + 1 HW |\u2207I 2 f | \u2212 max(|\u2207I reg vi |, |\u2207I ir |) 1 ,(17)\nwhere \u2207 denotes the Sobel gradient operator, which could measure the gradient of an image; |\u00b7| refers to the absolute operation, and max(\u00b7) indicates the element-wise maximum aggregation operator. It is worth noting that the fused image is also expected to integrate intensity information in the source images, especially the significant targets in the infrared image. Therefore, we devise an intensity maximization loss L Int to guide the fusion network to adaptively integrate intensity information of source images:\nL Int = 1 HW I 1 f \u2212 max(I vi , I reg ir ) 1 + 1 HW I 2 f \u2212 max(I reg vi , I ir ) 1 .(18)\nThe final fusion loss L F us employed to guide the training of our fusion model can be summarized as the weighted sum of the above three sub-losses, which is formulated as:\nL F us = L T ext + \u03b2 1 \u00b7 L SSIM + \u03b2 2 \u00b7 L Int ,(19)\nwhere \u03b2 1 and \u03b2 2 are employed to control the trade-off between different losses. It is worth noting that in the loss functions of the fusion network, I vi and I f specifically refer to the Y channel of the visible images and fused images, respectively, and I ir denotes the grayscale infrared images.\n\n3) Semantic-aware Loss Function: We introduce a semantic loss like SeAFusion [22] to prompt the fusion network to adequately consider the requirements of high-level vision tasks. However, SeAFusion employs only the simplest cross-entropy loss to model the semantic requirements, which potentially neglects the category imbalance issue. Thus, we introduce the Lovasz-Softmax loss [74] to calculate the error between the predicted results and ground truth.\n\nLet y \u2208 R H\u00d7W \u00d7C denote the network output probability,\u0177 \u2208 R H\u00d7W \u00d7C denote one-hot predicted label, and y * \u2208 R H\u00d7W \u00d7C denote the corresponding ground truth, where C represents the number of classes. Intersection over union (IoU) is the primary metric to measure the performance of segmentation. The IoU loss for the c-th class between the predicted label\u0177 c \u2208 {0, 1} H\u00d7W and the ground truth y * c \u2208 {0, 1} H\u00d7W , can be formulated as:\n\u2206(\u0177 c , y * c ) = 1 \u2212 |y * c \u2229\u0177 c | |y * c \u222a\u0177 c | .(20)\nIt is promising to directly minimize the IoU loss to achieve better performance, but IoU is not a differential function with respect to the network output probability of c-the class y c \u2208 (0, 1) H\u00d7W . Therefore, Lovasz-Softmax loss, the differential alternative of IoU loss, is proposed in [74]. To be specific, an error function err(y c ) of c-th class prediction should be formulated firstly as:\nerr(y c , y * c , i) = 1 \u2212 y c (i), if\u0177 c (i) \u00b7 y * c (i) == 1, y c (i), otherwise,(21)\nwhere i is the index of a pixel. Then the Lovasz-Softmax loss can be described as:\nL LS (y, y * ) = 1 |C| c\u2208C \u2206err(y c , y * c ),(22)\nwhere C is the set of classes. Specifically, the definition of \u2206err(y c , y * c ) is provided as follows:\n\u2206err(y c , y * c ) = i\u2208H\u00d7W err(y c , y * c , i) \u00b7 G(y c , y * c , i),(23)\nwhere G(y c , y * c , i) = \u2206(S c (i), y * )\u2212\u2206(S c (i\u22121), y * ) and S c (i) is the set of segmented pixels of the sorted y c . Specifically, y c is sorted by y c (0) \u2265 y c (1) \u2265 \u00b7 \u00b7 \u00b7 \u2265 y c (i) \u2265 \u00b7 \u00b7 \u00b7 \u2265= y c (H \u00d7 W ), then S c (i) = {\u0177 c (0),\u0177 c (1), \u00b7 \u00b7 \u00b7 ,\u0177 c (i)}. More details about the implementation of the Lovasz-Softmax loss can be found in the original paper [74]. Therefore, the semantic-aware loss L Sea utilized in our work is formulated as:\nL Sea = L LS (y 1 seg , y * ) + L LS (y 2 seg , y * ).(24)\nFinally, the full objective function of our SuperFusion is defined as:  where \u03bb 1 , \u03bb 2 , and \u03bb 3 are the hyper-parameters for balancing each component.\nL T otal = \u03bb 1 \u00b7 L Reg + \u03bb 2 \u00b7 L F us + \u03bb 3 \u00b7 L Sea ,(25)\n\nDense Matcher\n\nC. Network Architecture 1) Architecture of Dense Matcher: Dense matcher consisting of a pyramid feature extractor and iterative flow estimators is the key module of the registration network. Firstly, weightunsharing networks, which contain 4 convolutional layers with output channels [8,8,16,16], instance normalization, and Leaky Rectified Linear Unit (Leaky-ReLU), extract full-scale modal-adaptive features F 0 \u2208 R H\u00d7W \u00d716 for input images. Then, F 0 successively passes the three weight-sharing submodules with downsampling to extract F 1 \u2208 R H/2\u00d7W/2\u00d732 , F 2 \u2208 R H/4\u00d7W/4\u00d764 , and F 3 \u2208 R H/8\u00d7W/8\u00d7128 . Each submodule consists of 3 convolutional layers. Outputs of layers are normalized by instance normalization and activated by Leaky-ReLU, except the last one. And the downsampling is implemented by a stride of 2 in the second layer. Subsequently, features are collected and fed into the flow estimators.\n\nAssume the moved infrared image I ir and the visible image I vi are the source and target images, respectively. In the i-th flow estimator, we firstly warp the source feature F i ir with the last estimated deformation field, and label it with Z i ir . Note that, as shown in Fig. 4, the initial flow imported into the 3-rd flow estimator is 0. Then we can compute the local correlation volume Corr \u2208 R H/2 i \u00d7W/2 i \u00d77\u00d77 of F i ir and F i vi as:\nCorr i ir \u2192vi (j, k, m, n) = F i vi (j, k) T Z i ir (j + m, k + n),(26)\nwhere j, k are the indexes of all pixels at i-th scale, and m, n \u2208 {\u221212, \u22128, \u22124, 0, 4, 8, 12} are the shifts. Next, Corr is reshaped into a feature-like shape and concatenated with Z i ir . The following 4 convolutional layers would estimate the residual deformations \u2206\u03c6 i ir \u2192vi (Corr i ir \u2192vi ,Z i ir ) with the concatenation as the input. Each convolutional layer is followed by batch normalization and LeakReLU, except for the last one. The numbers of output channels of the first three layers are the floor division of the number of input channels. Finally, the two-channel deformation field at the i-th scale is represented as:\n\u03c6 i ir \u2192vi = (\u03c6 i+1 ir \u2192vi + \u2206\u03c6 i ir \u2192vi ) \u2191 2,(27)\nwhere \u2191 2 denotes the bilinear upsampling by a factor of two. Note that the first several layers of the pyramid feature extractor are weight-unsharing so that the modal variance can be eliminated in F 0 . Moreover, it is easy to obtain the inverse flow by inverting the order of inputs of flow estimator layers. The bidirectional estimation can force the F 0 to be free of modality-specific information.\n\n2) Architecture of Image Fusion Network: The architecture of our fusion network N F is presented in Fig. 5. The siamese feature extraction modules M ir E and M vi E are firstly deployed to extract infrared features F reg ir and visible features F vi from source images, respectively:\n{F reg ir , F vi } = {M ir E (I reg ir ), M vi E (I vi )}.(28)\nSpecifically, M ir E and M vi E consist of four convolutional layers followed by the Leaky-ReLU activation function. The kernel size of all convolutional layers is 3 \u00d7 3, and the stride is 1.\n\nAfter that, in order to achieve adaptive feature integration, infrared and visible features are cascaded in the channel dimension and then fed into the global spatial attention module (GSAM) to obtain the attention map A reg ir (i.e., fusion weight) of infrared features. This process can be formulated as:\nA reg ir = GSAM (concat(F reg ir , F vi )),(29)\nwhere concat(\u00b7) denotes the concatenation operation in the channel dimension. Moreover, the schematic illustration of GSAM is shown in Fig. 6. On the one hand, the concatenated features are first to cross three sequential convolutional layers to obtain the shared attention weights for four-directional context features. The first two convolutional layers are followed by the Leaky-ReLU activation function, and the third convolutional layer leverages the Sigmoid function to normalize the attention weights. On the other hand, the concatenated features are utilized as inputs to four-directional RNN after passing through a convolutional layer. Then, the contextual features yielded by RNN are multiplied with the attention weights and concatenated in the channel dimensions. The refined features are fed into a convolutional layer to reduce the channel dimension. Next, the second round of four-directional RNN is deployed to extract the contextual features with global perception, which is multiplied by the shared attention weights. Finally, two cascaded convolutional layers are employed to calculate the attention maps (i.e., fusion weights) for infrared features from the refined global features. The first and second layers use Leaky-ReLU and Sigmoid as the activation functions, respectively. The kernel size of all convolutional layers in GSAM is 3 \u00d7 3. It is also instructive to note that the fusion weights for infrared features are obtained by considering the importance of infrared and visible features jointly.\n\nConsidering that infrared and visible features are complementary, we define the fusion weight of visible features as 1 \u2212 A reg ir . Then, the features can be adaptively integrated as:\nF f = A reg ir F reg ir + (1 \u2212 A reg ir ) F vi ,(30)\nwhere stands for the element-wise multiplication operation.   Finally, the fused features F f are fed into the image reconstruction module M R to yield the fused image:\n\n\nF o u r-d ir ec ti o n a l re cu rr en t tr a n sl a ti o n\n\n\nF o u r-d ir ec ti o n a l re cu rr en t tr a n sl a ti o n\nI 1 f = M R (F f ).(31)\nThe image reconstruction module M R consists of four sequential convolutional layers, where the kernel size of each layer is 3 \u00d7 3, and the stride is set to 1. All layers are followed by the Leaky-ReLU activation function, except for the last layer, which employs Tanh as the activation function.\n\n\nIV. EXPERIMENTS RESULTS AND DISCUSSIONS\n\nIn this section, we compare SuperFusion with several state-ofthe-art algorithms on both the image registration, image fusion, and semantic segmentation tasks by quantitative and qualitative comparisons. Firstly, we provide some implementation details and experimental configurations. Subsequently, we present some qualitative and quantitative results compared to stateof-the-art alternatives. Finally, we also conduct ablation studies to demonstrate the effectiveness of the specific designs. 2) Experimental Configurations: Registration performance of our SuperFusion is compared with five state-of-the-art methods including DASC [62], RIFT [63], GLU-Net [76], UMF-CMGR [30], and CrossRAFT [66] on the MSRS [37] and RoadScene [43] datasets. The comparison is performed in two protocols. While the first is conducting synthetic random affine and elastic transform on infrared images, the second is on visible images. The rigid transform consists of a rotation of [\u221210, 10] degrees and a translation of [\u221210, 10] pixels. And, the elastic transform is obtained by blurring a two-channel [\u22121, 1] noise map with six Gaussian filters whose sigma is 15 and the kernel size is 45 \u00d7 45. All methods would estimate the transforms from the infrared image to the visible one in the two protocols. And the registration performance would be measured by re-projection error and end-point error.\n\nFor image fusion, we also select five state-of-the-art methods for comparison, including RFN-Nest [32], SeAFusion [22], UMF-CMGR [30], U2Fusion [43], and TarDAL [25]. Six metrics involving mutual information (MI), visual information fidelity (VIF), structural similarity index measure (SSIM), feature mutual information based on discrete cosine transform (F M I dct ), Q abf , and N abf , are selected to quantitatively evaluate fusion performance. MI, F M I dct , and Q abf measure  the amount of pixel information, feature information, and edge information transferred from source images to the fusion results, respectively. VIF assesses the information fidelity of fusion results from the perspective of human visual perception. SSIM reflects the similarity between the fused images and the source images from various perspectives, i.e., brightness, contrast, and structure. N abf reflects the artifacts introduced into the fusion results during the fusion process. A higher value of the metrics mentioned above indicates better performance, except for N abf . It is worth noting that UMF-CMGR [30] and our method utilize their own registration module to register the source images, while other comparison methods deploy the state-of-the-art multi-modal registration algorithms, i.e., CrossRAFT [66], to pre-process the misaligned source images. All comparison algorithms are publicly available, and all parameters are consistent with the original papers.\n\nB. Results on Multi-modal Image Registration 1) Quantitative Comparison: Registration performance is quantified by reprojection error (RE), which measures the distance between a landmark reprojected by the estimated deformation and its ground truth, and end-point error (EPE) which is the root-mean-square error of the deformation field. Note that, the EPE in Protocol 1 is hard to compute because it requires the inversion of synthetic transforms. As shown in Table I, the individually trained registration network of Super-Fusion (without fusion) performs well, which demonstrates the superiority of our loss formulation and network design for registration. And our full SuperFuion achieves the best scores on all protocols and metrics, which proves the fusion and L cc can improve the registration in our framework. Although CrossRAFT that is based on complex data augmentations shows generalization on infrared and visible images as claimed in their paper [66], it still cannot handle challenging scenes in the MSRS dataset, which results in relatively large margins in performance to our methods. The relative weak results of UMF-CMGR [30], which also focuses on a joint registration and fusion paradigm, can be boiled down to the noisy translation as mentioned before. Moreover, RIFT and DASC are sparse features so they might not adapt to the elastic deformations in our experiments. GLU-Net, the state-of-the-art method for mono-modal image registration, cannot exhibit the generalization on cross-modal tasks, which demonstrates that cross-modal-specific studies deserve more attention.\n\n2) Qualitative Comparison: The qualitative registration performance shown in Fig. 7 is generally consistent with the quantitative one. Our SuperFusion has the subtlest visible error between registered gradients and their ground truth in all scenes, especially in the highlighted regions. By contrast, the chaotic deformation fields estimated by DASC lead to weird qualitative results. Although CrossRAFT seems competitive with our methods in quantitative comparison, it still shows obvious flaws in the MSRS dataset, even in some salient regions, for example, well-bounded humans and objects, which probably reveals the limitation of data augmentation. It is worth mentioning that RIFT obtains overall decent results with the post homography matrix estimation, which are even better than CrossRAFT, except for in Row 3. It is those extreme failed registered examples as shown in Row 3 that significantly degenerate the quantitative indexes of RIFT. Additionally, the disappointing qualitative performance of GLU-Net and UMF-CMGR meet the corresponding quantitative indexes. The disappointing performance of UMF-CMFR would impose indicatively negative effects on the subsequent fusion experiments.\n\nC. Results on Infrared and Visible Image Fusion 1) Qualitative Comparison: The fusion results of different fusion algorithms on the MSRS and RoadScene datasets are presented in Fig. 8. From the results, we can find that even the state-of-the-art registration algorithm fails to achieve strict registration of the misaligned source images, such that severe artifacts (e.g., pedestrians, shrubs, and bicycles) are presented in the fusion results. In addition, UMF-CMGR fails in dealing with severe deformation or parallax, although it jointly models the image registration and image fusion problems. RFN-Nest not only weakens the salient targets from infrared images but also introduces thermal radiation interference in the background region (e.g., the red boxes in the first and third rows). Similar phenomena also occur in UMF-CMGR and U2Fusion. The wall in the second row is the most obvious. Although TarDAL generates fusion results with the highest contrast, the object detection-driven fusion model focuses only on the significant objects in scenes. In particular, TarDAL sharpens the salient targets but ignores the background textures, which is not beneficial for the full understanding of the imaging scene. Although another semantic-driven approach, i.e., SeAFusion is effective in maintaining salient targets and preserving abundant texture details, it suffers from severe artifacts, which are highlighted by the green box. From the fusion results, we can clearly observe that our SuperFusion is able to effectively maintain significant targets in infrared images while retaining clear scene detail from visible images. In particular, our fusion results do not present severe artifacts in the green box areas, which indicates that our method can effectively mitigate the effects caused by misalignment in source images. We attribute these advantages to two aspects. On the one hand, we jointly model the image registration and fusion tasks, which allows our method to be robust to misalignment in source images. On the other hand, we design a fusion layer based on the global spatial attention module to achieve adaptive feature fusion, which allows to effectively perceive and integrate significant targets and texture detail information in source images. 2) Quantitative Comparison: Quantitative comparison results of our SuperFusion and other state-of-the-art fusion approaches on the MSRS dataset are provided in Fig. 9. One can notice that our method achieves the best results on MI, SSIM, F M I dct , and Q abf metrics. The optimal results on these indicators mean that our method can transfer the most pixel information, structural information, feature information, and edge information from source images to fused images. In addition, our method only follows SeAFusion by a narrow margin in the VIF metric, which indicates that our fusion results are more consistent with the perception of the human visual system. Although our method does not show advantages on the N abf metric, this is justifiable. Specifically, RFN-Nest and UMF-CMGR merge information from all source images indiscriminately during the fusion process, which causes the fused images not only to be disturbed by irrelevant information but also weakens the gradient of the fused images. This phenomenon can be corroborated by the Q abf metric and qualitative results. N abf determines artifacts by comparing the gradients of source images and fused images, so RFN-Nest and UMF-CMGR have more advantages in N abf . It is worth mentioning that our method can effectively eliminate ghosts caused by misalignment, so our method outperforms the other rest approaches in the N abf metric. Fig. 10 shows the quantitative comparison results of different fusion methods on the RoadScene dataset. We can find that SuperFusion achieves the best results in MI, VIF, SSIM, and N abf metrics. In particular, the performance of SuperFusion on N abf is in line with our expectations, which indicates that the proposed method can reduce the artifacts by registering the input images. Moreover, our method is comparable to other state-of-the-art methods on the Q abf metric, which means that our method transfers as much edge information as possible to the fusion results. Although our method lags behind UMF-CMGR and U2Fusion in the F M I dct metric, it is still able to transfer enough feature information into the fused image. In conclusion, both quantitative and quantitative results demonstrate the superior performance of the proposed method.\n\n\nD. Results on Semantic Segmentation\n\nAs mentioned previously, our method takes into account the requirements of real-world applications and devises the semantic constraints to prompt the fusion network to integrate semantic information as much as possible. In this section, we compare different fusion algorithms in the semantic segmentation task. For a fair comparison, we employ the segmentation model provided by SeAFusion [22] to perform semantic segmentation on source images and fused images.\n\n1) Qualitative Comparison: The segmentation results are visualized in Fig. 11. As we can see, the segmentation model is only able to segment part of the objects from the source images due to the lack of comprehensive descriptions for the imaging scenes. The fused images generated by some fusion algorithms (RFN-Nest, UMF-CMGR, and U2Fusion) also fail to assist the segmentation model to sufficiently perceive all objects in the scenarios since their networks are trained without explicit semantic guidance. Although TarDAL uses the object detection task to guide the training of the fusion network, semantic information provides by object detection drives the fusion network to focus only on high-contrast regions of conspicuous objects. Therefore, the fusion results synthesized by TarDAL also do not provide comprehensive semantic descriptions for the segmentation model. Moreover, even though SeAFusion can provide more sufficient semantic information than other methods for the segmentation task, misalignment usually misleads the final segmentation model. Fortunately, the registration network of our method can alleviate the effect of misalignment. Furthermore, the imposed semantic constraint instructs the fusion network to integrate as much semantic information as possible to facilitate the segmentation model to comprehensively perceive the imaging scenes. Thus, the segmentation model can segment the most objects in our fusion results, such as pedestrians in each scene.\n\n2) Quantitative Comparison: We also perform the quantitative comparison to objectively analyze the effects of different fusion approaches on the segmentation model. As shown in Table II, SuperFusion achieves the highest pixel intersection over union (IoU) on most objects and the best mean IoU (mIoU). SeAFusion leverages semantic constraints to guide the training of the fusion model, thus it achieves suboptimal results on most objects. It is worth mentioning that the fusion results synthesized from source images without strict registration tend to degrade the performance of the segmentation. In contrast, our method can eliminate the effect of misalignment by preregistering the input images, thus allowing the segmentation model to maintain better performance.\n\n\nE. Computational Complexity Analysis\n\nWe conduct the complexity evaluation to analyze the operational efficiency of different algorithms from two perspectives, i.e., parameters and running time. The computational efficiency comparison results of different image registration and fusion approaches are illustrated in Table III. From the results, we can find that our method has a significant efficiency advantage in the image registration task i.e., the fewest parameters and lowest running time. We attribute this advantage to the fact that our dense matcher employs the three-layer pyramid structure to extract features and estimate the deformation field progressively.\n\nIt is worth noting that UMF-CMGR consumes a lot of time on the style transformation task despite it having a very lightweight registration network. All deep learning-based methods have a significant advantage in running time compared to traditional methods, benefiting from GPU acceleration. Moreover, it can be seen from the results that our fusion network has the least number of parameters. However, since our method deploys the two-round four-directional RNN-based GSAM for adaptive feature fusion, our method slightly lags behind TarDAL and SeAFusion in terms of running time. Nonetheless, our method can still meet the real-time requirements in practical applications. It is important to note that TarDAL sacrifices the accuracy of network parameters and data to seek higher operation efficiency. In conclusion, our SuperFusion has superior operational efficiency, which allows it to be deployed into the real-world applications.\n\nF. Ablation Study 1) Ablation on Registration Components: In our methods, several losses and designs are costumed for the registration network, including symmetric paradigm (Sym.), end-point loss (L EP ), photometric loss (L P H ), and fusion-conjoint (Fusion). To explore their effectiveness on registration, we conduct ablation studies on the RoadScene dataset, which is cheap for time. The results are recorded in Table IV. Our full formulation achieves the smallest error, followed by the registration network trained without fusion network and the related losses including L CC (w/o Fusion), which demonstrates that image fusion can bridge the gap between input modalities and then facilitate cross-modal registration. Moreover, while discarding the L P H only lead to a minor degradation, the registration network cannot yield a satisfactory performance without L EP . This is  because although L P H could increase the precision at textured regions as expected, L EP does force the network to regress deformation fields in areas containing severe modal variances that are critical to cross-modal registration. Additionally, since we force the network to estimate bidirectional deformation fields, the symmetric paradigm is useful to provide end-point supervision on each direction, which reasons for the weak performance of only top half of registration training scheme shown in Fig. 3 (w/o Sym.).\n\n2) Ablation on Fusion Component: We integrate a global spatial attention module (GSAM) into the fusion network to implement adaptive feature fusion. In particular, the fourdirectional RNN in GSAM can perceive structural information from multiple directions to generate more reasonable fusion weights for feature maps. Thus, we also perform an ablation study to verify the effectiveness of GSAM. As shown in Table V, the fusion results without GSAM exhibit a significant degradation in all metrics except N abf . Moreover, as shown in Fig. 12, the visualized results also illustrate that the fusion network fails to effectively integrate the significant objects in the source images after discarding GSAM. In contrast, SuperFusion with GSAM can adaptively integrate significant information from source images into the fused image.\n\n3) Ablation on Semantic Constraint: As mentioned above, we introduce the semantic constraint to prompt the fusion network to integrate more semantic information. Thus, we also perform an ablation study on the semantic constraint to verify its effectiveness. We only report IoU for some important categories i.e., car, person, bike, and background, as well as mIoU for all categories. As shown in Table VI, IoU of all categories is degraded after removing the semantic constraint. It is worth noting that even without the semantic constraint, the segmentation model still shows better performance in the fusion results compared to source images, which is attributed to effective image registration and sufficient information integration during the fusion process. Furthermore, the semantic constraint can guide our fusion network to pay more attention to the semantic information during the fusion process, thus allowing fused images to contribute to the performance improvement of the segmentation model.\n\n\nV. CONCLUSION\n\nIn this paper, we propose a versatile framework considering both image registration, image fusion, and requirements of highlevel vision tasks, termed SuperFusion. It significantly extends the scope of image fusion in practical applications. SuperFusion consists of three components, including image registration, fusion, and semantic segmentation networks. Firstly, the registration network is devised to estimate bidirectional deformation fields so that both photometric and end-point losses can be used more simply to improve precision. Moreover, a symmetric joint registration and fusion scheme is developed to balance the bias on input modalities and further promote the registration with a similarity constraint in the fused domain. Secondly, a global spatial attention mechanism, which emphasizes the significant areas and targets in source images, is employed to achieve adaptive feature integration and serve the preceding registration as well as the subsequent segmentation. Thirdly, we design a semantic constraint based on Lovasz-Softmax loss to promote the fusion network to generate more reasonable results, which facilitates the perception of both machines and humans. In conclusion, we are the first to integrate image registration, fusion, and semantic segmentation into a single framework and achieve the mutual promotion of image fusion and image registration. Extensive experiments prove that each module in our framework achieves state-of-the-art performance.\n\n\n, respectively: L P H =L P H (I reg ir , I ir ) + L P H (I ir , I ir )+ L P H (I reg vi , I vi ) + L P H (I vi , I vi ),\n\nFig. 4 .\n4The architecture of dense matcher, which consists of a pyramid feature extractor and iterative flow estimators. Flows are estimated in three scales iteratively and summed up.\n\n\nA. Implementation Details and Experimental Configurations 1) Implementation Details: Our framework is implemented in PyTorch[75] on an NVIDIA TITAN RTX GPU and a 2.60GHz Intel(R) Xeon(R) Platinum 8171M CPU. It takes 1200 epochs to train our network with Adam optimizer and a batch of 8 pairs of 256 \u00d7 256 infrared and visible images. The learning rate of optimizer is initialized with 0.001 and then decayed linearly after the 600-th epoch. All images are normalized to [0, 1] before being fed into networks. Moreover, the hyper-parameters that control the trade-off of each sub-loss term are empirically set as \u03b1 1 = 0.1, \u03b1 2 = 0.1, \u03b2 1 = 0.3, \u03b2 2 = 0.7, \u03bb 1 = 20, \u03bb 2 = 100, and \u03bb 3 = 20.\n\nFig. 7 .\n7Qualitative registration performance of DASC, RIFT, GLU-Net, UMF-CMGR, CrossRAFT, and our SuperFusion. The first four rows of images are from the MSRS dataset, and the last two are from the RoadScene dataset. The purple textures are the gradients of registered infrared images and the backgrounds are the corresponding ground truths. The discriminateive regions that demonstrate the superiority of our method are highlighted in boxes. Note that, the gradients of the second column images are from the warped images, i.e., the misaligned infrared images.\n\nFig. 8 .\n8Qualitative comparison results of SuperFusion with five state-of-the-art infrared and visible image fusion methods on the MSRS and RoadScene datasets. All methods employ the built-in registration module (e.g., UMF-CMGR[30] and our SuperFusion) or CrossRAFT[66] to register the source images.\n\nFig. 9 .Fig. 10 .\n910Quantitative comparison results of SuperFusion with five state-of-the-art alternatives on 361 image pairs from the MSRS dataset. A point (x, y) on the curve denotes that there are 100 * x percent of image pairs that have metric values no more than y. Quantitative comparison results of SuperFusion with five state-of-the-art alternatives on 25 image pairs from the RoadScene dataset.\n\nFig. 11 .\n11Segmentation results for source images and fused images from the MSRS dataset. Two rows denote a scene, and from top to bottom is: 01234N, 01368N, and 01502D. The fused image indicates the fusion result generated by our SuperFusion, and the pre-trained segmentation model is provided by SeAFusion[22].\n\n\n.12. Visualized results of ablation study on the global spatial attention module in the fusion network.\n\n\nFig. 5. Architecture of the fusion network N F . Conv(c, k) denotes a convolutional layer with c output channels and kernel size of k \u00d7 k; GSAM indicates the global spatial attention module.Conv(8, 3) \nLeaky-ReLU \nConv(16, 3) \nLeaky-ReLU \nConv(16, 3) \nLeaky-ReLU \nConv(32, 3) \nLeaky-ReLU \n\nConv(8, 3) \nLeaky-ReLU \nConv(16, 3) \nLeaky-ReLU \nConv(16, 3) \nLeaky-ReLU \nConv(32, 3) \nLeaky-ReLU \nConv(16, 3) \nLeaky-ReLU \nConv(16, 3) \nLeaky-ReLU \n\nConv(8, 3) \nLeaky-ReLU \n\nConv(1, 3) \nTanh \n\nAdaptive Fusion \n\nGSAM \n\nC \n\nConv (16, 3) \nLeaky-ReLU \n\nConv (16, 3) \nLeaky-ReLU \n\nConv (4, 3) \nSigmoid \n\nWu,l,d,r \n\n(share) \n\nConv (32, 3) \n\n\n\n\nFig. 6. The schematic illustration of the global spatial attention module (GSAM). The global attention is calculated by adapting a spatial RNN to aggregate the spatial context in four directions.Contextual \nFeatures \n\nConv (32, 3) \nLeaky-ReLU \n\nConv (32, 3) \nSigmoid \n\nIR Features \nConcat \nConcat \nAttention Maps \nFor IR \n\nVIS Features \n\nW right \n\nW left \n\nW down \n\nW right \n\nW up \n\n\n\nTABLE I QUANTITATIVE\nIREGISTRATION PERFORMANCE ON MSRS AND ROADSCENE. MEAN REPROJECTION ERROR (RE) AND END-POINT ERROR (EPE) ARE REPORTED. THE BEST AND THE SECOND SCORES ARE HIGHLIGHTED IN BOLD AND ITALIC, RESPECTIVELY. *: OUT OF SCOPERE/EPE \nRIFT [63] \nDASC [62] \nUMF-CMGR [30] \nGLU-Net [76] \nCrossRAFT [66] \nSuperFusion \nSuperFusion(Reg) \n\nRS 1 \n24.4/ \n321/ \n20.6/ \n37.5/ \n7.50/ \n7.43/ \n7.58/ \nRS 2 \n67.6/* \n99.9/* \n15.7/163 \n20.7/445 \n4.25/14.0 \n3.86/9.13 \n4.01/10.4 \nMSRS 1 \n56.2/ \n390/ \n56.2/ \n43.8/ \n8.35/ \n7.1/ \n7.21/ \nMSRS 2 \n84.7/* \n185/* \n17.5/253 \n25.1/636 \n8.54/39.7 \n7.09/15.2 \n7.29/18.0 \n\nMSRS \n\nRoadScene \n\n(a) Visible \n(b) Infrared \n(c) DASC \n(d) RIFT \n(e) GLU-Net \n(f) UMF-CMGR (g) CrossRAFT (h) SuperFusion \n\n\n\nTABLE II SEGMENTATION\nIIPERFORMANCE (IOU) OF VISIBLE, INFRARED, AND FUSED IMAGES ON THE MSRS DATASET. BLOD INDICATES THE BEST RESULT AND ITALIC INDICATES THE SECOND BEST RESULT. THE PRE-TRAINED SEGMENTATION MODEL IS PROVIDED BY SEAFUSION [22]TABLE III COMPUTATIONAL EFFICIENCY COMPARISON WITH STATE-OF-THE-ART IMAGE REGISTRATION AND FUSION METHODSBackground \nCar \nPerson \nBike \nCurve \nCar Stop \nGuardrail \nColor Tone \nBump \nmIoU \n\nVisible \n97.92 \n86.80 \n39.97 \n70.50 \n53.33 \n71.84 \n85.9 \n65.44 \n79.2 \n72.32 \nInfrared \n94.52 \n50.10 \n41.53 \n16.32 \n13.92 \n12.54 \n0 \n11.34 \n18.48 \n28.75 \nRFN-Nest \n98.14 \n87.70 \n66.23 \n68.50 \n52.24 \n71.15 \n83.58 \n58.89 \n65.48 \n72.43 \nSeAFusion \n98.34 \n89.10 \n67.12 \n71.47 \n56.36 \n73.40 \n83.99 \n64.10 \n75.44 \n75.48 \nUMF-CMGR \n97.51 \n84.40 \n52.49 \n64.84 \n39.67 \n67.71 \n74.97 \n52.38 \n54.45 \n65.38 \nU2Fusion \n97.92 \n85.20 \n64.05 \n66.60 \n43.67 \n65.92 \n84.17 \n57.58 \n59.68 \n69.42 \nTarDAL \n96.46 \n71.00 \n53.08 \n46.49 \n7.86 \n28.62 \n59.88 \n49.59 \n14.70 \n47.52 \nSuperFusion \n98.49 \n90.00 \n70.82 \n72.00 \n62.37 \n74.23 \n84.55 \n65.48 \n79.13 \n77.42 \n\nRegistration \n\nDASC \nRIFT \nUMF-CMGR \nGLU-Net \nCrossRAFT \nSuperFusion \n\nParameters(M) \n-\n-\n11.5429 \n13.5905 \n39.7634 \n1.9624 \n\nTime(s) \nMSRS \n46.6238 \n4.4573 \n0.3893 \n0.0426 \n0.0857 \n0.0294 \n\nRoadScene \n23.8478 \n3.6980 \n0.4524 \n0.0351 \n0.0634 \n0.0327 \n\nFusion \n\nRFN-Nest \nSeAFusion \nUMF-CMGR \nU2Fusion \nTarDAL \nSuperFusion \n\nParameters(M) \n7.5242 \n0.1669 \n0.6293 \n0.6592 \n0.2966 \n0.1387 \n\nTime(s) \nMSRS \n0.1564 \n0.0371 \n0.0428 \n0.1444 \n0.0195 \n0.0765 \n\nRoadScene \n0.0810 \n0.0191 \n0.3520 \n0.7418 \n0.0103 \n0.0339 \n\n\n\nTABLE IV ABLATION\nIVSTUDY ON COMPONENTS OF REGISTRATION MODULE RE/EPE w/o Sym. w/o L EP w/o L P H w/o Fusion OursTABLE V ABLATION STUDY ON THE GLOBAL SPATIAL ATTENTION MODULE (GSAM)RS 1 \n7.81/ \n8.20/ \n7.60/ \n7.58/ \n7.43/ \nRS 2 \n4.32/14.6 \n4.81/18.9 \n4.10/12.2 \n4.01/10.4 \n3.86/9.13 \n\nMI \nVIF \nSSIM \nF M I dct \nQ abf \nN abf \n\nMSRS \nw/o GSAM \n3.086 \n0.8415 0.6970 \n0.2238 \n0.5394 \n0.070 \nOurs \n4.2398 0.9758 0.6978 \n0.3430 \n0.6360 \n0.0839 \n\nRoad-\nScene \nw/o GSAM \n2.8741 0.7489 0.6903 \n0.2173 \n0.3354 0.0479 \nOurs \n3.8925 0.8327 0.7059 \n0.2957 \n0.4985 0.0403 \n\n\n\nTABLE VI ABLATION\nVISTUDY ON THE SEMANTIC CONSTRAINBackground \nCar \nPerson \nBike \nmIoU \n\nw/o L Sea \n98.44 \n89.4 \n70.3 \n70.79 \n76.64 \nOurs \n98.49 \n89.73 \n70.82 \n71.99 \n77.42 \n\n\n\nDeep learning-based multi-focus image fusion: A survey and a comparative study. X Zhang, IEEE Trans. Pattern Anal. Mach. Intell. 449X. Zhang, \"Deep learning-based multi-focus image fusion: A survey and a comparative study,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 9, pp. 4819-4838, 2022.\n\nInfrared and visible image fusion methods and applications: A survey. J Ma, Y Ma, C Li, Inf. Fusion. 45J. Ma, Y. Ma, and C. Li, \"Infrared and visible image fusion methods and applications: A survey,\" Inf. Fusion, vol. 45, pp. 153-178, 2019.\n\nVifb: A visible and infrared image fusion benchmark. X Zhang, P Ye, G Xiao, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops, 2020. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops, 2020X. Zhang, P. Ye, and G. Xiao, \"Vifb: A visible and infrared image fusion benchmark,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops, 2020, pp. 104-105.\n\nImage fusion meets deep learning: A survey and perspective. H Zhang, H Xu, X Tian, J Jiang, J Ma, Inf. Fusion. 76H. Zhang, H. Xu, X. Tian, J. Jiang, and J. Ma, \"Image fusion meets deep learning: A survey and perspective,\" Inf. Fusion, vol. 76, pp. 323-336, 2021.\n\nDeep learning-based image fusion: A survey. L Tang, H Zhang, H Xu, J Ma, J. Image Graph. L. Tang, H. Zhang, H. Xu, and J. Ma, \"Deep learning-based image fusion: A survey,\" J. Image Graph., 2022.\n\nMfnet: Towards real-time semantic segmentation for autonomous vehicles with multi-spectral scenes. Q Ha, K Watanabe, T Karasawa, Y Ushiku, T Harada, Proc. IEEE Int. Conf. Intell. Rob. Syst. IEEE Int. Conf. Intell. Rob. SystQ. Ha, K. Watanabe, T. Karasawa, Y. Ushiku, and T. Harada, \"Mfnet: Towards real-time semantic segmentation for autonomous vehicles with multi-spectral scenes,\" in Proc. IEEE Int. Conf. Intell. Rob. Syst., 2017, pp. 5108-5115.\n\nObject fusion tracking based on visible and infrared images: A comprehensive review. X Zhang, P Ye, H Leung, K Gong, G Xiao, Inf. Fusion. 63X. Zhang, P. Ye, H. Leung, K. Gong, and G. Xiao, \"Object fusion tracking based on visible and infrared images: A comprehensive review,\" Inf. Fusion, vol. 63, pp. 166-187, 2020.\n\nImage fusion algorithm based on nonsubsampled dual-tree complex contourlet transform and compressive sensing pulse coupled neural network. M Yin, J Pang, Y Wei, P Duan, J. Comput. Aided Des. Comput. Graph. 3M. Yin, J. Pang, Y. Wei, and P. Duan, \"Image fusion algorithm based on nonsubsampled dual-tree complex contourlet transform and compressive sensing pulse coupled neural network,\" J. Comput. Aided Des. Comput. Graph., no. 3, pp. 411-419, 2016.\n\nInfrared and visible image fusion based on target-enhanced multiscale transform decomposition. J Chen, X Li, L Luo, X Mei, J Ma, Inf. Sci. 508J. Chen, X. Li, L. Luo, X. Mei, and J. Ma, \"Infrared and visible image fusion based on target-enhanced multiscale transform decomposition,\" Inf. Sci., vol. 508, pp. 64-78, 2020.\n\nMdlatlrr: A novel decomposition method for infrared and visible image fusion. H Li, X.-J Wu, J Kitler, IEEE Trans. Image Process. 29H. Li, X.-J. Wu, and J. Kitler, \"Mdlatlrr: A novel decomposition method for infrared and visible image fusion,\" IEEE Trans. Image Process., vol. 29, pp. 4733-4746, 2020.\n\nImage fusion with convolutional sparse representation. Y Liu, X Chen, R K Ward, Z J Wang, IEEE Signal Process. Letters. 2312Y. Liu, X. Chen, R. K. Ward, and Z. J. Wang, \"Image fusion with convolutional sparse representation,\" IEEE Signal Process. Letters, vol. 23, no. 12, pp. 1882-1886, 2016.\n\nInfrared and visible images fusion based on rpca and nsct. Z Fu, X Wang, J Xu, N Zhou, Y Zhao, Infrared Phys. Technol. 77Z. Fu, X. Wang, J. Xu, N. Zhou, and Y. Zhao, \"Infrared and visible images fusion based on rpca and nsct,\" Infrared Phys. Technol., vol. 77, pp. 114-123, 2016.\n\nInfrared and visible image fusion based on visual saliency map and weighted least square optimization. J Ma, Z Zhou, B Wang, H Zong, Infrared Phys. Technol. 82J. Ma, Z. Zhou, B. Wang, and H. Zong, \"Infrared and visible image fusion based on visual saliency map and weighted least square optimization,\" Infrared Phys. Technol., vol. 82, pp. 8-17, 2017.\n\nInfrared and visible image fusion via gradient transfer and total variation minimization. J Ma, C Chen, C Li, J Huang, Inf. Fusion. 31J. Ma, C. Chen, C. Li, and J. Huang, \"Infrared and visible image fusion via gradient transfer and total variation minimization,\" Inf. Fusion, vol. 31, pp. 100-109, 2016.\n\nMultisensor image fusion and enhancement in spectral total variation domain. W Zhao, H Lu, D Wang, IEEE Trans. Multimed. 204W. Zhao, H. Lu, and D. Wang, \"Multisensor image fusion and enhance- ment in spectral total variation domain,\" IEEE Trans. Multimed., vol. 20, no. 4, pp. 866-879, 2017.\n\nDensefuse: A fusion approach to infrared and visible images. H Li, X.-J Wu, IEEE Trans. Image Process. 285H. Li and X.-J. Wu, \"Densefuse: A fusion approach to infrared and visible images,\" IEEE Trans. Image Process., vol. 28, no. 5, pp. 2614-2623, 2019.\n\nClassification saliency-based rule for visible and infrared image fusion. H Xu, H Zhang, J Ma, IEEE Trans. Comput. Imaging. 7H. Xu, H. Zhang, and J. Ma, \"Classification saliency-based rule for visible and infrared image fusion,\" IEEE Trans. Comput. Imaging, vol. 7, pp. 824-836, 2021.\n\nLearning a deep multi-scale feature ensemble and an edge-attention guidance for image fusion. J Liu, X Fan, J Jiang, R Liu, Z Luo, IEEE Trans. Circuits Syst. Video Technol. 321J. Liu, X. Fan, J. Jiang, R. Liu, and Z. Luo, \"Learning a deep multi-scale feature ensemble and an edge-attention guidance for image fusion,\" IEEE Trans. Circuits Syst. Video Technol., vol. 32, no. 1, pp. 105-119, 2022.\n\nSelf-supervised feature adaption for infrared and visible image fusion. F Zhao, W Zhao, L Yao, Y Liu, Inf. Fusion. 76F. Zhao, W. Zhao, L. Yao, and Y. Liu, \"Self-supervised feature adaption for infrared and visible image fusion,\" Inf. Fusion, vol. 76, pp. 189-203, 2021.\n\nStdfusionnet: An infrared and visible image fusion network based on salient target detection. J Ma, L Tang, M Xu, H Zhang, G Xiao, IEEE Trans. Instrum. Meas. 70J. Ma, L. Tang, M. Xu, H. Zhang, and G. Xiao, \"Stdfusionnet: An infrared and visible image fusion network based on salient target detection,\" IEEE Trans. Instrum. Meas., vol. 70, pp. 1-13, 2021.\n\nMultimodal mri volumetric data fusion with convolutional neural networks. Y Liu, Y Shi, F Mu, J Cheng, C Li, X Chen, IEEE Trans. Instrum. Meas. 71Y. Liu, Y. Shi, F. Mu, J. Cheng, C. Li, and X. Chen, \"Multimodal mri volumetric data fusion with convolutional neural networks,\" IEEE Trans. Instrum. Meas., vol. 71, pp. 1-15, 2022.\n\nImage fusion in the loop of high-level vision tasks: A semantic-aware real-time infrared and visible image fusion network. L Tang, J Yuan, J Ma, Inf. Fusion. 82L. Tang, J. Yuan, and J. Ma, \"Image fusion in the loop of high-level vision tasks: A semantic-aware real-time infrared and visible image fusion network,\" Inf. Fusion, vol. 82, pp. 28-42, 2022.\n\nFusiongan: A generative adversarial network for infrared and visible image fusion. J Ma, W Yu, P Liang, C Li, J Jiang, Inf. Fusion. 48J. Ma, W. Yu, P. Liang, C. Li, and J. Jiang, \"Fusiongan: A generative adversarial network for infrared and visible image fusion,\" Inf. Fusion, vol. 48, pp. 11-26, 2019.\n\nGanmcc: A generative adversarial network with multiclassification constraints for infrared and visible image fusion. J Ma, H Zhang, Z Shao, P Liang, H Xu, IEEE Trans. Instrum. Meas. 70J. Ma, H. Zhang, Z. Shao, P. Liang, and H. Xu, \"Ganmcc: A generative adversarial network with multiclassification constraints for infrared and visible image fusion,\" IEEE Trans. Instrum. Meas., vol. 70, pp. 1-14, 2021.\n\nTargetaware dual adversarial learning and a multi-scenario multi-modality benchmark to fuse infrared and visible for object detection. J Liu, X Fan, Z Huang, G Wu, R Liu, W Zhong, Z Luo, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitJ. Liu, X. Fan, Z. Huang, G. Wu, R. Liu, W. Zhong, and Z. Luo, \"Target- aware dual adversarial learning and a multi-scenario multi-modality benchmark to fuse infrared and visible for object detection,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022, pp. 5802-5811.\n\nInfrared and visible image fusion via texture conditional generative adversarial network. Y Yang, J Liu, S Huang, W Wan, W Wen, J Guan, IEEE Trans. Circuits Syst. Video Technol. 3112Y. Yang, J. Liu, S. Huang, W. Wan, W. Wen, and J. Guan, \"Infrared and visible image fusion via texture conditional generative adversarial network,\" IEEE Trans. Circuits Syst. Video Technol., vol. 31, no. 12, pp. 4771-4783, 2021.\n\nSwinfusion: Cross-domain long-range learning for general image fusion via swin transformer. J Ma, L Tang, F Fan, J Huang, X Mei, Y Ma, IEEE/CAA J. Autom. Sinica. 97J. Ma, L. Tang, F. Fan, J. Huang, X. Mei, and Y. Ma, \"Swinfusion: Cross-domain long-range learning for general image fusion via swin transformer,\" IEEE/CAA J. Autom. Sinica, vol. 9, no. 7, pp. 1200-1217, 2022.\n\nYdtr: infrared and visible image fusion via y-shape dynamic transformer. W Tang, F He, Y Liu, IEEE Trans. Multimed. W. Tang, F. He, and Y. Liu, \"Ydtr: infrared and visible image fusion via y-shape dynamic transformer,\" IEEE Trans. Multimed., 2022.\n\nCgtf: Convolution-guided transformer for infrared and visible image fusion. J Li, J Zhu, C Li, X Chen, B Yang, IEEE Trans. Instrum. Meas. 715012314J. Li, J. Zhu, C. Li, X. Chen, and B. Yang, \"Cgtf: Convolution-guided transformer for infrared and visible image fusion,\" IEEE Trans. Instrum. Meas., vol. 71, p. 5012314, 2022.\n\nUnsupervised misaligned infrared and visible image fusion via cross-modality image generation and registration. W Di, L Jinyuan, F Xin, R Liu, Int. Joint Conf. W. Di, L. Jinyuan, F. Xin, and R. Liu, \"Unsupervised misaligned infrared and visible image fusion via cross-modality image generation and registration,\" in Int. Joint Conf. Artif. Intell., 2022.\n\nNestfuse: An infrared and visible image fusion architecture based on nest connection and spatial/channel attention models. H Li, X.-J Wu, T Durrani, IEEE Trans. Instrum. Meas. 6912H. Li, X.-J. Wu, and T. Durrani, \"Nestfuse: An infrared and visible image fusion architecture based on nest connection and spatial/channel attention models,\" IEEE Trans. Instrum. Meas., vol. 69, no. 12, pp. 9645-9656, 2020.\n\nRfn-nest: An end-to-end residual fusion network for infrared and visible images. H Li, X.-J Wu, J Kittler, Inf. Fusion. 73H. Li, X.-J. Wu, and J. Kittler, \"Rfn-nest: An end-to-end residual fusion network for infrared and visible images,\" Inf. Fusion, vol. 73, pp. 72-86, 2021.\n\nDrf: Disentangled representation for visible and infrared image fusion. H Xu, X Wang, J Ma, IEEE Trans. Instrum. Meas. 705006713H. Xu, X. Wang, and J. Ma, \"Drf: Disentangled representation for visible and infrared image fusion,\" IEEE Trans. Instrum. Meas., vol. 70, p. 5006713, 2021.\n\nInfrared and visible image fusion via parallel scene and texture learning. M Xu, L Tang, H Zhang, J Ma, Pattern Recognit. 132108929M. Xu, L. Tang, H. Zhang, and J. Ma, \"Infrared and visible image fusion via parallel scene and texture learning,\" Pattern Recognit., vol. 132, p. 108929, 2022.\n\nDidfuse: deep image decomposition for infrared and visible image fusion. Z Zhao, S Xu, C Zhang, J Liu, J Zhang, P Li, Int. Joint Conf. Z. Zhao, S. Xu, C. Zhang, J. Liu, J. Zhang, and P. Li, \"Didfuse: deep image decomposition for infrared and visible image fusion,\" in Int. Joint Conf. Artif. Intell., 2020, pp. 970-976.\n\nLearning specific and general realm feature representations for image fusion. F Zhao, W Zhao, IEEE Trans. Multimed. 23F. Zhao and W. Zhao, \"Learning specific and general realm feature representations for image fusion,\" IEEE Trans. Multimed., vol. 23, pp. 2745-2756, 2020.\n\nPiafusion: A progressive infrared and visible image fusion network based on illumination aware. L Tang, J Yuan, H Zhang, X Jiang, J Ma, Inf. Fusion. 83L. Tang, J. Yuan, H. Zhang, X. Jiang, and J. Ma, \"Piafusion: A progressive infrared and visible image fusion network based on illumination aware,\" Inf. Fusion, vol. 83, pp. 79-92, 2022.\n\nRxdnfuse: A aggregated residual dense network for infrared and visible image fusion. Y Long, H Jia, Y Zhong, Y Jiang, Y Jia, Inf. Fusion. 69Y. Long, H. Jia, Y. Zhong, Y. Jiang, and Y. Jia, \"Rxdnfuse: A aggregated residual dense network for infrared and visible image fusion,\" Inf. Fusion, vol. 69, pp. 128-141, 2021.\n\nSearching a hierarchically aggregated fusion architecture for fast multi-modality image fusion. R Liu, Z Liu, J Liu, X Fan, Proc. ACM Int. Conf. Multimed., 2021. ACM Int. Conf. Multimed., 2021R. Liu, Z. Liu, J. Liu, and X. Fan, \"Searching a hierarchically aggregated fusion architecture for fast multi-modality image fusion,\" in Proc. ACM Int. Conf. Multimed., 2021, pp. 1600-1608.\n\nRethinking the image fusion: A fast unified image fusion network based on proportional maintenance of gradient and intensity. H Zhang, H Xu, Y Xiao, X Guo, J Ma, Proc. AAAI Conf. AAAI ConfH. Zhang, H. Xu, Y. Xiao, X. Guo, and J. Ma, \"Rethinking the image fusion: A fast unified image fusion network based on proportional maintenance of gradient and intensity.\" in Proc. AAAI Conf. Artif. Intell., 2020, pp. 12 797-12 804.\n\nSdnet: A versatile squeeze-and-decomposition network for real-time image fusion. H Zhang, J Ma, Int. J. Comput. Vis. 12910H. Zhang and J. Ma, \"Sdnet: A versatile squeeze-and-decomposition network for real-time image fusion,\" Int. J. Comput. Vis., vol. 129, no. 10, pp. 2761-2785, 2021.\n\nFusiondn: A unified densely connected network for image fusion. H Xu, J Ma, Z Le, J Jiang, X Guo, Proc. AAAI Conf. AAAI ConfH. Xu, J. Ma, Z. Le, J. Jiang, and X. Guo, \"Fusiondn: A unified densely connected network for image fusion,\" in Proc. AAAI Conf. Artif. Intell., 2020, pp. 12 484-12 491.\n\nU2fusion: A unified unsupervised image fusion network. H Xu, J Ma, J Jiang, X Guo, H Ling, IEEE Trans. Pattern Anal. Mach. Intell. 441H. Xu, J. Ma, J. Jiang, X. Guo, and H. Ling, \"U2fusion: A unified unsupervised image fusion network,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 1, pp. 502-518, 2022.\n\nIfcnn: A general image fusion framework based on convolutional neural network. Y Zhang, Y Liu, P Sun, H Yan, X Zhao, L Zhang, Inf. Fusion. 54Y. Zhang, Y. Liu, P. Sun, H. Yan, X. Zhao, and L. Zhang, \"Ifcnn: A general image fusion framework based on convolutional neural network,\" Inf. Fusion, vol. 54, pp. 99-118, 2020.\n\nSgfusion: A saliency guided deeplearning framework for pixel-level image fusion. J Liu, R Dian, S Li, H Liu, Inf. Fusion. J. Liu, R. Dian, S. Li, and H. Liu, \"Sgfusion: A saliency guided deep- learning framework for pixel-level image fusion,\" Inf. Fusion, 2022.\n\nPpt fusion: Pyramid patch transformerfor a case study in image fusion. Y Fu, T Xu, X Wu, J Kittler, arXivY. Fu, T. Xu, X. Wu, and J. Kittler, \"Ppt fusion: Pyramid patch transformerfor a case study in image fusion,\" arXiv, 2021.\n\nImage fusion transformer. V Vs, J M J Valanarasu, P Oza, V M Patel, arXivV. VS, J. M. J. Valanarasu, P. Oza, and V. M. Patel, \"Image fusion transformer,\" arXiv, 2021.\n\nDndt: Infrared and visible image fusion via densenet and dual-transformer. H Zhao, R Nie, Proc. Int. Conf. Inf. Technol. Int. Conf. Inf. TechnolH. Zhao and R. Nie, \"Dndt: Infrared and visible image fusion via densenet and dual-transformer,\" in Proc. Int. Conf. Inf. Technol. Biomed. Eng., 2021, pp. 71-75.\n\nInfrared and visible image fusion via detail preserving adversarial learning. J Ma, P Liang, W Yu, C Chen, X Guo, J Wu, J Jiang, Inf. Fusion. 54J. Ma, P. Liang, W. Yu, C. Chen, X. Guo, J. Wu, and J. Jiang, \"Infrared and visible image fusion via detail preserving adversarial learning,\" Inf. Fusion, vol. 54, pp. 85-98, 2020.\n\nDdcgan: A dualdiscriminator conditional generative adversarial network for multiresolution image fusion. J Ma, H Xu, J Jiang, X Mei, X.-P Zhang, IEEE Trans. Image Process. 29J. Ma, H. Xu, J. Jiang, X. Mei, and X.-P. Zhang, \"Ddcgan: A dual- discriminator conditional generative adversarial network for multi- resolution image fusion,\" IEEE Trans. Image Process., vol. 29, pp. 4980-4995, 2020.\n\nAttentionfgan: Infrared and visible image fusion using attention-based generative adversarial networks. J Li, H Huo, C Li, R Wang, Q Feng, IEEE Trans. Multimed. 23J. Li, H. Huo, C. Li, R. Wang, and Q. Feng, \"Attentionfgan: Infrared and visible image fusion using attention-based generative adversarial networks,\" IEEE Trans. Multimed., vol. 23, pp. 1383-1396, 2020.\n\nSemantic-supervised infrared and visible image fusion via a dual-discriminator generative adversarial network. H Zhou, W Wu, Y Zhang, J Ma, H Ling, IEEE Trans. Multimed. H. Zhou, W. Wu, Y. Zhang, J. Ma, and H. Ling, \"Semantic-supervised infrared and visible image fusion via a dual-discriminator generative adversarial network,\" IEEE Trans. Multimed., 2021.\n\nGan-fm: Infrared and visible image fusion using gan with full-scale skip connection and dual markovian discriminators. H Zhang, J Yuan, X Tian, J Ma, IEEE Trans. Comput. Imaging. 7H. Zhang, J. Yuan, X. Tian, and J. Ma, \"Gan-fm: Infrared and visible image fusion using gan with full-scale skip connection and dual markovian discriminators,\" IEEE Trans. Comput. Imaging, vol. 7, pp. 1134-1147, 2021.\n\nGlioma segmentationoriented multi-modal mr image fusion with adversarial learning. Y Liu, Y Shi, F Mu, J Cheng, X Chen, IEEE/CAA J. Autom. Sinica. 98Y. Liu, Y. Shi, F. Mu, J. Cheng, and X. Chen, \"Glioma segmentation- oriented multi-modal mr image fusion with adversarial learning,\" IEEE/CAA J. Autom. Sinica, vol. 9, no. 8, pp. 1528-1531, 2022.\n\nSf-net: A multi-task model for brain tumor segmentation in multimodal mri via image fusion. Y Liu, F Mu, Y Shi, X Chen, IEEE Signal Process. Letters. 29Y. Liu, F. Mu, Y. Shi, and X. Chen, \"Sf-net: A multi-task model for brain tumor segmentation in multimodal mri via image fusion,\" IEEE Signal Process. Letters, vol. 29, pp. 1799-1803, 2022.\n\nRfnet: Unsupervised network for mutually reinforcing multi-modal image registration and fusion. H Xu, J Ma, J Yuan, Z Le, W Liu, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitH. Xu, J. Ma, J. Yuan, Z. Le, and W. Liu, \"Rfnet: Unsupervised network for mutually reinforcing multi-modal image registration and fusion,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., June 2022, pp. 19 679-19 688.\n\nAffine image registration using a new information metric. J Zhang, A Rangarajan, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitJ. Zhang and A. Rangarajan, \"Affine image registration using a new information metric,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2004, pp. 1-8.\n\nNonrigid registration using free-form deformations: application to breast mr images. D Rueckert, L Sonoda, C Hayes, D Hill, M Leach, D Hawkes, IEEE Trans. Med. Imaging. 188D. Rueckert, L. Sonoda, C. Hayes, D. Hill, M. Leach, and D. Hawkes, \"Nonrigid registration using free-form deformations: application to breast mr images,\" IEEE Trans. Med. Imaging, vol. 18, no. 8, pp. 712-721, 1999.\n\nFast normalized cross-correlation. J.-C Yoo, T H Han, Int. J Circuits, Syst. signal Process. 286J.-C. Yoo and T. H. Han, \"Fast normalized cross-correlation,\" Int. J Circuits, Syst. signal Process., vol. 28, no. 6, pp. 819-843, 2009.\n\nMedical image registration using mutual information. F Maes, D Vandermeulen, P Suetens, Proc. IEEE. IEEE91F. Maes, D. Vandermeulen, and P. Suetens, \"Medical image registration using mutual information,\" Proc. IEEE, vol. 91, no. 10, pp. 1699-1722, 2003.\n\nMultispectral image feature points. C Aguilera, F Barrera, F Lumbreras, A D Sappa, R Toledo, Sensors. 129C. Aguilera, F. Barrera, F. Lumbreras, A. D. Sappa, and R. Toledo, \"Multispectral image feature points,\" Sensors, vol. 12, no. 9, pp. 12 661- 12 672, 2012.\n\nDasc: Robust dense descriptor for multi-modal and multi-spectral correspondence estimation. S Kim, D Min, B Ham, M N Do, K Sohn, IEEE Trans. Pattern Anal. Mach. Intell. 399S. Kim, D. Min, B. Ham, M. N. Do, and K. Sohn, \"Dasc: Robust dense descriptor for multi-modal and multi-spectral correspondence estimation,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 39, no. 9, pp. 1712-1729, 2016.\n\nRift: Multi-modal image matching based on radiation-variation insensitive feature transform. J Li, Q Hu, M Ai, IEEE Trans. Image Process. 29J. Li, Q. Hu, and M. Ai, \"Rift: Multi-modal image matching based on radiation-variation insensitive feature transform,\" IEEE Trans. Image Process., vol. 29, pp. 3296-3310, 2019.\n\nVoxelmorph: a learning framework for deformable medical image registration. G Balakrishnan, A Zhao, M R Sabuncu, J Guttag, A V Dalca, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern Recognit38G. Balakrishnan, A. Zhao, M. R. Sabuncu, J. Guttag, and A. V. Dalca, \"Voxelmorph: a learning framework for deformable medical image registration,\" Proc. IEEE Conf. Comput. Vis. Pattern Recognit., vol. 38, no. 8, pp. 1788-1800, 2019.\n\nUnsupervised multi-modal image registration via geometry preserving image-to-image translation. M Arar, Y Ginger, D Danon, A H Bermano, D Cohen-Or, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern Recognit13M. Arar, Y. Ginger, D. Danon, A. H. Bermano, and D. Cohen-Or, \"Unsupervised multi-modal image registration via geometry preserving image-to-image translation,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2020, pp. 13 410-13 419.\n\nPromoting single-modal optical flow network for diverse cross-modal flow estimation. S Zhou, W Tan, B Yan, Proc. AAAI Conf. AAAI ConfS. Zhou, W. Tan, and B. Yan, \"Promoting single-modal optical flow network for diverse cross-modal flow estimation,\" in Proc. AAAI Conf. Artif. Intell., 2022, pp. 3562-3570.\n\nA deep learning framework for remote sensing image registration. S Wang, D Quan, X Liang, M Ning, Y Guo, L Jiao, ISPRS J. Photogramm. Remote Sens. 145S. Wang, D. Quan, X. Liang, M. Ning, Y. Guo, and L. Jiao, \"A deep learning framework for remote sensing image registration,\" ISPRS J. Photogramm. Remote Sens., vol. 145, pp. 148-164, 2018.\n\nLearning phrase representations using rnn encoder-decoder for statistical machine translation. K Cho, B Van Merri\u00ebnboer, C Gulcehre, D Bahdanau, F Bougares, H Schwenk, Y Bengio, arXivK. Cho, B. Van Merri\u00ebnboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, \"Learning phrase representations using rnn encoder-decoder for statistical machine translation,\" arXiv, 2014.\n\nInside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. S Bell, C L Zitnick, K Bala, R Girshick, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitS. Bell, C. L. Zitnick, K. Bala, and R. Girshick, \"Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 2874-2883.\n\nDirection-aware spatial context features for shadow detection and removal. X Hu, C.-W Fu, L Zhu, J Qin, P.-A Heng, IEEE Trans. Pattern Anal. Mach. Intell. 4211X. Hu, C.-W. Fu, L. Zhu, J. Qin, and P.-A. Heng, \"Direction-aware spatial context features for shadow detection and removal,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 42, no. 11, pp. 2795-2808, 2019.\n\nSpatial attentive single-image deraining with a high quality real rain dataset. T Wang, X Yang, K Xu, S Chen, Q Zhang, R W Lau, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern Recognit12T. Wang, X. Yang, K. Xu, S. Chen, Q. Zhang, and R. W. Lau, \"Spatial attentive single-image deraining with a high quality real rain dataset,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 12 270- 12 279.\n\nUnflow: Unsupervised learning of optical flow with a bidirectional census loss. S Meister, J Hur, S Roth, Proc. AAAI Conf. AAAI ConfS. Meister, J. Hur, and S. Roth, \"Unflow: Unsupervised learning of optical flow with a bidirectional census loss,\" in Proc. AAAI Conf. Artif. Intell., 2018, pp. 7251-7259.\n\nImage quality assessment: from error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, IEEE Trans. Image Process. 134Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, \"Image quality assessment: from error visibility to structural similarity,\" IEEE Trans. Image Process., vol. 13, no. 4, pp. 600-612, 2004.\n\nThe lov\u00e1sz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks. M Berman, A R Triki, M B Blaschko, Proc. IEEE Conf. Comput. Vis. IEEE Conf. Comput. VisM. Berman, A. R. Triki, and M. B. Blaschko, \"The lov\u00e1sz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 4413-4421.\n\nPytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, Proc. nullA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., \"Pytorch: An imperative style, high-performance deep learning library,\" in Proc. Adv. Neural Inf. Process. Syst., 2019, pp. 8026-8037.\n\nGlu-net: Global-local universal network for dense flow and correspondences. P Truong, M Danelljan, R Timofte, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitP. Truong, M. Danelljan, and R. Timofte, \"Glu-net: Global-local universal network for dense flow and correspondences,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2020, pp. 6258-6268.\n\nHe is currently a Ph.D. student with the Electronic Information School, Wuhan University. His research interests include computer vision, machine learning, and pattern recognition. 2020Changsha, ChinaLinfeng Tang received the B.E. degree from the School of Computer Science and Engineering, Central South UniversityLinfeng Tang received the B.E. degree from the School of Computer Science and Engineering, Cen- tral South University, Changsha, China, in 2020. He is currently a Ph.D. student with the Electronic Information School, Wuhan University. His research interests include computer vision, machine learning, and pattern recognition.\n\nHe is currently a Ph.D. student at the Multi-Spectral Vision Processing Lab, Wuhan University. His current research interests include pattern recognition and machine learning. Wuhan, ChinaYuxin Deng received the B.S. degree from the Electronic Information School, Wuhan UniversityYuxin Deng received the B.S. degree from the Electronic Information School, Wuhan University, Wuhan, China, in 2019. He is currently a Ph.D. student at the Multi-Spectral Vision Processing Lab, Wuhan University. His current research interests include pattern recognition and machine learning.\n", "annotations": {"author": "[{\"end\":112,\"start\":103},{\"end\":116,\"start\":113},{\"end\":124,\"start\":117}]", "publisher": null, "author_last_name": "[{\"end\":111,\"start\":103},{\"end\":115,\"start\":113},{\"end\":123,\"start\":117}]", "author_first_name": null, "author_affiliation": null, "title": "[{\"end\":87,\"start\":1},{\"end\":211,\"start\":125}]", "venue": null, "abstract": "[{\"end\":2150,\"start\":356}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2338,\"start\":2335},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2587,\"start\":2584},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2592,\"start\":2589},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2597,\"start\":2594},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2602,\"start\":2599},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3792,\"start\":3789},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3797,\"start\":3794},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3802,\"start\":3799},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4188,\"start\":4185},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4193,\"start\":4190},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4199,\"start\":4195},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4240,\"start\":4236},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4261,\"start\":4257},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4282,\"start\":4278},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4310,\"start\":4306},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4316,\"start\":4312},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4451,\"start\":4447},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4457,\"start\":4453},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4463,\"start\":4459},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4510,\"start\":4506},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4516,\"start\":4512},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4522,\"start\":4518},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4528,\"start\":4524},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4577,\"start\":4573},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4583,\"start\":4579},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4589,\"start\":4585},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4595,\"start\":4591},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4623,\"start\":4619},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4629,\"start\":4625},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4635,\"start\":4631},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6001,\"start\":5997},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7784,\"start\":7780},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7800,\"start\":7796},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11875,\"start\":11871},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12157,\"start\":12153},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12163,\"start\":12159},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12192,\"start\":12188},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12198,\"start\":12194},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12219,\"start\":12215},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12645,\"start\":12641},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13074,\"start\":13070},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13305,\"start\":13301},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13568,\"start\":13564},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":13792,\"start\":13788},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14236,\"start\":14232},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":14374,\"start\":14370},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":14524,\"start\":14520},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":14530,\"start\":14526},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":14636,\"start\":14632},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14651,\"start\":14647},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":15214,\"start\":15210},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15498,\"start\":15494},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15515,\"start\":15511},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15530,\"start\":15526},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15722,\"start\":15718},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":15736,\"start\":15732},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16445,\"start\":16441},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":16451,\"start\":16447},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":16557,\"start\":16553},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":16577,\"start\":16573},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":16590,\"start\":16586},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":16607,\"start\":16603},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17125,\"start\":17121},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17927,\"start\":17923},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17943,\"start\":17939},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":18769,\"start\":18765},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":18953,\"start\":18949},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":19679,\"start\":19675},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":19769,\"start\":19765},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":19801,\"start\":19797},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":19997,\"start\":19993},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":20030,\"start\":20026},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":20182,\"start\":20178},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":20193,\"start\":20189},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":20208,\"start\":20204},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":20547,\"start\":20543},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":20692,\"start\":20688},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":20842,\"start\":20838},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":21259,\"start\":21255},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21387,\"start\":21383},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":21867,\"start\":21863},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":22100,\"start\":22096},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":29709,\"start\":29705},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30443,\"start\":30439},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31326,\"start\":31322},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":32717,\"start\":32713},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":34284,\"start\":34280},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":34586,\"start\":34582},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":35445,\"start\":35441},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":36323,\"start\":36319},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":36978,\"start\":36975},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":36980,\"start\":36978},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":36983,\"start\":36980},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":36986,\"start\":36983},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":43163,\"start\":43159},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":43174,\"start\":43170},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":43188,\"start\":43184},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":43203,\"start\":43199},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":43223,\"start\":43219},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":43240,\"start\":43236},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":43259,\"start\":43255},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":44012,\"start\":44008},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":44028,\"start\":44024},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":44043,\"start\":44039},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":44058,\"start\":44054},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":44075,\"start\":44071},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":45011,\"start\":45007},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":45212,\"start\":45208},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":46334,\"start\":46330},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":46514,\"start\":46510},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":53114,\"start\":53110},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":62227,\"start\":62223},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":63588,\"start\":63584},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":63626,\"start\":63622},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":64377,\"start\":64373},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":64384,\"start\":64382}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":61910,\"start\":61788},{\"attributes\":{\"id\":\"fig_1\"},\"end\":62096,\"start\":61911},{\"attributes\":{\"id\":\"fig_2\"},\"end\":62789,\"start\":62097},{\"attributes\":{\"id\":\"fig_3\"},\"end\":63354,\"start\":62790},{\"attributes\":{\"id\":\"fig_4\"},\"end\":63657,\"start\":63355},{\"attributes\":{\"id\":\"fig_5\"},\"end\":64063,\"start\":63658},{\"attributes\":{\"id\":\"fig_6\"},\"end\":64378,\"start\":64064},{\"attributes\":{\"id\":\"fig_7\"},\"end\":64484,\"start\":64379},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":65113,\"start\":64485},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":65499,\"start\":65114},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":66228,\"start\":65500},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":67808,\"start\":66229},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":68369,\"start\":67809},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":68546,\"start\":68370}]", "paragraph": "[{\"end\":3803,\"start\":2169},{\"end\":4780,\"start\":3805},{\"end\":4974,\"start\":4782},{\"end\":7388,\"start\":4976},{\"end\":8581,\"start\":7390},{\"end\":9171,\"start\":8583},{\"end\":9231,\"start\":9173},{\"end\":10446,\"start\":9233},{\"end\":10791,\"start\":10448},{\"end\":11029,\"start\":10793},{\"end\":11288,\"start\":11078},{\"end\":12646,\"start\":11290},{\"end\":14707,\"start\":12648},{\"end\":15941,\"start\":14709},{\"end\":16661,\"start\":15980},{\"end\":17872,\"start\":16699},{\"end\":19423,\"start\":17874},{\"end\":20520,\"start\":19461},{\"end\":21642,\"start\":20522},{\"end\":22373,\"start\":21693},{\"end\":23033,\"start\":22417},{\"end\":23161,\"start\":23035},{\"end\":23386,\"start\":23163},{\"end\":24466,\"start\":23430},{\"end\":24796,\"start\":24500},{\"end\":24917,\"start\":24832},{\"end\":25144,\"start\":24995},{\"end\":25744,\"start\":25180},{\"end\":26545,\"start\":25772},{\"end\":26780,\"start\":26645},{\"end\":27234,\"start\":26808},{\"end\":27487,\"start\":27255},{\"end\":28714,\"start\":27489},{\"end\":29463,\"start\":28716},{\"end\":29570,\"start\":29542},{\"end\":30187,\"start\":29572},{\"end\":30444,\"start\":30263},{\"end\":31245,\"start\":30446},{\"end\":31336,\"start\":31247},{\"end\":31609,\"start\":31397},{\"end\":31995,\"start\":31666},{\"end\":32130,\"start\":32042},{\"end\":32426,\"start\":32168},{\"end\":32953,\"start\":32547},{\"end\":33584,\"start\":33066},{\"end\":33847,\"start\":33675},{\"end\":34201,\"start\":33900},{\"end\":34657,\"start\":34203},{\"end\":35094,\"start\":34659},{\"end\":35548,\"start\":35151},{\"end\":35719,\"start\":35637},{\"end\":35876,\"start\":35771},{\"end\":36404,\"start\":35951},{\"end\":36616,\"start\":36464},{\"end\":37602,\"start\":36691},{\"end\":38048,\"start\":37604},{\"end\":38754,\"start\":38121},{\"end\":39210,\"start\":38807},{\"end\":39495,\"start\":39212},{\"end\":39750,\"start\":39559},{\"end\":40058,\"start\":39752},{\"end\":41632,\"start\":40107},{\"end\":41817,\"start\":41634},{\"end\":42039,\"start\":41871},{\"end\":42484,\"start\":42188},{\"end\":43908,\"start\":42528},{\"end\":45368,\"start\":43910},{\"end\":46965,\"start\":45370},{\"end\":48163,\"start\":46967},{\"end\":52681,\"start\":48165},{\"end\":53182,\"start\":52721},{\"end\":54668,\"start\":53184},{\"end\":55437,\"start\":54670},{\"end\":56110,\"start\":55478},{\"end\":57047,\"start\":56112},{\"end\":58453,\"start\":57049},{\"end\":59284,\"start\":58455},{\"end\":60290,\"start\":59286},{\"end\":61787,\"start\":60308}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":22416,\"start\":22374},{\"attributes\":{\"id\":\"formula_1\"},\"end\":24499,\"start\":24467},{\"attributes\":{\"id\":\"formula_2\"},\"end\":24831,\"start\":24797},{\"attributes\":{\"id\":\"formula_3\"},\"end\":24994,\"start\":24918},{\"attributes\":{\"id\":\"formula_4\"},\"end\":25179,\"start\":25145},{\"attributes\":{\"id\":\"formula_5\"},\"end\":25771,\"start\":25745},{\"attributes\":{\"id\":\"formula_6\"},\"end\":26610,\"start\":26546},{\"attributes\":{\"id\":\"formula_7\"},\"end\":26644,\"start\":26610},{\"attributes\":{\"id\":\"formula_8\"},\"end\":26807,\"start\":26781},{\"attributes\":{\"id\":\"formula_9\"},\"end\":29541,\"start\":29464},{\"attributes\":{\"id\":\"formula_10\"},\"end\":30262,\"start\":30188},{\"attributes\":{\"id\":\"formula_11\"},\"end\":31396,\"start\":31337},{\"attributes\":{\"id\":\"formula_13\"},\"end\":31665,\"start\":31610},{\"attributes\":{\"id\":\"formula_14\"},\"end\":32041,\"start\":31996},{\"attributes\":{\"id\":\"formula_15\"},\"end\":32546,\"start\":32427},{\"attributes\":{\"id\":\"formula_16\"},\"end\":33065,\"start\":32954},{\"attributes\":{\"id\":\"formula_17\"},\"end\":33674,\"start\":33585},{\"attributes\":{\"id\":\"formula_18\"},\"end\":33899,\"start\":33848},{\"attributes\":{\"id\":\"formula_19\"},\"end\":35150,\"start\":35095},{\"attributes\":{\"id\":\"formula_20\"},\"end\":35636,\"start\":35549},{\"attributes\":{\"id\":\"formula_21\"},\"end\":35770,\"start\":35720},{\"attributes\":{\"id\":\"formula_22\"},\"end\":35950,\"start\":35877},{\"attributes\":{\"id\":\"formula_23\"},\"end\":36463,\"start\":36405},{\"attributes\":{\"id\":\"formula_24\"},\"end\":36674,\"start\":36617},{\"attributes\":{\"id\":\"formula_25\"},\"end\":38120,\"start\":38049},{\"attributes\":{\"id\":\"formula_26\"},\"end\":38806,\"start\":38755},{\"attributes\":{\"id\":\"formula_27\"},\"end\":39558,\"start\":39496},{\"attributes\":{\"id\":\"formula_28\"},\"end\":40106,\"start\":40059},{\"attributes\":{\"id\":\"formula_29\"},\"end\":41870,\"start\":41818},{\"attributes\":{\"id\":\"formula_30\"},\"end\":42187,\"start\":42164}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":45838,\"start\":45831},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":54855,\"start\":54847},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":55765,\"start\":55756},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":57474,\"start\":57466},{\"end\":58869,\"start\":58862},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":59690,\"start\":59682}]", "section_header": "[{\"end\":2167,\"start\":2152},{\"end\":11076,\"start\":11032},{\"end\":15978,\"start\":15944},{\"end\":16697,\"start\":16664},{\"end\":19459,\"start\":19426},{\"end\":21691,\"start\":21645},{\"end\":23405,\"start\":23389},{\"end\":23428,\"start\":23408},{\"end\":27253,\"start\":27237},{\"end\":32166,\"start\":32133},{\"end\":36689,\"start\":36676},{\"end\":42101,\"start\":42042},{\"end\":42163,\"start\":42104},{\"end\":42526,\"start\":42487},{\"end\":52719,\"start\":52684},{\"end\":55476,\"start\":55440},{\"end\":60306,\"start\":60293},{\"end\":61920,\"start\":61912},{\"end\":62799,\"start\":62791},{\"end\":63364,\"start\":63356},{\"end\":63676,\"start\":63659},{\"end\":64074,\"start\":64065},{\"end\":65521,\"start\":65501},{\"end\":66251,\"start\":66230},{\"end\":67827,\"start\":67810},{\"end\":68388,\"start\":68371}]", "table": "[{\"end\":65113,\"start\":64677},{\"end\":65499,\"start\":65311},{\"end\":66228,\"start\":65736},{\"end\":67808,\"start\":66577},{\"end\":68369,\"start\":67991},{\"end\":68546,\"start\":68422}]", "figure_caption": "[{\"end\":61910,\"start\":61790},{\"end\":62096,\"start\":61922},{\"end\":62789,\"start\":62099},{\"end\":63354,\"start\":62801},{\"end\":63657,\"start\":63366},{\"end\":64063,\"start\":63680},{\"end\":64378,\"start\":64077},{\"end\":64484,\"start\":64381},{\"end\":64677,\"start\":64487},{\"end\":65311,\"start\":65116},{\"end\":65736,\"start\":65523},{\"end\":66577,\"start\":66254},{\"end\":67991,\"start\":67830},{\"end\":68422,\"start\":68391}]", "figure_ref": "[{\"end\":5422,\"start\":5416},{\"end\":5963,\"start\":5957},{\"end\":6272,\"start\":6266},{\"end\":8002,\"start\":7996},{\"end\":9016,\"start\":9010},{\"end\":22728,\"start\":22722},{\"end\":22735,\"start\":22729},{\"end\":24308,\"start\":24302},{\"end\":26354,\"start\":26348},{\"end\":28164,\"start\":28158},{\"end\":30992,\"start\":30986},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":37885,\"start\":37879},{\"end\":39318,\"start\":39312},{\"end\":40248,\"start\":40242},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":47050,\"start\":47044},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":48348,\"start\":48342},{\"end\":50598,\"start\":50592},{\"end\":51841,\"start\":51834},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":53261,\"start\":53254},{\"end\":58441,\"start\":58435},{\"end\":58996,\"start\":58989}]", "bib_author_first_name": "[{\"end\":68629,\"start\":68628},{\"end\":68922,\"start\":68921},{\"end\":68928,\"start\":68927},{\"end\":68934,\"start\":68933},{\"end\":69147,\"start\":69146},{\"end\":69156,\"start\":69155},{\"end\":69162,\"start\":69161},{\"end\":69519,\"start\":69518},{\"end\":69528,\"start\":69527},{\"end\":69534,\"start\":69533},{\"end\":69542,\"start\":69541},{\"end\":69551,\"start\":69550},{\"end\":69767,\"start\":69766},{\"end\":69775,\"start\":69774},{\"end\":69784,\"start\":69783},{\"end\":69790,\"start\":69789},{\"end\":70018,\"start\":70017},{\"end\":70024,\"start\":70023},{\"end\":70036,\"start\":70035},{\"end\":70048,\"start\":70047},{\"end\":70058,\"start\":70057},{\"end\":70454,\"start\":70453},{\"end\":70463,\"start\":70462},{\"end\":70469,\"start\":70468},{\"end\":70478,\"start\":70477},{\"end\":70486,\"start\":70485},{\"end\":70826,\"start\":70825},{\"end\":70833,\"start\":70832},{\"end\":70841,\"start\":70840},{\"end\":70848,\"start\":70847},{\"end\":71233,\"start\":71232},{\"end\":71241,\"start\":71240},{\"end\":71247,\"start\":71246},{\"end\":71254,\"start\":71253},{\"end\":71261,\"start\":71260},{\"end\":71537,\"start\":71536},{\"end\":71546,\"start\":71542},{\"end\":71552,\"start\":71551},{\"end\":71817,\"start\":71816},{\"end\":71824,\"start\":71823},{\"end\":71832,\"start\":71831},{\"end\":71834,\"start\":71833},{\"end\":71842,\"start\":71841},{\"end\":71844,\"start\":71843},{\"end\":72116,\"start\":72115},{\"end\":72122,\"start\":72121},{\"end\":72130,\"start\":72129},{\"end\":72136,\"start\":72135},{\"end\":72144,\"start\":72143},{\"end\":72441,\"start\":72440},{\"end\":72447,\"start\":72446},{\"end\":72455,\"start\":72454},{\"end\":72463,\"start\":72462},{\"end\":72781,\"start\":72780},{\"end\":72787,\"start\":72786},{\"end\":72795,\"start\":72794},{\"end\":72801,\"start\":72800},{\"end\":73073,\"start\":73072},{\"end\":73081,\"start\":73080},{\"end\":73087,\"start\":73086},{\"end\":73350,\"start\":73349},{\"end\":73359,\"start\":73355},{\"end\":73618,\"start\":73617},{\"end\":73624,\"start\":73623},{\"end\":73633,\"start\":73632},{\"end\":73924,\"start\":73923},{\"end\":73931,\"start\":73930},{\"end\":73938,\"start\":73937},{\"end\":73947,\"start\":73946},{\"end\":73954,\"start\":73953},{\"end\":74299,\"start\":74298},{\"end\":74307,\"start\":74306},{\"end\":74315,\"start\":74314},{\"end\":74322,\"start\":74321},{\"end\":74592,\"start\":74591},{\"end\":74598,\"start\":74597},{\"end\":74606,\"start\":74605},{\"end\":74612,\"start\":74611},{\"end\":74621,\"start\":74620},{\"end\":74928,\"start\":74927},{\"end\":74935,\"start\":74934},{\"end\":74942,\"start\":74941},{\"end\":74948,\"start\":74947},{\"end\":74957,\"start\":74956},{\"end\":74963,\"start\":74962},{\"end\":75306,\"start\":75305},{\"end\":75314,\"start\":75313},{\"end\":75322,\"start\":75321},{\"end\":75620,\"start\":75619},{\"end\":75626,\"start\":75625},{\"end\":75632,\"start\":75631},{\"end\":75641,\"start\":75640},{\"end\":75647,\"start\":75646},{\"end\":75958,\"start\":75957},{\"end\":75964,\"start\":75963},{\"end\":75973,\"start\":75972},{\"end\":75981,\"start\":75980},{\"end\":75990,\"start\":75989},{\"end\":76380,\"start\":76379},{\"end\":76387,\"start\":76386},{\"end\":76394,\"start\":76393},{\"end\":76403,\"start\":76402},{\"end\":76409,\"start\":76408},{\"end\":76416,\"start\":76415},{\"end\":76425,\"start\":76424},{\"end\":76886,\"start\":76885},{\"end\":76894,\"start\":76893},{\"end\":76901,\"start\":76900},{\"end\":76910,\"start\":76909},{\"end\":76917,\"start\":76916},{\"end\":76924,\"start\":76923},{\"end\":77300,\"start\":77299},{\"end\":77306,\"start\":77305},{\"end\":77314,\"start\":77313},{\"end\":77321,\"start\":77320},{\"end\":77330,\"start\":77329},{\"end\":77337,\"start\":77336},{\"end\":77656,\"start\":77655},{\"end\":77664,\"start\":77663},{\"end\":77670,\"start\":77669},{\"end\":77908,\"start\":77907},{\"end\":77914,\"start\":77913},{\"end\":77921,\"start\":77920},{\"end\":77927,\"start\":77926},{\"end\":77935,\"start\":77934},{\"end\":78269,\"start\":78268},{\"end\":78275,\"start\":78274},{\"end\":78286,\"start\":78285},{\"end\":78293,\"start\":78292},{\"end\":78636,\"start\":78635},{\"end\":78645,\"start\":78641},{\"end\":78651,\"start\":78650},{\"end\":78999,\"start\":78998},{\"end\":79008,\"start\":79004},{\"end\":79014,\"start\":79013},{\"end\":79268,\"start\":79267},{\"end\":79274,\"start\":79273},{\"end\":79282,\"start\":79281},{\"end\":79556,\"start\":79555},{\"end\":79562,\"start\":79561},{\"end\":79570,\"start\":79569},{\"end\":79579,\"start\":79578},{\"end\":79846,\"start\":79845},{\"end\":79854,\"start\":79853},{\"end\":79860,\"start\":79859},{\"end\":79869,\"start\":79868},{\"end\":79876,\"start\":79875},{\"end\":79885,\"start\":79884},{\"end\":80172,\"start\":80171},{\"end\":80180,\"start\":80179},{\"end\":80463,\"start\":80462},{\"end\":80471,\"start\":80470},{\"end\":80479,\"start\":80478},{\"end\":80488,\"start\":80487},{\"end\":80497,\"start\":80496},{\"end\":80790,\"start\":80789},{\"end\":80798,\"start\":80797},{\"end\":80805,\"start\":80804},{\"end\":80814,\"start\":80813},{\"end\":80823,\"start\":80822},{\"end\":81119,\"start\":81118},{\"end\":81126,\"start\":81125},{\"end\":81133,\"start\":81132},{\"end\":81140,\"start\":81139},{\"end\":81532,\"start\":81531},{\"end\":81541,\"start\":81540},{\"end\":81547,\"start\":81546},{\"end\":81555,\"start\":81554},{\"end\":81562,\"start\":81561},{\"end\":81910,\"start\":81909},{\"end\":81919,\"start\":81918},{\"end\":82180,\"start\":82179},{\"end\":82186,\"start\":82185},{\"end\":82192,\"start\":82191},{\"end\":82198,\"start\":82197},{\"end\":82207,\"start\":82206},{\"end\":82466,\"start\":82465},{\"end\":82472,\"start\":82471},{\"end\":82478,\"start\":82477},{\"end\":82487,\"start\":82486},{\"end\":82494,\"start\":82493},{\"end\":82803,\"start\":82802},{\"end\":82812,\"start\":82811},{\"end\":82819,\"start\":82818},{\"end\":82826,\"start\":82825},{\"end\":82833,\"start\":82832},{\"end\":82841,\"start\":82840},{\"end\":83125,\"start\":83124},{\"end\":83132,\"start\":83131},{\"end\":83140,\"start\":83139},{\"end\":83146,\"start\":83145},{\"end\":83378,\"start\":83377},{\"end\":83384,\"start\":83383},{\"end\":83390,\"start\":83389},{\"end\":83396,\"start\":83395},{\"end\":83562,\"start\":83561},{\"end\":83568,\"start\":83567},{\"end\":83572,\"start\":83569},{\"end\":83586,\"start\":83585},{\"end\":83593,\"start\":83592},{\"end\":83595,\"start\":83594},{\"end\":83779,\"start\":83778},{\"end\":83787,\"start\":83786},{\"end\":84089,\"start\":84088},{\"end\":84095,\"start\":84094},{\"end\":84104,\"start\":84103},{\"end\":84110,\"start\":84109},{\"end\":84118,\"start\":84117},{\"end\":84125,\"start\":84124},{\"end\":84131,\"start\":84130},{\"end\":84442,\"start\":84441},{\"end\":84448,\"start\":84447},{\"end\":84454,\"start\":84453},{\"end\":84463,\"start\":84462},{\"end\":84473,\"start\":84469},{\"end\":84834,\"start\":84833},{\"end\":84840,\"start\":84839},{\"end\":84847,\"start\":84846},{\"end\":84853,\"start\":84852},{\"end\":84861,\"start\":84860},{\"end\":85208,\"start\":85207},{\"end\":85216,\"start\":85215},{\"end\":85222,\"start\":85221},{\"end\":85231,\"start\":85230},{\"end\":85237,\"start\":85236},{\"end\":85575,\"start\":85574},{\"end\":85584,\"start\":85583},{\"end\":85592,\"start\":85591},{\"end\":85600,\"start\":85599},{\"end\":85938,\"start\":85937},{\"end\":85945,\"start\":85944},{\"end\":85952,\"start\":85951},{\"end\":85958,\"start\":85957},{\"end\":85967,\"start\":85966},{\"end\":86293,\"start\":86292},{\"end\":86300,\"start\":86299},{\"end\":86306,\"start\":86305},{\"end\":86313,\"start\":86312},{\"end\":86640,\"start\":86639},{\"end\":86646,\"start\":86645},{\"end\":86652,\"start\":86651},{\"end\":86660,\"start\":86659},{\"end\":86666,\"start\":86665},{\"end\":87042,\"start\":87041},{\"end\":87051,\"start\":87050},{\"end\":87394,\"start\":87393},{\"end\":87406,\"start\":87405},{\"end\":87416,\"start\":87415},{\"end\":87425,\"start\":87424},{\"end\":87433,\"start\":87432},{\"end\":87442,\"start\":87441},{\"end\":87736,\"start\":87732},{\"end\":87743,\"start\":87742},{\"end\":87745,\"start\":87744},{\"end\":87985,\"start\":87984},{\"end\":87993,\"start\":87992},{\"end\":88009,\"start\":88008},{\"end\":88222,\"start\":88221},{\"end\":88234,\"start\":88233},{\"end\":88245,\"start\":88244},{\"end\":88258,\"start\":88257},{\"end\":88260,\"start\":88259},{\"end\":88269,\"start\":88268},{\"end\":88540,\"start\":88539},{\"end\":88547,\"start\":88546},{\"end\":88554,\"start\":88553},{\"end\":88561,\"start\":88560},{\"end\":88563,\"start\":88562},{\"end\":88569,\"start\":88568},{\"end\":88933,\"start\":88932},{\"end\":88939,\"start\":88938},{\"end\":88945,\"start\":88944},{\"end\":89235,\"start\":89234},{\"end\":89251,\"start\":89250},{\"end\":89259,\"start\":89258},{\"end\":89261,\"start\":89260},{\"end\":89272,\"start\":89271},{\"end\":89282,\"start\":89281},{\"end\":89284,\"start\":89283},{\"end\":89713,\"start\":89712},{\"end\":89721,\"start\":89720},{\"end\":89731,\"start\":89730},{\"end\":89740,\"start\":89739},{\"end\":89742,\"start\":89741},{\"end\":89753,\"start\":89752},{\"end\":90178,\"start\":90177},{\"end\":90186,\"start\":90185},{\"end\":90193,\"start\":90192},{\"end\":90465,\"start\":90464},{\"end\":90473,\"start\":90472},{\"end\":90481,\"start\":90480},{\"end\":90490,\"start\":90489},{\"end\":90498,\"start\":90497},{\"end\":90505,\"start\":90504},{\"end\":90835,\"start\":90834},{\"end\":90842,\"start\":90841},{\"end\":90861,\"start\":90860},{\"end\":90873,\"start\":90872},{\"end\":90885,\"start\":90884},{\"end\":90897,\"start\":90896},{\"end\":90908,\"start\":90907},{\"end\":91226,\"start\":91225},{\"end\":91234,\"start\":91233},{\"end\":91236,\"start\":91235},{\"end\":91247,\"start\":91246},{\"end\":91255,\"start\":91254},{\"end\":91654,\"start\":91653},{\"end\":91663,\"start\":91659},{\"end\":91669,\"start\":91668},{\"end\":91676,\"start\":91675},{\"end\":91686,\"start\":91682},{\"end\":92024,\"start\":92023},{\"end\":92032,\"start\":92031},{\"end\":92040,\"start\":92039},{\"end\":92046,\"start\":92045},{\"end\":92054,\"start\":92053},{\"end\":92063,\"start\":92062},{\"end\":92065,\"start\":92064},{\"end\":92462,\"start\":92461},{\"end\":92473,\"start\":92472},{\"end\":92480,\"start\":92479},{\"end\":92761,\"start\":92760},{\"end\":92769,\"start\":92768},{\"end\":92771,\"start\":92770},{\"end\":92780,\"start\":92779},{\"end\":92782,\"start\":92781},{\"end\":92792,\"start\":92791},{\"end\":92794,\"start\":92793},{\"end\":93163,\"start\":93162},{\"end\":93173,\"start\":93172},{\"end\":93175,\"start\":93174},{\"end\":93184,\"start\":93183},{\"end\":93186,\"start\":93185},{\"end\":93567,\"start\":93566},{\"end\":93577,\"start\":93576},{\"end\":93586,\"start\":93585},{\"end\":93595,\"start\":93594},{\"end\":93604,\"start\":93603},{\"end\":93616,\"start\":93615},{\"end\":93626,\"start\":93625},{\"end\":93637,\"start\":93636},{\"end\":93644,\"start\":93643},{\"end\":93658,\"start\":93657},{\"end\":94008,\"start\":94007},{\"end\":94018,\"start\":94017},{\"end\":94031,\"start\":94030}]", "bib_author_last_name": "[{\"end\":68635,\"start\":68630},{\"end\":68925,\"start\":68923},{\"end\":68931,\"start\":68929},{\"end\":68937,\"start\":68935},{\"end\":69153,\"start\":69148},{\"end\":69159,\"start\":69157},{\"end\":69167,\"start\":69163},{\"end\":69525,\"start\":69520},{\"end\":69531,\"start\":69529},{\"end\":69539,\"start\":69535},{\"end\":69548,\"start\":69543},{\"end\":69554,\"start\":69552},{\"end\":69772,\"start\":69768},{\"end\":69781,\"start\":69776},{\"end\":69787,\"start\":69785},{\"end\":69793,\"start\":69791},{\"end\":70021,\"start\":70019},{\"end\":70033,\"start\":70025},{\"end\":70045,\"start\":70037},{\"end\":70055,\"start\":70049},{\"end\":70065,\"start\":70059},{\"end\":70460,\"start\":70455},{\"end\":70466,\"start\":70464},{\"end\":70475,\"start\":70470},{\"end\":70483,\"start\":70479},{\"end\":70491,\"start\":70487},{\"end\":70830,\"start\":70827},{\"end\":70838,\"start\":70834},{\"end\":70845,\"start\":70842},{\"end\":70853,\"start\":70849},{\"end\":71238,\"start\":71234},{\"end\":71244,\"start\":71242},{\"end\":71251,\"start\":71248},{\"end\":71258,\"start\":71255},{\"end\":71264,\"start\":71262},{\"end\":71540,\"start\":71538},{\"end\":71549,\"start\":71547},{\"end\":71559,\"start\":71553},{\"end\":71821,\"start\":71818},{\"end\":71829,\"start\":71825},{\"end\":71839,\"start\":71835},{\"end\":71849,\"start\":71845},{\"end\":72119,\"start\":72117},{\"end\":72127,\"start\":72123},{\"end\":72133,\"start\":72131},{\"end\":72141,\"start\":72137},{\"end\":72149,\"start\":72145},{\"end\":72444,\"start\":72442},{\"end\":72452,\"start\":72448},{\"end\":72460,\"start\":72456},{\"end\":72468,\"start\":72464},{\"end\":72784,\"start\":72782},{\"end\":72792,\"start\":72788},{\"end\":72798,\"start\":72796},{\"end\":72807,\"start\":72802},{\"end\":73078,\"start\":73074},{\"end\":73084,\"start\":73082},{\"end\":73092,\"start\":73088},{\"end\":73353,\"start\":73351},{\"end\":73362,\"start\":73360},{\"end\":73621,\"start\":73619},{\"end\":73630,\"start\":73625},{\"end\":73636,\"start\":73634},{\"end\":73928,\"start\":73925},{\"end\":73935,\"start\":73932},{\"end\":73944,\"start\":73939},{\"end\":73951,\"start\":73948},{\"end\":73958,\"start\":73955},{\"end\":74304,\"start\":74300},{\"end\":74312,\"start\":74308},{\"end\":74319,\"start\":74316},{\"end\":74326,\"start\":74323},{\"end\":74595,\"start\":74593},{\"end\":74603,\"start\":74599},{\"end\":74609,\"start\":74607},{\"end\":74618,\"start\":74613},{\"end\":74626,\"start\":74622},{\"end\":74932,\"start\":74929},{\"end\":74939,\"start\":74936},{\"end\":74945,\"start\":74943},{\"end\":74954,\"start\":74949},{\"end\":74960,\"start\":74958},{\"end\":74968,\"start\":74964},{\"end\":75311,\"start\":75307},{\"end\":75319,\"start\":75315},{\"end\":75325,\"start\":75323},{\"end\":75623,\"start\":75621},{\"end\":75629,\"start\":75627},{\"end\":75638,\"start\":75633},{\"end\":75644,\"start\":75642},{\"end\":75653,\"start\":75648},{\"end\":75961,\"start\":75959},{\"end\":75970,\"start\":75965},{\"end\":75978,\"start\":75974},{\"end\":75987,\"start\":75982},{\"end\":75993,\"start\":75991},{\"end\":76384,\"start\":76381},{\"end\":76391,\"start\":76388},{\"end\":76400,\"start\":76395},{\"end\":76406,\"start\":76404},{\"end\":76413,\"start\":76410},{\"end\":76422,\"start\":76417},{\"end\":76429,\"start\":76426},{\"end\":76891,\"start\":76887},{\"end\":76898,\"start\":76895},{\"end\":76907,\"start\":76902},{\"end\":76914,\"start\":76911},{\"end\":76921,\"start\":76918},{\"end\":76929,\"start\":76925},{\"end\":77303,\"start\":77301},{\"end\":77311,\"start\":77307},{\"end\":77318,\"start\":77315},{\"end\":77327,\"start\":77322},{\"end\":77334,\"start\":77331},{\"end\":77340,\"start\":77338},{\"end\":77661,\"start\":77657},{\"end\":77667,\"start\":77665},{\"end\":77674,\"start\":77671},{\"end\":77911,\"start\":77909},{\"end\":77918,\"start\":77915},{\"end\":77924,\"start\":77922},{\"end\":77932,\"start\":77928},{\"end\":77940,\"start\":77936},{\"end\":78272,\"start\":78270},{\"end\":78283,\"start\":78276},{\"end\":78290,\"start\":78287},{\"end\":78297,\"start\":78294},{\"end\":78639,\"start\":78637},{\"end\":78648,\"start\":78646},{\"end\":78659,\"start\":78652},{\"end\":79002,\"start\":79000},{\"end\":79011,\"start\":79009},{\"end\":79022,\"start\":79015},{\"end\":79271,\"start\":79269},{\"end\":79279,\"start\":79275},{\"end\":79285,\"start\":79283},{\"end\":79559,\"start\":79557},{\"end\":79567,\"start\":79563},{\"end\":79576,\"start\":79571},{\"end\":79582,\"start\":79580},{\"end\":79851,\"start\":79847},{\"end\":79857,\"start\":79855},{\"end\":79866,\"start\":79861},{\"end\":79873,\"start\":79870},{\"end\":79882,\"start\":79877},{\"end\":79888,\"start\":79886},{\"end\":80177,\"start\":80173},{\"end\":80185,\"start\":80181},{\"end\":80468,\"start\":80464},{\"end\":80476,\"start\":80472},{\"end\":80485,\"start\":80480},{\"end\":80494,\"start\":80489},{\"end\":80500,\"start\":80498},{\"end\":80795,\"start\":80791},{\"end\":80802,\"start\":80799},{\"end\":80811,\"start\":80806},{\"end\":80820,\"start\":80815},{\"end\":80827,\"start\":80824},{\"end\":81123,\"start\":81120},{\"end\":81130,\"start\":81127},{\"end\":81137,\"start\":81134},{\"end\":81144,\"start\":81141},{\"end\":81538,\"start\":81533},{\"end\":81544,\"start\":81542},{\"end\":81552,\"start\":81548},{\"end\":81559,\"start\":81556},{\"end\":81565,\"start\":81563},{\"end\":81916,\"start\":81911},{\"end\":81922,\"start\":81920},{\"end\":82183,\"start\":82181},{\"end\":82189,\"start\":82187},{\"end\":82195,\"start\":82193},{\"end\":82204,\"start\":82199},{\"end\":82211,\"start\":82208},{\"end\":82469,\"start\":82467},{\"end\":82475,\"start\":82473},{\"end\":82484,\"start\":82479},{\"end\":82491,\"start\":82488},{\"end\":82499,\"start\":82495},{\"end\":82809,\"start\":82804},{\"end\":82816,\"start\":82813},{\"end\":82823,\"start\":82820},{\"end\":82830,\"start\":82827},{\"end\":82838,\"start\":82834},{\"end\":82847,\"start\":82842},{\"end\":83129,\"start\":83126},{\"end\":83137,\"start\":83133},{\"end\":83143,\"start\":83141},{\"end\":83150,\"start\":83147},{\"end\":83381,\"start\":83379},{\"end\":83387,\"start\":83385},{\"end\":83393,\"start\":83391},{\"end\":83404,\"start\":83397},{\"end\":83565,\"start\":83563},{\"end\":83583,\"start\":83573},{\"end\":83590,\"start\":83587},{\"end\":83601,\"start\":83596},{\"end\":83784,\"start\":83780},{\"end\":83791,\"start\":83788},{\"end\":84092,\"start\":84090},{\"end\":84101,\"start\":84096},{\"end\":84107,\"start\":84105},{\"end\":84115,\"start\":84111},{\"end\":84122,\"start\":84119},{\"end\":84128,\"start\":84126},{\"end\":84137,\"start\":84132},{\"end\":84445,\"start\":84443},{\"end\":84451,\"start\":84449},{\"end\":84460,\"start\":84455},{\"end\":84467,\"start\":84464},{\"end\":84479,\"start\":84474},{\"end\":84837,\"start\":84835},{\"end\":84844,\"start\":84841},{\"end\":84850,\"start\":84848},{\"end\":84858,\"start\":84854},{\"end\":84866,\"start\":84862},{\"end\":85213,\"start\":85209},{\"end\":85219,\"start\":85217},{\"end\":85228,\"start\":85223},{\"end\":85234,\"start\":85232},{\"end\":85242,\"start\":85238},{\"end\":85581,\"start\":85576},{\"end\":85589,\"start\":85585},{\"end\":85597,\"start\":85593},{\"end\":85603,\"start\":85601},{\"end\":85942,\"start\":85939},{\"end\":85949,\"start\":85946},{\"end\":85955,\"start\":85953},{\"end\":85964,\"start\":85959},{\"end\":85972,\"start\":85968},{\"end\":86297,\"start\":86294},{\"end\":86303,\"start\":86301},{\"end\":86310,\"start\":86307},{\"end\":86318,\"start\":86314},{\"end\":86643,\"start\":86641},{\"end\":86649,\"start\":86647},{\"end\":86657,\"start\":86653},{\"end\":86663,\"start\":86661},{\"end\":86670,\"start\":86667},{\"end\":87048,\"start\":87043},{\"end\":87062,\"start\":87052},{\"end\":87403,\"start\":87395},{\"end\":87413,\"start\":87407},{\"end\":87422,\"start\":87417},{\"end\":87430,\"start\":87426},{\"end\":87439,\"start\":87434},{\"end\":87449,\"start\":87443},{\"end\":87740,\"start\":87737},{\"end\":87749,\"start\":87746},{\"end\":87990,\"start\":87986},{\"end\":88006,\"start\":87994},{\"end\":88017,\"start\":88010},{\"end\":88231,\"start\":88223},{\"end\":88242,\"start\":88235},{\"end\":88255,\"start\":88246},{\"end\":88266,\"start\":88261},{\"end\":88276,\"start\":88270},{\"end\":88544,\"start\":88541},{\"end\":88551,\"start\":88548},{\"end\":88558,\"start\":88555},{\"end\":88566,\"start\":88564},{\"end\":88574,\"start\":88570},{\"end\":88936,\"start\":88934},{\"end\":88942,\"start\":88940},{\"end\":88948,\"start\":88946},{\"end\":89248,\"start\":89236},{\"end\":89256,\"start\":89252},{\"end\":89269,\"start\":89262},{\"end\":89279,\"start\":89273},{\"end\":89290,\"start\":89285},{\"end\":89718,\"start\":89714},{\"end\":89728,\"start\":89722},{\"end\":89737,\"start\":89732},{\"end\":89750,\"start\":89743},{\"end\":89762,\"start\":89754},{\"end\":90183,\"start\":90179},{\"end\":90190,\"start\":90187},{\"end\":90197,\"start\":90194},{\"end\":90470,\"start\":90466},{\"end\":90478,\"start\":90474},{\"end\":90487,\"start\":90482},{\"end\":90495,\"start\":90491},{\"end\":90502,\"start\":90499},{\"end\":90510,\"start\":90506},{\"end\":90839,\"start\":90836},{\"end\":90858,\"start\":90843},{\"end\":90870,\"start\":90862},{\"end\":90882,\"start\":90874},{\"end\":90894,\"start\":90886},{\"end\":90905,\"start\":90898},{\"end\":90915,\"start\":90909},{\"end\":91231,\"start\":91227},{\"end\":91244,\"start\":91237},{\"end\":91252,\"start\":91248},{\"end\":91264,\"start\":91256},{\"end\":91657,\"start\":91655},{\"end\":91666,\"start\":91664},{\"end\":91673,\"start\":91670},{\"end\":91680,\"start\":91677},{\"end\":91691,\"start\":91687},{\"end\":92029,\"start\":92025},{\"end\":92037,\"start\":92033},{\"end\":92043,\"start\":92041},{\"end\":92051,\"start\":92047},{\"end\":92060,\"start\":92055},{\"end\":92069,\"start\":92066},{\"end\":92470,\"start\":92463},{\"end\":92477,\"start\":92474},{\"end\":92485,\"start\":92481},{\"end\":92766,\"start\":92762},{\"end\":92777,\"start\":92772},{\"end\":92789,\"start\":92783},{\"end\":92805,\"start\":92795},{\"end\":93170,\"start\":93164},{\"end\":93181,\"start\":93176},{\"end\":93195,\"start\":93187},{\"end\":93574,\"start\":93568},{\"end\":93583,\"start\":93578},{\"end\":93592,\"start\":93587},{\"end\":93601,\"start\":93596},{\"end\":93613,\"start\":93605},{\"end\":93623,\"start\":93617},{\"end\":93634,\"start\":93627},{\"end\":93641,\"start\":93638},{\"end\":93655,\"start\":93645},{\"end\":93665,\"start\":93659},{\"end\":94015,\"start\":94009},{\"end\":94028,\"start\":94019},{\"end\":94039,\"start\":94032}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":234473252},\"end\":68849,\"start\":68548},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52073194},\"end\":69091,\"start\":68851},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":211068851},\"end\":69456,\"start\":69093},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":237444708},\"end\":69720,\"start\":69458},{\"attributes\":{\"id\":\"b4\"},\"end\":69916,\"start\":69722},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2887474},\"end\":70366,\"start\":69918},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":218960676},\"end\":70684,\"start\":70368},{\"attributes\":{\"id\":\"b7\"},\"end\":71135,\"start\":70686},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":202745244},\"end\":71456,\"start\":71137},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":211677495},\"end\":71759,\"start\":71458},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":18649677},\"end\":72054,\"start\":71761},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":124156965},\"end\":72335,\"start\":72056},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":125122786},\"end\":72688,\"start\":72337},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":205432776},\"end\":72993,\"start\":72690},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3906428},\"end\":73286,\"start\":72995},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":5060429},\"end\":73541,\"start\":73288},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":237208866},\"end\":73827,\"start\":73543},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":234047512},\"end\":74224,\"start\":73829},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":236237688},\"end\":74495,\"start\":74226},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":233990943},\"end\":74851,\"start\":74497},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":249904861},\"end\":75180,\"start\":74853},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":245643604},\"end\":75534,\"start\":75182},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":71142966},\"end\":75838,\"start\":75536},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":229647400},\"end\":76242,\"start\":75840},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":247793193},\"end\":76793,\"start\":76244},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":234029527},\"end\":77205,\"start\":76795},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":250183118},\"end\":77580,\"start\":77207},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":250935185},\"end\":77829,\"start\":77582},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":248783943},\"end\":78154,\"start\":77831},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":249018020},\"end\":78510,\"start\":78156},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":220280328},\"end\":78915,\"start\":78512},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":232146779},\"end\":79193,\"start\":78917},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":232042583},\"end\":79478,\"start\":79195},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":251029588},\"end\":79770,\"start\":79480},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":214605606},\"end\":80091,\"start\":79772},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":226714147},\"end\":80364,\"start\":80093},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":247847452},\"end\":80702,\"start\":80366},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":229467398},\"end\":81020,\"start\":80704},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":239011619},\"end\":81403,\"start\":81022},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":214195213},\"end\":81826,\"start\":81405},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":238807929},\"end\":82113,\"start\":81828},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":213637621},\"end\":82408,\"start\":82115},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":220934367},\"end\":82721,\"start\":82410},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":199677411},\"end\":83041,\"start\":82723},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":252796243},\"end\":83304,\"start\":83043},{\"attributes\":{\"id\":\"b45\"},\"end\":83533,\"start\":83306},{\"attributes\":{\"id\":\"b46\"},\"end\":83701,\"start\":83535},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":246532058},\"end\":84008,\"start\":83703},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":199688641},\"end\":84334,\"start\":84010},{\"attributes\":{\"id\":\"b49\"},\"end\":84727,\"start\":84336},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":219955104},\"end\":85094,\"start\":84729},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":244507385},\"end\":85453,\"start\":85096},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":240002983},\"end\":85852,\"start\":85455},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":251329514},\"end\":86198,\"start\":85854},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":251708710},\"end\":86541,\"start\":86200},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":250602199},\"end\":86981,\"start\":86543},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":12540608},\"end\":87306,\"start\":86983},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":330039},\"end\":87695,\"start\":87308},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":2397291},\"end\":87929,\"start\":87697},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":18716029},\"end\":88183,\"start\":87931},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":8931631},\"end\":88445,\"start\":88185},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":22650607},\"end\":88837,\"start\":88447},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":209463301},\"end\":89156,\"start\":88839},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":52281312},\"end\":89614,\"start\":89158},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":212747560},\"end\":90090,\"start\":89616},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":250288468},\"end\":90397,\"start\":90092},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":125367985},\"end\":90737,\"start\":90399},{\"attributes\":{\"id\":\"b67\"},\"end\":91125,\"start\":90739},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":7148278},\"end\":91576,\"start\":91127},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":218674010},\"end\":91941,\"start\":91578},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":91184545},\"end\":92379,\"start\":91943},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":19160323},\"end\":92684,\"start\":92381},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":207761262},\"end\":93033,\"start\":92686},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":4716955},\"end\":93494,\"start\":93035},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":202786778},\"end\":93929,\"start\":93496},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":209202711},\"end\":94320,\"start\":93931},{\"attributes\":{\"id\":\"b76\"},\"end\":94962,\"start\":94322},{\"attributes\":{\"id\":\"b77\"},\"end\":95536,\"start\":94964}]", "bib_title": "[{\"end\":68626,\"start\":68548},{\"end\":68919,\"start\":68851},{\"end\":69144,\"start\":69093},{\"end\":69516,\"start\":69458},{\"end\":69764,\"start\":69722},{\"end\":70015,\"start\":69918},{\"end\":70451,\"start\":70368},{\"end\":70823,\"start\":70686},{\"end\":71230,\"start\":71137},{\"end\":71534,\"start\":71458},{\"end\":71814,\"start\":71761},{\"end\":72113,\"start\":72056},{\"end\":72438,\"start\":72337},{\"end\":72778,\"start\":72690},{\"end\":73070,\"start\":72995},{\"end\":73347,\"start\":73288},{\"end\":73615,\"start\":73543},{\"end\":73921,\"start\":73829},{\"end\":74296,\"start\":74226},{\"end\":74589,\"start\":74497},{\"end\":74925,\"start\":74853},{\"end\":75303,\"start\":75182},{\"end\":75617,\"start\":75536},{\"end\":75955,\"start\":75840},{\"end\":76377,\"start\":76244},{\"end\":76883,\"start\":76795},{\"end\":77297,\"start\":77207},{\"end\":77653,\"start\":77582},{\"end\":77905,\"start\":77831},{\"end\":78266,\"start\":78156},{\"end\":78633,\"start\":78512},{\"end\":78996,\"start\":78917},{\"end\":79265,\"start\":79195},{\"end\":79553,\"start\":79480},{\"end\":79843,\"start\":79772},{\"end\":80169,\"start\":80093},{\"end\":80460,\"start\":80366},{\"end\":80787,\"start\":80704},{\"end\":81116,\"start\":81022},{\"end\":81529,\"start\":81405},{\"end\":81907,\"start\":81828},{\"end\":82177,\"start\":82115},{\"end\":82463,\"start\":82410},{\"end\":82800,\"start\":82723},{\"end\":83122,\"start\":83043},{\"end\":83776,\"start\":83703},{\"end\":84086,\"start\":84010},{\"end\":84439,\"start\":84336},{\"end\":84831,\"start\":84729},{\"end\":85205,\"start\":85096},{\"end\":85572,\"start\":85455},{\"end\":85935,\"start\":85854},{\"end\":86290,\"start\":86200},{\"end\":86637,\"start\":86543},{\"end\":87039,\"start\":86983},{\"end\":87391,\"start\":87308},{\"end\":87730,\"start\":87697},{\"end\":87982,\"start\":87931},{\"end\":88219,\"start\":88185},{\"end\":88537,\"start\":88447},{\"end\":88930,\"start\":88839},{\"end\":89232,\"start\":89158},{\"end\":89710,\"start\":89616},{\"end\":90175,\"start\":90092},{\"end\":90462,\"start\":90399},{\"end\":91223,\"start\":91127},{\"end\":91651,\"start\":91578},{\"end\":92021,\"start\":91943},{\"end\":92459,\"start\":92381},{\"end\":92758,\"start\":92686},{\"end\":93160,\"start\":93035},{\"end\":93564,\"start\":93496},{\"end\":94005,\"start\":93931}]", "bib_author": "[{\"end\":68637,\"start\":68628},{\"end\":68927,\"start\":68921},{\"end\":68933,\"start\":68927},{\"end\":68939,\"start\":68933},{\"end\":69155,\"start\":69146},{\"end\":69161,\"start\":69155},{\"end\":69169,\"start\":69161},{\"end\":69527,\"start\":69518},{\"end\":69533,\"start\":69527},{\"end\":69541,\"start\":69533},{\"end\":69550,\"start\":69541},{\"end\":69556,\"start\":69550},{\"end\":69774,\"start\":69766},{\"end\":69783,\"start\":69774},{\"end\":69789,\"start\":69783},{\"end\":69795,\"start\":69789},{\"end\":70023,\"start\":70017},{\"end\":70035,\"start\":70023},{\"end\":70047,\"start\":70035},{\"end\":70057,\"start\":70047},{\"end\":70067,\"start\":70057},{\"end\":70462,\"start\":70453},{\"end\":70468,\"start\":70462},{\"end\":70477,\"start\":70468},{\"end\":70485,\"start\":70477},{\"end\":70493,\"start\":70485},{\"end\":70832,\"start\":70825},{\"end\":70840,\"start\":70832},{\"end\":70847,\"start\":70840},{\"end\":70855,\"start\":70847},{\"end\":71240,\"start\":71232},{\"end\":71246,\"start\":71240},{\"end\":71253,\"start\":71246},{\"end\":71260,\"start\":71253},{\"end\":71266,\"start\":71260},{\"end\":71542,\"start\":71536},{\"end\":71551,\"start\":71542},{\"end\":71561,\"start\":71551},{\"end\":71823,\"start\":71816},{\"end\":71831,\"start\":71823},{\"end\":71841,\"start\":71831},{\"end\":71851,\"start\":71841},{\"end\":72121,\"start\":72115},{\"end\":72129,\"start\":72121},{\"end\":72135,\"start\":72129},{\"end\":72143,\"start\":72135},{\"end\":72151,\"start\":72143},{\"end\":72446,\"start\":72440},{\"end\":72454,\"start\":72446},{\"end\":72462,\"start\":72454},{\"end\":72470,\"start\":72462},{\"end\":72786,\"start\":72780},{\"end\":72794,\"start\":72786},{\"end\":72800,\"start\":72794},{\"end\":72809,\"start\":72800},{\"end\":73080,\"start\":73072},{\"end\":73086,\"start\":73080},{\"end\":73094,\"start\":73086},{\"end\":73355,\"start\":73349},{\"end\":73364,\"start\":73355},{\"end\":73623,\"start\":73617},{\"end\":73632,\"start\":73623},{\"end\":73638,\"start\":73632},{\"end\":73930,\"start\":73923},{\"end\":73937,\"start\":73930},{\"end\":73946,\"start\":73937},{\"end\":73953,\"start\":73946},{\"end\":73960,\"start\":73953},{\"end\":74306,\"start\":74298},{\"end\":74314,\"start\":74306},{\"end\":74321,\"start\":74314},{\"end\":74328,\"start\":74321},{\"end\":74597,\"start\":74591},{\"end\":74605,\"start\":74597},{\"end\":74611,\"start\":74605},{\"end\":74620,\"start\":74611},{\"end\":74628,\"start\":74620},{\"end\":74934,\"start\":74927},{\"end\":74941,\"start\":74934},{\"end\":74947,\"start\":74941},{\"end\":74956,\"start\":74947},{\"end\":74962,\"start\":74956},{\"end\":74970,\"start\":74962},{\"end\":75313,\"start\":75305},{\"end\":75321,\"start\":75313},{\"end\":75327,\"start\":75321},{\"end\":75625,\"start\":75619},{\"end\":75631,\"start\":75625},{\"end\":75640,\"start\":75631},{\"end\":75646,\"start\":75640},{\"end\":75655,\"start\":75646},{\"end\":75963,\"start\":75957},{\"end\":75972,\"start\":75963},{\"end\":75980,\"start\":75972},{\"end\":75989,\"start\":75980},{\"end\":75995,\"start\":75989},{\"end\":76386,\"start\":76379},{\"end\":76393,\"start\":76386},{\"end\":76402,\"start\":76393},{\"end\":76408,\"start\":76402},{\"end\":76415,\"start\":76408},{\"end\":76424,\"start\":76415},{\"end\":76431,\"start\":76424},{\"end\":76893,\"start\":76885},{\"end\":76900,\"start\":76893},{\"end\":76909,\"start\":76900},{\"end\":76916,\"start\":76909},{\"end\":76923,\"start\":76916},{\"end\":76931,\"start\":76923},{\"end\":77305,\"start\":77299},{\"end\":77313,\"start\":77305},{\"end\":77320,\"start\":77313},{\"end\":77329,\"start\":77320},{\"end\":77336,\"start\":77329},{\"end\":77342,\"start\":77336},{\"end\":77663,\"start\":77655},{\"end\":77669,\"start\":77663},{\"end\":77676,\"start\":77669},{\"end\":77913,\"start\":77907},{\"end\":77920,\"start\":77913},{\"end\":77926,\"start\":77920},{\"end\":77934,\"start\":77926},{\"end\":77942,\"start\":77934},{\"end\":78274,\"start\":78268},{\"end\":78285,\"start\":78274},{\"end\":78292,\"start\":78285},{\"end\":78299,\"start\":78292},{\"end\":78641,\"start\":78635},{\"end\":78650,\"start\":78641},{\"end\":78661,\"start\":78650},{\"end\":79004,\"start\":78998},{\"end\":79013,\"start\":79004},{\"end\":79024,\"start\":79013},{\"end\":79273,\"start\":79267},{\"end\":79281,\"start\":79273},{\"end\":79287,\"start\":79281},{\"end\":79561,\"start\":79555},{\"end\":79569,\"start\":79561},{\"end\":79578,\"start\":79569},{\"end\":79584,\"start\":79578},{\"end\":79853,\"start\":79845},{\"end\":79859,\"start\":79853},{\"end\":79868,\"start\":79859},{\"end\":79875,\"start\":79868},{\"end\":79884,\"start\":79875},{\"end\":79890,\"start\":79884},{\"end\":80179,\"start\":80171},{\"end\":80187,\"start\":80179},{\"end\":80470,\"start\":80462},{\"end\":80478,\"start\":80470},{\"end\":80487,\"start\":80478},{\"end\":80496,\"start\":80487},{\"end\":80502,\"start\":80496},{\"end\":80797,\"start\":80789},{\"end\":80804,\"start\":80797},{\"end\":80813,\"start\":80804},{\"end\":80822,\"start\":80813},{\"end\":80829,\"start\":80822},{\"end\":81125,\"start\":81118},{\"end\":81132,\"start\":81125},{\"end\":81139,\"start\":81132},{\"end\":81146,\"start\":81139},{\"end\":81540,\"start\":81531},{\"end\":81546,\"start\":81540},{\"end\":81554,\"start\":81546},{\"end\":81561,\"start\":81554},{\"end\":81567,\"start\":81561},{\"end\":81918,\"start\":81909},{\"end\":81924,\"start\":81918},{\"end\":82185,\"start\":82179},{\"end\":82191,\"start\":82185},{\"end\":82197,\"start\":82191},{\"end\":82206,\"start\":82197},{\"end\":82213,\"start\":82206},{\"end\":82471,\"start\":82465},{\"end\":82477,\"start\":82471},{\"end\":82486,\"start\":82477},{\"end\":82493,\"start\":82486},{\"end\":82501,\"start\":82493},{\"end\":82811,\"start\":82802},{\"end\":82818,\"start\":82811},{\"end\":82825,\"start\":82818},{\"end\":82832,\"start\":82825},{\"end\":82840,\"start\":82832},{\"end\":82849,\"start\":82840},{\"end\":83131,\"start\":83124},{\"end\":83139,\"start\":83131},{\"end\":83145,\"start\":83139},{\"end\":83152,\"start\":83145},{\"end\":83383,\"start\":83377},{\"end\":83389,\"start\":83383},{\"end\":83395,\"start\":83389},{\"end\":83406,\"start\":83395},{\"end\":83567,\"start\":83561},{\"end\":83585,\"start\":83567},{\"end\":83592,\"start\":83585},{\"end\":83603,\"start\":83592},{\"end\":83786,\"start\":83778},{\"end\":83793,\"start\":83786},{\"end\":84094,\"start\":84088},{\"end\":84103,\"start\":84094},{\"end\":84109,\"start\":84103},{\"end\":84117,\"start\":84109},{\"end\":84124,\"start\":84117},{\"end\":84130,\"start\":84124},{\"end\":84139,\"start\":84130},{\"end\":84447,\"start\":84441},{\"end\":84453,\"start\":84447},{\"end\":84462,\"start\":84453},{\"end\":84469,\"start\":84462},{\"end\":84481,\"start\":84469},{\"end\":84839,\"start\":84833},{\"end\":84846,\"start\":84839},{\"end\":84852,\"start\":84846},{\"end\":84860,\"start\":84852},{\"end\":84868,\"start\":84860},{\"end\":85215,\"start\":85207},{\"end\":85221,\"start\":85215},{\"end\":85230,\"start\":85221},{\"end\":85236,\"start\":85230},{\"end\":85244,\"start\":85236},{\"end\":85583,\"start\":85574},{\"end\":85591,\"start\":85583},{\"end\":85599,\"start\":85591},{\"end\":85605,\"start\":85599},{\"end\":85944,\"start\":85937},{\"end\":85951,\"start\":85944},{\"end\":85957,\"start\":85951},{\"end\":85966,\"start\":85957},{\"end\":85974,\"start\":85966},{\"end\":86299,\"start\":86292},{\"end\":86305,\"start\":86299},{\"end\":86312,\"start\":86305},{\"end\":86320,\"start\":86312},{\"end\":86645,\"start\":86639},{\"end\":86651,\"start\":86645},{\"end\":86659,\"start\":86651},{\"end\":86665,\"start\":86659},{\"end\":86672,\"start\":86665},{\"end\":87050,\"start\":87041},{\"end\":87064,\"start\":87050},{\"end\":87405,\"start\":87393},{\"end\":87415,\"start\":87405},{\"end\":87424,\"start\":87415},{\"end\":87432,\"start\":87424},{\"end\":87441,\"start\":87432},{\"end\":87451,\"start\":87441},{\"end\":87742,\"start\":87732},{\"end\":87751,\"start\":87742},{\"end\":87992,\"start\":87984},{\"end\":88008,\"start\":87992},{\"end\":88019,\"start\":88008},{\"end\":88233,\"start\":88221},{\"end\":88244,\"start\":88233},{\"end\":88257,\"start\":88244},{\"end\":88268,\"start\":88257},{\"end\":88278,\"start\":88268},{\"end\":88546,\"start\":88539},{\"end\":88553,\"start\":88546},{\"end\":88560,\"start\":88553},{\"end\":88568,\"start\":88560},{\"end\":88576,\"start\":88568},{\"end\":88938,\"start\":88932},{\"end\":88944,\"start\":88938},{\"end\":88950,\"start\":88944},{\"end\":89250,\"start\":89234},{\"end\":89258,\"start\":89250},{\"end\":89271,\"start\":89258},{\"end\":89281,\"start\":89271},{\"end\":89292,\"start\":89281},{\"end\":89720,\"start\":89712},{\"end\":89730,\"start\":89720},{\"end\":89739,\"start\":89730},{\"end\":89752,\"start\":89739},{\"end\":89764,\"start\":89752},{\"end\":90185,\"start\":90177},{\"end\":90192,\"start\":90185},{\"end\":90199,\"start\":90192},{\"end\":90472,\"start\":90464},{\"end\":90480,\"start\":90472},{\"end\":90489,\"start\":90480},{\"end\":90497,\"start\":90489},{\"end\":90504,\"start\":90497},{\"end\":90512,\"start\":90504},{\"end\":90841,\"start\":90834},{\"end\":90860,\"start\":90841},{\"end\":90872,\"start\":90860},{\"end\":90884,\"start\":90872},{\"end\":90896,\"start\":90884},{\"end\":90907,\"start\":90896},{\"end\":90917,\"start\":90907},{\"end\":91233,\"start\":91225},{\"end\":91246,\"start\":91233},{\"end\":91254,\"start\":91246},{\"end\":91266,\"start\":91254},{\"end\":91659,\"start\":91653},{\"end\":91668,\"start\":91659},{\"end\":91675,\"start\":91668},{\"end\":91682,\"start\":91675},{\"end\":91693,\"start\":91682},{\"end\":92031,\"start\":92023},{\"end\":92039,\"start\":92031},{\"end\":92045,\"start\":92039},{\"end\":92053,\"start\":92045},{\"end\":92062,\"start\":92053},{\"end\":92071,\"start\":92062},{\"end\":92472,\"start\":92461},{\"end\":92479,\"start\":92472},{\"end\":92487,\"start\":92479},{\"end\":92768,\"start\":92760},{\"end\":92779,\"start\":92768},{\"end\":92791,\"start\":92779},{\"end\":92807,\"start\":92791},{\"end\":93172,\"start\":93162},{\"end\":93183,\"start\":93172},{\"end\":93197,\"start\":93183},{\"end\":93576,\"start\":93566},{\"end\":93585,\"start\":93576},{\"end\":93594,\"start\":93585},{\"end\":93603,\"start\":93594},{\"end\":93615,\"start\":93603},{\"end\":93625,\"start\":93615},{\"end\":93636,\"start\":93625},{\"end\":93643,\"start\":93636},{\"end\":93657,\"start\":93643},{\"end\":93667,\"start\":93657},{\"end\":94017,\"start\":94007},{\"end\":94030,\"start\":94017},{\"end\":94041,\"start\":94030}]", "bib_venue": "[{\"end\":68675,\"start\":68637},{\"end\":68950,\"start\":68939},{\"end\":69232,\"start\":69169},{\"end\":69567,\"start\":69556},{\"end\":69809,\"start\":69795},{\"end\":70106,\"start\":70067},{\"end\":70504,\"start\":70493},{\"end\":70890,\"start\":70855},{\"end\":71274,\"start\":71266},{\"end\":71586,\"start\":71561},{\"end\":71879,\"start\":71851},{\"end\":72173,\"start\":72151},{\"end\":72492,\"start\":72470},{\"end\":72820,\"start\":72809},{\"end\":73114,\"start\":73094},{\"end\":73389,\"start\":73364},{\"end\":73665,\"start\":73638},{\"end\":74000,\"start\":73960},{\"end\":74339,\"start\":74328},{\"end\":74653,\"start\":74628},{\"end\":74995,\"start\":74970},{\"end\":75338,\"start\":75327},{\"end\":75666,\"start\":75655},{\"end\":76020,\"start\":75995},{\"end\":76477,\"start\":76431},{\"end\":76971,\"start\":76931},{\"end\":77367,\"start\":77342},{\"end\":77696,\"start\":77676},{\"end\":77967,\"start\":77942},{\"end\":78314,\"start\":78299},{\"end\":78686,\"start\":78661},{\"end\":79035,\"start\":79024},{\"end\":79312,\"start\":79287},{\"end\":79600,\"start\":79584},{\"end\":79905,\"start\":79890},{\"end\":80207,\"start\":80187},{\"end\":80513,\"start\":80502},{\"end\":80840,\"start\":80829},{\"end\":81182,\"start\":81146},{\"end\":81582,\"start\":81567},{\"end\":81943,\"start\":81924},{\"end\":82228,\"start\":82213},{\"end\":82539,\"start\":82501},{\"end\":82860,\"start\":82849},{\"end\":83163,\"start\":83152},{\"end\":83375,\"start\":83306},{\"end\":83559,\"start\":83535},{\"end\":83822,\"start\":83793},{\"end\":84150,\"start\":84139},{\"end\":84506,\"start\":84481},{\"end\":84888,\"start\":84868},{\"end\":85264,\"start\":85244},{\"end\":85632,\"start\":85605},{\"end\":85999,\"start\":85974},{\"end\":86348,\"start\":86320},{\"end\":86718,\"start\":86672},{\"end\":87110,\"start\":87064},{\"end\":87475,\"start\":87451},{\"end\":87788,\"start\":87751},{\"end\":88029,\"start\":88019},{\"end\":88285,\"start\":88278},{\"end\":88614,\"start\":88576},{\"end\":88975,\"start\":88950},{\"end\":89338,\"start\":89292},{\"end\":89810,\"start\":89764},{\"end\":90214,\"start\":90199},{\"end\":90544,\"start\":90512},{\"end\":90832,\"start\":90739},{\"end\":91312,\"start\":91266},{\"end\":91731,\"start\":91693},{\"end\":92117,\"start\":92071},{\"end\":92502,\"start\":92487},{\"end\":92832,\"start\":92807},{\"end\":93225,\"start\":93197},{\"end\":93671,\"start\":93667},{\"end\":94087,\"start\":94041},{\"end\":94501,\"start\":94322},{\"end\":95138,\"start\":94964},{\"end\":69291,\"start\":69234},{\"end\":70141,\"start\":70108},{\"end\":76519,\"start\":76479},{\"end\":81214,\"start\":81184},{\"end\":81593,\"start\":81584},{\"end\":82239,\"start\":82230},{\"end\":83847,\"start\":83824},{\"end\":86760,\"start\":86720},{\"end\":87152,\"start\":87112},{\"end\":88035,\"start\":88031},{\"end\":89380,\"start\":89340},{\"end\":89852,\"start\":89812},{\"end\":90225,\"start\":90216},{\"end\":91354,\"start\":91314},{\"end\":92159,\"start\":92119},{\"end\":92513,\"start\":92504},{\"end\":93249,\"start\":93227},{\"end\":93677,\"start\":93673},{\"end\":94129,\"start\":94089}]"}}}, "year": 2023, "month": 12, "day": 17}