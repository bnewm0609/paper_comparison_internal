{"id": 233740054, "updated": "2023-10-06 04:25:23.804", "metadata": {"title": "Encoder Fusion Network with Co-Attention Embedding for Referring Image Segmentation", "authors": "[{\"first\":\"Guang\",\"last\":\"Feng\",\"middle\":[]},{\"first\":\"Zhiwei\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Lihe\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Huchuan\",\"last\":\"Lu\",\"middle\":[]}]", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2021, "month": 5, "day": 5}, "abstract": "Recently, referring image segmentation has aroused widespread interest. Previous methods perform the multi-modal fusion between language and vision at the decoding side of the network. And, linguistic feature interacts with visual feature of each scale separately, which ignores the continuous guidance of language to multi-scale visual features. In this work, we propose an encoder fusion network (EFN), which transforms the visual encoder into a multi-modal feature learning network, and uses language to refine the multi-modal features progressively. Moreover, a co-attention mechanism is embedded in the EFN to realize the parallel update of multi-modal features, which can promote the consistent of the cross-modal information representation in the semantic space. Finally, we propose a boundary enhancement module (BEM) to make the network pay more attention to the fine structure. The experiment results on four benchmark datasets demonstrate that the proposed approach achieves the state-of-the-art performance under different evaluation metrics without any post-processing.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2105.01839", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/FengHZL21", "doi": "10.1109/cvpr46437.2021.01525"}}, "content": {"source": {"pdf_hash": "102cbf1eb78b2c0bdb998ac514dcbae27143c85a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2105.01839v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2105.01839", "status": "GREEN"}}, "grobid": {"id": "5eecd021af5ec9c0f0e2715ba280a53da4ffa65e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/102cbf1eb78b2c0bdb998ac514dcbae27143c85a.txt", "contents": "\nEncoder Fusion Network with Co-Attention Embedding for Referring Image Segmentation\n\n\nGuang Feng fengguang.gg@gmail.com \nSchool of Information and Communication Engineering\nDalian University of Technology\n\n\nNingbo Institute\nDalian University of Technology\n\n\nZhiwei Hu \nSchool of Information and Communication Engineering\nDalian University of Technology\n\n\nNingbo Institute\nDalian University of Technology\n\n\nLihe Zhang zhanglihe@dlut.edu.cn \nSchool of Information and Communication Engineering\nDalian University of Technology\n\n\nNingbo Institute\nDalian University of Technology\n\n\nHuchuan Lu \nSchool of Information and Communication Engineering\nDalian University of Technology\n\n\nNingbo Institute\nDalian University of Technology\n\n\nEncoder Fusion Network with Co-Attention Embedding for Referring Image Segmentation\n\nRecently, referring image segmentation has aroused widespread interest. Previous methods perform the multimodal fusion between language and vision at the decoding side of the network. And, linguistic feature interacts with visual feature of each scale separately, which ignores the continuous guidance of language to multi-scale visual features. In this work, we propose an encoder fusion network (EFN), which transforms the visual encoder into a multimodal feature learning network, and uses language to refine the multi-modal features progressively. Moreover, a coattention mechanism is embedded in the EFN to realize the parallel update of multi-modal features, which can promote the consistent of the cross-modal information representation in the semantic space. Finally, we propose a boundary enhancement module (BEM) to make the network pay more attention to the fine structure. The experiment results on four benchmark datasets demonstrate that the proposed approach achieves the state-of-the-art performance under different evaluation metrics without any post-processing.\n\nIntroduction\n\nReferring image segmentation aims to extract the most relevant visual region (object or stuff) in an image based on the referring expression. Unlike the traditional semantic and instance segmentation, which require to correctly segment each semantic category or each object in an image, referring image segmentation needs to find a certain part of the image according to the understanding of the given language query. Therefore, it can be regarded as a pixel-wise foreground/background segmentation problem, and the output result is not limited by the predefined semantic categories or object classes. This task has a wide range of potential applications in language-based human-robot interaction. \u2020 Corresponding Author  The key of this task is to realize the cross-modal matching between visual and linguistic features. The deeplearning community has rapidly improved the results of vision-language tasks over a short period of time. The rapid development of convolutional neural network (CNN) and recurrent neural network (RNN) have made a qualitative leap in the ability of understanding vision and language, thereby they can solve more complex pixel-level cross-modal prediction tasks. Early referring image segmentation methods [14,26,23,33] mainly rely on the powerful learning ability of deep learning model. They directly concatenate linguistic features with visual features of each region, and then use the combined multi-modal features to generate the segmentation mask. Due to the lack of sufficient interaction between two modalities, such solutions can not meet the requirements of real-world applications. Recently, some works [36,38,1,16,17,19] began to consider the linguistic and visual attention mechanisms to better aggregate these two kinds of features.\n\nAlthough some referring image segmentation methods have been proposed in the last few years, there are still many problems that have not been explored. On the one hand, for the cross-modal fusion of vision and language. Previ-ous methods usually adopt the decoder fusion strategy, in which the RGB image and the referring expression are fed into CNN or RNN to generate their own feature representations separately, and then fuse these features in the decoding stage. However, this fusion strategy at the output side of the network either only considers the interaction between linguistic and highest-level visual features [26,23] or combines the linguistic features with the visual features of each level independently (as shown in Fig. 1 (a)) [38,16,19]. They do not investigate the deep guidance of language to multi-modal fused features. Besides, some works utilize visual and linguistic attention mechanisms for cross-modal feature matching. But they update the linguistic and visual features in a serial mode [36,1,16,17,19], that is, they only update the feature of one modality at a specific time, which will lead to the update delay of the features between different modalities and eventually weaken the consistency of the representation of multi-modal information. On the other hand, in CNNs, the repeated stride and pooling operations may lead to the loss of some important fine-structure information, but few referring image segmentation methods explicitly consider the problem of detail recovery.\n\nTo resolve the aforementioned problems, we propose an encoder fusion network with co-attention embedding (CEFNet) for referring image segmentation. Instead of the cross-modal information fusion at the output side, we adopt the encoder fusion strategy for the first time to progressively guide the multi-level cross-modal features by language. The original visual feature encoder (e.g., ResNet) is transformed into a multi-modal feature encoder (as shown in Fig. 1 (b)). The features of two modalities are deeply interleaved in the CNN encoder. Furthermore, to effectively play the guiding role of language, we adopt the co-attention mechanism to simultaneously update the features of different modalities. It utilizes the same affinity matrix to project different features to the common feature subspace in a parallel mode and better achieve the cross-modal matching to bridge the gap between coarse-grained referring expression and highly localized visual segmentation. We implement two simple and effective co-attention mechanisms such as vanilla co-attention and asymmetric co-attention, which offer a more insightful glimpse into the task of referring image segmentation. Finally, we design a boundary enhancement module (BEM), which captures and exploits boundary cues as guidance to gradually recover the details of the targeted region in the decoding stage of the network.\n\nOur main contributions are as follows:\n\n\u2022 We propose an encoder fusion network (EFN) that uses language to guide the multi-modal feature learning, thereby realizing deep interweaving between multimodal features. In the EFN, the co-attention mechanism is embedded to guarantee the semantic alignment of different modalities, which promotes the represen-tation ability of the language-targeted visual features.\n\n\u2022 We introduce a boundary enhancement module (BEM) to emphasize the attention of the network to the contour representation, which can help the network to gradually recover the finer details.\n\n\u2022 The proposed method achieves the state-of-the-art performance on four large-scale datasets including the UNC, UNC+, Google-Ref and ReferIt with the speed of 50 FPS on an Nvidia GTX 1080Ti GPU.\n\n\nRelated Work\n\nSemantic and Instance Segmentation. The former aims at grouping pixels in a semantically meaningful way without differentiating each instance. The latter requires to separate all instances of objects rather than the stuff. In recent years, many semantic segmentation methods adopt fully convolutional network (FCN) [29] for end-to-end prediction. On this basis, the multi-scale context [43,3,4,8,10] and attention mechanisms [44,11,45,18] are deeply examined. Some works [34,8] leverage the encoder-decoder structures to alleviate the loss of details caused by continuous downsampling. Also, RGB-D based methods [12,5] introduce depth prior to improve the performance. These methods provide inspirations for referring image segmentation.\n\nIn instance segmentation, Mask-RCNN [13] is a classical framework, which uses two-stage design to sequentially generates proposals and classify/segment them. In the follow-up works, feature pyramid [25], top-down and bottom-up [28], iterative optimization [2] and boundaryaware mechanisms [7] are explored. The success of boundary refinement strategy provides us with an important clue to solve the problem of referring image segmentation. Referring Image Comprehension. This task has two branches: localization and segmentation. For referring image localization, previous methods are mainly composed of two separate stages. They firstly use object detector to extract candidate regions, and then rank these regions according to the referring expression. Pioneering methods [15,32,31] use the CNN-LSTM structure to select the object with the maximum posterior probability of the expression, and other works [27,41] optimize the joint probability of the target object and the expression. Recently, some methods [37,35,24] use a one-stage framework. Instead of generating excessive candidate boxes, they directly predict the coordinates of the targeted region in an end-toend manner. The above methods all implement the multimodal fusion in the decoder.\n\nFor referring image segmentation, early methods [14,26,23,33] directly concatenate language and visual features and then completely depend on a fully convolutional network to infer the pixel-wise mask. These methods do  Figure 2: The overall architecture of our model. It mainly consists of the Bi-GRU encoder, ResNet-101 encoder (E1 \u223c E5), co-attention module (CAM), decoder blocks (D1 \u223c D5) and boundary enhancement module (BEM) assembled at the decoding end. The CAM is used to realize the matching between multi-modal features. The BEM captures the boundary cues and uses them to recover the details of the image, which produces a more accurate segmentation mask. The details of the proposed method are introduced in Sec. 3 not explicitly formulate the intra-modal and inter-modal relationships. Some recent works [36,38,1,16,17,19] consider the self-attention and cross-attention mechanisms of linguistic and visual information. For example, Shi et al. [36] adapt the vision-guided linguistic attention to learn the adaptive linguistic context of each visual region. Ye et al. [38] employ multiple non-local modules to update each pixel-word mixed features in a fully-connected manner. Hu et al. [16] design a bi-directional relationship inferring network to model the relationship between language and vision, which realizes the serial mutual guidance between multi-modal features. Huang et al. [17] firstly perceives all the entities in the image according to the entity and attribute words, and then use the relational words to model the relationships of all entities. LSCM [19] utilizes word graph based on dependency parsing tree to guide the learning of multi-modal context. Similarly, these methods also use the decoder fusion strategy. In addition, they do not update linguistic and visual features in parallel, which may weaken the consistency of language and vision in the semantic space. Different from the previous works, we design a parallel update mechanism to enhance the compatibility of multi-modal representation, and the multi-modal feature matching is performed in the encoder. We also propose a boundary enhancement module to guide the progressive fusion of multi-level features in the decoding stage.\nSTN STN C C C C C C C C C Spatial\n\nProposed Method\n\nThe overall architecture of the proposed method is illustrated in Fig. 2. In this section, we mainly introduce the co-attention based encoder fusion network and the boundary enhanced decoder network.\n\n\nEncoder Fusion with Co-Attention\n\nEncoder fusion network. For a input image, we use ResNet101 [42] to extract visual features. The ResNet101 is composed of five basic blocks: conv1, res2, res3, res4, and res5. The feature maps from these five blocks are represented as\n{E i } 5 i=1 .\nTo avoid losing excessive spatial details, the stride of the last block is set to 1. Unlike previous method that performs multi-modal fusion in the decoder, we insert language features after res3, res4, and res5 respectively. ResNet is converted into a multi-modal feature extractor. This design takes full advantage of the data fitting ability of deep CNN model and realizes the deep interleaving of cross-modal features. The experimental comparison between encoder fusion network (EFN) and decoder fusion network (DFN) is implemented, and the results on the UNC dataset are shown in Tab. 3. Multi-modal feature representation. For a given expression, we feed the word embeddings {e t } T t=1 into the Bi-GRU to generate the linguistic context {h t } T t=1 , where T represents the length of the language. Besides, we adopt a simple concatenation strategy to generate the initial multimodal feature and denote it as:\nm p = w[e p i , h T , s p i ],(1)\nwhere e p i is a feature vector of E i at position p. s p i represents 8-D spatial coordinates, which follows the design in [16]. w is the learnable parameters. Then we use m p to calculate the position-specific linguistic context l p :\n\u03b1 p,t = m p \u00b7 e t , l p = T t=1 e t \u00b7 exp(\u03b1 p,t ) T t=1 exp(\u03b1 p,t )\n.\n\n(2) . Their affinity matrix A \u2208 R HW \u00d7HW is calculated as follows:\nA = (W m M) (W l L),(3)\nwhere W m , W l \u2208 R C1\u00d7C are the learnable parameters. The element a i,j of A represents the similarity between the i th position of M and the j th position of L. Then, we use the softmax function to normalize the similarity matrix as follows:\nA 1 = softmax(A), A 2 = softmax(A ),(4)\nwhere A 1 and A 2 are the results of the row-wise and column-wise normalization, respectively. Thus, the feature maps M and L can be updated through weighted summing:\nM = MA 1 , L = LA 2 .(5)\nWe concatenate M and L along channel dimension and follow with a 3\u00d73 convolution to get the multi-modal features F \u2208 R C2\u00d7H\u00d7W . The F is normalized and added to the encoder feature E. Thus, the embedding of the multi-modal feature in the encoder is finished. This mechanism can provide extra complementary cues according to the information of the other modality to implement the mutual guidance between these two modalities. Fig. 3 (a) shows the detailed structure of vanilla co-attention module (VCM). Asymmetric co-attention. Furthermore, we propose an asymmetric co-attention module (ACM) to reduce the computational cost. Inspired by [45], we employ pyramid pooling module (PPM) to sample the feature maps M and L.\n\nThe PPM is composed of four-scale feature bins, which are then flattened and concatenated to form a matrix of size C 1 \u00d7N , N HW . Here, the sizes of the feature bins are set to 1\u00d71, 3\u00d73, 6\u00d76 and 8\u00d78, respectively. Thus, the self-affinity matrixes of M and L can be calculated as:\nSA m = (PPM(W 1 m M)) (W 2 m M), SA l = (PPM(W 1 l L)) (W 2 l L),(6)\nwhere SA m and SA l denote the modality-specific similarity matrixes. Their sizes are fixed to N \u00d7(HW ) through the PPM, which is asymmetric. W 1 m , W 2 m , W 1 l and W 2 l indicate the learnable parameters. We further combine these two matrices as follows:\nA 3 = softmax((SA m + SA l ) ).(7)\nThen, the row-wise normalized matrix A 3 \u2208 R (HW )\u00d7N is used to assist the update of multi-modal features:\nM = A 3 (PPM(W 3 m M)) , L = A 3 (PPM(W 3 l L)) .(8)\nSimilarly to the vanilla co-attention, M and L is concatenated to generate the final multi-modal output. The whole structure of ACM is shown in Fig. 3 (b).\n\n\nBoundary Enhancement Module\n\nIn CNNs, the repeated stride and pooling operations lead to the loss of fine structure information, which may blur the contour of the predicted region. Previous works [38,1,16,17,19] do not explicitly consider the restoration of details when performing multi-scale fusion in decoder. In this work, we design a boundary enhancement module (BEM), which uses boundary features as a guidance to make the network attend to finer details and realize the progressive refinement of the prediction. Its structure is shown in Fig. 2. Specifically, for the decoder features {D i } 5 i=1 , we first compute the boundary-aware features:\nB i = S i \u2212 STN(S i ),(9)\nwhere STN represents a spatial transformer networks [20].\n\nHere, we utilize it to sample the high-level abstract semantic information from S i . Thus, the residual B i describes the fine structure. The prediction process of the boundary map can be written as:\nB i\u22121 = Conv(Cat(B i , D i\u22121 )), BM i\u22121 = Sig(Conv( B i\u22121 )),(10)\nwhere Cat(\u00b7, \u00b7) is the concatenation operation along the channel axis. Conv and Sig denote the convolutional layer and sigmoid function, respectively. BM i\u22121 is supervised by the ground-truth contour of the targeted region. Next, we exploit boundary feature B i\u22121 to refine the segmentation mask as follows:\nS i\u22121 = Conv(Cat( B i\u22121 + STN(S i ), S i )), SM i\u22121 = Sig(Conv(S i\u22121 )),(11)\nwhere S i\u22121 actually combines the information of decoder features D i and D i\u22121 . SM i\u22121 denotes the refined mask, which is supervised by the ground-truth segmentation. The SM 1 from the last decoder block is taken as the final prediction map, as illustrated in Fig. 2.\n\n\nExperiments\n\n\nDatasets\n\nTo verify the effectiveness of the proposed method, we evaluate the performance on four datasets, which are the UNC [40], UNC+ [40], Google-Ref [32] and ReferIt [21].\n\nUNC: It contains 19,994 images with 142,209 language expressions for 50,000 segmented object regions. These data are selected from the MS COCO dataset using a twoplayer game [21]. There are multiple objects with the same category in each image. UNC+: It is also a subset of the MS COCO, which contains 141,564 language expressions for 49,856 objects in 19,992 images. However, the referring expression does not contain the words that indicate location information, which means that the matching of their language and visual region totally depend on the appearance information.\n\nGoogle-Ref: It includes 104,560 referring expressions for 54,822 objects in 26,711 images. The annotations are based on Mechanical Turk instead of using a two-player game. The average length of referring expressions in this dataset is 8.43 words.\n\nReferIt: It is collected from the IAPR TC-12 [9]. It is composed of 130,525 referring expressions for 96,654 object regions in 19,894 natural images. In addition, their annotations contain objects or stuff, and the expressions are usually shorter and more succinct than the other datasets.\n\n\nImplementation Details\n\nThe proposed framework is built on the public pytorch toolbox and is trained on an Nvidia GTX 1080Ti GPU for 200,000 iterations. Our network is trained by an end-toend strategy and using the SGD optimizer with an initial learning rate of 0.00075 and divided by 10 after 100,000 iterations. All input images are resized to 320\u00d7320. The weight decay and batch size are 0.0005 and 12, respectively. And when training G-ref, we use the UNC model as a pretraining model to avoid over-fitting. During the inference phase, the prediction map is resized to the same resolution as the original image. The binary cross entropy loss is used to supervise the boundary map and segmentation map, In addition, we also use the ground-truth segmentation (GT) to supervise the output of the STN.\n\nEvaluation Metrics: Following previous works [16,17,19], we employ Overall Intersection-over-Union (Overall IoU) and Prec@X to evaluate the segmentation accuracy. The Overall IoU metric represents the ratio of the total intersection regions and the total union regions between the predicted mask and the ground truth for all the test samples. The Prec@X metric calculates the percentage of the IoU score of the prediction mask in the test set that exceeds the threshold X, where X \u2208 {0.5, 0.6, 0.7, 0.8, 0.9}.\n\n\nPerformance Comparison\n\nTo verify the effectiveness of the proposed model, we compare it with thirteen methods, which are the LSTM-CNN [14], RMI [26], DMN [33], KWA [36], RRN [23], MAttNet [39], lang2seg [6], CMSA [38], STEP [1], CGAN [30], BRINet [16], LSCM [19], and CMPC [17].\n\nPerformance Evaluation: Tab. 1 shows the performance (IoU) comparison of different methods on four datasets, in which Our(VCM) and Our(ACM) represent the results of using vanilla co-attention module and asymmetric co-attention module, respectively. The proposed model consistently outperforms these competitors on most datasets except the UNC+ testB. Some methods like LSCM and  Fig. 4. It can be seen that our method can accurately segment the specific regions (object or stuff) according to the query expression. Following [26,16], we analyze the relationship between language length and segmentation accu-racy. The results are demonstrated in Tab. 2, which indicate that our method achieves the state-of-the-art performance.\n\n\nRuntime and Memory Statistics:\n\nWe implement all the tests on a NVIDIA GTX 1080 Ti GPU. The comparison of running time is reported in Tab. 4. Our method runs the fastest with a speed of 50 FPS. The GPU memory usage is shown in Tab. 5. From Tab. 4 and Tab. 5, we can find that although the VCM has advantages in speed, the large input size causes the memory usage to sharply increase. On the contrary, the VCM is not sensitive to the input size. Therefore, it is widely applicable.  \n\n\nAblation Study\n\nwe conduct a series of experiments on the UNC dataset to verify the benefit of each component.\n\nComparison of DFN and EFN: We first remove the coattention module and boundary enhancement module from the CEFNet in Fig. 2. Then, we achieve the multi-modal fusion in the encoder by Eq. (1), and the decoder adopts the FPN [25] structure. This network is taken as the baseline network (EFN) of encoder fusion. In addition, similar to previous works, we realize the multi-modal fusion in the FPN decoder by Eq. (1) to build the baseline (DFN) for decoder fusion. We evaluate the two baselines in Tab. 3, from which we can see that EFN is significantly better than DFN. With the help of ResNet, the encoder fusion strategy achieves more powerful feature coding without increasing additional computational burden.\n\nEffectiveness of Co-Attention: We evaluate the performance of the vanilla co-attention module (VCM) and asymmetric co-attention module(ACM). Compared with the baseline EFN, the VCM brings 6.8%, 6.9% and 8.2% IoU improvement on the UNC-val, UNC-testA, and UNC-testB, respectively. Similarly, the ACM achieves the gain of 7.6%, 7.6% and 8.8% on the same datasets, respectively. The ACM performs slightly better than the VCM. We attribute it to the modality-specific affinity learning, which focuses on important regions within the modality and achieves better contextual understanding of the modality itself. It is conducive to cross-modal alignment in the next stage.\n\nEffectiveness of BEM: Tab. 3 presents the ablation results of boundary enhancement module (BEM), which show that the special consideration of boundary refinement can significantly improve the performance. BEM can bring about 2%\u223c3% performance improvement (Overall IoU)) to the final prediction result. Some visual results in Fig. 5 demonstrate the benefits of BEM. These figures show that the prediction mask can fit the object boundary more closely after the refinement of BEM.\n\n\nFailure Cases\n\nWe visualize some interesting failure cases in Fig. 6. One type of failure occurs when queries are ambiguous. For example, for the top left example, the misspelling of a word (roght \u2192 right) causes part of the semantics of the sen-   tence to be lost. Also, for the top right example, there are two horse butts on the left. Another case is that when the query contains low-frequency or new words (e.g. in the bottom left example, cop rarely appears in the training data), our method sometimes fails to segment out the core region accurately. This problem may be alleviated by using one/zero-shot learning. Finally, we observed that sometimes small objects cannot be segmented completely (the bottom right example). This phenomenon can be alleviated by enlarging the scale of input images. Fortunately, the ACM is insensitive to the size (Tab. 5 for details). Through the analysis of successful (Fig. 4) and failure cases, we think that the co-attention module can learn the high-order cross-modal relationships even in some complicated semantic scenarios. It enables the network to pay more attention to the correlated, informative regions, and produce discriminative foreground features.\n\n\nConclusion\n\nIn this paper, we propose an encoder fusion network with co-attention embedding (CEFNet) to fuse multi-modal information for referring image segmentation. Compared with the decoder fusion strategy, our strategy adequately utilizes language to guide multi-model feature learning without increasing computational complexity. The designed co-attention module can promote the matching between multi-modal features and strengthen their targeting ability. Moreover, a boundary enhancement module is equipped to make the network pay more attention to the details. Extensive evaluations on four datasets demonstrate that the proposed approach outperforms previous state-ofthe-art methods both in performance and speed. In future, we can extend our co-attention module to the one-stage grounding to promote the integration of language and vision.\n\n\na). Decoder fusion for referring image segmentation (b). Encoder fusion for referring image segmentation Query: child in red Query: child in red Visual feature Multi-modal feature Linguistic progressive guidance\n\nFigure 1 :\n1Two multi-modal fusion mechanisms. Existing methods achieve the fusion between language and vision in the decoder, while the proposed method does it in the encoder.\n\nFigure 3 :\n3Two co-attention modules. M: Initial multi-modal features. L: Adaptive linguistic context. S: Softmax. PPM: Pyramid pooling module. C : Concatenation. \u00d7 : Matrix multiplication. + : Element-wise summation. C, H and W are channel number, height and width of feature maps, respectively. l p treats each word differently. It can suppress the noise in the language expression and highlight the desired region. Next, the feature maps M = [m p ] and L = [l p ] go through the co-attention module to achieve the multi-modal fusion. Vanilla co-attention. We design a co-attention scheme, which can model the dependencies between multi-modal features and project the multi-modal features to the common feature subspace. For the convenience of description, the size of M is defined as C \u00d7 H \u00d7 W , where H, W and C represent its height, width and channel number, respectively. The feature L has the same dimension as the M. At first, the features M and L are flattened into matrix representations with size C \u00d7 (HW )\n\nFigure 5 :\n5Query: \"tall suitcase\" Query: \"chair on right man with white shirt sitting in it\" Visual examples of the proposed modules.\n\nFigure 6 :\n6Query: \"second guy roght\"Query: \"left brown horse butt\" \"the grass to the left of Query: \"middle cop\" the red shirt green pant man\" Visual examples of the failure cases.\n\nTable 1 :\n1Quantitative evaluation of different methods on four datasets. -: no data available. DCRF: DenseCRF[22] post-processing.* \nReferIt \nUNC \nUNC+ \nG-Ref \ntest \nval \ntestA \ntestB \nval \ntestA \ntestB \nval \n\nLSTM-CNN16 [14] \n48.03 \n-\n-\n-\n-\n-\n-\n28.14 \nRMI+DCRF17 [26] \n58.73 \n45.18 \n45.69 \n45.57 \n29.86 \n30.48 \n29.50 \n34.52 \nDMN18 [33] \n52.81 \n49.78 \n54.83 \n45.13 \n38.88 \n44.22 \n32.29 \n36.76 \nKWA18 [36] \n59.19 \n-\n-\n-\n-\n-\n-\n36.92 \nRRN+DCRF18 [23] \n63.63 \n55.33 \n57.26 \n53.95 \n39.75 \n42.15 \n36.11 \n36.45 \nMAttNet18 [39] \n-\n56.51 \n62.37 \n51.70 \n46.67 \n52.39 \n40.08 \n-\nlang2seg19 [6] \n-\n58.90 \n61.77 \n53.81 \n-\n-\n-\n-\nCMSA+DCRF19 [38] \n63.80 \n58.32 \n60.61 \n55.09 \n43.76 \n47.60 \n37.89 \n39.98 \nSTEP19 [1] \n64.13 \n60.04 \n63.46 \n57.97 \n48.19 \n52.33 \n40.41 \n46.40 \nCGAN20 [30] \n-\n59.25 \n62.37 \n53.94 \n46.16 \n51.37 \n38.24 \n46.54 \nBRINet+DCRF20 [16] \n63.46 \n61.35 \n63.37 \n59.57 \n48.57 \n52.87 \n42.13 \n48.04 \nLSCM+DCRF20 [19] \n66.57 \n61.47 \n64.99 \n59.55 \n49.34 \n53.12 \n43.50 \n48.05 \nCMPC+DCRF20 [17] \n65.53 \n61.36 \n64.54 \n59.64 \n49.56 \n53.44 \n43.23 \n49.05 \n\nOurs(VCM) \n66.06 \n62.53 \n65.36 \n59.19 \n50.24 \n55.04 \n41.68 \n51.22 \nOurs(ACM) \n66.70 \n62.76 \n65.69 \n59.67 \n51.50 \n55.24 \n43.01 \n51.93 \nOurs coco \n\nVCM \n\n-\n69.27 \n70.56 \n66.36 \n57.46 \n61.75 \n50.66 \n57.51 \nOurs coco \n\nACM \n\n-\n68.97 \n71.13 \n66.95 \n57.48 \n61.35 \n51.97 \n57.49 \n\nImage \n\ndish in top right corner \nbowl of carrots \nwhite dish in the top \nright corner \ncarrots \nfront bowl with carrots in it \n\nImage \n\nred white shirt \npink shirt top \npink shirt guy in back \nguy in pink shirt \nwoman front row \n\nImage \n\nwater \n\nImage \n\ntrees top left corner \n\nImage \n\nbottom half of the picture \n\nFigure 4: Visual examples of referring image segmentation by our method. \n\nCMPC apply DenseCRF [22] to refine their final masks \nwhile our model does not need any post-processing. In par-\nticular, we achieve the gain of 5.9%, 3.4% and 3.9% over \nthe second best method CMPC [17] on the G-Ref, UNC+ \ntestA and val, respectively. In addition, because the UNC, \nUNC+ and G-Ref are all collected from the MS COCO \ndataset, we combine their training data into a larger train-\ning set. The results of the model trained on it are denoted as \nOurs coco \nVCM and Ours coco \nACM , which show that sufficient training \ndata can yield better results. We give some visual examples \nin \n\nTable 2 :\n2IoU for different length referring expressions on \nGoogle-Ref, UNC, UNC+ and ReferItGame. \n\nLength \n1-5 \n6-7 \n8-10 11-20 \n\nG-Ref \n\nR+LSTM [26] 32.29 28.27 27.33 26.61 \nR+RMI [26] \n35.34 31.76 30.66 30.56 \nBRINet [16] \n51.93 47.55 46.33 46.49 \nOurs(VCM) \n57.96 52.19 48.78 46.67 \nOurs(ACM) \n59.92 52.94 49.56 46.21 \n\nLength \n1-2 \n3 \n4-5 \n6-20 \n\nUNC \n\nR+LSTM [26] 43.66 40.60 33.98 24.91 \nR+RMI [26] \n44.51 41.86 35.05 25.95 \nBRINet [16] \n65.99 64.83 56.97 45.65 \nOurs(VCM) \n68.18 66.14 56.82 46.01 \nOurs(ACM) \n68.73 65.58 57.32 45.90 \n\nLength \n1-2 \n3 \n4-5 \n6-20 \n\nUNC+ \n\nR+LSTM [26] 34.40 24.04 19.31 12.30 \nR+RMI [26] \n35.72 25.41 21.73 14.37 \nBRINet [16] \n59.12 46.89 40.57 31.32 \nOurs(VCM) \n60.87 48.88 43.79 29.45 \nOurs(ACM) \n61.62 52.18 43.46 31.52 \n\nLength \n1 \n2 \n3-4 \n5-20 \n\nReferIt \n\nR+LSTM [26] 67.64 52.26 44.87 33.81 \nR+RMI [26] \n68.11 52.73 45.69 34.53 \nBRINet [16] \n75.28 62.62 56.14 44.40 \nOurs(VCM) \n77.73 66.02 59.74 45.75 \nOurs(ACM) \n78.19 66.63 60.30 46.18 \n\n\n\nTable 3 :\n3Ablation study on the UNC val, testA and testB datasets.DFN EFN VCM ACM BEM prec@0.5 prec@0.6 prec@0.7 prec@0.8 prec@0.9 overall IoUval \n\n57.30 \n50.40 \n42.00 \n28.42 \n9.00 \n52.86 \n64.16 \n58.45 \n51.16 \n37.53 \n13.39 \n55.87 \n68.61 \n63.02 \n54.55 \n40.20 \n13.67 \n59.65 \n69.22 \n64.11 \n56.33 \n41.67 \n15.32 \n60.09 \n74.07 \n68.84 \n61.76 \n48.74 \n20.06 \n62.53 \n73.95 \n69.58 \n62.59 \n49.61 \n20.63 \n62.76 \n\ntestA \n\n61.66 \n54.30 \n44.55 \n31.09 \n9.32 \n55.98 \n67.03 \n61.59 \n53.92 \n40.13 \n13.31 \n58.07 \n72.14 \n66.80 \n58.21 \n43.03 \n13.19 \n62.10 \n72.95 \n67.90 \n59.98 \n45.04 \n14.46 \n62.46 \n77.53 \n73.18 \n66.02 \n52.11 \n18.88 \n65.36 \n77.66 \n73.73 \n66.70 \n52.75 \n19.66 \n65.69 \n\ntestB \n\n52.31 \n45.48 \n37.51 \n26.85 \n10.40 \n49.66 \n58.61 \n52.62 \n45.67 \n34.56 \n15.03 \n52.48 \n64.53 \n57.96 \n50.54 \n37.94 \n16.39 \n56.76 \n65.02 \n58.33 \n50.34 \n38.74 \n16.31 \n57.09 \n69.74 \n63.75 \n56.90 \n45.20 \n22.51 \n59.19 \n69.66 \n65.14 \n58.31 \n46.18 \n22.43 \n59.67 \n\n\n\nTable 4 :\n4Runtime analysis of different methods. The time of post-processing is ignored.LSTM \nRMI \nRRN \nCMSA \nBRINet \nCMPC \nOurs(VCM) Ours(ACM) \n\nTime(ms) \n58ms \n72ms \n43ms \n79ms \n117ms \n60ms \n17ms \n20ms \n\n\n\nTable 5 :\n5GPU memory (MB) comparisons between VCM and ACM. The lower values are the better.Input size \n512\u00d720\u00d720 512\u00d740\u00d740 512\u00d796\u00d796 \n\nVCM \n9.93 \n54.35 \n1308.00 \nACM \n6.92 \n14.29 \n61.11 \n\n\nAcknowledgementsThis work was supported in part by the National Key R&D Program of China #2018AAA0102003, National Natural Science Foundation of China #61876202, #61725202, #61751212 and #61829102, the Dalian Science and Tech-nology Innovation Foundation #2019J12GX039, and the Fundamental Research Funds for the Central Universities #DUT20ZD212.\nSee-through-text grouping for referring image segmentation. Ding-Jie Chen, Songhao Jia, Yi-Chen Lo, Hwann-Tzong Chen, Tyng-Luh Liu, Int. Conf. Comput. Vis. 56Ding-Jie Chen, Songhao Jia, Yi-Chen Lo, Hwann-Tzong Chen, and Tyng-Luh Liu. See-through-text grouping for re- ferring image segmentation. In Int. Conf. Comput. Vis., pages 7454-7463, 2019. 1, 2, 3, 5, 6\n\nHybrid task cascade for instance segmentation. Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, IEEE Conf. Comput. Vis. Pattern Recog. Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaox- iao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, et al. Hybrid task cascade for instance seg- mentation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 4974-4983, 2019. 2\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille, IEEE Trans. Pattern Anal. Mach. Intell. 404Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolu- tion, and fully connected crfs. IEEE Trans. Pattern Anal. Mach. Intell., 40(4):834-848, 2017. 2\n\nRethinking atrous convolution for semantic image segmentation. Liang-Chieh Chen, George Papandreou, Florian Schroff, Hartwig Adam, arXiv:1706.05587arXiv preprintLiang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for seman- tic image segmentation. arXiv preprint arXiv:1706.05587, 2017. 2\n\nBi-directional cross-modality feature propagation with separation-andaggregation gate for rgb-d semantic segmentation. Xiaokang Chen, Kwan-Yee Lin, Jingbo Wang, Wayne Wu, Chen Qian, Hongsheng Li, Gang Zeng, arXiv:2007.09183arXiv preprintXiaokang Chen, Kwan-Yee Lin, Jingbo Wang, Wayne Wu, Chen Qian, Hongsheng Li, and Gang Zeng. Bi-directional cross-modality feature propagation with separation-and- aggregation gate for rgb-d semantic segmentation. arXiv preprint arXiv:2007.09183, 2020. 2\n\nReferring expression object segmentation with caption-aware consistency. Y.-W Chen, Y.-H Tsai, T Wang, Y.-Y. Lin, M.-H Yang, Brit. Mach. Vis. Conf. 56Y.-W. Chen, Y.-H. Tsai, T. Wang, Y.-Y. Lin, and M.- H. Yang. Referring expression object segmentation with caption-aware consistency. In Brit. Mach. Vis. Conf., 2019. 5, 6\n\n. Tianheng Cheng, Xinggang Wang, Lichao Huang, Wenyu Liu, arXiv:2007.08921Boundary-preserving mask r-cnn. arXiv preprintTianheng Cheng, Xinggang Wang, Lichao Huang, and Wenyu Liu. Boundary-preserving mask r-cnn. arXiv preprint arXiv:2007.08921, 2020. 2\n\nContext contrasted feature and gated multiscale aggregation for scene segmentation. Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, Gang Wang, IEEE Conf. Comput. Vis. Pattern Recog. Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, and Gang Wang. Context contrasted feature and gated multi- scale aggregation for scene segmentation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 2393-2402, 2018. 2\n\nThe segmented and annotated iapr tc-12 benchmark. Carlos A Hugo Jair Escalante, Jesus A Hern\u00e1ndez, Aurelio Gonzalez, Manuel L\u00f3pez-L\u00f3pez, Eduardo F Montes, Enrique Morales, Luis Sucar, Michael Villase\u00f1or, Grubinger, CVIU. 1144Hugo Jair Escalante, Carlos A Hern\u00e1ndez, Jesus A Gonzalez, Aurelio L\u00f3pez-L\u00f3pez, Manuel Montes, Eduardo F Morales, L Enrique Sucar, Luis Villase\u00f1or, and Michael Grubinger. The segmented and annotated iapr tc-12 benchmark. CVIU, 114(4):419-428, 2010. 5\n\nCacnet: Salient object detection via context aggregation and contrast embedding. Guang Feng, Hongguang Bo, Jiayu Sun, Lihe Zhang, Huchuan Lu, Neurocomputing. 4032Guang Feng, Hongguang Bo, Jiayu Sun, Lihe Zhang, and Huchuan Lu. Cacnet: Salient object detection via con- text aggregation and contrast embedding. Neurocomputing, 403:33-44, 2020. 2\n\nDual attention network for scene segmentation. Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, Hanqing Lu, IEEE Conf. Comput. Vis. Pattern Recog. Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention network for scene segmentation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 3146-3154, 2019. 2\n\nFusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture. Caner Hazirbas, Lingni Ma, Csaba Domokos, Daniel Cremers, ACCV. SpringerCaner Hazirbas, Lingni Ma, Csaba Domokos, and Daniel Cremers. Fusenet: Incorporating depth into semantic seg- mentation via fusion-based cnn architecture. In ACCV, pages 213-228. Springer, 2016. 2\n\nPiotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. Kaiming He, Georgia Gkioxari, Int. Conf. Comput. Vis. Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask r-cnn. In Int. Conf. Comput. Vis., pages 2961- 2969, 2017. 2\n\nSegmentation from natural language expressions. Ronghang Hu, Marcus Rohrbach, Trevor Darrell, Eur. Conf. Comput. Vis. Springer6Ronghang Hu, Marcus Rohrbach, and Trevor Darrell. Seg- mentation from natural language expressions. In Eur. Conf. Comput. Vis., pages 108-124. Springer, 2016. 1, 2, 5, 6\n\nNatural language object retrieval. Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, Trevor Darrell, IEEE Conf. Comput. Vis. Pattern Recog. Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, and Trevor Darrell. Natural language object retrieval. In IEEE Conf. Comput. Vis. Pattern Recog., pages 4555-4564, 2016. 2\n\nBi-directional relationship inferring network for referring image segmentation. Zhiwei Hu, Guang Feng, Jiayu Sun, Lihe Zhang, Huchuan Lu, IEEE Conf. Comput. Vis. Pattern Recog. 67Zhiwei Hu, Guang Feng, Jiayu Sun, Lihe Zhang, and Huchuan Lu. Bi-directional relationship inferring network for referring image segmentation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 4424-4433, 2020. 1, 2, 3, 5, 6, 7\n\nReferring image segmentation via cross-modal progressive comprehension. Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao Wei, Jizhong Han, Luoqi Liu, Bo Li, IEEE Conf. Comput. Vis. Pattern Recog. 56Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao Wei, Jizhong Han, Luoqi Liu, and Bo Li. Referring image segmentation via cross-modal progressive comprehension. In IEEE Conf. Comput. Vis. Pattern Recog., pages 10488- 10497, 2020. 1, 2, 3, 5, 6\n\nCcnet: Criss-cross attention for semantic segmentation. Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, Wenyu Liu, Int. Conf. Comput. Vis. Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross attention for semantic segmentation. In Int. Conf. Comput. Vis., pages 603-612, 2019. 2\n\nLinguistic structure guided context modeling for referring image segmentation. Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu, Faxi Zhang, Jizhong Han, arXiv:2010.0051556arXiv preprintTianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu, Faxi Zhang, and Jizhong Han. Linguistic structure guided context modeling for referring image segmentation. arXiv preprint arXiv:2010.00515, 2020. 1, 2, 3, 5, 6\n\nSpatial transformer networks. Max Jaderberg, Karen Simonyan, Andrew Zisserman, In Adv. Neural Inform. Process. Syst. 5Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Adv. Neural Inform. Pro- cess. Syst., pages 2017-2025, 2015. 5\n\nReferitgame: Referring to objects in photographs of natural scenes. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, Tamara Berg, EMNLP. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in pho- tographs of natural scenes. In EMNLP, pages 787-798, 2014. 5\n\nEfficient inference in fully connected crfs with gaussian edge potentials. Philipp Kr\u00e4henb\u00fchl, Vladlen Koltun, Adv. Neural Inform. Process. Syst. Philipp Kr\u00e4henb\u00fchl and Vladlen Koltun. Efficient inference in fully connected crfs with gaussian edge potentials. In Adv. Neural Inform. Process. Syst., pages 109-117, 2011. 6\n\nReferring image segmentation via recurrent refinement networks. Ruiyu Li, Kaican Li, Yi-Chun Kuo, Michelle Shu, Xiaojuan Qi, Xiaoyong Shen, Jiaya Jia, IEEE Conf. Comput. Vis. Pattern Recog. 6Ruiyu Li, Kaican Li, Yi-Chun Kuo, Michelle Shu, Xiaojuan Qi, Xiaoyong Shen, and Jiaya Jia. Referring image seg- mentation via recurrent refinement networks. In IEEE Conf. Comput. Vis. Pattern Recog., pages 5745-5753, 2018. 1, 2, 5, 6\n\nA real-time cross-modality correlation filtering method for referring expression comprehension. Yue Liao, Si Liu, Guanbin Li, Fei Wang, Yanjie Chen, Chen Qian, Bo Li, IEEE Conf. Comput. Vis. Pattern Recog. Yue Liao, Si Liu, Guanbin Li, Fei Wang, Yanjie Chen, Chen Qian, and Bo Li. A real-time cross-modality corre- lation filtering method for referring expression comprehen- sion. In IEEE Conf. Comput. Vis. Pattern Recog., pages 10880-10889, 2020. 2\n\nFeature pyramid networks for object detection. Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie, IEEE Conf. Comput. Vis. Pattern Recog. 27Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In IEEE Conf. Comput. Vis. Pattern Recog., pages 2117-2125, 2017. 2, 7\n\nRecurrent multimodal interaction for referring image segmentation. Chenxi Liu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Alan Yuille, Int. Conf. Comput. Vis. 67Chenxi Liu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, and Alan Yuille. Recurrent multimodal interaction for referring image segmentation. In Int. Conf. Comput. Vis., pages 1271- 1280, 2017. 1, 2, 5, 6, 7\n\nReferring expression generation and comprehension via attributes. Jingyu Liu, Liang Wang, Ming-Hsuan Yang, Int. Conf. Comput. Vis. Jingyu Liu, Liang Wang, and Ming-Hsuan Yang. Referring expression generation and comprehension via attributes. In Int. Conf. Comput. Vis., pages 4856-4864, 2017. 2\n\nPath aggregation network for instance segmentation. Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, Jiaya Jia, IEEE Conf. Comput. Vis. Pattern Recog. Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for instance segmentation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 8759-8768, 2018. 2\n\nFully convolutional networks for semantic segmentation. Jonathan Long, Evan Shelhamer, Trevor Darrell, IEEE Conf. Comput. Vis. Pattern Recog. Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 3431-3440, 2015. 2\n\nCascade grouped attention network for referring expression segmentation. Gen Luo, Yiyi Zhou, Rongrong Ji, Xiaoshuai Sun, Jinsong Su, Chia-Wen Lin, Qi Tian, ACM Int. Conf. Multimedia. 56Gen Luo, Yiyi Zhou, Rongrong Ji, Xiaoshuai Sun, Jinsong Su, Chia-Wen Lin, and Qi Tian. Cascade grouped attention network for referring expression segmentation. In ACM Int. Conf. Multimedia, pages 1274-1282, 2020. 5, 6\n\nComprehensionguided referring expressions. Ruotian Luo, Gregory Shakhnarovich, IEEE Conf. Comput. Vis. Pattern Recog. Ruotian Luo and Gregory Shakhnarovich. Comprehension- guided referring expressions. In IEEE Conf. Comput. Vis. Pattern Recog., pages 7102-7111, 2017. 2\n\nGeneration and comprehension of unambiguous object descriptions. Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, Kevin Murphy, IEEE Conf. Comput. Vis. Pattern Recog. 25Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In IEEE Conf. Comput. Vis. Pattern Recog., pages 11-20, 2016. 2, 5\n\nDynamic multimodal instance segmentation guided by natural language queries. Edgar Margffoy-Tuay, C Juan, Emilio P\u00e9rez, Pablo Botero, Arbel\u00e1ez, Eur. Conf. Comput. Vis. 6Edgar Margffoy-Tuay, Juan C P\u00e9rez, Emilio Botero, and Pablo Arbel\u00e1ez. Dynamic multimodal instance segmentation guided by natural language queries. In Eur. Conf. Comput. Vis., pages 630-645, 2018. 1, 2, 5, 6\n\nLearning deconvolution network for semantic segmentation. Hyeonwoo Noh, Seunghoon Hong, Bohyung Han, Int. Conf. Comput. Vis. Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. Learning deconvolution network for semantic segmentation. In Int. Conf. Comput. Vis., pages 1520-1528, 2015. 2\n\nZero-shot grounding of objects from natural language queries. Arka Sadhu, Kan Chen, Ram Nevatia, Int. Conf. Comput. Vis. Arka Sadhu, Kan Chen, and Ram Nevatia. Zero-shot ground- ing of objects from natural language queries. In Int. Conf. Comput. Vis., pages 4694-4703, 2019. 2\n\nKey-word-aware network for referring expression image segmentation. Hengcan Shi, Hongliang Li, Fanman Meng, Qingbo Wu, Eur. Conf. Comput. Vis. 56Hengcan Shi, Hongliang Li, Fanman Meng, and Qingbo Wu. Key-word-aware network for referring expression image seg- mentation. In Eur. Conf. Comput. Vis., pages 38-54, 2018. 1, 2, 3, 5, 6\n\nA fast and accurate onestage approach to visual grounding. Zhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, Jiebo Luo, Int. Conf. Comput. Vis. Zhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, and Jiebo Luo. A fast and accurate one- stage approach to visual grounding. In Int. Conf. Comput. Vis., pages 4683-4693, 2019. 2\n\nCross-modal self-attention network for referring image segmentation. Linwei Ye, Mrigank Rochan, Zhi Liu, Yang Wang, IEEE Conf. Comput. Vis. Pattern Recog. 56Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang. Cross-modal self-attention network for referring image seg- mentation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 10502-10511, 2019. 1, 2, 3, 5, 6\n\nMattnet: Modular attention network for referring expression comprehension. Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, Tamara L Berg, IEEE Conf. Comput. Vis. Pattern Recog. 56Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg. Mattnet: Modular at- tention network for referring expression comprehension. In IEEE Conf. Comput. Vis. Pattern Recog., pages 1307-1315, 2018. 5, 6\n\nModeling context in referring expressions. Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, Tamara L Berg, Eur. Conf. Comput. Vis. SpringerLicheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expres- sions. In Eur. Conf. Comput. Vis., pages 69-85. Springer, 2016. 5\n\nA joint speaker-listener-reinforcer model for referring expressions. Licheng Yu, Hao Tan, Mohit Bansal, Tamara L Berg, IEEE Conf. Comput. Vis. Pattern Recog. Licheng Yu, Hao Tan, Mohit Bansal, and Tamara L Berg. A joint speaker-listener-reinforcer model for referring expres- sions. In IEEE Conf. Comput. Vis. Pattern Recog., pages 7282-7290, 2017. 2\n\n. Sergey Zagoruyko, Nikos Komodakis, arXiv:1605.07146Wide residual networks. arXiv preprintSergey Zagoruyko and Nikos Komodakis. Wide residual net- works. arXiv preprint arXiv:1605.07146, 2016. 3\n\nPyramid scene parsing network. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia, IEEE Conf. Comput. Vis. Pattern Recog. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In IEEE Conf. Comput. Vis. Pattern Recog., pages 2881-2890, 2017. 2\n\nPsanet: Point-wise spatial attention network for scene parsing. Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen Change Loy, Dahua Lin, Jiaya Jia, Eur. Conf. Comput. Vis. Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen Change Loy, Dahua Lin, and Jiaya Jia. Psanet: Point-wise spatial attention network for scene parsing. In Eur. Conf. Comput. Vis., pages 267-283, 2018. 2\n\nAsymmetric non-local neural networks for semantic segmentation. Zhen Zhu, Mengde Xu, Song Bai, Tengteng Huang, Xiang Bai, Int. Conf. Comput. Vis. 24Zhen Zhu, Mengde Xu, Song Bai, Tengteng Huang, and Xi- ang Bai. Asymmetric non-local neural networks for seman- tic segmentation. In Int. Conf. Comput. Vis., pages 593-602, 2019. 2, 4\n", "annotations": {"author": "[{\"end\":258,\"start\":87},{\"end\":406,\"start\":259},{\"end\":577,\"start\":407},{\"end\":726,\"start\":578}]", "publisher": null, "author_last_name": "[{\"end\":97,\"start\":93},{\"end\":268,\"start\":266},{\"end\":417,\"start\":412},{\"end\":588,\"start\":586}]", "author_first_name": "[{\"end\":92,\"start\":87},{\"end\":265,\"start\":259},{\"end\":411,\"start\":407},{\"end\":585,\"start\":578}]", "author_affiliation": "[{\"end\":206,\"start\":122},{\"end\":257,\"start\":208},{\"end\":354,\"start\":270},{\"end\":405,\"start\":356},{\"end\":525,\"start\":441},{\"end\":576,\"start\":527},{\"end\":674,\"start\":590},{\"end\":725,\"start\":676}]", "title": "[{\"end\":84,\"start\":1},{\"end\":810,\"start\":727}]", "venue": null, "abstract": "[{\"end\":1891,\"start\":812}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3145,\"start\":3141},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3148,\"start\":3145},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3151,\"start\":3148},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3154,\"start\":3151},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3553,\"start\":3549},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3556,\"start\":3553},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3558,\"start\":3556},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3561,\"start\":3558},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3564,\"start\":3561},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3567,\"start\":3564},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4309,\"start\":4305},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4312,\"start\":4309},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4431,\"start\":4427},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4434,\"start\":4431},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4437,\"start\":4434},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4701,\"start\":4697},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4703,\"start\":4701},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4706,\"start\":4703},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4709,\"start\":4706},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4712,\"start\":4709},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7706,\"start\":7702},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7777,\"start\":7773},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7779,\"start\":7777},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7781,\"start\":7779},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7783,\"start\":7781},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7786,\"start\":7783},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7816,\"start\":7812},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7819,\"start\":7816},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7822,\"start\":7819},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7825,\"start\":7822},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7862,\"start\":7858},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7864,\"start\":7862},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8003,\"start\":7999},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8005,\"start\":8003},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8166,\"start\":8162},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8328,\"start\":8324},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8357,\"start\":8353},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8385,\"start\":8382},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8418,\"start\":8415},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8904,\"start\":8900},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8907,\"start\":8904},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8910,\"start\":8907},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9037,\"start\":9033},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9040,\"start\":9037},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9140,\"start\":9136},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9143,\"start\":9140},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9146,\"start\":9143},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9431,\"start\":9427},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9434,\"start\":9431},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9437,\"start\":9434},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9440,\"start\":9437},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10201,\"start\":10197},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10204,\"start\":10201},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10206,\"start\":10204},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10209,\"start\":10206},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10212,\"start\":10209},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10215,\"start\":10212},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10341,\"start\":10337},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10465,\"start\":10461},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10584,\"start\":10580},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10784,\"start\":10780},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10965,\"start\":10961},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11959,\"start\":11955},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13225,\"start\":13221},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14614,\"start\":14610},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15854,\"start\":15850},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15856,\"start\":15854},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15859,\"start\":15856},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15862,\"start\":15859},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":15865,\"start\":15862},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16389,\"start\":16385},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":17460,\"start\":17456},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":17471,\"start\":17467},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17488,\"start\":17484},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17505,\"start\":17501},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17686,\"start\":17682},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18382,\"start\":18379},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19478,\"start\":19474},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19481,\"start\":19478},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19484,\"start\":19481},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20080,\"start\":20076},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20090,\"start\":20086},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20100,\"start\":20096},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":20110,\"start\":20106},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20120,\"start\":20116},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20134,\"start\":20130},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20148,\"start\":20145},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":20159,\"start\":20155},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20169,\"start\":20166},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20180,\"start\":20176},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20193,\"start\":20189},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20204,\"start\":20200},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20219,\"start\":20215},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20751,\"start\":20747},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20754,\"start\":20751},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21776,\"start\":21772},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27311,\"start\":27307}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25679,\"start\":25466},{\"attributes\":{\"id\":\"fig_1\"},\"end\":25857,\"start\":25680},{\"attributes\":{\"id\":\"fig_2\"},\"end\":26876,\"start\":25858},{\"attributes\":{\"id\":\"fig_3\"},\"end\":27012,\"start\":26877},{\"attributes\":{\"id\":\"fig_4\"},\"end\":27195,\"start\":27013},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":29504,\"start\":27196},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30493,\"start\":29505},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31417,\"start\":30494},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":31626,\"start\":31418},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":31817,\"start\":31627}]", "paragraph": "[{\"end\":3681,\"start\":1907},{\"end\":5191,\"start\":3683},{\"end\":6572,\"start\":5193},{\"end\":6612,\"start\":6574},{\"end\":6982,\"start\":6614},{\"end\":7174,\"start\":6984},{\"end\":7370,\"start\":7176},{\"end\":8124,\"start\":7387},{\"end\":9377,\"start\":8126},{\"end\":11606,\"start\":9379},{\"end\":11858,\"start\":11659},{\"end\":12129,\"start\":11895},{\"end\":13062,\"start\":12145},{\"end\":13333,\"start\":13097},{\"end\":13403,\"start\":13402},{\"end\":13471,\"start\":13405},{\"end\":13739,\"start\":13496},{\"end\":13946,\"start\":13780},{\"end\":14690,\"start\":13972},{\"end\":14972,\"start\":14692},{\"end\":15300,\"start\":15042},{\"end\":15442,\"start\":15336},{\"end\":15651,\"start\":15496},{\"end\":16306,\"start\":15683},{\"end\":16390,\"start\":16333},{\"end\":16592,\"start\":16392},{\"end\":16966,\"start\":16659},{\"end\":17313,\"start\":17044},{\"end\":17506,\"start\":17340},{\"end\":18084,\"start\":17508},{\"end\":18332,\"start\":18086},{\"end\":18623,\"start\":18334},{\"end\":19427,\"start\":18650},{\"end\":19938,\"start\":19429},{\"end\":20220,\"start\":19965},{\"end\":20949,\"start\":20222},{\"end\":21434,\"start\":20984},{\"end\":21547,\"start\":21453},{\"end\":22259,\"start\":21549},{\"end\":22927,\"start\":22261},{\"end\":23407,\"start\":22929},{\"end\":24613,\"start\":23425},{\"end\":25465,\"start\":24628}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11640,\"start\":11607},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12144,\"start\":12130},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13096,\"start\":13063},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13401,\"start\":13334},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13495,\"start\":13472},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13779,\"start\":13740},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13971,\"start\":13947},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15041,\"start\":14973},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15335,\"start\":15301},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15495,\"start\":15443},{\"attributes\":{\"id\":\"formula_10\"},\"end\":16332,\"start\":16307},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16658,\"start\":16593},{\"attributes\":{\"id\":\"formula_12\"},\"end\":17043,\"start\":16967}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1905,\"start\":1893},{\"attributes\":{\"n\":\"2.\"},\"end\":7385,\"start\":7373},{\"attributes\":{\"n\":\"3.\"},\"end\":11657,\"start\":11642},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11893,\"start\":11861},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15681,\"start\":15654},{\"attributes\":{\"n\":\"4.\"},\"end\":17327,\"start\":17316},{\"attributes\":{\"n\":\"4.1.\"},\"end\":17338,\"start\":17330},{\"attributes\":{\"n\":\"4.2.\"},\"end\":18648,\"start\":18626},{\"attributes\":{\"n\":\"4.3.\"},\"end\":19963,\"start\":19941},{\"end\":20982,\"start\":20952},{\"attributes\":{\"n\":\"4.4.\"},\"end\":21451,\"start\":21437},{\"attributes\":{\"n\":\"4.5.\"},\"end\":23423,\"start\":23410},{\"attributes\":{\"n\":\"5.\"},\"end\":24626,\"start\":24616},{\"end\":25691,\"start\":25681},{\"end\":25869,\"start\":25859},{\"end\":26888,\"start\":26878},{\"end\":27024,\"start\":27014},{\"end\":27206,\"start\":27197},{\"end\":29515,\"start\":29506},{\"end\":30504,\"start\":30495},{\"end\":31428,\"start\":31419},{\"end\":31637,\"start\":31628}]", "table": "[{\"end\":29504,\"start\":27328},{\"end\":30493,\"start\":29517},{\"end\":31417,\"start\":30638},{\"end\":31626,\"start\":31508},{\"end\":31817,\"start\":31720}]", "figure_caption": "[{\"end\":25679,\"start\":25468},{\"end\":25857,\"start\":25693},{\"end\":26876,\"start\":25871},{\"end\":27012,\"start\":26890},{\"end\":27195,\"start\":27026},{\"end\":27328,\"start\":27208},{\"end\":30638,\"start\":30506},{\"end\":31508,\"start\":31430},{\"end\":31720,\"start\":31639}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":4425,\"start\":4415},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5660,\"start\":5650},{\"end\":9607,\"start\":9599},{\"end\":11731,\"start\":11725},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14407,\"start\":14397},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15650,\"start\":15640},{\"end\":16205,\"start\":16199},{\"end\":17312,\"start\":17306},{\"end\":20607,\"start\":20601},{\"end\":21672,\"start\":21666},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23260,\"start\":23254},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23478,\"start\":23472},{\"end\":24327,\"start\":24319}]", "bib_author_first_name": "[{\"end\":32233,\"start\":32225},{\"end\":32247,\"start\":32240},{\"end\":32260,\"start\":32253},{\"end\":32276,\"start\":32265},{\"end\":32291,\"start\":32283},{\"end\":32577,\"start\":32574},{\"end\":32593,\"start\":32584},{\"end\":32605,\"start\":32600},{\"end\":32614,\"start\":32612},{\"end\":32630,\"start\":32622},{\"end\":32642,\"start\":32635},{\"end\":32654,\"start\":32648},{\"end\":32666,\"start\":32661},{\"end\":32680,\"start\":32672},{\"end\":32691,\"start\":32686},{\"end\":33116,\"start\":33105},{\"end\":33129,\"start\":33123},{\"end\":33149,\"start\":33142},{\"end\":33165,\"start\":33160},{\"end\":33178,\"start\":33174},{\"end\":33180,\"start\":33179},{\"end\":33574,\"start\":33563},{\"end\":33587,\"start\":33581},{\"end\":33607,\"start\":33600},{\"end\":33624,\"start\":33617},{\"end\":33967,\"start\":33959},{\"end\":33982,\"start\":33974},{\"end\":33994,\"start\":33988},{\"end\":34006,\"start\":34001},{\"end\":34015,\"start\":34011},{\"end\":34031,\"start\":34022},{\"end\":34040,\"start\":34036},{\"end\":34409,\"start\":34405},{\"end\":34420,\"start\":34416},{\"end\":34428,\"start\":34427},{\"end\":34440,\"start\":34435},{\"end\":34450,\"start\":34446},{\"end\":34665,\"start\":34657},{\"end\":34681,\"start\":34673},{\"end\":34694,\"start\":34688},{\"end\":34707,\"start\":34702},{\"end\":35000,\"start\":34993},{\"end\":35013,\"start\":35007},{\"end\":35025,\"start\":35021},{\"end\":35035,\"start\":35033},{\"end\":35039,\"start\":35036},{\"end\":35049,\"start\":35045},{\"end\":35373,\"start\":35367},{\"end\":35375,\"start\":35374},{\"end\":35402,\"start\":35397},{\"end\":35404,\"start\":35403},{\"end\":35423,\"start\":35416},{\"end\":35440,\"start\":35434},{\"end\":35461,\"start\":35454},{\"end\":35463,\"start\":35462},{\"end\":35479,\"start\":35472},{\"end\":35493,\"start\":35489},{\"end\":35508,\"start\":35501},{\"end\":35880,\"start\":35875},{\"end\":35896,\"start\":35887},{\"end\":35906,\"start\":35901},{\"end\":35916,\"start\":35912},{\"end\":35931,\"start\":35924},{\"end\":36190,\"start\":36187},{\"end\":36199,\"start\":36195},{\"end\":36211,\"start\":36205},{\"end\":36222,\"start\":36218},{\"end\":36234,\"start\":36227},{\"end\":36246,\"start\":36240},{\"end\":36260,\"start\":36253},{\"end\":36598,\"start\":36593},{\"end\":36615,\"start\":36609},{\"end\":36625,\"start\":36620},{\"end\":36641,\"start\":36635},{\"end\":36915,\"start\":36908},{\"end\":36927,\"start\":36920},{\"end\":37150,\"start\":37142},{\"end\":37161,\"start\":37155},{\"end\":37178,\"start\":37172},{\"end\":37435,\"start\":37427},{\"end\":37446,\"start\":37440},{\"end\":37457,\"start\":37451},{\"end\":37474,\"start\":37468},{\"end\":37485,\"start\":37481},{\"end\":37500,\"start\":37494},{\"end\":37826,\"start\":37820},{\"end\":37836,\"start\":37831},{\"end\":37848,\"start\":37843},{\"end\":37858,\"start\":37854},{\"end\":37873,\"start\":37866},{\"end\":38224,\"start\":38217},{\"end\":38239,\"start\":38232},{\"end\":38247,\"start\":38245},{\"end\":38260,\"start\":38253},{\"end\":38272,\"start\":38265},{\"end\":38285,\"start\":38278},{\"end\":38296,\"start\":38291},{\"end\":38304,\"start\":38302},{\"end\":38664,\"start\":38658},{\"end\":38680,\"start\":38672},{\"end\":38693,\"start\":38687},{\"end\":38706,\"start\":38701},{\"end\":38721,\"start\":38714},{\"end\":38732,\"start\":38727},{\"end\":39040,\"start\":39033},{\"end\":39048,\"start\":39046},{\"end\":39061,\"start\":39054},{\"end\":39076,\"start\":39069},{\"end\":39086,\"start\":39081},{\"end\":39095,\"start\":39091},{\"end\":39110,\"start\":39103},{\"end\":39401,\"start\":39398},{\"end\":39418,\"start\":39413},{\"end\":39435,\"start\":39429},{\"end\":39712,\"start\":39707},{\"end\":39732,\"start\":39725},{\"end\":39746,\"start\":39742},{\"end\":39761,\"start\":39755},{\"end\":40026,\"start\":40019},{\"end\":40046,\"start\":40039},{\"end\":40336,\"start\":40331},{\"end\":40347,\"start\":40341},{\"end\":40359,\"start\":40352},{\"end\":40373,\"start\":40365},{\"end\":40387,\"start\":40379},{\"end\":40400,\"start\":40392},{\"end\":40412,\"start\":40407},{\"end\":40792,\"start\":40789},{\"end\":40801,\"start\":40799},{\"end\":40814,\"start\":40807},{\"end\":40822,\"start\":40819},{\"end\":40835,\"start\":40829},{\"end\":40846,\"start\":40842},{\"end\":40855,\"start\":40853},{\"end\":41200,\"start\":41192},{\"end\":41211,\"start\":41206},{\"end\":41224,\"start\":41220},{\"end\":41242,\"start\":41235},{\"end\":41254,\"start\":41247},{\"end\":41271,\"start\":41266},{\"end\":41609,\"start\":41603},{\"end\":41618,\"start\":41615},{\"end\":41631,\"start\":41624},{\"end\":41643,\"start\":41638},{\"end\":41653,\"start\":41650},{\"end\":41662,\"start\":41658},{\"end\":41975,\"start\":41969},{\"end\":41986,\"start\":41981},{\"end\":42003,\"start\":41993},{\"end\":42254,\"start\":42251},{\"end\":42262,\"start\":42260},{\"end\":42274,\"start\":42267},{\"end\":42288,\"start\":42280},{\"end\":42299,\"start\":42294},{\"end\":42587,\"start\":42579},{\"end\":42598,\"start\":42594},{\"end\":42616,\"start\":42610},{\"end\":42917,\"start\":42914},{\"end\":42927,\"start\":42923},{\"end\":42942,\"start\":42934},{\"end\":42956,\"start\":42947},{\"end\":42969,\"start\":42962},{\"end\":42982,\"start\":42974},{\"end\":42990,\"start\":42988},{\"end\":43295,\"start\":43288},{\"end\":43308,\"start\":43301},{\"end\":43587,\"start\":43581},{\"end\":43601,\"start\":43593},{\"end\":43618,\"start\":43609},{\"end\":43631,\"start\":43627},{\"end\":43645,\"start\":43641},{\"end\":43647,\"start\":43646},{\"end\":43661,\"start\":43656},{\"end\":44019,\"start\":44014},{\"end\":44036,\"start\":44035},{\"end\":44049,\"start\":44043},{\"end\":44062,\"start\":44057},{\"end\":44380,\"start\":44372},{\"end\":44395,\"start\":44386},{\"end\":44409,\"start\":44402},{\"end\":44664,\"start\":44660},{\"end\":44675,\"start\":44672},{\"end\":44685,\"start\":44682},{\"end\":44951,\"start\":44944},{\"end\":44966,\"start\":44957},{\"end\":44977,\"start\":44971},{\"end\":44990,\"start\":44984},{\"end\":45276,\"start\":45267},{\"end\":45289,\"start\":45283},{\"end\":45301,\"start\":45296},{\"end\":45315,\"start\":45308},{\"end\":45327,\"start\":45323},{\"end\":45337,\"start\":45332},{\"end\":45637,\"start\":45631},{\"end\":45649,\"start\":45642},{\"end\":45661,\"start\":45658},{\"end\":45671,\"start\":45667},{\"end\":46006,\"start\":45999},{\"end\":46014,\"start\":46011},{\"end\":46027,\"start\":46020},{\"end\":46039,\"start\":46034},{\"end\":46049,\"start\":46046},{\"end\":46059,\"start\":46054},{\"end\":46074,\"start\":46068},{\"end\":46076,\"start\":46075},{\"end\":46411,\"start\":46404},{\"end\":46423,\"start\":46416},{\"end\":46437,\"start\":46433},{\"end\":46453,\"start\":46444},{\"end\":46455,\"start\":46454},{\"end\":46468,\"start\":46462},{\"end\":46470,\"start\":46469},{\"end\":46767,\"start\":46760},{\"end\":46775,\"start\":46772},{\"end\":46786,\"start\":46781},{\"end\":46801,\"start\":46795},{\"end\":46803,\"start\":46802},{\"end\":47051,\"start\":47045},{\"end\":47068,\"start\":47063},{\"end\":47281,\"start\":47271},{\"end\":47296,\"start\":47288},{\"end\":47310,\"start\":47302},{\"end\":47323,\"start\":47315},{\"end\":47335,\"start\":47330},{\"end\":47628,\"start\":47618},{\"end\":47637,\"start\":47635},{\"end\":47648,\"start\":47645},{\"end\":47662,\"start\":47654},{\"end\":47672,\"start\":47668},{\"end\":47679,\"start\":47673},{\"end\":47690,\"start\":47685},{\"end\":47701,\"start\":47696},{\"end\":48008,\"start\":48004},{\"end\":48020,\"start\":48014},{\"end\":48029,\"start\":48025},{\"end\":48043,\"start\":48035},{\"end\":48056,\"start\":48051}]", "bib_author_last_name": "[{\"end\":32238,\"start\":32234},{\"end\":32251,\"start\":32248},{\"end\":32263,\"start\":32261},{\"end\":32281,\"start\":32277},{\"end\":32295,\"start\":32292},{\"end\":32582,\"start\":32578},{\"end\":32598,\"start\":32594},{\"end\":32610,\"start\":32606},{\"end\":32620,\"start\":32615},{\"end\":32633,\"start\":32631},{\"end\":32646,\"start\":32643},{\"end\":32659,\"start\":32655},{\"end\":32670,\"start\":32667},{\"end\":32684,\"start\":32681},{\"end\":32698,\"start\":32692},{\"end\":33121,\"start\":33117},{\"end\":33140,\"start\":33130},{\"end\":33158,\"start\":33150},{\"end\":33172,\"start\":33166},{\"end\":33187,\"start\":33181},{\"end\":33579,\"start\":33575},{\"end\":33598,\"start\":33588},{\"end\":33615,\"start\":33608},{\"end\":33629,\"start\":33625},{\"end\":33972,\"start\":33968},{\"end\":33986,\"start\":33983},{\"end\":33999,\"start\":33995},{\"end\":34009,\"start\":34007},{\"end\":34020,\"start\":34016},{\"end\":34034,\"start\":34032},{\"end\":34045,\"start\":34041},{\"end\":34414,\"start\":34410},{\"end\":34425,\"start\":34421},{\"end\":34433,\"start\":34429},{\"end\":34444,\"start\":34441},{\"end\":34455,\"start\":34451},{\"end\":34671,\"start\":34666},{\"end\":34686,\"start\":34682},{\"end\":34700,\"start\":34695},{\"end\":34711,\"start\":34708},{\"end\":35005,\"start\":35001},{\"end\":35019,\"start\":35014},{\"end\":35031,\"start\":35026},{\"end\":35043,\"start\":35040},{\"end\":35054,\"start\":35050},{\"end\":35395,\"start\":35376},{\"end\":35414,\"start\":35405},{\"end\":35432,\"start\":35424},{\"end\":35452,\"start\":35441},{\"end\":35470,\"start\":35464},{\"end\":35487,\"start\":35480},{\"end\":35499,\"start\":35494},{\"end\":35519,\"start\":35509},{\"end\":35530,\"start\":35521},{\"end\":35885,\"start\":35881},{\"end\":35899,\"start\":35897},{\"end\":35910,\"start\":35907},{\"end\":35922,\"start\":35917},{\"end\":35934,\"start\":35932},{\"end\":36193,\"start\":36191},{\"end\":36203,\"start\":36200},{\"end\":36216,\"start\":36212},{\"end\":36225,\"start\":36223},{\"end\":36238,\"start\":36235},{\"end\":36251,\"start\":36247},{\"end\":36263,\"start\":36261},{\"end\":36607,\"start\":36599},{\"end\":36618,\"start\":36616},{\"end\":36633,\"start\":36626},{\"end\":36649,\"start\":36642},{\"end\":36918,\"start\":36916},{\"end\":36936,\"start\":36928},{\"end\":37153,\"start\":37151},{\"end\":37170,\"start\":37162},{\"end\":37186,\"start\":37179},{\"end\":37438,\"start\":37436},{\"end\":37449,\"start\":37447},{\"end\":37466,\"start\":37458},{\"end\":37479,\"start\":37475},{\"end\":37492,\"start\":37486},{\"end\":37508,\"start\":37501},{\"end\":37829,\"start\":37827},{\"end\":37841,\"start\":37837},{\"end\":37852,\"start\":37849},{\"end\":37864,\"start\":37859},{\"end\":37876,\"start\":37874},{\"end\":38230,\"start\":38225},{\"end\":38243,\"start\":38240},{\"end\":38251,\"start\":38248},{\"end\":38263,\"start\":38261},{\"end\":38276,\"start\":38273},{\"end\":38289,\"start\":38286},{\"end\":38300,\"start\":38297},{\"end\":38307,\"start\":38305},{\"end\":38670,\"start\":38665},{\"end\":38685,\"start\":38681},{\"end\":38699,\"start\":38694},{\"end\":38712,\"start\":38707},{\"end\":38725,\"start\":38722},{\"end\":38736,\"start\":38733},{\"end\":39044,\"start\":39041},{\"end\":39052,\"start\":39049},{\"end\":39067,\"start\":39062},{\"end\":39079,\"start\":39077},{\"end\":39089,\"start\":39087},{\"end\":39101,\"start\":39096},{\"end\":39114,\"start\":39111},{\"end\":39411,\"start\":39402},{\"end\":39427,\"start\":39419},{\"end\":39445,\"start\":39436},{\"end\":39723,\"start\":39713},{\"end\":39740,\"start\":39733},{\"end\":39753,\"start\":39747},{\"end\":39766,\"start\":39762},{\"end\":40037,\"start\":40027},{\"end\":40053,\"start\":40047},{\"end\":40339,\"start\":40337},{\"end\":40350,\"start\":40348},{\"end\":40363,\"start\":40360},{\"end\":40377,\"start\":40374},{\"end\":40390,\"start\":40388},{\"end\":40405,\"start\":40401},{\"end\":40416,\"start\":40413},{\"end\":40797,\"start\":40793},{\"end\":40805,\"start\":40802},{\"end\":40817,\"start\":40815},{\"end\":40827,\"start\":40823},{\"end\":40840,\"start\":40836},{\"end\":40851,\"start\":40847},{\"end\":40858,\"start\":40856},{\"end\":41204,\"start\":41201},{\"end\":41218,\"start\":41212},{\"end\":41233,\"start\":41225},{\"end\":41245,\"start\":41243},{\"end\":41264,\"start\":41255},{\"end\":41280,\"start\":41272},{\"end\":41613,\"start\":41610},{\"end\":41622,\"start\":41619},{\"end\":41636,\"start\":41632},{\"end\":41648,\"start\":41644},{\"end\":41656,\"start\":41654},{\"end\":41669,\"start\":41663},{\"end\":41979,\"start\":41976},{\"end\":41991,\"start\":41987},{\"end\":42008,\"start\":42004},{\"end\":42258,\"start\":42255},{\"end\":42265,\"start\":42263},{\"end\":42278,\"start\":42275},{\"end\":42292,\"start\":42289},{\"end\":42303,\"start\":42300},{\"end\":42592,\"start\":42588},{\"end\":42608,\"start\":42599},{\"end\":42624,\"start\":42617},{\"end\":42921,\"start\":42918},{\"end\":42932,\"start\":42928},{\"end\":42945,\"start\":42943},{\"end\":42960,\"start\":42957},{\"end\":42972,\"start\":42970},{\"end\":42986,\"start\":42983},{\"end\":42995,\"start\":42991},{\"end\":43299,\"start\":43296},{\"end\":43322,\"start\":43309},{\"end\":43591,\"start\":43588},{\"end\":43607,\"start\":43602},{\"end\":43625,\"start\":43619},{\"end\":43639,\"start\":43632},{\"end\":43654,\"start\":43648},{\"end\":43668,\"start\":43662},{\"end\":44033,\"start\":44020},{\"end\":44041,\"start\":44037},{\"end\":44055,\"start\":44050},{\"end\":44069,\"start\":44063},{\"end\":44079,\"start\":44071},{\"end\":44384,\"start\":44381},{\"end\":44400,\"start\":44396},{\"end\":44413,\"start\":44410},{\"end\":44670,\"start\":44665},{\"end\":44680,\"start\":44676},{\"end\":44693,\"start\":44686},{\"end\":44955,\"start\":44952},{\"end\":44969,\"start\":44967},{\"end\":44982,\"start\":44978},{\"end\":44993,\"start\":44991},{\"end\":45281,\"start\":45277},{\"end\":45294,\"start\":45290},{\"end\":45306,\"start\":45302},{\"end\":45321,\"start\":45316},{\"end\":45330,\"start\":45328},{\"end\":45341,\"start\":45338},{\"end\":45640,\"start\":45638},{\"end\":45656,\"start\":45650},{\"end\":45665,\"start\":45662},{\"end\":45676,\"start\":45672},{\"end\":46009,\"start\":46007},{\"end\":46018,\"start\":46015},{\"end\":46032,\"start\":46028},{\"end\":46044,\"start\":46040},{\"end\":46052,\"start\":46050},{\"end\":46066,\"start\":46060},{\"end\":46081,\"start\":46077},{\"end\":46414,\"start\":46412},{\"end\":46431,\"start\":46424},{\"end\":46442,\"start\":46438},{\"end\":46460,\"start\":46456},{\"end\":46475,\"start\":46471},{\"end\":46770,\"start\":46768},{\"end\":46779,\"start\":46776},{\"end\":46793,\"start\":46787},{\"end\":46808,\"start\":46804},{\"end\":47061,\"start\":47052},{\"end\":47078,\"start\":47069},{\"end\":47286,\"start\":47282},{\"end\":47300,\"start\":47297},{\"end\":47313,\"start\":47311},{\"end\":47328,\"start\":47324},{\"end\":47339,\"start\":47336},{\"end\":47633,\"start\":47629},{\"end\":47643,\"start\":47638},{\"end\":47652,\"start\":47649},{\"end\":47666,\"start\":47663},{\"end\":47683,\"start\":47680},{\"end\":47694,\"start\":47691},{\"end\":47705,\"start\":47702},{\"end\":48012,\"start\":48009},{\"end\":48023,\"start\":48021},{\"end\":48033,\"start\":48030},{\"end\":48049,\"start\":48044},{\"end\":48060,\"start\":48057}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":207979929},\"end\":32525,\"start\":32165},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":58981777},\"end\":32990,\"start\":32527},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3429309},\"end\":33498,\"start\":32992},{\"attributes\":{\"doi\":\"arXiv:1706.05587\",\"id\":\"b3\"},\"end\":33838,\"start\":33500},{\"attributes\":{\"doi\":\"arXiv:2007.09183\",\"id\":\"b4\"},\"end\":34330,\"start\":33840},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":204008128},\"end\":34653,\"start\":34332},{\"attributes\":{\"doi\":\"arXiv:2007.08921\",\"id\":\"b6\"},\"end\":34907,\"start\":34655},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":52841003},\"end\":35315,\"start\":34909},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":37809600},\"end\":35792,\"start\":35317},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":219077446},\"end\":36138,\"start\":35794},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":52180375},\"end\":36500,\"start\":36140},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":178063},\"end\":36861,\"start\":36502},{\"attributes\":{\"id\":\"b12\"},\"end\":37092,\"start\":36863},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1931511},\"end\":37390,\"start\":37094},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9944232},\"end\":37738,\"start\":37392},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":219616239},\"end\":38143,\"start\":37740},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":219622248},\"end\":38600,\"start\":38145},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":53846561},\"end\":38952,\"start\":38602},{\"attributes\":{\"doi\":\"arXiv:2010.00515\",\"id\":\"b18\"},\"end\":39366,\"start\":38954},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6099034},\"end\":39637,\"start\":39368},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6308361},\"end\":39942,\"start\":39639},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5574079},\"end\":40265,\"start\":39944},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":52831465},\"end\":40691,\"start\":40267},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":202577846},\"end\":41143,\"start\":40693},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":10716717},\"end\":41534,\"start\":41145},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3518818},\"end\":41901,\"start\":41536},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":12483169},\"end\":42197,\"start\":41903},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3698141},\"end\":42521,\"start\":42199},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1629541},\"end\":42839,\"start\":42523},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":222278160},\"end\":43243,\"start\":42841},{\"attributes\":{\"id\":\"b30\"},\"end\":43514,\"start\":43245},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":8745888},\"end\":43935,\"start\":43516},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":49656155},\"end\":44312,\"start\":43937},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":623137},\"end\":44596,\"start\":44314},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":201103723},\"end\":44874,\"start\":44598},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":52955377},\"end\":45206,\"start\":44876},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":201070217},\"end\":45560,\"start\":45208},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":104292134},\"end\":45922,\"start\":45562},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":3441497},\"end\":46359,\"start\":45924},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":1688357},\"end\":46689,\"start\":46361},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":10132533},\"end\":47041,\"start\":46691},{\"attributes\":{\"doi\":\"arXiv:1605.07146\",\"id\":\"b41\"},\"end\":47238,\"start\":47043},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":5299559},\"end\":47552,\"start\":47240},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":52958802},\"end\":47938,\"start\":47554},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":201127121},\"end\":48271,\"start\":47940}]", "bib_title": "[{\"end\":32223,\"start\":32165},{\"end\":32572,\"start\":32527},{\"end\":33103,\"start\":32992},{\"end\":34403,\"start\":34332},{\"end\":34991,\"start\":34909},{\"end\":35365,\"start\":35317},{\"end\":35873,\"start\":35794},{\"end\":36185,\"start\":36140},{\"end\":36591,\"start\":36502},{\"end\":36906,\"start\":36863},{\"end\":37140,\"start\":37094},{\"end\":37425,\"start\":37392},{\"end\":37818,\"start\":37740},{\"end\":38215,\"start\":38145},{\"end\":38656,\"start\":38602},{\"end\":39396,\"start\":39368},{\"end\":39705,\"start\":39639},{\"end\":40017,\"start\":39944},{\"end\":40329,\"start\":40267},{\"end\":40787,\"start\":40693},{\"end\":41190,\"start\":41145},{\"end\":41601,\"start\":41536},{\"end\":41967,\"start\":41903},{\"end\":42249,\"start\":42199},{\"end\":42577,\"start\":42523},{\"end\":42912,\"start\":42841},{\"end\":43286,\"start\":43245},{\"end\":43579,\"start\":43516},{\"end\":44012,\"start\":43937},{\"end\":44370,\"start\":44314},{\"end\":44658,\"start\":44598},{\"end\":44942,\"start\":44876},{\"end\":45265,\"start\":45208},{\"end\":45629,\"start\":45562},{\"end\":45997,\"start\":45924},{\"end\":46402,\"start\":46361},{\"end\":46758,\"start\":46691},{\"end\":47269,\"start\":47240},{\"end\":47616,\"start\":47554},{\"end\":48002,\"start\":47940}]", "bib_author": "[{\"end\":32240,\"start\":32225},{\"end\":32253,\"start\":32240},{\"end\":32265,\"start\":32253},{\"end\":32283,\"start\":32265},{\"end\":32297,\"start\":32283},{\"end\":32584,\"start\":32574},{\"end\":32600,\"start\":32584},{\"end\":32612,\"start\":32600},{\"end\":32622,\"start\":32612},{\"end\":32635,\"start\":32622},{\"end\":32648,\"start\":32635},{\"end\":32661,\"start\":32648},{\"end\":32672,\"start\":32661},{\"end\":32686,\"start\":32672},{\"end\":32700,\"start\":32686},{\"end\":33123,\"start\":33105},{\"end\":33142,\"start\":33123},{\"end\":33160,\"start\":33142},{\"end\":33174,\"start\":33160},{\"end\":33189,\"start\":33174},{\"end\":33581,\"start\":33563},{\"end\":33600,\"start\":33581},{\"end\":33617,\"start\":33600},{\"end\":33631,\"start\":33617},{\"end\":33974,\"start\":33959},{\"end\":33988,\"start\":33974},{\"end\":34001,\"start\":33988},{\"end\":34011,\"start\":34001},{\"end\":34022,\"start\":34011},{\"end\":34036,\"start\":34022},{\"end\":34047,\"start\":34036},{\"end\":34416,\"start\":34405},{\"end\":34427,\"start\":34416},{\"end\":34435,\"start\":34427},{\"end\":34446,\"start\":34435},{\"end\":34457,\"start\":34446},{\"end\":34673,\"start\":34657},{\"end\":34688,\"start\":34673},{\"end\":34702,\"start\":34688},{\"end\":34713,\"start\":34702},{\"end\":35007,\"start\":34993},{\"end\":35021,\"start\":35007},{\"end\":35033,\"start\":35021},{\"end\":35045,\"start\":35033},{\"end\":35056,\"start\":35045},{\"end\":35397,\"start\":35367},{\"end\":35416,\"start\":35397},{\"end\":35434,\"start\":35416},{\"end\":35454,\"start\":35434},{\"end\":35472,\"start\":35454},{\"end\":35489,\"start\":35472},{\"end\":35501,\"start\":35489},{\"end\":35521,\"start\":35501},{\"end\":35532,\"start\":35521},{\"end\":35887,\"start\":35875},{\"end\":35901,\"start\":35887},{\"end\":35912,\"start\":35901},{\"end\":35924,\"start\":35912},{\"end\":35936,\"start\":35924},{\"end\":36195,\"start\":36187},{\"end\":36205,\"start\":36195},{\"end\":36218,\"start\":36205},{\"end\":36227,\"start\":36218},{\"end\":36240,\"start\":36227},{\"end\":36253,\"start\":36240},{\"end\":36265,\"start\":36253},{\"end\":36609,\"start\":36593},{\"end\":36620,\"start\":36609},{\"end\":36635,\"start\":36620},{\"end\":36651,\"start\":36635},{\"end\":36920,\"start\":36908},{\"end\":36938,\"start\":36920},{\"end\":37155,\"start\":37142},{\"end\":37172,\"start\":37155},{\"end\":37188,\"start\":37172},{\"end\":37440,\"start\":37427},{\"end\":37451,\"start\":37440},{\"end\":37468,\"start\":37451},{\"end\":37481,\"start\":37468},{\"end\":37494,\"start\":37481},{\"end\":37510,\"start\":37494},{\"end\":37831,\"start\":37820},{\"end\":37843,\"start\":37831},{\"end\":37854,\"start\":37843},{\"end\":37866,\"start\":37854},{\"end\":37878,\"start\":37866},{\"end\":38232,\"start\":38217},{\"end\":38245,\"start\":38232},{\"end\":38253,\"start\":38245},{\"end\":38265,\"start\":38253},{\"end\":38278,\"start\":38265},{\"end\":38291,\"start\":38278},{\"end\":38302,\"start\":38291},{\"end\":38309,\"start\":38302},{\"end\":38672,\"start\":38658},{\"end\":38687,\"start\":38672},{\"end\":38701,\"start\":38687},{\"end\":38714,\"start\":38701},{\"end\":38727,\"start\":38714},{\"end\":38738,\"start\":38727},{\"end\":39046,\"start\":39033},{\"end\":39054,\"start\":39046},{\"end\":39069,\"start\":39054},{\"end\":39081,\"start\":39069},{\"end\":39091,\"start\":39081},{\"end\":39103,\"start\":39091},{\"end\":39116,\"start\":39103},{\"end\":39413,\"start\":39398},{\"end\":39429,\"start\":39413},{\"end\":39447,\"start\":39429},{\"end\":39725,\"start\":39707},{\"end\":39742,\"start\":39725},{\"end\":39755,\"start\":39742},{\"end\":39768,\"start\":39755},{\"end\":40039,\"start\":40019},{\"end\":40055,\"start\":40039},{\"end\":40341,\"start\":40331},{\"end\":40352,\"start\":40341},{\"end\":40365,\"start\":40352},{\"end\":40379,\"start\":40365},{\"end\":40392,\"start\":40379},{\"end\":40407,\"start\":40392},{\"end\":40418,\"start\":40407},{\"end\":40799,\"start\":40789},{\"end\":40807,\"start\":40799},{\"end\":40819,\"start\":40807},{\"end\":40829,\"start\":40819},{\"end\":40842,\"start\":40829},{\"end\":40853,\"start\":40842},{\"end\":40860,\"start\":40853},{\"end\":41206,\"start\":41192},{\"end\":41220,\"start\":41206},{\"end\":41235,\"start\":41220},{\"end\":41247,\"start\":41235},{\"end\":41266,\"start\":41247},{\"end\":41282,\"start\":41266},{\"end\":41615,\"start\":41603},{\"end\":41624,\"start\":41615},{\"end\":41638,\"start\":41624},{\"end\":41650,\"start\":41638},{\"end\":41658,\"start\":41650},{\"end\":41671,\"start\":41658},{\"end\":41981,\"start\":41969},{\"end\":41993,\"start\":41981},{\"end\":42010,\"start\":41993},{\"end\":42260,\"start\":42251},{\"end\":42267,\"start\":42260},{\"end\":42280,\"start\":42267},{\"end\":42294,\"start\":42280},{\"end\":42305,\"start\":42294},{\"end\":42594,\"start\":42579},{\"end\":42610,\"start\":42594},{\"end\":42626,\"start\":42610},{\"end\":42923,\"start\":42914},{\"end\":42934,\"start\":42923},{\"end\":42947,\"start\":42934},{\"end\":42962,\"start\":42947},{\"end\":42974,\"start\":42962},{\"end\":42988,\"start\":42974},{\"end\":42997,\"start\":42988},{\"end\":43301,\"start\":43288},{\"end\":43324,\"start\":43301},{\"end\":43593,\"start\":43581},{\"end\":43609,\"start\":43593},{\"end\":43627,\"start\":43609},{\"end\":43641,\"start\":43627},{\"end\":43656,\"start\":43641},{\"end\":43670,\"start\":43656},{\"end\":44035,\"start\":44014},{\"end\":44043,\"start\":44035},{\"end\":44057,\"start\":44043},{\"end\":44071,\"start\":44057},{\"end\":44081,\"start\":44071},{\"end\":44386,\"start\":44372},{\"end\":44402,\"start\":44386},{\"end\":44415,\"start\":44402},{\"end\":44672,\"start\":44660},{\"end\":44682,\"start\":44672},{\"end\":44695,\"start\":44682},{\"end\":44957,\"start\":44944},{\"end\":44971,\"start\":44957},{\"end\":44984,\"start\":44971},{\"end\":44995,\"start\":44984},{\"end\":45283,\"start\":45267},{\"end\":45296,\"start\":45283},{\"end\":45308,\"start\":45296},{\"end\":45323,\"start\":45308},{\"end\":45332,\"start\":45323},{\"end\":45343,\"start\":45332},{\"end\":45642,\"start\":45631},{\"end\":45658,\"start\":45642},{\"end\":45667,\"start\":45658},{\"end\":45678,\"start\":45667},{\"end\":46011,\"start\":45999},{\"end\":46020,\"start\":46011},{\"end\":46034,\"start\":46020},{\"end\":46046,\"start\":46034},{\"end\":46054,\"start\":46046},{\"end\":46068,\"start\":46054},{\"end\":46083,\"start\":46068},{\"end\":46416,\"start\":46404},{\"end\":46433,\"start\":46416},{\"end\":46444,\"start\":46433},{\"end\":46462,\"start\":46444},{\"end\":46477,\"start\":46462},{\"end\":46772,\"start\":46760},{\"end\":46781,\"start\":46772},{\"end\":46795,\"start\":46781},{\"end\":46810,\"start\":46795},{\"end\":47063,\"start\":47045},{\"end\":47080,\"start\":47063},{\"end\":47288,\"start\":47271},{\"end\":47302,\"start\":47288},{\"end\":47315,\"start\":47302},{\"end\":47330,\"start\":47315},{\"end\":47341,\"start\":47330},{\"end\":47635,\"start\":47618},{\"end\":47645,\"start\":47635},{\"end\":47654,\"start\":47645},{\"end\":47668,\"start\":47654},{\"end\":47685,\"start\":47668},{\"end\":47696,\"start\":47685},{\"end\":47707,\"start\":47696},{\"end\":48014,\"start\":48004},{\"end\":48025,\"start\":48014},{\"end\":48035,\"start\":48025},{\"end\":48051,\"start\":48035},{\"end\":48062,\"start\":48051}]", "bib_venue": "[{\"end\":32319,\"start\":32297},{\"end\":32737,\"start\":32700},{\"end\":33227,\"start\":33189},{\"end\":33561,\"start\":33500},{\"end\":33957,\"start\":33840},{\"end\":34478,\"start\":34457},{\"end\":35093,\"start\":35056},{\"end\":35536,\"start\":35532},{\"end\":35950,\"start\":35936},{\"end\":36302,\"start\":36265},{\"end\":36655,\"start\":36651},{\"end\":36960,\"start\":36938},{\"end\":37210,\"start\":37188},{\"end\":37547,\"start\":37510},{\"end\":37915,\"start\":37878},{\"end\":38346,\"start\":38309},{\"end\":38760,\"start\":38738},{\"end\":39031,\"start\":38954},{\"end\":39483,\"start\":39447},{\"end\":39773,\"start\":39768},{\"end\":40088,\"start\":40055},{\"end\":40455,\"start\":40418},{\"end\":40897,\"start\":40860},{\"end\":41319,\"start\":41282},{\"end\":41693,\"start\":41671},{\"end\":42032,\"start\":42010},{\"end\":42342,\"start\":42305},{\"end\":42663,\"start\":42626},{\"end\":43022,\"start\":42997},{\"end\":43361,\"start\":43324},{\"end\":43707,\"start\":43670},{\"end\":44103,\"start\":44081},{\"end\":44437,\"start\":44415},{\"end\":44717,\"start\":44695},{\"end\":45017,\"start\":44995},{\"end\":45365,\"start\":45343},{\"end\":45715,\"start\":45678},{\"end\":46120,\"start\":46083},{\"end\":46499,\"start\":46477},{\"end\":46847,\"start\":46810},{\"end\":47378,\"start\":47341},{\"end\":47729,\"start\":47707},{\"end\":48084,\"start\":48062}]"}}}, "year": 2023, "month": 12, "day": 17}