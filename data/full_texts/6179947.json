{"id": 6179947, "updated": "2023-11-11 01:19:03.46", "metadata": {"title": "Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning", "authors": "[{\"first\":\"Tiancheng\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Maxine\",\"last\":\"Eskenazi\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue", "publication_date": {"year": 2016, "month": null, "day": null}, "abstract": "This paper presents an end-to-end framework for task-oriented dialog systems using a variant of Deep Recurrent Q-Networks (DRQN). The model is able to interface with a relational database and jointly learn policies for both language understanding and dialog strategy. Moreover, we propose a hybrid algorithm that combines the strength of reinforcement learning and supervised learning to achieve faster learning speed. We evaluated the proposed model on a 20 Question Game conversational game simulator. Results show that the proposed method outperforms the modular-based baseline and learns a distributed representation of the latent dialog state.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2962776342", "acl": "W16-3601", "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/ZhaoE16", "doi": "10.18653/v1/w16-3601"}}, "content": {"source": {"pdf_hash": "ed5666952c22b27debaa85b75baae12e4d97465c", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/W16-3601.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/W16-3601.pdf", "status": "HYBRID"}}, "grobid": {"id": "33c54e0bb9ae256983313a4d05c3f756c8f8c1df", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ed5666952c22b27debaa85b75baae12e4d97465c.txt", "contents": "\nTowards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning\nAssociation for Computational LinguisticsCopyright Association for Computational LinguisticsSeptember 2016. 2016\n\nTiancheng Zhao tianchez@cs.cmu.edu \nLanguage Technologies Institute Carnegie Mellon University\n\n\nMaxine Eskenazi \nLanguage Technologies Institute Carnegie Mellon University\n\n\nTowards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning\n\nProceedings of the SIGDIAL 2016 Conference\nthe SIGDIAL 2016 ConferenceLos Angeles, USAAssociation for Computational LinguisticsSeptember 2016. 2016\nThis paper presents an end-to-end framework for task-oriented dialog systems using a variant of Deep Recurrent Q-Networks (DRQN). The model is able to interface with a relational database and jointly learn policies for both language understanding and dialog strategy. Moreover, we propose a hybrid algorithm that combines the strength of reinforcement learning and supervised learning to achieve faster learning speed. We evaluated the proposed model on a 20 Question Game conversational game simulator. Results show that the proposed method outperforms the modular-based baseline and learns a distributed representation of the latent dialog state.\n\nIntroduction\n\nTask-oriented dialog systems have been an important branch of spoken dialog system (SDS) research (Raux et al., 2005;Young, 2006;Bohus and Rudnicky, 2003). The SDS agent has to achieve some predefined targets (e.g. booking a flight) through natural language interaction with the users. The typical structure of a task-oriented dialog system is outlined in Figure 1 (Young, 2006). This pipeline consists of several independently-developed modules: natural language understanding (the NLU) maps the user utterances to some semantic representation. This information is further processed by the dialog state tracker (DST), which accumulates the input of the turn along with the dialog history. The DST outputs the current dialog state and the dialog policy selects the next system action based on the dialog state. Then natural language generation (NLG) maps the selected action to its surface form which is sent to the TTS (Text-to-Speech). This process repeats until the agent's goal is satisfied. The conventional SDS pipeline has limitations. The first issue is the credit assignment problem. Developers usually only get feedback from the end users, who inform them about system performance quality. Determining the source of the error requires tedious error analysis in each module because errors from upstream modules can propagate to the rest of the pipeline. The second limitation is process interdependence, which makes online adaptation challenging. For example, when one module (e.g. NLU) is retrained with new data, all the others (e.g DM) that depend on it become sub-optimal due to the fact that they were trained on the output distributions of the older version of the module. Although the ideal solution is to retrain the entire pipeline to ensure global optimality, this requires significant human effort.\n\nDue to these limitations, the goal of this study is to develop an end-to-end framework for taskoriented SDS that replaces 3 important modules: the NLU, the DST and the dialog policy with a single module that can be jointly optimized. Developing such a model for task-oriented dialog sys-tems faces several challenges. The foremost challenge is that a task-oriented system must learn a strategic dialog policy that can achieve the goal of a given task which is beyond the ability of standard supervised learning (Li et al., 2014). The second challenge is that often a task-oriented agent needs to interface with structured external databases, which have symbolic query formats (e.g. SQL query). In order to find answers to the users' requests from the databases, the agent must formulate a valid database query. This is difficult for conventional neural network models which do not provide intermediate symbolic representations.\n\nThis paper describes a deep reinforcement learning based end-to-end framework for both dialog state tracking and dialog policy that addresses the above-mentioned issues. We evaluated the proposed approach on a conversational game simulator that requires both language understanding and strategic planning. Our studies yield promising results 1) in jointly learning policies for state tracking and dialog strategies that are superior to a modular-based baseline, 2) in efficiently incorporating various types of labelled data and 3) in learning dialog state representations.\n\nSection 2 of the paper discusses related work; Section 3 reviews the basics of deep reinforcement learning; Section 4 describes the proposed framework; Section 5 gives experimental results and model analysis; and Section 6 concludes.\n\n\nRelated Work\n\nDialog State Tracking: The process of constantly representing the state of the dialog is called dialog state tracking (DST). Most industrial systems use rule-based heuristics to update the dialog state by selecting a high-confidence output from the NLU (Williams et al., 2013). Numerous advanced statistical methods have been proposed to exploit the correlation between turns to make the system more robust given the uncertainty of the automatic speech recognition (ASR) and the NLU (Bohus and Rudnicky, 2006;. The Dialog State Tracking Challenge (DSTC) (Williams et al., 2013) formalizes the problem as a supervised sequential labelling task where the state tracker estimates the true slot values based on a sequence of NLU outputs. In practice the output of the state tracker is used by a different dialog policy, so that the distribution in the training data and in the live data are mis-matched (Williams et al., 2013). Therefore one of the basic assumptions of DSTC is that the state tracker's performance will translate to better dialog policy performance. Lee (2014) showed positive results following this assumption by showing a positive correlation between end-to-end dialog performance and state tracking performance.\n\nReinforcement Learning (RL): RL has been a popular approach for learning the optimal dialog policy of a task-oriented dialog system (Singh et al., 2002;Williams and Young, 2007;Georgila and Traum, 2011;Lee and Eskenazi, 2012). A dialog policy is formulated as a Partially Observable Markov Decision Process (POMDP) which models the uncertainty existing in both the users' goals and the outputs of the ASR and the NLU. Williams (2007) showed that POMDP-based systems perform significantly better than rule-based systems especially when the ASR word error rate (WER) is high. Other work has explored methods that improve the amount of training data needed for a POMDP-based dialog manager. Ga\u0161i\u0107 (2010) utilized Gaussian Process RL algorithms and greatly reduced the data needed for training. Existing applications of RL to dialog management assume a given dialog state representation. Instead, our approach learns its own dialog state representation from the raw dialogs along with a dialog policy in an end-to-end fashion.\n\nEnd-to-End SDSs: There have been many attempts to develop end-to-end chat-oriented dialog systems that can directly map from the history of a conversation to the next system response (Vinyals and Le, 2015;Serban et al., 2015;Shang et al., 2015). These methods train sequence-to-sequence models (Sutskever et al., 2014) on large humanhuman conversation corpora. The resulting models are able to do basic chatting with users. The work in this paper differs from them by focusing on building a task-oriented system that can interface with structured databases and provide real information to users.\n\nRecently, Wen el al. (2016) introduced a network-based end-to-end trainable taskedoriented dialog system. Their approach treated a dialog system as a mapping problem between the dialog history and the system response. They learned this mapping via a novel variant of the encoder-decoder model. The main differences between our models and theirs are that ours has the advantage of learning a strategic plan using RL and jointly optimizing state tracking beyond standard supervised learning.\n\n\nDeep Reinforcement Learning\n\nBefore describing the proposed algorithms, we briefly review deep reinforcement learning (RL). RL models are based on the Markov Decision Process (MDP). An MDP is a tuple (S, A, P, \u03b3, R), where S is a set of states; A is a set of actions; P defines the transition probability P (s |s, a); R defines the expected immediate reward R(s, a); and \u03b3 \u2208 [0, 1) is the discounting factor. The goal of reinforcement learning is to find the optimal policy \u03c0 * , such that the expected cumulative return is maximized (Sutton and Barto, 1998). MDPs assume full observability of the internal states of the world, which is rarely true for real-world applications. The Partially Observable Markov Decision Process (POMDP) takes the uncertainty in the state variable into account. A POMDP is defined by a tuple (S, A, P, \u03b3, R, O, Z). O is a set of observations and Z defines an observation probability P (o|s, a). The other variables are the same as the ones in MDPs. Solving a POMDP usually requires computing the belief state b(s), which is the probability distribution of all possible states, such that s b(s) = 1. It has been shown that the belief state is sufficient for optimal control (Monahan, 1982), so that the objective is to find \u03c0 * : b \u2192 a that maximizes the expected future return.\n\n\nDeep Q-Network\n\nThe deep Q-Network (DQN) introduced by Mnih (2015) uses a deep neural network (DNN) to parametrize the Q-value function Q(s, a; \u03b8) and achieves human-level performance in playing many Atari games. DQN keeps two separate models: a target network \u03b8 \u2212 i and a behavior network \u03b8 i . For every K new samples, DQN uses \u03b8 \u2212 i to compute the target values y DQN and updates the parameters in \u03b8 i . Only after every C updates, the new weights of \u03b8 i are copied over to \u03b8 \u2212 i . Furthermore, DQN utilizes experience replay to store all previous experience tuples (s, a, r, s ). Before a new model update, the algorithm samples a minibatch of experiences of size M from the memory and computes the gradient of the following loss function:\nL(\u03b8 i ) = E (s,a,r,s ) [(y DQN \u2212 Q(s, a; \u03b8 i )) 2 ] (1) y DQN = r + \u03b3 max a Q(s , a ; \u03b8 \u2212 i )(2)\nRecently, Hasselt et al. (2015) leveraged the overestimation problem of standard Q-Learning by introducing double DQN and Schaul et al. (2015) improves the convergence speed of DQN via prioritized experience replay. We found both modifications useful and included them in our studies.\n\n\nDeep Recurrent Q-Network\n\nAn extension to DQN is a Deep Recurrent Q-Network (DRQN) which introduces a Long Short-Term Memory (LSTM) layer (Hochreiter and Schmidhuber, 1997) on top of the convolutional layer of the original DQN model (Hausknecht and Stone, 2015) which allows DRQN to solve POMDPs. The recurrent neural network can thus be viewed as an approximation of the belief state that can aggregate information from a sequence of observations. Hausknecht (2015) shows that DRQN performs significantly better than DQN when an agent only observes partial states. A similar model was proposed by Narasimhan and Kulkarni (2015) and learns to play Multi-User Dungeon (MUD) games (Curtis, 1992) with game states hidden in natural language paragraphs.\n\n4 Proposed Model\n\n\nOverview\n\nEnd-to-end learning refers to models that can back-propagate error signals from the end output to the raw inputs. Prior work in end-to-end state tracking (Henderson et al., 2014) learns a sequential classifier that estimates the dialog state based on ASR output without the need of an NLU. Instead of treating state tracking as a standard supervised learning task, we propose to unify dialog state tracking with the dialog policy so that both are treated as actions available to a reinforcement learning agent. Specifically, we learn an optimal policy that either generates a verbal response or modifies the current estimated dialog state based on the new observations. This formulation makes it possible to obtain a state tracker even without the labelled data required for DSTC, as long as the rewards from the users and the databases are available. Furthermore, in cases where dialog state tracking labels are available, the proposed model can incorporate them with minimum modification and greatly accelerate its learning speed. Thus, the following sections describe two models: RL and Hybrid-RL, corresponding to two labelling scenarios: 1) only dialog success labels and 2) dialog success labels with state tracking labels. Figure 2 shows an overview of the framework. We consider a task-oriented dialog task, in which there are S slots, each with cardinality C i , i \u2208 [0, S).\n\n\nLearning from the Users and Databases\n\nThe environment consists of a user, E u and a database E db . The agent can send verbal actions, a v \u2208 A v to the user and the user will reply with natural language responses o u and rewards r u . In order to interface with the database environment E db , the agent can apply special actions a h \u2208 A h that can modify a query hypothesis h. The hypothesis is a slot-filling form that represents the most likely slot values given the observed evidence. Given this hypothesis, h, the database can perform a normal query and give the results as observations, o db and rewards r db . At each turn t, the agent applies its selected action a t \u2208 {A v , A h } and receives the observations from either the user or the database. We can then define the observation o t of turn t as,\no t = \uf8ee \uf8f0 a t o u t o db t \uf8f9 \uf8fb(3)\nWe then use the LSTM network as the dialog state tracker that is capable of aggregating information over turns and generating a dialog state represen-\ntation, b t = LST M (o t , b t\u22121 ),\nwhere b t is an approximation of the belief state at turn t. Finally, the dialog state representation from the LSTM network is the input to S + 1 policy networks implemented as Multilayer Perceptrons (MLP). The first policy network approximates the Q-value function for all verbal actions Q(b t , a v ) while the rest estimate the Q-value function for each slot, Q(b t , a h ), as shown in Figure 3. \n\n\nIncorporating State Tracking Labels\n\nThe pure RL approach described in the previous section could suffer from slow convergence when the cardinality of slots is large. This is due to the nature of reinforcement learning: that it has to try different actions (possible values of a slot) in order to estimate the expected long-term payoff. On the other hand, a supervised classifier can learn much more efficiently. A typical multi-class classification loss function (e.g. categorical cross entropy) assumes that there is a single correct label such that it encourages the probability of the correct label and suppresses the probabilities of the all the wrong ones. Modeling dialog state tracking as a Q-value function has advantages over a local classifier. For instance, take the situation where a user wants to send an email and the state tracker needs to estimate the user's goal from among three possible values: send, edit and delete. In a classification task, all the incorrect labels (edit, delete) are treated as equally undesirable. However, the cost of mistakenly recognizing the user goal as delete is much larger than edit, which can only be learned from the future rewards. In order to train the slot-filling policy with both short-term and long-term supervision signals, we decompose the reward function for A h into two parts:\nQ \u03c0 (b, a h ) =R(b, a) + \u03b3 b P (b |b, a h )V \u03c0 (b )(4)R(b, a, b ) = R(b, a h ) + P (a h |b)(5)\nwhere P (a h |b) is the conditional probability that the correct label of the slots is a h given the cur-rent belief state. In practice, instead of training a separate model estimating P (a h |b), we can replace P (a h |b) by 1(y = a h ) as the sample reward r, where y is the label. Furthermore, a key observation is that although it is expensive to collect data from the user E u , one can easily sample trajectories of interaction with the database since P (b |b, a h ) is known. Therefore, we can accelerate learning by generating synthetic experiences, i.e. tuple (b, a h , r, b )\u2200a h \u2208 A h and add them to the experience replay buffer. This approach is closely related to the Dyna Q-Learning proposed in (Sutton, 1990). The difference is that Dyna Qlearning uses the estimated environment dynamics to generating experiences, while our method only uses the known transition function (i.e. the dynamics of the database) to generate synthetic samples.\n\n\nImplementation Details\n\nWe can optimize the network architecture in several ways to improve its efficiency: Shared State Tracking Policies: it is more efficient to tie the weights of the policy networks for similar slots and use the index of slot as an input. This can reduce the number of parameters that needs to be learned and encourage shared structures. The studies in Section 5 illustrate one example.\n\nConstrained Action Mask: We can constrain the available actions at each turn to force the agent to alternate between verbal response and slot-filling. We define A mask as a function that takes state s and outputs a set of available actions for:\n\nA mask (s) = A h new inputs from the user (6)\n= A v otherwise(7)\nReward Shaping based on the Database: the reward signals from the users are usually sparse (at the end of a dialog), the database, however, can provide frequent rewards to the agent. Reward shaping is a technique used to speed up learning. Ng et al. (1999) showed that potential-based reward shaping does not alter the optimal solution; it only impacts the learning speed. The pseudo reward function F (s, a, s ) is defined as: R(s, a, s ) = R(s, a, s ) + F (s, a, s ) (8)\nF (s, a, s ) = \u03b3\u03c6(s ) \u2212 \u03c6(s)(9)\nLet the total number of entities in the database be D and P max be the max potential, the potential \u03c6(s) is:\n\u03c6(s t ) = P max (1 \u2212 d t D ) if d t > 0 (10) \u03c6(s t ) = 0 if d t = 0(11)\nThe intuition of this potential function is to encourage the agent to narrow down the possible range of valid entities as quickly as possible. Meanwhile, if no entities are consistent with the current hypothesis, this implies that there are mistakes in previous slot filling, which gives a potential of 0.\n\n\nExperiments\n\n\n20Q Game as Task-oriented Dialog\n\nIn order to test the proposed framework, we chose the 20 Question Game (20Q). The game rules are as follows: at the beginning of each game, the user thinks of a famous person. Then the agent asks the user a series of Yes/No questions. The user honestly answers, using one of three answers: yes, no or I don't know. In order to have this resemble a dialog, our user can answer with any natural utterance representing one of the three intents. The agent can make guesses at any turn, but a wrong guess results in a negative reward. The goal is to guess the correct person within a maximum number of turns with the least number of wrong guesses. An example game conversation is as follows:\n\nSys: Is this person male? User: Yes I think so. Sys: Is this person an artist? User: He is not an artist. ... Sys: I guess this person is Bill Gates. User: Correct.\n\nWe can formulate the game as a slot-filling dialog. Assume the system has |Q| available questions to select from at each turn. The answer to each question becomes a slot and each slot has three possible values: yes/no/unknown. Due to the length limit and wrong guess penalty, the optimal policy does not allow the agent to ask all of the questions regardless of the context or guess every person in the database one by one.\n\n\nSimulator Construction\n\nWe constructed a simulator for 20Q. The simulator has two parts: a database of 100 famous people and a user simulator.\n\nWe selected 100 people from Freebase (Bollacker et al., 2008), each of them has 6 attributes: birthplace, degree, gender, profession and birthday. We manually designed several Yes/No questions for each attribute that is available to the agent. Each question covers a different set of possible values for a given attribute and thus carries a different discriminative power to pinpoint the person that the user is thinking of. As a result, the agent needs to judiciously select the question, given the context of the game, in order to narrow down the range of valid people. There are 31 questions. At the beginning of each game, the simulator will first uniformly sample a person from the database as the person it is thinking of. Also there is a 5% chance that the simulator will consider unknown as an attribute and thus it will answer with unknown intent for any question related to it. After the game begins, when the agent asks a question, the simulator first determines the answer (yes, no or unknown) and replies using natural language. In order to generate realistic natural language with the yes/no/unknown intent, we collected utterances from the Switchboard Dialog Act (SWDA) Corpus (Jurafsky et al., 1997). Table 2 presents the mapping from the SWDA dialog acts to yes/no/unknown. We further post-processed results and removed irrelevant utterances, which led to 508, 445 and 251 unique utterances with intent respectively yes/no/unknown. We keep the frequency counts for each unique expression. Thus at run time, the simulator can sample a response according to the original distribution in the SWDA Corpus.\n\n\nIntent\n\n\nSWDA tags Yes\n\nAgree, Yes answers, Affirmative non-yes answers No\n\nNo answers, Reject, Negative non-no answers Unknown Maybe, Other Answer Table 2: Dialog act mapping from SWDA to yes/no/unknown A game is terminated when one of the four conditions is fulfilled: 1) the agent guesses the correct answer, 2) there are no people in the database consistent with the current hypothesis, 3) the max game length (100 steps) is reached and 4) the max number of guesses is reached (10 guesses). Only if the agent guesses the correct answer (condition 1) treated as a game victory. The win and loss rewards are 30 and \u221230 and a wrong guess leads to a \u22125 penalty.\n\n\nTraining Details\n\nThe user environment E u is the simulator that only accepts verbal actions, either a Yes/No question or a guess, and replies with a natural language utterance. Therefore A v contains |Q| + 1 actions, in which the first |Q| actions are questions and the last action makes a guess, given the results from the database.\n\nThe database environment reads in a query hypothesis h and returns a list of people that satisfy the constraints in the query. h has a size of |Q| and each dimension can be one of the three values: yes/no/unknown. Since the cardinality for all slots is the same, we only need 1 slot-filling policy network with 3 Q-value outputs for yes/no/unknown, to modify the value of the latest asked question, which is the shared policy approach mentioned in Section 4. Thus A h = {yes, no, unknown}. For example, considering Q = 3 and the hypothesis h is: [unknown, unknown, unknown]. If the latest asked question is Q 1 (1-based), then applying action a h = yes will result in the new hypothesis: [yes, unknown, unknown].\n\nTo represent the observation o t in vectorial form, we use a bag-of-bigrams feature vector to represent a user utterance; a one-hot vector to represent a system action and a single discrete number to represent the number of people satisfying the current hypothesis.\n\nThe hyper-parameters of the neural network model are as follows: the size of turn embedding is 30; the size of LSTMs is 256; each policy network has a hidden layer of 128 with tanh activation. We also add a dropout rate of 0.3 for both LSTMs and tanh layer outputs. The network has a total of 470,005 parameters. The network was trained through RM SP rop (Tieleman and Hinton, 2012). For hyper-parameters of DRQN, the behavior network was updated every 4 steps and the interval between each target network update C is 1000.greedy exploration is used for training, where is linearly decreased from 1 to 0.1. The reward shaping constant P max is 2 and the discounting factor \u03b3 is 0.99. The resulting network was evaluated every 5000 steps and the model was trained up to 120,000 steps. Each evaluation records the agent's performance with a greedy policy for 200 independent episodes.\n\n\nDialog Policy Analysis\n\nWe compare the performance of three models: a strong modular baseline, RL and Hybrid-RL. The baseline has an independently trained state tracker and dialog policy. The state tracker is also an LSTM-based classifier that inputs a dialog history and predicts the slot-value of the latest question. The dialog policy is a DRQN that assumes perfect slot-filling during training and simply controls the next verbal action. Thus the essential difference between the baseline and the proposed models is that the state tracker and dialog policy are not trained jointly. Also, since hybrid-RL effectively changes the reward function, the typical average cumulative reward metric is not applicable for performance comparison. Therefore, we directly compare the win rate and average game length in later discussions.   Table 3 shows that both proposed models achieve significantly higher win rate than the baseline by asking more questions before making guesses. Figure 4 illustrates the learning process of the three models. The horizontal axis is the total number of interaction between the agent and either the user or the database. The baseline model has the fastest learning speed but its performance saturated quickly because the dialog policy was not trained together with the state tracker. So the dialog policy is not aware of the uncertainty in slotfilling and the slot-filler does not distinguish between the consequences of different wrong labels (e.g classify yes to no versus to unknown). On the other hand, although RL reaches high performance at the end of the training, it struggles in the early stages and suffers from slow convergence. This is due to that fact that correct slot-filling is a prerequisite for winning 20Q, while the reward signal has a long delayed horizon in the RL approach. Finally, the hybrid-RL approach is able to converge to the optimal solution much faster than RL due to the fact that it efficiently exploits the information in the state tracking label. \n\n\nState Tracking Analysis\n\nOne of the hypotheses is that the RL approach can learn a good state tracker using only dialog success reward signals. We ran the best trained models using a greedy policy and collected 10,000 samples. cate that the RL model learns a completely dif-ferent strategy compared to the baseline. The RL model aims for high precision so that it predicts unknown when the input is ambiguous, which is a safer option than predicting yes/no, because confusing between yes and no may potentially lead to a contradiction and a game failure. This is very different from the baseline which does not distinguish between incorrect labels. Therefore, although the baseline achieves better classification metrics, it does not take into account the longterm payoff and performs sub-optimally in terms of overall performance.\n\n\nDialog State Representation Analysis\n\nTracking the state over multiple turns is crucial because the agent's optimal action depends on the history, e.g. the question it has already asked, the number of guesses it has spent. Furthermore, one of the assumptions is that the output of the LSTM network is an approximation of the belief state in the POMDP. We conducted two studies to test these hypotheses. For both studies, we ran the Hybrid-RL models saved at 20K, 50K and 100K steps against the simulator with a greedy policy and recorded 10,000 samples for each model.\n\nThe first study checks whether we can reconstruct an important state feature: the number of guesses the agent has made from the dialog state embedding. We divide the collected 10,000 samples into 80% for training and 20% for testing. We used the LSTM output as input features to a linear regression model with l2 regularization. Table 5 shows the correlation of determination r 2 increases for the model that was trained with more data.\n\n\nModel\n\n20K 50K 100K r 2 0.05 0.51 0.77 Table 5: r 2 of the linear regression for predicting the number of guesses in the test dataset.\n\nThe second study is a retrieval task. The latent state of the 20Q game is the true intent of the users' answers to all the questions that have been asked so far. Therefore, the true state vector, s has a size of 31 and each slot, s[k], k \u2208 [0, 31) is one of the four values: not yet asked, yes, no, unknown. Therefore, if the LSTM output b is in fact implicitly learning the distribution over this latent state s, they must be highly correlated for a well-trained model. Therefore, for each b i , i \u2208 [0, 10, 000), we find its nearest 5 neighbors based on cosine distance measuring and record their latent states, N (b i ) : B \u2192 [S]. Then we compute the empirical probability that each slot of the true state s differs from the retrieved neighbors:\np diff (s[k]) = E s i 4 n=0 1(N (b i )[n][k] = s i [k]) 5\n(12) where 1 is the indicator function, k is the slot index and n is the neighbor index. Figure 5: Performance of retrieving similar true dialog states using learned dialog state embedding. Figure 5 shows that the retrieval error continues to decrease for the model that was trained better, which confirms our assumption that the LSTM output is an approximation of the belief state.\n\n\nConclusion\n\nThis paper identifies the limitations of the conventional SDS pipeline and describes a novel end-toend framework for a task-oriented dialog system using deep reinforcement learning. We have assessed the model on the 20Q game. The proposed models show superior performance for both natural language understanding and dialog strategy. Furthermore, our analysis confirms our hypotheses that the proposed models implicitly capture essential information in the latent dialog states.\n\nOne limitation of the proposed approach is poor scalability due to the large number of samples needed for convergence. So future studies will include developing full-fledged task-orientated dialog systems and exploring methods to improve the sample efficiency. Also, investigating techniques that allow easy integration of domain knowledge so that the system can be more easily debugged and corrected is another important direction.\n\n\nAcknowledgements\n\nThis work was funded by NSF grant CNS-1512973. The opinions expressed in this paper do not necessarily reflect those of NSF. We would also like to thank Alan W Black for discussions on this paper.\n\nFigure 1 :\n1Conventional pipeline of an SDS. The proposed model replaces the modules in the dotted-line box with one end-to-end model.\n\nFigure 2 :\n2An overview of the proposed end-to-end task-oriented dialog management framework.\n\nFigure 3 :\n3The network takes the observation o t at turn t. The recurrent unit updates its hidden state based on both the history and the current turn embedding. Then the model outputs the Q-values for all actions. The policy network in grey is masked by the action mask\n\nFigure 4 :\n4Graphs showing the evolution of the win rate during training.\n\nTable 1 :\n1Table 1shows a summary. Is he/she an artist? Nationality 5 Is he/she a citizen of an Asian country? Summary of the available questions. Q a is the number of questions for attribute a.Attribute \nQ a Example Question \nBirthday \n3 Was he/she born before 1950? \nBirthplace 9 Was he/she born in USA? \nDegree \n4 Does he/she have a PhD? \nGender \n2 Is this person male? \nProfession 8 \n\nTable 3 :\n3Performance of the three systems\n\nTable 4 Table 4 :\n44reports the precision and recall of slot filling in these trajectories. The results indi-Hybrid-RL 0.54/0.60 0.98/0.92 0.94/0.93 State tracking performance of the three systems. The results are in the format of precision/recallUnknown Yes \nNo \nBaseline \n0.99/0.60 0.96/0.97 0.94/0.95 \nRL \n0.21/0.77 1.00/0.93 0.95/0.51 \n\n\nRavenclaw: Dialog management using hierarchical task decomposition and an expectation agenda. Dan Bohus, Alexander I Rudnicky, Dan Bohus and Alexander I Rudnicky. 2003. Raven- claw: Dialog management using hierarchical task decomposition and an expectation agenda.\n\nA k hypothe-ses+ otherbelief updating model. Dan Bohus, Alex Rudnicky, Proc. of the AAAI Workshop on Statistical and Empirical Methods in Spoken Dialogue Systems. of the AAAI Workshop on Statistical and Empirical Methods in Spoken Dialogue SystemsDan Bohus and Alex Rudnicky. 2006. A k hypothe- ses+ otherbelief updating model. In Proc. of the AAAI Workshop on Statistical and Empirical Meth- ods in Spoken Dialogue Systems.\n\nFreebase: a collaboratively created graph database for structuring human knowledge. Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, Jamie Taylor, Proceedings of the 2008 ACM SIGMOD international conference on Management of data. the 2008 ACM SIGMOD international conference on Management of dataACMKurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a col- laboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247-1250. ACM.\n\nMudding: Social phenomena in text-based virtual realities. High noon on the electronic frontier: Conceptual issues in cyberspace. Pavel Curtis, Pavel Curtis. 1992. Mudding: Social phenomena in text-based virtual realities. High noon on the elec- tronic frontier: Conceptual issues in cyberspace, pages 347-374.\n\nGaussian processes for fast policy optimisation of pomdp-based dialogue managers. M Ga\u0161i\u0107, Simon Jur\u010d\u00ed\u010dek, Fran\u00e7ois Keizer, Blaise Mairesse, Kai Thomson, Steve Yu, Young, Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue. the 11th Annual Meeting of the Special Interest Group on Discourse and DialogueAssociation for Computational LinguisticsM Ga\u0161i\u0107, F Jur\u010d\u00ed\u010dek, Simon Keizer, Fran\u00e7ois Mairesse, Blaise Thomson, Kai Yu, and Steve Young. 2010. Gaussian processes for fast policy optimisation of pomdp-based dialogue managers. In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 201-204. Association for Computational Linguistics.\n\nReinforcement learning of argumentation dialogue policies in negotiation. Kallirroi Georgila, David R Traum, INTERSPEECH. Kallirroi Georgila and David R Traum. 2011. Rein- forcement learning of argumentation dialogue poli- cies in negotiation. In INTERSPEECH, pages 2073- 2076.\n\nMatthew Hausknecht, Peter Stone, arXiv:1507.06527Deep recurrent q-learning for partially observable mdps. arXiv preprintMatthew Hausknecht and Peter Stone. 2015. Deep recurrent q-learning for partially observable mdps. arXiv preprint arXiv:1507.06527.\n\nWord-based dialog state tracking with recurrent neural networks. Matthew Henderson, Blaise Thomson, Steve Young, Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL). the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)Matthew Henderson, Blaise Thomson, and Steve Young. 2014. Word-based dialog state tracking with recurrent neural networks. In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 292- 299.\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. 98Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.\n\nSwitchboard swbd-damsl shallow-discoursefunction annotation coders manual. Institute of Cognitive. Dan Jurafsky, Elizabeth Shriberg, Debra Biasca, Science Technical Report. Dan Jurafsky, Elizabeth Shriberg, and Debra Biasca. 1997. Switchboard swbd-damsl shallow-discourse- function annotation coders manual. Institute of Cog- nitive Science Technical Report, pages 97-102.\n\nPomdpbased let's go system for spoken dialog challenge. Sungjin Lee, Maxine Eskenazi, Spoken Language Technology Workshop (SLT). IEEESungjin Lee and Maxine Eskenazi. 2012. Pomdp- based let's go system for spoken dialog challenge. In Spoken Language Technology Workshop (SLT), 2012 IEEE, pages 61-66. IEEE.\n\nExtrinsic evaluation of dialog state tracking and predictive metrics for dialog policy optimization. Sungjin Lee, 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue. 310Sungjin Lee. 2014. Extrinsic evaluation of dialog state tracking and predictive metrics for dialog policy op- timization. In 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue, page 310.\n\nTemporal supervised learning for inferring a dialog policy from example conversations. Lihong Li, He He, Jason D Williams, Spoken Language Technology Workshop (SLT). IEEELihong Li, He He, and Jason D Williams. 2014. Temporal supervised learning for inferring a dia- log policy from example conversations. In Spoken Language Technology Workshop (SLT), 2014 IEEE, pages 312-317. IEEE.\n\nHuman-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, G Marc, Alex Bellemare, Martin Graves, Andreas K Riedmiller, Georg Fidjeland, Ostrovski, Nature. 5187540Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidje- land, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement learning. Na- ture, 518(7540):529-533.\n\nState of the arta survey of partially observable markov decision processes: theory, models, and algorithms. George E Monahan, Management Science. 281George E Monahan. 1982. State of the arta survey of partially observable markov decision processes: the- ory, models, and algorithms. Management Science, 28(1):1-16.\n\nLanguage understanding for textbased games using deep reinforcement learning. Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay, arXiv:1506.08941arXiv preprintKarthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. 2015. Language understanding for text- based games using deep reinforcement learning. arXiv preprint arXiv:1506.08941.\n\nPolicy invariance under reward transformations: Theory and application to reward shaping. Y Andrew, Daishi Ng, Stuart Harada, Russell, ICML. 99Andrew Y Ng, Daishi Harada, and Stuart Russell. 1999. Policy invariance under reward transforma- tions: Theory and application to reward shaping. In ICML, volume 99, pages 278-287.\n\nLets go public! taking a spoken dialog system to the real world. Antoine Raux, Brian Langner, Dan Bohus, Alan W Black, Maxine Eskenazi, Proc. of Interspeech. of InterspeechCiteseerAntoine Raux, Brian Langner, Dan Bohus, Alan W Black, and Maxine Eskenazi. 2005. Lets go pub- lic! taking a spoken dialog system to the real world. In in Proc. of Interspeech 2005. Citeseer.\n\nTom Schaul, John Quan, Ioannis Antonoglou, David Silver, arXiv:1511.05952Prioritized experience replay. arXiv preprintTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. 2015. Prioritized experience replay. arXiv preprint arXiv:1511.05952.\n\nBuilding end-to-end dialogue systems using generative hierarchical neural network models. Alessandro Iulian V Serban, Yoshua Sordoni, Aaron Bengio, Joelle Courville, Pineau, arXiv:1507.04808arXiv preprintIulian V Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau. 2015. Build- ing end-to-end dialogue systems using generative hi- erarchical neural network models. arXiv preprint arXiv:1507.04808.\n\nLifeng Shang, Zhengdong Lu, Hang Li, arXiv:1503.02364Neural responding machine for short-text conversation. arXiv preprintLifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neural responding machine for short-text conversa- tion. arXiv preprint arXiv:1503.02364.\n\nOptimizing dialogue management with reinforcement learning: Experiments with the njfun system. Satinder Singh, Diane Litman, Michael Kearns, Marilyn Walker, Journal of Artificial Intelligence Research. Satinder Singh, Diane Litman, Michael Kearns, and Marilyn Walker. 2002. Optimizing dialogue man- agement with reinforcement learning: Experiments with the njfun system. Journal of Artificial Intelli- gence Research, pages 105-133.\n\nSequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, Advances in neural information processing systems. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural net- works. In Advances in neural information process- ing systems, pages 3104-3112.\n\nIntroduction to reinforcement learning. S Richard, Andrew G Sutton, Barto, MIT PressRichard S Sutton and Andrew G Barto. 1998. Intro- duction to reinforcement learning. MIT Press.\n\nIntegrated architectures for learning, planning, and reacting based on approximating dynamic programming. S Richard, Sutton, Proceedings of the seventh international conference on machine learning. the seventh international conference on machine learningRichard S Sutton. 1990. Integrated architectures for learning, planning, and reacting based on ap- proximating dynamic programming. In Proceedings of the seventh international conference on machine learning, pages 216-224.\n\nBayesian update of dialogue state: A pomdp framework for spoken dialogue systems. Blaise Thomson, Steve Young, Computer Speech & Language. 244Blaise Thomson and Steve Young. 2010. Bayesian update of dialogue state: A pomdp framework for spoken dialogue systems. Computer Speech & Lan- guage, 24(4):562-588.\n\nLecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. Tijmen Tieleman, Geoffrey Hinton, COURSERA: Neural Networks for Machine Learning. 4Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture 6.5-rmsprop: Divide the gradient by a running av- erage of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4:2.\n\nArthur Hado Van Hasselt, David Guez, Silver, arXiv:1509.06461Deep reinforcement learning with double qlearning. arXiv preprintHado Van Hasselt, Arthur Guez, and David Silver. 2015. Deep reinforcement learning with double q- learning. arXiv preprint arXiv:1509.06461.\n\nOriol Vinyals, Quoc Le, arXiv:1506.05869A neural conversational model. arXiv preprintOriol Vinyals and Quoc Le. 2015. A neural conversa- tional model. arXiv preprint arXiv:1506.05869.\n\nA network-based end-to-end trainable task-oriented dialogue system. Milica Tsung-Hsien Wen, Nikola Gasic, Lina M Mrksic, Pei-Hao Rojas-Barahona, Stefan Su, David Ultes, Steve Vandyke, Young, arXiv:1604.04562arXiv preprintTsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Lina M Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. 2016. A network-based end-to-end trainable task-oriented di- alogue system. arXiv preprint arXiv:1604.04562.\n\nPartially observable markov decision processes for spoken dialog systems. D Jason, Steve Williams, Young, Computer Speech & Language. 212Jason D Williams and Steve Young. 2007. Partially observable markov decision processes for spoken dialog systems. Computer Speech & Language, 21(2):393-422.\n\nDeepak Ramachandran, and Alan Black. Jason Williams, Antoine Raux, Proceedings of the SIGDIAL 2013 Conference. the SIGDIAL 2013 ConferenceThe dialog state tracking challengeJason Williams, Antoine Raux, Deepak Ramachan- dran, and Alan Black. 2013. The dialog state track- ing challenge. In Proceedings of the SIGDIAL 2013 Conference, pages 404-413.\n\nUsing pomdps for dialog management. J Steve, Young, SLT. Steve J Young. 2006. Using pomdps for dialog man- agement. In SLT, pages 8-13.\n", "annotations": {"author": "[{\"end\":314,\"start\":218},{\"end\":392,\"start\":315}]", "publisher": "[{\"end\":145,\"start\":104},{\"end\":624,\"start\":583}]", "author_last_name": "[{\"end\":232,\"start\":228},{\"end\":330,\"start\":322}]", "author_first_name": "[{\"end\":227,\"start\":218},{\"end\":321,\"start\":315}]", "author_affiliation": "[{\"end\":313,\"start\":254},{\"end\":391,\"start\":332}]", "title": "[{\"end\":103,\"start\":1},{\"end\":495,\"start\":393}]", "venue": "[{\"end\":539,\"start\":497}]", "abstract": "[{\"end\":1293,\"start\":645}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1426,\"start\":1407},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":1438,\"start\":1426},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1463,\"start\":1438},{\"end\":1687,\"start\":1665},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3657,\"start\":3640},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5159,\"start\":5136},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5392,\"start\":5366},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5460,\"start\":5437},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5805,\"start\":5782},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5956,\"start\":5946},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6264,\"start\":6244},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6289,\"start\":6264},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6314,\"start\":6289},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6337,\"start\":6314},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6545,\"start\":6530},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6812,\"start\":6800},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7341,\"start\":7319},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7361,\"start\":7341},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7380,\"start\":7361},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7454,\"start\":7430},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8783,\"start\":8771},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9444,\"start\":9429},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10408,\"start\":10387},{\"end\":10519,\"start\":10491},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10836,\"start\":10802},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10925,\"start\":10897},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11292,\"start\":11262},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11357,\"start\":11343},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11621,\"start\":11598},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16425,\"start\":16411},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17634,\"start\":17618},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19905,\"start\":19881},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21059,\"start\":21036},{\"end\":23038,\"start\":23011},{\"end\":23176,\"start\":23153},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23828,\"start\":23801}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30792,\"start\":30657},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30887,\"start\":30793},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31160,\"start\":30888},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31235,\"start\":31161},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31624,\"start\":31236},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31669,\"start\":31625},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":32011,\"start\":31670}]", "paragraph": "[{\"end\":3127,\"start\":1309},{\"end\":4056,\"start\":3129},{\"end\":4631,\"start\":4058},{\"end\":4866,\"start\":4633},{\"end\":6110,\"start\":4883},{\"end\":7134,\"start\":6112},{\"end\":7731,\"start\":7136},{\"end\":8222,\"start\":7733},{\"end\":9533,\"start\":8254},{\"end\":10279,\"start\":9552},{\"end\":10661,\"start\":10377},{\"end\":11413,\"start\":10690},{\"end\":11431,\"start\":11415},{\"end\":12827,\"start\":11444},{\"end\":13641,\"start\":12869},{\"end\":13826,\"start\":13676},{\"end\":14263,\"start\":13863},{\"end\":15605,\"start\":14303},{\"end\":16655,\"start\":15701},{\"end\":17065,\"start\":16682},{\"end\":17311,\"start\":17067},{\"end\":17358,\"start\":17313},{\"end\":17850,\"start\":17378},{\"end\":17991,\"start\":17883},{\"end\":18369,\"start\":18064},{\"end\":19106,\"start\":18420},{\"end\":19272,\"start\":19108},{\"end\":19697,\"start\":19274},{\"end\":19842,\"start\":19724},{\"end\":21462,\"start\":19844},{\"end\":21539,\"start\":21489},{\"end\":22126,\"start\":21541},{\"end\":22463,\"start\":22147},{\"end\":23177,\"start\":22465},{\"end\":23444,\"start\":23179},{\"end\":24328,\"start\":23446},{\"end\":26342,\"start\":24355},{\"end\":27176,\"start\":26370},{\"end\":27747,\"start\":27217},{\"end\":28185,\"start\":27749},{\"end\":28322,\"start\":28195},{\"end\":29072,\"start\":28324},{\"end\":29513,\"start\":29131},{\"end\":30005,\"start\":29528},{\"end\":30439,\"start\":30007},{\"end\":30656,\"start\":30460}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10376,\"start\":10280},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13675,\"start\":13642},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13862,\"start\":13827},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15660,\"start\":15606},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15700,\"start\":15660},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17377,\"start\":17359},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17882,\"start\":17851},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18063,\"start\":17992},{\"attributes\":{\"id\":\"formula_8\"},\"end\":29130,\"start\":29073}]", "table_ref": "[{\"end\":21068,\"start\":21061},{\"end\":21620,\"start\":21613},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25170,\"start\":25163},{\"end\":28234,\"start\":28227}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1307,\"start\":1295},{\"attributes\":{\"n\":\"2\"},\"end\":4881,\"start\":4869},{\"attributes\":{\"n\":\"3\"},\"end\":8252,\"start\":8225},{\"attributes\":{\"n\":\"3.1\"},\"end\":9550,\"start\":9536},{\"attributes\":{\"n\":\"3.2\"},\"end\":10688,\"start\":10664},{\"attributes\":{\"n\":\"4.1\"},\"end\":11442,\"start\":11434},{\"attributes\":{\"n\":\"4.2\"},\"end\":12867,\"start\":12830},{\"attributes\":{\"n\":\"4.3\"},\"end\":14301,\"start\":14266},{\"attributes\":{\"n\":\"4.4\"},\"end\":16680,\"start\":16658},{\"attributes\":{\"n\":\"5\"},\"end\":18383,\"start\":18372},{\"attributes\":{\"n\":\"5.1\"},\"end\":18418,\"start\":18386},{\"attributes\":{\"n\":\"5.2\"},\"end\":19722,\"start\":19700},{\"end\":21471,\"start\":21465},{\"end\":21487,\"start\":21474},{\"attributes\":{\"n\":\"5.3\"},\"end\":22145,\"start\":22129},{\"attributes\":{\"n\":\"5.4\"},\"end\":24353,\"start\":24331},{\"attributes\":{\"n\":\"5.5\"},\"end\":26368,\"start\":26345},{\"attributes\":{\"n\":\"5.6\"},\"end\":27215,\"start\":27179},{\"end\":28193,\"start\":28188},{\"attributes\":{\"n\":\"6\"},\"end\":29526,\"start\":29516},{\"attributes\":{\"n\":\"7\"},\"end\":30458,\"start\":30442},{\"end\":30668,\"start\":30658},{\"end\":30804,\"start\":30794},{\"end\":30899,\"start\":30889},{\"end\":31172,\"start\":31162},{\"end\":31246,\"start\":31237},{\"end\":31635,\"start\":31626},{\"end\":31688,\"start\":31671}]", "table": "[{\"end\":31624,\"start\":31431},{\"end\":32011,\"start\":31918}]", "figure_caption": "[{\"end\":30792,\"start\":30670},{\"end\":30887,\"start\":30806},{\"end\":31160,\"start\":30901},{\"end\":31235,\"start\":31174},{\"end\":31431,\"start\":31248},{\"end\":31669,\"start\":31637},{\"end\":31918,\"start\":31691}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12682,\"start\":12674},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14261,\"start\":14253},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25315,\"start\":25307},{\"end\":29228,\"start\":29220},{\"end\":29329,\"start\":29321}]", "bib_author_first_name": "[{\"end\":32110,\"start\":32107},{\"end\":32127,\"start\":32118},{\"end\":32129,\"start\":32128},{\"end\":32327,\"start\":32324},{\"end\":32339,\"start\":32335},{\"end\":32793,\"start\":32789},{\"end\":32810,\"start\":32805},{\"end\":32825,\"start\":32818},{\"end\":32839,\"start\":32836},{\"end\":32853,\"start\":32848},{\"end\":33427,\"start\":33422},{\"end\":33687,\"start\":33686},{\"end\":33700,\"start\":33695},{\"end\":33719,\"start\":33711},{\"end\":33734,\"start\":33728},{\"end\":33748,\"start\":33745},{\"end\":33763,\"start\":33758},{\"end\":34415,\"start\":34406},{\"end\":34618,\"start\":34611},{\"end\":34636,\"start\":34631},{\"end\":34936,\"start\":34929},{\"end\":34954,\"start\":34948},{\"end\":34969,\"start\":34964},{\"end\":35449,\"start\":35445},{\"end\":35468,\"start\":35462},{\"end\":35713,\"start\":35710},{\"end\":35733,\"start\":35724},{\"end\":35749,\"start\":35744},{\"end\":36048,\"start\":36041},{\"end\":36060,\"start\":36054},{\"end\":36400,\"start\":36393},{\"end\":36792,\"start\":36786},{\"end\":36799,\"start\":36797},{\"end\":36809,\"start\":36804},{\"end\":36811,\"start\":36810},{\"end\":37149,\"start\":37140},{\"end\":37161,\"start\":37156},{\"end\":37180,\"start\":37175},{\"end\":37195,\"start\":37189},{\"end\":37197,\"start\":37196},{\"end\":37208,\"start\":37204},{\"end\":37218,\"start\":37217},{\"end\":37229,\"start\":37225},{\"end\":37247,\"start\":37241},{\"end\":37263,\"start\":37256},{\"end\":37265,\"start\":37264},{\"end\":37283,\"start\":37278},{\"end\":37989,\"start\":37982},{\"end\":38007,\"start\":38002},{\"end\":38024,\"start\":38018},{\"end\":38333,\"start\":38332},{\"end\":38348,\"start\":38342},{\"end\":38359,\"start\":38353},{\"end\":38639,\"start\":38632},{\"end\":38651,\"start\":38646},{\"end\":38664,\"start\":38661},{\"end\":38676,\"start\":38672},{\"end\":38678,\"start\":38677},{\"end\":38692,\"start\":38686},{\"end\":38942,\"start\":38939},{\"end\":38955,\"start\":38951},{\"end\":38969,\"start\":38962},{\"end\":38987,\"start\":38982},{\"end\":39289,\"start\":39279},{\"end\":39313,\"start\":39307},{\"end\":39328,\"start\":39323},{\"end\":39343,\"start\":39337},{\"end\":39621,\"start\":39615},{\"end\":39638,\"start\":39629},{\"end\":39647,\"start\":39643},{\"end\":39978,\"start\":39970},{\"end\":39991,\"start\":39986},{\"end\":40007,\"start\":40000},{\"end\":40023,\"start\":40016},{\"end\":40365,\"start\":40361},{\"end\":40382,\"start\":40377},{\"end\":40398,\"start\":40392},{\"end\":40675,\"start\":40674},{\"end\":40691,\"start\":40685},{\"end\":40693,\"start\":40692},{\"end\":40922,\"start\":40921},{\"end\":41381,\"start\":41375},{\"end\":41396,\"start\":41391},{\"end\":41694,\"start\":41688},{\"end\":41713,\"start\":41705},{\"end\":41963,\"start\":41957},{\"end\":41987,\"start\":41982},{\"end\":42230,\"start\":42225},{\"end\":42244,\"start\":42240},{\"end\":42484,\"start\":42478},{\"end\":42508,\"start\":42502},{\"end\":42520,\"start\":42516},{\"end\":42522,\"start\":42521},{\"end\":42538,\"start\":42531},{\"end\":42561,\"start\":42555},{\"end\":42571,\"start\":42566},{\"end\":42584,\"start\":42579},{\"end\":42943,\"start\":42942},{\"end\":42956,\"start\":42951},{\"end\":43205,\"start\":43200},{\"end\":43223,\"start\":43216},{\"end\":43550,\"start\":43549}]", "bib_author_last_name": "[{\"end\":32116,\"start\":32111},{\"end\":32138,\"start\":32130},{\"end\":32333,\"start\":32328},{\"end\":32348,\"start\":32340},{\"end\":32803,\"start\":32794},{\"end\":32816,\"start\":32811},{\"end\":32834,\"start\":32826},{\"end\":32846,\"start\":32840},{\"end\":32860,\"start\":32854},{\"end\":33434,\"start\":33428},{\"end\":33693,\"start\":33688},{\"end\":33709,\"start\":33701},{\"end\":33726,\"start\":33720},{\"end\":33743,\"start\":33735},{\"end\":33756,\"start\":33749},{\"end\":33766,\"start\":33764},{\"end\":33773,\"start\":33768},{\"end\":34424,\"start\":34416},{\"end\":34439,\"start\":34426},{\"end\":34629,\"start\":34619},{\"end\":34642,\"start\":34637},{\"end\":34946,\"start\":34937},{\"end\":34962,\"start\":34955},{\"end\":34975,\"start\":34970},{\"end\":35460,\"start\":35450},{\"end\":35480,\"start\":35469},{\"end\":35722,\"start\":35714},{\"end\":35742,\"start\":35734},{\"end\":35756,\"start\":35750},{\"end\":36052,\"start\":36049},{\"end\":36069,\"start\":36061},{\"end\":36404,\"start\":36401},{\"end\":36795,\"start\":36793},{\"end\":36802,\"start\":36800},{\"end\":36820,\"start\":36812},{\"end\":37154,\"start\":37150},{\"end\":37173,\"start\":37162},{\"end\":37187,\"start\":37181},{\"end\":37202,\"start\":37198},{\"end\":37215,\"start\":37209},{\"end\":37223,\"start\":37219},{\"end\":37239,\"start\":37230},{\"end\":37254,\"start\":37248},{\"end\":37276,\"start\":37266},{\"end\":37293,\"start\":37284},{\"end\":37304,\"start\":37295},{\"end\":37712,\"start\":37696},{\"end\":38000,\"start\":37990},{\"end\":38016,\"start\":38008},{\"end\":38033,\"start\":38025},{\"end\":38340,\"start\":38334},{\"end\":38351,\"start\":38349},{\"end\":38366,\"start\":38360},{\"end\":38375,\"start\":38368},{\"end\":38644,\"start\":38640},{\"end\":38659,\"start\":38652},{\"end\":38670,\"start\":38665},{\"end\":38684,\"start\":38679},{\"end\":38701,\"start\":38693},{\"end\":38949,\"start\":38943},{\"end\":38960,\"start\":38956},{\"end\":38980,\"start\":38970},{\"end\":38994,\"start\":38988},{\"end\":39305,\"start\":39290},{\"end\":39321,\"start\":39314},{\"end\":39335,\"start\":39329},{\"end\":39353,\"start\":39344},{\"end\":39361,\"start\":39355},{\"end\":39627,\"start\":39622},{\"end\":39641,\"start\":39639},{\"end\":39650,\"start\":39648},{\"end\":39984,\"start\":39979},{\"end\":39998,\"start\":39992},{\"end\":40014,\"start\":40008},{\"end\":40030,\"start\":40024},{\"end\":40375,\"start\":40366},{\"end\":40390,\"start\":40383},{\"end\":40401,\"start\":40399},{\"end\":40683,\"start\":40676},{\"end\":40700,\"start\":40694},{\"end\":40707,\"start\":40702},{\"end\":40930,\"start\":40923},{\"end\":40938,\"start\":40932},{\"end\":41389,\"start\":41382},{\"end\":41402,\"start\":41397},{\"end\":41703,\"start\":41695},{\"end\":41720,\"start\":41714},{\"end\":41980,\"start\":41964},{\"end\":41992,\"start\":41988},{\"end\":42000,\"start\":41994},{\"end\":42238,\"start\":42231},{\"end\":42247,\"start\":42245},{\"end\":42500,\"start\":42485},{\"end\":42514,\"start\":42509},{\"end\":42529,\"start\":42523},{\"end\":42553,\"start\":42539},{\"end\":42564,\"start\":42562},{\"end\":42577,\"start\":42572},{\"end\":42592,\"start\":42585},{\"end\":42599,\"start\":42594},{\"end\":42949,\"start\":42944},{\"end\":42965,\"start\":42957},{\"end\":42972,\"start\":42967},{\"end\":43214,\"start\":43206},{\"end\":43228,\"start\":43224},{\"end\":43556,\"start\":43551},{\"end\":43563,\"start\":43558}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":32277,\"start\":32013},{\"attributes\":{\"id\":\"b1\"},\"end\":32703,\"start\":32279},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":207167677},\"end\":33290,\"start\":32705},{\"attributes\":{\"id\":\"b3\"},\"end\":33602,\"start\":33292},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3083188},\"end\":34330,\"start\":33604},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1856002},\"end\":34609,\"start\":34332},{\"attributes\":{\"doi\":\"arXiv:1507.06527\",\"id\":\"b6\"},\"end\":34862,\"start\":34611},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":8021351},\"end\":35419,\"start\":34864},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1915014},\"end\":35609,\"start\":35421},{\"attributes\":{\"id\":\"b9\"},\"end\":35983,\"start\":35611},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":39627777},\"end\":36290,\"start\":35985},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":15680042},\"end\":36697,\"start\":36292},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":8713058},\"end\":37081,\"start\":36699},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":205242740},\"end\":37586,\"start\":37083},{\"attributes\":{\"id\":\"b14\"},\"end\":37902,\"start\":37588},{\"attributes\":{\"doi\":\"arXiv:1506.08941\",\"id\":\"b15\"},\"end\":38240,\"start\":37904},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5730166},\"end\":38565,\"start\":38242},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":281507},\"end\":38937,\"start\":38567},{\"attributes\":{\"doi\":\"arXiv:1511.05952\",\"id\":\"b18\"},\"end\":39187,\"start\":38939},{\"attributes\":{\"doi\":\"arXiv:1507.04808\",\"id\":\"b19\"},\"end\":39613,\"start\":39189},{\"attributes\":{\"doi\":\"arXiv:1503.02364\",\"id\":\"b20\"},\"end\":39873,\"start\":39615},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2198541},\"end\":40307,\"start\":39875},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7961699},\"end\":40632,\"start\":40309},{\"attributes\":{\"id\":\"b23\"},\"end\":40813,\"start\":40634},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":7962049},\"end\":41291,\"start\":40815},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":973845},\"end\":41599,\"start\":41293},{\"attributes\":{\"id\":\"b26\"},\"end\":41955,\"start\":41601},{\"attributes\":{\"doi\":\"arXiv:1509.06461\",\"id\":\"b27\"},\"end\":42223,\"start\":41957},{\"attributes\":{\"doi\":\"arXiv:1506.05869\",\"id\":\"b28\"},\"end\":42408,\"start\":42225},{\"attributes\":{\"doi\":\"arXiv:1604.04562\",\"id\":\"b29\"},\"end\":42866,\"start\":42410},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":13903063},\"end\":43161,\"start\":42868},{\"attributes\":{\"id\":\"b31\"},\"end\":43511,\"start\":43163},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":850480},\"end\":43648,\"start\":43513}]", "bib_title": "[{\"end\":32322,\"start\":32279},{\"end\":32787,\"start\":32705},{\"end\":33684,\"start\":33604},{\"end\":34404,\"start\":34332},{\"end\":34927,\"start\":34864},{\"end\":35443,\"start\":35421},{\"end\":35708,\"start\":35611},{\"end\":36039,\"start\":35985},{\"end\":36391,\"start\":36292},{\"end\":36784,\"start\":36699},{\"end\":37138,\"start\":37083},{\"end\":37694,\"start\":37588},{\"end\":38330,\"start\":38242},{\"end\":38630,\"start\":38567},{\"end\":39968,\"start\":39875},{\"end\":40359,\"start\":40309},{\"end\":40919,\"start\":40815},{\"end\":41373,\"start\":41293},{\"end\":41686,\"start\":41601},{\"end\":42940,\"start\":42868},{\"end\":43198,\"start\":43163},{\"end\":43547,\"start\":43513}]", "bib_author": "[{\"end\":32118,\"start\":32107},{\"end\":32140,\"start\":32118},{\"end\":32335,\"start\":32324},{\"end\":32350,\"start\":32335},{\"end\":32805,\"start\":32789},{\"end\":32818,\"start\":32805},{\"end\":32836,\"start\":32818},{\"end\":32848,\"start\":32836},{\"end\":32862,\"start\":32848},{\"end\":33436,\"start\":33422},{\"end\":33695,\"start\":33686},{\"end\":33711,\"start\":33695},{\"end\":33728,\"start\":33711},{\"end\":33745,\"start\":33728},{\"end\":33758,\"start\":33745},{\"end\":33768,\"start\":33758},{\"end\":33775,\"start\":33768},{\"end\":34426,\"start\":34406},{\"end\":34441,\"start\":34426},{\"end\":34631,\"start\":34611},{\"end\":34644,\"start\":34631},{\"end\":34948,\"start\":34929},{\"end\":34964,\"start\":34948},{\"end\":34977,\"start\":34964},{\"end\":35462,\"start\":35445},{\"end\":35482,\"start\":35462},{\"end\":35724,\"start\":35710},{\"end\":35744,\"start\":35724},{\"end\":35758,\"start\":35744},{\"end\":36054,\"start\":36041},{\"end\":36071,\"start\":36054},{\"end\":36406,\"start\":36393},{\"end\":36797,\"start\":36786},{\"end\":36804,\"start\":36797},{\"end\":36822,\"start\":36804},{\"end\":37156,\"start\":37140},{\"end\":37175,\"start\":37156},{\"end\":37189,\"start\":37175},{\"end\":37204,\"start\":37189},{\"end\":37217,\"start\":37204},{\"end\":37225,\"start\":37217},{\"end\":37241,\"start\":37225},{\"end\":37256,\"start\":37241},{\"end\":37278,\"start\":37256},{\"end\":37295,\"start\":37278},{\"end\":37306,\"start\":37295},{\"end\":37714,\"start\":37696},{\"end\":38002,\"start\":37982},{\"end\":38018,\"start\":38002},{\"end\":38035,\"start\":38018},{\"end\":38342,\"start\":38332},{\"end\":38353,\"start\":38342},{\"end\":38368,\"start\":38353},{\"end\":38377,\"start\":38368},{\"end\":38646,\"start\":38632},{\"end\":38661,\"start\":38646},{\"end\":38672,\"start\":38661},{\"end\":38686,\"start\":38672},{\"end\":38703,\"start\":38686},{\"end\":38951,\"start\":38939},{\"end\":38962,\"start\":38951},{\"end\":38982,\"start\":38962},{\"end\":38996,\"start\":38982},{\"end\":39307,\"start\":39279},{\"end\":39323,\"start\":39307},{\"end\":39337,\"start\":39323},{\"end\":39355,\"start\":39337},{\"end\":39363,\"start\":39355},{\"end\":39629,\"start\":39615},{\"end\":39643,\"start\":39629},{\"end\":39652,\"start\":39643},{\"end\":39986,\"start\":39970},{\"end\":40000,\"start\":39986},{\"end\":40016,\"start\":40000},{\"end\":40032,\"start\":40016},{\"end\":40377,\"start\":40361},{\"end\":40392,\"start\":40377},{\"end\":40403,\"start\":40392},{\"end\":40685,\"start\":40674},{\"end\":40702,\"start\":40685},{\"end\":40709,\"start\":40702},{\"end\":40932,\"start\":40921},{\"end\":40940,\"start\":40932},{\"end\":41391,\"start\":41375},{\"end\":41404,\"start\":41391},{\"end\":41705,\"start\":41688},{\"end\":41722,\"start\":41705},{\"end\":41982,\"start\":41957},{\"end\":41994,\"start\":41982},{\"end\":42002,\"start\":41994},{\"end\":42240,\"start\":42225},{\"end\":42249,\"start\":42240},{\"end\":42502,\"start\":42478},{\"end\":42516,\"start\":42502},{\"end\":42531,\"start\":42516},{\"end\":42555,\"start\":42531},{\"end\":42566,\"start\":42555},{\"end\":42579,\"start\":42566},{\"end\":42594,\"start\":42579},{\"end\":42601,\"start\":42594},{\"end\":42951,\"start\":42942},{\"end\":42967,\"start\":42951},{\"end\":42974,\"start\":42967},{\"end\":43216,\"start\":43200},{\"end\":43230,\"start\":43216},{\"end\":43558,\"start\":43549},{\"end\":43565,\"start\":43558}]", "bib_venue": "[{\"end\":32526,\"start\":32442},{\"end\":33011,\"start\":32945},{\"end\":33950,\"start\":33871},{\"end\":35172,\"start\":35083},{\"end\":38739,\"start\":38725},{\"end\":41069,\"start\":41013},{\"end\":43301,\"start\":43274},{\"end\":32105,\"start\":32013},{\"end\":32440,\"start\":32350},{\"end\":32943,\"start\":32862},{\"end\":33420,\"start\":33292},{\"end\":33869,\"start\":33775},{\"end\":34452,\"start\":34441},{\"end\":34715,\"start\":34660},{\"end\":35081,\"start\":34977},{\"end\":35500,\"start\":35482},{\"end\":35782,\"start\":35758},{\"end\":36112,\"start\":36071},{\"end\":36481,\"start\":36406},{\"end\":36863,\"start\":36822},{\"end\":37312,\"start\":37306},{\"end\":37732,\"start\":37714},{\"end\":37980,\"start\":37904},{\"end\":38381,\"start\":38377},{\"end\":38723,\"start\":38703},{\"end\":39041,\"start\":39012},{\"end\":39277,\"start\":39189},{\"end\":39721,\"start\":39668},{\"end\":40075,\"start\":40032},{\"end\":40452,\"start\":40403},{\"end\":40672,\"start\":40634},{\"end\":41011,\"start\":40940},{\"end\":41430,\"start\":41404},{\"end\":41768,\"start\":41722},{\"end\":42067,\"start\":42018},{\"end\":42294,\"start\":42265},{\"end\":42476,\"start\":42410},{\"end\":43000,\"start\":42974},{\"end\":43272,\"start\":43230},{\"end\":43568,\"start\":43565}]"}}}, "year": 2023, "month": 12, "day": 17}