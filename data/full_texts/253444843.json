{"id": 253444843, "updated": "2023-10-05 08:25:41.479", "metadata": {"title": "Which Pixel to Annotate: a Label-Efficient Nuclei Segmentation Framework", "authors": "[{\"first\":\"Wei\",\"last\":\"Lou\",\"middle\":[]},{\"first\":\"Haofeng\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Guanbin\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Xiaoguang\",\"last\":\"Han\",\"middle\":[]},{\"first\":\"Xiang\",\"last\":\"Wan\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Recently deep neural networks, which require a large amount of annotated samples, have been widely applied in nuclei instance segmentation of H\\&E stained pathology images. However, it is inefficient and unnecessary to label all pixels for a dataset of nuclei images which usually contain similar and redundant patterns. Although unsupervised and semi-supervised learning methods have been studied for nuclei segmentation, very few works have delved into the selective labeling of samples to reduce the workload of annotation. Thus, in this paper, we propose a novel full nuclei segmentation framework that chooses only a few image patches to be annotated, augments the training set from the selected samples, and achieves nuclei segmentation in a semi-supervised manner. In the proposed framework, we first develop a novel consistency-based patch selection method to determine which image patches are the most beneficial to the training. Then we introduce a conditional single-image GAN with a component-wise discriminator, to synthesize more training samples. Lastly, our proposed framework trains an existing segmentation model with the above augmented samples. The experimental results show that our proposed method could obtain the same-level performance as a fully-supervised baseline by annotating less than 5% pixels on some benchmarks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2212.10305", "mag": null, "acl": null, "pubmed": "36355729", "pubmedcentral": null, "dblp": "journals/tmi/LouLLHW23", "doi": "10.1109/tmi.2022.3221666"}}, "content": {"source": {"pdf_hash": "42c693d8a74b67b1d768a57e124a366bfa8654cb", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2212.10305v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "5a6eb49c92c1bde5479e40ee35cf350534dca377", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/42c693d8a74b67b1d768a57e124a366bfa8654cb.txt", "contents": "\nWhich Pixel to Annotate: a Label-Efficient Nuclei Segmentation Framework Index Terms-Nuclei segmentation, Sample selection, Label-efficient learning, Generative adversarial networks\n\n\nWei Lou \nHaofeng Li \nGuanbin Li liguanbin@mail.sysu.edu.cn. \nXiaoguang Han hanxiaoguang@cuhk.edu.cn. \nXiang Wan \nWei Lou \nHaofeng Li \nHaofeng Li Wei Lou \nHaofeng Li \nXiaoguang Han \nShenzhen \nGuanbin Li \n\nSchool of Computer Science and Engi-neering, Sun Yat-sen University\nGuangdong Provincial Key Laboratory of Big Data Computing\nResearch Institute of Big Data\nThe Chinese University of Hong Kong at Shen-zhen\n518172, 510006Shenzhen, GuangzhouChina, China\n\n\nGuangdong Provincial Key Laboratory of Big Data Computing\nThe Chinese University of Hong Kong at Shenzhen\n518172ShenzhenChina\n\n\nKey Laboratory of Big Data Computing\nThe Chinese University of Hong Kong\nShenzhen\n\nWhich Pixel to Annotate: a Label-Efficient Nuclei Segmentation Framework Index Terms-Nuclei segmentation, Sample selection, Label-efficient learning, Generative adversarial networks\n\nIEEE TRANSACTIONS ON MEDICAL IMAGING\nXX1contributed equally to this work. Corre-sponding author: Xiang Wan is with Shenzhen Research Institute of Big Data,\nRecently deep neural networks, which require a large amount of annotated samples, have been widely applied in nuclei instance segmentation of H&E stained pathology images. However, it is inefficient and unnecessary to label all pixels for a dataset of nuclei images which usually contain similar and redundant patterns. Although unsupervised and semi-supervised learning methods have been studied for nuclei segmentation, very few works have delved into the selective labeling of samples to reduce the workload of annotation. Thus, in this paper, we propose a novel full nuclei segmentation framework that chooses only a few image patches to be annotated, augments the training set from the selected samples, and achieves nuclei segmentation in a semi-supervised manner. In the proposed framework, we first develop a novel consistency-based patch selection method to determine which image patches are the most beneficial to the training. Then we introduce a conditional single-image GAN with a component-wise discriminator, to synthesize more training samples. Lastly, our proposed framework trains an existing segmentation model with the above augmented samples. The experimental results show that our proposed method could obtain the same-level performance as a fully-supervised baseline by annotating less than 5% pixels on some benchmarks.\n\nI. INTRODUCTION\n\nN uclei segmentation aims at labeling all pixels for each single nucleus in a histopathology image. The task could provide fundamental visual information and morphological features including the size, shape or color of nuclei [1], [2], which are not only beneficial for middle-level understanding of histopathology image (for examples, cell classification, tissue segmentation, etc), but also related to high-level clinical analysis such as cancer diagnosis, assessment and prognostic prediction. Thus nuclei segmentation acts as a critical prerequisite in computer-aided diagnosis systems (CAD) [  the distraction by complex background, the clutter and partial occlusion of nuclei, separating each nucleus accurately still remains a challenging problem. Currently, fully supervised models [4]- [8] are the most popular nuclei segmentation methods for their high accuracy, but they need pixel-level annotations which are expensive and time-consuming. Unsupervised methods based on domain adaptation [9]- [11] adapt an unlabeled dataset to a labeled one, while in real applications it is acceptable to annotate some small image patches. Semi-supervised segmentation methods [12]- [15] improve the performance by training with both labeled and unlabeled images, but they rarely discuss the impact of selecting different samples for labeling on performance. Active learning-based methods [16], [17] choose high-quality samples for labeling in an iterative way. They rely on an iteratively-trained model and few of them investigate the sample selection when no annotations are available. Since histopathology images consist of similar textures and redundant patterns, there is no need to label the entire image. As shown in Fig. 1, we claim that searching for several representative small patches from the whole dataset may be sufficient for training a segmentation model.\n\nAfter locating and labeling several image patches, how to exploit such a small amount of data for training poses another challenge. A straightforward idea is to synthesize more training pairs from the existing ones. In recent years generative adversarial networks (GANs) [18]- [20], which adversarially train a discriminator to help improve the generator, have achieved success in sampling images from noise vectors. Conditional GANs [21]- [24] could generate images from given priors such as labels, masks or texts. The above methods require collecting a number of samples to avoid over-fitting. Some recent work [25] can utilize only a single image for training but it is unconditional. Thus, it is worthwhile to conceive a conditional GAN that could efficiently augment a lot of training pairs from a single one.\n\nMotivated by the above observations, we propose a novel framework that learns to segment nuclei in a label-efficient way, as shown in Fig. 2. The proposed method consists of sample selection, data augmentation and instance segmentation. To select the useful image patches for labeling, we enumerate a wide range of possible image patches and find out those with high representativeness and intra-consistency. We experimentally verify that such selected samples could better represent the whole training data and ease the GAN training at a later stage. Since only labeling a small amount of patches is not enough for training an effective segmentation model, we develop an efficient conditional single-image GAN that can be trained with only one pair of image and mask. Afterwards, we train an existing nuclei segmentation model with these augmented samples in a semi-supervised manner.\n\nTo summarize, our contributions have four folds:\n\n\u2022 We propose a novel label-efficient nuclei segmentation framework for semi-supervised setting. \u2022 We develop a consistency-based patch selection method that only selects a part of image patches to annotate and reduces the labeling cost significantly. \u2022 We propose a data augmentation mechanism based on a conditional SinGAN with a novel component-wise discriminator. \u2022 The experiments show that our proposed framework could attain the approximate performance with a fullysupervised baseline by labeling less than 5% pixels on some datasets.\n\n\nII. RELATED WORK\n\n\nA. Deep Learning-based Nuclei Segmentation\n\nNuclei segmentation is a critical step before the clinical analysis in modern computer-aided diagnosis systems. The goal is to locate the boundaries of nuclei in histopathology images. Fully supervised methods [4]- [8], [26]- [32] use a large-scale labeled dataset to train a segmentation model. Among them, Mask-RCNN [33] is a popular object segmentation method for natural and medical images. It is a twostage segmentation model that predicts bounding boxes for nuclei and then segments them inside the predicted boxes. Hover-net [8], which is a unified FCN model for segmentation and classification of kernel instances, effectively encodes the horizontal and vertical distance information from kernel pixels to its centroid. However, most of these methods require a large number of labeled images which increase tremendous time and money cost for data collection and annotation. We adapt these two segmentation baselines with our label-efficient nuclei segmentation framework to exhibit the generalization of our methods.\n\nUnsupervised methods [9], [11], [34] are proposed to segment nuclei of a new dataset without annotations. Some are targeting the translation from a labeled domain to an unlabeled domain. PDAM [9] uses a semantic segmentation branch with a domain discriminator to narrow the domain gap between an unlabeled histopathology dataset and a labeled microscopy dataset. DARCNN [11] adapts a labeled dataset of natural images to unlabeled datasets of biomedical images. However, these domain adaption based methods need to collect a large annotated dataset as the source domain in advance. Sahasrabudhe et al. [34] adopt the prediction of magnification level as a self-supervision, and take an intermediate attention map of the neural model as the unsupervised result of nuclei segmentation. In real applications, it is usually affordable to annotate some image patches for a new dataset. Thus in this paper we adopt semi-supervised setting which is more practical.\n\n\nB. Semi-Supervised Learning\n\nSemi-supervised learning [12]- [15], [35]- [37] is a common scenario in medical applications, where only a small subset of the training images is assumed to have full annotations. Among existing semi-supervised methods, we only briefly introduce pseudo labeling [38]- [41] which is most related to our work. In pseudo-label methods, a small amount of labeled data is first used to train a target model, and then the trained model is used to produce pseudo labels for unlabeled data. The pairs of pseudo labels and their corresponding images are added to the training set for the next round of model training. Although we use a semi-supervised setting in this paper, we mainly focus on selective labeling which is seldom considered by existing semi-supervised methods. We simply employ a vanilla pseudo-labeling method in our framework, which is to predict pseudo masks for unlabeled image patches using a pretrained segmentation model trained by a few labeled patches.\n\n\nC. Sample Selection\n\nThe sample selection [42]- [44] aims to select the most useful samples from an unlabeled dataset and label them as training data to avoid annotating the whole dataset. Active learning is a commonly used sample selection method. Active learning based methods [16], [17], [45], [46] reduce the annotation cost by iteratively labelling the high-value samples selected by a continuous training model. First, the training model is pre-trained or randomly initialized, and the unlabeled images are predicted by the training model and sorted according to uncertainty strategy. Second, experts are required to label the samples with high uncertainty. Then, these labeled samples are used to fine-tune and update the training model. Finally, repeat these steps until the performance converges or the available annotation resources (money, experts) are exhausted. However, on one hand, active learning methods seldom discuss which samples to choose at the very beginning when no trained models are available. On the other hand, very few researchers focus on the study of which regions in pathological images are more effective for nuclei segmentation learning than other regions. Meanwhile, the iterative learning and annotation process can still be costly. Therefore, our sample selection method focuses on the selection of small areas inside large images without iterative training.\n\n\nD. GAN-based Image Synthesis\n\nImage synthesis is the process that produces synthetic images in the target image domain (named as target domain) from the input images that reside in a different image domain (named as source domain). This process has been widely used for data augmentation. Generative Adversarial Networks (GANs) [18]- [20], [47]- [51] have made great success in image synthesis. GANs learn a discriminator loss to classify if the output image is real or fake and train the generator to minimise the error. Traditional GANs (unconditional GANs) [18], [19] synthesize images from random noise. Since the generation process is trained without restrictions, the synthesized images may not have certain properties (e.g., corresponding to some given mask) that we need. Therefore, the approaches using conditional image constraints to produce images with the desired properties have been proposed. Conditional-GANs (cGANs) [21], [22] synthesize images with prior information (conditions) which can be labels, masks or texts. Isola et al. [23] employ many paired image-mask data to train a generative model to solve labelto-image tasks. Zhu et al. [24] introduce cycleGANs that use many unpaired images from two different domains to train a domain-translation generator by minimizing a cycleconsistency loss. However, some of these methods require a large number of training images pairs that cause unaffordable costs. Shaham et al. [25] proposed an unconditional singleimage GAN (sinGAN) which can efficiently sample images from random noises by training with only a single real image. In this paper we propose a novel conditional single-image GAN that achieves the translation from mask to image using only a single real pair of nuclei image and mask.\n\n\nIII. METHOD\n\nIn this section, we describe the proposed label-efficient nuclei segmentation framework from three aspects: how to determine the image patches to be annotated, how to synthesize more training pairs with those labeled patches, how to train a segmentation model using both labeled and unlabeled images.\n\n\nA. Consistency-based Patch Selection\n\nTo locate the image patches that mostly benefit the final performance of nuclei segmentation, we consider two attributes of an image patch, an inter-patch attribute called representativeness, and an intra-patch one called consistency. Representativeness is an important inter-patch attribute that reflects the relationship among different image patches in a large dataset. Consider that in some latent space, the image patches that are relatively close to each other form a cluster. If the distance between some image patch x and the others in a cluster is the smallest, then x is better representative than any other members in the cluster. Hence, higher representativeness means smaller distance from the cluster center. We propose to label the image patches representing different clusters so that these image patches can well replace the whole trainset and decrease the redundancy.\n\nTo ease the GAN training for later data augmentation, we suggest to select the image patches with high intra-patch ( 1 + 2 + 3 )\n\nFor each cluster k in 1 clusters: Fig. 3: Consistency-based Patch Selection (CPS). The left half shows the coarse-level and fine-level clustering process. The image patches/sub-regions with green borders are the one closest to their cluster center. The right half shows the criteria to select an image patch. 'Crop1' is to cut an image into s \u00d7 s image patches. 'Crop2' is to cut an image patch into s 2 \u00d7 s 2 sub-regions. c * denotes the largest cluster among K 2 fine-level clusters. x k denotes the selected image patch from the k-th cluster among K 1 coarse-level clusters. consistency, which measures the level of self-similarity inside an image patch. Consider that an image patch is divided into several smaller sub-regions. If these sub-regions are visually approximate to each other, then the image patch is of high consistency. Note that our proposed segmentation framework adopts a GAN-based augmentation following the patch selection stage. Considering the difficulty of image synthesis, higher intra-patch consistency that means less variations in texture can benefit the convergence of GANs to produce highquality augmented samples.\nf f( ) f( ) f f( ) c(x) 1 = 2 = 3 = f f(x) C(x) x\nTo search for the image patches with high representativeness and intra-patch consistency, we develop the Consistency-based Patch Selection (CPS) algorithm that is shown in Fig. 3 and Algorithm 1. The algorithm consists of 3 stages: image patch sampling, dual-level clustering, and criterion computing.\n\nImage patch sampling We crop a certain amount of s \u00d7 s image patches from all images in the training set with a uniformly-sliding window of step t. These sampled image patches are fixed and no longer updated. The finally labeled image patches are only from these sampled ones and no more image patches will be sampled.\n\nDual-level clustering To estimate the representativeness of an image patch, we adopt K-Means algorithm [52] to group image patches. To rate the intra-patch consistency, an image patch is split into sub-regions. We further group these sub-regions to obtain fine-grained representativeness. Thus, we conduct dual-level clustering that consists of a coarse level and a fine one. The feature vectors of image patches or sub-regions are computed by an ImageNet-pretrained [53] ResNet50 model [54] which is able to extract generic repre-sentations [55] for repetitive textures [56]. We adopt Euclidean distance of the extracted features for K-Means, inspired by the works [52], [57]. Using these features, we conduct the coarselevel clustering to divide the image patches into K 1 clusters. For each coarse-level cluster C k , we cut each s \u00d7 s image patch of C k into 2-by-2 sub-regions of size s 2 \u00d7 s 2 . Then we perform the fine-level clustering that groups the sub-regions of the same coarse-level cluster into K 2 clusters. In total, K 1 \u00d7K 2 fine-level clusters are obtained.\n\nCriterion computing For each coarse-level cluster C k , we traverse all s \u00d7 s image patches in C k for one time, compute the selection criterion for each image patch, and search for the one that maximizes the coarse-to-fine representativeness as well as the intra-patch consistency. The selection criterion could be defined as Eq. (1):\nx k = arg min x\u2208C k f (x) \u2212 C(x) + 1 4 4 p=1 f (x p ) \u2212 c(x) + max (i,j)\u2208{\u2200i,j:1\u2264i<j\u22644} { f (x i ) \u2212 f (x j ) }(1)\nwhere x denotes an s \u00d7 s image patch. x k is the selected image patch from C k , the k-th coarse-level cluster. f (\u00b7) is the feature extractor that outputs a feature vector of the same size for different input sizes.\nx p /x i /x j is one of the s 2 \u00d7 s 2\nsub-regions within x. C(x) returns the center vector of the coarse-level cluster C k which x belong to. A center vector is a vector that is the center of some cluster (a coarselevel or fine-level cluster). The center of a cluster is defined as the mean of all the elements (vectors) belonging to the cluster, following the K-means algorithm. From the K 2 fine-\n\n\nAlgorithm 1 Consistency-based Patch Selection\n\nInput: Imgs (a list of training images), f (a feature extractor), K 1 (the number of coarse-level clusters), K 2 (the number of fine-level clusters in each coarse-level cluster). Output: Results (a list of K 1 selected image patches).\n\n1: function DUAL LEVEL CLUSTERING(P ats,\nK 1 , K 2 , f ) 2: c f ine = list() 3: Ccoarse = Kmeans(f (Pats), n clusters = K 1 ) 4: for k in range (K 1 ) do 5: C k = Ccoarse[k] 6: Regions = crop2(C k ) 7: c list = Kmeans(f (Regions), n clusters = K 2 ) 8: c f ine .append(c list) 9:\nend for 10:\n\nreturn Ccoarse, c f ine 11: end function 12: Pats = crop1(Imgs) // Image patch sampling 13:\nCcoarse, c f ine = DUAL LEVEL CLUSTERING(Pats, K 1 , K 2 , f ) 14: // Ccoarse is a list of K 1 coarse-level clusters. 15: // c f ine is a 2D list of K 1 \u00d7K 2 fine-level clusters. 16: Results = list() 17: for k in range (K 1 ) do 18: C k = Ccoarse[k] 19: vcoarse = center vector of (C k ) 20: c list = c f ine [k] // a list of K 2 fine-level clusters 21: c size list = list([element number(c ) for c in c list]) 22: c * = c list[argmax(c size list)] 23: v f ine = center vector of (c * ) 24: for x in C k do 25: C(x) = vcoarse, c(x) = v f ine 26: [x 1 , x 2 , x 3 , x 4 ] = crop2(x) 27: d 1 = f (x) \u2212 C(x) , d 2 = 1 4 4 p=1 f (xp) \u2212 c(x) 28: d 3 = max (i,j)\u2208{\u2200i,j:1\u2264i<j\u22644} f (x i ) \u2212 f x j 29: d.append(d 1 + d 2 + d 3 ) 30:\nend for 31:\n\nResults.append(x k = C k [argmin(d)]) 32: end for 33: return Results level clusters in C k , we set c * as the one with the largest number of sub-regions. c(x) returns the center vector of c * . The selection criterion in Eq. (1) contains 3 cost terms that denote the coarse-level representativeness (the distance to the coarse-level cluster center), the fine-level representativeness (the summed distance from each sub-region to the largest finelevel cluster), and the intra-patch consistency (the maximum distance of any two sub-regions) respectively. Lower costs mean higher representativeness or consistency. After running the CPS algorithm, we choose an image patch for each coarselevel cluster, and obtain exactly K 1 image patches which are then annotated by experts.\n\n\nB. Conditional SinGAN Augmentation\n\nIn this section, we propose a novel data augmentation strategy based on a Conditional Single-image GAN (CSinGAN). After the image patch selection in Section III-A, we obtain K 1 pairs of image and mask. For each pair of image and mask, we propose a mask synthesis algorithm to produce lots of masks. Then the image patch, the real mask and these synthetic masks are employed to train the proposed CSinGAN. The trained generator synthesizes fake images corresponding to the synthetic masks. Following the above procedures, many pairs of synthetic image and mask are created from a real pair with our proposed CSinGAN.\n\nTo synthesize nuclei masks, Hou et al. [58] sample random polygons from a pre-defined distribution which is built on statistical features such as radius, irregularity and spike of a polygon. However, the existing method requires a large number of mask annotations and it is difficult to estimate an accurate distribution of nucleus shape using a single mask. Therefore, we proposed a novel mask synthesis algorithm as shown in Fig. 4 (a). Given one of the K 1 masks, we perform data augmentation (flip, random cropping, rotation) on the mask and collect all the augmented nuclei to form a set E. Then, we create a new empty h \u00d7 w map. The main idea is iteratively choosing a nucleus from E and placing the nucleus into the empty map, while ensuring that the placed nuclei are not overlapped with each other. The proposed mask synthesis algorithm has Q iterations. At the q-th iteration, we first randomly select a nucleus denoted as e q from E, and then calculate the maximum distance (denoted as R q ) from the nucleus center to the nucleus boundary. To avoid overlapping with the previously placed nuclei, we dilate M q\u22121 (the synthetic mask produced by the last iteration) with a radius of R q . Next, we select a random position from the background (with black color in Fig. 4) of the dilated mask, and insert e q into M q\u22121 at the selected position. After Q iterations, we obtain a mask map M Q with Q nuclei, and then crop the map to a smaller size h \u00d7 w to simulate the incomplete nuclei in real masks. As shown in Fig. 4 (b), with the same limited number of nuclei annotations our proposed mask synthesis algorithm generates more realistic shapes, while the existing method [58] produces nuclei of unsmoothed boundaries.\n\nTo generate corresponding nuclei images for the synthetic masks, we introduce a novel conditional SinGAN that consists of a multi-scale conditional generator and a novel componentwise discriminator, as shown in sively as:\ny n = G n ([m n , z n + y n\u22121 ]), 0 < n \u2264 N G 0 ([m 0 , z 0 ]), n = 0(2)\nwhere m n and m 0 denote a synthetic mask or the real one. When computing y n , all m i (0 \u2264 i \u2264 n) are obtained by resizing the same mask m to different scales. m is randomly sampled from the real mask and the synthetic masks. z n and z 0 denote a 3-channel Gaussian noise and z n has the same shape as y n\u22121 .\n\n[\u00b7, \u00b7] means concatenating two variables along the channel dimension. The internal architecture of G n is based on [25]. At each scale the training loss contains an adversarial term and a reconstruction term: min Gn max Dn L ADV (x n , y n , m n , r n , D n ) + \u03b1L REC (G n ).\n\nThe reconstruction loss is computed as L REC = y n \u2212 x n 2 by setting m n as the real mask, z n as zeros, x n as the real image.\n\nFor the adversarial loss, we develop a novel component-wise discriminator (CwD) that separates an image into different components and classifies them respectively. The discriminator D n has a foreground sub-net D f , a background sub-net D b and a global one D g . By setting all m n as synthetic masks, the overall adversarial loss is calculated as:\n\nL ADV =\u03b2L adv (D g , y n , x n ) + \u03b3L adv (D f , y n \u2297 m n , x n \u2297 r n ) + \u03b4L adv (D b , y n \u2297 \u00acm n , x n \u2297 \u00acr n )\n\nwhere r n denotes the real mask resized to scale n. \u2297 denotes element-wise multiplication and can be viewed as a masking operation. x n \u2297 r n and x n \u2297 \u00acr n mean extracting the foreground region and the background respectively for x n . Thus different sub-nets could focus on different components.\n\nD g , D f and D b do not share weights but adopt the same architecture following [25]. L adv denotes WGAN-GP loss [59]. WGAN-GP loss is a loss for generative adversarial networks that augments the Wasserstein GAN (WGAN) loss [19] by imposing a gradient norm penalty on random samples to achieve Lipschitz continuity. The training of the WGAN-GP loss requires very little hyper-parameters tuning, and is more stable than that of the WGAN loss. The detailed WGAN-GP loss is shown as Equation (4):\nL adv (D, y, x) =E[D(y)] \u2212 E[D(x)]+ \u03bbE ( \u2207xD(x) 2 \u2212 1) 2(4)\nwhere y and x mean a generated image and the real image.x is uniformly sampled along straight lines between two points which are sampled from the distribution of x and y respectively. D(\u00b7) is a discriminator. E [\u00b7] denotes the expectation. \u03bb is the penalty coefficient to weight the gradient penalty term.\n\n\nC. Semi-supervised Nuclei Segmentation\n\nIn this section, we describe how to train a CNN model for nuclei segmentation. After the augmentation in Section III-B, we have a few real pairs of image and mask, many synthetic pairs and lots of unlabeled images. The fake pairs can be regarded as labeled images so the task is formulated as a semisupervised problem. In this paper semi-supervised learning (SSL) is not a contribution but a training paradigm of our proposed framework, and we simply adopt a classical SSL method pseudo-labeling. Pseudo-labeling methods [38] usually apply a trained model to predict results for unlabeled data. The predicted results can be viewed as and converted into the 'ground-truth' labels of the unlabeled data. These predicted 'ground-truth' annotations are referred to as pseudo labels which can be used to re-train or fine-tune the model.\n\nTo segment nuclei, the proposed framework employs a popular instance segmentation model, Mask-RCNN [33]. First, the prediction model (e.g. Mask-RCNN) is trained with the real and fake pairs. Then the model predicts nuclei masks for each image of the trainset. These predicted masks are saved as the 'ground-truth' annotations, namely, the pseudo labels. The pairs of pseudo labels and their corresponding images are added to the training set for the next round of prediction model training. The performance is not improved any more after 2-3 iterations. The last model is chosen as the final model. We claim that the proposed label-efficient framework could also work with other existing nuclei segmentation models based on CNN, like Hover-net. Other SSL methods that are proposed for instance segmentation could be directly applied to our proposed segmentation framework.\n\n\nIV. EXPERIMENTS\n\n\nA. Implementation Details\n\nDatasets We conduct experiments on three datasets: TCGA-KUMAR database [5], TNBC [6] and MoNuSeg [60]. The TCGA-KUMAR dataset has 30 labeled images of size 1000 \u00d7 1000 at 40\u00d7 magnification. It is obtained from The Cancer Genome Atlas (TCGA) and each image is from one of the seven organs, including breast, bladder, colon, kidney, liver, prostate, and stomach. We split the data into 12 training images, 4 validation images and 14 testing images. following [4], [6]. The TNBC dataset, which contains 50 annotated images of size 500\u00d7500, is collected from 11 different patients of the Curie Institute. The dataset was split into 30 training images, 7 validation images, and 13 test images in our experiments. The MoNuSeg dataset consists of 44 labeled images, 30 for training and 14 for testing. The image size of MoNuSeg is 1000 \u00d7 1000.\n\nPreprocessing Following the previous work, all images were cropped into 256 \u00d7 256 patches for training the network efficiently. Naive data augmentation strategies including rotation (90\u00b0,180\u00b0,270\u00b0), flip, gaussian blur, gaussian noise are applied to these cropped image patches.\n\nPatch Selection For consistency-based patch selection (CPS), we set s = 256, t = 15, K 1 = 6/9/18 (corresponding to 5%/5%/7% labels of the trainset) for the TNBC/TCGA-KUMAR/MoNuSeg dataset respectively, K 2 = 4. The training set of these datasets are cropped into 256 \u00d7 256 image patches with stride 15. The CPS algorithm takes around 1-3 hours for each dataset.\n\nPaired Sample Synthesis In each scale of the proposed CSinGAN, the generator consists of 5 convolution blocks. Each block contains a convolution layer of kernel size 3 \u00d7 3, a BatchNorm layer and a LeakyReLU function. The channel number of each block starts from 32 and doubles every 4 scales. The multi-scale discriminators have the same architecture as the generators but they are trained with a different loss from the generators. We use K 1 CsinGANs and each CsinGAN augments data from a representative image patches. For each CSinGAN, 50 synthetic masks of size 256 \u00d7 256 are synthesized for training the nuclei segmentation model. For the adversarial loss of CSinGAN, we set \u03b2, \u03b3, \u03b4, \u03bb to 1.0, 1.0, 1.0, 0.1. We follow a related work SinGAN [25] to set the above hyper-parameters.\n\nSemi-supervised Nuclei Segmentation Baselines Mask-RCNNs are trained for 200 epochs with an SGD optimizer with an NVIDIA V-100 GPU, an initial learning rate of 0.005, a weight decay of 10 \u22124 , a momentum of 0.9, a batch size of 6, and the input size 256\u00d7256. During the testing stage the input size is set as the original image size of each dataset. Hovernets [8] are trained for 100 epochs with an Adam optimizer with a V-100 GPU, an initial learning rate of 10 \u22124 . At the begining of the training, all models are initialized with the model weights pre-trained on ImageNet [53]. For each round of semi-supervised learning, 600 pseudo labels are predicted by the last supervised segmentation model and added to the training data. The time cost of the semi-supervised training is 1-2 days.\n\n\nB. Evaluation Criteria\n\nTo evaluate nuclei segmentation models, both pixel-level and object-level criteria are applied. The object-level evaluation is for cell nucleus localization, and the pixel-level correlation evaluation is for preserving fine boundaries. In our quantitive study, we use two different metrics, Aggregated Jaccard Index (AJI) [5] and Dice Coefficient.\n\nAggregated Jaccard Index (AJI): AJI [61] is an extension of the global Jaccard index. It is defined as:\nAJI = NP i=1 |Ti\u2229S * j (i)| NG i=1 |Ti\u222aS * j (i)|+ g\u2208U |Sg|\nwhere S j is a prediction result and T i is the ground truth. S * j (i) represents the connected component that has the maximum intersection with the ground truth. U is the set of areas that do not overlap with any real nuclei. NG, NP are the number of ground truths and predictions respectively. AJI measures the overlapping areas of multiple objects and is recognized as an object-level criterion for segmentation evaluation.\n\nDice Coefficient (Dice): Dice is a typical pixel-level metric for validating medical image segmentation, and is computed as 2|X\u2229Y | |X|+|Y | . X, Y denote ground truths and predictions respectively. A higher Dice value indicates that the predicted segmentation mask has a larger intersection with the ground truth, and means that the segmentation result is more accurate.\n\n\nC. Effectiveness of Consistency-based Patch Selection\n\nThis section shows if our proposed consistency-based patch selection (CPS) is effective.   full images from the trainset, and crops the centering s \u00d7 s image patch for each selected full image. The random seeds (Seed1-Seed4) used in these experiments are 21, 100, 500, 1000. TABLE I shows the CPS significantly outperforms these random selections by about 2% -6% AJI.\n\nWe conduct ablation studies for the three-term selection function (Eq. (1)) in CPS by removing some terms to study their effectiveness. K-means denotes the method removing the 2nd term and the 3rd term in Eq. (1). We also test only removing the 2nd or the 3rd term. As TABLE I shows, without using the pseudo-label method, CPS outperforms Kmeans by 1.96% AJI, CPS w/o 2nd/3rd term by 1.85%/0.95% AJI respectively. With the pseudo-labeling method, the proposed CPS still surpasses the other three methods by 2.77%, 1.10% and 0.85% AJI respectively. These results suggest that both fine-grain representativeness (the 2nd term) and intra-patch consistency (the 3rd term) are beneficial for sample selection. Besides, we perform statistical significance analysis of AJI improvements by using paired t-test on TCGA-KUMAR dataset.    HoverNet+ours mean a Mask-rcnn and a Hover-net with our proposed framework respectively. The methods denoted as '* + Ours' only adopt about 5%/5%/7% pixel-level annotations of TCGA-KUMAR/TBNC/MoNuSeg datasets respectively. Other state-of-the-art models are supervised with all the training labels. Smaller P -value of a model means that the performance difference between the model and 'Hover-net+ours' is more significant. The best result is in bold while the second best is underlined.\n\nsignificance. No matter whether using the pseudo-labeling method, all the performance improvements are statistically significant with a P -value < 0.05. Fig. 6 illustrates that our CPS method chooses a training subset with less redundancy. Fig. 6(a) visualizes the image patches selected by the RndCrop strategy. Fig. 6(b) shows the CPS-selected image patches. In Fig. 6(a), the 8th and the 9th image patches selected by the RndCrop method have very similar patterns. In contrast, the patches chosen by CPS show higher diversity than those selected by RndCrop, which may better represent the original trainset.\n\n\nD. Effectiveness of Conditional SinGAN Augmentation\n\nIn this section the proposed CSinGAN is compared to other data augmentations on the TCGA-KUMAR dataset, shown in TABLE II. NaiveAug is a set of transformations (rotation, flipping, color jittering, ...). Pix2pix [23] learns to translate K 1 masks to K 1 image patches. By taking real and synthetic masks as a domain, cycleGAN [24] attempts to map the domain to a domain of nuclei image. The 'paint2image' application of SinGAN [25] could convert colored masks to images, which is denoted as 'SinGAN' in TABLE II. With CPS-selected image patches, the above methods synthesize samples to train a Mask-RCNN whose testing results reflect the strength of the corresponding augmentation strategy. TA-BLE II displays that our proposed CSinGAN surpasses the existing methods by 1% to 9.6% AJI.\n\n\nE. Effectiveness and Generalization of the Proposed Framework\n\nTABLE III verifies the effectiveness of our proposed method on the TCGA-KUMAR dataset. 'Ours' denotes the full proposed framework trained with a Mask-RCNN as the segmentation model, and no more than 5% pixel-wise labels. The actual labeling ratio is (256 2 \u00d7 9)/(1000 2 \u00d7 12) \u2248 4.93%. 'Plabel' denotes pseudo-labeling [38] that is a simple semi-supervised method adopted by our proposed framework and is well described in Section III-C. As TABLE III displays, our proposed method obtains even slightly better AJI and Dice scores than a full-supervised Mask-RCNN, by only annotating 5% pixels. Interestingly, the fully-supervised MRCNN achieves higher AJI on seen categories while our proposed framework performs better with unseen categories. It indicates that the fully-supervised model may over-fit to some degree and that our framework has better generalization and higher robustness. In TABLE III the proposed framework also outperforms the combinations of some existing methods which are denoted as a) -f).\n\nTo understand the generalization of our proposed framework, we further evaluate the framework with another state-ofthe-art nuclei segmentation model, Hover-net [8], on TCGA-KUMAR, TNBC and MoNuSeg datasets. As Table IV   our proposed framework using Hover-net and 5% labels achieves almost the same AJI value with a fully-supervised Hover-net on the TCGA-KUMAR dataset. On the TNBC dataset, our proposed method with Hover-net is trained with only 5% annotations, and even outperforms the fullysupervised Hover-net by 1% AJI. On the MoNuSeg dataset, the proposed framework using MRCNN and 7% labels is only 0.5% AJI & Dice lower than the fully-supervised MR-CNN. These results indicate that our method could effectively work with different nuclei segmentation models and datasets. Besides, Hover-net+ours also surpasses some state-of-theart nuclei segmentation models, PFFNET [64] and MDC-NET [63], by 1% and 3% AJI on the TNBC dataset. The P -values in TABLE IV show if the AJI differences between Hover-net+Ours and other methods are statistically significant on these datasets.\n\nTo understand the multi-organ performance of our proposed framework, we report the organ-wise results on the TCGA-KUMAR dataset. As shown in TABLE V, our proposed approach does not favor some specific organs or cancer-types. For example, for liver MRCNN+Ours outperforms MRCNN by 2.17% AJI while Hover-net+Ours is worse than Hover-net. However, we find that when using our proposed approach, the variation of AJI results among different organs is relatively smaller. For example, the AJI variation of different organs is 13.6% (46.6%-60.2%) for the MRCNN using our proposed approach. For the MRCNN without using our method, the AJI variation is 16.8% (43.6%-60.4%). Similarly, the AJI variation of different organs is 19.3% (51.6%-70.9%) for the Hover-net+Ours. For the Hover-net without our approach, the AJI variation is 22.4% (49.2%-71.6%). The above results could indicate that using our proposed method leads to smaller variation and better generalization across different organs. Fig. 7 presents some of the visualized segmentation results on the TCGA-KUMAR dataset. The ground truth areas are surrounded by green boundaries while the predicted nuclei are marked with red color. Ours+Hover net and Ours+MRCNN denote our proposed framework with these two segmentation models using less than 5% annotations. Fully-supervised Hover net and MRCNN represent the segmentation models using 100% training annotations. NaiveAug+PseudoLabel denotes a Mask-RCNN trained with naive data augmentation and pseudo labeling while Pix2pix+MRCNN denotes the Mask-RCNN that uses a pix2pix model to augment data. As Fig. 7 displays, NaiveAug+PseudoLabel and Pix2pix+MRCNN predict loose nucleus regions, while our methods with HoverNet  and MRCNN locate nuclei more accurately.\n\n\nF. Investigation of Hyper-parameters\n\nIn TABLE VI, two hyper-parameters K 1 and K 2 of the proposed CPS are studied on the TCGA-KUMAR dataset. 'w/ CSinGAN' denotes the setting that uses a CSinGAN for data augmentation. 'w/o CSinGAN' means that the CPSselected samples are augmented by naive data augmentation instead of CSinGAN. All the experiment settings in TABLE VI adopt a Mask-RCNN as the nuclei segmentation model. For simplicity, pseudo labeling is not used in these experiments. Using CSinGAN for data augmentation, the AJI results range from 0.4517 to 0.4920 with K 1 as 6/9/12. It shows that the segmentation performance of our proposed method will improve when the K 1 is set from 6 to 9, but it does not improve when change it to 12. It may be due to that the newly added samples are of relatively low intra-patch consistency which could lead to low-quality CSinGAN-synthesized images. Thus, for the TCGA-KUMAR dataset, setting K 1 as 9 is slightly better. Setting K 2 as 4 is moderate and obtains higher AJI than other two candidate values.\n\n\nV. CONCLUSION\n\nIn this paper, we first introduce a novel label-efficient framework that can segment nuclei with only 5% pixel-level annotations, and presents even slightly better results than the fullysupervised baseline on some benchmarks. Second, we develop a consistency-based patch selection algorithm that leads to higher segmentation accuracy than K-means based or random selections. Furthermore, we propose a novel component-wise discriminator that considerably enhances the quality of image synthesis for a conditional single-image GAN. In short, our proposed framework reveals the importance of selecting which image pixels for labeling. The work points out a new trend in training medical image models with low-cost annotations.\n\nFig. 1 :\n1Idea of image patch selection in the proposed nuclei segmentation framework. (a). Standard nuclei segmentation algorithms usually annotate all pixels of many nuclei images. (b). The proposed method only samples a few small image patches for labeling. Even with a smaller number of annotations, our proposed label-efficient framework achieves the same-level segmentation performance as a fully supervised baseline.\n\nFig. 2 :\n2Overview of our proposed nuclei segmentation framework.\n\nFig. 4 :\n4Mask synthesis process and the synthetic results. (a) shows the synthesis process that has Q iterations. Each iteration has four steps (1)-(4). (b) shows the synthesis results of an existing method[58] and our proposed mask synthesis.\n\nFig. 5 .Fig. 5 :\n55The generator and the discriminator are denoted as {G 0 , ..., G N } and {D 0 , ..., D N } respectively. The multi-scale generator could be defined recur-Conditional Single-image GAN (CSinGAN). CSinGAN contains a multi-scale conditional generator and a componentwise discriminator. m N is sampled from real and synthetic labels. The reconstruction loss L REC is computed when m N is real.\n\nFig. 6 :\n6Visualized comparison between the proposed consistency-based patch selection and the random crop method. (a) shows that the random crop method may choose two image patch which have redundant patterns. (b) displays that the image patches selected by the proposed CPS contain less redundant textures, and can better replace the original trainset.\n\n\n3]. Due to arXiv:2212.10305v1 [cs.CV] 20 Dec 2022Consistency-\nbased Patch \nSelection \n\nConditional \nSinGAN-based \nAugmentation \n\nPseudo-label based \nSemi-Supervised \nSegmentation \nlabelled by \nexperts a real pair \n\na selected \nsmall patch \n\nmany synthetic \npairs \n\npathological images \n\n\n\n\nTABLE I presents the performance of different patch selection methods on the TGCA-KUMAR dataset. The samples chosen by each selection method are augmented by the proposed CSinGAN. Then the augmented samples are used to train a Mask-RCNN whose testing results are logged as the performance of the selection method. In TABLE I. our proposed CPS is compared with two kinds of random selections, RndCrop and RndCenCrop. The RndCrop strategy cuts out many s \u00d7 s image patches from the whole trainset, and then randomly chooses K 1 patches to annotate. The RndCenCrop strategy randomly selects K 1Methods \nAJI \nDice \nP -value \nRndCrop + MRCNN \n0.4376 \n0.6374 \n9.76 \u00d7 10 \u221223 \nRndCenCrop + MRCNN \n0.4637 \n0.6860 \n4.82 \u00d7 10 \u221218 \nK-means + MRCNN \n0.4724 \n0.7093 \n2.17 \u00d7 10 \u22124 \nCPS w/o 3rd term + MRCNN \n0.4825 \n0.6962 \n9.03 \u00d7 10 \u22128 \nCPS w/o 2nd term + MRCNN \n0.4735 \n0.6863 \n7.63 \u00d7 10 \u22126 \nCPS + MRCNN \n0.4920 \n0.7105 \n-\nK-means + MRCNN + Plabel \n0.5097 \n0.7328 \n5.60 \u00d7 10 \u221229 \nCPS w/o 3rd term + MRCNN + Plabel \n0.5289 \n0.7403 \n3.43 \u00d7 10 \u22129 \nCPS w/o 2nd term + MRCNN + Plabel \n0.5264 \n0.7373 \n4.82 \u00d7 10 \u221212 \nCPS + MRCNN + Plabel \n0.5374 \n0.7536 \n-\n\n\n\nTABLE I :\nIEffectivenessof the proposed Consistency-based Patch Selection (CPS) algorithm. Each patch selection method is \nfollowed by the proposed CSinGAN augmentation and then a Mask-RCNN is trained and tested to obtain the above results. \nRndCrop denotes the mean performance of four random seeds (21,100,500,1000). MRCNN and Plabel donate Mask-RCNN \n[33] and Pseudo-labeling respectively. \n\nSynthetic \nimages \n\nSynthetic \nmasks \n\n(b) CPS \uf0e0 49.20% AJI \n\nSelected \npatches \n\n(a) Random Crop (RndCrop) \uf0e0 43.76% AJI \n\nSelected \npatches \n\nSynthetic \nimages \n\nSynthetic \nmasks \n\nRedundant \npatterns \n\nLess \nredundant \n\n\n\nTABLE II :\nIIEffectivenessof the proposed Conditional SinGAN \naugmentation on the TCGA-KUMAR dataset. Each augmenta-\ntion strategy is based on the samples selected by our proposed \nCPS algorithm. These augmentation methods are evaluated by \ntraining a Mask-RCNN on their synthesized samples and test-\ning the Mask-RCNN. CwD denotes the proposed component-\nwise discriminator. The CSinGAN (w/o CwD) does not use \nour proposed component-wise discriminator, but only uses a \nsingle discriminator with the standard adversarial loss. \n\nThe P -values in TABLE I show if the AJI differences between \nour proposed CPS method and other approaches is of statistical \nMethods \nAJI-s \nAJI-u \nAJI \nDice \na).RndCrop + MRCNN \n0.4371 \n0.4395 \n0.4381 \n0.6499 \nb).RndCenCrop + MRCNN \n0.4700 \n0.4321 \n0.4537 \n0.6662 \nc).RndCrop + Na\u00efveAug + MRCNN \n0.4673 \n0.4481 \n0.4577 \n0.6638 \nd).RndCenCrop + Na\u00efveAug + MRCNN \n0.4701 \n0.4623 \n0.4662 \n0.6808 \ne).RndCrop + Na\u00efveAug + MRCNN + Plabel \n0.5182 \n0.5160 \n0.5171 \n0.7282 \nf).RndCenCrop + Na\u00efveAug + MRCNN + Plabel \n0.5236 \n0.5134 \n0.5185 \n0.7255 \nOurs \n0.5348 \n0.5408 \n0.5374 \n0.7536 \nFully-supervised Mask-RCNN [33] \n0.5389 \n0.5332 \n0.5364 * \n0.7459 \n\n\n\nTABLE III :\nIIIEffectivenessof our proposed label-efficient nuclei segmentation framework (denoted as 'Ours') on the TCGA-\nKUMAR dataset. AJI-s/AJI-u denote AJI on seen/unseen categories (organs). MRCNN and Plabel denote Mask-RCNN [33] \nand Pseudo-labeling respectively. \n\nTCGA-KUMAR \nTNBC \nMoNuSeg \nMethods \nAJI \nDice \nP -value \nAJI \nDice \nP -value \nAJI \nDice \nP -value \nMRCNN [33] \n0.5364 \n0.7459 \n6.75 \u00d7 10 \u221218 \n0.5279 \n0.6978 \n2.84 \u00d7 10 \u221227 \n0.5950 \n0.7677 \n4.79 \u00d7 10 \u221220 \nDIST [6] \n0.5598 \n0.7863 \n8.68 \u00d7 10 \u221219 \n0.5258 \n0.7368 \n3.43 \u00d7 10 \u221220 \n0.5839 \n0.7589 \n7.83 \u00d7 10 \u221218 \nMicro-Net [62] \n0.5631 \n0.7961 \n9.61 \u00d7 10 \u221216 \n0.5064 \n0.6848 \n1.39 \u00d7 10 \u221218 \n0.6011 \n0.7762 \n2.18 \u00d7 10 \u221221 \nMDC NET [63] \n0.5803 \n-\n-\n0.6103 \n-\n-\n-\n-\n-\nDPMSFF [64] \n0.5854 \n0.7936 \n-\n0.5865 \n0.7792 \n-\n-\n-\n-\nPFFNET [4] \n0.5980  \u2020 \n0.7981  \u2020 \n6.86 \u00d7 10 \u221223 \n0.6283  \u2020 \n0.8127  \u2020 \n3.38 \u00d7 10 \u221220 \n0.6209 \n0.7878 \n3.77 \u00d7 10 \u22127 \nHover-net [8] \n0.6107 \n0.8211 \n6.87 \u00d7 10 \u22124 \n0.6307 \n0.7819 \n4.36 \u00d7 10 \u22128 \n0.6602 \n0.8213 \n5.73 \u00d7 10 \u221235 \nMRCNN + Ours \n0.5374 \n0.7536 \n4.78 \u00d7 10 \u221223 \n0.5281 \n0.6994 \n7.81 \u00d7 10 \u221235 \n0.5906 \n0.7626 \n4.71 \u00d7 10 \u221215 \nHover-net + Ours \n0.6086 \n0.8188 \n-\n0.6414 \n0.7938 \n-\n0.6485 \n0.8129 \n-\n\n\n\nTABLE IV :\nIVEffectiveness of the proposed framework on TCGA-KUMAR, TNBC and MoNuSeg datasets. MRCNN+Ours and\n\n\nshows, * AJI of other MRCNN reproductions: 0.5396 [4] / 0.5382 [64] / 0.5002 [6] \u2020 Published AJI/Dice performance on TCGA-KUMAR dataset: 0.6107/0.8091, on TNBC dataset: 0.6313/0.8037 of PFFNET [4]Fig. 7: Visualized comparisons between our proposed method and other existing models. The ground truth nuclei are marked with green boundaries and the predicted ones are marked with red color.Ours + \nHover_net \n\nFully-supervised + \nHover_net \n\nOurs + \nMRCNN \n\nNa\u00ef ve aug + \nPseudo_label \n\nPix2pix + \nMRCNN \n\nPr \n\nFully-supervised \n+ MRCNN \n\nOrgans \nStomach \nProstate \nBladder \nLiver \nKidney \nBreast \nColon \nMRCNN+Ours \n0.6019 \n0.5561 \n0.5549 \n0.4902 \n0.5806 \n0.5123 \n0.4657 \nMRCNN [33] \n0.6043 \n0.5850 \n0.5599 \n0.4685 \n0.5677 \n0.5333 \n0.4355 \nHover-net+Ours \n0.7091 \n0.6204 \n0.6017 \n0.5712 \n0.6415 \n0.6006 \n0.5159 \nHover-net [8] \n0.7158 \n0.6435 \n0.6113 \n0.5726 \n0.6312 \n0.6140 \n0.4924 \n\n\n\nTABLE V :\nVOrgan-wise AJI results on the TCGA-KUMAR dataset. MRCNN+Ours and Hover-net+Ours represent a Mask-rcnn and a Hover-net with our proposed framework respectively.\n\nTABLE VI :\nVIHyper-parameters investigation of the proposed consistency-based patch selection on the TCGA-KUMAR dataset. Three values of K 1(6,9,12) are tested when K 2 is set as 4. Three values of K 2(3,4,8) are tested when K 1 is set as 9. The 'Annos' donates the annotation proportion of the training set. Each CPS experiment is followed by CSinGAN or naive augmentation (w/o CSinGAN) to obtain the AJI and Dice values.\n\nPathologic correlates of survival in 378 lymph nodenegative infiltrating ductal breast carcinomas. mitotic count is the best single predictor. F Clayton, Cancer. 686F. Clayton, \"Pathologic correlates of survival in 378 lymph node- negative infiltrating ductal breast carcinomas. mitotic count is the best single predictor,\" Cancer, vol. 68, no. 6, pp. 1309-1317, 1991.\n\nPathological prognostic factors in breast cancer. i. the value of histological grade in breast cancer: experience from a large study with long-term follow-up. C W Elston, I O Ellis, Histopathology. 195C. W. Elston and I. O. Ellis, \"Pathological prognostic factors in breast cancer. i. the value of histological grade in breast cancer: experience from a large study with long-term follow-up,\" Histopathology, vol. 19, no. 5, pp. 403-410, 1991.\n\nRobust nucleus/cell detection and segmentation in digital pathology and microscopy images: a comprehensive review. F Xing, L Yang, IEEE REV BIOMED ENG. 9F. Xing and L. Yang, \"Robust nucleus/cell detection and segmentation in digital pathology and microscopy images: a comprehensive review,\" IEEE REV BIOMED ENG, vol. 9, pp. 234-263, 2016.\n\nPanoptic feature fusion net: A novel instance segmentation paradigm for biomedical and biological images. D Liu, D Zhang, Y Song, H Huang, W Cai, IEEE Transactions on Image Processing. 30D. Liu, D. Zhang, Y. Song, H. Huang, and W. Cai, \"Panoptic feature fusion net: A novel instance segmentation paradigm for biomedical and biological images,\" IEEE Transactions on Image Processing, vol. 30, pp. 2045-2059, 2021.\n\nA dataset and a technique for generalized nuclear segmentation for computational pathology. N Kumar, R Verma, S Sharma, S Bhargava, A Vahadane, A Sethi, IEEE Transactions on Medical Imaging. 367N. Kumar, R. Verma, S. Sharma, S. Bhargava, A. Vahadane, and A. Sethi, \"A dataset and a technique for generalized nuclear segmen- tation for computational pathology,\" IEEE Transactions on Medical Imaging, vol. 36, no. 7, pp. 1550-1560, 2017.\n\nSegmentation of nuclei in histopathology images by deep regression of the distance map. P Naylor, M La\u00e9, F Reyal, T Walter, IEEE Transactions on Medical Imaging. 382P. Naylor, M. La\u00e9, F. Reyal, and T. Walter, \"Segmentation of nuclei in histopathology images by deep regression of the distance map,\" IEEE Transactions on Medical Imaging, vol. 38, no. 2, pp. 448-459, 2018.\n\nDCAN: Deep contour-aware networks for object instance segmentation from histology images. H Chen, X Qi, L Yu, Q Dou, J Qin, P.-A Heng, Medical Image Analysis. 36H. Chen, X. Qi, L. Yu, Q. Dou, J. Qin, and P.-A. Heng, \"DCAN: Deep contour-aware networks for object instance segmentation from histology images,\" Medical Image Analysis, vol. 36, pp. 135-146, 2017.\n\nHover-Net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images. S Graham, Medical Image Analysis. 58101563S. Graham et al., \"Hover-Net: Simultaneous segmentation and classifica- tion of nuclei in multi-tissue histology images,\" Medical Image Analysis, vol. 58, p. 101563, 2019.\n\nUnsupervised instance segmentation in microscopy images via panoptic domain adaptation and task re-weighting. D Liu, CVPR. IEEE, 2020. D. Liu et al., \"Unsupervised instance segmentation in microscopy images via panoptic domain adaptation and task re-weighting.\" in CVPR. IEEE, 2020, pp. 4243-4252.\n\nAdversarial discriminative domain adaptation. E Tzeng, J Hoffman, K Saenko, T Darrell, CVPR. IEEEE. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, \"Adversarial discrim- inative domain adaptation,\" in CVPR. IEEE, 2017, pp. 7167-7176.\n\nDARCNN: Domain adaptive regionbased convolutional neural network for unsupervised instance segmentation in biomedical images. J Hsu, W Chiu, S Yeung, CVPR. IEEE. J. Hsu, W. Chiu, and S. Yeung, \"DARCNN: Domain adaptive region- based convolutional neural network for unsupervised instance segmenta- tion in biomedical images,\" in CVPR. IEEE, June 2021, pp. 1003-1012.\n\nSelf-loop uncertainty: A novel pseudo-label for semi-supervised medical image segmentation. Y Li, J Chen, X Xie, K Ma, Y Zheng, MICCAI. SpringerY. Li, J. Chen, X. Xie, K. Ma, and Y. Zheng, \"Self-loop uncertainty: A novel pseudo-label for semi-supervised medical image segmentation.\" in MICCAI. Springer, 2020, pp. 614-623.\n\nWeakly supervised deep nuclei segmentation using partial points annotation in histopathology images. H Qu, IEEE Transactions on Medical Imaging. 3911H. Qu et al., \"Weakly supervised deep nuclei segmentation using partial points annotation in histopathology images,\" IEEE Transactions on Medical Imaging, vol. 39, no. 11, pp. 3655-3666, 2020.\n\nConnecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora. R Socher, L Fei-Fei, CVPR. IEEER. Socher and L. Fei-Fei, \"Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora.\" in CVPR. IEEE, 2010, pp. 966-973.\n\nCollaborative learning of semi-supervised segmentation and classification for medical images. Y Zhou, CVPR. IEEEY. Zhou et al., \"Collaborative learning of semi-supervised segmentation and classification for medical images,\" in CVPR. IEEE, 2019, pp. 2079-2088.\n\nSuggestive annotation: A deep active learning framework for biomedical image segmentation. L Yang, Y Zhang, J Chen, S Zhang, D Z Chen, MICCAI. SpringerL. Yang, Y. Zhang, J. Chen, S. Zhang, and D. Z. Chen, \"Suggestive annotation: A deep active learning framework for biomedical image segmentation.\" in MICCAI. Springer, 2017, pp. 399-407.\n\nFinetuning convolutional neural networks for biomedical image analysis: actively and incrementally. Z Zhou, J Shin, L Zhang, S Gurudu, M Gotway, J Liang, CVPR. IEEEZ. Zhou, J. Shin, L. Zhang, S. Gurudu, M. Gotway, and J. Liang, \"Fine- tuning convolutional neural networks for biomedical image analysis: actively and incrementally.\" in CVPR. IEEE, 2017, pp. 7340-7351.\n\nGenerative adversarial nets. I Goodfellow, Advances in Neural Information Processing Systems. I. Goodfellow et al., \"Generative adversarial nets,\" in Advances in Neural Information Processing Systems, 2014.\n\nWasserstein generative adversarial networks. M Arjovsky, S Chintala, L Bottou, ICML. M. Arjovsky, S. Chintala, and L. Bottou, \"Wasserstein generative adver- sarial networks.\" in ICML. PMLR, 2017, pp. 214-223.\n\nA style-based generator architecture for generative adversarial networks. T Karras, S Laine, T Aila, CVPR. IEEET. Karras, S. Laine, and T. Aila, \"A style-based generator architecture for generative adversarial networks.\" in CVPR. IEEE, 2019, pp. 4401- 4410.\n\nConditional generative adversarial nets. M Mirza, S Osindero, arXiv:1411.1784arXiv preprintM. Mirza and S. Osindero, \"Conditional generative adversarial nets,\" arXiv preprint arXiv:1411.1784, 2014.\n\nGenerative adversarial text to image synthesis. S Reed, Z Akata, X Yan, L Logeswaran, B Schiele, H Lee, ICML. PMLR. S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee, \"Generative adversarial text to image synthesis.\" in ICML. PMLR, 2016, pp. 1060-1069.\n\nImage-to-image translation with conditional adversarial networks. P Isola, J.-Y Zhu, T Zhou, A A Efros, CVPR. IEEEP. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, \"Image-to-image translation with conditional adversarial networks.\" in CVPR. IEEE, 2017, pp. 1125-1134.\n\nUnpaired image-to-image translation using cycle-consistent adversarial networks. J.-Y Zhu, T Park, P Isola, A A Efros, ICCV. IEEEJ.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, \"Unpaired image-to-image translation using cycle-consistent adversarial networks,\" in ICCV. IEEE, 2017, pp. 2223-2232.\n\nSinGAN: Learning a generative model from a single natural image. T R Shaham, T Dekel, T Michaeli, ICCV. IEEET. R. Shaham, T. Dekel, and T. Michaeli, \"SinGAN: Learning a generative model from a single natural image.\" in ICCV. IEEE, 2019, pp. 4570-4580.\n\nCIA-Net: Robust nuclei instance segmentation with contouraware information aggregation. Y Zhou, O F Onder, Q Dou, E Tsougenis, H Chen, P.-A Heng, IPMI. SpringerY. Zhou, O. F. Onder, Q. Dou, E. Tsougenis, H. Chen, and P.-A. Heng, \"CIA-Net: Robust nuclei instance segmentation with contour- aware information aggregation,\" in IPMI. Springer, 2019, pp. 682-693.\n\nTriple U-Net: Hematoxylin-aware nuclei segmentation with progressive dense feature aggregation. B Zhao, Medical Image Analysis. 65101786B. Zhao et al., \"Triple U-Net: Hematoxylin-aware nuclei segmentation with progressive dense feature aggregation,\" Medical Image Analysis, vol. 65, p. 101786, 2020.\n\nBoundary-assisted region proposal networks for nucleus segmentation. S Chen, C Ding, D Tao, MICCAI. SpringerS. Chen, C. Ding, and D. Tao, \"Boundary-assisted region proposal networks for nucleus segmentation,\" in MICCAI. Springer, 2020, pp. 279-288.\n\nInstance-aware self-supervised learning for nuclei segmentation. X Xie, MICCAI. SpringerX. Xie et al., \"Instance-aware self-supervised learning for nuclei seg- mentation,\" in MICCAI. Springer, 2020, pp. 341-350.\n\nBending loss regularized network for nuclei segmentation in histopathology images. H Wang, M Xian, A Vakanski, ISBI. IEEEH. Wang, M. Xian, and A. Vakanski, \"Bending loss regularized network for nuclei segmentation in histopathology images,\" in ISBI. IEEE, 2020, pp. 1-5.\n\nNuclei segmentation using mixed points and masks selected from uncertainty,\" in ISBI. H Qu, J Yi, Q Huang, P Wu, D Metaxas, IEEEH. Qu, J. Yi, Q. Huang, P. Wu, and D. Metaxas, \"Nuclei segmentation using mixed points and masks selected from uncertainty,\" in ISBI. IEEE, 2020, pp. 973-976.\n\nObject-guided instance segmentation with auxiliary feature refinement for biological images. J Yi, IEEE Transactions on Medical Imaging. J. Yi et al., \"Object-guided instance segmentation with auxiliary fea- ture refinement for biological images,\" IEEE Transactions on Medical Imaging, 2021.\n\nMask R-CNN,\" in ICCV. K He, G Gkioxari, P Doll\u00e1r, R Girshick, IEEEK. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick, \"Mask R-CNN,\" in ICCV. IEEE, 2017, pp. 2961-2969.\n\nSelf-supervised nuclei segmentation in histopathological images using attention. M Sahasrabudhe, MICCAI. SpringerM. Sahasrabudhe et al., \"Self-supervised nuclei segmentation in histopathological images using attention.\" in MICCAI. Springer, 2020, pp. 393-402.\n\nSemi-supervised medical image segmentation via learning consistency under transformations. G Bortsova, F Dubost, L Hogeweg, I Katramados, M De Bruijne, MICCAI. SpringerG. Bortsova, F. Dubost, L. Hogeweg, I. Katramados, and M. de Bruijne, \"Semi-supervised medical image segmentation via learning consistency under transformations,\" in MICCAI. Springer, 2019, pp. 810-818.\n\nConsistency-based semisupervised learning for object detection. J Jeong, S Lee, J Kim, N Kwak, Advances in Neural Information Processing Systems. 32J. Jeong, S. Lee, J. Kim, and N. Kwak, \"Consistency-based semi- supervised learning for object detection,\" Advances in Neural Informa- tion Processing Systems, vol. 32, pp. 10 759-10 768, 2019.\n\nSSMD: semi-supervised medical image detection with adaptive consistency and heterogeneous perturbation. H.-Y Zhou, C Wang, H Li, G Wang, S Zhang, W Li, Y Yu, Medical Image Analysis. 72102117H.-Y. Zhou, C. Wang, H. Li, G. Wang, S. Zhang, W. Li, and Y. Yu, \"SSMD: semi-supervised medical image detection with adaptive consis- tency and heterogeneous perturbation,\" Medical Image Analysis, vol. 72, p. 102117, 2021.\n\nPseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. D.-H Lee, Workshop on challenges in representation learning, ICML. 3896D.-H. Lee et al., \"Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks,\" in Workshop on challenges in representation learning, ICML, vol. 3, no. 2, 2013, p. 896.\n\nPseudo-labeling and confirmation bias in deep semi-supervised learning. E Arazo, D Ortego, P Albert, N E O&apos;connor, K Mcguinness, IJCNN. IEEEE. Arazo, D. Ortego, P. Albert, N. E. O'Connor, and K. McGuinness, \"Pseudo-labeling and confirmation bias in deep semi-supervised learn- ing,\" in IJCNN. IEEE, 2020, pp. 1-8.\n\nLabel propagation with augmented anchors: A simple semi-supervised learning baseline for unsupervised domain adaptation. Z Yabin, D Bin, J Kui, Z Lei, ECCV. SpringerZ. Yabin, D. Bin, J. Kui, and Z. Lei, \"Label propagation with augmented anchors: A simple semi-supervised learning baseline for unsupervised domain adaptation,\" in ECCV. Springer, 2020, pp. 781-797.\n\nTransductive semisupervised deep learning using min-max features. W Shi, Y Gong, C Ding, Z M Tao, N Zheng, ECCV. SpringerW. Shi, Y. Gong, C. Ding, Z. M. Tao, and N. Zheng, \"Transductive semi- supervised deep learning using min-max features,\" in ECCV. Springer, September 2018.\n\nMask-guided sample selection for semi-supervised instance segmentation. M Bellver, A Salvador, J Torres, X Giro-I Nieto, Multimedia Tools and Applications. 79M. Bellver, A. Salvador, J. Torres, and X. Giro-i Nieto, \"Mask-guided sample selection for semi-supervised instance segmentation,\" Multime- dia Tools and Applications, vol. 79, no. 35, pp. 25 551-25 569, 2020.\n\nEfficient active learning for image classification and segmentation using a sample selection and conditional generative adversarial network. D Mahapatra, B Bozorgtabar, J.-P Thiran, M Reyes, MICCAI. SpringerD. Mahapatra, B. Bozorgtabar, J.-P. Thiran, and M. Reyes, \"Efficient active learning for image classification and segmentation using a sample selection and conditional generative adversarial network,\" in MICCAI. Springer, 2018, pp. 580-588.\n\nSemi-supervised hierarchical mult imodal feature and sample selection for Alzheimer's disease diagnosis. L An, E Adeli, M Liu, J Zhang, D Shen, MICCAI. SpringerL. An, E. Adeli, M. Liu, J. Zhang, and D. Shen, \"Semi-supervised hierarchical mult imodal feature and sample selection for Alzheimer's disease diagnosis,\" in MICCAI. Springer, 2016, pp. 79-87.\n\nCommittee-based sampling for training probabilistic classifiers. I Dagan, S P Engelson, Machine Learning Proceedings. I. Dagan and S. P. Engelson, \"Committee-based sampling for training probabilistic classifiers,\" in Machine Learning Proceedings 1995. El- sevier, 1995, pp. 150-157.\n\nAlgorithms for optimal scheduling and management of hidden Markov model sensors. V Krishnamurthy, IEEE Transactions on Signal Processing. 506V. Krishnamurthy, \"Algorithms for optimal scheduling and management of hidden Markov model sensors,\" IEEE Transactions on Signal Pro- cessing, vol. 50, no. 6, pp. 1382-1397, 2002.\n\nCell image segmentation using generative adversarial networks, transfer learning, and augmentations. M Majurski, CVPR Workshops. IEEE. M. Majurski et al., \"Cell image segmentation using generative adversar- ial networks, transfer learning, and augmentations,\" in CVPR Workshops. IEEE, 2019.\n\nGeneration of 3D brain MRI using auto-encoding generative adversarial networks. G Kwon, C Han, D.-S Kim, MICCAI. SpringerG. Kwon, C. Han, and D.-s. Kim, \"Generation of 3D brain MRI using auto-encoding generative adversarial networks,\" in MICCAI. Springer, 2019, pp. 118-126.\n\nLow-dose CT image denoising using a generative adversarial network with Wasserstein distance and perceptual loss. Q Yang, IEEE Transactions on Medical Imaging. 376Q. Yang et al., \"Low-dose CT image denoising using a generative adversarial network with Wasserstein distance and perceptual loss,\" IEEE Transactions on Medical Imaging, vol. 37, no. 6, pp. 1348-1357, 2018.\n\nDAGAN: Deep de-aliasing generative adversarial networks for fast compressed sensing MRI reconstruction. G Yang, IEEE Transactions on Medical Imaging. 376G. Yang et al., \"DAGAN: Deep de-aliasing generative adversarial networks for fast compressed sensing MRI reconstruction,\" IEEE Trans- actions on Medical Imaging, vol. 37, no. 6, pp. 1310-1321, 2017.\n\nContext-aware semantic inpainting. H Li, G Li, L Lin, H Yu, Y Yu, IEEE Transactions on Cybernetics. 4912H. Li, G. Li, L. Lin, H. Yu, and Y. Yu, \"Context-aware semantic inpainting,\" IEEE Transactions on Cybernetics, vol. 49, no. 12, pp. 4398-4411, 2018.\n\nDeep clustering with concrete K-means. B Gao, Y Yang, H Gouk, T M Hospedales, International Conference on Acoustics, Speech and Signal processing (ICASSP). IEEEB. Gao, Y. Yang, H. Gouk, and T. M. Hospedales, \"Deep clustering with concrete K-means,\" in International Conference on Acoustics, Speech and Signal processing (ICASSP). IEEE, 2020, pp. 4252-4256.\n\nImageNet : A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, CVPR. IEEEJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"ImageNet : A large-scale hierarchical image database,\" in CVPR. IEEE, 2009, pp. 248-255.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. IEEEK. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in CVPR. IEEE, 2016, pp. 770-778.\n\nDeCAF: A deep convolutional activation feature for generic visual recognition. J Donahue, ICML. PMLRJ. Donahue et al., \"DeCAF: A deep convolutional activation feature for generic visual recognition,\" in ICML. PMLR, 2014, pp. 647-655.\n\nDescribing textures in the wild. M Cimpoi, S Maji, I Kokkinos, S Mohamed, A Vedaldi, CVPR. M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi, \"Describing textures in the wild,\" in CVPR, 2014, pp. 3606-3613.\n\nDeep embedding network for clustering. P Huang, Y Huang, W Wang, L Wang, 2014 22nd International Conference on Pattern Recognition. IEEEP. Huang, Y. Huang, W. Wang, and L. Wang, \"Deep embedding network for clustering,\" in 2014 22nd International Conference on Pattern Recognition. IEEE, 2014, pp. 1532-1537.\n\nRobust histopathology image analysis: To label or to synthesize?\" in CVPR. L Hou, A Agarwal, D Samaras, T M Kurc, R R Gupta, J H Saltz, IEEEL. Hou, A. Agarwal, D. Samaras, T. M. Kurc, R. R. Gupta, and J. H. Saltz, \"Robust histopathology image analysis: To label or to synthesize?\" in CVPR. IEEE, 2019, pp. 8533-8542.\n\nImproved training of Wasserstein GANs. I Gulrajani, F Ahmed, M Arjovsky, V Dumoulin, A Courville, Advances in Neural Information Processing Systems. I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville, \"Improved training of Wasserstein GANs,\" in Advances in Neural Information Processing Systems , 2017.\n\nA multi-organ nucleus segmentation challenge. N Kumar, IEEE Transactions on Medical Imaging. 395N. Kumar et al., \"A multi-organ nucleus segmentation challenge,\" IEEE Transactions on Medical Imaging, vol. 39, no. 5, pp. 1380-1391, 2019.\n\nDeep adversarial training for multi-organ nuclei segmentation in histopathology images. F Mahmood, IEEE Transactions on Medical Imaging. 3911F. Mahmood et al., \"Deep adversarial training for multi-organ nuclei segmentation in histopathology images,\" IEEE Transactions on Medical Imaging, vol. 39, no. 11, pp. 3257-3267, 2019.\n\nMicro-Net : A unified model for segmentation of various objects in microscopy images. S E A Raza, Medical Image Analysis. 52S. E. A. Raza et al., \"Micro-Net : A unified model for segmentation of various objects in microscopy images,\" Medical Image Analysis, vol. 52, pp. 160-173, 2019.\n\nMDC-Net : A new convolutional neural network for nucleus segmentation in histopathology images with distance maps and contour information. X Liu, Z Guo, J Cao, J Tang, Computers in Biology and Medicine. 104543X. Liu, Z. Guo, J. Cao, and J. Tang, \"MDC-Net : A new convolutional neural network for nucleus segmentation in histopathology images with distance maps and contour information,\" Computers in Biology and Medicine, p. 104543, 2021.\n\nNuclei segmentation via a deep panoptic model with semantic feature fusion. D Liu, IJCAI. D. Liu et al., \"Nuclei segmentation via a deep panoptic model with semantic feature fusion,\" in IJCAI, 2019, pp. 861-868.\n", "annotations": {"author": "[{\"end\":193,\"start\":185},{\"end\":205,\"start\":194},{\"end\":245,\"start\":206},{\"end\":286,\"start\":246},{\"end\":297,\"start\":287},{\"end\":306,\"start\":298},{\"end\":318,\"start\":307},{\"end\":338,\"start\":319},{\"end\":350,\"start\":339},{\"end\":365,\"start\":351},{\"end\":375,\"start\":366},{\"end\":387,\"start\":376},{\"end\":641,\"start\":388},{\"end\":769,\"start\":642},{\"end\":853,\"start\":770}]", "publisher": null, "author_last_name": "[{\"end\":192,\"start\":189},{\"end\":204,\"start\":202},{\"end\":216,\"start\":214},{\"end\":259,\"start\":256},{\"end\":296,\"start\":293},{\"end\":305,\"start\":302},{\"end\":317,\"start\":315},{\"end\":337,\"start\":334},{\"end\":349,\"start\":347},{\"end\":364,\"start\":361},{\"end\":374,\"start\":366},{\"end\":386,\"start\":384}]", "author_first_name": "[{\"end\":188,\"start\":185},{\"end\":201,\"start\":194},{\"end\":213,\"start\":206},{\"end\":255,\"start\":246},{\"end\":292,\"start\":287},{\"end\":301,\"start\":298},{\"end\":314,\"start\":307},{\"end\":326,\"start\":319},{\"end\":333,\"start\":327},{\"end\":346,\"start\":339},{\"end\":360,\"start\":351},{\"end\":383,\"start\":376}]", "author_affiliation": "[{\"end\":640,\"start\":389},{\"end\":768,\"start\":643},{\"end\":852,\"start\":771}]", "title": "[{\"end\":182,\"start\":1},{\"end\":1035,\"start\":854}]", "venue": "[{\"end\":1073,\"start\":1037}]", "abstract": "[{\"end\":2536,\"start\":1193}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2784,\"start\":2781},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2789,\"start\":2786},{\"end\":3152,\"start\":3151},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3348,\"start\":3345},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3353,\"start\":3350},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3557,\"start\":3554},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3563,\"start\":3559},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3732,\"start\":3728},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3738,\"start\":3734},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3944,\"start\":3940},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3950,\"start\":3946},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4700,\"start\":4696},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4706,\"start\":4702},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4863,\"start\":4859},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4869,\"start\":4865},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5043,\"start\":5039},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6998,\"start\":6995},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7003,\"start\":7000},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7009,\"start\":7005},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7015,\"start\":7011},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7107,\"start\":7103},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7320,\"start\":7317},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7835,\"start\":7832},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7841,\"start\":7837},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7847,\"start\":7843},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8006,\"start\":8003},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8185,\"start\":8181},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8417,\"start\":8413},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8829,\"start\":8825},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8835,\"start\":8831},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8841,\"start\":8837},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8847,\"start\":8843},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9066,\"start\":9062},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9072,\"start\":9068},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9817,\"start\":9813},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9823,\"start\":9819},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10054,\"start\":10050},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10060,\"start\":10056},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10066,\"start\":10062},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10072,\"start\":10068},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11501,\"start\":11497},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11507,\"start\":11503},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":11513,\"start\":11509},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11519,\"start\":11515},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11733,\"start\":11729},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11739,\"start\":11735},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12106,\"start\":12102},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12112,\"start\":12108},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12221,\"start\":12217},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12330,\"start\":12326},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12615,\"start\":12611},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":16232,\"start\":16228},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":16596,\"start\":16592},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":16616,\"start\":16612},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":16671,\"start\":16667},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":16700,\"start\":16696},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":16795,\"start\":16791},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":16801,\"start\":16797},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":18938,\"start\":18936},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":21151,\"start\":21147},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":22794,\"start\":22790},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23565,\"start\":23561},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24706,\"start\":24702},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":24739,\"start\":24735},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24850,\"start\":24846},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26049,\"start\":26045},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":26460,\"start\":26456},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27351,\"start\":27348},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27361,\"start\":27358},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":27378,\"start\":27374},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27737,\"start\":27734},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27742,\"start\":27739},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":29509,\"start\":29505},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":29909,\"start\":29906},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":30125,\"start\":30121},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":30687,\"start\":30684},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":30751,\"start\":30747},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":34301,\"start\":34297},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":34415,\"start\":34411},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":34516,\"start\":34512},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":35258,\"start\":35254},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":36112,\"start\":36109},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":36828,\"start\":36824},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":36845,\"start\":36841},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":41294,\"start\":41290},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":47831,\"start\":47828},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":47833,\"start\":47831},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":47836,\"start\":47833},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":47892,\"start\":47889},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":47894,\"start\":47892},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":47896,\"start\":47894}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41014,\"start\":40590},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41081,\"start\":41015},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41327,\"start\":41082},{\"attributes\":{\"id\":\"fig_4\"},\"end\":41736,\"start\":41328},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42092,\"start\":41737},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":42382,\"start\":42093},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":43524,\"start\":42383},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":44143,\"start\":43525},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":45325,\"start\":44144},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":46517,\"start\":45326},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":46628,\"start\":46518},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":47514,\"start\":46629},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":47686,\"start\":47515},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":48110,\"start\":47687}]", "paragraph": "[{\"end\":4423,\"start\":2555},{\"end\":5240,\"start\":4425},{\"end\":6127,\"start\":5242},{\"end\":6177,\"start\":6129},{\"end\":6719,\"start\":6179},{\"end\":7809,\"start\":6785},{\"end\":8768,\"start\":7811},{\"end\":9768,\"start\":8800},{\"end\":11166,\"start\":9792},{\"end\":12931,\"start\":11199},{\"end\":13247,\"start\":12947},{\"end\":14173,\"start\":13288},{\"end\":14303,\"start\":14175},{\"end\":15451,\"start\":14305},{\"end\":15803,\"start\":15502},{\"end\":16123,\"start\":15805},{\"end\":17201,\"start\":16125},{\"end\":17538,\"start\":17203},{\"end\":17870,\"start\":17654},{\"end\":18269,\"start\":17909},{\"end\":18553,\"start\":18319},{\"end\":18595,\"start\":18555},{\"end\":18846,\"start\":18835},{\"end\":18939,\"start\":18848},{\"end\":19675,\"start\":19664},{\"end\":20451,\"start\":19677},{\"end\":21106,\"start\":20490},{\"end\":22836,\"start\":21108},{\"end\":23059,\"start\":22838},{\"end\":23444,\"start\":23133},{\"end\":23722,\"start\":23446},{\"end\":23852,\"start\":23724},{\"end\":24204,\"start\":23854},{\"end\":24320,\"start\":24206},{\"end\":24619,\"start\":24322},{\"end\":25115,\"start\":24621},{\"end\":25481,\"start\":25176},{\"end\":26355,\"start\":25524},{\"end\":27229,\"start\":26357},{\"end\":28113,\"start\":27277},{\"end\":28393,\"start\":28115},{\"end\":28757,\"start\":28395},{\"end\":29544,\"start\":28759},{\"end\":30335,\"start\":29546},{\"end\":30709,\"start\":30362},{\"end\":30814,\"start\":30711},{\"end\":31302,\"start\":30875},{\"end\":31675,\"start\":31304},{\"end\":32100,\"start\":31733},{\"end\":33417,\"start\":32102},{\"end\":34029,\"start\":33419},{\"end\":34870,\"start\":34085},{\"end\":35947,\"start\":34936},{\"end\":37028,\"start\":35949},{\"end\":38792,\"start\":37030},{\"end\":39848,\"start\":38833},{\"end\":40589,\"start\":39866}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15501,\"start\":15452},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17653,\"start\":17539},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17908,\"start\":17871},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18834,\"start\":18596},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19663,\"start\":18940},{\"attributes\":{\"id\":\"formula_5\"},\"end\":23132,\"start\":23060},{\"attributes\":{\"id\":\"formula_7\"},\"end\":25175,\"start\":25116},{\"attributes\":{\"id\":\"formula_8\"},\"end\":30874,\"start\":30815}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":36167,\"start\":36159}]", "section_header": "[{\"end\":2553,\"start\":2538},{\"end\":6738,\"start\":6722},{\"end\":6783,\"start\":6741},{\"end\":8798,\"start\":8771},{\"end\":9790,\"start\":9771},{\"end\":11197,\"start\":11169},{\"end\":12945,\"start\":12934},{\"end\":13286,\"start\":13250},{\"end\":18317,\"start\":18272},{\"end\":20488,\"start\":20454},{\"end\":25522,\"start\":25484},{\"end\":27247,\"start\":27232},{\"end\":27275,\"start\":27250},{\"end\":30360,\"start\":30338},{\"end\":31731,\"start\":31678},{\"end\":34083,\"start\":34032},{\"end\":34934,\"start\":34873},{\"end\":38831,\"start\":38795},{\"end\":39864,\"start\":39851},{\"end\":40599,\"start\":40591},{\"end\":41024,\"start\":41016},{\"end\":41091,\"start\":41083},{\"end\":41345,\"start\":41329},{\"end\":41746,\"start\":41738},{\"end\":43535,\"start\":43526},{\"end\":44155,\"start\":44145},{\"end\":45338,\"start\":45327},{\"end\":46529,\"start\":46519},{\"end\":47525,\"start\":47516},{\"end\":47698,\"start\":47688}]", "table": "[{\"end\":42382,\"start\":42144},{\"end\":43524,\"start\":42976},{\"end\":44143,\"start\":43550},{\"end\":45325,\"start\":44171},{\"end\":46517,\"start\":45355},{\"end\":47514,\"start\":47019}]", "figure_caption": "[{\"end\":41014,\"start\":40601},{\"end\":41081,\"start\":41026},{\"end\":41327,\"start\":41093},{\"end\":41736,\"start\":41348},{\"end\":42092,\"start\":41748},{\"end\":42144,\"start\":42095},{\"end\":42976,\"start\":42385},{\"end\":43550,\"start\":43537},{\"end\":44171,\"start\":44158},{\"end\":45355,\"start\":45342},{\"end\":46628,\"start\":46532},{\"end\":47019,\"start\":46631},{\"end\":47686,\"start\":47527},{\"end\":48110,\"start\":47701}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4281,\"start\":4275},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5382,\"start\":5376},{\"end\":14345,\"start\":14339},{\"end\":15680,\"start\":15674},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21545,\"start\":21535},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22388,\"start\":22382},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22640,\"start\":22630},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":33578,\"start\":33572},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":33665,\"start\":33659},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":33741,\"start\":33732},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":33792,\"start\":33783},{\"end\":38022,\"start\":38016},{\"end\":38638,\"start\":38632}]", "bib_author_first_name": "[{\"end\":48256,\"start\":48255},{\"end\":48642,\"start\":48641},{\"end\":48644,\"start\":48643},{\"end\":48654,\"start\":48653},{\"end\":48656,\"start\":48655},{\"end\":49042,\"start\":49041},{\"end\":49050,\"start\":49049},{\"end\":49373,\"start\":49372},{\"end\":49380,\"start\":49379},{\"end\":49389,\"start\":49388},{\"end\":49397,\"start\":49396},{\"end\":49406,\"start\":49405},{\"end\":49773,\"start\":49772},{\"end\":49782,\"start\":49781},{\"end\":49791,\"start\":49790},{\"end\":49801,\"start\":49800},{\"end\":49813,\"start\":49812},{\"end\":49825,\"start\":49824},{\"end\":50206,\"start\":50205},{\"end\":50216,\"start\":50215},{\"end\":50223,\"start\":50222},{\"end\":50232,\"start\":50231},{\"end\":50581,\"start\":50580},{\"end\":50589,\"start\":50588},{\"end\":50595,\"start\":50594},{\"end\":50601,\"start\":50600},{\"end\":50608,\"start\":50607},{\"end\":50618,\"start\":50614},{\"end\":50952,\"start\":50951},{\"end\":51277,\"start\":51276},{\"end\":51512,\"start\":51511},{\"end\":51521,\"start\":51520},{\"end\":51532,\"start\":51531},{\"end\":51542,\"start\":51541},{\"end\":51825,\"start\":51824},{\"end\":51832,\"start\":51831},{\"end\":51840,\"start\":51839},{\"end\":52158,\"start\":52157},{\"end\":52164,\"start\":52163},{\"end\":52172,\"start\":52171},{\"end\":52179,\"start\":52178},{\"end\":52185,\"start\":52184},{\"end\":52491,\"start\":52490},{\"end\":52840,\"start\":52839},{\"end\":52850,\"start\":52849},{\"end\":53135,\"start\":53134},{\"end\":53393,\"start\":53392},{\"end\":53401,\"start\":53400},{\"end\":53410,\"start\":53409},{\"end\":53418,\"start\":53417},{\"end\":53427,\"start\":53426},{\"end\":53429,\"start\":53428},{\"end\":53741,\"start\":53740},{\"end\":53749,\"start\":53748},{\"end\":53757,\"start\":53756},{\"end\":53766,\"start\":53765},{\"end\":53776,\"start\":53775},{\"end\":53786,\"start\":53785},{\"end\":54039,\"start\":54038},{\"end\":54263,\"start\":54262},{\"end\":54275,\"start\":54274},{\"end\":54287,\"start\":54286},{\"end\":54502,\"start\":54501},{\"end\":54512,\"start\":54511},{\"end\":54521,\"start\":54520},{\"end\":54728,\"start\":54727},{\"end\":54737,\"start\":54736},{\"end\":54934,\"start\":54933},{\"end\":54942,\"start\":54941},{\"end\":54951,\"start\":54950},{\"end\":54958,\"start\":54957},{\"end\":54972,\"start\":54971},{\"end\":54983,\"start\":54982},{\"end\":55221,\"start\":55220},{\"end\":55233,\"start\":55229},{\"end\":55240,\"start\":55239},{\"end\":55248,\"start\":55247},{\"end\":55250,\"start\":55249},{\"end\":55505,\"start\":55501},{\"end\":55512,\"start\":55511},{\"end\":55520,\"start\":55519},{\"end\":55529,\"start\":55528},{\"end\":55531,\"start\":55530},{\"end\":55782,\"start\":55781},{\"end\":55784,\"start\":55783},{\"end\":55794,\"start\":55793},{\"end\":55803,\"start\":55802},{\"end\":56058,\"start\":56057},{\"end\":56066,\"start\":56065},{\"end\":56068,\"start\":56067},{\"end\":56077,\"start\":56076},{\"end\":56084,\"start\":56083},{\"end\":56097,\"start\":56096},{\"end\":56108,\"start\":56104},{\"end\":56426,\"start\":56425},{\"end\":56700,\"start\":56699},{\"end\":56708,\"start\":56707},{\"end\":56716,\"start\":56715},{\"end\":56946,\"start\":56945},{\"end\":57177,\"start\":57176},{\"end\":57185,\"start\":57184},{\"end\":57193,\"start\":57192},{\"end\":57452,\"start\":57451},{\"end\":57458,\"start\":57457},{\"end\":57464,\"start\":57463},{\"end\":57473,\"start\":57472},{\"end\":57479,\"start\":57478},{\"end\":57747,\"start\":57746},{\"end\":57969,\"start\":57968},{\"end\":57975,\"start\":57974},{\"end\":57987,\"start\":57986},{\"end\":57997,\"start\":57996},{\"end\":58193,\"start\":58192},{\"end\":58464,\"start\":58463},{\"end\":58476,\"start\":58475},{\"end\":58486,\"start\":58485},{\"end\":58497,\"start\":58496},{\"end\":58511,\"start\":58510},{\"end\":58809,\"start\":58808},{\"end\":58818,\"start\":58817},{\"end\":58825,\"start\":58824},{\"end\":58832,\"start\":58831},{\"end\":59195,\"start\":59191},{\"end\":59203,\"start\":59202},{\"end\":59211,\"start\":59210},{\"end\":59217,\"start\":59216},{\"end\":59225,\"start\":59224},{\"end\":59234,\"start\":59233},{\"end\":59240,\"start\":59239},{\"end\":59602,\"start\":59598},{\"end\":59949,\"start\":59948},{\"end\":59958,\"start\":59957},{\"end\":59968,\"start\":59967},{\"end\":59978,\"start\":59977},{\"end\":59980,\"start\":59979},{\"end\":59997,\"start\":59996},{\"end\":60318,\"start\":60317},{\"end\":60327,\"start\":60326},{\"end\":60334,\"start\":60333},{\"end\":60341,\"start\":60340},{\"end\":60628,\"start\":60627},{\"end\":60635,\"start\":60634},{\"end\":60643,\"start\":60642},{\"end\":60651,\"start\":60650},{\"end\":60653,\"start\":60652},{\"end\":60660,\"start\":60659},{\"end\":60912,\"start\":60911},{\"end\":60923,\"start\":60922},{\"end\":60935,\"start\":60934},{\"end\":60945,\"start\":60944},{\"end\":61350,\"start\":61349},{\"end\":61363,\"start\":61362},{\"end\":61381,\"start\":61377},{\"end\":61391,\"start\":61390},{\"end\":61763,\"start\":61762},{\"end\":61769,\"start\":61768},{\"end\":61778,\"start\":61777},{\"end\":61785,\"start\":61784},{\"end\":61794,\"start\":61793},{\"end\":62077,\"start\":62076},{\"end\":62086,\"start\":62085},{\"end\":62088,\"start\":62087},{\"end\":62377,\"start\":62376},{\"end\":62719,\"start\":62718},{\"end\":62990,\"start\":62989},{\"end\":62998,\"start\":62997},{\"end\":63008,\"start\":63004},{\"end\":63300,\"start\":63299},{\"end\":63661,\"start\":63660},{\"end\":63945,\"start\":63944},{\"end\":63951,\"start\":63950},{\"end\":63957,\"start\":63956},{\"end\":63964,\"start\":63963},{\"end\":63970,\"start\":63969},{\"end\":64203,\"start\":64202},{\"end\":64210,\"start\":64209},{\"end\":64218,\"start\":64217},{\"end\":64226,\"start\":64225},{\"end\":64228,\"start\":64227},{\"end\":64576,\"start\":64575},{\"end\":64584,\"start\":64583},{\"end\":64592,\"start\":64591},{\"end\":64605,\"start\":64601},{\"end\":64611,\"start\":64610},{\"end\":64617,\"start\":64616},{\"end\":64837,\"start\":64836},{\"end\":64843,\"start\":64842},{\"end\":64852,\"start\":64851},{\"end\":64859,\"start\":64858},{\"end\":65075,\"start\":65074},{\"end\":65264,\"start\":65263},{\"end\":65274,\"start\":65273},{\"end\":65282,\"start\":65281},{\"end\":65294,\"start\":65293},{\"end\":65305,\"start\":65304},{\"end\":65488,\"start\":65487},{\"end\":65497,\"start\":65496},{\"end\":65506,\"start\":65505},{\"end\":65514,\"start\":65513},{\"end\":65833,\"start\":65832},{\"end\":65840,\"start\":65839},{\"end\":65851,\"start\":65850},{\"end\":65862,\"start\":65861},{\"end\":65864,\"start\":65863},{\"end\":65872,\"start\":65871},{\"end\":65874,\"start\":65873},{\"end\":65883,\"start\":65882},{\"end\":65885,\"start\":65884},{\"end\":66115,\"start\":66114},{\"end\":66128,\"start\":66127},{\"end\":66137,\"start\":66136},{\"end\":66149,\"start\":66148},{\"end\":66161,\"start\":66160},{\"end\":66442,\"start\":66441},{\"end\":66721,\"start\":66720},{\"end\":67046,\"start\":67045},{\"end\":67050,\"start\":67047},{\"end\":67386,\"start\":67385},{\"end\":67393,\"start\":67392},{\"end\":67400,\"start\":67399},{\"end\":67407,\"start\":67406},{\"end\":67763,\"start\":67762}]", "bib_author_last_name": "[{\"end\":48264,\"start\":48257},{\"end\":48651,\"start\":48645},{\"end\":48662,\"start\":48657},{\"end\":49047,\"start\":49043},{\"end\":49055,\"start\":49051},{\"end\":49377,\"start\":49374},{\"end\":49386,\"start\":49381},{\"end\":49394,\"start\":49390},{\"end\":49403,\"start\":49398},{\"end\":49410,\"start\":49407},{\"end\":49779,\"start\":49774},{\"end\":49788,\"start\":49783},{\"end\":49798,\"start\":49792},{\"end\":49810,\"start\":49802},{\"end\":49822,\"start\":49814},{\"end\":49831,\"start\":49826},{\"end\":50213,\"start\":50207},{\"end\":50220,\"start\":50217},{\"end\":50229,\"start\":50224},{\"end\":50239,\"start\":50233},{\"end\":50586,\"start\":50582},{\"end\":50592,\"start\":50590},{\"end\":50598,\"start\":50596},{\"end\":50605,\"start\":50602},{\"end\":50612,\"start\":50609},{\"end\":50623,\"start\":50619},{\"end\":50959,\"start\":50953},{\"end\":51281,\"start\":51278},{\"end\":51518,\"start\":51513},{\"end\":51529,\"start\":51522},{\"end\":51539,\"start\":51533},{\"end\":51550,\"start\":51543},{\"end\":51829,\"start\":51826},{\"end\":51837,\"start\":51833},{\"end\":51846,\"start\":51841},{\"end\":52161,\"start\":52159},{\"end\":52169,\"start\":52165},{\"end\":52176,\"start\":52173},{\"end\":52182,\"start\":52180},{\"end\":52191,\"start\":52186},{\"end\":52494,\"start\":52492},{\"end\":52847,\"start\":52841},{\"end\":52858,\"start\":52851},{\"end\":53140,\"start\":53136},{\"end\":53398,\"start\":53394},{\"end\":53407,\"start\":53402},{\"end\":53415,\"start\":53411},{\"end\":53424,\"start\":53419},{\"end\":53434,\"start\":53430},{\"end\":53746,\"start\":53742},{\"end\":53754,\"start\":53750},{\"end\":53763,\"start\":53758},{\"end\":53773,\"start\":53767},{\"end\":53783,\"start\":53777},{\"end\":53792,\"start\":53787},{\"end\":54050,\"start\":54040},{\"end\":54272,\"start\":54264},{\"end\":54284,\"start\":54276},{\"end\":54294,\"start\":54288},{\"end\":54509,\"start\":54503},{\"end\":54518,\"start\":54513},{\"end\":54526,\"start\":54522},{\"end\":54734,\"start\":54729},{\"end\":54746,\"start\":54738},{\"end\":54939,\"start\":54935},{\"end\":54948,\"start\":54943},{\"end\":54955,\"start\":54952},{\"end\":54969,\"start\":54959},{\"end\":54980,\"start\":54973},{\"end\":54987,\"start\":54984},{\"end\":55227,\"start\":55222},{\"end\":55237,\"start\":55234},{\"end\":55245,\"start\":55241},{\"end\":55256,\"start\":55251},{\"end\":55509,\"start\":55506},{\"end\":55517,\"start\":55513},{\"end\":55526,\"start\":55521},{\"end\":55537,\"start\":55532},{\"end\":55791,\"start\":55785},{\"end\":55800,\"start\":55795},{\"end\":55812,\"start\":55804},{\"end\":56063,\"start\":56059},{\"end\":56074,\"start\":56069},{\"end\":56081,\"start\":56078},{\"end\":56094,\"start\":56085},{\"end\":56102,\"start\":56098},{\"end\":56113,\"start\":56109},{\"end\":56431,\"start\":56427},{\"end\":56705,\"start\":56701},{\"end\":56713,\"start\":56709},{\"end\":56720,\"start\":56717},{\"end\":56950,\"start\":56947},{\"end\":57182,\"start\":57178},{\"end\":57190,\"start\":57186},{\"end\":57202,\"start\":57194},{\"end\":57455,\"start\":57453},{\"end\":57461,\"start\":57459},{\"end\":57470,\"start\":57465},{\"end\":57476,\"start\":57474},{\"end\":57487,\"start\":57480},{\"end\":57750,\"start\":57748},{\"end\":57972,\"start\":57970},{\"end\":57984,\"start\":57976},{\"end\":57994,\"start\":57988},{\"end\":58006,\"start\":57998},{\"end\":58206,\"start\":58194},{\"end\":58473,\"start\":58465},{\"end\":58483,\"start\":58477},{\"end\":58494,\"start\":58487},{\"end\":58508,\"start\":58498},{\"end\":58522,\"start\":58512},{\"end\":58815,\"start\":58810},{\"end\":58822,\"start\":58819},{\"end\":58829,\"start\":58826},{\"end\":58837,\"start\":58833},{\"end\":59200,\"start\":59196},{\"end\":59208,\"start\":59204},{\"end\":59214,\"start\":59212},{\"end\":59222,\"start\":59218},{\"end\":59231,\"start\":59226},{\"end\":59237,\"start\":59235},{\"end\":59243,\"start\":59241},{\"end\":59606,\"start\":59603},{\"end\":59955,\"start\":59950},{\"end\":59965,\"start\":59959},{\"end\":59975,\"start\":59969},{\"end\":59994,\"start\":59981},{\"end\":60008,\"start\":59998},{\"end\":60324,\"start\":60319},{\"end\":60331,\"start\":60328},{\"end\":60338,\"start\":60335},{\"end\":60345,\"start\":60342},{\"end\":60632,\"start\":60629},{\"end\":60640,\"start\":60636},{\"end\":60648,\"start\":60644},{\"end\":60657,\"start\":60654},{\"end\":60666,\"start\":60661},{\"end\":60920,\"start\":60913},{\"end\":60932,\"start\":60924},{\"end\":60942,\"start\":60936},{\"end\":60958,\"start\":60946},{\"end\":61360,\"start\":61351},{\"end\":61375,\"start\":61364},{\"end\":61388,\"start\":61382},{\"end\":61397,\"start\":61392},{\"end\":61766,\"start\":61764},{\"end\":61775,\"start\":61770},{\"end\":61782,\"start\":61779},{\"end\":61791,\"start\":61786},{\"end\":61799,\"start\":61795},{\"end\":62083,\"start\":62078},{\"end\":62097,\"start\":62089},{\"end\":62391,\"start\":62378},{\"end\":62728,\"start\":62720},{\"end\":62995,\"start\":62991},{\"end\":63002,\"start\":62999},{\"end\":63012,\"start\":63009},{\"end\":63305,\"start\":63301},{\"end\":63666,\"start\":63662},{\"end\":63948,\"start\":63946},{\"end\":63954,\"start\":63952},{\"end\":63961,\"start\":63958},{\"end\":63967,\"start\":63965},{\"end\":63973,\"start\":63971},{\"end\":64207,\"start\":64204},{\"end\":64215,\"start\":64211},{\"end\":64223,\"start\":64219},{\"end\":64239,\"start\":64229},{\"end\":64581,\"start\":64577},{\"end\":64589,\"start\":64585},{\"end\":64599,\"start\":64593},{\"end\":64608,\"start\":64606},{\"end\":64614,\"start\":64612},{\"end\":64625,\"start\":64618},{\"end\":64840,\"start\":64838},{\"end\":64849,\"start\":64844},{\"end\":64856,\"start\":64853},{\"end\":64863,\"start\":64860},{\"end\":65083,\"start\":65076},{\"end\":65271,\"start\":65265},{\"end\":65279,\"start\":65275},{\"end\":65291,\"start\":65283},{\"end\":65302,\"start\":65295},{\"end\":65313,\"start\":65306},{\"end\":65494,\"start\":65489},{\"end\":65503,\"start\":65498},{\"end\":65511,\"start\":65507},{\"end\":65519,\"start\":65515},{\"end\":65837,\"start\":65834},{\"end\":65848,\"start\":65841},{\"end\":65859,\"start\":65852},{\"end\":65869,\"start\":65865},{\"end\":65880,\"start\":65875},{\"end\":65891,\"start\":65886},{\"end\":66125,\"start\":66116},{\"end\":66134,\"start\":66129},{\"end\":66146,\"start\":66138},{\"end\":66158,\"start\":66150},{\"end\":66171,\"start\":66162},{\"end\":66448,\"start\":66443},{\"end\":66729,\"start\":66722},{\"end\":67055,\"start\":67051},{\"end\":67390,\"start\":67387},{\"end\":67397,\"start\":67394},{\"end\":67404,\"start\":67401},{\"end\":67412,\"start\":67408},{\"end\":67767,\"start\":67764}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":28581791},\"end\":48480,\"start\":48112},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":17622089},\"end\":48924,\"start\":48482},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6538247},\"end\":49264,\"start\":48926},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":230501627},\"end\":49678,\"start\":49266},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":5162860},\"end\":50115,\"start\":49680},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":59601271},\"end\":50488,\"start\":50117},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":23873598},\"end\":50849,\"start\":50490},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":195583959},\"end\":51164,\"start\":50851},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":218502373},\"end\":51463,\"start\":51166},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":4357800},\"end\":51696,\"start\":51465},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":233024940},\"end\":52063,\"start\":51698},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":220646752},\"end\":52387,\"start\":52065},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":220486896},\"end\":52730,\"start\":52389},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":9197086},\"end\":53038,\"start\":52732},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":195441638},\"end\":53299,\"start\":53040},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":22622013},\"end\":53638,\"start\":53301},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":7284862},\"end\":54007,\"start\":53640},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1033682},\"end\":54215,\"start\":54009},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":2057420},\"end\":54425,\"start\":54217},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":54482423},\"end\":54684,\"start\":54427},{\"attributes\":{\"doi\":\"arXiv:1411.1784\",\"id\":\"b20\"},\"end\":54883,\"start\":54686},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1563370},\"end\":55152,\"start\":54885},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6200260},\"end\":55418,\"start\":55154},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":195944196},\"end\":55714,\"start\":55420},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":145052179},\"end\":55967,\"start\":55716},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":76666202},\"end\":56327,\"start\":55969},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":220798108},\"end\":56628,\"start\":56329},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":219303407},\"end\":56878,\"start\":56630},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":220686452},\"end\":57091,\"start\":56880},{\"attributes\":{\"id\":\"b29\"},\"end\":57363,\"start\":57093},{\"attributes\":{\"id\":\"b30\"},\"end\":57651,\"start\":57365},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":233744385},\"end\":57944,\"start\":57653},{\"attributes\":{\"id\":\"b32\"},\"end\":58109,\"start\":57946},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":220545987},\"end\":58370,\"start\":58111},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":204539180},\"end\":58742,\"start\":58372},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":202782547},\"end\":59085,\"start\":58744},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":235313428},\"end\":59499,\"start\":59087},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":18507866},\"end\":59874,\"start\":59501},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":199501839},\"end\":60194,\"start\":59876},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":220525490},\"end\":60559,\"start\":60196},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":52958532},\"end\":60837,\"start\":60561},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":220324610},\"end\":61206,\"start\":60839},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":49215665},\"end\":61655,\"start\":61208},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":1177125},\"end\":62009,\"start\":61657},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":17288818},\"end\":62293,\"start\":62011},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":486254},\"end\":62615,\"start\":62295},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":198181171},\"end\":62907,\"start\":62617},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":199472854},\"end\":63183,\"start\":62909},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":2286534},\"end\":63554,\"start\":63185},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":21706467},\"end\":63907,\"start\":63556},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":35286171},\"end\":64161,\"start\":63909},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":204743907},\"end\":64519,\"start\":64163},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":57246310},\"end\":64788,\"start\":64521},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":206594692},\"end\":64993,\"start\":64790},{\"attributes\":{\"id\":\"b54\"},\"end\":65228,\"start\":64995},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":4309276},\"end\":65446,\"start\":65230},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":18509954},\"end\":65755,\"start\":65448},{\"attributes\":{\"id\":\"b57\"},\"end\":66073,\"start\":65757},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":10894094},\"end\":66393,\"start\":66075},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":204866526},\"end\":66630,\"start\":66395},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":52901563},\"end\":66957,\"start\":66632},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":5078022},\"end\":67244,\"start\":66959},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":235486149},\"end\":67684,\"start\":67246},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":199466111},\"end\":67897,\"start\":67686}]", "bib_title": "[{\"end\":48253,\"start\":48112},{\"end\":48639,\"start\":48482},{\"end\":49039,\"start\":48926},{\"end\":49370,\"start\":49266},{\"end\":49770,\"start\":49680},{\"end\":50203,\"start\":50117},{\"end\":50578,\"start\":50490},{\"end\":50949,\"start\":50851},{\"end\":51274,\"start\":51166},{\"end\":51509,\"start\":51465},{\"end\":51822,\"start\":51698},{\"end\":52155,\"start\":52065},{\"end\":52488,\"start\":52389},{\"end\":52837,\"start\":52732},{\"end\":53132,\"start\":53040},{\"end\":53390,\"start\":53301},{\"end\":53738,\"start\":53640},{\"end\":54036,\"start\":54009},{\"end\":54260,\"start\":54217},{\"end\":54499,\"start\":54427},{\"end\":54931,\"start\":54885},{\"end\":55218,\"start\":55154},{\"end\":55499,\"start\":55420},{\"end\":55779,\"start\":55716},{\"end\":56055,\"start\":55969},{\"end\":56423,\"start\":56329},{\"end\":56697,\"start\":56630},{\"end\":56943,\"start\":56880},{\"end\":57744,\"start\":57653},{\"end\":58190,\"start\":58111},{\"end\":58461,\"start\":58372},{\"end\":58806,\"start\":58744},{\"end\":59189,\"start\":59087},{\"end\":59596,\"start\":59501},{\"end\":59946,\"start\":59876},{\"end\":60315,\"start\":60196},{\"end\":60625,\"start\":60561},{\"end\":60909,\"start\":60839},{\"end\":61347,\"start\":61208},{\"end\":61760,\"start\":61657},{\"end\":62074,\"start\":62011},{\"end\":62374,\"start\":62295},{\"end\":62716,\"start\":62617},{\"end\":62987,\"start\":62909},{\"end\":63297,\"start\":63185},{\"end\":63658,\"start\":63556},{\"end\":63942,\"start\":63909},{\"end\":64200,\"start\":64163},{\"end\":64573,\"start\":64521},{\"end\":64834,\"start\":64790},{\"end\":65261,\"start\":65230},{\"end\":65485,\"start\":65448},{\"end\":66112,\"start\":66075},{\"end\":66439,\"start\":66395},{\"end\":66718,\"start\":66632},{\"end\":67043,\"start\":66959},{\"end\":67383,\"start\":67246},{\"end\":67760,\"start\":67686}]", "bib_author": "[{\"end\":48266,\"start\":48255},{\"end\":48653,\"start\":48641},{\"end\":48664,\"start\":48653},{\"end\":49049,\"start\":49041},{\"end\":49057,\"start\":49049},{\"end\":49379,\"start\":49372},{\"end\":49388,\"start\":49379},{\"end\":49396,\"start\":49388},{\"end\":49405,\"start\":49396},{\"end\":49412,\"start\":49405},{\"end\":49781,\"start\":49772},{\"end\":49790,\"start\":49781},{\"end\":49800,\"start\":49790},{\"end\":49812,\"start\":49800},{\"end\":49824,\"start\":49812},{\"end\":49833,\"start\":49824},{\"end\":50215,\"start\":50205},{\"end\":50222,\"start\":50215},{\"end\":50231,\"start\":50222},{\"end\":50241,\"start\":50231},{\"end\":50588,\"start\":50580},{\"end\":50594,\"start\":50588},{\"end\":50600,\"start\":50594},{\"end\":50607,\"start\":50600},{\"end\":50614,\"start\":50607},{\"end\":50625,\"start\":50614},{\"end\":50961,\"start\":50951},{\"end\":51283,\"start\":51276},{\"end\":51520,\"start\":51511},{\"end\":51531,\"start\":51520},{\"end\":51541,\"start\":51531},{\"end\":51552,\"start\":51541},{\"end\":51831,\"start\":51824},{\"end\":51839,\"start\":51831},{\"end\":51848,\"start\":51839},{\"end\":52163,\"start\":52157},{\"end\":52171,\"start\":52163},{\"end\":52178,\"start\":52171},{\"end\":52184,\"start\":52178},{\"end\":52193,\"start\":52184},{\"end\":52496,\"start\":52490},{\"end\":52849,\"start\":52839},{\"end\":52860,\"start\":52849},{\"end\":53142,\"start\":53134},{\"end\":53400,\"start\":53392},{\"end\":53409,\"start\":53400},{\"end\":53417,\"start\":53409},{\"end\":53426,\"start\":53417},{\"end\":53436,\"start\":53426},{\"end\":53748,\"start\":53740},{\"end\":53756,\"start\":53748},{\"end\":53765,\"start\":53756},{\"end\":53775,\"start\":53765},{\"end\":53785,\"start\":53775},{\"end\":53794,\"start\":53785},{\"end\":54052,\"start\":54038},{\"end\":54274,\"start\":54262},{\"end\":54286,\"start\":54274},{\"end\":54296,\"start\":54286},{\"end\":54511,\"start\":54501},{\"end\":54520,\"start\":54511},{\"end\":54528,\"start\":54520},{\"end\":54736,\"start\":54727},{\"end\":54748,\"start\":54736},{\"end\":54941,\"start\":54933},{\"end\":54950,\"start\":54941},{\"end\":54957,\"start\":54950},{\"end\":54971,\"start\":54957},{\"end\":54982,\"start\":54971},{\"end\":54989,\"start\":54982},{\"end\":55229,\"start\":55220},{\"end\":55239,\"start\":55229},{\"end\":55247,\"start\":55239},{\"end\":55258,\"start\":55247},{\"end\":55511,\"start\":55501},{\"end\":55519,\"start\":55511},{\"end\":55528,\"start\":55519},{\"end\":55539,\"start\":55528},{\"end\":55793,\"start\":55781},{\"end\":55802,\"start\":55793},{\"end\":55814,\"start\":55802},{\"end\":56065,\"start\":56057},{\"end\":56076,\"start\":56065},{\"end\":56083,\"start\":56076},{\"end\":56096,\"start\":56083},{\"end\":56104,\"start\":56096},{\"end\":56115,\"start\":56104},{\"end\":56433,\"start\":56425},{\"end\":56707,\"start\":56699},{\"end\":56715,\"start\":56707},{\"end\":56722,\"start\":56715},{\"end\":56952,\"start\":56945},{\"end\":57184,\"start\":57176},{\"end\":57192,\"start\":57184},{\"end\":57204,\"start\":57192},{\"end\":57457,\"start\":57451},{\"end\":57463,\"start\":57457},{\"end\":57472,\"start\":57463},{\"end\":57478,\"start\":57472},{\"end\":57489,\"start\":57478},{\"end\":57752,\"start\":57746},{\"end\":57974,\"start\":57968},{\"end\":57986,\"start\":57974},{\"end\":57996,\"start\":57986},{\"end\":58008,\"start\":57996},{\"end\":58208,\"start\":58192},{\"end\":58475,\"start\":58463},{\"end\":58485,\"start\":58475},{\"end\":58496,\"start\":58485},{\"end\":58510,\"start\":58496},{\"end\":58524,\"start\":58510},{\"end\":58817,\"start\":58808},{\"end\":58824,\"start\":58817},{\"end\":58831,\"start\":58824},{\"end\":58839,\"start\":58831},{\"end\":59202,\"start\":59191},{\"end\":59210,\"start\":59202},{\"end\":59216,\"start\":59210},{\"end\":59224,\"start\":59216},{\"end\":59233,\"start\":59224},{\"end\":59239,\"start\":59233},{\"end\":59245,\"start\":59239},{\"end\":59608,\"start\":59598},{\"end\":59957,\"start\":59948},{\"end\":59967,\"start\":59957},{\"end\":59977,\"start\":59967},{\"end\":59996,\"start\":59977},{\"end\":60010,\"start\":59996},{\"end\":60326,\"start\":60317},{\"end\":60333,\"start\":60326},{\"end\":60340,\"start\":60333},{\"end\":60347,\"start\":60340},{\"end\":60634,\"start\":60627},{\"end\":60642,\"start\":60634},{\"end\":60650,\"start\":60642},{\"end\":60659,\"start\":60650},{\"end\":60668,\"start\":60659},{\"end\":60922,\"start\":60911},{\"end\":60934,\"start\":60922},{\"end\":60944,\"start\":60934},{\"end\":60960,\"start\":60944},{\"end\":61362,\"start\":61349},{\"end\":61377,\"start\":61362},{\"end\":61390,\"start\":61377},{\"end\":61399,\"start\":61390},{\"end\":61768,\"start\":61762},{\"end\":61777,\"start\":61768},{\"end\":61784,\"start\":61777},{\"end\":61793,\"start\":61784},{\"end\":61801,\"start\":61793},{\"end\":62085,\"start\":62076},{\"end\":62099,\"start\":62085},{\"end\":62393,\"start\":62376},{\"end\":62730,\"start\":62718},{\"end\":62997,\"start\":62989},{\"end\":63004,\"start\":62997},{\"end\":63014,\"start\":63004},{\"end\":63307,\"start\":63299},{\"end\":63668,\"start\":63660},{\"end\":63950,\"start\":63944},{\"end\":63956,\"start\":63950},{\"end\":63963,\"start\":63956},{\"end\":63969,\"start\":63963},{\"end\":63975,\"start\":63969},{\"end\":64209,\"start\":64202},{\"end\":64217,\"start\":64209},{\"end\":64225,\"start\":64217},{\"end\":64241,\"start\":64225},{\"end\":64583,\"start\":64575},{\"end\":64591,\"start\":64583},{\"end\":64601,\"start\":64591},{\"end\":64610,\"start\":64601},{\"end\":64616,\"start\":64610},{\"end\":64627,\"start\":64616},{\"end\":64842,\"start\":64836},{\"end\":64851,\"start\":64842},{\"end\":64858,\"start\":64851},{\"end\":64865,\"start\":64858},{\"end\":65085,\"start\":65074},{\"end\":65273,\"start\":65263},{\"end\":65281,\"start\":65273},{\"end\":65293,\"start\":65281},{\"end\":65304,\"start\":65293},{\"end\":65315,\"start\":65304},{\"end\":65496,\"start\":65487},{\"end\":65505,\"start\":65496},{\"end\":65513,\"start\":65505},{\"end\":65521,\"start\":65513},{\"end\":65839,\"start\":65832},{\"end\":65850,\"start\":65839},{\"end\":65861,\"start\":65850},{\"end\":65871,\"start\":65861},{\"end\":65882,\"start\":65871},{\"end\":65893,\"start\":65882},{\"end\":66127,\"start\":66114},{\"end\":66136,\"start\":66127},{\"end\":66148,\"start\":66136},{\"end\":66160,\"start\":66148},{\"end\":66173,\"start\":66160},{\"end\":66450,\"start\":66441},{\"end\":66731,\"start\":66720},{\"end\":67057,\"start\":67045},{\"end\":67392,\"start\":67385},{\"end\":67399,\"start\":67392},{\"end\":67406,\"start\":67399},{\"end\":67414,\"start\":67406},{\"end\":67769,\"start\":67762}]", "bib_venue": "[{\"end\":48272,\"start\":48266},{\"end\":48678,\"start\":48664},{\"end\":49076,\"start\":49057},{\"end\":49449,\"start\":49412},{\"end\":49869,\"start\":49833},{\"end\":50277,\"start\":50241},{\"end\":50647,\"start\":50625},{\"end\":50983,\"start\":50961},{\"end\":51299,\"start\":51283},{\"end\":51556,\"start\":51552},{\"end\":51858,\"start\":51848},{\"end\":52199,\"start\":52193},{\"end\":52532,\"start\":52496},{\"end\":52864,\"start\":52860},{\"end\":53146,\"start\":53142},{\"end\":53442,\"start\":53436},{\"end\":53798,\"start\":53794},{\"end\":54101,\"start\":54052},{\"end\":54300,\"start\":54296},{\"end\":54532,\"start\":54528},{\"end\":54725,\"start\":54686},{\"end\":54999,\"start\":54989},{\"end\":55262,\"start\":55258},{\"end\":55543,\"start\":55539},{\"end\":55818,\"start\":55814},{\"end\":56119,\"start\":56115},{\"end\":56455,\"start\":56433},{\"end\":56728,\"start\":56722},{\"end\":56958,\"start\":56952},{\"end\":57174,\"start\":57093},{\"end\":57449,\"start\":57365},{\"end\":57788,\"start\":57752},{\"end\":57966,\"start\":57946},{\"end\":58214,\"start\":58208},{\"end\":58530,\"start\":58524},{\"end\":58888,\"start\":58839},{\"end\":59267,\"start\":59245},{\"end\":59663,\"start\":59608},{\"end\":60015,\"start\":60010},{\"end\":60351,\"start\":60347},{\"end\":60672,\"start\":60668},{\"end\":60993,\"start\":60960},{\"end\":61405,\"start\":61399},{\"end\":61807,\"start\":61801},{\"end\":62127,\"start\":62099},{\"end\":62431,\"start\":62393},{\"end\":62750,\"start\":62730},{\"end\":63020,\"start\":63014},{\"end\":63343,\"start\":63307},{\"end\":63704,\"start\":63668},{\"end\":64007,\"start\":63975},{\"end\":64317,\"start\":64241},{\"end\":64631,\"start\":64627},{\"end\":64869,\"start\":64865},{\"end\":65072,\"start\":64995},{\"end\":65319,\"start\":65315},{\"end\":65578,\"start\":65521},{\"end\":65830,\"start\":65757},{\"end\":66222,\"start\":66173},{\"end\":66486,\"start\":66450},{\"end\":66767,\"start\":66731},{\"end\":67079,\"start\":67057},{\"end\":67447,\"start\":67414},{\"end\":67774,\"start\":67769}]"}}}, "year": 2023, "month": 12, "day": 17}