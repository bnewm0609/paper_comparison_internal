{"id": 244950822, "updated": "2023-04-05 10:13:29.147", "metadata": {"title": "UniDoc: Uni\ufb01ed Pretraining Framework for Document Understanding", "authors": "[{\"first\":\"Jiuxiang\",\"last\":\"Gu\",\"middle\":[]},{\"first\":\"Jason\",\"last\":\"Kuen\",\"middle\":[]},{\"first\":\"Vlad\",\"last\":\"Morariu\",\"middle\":[\"I.\"]},{\"first\":\"Handong\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Rajiv\",\"last\":\"Jain\",\"middle\":[]},{\"first\":\"Nikolaos\",\"last\":\"Barmpalios\",\"middle\":[]},{\"first\":\"Ani\",\"last\":\"Nenkova\",\"middle\":[]},{\"first\":\"Tong\",\"last\":\"Sun\",\"middle\":[]}]", "venue": "NeurIPS", "journal": "39-50", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Document intelligence automates the extraction of information from documents and supports many business applications. Recent self-supervised learning methods on large-scale unlabeled document datasets have opened up promising directions towards reducing annotation efforts by training models with self-supervised objectives. However, most of the existing document pretraining methods are still language-dominated. We present UniDoc, a new uni\ufb01ed pretraining framework for document understanding. UniDoc is designed to support most document understanding tasks, extending the Transformer to take multimodal embeddings as input. Each input element is composed of words and visual features from a semantic region of the input document image. An important feature of UniDoc is that it learns a generic representation by making use of three self-supervised losses, encourag-ing the representation to model sentences, learn similarities, and align modalities. Extensive empirical analysis demonstrates that the pretraining procedure learns better joint representations and leads to improvements in downstream tasks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/GuKMZJBNS21", "doi": null}}, "content": {"source": {"pdf_hash": "6dae41d63eeed6a6aaccbb1c147826bdf34e14d5", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "5162ff352a0451dcf12a093eca12186435b7d94d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6dae41d63eeed6a6aaccbb1c147826bdf34e14d5.txt", "contents": "\nUniDoc: Unified Pretraining Framework for Document Understanding\n\n\nJiuxiang Gu jigu@adobe.com \nAdobe Research\n\n\nJason Kuen kuen@adobe.com \nAdobe Research\n\n\nVlad I Morariu morariu@adobe.com \nAdobe Research\n\n\nHandong Zhao hazhao@adobe.com \nAdobe Research\n\n\nNikolaos Barmpalios barmpali@adobe.com \nAdobe Document Cloud\n\n\nRajiv Jain rajijain@adobe.com \nAdobe Research\n\n\nAni Nenkova nenkova@adobe.com \nAdobe Research\n\n\nTong Sun tsun@adobe.com \nAdobe Research\n\n\nUniDoc: Unified Pretraining Framework for Document Understanding\n\nDocument intelligence automates the extraction of information from documents and supports many business applications. Recent self-supervised learning methods on large-scale unlabeled document datasets have opened up promising directions towards reducing annotation efforts by training models with self-supervised objectives. However, most of the existing document pretraining methods are still language-dominated. We present UniDoc, a new unified pretraining framework for document understanding. UniDoc is designed to support most document understanding tasks, extending the Transformer to take multimodal embeddings as input. Each input element is composed of words and visual features from a semantic region of the input document image. An important feature of UniDoc is that it learns a generic representation by making use of three self-supervised losses, encouraging the representation to model sentences, learn similarities, and align modalities. Extensive empirical analysis demonstrates that the pretraining procedure learns better joint representations and leads to improvements in downstream tasks.Huge training datasets help pretraining models to learn a good representation for downstream tasks, however, we observe three major problems with the current pretraining setup: (1) documents are composed of semantic regions. Most of the recent document pretraining works follow BERT and split documents into words. However, unlike the sequence-to-sequence learning in NLP, documents have a hierarchical structure (words form sentences, sentences form a semantic region, and semantic regions form a document). Also, the importance of words and sentences are highly context-dependent, i.e., the same word or sentence may have different importance in a different context. Moreover, current transformer-based document pretraining models suffer from input length constraints. Also, input 35th Conference on Neural Information Processing Systems (NeurIPS 2021).\n\nIntroduction\n\nDocument intelligence is a broad research area that includes techniques for information extraction and understanding. Unlike plain-text documents in natural language processing (NLP) [1,2], a physical document can be composed of multiple elements: tables, figures, charts, etc. In addition, a document usually includes rich visual information, and can be one of various types of documents (scientific paper, form, resume, etc.), with various combinations of multiple elements and layouts. Complex content and layout, noisy data, font and style variations make automatic document understanding very challenging. For example, to understand text-rich documents such as letters, a system needs to focus almost exclusively on text content, paying attention to a long sequential context, while processing semi-structured documents such as forms requires the system to analyze spatially distributed short words, paying particular attention to the spatial arrangement of the words. Following the success of BERT [3] on NLP tasks, there has been growing interest in developing pretraining methods for document understanding [4,5,6]. Pretrained models have achieved state-of-the-art (SoTA) performance across diverse document understanding tasks [7,8].\n\nlength becomes a problem for text-rich documents or multi-page documents. (2) documents are more than words. The semantic structure of the document is not only determined by the text within it but also the visual features such as table, font size and style, and figure, etc. Moreover, the visual appearance of the text within a block are often overlooked. Most of recent BERT-based pretraining works only take the words as input without considering multimodal content and alignment of multimodal information within semantic regions. (3) documents have spatial layout. Visual and layout information is critical for document understanding. Recent works encode spatial information via 2D position encoding and model spatial relationships with self-attention, which computes attention weights for long inputs [4,5]. However, for semi-structured documents, such as forms and receipts, words are more related to their local surroundings. This corresponds strongly with human intuitionwhen we look at magazines or newspapers, the receptive fields are modulated by our reading order and attention. Based on the above observations, we ask the following question: can unified document pretraining benefit all of these different kinds of documents?\n\nWe propose a unified pretraining framework for document understanding, shown in Fig. 1. Our model integrates image information in the pretraining stage by taking advantage of the transformer architecture to learn cross-modal interactions between visual and textual information. To handle textual information, we encode sentences using a hierarchical transformer encoder. The first level of the hierarchical encoder models the formation of the sentences from words. The second level models the formation of the document from sentences. With the help of the hierarchical structure, UniDoc learns how words form sentences and how sentences form documents. Meanwhile, it reduces model computation complexity exponentially and increases the number of input words. This also mimics human reading behaviors since the sentence/paragraph is a reasonable unit for people to read and understand-people rarely check the interactions between arbitrary words across different regions in order to understand an article. Convolution has been very successful in the extraction of local features that encode visual and spatial information [9], so we use convolution layers as a more efficient complement to self-attention for addressing local intra-region dependencies in a document image. Meanwhile, self-attention uses all input tokens to generate attention weights for capturing global dependencies. Thus, we combine convolution with self-attention to form a mixed attention mechanism that combines the advantages of the two operations.\n\nWe depart from previous vision-language pretraining [10,11] by extracting both the textual and visual features for each semantic region. We propose a novel gated cross-attentional transformer that enables information exchange between modalities. A visually-rich region (figure, chart, etc) may have stronger visual information than textual information. Instead of treating outputs from both modalities identically, we design a gating mechanism that can dynamically control the influence of textual and visual features. This approach enables cross-modal connections and allows for variable highlight the relevant information in visual and textual modality and enables cross-modal connections. During pretraining, the CNN-based visual backbone and multi-layer gated cross-attention encoder are jointly trained in both pretraining and fine-tuning phase.\n\nOur contributions are summarized as follows: (1) We introduce UniDoc, a powerful pretraining framework for document understanding. UniDoc is capable of learning contextual textual and visual information and cross-modal correlations within a single framework, which leads to better performance. (2) We present Masked Sentence Modeling for language modeling, Visual Contrastive Learning for vision modeling, and Vision-Language Alignment for pretraining. (3) We present extensive experiments and analyses to validate the effectiveness of the proposed UniDoc. Extensive experiments and analysis provide useful insights on the effectiveness of the pretraining tasks and show outstanding performance on various downstream tasks.\n\n\nRelated Work\n\nSelf-supervised learning has shown great success in producing generic representations that learn from large-scale unlabeled corpora [3]. Like the development of pretraining in computer vision [12] and NLP [3], there has been a surging interest in self-supervised learning for Vision-Language (VL) tasks [10,13,14,11]. Transformers [3] are the key technology that enables learning contextualized representations from large-scale unlabeled training data. The unique characteristics of document images (spatial layout and multiple elements) distinguish document image pretraining from pretraining works in NLP and VL domains. In the NLP domain, the inputs are pure texts without spatial layouts (bounding boxes). In the VL domain, the inputs are the visual objects and captions. While for  features with OCR bounding boxes and generates a multimodal embedding by combining the textual embedding and position encoding. The transformer-based encoder takes a set of masked multimodal embeddings as input and is pretrained with three pretraining tasks. All the network parameters except those of the textual encoder are jointly trained during both pretraining and fine-tuning phases. document images, the input elements are spatially distributed, and the visual and textual information co-occur within the semantic regions.\n\nSeveral recent works have explored pretraining on document images [4,5,15]. LayoutLM [4] extends BERT to learn contextualized word representations for document images through multi-task learning. It takes a sequence of OCR words as input during pretraining and incorporates the 2D position embedding as input for each token. However, LayoutLM only considers textual information during pretraining without modeling the alignment between visual and textual information-visual information is only incorporated into the model during the fine-tuning stage. The most recent version, LayoutLMv2 [5], improves on this by incorporating the image encoder into pretraining and jointly training the image encoder along with the BERT model. LayoutLMv2 splits the document image into several parts and concatenates the visual embeddings and text embeddings into a single sequence. Apart from masked language learning (MLM), LayoutLMv2 also considers image-text alignment and image-text matching during pretraining. The most related work to ours is SelfDoc [6], which proposes a multimodal document pretraining framework. It first extracts the document object proposals from pre-trained Faster R-CNN [16] and then applies OCR for each proposal to get the words. It takes the pre-extracted RoI features and sentence embeddings as input, and models the perform learning over the textual and visual information using the cross-modality encoder.\n\nThere is a noticeable difference between our proposed method, UniDoc, and other concurrent works in document image pretraining. UniDoc is a multimodal end-to-end pretraining framework for document images. Unlike the fixed document object detector in [6], the parameters of the image encoder with RoI align, which derive the visual features for semantic regions, are also updated in UniDoc. In contrast to [5], our visual features come from the semantic regions instead of splitting the image into fixed regions. Like the object-level semantic elements in natural images, for document images, we represent the typical document layout elements such as paragraph, title, figure, and table as semantic regions. Moreover, to learn the contextualized visual representations, UniDoc masks visual information in the latent space and learns contextualized representations by solving a contrastive learning task defined over a quantization of the latent visual embeddings.\n\n\nMethod\n\n\nModel Architecture\n\nFig. 1 illustrates our approach, UniDoc, which consists of four components: feature extraction, feature embedding, multi-layer gated cross-attention encoder, and pretraining tasks. Given a document image and the locations of document elements (sentence or RoI), UniDoc takes image regions and words that correspond to each document elements as inputs, and extracts their respective embeddings through a visual feature extractor and a sentence encoder. These embeddings are then fed into a transformer-based encoder to learn the cross-modal contextualized embeddings that integrate both visual features and textual features.\n\nIn the feature extraction step, we first employ an off-the-shelf OCR tool [17] to extract text from a document image I, where the words are grouped into sentences S = {s 1 , . . . , s N } whose corresponding bounding boxes are P = {p 1 , . . . , p N }. For each sentence bounding box p i , we use a ConvNet-based backbone f ImEnc and RoI Align [18] f RoIAlign to extract the pooled RoI features v i . To obtain a feature embedding, we extract the sentence embedding s i for each sentence s i via a pretrained sentence encoder f SentEnc . Each region's RoI feature v i is discretized into a finite set of visual representations v Q i \u2208 V Q via product quantization [19]. The multi-layer Gated Cross-Attention encoder takes the position information, masked visual features\u1e7c and masked textual featuresS as inputs, and then it generates the contextualized multimodal representations (H l V and H l S , l \u2208 [1, L]) and outputs the predicted features (V and\u015c), where L is the number of stacked transformer blocks.\n\nMore formally, the pretraining procedure can be decomposed into the following steps: (1) where f Mask denotes the masking function that randomly masks RoI features and sentence embeddings with the respective probabilities p v Mask and p s Mask . L Pretraining is composed of three pretraining tasks: Masked Sentence Modeling (MSM), Visual Contrastive Learning (VCL), and Vision-Language Alignment (VLA). Next, we provide details mentioned in Eq. 1.\nI OCR \u2212\u2212\u2192 P S fImEnc+fRoIAlign \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 fSentEnc V, V Q S \u2212 \u2212\u2212 \u2192 fMask \u1e7c S \u2212 \u2192 H l V H l S \u2212 \u2192 V S \u2212 \u2192 L Pretraining\nFeature Extraction and Embedding. Formally, a document image I \u2208 R W \u00d7H consists of N regions, where each region's bounding box is characterized by a 6-d vector, as Quantization Module. Unlike the fixed image encoder in [6], we jointly learn the image encoder in an end-to-end fashion alongside the multimodal model. A visual representation can be learned by predicting the visual features of the masked regions, but it is challenging to predict such features exactly, since they are unconstrained and of continuous representation. To constrain the representation space of the visual features and facilitate the end-to-end learning of image encoder (see Task #2 in Sec. 3.2), we follow [20,21] and use vector quantization to discretize the visual features\np i = { xLT W , yLT H , xRB W , yRB H , w W , h H },V = {v 1 , . . . , v N } into a finite set of representations V Q = {v Q 1 , . . . , v Q N }.\nSpecifically, we define latent embedding spaces e \u2208 R C\u00d7E , where C is the number of codebooks, and E is the number of entries for each codebook. For each v i , we first map it to logits v i \u2208 R C\u00d7E , and calculate the probability for the j-th codebook entry in i-th group as\np c,e = exp((v c,e + g e )/\u03c4 )/ E k=1 exp((v c,k + g k )/\u03c4 ),\nwhere \u03c4 is a non-negative temperature, g 1:E are i.i.d samples drawn from Gumbel(0,1) distribution. During the forward pass, we choose one entry vector from each codebook by\u1ebd i \u223c argmax e p c,e and generate the quantized representation v Q i by a concatenation of {\u1ebd 1 , . . . ,\u1ebd G } which is then followed by a linear transformation. During the backward pass, the gradients are computed through a Gumbel-Softmax estimator [22].\n\nGated Cross-Attention. To model the interactions among multimodal inputs, we introduce a multimodal transformer with gated cross-attention to model the cross-modality relationships. Let H l+1 m be output features at the l-th layer for one modality m, and let n be another modality (m, n \u2208 {V, S}). We obtain the features at (l + 1)-th layer as:\nH l+1 m = f LN f LN H l m + f l Cross-Att (H l m |H l n ) + f l FF f LN (H l m + f l Cross-Att (H l m |H l n ))(2)\nwhere f LN denotes layer normalization [23]. The feed-forward sub-layer f FF in Eq. 2 is further composed of two fully-connected sub-layers, both wrapped in residual adds and f LN .\n\nThe core part of Eq. 2 is the cross-attention f Cross-Att (\u00b7). Given the intermediate representations H l m and H l n , the cross-attention output for modality m is computed as:\nf Cross-Att (H l m |H l n ) = [Cross-Att 1 (H l m |H l n ); . . . ; Cross-Att h (H l m |H l n )]U (3) Cross-Att i (H l m |H l n ) = softmax f i q (H l m )f i k (H l n ) T / \u221a d f i v (H l n ) (4) where f i q (H l m ), f i k (H l n ), and f i v (H l n )\nare the query, key, and value calculated by linear mapping layers for the i-th head. d is the model dimension, h is the number of heads, and U is the weight matrix that combines the outputs of the heads.\n\nConsidering the substantial diversity of document images and the different information needs of differing document types, we use a gating mechanism [24] to dynamically weight the outputs of the visual and textual branches. Specifically, we feed the concatenated the visual and textual features to a non-linear network f Gate ([H l+1 m ; H l+1 n ]), which generates the modality-specific attention weights \u03b1 l m and \u03b1 l n , and returns the weights separately to their respective modality-specific branches to perform element-wise products. We multiply the features for modality m with its modality-specific attention weight, and compute the updated feature as: H l+1 m = H l+1 m (1 + \u03b1 l m ), same that for modality n.\n\n\nTraining Tasks and Objectives\n\nThe full pretraining objective of UniDoc (right block in Fig. 1) is defined as: L Pretraining = L MSM + L VCL + L VLA . In the rest of this section, we describe each task in detail.\n\nTask #1 : Masked Sentence Modeling. This task is similar to the MLM task utilized in BERT.\n\nThe key difference is that we mask sentences instead of tokens. During pretraining, each sentence and RoI of the input document is randomly and independently masked. For the masked sentence, its token is replaced with a special sentence of [MASK]. The model is trained to predict the masked sentence feature, based on the unmasked words and the visual features. The goal is to predict the masked sentence embeddings based on the contextual information from the surrounding sentences and image regions, by minimizing the smooth L1 loss [16]:\nL MSM (\u0398) = i smooth L1 (s i \u2212 f UniDoc (s i |s \\i ,\u1e7c))(5)\nwhere \u0398 is the trainable parameters and f UniDoc (.) outputs the unmasked textual feature, s \\i is the surrounding features for the i-th input,\u1e7c are the image features with random masking.\n\nTask #2 : Visual Contrastive Learning. We learn visual feature representations by solving a visual contrastive learning task which requires estimating the true quantized latent RoI representation. Given a predictionv i \u2208V for the masked RoI\u1e7d i \u2208\u1e7c, the model needs to estimate the positive quantized representation v Q i in a set of quantized candidate representations V Q . Good representations are learned by maximizing the agreement between output representation and quantized representation of the same RoIs as follows:\nL VCL (\u0398) = \u2212 \u1e7di\u2208\u1e7c log exp(sim(v i , v Q i )/\u03ba) v Q j exp(sim(v i , v Q j )/\u03ba) + \u03bb 1 CE C c=1 E e=1 p c,e log p c,e(6)\nwhere sim(\u00b7, \u00b7) computes the cosine similarity between two vectors, \u03bb is a hyperparameter, and \u03ba is a temperature scalar. The second term encourages the model to use the codebook entries more equally.\n\nTask #3 : Vision-Language Alignment. To enforce the alignment among different modalities, we explicitly encourage alignment between words and image regions via similarity-preserving knowledge distillation [25]. Note that, unlike the text-image alignment in LayoutLMv2 [5] which splits the image into four regions and predicts whether the given word is covered or not on the image side, we align the image and text belonging to the same region. The goal is to minimize the differences between the pairwise similarities of sentence embeddings and the pairwise similarities of image region features:\nL VLA (\u0398) = 1 N \u00d7 N ||f Norm (S \u00b7 S ) \u2212 f Norm (H L V \u00b7 H L V )|| 2 F(7)\nwhere S is the unmasked input sentence embeddings, H L V is the mapped visual representations of the final layer, || \u00b7 || F is the Frobenius norm, and f Norm performs L2 normalization.\n\n\nExperiment\n\n\nPretraining UniDoc\n\nPretraining corpus. We build our pretraining corpus based on IIT-CDIP Test Collection 1.0 [26], which contains more than 11M scanned document images. To differentiate pretraining from finetuning, we filter out the document images of RVL-CDIP [8] from IIT-CDIP since it is a subset of IIT-CDIP, and sample 1M document images as our pretraining corpus. Table 1: Comparison of the datasets used for pretraining and finetuning process. 'Box', 'Label', and 'Text' indicate the availability of location, label and text annotations for document entities. 'Tag' denotes the document class label availability.\n\n\nDataset\n\nType Size Box Label Text Tag IIT-CDIP [26] Misc 11M RVL-CDIP [8] Misc 400K CORD [7] Receipt 1K FUNSD [27] Form 0.2K PubLayNet [28] Article 347K Table 1 shows the dataset statistics. IIT-CDIP only provides the OCR texts in XML format. We extract words and their locations by applying EasyOCR [17] on document images. As shown in Fig. 3 (a), EasyOCR provides two kinds of output modes: non-paragraph and paragraph. The paragraph mode groups the non-paragraph results into text regions. We think document image pretraining should be treated differently than sequence-based pretraining in NLP, since the words in the document (2D) are arranged according to spatial layouts, while the words in NLP corpora are sequential (1D). Considering the special characteristics of documents (complex layout, multi-pages) and the limited input length of BERT models, it is not intuitive to formulate the input at the word level. Hence, we adopt the paragraph-level outputs as the basic input elements since textual regions provide semantically more meaningful information than independent words. There are some advantages to our design: (1) the region-level design hierarchically encodes document elements and this facilitates the modeling of latent relationships at the region level which has higher-level semantics than the word level. (2) the hierarchical encoding also overcomes the input size limitation of word-level BERT-based models [4,5]. Fig. 2 shows the distribution of words per region on RVL-CDIP. It can be seen that even though we consider regionlevel input, for some semi-structured documents, single-words dominate the inputs; this somehow forces UniDoc to pay attention to word-level inputs. Unlike MLM that predicts the masked word, UniDoc predicts the textual embedding of the masked input with MSM.\n\nPretraining setting. We initialize the sentence encoder f SentEnc with BERT-NLI-STSb-base [29] pretrained for NLI [30] and STS-B [31]. The ResNet-50 backbone in the image encoder is pretrained on the PubLayNet training set [28]. All the parameters (except f SentEnc and f ImEnc ) are randomly initialized. During pretraining, we freeze the parameters of f SentEnc and jointly train the visual encoder and multi-modal UniDoc model in an end-to-end fashion. Such an end-to-end training allows the ConvNet and Transformer to realize their full potentials in spatial and sequence modeling for pretraining. UniDoc contains 12 layers of gated cross-attention transformer blocks. We set the hidden size to 768 and the number of heads to 12, the maximum number of regions N to 64, and the maximum input sequence length for f SentEnc to 512. The pretraining is conducted on 8 NVIDIA Tesla V100 32GB GPUs with a batch size of 64. It is trained with Adam optimizer [32], with an initial learning rate of 10 \u22125 , weight decay of 10 \u22124 , and learning rate warmup in the first 20% iterations.\n\nTo learn a useful multimodal representation, random masking is applied to both textual and visual inputs. For MSM, we set the mask probability p s Mask for input sentences to 15%. 80% among the masked sentences are replaced by special sentence [CLS, MASK, SEP], while 10% sentences are replaced by random sentences sampled from other documents, and 10% remains unchanged. For VCL, the \u03bb is set to 0.1, \u03ba is set to 0.1, the mask probability p v Mask is set to 7.5% and the masked RoI features are filled with zeros. The temperature \u03c4 is annealed from 2.0 to 0.5 by a factor of of 0.999995 at every iteration. We select the pretraining checkpoint with the lowest L Pretraining for finetuning stage.\n\n\nFinetuning Tasks\n\nForm Understanding. Form understanding requires the model to predict the label for each semantic entity. We use FUNSD [27] as the evaluation dataset. It contains 149/50 training/testing images. Fig. 3 (b) shows a sample from FUNSD. Each semantic entity comprises a list of words, a label, and a bounding box. The officially-provided OCR texts and bounding boxes are used during training and testing. We take the semantic entities as input and feed the concatenated visual and textual output representations to a classifier. We apply cross-entropy loss for finetuning. The model is finetuned for 100 epochs with a learning rate of 10 \u22125 and batch size of 16. All the parameters except f SentEnc are trained. One of question, answer, header or other is predicted for each semantic entity. We use entity-level F1 score as the evaluation metric.\n\nReceipt Understanding. Receipt understanding requires the model to recognize a list of text lines with bounding boxes. The performance on this task is evaluated on CORD [7] dataset. It contains 626/247 receipts for training/testing. The receipts are labeled with 30 types of entities under 4 categories: company, date, address, and total. Like FUNSD, we feed the concatenated visual and textual output representations to the classifier. The model is finetuned for 200 epochs with a batch size of 16 and a learning rate of 10 \u22125 . The evaluation metric is entity-level F1 score. Document Classification. Document classification involves predicting the category for each document image. We use RVL-CDIP [8] as the target dataset. It consists of 320K/40K/40K training/validation/testing images under 16 categories. The OCR words and bounding boxes are extracted by EacyOCR. To fine-tune UniDoc on RVL-CDIP, we compute the overall representation as an element-wise product between the visual and textual representations averaged from all sentences/regions, and learn a classifier on top of the overall representation with cross-entropy loss. We fine-tune the model for 30 epochs with a batch size of 64 and a learning rate of 10 \u22125 . Classification accuracy over 16 categories is used to measure model performance.\n\nDocument Object Detection. Document object detection involves decomposing a document image into semantic units. We evaluate the effectiveness of our pretrained visual backbone on Pub-LayNet [28]. As shown Fig. 3 (d), the documents in PubLayNet are scientific articles. PubLayNet consists of 336K/11K training/validation images with six category labels (text, title, list, figure, and table). We train Faster-RCNN (F-RCNN) using Detectron2 [33] and initialize the visual backbone with the pretrained ResNet-50 from UniDoc. The model is trained for 180k iterations with a base learning rate of 0.01 and a batch size of 8. Mean average precision (MAP) @ intersection over union (IOU) [0.50:0.95] of bounding boxes is used to measure the performance.\n\n\nResults and Discussion\n\nThe importance of multimodal learning. To study the effect of multimodal learning, we experiment in three different settings (1) Vision only (V): this setting omits the textual components of UniDoc and adopts multilayer self-attention transformer to learn the visual representation. (2) Language only (L): this setting omits the visual encoder and keeps only the textual components.\n\n(3) Vision-Language (V+L): this setting considers both vision and language information. We first train three settings without pretraining. Table 2 shows consistent improvement across tasks for V+L over the single-stream baselines (V or L). This demonstrates that our UniDoc model is able to learn important visual-linguistic relationships that benefit downstream tasks even without pretraining.\n\nIn Table 2, we find that visual information dominates the performance of document classification, while language information contributes a lot to form understanding and receipt understanding. The results also indicate that different document tasks rely on different information. For document entity recognition tasks, language information is more important than visual features. As can be seen in Fig. 3 (b) and (c), entity recognition is more word-oriented. On the other hand, document classification is more focused on global-level understanding. As a result, visual and layout information contribute a lot to the final prediction of the document classification model. This matches well with the innate abilities of humans to distinguish between document types without fully understanding the words. We also observe that gated cross-attention (V+L) achieves a better performance than the non-gated version (V+L ), as its gating mechanism can learn to adaptively determine how much each modality contributes to the output features.\n\nEffect of pretraining tasks. We analyze the effectiveness of different pretraining settings through ablation studies over FUNSD, CORD, and RVL-CDIP, which are representative document benchmarks. Table 2 ablates the key design choices in pretraining UniDoc. For experimental efficiency, UniDoc models evaluated here are trained with 5 epochs on 300k training corpus. Overall, the pretraining of UniDoc consistently improves the performance over all three downstream tasks. The improvement gains vary among different tasks. We first establish two baselines: MSM+MVM in Table 2 indicates the combination of masked sentence learning and masked visual feature prediction. Similar to MSM, for MVM, we freeze the visual backbone and perform masked visual feature prediction via RoI-feature regression. MSM+VCL jointly trains the visual backbone end-to-end with contrastive learning. As shown in the Table 2, MSM+MVM achieves better results than the model without pretraining. Furthermore, when combining VCL together with MSM, consistent performance gains are observed across all the benchmarks. Among the three finetuning tasks, the improvements on FUNSD and CORD are bigger than on RVL-CDIP. We think the local context modeling capability of the ConvNet-based image encoder brings more benefits to entity recognition, since entities are heavily linked and correlated to their local surroundings. When MSM, VCL, and VLA are jointly trained, we observe further performance gains across all the benchmarks. For VCL, instead of sampling the negatives from the same input document, we also try including the negative samples from other document images of the same batch. However, we find that sampling negatives from the entire batch of document images hurt the performance. This is likely because the negatives from other document images are easy to distinguish from each other.  We also consider the image-text matching task (Rel) [5],   Table 3. UniDoc outperforms previous models on FUNSD and CORD, by a significantly large margin, demonstrating that our proposed approach is highly effective, partially due to the end-to-end training of the image encoder that improves the semantic alignments between images and texts. Note that UniDoc is pretrained on a subset of IIT-CDIP (1M document images), which is considerably less than the 11M document images used in LayoutLM [4] and LayoutLMv2 [5]. TILT [34] builds a 1.1M pretraining corpus by combining RVL-CDIP, UCSF Industry Documents Library, and Common Crawl. UniDoc also achieves promising results on document classification. Note that both LayoutLM v2 and TILT use Microsoft OCR, which is a commercial service with a stronger OCR performance than EasyOCR, which is used in our experiments. We find that OCR plays a key role in document classification performance. As shown in Fig. 4, UniDoc performs the best on the 'email' category but worst on the 'form' category. We also report the results with different OCR engines: 93.42 (Tesseract [35]) vs. 93.96 (EasyOCR [17]) vs. 94.10 (Google OCR [36]). UniDoc with EasyOCR achieves a better performance than with Tesseract since EasyOCR is powered by an advanced neural network, while Tesseract is based on less sophisticated techniques. Since different tasks require task-specific input embeddings to perform well, instead of finetuning the sentence encoder during pretraining, we explore unfreezing the sentence encoder during the finetuning stage (named as UniDoc * ) and report the results in Table 3. Unsurprisingly, we see performance improvements on several downstream applications. E.g., RVL-CDIP: 93.96 (UniDoc) vs. 95.05\u2191 (UniDoc * ). However, this also makes the training more challenging in terms of computational resources and training time. Effect of visual backbone. Additionally, we apply the trained visual backbone to document object detection on PubLayNet. The performance of the F-RCNN on the validation set is depicted in Table 4. To better compare, we establish two F-RCNN models with: (1) backbone initialized with ResNet-50 pretrained on ImageNet; (2) backbone initialized from UniDoc's pretrained visual backbone. It can be seen that our pretrained backbone outperforms ImageNet-pretrained backbones. By leveraging UniDoc, we can train different variants of the visual backbone and apply them to document-specific downstream applications, without relying on incompatible pretrained backbones from other domains (e.g., natural image). Moreover, the visual backbone of UniDoc does not require any custom layers, and thus any ConvNet architecture can be used in place of ResNet.\n\n\nConclusion, Limitations, and Future Works\n\nWe develop UniDoc, a unified pretraining framework for document understanding. Our model introduces a novel joint training framework that effectively exploits the visual and textual information during pretraining and finetuning. We evaluate the UniDoc comprehensively on three downstream tasks: form understanding, receipt understanding, and document image classification. Extensive empirical analysis demonstrates that the pretraining procedure can take advantage of multimodal inputs and effectively aggregating and aligning visual and textual information of document images with the proxy tasks. This work has a broader impact on document applications. By finetuning the pretrained UniDoc on task-specific data, document processing systems can provide better results and reduce the expensive data annotations costs. In terms of negative social impact, the document images used for pretraining may contain sensitive information and therefore the models trained on such data may inappropriately leak some private information. To address the privacy leakage, it is worthwhile to explore the combination of privacy-preserving learning and self-supervised learning.\n\nThere are interesting short-and long-term research directions for UniDoc: (1) we freeze the sentence encoder during pretraining and fine-tuning phases due to computational constraints. A better document representation can be learned by jointly training the sentence encoder, visual backbone and cross-attention encoder in a completely end-to-end fashion. (2) Although impressive performance has been achieved in document entity recognition tasks such as form and receipt understanding, the classification accuracy on semi-structured documents such as forms is still inferior to that of rich-text documents. It is possible to devise a better method to model the spatial relationship among words.\n\n(3) An interesting direction is to extend UniDoc to multipage/multilingual document pretraining. Additionally, there exist many text-based labeled document datasets in the NLP domain, such as document summarization. Can we transfer the knowledge learned from the text-based document domain to the image-based document domain? How to unify the pretraining of the pure-text document (1D) and image-based document (2D) in a single framework is also worth to try. Lastly, the use of different OCR tools is one of the major sources of inconsistency among the existing document pretraining works. It is worthwhile and essential to build standardized pretraining document image datasets with preprovided OCR results. In addition to scanned documents, using digital PDF as part of the pretraining data is a direction worth exploring since it provides rich metadata which could be beneficial for multimodal learning.\n\nFigure 1 :\n1Overview of the proposed approach, UniDoc. UniDoc first uses a CNN-based visual backbone to learn visual representations. The model then extracts the Region of Interest (RoI)\n\n\nwhere w and h are of the width and height the region, W and H are the width and height of I, while (x LT , y LT ) and (x RB , y RB ) denote the coordinates of the top-left and bottom-right corners respectively. The 6-d vector is mapped into a high-dimensional representation via a linear mapping function. The visual embedding is the sum of the mapped RoI feature and position embedding. Likewise, the textual embedding is the sum of sentence embedding and position embedding. We also have different types of segments to distinguish different modalities. The input sequence to the transformer-based encoder starts with a special start element ([CLS] and full visual features), then it is followed by multimodal elements, and it ends with a special ending element ([SEP]+full visual features). For the special elements ([CLS] and [SEP]), the corresponding full visual features are features extracted from the whole input image, by applying f ImEnc to an RoI covering the whole input image.\n\nFigure 2 :\n2Distribution of words per region on RVL-CDIP according to the categories.\n\nFigure 3 :\n3Document image samples. The boxes in red/green are OCR bounding boxes obtained with/without paragraph mode, while the boxes in blue are officially-provided bounding boxes.\n\n( a )\naSamples from RVL -CDI P (b) Per for mance Compar sion on 16 Classes\n\nFigure 4 :\n4For (a) we show the samples from RVL-CDIP. The boxes in orange color are grouped OCR bounding boxes. For (b) we plot the accuracies on 16 classes achieved by different models that are represented by different colors in the bar chart.\n\n\nPretr aining Task: Visual Contr astive L ear ning Pretr aining Task: Vision-L anguage Alignment ....OCR \n\n[CL S] \n\nI mg Feat \n\nL ocations \n+ \nWor ds \n+ \nRoI Features \n\nSentence 1 \n\nRoI Feat 1 \n\nSentence 2 \n\nRoI Feat 2 \n\nSentence 3 \n\nRoI Feat 3 \n\n[SEP] \n\nI mg Feat \n\nCross-Attention \n\nCross-Attention \n\nFeature Extr action \nGated Cross-Attention \n\nFeature Embedding \nPretr aining Tasks \n\n... \n\nVL A \n\nQuantization \n\nQuantization \n\nQuantization \n\nNegative \nPositive \n\n... \n\nVL A \n\nPretr aining Task: M asked Sentence M odeling \n\nM SM \n\nVL A \n\nUnlabeled Document I mages \n\nM ASK \n\nM ASK \n\n... \n\n\n\nTable 2 :\n2Experimental results and comparison on FUNSD, CORD, and RVL-CDIP test sets.Pretraining \nFUNSD CORD RVL-CDIP \nEnable #Data Modality Max #Words #Param. \nTasks \nEpoch F1 \nF1 Accuracy \n\n-\nV \n-\n85M \n-\n-\n77.49 57.08 \n91.35 \n-\nL \n-\n153M \n-\n-\n78.46 71.52 \n86.82 \n-\nV+L \n-\n255M \n-\n-\n80.60 95.98 \n92.76 \n-\nV+L \n-\n267M \n-\n-\n83.34 96.59 \n92.93 \n\n300K V+L \n64 \u00d7 512 \n270M \nMSM + MVM \n5 \n84.37 97.44 \n93.10 \n300K V+L \n64 \u00d7 512 \n272M \nMSM + VCL \n5 \n86.87 98.70 \n93.59 \n300K V+L \n64 \u00d7 512 \n272M \nMSM + VCL + VLA \n5 \n87.38 98.75 \n93.92 \n300K V+L \n64 \u00d7 512 \n274M MSM + VCL + VLA + REL 5 \n87.20 98.13 \n93.64 \n\n\n\nTable 3 :\n3Comparison with state-of-the-art methods. The symbol \u2021 implies using Google OCR engine.Performance Comparison with SoTA. We further pretrain UniDoc on 1M document images with 5 epochs and report the finetuning results inMethod \nPretraining \nFUNSD CORD RVL-CDIP \nSource #Data Scale Max #Words Modality #Param. F1 \nF1 Accuracy \nBERTBASE [5] \n-\n-\nWord \n512 \nL \n110M 60.26 89.68 \n89.81 \nBERTLARGE [5] \n-\n-\nWord \n512 \nL \n340M 65.63 90.25 \n89.92 \nLayoutLM BASE [5] \nIIT-CDIP 11M Word \n512 \nL \n113M 78.66 94.72 \n94.42 \nLayoutLM LARGE [5] \nIIT-CDIP 11M Word \n512 \nL \n343M 78.95 94.93 \n94.43 \nLayoutLMv2 BASE [5] IIT-CDIP 11M Word \n512 \nV+L \n200M 82.76 94.95 \n95.25 \nLayoutLMv2 LARGE [5] IIT-CDIP 11M Word \n512 \nV+L \n426M 84.20 96.01 \n95.64 \nSelfDoc [6] \nRVL-CDIP 320K Region 50\u00d7512 \nV+L \n-\n83.36 \n-\n92.81 \nSelfDoc+VGG-16 [6] RVL-CDIP 320K Region 50\u00d7512 \nV+L \n-\n-\n-\n93.81 \nTILT-Base [34] \nRVL-CDIP+ 1.1M Word \n512 \nV+L \n230M \n-\n95.11 \n95.25 \nTILT-Large [34] \nRVL-CDIP+ 1.1M Word \n512 \nV+L \n780M \n-\n96.33 \n95.52 \nUniDoc \nIIT-CDIP 1M Region 64\u00d7512 \nV+L \n272M 87.96 98.85 \n93.96 \nUniDoc  *  \nIIT-CDIP 1M Region 64\u00d7512 \nV+L \n272M 87.93 98.94 95.05  \u2021 \n\n\n\nTable 4 :\n4MAP @ IOU [0.50:0.95] of the document detection models on PubLayNet dev set.Method \nText Title List Table Figure mAP \nF-RCNN (ResNet-101) [28] \n91.0 82.6 88.3 95.4 93.7 90.0 \nM-RCNN (ResNet-101) [28] 91.6 84.0 88.6 96.0 94.9 90.7 \nF-RCNN (ResNet-50) \n92.2 84.4 89.5 96.5 94.5 91.4 \nF-RCNN (UniDoc, ResNet-50) 93.9 88.5 93.7 97.3 96.4 93.9 \n\n\n\nXiaodong He, Alex Smola, and Eduard Hovy. Hierarchical attention networks for document classification. Zichao Yang, Diyi Yang, Chris Dyer, NAACL. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. Hierarchical attention networks for document classification. In NAACL, 2016.\n\nMulti-document summarization by sentence extraction. Jade Goldstein, O Vibhu, Jaime G Mittal, Mark Carbonell, Kantrowitz, NAACL-ANLP Workshop. Jade Goldstein, Vibhu O Mittal, Jaime G Carbonell, and Mark Kantrowitz. Multi-document summarization by sentence extraction. In NAACL-ANLP Workshop, 2000.\n\nTransformer-xl: Attentive language models beyond a fixed-length context. Zihang Dai, Zhilin Yang, Yiming Yang, W William, Jaime Cohen, Carbonell, V Quoc, Ruslan Le, Salakhutdinov, ACL. Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In ACL, 2019.\n\nLayoutlm: Pre-training of text and layout for document image understanding. Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, SIGKDD. Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In SIGKDD, 2020.\n\nLayoutlmv2: Multi-modal pre-training for visuallyrich document understanding. Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, ACL. 2021Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, et al. Layoutlmv2: Multi-modal pre-training for visually- rich document understanding. In ACL, 2021.\n\nSelfdoc: Self-supervised document representation learning. Peizhao Li, Jiuxiang Gu, Jason Kuen, Vlad Morariu, Handong Zhao, Rajiv Jain, Varun Manjunatha, Hongfu Liu, CVPR. 2021Peizhao Li, Jiuxiang Gu, Jason Kuen, Vlad Morariu, Handong Zhao, Rajiv Jain, Varun Manju- natha, and Hongfu Liu. Selfdoc: Self-supervised document representation learning. In CVPR, 2021.\n\nCord: A consolidated receipt dataset for post-ocr parsing. Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo, Hwalsuk Lee, NeurIPS Workshop. Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo, and Hwalsuk Lee. Cord: A consolidated receipt dataset for post-ocr parsing. In NeurIPS Workshop, 2019.\n\nEvaluation of deep convolutional nets for document image classification and retrieval. W Adam, Alex Harley, Konstantinos G Ufkes, Derpanis, ICDAR. Adam W Harley, Alex Ufkes, and Konstantinos G Derpanis. Evaluation of deep convolutional nets for document image classification and retrieval. In ICDAR, 2015.\n\nRecent advances in convolutional neural networks. Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai, Ting Liu, Xingxing Wang, Gang Wang, Jianfei Cai, Pattern Recognition. Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai, Ting Liu, Xingxing Wang, Gang Wang, Jianfei Cai, et al. Recent advances in convolutional neural networks. Pattern Recognition, 2018.\n\nVilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, NeurIPS. Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In NeurIPS, 2019.\n\nSelf-supervised relationship probing. Jiuxiang Gu, Jason Kuen, Shafiq Joty, Jianfei Cai, Vlad Morariu, Handong Zhao, Tong Sun, NeurIPS. Jiuxiang Gu, Jason Kuen, Shafiq Joty, Jianfei Cai, Vlad Morariu, Handong Zhao, and Tong Sun. Self-supervised relationship probing. In NeurIPS, 2020.\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, CVPR. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.\n\nVl-bert: Pre-training of generic visual-linguistic representations. Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai, ICLR. Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-linguistic representations. In ICLR, 2019.\n\nUnicoder-vl: A universal encoder for vision and language by cross-modal pre-training. Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, Ming Zhou, AAAI. Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and Ming Zhou. Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training. In AAAI, 2020.\n\nRead: Recursive autoencoders for document layout generation. Akshay Gadi Patil, Omri Ben-Eliezer, Or Perel, Hadar Averbuch-Elor, CVPR Workshop. Akshay Gadi Patil, Omri Ben-Eliezer, Or Perel, and Hadar Averbuch-Elor. Read: Recursive autoencoders for document layout generation. In CVPR Workshop, 2020.\n\nFast r-cnn. Ross Girshick, CVPR. Ross Girshick. Fast r-cnn. In CVPR, 2015.\n\n. Easyocr, Easyocr. https://github.com/JaidedAI/EasyOCR, 2020.\n\nMask r-cnn. Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick, ICCV. Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In ICCV, 2017.\n\nProduct quantization for nearest neighbor search. Herve Jegou, Matthijs Douze, Cordelia Schmid, PAMIHerve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. PAMI, 2010.\n\nNeural discrete representation learning. Aaron Van Den Oord, Oriol Vinyals, NeurIPS. Aaron van den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017.\n\nAbdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. Alexei Baevski, Henry Zhou, NeurIPS. Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. In NeurIPS, 2020.\n\nCategorical reparameterization with gumbel-softmax. Eric Jang, Shixiang Gu, Ben Poole, ICLR. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In ICLR, 2017.\n\n. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, arXiv:1607.06450Layer normalization. arXiv preprintJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n\nSqueeze-and-excitation networks. Jie Hu, Li Shen, Gang Sun, CVPR. Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018.\n\nSimilarity-preserving knowledge distillation. Frederick Tung, Greg Mori, ICCV. Frederick Tung and Greg Mori. Similarity-preserving knowledge distillation. In ICCV, 2019.\n\nBuilding a test collection for complex document information processing. D Lewis, G Agam, S Argamon, O Frieder, D Grossman, J Heard, SIGIR. D. Lewis, G. Agam, S. Argamon, O. Frieder, D. Grossman, and J. Heard. Building a test collection for complex document information processing. In SIGIR, 2006.\n\nFunsd: A dataset for form understanding in noisy scanned documents. G Jaume, H Ekenel, J Thiran, ICDAR Workshop. G. Jaume, H. Kemal Ekenel, and J. Thiran. Funsd: A dataset for form understanding in noisy scanned documents. In ICDAR Workshop, 2019.\n\nPublaynet: largest dataset ever for document layout analysis. Xu Zhong, Jianbin Tang, Antonio Jimeno Yepes, ICDAR. Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. Publaynet: largest dataset ever for document layout analysis. In ICDAR, 2019.\n\nSentence-bert: Sentence embeddings using siamese bertnetworks. Nils Reimers, Iryna Gurevych, EMNLP. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert- networks. In EMNLP, 2019.\n\nA broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel R Bowman, NAACL. Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL, 2018.\n\nSemeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, Lucia Specia, Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. In ACL, 2017.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, ICLR. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2014.\n\n. Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, Ross Girshick, Detectron2, Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github.com/facebookresearch/detectron2, 2019.\n\nGoing full-tilt boogie on document understanding with text-image-layout transformer. Rafa\u0142 Powalski, \u0141ukasz Borchmann, Dawid Jurkiewicz, Tomasz Dwojak, Micha\u0142 Pietruszka, Gabriela Pa\u0142ka, ICDAR. 2021Rafa\u0142 Powalski, \u0141ukasz Borchmann, Dawid Jurkiewicz, Tomasz Dwojak, Micha\u0142 Pietruszka, and Gabriela Pa\u0142ka. Going full-tilt boogie on document understanding with text-image-layout transformer. In ICDAR, 2021.\n\nTesseract: An open-source optical character recognition engine. Anthony Kay, Linux J. 159Anthony Kay. Tesseract: An open-source optical character recognition engine. Linux J., 2007(159):2, July 2007.\n\nGoogle cloud vision ocr. 2021Google cloud vision ocr. https://cloud.google.com/vision/docs/ocr, 2021.\n", "annotations": {"author": "[{\"end\":112,\"start\":68},{\"end\":156,\"start\":113},{\"end\":207,\"start\":157},{\"end\":255,\"start\":208},{\"end\":318,\"start\":256},{\"end\":366,\"start\":319},{\"end\":414,\"start\":367},{\"end\":456,\"start\":415}]", "publisher": null, "author_last_name": "[{\"end\":79,\"start\":77},{\"end\":123,\"start\":119},{\"end\":171,\"start\":164},{\"end\":220,\"start\":216},{\"end\":275,\"start\":265},{\"end\":329,\"start\":325},{\"end\":378,\"start\":371},{\"end\":423,\"start\":420}]", "author_first_name": "[{\"end\":76,\"start\":68},{\"end\":118,\"start\":113},{\"end\":161,\"start\":157},{\"end\":163,\"start\":162},{\"end\":215,\"start\":208},{\"end\":264,\"start\":256},{\"end\":324,\"start\":319},{\"end\":370,\"start\":367},{\"end\":419,\"start\":415}]", "author_affiliation": "[{\"end\":111,\"start\":96},{\"end\":155,\"start\":140},{\"end\":206,\"start\":191},{\"end\":254,\"start\":239},{\"end\":317,\"start\":296},{\"end\":365,\"start\":350},{\"end\":413,\"start\":398},{\"end\":455,\"start\":440}]", "title": "[{\"end\":65,\"start\":1},{\"end\":521,\"start\":457}]", "venue": null, "abstract": "[{\"end\":2487,\"start\":523}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2689,\"start\":2686},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2691,\"start\":2689},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3510,\"start\":3507},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3621,\"start\":3618},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3623,\"start\":3621},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3625,\"start\":3623},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3742,\"start\":3739},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3744,\"start\":3742},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3824,\"start\":3821},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4555,\"start\":4552},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4557,\"start\":4555},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6110,\"start\":6107},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6565,\"start\":6561},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6568,\"start\":6565},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8236,\"start\":8233},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8297,\"start\":8293},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8309,\"start\":8306},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8408,\"start\":8404},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8411,\"start\":8408},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8414,\"start\":8411},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8417,\"start\":8414},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8435,\"start\":8432},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9488,\"start\":9485},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9490,\"start\":9488},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9493,\"start\":9490},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9507,\"start\":9504},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10010,\"start\":10007},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10464,\"start\":10461},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10608,\"start\":10604},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11100,\"start\":11097},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11255,\"start\":11252},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12544,\"start\":12540},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12814,\"start\":12810},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13134,\"start\":13130},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14266,\"start\":14263},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14733,\"start\":14729},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14736,\"start\":14733},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15710,\"start\":15706},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16216,\"start\":16212},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17144,\"start\":17140},{\"end\":18264,\"start\":18258},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18557,\"start\":18553},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":19861,\"start\":19857},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19923,\"start\":19920},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20636,\"start\":20632},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20787,\"start\":20784},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21196,\"start\":21192},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21218,\"start\":21215},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21237,\"start\":21234},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21259,\"start\":21255},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21284,\"start\":21280},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22581,\"start\":22578},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22583,\"start\":22581},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23052,\"start\":23048},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23076,\"start\":23072},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23091,\"start\":23087},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23185,\"start\":23181},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":23916,\"start\":23912},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24877,\"start\":24873},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25770,\"start\":25767},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26302,\"start\":26299},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":27104,\"start\":27100},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27353,\"start\":27349},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27969,\"start\":27966},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31423,\"start\":31420},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31864,\"start\":31861},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31883,\"start\":31880},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":31894,\"start\":31890},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":32487,\"start\":32483},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":32512,\"start\":32508},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":32540,\"start\":32536}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":37092,\"start\":36905},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38083,\"start\":37093},{\"attributes\":{\"id\":\"fig_2\"},\"end\":38170,\"start\":38084},{\"attributes\":{\"id\":\"fig_3\"},\"end\":38355,\"start\":38171},{\"attributes\":{\"id\":\"fig_4\"},\"end\":38431,\"start\":38356},{\"attributes\":{\"id\":\"fig_5\"},\"end\":38678,\"start\":38432},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":39273,\"start\":38679},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":39877,\"start\":39274},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":41030,\"start\":39878},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":41384,\"start\":41031}]", "paragraph": "[{\"end\":3745,\"start\":2503},{\"end\":4984,\"start\":3747},{\"end\":6507,\"start\":4986},{\"end\":7359,\"start\":6509},{\"end\":8084,\"start\":7361},{\"end\":9417,\"start\":8101},{\"end\":10845,\"start\":9419},{\"end\":11809,\"start\":10847},{\"end\":12464,\"start\":11841},{\"end\":13474,\"start\":12466},{\"end\":13924,\"start\":13476},{\"end\":14798,\"start\":14043},{\"end\":15220,\"start\":14945},{\"end\":15711,\"start\":15283},{\"end\":16057,\"start\":15713},{\"end\":16354,\"start\":16173},{\"end\":16533,\"start\":16356},{\"end\":16990,\"start\":16787},{\"end\":17709,\"start\":16992},{\"end\":17924,\"start\":17743},{\"end\":18016,\"start\":17926},{\"end\":18558,\"start\":18018},{\"end\":18806,\"start\":18618},{\"end\":19330,\"start\":18808},{\"end\":19650,\"start\":19450},{\"end\":20248,\"start\":19652},{\"end\":20506,\"start\":20322},{\"end\":21142,\"start\":20542},{\"end\":22956,\"start\":21154},{\"end\":24036,\"start\":22958},{\"end\":24734,\"start\":24038},{\"end\":25596,\"start\":24755},{\"end\":26908,\"start\":25598},{\"end\":27656,\"start\":26910},{\"end\":28065,\"start\":27683},{\"end\":28461,\"start\":28067},{\"end\":29495,\"start\":28463},{\"end\":34090,\"start\":29497},{\"end\":35299,\"start\":34136},{\"end\":35995,\"start\":35301},{\"end\":36904,\"start\":35997}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14042,\"start\":13925},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14851,\"start\":14799},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14944,\"start\":14851},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15282,\"start\":15221},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16172,\"start\":16058},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16786,\"start\":16534},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18617,\"start\":18559},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19449,\"start\":19331},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20321,\"start\":20249}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14704,\"start\":14697},{\"end\":20900,\"start\":20893},{\"end\":21305,\"start\":21298},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28213,\"start\":28206},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28473,\"start\":28466},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29699,\"start\":29692},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30071,\"start\":30064},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30396,\"start\":30389},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31434,\"start\":31427},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32994,\"start\":32987},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":33440,\"start\":33433}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2501,\"start\":2489},{\"attributes\":{\"n\":\"2\"},\"end\":8099,\"start\":8087},{\"attributes\":{\"n\":\"3\"},\"end\":11818,\"start\":11812},{\"attributes\":{\"n\":\"3.1\"},\"end\":11839,\"start\":11821},{\"attributes\":{\"n\":\"3.2\"},\"end\":17741,\"start\":17712},{\"attributes\":{\"n\":\"4\"},\"end\":20519,\"start\":20509},{\"attributes\":{\"n\":\"4.1\"},\"end\":20540,\"start\":20522},{\"end\":21152,\"start\":21145},{\"attributes\":{\"n\":\"4.2\"},\"end\":24753,\"start\":24737},{\"attributes\":{\"n\":\"4.3\"},\"end\":27681,\"start\":27659},{\"attributes\":{\"n\":\"5\"},\"end\":34134,\"start\":34093},{\"end\":36916,\"start\":36906},{\"end\":38095,\"start\":38085},{\"end\":38182,\"start\":38172},{\"end\":38362,\"start\":38357},{\"end\":38443,\"start\":38433},{\"end\":39284,\"start\":39275},{\"end\":39888,\"start\":39879},{\"end\":41041,\"start\":41032}]", "table": "[{\"end\":39273,\"start\":38781},{\"end\":39877,\"start\":39361},{\"end\":41030,\"start\":40110},{\"end\":41384,\"start\":41119}]", "figure_caption": "[{\"end\":37092,\"start\":36918},{\"end\":38083,\"start\":37095},{\"end\":38170,\"start\":38097},{\"end\":38355,\"start\":38184},{\"end\":38431,\"start\":38364},{\"end\":38678,\"start\":38445},{\"end\":38781,\"start\":38681},{\"end\":39361,\"start\":39286},{\"end\":40110,\"start\":39890},{\"end\":41119,\"start\":41043}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5072,\"start\":5066},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17806,\"start\":17800},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21492,\"start\":21482},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22591,\"start\":22585},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24959,\"start\":24949},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27125,\"start\":27115},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28870,\"start\":28860},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":32326,\"start\":32320}]", "bib_author_first_name": "[{\"end\":41495,\"start\":41489},{\"end\":41506,\"start\":41502},{\"end\":41518,\"start\":41513},{\"end\":41745,\"start\":41741},{\"end\":41758,\"start\":41757},{\"end\":41771,\"start\":41766},{\"end\":41773,\"start\":41772},{\"end\":41786,\"start\":41782},{\"end\":42066,\"start\":42060},{\"end\":42078,\"start\":42072},{\"end\":42091,\"start\":42085},{\"end\":42099,\"start\":42098},{\"end\":42114,\"start\":42109},{\"end\":42134,\"start\":42133},{\"end\":42147,\"start\":42141},{\"end\":42451,\"start\":42445},{\"end\":42463,\"start\":42456},{\"end\":42471,\"start\":42468},{\"end\":42484,\"start\":42477},{\"end\":42496,\"start\":42492},{\"end\":42506,\"start\":42502},{\"end\":42769,\"start\":42765},{\"end\":42780,\"start\":42774},{\"end\":42793,\"start\":42785},{\"end\":42801,\"start\":42798},{\"end\":42811,\"start\":42807},{\"end\":42823,\"start\":42817},{\"end\":42836,\"start\":42830},{\"end\":42846,\"start\":42841},{\"end\":42861,\"start\":42858},{\"end\":42877,\"start\":42869},{\"end\":43178,\"start\":43171},{\"end\":43191,\"start\":43183},{\"end\":43201,\"start\":43196},{\"end\":43212,\"start\":43208},{\"end\":43229,\"start\":43222},{\"end\":43241,\"start\":43236},{\"end\":43253,\"start\":43248},{\"end\":43272,\"start\":43266},{\"end\":43544,\"start\":43535},{\"end\":43556,\"start\":43551},{\"end\":43567,\"start\":43563},{\"end\":43580,\"start\":43573},{\"end\":43594,\"start\":43586},{\"end\":43608,\"start\":43601},{\"end\":43621,\"start\":43614},{\"end\":43916,\"start\":43915},{\"end\":43927,\"start\":43923},{\"end\":43950,\"start\":43936},{\"end\":44193,\"start\":44185},{\"end\":44205,\"start\":44198},{\"end\":44217,\"start\":44212},{\"end\":44232,\"start\":44224},{\"end\":44241,\"start\":44237},{\"end\":44257,\"start\":44253},{\"end\":44269,\"start\":44265},{\"end\":44283,\"start\":44275},{\"end\":44294,\"start\":44290},{\"end\":44308,\"start\":44301},{\"end\":44653,\"start\":44647},{\"end\":44663,\"start\":44658},{\"end\":44675,\"start\":44671},{\"end\":44690,\"start\":44684},{\"end\":44921,\"start\":44913},{\"end\":44931,\"start\":44926},{\"end\":44944,\"start\":44938},{\"end\":44958,\"start\":44951},{\"end\":44968,\"start\":44964},{\"end\":44985,\"start\":44978},{\"end\":44996,\"start\":44992},{\"end\":45217,\"start\":45214},{\"end\":45227,\"start\":45224},{\"end\":45241,\"start\":45234},{\"end\":45256,\"start\":45250},{\"end\":45264,\"start\":45261},{\"end\":45271,\"start\":45269},{\"end\":45501,\"start\":45495},{\"end\":45512,\"start\":45506},{\"end\":45521,\"start\":45518},{\"end\":45530,\"start\":45527},{\"end\":45540,\"start\":45535},{\"end\":45549,\"start\":45545},{\"end\":45561,\"start\":45555},{\"end\":45822,\"start\":45819},{\"end\":45830,\"start\":45827},{\"end\":45844,\"start\":45837},{\"end\":45856,\"start\":45851},{\"end\":45868,\"start\":45864},{\"end\":46110,\"start\":46104},{\"end\":46127,\"start\":46123},{\"end\":46143,\"start\":46141},{\"end\":46156,\"start\":46151},{\"end\":46361,\"start\":46357},{\"end\":46504,\"start\":46497},{\"end\":46516,\"start\":46509},{\"end\":46532,\"start\":46527},{\"end\":46545,\"start\":46541},{\"end\":46708,\"start\":46703},{\"end\":46724,\"start\":46716},{\"end\":46740,\"start\":46732},{\"end\":46912,\"start\":46907},{\"end\":46932,\"start\":46927},{\"end\":47179,\"start\":47173},{\"end\":47194,\"start\":47189},{\"end\":47433,\"start\":47429},{\"end\":47448,\"start\":47440},{\"end\":47456,\"start\":47453},{\"end\":47584,\"start\":47579},{\"end\":47588,\"start\":47585},{\"end\":47598,\"start\":47593},{\"end\":47603,\"start\":47599},{\"end\":47619,\"start\":47611},{\"end\":47621,\"start\":47620},{\"end\":47833,\"start\":47830},{\"end\":47840,\"start\":47838},{\"end\":47851,\"start\":47847},{\"end\":47998,\"start\":47989},{\"end\":48009,\"start\":48005},{\"end\":48187,\"start\":48186},{\"end\":48196,\"start\":48195},{\"end\":48204,\"start\":48203},{\"end\":48215,\"start\":48214},{\"end\":48226,\"start\":48225},{\"end\":48238,\"start\":48237},{\"end\":48481,\"start\":48480},{\"end\":48490,\"start\":48489},{\"end\":48500,\"start\":48499},{\"end\":48725,\"start\":48723},{\"end\":48740,\"start\":48733},{\"end\":48761,\"start\":48747},{\"end\":48972,\"start\":48968},{\"end\":48987,\"start\":48982},{\"end\":49205,\"start\":49200},{\"end\":49222,\"start\":49216},{\"end\":49239,\"start\":49231},{\"end\":49510,\"start\":49504},{\"end\":49520,\"start\":49516},{\"end\":49532,\"start\":49527},{\"end\":49546,\"start\":49541},{\"end\":49566,\"start\":49561},{\"end\":49810,\"start\":49809},{\"end\":49826,\"start\":49821},{\"end\":49944,\"start\":49939},{\"end\":49958,\"start\":49949},{\"end\":49978,\"start\":49969},{\"end\":49993,\"start\":49986},{\"end\":50002,\"start\":49998},{\"end\":50260,\"start\":50255},{\"end\":50277,\"start\":50271},{\"end\":50294,\"start\":50289},{\"end\":50313,\"start\":50307},{\"end\":50328,\"start\":50322},{\"end\":50349,\"start\":50341},{\"end\":50647,\"start\":50640}]", "bib_author_last_name": "[{\"end\":41500,\"start\":41496},{\"end\":41511,\"start\":41507},{\"end\":41523,\"start\":41519},{\"end\":41755,\"start\":41746},{\"end\":41764,\"start\":41759},{\"end\":41780,\"start\":41774},{\"end\":41796,\"start\":41787},{\"end\":41808,\"start\":41798},{\"end\":42070,\"start\":42067},{\"end\":42083,\"start\":42079},{\"end\":42096,\"start\":42092},{\"end\":42107,\"start\":42100},{\"end\":42120,\"start\":42115},{\"end\":42131,\"start\":42122},{\"end\":42139,\"start\":42135},{\"end\":42150,\"start\":42148},{\"end\":42165,\"start\":42152},{\"end\":42454,\"start\":42452},{\"end\":42466,\"start\":42464},{\"end\":42475,\"start\":42472},{\"end\":42490,\"start\":42485},{\"end\":42500,\"start\":42497},{\"end\":42511,\"start\":42507},{\"end\":42772,\"start\":42770},{\"end\":42783,\"start\":42781},{\"end\":42796,\"start\":42794},{\"end\":42805,\"start\":42802},{\"end\":42815,\"start\":42812},{\"end\":42828,\"start\":42824},{\"end\":42839,\"start\":42837},{\"end\":42856,\"start\":42847},{\"end\":42867,\"start\":42862},{\"end\":42881,\"start\":42878},{\"end\":43181,\"start\":43179},{\"end\":43194,\"start\":43192},{\"end\":43206,\"start\":43202},{\"end\":43220,\"start\":43213},{\"end\":43234,\"start\":43230},{\"end\":43246,\"start\":43242},{\"end\":43264,\"start\":43254},{\"end\":43276,\"start\":43273},{\"end\":43549,\"start\":43545},{\"end\":43561,\"start\":43557},{\"end\":43571,\"start\":43568},{\"end\":43584,\"start\":43581},{\"end\":43599,\"start\":43595},{\"end\":43612,\"start\":43609},{\"end\":43625,\"start\":43622},{\"end\":43921,\"start\":43917},{\"end\":43934,\"start\":43928},{\"end\":43956,\"start\":43951},{\"end\":43966,\"start\":43958},{\"end\":44196,\"start\":44194},{\"end\":44210,\"start\":44206},{\"end\":44222,\"start\":44218},{\"end\":44235,\"start\":44233},{\"end\":44251,\"start\":44242},{\"end\":44263,\"start\":44258},{\"end\":44273,\"start\":44270},{\"end\":44288,\"start\":44284},{\"end\":44299,\"start\":44295},{\"end\":44312,\"start\":44309},{\"end\":44656,\"start\":44654},{\"end\":44669,\"start\":44664},{\"end\":44682,\"start\":44676},{\"end\":44694,\"start\":44691},{\"end\":44924,\"start\":44922},{\"end\":44936,\"start\":44932},{\"end\":44949,\"start\":44945},{\"end\":44962,\"start\":44959},{\"end\":44976,\"start\":44969},{\"end\":44990,\"start\":44986},{\"end\":45000,\"start\":44997},{\"end\":45222,\"start\":45218},{\"end\":45232,\"start\":45228},{\"end\":45248,\"start\":45242},{\"end\":45259,\"start\":45257},{\"end\":45267,\"start\":45265},{\"end\":45279,\"start\":45272},{\"end\":45504,\"start\":45502},{\"end\":45516,\"start\":45513},{\"end\":45525,\"start\":45522},{\"end\":45533,\"start\":45531},{\"end\":45543,\"start\":45541},{\"end\":45553,\"start\":45550},{\"end\":45565,\"start\":45562},{\"end\":45825,\"start\":45823},{\"end\":45835,\"start\":45831},{\"end\":45849,\"start\":45845},{\"end\":45862,\"start\":45857},{\"end\":45873,\"start\":45869},{\"end\":46121,\"start\":46111},{\"end\":46139,\"start\":46128},{\"end\":46149,\"start\":46144},{\"end\":46170,\"start\":46157},{\"end\":46370,\"start\":46362},{\"end\":46430,\"start\":46423},{\"end\":46507,\"start\":46505},{\"end\":46525,\"start\":46517},{\"end\":46539,\"start\":46533},{\"end\":46554,\"start\":46546},{\"end\":46714,\"start\":46709},{\"end\":46730,\"start\":46725},{\"end\":46747,\"start\":46741},{\"end\":46925,\"start\":46913},{\"end\":46940,\"start\":46933},{\"end\":47187,\"start\":47180},{\"end\":47199,\"start\":47195},{\"end\":47438,\"start\":47434},{\"end\":47451,\"start\":47449},{\"end\":47462,\"start\":47457},{\"end\":47591,\"start\":47589},{\"end\":47609,\"start\":47604},{\"end\":47628,\"start\":47622},{\"end\":47836,\"start\":47834},{\"end\":47845,\"start\":47841},{\"end\":47855,\"start\":47852},{\"end\":48003,\"start\":47999},{\"end\":48014,\"start\":48010},{\"end\":48193,\"start\":48188},{\"end\":48201,\"start\":48197},{\"end\":48212,\"start\":48205},{\"end\":48223,\"start\":48216},{\"end\":48235,\"start\":48227},{\"end\":48244,\"start\":48239},{\"end\":48487,\"start\":48482},{\"end\":48497,\"start\":48491},{\"end\":48507,\"start\":48501},{\"end\":48731,\"start\":48726},{\"end\":48745,\"start\":48741},{\"end\":48767,\"start\":48762},{\"end\":48980,\"start\":48973},{\"end\":48996,\"start\":48988},{\"end\":49214,\"start\":49206},{\"end\":49229,\"start\":49223},{\"end\":49246,\"start\":49240},{\"end\":49514,\"start\":49511},{\"end\":49525,\"start\":49521},{\"end\":49539,\"start\":49533},{\"end\":49559,\"start\":49547},{\"end\":49573,\"start\":49567},{\"end\":49819,\"start\":49811},{\"end\":49833,\"start\":49827},{\"end\":49837,\"start\":49835},{\"end\":49947,\"start\":49945},{\"end\":49967,\"start\":49959},{\"end\":49984,\"start\":49979},{\"end\":49996,\"start\":49994},{\"end\":50011,\"start\":50003},{\"end\":50023,\"start\":50013},{\"end\":50269,\"start\":50261},{\"end\":50287,\"start\":50278},{\"end\":50305,\"start\":50295},{\"end\":50320,\"start\":50314},{\"end\":50339,\"start\":50329},{\"end\":50355,\"start\":50350},{\"end\":50651,\"start\":50648}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6857205},\"end\":41686,\"start\":41386},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":8294822},\"end\":41985,\"start\":41688},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":57759363},\"end\":42367,\"start\":41987},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":209515395},\"end\":42685,\"start\":42369},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":229923949},\"end\":43110,\"start\":42687},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":235358528},\"end\":43474,\"start\":43112},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":207900784},\"end\":43826,\"start\":43476},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":2760893},\"end\":44133,\"start\":43828},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3879949},\"end\":44547,\"start\":44135},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":199453025},\"end\":44873,\"start\":44549},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":227276155},\"end\":45159,\"start\":44875},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":57246310},\"end\":45425,\"start\":45161},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":201317624},\"end\":45731,\"start\":45427},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":201058752},\"end\":46041,\"start\":45733},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":202540277},\"end\":46343,\"start\":46043},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206770307},\"end\":46419,\"start\":46345},{\"attributes\":{\"id\":\"b16\"},\"end\":46483,\"start\":46421},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":54465873},\"end\":46651,\"start\":46485},{\"attributes\":{\"id\":\"b18\"},\"end\":46864,\"start\":46653},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":20282961},\"end\":47051,\"start\":46866},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":219966759},\"end\":47375,\"start\":47053},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2428314},\"end\":47575,\"start\":47377},{\"attributes\":{\"doi\":\"arXiv:1607.06450\",\"id\":\"b22\"},\"end\":47795,\"start\":47577},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":140309863},\"end\":47941,\"start\":47797},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":198179476},\"end\":48112,\"start\":47943},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":19516087},\"end\":48410,\"start\":48114},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":173188931},\"end\":48659,\"start\":48412},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":201124789},\"end\":48903,\"start\":48661},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":201646309},\"end\":49118,\"start\":48905},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3432876},\"end\":49402,\"start\":49120},{\"attributes\":{\"id\":\"b30\"},\"end\":49763,\"start\":49404},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":6628106},\"end\":49935,\"start\":49765},{\"attributes\":{\"id\":\"b32\"},\"end\":50168,\"start\":49937},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":231951453},\"end\":50574,\"start\":50170},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":59749777},\"end\":50775,\"start\":50576},{\"attributes\":{\"id\":\"b35\"},\"end\":50878,\"start\":50777}]", "bib_title": "[{\"end\":41487,\"start\":41386},{\"end\":41739,\"start\":41688},{\"end\":42058,\"start\":41987},{\"end\":42443,\"start\":42369},{\"end\":42763,\"start\":42687},{\"end\":43169,\"start\":43112},{\"end\":43533,\"start\":43476},{\"end\":43913,\"start\":43828},{\"end\":44183,\"start\":44135},{\"end\":44645,\"start\":44549},{\"end\":44911,\"start\":44875},{\"end\":45212,\"start\":45161},{\"end\":45493,\"start\":45427},{\"end\":45817,\"start\":45733},{\"end\":46102,\"start\":46043},{\"end\":46355,\"start\":46345},{\"end\":46495,\"start\":46485},{\"end\":46905,\"start\":46866},{\"end\":47171,\"start\":47053},{\"end\":47427,\"start\":47377},{\"end\":47828,\"start\":47797},{\"end\":47987,\"start\":47943},{\"end\":48184,\"start\":48114},{\"end\":48478,\"start\":48412},{\"end\":48721,\"start\":48661},{\"end\":48966,\"start\":48905},{\"end\":49198,\"start\":49120},{\"end\":49807,\"start\":49765},{\"end\":50253,\"start\":50170},{\"end\":50638,\"start\":50576}]", "bib_author": "[{\"end\":41502,\"start\":41489},{\"end\":41513,\"start\":41502},{\"end\":41525,\"start\":41513},{\"end\":41757,\"start\":41741},{\"end\":41766,\"start\":41757},{\"end\":41782,\"start\":41766},{\"end\":41798,\"start\":41782},{\"end\":41810,\"start\":41798},{\"end\":42072,\"start\":42060},{\"end\":42085,\"start\":42072},{\"end\":42098,\"start\":42085},{\"end\":42109,\"start\":42098},{\"end\":42122,\"start\":42109},{\"end\":42133,\"start\":42122},{\"end\":42141,\"start\":42133},{\"end\":42152,\"start\":42141},{\"end\":42167,\"start\":42152},{\"end\":42456,\"start\":42445},{\"end\":42468,\"start\":42456},{\"end\":42477,\"start\":42468},{\"end\":42492,\"start\":42477},{\"end\":42502,\"start\":42492},{\"end\":42513,\"start\":42502},{\"end\":42774,\"start\":42765},{\"end\":42785,\"start\":42774},{\"end\":42798,\"start\":42785},{\"end\":42807,\"start\":42798},{\"end\":42817,\"start\":42807},{\"end\":42830,\"start\":42817},{\"end\":42841,\"start\":42830},{\"end\":42858,\"start\":42841},{\"end\":42869,\"start\":42858},{\"end\":42883,\"start\":42869},{\"end\":43183,\"start\":43171},{\"end\":43196,\"start\":43183},{\"end\":43208,\"start\":43196},{\"end\":43222,\"start\":43208},{\"end\":43236,\"start\":43222},{\"end\":43248,\"start\":43236},{\"end\":43266,\"start\":43248},{\"end\":43278,\"start\":43266},{\"end\":43551,\"start\":43535},{\"end\":43563,\"start\":43551},{\"end\":43573,\"start\":43563},{\"end\":43586,\"start\":43573},{\"end\":43601,\"start\":43586},{\"end\":43614,\"start\":43601},{\"end\":43627,\"start\":43614},{\"end\":43923,\"start\":43915},{\"end\":43936,\"start\":43923},{\"end\":43958,\"start\":43936},{\"end\":43968,\"start\":43958},{\"end\":44198,\"start\":44185},{\"end\":44212,\"start\":44198},{\"end\":44224,\"start\":44212},{\"end\":44237,\"start\":44224},{\"end\":44253,\"start\":44237},{\"end\":44265,\"start\":44253},{\"end\":44275,\"start\":44265},{\"end\":44290,\"start\":44275},{\"end\":44301,\"start\":44290},{\"end\":44314,\"start\":44301},{\"end\":44658,\"start\":44647},{\"end\":44671,\"start\":44658},{\"end\":44684,\"start\":44671},{\"end\":44696,\"start\":44684},{\"end\":44926,\"start\":44913},{\"end\":44938,\"start\":44926},{\"end\":44951,\"start\":44938},{\"end\":44964,\"start\":44951},{\"end\":44978,\"start\":44964},{\"end\":44992,\"start\":44978},{\"end\":45002,\"start\":44992},{\"end\":45224,\"start\":45214},{\"end\":45234,\"start\":45224},{\"end\":45250,\"start\":45234},{\"end\":45261,\"start\":45250},{\"end\":45269,\"start\":45261},{\"end\":45281,\"start\":45269},{\"end\":45506,\"start\":45495},{\"end\":45518,\"start\":45506},{\"end\":45527,\"start\":45518},{\"end\":45535,\"start\":45527},{\"end\":45545,\"start\":45535},{\"end\":45555,\"start\":45545},{\"end\":45567,\"start\":45555},{\"end\":45827,\"start\":45819},{\"end\":45837,\"start\":45827},{\"end\":45851,\"start\":45837},{\"end\":45864,\"start\":45851},{\"end\":45875,\"start\":45864},{\"end\":46123,\"start\":46104},{\"end\":46141,\"start\":46123},{\"end\":46151,\"start\":46141},{\"end\":46172,\"start\":46151},{\"end\":46372,\"start\":46357},{\"end\":46432,\"start\":46423},{\"end\":46509,\"start\":46497},{\"end\":46527,\"start\":46509},{\"end\":46541,\"start\":46527},{\"end\":46556,\"start\":46541},{\"end\":46716,\"start\":46703},{\"end\":46732,\"start\":46716},{\"end\":46749,\"start\":46732},{\"end\":46927,\"start\":46907},{\"end\":46942,\"start\":46927},{\"end\":47189,\"start\":47173},{\"end\":47201,\"start\":47189},{\"end\":47440,\"start\":47429},{\"end\":47453,\"start\":47440},{\"end\":47464,\"start\":47453},{\"end\":47593,\"start\":47579},{\"end\":47611,\"start\":47593},{\"end\":47630,\"start\":47611},{\"end\":47838,\"start\":47830},{\"end\":47847,\"start\":47838},{\"end\":47857,\"start\":47847},{\"end\":48005,\"start\":47989},{\"end\":48016,\"start\":48005},{\"end\":48195,\"start\":48186},{\"end\":48203,\"start\":48195},{\"end\":48214,\"start\":48203},{\"end\":48225,\"start\":48214},{\"end\":48237,\"start\":48225},{\"end\":48246,\"start\":48237},{\"end\":48489,\"start\":48480},{\"end\":48499,\"start\":48489},{\"end\":48509,\"start\":48499},{\"end\":48733,\"start\":48723},{\"end\":48747,\"start\":48733},{\"end\":48769,\"start\":48747},{\"end\":48982,\"start\":48968},{\"end\":48998,\"start\":48982},{\"end\":49216,\"start\":49200},{\"end\":49231,\"start\":49216},{\"end\":49248,\"start\":49231},{\"end\":49516,\"start\":49504},{\"end\":49527,\"start\":49516},{\"end\":49541,\"start\":49527},{\"end\":49561,\"start\":49541},{\"end\":49575,\"start\":49561},{\"end\":49821,\"start\":49809},{\"end\":49835,\"start\":49821},{\"end\":49839,\"start\":49835},{\"end\":49949,\"start\":49939},{\"end\":49969,\"start\":49949},{\"end\":49986,\"start\":49969},{\"end\":49998,\"start\":49986},{\"end\":50013,\"start\":49998},{\"end\":50025,\"start\":50013},{\"end\":50271,\"start\":50255},{\"end\":50289,\"start\":50271},{\"end\":50307,\"start\":50289},{\"end\":50322,\"start\":50307},{\"end\":50341,\"start\":50322},{\"end\":50357,\"start\":50341},{\"end\":50653,\"start\":50640}]", "bib_venue": "[{\"end\":41530,\"start\":41525},{\"end\":41829,\"start\":41810},{\"end\":42170,\"start\":42167},{\"end\":42519,\"start\":42513},{\"end\":42886,\"start\":42883},{\"end\":43282,\"start\":43278},{\"end\":43643,\"start\":43627},{\"end\":43973,\"start\":43968},{\"end\":44333,\"start\":44314},{\"end\":44703,\"start\":44696},{\"end\":45009,\"start\":45002},{\"end\":45285,\"start\":45281},{\"end\":45571,\"start\":45567},{\"end\":45879,\"start\":45875},{\"end\":46185,\"start\":46172},{\"end\":46376,\"start\":46372},{\"end\":46560,\"start\":46556},{\"end\":46701,\"start\":46653},{\"end\":46949,\"start\":46942},{\"end\":47208,\"start\":47201},{\"end\":47468,\"start\":47464},{\"end\":47861,\"start\":47857},{\"end\":48020,\"start\":48016},{\"end\":48251,\"start\":48246},{\"end\":48523,\"start\":48509},{\"end\":48774,\"start\":48769},{\"end\":49003,\"start\":48998},{\"end\":49253,\"start\":49248},{\"end\":49502,\"start\":49404},{\"end\":49843,\"start\":49839},{\"end\":50362,\"start\":50357},{\"end\":50660,\"start\":50653},{\"end\":50800,\"start\":50777}]"}}}, "year": 2023, "month": 12, "day": 17}