{"id": 249205135, "updated": "2022-12-11 17:18:53.024", "metadata": {"title": "BioHD: an efficient genome sequence search platform using HyperDimensional memorization", "authors": "[{\"first\":\"Zhuowen\",\"last\":\"Zou\",\"middle\":[]},{\"first\":\"Hanning\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Prathyush\",\"last\":\"Poduval\",\"middle\":[]},{\"first\":\"Yeseong\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Mahdi\",\"last\":\"Imani\",\"middle\":[]},{\"first\":\"Elaheh\",\"last\":\"Sadredini\",\"middle\":[]},{\"first\":\"Rosario\",\"last\":\"Cammarota\",\"middle\":[]},{\"first\":\"Mohsen\",\"last\":\"Imani\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 49th Annual International Symposium on Computer Architecture", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "In this paper, we propose BioHD, a novel genomic sequence searching platform based on Hyper-Dimensional Computing (HDC) for hardware-friendly computation. BioHD transforms inherent sequential processes of genome matching to highly-parallelizable computation tasks. We exploit HDC memorization to encode and represent the genome sequences using high-dimensional vectors. Then, it combines the genome sequences to generate an HDC reference library. During the sequence searching, BioHD performs exact or approximate similarity check of an encoded query with the HDC reference library. Our framework simplifies the required sequence matching operations while introducing a statistical model to control the alignment quality. To get actual advantage from BioHD inherent robustness and parallelism, we design a processing in-memory (PIM) architecture with massive parallelism and compatible with the existing crossbar memory. Our PIM architecture supports all essential BioHD operations natively in memory with minimal modification on the array. We evaluate BioHD accuracy and efficiency on a wide range of genomics data, including COVID-19 databases. Our results indicate that PIM provides 102.8\u00d7 and 116.1\u00d7 (9.3\u00d7 and 13.2\u00d7) speedup and energy efficiency compared to the state-of-the-art pattern matching algorithm running on GeForce RTX 3060 Ti GPU (state-of-the-art PIM accelerator).", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/isca/ZouCPKISCI22", "doi": "10.1145/3470496.3527422"}}, "content": {"source": {"pdf_hash": "6b8f75393b932d0f51ee0c0213f05b2fef3861af", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "360f6ab5e47741dad9f7b754f6703fe8a52ba447", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6b8f75393b932d0f51ee0c0213f05b2fef3861af.txt", "contents": "\nBioHD: An Efficient Genome Sequence Search Platform Using HyperDi-mensional Memorization\nJune 18-22, 2022\n\nZhuowen Zou \nUniversity of California Irvine\n\n\nHanning Chen \nUniversity of California Irvine\n\n\nPrathyush Poduval \nIndian Institute of Science\n\n\nYeseong Kim \nDaegu Gyeongbuk Institute of Science and Technology\n\n\nNortheastern University\n\n\nElaheh Sadredini \nUniversity of California Riverside\n\n\nRosario Cammarota \nIntel Labs\n\n\nMohsen Imani \nUniversity of California Irvine\n\n\nZhuowen Zou \nUniversity of California Irvine\n\n\nHanning Chen \nUniversity of California Irvine\n\n\nPrathyush Poduval \nIndian Institute of Science\n\n\nYeseong Kim \nDaegu Gyeongbuk Institute of Science and Technology\n\n\nNortheastern University\n\n\nElaheh Sadredini \nUniversity of California Riverside\n\n\nRosario Cammarota \nIntel Labs\n\n\nMohsen \nBioHD: An Efficient Genome Sequence Search Platform Using HyperDi-mensional Memorization\n\nThe 49th Annual International Symposium on Computer Architecture (ISCA '22)\nJune 18-22, 202210.1145/3470496.3527422ACM Reference Format: New York, NY, USA. ACM, New York, NY, USA, 14 pages. https://\nIn this paper, we propose BioHD, a novel genomic sequence searching platform based on Hyper-Dimensional Computing (HDC) for hardware-friendly computation. BioHD transforms inherent sequential processes of genome matching to highly-parallelizable computation tasks. We exploit HDC memorization to encode and represent the genome sequences using high-dimensional vectors. Then, it combines the genome sequences to generate an HDC reference library. During the sequence searching, BioHD performs exact or approximate similarity check of an encoded query with the HDC reference library. Our framework simplifies the required sequence matching operations while introducing a statistical model to control the alignment quality. To get actual advantage from BioHD inherent robustness and parallelism, we design a processing in-memory (PIM) architecture with massive parallelism and compatible with the existing crossbar memory. Our PIM architecture supports all essential BioHD operations natively in memory with minimal modification on the array. We evaluate BioHD accuracy and efficiency on a wide range of genomics data, including COVID-19 databases.Our results indicate that PIM provides 102.8\u00d7 and 116.1\u00d7 (9.3\u00d7 and 13.2\u00d7) speedup and energy efficiency compared to the state-of-theart pattern matching algorithm running on GeForce RTX 3060 Ti GPU (state-of-the-art PIM accelerator).\n\nINTRODUCTION\n\nIn December 2019, a series of pneumonia cases of unknown cause were documented. Deep sequencing of lower respiratory tract samples indicated a novel coronavirus [1][2][3] and the corresponding disease \"COVID-19\". Within the last few months, the global number Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ISCA '22, June 18-22, 2022 of cases and deaths exceeded 413 million and 5.8 million, respectively [4]. The number of available COVID-19 genome assemblies is growing rapidly which in turn creates a need for scalable methods to enable real-time analysis and dissemination [5][6][7].\n\nUnfortunately, the optimal solution for the sequence alignment or sequence matching scales poorly with the number of sequences. An underlying reason is that data movement costs between the processor and memory still hinder the higher efficiency of application performance, although new processor technology has evolved to serve computationally complex tasks more efficiently. Processing in-memory (PIM) is a promising solution to accelerate applications with a large amount of parallelism [8][9][10][11][12][13][14][15][16][17][18][19]. Several recent works have explored the advantage of PIM-based architectures to accelerate machine learning algorithms [20,21]. However, there are several main challenges in using existing PIM architectures to accelerate the sequence matching: (i) the existing PIM architectures are mostly a dot product engine and accelerate vector-matrix multiplication. In contrast, sequence matching involves pairwise distance computation and similarity search, which cannot be supported entirely by existing PIM architectures [22,23]. (ii) Most PIM architectures are analog-based [14,20,21]; thus, they perform computation after mapping data points into analog space. The data converter between analog and digital space takes the majority of chip area and energy [21]. (iii) The existing PIM architectures require separate storage and computing memory units, resulting in a large amount of internal data movements. This not only reduces the computation efficiency but also affects the design scalability. (iv) The emerging devices, i.e., Non-Volatile Memories (NVMs), have various reliability issues such as endurance, durability, and variability [24][25][26]. This, coupled with the high computation precision required for sequence matching algorithms, limits the applicability of the PIM accelerators.\n\nIn this paper, we present a novel strategy that effectively adopts the PIM architecture for the sequence matching problem. At heart, an sequence matching solution requires to perform multiple search as a fundamental procedure, i.e., checking the existence of a gene series in a database [27][28][29][30][31][32][33][34]. In this sense, the problem is equivalent to memorization in that we should memorize and recall genomic/proteomic sequences. We accelerate the sequence searching in hardware by redesigning the sequence searching based on a new computing paradigm, Hyper-Dimensional Computing (HDC) [35][36][37][38]. HDC is a human memory-inspired method to implement efficient memorization using high-dimensional vectors, called hypervectors. The HDC provides several features that make it well-suited to address the alignment problem: (i) it transforms inherent sequential processes of the sequence searching to highly-parallelizable computation tasks, where the operations can be natively supported by PIM, (ii) BioHD is memory-centric and highly-parallel, making PIM architecture an ideal platform for hardware acceleration, (iii) it provides strong robustness to noise -a key strength for emerging technologies.\n\nWe propose BioHD, a Hyper-Dimensional genome analysis platform. Instead of working with original sequences, BioHD maps the genome sequences into high-dimensional space and performs sequence matching with simple and parallel similarity searches. The main contributions of the paper are summarized as follows:\n\n\u2022 To the best of our knowledge, BioHD is the first end-to-end PIM platform for sequence searching based on Hyper-Dimensional computing. At algorithm-level, BioHD revisits the sequence searching with brain-like memorization that HDC natively supports. BioHD creates a library for reference sequences by memorizing the patterns in high-dimension. \u2022 BioHD simplifies the alignment by recalling the exact or approximate existence of a query sequence with the reference library. Instead of working on the original data, BioHD maps all data points into high-dimensional space, enabling the main sequence searching operations to process in a hardwarefriendly way. BioHD encoding preserves the similarity of sequences and simplifies the similarity metric to Hamming distance. We also proposed a statistical model that provides a theoretical control of the quality of the alignment. \u2022 We propose a novel PIM architecture that accelerates BioHD on the existing crossbar memory with essential alignment operations implemented in memory blocks. This includes supporting search-based distance computing as well as rowparallel arithmetic in memory. The enhanced crossbar array can be used in the pipeline to accelerate BioHD in a massively parallel way.\n\nWe evaluate BioHD efficiency on a wide range of practical problems, including the COVID-19 database. Our solution provides, on average, 124.1\u00d7 and 102.8\u00d7 and 116.1\u00d7 (9.3\u00d7 and 13.2\u00d7) speedup and energy efficiency improvement compared to the state-of-theart alignment algorithm running on GeForce RTX 3060 Ti GPU (state-of-the-art PIM accelerator [39]).\n\n\nBACKGROUND\n\nGenome Sequence search: The process of predicting the possible DNA/RNA sequence that a specific protein has originated from is called back-translation. Aligning the back-translated RNA sequence against the database locates the most similar sequences used to predict the functionality of the unknown protein sequence. Proteins are made up of one or more chains of 20 common amino acids. An unknown protein can be characterized when its sequence shares significant similarity with a protein with known characteristics [28][29][30][31][40][41][42][43]. Figure 1a describes the overview of BioHD computational task flow. From a computational standpoint, a protein sequence can be considered a string over a set that includes different amino acid letters, e.g., S = Met, Phe, \u00b7 \u00b7 \u00b7 . The protein sequence is originally translated from an mRNA, which can be considered a string over the four alphabets, {A,C,G,U}. Each nonoverlapping three-letter window of the mRNA, known as a Codon, encodes a specific letter in the amino acid alphabet according to the   (Codon table).\n\nCodon table (Figure 1b). The protein sequence is the result of replacing each codon of the mRNA sequence from start to end with its corresponding amino acid. The task of \"back-translation\" uses the Codon table to generate an mRNA sequence representing the most likely non-degenerate coding sequence. Since the back-translation in most cases does not yield a unique result, a consensus sequence derived from all possible Codons is needed. The sequence searching finds the regions with high similarity to the query sequence. The regions with high similarity, called hits, are used in many applications to predict the functionality of the unknown query. Hyper-dimensional computing (HDC): HDC effectively mimics several important functionalities of the human memory and allows energy-efficient computation based on its massively parallel computation flow [35,37,[44][45][46]. HDC is motivated by an observation that the human brain operates on a robust high-dimensional representation of data due to the large size of brain circuits [47][48][49]. HDC mimics the properties based on the idea that we can represent the information with a hypervector and the correlation with the hyperspace's distance. The dimensionality should be large enough to ensure two -components vectors that are randomly chosen from {\u22121, 1} or {0,1}, are near-orthogonal, referring that the similarity in the vector space is almost zero. As a result, HDC applications use hypervectors that have thousands dimensionality, e.g., = 10, 000.\n\nLet us assume \u00ec A and \u00ec B are two random generated hypervectors in high-dimensional space. HDC encoding works based on a well-defined set of operations: (i) Bundling (+) is an addition or majority operation of multiple hypervectors into a single reference hypervector ( \u00ec R = \u00ec A + \u00ec B). The result of the bundling preserves similarity to its component hypervectors i.e., ( \u00ec R, \u00ec A) >> 0, where is a cosine similarity. The bundling operation is well suited for representing sets. (ii) Binding (*) associates multiple orthogonal hypervectors (e.g., \u00ec A, \u00ec B) into a single hypervector ( \u00ec R = \u00ec A * \u00ec B). Binding is done by the component-wise multiplication (XOR in the binary representation) between two hypervectors. Binding is used for variable-value association and, more generally, for the mapping. given hypervector ( ( \u00ec A, \u00ec A) 0). It is commonly used for storing a sequence of tokens in a single hypervector. Figure 2 shows an overview of BioHD sequence search in the highdimensional space. The first step of BioHD is to map the genome sequence into a high-dimensional space. BioHD assigns a hypervector corresponding to each base alphabet in \u03a3 = { , , , } for DNA and \u03a3 = { , , , } for RNA. The encoding module depends on the data type and the genomics task. In terms of protein data, BioHD assigns a hypervector representing each RNA bases and then combines them to create a hypervector representing each amino acid (as explained in Section 2, and Figure 1). The amino acids' hypervectors are combined by mapping each protein sequence into a high-dimensional space (explained in Section 3.1). BioHD aggregates all encoded protein sequences to generate a reference genome, called HDC Library. An HDC library consists of several reference hypervectors, where each hypervector memorizes thousands of genome sequences in high-dimensional space. Similar to the human memorization that requires practice, BioHD iteratively checks the correctness of memorized information in each library hypervector to find the most refined hypervectors. During the sequence searching, BioHD uses the same encoding to map a query sequence into a hypervector. We perform a similarity computation between a query and each reference hypervector. By searching for an exact or approximate match, BioHD identifies a query's closeness with thousands of memorized patterns stored in each HDC library hypervector.\n\n\nBIOHD: GENOME SEQUENCE SEARCH\n\nWe design a novel processing in-memory (PIM) architecture that accelerates BioHD with minor modification on existing crossbar memory (Section 4). Our approach is general and can work with any bipolar non-volatile memory (NVM) devices. Each memory block in PIM architecture supports the essential alignment operations, including row-parallel distance computing and vector-based binding operation (Section 5). These PIM functionalities are implemented over digital data stored in memory using search-based and rowparallel arithmetic computation. For scalability, BioHD architecture enables row-parallel bit-serial data transfer between neighbor memory blocks. Figure 2b shows an example of PIM functionality. In the first step, BioHD encodes the coming genome sequence using the encoding block. Next, BioHD checks the similarity of the encoded query with all reference hypervectors stored in memory in a massively parallel way. In each block, we support the similarity measurement using row-parallel Hamming distance and dot-product distance metrics. Finally, the search result will be written to the comparison block, and this block will identify reference hypervectors that have been closely matched.\n\n\nBioHD Genome Sequence Encoding\n\nThe entire BioHD computation happens after mapping genomics data (DNA, RNA, or protein) into high-dimensional space. To encode a sequence to a hypervector, BioHD assigns a hypervector corresponding to each base alphabet in \u03a3 = {A, C, G, T} for DNA and \u03a3 = {A, C, G, U} for RNA. We call hypervectors as base hypervectors, and denote them with\n\u03a3 = { \u00ec A, \u00ec C, \u00ec G, \u00ec T} and \u03a3 = { \u00ec A, \u00ec C, \u00ec G, \u00ec U}.\nThese bases are randomly generated, thus they are nearly orthogonal. To be more precise, their dot products, an indicator of similarity, follow a certain distribution: \u00ec A \u00b7 \u00ec C \u223c 2 ( , 0.5) \u2212 , where ( , 0.5) denotes the binomial distribution with parameter = , the HDC dimension, and = 0.5. (Figure 3 \u2022 1 ).\n\nProtein Encoding: BioHD encodes each protein sequence into high-dimensional space by (i) encoding the mRNA sequences into high-dimensional space, (ii) combining multiple mRNA hypervectors to generate a hypervector for each amino acid (Figure 1b), and (iii) combining the amino acids sequences to create a hypervector for each protein sequence. (i) mRNA Encoding: BioHD encodes each DNA or mRNA sequence by binding its corresponding base hypervectors. Let us consider a short query string, 'ACGU'. BioHD encodes the sequence by binding the base hypervectors into high-dimensional space (Figure 3 \u2022 2 ).\n\u00ec H = \u00ec A * 1 ( \u00ec C) * 2 ( \u00ec G) * 3 ( \u00ec U),\nwhere represents -bit(s) rotational shift. The independence among the base hypervectors and their permuted forms ensures that the generated hypervectors for different RNA sequences are nearly orthogonal. Given \u00ec H 1 and \u00ec H 2 generated from different sequences, \u00ec H 1 \u00b7 \u00ec H 2 \u223c 2 ( , 0.5) \u2212 . (ii) Amino Acid Encoding: As the Codon table shows, each amino acid may correspond to multiple mRNA sequences. This makes the alignment problem more complicated as we need to consider all possibilities that a protein sequence may be back-translated to. The amino acid hypervector needs to memorize the information of multiple mRNA hypervectors. We explain amino acid encoding in an example. Based on Codon table, Phe amino acid corresponds to UUU and UUC sequences. Our encoding represents this amino acid as:\n\u00ec H \u210e = ( \u00ec U * 1 \u00ec U * 2 \u00ec U) \u2322 ( \u00ec U * 1 \u00ec U * 2 \u00ec C)\nwhere the term ' \u2322 ' indicates the probabilistic merging. This approach randomly samples half of the dimensions from the first and second hypervectors and generates a new hypervector with half  Figure 3: BioHD protein alignment steps.\n\nsimilarity to each generated bases. This encoding preserves the similarity while representing each amino acid as a single binary hypervector ( Figure 3 \u2022 2 ). This compact representation significantly reduces the noise and the complexity of the BioHD alignment.\n\n(iii) Protein Encoding: Protein is a sequence of amino acids. BioHD exploits the generated amino acid hypervectors to encode different protein sequences. For the sequence searching, the encoding remembers each amino acid and its corresponding position in a sequence. For example, protein sequence of S = Met, Phe, Ser, Gly, Stop can be decoded as:\n\u00ec S = \u00ec H * \u00ec H \u210e * 2 \u00ec H * 3 \u00ec H * 4 \u00ec H\n(iv) Insertion/Deletion: BioHD encoding can also count for a possible insertion and deletion in a sequence. For the sequence searching, our encoding module needs to keep the similarity of the sequences in the high-dimensional space. Instead of encoding the entire sequence at once, BioHD split the sequence into small partitions. Then, BioHD exploits the binding to map the small partition into high-dimensional space. BioHD splits sequence into small nonoverlapped partitions with the length of . For a query sequence with length of , BioHD splits sequence to / number of partitions. Each chunk encodes into high-dimensional space using the same encoding method used for exact pattern matching. For example, for \u210e partition, the encoding performs as: = 1 * 2 * \u00b7 \u00b7 \u00b7 * \u22121 .\n\nSince the partitions have small sizes, they have a high possibility that all genome digits to be the same in each partition (4 distinct patterns for partitions with the size of ). Partitions are bundled into substrings of the length equal to the query. Thus, each substring has / partitions. BioHD aggregates the information of different partitions by preserving their position using a set of position hypervectors: \u00ec = \u00ec P 1 * \u00ec S 1 + \u00ec P 2 * 2 + \u00b7 \u00b7 \u00b7 + \u00ec P * \u00ec S , where \u00ec Ps are a set of correlated hypervectors, where \u00ec P 1 and \u00ec P / are nearly orthogonal while \u00ec P and \u00ec P +1 have high similarity.\n\n\nReference Generation\n\nThe query sequence is typically short, while the reference protein is a long-length sequence. The reference can be encoded by mapping each protein sequence (Figure 3 \u2022 4 ). This encoding is repeated over the reference genome using a sliding window scheme. A window moves through a protein sequence, and BioHD encodes a protein sequence in each window. Next, BioHD accumulates multiple sequence hypervectors into a single reference hypervector:\n\u00ec R = =1 \u00ec S .\nWe check the existence of a query, \u00ec , in the reference hypervector by performing the following similarity measurement:\n( \u00ec R, \u00ec Q) = ( \u00ec S , \u00ec ) Signal + =1, \u2260 ( \u00ec S , \u00ec ) Noise(1)\nIf \u00ec S = \u00ec for some , the output of the function will be 1, For reference patterns that do not match with the query, the similarity is nearly zero, ( \u00ec S , \u00ec ) 0. The level of noise is increasing the number of non-matched sequences. This limits the number of sequence hypervectors that can be stored in each reference hypervector. BioHD needs to ensure that the level of noise is small enough that makes the signal identification non-distinguishable. To address the issue, BioHD exploits multiple hypervectors representing the reference sequence. Each hypervector stores the information of a pre-defined number of patterns (evaluated in Section 6.2).\n\n\nProtein Pattern Matching\n\nBioHD performs the sequence searching by checking the similarity of an encoded protein sequence with the HDC library. Depending on the encoding, BioHD searches for an exact or approximate match of a query with each reference hypervector. The search checks if \u00ec exists in \u00ec R using a -membership function:\n1 \u00ec S \u00b7 \u00ec \u2265 \u00d7 0 \u210e (2)\nwhere an output of 1 indicates membership, and 0 otherwise. We exploit different for an exact or approximate match. In case of the exact matching, the theoretical recall rate (correctly identify match) can be evaluated precisely:\n( , ( ) = 1| \u2208 ) \u2265 ( \u22121) = 1 2 ( \u22121) ( \u2212 1)(3)\nwhere = ( + \u22122 ) \u00d7 /2. The last step achieves equality when the query appears exactly once in the reference. This indicates that we can improve the matching accuracy by (i) using a larger threshold value , (ii) increasing dimensionality , or (iii) retraining the model (increasing ). Note that increasing the results in larger true negative cases. Also, the increase in dimensionality comes at the cost of lower computation efficiency (evaluated in Section 6.2). Adaptive Library Refinement: Similar to the human brain, it is difficult for BioHD to remember the information of all accumulated data into each reference hypervector using single-pass memorization. The pattern of the most common sequence dominates the reference hypervector and results in vanishing the pattern of the less frequent sequences. To better memorize the information, BioHD looks at the same object multiple times to boost the accuracy by discarding the mispredicted queries from the corresponding model hypervector and adding them to the right one. If a query ( \u00ec S) corresponding to reference mispredicts with reference hypervector, we update the library:\n\u00ec R = \u00ec R + (1 \u2212 \u00ec R \u00b7 \u00ec S/ ) \u00d7 \u00ec S & \u00ec R = \u00ec R \u2212 (1 \u2212 \u00ec R \u00b7 S/ ) \u00d7 \u00ec S\nwhere the term 1 \u2212 \u00ec R \u00b7 \u00ec S/ ensures that both reference hypervectors update depending on how far a query is mispredicted.\n\n\nHARDWARE ACCELERATION\n\nBioHD can be accelerated on existing parallel architectures. For example, BioHD provides high computation efficiency when running on GPU platforms (explained in Section 6.4). However, in existing platforms, data movement between memory and computing units (moving the reference library to a processing core) takes most of the alignment performance and energy. We propose a novel processing in-memory (PIM) architecture to accelerate BioHD computation. Besides data movement, BioHD has several features that make it well-suited for PIM: (i) the operations of the proposed algorithms can be natively supported in the memory, e.g., Hamming distance computing, (ii) BioHD is memory-centric and highly parallel at heart, and (iii) it provides strong robustness to noise. In this section, we explain how PIM can support all required BioHD operations by making minor modifications to existing crossbar memory. Figure 4 shows the summary of the supported operations: (i) search-based Hamming computing to measure the distance between a query and reference hypervectors, (ii) row-parallel arithmetic operations to support high-precision dot-product distance computation as well as to check the existence of a query in a reference hypervectors, and (iii) a row-parallel binding operation to perform encoding operation.\n\n\nSearch-based Distance Computing\n\nExact Search: Content addressable memories (CAMs) support exact search operation [50]. As Figure 4 shows, in conventional CAM, each cell represents using two memory elements, where the elements take complementary values. For example, in a crossbar memory, a CAM cell represents using two NVM devices (0T-2R). The memory elements stores complementary values. A CAM array consists of a match-line (ML) shared among all cells in a row and a bitline that controls all cells in a column. Before search operation, all MLs are pre-charged to high voltage. During the search, the bitlines load the search query among all CAM rows in parallel. If the value of the CAM cells matches the search query, they do not discharge the ML. Otherwise, mismatch cells add discharging currents. CAMs assign a sense amplifier to the tail of each ML to detect a row with all cells matched. Hamming Distance Computing: Several recent works modified the CAM sense amplifier to detect the search for the nearest Hamming distance search [37,51,52]. Sense amplifiers in different CAM rows are competing to find an ML that has the minimum discharging current. Although the nearest search is important functionality, in many real-world problems, we must measure the actual distance of the query with CAM rows. Similarly, BioHD also requires computing the distance of an encoded query with the HDC library. In CAM, the ML discharging current is related to the number of mismatched cells. The more mismatches, the higher current passes through the ML. However, this current does not linearly scale with the number of mismatches (Figure 4a). This non-linear behavior makes Hamming computing challenges.\n\nTo address this issue, we propose a technique that enables ML discharging current to better scale with the number of mismatches. We observe that the dropping ML voltage is the main reason that the ML current does not scale with the number of mismatches. Ideally, each CAM cell needs to have a discharging current equivalent to: \n\n\nCout = ((A+B)' + (B+C)' + (C+A)')' S = (((A'+B'+C')' + ((A+B+C)' + Cout)')')' pc1 pc10\n\nIntermediate results = / , where is the ML voltage and is the resistance value of CAM cell during mismatch. Due to the delay of the sense amplifier in sampling the ML current and quick drop in ML voltage, the ML does not have a linear value over all mismatches. We exploit a voltage stabilizer that connects the ML to a constant supply voltage to ensure voltage does not drop lower than . Figure 4a shows the enhanced CAM array. This ensures that every mismatch will have the same impact on the ML discharging current. Figure 4a also shows that the ML current that was initially saturating scales linearly up when using the voltage stabilizer. BioHD exploits a low-precision analog-todigital converter (ADC) for Hamming computing. When searching over a very large CAM, the ML discharging current can still be close, and thus the sense amplifier may not detect their difference. To address this challenge, we perform sequential Hamming computing over small windows sizes.\n\n\nSearch-based PIM\n\n\nRow-Parallel PIM-based Arithmetic\n\nBioHD supports arithmetic operations directly on digital data stored in memory without reading them out of sense amplifiers [22,[53][54][55][56][57][58]. Our design exploits the NVM switching characteristic to implement NOR gates in digital memory [22,54]. BioHD selects three or more columns of the memory as input NOR operands by connecting them to ground voltage (shown in Figure 4b). This NOR computation performs in row-parallel on all the activated memory rows by the row-driver. Since NOR is a universal logic gate, it can be used to implement other logic operations like addition [59] and multiplication [22].\n\n\nRow-parallel Binding\n\nBioHD encoding requires to support the binding operation. The binding over multiple binary vectors is an XOR operation. In fact, the binding computes if there are even or odd numbers of 1-bits among the selected inputs. The XOR operation can be computed in a digital way by performing a series of row-parallel NOR operations. However, this requires several sequential cycles. To support fast encoding, BioHD exploits the same ADC block used for Hamming distance computing to perform binding in an analog way. During computing mode, each memory cell adds a current to an ML. The cells storing '1' bit are adding current, while cells are storing '0' bit add a small current to the bitline, where >> . This current is accumulated in an analog way using the ADC block.     [60,61], each memory block is assumed to have a size of 1 \u00d7 1 . BioHD consists of two types of memory blocks: encoding and distance computing. Both blocks are the same conventional crossbar array; they are organized in each tile to enable fast and parallel sequence searching. Both memory blocks support parallel computation. Thus, we translate all BioHD computation to vector-based operations. Our computation is performed in a row-parallel way by activating multiple memory columns and computing the result on the sense amplifier located next to each memory row. As Figure 5 \u2022 shows, the block sense amplifiers are low-precision ADCs that are shared among several memory blocks. Unlike the existing analog-based PIM, crossbar memory takes the majority of our architecture, and ADCs only take a tiny area. Each memory block also supports row-parallel bit-serial data transfer, where a single bit can be written to the next memory block over all selected rows. The memory controller is responsible for assigning the task to each memory block in a pipeline manner to provide high parallelism. Here, we describe the encoding and distance computing blocks and how they work in a pipeline.\n\n\nBIOHD PIM ARCHITECTURE\n\n\nEncoding Acceleration\n\nPIM-based implementation of BioHD encoding requires (i) reading the pre-stored acid hypervectors from memory blocks, (ii) copying it to a new block, and performing permutation depending on the acid position in a sequence, and (iii) finally binding all permuted hypervectors. These steps are computationally costly and incur a lot of internal data movement and write operations, which are significantly slower in PIM architecture.\n\nCodebooks: Figure 5 \u2022 shows the first step of the encoding that assigns a codebook to each amino acid. Since there are 20 acids in the Codon table (Figure 1b), these objects can be addressed using 5-bit codebooks. In hardware, the codebooks are assigned using a lookup table (CAM) with the exact search functionality. We also store -bits value along with each codebook specifying the amino acid position. The value of depends on the length of the protein sequence.\n\nFor example, for a sequence with 256 proteins, we require 8-bits for addressing each protein. For example, codebook 00100 00010 indicates that 5 \u210e protein in the sequence is Leu which corresponds to 2 acid in the Codon table.\n\nEncoding Memorization: Instead of naive encoding implementation, we exploit computational reuse to avoid redundant encoding computation. Let us assume a memory block with 1 \u00d7 1 size. Our approach pre-stores all amino acid hypervectors in the first 32-columns of the memory block. If BioHD dimensionality is larger than 1k, we exploit multiple neighbor (vertical) blocks to store the entire hypervector. For example, for hypervectors with = 4 , four vertical blocks store BioHD dimensionality. Next, we permute all protein hypervectors ( : single permutation) and store them on the second 32-columns of memory. Similarly, we perform more permutations on the acid hypervectors and store them on the next set of columns. For a block with 1k-columns, each block stores 32 permutations of the acid hypervectors. As Figure 5 \u2022 shows, each 32-columns stores all possible hypervectors that a protein in a specific position can take. These hypervectors can be directly addressed using our codebooks. For example, codebook 00110 00111 addresses to 130 \u210e column of memory storing 5 . This address directly indicates that the 5 \u210e amino acid in the sequence is 7 \u210e acid in the Codon table. Hypervector Binding: By accessing codebooks in parallel, we can activate columns of the memory block at the same time. Note that in every 32-columns chunk, only a single memory column will be activated, depending on the acid codebook. These pre-stored and permuted hypervectors eliminate the costly and sequential memory read/write and rotational shift. Finally, the encoding block binds all selected memory columns using the ADC block shown in Figure 5 \u2022 . Depending on the protein length (which often does not fit in a single memory block), our PIM computes the final binding result during multiple sequential cycles. As Figure 5 \u2022 shows, each memory block is enhanced with a permutation block. After reading elements in a sequence, BioHD shifts the binded result depending on the position of the windows in the sequence. To support a protein sequence with a length of , we compute the final binding result in / iterations. The first window does not get any permutation on the binding results, while \u210e windows is permuted by \u00d7 positions.\n\n\nDistance Computing\n\nDistance computing is the most frequent BioHD operation. The goal of BioHD is to compute the distance of a query with large-scale reference hypervectors stored in memory.\n\n\nSimilarity Metric & Precision:\n\nThe capability of memory to support search operations depends on the distance metric. The distance metric by itself depends on the representation of the values in the HDC library. For a library with binary representation, BioHD uses Hamming distance as a similarity metric. Although BioHD supports Hamming computing in a fast and efficient way, each binary reference hypervector can only store limited information. This means that the HDC library will consist of several binary reference hypervectors. On the other hand, BioHD can represent the library hypervector with higher precision (e.g.,32-bit values). This increases the hypervector capacity to store more information in each library hypervector. However, in this representation, we require to use costly cosine distance as a similarity metric. Here, we propose a hardware solution to support both Hamming distance and dot product distance computing in a parallel way. For binary Hamming distance, we exploit CAM blocks to support ultra-fast search operation using shared and low-precision ADC blocks, while the dot product operation happens entirely in digital space without any ADC blocks.\n\nHamming computing: Figure 5 \u2022 shows a row-parallel Hamming computing between a query and all reference hypervectors stored in the memory. BioHD performs sequential Hamming computing on a small search window. In this work, we use 32-bits windows to ensure 5-bit ADC precision. To limit the cost of ADC blocks, we share ADCs among multiple distance computing blocks. We use sample & hold (S+H) circuit to record the ML discharging current of each block and use time multiplexing to share ADC among 128 memory blocks. The result of distance computing will be written in the next memory block, called comparison memory, in a bit-serial row-parallel way. Since the write speed is slower than the search, we use registers to save the ADC values and write them sequentially in the next memory block. The comparison memory will accumulate the partial distance values using digital-based PIM operations explained in Section 4.2. Since the search is performed over dimensional vectors, we require /32 vector-based addition to compute the final distance result. Note that the addition is performed in the pipeline with the search operation (explained in section 5.5). Dot Product Distance Computing: Using a high-precision HDC library, we can compute the dot product similarity of a query with multiple reference patterns stored in memory. Dot product operation is an expensive operation, especially due to the complexity of the multiplication operation. However, BioHD encoding generates a query that is always a binary hypervector. This simplifies the dot product operation to accumulate multiple vectors in memory. Figure 5 \u2022 shows row-parallel dot product operation between a query and stored reference hypervectors. As discussed in Section 4.2, BioHD arithmetic operations can add or subtract digital data located in different memory columns. BioHD uses query data as a decider to add or subtract different dimensions of the reference hypervectors. Each reference dimension (memory column) corresponding to a positive query value (+1) will be added, while dimensions corresponding to zero query value will be subtracted from the final dot product result. Our PIM solution enables addition/subtraction independent of the number of memory rows. Since the search happens on long-size vectors, the vector accumulation may not fit in a single memory block. Therefore, we exploit multiple memory blocks to compute the dot product operation partially. The partial results can be aggregated using bit-serial row-parallel data transfer between neighbor blocks. Note that, unlike Hamming computing, dot product similarity does not require costly ADC blocks for distance computing. This increases BioHD area efficiency.\n\n\nThresholding\n\nAfter distance computing, BioHD needs to select a reference hypervector with close distance with the query data. The closeness defines compared to a fixed pre-defined threshold ( ). Depending on the exact or approximate match, we may use different threshold values. Like distance computing, this operation requires massive parallelism as we need the same comparison over all reference hypervectors. Using Hamming or dot product distance metric, the result of distance computing will be stored in the comparison block. The comparison block is exactly one of the distance computing blocks. The only difference is that this block pre-stores the copy of the threshold in every row (single column) of the block. To check for a match, we subtract column from all distance vectors stored in the same memory block. Any subtracted value with a positive sign bit indicates a match.\n\n\nEarly Search Termination\n\nDuring alignment, a query sequence will most likely match with one or a few sequences in the HDC library. In fact, this is unlikely to find several references with high score matching. Conventionally, we need to compute the distance when the sequential distance computation covers the entire dimensions of hypervectors. In order to eliminate the significant distance computing cost, we propose the idea of Early Search Termination (EST). In this approach, the comparison block identifies the reference whose distance to query has already passed an acceptable threshold (1 \u2212 ). Next, BioHD sends a signal to the row driver of distance computing and stops the search operation on the selected rows. This selective activation lowers BioHD computation cost (Section 6.4).\n\n\nBioHD Pipeline\n\nBioHD architecture works in a pipeline (shown in Figure 5 \u2022 ). At first, the encoding block maps data into high-dimensional space. Instead of assigning large PIM resources to generate all dimensions of a query hypervector, BioHD only generates dimensions that can be used by the distance computing block ( < ). Next, BioHD computes the distance of the query hypervector with all pre-stored reference hypervectors (over windows of dimensions). During search, BioHD pipeline ensures maximum utilization of the memory blocks. As soon as a search is finished, the search for the second window starts in the same memory block. All partial distances are transferred to the comparison block. The comparison block accumulates the partial distance and checks if it is pass a \n\n\nOther Genomics Data\n\nBioHD is a general alignment platform that can be used to accelerate other genomic data rather than protein. For DNA and RNA data, BioHD exploits the same framework to support high-dimensional alignment. The only difference is that for DNA and RNA alignment, we do not require an intermediate step to generate amino acid hypervectors. Similar to protein, the encoding depends on the desired alignment task. For an exact match, BioHD binds the entire DNA sequence into a single hypervector. To count for mismatches, BioHD encoding is performed over smaller windows sizes. We exploit the same hardware architecture to accelerate BioHD computation. For encoding, PIM pre-stores the hypervector corresponding to each DNA or RNA in the encoding block. Each chunk stores all possible characters that each DNA or RNA digits can take. This limits the size of each chunk to four hypervectors ({ \u00ec A, \u00ec C, \u00ec G, \u00ec T} for DNA data). The chunks are repeated by pre-storing different permutations of the hypervectors. For example of DNA data, the \u210e chunk stores { \u00ec A, \u00ec C, \u00ec G, \u00ec T} hypervectors. Since the size of the query and chunk hypervectors are often limited to 200 digits [62,63], a single memory block (>800 bitlines) can store all permuted base hypervectors. It eliminates the necessity of using permutation block and data aggregation to compute the encoding results. Regardless of data type or query length, BioHD maps both query and reference library to hypervectors. Thus, it can exploit the same hardware for distance computing. \n\n\nEVALUATION 6.1 Experimental Setup\n\nThe proposed BioHD framework has been implemented with two co-designed modules: software framework and hardware accelerator. Our software framework supports HDC library generation, updates the model, and tests BioHD quality of the alignment. Thanks to BioHD vector-based operations, we fully integrated BioHD with TensorFlow [64], and designed a cycle-accurate simulator to emulate BioHD functionality during different alignment tasks. For the hardware design, we use HSPICE for circuit-level simulations to measure the energy consumption and performance of all the BioHD operations in 28nm technology. We used System Verilog and Synopsys Design Compiler [65] to implement and synthesize the BioHD controller. For parasitics, we used the same simulation setup considered by work in [59]. The interconnects are modeled in both circuit and architecture levels. The robustness of all circuits have been verified by considering 10% process variations on the size and threshold voltage of transistors. Our PIM works with any bipolar resistive technologies, which are the most commonly NVMS. To have the highest similarity to commercially available 3D Xpoint, we adopt the memristor device with a VTEAM model [66].\n\nThe memristor's model parameters are chosen to produce a switching delay of 1ns, a voltage pulse of 1V and 2V for RESET and SET operations to fit practical devices [54,55]. Table 1 shows the detailed configurations of BioHD consisting of 128 tiles. Each tile has 128 crossbar blocks. BioHD has two configurations: Hamming computing that uses shared ADC blocks for distance computing, and Dot Product computing (DOT), where the distance is computed using row-parallel PIM arithmetic. In DOTtile, the crossbar memory takes the majority of the area and power consumption, while in HAM-tile, ADCs are taking 28% and 15% of total area and power consumption. Each HAM-tile (DOT-tile) takes 0.57 2 (0.41 2 ) area and consumes 1.07W (0.93W) power. The total HAM-chip (DOT-chip) area and average power consumption are 73.52 2 and 137.81W (53.04 2 and 119.79W), respectively. All our evaluations are performed when BioHD provides the same area in both configurations. Note that our HAM chip can be configured to perform both Hamming distance and dot product similarity, while DOT Chip is an optimized version that just supports dot product similarity.\n\nWe compare BioHD efficiency with two state-of-the-art alignment tools running on GPU: (i) NVBIO [67]: GPU-accelerated C++ framework for high-throughput sequence analysis, and (ii) GPU-BLAST [68]: a GPU-accelerated nucleotide alignment tool based on NCBI-BLAST. We test BioHD efficiency on popular genomic data. Table 2 summarizes the evaluated sequence datasets. This includes DNA datasets such as E.coli (MG1655) and recent COVID-19 reference genome [69], to large-scaled protein databases.\n\n\nQuality of BioHD Sequence Searching\n\nIn BioHD, each reference hypervector has a limited capacity to store information of other encoded sequences. This capacity depends on the dimensionality and the precision of the hypervector elements. Figure 6a shows the theoretical capacity of a reference hypervector with 32-bit and 1-bit precision to memorize the information of bundled patterns (Equation 3). The results indicate that BioHD memorizes more patterns using higher dimensionality and precision. Figure 6b compares the experimental and theoretical capacity of a reference hypervector (32-bit precision) after the HDC library refinement, explained in Section 3.3. BioHD experimental capacity is saturating by increasing the number of bundled patterns due to the non-orthogonality of the hypervectors and the weakness of the refinement method. A 32-bit precision (binary) follows the theoretical capacity using = 40 ( = 10 ) dimensionality. Further increasing dimensionality will have less impact on increasing the hypervector capacity. Figure 6c shows the distribution of the signal and noise when we bundle a different number of patterns into a single reference hypervector. Our evaluation shows that increasing the number of patterns increases the amount of noise. By selecting a suitable threshold value, we can identify the signal from noise. The threshold value decides for an exact or approximate match. Figure 6d is a Receiver Operating Characteristic (ROC) curve that shows the impact of the threshold value, storing a different number of patterns ( ). The ROC curve shows the quality of the alignment based on false positive and true positive using different threshold values. As the graphs show, storing a large number of hypervectors into a reference ( = 10 9 , which is more than experimental capacity) results in a BioHD quality loss (true positive less than 100%). This happens as the Gaussian noise (in Figure 6c) overlaps with the main signal. Regardless of the number of patterns, a large threshold increases the false positive rate, while a smaller threshold increases the rate of a true positive. Depending on the number of vectors stored in each reference, we select a point in the top-left region of the curve that provides maximum accuracy. In the rest of the section, we limit the capacity of each reference hypervector to its experimental level to ensure 100% true positive rates.\n\n\nBioHD on PIM Architecture\n\nUnlike the traditional alignment algorithms, BioHD has a memorycentric architecture. This makes our proposed PIM architecture a promising solution for computing local data stored in storageclass memory. Figure 7b compares BioHD efficiency running on our PIM architecture. All results are normalized to GPU running BioHD-DOT. Regardless of configuration, our PIM solution provides significantly higher efficiency compared to GPU architecture by: (i) addressing the data movement issue by enabling in-place computation, (ii) enabling massive parallelism to accelerate BioHD computation, and (iii) accelerating the essential BioHD operations including distance computing and thresholding.  Encoding that implements an optimized version of the encoding without exploiting the computational reuse (Section 5.1). (ii) Naive Thresholding that implements the thresholding by performing the tree-based comparison, rather than using the proposed thresholding (Section 5.3). (iii) PIM-DOT and PIM-HAM that use the dot product and Hamming computing circuit for distance measurement, (ii) PIM-DOT-EST and PIM-HAM-EST that enhance the distance measurement with Early Search Termination (Section 5.4). Dot Product vs. Hamming Distance As explained in Section 4, BioHD has an option of using binary or full precision HDC library. Although the binary library can exploit low-cost Hamming distance, it requires a large number of reference hypervectors. The library is more compact but requires a more costly distance metric in high precision. Figure 7a-b compares the average BioHD efficiency using Hamming and dot product distance metrics. On GPU (Figure 7a), GPU-DOT achieves significantly higher efficiency than GPU-HAM as the performance is bounded by the number of resources. In contrast, our PIM architecture (Figure 7b) provides massive parallelism by computing over the local data. Our evaluation shows that PIM-HAM provides 1.7\u00d7 faster and 1.4\u00d7 higher energy efficiency compared to PIM-DOT. Early Search Termination: Figure 7b compares BioHD efficiency using early search termination (EST). In this configuration, BioHD stops costly distance computing on the majority of the memory rows during the alignment process. The number of terminated rows depends on the threshold margin ( ). Using = 0.9 for DNA matching, the EST technique reduces the number of required comparisons by 63% on average. Our evaluation shows that the EST does not affect BioHD performance while improving energy efficiency by 1.3\u00d7 and 1.4\u00d7 in PIM-HAM and PIM-DOT, respectively. In summary, PIM-HAM is 102.8\u00d7 faster and 116.1\u00d7 higher energy efficient than GPU running the baseline BioHD. Although we do not explore, however, EST can further enhance BioHD performance using sparsity-aware framework. Memory blocks that are early terminated can start querying the next data in the pipeline. This approach increases the BioHD resource-utilization and provides speedup. BioHD Encoding & Thresholding: Figure 7b shows the impact of optimized encoding and thresholding on PIM efficiency. Our evaluation shows that our encoding provides 1.4\u00d7 speedup compared to the Naive-Encoding implementation. The proposed encoding has a minor impact on energy efficiency as the encoding takes a small portion of total energy. In BioHD, thresholding performs over large-scaled reference data. Our parallel thresholding results in 1.9\u00d7 speedup and 1.1\u00d7 energy efficiency than the naive approach. Figure 7a compares BioHD computation efficiency with the stateof-the-art alignment algorithms running on GeForce RTX 3060 Ti GPU. All results are normalized to the energy and execution of GPU running NVBIO running nvBowtie [74]. The G-BLAST provides higher performance and efficiency compared to NVBIO by performing a heuristic alignment since G-BLAST has (1) higher potential for GPU acceleration compared to bowtie, and (2) it enables heuristic and approximation, which can cause quality loss. All our BioHD evaluations are performed in a setting that guarantees 100% quality of the alignment (100% true positive rate). In BioHD, the library generation has a one-time cost, and generation overhead will be amortized over several alignment queries. Our evaluation shows that BioHD provides higher efficiency compared to the baseline running on the GPU platform. This efficiency comes from the following reasons: (i) BioHD memorization reduces the number of required computations to perform an alignment task. In BioHD, each reference hypervector represents thousands of sequences. Therefore, a single similarity comparison between a query and a reference hypervector is equivalent to thousands of pattern matching in the original space. (ii) Alignment problem is not computationally expensive, while it requires bringing a large amount of data to on-chip memory and processor to perform alignment comparison. BioHD reduces this data movement by creating a compressed reference library that can be directly used for massively parallel computation. Our evaluation shows that BioHD using Hamming (HAM) and dot product (DOT) distance provide, on average, 1.6\u00d7 and 2.3\u00d7 faster and 3.7\u00d7 and 4.9\u00d7 higher energy efficiency compared to NVBIO. Note that BioHD provides higher efficiency for protein alignment rather than DNA. In protein alignment, BioHD represents each acid with a single hypervector, reducing the required operations. The efficiency values are reported compared to GPU. We compute the efficiency of PIM accelerator using our cycle-accuracy simulator. The results are validated with the performance and efficiency reported on each original paper. PipeLayer [13] and FloatPIM [22] are neural network accelerators, but their operations can be used to accelerate the sequence searching algorithm. Our evaluation shows that BioHD provides significant efficiency improvement compared to PIM architectures. This efficiency comes from: (i) BioHD capability in revisiting alignment using HDC with hardware-friendly operations, (ii) BioHD PIM architecture supporting highly parallel essential operations, and (iii) data flow in BioHD that eliminates internal data movement. In contrast, PipeLayer and FloatPIM require a large amount of internal data movement and costly operations with lower parallelism. PIM-Aligner [75] and RAPID [39] are also recent PIM accelerators for alignment based on magnetic and resistive devices. Both accelerators have limited programmability and lack of their architectural support results in a large amount of internal data movement. Figure 8 also compares BioHD efficiency with DRAM-based accelerators: Ambit [76], ComputeDRAM [77], and Newton [78]. DRAM-based PIMs are suitable to accelerate existing alignment algorithms that rely on extensively parallel bitwise and arithmetic computation. In contrast, these accelerators do not support associative search, which is the key functionality of BioHD computation. This makes DRAM-based solution ineffective for BioHD acceleration. In addition, DRAM-based PIM operations are often destructive, thus can result in losing the information of input operands. Our evaluation shows that, in the same area, BioHD provides 7.3\u00d7 and 12.0\u00d7 (14.8\u00d7 and 15.3\u00d7) faster and higher energy efficiency compared to Newtown (ComputeDRAM). This efficiency mainly comes from BioHD algorithm-hardware co-design, which makes matching nearly ideal for search-based operation.\n\n\nT T T m T T T T m T T T T =0.6\n\n\nArea of interest\n\n\nT T T m T T T T T m m T T T T T T =0.8 T T T m T T T T T m m T T T T T T =0.75\n\n\nBioHD on GPU & PIM\n\n\nBioHD-PIM vs State-of-the-art PIM\n\n\nBioHD Scalability & Robustness\n\nTechnology Scaling: Figure 9 compares BioHD area and computation efficiency in different technology nodes. The results are computed based on the detailed area-efficiency analysis of each technology. Our evaluation shows that although BioHD-HAM has higher efficiency in 45nm and 28nm technology node, BioHD-DOT is expected to provide higher efficiency in 16nm technology and beyond. This is because the ADCs take larger chip areas, as mixedsignal circuits do not scale as fast as digital technology. This results in low performance/area efficiency of BioHD-HAM in scaled technology. We also expect NVM scaling to further improve BioHD-DOT efficiency as it relies on minimal analog circuits. As NVMs are moving towards multi-bit technologies [79,80], BioHD-DOT is expected to provide higher density and efficiency. Robustness: Figure 10a compares BioHD robustness to noise in the memory devices. The results are reported for the Bowtie2 Alignment, and BioHD using DOT and HAM distance similarity. As we expected, regardless of the distance metric, BioHD provides significantly higher robustness to memory noise than Bowtie2 alignment. On the other hand, BioHD-HAM has higher robustness to noise compared to BioHD-DOT. In binary representation, an error only flips a reference dimension from 0 to 1 or 1 to 0. This results in minor changes in the entire hypervector pattern. In contrast, an error in BioHD-DOT can happen in the most significant bits, which can significantly impact the absolute value and robustness. Our results indicate that 9% failure in memory cells results in 1.9% and 7.5% loss on BioHD-HAM and BioHD-DOT quality of the alignment. Figure 10b explores the impact of limited NVM endurance on BioHD quality of the alignment. We assume an endurance model with 10 7 and 10 4 [24]. Our evaluation shows that after a few years of using our PIM-based platform, similar to the human brain, BioHD starts forgetting information stored in reference hypervector. As Figure 10b shows, the aging is faster for technologies with lower endurance. To address this issue, we perform wear-leveling to distribute writes on different memory blocks uniformly. The overhead of wear-leveling is minor as BioHD has a predictable write pattern and wear-leveling only happens in long-time periods. Wear-leveling aims to move the computation such that (1) each block does not use the same memory columns as reserved columns for PIM operation. (2) it distributes the tasks among the different memory blocks. BioHD uses two counters for each memory block to implement Wear-leveling reduces the average writes in each memory cell by 18\u00d7. It creates 0.4% area overhead while having no impact on BioHD performance. Our evaluation shows that BioHD-DOT has higher sensitivity to the endurance. This is because BioHD-DOT distance computation requires PIM arithmetic operation that involves several device switching. In contrast, BioHD-HAM computes distance using a sense amplifier with minimal write operation in memory. Figure 11 shows BioHD PIM efficiency over several synthetic reference DNA when the data size increases from 1GB to 16GB. The results are normalized to PIM performance using 1GB reference size. Our evaluation shows that PIM performance improves linearly with the reference size. For reference genome larger than PIM capacity, BioHD needs to pay an extra data movement cost to write reference genome on PIM. The performance saturation and energy efficiency degradation in Figure 11 are the result of this sequential data transfer. For example, PIM with 32GB (2GB) reference size provides 2.5\u00d7 (1.8\u00d7) speedup compared to PIM with 1GB reference size. Figure 11 also shows BioHD performance scalability with the number of chips. At the algorithm level, BioHD has very low data dependency. The distance computing and thresholding, two major BioHD operations, can perform independently for reference sequences. This results in a linear performance improvement of PIM with the number of chips. Figure 12 shows the breakdown of BioHD performance on GPU and the proposed PIM platforms. In GPU, data movement takes the majority of BioHD execution time (83%). In contrast, PIM-based solutions significantly reduce the amount of data movement. In PIM-HAM and PIM-DOT, the distance computing takes the biggest portion of execution time. After that, the thresholding step is a dominant factor. PIM-DOT requires more internal data movement for aggregating of partial dot products during the distance computation. PIM-HAM has more compact data aggregation using ADC blocks.\n\n\nBioHD & Reference Genome\n\n\nBioHD Efficiency Breakdown\n\n\nRELATED WORK\n\nHyperdimensional Computing: The field of hyperdimensional computing is introduced by P. Kanerva, a computational neuroscientist [35]. Since then, prior research have applied the idea into diverse cognitive tasks [44,45,[81][82][83][84][85][86][87][88]. Although the mainstream of the research is on learning tasks, HDC is the model of human memory with significant promises for memorization. For example, work in [44] showed the HDC application to enable memorization in robotics. Work in [38,89] exploit HDC for DNA pattern matching on CMOS technology. However, this approach only supports exact matching with limited applications to DNA data. In contrast, BioHD is a general framework for genome sequence search, highly advanced with the PIM architecture. Processing in-memory: The capability of non-volatile memories (NVMs) to act as both storage and processing units has encouraged research in Processing In-Memory (PIM) [17,20,22]. NVM-based PIMs have been used to accelerate a wide range of big data applications such as supervised learning [13, 15-17, 20, 21, 90], graph processing [8,9,91]. There are also multiple PIM accelerator for alignments, e.g., PIM-Aligner [75] and RAPID [39], Genasm [34], EXMA [92], Darwin-WGA [33], FastHASH [32], and GateKeeper [93]. However, these architectures incur large amounts of internal data movement as genomic algorithms are not memory friendly. In addition, all existing PIM-based accelerators are significantly sensitive to even small amounts of noise in hardware (e.g., 1%). However, in practice, memory devices have various reliability issues such as endurance, durability, and variability. Our solution is the first approach that introduces the idea of holographic high-dimensional representation for computation, thus enabling highly robust and efficient computation. Moreover, BioHD simplifies genome matching to hardware-friendly operations which are ideal for PIM technology.\n\n\nCONCLUSION\n\nWe propose a novel Hyperdimensional sequence search platform for hardware-friendly computation. BioHD revisits the alignment problem with human memorization, where the reference genomes can be stored and recalled exactly or approximately during alignment. We also design a processing in-memory architecture with massive parallelism and compatible with the existing crossbar memory. Our results indicate that BioHD provides 102.8\u00d7 speedup and 116.1\u00d7 energy efficiency compared to GPU-based alignments.\n\nFigure 1 :\n1(a) Protein back-translation and alignment flow, (b) RNA translation to amino acids\n\nFigure 2 :\n2(iii) Permutation ( ) defines as a single rotational shift. The permutation operation generates a hypervector, which is unrelated to the(b) BioHD Processing In-Memory Architecture HD Library (Overview of (a) BioHD alignment in high-dimensional space, (b) the proposed PIM accelerator.\n\nFigure 4 :\n4PIM Search-based & arithmetic computation.\n\nFigure 5 :\n5Overview of PIM-based BioHD architecture along with encoding and distance computing blocks.\n\nFigure 5 \u2022\n5shows an overview of the PIM architecture consisting of 128 tiles. Each tile consists of 128 crossbar memory blocks. Due to the existing challenges of crossbar memory\n\n\nFor example, PIM-HAM (PIM-DOT) provides, on average, 103.0\u00d7 (60.5\u00d7) faster and 89.3\u00d7 (61.7\u00d7) higher energy efficiency compared to BioHD-DOT running on GPU. BioHD Configurations: To show the impact of each optimization, we report PIM efficiency on different configurations: (i) Naive\n\nFigure 6 :\n6(a-b) Theoretical and experimental capacity of a hypervector. (c) The signal-noise distribution, (d) ROC curve indicating false positive and true positive rates.\n\nFigure 7 :Figure 8 :\n78BioHD vs State-of-the-art on GPU (b) BioHD on PIM BioHD efficiency on GPU and PIM in different configurations. Results are normalized to GeForce RTX 3060 Ti GPU. BioHD-PIM comparison with state-of-the-art.\n\nFigure 8 compares\n8BioHD efficiency with other PIM accelerators. All PIMs have the same area as BioHD in the 1-chip configuration.\n\nFigure 9 :Figure 10 :\n910Technology scaling and BioHD efficiency. BioHD quality vs. noise and endurance.\n\nFigure 11 :Figure 12 :\n1112Scalability to the reference size and # of chips. BioHD-PIM performance breakdown. wear-leveling. The counters keep track of the number of writes and the memory columns used for intermediate PIM computation. Our technique shifts the computation across the columns and blocks.\n\n\n, New York, NY, USA \u00a9 2022 Association for Computing Machinery. ACM ISBN 978-1-4503-8610-4/22/06. . . $15.00 https://doi.org/10.1145/3470496.3527422\n\nTable 1 :\n1BioHD PIM Parameters.Component \nParam \nSpec \nArea \nPower \n\nCrossbar array \nsize \n1Mb (1 \u00d7 1 ) \n3136 2 \n6.14mW \nSense Amp \nnumber \n1k \n49.2 2 \n0.09mW \nMemory Block \nnumber \n1 \n3185.2 2 6.23mW \n\nHamming Computing (HAM) \nTile Memory \nnumber \n128 blocks \n0.40 \n\n2 \n\n0.78W \n\nADC \nresolution 5-bits, 1.75GS/s \n0.16 \n\n2 \n\n0.14W \nnumber \n128 \nS+H \nnumber \n128 \u00d7 1k \n857.6 2 14.97mW \nInterconnect \nsize \n1k/row \n0.01 \n\n2 \n\n31.04mW \nController \nnumber \n1 \n146.5 2 118.9mW \nHAM-tile \nsize \n128Gb \n0.52 \n\n2 \n\n1.07W \n\nDot Product Computing (DOT) \nTile Memory \nnumber \n128 blocks \n0.40 \n\n2 \n\n0.78mW \nInterconnect \nsize \n1k/row \n0.01 \n\n2 \n\n31.04mW \nController \nnumber \n1 \n146.5 2 118.9mW \nDOT-tile \nsize \n128Gb \n0.41 \n\n2 \n\n0.93W \n\nHAM Chip \nnumber \n128 Tiles \n73.52mm 2 137.81W \nsize \n2GB \n\nDOT Chip \nnumber \n128 Tiles \n53.04mm 2 119.79W \nsize \n2GB \n\ndesired threshold (EST check). This sequential process continues \nuntil covering all dimensions. \n\n\n\nTable 2 :\n2Evaluated Genome Sequence DatasetsDetails \nData Type \nSize \nQ.Length \n\nE.Coli \nEscherichia coli [70] \nDNA \n4.6MB \n200 \nHuman \nHuman chromosome 14 [70] \nDNA \n107MB \n200 \nCOVID-19 SRR11092057, SRR11513776 [69] \nDNA \n4.9GB \n200 \nNBCI-Gene \nNCBI Gene data [71] \nProtein \n20 GB \n150 \n1KGene \n1000Genomes data [72] \nProtein \n15GB \n180 \nRefSeq \nReference Sequence [73] \nprotein \n40GB \n140 \n\n\nACKNOWLEDGEMENTSThis work was supported in part by National Science Foundation (NSF) #2127780, Semiconductor Research Corporation (SRC) Task #2988.001, Department of the Navy, Office of Naval Research, grant #N00014-21-1-2225, Air Force Office of Scientific Research, and a generous gift from Cisco.\nEarly transmission dynamics in wuhan, china, of novel coronavirus-infected pneumonia. Q Li, X Guan, P Wu, X Wang, L Zhou, Y Tong, R Ren, K S Leung, E H Lau, J Y Wong, New England Journal of Medicine. Q. Li, X. Guan, P. Wu, X. Wang, L. Zhou, Y. Tong, R. Ren, K. S. Leung, E. H. Lau, J. Y. Wong, et al., \"Early transmission dynamics in wuhan, china, of novel coronavirus-infected pneumonia, \" New England Journal of Medicine, 2020.\n\nWorld health organization declares global emergency: A review of the. C Sohrabi, Z Alsafi, N O&apos;neill, M Khan, A Kerwan, A Al-Jabir, C Iosifidis, R Agha, International Journal of Surgery. novel coronavirus (covid-19C. Sohrabi, Z. Alsafi, N. O'Neill, M. Khan, A. Kerwan, A. Al-Jabir, C. Iosifidis, and R. Agha, \"World health organization declares global emergency: A review of the 2019 novel coronavirus (covid-19), \" International Journal of Surgery, 2020.\n\nClinical characteristics of coronavirus disease 2019 in china. D S Lei, Hui, New England journal of medicine. 38218Lei, D. S. Hui, et al., \"Clinical characteristics of coronavirus disease 2019 in china, \" New England journal of medicine, vol. 382, no. 18, pp. 1708-1720, 2020.\n\nCovid-19: The first documented coronavirus pandemic in history. Y.-C Liu, R.-L Kuo, S.-R Shih, Biomedical journal. 434Y.-C. Liu, R.-L. Kuo, and S.-R. Shih, \"Covid-19: The first documented coronavirus pandemic in history, \" Biomedical journal, vol. 43, no. 4, pp. 328-333, 2020.\n\nNih genome assembly for coronavirus. \"Nih genome assembly for coronavirus. \" https://www.ncbi.nlm.nih.gov/genome/ browse#!/viruses/31360/ https://www.ncbi.nlm.nih.gov/datasets/coronavirus/genomes/.\n\nA genomic perspective on the origin and emergence of sars-cov-2. Y.-Z Zhang, E C Holmes, Cell. Y.-Z. Zhang and E. C. Holmes, \"A genomic perspective on the origin and emer- gence of sars-cov-2, \" Cell, 2020.\n\nA dynamic nomenclature proposal for sars-cov-2 to assist genomic epidemiology. A Rambaut, E C Holmes, V Hill, A Otoole, J Mccrone, C Ruis, L Plessis, O Pybus, bioRxivA. Rambaut, E. C. Holmes, V. Hill, A. OToole, J. McCrone, C. Ruis, L. du Plessis, and O. Pybus, \"A dynamic nomenclature proposal for sars-cov-2 to assist genomic epidemiology, \" bioRxiv, 2020.\n\nA scalable processing-in-memory accelerator for parallel graph processing. J Ahn, S Hong, S Yoo, O Mutlu, K Choi, ACM SIGARCH Computer Architecture News. 433J. Ahn, S. Hong, S. Yoo, O. Mutlu, and K. Choi, \"A scalable processing-in-memory accelerator for parallel graph processing, \" ACM SIGARCH Computer Architecture News, vol. 43, no. 3, pp. 105-117, 2016.\n\nPim-enabled instructions: A low-overhead, locality-aware processing-in-memory architecture. J Ahn, S Yoo, O Mutlu, K Choi, Computer Architecture (ISCA), 2015 ACM/IEEE 42nd Annual International Symposium on. IEEEJ. Ahn, S. Yoo, O. Mutlu, and K. Choi, \"Pim-enabled instructions: A low-overhead, locality-aware processing-in-memory architecture,\" in Computer Architecture (ISCA), 2015 ACM/IEEE 42nd Annual International Symposium on, pp. 336-348, IEEE, 2015.\n\nProcessing in memory: The terasys massively parallel pim array. M Gokhale, B Holmes, K Iobst, Computer. 284M. Gokhale, B. Holmes, and K. Iobst, \"Processing in memory: The terasys mas- sively parallel pim array, \" Computer, vol. 28, no. 4, pp. 23-31, 1995.\n\nTop-pim: throughput-oriented programmable processing in memory. D Zhang, N Jayasena, A Lyashevsky, J L Greathouse, L Xu, M Ignatowski, Proceedings of the 23rd international symposium on High-performance parallel and distributed computing. the 23rd international symposium on High-performance parallel and distributed computingACMD. Zhang, N. Jayasena, A. Lyashevsky, J. L. Greathouse, L. Xu, and M. Ignatowski, \"Top-pim: throughput-oriented programmable processing in memory,\" in Pro- ceedings of the 23rd international symposium on High-performance parallel and distributed computing, pp. 85-98, ACM, 2014.\n\nThe architecture of the diva processing-in-memory chip. J Draper, J Chame, M Hall, C Steele, T Barrett, J Lacoss, J Granacki, J Shin, C Chen, C W Kang, Proceedings of the 16th international conference on Supercomputing. the 16th international conference on SupercomputingACMJ. Draper, J. Chame, M. Hall, C. Steele, T. Barrett, J. LaCoss, J. Granacki, J. Shin, C. Chen, C. W. Kang, et al., \"The architecture of the diva processing-in-memory chip, \" in Proceedings of the 16th international conference on Supercomputing, pp. 14- 25, ACM, 2002.\n\nPipelayer: A pipelined reram-based accelerator for deep learning. L Song, X Qian, H Li, Y Chen, L. Song, X. Qian, H. Li, and Y. Chen, \"Pipelayer: A pipelined reram-based acceler- ator for deep learning, \" HPCA, 2017.\n\nIn-memory data parallel processor. D Fujiki, S Mahlke, R Das, Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems. the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating SystemsACMD. Fujiki, S. Mahlke, and R. Das, \"In-memory data parallel processor,\" in Pro- ceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems, pp. 1-14, ACM, 2018.\n\nParapim: a parallel processing-in-memory accelerator for binary-weight deep neural networks. S Angizi, Z He, D Fan, Proceedings of the 24th Asia and South Pacific Design Automation Conference. the 24th Asia and South Pacific Design Automation ConferenceACMS. Angizi, Z. He, and D. Fan, \"Parapim: a parallel processing-in-memory accelera- tor for binary-weight deep neural networks, \" in Proceedings of the 24th Asia and South Pacific Design Automation Conference, pp. 127-132, ACM, 2019.\n\nPima-logic: a novel processing-in-memory architecture for highly flexible and energy-efficient logic computation. S Angizi, Z He, D Fan, Proceedings of the 55th Annual Design Automation Conference. the 55th Annual Design Automation ConferenceACM162S. Angizi, Z. He, and D. Fan, \"Pima-logic: a novel processing-in-memory archi- tecture for highly flexible and energy-efficient logic computation, \" in Proceedings of the 55th Annual Design Automation Conference, p. 162, ACM, 2018.\n\nLiquid silicon: A nonvolatile fully programmable processing-in-memory processor with monolithically integrated reram. Y Zha, E Nowak, J Li, IEEE Journal of Solid-State Circuits. 554Y. Zha, E. Nowak, and J. Li, \"Liquid silicon: A nonvolatile fully programmable processing-in-memory processor with monolithically integrated reram,\" IEEE Journal of Solid-State Circuits, vol. 55, no. 4, pp. 908-919, 2020.\n\nImpala: Algorithm/architecture co-design for in-memory multi-stride pattern matching. E Sadredini, R Rahimi, M Lenjani, M Stan, K Skadron, 2020 IEEE international symposium on high performance computer architecture (HPCA). IEEEE. Sadredini, R. Rahimi, M. Lenjani, M. Stan, and K. Skadron, \"Impala: Algo- rithm/architecture co-design for in-memory multi-stride pattern matching,\" in 2020 IEEE international symposium on high performance computer architecture (HPCA), pp. 86-98, IEEE, 2020.\n\neap: A scalable and efficient in-memory accelerator for automata processing. E Sadredini, R Rahimi, V Verma, M Stan, K Skadron, Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture. the 52nd Annual IEEE/ACM International Symposium on MicroarchitectureE. Sadredini, R. Rahimi, V. Verma, M. Stan, and K. Skadron, \"eap: A scalable and efficient in-memory accelerator for automata processing,\" in Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture, pp. 87-99, 2019.\n\nPrime: A novel processing-in-memory architecture for neural network computation in reram-based main memory. P Chi, S Li, C Xu, T Zhang, J Zhao, Y Liu, Y Wang, Y Xie, Proceedings of the 43rd International Symposium on Computer Architecture. the 43rd International Symposium on Computer ArchitectureIEEE PressP. Chi, S. Li, C. Xu, T. Zhang, J. Zhao, Y. Liu, Y. Wang, and Y. Xie, \"Prime: A novel processing-in-memory architecture for neural network computation in reram-based main memory, \" in Proceedings of the 43rd International Symposium on Computer Architecture, pp. 27-39, IEEE Press, 2016.\n\nIsaac: A convolutional neural network accelerator with in-situ analog arithmetic in crossbars. A Shafiee, A Nag, N Muralimanohar, R Balasubramonian, J P Strachan, M Hu, R S Williams, V Srikumar, Proceedings of the 43rd International Symposium on Computer Architecture. the 43rd International Symposium on Computer ArchitectureIEEE PressA. Shafiee, A. Nag, N. Muralimanohar, R. Balasubramonian, J. P. Strachan, M. Hu, R. S. Williams, and V. Srikumar, \"Isaac: A convolutional neural network accel- erator with in-situ analog arithmetic in crossbars,\" in Proceedings of the 43rd International Symposium on Computer Architecture, pp. 14-26, IEEE Press, 2016.\n\nFloatpim: In-memory acceleration of deep neural network training with high precision. M Imani, S Gupta, Y Kim, T Rosing, Proceedings of the 46th International Symposium on Computer Architecture. the 46th International Symposium on Computer ArchitectureACMM. Imani, S. Gupta, Y. Kim, and T. Rosing, \"Floatpim: In-memory acceleration of deep neural network training with high precision, \" in Proceedings of the 46th International Symposium on Computer Architecture, pp. 802-815, ACM, 2019.\n\nSearching for potential grna offtarget sites for crispr/cas9 using automata processing across different platforms. C Bo, V Dang, E Sadredini, K Skadron, 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEEC. Bo, V. Dang, E. Sadredini, and K. Skadron, \"Searching for potential grna off- target sites for crispr/cas9 using automata processing across different platforms, \" in 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), pp. 737-748, IEEE, 2018.\n\nRe-nuca: A practical nuca architecture for reram based last-level caches. J B Kotra, M Arjomand, D Guttman, M T Kandemir, C R Das, 2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS). IEEEJ. B. Kotra, M. Arjomand, D. Guttman, M. T. Kandemir, and C. R. Das, \"Re-nuca: A practical nuca architecture for reram based last-level caches,\" in 2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS), pp. 576-585, IEEE, 2016.\n\nMemristors for neural branch prediction: a case study in strict latency and write endurance challenges. H Saadeldeen, D Franklin, G Long, C Hill, A Browne, D Strukov, T Sherwood, F T Chong, Proceedings of the ACM International Conference on Computing Frontiers. the ACM International Conference on Computing FrontiersH. Saadeldeen, D. Franklin, G. Long, C. Hill, A. Browne, D. Strukov, T. Sherwood, and F. T. Chong, \"Memristors for neural branch prediction: a case study in strict latency and write endurance challenges, \" in Proceedings of the ACM International Conference on Computing Frontiers, pp. 1-10, 2013.\n\nHybrid cmos/nanoelectronic circuits: Opportunities and challenges. K K Likharev, Journal of Nanoelectronics and Optoelectronics. 33K. K. Likharev, \"Hybrid cmos/nanoelectronic circuits: Opportunities and chal- lenges, \" Journal of Nanoelectronics and Optoelectronics, vol. 3, no. 3, pp. 203-230, 2008.\n\nThe blast sequence analysis tool. T Madden, The NCBI Handbook. Internet. 2nd editionT. Madden, \"The blast sequence analysis tool, \" in The NCBI Handbook [Internet]. 2nd edition, National Center for Biotechnology Information (US), 2013.\n\nMinimap2: pairwise alignment for nucleotide sequences. H Li, Bioinformatics. 3418H. Li, \"Minimap2: pairwise alignment for nucleotide sequences, \" Bioinformatics, vol. 34, no. 18, pp. 3094-3100, 2018.\n\nImproved metagenomic analysis with kraken 2. D E Wood, J Lu, B Langmead, Genome biology. 201D. E. Wood, J. Lu, and B. Langmead, \"Improved metagenomic analysis with kraken 2, \" Genome biology, vol. 20, no. 1, pp. 1-13, 2019.\n\nSequencing human genome: the contributions of francis collins and craig venter. J Adams, Nature Education. 11133J. Adams, \"Sequencing human genome: the contributions of francis collins and craig venter, \" Nature Education, vol. 1, no. 1, p. 133, 2008.\n\nShotgun sequencing of the human genome. J C Venter, M D Adams, G G Sutton, A R Kerlavage, H O Smith, M Hunkapiller, J. C. Venter, M. D. Adams, G. G. Sutton, A. R. Kerlavage, H. O. Smith, and M. Hunkapiller, \"Shotgun sequencing of the human genome, \" 1998.\n\nAccelerating read mapping with fasthash. H Xin, D Lee, F Hormozdiari, S Yedkar, O Mutlu, C Alkan, BMC genomics. 14SpringerH. Xin, D. Lee, F. Hormozdiari, S. Yedkar, O. Mutlu, and C. Alkan, \"Accelerating read mapping with fasthash, \" in BMC genomics, vol. 14, pp. 1-13, Springer, 2013.\n\nDarwin-wga: A coprocessor provides increased sensitivity in whole genome alignments with high speedup. Y Turakhia, S D Goenka, G Bejerano, W J Dally, 2019 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEEY. Turakhia, S. D. Goenka, G. Bejerano, and W. J. Dally, \"Darwin-wga: A co- processor provides increased sensitivity in whole genome alignments with high speedup, \" in 2019 IEEE International Symposium on High Performance Computer Architecture (HPCA), pp. 359-372, IEEE, 2019.\n\nGenasm: A high-performance, low-power approximate string matching acceleration framework for genome sequence analysis. D S Cali, G S Kalsi, Z Bing\u00f6l, C Firtina, L Subramanian, J S Kim, R Ausavarungnirun, M Alser, J Gomez-Luna, A Boroumand, 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEED. S. Cali, G. S. Kalsi, Z. Bing\u00f6l, C. Firtina, L. Subramanian, J. S. Kim, R. Ausavarungnirun, M. Alser, J. Gomez-Luna, A. Boroumand, et al., \"Genasm: A high-performance, low-power approximate string matching acceleration frame- work for genome sequence analysis, \" in 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pp. 951-966, IEEE, 2020.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cognitive Computation. 12P. Kanerva, \"Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors,\" Cognitive Computation, vol. 1, no. 2, pp. 139-159, 2009.\n\nGraphd: Graphbased hyperdimensional memorization for brain-like cognitive learning. P Poduval, A Zakeri, F Imani, H Alimohamadi, M Imani, Frontiers in Neuroscience. 5P. Poduval, A. Zakeri, F. Imani, H. Alimohamadi, and M. Imani, \"Graphd: Graph- based hyperdimensional memorization for brain-like cognitive learning, \" Frontiers in Neuroscience, p. 5, 2022.\n\nExploring hyperdimensional associative memory. M Imani, A Rahimi, D Kong, T Rosing, J M Rabaey, High Performance Computer Architecture (HPCA. IEEE2017 IEEE International Symposium onM. Imani, A. Rahimi, D. Kong, T. Rosing, and J. M. Rabaey, \"Exploring hyper- dimensional associative memory,\" in High Performance Computer Architecture (HPCA), 2017 IEEE International Symposium on, pp. 445-456, IEEE, 2017.\n\nGeniehd: efficient dna pattern matching accelerator using hyperdimensional computing. Y Kim, M Imani, N Moshiri, T Rosing, 2020 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEEY. Kim, M. Imani, N. Moshiri, and T. Rosing, \"Geniehd: efficient dna pattern matching accelerator using hyperdimensional computing,\" in 2020 Design, Au- tomation & Test in Europe Conference & Exhibition (DATE), pp. 115-120, IEEE, 2020.\n\nRapid: A reram processing in-memory architecture for dna sequence alignment. S Gupta, M Imani, B Khaleghi, V Kumar, T Rosing, 2019 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED). IEEES. Gupta, M. Imani, B. Khaleghi, V. Kumar, and T. Rosing, \"Rapid: A reram pro- cessing in-memory architecture for dna sequence alignment, \" in 2019 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED), pp. 1-6, IEEE, 2019.\n\nAltschul, sf, gish, w., miller, w., myers, ew & lipman, dj (1990). D Baulcombe, H B\u00e4umlein, U Wobus, J Pustell, F Kafatos, A Bendich, P Benfey, H Takatsuji, L Ren, D Shah, J. Mol. Biol. 215D. Baulcombe, H. B\u00e4umlein, U. Wobus, J. Pustell, F. Kafatos, A. Bendich, P. Benfey, H. Takatsuji, L. Ren, D. Shah, et al., \"Altschul, sf, gish, w., miller, w., myers, ew & lipman, dj (1990), \" J. Mol. Biol, vol. 215, pp. 403-410, 1998.\n\nPairwise alignment of protein interaction networks. M Koyut\u00fcrk, Y Kim, U Topkara, S Subramaniam, W Szpankowski, A Grama, Journal of Computational Biology. 132M. Koyut\u00fcrk, Y. Kim, U. Topkara, S. Subramaniam, W. Szpankowski, and A. Grama, \"Pairwise alignment of protein interaction networks, \" Journal of Computational Biology, vol. 13, no. 2, pp. 182-199, 2006.\n\nHardware acceleration of long read pairwise overlapping in genome sequencing: A race between fpga and gpu. L Guo, J Lau, Z Ruan, P Wei, J Cong, 2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM). IEEEL. Guo, J. Lau, Z. Ruan, P. Wei, and J. Cong, \"Hardware acceleration of long read pairwise overlapping in genome sequencing: A race between fpga and gpu,\" in 2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM), pp. 127-135, IEEE, 2019.\n\nAccelerating smith-waterman alignment of long dna sequences with opencl on fpga. E Rucci, C Garcia, G Botella, A De Giusti, M Naiouf, M Prieto-Matias, International Conference on Bioinformatics and Biomedical Engineering. SpringerE. Rucci, C. Garcia, G. Botella, A. De Giusti, M. Naiouf, and M. Prieto-Matias, \"Accelerating smith-waterman alignment of long dna sequences with opencl on fpga, \" in International Conference on Bioinformatics and Biomedical Engineering, pp. 500-511, Springer, 2017.\n\nLearning sensorimotor control with neuromorphic sensors: Toward hyperdimensional active perception. A Mitrokhin, P Sutor, C Ferm\u00fcller, Y Aloimonos, Science Robotics. 430A. Mitrokhin, P. Sutor, C. Ferm\u00fcller, and Y. Aloimonos, \"Learning sensorimotor control with neuromorphic sensors: Toward hyperdimensional active perception, \" Science Robotics, vol. 4, no. 30, 2019.\n\nA robust and energy-efficient classifier using brain-inspired hyperdimensional computing. A Rahimi, P Kanerva, J M Rabaey, Proceedings of the 2016 International Symposium on Low Power Electronics and Design. the 2016 International Symposium on Low Power Electronics and DesignA. Rahimi, P. Kanerva, and J. M. Rabaey, \"A robust and energy-efficient classifier using brain-inspired hyperdimensional computing,\" in Proceedings of the 2016 International Symposium on Low Power Electronics and Design, pp. 64-69, 2016.\n\nScalable edge-based hyperdimensional learning system with brain-like neural adaptation. Z Zou, Y Kim, F Imani, H Alimohamadi, R Cammarota, M Imani, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. the International Conference for High Performance Computing, Networking, Storage and AnalysisZ. Zou, Y. Kim, F. Imani, H. Alimohamadi, R. Cammarota, and M. Imani, \"Scalable edge-based hyperdimensional learning system with brain-like neural adaptation, \" in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1-15, 2021.\n\nSparseness and expansion in sensory representations. B Babadi, H Sompolinsky, Neuron. 835B. Babadi and H. Sompolinsky, \"Sparseness and expansion in sensory representa- tions, \" Neuron, vol. 83, no. 5, pp. 1213-1226, 2014.\n\nRandom indexing of text samples for latent semantic analysis. P Kanerva, J Kristofersson, A Holst, Proceedings of the 22nd annual conference of the cognitive science society. the 22nd annual conference of the cognitive science societyCiteseer1036P. Kanerva, J. Kristofersson, and A. Holst, \"Random indexing of text samples for latent semantic analysis, \" in Proceedings of the 22nd annual conference of the cognitive science society, vol. 1036, Citeseer, 2000.\n\nSpiking hyperdimensional network: Neuromorphic models integrated with memory-inspired framework. Z Zou, H Alimohamadi, F Imani, Y Kim, M Imani, arXiv:2110.00214arXiv preprintZ. Zou, H. Alimohamadi, F. Imani, Y. Kim, and M. Imani, \"Spiking hyperdi- mensional network: Neuromorphic models integrated with memory-inspired framework, \" arXiv preprint arXiv:2110.00214, 2021.\n\nContent-addressable memory (cam) circuits and architectures: A tutorial and survey. K Pagiamtzis, A Sheikholeslami, IEEE journal of solid-state circuits. 413K. Pagiamtzis and A. Sheikholeslami, \"Content-addressable memory (cam) circuits and architectures: A tutorial and survey, \" IEEE journal of solid-state circuits, vol. 41, no. 3, pp. 712-727, 2006.\n\nFerroelectric ternary content-addressable memory for one-shot learning. K Ni, X Yin, A F Laguna, S Joshi, S D\u00fcnkel, M Trentzsch, J M\u00fceller, S Beyer, M Niemier, X S Hu, Nature Electronics. 211K. Ni, X. Yin, A. F. Laguna, S. Joshi, S. D\u00fcnkel, M. Trentzsch, J. M\u00fceller, S. Beyer, M. Niemier, X. S. Hu, et al., \"Ferroelectric ternary content-addressable memory for one-shot learning, \" Nature Electronics, vol. 2, no. 11, pp. 521-529, 2019.\n\nMixed digitalanalog associative memory enabling fully-parallel nearest euclidean distance search. M A Abedin, Y Tanaka, A Ahmadi, T Koide, H J Mattausch, Japanese journal of applied physics. 464S2231M. A. Abedin, Y. Tanaka, A. Ahmadi, T. Koide, and H. J. Mattausch, \"Mixed digital- analog associative memory enabling fully-parallel nearest euclidean distance search, \" Japanese journal of applied physics, vol. 46, no. 4S, p. 2231, 2007.\n\nDigitalpim: Digital-based processing in-memory for big data acceleration. M Imani, S Gupta, Y Kim, M Zhou, T Rosing, Proceedings of the 2019 on Great Lakes Symposium on VLSI. the 2019 on Great Lakes Symposium on VLSIM. Imani, S. Gupta, Y. Kim, M. Zhou, and T. Rosing, \"Digitalpim: Digital-based processing in-memory for big data acceleration,\" in Proceedings of the 2019 on Great Lakes Symposium on VLSI, pp. 429-434, 2019.\n\nMagic-memristor-aided logic. S Kvatinsky, D Belousov, S Liman, G Satat, N Wald, E G Friedman, A Kolodny, U C Weiser, IEEE Transactions on Circuits and Systems II: Express Briefs. 6111S. Kvatinsky, D. Belousov, S. Liman, G. Satat, N. Wald, E. G. Friedman, A. Kolodny, and U. C. Weiser, \"Magic-memristor-aided logic, \" IEEE Transactions on Circuits and Systems II: Express Briefs, vol. 61, no. 11, pp. 895-899, 2014.\n\nSimpler magic: synthesis and mapping of in-memory logic executed in a single row to improve throughput. R Ben-Hur, R Ronen, A Haj-Ali, D Bhattacharjee, A Eliahu, N Peled, S Kvatinsky, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. R. Ben-Hur, R. Ronen, A. Haj-Ali, D. Bhattacharjee, A. Eliahu, N. Peled, and S. Kvatinsky, \"Simpler magic: synthesis and mapping of in-memory logic executed in a single row to improve throughput,\" IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2019.\n\nA complementary resistive switchbased crossbar array adder. A Siemon, S Menzel, R Waser, E Linn, IEEE journal on emerging and selected topics in circuits and systems. 51A. Siemon, S. Menzel, R. Waser, and E. Linn, \"A complementary resistive switch- based crossbar array adder, \" IEEE journal on emerging and selected topics in circuits and systems, vol. 5, no. 1, pp. 64-74, 2015.\n\nMemristor-based material implication (IMPLY) logic: design principles and methodologies. S Kvatinsky, G Satat, N Wald, E G Friedman, A Kolodny, U C Weiser, TVLSI. 2210S. Kvatinsky, G. Satat, N. Wald, E. G. Friedman, A. Kolodny, and U. C. Weiser, \"Memristor-based material implication (IMPLY) logic: design principles and methodologies, \" TVLSI, vol. 22, no. 10, pp. 2054-2066, 2014.\n\nMemristive switches enable stateful logic operations via material implication. J Borghetti, G S Snider, P J Kuekes, J J Yang, D R Stewart, R S Williams, Nature. 4647290J. Borghetti, G. S. Snider, P. J. Kuekes, J. J. Yang, D. R. Stewart, and R. S. Williams, \"Memristive switches enable stateful logic operations via material implication, \" Nature, vol. 464, no. 7290, pp. 873-876, 2010.\n\nLogic design within memristive memories using memristor-aided logic (magic). N Talati, S Gupta, P Mane, S Kvatinsky, IEEE Transactions on Nanotechnology. 154N. Talati, S. Gupta, P. Mane, and S. Kvatinsky, \"Logic design within memristive memories using memristor-aided logic (magic), \" IEEE Transactions on Nanotech- nology, vol. 15, no. 4, pp. 635-650, 2016.\n\nOvercoming the challenges of crossbar resistive memory architectures. C Xu, D Niu, N Muralimanohar, R Balasubramonian, T Zhang, S Yu, Y Xie, 2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA). IEEEC. Xu, D. Niu, N. Muralimanohar, R. Balasubramonian, T. Zhang, S. Yu, and Y. Xie, \"Overcoming the challenges of crossbar resistive memory architectures, \" in 2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA), pp. 476-488, IEEE, 2015.\n\nNewton: Gravitating towards the physical limits of crossbar acceleration. A Nag, R Balasubramonian, V Srikumar, R Walker, A Shafiee, J P Strachan, N Muralimanohar, IEEE Micro. 385A. Nag, R. Balasubramonian, V. Srikumar, R. Walker, A. Shafiee, J. P. Strachan, and N. Muralimanohar, \"Newton: Gravitating towards the physical limits of crossbar acceleration, \" IEEE Micro, vol. 38, no. 5, pp. 41-49, 2018.\n\nOn the total number of genes and their length distribution in complete microbial genomes. M Skovgaard, L J Jensen, S Brunak, D Ussery, A Krogh, TRENDS in Genetics. 178M. Skovgaard, L. J. Jensen, S. Brunak, D. Ussery, and A. Krogh, \"On the total number of genes and their length distribution in complete microbial genomes, \" TRENDS in Genetics, vol. 17, no. 8, pp. 425-428, 2001.\n\nHiv-1 nucleotide sequence comprehensive analysis: a computational approach. J Kasprzykowski, K Ferreira Fukutani, H F\u00e1bio, A Maria Prado Barral, A Trancoso Lopo De Queiroz, Current Bioinformatics. 124J. Irahe Kasprzykowski, K. Ferreira Fukutani, H. F\u00e1bio, A. Maria Prado Barral, and A. Trancoso Lopo de Queiroz, \"Hiv-1 nucleotide sequence comprehensive analysis: a computational approach, \" Current Bioinformatics, vol. 12, no. 4, pp. 303- 311, 2017.\n\nTensorflow: Large-scale machine learning on heterogeneous distributed systems. M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen, C Citro, G S Corrado, A Davis, J Dean, M Devin, arXiv:1603.04467arXiv preprintM. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al., \"Tensorflow: Large-scale machine learning on heterogeneous distributed systems, \" arXiv preprint arXiv:1603.04467, 2016.\n\nSynopsys. D Compiler, R User, M Guide, D. Compiler, R. User, and M. Guide, \"Synopsys,\" Inc., see http://www. synopsys. com, 2000.\n\nVteam: A general model for voltage-controlled memristors. S Kvatinsky, M Ramadan, E G Friedman, A Kolodny, IEEE Transactions on Circuits and Systems II: Express Briefs. 628S. Kvatinsky, M. Ramadan, E. G. Friedman, and A. Kolodny, \"Vteam: A general model for voltage-controlled memristors,\" IEEE Transactions on Circuits and Systems II: Express Briefs, vol. 62, no. 8, pp. 786-790, 2015.\n\nNVBIO: GPU-accelerated C++ framework for High-Throughput Sequence Analysis. \"NVBIO: GPU-accelerated C++ framework for High-Throughput Sequence Anal- ysis. \" https://developer.nvidia.com/nvbio.\n\nGpu-blast: using graphics processors to accelerate protein sequence alignment. P D Vouzis, N V Sahinidis, Bioinformatics. 272P. D. Vouzis and N. V. Sahinidis, \"Gpu-blast: using graphics processors to accel- erate protein sequence alignment,\" Bioinformatics, vol. 27, no. 2, pp. 182-188, 2011.\n\nNIH SARS-COV-2 Data. \"NIH SARS-COV-2 Data.\" https://www.ncbi.nlm.nih.gov/datasets/docs/ command-line-virus/.\n\nNIH National Library of Medicine. \"NIH National Library of Medicine. \" https://www.ncbi.nlm.nih.gov/sars-cov-2/.\n\nNCBI Gene Database. \"NCBI Gene Database. \" https://www.ncbi.nlm.nih.gov/gene/.\n\n1000 Genomes. \"1000 Genomes. \" https://www.internationalgenome.org/data/.\n\nNCBI Reference Sequence. \"NCBI Reference Sequence. \" https://www.ncbi.nlm.nih.gov/refseq/.\n\nFast gapped-read alignment with bowtie 2. B Langmead, S L Salzberg, Nature methods. 94357B. Langmead and S. L. Salzberg, \"Fast gapped-read alignment with bowtie 2,\" Nature methods, vol. 9, no. 4, p. 357, 2012.\n\nPim-aligner: a processing-in-mram platform for biological sequence alignment. S Angizi, J Sun, W Zhang, D Fan, 2020 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEES. Angizi, J. Sun, W. Zhang, and D. Fan, \"Pim-aligner: a processing-in-mram platform for biological sequence alignment, \" in 2020 Design, Automation & Test in Europe Conference & Exhibition (DATE), pp. 1265-1270, IEEE, 2020.\n\nAmbit: In-memory accelerator for bulk bitwise operations using commodity dram technology. V Seshadri, D Lee, T Mullins, H Hassan, A Boroumand, J Kim, M A Kozuch, O Mutlu, P B Gibbons, T C Mowry, 2017 50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEEV. Seshadri, D. Lee, T. Mullins, H. Hassan, A. Boroumand, J. Kim, M. A. Kozuch, O. Mutlu, P. B. Gibbons, and T. C. Mowry, \"Ambit: In-memory accelerator for bulk bitwise operations using commodity dram technology, \" in 2017 50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pp. 273-287, IEEE, 2017.\n\nComputedram: In-memory compute using off-the-shelf drams. F Gao, G Tziantzioulis, D Wentzlaff, Proceedings of the 52nd annual IEEE/ACM international symposium on microarchitecture. the 52nd annual IEEE/ACM international symposium on microarchitectureF. Gao, G. Tziantzioulis, and D. Wentzlaff, \"Computedram: In-memory compute using off-the-shelf drams, \" in Proceedings of the 52nd annual IEEE/ACM interna- tional symposium on microarchitecture, pp. 100-113, 2019.\n\nNewton: A dram-maker's accelerator-in-memory (aim) architecture for machine learning. M He, C Song, I Kim, C Jeong, S Kim, I Park, M Thottethodi, T Vijaykumar, 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEEM. He, C. Song, I. Kim, C. Jeong, S. Kim, I. Park, M. Thottethodi, and T. Vijayku- mar, \"Newton: A dram-maker's accelerator-in-memory (aim) architecture for machine learning,\" in 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pp. 372-385, IEEE, 2020.\n\nMonolithic 3d integration of high endurance multi-bit ferroelectric fet for accelerating compute-in-memory. S Dutta, H Ye, W Chakraborty, Y.-C Luo, M Jose, B Grisafe, A Khanna, I Lightcap, S Shinde, S Yu, 2020 IEEE International Electron Devices Meeting (IEDM). IEEES. Dutta, H. Ye, W. Chakraborty, Y.-C. Luo, M. San Jose, B. Grisafe, A. Khanna, I. Lightcap, S. Shinde, S. Yu, et al., \"Monolithic 3d integration of high endurance multi-bit ferroelectric fet for accelerating compute-in-memory,\" in 2020 IEEE International Electron Devices Meeting (IEDM), pp. 36-4, IEEE, 2020.\n\nA 22nm 4mb 8b-precision reram computingin-memory macro with 11.91 to 195.7 tops/w for tiny ai edge devices. C.-X Xue, J.-M Hung, H.-Y Kao, Y.-H Huang, S.-P Huang, F.-C Chang, P Chen, T.-W Liu, C.-J Jhang, C.-I Su, 2021 IEEE International Solid-State Circuits Conference (ISSCC). IEEE642021C.-X. Xue, J.-M. Hung, H.-Y. Kao, Y.-H. Huang, S.-P. Huang, F.-C. Chang, P. Chen, T.-W. Liu, C.-J. Jhang, C.-I. Su, et al., \"A 22nm 4mb 8b-precision reram computing- in-memory macro with 11.91 to 195.7 tops/w for tiny ai edge devices,\" in 2021 IEEE International Solid-State Circuits Conference (ISSCC), vol. 64, pp. 245-247, IEEE, 2021.\n\nA framework for collaborative learning in secure high-dimensional space. M Imani, Y Kim, S Riazi, J Messerly, P Liu, F Koushanfar, T Rosing, 2019 IEEE 12th International Conference on Cloud Computing (CLOUD). IEEEM. Imani, Y. Kim, S. Riazi, J. Messerly, P. Liu, F. Koushanfar, and T. Rosing, \"A framework for collaborative learning in secure high-dimensional space, \" in 2019 IEEE 12th International Conference on Cloud Computing (CLOUD), pp. 435-446, IEEE, 2019.\n\nClassification using hyperdimensional computing: A review. L Ge, K K Parhi, IEEE Circuits and Systems Magazine. 202L. Ge and K. K. Parhi, \"Classification using hyperdimensional computing: A review, \" IEEE Circuits and Systems Magazine, vol. 20, no. 2, pp. 30-47, 2020.\n\nBiological gender classification from fmri via hyperdimensional computing. R Billmeyer, K K Parhi, 2021 55th Asilomar Conference on Signals, Systems, and Computers. IEEE2021R. Billmeyer and K. K. Parhi, \"Biological gender classification from fmri via hy- perdimensional computing, \" in 2021 55th Asilomar Conference on Signals, Systems, and Computers, pp. 578-582, IEEE, 2021.\n\nPrid: Model inversion privacy attacks in hyperdimensional learning systems. A H\u00e9rnandez-Cano, R Cammarota, M Imani, 2021 58th ACM/IEEE Design Automation Conference (DAC). IEEE2021A. H\u00e9rnandez-Cano, R. Cammarota, and M. Imani, \"Prid: Model inversion privacy attacks in hyperdimensional learning systems,\" in 2021 58th ACM/IEEE Design Automation Conference (DAC), pp. 553-558, IEEE, 2021.\n\nVector symbolic architectures as a computing framework for nanoscale hardware. D Kleyko, M Davies, E P Frady, P Kanerva, S J Kent, B A Olshausen, E Osipov, J M Rabaey, D A Rachkovskij, A Rahimi, arXiv:2106.05268arXiv preprintD. Kleyko, M. Davies, E. P. Frady, P. Kanerva, S. J. Kent, B. A. Olshausen, E. Osipov, J. M. Rabaey, D. A. Rachkovskij, A. Rahimi, et al., \"Vector symbolic architectures as a computing framework for nanoscale hardware, \" arXiv preprint arXiv:2106.05268, 2021.\n\nE P Frady, D Kleyko, C J Kymn, B A Olshausen, F T Sommer, arXiv:2109.03429Computing on functions using randomized vector representations. arXiv preprintE. P. Frady, D. Kleyko, C. J. Kymn, B. A. Olshausen, and F. T. Sommer, \"Com- puting on functions using randomized vector representations,\" arXiv preprint arXiv:2109.03429, 2021.\n\nStochd: Stochastic hyperdimensional system for efficient and robust learning from raw data. P Poduval, Z Zou, H Najafi, H Homayoun, M Imani, 2021 58th ACM/IEEE Design Automation Conference (DAC). IEEE2021P. Poduval, Z. Zou, H. Najafi, H. Homayoun, and M. Imani, \"Stochd: Stochastic hyperdimensional system for efficient and robust learning from raw data,\" in 2021 58th ACM/IEEE Design Automation Conference (DAC), pp. 1195-1200, IEEE, 2021.\n\nRrambased cam combined with time-domain circuits for hyperdimensional computing. Y Halawani, D Kilani, E Hassan, H Tesfai, H Saleh, B Mohammad, Scientific reports. 111Y. Halawani, D. Kilani, E. Hassan, H. Tesfai, H. Saleh, and B. Mohammad, \"Rram- based cam combined with time-domain circuits for hyperdimensional computing, \" Scientific reports, vol. 11, no. 1, pp. 1-11, 2021.\n\nCognitive correlative encoding for genome sequence matching in hyperdimensional system. P Poduval, Z Zou, X Yin, E Sadredini, M Imani, 2021 58th ACM/IEEE Design Automation Conference (DAC). IEEE2021P. Poduval, Z. Zou, X. Yin, E. Sadredini, and M. Imani, \"Cognitive correlative encoding for genome sequence matching in hyperdimensional system, \" in 2021 58th ACM/IEEE Design Automation Conference (DAC), pp. 781-786, IEEE, 2021.\n\nDima: a depthwise cnn in-memory accelerator. S Angizi, Z He, D Fan, 2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD). IEEES. Angizi, Z. He, and D. Fan, \"Dima: a depthwise cnn in-memory accelerator,\" in 2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD), pp. 1-8, IEEE, 2018.\n\nGraphr: Accelerating graph processing using reram. L Song, Y Zhuo, X Qian, H Li, Y Chen, 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEEL. Song, Y. Zhuo, X. Qian, H. Li, and Y. Chen, \"Graphr: Accelerating graph pro- cessing using reram, \" in 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), pp. 531-543, IEEE, 2018.\n\nExma: A genomics accelerator for exact-matching. L Jiang, F Zokaee, 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE2021L. Jiang and F. Zokaee, \"Exma: A genomics accelerator for exact-matching,\" in 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA), pp. 399-411, IEEE, 2021.\n\nGatekeeper: a new hardware architecture for accelerating pre-alignment in dna short read mapping. M Alser, H Hassan, H Xin, O Ergin, O Mutlu, C Alkan, Bioinformatics. 3321M. Alser, H. Hassan, H. Xin, O. Ergin, O. Mutlu, and C. Alkan, \"Gatekeeper: a new hardware architecture for accelerating pre-alignment in dna short read mapping, \" Bioinformatics, vol. 33, no. 21, pp. 3355-3363, 2017.\n", "annotations": {"author": "[{\"end\":154,\"start\":108},{\"end\":202,\"start\":155},{\"end\":251,\"start\":203},{\"end\":344,\"start\":252},{\"end\":399,\"start\":345},{\"end\":431,\"start\":400},{\"end\":479,\"start\":432},{\"end\":526,\"start\":480},{\"end\":574,\"start\":527},{\"end\":623,\"start\":575},{\"end\":716,\"start\":624},{\"end\":771,\"start\":717},{\"end\":803,\"start\":772},{\"end\":811,\"start\":804}]", "publisher": null, "author_last_name": "[{\"end\":119,\"start\":116},{\"end\":167,\"start\":163},{\"end\":220,\"start\":213},{\"end\":263,\"start\":260},{\"end\":361,\"start\":352},{\"end\":417,\"start\":408},{\"end\":444,\"start\":439},{\"end\":491,\"start\":488},{\"end\":539,\"start\":535},{\"end\":592,\"start\":585},{\"end\":635,\"start\":632},{\"end\":733,\"start\":724},{\"end\":789,\"start\":780}]", "author_first_name": "[{\"end\":115,\"start\":108},{\"end\":162,\"start\":155},{\"end\":212,\"start\":203},{\"end\":259,\"start\":252},{\"end\":351,\"start\":345},{\"end\":407,\"start\":400},{\"end\":438,\"start\":432},{\"end\":487,\"start\":480},{\"end\":534,\"start\":527},{\"end\":584,\"start\":575},{\"end\":631,\"start\":624},{\"end\":723,\"start\":717},{\"end\":779,\"start\":772},{\"end\":810,\"start\":804}]", "author_affiliation": "[{\"end\":153,\"start\":121},{\"end\":201,\"start\":169},{\"end\":250,\"start\":222},{\"end\":317,\"start\":265},{\"end\":343,\"start\":319},{\"end\":398,\"start\":363},{\"end\":430,\"start\":419},{\"end\":478,\"start\":446},{\"end\":525,\"start\":493},{\"end\":573,\"start\":541},{\"end\":622,\"start\":594},{\"end\":689,\"start\":637},{\"end\":715,\"start\":691},{\"end\":770,\"start\":735},{\"end\":802,\"start\":791}]", "title": "[{\"end\":89,\"start\":1},{\"end\":900,\"start\":812}]", "venue": "[{\"end\":977,\"start\":902}]", "abstract": "[{\"end\":2480,\"start\":1101}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2660,\"start\":2657},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2663,\"start\":2660},{\"end\":2666,\"start\":2663},{\"end\":3357,\"start\":3336},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3432,\"start\":3429},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3604,\"start\":3601},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3607,\"start\":3604},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3610,\"start\":3607},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4105,\"start\":4102},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4108,\"start\":4105},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4112,\"start\":4108},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4116,\"start\":4112},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4120,\"start\":4116},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4124,\"start\":4120},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4128,\"start\":4124},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4132,\"start\":4128},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4136,\"start\":4132},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4140,\"start\":4136},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4144,\"start\":4140},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4148,\"start\":4144},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4272,\"start\":4268},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4275,\"start\":4272},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4667,\"start\":4663},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4670,\"start\":4667},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4721,\"start\":4717},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4724,\"start\":4721},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4727,\"start\":4724},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4904,\"start\":4900},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5288,\"start\":5284},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5292,\"start\":5288},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5296,\"start\":5292},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5733,\"start\":5729},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5737,\"start\":5733},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5741,\"start\":5737},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5745,\"start\":5741},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5749,\"start\":5745},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5753,\"start\":5749},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5757,\"start\":5753},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5761,\"start\":5757},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6047,\"start\":6043},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6051,\"start\":6047},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6055,\"start\":6051},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6059,\"start\":6055},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8561,\"start\":8557},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9098,\"start\":9094},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9102,\"start\":9098},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9106,\"start\":9102},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9110,\"start\":9106},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9114,\"start\":9110},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9118,\"start\":9114},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9122,\"start\":9118},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9126,\"start\":9122},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10501,\"start\":10497},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10504,\"start\":10501},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10508,\"start\":10504},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10512,\"start\":10508},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10516,\"start\":10512},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10679,\"start\":10675},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":10683,\"start\":10679},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10687,\"start\":10683},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":24030,\"start\":24026},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":24958,\"start\":24954},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":24961,\"start\":24958},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":24964,\"start\":24961},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27189,\"start\":27185},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":27193,\"start\":27189},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":27197,\"start\":27193},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":27201,\"start\":27197},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":27205,\"start\":27201},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":27209,\"start\":27205},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":27213,\"start\":27209},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27313,\"start\":27309},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":27316,\"start\":27313},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":27653,\"start\":27649},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27677,\"start\":27673},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":28476,\"start\":28472},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":28479,\"start\":28476},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31902,\"start\":31901},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":40792,\"start\":40788},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":40795,\"start\":40792},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":41518,\"start\":41514},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":41848,\"start\":41844},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":41975,\"start\":41971},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":42396,\"start\":42392},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":42567,\"start\":42563},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":42570,\"start\":42567},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":43215,\"start\":43214},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":43642,\"start\":43638},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":43736,\"start\":43732},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":43997,\"start\":43993},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":50136,\"start\":50132},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":52078,\"start\":52074},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":52096,\"start\":52092},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":52729,\"start\":52725},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":52744,\"start\":52740},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":53053,\"start\":53049},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":53071,\"start\":53067},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":53088,\"start\":53084},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":54807,\"start\":54803},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":54810,\"start\":54807},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":55856,\"start\":55852},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":56499,\"start\":56496},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":58827,\"start\":58823},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":58911,\"start\":58907},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":58914,\"start\":58911},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":58918,\"start\":58914},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":58922,\"start\":58918},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":58926,\"start\":58922},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":58930,\"start\":58926},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":58934,\"start\":58930},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":58938,\"start\":58934},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":58942,\"start\":58938},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":58946,\"start\":58942},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":59112,\"start\":59108},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":59188,\"start\":59184},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":59191,\"start\":59188},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":59624,\"start\":59620},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":59627,\"start\":59624},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":59630,\"start\":59627},{\"end\":59765,\"start\":59742},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":59787,\"start\":59784},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":59789,\"start\":59787},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":59792,\"start\":59789},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":59872,\"start\":59868},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":59887,\"start\":59883},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":59900,\"start\":59896},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":59911,\"start\":59907},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":59928,\"start\":59924},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":59943,\"start\":59939},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":59964,\"start\":59960}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":61238,\"start\":61142},{\"attributes\":{\"id\":\"fig_1\"},\"end\":61536,\"start\":61239},{\"attributes\":{\"id\":\"fig_3\"},\"end\":61592,\"start\":61537},{\"attributes\":{\"id\":\"fig_5\"},\"end\":61697,\"start\":61593},{\"attributes\":{\"id\":\"fig_6\"},\"end\":61877,\"start\":61698},{\"attributes\":{\"id\":\"fig_7\"},\"end\":62162,\"start\":61878},{\"attributes\":{\"id\":\"fig_8\"},\"end\":62337,\"start\":62163},{\"attributes\":{\"id\":\"fig_9\"},\"end\":62567,\"start\":62338},{\"attributes\":{\"id\":\"fig_10\"},\"end\":62699,\"start\":62568},{\"attributes\":{\"id\":\"fig_11\"},\"end\":62805,\"start\":62700},{\"attributes\":{\"id\":\"fig_12\"},\"end\":63109,\"start\":62806},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":63260,\"start\":63110},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":64208,\"start\":63261},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":64605,\"start\":64209}]", "paragraph": "[{\"end\":3611,\"start\":2496},{\"end\":5440,\"start\":3613},{\"end\":6660,\"start\":5442},{\"end\":6969,\"start\":6662},{\"end\":8210,\"start\":6971},{\"end\":8563,\"start\":8212},{\"end\":9643,\"start\":8578},{\"end\":11152,\"start\":9645},{\"end\":13546,\"start\":11154},{\"end\":14780,\"start\":13580},{\"end\":15156,\"start\":14815},{\"end\":15523,\"start\":15214},{\"end\":16126,\"start\":15525},{\"end\":16973,\"start\":16171},{\"end\":17264,\"start\":17030},{\"end\":17527,\"start\":17266},{\"end\":17876,\"start\":17529},{\"end\":18693,\"start\":17919},{\"end\":19298,\"start\":18695},{\"end\":19766,\"start\":19323},{\"end\":19901,\"start\":19782},{\"end\":20614,\"start\":19964},{\"end\":20947,\"start\":20643},{\"end\":21199,\"start\":20970},{\"end\":22379,\"start\":21247},{\"end\":22575,\"start\":22452},{\"end\":23909,\"start\":22601},{\"end\":25613,\"start\":23945},{\"end\":25943,\"start\":25615},{\"end\":27004,\"start\":26034},{\"end\":27678,\"start\":27061},{\"end\":29657,\"start\":27703},{\"end\":30137,\"start\":29708},{\"end\":30603,\"start\":30139},{\"end\":30830,\"start\":30605},{\"end\":33048,\"start\":30832},{\"end\":33241,\"start\":33071},{\"end\":34424,\"start\":33276},{\"end\":37128,\"start\":34426},{\"end\":38016,\"start\":37145},{\"end\":38812,\"start\":38045},{\"end\":39597,\"start\":38831},{\"end\":41151,\"start\":39621},{\"end\":42397,\"start\":41189},{\"end\":43540,\"start\":42399},{\"end\":44033,\"start\":43542},{\"end\":46441,\"start\":44073},{\"end\":53838,\"start\":46471},{\"end\":58622,\"start\":54063},{\"end\":60626,\"start\":58695},{\"end\":61141,\"start\":60641}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15213,\"start\":15157},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16170,\"start\":16127},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17029,\"start\":16974},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17918,\"start\":17877},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19781,\"start\":19767},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19963,\"start\":19902},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20969,\"start\":20948},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21246,\"start\":21200},{\"attributes\":{\"id\":\"formula_8\"},\"end\":22451,\"start\":22380}]", "table_ref": "[{\"end\":9642,\"start\":9629},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":42579,\"start\":42572},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":43860,\"start\":43853}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2494,\"start\":2482},{\"attributes\":{\"n\":\"2\"},\"end\":8576,\"start\":8566},{\"attributes\":{\"n\":\"3\"},\"end\":13578,\"start\":13549},{\"attributes\":{\"n\":\"3.1\"},\"end\":14813,\"start\":14783},{\"attributes\":{\"n\":\"3.2\"},\"end\":19321,\"start\":19301},{\"attributes\":{\"n\":\"3.3\"},\"end\":20641,\"start\":20617},{\"attributes\":{\"n\":\"4\"},\"end\":22599,\"start\":22578},{\"attributes\":{\"n\":\"4.1\"},\"end\":23943,\"start\":23912},{\"end\":26032,\"start\":25946},{\"end\":27023,\"start\":27007},{\"attributes\":{\"n\":\"4.2\"},\"end\":27059,\"start\":27026},{\"attributes\":{\"n\":\"4.3\"},\"end\":27701,\"start\":27681},{\"attributes\":{\"n\":\"5\"},\"end\":29682,\"start\":29660},{\"attributes\":{\"n\":\"5.1\"},\"end\":29706,\"start\":29685},{\"attributes\":{\"n\":\"5.2\"},\"end\":33069,\"start\":33051},{\"end\":33274,\"start\":33244},{\"attributes\":{\"n\":\"5.3\"},\"end\":37143,\"start\":37131},{\"attributes\":{\"n\":\"5.4\"},\"end\":38043,\"start\":38019},{\"attributes\":{\"n\":\"5.5\"},\"end\":38829,\"start\":38815},{\"attributes\":{\"n\":\"5.6\"},\"end\":39619,\"start\":39600},{\"attributes\":{\"n\":\"6\"},\"end\":41187,\"start\":41154},{\"attributes\":{\"n\":\"6.2\"},\"end\":44071,\"start\":44036},{\"attributes\":{\"n\":\"6.3\"},\"end\":46469,\"start\":46444},{\"end\":53871,\"start\":53841},{\"end\":53890,\"start\":53874},{\"end\":53971,\"start\":53893},{\"attributes\":{\"n\":\"6.4\"},\"end\":53992,\"start\":53974},{\"attributes\":{\"n\":\"6.5\"},\"end\":54028,\"start\":53995},{\"attributes\":{\"n\":\"6.6\"},\"end\":54061,\"start\":54031},{\"attributes\":{\"n\":\"6.7\"},\"end\":58649,\"start\":58625},{\"attributes\":{\"n\":\"6.8\"},\"end\":58678,\"start\":58652},{\"attributes\":{\"n\":\"7\"},\"end\":58693,\"start\":58681},{\"attributes\":{\"n\":\"8\"},\"end\":60639,\"start\":60629},{\"end\":61153,\"start\":61143},{\"end\":61250,\"start\":61240},{\"end\":61548,\"start\":61538},{\"end\":61604,\"start\":61594},{\"end\":61709,\"start\":61699},{\"end\":62174,\"start\":62164},{\"end\":62359,\"start\":62339},{\"end\":62586,\"start\":62569},{\"end\":62722,\"start\":62701},{\"end\":62829,\"start\":62807},{\"end\":63271,\"start\":63262},{\"end\":64219,\"start\":64210}]", "table": "[{\"end\":64208,\"start\":63294},{\"end\":64605,\"start\":64255}]", "figure_caption": "[{\"end\":61238,\"start\":61155},{\"end\":61536,\"start\":61252},{\"end\":61592,\"start\":61550},{\"end\":61697,\"start\":61606},{\"end\":61877,\"start\":61711},{\"end\":62162,\"start\":61880},{\"end\":62337,\"start\":62176},{\"end\":62567,\"start\":62362},{\"end\":62699,\"start\":62588},{\"end\":62805,\"start\":62726},{\"end\":63109,\"start\":62834},{\"end\":63260,\"start\":63112},{\"end\":63294,\"start\":63273},{\"end\":64255,\"start\":64221}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9137,\"start\":9128},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9668,\"start\":9657},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12080,\"start\":12072},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12621,\"start\":12613},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14247,\"start\":14238},{\"end\":15516,\"start\":15507},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15770,\"start\":15759},{\"end\":16119,\"start\":16110},{\"end\":17232,\"start\":17224},{\"end\":17417,\"start\":17409},{\"end\":19488,\"start\":19479},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23512,\"start\":23504},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24043,\"start\":24035},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25550,\"start\":25540},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26432,\"start\":26423},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26562,\"start\":26553},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27446,\"start\":27437},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29048,\"start\":29040},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":30158,\"start\":30150},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30297,\"start\":30286},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":31650,\"start\":31642},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":32462,\"start\":32454},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":32640,\"start\":32632},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34453,\"start\":34445},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":36041,\"start\":36033},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":38888,\"start\":38880},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":44282,\"start\":44273},{\"end\":44432,\"start\":44421},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":44543,\"start\":44534},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":45082,\"start\":45073},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":45456,\"start\":45447},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":45964,\"start\":45955},{\"end\":46683,\"start\":46674},{\"end\":48005,\"start\":47996},{\"end\":48111,\"start\":48101},{\"end\":48278,\"start\":48268},{\"end\":48488,\"start\":48479},{\"end\":49440,\"start\":49431},{\"end\":49918,\"start\":49909},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":52981,\"start\":52973},{\"end\":54091,\"start\":54083},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":54898,\"start\":54888},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":55723,\"start\":55713},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":56045,\"start\":56035},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":57075,\"start\":57066},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":57545,\"start\":57536},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":57722,\"start\":57713},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":58061,\"start\":58052}]", "bib_author_first_name": "[{\"end\":64993,\"start\":64992},{\"end\":64999,\"start\":64998},{\"end\":65007,\"start\":65006},{\"end\":65013,\"start\":65012},{\"end\":65021,\"start\":65020},{\"end\":65029,\"start\":65028},{\"end\":65037,\"start\":65036},{\"end\":65044,\"start\":65043},{\"end\":65046,\"start\":65045},{\"end\":65055,\"start\":65054},{\"end\":65057,\"start\":65056},{\"end\":65064,\"start\":65063},{\"end\":65066,\"start\":65065},{\"end\":65408,\"start\":65407},{\"end\":65419,\"start\":65418},{\"end\":65429,\"start\":65428},{\"end\":65445,\"start\":65444},{\"end\":65453,\"start\":65452},{\"end\":65463,\"start\":65462},{\"end\":65475,\"start\":65474},{\"end\":65488,\"start\":65487},{\"end\":65863,\"start\":65862},{\"end\":65865,\"start\":65864},{\"end\":66145,\"start\":66141},{\"end\":66155,\"start\":66151},{\"end\":66165,\"start\":66161},{\"end\":66624,\"start\":66620},{\"end\":66633,\"start\":66632},{\"end\":66635,\"start\":66634},{\"end\":66843,\"start\":66842},{\"end\":66854,\"start\":66853},{\"end\":66856,\"start\":66855},{\"end\":66866,\"start\":66865},{\"end\":66874,\"start\":66873},{\"end\":66884,\"start\":66883},{\"end\":66895,\"start\":66894},{\"end\":66903,\"start\":66902},{\"end\":66914,\"start\":66913},{\"end\":67199,\"start\":67198},{\"end\":67206,\"start\":67205},{\"end\":67214,\"start\":67213},{\"end\":67221,\"start\":67220},{\"end\":67230,\"start\":67229},{\"end\":67575,\"start\":67574},{\"end\":67582,\"start\":67581},{\"end\":67589,\"start\":67588},{\"end\":67598,\"start\":67597},{\"end\":68004,\"start\":68003},{\"end\":68015,\"start\":68014},{\"end\":68025,\"start\":68024},{\"end\":68261,\"start\":68260},{\"end\":68270,\"start\":68269},{\"end\":68282,\"start\":68281},{\"end\":68296,\"start\":68295},{\"end\":68298,\"start\":68297},{\"end\":68312,\"start\":68311},{\"end\":68318,\"start\":68317},{\"end\":68862,\"start\":68861},{\"end\":68872,\"start\":68871},{\"end\":68881,\"start\":68880},{\"end\":68889,\"start\":68888},{\"end\":68899,\"start\":68898},{\"end\":68910,\"start\":68909},{\"end\":68920,\"start\":68919},{\"end\":68932,\"start\":68931},{\"end\":68940,\"start\":68939},{\"end\":68948,\"start\":68947},{\"end\":68950,\"start\":68949},{\"end\":69415,\"start\":69414},{\"end\":69423,\"start\":69422},{\"end\":69431,\"start\":69430},{\"end\":69437,\"start\":69436},{\"end\":69602,\"start\":69601},{\"end\":69612,\"start\":69611},{\"end\":69622,\"start\":69621},{\"end\":70199,\"start\":70198},{\"end\":70209,\"start\":70208},{\"end\":70215,\"start\":70214},{\"end\":70709,\"start\":70708},{\"end\":70719,\"start\":70718},{\"end\":70725,\"start\":70724},{\"end\":71194,\"start\":71193},{\"end\":71201,\"start\":71200},{\"end\":71210,\"start\":71209},{\"end\":71566,\"start\":71565},{\"end\":71579,\"start\":71578},{\"end\":71589,\"start\":71588},{\"end\":71600,\"start\":71599},{\"end\":71608,\"start\":71607},{\"end\":72047,\"start\":72046},{\"end\":72060,\"start\":72059},{\"end\":72070,\"start\":72069},{\"end\":72079,\"start\":72078},{\"end\":72087,\"start\":72086},{\"end\":72607,\"start\":72606},{\"end\":72614,\"start\":72613},{\"end\":72620,\"start\":72619},{\"end\":72626,\"start\":72625},{\"end\":72635,\"start\":72634},{\"end\":72643,\"start\":72642},{\"end\":72650,\"start\":72649},{\"end\":72658,\"start\":72657},{\"end\":73189,\"start\":73188},{\"end\":73200,\"start\":73199},{\"end\":73207,\"start\":73206},{\"end\":73224,\"start\":73223},{\"end\":73243,\"start\":73242},{\"end\":73245,\"start\":73244},{\"end\":73257,\"start\":73256},{\"end\":73263,\"start\":73262},{\"end\":73265,\"start\":73264},{\"end\":73277,\"start\":73276},{\"end\":73836,\"start\":73835},{\"end\":73845,\"start\":73844},{\"end\":73854,\"start\":73853},{\"end\":73861,\"start\":73860},{\"end\":74354,\"start\":74353},{\"end\":74360,\"start\":74359},{\"end\":74368,\"start\":74367},{\"end\":74381,\"start\":74380},{\"end\":74833,\"start\":74832},{\"end\":74835,\"start\":74834},{\"end\":74844,\"start\":74843},{\"end\":74856,\"start\":74855},{\"end\":74867,\"start\":74866},{\"end\":74869,\"start\":74868},{\"end\":74881,\"start\":74880},{\"end\":74883,\"start\":74882},{\"end\":75330,\"start\":75329},{\"end\":75344,\"start\":75343},{\"end\":75356,\"start\":75355},{\"end\":75364,\"start\":75363},{\"end\":75372,\"start\":75371},{\"end\":75382,\"start\":75381},{\"end\":75393,\"start\":75392},{\"end\":75405,\"start\":75404},{\"end\":75407,\"start\":75406},{\"end\":75908,\"start\":75907},{\"end\":75910,\"start\":75909},{\"end\":76177,\"start\":76176},{\"end\":76435,\"start\":76434},{\"end\":76626,\"start\":76625},{\"end\":76628,\"start\":76627},{\"end\":76636,\"start\":76635},{\"end\":76642,\"start\":76641},{\"end\":76886,\"start\":76885},{\"end\":77099,\"start\":77098},{\"end\":77101,\"start\":77100},{\"end\":77111,\"start\":77110},{\"end\":77113,\"start\":77112},{\"end\":77122,\"start\":77121},{\"end\":77124,\"start\":77123},{\"end\":77134,\"start\":77133},{\"end\":77136,\"start\":77135},{\"end\":77149,\"start\":77148},{\"end\":77151,\"start\":77150},{\"end\":77160,\"start\":77159},{\"end\":77357,\"start\":77356},{\"end\":77364,\"start\":77363},{\"end\":77371,\"start\":77370},{\"end\":77386,\"start\":77385},{\"end\":77396,\"start\":77395},{\"end\":77405,\"start\":77404},{\"end\":77705,\"start\":77704},{\"end\":77717,\"start\":77716},{\"end\":77719,\"start\":77718},{\"end\":77729,\"start\":77728},{\"end\":77741,\"start\":77740},{\"end\":77743,\"start\":77742},{\"end\":78237,\"start\":78236},{\"end\":78239,\"start\":78238},{\"end\":78247,\"start\":78246},{\"end\":78249,\"start\":78248},{\"end\":78258,\"start\":78257},{\"end\":78268,\"start\":78267},{\"end\":78279,\"start\":78278},{\"end\":78294,\"start\":78293},{\"end\":78296,\"start\":78295},{\"end\":78303,\"start\":78302},{\"end\":78322,\"start\":78321},{\"end\":78331,\"start\":78330},{\"end\":78345,\"start\":78344},{\"end\":78942,\"start\":78941},{\"end\":79259,\"start\":79258},{\"end\":79270,\"start\":79269},{\"end\":79280,\"start\":79279},{\"end\":79289,\"start\":79288},{\"end\":79304,\"start\":79303},{\"end\":79580,\"start\":79579},{\"end\":79589,\"start\":79588},{\"end\":79599,\"start\":79598},{\"end\":79607,\"start\":79606},{\"end\":79617,\"start\":79616},{\"end\":79619,\"start\":79618},{\"end\":80025,\"start\":80024},{\"end\":80032,\"start\":80031},{\"end\":80041,\"start\":80040},{\"end\":80052,\"start\":80051},{\"end\":80453,\"start\":80452},{\"end\":80462,\"start\":80461},{\"end\":80471,\"start\":80470},{\"end\":80483,\"start\":80482},{\"end\":80492,\"start\":80491},{\"end\":80906,\"start\":80905},{\"end\":80919,\"start\":80918},{\"end\":80931,\"start\":80930},{\"end\":80940,\"start\":80939},{\"end\":80951,\"start\":80950},{\"end\":80962,\"start\":80961},{\"end\":80973,\"start\":80972},{\"end\":80983,\"start\":80982},{\"end\":80996,\"start\":80995},{\"end\":81003,\"start\":81002},{\"end\":81317,\"start\":81316},{\"end\":81329,\"start\":81328},{\"end\":81336,\"start\":81335},{\"end\":81347,\"start\":81346},{\"end\":81362,\"start\":81361},{\"end\":81377,\"start\":81376},{\"end\":81734,\"start\":81733},{\"end\":81741,\"start\":81740},{\"end\":81748,\"start\":81747},{\"end\":81756,\"start\":81755},{\"end\":81763,\"start\":81762},{\"end\":82244,\"start\":82243},{\"end\":82253,\"start\":82252},{\"end\":82263,\"start\":82262},{\"end\":82274,\"start\":82273},{\"end\":82287,\"start\":82286},{\"end\":82297,\"start\":82296},{\"end\":82761,\"start\":82760},{\"end\":82774,\"start\":82773},{\"end\":82783,\"start\":82782},{\"end\":82796,\"start\":82795},{\"end\":83120,\"start\":83119},{\"end\":83130,\"start\":83129},{\"end\":83141,\"start\":83140},{\"end\":83143,\"start\":83142},{\"end\":83633,\"start\":83632},{\"end\":83640,\"start\":83639},{\"end\":83647,\"start\":83646},{\"end\":83656,\"start\":83655},{\"end\":83671,\"start\":83670},{\"end\":83684,\"start\":83683},{\"end\":84240,\"start\":84239},{\"end\":84250,\"start\":84249},{\"end\":84472,\"start\":84471},{\"end\":84483,\"start\":84482},{\"end\":84500,\"start\":84499},{\"end\":84969,\"start\":84968},{\"end\":84976,\"start\":84975},{\"end\":84991,\"start\":84990},{\"end\":85000,\"start\":84999},{\"end\":85007,\"start\":85006},{\"end\":85328,\"start\":85327},{\"end\":85342,\"start\":85341},{\"end\":85671,\"start\":85670},{\"end\":85677,\"start\":85676},{\"end\":85684,\"start\":85683},{\"end\":85686,\"start\":85685},{\"end\":85696,\"start\":85695},{\"end\":85705,\"start\":85704},{\"end\":85715,\"start\":85714},{\"end\":85728,\"start\":85727},{\"end\":85739,\"start\":85738},{\"end\":85748,\"start\":85747},{\"end\":85759,\"start\":85758},{\"end\":85761,\"start\":85760},{\"end\":86135,\"start\":86134},{\"end\":86137,\"start\":86136},{\"end\":86147,\"start\":86146},{\"end\":86157,\"start\":86156},{\"end\":86167,\"start\":86166},{\"end\":86176,\"start\":86175},{\"end\":86178,\"start\":86177},{\"end\":86550,\"start\":86549},{\"end\":86559,\"start\":86558},{\"end\":86568,\"start\":86567},{\"end\":86575,\"start\":86574},{\"end\":86583,\"start\":86582},{\"end\":86930,\"start\":86929},{\"end\":86943,\"start\":86942},{\"end\":86955,\"start\":86954},{\"end\":86964,\"start\":86963},{\"end\":86973,\"start\":86972},{\"end\":86981,\"start\":86980},{\"end\":86983,\"start\":86982},{\"end\":86995,\"start\":86994},{\"end\":87006,\"start\":87005},{\"end\":87008,\"start\":87007},{\"end\":87421,\"start\":87420},{\"end\":87432,\"start\":87431},{\"end\":87441,\"start\":87440},{\"end\":87452,\"start\":87451},{\"end\":87469,\"start\":87468},{\"end\":87479,\"start\":87478},{\"end\":87488,\"start\":87487},{\"end\":87923,\"start\":87922},{\"end\":87933,\"start\":87932},{\"end\":87943,\"start\":87942},{\"end\":87952,\"start\":87951},{\"end\":88334,\"start\":88333},{\"end\":88347,\"start\":88346},{\"end\":88356,\"start\":88355},{\"end\":88364,\"start\":88363},{\"end\":88366,\"start\":88365},{\"end\":88378,\"start\":88377},{\"end\":88389,\"start\":88388},{\"end\":88391,\"start\":88390},{\"end\":88708,\"start\":88707},{\"end\":88721,\"start\":88720},{\"end\":88723,\"start\":88722},{\"end\":88733,\"start\":88732},{\"end\":88735,\"start\":88734},{\"end\":88745,\"start\":88744},{\"end\":88747,\"start\":88746},{\"end\":88755,\"start\":88754},{\"end\":88757,\"start\":88756},{\"end\":88768,\"start\":88767},{\"end\":88770,\"start\":88769},{\"end\":89093,\"start\":89092},{\"end\":89103,\"start\":89102},{\"end\":89112,\"start\":89111},{\"end\":89120,\"start\":89119},{\"end\":89446,\"start\":89445},{\"end\":89452,\"start\":89451},{\"end\":89459,\"start\":89458},{\"end\":89476,\"start\":89475},{\"end\":89495,\"start\":89494},{\"end\":89504,\"start\":89503},{\"end\":89510,\"start\":89509},{\"end\":89957,\"start\":89956},{\"end\":89964,\"start\":89963},{\"end\":89983,\"start\":89982},{\"end\":89995,\"start\":89994},{\"end\":90005,\"start\":90004},{\"end\":90016,\"start\":90015},{\"end\":90018,\"start\":90017},{\"end\":90030,\"start\":90029},{\"end\":90377,\"start\":90376},{\"end\":90390,\"start\":90389},{\"end\":90392,\"start\":90391},{\"end\":90402,\"start\":90401},{\"end\":90412,\"start\":90411},{\"end\":90422,\"start\":90421},{\"end\":90743,\"start\":90742},{\"end\":90760,\"start\":90759},{\"end\":90781,\"start\":90780},{\"end\":90790,\"start\":90789},{\"end\":90796,\"start\":90791},{\"end\":90812,\"start\":90811},{\"end\":91198,\"start\":91197},{\"end\":91207,\"start\":91206},{\"end\":91218,\"start\":91217},{\"end\":91228,\"start\":91227},{\"end\":91238,\"start\":91237},{\"end\":91246,\"start\":91245},{\"end\":91255,\"start\":91254},{\"end\":91257,\"start\":91256},{\"end\":91268,\"start\":91267},{\"end\":91277,\"start\":91276},{\"end\":91285,\"start\":91284},{\"end\":91571,\"start\":91570},{\"end\":91583,\"start\":91582},{\"end\":91591,\"start\":91590},{\"end\":91750,\"start\":91749},{\"end\":91763,\"start\":91762},{\"end\":91774,\"start\":91773},{\"end\":91776,\"start\":91775},{\"end\":91788,\"start\":91787},{\"end\":92353,\"start\":92352},{\"end\":92355,\"start\":92354},{\"end\":92365,\"start\":92364},{\"end\":92367,\"start\":92366},{\"end\":93081,\"start\":93080},{\"end\":93093,\"start\":93092},{\"end\":93095,\"start\":93094},{\"end\":93328,\"start\":93327},{\"end\":93338,\"start\":93337},{\"end\":93345,\"start\":93344},{\"end\":93354,\"start\":93353},{\"end\":93754,\"start\":93753},{\"end\":93766,\"start\":93765},{\"end\":93773,\"start\":93772},{\"end\":93784,\"start\":93783},{\"end\":93794,\"start\":93793},{\"end\":93807,\"start\":93806},{\"end\":93814,\"start\":93813},{\"end\":93816,\"start\":93815},{\"end\":93826,\"start\":93825},{\"end\":93835,\"start\":93834},{\"end\":93837,\"start\":93836},{\"end\":93848,\"start\":93847},{\"end\":93850,\"start\":93849},{\"end\":94325,\"start\":94324},{\"end\":94332,\"start\":94331},{\"end\":94349,\"start\":94348},{\"end\":94819,\"start\":94818},{\"end\":94825,\"start\":94824},{\"end\":94833,\"start\":94832},{\"end\":94840,\"start\":94839},{\"end\":94849,\"start\":94848},{\"end\":94856,\"start\":94855},{\"end\":94864,\"start\":94863},{\"end\":94879,\"start\":94878},{\"end\":95370,\"start\":95369},{\"end\":95379,\"start\":95378},{\"end\":95385,\"start\":95384},{\"end\":95403,\"start\":95399},{\"end\":95410,\"start\":95409},{\"end\":95418,\"start\":95417},{\"end\":95429,\"start\":95428},{\"end\":95439,\"start\":95438},{\"end\":95451,\"start\":95450},{\"end\":95461,\"start\":95460},{\"end\":95951,\"start\":95947},{\"end\":95961,\"start\":95957},{\"end\":95972,\"start\":95968},{\"end\":95982,\"start\":95978},{\"end\":95994,\"start\":95990},{\"end\":96006,\"start\":96002},{\"end\":96015,\"start\":96014},{\"end\":96026,\"start\":96022},{\"end\":96036,\"start\":96032},{\"end\":96048,\"start\":96044},{\"end\":96541,\"start\":96540},{\"end\":96550,\"start\":96549},{\"end\":96557,\"start\":96556},{\"end\":96566,\"start\":96565},{\"end\":96578,\"start\":96577},{\"end\":96585,\"start\":96584},{\"end\":96599,\"start\":96598},{\"end\":96992,\"start\":96991},{\"end\":96998,\"start\":96997},{\"end\":97000,\"start\":96999},{\"end\":97278,\"start\":97277},{\"end\":97291,\"start\":97290},{\"end\":97293,\"start\":97292},{\"end\":97657,\"start\":97656},{\"end\":97675,\"start\":97674},{\"end\":97688,\"start\":97687},{\"end\":98048,\"start\":98047},{\"end\":98058,\"start\":98057},{\"end\":98068,\"start\":98067},{\"end\":98070,\"start\":98069},{\"end\":98079,\"start\":98078},{\"end\":98090,\"start\":98089},{\"end\":98092,\"start\":98091},{\"end\":98100,\"start\":98099},{\"end\":98102,\"start\":98101},{\"end\":98115,\"start\":98114},{\"end\":98125,\"start\":98124},{\"end\":98127,\"start\":98126},{\"end\":98137,\"start\":98136},{\"end\":98139,\"start\":98138},{\"end\":98154,\"start\":98153},{\"end\":98455,\"start\":98454},{\"end\":98457,\"start\":98456},{\"end\":98466,\"start\":98465},{\"end\":98476,\"start\":98475},{\"end\":98478,\"start\":98477},{\"end\":98486,\"start\":98485},{\"end\":98488,\"start\":98487},{\"end\":98501,\"start\":98500},{\"end\":98503,\"start\":98502},{\"end\":98878,\"start\":98877},{\"end\":98889,\"start\":98888},{\"end\":98896,\"start\":98895},{\"end\":98906,\"start\":98905},{\"end\":98918,\"start\":98917},{\"end\":99309,\"start\":99308},{\"end\":99321,\"start\":99320},{\"end\":99331,\"start\":99330},{\"end\":99341,\"start\":99340},{\"end\":99351,\"start\":99350},{\"end\":99360,\"start\":99359},{\"end\":99695,\"start\":99694},{\"end\":99706,\"start\":99705},{\"end\":99713,\"start\":99712},{\"end\":99720,\"start\":99719},{\"end\":99733,\"start\":99732},{\"end\":100081,\"start\":100080},{\"end\":100091,\"start\":100090},{\"end\":100097,\"start\":100096},{\"end\":100407,\"start\":100406},{\"end\":100415,\"start\":100414},{\"end\":100423,\"start\":100422},{\"end\":100431,\"start\":100430},{\"end\":100437,\"start\":100436},{\"end\":100798,\"start\":100797},{\"end\":100807,\"start\":100806},{\"end\":101195,\"start\":101194},{\"end\":101204,\"start\":101203},{\"end\":101214,\"start\":101213},{\"end\":101221,\"start\":101220},{\"end\":101230,\"start\":101229},{\"end\":101239,\"start\":101238}]", "bib_author_last_name": "[{\"end\":64996,\"start\":64994},{\"end\":65004,\"start\":65000},{\"end\":65010,\"start\":65008},{\"end\":65018,\"start\":65014},{\"end\":65026,\"start\":65022},{\"end\":65034,\"start\":65030},{\"end\":65041,\"start\":65038},{\"end\":65052,\"start\":65047},{\"end\":65061,\"start\":65058},{\"end\":65071,\"start\":65067},{\"end\":65416,\"start\":65409},{\"end\":65426,\"start\":65420},{\"end\":65442,\"start\":65430},{\"end\":65450,\"start\":65446},{\"end\":65460,\"start\":65454},{\"end\":65472,\"start\":65464},{\"end\":65485,\"start\":65476},{\"end\":65493,\"start\":65489},{\"end\":65869,\"start\":65866},{\"end\":65874,\"start\":65871},{\"end\":66149,\"start\":66146},{\"end\":66159,\"start\":66156},{\"end\":66170,\"start\":66166},{\"end\":66630,\"start\":66625},{\"end\":66642,\"start\":66636},{\"end\":66851,\"start\":66844},{\"end\":66863,\"start\":66857},{\"end\":66871,\"start\":66867},{\"end\":66881,\"start\":66875},{\"end\":66892,\"start\":66885},{\"end\":66900,\"start\":66896},{\"end\":66911,\"start\":66904},{\"end\":66920,\"start\":66915},{\"end\":67203,\"start\":67200},{\"end\":67211,\"start\":67207},{\"end\":67218,\"start\":67215},{\"end\":67227,\"start\":67222},{\"end\":67235,\"start\":67231},{\"end\":67579,\"start\":67576},{\"end\":67586,\"start\":67583},{\"end\":67595,\"start\":67590},{\"end\":67603,\"start\":67599},{\"end\":68012,\"start\":68005},{\"end\":68022,\"start\":68016},{\"end\":68031,\"start\":68026},{\"end\":68267,\"start\":68262},{\"end\":68279,\"start\":68271},{\"end\":68293,\"start\":68283},{\"end\":68309,\"start\":68299},{\"end\":68315,\"start\":68313},{\"end\":68329,\"start\":68319},{\"end\":68869,\"start\":68863},{\"end\":68878,\"start\":68873},{\"end\":68886,\"start\":68882},{\"end\":68896,\"start\":68890},{\"end\":68907,\"start\":68900},{\"end\":68917,\"start\":68911},{\"end\":68929,\"start\":68921},{\"end\":68937,\"start\":68933},{\"end\":68945,\"start\":68941},{\"end\":68955,\"start\":68951},{\"end\":69420,\"start\":69416},{\"end\":69428,\"start\":69424},{\"end\":69434,\"start\":69432},{\"end\":69442,\"start\":69438},{\"end\":69609,\"start\":69603},{\"end\":69619,\"start\":69613},{\"end\":69626,\"start\":69623},{\"end\":70206,\"start\":70200},{\"end\":70212,\"start\":70210},{\"end\":70219,\"start\":70216},{\"end\":70716,\"start\":70710},{\"end\":70722,\"start\":70720},{\"end\":70729,\"start\":70726},{\"end\":71198,\"start\":71195},{\"end\":71207,\"start\":71202},{\"end\":71213,\"start\":71211},{\"end\":71576,\"start\":71567},{\"end\":71586,\"start\":71580},{\"end\":71597,\"start\":71590},{\"end\":71605,\"start\":71601},{\"end\":71616,\"start\":71609},{\"end\":72057,\"start\":72048},{\"end\":72067,\"start\":72061},{\"end\":72076,\"start\":72071},{\"end\":72084,\"start\":72080},{\"end\":72095,\"start\":72088},{\"end\":72611,\"start\":72608},{\"end\":72617,\"start\":72615},{\"end\":72623,\"start\":72621},{\"end\":72632,\"start\":72627},{\"end\":72640,\"start\":72636},{\"end\":72647,\"start\":72644},{\"end\":72655,\"start\":72651},{\"end\":72662,\"start\":72659},{\"end\":73197,\"start\":73190},{\"end\":73204,\"start\":73201},{\"end\":73221,\"start\":73208},{\"end\":73240,\"start\":73225},{\"end\":73254,\"start\":73246},{\"end\":73260,\"start\":73258},{\"end\":73274,\"start\":73266},{\"end\":73286,\"start\":73278},{\"end\":73842,\"start\":73837},{\"end\":73851,\"start\":73846},{\"end\":73858,\"start\":73855},{\"end\":73868,\"start\":73862},{\"end\":74357,\"start\":74355},{\"end\":74365,\"start\":74361},{\"end\":74378,\"start\":74369},{\"end\":74389,\"start\":74382},{\"end\":74841,\"start\":74836},{\"end\":74853,\"start\":74845},{\"end\":74864,\"start\":74857},{\"end\":74878,\"start\":74870},{\"end\":74887,\"start\":74884},{\"end\":75341,\"start\":75331},{\"end\":75353,\"start\":75345},{\"end\":75361,\"start\":75357},{\"end\":75369,\"start\":75365},{\"end\":75379,\"start\":75373},{\"end\":75390,\"start\":75383},{\"end\":75402,\"start\":75394},{\"end\":75413,\"start\":75408},{\"end\":75919,\"start\":75911},{\"end\":76184,\"start\":76178},{\"end\":76438,\"start\":76436},{\"end\":76633,\"start\":76629},{\"end\":76639,\"start\":76637},{\"end\":76651,\"start\":76643},{\"end\":76892,\"start\":76887},{\"end\":77108,\"start\":77102},{\"end\":77119,\"start\":77114},{\"end\":77131,\"start\":77125},{\"end\":77146,\"start\":77137},{\"end\":77157,\"start\":77152},{\"end\":77172,\"start\":77161},{\"end\":77361,\"start\":77358},{\"end\":77368,\"start\":77365},{\"end\":77383,\"start\":77372},{\"end\":77393,\"start\":77387},{\"end\":77402,\"start\":77397},{\"end\":77411,\"start\":77406},{\"end\":77714,\"start\":77706},{\"end\":77726,\"start\":77720},{\"end\":77738,\"start\":77730},{\"end\":77749,\"start\":77744},{\"end\":78244,\"start\":78240},{\"end\":78255,\"start\":78250},{\"end\":78265,\"start\":78259},{\"end\":78276,\"start\":78269},{\"end\":78291,\"start\":78280},{\"end\":78300,\"start\":78297},{\"end\":78319,\"start\":78304},{\"end\":78328,\"start\":78323},{\"end\":78342,\"start\":78332},{\"end\":78355,\"start\":78346},{\"end\":78950,\"start\":78943},{\"end\":79267,\"start\":79260},{\"end\":79277,\"start\":79271},{\"end\":79286,\"start\":79281},{\"end\":79301,\"start\":79290},{\"end\":79310,\"start\":79305},{\"end\":79586,\"start\":79581},{\"end\":79596,\"start\":79590},{\"end\":79604,\"start\":79600},{\"end\":79614,\"start\":79608},{\"end\":79626,\"start\":79620},{\"end\":80029,\"start\":80026},{\"end\":80038,\"start\":80033},{\"end\":80049,\"start\":80042},{\"end\":80059,\"start\":80053},{\"end\":80459,\"start\":80454},{\"end\":80468,\"start\":80463},{\"end\":80480,\"start\":80472},{\"end\":80489,\"start\":80484},{\"end\":80499,\"start\":80493},{\"end\":80916,\"start\":80907},{\"end\":80928,\"start\":80920},{\"end\":80937,\"start\":80932},{\"end\":80948,\"start\":80941},{\"end\":80959,\"start\":80952},{\"end\":80970,\"start\":80963},{\"end\":80980,\"start\":80974},{\"end\":80993,\"start\":80984},{\"end\":81000,\"start\":80997},{\"end\":81008,\"start\":81004},{\"end\":81326,\"start\":81318},{\"end\":81333,\"start\":81330},{\"end\":81344,\"start\":81337},{\"end\":81359,\"start\":81348},{\"end\":81374,\"start\":81363},{\"end\":81383,\"start\":81378},{\"end\":81738,\"start\":81735},{\"end\":81745,\"start\":81742},{\"end\":81753,\"start\":81749},{\"end\":81760,\"start\":81757},{\"end\":81768,\"start\":81764},{\"end\":82250,\"start\":82245},{\"end\":82260,\"start\":82254},{\"end\":82271,\"start\":82264},{\"end\":82284,\"start\":82275},{\"end\":82294,\"start\":82288},{\"end\":82311,\"start\":82298},{\"end\":82771,\"start\":82762},{\"end\":82780,\"start\":82775},{\"end\":82793,\"start\":82784},{\"end\":82806,\"start\":82797},{\"end\":83127,\"start\":83121},{\"end\":83138,\"start\":83131},{\"end\":83150,\"start\":83144},{\"end\":83637,\"start\":83634},{\"end\":83644,\"start\":83641},{\"end\":83653,\"start\":83648},{\"end\":83668,\"start\":83657},{\"end\":83681,\"start\":83672},{\"end\":83690,\"start\":83685},{\"end\":84247,\"start\":84241},{\"end\":84262,\"start\":84251},{\"end\":84480,\"start\":84473},{\"end\":84497,\"start\":84484},{\"end\":84506,\"start\":84501},{\"end\":84973,\"start\":84970},{\"end\":84988,\"start\":84977},{\"end\":84997,\"start\":84992},{\"end\":85004,\"start\":85001},{\"end\":85013,\"start\":85008},{\"end\":85339,\"start\":85329},{\"end\":85357,\"start\":85343},{\"end\":85674,\"start\":85672},{\"end\":85681,\"start\":85678},{\"end\":85693,\"start\":85687},{\"end\":85702,\"start\":85697},{\"end\":85712,\"start\":85706},{\"end\":85725,\"start\":85716},{\"end\":85736,\"start\":85729},{\"end\":85745,\"start\":85740},{\"end\":85756,\"start\":85749},{\"end\":85764,\"start\":85762},{\"end\":86144,\"start\":86138},{\"end\":86154,\"start\":86148},{\"end\":86164,\"start\":86158},{\"end\":86173,\"start\":86168},{\"end\":86188,\"start\":86179},{\"end\":86556,\"start\":86551},{\"end\":86565,\"start\":86560},{\"end\":86572,\"start\":86569},{\"end\":86580,\"start\":86576},{\"end\":86590,\"start\":86584},{\"end\":86940,\"start\":86931},{\"end\":86952,\"start\":86944},{\"end\":86961,\"start\":86956},{\"end\":86970,\"start\":86965},{\"end\":86978,\"start\":86974},{\"end\":86992,\"start\":86984},{\"end\":87003,\"start\":86996},{\"end\":87015,\"start\":87009},{\"end\":87429,\"start\":87422},{\"end\":87438,\"start\":87433},{\"end\":87449,\"start\":87442},{\"end\":87466,\"start\":87453},{\"end\":87476,\"start\":87470},{\"end\":87485,\"start\":87480},{\"end\":87498,\"start\":87489},{\"end\":87930,\"start\":87924},{\"end\":87940,\"start\":87934},{\"end\":87949,\"start\":87944},{\"end\":87957,\"start\":87953},{\"end\":88344,\"start\":88335},{\"end\":88353,\"start\":88348},{\"end\":88361,\"start\":88357},{\"end\":88375,\"start\":88367},{\"end\":88386,\"start\":88379},{\"end\":88398,\"start\":88392},{\"end\":88718,\"start\":88709},{\"end\":88730,\"start\":88724},{\"end\":88742,\"start\":88736},{\"end\":88752,\"start\":88748},{\"end\":88765,\"start\":88758},{\"end\":88779,\"start\":88771},{\"end\":89100,\"start\":89094},{\"end\":89109,\"start\":89104},{\"end\":89117,\"start\":89113},{\"end\":89130,\"start\":89121},{\"end\":89449,\"start\":89447},{\"end\":89456,\"start\":89453},{\"end\":89473,\"start\":89460},{\"end\":89492,\"start\":89477},{\"end\":89501,\"start\":89496},{\"end\":89507,\"start\":89505},{\"end\":89514,\"start\":89511},{\"end\":89961,\"start\":89958},{\"end\":89980,\"start\":89965},{\"end\":89992,\"start\":89984},{\"end\":90002,\"start\":89996},{\"end\":90013,\"start\":90006},{\"end\":90027,\"start\":90019},{\"end\":90044,\"start\":90031},{\"end\":90387,\"start\":90378},{\"end\":90399,\"start\":90393},{\"end\":90409,\"start\":90403},{\"end\":90419,\"start\":90413},{\"end\":90428,\"start\":90423},{\"end\":90757,\"start\":90744},{\"end\":90778,\"start\":90761},{\"end\":90787,\"start\":90782},{\"end\":90809,\"start\":90797},{\"end\":90837,\"start\":90813},{\"end\":91204,\"start\":91199},{\"end\":91215,\"start\":91208},{\"end\":91225,\"start\":91219},{\"end\":91235,\"start\":91229},{\"end\":91243,\"start\":91239},{\"end\":91252,\"start\":91247},{\"end\":91265,\"start\":91258},{\"end\":91274,\"start\":91269},{\"end\":91282,\"start\":91278},{\"end\":91291,\"start\":91286},{\"end\":91580,\"start\":91572},{\"end\":91588,\"start\":91584},{\"end\":91597,\"start\":91592},{\"end\":91760,\"start\":91751},{\"end\":91771,\"start\":91764},{\"end\":91785,\"start\":91777},{\"end\":91796,\"start\":91789},{\"end\":92362,\"start\":92356},{\"end\":92377,\"start\":92368},{\"end\":93090,\"start\":93082},{\"end\":93104,\"start\":93096},{\"end\":93335,\"start\":93329},{\"end\":93342,\"start\":93339},{\"end\":93351,\"start\":93346},{\"end\":93358,\"start\":93355},{\"end\":93763,\"start\":93755},{\"end\":93770,\"start\":93767},{\"end\":93781,\"start\":93774},{\"end\":93791,\"start\":93785},{\"end\":93804,\"start\":93795},{\"end\":93811,\"start\":93808},{\"end\":93823,\"start\":93817},{\"end\":93832,\"start\":93827},{\"end\":93845,\"start\":93838},{\"end\":93856,\"start\":93851},{\"end\":94329,\"start\":94326},{\"end\":94346,\"start\":94333},{\"end\":94359,\"start\":94350},{\"end\":94822,\"start\":94820},{\"end\":94830,\"start\":94826},{\"end\":94837,\"start\":94834},{\"end\":94846,\"start\":94841},{\"end\":94853,\"start\":94850},{\"end\":94861,\"start\":94857},{\"end\":94876,\"start\":94865},{\"end\":94890,\"start\":94880},{\"end\":95376,\"start\":95371},{\"end\":95382,\"start\":95380},{\"end\":95397,\"start\":95386},{\"end\":95407,\"start\":95404},{\"end\":95415,\"start\":95411},{\"end\":95426,\"start\":95419},{\"end\":95436,\"start\":95430},{\"end\":95448,\"start\":95440},{\"end\":95458,\"start\":95452},{\"end\":95464,\"start\":95462},{\"end\":95955,\"start\":95952},{\"end\":95966,\"start\":95962},{\"end\":95976,\"start\":95973},{\"end\":95988,\"start\":95983},{\"end\":96000,\"start\":95995},{\"end\":96012,\"start\":96007},{\"end\":96020,\"start\":96016},{\"end\":96030,\"start\":96027},{\"end\":96042,\"start\":96037},{\"end\":96051,\"start\":96049},{\"end\":96547,\"start\":96542},{\"end\":96554,\"start\":96551},{\"end\":96563,\"start\":96558},{\"end\":96575,\"start\":96567},{\"end\":96582,\"start\":96579},{\"end\":96596,\"start\":96586},{\"end\":96606,\"start\":96600},{\"end\":96995,\"start\":96993},{\"end\":97006,\"start\":97001},{\"end\":97288,\"start\":97279},{\"end\":97299,\"start\":97294},{\"end\":97672,\"start\":97658},{\"end\":97685,\"start\":97676},{\"end\":97694,\"start\":97689},{\"end\":98055,\"start\":98049},{\"end\":98065,\"start\":98059},{\"end\":98076,\"start\":98071},{\"end\":98087,\"start\":98080},{\"end\":98097,\"start\":98093},{\"end\":98112,\"start\":98103},{\"end\":98122,\"start\":98116},{\"end\":98134,\"start\":98128},{\"end\":98151,\"start\":98140},{\"end\":98161,\"start\":98155},{\"end\":98463,\"start\":98458},{\"end\":98473,\"start\":98467},{\"end\":98483,\"start\":98479},{\"end\":98498,\"start\":98489},{\"end\":98510,\"start\":98504},{\"end\":98886,\"start\":98879},{\"end\":98893,\"start\":98890},{\"end\":98903,\"start\":98897},{\"end\":98915,\"start\":98907},{\"end\":98924,\"start\":98919},{\"end\":99318,\"start\":99310},{\"end\":99328,\"start\":99322},{\"end\":99338,\"start\":99332},{\"end\":99348,\"start\":99342},{\"end\":99357,\"start\":99352},{\"end\":99369,\"start\":99361},{\"end\":99703,\"start\":99696},{\"end\":99710,\"start\":99707},{\"end\":99717,\"start\":99714},{\"end\":99730,\"start\":99721},{\"end\":99739,\"start\":99734},{\"end\":100088,\"start\":100082},{\"end\":100094,\"start\":100092},{\"end\":100101,\"start\":100098},{\"end\":100412,\"start\":100408},{\"end\":100420,\"start\":100416},{\"end\":100428,\"start\":100424},{\"end\":100434,\"start\":100432},{\"end\":100442,\"start\":100438},{\"end\":100804,\"start\":100799},{\"end\":100814,\"start\":100808},{\"end\":101201,\"start\":101196},{\"end\":101211,\"start\":101205},{\"end\":101218,\"start\":101215},{\"end\":101227,\"start\":101222},{\"end\":101236,\"start\":101231},{\"end\":101245,\"start\":101240}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":210946693},\"end\":65335,\"start\":64906},{\"attributes\":{\"id\":\"b1\"},\"end\":65797,\"start\":65337},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":211556295},\"end\":66075,\"start\":65799},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218503573},\"end\":66354,\"start\":66077},{\"attributes\":{\"id\":\"b4\"},\"end\":66553,\"start\":66356},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":214694689},\"end\":66761,\"start\":66555},{\"attributes\":{\"id\":\"b6\"},\"end\":67121,\"start\":66763},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1615412},\"end\":67480,\"start\":67123},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":980581},\"end\":67937,\"start\":67482},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":11433762},\"end\":68194,\"start\":67939},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2385670},\"end\":68803,\"start\":68196},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":14466366},\"end\":69346,\"start\":68805},{\"attributes\":{\"id\":\"b12\"},\"end\":69564,\"start\":69348},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":3976946},\"end\":70103,\"start\":69566},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":58027666},\"end\":70592,\"start\":70105},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":49292274},\"end\":71073,\"start\":70594},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":214394579},\"end\":71477,\"start\":71075},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":211165936},\"end\":71967,\"start\":71479},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":203648556},\"end\":72496,\"start\":71969},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":52824162},\"end\":73091,\"start\":72498},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6329628},\"end\":73747,\"start\":73093},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":189818835},\"end\":74236,\"start\":73749},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":4566366},\"end\":74756,\"start\":74238},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14440627},\"end\":75223,\"start\":74758},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":7794823},\"end\":75838,\"start\":75225},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":100842614},\"end\":76140,\"start\":75840},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":7454137},\"end\":76377,\"start\":76142},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3964249},\"end\":76578,\"start\":76379},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":202863748},\"end\":76803,\"start\":76580},{\"attributes\":{\"id\":\"b29\"},\"end\":77056,\"start\":76805},{\"attributes\":{\"id\":\"b30\"},\"end\":77313,\"start\":77058},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2895937},\"end\":77599,\"start\":77315},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":89617611},\"end\":78115,\"start\":77601},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":221739007},\"end\":78814,\"start\":78117},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":733980},\"end\":79172,\"start\":78816},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":246531628},\"end\":79530,\"start\":79174},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1677864},\"end\":79936,\"start\":79532},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":219858990},\"end\":80373,\"start\":79938},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":201847380},\"end\":80836,\"start\":80375},{\"attributes\":{\"id\":\"b39\"},\"end\":81262,\"start\":80838},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":7036086},\"end\":81624,\"start\":81264},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":189824943},\"end\":82160,\"start\":81626},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":5667235},\"end\":82658,\"start\":82162},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":182118830},\"end\":83027,\"start\":82660},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":9812826},\"end\":83542,\"start\":83029},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":239036966},\"end\":84184,\"start\":83544},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":7108827},\"end\":84407,\"start\":84186},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":60571601},\"end\":84869,\"start\":84409},{\"attributes\":{\"doi\":\"arXiv:2110.00214\",\"id\":\"b48\"},\"end\":85241,\"start\":84871},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":11178331},\"end\":85596,\"start\":85243},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":209063048},\"end\":86034,\"start\":85598},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":119425515},\"end\":86473,\"start\":86036},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":155100847},\"end\":86898,\"start\":86475},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":206655274},\"end\":87314,\"start\":86900},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":201128119},\"end\":87860,\"start\":87316},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":16829375},\"end\":88242,\"start\":87862},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":166787},\"end\":88626,\"start\":88244},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":4430199},\"end\":89013,\"start\":88628},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":14637261},\"end\":89373,\"start\":89015},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":15419565},\"end\":89880,\"start\":89375},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":3986108},\"end\":90284,\"start\":89882},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":18482293},\"end\":90664,\"start\":90286},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":67813704},\"end\":91116,\"start\":90666},{\"attributes\":{\"doi\":\"arXiv:1603.04467\",\"id\":\"b63\"},\"end\":91558,\"start\":91118},{\"attributes\":{\"id\":\"b64\"},\"end\":91689,\"start\":91560},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":206655679},\"end\":92077,\"start\":91691},{\"attributes\":{\"id\":\"b66\"},\"end\":92271,\"start\":92079},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":17804910},\"end\":92565,\"start\":92273},{\"attributes\":{\"id\":\"b68\"},\"end\":92675,\"start\":92567},{\"attributes\":{\"id\":\"b69\"},\"end\":92789,\"start\":92677},{\"attributes\":{\"id\":\"b70\"},\"end\":92869,\"start\":92791},{\"attributes\":{\"id\":\"b71\"},\"end\":92944,\"start\":92871},{\"attributes\":{\"id\":\"b72\"},\"end\":93036,\"start\":92946},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":205420407},\"end\":93247,\"start\":93038},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":218064324},\"end\":93661,\"start\":93249},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":26102636},\"end\":94264,\"start\":93663},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":204732684},\"end\":94730,\"start\":94266},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":222316976},\"end\":95259,\"start\":94732},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":232267244},\"end\":95837,\"start\":95261},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":232153426},\"end\":96465,\"start\":95839},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":197642766},\"end\":96930,\"start\":96467},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":216080530},\"end\":97200,\"start\":96932},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":247230914},\"end\":97578,\"start\":97202},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":243928303},\"end\":97966,\"start\":97580},{\"attributes\":{\"doi\":\"arXiv:2106.05268\",\"id\":\"b84\"},\"end\":98452,\"start\":97968},{\"attributes\":{\"doi\":\"arXiv:2109.03429\",\"id\":\"b85\"},\"end\":98783,\"start\":98454},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":243902037},\"end\":99225,\"start\":98785},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":236229627},\"end\":99604,\"start\":99227},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":243896150},\"end\":100033,\"start\":99606},{\"attributes\":{\"id\":\"b89\",\"matched_paper_id\":53223494},\"end\":100353,\"start\":100035},{\"attributes\":{\"id\":\"b90\",\"matched_paper_id\":51618},\"end\":100746,\"start\":100355},{\"attributes\":{\"id\":\"b91\",\"matched_paper_id\":231603233},\"end\":101094,\"start\":100748},{\"attributes\":{\"id\":\"b92\",\"matched_paper_id\":4134322},\"end\":101484,\"start\":101096}]", "bib_title": "[{\"end\":64990,\"start\":64906},{\"end\":65405,\"start\":65337},{\"end\":65860,\"start\":65799},{\"end\":66139,\"start\":66077},{\"end\":66618,\"start\":66555},{\"end\":67196,\"start\":67123},{\"end\":67572,\"start\":67482},{\"end\":68001,\"start\":67939},{\"end\":68258,\"start\":68196},{\"end\":68859,\"start\":68805},{\"end\":69599,\"start\":69566},{\"end\":70196,\"start\":70105},{\"end\":70706,\"start\":70594},{\"end\":71191,\"start\":71075},{\"end\":71563,\"start\":71479},{\"end\":72044,\"start\":71969},{\"end\":72604,\"start\":72498},{\"end\":73186,\"start\":73093},{\"end\":73833,\"start\":73749},{\"end\":74351,\"start\":74238},{\"end\":74830,\"start\":74758},{\"end\":75327,\"start\":75225},{\"end\":75905,\"start\":75840},{\"end\":76174,\"start\":76142},{\"end\":76432,\"start\":76379},{\"end\":76623,\"start\":76580},{\"end\":76883,\"start\":76805},{\"end\":77354,\"start\":77315},{\"end\":77702,\"start\":77601},{\"end\":78234,\"start\":78117},{\"end\":78939,\"start\":78816},{\"end\":79256,\"start\":79174},{\"end\":79577,\"start\":79532},{\"end\":80022,\"start\":79938},{\"end\":80450,\"start\":80375},{\"end\":80903,\"start\":80838},{\"end\":81314,\"start\":81264},{\"end\":81731,\"start\":81626},{\"end\":82241,\"start\":82162},{\"end\":82758,\"start\":82660},{\"end\":83117,\"start\":83029},{\"end\":83630,\"start\":83544},{\"end\":84237,\"start\":84186},{\"end\":84469,\"start\":84409},{\"end\":85325,\"start\":85243},{\"end\":85668,\"start\":85598},{\"end\":86132,\"start\":86036},{\"end\":86547,\"start\":86475},{\"end\":86927,\"start\":86900},{\"end\":87418,\"start\":87316},{\"end\":87920,\"start\":87862},{\"end\":88331,\"start\":88244},{\"end\":88705,\"start\":88628},{\"end\":89090,\"start\":89015},{\"end\":89443,\"start\":89375},{\"end\":89954,\"start\":89882},{\"end\":90374,\"start\":90286},{\"end\":90740,\"start\":90666},{\"end\":91747,\"start\":91691},{\"end\":92350,\"start\":92273},{\"end\":93078,\"start\":93038},{\"end\":93325,\"start\":93249},{\"end\":93751,\"start\":93663},{\"end\":94322,\"start\":94266},{\"end\":94816,\"start\":94732},{\"end\":95367,\"start\":95261},{\"end\":95945,\"start\":95839},{\"end\":96538,\"start\":96467},{\"end\":96989,\"start\":96932},{\"end\":97275,\"start\":97202},{\"end\":97654,\"start\":97580},{\"end\":98875,\"start\":98785},{\"end\":99306,\"start\":99227},{\"end\":99692,\"start\":99606},{\"end\":100078,\"start\":100035},{\"end\":100404,\"start\":100355},{\"end\":100795,\"start\":100748},{\"end\":101192,\"start\":101096}]", "bib_author": "[{\"end\":64998,\"start\":64992},{\"end\":65006,\"start\":64998},{\"end\":65012,\"start\":65006},{\"end\":65020,\"start\":65012},{\"end\":65028,\"start\":65020},{\"end\":65036,\"start\":65028},{\"end\":65043,\"start\":65036},{\"end\":65054,\"start\":65043},{\"end\":65063,\"start\":65054},{\"end\":65073,\"start\":65063},{\"end\":65418,\"start\":65407},{\"end\":65428,\"start\":65418},{\"end\":65444,\"start\":65428},{\"end\":65452,\"start\":65444},{\"end\":65462,\"start\":65452},{\"end\":65474,\"start\":65462},{\"end\":65487,\"start\":65474},{\"end\":65495,\"start\":65487},{\"end\":65871,\"start\":65862},{\"end\":65876,\"start\":65871},{\"end\":66151,\"start\":66141},{\"end\":66161,\"start\":66151},{\"end\":66172,\"start\":66161},{\"end\":66632,\"start\":66620},{\"end\":66644,\"start\":66632},{\"end\":66853,\"start\":66842},{\"end\":66865,\"start\":66853},{\"end\":66873,\"start\":66865},{\"end\":66883,\"start\":66873},{\"end\":66894,\"start\":66883},{\"end\":66902,\"start\":66894},{\"end\":66913,\"start\":66902},{\"end\":66922,\"start\":66913},{\"end\":67205,\"start\":67198},{\"end\":67213,\"start\":67205},{\"end\":67220,\"start\":67213},{\"end\":67229,\"start\":67220},{\"end\":67237,\"start\":67229},{\"end\":67581,\"start\":67574},{\"end\":67588,\"start\":67581},{\"end\":67597,\"start\":67588},{\"end\":67605,\"start\":67597},{\"end\":68014,\"start\":68003},{\"end\":68024,\"start\":68014},{\"end\":68033,\"start\":68024},{\"end\":68269,\"start\":68260},{\"end\":68281,\"start\":68269},{\"end\":68295,\"start\":68281},{\"end\":68311,\"start\":68295},{\"end\":68317,\"start\":68311},{\"end\":68331,\"start\":68317},{\"end\":68871,\"start\":68861},{\"end\":68880,\"start\":68871},{\"end\":68888,\"start\":68880},{\"end\":68898,\"start\":68888},{\"end\":68909,\"start\":68898},{\"end\":68919,\"start\":68909},{\"end\":68931,\"start\":68919},{\"end\":68939,\"start\":68931},{\"end\":68947,\"start\":68939},{\"end\":68957,\"start\":68947},{\"end\":69422,\"start\":69414},{\"end\":69430,\"start\":69422},{\"end\":69436,\"start\":69430},{\"end\":69444,\"start\":69436},{\"end\":69611,\"start\":69601},{\"end\":69621,\"start\":69611},{\"end\":69628,\"start\":69621},{\"end\":70208,\"start\":70198},{\"end\":70214,\"start\":70208},{\"end\":70221,\"start\":70214},{\"end\":70718,\"start\":70708},{\"end\":70724,\"start\":70718},{\"end\":70731,\"start\":70724},{\"end\":71200,\"start\":71193},{\"end\":71209,\"start\":71200},{\"end\":71215,\"start\":71209},{\"end\":71578,\"start\":71565},{\"end\":71588,\"start\":71578},{\"end\":71599,\"start\":71588},{\"end\":71607,\"start\":71599},{\"end\":71618,\"start\":71607},{\"end\":72059,\"start\":72046},{\"end\":72069,\"start\":72059},{\"end\":72078,\"start\":72069},{\"end\":72086,\"start\":72078},{\"end\":72097,\"start\":72086},{\"end\":72613,\"start\":72606},{\"end\":72619,\"start\":72613},{\"end\":72625,\"start\":72619},{\"end\":72634,\"start\":72625},{\"end\":72642,\"start\":72634},{\"end\":72649,\"start\":72642},{\"end\":72657,\"start\":72649},{\"end\":72664,\"start\":72657},{\"end\":73199,\"start\":73188},{\"end\":73206,\"start\":73199},{\"end\":73223,\"start\":73206},{\"end\":73242,\"start\":73223},{\"end\":73256,\"start\":73242},{\"end\":73262,\"start\":73256},{\"end\":73276,\"start\":73262},{\"end\":73288,\"start\":73276},{\"end\":73844,\"start\":73835},{\"end\":73853,\"start\":73844},{\"end\":73860,\"start\":73853},{\"end\":73870,\"start\":73860},{\"end\":74359,\"start\":74353},{\"end\":74367,\"start\":74359},{\"end\":74380,\"start\":74367},{\"end\":74391,\"start\":74380},{\"end\":74843,\"start\":74832},{\"end\":74855,\"start\":74843},{\"end\":74866,\"start\":74855},{\"end\":74880,\"start\":74866},{\"end\":74889,\"start\":74880},{\"end\":75343,\"start\":75329},{\"end\":75355,\"start\":75343},{\"end\":75363,\"start\":75355},{\"end\":75371,\"start\":75363},{\"end\":75381,\"start\":75371},{\"end\":75392,\"start\":75381},{\"end\":75404,\"start\":75392},{\"end\":75415,\"start\":75404},{\"end\":75921,\"start\":75907},{\"end\":76186,\"start\":76176},{\"end\":76440,\"start\":76434},{\"end\":76635,\"start\":76625},{\"end\":76641,\"start\":76635},{\"end\":76653,\"start\":76641},{\"end\":76894,\"start\":76885},{\"end\":77110,\"start\":77098},{\"end\":77121,\"start\":77110},{\"end\":77133,\"start\":77121},{\"end\":77148,\"start\":77133},{\"end\":77159,\"start\":77148},{\"end\":77174,\"start\":77159},{\"end\":77363,\"start\":77356},{\"end\":77370,\"start\":77363},{\"end\":77385,\"start\":77370},{\"end\":77395,\"start\":77385},{\"end\":77404,\"start\":77395},{\"end\":77413,\"start\":77404},{\"end\":77716,\"start\":77704},{\"end\":77728,\"start\":77716},{\"end\":77740,\"start\":77728},{\"end\":77751,\"start\":77740},{\"end\":78246,\"start\":78236},{\"end\":78257,\"start\":78246},{\"end\":78267,\"start\":78257},{\"end\":78278,\"start\":78267},{\"end\":78293,\"start\":78278},{\"end\":78302,\"start\":78293},{\"end\":78321,\"start\":78302},{\"end\":78330,\"start\":78321},{\"end\":78344,\"start\":78330},{\"end\":78357,\"start\":78344},{\"end\":78952,\"start\":78941},{\"end\":79269,\"start\":79258},{\"end\":79279,\"start\":79269},{\"end\":79288,\"start\":79279},{\"end\":79303,\"start\":79288},{\"end\":79312,\"start\":79303},{\"end\":79588,\"start\":79579},{\"end\":79598,\"start\":79588},{\"end\":79606,\"start\":79598},{\"end\":79616,\"start\":79606},{\"end\":79628,\"start\":79616},{\"end\":80031,\"start\":80024},{\"end\":80040,\"start\":80031},{\"end\":80051,\"start\":80040},{\"end\":80061,\"start\":80051},{\"end\":80461,\"start\":80452},{\"end\":80470,\"start\":80461},{\"end\":80482,\"start\":80470},{\"end\":80491,\"start\":80482},{\"end\":80501,\"start\":80491},{\"end\":80918,\"start\":80905},{\"end\":80930,\"start\":80918},{\"end\":80939,\"start\":80930},{\"end\":80950,\"start\":80939},{\"end\":80961,\"start\":80950},{\"end\":80972,\"start\":80961},{\"end\":80982,\"start\":80972},{\"end\":80995,\"start\":80982},{\"end\":81002,\"start\":80995},{\"end\":81010,\"start\":81002},{\"end\":81328,\"start\":81316},{\"end\":81335,\"start\":81328},{\"end\":81346,\"start\":81335},{\"end\":81361,\"start\":81346},{\"end\":81376,\"start\":81361},{\"end\":81385,\"start\":81376},{\"end\":81740,\"start\":81733},{\"end\":81747,\"start\":81740},{\"end\":81755,\"start\":81747},{\"end\":81762,\"start\":81755},{\"end\":81770,\"start\":81762},{\"end\":82252,\"start\":82243},{\"end\":82262,\"start\":82252},{\"end\":82273,\"start\":82262},{\"end\":82286,\"start\":82273},{\"end\":82296,\"start\":82286},{\"end\":82313,\"start\":82296},{\"end\":82773,\"start\":82760},{\"end\":82782,\"start\":82773},{\"end\":82795,\"start\":82782},{\"end\":82808,\"start\":82795},{\"end\":83129,\"start\":83119},{\"end\":83140,\"start\":83129},{\"end\":83152,\"start\":83140},{\"end\":83639,\"start\":83632},{\"end\":83646,\"start\":83639},{\"end\":83655,\"start\":83646},{\"end\":83670,\"start\":83655},{\"end\":83683,\"start\":83670},{\"end\":83692,\"start\":83683},{\"end\":84249,\"start\":84239},{\"end\":84264,\"start\":84249},{\"end\":84482,\"start\":84471},{\"end\":84499,\"start\":84482},{\"end\":84508,\"start\":84499},{\"end\":84975,\"start\":84968},{\"end\":84990,\"start\":84975},{\"end\":84999,\"start\":84990},{\"end\":85006,\"start\":84999},{\"end\":85015,\"start\":85006},{\"end\":85341,\"start\":85327},{\"end\":85359,\"start\":85341},{\"end\":85676,\"start\":85670},{\"end\":85683,\"start\":85676},{\"end\":85695,\"start\":85683},{\"end\":85704,\"start\":85695},{\"end\":85714,\"start\":85704},{\"end\":85727,\"start\":85714},{\"end\":85738,\"start\":85727},{\"end\":85747,\"start\":85738},{\"end\":85758,\"start\":85747},{\"end\":85766,\"start\":85758},{\"end\":86146,\"start\":86134},{\"end\":86156,\"start\":86146},{\"end\":86166,\"start\":86156},{\"end\":86175,\"start\":86166},{\"end\":86190,\"start\":86175},{\"end\":86558,\"start\":86549},{\"end\":86567,\"start\":86558},{\"end\":86574,\"start\":86567},{\"end\":86582,\"start\":86574},{\"end\":86592,\"start\":86582},{\"end\":86942,\"start\":86929},{\"end\":86954,\"start\":86942},{\"end\":86963,\"start\":86954},{\"end\":86972,\"start\":86963},{\"end\":86980,\"start\":86972},{\"end\":86994,\"start\":86980},{\"end\":87005,\"start\":86994},{\"end\":87017,\"start\":87005},{\"end\":87431,\"start\":87420},{\"end\":87440,\"start\":87431},{\"end\":87451,\"start\":87440},{\"end\":87468,\"start\":87451},{\"end\":87478,\"start\":87468},{\"end\":87487,\"start\":87478},{\"end\":87500,\"start\":87487},{\"end\":87932,\"start\":87922},{\"end\":87942,\"start\":87932},{\"end\":87951,\"start\":87942},{\"end\":87959,\"start\":87951},{\"end\":88346,\"start\":88333},{\"end\":88355,\"start\":88346},{\"end\":88363,\"start\":88355},{\"end\":88377,\"start\":88363},{\"end\":88388,\"start\":88377},{\"end\":88400,\"start\":88388},{\"end\":88720,\"start\":88707},{\"end\":88732,\"start\":88720},{\"end\":88744,\"start\":88732},{\"end\":88754,\"start\":88744},{\"end\":88767,\"start\":88754},{\"end\":88781,\"start\":88767},{\"end\":89102,\"start\":89092},{\"end\":89111,\"start\":89102},{\"end\":89119,\"start\":89111},{\"end\":89132,\"start\":89119},{\"end\":89451,\"start\":89445},{\"end\":89458,\"start\":89451},{\"end\":89475,\"start\":89458},{\"end\":89494,\"start\":89475},{\"end\":89503,\"start\":89494},{\"end\":89509,\"start\":89503},{\"end\":89516,\"start\":89509},{\"end\":89963,\"start\":89956},{\"end\":89982,\"start\":89963},{\"end\":89994,\"start\":89982},{\"end\":90004,\"start\":89994},{\"end\":90015,\"start\":90004},{\"end\":90029,\"start\":90015},{\"end\":90046,\"start\":90029},{\"end\":90389,\"start\":90376},{\"end\":90401,\"start\":90389},{\"end\":90411,\"start\":90401},{\"end\":90421,\"start\":90411},{\"end\":90430,\"start\":90421},{\"end\":90759,\"start\":90742},{\"end\":90780,\"start\":90759},{\"end\":90789,\"start\":90780},{\"end\":90811,\"start\":90789},{\"end\":90839,\"start\":90811},{\"end\":91206,\"start\":91197},{\"end\":91217,\"start\":91206},{\"end\":91227,\"start\":91217},{\"end\":91237,\"start\":91227},{\"end\":91245,\"start\":91237},{\"end\":91254,\"start\":91245},{\"end\":91267,\"start\":91254},{\"end\":91276,\"start\":91267},{\"end\":91284,\"start\":91276},{\"end\":91293,\"start\":91284},{\"end\":91582,\"start\":91570},{\"end\":91590,\"start\":91582},{\"end\":91599,\"start\":91590},{\"end\":91762,\"start\":91749},{\"end\":91773,\"start\":91762},{\"end\":91787,\"start\":91773},{\"end\":91798,\"start\":91787},{\"end\":92364,\"start\":92352},{\"end\":92379,\"start\":92364},{\"end\":93092,\"start\":93080},{\"end\":93106,\"start\":93092},{\"end\":93337,\"start\":93327},{\"end\":93344,\"start\":93337},{\"end\":93353,\"start\":93344},{\"end\":93360,\"start\":93353},{\"end\":93765,\"start\":93753},{\"end\":93772,\"start\":93765},{\"end\":93783,\"start\":93772},{\"end\":93793,\"start\":93783},{\"end\":93806,\"start\":93793},{\"end\":93813,\"start\":93806},{\"end\":93825,\"start\":93813},{\"end\":93834,\"start\":93825},{\"end\":93847,\"start\":93834},{\"end\":93858,\"start\":93847},{\"end\":94331,\"start\":94324},{\"end\":94348,\"start\":94331},{\"end\":94361,\"start\":94348},{\"end\":94824,\"start\":94818},{\"end\":94832,\"start\":94824},{\"end\":94839,\"start\":94832},{\"end\":94848,\"start\":94839},{\"end\":94855,\"start\":94848},{\"end\":94863,\"start\":94855},{\"end\":94878,\"start\":94863},{\"end\":94892,\"start\":94878},{\"end\":95378,\"start\":95369},{\"end\":95384,\"start\":95378},{\"end\":95399,\"start\":95384},{\"end\":95409,\"start\":95399},{\"end\":95417,\"start\":95409},{\"end\":95428,\"start\":95417},{\"end\":95438,\"start\":95428},{\"end\":95450,\"start\":95438},{\"end\":95460,\"start\":95450},{\"end\":95466,\"start\":95460},{\"end\":95957,\"start\":95947},{\"end\":95968,\"start\":95957},{\"end\":95978,\"start\":95968},{\"end\":95990,\"start\":95978},{\"end\":96002,\"start\":95990},{\"end\":96014,\"start\":96002},{\"end\":96022,\"start\":96014},{\"end\":96032,\"start\":96022},{\"end\":96044,\"start\":96032},{\"end\":96053,\"start\":96044},{\"end\":96549,\"start\":96540},{\"end\":96556,\"start\":96549},{\"end\":96565,\"start\":96556},{\"end\":96577,\"start\":96565},{\"end\":96584,\"start\":96577},{\"end\":96598,\"start\":96584},{\"end\":96608,\"start\":96598},{\"end\":96997,\"start\":96991},{\"end\":97008,\"start\":96997},{\"end\":97290,\"start\":97277},{\"end\":97301,\"start\":97290},{\"end\":97674,\"start\":97656},{\"end\":97687,\"start\":97674},{\"end\":97696,\"start\":97687},{\"end\":98057,\"start\":98047},{\"end\":98067,\"start\":98057},{\"end\":98078,\"start\":98067},{\"end\":98089,\"start\":98078},{\"end\":98099,\"start\":98089},{\"end\":98114,\"start\":98099},{\"end\":98124,\"start\":98114},{\"end\":98136,\"start\":98124},{\"end\":98153,\"start\":98136},{\"end\":98163,\"start\":98153},{\"end\":98465,\"start\":98454},{\"end\":98475,\"start\":98465},{\"end\":98485,\"start\":98475},{\"end\":98500,\"start\":98485},{\"end\":98512,\"start\":98500},{\"end\":98888,\"start\":98877},{\"end\":98895,\"start\":98888},{\"end\":98905,\"start\":98895},{\"end\":98917,\"start\":98905},{\"end\":98926,\"start\":98917},{\"end\":99320,\"start\":99308},{\"end\":99330,\"start\":99320},{\"end\":99340,\"start\":99330},{\"end\":99350,\"start\":99340},{\"end\":99359,\"start\":99350},{\"end\":99371,\"start\":99359},{\"end\":99705,\"start\":99694},{\"end\":99712,\"start\":99705},{\"end\":99719,\"start\":99712},{\"end\":99732,\"start\":99719},{\"end\":99741,\"start\":99732},{\"end\":100090,\"start\":100080},{\"end\":100096,\"start\":100090},{\"end\":100103,\"start\":100096},{\"end\":100414,\"start\":100406},{\"end\":100422,\"start\":100414},{\"end\":100430,\"start\":100422},{\"end\":100436,\"start\":100430},{\"end\":100444,\"start\":100436},{\"end\":100806,\"start\":100797},{\"end\":100816,\"start\":100806},{\"end\":101203,\"start\":101194},{\"end\":101213,\"start\":101203},{\"end\":101220,\"start\":101213},{\"end\":101229,\"start\":101220},{\"end\":101238,\"start\":101229},{\"end\":101247,\"start\":101238}]", "bib_venue": "[{\"end\":65104,\"start\":65073},{\"end\":65527,\"start\":65495},{\"end\":65907,\"start\":65876},{\"end\":66190,\"start\":66172},{\"end\":66391,\"start\":66356},{\"end\":66648,\"start\":66644},{\"end\":66840,\"start\":66763},{\"end\":67275,\"start\":67237},{\"end\":67687,\"start\":67605},{\"end\":68041,\"start\":68033},{\"end\":68433,\"start\":68331},{\"end\":69023,\"start\":68957},{\"end\":69412,\"start\":69348},{\"end\":69757,\"start\":69628},{\"end\":70296,\"start\":70221},{\"end\":70790,\"start\":70731},{\"end\":71251,\"start\":71215},{\"end\":71700,\"start\":71618},{\"end\":72181,\"start\":72097},{\"end\":72736,\"start\":72664},{\"end\":73360,\"start\":73288},{\"end\":73942,\"start\":73870},{\"end\":74473,\"start\":74391},{\"end\":74966,\"start\":74889},{\"end\":75485,\"start\":75415},{\"end\":75967,\"start\":75921},{\"end\":76203,\"start\":76186},{\"end\":76454,\"start\":76440},{\"end\":76667,\"start\":76653},{\"end\":76910,\"start\":76894},{\"end\":77096,\"start\":77058},{\"end\":77425,\"start\":77413},{\"end\":77833,\"start\":77751},{\"end\":78435,\"start\":78357},{\"end\":78973,\"start\":78952},{\"end\":79337,\"start\":79312},{\"end\":79672,\"start\":79628},{\"end\":80132,\"start\":80061},{\"end\":80583,\"start\":80501},{\"end\":81022,\"start\":81010},{\"end\":81417,\"start\":81385},{\"end\":81870,\"start\":81770},{\"end\":82382,\"start\":82313},{\"end\":82824,\"start\":82808},{\"end\":83235,\"start\":83152},{\"end\":83800,\"start\":83692},{\"end\":84270,\"start\":84264},{\"end\":84582,\"start\":84508},{\"end\":84966,\"start\":84871},{\"end\":85395,\"start\":85359},{\"end\":85784,\"start\":85766},{\"end\":86225,\"start\":86190},{\"end\":86648,\"start\":86592},{\"end\":87077,\"start\":87017},{\"end\":87577,\"start\":87500},{\"end\":88027,\"start\":87959},{\"end\":88405,\"start\":88400},{\"end\":88787,\"start\":88781},{\"end\":89167,\"start\":89132},{\"end\":89603,\"start\":89516},{\"end\":90056,\"start\":90046},{\"end\":90448,\"start\":90430},{\"end\":90861,\"start\":90839},{\"end\":91195,\"start\":91118},{\"end\":91568,\"start\":91560},{\"end\":91858,\"start\":91798},{\"end\":92153,\"start\":92079},{\"end\":92393,\"start\":92379},{\"end\":92586,\"start\":92567},{\"end\":92709,\"start\":92677},{\"end\":92809,\"start\":92791},{\"end\":92883,\"start\":92871},{\"end\":92969,\"start\":92946},{\"end\":93120,\"start\":93106},{\"end\":93431,\"start\":93360},{\"end\":93936,\"start\":93858},{\"end\":94445,\"start\":94361},{\"end\":94970,\"start\":94892},{\"end\":95521,\"start\":95466},{\"end\":96116,\"start\":96053},{\"end\":96674,\"start\":96608},{\"end\":97042,\"start\":97008},{\"end\":97365,\"start\":97301},{\"end\":97749,\"start\":97696},{\"end\":98045,\"start\":97968},{\"end\":98590,\"start\":98528},{\"end\":98979,\"start\":98926},{\"end\":99389,\"start\":99371},{\"end\":99794,\"start\":99741},{\"end\":100174,\"start\":100103},{\"end\":100526,\"start\":100444},{\"end\":100898,\"start\":100816},{\"end\":101261,\"start\":101247},{\"end\":68522,\"start\":68435},{\"end\":69076,\"start\":69025},{\"end\":69873,\"start\":69759},{\"end\":70358,\"start\":70298},{\"end\":70836,\"start\":70792},{\"end\":72252,\"start\":72183},{\"end\":72795,\"start\":72738},{\"end\":73419,\"start\":73362},{\"end\":74001,\"start\":73944},{\"end\":75542,\"start\":75487},{\"end\":83305,\"start\":83237},{\"end\":83895,\"start\":83802},{\"end\":84643,\"start\":84584},{\"end\":86691,\"start\":86650},{\"end\":94516,\"start\":94447}]"}}}, "year": 2023, "month": 12, "day": 17}