{"id": 258762579, "updated": "2023-10-05 00:42:00.11", "metadata": {"title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks", "authors": "[{\"first\":\"Wenhai\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Zhe\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Xiaokang\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Jiannan\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Xizhou\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Gang\",\"last\":\"Zeng\",\"middle\":[]},{\"first\":\"Ping\",\"last\":\"Luo\",\"middle\":[]},{\"first\":\"Tong\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Jie\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Yu\",\"last\":\"Qiao\",\"middle\":[]},{\"first\":\"Jifeng\",\"last\":\"Dai\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Large language models (LLMs) have notably accelerated progress towards artificial general intelligence (AGI), with their impressive zero-shot capacity for user-tailored tasks, endowing them with immense potential across a range of applications. However, in the field of computer vision, despite the availability of numerous powerful vision foundation models (VFMs), they are still restricted to tasks in a pre-defined form, struggling to match the open-ended task capabilities of LLMs. In this work, we present an LLM-based framework for vision-centric tasks, termed VisionLLM. This framework provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-centric tasks with language tasks that can be flexibly defined and managed using language instructions. An LLM-based decoder can then make appropriate predictions based on these instructions for open-ended tasks. Extensive experiments show that the proposed VisionLLM can achieve different levels of task customization through language instructions, from fine-grained object-level to coarse-grained task-level customization, all with good results. It's noteworthy that, with a generalist LLM-based framework, our model can achieve over 60\\% mAP on COCO, on par with detection-specific models. We hope this model can set a new baseline for generalist vision and language models. The demo shall be released based on https://github.com/OpenGVLab/InternGPT. The code shall be released at https://github.com/OpenGVLab/VisionLLM.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2305.11175", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-11175", "doi": "10.48550/arxiv.2305.11175"}}, "content": {"source": {"pdf_hash": "42a30dc5470f54ec249f25d3c31e05d7c376c8e3", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.11175v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "5fc0abfb3549228f01150d71e74f804644b6b784", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/42a30dc5470f54ec249f25d3c31e05d7c376c8e3.txt", "contents": "\nVisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks\n\n\nWenhai Wang \nOpenGVLab\nShanghai\n\nAI Laboratory\n\n\nZhe Chen \nOpenGVLab\nShanghai\n\nAI Laboratory\n\n\nNanjing University\n\n\nXiaokang Chen \nPeking University\n\n\nJiannan Wu \nOpenGVLab\nShanghai\n\nAI Laboratory\n\n\nThe University of HongKong\n\n\nXizhou Zhu \nOpenGVLab\nShanghai\n\nAI Laboratory\n\n\nSenseTime Research\n\n\nGang Zeng \nOpenGVLab\nShanghai\n\nAI Laboratory\n\n\nPeking University\n\n\nPing Luo \nOpenGVLab\nShanghai\n\nAI Laboratory\n\n\nThe University of HongKong\n\n\nTong Lu \nNanjing University\n\n\nJie Zhou \nTsinghua University\n\n\nYu Qiao \nOpenGVLab\nShanghai\n\nAI Laboratory\n\n\nJifeng Dai \nOpenGVLab\nShanghai\n\nAI Laboratory\n\n\nTsinghua University\n\n\nVisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks\n\nLarge language models (LLMs) have notably accelerated progress towards artificial general intelligence (AGI), with their impressive zero-shot capacity for user-tailored tasks, endowing them with immense potential across a range of applications. However, in the field of computer vision, despite the availability of numerous powerful vision foundation models (VFMs), they are still restricted to tasks in a pre-defined form, struggling to match the open-ended task capabilities of LLMs. In this work, we present an LLM-based framework for vision-centric tasks, termed VisionLLM. This framework provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-centric tasks with language tasks that can be flexibly defined and managed using language instructions. An LLM-based decoder can then make appropriate predictions based on these instructions for open-ended tasks. Extensive experiments show that the proposed VisionLLM can achieve different levels of task customization through language instructions, from fine-grained object-level to coarse-grained task-level customization, all with good results. It's noteworthy that, with a generalist LLMbased framework, our model can achieve over 60% mAP on COCO, on par with detection-specific models. We hope this model can set a new baseline for generalist vision and language models. The code and demo shall be released.\n\nIntroduction\n\nThe emergence of large language models (LLMs) like ChatGPT [41] has revolutionized the landscape of artificial general intelligence (AGI), showcasing their impressive zero-shot capabilities in addressing various natural language processing (NLP) tasks through user-tailored prompts or language instructions. Despite these advancements, it's essential to note that the triumph of LLMs does not effortlessly extend to pure vision and vision-language tasks, due to the inherent disparities between modalities and task formats.\n\nThe field of computer vision presents a unique set of challenges and paradigms that differ from those of NLP. The traditional paradigm of vision foundation models is pre-training followed by fine-tuning [59,12,51,61,18,52], which is effective but comes with significant marginal costs when adapting to diverse downstream scenarios. As shown in Figure 1a, while approaches such as multi-task unification [44,58,1,57,81] have been used to achieve generalist capability, they often struggle to overcome the limitations imposed by pre-defined tasks, resulting in a gap in open-ended\n\n\nVision Generalist Model\n\nPre-defined tasks: detection, captioning, VQA, grounding, ...\n\n(a) Vision generalist models [59,61,83] are constrained by the format of pre-defined tasks.\n\n\nVisual Prompt Tuning\n\n(b) Visual prompt tuning [26,64,62] are inconsistent with the format of LLMs.\n\n\nVision + LLM\n\n\nTask defined by instruc6ons\n\nDesired output: <c1> <p1> <p3> ...  task capabilities compared to LLMs. Recently, visual prompt tuning [26,74,79,76,62] has emerged as a way to flexibly outline some pure vision tasks (see Figure 1b), such as object detection, instance segmentation, and pose estimation, using visual masking. However, the format of visual prompts considerably deviates from that of language instructions, making it challenging to directly apply the reasoning abilities and world knowledge of LLMs to vision tasks. Therefore, there is an urgent need for a unified generalist framework that can seamlessly integrate the strengths of LLMs with the specific requirements of vision-centric tasks.\n\nIn this work, we present VisionLLM, a novel framework that aligns the definitions of vision-centric tasks with the methodologies of LLMs. Leveraging the reasoning and parsing capacities of LLMs, VisionLLM is designed to empower open-ended task capabilities for vision-centric tasks. Specifically, it comprises three core components: (1) a unified language instruction designed for vision and vision-language tasks, (2) a language-guided image tokenizer, and (3) an LLM-based open-ended task decoder that orchestrates various tasks using language instructions. With this framework, a wide range of vision-centric tasks can be seamlessly integrated, including object detection, instance segmentation, image captioning, and visual grounding. In addition, the framework also facilitates task customization at different levels of granularity, allowing for the customization of target objects, output formats, task descriptions, etc.\n\nCompared to current popular API-based applications [68,73,50,35,30], our model takes a unified, end-to-end approach to integrate VFMs and LLMs, streamlining and enhancing the overall efficiency of the overall process, and leveraging the strengths and data of both VFMs and LLMs within a single, cohesive system. Furthermore, our model surpasses the limitations of generalist vision models pre-trained on pre-defined tasks. VisionLLM can effectively manage vision-centric tasks through language instructions, embodying a flexible and open-ended approach that is not constrained by pre-set tasks. This versatility makes VisionLLM a robust and powerful generalist model for vision and vision-language tasks, opening up new possibilities for the development of unified generalist models that bridge the domains of vision and language.\n\nIn summary, our main contributions are as follows:\n\n(1) We propose VisionLLM, the first framework that leverages the power of LLMs to address visioncentric tasks in an open-ended and customizable manner. By aligning the definitions of vision-centric tasks with LLM methodologies, VisionLLM breaks new ground in enabling the unified modeling of vision and language, opening up possibilities for advancing the field.\n\n(2) We overcome many difficulties when porting LLMs to vision-centric tasks, by designing unified language instruction that matches the format of language models and covers various vision-centric tasks including visual perception. Correspondingly, we develop a language-guided image tokenizer and an LLM-based task decoder that can handle open-ended tasks according to the given language instructions based on the LLMs' reasoning and parsing capabilities.  [(<c0>, 135.3, 95.7, 123.4, 53.4, 84.9, 57.6, 66.8, 60.5, 60.1, 72.3, 34.2, 71.4, ..., 124.9,   (3) We construct a series of tasks with different granularities to verify the effectiveness of our models, ranging from easy to hard, and from pre-defined to flexible. Through these validations, we demonstrate the remarkable generality of our models, showcasing their ability to handle diverse scenarios, including random object categories, random output formats, and random task descriptions, as shown in Figure 2. The successful outcomes of these validations underscore the tremendous potential of our model in harnessing the capabilities of LLMs to control and guide vision-centric tasks. In addition, with a generalist LLM-based framework, our model also yields promising results on various vision-centric tasks. Notably, our generalist model achieves an impressive mAP score of 60+% on the COCO dataset, surpassing many detection-specific models [82,7,22] and approaching the state-of-the-art record.\n\n\nRelated Work\n\n\nLarge Language Model\n\nLarge language models (LLMs) have gained significant attention in the field of natural language processing (NLP) and artificial general intelligence (AGI), due to their impressive capabilities in language generation, in-context learning, world knowledge, and reasoning. The GPT family, including GPT-3 [6], ChatGPT [41], GPT-4 [40], and InstructGPT [42] are most representative works of LLMs. Other LLMs like OPT [78], LLaMA [54], MOSS [15], and GLM [77] have also made substantial contributions to the field. These models achieve high performance and are open-sourced, serving as valuable resources for training large models and as foundations for further fine-tuning for specific purposes. For instance, Alpaca [53] introduces a self-instruct framework that facilitates instruction tuning of the LLaMA model, reducing the reliance on human-written instruction data. Recently, the emergence of these LLMs has also opened up API-based applications for solving vision-centric tasks. These applications have integrated visual APIs with language models to enable decision-making or planning based on visual information, such as Visual ChatGPT [68], MM-REACT [73], HuggingGPT [50], InternGPT [35], and VideoChat [30]. However, despite the convenience of using language-based instructions to define tasks and describe visual elements, these interactive systems [68,73,50,35,30] still face limitations in capturing fine-grained visual details and understanding complex visual contexts, which hinder their ability to effectively connecting vision and language models. In summary, while LLMs have shown tremendous potential in various NLP applications, their applicability to vision-centric tasks has been limited by the challenges posed by modalities and task formats.\n\n\nVision Generalist Model\n\nThe pursuit of generalist models [83,38,70], which aim to handle a wide range of tasks using a shared architecture and parameters, has been a long-standing goal in the machine learning community. Inspired by the success of sequence-to-sequence (seq2seq) models in the field of NLP [44], recent advancements such as OFA [58], Flamingo [1], and GIT [57] propose modeling diverse tasks as sequence generation tasks. Unified-IO [38], Pix2Seq v2 [9], and UniTab [71] extend this idea by using discrete coordinate tokens to encode and decode spatial information for more tasks. Gato [47] also incorporates reinforcement learning tasks into the seq2seq framework, while GPV [21] develops a general-purpose vision system by combining a seq2seq module with a DETR-based visual encoder [7]. However, these methods suffer from some limitations, such as slow inference speed and performance degradation due to the non-parallel auto-regressive decoding process. Uni-Perceivers [83,81,28] solve these issues by unifying different tasks using the maximum likelihood target for each input based on representation similarity, regardless of their modality, making it possible to support both generation and non-generation tasks in a unified framework. Nevertheless, these generalist models are still restricted by pre-defined tasks and cannot support flexible open-ended task customization based on language instructions like LLMs.\n\n\nInstruction Tuning\n\nLanguage instructions are a powerful way to express various NLP tasks and examples for LLMs, as introduced by GPT-3 [6]. Following this idea, subsequent works, such as InstructGPT [42], FLAN [14,67], and OPT-IML [25], explore the instruction-tuning method [66,65] and demonstrate that this simple approach effectively enhances the zero-shot and few-shot capabilities of LLMs. The language instruction paradigm has also been adopted by the computer vision community to define image-to-text tasks. Flamingo [1] is a milestone work that uses vision and language inputs as prompts and achieves remarkable few-shot results in various vision-language tasks, such as image captioning [10] and VQA [3]. BLIP-2 [29] further connects the visual encoder with LLMs through a querying transformer and a linear projection layer to build strong multimodal models. MiniGPT-4 [80] and LLaVA [33] finetune the BLIP-2-style models on synthetic multimodal instruction-following data to unleash the potential of LLMs. However, these models mainly focus on image-to-text tasks and fail to address visual perception, such as object detection, instance segmentation, pose estimation, etc. To tackle image inpainting tasks, Bar et al. [4] introduces the first visual prompting framework that utilizes inpainting with discrete tokens on images. Painter [63] and SegGPT [64] employ masked image modeling on raw pixels for in-context learning with paired images. While these visual prompt models demonstrate good results in segmentation tasks, their applicability to numerous real-world vision tasks is challenging. Moreover, defining the visual prompts as image inpainting is inconsistent with the language instructions in LLMs, hard to leverage the reasoning, parsing ability, and world knowledge of LLMs. In this work, we aim to align vision-centric tasks with language tasks, use language instructions to unifiedly and flexibly define all tasks, and solve them with a shared LLM-based task decoder.\n\n\nVisionLLM\n\n\nOverall Architecture\n\nThis work targets to provide a unified generalist framework that can seamlessly integrate the strengths of large language models (LLMs) with the specific requirements of vision-centric tasks. As shown in  (2) a language-guided image tokenizer, which encodes visual information in alignment with the given language prompt, enabling the model to comprehend and parse the visual content effectively; and (3) an LLM-based open-task decoder, which utilizes the encoded visual information and language instructions to generate satisfactory predictions or outputs. The three designs work together to achieve a flexible and open-ended framework that can handle various vision-centric tasks at different levels of task customization through language instructions. Different from previous interactive systems [68,73,50,35,30] that rely on APIs, our VisionLLM presents a more flexible and end-to-end pipeline. Given language instructions that describe the current tasks and an input image, the model first uses a language-guided image tokenizer to encode the image tokens based on the given prompt. Then, the image tokens and language instructions are fed to an LLM-based open-ended task decoder. Finally, it evaluates the generated outputs against the task definition given by the unified language instructions, enabling the model to produce task-specific results. This seamless, end-to-end pipeline enables VisionLLM to effectively combine vision and language, achieving remarkable performance in open-ended and customizable vision-centric tasks.\n\n\nUnified Language Instruction\n\nWe first introduce unified language instructions to describe vision-centric tasks. This design enables the unification of various vision-only and vision-language task descriptions and allows for flexible task customization.\n\nVision-Language Tasks. The instructions for vision-language tasks such as image captioning and visual question answering (VQA) are straightforward and similar to NLP tasks. Following previous methods [29,83,33], we describe the image captioning task like \"The image is <image>. Please generate a caption for the image: \", and the VQA task like \"The image is <image>. Please generate an answer for the image according to the question: <question>\". Here, <image> and <question> are the placeholdersok of the image tokens and the question, respectively.\n\nVision-Only Tasks. Designing effective language instructions for vision tasks is a challenging endeavor due to the differences in modality and task format between vision and language. Here, we describe vision tasks by providing a task description and specifying the desired output format via language instructions.\n\n(1) The task description conveys the intended task to the language model. Following self-instruct [65], we design a set of seed instructions with placeholders and employ LLMs to generate a large number of related task descriptions and randomly select one of them during training.\n\n(2) For conventional visual perception tasks like object detection and instance segmentation, we propose a unified output format represented as a tuple (C, P ), where C denotes the class index in the category set <class>, and P = {x i , y i } N i=1 represents N points that locate the object. To align with the format of word tokens, both the class index C and the coordinates of points x i , y i are transformed into discretized tokens. Specifically, the class index is an integer starting from 0, and the continuous coordinates of the points are uniformly discretized into an integer within the range [-<range>, <range>]. For object detection and visual grounding tasks, the point number N is equal to 2, representing the the top-left and bottom-right points of object's bounding box. In the case of instance segmentation, we employ multiple (N > 8) points along the object boundary to represent an instance mask [69]. Other perception tasks such as pose estimation (keypoint detection) can also be formulated as language instructions in this way.\n\nAn example of language instruction for the instance segmentation task is as follows: \"Segment all the objects of category set <class> within the <range> of the image and generate a list of the format (c, x1, y1, x2, y2, ..., x8, y8). Here, c represents the index of the class label starting from 0, and (x1, y1, x2, y2, ..., x8, y8) correspond to the offsets of boundary points of the object relative to the center point. The image is: <image>\".\n\n\nLanguage-Guided Image Tokenizer\n\nVisionLLM considers images as a kind of foreign language and converts them into token representations. Unlike previous works [17,60,34] that utilize fixed-size patch embeddings to represent images, we introduce the language-guided image tokenizer to flexibly encode visual information that aligns with task-specific language prompts or instructions.\n\nSpecifically, give an image X \u2208 R H\u00d7W \u00d73 with height H and width W , we first feed it to the image backbones (e.g., ResNet [23]) and extract visual features F v of four different scales. Additionally, we leverage a text encoder (e.g., BERT [16]) to extract the language features F l from given prompts. The language features are then injected into each scale of visual features through crossattention [55], yielding multi-scale language-aware visual features, enabling the alignment of features across modalities.\n\nAfterward, we propose to adopt a transformer-based network (e.g., Deformable DETR [82]) with M random-initialized queries Q = {q i } M i=1 to capture the high-level information of images. We build the transformer-based network on top of the multi-scale language-aware visual features to extract M image tokens T = {(e i , l i )} M i=1 , each of which is represented by an embedding e i and a location l i , denoting the semantic and positional information of the token. This design not only represents the images independent of input resolution but also extracts the visual representation that is informative with respect to the language prompts.\n\n\nLLM-based Open-Ended Task Decoder\n\nWe build our decoder on Alpaca [53], an LLM that is adapted from LLaMA [54], to handle various vision-related tasks with language guidance. However, Alpaca has some inherent drawbacks for vision-centric tasks, such as (1) It only has a few digit tokens (e.g., 0\u223c9) in its vocabulary, which restricts its ability to locate objects by numbers; (2) It uses multiple tokens to represent the category name, resulting in an inefficient scheme in object classification; and (3) It is a causal model that is inefficient for visual perception tasks.\n\nTo tackle these issues, we expand the vocabulary of LLM with additional tokens specially designed for vision-centric tasks. First, we add a set of location tokens, denoted as {<p-512>, ..., <p0>, ..., <p512>}, where <p i> represents the discretized offset of i \u2208 [\u2212512, 512] to the location l i of the image token, and the relative value to image height or width is equal to i/512. These tokens successfully transform the object localization task from continuous variable prediction to more unified discrete bin classification. Second, we introduce semantics-agnostic classification tokens {<c0>, <c1>, ..., <c511>} to replace category name tokens, which overcomes the inefficiency of using multiple tokens to represent categories. The mapping between category names and the classification tokens is flexibly provided in the category set <class> of language instructions, such as {\"person\":<c0>, \"car\":<c1>, \"black cat\":<c2>,...}. This design allows our model to select the appropriate category name from the provided category set, facilitating efficient and accurate object classification.\n\n\nLLM-based Open-Ended Task Decoder\n\nTask defined by instructions parsing format 1: \"< cls > <x1> <y1> ...\" format 2: \"<bos>\" ... format n: ... Figure 4: Illustration of the \"output-format-asquery\" decoding process. \"<cls> <x1> <y1> ...\" denote the queries of the object's class index and boundary points, and \"<bos>\" denotes the beginning of string.\n\nMoreover, to address the inefficiency caused by the causal framework, we introduce outputformat-as-query decoding. We first use LLMs to parse the structural output format from the task instructions (e.g., \"<cls> <x1> <y1> <x2> <y2>\" for object detection, \"<bos>\" for image captioning), and then feed the tokens of structural output format as queries to the decoder to generate the desired output according to the queries. This simple method enables our model to not only avoid inefficient token-by-token decoding in visual perception tasks, but also keep a unified framework for vision-language tasks.\n\nIn this way, the output of object location and classification is formulated as a foreign language, thus unifying these vision-centric tasks into the format of token classification. Therefore, both vision-language and vision-only tasks can be supervised with the cross entropy loss like language tasks. In addition, for efficient training, we adopt the Low-Rank Adaptation (LoRA) approach [24], which allows us to train and fine-tune the models without excessive computational costs. It also acts as a bridge between the language and visual tokens, facilitating effective alignment between the two modalities, ensuring better task customization, and improving the convergence of the overall system.\n\n\nExperiment\n\n\nExperimental Settings\n\nDatasets. VisionLLM unifies the output formats of vision and language tasks as vocabulary generation, which enables models to be jointly trained on a wide range of tasks. In the experiments, we investigate the general modeling capacities of VisionLLM on five vision-centric tasks, including object detection, instance segmentation, visual grounding, image captioning, and visual question answering. For object detection and instance segmentation, COCO2017 [32] is used for training and evaluation. For visual grounding, we combine the annotations of RefCOCO [75], RefCOCO+ [75] and RefCOCOg [39] for training, resulting in over 120k referred objects in total. And our models are evaluated on the validation set of RefCOCO. For image captioning and visual question answering, we adopt COCO Caption [10] and LLaVA-Instruct-150K [33] as the training source. We evaluate the image captioning performance on the COCO Karpathy test split following common practice [28,58,71]. We mainly use qualitative results (see Figure 2d) to demonstrate the VQA capability of our model, as LLaVA-Instruct-150K is not compatible with the standard VQA benchmark. These tasks differ in their granularity, ranging from coarse-grained image level to fine-grained pixel level, enabling a comprehensive evaluation of the model's ability to adapt to different levels of customization through language instructions. Implementation Details. We implement two variants of VisionLLM with two image backbones, i.e., ResNet [23] and InternImage-H [59]. For the language-guided image tokenizer, we adopt BERT-Base [5] as the text encoder and Deformable DETR (D-DETR) [82] to capture high-level information.\n\nWe set the number of queries M to 100, and the number of encoder/decoder layers to 6 for D-DETR.\n\nFor the LLM, we employ Alpaca-7B [53], a LLaMA [54] model fine-tuned with instructions, and equip it with LoRA [24] for parameter-efficient fine-tuning.\n\nThe model is trained in two stages. In the first stage, we initialize the model with the pre-trained weights of D-DETR, BERT, and Alpaca-7B, and train the visual backbone and the language-guided image tokenizer, while freezing most parameters of the LLM except a few LoRA parameters. To simplify the training complexity, in this stage, we mainly focus on object detection tasks with random object categories and task descriptions. In the second stage, we freeze the visual backbone and introduce the unified supervision of multiple tasks. Unless otherwise specified, the training runs for 50 epochs on 4 \u00d7 8 NVIDIA A100 GPUs. AdamW [36] is used as the optimizer, with one sample per GPU. We employ the cosine annealing schedule [37] as the learning policy, with an initial learning  [11] ResNet-101 -\n- - - - - - 81.4 - - VILLA [19] ResNet-101 - - - - - - - 82.4 - - MDETR [27] ResNet-101 - - - - - - - 86.8 - - VL-T5 [13] T5-B - - - - - - - - - 116.5 Generalist Models UniTab [72] ResNet-101 - - - - - - - 88.6 - 115.8 Uni-Perceiver [83] ViT-B - - - - - - - - 32.0 - Uni-Perceiver-MoE [81] ViT-B - - - - - - - - 33.2 - Uni-Perceiver-V2 [28]\nViT-B -58. rate of 2 \u00d7 10 \u22124 . In addition to the experiments in the main paper, more experimental settings and ablation studies are provided in the supplementary material due to space limitations.\n\n\nTask-Level Customization\n\nWe first evaluate the task-level customization capability of VisionLLM. VisionLLM supports coarsegrained task customization, including visual perception tasks and visual-language tasks. Table 1 presents the evaluation results on four standard vision-centric tasks, including object detection, instance segmentation, visual grounding, and image captioning. We compare our model with taskspecific methods as well as recently-proposed vision generalist models. Note that, unless specifically mentioned, the results of our model come from a shared-parameter generalist model and switch different tasks by changing the language instructions only. Detailed instructions could be found in the supplementary material.\n\nObject Detection. Object detection is a fundamental computer vision task that involves identifying and localizing objects of interest within an image. Our method achieves comparable or higher results to others, 44.6 mAP, with a ResNet-50 [23] backbone. With the same backbone i.e. ResNet-50, our method outperforms Pix2Seq [8] by 1.4 mAP, which also discretizes the output coordinates to integers. Furthermore, benefiting from the output-format-as-query framework (see Sec. 3.4), we can decode multiple predictions in parallel during inference, making our approach more efficient. Using InternImage-H [59] as the visual backbone, we obtained 60.2% mAP, which is close to the current state-of-the-art detection-specific model [59], demonstrating the scalability of our generalist model.\n\nVisual Grounding. Visual grounding associates textual descriptions with corresponding regions or objects within an image. Training visual grounding and object detection can potentially conflict with each other, as object detection aims to detect all the objects, while visual grounding should only localize the referred object and suppress other objects. Benefiting from our unified task instructions and the strong instruction comprehension capabilities of LLMs, our model performs both tasks effectively and achieves a result of 80.6 P@0.5 for visual grounding. With InternImage-H as the backbone, we achieve 86.7 P@0.5 on the validation set of RefCOCO.\n\nInstance Segmentation. Instance segmentation involves identifying and segmenting individual objects within an image. We employ a flexible number of points (i.e., 8\u223c24) along the object boundary to represent an instance mask. Compared to mainstream models specific to instance segmentation, our model has a comparable mask AP 50 (61.2% with InternImage-H [59]) but relatively low mask AP 75 . This gap could potentially arise from factors as follows: (1) We discretize the output Table 2: Experiments of object-level and output format customization. We conduct these experiments based on VisionLLM-R50, and report the performance of box AP and mask AP on COCO minival for (a) and (b), respectively. \"#Classes\" and \"#Points\" indicate the number of classes and boundary points, respectively. \"*\" indicates that we report the mean AP of the given classes, e.g., 10 classes.  coordinates to integers for unifying tasks, which introduces information loss; (2) Due to the memory and computational constraint, the number of points in our model is limited, which also results in a performance drop; and (3) Point-based methods typically yield lower results compared to direct mask prediction methods, such as Mask R-CNN [22].\n\nImage Captioning. We also evaluate our model in a representative vision-language task, i.e. image captioning task, and report the BLEU-4 [43] and CIDEr [56] metrics. Note that we do not adopt the beam search [2] or CIDEr optimization [49]. We can observe that VisionLLM achieves competitive performance to previous methods. With ResNet-50, we obtain a BLEU-4 score of 31.0 and a CIDEr score of 112.5. When using InternImage-H as the backbone, our model achieves a comparable BLEU-4 score of 32.1 and a CIDEr score of 114.2. These results demonstrate the effectiveness of VisionLLM in generating descriptive and contextually relevant captions for images.\n\n\nObject-Level & Output Format Customization\n\nOur VisionLLM not only allows for customizing the task description, but also for adjusting the target object and the output format using language instructions. Here, we evaluate our model's fine-grained customization ability on COCO. In particular, to customize the target object, we modify the <class> in language instructions to change the model's recognition target from 10 classes to 80 classes. Likewise, to customize the output format, we modify the number of points in language instructions to change the task output format. Table 2 shows that our method can perform well for both object-level and output format changes.\n\n\nAblation Study\n\nIn this section, we analyze the effect of key components and hyper-parameters on VisionLLM. Unless otherwise specified, we use ResNet-50 [23] backbone and perform the ablation experiments for object detection tasks with random classes and task descriptions on COCO2017 [32].\n\nSingle Task vs. Multiple Tasks. We perform an ablation study to assess the impact of multi-task learning with language instructions on VisionLLM. As shown in Table 1, the single-task trained model VisionLLM-R50 sep is slightly better than the jointly trained model VisionLLM-R50 except image captioning. This is due to the multitasking conflicts that also affect previous generalist models [83,81], and it reflects a trade-off between accuracy and generalization.\n\nText Encoder in Language-Guided Image Tokenizer. We examine the role of text encoder (i.e., BERT) in our language-guided image tokenizer in Table 3a, where we report the results for object detection and visual grounding. The first two rows show that BERT is not essential for object detection but it is crucial for visual grounding. We also investigate the effect of freezing the text encoder during training. The last row indicates that freezing BERT hinders the alignment of vision and language modalities and thus degrades the performance for both tasks.\n\nImage Tokenization Method. As a comparison to our query-based tokenization, we employ average pooling on the feature maps from the D-DETR encoder to obtain M patch embeddings, which serve as token representations for the image. Results in Table 3b indicate a clear advantage of our method. This is due to its ability to capture information from objects of various sizes in a more flexible way.\n\nNumber of Localization Tokens. We vary the number of localization tokens from 257 (i.e., -128\u223c128) to 2049 (i.e., -1024\u223c1024), to investigate its impact on visual perception performance. As presented in Table 3c, the model consistently exhibits improvement as the number of localization tokens increases until it reaches a saturation point. Remarkably, a substantial performance boost is observed when the number is raised from 257 to 1025 (+9.9 AP). These results indicate that a higher number of localization tokens enables the models to achieve finer localization abilities, thereby improving localization accuracy.\n\n\nConclusion\n\nIn this paper, we have presented VisionLLM, a novel framework that leverages the power of large language models (LLMs) to address vision-centric tasks in an open-ended and customizable manner. We have designed unified language instruction that matches the format of language models and covers various vision-centric tasks including visual perception. We have also developed a language-guided image tokenizer and an LLM-based task decoder that can handle open-ended tasks according to the given language instructions. We have verified the effectiveness of our models on a series of tasks with different granularities, demonstrating their remarkable generality and flexibility.\n\nBroader Impact. We envision that this work will promote the fusion of visual and language tasks. In addition, since our work is built on open-source pre-trained vision foundation models and large language models, requiring low training resources, thus reducing the carbon footprint. We do not foresee obvious undesirable ethical/social impacts at this moment.\n\n\nAppendix A Example Instructions\n\nAs described in Sec. 3.2 of the main paper, we follow self-instruct [65] to design a set of seed instructions with placeholders and employ LLMs to create diverse related task descriptions for coarse-grained task-level customization. Here, we show some examples of instructions for task-level customization, including object detection, instance segmentation, visual grounding, image captioning, and visual question answering (VQA). Following various instructions, our model can elegantly switch among different vision-centric tasks and accomplish them in a unified manner like LLMs. A.2 Instance Segmentation Example 1. \"Segment the objects from the image with class labels from <class> and output their coordinates within range <range>. The coordinates should be given as the boundary points relative to the center point, and the output format should be (c, x1, y1, x2, y2, ..., x20, y20), where c is the index of the class label that starts from 0. The image is: <image>\" Example 2. \"Segment all the objects from the category set <class> in the provided image and output a tuple (c, x1, y1, x2, y2, ..., x14, y14) for each, where c is the index of the class label in the category set that starts from 0, and (x1, y1, x2, y2, ..., x14, y14) correspond to the offsets of boundary points on the instance mask relative to the center point which should be within <range>. The image is: <image>\" \n\n\nB Loss Function\n\nVisionLLM consists of two model components: language-guided image tokenizer and LLM-based open-task decoder. So the total loss L of our model can be written as:\nL = L tok + L dec ,(1)\nwhere L tok and L dec denote the loss of language-guided image tokenizer and LLM-based open-task decoder, respectively. We introduce the two loss functions as follows:\n\nLanguage-Guided Image Tokenizer. Different from the Q-Former [29], we use a supervision method similar to that of Deformable DETR [82], but with a different loss L tok : category-agnostic classification (focal loss [31]) and center point regression (L 1 loss). As explained in Sec. 3.3, our image tokenizer extracts M image tokens T = {(e i , l i )} M i=1 , each of which is represented by an embedding e i and a location l i (i.e., absolute coordinates of the center point).\n\nLLM-Based Open-Ended Task Decoder. We handle two cases in decoding processing differently.\n\n(1) For regular word prediction, we train with standard next-token supervision [54,45,6,46]; (2) For unordered set prediction (e.g., bounding boxes), we first output a sequence of tokens according to the output format (see the output-format-as-query paradigm in Sec. 3.4), then use bipartite matching to align the LLM-predicted outputs with the ground truths. Despite the differences, we use cross-entropy to compute the loss L dec in a unified way for both cases.\n\n\nC Training Schedule\n\nAs shown in Figure 5, to speed up the convergence of VisionLLM, we split the training schedule of VisionLLM into two stages: Stage 1. In this stage, we initialize the language-guided image tokenizer by loading the pre-trained weights of Deformable DETR [82] and BERT [16]. Additionally, Alpaca [53] is employed as the LLM-based open-ended task decoder. To align visual tokens with text tokens, we make the languageguided image tokenizer trainable while freezing most parameters of the pre-trained Alpaca, with only a few LoRA [24] parameters left tunable. We only focus on object detection in this stage to simplify the training difficulty, with random task descriptions and object categories.  Figure 5: Comparison of two training schedules for VisionLLM. We found that a two-stage training from easy to hard converges faster than a single-stage training. this stage introduces the unified supervision of multiple tasks, including object detection, instance segmentation, visual grounding, image captioning, and VQA, facilitating the model to leverage the power of LLMs to understand and manipulate visual information holistically.\n\n\nD More Ablation Studies\n\nIn this section, we provide more ablation studies and analysis of VisionLLM. Unless otherwise specified, we use ResNet-50 [23] backbone and perform the ablation experiments for object detection tasks with random task descriptions and object categories on COCO 2017 [32].\n\nRandomness. In Table 4a, we examine the effect of introducing randomness during training for VisionLLM, including randomness in task descriptions, object categories, and output formats (i.e., multi-task joint training). Initially, without any randomness, the model achieves a box AP of 45.2. However, as randomness is gradually applied, interesting phenomena emerge: while there is a slight decrease (45.2 \u2192 44.6) in the AP of standard detection with the introduction of randomness, the overall benefits of enhanced task customization and open-ended capabilities outweigh this minor trade-off. Overall, introducing randomness during training in VisionLLM positively impacts its capacity for open-ended tasks and customization.\n\nLow-Rank Adaptation (LoRA). As shown in Table 4b, when randomness is not applied, the model achieves 45.2 box AP without using LoRA [24]. However, when randomness is employed, it is observed that the model fails to converge without using LoRA. Conversely, when LoRA and randomness are used together, the model is able to converge. This indicates that LoRA plays a crucial role as a bridge between the language and visual tokens, enabling effective alignment between the two modalities and improving the convergence of the overall system.\n\nNumber of Image Tokens. We vary the number of image tokens from 50 to 300 to investigate their impact on the performance. Results are presented in Table 4c. As the number of image tokens increases, the performance continues to improve. This makes sense because a larger number of  Figure 6: Evaluation results using eight different prompts. The first six prompts use different task descriptions of object detection, while the last two prompts employ random category orders. These results show that the performance of different prompts is similar, only a 0.1 AP gap is observed.  image tokens provides a more detailed description of the image content. Considering computational complexity, we adopted 100 image tokens in our experiments.\n\nRobustness to Prompt Changes. Since VisionLLM is trained with random prompts, including random task descriptions and random categories, one may ask whether there is a large performance variance across different prompts. To validate the stability of VisionLLM, we conduct experiments using eight different prompts. The first six prompts employ different task descriptions, while the last two prompts involve random category orders. In the case of random category orders, we map the categories back to the COCO standard category order for evaluation. As shown in Figure 6, most evaluation results are distributed closely to 44.8 AP. The performance differences among prompts are marginal, demonstrating that VisionLLM is robust to different prompts. Figure 7, when the prompt only contains 40 classes, the performance for these categories remains normal, while the performance for the remaining categories is close to zero. This indicates that VisionLLM can dynamically detect objects based on the given class set <class> in instructions while disregarding the other classes that are not mentioned. This result highlights the flexibility of VisionLLM in adhering to instructions.\n\n\nInstruction Following Capability. As shown in\n\nOutput-Format-As-Query vs. Seq2Seq. In VisionLLM, we introduce the output-format-as-query framework for LLM decoder. Alternatively, we also experiment with the sequence generation method like Pix2Seq [8] for object detection with random task descriptions and object categories. However, we find that the loss is hard to converge in this paradigm, which indicates that the seq2seq decoding may need a more detailed design or a longer training schedule for the open-ended visual tasks, while the proposed output-format-as-query framework is more effective for open-ended tasks.  predict the results in a sliding-window manner. As shown in Table 4e, without tricks like federal loss, VisionLLM-R50 can achieve 18.9 mAP on LVIS.\n\n\nE Qualitative Analysis\n\nCustomization of Segmentation Points. In this experiment, we focus on modifying the output format mentioned in the prompt, specifically the number of points for instance segmentation (see Sec. A.2). The results are visualized in Figure 8. Remarkably, by increasing the number of points for segmentation, we observe that the model successfully predicts more refined object masks. This validates the capability of our method to precisely customize the output format, showcasing finegrained control over the segmentation process.\n\nCustomization of Category Set. We change the content of the category set <class> in language instructions and visualize the predictions in Figure 9. It is observed that the model can correctly predict the object category according to the provided category set. This demonstrates that VisionLLM has a strong capacity to understand and reason over the semantic information of language instructions, which allows it for flexible category customization in open-vocabulary scenarios. VisionLLM: \"There could be several reasons. One possibility is that the cat is seeking warmth or comfort, as shoes can provide a cozy and warm surface for the cat to rest on. Another reason could be that the cat is seeking attention or affection from its owner, who is standing nearby. Cats are known to be curious creatures, and they may be seeking attention or attention from their owners.\"\n\nVisionLLM: \"The working environment in this office space appears to be open, spacious, and well-equipped. There are several desks set up for employees, providing ample workspace for each individual. The office features a modern and contemporary design, which suggests a creative and collaborative work environment. The presence of several chairs and the organization of the office, including the laptops and monitors, indicate that the space is wellmaintained and conducive to productivity. Overall, the office appears to be a pleasant and efficient work environment for its employees.\"\n\nInstruction: \"The image is: <image>. What do you think of the working environment here?\"\n\n(b) Visual question answering with reasoning Figure 10: Visualization of the image description and VQA capabilities of VisionLLM.\n\n\n(c) VisionLLM (ours) can flexibly manage vision-centric tasks using language instructions like LLMs.\n\nFigure 1 :\n1Comparison of our VisionLLM with popular paradigms. Unlike current vision generalist models that depend on pre-defined task formats and visual prompt tuning models that are inconsistent with large language models (LLMs), VisionLLM leverages the power of LLMs for open-ended vision tasks by using language instructions.\n\nFigure 2 :\n2Results and visualizations of our VisionLLM. Guided by language instructions, our unified generalist framework showcases its effectiveness on diverse open-ended vision-centric tasks. The text marked with a gray background indicates the customized instructions and the desired outputs.\n\nFigure 3 :\n3Vision-language example: \"Describe the image <image> in details.\" Vision-only example: \"For each object in image <image> that is a member of class set <class>, output a tuple with the class label and the coordinates of a polygon with 16 points that encloses the object. The coordinates should be within range <range>. The output format should be(c, x1, y1, ...).\" Overall architecture of the proposed VisionLLM. It consists of three parts: a unified language instruction designed to accommodate both vision and vision-language tasks, an image tokenizer that encodes visual information guided by language instructions, and an LLM-based openended task decoder that executes diverse tasks defined by language instructions.\n\nFigure 3 ,\n3the overall architecture of VisionLLM consists of three key designs: (1) a unified language instruction that provides a consistent interface for vision-centric task definition and customization;\n\n\n. \"Please examine the image and identify all objects in the category set <class>. For each object, specify its location within the range <range> by determining the top-left and bottom-right corners of its bounding box. To indicate the object's class and location, provide the output in the format (c, x1, y1, x2, y2), where 'c' represents the class index starting from 0, and (x1, y1, x2, y2) correspond to the offsets of the bounding box corners relative to the center point. The image is: <image>\" Example 2. \"Identify all the objects in the image that belong to the category set <class> and predict a bounding box around each one. The output should be a list in the format (c, x1, y1, x2, y2), where c represents the index of the class label starting from 0, and x1, y1, x2, y2 are the offsets of the top-left and bottom-right corners of the box relative to the center point. The coordinates should be within <range>. The image is: <image>\" Example 3. \"For each object in the image that is a member of the category set <class>, output a tuple with the index of class label starting from 0 and the offsets of corners relative to the center point that encloses the object. The offsets should be in the order of top-left and bottom-right corners of the rectangle and should be within <range>. The output format should be (c, x1, y1, x2, y2). The image is: <image>\"\n\nExample 3 .\n3\"In the provided image, please segment all the objects in category set <class> within the range <range> by providing their coordinates in the (c, x1, y1, x2, y2, ..., x24, y24) format, where 'c' denotes the index of the class label starting from 0, and (x1, y1, x2, y2, ..., x24, y24) stand for the offsets of boundary points relative to the center point. The image is: <image>\"A.3 Visual Grounding Example 1. \"Please find the object in the category set {<expression>:<cls0>} within the range <range>. Please provide the output in the format (c, x1, y1, x2, y2), where c is the class index starting from 0, and (x1, y1, x2, y2) are the offsets of the top-left and bottom-right corners of the bounding box relative to the center point. The image is: <image>\" Example 2. \"Given the input image, category set {<expression>:<cls0>}, and the range <range>, please locate the object in the image and output the corresponding coordinates in the tuple (c, x1, y1, x2, y2), where c is the index of the class label starting from 0, and (x1, y1, x2, y2) are the offsets of the top-left and bottom-right corners of the rectangle relative to the center point. The image is: <image>\" Example 3. \"For each object in the image that belongs to the {<expression>:<cls0>} category set, please provide the class label (starting from 0) and the offsets from the center of a bounding box that encloses the object. The corner offsets should be in the order of top-left and bottom-right, and within the range <range>. The output should be in the format (c, x1, y1, x2, y2). The image is: <image>\" A.4 Image Captioning Example 1. \"The image is <image>. Write a caption: \" Example 2. \"The image is <image>. Please describe this image: \" Example 3. \"With the objects in the <image>, please generate a caption for the image: \" A.5 Visual Question Answering Example 1. \"The image is <image>. Please generate an answer according to the question: <question>. \" Example 2. \"The image is <image>. Please answer the question <question> according to the image. \" Example 3. \"With the objects in the <image>, <question>. \"\n\nStage 2 .\n2The second stage builds upon the model weights obtained from the first stage. For efficiency, we freeze the visual backbone (e.g., ResNet[23]) in the language-guided image tokenizer. Notably,\n\n\nWe randomly select 40 classes in random order to form the category set. We randomly change the order of 80 classes to form the category set.\n\nFigure 7 :\n7Per-category AP on COCO dataset. We randomly select some categories to form the category set <class> in language instructions.\n\nFigure 8 :\n8Customization of instance masks using the different number of points. Notably, we only modify the output format mentioned in the prompt, i.e. the number of segmentation points. For more details, please see the example prompts provided in Sec. A.2.\n\nImageFigure 9 :\n9Description & VQA. Benefiting from the power of LLMs, VisionLLM exhibits a strong ability in generating long descriptions for images and answering visual questions with complex reasoning. We show the examples in Figure 10. Instruction: \"Identify the objects in the image that belong to {'person': <c0>, ..., 'frisbee': <c29>, ...} and draw a bounding box around each one. The output should be a list of tuples in the format (c, x1, y1, x2, y2), where 'c' represents the index of the class label starting from 0, and x1, y1, x2, y2 are the offsets of the top-left and bottom-right corners of the box relative to the center point. The coordinates should be within <range>. The image is: <image>\" Instruction: \"Identify the objects in the image that belong to {'person': <c0>, 'bicycle': <c1>, ..., 'backpack': <c24>, ..., 'toothbrush': <c79>} and draw a bounding box around each one. The output should be a list of tuples in the format (c, x1, y1, x2, y2), where 'c' represents the index of the class label starting from 0, and x1, y1, x2, y2 are the offsets of the topleft and bottom-right corners of the box relative to the center point. The coordinates should be within <range>. The image is: <image>\" Instruction : \"Identify the objects in the image that belong to {'frisbee': <c0>} and draw a bounding box around each one. The output should be a list of tuples in the format (c, x1, y1, x2, y2), where 'c' represents the index of the class label starting from 0, and x1, y1, x2, y2 are the offsets of the topleft and bottom-right corners of the box relative to the center point. The coordinates should be within <range>. The image is: <image>\" Instruction: \"Identify the objects in the image that belong to {'bicycle': <c0>, 'surfboard': <c1>, 'sports ball': <c2>, 'backpack': <c3>, 'the man wearing blue T-shirt': <c4>} and draw a bounding box around each one. The output should be a list of tuples in the format (c, x1, y1, x2, y2), where 'c' represents the index of the class label starting from 0, and x1, y1, x2, y2 are the offsets of the top-left and bottom-right corners of the box relative to the center point. The coordinates should be within <range>. The image is: <image>\" Customization of category set. These results demonstrate that VisionLLM can correctly detect the specified objects according to the category set <class> provided in language instructions. Instruction: \"The image is: <image>. Analyze the image in a comprehensive and detailed manner.\" VisionLLM: \"The image features a busy city street with a large red double-decker bus driving down the road. There are several people walking on the sidewalk, some of whom are carrying backpacks. The bus is positioned towards the left side of the image. There are several people walking along the sidewalk, with some closer to the bus and others further away. The overall atmosphere suggests a busy urban environment with people going about their daily routines.\" VisionLLM: \"The image features a large, open-air market filled with a variety of fruits and vegetables. There are numerous mangoes, watermelons and apples displayed in the market, with some fruits placed closer to the foreground and others further back. The market is bustling with activity as people can be seen browsing and purchasing the fresh produce. The market is likely a popular destination for those seeking a fresh and healthy diet.\" Instruction: \"The image is: <image>. Write a detailed description of the given image.\" Instruction: \"The image is: <image>. Write a short description of the given image.\" VisionLLM: \"The image features a lively open-air market with a variety of fruits and vegetables. People are busy browsing and buying fresh produce.\" (a) Image description with controllable text length Instruction: \"The image is: <image>. Please generate an answer according to the question: what reasons could there be for the cat squatting on the shoes?\"\n\n\nHuman: \"Identify the objects in <image> that belong to {'What is the child eating?': <c0>, 'red gamepad': <c1>} and draw a bounding box around each one. The output should be a list of tuples in the format (c, x1, y1, x2, y2), where c is the class label and x1, y1, x2, y2 are the coordinates of the top-left and bottom-right corners of the bounding box. The coordinates should be within the range 1024.\" VisionLLM: \"The detected polygons areVisionLLM: \"The bounding boxes are [(<c0>, 226.4, 229.8, \n363.1, 347.4), (<c1>, 441.1, 183.5, 538.6, 269.9)].\" \n\n<c1> \n\n<c0> \n\n(a) Object Detection with Customized Class Set (e.g., \nquestion, reasoning text, open-vocabulary descrip-\ntion) \n\nHuman: \"For each object in \n<image> that is a member \nof {'motorbike': <c0>}, \noutput a tuple with the \nclass label and the \ncoordinates of a polygon \nwith 16 points that \nencloses the object. The coordinates should be within \nthe range 1024. The output format should be (c, x1, y1, \nx2, y2, ..., x16, y16).\" \n\n\n\n\nObject Location with Editable Output Format which is placed on top of a wooden table. The dog has its head in the middle of the cooling pad, and a fan is positioned behind it, blowing air onto the pad. The table is outdoors, in a grassy area, and there are two chairs placed around it, one on each side. The scene appears to be set in a sunny weather, with a bright and sunny sky, which contributes to the overall outdoor atmosphere.\" (c) Image Description with Controllable Text Length Human: \"With the objects in the <image>, if you feel hot, what will you do?\" VisionLLM: \"First, You can also try turning off the computer and other electrical appliances in the room to reduce the heat generated by their operation. As an alternative, you can use the desk fan provided in the image to create a cooling effect in the room. You can plug in the fan and adjust its settings to ensure it blows air in a direction that reaches the room's occupants effectively.\" (d) Visual Question Answer with Complex Reasoning119.3)].\" \n\n<c0> \n\n(b) Human: \"Provide a long \ndescription of the <image>.\" \n\nVisionLLM: \"The image \nfeatures a white dog laying \ndown on a foldable outdoor \ncooling pad , \n\n\n\nTable 1 :\n1Results on standard vision-centric tasks. 'Intern-H\" denotes InternImage-H[59]. \"sep\" indicates that the model is separately trained on each task.Method \nBackbone \nOpen-\nEnded \n\nDetection \nInstance Seg. Grounding Captioning \n\nAP AP50 AP75 AP AP50 AP75 P@0.5 BLEU-4 CIDEr \nSpecialist Models \nFaster R-CNN-FPN [48] ResNet-50 \n-40.3 61.0 44.0 -\n-\n-\n-\n-\n-\nDETR-DC5 [7] \nResNet-50 \n-43.3 63.1 45.9 -\n-\n-\n-\n-\n-\nDeformable-DETR [82] ResNet-50 \n-45.7 65.0 49.1 -\n-\n-\n-\n-\n-\nMask R-CNN [22] \nResNet-50 \n-41.0 61.7 44.9 37.1 58.4 40.1 \n-\n-\n-\nPolar Mask [69] \nResNet-50 \n-\n-\n-\n-30.5 52.0 31.1 \n-\n-\n-\nPix2Seq [8] \nResNet-50 \n-43.2 61.0 46.1 -\n-\n-\n-\n-\n-\nUNITER \n\nTable 3 :\n3Ablation studies on language-guided image tokenizer and hyper-parameters.(a) Effect of text encoder in the \nlanguage-guided image tokenizer. \n\nw/ BERT Freeze COCO RefCOCO \n-\n-\n44.7 \n48.1 \n\u2713 \n-\n44.8 \n84.1 \n\u2713 \n\u2713 \n1.3 \n34.3 \n\n(b) Effect of image tokenization \nmethod. \n\nTokenization \nAP \nAverage Pooling 23.1 \nOurs \n44.8 \n\n(c) Effect of the num-\nber of bins (#Bins). \n\n#Bins \nAP \n257 \n34.9 \n513 \n40.8 \n1025 \n44.8 \n2049 \n44.8 \n\n\n\nTable 4 :\n4More ablation studies for VisionLLM.(a) Effect of randomness. \n\nRandomness \nAP \nNone \n45.2 \n+ Random Task Description \n45.1 \n++ Random Object Category \n44.8 \n+++ Random Output Format \n44.6 \n(Multi-task Joint Training) \n\n(b) Effect of LoRA [24]. \n\nLoRA Randomness \nAP \n\u2717 \n\u2717 \n45.2 \n\u2717 \n\u2713 \n1.2 \n\u2713 \n\u2713 \n44.8 \n\n(c) Effect of the number \nof image tokens. \n\n#Tokens AP \n50 \n44.5 \n100 \n44.8 \n200 \n45.1 \n300 \n45.2 \n\n(d) Effect of Seq2Seq. \n\nSeq2Seq AP \n\u2713 \n-\n\u2717 \n44.8 \n\n(e) Large vocabulary object detection. \n\nDataset #Classes AP \nCOCO \n80 \n44.8 \nLVIS \n1203 \n18.9 \n\n\n\n\nLarge-Vocabulary Object Recognization. To validate the capacity of VisionLLM in the largevocabulary scenario, we further conduct the experiments on the challenging dataset LVIS[20] with 1203 categories. Due to the limited number of language tokens, we randomly select 80 classes for training in each iteration. During inference, we divide the 1203 categories into 16 groups and#Points=8 \n\n#Points=14 \n#Points=16 \n#Points=24 \n\n\n\nFlamingo: a visual language model for few-shot learning. Jeff Jean-Baptiste Alayrac, Pauline Donahue, Antoine Luc, Iain Miech, Yana Barr, Karel Hasson, Arthur Lenc, Katie Mensch, Malcolm Millican, Reynolds, arXiv:2204.1419814arXiv preprintJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022. 1, 4\n\nBottom-up and top-down attention for image captioning and visual question answering. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionPeter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018. 9\n\nVqa: Visual question answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, Devi Parikh, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE International Conference on Computer Vision, 2015. 4\n\nVisual prompting via image inpainting. Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, Alexei Efros, Advances in Neural Information Processing Systems. 2022Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting via image inpainting. Advances in Neural Information Processing Systems, 2022. 4\n\nIs space-time attention all you need for video understanding. Gedas Bertasius, Heng Wang, Lorenzo Torresani, International Conference on Machine Learning. Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In International Conference on Machine Learning, 2021. 7\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in Neural Information Processing Systems. 312Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 2020. 3, 4, 12\n\nEnd-to-end object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, European Conference on Computer Vision. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision, 2020. 3, 4, 8\n\nPix2seq: A language modeling framework for object detection. Ting Chen, Saurabh Saxena, Lala Li, J David, Geoffrey Fleet, Hinton, arXiv:2109.10852814arXiv preprintTing Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language modeling framework for object detection. arXiv preprint arXiv:2109.10852, 2021. 8, 14\n\nA unified sequence interface for vision tasks. Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet, Geoffrey Hinton, arXiv:2206.076694arXiv preprintTing Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet, and Geoffrey Hinton. A unified sequence interface for vision tasks. arXiv preprint arXiv:2206.07669, 2022. 4, 8\n\nMicrosoft coco captions: Data collection and evaluation server. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, C Lawrence Zitnick, arXiv:1504.0032547arXiv preprintXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. 4, 7\n\nUniter: Universal image-text representation learning. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXX. Springer, 2020. 8\n\nVision transformer adapter for dense predictions. Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, Yu Qiao, International Conference on Learning Representations. 2023Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for dense predictions. In International Conference on Learning Representations, 2023. 1\n\nUnifying vision-and-language tasks via text generation. Jaemin Cho, Jie Lei, Hao Tan, Mohit Bansal, International Conference on Machine Learning. Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. In International Conference on Machine Learning, 2021. 8\n\nHyung Won, Le Chung, Shayne Hou, Barret Longpre, Yi Zoph, William Tay, Eric Fedus, Xuezhi Li, Mostafa Wang, Dehghani, arXiv:2210.11416Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprintHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. 4\n\n. Moss, Moss, MOSS contributors. Moss. https://github.com/OpenLMLab/MOSS, 2023. 3\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. 612arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec- tional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 6, 12\n\nAn image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, International Conference on Learning Representations. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. 6\n\nExploring the limits of masked visual representation learning at scale. Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao, Eva, arXiv:2211.07636arXiv preprintYuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. arXiv preprint arXiv:2211.07636, 2022. 1\n\nLarge-scale adversarial training for vision-and-language representation learning. Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, Jingjing Liu, Advances in Neural Information Processing Systems. Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for vision-and-language representation learning. Advances in Neural Information Processing Systems, 2020. 8\n\nLvis: A dataset for large vocabulary instance segmentation. Agrim Gupta, Piotr Dollar, Ross Girshick, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition14Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5356-5364, 2019. 14\n\nTowards general purpose vision systems. Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, Derek Hoiem, arXiv:2104.00743arXiv preprintTanmay Gupta, Amita Kamath, Aniruddha Kembhavi, and Derek Hoiem. Towards general purpose vision systems. arXiv preprint arXiv:2104.00743, 2021. 4\n\nMask r-cnn. Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision89Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2017. 3, 8, 9\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition1213Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016. 6, 7, 8, 9, 12, 13\n\nJ Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, Lora, arXiv:2106.09685Low-rank adaptation of large language models. 1213arXiv preprintEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 7, 12, 13\n\nOpt-iml: Scaling language model instruction meta learning through the lens of generalization. Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, D\u00e1niel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, arXiv:2212.12017arXiv preprintSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, D\u00e1niel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017, 2022. 4\n\nVisual prompt tuning. Menglin Jia, Luming Tang, Claire Bor-Chun Chen, Serge Cardie, Bharath Belongie, Ser-Nam Hariharan, Lim, European Conference on Computer Vision. 2022Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, 2022. 2\n\nIshan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. Aishwarya Kamath, Mannat Singh, Yann Lecun, Gabriel Synnaeve, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionAishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. 8\n\nUni-perceiver v2: A generalist model for large-scale vision and vision-language tasks. Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, arXiv:2211.098084arXiv preprintHao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision and vision-language tasks. arXiv preprint arXiv:2211.09808, 2022. 4, 7, 8\n\nBlip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXiv:2301.12597412arXiv preprintJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 4, 5, 12\n\nVideochat: Chat-centric video understanding. Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, Yu Qiao, arXiv:2305.063555arXiv preprintKunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 2, 4, 5\n\nKaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision12Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2017. 12\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European Conference on Computer Vision. Springer713Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision. Springer, 2014. 7, 9, 13\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, arXiv:2304.08485Visual instruction tuning. 47arXiv preprintHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 4, 5, 7\n\nSwin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. 6\n\nZhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Yang Yang, Qingyun Li, Jiashuo Yu, arXiv:2305.05662Solving vision-centric tasks by interacting with chatbots beyond language. 5arXiv preprintZhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Yang Yang, Qingyun Li, Jiashuo Yu, et al. Interngpt: Solving vision-centric tasks by interacting with chatbots beyond language. arXiv preprint arXiv:2305.05662, 2023. 2, 4, 5\n\nDecoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations. 7\n\nSgdr: Stochastic gradient descent with warm restarts. Ilya Loshchilov, Frank Hutter, arXiv:1608.03983arXiv preprintIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. 7\n\nUnified-io: A unified model for vision, language, and multi-modal tasks. Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, Aniruddha Kembhavi, arXiv:2206.089162022. 4arXiv preprintJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. arXiv preprint arXiv:2206.08916, 2022. 4\n\nGeneration and comprehension of unambiguous object descriptions. Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, Kevin Murphy, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJunhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016. 7\n\n. Openai, arXivOpenAI. Gpt-4 technical report. arXiv, 2023. 3\n\nChatgpt: Optimizing language models for dialogue. OpenAI. Tb Openai, 13TB OpenAI. Chatgpt: Optimizing language models for dialogue. OpenAI, 2022. 1, 3\n\nTraining language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 34Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 2022. 3, 4\n\nBleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002. 9\n\nImproving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 14Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. 1, 4\n\nImproving language understanding by generative pre-training. OpenAI. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 12Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. OpenAI, 2018. 12\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1812Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 12\n\n. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, arXiv:2205.06175A generalist agent. arXiv preprintScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022. 4\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, Advances in Neural Information Processing Systems. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in Neural Information Processing Systems, 2015. 8\n\nSelf-critical sequence training for image captioning. J Steven, Etienne Rennie, Youssef Marcheret, Jerret Mroueh, Vaibhava Ross, Goel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSteven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical sequence training for image captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017. 9\n\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, arXiv:2303.175805arXiv preprintYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023. 2, 4, 5\n\nTowards all-in-one pre-training via maximizing multi-modal mutual information. Weijie Su, Xizhou Zhu, Chenxin Tao, Lewei Lu, Bin Li, Gao Huang, Yu Qiao, Xiaogang Wang, Jie Zhou, Jifeng Dai, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionWeijie Su, Xizhou Zhu, Chenxin Tao, Lewei Lu, Bin Li, Gao Huang, Yu Qiao, Xiaogang Wang, Jie Zhou, and Jifeng Dai. Towards all-in-one pre-training via maximizing multi-modal mutual information. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. 1\n\nSiamese image modeling for self-supervised vision representation learning. Chenxin Tao, Xizhou Zhu, Gao Huang, Yu Qiao, Xiaogang Wang, Jifeng Dai, arXiv:2206.012042022arXiv preprintChenxin Tao, Xizhou Zhu, Gao Huang, Yu Qiao, Xiaogang Wang, and Jifeng Dai. Siamese image modeling for self-supervised vision representation learning. arXiv preprint arXiv:2206.01204, 2022. 1\n\nAlpaca: A strong, replicable instruction-following model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 712Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 2023. 3, 6, 7, 12\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Naman Baptiste Rozi\u00e8re, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Open and efficient foundation language models. 712arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3, 6, 7, 12\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. 30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017. 6\n\nCider: Consensus-based image description evaluation. Ramakrishna Vedantam, Lawrence Zitnick, Devi Parikh, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015. 9\n\nGit: A generative image-to-text transformer for vision and language. Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang, arXiv:2205.1410014arXiv preprintJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022. 1, 4\n\nUnifying architectures, tasks, and modalities through a simple sequence-tosequence learning framework. Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang, arXiv:2202.03052arXiv preprintPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Unifying architectures, tasks, and modalities through a simple sequence-to- sequence learning framework. arXiv preprint arXiv:2202.03052, 2022. 1, 4, 7\n\nExploring large-scale vision foundation models with deformable convolutions. Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionWenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation models with deformable convolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. 1, 2, 7, 8\n\nPvt v2: Improved baselines with pyramid vision transformer. Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao, Computational Visual Media. 83Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media, 8(3):415-424, 2022. 6\n\nImage as a foreign language: Beit pretraining for all vision and vision-language tasks. Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Saksham Owais Khan Mohammed, Subhojit Singhal, Som, arXiv:2208.104421arXiv preprintWenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022. 1, 2\n\nImages speak in images: A generalist painter for in-context visual learning. Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, Tiejun Huang, arXiv:2212.024992022arXiv preprintXinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A generalist painter for in-context visual learning. arXiv preprint arXiv:2212.02499, 2022. 2\n\nImages speak in images: A generalist painter for in-context visual learning. Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, Tiejun Huang, arXiv:2212.024992022. 4arXiv preprintXinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A generalist painter for in-context visual learning. arXiv preprint arXiv:2212.02499, 2022. 4\n\nXinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, Tiejun Huang, arXiv:2304.03284Seggpt: Segmenting everything in context. 24arXiv preprintXinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt: Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023. 2, 4\n\nSelf-instruct: Aligning language model with self generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, A Noah, Daniel Smith, Hannaneh Khashabi, Hajishirzi, arXiv:2212.10560411arXiv preprintYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022. 4, 5, 11\n\nBenchmarking generalization via in-context instructions on 1,600+ language tasks. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, arXiv:2204.07705arXiv preprintYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv preprint arXiv:2204.07705, 2022. 4\n\nFinetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, M Andrew, Quoc V Dai, Le, arXiv:2109.01652arXiv preprintJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. 4\n\nVisual chatgpt: Talking, drawing and editing with visual foundation models. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan, arXiv:2303.046715arXiv preprintChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. 2, 4, 5\n\nPolarmask: Single shot instance segmentation with polar representation. Enze Xie, Peize Sun, Xiaoge Song, Wenhai Wang, Xuebo Liu, Ding Liang, Chunhua Shen, Ping Luo, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition6Enze Xie, Peize Sun, Xiaoge Song, Wenhai Wang, Xuebo Liu, Ding Liang, Chunhua Shen, and Ping Luo. Polarmask: Single shot instance segmentation with polar representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. 6, 8\n\nUniversal instance perception as object discovery and retrieval. Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, Huchuan Lu, arXiv:2303.06674arXiv preprintBin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance perception as object discovery and retrieval. arXiv preprint arXiv:2303.06674, 2023. 4\n\nUnitab: Unifying text and box outputs for grounded vision-language modeling. Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, Lijuan Wang, European Conference on Computer Vision. Springer47Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language modeling. In European Conference on Computer Vision, pages 521-539. Springer, 2022. 4, 7\n\nUnitab: Unifying text and box outputs for grounded vision-language modeling. Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, Lijuan Wang, European Conference on Computer Vision. Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language modeling. In European Conference on Computer Vision, 2022. 8\n\nMm-react: Prompting chatgpt for multimodal reasoning and action. Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang, arXiv:2303.113815arXiv preprintZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023. 2, 4, 5\n\nCpt: Colorful prompt tuning for pre-trained vision-language models. Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, arXiv:2109.11797Tat-Seng Chua, and Maosong Sun. arXiv preprintYuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Cpt: Colorful prompt tuning for pre-trained vision-language models. arXiv preprint arXiv:2109.11797, 2021. 2\n\nModeling context in referring expressions. Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, Tamara L Berg, European Conference on Computer Vision. SpringerLicheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In European Conference on Computer Vision. Springer, 2016. 7\n\nUnified vision and language prompt learning. Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, Chen Change Loy, arXiv:2210.072252022arXiv preprintYuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen Change Loy. Unified vision and language prompt learning. arXiv preprint arXiv:2210.07225, 2022. 2\n\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, arXiv:2210.02414Glm-130b: An open bilingual pre-trained model. 2022arXiv preprintAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022. 3\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.01068Open pre-trained transformer language models. 2022arXiv preprintSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. 3\n\n. Yuanhan Zhang, Kaiyang Zhou, Ziwei Liu, arXiv:2206.046732022Neural prompt search. arXiv preprintYuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. arXiv preprint arXiv:2206.04673, 2022. 2\n\nMinigpt-4: Enhancing visionlanguage understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.10592arXiv preprintDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision- language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 4\n\nUni-perceiver-moe: Learning sparse generalist models with conditional moes. Jinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang, Hongsheng Li, Xiaogang Wang, Jifeng Dai, arXiv:2206.0467489arXiv preprintJinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang, Hongsheng Li, Xiaogang Wang, and Jifeng Dai. Uni-perceiver-moe: Learning sparse generalist models with conditional moes. arXiv preprint arXiv:2206.04674, 2022. 1, 4, 8, 9\n\nDeformable detr: Deformable transformers for end-to-end object detection. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai, International Conference on Learning Representations. 812Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In International Conference on Learning Representations, 2021. 3, 6, 7, 8, 12\n\nUniperceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot tasks. Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng Li, Xiaohua Wang, Jifeng Dai, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition89Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng Li, Xiaohua Wang, and Jifeng Dai. Uni- perceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 2, 4, 5, 8, 9\n", "annotations": {"author": "[{\"end\":138,\"start\":90},{\"end\":205,\"start\":139},{\"end\":240,\"start\":206},{\"end\":317,\"start\":241},{\"end\":386,\"start\":318},{\"end\":453,\"start\":387},{\"end\":528,\"start\":454},{\"end\":558,\"start\":529},{\"end\":590,\"start\":559},{\"end\":635,\"start\":591},{\"end\":705,\"start\":636}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":97},{\"end\":147,\"start\":143},{\"end\":219,\"start\":215},{\"end\":251,\"start\":249},{\"end\":328,\"start\":325},{\"end\":396,\"start\":392},{\"end\":462,\"start\":459},{\"end\":536,\"start\":534},{\"end\":567,\"start\":563},{\"end\":598,\"start\":594},{\"end\":646,\"start\":643}]", "author_first_name": "[{\"end\":96,\"start\":90},{\"end\":142,\"start\":139},{\"end\":214,\"start\":206},{\"end\":248,\"start\":241},{\"end\":324,\"start\":318},{\"end\":391,\"start\":387},{\"end\":458,\"start\":454},{\"end\":533,\"start\":529},{\"end\":562,\"start\":559},{\"end\":593,\"start\":591},{\"end\":642,\"start\":636}]", "author_affiliation": "[{\"end\":121,\"start\":103},{\"end\":137,\"start\":123},{\"end\":167,\"start\":149},{\"end\":183,\"start\":169},{\"end\":204,\"start\":185},{\"end\":239,\"start\":221},{\"end\":271,\"start\":253},{\"end\":287,\"start\":273},{\"end\":316,\"start\":289},{\"end\":348,\"start\":330},{\"end\":364,\"start\":350},{\"end\":385,\"start\":366},{\"end\":416,\"start\":398},{\"end\":432,\"start\":418},{\"end\":452,\"start\":434},{\"end\":482,\"start\":464},{\"end\":498,\"start\":484},{\"end\":527,\"start\":500},{\"end\":557,\"start\":538},{\"end\":589,\"start\":569},{\"end\":618,\"start\":600},{\"end\":634,\"start\":620},{\"end\":666,\"start\":648},{\"end\":682,\"start\":668},{\"end\":704,\"start\":684}]", "title": "[{\"end\":87,\"start\":1},{\"end\":792,\"start\":706}]", "venue": null, "abstract": "[{\"end\":2221,\"start\":794}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2300,\"start\":2296},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":2969,\"start\":2965},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2972,\"start\":2969},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":2975,\"start\":2972},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":2978,\"start\":2975},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2981,\"start\":2978},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":2984,\"start\":2981},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3169,\"start\":3165},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":3172,\"start\":3169},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3174,\"start\":3172},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":3177,\"start\":3174},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":3180,\"start\":3177},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":3464,\"start\":3460},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":3467,\"start\":3464},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":3470,\"start\":3467},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3576,\"start\":3572},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":3579,\"start\":3576},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":3582,\"start\":3579},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3778,\"start\":3774},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":3781,\"start\":3778},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":3784,\"start\":3781},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":3787,\"start\":3784},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":3790,\"start\":3787},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":5332,\"start\":5328},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":5335,\"start\":5332},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":5338,\"start\":5335},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5341,\"start\":5338},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5344,\"start\":5341},{\"end\":7075,\"start\":6982},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":7933,\"start\":7929},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7935,\"start\":7933},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7938,\"start\":7935},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8328,\"start\":8325},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8342,\"start\":8338},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8354,\"start\":8350},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8376,\"start\":8372},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":8440,\"start\":8436},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8452,\"start\":8448},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8463,\"start\":8459},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":8477,\"start\":8473},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8740,\"start\":8736},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":9167,\"start\":9163},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":9182,\"start\":9178},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9199,\"start\":9195},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9215,\"start\":9211},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9235,\"start\":9231},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":9383,\"start\":9379},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":9386,\"start\":9383},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9389,\"start\":9386},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9392,\"start\":9389},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9395,\"start\":9392},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":9849,\"start\":9845},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9852,\"start\":9849},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":9855,\"start\":9852},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10097,\"start\":10093},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":10135,\"start\":10131},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10149,\"start\":10146},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":10163,\"start\":10159},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10240,\"start\":10236},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10256,\"start\":10253},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":10273,\"start\":10269},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10393,\"start\":10389},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10483,\"start\":10479},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10591,\"start\":10588},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":10780,\"start\":10776},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":10783,\"start\":10780},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10786,\"start\":10783},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11367,\"start\":11364},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11432,\"start\":11428},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11443,\"start\":11439},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":11446,\"start\":11443},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11464,\"start\":11460},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":11508,\"start\":11504},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":11511,\"start\":11508},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11756,\"start\":11753},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11929,\"start\":11925},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11941,\"start\":11938},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11954,\"start\":11950},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":12111,\"start\":12107},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12126,\"start\":12122},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12461,\"start\":12458},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":12579,\"start\":12575},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":12595,\"start\":12591},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":14062,\"start\":14058},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":14065,\"start\":14062},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":14068,\"start\":14065},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14071,\"start\":14068},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14074,\"start\":14071},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15258,\"start\":15254},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":15261,\"start\":15258},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15264,\"start\":15261},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":16024,\"start\":16020},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":17122,\"start\":17118},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17864,\"start\":17860},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":17867,\"start\":17864},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17870,\"start\":17867},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18213,\"start\":18209},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18330,\"start\":18326},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":18491,\"start\":18487},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":18687,\"start\":18683},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":19320,\"start\":19316},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":19360,\"start\":19356},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22265,\"start\":22261},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":23069,\"start\":23065},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":23171,\"start\":23167},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":23186,\"start\":23182},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":23204,\"start\":23200},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23410,\"start\":23406},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23439,\"start\":23435},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23571,\"start\":23567},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":23574,\"start\":23571},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":23577,\"start\":23574},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24103,\"start\":24099},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":24126,\"start\":24122},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24191,\"start\":24188},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":24245,\"start\":24241},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":24417,\"start\":24413},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":24431,\"start\":24427},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24495,\"start\":24491},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":25170,\"start\":25166},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":25266,\"start\":25262},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25321,\"start\":25317},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26855,\"start\":26851},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":27218,\"start\":27214},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":27342,\"start\":27338},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":28415,\"start\":28411},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":28446,\"start\":28444},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29272,\"start\":29268},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":29416,\"start\":29412},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":29431,\"start\":29427},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29486,\"start\":29483},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":29513,\"start\":29509},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":30762,\"start\":30758},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":30894,\"start\":30890},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":31291,\"start\":31287},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":31294,\"start\":31291},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":34093,\"start\":34089},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":35850,\"start\":35846},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":35919,\"start\":35915},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":36004,\"start\":36000},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":36437,\"start\":36433},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":36440,\"start\":36437},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":36442,\"start\":36440},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":36445,\"start\":36442},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":37099,\"start\":37095},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":37113,\"start\":37109},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":37140,\"start\":37136},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":37372,\"start\":37368},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":38128,\"start\":38124},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":38271,\"start\":38267},{\"end\":38687,\"start\":38674},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":39138,\"start\":39134},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":41709,\"start\":41706},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":49761,\"start\":49757},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":56549,\"start\":56545},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":58305,\"start\":58301}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":44568,\"start\":44466},{\"attributes\":{\"id\":\"fig_1\"},\"end\":44900,\"start\":44569},{\"attributes\":{\"id\":\"fig_2\"},\"end\":45198,\"start\":44901},{\"attributes\":{\"id\":\"fig_3\"},\"end\":45931,\"start\":45199},{\"attributes\":{\"id\":\"fig_4\"},\"end\":46139,\"start\":45932},{\"attributes\":{\"id\":\"fig_5\"},\"end\":47506,\"start\":46140},{\"attributes\":{\"id\":\"fig_6\"},\"end\":49607,\"start\":47507},{\"attributes\":{\"id\":\"fig_7\"},\"end\":49811,\"start\":49608},{\"attributes\":{\"id\":\"fig_8\"},\"end\":49954,\"start\":49812},{\"attributes\":{\"id\":\"fig_9\"},\"end\":50094,\"start\":49955},{\"attributes\":{\"id\":\"fig_10\"},\"end\":50355,\"start\":50095},{\"attributes\":{\"id\":\"fig_11\"},\"end\":54278,\"start\":50356},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":55274,\"start\":54279},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":56458,\"start\":55275},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":57118,\"start\":56459},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":57555,\"start\":57119},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":58122,\"start\":57556},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":58551,\"start\":58123}]", "paragraph": "[{\"end\":2760,\"start\":2237},{\"end\":3340,\"start\":2762},{\"end\":3429,\"start\":3368},{\"end\":3522,\"start\":3431},{\"end\":3624,\"start\":3547},{\"end\":4346,\"start\":3671},{\"end\":5275,\"start\":4348},{\"end\":6107,\"start\":5277},{\"end\":6159,\"start\":6109},{\"end\":6523,\"start\":6161},{\"end\":7983,\"start\":6525},{\"end\":9784,\"start\":8023},{\"end\":11225,\"start\":9812},{\"end\":13222,\"start\":11248},{\"end\":14796,\"start\":13259},{\"end\":15052,\"start\":14829},{\"end\":15604,\"start\":15054},{\"end\":15920,\"start\":15606},{\"end\":16201,\"start\":15922},{\"end\":17252,\"start\":16203},{\"end\":17699,\"start\":17254},{\"end\":18084,\"start\":17735},{\"end\":18599,\"start\":18086},{\"end\":19247,\"start\":18601},{\"end\":19825,\"start\":19285},{\"end\":20917,\"start\":19827},{\"end\":21268,\"start\":20955},{\"end\":21871,\"start\":21270},{\"end\":22570,\"start\":21873},{\"end\":24280,\"start\":22609},{\"end\":24378,\"start\":24282},{\"end\":24532,\"start\":24380},{\"end\":25334,\"start\":24534},{\"end\":25873,\"start\":25676},{\"end\":26611,\"start\":25902},{\"end\":27398,\"start\":26613},{\"end\":28055,\"start\":27400},{\"end\":29273,\"start\":28057},{\"end\":29928,\"start\":29275},{\"end\":30602,\"start\":29975},{\"end\":30895,\"start\":30621},{\"end\":31360,\"start\":30897},{\"end\":31919,\"start\":31362},{\"end\":32314,\"start\":31921},{\"end\":32934,\"start\":32316},{\"end\":33624,\"start\":32949},{\"end\":33985,\"start\":33626},{\"end\":35412,\"start\":34021},{\"end\":35592,\"start\":35432},{\"end\":35783,\"start\":35616},{\"end\":36260,\"start\":35785},{\"end\":36352,\"start\":36262},{\"end\":36818,\"start\":36354},{\"end\":37974,\"start\":36842},{\"end\":38272,\"start\":38002},{\"end\":39000,\"start\":38274},{\"end\":39539,\"start\":39002},{\"end\":40277,\"start\":39541},{\"end\":41456,\"start\":40279},{\"end\":42230,\"start\":41506},{\"end\":42783,\"start\":42257},{\"end\":43656,\"start\":42785},{\"end\":44244,\"start\":43658},{\"end\":44334,\"start\":44246},{\"end\":44465,\"start\":44336}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":25675,\"start\":25335},{\"attributes\":{\"id\":\"formula_1\"},\"end\":35615,\"start\":35593}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26095,\"start\":26088},{\"end\":28543,\"start\":28536},{\"end\":30514,\"start\":30507},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":31062,\"start\":31055},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":31510,\"start\":31502},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":32168,\"start\":32160},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":32527,\"start\":32519},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":38297,\"start\":38289},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":39050,\"start\":39042},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":39696,\"start\":39688},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":42151,\"start\":42143}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2235,\"start\":2223},{\"end\":3366,\"start\":3343},{\"end\":3545,\"start\":3525},{\"end\":3639,\"start\":3627},{\"end\":3669,\"start\":3642},{\"attributes\":{\"n\":\"2\"},\"end\":7998,\"start\":7986},{\"attributes\":{\"n\":\"2.1\"},\"end\":8021,\"start\":8001},{\"attributes\":{\"n\":\"2.2\"},\"end\":9810,\"start\":9787},{\"attributes\":{\"n\":\"2.3\"},\"end\":11246,\"start\":11228},{\"attributes\":{\"n\":\"3\"},\"end\":13234,\"start\":13225},{\"attributes\":{\"n\":\"3.1\"},\"end\":13257,\"start\":13237},{\"attributes\":{\"n\":\"3.2\"},\"end\":14827,\"start\":14799},{\"attributes\":{\"n\":\"3.3\"},\"end\":17733,\"start\":17702},{\"attributes\":{\"n\":\"3.4\"},\"end\":19283,\"start\":19250},{\"end\":20953,\"start\":20920},{\"attributes\":{\"n\":\"4\"},\"end\":22583,\"start\":22573},{\"attributes\":{\"n\":\"4.1\"},\"end\":22607,\"start\":22586},{\"attributes\":{\"n\":\"4.2\"},\"end\":25900,\"start\":25876},{\"attributes\":{\"n\":\"4.3\"},\"end\":29973,\"start\":29931},{\"attributes\":{\"n\":\"4.4\"},\"end\":30619,\"start\":30605},{\"attributes\":{\"n\":\"5\"},\"end\":32947,\"start\":32937},{\"end\":34019,\"start\":33988},{\"end\":35430,\"start\":35415},{\"end\":36840,\"start\":36821},{\"end\":38000,\"start\":37977},{\"end\":41504,\"start\":41459},{\"end\":42255,\"start\":42233},{\"end\":44580,\"start\":44570},{\"end\":44912,\"start\":44902},{\"end\":45210,\"start\":45200},{\"end\":45943,\"start\":45933},{\"end\":47519,\"start\":47508},{\"end\":49618,\"start\":49609},{\"end\":49966,\"start\":49956},{\"end\":50106,\"start\":50096},{\"end\":50372,\"start\":50357},{\"end\":56469,\"start\":56460},{\"end\":57129,\"start\":57120},{\"end\":57566,\"start\":57557}]", "table": "[{\"end\":55274,\"start\":54722},{\"end\":56458,\"start\":56284},{\"end\":57118,\"start\":56617},{\"end\":57555,\"start\":57204},{\"end\":58122,\"start\":57604},{\"end\":58551,\"start\":58502}]", "figure_caption": "[{\"end\":44568,\"start\":44468},{\"end\":44900,\"start\":44582},{\"end\":45198,\"start\":44914},{\"end\":45931,\"start\":45212},{\"end\":46139,\"start\":45945},{\"end\":47506,\"start\":46142},{\"end\":49607,\"start\":47521},{\"end\":49811,\"start\":49620},{\"end\":49954,\"start\":49814},{\"end\":50094,\"start\":49968},{\"end\":50355,\"start\":50108},{\"end\":54278,\"start\":50374},{\"end\":54722,\"start\":54281},{\"end\":56284,\"start\":55277},{\"end\":56617,\"start\":56471},{\"end\":57204,\"start\":57131},{\"end\":57604,\"start\":57568},{\"end\":58502,\"start\":58125}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3115,\"start\":3106},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3869,\"start\":3860},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":7492,\"start\":7484},{\"end\":21070,\"start\":21062},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23627,\"start\":23618},{\"end\":36862,\"start\":36854},{\"end\":37545,\"start\":37537},{\"end\":39830,\"start\":39822},{\"end\":40848,\"start\":40840},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":41035,\"start\":41027},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":42494,\"start\":42486},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":42932,\"start\":42924},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":44390,\"start\":44381}]", "bib_author_first_name": "[{\"end\":58614,\"start\":58610},{\"end\":58645,\"start\":58638},{\"end\":58662,\"start\":58655},{\"end\":58672,\"start\":58668},{\"end\":58684,\"start\":58680},{\"end\":58696,\"start\":58691},{\"end\":58711,\"start\":58705},{\"end\":58723,\"start\":58718},{\"end\":58739,\"start\":58732},{\"end\":59141,\"start\":59136},{\"end\":59160,\"start\":59152},{\"end\":59170,\"start\":59165},{\"end\":59186,\"start\":59180},{\"end\":59198,\"start\":59194},{\"end\":59215,\"start\":59208},{\"end\":59226,\"start\":59223},{\"end\":59694,\"start\":59685},{\"end\":59711,\"start\":59702},{\"end\":59727,\"start\":59721},{\"end\":59740,\"start\":59732},{\"end\":59756,\"start\":59751},{\"end\":59772,\"start\":59764},{\"end\":59786,\"start\":59782},{\"end\":60188,\"start\":60184},{\"end\":60199,\"start\":60194},{\"end\":60218,\"start\":60212},{\"end\":60232,\"start\":60228},{\"end\":60250,\"start\":60244},{\"end\":60557,\"start\":60552},{\"end\":60573,\"start\":60569},{\"end\":60587,\"start\":60580},{\"end\":60858,\"start\":60855},{\"end\":60874,\"start\":60866},{\"end\":60885,\"start\":60881},{\"end\":60900,\"start\":60893},{\"end\":60915,\"start\":60910},{\"end\":60917,\"start\":60916},{\"end\":60934,\"start\":60926},{\"end\":60951,\"start\":60945},{\"end\":60971,\"start\":60965},{\"end\":60985,\"start\":60979},{\"end\":61000,\"start\":60994},{\"end\":61384,\"start\":61377},{\"end\":61402,\"start\":61393},{\"end\":61417,\"start\":61410},{\"end\":61435,\"start\":61428},{\"end\":61454,\"start\":61445},{\"end\":61471,\"start\":61465},{\"end\":61803,\"start\":61799},{\"end\":61817,\"start\":61810},{\"end\":61830,\"start\":61826},{\"end\":61836,\"start\":61835},{\"end\":61852,\"start\":61844},{\"end\":62131,\"start\":62127},{\"end\":62145,\"start\":62138},{\"end\":62158,\"start\":62154},{\"end\":62171,\"start\":62163},{\"end\":62182,\"start\":62177},{\"end\":62184,\"start\":62183},{\"end\":62200,\"start\":62192},{\"end\":62488,\"start\":62482},{\"end\":62498,\"start\":62495},{\"end\":62513,\"start\":62505},{\"end\":62530,\"start\":62519},{\"end\":62548,\"start\":62541},{\"end\":62561,\"start\":62556},{\"end\":62580,\"start\":62570},{\"end\":62905,\"start\":62897},{\"end\":62918,\"start\":62912},{\"end\":62930,\"start\":62923},{\"end\":62940,\"start\":62935},{\"end\":62943,\"start\":62941},{\"end\":62957,\"start\":62951},{\"end\":62968,\"start\":62965},{\"end\":62976,\"start\":62974},{\"end\":62992,\"start\":62984},{\"end\":63413,\"start\":63410},{\"end\":63426,\"start\":63420},{\"end\":63439,\"start\":63433},{\"end\":63452,\"start\":63446},{\"end\":63461,\"start\":63457},{\"end\":63472,\"start\":63466},{\"end\":63480,\"start\":63478},{\"end\":63804,\"start\":63798},{\"end\":63813,\"start\":63810},{\"end\":63822,\"start\":63819},{\"end\":63833,\"start\":63828},{\"end\":64063,\"start\":64061},{\"end\":64077,\"start\":64071},{\"end\":64089,\"start\":64083},{\"end\":64101,\"start\":64099},{\"end\":64115,\"start\":64108},{\"end\":64125,\"start\":64121},{\"end\":64139,\"start\":64133},{\"end\":64151,\"start\":64144},{\"end\":64591,\"start\":64586},{\"end\":64608,\"start\":64600},{\"end\":64622,\"start\":64616},{\"end\":64636,\"start\":64628},{\"end\":64646,\"start\":64637},{\"end\":65040,\"start\":65034},{\"end\":65059,\"start\":65054},{\"end\":65076,\"start\":65067},{\"end\":65093,\"start\":65089},{\"end\":65114,\"start\":65107},{\"end\":65127,\"start\":65121},{\"end\":65148,\"start\":65141},{\"end\":65167,\"start\":65159},{\"end\":65183,\"start\":65178},{\"end\":65200,\"start\":65193},{\"end\":65662,\"start\":65657},{\"end\":65672,\"start\":65669},{\"end\":65685,\"start\":65679},{\"end\":65695,\"start\":65691},{\"end\":65707,\"start\":65701},{\"end\":65720,\"start\":65712},{\"end\":65733,\"start\":65727},{\"end\":65748,\"start\":65741},{\"end\":65758,\"start\":65755},{\"end\":66114,\"start\":66111},{\"end\":66128,\"start\":66120},{\"end\":66141,\"start\":66135},{\"end\":66150,\"start\":66146},{\"end\":66158,\"start\":66156},{\"end\":66174,\"start\":66166},{\"end\":66511,\"start\":66506},{\"end\":66524,\"start\":66519},{\"end\":66537,\"start\":66533},{\"end\":66964,\"start\":66958},{\"end\":66977,\"start\":66972},{\"end\":66995,\"start\":66986},{\"end\":67011,\"start\":67006},{\"end\":67215,\"start\":67208},{\"end\":67227,\"start\":67220},{\"end\":67243,\"start\":67238},{\"end\":67256,\"start\":67252},{\"end\":67617,\"start\":67610},{\"end\":67629,\"start\":67622},{\"end\":67645,\"start\":67637},{\"end\":67655,\"start\":67651},{\"end\":68016,\"start\":68015},{\"end\":68031,\"start\":68025},{\"end\":68043,\"start\":68036},{\"end\":68056,\"start\":68050},{\"end\":68072,\"start\":68065},{\"end\":68089,\"start\":68084},{\"end\":68096,\"start\":68094},{\"end\":68109,\"start\":68103},{\"end\":68524,\"start\":68514},{\"end\":68533,\"start\":68531},{\"end\":68557,\"start\":68548},{\"end\":68573,\"start\":68568},{\"end\":68590,\"start\":68584},{\"end\":68602,\"start\":68598},{\"end\":68611,\"start\":68607},{\"end\":68627,\"start\":68621},{\"end\":68638,\"start\":68634},{\"end\":68649,\"start\":68644},{\"end\":69014,\"start\":69007},{\"end\":69026,\"start\":69020},{\"end\":69039,\"start\":69033},{\"end\":69060,\"start\":69055},{\"end\":69076,\"start\":69069},{\"end\":69094,\"start\":69087},{\"end\":69447,\"start\":69438},{\"end\":69462,\"start\":69456},{\"end\":69474,\"start\":69470},{\"end\":69489,\"start\":69482},{\"end\":69967,\"start\":69964},{\"end\":69978,\"start\":69972},{\"end\":69990,\"start\":69984},{\"end\":70004,\"start\":69998},{\"end\":70019,\"start\":70010},{\"end\":70028,\"start\":70024},{\"end\":70042,\"start\":70035},{\"end\":70051,\"start\":70049},{\"end\":70066,\"start\":70058},{\"end\":70079,\"start\":70073},{\"end\":70491,\"start\":70485},{\"end\":70502,\"start\":70496},{\"end\":70513,\"start\":70507},{\"end\":70530,\"start\":70524},{\"end\":70830,\"start\":70822},{\"end\":70840,\"start\":70835},{\"end\":70847,\"start\":70845},{\"end\":70860,\"start\":70854},{\"end\":70871,\"start\":70865},{\"end\":70882,\"start\":70878},{\"end\":70892,\"start\":70888},{\"end\":70904,\"start\":70899},{\"end\":70913,\"start\":70911},{\"end\":71223,\"start\":71215},{\"end\":71234,\"start\":71229},{\"end\":71246,\"start\":71242},{\"end\":71636,\"start\":71628},{\"end\":71649,\"start\":71642},{\"end\":71662,\"start\":71657},{\"end\":71678,\"start\":71673},{\"end\":71691,\"start\":71685},{\"end\":71704,\"start\":71700},{\"end\":71719,\"start\":71714},{\"end\":71738,\"start\":71728},{\"end\":72042,\"start\":72035},{\"end\":72056,\"start\":72048},{\"end\":72069,\"start\":72061},{\"end\":72078,\"start\":72074},{\"end\":72082,\"start\":72079},{\"end\":72354,\"start\":72352},{\"end\":72366,\"start\":72360},{\"end\":72375,\"start\":72372},{\"end\":72384,\"start\":72381},{\"end\":72395,\"start\":72389},{\"end\":72406,\"start\":72401},{\"end\":72421,\"start\":72414},{\"end\":72434,\"start\":72427},{\"end\":72827,\"start\":72819},{\"end\":72838,\"start\":72833},{\"end\":72849,\"start\":72843},{\"end\":72862,\"start\":72856},{\"end\":72871,\"start\":72869},{\"end\":72884,\"start\":72878},{\"end\":72899,\"start\":72891},{\"end\":72911,\"start\":72907},{\"end\":72925,\"start\":72918},{\"end\":72937,\"start\":72930},{\"end\":73355,\"start\":73351},{\"end\":73373,\"start\":73368},{\"end\":73627,\"start\":73623},{\"end\":73645,\"start\":73640},{\"end\":73893,\"start\":73887},{\"end\":73909,\"start\":73898},{\"end\":73922,\"start\":73917},{\"end\":73939,\"start\":73932},{\"end\":73959,\"start\":73950},{\"end\":74280,\"start\":74274},{\"end\":74294,\"start\":74286},{\"end\":74311,\"start\":74302},{\"end\":74324,\"start\":74320},{\"end\":74338,\"start\":74334},{\"end\":74340,\"start\":74339},{\"end\":74354,\"start\":74349},{\"end\":75041,\"start\":75037},{\"end\":75057,\"start\":75050},{\"end\":75064,\"start\":75062},{\"end\":75077,\"start\":75072},{\"end\":75094,\"start\":75087},{\"end\":75113,\"start\":75107},{\"end\":75128,\"start\":75123},{\"end\":75144,\"start\":75136},{\"end\":75162,\"start\":75154},{\"end\":75174,\"start\":75170},{\"end\":75586,\"start\":75579},{\"end\":75602,\"start\":75597},{\"end\":75615,\"start\":75611},{\"end\":75630,\"start\":75622},{\"end\":76088,\"start\":76084},{\"end\":76105,\"start\":76098},{\"end\":76121,\"start\":76118},{\"end\":76136,\"start\":76132},{\"end\":76367,\"start\":76363},{\"end\":76384,\"start\":76377},{\"end\":76400,\"start\":76397},{\"end\":76415,\"start\":76411},{\"end\":76636,\"start\":76632},{\"end\":76653,\"start\":76646},{\"end\":76663,\"start\":76658},{\"end\":76676,\"start\":76671},{\"end\":76688,\"start\":76683},{\"end\":76701,\"start\":76697},{\"end\":76909,\"start\":76904},{\"end\":76922,\"start\":76916},{\"end\":76936,\"start\":76930},{\"end\":76954,\"start\":76948},{\"end\":76960,\"start\":76955},{\"end\":76983,\"start\":76974},{\"end\":77000,\"start\":76993},{\"end\":77017,\"start\":77014},{\"end\":77031,\"start\":77027},{\"end\":77046,\"start\":77040},{\"end\":77056,\"start\":77052},{\"end\":77063,\"start\":77057},{\"end\":77458,\"start\":77451},{\"end\":77477,\"start\":77473},{\"end\":77486,\"start\":77482},{\"end\":77806,\"start\":77805},{\"end\":77822,\"start\":77815},{\"end\":77838,\"start\":77831},{\"end\":77856,\"start\":77850},{\"end\":77873,\"start\":77865},{\"end\":78339,\"start\":78330},{\"end\":78352,\"start\":78346},{\"end\":78361,\"start\":78359},{\"end\":78376,\"start\":78367},{\"end\":78388,\"start\":78381},{\"end\":78400,\"start\":78393},{\"end\":78730,\"start\":78724},{\"end\":78741,\"start\":78735},{\"end\":78754,\"start\":78747},{\"end\":78765,\"start\":78760},{\"end\":78773,\"start\":78770},{\"end\":78781,\"start\":78778},{\"end\":78791,\"start\":78789},{\"end\":78806,\"start\":78798},{\"end\":78816,\"start\":78813},{\"end\":78829,\"start\":78823},{\"end\":79325,\"start\":79318},{\"end\":79337,\"start\":79331},{\"end\":79346,\"start\":79343},{\"end\":79356,\"start\":79354},{\"end\":79371,\"start\":79363},{\"end\":79384,\"start\":79378},{\"end\":79680,\"start\":79675},{\"end\":79694,\"start\":79688},{\"end\":79712,\"start\":79706},{\"end\":79724,\"start\":79720},{\"end\":79740,\"start\":79733},{\"end\":79751,\"start\":79745},{\"end\":79767,\"start\":79762},{\"end\":79786,\"start\":79775},{\"end\":80113,\"start\":80109},{\"end\":80130,\"start\":80123},{\"end\":80146,\"start\":80139},{\"end\":80162,\"start\":80156},{\"end\":80183,\"start\":80173},{\"end\":80201,\"start\":80193},{\"end\":80216,\"start\":80211},{\"end\":80239,\"start\":80235},{\"end\":80253,\"start\":80247},{\"end\":80655,\"start\":80649},{\"end\":80669,\"start\":80665},{\"end\":80683,\"start\":80679},{\"end\":80697,\"start\":80692},{\"end\":80714,\"start\":80709},{\"end\":80727,\"start\":80722},{\"end\":80729,\"start\":80728},{\"end\":80743,\"start\":80737},{\"end\":80757,\"start\":80752},{\"end\":81103,\"start\":81092},{\"end\":81122,\"start\":81114},{\"end\":81136,\"start\":81132},{\"end\":81566,\"start\":81558},{\"end\":81582,\"start\":81573},{\"end\":81596,\"start\":81589},{\"end\":81607,\"start\":81601},{\"end\":81617,\"start\":81612},{\"end\":81626,\"start\":81623},{\"end\":81639,\"start\":81632},{\"end\":81647,\"start\":81645},{\"end\":81659,\"start\":81653},{\"end\":82031,\"start\":82027},{\"end\":82040,\"start\":82038},{\"end\":82050,\"start\":82047},{\"end\":82063,\"start\":82056},{\"end\":82074,\"start\":82069},{\"end\":82087,\"start\":82080},{\"end\":82099,\"start\":82092},{\"end\":82109,\"start\":82104},{\"end\":82123,\"start\":82116},{\"end\":82137,\"start\":82130},{\"end\":82531,\"start\":82525},{\"end\":82544,\"start\":82538},{\"end\":82553,\"start\":82550},{\"end\":82568,\"start\":82560},{\"end\":82581,\"start\":82576},{\"end\":82592,\"start\":82586},{\"end\":82605,\"start\":82598},{\"end\":82614,\"start\":82610},{\"end\":82624,\"start\":82619},{\"end\":82638,\"start\":82629},{\"end\":83147,\"start\":83141},{\"end\":83158,\"start\":83154},{\"end\":83169,\"start\":83164},{\"end\":83183,\"start\":83174},{\"end\":83195,\"start\":83189},{\"end\":83206,\"start\":83202},{\"end\":83218,\"start\":83214},{\"end\":83227,\"start\":83223},{\"end\":83237,\"start\":83233},{\"end\":83586,\"start\":83580},{\"end\":83599,\"start\":83593},{\"end\":83607,\"start\":83605},{\"end\":83619,\"start\":83614},{\"end\":83636,\"start\":83628},{\"end\":83648,\"start\":83643},{\"end\":83659,\"start\":83654},{\"end\":83677,\"start\":83670},{\"end\":83707,\"start\":83699},{\"end\":84119,\"start\":84112},{\"end\":84129,\"start\":84126},{\"end\":84139,\"start\":84136},{\"end\":84152,\"start\":84145},{\"end\":84165,\"start\":84159},{\"end\":84475,\"start\":84468},{\"end\":84485,\"start\":84482},{\"end\":84495,\"start\":84492},{\"end\":84508,\"start\":84501},{\"end\":84521,\"start\":84515},{\"end\":84757,\"start\":84750},{\"end\":84772,\"start\":84764},{\"end\":84783,\"start\":84780},{\"end\":84792,\"start\":84789},{\"end\":84806,\"start\":84799},{\"end\":84819,\"start\":84813},{\"end\":85149,\"start\":85142},{\"end\":85163,\"start\":85156},{\"end\":85178,\"start\":85171},{\"end\":85192,\"start\":85187},{\"end\":85199,\"start\":85198},{\"end\":85212,\"start\":85206},{\"end\":85228,\"start\":85220},{\"end\":85607,\"start\":85600},{\"end\":85621,\"start\":85614},{\"end\":85635,\"start\":85630},{\"end\":85661,\"start\":85654},{\"end\":85677,\"start\":85669},{\"end\":85693,\"start\":85687},{\"end\":85710,\"start\":85705},{\"end\":85722,\"start\":85718},{\"end\":85751,\"start\":85744},{\"end\":85763,\"start\":85758},{\"end\":86156,\"start\":86151},{\"end\":86169,\"start\":86162},{\"end\":86178,\"start\":86177},{\"end\":86194,\"start\":86188},{\"end\":86206,\"start\":86201},{\"end\":86210,\"start\":86207},{\"end\":86221,\"start\":86216},{\"end\":86229,\"start\":86226},{\"end\":86243,\"start\":86242},{\"end\":86258,\"start\":86252},{\"end\":86592,\"start\":86585},{\"end\":86606,\"start\":86597},{\"end\":86619,\"start\":86612},{\"end\":86632,\"start\":86624},{\"end\":86646,\"start\":86639},{\"end\":86656,\"start\":86653},{\"end\":86976,\"start\":86972},{\"end\":86987,\"start\":86982},{\"end\":86999,\"start\":86993},{\"end\":87012,\"start\":87006},{\"end\":87024,\"start\":87019},{\"end\":87034,\"start\":87030},{\"end\":87049,\"start\":87042},{\"end\":87060,\"start\":87056},{\"end\":87552,\"start\":87549},{\"end\":87560,\"start\":87558},{\"end\":87575,\"start\":87568},{\"end\":87584,\"start\":87580},{\"end\":87595,\"start\":87591},{\"end\":87607,\"start\":87601},{\"end\":87621,\"start\":87614},{\"end\":87930,\"start\":87921},{\"end\":87940,\"start\":87937},{\"end\":87954,\"start\":87946},{\"end\":87968,\"start\":87961},{\"end\":87979,\"start\":87973},{\"end\":87994,\"start\":87987},{\"end\":88005,\"start\":88000},{\"end\":88016,\"start\":88010},{\"end\":88422,\"start\":88413},{\"end\":88432,\"start\":88429},{\"end\":88446,\"start\":88438},{\"end\":88460,\"start\":88453},{\"end\":88471,\"start\":88465},{\"end\":88486,\"start\":88479},{\"end\":88497,\"start\":88492},{\"end\":88508,\"start\":88502},{\"end\":88864,\"start\":88855},{\"end\":88877,\"start\":88871},{\"end\":88890,\"start\":88882},{\"end\":88902,\"start\":88897},{\"end\":88913,\"start\":88908},{\"end\":88931,\"start\":88925},{\"end\":88946,\"start\":88939},{\"end\":88954,\"start\":88952},{\"end\":88967,\"start\":88960},{\"end\":88980,\"start\":88974},{\"end\":89339,\"start\":89335},{\"end\":89347,\"start\":89345},{\"end\":89363,\"start\":89355},{\"end\":89378,\"start\":89371},{\"end\":89687,\"start\":89680},{\"end\":89699,\"start\":89692},{\"end\":89713,\"start\":89709},{\"end\":89729,\"start\":89720},{\"end\":89731,\"start\":89730},{\"end\":89744,\"start\":89738},{\"end\":89746,\"start\":89745},{\"end\":90034,\"start\":90028},{\"end\":90044,\"start\":90041},{\"end\":90056,\"start\":90049},{\"end\":90067,\"start\":90063},{\"end\":90086,\"start\":90075},{\"end\":90286,\"start\":90281},{\"end\":90297,\"start\":90293},{\"end\":90312,\"start\":90303},{\"end\":90322,\"start\":90317},{\"end\":90334,\"start\":90329},{\"end\":90344,\"start\":90340},{\"end\":90357,\"start\":90351},{\"end\":90369,\"start\":90364},{\"end\":90379,\"start\":90374},{\"end\":90391,\"start\":90387},{\"end\":90695,\"start\":90690},{\"end\":90710,\"start\":90703},{\"end\":90724,\"start\":90719},{\"end\":90737,\"start\":90732},{\"end\":90751,\"start\":90747},{\"end\":90765,\"start\":90758},{\"end\":90783,\"start\":90772},{\"end\":90795,\"start\":90791},{\"end\":90806,\"start\":90802},{\"end\":90813,\"start\":90811},{\"end\":91155,\"start\":91148},{\"end\":91170,\"start\":91163},{\"end\":91182,\"start\":91177},{\"end\":91444,\"start\":91439},{\"end\":91453,\"start\":91450},{\"end\":91468,\"start\":91460},{\"end\":91480,\"start\":91475},{\"end\":91492,\"start\":91485},{\"end\":91816,\"start\":91810},{\"end\":91828,\"start\":91822},{\"end\":91840,\"start\":91834},{\"end\":91854,\"start\":91847},{\"end\":91870,\"start\":91861},{\"end\":91883,\"start\":91875},{\"end\":91896,\"start\":91890},{\"end\":92237,\"start\":92231},{\"end\":92249,\"start\":92243},{\"end\":92259,\"start\":92254},{\"end\":92267,\"start\":92264},{\"end\":92280,\"start\":92272},{\"end\":92293,\"start\":92287},{\"end\":92692,\"start\":92686},{\"end\":92704,\"start\":92698},{\"end\":92713,\"start\":92710},{\"end\":92725,\"start\":92718},{\"end\":92739,\"start\":92730},{\"end\":92751,\"start\":92744},{\"end\":92764,\"start\":92758}]", "bib_author_last_name": "[{\"end\":58636,\"start\":58615},{\"end\":58653,\"start\":58646},{\"end\":58666,\"start\":58663},{\"end\":58678,\"start\":58673},{\"end\":58689,\"start\":58685},{\"end\":58703,\"start\":58697},{\"end\":58716,\"start\":58712},{\"end\":58730,\"start\":58724},{\"end\":58748,\"start\":58740},{\"end\":58758,\"start\":58750},{\"end\":59150,\"start\":59142},{\"end\":59163,\"start\":59161},{\"end\":59178,\"start\":59171},{\"end\":59192,\"start\":59187},{\"end\":59206,\"start\":59199},{\"end\":59221,\"start\":59216},{\"end\":59232,\"start\":59227},{\"end\":59700,\"start\":59695},{\"end\":59719,\"start\":59712},{\"end\":59730,\"start\":59728},{\"end\":59749,\"start\":59741},{\"end\":59762,\"start\":59757},{\"end\":59780,\"start\":59773},{\"end\":59793,\"start\":59787},{\"end\":60192,\"start\":60189},{\"end\":60210,\"start\":60200},{\"end\":60226,\"start\":60219},{\"end\":60242,\"start\":60233},{\"end\":60256,\"start\":60251},{\"end\":60567,\"start\":60558},{\"end\":60578,\"start\":60574},{\"end\":60597,\"start\":60588},{\"end\":60864,\"start\":60859},{\"end\":60879,\"start\":60875},{\"end\":60891,\"start\":60886},{\"end\":60908,\"start\":60901},{\"end\":60924,\"start\":60918},{\"end\":60943,\"start\":60935},{\"end\":60963,\"start\":60952},{\"end\":60977,\"start\":60972},{\"end\":60992,\"start\":60986},{\"end\":61007,\"start\":61001},{\"end\":61391,\"start\":61385},{\"end\":61408,\"start\":61403},{\"end\":61426,\"start\":61418},{\"end\":61443,\"start\":61436},{\"end\":61463,\"start\":61455},{\"end\":61481,\"start\":61472},{\"end\":61808,\"start\":61804},{\"end\":61824,\"start\":61818},{\"end\":61833,\"start\":61831},{\"end\":61842,\"start\":61837},{\"end\":61858,\"start\":61853},{\"end\":61866,\"start\":61860},{\"end\":62136,\"start\":62132},{\"end\":62152,\"start\":62146},{\"end\":62161,\"start\":62159},{\"end\":62175,\"start\":62172},{\"end\":62190,\"start\":62185},{\"end\":62207,\"start\":62201},{\"end\":62493,\"start\":62489},{\"end\":62503,\"start\":62499},{\"end\":62517,\"start\":62514},{\"end\":62539,\"start\":62531},{\"end\":62554,\"start\":62549},{\"end\":62568,\"start\":62562},{\"end\":62588,\"start\":62581},{\"end\":62910,\"start\":62906},{\"end\":62921,\"start\":62919},{\"end\":62933,\"start\":62931},{\"end\":62949,\"start\":62944},{\"end\":62963,\"start\":62958},{\"end\":62972,\"start\":62969},{\"end\":62982,\"start\":62977},{\"end\":62996,\"start\":62993},{\"end\":63418,\"start\":63414},{\"end\":63431,\"start\":63427},{\"end\":63444,\"start\":63440},{\"end\":63455,\"start\":63453},{\"end\":63464,\"start\":63462},{\"end\":63476,\"start\":63473},{\"end\":63485,\"start\":63481},{\"end\":63808,\"start\":63805},{\"end\":63817,\"start\":63814},{\"end\":63826,\"start\":63823},{\"end\":63840,\"start\":63834},{\"end\":64059,\"start\":64050},{\"end\":64069,\"start\":64064},{\"end\":64081,\"start\":64078},{\"end\":64097,\"start\":64090},{\"end\":64106,\"start\":64102},{\"end\":64119,\"start\":64116},{\"end\":64131,\"start\":64126},{\"end\":64142,\"start\":64140},{\"end\":64156,\"start\":64152},{\"end\":64166,\"start\":64158},{\"end\":64509,\"start\":64505},{\"end\":64515,\"start\":64511},{\"end\":64598,\"start\":64592},{\"end\":64614,\"start\":64609},{\"end\":64626,\"start\":64623},{\"end\":64651,\"start\":64647},{\"end\":65052,\"start\":65041},{\"end\":65065,\"start\":65060},{\"end\":65087,\"start\":65077},{\"end\":65105,\"start\":65094},{\"end\":65119,\"start\":65115},{\"end\":65139,\"start\":65128},{\"end\":65157,\"start\":65149},{\"end\":65176,\"start\":65168},{\"end\":65191,\"start\":65184},{\"end\":65206,\"start\":65201},{\"end\":65667,\"start\":65663},{\"end\":65677,\"start\":65673},{\"end\":65689,\"start\":65686},{\"end\":65699,\"start\":65696},{\"end\":65710,\"start\":65708},{\"end\":65725,\"start\":65721},{\"end\":65739,\"start\":65734},{\"end\":65753,\"start\":65749},{\"end\":65762,\"start\":65759},{\"end\":65767,\"start\":65764},{\"end\":66118,\"start\":66115},{\"end\":66133,\"start\":66129},{\"end\":66144,\"start\":66142},{\"end\":66154,\"start\":66151},{\"end\":66164,\"start\":66159},{\"end\":66178,\"start\":66175},{\"end\":66517,\"start\":66512},{\"end\":66531,\"start\":66525},{\"end\":66546,\"start\":66538},{\"end\":66970,\"start\":66965},{\"end\":66984,\"start\":66978},{\"end\":67004,\"start\":66996},{\"end\":67017,\"start\":67012},{\"end\":67218,\"start\":67216},{\"end\":67236,\"start\":67228},{\"end\":67250,\"start\":67244},{\"end\":67265,\"start\":67257},{\"end\":67620,\"start\":67618},{\"end\":67635,\"start\":67630},{\"end\":67649,\"start\":67646},{\"end\":67659,\"start\":67656},{\"end\":68023,\"start\":68017},{\"end\":68034,\"start\":68032},{\"end\":68048,\"start\":68044},{\"end\":68063,\"start\":68057},{\"end\":68082,\"start\":68073},{\"end\":68092,\"start\":68090},{\"end\":68101,\"start\":68097},{\"end\":68114,\"start\":68110},{\"end\":68120,\"start\":68116},{\"end\":68126,\"start\":68122},{\"end\":68529,\"start\":68525},{\"end\":68546,\"start\":68534},{\"end\":68566,\"start\":68558},{\"end\":68582,\"start\":68574},{\"end\":68596,\"start\":68591},{\"end\":68605,\"start\":68603},{\"end\":68619,\"start\":68612},{\"end\":68632,\"start\":68628},{\"end\":68642,\"start\":68639},{\"end\":68661,\"start\":68650},{\"end\":69018,\"start\":69015},{\"end\":69031,\"start\":69027},{\"end\":69053,\"start\":69040},{\"end\":69067,\"start\":69061},{\"end\":69085,\"start\":69077},{\"end\":69104,\"start\":69095},{\"end\":69109,\"start\":69106},{\"end\":69454,\"start\":69448},{\"end\":69468,\"start\":69463},{\"end\":69480,\"start\":69475},{\"end\":69498,\"start\":69490},{\"end\":69970,\"start\":69968},{\"end\":69982,\"start\":69979},{\"end\":69996,\"start\":69991},{\"end\":70008,\"start\":70005},{\"end\":70022,\"start\":70020},{\"end\":70033,\"start\":70029},{\"end\":70047,\"start\":70043},{\"end\":70056,\"start\":70052},{\"end\":70071,\"start\":70067},{\"end\":70084,\"start\":70080},{\"end\":70494,\"start\":70492},{\"end\":70505,\"start\":70503},{\"end\":70522,\"start\":70514},{\"end\":70534,\"start\":70531},{\"end\":70833,\"start\":70831},{\"end\":70843,\"start\":70841},{\"end\":70852,\"start\":70848},{\"end\":70863,\"start\":70861},{\"end\":70876,\"start\":70872},{\"end\":70886,\"start\":70883},{\"end\":70897,\"start\":70893},{\"end\":70909,\"start\":70905},{\"end\":70918,\"start\":70914},{\"end\":71227,\"start\":71224},{\"end\":71240,\"start\":71235},{\"end\":71255,\"start\":71247},{\"end\":71640,\"start\":71637},{\"end\":71655,\"start\":71650},{\"end\":71671,\"start\":71663},{\"end\":71683,\"start\":71679},{\"end\":71698,\"start\":71692},{\"end\":71712,\"start\":71705},{\"end\":71726,\"start\":71720},{\"end\":71746,\"start\":71739},{\"end\":72046,\"start\":72043},{\"end\":72059,\"start\":72057},{\"end\":72072,\"start\":72070},{\"end\":72086,\"start\":72083},{\"end\":72358,\"start\":72355},{\"end\":72370,\"start\":72367},{\"end\":72379,\"start\":72376},{\"end\":72387,\"start\":72385},{\"end\":72399,\"start\":72396},{\"end\":72412,\"start\":72407},{\"end\":72425,\"start\":72422},{\"end\":72438,\"start\":72435},{\"end\":72831,\"start\":72828},{\"end\":72841,\"start\":72839},{\"end\":72854,\"start\":72850},{\"end\":72867,\"start\":72863},{\"end\":72876,\"start\":72872},{\"end\":72889,\"start\":72885},{\"end\":72905,\"start\":72900},{\"end\":72916,\"start\":72912},{\"end\":72928,\"start\":72926},{\"end\":72940,\"start\":72938},{\"end\":73366,\"start\":73356},{\"end\":73380,\"start\":73374},{\"end\":73638,\"start\":73628},{\"end\":73652,\"start\":73646},{\"end\":73896,\"start\":73894},{\"end\":73915,\"start\":73910},{\"end\":73930,\"start\":73923},{\"end\":73948,\"start\":73940},{\"end\":73968,\"start\":73960},{\"end\":74284,\"start\":74281},{\"end\":74300,\"start\":74295},{\"end\":74318,\"start\":74312},{\"end\":74332,\"start\":74325},{\"end\":74347,\"start\":74341},{\"end\":74361,\"start\":74355},{\"end\":74761,\"start\":74755},{\"end\":74883,\"start\":74874},{\"end\":75048,\"start\":75042},{\"end\":75060,\"start\":75058},{\"end\":75070,\"start\":75065},{\"end\":75085,\"start\":75078},{\"end\":75105,\"start\":75095},{\"end\":75121,\"start\":75114},{\"end\":75134,\"start\":75129},{\"end\":75152,\"start\":75145},{\"end\":75168,\"start\":75163},{\"end\":75178,\"start\":75175},{\"end\":75595,\"start\":75587},{\"end\":75609,\"start\":75603},{\"end\":75620,\"start\":75616},{\"end\":75634,\"start\":75631},{\"end\":76096,\"start\":76089},{\"end\":76116,\"start\":76106},{\"end\":76130,\"start\":76122},{\"end\":76146,\"start\":76137},{\"end\":76375,\"start\":76368},{\"end\":76395,\"start\":76385},{\"end\":76409,\"start\":76401},{\"end\":76425,\"start\":76416},{\"end\":76644,\"start\":76637},{\"end\":76656,\"start\":76654},{\"end\":76669,\"start\":76664},{\"end\":76681,\"start\":76677},{\"end\":76695,\"start\":76689},{\"end\":76711,\"start\":76702},{\"end\":76914,\"start\":76910},{\"end\":76928,\"start\":76923},{\"end\":76946,\"start\":76937},{\"end\":76972,\"start\":76961},{\"end\":76991,\"start\":76984},{\"end\":77012,\"start\":77001},{\"end\":77025,\"start\":77018},{\"end\":77038,\"start\":77032},{\"end\":77050,\"start\":77047},{\"end\":77076,\"start\":77064},{\"end\":77471,\"start\":77459},{\"end\":77480,\"start\":77478},{\"end\":77495,\"start\":77487},{\"end\":77500,\"start\":77497},{\"end\":77813,\"start\":77807},{\"end\":77829,\"start\":77823},{\"end\":77848,\"start\":77839},{\"end\":77863,\"start\":77857},{\"end\":77878,\"start\":77874},{\"end\":77884,\"start\":77880},{\"end\":78344,\"start\":78340},{\"end\":78357,\"start\":78353},{\"end\":78365,\"start\":78362},{\"end\":78379,\"start\":78377},{\"end\":78391,\"start\":78389},{\"end\":78407,\"start\":78401},{\"end\":78733,\"start\":78731},{\"end\":78745,\"start\":78742},{\"end\":78758,\"start\":78755},{\"end\":78768,\"start\":78766},{\"end\":78776,\"start\":78774},{\"end\":78787,\"start\":78782},{\"end\":78796,\"start\":78792},{\"end\":78811,\"start\":78807},{\"end\":78821,\"start\":78817},{\"end\":78833,\"start\":78830},{\"end\":79329,\"start\":79326},{\"end\":79341,\"start\":79338},{\"end\":79352,\"start\":79347},{\"end\":79361,\"start\":79357},{\"end\":79376,\"start\":79372},{\"end\":79388,\"start\":79385},{\"end\":79686,\"start\":79681},{\"end\":79704,\"start\":79695},{\"end\":79718,\"start\":79713},{\"end\":79731,\"start\":79725},{\"end\":79743,\"start\":79741},{\"end\":79760,\"start\":79752},{\"end\":79773,\"start\":79768},{\"end\":79796,\"start\":79787},{\"end\":80121,\"start\":80114},{\"end\":80137,\"start\":80131},{\"end\":80154,\"start\":80147},{\"end\":80171,\"start\":80163},{\"end\":80191,\"start\":80184},{\"end\":80209,\"start\":80202},{\"end\":80233,\"start\":80217},{\"end\":80245,\"start\":80240},{\"end\":80260,\"start\":80254},{\"end\":80267,\"start\":80262},{\"end\":80663,\"start\":80656},{\"end\":80677,\"start\":80670},{\"end\":80690,\"start\":80684},{\"end\":80707,\"start\":80698},{\"end\":80720,\"start\":80715},{\"end\":80735,\"start\":80730},{\"end\":80750,\"start\":80744},{\"end\":80768,\"start\":80758},{\"end\":81112,\"start\":81104},{\"end\":81130,\"start\":81123},{\"end\":81143,\"start\":81137},{\"end\":81571,\"start\":81567},{\"end\":81587,\"start\":81583},{\"end\":81599,\"start\":81597},{\"end\":81610,\"start\":81608},{\"end\":81621,\"start\":81618},{\"end\":81630,\"start\":81627},{\"end\":81643,\"start\":81640},{\"end\":81651,\"start\":81648},{\"end\":81664,\"start\":81660},{\"end\":82036,\"start\":82032},{\"end\":82045,\"start\":82041},{\"end\":82054,\"start\":82051},{\"end\":82067,\"start\":82064},{\"end\":82078,\"start\":82075},{\"end\":82090,\"start\":82088},{\"end\":82102,\"start\":82100},{\"end\":82114,\"start\":82110},{\"end\":82128,\"start\":82124},{\"end\":82142,\"start\":82138},{\"end\":82536,\"start\":82532},{\"end\":82548,\"start\":82545},{\"end\":82558,\"start\":82554},{\"end\":82574,\"start\":82569},{\"end\":82584,\"start\":82582},{\"end\":82596,\"start\":82593},{\"end\":82608,\"start\":82606},{\"end\":82617,\"start\":82615},{\"end\":82627,\"start\":82625},{\"end\":82641,\"start\":82639},{\"end\":83152,\"start\":83148},{\"end\":83162,\"start\":83159},{\"end\":83172,\"start\":83170},{\"end\":83187,\"start\":83184},{\"end\":83200,\"start\":83196},{\"end\":83212,\"start\":83207},{\"end\":83221,\"start\":83219},{\"end\":83231,\"start\":83228},{\"end\":83242,\"start\":83238},{\"end\":83591,\"start\":83587},{\"end\":83603,\"start\":83600},{\"end\":83612,\"start\":83608},{\"end\":83626,\"start\":83620},{\"end\":83641,\"start\":83637},{\"end\":83652,\"start\":83649},{\"end\":83668,\"start\":83660},{\"end\":83697,\"start\":83678},{\"end\":83715,\"start\":83708},{\"end\":83720,\"start\":83717},{\"end\":84124,\"start\":84120},{\"end\":84134,\"start\":84130},{\"end\":84143,\"start\":84140},{\"end\":84157,\"start\":84153},{\"end\":84171,\"start\":84166},{\"end\":84480,\"start\":84476},{\"end\":84490,\"start\":84486},{\"end\":84499,\"start\":84496},{\"end\":84513,\"start\":84509},{\"end\":84527,\"start\":84522},{\"end\":84762,\"start\":84758},{\"end\":84778,\"start\":84773},{\"end\":84787,\"start\":84784},{\"end\":84797,\"start\":84793},{\"end\":84811,\"start\":84807},{\"end\":84825,\"start\":84820},{\"end\":85154,\"start\":85150},{\"end\":85169,\"start\":85164},{\"end\":85185,\"start\":85179},{\"end\":85196,\"start\":85193},{\"end\":85204,\"start\":85200},{\"end\":85218,\"start\":85213},{\"end\":85237,\"start\":85229},{\"end\":85249,\"start\":85239},{\"end\":85612,\"start\":85608},{\"end\":85628,\"start\":85622},{\"end\":85652,\"start\":85636},{\"end\":85667,\"start\":85662},{\"end\":85685,\"start\":85678},{\"end\":85703,\"start\":85694},{\"end\":85716,\"start\":85711},{\"end\":85742,\"start\":85723},{\"end\":85756,\"start\":85752},{\"end\":85768,\"start\":85764},{\"end\":86160,\"start\":86157},{\"end\":86175,\"start\":86170},{\"end\":86186,\"start\":86179},{\"end\":86199,\"start\":86195},{\"end\":86214,\"start\":86211},{\"end\":86224,\"start\":86222},{\"end\":86236,\"start\":86230},{\"end\":86240,\"start\":86238},{\"end\":86250,\"start\":86244},{\"end\":86262,\"start\":86259},{\"end\":86266,\"start\":86264},{\"end\":86595,\"start\":86593},{\"end\":86610,\"start\":86607},{\"end\":86622,\"start\":86620},{\"end\":86637,\"start\":86633},{\"end\":86651,\"start\":86647},{\"end\":86661,\"start\":86657},{\"end\":86980,\"start\":86977},{\"end\":86991,\"start\":86988},{\"end\":87004,\"start\":87000},{\"end\":87017,\"start\":87013},{\"end\":87028,\"start\":87025},{\"end\":87040,\"start\":87035},{\"end\":87054,\"start\":87050},{\"end\":87064,\"start\":87061},{\"end\":87556,\"start\":87553},{\"end\":87566,\"start\":87561},{\"end\":87578,\"start\":87576},{\"end\":87589,\"start\":87585},{\"end\":87599,\"start\":87596},{\"end\":87612,\"start\":87608},{\"end\":87624,\"start\":87622},{\"end\":87935,\"start\":87931},{\"end\":87944,\"start\":87941},{\"end\":87959,\"start\":87955},{\"end\":87971,\"start\":87969},{\"end\":87985,\"start\":87980},{\"end\":87998,\"start\":87995},{\"end\":88008,\"start\":88006},{\"end\":88021,\"start\":88017},{\"end\":88427,\"start\":88423},{\"end\":88436,\"start\":88433},{\"end\":88451,\"start\":88447},{\"end\":88463,\"start\":88461},{\"end\":88477,\"start\":88472},{\"end\":88490,\"start\":88487},{\"end\":88500,\"start\":88498},{\"end\":88513,\"start\":88509},{\"end\":88869,\"start\":88865},{\"end\":88880,\"start\":88878},{\"end\":88895,\"start\":88891},{\"end\":88906,\"start\":88903},{\"end\":88923,\"start\":88914},{\"end\":88937,\"start\":88932},{\"end\":88950,\"start\":88947},{\"end\":88958,\"start\":88955},{\"end\":88972,\"start\":88968},{\"end\":88985,\"start\":88981},{\"end\":89343,\"start\":89340},{\"end\":89353,\"start\":89348},{\"end\":89369,\"start\":89364},{\"end\":89382,\"start\":89379},{\"end\":89690,\"start\":89688},{\"end\":89707,\"start\":89700},{\"end\":89718,\"start\":89714},{\"end\":89736,\"start\":89732},{\"end\":89751,\"start\":89747},{\"end\":90039,\"start\":90035},{\"end\":90047,\"start\":90045},{\"end\":90061,\"start\":90057},{\"end\":90073,\"start\":90068},{\"end\":90090,\"start\":90087},{\"end\":90291,\"start\":90287},{\"end\":90301,\"start\":90298},{\"end\":90315,\"start\":90313},{\"end\":90327,\"start\":90323},{\"end\":90338,\"start\":90335},{\"end\":90349,\"start\":90345},{\"end\":90362,\"start\":90358},{\"end\":90372,\"start\":90370},{\"end\":90385,\"start\":90380},{\"end\":90395,\"start\":90392},{\"end\":90701,\"start\":90696},{\"end\":90717,\"start\":90711},{\"end\":90730,\"start\":90725},{\"end\":90745,\"start\":90738},{\"end\":90756,\"start\":90752},{\"end\":90770,\"start\":90766},{\"end\":90789,\"start\":90784},{\"end\":90800,\"start\":90796},{\"end\":90809,\"start\":90807},{\"end\":90826,\"start\":90814},{\"end\":91161,\"start\":91156},{\"end\":91175,\"start\":91171},{\"end\":91186,\"start\":91183},{\"end\":91448,\"start\":91445},{\"end\":91458,\"start\":91454},{\"end\":91473,\"start\":91469},{\"end\":91483,\"start\":91481},{\"end\":91502,\"start\":91493},{\"end\":91820,\"start\":91817},{\"end\":91832,\"start\":91829},{\"end\":91845,\"start\":91841},{\"end\":91859,\"start\":91855},{\"end\":91873,\"start\":91871},{\"end\":91888,\"start\":91884},{\"end\":91900,\"start\":91897},{\"end\":92241,\"start\":92238},{\"end\":92252,\"start\":92250},{\"end\":92262,\"start\":92260},{\"end\":92270,\"start\":92268},{\"end\":92285,\"start\":92281},{\"end\":92297,\"start\":92294},{\"end\":92696,\"start\":92693},{\"end\":92708,\"start\":92705},{\"end\":92716,\"start\":92714},{\"end\":92728,\"start\":92726},{\"end\":92742,\"start\":92740},{\"end\":92756,\"start\":92752},{\"end\":92768,\"start\":92765}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2204.14198\",\"id\":\"b0\"},\"end\":59049,\"start\":58553},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3753452},\"end\":59651,\"start\":59051},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3180429},\"end\":60143,\"start\":59653},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":251979350},\"end\":60488,\"start\":60145},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":231861462},\"end\":60814,\"start\":60490},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":218971783},\"end\":61328,\"start\":60816},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":218889832},\"end\":61736,\"start\":61330},{\"attributes\":{\"doi\":\"arXiv:2109.10852\",\"id\":\"b7\"},\"end\":62078,\"start\":61738},{\"attributes\":{\"doi\":\"arXiv:2206.07669\",\"id\":\"b8\"},\"end\":62416,\"start\":62080},{\"attributes\":{\"doi\":\"arXiv:1504.00325\",\"id\":\"b9\"},\"end\":62841,\"start\":62418},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":216080982},\"end\":63358,\"start\":62843},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":248834106},\"end\":63740,\"start\":63360},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":231802355},\"end\":64048,\"start\":63742},{\"attributes\":{\"doi\":\"arXiv:2210.11416\",\"id\":\"b13\"},\"end\":64501,\"start\":64050},{\"attributes\":{\"id\":\"b14\"},\"end\":64584,\"start\":64503},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b15\"},\"end\":64956,\"start\":64586},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":225039882},\"end\":65583,\"start\":64958},{\"attributes\":{\"doi\":\"arXiv:2211.07636\",\"id\":\"b17\"},\"end\":66027,\"start\":65585},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":219573512},\"end\":66444,\"start\":66029},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":195441339},\"end\":66916,\"start\":66446},{\"attributes\":{\"doi\":\"arXiv:2104.00743\",\"id\":\"b20\"},\"end\":67194,\"start\":66918},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":54465873},\"end\":67562,\"start\":67196},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":206594692},\"end\":68013,\"start\":67564},{\"attributes\":{\"doi\":\"arXiv:2106.09685\",\"id\":\"b23\"},\"end\":68418,\"start\":68015},{\"attributes\":{\"doi\":\"arXiv:2212.12017\",\"id\":\"b24\"},\"end\":68983,\"start\":68420},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":247618727},\"end\":69335,\"start\":68985},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":233393962},\"end\":69875,\"start\":69337},{\"attributes\":{\"doi\":\"arXiv:2211.09808\",\"id\":\"b27\"},\"end\":70379,\"start\":69877},{\"attributes\":{\"doi\":\"arXiv:2301.12597\",\"id\":\"b28\"},\"end\":70775,\"start\":70381},{\"attributes\":{\"doi\":\"arXiv:2305.06355\",\"id\":\"b29\"},\"end\":71144,\"start\":70777},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":206771220},\"end\":71583,\"start\":71146},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":14113767},\"end\":72033,\"start\":71585},{\"attributes\":{\"doi\":\"arXiv:2304.08485\",\"id\":\"b32\"},\"end\":72277,\"start\":72035},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":232352874},\"end\":72817,\"start\":72279},{\"attributes\":{\"doi\":\"arXiv:2305.05662\",\"id\":\"b34\"},\"end\":73310,\"start\":72819},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":53592270},\"end\":73567,\"start\":73312},{\"attributes\":{\"doi\":\"arXiv:1608.03983\",\"id\":\"b36\"},\"end\":73812,\"start\":73569},{\"attributes\":{\"doi\":\"arXiv:2206.08916\",\"id\":\"b37\"},\"end\":74207,\"start\":73814},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":8745888},\"end\":74751,\"start\":74209},{\"attributes\":{\"id\":\"b39\"},\"end\":74814,\"start\":74753},{\"attributes\":{\"id\":\"b40\"},\"end\":74966,\"start\":74816},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":246426909},\"end\":75513,\"start\":74968},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":11080756},\"end\":76021,\"start\":75515},{\"attributes\":{\"id\":\"b43\"},\"end\":76292,\"start\":76023},{\"attributes\":{\"id\":\"b44\"},\"end\":76577,\"start\":76294},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":160025533},\"end\":76900,\"start\":76579},{\"attributes\":{\"id\":\"b46\"},\"end\":77369,\"start\":76902},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":10328909},\"end\":77749,\"start\":77371},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":206594923},\"end\":78254,\"start\":77751},{\"attributes\":{\"id\":\"b49\"},\"end\":78643,\"start\":78256},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":253581779},\"end\":79241,\"start\":78645},{\"attributes\":{\"id\":\"b51\"},\"end\":79615,\"start\":79243},{\"attributes\":{\"id\":\"b52\"},\"end\":80107,\"start\":79617},{\"attributes\":{\"id\":\"b53\"},\"end\":80620,\"start\":80109},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":13756489},\"end\":81037,\"start\":80622},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":9026666},\"end\":81487,\"start\":81039},{\"attributes\":{\"id\":\"b56\"},\"end\":81922,\"start\":81489},{\"attributes\":{\"id\":\"b57\"},\"end\":82446,\"start\":81924},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":253446956},\"end\":83079,\"start\":82448},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":235652212},\"end\":83490,\"start\":83081},{\"attributes\":{\"id\":\"b60\"},\"end\":84033,\"start\":83492},{\"attributes\":{\"id\":\"b61\"},\"end\":84389,\"start\":84035},{\"attributes\":{\"id\":\"b62\"},\"end\":84748,\"start\":84391},{\"attributes\":{\"id\":\"b63\"},\"end\":85067,\"start\":84750},{\"attributes\":{\"id\":\"b64\"},\"end\":85516,\"start\":85069},{\"attributes\":{\"id\":\"b65\"},\"end\":86099,\"start\":85518},{\"attributes\":{\"id\":\"b66\"},\"end\":86507,\"start\":86101},{\"attributes\":{\"id\":\"b67\"},\"end\":86898,\"start\":86509},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":203593662},\"end\":87482,\"start\":86900},{\"attributes\":{\"id\":\"b69\"},\"end\":87842,\"start\":87484},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":251105279},\"end\":88334,\"start\":87844},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":251105279},\"end\":88788,\"start\":88336},{\"attributes\":{\"id\":\"b72\"},\"end\":89265,\"start\":88790},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":237635382},\"end\":89635,\"start\":89267},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":1688357},\"end\":89981,\"start\":89637},{\"attributes\":{\"id\":\"b75\"},\"end\":90279,\"start\":89983},{\"attributes\":{\"id\":\"b76\"},\"end\":90688,\"start\":90281},{\"attributes\":{\"id\":\"b77\"},\"end\":91144,\"start\":90690},{\"attributes\":{\"id\":\"b78\"},\"end\":91350,\"start\":91146},{\"attributes\":{\"id\":\"b79\"},\"end\":91732,\"start\":91352},{\"attributes\":{\"id\":\"b80\"},\"end\":92155,\"start\":91734},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":222208633},\"end\":92579,\"start\":92157},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":244799261},\"end\":93221,\"start\":92581}]", "bib_title": "[{\"end\":59134,\"start\":59051},{\"end\":59683,\"start\":59653},{\"end\":60182,\"start\":60145},{\"end\":60550,\"start\":60490},{\"end\":60853,\"start\":60816},{\"end\":61375,\"start\":61330},{\"end\":62895,\"start\":62843},{\"end\":63408,\"start\":63360},{\"end\":63796,\"start\":63742},{\"end\":65032,\"start\":64958},{\"end\":66109,\"start\":66029},{\"end\":66504,\"start\":66446},{\"end\":67206,\"start\":67196},{\"end\":67608,\"start\":67564},{\"end\":69005,\"start\":68985},{\"end\":69436,\"start\":69337},{\"end\":71213,\"start\":71146},{\"end\":71626,\"start\":71585},{\"end\":72350,\"start\":72279},{\"end\":73349,\"start\":73312},{\"end\":74272,\"start\":74209},{\"end\":75035,\"start\":74968},{\"end\":75577,\"start\":75515},{\"end\":76630,\"start\":76579},{\"end\":77449,\"start\":77371},{\"end\":77803,\"start\":77751},{\"end\":78722,\"start\":78645},{\"end\":80647,\"start\":80622},{\"end\":81090,\"start\":81039},{\"end\":82523,\"start\":82448},{\"end\":83139,\"start\":83081},{\"end\":86970,\"start\":86900},{\"end\":87919,\"start\":87844},{\"end\":88411,\"start\":88336},{\"end\":89333,\"start\":89267},{\"end\":89678,\"start\":89637},{\"end\":92229,\"start\":92157},{\"end\":92684,\"start\":92581}]", "bib_author": "[{\"end\":58638,\"start\":58610},{\"end\":58655,\"start\":58638},{\"end\":58668,\"start\":58655},{\"end\":58680,\"start\":58668},{\"end\":58691,\"start\":58680},{\"end\":58705,\"start\":58691},{\"end\":58718,\"start\":58705},{\"end\":58732,\"start\":58718},{\"end\":58750,\"start\":58732},{\"end\":58760,\"start\":58750},{\"end\":59152,\"start\":59136},{\"end\":59165,\"start\":59152},{\"end\":59180,\"start\":59165},{\"end\":59194,\"start\":59180},{\"end\":59208,\"start\":59194},{\"end\":59223,\"start\":59208},{\"end\":59234,\"start\":59223},{\"end\":59702,\"start\":59685},{\"end\":59721,\"start\":59702},{\"end\":59732,\"start\":59721},{\"end\":59751,\"start\":59732},{\"end\":59764,\"start\":59751},{\"end\":59782,\"start\":59764},{\"end\":59795,\"start\":59782},{\"end\":60194,\"start\":60184},{\"end\":60212,\"start\":60194},{\"end\":60228,\"start\":60212},{\"end\":60244,\"start\":60228},{\"end\":60258,\"start\":60244},{\"end\":60569,\"start\":60552},{\"end\":60580,\"start\":60569},{\"end\":60599,\"start\":60580},{\"end\":60866,\"start\":60855},{\"end\":60881,\"start\":60866},{\"end\":60893,\"start\":60881},{\"end\":60910,\"start\":60893},{\"end\":60926,\"start\":60910},{\"end\":60945,\"start\":60926},{\"end\":60965,\"start\":60945},{\"end\":60979,\"start\":60965},{\"end\":60994,\"start\":60979},{\"end\":61009,\"start\":60994},{\"end\":61393,\"start\":61377},{\"end\":61410,\"start\":61393},{\"end\":61428,\"start\":61410},{\"end\":61445,\"start\":61428},{\"end\":61465,\"start\":61445},{\"end\":61483,\"start\":61465},{\"end\":61810,\"start\":61799},{\"end\":61826,\"start\":61810},{\"end\":61835,\"start\":61826},{\"end\":61844,\"start\":61835},{\"end\":61860,\"start\":61844},{\"end\":61868,\"start\":61860},{\"end\":62138,\"start\":62127},{\"end\":62154,\"start\":62138},{\"end\":62163,\"start\":62154},{\"end\":62177,\"start\":62163},{\"end\":62192,\"start\":62177},{\"end\":62209,\"start\":62192},{\"end\":62495,\"start\":62482},{\"end\":62505,\"start\":62495},{\"end\":62519,\"start\":62505},{\"end\":62541,\"start\":62519},{\"end\":62556,\"start\":62541},{\"end\":62570,\"start\":62556},{\"end\":62590,\"start\":62570},{\"end\":62912,\"start\":62897},{\"end\":62923,\"start\":62912},{\"end\":62935,\"start\":62923},{\"end\":62951,\"start\":62935},{\"end\":62965,\"start\":62951},{\"end\":62974,\"start\":62965},{\"end\":62984,\"start\":62974},{\"end\":62998,\"start\":62984},{\"end\":63420,\"start\":63410},{\"end\":63433,\"start\":63420},{\"end\":63446,\"start\":63433},{\"end\":63457,\"start\":63446},{\"end\":63466,\"start\":63457},{\"end\":63478,\"start\":63466},{\"end\":63487,\"start\":63478},{\"end\":63810,\"start\":63798},{\"end\":63819,\"start\":63810},{\"end\":63828,\"start\":63819},{\"end\":63842,\"start\":63828},{\"end\":64061,\"start\":64050},{\"end\":64071,\"start\":64061},{\"end\":64083,\"start\":64071},{\"end\":64099,\"start\":64083},{\"end\":64108,\"start\":64099},{\"end\":64121,\"start\":64108},{\"end\":64133,\"start\":64121},{\"end\":64144,\"start\":64133},{\"end\":64158,\"start\":64144},{\"end\":64168,\"start\":64158},{\"end\":64511,\"start\":64505},{\"end\":64517,\"start\":64511},{\"end\":64600,\"start\":64586},{\"end\":64616,\"start\":64600},{\"end\":64628,\"start\":64616},{\"end\":64653,\"start\":64628},{\"end\":65054,\"start\":65034},{\"end\":65067,\"start\":65054},{\"end\":65089,\"start\":65067},{\"end\":65107,\"start\":65089},{\"end\":65121,\"start\":65107},{\"end\":65141,\"start\":65121},{\"end\":65159,\"start\":65141},{\"end\":65178,\"start\":65159},{\"end\":65193,\"start\":65178},{\"end\":65208,\"start\":65193},{\"end\":65669,\"start\":65657},{\"end\":65679,\"start\":65669},{\"end\":65691,\"start\":65679},{\"end\":65701,\"start\":65691},{\"end\":65712,\"start\":65701},{\"end\":65727,\"start\":65712},{\"end\":65741,\"start\":65727},{\"end\":65755,\"start\":65741},{\"end\":65764,\"start\":65755},{\"end\":65769,\"start\":65764},{\"end\":66120,\"start\":66111},{\"end\":66135,\"start\":66120},{\"end\":66146,\"start\":66135},{\"end\":66156,\"start\":66146},{\"end\":66166,\"start\":66156},{\"end\":66180,\"start\":66166},{\"end\":66519,\"start\":66506},{\"end\":66533,\"start\":66519},{\"end\":66548,\"start\":66533},{\"end\":66972,\"start\":66958},{\"end\":66986,\"start\":66972},{\"end\":67006,\"start\":66986},{\"end\":67019,\"start\":67006},{\"end\":67220,\"start\":67208},{\"end\":67238,\"start\":67220},{\"end\":67252,\"start\":67238},{\"end\":67267,\"start\":67252},{\"end\":67622,\"start\":67610},{\"end\":67637,\"start\":67622},{\"end\":67651,\"start\":67637},{\"end\":67661,\"start\":67651},{\"end\":68025,\"start\":68015},{\"end\":68036,\"start\":68025},{\"end\":68050,\"start\":68036},{\"end\":68065,\"start\":68050},{\"end\":68084,\"start\":68065},{\"end\":68094,\"start\":68084},{\"end\":68103,\"start\":68094},{\"end\":68116,\"start\":68103},{\"end\":68122,\"start\":68116},{\"end\":68128,\"start\":68122},{\"end\":68531,\"start\":68514},{\"end\":68548,\"start\":68531},{\"end\":68568,\"start\":68548},{\"end\":68584,\"start\":68568},{\"end\":68598,\"start\":68584},{\"end\":68607,\"start\":68598},{\"end\":68621,\"start\":68607},{\"end\":68634,\"start\":68621},{\"end\":68644,\"start\":68634},{\"end\":68663,\"start\":68644},{\"end\":69020,\"start\":69007},{\"end\":69033,\"start\":69020},{\"end\":69055,\"start\":69033},{\"end\":69069,\"start\":69055},{\"end\":69087,\"start\":69069},{\"end\":69106,\"start\":69087},{\"end\":69111,\"start\":69106},{\"end\":69456,\"start\":69438},{\"end\":69470,\"start\":69456},{\"end\":69482,\"start\":69470},{\"end\":69500,\"start\":69482},{\"end\":69972,\"start\":69964},{\"end\":69984,\"start\":69972},{\"end\":69998,\"start\":69984},{\"end\":70010,\"start\":69998},{\"end\":70024,\"start\":70010},{\"end\":70035,\"start\":70024},{\"end\":70049,\"start\":70035},{\"end\":70058,\"start\":70049},{\"end\":70073,\"start\":70058},{\"end\":70086,\"start\":70073},{\"end\":70496,\"start\":70485},{\"end\":70507,\"start\":70496},{\"end\":70524,\"start\":70507},{\"end\":70536,\"start\":70524},{\"end\":70835,\"start\":70822},{\"end\":70845,\"start\":70835},{\"end\":70854,\"start\":70845},{\"end\":70865,\"start\":70854},{\"end\":70878,\"start\":70865},{\"end\":70888,\"start\":70878},{\"end\":70899,\"start\":70888},{\"end\":70911,\"start\":70899},{\"end\":70920,\"start\":70911},{\"end\":71229,\"start\":71215},{\"end\":71242,\"start\":71229},{\"end\":71257,\"start\":71242},{\"end\":71642,\"start\":71628},{\"end\":71657,\"start\":71642},{\"end\":71673,\"start\":71657},{\"end\":71685,\"start\":71673},{\"end\":71700,\"start\":71685},{\"end\":71714,\"start\":71700},{\"end\":71728,\"start\":71714},{\"end\":71748,\"start\":71728},{\"end\":72048,\"start\":72035},{\"end\":72061,\"start\":72048},{\"end\":72074,\"start\":72061},{\"end\":72088,\"start\":72074},{\"end\":72360,\"start\":72352},{\"end\":72372,\"start\":72360},{\"end\":72381,\"start\":72372},{\"end\":72389,\"start\":72381},{\"end\":72401,\"start\":72389},{\"end\":72414,\"start\":72401},{\"end\":72427,\"start\":72414},{\"end\":72440,\"start\":72427},{\"end\":72833,\"start\":72819},{\"end\":72843,\"start\":72833},{\"end\":72856,\"start\":72843},{\"end\":72869,\"start\":72856},{\"end\":72878,\"start\":72869},{\"end\":72891,\"start\":72878},{\"end\":72907,\"start\":72891},{\"end\":72918,\"start\":72907},{\"end\":72930,\"start\":72918},{\"end\":72942,\"start\":72930},{\"end\":73368,\"start\":73351},{\"end\":73382,\"start\":73368},{\"end\":73640,\"start\":73623},{\"end\":73654,\"start\":73640},{\"end\":73898,\"start\":73887},{\"end\":73917,\"start\":73898},{\"end\":73932,\"start\":73917},{\"end\":73950,\"start\":73932},{\"end\":73970,\"start\":73950},{\"end\":74286,\"start\":74274},{\"end\":74302,\"start\":74286},{\"end\":74320,\"start\":74302},{\"end\":74334,\"start\":74320},{\"end\":74349,\"start\":74334},{\"end\":74363,\"start\":74349},{\"end\":74763,\"start\":74755},{\"end\":74885,\"start\":74874},{\"end\":75050,\"start\":75037},{\"end\":75062,\"start\":75050},{\"end\":75072,\"start\":75062},{\"end\":75087,\"start\":75072},{\"end\":75107,\"start\":75087},{\"end\":75123,\"start\":75107},{\"end\":75136,\"start\":75123},{\"end\":75154,\"start\":75136},{\"end\":75170,\"start\":75154},{\"end\":75180,\"start\":75170},{\"end\":75597,\"start\":75579},{\"end\":75611,\"start\":75597},{\"end\":75622,\"start\":75611},{\"end\":75636,\"start\":75622},{\"end\":76098,\"start\":76084},{\"end\":76118,\"start\":76098},{\"end\":76132,\"start\":76118},{\"end\":76148,\"start\":76132},{\"end\":76377,\"start\":76363},{\"end\":76397,\"start\":76377},{\"end\":76411,\"start\":76397},{\"end\":76427,\"start\":76411},{\"end\":76646,\"start\":76632},{\"end\":76658,\"start\":76646},{\"end\":76671,\"start\":76658},{\"end\":76683,\"start\":76671},{\"end\":76697,\"start\":76683},{\"end\":76713,\"start\":76697},{\"end\":76916,\"start\":76904},{\"end\":76930,\"start\":76916},{\"end\":76948,\"start\":76930},{\"end\":76974,\"start\":76948},{\"end\":76993,\"start\":76974},{\"end\":77014,\"start\":76993},{\"end\":77027,\"start\":77014},{\"end\":77040,\"start\":77027},{\"end\":77052,\"start\":77040},{\"end\":77078,\"start\":77052},{\"end\":77473,\"start\":77451},{\"end\":77482,\"start\":77473},{\"end\":77497,\"start\":77482},{\"end\":77502,\"start\":77497},{\"end\":77815,\"start\":77805},{\"end\":77831,\"start\":77815},{\"end\":77850,\"start\":77831},{\"end\":77865,\"start\":77850},{\"end\":77880,\"start\":77865},{\"end\":77886,\"start\":77880},{\"end\":78346,\"start\":78330},{\"end\":78359,\"start\":78346},{\"end\":78367,\"start\":78359},{\"end\":78381,\"start\":78367},{\"end\":78393,\"start\":78381},{\"end\":78409,\"start\":78393},{\"end\":78735,\"start\":78724},{\"end\":78747,\"start\":78735},{\"end\":78760,\"start\":78747},{\"end\":78770,\"start\":78760},{\"end\":78778,\"start\":78770},{\"end\":78789,\"start\":78778},{\"end\":78798,\"start\":78789},{\"end\":78813,\"start\":78798},{\"end\":78823,\"start\":78813},{\"end\":78835,\"start\":78823},{\"end\":79331,\"start\":79318},{\"end\":79343,\"start\":79331},{\"end\":79354,\"start\":79343},{\"end\":79363,\"start\":79354},{\"end\":79378,\"start\":79363},{\"end\":79390,\"start\":79378},{\"end\":79688,\"start\":79675},{\"end\":79706,\"start\":79688},{\"end\":79720,\"start\":79706},{\"end\":79733,\"start\":79720},{\"end\":79745,\"start\":79733},{\"end\":79762,\"start\":79745},{\"end\":79775,\"start\":79762},{\"end\":79798,\"start\":79775},{\"end\":80123,\"start\":80109},{\"end\":80139,\"start\":80123},{\"end\":80156,\"start\":80139},{\"end\":80173,\"start\":80156},{\"end\":80193,\"start\":80173},{\"end\":80211,\"start\":80193},{\"end\":80235,\"start\":80211},{\"end\":80247,\"start\":80235},{\"end\":80262,\"start\":80247},{\"end\":80269,\"start\":80262},{\"end\":80665,\"start\":80649},{\"end\":80679,\"start\":80665},{\"end\":80692,\"start\":80679},{\"end\":80709,\"start\":80692},{\"end\":80722,\"start\":80709},{\"end\":80737,\"start\":80722},{\"end\":80752,\"start\":80737},{\"end\":80770,\"start\":80752},{\"end\":81114,\"start\":81092},{\"end\":81132,\"start\":81114},{\"end\":81145,\"start\":81132},{\"end\":81573,\"start\":81558},{\"end\":81589,\"start\":81573},{\"end\":81601,\"start\":81589},{\"end\":81612,\"start\":81601},{\"end\":81623,\"start\":81612},{\"end\":81632,\"start\":81623},{\"end\":81645,\"start\":81632},{\"end\":81653,\"start\":81645},{\"end\":81666,\"start\":81653},{\"end\":82038,\"start\":82027},{\"end\":82047,\"start\":82038},{\"end\":82056,\"start\":82047},{\"end\":82069,\"start\":82056},{\"end\":82080,\"start\":82069},{\"end\":82092,\"start\":82080},{\"end\":82104,\"start\":82092},{\"end\":82116,\"start\":82104},{\"end\":82130,\"start\":82116},{\"end\":82144,\"start\":82130},{\"end\":82538,\"start\":82525},{\"end\":82550,\"start\":82538},{\"end\":82560,\"start\":82550},{\"end\":82576,\"start\":82560},{\"end\":82586,\"start\":82576},{\"end\":82598,\"start\":82586},{\"end\":82610,\"start\":82598},{\"end\":82619,\"start\":82610},{\"end\":82629,\"start\":82619},{\"end\":82643,\"start\":82629},{\"end\":83154,\"start\":83141},{\"end\":83164,\"start\":83154},{\"end\":83174,\"start\":83164},{\"end\":83189,\"start\":83174},{\"end\":83202,\"start\":83189},{\"end\":83214,\"start\":83202},{\"end\":83223,\"start\":83214},{\"end\":83233,\"start\":83223},{\"end\":83244,\"start\":83233},{\"end\":83593,\"start\":83580},{\"end\":83605,\"start\":83593},{\"end\":83614,\"start\":83605},{\"end\":83628,\"start\":83614},{\"end\":83643,\"start\":83628},{\"end\":83654,\"start\":83643},{\"end\":83670,\"start\":83654},{\"end\":83699,\"start\":83670},{\"end\":83717,\"start\":83699},{\"end\":83722,\"start\":83717},{\"end\":84126,\"start\":84112},{\"end\":84136,\"start\":84126},{\"end\":84145,\"start\":84136},{\"end\":84159,\"start\":84145},{\"end\":84173,\"start\":84159},{\"end\":84482,\"start\":84468},{\"end\":84492,\"start\":84482},{\"end\":84501,\"start\":84492},{\"end\":84515,\"start\":84501},{\"end\":84529,\"start\":84515},{\"end\":84764,\"start\":84750},{\"end\":84780,\"start\":84764},{\"end\":84789,\"start\":84780},{\"end\":84799,\"start\":84789},{\"end\":84813,\"start\":84799},{\"end\":84827,\"start\":84813},{\"end\":85156,\"start\":85142},{\"end\":85171,\"start\":85156},{\"end\":85187,\"start\":85171},{\"end\":85198,\"start\":85187},{\"end\":85206,\"start\":85198},{\"end\":85220,\"start\":85206},{\"end\":85239,\"start\":85220},{\"end\":85251,\"start\":85239},{\"end\":85614,\"start\":85600},{\"end\":85630,\"start\":85614},{\"end\":85654,\"start\":85630},{\"end\":85669,\"start\":85654},{\"end\":85687,\"start\":85669},{\"end\":85705,\"start\":85687},{\"end\":85718,\"start\":85705},{\"end\":85744,\"start\":85718},{\"end\":85758,\"start\":85744},{\"end\":85770,\"start\":85758},{\"end\":86162,\"start\":86151},{\"end\":86177,\"start\":86162},{\"end\":86188,\"start\":86177},{\"end\":86201,\"start\":86188},{\"end\":86216,\"start\":86201},{\"end\":86226,\"start\":86216},{\"end\":86238,\"start\":86226},{\"end\":86242,\"start\":86238},{\"end\":86252,\"start\":86242},{\"end\":86264,\"start\":86252},{\"end\":86268,\"start\":86264},{\"end\":86597,\"start\":86585},{\"end\":86612,\"start\":86597},{\"end\":86624,\"start\":86612},{\"end\":86639,\"start\":86624},{\"end\":86653,\"start\":86639},{\"end\":86663,\"start\":86653},{\"end\":86982,\"start\":86972},{\"end\":86993,\"start\":86982},{\"end\":87006,\"start\":86993},{\"end\":87019,\"start\":87006},{\"end\":87030,\"start\":87019},{\"end\":87042,\"start\":87030},{\"end\":87056,\"start\":87042},{\"end\":87066,\"start\":87056},{\"end\":87558,\"start\":87549},{\"end\":87568,\"start\":87558},{\"end\":87580,\"start\":87568},{\"end\":87591,\"start\":87580},{\"end\":87601,\"start\":87591},{\"end\":87614,\"start\":87601},{\"end\":87626,\"start\":87614},{\"end\":87937,\"start\":87921},{\"end\":87946,\"start\":87937},{\"end\":87961,\"start\":87946},{\"end\":87973,\"start\":87961},{\"end\":87987,\"start\":87973},{\"end\":88000,\"start\":87987},{\"end\":88010,\"start\":88000},{\"end\":88023,\"start\":88010},{\"end\":88429,\"start\":88413},{\"end\":88438,\"start\":88429},{\"end\":88453,\"start\":88438},{\"end\":88465,\"start\":88453},{\"end\":88479,\"start\":88465},{\"end\":88492,\"start\":88479},{\"end\":88502,\"start\":88492},{\"end\":88515,\"start\":88502},{\"end\":88871,\"start\":88855},{\"end\":88882,\"start\":88871},{\"end\":88897,\"start\":88882},{\"end\":88908,\"start\":88897},{\"end\":88925,\"start\":88908},{\"end\":88939,\"start\":88925},{\"end\":88952,\"start\":88939},{\"end\":88960,\"start\":88952},{\"end\":88974,\"start\":88960},{\"end\":88987,\"start\":88974},{\"end\":89345,\"start\":89335},{\"end\":89355,\"start\":89345},{\"end\":89371,\"start\":89355},{\"end\":89384,\"start\":89371},{\"end\":89692,\"start\":89680},{\"end\":89709,\"start\":89692},{\"end\":89720,\"start\":89709},{\"end\":89738,\"start\":89720},{\"end\":89753,\"start\":89738},{\"end\":90041,\"start\":90028},{\"end\":90049,\"start\":90041},{\"end\":90063,\"start\":90049},{\"end\":90075,\"start\":90063},{\"end\":90092,\"start\":90075},{\"end\":90293,\"start\":90281},{\"end\":90303,\"start\":90293},{\"end\":90317,\"start\":90303},{\"end\":90329,\"start\":90317},{\"end\":90340,\"start\":90329},{\"end\":90351,\"start\":90340},{\"end\":90364,\"start\":90351},{\"end\":90374,\"start\":90364},{\"end\":90387,\"start\":90374},{\"end\":90397,\"start\":90387},{\"end\":90703,\"start\":90690},{\"end\":90719,\"start\":90703},{\"end\":90732,\"start\":90719},{\"end\":90747,\"start\":90732},{\"end\":90758,\"start\":90747},{\"end\":90772,\"start\":90758},{\"end\":90791,\"start\":90772},{\"end\":90802,\"start\":90791},{\"end\":90811,\"start\":90802},{\"end\":90828,\"start\":90811},{\"end\":91163,\"start\":91148},{\"end\":91177,\"start\":91163},{\"end\":91188,\"start\":91177},{\"end\":91450,\"start\":91439},{\"end\":91460,\"start\":91450},{\"end\":91475,\"start\":91460},{\"end\":91485,\"start\":91475},{\"end\":91504,\"start\":91485},{\"end\":91822,\"start\":91810},{\"end\":91834,\"start\":91822},{\"end\":91847,\"start\":91834},{\"end\":91861,\"start\":91847},{\"end\":91875,\"start\":91861},{\"end\":91890,\"start\":91875},{\"end\":91902,\"start\":91890},{\"end\":92243,\"start\":92231},{\"end\":92254,\"start\":92243},{\"end\":92264,\"start\":92254},{\"end\":92272,\"start\":92264},{\"end\":92287,\"start\":92272},{\"end\":92299,\"start\":92287},{\"end\":92698,\"start\":92686},{\"end\":92710,\"start\":92698},{\"end\":92718,\"start\":92710},{\"end\":92730,\"start\":92718},{\"end\":92744,\"start\":92730},{\"end\":92758,\"start\":92744},{\"end\":92770,\"start\":92758}]", "bib_venue": "[{\"end\":59375,\"start\":59313},{\"end\":59916,\"start\":59864},{\"end\":63062,\"start\":63051},{\"end\":66697,\"start\":66631},{\"end\":67396,\"start\":67340},{\"end\":67802,\"start\":67740},{\"end\":69629,\"start\":69573},{\"end\":71386,\"start\":71330},{\"end\":72569,\"start\":72513},{\"end\":74504,\"start\":74442},{\"end\":75797,\"start\":75725},{\"end\":78027,\"start\":77965},{\"end\":78964,\"start\":78908},{\"end\":81286,\"start\":81224},{\"end\":82772,\"start\":82716},{\"end\":87215,\"start\":87149},{\"end\":92919,\"start\":92853},{\"end\":58608,\"start\":58553},{\"end\":59311,\"start\":59234},{\"end\":59862,\"start\":59795},{\"end\":60307,\"start\":60258},{\"end\":60643,\"start\":60599},{\"end\":61058,\"start\":61009},{\"end\":61521,\"start\":61483},{\"end\":61797,\"start\":61738},{\"end\":62125,\"start\":62080},{\"end\":62480,\"start\":62418},{\"end\":63049,\"start\":62998},{\"end\":63539,\"start\":63487},{\"end\":63886,\"start\":63842},{\"end\":64255,\"start\":64184},{\"end\":64743,\"start\":64669},{\"end\":65260,\"start\":65208},{\"end\":65655,\"start\":65585},{\"end\":66229,\"start\":66180},{\"end\":66629,\"start\":66548},{\"end\":66956,\"start\":66918},{\"end\":67338,\"start\":67267},{\"end\":67738,\"start\":67661},{\"end\":68188,\"start\":68144},{\"end\":68512,\"start\":68420},{\"end\":69149,\"start\":69111},{\"end\":69571,\"start\":69500},{\"end\":69962,\"start\":69877},{\"end\":70483,\"start\":70381},{\"end\":70820,\"start\":70777},{\"end\":71328,\"start\":71257},{\"end\":71786,\"start\":71748},{\"end\":72129,\"start\":72104},{\"end\":72511,\"start\":72440},{\"end\":73031,\"start\":72958},{\"end\":73434,\"start\":73382},{\"end\":73621,\"start\":73569},{\"end\":73885,\"start\":73814},{\"end\":74440,\"start\":74363},{\"end\":74872,\"start\":74816},{\"end\":75229,\"start\":75180},{\"end\":75723,\"start\":75636},{\"end\":76082,\"start\":76023},{\"end\":76361,\"start\":76294},{\"end\":76724,\"start\":76713},{\"end\":77551,\"start\":77502},{\"end\":77963,\"start\":77886},{\"end\":78328,\"start\":78256},{\"end\":78906,\"start\":78835},{\"end\":79316,\"start\":79243},{\"end\":79673,\"start\":79617},{\"end\":80330,\"start\":80285},{\"end\":80819,\"start\":80770},{\"end\":81222,\"start\":81145},{\"end\":81556,\"start\":81489},{\"end\":82025,\"start\":81924},{\"end\":82714,\"start\":82643},{\"end\":83270,\"start\":83244},{\"end\":83578,\"start\":83492},{\"end\":84110,\"start\":84035},{\"end\":84466,\"start\":84391},{\"end\":84883,\"start\":84843},{\"end\":85140,\"start\":85069},{\"end\":85598,\"start\":85518},{\"end\":86149,\"start\":86101},{\"end\":86583,\"start\":86509},{\"end\":87147,\"start\":87066},{\"end\":87547,\"start\":87484},{\"end\":88061,\"start\":88023},{\"end\":88553,\"start\":88515},{\"end\":88853,\"start\":88790},{\"end\":89430,\"start\":89400},{\"end\":89791,\"start\":89753},{\"end\":90026,\"start\":89983},{\"end\":90458,\"start\":90413},{\"end\":90888,\"start\":90844},{\"end\":91437,\"start\":91352},{\"end\":91808,\"start\":91734},{\"end\":92351,\"start\":92299},{\"end\":92851,\"start\":92770}]"}}}, "year": 2023, "month": 12, "day": 17}