{"id": 209405007, "updated": "2023-10-06 20:54:32.676", "metadata": {"title": "Distributional Reinforcement Learning for Energy-Based Sequential Models", "authors": "[{\"first\":\"Tetiana\",\"last\":\"Parshakova\",\"middle\":[]},{\"first\":\"Jean-Marc\",\"last\":\"Andreoli\",\"middle\":[]},{\"first\":\"Marc\",\"last\":\"Dymetman\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": 12, "day": 18}, "abstract": "Global Autoregressive Models (GAMs) are a recent proposal [Parshakova et al., CoNLL 2019] for exploiting global properties of sequences for data-efficient learning of seq2seq models. In the first phase of training, an Energy-Based model (EBM) over sequences is derived. This EBM has high representational power, but is unnormalized and cannot be directly exploited for sampling. To address this issue [Parshakova et al., CoNLL 2019] proposes a distillation technique, which can only be applied under limited conditions. By relating this problem to Policy Gradient techniques in RL, but in a \\emph{distributional} rather than \\emph{optimization} perspective, we propose a general approach applicable to any sequential EBM. Its effectiveness is illustrated on GAM-based experiments.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1912.08517", "mag": "2995781845", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1912-08517", "doi": null}}, "content": {"source": {"pdf_hash": "3f8af4ad61f68c0c884fac71f98369863ebf5ed4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1912.08517v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "c32682e1fb784bd3f19543eaa18bdfe551fdf6aa", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3f8af4ad61f68c0c884fac71f98369863ebf5ed4.txt", "contents": "\nDistributional Reinforcement Learning for Energy-Based Sequential Models\n\n\nTetiana Parshakova tetianap@stanford.edu \nJean-Marc Andreoli jean-marc.andreoli@naverlabs.com \nMarc Dymetman marc.dymetman@naverlabs.com \n\nStanford University\n\n\n\nNaver Labs Europe\n\n\nDistributional Reinforcement Learning for Energy-Based Sequential Models\n\nGlobal Autoregressive Models (GAMs) are a recent proposal[15]for exploiting global properties of sequences for data-efficient learning of seq2seq models. In the first phase of training, an Energy-Based model (EBM) [10] over sequences is derived. This EBM has high representational power, but is unnormalized and cannot be directly exploited for sampling. To address this issue[15]proposes a distillation technique, which can only be applied under limited conditions. By relating this problem to Policy Gradient techniques in RL, but in a distributional rather than optimization perspective, we propose a general approach applicable to any sequential EBM. Its effectiveness is illustrated on GAM-based experiments.\n\nIntroduction\n\nThe mainstream autoregressive sequence models [6,22,5,24]) form a subclass of sequential energy-based models (sequential EBMs) [10]. While the former are locally normalized and easy to train and sample from, the latter allow global constraints, greater expressivity, and potentially better sample efficiency, but lead to unnormalized distributions and are more difficult to use for inference and evaluation. We exploit a recently introducaked class of energy-based models, Global Autoregressive Models (GAMs) [15], which combine a locally normalized component (that is, a first, standard, autoregressive model, denoted r) with a global component and use these to explore some core research questions about sequential EBMs, focussing our experiments on synthetic data for which we can directly control experimental conditions. We dissociate the (relatively easy) task of learning from the available data an energy-based representation (Training-1), from the more challenging task of exploiting that representation to produce samples or evaluations (Training-2).\n\nIn this paper, we provide a short self-contained introduction to GAMs and to their two-stage training procedure. However our main focus is about Training-2.\n\nFor that task [15] proposed a Distillation technique to project the Energy-Based representation (denoted by P \u03bb ) obtained at the end of Training-1 into a final autoregressive model (denoted \u03c0 \u03b8 ), with better test perplexity than the initial r, but this technique was limited to cases where it was possible to sample from P \u03bb at training time. One key observation of the current submission is that Training-2, considered as the general problem of deriving an autoregressive model from an energy-based model (not necessarily obtained through Training-1) has strong similarities with the training of policies in Reinforcement Learning (RL), but in a distributional rather than in an optimization perspective as in standard RL. We then propose a distributional variant of the Policy Gradient technique (Distributional Policy Gradient: DPG) which has wider applicability than distillation. We conduct GAM-based experiments to compare this technique with distillation, in synthetic data conditions where distillation is feasible, and show that DPG works as well as distillation. In both cases, in small data conditions, the policies (aka autoregressive) models \u03c0 \u03b8 obtained at the end of the process are very similar and show strong perplexity reduction over the standard autoregressive models.\n\nSection 2 provides an overview of GAMs. Section 3 explains the training procedure, with focus on EBMs and relations to RL. Section 4 presents experiments and results. For space reasons we use the Supplementary Material (Sup. Mat.) to provide some details and to discuss related work.\n\n\nModel\n\n\nBackground\n\nAutoregressive models (AMs) These are currently the standard for neural seq2seq processing, with such representatives as RNN/LSTMs [6,22], ConvS2S [5], Transformer [24]). Formally, they are defined though a distribution r \u03b7 (x|C), where x is a target sequence to be generated, and C is a context, with r \u03b7 (x|C) . = i s \u03b7 (x i |x 1 , . . . , x i\u22121 , C), and where each s \u03b7 (x i |x 1 , . . . , x i\u22121 , C) is a normalized conditional probability over the next symbol of the sequence, computed by a neural network (NN) with parameters \u03b7. The local normalization of the incremental probabilities implies the overall normalization of the distribution r \u03b7 (x|C). In RL terminology, AMs can also be seen as policies where actions are symbols and states are sequence prefixes.\n\nEnergy-Based Models (EBMs) EBMs are a generic class of models, characterized by an energy function U \u03b7 (x|C) computed by a neural network parametrized by \u03b7 [10]. Equivalently, they can be seen as directly defining a potential (an unnormalized probability distribution) P \u03b7 (x|C) = e \u2212U\u03b7(x|C) , and indirectly the normalized distribution p \u03b7 (x|C) = 1/Z \u03b7 (C) P \u03b7 (x|C), with Z \u03b7 (C) = x P \u03b7 (x|C). Here we will identify an EBM with its potential (the P \u03b7 form) and be concerned exclusively with sequential EBMs, that is, the case where x is a sequence.\n\n\nGAMs\n\nWe employ a specific class of sequential EBMs, Global Autoregressive Models (GAMs), which we summarize here (for details please see [15]). GAMs exploit both local autoregressive properties as well as global properties of the sequence x. A GAM is an unnormalized potential P \u03b7 (x|C) over x, parametrized by a vector \u03b7 = \u03b7 1 \u2295 \u03b7 2 , which is the product of two factors:\nP \u03b7 (x|C) = r \u03b71 (x|C) \u00b7 e \u03bb\u03b7 2 (C), \u03c6(x;C) .(1)\nHere the factor r \u03b71 (x|C) is an autoregressive model for generating x in the context C, parametrized by \u03b7 1 . The factor e \u03bb\u03b7 2 (C), \u03c6(x;C) on the other hand, is a log-linear potential [8], where \u03c6(x; C) is a vector of predefined real features of the pair (x, C), which is combined by a scalar product with a real vector \u03bb \u03b72 (C) of the same dimension, computed by a network parametrized by \u03b7 2 . The normalized distribution associated with the GAM is p \u03b7 (x|C) = P\u03b7(x|C)\nZ\u03b7(C) , where Z \u03b7 (C) = x P \u03b7 (x|C).\nThe motivations for GAMs are as follows. The first factor guarantees that the GAM will have at least the same effectiveness as standard autoregressive models to model the local, incremental, aspects of sequential data.The second factor can be seen as providing a \"modulation\" on the first one. While we could have chosen any energy-based potential for that factor, the log-linear form has several advantages. First, the features \u03c6(x; C) provide prior knowledge to the model by drawing its attention to potentially useful global sequence properties that may be difficult for the AM component to discover on its own. Second, log-linear models enjoy the following important property: at maximum likelihood, the features expectations according to the model and to the data are equal (\"moment matching\" property).\n\nIn our experiments, we focus on a simple unconditional (language modelling) version of GAMs, of the form:\nP \u03bb (x) . = r(x) \u00b7 e \u03bb, \u03c6(x) ,(2)\nwhere the autoregressive factor r = r \u03b71 is first learnt on the training dataset of sequences D and then kept fixed, and where the parameter vector \u03bb is then trained on top of r, also on D. We denote by p \u03bb (x) the normalized distribution associated with P \u03bb (x).\n\n\nTraining\n\nWe assume that we are given a training data set D (resp. a validation set V , a test set T ) of sequences x, and a finite collection of real-valued feature functions \u03c6 1 , . . . , \u03c6 k . The GAM training procedure then is performed in two stages (see Fig. 1).\n\n3.1 Training-1: from data to energy-based representation\nr(x) \u03c0 \u03b8 (x) P \u03bb (x)\nTraining-1\n\nTraining-2 Figure 1: Two-stage training. At the end of the process, we compare the perplexities of r and \u03c0 \u03b8 on test data:\n\nCE(T, r) vs. CE(T, \u03c0 \u03b8 ). This phase consists in training P \u03bb by max-likelihood (ML) on D. We start by training an AM r = r \u03b71 (our initial policy) on D, in the standard way. We then fit the log-linear weight vector \u03bb to the data. In order to do that, we denote by log p \u03bb (D) the log-likelihood of the data, and perform SGD over \u03bb by observing that (2) implies:\n\u2207 \u03bb log p \u03bb (D) = |D|\u00b7[E x\u223cp D (x) \u03c6(x)\u2212E x\u223cp \u03bb (\u00b7) \u03c6(x)], (3) where E x\u223cp D (x) \u03c6(x) (resp. E x\u223cp \u03bb (\u00b7) \u03c6(x)\n) denotes the expectation (aka moment) of the feature vector relative to the data (resp. to the model). The first moment can be directly computed from the data, but the second moment requires more effort. The most direct way for estimating E x\u223cp \u03bb (\u00b7) \u03c6(x) would be to produce a random sample from p \u03bb (\u00b7) and to compute the mean of \u03c6(x) over this sample. In general, when starting from an unnormalized P \u03bb as here, obtaining samples from p \u03bb can be difficult. One approach consists in applying a Monte-Carlo sampling technique, such as Rejection Sampling (rs) [18], and this is one of two techniques that can be applied in the experimental conditions both of [15] and of this paper. However rejection sampling is feasible only in situations where reasonable upper-bounds of the ratio P (x)/q(x) (for q a proposal distribution) can be derived. 1 This is why [15] proposes another technique of wider applicability, Self-Normalized Importance Sampling (snis) [14, ?].This technique directly estimates the expectation E x\u223cp \u03bb (\u00b7) \u03c6(x) without requiring samples from p \u03bb .\n\n\nTraining-2: from energy-based representation to distributional policy\n\nThe output of the previous stage is an unnormalized EBM, which allows us to compute the potential P (x) = P \u03bb (x) of any given x, but not directly to compute the partition function Z = x P (x) nor the normalized distribution p(x) = 1/Z P (x) = p \u03bb (x) or to sample from it. 2 In RL terms, the score P (x) can be seen as a reward. The standard RL-as-optimization view would lead us to search for a way to maximize the expectation of this reward, in other words for a policy \u03c0 \u03b8 * with \u03b8 * = argmax \u03b8 E x\u223c\u03c0 \u03b8 (\u00b7) P (x), which would tend to concentrate all its mass on a few sequences. By contrast, our RL-as-sampling (distributional) view consists in trying to find a policy \u03c0 \u03b8 * that approximates the distribution p as closely as possible, in terms of cross-entropy CE. We are thus trying to solve \u03b8 * = argmin \u03b8 CE(p, \u03c0 \u03b8 ), with CE(p, \u03c0 \u03b8 ) = \u2212 x p(x) log \u03c0 \u03b8 (x). We have:\n\u2207 \u03b8 CE(p, \u03c0 \u03b8 ) = \u2212 x p(x) \u2207 \u03b8 log \u03c0 \u03b8 (x) = \u2212 E x\u223cp(\u00b7) \u2207 \u03b8 log \u03c0 \u03b8 (x).(4)\nWe can apply (4) for SGD optimization, using different approaches. The simplest approach, Distillation, can be employed in situations where we are able to draw, in reasonable training time, a large number of samples x 1 , . . . , x K from p. We can then exploit (4) directly to update \u03b8, which is in fact equivalent to performing a standard supervised log-likelihood SGD training on the set {x 1 , . . . , x K }. This is the approach to Training-2 taken in [15], using rejection sampling at training time for obtaining the samples, and then training \u03b8 on these samples to obtain a final AM \u03c0 \u03b8 which can be used for efficient sampling at test time and for evaluation. The advantage of this approach is that supervised training of this sort is very succesful for standard autoregressive models, with good stability and convergence properties, and an efficient use of the training data through epoch iteration. 3 However, the big disadvantage is its limited applicability, due to restrictive conditions for rejection sampling, as explained earlier.\n\nA central contribution of the present paper is to propose another class of approaches, which does not involve sampling from p, and which relates to standard techniques in RL. We can rewrite the last formula of (4) as:\nx p(x) \u2207 \u03b8 log \u03c0 \u03b8 (x) = 1 Z E x\u223c\u03c0 \u03b8 (\u00b7) P (x) \u03c0 \u03b8 (x) \u2207 \u03b8 log \u03c0 \u03b8 (x).(5)\nThis formula is very close to the vanilla formulation (aka REINFORCE [25]), we have a reward R(x) and we try to maximize the expectation E x\u223c\u03c0 \u03b8 (\u00b7) R(x). It can be shown [23] \nthat \u2207 \u03b8 E x\u223c\u03c0 \u03b8 (\u00b7) R(x) = E x\u223c\u03c0 \u03b8 (\u00b7) R(x) \u2207 \u03b8 log \u03c0 \u03b8 (x).\nThus, in the RL case, an SGD step consists in sampling x from \u03c0 \u03b8 and computing R(x)\u2207 \u03b8 log \u03c0 \u03b8 (x), while the SGD step in (5) only differs by replacing R(x) by\nP (x)\n\u03c0 \u03b8 (x) . 4 We will refer to the approach (5) through the name Distributional Policy Gradient (on-policy version) or DPG on (\"on-policy\" because the sampling is done according to the same policy \u03c0 \u03b8 that is being learnt). An off-policy variant DPG off of (5) is also possible. Here we assume that we are given some fixed proposal distribution q and we write:\nx p(x) \u2207 \u03b8 log \u03c0 \u03b8 (x) = 1 Z E x\u223cq(\u00b7) P (x) q(x) \u2207 \u03b8 log \u03c0 \u03b8 (x).(6)\nHere the sampling policy q is different from the policy being learnt, and the formula (6) represents a form of Importance Sampling, with q the proposal, typically chosen to be an approximation to p. We did some initial experiments with DPG on , but found that the method had difficulty converging, probably due in part to the instability induced by the constant change of sampling distribution (namely \u03c0 \u03b8 ). A similar phenomenon is well documented in the case of the vanilla Policy Gradient in standard RL, and techniques such as TRPO [20] or PPO [21] have been developed to control the rate of change of the sampling distribution. In order to avoid such instability, we decided to focus on DPG off , based on Algorithm 1 below.\n\n\nAlgorithm 1 DPG off\n\nInput: P , initial policy q 1: \u03c0 \u03b8 \u2190 q 2: for each iteration do 3: for each episode do 4:\nsample x from q(\u00b7) 5: \u03b8 \u2190 \u03b8 + \u03b1 (\u03b8) P (x) q(x) \u2207 \u03b8 log \u03c0 \u03b8 (x) 6:\nif \u03c0 \u03b8 is superior to q then 7:\n\nq \u2190 \u03c0 \u03b8 Output: \u03c0 \u03b8\n\nIn this algorithm, we suppose that we have as input a potential function P , and an initial proposal distribution q; in the case of GAMs, we take P = P \u03bb and a good \u03c0 \u03b80 is provided by r. We then iterate the collection of episodes x sampled with the same q (line 4), and perform SGD updates (line 5) according to (6) (\u03b1 (\u03b8) is the learning rate). We do update the proposal q at certain times (line 7), but only based on the condition that the current \u03c0 \u03b8 is superior to q in terms of perplexity measured on the validation set V , thus ensuring a certain stability of the proposal.\n\nThis algorithm worked much better than the DPG on version, and we retained it as our implementation of DPG in all our experiments.\n\n\nExperiments\n\nIn order to assess the validity of our approach, we perform experiments under controllable conditions based on synthetic binary sequences. Our setup is similar to that of [15]. We generate datasets D, V, T of binary sequences according to a underlying process p true . This process produces random \"white noise\" binary strings with fixed length n = 30 that are filtered according to whether they contain a specific, fixed, substring (\"motif\") anywhere inside the sequence. The interest of such a process is that one can efficiently generate datasets (by implementing the filtering process through a probabilistic finite-state automaton) and also directly compute the theoretical entropy (perplexity) of the process (see [15]). Also, [15] observed that p true (x) could be well approximated by a standard autoregressive model r(x) when the training dataset was large.\n\nIn these experiments, we employed a GAM architecture according to (2), using a fixed set of five binary features 5 : one feature corresponding to the presence/absence of the motif in the candidate sequence, and four \"distractor\" features with no (or little) predictive value for the validity of the candidate sequence (this feature set, using [15] notation, is denoted in the figures by the mask f t = 1001111). We vary the motifs m used, the size of the training set D, and the seeds employed.\n\nOur implementation is based on PyTorch [16], with policies (i.e. autoregressive models r and \u03c0 \u03b8 ) implemented as LSTM models over the vocabulary {0, 1, EOS }, with each token represented as a one-hot vector.\n\nThe specific experimental setup that we use, due to the nature of the features (binary features or length features M, v), permits to perform Training-2 through distillation (the method used in [15]). In these experiments, we want to confirm that the more generally applicable DPG method works equally well. We do so by varying the training dataset size D and by computing the test perplexity (cross-entropy) of the \u03c0 \u03b8 obtained at the end of Training-1 + Training-2, and then checking that both distillation and DPG lower this perplexity relative to that of the initial r, under small data conditions (data efficiency). But we also confirm that in Training-2, both distillation and DPG are able to almost perfectly approximate the EBM P \u03bb obtained at the end of Training-1 (that is, to approximate the associated normalized p \u03bb ); in other words, when P \u03bb is able to model the p true accurately (which depends on both the quality of the initial r and on the ability of the features to fit the underlying process), then DPG is able to produce a \u03c0 \u03b8 that accurately represents p true .\n\n\nFigure 2: Distillation vs. DPG\n\nOverall Training: Distillation vs. DPG We consider a situation where Training-1 is done through snis, but Training-2 is done either through Distillation or through DPG (i.e. DPG off ). Figure 2 illustrates this case. Here the motif, feature vector, and seed are fixed, but the training size |D| varies from 500 to 2 \u00b7 10 4 ) (the size of the test set T is fixed at 5 \u00b7 10 3 ).\n\nThe solid lines represent the cross entropies of the final \u03c0 \u03b8 relative to the test set, with the scale located on the left side of the figure, while the dashed lines are the frequencies of the motif m (computed on 2000 strings sampled from \u03c0 \u03b8 ) with the corresponding scale on the right. We distinguish two versions of Training-2, one based on distillation (distill), the other on DPG (dpg).\n\nFirst consider the points above |D| = 5000, and the solid lines: for both distill and dpg, we have CE(T, r) CE(T, \u03c0 \u03b8 ) \u2248 H(p true ): \u03c0 \u03b8 is more data efficient than the initial AM r. For smaller data conditions, the tendency is even stronger, while larger D lead to an initial r which is already very good, and on which the two-stage training cannot improve.\n\nSimilar conclusions hold for the motif frequencies of \u03c0 \u03b8 compared to r: in small data conditions, the motif is much more frequently present when using \u03c0 \u03b8 .\n\nFinally, comparing distill and dpg, we see that the performances are very comparable, in this case with a slight advantage of distill over dpg in perplexities but the reverse in motif frequencies. Effectiveness of DPG in approximating p To emphasize the performance of DPG in Training-2 (that is, its effectiveness at finding a distributional policy \u03c0 \u03b8 for an EBM representation P (x)), independently of the quality of Training-1), we considered two alternatives for P . The first one took P = P \u03bb , the energy-based model obtained from Training-1. In our specific experimental conditions, we were able to accurately estimate (via importance sampling) the partition function Z and therefore to compute the cross entropy CE(T, p \u03bb ), and to compare it with CE(T, \u03c0 \u03b8 ): they were extremely close. We confirmed that finding by considering an alternative where P was defined a priori in such a way that we could compute p and CE(T, p) exactly, observing the same behavior. Details are provided in Sup. Mat. A.3.\n\n\nResults\n\nIn Table 1 we compute the means of ratios of different quantities across experiments with different motifs, features and seeds: motif \u2208 {1000101000101, 1011100111001, 10001011111000}, f t \u2208 {1001111, M v1001111}, seed \u2208 {1234, 4444}. In all cases Training-1 is performed using snis. These statistics confirm the tendencies illustrated in the previous plots. Namely, when |D| increases the test cross entropy CE(T, \u03c0 \u03b8 ) gets closer to the theoretical one H(p true ). Also \u03c0 \u03b8 outperforms r in small conditions of |D| for the two modes of Training-2: the columns CE(T,\u03c0 dpg \u03b8 ) CE(T,r) and CE(T,\u03c0 dis \u03b8 ) CE(T,r) show that the models approximate the true process more closely than the initial r in settings with |D| < 10 4 . Similar conclusions can be drawn when comparing the motif frequencies of \u03c0 \u03b8 and r. Further, according to data in columns CE(T,\u03c0 dpg \u03b8 ) CE(T,\u03c0 dis \u03b8 ) and mtf_frq(\u03c0 dpg \u03b8 ) mtf_frq(\u03c0 dis \u03b8 ) , we see that DPG and distillation have comparable efficiency for obtaining the final policy. DPG gives rise to a policy that has better motif frequency but slightly worse cross-entropy than the one from distillation.\n\n\nConclusion\n\nMotivated by the GAM formalism for learning sequential models, 6 we proposed some RL-inspired techniques for obtaining distributional policies approximating the normalized distribution associated with an energy-based model over sequences.\n\nWe took some first experimental steps, in controlled synthetic conditions, for confirming that these techniques were working.\n\nWhile the main algorithm (DPG off ) proposed here for computing distributional policies is generic in the sense that it only requires a potential P (x) and a proposal q, the fact that GAMs intrinsically enclose an autoregressive policy r that can be used to initialize such a proposal is an important advantage. It should also be observed that the division of work in GAMs between Training-1 and Training-2 helps clarifying a distinction that should be made about training sequential EBMs from data. [15] already observed that training the representation P \u03bb could be much easier than extracting an autoregressive model from it. 7 If we think in the terms of the current paper, we can further observe that while Training-2 has direct connections to RL (exploiting a given reward to obtain a policy), Training-1 has some similarities to Inverse RL [19,12]: deriving a reward from the training data, here purely inside a max-likelihood approach. Trying to combine the two aspects in one direct algorithm would only blur the true nature of the problem.\n\nThe move from the standard optimization view of RL and the sampling (aka distributional) view advocated here is a natural one. Optimization can be seen as an extreme case of sampling with a low temperature, and the approach to distributional policies developped in our Algorithm 1 might be a way for developing stable algorithms for standard RL purposes (a related approach is proposed in [13]).\n\nOur importation of policy gradient from standard RL to the distributional view only scratches the surface, and another promising line of research would be to adapt methods for local credit assignment, such as actor-critic techniques, to the problem of sampling from an energy-based model.\n\n\nA Supplementary Material\n\n\nA.1 Related Work\n\nGAMs have been introduced in [15]. While that paper already proposes the division of training in two stages, it only considers a distillation method, of limited application, for Training-2. It mentions a possible relation with RL as future work, but does not elaborate, while this is a central focus of the present submission.\n\nGlobal and Energy-Based approaches to neural seq2seq models have been considered in several works. Among those, [1] consider transition-based neural networks, and contrast local to global normalization of decision sequences, showing how the global approach avoids the label bias problem for tasks such as tagging or parsing. Contrarily to us, they focus on inference as maximization, for instance finding the best sequence of tags for a sequence of words. [3] address a similar class of problems (multi-labelling problems such as sequence tagging), employing an energy-based generalization of CRFs, also focussing on inference as optimization. [9], similar to us, consider probabilistic generative processes defined through an energy-based model. Their focus is on the generation of non-sequential objects, using GAN-type binary discriminators to train the energy representation on the available data. They do not exploit connections to RL.\n\nReinforcement Learning approaches for seq2seq problems have also been studied in many works. Among those, [17] use an hybrid loss function to interpolate between perplexity (aka cross-entropy) training and reward optimization, with the reward being defined by evaluation measures (such as BLEU in machine translation) differing from perplexity. [2], still in a RL-as-optimization framework, and with similar objectives, exploit an actor-critic method, where the critic (value function) helps the actor (policy) by reducing variance. [7] and [13] attempt to combine log-likelihood (aka perplexity) and reward-based training in a more integrated way. In the first paper the rewards are directly defined by a priori scores on the quality of the output, which can be computed not only at training time but also at test time. In the second paper, the way in which the rewards are integrated is done by exploiting a probabilistic formulation of rewards close to ours, but used in a different way, in particular without our notion of proposal distribution and with no explicit connection to energy-based modelling. In all these cases, the focus is on inference as optimization, not inference as sampling as in the present submission.\n\nFinally, [4] use a different notion of \"distributional RL\" from ours. During policy evaluation, they replace evaluation of the mean return from a state by the evaluation of the full distribution over returns from that state, and define a Bellman operator for such distribution. Their goal is still to find a policy in the standard (optimization) sense, but with better robustness and stability properties.\n\n\nA.2 Rejection Sampling vs. SNIS in Training-1\n\nTraining-1 consists in training the model P \u03bb on D. This is done by first training r on D in the standard way (by cross-entropy) and then by training \u03bb by SGD with the formula:\n\u2207 \u03bb log p \u03bb (x) = \u03c6(x) \u2212 E x\u223cp \u03bb (\u00b7) \u03c6(x).(7)\nThe main difficulty then consists in computing an estimate of the model moments E x\u223cp \u03bb (\u00b7) \u03c6(x). In experiments, [15] compares two Monte-Carlo approaches [18] for addressing this problem: (i) Rejection Sampling (rs), using r as the proposal distribution and (ii) Self-Normalized Importance Sampling (snis) [26], also using r as the proposal. Rejection sampling is performed as follows. We use r(x) as the proposal, and P \u03bb (x) = r(x) e \u03bb\u00b7\u03c6(x) as the unnormalized target distribution; for any specific \u03bb, because our features are binary, we can easily upper-bound the ratio\nP \u03bb (x)\nr(x) = e \u03bb\u00b7\u03c6(x) by a number \u03b2; we then sample x from r, compute the ratio \u03c1(x) = P \u03bb (x) \u03b2 r(x) \u2264 1, and accept x with probability \u03c1(x). The accepted samples are unbiased samples from p \u03bb (x) and can be used to estimate model moments.\n\nSnis also uses the proposal distribution r, but does not require an upperbound, and is directly oriented towards the computation of expectations. In this case, we sample a number of points x 1 , . . . , x N from r, compute \"importance ratios\" w(\nx i ) = P \u03bb (xi) r(xi) , and estimate E x\u223cp \u03bb (\u00b7) \u03c6(x) through\u00ca = i w(xi)\u03c6(xi) i w(xi)\n. The estimate is biased for a given N , but consistent (that is, it converges to the true E for N \u2192 \u221e).\n\nIn the experiments with DPG in the main text, we only considered cases where Training-1 is done through snis. This made sense, as both snis and DPG are motivated by situations in which sampling techniques such as rejection sampling do not work. 8 Fig. 3 compares snis with rs (using only distillation as the Training-2 technique). It can be seen that both techniques produce very similar results. \n\n\nA.3 Effectiveness of DPG in approximating p : details\n\nTo emphasize the performance of DPG in Training-2 (that is, its effectiveness at finding a distributional policy for an EBM representation P (x)), independently of the quality of Training-1), we considered two alternatives for P (see Figure 4). The first one took P = P \u03bb , the energy-based model obtained from Training-1 (the conditions were the same as in Figure 3, but we only considered snis for Training-1). For these specific experimental conditions, we were able to accurately estimate (via importance sampling) the partition function Z and therefore to compute the cross entropy CE(T, p \u03bb ), represented by the points labelled p_lambda in the figure. We could then verify that the policy \u03c0 \u03b8 obtained from P \u03bb by DPG (line dpg pl) was very close to these points. We then considered a second alternative for P , namely P (x) = wn(x) \u00b7 F (x), with wn(x) being the white-noise process filtered by a binary predicate F (x) checking for the presence of the motif; in other words P (x) is an unnormalized version of the true underlying process p true (x). We then applied dpg to this P obtaining the policy represented by the line wn_dpg pl and we could also verify that this line was very close to the line corresponding to p true (shown as true in the figure, but almost hidden by the other line). \n\n\nA.4 Beyond Binary Features in Training-1: Length\n\nWhile the emphasis of the current paper is on Training-2 and its relationship with distributional policies in RL, we also wanted to go beyond one of the limiting assumptions of [15], namely its reliance on binary features only: e.g., presence of a substring, value of the first bit, etc. We wanted to confirm that GAMs can be applied continuous features as well, and in fact to features that have a strong inter-dependence. We also wanted to consider features that relied on weaker prior knowledge than the presence of specific motifs.\n\nTo do that, we considered an additional length feature with two components, namely |x| max_len \u2208 [0, 1] denoted as M and |x| 2 max_len 2 \u2208 [0, 1] denoted as v. We note that the moments of these two features correspond to sufficient statistics for the normal distribution, and roughly speaking GAMs are obtained by matching moments of the given dataset D.\n\nWe were then able during Training-1 to learn the corresponding \u03bb parameters using either snis without modification or rs with a modification for computing the upper bound (since the two components are inter-dependent).\n\nHowever, we noticed that the performance of two training setups (distillation and DPG) was rather similar whether the length feature was on or off (see Figure 5). We speculate that in order to see the impact of the length feature, the strings in D should be longer so that the original AM r would be weaker in characterizing the length. \n\nFigure 3 :\n3snis vs. rs for Training-1. In Training-2, only distillation was used.\n\nFigure 4 :\n4DPG vs. p\n\nFigure 5 :\n5DPG vs Distillation with length feature on (top) or off (bottom).\n\nTable 1 :\n1Statistics over: motif \u2208 {1000101000101, 1011100111001, 10001011111000}, f t \u2208 {1001111, M v1001111}, seed \u2208 {1234, 4444}.|D| \n\nCE(T,\u03c0 \n\ndpg \n\u03b8 \n\n) \n\nCE(T,\u03c0 dis \n\u03b8 ) \n\nmtf_frq(\u03c0 \n\ndpg \n\u03b8 \n\n) \n\nmtf_frq(\u03c0 dis \n\u03b8 ) \n\nCE(T,\u03c0 \n\ndpg \n\u03b8 \n\n) \nCE(T,r) \n\nCE(T,\u03c0 \n\ndpg \n\u03b8 \n\n) \nH(p true ) \n\nmtf_frq(\u03c0 \n\ndpg \n\u03b8 \n\n) \nmtf_frq(r) \n\nCE(T,\u03c0 dis \n\u03b8 ) \nCE(T,r) \n\nmtf_frq(\u03c0 dis \n\u03b8 ) \nmtf_frq(r) \n\n500 \n1.008 \n1.252 \n0.76 \n1.18 \n281.51 \n0.758 \n224.94 \n1000 \n1.014 \n1.102 \n0.762 \n1.178 \n240.40 \n0.76 \n218.24 \n5000 \n1.019 \n1.21 \n0.865 \n1.059 \n34.73 \n0.847 \n28.69 \n10000 \n1.014 \n1.067 \n0.968 \n1.023 \n2.17 \n0.963 \n2.04 \n20000 \n1.004 \n1.023 \n1.0 \n1.006 \n1.03 \n1.002 \n1.01 \n\n\nMore sophisticated MCMC sampling techniques with broader applicability exist[18], but they are typically difficult to control and slow to converge.2 In our discussion of Training-2, to stress the generality of the techniques employed, we will use P (x) to denote any EBM potential over sequences, and p(x) = 1/Z P (x), with Z = x P (x), to denote the associated normalized distribution. Whether P (x) is obtained or not through Training-1 in a GAM-style approach is irrelevant to this discussion.\nEpoch iteration might actually be seen as a form of \"experience replay\", to borrow RL terminology[11].4 The constant factor 1/Z can be ignored here: during SGD, it has the effect of rescaling the learning rate.\nWe also did experiments involving two continuous features (M and v) assessing length, see A.4 in Sup. Mat.\nThe limitation to sequential EBMs is not as serious as it seems. Many objects can be decomposed into sequences of actions, and EBMs over such objects could then be handled in similar ways to those proposed here.\nIt is also interesting to note that both snis and DPG off use importance sampling as the underlying technique.\nAcknowledgements Thanks to Tomi Silander and Hady Elsahar for discussions and feedback.7There are some extreme situations where the P \u03bb obtained at the end of Training-1 can perfectly represent the true underlying process, but no policy has a chance to approximate p \u03bb . This can happen with features associated with complex filters (e.g. of a cryptographic nature) used for generating the data, which can be easily detected as useful during Training-1, but cannot feasibly be projected back onto incremental policies.\nGlobally normalized transition-based neural networks. Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Proceedings of the 54th. the 54thSlav Petrov, and Michael CollinsDaniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, and Michael Collins. Globally normalized transition-based neural networks. In Proceedings of the 54th\n\nAnnual Meeting of the Association for Computational Linguistics. Berlin, GermanyAssociation for Computational Linguistics1Long Papers)Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2442-2452, Berlin, Germany, August 2016. Association for Computational Linguistics.\n\nAn Actor-Critic Algorithm for Sequence Prediction. Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, Yoshua Bengio, Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. An Actor-Critic Algorithm for Sequence Prediction. (2015):1-17, 2016.\n\nStructured prediction energy networks. David Belanger, Andrew Mccallum, Proceedings of the 33rd International Conference on International Conference on Machine Learning. the 33rd International Conference on International Conference on Machine Learning48ICML'16David Belanger and Andrew McCallum. Structured prediction energy net- works. In Proceedings of the 33rd International Conference on International Conference on Machine Learning -Volume 48, ICML'16, pages 983-992. JMLR.org, 2016.\n\n. G Marc, Will Bellemare, R\u00e9mi Dabney, Munos, arXiv:1707.06887arXiv: 1707.06887A Distributional Perspective on Reinforcement Learning. cs, statMarc G. Bellemare, Will Dabney, and R\u00e9mi Munos. A Distributional Perspective on Reinforcement Learning. arXiv:1707.06887 [cs, stat], July 2017. arXiv: 1707.06887.\n\nConvolutional sequence to sequence learning. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N Dauphin, arxiv:1705.03122Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. CoRR, 2017. cite arxiv:1705.03122.\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. 98Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997.\n\nTuning recurrent neural networks with reinforcement learning. Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jose Miguel Hernandez Lobato, Richard E Turner, Doug Eck, Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jose Miguel Hernandez Lobato, Richard E. Turner, and Doug Eck. Tuning recurrent neural networks with reinforcement learning. 2017.\n\nLog-Linear Models, Logistic Regression and Conditional Random Fields. Tony Jebara, Tony Jebara. Log-Linear Models, Logistic Regression and Conditional Random Fields, 2013.\n\nDeep directed generative models with energy-based probability estimation. Taesup Kim, Yoshua Bengio, abs/1606.03439CoRRTaesup Kim and Yoshua Bengio. Deep directed generative models with energy-based probability estimation. CoRR, abs/1606.03439, 2016.\n\n. Yann Lecun, Sumit Chopra, Raia Hadsell, Marc&apos;aurelio Ranzato, Fu Jie Huang, A Tutorial on Energy-Based Learning. Predicting Structured Data. Yann LeCun, Sumit Chopra, Raia Hadsell, Marc'Aurelio Ranzato, and Fu Jie Huang. A Tutorial on Energy-Based Learning. Predicting Structured Data, pages 191-246, 2006.\n\nHuman-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, G Marc, Alex Bellemare, Martin Graves, Andreas K Riedmiller, Georg Fidjeland, Ostrovski, Nature. 5187540529Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep rein- forcement learning. Nature, 518(7540):529, 2015.\n\nAlgorithms for inverse reinforcement learning. Y Andrew, Stuart J Ng, Russell, Proceedings of the Seventeenth International Conference on Machine Learning, ICML '00. the Seventeenth International Conference on Machine Learning, ICML '00San Francisco, CA, USAMorgan Kaufmann Publishers IncAndrew Y. Ng and Stuart J. Russell. Algorithms for inverse reinforcement learning. In Proceedings of the Seventeenth International Conference on Machine Learning, ICML '00, pages 663-670, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc.\n\nReward augmented maximum likelihood for neural structured prediction. Mohammad Norouzi, Samy Bengio, Zhifeng Chen, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans, Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS'16. the 30th International Conference on Neural Information Processing Systems, NIPS'16USACurran Associates IncMohammad Norouzi, Samy Bengio, Zhifeng Chen, Navdeep Jaitly, Mike Schuster, Yonghui Wu, and Dale Schuurmans. Reward augmented max- imum likelihood for neural structured prediction. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS'16, pages 1731-1739, USA, 2016. Curran Associates Inc.\n\nAdaptive Importance Sampling (slides). Art Owen, Art Owen. Adaptive Importance Sampling (slides). 2017.\n\nGlobal Autoregressive Models for Data-Efficient Sequence Learning. Tetiana Parshakova, Jean-Marc Andreoli, Marc Dymetman, Hong KongTetiana Parshakova, Jean-Marc Andreoli, and Marc Dymetman. Global Autoregressive Models for Data-Efficient Sequence Learning. In CoNLL 2019, Hong Kong, November 2019.\n\nAutomatic differentiation in PyTorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, NIPS Autodiff Workshop. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. In NIPS Autodiff Workshop, 2017.\n\nSequence level training with recurrent neural networks. Aurelio Marc, Sumit Ranzato, Michael Chopra, Wojciech Auli, Zaremba, 4th International Conference on Learning Representations. San Juan, Puerto RicoConference Track ProceedingsMarc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016.\n\nMonte Carlo Statistical Methods (Springer Texts in Statistics). P Christian, George Robert, Casella, Springer-VerlagBerlin, HeidelbergChristian P. Robert and George Casella. Monte Carlo Statistical Methods (Springer Texts in Statistics). Springer-Verlag, Berlin, Heidelberg, 2005.\n\nLearning agents for uncertain environments (extended abstract). Stuart Russell, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT' 98. the Eleventh Annual Conference on Computational Learning Theory, COLT' 98New York, NY, USAACMStuart Russell. Learning agents for uncertain environments (extended ab- stract). In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT' 98, pages 101-103, New York, NY, USA, 1998. ACM.\n\nTrust region policy optimization. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz, International conference on machine learning. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pages 1889-1897, 2015.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv preprint: 1707.06347Proximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint: 1707.06347, 2017.\n\nSequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems. Montreal, Quebec, CanadaIlya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 3104-3112, 2014.\n\nReinforcement Learning: An Introduction. Richard S Sutton, Andrew G Barto, The MIT Presssecond editionRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Long Beach, CA, USAAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 6000-6010, 2017.\n\nSimple statistical gradient-following algorithms for connectionist reinforcement learning. Ronald J Williams, Machine Learning. Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Machine Learning, pages 229-256, 1992.\n\nAdaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model. Y Bengio, J S Senecal, Ieee Transactions on Neural Networks. 194Y. Bengio and J. S. Senecal. Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model. Ieee Transactions on Neural Networks, 19(4):713-722, 2008.\n", "annotations": {"author": "[{\"end\":117,\"start\":76},{\"end\":170,\"start\":118},{\"end\":213,\"start\":171},{\"end\":236,\"start\":214},{\"end\":257,\"start\":237}]", "publisher": null, "author_last_name": "[{\"end\":94,\"start\":84},{\"end\":136,\"start\":128},{\"end\":184,\"start\":176}]", "author_first_name": "[{\"end\":83,\"start\":76},{\"end\":127,\"start\":118},{\"end\":175,\"start\":171}]", "author_affiliation": "[{\"end\":235,\"start\":215},{\"end\":256,\"start\":238}]", "title": "[{\"end\":73,\"start\":1},{\"end\":330,\"start\":258}]", "venue": null, "abstract": "[{\"end\":1045,\"start\":332}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1110,\"start\":1107},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":1113,\"start\":1110},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1115,\"start\":1113},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1118,\"start\":1115},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1192,\"start\":1188},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1574,\"start\":1570},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2299,\"start\":2295},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4013,\"start\":4010},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4016,\"start\":4013},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4029,\"start\":4026},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4047,\"start\":4043},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4809,\"start\":4805},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5346,\"start\":5342},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5816,\"start\":5813},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8875,\"start\":8871},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8974,\"start\":8970},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9172,\"start\":9168},{\"end\":9274,\"start\":9267},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9727,\"start\":9726},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10865,\"start\":10861},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11314,\"start\":11313},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11818,\"start\":11814},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11920,\"start\":11916},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12162,\"start\":12161},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13119,\"start\":13115},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13131,\"start\":13127},{\"end\":13398,\"start\":13396},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14445,\"start\":14441},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14994,\"start\":14990},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15007,\"start\":15003},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15485,\"start\":15481},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15677,\"start\":15673},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16041,\"start\":16037},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20488,\"start\":20487},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21295,\"start\":21291},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21421,\"start\":21420},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21642,\"start\":21638},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21645,\"start\":21642},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22235,\"start\":22231},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22608,\"start\":22604},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23018,\"start\":23015},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23362,\"start\":23359},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23550,\"start\":23547},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23955,\"start\":23951},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24193,\"start\":24190},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24381,\"start\":24378},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24390,\"start\":24386},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25085,\"start\":25082},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25869,\"start\":25865},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25910,\"start\":25906},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26062,\"start\":26058},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28999,\"start\":28995},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31195,\"start\":31191},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31263,\"start\":31262},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31713,\"start\":31709},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31715,\"start\":31714}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30352,\"start\":30269},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30375,\"start\":30353},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30454,\"start\":30376},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31114,\"start\":30455}]", "paragraph": "[{\"end\":2121,\"start\":1061},{\"end\":2279,\"start\":2123},{\"end\":3571,\"start\":2281},{\"end\":3856,\"start\":3573},{\"end\":4647,\"start\":3879},{\"end\":5201,\"start\":4649},{\"end\":5577,\"start\":5210},{\"end\":6099,\"start\":5627},{\"end\":6945,\"start\":6137},{\"end\":7052,\"start\":6947},{\"end\":7350,\"start\":7087},{\"end\":7621,\"start\":7363},{\"end\":7679,\"start\":7623},{\"end\":7711,\"start\":7701},{\"end\":7835,\"start\":7713},{\"end\":8199,\"start\":7837},{\"end\":9378,\"start\":8310},{\"end\":10327,\"start\":9452},{\"end\":11450,\"start\":10404},{\"end\":11669,\"start\":11452},{\"end\":11921,\"start\":11745},{\"end\":12144,\"start\":11984},{\"end\":12509,\"start\":12151},{\"end\":13308,\"start\":12579},{\"end\":13421,\"start\":13332},{\"end\":13519,\"start\":13488},{\"end\":13540,\"start\":13521},{\"end\":14122,\"start\":13542},{\"end\":14254,\"start\":14124},{\"end\":15136,\"start\":14270},{\"end\":15632,\"start\":15138},{\"end\":15842,\"start\":15634},{\"end\":16927,\"start\":15844},{\"end\":17338,\"start\":16962},{\"end\":17733,\"start\":17340},{\"end\":18094,\"start\":17735},{\"end\":18253,\"start\":18096},{\"end\":19264,\"start\":18255},{\"end\":20409,\"start\":19276},{\"end\":20662,\"start\":20424},{\"end\":20789,\"start\":20664},{\"end\":21840,\"start\":20791},{\"end\":22237,\"start\":21842},{\"end\":22527,\"start\":22239},{\"end\":22901,\"start\":22575},{\"end\":23843,\"start\":22903},{\"end\":25071,\"start\":23845},{\"end\":25478,\"start\":25073},{\"end\":25704,\"start\":25528},{\"end\":26324,\"start\":25751},{\"end\":26567,\"start\":26333},{\"end\":26814,\"start\":26569},{\"end\":27006,\"start\":26902},{\"end\":27405,\"start\":27008},{\"end\":28765,\"start\":27463},{\"end\":29353,\"start\":28818},{\"end\":29709,\"start\":29355},{\"end\":29929,\"start\":29711},{\"end\":30268,\"start\":29931}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5626,\"start\":5578},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6136,\"start\":6100},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7086,\"start\":7053},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7700,\"start\":7680},{\"attributes\":{\"id\":\"formula_4\"},\"end\":8309,\"start\":8200},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10403,\"start\":10328},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11744,\"start\":11670},{\"attributes\":{\"id\":\"formula_7\"},\"end\":11983,\"start\":11922},{\"attributes\":{\"id\":\"formula_8\"},\"end\":12150,\"start\":12145},{\"attributes\":{\"id\":\"formula_9\"},\"end\":12578,\"start\":12510},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13487,\"start\":13422},{\"attributes\":{\"id\":\"formula_11\"},\"end\":25750,\"start\":25705},{\"attributes\":{\"id\":\"formula_12\"},\"end\":26332,\"start\":26325},{\"attributes\":{\"id\":\"formula_13\"},\"end\":26901,\"start\":26815}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19286,\"start\":19279}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1059,\"start\":1047},{\"attributes\":{\"n\":\"2\"},\"end\":3864,\"start\":3859},{\"attributes\":{\"n\":\"2.1\"},\"end\":3877,\"start\":3867},{\"attributes\":{\"n\":\"2.2\"},\"end\":5208,\"start\":5204},{\"attributes\":{\"n\":\"3\"},\"end\":7361,\"start\":7353},{\"attributes\":{\"n\":\"3.2\"},\"end\":9450,\"start\":9381},{\"end\":13330,\"start\":13311},{\"attributes\":{\"n\":\"4\"},\"end\":14268,\"start\":14257},{\"end\":16960,\"start\":16930},{\"end\":19274,\"start\":19267},{\"attributes\":{\"n\":\"5\"},\"end\":20422,\"start\":20412},{\"end\":22554,\"start\":22530},{\"end\":22573,\"start\":22557},{\"end\":25526,\"start\":25481},{\"end\":27461,\"start\":27408},{\"end\":28816,\"start\":28768},{\"end\":30280,\"start\":30270},{\"end\":30364,\"start\":30354},{\"end\":30387,\"start\":30377},{\"end\":30465,\"start\":30456}]", "table": "[{\"end\":31114,\"start\":30589}]", "figure_caption": "[{\"end\":30352,\"start\":30282},{\"end\":30375,\"start\":30366},{\"end\":30454,\"start\":30389},{\"end\":30589,\"start\":30467}]", "figure_ref": "[{\"end\":7619,\"start\":7613},{\"end\":7732,\"start\":7724},{\"end\":17155,\"start\":17147},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27261,\"start\":27255},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27705,\"start\":27697},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27829,\"start\":27821},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30091,\"start\":30083}]", "bib_author_first_name": "[{\"end\":32832,\"start\":32826},{\"end\":32845,\"start\":32840},{\"end\":32860,\"start\":32855},{\"end\":32876,\"start\":32868},{\"end\":32896,\"start\":32886},{\"end\":32911,\"start\":32905},{\"end\":33569,\"start\":33562},{\"end\":33588,\"start\":33580},{\"end\":33603,\"start\":33597},{\"end\":33615,\"start\":33608},{\"end\":33627,\"start\":33623},{\"end\":33640,\"start\":33634},{\"end\":33654,\"start\":33649},{\"end\":33672,\"start\":33666},{\"end\":33919,\"start\":33914},{\"end\":33936,\"start\":33930},{\"end\":34368,\"start\":34367},{\"end\":34379,\"start\":34375},{\"end\":34395,\"start\":34391},{\"end\":34722,\"start\":34717},{\"end\":34739,\"start\":34732},{\"end\":34751,\"start\":34746},{\"end\":34767,\"start\":34762},{\"end\":34780,\"start\":34776},{\"end\":34782,\"start\":34781},{\"end\":34997,\"start\":34993},{\"end\":35016,\"start\":35010},{\"end\":35228,\"start\":35221},{\"end\":35245,\"start\":35237},{\"end\":35257,\"start\":35250},{\"end\":35289,\"start\":35268},{\"end\":35305,\"start\":35298},{\"end\":35307,\"start\":35306},{\"end\":35320,\"start\":35316},{\"end\":35579,\"start\":35575},{\"end\":35758,\"start\":35752},{\"end\":35770,\"start\":35764},{\"end\":35936,\"start\":35932},{\"end\":35949,\"start\":35944},{\"end\":35962,\"start\":35958},{\"end\":35989,\"start\":35972},{\"end\":36005,\"start\":35999},{\"end\":36311,\"start\":36302},{\"end\":36323,\"start\":36318},{\"end\":36342,\"start\":36337},{\"end\":36357,\"start\":36351},{\"end\":36359,\"start\":36358},{\"end\":36370,\"start\":36366},{\"end\":36380,\"start\":36379},{\"end\":36391,\"start\":36387},{\"end\":36409,\"start\":36403},{\"end\":36425,\"start\":36418},{\"end\":36427,\"start\":36426},{\"end\":36445,\"start\":36440},{\"end\":36795,\"start\":36794},{\"end\":36810,\"start\":36804},{\"end\":36812,\"start\":36811},{\"end\":37364,\"start\":37356},{\"end\":37378,\"start\":37374},{\"end\":37394,\"start\":37387},{\"end\":37408,\"start\":37401},{\"end\":37421,\"start\":37417},{\"end\":37439,\"start\":37432},{\"end\":37448,\"start\":37444},{\"end\":38046,\"start\":38043},{\"end\":38183,\"start\":38176},{\"end\":38205,\"start\":38196},{\"end\":38220,\"start\":38216},{\"end\":38450,\"start\":38446},{\"end\":38462,\"start\":38459},{\"end\":38477,\"start\":38470},{\"end\":38495,\"start\":38488},{\"end\":38510,\"start\":38504},{\"end\":38524,\"start\":38517},{\"end\":38539,\"start\":38533},{\"end\":38550,\"start\":38545},{\"end\":38566,\"start\":38562},{\"end\":38579,\"start\":38575},{\"end\":38891,\"start\":38884},{\"end\":38903,\"start\":38898},{\"end\":38920,\"start\":38913},{\"end\":38937,\"start\":38929},{\"end\":39400,\"start\":39399},{\"end\":39418,\"start\":39412},{\"end\":39687,\"start\":39681},{\"end\":40139,\"start\":40135},{\"end\":40156,\"start\":40150},{\"end\":40171,\"start\":40165},{\"end\":40187,\"start\":40180},{\"end\":40203,\"start\":40196},{\"end\":40450,\"start\":40446},{\"end\":40466,\"start\":40461},{\"end\":40483,\"start\":40475},{\"end\":40498,\"start\":40494},{\"end\":40512,\"start\":40508},{\"end\":40799,\"start\":40795},{\"end\":40816,\"start\":40811},{\"end\":40830,\"start\":40826},{\"end\":40832,\"start\":40831},{\"end\":41314,\"start\":41307},{\"end\":41316,\"start\":41315},{\"end\":41331,\"start\":41325},{\"end\":41333,\"start\":41332},{\"end\":41519,\"start\":41513},{\"end\":41533,\"start\":41529},{\"end\":41547,\"start\":41543},{\"end\":41561,\"start\":41556},{\"end\":41578,\"start\":41573},{\"end\":41591,\"start\":41586},{\"end\":41593,\"start\":41592},{\"end\":41607,\"start\":41601},{\"end\":41621,\"start\":41616},{\"end\":42203,\"start\":42197},{\"end\":42205,\"start\":42204},{\"end\":42483,\"start\":42482},{\"end\":42493,\"start\":42492},{\"end\":42495,\"start\":42494}]", "bib_author_last_name": "[{\"end\":32838,\"start\":32833},{\"end\":32853,\"start\":32846},{\"end\":32866,\"start\":32861},{\"end\":32884,\"start\":32877},{\"end\":32903,\"start\":32897},{\"end\":32919,\"start\":32912},{\"end\":33578,\"start\":33570},{\"end\":33595,\"start\":33589},{\"end\":33606,\"start\":33604},{\"end\":33621,\"start\":33616},{\"end\":33632,\"start\":33628},{\"end\":33647,\"start\":33641},{\"end\":33664,\"start\":33655},{\"end\":33679,\"start\":33673},{\"end\":33928,\"start\":33920},{\"end\":33945,\"start\":33937},{\"end\":34373,\"start\":34369},{\"end\":34389,\"start\":34380},{\"end\":34402,\"start\":34396},{\"end\":34409,\"start\":34404},{\"end\":34730,\"start\":34723},{\"end\":34744,\"start\":34740},{\"end\":34760,\"start\":34752},{\"end\":34774,\"start\":34768},{\"end\":34790,\"start\":34783},{\"end\":35008,\"start\":34998},{\"end\":35028,\"start\":35017},{\"end\":35235,\"start\":35229},{\"end\":35248,\"start\":35246},{\"end\":35266,\"start\":35258},{\"end\":35296,\"start\":35290},{\"end\":35314,\"start\":35308},{\"end\":35324,\"start\":35321},{\"end\":35586,\"start\":35580},{\"end\":35762,\"start\":35759},{\"end\":35777,\"start\":35771},{\"end\":35942,\"start\":35937},{\"end\":35956,\"start\":35950},{\"end\":35970,\"start\":35963},{\"end\":35997,\"start\":35990},{\"end\":36011,\"start\":36006},{\"end\":36316,\"start\":36312},{\"end\":36335,\"start\":36324},{\"end\":36349,\"start\":36343},{\"end\":36364,\"start\":36360},{\"end\":36377,\"start\":36371},{\"end\":36385,\"start\":36381},{\"end\":36401,\"start\":36392},{\"end\":36416,\"start\":36410},{\"end\":36438,\"start\":36428},{\"end\":36455,\"start\":36446},{\"end\":36466,\"start\":36457},{\"end\":36802,\"start\":36796},{\"end\":36815,\"start\":36813},{\"end\":36824,\"start\":36817},{\"end\":37372,\"start\":37365},{\"end\":37385,\"start\":37379},{\"end\":37399,\"start\":37395},{\"end\":37415,\"start\":37409},{\"end\":37430,\"start\":37422},{\"end\":37442,\"start\":37440},{\"end\":37459,\"start\":37449},{\"end\":38051,\"start\":38047},{\"end\":38194,\"start\":38184},{\"end\":38214,\"start\":38206},{\"end\":38229,\"start\":38221},{\"end\":38457,\"start\":38451},{\"end\":38468,\"start\":38463},{\"end\":38486,\"start\":38478},{\"end\":38502,\"start\":38496},{\"end\":38515,\"start\":38511},{\"end\":38531,\"start\":38525},{\"end\":38543,\"start\":38540},{\"end\":38560,\"start\":38551},{\"end\":38573,\"start\":38567},{\"end\":38585,\"start\":38580},{\"end\":38896,\"start\":38892},{\"end\":38911,\"start\":38904},{\"end\":38927,\"start\":38921},{\"end\":38942,\"start\":38938},{\"end\":38951,\"start\":38944},{\"end\":39410,\"start\":39401},{\"end\":39425,\"start\":39419},{\"end\":39434,\"start\":39427},{\"end\":39695,\"start\":39688},{\"end\":40148,\"start\":40140},{\"end\":40163,\"start\":40157},{\"end\":40178,\"start\":40172},{\"end\":40194,\"start\":40188},{\"end\":40210,\"start\":40204},{\"end\":40459,\"start\":40451},{\"end\":40473,\"start\":40467},{\"end\":40492,\"start\":40484},{\"end\":40506,\"start\":40499},{\"end\":40519,\"start\":40513},{\"end\":40809,\"start\":40800},{\"end\":40824,\"start\":40817},{\"end\":40835,\"start\":40833},{\"end\":41323,\"start\":41317},{\"end\":41339,\"start\":41334},{\"end\":41527,\"start\":41520},{\"end\":41541,\"start\":41534},{\"end\":41554,\"start\":41548},{\"end\":41571,\"start\":41562},{\"end\":41584,\"start\":41579},{\"end\":41599,\"start\":41594},{\"end\":41614,\"start\":41608},{\"end\":41632,\"start\":41622},{\"end\":42214,\"start\":42206},{\"end\":42490,\"start\":42484},{\"end\":42503,\"start\":42496}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2952144},\"end\":33195,\"start\":32772},{\"attributes\":{\"id\":\"b1\"},\"end\":33509,\"start\":33197},{\"attributes\":{\"id\":\"b2\"},\"end\":33873,\"start\":33511},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":6366436},\"end\":34363,\"start\":33875},{\"attributes\":{\"doi\":\"arXiv:1707.06887\",\"id\":\"b4\"},\"end\":34670,\"start\":34365},{\"attributes\":{\"id\":\"b5\"},\"end\":34967,\"start\":34672},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1915014},\"end\":35157,\"start\":34969},{\"attributes\":{\"id\":\"b7\"},\"end\":35503,\"start\":35159},{\"attributes\":{\"id\":\"b8\"},\"end\":35676,\"start\":35505},{\"attributes\":{\"id\":\"b9\"},\"end\":35928,\"start\":35678},{\"attributes\":{\"id\":\"b10\"},\"end\":36243,\"start\":35930},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":205242740},\"end\":36745,\"start\":36245},{\"attributes\":{\"id\":\"b12\"},\"end\":37284,\"start\":36747},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":3631537},\"end\":38002,\"start\":37286},{\"attributes\":{\"id\":\"b14\"},\"end\":38107,\"start\":38004},{\"attributes\":{\"id\":\"b15\"},\"end\":38406,\"start\":38109},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":40027675},\"end\":38826,\"start\":38408},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":7147309},\"end\":39333,\"start\":38828},{\"attributes\":{\"id\":\"b18\"},\"end\":39615,\"start\":39335},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":546942},\"end\":40099,\"start\":39617},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":16046818},\"end\":40444,\"start\":40101},{\"attributes\":{\"id\":\"b21\"},\"end\":40741,\"start\":40446},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7961699},\"end\":41264,\"start\":40743},{\"attributes\":{\"id\":\"b23\"},\"end\":41484,\"start\":41266},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":13756489},\"end\":42104,\"start\":41486},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2332513},\"end\":42386,\"start\":42106},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":9147661},\"end\":42727,\"start\":42388}]", "bib_title": "[{\"end\":32824,\"start\":32772},{\"end\":33912,\"start\":33875},{\"end\":34991,\"start\":34969},{\"end\":36300,\"start\":36245},{\"end\":36792,\"start\":36747},{\"end\":37354,\"start\":37286},{\"end\":38444,\"start\":38408},{\"end\":38882,\"start\":38828},{\"end\":39679,\"start\":39617},{\"end\":40133,\"start\":40101},{\"end\":40793,\"start\":40743},{\"end\":41511,\"start\":41486},{\"end\":42195,\"start\":42106},{\"end\":42480,\"start\":42388}]", "bib_author": "[{\"end\":32840,\"start\":32826},{\"end\":32855,\"start\":32840},{\"end\":32868,\"start\":32855},{\"end\":32886,\"start\":32868},{\"end\":32905,\"start\":32886},{\"end\":32921,\"start\":32905},{\"end\":33580,\"start\":33562},{\"end\":33597,\"start\":33580},{\"end\":33608,\"start\":33597},{\"end\":33623,\"start\":33608},{\"end\":33634,\"start\":33623},{\"end\":33649,\"start\":33634},{\"end\":33666,\"start\":33649},{\"end\":33681,\"start\":33666},{\"end\":33930,\"start\":33914},{\"end\":33947,\"start\":33930},{\"end\":34375,\"start\":34367},{\"end\":34391,\"start\":34375},{\"end\":34404,\"start\":34391},{\"end\":34411,\"start\":34404},{\"end\":34732,\"start\":34717},{\"end\":34746,\"start\":34732},{\"end\":34762,\"start\":34746},{\"end\":34776,\"start\":34762},{\"end\":34792,\"start\":34776},{\"end\":35010,\"start\":34993},{\"end\":35030,\"start\":35010},{\"end\":35237,\"start\":35221},{\"end\":35250,\"start\":35237},{\"end\":35268,\"start\":35250},{\"end\":35298,\"start\":35268},{\"end\":35316,\"start\":35298},{\"end\":35326,\"start\":35316},{\"end\":35588,\"start\":35575},{\"end\":35764,\"start\":35752},{\"end\":35779,\"start\":35764},{\"end\":35944,\"start\":35932},{\"end\":35958,\"start\":35944},{\"end\":35972,\"start\":35958},{\"end\":35999,\"start\":35972},{\"end\":36013,\"start\":35999},{\"end\":36318,\"start\":36302},{\"end\":36337,\"start\":36318},{\"end\":36351,\"start\":36337},{\"end\":36366,\"start\":36351},{\"end\":36379,\"start\":36366},{\"end\":36387,\"start\":36379},{\"end\":36403,\"start\":36387},{\"end\":36418,\"start\":36403},{\"end\":36440,\"start\":36418},{\"end\":36457,\"start\":36440},{\"end\":36468,\"start\":36457},{\"end\":36804,\"start\":36794},{\"end\":36817,\"start\":36804},{\"end\":36826,\"start\":36817},{\"end\":37374,\"start\":37356},{\"end\":37387,\"start\":37374},{\"end\":37401,\"start\":37387},{\"end\":37417,\"start\":37401},{\"end\":37432,\"start\":37417},{\"end\":37444,\"start\":37432},{\"end\":37461,\"start\":37444},{\"end\":38053,\"start\":38043},{\"end\":38196,\"start\":38176},{\"end\":38216,\"start\":38196},{\"end\":38231,\"start\":38216},{\"end\":38459,\"start\":38446},{\"end\":38470,\"start\":38459},{\"end\":38488,\"start\":38470},{\"end\":38504,\"start\":38488},{\"end\":38517,\"start\":38504},{\"end\":38533,\"start\":38517},{\"end\":38545,\"start\":38533},{\"end\":38562,\"start\":38545},{\"end\":38575,\"start\":38562},{\"end\":38587,\"start\":38575},{\"end\":38898,\"start\":38884},{\"end\":38913,\"start\":38898},{\"end\":38929,\"start\":38913},{\"end\":38944,\"start\":38929},{\"end\":38953,\"start\":38944},{\"end\":39412,\"start\":39399},{\"end\":39427,\"start\":39412},{\"end\":39436,\"start\":39427},{\"end\":39697,\"start\":39681},{\"end\":40150,\"start\":40135},{\"end\":40165,\"start\":40150},{\"end\":40180,\"start\":40165},{\"end\":40196,\"start\":40180},{\"end\":40212,\"start\":40196},{\"end\":40461,\"start\":40446},{\"end\":40475,\"start\":40461},{\"end\":40494,\"start\":40475},{\"end\":40508,\"start\":40494},{\"end\":40521,\"start\":40508},{\"end\":40811,\"start\":40795},{\"end\":40826,\"start\":40811},{\"end\":40837,\"start\":40826},{\"end\":41325,\"start\":41307},{\"end\":41341,\"start\":41325},{\"end\":41529,\"start\":41513},{\"end\":41543,\"start\":41529},{\"end\":41556,\"start\":41543},{\"end\":41573,\"start\":41556},{\"end\":41586,\"start\":41573},{\"end\":41601,\"start\":41586},{\"end\":41616,\"start\":41601},{\"end\":41634,\"start\":41616},{\"end\":42216,\"start\":42197},{\"end\":42492,\"start\":42482},{\"end\":42505,\"start\":42492}]", "bib_venue": "[{\"end\":32986,\"start\":32946},{\"end\":33277,\"start\":33262},{\"end\":34126,\"start\":34045},{\"end\":37005,\"start\":36913},{\"end\":37647,\"start\":37561},{\"end\":39032,\"start\":39011},{\"end\":39877,\"start\":39787},{\"end\":40975,\"start\":40951},{\"end\":41767,\"start\":41748},{\"end\":32944,\"start\":32921},{\"end\":33260,\"start\":33197},{\"end\":33560,\"start\":33511},{\"end\":34043,\"start\":33947},{\"end\":34498,\"start\":34444},{\"end\":34715,\"start\":34672},{\"end\":35048,\"start\":35030},{\"end\":35219,\"start\":35159},{\"end\":35573,\"start\":35505},{\"end\":35750,\"start\":35678},{\"end\":36076,\"start\":36013},{\"end\":36474,\"start\":36468},{\"end\":36911,\"start\":36826},{\"end\":37559,\"start\":37461},{\"end\":38041,\"start\":38004},{\"end\":38174,\"start\":38109},{\"end\":38609,\"start\":38587},{\"end\":39009,\"start\":38953},{\"end\":39397,\"start\":39335},{\"end\":39785,\"start\":39697},{\"end\":40256,\"start\":40212},{\"end\":40586,\"start\":40547},{\"end\":40949,\"start\":40837},{\"end\":41305,\"start\":41266},{\"end\":41746,\"start\":41634},{\"end\":42232,\"start\":42216},{\"end\":42541,\"start\":42505}]"}}}, "year": 2023, "month": 12, "day": 17}