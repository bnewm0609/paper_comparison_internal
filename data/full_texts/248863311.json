{"id": 248863311, "updated": "2023-10-05 14:29:32.087", "metadata": {"title": "Dialog Inpainting: Turning Documents into Dialogs", "authors": "[{\"first\":\"Zhuyun\",\"last\":\"Dai\",\"middle\":[]},{\"first\":\"Arun\",\"last\":\"Chaganty\",\"middle\":[\"Tejasvi\"]},{\"first\":\"Vincent\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Aida\",\"last\":\"Amini\",\"middle\":[]},{\"first\":\"Qazi\",\"last\":\"Rashid\",\"middle\":[\"Mamunur\"]},{\"first\":\"Mike\",\"last\":\"Green\",\"middle\":[]},{\"first\":\"Kelvin\",\"last\":\"Guu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Many important questions (e.g.\"How to eat healthier?\") require conversation to establish context and explore in depth. However, conversational question answering (ConvQA) systems have long been stymied by scarce training data that is expensive to collect. To address this problem, we propose a new technique for synthetically generating diverse and high-quality dialog data: dialog inpainting. Our approach takes the text of any document and transforms it into a two-person dialog between the writer and an imagined reader: we treat sentences from the article as utterances spoken by the writer, and then use a dialog inpainter to predict what the imagined reader asked or said in between each of the writer's utterances. By applying this approach to passages from Wikipedia and the web, we produce WikiDialog and WebDialog, two datasets totalling 19 million diverse information-seeking dialogs -- 1,000x larger than the largest existing ConvQA dataset. Furthermore, human raters judge the answer adequacy and conversationality of WikiDialog to be as good or better than existing manually-collected datasets. Using our inpainted data to pre-train ConvQA retrieval systems, we significantly advance state-of-the-art across three benchmarks (QReCC, OR-QuAC, TREC CAsT) yielding up to 40% relative gains on standard evaluation metrics.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2205.09073", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/DaiCZARGG22", "doi": "10.48550/arxiv.2205.09073"}}, "content": {"source": {"pdf_hash": "3bbf28bac4c4a150bc271bf17a03e2094def5f65", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2205.09073v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d6656b84cacb6c4fca65defea5a50b9dc7f0bbf5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3bbf28bac4c4a150bc271bf17a03e2094def5f65.txt", "contents": "\nDialog Inpainting: Turning Documents into Dialogs\n\n\nZhuyun Dai \nArun Tejasvi Chaganty \nVincent Zhao \nAida Amini \nQazi Mamunur Rashid \nMike Green \nKelvin Guu \nDialog Inpainting: Turning Documents into Dialogs\n\nMany important questions (e.g. \"How to eat healthier?\") require conversation to establish context and explore in depth. However, conversational question answering (ConvQA) systems have long been stymied by scarce training data that is expensive to collect. To address this problem, we propose a new technique for synthetically generating diverse and high-quality dialog data: dialog inpainting. Our approach takes the text of any document and transforms it into a twoperson dialog between the writer and an imagined reader: we treat sentences from the article as utterances spoken by the writer, and then use a dialog inpainter to predict what the imagined reader asked or said in between each of the writer's utterances. By applying this approach to passages from Wikipedia and the web, we produce WikiDialog and WebDialog, two datasets totalling 19 million diverse information-seeking dialogs-1,000x larger than the largest existing ConvQA dataset. Furthermore, human raters judge the answer adequacy and conversationality of WikiDialog to be as good or better than existing manually-collected datasets. Using our inpainted data to pre-train ConvQA retrieval systems, we significantly advance state-of-the-art across three benchmarks (QReCC, OR-QuAC, TREC CAsT) yielding up to 40% relative gains on standard evaluation metrics.\n\nIntroduction\n\nModern information-seeking tools such as web search and question answering (Karpukhin et al., 2020;Zhu et al., 2021) excel at questions that have well-defined answers (e.g., \"Where was Barack Obama born?\"). But many important questions are more open-ended-e.g., \"How to eat healthier?\"-and require conversation to elicit context and explore in depth: \"How do I eat more protein?\", \"What about vegetarians?\". Conversational question answering systems (ConvQA) (Stede & Schlangen, 2004;Radlinski & Craswell, 2017;Culpepper et al., 2018), would empower users to answer these questions as if they could discuss with an expert at any time.\n\nDespite this promising vision, progress has been stymied by scarce training data. While conversational data is abundant in online forums, much of it focuses on personal anecdotes and subjective opinions, and is thus unsuitable for an information-seeking system: we desire responses that minimize personal biases and cite reliable sources. Directly crowdsourcing dialogs is also hard: crowdworkers are rarely experts in the domain of interest and tend to overlook important questions or provide shallow answers (Li et al., 2021). It is also expensive: the largest extant datasets contain only about 10,000 conversations each (Choi et al., 2018;Reddy arXiv:2205.09073v2 [cs.CL] 31 May 2022et al., 2019Dinan et al., 2018;Saeidi et al., 2018;Campos et al., 2020;Feng et al., 2020;Anantha et al., 2021).\n\nOn the other hand, high-quality documents, such as those in Wikipedia or PubMed, are abundant. These documents are often edited or written by experts who have invested significant time streamlining their discourse and anticipating a reader's questions. What if we could rewrite these documents into dialogs between the writer and their imagined reader? This would yield an enormous corpus of information-seeking dialogs with attributable, expert answers that could then be used to train a ConvQA system. We aim to achieve this with dialog inpainting.\n\nTo transform any document into a dialog, our first observation is that we already know what the writer wants to discuss-that is reflected in the original text of the document. So, we pretend that the original document is the transcript of what the writer said to the reader in an imagined dialog. But we are still missing what the reader asked. This is like overhearing someone else's phone call: you hear one side, but not the other. Oftentimes, one can still guess what the other side was saying -we call this prediction task dialog inpainting, because we are \"inpainting\" the missing parts of the dialog that we did not hear (inspired by the term's usage in computer vision (Iizuka et al., 2017;Liu et al., 2018;Yu et al., 2018)). Drawing on this intuition, we train an inpainter model to predict missing utterances in a dialog, and use it to predict the unobserved questions in a document. By interleaving the generated questions and sentences from the document, we form a dialog ( Figure 1).\n\nWe apply our inpainter to passages from Wikipedia and the web, yielding WikiDialog and WebDialog, 1 two datasets totalling 19M+ dialogs -1,000x larger than the largest existing ConvQA dataset. When evaluated for conversationality and answer adequacy, we surprisingly find that our synthetically generated data is as good or better than previous crowd-sourced datasets (Section 3). Our generated dialogs inherit the good qualities of the professionally written documents we inpaint (topical diversity, coherent discourse, evidence-backed claims, etc.) without needing to train on dialog data of the same quality.\n\nImportantly, we find that our inpainted datasets are powerful sources of training data for ConvQA systems (Section 4). When used to pre-train standard retriever and reranker architectures, they advance state-of-the-art across three different ConvQA retrieval benchmarks (QRECC, OR-QUAC, TREC-CAST), delivering up to 40% relative gains on standard evaluation metrics (Section 5). Remarkably, we find that just pre-training on WikiDialog enables strong zero-shot retrieval performance-up to 95% of a finetuned retriever's performance-without using any in-domain ConvQA data.\n\n\nDialog Inpainting\n\nThe goal of dialog inpainting is to take a partial dialog (one where some of the speaker turns are unobserved), and generate a complete dialog (one where all unobserved turns have been filled in with the model's predictions).\n\nFormally, a complete dialog d is a sequence of speaker utterances, d = (u 1 , u 2 , . . . , u t , . . . , u T ). We use the same notation for partial dialogs, denoting unobserved utterances with the symbol. For example, (u 1 , u 2 , , u 4 , ) is a partial dialog where utterances u 3 and u 5 are unobserved. We refer to these as \"masked\" utterances. We also use the shorthand d m (3,5) to denote a dialog d with utterances 3 and 5 masked.\n\nTo complete the partial dialog d m(3,5) , we generate predictions for utterances 3 and 5, denoted\u00fb 3 and\u00fb 5 . The inpainted dialog is then: Inpaint(d m(3,5) ) = (u 1 , u 2 ,\u00fb 3 , u 4 ,\u00fb 5 ).\n\n\nTraining: Dialog reconstruction\n\nWe train a dialog inpainter using the following dialog reconstruction task: Given a complete dialog, d = (u 1 , u 2 , . . . , u T ), we randomly mask one utterance, u t , yielding a partial dialog: d m(t) = (u 1 , . . . , u t\u22121 , , u t+1 , . . . , u T ).\n\nGiven this partial dialog, we train our model to predict u t , the original value of the masked utterance. This is similar to the masked language modeling task used by BERT (Devlin et al., 2019), but instead of reconstructing missing tokens in a passage, we reconstruct a missing utterance in a dialog.\n\nMore precisely, let us assume that the dialog inpainter is a generative model with parameters \u03b8 specifying a probability distribution p \u03b8 (u t | d m(t) ). Then, our training objective is to minimize the following loss:\nL(\u03b8) = \u2212 d\u2208D E ut\u223cd [log p \u03b8 (u t | d m(t) )],(1)\nwhere D is a corpus of complete dialogs and u t is a randomly sampled utterance from the dialog d.\n\nWe implement our inpainter using the T5 (Raffel et al., 2020) text-to-text encoder-decoder Transformer (Vaswani et al., 2017), where the input x and output y are represented as text strings. We convert each dialog d into an (x, y) pair as follows: First, we randomly sample a turn t to mask, yielding partial dialog d m(t) and target output u t . To form input string x, we prepend each utterance in d m(t) with its corresponding speaker id (e.g., 0 or 1 in a two-speaker dialog), and simply concatenate them together, for example, \"0:u 1 1:u 2 0: 1:u 4 \". The output string y is simply the masked utterance u t . We optimize this model using a standard cross-entropy loss which is equivalent to Eq. 1.\n\n\nInference: Transforming documents into dialogs\n\nWe now show how to use a trained inpainter to transform a document into a dialog. Suppose we have a document or passage p consisting of sentences (s 1 , s 2 , . . . , s m ). Imagine that each of these sentences was an utterance spoken by the writer in an imagined dialog with the reader: we want to know what the imagined reader said between each of the writer's utterances. We can pose this question to our inpainter by asking it to complete the following partial dialog: ( , s 1 , , s 2 , , . . . , , s m )-each utterance from the imagined reader starts masked and is responded to by the writer with a sentence from the passage. While the model knows the masked utterances come from another speaker, it still lacks any hint that the speaker should ask questions. To provide this hint, we prepend an utterance from the writer s prompt that says: \"Hello, I am an automated assistant and can answer questions about (document title)\". Hence, the overall partial dialog has the form:\n\nPartialDialog(p) = (s prompt , , s 1 , , . . . , , s m ).\n\nNote, however, that this partial dialog contains multiple masked utterances, while our inpainter is only trained to inpaint a single utterance at a time. To handle this, we use the model autoregressively: we begin by providing (s prompt , , s 1 ) as input to the inpainter and generate\u00fb 1 via greedy decoding. We then replace the first mask with\u00fb 1 and use a new input (s prompt ,\u00fb 1 , s 1 , , s 2 ) to generate\u00fb 2 and so on until all masks are filled and the dialog is complete.\n\n2.3. Case study: Applying dialog inpainting to generate an information seeking dialog dataset\n\nWe now have all the ingredients necessary to generate synthetic information-seeking dialogs at scale. In this section, we will outline how we trained inpainters and applied them on two document corpora to respectively generate two such datasets: WikiDialog and WebDialog.\n\n\nInpainter model training.\n\nWe used PublicDialog, TaskMaster, OR-QuAC, and QReCC (Table 2) to train three different inpainters: Inpaint PT , Inpaint OQ and Inpaint PTOQ , where the subscripts indicate which of the above datasets were used. We chose these subsets to study how the type and quantity of training data affect inpainter quality: Inpaint PT only uses open-domain conversational datasets that do not contain any explicit question answering, while Inpaint OQ only uses relatively small conversational question answering datasets, and Inpaint PTOQ uses all of the above. Each inpainter model was initialized from a pre-trained T5-XXL (11B parameters) checkpoint and finetuned on its corre-sponding training data. See Appendix A.2 for details on the hyperparameters used. Document processing. We apply dialog inpainting to two document corpora: WIKI, a collection of 11.4M passages from 5.9M English Wikipedia articles in the OR-QuAC retrieval corpus (Qu et al., 2020), and WEB, a collection of 8.4M English web passages from the MS Marco retrieval corpus (Nguyen et al., 2016). Both corpora were used as is without any further filtering. The passages were split into sentences using the Google Cloud Natural Language API. To limit computation, we only use the first 6 sentences of each passage. 2 The passages were then converted to partial dialogs and inpainted using the method described in Section 2.2. The final results are the datasets WikiDialog PT , WikiDialog OQ and WikiDialog PTOQ and WebDialog PT , where the subscripts indicate which inpainter model was used to generate the data (Table 2).\n\n\nEvaluating WikiDialog as a Dataset\n\nWe now turn to a qualitative evaluation of the dialogs generated in Section 2. We will show that they largely contain information-seeking dialogs with well-matched questions and answers. This makes the data suitable for ConvQA systems, an application explored later in the paper. As WikiDialog is generated using passages from OR-QuAC, the corresponding OR-QuAC dialogs form a natural point of reference: both datasets rely on the same content, but are respectively automatically and manually generated. For this reason, our analysis focuses on the WikiDialog variants.\n\nOur analysis combines automatic and human evaluation. The human evaluation was conducted by asking human raters the subjective questions listed in Table 1 for each turn of a dialog. 3 We found substantial inter-annotator agreement on all four questions, with a Krippendorff's \u03b1 of at least 0.89, and report results aggregated over dialogs corresponding to a common set of 200 randomly chosen passages using the mean score of three raters.\n\nHow information seeking are the generated utterances? Raters judge the generated utterances to almost always be information seeking and topically relevant. We note that, prompt aside, our method is not tailored to explicitly generate information seeking utterances in any way. It is remarkable then that raters found 94.5% of utterances in WikiDialog PT , which was generated without using any ConvQA data, to be information seeking; by training the inpainter using ConvQA data, this number goes to 99-100%. Almost all of these are single sentences phrased as ques- tions, though WikiDialog PT includes some exceptions, e.g., \"Exciting! I wonder if they got a #1 debut with that song.\".\n\nWhat types of questions are generated? Next, we study the distribution of questions generated by clustering utterances on their first two words. Figure 2 shows how this distribution changes over turns: dialogs start with more definitional questions (e.g., what is, who is, where is, etc.) but then diversify into a range of follow-up questions (what happened, did, is, how, why, etc.). A good conversation should have a mix of both specific and broad questions.\n\nHere, raters find that the WikiDialog variants primarily differ from OR-QuAC in having more \"somewhat specific\" questions (e.g., \"Is there anything else interesting about his guitar?\") and fewer \"very specific\" questions (e.g., \"Did people enjoy their music?\"). questions in OR-QuAC are often answered better.\n\nHow conversational are the data? A key distinguishing feature of dialog data is its dependence on prior dialog context. Following prior work (Choi et al., 2018), we use the degree of context dependence as a measure of conversation. Raters judge that questions in the WikiDialog variants follow-up on dialog context significantly more often than those in OR-QuAC. We hypothesize that the tendency to follow-up likely arises from the structure of the underlying passages from which WikiDialog is derived: when a paragraph in Wikipedia explores a single topic in depth, so too does the dialog generated from it in WikiDialog.\n\nDo the data contain potentially sensitive language that may perpetuate unfair bias? Evaluating a dataset of this size for potential unfair bias is a daunting task that requires diligent and thorough investigation. We approached the problem by curating a set of 700 terms from the literature (Bolukbasi et al., 2016;Garg et al., 2018;May et al., 2019;Nadeem et al., 2020;Abid et al., 2021) related to sensitive characteristics-such as race, ethnicity, gender, and sexual orientation. Many instances of these terms are well-motivated: for example, a dialog from a passage about transgender rights in Canada includes the question \"What does anti-discrimination act mean in relation to transgender people?\". We further refined the approach to instead look at co-occurrences between these terms and adjectives that may have negative connotations, focusing on instances where the terms were not explicitly mentioned in the passage. We find that 0.2-0.5% of dialogs in the dataset contain such potentially sensitive interactions, but it is difficult to establish if they perpetuate unfair bias without expert manual review. Therefore, we advise users to note these observations and 0.05 level. . We use a two-stage ConvQA retrieval system. We first retrieve top-K passages from the corpus using a dual-encoder model and then rerank them using a cross-attention model. exercise care while using the dataset. 5\n\n\nApplication: Open-domain Conversational Retrieval\n\nIn this section, we show how our inpainted datasets can serve as a powerful source of training data for ConvQA systems.\n\nA ConvQA system engages with a user through multi-turn dialog, where typically the user poses questions and the system answers (there can be exceptions, e.g., the system asks a clarifying question). During a dialog, whenever it is the system's turn to speak (at some time t), it looks at all previous dialog turns d 1:t = (u 1 , u 2 , . . . , u t ) which we call the dialog history, and outputs a new utterance, u t+1 .\n\nBecause ConvQA dialogs are knowledge-intensive, many systems decompose the task into a two-part retrieve-thengenerate process (Qu et al., 2020;Anantha et al., 2021). First, they employ a conversational retriever to retrieve passages that are relevant to the conversation based on the dialog history d 1:t . Second, they employ a generator which uses both the dialog history (d 1:t ) and the retrieved passages to generate a response, u t+1 . While both steps are important, the conversational retriever is key to helping the model access the right knowledge and also for showing people evidence for an answer.\n\nThis work focuses on the conversational retriever, showing how to improve it by pre-training on our inpainted data, leaving improvements to the generator for future work.\n\nModels. The input to a conversational retriever is the dialog history (d 1:t ) and a passage (p). The output is a score, s(d 1:t , p), indicating the passage's relevance. Retrieval is performed by selecting the passages with the highest scores.\n\nWe also refer to the dialog history as the \"query\" and denote it as q. In some benchmarks that we study, the \"dialog history\" is defined to be all previous utterances, while in others the history is defined to only include the user's questions but not the system's responses.\n\nWe employ two standard models for retrieval: first, we use a dual encoder (Reimers & Gurevych, 2019;Karpukhin et al., 2020;Ni et al., 2021) to select an initial set of candidates. We then rescore those candidates using a cross-attention reranker (Nogueira & Cho, 2019;Nogueira et al., 2020). Model architectures and training objectives are detailed in Appendix C. To train these models, we need a corpus of (q, p * ) pairs, where q is a dialog history and p * is a relevant passage. The following sections describe such data. Figure 3 illustrates our system.\n\nPre-training and Fine-tuning. As outlined in Section 2.2, each dialog generated by our inpainter tends to consist of alternating question and answer utterances: d = (s prompt ,\u00fb 1 , s 1 , ...,\u00fb m , s m ), where inpainted utterances\u00fb i are questions, and their subsequent answers s i are sentences from the original passage p. Intuitively, for each question in the dialog, p is a highly relevant passage that should be retrieved. Based on this observation, we generate examples as follows: first, we randomly select a dialog prefix that ends in a question to be the dialog history: q i = (\u00fb 1 , s 1 , . . . ,\u00fb i ). 6 We then wish to mark the original passage p as a positive passage to retrieve. However, directly using p as a positive example will not yield good results: the dialog history (q i ) includes exact sentences from p, which would cause our retriever to simply learn to string-match, rather than to generalize. To eliminate this problem, we form a new passage that consists only of the remaining sentences in p that haven't appeared in q i yet: p * i def = Concat(s j where j > i).\n\nAfter pre-training (q i , p * i ) pairs from the inpainted data, our retriever is fine-tuned on a downstream ConvQA dataset.\n\n\nEvaluation\n\nWe report quantitative evaluation of dialog inpainting by measuring the impact of WikiDialog and WebDialog on ConvQA retrieval systems.\n\n\nExperimental setup\n\nThe following presents a summary of our experimental setup; see Appendix D for full details.  we use only the current and previous questions as inputs for OR-QuAC, TREC CAsT-19 and CAsT-20, and use previous answers in addition to these as input for QReCC. We report mean reciprocal rank at rank 5 (MRR@5) for OR-QuAC and mean reciprocal rank at full rank (MRR) for QReCC and TREC CAsT. Additional metrics are reported in Appendix E.\n\nModel implementation. We initialize our dual-encoder retrievers and rerankers from pre-trained T5 checkpoints following prior work (Ni et al., 2021;Nogueira et al., 2020). Both retrievers and rerankers are pre-trained on our inpainted datasets. We use the notation T5-Large DE WikiD to denote a dual-encoder that was initialized from a T5-Large checkpoint and pre-trained on WikiDialog (we use WikiDialog PTOQ unless otherwise specified), and T5-Large DE WikiD+WebD to denote a model pre-trained on the union of WikiDialog and WebDialog. Similar notation is used for T5-Base initialized models and rerankers.\n\nDuring fine-tuning, we separately train retrievers and rerankers on OR-QuAC and QReCC, using their validation sets to select checkpoints. Because CAsT19 and CAsT20 are extremely small datasets and do not include a training split, we do not fine-tune dual-enocoder retrievers on these datasets, instead using a retriever finetuned on QReCC data 7 . 7 The QReCC training data does not include TREC CAsT.\n\nWe follow Yu et al. (2021) and use 5-fold cross-validation to finetune rerankers on CAsT19 and CAsT20: for each fold, we split the data into 5 splits based on dialogs, train a reranker on 3 splits of the data, select a checkpoint on one split and test on the remaining split.  Table 3 compares our models with baselines on four Con-vQA retrieval benchmarks. We first note that our relatively simple base retriever model, T5-Large DE, is a strong baseline and significantly outperforms the existing state-of-theart results on QReCC. By simply pre-training on WikiDialog, we observe a 9-30% gain, and outperform all baselines on QReCC and OR-QuAC with large margins. Including WebDialog increases this gain by a further 3-15% MRR, with the most significant gains accruing to CAsT-19 and CAsT-20. With this model, we observe a 43% relative MRR gain over the current state-of-the-art on QReCC, a 12% relative MRR@5 gain on OR-QuAC, and comparable or better performance on CAsT-19 and CAsT-20.\n\n\nMain Results\n\nThe gains achieved in the retrieval stage remain when the results are reranked: we outperformed the current state-ofthe-art on OR-QuAC and TREC CAsT-19, and achieved comparable performance on CAsT-20. We could not find existing published reranker results for QReCC, but observe a 5% relative MRR gain over reranking the non pre-trained T5-Large DE model, which we found to be a strong baseline. 8 Appendix D.3 includes brief summaries of these methods. 9 TREC CAsT has several settings based on if a model uses additional inputs such as manual query rewrites. We follow the automatic setting that does not use additional inputs, and compare against top automatic runs reported in Dalton et al. (2019;2020 \n\n\nAnalysis\n\nNext, we dive into our results in greater detail and study factors contributing to performance:\n\nHow much does in-domain training matter for inpainter quality? Table 4 compares the performance of retrievers pre-trained on three variants of WikiDialog introduced in Section 3: PT, OQ, and PTOQ. Remarkably, using WikiDialog PT is sufficient to significantly outperform current state-of-the-art methods, despite being generated using only open-domain conversational data. Next, we observe that using an inpainter trained on OR-QuAC and QReCC data (WikiDialog OQ ) results in slightly better performance; it is notable that an inpainter trained using only 20K dialogs is able to generate such high-quality data, and can improve performance over a retriever just fine-tuned on the same data by up to 20%. Finally, we observed that an inpainter trained on both types of data (WikiDialog PTOQ ) is able to generalize better to CAsT-19, an out-of-domain task we evaluate on using a QReCC retriever.\n\nHow does WikiDialog compare to other retriever pretraining datasets? et al., 2021), the largest existing (non-conversational) question-answering dataset, also automatically generated from Wikipedia; and PublicDialog + TaskMaster (PT), the open-domain dialog data introduced in Section 2 as a training dataset for the inpainter-we use it here to directly pre-train a conversational retriever using a next utterance retrieval task. Table 2 lists their characteristics.\n\nWe observe that pre-training on any of the WikiDialog variants significantly outperforms the two classic nonconversational QA datasets, MS Marco and PAQ. WikiDialog also outperforms open-domain dialog data PublicDi-alog+TaskMaster (PT). Despite being generated using PT, pre-training on WikiDialog PT far outperforms it, showing that the proposed dialog inpainting recipe provides additional signal not present in PT.\n\nIn addition, we found that pre-training a retriever on PT does surprisingly well, significantly outperforming MS Marco on all datasets. Prior ConvQA retrieval systems have focused on traditional QA data for pre-training, but we find that open-domain conversational data can be just as useful! Does pre-training on WikiDialog enable zero/few-shot learning? We now explore how much fine-tuning data is needed after pre-training on WikiDialog. Figure 4 (a) plots the retrieval results of a T5-Base retriever pre-trained on WikiDialog PT -which does not use any in-domain dataand finetuned on varying percentages of the QReCC training dataset 10 .\n\nSurprisingly, we observe that the zero-shot performance of the pre-trained retriever is already quite good: achieving nearly 95% the MRR of a model that uses the full dataset. Furthermore, pre-trained retriever needs fewer fine-tuning data -by fine-tuning on just 10% of the data, the pre-trained retriever exceeds the performance of the full-data baseline.\n\n10 Unlike the other experiments presented in this paper that uses mined hard negatives to fine-tune retrievers, these results do not use hard negatives because our hard negative mining model was fine-tuned on the whole QReCC dataset (Appendix C). Does our method scale with inpainting model size and data size? We now explore if our dialog inpainting method can benefit from scaling up along two dimensions: the inpainter model size, and the inpainted WikiDialog data size. Results are shown in Figure 4 (b) and (c).\n\nFrom Figure 4 (b), we observe that retriever performance increases with inpainter model size with one exception: the T5-XL model slightly outperforms T5-XXL; we hypothesize this is due to insufficient hyperparameter search for T5-XXL. Surprisingly, the quality of data generated by T5-Small is already sufficient to significantly outperform current state-of-the-art methods.\n\nIn Figure 4 (c), we evaluate how retrievers pre-trained with 10K-11M dialogs sampled from WikiDialog perform on QReCC. We observe a roughly log-linear relationship between performance and pre-training data size that has not yet plateaued: simply inpainting more passages may further increase retrieval performance.\n\n\nRelated Work\n\nConversational question answering retrieval. Several manually collected conversational question answering datasets have been proposed to address the scarcity of highquality training data (Choi et al., 2018;Reddy et al., 2019;Dinan et al., 2018;Saeidi et al., 2018;Dalton et al., 2019;Campos et al., 2020;Dalton et al., 2020;Qu et al., 2020;Feng et al., 2020;Anantha et al., 2021). However, because they are relatively small, existing retrieval systems all depend on rewriting queries to use with a non-conversational retrieval system (Yang et al., 2019;Dalton et al., 2020;Yu et al., 2020;2021;Wu et al., 2021). Query rewriting is a hard problem in itself (Vakulenko et al., 2020)-prior systems rely on proprietary search logs (Yu et al., 2020), reinforcement learning (Wu et al., 2021) or distillation recipes (Yu et al., 2021)-and some queries cannot be rewritten at all, e.g., \"What else were they famous for?\". Here, we use WikiDialog, a 1000x larger ConvQA dataset, to train a standard retriever without requiring query rewrites.\n\nConversational language models. Large conversational language models such as DialogGPT (Zhang et al., 2020) and Meena (Adiwardana et al., 2020) have shown impressive open-ended conversational capabilities, and even the ability to directly answer many questions. However, they often hallucinate answers and amplify unfair biases present in their training data. Subsequent work address this problem by instead posing queries to a non-conversational retriever and using its answers (Roller et al., 2021;Komeili et al., 2021;Nakano et al., 2021;Thoppilan et al., 2022;Elgohary et al., 2019). In this paper, we instead use a masked conversational language model to generate ConvQA data: the data can be readily audited for unfair biases and to train a conversational retrieval system.\n\nData augmentation and synthetic data generation. Data augmentation has been widely used to improve the performance of document retrieval and related tasks (Lee et al., 2019;Chang et al., 2020;Gao et al., 2021) by using training data more efficiently. However, these methods typically do not result in high-quality data that can be used outside of a training recipe. More recently, Ma et al. (2021) and Lewis et al. (2021) train special-purpose question generation models to synthetically generate millions of high-quality question answer pairs. We extend this line of work to generate millions information seeking dialogs, and show that even a general purpose inpainter model can generate high-quality data.\n\n\nDiscussion\n\nIn this paper, we have presented dialog inpainting, a novel approach to generating synthetic conversational data. We showed that it is possible to generate compelling information-seeking dialogs using only general-purpose data, suggesting applications to other conversational tasks. While synthetic data cannot entirely replace real data, it can help bootstrap interactive conversation systems and create a virtuous cycle wherein users find it natural to engage with and improve the system. We are particularly optimistic about applying the dialog inpainting data to (1) distillation, where the inpainted datasets serve as large-scale distillation sets, (2) end-to-end conversational question answering, and (3) zero-shot conversational QA, which is motivated by the zero-shot retrieval capabilities shown in this work.\n\nIt is important to be aware of the biases that generating data can introduce or amplify. We want to encourage good inductive biases that make conversations conversationale.g., use of anaphora or elision of context-and to introduce further control over the dialogs generated-e.g., persona or dialog acts. At the same time, we must interrogate the generated data and work towards minimizing instances of potentially sensitive language that may perpetuate unfair biases.\n\n\nReferences\n\nAbid, A., Farooqi, M., and Zou, J. Large language models associate muslims with violence. Nature Machine Intelligence, 3 (6) . Each dialog is mined from public forums and then scored and filtered using LaMDA's SSI and safety model. While the original dataset contains multi-speaker dialogs, we only use a subset consisting of two-speaker dialogs. Note that most of the dialogs in PublicDialog are short and have only two turns.\n\n2. TaskMaster 11 (Byrne et al., 2019) is a crowd-sourced dataset of task-oriented dialogs between two speakers. We only use the subset of dialogs related to movie ticket and restaurant reservations. While TaskMaster is significantly smaller than PublicDialog, its dialogs have many more turns, which helps mitigate turn bias. There are three datasets, Taskmaster-1, Taskmaster-2, and Taskmaster-3; we use Taskmaster-1. Also see https: //github.com/google-research-datasets/ Taskmaster. 12 The NaturalQuestions dataset does not originally contain question sequences, so the authors asked human annotators to come up with follow-up questions first.\n\n\nOR-QuAC\n\nFor each question sequence, annotators search a document corpus to find relevant passages and answers. Though the questions in QReCC overlap with those in OR-QuAC, their answers can be completely different.\n\n\nA.2. Training details\n\nUnless otherwise specified, all our dialog inpainters are initialized from T5-XXL (11B parameters) 13 and finetuned using 64 TPU v3 chips 14 with constant learning rate 0.01, dropout rate 0.1 and batch size 128. We trained the Inpaint PT and Inpaint PTOQ using 100k steps, and Inpaint OQ using 10k steps because its training set is significantly smaller.\n\n\nA.3. Inference details\n\nTo generate the inpainted datasets, we used https://beam.apache.org to parallelize our computation. On average, it took from 20ms (for T5-Small) to -141ms (for T5-XXL) to inpaint each utterance in a dialog, and required between 100 TPUv3-hours (for T5-Small) and 1900 TPUv3-hours (for T5-XXL) to inpaint each WikiDialog variant, and would cost between $240 and $4560 using preemptible TPUs on Google Cloud.\n\n\nB. Human Evaluation Protocol\n\nIn this section we overview the human evaluation protocol used to evaluate WikiDialog variants and OR-QuAC in Section 3; the results of this evaluation were presented in Table 1.\n\nTask design and iteration. Figure 5 provides a screenshot of the annotation interface and a description of its features. The task was established as follows:\n\nIn this task, you will be spotting nonsensical or factually incorrect messages in artificially generated information-seeking conversations between a user and a system.\n\nIn each turn, raters were asked to answer the subjective questions in Table 1 and were provided the instructions and adjoining examples in Figures 6-10. We iterated on our task design and instructions over two pilot annotations runs. Feedback from these pilots led to two main changes: (i) we extended the specificity and answer adequacy questions to be Likert scales rather than yes/no questions and (ii) we omitted a laborious question that asked 13 We use t5.1.1 checkpoints from https: //github.com/google-research/ text-to-text-transfer-transformer/blob/ main/released_checkpoints.md 14 https://cloud.google.com/tpu/ Figure 5. A screenshot of the annotation interface. On the right, raters see the conversation history and the highlight turn they must rate in the form on the left. When all questions for a given turn are completed, they are allowed to move forward to the next turn (round in the figure). The task can be submitted when all turns are complete.\n\nDialog Inpainting: Turning Documents into Dialogs raters to attest whether the passage supports the answer when interpreted in the context of the conversation historywe found that this perfectly correlated with whether or not the question was adequately answered or not.\n\nRater recruitment and training. We engaged with a vendor supplier of full-time crowd workers to recruit human annotators for our task. Raters were asked to review the above instructions and were provided direct feedback on their responses during the pilot annotation runs.\n\n\nC. Retrieval models C.1. Dual encoder retriever\n\nThe dual-encoder maps a query (q) and a passage (p) into dense embedding vectors embed \u03b3 (q) and embed \u03b3 (p), where \u03b3 denotes model parameters. The relevance score between the two is their vector cosine similarity:\ns \u03b3 (q, p) = embed \u03b3 (q) embed \u03b3 (p) embed \u03b3 (q) \u00b7 embed \u03b3 (p)\nThis particular function enables one to retrieve the top-K highest-scoring passages for a given query using fast similarity search methods that run in sub-linear time (Ram & Gray, 2012).\n\nFor training, we minimize a standard contrastive loss with temperature \u03c4 :\nL(\u03b3) = \u2212log exp(s \u03b3 (q, p * )/\u03c4 ) p\u2208p * \u222aN (q) exp(s \u03b3 (q, p)/\u03c4 ) ,(2)\nwhere p * is a positive passage for q and N (q) denotes negative passages.\n\nWe implement the dual-encoder retriever following recent work (Ni et al., 2021): in particular, we use a shared Transformer encoder initialized from a T5 checkpoint, take the mean pooling of the top-most encoder layer, and project it to a fixed 768-dimensional embedding.\n\n\nC.2. Reranker\n\nThe reranker model takes the same inputs as the dual encoder, but instead of encoding q and p into two separate vectors, it jointly encodes them into a single vector, embed \u03c8 (q, p), where \u03c8 denotes model parameters. It outputs a relevance score:\ns \u03c8 (q, p) = w embed \u03c8 (q, p)\nwhere w is also a model parameter. Unlike the dual encoder, this function does not support fast top-K retrieval in sublinear time. Therefore, for computational tractability, we only use it to rerank a short-list of candidates retrieved by the dual encoder. However, the joint embedding of q and p permits a more expressive relevance function (e.g. crossattention between q and p), so it can improve over the dual encoder's relevance scores.  Figure 11. Our pipeline for training conversational retrieval system. It follows the standard multi-stage training scheme used in previous work (Lin et al., 2021). We first train an initial retriever with in-batch negatives. We then train a second retriever on hard negatives from the initial retriever. Finally, we train a reranker on hard negatives from Retriever. Performance is further improved by pre-training Retriever and Reranker on document derived-dialogs.\n\nFor training, we minimize a weighted binary classification loss:\nL(\u03c8) = \u2212 log \u03c3(s \u03c8 (q, p * )) \u2212 N (q) \u22121 p\u2208N (q) log[1 \u2212 \u03c3(s \u03c8 (q, p))],\nwhere \u03c3 denotes the sigmoid function.\n\nSimilar to our retriever, the reranker is also initialized from a T5 encoder. Our reranker implementation follows the implementation described in Nogueira et al. (2020).\n\n\nC.3. Model Training\n\nA training example from a typical retrieval dataset consists of a query paired with a positive passage, (q, p * ). However, negative passages N (q) are usually not provided. Hence, we need to generate our own. We use two types of commonly used negatives: in-batch negatives , and \"hard\" negatives mined with a multi-stage training strategy.\n\nIn-batch negatives. When training with a batch, we treat the positive passage for example i as a negative for all other examples = i in the same batch.\n\n\"Hard\" negatives. Previous work has identified several weakness of in-batch negatives and identified the importance of selecting challenging negative examples for training retrievers (Qu et al., 2021;Xiong et al., 2021;Santhanam et al., 2021;Lin et al., 2021). Hence, following prior work (Lin et al., 2021), we adopt a multi-stage training strategy where new negatives are mined at each stage ( Figure 11):\n\n1. We train an initial retriever using only positives and in-batch negatives.\n\n2. We run top-K retrieval using the initial retriever, and then randomly sample a subset of those to serve as negatives (when K is large, a high percentage of these are true negatives). The hard negatives are then combined with positives to train a second retriever. 3. We use the second retriever to again generate hard negatives. We train our reranker on these hard negatives combined with any positives retrieved by the second retriever.\n\nWhen pre-training on WikiDialog, we use in-batch negatives. When fine-tuning for a downstream task, we use the multistage hard negative strategy.\n\nNote that at inference time, we use the second retriever for top-K retrieval, and use the reranker to refine the top-K ranking (the initial retriever is not used at inference time).  Note, QReCC reuses question sequences from QuAC and TREC CAsT-19 (Byrne et al., 2019). However, although the questions are the same as existing datasets, the relevant passages and answers are different, as it asked human raters to retrieve passages with a search engine and generate answers. In addition, TREC questions only appear in the test set.\n\n\nD. Detailed Experimental Setup\n\nIn OR-QuAC, at least 99.5% of dialogs contain answers from the same answer passage, while in CAsT and QReCC, each question turn can be answered by a different passage.\n\nWe follow the official automatic setting (Byrne et al., 2019;Dalton et al., 2020) for both datasets, which only uses the questions as retrieval inputs. We follow the authors of QReCC (Anantha et al., 2021) and (Wu et al., 2021) and use the both questions and gold answers from conversation history as retrieval inputs.\n\n\nD.2. Implementation\n\nWe implement dual-encoder retrievers and rerankers in JAX.\n\nRetrievers. For pre-training on our inpainted datasets, we used a softmax temperature \u03c4 of 0.01, batch size 2048, and dropout rate 0.1. The models were trained with Adafactor optimizer with learning rate 1e \u22123 and 1k warm up steps. For checkpoint selection, we tested checkpoints at 50k and 100k steps and reported the better one based on each finetune datasets' dev set performance. we fine-tuned the retrievers for 500 steps on OR-QuAC and QReCC. We did not finetune the retrievers on TREC CAsT-19 and CAsT-20 due to the small data size. We report TREC performance by retrieving with a QReCC retriever (trained without answers in the inputs).\n\nQuestions and passages are always lowercased. Maximum query length was set to 128 for all pretrain and fine-tune datasets except for QReCC, which uses 512 because it allows using previous answers in the queries. Maximum passage length was set to 256.\n\nRerankers. Rerankers were implemented using T5 encoderdecoder architecture. We pre-trained the rerankers on synthetic dialogs for 1.7M steps. For fine-tuning on OR-QuAC and QReCC, we used 10 hard negatives for each question sampled from the top 100 passages returned by the retriever. Fir TREC CaST-19 and Cast-20, we follow prior work (Yu et al., 2021) and used 5-fold cross-validation to fine-tune the reranker 15 . We used 20 hard negatives per question randomly sampled from the top 100 retrieved results. We warm up the TREC rerankers on QReCC.\n\nAll reranker training used the Adafactor optimizer with constant learning rate 1e \u22123 and dropout rate 0.1. We used batch size 512 for pre-training, 128 for fine-tuning of OR-QuAC and QReCC, and 32 for fine-tuning of TREC CaST-19 and CaST-20.\n\n\nD.3. Published baselines\n\nWe include five published retreival-only baselines to compare with our DI retrievers.\n\nBM25-Query Rewriter (Yu et al., 2021) and BM25-T5QR (Wu et al., 2021) are two query rewriting approaches that trains a model to rewrite the dialog history into a contextualized, keyword-like query. The former trains a GPT-2 query rewriter on ad hoc search sessions (Yu et al., 2020). The latter trains a T5 query rewriter on human-generated query rewrites from QReCC. Both systems issue the rewritten query to a classic BM25 lexical retrieval system. CONQRR (Wu et al., 2021) is the previous state-of-the-art retrieval system on QReCC. CONQRR uses reinforcement learning to optimize the query rewriter for the retriever.Its retriever is a t5-base dual-encoder trained on MS Marco.\n\nConvDR (Yu et al., 2021) is the previous state-of-the-art conversational dense retrieval system on OR-QuAC and TREC CaST tasks. Unlike the above approaches which all use query rewriting, ConvDR learns a conversational query encoder that directly maps the entire dialog history into a dense embedding. To address the bottleneck of limited conversational training data, ConvDR uses a teacher-student framework that trains the student conversational query encoder to \"mimic\" the representation of the oracle query rewrite from a non-conversational teacher.\n\nIn addition to the retreval baselines, we include three published state-of-the-art conversational search systems that uses the retrieval+reranker pipeline, serving as baselines for our DI retriever + reranker system.  Table 6 reports additional retrieval metrics in addition to the MRR reported in Table 3. Specifically, we report recall and mean reciprocal rank at rank 5 (R@5 and MRR@5) following previous work Qu et al. (2020); Yu et al. (2021). On QReCC, we report recall at rank 10 (R@10) and mean reciprocal rank without rank cut off (MRR) following Anantha et al. (2021) Tables 7-10 show additional examples of inpainted dialogs.\n\n\nE. Additional Experimental Results\n\n\nF. Inpainted Dialog Examples\n\n\nG. Studying Sensitive Language in the Dataset\n\nPrior to evaluating the dataset for fairness implications, we found it necessary to define \"fairness\" in the context of the model's generated queries. We began our inquiry by considering conversational fairness -that is, what constitutes insensitive, offensive, or prejudiced speech in a dialogue? We then qualitatively assessed a small sample of the dataset and described the following potential fairness related concerns that may arise in the dataset:\n\n\u2022 Model generates a query that contains sensitive characteristics that were not explicitly mentioned in the passage.\n\n\u2022 Model overemphasizes specific sensitive characteristics or identities in a negative or offensive way. For example: the model continues to generate questions related to a writer's biracial identity when it is not the intended focus of the passage.\n\n\u2022 Model ascribes negative values to sensitive characteristics without that being explicitly mentioned in the passage. For example, the model generates a question about \"how barbaric\" a society is for a passage about a non-Western society.\n\n\u2022 Model produces \"unhelpful\" or repetitive questions when the source passage contains sensitive characteristics as compared to passages with non-sensitive characteristics.\n\nThe primary approach to evaluating fairness in the dataset involved scanning for sensitive terms and interactions between those terms. To this end, we curated a list of sensitive terms from the literature that includes nouns and adjectives that are considered harmful or insensitive when applied to particular groups (citation). When a combination of these terms appeared  The format is often used in articles, websites, email lists, and online forums where common questions tend to recur, for example through posts or queries by new users related to common knowledge gaps. 1: Is there anything else important? 0: The purpose of an FAQ is generally to provide information on frequent questions or concerns; however, the format is a useful means of organizing information, and text consisting of questions and their answers may thus be called an FAQ regardless of whether the questions are actually \"frequently\" asked. 1: Any other information? 0: Since the acronym \"FAQ\" originated in textual media, its pronunciation varies. 1: How does it vary? 0: FAQ is most commonly pronounced as an initialism, \"F-A-Q\", but may also be pronounced as an acronym, \"FAQ\".\n\nInpaint OQ 1: What is meant by a \"Frequently asked question\"? 0: An FAQ is a list of frequently asked questions (FAQs) and answers on a particular topic (also known as Questions and Answers [Q&A] or Frequently Answered Questions). 1: Who asks the questions? 0: The format is often used in articles, websites, email lists, and online forums where common questions tend to recur, for example through posts or queries by new users related to common knowledge gaps. 1: Is the information intended for a specific audience? 0: The purpose of an FAQ is generally to provide information on frequent questions or concerns; however, the format is a useful means of organizing information, and text consisting of questions and their answers may thus be called an FAQ regardless of whether the questions are actually \"frequently\" asked. 1: How is the term \"FAQ\" pronounced? 0: Since the acronym \"FAQ\" originated in textual media, its pronunciation varies. 1: What does it vary in pronunciation? 0: FAQ is most commonly pronounced as an initialism, \"F-A-Q\", but may also be pronounced as an acronym, \"FAQ\".\n\nInpaint PTOQ 1: what is the purpose of a faq 0: An FAQ is a list of frequently asked questions (FAQs) and answers on a particular topic (also known as Questions and Answers [Q&A] or Frequently Answered Questions). 1: Are these often used on a website? 0: The format is often used in articles, websites, email lists, and online forums where common questions tend to recur, for example through posts or queries by new users related to common knowledge gaps. 1: What questions are asked most frequently? 0: The purpose of an FAQ is generally to provide information on frequent questions or concerns; however, the format is a useful means of organizing information, and text consisting of questions and their answers may thus be called an FAQ regardless of whether the questions are actually \"frequently\" asked. 1: Is FAQ pronounced as an acronym? 0: Since the acronym \"FAQ\" originated in textual media, its pronunciation varies. 1: How is it pronounced? 0: FAQ is most commonly pronounced as an initialism, \"F-A-Q\", but may also be pronounced as an acronym, \"FAQ\". Table 9. Synthetic dialogs about FAQ generated by Inpaint PT ,Inpaint OQ , and Inpaint PTOQ .   One of the forms of this is discrimination based on religion; often times, Indians, specifically Sikhs, in America face racism that that is a result of mislabeling like being called Middle-Eastern or Muslim by the way they look, whether they actually identify with those groups, with Muslim Americans being markedly poorer than Hindu Americans. 1: And how does this impact other Asian Americans? 0: The mass grouping of all different Asian ethnicities into one \"positive\" stereotype delegitimizes the problems that certain Asian immigrants face, because the majority might not face those same negative stereotypes. 1: What can the Sikhs do to help other Asian immigrants? 0: Another result of the Model Minority status of Indian-Asians is their complacency, expressed more often than not. 1: How does this affect other Asian immigrants? 0: There is conflict within the Indian-Asian immigrant population in the US, as some exalt their \"positive\" minority status and wish to uphold it, resulting in complacency when attacked racially and even sometimes going as far as to support political candidates that are anti-immigration. Table 12. This dialog was flagged as potentially sensitive because it contained sensitive terms \"african\" and \"hispanic\".\n\n\nDialog Inpainting: Turning Documents into Dialogs\n\nArticle: The Queers 1: What was the band The Queers' response to the November 2015 Woolsey Fire disaster? 0: In an interview with Noisey, after their February 2016 show was cancelled due to a call for a petition to boycott the show from a local collective, Babely Shades, Joe Queer voiced his opinion when asked if he \"supports racist, misogynistic or homophobic values?\" he replied \"Of course not.\" 1: Did Joe Queer think that the Woolsey Fire was man-made? 0: \"What normal minded person does?\" 1: Did Joe Queer have any black friends? 0: \"My closest pals here in Atlanta are Chris and Chester, two gay black guys.\" 1: Was Joe Queer in school when the Woolsey Fire happened? 0: \"I was actually working a part time job.\" 1: Was Joe Queer working with any black people at the time? 0: \"I was one of two white people on the whole job.\" 1: Did Joe Queer think that African Americans were lazy and lacked work ethic? 0: \"If I was so racist, I wouldn't have worked there or be living in Atlanta!\" Table 13. This dialog was flagged as potentially sensitive because it contains sensitive terms \"african\" and \"lazy\".\n\nArticle: Female intrasexual competition 1: How does a woman's ovulation cycle affect the way she competes with other women? 0: This indirect form of competition appears exclusive toward females as findings also showed that women, irrespective of ovulation status (high or low), showed no difference in the rating of male faces. 1: What other behaviors or factors affect the way women compete with each other? 0: Supporting research has also found that younger women who are considered as having high fertility, gossip about other women more than older women, who are no longer at their most fertile stage. 1: Do women tend to be more aggressive toward each other than men? 0: Indeed, indirect aggression appears more prevalent amongst (or exclusive to) females than males who are said to engage in more direct forms of competition. 1: Are there any other interesting aspects about this article? 0: Research studying the relationship between indicators of attractiveness, such as physical attractiveness and indirect victimisation, showed that the likelihood of experiencing indirect victimization increased by 35% for females who perceived themselves as physically attractive. 1: Is this something that also happens with men? 0: In contrast, being a male who is physically attractive decreased the chances of experiencing such indirect victimization. 1: Why is this difference in behavior between men and women important? 0: This also highlights how the physical attractiveness a female is a trigger for indirect aggression and forms a core part of intersexual selection between the sexes. Table 14. The dialog is potentially sensitive due to containing sensitive terms \"aggressive\" and \"men\". 1: what were her parents like? 0: Alexander's mother died when she was 4 years old, and at age 13 her father's riding academy closed. 1: what did she do after her parents' academy closed? 0: Alexander withdrew from school to help relieve the resulting economic strain on her family, but her father insisted that she finish her education. 1: where did she go to school? 0: Alexander attended high school at the William Penn High School for Girls, where she graduated with honors before receiving a scholarship that allowed her to attend the University of Pennsylvania to complete her undergraduate education. Table 15. This dialog is flagged as potentially sensitive because it contains the sensitive terms \"African-American\" and \"doctor\". The first answer is not factually accurate-Virginia M. Alexander is not the first African-American physician.\n\nArticle: Symbolism of terrorism 1: \"Allah is most gracious to the oppressed (Quran passage)\" is mentioned in the analysis of a terrorist attack. Is this quote from the Quran a part of the analysis? 0: The analysis does not discuss the quote on the Quran. 1: What is the image in the analysis? 0: The central figure of the image is a yellow tiger, which is surrounded by a ring of bullets and crossed by two rifles. 1: What is the significance of the image? 0: The script both around the tiger, in Tamil, and below the tiger, in English, reads \"Liberation Tigers of Tamil Eelam.\" Eelam is Tamil for the Island of Sri Lanka, which the groups seeks to liberate from the control of the Sinhalese majority. 1: What is the significance of the tiger? 0: The tiger is a culturally important animal that symbolizes heroism, militancy, and patriotism. 1: What is the significance of the tiger in this image? 0: Its centrality in this image indicates the desire of the Liberation Tigers to be self-determined and independent. 1: What is the significance of the bullets and rifles? 0: The bullets and rifles indicate their employment of violence as the means to achieve these goals. Table 16. This dialog is flagged as potentially sensitive because it contains the sensitive terms \"allah\" and \"attack\".\n\nFigure 1 .\n1A real example of a dialog inferred from a Wikipedia passage using dialog inpainting. Highlighted utterances are original sentences from the article. All other utterances are generated by the dialog inpainter.\n\nFigure 3\n3Figure 3. We use a two-stage ConvQA retrieval system. We first retrieve top-K passages from the corpus using a dual-encoder model and then rerank them using a cross-attention model.\n\n\nDatasets.We use three open-domain conversational QA retrieval benchmarks: OR-QuAC (Qu et al., 2020),QReCC (Anantha et al., 2021), and TREC CAsT19 and CAsT20(Dalton et al., 2019; 2020).\n\nFigure 4 .\n4Retriever performance on QReCC when T5-Base DE WikiDialog PT is trained with (a) varying fine-tuning data sizes, (b) different sizes inpainter models, and (c) varying pre-training data sizes. Results in (a) do not include mined hard-negatives.\n\n\nD., Luong, M.-T., So, D. R., Hall, J., Fiedel, N., Thoppilan, R., Yang, Z., Kulshreshtha, A., Nemade, G., Lu, Y., et al. Towards a human-like open-domain chatbot. arXiv preprint arXiv:2001.09977, 2020. Anantha, R., Vakulenko, S., Tu, Z., Longpre, S., Pulman, S., and Chappidi, S. Open-domain question answering goes conversational via question rewriting. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT. Association for Computational Linguistics, 2021. Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., and Kalai, A. T. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29, 2016. Byrne, B., Krishnamoorthi, K., Sankar, C., Neelakantan, A., Goodrich, B., Duckworth, D., Yavuz, S., Dubey, A., Kim, K., and Cedilnik, A. Taskmaster-1: Toward a realistic and diverse dialog dataset. In Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP. Association for Computational Linguistics, 2019.Campos, J. A., Otegi, A., Soroa, A., Deriu, J. M., Cieliebak, M., and Agirre, E. Doqa-accessing domain-specific faqs via conversational qa. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020. Chang, W., Yu, F. X., Chang, Y., Yang, Y., and Kumar, S. Pre-training tasks for embedding-based large-scale retrieval. In 8th International Conference on Learning Representations, ICLR, 2020. Choi, E., He, H., Iyyer, M., Yatskar, M., Yih, W., Choi, Y., Liang, P., and Zettlemoyer, L. Quac: Question answering in context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP-IJCNLP. Association for Computational Linguistics, 2018. Culpepper, J. S., Diaz, F., and Smucker, M. D. Research frontiers in information retrieval: Report from the third strategic workshop on information retrieval in lorne (SWIRL). SIGIR Forum, 2018. Dalton, J., Xiong, C., and Callan, J. TREC cast 2019: The conversational assistance track overview. In Proceedings of the Twenty-Ninth Text REtrieval Conference, TREC, NIST Special Publication. National Institute of Standards and Technology (NIST), 2019. Dalton, J., Xiong, C., and Callan, J. Cast 2020: The conversational assistance track overview. In Proceedings of the Twenty-Ninth Text REtrieval Conference, TREC, NIST Special Publication. National Institute of Standards and Technology (NIST), 2020. Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. In Burstein, J., Doran, C., and Solorio, T. (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT. Association for Computational Linguistics, 2019. Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston, J. Wizard of wikipedia: Knowledge-powered conversational agents. arXiv preprint arXiv:1811.01241, 2018. Dusart, A., Hubert, G., and Pinel-Sauvagnat, K. IRIT at TREC 2019: Incident streams and complex answer retrieval tracks. In Proceedings of the Twenty-Eighth Text REtrieval Conference, TREC, NIST Special Publication. National Institute of Standards and Technology (NIST), 2019. Elgohary, A., Peskov, D., and Boyd-Graber, J. L. Can you unpack that? learning to rewrite questions-in-context. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP. Association for Computational Linguistics, 2019.Feng, S., Wan, H., Gunasekara, C., Patel, S., Joshi, S., and Lastras, L. Doc2dial: A goal-oriented documentgrounded dialogue dataset. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. Gao, T., Yao, X., and Chen, D. Simcse: Simple contrastive learning of sentence embeddings. In Moens, M., Huang, X., Specia, L., and Yih, S. W. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP. Association for Computational Linguistics, 2021. Garg, N., Schiebinger, L., Jurafsky, D., and Zou, J. Word embeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences, 115(16):E3635-E3644, 2018. Iizuka, S., Simo-Serra, E., and Ishikawa, H. Globally and locally consistent image completion. ACM Trans. Graph., V., Oguz, B., Min, S., Lewis, P. S. H., Wu, L., Edunov, S., Chen, D., and Yih, W. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP. Association for Computational Linguistics, 2020. Komeili, M., Shuster, K., and Weston, J. Internet-augmented dialogue generation. arXiv preprint arXiv:2107.07566, 2021. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A. P., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S. Natural questions: a benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 2019. Lee, K., Chang, M., and Toutanova, K. Latent retrieval for weakly supervised open domain question answering. In Korhonen, A., Traum, D. R., and M\u00e0rquez, L. (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL. Association for Computational Linguistics, 2019. Lewis, P., Wu, Y., Liu, L., Minervini, P., K\u00fcttler, H., Piktus, A., Stenetorp, P., and Riedel, S. PAQ: 65 million probablyasked questions and what you can do with them. arXiv preprint arXiv:2102.07033, 2021. Li, H., Gao, T., Goenka, M., and Chen, D. Ditch the gold standard: Re-evaluating conversational question answering. arXiv preprint arXiv:2112.08812, 2021. Lin, S.-C., Yang, J.-H., and Lin, J. In-batch negatives for knowledge distillation with tightly-coupled teachers for dense retrieval. In Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP), Online, August 2021. Association for Computational Linguistics. Liu, G., Reda, F. A., Shih, K. J., Wang, T., Tao, A., and Catanzaro, B. Image inpainting for irregular holes using partial convolutions. In Ferrari, V., Hebert, M., Sminchisescu, C., and Weiss, Y. (eds.), Computer Vision -ECCV 2018 -15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XI, Lecture Notes in Computer Science. Springer, 2018. Ma, J., Korotkov, I., Yang, Y., Hall, K. B., and McDonald, R. T. Zero-shot neural passage retrieval via domaintargeted synthetic question generation. In Merlo, P., Tiedemann, J., and Tsarfaty, R. (eds.), Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL. Association for Computational Linguistics, 2021. May, C., Wang, A., Bordia, S., Bowman, S., and Rudinger, R. On measuring social biases in sentence encoders. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL, 2019. Nadeem, M., Bethke, A., and Reddy, S. Stereoset: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456, 2020. Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al. WebGPT: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., and Deng, L. MS MARCO: A human generated machine reading comprehension dataset. In Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 colocated with the 30th Annual Conference on Neural Information Processing Systems, NIPS, 2016. Ni, J.,\u00c1brego, G. H., Constant, N., Ma, J., Hall, K. B., Cer, D., and Yang, Y. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. arXiv preprint arXiv:2108.08877, 2021. Nogueira, R. and Cho, K. Passage re-ranking with bert. arXiv preprint arXiv:1901.04085, 2019. Nogueira, R., Jiang, Z., Pradeep, R., and Lin, J. Document ranking with a pretrained sequence-to-sequence model. In Findings of the Association for Computational Linguistics: EMNLP. Association for Computational Linguistics, 2020. Qu, C., Yang, L., Chen, C., Qiu, M., Croft, W. B., and Iyyer, M. Open-retrieval conversational question answering. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR. ACM, 2020. Qu, Y., Ding, Y., Liu, J., Liu, K., Ren, R., Zhao, W. X., Dong, D., Wu, H., and Wang, H. RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering. In Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-T\u00fcr, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., and Zhou, Y. (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT. Association for Computational Linguistics, 2021. Radlinski, F. and Craswell, N. A theoretical framework for conversational search. In Proceedings of the 2017 conference on conference human information interaction and retrieval, 2017. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 2020. Ram, P. and Gray, A. G. Maximum inner-product search using cone trees. In Yang, Q., Agarwal, D., and Pei, J. (eds.), The 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '12, Beijing, China, August 12-16, 2012, pp. 931-939. ACM, 2012. Reddy, S., Chen, D., and Manning, C. D. Coqa: A conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249-266, 2019. Reimers, N. and Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. In Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP. Association for Computational Linguistics, 2019. Roller, S., Dinan, E., Goyal, N., Ju, D., Williamson, M., Liu, Y., Xu, J., Ott, M., Smith, E. M., Boureau, Y.-L., et al. Recipes for building an open-domain chatbot. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, 2021. Saeidi, M., Bartolo, M., Lewis, P., Singh, S., Rockt\u00e4schel, T., Sheldon, M., Bouchard, G., and Riedel, S. Interpretation of natural language rules in conversational machine reading. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2087-2097, 2018. Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C., and Zaharia, M. Colbertv2: Effective and efficient retrieval via lightweight late interaction. arXiv preprint arXiv:2112.01488, 2021. Stede, M. and Schlangen, D. Information-seeking chat: Dialogues driven by topic-structure. In Proceedings of Catalog (the 8th workshop on the semantics and pragmatics of dialogue; SemDial04). Citeseer, 2004. Thoppilan, R., Freitas, D. D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Lee, H., Zheng, H. S., Ghafouri, A., Menegali, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J., Chen, D., Xu, Y., Chen, Z., Roberts, A., Bosma, M., Zhou, Y., Chang, C.-C., Krivokon, I., Rusch, W., Pickett, M., Meier-Hellstern, K., Morris, M. R., Doshi, T., Santos, R. D., Duke, T., Soraker, J., Zevenbergen, B., Prabhakaran, V., Diaz, M., Hutchinson, B., Olson, K., Molina, A., Hoffman-John, E., Lee, J., Aroyo, L., Rajakumar, R., Butryna, A., Lamm, M., Kuzmina, V., Fenton, J., Cohen, A., Bernstein, R., Kurzweil, R., Aguera-Arcas, B., Cui, C., Croak, M., Chi, E., and Le, Q. Lamda: Language models for dialog applications, 2022. Vakulenko, S., Longpre, S., Tu, Z., and Anantha, R. A wrong answer or a wrong question? an intricate relationship between question reformulation and answer selection in conversational question answering. In Proceedings of the 5th International Workshop on Search-Oriented Conversational AI (SCAI), 2020. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. In Advances in neural information processing systems, pp. 5998-6008, 2017. Wu, Z., Luan, Y., Rashkin, H., Reitter, D., and Tomar, G. S. CONQRR: Conversational query rewriting for retrieval with reinforcement learning. arXiv preprint arXiv:2112.08558, 2021. Xiong, L., Xiong, C., Li, Y., Tang, K., Liu, J., Bennett, P. N., Ahmed, J., and Overwijk, A. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In 9th International Conference on Learning Representations, ICLR, 2021. Yang, J., Lin, S., Wang, C., Lin, J., and Tsai, M. Query and answer expansion from conversation history. In Voorhees, E. M. and Ellis, A. (eds.), Proceedings of the Twenty-Eighth Text REtrieval Conference, NIST Special Publication. National Institute of Standards and Technology (NIST), 2019. Yu, J., Lin, Z., Yang, J., Shen, X., Lu, X., and Huang, T. S. Generative image inpainting with contextual attention. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR. Computer Vision Foundation / IEEE Computer Society, 2018. Yu, S., Liu, J., Yang, J., Xiong, C., Bennett, P. N., Gao, J., and Liu, Z. Few-shot generative conversational query rewriting. In Huang, J., Chang, Y., Cheng, X., Kamps, J., Murdock, V., Wen, J., and Liu, Y. (eds.), Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR. ACM, 2020. Yu, S., Liu, Z., Xiong, C., Feng, T., and Liu, Z. Few-shot conversational dense retrieval. In SIGIR '21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2021. Zhang, Y., Sun, S., Galley, M., Chen, Y., Brockett, C., Gao, X., Gao, J., Liu, J., and Dolan, B. DIALOGPT : Largescale generative pre-training for conversational response generation. In Celikyilmaz, A. and Wen, T. (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL. Association for Computational Linguistics, 2020. Zhu, F., Lei, W., Wang, C., Zheng, J., Poria, S., and Chua, T.-S. Retrieving and reading: A comprehensive survey on open-domain question answering. arXiv preprint arXiv:2101.00774, 2021. A. Inpainting Models A.1. Datasets for dialog inpainter training 1. PublicDialog is an open-domain dialog dataset proposed by LaMDA (Thoppilan et al., 2022)\n\n\n(Qu et al., 2020) is a version of the conversational question answering dataset QuAC (Choi et al., 2018) that consists of real human-human QA dialogs about a given Wikipedia article. OR-QuAC extends QuAC to the open-domain retrieval setting by making the first question context-independent and pairing each answer with a relevant passage from Wikipedia. 4. QReCC (Anantha et al., 2021) is another conversational question answering dataset constructed using question sequences from QuAC, TREC Conversational Assistant Track, and NaturalQuestions (Choi et al., 2018; Dalton et al., 2019; Kwiatkowski et al., 2019). 12 11\n\nFigure 6 .Figure 7 .Figure 8 .Figure 9 .Figure 10 .\n678910Instructions and example provided for the question \"Is the query information-seeking?\" Instructions and example provided for the question \"How is the query relevant to the conversation?\" Instructions and example provided for the question \"How specific is the query?\" Instructions and example provided for the question \"How well does the response answer the query?\" An additional example for the question \"How well does the response answer the query?\"\n\nD. 1 .\n1Datasets We use four open-domain conversational QA retrieval benchmarks: OR-QuAC (Qu et al., 2020), TREC CAsT-19 (Byrne et al., 2019), TREC CAsT-20 (Dalton et al., 2020), and QReCC (Anantha et al., 2021).\n\n\nANCE-Query Rewriter(Yu et al., 2021)  uses the GPT-2 query rewriter from(Yu et al., 2021), but it runs the rewritten query withANCE (Xiong et al., 2021), is a strong nonconversational dense retriever trained on MS Marco.\n\n\nTable 1. Results from a human evaluation of the generated utterances in three WikiDialog variants vs. a dataset manually collected on the same passages, OR-QuAC. The table also presents inter-annotator agreement using Krippendorff's \u03b1. Underlined numbers statistically differ from corresponding OR-QuAC ones at a p < 0.05 level based on a paired randomization test.Question (\u03b1) \nWikiDialog \nOR-\n\nAnswer \nPT \nOQ PTOQ QuAC \n\nIs the question information seeking? (0.98) \nYes \n94.5% 100% 99.3% \n100% \nHow relevant is question to the conversation? (0.94) \nNot at all \n0.3% \n0% \n0% \n0% \nTopic only \n45.8% 49.5% 42.1% 52.6% \nFollows up \n53.9% 50.5% 57.9% 47.4% \nHow specific is the question? (0.91) \nNot at all \n6 \n12% \n5.8% \n5.4% \nSomewhat \n15% 28.7% 22.4% \n12% \nVery \n79% 59.3% 71.7% 82.6% \nHow well answered is the question? (0.89) \nNot at all \n0.1% \n0.1% \n0.0% \n0.8% \nIncompletely 15.6% 19.7% 25.4% 22.8% \nSufficiently \n52.4% 46.8% 50.2% 36.6% \nPerfectly \n31.9% 33.4% 24.4% 39.8% \n\n\n\nTable 2\n2summarizes basic statistics of the each dataset; further details are provided in Appendix D.3. corpora. We use standard retrieval corpora andRetrieval \n\n\n). OR-QuAC, and TREC CAsT. WikiD, WikiD+WebD: We use WikiDialog PTOQ for all tasks except when WikiD+WebD is used in TREC CAsT, where WikiDialog PT gives higher performance. Additional metrics are reported in Appendix E.TREC CAsT \nQReCC OR-QuAC \n19 \n20 \n\nSystem \nMRR \nMRR@5 MRR MRR \n\nRetrieval \nBM25-QR \n-\n20.2 \n58.1 \n25.0 \nANCE-QR \n-\n45.7 \n66.5 \n37.5 \nConvDR \n-\n61.6 \n74.0 \n50.1 \nBM25-T5QR \n32.8 \n-\n-\n-\nCONQRR \n41.8 \n-\n-\n-\n\nT5-Large DE \n55.7 \n56.9 \n61.0 \n34.3 \nWikiD \n60.4 \n66.5 \n68.1 \n43.7 \nWikiD+WebD \n60.7 \n68.7 \n74.1 \n51.3 \n\nRetrieval + Reranking \nCFDA CLIP RUN7 \n-\n-\n71.4 \n-\nh2oloo RUN4 \n-\n-\n-\n59.3 \nConvDR\u2192BERT \n-\n77.3 \n79.9 \n54.5 \n\nT5-Large DE \n\u2192 reranker \n68.9 \n72.6 \n75.3 \n55.1 \nWikiD \n70.7 \n79.7 \n79.3 \n60.3 \nWikiD+WebD \n71.8 \n81.2 \n82.0 \n59.7 \n\nTable 3. We evaluate models trained using our inpainted datasets \nagainst baselines on three different ConvQA retrieval tasks: \nQReCC, \n\nTable 4\n4also reports results for re-\ntrievers pre-trained on several alternatives to WikiDialog: \nMS Marco (Nguyen et al., 2016), a search dataset that \nis widely used for retriever pre-training; PAQ (Lewis \n\n\n\nTable 5\n5lists their statistics.OR-QuAC and QReCC were introduced in Appendix A.1. \n\nTREC CAsT-19 and CAsT-20 are two datasets from the \nThe TREC Conversational Assistance Track (CAsT) shared \ntask (Dalton et al., 2019; 2020) with small numbers of di-\nalogues for evaluating information-seeking conversational \nsearch systems. Questions in a dialogue are constructed \nmanually to mimic a \"real\" dialogue on a certain topic. \nThe retrieval corpus includes web passages from MS \nMARCO (Nguyen et al., 2016) and wikipedia passages from \nTREC Complex Answer Retrieval (CAR) (Dusart et al., \n2019). CAsT-19 provides human relevance labels for 173 \nquestions in 20 test dialogues. CAsT-20 provides human \nrelevance labels for 208 questions in 25 test dialogues. \n\n\n\n\nTable 6. Full evaluation on QReCC, OR-QuAC and TREC CAsT. This table reports additional metrics for these datasets following prior work(Yu et al., 2021; Wu et al., 2021)  in addition to MRR reported inTable 3.; \nWu et al. (2021). On TREC CAsT-19 and CAsT-20, we use the official metrics MRR and NDCG@3 suggested by Dalton \net al. (2019; 2020). Note that TREC CAsT-19 uses relevant grade \u2265 1 as positive for MRR but TREC CAsT-20 uses \nrelevance grade \u2265 2 as positive for MRR (Dalton et al., 2020) \n\nQReCC \nOR-QuAC \nTREC CAsT-19 \nTREC CAsT-20 \nSystem \nMRR R@10 MRR@5 R@5 MRR NDCG@3 MRR NDCG@3 \n\nRetrieval \nBM25-QR \n--\n20.2 30.2 \n58.1 27.7 \n25.0 15.9 \nANCE-QR \n--\n45.7 58.4 \n66.5 40.9 \n37.5 25.5 \nConvDR \n--\n61.6 75.0 \n74.0 46.6 \n50.1 34.0 \nBM25-T5QR \n32.8 52.5 \n--\n--\n--\nCONQRR \n41.8 65.1 \n--\n--\n--\n\nT5-Large DE \n55.7 78.7 \n56.9 69.7 \n61.0 32.5 \n34.3 19.9 \nWIKI \n60.4 83.3 \n66.5 77.9 \n68.1 40.3 \n43.7 27.9 \nWIKI+WEB \n60.7 83.1 \n68.7 80.3 \n74.1 47.0 \n51.3 33.2 \n\nRetrieval + Reranking \nCFDA CLIP RUN7 \n--\n--\n71.4 43.6 \n--\nh2oloo RUN4 \n--\n--\n--\n59.3 45.8 \nConvDR\u2192BERT \n--\n77.3 85.0 \n79.9 54.1 \n54.5 39.2 \n\nT5-Large DE \u2192 reranker \n68.9 86.0 \n72.6 78.6 \n75.3 48.3 \n55.1 36.7 \nWIKI \n70.7 88.8 \n79.7 86.8 \n79.3 50.7 \n60.3 43.0 \nWIKI+WEB \n71.8 88.9 \n81.2 87.9 \n82.0 53.6 \n59.7 42.5 \n\n\n\n\nTable 7. Synthetic dialogs about European School, Munich generated by Inpaint PT ,Inpaint OQ , and Inpaint PTOQ . The division is led by a Deputy Secretary, presently Jim Longley, who reports to the Minister for Ageing and Minister for Disability Services, presently the Hon. John Ajaka MLC.Article: European School, Munich \n\nInpaint PT \n1: What is the European School, Munich? \n0: The European School, Munich (ESM) is one of thirteen European Schools and one of three in \nGermany. \n1: What did the European School, Munich do? \n0: First established in 1977, it moved to its current location in Neuperlach, a district in the south-east \nof Munich, in the state of Bavaria in 1981. \n1: Are there any other interesting aspects about this article? \n0: The ESM was principally established to serve the schooling needs of children of the staff of the \nEuropean Patent Office (EPO) -the executive body of the European Patent Organisation. \n1: Did it serve any other students? \n0: However, enrolment is open to other prospective students. \n1: What other services did it offer? \n0: The school offers the European Baccalaureate as its secondary leaving qualification.\" \n\nInpaint OQ \n1: What is the European School in Munich? \n0: The European School, Munich (ESM) is one of thirteen European Schools and one of three in \nGermany. \n1: Where is it located? \n0: First established in 1977, it moved to its current location in Neuperlach, a district in the south-east \nof Munich, in the state of Bavaria in 1981. \n1: Who attended/teaches there? \n0: The ESM was principally established to serve the schooling needs of children of the staff of the \nEuropean Patent Office (EPO) -the executive body of the European Patent Organisation. \n1: Was it later expanded to serve the needs of the general public? \n0: However, enrolment is open to other prospective students. \n1: what types of classes are taught there? \n0: The school offers the European Baccalaureate as its secondary leaving qualification.\" \n\nInpaint PTOQ 1: What is the purpose of the European School, Munich? \n0: The European School, Munich (ESM) is one of thirteen European Schools and one of three in \nGermany. \n1: Where is it located? \n0: First established in 1977, it moved to its current location in Neuperlach, a district in the south-east \nof Munich, in the state of Bavaria in 1981. \n1: Is it open to anyone? \n0: The ESM was principally established to serve the schooling needs of children of the staff of the \nEuropean Patent Office (EPO) -the executive body of the European Patent Organisation. \n1: Are there any other interesting aspects about this article? \n0: However, enrolment is open to other prospective students. \n1: What are some of the courses taught? \n0: The school offers the European Baccalaureate as its secondary leaving qualification.\" \n\nDialog Inpainting: Turning Documents into Dialogs \n\nArticle: Ageing, Disability and Home Care NSW \n\nInpaint PT \n1: What is Ageing, Disability and Home Care NSW? \n0: Ageing, Disability and Home Care NSW (ADHC) is a division of the Department of Family and \nCommunity Services in the Government of New South Wales that is responsible for the provision of \nservices to older people, people with a disability, and their families and carers in the state of New \nSouth Wales, Australia. \n1: What services does it provide? \n0: The ADHC division supports approximately 300,000 vulnerable people from its annual budget of \n2.5 billion. \n1: Does it have a large staff? \n0: It employs more than 13,000 staff. \n1: What is it known for? \n0: ADHC delivers services and manages 900 non-government and local government service providers \nthat are funded to deliver services such as early intervention, therapy, respite, supported accommoda-\ntion, case management and post-school programs. \n1: Who runs it? \n0: The division is led by a Deputy Secretary, presently Jim Longley, who reports to the Minister for \nAgeing and Minister for Disability Services, presently the Hon. John Ajaka MLC. \n\nInpaint OQ \n1: What is Ageing, Disability and Home Care NSW? \n0: Ageing, Disability and Home Care NSW (ADHC) is a division of the Department of Family and \nCommunity Services in the Government of New South Wales that is responsible for the provision of \nservices to older people, people with a disability, and their families and carers in the state of New \nSouth Wales, Australia. \n1: How many people do they serve? \n0: The ADHC division supports approximately 300,000 vulnerable people from its annual budget of \n2.5 billion. \n1: How many employees do they have? \n0: It employs more than 13,000 staff. \n1: What kind of services do they provide? \n0: ADHC delivers services and manages 900 non-government and local government service providers \nthat are funded to deliver services such as early intervention, therapy, respite, supported accommoda-\ntion, case management and post-school programs. \n1: Who are the executives? \n0: Inpaint PTOQ 1: Who does Ageing, Disability and Home Care NSW (ADHC) work for? \n0: Ageing, Disability and Home Care NSW (ADHC) is a division of the Department of Family and \nCommunity Services in the Government of New South Wales that is responsible for the provision of \nservices to older people, people with a disability, and their families and carers in the state of New \nSouth Wales, Australia. \n1: How much does it cost? \n0: The ADHC division supports approximately 300,000 vulnerable people from its annual budget of \n2.5 billion. \n1: How many people work for the division? \n0: It employs more than 13,000 staff. \n1: What services does the division provide? \n0: ADHC delivers services and manages 900 non-government and local government service providers \nthat are funded to deliver services such as early intervention, therapy, respite, supported accommoda-\ntion, case management and post-school programs. \n1: Who is in charge of the division? \n0: The division is led by a Deputy Secretary, presently Jim Longley, who reports to the Minister for \nAgeing and Minister for Disability Services, presently the Hon. John Ajaka MLC. \n\n\n\nTable 8 .\n8Synthetic dialogs about Ageing, Disability and Home Care NSW generated by Inpaint PT ,Inpaint OQ , and Inpaint PTOQ . Questions and Answers [Q&A] or Frequently Answered Questions). 1: Are there any other interesting aspects about this article? 0:Dialog Inpainting: Turning Documents into Dialogs \n\nArticle: FAQ \n\nInpaint PT \n1: What is FAQ? \n0: An FAQ is a list of frequently asked questions (FAQs) and answers on a particular topic (also \nknown as \n\n\nDialog Inpainting: Turning Documents into Dialogs Article: George B. McClellan Inpaint PT 1: Did George B. McClellan subordinates blame him for the failures on the campaign of 1862? 0: Although McClellan's subordinates can claim their share of responsibility for delays (such as Ambrose Burnside's misadventures at Burnside Bridge) and blunders (Edwin V. Sumner's attack without reconnaissance), these were localized problems from which the full army could have recovered. 1: What other blunder happened during the campaign? 0: Historians have faulted McClellan for accepting the cautious advice about saving his reserves, such as when a significant breakthrough in the center of the Confederate line could have been exploited, but Fitz John Porter is said to have told McClellan, \"Remember, General , I command the last reserve of the last Army of the Republic.\" However, the veracity of this supposed statement is in doubt. 1: Why is it in doubt? 0: Porter in the post-war period never claimed to have made the statement and it also fails to note the several Union corps at that time defending Washington. 1: Did the campaign end in a draw? 0: Despite being a tactical draw, Antietam is considered a turning point of the war and a victory for the Union because it ended Lee's strategic campaign (his first invasion of the North) and it allowed President Lincoln to issue the Emancipation Proclamation on September 22, taking effect on January 1, 1863. Inpaint OQ 1: What was George B. McClellan's biggest failure? 0: Although McClellan's subordinates can claim their share of responsibility for delays (such as Ambrose Burnside's misadventures at Burnside Bridge) and blunders (Edwin V. Sumner's attack without reconnaissance), these were localized problems from which the full army could have recovered. 1: What was his biggest success? 0: Historians have faulted McClellan for accepting the cautious advice about saving his reserves, such as when a significant breakthrough in the center of the Confederate line could have been exploited, but Fitz John Porter is said to have told McClellan, \"Remember, General , I command the last reserve of the last Army of the Republic.\" However, the veracity of this supposed statement is in doubt. 1: Why is the veracity of this statement in doubt? 0: Porter in the post-war period never claimed to have made the statement and it also fails to note the several Union corps at that time defending Washington. 1: What was the biggest battle he was involved in? 0: Despite being a tactical draw, Antietam is considered a turning point of the war and a victory for the Union because it ended Lee's strategic campaign (his first invasion of the North) and it allowed President Lincoln to issue the Emancipation Proclamation on September 22, taking effect on January 1, 1863. Inpaint PTOQ 1: What did George B. McClellan do during the Seven Days' Campaigns? 0: Although McClellan's subordinates can claim their share of responsibility for delays (such as Ambrose Burnside's misadventures at Burnside Bridge) and blunders (Edwin V. Sumner's attack without reconnaissance), these were localized problems from which the full army could have recovered. 1: Are there any other interesting aspects about this article? 0: Historians have faulted McClellan for accepting the cautious advice about saving his reserves, such as when a significant breakthrough in the center of the Confederate line could have been exploited, but Fitz John Porter is said to have told McClellan, \"Remember, General , I command the last reserve of the last Army of the Republic.\" However, the veracity of this supposed statement is in doubt. 1: Why is it in doubt? 0: Porter in the post-war period never claimed to have made the statement and it also fails to note the several Union corps at that time defending Washington. 1: Was it a successful campaign? 0: Despite being a tactical draw, Antietam is considered a turning point of the war and a victory for the Union because it ended Lee's strategic campaign (his first invasion of the North) and it allowed President Lincoln to issue the Emancipation Proclamation on September 22, taking effect on January 1, 1863.\n\nTable 10 .\n10Synthetic dialogs about George B. McClellan generated by Inpaint PT ,Inpaint OQ , and Inpaint PTOQ . Dialog Inpainting: Turning Documents into Dialogs in a question in the dataset(Table 11), we considered it to be potentially sensitive. From this initial analysis, we observed that 0.2-0.5% of the dataset contains such potentially sensitive interactions of terms in generated queries. SeeTables 12-16for selected examples that matched this filter.Race/Ethnicity Nationality Gender Region Political ReligionAdjective \nViolence Related \nCompetence Adjective \nPhysical Appearance Adjectives \nOutsider Adjectives \nHarmful Racial Stereotypes \nRace/Ethnicity Adjectives \nGender Adjectives \nPolitical \nSexual Orientation \nSocio-economic/class \nOccupation \n\n\n\nTable 11 .\n11The categories of sensitive terms and their interactions. Article: Model minority 1: The term model minority is often used in reference to African Americans, Hispanic Americans, and Asian Americans. But what does the term actually mean? 0: The model minority label also includes South Asian communities, in particular, Indian Americans, because of their high socioeconomic success. 1: And what can Indian Americans do to help other Asian Americans? 0: But, Indian-Asians face a type of racism and discrimination, despite their Model Minority status, that other Asians do not face. 1: Can you tell me more about this racism? 0:\n\n\nDialog Inpainting: Turning Documents into Dialogs Article: Virginia M. Alexander 1: who was the first african american woman to become a doctor in the united states? 0: Virginia M. Alexander Virginia M. Alexander (February 4, 1899 -July 24, 1949) was an American physician, public health researcher, and the founder of the Aspiranto Health Home in Philadelphia, Pennsylvania. 1: what was her childhood like? 0: Virginia M. Alexander was born in Philadelphia, Pennsylvania on February 4, 1899 to Hilliard Alexander and Virginia Pace. 1: did she have siblings? 0: She had four siblings, including the prominent attorney Raymond Pace Alexander.\nWe released WikiDialog at https://github.com/ google-research/dialog-inpainting\nWe truncated about 64% of passages containing 7+ sentences. 3 Additional details, including screenshots of the annotation interface, instructions and examples, are provided in Appendix B.\nWhen considering the net percentage of questions sufficiently or perfectly answered, the differences between the WikiDialog variants and OR-QuAC are not statistically significant at the p <\nFurther details of our approach and some examples are provided in Appendix G.\nWe omit the leading prompt utterance (sprompt) from the history. Also, we optionally omit answer turns, depending on the task setup.\nTheoretically, one could also use cross-validation to fine-tune retrievers on TREC CaST. However, it is very expansive due to inference on the retrieval corpus, so we did not fine-tune retriever in this work.\n", "annotations": {"author": "[{\"end\":64,\"start\":53},{\"end\":87,\"start\":65},{\"end\":101,\"start\":88},{\"end\":113,\"start\":102},{\"end\":134,\"start\":114},{\"end\":146,\"start\":135},{\"end\":158,\"start\":147}]", "publisher": null, "author_last_name": "[{\"end\":63,\"start\":60},{\"end\":86,\"start\":78},{\"end\":100,\"start\":96},{\"end\":112,\"start\":107},{\"end\":133,\"start\":127},{\"end\":145,\"start\":140},{\"end\":157,\"start\":154}]", "author_first_name": "[{\"end\":59,\"start\":53},{\"end\":69,\"start\":65},{\"end\":77,\"start\":70},{\"end\":95,\"start\":88},{\"end\":106,\"start\":102},{\"end\":118,\"start\":114},{\"end\":126,\"start\":119},{\"end\":139,\"start\":135},{\"end\":153,\"start\":147}]", "author_affiliation": null, "title": "[{\"end\":50,\"start\":1},{\"end\":208,\"start\":159}]", "venue": null, "abstract": "[{\"end\":1539,\"start\":210}]", "bib_ref": "[{\"end\":1654,\"start\":1630},{\"end\":1671,\"start\":1654},{\"end\":2039,\"start\":2014},{\"end\":2066,\"start\":2039},{\"end\":2089,\"start\":2066},{\"end\":2718,\"start\":2701},{\"end\":2834,\"start\":2815},{\"end\":2850,\"start\":2834},{\"end\":2878,\"start\":2850},{\"end\":2890,\"start\":2878},{\"end\":2909,\"start\":2890},{\"end\":2929,\"start\":2909},{\"end\":2949,\"start\":2929},{\"end\":2967,\"start\":2949},{\"end\":2988,\"start\":2967},{\"end\":4241,\"start\":4220},{\"end\":4258,\"start\":4241},{\"end\":4274,\"start\":4258},{\"end\":6358,\"start\":6355},{\"end\":6360,\"start\":6358},{\"end\":7091,\"start\":7065},{\"end\":7631,\"start\":7607},{\"end\":7695,\"start\":7673},{\"end\":11188,\"start\":11171},{\"end\":11297,\"start\":11276},{\"end\":12616,\"start\":12615},{\"end\":14495,\"start\":14476},{\"end\":15274,\"start\":15250},{\"end\":15292,\"start\":15274},{\"end\":15309,\"start\":15292},{\"end\":15329,\"start\":15309},{\"end\":15347,\"start\":15329},{\"end\":17099,\"start\":17082},{\"end\":17120,\"start\":17099},{\"end\":18362,\"start\":18336},{\"end\":18385,\"start\":18362},{\"end\":18401,\"start\":18385},{\"end\":18530,\"start\":18508},{\"end\":18552,\"start\":18530},{\"end\":19437,\"start\":19436},{\"end\":20796,\"start\":20779},{\"end\":20818,\"start\":20796},{\"end\":21607,\"start\":21606},{\"end\":23366,\"start\":23346},{\"end\":23370,\"start\":23366},{\"end\":25905,\"start\":25903},{\"end\":27699,\"start\":27680},{\"end\":27718,\"start\":27699},{\"end\":27737,\"start\":27718},{\"end\":27757,\"start\":27737},{\"end\":27777,\"start\":27757},{\"end\":27797,\"start\":27777},{\"end\":27817,\"start\":27797},{\"end\":27833,\"start\":27817},{\"end\":27851,\"start\":27833},{\"end\":27872,\"start\":27851},{\"end\":28046,\"start\":28027},{\"end\":28066,\"start\":28046},{\"end\":28082,\"start\":28066},{\"end\":28087,\"start\":28082},{\"end\":28103,\"start\":28087},{\"end\":28279,\"start\":28262},{\"end\":28636,\"start\":28606},{\"end\":28672,\"start\":28641},{\"end\":29029,\"start\":29008},{\"end\":29050,\"start\":29029},{\"end\":29070,\"start\":29050},{\"end\":29093,\"start\":29070},{\"end\":29115,\"start\":29093},{\"end\":29483,\"start\":29465},{\"end\":29502,\"start\":29483},{\"end\":29519,\"start\":29502},{\"end\":29731,\"start\":29712},{\"end\":32252,\"start\":32250},{\"end\":34433,\"start\":34431},{\"end\":36008,\"start\":35990},{\"end\":36312,\"start\":36295},{\"end\":37403,\"start\":37385},{\"end\":38774,\"start\":38757},{\"end\":38793,\"start\":38774},{\"end\":38816,\"start\":38793},{\"end\":38833,\"start\":38816},{\"end\":39919,\"start\":39891},{\"end\":40447,\"start\":40427},{\"end\":40467,\"start\":40447},{\"end\":40613,\"start\":40596},{\"end\":42631,\"start\":42614},{\"end\":42663,\"start\":42636},{\"end\":43069,\"start\":43052},{\"end\":43300,\"start\":43283},{\"end\":44278,\"start\":44262},{\"end\":47175,\"start\":47170},{\"end\":48253,\"start\":48248}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":55701,\"start\":55479},{\"attributes\":{\"id\":\"fig_1\"},\"end\":55894,\"start\":55702},{\"attributes\":{\"id\":\"fig_2\"},\"end\":56081,\"start\":55895},{\"attributes\":{\"id\":\"fig_3\"},\"end\":56338,\"start\":56082},{\"attributes\":{\"id\":\"fig_4\"},\"end\":71531,\"start\":56339},{\"attributes\":{\"id\":\"fig_5\"},\"end\":72152,\"start\":71532},{\"attributes\":{\"id\":\"fig_6\"},\"end\":72662,\"start\":72153},{\"attributes\":{\"id\":\"fig_7\"},\"end\":72876,\"start\":72663},{\"attributes\":{\"id\":\"fig_8\"},\"end\":73099,\"start\":72877},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":74081,\"start\":73100},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":74243,\"start\":74082},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":75138,\"start\":74244},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":75350,\"start\":75139},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":76110,\"start\":75351},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":77388,\"start\":76111},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":83435,\"start\":77389},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":83897,\"start\":83436},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":88075,\"start\":83898},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":88841,\"start\":88076},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":89482,\"start\":88842},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":90126,\"start\":89483}]", "paragraph": "[{\"end\":2189,\"start\":1555},{\"end\":2989,\"start\":2191},{\"end\":3541,\"start\":2991},{\"end\":4539,\"start\":3543},{\"end\":5152,\"start\":4541},{\"end\":5726,\"start\":5154},{\"end\":5973,\"start\":5748},{\"end\":6413,\"start\":5975},{\"end\":6605,\"start\":6415},{\"end\":6895,\"start\":6641},{\"end\":7199,\"start\":6897},{\"end\":7419,\"start\":7201},{\"end\":7568,\"start\":7470},{\"end\":8272,\"start\":7570},{\"end\":9303,\"start\":8323},{\"end\":9362,\"start\":9305},{\"end\":9843,\"start\":9364},{\"end\":9938,\"start\":9845},{\"end\":10211,\"start\":9940},{\"end\":11823,\"start\":10241},{\"end\":12431,\"start\":11862},{\"end\":12871,\"start\":12433},{\"end\":13559,\"start\":12873},{\"end\":14022,\"start\":13561},{\"end\":14333,\"start\":14024},{\"end\":14957,\"start\":14335},{\"end\":16360,\"start\":14959},{\"end\":16533,\"start\":16414},{\"end\":16954,\"start\":16535},{\"end\":17565,\"start\":16956},{\"end\":17737,\"start\":17567},{\"end\":17983,\"start\":17739},{\"end\":18260,\"start\":17985},{\"end\":18820,\"start\":18262},{\"end\":19915,\"start\":18822},{\"end\":20041,\"start\":19917},{\"end\":20191,\"start\":20056},{\"end\":20646,\"start\":20214},{\"end\":21256,\"start\":20648},{\"end\":21659,\"start\":21258},{\"end\":22649,\"start\":21661},{\"end\":23371,\"start\":22666},{\"end\":23479,\"start\":23384},{\"end\":24375,\"start\":23481},{\"end\":24843,\"start\":24377},{\"end\":25262,\"start\":24845},{\"end\":25907,\"start\":25264},{\"end\":26266,\"start\":25909},{\"end\":26784,\"start\":26268},{\"end\":27160,\"start\":26786},{\"end\":27476,\"start\":27162},{\"end\":28527,\"start\":27493},{\"end\":29308,\"start\":28529},{\"end\":30017,\"start\":29310},{\"end\":30851,\"start\":30032},{\"end\":31320,\"start\":30853},{\"end\":31762,\"start\":31335},{\"end\":32410,\"start\":31764},{\"end\":32628,\"start\":32422},{\"end\":33008,\"start\":32654},{\"end\":33441,\"start\":33035},{\"end\":33652,\"start\":33474},{\"end\":33811,\"start\":33654},{\"end\":33980,\"start\":33813},{\"end\":34947,\"start\":33982},{\"end\":35219,\"start\":34949},{\"end\":35493,\"start\":35221},{\"end\":35759,\"start\":35545},{\"end\":36009,\"start\":35823},{\"end\":36085,\"start\":36011},{\"end\":36231,\"start\":36157},{\"end\":36504,\"start\":36233},{\"end\":36768,\"start\":36522},{\"end\":37707,\"start\":36799},{\"end\":37773,\"start\":37709},{\"end\":37884,\"start\":37847},{\"end\":38055,\"start\":37886},{\"end\":38419,\"start\":38079},{\"end\":38572,\"start\":38421},{\"end\":38981,\"start\":38574},{\"end\":39060,\"start\":38983},{\"end\":39502,\"start\":39062},{\"end\":39649,\"start\":39504},{\"end\":40182,\"start\":39651},{\"end\":40384,\"start\":40217},{\"end\":40704,\"start\":40386},{\"end\":40786,\"start\":40728},{\"end\":41432,\"start\":40788},{\"end\":41684,\"start\":41434},{\"end\":42235,\"start\":41686},{\"end\":42478,\"start\":42237},{\"end\":42592,\"start\":42507},{\"end\":43274,\"start\":42594},{\"end\":43829,\"start\":43276},{\"end\":44467,\"start\":43831},{\"end\":45038,\"start\":44585},{\"end\":45156,\"start\":45040},{\"end\":45406,\"start\":45158},{\"end\":45646,\"start\":45408},{\"end\":45819,\"start\":45648},{\"end\":46978,\"start\":45821},{\"end\":48073,\"start\":46980},{\"end\":50480,\"start\":48075},{\"end\":51642,\"start\":50534},{\"end\":54186,\"start\":51644},{\"end\":55478,\"start\":54188}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7469,\"start\":7420},{\"attributes\":{\"id\":\"formula_1\"},\"end\":35822,\"start\":35760},{\"attributes\":{\"id\":\"formula_2\"},\"end\":36156,\"start\":36086},{\"attributes\":{\"id\":\"formula_3\"},\"end\":36798,\"start\":36769},{\"attributes\":{\"id\":\"formula_4\"},\"end\":37846,\"start\":37774}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":10303,\"start\":10294},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":11822,\"start\":11813},{\"end\":12587,\"start\":12580},{\"end\":21945,\"start\":21938},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23551,\"start\":23544},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24814,\"start\":24807},{\"end\":33651,\"start\":33644},{\"end\":34059,\"start\":34052},{\"end\":44056,\"start\":44049},{\"end\":44136,\"start\":44129},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":44420,\"start\":44409},{\"end\":49144,\"start\":49137},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":50367,\"start\":50359},{\"end\":51534,\"start\":51526},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":53242,\"start\":53234},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":53954,\"start\":53946},{\"end\":55367,\"start\":55359}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1553,\"start\":1541},{\"attributes\":{\"n\":\"2.\"},\"end\":5746,\"start\":5729},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6639,\"start\":6608},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8321,\"start\":8275},{\"end\":10239,\"start\":10214},{\"attributes\":{\"n\":\"3.\"},\"end\":11860,\"start\":11826},{\"attributes\":{\"n\":\"4.\"},\"end\":16412,\"start\":16363},{\"attributes\":{\"n\":\"5.\"},\"end\":20054,\"start\":20044},{\"attributes\":{\"n\":\"5.1.\"},\"end\":20212,\"start\":20194},{\"attributes\":{\"n\":\"5.2.\"},\"end\":22664,\"start\":22652},{\"attributes\":{\"n\":\"5.3.\"},\"end\":23382,\"start\":23374},{\"attributes\":{\"n\":\"6.\"},\"end\":27491,\"start\":27479},{\"attributes\":{\"n\":\"7.\"},\"end\":30030,\"start\":30020},{\"end\":31333,\"start\":31323},{\"attributes\":{\"n\":\"3.\"},\"end\":32420,\"start\":32413},{\"end\":32652,\"start\":32631},{\"end\":33033,\"start\":33011},{\"end\":33472,\"start\":33444},{\"end\":35543,\"start\":35496},{\"end\":36520,\"start\":36507},{\"end\":38077,\"start\":38058},{\"end\":40215,\"start\":40185},{\"end\":40726,\"start\":40707},{\"end\":42505,\"start\":42481},{\"end\":44504,\"start\":44470},{\"end\":44535,\"start\":44507},{\"end\":44583,\"start\":44538},{\"end\":50532,\"start\":50483},{\"end\":55490,\"start\":55480},{\"end\":55711,\"start\":55703},{\"end\":56093,\"start\":56083},{\"end\":72205,\"start\":72154},{\"end\":72670,\"start\":72664},{\"end\":74090,\"start\":74083},{\"end\":75147,\"start\":75140},{\"end\":75359,\"start\":75352},{\"end\":83446,\"start\":83437},{\"end\":88087,\"start\":88077},{\"end\":88853,\"start\":88843}]", "table": "[{\"end\":74081,\"start\":73467},{\"end\":74243,\"start\":74233},{\"end\":75138,\"start\":74466},{\"end\":75350,\"start\":75149},{\"end\":76110,\"start\":75384},{\"end\":77388,\"start\":76322},{\"end\":83435,\"start\":77682},{\"end\":83897,\"start\":83694},{\"end\":88841,\"start\":88597}]", "figure_caption": "[{\"end\":55701,\"start\":55492},{\"end\":55894,\"start\":55713},{\"end\":56081,\"start\":55897},{\"end\":56338,\"start\":56095},{\"end\":71531,\"start\":56341},{\"end\":72152,\"start\":71534},{\"end\":72662,\"start\":72212},{\"end\":72876,\"start\":72672},{\"end\":73099,\"start\":72879},{\"end\":73467,\"start\":73102},{\"end\":74233,\"start\":74092},{\"end\":74466,\"start\":74246},{\"end\":75384,\"start\":75361},{\"end\":76322,\"start\":76113},{\"end\":77682,\"start\":77391},{\"end\":83694,\"start\":83448},{\"end\":88075,\"start\":83900},{\"end\":88597,\"start\":88090},{\"end\":89482,\"start\":88856},{\"end\":90126,\"start\":89485}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4537,\"start\":4529},{\"end\":13714,\"start\":13706},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18796,\"start\":18788},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25713,\"start\":25705},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26771,\"start\":26763},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26799,\"start\":26791},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27173,\"start\":27165},{\"end\":33689,\"start\":33681},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34133,\"start\":34121},{\"end\":34612,\"start\":34604},{\"end\":34891,\"start\":34884},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37250,\"start\":37241},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38979,\"start\":38970},{\"end\":42876,\"start\":42859}]", "bib_author_first_name": null, "bib_author_last_name": null, "bib_entry": null, "bib_title": null, "bib_author": null, "bib_venue": null}}}, "year": 2023, "month": 12, "day": 17}