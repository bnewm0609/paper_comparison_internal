{"id": 102352622, "updated": "2023-09-28 04:23:51.687", "metadata": {"title": "Context-Aware Human Motion Prediction", "authors": "[{\"first\":\"Enric\",\"last\":\"Corona\",\"middle\":[]},{\"first\":\"Albert\",\"last\":\"Pumarola\",\"middle\":[]},{\"first\":\"Guillem\",\"last\":\"Aleny\u00e0\",\"middle\":[]},{\"first\":\"Francesc\",\"last\":\"Moreno-Noguer\",\"middle\":[]}]", "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "The problem of predicting human motion given a sequence of past observations is at the core of many applications in robotics and computer vision. Current state-of-the-art formulates this problem as a sequence-to-sequence task, in which a historical of 3D skeletons feeds a Recurrent Neural Network (RNN) that predicts future movements, typically in the order of 1 to 2 seconds. However, one aspect that has been obviated so far, is the fact that human motion is inherently driven by interactions with objects and/or other humans in the environment. In this paper, we explore this scenario using a novel context-aware motion prediction architecture. We use a semantic-graph model where the nodes parameterize the human and objects in the scene and the edges their mutual interactions. These interactions are iteratively learned through a graph attention layer, fed with the past observations, which now include both object and human body motions. Once this semantic graph is learned, we inject it to a standard RNN to predict future movements of the human/s and object/s. We consider two variants of our architecture, either freezing the contextual interactions in the future of updating them. A thorough evaluation in the Whole-Body Human Motion Database shows that in both cases, our context-aware networks clearly outperform baselines in which the context information is not considered.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1904.03419", "mag": "3034423770", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/CoronaPAM20", "doi": "10.1109/cvpr42600.2020.00702"}}, "content": {"source": {"pdf_hash": "c0ca65536652bc63f87975ceb33b602acfaefd0d", "pdf_src": "MergedPDFExtraction", "pdf_uri": "[\"https://arxiv.org/pdf/1904.03419v3.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBYNCND", "open_access_url": "https://upcommons.upc.edu/bitstream/2117/334913/3/2340-Context-aware-human-motion-prediction.pdf", "status": "GREEN"}}, "grobid": {"id": "c93a5ca83ecbddab2286acebc824e3903d2ab835", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c0ca65536652bc63f87975ceb33b602acfaefd0d.txt", "contents": "\nContext-aware Human Motion Prediction\n\n\nEnric Corona ecorona@iri.upc.edu \nInstitut de Rob\u00f2tica i Inform\u00e0tica Industrial\nCSIC-UPC\n08028BarcelonaSpain\n\nAlbert Pumarola apumarola@iri.upc.edu \nInstitut de Rob\u00f2tica i Inform\u00e0tica Industrial\nCSIC-UPC\n08028BarcelonaSpain\n\nGuillem Aleny\u00e0 galenya@iri.upc.edu \nInstitut de Rob\u00f2tica i Inform\u00e0tica Industrial\nCSIC-UPC\n08028BarcelonaSpain\n\nFrancesc Moreno-Noguer \nInstitut de Rob\u00f2tica i Inform\u00e0tica Industrial\nCSIC-UPC\n08028BarcelonaSpain\n\nContext-aware Human Motion Prediction\n\nThe problem of predicting human motion given a sequence of past observations is at the core of many applications in robotics and computer vision. Current state-ofthe-art formulate this problem as a sequence-to-sequence task, in which a historical of 3D skeletons feeds a Recurrent Neural Network (RNN) that predicts future movements, typically in the order of 1 to 2 seconds. However, one aspect that has been obviated so far, is the fact that human motion is inherently driven by interactions with objects and/or other humans in the environment.In this paper, we explore this scenario using a novel context-aware motion prediction architecture. We use a semantic-graph model where the nodes parameterize the human and objects in the scene and the edges their mutual interactions. These interactions are iteratively learned through a graph attention layer, fed with the past observations, which now include both object and human body motions. Once this semantic graph is learned, we inject it to a standard RNN to predict future movements of the human/s and object/s. We consider two variants of our architecture, either freezing the contextual interactions in the future of updating them. A thorough evaluation in the \"Whole-Body Human Motion Database\"[29]shows that in both cases, our context-aware networks clearly outperform baselines in which the context information is not considered.\n\nIntroduction\n\nThe ability to predict and anticipate future human motion based on past observations is essential for interacting with other people and the world around us. While this seems a trivial task for a person, it involves multiple sensory modalities and complex semantic understanding of the environment and the relations between all objects in it. Modeling and transferring this kind of knowledge to autonomous agents would have a major impact in many different fields, mainly in human-robot interaction [30] and autonomous driving [47], but also in motion generation for computer graphics animation [31] or image understanding [10].\n\nThe explosion of deep learning, combined with largescale datasets of human motion such as Human3.6M [24] or the CMU motion capture dataset [34], has led to a significant amount of recent literature that tackles the problem of forecasting 3D human motion from past observations [14,25,43,20,3,37,15,42,26,66]. These algorithms typically formulate the problem as sequence-tosequence task, in which past observations represented 3D skeleton data are injected to a Recurrent Neural Network (RNN) which then predicts movements in the near future (less than 2 seconds).\n\nNevertheless, while promising results have been achieved, we argue that the standard definition of the problem used so far lacks an important factor, which is the influence of the rest of the environment on the movement of the person. For instance, if a person is carrying a box, the configuration of the body arms and legs will be highly constrained by the 3D position of that box. Discovering such interrelations between the person and the object/s of the context (or another person he/she is interacting with), and how these interrelations constrain the body motion, is the principal motivation of this paper.\n\nIn order to explore this new paradigm, we devise a context-aware motion prediction architecture, that models the interactions between all objects of the scene and the human using a directed semantic graph. The nodes of this graph represent the state of the person and objects (e.g. positional features) and the edges their mutual interactions. These interactions are iteratively learned with the past observations of the human and objects motion and fed into a standard RNN which is then responsible for predicting the future movement of all elements in the scene (for both rigid objects and non-rigid human skeletons). Additionally, we propose a variant of this model that also predicts the evolution of the adjacency matrix representing the interaction between the elements of the scene.\n\nPresumably, one of the reasons why current state-of-theart has not considered an scenario like ours is because all methods are trained and evaluated on benchmarks (mostly the aforementioned Human3.6M dataset [24]) annotated only with human motion. In this paper, we thoroughly evaluate our approach in the \"Whole-Body Human Motion Database\" [29], that contains about 200 videos of people performing several tasks and interacting with objects. This dataset is annotated with MoCap data for the humans and rigid displacement for the rest of objects, being thus, a perfect benchmark to validate our ideas. We also evaluate our method in the CMU MoCap database [34] with only two people being tracked. The results obtained in both datasets show that our methodology is able to accurately predict the future motion of people and objects while simultaneously learning very coherent interaction relations. Additionally, all context-aware versions, clearly outperform the baselines which uniquely rely on human past observations of the human (see Fig. 1). Since all previous works evaluate their methods using past observations of ground truth skeletons, we finally discuss the applicability of state-of-art motion prediction methods, with an ablation study of our models and baselines when considering noisy observations.\n\n\nRelated work\n\nHuman motion prediction. Since the release of large-scale MoCap datasets [52,24,29], there has been a growing interest in the problem of estimating 3D human pose from single images [5,52,49,60,56,55,44,57]. More recently, the community is focusing in predicting 3D human motion from past observations. Most approaches build upon RNNs [14,43,20,3,50,1] that encode historical motion of the human and predict the future configuration that minimizes different sort of losses. Martinez et al. [43], for instance, minimize the L2 distance and provide one of the baselines in our work. This work also compares against a zero-velocity baseline, which despite steadily predicting the last observed frame, yields very reasonable results under the L2 metric. This phenomenon has been recently discussed by Ruiz et al. [54], that argue that L2 distance is not an appropriate metric to capture the actual distribution of human motion, and that a network trained using only this metric is prone to converge to a mean body pose. To better capture real distributions of human movement, recent approaches use adversarial networks [17,2] in combination with geometric losses [3,20,54,33]. There exist alternative approaches other than RNNs. For instance, Jain et al. [25] consider a hand-crafted spatialtemporal graph adapted to the skeleton shape. Li et al. [37] use Convolutional Neural Networks to encode and decode skeleton sequences instead of RNNs.\n\nAll methods described in this section formulate the human prediction problem without considering the context information. In this paper, we aim to fill this gap.\n\nRigid 3D object motion prediction. While there is a vast amount of works on 3D object reconstruction [51,19,41], detection [9,18,11] and tracking [8,4], only very few approaches address the problem of predicting future rigid motion [6,63,32,59]. Among these, it is worth to mention Byravan et al. [6], that predict the future 3D pose given an image of an object and the action being applied to it. In our case, the action applied to each object is implicitly encoded in the previous observations.\n\nHuman-Object Interaction (HOI). Even though our work does not aim to identify Human-Object relationships, we have been inspired by a few papers on this topic. The standard formulation of the problem consists in representing an image with several detected objects and people as a graph encoding the context [22,45,53,39,16], or some other structured representation [36,65,12]. The most recent approaches [37,53,22] extract features of the detected entities using some image-based classification CNNs. Then, they compare pairs of features to predict their mutual interaction. Qi et al. [53] refine the representations and predicted interactions in a recursive manner. In this work, we use a similar idea to progressively refine the estimation of the interactions between objects.\n\nGraph-based context reasoning. A few works leverage context information to boost the performance of different tasks [48,38,23,35,46] g( ) g( ) g( ) Figure 2: Overview of our context-aware motion prediction model. The blue branch represents a basic RNN that encodes past poses and decodes future human motion using a residual layer [43]. The upper branch corresponds to an RNN that encodes the contextual representation for each object in the scene. This branch contains two modules (depicted in brown and green). In brown, the past object position, class, and human joints are used to predict interactions and context feature vectors. The node corresponding to the human context representation is then used in conjunction with the human motion hidden state, to predict human motion. In green, the model is extended to predict motion of all observed objects. Best viewed in color.\n\n(GCNs) [28] were recently proposed for improved semisupervised classification. Jain et al. [25] used Structural RNNs to model spatio-temporal graphs. Wang et al. [61] propose to use GCNs, in which the interactions between objects depend on the intersection over union of their detected bounding boxes. Chen et al. [10] introduce an approach for image segmentation in which features from a 2D image coordinate space are represented in a graph reasoning space.\n\n\nProblem formulation\n\nRecent methods for human motion prediction consist of a model M, typically a deep neural network, that encodes motion from time t o until t \u2212 1. The goal is then to predict future human motion until t f , namely P t:t f , where P stands for the human pose represented by 3D joint coordinates. Previous approaches have formulated the problem as M : P to:t\u22121 \u2192 P t:t f , i.e. future motion is estimated only from past observations. In this paper, we conjecture that future motion is also driven by the context and the action the human is performing. We therefore consider other objects O of type T in the scene with which the human may interact. The objects can be other people or any object in the scene. We will design our approach to be able to predict the motion of such objects of the context.\n\nAdditionally, the influence that objects will have in the future motion of other objects is unclear. Thus, we also aim to build a model that learns these interactions in an unsupervised manner. Considering all this, we reformulate our problem as the estimation of the following mapping:\nM : {P to:t\u22121 , O to:t\u22121 , T } \u2192 {P t:t f , O t:t f , I to:t f } ,(1)\nwhere I corresponds to the predicted interactions. Figure 2 shows the main architecture used in this work. It consists of two branches that separately process human motion and object relationships. We use the latter to obtain a representation for all the observed entities, including the human, which we then use to predict both human and object motion prediction. We next describe these two branches.\n\n\nApproach\n\n\nHuman motion branch\n\nThis branch builds upon the RNN network proposed by Martinez et al. [43]. This model, depicted in blue in Figure 2, is based on a residual architecture [21] that, at each step, uses a fully connected layer to predict the velocity of the body joints. As in a typical sequence-to-sequence network, the predictions are fed to the next step.\n\n\nContext branch\n\nThe context information is represented using a directed graph structure where each node denotes an object or person. We then store a state for each entity and frame, encoding context information relevant to each node. These states are iteratively refined as new observations are processed. Object representation. At each frame t, we define a matrix X t \u2208 R N \u00d7F0 = [O t , T t , P t ] that gathers the representation of all N nodes. F 0 is the length of the state vector of each node. This state vector contains the object 3D bounding box O t , their object type T as a one-hot vector, and the joints of the person P t . If the node does not correspond to a person, the joints in the representation are set to a zero vector of same size. The object type helps to identify the task the human is performing and the motion defined for that task.\n\nBy doing this, we aim to capture the semantic difference between the motion of a person when handling a knife or when using a whisk. Modelling contextual object representations. Recent works on Graph Convolutional Networks (GCNs) [28] have shown very promising results in a variety of problems requiring the manipulation of graph-structured data. In GCNs, a feature vector of a certain node R i is expressed as a function of other nodes x, as\nR i = \u03c3( N j\u00c3 ij W x j ),\nwhere W are trainable weights, \u03c3 is an activation and N the number of nodes of the graph connected to the i-th node. A \u2208 R N \u00d7N is a normalized weighted adjacency matrix that defines interactions between nodes.\n\nGraph Attention Networks (GATs) [58] have been proposed as an extension of GCNs, and introduce an attention model on every graph node. In this paper we also investigate the use of Edge Convolutions [62], which are indeed very similar to GATs. In ECs the update rule for a feature vector of each entity considers the representations of other relevant objects as follows:\nR i = \u03c3( N j\u00c3 ij W [x i ; x i \u2212 x j ]).(2)\nThe intuition behind this equation is that x i encodes a global representation of the node, while x i \u2212 x j provides local information. EC proposes combining both types of information in an asymmetric graph function. We keep track of the context representations during all observations through a second RNN. Each node on the scene has a hidden state H that is updated every frame t:\nH t+1 i = RNN(R t i , H t i ).(3)\nLearning interactions. As we shall see in the experimental section, we initially evaluate a simplified version of our Context-RNN (C-RNN) that uses a heuristic to define the adjacency matrices, setting A ij = 1 if the center of gravity of objects i and j is closer than 1 meter. In practice, interactions between entities are not known a priory, and furthermore, they change over time. Our goal is to automatically learn these changing interactions with no supervision. For this purpose we devise an iterative process in which, for the first frame, we set A to a diagonal matrix, i.e.\u00c3 t0 = I N , meaning that the initial hidden representation of every object depends only on itself. We then predict the value of the interaction between two objects given the hidden state of both. We consider asymmetric weighted adjacency matrices, that for a frame t are estimated as:\nA t ij = g(H t i , H t i \u2212 H t j ),(4)\nwith similar structure as in Eq. 2. The function g represents the output of a neural network layer, in our case a fully connected. We normalize the interactions for each node using a Softmax function, which we shall denote\u00c3.\n\nIntuitively, we can consider this as a complete graph, where a graph attention mechanism [58] decides on the strength of interactions based on past observations. Note that while existing works typically use binary adjacency matrices from ground truth relationships [28], spatial assumptions [61] or K-NN on node representations [62], in this work we consider a differentiable continuous space of interactions, learned using back-propagation. In the rest of the paper we will denote the models that learn interactions with the suffix \"-LI\" (e.g. C-RNN+LI).\n\nObject motion prediction. We propose two methods that exploit context at different levels. First, in the blue+brown modules of Fig. 2, we consider a model that reasons about the past context observations and iteratively improves hidden representations. The refined context representation of the human node is concatenated to the baseline branch (in blue) representation at every time step, and used by a fully connected layer to predict human velocity in that step. This is followed by a residual layer that yields skeleton poses.\n\nOur second approach consists of the complete model depicted in Fig. 2 which, apart from past context, predicts object motion for all objects using a residual fully connected layer on each object hidden state. Analogous to the human motion branch, the predicted positions are forwarded to the next step, allowing to extend the context analysis into the future. The joints in the feature representations for those nodes describing people are also updated with the joint predictions of the human branch.\n\nAdditionally, when tracking several people, the human motion branch is repeated for each of them, and the model provides complete future motion for all available entities. In the rest of the document, we will denote the models that predict object motion with the suffix \"-OPM\".\n\n\nImplementation details\n\nOur model builds on the residual architecture of Martinez et al. [43] to allow an unbiased comparison with their work. The size of the human and object RNN hidden representations are 1024 and 256, respectively.\n\nAfter the motion seed, we sample an observation every 100 ms. In all experiments, we encode and decode 10 (1 sec.) and 20 frames (2 sec.) respectively. Larger encoding times did not help in improving the results and significantly increased training time. We augment the train set through random rotation over the height Z in the range (\u2212180, 180] \u2022 and random translation X, Y \u2208 (\u22121500, 1500)mm.\n\nWe use a similar approach as in [53] to obtain the adjacency matrix. We build a 4D matrix A such that A ij contains the hidden representations [H t i ; H t i \u2212 H j ] of nodes i and j, extending over the channel dimension. The function g(\u00b7) is formed by two Convolutional Layers of output kernel size 1 to make computation faster. We do not use bias term in these Convolutional layers nor in the Edge Convolutions.  132 162  82  158 211  254  48  103  140  180  68  135 190  226  27  54  65  71  QuaterNet  62  145  211 267  208 209 248  292  87  211  308  389  192 237 296  345  39  87 121  144  C-RNN  47  102  141 177  76  149 203  247  49  100  124  158  70  158 214  247  26  53  63  69  C-RNN+OMP  53  99  127 155  128 154 197  239  49  96  121  149  61  127 168  199  29  55  65  70  C-RNN+LI  43  89  117 142  72  141 188  230  47  92  117  147  72  145 194  219  27  53 Table 1: Class-specific models results. In this table, every action is independently trained. The results report the mean Euclidean error (in mm), for the 2s prediction of the human motion (top) and object motion (bottom). In all cases, 1s of past observations is provided. The context-based models we propose in this paper are those with the suffixes \"OMP\" and \"LI\". They provide the best results in most sequences.  Object representations are formed first by the bounding box position, defined by the minimum and maximum 3D Cartesian points. We train the model to minimize L2 distance between the predicted and the actual future motion L = ||M (P to:t\u22121 ) \u2212 P t:t f || 2 . The model is trained until convergence, using Adam [27] with learning rate of 0.0005, beta1 0.5, beta2 0.99 and batch size 16.\n\n\nExperiments\n\n\nPreliminaries\n\nDatasets. Large-scale MoCap datasets [29,24,34] provide annotations on the human poses but do not give any annotation about objects of the scene or any relevant context information. Therefore, most recent works on human motion prediction are evaluated without considering context information. Martinez et al. [43] show that for certain cases, even a simple zero-velocity baseline may yield better results than context-less learning models.\n\nTo demonstrate the merits of our approach, we leverage on the Whole-Body Human Motion (WBHM) Database [40], a large-scale publicly available dataset containing 3D raw data of multiple individuals and objects. In particular, we use all the activities where human joints are provided and include at least a table. This results in 190 videos and 198K frames, and a total of 15 tracked object classes. We use the raw recordings Vicon files at 100 Hz to obtain the bounding box of each object in each frame, and select 18 joints to represent the human skeleton.\n\nWe extract different actions representing different levels of complexity on the contextual information. The statistics of this dataset are the following: We will report results on both action-specific models and also on models trained with the entire dataset.\n\nWe also run experiments on the CMU Mocap Database [34]. We select the actions that include two people interacting, which include 34 videos with different activities like dancing, talking with hand gestures or boxing. In this case, the objects are not annotated, but we will show that context information from the two users is useful to improve over context-less models.\n\nBaselines. We compare our models to the context-less models proposed in [43]. First, we consider the basic residual Detail of the predictions obtained with our approaches, compared with the ground truth. Human and object motion are represented from light blue to dark blue and light green to dark green, respectively. Actions, from top to bottom are: A human supports on a table to kick a box, human leaning on a table, and two people (one of them standing on a ladder) passing an object. Right: Predicted adjacency matrices representing the interactions learned by our model. Note that these relations are directional (e.g. in the last example the ladder highly influences the motion of the Human#1 (50%) but the human has little influence over the ladder (11%). Best viewed in color with zoom.\n\nRNN. We also consider a Zero-Velocity (ZV) baseline that constantly predicts the last observed frame. We also compare to QuaterNet [50] using their available code, to predict absolute motion prediction. For object motion prediction, we also use a ZV and RNN models [43], where the position of an object is defined by its 3D bounding box. Our models. We run our context-aware models (C-RNN), incrementally adding the main ideas described in the paper. The basic C-RNN in our experiments uses the spatial heuristic described in Section 4.2 where interactions depend only on the distance between objects. This model processes context during the past frames, and then uses the last hidden state of the human node for human motion prediction at each step. This is extended by additionally predicting object motion (OMP) and recomputing object interaction from the previous assumption on the predicted positions. We then evaluate the efficiency of our model for learning interactions (LI). Like in the previously defined experiments, we evaluate a model that considers past contextual information and a model that prolongs object analysis into the future.\n\nEvaluation metric. Previous works on human motion prediction focus mainly on predicting relative motion [43,20,50], using joint angles. However, our model reasons about the full scene and is able to predict absolute motion in Cartesian coordinates. Therefore, we use the mean Euclidean Distance (in mm) between predictions and real future motion, obtained from the unnormalized predictions in the 3D space. For human motion prediction, we take into account the 18 joints defined in the human skeleton. For objects, we consider the eight 3D vertices of their bounding boxes.  Table  Table to Sponge  Table to Figure 4: Average interactions refined by the model during the past observations of the context. In the left and center plots, we depict relevant interactions for table cleaning and moving box activities respectively. In the first case, notice the table affects significantly the sponge and human, which initially moves towards the table to clean it. Similarly, in the second case, the human moves towards a box on the ground, picks it up and puts it on the table. The right plot shows average self-interaction percentages among all the test samples, for relevant object types. We found that non-moving objects like tables or ladders consistently have very little influence from other objects. Likewise, passive objects that are often moved by a human, such as knives or bottles, are more influenced by them and leave self-influence relatively low.\n\n\nResults on the WBHM Dataset\n\nQuantitative results. Table 1 summarizes the performance of class-specific models trained on different activities. Table 2 provides results at much higher temporal resolution for models trained using all the dataset, reporting the mean Euclidean distance between predictions and ground truth every 100 ms. In all cases, 1 second of past observations is provided and 2 seconds are predicted.\n\nThe performance of models that consider a thresholdbased binary interaction vary significantly between classes, suggesting they are effectively unable to understand the context as done by models that learn the actual interactions (LI). Notice that even the basic C-RNN does not yet provide a consistent improvement compared to state-of-art models. The same model that additionally learns interactions (C-RNN+LI) obtains a significant boost in most cases. Nonetheless, activities such as passing objects or grasping require attending to items that are at variable distances.\n\nRegarding the complexity of the scene, most improvement comes from scenes with a small number of objects where interactions are well defined and actions are more predictable. For cooking activities, there are several objects in a table next to the human. Different motion options are possible and, as uncertainty grows, the model seems unable to confidently understand interactions. Because of this, context-aware models do not provide such a significant improvement as in previous activities. Considering all actions simultaneously seems to favor even more the context-aware approaches and, specially, those that learn interactions (C-RNN+LI and C-RNN+OPM+LI).\n\nQualitative results. Figure 3-left shows the motion generation results of our two main models, compared to the baseline [43] on different classes. We did not include the Zero-Interaction weight (EC)\n\nInteraction weight (GAT) Figure 5: Interaction strength histogram predicted by EC and GAT models. These include interactions predicted among all humans and objects after two observations are given to the networks. For simplicity, we depict the histogram from tasks whose context only contain two or three nodes. On the left, interactions learnt by EC-based model, spanning a wide range of values, up to interaction strengths of more than 80%. On the right, GAT-based model, which predicts all interaction weights similarly and therefore we can only see peaks at 1/2 and 1/3.\n\nVelocity baseline as it does not provide interesting motion even though it has remained a difficult baseline on uncertain activities. We have marked some specific frames in which context-aware approaches improve the RNN baseline. For human motion prediction, poses generated are frequently more semantically-related to their closest objects than context-less models. For instance, as shown in the last action of Figure 3, people holding objects tend to move the relevant hand. For object motion prediction, context-less model predictions hardly move from their original position.\n\nRegarding the interactions predicted by the model, we notice coherent patterns in many activities. For example, drinking videos generate strong Cup-Human relationships. In Figure 4, we represent the average predicted interactions for different actions. These are gathered from the C-   RNN+OMP+LI. This model provides more intense Object-Object interactions than C-RNN+LI, which does not need to obtain such meaningful representations for objects as only human contextual representations are used. Note that the models learn to predict interactions that provide information relevant to future pose, and thus improve motion predictions. Interactions here do not necessarily respond to actual action relationships. We finally study the effect of the Graph architecture in the learned interactions. Graph Attention Networks (GATs) and Edge Convolutions (EC) provide an attention mechanism to measure the interaction strength. Nevertheless, we found that GAT-based networks consider all interactions of similar importance, while EC-based architectures are able to predict a continuous and wide range of attention values. We show this in Figure 5.\n\n\nResults on the CMU MoCap Dataset\n\nWe train the models again on the CMU MoCap Database, obtaining the results depicted in Table 4. In this setup, the users perform very energetic activities like dancing or boxing, which implies that absolute motion is larger, and error on the CMU MoCap database being in average more than twice that in the former database. In this case, only two nodes are observed in each video for the two people being tracked. Since no information about actions or objects is given, we do not provide results on OMP. However, we find our proposed model C-RNN+LI outperforms all other baselines significantly, specially in the long-term.\n\n\nRobustness to noise\n\nAll previous works on human motion prediction use ground truth MoCap data as past observations. Nevertheless, real applications will receive joint observations from e.g. human pose estimation models, such as OpenPose [7] or AlphaPose [13,64], which are prone to suffer from noise and mis-detections, specially under strong occlusions. In these subsection, we therefore evaluate the resilience of our proposed models and previous baselines to noise in the input observations. Predictions are evaluated on the original ground truth data. The 3D coordinates of past observations (both in human and objects positions) are corrupted by additive Gaussian noise N(0, \u03c3 2 ). In Table 3 we show the results of this experiment, with different values of \u03c3. Interestingly, the error in the predictions gracefully increases with the noise, but still, our approach performs consistently better than those approaches that do not consider the context information. Indeed, the best context-aware models (C-RNN+LI and C-RNN+OMP+LI) with noise up to \u03c3 = 50mm, perform better than context-less baselines with no noise in the input.\n\n\nConclusion\n\nIn this work, we explore a context-aware motion prediction architecture, using a semantic-graph representation where objects and humans are represented by nodes independently of the number of objects or complexity of the environment. We extensively analyze their contribution for human motion prediction. The results observed in different actions suggest that the models proposed are able to understand human activities significantly better than state-of-art models which do not use context, improving both human and object motion prediction.\n\nFigure 3 :\n3Qualitative motion generation up to two seconds. Left: Predicted sample frames of our approaches and the baselines. Center:\n\nTABLE HUMAN CUP\nHUMANContext-aware human motion prediction. (a) Sample image of a sequence with a person placing a cup on a table. This image is shown solely for illustrative purposes, our approach only relies on positional data. (b) Past observations of all elements of the scene, the person, the cup and the table. (c) Ground truth future movements. (d) Human motion predicted using[43], consisting of an RNN that is agnostic of the context information. Note that there is a large gap with the ground truth, especially for the final frames of the sequence. (e) Cup and human motion prediction obtained with our context-aware model. While the arm of the person is not fully extended, the forecasted motion highly resembles the ground truth. Interestingly, the interaction with the table also helps to set the motion boundaries. (f) Main interactions that are learned with our approach in which dominates the influence of the table over both the cup and the person.(a) \n(b) \n(c) \n(d) \n(e) \n(f) \n\nFigure 1: \n\n\n. Graph Convolutional NetworksH to \n\nX t o \nX t o +1 \n\nP t o \nP t o +1 \nP t-1 \n\nRN N \n\nRN N \nRN N \n\nRN N \nRN N \n\nRN N \nRN N \n\nRN N \nRN N \nRN N \n\ng( ) \n\nP ' \n\nt \n\nP ' \n\nt+ 1 \n\nX ' \n\nt \n\nX ' \n\nt+ 1 \n\nFC \n\nFC \n\nFC \nFC \n\nEC \nEC \n\nEC \nEC \nEC \n\nHuman motion prediction baseline \n\nObject context understanding \n\nObject motion prediction \n\nResidual connection \n\nConcatenation \n\nX t-1 \n\n.8 \n.2 \n\n.1 \n.9 \n\n[ ,T , ] \n[ ,T , ] \n[ ,T , ] \n[ ,T , ] \n[ ,T , ] \n\n.8 \n.2 \n\n.1 \n.9 \n.8 \n.2 \n\n.1 \n.9 \n.8 \n.2 \n\n.1 \n.9 \n\n\n\nTable 2 :\n2Training with all actions simultaneously. For each method we train a single model using all actions simultaneously. See also caption inTable 1.\n\n\nNoise-free inputNoisy input (HMP) \nModel \nHMP Models \n25 mm \n50 mm \n100 mm \nTime (s) \n0.5 \n1 \n1.5 \n2 \n0.5 \n1 \n1.5 \n2 \n0.5 \n1 \n1.5 \n2 \n0.5 \n1 \n1.5 \n2 \nZV [43] \n61 \n150 223 281 \n70 \n156 227 \n284 \n87 \n168 \n236 292 \n122 195 \n260 313 \nRNN [43] \n55 \n110 148 179 \n72 \n122 156 \n185 \n83 \n127 \n162 192 \n112 184 \n214 226 \nC-RNN+LI \n52 \n104 136 161 \n58 \n107 139 \n166 \n69 \n113 \n147 175 \n99 \n136 \n175 208 \nC-RNN+OMP+LI \n56 \n109 140 165 \n62 \n111 142 \n167 \n71 \n116 \n146 171 \n103 132 \n162 187 \nModel \nOMP Models \n25 mm \n50 mm \n100 mm \nZV \n42 \n100 149 188 \n53 \n106 154 \n191 \n66 \n118 \n164 199 \n106 151 \n187 223 \nRNN \n41 \n93 \n135 169 \n52 \n99 \n141 \n174 \n69 \n113 \n151 183 \n105 142 \n181 208 \nC-RNN+OMP+LI \n40 \n81 \n109 129 \n51 \n88 \n115 \n134 \n67 \n100 \n126 144 \n106 132 \n156 172 \n\n\n\nTable 3 :\n3Robustness to noise in Human and Object Motion Prediction. Average performance of the principal models when using the original test set (Noise-free input), compared to their performance when seeing noisy observations.CMU MoCap Dataset \nTime (s) \n0.5 \n1.0 \n1.5 \n2.0 \n\u00b5 \n\u03c3 \n\u00b5 \n\u03c3 \n\u00b5 \n\u03c3 \n\u00b5 \n\u03c3 \nZV \n127 32 \n271 66 374 \n86 460 97 \nRNN \n125 28 \n267 58 378 \n77 477 92 \nQuaterNet \n138 26 \n279 58 378 \n82 466 95 \nC-RNN+LI \n124 27 \n257 53 352 \n65 435 78 \n\n\n\nTable 4 :\n4Mean and Std of prediction errors (mm) on the CMU dataset. Our Context-aware model C-RNN+LI outperforms baselines even though context only consists of two people.\nAcknowledgements: This work has been partially funded by the Spanish government under projects HuMoUR TIN2017-90086-R, ERA-Net Chistera project IPALM PCI2019-103386 and Mar\u00eda de Maeztu Seal of Excellence MDM-2016-0656. We also thank Nvidia for hardware donation.\nStructured prediction helps 3d human motion modelling. Emre Aksan, Manuel Kaufmann, Otmar Hilliges, ICCV. Emre Aksan, Manuel Kaufmann, and Otmar Hilliges. Struc- tured prediction helps 3d human motion modelling. In ICCV, 2019. 2\n\nMartin Arjovsky, arXiv:1701.07875Soumith Chintala, and L\u00e9on Bottou. Wasserstein gan. arXiv preprintMartin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017. 2\n\nHp-gan: Probabilistic 3d human motion prediction via gan. Emad Barsoum, John Kender, Zicheng Liu, CVPR-Workshop. 1Emad Barsoum, John Kender, and Zicheng Liu. Hp-gan: Probabilistic 3d human motion prediction via gan. In CVPR- Workshop, 2018. 1, 2\n\nFantrack: 3d multi-object tracking with feature association network. Erkan Baser, Venkateshwaran Balasubramanian, Prarthana Bhattacharyya, Krzysztof Czarnecki, IV. Erkan Baser, Venkateshwaran Balasubramanian, Prarthana Bhattacharyya, and Krzysztof Czarnecki. Fantrack: 3d multi-object tracking with feature association network. In IV, 2019. 2\n\nKeep it smpl: Automatic estimation of 3d human pose and shape from a single image. Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, Michael J Black, ECCV. Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J Black. Keep it smpl: Automatic estimation of 3d human pose and shape from a single image. In ECCV, 2016. 2\n\nSe3-nets: Learning rigid body motion using deep neural networks. Arunkumar Byravan, Dieter Fox, ICRA. Arunkumar Byravan and Dieter Fox. Se3-nets: Learning rigid body motion using deep neural networks. In ICRA, 2017. 2\n\nOpenpose: realtime multi-person 2d pose estimation using part affinity fields. Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, Yaser Sheikh, arXiv:1812.08008arXiv preprintZhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Openpose: realtime multi-person 2d pose estimation using part affinity fields. arXiv preprint arXiv:1812.08008, 2018. 8\n\nArgoverse: 3d tracking and forecasting with rich maps. Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionMing-Fang Chang, John Lambert, Patsorn Sangkloy, Jag- jeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Pe- ter Carr, Simon Lucey, Deva Ramanan, et al. Argoverse: 3d tracking and forecasting with rich maps. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. 2\n\nSanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection. Xiaozhi Chen, Kaustav Kundu, Yukun Zhu, G Andrew, Huimin Berneshawi, Ma, Advances in Neural Information Processing Systems. Xiaozhi Chen, Kaustav Kundu, Yukun Zhu, Andrew G Berneshawi, Huimin Ma, Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection. In Advances in Neural Information Processing Systems, pages 424-432, 2015. 2\n\nYunpeng Chen, Marcus Rohrbach, Zhicheng Yan, Shuicheng Yan, arXiv:1811.12814Jiashi Feng, and Yannis Kalantidis. Graph-based global reasoning networks. 13arXiv preprintYunpeng Chen, Marcus Rohrbach, Zhicheng Yan, Shuicheng Yan, Jiashi Feng, and Yannis Kalantidis. Graph-based global reasoning networks. arXiv preprint arXiv:1811.12814, 2018. 1, 3\n\nPose estimation for objects with rotational symmetry. Enric Corona, Kaustav Kundu, Sanja Fidler, IROS. Enric Corona, Kaustav Kundu, and Sanja Fidler. Pose esti- mation for objects with rotational symmetry. In IROS, 2018. 2\n\nGanhand: Predicting human grasp affordances in multi-object scenes. Enric Corona, Albert Pumarola, Guillem Aleny\u00e0, Francesc Moreno-Noguer, Gr\u00e9gory Rogez, CVPR. 2020Enric Corona, Albert Pumarola, Guillem Aleny\u00e0, Francesc Moreno-Noguer, and Gr\u00e9gory Rogez. Ganhand: Predicting human grasp affordances in multi-object scenes. In CVPR, 2020. 2\n\nRmpe: Regional multi-person pose estimation. Shuqin Hao-Shu Fang, Yu-Wing Xie, Cewu Tai, Lu, CVPR. Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu. Rmpe: Regional multi-person pose estimation. In CVPR, 2017. 8\n\nRecurrent network models for human dynamics. Katerina Fragkiadaki, Sergey Levine, Panna Felsen, Jitendra Malik, ICCV. 1Katerina Fragkiadaki, Sergey Levine, Panna Felsen, and Ji- tendra Malik. Recurrent network models for human dynam- ics. In ICCV, 2015. 1, 2\n\nLearning human motion models for long-term predictions. Partha Ghosh, Jie Song, Emre Aksan, Otmar Hilliges, 2017. 12017 International Conference on 3D Vision. Partha Ghosh, Jie Song, Emre Aksan, and Otmar Hilliges. Learning human motion models for long-term predictions. In 2017 International Conference on 3D Vision (3DV), 2017. 1\n\nActions and attributes from wholes and parts. Georgia Gkioxari, Ross Girshick, Jitendra Malik, ICCV. Georgia Gkioxari, Ross Girshick, and Jitendra Malik. Ac- tions and attributes from wholes and parts. In ICCV, 2015. 2\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, NIPS. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014. 2\n\n3d pose estimation and 3d model retrieval for objects in the wild. Alexander Grabner, M Peter, Vincent Roth, Lepetit, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionAlexander Grabner, Peter M Roth, and Vincent Lepetit. 3d pose estimation and 3d model retrieval for objects in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3022-3031, 2018. 2\n\nAtlasNet: A Papier-M\u00e2ch\u00e9 Approach to Learning 3D Surface Generation. Thibault Groueix, Matthew Fisher, Vladimir G Kim, Bryan Russell, Mathieu Aubry, CVPR. Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan Russell, and Mathieu Aubry. AtlasNet: A Papier-M\u00e2ch\u00e9 Ap- proach to Learning 3D Surface Generation. In CVPR, 2018. 2\n\nAdversarial geometry-aware human motion prediction. Yu-Xiong Liang-Yan Gui, Xiaodan Wang, Liang, Moura, ECCV. 6Liang-Yan Gui, Yu-Xiong Wang, Xiaodan Liang, and Jos\u00e9 MF Moura. Adversarial geometry-aware human motion prediction. In ECCV, 2018. 1, 2, 6\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 3\n\nJonathan Berant, and Amir Globerson. Mapping images to scene graphs with permutation-invariant structured prediction. Roei Herzig, Moshiko Raboh, Gal Chechik, NIPS. Roei Herzig, Moshiko Raboh, Gal Chechik, Jonathan Be- rant, and Amir Globerson. Mapping images to scene graphs with permutation-invariant structured prediction. In NIPS, 2018. 2\n\nRelation networks for object detection. Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei, CVPR. Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object detection. In CVPR, 2018. 2\n\nHuman3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. TPAMI. Catalin Ionescu, Dragos Papava, Vlad Olaru, Cristian Sminchisescu, Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3. 6m: Large scale datasets and pre- dictive methods for 3d human sensing in natural environ- ments. TPAMI, 2014. 1, 2, 5\n\nStructural-rnn: Deep learning on spatio-temporal graphs. Ashesh Jain, Silvio Amir R Zamir, Ashutosh Savarese, Saxena, CVPR. 13Ashesh Jain, Amir R Zamir, Silvio Savarese, and Ashutosh Saxena. Structural-rnn: Deep learning on spatio-temporal graphs. In CVPR, 2016. 1, 2, 3\n\nInstance-level future motion estimation in a single image based on ordinal regression. Kyung-Rae Kim, Whan Choi, Yeong Jun Koh, Seong-Gyun, Chang-Su Jeong, Kim, ICCV. Kyung-Rae Kim, Whan Choi, Yeong Jun Koh, Seong-Gyun Jeong, and Chang-Su Kim. Instance-level future motion es- timation in a single image based on ordinal regression. In ICCV, 2019. 1\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 5\n\nSemi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.0290734arXiv preprintThomas N Kipf and Max Welling. Semi-supervised classi- fication with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. 3, 4\n\nLearning human activities and object affordances from rgb-d videos. IJRR. Rudhir Hema Swetha Koppula, Ashutosh Gupta, Saxena, 25Hema Swetha Koppula, Rudhir Gupta, and Ashutosh Sax- ena. Learning human activities and object affordances from rgb-d videos. IJRR, 2013. 1, 2, 5\n\nAnticipating human activities using object affordances for reactive robotic response. S Hema, Ashutosh Koppula, Saxena, TPAMI. 1Hema S Koppula and Ashutosh Saxena. Anticipating hu- man activities using object affordances for reactive robotic response. TPAMI, 2016. 1\n\nMotion graphs. Lucas Kovar, Michael Gleicher, Fr\u00e9d\u00e9ric Pighin, SIGGRAPH. Lucas Kovar, Michael Gleicher, and Fr\u00e9d\u00e9ric Pighin. Motion graphs. In SIGGRAPH, 2008. 1\n\nMotion prediction with recurrent neural network dynamical models and trajectory optimization. Philipp Kratzer, Marc Toussaint, Jim Mainprice, arXiv:1906.12279arXiv preprintPhilipp Kratzer, Marc Toussaint, and Jim Mainprice. Mo- tion prediction with recurrent neural network dynami- cal models and trajectory optimization. arXiv preprint arXiv:1906.12279, 2019. 2\n\nBihmp-gan: Bidirectional 3d human motion prediction gan. Jogendra Nath Kundu, Maharshi Gor, R Venkatesh Babu, arXiv:1812.02591arXiv preprintJogendra Nath Kundu, Maharshi Gor, and R Venkatesh Babu. Bihmp-gan: Bidirectional 3d human motion predic- tion gan. arXiv preprint arXiv:1812.02591, 2018. 2\n\nCmu motion capture database. 25CMU Graphics LabCMU Graphics Lab. Cmu motion capture database. http: //mocap.cs.cmu.edu/. 1, 2, 5\n\nSegmental spatiotemporal cnns for fine-grained action segmentation. Colin Lea, Austin Reiter, Ren\u00e9 Vidal, Gregory D Hager, ECCV. Colin Lea, Austin Reiter, Ren\u00e9 Vidal, and Gregory D Hager. Segmental spatiotemporal cnns for fine-grained action seg- mentation. In ECCV, 2016. 2\n\nSituation recognition with graph neural networks. Ruiyu Li, Makarand Tapaswi, Renjie Liao, Jiaya Jia, Raquel Urtasun, Sanja Fidler, ICCV. Ruiyu Li, Makarand Tapaswi, Renjie Liao, Jiaya Jia, Raquel Urtasun, and Sanja Fidler. Situation recognition with graph neural networks. In ICCV, 2017. 2\n\nTransferable interactiveness prior for human-object interaction detection. Yong-Lu Li, Siyuan Zhou, Xijie Huang, Liang Xu, Ze Ma, Hao-Shu Fang, Yan-Feng Wang, Cewu Lu, arXiv:1811.082641arXiv preprintYong-Lu Li, Siyuan Zhou, Xijie Huang, Liang Xu, Ze Ma, Hao-Shu Fang, Yan-Feng Wang, and Cewu Lu. Transferable interactiveness prior for human-object interaction detection. arXiv preprint arXiv:1811.08264, 2018. 1, 2\n\nStructure inference net: object detection using scene-level context and instance-level relationships. Yong Liu, Ruiping Wang, Shiguang Shan, Xilin Chen, In CVPR. 2Yong Liu, Ruiping Wang, Shiguang Shan, and Xilin Chen. Structure inference net: object detection using scene-level context and instance-level relationships. In CVPR, 2018. 2\n\nAttend and interact: Higherorder object interactions for video understanding. Chih-Yao Ma, Asim Kadav, Iain Melvin, Zsolt Kira, Ghassan Alregib, Hans Peter Graf, CVPR. Chih-Yao Ma, Asim Kadav, Iain Melvin, Zsolt Kira, Ghassan AlRegib, and Hans Peter Graf. Attend and interact: Higher- order object interactions for video understanding. In CVPR, 2018. 2\n\nThe kit whole-body human motion database. Christian Mandery, \u00d6mer Terlemez, Martin Do, Nikolaus Vahrenkamp, Tamim Asfour, ICAR. Christian Mandery,\u00d6mer Terlemez, Martin Do, Nikolaus Vahrenkamp, and Tamim Asfour. The kit whole-body hu- man motion database. In ICAR, 2015. 5\n\nDense 3d point cloud reconstruction using a deep pyramid network. Priyanka Mandikal, Venkatesh Babu Radhakrishnan, WACV. Priyanka Mandikal and Venkatesh Babu Radhakrishnan. Dense 3d point cloud reconstruction using a deep pyramid network. In WACV, 2019. 2\n\nLearning trajectory dependencies for human motion prediction. Wei Mao, Miaomiao Liu, Mathieu Salzmann, Hongdong Li, ICCV. Wei Mao, Miaomiao Liu, Mathieu Salzmann, and Hongdong Li. Learning trajectory dependencies for human motion pre- diction. In ICCV, 2019. 1\n\nOn human motion prediction using recurrent neural networks. Julieta Martinez, J Michael, Javier Black, Romero, CVPR. 7Julieta Martinez, Michael J Black, and Javier Romero. On human motion prediction using recurrent neural networks. In CVPR, 2017. 1, 2, 3, 4, 5, 6, 7, 8\n\n3d human pose estimation from a single image via distance matrix regression. Francesc Moreno-Noguer, CVPR. Francesc Moreno-Noguer. 3d human pose estimation from a single image via distance matrix regression. In CVPR, 2017. 2\n\nPixels to graphs by associative embedding. Alejandro Newell, Jia Deng, NIPS. Alejandro Newell and Jia Deng. Pixels to graphs by associa- tive embedding. In NIPS, 2017. 2\n\nProgressively parsing interactional objects for fine grained action detection. Bingbing Ni, Xiaokang Yang, Shenghua Gao, CVPR. Bingbing Ni, Xiaokang Yang, and Shenghua Gao. Progres- sively parsing interactional objects for fine grained action de- tection. In CVPR, 2016. 2\n\nA survey of motion planning and control techniques for self-driving urban vehicles. IV. Brian Paden, Sze Zheng Michal\u010d\u00e1p, Dmitry Yong, Emilio Yershov, Frazzoli, Brian Paden, Michal\u010c\u00e1p, Sze Zheng Yong, Dmitry Yershov, and Emilio Frazzoli. A survey of motion planning and con- trol techniques for self-driving urban vehicles. IV, 2016. 1\n\nDisease prediction using graph convolutional networks: Application to autism spectrum disorder and alzheimer's disease. Sarah Parisot, Sofia Ira Ktena, Enzo Ferrante, Matthew Lee, Ricardo Guerrero, Ben Glocker, Daniel Rueckert, Medical image analysis. 482Sarah Parisot, Sofia Ira Ktena, Enzo Ferrante, Matthew Lee, Ricardo Guerrero, Ben Glocker, and Daniel Rueckert. Dis- ease prediction using graph convolutional networks: Appli- cation to autism spectrum disorder and alzheimer's disease. Medical image analysis, 48, 2018. 2\n\nCoarse-to-fine volumetric prediction for single-image 3d human pose. Georgios Pavlakos, Xiaowei Zhou, G Konstantinos, Kostas Derpanis, Daniilidis, CVPR. Georgios Pavlakos, Xiaowei Zhou, Konstantinos G Derpa- nis, and Kostas Daniilidis. Coarse-to-fine volumetric predic- tion for single-image 3d human pose. In CVPR, 2017. 2\n\nQuaternet: A quaternion-based recurrent model for human motion. Dario Pavllo, David Grangier, Michael Auli, arXiv:1805.0648526arXiv preprintDario Pavllo, David Grangier, and Michael Auli. Quater- net: A quaternion-based recurrent model for human motion. arXiv preprint arXiv:1805.06485, 2018. 2, 6\n\nC-Flow: Conditional Generative Flow Models for Images and 3D Point Clouds. Albert Pumarola, Stefan Popov, Francesc Moreno-Noguer, Vittorio Ferrari, CVPR. 2020Albert Pumarola, Stefan Popov, Francesc Moreno-Noguer, and Vittorio Ferrari. C-Flow: Conditional Generative Flow Models for Images and 3D Point Clouds. In CVPR, 2020. 2\n\nModeling the geometry of dressed humans. Albert Pumarola, Jordi Sanchez-Riera, Gary Choi, Alberto Sanfeliu, Francesc Moreno-Noguer, ICCV. 3Albert Pumarola, Jordi Sanchez-Riera, Gary Choi, Alberto Sanfeliu, and Francesc Moreno-Noguer. 3dpeople: Model- ing the geometry of dressed humans. In ICCV, 2019. 2\n\nLearning human-object interactions by graph parsing neural networks. Siyuan Qi, Wenguan Wang, Baoxiong Jia, Jianbing Shen, Song-Chun Zhu, In ECCV. 24Siyuan Qi, Wenguan Wang, Baoxiong Jia, Jianbing Shen, and Song-Chun Zhu. Learning human-object interactions by graph parsing neural networks. In ECCV, 2018. 2, 4\n\nHuman motion prediction via spatiotemporal inpainting. Alejandro Hernandez Ruiz, Juergen Gall, Francesc Moreno-Noguer, ICCV. Alejandro Hernandez Ruiz, Juergen Gall, and Francesc Moreno-Noguer. Human motion prediction via spatio- temporal inpainting. In ICCV, 2019. 2\n\nA joint model for 2d and 3d pose estimation from a single image. Edgar Simo-Serra, Ariadna Quattoni, Carme Torras, Francesc Moreno-Noguer, CVPR. Edgar Simo-Serra, Ariadna Quattoni, Carme Torras, and Francesc Moreno-Noguer. A joint model for 2d and 3d pose estimation from a single image. In CVPR, 2013. 2\n\nSingle image 3d human pose estimation from noisy observations. Edgar Simo-Serra, Arnau Ramisa, Guillem Aleny\u00e0, Carme Torras, Francesc Moreno-Noguer, CVPR. Edgar Simo-Serra, Arnau Ramisa, Guillem Aleny\u00e0, Carme Torras, and Francesc Moreno-Noguer. Single image 3d hu- man pose estimation from noisy observations. In CVPR, 2012. 2\n\n3d human pose tracking priors using geodesic mixture models. Edgar Simo-Serra, Carme Torras, Francesc Moreno-Noguer, Edgar Simo-Serra, Carme Torras, and Francesc Moreno- Noguer. 3d human pose tracking priors using geodesic mix- ture models. 2017. 2\n\nPetar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, arXiv:1710.10903Graph attention networks. arXiv preprintPetar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph at- tention networks. arXiv preprint arXiv:1710.10903, 2017. 4\n\nSudheendra Vijayanarasimhan, Susanna Ricco, Cordelia Schmid, Rahul Sukthankar, Katerina Fragkiadaki, arXiv:1704.07804Sfmnet: Learning of structure and motion from video. arXiv preprintSudheendra Vijayanarasimhan, Susanna Ricco, Cordelia Schmid, Rahul Sukthankar, and Katerina Fragkiadaki. Sfm- net: Learning of structure and motion from video. arXiv preprint arXiv:1704.07804, 2017. 2\n\nRobust estimation of 3d human poses from a single image. Chunyu Wang, Yizhou Wang, Zhouchen Lin, Alan L Yuille, Wen Gao, CVPR. Chunyu Wang, Yizhou Wang, Zhouchen Lin, Alan L Yuille, and Wen Gao. Robust estimation of 3d human poses from a single image. In CVPR, 2014. 2\n\nVideos as space-time region graphs. Xiaolong Wang, Abhinav Gupta, ECCV. 34Xiaolong Wang and Abhinav Gupta. Videos as space-time region graphs. In ECCV, 2018. 3, 4\n\nYue Wang, Yongbin Sun, Ziwei Liu, E Sanjay, Sarma, Justin M Michael M Bronstein, Solomon, arXiv:1801.07829Dynamic graph cnn for learning on point clouds. arXiv preprintYue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. arXiv preprint arXiv:1801.07829, 2018. 4\n\nRigid body motion prediction with planar non-convex contact patch. Jiayin Xie, Nilanjan Chakraborty, ICRA. Jiayin Xie and Nilanjan Chakraborty. Rigid body motion prediction with planar non-convex contact patch. In ICRA, 2019. 2\n\nYuliang Xiu, Jiefeng Li, Haoyu Wang, Yinghong Fang, Cewu Lu, arXiv:1802.00977Pose flow: Efficient online pose tracking. arXiv preprintYuliang Xiu, Jiefeng Li, Haoyu Wang, Yinghong Fang, and Cewu Lu. Pose flow: Efficient online pose tracking. arXiv preprint arXiv:1802.00977, 2018. 8\n\nSituation recognition: Visual semantic role labeling for image understanding. Mark Yatskar, Luke Zettlemoyer, Ali Farhadi, CVPR. Mark Yatskar, Luke Zettlemoyer, and Ali Farhadi. Situation recognition: Visual semantic role labeling for image under- standing. In CVPR, 2016. 2\n\nPredicting 3d human dynamics from video. Y Jason, Panna Zhang, Angjoo Felsen, Jitendra Kanazawa, Malik, ICCV. Jason Y Zhang, Panna Felsen, Angjoo Kanazawa, and Jiten- dra Malik. Predicting 3d human dynamics from video. In ICCV, 2019. 1\n", "annotations": {"author": "[{\"end\":150,\"start\":41},{\"end\":265,\"start\":151},{\"end\":377,\"start\":266},{\"end\":477,\"start\":378}]", "publisher": null, "author_last_name": "[{\"end\":53,\"start\":47},{\"end\":166,\"start\":158},{\"end\":280,\"start\":274},{\"end\":400,\"start\":387}]", "author_first_name": "[{\"end\":46,\"start\":41},{\"end\":157,\"start\":151},{\"end\":273,\"start\":266},{\"end\":386,\"start\":378}]", "author_affiliation": "[{\"end\":149,\"start\":75},{\"end\":264,\"start\":190},{\"end\":376,\"start\":302},{\"end\":476,\"start\":402}]", "title": "[{\"end\":38,\"start\":1},{\"end\":515,\"start\":478}]", "venue": null, "abstract": "[{\"end\":1907,\"start\":517}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2425,\"start\":2421},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2453,\"start\":2449},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2521,\"start\":2517},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2549,\"start\":2545},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2656,\"start\":2652},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2695,\"start\":2691},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2833,\"start\":2829},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2836,\"start\":2833},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2839,\"start\":2836},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2842,\"start\":2839},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2844,\"start\":2842},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2847,\"start\":2844},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2850,\"start\":2847},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2853,\"start\":2850},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2856,\"start\":2853},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":2859,\"start\":2856},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4734,\"start\":4730},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4867,\"start\":4863},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5183,\"start\":5179},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5930,\"start\":5926},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5933,\"start\":5930},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5936,\"start\":5933},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6037,\"start\":6034},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":6040,\"start\":6037},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6043,\"start\":6040},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":6046,\"start\":6043},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":6049,\"start\":6046},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":6052,\"start\":6049},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6055,\"start\":6052},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":6058,\"start\":6055},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6191,\"start\":6187},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6194,\"start\":6191},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6197,\"start\":6194},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6199,\"start\":6197},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6202,\"start\":6199},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6204,\"start\":6202},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6346,\"start\":6342},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":6665,\"start\":6661},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6971,\"start\":6967},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6973,\"start\":6971},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7014,\"start\":7011},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7017,\"start\":7014},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7020,\"start\":7017},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7023,\"start\":7020},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7107,\"start\":7103},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7199,\"start\":7195},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":7560,\"start\":7556},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7563,\"start\":7560},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7566,\"start\":7563},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7581,\"start\":7578},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7584,\"start\":7581},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7587,\"start\":7584},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7604,\"start\":7601},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7606,\"start\":7604},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7690,\"start\":7687},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":7693,\"start\":7690},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7696,\"start\":7693},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":7699,\"start\":7696},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7755,\"start\":7752},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8263,\"start\":8259},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8266,\"start\":8263},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8269,\"start\":8266},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8272,\"start\":8269},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8275,\"start\":8272},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8321,\"start\":8317},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":8324,\"start\":8321},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8327,\"start\":8324},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8360,\"start\":8356},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8363,\"start\":8360},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8366,\"start\":8363},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8541,\"start\":8537},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8852,\"start\":8848},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8855,\"start\":8852},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8858,\"start\":8855},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8861,\"start\":8858},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8864,\"start\":8861},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9067,\"start\":9063},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9624,\"start\":9620},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9708,\"start\":9704},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":9779,\"start\":9775},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9931,\"start\":9927},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11758,\"start\":11754},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11842,\"start\":11838},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13119,\"start\":13115},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":13602,\"start\":13598},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":13768,\"start\":13764},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":15624,\"start\":15620},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15800,\"start\":15796},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":15826,\"start\":15822},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":15863,\"start\":15859},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":17495,\"start\":17491},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":18071,\"start\":18067},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19643,\"start\":19639},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":19787,\"start\":19783},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19790,\"start\":19787},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":19793,\"start\":19790},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":20059,\"start\":20055},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":20293,\"start\":20289},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":21060,\"start\":21056},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":21453,\"start\":21449},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22309,\"start\":22305},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":22443,\"start\":22439},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":23433,\"start\":23429},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23436,\"start\":23433},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":23439,\"start\":23436},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":26567,\"start\":26563},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":29845,\"start\":29842},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29863,\"start\":29859},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":29866,\"start\":29863},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":31820,\"start\":31816}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31430,\"start\":31294},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32438,\"start\":31431},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32940,\"start\":32439},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":33096,\"start\":32941},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":33854,\"start\":33097},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":34312,\"start\":33855},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":34487,\"start\":34313}]", "paragraph": "[{\"end\":2550,\"start\":1923},{\"end\":3115,\"start\":2552},{\"end\":3729,\"start\":3117},{\"end\":4520,\"start\":3731},{\"end\":5836,\"start\":4522},{\"end\":7290,\"start\":5853},{\"end\":7453,\"start\":7292},{\"end\":7951,\"start\":7455},{\"end\":8730,\"start\":7953},{\"end\":9611,\"start\":8732},{\"end\":10071,\"start\":9613},{\"end\":10891,\"start\":10095},{\"end\":11179,\"start\":10893},{\"end\":11651,\"start\":11250},{\"end\":12023,\"start\":11686},{\"end\":12883,\"start\":12042},{\"end\":13327,\"start\":12885},{\"end\":13564,\"start\":13354},{\"end\":13935,\"start\":13566},{\"end\":14361,\"start\":13979},{\"end\":15265,\"start\":14396},{\"end\":15529,\"start\":15305},{\"end\":16086,\"start\":15531},{\"end\":16618,\"start\":16088},{\"end\":17120,\"start\":16620},{\"end\":17399,\"start\":17122},{\"end\":17636,\"start\":17426},{\"end\":18033,\"start\":17638},{\"end\":19714,\"start\":18035},{\"end\":20185,\"start\":19746},{\"end\":20743,\"start\":20187},{\"end\":21004,\"start\":20745},{\"end\":21375,\"start\":21006},{\"end\":22172,\"start\":21377},{\"end\":23323,\"start\":22174},{\"end\":24781,\"start\":23325},{\"end\":25203,\"start\":24813},{\"end\":25778,\"start\":25205},{\"end\":26441,\"start\":25780},{\"end\":26641,\"start\":26443},{\"end\":27217,\"start\":26643},{\"end\":27798,\"start\":27219},{\"end\":28942,\"start\":27800},{\"end\":29601,\"start\":28979},{\"end\":30736,\"start\":29625},{\"end\":31293,\"start\":30751}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11249,\"start\":11180},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13353,\"start\":13328},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13978,\"start\":13936},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14395,\"start\":14362},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15304,\"start\":15266}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":18912,\"start\":18450},{\"end\":18920,\"start\":18913},{\"end\":23932,\"start\":23900},{\"end\":24842,\"start\":24835},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":29073,\"start\":29066},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":30302,\"start\":30295}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1921,\"start\":1909},{\"attributes\":{\"n\":\"2.\"},\"end\":5851,\"start\":5839},{\"attributes\":{\"n\":\"3.\"},\"end\":10093,\"start\":10074},{\"attributes\":{\"n\":\"4.\"},\"end\":11662,\"start\":11654},{\"attributes\":{\"n\":\"4.1.\"},\"end\":11684,\"start\":11665},{\"attributes\":{\"n\":\"4.2.\"},\"end\":12040,\"start\":12026},{\"attributes\":{\"n\":\"5.\"},\"end\":17424,\"start\":17402},{\"attributes\":{\"n\":\"6.\"},\"end\":19728,\"start\":19717},{\"attributes\":{\"n\":\"6.1.\"},\"end\":19744,\"start\":19731},{\"attributes\":{\"n\":\"6.2.\"},\"end\":24811,\"start\":24784},{\"attributes\":{\"n\":\"6.3.\"},\"end\":28977,\"start\":28945},{\"attributes\":{\"n\":\"6.4.\"},\"end\":29623,\"start\":29604},{\"attributes\":{\"n\":\"7.\"},\"end\":30749,\"start\":30739},{\"end\":31305,\"start\":31295},{\"end\":31447,\"start\":31432},{\"end\":32951,\"start\":32942},{\"end\":33865,\"start\":33856},{\"end\":34323,\"start\":34314}]", "table": "[{\"end\":32438,\"start\":32397},{\"end\":32940,\"start\":32471},{\"end\":33854,\"start\":33115},{\"end\":34312,\"start\":34084}]", "figure_caption": "[{\"end\":31430,\"start\":31307},{\"end\":32397,\"start\":31453},{\"end\":32471,\"start\":32441},{\"end\":33096,\"start\":32953},{\"end\":33115,\"start\":33099},{\"end\":34084,\"start\":33867},{\"end\":34487,\"start\":34325}]", "figure_ref": "[{\"end\":5567,\"start\":5561},{\"end\":8888,\"start\":8880},{\"end\":11309,\"start\":11301},{\"end\":11800,\"start\":11792},{\"end\":16221,\"start\":16215},{\"end\":16689,\"start\":16683},{\"end\":23941,\"start\":23933},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26472,\"start\":26464},{\"end\":26676,\"start\":26668},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27639,\"start\":27631},{\"end\":27980,\"start\":27972},{\"end\":28941,\"start\":28933}]", "bib_author_first_name": "[{\"end\":34810,\"start\":34806},{\"end\":34824,\"start\":34818},{\"end\":34840,\"start\":34835},{\"end\":34987,\"start\":34981},{\"end\":35253,\"start\":35249},{\"end\":35267,\"start\":35263},{\"end\":35283,\"start\":35276},{\"end\":35512,\"start\":35507},{\"end\":35534,\"start\":35520},{\"end\":35561,\"start\":35552},{\"end\":35586,\"start\":35577},{\"end\":35873,\"start\":35865},{\"end\":35886,\"start\":35880},{\"end\":35906,\"start\":35897},{\"end\":35921,\"start\":35916},{\"end\":35936,\"start\":35930},{\"end\":35954,\"start\":35945},{\"end\":36244,\"start\":36235},{\"end\":36260,\"start\":36254},{\"end\":36471,\"start\":36468},{\"end\":36482,\"start\":36477},{\"end\":36497,\"start\":36492},{\"end\":36512,\"start\":36505},{\"end\":36523,\"start\":36518},{\"end\":36815,\"start\":36806},{\"end\":36827,\"start\":36823},{\"end\":36844,\"start\":36837},{\"end\":36862,\"start\":36855},{\"end\":36878,\"start\":36870},{\"end\":36890,\"start\":36884},{\"end\":36903,\"start\":36901},{\"end\":36915,\"start\":36910},{\"end\":36927,\"start\":36922},{\"end\":36939,\"start\":36935},{\"end\":37490,\"start\":37483},{\"end\":37504,\"start\":37497},{\"end\":37517,\"start\":37512},{\"end\":37524,\"start\":37523},{\"end\":37539,\"start\":37533},{\"end\":37855,\"start\":37848},{\"end\":37868,\"start\":37862},{\"end\":37887,\"start\":37879},{\"end\":37902,\"start\":37893},{\"end\":38254,\"start\":38249},{\"end\":38270,\"start\":38263},{\"end\":38283,\"start\":38278},{\"end\":38492,\"start\":38487},{\"end\":38507,\"start\":38501},{\"end\":38525,\"start\":38518},{\"end\":38542,\"start\":38534},{\"end\":38565,\"start\":38558},{\"end\":38810,\"start\":38804},{\"end\":38832,\"start\":38825},{\"end\":38842,\"start\":38838},{\"end\":39026,\"start\":39018},{\"end\":39046,\"start\":39040},{\"end\":39060,\"start\":39055},{\"end\":39077,\"start\":39069},{\"end\":39295,\"start\":39289},{\"end\":39306,\"start\":39303},{\"end\":39317,\"start\":39313},{\"end\":39330,\"start\":39325},{\"end\":39619,\"start\":39612},{\"end\":39634,\"start\":39630},{\"end\":39653,\"start\":39645},{\"end\":39818,\"start\":39815},{\"end\":39835,\"start\":39831},{\"end\":39856,\"start\":39851},{\"end\":39868,\"start\":39864},{\"end\":39878,\"start\":39873},{\"end\":39900,\"start\":39893},{\"end\":39913,\"start\":39908},{\"end\":39931,\"start\":39925},{\"end\":40198,\"start\":40189},{\"end\":40209,\"start\":40208},{\"end\":40224,\"start\":40217},{\"end\":40687,\"start\":40679},{\"end\":40704,\"start\":40697},{\"end\":40721,\"start\":40713},{\"end\":40723,\"start\":40722},{\"end\":40734,\"start\":40729},{\"end\":40751,\"start\":40744},{\"end\":40999,\"start\":40991},{\"end\":41022,\"start\":41015},{\"end\":41243,\"start\":41236},{\"end\":41255,\"start\":41248},{\"end\":41271,\"start\":41263},{\"end\":41281,\"start\":41277},{\"end\":41534,\"start\":41530},{\"end\":41550,\"start\":41543},{\"end\":41561,\"start\":41558},{\"end\":41799,\"start\":41796},{\"end\":41811,\"start\":41804},{\"end\":41821,\"start\":41816},{\"end\":41835,\"start\":41829},{\"end\":41847,\"start\":41841},{\"end\":42094,\"start\":42087},{\"end\":42110,\"start\":42104},{\"end\":42123,\"start\":42119},{\"end\":42139,\"start\":42131},{\"end\":42416,\"start\":42410},{\"end\":42429,\"start\":42423},{\"end\":42452,\"start\":42444},{\"end\":42721,\"start\":42712},{\"end\":42731,\"start\":42727},{\"end\":42743,\"start\":42738},{\"end\":42747,\"start\":42744},{\"end\":42773,\"start\":42765},{\"end\":43021,\"start\":43020},{\"end\":43037,\"start\":43032},{\"end\":43263,\"start\":43262},{\"end\":43275,\"start\":43272},{\"end\":43547,\"start\":43541},{\"end\":43577,\"start\":43569},{\"end\":43829,\"start\":43828},{\"end\":43844,\"start\":43836},{\"end\":44030,\"start\":44025},{\"end\":44045,\"start\":44038},{\"end\":44064,\"start\":44056},{\"end\":44273,\"start\":44266},{\"end\":44287,\"start\":44283},{\"end\":44302,\"start\":44299},{\"end\":44601,\"start\":44593},{\"end\":44622,\"start\":44614},{\"end\":44639,\"start\":44628},{\"end\":45037,\"start\":45032},{\"end\":45049,\"start\":45043},{\"end\":45062,\"start\":45058},{\"end\":45079,\"start\":45070},{\"end\":45295,\"start\":45290},{\"end\":45308,\"start\":45300},{\"end\":45324,\"start\":45318},{\"end\":45336,\"start\":45331},{\"end\":45348,\"start\":45342},{\"end\":45363,\"start\":45358},{\"end\":45614,\"start\":45607},{\"end\":45625,\"start\":45619},{\"end\":45637,\"start\":45632},{\"end\":45650,\"start\":45645},{\"end\":45657,\"start\":45655},{\"end\":45669,\"start\":45662},{\"end\":45684,\"start\":45676},{\"end\":45695,\"start\":45691},{\"end\":46054,\"start\":46050},{\"end\":46067,\"start\":46060},{\"end\":46082,\"start\":46074},{\"end\":46094,\"start\":46089},{\"end\":46372,\"start\":46364},{\"end\":46381,\"start\":46377},{\"end\":46393,\"start\":46389},{\"end\":46407,\"start\":46402},{\"end\":46421,\"start\":46414},{\"end\":46435,\"start\":46431},{\"end\":46441,\"start\":46436},{\"end\":46691,\"start\":46682},{\"end\":46705,\"start\":46701},{\"end\":46722,\"start\":46716},{\"end\":46735,\"start\":46727},{\"end\":46753,\"start\":46748},{\"end\":46987,\"start\":46979},{\"end\":47235,\"start\":47232},{\"end\":47249,\"start\":47241},{\"end\":47262,\"start\":47255},{\"end\":47281,\"start\":47273},{\"end\":47499,\"start\":47492},{\"end\":47511,\"start\":47510},{\"end\":47527,\"start\":47521},{\"end\":47788,\"start\":47780},{\"end\":47981,\"start\":47972},{\"end\":47993,\"start\":47990},{\"end\":48187,\"start\":48179},{\"end\":48200,\"start\":48192},{\"end\":48215,\"start\":48207},{\"end\":48467,\"start\":48462},{\"end\":48484,\"start\":48475},{\"end\":48502,\"start\":48496},{\"end\":48515,\"start\":48509},{\"end\":48836,\"start\":48831},{\"end\":48851,\"start\":48846},{\"end\":48855,\"start\":48852},{\"end\":48867,\"start\":48863},{\"end\":48885,\"start\":48878},{\"end\":48898,\"start\":48891},{\"end\":48912,\"start\":48909},{\"end\":48928,\"start\":48922},{\"end\":49316,\"start\":49308},{\"end\":49334,\"start\":49327},{\"end\":49342,\"start\":49341},{\"end\":49363,\"start\":49357},{\"end\":49633,\"start\":49628},{\"end\":49647,\"start\":49642},{\"end\":49665,\"start\":49658},{\"end\":49944,\"start\":49938},{\"end\":49961,\"start\":49955},{\"end\":49977,\"start\":49969},{\"end\":50001,\"start\":49993},{\"end\":50238,\"start\":50232},{\"end\":50254,\"start\":50249},{\"end\":50274,\"start\":50270},{\"end\":50288,\"start\":50281},{\"end\":50307,\"start\":50299},{\"end\":50571,\"start\":50565},{\"end\":50583,\"start\":50576},{\"end\":50598,\"start\":50590},{\"end\":50612,\"start\":50604},{\"end\":50628,\"start\":50619},{\"end\":50872,\"start\":50863},{\"end\":50896,\"start\":50889},{\"end\":50911,\"start\":50903},{\"end\":51146,\"start\":51141},{\"end\":51166,\"start\":51159},{\"end\":51182,\"start\":51177},{\"end\":51199,\"start\":51191},{\"end\":51450,\"start\":51445},{\"end\":51468,\"start\":51463},{\"end\":51484,\"start\":51477},{\"end\":51498,\"start\":51493},{\"end\":51515,\"start\":51507},{\"end\":51776,\"start\":51771},{\"end\":51794,\"start\":51789},{\"end\":51811,\"start\":51803},{\"end\":51965,\"start\":51960},{\"end\":51985,\"start\":51978},{\"end\":52003,\"start\":51996},{\"end\":52021,\"start\":52014},{\"end\":52036,\"start\":52030},{\"end\":52048,\"start\":52042},{\"end\":52294,\"start\":52284},{\"end\":52320,\"start\":52313},{\"end\":52336,\"start\":52328},{\"end\":52350,\"start\":52345},{\"end\":52371,\"start\":52363},{\"end\":52733,\"start\":52727},{\"end\":52746,\"start\":52740},{\"end\":52761,\"start\":52753},{\"end\":52771,\"start\":52767},{\"end\":52773,\"start\":52772},{\"end\":52785,\"start\":52782},{\"end\":52984,\"start\":52976},{\"end\":52998,\"start\":52991},{\"end\":53107,\"start\":53104},{\"end\":53121,\"start\":53114},{\"end\":53132,\"start\":53127},{\"end\":53139,\"start\":53138},{\"end\":53161,\"start\":53155},{\"end\":53163,\"start\":53162},{\"end\":53528,\"start\":53522},{\"end\":53542,\"start\":53534},{\"end\":53691,\"start\":53684},{\"end\":53704,\"start\":53697},{\"end\":53714,\"start\":53709},{\"end\":53729,\"start\":53721},{\"end\":53740,\"start\":53736},{\"end\":54050,\"start\":54046},{\"end\":54064,\"start\":54060},{\"end\":54081,\"start\":54078},{\"end\":54286,\"start\":54285},{\"end\":54299,\"start\":54294},{\"end\":54313,\"start\":54307},{\"end\":54330,\"start\":54322}]", "bib_author_last_name": "[{\"end\":34816,\"start\":34811},{\"end\":34833,\"start\":34825},{\"end\":34849,\"start\":34841},{\"end\":34996,\"start\":34988},{\"end\":35261,\"start\":35254},{\"end\":35274,\"start\":35268},{\"end\":35287,\"start\":35284},{\"end\":35518,\"start\":35513},{\"end\":35550,\"start\":35535},{\"end\":35575,\"start\":35562},{\"end\":35596,\"start\":35587},{\"end\":35878,\"start\":35874},{\"end\":35895,\"start\":35887},{\"end\":35914,\"start\":35907},{\"end\":35928,\"start\":35922},{\"end\":35943,\"start\":35937},{\"end\":35960,\"start\":35955},{\"end\":36252,\"start\":36245},{\"end\":36264,\"start\":36261},{\"end\":36475,\"start\":36472},{\"end\":36490,\"start\":36483},{\"end\":36503,\"start\":36498},{\"end\":36516,\"start\":36513},{\"end\":36530,\"start\":36524},{\"end\":36821,\"start\":36816},{\"end\":36835,\"start\":36828},{\"end\":36853,\"start\":36845},{\"end\":36868,\"start\":36863},{\"end\":36882,\"start\":36879},{\"end\":36899,\"start\":36891},{\"end\":36908,\"start\":36904},{\"end\":36920,\"start\":36916},{\"end\":36933,\"start\":36928},{\"end\":36947,\"start\":36940},{\"end\":37495,\"start\":37491},{\"end\":37510,\"start\":37505},{\"end\":37521,\"start\":37518},{\"end\":37531,\"start\":37525},{\"end\":37550,\"start\":37540},{\"end\":37554,\"start\":37552},{\"end\":37860,\"start\":37856},{\"end\":37877,\"start\":37869},{\"end\":37891,\"start\":37888},{\"end\":37906,\"start\":37903},{\"end\":38261,\"start\":38255},{\"end\":38276,\"start\":38271},{\"end\":38290,\"start\":38284},{\"end\":38499,\"start\":38493},{\"end\":38516,\"start\":38508},{\"end\":38532,\"start\":38526},{\"end\":38556,\"start\":38543},{\"end\":38571,\"start\":38566},{\"end\":38823,\"start\":38811},{\"end\":38836,\"start\":38833},{\"end\":38846,\"start\":38843},{\"end\":38850,\"start\":38848},{\"end\":39038,\"start\":39027},{\"end\":39053,\"start\":39047},{\"end\":39067,\"start\":39061},{\"end\":39083,\"start\":39078},{\"end\":39301,\"start\":39296},{\"end\":39311,\"start\":39307},{\"end\":39323,\"start\":39318},{\"end\":39339,\"start\":39331},{\"end\":39628,\"start\":39620},{\"end\":39643,\"start\":39635},{\"end\":39659,\"start\":39654},{\"end\":39829,\"start\":39819},{\"end\":39849,\"start\":39836},{\"end\":39862,\"start\":39857},{\"end\":39871,\"start\":39869},{\"end\":39891,\"start\":39879},{\"end\":39906,\"start\":39901},{\"end\":39923,\"start\":39914},{\"end\":39938,\"start\":39932},{\"end\":40206,\"start\":40199},{\"end\":40215,\"start\":40210},{\"end\":40229,\"start\":40225},{\"end\":40238,\"start\":40231},{\"end\":40695,\"start\":40688},{\"end\":40711,\"start\":40705},{\"end\":40727,\"start\":40724},{\"end\":40742,\"start\":40735},{\"end\":40757,\"start\":40752},{\"end\":41013,\"start\":41000},{\"end\":41027,\"start\":41023},{\"end\":41034,\"start\":41029},{\"end\":41041,\"start\":41036},{\"end\":41246,\"start\":41244},{\"end\":41261,\"start\":41256},{\"end\":41275,\"start\":41272},{\"end\":41285,\"start\":41282},{\"end\":41541,\"start\":41535},{\"end\":41556,\"start\":41551},{\"end\":41569,\"start\":41562},{\"end\":41802,\"start\":41800},{\"end\":41814,\"start\":41812},{\"end\":41827,\"start\":41822},{\"end\":41839,\"start\":41836},{\"end\":41851,\"start\":41848},{\"end\":42102,\"start\":42095},{\"end\":42117,\"start\":42111},{\"end\":42129,\"start\":42124},{\"end\":42152,\"start\":42140},{\"end\":42421,\"start\":42417},{\"end\":42442,\"start\":42430},{\"end\":42461,\"start\":42453},{\"end\":42469,\"start\":42463},{\"end\":42725,\"start\":42722},{\"end\":42736,\"start\":42732},{\"end\":42751,\"start\":42748},{\"end\":42763,\"start\":42753},{\"end\":42779,\"start\":42774},{\"end\":42784,\"start\":42781},{\"end\":43030,\"start\":43022},{\"end\":43044,\"start\":43038},{\"end\":43048,\"start\":43046},{\"end\":43270,\"start\":43264},{\"end\":43280,\"start\":43276},{\"end\":43289,\"start\":43282},{\"end\":43567,\"start\":43548},{\"end\":43583,\"start\":43578},{\"end\":43591,\"start\":43585},{\"end\":43834,\"start\":43830},{\"end\":43852,\"start\":43845},{\"end\":43860,\"start\":43854},{\"end\":44036,\"start\":44031},{\"end\":44054,\"start\":44046},{\"end\":44071,\"start\":44065},{\"end\":44281,\"start\":44274},{\"end\":44297,\"start\":44288},{\"end\":44312,\"start\":44303},{\"end\":44612,\"start\":44602},{\"end\":44626,\"start\":44623},{\"end\":44644,\"start\":44640},{\"end\":45041,\"start\":45038},{\"end\":45056,\"start\":45050},{\"end\":45068,\"start\":45063},{\"end\":45085,\"start\":45080},{\"end\":45298,\"start\":45296},{\"end\":45316,\"start\":45309},{\"end\":45329,\"start\":45325},{\"end\":45340,\"start\":45337},{\"end\":45356,\"start\":45349},{\"end\":45370,\"start\":45364},{\"end\":45617,\"start\":45615},{\"end\":45630,\"start\":45626},{\"end\":45643,\"start\":45638},{\"end\":45653,\"start\":45651},{\"end\":45660,\"start\":45658},{\"end\":45674,\"start\":45670},{\"end\":45689,\"start\":45685},{\"end\":45698,\"start\":45696},{\"end\":46058,\"start\":46055},{\"end\":46072,\"start\":46068},{\"end\":46087,\"start\":46083},{\"end\":46099,\"start\":46095},{\"end\":46375,\"start\":46373},{\"end\":46387,\"start\":46382},{\"end\":46400,\"start\":46394},{\"end\":46412,\"start\":46408},{\"end\":46429,\"start\":46422},{\"end\":46446,\"start\":46442},{\"end\":46699,\"start\":46692},{\"end\":46714,\"start\":46706},{\"end\":46725,\"start\":46723},{\"end\":46746,\"start\":46736},{\"end\":46760,\"start\":46754},{\"end\":46996,\"start\":46988},{\"end\":47026,\"start\":46998},{\"end\":47239,\"start\":47236},{\"end\":47253,\"start\":47250},{\"end\":47271,\"start\":47263},{\"end\":47284,\"start\":47282},{\"end\":47508,\"start\":47500},{\"end\":47519,\"start\":47512},{\"end\":47533,\"start\":47528},{\"end\":47541,\"start\":47535},{\"end\":47802,\"start\":47789},{\"end\":47988,\"start\":47982},{\"end\":47998,\"start\":47994},{\"end\":48190,\"start\":48188},{\"end\":48205,\"start\":48201},{\"end\":48219,\"start\":48216},{\"end\":48473,\"start\":48468},{\"end\":48494,\"start\":48485},{\"end\":48507,\"start\":48503},{\"end\":48523,\"start\":48516},{\"end\":48533,\"start\":48525},{\"end\":48844,\"start\":48837},{\"end\":48861,\"start\":48856},{\"end\":48876,\"start\":48868},{\"end\":48889,\"start\":48886},{\"end\":48907,\"start\":48899},{\"end\":48920,\"start\":48913},{\"end\":48937,\"start\":48929},{\"end\":49325,\"start\":49317},{\"end\":49339,\"start\":49335},{\"end\":49355,\"start\":49343},{\"end\":49372,\"start\":49364},{\"end\":49384,\"start\":49374},{\"end\":49640,\"start\":49634},{\"end\":49656,\"start\":49648},{\"end\":49670,\"start\":49666},{\"end\":49953,\"start\":49945},{\"end\":49967,\"start\":49962},{\"end\":49991,\"start\":49978},{\"end\":50009,\"start\":50002},{\"end\":50247,\"start\":50239},{\"end\":50268,\"start\":50255},{\"end\":50279,\"start\":50275},{\"end\":50297,\"start\":50289},{\"end\":50321,\"start\":50308},{\"end\":50574,\"start\":50572},{\"end\":50588,\"start\":50584},{\"end\":50602,\"start\":50599},{\"end\":50617,\"start\":50613},{\"end\":50632,\"start\":50629},{\"end\":50887,\"start\":50873},{\"end\":50901,\"start\":50897},{\"end\":50925,\"start\":50912},{\"end\":51157,\"start\":51147},{\"end\":51175,\"start\":51167},{\"end\":51189,\"start\":51183},{\"end\":51213,\"start\":51200},{\"end\":51461,\"start\":51451},{\"end\":51475,\"start\":51469},{\"end\":51491,\"start\":51485},{\"end\":51505,\"start\":51499},{\"end\":51529,\"start\":51516},{\"end\":51787,\"start\":51777},{\"end\":51801,\"start\":51795},{\"end\":51825,\"start\":51812},{\"end\":51976,\"start\":51966},{\"end\":51994,\"start\":51986},{\"end\":52012,\"start\":52004},{\"end\":52028,\"start\":52022},{\"end\":52040,\"start\":52037},{\"end\":52055,\"start\":52049},{\"end\":52311,\"start\":52295},{\"end\":52326,\"start\":52321},{\"end\":52343,\"start\":52337},{\"end\":52361,\"start\":52351},{\"end\":52383,\"start\":52372},{\"end\":52738,\"start\":52734},{\"end\":52751,\"start\":52747},{\"end\":52765,\"start\":52762},{\"end\":52780,\"start\":52774},{\"end\":52789,\"start\":52786},{\"end\":52989,\"start\":52985},{\"end\":53004,\"start\":52999},{\"end\":53112,\"start\":53108},{\"end\":53125,\"start\":53122},{\"end\":53136,\"start\":53133},{\"end\":53146,\"start\":53140},{\"end\":53153,\"start\":53148},{\"end\":53183,\"start\":53164},{\"end\":53192,\"start\":53185},{\"end\":53532,\"start\":53529},{\"end\":53554,\"start\":53543},{\"end\":53695,\"start\":53692},{\"end\":53707,\"start\":53705},{\"end\":53719,\"start\":53715},{\"end\":53734,\"start\":53730},{\"end\":53743,\"start\":53741},{\"end\":54058,\"start\":54051},{\"end\":54076,\"start\":54065},{\"end\":54089,\"start\":54082},{\"end\":54292,\"start\":54287},{\"end\":54305,\"start\":54300},{\"end\":54320,\"start\":54314},{\"end\":54339,\"start\":54331},{\"end\":54346,\"start\":54341}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":204800967},\"end\":34979,\"start\":34751},{\"attributes\":{\"doi\":\"arXiv:1701.07875\",\"id\":\"b1\"},\"end\":35189,\"start\":34981},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":22857558},\"end\":35436,\"start\":35191},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":147703955},\"end\":35780,\"start\":35438},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":13438951},\"end\":36168,\"start\":35782},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":142734},\"end\":36387,\"start\":36170},{\"attributes\":{\"doi\":\"arXiv:1812.08008\",\"id\":\"b6\"},\"end\":36749,\"start\":36389},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":198162846},\"end\":37390,\"start\":36751},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":10236420},\"end\":37846,\"start\":37392},{\"attributes\":{\"doi\":\"arXiv:1811.12814\",\"id\":\"b9\"},\"end\":38193,\"start\":37848},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":53113646},\"end\":38417,\"start\":38195},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":219962806},\"end\":38757,\"start\":38419},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6529517},\"end\":38971,\"start\":38759},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":128024},\"end\":39231,\"start\":38973},{\"attributes\":{\"doi\":\"2017. 1\",\"id\":\"b14\",\"matched_paper_id\":13549534},\"end\":39564,\"start\":39233},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":2863739},\"end\":39784,\"start\":39566},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1033682},\"end\":40120,\"start\":39786},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4495230},\"end\":40608,\"start\":40122},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":208028203},\"end\":40937,\"start\":40610},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":52965638},\"end\":41188,\"start\":40939},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":206594692},\"end\":41410,\"start\":41190},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3607155},\"end\":41754,\"start\":41412},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":37158713},\"end\":41976,\"start\":41756},{\"attributes\":{\"id\":\"b23\"},\"end\":42351,\"start\":41978},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":563473},\"end\":42623,\"start\":42353},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":207971107},\"end\":42974,\"start\":42625},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b26\"},\"end\":43194,\"start\":42976},{\"attributes\":{\"doi\":\"arXiv:1609.02907\",\"id\":\"b27\"},\"end\":43465,\"start\":43196},{\"attributes\":{\"id\":\"b28\"},\"end\":43740,\"start\":43467},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1121245},\"end\":44008,\"start\":43742},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2063215},\"end\":44170,\"start\":44010},{\"attributes\":{\"doi\":\"arXiv:1906.12279\",\"id\":\"b31\"},\"end\":44534,\"start\":44172},{\"attributes\":{\"doi\":\"arXiv:1812.02591\",\"id\":\"b32\"},\"end\":44832,\"start\":44536},{\"attributes\":{\"id\":\"b33\"},\"end\":44962,\"start\":44834},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":13117003},\"end\":45238,\"start\":44964},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":9763852},\"end\":45530,\"start\":45240},{\"attributes\":{\"doi\":\"arXiv:1811.08264\",\"id\":\"b36\"},\"end\":45946,\"start\":45532},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":49557311},\"end\":46284,\"start\":45948},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":3995697},\"end\":46638,\"start\":46286},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":7546733},\"end\":46911,\"start\":46640},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":59291944},\"end\":47168,\"start\":46913},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":199668903},\"end\":47430,\"start\":47170},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":645845},\"end\":47701,\"start\":47432},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":17035965},\"end\":47927,\"start\":47703},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":4409978},\"end\":48098,\"start\":47929},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":17105663},\"end\":48372,\"start\":48100},{\"attributes\":{\"id\":\"b46\"},\"end\":48709,\"start\":48374},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":46941272},\"end\":49237,\"start\":48711},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":5417293},\"end\":49562,\"start\":49239},{\"attributes\":{\"doi\":\"arXiv:1805.06485\",\"id\":\"b49\"},\"end\":49861,\"start\":49564},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":209377045},\"end\":50189,\"start\":49863},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":104292265},\"end\":50494,\"start\":50191},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":51992868},\"end\":50806,\"start\":50496},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":55220882},\"end\":51074,\"start\":50808},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":1795159},\"end\":51380,\"start\":51076},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":3044692},\"end\":51708,\"start\":51382},{\"attributes\":{\"id\":\"b56\"},\"end\":51958,\"start\":51710},{\"attributes\":{\"doi\":\"arXiv:1710.10903\",\"id\":\"b57\"},\"end\":52282,\"start\":51960},{\"attributes\":{\"doi\":\"arXiv:1704.07804\",\"id\":\"b58\"},\"end\":52668,\"start\":52284},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":40236},\"end\":52938,\"start\":52670},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":46940850},\"end\":53102,\"start\":52940},{\"attributes\":{\"doi\":\"arXiv:1801.07829\",\"id\":\"b61\"},\"end\":53453,\"start\":53104},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":118681256},\"end\":53682,\"start\":53455},{\"attributes\":{\"doi\":\"arXiv:1802.00977\",\"id\":\"b63\"},\"end\":53966,\"start\":53684},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":2424223},\"end\":54242,\"start\":53968},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":199551951},\"end\":54479,\"start\":54244}]", "bib_title": "[{\"end\":34804,\"start\":34751},{\"end\":35247,\"start\":35191},{\"end\":35505,\"start\":35438},{\"end\":35863,\"start\":35782},{\"end\":36233,\"start\":36170},{\"end\":36804,\"start\":36751},{\"end\":37481,\"start\":37392},{\"end\":38247,\"start\":38195},{\"end\":38485,\"start\":38419},{\"end\":38802,\"start\":38759},{\"end\":39016,\"start\":38973},{\"end\":39287,\"start\":39233},{\"end\":39610,\"start\":39566},{\"end\":39813,\"start\":39786},{\"end\":40187,\"start\":40122},{\"end\":40677,\"start\":40610},{\"end\":40989,\"start\":40939},{\"end\":41234,\"start\":41190},{\"end\":41528,\"start\":41412},{\"end\":41794,\"start\":41756},{\"end\":42408,\"start\":42353},{\"end\":42710,\"start\":42625},{\"end\":43826,\"start\":43742},{\"end\":44023,\"start\":44010},{\"end\":45030,\"start\":44964},{\"end\":45288,\"start\":45240},{\"end\":46048,\"start\":45948},{\"end\":46362,\"start\":46286},{\"end\":46680,\"start\":46640},{\"end\":46977,\"start\":46913},{\"end\":47230,\"start\":47170},{\"end\":47490,\"start\":47432},{\"end\":47778,\"start\":47703},{\"end\":47970,\"start\":47929},{\"end\":48177,\"start\":48100},{\"end\":48829,\"start\":48711},{\"end\":49306,\"start\":49239},{\"end\":49936,\"start\":49863},{\"end\":50230,\"start\":50191},{\"end\":50563,\"start\":50496},{\"end\":50861,\"start\":50808},{\"end\":51139,\"start\":51076},{\"end\":51443,\"start\":51382},{\"end\":52725,\"start\":52670},{\"end\":52974,\"start\":52940},{\"end\":53520,\"start\":53455},{\"end\":54044,\"start\":53968},{\"end\":54283,\"start\":54244}]", "bib_author": "[{\"end\":34818,\"start\":34806},{\"end\":34835,\"start\":34818},{\"end\":34851,\"start\":34835},{\"end\":34998,\"start\":34981},{\"end\":35263,\"start\":35249},{\"end\":35276,\"start\":35263},{\"end\":35289,\"start\":35276},{\"end\":35520,\"start\":35507},{\"end\":35552,\"start\":35520},{\"end\":35577,\"start\":35552},{\"end\":35598,\"start\":35577},{\"end\":35880,\"start\":35865},{\"end\":35897,\"start\":35880},{\"end\":35916,\"start\":35897},{\"end\":35930,\"start\":35916},{\"end\":35945,\"start\":35930},{\"end\":35962,\"start\":35945},{\"end\":36254,\"start\":36235},{\"end\":36266,\"start\":36254},{\"end\":36477,\"start\":36468},{\"end\":36492,\"start\":36477},{\"end\":36505,\"start\":36492},{\"end\":36518,\"start\":36505},{\"end\":36532,\"start\":36518},{\"end\":36823,\"start\":36806},{\"end\":36837,\"start\":36823},{\"end\":36855,\"start\":36837},{\"end\":36870,\"start\":36855},{\"end\":36884,\"start\":36870},{\"end\":36901,\"start\":36884},{\"end\":36910,\"start\":36901},{\"end\":36922,\"start\":36910},{\"end\":36935,\"start\":36922},{\"end\":36949,\"start\":36935},{\"end\":37497,\"start\":37483},{\"end\":37512,\"start\":37497},{\"end\":37523,\"start\":37512},{\"end\":37533,\"start\":37523},{\"end\":37552,\"start\":37533},{\"end\":37556,\"start\":37552},{\"end\":37862,\"start\":37848},{\"end\":37879,\"start\":37862},{\"end\":37893,\"start\":37879},{\"end\":37908,\"start\":37893},{\"end\":38263,\"start\":38249},{\"end\":38278,\"start\":38263},{\"end\":38292,\"start\":38278},{\"end\":38501,\"start\":38487},{\"end\":38518,\"start\":38501},{\"end\":38534,\"start\":38518},{\"end\":38558,\"start\":38534},{\"end\":38573,\"start\":38558},{\"end\":38825,\"start\":38804},{\"end\":38838,\"start\":38825},{\"end\":38848,\"start\":38838},{\"end\":38852,\"start\":38848},{\"end\":39040,\"start\":39018},{\"end\":39055,\"start\":39040},{\"end\":39069,\"start\":39055},{\"end\":39085,\"start\":39069},{\"end\":39303,\"start\":39289},{\"end\":39313,\"start\":39303},{\"end\":39325,\"start\":39313},{\"end\":39341,\"start\":39325},{\"end\":39630,\"start\":39612},{\"end\":39645,\"start\":39630},{\"end\":39661,\"start\":39645},{\"end\":39831,\"start\":39815},{\"end\":39851,\"start\":39831},{\"end\":39864,\"start\":39851},{\"end\":39873,\"start\":39864},{\"end\":39893,\"start\":39873},{\"end\":39908,\"start\":39893},{\"end\":39925,\"start\":39908},{\"end\":39940,\"start\":39925},{\"end\":40208,\"start\":40189},{\"end\":40217,\"start\":40208},{\"end\":40231,\"start\":40217},{\"end\":40240,\"start\":40231},{\"end\":40697,\"start\":40679},{\"end\":40713,\"start\":40697},{\"end\":40729,\"start\":40713},{\"end\":40744,\"start\":40729},{\"end\":40759,\"start\":40744},{\"end\":41015,\"start\":40991},{\"end\":41029,\"start\":41015},{\"end\":41036,\"start\":41029},{\"end\":41043,\"start\":41036},{\"end\":41248,\"start\":41236},{\"end\":41263,\"start\":41248},{\"end\":41277,\"start\":41263},{\"end\":41287,\"start\":41277},{\"end\":41543,\"start\":41530},{\"end\":41558,\"start\":41543},{\"end\":41571,\"start\":41558},{\"end\":41804,\"start\":41796},{\"end\":41816,\"start\":41804},{\"end\":41829,\"start\":41816},{\"end\":41841,\"start\":41829},{\"end\":41853,\"start\":41841},{\"end\":42104,\"start\":42087},{\"end\":42119,\"start\":42104},{\"end\":42131,\"start\":42119},{\"end\":42154,\"start\":42131},{\"end\":42423,\"start\":42410},{\"end\":42444,\"start\":42423},{\"end\":42463,\"start\":42444},{\"end\":42471,\"start\":42463},{\"end\":42727,\"start\":42712},{\"end\":42738,\"start\":42727},{\"end\":42753,\"start\":42738},{\"end\":42765,\"start\":42753},{\"end\":42781,\"start\":42765},{\"end\":42786,\"start\":42781},{\"end\":43032,\"start\":43020},{\"end\":43046,\"start\":43032},{\"end\":43050,\"start\":43046},{\"end\":43272,\"start\":43262},{\"end\":43282,\"start\":43272},{\"end\":43291,\"start\":43282},{\"end\":43569,\"start\":43541},{\"end\":43585,\"start\":43569},{\"end\":43593,\"start\":43585},{\"end\":43836,\"start\":43828},{\"end\":43854,\"start\":43836},{\"end\":43862,\"start\":43854},{\"end\":44038,\"start\":44025},{\"end\":44056,\"start\":44038},{\"end\":44073,\"start\":44056},{\"end\":44283,\"start\":44266},{\"end\":44299,\"start\":44283},{\"end\":44314,\"start\":44299},{\"end\":44614,\"start\":44593},{\"end\":44628,\"start\":44614},{\"end\":44646,\"start\":44628},{\"end\":45043,\"start\":45032},{\"end\":45058,\"start\":45043},{\"end\":45070,\"start\":45058},{\"end\":45087,\"start\":45070},{\"end\":45300,\"start\":45290},{\"end\":45318,\"start\":45300},{\"end\":45331,\"start\":45318},{\"end\":45342,\"start\":45331},{\"end\":45358,\"start\":45342},{\"end\":45372,\"start\":45358},{\"end\":45619,\"start\":45607},{\"end\":45632,\"start\":45619},{\"end\":45645,\"start\":45632},{\"end\":45655,\"start\":45645},{\"end\":45662,\"start\":45655},{\"end\":45676,\"start\":45662},{\"end\":45691,\"start\":45676},{\"end\":45700,\"start\":45691},{\"end\":46060,\"start\":46050},{\"end\":46074,\"start\":46060},{\"end\":46089,\"start\":46074},{\"end\":46101,\"start\":46089},{\"end\":46377,\"start\":46364},{\"end\":46389,\"start\":46377},{\"end\":46402,\"start\":46389},{\"end\":46414,\"start\":46402},{\"end\":46431,\"start\":46414},{\"end\":46448,\"start\":46431},{\"end\":46701,\"start\":46682},{\"end\":46716,\"start\":46701},{\"end\":46727,\"start\":46716},{\"end\":46748,\"start\":46727},{\"end\":46762,\"start\":46748},{\"end\":46998,\"start\":46979},{\"end\":47028,\"start\":46998},{\"end\":47241,\"start\":47232},{\"end\":47255,\"start\":47241},{\"end\":47273,\"start\":47255},{\"end\":47286,\"start\":47273},{\"end\":47510,\"start\":47492},{\"end\":47521,\"start\":47510},{\"end\":47535,\"start\":47521},{\"end\":47543,\"start\":47535},{\"end\":47804,\"start\":47780},{\"end\":47990,\"start\":47972},{\"end\":48000,\"start\":47990},{\"end\":48192,\"start\":48179},{\"end\":48207,\"start\":48192},{\"end\":48221,\"start\":48207},{\"end\":48475,\"start\":48462},{\"end\":48496,\"start\":48475},{\"end\":48509,\"start\":48496},{\"end\":48525,\"start\":48509},{\"end\":48535,\"start\":48525},{\"end\":48846,\"start\":48831},{\"end\":48863,\"start\":48846},{\"end\":48878,\"start\":48863},{\"end\":48891,\"start\":48878},{\"end\":48909,\"start\":48891},{\"end\":48922,\"start\":48909},{\"end\":48939,\"start\":48922},{\"end\":49327,\"start\":49308},{\"end\":49341,\"start\":49327},{\"end\":49357,\"start\":49341},{\"end\":49374,\"start\":49357},{\"end\":49386,\"start\":49374},{\"end\":49642,\"start\":49628},{\"end\":49658,\"start\":49642},{\"end\":49672,\"start\":49658},{\"end\":49955,\"start\":49938},{\"end\":49969,\"start\":49955},{\"end\":49993,\"start\":49969},{\"end\":50011,\"start\":49993},{\"end\":50249,\"start\":50232},{\"end\":50270,\"start\":50249},{\"end\":50281,\"start\":50270},{\"end\":50299,\"start\":50281},{\"end\":50323,\"start\":50299},{\"end\":50576,\"start\":50565},{\"end\":50590,\"start\":50576},{\"end\":50604,\"start\":50590},{\"end\":50619,\"start\":50604},{\"end\":50634,\"start\":50619},{\"end\":50889,\"start\":50863},{\"end\":50903,\"start\":50889},{\"end\":50927,\"start\":50903},{\"end\":51159,\"start\":51141},{\"end\":51177,\"start\":51159},{\"end\":51191,\"start\":51177},{\"end\":51215,\"start\":51191},{\"end\":51463,\"start\":51445},{\"end\":51477,\"start\":51463},{\"end\":51493,\"start\":51477},{\"end\":51507,\"start\":51493},{\"end\":51531,\"start\":51507},{\"end\":51789,\"start\":51771},{\"end\":51803,\"start\":51789},{\"end\":51827,\"start\":51803},{\"end\":51978,\"start\":51960},{\"end\":51996,\"start\":51978},{\"end\":52014,\"start\":51996},{\"end\":52030,\"start\":52014},{\"end\":52042,\"start\":52030},{\"end\":52057,\"start\":52042},{\"end\":52313,\"start\":52284},{\"end\":52328,\"start\":52313},{\"end\":52345,\"start\":52328},{\"end\":52363,\"start\":52345},{\"end\":52385,\"start\":52363},{\"end\":52740,\"start\":52727},{\"end\":52753,\"start\":52740},{\"end\":52767,\"start\":52753},{\"end\":52782,\"start\":52767},{\"end\":52791,\"start\":52782},{\"end\":52991,\"start\":52976},{\"end\":53006,\"start\":52991},{\"end\":53114,\"start\":53104},{\"end\":53127,\"start\":53114},{\"end\":53138,\"start\":53127},{\"end\":53148,\"start\":53138},{\"end\":53155,\"start\":53148},{\"end\":53185,\"start\":53155},{\"end\":53194,\"start\":53185},{\"end\":53534,\"start\":53522},{\"end\":53556,\"start\":53534},{\"end\":53697,\"start\":53684},{\"end\":53709,\"start\":53697},{\"end\":53721,\"start\":53709},{\"end\":53736,\"start\":53721},{\"end\":53745,\"start\":53736},{\"end\":54060,\"start\":54046},{\"end\":54078,\"start\":54060},{\"end\":54091,\"start\":54078},{\"end\":54294,\"start\":54285},{\"end\":54307,\"start\":54294},{\"end\":54322,\"start\":54307},{\"end\":54341,\"start\":54322},{\"end\":54348,\"start\":54341}]", "bib_venue": "[{\"end\":37090,\"start\":37028},{\"end\":40381,\"start\":40319},{\"end\":34855,\"start\":34851},{\"end\":35064,\"start\":35014},{\"end\":35302,\"start\":35289},{\"end\":35600,\"start\":35598},{\"end\":35966,\"start\":35962},{\"end\":36270,\"start\":36266},{\"end\":36466,\"start\":36389},{\"end\":37026,\"start\":36949},{\"end\":37605,\"start\":37556},{\"end\":37997,\"start\":37924},{\"end\":38296,\"start\":38292},{\"end\":38577,\"start\":38573},{\"end\":38856,\"start\":38852},{\"end\":39089,\"start\":39085},{\"end\":39390,\"start\":39348},{\"end\":39665,\"start\":39661},{\"end\":39944,\"start\":39940},{\"end\":40317,\"start\":40240},{\"end\":40763,\"start\":40759},{\"end\":41047,\"start\":41043},{\"end\":41291,\"start\":41287},{\"end\":41575,\"start\":41571},{\"end\":41857,\"start\":41853},{\"end\":42085,\"start\":41978},{\"end\":42475,\"start\":42471},{\"end\":42790,\"start\":42786},{\"end\":43018,\"start\":42976},{\"end\":43260,\"start\":43196},{\"end\":43539,\"start\":43467},{\"end\":43867,\"start\":43862},{\"end\":44081,\"start\":44073},{\"end\":44264,\"start\":44172},{\"end\":44591,\"start\":44536},{\"end\":44861,\"start\":44834},{\"end\":45091,\"start\":45087},{\"end\":45376,\"start\":45372},{\"end\":45605,\"start\":45532},{\"end\":46108,\"start\":46101},{\"end\":46452,\"start\":46448},{\"end\":46766,\"start\":46762},{\"end\":47032,\"start\":47028},{\"end\":47290,\"start\":47286},{\"end\":47547,\"start\":47543},{\"end\":47808,\"start\":47804},{\"end\":48004,\"start\":48000},{\"end\":48225,\"start\":48221},{\"end\":48460,\"start\":48374},{\"end\":48961,\"start\":48939},{\"end\":49390,\"start\":49386},{\"end\":49626,\"start\":49564},{\"end\":50015,\"start\":50011},{\"end\":50327,\"start\":50323},{\"end\":50641,\"start\":50634},{\"end\":50931,\"start\":50927},{\"end\":51219,\"start\":51215},{\"end\":51535,\"start\":51531},{\"end\":51769,\"start\":51710},{\"end\":52097,\"start\":52073},{\"end\":52452,\"start\":52401},{\"end\":52795,\"start\":52791},{\"end\":53010,\"start\":53006},{\"end\":53256,\"start\":53210},{\"end\":53560,\"start\":53556},{\"end\":53802,\"start\":53761},{\"end\":54095,\"start\":54091},{\"end\":54352,\"start\":54348}]"}}}, "year": 2023, "month": 12, "day": 17}