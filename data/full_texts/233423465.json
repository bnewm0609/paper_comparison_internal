{"id": 233423465, "updated": "2023-10-06 04:32:22.469", "metadata": {"title": "ACDC: The Adverse Conditions Dataset with Correspondences for Semantic Driving Scene Understanding", "authors": "[{\"first\":\"Christos\",\"last\":\"Sakaridis\",\"middle\":[]},{\"first\":\"Dengxin\",\"last\":\"Dai\",\"middle\":[]},{\"first\":\"Luc\",\"last\":\"Gool\",\"middle\":[\"Van\"]}]", "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2021, "month": 4, "day": 27}, "abstract": "Level 5 autonomy for self-driving cars requires a robust visual perception system that can parse input images under any visual condition. However, existing semantic segmentation datasets are either dominated by images captured under normal conditions or are small in scale. To address this, we introduce ACDC, the Adverse Conditions Dataset with Correspondences for training and testing semantic segmentation methods on adverse visual conditions. ACDC consists of a large set of 4006 images which are equally distributed between four common adverse conditions: fog, nighttime, rain, and snow. Each adverse-condition image comes with a high-quality fine pixel-level semantic annotation, a corresponding image of the same scene taken under normal conditions, and a binary mask that distinguishes between intra-image regions of clear and uncertain semantic content. Thus, ACDC supports both standard semantic segmentation and the newly introduced uncertainty-aware semantic segmentation. A detailed empirical study demonstrates the challenges that the adverse domains of ACDC pose to state-of-the-art supervised and unsupervised approaches and indicates the value of our dataset in steering future progress in the field. Our dataset and benchmark are publicly available.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2104.13395", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/SakaridisDG21", "doi": "10.1109/iccv48922.2021.01059"}}, "content": {"source": {"pdf_hash": "787b9f2a81412a05cb26269a1e2740c746192f04", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2104.13395v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "11bfd7677c2eb602a8a62c627a2566423ae26c76", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/787b9f2a81412a05cb26269a1e2740c746192f04.txt", "contents": "\nACDC: The Adverse Conditions Dataset with Correspondences for Semantic Driving Scene Understanding\n\n\nChristos Sakaridis \nMPI for Informatics\n3 KULeuven\n\nDengxin Dai \nMPI for Informatics\n3 KULeuven\n\nLuc Van Gool \nMPI for Informatics\n3 KULeuven\n\nEth Z\u00fcrich \nMPI for Informatics\n3 KULeuven\n\nACDC: The Adverse Conditions Dataset with Correspondences for Semantic Driving Scene Understanding\n\nLevel 5 autonomy for self-driving cars requires a robust visual perception system that can parse input images under any visual condition. However, existing semantic segmentation datasets are either dominated by images captured under normal conditions or are small in scale. To address this, we introduce ACDC, the Adverse Conditions Dataset with Correspondences for training and testing semantic segmentation methods on adverse visual conditions. ACDC consists of a large set of 4006 images which are equally distributed between four common adverse conditions: fog, nighttime, rain, and snow. Each adverse-condition image comes with a high-quality fine pixel-level semantic annotation, a corresponding image of the same scene taken under normal conditions, and a binary mask that distinguishes between intra-image regions of clear and uncertain semantic content. Thus, ACDC supports both standard semantic segmentation and the newly introduced uncertainty-aware semantic segmentation. A detailed empirical study demonstrates the challenges that the adverse domains of ACDC pose to state-of-the-art supervised and unsupervised approaches and indicates the value of our dataset in steering future progress in the field. Our dataset and benchmark are publicly available.\n\nIntroduction\n\nMost of the prominent large-scale image-based datasets for driving scene understanding, including Cityscapes [8], Vistas [28] and KITTI [13], are dominated by images captured under normal visual conditions, i.e., at daytime and in clear weather. Yet, vision applications such as autonomous driving impose a strict requirement on perception algorithms to maintain satisfactory performance in adverse do-mains. Although there are recent efforts to include adverse visual domains in large-scale datasets, such as Oxford RobotCar [27] and BDD100K [55], these efforts focus either on localization/mapping tasks [27,49] or on recognition tasks which do not involve dense pixel-level outputs, such as object detection [3, 42,55]. For instance, while a notable 40% of the object detection set of BDD100K pertains to nighttime, only 3% of the images in its semantic segmentation set, namely 345 images, are captured at nighttime [40]. In addition, the pixel-level annotation process for adversecondition images is kept identical in [55] to the normalcondition case, which leads to errors in the ground truth and renders it unreliable [40]. In contrast, seminal previous work [8] has underlined the need for specialized techniques and datasets for pixel-level semantic scene understanding in adverse visual conditions, due to the inherent aleatory uncertainty in images captured in such conditions. These render entire image regions indiscernible even for humans.\n\nACDC constitutes a response to this need for a largescale driving dataset specialized to adverse conditions, in terms of (i) size, (ii) domain adversity, and (iii) featured tasks. ACDC includes 4006 images with high-quality pixellevel semantic annotations, which are distributed equally among four common adverse conditions in real-world driving environments, namely fog, nighttime, rain, and snow, thus featuring a scale of the same order as Cityscapes. The dataset was deliberately recorded with the respective adverse conditions clearly present. Thus, a large domain shift from the normal clear-weather daytime conditions was achieved. Moreover, for each adverse-condition image, a corresponding normal-condition image of the same scene from approximately the same viewpoint is provided, intended for use by weakly supervised methods.\n\nAs to the tasks that our dataset supports, apart from standard semantic segmentation, we add the task of uncertaintyaware semantic segmentation. For the latter we intro- duce a specialized annotation protocol and a dedicated performance metric, termed average uncertainty-aware IoU (AUIoU). The key characteristic of uncertainty-aware semantic segmentation is the principled inclusion of image regions with indiscernible semantic content-invalid regions-in annotation and evaluation. In particular, the annotation protocol for our adverse-condition images leverages privileged information in the form of the corresponding normal-condition images and the original adversecondition videos, which enables to reliably assign legitimate semantic labels to invalid regions and to include them in the evaluation both for standard and uncertainty-aware semantic segmentation. For the latter task, the separation of labeled pixels into invalid and valid is encoded in a binary mask. While both tasks require a hard semantic prediction, the uncertainty-aware task additionally expects a confidence map prediction. AUIoU is designed to take into account both the semantic and the confidence prediction and to reward predictions with low confidence on invalid pixels and high confidence on valid pixels. The requirement for an additional confidence prediction is relevant for safety-oriented applications, as it can help the downstream decision-making system avoid the fatal consequences of a low-confidence prediction being false, e.g. when a pedestrian is missed.\n\nApart from being a challenging benchmark for supervised semantic segmentation approaches, ACDC is a wellsuited test bed for domain adaptation. A multitude of recent works [7,15,22,23,26,41,43,44,46,48,51,53,59,60,62,65,66] have focused on unsupervised domain adaptation (UDA) for semantic segmentation, but most of them are validated only on an artificial synthetic-to-real setting, using GTA5 [34] and SYNTHIA [36] as source datasets and Cityscapes [8] as the target dataset. The normal-to-adverse domain adaptation scenario for semantic segmentation, which is much more relevant for realworld deployment of autonomous cars due to the difficulty of both acquiring and annotating adverse-condition data, has largely been overlooked. In particular, much fewer works consider normal-to-adverse adaptation in their experiments [10,11,32,37,38,39,40] and whenever they do, they either restrict the target adverse domain to a single condition, e.g. nighttime [10, 39,40], fog [37,38], or rain [11], or do not include a quantitative evaluation on the real tar-get domain altogether [32]. We attribute this fragmentation of normal-to-adverse adaptation works to the absence of a general large-scale dataset for semantic segmentation that evenly covers the majority of common adverse conditions and provides reliable ground truth for a sound evaluation in such challenging domains. ACDC answers exactly the need for such a dataset and will serve as a test bed for unsupervised and weakly supervised domain adaptation. Experiments such as Cityscapes\u2192ACDC adaptation are straightforward thanks to the identical label sets of the two datasets, which facilitates validation of new domain adaptation approaches in the normal-to-adverse setting.\n\nWe experiment with ACDC in four main directions: evaluation of models pre-trained on normal conditions, supervised learning in adverse conditions, unsupervised and weakly supervised normal-to-adverse domain adaptation, and evaluation of uncertainty-aware semantic segmentation baselines and oracles. Results show that access to groundtruth annotations under adverse conditions is indispensable for achieving high performance, as pre-trained models severely deteriorate under adverse conditions. Moreover, the real-world Cityscapes\u2192ACDC adaptation scenario poses significant challenges to all state-of-the-art UDA methods, which recover at best only a small portion of the performance gain over the source-domain model compared to using full supervision. This underlines the need for UDA methods that perform better when handling adverse target domains and highlights the importance of ACDC in steering future work in this direction. Finally, the uncertainty-aware annotations of ACDC create significant room for improvement over simple confidence prediction baselines and help promote future work on semantic segmentation methods that simultaneously models uncertainty.\n\n\nRelated Work\n\nDatasets for driving scene understanding include realworld and synthetic sets that support geometric and recognition tasks. KITTI [13] and Cityscapes [8] pioneered this area with LiDAR and semantic image annotations, respectively. Subsequent datasets mostly aimed at increasing the scale [17], diversity [28] and number of tasks [55].\n\nAs high-quality pixel-level annotations proved hard to ac-quire [8,28], another line of work focused on creating synthetic sets at an even larger scale [19,33,34,36] and in which ground truth is automatically generated, as well as translating real datasets to adverse conditions such as fog or rain [14,37,38]. Oxford Robotcar [27] was the first real-world large-scale dataset in which adverse visual conditions such as nighttime, rain and snow were significantly represented, but it did not feature semantic annotations. While more recent large-scale sets [2,30] that cover adverse conditions, such as Waymo Open [42] and nuScenes [3], include bounding boxes, they still lack dense pixel-level semantic annotations, which are vital for real-world autonomous agents [63]. BDD100K [55] is the only exception to this rule, with ca. 13% of its 10000 pixel-level annotations pertaining to adverse conditions but containing severe errors [40], while only a small portion of each of the 1881 adverse-condition images in ADUULM [29] is annotated. On the other hand, several sets with small-scale pixellevel annotations covering adverse conditions [58] were recently presented, focusing on fog [9,38], nighttime [10,40], and rain [45]. A notable case is Dark Zurich [40], with 201 fine pixel-level nighttime annotations and a dedicated annotation protocol and evaluation metric that handles regions with ambiguous content. ACDC improves both upon BDD100K, in terms of ground truth quality, and Dark Zurich, in terms of scale and condition diversity, featuring 4006 high-quality fine pixel-level annotations in which fog, night, rain and snow are equally represented.\n\nSemantic segmentation has progressed rapidly over the last years, primarily through the design of convolutional neural networks. Based on fully convolutional architectures [25], seminal works introduced atrous convolution [4, 5, 56] and encoder-decoder structures with skip connections [35] to exploit context and improve localization, respectively. Balancing between global and local information was further addressed by parallel branches of different resolutions [24,31]  with high-resolution representations. While performance on the popular Cityscapes benchmark is increasingly saturating, we demonstrate that state-of-the-art methods achieve much lower performance on ACDC (see Sec. 4). Thus, ACDC provides a more challenging benchmark for semantic segmentation thanks to the adversity of its domains and is therefore able to foster further progress in the field.\n\nAdaptation of semantic segmentation networks to domains where full supervision is not available was launched shortly after the introduction of supervised approaches [16]. A major class of UDA works employs adversarial domain adaptation to implicitly align the source and target domains at the level of pixels and/or features [7,15,26,41,43,44,46,48,60]. Other approaches to UDA rely on self-training with pseudo-labels in the target domain [65,66] or combine self-training with adversarial adaptation [23] or with pixellevel adaptation via explicit transforms from source to target [22,53]. However, all aforementioned approaches have been evaluated only on the artificial scenario of syntheticto-real adaptation and overlook normal-to-adverse adaptation, which is of higher practical importance for autonomous cars. ACDC constitutes the large-scale targetdomain dataset which has been missing so far for such a normal-to-adverse experiment and aims to steer the development of unsupervised and weakly supervised adaptation approaches that can cope with adverse target domains.\n\n\nACDC Dataset\n\nWe base the design of ACDC on the same general principles as seminal normal-condition datasets [8] and adapt the collection and annotation process to fit better the adverse condition setting at hand.\n\n\nCollection\n\nOur data collection is guided by the decision to record the same set of scenes both under adverse and normal conditions. We define the domain of normal conditions as the combination of daytime and clear weather, i.e. good visibility and no precipitation or snow cover on the ground. While the focus of ACDC is on adverse conditions, the acquisition of the corresponding normal-condition images is vital both for the subsequent annotation step and to support weakly supervised methods, as the same scene can be much easier to parse in normal conditions, both for humans and machines.\n\nThus, we recorded several days of video in Switzerland by driving around in a car, primarily in urban areas but also on highways and in rural regions. In order to have a clear domain separation between different adverse conditions, we use the following criterion for the adverse-condition recordings: each recording takes place under only one type of adversity from a set of four items, i.e., fog, nighttime, rain, and snow. For example, our foggy recordings are performed at daytime and without rain or snow. For snow, both snowfall and snow cover on the ground are admissible. Moreover, we keep for further processing only the parts of the adversecondition recordings that correspond to an intense presence of the respective condition, so as to maximize the domain shift from normal conditions as well as domain adversity.\n\nWe record with a 1080p GoPro Hero 5 camera, mounted in front of the windshield at nighttime and in normal conditions and behind the windshield in fog, rain, and snow. The camera records 8-bit RGB frames at a rate of 30 Hz. \n\n\nCorrespondence Establishment\n\nOur camera also provides GPS readings, which allow us to establish image-level correspondences between adversecondition and normal-condition recordings. In particular, for each adverse-condition recording, we perform a normal-condition recording along exactly the same route. We then use the sequences of GPS measurements of the two recordings to perform a global dynamic-programmingbased matching of the adverse GPS sequence to the normal one, where the objective is defined by the Euclidean distances of matched pairs of GPS samples. Our global matching handles routes with loops better than simple nearest neighbors. Each adverse-condition frame is then matched to a normal-condition frame based on the corresponding matched samples of the GPS sequences.\n\n\nDataset Splits\n\nACDC is split into four sets corresponding to the examined conditions. We manually selected 1000 foggy, 1006 nighttime, 1000 rainy and 1000 snowy images from the recordings for dense pixel-level semantic annotation, for a total of 4006 adverse-condition images. The selection process aimed at maximizing the complexity and diversity of captured scenes. Within each recording, any pair of selected images is at least 20 s or 50 m apart (whatever comes first).\n\nThe dataset is also split into training, validation, and test sets. We apply a global geographical split across all conditions, so that there is zero overlap between the three sets, even for different conditions. Given the abundance of training data from normal-condition datasets [8, 28,55] that allow to pre-train semantic segmentation models, we opt for a split with a greater test set size than usual. This aims at providing a highly challenging benchmark for semantic segmentation, both in terms of scale and domain adversity. In particular, we split the set of each adverse condition into 400 training, 100 validation and 500 test images, except the nighttime set with 106 validation images. This results in a total of 1600 training and 406 validation images with public annotations and 2000 test images with annotations withheld for benchmarking purposes, as per standard practice [8].\n\n\nAnnotation\n\nImages captured under adverse conditions contain invalid regions, i.e. regions with indiscernible semantic content, which generally co-exist with valid regions in the same image. We take this into account for creating annotations of ACDC and design a specialized annotation protocol, which leverages privileged information from the corresponding normal-condition images and the original adverse-condition videos and allows (i) the reliable assignment of semantic labels to invalid regions and (ii) the creation of a binary mask that distinguishes valid from invalid regions.\n\nOur annotation protocol consists of two cascaded annotation stages. At stage 1, a semantic labeling draft is manually produced from the adverse-condition image I, in which pixels that cannot be unquestionably assigned to a single semantic class are left unlabeled. At stage 2, the corresponding normal-condition image I and the adverse-condition video from which I was extracted are used to augment and finalize the annotation. In particular, the annotator can assign a legitimate label to pixels that were left unlabeled in stage 1 and correct pixels that were incorrectly labeled in stage 1. Pixels that remain unclear in stage 2 are left unlabeled and are not used for training or evaluation.\n\nThe final annotation outputs are twofold: (i) the final semantic annotation H after stage 2, and (ii) a binary invalid mask J, where pixels whose label changed from stage 1 to stage 2 are set to 1 (invalid) and pixels with the same semantic label for both stages are set to 0 (valid). J enables the introduction of the new task of uncertainty-aware semantic segmentation, which we detail in Sec. 5.\n\nThe 4006 fine-pixel annotations of ACDC were created by a professional team of annotators to ensure high-quality ground truth. Annotators were asked to be conservative in labeling pixels in both stages, so as to minimize errors. Both the initial draft from stage 1 and the final annotation from stage 2 passed through quality control. The total time required for annotating a single image was 3.3 h on average.\n\nThe class specifications of ACDC are directly inherited from Cityscapes. In particular, we annotate the 19 evaluation classes of Cityscapes, which include the most common and traffic-related objects in driving scenes. Objects that belong to classes outside this set receive a fall-back label and are not used for training or evaluation. This choice of classes provides full compatibility of ACDC to Cityscapes and other normal-condition datasets for semantic segmentation [28,55]. Detailed annotation statistics are presented in Fig. 1. An example of our two-stage annotation protocol is shown in Fig. 2 for a snowy image. Note the assignment of a region in the lower right part of the image that is unlabeled Table 1. Comparison of ACDC against adverse-condition semantic segmentation datasets. \"Adverse annot.\": total annotated adversecondition images, \"Fog\"/\"Night\"/\"Rain\"/\"Snow\": annotated foggy/nighttime/rainy/snowy images, \"Inv. regions\": can invalid regions get legitimate labels?, \"Corr. normal\": are corresponding normal-condition images available?, \"Inv. masks\": are invalid masks available? \n\n\nComparison to Related Datasets\n\nTo the best of our knowledge, ACDC constitutes the largest adverse-condition semantic segmentation dataset to date. In Table 1, we compare ACDC to existing datasets that also address semantic segmentation under adverse conditions. Most of these datasets focus on a single condition and are of small scale. WildDash covers a wider variety of adverse conditions but also has a small scale. BDD100K includes 10000 images with semantic segmentation annotations. We inspected these images manually to identify those that pertain to fog, night, rain, and snow. We found that only 1346/10000 images pertain to any of these four conditions. By contrast, ACDC is fully composed of these four common adverse conditions. Notably, it contains one order of magnitude more annotated images than any other competing dataset for each of fog, night and rain. At the same time, our specialized annotation protocol using corre-sponding normal-condition images ensures reliable annotations even for invalid regions, making ACDC a high-quality dataset for training and evaluation for adverse conditions.\n\n\nSemantic Segmentation\n\nThe first task ACDC supports is standard semantic segmentation. All results in Sec. 4 are reported for the test set of ACDC using the IoU metric. We experiment for our dataset with domain adaptation methods, externally pretrained models and supervised approaches.\n\n\nNormal-to-Adverse Adaptation\n\nWe present a new benchmark for UDA of semantic segmentation: Cityscapes\u2192ACDC. We select eight representative state-of-the-art UDA methods, train them with their default configurations for adaptation from Cityscapes to the entire ACDC and present the results in Table 2. All eight methods share the same DeepLabv2-based architecture [5]. Whereas these methods have achieved significant performance gains in the popular synthetic-to-real adaptation set- Table 3. Comparison of state-of-the-art unsupervised domain adaptation methods on Cityscapes\u2192ACDC adaptation for individual conditions. We train a separate model on each conditionspecific subset of ACDC and evaluate each model on the condition it has been trained for. Performance of the model trained only on the source domain (Source model) and of oracles with access to the target domain labels for each condition (Oracle) is also reported.\n\n\nMethod\n\nFog ting, we observe that most of them do not improve upon the source-domain baseline in our normal-to-adverse setting. The best-performing UDA method is FDA, which is based on a pixel-level adaptation strategy with an explicit Fourier prior. Even FDA is outperformed by the model that is supervised with only 100 target-domain labels, indicating that there is a lot of room for improvement for UDA methods on this new challenging normal-to-adverse benchmark. The image-level correspondences of ACDC between adverse and normal conditions act as weak supervision. We experiment with MGCDA, a weakly supervised method that exploits such correspondences. MGCDA outperforms FDA but is still inferior to its fully supervised counterpart.\n\nIn addition, we train state-of-the-art UDA methods to adapt from Cityscapes to individual conditions of ACDC in Table 3. The increased uniformity of the target domains in this setting results in larger performance gains overall compared to Table 2. However, night and snow prove particularly challenging for most methods and only FDA brings a performance gain on snow.\n\n\nEvaluation of Pre-trained Models on ACDC\n\nIn Table 4, we use ACDC to evaluate semantic segmentation models which have been pre-trained on external datasets. For models pre-trained on Cityscapes, the performance drop is larger on the nighttime set, implying that the domain shift from the normal-condition domain is larger for this set. Methods that specialize on fog or nighttime generally perform better on that condition compared to models pre-trained on Cityscapes. Moreover, most of these specialized methods also improve the performance on conditions other than the one encountered at training time.\n\n\nSupervised Learning on Adverse Conditions\n\nWe use ACDC to train four state-of-the-art supervised semantic segmentation methods and report their perfor-  Table 5. Qualitative results are shown in Fig. 3 for two supervised methods and one UDA method. We draw the following conclusions: (1) full supervision in adverse conditions is more valuable than designing a better architecture trained solely on normal conditions, as even an earlier method [5] performs better with full supervision than the top-performing externally pre-trained model (cf . Table 4).\n\n(2) ACDC is a challenging benchmark for supervised methods due to its hard visual domains; even the very recent HRNet scores only 75.0% mIoU on the test set, which is 5.4% lower than its respective performance of 80.4% on Cityscapes [47].\n\n(3) The rankings of the supervised and the pre-trained models do not correlate well, as can be seen from the results in Tables 5 and 4. The last point suggests that state-of-the-art networks such as HRNet have enough capacity to overfit to datasets such as Cityscapes, which would explain the low performance of the Cityscapes pre-trained HRNet model on ACDC. We test this hypothesis by training HRNet jointly on Cityscapes and ACDC; our expectation is that the jointly trained model will at least match the performance of the individually trained models on each dataset. This is confirmed, as the jointly trained model gets 81.2% mIoU on Cityscapes and 74.8% on ACDC, beating and being on a par with the respective individually trained models. Thus, even if ACDC is not of very large scale, it helps to efficiently regularize segmentation models for normal conditions as well. Table 6 compares models trained on a single adverse condition, termed condition experts, against models trained on the entire training set, termed uber models. Each condition expert is evaluated on the condition it has been trained on. The uber models generally beat the respective condition experts across different conditions and segmentation networks. This hints that the capacity of these networks   is large enough to discover discriminative representations for all conditions simultaneously. We also evaluate ensembles of condition experts against uber models on the complete test set (\"All\"), where the ensemble uses the expert corresponding to the condition of the input image for prediction. Again, the uber models outperform the ensembles of experts for all examined methods. Moreover, all methods perform worst at nighttime, indicating that the nighttime set of ACDC represents a harder domain than the other sets. We focus on the widely used DeepLabv3+ network [6] for a detailed study of class-level performance across different conditions and compare the performance of the four condition experts in Table 7. We make the following observations: (1) the lowest performance for road and sidewalk occurs in snow, which can be attributed to confusion between the two classes due to similar appearance in the pres-ence of snow cover.\n\n(2) Classes that usually appear dark or are not well-lit at nighttime, e.g., building, vegetation, traffic sign, and sky, are harder to segment at nighttime. (3) Performance on classes with instances of small size, such as person, rider, and bicycle, is lowest on fog, probably due to the combined effect of contrast reduction and low resolution for instances of these classes that are far from the camera.\n\nWe also evaluate in Table 8 the four DeepLabv3+ condition experts on conditions that are not encountered at training. Excluding nighttime, the results are close to symmetric with respect to training versus evaluation condition; e.g., training on fog and testing on snow results in a similar performance to training on snow and testing on fog. In contrast, performance of the night expert on other conditions is much higher than performance of other experts at night, implying that representations learned from the nighttime domain can generalize better to the other conditions than vice versa.\n\n\nUncertainty-Aware Semantic Segmentation\n\nExisting works that model uncertainty in semantic segmentation [1, 21] are evaluated only with IoU, which does not assess the predicted confidence. In contrast, for uncertainty-aware semantic segmentation, algorithms are required to output both a hard semantic prediction\u0124 and a confidence map C with values in the range [0, 1]. The average UIoU (AUIoU) metric is computed by thresholding C at multiple thresholds across the range [0, 1], calculating the UIoU [40] for each threshold and averaging the results. A pixel p with confidence value below the examined threshold is treated as invalid and contributes positively if J(p) = 1 (true invalid) and negatively if J(p) = 0 (false invalid).\n\n\nBaselines and Oracles\n\nWe present the results of straightforward baselines for uncertainty-aware segmentation that are based on methods for standard semantic segmentation in Table 9. We first evaluate three state-of-the-art methods using confidence maps that are constant and equal to 1, i.e., not modeling confidence. In this case, AUIoU reduces to IoU. Any sensible method that models confidence should improve upon this baseline. Using the max-softmax scores output by these methods as confidence maps generally yields inferior results to globally constant confidence, as softmax is not a good proxy for confidence. An upper bound for the per-formance of the examined methods is obtained by using a confidence oracle. More specifically, we use the binary complement of the ground-truth invalid mask J as the confidence prediction. This raises AUIoU performance significantly across all conditions compared to the globally constant confidence baseline. The performance gap between the oracle and the baseline is largest for night, indicating that explicitly modeling uncertainty has the potential to improve performance especially in the nighttime domain.\n\nWe have also trained [1] on ACDC, using the GT invalid masks for training its outlier detection part. The learned confidence by [1] leads to lower test set AUIoU (52.0%) than constant confidence (53.0%), indicating that a better modeling of uncertainty is needed in future approaches.\n\n\nConclusion and Outlook\n\nIn this paper, we have presented ACDC, a large-scale dataset and benchmark suite for semantic driving scene understanding in adverse conditions. Our dataset covers adverse visual domains that are common in driving scenarios and features high-quality pixel-level annotations which also include visually degraded image regions. Our annotations support both the standard and the new uncertainty-aware semantic segmentation task.\n\nWe have evaluated several state-of-the-art approaches on our benchmark, both in the supervised and the unsupervised setting. The conclusions from this evaluation show the importance of ACDC in steering future progress in the field: (i) ACDC provides a challenging target domain for unsupervised domain adaptation approaches in the normalto-adverse adaptation setting, as most state-of-the-art approaches yield at best marginal performance gains, (ii) ACDC is a hard benchmark for supervised semantic segmentation methods, as the best baseline obtains an IoU of only 75.0%, whereas the same baseline scores 80.4% on Cityscapes, (iii) ACDC can be used jointly with existing normal-condition datasets for training in order to regularize models better and improve their performance both under normal and adverse conditions. \n\n\nReferences\n\n\nA. Training Details\n\nWe provide the detailed training configurations for the various methods for semantic segmentation that have been used in Sec. 4 and for the method in [1] for uncertaintyaware semantic segmentation that has been used in Sec. 5.\n\n\nA.1. Normal-to-Adverse Adaptation\n\nFor the comparison in Table 2, we use as sourcedomain model the DeepLabv2 [5] model that is used as the Cityscapes oracle in AdaptSegNet [43], with a performance of 65.1% mIoU on the Cityscapes validation set. For all eight unsupervised domain adaptation (UDA) methods that are compared, we use their default training configurations, including the learning rate schedule and the weights of the various losses. The number of training iterations run for each method as well as the number of self-supervised learning rounds that are used by some of the methods are reported in Table 10. For FDA, SIM and MRNet, we run a first training round without self-training followed by a second training round with self-training, as per default implementation of these methods. For FDA, we train three separate models in each training round, one for each different value of the \u03b2 parameter from the set {0.01, 0.05 0.09}, and use the average prediction of the three models at test time. In all cases, we use the model weights corresponding to the final training iteration for testing.\n\nThe same source-domain model is also used for the experiment on adaptation to individual conditions presented in Table 3. Again, we use the default training configurations for all examined methods and across all four conditions. The number of training iterations run for each method to adapt to each condition as well as the number of selfsupervised learning rounds that are used by some of the methods are reported in Table 11. For MRNet and fog, the self-supervised training round includes 35k iterations instead of 40k. In addition, for MRNet and rain, the first \n\n\nA.2. Supervised Learning on Adverse Conditions\n\nFor training the four semantic segmentation methods that are compared in Tables 5 and 6, we have generally used the default configuration for each method both in the case of condition experts and uber models. For DeepLabv2 [5], we use the architecture employed in AdaptSegNet [43] in the context of domain adaptation and not the original architecture. We have used the default learning rate schedule for each method, with the base learning rates that are reported in Table 12. We generally use 60 training epochs for all four methods, which yields 96k training iterations for uber models and 24k training iterations for condition experts. Exceptions to this rule are RefineNet and fog where we use 30 epochs, DeepLabv2 and fog where we use 45 epochs, DeepLabv2 and night where we use 240 epochs, and the DeepLabv3+ uber model for which we use 30 epochs. For HRNet, we use the snapshot with the best mIoU performance on the respective validation set of ACDC for predicting on the test set, while for the rest of the methods we use the final training snapshot for the same purpose.\n\n\nA.3. Uncertainty-Aware Semantic Segmentation\n\nWe have used the two-head model designed in [1] and trained it on the entire training set of ACDC for 60 epochs. We use the default learning rate schedule of [1], with a base learning rate of 4 \u00d7 10 \u22124 , which is equal to the default. For predicting on the test set, we use the final training snapshot. \n\n\nB. Comparison to Other Adaptation Benchmarks\n\nIn Fig. 4, we present the comparative performance of the eight UDA methods we have used in Table 2 on two additional popular domain adaptation benchmarks, i.e. GTA5\u2192Cityscapes and SYNTHIA\u2192Cityscapes. All examined methods have been configured to optimize performance on the synthetic-to-real UDA setting of these two benchmarks, but this configuration does not generalize well to the real-world normal-to-adverse adaptation setting of Cityscapes\u2192ACDC, as most methods cannot improve significantly upon the source-trained baseline. Furthermore, the comparative performance of the various methods on either of the synthetic-to-real benchmarks does not correlate well with the comparative performance on Cityscapes\u2192ACDC. This indicates that adaptation strategies that work well for the synthetic-to-real setting may not be as beneficial in the real-world normal-to-adverse adaptation setting and that new strategies and algorithms need to be devised for the latter.\n\n\nC. Detailed Class-level Results\n\nWe provide class-level performance for the experiments for which only mean performance over all classes is reported in the paper due to space limitations.\n\n\nC.1. Normal-to-Adverse Adaptation\n\nIn Tables 13-16, we present the class-level IoU performance of the UDA methods that are examined in the setting of adaptation to individual conditions in Table 3.\n\n\nC.2. Evaluation of Pre-trained Models on ACDC\n\nIn Tables 17-21, we present the class-level IoU performance of the externally pre-trained models that are evaluated in Table 4.\n\n\nC.3. Supervised Learning on Adverse Conditions\n\nIn Tables 22-25, we present the class-level IoU performance of the supervised semantic segmentation methods that are examined in Table 6. In particular, we consider the individual conditions of ACDC separately for evaluation, and evaluate on each condition both the respective condition experts that have been trained only on that condition and uber models trained on all conditions.\n\n\nC.4. Uncertainty-aware Semantic Segmentation\n\nIn Tables 26-30, we present the class-level average uncertainty-aware IoU (AUIoU) performance of the baselines and oracles that are examined in Table 9. More specifically, Table 26 considers methods trained jointly on all conditions of ACDC and also evaluated jointly on all conditions, while Tables 27-30 present methods trained and evaluated on individual conditions. The results corresponding to the baseline that uses constant confidence equal to 1 are omitted, as they are identical by definition to IoU results and are thus already included in Table 5 and Tables 22-25.\n\n\nD. Additional Details on ACDC Dataset\n\nWe provide additional details on the construction and the characteristics of ACDC. We have implemented a website and evaluation server for the ACDC benchmark and have made it publicly available. An indicative screenshot from the submission page of the website is provided in Fig. 6.\n\n\nD.1. Collection\n\nOur recordings were performed in Switzerland. Therefore, the geographic distribution of ACDC is similar to Cityscapes, which was also recorded in central Europe. This eliminates geographic location from the set of factors that introduce a domain shift between Cityscapes and ACDC and allows to study in isolation the effect of visual conditions at time of capture on the performance of semantic segmentation methods, both in the supervised setting and the unsupervised domain adaptation setting.\n\n\nD.2. Correspondence Establishment\n\nWe present in Algorithm 1 the dynamic programming algorithm that we use for matching the GPS sequences of adverse-condition recordings and normal-condition recordings of ACDC. The algorithm takes into account the sequential nature of the GPS measurements from the two recordings in computing the correspondence function A. In particular, we enforce k < i \u21d2 A(k) \u2264 A(i). That is, for  a given sample i of the adverse-condition sequence P , its matched sample A(i) of the normal-condition sequence R is restricted to not precede in time any sample of R that has been matched to a sample k of P that precedes i. This constraint is based on the fact that the routes of the two record-ings are driven in the same direction and thus in the same order. Consequently, for routes that contain loops, our formulation prevents the matching of samples that are nearest neighbors but correspond to different passes from the same location and are thus potentially associated with different iii driving directions and 3D rotations of the camera.\n\n\nD.3. Annotation\n\nIn Fig. 5, we show the percentage of the pixels of each semantic class in ACDC that are marked as invalid in the ground-truth invalid mask J. For the majority of the classes, a notable percentage of more than 5% of the pixels are labeled as invalid, which demonstrates the ability of our specialized annotation protocol with privileged information to assign a legitimate semantic label even to invalid regions with ambiguous semantic content.\n\nThe total number of annotated pixels in ACDC is preiv sented in Table 31. Note that labeled pixels that are marked as valid in the ground-truth invalid masks J constitute ca. 85% of the total pixels. From the remaining 15% of pixels that did not receive a legitimate semantic label in stage 1 of the annotation because of their ambiguity, it was possible to label half of them (7.5%) with a legitimate seman-tic label in stage 2 of the annotation, by making use of the additional privileged information in the form of corresponding normal-condition images and original adverse-condition videos. Note that for stage 2 of the annotation, we explicitly set the time budget (excluding quality control) to 20 minutes and asked the annotators to prioritize labeling of (i) traffic v  Table 29. Uncertainty-aware semantic segmentation baseline results on ACDC for rain. Supervised methods for standard semantic segmentation are trained and evaluated on rain for semantic label prediction. Confidence prediction baselines: max-softmax network outputs (Max-Softmax) and ground-truth invalid masks (GT).  vii Table 30. Uncertainty-aware semantic segmentation baseline results on ACDC for snow. Supervised methods for standard semantic segmentation are trained and evaluated on snow for semantic label prediction. Confidence prediction baselines: max-softmax network outputs (Max-Softmax) and ground-truth invalid masks (GT). Algorithm 1 Dynamic programming algorithm for GPS sequence matching Input: Adverse-condition GPS sequence P = (p 1 , . . . , p n ), normal-condition GPS sequence R = (r 1 , . . . , r m ) Output: Correspondence function A : {1, . . . , n} \u2192 {1, . . . , m} 1:\n\nCompute pairwise Euclidean distances of GPS samples 2: d ij \u2190 p i \u2212 r j , 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 m 3:\n\nCompute cost matrix C (n \u00d7 m) 4: C 1j \u2190 d 1j , 1 \u2264 j \u2264 m 5: C ij \u2190 min k\u2264j {C i\u22121,k } + d ij , 2 \u2264 i \u2264 n, 1 \u2264 j \u2264 m  Figure 6. The submission page of our benchmark website. Our evaluation server supports the two tasks and five condition configurations of ACDC, accepting submissions both for individual conditions and for all conditions. Best viewed on a screen. Table 31. Overall annotation statistics for ACDC. We report the total number of pixels assigned to a legitimate semantic label (Labeled) and of pixels not assigned to any semantic label (Unlabeled) as well as the respective percentages.\n\nNumber of pixels Percentage of pixels (%) Labeled 7.682 \u00d7 10 9 92.47 -out of which Valid 7.055 \u00d7 10 9 84.93 -out of which Invalid 0.627 \u00d7 10 9 7.54 Unlabeled 0.625 \u00d7 10 9 7.53\n\nTotal 8.307 \u00d7 10 9 100.00 ix\n\nFigure 1 .\n1Number of finely annotated pixels per class in ACDC.\n\nFigure 2 .\n2Illustration of annotation protocol for ACDC. The color coding of the semantic classes matches Fig. 1. All annotations in (b), (d) and (e) pertain to the input image I in (a). A white color in (b) and (d) denotes unlabeled pixels.\n\nFigure 3 .\n3Qualitative results of selected semantic segmentation methods on ACDC. From left to right: image, ground-truth annotation, FDA [53], DeepLabv3+ [6], and HRNet [47]. The color coding of the semantic classes matches Fig. 1.\n\nFigure 4 .\n4Comparison of Cityscapes\u2192ACDC to other domain adaptation benchmarks.We compare the performance of state-of-the-art domain adaptation methods on three benchmarks: GTA5\u2192Cityscapes, SYNTHIA\u2192Cityscapes, and Cityscapes\u2192ACDC. Performance of models trained only on the source domain (Source) and on the target domain with supervision (Oracle) is also presented. \"AdaptSeg\": AdaptSegNet.\n\n.\nComparison of state-of-the-art unsupervised domain adaptation methods on Cityscapes\u2192ACDC adaptation for rain. Performance of the model trained only on the source domain (Source model) and of the oracle with access to the target domain labels (Oracle) is also reported.\n\nFigure 5 .\n5Per-class percentages of labeled pixels that are marked as invalid in ACDC.\n\n\nindices matrix \u03b1 7: \u03b1 ij \u2190 arg min k\u2264j {C i\u22121,k }, 2 \u2264 i \u2264 n, 1 \u2264 j \u2264 m A(i) \u2190 \u03b1 i+1,A(i+1) , 1 \u2264 i \u2264 n \u2212 1 viii\n\n\nand global pooling [61]. Other works focused on real-time performance [54], leveraging different modalities such as depth [52], and defining neighborhoodbased supervision [20] for segmentation. The current state of the art includes i.a. DeepLabv3+ [6] and ANN [64] with pyramid pooling modules, DANet [12] and CCNet [18] with attention mechanisms, and HRNet [47] and OCR [57]\n\n\nDatasetAdverse annot. Fog Night Rain Snow Classes Reliable GT Fine GT Inv. regions Corr. normal Inv. masksFoggy Driving [38] \n101 \n101 \n0 \n0 \n0 \n19 \n\u00d7 \n\u00d7 \n\u00d7 \nFoggy Zurich [9] \n40 \n40 \n0 \n0 \n0 \n19 \n\u00d7 \n\u00d7 \n\u00d7 \nNighttime Driving [10] \n50 \n0 \n50 \n0 \n0 \n19 \n\u00d7 \n\u00d7 \n\u00d7 \n\u00d7 \nDark Zurich [40] \n201 \n0 \n201 \n0 \n0 \n19 \nRaincouver [45] \n326 \n0 \n95 326 \n0 \n3 \n\u00d7 \n\u00d7 \n\u00d7 \n\u00d7 \nWildDash [58] \n226 \n10 \n13 \n13 \n26 \n19 \n\u00d7 \n\u00d7 \n\u00d7 \nBDD100K [55] \n1346 \n23 345 213 765 \n19 \n\u00d7 \n\u00d7 \n\u00d7 \n\u00d7 \nACDC \n4006 \n1000 1006 1000 1000 \n19 \n\nTable 2. Comparison of state-of-the-art domain adaptation methods on Cityscapes\u2192ACDC adaptation. Cityscapes serves as the \nsource domain and the entire ACDC including all four conditions serves as the target domain. The first and second groups of rows \npresent unsupervised and weakly supervised methods, respectively. All unsupervised methods share the same network architecture. The \nperformance of the respective models trained on Cityscapes (Source model) and of the oracle models trained on ACDC with 100 labels \n(Oracle-100), 200 labels (Oracle-200), and all 1600 labels (Oracle) are also reported. \n\nMethod \nroad \nsidew. \nbuild. \nwall \nfence \npole \nlight \nsign \nveget. \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorc. \nbicycle \nmIoU \n\nSource model [5] 71.9 26.2 51.1 18.8 22.5 19.7 33.0 27.7 67.9 28.6 44.2 43.1 22.1 71.2 29.8 33.3 48.4 26.2 35.8 38.0 \nAdaptSegNet [43] 69.4 34.0 52.8 13.5 18.0 4.3 14.9 9.7 64.0 23.1 38.2 38.6 20.1 59.3 35.6 30.6 53.9 19.8 33.9 33.4 \nADVENT [46] \n72.9 14.3 40.5 16.6 21.2 9.3 17.4 21.2 63.8 23.8 18.3 32.6 19.5 69.5 36.2 34.5 46.2 26.9 36.1 32.7 \nBDL [23] \n56.0 32.5 68.1 20.1 17.4 15.8 30.2 28.7 59.9 25.3 37.7 28.7 25.5 70.2 39.6 40.5 52.7 29.2 38.4 37.7 \nCLAN [26] \n79.1 29.5 45.9 18.1 21.3 22.1 35.3 40.7 67.4 29.4 32.8 42.7 18.5 73.6 42.0 31.6 55.7 25.4 30.7 39.0 \nCRST [65] \n51.7 24.4 67.8 13.3 9.7 30.2 38.2 34.1 58.0 25.2 76.8 39.9 17.1 65.4 3.7 6.6 39.6 11.8 8.6 32.8 \nFDA [53] \n73.2 34.7 59.0 24.8 29.5 28.6 43.3 44.9 70.1 28.2 54.7 47.0 28.5 74.6 44.8 52.3 63.3 28.3 39.5 45.7 \nSIM [48] \n53.8 6.8 75.5 11.6 22.3 11.7 23.4 25.7 66.1 8.3 80.6 41.8 24.8 49.7 38.6 21.0 41.8 25.1 29.6 34.6 \nMRNet [62] \n72.2 8.2 36.4 13.7 18.5 20.4 38.7 45.4 70.2 35.7 5.0 47.8 19.1 73.6 42.1 36.0 47.4 17.7 37.4 36.1 \nOracle-100 \n84.4 54.8 76.4 19.3 28.9 29.5 36.5 42.6 74.2 40.3 87.7 42.5 16.5 74.9 36.5 28.6 55.9 27.3 38.6 47.1 \nOracle-200 \n86.2 55.0 77.9 21.7 30.9 30.0 37.6 42.5 76.8 45.8 90.2 45.4 19.1 75.8 38.5 38.0 64.2 21.6 39.5 49.3 \nOracle \n88.0 62.3 80.8 37.0 35.1 33.9 49.8 49.5 80.1 50.7 92.5 51.1 26.5 79.9 49.0 41.1 72.2 26.5 44.2 55.3 \n\nSource model [24] 66.3 28.9 67.6 19.2 25.9 36.7 50.0 47.5 69.4 28.8 83.0 42.1 17.7 72.6 30.9 31.6 48.9 26.1 36.7 43.7 \nMGCDA [40] \n73.4 28.7 69.9 19.3 26.3 36.8 53.0 53.3 75.4 32.0 84.6 51.0 26.1 77.6 43.2 45.9 53.9 32.7 41.5 48.7 \nOracle \n92.5 71.2 86.2 39.0 44.0 53.2 68.8 66.0 85.1 59.3 94.9 65.2 38.5 85.8 53.8 59.7 76.2 47.5 54.5 65.3 \n\nat stage 1 (Fig. 2b) to the road label at stage 2 (Fig. 2d), \nthanks to the clear view from the normal-condition image. \n\n\n\nTable 4 .\n4Comparison of externally pre-trained models on ACDC for individual conditions and jointly for all conditions. The three groups of rows present models pre-trained on normal, foggy, and nighttime conditions respectively. CS: Cityscapes [8], FC: Foggy Cityscapes[38], FC-DBF: Foggy Cityscapes-DBF[37], FZ: Foggy Zurich[37], ND: Nighttime Driving [10], DZ: Dark Zurich[40].Method \nTrained on Fog Night Rain Snow All \n\nRefineNet [24] \nCS \n46.4 29.0 52.6 43.3 43.7 \nDeepLabv2 [5] \nCS \n33.5 30.1 44.5 40.2 38.0 \nDeepLabv3+ [6] \nCS \n45.7 25.0 50.0 42.0 41.6 \nDANet [12] \nCS \n34.7 19.1 41.5 33.3 33.1 \nHRNet [47] \nCS \n38.4 20.6 44.8 35.1 35.3 \n\nSFSU [38] \nFC \n45.6 29.5 51.6 41.4 42.9 \nCMAda [37] \nFC-DBF+FZ 51.2 32.0 53.4 47.6 47.1 \n\nDMAda [10] \nND \n50.7 32.7 54.9 48.9 47.9 \nGCMA [39] \nCS+DZ \n52.4 42.9 58.0 53.8 53.4 \nMGCDA [40] \nCS+DZ \n45.9 40.8 54.2 50.5 48.9 \nDANNet [50] \nCS+DZ \n-\n47.6 \n-\n-\n-\n\nmance in \n\nTable 5 .\n5Comparison of state-of-the-art supervised semantic segmentation methods on ACDC. Training and evaluation are performed using the complete training and test sets, respectively.Method \nroad \nsidew. \nbuild. \nwall \nfence \npole \nlight \nsign \nveget. \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorc. \nbicycle \nmIoU \n\nRefineNet [24] 92.5 71.2 86.2 39.0 44.0 53.2 68.8 66.0 85.1 59.3 94.9 65.2 38.5 85.8 53.8 59.7 76.2 47.5 54.5 65.3 \nDeepLabv2 [5] 88.0 62.3 80.8 37.0 35.1 33.9 49.8 49.5 80.1 50.7 92.5 51.1 26.5 79.9 49.0 41.1 72.2 26.5 44.2 55.3 \nDeepLabv3+ [6] 93.4 74.8 89.2 53.0 49.0 58.7 71.1 67.4 87.8 62.7 95.9 69.7 36.0 88.1 67.7 71.8 85.1 48.0 59.8 70.0 \nHRNet [47] \n95.3 79.9 90.7 53.7 57.4 65.9 78.4 75.9 88.8 68.6 96.1 75.5 54.0 91.2 68.2 76.2 85.4 58.4 65.1 75.0 \n\n\n\nTable 6 .\n6Comparison of condition experts vs. uber models on the different conditions of ACDC. The first group of rows presents condition-specific expert models trained on a single condition, while the second group presents uber models trained on all conditions. Note that the performance on all conditions is not an average of the respective performances on individual conditions.Method \nFog \nNight \nRain \nSnow \nAll \n\nRefineNet [24] \n63.6 \n52.2 \n66.4 \n62.5 \n62.8 \nDeepLabv2 [5] \n52.2 \n45.4 \n57.6 \n56.8 \n54.9 \nDeepLabv3+ [6] \n68.7 \n59.2 \n73.5 \n70.5 \n69.6 \nHRNet [47] \n70.8 \n63.2 \n72.7 \n70.2 \n70.9 \n\nRefineNet [24] \n65.7 \n55.5 \n68.7 \n65.9 \n65.3 \nDeepLabv2 [5] \n54.5 \n45.3 \n59.3 \n57.1 \n55.3 \nDeepLabv3+ [6] \n69.1 \n60.9 \n74.1 \n69.6 \n70.0 \nHRNet [47] \n74.7 \n65.3 \n77.7 \n76.3 \n75.0 \n\n\n\nTable 7 .\n7Comparison of class-level performance of DeepLabv3+ condition experts on the various conditions of ACDC. A different model is trained on each individual condition and then evaluated on this condition.Table 8. Cross-evaluation of DeepLabv3+ condition experts on the various conditions of ACDC. Each model is trained on an individual condition and evaluated on each condition separately. Performance of the Cityscapes pre-trained model is also reported.Table 9. Uncertainty-aware semantic segmentation baseline results using AUIoU. Supervised methods for standard semantic segmentation are trained and evaluated either separately on each condition or jointly on all conditions for semantic label prediction. Confidence prediction baselines: globally constant and equal to 100% (Constant 100%), max-softmax network outputs (Max-Softmax), ground-truth invalid masks (GT).Condition \nroad \nsidew. \nbuild. \nwall \nfence \npole \nlight \nsign \nveget. \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorc. \nbicycle \nmIoU \n\nFog \n93.8 77.4 88.8 51.0 43.3 54.2 68.2 71.7 87.7 74.6 98.2 53.5 32.1 83.8 69.3 84.4 85.3 47.2 40.1 68.7 \nNight \n94.7 75.9 85.0 48.4 38.6 52.2 55.8 54.4 76.1 30.3 84.2 67.4 41.1 85.0 8.3 62.3 80.6 35.6 49.8 59.2 \nRain \n92.8 77.4 93.9 67.3 58.1 64.1 74.4 75.9 94.2 50.8 98.6 70.8 33.4 90.4 67.7 79.2 86.8 54.6 66.1 73.5 \nSnow \n91.9 70.9 90.1 48.9 52.0 62.2 79.2 74.5 92.0 47.0 97.6 78.2 35.9 90.4 61.7 64.3 89.2 43.9 69.4 70.5 \n\nTrain/Eval Fog Night Rain Snow \n\nNormal 45.7 25.0 50.0 42.0 \n\nFog \n68.7 40.7 63.5 59.1 \nNight \n58.5 59.2 55.6 49.6 \nRain \n65.2 46.0 73.5 63.5 \nSnow \n59.2 38.0 69.3 70.5 \n\nMethod \nConfidence \nFog Night Rain Snow All \n\nRefineNet [24] Constant 100% 63.6 52.2 66.4 62.5 65.3 \nRefineNet [24] Max-Softmax 60.6 51.4 62.5 59.9 62.5 \nRefineNet [24] GT \n67.9 61.1 67.9 64.0 68.8 \n\nDeepLabv2 [5] Constant 100% 52.2 45.4 57.6 56.8 55.3 \nDeepLabv2 [5] Max-Softmax 51.9 45.9 56.0 56.8 54.7 \nDeepLabv2 [5] GT \n56.7 54.7 59.1 58.4 58.9 \n\nDeepLabv3+ [6] Constant 100% 68.7 59.2 73.5 70.5 70.0 \nDeepLabv3+ [6] Max-Softmax 66.4 59.1 70.6 67.9 67.8 \nDeepLabv3+ [6] GT \n73.1 67.1 75.0 72.0 73.3 \n\n\n\n[ 1 ]\n1Petra Bevandi\u0107, Ivan Kre\u0161o, Marin Or\u0161i\u0107, and Sini\u0161a\u0160egvi\u0107. Xinyu Huang, Peng Wang, Xinjing Cheng, Dingfu Zhou, Qichuan Geng, and Ruigang Yang. The ApolloScape open dataset for autonomous driving and its application. IEEE Transactions on Pattern Analysis and Machine Intelligence, Matthew Johnson-Roberson, Charles Barto, Rounak Mehta, Sharath Nittur Sridhar, Karl Rosaen, and Ram Vasudevan. Driving in the matrix: Can virtual worlds replace humangenerated annotations for real world tasks? In IEEE International Conference on Robotics and Automation, 2017. 3 [20] Tsung-Wei Ke, Jyh-Jing Hwang, Ziwei Liu, and Stella X. Yu. Adaptive affinity fields for semantic segmentation. In Patrick Wenzel, Rui Wang, Nan Yang, Qing Cheng, Qadeer Khan, Lukas von Stumberg, Niclas Zeller, and Daniel Cremers. 4Seasons: A cross-season dataset for multi-weather SLAM in autonomous driving. In Proceedings of the German Conference on Pattern Recognition (GCPR), 2020. 1 [50] Xinyi Wu, Zhenyao Wu, Hao Guo, Lili Ju, and Song Wang. Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. BiSeNet: Bilateral segmentation network for real-time semantic segmentation. Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In International Conference on Learning Representations, 2016. 3 [57] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Objectcontextual representations for semantic segmentation. In The Table 10. Training details for UDA methods in Cityscapes\u2192ACDC adaptation.\"SSL rounds\": number of training rounds that include supervision from pseudo-labels; if not relevant for a method, -is reported. \"Training iterations\": number of SGD iterations for each training round (number of epochs for each training round is alternatively reported).Simultaneous semantic segmentation and outlier detection in \npresence of domain shift. In German Conference on Pattern \nRecognition, 2019. 7, 8, i \n[2] Mario Bijelic, Tobias Gruber, Fahim Mannan, Florian Kraus, \nWerner Ritter, Klaus Dietmayer, and Felix Heide. See-\ning through fog without seeing fog: Deep multimodal sen-\nsor fusion in unseen adverse weather. In Proceedings of \nthe IEEE/CVF Conference on Computer Vision and Pattern \nRecognition (CVPR), June 2020. 3 \n[3] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, \nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\nancarlo Baldan, and Oscar Beijbom. nuScenes: A multi-\nmodal dataset for autonomous driving. In IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR), \nJune 2020. 1, 3 \n[4] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, \nKevin Murphy, and Alan Yuille. Semantic image segmenta-\ntion with deep convolutional nets and fully connected CRFs. \nIn International Conference on Learning Representations, \nMay 2015. 3 \n[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, \nKevin Murphy, and Alan L. Yuille. DeepLab: Semantic im-\nage segmentation with deep convolutional nets, atrous con-\nvolution, and fully connected CRFs. IEEE Transactions on \nPattern Analysis and Machine Intelligence, 40(4):834-848, \n2018. 3, 5, 6, 7, 8, i \n[6] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Flo-\nrian Schroff, and Hartwig Adam. Encoder-decoder with \natrous separable convolution for semantic image segmen-\ntation. In The European Conference on Computer Vision \n(ECCV), September 2018. 3, 6, 7, 8 \n[7] Yuhua Chen, Wen Li, and Luc Van Gool. ROAD: Reality ori-\nented adaptation for semantic segmentation of urban scenes. \nIn The IEEE Conference on Computer Vision and Pattern \nRecognition (CVPR), 2018. 2, 3 \n[8] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo \nRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe \nFranke, Stefan Roth, and Bernt Schiele. The Cityscapes \ndataset for semantic urban scene understanding. In The \nIEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2016. 1, 2, 3, 4, 6 \n[9] Dengxin Dai, Christos Sakaridis, Simon Hecker, and Luc \nVan Gool. Curriculum model adaptation with synthetic and \nreal data for semantic foggy scene understanding. Interna-\ntional Journal of Computer Vision, 128(5):1182-1204, 2020. \n3, 5 \n[10] Dengxin Dai and Luc Van Gool. Dark model adaptation: \nSemantic image segmentation from daytime to nighttime. In \nIEEE International Conference on Intelligent Transportation \nSystems, 2018. 2, 3, 5, 6 \n[11] Shuai Di, Qi Feng, Chun-Guang Li, Mei Zhang, Honggang \nZhang, Semir Elezovikj, Chiu C. Tan, and Haibin Ling. \nRainy night scene understanding with near scene semantic \nadaptation. IEEE Transactions on Intelligent Transportation \nSystems, 22(3):1594-1602, 2021. 2 \n\n[12] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei \nFang, and Hanqing Lu. Dual attention network for scene \nsegmentation. In Proceedings of the IEEE/CVF Conference \non Computer Vision and Pattern Recognition (CVPR), June \n2019. 3, 6 \n[13] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we \nready for autonomous driving? The KITTI vision benchmark \nsuite. In IEEE Conference on Computer Vision and Pattern \nRecognition (CVPR), 2012. 1, 2 \n[14] Shirsendu Sukanta Halder, Jean-Francois Lalonde, and \nRaoul de Charette. Physics-based rendering for improving \nrobustness to rain. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV), October \n2019. 3 \n[15] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, \nPhillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell. \nCyCADA: Cycle-consistent adversarial domain adaptation. \nIn International Conference on Machine Learning, 2018. 2, \n3 \n[16] Judy Hoffman, Dequan Wang, Fisher Yu, and Trevor Dar-\nrell. FCNs in the wild: Pixel-level adversarial and constraint-\nbased adaptation. arXiv e-prints, abs/1612.02649, December \n2016. 3 \n[17] 42(10):2702-2719, 2020. 2 \n[18] Zilong Huang, Xinggang Wang, Lichao Huang, Chang \nHuang, Yunchao Wei, and Wenyu Liu. CCNet: Criss-cross \nattention for semantic segmentation. In Proceedings of the \nIEEE/CVF International Conference on Computer Vision \n(ICCV), October 2019. 3 \n[19] Proceedings of the European Conference on Computer Vi-\nsion (ECCV), September 2018. 3 \n[21] Alex Kendall and Yarin Gal. What uncertainties do we need \nin Bayesian deep learning for computer vision? In Advances \nin Neural Information Processing Systems, 2017. 7 \n[22] Myeongjin Kim and Hyeran Byun. Learning texture invari-\nant representation for domain adaptation of semantic seg-\nmentation. In IEEE/CVF Conference on Computer Vision \nand Pattern Recognition (CVPR), June 2020. 2, 3 \n[23] Yunsheng Li, Lu Yuan, and Nuno Vasconcelos. Bidirectional \nlearning for domain adaptation of semantic segmentation. \nIn The IEEE Conference on Computer Vision and Pattern \nRecognition (CVPR), 2019. 2, 3, 5, 6 \n[24] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid. \nRefineNet: Multi-path refinement networks with identity \nmappings for high-resolution semantic segmentation. In \nIEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2017. 3, 5, 6, 7, 8 \ntation. In IEEE/CVF Conference on Computer Vision and \nPattern Recognition (CVPR), June 2020. 2, 3, 5, 6 \n[49] DANNet: A one-stage domain adaption network for unsu-\npervised nighttime semantic segmentation. In IEEE/CVF \nConference on Computer Vision and Pattern Recognition \n(CVPR), June 2021. 6 \n[51] Zuxuan Wu, Xintong Han, Yen-Liang Lin, Mustafa Gokhan \nUzunbas, Tom Goldstein, Ser Nam Lim, and Larry S. Davis. \nDCAN: Dual channel-wise alignment networks for unsuper-\nvised scene adaptation. In The European Conference on \nComputer Vision (ECCV), 2018. 2 \n[52] Dan Xu, Wanli Ouyang, Xiaogang Wang, and Nicu Sebe. \nPAD-Net: Multi-tasks guided prediction-and-distillation net-\nwork for simultaneous depth estimation and scene parsing. \nIn Proceedings of the IEEE Conference on Computer Vision \nand Pattern Recognition (CVPR), June 2018. 3 \n[53] Yanchao Yang and Stefano Soatto. FDA: Fourier domain \nadaptation for semantic segmentation. In IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR), \nJune 2020. 2, 3, 5, 6, 7 \n[54] In Pro-\nceedings of the European Conference on Computer Vision \n(ECCV), September 2018. 3 \n[55] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying \nChen, Fangchen Liu, Vashisht Madhavan, and Trevor Dar-\nrell. BDD100K: A diverse driving dataset for heterogeneous \nmultitask learning. In IEEE/CVF Conference on Computer \nVision and Pattern Recognition (CVPR), June 2020. 1, 2, 3, \n4, 5 \n[56] European Conference on Computer Vision (ECCV), pages \n173-190, 2020. 3 \n[58] Oliver Zendel, Katrin Honauer, Markus Murschitz, Daniel \nSteininger, and Gustavo Fernandez Dominguez. WildDash -\ncreating hazard-aware benchmarks. In The European Con-\nference on Computer Vision (ECCV), 2018. 3, 5 \n[59] Yang Zhang, Philip David, and Boqing Gong. Curricu-\nlum domain adaptation for semantic segmentation of urban \nscenes. In The IEEE International Conference on Computer \nVision (ICCV), 2017. 2 \n[60] Yiheng Zhang, Zhaofan Qiu, Ting Yao, Dong Liu, and Tao \nMei. Fully convolutional adaptation networks for semantic \nsegmentation. In The IEEE Conference on Computer Vision \nand Pattern Recognition (CVPR), 2018. 2, 3 \n[61] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang \nWang, and Jiaya Jia. Pyramid scene parsing network. In The \n\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2017. 3 \n[62] Zhedong Zheng and Yi Yang. Rectifying pseudo label learn-\ning via uncertainty estimation for domain adaptive seman-\ntic segmentation. International Journal of Computer Vision, \n2021. 2, 5, 6 \n[63] Brady Zhou, Philipp Kr\u00e4henb\u00fchl, and Vladlen Koltun. Does \ncomputer vision matter for action? Science Robotics, 4(30), \n2019. 3 \n[64] Zhen Zhu, Mengde Xu, Song Bai, Tengteng Huang, and Xi-\nang Bai. Asymmetric non-local neural networks for seman-\ntic segmentation. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV), October \n2019. 3 \n[65] Yang Zou, Zhiding Yu, Xiaofeng Liu, B.V.K. Vijaya Kumar, \nand Jinsong Wang. Confidence regularized self-training. \nIn IEEE/CVF International Conference on Computer Vision \n(ICCV), October 2019. 2, 3, 5 \n[66] Yang Zou, Zhiding Yu, B.V.K. Vijaya Kumar, and Jinsong \nWang. Unsupervised domain adaptation for semantic seg-\nmentation via class-balanced self-training. In The European \nConference on Computer Vision (ECCV), 2018. 2, 3 \nMethod \nSSL rounds Training iterations \n\nAdaptSegNet \n-\n95k \nADVENT \n-\n80k \nBDL \n0 \n80k \nCLAN \n-\n90k \nCRST \n3 \n2 epochs \nFDA \n1 \n80k \nSIM \n1 \n80k \nMRNet \n1 \n50k \n\n\n\nTable 11 .Table 12 .\n1112Training details for UDA methods in Cityscapes\u2192ACDC adaptation for individual conditions. \"SSL rounds\": number of training rounds that include supervision from pseudo-labels; if not relevant for a method, -is reported. \"Training iterations\": number of SGD iterations for each training round. Training details for supervised methods on ACDC.Method \nSSL rounds Training iterations \n\nAdaptSegNet \n-\n40k \nADVENT \n-\n40k \nBDL \n0 \n40k \nCLAN \n-\n40k \nFDA \n1 \n40k \nSIM \n1 \n40k \nMRNet \n1 \n40k \n\nMethod \nBase LR Training epochs \n\nRefineNet \n5 \u00d7 10 \u22125 \n60 \nDeepLabv2 2.5 \u00d7 10 \u22124 \n60 \nDeepLabv3+ \n10 \u22124 \n60 \nHRNet \n10 \u22124 \n60 \n\ntraining round without self-supervised training includes 25k \niterations instead of 40k. \n\n\n\nTable 13 .\n13Comparison of state-of-the-art unsupervised domain adaptation methods on Cityscapes\u2192ACDC adaptation for fog. Performance of the model trained only on the source domain (Source model) and of the oracle with access to the target domain labels (Oracle) is also reported.Method \nroad \nsidew. \nbuild. \nwall \nfence \npole \nlight \nsign \nveget. \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorc. \nbicycle \nmIoU \n\nSource model 66.4 31.2 26.8 22.9 18.6 8.2 32.3 10.7 70.7 39.0 31.3 17.6 41.1 65.0 30.0 34.3 18.3 42.3 29.0 33.5 \n\nAdaptSegNet 35.4 45.9 35.4 25.6 17.5 9.0 32.5 23.1 70.5 47.4 11.6 22.3 28.2 44.4 43.9 35.0 46.0 15.6 15.0 31.8 \nADVENT \n44.2 38.9 26.4 20.7 20.1 7.9 34.4 23.6 70.7 35.6 8.3 17.3 43.5 60.0 48.6 46.8 40.5 19.9 17.6 32.9 \nBDL \n36.9 37.8 47.0 28.2 21.6 13.7 37.2 34.5 67.2 49.4 27.6 29.1 51.3 58.5 49.4 51.8 30.3 21.4 22.5 37.7 \nCLAN \n48.8 41.3 29.6 27.2 21.0 16.1 41.1 39.6 67.7 50.2 15.4 36.2 30.8 72.2 52.2 54.4 47.2 27.1 22.6 39.0 \nFDA \n68.8 37.3 27.1 27.6 19.8 21.6 37.5 43.3 74.9 43.7 33.1 35.0 21.5 65.7 44.6 45.3 47.1 41.5 15.8 39.5 \nSIM \n76.7 43.1 23.5 23.6 17.9 10.9 32.1 15.3 70.4 50.5 21.4 34.8 44.3 58.4 50.5 55.2 34.7 23.0 8.8 36.6 \nMRNet \n78.6 26.1 19.6 29.0 13.5 12.0 41.9 49.0 78.2 59.0 6.6 39.8 26.1 72.5 44.8 37.9 59.6 19.1 24.1 38.8 \n\nOracle \n89.9 65.6 81.2 39.1 25.9 28.1 45.9 47.7 83.0 67.4 96.7 35.2 38.4 73.5 46.1 29.8 37.9 28.4 31.6 52.2 \n\nTable 14. Comparison of state-of-the-art unsupervised domain adaptation methods on Cityscapes\u2192ACDC adaptation for night-\ntime. Performance of the model trained only on the source domain (Source model) and of the oracle with access to the target domain labels \n(Oracle) is also reported. \n\nMethod \nroad \nsidew. \nbuild. \nwall \nfence \npole \nlight \nsign \nveget. \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorc. \nbicycle \nmIoU \n\nSource model 77.0 22.9 56.3 13.5 9.2 23.8 22.9 25.6 41.4 16.1 2.9 44.1 17.5 64.1 11.9 34.5 42.4 22.6 22.7 30.1 \n\nAdaptSegNet 84.9 39.9 66.8 17.2 17.7 13.4 17.6 16.4 39.6 16.1 5.7 42.8 21.4 44.8 11.9 13.0 39.1 27.5 28.4 29.7 \nADVENT \n86.5 45.3 60.8 23.2 12.5 15.4 18.0 19.4 41.2 18.3 2.7 43.8 21.3 61.6 12.6 19.1 43.0 30.2 27.6 31.7 \nBDL \n87.1 49.6 68.8 20.2 17.5 16.7 19.9 24.\n\nTable 16 .\n16Comparison of state-of-the-art unsupervised domain adaptation methods on Cityscapes\u2192ACDC adaptation for snow.Performance of the model trained only on the source domain (Source model) and of the oracle with access to the target domain labels \n(Oracle) is also reported. \n\nMethod \nroad \nsidew. \nbuild. \nwall \nfence \npole \nlight \nsign \nveget. \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorc. \nbicycle \nmIoU \n\nSource model 68.5 26.6 52.7 18.8 26.9 22.2 35.7 40.7 76.5 3.6 49.9 50.4 27.1 73.7 27.6 39.1 60.9 21.1 42.5 40.2 \n\nAdaptSegNet 51.3 32.5 47.3 21.5 31.5 13.2 37.8 23.2 76.0 2.6 4.5 49.9 23.1 68.7 38.3 31.8 51.5 21.7 45.0 35.3 \nADVENT \n50.8 24.8 46.2 15.5 26.0 15.5 27.9 23.0 70.0 2.1 9.5 44.2 25.3 68.5 22.9 24.9 50.1 23.9 38.9 32.1 \nBDL \n42.3 36.4 60.2 15.7 30.4 15.1 41.4 30.4 71.3 1.7 11.2 46.8 27.8 57.7 38.6 34.1 59.2 28.1 43.7 36.4 \nCLAN \n71.8 26.0 37.3 12.5 27.0 21.1 32.0 41.1 78.5 1.9 0.9 50.9 23.9 82.4 43.2 39.5 61.6 25.2 39.4 37.7 \nFDA \n74.6 30.9 56.1 20.5 34.8 28.7 53.9 47.8 80.5 1.1 55.9 53.1 37.9 79.7 40.5 51.9 67.4 34.3 41.8 46.9 \nSIM \n72.1 26.7 39.4 13.3 29.5 15.3 26.4 17.9 76.4 4.8 5.1 45.9 32.0 76.2 29.8 26.6 48.3 23.2 24.2 33.3 \nMRNet \n67.7 3.5 36.8 8.3 24.8 18.0 52.6 55.4 82.4 0.5 0.1 62.2 30.2 79.2 32.1 59.3 58.4 29.1 35.8 38.7 \n\nOracle \n89.1 61.7 82.7 26.4 40.9 35.5 56.5 54.1 85.2 39.0 95.1 55.0 25.7 84.3 38.6 53.8 77.6 29.0 49.5 56.8 \n\nTable 17. Comparison of externally pre-trained models on the complete test set of ACDC including all conditions. The three groups of \nrows present models pre-trained on normal, foggy, and nighttime conditions respectively. CS: Cityscapes, FC: Foggy Cityscapes, FC-DBF: \nFoggy Cityscapes-DBF, FZ: Foggy Zurich, ND: Nighttime Driving, DZ: Dark Zurich. \n\nMethod \nTrained on \nroad \nsidew. \nbuild. \nwall \nfence \npole \nlight \nsign \nveget. \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorc. \nbicycle \nmIoU \n\nRefineNet CS \n66.3 28.9 67.6 19.2 25.9 36.7 50.0 47.5 69.4 28.8 83.0 42.1 17.7 72.6 30.9 31.6 48.9 26.1 36.7 43.7 \nDeepLabv2 CS \n71.9 26.2 51.1 18.8 22.5 19.7 33.0 27.7 67.9 28.6 44.2 43.1 22.1 71.2 29.8 33.3 48.4 26.2 35.8 38.0 \nDeepLabv3+ CS \n75.1 32.8 65.9 17.5 20.2 32.2 46.7 45.2 70.5 33.5 80.9 23.9 14.7 71.5 40.1 20.3 51.2 20.2 28.8 41.6 \nDANet \nCS \n58.0 6.0 57.3 6.8 22.3 27.7 41.3 42.1 66.4 19.9 69.2 32.2 10.2 46.5 22.4 19.1 43.1 13.2 25.5 33.1 \nHRNet \nCS \n55.6 10.9 55.4 7.7 15.9 21.7 37.8 42.5 67.4 13.3 59.0 38.7 14.0 68.3 23.8 48.0 48.3 17.9 23.6 35.3 \n\nSFSU \nFC \n72.9 28.8 68.3 19.6 23.9 37.3 49.3 47.0 60.4 33.4 72.3 43.1 14.8 72.7 31.7 31.2 47.0 25.4 35.5 42.9 \nCMAda \nFC-DBF+FZ 79.9 32.5 69.5 14.7 24.7 41.1 53.6 51.3 67.4 34.8 83.8 49.0 19.9 77.0 34.1 38.5 51.1 29.6 42.7 47.1 \n\nDMAda \nND \n75.3 35.5 67.4 19.2 27.1 40.0 53.7 50.9 74.6 30.9 84.9 48.8 23.1 76.6 39.7 37.4 52.5 29.1 42.1 47.9 \nGCMA \nCS+DZ \n79.7 48.7 71.5 21.6 29.9 42.5 56.7 57.7 75.8 39.5 87.2 57.4 29.7 80.6 44.9 46.2 62.0 37.2 46.5 53.4 \nMGCDA \nCS+DZ \n76.0 49.4 72.0 11.3 21.7 39.5 52.0 54.9 73.7 24.7 88.6 54.1 27.2 78.2 30.9 41.9 58.2 31.1 44.4 48.9 \n\nTable 18. Comparison of externally pre-trained models on ACDC for fog. The three groups of rows present models pre-trained on \nnormal, foggy, and nighttime conditions respectively. CS: Cityscapes, FC: Foggy Cityscapes, FC-DBF: Foggy Cityscapes-DBF, FZ: \nFoggy Zurich, ND: Nighttime Driving, DZ: Dark Zurich. \n\nMethod \nTrained on \nroad \nsidew. \nbuild. \nwall \nfence \npole \nlight \nsign \nveget. \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorc. \nbicycle \nmIoU \n\nRefineNet CS \n64.4 40.0 69.6 24.2 19.7 36.5 52.7 55.2 71.1 35.4 93.9 27.4 19.2 72.7 42.0 42.1 69.3 30.3 15.8 46.4 \nDeepLabv2 CS \n66.4 31.2 26.8 22.9 18.6 8.2 32.3 10.7 70.7 39.0 31.3 17.6 41.1 65.0 30.0 34.3 18.3 42.3 29.0 33.5 \nDeepLabv3+ CS \n82.3 57.6 61.5 18.1 16.4 33.3 49.6 54.5 76.0 44.1 90.0 9.6 28.7 69.0 35.1 34.5 28.9 41.7 37.5 45.7 \nDANet \nCS \n52.1 14.5 49.7 5.5 16.9 30.0 47.9 51.5 72.2 23.3 80.1 24.2 3.0 44.7 32.4 27.5 65.1 10.8 7.7 34.7 \nHRNet \nCS \n57.3 19.3 49.1 12.8 17.8 27.3 44.0 54.7 72.8 15.5 81.7 28.3 3.9 66.6 28.4 52.0 72.7 7.2 18.1 38.4 \n\nSFSU \nFC \n72.3 37.9 74.4 28.9 19.3 37.5 49.4 54.6 58.0 43.7 77.9 28.6 5.3 73.6 42.4 44.0 72.7 31.4 14.9 45.6 \nCMAda \nFC-DBF+FZ 81.7 43.5 72.8 25.6 19.5 39.8 51.0 58.9 80.5 51.3 95.3 36.9 12.7 76.5 45.2 51.2 77.1 33.2 19.9 51.2 \n\nDMAda \nND \n75.5 44.7 72.6 26.4 20.8 38.3 52.9 57.8 75.9 38.6 96.3 35.5 26.8 75.8 47.7 50.7 73.9 35.8 17.3 50.7 \nGCMA \nCS+DZ \n80.8 53.5 70.1 29.2 20.7 38.4 53.0 60.9 70.2 46.5 95.4 44.2 38.0 76.6 52.4 49.7 56.8 41.0 17.6 52.4 \nMGCDA \nCS+DZ \n71.7 47.3 65.7 18.2 15.3 34.4 48.6 59.9 64.9 24.7 95.4 44.8 23.8 73.3 36.1 45.4 63.9 23.9 15.4 45.9 \n\n\n\nTable 19 .\n19Comparison of externally pre-trained models on ACDC for nighttime. The three groups of rows present models pre-trained on normal, foggy, and nighttime conditions respectively. CS: Cityscapes, FC: Foggy Cityscapes, FC-DBF: Foggy Cityscapes-DBF, FZ: Foggy Zurich, ND: Nighttime Driving, DZ: Dark Zurich.Method \nTrained on \nroad \nsidew. \nbuild. \nwall \nfence \npole \nlight \nsign \nveget. \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorc. \nbicycle \nmIoU \n\nRefineNet CS \n66.5 24.0 50.3 16.9 11.6 26.4 34.2 25.5 44.2 21.6 0.1 40.8 24.8 57.4 6.8 37.3 20.5 23.9 19.1 29.0 \nDeepLabv2 CS \n77.0 22.9 56.3 13.5 9.2 23.8 22.9 25.6 41.4 16.1 2.9 44.1 17.5 64.1 11.9 34.5 42.4 22.6 22.7 30.1 \nDeepLabv3+ CS \n73.0 20.8 50.4 22.2 5.4 22.6 31.8 23.0 42.9 16.1 6.6 19.2 11.7 48.9 0.9 13.9 42.4 10.5 13.7 25.0 \nDANet \nCS \n67.1 4.5 46.7 5.5 5.1 13.1 29.3 19.6 36.6 15.6 0.1 29.3 12.4 29.1 4.5 12.3 9.0 10.3 13.3 19.1 \nHRNet \nCS \n50.0 10.1 59.9 0.7 6.0 14.2 25.6 22.3 19.1 3.4 0.1 37.6 7.9 49.4 6.9 45.9 13.9 7.8 11.3 20.6 \n\nSFSU \nFC \n76.9 26.2 50.4 18.1 9.6 27.4 33.3 25.3 41.0 21.5 0.0 41.5 25.3 58.7 7.3 40.7 17.9 22.0 17.9 29.5 \nCMAda \nFC-DBF+FZ 82.6 25.4 53.9 10.1 11.2 30.5 36.7 30.0 38.7 16.5 0.1 46.0 26.2 65.8 13.9 50.9 20.4 24.8 23.8 32.0 \n\nDMAda \nND \n74.7 29.5 49.4 17.1 12.6 31.0 38.2 30.0 48.0 22.8 0.2 47.0 25.4 63.8 12.8 46.1 23.1 24.7 24.6 32.7 \nGCMA \nCS+DZ \n78.6 45.9 58.5 17.7 18.6 37.5 43.6 43.5 58.7 39.2 22.4 57.9 29.9 72.1 21.5 56.2 41.8 35.7 35.4 42.9 \nMGCDA \nCS+DZ \n74.5 52.5 69.4 7.7 10.8 38.4 40.2 43.3 61.5 36.3 37.6 55.3 25.6 71.2 10.9 46.4 32.6 27.3 33.8 40.8 \nDANNet \nCS+DZ \n90.7 61.1 75.5 35.9 28.8 26.6 31.4 30.6 70.8 39.4 78.7 49.9 28.8 65.9 24.7 44.1 61.1 25.9 34.5 47.6 \n\nTable 20. Comparison of externally pre-trained models on ACDC for rain. The three groups of rows present models pre-trained on \nnormal, foggy, and nighttime conditions respectively. CS: Cityscapes, FC: Foggy Cityscapes, FC-DBF: Foggy Cityscapes-DBF, FZ: Foggy \nZurich, ND: Nighttime Driving, DZ: Dark Zurich. \n\nMethod \nTrained on \nroad \nsidew. \nbuild. \nwall \nfence \npole \nlight \nsign \nveget. \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorc. \nbicycle \nmIoU \n\nRefineNet CS \n73.9 29.9 82.9 26.3 37.2 46.3 61.8 57.9 89.4 42.5 96.6 44.2 13.2 80.5 40.7 22.9 66.8 32.0 53.5 52.6 \nDeepLabv2 CS \n71.2 26.7 73.8 20.8 27.1 29.9 39.3 44.4 87.3 25.2 82.0 42.0 14.3 76.2 36.3 26.6 49.8 30.3 42.2 44.5 \nDeepLabv3+ CS \n74.4 29.8 82.3 18.1 28.8 41.7 54.3 55.6 88.7 32.8 97.2 36.7 8.5 84.7 51.7 34.0 61.5 29.7 40.0 50.0 \nDANet \nCS \n59.9 2.4 75.9 12.9 31.5 37.7 49.5 53.3 85.5 35.5 91.1 35.4 8.4 53.5 26.0 16.4 57.8 17.9 38.9 41.5 \nHRNet \nCS \n65.0 6.7 70.3 16.1 20.2 29.5 48.5 54.7 87.5 36.1 80.1 40.6 8.6 78.2 34.1 44.6 67.3 29.4 34.6 44.8 \n\nSFSU \nFC \n74.6 29.9 81.4 24.1 33.8 46.2 59.9 56.7 86.8 40.8 93.4 46.4 15.1 80.5 40.5 18.6 65.7 33.6 52.5 51.6 \nCMAda \nFC-DBF+FZ 78.1 34.8 80.7 18.9 33.3 50.0 63.1 62.2 87.4 38.8 96.6 51.1 16.9 83.3 37.9 21.9 68.7 36.5 55.1 53.4 \n\nDMAda \nND \n78.3 37.7 82.5 24.2 36.8 49.0 64.5 61.5 90.6 42.8 97.3 49.6 18.2 83.4 45.1 21.6 70.2 35.2 54.8 54.9 \nGCMA \nCS+DZ \n81.1 48.0 84.8 25.0 37.3 49.8 66.5 66.2 92.1 43.5 97.6 54.5 20.4 85.5 47.3 34.6 71.3 40.3 56.7 58.0 \nMGCDA \nCS+DZ \n80.5 46.5 79.9 16.0 28.8 44.9 60.0 61.5 90.3 44.8 97.1 51.1 23.1 82.3 33.4 30.2 69.1 36.5 53.8 54.2 \n\nTable 21. Comparison of externally pre-trained models on ACDC for snow. The three groups of rows present models pre-trained on \nnormal, foggy, and nighttime conditions respectively. CS: Cityscapes, FC: Foggy Cityscapes, FC-DBF: Foggy Cityscapes-DBF, FZ: Foggy \nZurich, ND: Nighttime Driving, DZ: Dark Zurich. \n\nMethod \nTrained on \nroad \nsidew. \nbuild. \nwall \nfence \npole \nlight \nsign \nveget. \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorc. \nbicycle \nmIoU \n\nRefineNet CS \n61.0 25.5 73.7 11.7 31.1 37.2 53.1 57.7 71.3 0.9 92.7 44.1 14.7 77.0 30.3 26.9 57.2 18.4 38.5 43.3 \nDeepLabv2 CS \n68.5 26.6 52.7 18.8 26.9 22.2 35.7 40.7 76.5 3.6 49.9 50.4 27.1 73.7 27.6 39.1 60.9 21.1 42.5 40.2 \nDeepLabv3+ CS \n73.9 32.6 71.3 11.1 25.6 31.4 50.6 54.4 77.8 4.1 87.0 25.1 14.6 82.7 39.5 17.2 55.2 12.0 31.2 42.0 \nDANet \nCS \n47.6 5.4 57.5 2.9 29.1 29.3 41.4 51.2 71.1 0.5 64.8 32.7 11.7 56.5 14.5 27.9 53.7 8.1 25.9 33.3 \nHRNet \nCS \n59.6 9.3 43.9 4.0 17.8 17.6 35.6 47.0 77.0 0.0 32.5 39.4 39.2 74.2 13.4 54.0 61.1 15.9 26.1 35.1 \n\nSFSU \nFC \n64.5 24.0 72.6 10.9 28.8 37.8 54.9 58.1 62.4 0.8 78.4 44.2 9.5 76.0 29.5 25.6 55.2 16.7 37.3 41.4 \nCMAda \nFC-DBF+FZ 74.6 31.6 73.6 9.4 30.3 43.1 61.9 61.7 75.7 0.7 93.5 53.1 19.1 79.6 29.7 31.6 61.9 22.9 50.3 47.6 \n\nDMAda \nND \n73.6 34.4 74.9 12.3 33.4 41.1 58.4 60.1 79.9 0.6 95.4 53.1 23.0 80.4 40.3 34.5 62.9 22.7 48.6 48.9 \nGCMA \nCS+DZ \n79.7 49.5 75.3 17.5 37.9 43.2 59.0 61.9 78.8 2.2 95.5 62.5 33.6 83.2 42.5 43.4 72.1 32.2 51.1 53.7 \nMGCDA \nCS+DZ \n80.1 49.5 70.2 6.1 27.8 39.6 55.4 58.0 76.0 0.3 95.5 57.5 35.7 81.0 28.6 48.9 70.3 27.8 50.5 50.5 \n\n\n\nTable 22 .Table 26 .\n2226Comparison of state-of-the-art supervised methods on ACDC for fog. The first group of rows presents condition-specific expert models trained only on fog, while the second group presents uber models trained on all conditions. DeepLabv2 89.9 65.6 81.2  39.1 25.9 28.1 45.9 47.7 83.0 67.4 96.7 35.2 38.4 73.5 46.1 29.8 37.9 28.4 31.6 52.2 DeepLabv3+ 93.8 77.4 88.8 51.0 43.3 54.2 68.2 71.7 87.7 74.6 98.2 53.5 32.1 83.8 69.3 84.4 85.3 47.2 40.1 68.Table 23. Comparison of state-of-the-art supervised methods on ACDC for nighttime. The first group of rows presents conditionspecific expert models trained only on nighttime, while the second group presents uber models trained on all conditions. 57.8 71.7 30.3 23.6 31.8 37.4 38.9 60.0 26.8 72.8 47.6 25.1 71.1 16.9 27.8 65.1 30.6 38.5 45.3 DeepLabv3+ 94.7 75.3 84.9 46.9 37.8 53.8 57.3 52.1 75.7 41.2 82.9 66.6 40.2 83.6 24.7 67.9 80.8 41.7 49.4 60.9 HRNet 95.7 79.0 86.2 46.8 43.5 59.2 64.9 64.5 75.3 40.3 82.7 72.1 52.6 86.9 18.8 78.8 83.6 52.5 57.3 65.3 Table 24. Comparison of state-of-the-art supervised methods on ACDC for rain. The first group of rows presents condition-specific expert models trained only on rain, while the second group presents uber models trained on all conditions. DeepLabv2 87.3 63.9 89.0 50.3 40.6 38.4 52.2 53.4 89.2 42.2 96.7 51.5 13.0 81.9 47.9 47.2 72.2 29.1 48.8 57.6 DeepLabv3+ 92.8 77.4 93.9 67.3 58.1 64.1 74.4 75.9 94.2 50.8 98.6 70.8 33.4 90.4 67.7 79.2 86.8 54.6 66.1 73.DeepLabv2 87.4 64.8 88.1 48.2 40.4 38.4 52.0 56.9 89.3 40.2 96.5 52.3 17.4 83.9 55.5 63.0 75.8 28.9 47.2 59.3 DeepLabv3+ 92.7 76.5 93.5 64.8 58.0 63.8 75.8 77.3 94.1 50.0 98.0 70.5 33.1 91.2 75.9 85.1 86.2 55.8 65.0 74.1 Table 25. Comparison of state-of-the-art supervised methods on ACDC for snow. The first group of rows presents condition-specific expert models trained only on snow, while the second group presents uber models trained on all conditions. DeepLabv2 89.1 61.7 82.7 26.4 40.9 35.5 56.5 54.1 85.2 39.0 95.1 55.0 25.7 84.3 38.6 53.8 77.6 29.0 49.5 56.8 DeepLabv3+ 91.9 70.9 90.1 48.9 52.0 62.2 79.2 74.5 92.0 47.0 97.6 78.2 35.9 90.4 61.7 64.3 89.2 43.9 69.4 70.DeepLabv2 88.7 62.5 82.5 35.3 41.7 35.0 59.0 52.8 84.4 36.0 95.2 58.1 29.8 84.8 48.9 30.9 77.9 32.9 48.4 57.1 DeepLabv3+ 91.4 69.6 88.8 48.8 53.9 60.6 79.5 72.9 90.5 44.7 97.4 77.4 37.2 90.0 64.3 55.0 87.8 41.7 70.0 69.participants and (ii) distant and/or unclear objects that were affected the most by the adverse conditions at the time of capture. Uncertainty-aware semantic segmentation baseline results on the complete test set of ACDC including all conditions. Supervised methods for standard semantic segmentation are trained and evaluated jointly on all conditions for semantic label prediction. Confidence prediction baselines: max-softmax network outputs (Max-Softmax) and ground-truth invalid masks (GT). DeepLabv2 Max-Softmax 87.1 60.4 79.7 36.1 35.7 32.6 47.3 48.7 80.2 49.2 92.2 49.0 24.7 79.0 51.1 43.3 72.3 26.3 45.1 DeepLabv3+ Max-Softmax 92.1 71.3 88.2 49.0 47.3 54.9 68.7 65.6 88.0 60.7 96.0 65.0 33.9 87.5 66.7 72.6 81.3 43.Table 27. Uncertainty-aware semantic segmentation baseline results on ACDC for fog. Supervised methods for standard semantic segmentation are trained and evaluated on fog for semantic label prediction. Confidence prediction baselines: max-softmax network outputs (Max-Softmax) and ground-truth invalid masks (GT). DeepLabv2 Max-Softmax 89.7 63.0 79.2 39.4 25.9 25.0 41.4 46.6 82.5 66.7 95.6 36.4 35.6 72.7 49.5 29.6 44.5 29.DeepLabv3+ Max-Softmax 92.9 74.8 87.2 51.3 41.7 49.9 65.6 69.8 87.1 72.3 97.6 51.9 27.1 82.8 67.4 79.1 84.1 42.Table 28. Uncertainty-aware semantic segmentation baseline results on ACDC for nighttime. Supervised methods for standard semantic segmentation are trained and evaluated on nighttime for semantic label prediction. Confidence prediction baselines: max-softmax network outputs (Max-Softmax) and ground-truth invalid masks (GT). DeepLabv2 Max-Softmax 90.2 62.2 78.6 29.9 32.9 33.7 36.5 40.3 65.6 25.2 77.9 45.2 23.2 70.2 5.0 14.6 62.1 40.DeepLabv3+ Max-Softmax 93.8 73.3 85.2 47.0 43.4 51.3 53.7 54.3 80.7 28.7 87.9 62.1 40.9 84.8 10.4 65.2 78.8 34.Method \nroad \nsidew. \nbuild. \nwall \nfence \npole \nlight \nsign \nveget. \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorc. \nbicycle \nmIoU \n\nRefineNet \n93.2 75.5 86.1 44.1 37.6 46.0 64.2 64.8 85.5 70.8 97.9 46.1 34.8 79.3 59.4 64.8 82.4 36.6 38.8 63.6 \n7 \nHRNet \n94.6 79.6 89.9 53.6 44.9 59.4 74.3 76.1 88.9 77.6 98.3 61.5 53.3 86.0 66.6 80.0 88.5 41.1 30.2 70.8 \n\nRefineNet \n93.5 75.6 87.2 42.3 39.2 49.8 68.5 67.2 85.6 70.1 97.9 52.6 48.2 81.0 62.6 62.0 69.1 57.7 37.4 65.7 \nDeepLabv2 90.9 67.2 81.6 38.7 29.5 29.7 51.2 50.7 81.4 61.9 96.0 34.8 40.5 74.1 53.4 53.1 59.9 8.3 32.5 54.5 \nDeepLabv3+ 93.6 77.6 89.2 54.0 44.8 55.8 67.6 72.0 88.0 73.5 98.2 49.5 24.4 83.9 72.2 84.2 89.2 52.8 42.4 69.1 \nHRNet \n94.9 81.0 90.5 58.9 53.7 61.9 79.0 78.7 89.3 78.7 98.3 63.2 54.6 87.2 72.3 87.8 90.6 58.7 38.9 74.7 \n\nMethod \nroad \nsidew. \nbuild. \nwall \nfence \npole \nlight \nsign \nveget. \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorc. \nbicycle \nmIoU \n\nRefineNet \n93.4 70.3 78.6 34.3 34.1 46.9 52.2 54.2 66.3 18.7 78.1 60.3 35.5 76.2 4.7 47.8 59.4 36.0 45.3 52.2 \nDeepLabv2 90.5 63.7 78.0 30.0 29.6 32.9 37.0 41.2 61.9 25.2 75.3 47.9 23.4 69.5 2.7 15.4 60.3 39.7 37.9 45.4 \nDeepLabv3+ 94.7 75.9 85.0 48.4 38.6 52.2 55.8 54.4 76.1 30.3 84.2 67.4 41.1 85.0 8.3 62.3 80.6 35.6 49.8 59.2 \nHRNet \n95.5 78.8 86.5 49.2 44.1 58.0 64.5 63.2 75.6 41.0 83.9 71.7 48.8 84.6 15.5 76.9 81.2 25.9 55.9 63.2 \n\nRefineNet \n93.5 70.9 80.3 32.0 32.0 46.0 53.9 54.1 69.2 31.9 78.0 61.0 35.4 80.2 11.6 60.0 69.4 48.9 46.8 55.5 \nDeepLabv2 86.6 Method \nroad \nsidew. \nbuild. \nwall \nfence \npole \nlight \nsign \nveget. \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorc. \nbicycle \nmIoU \n\nRefineNet \n89.2 69.8 91.7 52.2 51.3 57.9 71.0 69.9 93.6 50.5 98.4 65.8 25.1 88.1 49.4 55.4 74.8 47.0 60.2 66.4 \n5 \nHRNet \n94.8 81.8 94.9 69.6 63.7 69.5 79.6 80.7 94.8 51.2 98.7 73.5 27.0 93.1 75.4 40.9 61.4 59.6 70.8 72.7 \n\nRefineNet \n91.5 73.5 91.1 51.0 51.6 58.3 72.5 73.7 92.9 51.2 97.9 65.5 29.5 89.2 59.8 68.2 80.3 48.0 59.5 68.7 \nHRNet \n95.6 83.1 94.2 60.1 66.3 71.2 82.3 82.4 94.6 55.1 98.6 75.2 39.7 93.4 73.8 86.2 85.9 66.4 71.3 77.7 \n\nMethod \nroad \nsidew. \nbuild. \nwall \nfence \npole \nlight \nsign \nveget. \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorc. \nbicycle \nmIoU \n\nRefineNet \n90.1 65.7 86.4 31.2 48.1 58.0 76.7 70.3 89.7 45.7 97.3 70.8 15.4 87.1 35.0 43.1 79.1 38.7 59.9 62.5 \n5 \nHRNet \n93.6 75.2 89.0 42.0 55.6 67.7 83.3 78.9 93.0 48.9 97.8 78.1 16.4 92.6 54.8 61.6 87.0 50.0 68.9 70.2 \n\nRefineNet \n90.2 65.7 86.5 33.7 50.6 57.8 78.0 71.5 89.2 44.5 97.0 73.8 46.0 88.4 50.0 48.0 79.9 40.6 60.3 65.9 \n6 \nHRNet \n94.4 77.3 91.5 53.1 63.6 70.2 85.1 81.4 92.1 57.7 97.7 83.3 69.6 93.6 71.8 54.5 86.3 52.7 73.1 76.3 \n\nvi \n\n\n\n\nDeepLabv2 Max-Softmax 85.9 62.3 87.2 48.3 38.9 35.8 48.6 51.5 87.3 41.8 95.9 47.2 13.5 80.8 46.2 50.2 69.3 23.Method \nConfidence \nroad \nsidew. \nbuild. \nwall \nfence \npole \nlight \nsign \nveget. \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorc. \nbicycle \nmAUIoU \n\nRefineNet Max-Softmax 86.0 67.8 89.9 44.9 45.7 53.2 65.1 67.3 92.1 48.4 97.8 58.6 23.6 86.6 44.1 53.1 65.6 40.3 56.6 \n62.5 \nRefineNet GT \n89.5 70.8 92.1 54.1 53.2 59.9 72.6 72.3 93.9 52.1 98.4 67.4 26.6 88.7 52.4 56.4 75.5 51.4 62.9 \n67.9 \n9 50.0 \n56.0 \nDeepLabv2 GT \n87.8 65.1 89.4 52.1 42.5 40.2 53.7 56.1 89.6 43.6 96.8 53.4 13.8 82.7 50.2 48.1 72.9 33.3 51.4 \n59.1 \nDeepLabv3+ Max-Softmax 91.\n\n\nDeepLabv2 Max-Softmax 89.1 61.7 82.7 26.4 40.9 35.5 56.5 54.1 85.2 39.0 95.1 55.0 25.7 84.3 38.6 53.8 77.6 29.DeepLabv3+ Max-Softmax 90.6 67.0 88.8 45.1 48.9 57.8 76.6 72.9 90.8 45.7 97.0 74.8 28.4 89.2 63.3 67.8 87.8 36.7 61.1Method \nConfidence \nroad \nsidew. \nbuild. \nwall \nfence \npole \nlight \nsign \nveget. \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorc. \nbicycle \nmAUIoU \n\nRefineNet Max-Softmax 89.1 59.9 83.8 25.8 43.8 53.1 72.6 69.2 88.6 43.5 96.8 65.9 11.7 85.8 39.5 48.4 74.1 36.9 48.8 \n59.9 \nRefineNet GT \n91.3 69.1 86.8 32.4 49.9 59.0 78.2 72.8 90.0 52.5 97.3 71.8 16.1 87.6 37.6 44.7 79.5 39.8 60.1 \n64.0 \n0 49.5 \n56.8 \nDeepLabv2 GT \n90.3 65.1 83.1 27.6 42.7 36.5 57.9 56.7 85.5 46.3 95.1 56.4 26.4 85.0 41.1 55.0 78.2 30.2 49.8 \n58.4 \n67.9 \nDeepLabv3+ GT \n92.9 74.0 90.4 50.3 53.9 63.4 80.5 77.4 92.2 53.6 97.6 79.2 36.6 90.9 64.4 65.9 90.0 45.2 69.8 \n72.0 \n\n\nAcknowledgements. This work is funded by Toyota Motor Europe via the research project TRACE-Z\u00fcrich. We thank Ren\u00e9 Zurbr\u00fcgg for contributing to the construction of the website, and Anton Obukhov and Yuhang Lu for their advice on running HRNet and DANet.\nFully convolutional networks for semantic segmentation. Jonathan Long, Evan Shelhamer, Trevor Darrell, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015. 3\n\nTaking a closer look at domain shift: Categorylevel adversaries for semantics consistent domain adaptation. Yawei Luo, Liang Zheng, Tao Guan, Junqing Yu, Yi Yang, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 56Yawei Luo, Liang Zheng, Tao Guan, Junqing Yu, and Yi Yang. Taking a closer look at domain shift: Category- level adversaries for semantics consistent domain adaptation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 2, 3, 5, 6\n\n1 year, 1000 km: The Oxford RobotCar dataset. Will Maddern, Geoffrey Pascoe, Chris Linegar, Paul Newman, The International Journal of Robotics Research. 3613Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul Newman. 1 year, 1000 km: The Oxford RobotCar dataset. The International Journal of Robotics Research, 36(1):3-15, 2017. 1, 3\n\nThe Mapillary Vistas dataset for semantic understanding of street scenes. Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bul\u00f2, Peter Kontschieder, The IEEE International Conference on Computer Vision (ICCV. Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bul\u00f2, and Peter Kontschieder. The Mapillary Vistas dataset for seman- tic understanding of street scenes. In The IEEE International Conference on Computer Vision (ICCV), 2017. 1, 2, 3, 4\n\nThe ADUULM-Dataset -a semantic segmentation dataset for sensor fusion. Andreas Pfeuffer, Markus Sch\u00f6n, Ditzel Carsten, Klaus Dietmayer, Proceedings of the British Machine Vision Conference (BMVC). the British Machine Vision Conference (BMVC)2020Andreas Pfeuffer, Markus Sch\u00f6n, Ditzel Carsten, and Klaus Dietmayer. The ADUULM-Dataset -a semantic segmenta- tion dataset for sensor fusion. In Proceedings of the British Machine Vision Conference (BMVC), 2020. 3\n\nCanadian adverse driving conditions dataset. Matthew Pitropov, Danson Evan Garcia, Jason Rebello, Michael Smart, Carlos Wang, Krzysztof Czarnecki, Steven Waslander, The International Journal of Robotics Research. 20203Matthew Pitropov, Danson Evan Garcia, Jason Rebello, Michael Smart, Carlos Wang, Krzysztof Czarnecki, and Steven Waslander. Canadian adverse driving conditions dataset. The International Journal of Robotics Research, 2020. 3\n\nFull-resolution residual networks for semantic segmentation in street scenes. Tobias Pohlen, Alexander Hermans, Markus Mathias, Bastian Leibe, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Tobias Pohlen, Alexander Hermans, Markus Mathias, and Bastian Leibe. Full-resolution residual networks for seman- tic segmentation in street scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR), July 2017. 3\n\nDon't worry about the weather: Unsupervised condition-dependent domain adaptation. Horia Porav, Tom Bruls, Paul Newman, 2019 IEEE Intelligent Transportation Systems Conference (ITSC). Horia Porav, Tom Bruls, and Paul Newman. Don't worry about the weather: Unsupervised condition-dependent do- main adaptation. In 2019 IEEE Intelligent Transportation Systems Conference (ITSC), pages 33-40, 2019. 2\n\nPlaying for benchmarks. Stephan R Richter, Zeeshan Hayder, Vladlen Koltun, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)Stephan R. Richter, Zeeshan Hayder, and Vladlen Koltun. Playing for benchmarks. In Proceedings of the IEEE Inter- national Conference on Computer Vision (ICCV), October 2017. 3\n\nPlaying for data: Ground truth from computer games. Stephan R Richter, Vibhav Vineet, Stefan Roth, Vladlen Koltun, European Conference on Computer Vision. Springer23Stephan R. Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from computer games. In European Conference on Computer Vision. Springer, 2016. 2, 3\n\nU-Net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- Net: Convolutional networks for biomedical image segmen- tation. In Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015, pages 234-241, 2015. 3\n\nThe SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes. German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, Antonio M Lopez, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 23German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M. Lopez. The SYNTHIA dataset: A large collection of synthetic images for semantic segmen- tation of urban scenes. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. 2, 3\n\nModel adaptation with synthetic and real data for semantic dense foggy scene understanding. Christos Sakaridis, Dengxin Dai, Simon Hecker, Luc Van Gool, The European Conference on Computer Vision (ECCV). 6Christos Sakaridis, Dengxin Dai, Simon Hecker, and Luc Van Gool. Model adaptation with synthetic and real data for semantic dense foggy scene understanding. In The European Conference on Computer Vision (ECCV), 2018. 2, 3, 6\n\nSemantic foggy scene understanding with synthetic data. Christos Sakaridis, Dengxin Dai, Luc Van Gool, International Journal of Computer Vision. 12696Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Seman- tic foggy scene understanding with synthetic data. Interna- tional Journal of Computer Vision, 126(9):973-992, 2018. 2, 3, 5, 6\n\nGuided curriculum model adaptation and uncertainty-aware evaluation for semantic nighttime image segmentation. Christos Sakaridis, Dengxin Dai, Luc Van Gool, The IEEE International Conference on Computer Vision (ICCV). 6Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Guided curriculum model adaptation and uncertainty-aware evalu- ation for semantic nighttime image segmentation. In The IEEE International Conference on Computer Vision (ICCV), 2019. 2, 6\n\nMapguided curriculum domain adaptation and uncertainty-aware evaluation for semantic nighttime image segmentation. Christos Sakaridis, Dengxin Dai, Luc Van Gool, IEEE Transactions on Pattern Analysis and Machine Intelligence. 67Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Map- guided curriculum domain adaptation and uncertainty-aware evaluation for semantic nighttime image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. 1, 2, 3, 5, 6, 7\n\nLearning from synthetic data: Addressing domain shift for semantic segmentation. Swami Sankaranarayanan, Yogesh Balaji, Arpit Jain, Nam Ser, Rama Lim, Chellappa, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 23Swami Sankaranarayanan, Yogesh Balaji, Arpit Jain, Ser Nam Lim, and Rama Chellappa. Learning from synthetic data: Addressing domain shift for semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), June 2018. 2, 3\n\nJonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset. Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 13Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Et- tinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. 1, 3\n\nLearning to adapt structured output space for semantic segmentation. Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, Mammohan Chandraker, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 6Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Mammohan Chandraker. Learning to adapt structured output space for semantic seg- mentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2, 3, 5, 6, i\n\nDomain adaptation for structured output via discriminative patch representations. Yi-Hsuan Tsai, Kihyuk Sohn, Samuel Schulter, Manmohan Chandraker, IEEE/CVF International Conference on Computer Vision (ICCV). 23Yi-Hsuan Tsai, Kihyuk Sohn, Samuel Schulter, and Manmo- han Chandraker. Domain adaptation for structured output via discriminative patch representations. In IEEE/CVF In- ternational Conference on Computer Vision (ICCV), October 2019. 2, 3\n\nThe Raincouver scene parsing benchmark for self-driving in adverse weather and at night. Frederick Tung, Jianhui Chen, Lili Meng, James J Little, IEEE Robotics and Automation Letters. 245Frederick Tung, Jianhui Chen, Lili Meng, and James J. Little. The Raincouver scene parsing benchmark for self-driving in adverse weather and at night. IEEE Robotics and Automation Letters, 2(4):2188-2193, 2017. 3, 5\n\nADVENT: Adversarial entropy minimization for domain adaptation in semantic segmentation. Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, Patrick Perez, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 56Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Perez. ADVENT: Adversarial entropy minimization for domain adaptation in semantic segmenta- tion. In The IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR), 2019. 2, 3, 5, 6\n\nDeep high-resolution representation learning for visual recognition. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, Bin Xiao, IEEE Transactions on Pattern Analysis and Machine Intelligence. 637Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, and Bin Xiao. Deep high-resolution representation learning for visual recogni- tion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. 3, 6, 7\n\nDifferential treatment for stuff and things: A simple unsupervised domain adaptation method for semantic segmen. Zhonghao Wang, Mo Yu, Yunchao Wei, Rogerio Feris, Jinjun Xiong, Wen-Mei Hwu, Thomas S Huang, Honghui Shi, Zhonghao Wang, Mo Yu, Yunchao Wei, Rogerio Feris, Jin- jun Xiong, Wen-mei Hwu, Thomas S. Huang, and Honghui Shi. Differential treatment for stuff and things: A simple un- supervised domain adaptation method for semantic segmen-\n", "annotations": {"author": "[{\"end\":153,\"start\":102},{\"end\":198,\"start\":154},{\"end\":244,\"start\":199},{\"end\":288,\"start\":245}]", "publisher": null, "author_last_name": "[{\"end\":120,\"start\":111},{\"end\":165,\"start\":162},{\"end\":211,\"start\":207},{\"end\":255,\"start\":249}]", "author_first_name": "[{\"end\":110,\"start\":102},{\"end\":161,\"start\":154},{\"end\":202,\"start\":199},{\"end\":206,\"start\":203},{\"end\":248,\"start\":245}]", "author_affiliation": "[{\"end\":152,\"start\":122},{\"end\":197,\"start\":167},{\"end\":243,\"start\":213},{\"end\":287,\"start\":257}]", "title": "[{\"end\":99,\"start\":1},{\"end\":387,\"start\":289}]", "venue": null, "abstract": "[{\"end\":1656,\"start\":389}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1797,\"start\":1793},{\"end\":1812,\"start\":1802},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2202,\"start\":2198},{\"end\":2219,\"start\":2215},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2282,\"start\":2278},{\"end\":2285,\"start\":2282},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2390,\"start\":2387},{\"end\":2393,\"start\":2390},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2596,\"start\":2592},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2801,\"start\":2797},{\"end\":2841,\"start\":2838},{\"end\":5695,\"start\":5692},{\"end\":5698,\"start\":5695},{\"end\":5701,\"start\":5698},{\"end\":5704,\"start\":5701},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5707,\"start\":5704},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5710,\"start\":5707},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5713,\"start\":5710},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5716,\"start\":5713},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5719,\"start\":5716},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5722,\"start\":5719},{\"end\":5725,\"start\":5722},{\"end\":5728,\"start\":5725},{\"end\":5731,\"start\":5728},{\"end\":5734,\"start\":5731},{\"end\":5737,\"start\":5734},{\"end\":5740,\"start\":5737},{\"end\":5743,\"start\":5740},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5919,\"start\":5915},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5936,\"start\":5932},{\"end\":6349,\"start\":6345},{\"end\":6352,\"start\":6349},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6355,\"start\":6352},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6358,\"start\":6355},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6361,\"start\":6358},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6364,\"start\":6361},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6367,\"start\":6364},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6483,\"start\":6480},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6486,\"start\":6483},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6496,\"start\":6492},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6499,\"start\":6496},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6601,\"start\":6597},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8748,\"start\":8744},{\"end\":8773,\"start\":8769},{\"end\":8843,\"start\":8840},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8846,\"start\":8843},{\"end\":8932,\"start\":8928},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8935,\"start\":8932},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8938,\"start\":8935},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8941,\"start\":8938},{\"end\":9079,\"start\":9075},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9082,\"start\":9079},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9085,\"start\":9082},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9107,\"start\":9103},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9394,\"start\":9390},{\"end\":9546,\"start\":9542},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9713,\"start\":9709},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9801,\"start\":9797},{\"end\":9965,\"start\":9962},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9968,\"start\":9965},{\"end\":9984,\"start\":9980},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9987,\"start\":9984},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10002,\"start\":9998},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10038,\"start\":10034},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10612,\"start\":10608},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10726,\"start\":10722},{\"end\":10905,\"start\":10901},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10908,\"start\":10905},{\"end\":11475,\"start\":11471},{\"end\":11634,\"start\":11631},{\"end\":11637,\"start\":11634},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11640,\"start\":11637},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11643,\"start\":11640},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11646,\"start\":11643},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11649,\"start\":11646},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11652,\"start\":11649},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11655,\"start\":11652},{\"end\":11658,\"start\":11655},{\"end\":11750,\"start\":11746},{\"end\":11753,\"start\":11750},{\"end\":11811,\"start\":11807},{\"end\":11892,\"start\":11888},{\"end\":11895,\"start\":11892},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15804,\"start\":15801},{\"end\":15806,\"start\":15804},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18984,\"start\":18980},{\"end\":18987,\"start\":18984},{\"end\":21385,\"start\":21382},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24461,\"start\":24457},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28195,\"start\":28191},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31584,\"start\":31580},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":33412,\"start\":33408},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":47277,\"start\":47273},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":47311,\"start\":47307},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":47333,\"start\":47329},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":47382,\"start\":47378}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":42214,\"start\":42149},{\"attributes\":{\"id\":\"fig_1\"},\"end\":42458,\"start\":42215},{\"attributes\":{\"id\":\"fig_2\"},\"end\":42693,\"start\":42459},{\"attributes\":{\"id\":\"fig_3\"},\"end\":43086,\"start\":42694},{\"attributes\":{\"id\":\"fig_4\"},\"end\":43358,\"start\":43087},{\"attributes\":{\"id\":\"fig_5\"},\"end\":43447,\"start\":43359},{\"attributes\":{\"id\":\"fig_6\"},\"end\":43562,\"start\":43448},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":43940,\"start\":43563},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":47001,\"start\":43941},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":47915,\"start\":47002},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":48712,\"start\":47916},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":49494,\"start\":48713},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":51629,\"start\":49495},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":62143,\"start\":51630},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":62874,\"start\":62144},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":65092,\"start\":62875},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":69749,\"start\":65093},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":74663,\"start\":69750},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":81575,\"start\":74664},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":82246,\"start\":81576},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":83132,\"start\":82247}]", "paragraph": "[{\"end\":3125,\"start\":1672},{\"end\":3964,\"start\":3127},{\"end\":5519,\"start\":3966},{\"end\":7252,\"start\":5521},{\"end\":8423,\"start\":7254},{\"end\":8774,\"start\":8440},{\"end\":10434,\"start\":8776},{\"end\":11304,\"start\":10436},{\"end\":12383,\"start\":11306},{\"end\":12599,\"start\":12400},{\"end\":13196,\"start\":12614},{\"end\":14022,\"start\":13198},{\"end\":14247,\"start\":14024},{\"end\":15037,\"start\":14280},{\"end\":15514,\"start\":15056},{\"end\":16408,\"start\":15516},{\"end\":16997,\"start\":16423},{\"end\":17694,\"start\":16999},{\"end\":18094,\"start\":17696},{\"end\":18506,\"start\":18096},{\"end\":19611,\"start\":18508},{\"end\":20728,\"start\":19646},{\"end\":21017,\"start\":20754},{\"end\":21945,\"start\":21050},{\"end\":22688,\"start\":21956},{\"end\":23058,\"start\":22690},{\"end\":23665,\"start\":23103},{\"end\":24222,\"start\":23711},{\"end\":24462,\"start\":24224},{\"end\":26684,\"start\":24464},{\"end\":27092,\"start\":26686},{\"end\":27687,\"start\":27094},{\"end\":28422,\"start\":27731},{\"end\":29582,\"start\":28448},{\"end\":29868,\"start\":29584},{\"end\":30320,\"start\":29895},{\"end\":31142,\"start\":30322},{\"end\":31405,\"start\":31179},{\"end\":32513,\"start\":31443},{\"end\":33081,\"start\":32515},{\"end\":34211,\"start\":33132},{\"end\":34563,\"start\":34260},{\"end\":35573,\"start\":34612},{\"end\":35763,\"start\":35609},{\"end\":35963,\"start\":35801},{\"end\":36140,\"start\":36013},{\"end\":36574,\"start\":36191},{\"end\":37198,\"start\":36623},{\"end\":37522,\"start\":37240},{\"end\":38037,\"start\":37542},{\"end\":39105,\"start\":38075},{\"end\":39567,\"start\":39125},{\"end\":41241,\"start\":39569},{\"end\":41340,\"start\":41243},{\"end\":41941,\"start\":41342},{\"end\":42118,\"start\":41943},{\"end\":42148,\"start\":42120}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":19225,\"start\":19218},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":19772,\"start\":19765},{\"end\":21318,\"start\":21311},{\"end\":21509,\"start\":21502},{\"end\":22809,\"start\":22802},{\"end\":22937,\"start\":22930},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23113,\"start\":23106},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":23828,\"start\":23821},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24220,\"start\":24211},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24598,\"start\":24584},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25349,\"start\":25342},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":26463,\"start\":26456},{\"end\":27121,\"start\":27114},{\"end\":28606,\"start\":28599},{\"end\":31472,\"start\":31465},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":32025,\"start\":32017},{\"end\":32635,\"start\":32628},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":32942,\"start\":32934},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":33607,\"start\":33599},{\"end\":34710,\"start\":34703},{\"end\":35962,\"start\":35955},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":36139,\"start\":36132},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":36206,\"start\":36194},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":36327,\"start\":36320},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":36638,\"start\":36626},{\"end\":36774,\"start\":36767},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":36803,\"start\":36795},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":37197,\"start\":37173},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":39641,\"start\":39633},{\"end\":40355,\"start\":40347},{\"end\":40676,\"start\":40668},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":41713,\"start\":41705}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1670,\"start\":1658},{\"attributes\":{\"n\":\"2.\"},\"end\":8438,\"start\":8426},{\"attributes\":{\"n\":\"3.\"},\"end\":12398,\"start\":12386},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12612,\"start\":12602},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14278,\"start\":14250},{\"attributes\":{\"n\":\"3.3.\"},\"end\":15054,\"start\":15040},{\"attributes\":{\"n\":\"3.4.\"},\"end\":16421,\"start\":16411},{\"attributes\":{\"n\":\"3.5.\"},\"end\":19644,\"start\":19614},{\"attributes\":{\"n\":\"4.\"},\"end\":20752,\"start\":20731},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21048,\"start\":21020},{\"end\":21954,\"start\":21948},{\"attributes\":{\"n\":\"4.2.\"},\"end\":23101,\"start\":23061},{\"attributes\":{\"n\":\"4.3.\"},\"end\":23709,\"start\":23668},{\"attributes\":{\"n\":\"5.\"},\"end\":27729,\"start\":27690},{\"attributes\":{\"n\":\"5.1.\"},\"end\":28446,\"start\":28425},{\"attributes\":{\"n\":\"6.\"},\"end\":29893,\"start\":29871},{\"end\":31155,\"start\":31145},{\"end\":31177,\"start\":31158},{\"end\":31441,\"start\":31408},{\"end\":33130,\"start\":33084},{\"end\":34258,\"start\":34214},{\"end\":34610,\"start\":34566},{\"end\":35607,\"start\":35576},{\"end\":35799,\"start\":35766},{\"end\":36011,\"start\":35966},{\"end\":36189,\"start\":36143},{\"end\":36621,\"start\":36577},{\"end\":37238,\"start\":37201},{\"end\":37540,\"start\":37525},{\"end\":38073,\"start\":38040},{\"end\":39123,\"start\":39108},{\"end\":42160,\"start\":42150},{\"end\":42226,\"start\":42216},{\"end\":42470,\"start\":42460},{\"end\":42705,\"start\":42695},{\"end\":43089,\"start\":43088},{\"end\":43370,\"start\":43360},{\"end\":47012,\"start\":47003},{\"end\":47926,\"start\":47917},{\"end\":48723,\"start\":48714},{\"end\":49505,\"start\":49496},{\"end\":51636,\"start\":51631},{\"end\":62165,\"start\":62145},{\"end\":62886,\"start\":62876},{\"end\":65104,\"start\":65094},{\"end\":69761,\"start\":69751},{\"end\":74685,\"start\":74665}]", "table": "[{\"end\":47001,\"start\":44049},{\"end\":47915,\"start\":47383},{\"end\":48712,\"start\":48103},{\"end\":49494,\"start\":49096},{\"end\":51629,\"start\":50374},{\"end\":62143,\"start\":53413},{\"end\":62874,\"start\":62510},{\"end\":65092,\"start\":63156},{\"end\":69749,\"start\":65216},{\"end\":74663,\"start\":70065},{\"end\":81575,\"start\":78850},{\"end\":82246,\"start\":81688},{\"end\":83132,\"start\":82476}]", "figure_caption": "[{\"end\":42214,\"start\":42162},{\"end\":42458,\"start\":42228},{\"end\":42693,\"start\":42472},{\"end\":43086,\"start\":42707},{\"end\":43358,\"start\":43090},{\"end\":43447,\"start\":43372},{\"end\":43562,\"start\":43450},{\"end\":43940,\"start\":43565},{\"end\":44049,\"start\":43943},{\"end\":47383,\"start\":47014},{\"end\":48103,\"start\":47928},{\"end\":49096,\"start\":48725},{\"end\":50374,\"start\":49507},{\"end\":53413,\"start\":51638},{\"end\":62510,\"start\":62170},{\"end\":63156,\"start\":62889},{\"end\":65216,\"start\":65107},{\"end\":70065,\"start\":69764},{\"end\":78850,\"start\":74690},{\"end\":81688,\"start\":81578},{\"end\":82476,\"start\":82249}]", "figure_ref": "[{\"end\":19041,\"start\":19037},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19111,\"start\":19105},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23869,\"start\":23863},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":34621,\"start\":34615},{\"end\":37521,\"start\":37515},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":39134,\"start\":39128},{\"end\":41467,\"start\":41459}]", "bib_author_first_name": "[{\"end\":83450,\"start\":83442},{\"end\":83461,\"start\":83457},{\"end\":83479,\"start\":83473},{\"end\":83969,\"start\":83964},{\"end\":83980,\"start\":83975},{\"end\":83991,\"start\":83988},{\"end\":84005,\"start\":83998},{\"end\":84012,\"start\":84010},{\"end\":84403,\"start\":84399},{\"end\":84421,\"start\":84413},{\"end\":84435,\"start\":84430},{\"end\":84449,\"start\":84445},{\"end\":84772,\"start\":84765},{\"end\":84788,\"start\":84782},{\"end\":84804,\"start\":84798},{\"end\":84809,\"start\":84805},{\"end\":84821,\"start\":84816},{\"end\":85207,\"start\":85200},{\"end\":85224,\"start\":85218},{\"end\":85238,\"start\":85232},{\"end\":85253,\"start\":85248},{\"end\":85641,\"start\":85634},{\"end\":85658,\"start\":85652},{\"end\":85663,\"start\":85659},{\"end\":85677,\"start\":85672},{\"end\":85694,\"start\":85687},{\"end\":85708,\"start\":85702},{\"end\":85724,\"start\":85715},{\"end\":85742,\"start\":85736},{\"end\":86117,\"start\":86111},{\"end\":86135,\"start\":86126},{\"end\":86151,\"start\":86145},{\"end\":86168,\"start\":86161},{\"end\":86673,\"start\":86668},{\"end\":86684,\"start\":86681},{\"end\":86696,\"start\":86692},{\"end\":87015,\"start\":87008},{\"end\":87017,\"start\":87016},{\"end\":87034,\"start\":87027},{\"end\":87050,\"start\":87043},{\"end\":87431,\"start\":87424},{\"end\":87433,\"start\":87432},{\"end\":87449,\"start\":87443},{\"end\":87464,\"start\":87458},{\"end\":87478,\"start\":87471},{\"end\":87791,\"start\":87787},{\"end\":87812,\"start\":87805},{\"end\":87828,\"start\":87822},{\"end\":88237,\"start\":88231},{\"end\":88248,\"start\":88243},{\"end\":88264,\"start\":88258},{\"end\":88283,\"start\":88278},{\"end\":88300,\"start\":88293},{\"end\":88302,\"start\":88301},{\"end\":88763,\"start\":88755},{\"end\":88782,\"start\":88775},{\"end\":88793,\"start\":88788},{\"end\":88805,\"start\":88802},{\"end\":89158,\"start\":89150},{\"end\":89177,\"start\":89170},{\"end\":89186,\"start\":89183},{\"end\":89550,\"start\":89542},{\"end\":89569,\"start\":89562},{\"end\":89578,\"start\":89575},{\"end\":90014,\"start\":90006},{\"end\":90033,\"start\":90026},{\"end\":90042,\"start\":90039},{\"end\":90461,\"start\":90456},{\"end\":90486,\"start\":90480},{\"end\":90500,\"start\":90495},{\"end\":90510,\"start\":90507},{\"end\":90520,\"start\":90516},{\"end\":90987,\"start\":90984},{\"end\":90999,\"start\":90993},{\"end\":91019,\"start\":91013},{\"end\":91039,\"start\":91031},{\"end\":91057,\"start\":91049},{\"end\":91071,\"start\":91067},{\"end\":91083,\"start\":91078},{\"end\":91092,\"start\":91089},{\"end\":91105,\"start\":91099},{\"end\":91120,\"start\":91112},{\"end\":91133,\"start\":91128},{\"end\":91148,\"start\":91145},{\"end\":91160,\"start\":91154},{\"end\":91172,\"start\":91168},{\"end\":91186,\"start\":91179},{\"end\":91202,\"start\":91197},{\"end\":91218,\"start\":91213},{\"end\":91232,\"start\":91229},{\"end\":91244,\"start\":91238},{\"end\":91254,\"start\":91252},{\"end\":91907,\"start\":91899},{\"end\":91922,\"start\":91914},{\"end\":91935,\"start\":91929},{\"end\":91952,\"start\":91946},{\"end\":91969,\"start\":91959},{\"end\":91984,\"start\":91976},{\"end\":92427,\"start\":92419},{\"end\":92440,\"start\":92434},{\"end\":92453,\"start\":92447},{\"end\":92472,\"start\":92464},{\"end\":92886,\"start\":92877},{\"end\":92900,\"start\":92893},{\"end\":92911,\"start\":92907},{\"end\":92923,\"start\":92918},{\"end\":92925,\"start\":92924},{\"end\":93290,\"start\":93281},{\"end\":93303,\"start\":93295},{\"end\":93316,\"start\":93310},{\"end\":93333,\"start\":93325},{\"end\":93347,\"start\":93340},{\"end\":93768,\"start\":93760},{\"end\":93777,\"start\":93775},{\"end\":93791,\"start\":93783},{\"end\":93804,\"start\":93799},{\"end\":93819,\"start\":93812},{\"end\":93830,\"start\":93826},{\"end\":93841,\"start\":93837},{\"end\":93853,\"start\":93847},{\"end\":93865,\"start\":93858},{\"end\":93879,\"start\":93871},{\"end\":93891,\"start\":93886},{\"end\":93900,\"start\":93897},{\"end\":94396,\"start\":94388},{\"end\":94405,\"start\":94403},{\"end\":94417,\"start\":94410},{\"end\":94430,\"start\":94423},{\"end\":94444,\"start\":94438},{\"end\":94459,\"start\":94452},{\"end\":94471,\"start\":94465},{\"end\":94473,\"start\":94472},{\"end\":94488,\"start\":94481}]", "bib_author_last_name": "[{\"end\":83455,\"start\":83451},{\"end\":83471,\"start\":83462},{\"end\":83487,\"start\":83480},{\"end\":83973,\"start\":83970},{\"end\":83986,\"start\":83981},{\"end\":83996,\"start\":83992},{\"end\":84008,\"start\":84006},{\"end\":84017,\"start\":84013},{\"end\":84411,\"start\":84404},{\"end\":84428,\"start\":84422},{\"end\":84443,\"start\":84436},{\"end\":84456,\"start\":84450},{\"end\":84780,\"start\":84773},{\"end\":84796,\"start\":84789},{\"end\":84814,\"start\":84810},{\"end\":84834,\"start\":84822},{\"end\":85216,\"start\":85208},{\"end\":85230,\"start\":85225},{\"end\":85246,\"start\":85239},{\"end\":85263,\"start\":85254},{\"end\":85650,\"start\":85642},{\"end\":85670,\"start\":85664},{\"end\":85685,\"start\":85678},{\"end\":85700,\"start\":85695},{\"end\":85713,\"start\":85709},{\"end\":85734,\"start\":85725},{\"end\":85752,\"start\":85743},{\"end\":86124,\"start\":86118},{\"end\":86143,\"start\":86136},{\"end\":86159,\"start\":86152},{\"end\":86174,\"start\":86169},{\"end\":86679,\"start\":86674},{\"end\":86690,\"start\":86685},{\"end\":86703,\"start\":86697},{\"end\":87025,\"start\":87018},{\"end\":87041,\"start\":87035},{\"end\":87057,\"start\":87051},{\"end\":87441,\"start\":87434},{\"end\":87456,\"start\":87450},{\"end\":87469,\"start\":87465},{\"end\":87485,\"start\":87479},{\"end\":87803,\"start\":87792},{\"end\":87820,\"start\":87813},{\"end\":87833,\"start\":87829},{\"end\":88241,\"start\":88238},{\"end\":88256,\"start\":88249},{\"end\":88276,\"start\":88265},{\"end\":88291,\"start\":88284},{\"end\":88308,\"start\":88303},{\"end\":88773,\"start\":88764},{\"end\":88786,\"start\":88783},{\"end\":88800,\"start\":88794},{\"end\":88814,\"start\":88806},{\"end\":89168,\"start\":89159},{\"end\":89181,\"start\":89178},{\"end\":89195,\"start\":89187},{\"end\":89560,\"start\":89551},{\"end\":89573,\"start\":89570},{\"end\":89587,\"start\":89579},{\"end\":90024,\"start\":90015},{\"end\":90037,\"start\":90034},{\"end\":90051,\"start\":90043},{\"end\":90478,\"start\":90462},{\"end\":90493,\"start\":90487},{\"end\":90505,\"start\":90501},{\"end\":90514,\"start\":90511},{\"end\":90524,\"start\":90521},{\"end\":90535,\"start\":90526},{\"end\":90991,\"start\":90988},{\"end\":91011,\"start\":91000},{\"end\":91029,\"start\":91020},{\"end\":91047,\"start\":91040},{\"end\":91065,\"start\":91058},{\"end\":91076,\"start\":91072},{\"end\":91087,\"start\":91084},{\"end\":91097,\"start\":91093},{\"end\":91110,\"start\":91106},{\"end\":91126,\"start\":91121},{\"end\":91143,\"start\":91134},{\"end\":91152,\"start\":91149},{\"end\":91166,\"start\":91161},{\"end\":91177,\"start\":91173},{\"end\":91195,\"start\":91187},{\"end\":91211,\"start\":91203},{\"end\":91227,\"start\":91219},{\"end\":91236,\"start\":91233},{\"end\":91250,\"start\":91245},{\"end\":91260,\"start\":91255},{\"end\":91912,\"start\":91908},{\"end\":91927,\"start\":91923},{\"end\":91944,\"start\":91936},{\"end\":91957,\"start\":91953},{\"end\":91974,\"start\":91970},{\"end\":91995,\"start\":91985},{\"end\":92432,\"start\":92428},{\"end\":92445,\"start\":92441},{\"end\":92462,\"start\":92454},{\"end\":92483,\"start\":92473},{\"end\":92891,\"start\":92887},{\"end\":92905,\"start\":92901},{\"end\":92916,\"start\":92912},{\"end\":92932,\"start\":92926},{\"end\":93293,\"start\":93291},{\"end\":93308,\"start\":93304},{\"end\":93323,\"start\":93317},{\"end\":93338,\"start\":93334},{\"end\":93353,\"start\":93348},{\"end\":93773,\"start\":93769},{\"end\":93781,\"start\":93778},{\"end\":93797,\"start\":93792},{\"end\":93810,\"start\":93805},{\"end\":93824,\"start\":93820},{\"end\":93835,\"start\":93831},{\"end\":93845,\"start\":93842},{\"end\":93856,\"start\":93854},{\"end\":93869,\"start\":93866},{\"end\":93884,\"start\":93880},{\"end\":93895,\"start\":93892},{\"end\":93905,\"start\":93901},{\"end\":94401,\"start\":94397},{\"end\":94408,\"start\":94406},{\"end\":94421,\"start\":94418},{\"end\":94436,\"start\":94431},{\"end\":94450,\"start\":94445},{\"end\":94463,\"start\":94460},{\"end\":94479,\"start\":94474},{\"end\":94492,\"start\":94489}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1629541},\"end\":83854,\"start\":83386},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52825087},\"end\":84351,\"start\":83856},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":22556995},\"end\":84689,\"start\":84353},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":5753855},\"end\":85127,\"start\":84691},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":221669945},\"end\":85587,\"start\":85129},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":210932700},\"end\":86031,\"start\":85589},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1873339},\"end\":86583,\"start\":86033},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":198895255},\"end\":86982,\"start\":86585},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":10060022},\"end\":87370,\"start\":86984},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5844139},\"end\":87720,\"start\":87372},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3719281},\"end\":88126,\"start\":87722},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206594095},\"end\":88661,\"start\":88128},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":51914060},\"end\":89092,\"start\":88663},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1103605},\"end\":89429,\"start\":89094},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":198953430},\"end\":89889,\"start\":89431},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":219124283},\"end\":90373,\"start\":89891},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4540721},\"end\":90858,\"start\":90375},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":209140225},\"end\":91828,\"start\":90860},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3556146},\"end\":92335,\"start\":91830},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":58014164},\"end\":92786,\"start\":92337},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":26498979},\"end\":93190,\"start\":92788},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":54216961},\"end\":93689,\"start\":93192},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":201124533},\"end\":94273,\"start\":93691},{\"attributes\":{\"id\":\"b23\"},\"end\":94721,\"start\":94275}]", "bib_title": "[{\"end\":83440,\"start\":83386},{\"end\":83962,\"start\":83856},{\"end\":84397,\"start\":84353},{\"end\":84763,\"start\":84691},{\"end\":85198,\"start\":85129},{\"end\":85632,\"start\":85589},{\"end\":86109,\"start\":86033},{\"end\":86666,\"start\":86585},{\"end\":87006,\"start\":86984},{\"end\":87422,\"start\":87372},{\"end\":87785,\"start\":87722},{\"end\":88229,\"start\":88128},{\"end\":88753,\"start\":88663},{\"end\":89148,\"start\":89094},{\"end\":89540,\"start\":89431},{\"end\":90004,\"start\":89891},{\"end\":90454,\"start\":90375},{\"end\":90982,\"start\":90860},{\"end\":91897,\"start\":91830},{\"end\":92417,\"start\":92337},{\"end\":92875,\"start\":92788},{\"end\":93279,\"start\":93192},{\"end\":93758,\"start\":93691}]", "bib_author": "[{\"end\":83457,\"start\":83442},{\"end\":83473,\"start\":83457},{\"end\":83489,\"start\":83473},{\"end\":83975,\"start\":83964},{\"end\":83988,\"start\":83975},{\"end\":83998,\"start\":83988},{\"end\":84010,\"start\":83998},{\"end\":84019,\"start\":84010},{\"end\":84413,\"start\":84399},{\"end\":84430,\"start\":84413},{\"end\":84445,\"start\":84430},{\"end\":84458,\"start\":84445},{\"end\":84782,\"start\":84765},{\"end\":84798,\"start\":84782},{\"end\":84816,\"start\":84798},{\"end\":84836,\"start\":84816},{\"end\":85218,\"start\":85200},{\"end\":85232,\"start\":85218},{\"end\":85248,\"start\":85232},{\"end\":85265,\"start\":85248},{\"end\":85652,\"start\":85634},{\"end\":85672,\"start\":85652},{\"end\":85687,\"start\":85672},{\"end\":85702,\"start\":85687},{\"end\":85715,\"start\":85702},{\"end\":85736,\"start\":85715},{\"end\":85754,\"start\":85736},{\"end\":86126,\"start\":86111},{\"end\":86145,\"start\":86126},{\"end\":86161,\"start\":86145},{\"end\":86176,\"start\":86161},{\"end\":86681,\"start\":86668},{\"end\":86692,\"start\":86681},{\"end\":86705,\"start\":86692},{\"end\":87027,\"start\":87008},{\"end\":87043,\"start\":87027},{\"end\":87059,\"start\":87043},{\"end\":87443,\"start\":87424},{\"end\":87458,\"start\":87443},{\"end\":87471,\"start\":87458},{\"end\":87487,\"start\":87471},{\"end\":87805,\"start\":87787},{\"end\":87822,\"start\":87805},{\"end\":87835,\"start\":87822},{\"end\":88243,\"start\":88231},{\"end\":88258,\"start\":88243},{\"end\":88278,\"start\":88258},{\"end\":88293,\"start\":88278},{\"end\":88310,\"start\":88293},{\"end\":88775,\"start\":88755},{\"end\":88788,\"start\":88775},{\"end\":88802,\"start\":88788},{\"end\":88816,\"start\":88802},{\"end\":89170,\"start\":89150},{\"end\":89183,\"start\":89170},{\"end\":89197,\"start\":89183},{\"end\":89562,\"start\":89542},{\"end\":89575,\"start\":89562},{\"end\":89589,\"start\":89575},{\"end\":90026,\"start\":90006},{\"end\":90039,\"start\":90026},{\"end\":90053,\"start\":90039},{\"end\":90480,\"start\":90456},{\"end\":90495,\"start\":90480},{\"end\":90507,\"start\":90495},{\"end\":90516,\"start\":90507},{\"end\":90526,\"start\":90516},{\"end\":90537,\"start\":90526},{\"end\":90993,\"start\":90984},{\"end\":91013,\"start\":90993},{\"end\":91031,\"start\":91013},{\"end\":91049,\"start\":91031},{\"end\":91067,\"start\":91049},{\"end\":91078,\"start\":91067},{\"end\":91089,\"start\":91078},{\"end\":91099,\"start\":91089},{\"end\":91112,\"start\":91099},{\"end\":91128,\"start\":91112},{\"end\":91145,\"start\":91128},{\"end\":91154,\"start\":91145},{\"end\":91168,\"start\":91154},{\"end\":91179,\"start\":91168},{\"end\":91197,\"start\":91179},{\"end\":91213,\"start\":91197},{\"end\":91229,\"start\":91213},{\"end\":91238,\"start\":91229},{\"end\":91252,\"start\":91238},{\"end\":91262,\"start\":91252},{\"end\":91914,\"start\":91899},{\"end\":91929,\"start\":91914},{\"end\":91946,\"start\":91929},{\"end\":91959,\"start\":91946},{\"end\":91976,\"start\":91959},{\"end\":91997,\"start\":91976},{\"end\":92434,\"start\":92419},{\"end\":92447,\"start\":92434},{\"end\":92464,\"start\":92447},{\"end\":92485,\"start\":92464},{\"end\":92893,\"start\":92877},{\"end\":92907,\"start\":92893},{\"end\":92918,\"start\":92907},{\"end\":92934,\"start\":92918},{\"end\":93295,\"start\":93281},{\"end\":93310,\"start\":93295},{\"end\":93325,\"start\":93310},{\"end\":93340,\"start\":93325},{\"end\":93355,\"start\":93340},{\"end\":93775,\"start\":93760},{\"end\":93783,\"start\":93775},{\"end\":93799,\"start\":93783},{\"end\":93812,\"start\":93799},{\"end\":93826,\"start\":93812},{\"end\":93837,\"start\":93826},{\"end\":93847,\"start\":93837},{\"end\":93858,\"start\":93847},{\"end\":93871,\"start\":93858},{\"end\":93886,\"start\":93871},{\"end\":93897,\"start\":93886},{\"end\":93907,\"start\":93897},{\"end\":94403,\"start\":94388},{\"end\":94410,\"start\":94403},{\"end\":94423,\"start\":94410},{\"end\":94438,\"start\":94423},{\"end\":94452,\"start\":94438},{\"end\":94465,\"start\":94452},{\"end\":94481,\"start\":94465},{\"end\":94494,\"start\":94481}]", "bib_venue": "[{\"end\":83644,\"start\":83575},{\"end\":85370,\"start\":85326},{\"end\":86331,\"start\":86262},{\"end\":87194,\"start\":87135},{\"end\":83573,\"start\":83489},{\"end\":84088,\"start\":84019},{\"end\":84504,\"start\":84458},{\"end\":84894,\"start\":84836},{\"end\":85324,\"start\":85265},{\"end\":85800,\"start\":85754},{\"end\":86260,\"start\":86176},{\"end\":86767,\"start\":86705},{\"end\":87133,\"start\":87059},{\"end\":87525,\"start\":87487},{\"end\":87906,\"start\":87835},{\"end\":88379,\"start\":88310},{\"end\":88865,\"start\":88816},{\"end\":89237,\"start\":89197},{\"end\":89648,\"start\":89589},{\"end\":90115,\"start\":90053},{\"end\":90602,\"start\":90537},{\"end\":91331,\"start\":91262},{\"end\":92066,\"start\":91997},{\"end\":92544,\"start\":92485},{\"end\":92970,\"start\":92934},{\"end\":93424,\"start\":93355},{\"end\":93969,\"start\":93907},{\"end\":94386,\"start\":94275}]"}}}, "year": 2023, "month": 12, "day": 17}