{"id": 688013, "updated": "2023-11-24 18:22:32.963", "metadata": {"title": "AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions", "authors": "[{\"first\":\"Chunhui\",\"last\":\"Gu\",\"middle\":[]},{\"first\":\"Chen\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Ross\",\"middle\":[\"A.\"]},{\"first\":\"Carl\",\"last\":\"Vondrick\",\"middle\":[]},{\"first\":\"Caroline\",\"last\":\"Pantofaru\",\"middle\":[]},{\"first\":\"Yeqing\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Sudheendra\",\"last\":\"Vijayanarasimhan\",\"middle\":[]},{\"first\":\"George\",\"last\":\"Toderici\",\"middle\":[]},{\"first\":\"Susanna\",\"last\":\"Ricco\",\"middle\":[]},{\"first\":\"Rahul\",\"last\":\"Sukthankar\",\"middle\":[]},{\"first\":\"Cordelia\",\"last\":\"Schmid\",\"middle\":[]},{\"first\":\"Jitendra\",\"last\":\"Malik\",\"middle\":[]}]", "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "journal": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "This paper introduces a video dataset of spatio-temporally localized Atomic Visual Actions (AVA). The AVA dataset densely annotates 80 atomic visual actions in 437 15-minute video clips, where actions are localized in space and time, resulting in 1.59M action labels with multiple labels per person occurring frequently. The key characteristics of our dataset are: (1) the definition of atomic visual actions, rather than composite actions; (2) precise spatio-temporal annotations with possibly multiple annotations for each person; (3) exhaustive annotation of these atomic actions over 15-minute video clips; (4) people temporally linked across consecutive segments; and (5) using movies to gather a varied set of action representations. This departs from existing datasets for spatio-temporal action recognition, which typically provide sparse annotations for composite actions in short video clips. AVA, with its realistic scene and action complexity, exposes the intrinsic difficulty of action recognition. To benchmark this, we present a novel approach for action localization that builds upon the current state-of-the-art methods, and demonstrates better performance on JHMDB and UCF101-24 categories. While setting a new state of the art on existing datasets, the overall results on AVA are low at 15.8% mAP, underscoring the need for developing new approaches for video understanding.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2949827582", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/GuSRVPLVTRSSM18", "doi": "10.1109/cvpr.2018.00633"}}, "content": {"source": {"pdf_hash": "67b3799bd1edcc86eb63670656129ee00bd191cb", "pdf_src": "ScienceParsePlus", "pdf_uri": "[\"https://arxiv.org/pdf/1705.08421v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://hal.inria.fr/hal-01764300/file/AVA_CVPR2018_Camera_Ready.pdf", "status": "GREEN"}}, "grobid": {"id": "a410bc2e1157b243fea5eeb72d771f8ecdd39c29", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/67b3799bd1edcc86eb63670656129ee00bd191cb.txt", "contents": "\nAVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions\n\n\nChunhui Gu \nChen Sun \nDavid A Ross \nCarl Vondrick \nCaroline Pantofaru \nYeqing Li \nSudheendra Vijayanarasimhan \nGeorge Toderici \nSusanna Ricco \nRahul Sukthankar \nCordelia Schmid \nJitendra Malik \nAVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions\n\nThis paper introduces a video dataset of spatiotemporally localized Atomic Visual Actions (AVA). The AVA dataset densely annotates 80 atomic visual actions in 437 15-minute video clips, where actions are localized in space and time, resulting in 1.59M action labels with multiple labels per person occurring frequently. The key characteristics of our dataset are: (1) the definition of atomic visual actions, rather than composite actions;(2) precise spatio-temporal annotations with possibly multiple annotations for each person; (3) exhaustive annotation of these atomic actions over 15-minute video clips; (4) people temporally linked across consecutive segments; and (5) using movies to gather a varied set of action representations. This departs from existing datasets for spatio-temporal action recognition, which typically provide sparse annotations for composite actions in short video clips.AVA, with its realistic scene and action complexity, exposes the intrinsic difficulty of action recognition. To benchmark this, we present a novel approach for action localization that builds upon the current state-of-the-art methods, and demonstrates better performance on JHMDB and UCF101-24 categories. While setting a new state of the art on existing datasets, the overall results on AVA are low at 15.8% mAP, underscoring the need for developing new approaches for video understanding.\n\nIntroduction\n\nWe introduce a new annotated video dataset, AVA, to advance action recognition research (see Fig. 1). The annotation is person-centric at a sampling frequency of 1 Hz. Every person is localized using a bounding box and the attached labels correspond to (possibly multiple) actions being performed by the actor: one action corresponding to the actor's pose (orange text) -standing, sitting, walking, swimming etc. -and there may be additional actions corresponding to interactions with objects (red text) or inter- actions with other persons (blue text). Each person in a frame containing multiple actors is labeled separately.\n\nTo label the actions performed by a person, a key choice is the annotation vocabulary, which in turn is determined by the temporal granularity at which actions are classified. We use short segments (\u00b11.5 seconds centered on a keyframe) to provide temporal context for labeling the actions in the middle frame. This enables the annotator to use movement cues for disambiguating actions such as pick up or put down that cannot be resolved in a static frame. We keep the temporal context relatively brief because we are interested in (temporally) fine-scale annotation of physical actions, which motivates \"Atomic Visual Actions\" (AVA). The vocabulary consists of 80 different atomic visual actions. Our dataset is sourced from the 15th to 30th minute time intervals of 437 different movies, which given the 1 Hz sampling frequency gives us 900 keyframes for each movie. In each keyframe, every person is labeled with (possibly multiple) actions from the AVA vocabulary. Each person is linked to the consecutive keyframes to provide short temporal sequences of action labels (Section 4.3). We now motivate the Figure 2. This figure illustrates the hierarchical nature of an activity. From Barker and Wright [3], pg. 247. main design choices of AVA. Atomic action categories. Barker & Wright [3] noted the hierarchical nature of activity (Fig. 2) in their classic study of the \"behavior episodes\" in the daily lives of the residents of a small town in Kansas. At the finest level, the actions consist of atomic body movements or object manipulation but at coarser levels, the most natural descriptions are in terms of intentionality and goal-directed behavior.\n\nThis hierarchy makes defining a vocabulary of action labels ill posed, contributing to the slower progress of our field compared to object recognition; exhaustively listing high-level behavioral episodes is impractical. However if we limit ourselves to fine time scales, then the actions are very physical in nature and have clear visual signatures. Here, we annotate keyframes at 1 Hz as this is sufficiently dense to capture the complete semantic content of actions while enabling us to avoid requiring unrealistically precise temporal annotation of action boundaries. The THUMOS challenge [18] observed that action boundaries (unlike objects) are inherently fuzzy, leading to significant inter-annotator disagreement. By contrast, annotators can easily determine (using \u00b11.5s of context) whether a frame contains a given action. Effectively, AVA localizes action start and end points to an acceptable precision of \u00b10.5 s. Person-centric action time series. While events such as trees falling do not involve people, our focus is on the activities of people, treated as single agents. There could be multiple people as in sports or two people hugging, but each one is an agent with individual choices, so we treat each separately. The action labels assigned to a person over time is a rich source of data for temporal modeling (Section 4.3). Annotation of movies. Ideally we would want behavior \"in the wild\". We do not have that, but movies are a compelling approximation, particularly when we consider the diversity of genres and countries with flourishing film industries. We do expect some bias in this process. Stories have to be interesting and there is a grammar of the film language [2] that communicates through the juxtaposition of shots. That said, in each shot we can expect an unfolding sequence of human actions, somewhat representative of reality, as conveyed by competent actors. AVA complements the current datasets sourced from user generated video because we ex-pect movies to contain a greater range of activities as befits the telling of diverse stories. Exhaustive action labeling. We label all the actions of all the people in all the keyframes. This will naturally result in a Zipf's law type of imbalance across action categories. There will be many more examples of typical actions (standing or sitting) than memorable ones (dancing), but this is how it should be! Recognition models need to operate on realistic \"long tailed\" action distributions [15] rather than being scaffolded using artificially balanced datasets. Another consequence of our protocol is that since we do not retrieve examples of action categories by explicit querying of internet video resources, we avoid a certain kind of bias: opening a door is a common event that occurs frequently in movie clips; however a door opening action that has been tagged as such on YouTube is likely attention worthy in a way that makes it atypical.\n\nWe believe that AVA, with its realistic complexity, exposes the inherent difficulty of action recognition hidden by many popular datasets in the field. A video clip of a single person performing a visually salient action like swimming in typical background is easy to discriminate from, say, one of a person running. Compare with AVA where we encounter multiple actors, small in image size, performing actions which are only subtly different such as touching vs. holding an object. To verify this intuition, we do comparative bench-marking on JHMDB [20], UCF101-24 categories [32] and AVA. The approach we use for spatiotemporal action localization (see Section 5) builds upon multi-frame approaches [16,41], but classifies tubelets with I3D convolutions [6]. We obtain state-of-the-art performance on JHMDB [20] and UCF101-24 categories [32] (see Section 6) while the mAP on AVA is only 15.8%.\n\nThe AVA dataset has been released publicly at https: //research.google.com/ava/.\n\n\nRelated work\n\nAction recognition datasets. Most popular action classification datasets, such as KTH [35], Weizmann [4], Hollywood-2 [26], HMDB [24], UCF101 [39] consist of short clips, manually trimmed to capture a single action. These datasets are ideally suited for training fullysupervised, whole-clip, forced-choice video classifiers. Recently, datasets, such as TrecVid MED [29], Sports-1M [21], YouTube-8M [1], Something-something [12], SLAC [48], Moments in Time [28], and Kinetics [22] have focused on large-scale video classification, often with automatically generated -and hence potentially noisy -annotations. They serve a valuable purpose but address a different need than AVA.\n\nSome recent work has moved towards temporal localization. ActivityNet [5], THUMOS [18], MultiTHUMOS [46] and Charades [37] use large numbers of untrimmed videos, each containing multiple actions, obtained either from YouTube (ActivityNet, THUMOS, MultiTHUMOS) or from crowdsourced actors (Charades). The datasets provide temporal (but not spatial) localization for each action of interest. AVA differs from them, as we provide spatiotemporal annotations for each subject performing an action and annotations are dense over 15-minute clips.\n\nA few datasets, such as CMU [23], MSR Actions [47], UCF Sports [32] and JHMDB [20] provide spatio-temporal annotations in each frame for short videos. The main differences with our AVA dataset are: the small number of actions; the small number of video clips; and the fact that clips are very short. Furthermore, actions are composite (e.g., pole-vaulting) and not atomic as in AVA. Recent extensions, such as UCF101 [39], DALY [44] and Hol-lywood2Tubes [27] evaluate spatio-temporal localization in untrimmed videos, which makes the task significantly harder and results in a performance drop. However, the action vocabulary is still restricted to a limited number of composite actions. Moreover, they do not densely cover the actions; a good example is BasketballDunk in UCF101, where only the dunking player is annotated. However, realworld applications often require a continuous annotations of atomic actions of all humans, which can then be composed into higher-level events. This motivates AVA's exhaustive labeling over 15-minute clips.\n\nAVA is also related to still image action recognition datasets [7,9,13] that are limited in two ways. First, the lack of motion can make action disambiguation difficult. Second, modeling composite events as a sequence of atomic actions is not possible in still images. This is arguably out of scope here, but clearly required in many real-world applications, for which AVA does provide training data. Methods for spatio-temporal action localization. Most recent approaches [11,30,34,43] rely on object detectors trained to discriminate action classes at the frame level with a two-stream variant, processing RGB and flow data separately. The resulting per-frame detections are then linked using dynamic programming [11,38] or tracking [43]. All these approaches rely on integrating frame-level detections. Very recently, multi-frame approaches have emerged: Tubelets [41] jointly estimate localization and classification over several frames, T-CNN [16] use 3D convolutions to estimate short tubes, micro-tubes rely on two successive frames [33] and pose-guided 3D convolutions add pose to a two-stream approach [49]. We build upon the idea of spatio-temporal tubes, but employ state-of-the-art I3D convolution [6] and Faster R-CNN [31] region proposals to outperform the state of the art.\n\n\nData collection\n\nAnnotation of the AVA dataset consists of five stages: action vocabulary generation, movie and segment selection, \n\n\nAction vocabulary generation\n\nWe follow three principles to generate our action vocabulary. The first one is generality. We collect generic actions in daily-life scenes, as opposed to specific activities in specific environments (e.g., playing basketball on a basketball court). The second one is atomicity. Our action classes have clear visual signatures, and are typically independent of interacted objects (e.g., hold without specifying what object to hold). This keeps our list short yet complete. The last one is exhaustivity. We initialized our list using knowledge from previous datasets, and iterated the list in several rounds until it covered \u223c99% of actions in the AVA dataset labeled by annotators. We end up with 14 pose classes, 49 personobject interaction classes and 17 person-person interaction classes in the vocabulary.\n\n\nMovie and segment selection\n\nThe raw video content of the AVA dataset comes from YouTube. We begin by assembling a list of top actors of many different nationalities. For each name we issue a YouTube search query, retrieving up to 2000 results. We only include videos with the \"film\" or \"television\" topic annotation, a duration of over 30 minutes, at least 1 year since upload, and at least 1000 views. We further exclude black & white, low resolution, animated, cartoon, and gaming videos, as well as those containing mature content.\n\nTo create a representative dataset within constraints, our selection criteria avoids filtering by action keywords, using automated action classifiers, or forcing a uniform label distribution. We aim to create an international collection of films by sampling from large film industries. However, the depiction of action in film is biased, e.g. by gender [10], and does not reflect the \"true\" distribution of human activity.\n\nEach movie contributes equally to the dataset, as we only label a sub-part ranging from the 15th to the 30th minute. We skip the beginning of the movie to avoid annotating titles or trailers. We choose a duration of 15 minutes so we are able to include more movies under a fixed annotation budget, and thus increase the diversity of our dataset. Each 15-min clip is then partitioned into 900 overlapping 3s movie segments with a stride of 1 second.\n\n\nPerson bounding box annotation\n\nWe localize a person and his or her actions with a bounding box. When multiple subjects are present in a keyframe, each subject is shown to the annotator separately for action annotation, and thus their action labels can be different.\n\nSince bounding box annotation is manually intensive, we choose a hybrid approach. First, we generate an initial set of bounding boxes using the Faster-RCNN person detector [31]. We set the operating point to ensure highprecision. Annotators then annotate the remaining bounding boxes missed by our detector. This hybrid approach ensures full bounding box recall which is essential for benchmarking, while minimizing the cost of manual annotation. This manual annotation retrieves only 5% more bounding boxes missed by our person detector, validating our design choice. Any incorrect bounding boxes are marked and removed by annotators in the next stage of action annotation.\n\n\nPerson link annotation\n\nWe link the bounding boxes over short periods of time to obtain ground-truth person tracklets. We calculate the pairwise similarity between bounding boxes in adjacent key frames using a person embedding [45] and solve for the optimal matching with the Hungarian algorithm [25]. While automatic matching is generally strong, we further remove false positives with human annotators who verify each match. This procedure results in 81,000 tracklets ranging from a few seconds to a few minutes.\n\n\nAction annotation\n\nThe action labels are generated by crowd-sourced annotators using the interface shown in Figure 3. The left panel shows both the middle frame of the target segment (top) and the segment as a looping embedded video (bottom). The bounding box overlaid on the middle frame specifies the person whose action needs to be labeled. On the right are text boxes for entering up to 7 action labels, including 1 pose action (required), 3 person-object interactions (optional), and 3 person-person interactions (optional). If none of the listed actions is descriptive, annotators can flag a check box called \"other action\". In addition, they could flag segments containing blocked or inappropriate content, or incorrect bounding boxes.\n\nIn practice, we observe that it is inevitable for annotators to miss correct actions when they are instructed to find all correct ones from a large vocabulary of 80 classes. Inspired by [36], we split the action annotation pipeline into two stages: action proposal and verification. We first ask multiple annotators to propose action candidates for each question, so the joint set possesses a higher recall than individual proposals. Annotators then verify these proposed candidates in the second stage. Results show significant recall improvement using this two-stage approach, especially on actions with fewer examples. See detailed analysis in the supplemental material. On average, annotators take 22 seconds to annotate a given video segment at the propose stage, and 19.7 seconds at the verify stage.\n\nEach video clip is annotated by three independent annotators and we only regard an action label as ground truth if it is verified by at least two annotators. Annotators are shown segments in randomized order. \n\n\nTraining, validation and test sets\n\n\nCharacteristics of the AVA dataset\n\nWe first build intuition on the diversity and difficulty of our AVA dataset through visual examples. Then, we characterize the annotations of our dataset quantitatively. Finally, we explore action and temporal structure.  Figure 4 shows examples of atomic actions as they change over consecutive segments. Besides variations in bounding box size and cinematography, many of the categories will require discriminating fine-grained differences, such as \"clinking glass\" versus \"drinking\" or leveraging temporal context, such as \"opening\" versus \"closing\". Figure 4 also shows two examples for the action \"open\". Even within an action class the appearance varies with vastly different contexts: the object being opened may even change. The wide intra-class variety will allow us to learn features that identify the critical spatio-temporal parts of an action -such as the breaking of a seal for \"opening\". Figure 5 shows the distribution of action annotations in AVA. The distribution roughly follows Zipf's law. Figure 6 illustrates bounding box size distribution. A large portion of people take up the full height of the frame. However, there are still many boxes with smaller sizes. The variability can be explained by both zoom level as well as pose. For example, boxes with the label \"enter\" show the typical pedestrian aspect ratio of 1:2 with average widths of 30% of the image width, and an average heights of 72%. On the other hand, boxes labeled \"lie/sleep\" are close to square, with average widths of 58% and heights of 67%. The box widths are widely distributed, showing the variety of poses people undertake to execute the labeled actions.\n\n\nDiversity and difficulty\n\n\nAnnotation Statistics\n\nThere are multiple labels for the majority of person bounding boxes. All bounding boxes have one pose label, 28% of bounding boxes have at least 1 person-object interaction label, and 67% of them have at least 1 person-person interaction label.\n\n\nTemporal Structure\n\nA key characteristic of AVA is the rich temporal structure that evolves from segment to segment. Since we have linked people between segments, we can discover common consecutive actions by looking at pairs of actions performed by the same person. We sort pairs by Normalized Pointwise Figure 6. Size and aspect ratio variations of annotated bounding boxes in the AVA dataset. Note that our bounding boxes consist of a large variation of sizes, many of which are small and hard to detect. Large variation also applies to the aspect ratios of bounding boxes, with mode at 2:1 ratio (e.g., sitting pose).\n\nMutual Information (NPMI) [8], which is commonly used in linguistics to represent the co-occurrence between two words: NPMI(x, y) = ln p(x,y) p(x)p(y) / (\u2212 ln p(x, y)). Values intuitively fall in the range (\u22121, 1], with \u22121 for pairs of words that never co-occur, 0 for independent pairs, and 1 for pairs that always co-occur. Table 1 shows pairs of actions with top NPMI in consecutive one-second segments for the same person. After removing identity transitions, some interesting common sense temporal patterns arise. Frequently, there are transitions from \"look at phone\" \u2192 \"answer phone\", \"fall down\" \u2192 \"lie\", or \"listen to\" \u2192 \"talk to\". We also analyze interperson action pairs. Table 2 shows top pairs of actions performed at the same time, but by different people. Several meaningful pairs emerge, such as \"ride\" \u2194 \"drive\", \"play music\" \u2194 \"listen\", or \"take\" \u2194 \"give/serve\". The transitions between atomic actions, despite the relatively coarse temporal sampling, provide excellent data for building more complex models of actions and activities with longer temporal structure.\n\n\nAction Localization Model\n\nPerformance numbers on popular action recognition datasets such as UCF101 or JHMDB have gone up considerably in recent years, but we believe that this may present an artificially rosy picture of the state of the art. When the video clip involves only a single person performing something visually characteristic like swimming in an equally characteristic background scene, it is easy to classify ac- curately. Difficulties come in when actors are multiple, or small in image size, or performing actions which are only subtly different, and when the background scenes are not enough to tell us what is going on. AVA has these aspects galore, and we will find that performance at AVA is much poorer as a result. Indeed this finding was foreshadowed by the poor performance at the Charades dataset [37].\n\nTo prove our point, we develop a state of the art action localization approach inspired by recent approaches for spatio-temporal action localization that operate on multiframe temporal information [16,41]. Here, we rely on the impact of larger temporal context based on I3D [6] for action detection. See Fig. 7 for an overview of our approach.\n\nFollowing Peng and Schmid [30], we apply the Faster RCNN algorithm [31] for end-to-end localization and classification of actions. However, in their approach, the temporal information is lost at the first layer where input channels from multiple frames are concatenated over time. We propose to use the Inception 3D (I3D) architecture by Carreira and Zisserman [6] to model temporal context. The I3D architecture is designed based on the Inception architecture [40], but replaces 2D convolutions with 3D convolutions. Temporal information is kept throughout the network. I3D achieves state-of-the-art performance on a wide range of video classification benchmarks.\n\nTo use I3D with Faster RCNN, we make the following changes to the model: first, we feed input frames of length T to the I3D model, and extract 3D feature maps of  size T \u00d7 W \u00d7 H \u00d7 C at the Mixed 4e layer of the network. The output feature map at Mixed 4e has a stride of 16, which is equivalent to the conv4 block of ResNet [14]. Second, for action proposal generation, we use a 2D ResNet-50 model on the keyframe as the input for the region proposal network, avoiding the impact of I3D with different input lengths on the quality of generated action proposals. Finally, we extend ROI Pooling to 3D by applying the 2D ROI Pooling at the same spatial location over all time steps. To understand the impact of optical flow for action detection, we fuse the RGB stream and the optical flow stream at the feature map level using average pooling. Baseline. To compare to a frame-based two-stream approach on AVA, we implement a variant of [30]. We use Faster RCNN [31] with ResNet-50 [14] to jointly learn action proposals and action labels. Region proposals are obtained with the RGB stream only. The region classifier takes as input RGB along with optical flow features stacked over 5 consecutive frames. As for our I3D approach, we jointly train the RGB and the optical flow streams by fusing the conv4 feature maps with average pooling. Implementation details. We implement FlowNet v2 [19] to extract optical flow features. We train Faster-RCNN with asynchronous SGD. For all training tasks, we use a validation set to determine the number of training steps, which ranges from 600K to 1M iterations. We fix the input resolution to be 320 by 400 pixels. All the other model parameters are set based on the recommended values from [17], which were tuned for object detection. The ResNet-50 networks are initialized with ImageNet pre-trained models. For the optical flow stream, we duplicate the conv1 filters to input 5 frames. The I3D networks are initialized with Kinetics [22] pre-trained models, for both the RGB and optical flow streams. Note that although I3D were pretrained on 64-frame inputs, the network is fully convolutional over time and can take any number of frames as input. All feature layers are jointly updated during training.\n\nThe output frame-level detections are post-processed with non-maximum suppression with threshold 0.6.\n\nOne key difference between AVA and existing action detection datasets is that the action labels of AVA are not mu-tually exclusive. To address this, we replace the standard softmax loss function by a sum of binary Sigmoid losses, one for each class. We use Sigmoid loss for AVA and softmax loss for all other datasets. Linking. Once we have per frame-level detections, we link them to construct action tubes. We report video-level performance based on average scores over the obtained tubes. We use the same linking algorithm as described in [38], except that we do not apply temporal labeling. Since AVA is annotated at 1 Hz and each tube may have multiple labels, we modify the video-level evaluation protocol to estimate an upper bound. We use ground truth links to infer detection links, and when computing IoU score of a class between a ground truth tube and a detection tube, we only take tube segments that are labeled by that class into account.\n\n\nExperiments and Analysis\n\nWe now experimentally analyze key characteristics of AVA and motivate challenges for action understanding.\n\n\nDatasets and Metrics\n\nAVA benchmark. Since the label distribution in AVA roughly follows Zipf's law ( Figure 5) and evaluation on a very small number of examples could be unreliable, we use classes that have at least 25 instances in validation and test splits to benchmark performance. Our resulting benchmark consists of a total of 214,622 training, 57,472 validation and 120,332 test examples on 60 classes. Unless otherwise mentioned, we report results trained on the training set and evaluated on the validation set. We randomly select 10% of the training data for model parameter tuning. Datasets. Besides AVA, we also analyze standard video datasets in order to compare difficulty. JHMDB [20] consists of 928 trimmed clips over 21 classes. We report results for split one in our ablation study, but results are averaged over three splits for comparison to the state of the art. For UCF101, we use spatio-temporal annotations for a 24-class subset with 3207 videos, provided by Singh et al. [38]. We conduct experiments on the official split1 as is standard. Metrics. For evaluation, we follow standard practice when possible. We report intersection-over-union (IoU) performance on frame level and video level. For frame-level IoU, we follow the standard protocol used by the PASCAL VOC challenge [9] and report the average precision (AP) using an IoU threshold of 0.5. For each class, we compute the average precision and report the average over all classes. For video-level IoU, we compute 3D IoUs between ground truth tubes and linked detection tubes at the threshold of 0.5. The mean AP is computed by averaging over all classes. Table 3 shows our model performance on two standard video datasets. Our 3D two-stream model obtains state-Frame-mAP JHMDB UCF101-24 Actionness [42] 39.9% -Peng w/o MR [30] 56.9% 64.8% Peng w/ MR [30] 58.5% 65.7% ACT [41] 65.7% 69.5% Our approach 73.3% 76.3%\n\n\nComparison to the state-of-the-art\n\nVideo-mAP JHMDB UCF101-24 Peng w/ MR [30] 73.1% 35.9% Singh et al. [38] 72.0% 46.3% ACT [41] 73.7% 51.4% TCNN [16] 76.9% -Our approach 78.6% 59.9% Table 3. Frame-mAP (top) and video-mAP (bottom) @ IoU 0.5 for JHMDB and UCF101-24. For JHMDB, we report averaged performance over three splits. Our approach outperforms previous state-of-the-art on both metrics by a considerable margin.\n\nof-the-art performance on UCF101 and JHMDB, outperforming well-established baselines for both frame-mAP and video-mAP metrics. However, the picture is less auspicious when recognizing atomic actions. Table 4 shows that the same model obtains relatively low performance on AVA validation set (frame-mAP of 15.8%, video-mAP of 12.3% at 0.5 IoU and 17.9% at 0.2 IoU), as well as test set (frame-mAP of 14.7%). We attribute this to the design principles behind AVA: we collected a vocabulary where context and object cues are not as discriminative for action recognition. Instead, recognizing fine-grained details and rich temporal models may be needed to succeed at AVA, posing a new challenge for visual action recognition. In the remainder of this paper, we analyze what makes AVA challenging and discuss how to move forward.\n\n\nAblation study\n\nHow important is temporal information for recognizing AVA categories? Table 4 shows the impact of the temporal length and the type of model. All 3D models outperform the 2D baseline on JHMDB and UCF101-24. For AVA, 3D models perform better after using more than 10 frames. We can also see that increasing the length of the temporal window helps for the 3D two-stream models across all datasets. As expected, combining RGB and optical flow features improves the performance over a single input modality. Moreover, AVA benefits more from larger temporal context than JHMDB and UCF101, whose performances saturate at 20 frames. This gain and the consecutive actions in Table 1 suggests that one may obtain further gains by leveraging the rich temporal context in AVA. How challenging is localization versus recognition? Table 5 compares the performance of end-to-end action localization and recognition versus class agnostic action localization. We can see that although action localization is more   Table 5. Frame-mAP @ IoU 0.5 for action detection and actor detection performance on JHMDB (split1), UCF101-24 (split1) and AVA benchmarks. Since human annotators are consistent, our results suggest there is significant headroom to improve on recongizing atomic visual actions.\n\nchallenging on AVA than on JHMDB, the gap between localization and end-to-end detection performance is nearly 60% on AVA, while less than 15% on JHMDB and UCF101. This suggests that the main difficulty of AVA lies in action classification rather than localization. Figure 9 shows examples of high-scoring false alarms, suggesting that the difficulty in recognition lies in the fine-grained details. Which categories are challenging? How important is number of training examples? Figure 8 breaks down performance by categories and the number of training examples. While more data generally yields better performance, the outliers reveals that not all categories are of equal complexity. Categories correlated with scenes and objects (such as swimming) or categories with low diversity (such as fall down) obtain high performance despite having fewer training examples. In contrast, categories with lots of data, Figure 9. Red boxes show high-scoring false alarms for smoking. The model often struggles to discriminate fine-grained details.\n\nsuch as touching and smoking, obtain relatively low performance possibly because they have large visual variations or require fine grained discrimination, motivating work on person-object interaction [7,12]. We hypothesize that the gains on recognizing atomic actions will need not only large datasets, such as AVA, but also rich models of motion and interactions.\n\n\nConclusion\n\nThis paper introduces the AVA dataset with spatiotemporal annotations of atomic actions at 1 Hz over diverse 15-min. movie segments. In addition we propose a method that outperforms the current state of the art on standard benchmarks to serve as a baseline. This method highlights the difficulty of the AVA dataset as its performance is significantly lower than on UCF101 or JHMDB, underscoring the need for developing new action recognition approaches.\n\nFuture work includes modeling more complex activities based on our atomic actions. Our present day visual classification technology may enable us to classify events such as \"eating in a restaurant\" at the coarse scene/video level, but models based on AVA's fine spatio-temporal granularity facilitate understanding at the level of an individual agents actions. These are essential steps towards imbuing computers with \"social visual intelligence\" -understanding what humans are doing, what they might do next, and what they are trying to achieve.\n\nFigure 1 .\n1The bounding box and action annotations in sample frames of the AVA dataset. Each bounding box is associated with 1 pose action (in orange), 0-3 interactions with objects (in red), and 0-3 interactions with other people (in blue). Note that some of these actions require temporal context to accurately label.\n\nFigure 3 .\n3User interface for action annotation. Details in Sec 3.5. person bounding box annotation, person linking and action annotation.\n\nFigure 4 .\n4We show examples of how atomic actions change over time in AVA. The text shows pairs of atomic actions for the people in red bounding boxes. Temporal information is key for recognizing many of the actions and appearance can substantially vary within an action category, such as opening a door or bottle.\n\nFigure 5 .\n5Sizes of each action class in the AVA train/val dataset sorted by descending order, with colors indicating action types.\n\nFigure 7 .\n7Illustration of our approach for spatio-temporal action localization. Region proposals are detected and regressed with Faster-RCNN on RGB keyframes. Spatio-temporal tubes are classified with two-stream I3D convolutions.\n\nFigure 8 .\n8Top: We plot the performance of models for each action class, sorting by the number of training examples. Bottom: We plot the number of training examples per class.While more data is better, the outliers suggest that not all classes are of equal complexity. For example, one of the smallest classes \"swim\" has one of the highest performances because the associated scenes make it relatively easy.\n\n\nOur training/validation/test sets are split at the video level, so that all segments of one video appear only in one split. The 437 videos are split into 239 training, 64 validation and 134 test videos, roughly a 55:15:30 split, resulting in 215k training, 57k validation and 120k test segments.\n\n\nWe show top pairs of consecutive actions that are likely to happen before/after for the same person. We sort by NPMI. We show top pairs of simultaneous actions by different people. We sort by NPMI.First Action \n\nSecond Action \nNPMI \nride (eg bike/car/horse) \ndrive (eg car/truck) \n0.68 \nwatch (eg TV) \nwork on a computer \n0.64 \ndrive (eg car/truck) \nride (eg car bike/car/horse) \n0.63 \nopen (eg window/door) \nclose (eg door/box) \n0.59 \ntext on/look at a cellphone answer phone \n0.53 \nlisten to (person) \ntalk to (person) \n0.47 \nfall down \nlie/sleep \n0.46 \ntalk to (person) \nlisten to (person) \n0.43 \nstand \nsit \n0.40 \nwalk \nstand \n0.40 \nTable 1. Person 1 Action \nPerson 2 Action \nNPMI \nride (eg bike/car/horse) \ndrive (eg car/truck) \n0.60 \nplay musical instrument listen (eg music) \n0.57 \ntake (object) \ngive/serve (object) \n0.51 \ntalk to (person) \nlisten to (person) \n0.46 \nstand \nsit \n0.31 \nplay musical instrument dance \n0.23 \nwalk \nstand \n0.21 \nwatch (person) \nwrite \n0.15 \nwalk \nrun/jog \n0.15 \nfight/hit (a person) \nstand \n0.14 \nTable 2. \n\n\nTable 4. Frame-mAP @ IoU 0.5 for action detection on JHMDB (split1), UCF101 (split1) and AVA. Note that JHMDB has up to 40 frames per clip. For UCF101-24, we randomly sample 20,000 frame subset for evaluation. Although our model obtains state-ofthe-art performance on JHMDB and UCF101-24, the fine-grained nature of AVA makes it a challenge.Model \nTemp.+ Mode \nJHMDB \nUCF101-24 \nAVA \n2D \n1 RGB + 5 Flow \n52.1% \n60.1% \n14.2% \n3D \n5 RGB + 5 Flow \n67.9% \n76.1% \n13.6% \n3D \n10 RGB + 10 Flow \n73.4% \n78.0% \n14.2% \n3D \n20 RGB + 20 Flow \n76.4% \n78.3% \n14.8% \n3D \n40 RGB + 40 Flow \n76.7% \n76.0% \n15.8% \n3D \n50 RGB + 50 Flow \n-\n73.2% \n15.7% \n3D \n20 RGB \n73.2% \n77.0% \n14.6% \n3D \n20 Flow \n67.0% \n71.3% \n10.1% \n\nJHMDB UCF101-24 \nAVA \nAction detection \n76.7% \n76.3% \n15.8% \nActor detection \n92.8% \n84.8% \n75.3% \n\n\nAcknowledgement We thank Abhinav Gupta, Abhinav Shrivastava, Andrew Gallagher, Irfan Essa, and Vicky Kalogeiton for discussion and comments about this work.\nS Abu-El-Haija, N Kothari, J Lee, P Natsev, G Toderici, B Varadarajan, S Vijayanarasimhan, arXiv:1609.08675YouTube-8M: A large-scale video classification benchmark. S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici, B. Varadarajan, and S. Vijayanarasimhan. YouTube- 8M: A large-scale video classification benchmark. arXiv:1609.08675, 2016. 2\n\nGrammar of the film language. D Arijon, Silman-James PressD. Arijon. Grammar of the film language. Silman-James Press, 1991. 2\n\nMidwest and its children: The psychological ecology of an American town. Row, Peterson and Company. R Barker, H Wright, R. Barker and H. Wright. Midwest and its children: The psychological ecology of an American town. Row, Peterson and Company, 1954. 2\n\nActions as space-time shapes. M Blank, L Gorelick, E Shechtman, M Irani, R Basri, ICCV. M. Blank, L. Gorelick, E. Shechtman, M. Irani, and R. Basri. Actions as space-time shapes. In ICCV, 2005. 2\n\nActivityNet: A large-scale video benchmark for human activity understanding. F Caba Heilbron, V Escorcia, B Ghanem, J C Niebles, CVPR. F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. C. Niebles. ActivityNet: A large-scale video benchmark for human ac- tivity understanding. In CVPR, 2015. 2\n\nQuo vadis, action recognition? A new model and the Kinetics dataset. J Carreira, A Zisserman, CVPR. 6J. Carreira and A. Zisserman. Quo vadis, action recognition? A new model and the Kinetics dataset. In CVPR, 2017. 2, 3, 6\n\nHICO: A benchmark for recognizing human-object interactions in images. Y.-W Chao, Z Wang, Y He, J Wang, J Deng, ICCV. 3Y.-W. Chao, Z. Wang, Y. He, J. Wang, and J. Deng. HICO: A benchmark for recognizing human-object interactions in images. In ICCV, 2015. 3, 8\n\nWord association norms, mutual information, and lexicoraphy. K.-W Church, P Hanks, Computational Linguistics. 161K.-W. Church and P. Hanks. Word association norms, mu- tual information, and lexicoraphy. Computational Linguis- tics, 16(1), 1990. 5\n\nThe PASCAL Visual Object Classes Challenge: A retrospective. M Everingham, S M A Eslami, L Van Gool, C K I Williams, J Winn, A Zisserman, 37M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge: A retrospective. IJCV, 2015. 3, 7\n\nGeena Davis Institute on Gender in Media. The Reel Truth: Women Aren't Seen or Heard. Geena Davis Institute on Gender in Media. The Reel Truth: Women Aren't Seen or Heard. https://seejane. org/research-informs-empowers/data/, 2016. 3\n\nFinding action tubes. G Gkioxari, J Malik, CVPR. G. Gkioxari and J. Malik. Finding action tubes. In CVPR, 2015. 3\n\nThe \"something something\" video database for learning and evaluating visual common sense. R Goyal, S E Kahou, V Michalski, J Materzynska, S Westphal, H Kim, V Haenel, I Fr\u00fcnd, P Yianilos, M Mueller-Freitag, F Hoppe, C Thurau, I Bax, R Memisevic, 2R. Goyal, S. E. Kahou, V. Michalski, J. Materzynska, S. Westphal, H. Kim, V. Haenel, I. Fr\u00fcnd, P. Yianilos, M. Mueller-Freitag, F. Hoppe, C. Thurau, I. Bax, and R. Memisevic. The \"something something\" video database for learning and evaluating visual common sense. In ICCV, 2017. 2, 8\n\nVisual semantic role labeling. S Gupta, J Malik, abs/1505.04474CoRRS. Gupta and J. Malik. Visual semantic role labeling. CoRR, abs/1505.04474, 2015. 3\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016. 6\n\nThe devil is in the tails: Finegrained classification in the wild. G V Horn, P Perona, arXiv:1709.01450G. V. Horn and P. Perona. The devil is in the tails: Fine- grained classification in the wild. arXiv:1709.01450, 2017. 2\n\nTube convolutional neural network (T-CNN) for action detection in videos. R Hou, C Chen, M Shah, ICCV. 67R. Hou, C. Chen, and M. Shah. Tube convolutional neural network (T-CNN) for action detection in videos. In ICCV, 2017. 2, 3, 6, 7\n\nSpeed/accuracy trade-offs for modern convolutional object detectors. J Huang, V Rathod, C Sun, M Zhu, A Korattikara, A Fathi, I Fischer, Z Wojna, Y Song, S Guadarrama, K Murphy, CVPR. J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, and K. Murphy. Speed/accuracy trade-offs for modern convolu- tional object detectors. In CVPR, 2017. 6\n\nThe THUMOS challenge on action recognition for videos. H Idrees, A R Zamir, Y Jiang, A Gorban, I Laptev, R Sukthankar, M Shah, CVIU. 2H. Idrees, A. R. Zamir, Y. Jiang, A. Gorban, I. Laptev, R. Sukthankar, and M. Shah. The THUMOS challenge on action recognition for videos \"in the wild\". CVIU, 2017. 2\n\nFlowNet 2.0: Evolution of optical flow estimation with deep networks. E Ilg, N Mayer, T Saikia, M Keuper, A Dosovitskiy, T Brox, CVPR. E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox. FlowNet 2.0: Evolution of optical flow estimation with deep networks. In CVPR, 2017. 6\n\nTowards understanding action recognition. H Jhuang, J Gall, S Zuffi, C Schmid, M Black, ICCV. 37H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. Black. To- wards understanding action recognition. In ICCV, 2013. 2, 3, 7\n\nLarge-scale video classification with convolutional neural networks. A Karpathy, G Toderici, S Shetty, T Leung, R Sukthankar, L Fei-Fei, CVPR. A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei. Large-scale video classification with convo- lutional neural networks. In CVPR, 2014. 2\n\nW Kay, J Carreira, K Simonyan, B Zhang, C Hillier, S Vijayanarasimhan, F Viola, T Green, T Back, P Natsev, M Suleyman, A Zisserman, arXiv:1705.06950The Kinetics human action video dataset. 26W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev, M. Suleyman, and A. Zisserman. The Kinetics human action video dataset. arXiv:1705.06950, 2017. 2, 6\n\nEfficient visual event detection using volumetric features. Y Ke, R Sukthankar, M Hebert, ICCV. Y. Ke, R. Sukthankar, and M. Hebert. Efficient visual event detection using volumetric features. In ICCV, 2005. 3\n\nHMDB: A large video database for human motion recognition. H Kuehne, H Jhuang, E Garrote, T Poggio, T Serre, ICCV. H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. HMDB: A large video database for human motion recogni- tion. In ICCV, 2011. 2\n\nThe Hungarian method for the assignment problem. H W Kuhn, Naval Research Logistics (NRL). 2H. W. Kuhn. The Hungarian method for the assignment prob- lem. Naval Research Logistics (NRL), 2(1-2):83-97, 1955. 4\n\nActions in context. M Marszalek, I Laptev, C Schmid, CVPR. M. Marszalek, I. Laptev, and C. Schmid. Actions in context. In CVPR, 2009. 2\n\nSpot On: Action localization from pointly-supervised proposals. P Mettes, J Van Gemert, C Snoek, ECCV. P. Mettes, J. van Gemert, and C. Snoek. Spot On: Action localization from pointly-supervised proposals. In ECCV, 2016. 3\n\nMoments in time dataset: one million videos for event understanding. M Monfort, B Zhou, S A Bargal, T Yan, A Andonian, K Ramakrishnan, L Brown, Q Fan, D Gutfruend, C Vondrick, M. Monfort, B. Zhou, S. A. Bargal, T. Yan, A. Andonian, K. Ramakrishnan, L. Brown, Q. Fan, D. Gutfruend, C. Von- drick, et al. Moments in time dataset: one million videos for event understanding. 2\n\nTRECVID 2014 -an overview of the goals, tasks, data, evaluation mechanisms and metrics. P Over, G Awad, M Michel, J Fiscus, G Sanders, W Kraaij, A Smeaton, G Qu\u00e9not, P. Over, G. Awad, M. Michel, J. Fiscus, G. Sanders, W. Kraaij, A. Smeaton, and G. Qu\u00e9not. TRECVID 2014 - an overview of the goals, tasks, data, evaluation mechanisms and metrics, 2014. 2\n\nMulti-region two-stream R-CNN for action detection. X Peng, C Schmid, ECCV. 67X. Peng and C. Schmid. Multi-region two-stream R-CNN for action detection. In ECCV, 2016. 3, 6, 7\n\nFaster R-CNN: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, NIPS. 36S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To- wards real-time object detection with region proposal net- works. In NIPS, 2015. 3, 4, 6\n\nAction MACH: a spatio-temporal maximum average correlation height filter for action recognition. M Rodriguez, J Ahmed, M Shah, CVPR. 23M. Rodriguez, J. Ahmed, and M. Shah. Action MACH: a spatio-temporal maximum average correlation height filter for action recognition. In CVPR, 2008. 2, 3\n\nAMTnet: Action-microtube regression by end-to-end trainable deep architecture. S Saha, G Sing, F Cuzzolin, ICCV. S. Saha, G.Sing, and F. Cuzzolin. AMTnet: Action-micro- tube regression by end-to-end trainable deep architecture. In ICCV, 2017. 3\n\nDeep learning for detecting multiple space-time action tubes in videos. S Saha, G Singh, M Sapienza, P Torr, F Cuzzolin, BMVC. S. Saha, G. Singh, M. Sapienza, P. Torr, and F. Cuzzolin. Deep learning for detecting multiple space-time action tubes in videos. In BMVC, 2016. 3\n\nRecognizing human actions: a local SVM approach. C Schuldt, I Laptev, B Caputo, ICPR. C. Schuldt, I. Laptev, and B. Caputo. Recognizing human actions: a local SVM approach. In ICPR, 2004. 2\n\nMuch ado about time: Exhaustive annotation of temporal data. G Sigurdsson, O Russakovsky, A Farhadi, I Laptev, A Gupta, Conference on Human Computation and Crowdsourcing. G. Sigurdsson, O. Russakovsky, A. Farhadi, I. Laptev, and A. Gupta. Much ado about time: Exhaustive annotation of temporal data. In Conference on Human Computation and Crowdsourcing, 2016. 4\n\nHollywood in homes: Crowdsourcing data collection for activity understanding. G Sigurdsson, G Varol, X Wang, A Farhadi, I Laptev, A Gupta, ECCV. 26G. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev, and A. Gupta. Hollywood in homes: Crowdsourcing data collec- tion for activity understanding. In ECCV, 2016. 2, 6\n\nOnline real-time multiple spatiotemporal action localisation and prediction. G Singh, S Saha, M Sapienza, P Torr, F Cuzzolin, ICCV. 37G. Singh, S. Saha, M. Sapienza, P. Torr, and F. Cuzzolin. Online real-time multiple spatiotemporal action localisation and prediction. In ICCV, 2017. 3, 7\n\nUCF101: A dataset of 101 human actions classes from videos in the wild. K Soomro, A Zamir, M Shah, CRCV-TR-12-0123University of Central FloridaTechnical ReportK. Soomro, A. Zamir, and M. Shah. UCF101: A dataset of 101 human actions classes from videos in the wild. Techni- cal Report CRCV-TR-12-01, University of Central Florida, 2012. 2, 3\n\nRethinking the inception architecture for computer vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, CVPR. C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016. 6\n\nAction tubelet detector for spatio-temporal action localization. V Kalogeiton, P Weinzaepfel, V Ferrari, C Schmid, ICCV. 67V.Kalogeiton, P. Weinzaepfel, V. Ferrari, and C. Schmid. Ac- tion tubelet detector for spatio-temporal action localization. In ICCV, 2017. 2, 3, 6, 7\n\nActionness estimation using hybrid fully convolutional networks. L Wang, Y Qiao, X Tang, L Van Gool, CVPR. L. Wang, Y. Qiao, X. Tang, and L. Van Gool. Actionness esti- mation using hybrid fully convolutional networks. In CVPR, 2016. 7\n\nLearning to track for spatio-temporal action localization. P Weinzaepfel, Z Harchaoui, C Schmid, ICCV. P. Weinzaepfel, Z. Harchaoui, and C. Schmid. Learning to track for spatio-temporal action localization. In ICCV, 2015. 3\n\nTowards weaklysupervised action localization. P Weinzaepfel, X Martin, C Schmid, arXiv:1605.05197P. Weinzaepfel, X. Martin, and C. Schmid. Towards weakly- supervised action localization. arXiv:1605.05197, 2016. 3\n\nL Wu, C Shen, A Van Den, Hengel, arXiv:1601.07255PersonNet: Person re-identification with deep convolutional neural networks. arXiv preprintL. Wu, C. Shen, and A. van den Hengel. PersonNet: Person re-identification with deep convolutional neural networks. arXiv preprint arXiv:1601.07255, 2016. 4\n\nEvery moment counts: Dense detailed labeling of actions in complex videos. S Yeung, O Russakovsky, N Jin, M Andriluka, G Mori, L Fei-Fei, IJCV. 2S. Yeung, O. Russakovsky, N. Jin, M. Andriluka, G. Mori, and L. Fei-Fei. Every moment counts: Dense detailed label- ing of actions in complex videos. IJCV, 2017. 2\n\nDiscriminative subvolume search for efficient action detection. J Yuan, Z Liu, Y Wu, CVPR. J. Yuan, Z. Liu, and Y. Wu. Discriminative subvolume search for efficient action detection. In CVPR, 2009. 3\n\nSLAC: A sparsely labeled dataset for action classification and localization. H Zhao, Z Yan, H Wang, L Torresani, A Torralba, arXiv:1712.09374arXiv preprintH. Zhao, Z. Yan, H. Wang, L. Torresani, and A. Torralba. SLAC: A sparsely labeled dataset for action classification and localization. arXiv preprint arXiv:1712.09374, 2017. 2\n\nChained multi-stream networks exploiting pose, motion, and appearance for action classification and detection. M Zolfaghari, G Oliveira, N Sedaghat, T Brox, ICCV. M. Zolfaghari, G. Oliveira, N. Sedaghat, and T. Brox. Chained multi-stream networks exploiting pose, motion, and appearance for action classification and detection. In ICCV, 2017. 3\n", "annotations": {"author": "[{\"end\":88,\"start\":77},{\"end\":98,\"start\":89},{\"end\":112,\"start\":99},{\"end\":127,\"start\":113},{\"end\":147,\"start\":128},{\"end\":158,\"start\":148},{\"end\":187,\"start\":159},{\"end\":204,\"start\":188},{\"end\":219,\"start\":205},{\"end\":237,\"start\":220},{\"end\":254,\"start\":238},{\"end\":270,\"start\":255}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":85},{\"end\":97,\"start\":94},{\"end\":111,\"start\":107},{\"end\":126,\"start\":118},{\"end\":146,\"start\":137},{\"end\":157,\"start\":155},{\"end\":186,\"start\":170},{\"end\":203,\"start\":195},{\"end\":218,\"start\":213},{\"end\":236,\"start\":226},{\"end\":253,\"start\":247},{\"end\":269,\"start\":264}]", "author_first_name": "[{\"end\":84,\"start\":77},{\"end\":93,\"start\":89},{\"end\":104,\"start\":99},{\"end\":106,\"start\":105},{\"end\":117,\"start\":113},{\"end\":136,\"start\":128},{\"end\":154,\"start\":148},{\"end\":169,\"start\":159},{\"end\":194,\"start\":188},{\"end\":212,\"start\":205},{\"end\":225,\"start\":220},{\"end\":246,\"start\":238},{\"end\":263,\"start\":255}]", "author_affiliation": null, "title": "[{\"end\":74,\"start\":1},{\"end\":344,\"start\":271}]", "venue": null, "abstract": "[{\"end\":1736,\"start\":346}]", "bib_ref": "[{\"end\":3495,\"start\":3487},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3587,\"start\":3584},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3671,\"start\":3668},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4634,\"start\":4630},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5733,\"start\":5730},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6517,\"start\":6513},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7523,\"start\":7519},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7550,\"start\":7546},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7674,\"start\":7670},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7677,\"start\":7674},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7728,\"start\":7725},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7782,\"start\":7778},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7812,\"start\":7808},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8053,\"start\":8049},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8067,\"start\":8064},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8085,\"start\":8081},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8096,\"start\":8092},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8109,\"start\":8105},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8332,\"start\":8328},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8348,\"start\":8344},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8364,\"start\":8361},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8390,\"start\":8386},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8401,\"start\":8397},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8423,\"start\":8419},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8442,\"start\":8438},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8714,\"start\":8711},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8727,\"start\":8723},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8745,\"start\":8741},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8763,\"start\":8759},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9214,\"start\":9210},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9232,\"start\":9228},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9249,\"start\":9245},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9264,\"start\":9260},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9603,\"start\":9599},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9614,\"start\":9610},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9640,\"start\":9636},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10294,\"start\":10291},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10296,\"start\":10294},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10299,\"start\":10296},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10705,\"start\":10701},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10708,\"start\":10705},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10711,\"start\":10708},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10714,\"start\":10711},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10947,\"start\":10943},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10950,\"start\":10947},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10967,\"start\":10963},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11099,\"start\":11095},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11180,\"start\":11176},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11272,\"start\":11268},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":11343,\"start\":11339},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11441,\"start\":11438},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11463,\"start\":11459},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13388,\"start\":13384},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14350,\"start\":14346},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":15082,\"start\":15078},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15151,\"start\":15147},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16302,\"start\":16298},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19806,\"start\":19803},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21689,\"start\":21685},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21893,\"start\":21889},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21896,\"start\":21893},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21969,\"start\":21966},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22067,\"start\":22063},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22108,\"start\":22104},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22401,\"start\":22398},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":22502,\"start\":22498},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23031,\"start\":23027},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23641,\"start\":23637},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23666,\"start\":23662},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23686,\"start\":23682},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24091,\"start\":24087},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24435,\"start\":24431},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24679,\"start\":24675},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25597,\"start\":25593},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":26840,\"start\":26836},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":27142,\"start\":27138},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27447,\"start\":27444},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":27928,\"start\":27924},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27952,\"start\":27948},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27980,\"start\":27976},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28001,\"start\":27997},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":28118,\"start\":28114},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":28148,\"start\":28144},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28169,\"start\":28165},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28191,\"start\":28187},{\"end\":31501,\"start\":31493},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31825,\"start\":31822},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31828,\"start\":31825}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33324,\"start\":33003},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33465,\"start\":33325},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33782,\"start\":33466},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33916,\"start\":33783},{\"attributes\":{\"id\":\"fig_4\"},\"end\":34149,\"start\":33917},{\"attributes\":{\"id\":\"fig_5\"},\"end\":34559,\"start\":34150},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":34857,\"start\":34560},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":35903,\"start\":34858},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":36707,\"start\":35904}]", "paragraph": "[{\"end\":2378,\"start\":1752},{\"end\":4036,\"start\":2380},{\"end\":6968,\"start\":4038},{\"end\":7864,\"start\":6970},{\"end\":7946,\"start\":7866},{\"end\":8639,\"start\":7963},{\"end\":9180,\"start\":8641},{\"end\":10226,\"start\":9182},{\"end\":11516,\"start\":10228},{\"end\":11650,\"start\":11536},{\"end\":12491,\"start\":11683},{\"end\":13029,\"start\":12523},{\"end\":13453,\"start\":13031},{\"end\":13903,\"start\":13455},{\"end\":14172,\"start\":13938},{\"end\":14848,\"start\":14174},{\"end\":15365,\"start\":14875},{\"end\":16110,\"start\":15387},{\"end\":16918,\"start\":16112},{\"end\":17129,\"start\":16920},{\"end\":18854,\"start\":17205},{\"end\":19151,\"start\":18907},{\"end\":19775,\"start\":19174},{\"end\":20860,\"start\":19777},{\"end\":21690,\"start\":20890},{\"end\":22035,\"start\":21692},{\"end\":22701,\"start\":22037},{\"end\":24946,\"start\":22703},{\"end\":25049,\"start\":24948},{\"end\":26004,\"start\":25051},{\"end\":26139,\"start\":26033},{\"end\":28038,\"start\":26164},{\"end\":28460,\"start\":28077},{\"end\":29286,\"start\":28462},{\"end\":30580,\"start\":29305},{\"end\":31620,\"start\":30582},{\"end\":31986,\"start\":31622},{\"end\":32454,\"start\":32001},{\"end\":33002,\"start\":32456}]", "formula": null, "table_ref": "[{\"end\":20110,\"start\":20103},{\"end\":20467,\"start\":20460},{\"end\":27788,\"start\":27781},{\"end\":28231,\"start\":28224},{\"end\":28669,\"start\":28662},{\"end\":29382,\"start\":29375},{\"end\":29978,\"start\":29971},{\"end\":30310,\"start\":30303}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1750,\"start\":1738},{\"attributes\":{\"n\":\"2.\"},\"end\":7961,\"start\":7949},{\"attributes\":{\"n\":\"3.\"},\"end\":11534,\"start\":11519},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11681,\"start\":11653},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12521,\"start\":12494},{\"attributes\":{\"n\":\"3.3.\"},\"end\":13936,\"start\":13906},{\"attributes\":{\"n\":\"3.4.\"},\"end\":14873,\"start\":14851},{\"attributes\":{\"n\":\"3.5.\"},\"end\":15385,\"start\":15368},{\"attributes\":{\"n\":\"3.6.\"},\"end\":17166,\"start\":17132},{\"attributes\":{\"n\":\"4.\"},\"end\":17203,\"start\":17169},{\"attributes\":{\"n\":\"4.1.\"},\"end\":18881,\"start\":18857},{\"attributes\":{\"n\":\"4.2.\"},\"end\":18905,\"start\":18884},{\"attributes\":{\"n\":\"4.3.\"},\"end\":19172,\"start\":19154},{\"attributes\":{\"n\":\"5.\"},\"end\":20888,\"start\":20863},{\"attributes\":{\"n\":\"6.\"},\"end\":26031,\"start\":26007},{\"attributes\":{\"n\":\"6.1.\"},\"end\":26162,\"start\":26142},{\"attributes\":{\"n\":\"6.2.\"},\"end\":28075,\"start\":28041},{\"attributes\":{\"n\":\"6.3.\"},\"end\":29303,\"start\":29289},{\"attributes\":{\"n\":\"7.\"},\"end\":31999,\"start\":31989},{\"end\":33014,\"start\":33004},{\"end\":33336,\"start\":33326},{\"end\":33477,\"start\":33467},{\"end\":33794,\"start\":33784},{\"end\":33928,\"start\":33918},{\"end\":34161,\"start\":34151}]", "table": "[{\"end\":35903,\"start\":35057},{\"end\":36707,\"start\":36247}]", "figure_caption": "[{\"end\":33324,\"start\":33016},{\"end\":33465,\"start\":33338},{\"end\":33782,\"start\":33479},{\"end\":33916,\"start\":33796},{\"end\":34149,\"start\":33930},{\"end\":34559,\"start\":34163},{\"end\":34857,\"start\":34562},{\"end\":35057,\"start\":34860},{\"end\":36247,\"start\":35906}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":1851,\"start\":1845},{\"end\":3722,\"start\":3714},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15484,\"start\":15476},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17435,\"start\":17427},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17767,\"start\":17759},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18116,\"start\":18108},{\"end\":18223,\"start\":18215},{\"end\":19467,\"start\":19459},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22002,\"start\":21996},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26252,\"start\":26244},{\"end\":30855,\"start\":30847},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":31069,\"start\":31061}]", "bib_author_first_name": "[{\"end\":36866,\"start\":36865},{\"end\":36882,\"start\":36881},{\"end\":36893,\"start\":36892},{\"end\":36900,\"start\":36899},{\"end\":36910,\"start\":36909},{\"end\":36922,\"start\":36921},{\"end\":36937,\"start\":36936},{\"end\":37249,\"start\":37248},{\"end\":37447,\"start\":37446},{\"end\":37457,\"start\":37456},{\"end\":37631,\"start\":37630},{\"end\":37640,\"start\":37639},{\"end\":37652,\"start\":37651},{\"end\":37665,\"start\":37664},{\"end\":37674,\"start\":37673},{\"end\":37875,\"start\":37874},{\"end\":37892,\"start\":37891},{\"end\":37904,\"start\":37903},{\"end\":37914,\"start\":37913},{\"end\":37916,\"start\":37915},{\"end\":38160,\"start\":38159},{\"end\":38172,\"start\":38171},{\"end\":38389,\"start\":38385},{\"end\":38397,\"start\":38396},{\"end\":38405,\"start\":38404},{\"end\":38411,\"start\":38410},{\"end\":38419,\"start\":38418},{\"end\":38640,\"start\":38636},{\"end\":38650,\"start\":38649},{\"end\":38885,\"start\":38884},{\"end\":38899,\"start\":38898},{\"end\":38903,\"start\":38900},{\"end\":38913,\"start\":38912},{\"end\":38925,\"start\":38924},{\"end\":38929,\"start\":38926},{\"end\":38941,\"start\":38940},{\"end\":38949,\"start\":38948},{\"end\":39391,\"start\":39390},{\"end\":39403,\"start\":39402},{\"end\":39574,\"start\":39573},{\"end\":39583,\"start\":39582},{\"end\":39585,\"start\":39584},{\"end\":39594,\"start\":39593},{\"end\":39607,\"start\":39606},{\"end\":39622,\"start\":39621},{\"end\":39634,\"start\":39633},{\"end\":39641,\"start\":39640},{\"end\":39651,\"start\":39650},{\"end\":39660,\"start\":39659},{\"end\":39672,\"start\":39671},{\"end\":39691,\"start\":39690},{\"end\":39700,\"start\":39699},{\"end\":39710,\"start\":39709},{\"end\":39717,\"start\":39716},{\"end\":40048,\"start\":40047},{\"end\":40057,\"start\":40056},{\"end\":40215,\"start\":40214},{\"end\":40221,\"start\":40220},{\"end\":40230,\"start\":40229},{\"end\":40237,\"start\":40236},{\"end\":40418,\"start\":40417},{\"end\":40420,\"start\":40419},{\"end\":40428,\"start\":40427},{\"end\":40650,\"start\":40649},{\"end\":40657,\"start\":40656},{\"end\":40665,\"start\":40664},{\"end\":40881,\"start\":40880},{\"end\":40890,\"start\":40889},{\"end\":40900,\"start\":40899},{\"end\":40907,\"start\":40906},{\"end\":40914,\"start\":40913},{\"end\":40929,\"start\":40928},{\"end\":40938,\"start\":40937},{\"end\":40949,\"start\":40948},{\"end\":40958,\"start\":40957},{\"end\":40966,\"start\":40965},{\"end\":40980,\"start\":40979},{\"end\":41264,\"start\":41263},{\"end\":41274,\"start\":41273},{\"end\":41276,\"start\":41275},{\"end\":41285,\"start\":41284},{\"end\":41294,\"start\":41293},{\"end\":41304,\"start\":41303},{\"end\":41314,\"start\":41313},{\"end\":41328,\"start\":41327},{\"end\":41581,\"start\":41580},{\"end\":41588,\"start\":41587},{\"end\":41597,\"start\":41596},{\"end\":41607,\"start\":41606},{\"end\":41617,\"start\":41616},{\"end\":41632,\"start\":41631},{\"end\":41845,\"start\":41844},{\"end\":41855,\"start\":41854},{\"end\":41863,\"start\":41862},{\"end\":41872,\"start\":41871},{\"end\":41882,\"start\":41881},{\"end\":42091,\"start\":42090},{\"end\":42103,\"start\":42102},{\"end\":42115,\"start\":42114},{\"end\":42125,\"start\":42124},{\"end\":42134,\"start\":42133},{\"end\":42148,\"start\":42147},{\"end\":42332,\"start\":42331},{\"end\":42339,\"start\":42338},{\"end\":42351,\"start\":42350},{\"end\":42363,\"start\":42362},{\"end\":42372,\"start\":42371},{\"end\":42383,\"start\":42382},{\"end\":42403,\"start\":42402},{\"end\":42412,\"start\":42411},{\"end\":42421,\"start\":42420},{\"end\":42429,\"start\":42428},{\"end\":42439,\"start\":42438},{\"end\":42451,\"start\":42450},{\"end\":42802,\"start\":42801},{\"end\":42808,\"start\":42807},{\"end\":42822,\"start\":42821},{\"end\":43012,\"start\":43011},{\"end\":43022,\"start\":43021},{\"end\":43032,\"start\":43031},{\"end\":43043,\"start\":43042},{\"end\":43053,\"start\":43052},{\"end\":43255,\"start\":43254},{\"end\":43257,\"start\":43256},{\"end\":43436,\"start\":43435},{\"end\":43449,\"start\":43448},{\"end\":43459,\"start\":43458},{\"end\":43617,\"start\":43616},{\"end\":43627,\"start\":43626},{\"end\":43641,\"start\":43640},{\"end\":43847,\"start\":43846},{\"end\":43858,\"start\":43857},{\"end\":43866,\"start\":43865},{\"end\":43868,\"start\":43867},{\"end\":43878,\"start\":43877},{\"end\":43885,\"start\":43884},{\"end\":43897,\"start\":43896},{\"end\":43913,\"start\":43912},{\"end\":43922,\"start\":43921},{\"end\":43929,\"start\":43928},{\"end\":43942,\"start\":43941},{\"end\":44241,\"start\":44240},{\"end\":44249,\"start\":44248},{\"end\":44257,\"start\":44256},{\"end\":44267,\"start\":44266},{\"end\":44277,\"start\":44276},{\"end\":44288,\"start\":44287},{\"end\":44298,\"start\":44297},{\"end\":44309,\"start\":44308},{\"end\":44559,\"start\":44558},{\"end\":44567,\"start\":44566},{\"end\":44764,\"start\":44763},{\"end\":44771,\"start\":44770},{\"end\":44777,\"start\":44776},{\"end\":44789,\"start\":44788},{\"end\":45049,\"start\":45048},{\"end\":45062,\"start\":45061},{\"end\":45071,\"start\":45070},{\"end\":45321,\"start\":45320},{\"end\":45329,\"start\":45328},{\"end\":45337,\"start\":45336},{\"end\":45560,\"start\":45559},{\"end\":45568,\"start\":45567},{\"end\":45577,\"start\":45576},{\"end\":45589,\"start\":45588},{\"end\":45597,\"start\":45596},{\"end\":45812,\"start\":45811},{\"end\":45823,\"start\":45822},{\"end\":45833,\"start\":45832},{\"end\":46015,\"start\":46014},{\"end\":46029,\"start\":46028},{\"end\":46044,\"start\":46043},{\"end\":46055,\"start\":46054},{\"end\":46065,\"start\":46064},{\"end\":46395,\"start\":46394},{\"end\":46409,\"start\":46408},{\"end\":46418,\"start\":46417},{\"end\":46426,\"start\":46425},{\"end\":46437,\"start\":46436},{\"end\":46447,\"start\":46446},{\"end\":46713,\"start\":46712},{\"end\":46722,\"start\":46721},{\"end\":46730,\"start\":46729},{\"end\":46742,\"start\":46741},{\"end\":46750,\"start\":46749},{\"end\":46998,\"start\":46997},{\"end\":47008,\"start\":47007},{\"end\":47017,\"start\":47016},{\"end\":47327,\"start\":47326},{\"end\":47338,\"start\":47337},{\"end\":47351,\"start\":47350},{\"end\":47360,\"start\":47359},{\"end\":47370,\"start\":47369},{\"end\":47588,\"start\":47587},{\"end\":47602,\"start\":47601},{\"end\":47617,\"start\":47616},{\"end\":47628,\"start\":47627},{\"end\":47862,\"start\":47861},{\"end\":47870,\"start\":47869},{\"end\":47878,\"start\":47877},{\"end\":47886,\"start\":47885},{\"end\":48092,\"start\":48091},{\"end\":48107,\"start\":48106},{\"end\":48120,\"start\":48119},{\"end\":48304,\"start\":48303},{\"end\":48319,\"start\":48318},{\"end\":48329,\"start\":48328},{\"end\":48472,\"start\":48471},{\"end\":48478,\"start\":48477},{\"end\":48486,\"start\":48485},{\"end\":48845,\"start\":48844},{\"end\":48854,\"start\":48853},{\"end\":48869,\"start\":48868},{\"end\":48876,\"start\":48875},{\"end\":48889,\"start\":48888},{\"end\":48897,\"start\":48896},{\"end\":49144,\"start\":49143},{\"end\":49152,\"start\":49151},{\"end\":49159,\"start\":49158},{\"end\":49358,\"start\":49357},{\"end\":49366,\"start\":49365},{\"end\":49373,\"start\":49372},{\"end\":49381,\"start\":49380},{\"end\":49394,\"start\":49393},{\"end\":49723,\"start\":49722},{\"end\":49737,\"start\":49736},{\"end\":49749,\"start\":49748},{\"end\":49761,\"start\":49760}]", "bib_author_last_name": "[{\"end\":36879,\"start\":36867},{\"end\":36890,\"start\":36883},{\"end\":36897,\"start\":36894},{\"end\":36907,\"start\":36901},{\"end\":36919,\"start\":36911},{\"end\":36934,\"start\":36923},{\"end\":36954,\"start\":36938},{\"end\":37256,\"start\":37250},{\"end\":37454,\"start\":37448},{\"end\":37464,\"start\":37458},{\"end\":37637,\"start\":37632},{\"end\":37649,\"start\":37641},{\"end\":37662,\"start\":37653},{\"end\":37671,\"start\":37666},{\"end\":37680,\"start\":37675},{\"end\":37889,\"start\":37876},{\"end\":37901,\"start\":37893},{\"end\":37911,\"start\":37905},{\"end\":37924,\"start\":37917},{\"end\":38169,\"start\":38161},{\"end\":38182,\"start\":38173},{\"end\":38394,\"start\":38390},{\"end\":38402,\"start\":38398},{\"end\":38408,\"start\":38406},{\"end\":38416,\"start\":38412},{\"end\":38424,\"start\":38420},{\"end\":38647,\"start\":38641},{\"end\":38656,\"start\":38651},{\"end\":38896,\"start\":38886},{\"end\":38910,\"start\":38904},{\"end\":38922,\"start\":38914},{\"end\":38938,\"start\":38930},{\"end\":38946,\"start\":38942},{\"end\":38959,\"start\":38950},{\"end\":39400,\"start\":39392},{\"end\":39409,\"start\":39404},{\"end\":39580,\"start\":39575},{\"end\":39591,\"start\":39586},{\"end\":39604,\"start\":39595},{\"end\":39619,\"start\":39608},{\"end\":39631,\"start\":39623},{\"end\":39638,\"start\":39635},{\"end\":39648,\"start\":39642},{\"end\":39657,\"start\":39652},{\"end\":39669,\"start\":39661},{\"end\":39688,\"start\":39673},{\"end\":39697,\"start\":39692},{\"end\":39707,\"start\":39701},{\"end\":39714,\"start\":39711},{\"end\":39727,\"start\":39718},{\"end\":40054,\"start\":40049},{\"end\":40063,\"start\":40058},{\"end\":40218,\"start\":40216},{\"end\":40227,\"start\":40222},{\"end\":40234,\"start\":40231},{\"end\":40241,\"start\":40238},{\"end\":40425,\"start\":40421},{\"end\":40435,\"start\":40429},{\"end\":40654,\"start\":40651},{\"end\":40662,\"start\":40658},{\"end\":40670,\"start\":40666},{\"end\":40887,\"start\":40882},{\"end\":40897,\"start\":40891},{\"end\":40904,\"start\":40901},{\"end\":40911,\"start\":40908},{\"end\":40926,\"start\":40915},{\"end\":40935,\"start\":40930},{\"end\":40946,\"start\":40939},{\"end\":40955,\"start\":40950},{\"end\":40963,\"start\":40959},{\"end\":40977,\"start\":40967},{\"end\":40987,\"start\":40981},{\"end\":41271,\"start\":41265},{\"end\":41282,\"start\":41277},{\"end\":41291,\"start\":41286},{\"end\":41301,\"start\":41295},{\"end\":41311,\"start\":41305},{\"end\":41325,\"start\":41315},{\"end\":41333,\"start\":41329},{\"end\":41585,\"start\":41582},{\"end\":41594,\"start\":41589},{\"end\":41604,\"start\":41598},{\"end\":41614,\"start\":41608},{\"end\":41629,\"start\":41618},{\"end\":41637,\"start\":41633},{\"end\":41852,\"start\":41846},{\"end\":41860,\"start\":41856},{\"end\":41869,\"start\":41864},{\"end\":41879,\"start\":41873},{\"end\":41888,\"start\":41883},{\"end\":42100,\"start\":42092},{\"end\":42112,\"start\":42104},{\"end\":42122,\"start\":42116},{\"end\":42131,\"start\":42126},{\"end\":42145,\"start\":42135},{\"end\":42156,\"start\":42149},{\"end\":42336,\"start\":42333},{\"end\":42348,\"start\":42340},{\"end\":42360,\"start\":42352},{\"end\":42369,\"start\":42364},{\"end\":42380,\"start\":42373},{\"end\":42400,\"start\":42384},{\"end\":42409,\"start\":42404},{\"end\":42418,\"start\":42413},{\"end\":42426,\"start\":42422},{\"end\":42436,\"start\":42430},{\"end\":42448,\"start\":42440},{\"end\":42461,\"start\":42452},{\"end\":42805,\"start\":42803},{\"end\":42819,\"start\":42809},{\"end\":42829,\"start\":42823},{\"end\":43019,\"start\":43013},{\"end\":43029,\"start\":43023},{\"end\":43040,\"start\":43033},{\"end\":43050,\"start\":43044},{\"end\":43059,\"start\":43054},{\"end\":43262,\"start\":43258},{\"end\":43446,\"start\":43437},{\"end\":43456,\"start\":43450},{\"end\":43466,\"start\":43460},{\"end\":43624,\"start\":43618},{\"end\":43638,\"start\":43628},{\"end\":43647,\"start\":43642},{\"end\":43855,\"start\":43848},{\"end\":43863,\"start\":43859},{\"end\":43875,\"start\":43869},{\"end\":43882,\"start\":43879},{\"end\":43894,\"start\":43886},{\"end\":43910,\"start\":43898},{\"end\":43919,\"start\":43914},{\"end\":43926,\"start\":43923},{\"end\":43939,\"start\":43930},{\"end\":43951,\"start\":43943},{\"end\":44246,\"start\":44242},{\"end\":44254,\"start\":44250},{\"end\":44264,\"start\":44258},{\"end\":44274,\"start\":44268},{\"end\":44285,\"start\":44278},{\"end\":44295,\"start\":44289},{\"end\":44306,\"start\":44299},{\"end\":44316,\"start\":44310},{\"end\":44564,\"start\":44560},{\"end\":44574,\"start\":44568},{\"end\":44768,\"start\":44765},{\"end\":44774,\"start\":44772},{\"end\":44786,\"start\":44778},{\"end\":44793,\"start\":44790},{\"end\":45059,\"start\":45050},{\"end\":45068,\"start\":45063},{\"end\":45076,\"start\":45072},{\"end\":45326,\"start\":45322},{\"end\":45334,\"start\":45330},{\"end\":45346,\"start\":45338},{\"end\":45565,\"start\":45561},{\"end\":45574,\"start\":45569},{\"end\":45586,\"start\":45578},{\"end\":45594,\"start\":45590},{\"end\":45606,\"start\":45598},{\"end\":45820,\"start\":45813},{\"end\":45830,\"start\":45824},{\"end\":45840,\"start\":45834},{\"end\":46026,\"start\":46016},{\"end\":46041,\"start\":46030},{\"end\":46052,\"start\":46045},{\"end\":46062,\"start\":46056},{\"end\":46071,\"start\":46066},{\"end\":46406,\"start\":46396},{\"end\":46415,\"start\":46410},{\"end\":46423,\"start\":46419},{\"end\":46434,\"start\":46427},{\"end\":46444,\"start\":46438},{\"end\":46453,\"start\":46448},{\"end\":46719,\"start\":46714},{\"end\":46727,\"start\":46723},{\"end\":46739,\"start\":46731},{\"end\":46747,\"start\":46743},{\"end\":46759,\"start\":46751},{\"end\":47005,\"start\":46999},{\"end\":47014,\"start\":47009},{\"end\":47022,\"start\":47018},{\"end\":47335,\"start\":47328},{\"end\":47348,\"start\":47339},{\"end\":47357,\"start\":47352},{\"end\":47367,\"start\":47361},{\"end\":47376,\"start\":47371},{\"end\":47599,\"start\":47589},{\"end\":47614,\"start\":47603},{\"end\":47625,\"start\":47618},{\"end\":47635,\"start\":47629},{\"end\":47867,\"start\":47863},{\"end\":47875,\"start\":47871},{\"end\":47883,\"start\":47879},{\"end\":47895,\"start\":47887},{\"end\":48104,\"start\":48093},{\"end\":48117,\"start\":48108},{\"end\":48127,\"start\":48121},{\"end\":48316,\"start\":48305},{\"end\":48326,\"start\":48320},{\"end\":48336,\"start\":48330},{\"end\":48475,\"start\":48473},{\"end\":48483,\"start\":48479},{\"end\":48494,\"start\":48487},{\"end\":48502,\"start\":48496},{\"end\":48851,\"start\":48846},{\"end\":48866,\"start\":48855},{\"end\":48873,\"start\":48870},{\"end\":48886,\"start\":48877},{\"end\":48894,\"start\":48890},{\"end\":48905,\"start\":48898},{\"end\":49149,\"start\":49145},{\"end\":49156,\"start\":49153},{\"end\":49162,\"start\":49160},{\"end\":49363,\"start\":49359},{\"end\":49370,\"start\":49367},{\"end\":49378,\"start\":49374},{\"end\":49391,\"start\":49382},{\"end\":49403,\"start\":49395},{\"end\":49734,\"start\":49724},{\"end\":49746,\"start\":49738},{\"end\":49758,\"start\":49750},{\"end\":49766,\"start\":49762}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1609.08675\",\"id\":\"b0\"},\"end\":37216,\"start\":36865},{\"attributes\":{\"id\":\"b1\"},\"end\":37344,\"start\":37218},{\"attributes\":{\"id\":\"b2\"},\"end\":37598,\"start\":37346},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":175905},\"end\":37795,\"start\":37600},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1710722},\"end\":38088,\"start\":37797},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":206596127},\"end\":38312,\"start\":38090},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":6774239},\"end\":38573,\"start\":38314},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":47390681},\"end\":38821,\"start\":38575},{\"attributes\":{\"id\":\"b8\"},\"end\":39131,\"start\":38823},{\"attributes\":{\"id\":\"b9\"},\"end\":39366,\"start\":39133},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1035098},\"end\":39481,\"start\":39368},{\"attributes\":{\"id\":\"b11\"},\"end\":40014,\"start\":39483},{\"attributes\":{\"doi\":\"abs/1505.04474\",\"id\":\"b12\"},\"end\":40166,\"start\":40016},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":206594692},\"end\":40348,\"start\":40168},{\"attributes\":{\"doi\":\"arXiv:1709.01450\",\"id\":\"b14\"},\"end\":40573,\"start\":40350},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206771624},\"end\":40809,\"start\":40575},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":206595627},\"end\":41206,\"start\":40811},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":14049355},\"end\":41508,\"start\":41208},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3759573},\"end\":41800,\"start\":41510},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":13000587},\"end\":42019,\"start\":41802},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":206592218},\"end\":42329,\"start\":42021},{\"attributes\":{\"doi\":\"arXiv:1705.06950\",\"id\":\"b21\"},\"end\":42739,\"start\":42331},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7138891},\"end\":42950,\"start\":42741},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":206769852},\"end\":43203,\"start\":42952},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":9426884},\"end\":43413,\"start\":43205},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3155054},\"end\":43550,\"start\":43415},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":3256374},\"end\":43775,\"start\":43552},{\"attributes\":{\"id\":\"b27\"},\"end\":44150,\"start\":43777},{\"attributes\":{\"id\":\"b28\"},\"end\":44504,\"start\":44152},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1995092},\"end\":44681,\"start\":44506},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":10328909},\"end\":44949,\"start\":44683},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":83721},\"end\":45239,\"start\":44951},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":7698871},\"end\":45485,\"start\":45241},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":14142501},\"end\":45760,\"start\":45487},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":8777811},\"end\":45951,\"start\":45762},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":3614922},\"end\":46314,\"start\":45953},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":18061547},\"end\":46633,\"start\":46316},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":1189033},\"end\":46923,\"start\":46635},{\"attributes\":{\"doi\":\"CRCV-TR-12-01\",\"id\":\"b38\"},\"end\":47265,\"start\":46925},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":206593880},\"end\":47520,\"start\":47267},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":1191338},\"end\":47794,\"start\":47522},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":17071670},\"end\":48030,\"start\":47796},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":6079438},\"end\":48255,\"start\":48032},{\"attributes\":{\"doi\":\"arXiv:1605.05197\",\"id\":\"b43\"},\"end\":48469,\"start\":48257},{\"attributes\":{\"doi\":\"arXiv:1601.07255\",\"id\":\"b44\"},\"end\":48767,\"start\":48471},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":3337929},\"end\":49077,\"start\":48769},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":246757},\"end\":49278,\"start\":49079},{\"attributes\":{\"doi\":\"arXiv:1712.09374\",\"id\":\"b47\"},\"end\":49609,\"start\":49280},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":10611376},\"end\":49955,\"start\":49611}]", "bib_title": "[{\"end\":37628,\"start\":37600},{\"end\":37872,\"start\":37797},{\"end\":38157,\"start\":38090},{\"end\":38383,\"start\":38314},{\"end\":38634,\"start\":38575},{\"end\":39388,\"start\":39368},{\"end\":40212,\"start\":40168},{\"end\":40647,\"start\":40575},{\"end\":40878,\"start\":40811},{\"end\":41261,\"start\":41208},{\"end\":41578,\"start\":41510},{\"end\":41842,\"start\":41802},{\"end\":42088,\"start\":42021},{\"end\":42799,\"start\":42741},{\"end\":43009,\"start\":42952},{\"end\":43252,\"start\":43205},{\"end\":43433,\"start\":43415},{\"end\":43614,\"start\":43552},{\"end\":44556,\"start\":44506},{\"end\":44761,\"start\":44683},{\"end\":45046,\"start\":44951},{\"end\":45318,\"start\":45241},{\"end\":45557,\"start\":45487},{\"end\":45809,\"start\":45762},{\"end\":46012,\"start\":45953},{\"end\":46392,\"start\":46316},{\"end\":46710,\"start\":46635},{\"end\":47324,\"start\":47267},{\"end\":47585,\"start\":47522},{\"end\":47859,\"start\":47796},{\"end\":48089,\"start\":48032},{\"end\":48842,\"start\":48769},{\"end\":49141,\"start\":49079},{\"end\":49720,\"start\":49611}]", "bib_author": "[{\"end\":36881,\"start\":36865},{\"end\":36892,\"start\":36881},{\"end\":36899,\"start\":36892},{\"end\":36909,\"start\":36899},{\"end\":36921,\"start\":36909},{\"end\":36936,\"start\":36921},{\"end\":36956,\"start\":36936},{\"end\":37258,\"start\":37248},{\"end\":37456,\"start\":37446},{\"end\":37466,\"start\":37456},{\"end\":37639,\"start\":37630},{\"end\":37651,\"start\":37639},{\"end\":37664,\"start\":37651},{\"end\":37673,\"start\":37664},{\"end\":37682,\"start\":37673},{\"end\":37891,\"start\":37874},{\"end\":37903,\"start\":37891},{\"end\":37913,\"start\":37903},{\"end\":37926,\"start\":37913},{\"end\":38171,\"start\":38159},{\"end\":38184,\"start\":38171},{\"end\":38396,\"start\":38385},{\"end\":38404,\"start\":38396},{\"end\":38410,\"start\":38404},{\"end\":38418,\"start\":38410},{\"end\":38426,\"start\":38418},{\"end\":38649,\"start\":38636},{\"end\":38658,\"start\":38649},{\"end\":38898,\"start\":38884},{\"end\":38912,\"start\":38898},{\"end\":38924,\"start\":38912},{\"end\":38940,\"start\":38924},{\"end\":38948,\"start\":38940},{\"end\":38961,\"start\":38948},{\"end\":39402,\"start\":39390},{\"end\":39411,\"start\":39402},{\"end\":39582,\"start\":39573},{\"end\":39593,\"start\":39582},{\"end\":39606,\"start\":39593},{\"end\":39621,\"start\":39606},{\"end\":39633,\"start\":39621},{\"end\":39640,\"start\":39633},{\"end\":39650,\"start\":39640},{\"end\":39659,\"start\":39650},{\"end\":39671,\"start\":39659},{\"end\":39690,\"start\":39671},{\"end\":39699,\"start\":39690},{\"end\":39709,\"start\":39699},{\"end\":39716,\"start\":39709},{\"end\":39729,\"start\":39716},{\"end\":40056,\"start\":40047},{\"end\":40065,\"start\":40056},{\"end\":40220,\"start\":40214},{\"end\":40229,\"start\":40220},{\"end\":40236,\"start\":40229},{\"end\":40243,\"start\":40236},{\"end\":40427,\"start\":40417},{\"end\":40437,\"start\":40427},{\"end\":40656,\"start\":40649},{\"end\":40664,\"start\":40656},{\"end\":40672,\"start\":40664},{\"end\":40889,\"start\":40880},{\"end\":40899,\"start\":40889},{\"end\":40906,\"start\":40899},{\"end\":40913,\"start\":40906},{\"end\":40928,\"start\":40913},{\"end\":40937,\"start\":40928},{\"end\":40948,\"start\":40937},{\"end\":40957,\"start\":40948},{\"end\":40965,\"start\":40957},{\"end\":40979,\"start\":40965},{\"end\":40989,\"start\":40979},{\"end\":41273,\"start\":41263},{\"end\":41284,\"start\":41273},{\"end\":41293,\"start\":41284},{\"end\":41303,\"start\":41293},{\"end\":41313,\"start\":41303},{\"end\":41327,\"start\":41313},{\"end\":41335,\"start\":41327},{\"end\":41587,\"start\":41580},{\"end\":41596,\"start\":41587},{\"end\":41606,\"start\":41596},{\"end\":41616,\"start\":41606},{\"end\":41631,\"start\":41616},{\"end\":41639,\"start\":41631},{\"end\":41854,\"start\":41844},{\"end\":41862,\"start\":41854},{\"end\":41871,\"start\":41862},{\"end\":41881,\"start\":41871},{\"end\":41890,\"start\":41881},{\"end\":42102,\"start\":42090},{\"end\":42114,\"start\":42102},{\"end\":42124,\"start\":42114},{\"end\":42133,\"start\":42124},{\"end\":42147,\"start\":42133},{\"end\":42158,\"start\":42147},{\"end\":42338,\"start\":42331},{\"end\":42350,\"start\":42338},{\"end\":42362,\"start\":42350},{\"end\":42371,\"start\":42362},{\"end\":42382,\"start\":42371},{\"end\":42402,\"start\":42382},{\"end\":42411,\"start\":42402},{\"end\":42420,\"start\":42411},{\"end\":42428,\"start\":42420},{\"end\":42438,\"start\":42428},{\"end\":42450,\"start\":42438},{\"end\":42463,\"start\":42450},{\"end\":42807,\"start\":42801},{\"end\":42821,\"start\":42807},{\"end\":42831,\"start\":42821},{\"end\":43021,\"start\":43011},{\"end\":43031,\"start\":43021},{\"end\":43042,\"start\":43031},{\"end\":43052,\"start\":43042},{\"end\":43061,\"start\":43052},{\"end\":43264,\"start\":43254},{\"end\":43448,\"start\":43435},{\"end\":43458,\"start\":43448},{\"end\":43468,\"start\":43458},{\"end\":43626,\"start\":43616},{\"end\":43640,\"start\":43626},{\"end\":43649,\"start\":43640},{\"end\":43857,\"start\":43846},{\"end\":43865,\"start\":43857},{\"end\":43877,\"start\":43865},{\"end\":43884,\"start\":43877},{\"end\":43896,\"start\":43884},{\"end\":43912,\"start\":43896},{\"end\":43921,\"start\":43912},{\"end\":43928,\"start\":43921},{\"end\":43941,\"start\":43928},{\"end\":43953,\"start\":43941},{\"end\":44248,\"start\":44240},{\"end\":44256,\"start\":44248},{\"end\":44266,\"start\":44256},{\"end\":44276,\"start\":44266},{\"end\":44287,\"start\":44276},{\"end\":44297,\"start\":44287},{\"end\":44308,\"start\":44297},{\"end\":44318,\"start\":44308},{\"end\":44566,\"start\":44558},{\"end\":44576,\"start\":44566},{\"end\":44770,\"start\":44763},{\"end\":44776,\"start\":44770},{\"end\":44788,\"start\":44776},{\"end\":44795,\"start\":44788},{\"end\":45061,\"start\":45048},{\"end\":45070,\"start\":45061},{\"end\":45078,\"start\":45070},{\"end\":45328,\"start\":45320},{\"end\":45336,\"start\":45328},{\"end\":45348,\"start\":45336},{\"end\":45567,\"start\":45559},{\"end\":45576,\"start\":45567},{\"end\":45588,\"start\":45576},{\"end\":45596,\"start\":45588},{\"end\":45608,\"start\":45596},{\"end\":45822,\"start\":45811},{\"end\":45832,\"start\":45822},{\"end\":45842,\"start\":45832},{\"end\":46028,\"start\":46014},{\"end\":46043,\"start\":46028},{\"end\":46054,\"start\":46043},{\"end\":46064,\"start\":46054},{\"end\":46073,\"start\":46064},{\"end\":46408,\"start\":46394},{\"end\":46417,\"start\":46408},{\"end\":46425,\"start\":46417},{\"end\":46436,\"start\":46425},{\"end\":46446,\"start\":46436},{\"end\":46455,\"start\":46446},{\"end\":46721,\"start\":46712},{\"end\":46729,\"start\":46721},{\"end\":46741,\"start\":46729},{\"end\":46749,\"start\":46741},{\"end\":46761,\"start\":46749},{\"end\":47007,\"start\":46997},{\"end\":47016,\"start\":47007},{\"end\":47024,\"start\":47016},{\"end\":47337,\"start\":47326},{\"end\":47350,\"start\":47337},{\"end\":47359,\"start\":47350},{\"end\":47369,\"start\":47359},{\"end\":47378,\"start\":47369},{\"end\":47601,\"start\":47587},{\"end\":47616,\"start\":47601},{\"end\":47627,\"start\":47616},{\"end\":47637,\"start\":47627},{\"end\":47869,\"start\":47861},{\"end\":47877,\"start\":47869},{\"end\":47885,\"start\":47877},{\"end\":47897,\"start\":47885},{\"end\":48106,\"start\":48091},{\"end\":48119,\"start\":48106},{\"end\":48129,\"start\":48119},{\"end\":48318,\"start\":48303},{\"end\":48328,\"start\":48318},{\"end\":48338,\"start\":48328},{\"end\":48477,\"start\":48471},{\"end\":48485,\"start\":48477},{\"end\":48496,\"start\":48485},{\"end\":48504,\"start\":48496},{\"end\":48853,\"start\":48844},{\"end\":48868,\"start\":48853},{\"end\":48875,\"start\":48868},{\"end\":48888,\"start\":48875},{\"end\":48896,\"start\":48888},{\"end\":48907,\"start\":48896},{\"end\":49151,\"start\":49143},{\"end\":49158,\"start\":49151},{\"end\":49164,\"start\":49158},{\"end\":49365,\"start\":49357},{\"end\":49372,\"start\":49365},{\"end\":49380,\"start\":49372},{\"end\":49393,\"start\":49380},{\"end\":49405,\"start\":49393},{\"end\":49736,\"start\":49722},{\"end\":49748,\"start\":49736},{\"end\":49760,\"start\":49748},{\"end\":49768,\"start\":49760}]", "bib_venue": "[{\"end\":37028,\"start\":36972},{\"end\":37246,\"start\":37218},{\"end\":37444,\"start\":37346},{\"end\":37686,\"start\":37682},{\"end\":37930,\"start\":37926},{\"end\":38188,\"start\":38184},{\"end\":38430,\"start\":38426},{\"end\":38683,\"start\":38658},{\"end\":38882,\"start\":38823},{\"end\":39217,\"start\":39133},{\"end\":39415,\"start\":39411},{\"end\":39571,\"start\":39483},{\"end\":40045,\"start\":40016},{\"end\":40247,\"start\":40243},{\"end\":40415,\"start\":40350},{\"end\":40676,\"start\":40672},{\"end\":40993,\"start\":40989},{\"end\":41339,\"start\":41335},{\"end\":41643,\"start\":41639},{\"end\":41894,\"start\":41890},{\"end\":42162,\"start\":42158},{\"end\":42518,\"start\":42479},{\"end\":42835,\"start\":42831},{\"end\":43065,\"start\":43061},{\"end\":43294,\"start\":43264},{\"end\":43472,\"start\":43468},{\"end\":43653,\"start\":43649},{\"end\":43844,\"start\":43777},{\"end\":44238,\"start\":44152},{\"end\":44580,\"start\":44576},{\"end\":44799,\"start\":44795},{\"end\":45082,\"start\":45078},{\"end\":45352,\"start\":45348},{\"end\":45612,\"start\":45608},{\"end\":45846,\"start\":45842},{\"end\":46122,\"start\":46073},{\"end\":46459,\"start\":46455},{\"end\":46765,\"start\":46761},{\"end\":46995,\"start\":46925},{\"end\":47382,\"start\":47378},{\"end\":47641,\"start\":47637},{\"end\":47901,\"start\":47897},{\"end\":48133,\"start\":48129},{\"end\":48301,\"start\":48257},{\"end\":48595,\"start\":48520},{\"end\":48911,\"start\":48907},{\"end\":49168,\"start\":49164},{\"end\":49355,\"start\":49280},{\"end\":49772,\"start\":49768}]"}}}, "year": 2023, "month": 12, "day": 17}