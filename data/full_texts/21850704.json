{"id": 21850704, "updated": "2023-10-01 02:14:27.395", "metadata": {"title": "A Deep Reinforced Model for Abstractive Summarization", "authors": "[{\"first\":\"Romain\",\"last\":\"Paulus\",\"middle\":[]},{\"first\":\"Caiming\",\"last\":\"Xiong\",\"middle\":[]},{\"first\":\"Richard\",\"last\":\"Socher\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 5, "day": 11}, "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. However, for longer documents and summaries, these models often include repetitive and incoherent phrases. We introduce a neural network model with intra-attention and a new training method. This method combines standard supervised word prediction and reinforcement learning (RL). Models trained only with the former often exhibit\"exposure bias\"-- they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, a 5.7 absolute points improvement over previous state-of-the-art models. It also performs well as the first abstractive model on the New York Times corpus. Human evaluation also shows that our model produces higher quality summaries.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1705.04304", "mag": "2952548909", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/PaulusXS18", "doi": null}}, "content": {"source": {"pdf_hash": "8a6a73b50ee31234bcf93aae3653d37b4e6ee3ba", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1705.04304v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4681367c6ff3fcc3cf7cac7c670532d0143141f3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8a6a73b50ee31234bcf93aae3653d37b4e6ee3ba.txt", "contents": "\nA Deep Reinforced Model for Abstractive Summarization\n\n\nRomain Paulus rpaulus@salesforce.com \nCaiming Xiong cxiong@salesforce.com \nRichard Socher rsocher@salesforce.com \nA Deep Reinforced Model for Abstractive Summarization\n\nAttentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. However, for longer documents and summaries, these models often include repetitive and incoherent phrases. We introduce a neural network model with intra-attention and a new training method. This method combines standard supervised word prediction and reinforcement learning (RL). Models trained only with the former often exhibit \"exposure bias\" -they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, a 5.7 absolute points improvement over previous state-of-the-art models. It also performs well as the first abstractive model on the New York Times corpus. Human evaluation also shows that our model produces higher quality summaries.\n\nIntroduction\n\nText summarization is the process of automatically generating natural language summaries from an input document while retaining the important points.\n\nBy condensing large quantities of information into short, informative summaries, summarization can aid many downstream applications such as creating news digests, search, and report generation.\n\nThere are two prominent types of summarization algorithms. First, extractive summarization systems form summaries by copying parts of the input (Neto et al., 2002;Dorr et al., 2003;Nallapati et al., 2017). Second, abstractive summarization systems generate new phrases, possibly rephrasing or using words that were not in the original text (Chopra et al., 2016;Zeng et al., 2016).\n\nRecently, neural network models Zeng et al., 2016), based on the attentional encoder-decoder model for machine translation (Bahdanau et al., 2014), were able to generate abstractive summaries with high ROUGE scores. However, these systems have typically focused on summarizing short input sequences (one or two sentences) to generate even shorter summaries. For example, the summaries on the DUC-2004 dataset generated by the state-of-the-art system by Zeng et al. (2016) are limited to 75 characters.  also applied their abstractive summarization model on the CNN/Daily Mail dataset (Hermann et al., 2015), which contains input sequences of up to 800 tokens and multisentence summaries of up to 100 tokens. The analysis by  illustrate a key problem with attentional encoder-decoder models: they often generate unnatural summaries consisting of repeated phrases.\n\nWe present a new abstractive summarization model that achieves state-of-the-art results on the CNN/Daily Mail and similarly good results on the New York Times dataset (NYT) (Sandhaus, 2008). To our knowledge, this is the first model for abstractive summarization on the NYT dataset. We introduce a key attention mechanism and a new learning objective to address the repeating phrase problem: (i) we use an intra-temporal attention in the encoder that records previous attention weights for each of the input tokens while a sequential intra-attention model in the decoder takes into account which words have already been generated by the decoder. (ii) we propose a new objective function by combining the maximum-likelihood cross-entropy loss used in prior work with rewards from policy gradient reinforcement learning to reduce exposure bias. We show that our model achieves 41.16 ROUGE-1 on the CNN/Daily Mail dataset, an absolute improvement of 5.70 to the previous state-of-the-art result. Moreover, we show, through human evaluation of generated outputs, that our model generates more readable summaries compared to other techniques.\n\n\nNeural Intra-attention Model\n\nIn this section, we present our intra-attention model based on the encoder-decoder network (Sutskever et al., 2014). In all our equations, x = {x 1 , x 2 , . . . , x n } represents the sequence of input (article) tokens, y = {y 1 , y 2 , . . . , y n } the sequence of output (summary) tokens, and denotes the vector concatenation operator.\n\nOur \n\n\nIntra-temporal attention on input sequence\n\nAt each decoding step t, we use an intra-temporal attention function to attend over specific parts of the encoded input sequence in addition to the decoder's own hidden state and the previouslygenerated word (Sankaran et al., 2016). This kind of attention prevents the model from attending over the sames parts of the input on different decoding steps.  have shown that such an intra-temporal attention can reduce the amount of repetitions when attending over long documents.\n\nWe define e ti as the attention score of the hidden input state h e i at decoding time step t:\ne ti = f (h d t , h e i ),(1)\nwhere f can be any function returning a scalar e ti from the h d t and h e i vectors. While some attention models use functions as simple as the dot-product between the two vectors, we choose to use a bilinear function:\nf (h d t , h e i ) = h d t T W e attn h e i .(2)\nWe normalize the attention weights with the following temporal attention function, penalizing input tokens that have obtained high attention scores in past decoding steps. We define new temporal scores e ti :\ne ti = \uf8f1 \uf8f2 \uf8f3 exp(e ti ) if t = 1 exp(e ti ) t\u22121 j=1 exp(e ji )\notherwise.\n\n( 3) Finally, we compute the normalized attention scores \u03b1 e ti across the inputs and use these weights to obtain the input context vector c e t :\n\u03b1 e ti = e ti n j=1 e tj(4)c e t = n i=1 \u03b1 e ti h e i .(5)\n\nIntra-decoder attention\n\nWhile this intra-temporal attention function ensures that different parts of the encoded input sequence are used, our decoder can still generate repeated phrases based on its own hidden states, especially when generating long sequences. To prevent that, we want to incorporate more information about the previously decoded sequence into the decoder. Looking back at previous decoding steps will allow our model to make more structured predictions and avoid repeating the same information, even if that information was generated many steps away. To achieve this, we introduce an intra-decoder attention mechanism. This mechanism is not present in current encoder-decoder models.\n\nFor each decoding step t, our model computes a new decoder context vector c d t . We set c d 1 to a vector of zeros since the generated sequence is empty on the first decoding step. For t > 1, we use the following equations:  Figure 1 illustrates the intra-attention context vector computation c d t , in addition to the encoder temporal attention, and their use in the decoder.\ne d tt = h d t T W d attn h d t(6)\u03b1 d tt = exp(e d tt ) t\u22121 j=1 exp(e d tj )(7)c d t = t\u22121 j=1 \u03b1 d tj h d j(8)\nA closely-related intra-RNN attention function has been introduced by Cheng et al. (2016) but their implementation works by modifying the underlying LSTM function, and they do not apply it to long sequence generation problems. This is a major difference with our method, which makes no assumptions about the type of decoder RNN, thus is more simple and widely applicable to other types of recurrent networks.\n\n\nToken generation and pointer\n\nTo generate a token, our decoder uses either a token-generation softmax layer or a pointer mechanism to copy rare or unseen from the input sequence. We use a switch function that decides at each decoding step whether to use the token generation or the pointer (Gulcehre et al., 2016;. We define u t as a binary value, equal to 1 if the pointer mechanism is used to output y t , and 0 otherwise. In the following equations, all probabilities are conditioned on y t , . . . , y t\u22121 , x, even when not explicitly stated.\n\nOur token-generation layer generates the following probability distribution:\np(y t |u t = 0) = softmax(W out [h d t c e t c d t ] + b out )(9)\nOn the other hand, the pointer mechanism uses the temporal attention weights \u03b1 e ti as the probability distribution to copy the input token x i .\np(y t = x i |u t = 1) = \u03b1 e ti(10)\nWe also compute the probability of using the copy mechanism for the decoding step t:\np(u t = 1) = \u03c3(W u [h d t c e t c d t ] + b u ),(11)\nwhere \u03c3 is the sigmoid activation function. Putting Equations 9 , 10 and 11 together, we obtain our final probability distribution for the output token y t :\np(y t ) = p(u t = 1)p(y t |u t = 1) +p(u t = 0)p(y t |u t = 0).(12)\nThe ground-truth value for u t and the corresponding i index of the target input token when u t = 1 are provided at every decoding step during training. We set u t = 1 either when y t is an out-of-vocabulary token or when it is a pre-defined named entity (see Section 5).\n\n\nSharing decoder weights\n\nIn addition to using the same embedding matrix W emb for the encoder and the decoder sequences, we introduce some weight-sharing between this embedding matrix and the W out matrix of the token-generation layer:\nW out = tanh(W emb W proj )(13)\nThe goal of this weight-sharing is to use the syntactic and semantic information contained in the embedding matrix to improve the tokengeneration function. Similar weight-sharing methods have been applied to language modeling (Inan et al., 2016;Press and Wolf, 2016). We believe this method is even more applicable to sequence-tosequence tasks like summarization where the input and output sequences are tightly related, sharing the same vocabulary and a similar syntax. In practice, we found that a summarization model using such shared weights converges much faster than when using separate W out and W emb matrices.\n\n\nRepetition avoidance at test time\n\nAnother way to avoid repetitions comes from our observation that in both the CNN/Daily Mail and NYT datasets, ground-truth summaries almost never contain the same trigram twice. Based on this observation, we force our decoder to never output the same trigram more than once during testing. We do this by setting p(y t ) = 0 during beam search, when outputting y t would create a trigram that already exists in the previously decoded sequence of the current beam. Even though this method makes assumptions about the output format and the dataset at hand, we believe that the majority of abstractive summarization tasks would benefit from this hard constraint. We apply this method to all our models in the experiments section.\n\n\nHybrid Learning Objective\n\nIn this section, we explore different ways of training our encoder-decoder model. In particular, we propose reinforcement learning-based algorithms and their application to our summarization task.\n\n\nSupervised learning with teacher forcing\n\nThe most widely used method to train a decoder RNN for sequence generation, called the teacher forcing\" algorithm (Williams and Zipser, 1989), minimizes a maximum-likelihood loss at each decoding step.\n\nWe define y * = {y * 1 , y * 2 , . . . , y * n } as the ground-truth output sequence for a given input sequence x. The maximum-likelihood training objective is the minimization of the following loss:\nL ml = \u2212 n t=1 log p(y * t |y * 1 , . . . , y * t\u22121 , x) (14)\nHowever, minimizing L ml does not always produce the best results on discrete evaluation metrics such as ROUGE (Lin, 2004). This phenomenon has been observed with similar sequence generation tasks like image captioning with CIDEr (Rennie et al., 2016) and machine translation with BLEU .\n\nThere are two main reasons for this discrepancy. The first one, called exposure bias (Ranzato et al., 2015), comes from the fact that the network is fully supervised at each output token during training, always knowing the ground truth sequence up to the next token to predict, but does not have such supervision when testing, hence accumulating errors as it predicts the sequence. The second reason is more specific to our summarization task: while we only have one ground truth sequence per example during training, a summary can still be considered valid by a human even if it is not equal to the reference summary word for word. The number of potentially valid summaries increases as sequences get longer, since there are more ways to arrange tokens to produce paraphrases or different sentence orders. The ROUGE metrics take some of this flexibility into account, but the maximumlikelihood objective does not.\n\n\nPolicy learning\n\nOne way to remedy this is to learn a policy that maximizes a specific discrete metric instead of minimizing the maximum-likelihood loss, which is made possible with reinforcement learning. In our model, we use the self-critical policy gradient training algorithm (Rennie et al., 2016).\n\nFor this training algorithm, we produce two separate output sequences at each training iteration: y s , which is obtained by sampling from the p(y s t |y s 1 , . . . , y s t\u22121 , x) probability distribution at each decoding time step, and\u0177, the baseline output, obtained by maximizing the output probability distribution at each time step, essentially performing a greedy search. We define r(y) as the reward function for an output sequence y, comparing it with the ground truth sequence y * with the evaluation metric of our choice.\n\nL rl = (r(\u0177)\u2212r(y s )) n t=1 log p(y s t |y s 1 , . . . , y s t\u22121 , x)\n\n(15) We can see that minimizing L rl is equivalent to maximizing the conditional likelihood of the sampled sequence y s if it obtains a higher reward than the baseline\u0177, thus increasing the reward expectation of our model.\n\n\nMixed training objective function\n\nOne potential issue of this reinforcement training objective is that optimizing for a specific discrete metric like ROUGE does not guarantee an increase in quality and readability of the output. It is possible to game such discrete metrics and increase their score without an actual increase in readability or relevance (Liu et al., 2016). While ROUGE measures the n-gram overlap between our generated summary and a reference sequence, humanreadability is better captured by a language model, which is usually measured by perplexity.\n\nSince our maximum-likelihood training objective (Equation 14) is essentially a conditional language model, calculating the probability of a token y t based on the previously predicted sequence {y 1 , . . . , y t\u22121 } and the input sequence x, we hypothesize that it can assist our policy learning algorithm to generate more natural summaries. This motivates us to define a mixed learning objective function that combines equations 14 and 15:\nL mixed = \u03b3L rl + (1 \u2212 \u03b3)L ml ,(16)\nwhere \u03b3 is a scaling factor accounting for the difference in magnitude between L rl and L ml . A similar mixed-objective learning function has been used by  for machine translation on short sequences, but this is its first use in combination with self-critical policy learning for long summarization to explicitly improve readability in addition to evaluation metrics.\n\n4 Related Work\n\n\nNeural encoder-decoder sequence models\n\nNeural encoder-decoder models are widely used in NLP applications such as machine translation (Sutskever et al., 2014), summarization (Chopra et al., 2016;, and question answering (Hermann et al., 2015). These models use recurrent neural networks (RNN), such as long-short term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) to encode an input sentence into a fixed vector, and create a new output sequence from that vector using another RNN. To apply this sequence-to-sequence approach to natural language, word embeddings (Mikolov et al., 2013;Pennington et al., 2014) are used to convert language tokens to vectors that can be used as inputs for these networks. Attention mechanisms (Bahdanau et al., 2014) make these models more performant and scalable, allowing them to look back at parts of the encoded input sequence while the output is generated. These models often use a fixed input and output vocabulary, which prevents them from learning representations for new words. One way to fix this is to allow the decoder network to point back to some specific words or sub-sequences of the input and copy them onto the output sequence (Vinyals et al., 2015;. Gulcehre et al. (2016) and Merity et al. (2016) combine this pointer mechanism with the original word generation layer in the decoder to allow the model to use either method at each decoding step.\n\n\nReinforcement learning for sequence generation\n\nReinforcement learning (RL) is a way of training an agent to interact with a given environment in order to maximize a reward. RL has been used to solve a wide variety of problems, usually when an agent has to perform discrete actions before obtaining a reward, or when the metric to optimize is not differentiable and traditional supervised learning methods cannot be used. This is applicable to sequence generation tasks, because many of the metrics used to evaluate these tasks (like BLEU, ROUGE or METEOR) are not differentiable. In order to optimize that metric directly, Ranzato et al. (2015) have applied the REINFORCE algorithm (Williams, 1992) to train various RNNbased models for sequence generation tasks, leading to significant improvements compared to previous supervised learning methods. While their method requires an additional neural network, called a critic model, to predict the expected reward and stabilize the objective function gradients, Rennie et al. (2016) designed a self-critical sequence training method that does not require this critic model and lead to further improvements on image captioning tasks.\n\n\nText summarization\n\nMost summarization models studied in the past are extractive in nature (Neto et al., 2002;Dorr et al., 2003;Filippova and Altun, 2013;Colmenares et al., 2015;Nallapati et al., 2017), which usually work by identifying the most important phrases of an input document and re-arranging them into a new summary sequence. The more recent abstractive summarization models have more degrees of freedom and can create more novel sequences. Many abstractive models such as Rush et al. (2015), Chopra et al. (2016), Zeng et al. (2016) and  are all based on the neural encoder-decoder architecture (Section 4.1).\n\nA well-studied set of summarization tasks is the Document Understanding Conference (DUC) 1 . These summarization tasks are varied, including short summaries of a single document and long summaries of multiple documents categorized by subject. Most abstractive summarization models have been evaluated on the DUC-2004 dataset, and outperform extractive models on that task (Dorr et al., 2003). However, models trained on the DUC-2004 task can only generate very short summaries up to 75 characters, and are usually used with one or two input sentences. Chen et al. \n\n\nDatasets\n\n\nCNN/Daily Mail\n\nWe evaluate our model on a modified version of the CNN/Daily Mail dataset (Hermann et al., 2015), following the same pre-processing steps described in . We refer the reader to that paper for a detailed description. The final dataset contains 286,817 training examples, 13,368 validation examples and 11,487 testing examples. After limiting the input length to 800 tokens and output length to 100 tokens, the average input and output lengths are respectively 632 and 53 tokens.\n\n\nNew York Times\n\nThe New York Times (NYT) dataset (Sandhaus, 2008) is a large collection of articles published between 1996 and 2007. Even though this dataset has been used to train extractive summarization systems (Hong and Nenkova, 2014; or closely-related models for predicting the importance of a phrase in an article (Yang and Nenkova, 2014;Nye and Nenkova, 2015;Hong et al., 2015), we are the first group to run an end-to-end abstractive summarization model on the article-abstract pairs of this dataset. While CNN/Daily Mail summaries have a similar wording to their corresponding articles, NYT abstracts are more varied, are shorter and can use a higher level of abstraction and paraphrase. We believe that these two formats are a good complement to each other for abstractive summarization models. Preprocessing: We remove all documents that do not have a full article text, abstract or headline. We concatenate the headline, byline and full article text, separated by special tokens, to produce a single input sequence for each example. We tokenize the input and abstract pairs with the Stanford tokenizer . We convert all tokens to lower-case and replace all numbers with \"0\", remove \"(s)\" and \"(m)\" marks in the abstracts and all occurrences of the following words, singular or plural, if they are surrounded by semicolons or at the end of the abstract: \"photo\", \"graph\", \"chart\", \"map\", \"table\" and \"drawing\". Since the NYT abstracts almost never contain periods, we consider them multi-sentence summaries if we split sentences based on semicolons. This allows us to make the summary format and evaluation procedure similar to the CNN/Daily Mail dataset. These pre-processing steps give us an average of 549 input tokens and 40 output tokens per example, after limiting the input and output lengths to 800 and 100 tokens. Pointer supervision: We run each input and abstract sequence through the Stanford named entity recognizer (NER) . For all named entity tokens in the abstract if the type \"PERSON\", \"LOCATION\", \"ORGANIZATION\" or \"MISC\", we find their first occurrence in the input sequence. We use this information to supervise p(u t ) (Equation 11) and \u03b1 e ti (Equation 4) during training. Note that the NER tagger is only used to create the dataset and is no longer needed during testing, thus we're not adding any dependencies to our model. We also add pointer supervision for out-of-vocabulary output tokens if they are present in the input. Dataset splits: We created our own training,validation, and testing splits for this dataset. Instead of producing random splits, we sorted the documents by their publication date in chronological order and used the first 90% (589,284 examples) for training, the next 5% (32,736) for validation, and the remaining 5% (32,739) for testing. This makes our dataset splits easily reproducible and follows the intuition that if used in a production environment, such a summarization model would be used on recent articles rather than random ones.\n\n\nResults\n\n\nExperiments\n\nSetup: We evaluate the intra-decoder attention mechanism and the mixed-objective learning by running the following experiments on both datasets. We first run maximum-likelihood (ML) training with and without intra-decoder attention (removing c d t from Equations 9 and 11 to disable intra-attention) and select the best performing architecture. Next, we initialize our model with the best ML parameters and we compare reinforcement learning (RL) with our mixed-objective learning (ML+RL), following our objective functions in Equation 15 and 16. For ML training, we use the teacher forcing algorithm with the only difference that at each decoding step, we choose with a 25% probability the previously generated token instead of the ground-truth token as the decoder input token y t\u22121 , which reduces exposure bias (Venkatraman et al., 2015). We use a \u03b3 = 0.9984 for the ML+RL loss function. Implementation details: We use two 200dimensional LSTMs for the bidirectional encoder and one 400-dimensional LSTM for the decoder. We limit the input vocabulary size to 150,000 tokens, and the output vocabulary to 50,000 tokens by selecting the most frequent tokens in the training set. Input word embeddings are 100dimensional and are initialized with GloVe (Pen-nington et al., 2014). We train all our models with Adam (Kingma and Ba, 2014) with a batch size of 50 and a learning rate \u03b1 of 0.001 for ML training and 0.0001 for RL and ML+RL training. At test time, we use beam search of width 5 on all our models to generate our final predictions. ROUGE metrics and options: We report the fulllength F-1 score of the ROUGE-1, ROUGE-2 and ROUGE-L metrics with the Porter stemmer option. For RL and ML+RL training, we use the ROUGE-L score as a reinforcement reward. We also tried ROUGE-2 but we found that it created summaries that almost always reached the maximum length, often ending sentences abruptly.\n\n\nQuantitative analysis\n\nOur results for the CNN/Daily Mail dataset are shown in Table 1, and for the NYT dataset in Table 2. We observe that the intra-decoder attention function helps our model achieve better ROUGE scores on the CNN/Daily Mail but not on the NYT dataset. We believe that the difference in summary lengths between the CNN/Daily Mail and NYT datasets is one of the main reason for this difference in outcome, given that our intra-decoder was designed to improve performance over long output sequences. Further differences in the nature of the summaries and the level of complexity and abstraction between these datasets could also explain these intra-attention results, as well as the absolute ROUGE score differences between CNN/Daily Mail and NYT results.\n\nIn addition, we can see that on all datasets, both the RL and ML+RL models obtain much higher scores than the ML model. In particular, these methods clearly surpass the state-of-the-art model from  on the CNN/Daily Mail dataset.\n\n\nQualitative analysis\n\nWe perform human evaluation to ensure that our increase in ROUGE scores is also followed by an increase in human readability and quality. In particular, we want to know whether the ML+RL training objective did improve readability compared to RL. Evaluation setup: To perform this evaluation, we randomly select 100 test examples from the CNN/Daily Mail dataset. For each example, we show the ground truth summary as well as summaries generated by different models side by side to a human evaluator. The human evaluator does Model ROUGE-1 ROUGE-2 ROUGE-L words-lvt2k-temp-att      not know which summaries come from which model or which one is the ground truth. A score from 1 to 10 is then assigned to each summary, 1 corresponding to the lower level of readability and 10 the highest. Results: Our human evaluation results are shown in Table 4. We can see that even though RL has the highest ROUGE-1 and ROUGE-L scores, it produces the least readable summaries among our experiments. The most common readability issue observed in our RL results, as shown in the example of Table 3, is the presence of short and truncated sentences towards the end of sequences. This confirms that optimizing for single discrete evaluation metric such as ROUGE with RL can be detrimental to the model quality.\n\nOn the other hand, our RL+ML summaries obtain the highest readability scores among our models, hence solving the readability issues of the RL model while also having a higher ROUGE score than ML. This demonstrates the usefulness and value of our RL+ML training method for abstractive summarization.\n\n\nConclusion\n\nWe presented a new model and training procedure that obtains state-of-the-art results in text summarization for the CNN/Daily Mail, improves the readability of the generated summaries and is better suited to long output sequences. We also run our abstractive model on the NYT dataset for the first time. We saw that despite their common use for evaluation, ROUGE scores have their shortcomings and should not be the only metric to optimize on summarization model for long sequences. We believe that our intra-attention decoder and combined training objective could be applied to other sequence-to-sequence tasks with long inputs and outputs, which is an interesting direction for further research.\n\nFigure 1 :\n1Illustration of the encoder and decoder attention functions combined. The two context vectors (marked \"C\") are computed from attending over the encoder hidden states and decoder hidden states. Using these two contexts and the current decoder hidden state (\"H\"), a new word is generated and added to the output sequence.\n\n\n(2016) applied different kinds of attention mechanisms for summarization on the CNN dataset, and Nallapati et al. (2016) used different attention and pointer functions on the CNN and Daily Mail datasets combined. In parallel of our work, See et al. (2017) also developed an abstractive summarization model on this dataset with an extra loss term to increase temporal coverage of the encoder attention function.\n\n\n] from the embedding vectors of x i . We use a single LSTM decoder RNN d , computing hidden states h d t from the embedding vectors of y t . Both input and output embeddings are taken from the same matrix W emb . We initialize the decoder hidden state withmodel reads the input sequence \nwith \na \nbi-directional \nLSTM \nencoder \n{RNN e f wd , RNN e bwd } \ncomputing \nhidden \nstates h e \ni = [h e f wd \n\ni \n\nh e bwd \n\ni \n\nh d \n0 = h e \nn . \n\n\n\nTable 2 :\n2Quantitative results for various models on the New York Times test dataset but the tweet which courted the most attention was a rather mischievous one: 'Ooh is Lewis backing his team mate into Vettel?' he quizzed after Rosberg accused Hamilton of pulling off such a manoeuvre in China. Jenson Button waves to the crowd ahead of the Bahrain Grand Prix which he failed to start Perhaps a career in the media beckons Lewis Hamilton has out-qualified and finished ahead of Nico Rosberg at every race this season. Indeed Rosberg has now beaten his Mercedes team-mate only once in the 11 races since the pair infamously collided in Belgium last year. Hamilton secured the 36th win of his career in Bahrain and his 21st from pole position. Only Michael Schumacher (40), Ayrton Senna (29) and Sebastian Vettel (27) have more. He also became only the sixth F1 driver to lead 2,000 laps. Nico Rosberg has been left in the shade by Lewis Hamilton who celebrates winning his third race of the year Kimi Raikkonen secured a record seventh podium finish in Bahrain following his superb late salvo, although the Ferrari driver has never won in the Gulf Kingdom. It was the Finn's first trip to the rostrum since the 2013 Korean Grand Prix, but his triumph brought a typically deadpan response: 'You're never happy when you finish second... I'm a bit pleased to get a result.' Sparks fly off the back of Kimi Raikkonen's Ferrari en route to finishing second in Bahrain Bernie Ecclestone was in the Bahrain paddock this weekend. He denied trying to engineer a deal for Hamilton, out of contract at the end of the season, to join Ferrari despite earlier insisting that such a move would be 'great' for the sport. The 84-year-old also confirmed that F1 would be in Azerbaijan for the first time next year, even with concerns surrounding the countrys human rights record. 'I think everybody seems to be happy,' Ecclestone said. 'There doesn't seem to be any big problem there. There's no question of it not being on the calendar. It's going to be another good race. Formula One supremo Bernie Ecclestone speaks to Nico Rosberg ahead of the Bahrain Grand Prix Ground truth summary Button denied 100th race start for McLaren after ERS failure. Button then spent much of the Bahrain Grand Prix on Twitter delivering his verdict on the action as it unfolded. Lewis Hamilton has out-qualified and finished ahead of Mercedes team-mate Nico Rosberg at every race this season. Bernie Ecclestone confirms F1 will make its bow in Azerbaijan next season. ML, with intra-attention (ROUGE-1 41.58) Button was denied his 100th race for McLaren. ERS prevented him from making it to the start-line. The Briton. He quizzed after Nico Rosberg accused Lewis Hamilton of pulling off such a manoeuvre in China. Button has been in Azerbaijan for the first time since 2013. RL, with intra-attention (ROUGE-1 50.00) Button was denied his 100th race for McLaren after an ERS prevented him from making it to the start-line. It capped a miserable weekend for the Briton. Button has out-qualified. Finished ahead of Nico Rosberg at Bahrain. Lewis Hamilton has. In 11 races. . The race. To lead 2,000 laps. . In. . . And. . ML+RL, with intra-attention (ROUGE-1 44.00) Button was denied his 100th race for McLaren. The ERS prevented him from making it to the start-line. Button was his team mate in the 11 races in Bahrain. He quizzed after Nico Rosberg accused Lewis Hamilton of pulling off such a manoeuvre in China.Source document \nJenson Button was denied his 100th race for McLaren after an ERS prevented him from making it to the start-line. It \ncapped a miserable weekend for the Briton; his time in Bahrain plagued by reliability issues. Button spent much of the \nrace on Twitter delivering his verdict as the action unfolded. 'Kimi is the man to watch,' and 'loving the sparks', were \namong his pearls of wisdom, \n\nTable 3 :\n3Example from the CNN/Daily Mail test dataset showing the outputs of our three best models after de-tokenization, recapitalization, replacing anonymized entities, and replacing numbers. The ROUGE score corresponds to the specific example.Model \n\nAverage readability \nML \n7.88 \nRL \n5.43 \nML+RL \n8.15 \nGround truth 9.85 \n\n\n\nTable 4 :\n4Comparison of human readability scores on a random subset of the CNN/Daily Mail test dataset. All models are with intra-decoder attention.\nhttp://duc.nist.gov/\n\nNeural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, arXiv:1409.0473arXiv preprintDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 .\n\nDistraction-based neural networks for modeling documents. Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang, Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI-16). the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI-16)Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, and Hui Jiang. 2016. Distraction-based neural networks for modeling documents. In Proceedings of the Twenty-Fifth International Joint Conference on Ar- tificial Intelligence (IJCAI-16). pages 2754-2760.\n\nLong short-term memory-networks for machine reading. Jianpeng Cheng, Li Dong, Mirella Lapata, arXiv:1601.06733arXiv preprintJianpeng Cheng, Li Dong, and Mirella Lapata. 2016. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733 .\n\nAbstractive sentence summarization with attentive recurrent neural networks. Sumit Chopra, Michael Auli, Alexander M Rush, Harvard, Proceedings of NAACL-HLT16 pages. NAACL-HLT16 pagesSumit Chopra, Michael Auli, Alexander M Rush, and SEAS Harvard. 2016. Abstractive sentence sum- marization with attentive recurrent neural networks. Proceedings of NAACL-HLT16 pages 93-98.\n\nHeads: Headline generation as sequence prediction using an abstract feature-rich space. Marina Carlos A Colmenares, Amin Litvak, Fabrizio Mantrach, Silvestri, HLT-NAACL. Carlos A Colmenares, Marina Litvak, Amin Mantrach, and Fabrizio Silvestri. 2015. Heads: Headline gen- eration as sequence prediction using an abstract feature-rich space. In HLT-NAACL. pages 133-142.\n\nHedge trimmer: A parse-and-trim approach to headline generation. Bonnie Dorr, David Zajic, Richard Schwartz, Proceedings of the HLT-NAACL 03 on Text summarization workshop. the HLT-NAACL 03 on Text summarization workshop5Association for Computational LinguisticsBonnie Dorr, David Zajic, and Richard Schwartz. 2003. Hedge trimmer: A parse-and-trim approach to head- line generation. In Proceedings of the HLT-NAACL 03 on Text summarization workshop-Volume 5. As- sociation for Computational Linguistics, pages 1-8.\n\nOvercoming the lack of parallel data in sentence compression. Katja Filippova, Yasemin Altun, EMNLP. Citeseer. Katja Filippova and Yasemin Altun. 2013. Overcom- ing the lack of parallel data in sentence compression. In EMNLP. Citeseer, pages 1481-1491.\n\nCaglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, Yoshua Bengio, arXiv:1603.08148Pointing the unknown words. arXiv preprintCaglar Gulcehre, Sungjin Ahn, Ramesh Nallap- ati, Bowen Zhou, and Yoshua Bengio. 2016. Pointing the unknown words. arXiv preprint arXiv:1603.08148 .\n\nTeaching machines to read and comprehend. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Advances in Neural Information Processing Systems. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su- leyman, and Phil Blunsom. 2015. Teaching ma- chines to read and comprehend. In Advances in Neu- ral Information Processing Systems. pages 1693- 1701.\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. 98Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation 9(8):1735-1780.\n\nSystem combination for multi-document summarization. Kai Hong, Mitchell Marcus, Ani Nenkova, EMNLP. Kai Hong, Mitchell Marcus, and Ani Nenkova. 2015. System combination for multi-document summa- rization. In EMNLP. pages 107-117.\n\nImproving the estimation of word importance for news multidocument summarization-extended technical report. Kai Hong, Ani Nenkova, Kai Hong and Ani Nenkova. 2014. Improving the estimation of word importance for news multi- document summarization-extended technical report .\n\nTying word vectors and word classifiers: A loss framework for language modeling. Khashayar Hakan Inan, Richard Khosravi, Socher, arXiv:1611.01462arXiv preprintHakan Inan, Khashayar Khosravi, and Richard Socher. 2016. Tying word vectors and word classifiers: A loss framework for language modeling. arXiv preprint arXiv:1611.01462 .\n\nAdam: A method for stochastic optimization. Diederik Kingma, Jimmy Ba, arXiv:1412.6980arXiv preprintDiederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 .\n\nThe role of discourse units in near-extractive summarization. Jessy Junyi, Kapil Li, Amanda Thadani, Stent, 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. 137Junyi Jessy Li, Kapil Thadani, and Amanda Stent. 2016. The role of discourse units in near-extractive summarization. In 17th Annual Meeting of the Spe- cial Interest Group on Discourse and Dialogue. page 137.\n\nRouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out: Proceedings of the ACL-04 workshop. Barcelona, Spain8Chin-Yew Lin. 2004. Rouge: A package for auto- matic evaluation of summaries. In Text summariza- tion branches out: Proceedings of the ACL-04 work- shop. Barcelona, Spain, volume 8.\n\nLatent predictor networks for code generation. Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tom\u00e1\u0161 Ko\u010disk\u1ef3, Andrew Senior, Fumin Wang, Phil Blunsom, arXiv:1603.06744arXiv preprintWang Ling, Edward Grefenstette, Karl Moritz Her- mann, Tom\u00e1\u0161 Ko\u010disk\u1ef3, Andrew Senior, Fumin Wang, and Phil Blunsom. 2016. Latent predic- tor networks for code generation. arXiv preprint arXiv:1603.06744 .\n\nHow not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. Chia-Wei Liu, Ryan Lowe, V Iulian, Michael Serban, Laurent Noseworthy, Joelle Charlin, Pineau, arXiv:1603.08023arXiv preprintChia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How not to evaluate your dialogue system: An empirical study of unsupervised evaluation met- rics for dialogue response generation. arXiv preprint arXiv:1603.08023 .\n\nThe stanford corenlp natural language processing toolkit. D Christopher, Mihai Manning, John Surdeanu, Jenny Rose Bauer, Steven Finkel, David Bethard, Mc-Closky, ACL (System Demonstrations). Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David Mc- Closky. 2014. The stanford corenlp natural lan- guage processing toolkit. In ACL (System Demon- strations). pages 55-60.\n\nPointer sentinel mixture models. Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, arXiv:1609.07843arXiv preprintStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843 .\n\nDistributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, Advances in neural information processing systems. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in neural information processing systems. pages 3111-3119.\n\nSummarunner: A recurrent neural network based sequence model for extractive summarization of documents. Ramesh Nallapati, Feifei Zhai, Bowen Zhou, hiP. yi= 1-hi, si, d) 1:1Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017. Summarunner: A recurrent neural network based se- quence model for extractive summarization of docu- ments. hiP (yi= 1-hi, si, d) 1:1.\n\nAbstractive text summarization using sequence-to-sequence rnns and beyond. Ramesh Nallapati, Bowen Zhou, Bing Aglar G\u00fcl\u00e7ehre, Xiang, arXiv:1602.06023arXiv preprintRamesh Nallapati, Bowen Zhou, \u00c7 aglar G\u00fcl\u00e7ehre, Bing Xiang, et al. 2016. Abstractive text summa- rization using sequence-to-sequence rnns and be- yond. arXiv preprint arXiv:1602.06023 .\n\nAutomatic text summarization using a machine learning approach. Joel Larocca Neto, Alex A Freitas, A A Celso, Kaestner, Brazilian Symposium on Artificial Intelligence. SpringerJoel Larocca Neto, Alex A Freitas, and Celso AA Kaestner. 2002. Automatic text summarization us- ing a machine learning approach. In Brazilian Sym- posium on Artificial Intelligence. Springer, pages 205-215.\n\nReward augmented maximum likelihood for neural structured prediction. Mohammad Norouzi, Samy Bengio, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans, Advances In Neural Information Processing Systems. Mohammad Norouzi, Samy Bengio, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans, et al. 2016. Reward augmented maximum likeli- hood for neural structured prediction. In Advances In Neural Information Processing Systems. pages 1723-1731.\n\nIdentification and characterization of newsworthy verbs in world news. Benjamin Nye, Ani Nenkova, HLT-NAACL. Benjamin Nye and Ani Nenkova. 2015. Identification and characterization of newsworthy verbs in world news. In HLT-NAACL. pages 1440-1445.\n\nGlove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, EMNLP. 14Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In EMNLP. volume 14, pages 1532- 1543.\n\nUsing the output embedding to improve language models. Ofir Press, Lior Wolf, arXiv:1608.05859arXiv preprintOfir Press and Lior Wolf. 2016. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859 .\n\nAurelio Marc, Sumit Ranzato, Michael Chopra, Wojciech Auli, Zaremba, arXiv:1511.06732Sequence level training with recurrent neural networks. arXiv preprintMarc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2015. Sequence level train- ing with recurrent neural networks. arXiv preprint arXiv:1511.06732 .\n\nSelf-critical sequence training for image captioning. J Steven, Etienne Rennie, Youssef Marcheret, Jarret Mroueh, Vaibhava Ross, Goel, arXiv:1612.00563arXiv preprintSteven J Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. 2016. Self-critical sequence training for image captioning. arXiv preprint arXiv:1612.00563 .\n\nM Alexander, Rush, arXiv:1509.00685Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization. arXiv preprintAlexander M Rush, Sumit Chopra, and Jason We- ston. 2015. A neural attention model for ab- stractive sentence summarization. arXiv preprint arXiv:1509.00685 .\n\nThe new york times annotated corpus. Linguistic Data Consortium. Evan Sandhaus, 626752PhiladelphiaEvan Sandhaus. 2008. The new york times annotated corpus. Linguistic Data Consortium, Philadelphia 6(12):e26752.\n\nTemporal attention model for neural machine translation. Haitao Baskaran Sankaran, Yaser Mi, Abe Al-Onaizan, Ittycheriah, arXiv:1608.02927arXiv preprintBaskaran Sankaran, Haitao Mi, Yaser Al-Onaizan, and Abe Ittycheriah. 2016. Temporal attention model for neural machine translation. arXiv preprint arXiv:1608.02927 .\n\nGet to the point: Summarization with pointer-generator networks. Abigail See, J Peter, Christopher D Liu, Manning, arXiv:1704.04368arXiv preprintAbigail See, Peter J Liu, and Christopher D Man- ning. 2017. Get to the point: Summarization with pointer-generator networks. arXiv preprint arXiv:1704.04368 .\n\nSequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, Advances in neural information processing systems. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural net- works. In Advances in neural information process- ing systems. pages 3104-3112.\n\nImproving multi-step prediction of learned time series models. Arun Venkatraman, J Andrew Hebert, Bagnell, AAAI. Arun Venkatraman, Martial Hebert, and J Andrew Bagnell. 2015. Improving multi-step prediction of learned time series models. In AAAI. pages 3024- 3030.\n\nPointer networks. Oriol Vinyals, Meire Fortunato, Navdeep Jaitly, Advances in Neural Information Processing Systems. Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In Advances in Neural In- formation Processing Systems. pages 2692-2700.\n\nSimple statistical gradientfollowing algorithms for connectionist reinforcement learning. J Ronald, Williams, Machine learning. 83-4Ronald J Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning. Machine learning 8(3-4):229-256.\n\nA learning algorithm for continually running fully recurrent neural networks. J Ronald, David Williams, Zipser, Neural computation. 12Ronald J Williams and David Zipser. 1989. A learn- ing algorithm for continually running fully recurrent neural networks. Neural computation 1(2):270-280.\n\nGoogle's neural machine translation system. Yonghui Wu, Mike Schuster, Zhifeng Chen, V Quoc, Mohammad Le, Wolfgang Norouzi, Maxim Macherey, Yuan Krikun, Qin Cao, Klaus Gao, Macherey, arXiv:1609.08144Bridging the gap between human and machine translation. arXiv preprintYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural ma- chine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144 .\n\nDetecting information-dense texts in multiple news domains. Yinfei Yang, Ani Nenkova, AAAI. Yinfei Yang and Ani Nenkova. 2014. Detecting information-dense texts in multiple news domains. In AAAI. pages 1650-1656.\n\nEfficient summarization with read-again and copy mechanism. Wenyuan Zeng, Wenjie Luo, Sanja Fidler, Raquel Urtasun, arXiv:1611.03382arXiv preprintWenyuan Zeng, Wenjie Luo, Sanja Fidler, and Raquel Urtasun. 2016. Efficient summarization with read-again and copy mechanism. arXiv preprint arXiv:1611.03382 .\n", "annotations": {"author": "[{\"end\":94,\"start\":57},{\"end\":131,\"start\":95},{\"end\":170,\"start\":132}]", "publisher": null, "author_last_name": "[{\"end\":70,\"start\":64},{\"end\":108,\"start\":103},{\"end\":146,\"start\":140}]", "author_first_name": "[{\"end\":63,\"start\":57},{\"end\":102,\"start\":95},{\"end\":139,\"start\":132}]", "author_affiliation": null, "title": "[{\"end\":54,\"start\":1},{\"end\":224,\"start\":171}]", "venue": null, "abstract": "[{\"end\":1311,\"start\":226}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1836,\"start\":1817},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1854,\"start\":1836},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1877,\"start\":1854},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2034,\"start\":2013},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2052,\"start\":2034},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2105,\"start\":2087},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2201,\"start\":2178},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2526,\"start\":2508},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2661,\"start\":2639},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3108,\"start\":3092},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4204,\"start\":4180},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4712,\"start\":4689},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7761,\"start\":7738},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9472,\"start\":9453},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9493,\"start\":9472},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11466,\"start\":11455},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11740,\"start\":11718},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12851,\"start\":12830},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14057,\"start\":14039},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15276,\"start\":15252},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15313,\"start\":15292},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15360,\"start\":15338},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15492,\"start\":15458},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15714,\"start\":15692},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15738,\"start\":15714},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15876,\"start\":15854},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":16328,\"start\":16306},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16352,\"start\":16330},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16377,\"start\":16357},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17174,\"start\":17153},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17822,\"start\":17803},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17840,\"start\":17822},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17866,\"start\":17840},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17890,\"start\":17866},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17913,\"start\":17890},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18235,\"start\":18215},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18255,\"start\":18237},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18725,\"start\":18706},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19024,\"start\":19002},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19472,\"start\":19456},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19645,\"start\":19621},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":19752,\"start\":19728},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19774,\"start\":19752},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19792,\"start\":19774},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":23274,\"start\":23248},{\"end\":23711,\"start\":23685},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23768,\"start\":23747}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27998,\"start\":27666},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28411,\"start\":27999},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":28854,\"start\":28412},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":32739,\"start\":28855},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33071,\"start\":32740},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":33222,\"start\":33072}]", "paragraph": "[{\"end\":1476,\"start\":1327},{\"end\":1671,\"start\":1478},{\"end\":2053,\"start\":1673},{\"end\":2917,\"start\":2055},{\"end\":4056,\"start\":2919},{\"end\":4428,\"start\":4089},{\"end\":4434,\"start\":4430},{\"end\":4956,\"start\":4481},{\"end\":5052,\"start\":4958},{\"end\":5302,\"start\":5083},{\"end\":5560,\"start\":5352},{\"end\":5634,\"start\":5624},{\"end\":5782,\"start\":5636},{\"end\":6545,\"start\":5868},{\"end\":6925,\"start\":6547},{\"end\":7445,\"start\":7037},{\"end\":7995,\"start\":7478},{\"end\":8073,\"start\":7997},{\"end\":8285,\"start\":8140},{\"end\":8405,\"start\":8321},{\"end\":8616,\"start\":8459},{\"end\":8956,\"start\":8685},{\"end\":9194,\"start\":8984},{\"end\":9845,\"start\":9227},{\"end\":10608,\"start\":9883},{\"end\":10834,\"start\":10638},{\"end\":11080,\"start\":10879},{\"end\":11281,\"start\":11082},{\"end\":11631,\"start\":11344},{\"end\":12547,\"start\":11633},{\"end\":12852,\"start\":12567},{\"end\":13386,\"start\":12854},{\"end\":13457,\"start\":13388},{\"end\":13681,\"start\":13459},{\"end\":14252,\"start\":13719},{\"end\":14694,\"start\":14254},{\"end\":15099,\"start\":14731},{\"end\":15115,\"start\":15101},{\"end\":16526,\"start\":15158},{\"end\":17709,\"start\":16577},{\"end\":18332,\"start\":17732},{\"end\":18898,\"start\":18334},{\"end\":19404,\"start\":18928},{\"end\":22408,\"start\":19423},{\"end\":24332,\"start\":22434},{\"end\":25106,\"start\":24358},{\"end\":25336,\"start\":25108},{\"end\":26653,\"start\":25361},{\"end\":26953,\"start\":26655},{\"end\":27665,\"start\":26968}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5082,\"start\":5053},{\"attributes\":{\"id\":\"formula_1\"},\"end\":5351,\"start\":5303},{\"attributes\":{\"id\":\"formula_2\"},\"end\":5623,\"start\":5561},{\"attributes\":{\"id\":\"formula_3\"},\"end\":5810,\"start\":5783},{\"attributes\":{\"id\":\"formula_4\"},\"end\":5841,\"start\":5810},{\"attributes\":{\"id\":\"formula_5\"},\"end\":6960,\"start\":6926},{\"attributes\":{\"id\":\"formula_6\"},\"end\":7005,\"start\":6960},{\"attributes\":{\"id\":\"formula_7\"},\"end\":7036,\"start\":7005},{\"attributes\":{\"id\":\"formula_8\"},\"end\":8139,\"start\":8074},{\"attributes\":{\"id\":\"formula_9\"},\"end\":8320,\"start\":8286},{\"attributes\":{\"id\":\"formula_10\"},\"end\":8458,\"start\":8406},{\"attributes\":{\"id\":\"formula_11\"},\"end\":8684,\"start\":8617},{\"attributes\":{\"id\":\"formula_12\"},\"end\":9226,\"start\":9195},{\"attributes\":{\"id\":\"formula_13\"},\"end\":11343,\"start\":11282},{\"attributes\":{\"id\":\"formula_14\"},\"end\":14730,\"start\":14695}]", "table_ref": "[{\"end\":24421,\"start\":24414},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":26205,\"start\":26198},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26442,\"start\":26435}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1325,\"start\":1313},{\"attributes\":{\"n\":\"2\"},\"end\":4087,\"start\":4059},{\"attributes\":{\"n\":\"2.1\"},\"end\":4479,\"start\":4437},{\"attributes\":{\"n\":\"2.2\"},\"end\":5866,\"start\":5843},{\"attributes\":{\"n\":\"2.3\"},\"end\":7476,\"start\":7448},{\"attributes\":{\"n\":\"2.4\"},\"end\":8982,\"start\":8959},{\"attributes\":{\"n\":\"2.5\"},\"end\":9881,\"start\":9848},{\"attributes\":{\"n\":\"3\"},\"end\":10636,\"start\":10611},{\"attributes\":{\"n\":\"3.1\"},\"end\":10877,\"start\":10837},{\"attributes\":{\"n\":\"3.2\"},\"end\":12565,\"start\":12550},{\"attributes\":{\"n\":\"3.3\"},\"end\":13717,\"start\":13684},{\"attributes\":{\"n\":\"4.1\"},\"end\":15156,\"start\":15118},{\"attributes\":{\"n\":\"4.2\"},\"end\":16575,\"start\":16529},{\"attributes\":{\"n\":\"4.3\"},\"end\":17730,\"start\":17712},{\"attributes\":{\"n\":\"5\"},\"end\":18909,\"start\":18901},{\"attributes\":{\"n\":\"5.1\"},\"end\":18926,\"start\":18912},{\"attributes\":{\"n\":\"5.2\"},\"end\":19421,\"start\":19407},{\"attributes\":{\"n\":\"6\"},\"end\":22418,\"start\":22411},{\"attributes\":{\"n\":\"6.1\"},\"end\":22432,\"start\":22421},{\"attributes\":{\"n\":\"6.2\"},\"end\":24356,\"start\":24335},{\"attributes\":{\"n\":\"6.3\"},\"end\":25359,\"start\":25339},{\"attributes\":{\"n\":\"7\"},\"end\":26966,\"start\":26956},{\"end\":27677,\"start\":27667},{\"end\":28865,\"start\":28856},{\"end\":32750,\"start\":32741},{\"end\":33082,\"start\":33073}]", "table": "[{\"end\":28854,\"start\":28670},{\"end\":32739,\"start\":32335},{\"end\":33071,\"start\":32989}]", "figure_caption": "[{\"end\":27998,\"start\":27679},{\"end\":28411,\"start\":28001},{\"end\":28670,\"start\":28414},{\"end\":32335,\"start\":28867},{\"end\":32989,\"start\":32752},{\"end\":33222,\"start\":33084}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6781,\"start\":6773}]", "bib_author_first_name": "[{\"end\":33323,\"start\":33316},{\"end\":33343,\"start\":33334},{\"end\":33355,\"start\":33349},{\"end\":33620,\"start\":33616},{\"end\":33634,\"start\":33627},{\"end\":33647,\"start\":33640},{\"end\":33656,\"start\":33654},{\"end\":33665,\"start\":33662},{\"end\":34171,\"start\":34163},{\"end\":34181,\"start\":34179},{\"end\":34195,\"start\":34188},{\"end\":34455,\"start\":34450},{\"end\":34471,\"start\":34464},{\"end\":34487,\"start\":34478},{\"end\":34489,\"start\":34488},{\"end\":34840,\"start\":34834},{\"end\":34866,\"start\":34862},{\"end\":34883,\"start\":34875},{\"end\":35188,\"start\":35182},{\"end\":35200,\"start\":35195},{\"end\":35215,\"start\":35208},{\"end\":35700,\"start\":35695},{\"end\":35719,\"start\":35712},{\"end\":35893,\"start\":35887},{\"end\":35911,\"start\":35904},{\"end\":35923,\"start\":35917},{\"end\":35940,\"start\":35935},{\"end\":35953,\"start\":35947},{\"end\":36216,\"start\":36212},{\"end\":36238,\"start\":36233},{\"end\":36254,\"start\":36248},{\"end\":36274,\"start\":36269},{\"end\":36289,\"start\":36285},{\"end\":36302,\"start\":36295},{\"end\":36317,\"start\":36313},{\"end\":36652,\"start\":36648},{\"end\":36671,\"start\":36665},{\"end\":36869,\"start\":36866},{\"end\":36884,\"start\":36876},{\"end\":36896,\"start\":36893},{\"end\":37155,\"start\":37152},{\"end\":37165,\"start\":37162},{\"end\":37409,\"start\":37400},{\"end\":37429,\"start\":37422},{\"end\":37704,\"start\":37696},{\"end\":37718,\"start\":37713},{\"end\":37933,\"start\":37928},{\"end\":37946,\"start\":37941},{\"end\":37957,\"start\":37951},{\"end\":38328,\"start\":38320},{\"end\":38654,\"start\":38650},{\"end\":38667,\"start\":38661},{\"end\":38686,\"start\":38682},{\"end\":38693,\"start\":38687},{\"end\":38708,\"start\":38703},{\"end\":38724,\"start\":38718},{\"end\":38738,\"start\":38733},{\"end\":38749,\"start\":38745},{\"end\":39132,\"start\":39124},{\"end\":39142,\"start\":39138},{\"end\":39150,\"start\":39149},{\"end\":39166,\"start\":39159},{\"end\":39182,\"start\":39175},{\"end\":39201,\"start\":39195},{\"end\":39579,\"start\":39578},{\"end\":39598,\"start\":39593},{\"end\":39612,\"start\":39608},{\"end\":39628,\"start\":39623},{\"end\":39633,\"start\":39629},{\"end\":39647,\"start\":39641},{\"end\":39661,\"start\":39656},{\"end\":39973,\"start\":39966},{\"end\":39989,\"start\":39982},{\"end\":40002,\"start\":39997},{\"end\":40020,\"start\":40013},{\"end\":40282,\"start\":40277},{\"end\":40296,\"start\":40292},{\"end\":40311,\"start\":40308},{\"end\":40322,\"start\":40318},{\"end\":40324,\"start\":40323},{\"end\":40338,\"start\":40334},{\"end\":40739,\"start\":40733},{\"end\":40757,\"start\":40751},{\"end\":40769,\"start\":40764},{\"end\":41071,\"start\":41065},{\"end\":41088,\"start\":41083},{\"end\":41099,\"start\":41095},{\"end\":41408,\"start\":41404},{\"end\":41416,\"start\":41409},{\"end\":41427,\"start\":41423},{\"end\":41429,\"start\":41428},{\"end\":41440,\"start\":41439},{\"end\":41442,\"start\":41441},{\"end\":41803,\"start\":41795},{\"end\":41817,\"start\":41813},{\"end\":41833,\"start\":41826},{\"end\":41846,\"start\":41842},{\"end\":41864,\"start\":41857},{\"end\":41873,\"start\":41869},{\"end\":42264,\"start\":42256},{\"end\":42273,\"start\":42270},{\"end\":42487,\"start\":42480},{\"end\":42507,\"start\":42500},{\"end\":42529,\"start\":42516},{\"end\":42763,\"start\":42759},{\"end\":42775,\"start\":42771},{\"end\":42941,\"start\":42934},{\"end\":42953,\"start\":42948},{\"end\":42970,\"start\":42963},{\"end\":42987,\"start\":42979},{\"end\":43315,\"start\":43314},{\"end\":43331,\"start\":43324},{\"end\":43347,\"start\":43340},{\"end\":43365,\"start\":43359},{\"end\":43382,\"start\":43374},{\"end\":43605,\"start\":43604},{\"end\":43985,\"start\":43981},{\"end\":44191,\"start\":44185},{\"end\":44216,\"start\":44211},{\"end\":44224,\"start\":44221},{\"end\":44519,\"start\":44512},{\"end\":44526,\"start\":44525},{\"end\":44547,\"start\":44534},{\"end\":44809,\"start\":44805},{\"end\":44826,\"start\":44821},{\"end\":44842,\"start\":44836},{\"end\":45145,\"start\":45141},{\"end\":45167,\"start\":45159},{\"end\":45367,\"start\":45362},{\"end\":45382,\"start\":45377},{\"end\":45401,\"start\":45394},{\"end\":45702,\"start\":45701},{\"end\":45975,\"start\":45974},{\"end\":45989,\"start\":45984},{\"end\":46237,\"start\":46230},{\"end\":46246,\"start\":46242},{\"end\":46264,\"start\":46257},{\"end\":46272,\"start\":46271},{\"end\":46287,\"start\":46279},{\"end\":46300,\"start\":46292},{\"end\":46315,\"start\":46310},{\"end\":46330,\"start\":46326},{\"end\":46342,\"start\":46339},{\"end\":46353,\"start\":46348},{\"end\":46809,\"start\":46803},{\"end\":46819,\"start\":46816},{\"end\":47024,\"start\":47017},{\"end\":47037,\"start\":47031},{\"end\":47048,\"start\":47043},{\"end\":47063,\"start\":47057}]", "bib_author_last_name": "[{\"end\":33332,\"start\":33324},{\"end\":33347,\"start\":33344},{\"end\":33362,\"start\":33356},{\"end\":33625,\"start\":33621},{\"end\":33638,\"start\":33635},{\"end\":33652,\"start\":33648},{\"end\":33660,\"start\":33657},{\"end\":33671,\"start\":33666},{\"end\":34177,\"start\":34172},{\"end\":34186,\"start\":34182},{\"end\":34202,\"start\":34196},{\"end\":34462,\"start\":34456},{\"end\":34476,\"start\":34472},{\"end\":34494,\"start\":34490},{\"end\":34503,\"start\":34496},{\"end\":34860,\"start\":34841},{\"end\":34873,\"start\":34867},{\"end\":34892,\"start\":34884},{\"end\":34903,\"start\":34894},{\"end\":35193,\"start\":35189},{\"end\":35206,\"start\":35201},{\"end\":35224,\"start\":35216},{\"end\":35710,\"start\":35701},{\"end\":35725,\"start\":35720},{\"end\":35902,\"start\":35894},{\"end\":35915,\"start\":35912},{\"end\":35933,\"start\":35924},{\"end\":35945,\"start\":35941},{\"end\":35960,\"start\":35954},{\"end\":36231,\"start\":36217},{\"end\":36246,\"start\":36239},{\"end\":36267,\"start\":36255},{\"end\":36283,\"start\":36275},{\"end\":36293,\"start\":36290},{\"end\":36311,\"start\":36303},{\"end\":36325,\"start\":36318},{\"end\":36663,\"start\":36653},{\"end\":36683,\"start\":36672},{\"end\":36874,\"start\":36870},{\"end\":36891,\"start\":36885},{\"end\":36904,\"start\":36897},{\"end\":37160,\"start\":37156},{\"end\":37173,\"start\":37166},{\"end\":37420,\"start\":37410},{\"end\":37438,\"start\":37430},{\"end\":37446,\"start\":37440},{\"end\":37711,\"start\":37705},{\"end\":37721,\"start\":37719},{\"end\":37939,\"start\":37934},{\"end\":37949,\"start\":37947},{\"end\":37965,\"start\":37958},{\"end\":37972,\"start\":37967},{\"end\":38332,\"start\":38329},{\"end\":38659,\"start\":38655},{\"end\":38680,\"start\":38668},{\"end\":38701,\"start\":38694},{\"end\":38716,\"start\":38709},{\"end\":38731,\"start\":38725},{\"end\":38743,\"start\":38739},{\"end\":38757,\"start\":38750},{\"end\":39136,\"start\":39133},{\"end\":39147,\"start\":39143},{\"end\":39157,\"start\":39151},{\"end\":39173,\"start\":39167},{\"end\":39193,\"start\":39183},{\"end\":39209,\"start\":39202},{\"end\":39217,\"start\":39211},{\"end\":39591,\"start\":39580},{\"end\":39606,\"start\":39599},{\"end\":39621,\"start\":39613},{\"end\":39639,\"start\":39634},{\"end\":39654,\"start\":39648},{\"end\":39669,\"start\":39662},{\"end\":39680,\"start\":39671},{\"end\":39980,\"start\":39974},{\"end\":39995,\"start\":39990},{\"end\":40011,\"start\":40003},{\"end\":40027,\"start\":40021},{\"end\":40290,\"start\":40283},{\"end\":40306,\"start\":40297},{\"end\":40316,\"start\":40312},{\"end\":40332,\"start\":40325},{\"end\":40343,\"start\":40339},{\"end\":40749,\"start\":40740},{\"end\":40762,\"start\":40758},{\"end\":40774,\"start\":40770},{\"end\":41081,\"start\":41072},{\"end\":41093,\"start\":41089},{\"end\":41114,\"start\":41100},{\"end\":41121,\"start\":41116},{\"end\":41421,\"start\":41417},{\"end\":41437,\"start\":41430},{\"end\":41448,\"start\":41443},{\"end\":41458,\"start\":41450},{\"end\":41811,\"start\":41804},{\"end\":41824,\"start\":41818},{\"end\":41840,\"start\":41834},{\"end\":41855,\"start\":41847},{\"end\":41867,\"start\":41865},{\"end\":41884,\"start\":41874},{\"end\":42268,\"start\":42265},{\"end\":42281,\"start\":42274},{\"end\":42498,\"start\":42488},{\"end\":42514,\"start\":42508},{\"end\":42537,\"start\":42530},{\"end\":42769,\"start\":42764},{\"end\":42780,\"start\":42776},{\"end\":42946,\"start\":42942},{\"end\":42961,\"start\":42954},{\"end\":42977,\"start\":42971},{\"end\":42992,\"start\":42988},{\"end\":43001,\"start\":42994},{\"end\":43322,\"start\":43316},{\"end\":43338,\"start\":43332},{\"end\":43357,\"start\":43348},{\"end\":43372,\"start\":43366},{\"end\":43387,\"start\":43383},{\"end\":43393,\"start\":43389},{\"end\":43615,\"start\":43606},{\"end\":43621,\"start\":43617},{\"end\":43994,\"start\":43986},{\"end\":44209,\"start\":44192},{\"end\":44219,\"start\":44217},{\"end\":44235,\"start\":44225},{\"end\":44248,\"start\":44237},{\"end\":44523,\"start\":44520},{\"end\":44532,\"start\":44527},{\"end\":44551,\"start\":44548},{\"end\":44560,\"start\":44553},{\"end\":44819,\"start\":44810},{\"end\":44834,\"start\":44827},{\"end\":44845,\"start\":44843},{\"end\":45157,\"start\":45146},{\"end\":45174,\"start\":45168},{\"end\":45183,\"start\":45176},{\"end\":45375,\"start\":45368},{\"end\":45392,\"start\":45383},{\"end\":45408,\"start\":45402},{\"end\":45709,\"start\":45703},{\"end\":45719,\"start\":45711},{\"end\":45982,\"start\":45976},{\"end\":45998,\"start\":45990},{\"end\":46006,\"start\":46000},{\"end\":46240,\"start\":46238},{\"end\":46255,\"start\":46247},{\"end\":46269,\"start\":46265},{\"end\":46277,\"start\":46273},{\"end\":46290,\"start\":46288},{\"end\":46308,\"start\":46301},{\"end\":46324,\"start\":46316},{\"end\":46337,\"start\":46331},{\"end\":46346,\"start\":46343},{\"end\":46357,\"start\":46354},{\"end\":46367,\"start\":46359},{\"end\":46814,\"start\":46810},{\"end\":46827,\"start\":46820},{\"end\":47029,\"start\":47025},{\"end\":47041,\"start\":47038},{\"end\":47055,\"start\":47049},{\"end\":47071,\"start\":47064}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1409.0473\",\"id\":\"b0\"},\"end\":33556,\"start\":33245},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14288483},\"end\":34108,\"start\":33558},{\"attributes\":{\"doi\":\"arXiv:1601.06733\",\"id\":\"b2\"},\"end\":34371,\"start\":34110},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":133195},\"end\":34744,\"start\":34373},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":16992492},\"end\":35115,\"start\":34746},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1729177},\"end\":35631,\"start\":35117},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":9751546},\"end\":35885,\"start\":35633},{\"attributes\":{\"doi\":\"arXiv:1603.08148\",\"id\":\"b7\"},\"end\":36168,\"start\":35887},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6203757},\"end\":36622,\"start\":36170},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1915014},\"end\":36811,\"start\":36624},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":10151113},\"end\":37042,\"start\":36813},{\"attributes\":{\"id\":\"b11\"},\"end\":37317,\"start\":37044},{\"attributes\":{\"doi\":\"arXiv:1611.01462\",\"id\":\"b12\"},\"end\":37650,\"start\":37319},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b13\"},\"end\":37864,\"start\":37652},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3937849},\"end\":38262,\"start\":37866},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":964287},\"end\":38601,\"start\":38264},{\"attributes\":{\"doi\":\"arXiv:1603.06744\",\"id\":\"b16\"},\"end\":38992,\"start\":38603},{\"attributes\":{\"doi\":\"arXiv:1603.08023\",\"id\":\"b17\"},\"end\":39518,\"start\":38994},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14068874},\"end\":39931,\"start\":39520},{\"attributes\":{\"doi\":\"arXiv:1609.07843\",\"id\":\"b19\"},\"end\":40198,\"start\":39933},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":16447573},\"end\":40627,\"start\":40200},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6405271},\"end\":40988,\"start\":40629},{\"attributes\":{\"doi\":\"arXiv:1602.06023\",\"id\":\"b22\"},\"end\":41338,\"start\":40990},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1356305},\"end\":41723,\"start\":41340},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3631537},\"end\":42183,\"start\":41725},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":15593482},\"end\":42431,\"start\":42185},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1957433},\"end\":42702,\"start\":42433},{\"attributes\":{\"doi\":\"arXiv:1608.05859\",\"id\":\"b27\"},\"end\":42932,\"start\":42704},{\"attributes\":{\"doi\":\"arXiv:1511.06732\",\"id\":\"b28\"},\"end\":43258,\"start\":42934},{\"attributes\":{\"doi\":\"arXiv:1612.00563\",\"id\":\"b29\"},\"end\":43602,\"start\":43260},{\"attributes\":{\"doi\":\"arXiv:1509.00685\",\"id\":\"b30\"},\"end\":43914,\"start\":43604},{\"attributes\":{\"id\":\"b31\"},\"end\":44126,\"start\":43916},{\"attributes\":{\"doi\":\"arXiv:1608.02927\",\"id\":\"b32\"},\"end\":44445,\"start\":44128},{\"attributes\":{\"doi\":\"arXiv:1704.04368\",\"id\":\"b33\"},\"end\":44751,\"start\":44447},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":7961699},\"end\":45076,\"start\":44753},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":15080650},\"end\":45342,\"start\":45078},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":5692837},\"end\":45609,\"start\":45344},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":2332513},\"end\":45894,\"start\":45611},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":14711886},\"end\":46184,\"start\":45896},{\"attributes\":{\"doi\":\"arXiv:1609.08144\",\"id\":\"b39\",\"matched_paper_id\":3603249},\"end\":46741,\"start\":46186},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":16878278},\"end\":46955,\"start\":46743},{\"attributes\":{\"doi\":\"arXiv:1611.03382\",\"id\":\"b41\"},\"end\":47262,\"start\":46957}]", "bib_title": "[{\"end\":33614,\"start\":33558},{\"end\":34448,\"start\":34373},{\"end\":34832,\"start\":34746},{\"end\":35180,\"start\":35117},{\"end\":35693,\"start\":35633},{\"end\":36210,\"start\":36170},{\"end\":36646,\"start\":36624},{\"end\":36864,\"start\":36813},{\"end\":37926,\"start\":37866},{\"end\":38318,\"start\":38264},{\"end\":39576,\"start\":39520},{\"end\":40275,\"start\":40200},{\"end\":40731,\"start\":40629},{\"end\":41402,\"start\":41340},{\"end\":41793,\"start\":41725},{\"end\":42254,\"start\":42185},{\"end\":42478,\"start\":42433},{\"end\":44803,\"start\":44753},{\"end\":45139,\"start\":45078},{\"end\":45360,\"start\":45344},{\"end\":45699,\"start\":45611},{\"end\":45972,\"start\":45896},{\"end\":46228,\"start\":46186},{\"end\":46801,\"start\":46743}]", "bib_author": "[{\"end\":33334,\"start\":33316},{\"end\":33349,\"start\":33334},{\"end\":33364,\"start\":33349},{\"end\":33627,\"start\":33616},{\"end\":33640,\"start\":33627},{\"end\":33654,\"start\":33640},{\"end\":33662,\"start\":33654},{\"end\":33673,\"start\":33662},{\"end\":34179,\"start\":34163},{\"end\":34188,\"start\":34179},{\"end\":34204,\"start\":34188},{\"end\":34464,\"start\":34450},{\"end\":34478,\"start\":34464},{\"end\":34496,\"start\":34478},{\"end\":34505,\"start\":34496},{\"end\":34862,\"start\":34834},{\"end\":34875,\"start\":34862},{\"end\":34894,\"start\":34875},{\"end\":34905,\"start\":34894},{\"end\":35195,\"start\":35182},{\"end\":35208,\"start\":35195},{\"end\":35226,\"start\":35208},{\"end\":35712,\"start\":35695},{\"end\":35727,\"start\":35712},{\"end\":35904,\"start\":35887},{\"end\":35917,\"start\":35904},{\"end\":35935,\"start\":35917},{\"end\":35947,\"start\":35935},{\"end\":35962,\"start\":35947},{\"end\":36233,\"start\":36212},{\"end\":36248,\"start\":36233},{\"end\":36269,\"start\":36248},{\"end\":36285,\"start\":36269},{\"end\":36295,\"start\":36285},{\"end\":36313,\"start\":36295},{\"end\":36327,\"start\":36313},{\"end\":36665,\"start\":36648},{\"end\":36685,\"start\":36665},{\"end\":36876,\"start\":36866},{\"end\":36893,\"start\":36876},{\"end\":36906,\"start\":36893},{\"end\":37162,\"start\":37152},{\"end\":37175,\"start\":37162},{\"end\":37422,\"start\":37400},{\"end\":37440,\"start\":37422},{\"end\":37448,\"start\":37440},{\"end\":37713,\"start\":37696},{\"end\":37723,\"start\":37713},{\"end\":37941,\"start\":37928},{\"end\":37951,\"start\":37941},{\"end\":37967,\"start\":37951},{\"end\":37974,\"start\":37967},{\"end\":38334,\"start\":38320},{\"end\":38661,\"start\":38650},{\"end\":38682,\"start\":38661},{\"end\":38703,\"start\":38682},{\"end\":38718,\"start\":38703},{\"end\":38733,\"start\":38718},{\"end\":38745,\"start\":38733},{\"end\":38759,\"start\":38745},{\"end\":39138,\"start\":39124},{\"end\":39149,\"start\":39138},{\"end\":39159,\"start\":39149},{\"end\":39175,\"start\":39159},{\"end\":39195,\"start\":39175},{\"end\":39211,\"start\":39195},{\"end\":39219,\"start\":39211},{\"end\":39593,\"start\":39578},{\"end\":39608,\"start\":39593},{\"end\":39623,\"start\":39608},{\"end\":39641,\"start\":39623},{\"end\":39656,\"start\":39641},{\"end\":39671,\"start\":39656},{\"end\":39682,\"start\":39671},{\"end\":39982,\"start\":39966},{\"end\":39997,\"start\":39982},{\"end\":40013,\"start\":39997},{\"end\":40029,\"start\":40013},{\"end\":40292,\"start\":40277},{\"end\":40308,\"start\":40292},{\"end\":40318,\"start\":40308},{\"end\":40334,\"start\":40318},{\"end\":40345,\"start\":40334},{\"end\":40751,\"start\":40733},{\"end\":40764,\"start\":40751},{\"end\":40776,\"start\":40764},{\"end\":41083,\"start\":41065},{\"end\":41095,\"start\":41083},{\"end\":41116,\"start\":41095},{\"end\":41123,\"start\":41116},{\"end\":41423,\"start\":41404},{\"end\":41439,\"start\":41423},{\"end\":41450,\"start\":41439},{\"end\":41460,\"start\":41450},{\"end\":41813,\"start\":41795},{\"end\":41826,\"start\":41813},{\"end\":41842,\"start\":41826},{\"end\":41857,\"start\":41842},{\"end\":41869,\"start\":41857},{\"end\":41886,\"start\":41869},{\"end\":42270,\"start\":42256},{\"end\":42283,\"start\":42270},{\"end\":42500,\"start\":42480},{\"end\":42516,\"start\":42500},{\"end\":42539,\"start\":42516},{\"end\":42771,\"start\":42759},{\"end\":42782,\"start\":42771},{\"end\":42948,\"start\":42934},{\"end\":42963,\"start\":42948},{\"end\":42979,\"start\":42963},{\"end\":42994,\"start\":42979},{\"end\":43003,\"start\":42994},{\"end\":43324,\"start\":43314},{\"end\":43340,\"start\":43324},{\"end\":43359,\"start\":43340},{\"end\":43374,\"start\":43359},{\"end\":43389,\"start\":43374},{\"end\":43395,\"start\":43389},{\"end\":43617,\"start\":43604},{\"end\":43623,\"start\":43617},{\"end\":43996,\"start\":43981},{\"end\":44211,\"start\":44185},{\"end\":44221,\"start\":44211},{\"end\":44237,\"start\":44221},{\"end\":44250,\"start\":44237},{\"end\":44525,\"start\":44512},{\"end\":44534,\"start\":44525},{\"end\":44553,\"start\":44534},{\"end\":44562,\"start\":44553},{\"end\":44821,\"start\":44805},{\"end\":44836,\"start\":44821},{\"end\":44847,\"start\":44836},{\"end\":45159,\"start\":45141},{\"end\":45176,\"start\":45159},{\"end\":45185,\"start\":45176},{\"end\":45377,\"start\":45362},{\"end\":45394,\"start\":45377},{\"end\":45410,\"start\":45394},{\"end\":45711,\"start\":45701},{\"end\":45721,\"start\":45711},{\"end\":45984,\"start\":45974},{\"end\":46000,\"start\":45984},{\"end\":46008,\"start\":46000},{\"end\":46242,\"start\":46230},{\"end\":46257,\"start\":46242},{\"end\":46271,\"start\":46257},{\"end\":46279,\"start\":46271},{\"end\":46292,\"start\":46279},{\"end\":46310,\"start\":46292},{\"end\":46326,\"start\":46310},{\"end\":46339,\"start\":46326},{\"end\":46348,\"start\":46339},{\"end\":46359,\"start\":46348},{\"end\":46369,\"start\":46359},{\"end\":46816,\"start\":46803},{\"end\":46829,\"start\":46816},{\"end\":47031,\"start\":47017},{\"end\":47043,\"start\":47031},{\"end\":47057,\"start\":47043},{\"end\":47073,\"start\":47057}]", "bib_venue": "[{\"end\":33860,\"start\":33775},{\"end\":34556,\"start\":34539},{\"end\":35337,\"start\":35290},{\"end\":38419,\"start\":38403},{\"end\":33314,\"start\":33245},{\"end\":33773,\"start\":33673},{\"end\":34161,\"start\":34110},{\"end\":34537,\"start\":34505},{\"end\":34914,\"start\":34905},{\"end\":35288,\"start\":35226},{\"end\":35742,\"start\":35727},{\"end\":36004,\"start\":35978},{\"end\":36376,\"start\":36327},{\"end\":36703,\"start\":36685},{\"end\":36911,\"start\":36906},{\"end\":37150,\"start\":37044},{\"end\":37398,\"start\":37319},{\"end\":37694,\"start\":37652},{\"end\":38049,\"start\":37974},{\"end\":38401,\"start\":38334},{\"end\":38648,\"start\":38603},{\"end\":39122,\"start\":38994},{\"end\":39709,\"start\":39682},{\"end\":39964,\"start\":39933},{\"end\":40394,\"start\":40345},{\"end\":40779,\"start\":40776},{\"end\":41063,\"start\":40990},{\"end\":41506,\"start\":41460},{\"end\":41935,\"start\":41886},{\"end\":42292,\"start\":42283},{\"end\":42544,\"start\":42539},{\"end\":42757,\"start\":42704},{\"end\":43073,\"start\":43019},{\"end\":43312,\"start\":43260},{\"end\":43740,\"start\":43639},{\"end\":43979,\"start\":43916},{\"end\":44183,\"start\":44128},{\"end\":44510,\"start\":44447},{\"end\":44896,\"start\":44847},{\"end\":45189,\"start\":45185},{\"end\":45459,\"start\":45410},{\"end\":45737,\"start\":45721},{\"end\":46026,\"start\":46008},{\"end\":46439,\"start\":46385},{\"end\":46833,\"start\":46829},{\"end\":47015,\"start\":46957}]"}}}, "year": 2023, "month": 12, "day": 17}