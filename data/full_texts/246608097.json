{"id": 246608097, "updated": "2023-10-05 16:57:11.572", "metadata": {"title": "From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer", "authors": "[{\"first\":\"Xin\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Ningyu\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Zhoubo\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Shumin\",\"last\":\"Deng\",\"middle\":[]},{\"first\":\"Hui\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Feiyu\",\"last\":\"Xiong\",\"middle\":[]},{\"first\":\"Mosha\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Huajun\",\"last\":\"Chen\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Knowledge graph completion aims to address the problem of extending a KG with missing triples. In this paper, we provide an approach GenKGC, which converts knowledge graph completion to sequence-to-sequence generation task with the pre-trained language model. We further introduce relation-guided demonstration and entity-aware hierarchical decoding for better representation learning and fast inference. Experimental results on three datasets show that our approach can obtain better or comparable performance than baselines and achieve faster inference speed compared with previous methods with pre-trained language models. We also release a new large-scale Chinese knowledge graph dataset AliopenKG500 for research purpose. Code and datasets are available in https://github.com/zjunlp/PromptKG/tree/main/GenKGC.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2202.02113", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/www/XieZLDCXCC22", "doi": "10.1145/3487553.3524238"}}, "content": {"source": {"pdf_hash": "03d9b59c84d648007d05fe8d50cdb20e0349b333", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2202.02113v7.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2202.02113", "status": "GREEN"}}, "grobid": {"id": "53e0e4f7803a3ac40c1839fa95704e2d1729aa4a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/03d9b59c84d648007d05fe8d50cdb20e0349b333.txt", "contents": "\nFrom Discrimination to Generation: Knowledge Graph Completion with Generative Transformer\nApril 25-29, 2022\n\nXin Xie \nNingyu Zhang zhangningyu@zju.edu.cn \nZhoubo Li zhoubo.li@zju.edu.cn \nShumin Deng \nHui Chen \nFeiyu Xiong feiyu.xfy@alibaba-inc.com \nMosha Chen chenmosha.cms@alibaba-inc.com \nHuajun Chen huajunsir@zju.edu.cn \nXin Xie \nNingyu Zhang \nZhoubo Li \nShumin Deng \nHui Chen \nFeiyu Xiong \n\nAZFT Joint Lab for Knowledge Engine Hangzhou\nZhejiang University\nChina\n\n\nZhejiang University & AZFT Joint Lab for Knowledge Engine Hangzhou\nChina\n\n\nZhejiang University & AZFT Joint Lab for Knowledge Engine Hangzhou\nChina\n\n\nAlibaba Group Hangzhou\nChina\n\n\nAlibaba Group Hangzhou\nChina\n\n\nZhejiang University & AZFT Joint Lab for Knowledge Engine Hangzhou\nChina\n\nFrom Discrimination to Generation: Knowledge Graph Completion with Generative Transformer\n\nVirtual Event\nLyon, FranceApril 25-29, 202210.1145/3487553.3524238ACM Reference Format: Mosha Chen, and Huajun Chen. 2022. From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer. In Companion Proceedings of the Web Conference 2022 (WWW '22 Companion), April 25-29, 2022, Virtual Event, Lyon, France. ACM, New York, NY, USA, 4 pages. ACM ISBN 978-1-4503-9130-6/22/04. . . $15.00CCS CONCEPTS \u2022 Computing methodologies \u2192 Knowledge representation and reasoning KEYWORDS Knowledge Graph CompletionGenerationTransformer\nKnowledge graph completion aims to address the problem of extending a KG with missing triples. In this paper, we provide an approach GenKGC, which converts knowledge graph completion to sequence-to-sequence generation task with the pre-trained language model. We further introduce relation-guided demonstration and entity-aware hierarchical decoding for better representation learning and fast inference. Experimental results on three datasets show that our approach can obtain better or comparable performance than baselines and achieve faster inference speed compared with previous methods with pre-trained language models. We also release a new large-scale Chinese knowledge graph dataset OpenBG500 for research purpose 1 . relation) pair. While our method only needs to generate the topentities with entity-aware hierarchical decoding, which reduces lots of computing resources.\n\nINTRODUCTION\n\nKnowledge Graphs (KGs) treat the knowledge in the real world as fact triples in the form of <subject, predicate, object>, abridged as ( , , ), where and denote entities and are the relations between entities, which can benefit a wide range of knowledgeintensive tasks. Knowledge graph completion (KGC) aims to complete the knowledge graph by predicting the missing triples. In this paper, we mainly target link prediction task for KGC based on the powerful pre-trained language models.\n\nMost previous KG completion methods, such as TransE [2], Com-plEx [11], and RotatE [9], are knowledge embedding techniques that embed the entities and relations into a vector space and then obtain the predicted triples by leveraging pre-defined scoring functions to those vectors. Recently, some textual encoding methods (e.g., KG-BERT [14]) have been proposed, which utilize the pretrained language model to encode triples and output the score for each candidate. Obviously, most previous approaches leverage the discrimination paradigm with a pre-defined scoring function to knowledge embeddings. However, such a discrimination strategy has to costly scoring of all possible triples in inference and suffer from the instability of negative sampling. Moreover, those dense knowledge embedding approaches (e.g., TransE) ignore the finegrained interactions between entities and relations and have to allocate a large memory footprint for the large-scale real-world knowledge graph. Therefore, it is intuitive to find a new technical solution for knowledge graph completion.\n\nIn this paper, we take the first step to model the knowledge graph completion with sequence to sequence generation and propose a novel approach GenKGC. To be specific, we represent entities and relations as input sequences and utilize the pre-trained language model to generate target entities. Following GPT-3's naive \"in-context learning\" paradigm, in which the model can learn correct output answers by concatenating the selected samples relevant to the input, we propose relation-guided demonstration by adding triples of the same relation. Moreover, during generation, we propose entity-aware hierarchical decoding to reduce the time complexity of generation. Experimental results on two datasets WN18RR, FB15k-237 and a newly released large-scale Chinese KG dataset OpenBG500 demonstrate the effectiveness of the proposed approach. The contributions of our work are as follows:  Figure 1: Architecture of GenKGC. We augment the input text of entity and relation with demonstrations, and introduce entity-aware hierarchical decoding for fast inference.\n\n\u2022 We convert link prediction to sequence to sequence generation and propose GenKGC, which can reduce the inference time while maintaining the performance. \u2022 We propose relation-guided demonstration and entity-aware hierarchical decoding, which can better represent entities and relations and reduce the time complexity of generation. \u2022 We report results on two datasets and release a new largescale KG dataset, OpenBG500, for research purposes.\n\n\nMETHOD 2.1 Link Prediction as Seq2Seq Generation\n\nKnowledge graph is defined with entity categories and entity descriptions as a tuple G = (E, R, T , C, D), where E represents a set of entities, R represents relation types, T refers to a set of triples , C refers to the entity categories and D refers to the entity descriptions. For each triple \u2208 T , there is the form ( , , ) where , \u2208 E is the head and tail entity respectively. For each entity \u2208 E, there exists category to define the and a text to describe . To complete missing triples in KGs, link prediction aims at predicting the tail entity given the head entity and the query relation, denoted by ( , , ?).\n\nIn this paper, we utilize the standard encoder-decoder architecture for sequence-to-sequence generation. Note that we regard each entity and relation as the sequence of tokens. Concretely, we follow [14] to use plain text to represent the entities and relations instead of unique embedding to bridge the gap between the triples in the knowledge graph and pre-trained language models. To be specific, given a triple with tail entity missing ( , , ?), we obtain the description of and respectively, and concatenate them together. For example, as shown in Figure 1, \"Michael Chabon studied at\" (head entity, relation) is the major part of the input sequence, and \"UC, Irvine\" (target entity) is part of the output sequence. Thus, we have the input sequence of < , > pair and generate the output sequence of . We leverage BART for model training and inference. \n( + 1) 0.08ms KG-BERT (| | 2 \u00d7 ( + 1)) 72ms GenKGC (| | 2 ) 2.35ms Inference TransE (|E |) 0.02s KG-BERT (| | 2 \u00d7 |E |) 10100s GenKGC (| | 2 \u00d7 | | ) 0.96s\n\nRelation-guided Demonstration\n\nInspired by prompt-tuning [18], we propose relation-guided demonstration for the encoder. Note that there exist long-tailed distributions in the KGs, for example, there exist only 37 instances of the relation film/type_of_appearance in the FB15k-237 dataset.\n\nPrevious study [18] illustrates that concatenating randomly sampled instances as demonstrations can yield better few-shot performance. Thus, we construct relation-guided demonstration examples { in , train }. To be specific, we sample those demonstrations with the guidance of relation , which consists of several triples with the same relation of input from training set. Formally, we have the final input sequence as:\n\n= <bos> demonstration( ) <sep> <sep>\n\n\nEntity-aware Hierarchical Decoding\n\nIn the vanilla decoding setting, we have to enumerate all entities in the E and then sort them by the score function. However, this process can be time-consuming, as shown in Table 1 when E is very large, it is costly scoring of all possible triples. In our approach, we follow [3] to use the Beam Search to obtain top-entities in the E ( is the hyperparameter of beam size). Intuitively, there is no need for negative sampling as we directly optimize by predicting the correct To be more specific, given with a triple with tail or head entity missing ( , , ?), GenKGC rank each \u2208 E by computing a score with an autoregressive formulation:\n( | ) = | | =1 ( | < , ) =| |+1 ( | < , ) ,(1)\nwhere is the set of | | tokens in the category , is the set of tokens in the textual representation of .\n\nSince KG contains rich semantic information such as entity types, it is intuitive to constrain the decoding for fast inference. We sample the type category with the lowest frequency of occurrence in the training set to constrain the entity decoding since it is challenging to discriminate those low-frequent entities. Then, we add special tokens as types in the vocabulary of pre-trained language model to constrain the decoding. To ensure that the generated entities are inside the entity candidate set, we construct a prefix tree (trie tree) to decode our entity name as shown in Figure 1. Similar to the ordinary sequence-to-sequence model, we optimize GenKGC using a standard seq2seq objective function:\nL = \u2212 log ( | )(2)\n\nEXPERIMENT\n\nDatasets. We evaluate our method on FB15k-237 [10], WN18RR [5], which are widely used in the link prediction literature, and a new real-world large-scale Chinese KG dataset OpenBG500 2 . In FB15k-237, descriptions of entities are obtained from the introduction section of the Wikipedia page of each entity. In WN18RR, each entity corresponds to a word sense, and description is the word definition. In OpenBG500, the descriptions of entities and relations come from the e-commerce description page. More details about datasets are listed in Table 3. 2 OpenBG500 is a subset of open business KG from https://kg.alibaba.com/. Setting. We adopt BART-base as our backbone for comparison with other BERT-based KGC methods like KG-BERT. Following StAR [12], we choose the two kinds of KGE methods as our baseline models, which can be classified as graph embedding approach and textual encoding approach. Grid search is used over the three datasets and the results are reported on the test set with hyperparameters of the best performance determined by the dev set.\n\nMetrics. We evaluate the test set of triples T \u2032 disjoint from the set of training triples T . For inference on a test triple ( , , ), we make sure predict the entity in the candidate set E. We use the metrics of hits@1, hits@3 and hits@10.\n\nComparison with Discrimination Method. From Table 2, we notice that GenKGC achieves better performance than KG-BERT [14] across all datasets and maintains high speed during inference. The translation-based method like TransE, which treats entities and relations as dense vectors in the same space, will face the memory explosion problem. For the memory cost, TransE has to consume 260M parameters to store the entities and relations for OpenBG500 with more than 260k entities, while pre-trained models (BERT or BART) only utilize 110M parameters, which demonstrates the memory efficiency of our approach. Note that this problem will be more severe when the entities become more numerous because the space complexity is ( ). For the inference speed, KG-BERT encodes the relational triples with the pre-trained language model and ranks all candidate triples with correct and wrong entities for inference. When the candidate entities set is huge, it is time-consuming for inference; for example, as shown in Table 1, KG-BERT takes about 100100s to the scoring of all possible triples given a single (entity, Table 4: We list a query and first five entities with their probability predicted by GenKGC w/o entity-aware decoding, and its reranking with GenKGC.\n\n\nCase Study\n\nFor different decoding strategies we conduct case study to analyze the result. For GenKGC w/o hierarchical decoding, we utilize the normal beam search to decode the text name of the missing entity. From Table 4, we observe that GenKGC obtain better entity generation results while in normal beam search, the model may stop early at correct but not precise enough answer. We argue that this is caused by the bias of the pre-trained language model (e.g., common token bias) since high-frequent tokens will lead the pre-trained language model to be biased toward certain answers. Our entityaware hierarchical decoding can constrain the decoding process and mitigate the bias effect caused by pre-trained language models.\n\n\nRELATED WORK\n\nKnowledge Graph Embedding Models. There are lots of methods of KGC are based on knowledge graph embeddings (KGE), which generally leverage an embedding vector in the continuous embedding space to represent the entity and the relation in KG [17]. One kind of KGE methods is translation-based, such as TransE [2], ConE [20], TotatE [9], which consider relations as the mapping function between entities.The other kind of KGE method is semantic matching models, where they get the semantic similarity by using the multi-linear or bilinear product.\n\nPre-trained Language Models for KGC. Recently, since pre-trained language models, such as BERT [6], have shown significant improvement on several natural language processing tasks, several works use the transformer-based models to tackle the KGC problems [15,16,19]. KG-BERT [14] first propose to use BERT for KGC by seeing a triple as a sequence and converts KGC into a sequence classification problem with the binary cross-entropy object. [7] proposes to use a transformer encoder-decoder model that takes plain text as input and output a structured triple of the information hide in it.\n\n\nCONCLUSION\n\nIn this paper, we propose GenKGC, which can reach comparable results while reducing inference and training cost in link prediction with pre-trained models. Experimental results on three benchmark datasets demonstrate the effectiveness of our approach, especially in inference time.\n\n\narXiv:2202.02113v7 [cs.CL] 14 Mar 2023Pre-trained Encoder \nPre-trained Decoder \n\nMichael Chabon studied at \n\nTriple:<Michael Chabon,study at,?> \n\n[Univ] \n\nType \nDecode \n\nEntity \nDecode \n\ndemonstration \n\n<bos> \n\n[Univ] \n\n<eos> \n\n<bos> \n\n[LOC] \n[ORG] \n[Univ] \n\n... \n\nIrvine \nDavis \n\n... \n\nUC \n\nSan \n\n... \n\n<eos> <eos> <eos> \n\nHarvard University \n\nUC \n\nUC \n\nIrvine \n\nD \n\nObama studied at Harvard. \nYau studied at, UC Berkeley. \n\nIrvine \n\n\n\nTable 1 :\n1Inference and training efficiency comparison. | | is the length of the entity description. |E | is the numbers of all unique entities in the KG. is the negative samples for KG-BERT and the beam size for our GenKGC . The time under RTX 3090 is used to estimate the speed of training and inference given a single (entity,relation) pair on OpenBG500.For One Triple Method \nComplexity \nTime under RTX 3090 \n\nTraining \n\nTransE \n\n\nTable 2 :\n2Experiment results on WN18RR, FB15k-237 and OpenBG500. \u22c4Resulting numbers are reported by[8], we reproduce the model result on OpenBG500 and take other results from the original papers.WN18RR \nFB15k-237 \nOpenBG500 \n\nMethod \nHits@1 Hits@3 Hits@10 Hits@1 Hits@3 Hits@10 Hits@1 Hits@3 Hits@10 \n\nGraph embedding approach \n\nTransE [2] \u22c4 \n0.043 \n0.441 \n0.532 \n0.198 \n0.376 \n0.441 \n0.207 \n0.340 \n0.513 \nDistMult [13] \u22c4 \n0.412 \n0.470 \n0.504 \n0.199 \n0.301 \n0.446 \n0.049 \n0.088 \n0.216 \nComplEx [11] \u22c4 \n0.409 \n0.469 \n0.530 \n0.194 \n0.297 \n0.450 \n0.053 \n0.120 \n0.266 \nRotatE [9] \n0.428 \n0.492 \n0.571 \n0.241 \n0.375 \n0.533 \n-\n-\n-\nTuckER [1] \n0.443 \n0.482 \n0.526 \n0.226 \n0.394 \n0.544 \n-\n-\n-\nATTH [4] \n0.443 \n0.499 \n0.486 \n0.252 \n0.384 \n0.549 \n-\n-\n-\n\nTextual encoding approach \n\nKG-BERT [14] \n0.041 \n0.302 \n0.524 \n-\n-\n0.420 \n0.023 \n0.049 \n0.241 \nStAR [12] \n0.243 \n0.491 \n0.709 \n0.205 \n0.322 \n0.482 \n-\n-\n-\n\nGenKGC \n0.287 \n0.403 \n0.535 \n0.192 \n0.355 \n0.439 \n0.203 \n0.280 \n0.351 \n\nentity in decoding. \n\nTable 3 :\n3Summary statistics of benchmark datasets.Dataset \n# Ent # Rel # Train # Dev # Test \n\nWN18RR \n40,943 \n11 \n86,835 \n3,034 \n3,134 \nFB15k-237 \n14,541 \n237 \n272,115 17,535 20,466 \nOpenBG500 269,658 500 1,242,550 5,000 \n5,000 \n\n\nACKNOWLEDGMENTS\nTuckER: Tensor Factorization for Knowledge Graph Completion. Ivana Balazevic, Carl Allen, Timothy M Hospedales, Proc. of EMNLP. of EMNLPIvana Balazevic, Carl Allen, and Timothy M. Hospedales. 2019. TuckER: Tensor Factorization for Knowledge Graph Completion. In Proc. of EMNLP.\n\nTranslating embeddings for modeling multi-relational data. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, Oksana Yakhnenko, Proc. of NeurIPS. of NeurIPSAntoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Ok- sana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In Proc. of NeurIPS.\n\nAutoregressive Entity Retrieval. Nicola De Cao, Gautier Izacard, Sebastian Riedel, Fabio Petroni, Proc. of ICLR. of ICLRNicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2021. Au- toregressive Entity Retrieval. In Proc. of ICLR.\n\nLow-Dimensional Hyperbolic Knowledge Graph Embeddings. Ines Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi, Christopher R\u00e9, Proc. of ACL. of ACLInes Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi, and Christo- pher R\u00e9. 2020. Low-Dimensional Hyperbolic Knowledge Graph Embeddings. In Proc. of ACL.\n\nConvolutional 2D Knowledge Graph Embeddings. Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel, Proc. of AAAI. of AAAITim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018. Convolutional 2D Knowledge Graph Embeddings. In Proc. of AAAI.\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proc. of NAACL. of NAACLJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proc. of NAACL.\n\nGenIE: Generative Information Extraction. Martin Josifoski, Nicola De Cao, Maxime Peyrard, Robert West, CoRRMartin Josifoski, Nicola De Cao, Maxime Peyrard, and Robert West. 2021. GenIE: Generative Information Extraction. CoRR (2021).\n\nLearning Attention-based Embeddings for Relation Prediction in Knowledge Graphs. Deepak Nathani, Jatin Chauhan, Charu Sharma, Manohar Kaul, Proc. of ACL. of ACLDeepak Nathani, Jatin Chauhan, Charu Sharma, and Manohar Kaul. 2019. Learn- ing Attention-based Embeddings for Relation Prediction in Knowledge Graphs. In Proc. of ACL.\n\nRotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space. Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, Jian Tang, ICLR. Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019. RotatE: Knowl- edge Graph Embedding by Relational Rotation in Complex Space. In ICLR.\n\nRepresenting Text for Joint Embedding of Text and Knowledge Bases. Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, Michael Gamon, Proc. of EMNLP. of EMNLPKristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choud- hury, and Michael Gamon. 2015. Representing Text for Joint Embedding of Text and Knowledge Bases. In Proc. of EMNLP.\n\nComplex Embeddings for Simple Link Prediction. Th\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, Guillaume Bouchard, Proc. of ICML. of ICMLTh\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, and Guillaume Bouchard. 2016. Complex Embeddings for Simple Link Prediction. In Proc. of ICML.\n\nStructure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion. Bo Wang, Tao Shen, Guodong Long, Tianyi Zhou, Ying Wang, Yi Chang, Proc. of WWW. of WWWBo Wang, Tao Shen, Guodong Long, Tianyi Zhou, Ying Wang, and Yi Chang. 2021. Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion. In Proc. of WWW.\n\nEmbedding Entities and Relations for Learning and Inference in Knowledge Bases. Bishan Yang, Wen-Tau Yih, Xiaodong He, Jianfeng Gao, Li Deng, Proc. of ICLR. of ICLRBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Em- bedding Entities and Relations for Learning and Inference in Knowledge Bases. In Proc. of ICLR.\n\nKG-BERT: BERT for Knowledge Graph Completion. Liang Yao, Chengsheng Mao, Yuan Luo, CoRRLiang Yao, Chengsheng Mao, and Yuan Luo. 2019. KG-BERT: BERT for Knowledge Graph Completion. CoRR (2019).\n\nOntology-enhanced Prompt-tuning for Fewshot Learning. Hongbin Ye, Ningyu Zhang, Shumin Deng, Xiang Chen, Hui Chen, Feiyu Xiong, Xi Chen, Huajun Chen, Proc. of WWW. of WWWHongbin Ye, Ningyu Zhang, Shumin Deng, Xiang Chen, Hui Chen, Feiyu Xiong, Xi Chen, and Huajun Chen. 2022. Ontology-enhanced Prompt-tuning for Few- shot Learning. In Proc. of WWW.\n\nOntoProtein: Protein Pretraining With Gene Ontology Embedding. Ningyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan Cheng, Haosen Hong, Shumin Deng, Jiazhang Lian, Qiang Zhang, Huajun Chen, Proc. of ICLR. of ICLRNingyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan Cheng, Haosen Hong, Shumin Deng, Jiazhang Lian, Qiang Zhang, and Huajun Chen. 2022. OntoProtein: Protein Pretraining With Gene Ontology Embedding. In Proc. of ICLR.\n\nRelation Adversarial Network for Low resource Knowledge Graph Completion. Ningyu Zhang, Shumin Deng, Zhanlin Sun, Jiaoyan Chen, Wei Zhang, Huajun Chen, Proc. of WWW. of WWWNingyu Zhang, Shumin Deng, Zhanlin Sun, Jiaoyan Chen, Wei Zhang, and Huajun Chen. 2020. Relation Adversarial Network for Low resource Knowledge Graph Completion. In Proc. of WWW.\n\nDifferentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners. Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, Huajun Chen, Proc. of ICLR. of ICLRNingyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, and Huajun Chen. 2022. Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners. In Proc. of ICLR.\n\nReasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings. Ningyu Zhang, Xin Xie, Xiang Chen, Shumin Deng, Chuanqi Tan, Fei Huang, Xu Cheng, Huajun Chen, arXiv:2201.05575Ningyu Zhang, Xin Xie, Xiang Chen, Shumin Deng, Chuanqi Tan, Fei Huang, Xu Cheng, and Huajun Chen. 2022. Reasoning Through Memorization: Near- est Neighbor Knowledge Graph Embeddings. CoRR abs/2201.05575 (2022). arXiv:2201.05575 https://arxiv.org/abs/2201.05575\n\nConE: Cone Embeddings for Multi-Hop Reasoning over Knowledge Graphs. Zhanqiu Zhang, Jie Wang, Jiajun Chen, Shuiwang Ji, Feng Wu, Proc. of NeurIPS. of NeurIPSZhanqiu Zhang, Jie Wang, Jiajun Chen, Shuiwang Ji, and Feng Wu. 2021. ConE: Cone Embeddings for Multi-Hop Reasoning over Knowledge Graphs. Proc. of NeurIPS (2021).\n", "annotations": {"author": "[{\"end\":118,\"start\":110},{\"end\":155,\"start\":119},{\"end\":187,\"start\":156},{\"end\":200,\"start\":188},{\"end\":210,\"start\":201},{\"end\":249,\"start\":211},{\"end\":291,\"start\":250},{\"end\":325,\"start\":292},{\"end\":334,\"start\":326},{\"end\":348,\"start\":335},{\"end\":359,\"start\":349},{\"end\":372,\"start\":360},{\"end\":382,\"start\":373},{\"end\":395,\"start\":383},{\"end\":468,\"start\":396},{\"end\":543,\"start\":469},{\"end\":618,\"start\":544},{\"end\":649,\"start\":619},{\"end\":680,\"start\":650},{\"end\":755,\"start\":681}]", "publisher": null, "author_last_name": "[{\"end\":117,\"start\":114},{\"end\":131,\"start\":126},{\"end\":165,\"start\":163},{\"end\":199,\"start\":195},{\"end\":209,\"start\":205},{\"end\":222,\"start\":217},{\"end\":260,\"start\":256},{\"end\":303,\"start\":299},{\"end\":333,\"start\":330},{\"end\":347,\"start\":342},{\"end\":358,\"start\":356},{\"end\":371,\"start\":367},{\"end\":381,\"start\":377},{\"end\":394,\"start\":389}]", "author_first_name": "[{\"end\":113,\"start\":110},{\"end\":125,\"start\":119},{\"end\":162,\"start\":156},{\"end\":194,\"start\":188},{\"end\":204,\"start\":201},{\"end\":216,\"start\":211},{\"end\":255,\"start\":250},{\"end\":298,\"start\":292},{\"end\":329,\"start\":326},{\"end\":341,\"start\":335},{\"end\":355,\"start\":349},{\"end\":366,\"start\":360},{\"end\":376,\"start\":373},{\"end\":388,\"start\":383}]", "author_affiliation": "[{\"end\":467,\"start\":397},{\"end\":542,\"start\":470},{\"end\":617,\"start\":545},{\"end\":648,\"start\":620},{\"end\":679,\"start\":651},{\"end\":754,\"start\":682}]", "title": "[{\"end\":90,\"start\":1},{\"end\":845,\"start\":756}]", "venue": "[{\"end\":860,\"start\":847}]", "abstract": "[{\"end\":2278,\"start\":1396}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2836,\"start\":2833},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2851,\"start\":2847},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2867,\"start\":2864},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3121,\"start\":3117},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6233,\"start\":6229},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7105,\"start\":7101},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7354,\"start\":7350},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8112,\"start\":8109},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9414,\"start\":9410},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9426,\"start\":9423},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10114,\"start\":10110},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10786,\"start\":10782},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12913,\"start\":12909},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12979,\"start\":12976},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12990,\"start\":12986},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13002,\"start\":12999},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13313,\"start\":13310},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13474,\"start\":13470},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13477,\"start\":13474},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13480,\"start\":13477},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13494,\"start\":13490},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13659,\"start\":13656},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15079,\"start\":15076}]", "figure": "[{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":14538,\"start\":14101},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":14974,\"start\":14539},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":15968,\"start\":14975},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":16202,\"start\":15969}]", "paragraph": "[{\"end\":2779,\"start\":2294},{\"end\":3853,\"start\":2781},{\"end\":4912,\"start\":3855},{\"end\":5358,\"start\":4914},{\"end\":6028,\"start\":5411},{\"end\":6887,\"start\":6030},{\"end\":7333,\"start\":7075},{\"end\":7754,\"start\":7335},{\"end\":7792,\"start\":7756},{\"end\":8470,\"start\":7831},{\"end\":8622,\"start\":8518},{\"end\":9331,\"start\":8624},{\"end\":10422,\"start\":9364},{\"end\":10664,\"start\":10424},{\"end\":11920,\"start\":10666},{\"end\":12652,\"start\":11935},{\"end\":13213,\"start\":12669},{\"end\":13804,\"start\":13215},{\"end\":14100,\"start\":13819}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7042,\"start\":6888},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8517,\"start\":8471},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9350,\"start\":9332}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":8013,\"start\":8006},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":9913,\"start\":9905},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":10717,\"start\":10710},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":11678,\"start\":11671},{\"end\":11778,\"start\":11771},{\"end\":12145,\"start\":12138}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2292,\"start\":2280},{\"attributes\":{\"n\":\"2\"},\"end\":5409,\"start\":5361},{\"attributes\":{\"n\":\"2.2\"},\"end\":7073,\"start\":7044},{\"attributes\":{\"n\":\"2.3\"},\"end\":7829,\"start\":7795},{\"attributes\":{\"n\":\"3\"},\"end\":9362,\"start\":9352},{\"attributes\":{\"n\":\"3.1\"},\"end\":11933,\"start\":11923},{\"attributes\":{\"n\":\"4\"},\"end\":12667,\"start\":12655},{\"attributes\":{\"n\":\"5\"},\"end\":13817,\"start\":13807},{\"end\":14549,\"start\":14540},{\"end\":14985,\"start\":14976},{\"end\":15979,\"start\":15970}]", "table": "[{\"end\":14538,\"start\":14141},{\"end\":14974,\"start\":14898},{\"end\":15968,\"start\":15172},{\"end\":16202,\"start\":16022}]", "figure_caption": "[{\"end\":14141,\"start\":14103},{\"end\":14898,\"start\":14551},{\"end\":15172,\"start\":14987},{\"end\":16022,\"start\":15981}]", "figure_ref": "[{\"end\":4748,\"start\":4740},{\"end\":6591,\"start\":6583},{\"end\":9214,\"start\":9206}]", "bib_author_first_name": "[{\"end\":16285,\"start\":16280},{\"end\":16301,\"start\":16297},{\"end\":16316,\"start\":16309},{\"end\":16318,\"start\":16317},{\"end\":16564,\"start\":16557},{\"end\":16580,\"start\":16573},{\"end\":16597,\"start\":16590},{\"end\":16617,\"start\":16612},{\"end\":16632,\"start\":16626},{\"end\":16891,\"start\":16885},{\"end\":16907,\"start\":16900},{\"end\":16926,\"start\":16917},{\"end\":16940,\"start\":16935},{\"end\":17160,\"start\":17156},{\"end\":17172,\"start\":17168},{\"end\":17187,\"start\":17179},{\"end\":17202,\"start\":17194},{\"end\":17215,\"start\":17209},{\"end\":17233,\"start\":17222},{\"end\":17473,\"start\":17470},{\"end\":17492,\"start\":17484},{\"end\":17510,\"start\":17504},{\"end\":17531,\"start\":17522},{\"end\":17793,\"start\":17788},{\"end\":17810,\"start\":17802},{\"end\":17824,\"start\":17818},{\"end\":17838,\"start\":17830},{\"end\":18096,\"start\":18090},{\"end\":18114,\"start\":18108},{\"end\":18117,\"start\":18115},{\"end\":18129,\"start\":18123},{\"end\":18145,\"start\":18139},{\"end\":18371,\"start\":18365},{\"end\":18386,\"start\":18381},{\"end\":18401,\"start\":18396},{\"end\":18417,\"start\":18410},{\"end\":18696,\"start\":18689},{\"end\":18710,\"start\":18702},{\"end\":18725,\"start\":18717},{\"end\":18735,\"start\":18731},{\"end\":18973,\"start\":18965},{\"end\":18990,\"start\":18985},{\"end\":19004,\"start\":18997},{\"end\":19020,\"start\":19013},{\"end\":19034,\"start\":19027},{\"end\":19053,\"start\":19046},{\"end\":19331,\"start\":19327},{\"end\":19351,\"start\":19343},{\"end\":19368,\"start\":19359},{\"end\":19381,\"start\":19377},{\"end\":19401,\"start\":19392},{\"end\":19688,\"start\":19686},{\"end\":19698,\"start\":19695},{\"end\":19712,\"start\":19705},{\"end\":19725,\"start\":19719},{\"end\":19736,\"start\":19732},{\"end\":19745,\"start\":19743},{\"end\":20045,\"start\":20039},{\"end\":20059,\"start\":20052},{\"end\":20073,\"start\":20065},{\"end\":20086,\"start\":20078},{\"end\":20094,\"start\":20092},{\"end\":20347,\"start\":20342},{\"end\":20363,\"start\":20353},{\"end\":20373,\"start\":20369},{\"end\":20551,\"start\":20544},{\"end\":20562,\"start\":20556},{\"end\":20576,\"start\":20570},{\"end\":20588,\"start\":20583},{\"end\":20598,\"start\":20595},{\"end\":20610,\"start\":20605},{\"end\":20620,\"start\":20618},{\"end\":20633,\"start\":20627},{\"end\":20909,\"start\":20903},{\"end\":20921,\"start\":20917},{\"end\":20935,\"start\":20926},{\"end\":20949,\"start\":20943},{\"end\":20963,\"start\":20957},{\"end\":20976,\"start\":20970},{\"end\":20991,\"start\":20983},{\"end\":21003,\"start\":20998},{\"end\":21017,\"start\":21011},{\"end\":21339,\"start\":21333},{\"end\":21353,\"start\":21347},{\"end\":21367,\"start\":21360},{\"end\":21380,\"start\":21373},{\"end\":21390,\"start\":21387},{\"end\":21404,\"start\":21398},{\"end\":21699,\"start\":21693},{\"end\":21713,\"start\":21707},{\"end\":21723,\"start\":21718},{\"end\":21736,\"start\":21730},{\"end\":21747,\"start\":21743},{\"end\":21759,\"start\":21752},{\"end\":21768,\"start\":21765},{\"end\":21782,\"start\":21776},{\"end\":22101,\"start\":22095},{\"end\":22112,\"start\":22109},{\"end\":22123,\"start\":22118},{\"end\":22136,\"start\":22130},{\"end\":22150,\"start\":22143},{\"end\":22159,\"start\":22156},{\"end\":22169,\"start\":22167},{\"end\":22183,\"start\":22177},{\"end\":22545,\"start\":22538},{\"end\":22556,\"start\":22553},{\"end\":22569,\"start\":22563},{\"end\":22584,\"start\":22576},{\"end\":22593,\"start\":22589}]", "bib_author_last_name": "[{\"end\":16295,\"start\":16286},{\"end\":16307,\"start\":16302},{\"end\":16329,\"start\":16319},{\"end\":16571,\"start\":16565},{\"end\":16588,\"start\":16581},{\"end\":16610,\"start\":16598},{\"end\":16624,\"start\":16618},{\"end\":16642,\"start\":16633},{\"end\":16898,\"start\":16892},{\"end\":16915,\"start\":16908},{\"end\":16933,\"start\":16927},{\"end\":16948,\"start\":16941},{\"end\":17166,\"start\":17161},{\"end\":17177,\"start\":17173},{\"end\":17192,\"start\":17188},{\"end\":17207,\"start\":17203},{\"end\":17220,\"start\":17216},{\"end\":17236,\"start\":17234},{\"end\":17482,\"start\":17474},{\"end\":17502,\"start\":17493},{\"end\":17520,\"start\":17511},{\"end\":17538,\"start\":17532},{\"end\":17800,\"start\":17794},{\"end\":17816,\"start\":17811},{\"end\":17828,\"start\":17825},{\"end\":17848,\"start\":17839},{\"end\":18106,\"start\":18097},{\"end\":18121,\"start\":18118},{\"end\":18137,\"start\":18130},{\"end\":18150,\"start\":18146},{\"end\":18379,\"start\":18372},{\"end\":18394,\"start\":18387},{\"end\":18408,\"start\":18402},{\"end\":18422,\"start\":18418},{\"end\":18700,\"start\":18697},{\"end\":18715,\"start\":18711},{\"end\":18729,\"start\":18726},{\"end\":18740,\"start\":18736},{\"end\":18983,\"start\":18974},{\"end\":18995,\"start\":18991},{\"end\":19011,\"start\":19005},{\"end\":19025,\"start\":19021},{\"end\":19044,\"start\":19035},{\"end\":19059,\"start\":19054},{\"end\":19341,\"start\":19332},{\"end\":19357,\"start\":19352},{\"end\":19375,\"start\":19369},{\"end\":19390,\"start\":19382},{\"end\":19410,\"start\":19402},{\"end\":19693,\"start\":19689},{\"end\":19703,\"start\":19699},{\"end\":19717,\"start\":19713},{\"end\":19730,\"start\":19726},{\"end\":19741,\"start\":19737},{\"end\":19751,\"start\":19746},{\"end\":20050,\"start\":20046},{\"end\":20063,\"start\":20060},{\"end\":20076,\"start\":20074},{\"end\":20090,\"start\":20087},{\"end\":20099,\"start\":20095},{\"end\":20351,\"start\":20348},{\"end\":20367,\"start\":20364},{\"end\":20377,\"start\":20374},{\"end\":20554,\"start\":20552},{\"end\":20568,\"start\":20563},{\"end\":20581,\"start\":20577},{\"end\":20593,\"start\":20589},{\"end\":20603,\"start\":20599},{\"end\":20616,\"start\":20611},{\"end\":20625,\"start\":20621},{\"end\":20638,\"start\":20634},{\"end\":20915,\"start\":20910},{\"end\":20924,\"start\":20922},{\"end\":20941,\"start\":20936},{\"end\":20955,\"start\":20950},{\"end\":20968,\"start\":20964},{\"end\":20981,\"start\":20977},{\"end\":20996,\"start\":20992},{\"end\":21009,\"start\":21004},{\"end\":21022,\"start\":21018},{\"end\":21345,\"start\":21340},{\"end\":21358,\"start\":21354},{\"end\":21371,\"start\":21368},{\"end\":21385,\"start\":21381},{\"end\":21396,\"start\":21391},{\"end\":21409,\"start\":21405},{\"end\":21705,\"start\":21700},{\"end\":21716,\"start\":21714},{\"end\":21728,\"start\":21724},{\"end\":21741,\"start\":21737},{\"end\":21750,\"start\":21748},{\"end\":21763,\"start\":21760},{\"end\":21774,\"start\":21769},{\"end\":21787,\"start\":21783},{\"end\":22107,\"start\":22102},{\"end\":22116,\"start\":22113},{\"end\":22128,\"start\":22124},{\"end\":22141,\"start\":22137},{\"end\":22154,\"start\":22151},{\"end\":22165,\"start\":22160},{\"end\":22175,\"start\":22170},{\"end\":22188,\"start\":22184},{\"end\":22551,\"start\":22546},{\"end\":22561,\"start\":22557},{\"end\":22574,\"start\":22570},{\"end\":22587,\"start\":22585},{\"end\":22596,\"start\":22594}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":59316623},\"end\":16496,\"start\":16219},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14941970},\"end\":16850,\"start\":16498},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":222125277},\"end\":17099,\"start\":16852},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218487100},\"end\":17423,\"start\":17101},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4328400},\"end\":17704,\"start\":17425},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":52967399},\"end\":18046,\"start\":17706},{\"attributes\":{\"id\":\"b6\"},\"end\":18282,\"start\":18048},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":174797737},\"end\":18612,\"start\":18284},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":67855617},\"end\":18896,\"start\":18614},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2127100},\"end\":19278,\"start\":18898},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":15150247},\"end\":19593,\"start\":19280},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":235016565},\"end\":19957,\"start\":19595},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2768038},\"end\":20294,\"start\":19959},{\"attributes\":{\"id\":\"b13\"},\"end\":20488,\"start\":20296},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":246294860},\"end\":20838,\"start\":20490},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":246294898},\"end\":21257,\"start\":20840},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":207847706},\"end\":21609,\"start\":21259},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":237353222},\"end\":22016,\"start\":21611},{\"attributes\":{\"doi\":\"arXiv:2201.05575\",\"id\":\"b18\"},\"end\":22467,\"start\":22018},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":239885601},\"end\":22789,\"start\":22469}]", "bib_title": "[{\"end\":16278,\"start\":16219},{\"end\":16555,\"start\":16498},{\"end\":16883,\"start\":16852},{\"end\":17154,\"start\":17101},{\"end\":17468,\"start\":17425},{\"end\":17786,\"start\":17706},{\"end\":18363,\"start\":18284},{\"end\":18687,\"start\":18614},{\"end\":18963,\"start\":18898},{\"end\":19325,\"start\":19280},{\"end\":19684,\"start\":19595},{\"end\":20037,\"start\":19959},{\"end\":20542,\"start\":20490},{\"end\":20901,\"start\":20840},{\"end\":21331,\"start\":21259},{\"end\":21691,\"start\":21611},{\"end\":22536,\"start\":22469}]", "bib_author": "[{\"end\":16297,\"start\":16280},{\"end\":16309,\"start\":16297},{\"end\":16331,\"start\":16309},{\"end\":16573,\"start\":16557},{\"end\":16590,\"start\":16573},{\"end\":16612,\"start\":16590},{\"end\":16626,\"start\":16612},{\"end\":16644,\"start\":16626},{\"end\":16900,\"start\":16885},{\"end\":16917,\"start\":16900},{\"end\":16935,\"start\":16917},{\"end\":16950,\"start\":16935},{\"end\":17168,\"start\":17156},{\"end\":17179,\"start\":17168},{\"end\":17194,\"start\":17179},{\"end\":17209,\"start\":17194},{\"end\":17222,\"start\":17209},{\"end\":17238,\"start\":17222},{\"end\":17484,\"start\":17470},{\"end\":17504,\"start\":17484},{\"end\":17522,\"start\":17504},{\"end\":17540,\"start\":17522},{\"end\":17802,\"start\":17788},{\"end\":17818,\"start\":17802},{\"end\":17830,\"start\":17818},{\"end\":17850,\"start\":17830},{\"end\":18108,\"start\":18090},{\"end\":18123,\"start\":18108},{\"end\":18139,\"start\":18123},{\"end\":18152,\"start\":18139},{\"end\":18381,\"start\":18365},{\"end\":18396,\"start\":18381},{\"end\":18410,\"start\":18396},{\"end\":18424,\"start\":18410},{\"end\":18702,\"start\":18689},{\"end\":18717,\"start\":18702},{\"end\":18731,\"start\":18717},{\"end\":18742,\"start\":18731},{\"end\":18985,\"start\":18965},{\"end\":18997,\"start\":18985},{\"end\":19013,\"start\":18997},{\"end\":19027,\"start\":19013},{\"end\":19046,\"start\":19027},{\"end\":19061,\"start\":19046},{\"end\":19343,\"start\":19327},{\"end\":19359,\"start\":19343},{\"end\":19377,\"start\":19359},{\"end\":19392,\"start\":19377},{\"end\":19412,\"start\":19392},{\"end\":19695,\"start\":19686},{\"end\":19705,\"start\":19695},{\"end\":19719,\"start\":19705},{\"end\":19732,\"start\":19719},{\"end\":19743,\"start\":19732},{\"end\":19753,\"start\":19743},{\"end\":20052,\"start\":20039},{\"end\":20065,\"start\":20052},{\"end\":20078,\"start\":20065},{\"end\":20092,\"start\":20078},{\"end\":20101,\"start\":20092},{\"end\":20353,\"start\":20342},{\"end\":20369,\"start\":20353},{\"end\":20379,\"start\":20369},{\"end\":20556,\"start\":20544},{\"end\":20570,\"start\":20556},{\"end\":20583,\"start\":20570},{\"end\":20595,\"start\":20583},{\"end\":20605,\"start\":20595},{\"end\":20618,\"start\":20605},{\"end\":20627,\"start\":20618},{\"end\":20640,\"start\":20627},{\"end\":20917,\"start\":20903},{\"end\":20926,\"start\":20917},{\"end\":20943,\"start\":20926},{\"end\":20957,\"start\":20943},{\"end\":20970,\"start\":20957},{\"end\":20983,\"start\":20970},{\"end\":20998,\"start\":20983},{\"end\":21011,\"start\":20998},{\"end\":21024,\"start\":21011},{\"end\":21347,\"start\":21333},{\"end\":21360,\"start\":21347},{\"end\":21373,\"start\":21360},{\"end\":21387,\"start\":21373},{\"end\":21398,\"start\":21387},{\"end\":21411,\"start\":21398},{\"end\":21707,\"start\":21693},{\"end\":21718,\"start\":21707},{\"end\":21730,\"start\":21718},{\"end\":21743,\"start\":21730},{\"end\":21752,\"start\":21743},{\"end\":21765,\"start\":21752},{\"end\":21776,\"start\":21765},{\"end\":21789,\"start\":21776},{\"end\":22109,\"start\":22095},{\"end\":22118,\"start\":22109},{\"end\":22130,\"start\":22118},{\"end\":22143,\"start\":22130},{\"end\":22156,\"start\":22143},{\"end\":22167,\"start\":22156},{\"end\":22177,\"start\":22167},{\"end\":22190,\"start\":22177},{\"end\":22553,\"start\":22538},{\"end\":22563,\"start\":22553},{\"end\":22576,\"start\":22563},{\"end\":22589,\"start\":22576},{\"end\":22598,\"start\":22589}]", "bib_venue": "[{\"end\":16345,\"start\":16331},{\"end\":16660,\"start\":16644},{\"end\":16963,\"start\":16950},{\"end\":17250,\"start\":17238},{\"end\":17553,\"start\":17540},{\"end\":17864,\"start\":17850},{\"end\":18088,\"start\":18048},{\"end\":18436,\"start\":18424},{\"end\":18746,\"start\":18742},{\"end\":19075,\"start\":19061},{\"end\":19425,\"start\":19412},{\"end\":19765,\"start\":19753},{\"end\":20114,\"start\":20101},{\"end\":20340,\"start\":20296},{\"end\":20652,\"start\":20640},{\"end\":21037,\"start\":21024},{\"end\":21423,\"start\":21411},{\"end\":21802,\"start\":21789},{\"end\":22093,\"start\":22018},{\"end\":22614,\"start\":22598},{\"end\":16355,\"start\":16347},{\"end\":16672,\"start\":16662},{\"end\":16972,\"start\":16965},{\"end\":17258,\"start\":17252},{\"end\":17562,\"start\":17555},{\"end\":17874,\"start\":17866},{\"end\":18444,\"start\":18438},{\"end\":19085,\"start\":19077},{\"end\":19434,\"start\":19427},{\"end\":19773,\"start\":19767},{\"end\":20123,\"start\":20116},{\"end\":20660,\"start\":20654},{\"end\":21046,\"start\":21039},{\"end\":21431,\"start\":21425},{\"end\":21811,\"start\":21804},{\"end\":22626,\"start\":22616}]"}}}, "year": 2023, "month": 12, "day": 17}