{"id": 231786590, "updated": "2023-11-11 01:55:48.285", "metadata": {"title": "Relaxed Transformer Decoders for Direct Action Proposal Generation", "authors": "[{\"first\":\"Jing\",\"last\":\"Tan\",\"middle\":[]},{\"first\":\"Jiaqi\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Limin\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Gangshan\",\"last\":\"Wu\",\"middle\":[]}]", "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2021, "month": 10, "day": 1}, "abstract": "Temporal action proposal generation is an important and challenging task in video understanding, which aims at detecting all temporal segments containing action in-stances of interest. The existing proposal generation approaches are generally based on pre-defined anchor windows or heuristic bottom-up boundary matching strategies. This paper presents a simple and efficient framework (RTD-Net) for direct action proposal generation, by re-purposing a Transformer-alike architecture. To tackle the essential visual difference between time and space, we make three important improvements over the original transformer detection framework (DETR). First, to deal with slowness prior in videos, we replace the original Transformer en-coder with a boundary attentive module to better capture long-range temporal information. Second, due to the ambiguous temporal boundary and relatively sparse annotations, we present a relaxed matching scheme to relieve the strict criteria of single assignment to each groundtruth. Finally, we devise a three-branch head to further improve the proposal confidence estimation by explicitly predicting its completeness. Extensive experiments on THUMOS14 and ActivityNet-1.3 benchmarks demonstrate the effectiveness of RTD-Net, on both tasks of temporal action proposal generation and temporal action detection. Moreover, due to its simplicity in design, our framework is more efficient than previous proposal generation methods, without non-maximum suppression post-processing. The code and models are made available at https://github.com/MCG-NJU/RTD-Action.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/TanT0W21", "doi": "10.1109/iccv48922.2021.01327"}}, "content": {"source": {"pdf_hash": "5542a9e986261df2a0dfb47c800482f26947486c", "pdf_src": "IEEE", "pdf_uri": "[\"https://arxiv.org/pdf/2102.01894v3.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/2102.01894", "status": "GREEN"}}, "grobid": {"id": "69a57e3fc473d7b2cdb5d4df55c6cd09ec58d101", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/5542a9e986261df2a0dfb47c800482f26947486c.txt", "contents": "\nRelaxed Transformer Decoders for Direct Action Proposal Generation\n\n\nJing Tan \nState Key Laboratory for Novel Software Technology\nNanjing University\nChina\n\nJiaqi Tang jqtang@smail.nju.edu.cn \nState Key Laboratory for Novel Software Technology\nNanjing University\nChina\n\nLimin Wang \nState Key Laboratory for Novel Software Technology\nNanjing University\nChina\n\nGangshan Wu gswu@nju.edu.cn \nState Key Laboratory for Novel Software Technology\nNanjing University\nChina\n\nRelaxed Transformer Decoders for Direct Action Proposal Generation\n10.1109/ICCV48922.2021.01327\nTemporal action proposal generation is an important and challenging task in video understanding, which aims at detecting all temporal segments containing action instances of interest. The existing proposal generation approaches are generally based on pre-defined anchor windows or heuristic bottom-up boundary matching strategies. This paper presents a simple and efficient framework (RTD-Net) for direct action proposal generation, by re-purposing a Transformer-alike architecture. To tackle the essential visual difference between time and space, we make three important improvements over the original transformer detection framework (DETR). First, to deal with slowness prior in videos, we replace the original Transformer encoder with a boundary attentive module to better capture long-range temporal information. Second, due to the ambiguous temporal boundary and relatively sparse annotations, we present a relaxed matching scheme to relieve the strict criteria of single assignment to each groundtruth. Finally, we devise a three-branch head to further improve the proposal confidence estimation by explicitly predicting its completeness. Extensive experiments on THUMOS14 and ActivityNet-1.3 benchmarks demonstrate the effectiveness of RTD-Net, on both tasks of temporal action proposal generation and temporal action detection. Moreover, due to its simplicity in design, our framework is more efficient than previous proposal generation methods, without non-maximum suppression post-processing. The code and models are made available at https://github.com/ MCG-NJU/RTD-Action.\n\nIntroduction\n\nAs large numbers of videos are captured and uploaded online (e.g., YouTube, Instagram, and TikTok), video understanding is becoming an important problem in computer vision. Action recognition [33,39,5,36,47,37] has received much research attention from both academia and in-*: Equal contribution. : Corresponding author. dustry, with a focus on classifying trimmed video clip into action labels. However, these action recognition methods cannot be directly applied for realistic video analysis due to the fact that these web videos are untrimmed in nature. Therefore, temporal action detection [26,24,42,41] is a demanding technique, which aims to localize each action instance in long untrimmed videos with the action category and as well its temporal duration. In general, temporal action detection task is composed of two subtasks: temporal action proposal generation and action classification. As for temporal proposal generation task, there are two mainstream approaches. The first type is an anchorbased [3,19,11,15] method, which generates action proposals based on dense and multi-scale box placement. As the duration of action instances varies from seconds to minutes, it is almost impossible for these anchor-based methods to cover all these ground-truth instances under a reasonable computation consumption. The second type is a boundary-based [46,26,24] method, which first predicts the boundary confidence at all frames, and then employs a bottom-up grouping strategy to match pairs of start and end. These methods extract the boundary information at a local window and simply utilize the local context for modeling. Therefore, these boundary-based methods might be sensitive to noise and fail to yield robust detection results, as they easily produce incomplete proposals. Furthermore, the performance of these two kinds of methods is highly dependent on the carefully-designed anchor placement or sophisticated boundary matching mechanisms, which are hand-crafted with human prior knowledge and require specific tuning.\n\nWe contend that long-range temporal context modeling is vital for proposal generation. Viewing videos as temporal sequences and employing Transformer architecture to model global one-dimensional dependencies boosts localization performance. We propose a direct action proposal generation framework with Transformers. This direct action proposal generation with parallel decoding allows us to better capture inter-proposal relationships from a global view, thus resulting in more complete and precise localization results. Moreover, our temporal detection framework streamlines the complex action proposal generation pipeline with a neat set prediction paradigm, where hand-crafted designs such as anchor box placement, boundary matching strategy, and time-consuming non-maximum suppression are removed. As a result, our framework conducts inference with a noticeable faster speed. However, due to the essential visual property difference between time and space, it is nontrivial to adapt the image detection Transformer architecture for videos.\n\nWe observe that the feature slowness in videos [45] and ambiguous temporal boundaries [31] are two key issues that require specific consideration for building a direct action proposal generation method with Transformers. First, although there are many frames along the temporal dimension, their features change at a very low speed. Direct employment of self-attention mechanism as in Transformer encoder will lead to an over-smoothing issue and reduce the discrimination ability of action boundary. Second, due to the high-level semantic for action concept, its temporal boundary might be not so clear as object boundary, and the ground-truth labels might also contain some noise due to inconsistency among different labors. So a strict set matching loss might have a negative effect on the convergence of Transformer, and not be optimal for training and generalization.\n\nTo address the above issues, we present a Relaxed Transformer Decoder (RTD) architecture for direct action proposal generation, as shown in Figure 1. Compared with the original object detection Transformer, we make three notable improvements to adapt for the video task. First, we replace the original Transformer encoder with a customized boundary-attentive architecture to overcome the over-smoothing issue. Second, we propose a relaxed matcher to relieve the strict criteria of single assignment to a ground-truth. Finally, we devise a three-branch detection head for training and inference. A completeness head is added to explicitly estimate the tIoU between regressed temporal box and ground-truth box. We observe that this tIoU loss can guide the training of Transformer and regularize three heads to converge to a stable solution.\n\nIn summary, our main contributions are as follows:\n\n\u2022 For the first time, we adapt the Transformer architecture for direct action proposal generation in videos to model inter-proposal dependencies from a global view, and reduce the inference time greatly by streamlining temporal action proposal generation pipeline with a simple and neat framework, removing the hand-crafted designs.\n\n\u2022 We make three important improvements over DETR [4] to address the essential difference between temporal location in videos and spatial detection in images, including boundary attentive representation, relaxation mechanism, and three-branch head design.\n\n\u2022 Experiments demonstrate that our method outperforms the existing state-of-the-art methods on THUMOS14 and achieves comparable performance on ActivityNet-1.3, in both temporal action proposal generation task and temporal action detection task.\n\n\nRelated Work\n\nAction Recognition. Action recognition is a fundamental task in video understanding, the same as image classification in the image domain. In addition to provide semantic labels for trimmed videos, action recognition is also eligible for extracting snippet-level features in untrimmed videos, which are used in downstream tasks, such as temporal action detection [46,24], language-based video grounding [44,43], and spatio-temporal action detection [22,21]. There are two main types of video architectures: two-stream networks [33,39,12] extracted video appearance and motion information from RGB image and stacked optical flow; 3D convolution networks [34,5,30] directly captured appearance and motion clues with spatio-temporal kernels. We use I3D [5] model to extract video feature sequences as RTD-Net input.\n\nTemporal Action Proposal Generation. The goal of temporal action proposal generation is to generate proposals in untrimmed videos flexibly and precisely. Among temporal action proposal generation methods, anchor-based methods [3,19,11,15,40,6] retrieved proposals based on multi-scale and dense anchors, which is inflexible and cannot cover all action instances. Boundary-based methods [46,26,24,23] first evaluated the confidence of starting  Figure 2. Pipeline of RTD-Net. Our RTD-Net streamlines the process of temporal action proposal generation by viewing it as a direct set prediction problem. It is composed of three unique designs: a boundary-attentive module for feature extraction, a transformer decoder for direct and parallel decoding of queries, and a relaxed matcher for training label assignment. Our RTD-Net is able to efficiently generate a set of smaller number of proposals without any post processing. and ending points and then matched them to form proposal candidates. However, they generated results only based on local information and were easily affected by noise. On the contrary, our framework makes predictions based on the whole feature sequence and fully leverages the global temporal context. Recently, graph-based methods [41,2] gained popularity in this field, they exploited long-range context based on pre-defined graph structure, the construction of which is highly dependent on human prior knowledge. In contrast, RTD-Net learns its own queries and directly generates complete and precise proposals without any hand-crafted design (anchor matching strategy or graph construction), and is free of time-consuming NMS module.\n\nTransformer and Self-Attention Mechanism. Transformer was firstly introduced by [35] in machine translation task. It tackles the problem of long-range dependency modeling in sequence modeling task. The major feature in Transformer is the self-attention mechanism, which summarizes content from the source sequence and is capable of modeling complex and arbitrary dependencies within a limited number of layers. Inspired by the recent advances in NLP tasks [9,8], self-attention was applied to vision tasks to leverage large-scale or long-range context. For instance, works based on self-attention blocks appeared in image generation [29], image recognition [29,28,7] , action recognition [16] and object detection [4]. Some [29,28,7] used specialized self-attention blocks as substitutes for convolutions, and others used self-attention blocks to replace components in the convolution networks. Recent work [10] showed that with Transformer architecture alone, self-attention blocks could achieve excellent results for image recognition. We use decoder-only Transformer on videos for temporal proposal generation, where our model is able to fully exploit the global temporal context and generate action proposals in a novel and direct paradigm.\n\n\nMethod\n\n\nOverview\n\nWe propose a relaxed transformer decoder network (RTD-Net) to directly generate temporal action proposals. Specifically, given an input video X with l f frames, RTD-Net aims at generating a set of proposals \u03a8 = {\u03c8 n = (t n s , t n e )}, locating the underlying human action instance\u015d \u03a8 = {\u03c8 n = (t n s ,t n e )} Ng n=1 , with N g as the number of action instances in video X.\n\nIn order to tackle the issues caused by feature slowness and ambiguous temporal boundary, RTD-Net features three major components: a boundary-attentive module, a relaxed Transformer decoder, and a three-branch detection head. The main architecture is illustrated in Figure 2. First, we use backbone network to extract short-term features. Then the boundary-attentive module enhances them with discriminative boundary scores, and outputs compact boundaryattentive representations to be fed into a transformer decoder. As shown in experiment, we find that this boundaryattentive module is important for the subsequent decoding process. After this, the transformer decoder uses a set of learned queries to attend to the boundary-attentive representations. This parallel decoding process is able to model all pair-wise constraints among proposal candidates explicitly and capture inter-proposal context information with a global view. Eventually, a three-branch detection head transforms the decoder embedding to our final prediction results. Boundary head directly generates temporal boxes, and binary classification head combined with completeness head gives a confidence score for each predicted box. For training, we give a relaxed matching criteria in the matcher, which alleviates the impact of ambiguous temporal boundaries and allows more well-predicted proposals to be assigned as positive samples.\n\n\nFeature Encoding\n\nWe adopt two-stream networks [39,5] to extract appearance features F A = {f tn,A } from RGB frames and motion features F M = {f tn,M } from stacked optical flow at time t n . Features are extracted with a sliding window of temporal stride \u03c4 and arranged into a sequence of length l s . Following the common practice, we take features after the global pooling layer and before the top fullyconnected layer from feature extractor networks. Appearance features and motion features are concatenated along channel dimension to form our final input feature sequence\nF = {f tn } ls n=1 , where f tn = (f tn,A , f tn,M ).\n\nDirect Action Proposal Generation Mechanism\n\nBoundary-attentive representations. As analyzed above, slowness is a general prior for video data, where short-term features change very slowly in a local window. Meanwhile, our short-term features are usually extracted from a short video segment with overlap, which will further smooth visual features. For temporal action proposal generation, it is crucial to keep sharp boundary information in visual representations to allows for the subsequent decoding processing. To alleviate the issue of feature slowness, we propose the boundary-attentive module to explicitly enhance shortterm features with discriminative action boundary information. Specifically, we multiply the original features with its own action starting and ending scores, where the scores of action boundary at each time are estimated with a temporal evaluation module [26]. In experiments, we find that this boundary-attentive representation is helpful for our transformer decoder to generate more accurate action proposals, thanks to the explicit leverage of action boundary information. MLP encoder is employed to transform the boundaryattentive representation into a more compact form.\n\nRelaxed Transformer decoder. We use the vanilla Transformer decoder to directly output temporal action proposals. The decoder takes a set of proposal queries and boundaryattentive representations as input, and outputs action proposal embedding for each query via stacked multi-head self-attention and encoder-decoder attention blocks. Selfattention layers model the temporal dependencies between proposals and refine the corresponding query embedding.\n\nIn 'encoder-decoder' attention layers, proposal queries attend to all time steps and aggregate action information at high activation into each query embedding. During training procedure, this decoder collaborates with a Hungarian matcher to align positive proposals with groundtruth and the whole pipeline is trained with a set prediction loss. Unlike object detection in images, temporal action proposal generation is more ambiguous and sparse in annotation. For instance, only a few actions appear in an observation window for THUMOS14 and the average number of action instances in ActivityNet-1.3 is only 1.5. In addition, the temporal variation of action instances is significant across different videos, in particular for ActivityNet dataset. So, the matching criteria that only a single detection result matches a groundtruth instance might be sub-optimal for temporal action proposal generation. In practice, we observe that the visual difference between some temporal segments around the groundtruth is very small, and the strict matching criteria will make whole network confused and thereby hard to converge to a stable solution.\n\nTo deal with this problem, we propose a relaxed matching scheme, where multiple detected action proposals are assigned as positive when matching to the groundtruth. Specifically, we use a tIoU threshold to distinguish positive and negative samples, where tIoU is calculated as the intersection between target and prediction over their union. The predictions with tIoU higher than a certain threshold will be marked as positive samples. In experiments, we observe that this simple relaxation will relieve the training difficulty of RTD-Net and is helpful to improve the final performance. Three-branch head design. RTD-Net generates final predictions by designing three feed forward networks (FFNs) as detection heads. We generalize the box head and class head in object detection to predict temporal action proposals. Boundary head decodes temporal boundary tuple of an action proposal \u03c8 n = (t n s , t n e ), which consists of a starting frame t n s and an ending frame t n e . Binary classification head predicts foreground confidence score p bc of each proposal. In addition, a completeness head is proposed to evaluate prediction completeness p c with respect to the groundtruth.\n\nA high-quality proposal requires not only high foreground confidence but also accurate boundaries. Sometimes, the binary classification score alone fails to be a reliable measure of predictions due to the confused action boundaries. RTD-Net introduces a completeness head to predict a completeness score p c that measures the overlap between predictions and targets. This additional completeness score is able to explicitly incorporate temporal localization quality to improve the proposal confidence score estimation, thus making the whole pipeline more stable.\n\n\nTraining\n\nIn our training, we first scale video features into a fixed length for subsequent processing. Specifically, following the common practice, we employ a sliding window strategy with a fixed overlap ratio on THUMOS14 dataset and a re-scaling operation on ActivityNet-1.3 dataset. In THU-MOS14, only observation windows that contain at least one target are selected for training. Boundary-attentive module. Starting and ending scores are predicted as boundary probabilities. We follow the footsteps of BSN [26], and use a three-layer convolution network as the boundary probability predictor. This predictor is trained in a frame level to generate starting and ending probability p tn,s and p tn,e for each temporal location t n . Label assignment of RTD-Net. The ground-truth instance set\u03a8 = {\u03c8 n = (t n s ,t n e )} Ng n=1 is composed of N g targets, wheret n s andt n e are starting and ending temporal locations of\u03c8 n . Likewise, the prediction set of N p samples is denoted as \u03a8 = {\u03c8 n = (t n s , t n e )} Np n=1 . We assume N p is larger than N g and augment\u03a8 to be size N p by padding \u2205. Similar to DETR [4], RTD-Net first searches for an optimal bipartite matching between these two sets and the cost of the matcher is defined as:\nC = n:\u03c3(n)\u0338 =\u2205 \u03b1 \u00b7 \u21131(\u03c8n,\u03c8(\u03c3(n)))\u2212\u03b2 \u00b7 tIoU (\u03c8n,\u03c8(\u03c3(n))\u2212\u03b3\u00b7p bc,n ,(1)\nwhere \u03c3 is a permutation of N p elements to match the prediction to targets, \u03b1, \u03b2, and \u03b3 are hyper-parameters and specified as 1, 5, 2 in experiments. Here we use both \u2113 1 loss and tIoU loss for bipartite matching due to its complementarity. Based on the Hungarian algorithm, the matcher is able to search for the best permutation with the lowest cost. Besides, a relaxation mechanism is proposed to address sparse annotations and ambiguous boundaries of action instances. We calculate tIoU between targets and predictions, and also mark those predictions with tIoU higher than a certain threshold as positive samples. After relaxation, the updated assignment of predictions is notated as \u03c3 \u2032 . Loss of the binary classification head. We define the binary classification loss function as:\nL cls = \u2212\u03b3 \u00b7 1 N N n=1\n(p n log p bc,n +(1\u2212p n ) log(1\u2212p bc,n )),\n\n(2) where p bc,n is the binary classification probability and N is the total number of training proposals.p n is 1 if the sample is marked positive, and otherwise 0. Loss of the boundary head. Training loss function for the boundary head is defined as follows:\nL boundary = 1 N pos n:\u03c3 \u2032 (n)\u0338 =\u2205 (\u03b1 \u00b7 L loc,n + \u03b2 \u00b7 L overlap,n ),(3)\nwhere \u2113 1 loss is used in localization loss and tIoU loss is used in overlap measure:\nL loc,n = ||t \u03c3 \u2032 (n) s \u2212 t s (n)|| l1 + ||t \u03c3 \u2032 (n) e\n\u2212 t e (n)|| l1 , L overlap,n = 1 \u2212 tIoU (\u03c8 n ,\u03c8(\u03c3 \u2032 (n))).\n\n\n(4)\n\nLoss of the completeness head. To generate a robust and reliable measure of predictions, we introduce a completeness head to aid the binary classification head. Each proposal sampled for training calculates the tIoU with all targets, and the maximum tIoU is denoted as\u011d tIoU . We adopt temporal convolution layers followed with one fully connected layer upon decoder outputs to predict completeness. To guide the training completeness branch, a loss based on tIoU is proposed:\nL complete = 1 N train Ntrain n=1 (p c,n \u2212\u011d tIoU,n ) 2 .(5)\nAt the beginning, the boundary head fails to predict highquality proposals, and thus the completeness head cannot be effectively trained with low-quality proposals. We follow DRN [43] to apply a two-step training strategy. In the first step, we freeze the parameters of the completeness head and train RTD-Net by minimizing Equations (2) and (3). In the second step, we fix other parts of RTD-Net and only train the completeness head.\n\n\nInference\n\nDue to the direct proposal generation scheme in our RTD, we follow a simple proposal generation pipeline without post-processing methods as non-maximum suppression, that are widely used in previous methods [24,26,6].\n\n\nBoundary-attentive module.\n\nTo preserve the magnitude of features, we normalize the probability sequence {(p tn,s , p tn,e )} ls n=1 to the range of [0,1] and then scale it by \u03b1 r . \u03b1 r is a scaling factor that re-scales boundary scores for stronger discrimination ability, and its choice will be discussed in ablation study. Feature sequence F = {f tn } ls n=1 are multiplied with starting and ending scores separately, and then concatenated along channel dimension. Equipped with positional embedding, the visual representations are forward to a three-layer MLP encoder. Positional embedding is introduced here for temporal discrimination. The MLP encoder models the channel correlation and compacts boundary-attentive representations.\n\nProposal generation. In Transformer decoder, the previous boundary attentive representations are directly retrieved with a set of learnt queries. In the end, for each query, three heads directly output its proposal boundaries, binary classification score, and completeness score.\n\nScore fusion. To make a more reliable confidence estimation for each proposal, we fuse the binary classification score p bc and completeness score p c for each proposal with a simple average. The resulted final proposal set is directly evaluated without any post-processing method.  [18]. ActivityNet-1.3 dataset contains 19,994 untrimmed videos with 200 action categories temporally annotated, and it is divided into training, validation and testing sets by the ratio of 2:1:1. Implementation details. We adopt two-stream network TSN [39] and I3D [5] for feature encoding. Since TSN features better preserve local information, they are fed into the temporal evaluation module [26] for boundary confidence prediction. Compared with TSN features, I3D features have larger receptive fields and contain more contextual information. I3D features are enhanced by boundary probabilities and then input into MLP encoder for transformation and compression. During THUMOS14 feature extraction, the frame stride is set to 8 and 5 for I3D and TSN respectively. As for ActivityNet-1.3, the sampling frame stride is 16. On THUMOS14, we perform proposal generation in a sliding window manner and the length of each sliding window is set to 100 and the overlap ratio is set to 0.75 and 0.5 at training and testing respectively. As for ActivityNet-1.3, feature sequences are rescaled to 100 via linear interpolation. To train RTD-Net from scratch, we use AdamW for optimization. The batch size is set to 32 and the learning rate is set to 0.0001.\n\n\nTemporal Action Proposal Generation\n\nEvaluation metrics. To evaluate the quality of proposals, we calculate Average Recall (AR) with Average Number (AN) of proposals and area under AR vs AN curve per Comparison with state-of-the-art methods. Due to the high discriminative power of I3D features, we use it in our RTD-Net for proposal generation. For fair comparison, we also implement BSN [26] and BMN [24] with the same I3D features by the public available code. The experiment results on THUMOS14 are summarized in Table 1. Since BSN and BMN are highly dependent on the local context, therefore its performance drops on I3D features with large receptive fields. The result demonstrates that our method can fully exploit rich contexts of I3D features and generate better results. Compared with previous state-of-the-art methods, our method achieves the best performance. Meanwhile, the performance improvement for smaller AN is slightly more evident and our RTD does not employ any post-processing method such as NMS. As illustrated in Table 2, RTD-Net also achieves comparable results on ActivityNet-1.3. We analyze that annotations in ActivityNet-1.3 are relatively sparse and the average number of instance is 1.54 in a video (THUMOS14: 15.29). However, our RTD-Net models the pairwise context, and thus it generally requires multiple instances in each video.\n\nIn-depth analysis of RTD-Net proposals. We compare the results of RTD-Net with bottom-up methods BSN and BMN, via a false positive analysis. Inspired by [1], we sort predictions by their scores and take the top-10G predictions per video. Two major errors in proposal generation task are discussed, localization error and background error. Localization error is when a proposal is predicted as foreground, has a minimum tIoU of 0.1 but does not meet the tIoU threshold. Background error is when a proposal is predicted as foreground but its tIoU with ground truth instance is smaller than 0.1. In Figure 4, we observe RTD-Net predictions has the most of true positive samples at every amount of predictions. The proportion of localization error in RTD-Net is notably smaller than those in BSN and BMN, confirming the overall precision of RTD predictions. We visualize qualitative results in Figure 3. Specifically, while BSN makes two incomplete predictions for one action instance, RTD-Net predicts one complete proposal that accurately covers the whole action (the first row). Bottom-up methods like BSN only exploit context in a local window, hence they are unaware of similar features out of range. As a result, they are not robust to local noise and eas- Table 3. Ablation study on the boundary probability scaling factor on THUMOS14, measured by AR@AN.\n\nScaling factor \u03b1 @50 @100 @200 @500 ily yield incomplete proposals. In the multi-instance setting (the second row), RTD-Net has better localization results with more precise boundaries or larger overlap with groundtruths. Benefit from global contextual information, RTD-Net is better aware of visual relationships between action proposals, and visual differences between foregrounds and backgrounds. Therefore, RTD-Net can easily distinguish between foreground and background segments, and localize proposals precisely. Time analysis in inference. RTD-Net also has a notable advantage in inference speed. Compared with BSN, the inference time per sample of RTD-Net is much less (0.114s vs 5.804s, where 5.794s for BSN post-processing). Due to the direct proposal generation mechanism, RTD-Net is free of time-consuming post-processing methods such as nonmaximum suppression. The experiment of inference speed is conducted on one RTX 2080Ti GPU. Detailed efficiency analysis is provided in Appendices C.\n\n\nAblation Study\n\nStudy on scaling factor. We re-weight video features with the predicted boundary probability sequence to enhance the features at possible boundary locations. Scaling factor \u03b1 r needs careful consideration, because it determines a probability threshold to decide the features at which location to enhance and to suppress, i.e. \u03b1 r = 2 enhances features at locations with a boundary probability more than 0.5 and suppresses those at locations with a probability less than 0.5. Table 3 shows AR@AN on THUMOS14 dataset under different settings of the scaling factor. Comparing the results under different \u03b1 r settings, we observe that boundaryattentive representation boosts the performance up to 4% of average recall, and \u03b1 r = 2 maximizes the improvement. Study on feature encoders. We analyze the design of the boundary-attentive module by experimenting on different encoder choices and input feature with different receptive  field sizes. We compare results between MLP and Transformer encoder with the same high-level feature inputs. The first two rows in Table 4 show that MLP outperforms Transformer encoder by a large margin and we analyze that the performance drop might be caused by the over-smooth of self-attention in Transformer. To further investigate the performance, we experiment on the Transformer encoder with features of a smaller receptive field to reduce the oversmoothing effect, and the performance increases to around 36%@50 but is still worse than our MLP encoder. Study on relaxed matcher. With the relaxed matching criteria, some high-quality proposals, assigned negative in the original matcher, will become positive samples. As illustrated in Table 5, the relaxed matcher can improve the AR and AUC metrics. In practice, we first train with the strict bipartite matching criteria to generate sparse predictions, then finetune with the relaxed matching scheme to improve the overall recall (more details in Appendices A.2). Study on completeness modeling. The completeness head is designed to aid the binary classification score for a more reliable measure of predictions. We perform experiments on THUMOS14 testing set, and evaluate proposals in terms of AR@AN. Table 6 reports the results of ablation study on  the completeness head. We see that the combination classification and completeness scores outperforms the result of simply using a classification score. We find that the estimated tIoU score is able to correct some well-predicted proposals but with a low classification score, and hence can boost the AR metrics especially at a smaller AN. Comparison with state-of-the-art methods. To evaluate the quality of our proposals for action detection, we follow a two-stage temporal action detection pipeline. First, we generate a set of action proposals for each video with our RTD-Net and keep top-200 and top-100 proposals for subsequent detection on THUMOS14 and ActivityNet-1.3. Then we score each proposal with two specific strategies. One strategy is using a global classification score from Untrimmed-Net [38] and keeping top-2 predicted labels for each video. Then, we assign the classification score to each proposal and use fusion proposal confidence score and global classification score as detection score. The other strategy is that we employ the proposal-level classifier P-GCN [42] to pre-  dict action labels for each proposal and use the predicted score for evaluation. The result on THUMOS14 is shown in Table 7 and our RTD-Net based detection outperforms other state-of-the-art methods especially under high tIoU settings, which indicates that proposals generated by RTD-Net are more accurate. When combined with P-GCN classifier, our method achieves mAP improvements over other proposal generation methods such as BSN [26] and G-TAD [41] at all tIoU thresholds. This experiment demonstrates that RTD proposals are able to boost the performance of temporal action detection task. As Table 8 illustrates, we achieve comparable results on ActivityNet-1.3. BSN and BMN [24] predict a large number of proposals (nearly 900 proposals per video) and select top-100 of them, while RTD-Net only makes 100 predictions. Compared with BSN and BMN, RTD-Net improves mAP under high tIoU settings (tIoU = 0.95), since RTD-Net proposals have more precise boundaries.\n\n\nAction Detection with RTD proposals\n\n\nConclusion\n\nIn this paper, we have proposed a simple pipeline for direct action proposal generation by re-purposing a Transformer-alike architecture. To bridge the essential difference between videos and images, we introduce three important improvements over the original DETR framework, namely a boundary attentive representation, a relaxed Transformer decoder (RTD), and a three-branch prediction head design. Thanks to the parallel decoding of multiple proposals with explicit context modeling, our RTD-Net outperforms the previous state-of-the-art methods in temporal action proposal generation task on THUMOS14 and also yields a superior performance for action detection on this dataset. In addition, free of NMS post-processing, our detection pipeline is more efficient than previous methods.\n\nFigure 1 .\n1Overview of RTD-Net. Given an untrimmed video, RTD-Net directly generates action proposals based on boundaryattentive features without hand-crafted design, such as dense anchor placement, heuristic matching strategy, and non-maximum suppression.\n\nFigure 3 .\n3Qualitative results of RTD-Net on THUMOS14. The proposals shown are the top predictions for corresponding groundtruths based on the scoring scheme for each model.\n\nFigure 4 .\n4False positive profile of three proposal generation methods: RTD-Net, BSN and BMN. The three graphs demonstrate the FP error breakdown within the top 10-G (G = the number of ground truths) predictions per video. Maximum tIoU for localization error is set to 0.5.\n\nTable 1 .\n1Comparison with other state-of-the-art proposal generation methods on the test set of THUMOS14 in terms of AR@AN. SNMS stands for Soft-NMS.Method \n@50 @100 @200 @500 \nTAG+NMS [46] \n18.55 29.00 39.61 \n-\nTURN+NMS [15] \n21.86 31.89 43.02 57.63 \nCTAP+NMS [13] \n32.49 42.61 51.97 \n-\nBSN+SNMS [26] \n37.46 46.06 53.21 60.64 \nBSN*+SNMS \n36.73 44.14 49.12 52.26 \nMGG [27] \n39.93 47.75 54.65 61.36 \nBMN+SNMS [24] \n39.36 47.72 54.70 62.07 \nBMN*+SNMS \n37.03 44.12 49.49 54.27 \nDBG+SNMS [23] \n37.32 46.67 54.50 62.21 \nRapNet+SNMS [14] 40.35 48.23 54.92 61.41 \nBC-GNN+SNMS [2] 40.50 49.60 56.33 62.80 \nRTD-Net* \n41.52 49.32 56.41 62.91 \n\n* results are reported based on P-GCN I3D features. \n\n4. Experiments \n\n4.1. Dataset and Setup \n\nTHUMOS14 [20]. THUMOS14 dataset consists of 1,010 \nvalidation videos and 1,574 testing videos of 101 action \nclasses in total. Among them 20 action classes are se-\nlected for temporal action detection. It contains 200 and \n213 untrimmed videos with temporal annotations in valida-\ntion and testing sets. \nActivityNet-1.3 \n\nTable 2 .\n2Comparison with other state-of-the-art proposal generation methods on validation set of ActivityNet-1.3 in terms of AR@AN and AUC. Among them, only RTD-Net is free of NMS.Method \n[25] CTAP [13] BSN [26] MGG [27] BMN [24] RTD-Net \nAR@1 (val) \n-\n-\n32.17 \n-\n-\n33.05 \nAR@100 (val) 73.01 \n73.17 \n74.16 \n74.54 \n75.01 \n73.21 \nAUC (val) \n64.40 \n65.72 \n66.17 \n66.43 \n67.10 \n65.78 \n\nvideo, which are denoted by AR@AN and AUC. Follow-\ning the standard protocol, we use tIoU thresholds set [0.5 \n: 0.05 : 1.0] on THUMOS14 and [0.5 : 0.05 : 0.95] on \nActivityNet-1.3. \n\n\nTable 5 .\n5Ablation study on the relaxed matcher on THUMOS14 and ActivityNet-1.3, measured by AR@AN and AUC.Relaxed matcher @50 @100 @200 @500 AR@1 AR@100 AUC \n\u2717 \n41.07 49.20 56.23 62.77 \n32.73 \n71.88 \n65.50 \n\u2713 \n41.52 49.32 56.41 62.91 \n33.05 \n73.21 \n65.78 \n\nTable 6. Ablation study on the tIoU guided ranking on THU-\nMOS14, measured by AR@AN. \n\nScore \n@50 @100 @200 @500 \nClassification \n41.08 49.03 56.07 62.93 \nClassification + Completeness 41.52 49.32 56.41 62.91 \n\nGround Truth \nBSN Proposals \nRTD Proposals \n\ntime \n\n95.1s \n99.3s \n\n97.1s \n99.4s \n\nscore = 0.561 \n\n94.9s \n96.7s \n\nscore = 0.568 \n\n94.6s \n99.2s \n\nscore = 0.744 \n\n94.7s \n\n94.7s \n\n101.3s \n\n101.4s \n\n93.6s \n\n101.2s \n\nscore = 0.851 \n\nscore = 0.899 \n\nscore = 0.736 \n\nscore = 0.770 \n\n137.4s \n\n141.2s \n\n141.3s \n\n134.9s \n\n134.4s \n141.1s \n\n\n\nTable 7 .\n7Temporal action detection results on the test set of THU-MOS14 in terms of mAP at different tIoU thresholds. Proposals are combined with the classifiers of UntrimmedNet[38] and P-GCN[42].Method \nClassifier 0.7 \n0.6 \n0.5 \n0.4 \n0.3 \nSST [3] \nUNet \n4.7 10.9 20.0 31.5 41.2 \nTURN [15] \nUNet \n6.3 14.1 24.5 35.3 46.3 \nBSN [26] \nUNet \n20.0 28.4 36.9 45.0 53.5 \nMGG [27] \nUNet \n21.3 29.5 37.4 46.8 53.9 \nBMN [24] \nUNet \n20.5 29.7 38.8 47.4 56.0 \nDBG [23] \nUNet \n21.7 30.2 39.8 49.4 57.8 \nG-TAD [41] \nUNet \n23.4 30.8 40.2 47.6 54.5 \nBC-GNN [2] \nUNet \n23.1 31.2 40.4 49.1 57.1 \nRTD-Net \nUNet \n25.0 36.4 45.1 53.1 58.5 \nBSN [26] \nP-GCN \n-\n-\n49.1 57.8 63.6 \nG-TAD [41] \nP-GCN \n22.9 37.6 51.6 60.4 66.4 \nRTD-Net \nP-GCN \n23.7 38.8 51.9 62.3 68.3 \n\n\n\nTable 8 .\n8Temporal action detection results on the validation set of ActivityNet-1.3 in terms of mAP at different tIoU thresholds. Proposals are combined with the classifier of UntrimmedNet[38].Method \n0.95 0.75 \n0.5 \nAverage \nSCC [17] \n4.70 17.90 40.00 \n21.70 \nCDC [32] \n0.21 25.88 43.83 \n22.77 \nSSN [46] \n5.49 23.48 39.12 \n23.98 \nLin et al.[25] 7.09 29.65 44.39 \n29.17 \nBSN [26] \n8.02 29.96 46.45 \n30.03 \nBMN [24] \n8.29 34.78 50.07 \n33.85 \nRTD-Net \n8.61 30.68 47.21 \n30.83 \n\n\n\n\nEvaluation metrics. To evaluate the results of temporal action detection task, we calculate Mean Average Precision (mAP). On THUMOS14, mAP with tIoU thresholds set [0.3 : 0.1 : 0.7] are calculated. On ActivityNet-1.3, mAP with tIoU thresholds set {0.5, 0.75, 0.95} and average mAP with tIoU thresholds set [0.5 : 0.05 : 0.95] are reported.\n\n\nError Breakdown (%) RTD-Net 1G 2G 3G 4G 5G 6G 7G 8G 9G 10G BSN 1G 2G 3G 4G 5G 6G 7G 8G 9G 10G1G 2G 3G 4G 5G 6G 7G 8G 9G 10G \n0 \n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100 \n\nBMN \n\nTrue Positive \nLocalization Error \nBackground Error \n\n\nAcknowledgements. This work is supported by National Nat-\nDiagnosing error in temporal action detectors. Humam Alwassel, Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, ECCV (3). Springer11207Humam Alwassel, Fabian Caba Heilbron, Victor Escorcia, and Bernard Ghanem. Diagnosing error in temporal action detectors. In ECCV (3), volume 11207 of Lecture Notes in Computer Science, pages 264-280. Springer, 2018. 6\n\nBoundary content graph neural network for temporal action proposal generation. Yueran Bai, Yingying Wang, Yunhai Tong, Yang Yang, Qiyue Liu, Junhui Liu, ECCV (28). Springer12373Yueran Bai, Yingying Wang, Yunhai Tong, Yang Yang, Qiyue Liu, and Junhui Liu. Boundary content graph neural network for temporal action proposal generation. In ECCV (28), volume 12373 of Lecture Notes in Computer Science, pages 121-137. Springer, 2020. 3, 6, 8\n\nEnd-to-end, single-stream temporal action detection in untrimmed videos. Shyamal Buch, Victor Escorcia, Bernard Ghanem, Li Fei-Fei, Juan Carlos Niebles, BMVC. Shyamal Buch, Victor Escorcia, Bernard Ghanem, Li Fei- Fei, and Juan Carlos Niebles. End-to-end, single-stream tem- poral action detection in untrimmed videos. In BMVC, 2017. 1, 2, 8\n\nEndto-end object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, ECCV. 25Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End- to-end object detection with transformers. In ECCV, pages 213-229, 2020. 2, 3, 5\n\nQuo vadis, action recognition? A new model and the kinetics dataset. Jo\u00e3o Carreira, Andrew Zisserman, CVPR. 6Jo\u00e3o Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In CVPR, pages 4724-4733, 2017. 1, 2, 4, 6\n\nRethinking the faster R-CNN architecture for temporal action localization. Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Seybold, David A Ross, Jia Deng, Rahul Sukthankar, CVPR. 25Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Sey- bold, David A. Ross, Jia Deng, and Rahul Sukthankar. Re- thinking the faster R-CNN architecture for temporal action localization. In CVPR, pages 1130-1139, 2018. 2, 5\n\nOn the relationship between self-attention and convolutional layers. Jean-Baptiste Cordonnier, Andreas Loukas, Martin Jaggi, ICLR. 2020Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-attention and con- volutional layers. In ICLR, 2020. 3\n\nTransformer-xl: Attentive language models beyond a fixed-length context. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Viet Le, Ruslan Salakhutdinov, ACL. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In ACL, pages 2978-2988, 2019. 3\n\nBERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL-HLT. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional trans- formers for language understanding. In NAACL-HLT, pages 4171-4186, 2019. 3\n\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. CoRR, abs. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. CoRR, abs/2010.11929, 2020. 3\n\nDaps: Deep action proposals for action understanding. Victor Escorcia, Fabian Caba Heilbron, Juan Carlos Niebles, Bernard Ghanem, ECCV. 9907Victor Escorcia, Fabian Caba Heilbron, Juan Carlos Niebles, and Bernard Ghanem. Daps: Deep action proposals for ac- tion understanding. In ECCV, volume 9907, pages 768-784, 2016. 1, 2\n\nConvolutional two-stream network fusion for video action recognition. Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman, CVPR. Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman. Convolutional two-stream network fusion for video action recognition. In CVPR, pages 1933-1941, 2016. 2\n\nCTAP: complementary temporal action proposal generation. Jiyang Gao, Kan Chen, Ram Nevatia, ECCV. SpringerJiyang Gao, Kan Chen, and Ram Nevatia. CTAP: com- plementary temporal action proposal generation. In ECCV, pages 70-85. Springer, 2018. 6\n\nAccurate temporal action proposal generation with relation-aware pyramid network. Jialin Gao, Zhixiang Shi, Guanshuo Wang, Jiani Li, Yufeng Yuan, Shiming Ge, Xi Zhou, AAAI. Jialin Gao, Zhixiang Shi, Guanshuo Wang, Jiani Li, Yufeng Yuan, Shiming Ge, and Xi Zhou. Accurate temporal action proposal generation with relation-aware pyramid network. In AAAI, pages 10810-10817, 2020. 6\n\nTURN TAP: temporal unit regression network for temporal action proposals. Jiyang Gao, Zhenheng Yang, Chen Sun, Kan Chen, Ram Nevatia, ICCV. Jiyang Gao, Zhenheng Yang, Chen Sun, Kan Chen, and Ram Nevatia. TURN TAP: temporal unit regression network for temporal action proposals. In ICCV, pages 3648-3656, 2017. 1, 2, 6, 8\n\nand Andrew Zisserman. Video action transformer network. Rohit Girdhar, Jo\u00e3o Carreira, Carl Doersch, CVPR. Rohit Girdhar, Jo\u00e3o Carreira, Carl Doersch, and Andrew Zis- serman. Video action transformer network. In CVPR, pages 244-253, 2019. 3\n\nSCC: semantic context cascade for efficient action detection. Wayner Fabian Caba Heilbron, Victor Barrios, Bernard Escorcia, Ghanem, CVPR. Fabian Caba Heilbron, Wayner Barrios, Victor Escorcia, and Bernard Ghanem. SCC: semantic context cascade for effi- cient action detection. In CVPR, pages 3175-3184, 2017. 8\n\nActivitynet: A large-scale video benchmark for human activity understanding. Victor Fabian Caba Heilbron, Bernard Escorcia, Juan Carlos Ghanem, Niebles, CVPR. Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In CVPR, pages 961-970, 2015. 6\n\nFast temporal activity proposals for efficient detection of human actions in untrimmed videos. Juan Carlos Fabian Caba Heilbron, Bernard Niebles, Ghanem, CVPR. 1Fabian Caba Heilbron, Juan Carlos Niebles, and Bernard Ghanem. Fast temporal activity proposals for efficient de- tection of human actions in untrimmed videos. In CVPR, pages 1914-1923, 2016. 1, 2\n\nY.-G Jiang, J Liu, A Zamir, G Toderici, I Laptev, M Shah, R Sukthankar, THUMOS challenge: Action recognition with a large number of classes. Y.-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev, M. Shah, and R. Sukthankar. THUMOS challenge: Ac- tion recognition with a large number of classes. http: //crcv.ucf.edu/THUMOS14/, 2014. 6\n\nAction tubelet detector for spatiotemporal action localization. Vicky Kalogeiton, Philippe Weinzaepfel, Vittorio Ferrari, Cordelia Schmid, ICCV. Vicky Kalogeiton, Philippe Weinzaepfel, Vittorio Ferrari, and Cordelia Schmid. Action tubelet detector for spatio- temporal action localization. In ICCV, pages 4415-4423, 2017. 2\n\nActions as moving points. Yixuan Li, Zixu Wang, Limin Wang, Gangshan Wu, ECCV. 2020Yixuan Li, Zixu Wang, Limin Wang, and Gangshan Wu. Ac- tions as moving points. In ECCV, pages 68-84, 2020. 2\n\nFast learning of temporal action proposal via dense boundary generator. Chuming Lin, Jian Li, Yabiao Wang, Ying Tai, Donghao Luo, Zhipeng Cui, Chengjie Wang, Jilin Li, Feiyue Huang, Rongrong Ji, AAAI. AAAI Press6Chuming Lin, Jian Li, Yabiao Wang, Ying Tai, Donghao Luo, Zhipeng Cui, Chengjie Wang, Jilin Li, Feiyue Huang, and Rongrong Ji. Fast learning of temporal action proposal via dense boundary generator. In AAAI, pages 11499-11506. AAAI Press, 2020. 2, 6, 8\n\nBMN: boundary-matching network for temporal action proposal generation. Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, Shilei Wen, ICCV. 6Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen. BMN: boundary-matching network for temporal action pro- posal generation. In ICCV, pages 3888-3897, 2019. 1, 2, 5, 6, 8\n\nTemporal convolution based action proposal: Submission to activitynet 2017. Tianwei Lin, Xu Zhao, Zheng Shou, abs/1707.06750CoRR. 6Tianwei Lin, Xu Zhao, and Zheng Shou. Temporal convolu- tion based action proposal: Submission to activitynet 2017. CoRR, abs/1707.06750, 2017. 6, 8\n\nBSN: boundary sensitive network for temporal action proposal generation. Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, Ming Yang, ECCV. 6Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, and Ming Yang. BSN: boundary sensitive network for temporal action proposal generation. In ECCV, pages 3-21, 2018. 1, 2, 4, 5, 6, 8\n\nMulti-granularity generator for temporal action proposal. Yuan Liu, Lin Ma, Yifeng Zhang, Wei Liu, Shih-Fu Chang, CVPR. 6Yuan Liu, Lin Ma, Yifeng Zhang, Wei Liu, and Shih-Fu Chang. Multi-granularity generator for temporal action pro- posal. In CVPR, pages 3604-3613, 2019. 6, 8\n\nStand-alone selfattention in vision models. Niki Parmar, Prajit Ramachandran, Ashish Vaswani, Irwan Bello, Anselm Levskaya, Jon Shlens, NeurIPS. Niki Parmar, Prajit Ramachandran, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens. Stand-alone self- attention in vision models. In NeurIPS, pages 68-80, 2019. 3\n\nImage transformer. Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, Dustin Tran, ICML. 80Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im- age transformer. In ICML, volume 80, pages 4052-4061, 2018. 3\n\nLearning spatiotemporal representation with pseudo-3d residual networks. Zhaofan Qiu, Ting Yao, Tao Mei, ICCV. Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio- temporal representation with pseudo-3d residual networks. In ICCV, pages 5534-5542, 2017. 2\n\nModeling the temporal extent of actions. Scott Satkin, Hebert, ECCV (1). Springer6311Scott Satkin and Martial Hebert. Modeling the temporal ex- tent of actions. In ECCV (1), volume 6311 of Lecture Notes in Computer Science, pages 536-548. Springer, 2010. 2\n\nCDC: convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos. Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki Miyazawa, Shih-Fu Chang, CVPR. Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki Miyazawa, and Shih-Fu Chang. CDC: convolutional-de- convolutional networks for precise temporal action localiza- tion in untrimmed videos. In CVPR, pages 1417-1426, 2017. 8\n\nTwo-stream convolutional networks for action recognition in videos. Karen Simonyan, Andrew Zisserman, NIPS. 1Karen Simonyan and Andrew Zisserman. Two-stream con- volutional networks for action recognition in videos. In NIPS, pages 568-576, 2014. 1, 2\n\nLearning spatiotemporal features with 3d convolutional networks. Du Tran, D Lubomir, Rob Bourdev, Lorenzo Fergus, Manohar Torresani, Paluri, ICCV. Du Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo Torre- sani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In ICCV, pages 4489-4497, 2015. 2\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, NIPS. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pages 5998- 6008, 2017. 3\n\nAppearance-and-relation networks for video classification. Limin Wang, Wei Li, Wen Li, Luc Van Gool, CVPR. Limin Wang, Wei Li, Wen Li, and Luc Van Gool. Appearance-and-relation networks for video classification. In CVPR, pages 1430-1439, 2018. 1\n\nTDN: temporal difference networks for efficient action recognition. CoRR, abs. Limin Wang, Zhan Tong, Bin Ji, Gangshan Wu, Limin Wang, Zhan Tong, Bin Ji, and Gangshan Wu. TDN: temporal difference networks for efficient action recognition. CoRR, abs/2012.10071, 2020. 1\n\nUntrimmednets for weakly supervised action recognition and detection. Limin Wang, Yuanjun Xiong, Dahua Lin, Luc Van Gool, CVPR. Limin Wang, Yuanjun Xiong, Dahua Lin, and Luc Van Gool. Untrimmednets for weakly supervised action recog- nition and detection. In CVPR, pages 6402-6411, 2017. 8\n\nTemporal segment networks: Towards good practices for deep action recognition. Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, Luc Van Gool, ECCV. 6Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recogni- tion. In ECCV, pages 20-36, 2016. 1, 2, 4, 6\n\nR-C3D: region convolutional 3d network for temporal activity detection. Huijuan Xu, Abir Das, Kate Saenko, ICCV. Huijuan Xu, Abir Das, and Kate Saenko. R-C3D: region convolutional 3d network for temporal activity detection. In ICCV, pages 5794-5803, 2017. 2\n\nG-TAD: sub-graph localization for temporal action detection. Mengmeng Xu, Chen Zhao, David S Rojas, Ali K Thabet, Bernard Ghanem, CVPR. Mengmeng Xu, Chen Zhao, David S. Rojas, Ali K. Thabet, and Bernard Ghanem. G-TAD: sub-graph localization for temporal action detection. In CVPR, pages 10153-10162, 2020. 1, 3, 8\n\nGraph convolutional networks for temporal action localization. Runhao Zeng, Wenbing Huang, Chuang Gan, Mingkui Tan, Yu Rong, Peilin Zhao, Junzhou Huang, ICCV. 1Runhao Zeng, Wenbing Huang, Chuang Gan, Mingkui Tan, Yu Rong, Peilin Zhao, and Junzhou Huang. Graph convo- lutional networks for temporal action localization. In ICCV, pages 7093-7102, 2019. 1, 8\n\nDense regression network for video grounding. Runhao Zeng, Haoming Xu, Wenbing Huang, Peihao Chen, Mingkui Tan, Chuang Gan, CVPR. 25Runhao Zeng, Haoming Xu, Wenbing Huang, Peihao Chen, Mingkui Tan, and Chuang Gan. Dense regression network for video grounding. In CVPR, pages 10284-10293, 2020. 2, 5\n\nLearning 2d temporal adjacent networks for moment localization with natural language. Songyang Zhang, Houwen Peng, Jianlong Fu, Jiebo Luo, AAAI. 2020Songyang Zhang, Houwen Peng, Jianlong Fu, and Jiebo Luo. Learning 2d temporal adjacent networks for moment local- ization with natural language. In AAAI, pages 12870-12877, 2020. 2\n\nSlow feature analysis for human action recognition. Zhang Zhang, Dacheng Tao, IEEE Trans. Pattern Anal. Mach. Intell. 343Zhang Zhang and Dacheng Tao. Slow feature analysis for human action recognition. IEEE Trans. Pattern Anal. Mach. Intell., 34(3):436-450, 2012. 2\n\nTemporal action detection with structured segment networks. Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xiaoou Tang, Dahua Lin, ICCV. Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xi- aoou Tang, and Dahua Lin. Temporal action detection with structured segment networks. In ICCV, pages 2933-2942, 2017. 1, 2, 6, 8\n\nMgsampler: An explainable sampling strategy for video action recognition. Yuan Zhi, Zhan Tong, Limin Wang, Gangshan Wu, abs/2104.09952CoRRYuan Zhi, Zhan Tong, Limin Wang, and Gangshan Wu. Mgsampler: An explainable sampling strategy for video ac- tion recognition. CoRR, abs/2104.09952, 2021. 1\n", "annotations": {"author": "[{\"end\":156,\"start\":70},{\"end\":269,\"start\":157},{\"end\":358,\"start\":270},{\"end\":464,\"start\":359}]", "publisher": null, "author_last_name": "[{\"end\":78,\"start\":75},{\"end\":167,\"start\":163},{\"end\":280,\"start\":276},{\"end\":370,\"start\":368}]", "author_first_name": "[{\"end\":74,\"start\":70},{\"end\":162,\"start\":157},{\"end\":275,\"start\":270},{\"end\":367,\"start\":359}]", "author_affiliation": "[{\"end\":155,\"start\":80},{\"end\":268,\"start\":193},{\"end\":357,\"start\":282},{\"end\":463,\"start\":388}]", "title": "[{\"end\":67,\"start\":1},{\"end\":531,\"start\":465}]", "venue": null, "abstract": "[{\"end\":2146,\"start\":561}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2358,\"start\":2354},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2361,\"start\":2358},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2363,\"start\":2361},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2366,\"start\":2363},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2369,\"start\":2366},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2372,\"start\":2369},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2760,\"start\":2756},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2763,\"start\":2760},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2766,\"start\":2763},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2769,\"start\":2766},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3175,\"start\":3172},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3178,\"start\":3175},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3181,\"start\":3178},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3184,\"start\":3181},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3521,\"start\":3517},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3524,\"start\":3521},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3527,\"start\":3524},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":5295,\"start\":5291},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5334,\"start\":5330},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7394,\"start\":7391},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8226,\"start\":8222},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8229,\"start\":8226},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8266,\"start\":8262},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8269,\"start\":8266},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8312,\"start\":8308},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8315,\"start\":8312},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8390,\"start\":8386},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8393,\"start\":8390},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8396,\"start\":8393},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8516,\"start\":8512},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8518,\"start\":8516},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8521,\"start\":8518},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8612,\"start\":8609},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8902,\"start\":8899},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8905,\"start\":8902},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8908,\"start\":8905},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8911,\"start\":8908},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8914,\"start\":8911},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8916,\"start\":8914},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9063,\"start\":9059},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9066,\"start\":9063},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9069,\"start\":9066},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9072,\"start\":9069},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9931,\"start\":9927},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9933,\"start\":9931},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10418,\"start\":10414},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10793,\"start\":10790},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10795,\"start\":10793},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10971,\"start\":10967},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10995,\"start\":10991},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10998,\"start\":10995},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11000,\"start\":10998},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11026,\"start\":11022},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11051,\"start\":11048},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11062,\"start\":11058},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11065,\"start\":11062},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11067,\"start\":11065},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11245,\"start\":11241},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":13434,\"start\":13430},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13436,\"start\":13434},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14903,\"start\":14899},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19081,\"start\":19077},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19684,\"start\":19681},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":21994,\"start\":21990},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22469,\"start\":22465},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22472,\"start\":22469},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22474,\"start\":22472},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23785,\"start\":23781},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":24037,\"start\":24033},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24049,\"start\":24046},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24179,\"start\":24175},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25424,\"start\":25420},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25437,\"start\":25433},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26552,\"start\":26549},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":31824,\"start\":31820},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":32104,\"start\":32100},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":32550,\"start\":32546},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":32565,\"start\":32561},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":32797,\"start\":32793},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":37236,\"start\":37232},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":37250,\"start\":37246},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":37995,\"start\":37991}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34176,\"start\":33918},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34352,\"start\":34177},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34628,\"start\":34353},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":35682,\"start\":34629},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36251,\"start\":35683},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":37051,\"start\":36252},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":37799,\"start\":37052},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":38279,\"start\":37800},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":38621,\"start\":38280},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":38854,\"start\":38622}]", "paragraph": "[{\"end\":4196,\"start\":2162},{\"end\":5242,\"start\":4198},{\"end\":6114,\"start\":5244},{\"end\":6954,\"start\":6116},{\"end\":7006,\"start\":6956},{\"end\":7340,\"start\":7008},{\"end\":7596,\"start\":7342},{\"end\":7842,\"start\":7598},{\"end\":8671,\"start\":7859},{\"end\":10332,\"start\":8673},{\"end\":11578,\"start\":10334},{\"end\":11975,\"start\":11600},{\"end\":13380,\"start\":11977},{\"end\":13960,\"start\":13401},{\"end\":15219,\"start\":14061},{\"end\":15672,\"start\":15221},{\"end\":16813,\"start\":15674},{\"end\":17998,\"start\":16815},{\"end\":18562,\"start\":18000},{\"end\":19808,\"start\":18575},{\"end\":20666,\"start\":19878},{\"end\":20732,\"start\":20690},{\"end\":20994,\"start\":20734},{\"end\":21152,\"start\":21067},{\"end\":21266,\"start\":21208},{\"end\":21750,\"start\":21274},{\"end\":22245,\"start\":21811},{\"end\":22475,\"start\":22259},{\"end\":23215,\"start\":22506},{\"end\":23496,\"start\":23217},{\"end\":25028,\"start\":23498},{\"end\":26394,\"start\":25068},{\"end\":27753,\"start\":26396},{\"end\":28757,\"start\":27755},{\"end\":33078,\"start\":28776},{\"end\":33917,\"start\":33131}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14014,\"start\":13961},{\"attributes\":{\"id\":\"formula_1\"},\"end\":19877,\"start\":19809},{\"attributes\":{\"id\":\"formula_2\"},\"end\":20689,\"start\":20667},{\"attributes\":{\"id\":\"formula_3\"},\"end\":21066,\"start\":20995},{\"attributes\":{\"id\":\"formula_4\"},\"end\":21207,\"start\":21153},{\"attributes\":{\"id\":\"formula_5\"},\"end\":21810,\"start\":21751}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25555,\"start\":25548},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26075,\"start\":26068},{\"end\":27662,\"start\":27655},{\"end\":29258,\"start\":29251},{\"end\":29840,\"start\":29833},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":30452,\"start\":30445},{\"end\":30971,\"start\":30964},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":32237,\"start\":32230},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":32717,\"start\":32710}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2160,\"start\":2148},{\"attributes\":{\"n\":\"2.\"},\"end\":7857,\"start\":7845},{\"attributes\":{\"n\":\"3.\"},\"end\":11587,\"start\":11581},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11598,\"start\":11590},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13399,\"start\":13383},{\"attributes\":{\"n\":\"3.3.\"},\"end\":14059,\"start\":14016},{\"attributes\":{\"n\":\"3.4.\"},\"end\":18573,\"start\":18565},{\"end\":21272,\"start\":21269},{\"attributes\":{\"n\":\"3.5.\"},\"end\":22257,\"start\":22248},{\"end\":22504,\"start\":22478},{\"attributes\":{\"n\":\"4.2.\"},\"end\":25066,\"start\":25031},{\"attributes\":{\"n\":\"4.3.\"},\"end\":28774,\"start\":28760},{\"attributes\":{\"n\":\"4.4.\"},\"end\":33116,\"start\":33081},{\"attributes\":{\"n\":\"5.\"},\"end\":33129,\"start\":33119},{\"end\":33929,\"start\":33919},{\"end\":34188,\"start\":34178},{\"end\":34364,\"start\":34354},{\"end\":34639,\"start\":34630},{\"end\":35693,\"start\":35684},{\"end\":36262,\"start\":36253},{\"end\":37062,\"start\":37053},{\"end\":37810,\"start\":37801}]", "table": "[{\"end\":35682,\"start\":34780},{\"end\":36251,\"start\":35866},{\"end\":37051,\"start\":36361},{\"end\":37799,\"start\":37251},{\"end\":38279,\"start\":37996},{\"end\":38854,\"start\":38717}]", "figure_caption": "[{\"end\":34176,\"start\":33931},{\"end\":34352,\"start\":34190},{\"end\":34628,\"start\":34366},{\"end\":34780,\"start\":34641},{\"end\":35866,\"start\":35695},{\"end\":36361,\"start\":36264},{\"end\":37251,\"start\":37064},{\"end\":37996,\"start\":37812},{\"end\":38621,\"start\":38282},{\"end\":38717,\"start\":38624}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6264,\"start\":6256},{\"end\":9125,\"start\":9117},{\"end\":12251,\"start\":12243},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27000,\"start\":26992},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27294,\"start\":27286}]", "bib_author_first_name": "[{\"end\":38965,\"start\":38960},{\"end\":38982,\"start\":38976},{\"end\":38987,\"start\":38983},{\"end\":39004,\"start\":38998},{\"end\":39022,\"start\":39015},{\"end\":39359,\"start\":39353},{\"end\":39373,\"start\":39365},{\"end\":39386,\"start\":39380},{\"end\":39397,\"start\":39393},{\"end\":39409,\"start\":39404},{\"end\":39421,\"start\":39415},{\"end\":39793,\"start\":39786},{\"end\":39806,\"start\":39800},{\"end\":39824,\"start\":39817},{\"end\":39835,\"start\":39833},{\"end\":39849,\"start\":39845},{\"end\":39856,\"start\":39850},{\"end\":40109,\"start\":40102},{\"end\":40127,\"start\":40118},{\"end\":40142,\"start\":40135},{\"end\":40160,\"start\":40153},{\"end\":40179,\"start\":40170},{\"end\":40196,\"start\":40190},{\"end\":40486,\"start\":40482},{\"end\":40503,\"start\":40497},{\"end\":40752,\"start\":40746},{\"end\":40769,\"start\":40759},{\"end\":40793,\"start\":40788},{\"end\":40808,\"start\":40803},{\"end\":40810,\"start\":40809},{\"end\":40820,\"start\":40817},{\"end\":40832,\"start\":40827},{\"end\":41156,\"start\":41143},{\"end\":41176,\"start\":41169},{\"end\":41191,\"start\":41185},{\"end\":41437,\"start\":41431},{\"end\":41449,\"start\":41443},{\"end\":41462,\"start\":41456},{\"end\":41474,\"start\":41469},{\"end\":41476,\"start\":41475},{\"end\":41492,\"start\":41488},{\"end\":41508,\"start\":41502},{\"end\":41821,\"start\":41816},{\"end\":41838,\"start\":41830},{\"end\":41852,\"start\":41846},{\"end\":41866,\"start\":41858},{\"end\":42222,\"start\":42216},{\"end\":42241,\"start\":42236},{\"end\":42258,\"start\":42249},{\"end\":42275,\"start\":42271},{\"end\":42296,\"start\":42289},{\"end\":42309,\"start\":42303},{\"end\":42330,\"start\":42323},{\"end\":42349,\"start\":42341},{\"end\":42365,\"start\":42360},{\"end\":42753,\"start\":42747},{\"end\":42770,\"start\":42764},{\"end\":42775,\"start\":42771},{\"end\":42790,\"start\":42786},{\"end\":42797,\"start\":42791},{\"end\":42814,\"start\":42807},{\"end\":43097,\"start\":43088},{\"end\":43117,\"start\":43113},{\"end\":43130,\"start\":43124},{\"end\":43374,\"start\":43368},{\"end\":43383,\"start\":43380},{\"end\":43393,\"start\":43390},{\"end\":43644,\"start\":43638},{\"end\":43658,\"start\":43650},{\"end\":43672,\"start\":43664},{\"end\":43684,\"start\":43679},{\"end\":43695,\"start\":43689},{\"end\":43709,\"start\":43702},{\"end\":43716,\"start\":43714},{\"end\":44017,\"start\":44011},{\"end\":44031,\"start\":44023},{\"end\":44042,\"start\":44038},{\"end\":44051,\"start\":44048},{\"end\":44061,\"start\":44058},{\"end\":44320,\"start\":44315},{\"end\":44334,\"start\":44330},{\"end\":44349,\"start\":44345},{\"end\":44568,\"start\":44562},{\"end\":44597,\"start\":44591},{\"end\":44614,\"start\":44607},{\"end\":44896,\"start\":44890},{\"end\":44926,\"start\":44919},{\"end\":44941,\"start\":44937},{\"end\":44948,\"start\":44942},{\"end\":45261,\"start\":45257},{\"end\":45268,\"start\":45262},{\"end\":45298,\"start\":45291},{\"end\":45525,\"start\":45521},{\"end\":45534,\"start\":45533},{\"end\":45541,\"start\":45540},{\"end\":45550,\"start\":45549},{\"end\":45562,\"start\":45561},{\"end\":45572,\"start\":45571},{\"end\":45580,\"start\":45579},{\"end\":45933,\"start\":45928},{\"end\":45954,\"start\":45946},{\"end\":45976,\"start\":45968},{\"end\":45994,\"start\":45986},{\"end\":46221,\"start\":46215},{\"end\":46230,\"start\":46226},{\"end\":46242,\"start\":46237},{\"end\":46257,\"start\":46249},{\"end\":46461,\"start\":46454},{\"end\":46471,\"start\":46467},{\"end\":46482,\"start\":46476},{\"end\":46493,\"start\":46489},{\"end\":46506,\"start\":46499},{\"end\":46519,\"start\":46512},{\"end\":46533,\"start\":46525},{\"end\":46545,\"start\":46540},{\"end\":46556,\"start\":46550},{\"end\":46572,\"start\":46564},{\"end\":46927,\"start\":46920},{\"end\":46937,\"start\":46933},{\"end\":46946,\"start\":46943},{\"end\":46956,\"start\":46951},{\"end\":46969,\"start\":46963},{\"end\":47245,\"start\":47238},{\"end\":47253,\"start\":47251},{\"end\":47265,\"start\":47260},{\"end\":47523,\"start\":47516},{\"end\":47531,\"start\":47529},{\"end\":47546,\"start\":47538},{\"end\":47560,\"start\":47551},{\"end\":47571,\"start\":47567},{\"end\":47831,\"start\":47827},{\"end\":47840,\"start\":47837},{\"end\":47851,\"start\":47845},{\"end\":47862,\"start\":47859},{\"end\":47875,\"start\":47868},{\"end\":48096,\"start\":48092},{\"end\":48111,\"start\":48105},{\"end\":48132,\"start\":48126},{\"end\":48147,\"start\":48142},{\"end\":48161,\"start\":48155},{\"end\":48175,\"start\":48172},{\"end\":48392,\"start\":48388},{\"end\":48407,\"start\":48401},{\"end\":48422,\"start\":48417},{\"end\":48440,\"start\":48434},{\"end\":48453,\"start\":48449},{\"end\":48472,\"start\":48463},{\"end\":48483,\"start\":48477},{\"end\":48751,\"start\":48744},{\"end\":48761,\"start\":48757},{\"end\":48770,\"start\":48767},{\"end\":48974,\"start\":48969},{\"end\":49297,\"start\":49292},{\"end\":49312,\"start\":49304},{\"end\":49326,\"start\":49319},{\"end\":49344,\"start\":49336},{\"end\":49362,\"start\":49355},{\"end\":49676,\"start\":49671},{\"end\":49693,\"start\":49687},{\"end\":49922,\"start\":49920},{\"end\":49930,\"start\":49929},{\"end\":49943,\"start\":49940},{\"end\":49960,\"start\":49953},{\"end\":49976,\"start\":49969},{\"end\":50217,\"start\":50211},{\"end\":50231,\"start\":50227},{\"end\":50245,\"start\":50241},{\"end\":50259,\"start\":50254},{\"end\":50276,\"start\":50271},{\"end\":50289,\"start\":50284},{\"end\":50291,\"start\":50290},{\"end\":50305,\"start\":50299},{\"end\":50319,\"start\":50314},{\"end\":50593,\"start\":50588},{\"end\":50603,\"start\":50600},{\"end\":50611,\"start\":50608},{\"end\":50619,\"start\":50616},{\"end\":50860,\"start\":50855},{\"end\":50871,\"start\":50867},{\"end\":50881,\"start\":50878},{\"end\":50894,\"start\":50886},{\"end\":51121,\"start\":51116},{\"end\":51135,\"start\":51128},{\"end\":51148,\"start\":51143},{\"end\":51157,\"start\":51154},{\"end\":51421,\"start\":51416},{\"end\":51435,\"start\":51428},{\"end\":51446,\"start\":51443},{\"end\":51455,\"start\":51453},{\"end\":51467,\"start\":51462},{\"end\":51479,\"start\":51473},{\"end\":51489,\"start\":51486},{\"end\":51795,\"start\":51788},{\"end\":51804,\"start\":51800},{\"end\":51814,\"start\":51810},{\"end\":52044,\"start\":52036},{\"end\":52053,\"start\":52049},{\"end\":52065,\"start\":52060},{\"end\":52067,\"start\":52066},{\"end\":52078,\"start\":52075},{\"end\":52080,\"start\":52079},{\"end\":52096,\"start\":52089},{\"end\":52359,\"start\":52353},{\"end\":52373,\"start\":52366},{\"end\":52387,\"start\":52381},{\"end\":52400,\"start\":52393},{\"end\":52408,\"start\":52406},{\"end\":52421,\"start\":52415},{\"end\":52435,\"start\":52428},{\"end\":52699,\"start\":52693},{\"end\":52713,\"start\":52706},{\"end\":52725,\"start\":52718},{\"end\":52739,\"start\":52733},{\"end\":52753,\"start\":52746},{\"end\":52765,\"start\":52759},{\"end\":53041,\"start\":53033},{\"end\":53055,\"start\":53049},{\"end\":53070,\"start\":53062},{\"end\":53080,\"start\":53075},{\"end\":53335,\"start\":53330},{\"end\":53350,\"start\":53343},{\"end\":53608,\"start\":53605},{\"end\":53622,\"start\":53615},{\"end\":53635,\"start\":53630},{\"end\":53649,\"start\":53642},{\"end\":53660,\"start\":53654},{\"end\":53672,\"start\":53667},{\"end\":53945,\"start\":53941},{\"end\":53955,\"start\":53951},{\"end\":53967,\"start\":53962},{\"end\":53982,\"start\":53974}]", "bib_author_last_name": "[{\"end\":38974,\"start\":38966},{\"end\":38996,\"start\":38988},{\"end\":39013,\"start\":39005},{\"end\":39029,\"start\":39023},{\"end\":39363,\"start\":39360},{\"end\":39378,\"start\":39374},{\"end\":39391,\"start\":39387},{\"end\":39402,\"start\":39398},{\"end\":39413,\"start\":39410},{\"end\":39425,\"start\":39422},{\"end\":39798,\"start\":39794},{\"end\":39815,\"start\":39807},{\"end\":39831,\"start\":39825},{\"end\":39843,\"start\":39836},{\"end\":39864,\"start\":39857},{\"end\":40116,\"start\":40110},{\"end\":40133,\"start\":40128},{\"end\":40151,\"start\":40143},{\"end\":40168,\"start\":40161},{\"end\":40188,\"start\":40180},{\"end\":40206,\"start\":40197},{\"end\":40495,\"start\":40487},{\"end\":40513,\"start\":40504},{\"end\":40757,\"start\":40753},{\"end\":40786,\"start\":40770},{\"end\":40801,\"start\":40794},{\"end\":40815,\"start\":40811},{\"end\":40825,\"start\":40821},{\"end\":40843,\"start\":40833},{\"end\":41167,\"start\":41157},{\"end\":41183,\"start\":41177},{\"end\":41197,\"start\":41192},{\"end\":41441,\"start\":41438},{\"end\":41454,\"start\":41450},{\"end\":41467,\"start\":41463},{\"end\":41486,\"start\":41477},{\"end\":41500,\"start\":41493},{\"end\":41522,\"start\":41509},{\"end\":41828,\"start\":41822},{\"end\":41844,\"start\":41839},{\"end\":41856,\"start\":41853},{\"end\":41876,\"start\":41867},{\"end\":42234,\"start\":42223},{\"end\":42247,\"start\":42242},{\"end\":42269,\"start\":42259},{\"end\":42287,\"start\":42276},{\"end\":42301,\"start\":42297},{\"end\":42321,\"start\":42310},{\"end\":42339,\"start\":42331},{\"end\":42358,\"start\":42350},{\"end\":42373,\"start\":42366},{\"end\":42762,\"start\":42754},{\"end\":42784,\"start\":42776},{\"end\":42805,\"start\":42798},{\"end\":42821,\"start\":42815},{\"end\":43111,\"start\":43098},{\"end\":43122,\"start\":43118},{\"end\":43140,\"start\":43131},{\"end\":43378,\"start\":43375},{\"end\":43388,\"start\":43384},{\"end\":43401,\"start\":43394},{\"end\":43648,\"start\":43645},{\"end\":43662,\"start\":43659},{\"end\":43677,\"start\":43673},{\"end\":43687,\"start\":43685},{\"end\":43700,\"start\":43696},{\"end\":43712,\"start\":43710},{\"end\":43721,\"start\":43717},{\"end\":44021,\"start\":44018},{\"end\":44036,\"start\":44032},{\"end\":44046,\"start\":44043},{\"end\":44056,\"start\":44052},{\"end\":44069,\"start\":44062},{\"end\":44328,\"start\":44321},{\"end\":44343,\"start\":44335},{\"end\":44357,\"start\":44350},{\"end\":44589,\"start\":44569},{\"end\":44605,\"start\":44598},{\"end\":44623,\"start\":44615},{\"end\":44631,\"start\":44625},{\"end\":44917,\"start\":44897},{\"end\":44935,\"start\":44927},{\"end\":44955,\"start\":44949},{\"end\":44964,\"start\":44957},{\"end\":45289,\"start\":45269},{\"end\":45306,\"start\":45299},{\"end\":45314,\"start\":45308},{\"end\":45531,\"start\":45526},{\"end\":45538,\"start\":45535},{\"end\":45547,\"start\":45542},{\"end\":45559,\"start\":45551},{\"end\":45569,\"start\":45563},{\"end\":45577,\"start\":45573},{\"end\":45591,\"start\":45581},{\"end\":45944,\"start\":45934},{\"end\":45966,\"start\":45955},{\"end\":45984,\"start\":45977},{\"end\":46001,\"start\":45995},{\"end\":46224,\"start\":46222},{\"end\":46235,\"start\":46231},{\"end\":46247,\"start\":46243},{\"end\":46260,\"start\":46258},{\"end\":46465,\"start\":46462},{\"end\":46474,\"start\":46472},{\"end\":46487,\"start\":46483},{\"end\":46497,\"start\":46494},{\"end\":46510,\"start\":46507},{\"end\":46523,\"start\":46520},{\"end\":46538,\"start\":46534},{\"end\":46548,\"start\":46546},{\"end\":46562,\"start\":46557},{\"end\":46575,\"start\":46573},{\"end\":46931,\"start\":46928},{\"end\":46941,\"start\":46938},{\"end\":46949,\"start\":46947},{\"end\":46961,\"start\":46957},{\"end\":46973,\"start\":46970},{\"end\":47249,\"start\":47246},{\"end\":47258,\"start\":47254},{\"end\":47270,\"start\":47266},{\"end\":47527,\"start\":47524},{\"end\":47536,\"start\":47532},{\"end\":47549,\"start\":47547},{\"end\":47565,\"start\":47561},{\"end\":47576,\"start\":47572},{\"end\":47835,\"start\":47832},{\"end\":47843,\"start\":47841},{\"end\":47857,\"start\":47852},{\"end\":47866,\"start\":47863},{\"end\":47881,\"start\":47876},{\"end\":48103,\"start\":48097},{\"end\":48124,\"start\":48112},{\"end\":48140,\"start\":48133},{\"end\":48153,\"start\":48148},{\"end\":48170,\"start\":48162},{\"end\":48182,\"start\":48176},{\"end\":48399,\"start\":48393},{\"end\":48415,\"start\":48408},{\"end\":48432,\"start\":48423},{\"end\":48447,\"start\":48441},{\"end\":48461,\"start\":48454},{\"end\":48475,\"start\":48473},{\"end\":48488,\"start\":48484},{\"end\":48755,\"start\":48752},{\"end\":48765,\"start\":48762},{\"end\":48774,\"start\":48771},{\"end\":48981,\"start\":48975},{\"end\":48989,\"start\":48983},{\"end\":49302,\"start\":49298},{\"end\":49317,\"start\":49313},{\"end\":49334,\"start\":49327},{\"end\":49353,\"start\":49345},{\"end\":49368,\"start\":49363},{\"end\":49685,\"start\":49677},{\"end\":49703,\"start\":49694},{\"end\":49927,\"start\":49923},{\"end\":49938,\"start\":49931},{\"end\":49951,\"start\":49944},{\"end\":49967,\"start\":49961},{\"end\":49986,\"start\":49977},{\"end\":49994,\"start\":49988},{\"end\":50225,\"start\":50218},{\"end\":50239,\"start\":50232},{\"end\":50252,\"start\":50246},{\"end\":50269,\"start\":50260},{\"end\":50282,\"start\":50277},{\"end\":50297,\"start\":50292},{\"end\":50312,\"start\":50306},{\"end\":50330,\"start\":50320},{\"end\":50598,\"start\":50594},{\"end\":50606,\"start\":50604},{\"end\":50614,\"start\":50612},{\"end\":50628,\"start\":50620},{\"end\":50865,\"start\":50861},{\"end\":50876,\"start\":50872},{\"end\":50884,\"start\":50882},{\"end\":50897,\"start\":50895},{\"end\":51126,\"start\":51122},{\"end\":51141,\"start\":51136},{\"end\":51152,\"start\":51149},{\"end\":51166,\"start\":51158},{\"end\":51426,\"start\":51422},{\"end\":51441,\"start\":51436},{\"end\":51451,\"start\":51447},{\"end\":51460,\"start\":51456},{\"end\":51471,\"start\":51468},{\"end\":51484,\"start\":51480},{\"end\":51498,\"start\":51490},{\"end\":51798,\"start\":51796},{\"end\":51808,\"start\":51805},{\"end\":51821,\"start\":51815},{\"end\":52047,\"start\":52045},{\"end\":52058,\"start\":52054},{\"end\":52073,\"start\":52068},{\"end\":52087,\"start\":52081},{\"end\":52103,\"start\":52097},{\"end\":52364,\"start\":52360},{\"end\":52379,\"start\":52374},{\"end\":52391,\"start\":52388},{\"end\":52404,\"start\":52401},{\"end\":52413,\"start\":52409},{\"end\":52426,\"start\":52422},{\"end\":52441,\"start\":52436},{\"end\":52704,\"start\":52700},{\"end\":52716,\"start\":52714},{\"end\":52731,\"start\":52726},{\"end\":52744,\"start\":52740},{\"end\":52757,\"start\":52754},{\"end\":52769,\"start\":52766},{\"end\":53047,\"start\":53042},{\"end\":53060,\"start\":53056},{\"end\":53073,\"start\":53071},{\"end\":53084,\"start\":53081},{\"end\":53341,\"start\":53336},{\"end\":53354,\"start\":53351},{\"end\":53613,\"start\":53609},{\"end\":53628,\"start\":53623},{\"end\":53640,\"start\":53636},{\"end\":53652,\"start\":53650},{\"end\":53665,\"start\":53661},{\"end\":53676,\"start\":53673},{\"end\":53949,\"start\":53946},{\"end\":53960,\"start\":53956},{\"end\":53972,\"start\":53968},{\"end\":53985,\"start\":53983}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":51865607},\"end\":39272,\"start\":38913},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":220961523},\"end\":39711,\"start\":39274},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4021276},\"end\":40054,\"start\":39713},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218889832},\"end\":40411,\"start\":40056},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":206596127},\"end\":40669,\"start\":40413},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":5011503},\"end\":41072,\"start\":40671},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":207852415},\"end\":41356,\"start\":41074},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":57759363},\"end\":41732,\"start\":41358},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":52967399},\"end\":42077,\"start\":41734},{\"attributes\":{\"id\":\"b9\"},\"end\":42691,\"start\":42079},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":11171922},\"end\":43016,\"start\":42693},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":12289712},\"end\":43309,\"start\":43018},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":49742219},\"end\":43554,\"start\":43311},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":212633458},\"end\":43935,\"start\":43556},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3060010},\"end\":44257,\"start\":43937},{\"attributes\":{\"id\":\"b15\"},\"end\":44498,\"start\":44259},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":3748011},\"end\":44811,\"start\":44500},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1710722},\"end\":45160,\"start\":44813},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":827570},\"end\":45519,\"start\":45162},{\"attributes\":{\"id\":\"b19\"},\"end\":45862,\"start\":45521},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1191338},\"end\":46187,\"start\":45864},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":210473278},\"end\":46380,\"start\":46189},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":207852689},\"end\":46846,\"start\":46382},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":198179957},\"end\":47160,\"start\":46848},{\"attributes\":{\"doi\":\"abs/1707.06750\",\"id\":\"b24\",\"matched_paper_id\":29277844},\"end\":47441,\"start\":47162},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":47009464},\"end\":47767,\"start\":47443},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":53848229},\"end\":48046,\"start\":47769},{\"attributes\":{\"id\":\"b27\"},\"end\":48367,\"start\":48048},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3353110},\"end\":48669,\"start\":48369},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":6070160},\"end\":48926,\"start\":48671},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":3238544},\"end\":49184,\"start\":48928},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":456389},\"end\":49601,\"start\":49186},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":11797475},\"end\":49853,\"start\":49603},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1122604},\"end\":50182,\"start\":49855},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":13756489},\"end\":50527,\"start\":50184},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":19132897},\"end\":50774,\"start\":50529},{\"attributes\":{\"id\":\"b36\"},\"end\":51044,\"start\":50776},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":2398945},\"end\":51335,\"start\":51046},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":5711057},\"end\":51714,\"start\":51337},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":10140667},\"end\":51973,\"start\":51716},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":208291175},\"end\":52288,\"start\":51975},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":202538533},\"end\":52645,\"start\":52290},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":215238602},\"end\":52945,\"start\":52647},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":208910773},\"end\":53276,\"start\":52947},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":18780193},\"end\":53543,\"start\":53278},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":1353488},\"end\":53865,\"start\":53545},{\"attributes\":{\"doi\":\"abs/2104.09952\",\"id\":\"b46\"},\"end\":54160,\"start\":53867}]", "bib_title": "[{\"end\":38958,\"start\":38913},{\"end\":39351,\"start\":39274},{\"end\":39784,\"start\":39713},{\"end\":40100,\"start\":40056},{\"end\":40480,\"start\":40413},{\"end\":40744,\"start\":40671},{\"end\":41141,\"start\":41074},{\"end\":41429,\"start\":41358},{\"end\":41814,\"start\":41734},{\"end\":42745,\"start\":42693},{\"end\":43086,\"start\":43018},{\"end\":43366,\"start\":43311},{\"end\":43636,\"start\":43556},{\"end\":44009,\"start\":43937},{\"end\":44313,\"start\":44259},{\"end\":44560,\"start\":44500},{\"end\":44888,\"start\":44813},{\"end\":45255,\"start\":45162},{\"end\":45926,\"start\":45864},{\"end\":46213,\"start\":46189},{\"end\":46452,\"start\":46382},{\"end\":46918,\"start\":46848},{\"end\":47236,\"start\":47162},{\"end\":47514,\"start\":47443},{\"end\":47825,\"start\":47769},{\"end\":48090,\"start\":48048},{\"end\":48386,\"start\":48369},{\"end\":48742,\"start\":48671},{\"end\":48967,\"start\":48928},{\"end\":49290,\"start\":49186},{\"end\":49669,\"start\":49603},{\"end\":49918,\"start\":49855},{\"end\":50209,\"start\":50184},{\"end\":50586,\"start\":50529},{\"end\":51114,\"start\":51046},{\"end\":51414,\"start\":51337},{\"end\":51786,\"start\":51716},{\"end\":52034,\"start\":51975},{\"end\":52351,\"start\":52290},{\"end\":52691,\"start\":52647},{\"end\":53031,\"start\":52947},{\"end\":53328,\"start\":53278},{\"end\":53603,\"start\":53545}]", "bib_author": "[{\"end\":38976,\"start\":38960},{\"end\":38998,\"start\":38976},{\"end\":39015,\"start\":38998},{\"end\":39031,\"start\":39015},{\"end\":39365,\"start\":39353},{\"end\":39380,\"start\":39365},{\"end\":39393,\"start\":39380},{\"end\":39404,\"start\":39393},{\"end\":39415,\"start\":39404},{\"end\":39427,\"start\":39415},{\"end\":39800,\"start\":39786},{\"end\":39817,\"start\":39800},{\"end\":39833,\"start\":39817},{\"end\":39845,\"start\":39833},{\"end\":39866,\"start\":39845},{\"end\":40118,\"start\":40102},{\"end\":40135,\"start\":40118},{\"end\":40153,\"start\":40135},{\"end\":40170,\"start\":40153},{\"end\":40190,\"start\":40170},{\"end\":40208,\"start\":40190},{\"end\":40497,\"start\":40482},{\"end\":40515,\"start\":40497},{\"end\":40759,\"start\":40746},{\"end\":40788,\"start\":40759},{\"end\":40803,\"start\":40788},{\"end\":40817,\"start\":40803},{\"end\":40827,\"start\":40817},{\"end\":40845,\"start\":40827},{\"end\":41169,\"start\":41143},{\"end\":41185,\"start\":41169},{\"end\":41199,\"start\":41185},{\"end\":41443,\"start\":41431},{\"end\":41456,\"start\":41443},{\"end\":41469,\"start\":41456},{\"end\":41488,\"start\":41469},{\"end\":41502,\"start\":41488},{\"end\":41524,\"start\":41502},{\"end\":41830,\"start\":41816},{\"end\":41846,\"start\":41830},{\"end\":41858,\"start\":41846},{\"end\":41878,\"start\":41858},{\"end\":42236,\"start\":42216},{\"end\":42249,\"start\":42236},{\"end\":42271,\"start\":42249},{\"end\":42289,\"start\":42271},{\"end\":42303,\"start\":42289},{\"end\":42323,\"start\":42303},{\"end\":42341,\"start\":42323},{\"end\":42360,\"start\":42341},{\"end\":42375,\"start\":42360},{\"end\":42764,\"start\":42747},{\"end\":42786,\"start\":42764},{\"end\":42807,\"start\":42786},{\"end\":42823,\"start\":42807},{\"end\":43113,\"start\":43088},{\"end\":43124,\"start\":43113},{\"end\":43142,\"start\":43124},{\"end\":43380,\"start\":43368},{\"end\":43390,\"start\":43380},{\"end\":43403,\"start\":43390},{\"end\":43650,\"start\":43638},{\"end\":43664,\"start\":43650},{\"end\":43679,\"start\":43664},{\"end\":43689,\"start\":43679},{\"end\":43702,\"start\":43689},{\"end\":43714,\"start\":43702},{\"end\":43723,\"start\":43714},{\"end\":44023,\"start\":44011},{\"end\":44038,\"start\":44023},{\"end\":44048,\"start\":44038},{\"end\":44058,\"start\":44048},{\"end\":44071,\"start\":44058},{\"end\":44330,\"start\":44315},{\"end\":44345,\"start\":44330},{\"end\":44359,\"start\":44345},{\"end\":44591,\"start\":44562},{\"end\":44607,\"start\":44591},{\"end\":44625,\"start\":44607},{\"end\":44633,\"start\":44625},{\"end\":44919,\"start\":44890},{\"end\":44937,\"start\":44919},{\"end\":44957,\"start\":44937},{\"end\":44966,\"start\":44957},{\"end\":45291,\"start\":45257},{\"end\":45308,\"start\":45291},{\"end\":45316,\"start\":45308},{\"end\":45533,\"start\":45521},{\"end\":45540,\"start\":45533},{\"end\":45549,\"start\":45540},{\"end\":45561,\"start\":45549},{\"end\":45571,\"start\":45561},{\"end\":45579,\"start\":45571},{\"end\":45593,\"start\":45579},{\"end\":45946,\"start\":45928},{\"end\":45968,\"start\":45946},{\"end\":45986,\"start\":45968},{\"end\":46003,\"start\":45986},{\"end\":46226,\"start\":46215},{\"end\":46237,\"start\":46226},{\"end\":46249,\"start\":46237},{\"end\":46262,\"start\":46249},{\"end\":46467,\"start\":46454},{\"end\":46476,\"start\":46467},{\"end\":46489,\"start\":46476},{\"end\":46499,\"start\":46489},{\"end\":46512,\"start\":46499},{\"end\":46525,\"start\":46512},{\"end\":46540,\"start\":46525},{\"end\":46550,\"start\":46540},{\"end\":46564,\"start\":46550},{\"end\":46577,\"start\":46564},{\"end\":46933,\"start\":46920},{\"end\":46943,\"start\":46933},{\"end\":46951,\"start\":46943},{\"end\":46963,\"start\":46951},{\"end\":46975,\"start\":46963},{\"end\":47251,\"start\":47238},{\"end\":47260,\"start\":47251},{\"end\":47272,\"start\":47260},{\"end\":47529,\"start\":47516},{\"end\":47538,\"start\":47529},{\"end\":47551,\"start\":47538},{\"end\":47567,\"start\":47551},{\"end\":47578,\"start\":47567},{\"end\":47837,\"start\":47827},{\"end\":47845,\"start\":47837},{\"end\":47859,\"start\":47845},{\"end\":47868,\"start\":47859},{\"end\":47883,\"start\":47868},{\"end\":48105,\"start\":48092},{\"end\":48126,\"start\":48105},{\"end\":48142,\"start\":48126},{\"end\":48155,\"start\":48142},{\"end\":48172,\"start\":48155},{\"end\":48184,\"start\":48172},{\"end\":48401,\"start\":48388},{\"end\":48417,\"start\":48401},{\"end\":48434,\"start\":48417},{\"end\":48449,\"start\":48434},{\"end\":48463,\"start\":48449},{\"end\":48477,\"start\":48463},{\"end\":48490,\"start\":48477},{\"end\":48757,\"start\":48744},{\"end\":48767,\"start\":48757},{\"end\":48776,\"start\":48767},{\"end\":48983,\"start\":48969},{\"end\":48991,\"start\":48983},{\"end\":49304,\"start\":49292},{\"end\":49319,\"start\":49304},{\"end\":49336,\"start\":49319},{\"end\":49355,\"start\":49336},{\"end\":49370,\"start\":49355},{\"end\":49687,\"start\":49671},{\"end\":49705,\"start\":49687},{\"end\":49929,\"start\":49920},{\"end\":49940,\"start\":49929},{\"end\":49953,\"start\":49940},{\"end\":49969,\"start\":49953},{\"end\":49988,\"start\":49969},{\"end\":49996,\"start\":49988},{\"end\":50227,\"start\":50211},{\"end\":50241,\"start\":50227},{\"end\":50254,\"start\":50241},{\"end\":50271,\"start\":50254},{\"end\":50284,\"start\":50271},{\"end\":50299,\"start\":50284},{\"end\":50314,\"start\":50299},{\"end\":50332,\"start\":50314},{\"end\":50600,\"start\":50588},{\"end\":50608,\"start\":50600},{\"end\":50616,\"start\":50608},{\"end\":50630,\"start\":50616},{\"end\":50867,\"start\":50855},{\"end\":50878,\"start\":50867},{\"end\":50886,\"start\":50878},{\"end\":50899,\"start\":50886},{\"end\":51128,\"start\":51116},{\"end\":51143,\"start\":51128},{\"end\":51154,\"start\":51143},{\"end\":51168,\"start\":51154},{\"end\":51428,\"start\":51416},{\"end\":51443,\"start\":51428},{\"end\":51453,\"start\":51443},{\"end\":51462,\"start\":51453},{\"end\":51473,\"start\":51462},{\"end\":51486,\"start\":51473},{\"end\":51500,\"start\":51486},{\"end\":51800,\"start\":51788},{\"end\":51810,\"start\":51800},{\"end\":51823,\"start\":51810},{\"end\":52049,\"start\":52036},{\"end\":52060,\"start\":52049},{\"end\":52075,\"start\":52060},{\"end\":52089,\"start\":52075},{\"end\":52105,\"start\":52089},{\"end\":52366,\"start\":52353},{\"end\":52381,\"start\":52366},{\"end\":52393,\"start\":52381},{\"end\":52406,\"start\":52393},{\"end\":52415,\"start\":52406},{\"end\":52428,\"start\":52415},{\"end\":52443,\"start\":52428},{\"end\":52706,\"start\":52693},{\"end\":52718,\"start\":52706},{\"end\":52733,\"start\":52718},{\"end\":52746,\"start\":52733},{\"end\":52759,\"start\":52746},{\"end\":52771,\"start\":52759},{\"end\":53049,\"start\":53033},{\"end\":53062,\"start\":53049},{\"end\":53075,\"start\":53062},{\"end\":53086,\"start\":53075},{\"end\":53343,\"start\":53330},{\"end\":53356,\"start\":53343},{\"end\":53615,\"start\":53605},{\"end\":53630,\"start\":53615},{\"end\":53642,\"start\":53630},{\"end\":53654,\"start\":53642},{\"end\":53667,\"start\":53654},{\"end\":53678,\"start\":53667},{\"end\":53951,\"start\":53941},{\"end\":53962,\"start\":53951},{\"end\":53974,\"start\":53962},{\"end\":53987,\"start\":53974}]", "bib_venue": "[{\"end\":39039,\"start\":39031},{\"end\":39436,\"start\":39427},{\"end\":39870,\"start\":39866},{\"end\":40212,\"start\":40208},{\"end\":40519,\"start\":40515},{\"end\":40849,\"start\":40845},{\"end\":41203,\"start\":41199},{\"end\":41527,\"start\":41524},{\"end\":41887,\"start\":41878},{\"end\":42214,\"start\":42079},{\"end\":42827,\"start\":42823},{\"end\":43146,\"start\":43142},{\"end\":43407,\"start\":43403},{\"end\":43727,\"start\":43723},{\"end\":44075,\"start\":44071},{\"end\":44363,\"start\":44359},{\"end\":44637,\"start\":44633},{\"end\":44970,\"start\":44966},{\"end\":45320,\"start\":45316},{\"end\":45660,\"start\":45593},{\"end\":46007,\"start\":46003},{\"end\":46266,\"start\":46262},{\"end\":46581,\"start\":46577},{\"end\":46979,\"start\":46975},{\"end\":47290,\"start\":47286},{\"end\":47582,\"start\":47578},{\"end\":47887,\"start\":47883},{\"end\":48191,\"start\":48184},{\"end\":48494,\"start\":48490},{\"end\":48780,\"start\":48776},{\"end\":48999,\"start\":48991},{\"end\":49374,\"start\":49370},{\"end\":49709,\"start\":49705},{\"end\":50000,\"start\":49996},{\"end\":50336,\"start\":50332},{\"end\":50634,\"start\":50630},{\"end\":50853,\"start\":50776},{\"end\":51172,\"start\":51168},{\"end\":51504,\"start\":51500},{\"end\":51827,\"start\":51823},{\"end\":52109,\"start\":52105},{\"end\":52447,\"start\":52443},{\"end\":52775,\"start\":52771},{\"end\":53090,\"start\":53086},{\"end\":53394,\"start\":53356},{\"end\":53682,\"start\":53678},{\"end\":53939,\"start\":53867}]"}}}, "year": 2023, "month": 12, "day": 17}