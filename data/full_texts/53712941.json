{"id": 53712941, "updated": "2023-09-20 02:25:50.27", "metadata": {"title": "Descriptor : A dataset of clinically generated visual questions and answers about radiology images", "authors": "[{\"first\":\"Jason\",\"last\":\"Lau\",\"middle\":[\"J.\"]},{\"first\":\"Soumya\",\"last\":\"Gayen\",\"middle\":[]},{\"first\":\"Asma\",\"last\":\"Abacha\",\"middle\":[\"Ben\"]},{\"first\":\"Dina\",\"last\":\"Demner-Fushman\",\"middle\":[]}]", "venue": "Scientific Data", "journal": "Scientific Data", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "Radiology images are an essential part of clinical decision making and population screening, e.g., for cancer. Automated systems could help clinicians cope with large amounts of images by answering questions about the image contents. An emerging area of artificial intelligence, Visual Question Answering (VQA) in the medical domain explores approaches to this form of clinical decision support. Success of such machine learning tools hinges on availability and design of collections composed of medical images augmented with question-answer pairs directed at the content of the image. We introduce VQA-RAD, the first manually constructed dataset where clinicians asked naturally occurring questions about radiology images and provided reference answers. Manual categorization of images and questions provides insight into clinically relevant tasks and the natural language to phrase them. Evaluating with well-known algorithms, we demonstrate the rich quality of this dataset over other automatically constructed ones. We propose VQARAD to encourage the community to design VQA tools with the goals of improving patient care.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": "10.1038/sdata.2018.251"}}, "content": {"source": {"pdf_hash": "18f9a6045ba01cb079c4fa49a630d71bbd27cd92", "pdf_src": "PubMedCentral", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.nature.com/articles/sdata2018251.pdf", "status": "GOLD"}}, "grobid": {"id": "8eec8bbe1fc395170e1f368655f73a9d5ae70574", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/18f9a6045ba01cb079c4fa49a630d71bbd27cd92.txt", "contents": "\nData Descriptor: A dataset of clinically generated visual questions and answers about radiology images Background & Summary\nPublished: 20 November 2018\n\nJason J Lau \nLister Hill National Center for Biomedical Communications\nNational Library of Medicine\nBethesdaMDUSA\n\nSoumya Gayen \nLister Hill National Center for Biomedical Communications\nNational Library of Medicine\nBethesdaMDUSA\n\nAsma Ben \nLister Hill National Center for Biomedical Communications\nNational Library of Medicine\nBethesdaMDUSA\n\nAbacha \nLister Hill National Center for Biomedical Communications\nNational Library of Medicine\nBethesdaMDUSA\n\nDina Demner-Fushman \nLister Hill National Center for Biomedical Communications\nNational Library of Medicine\nBethesdaMDUSA\n\nData Descriptor: A dataset of clinically generated visual questions and answers about radiology images Background & Summary\nPublished: 20 November 201810.1038/sdata.2018.251Received: 18 June 2018 Accepted: 16 September 2018Correspondence and requests for materials should be addressed to D.D.-F. (email: ddemner@mail.nih.gov) OPEN www.nature.com/scientificdata SCIENTIFIC DATA | 5:180251 | 1\nRadiology images are an essential part of clinical decision making and population screening, e.g., for cancer. Automated systems could help clinicians cope with large amounts of images by answering questions about the image contents. An emerging area of artificial intelligence, Visual Question Answering (VQA) in the medical domain explores approaches to this form of clinical decision support. Success of such machine learning tools hinges on availability and design of collections composed of medical images augmented with question-answer pairs directed at the content of the image. We introduce VQA-RAD, the first manually constructed dataset where clinicians asked naturally occurring questions about radiology images and provided reference answers. Manual categorization of images and questions provides insight into clinically relevant tasks and the natural language to phrase them. Evaluating with well-known algorithms, we demonstrate the rich quality of this dataset over other automatically constructed ones. We propose VQA-RAD to encourage the community to design VQA tools with the goals of improving patient care. Design Type(s) image creation and editing objective \u2022 anatomical image analysis objective Measurement Type(s) image analysis Technology Type(s) visual observation method Factor Type(s) question type \u2022 answer type Sample Characteristic(s) Homo sapiens \u2022 head \u2022 chest \u2022 abdomen\n\nBackground & Summary\n\nVisual question answering (VQA) is a computer vision and artificial intelligence (AI) problem that aims to answer questions about images. As more of medicine is digitized and medical data continues to grow, there is enormous opportunity for multimodal tools such as VQA to benefit patients, clinicians, and researchers. Radiology, with its wealth of images and textual reports, is a prime area where VQA could assist radiologists in reporting findings for a complicated patient or benefit trainees who have questions about the size of a mass or presence of a fracture. Many different techniques are applied to build VQA systems including computer vision, natural language processing, and deep learning. These systems need to be trained for the task and evaluated on large data collections consisting of images and pairs of questions asked about the images with corresponding answers. Although there has been great progress in image recognition in radiology 1 , the datasets that allowed this are not quite generalizable to VQA because none of the datasets have question-answer pairs directed at the images 2,3 . Deep learning models require vast amounts of data to approach abilities of humans and as a result, the most popular training datasets are crowdsourced with general public knowledge [4][5][6][7][8] or synthetically generated 9,10 . The images in most of these datasets were taken from Microsoft Common Objects in Context (MSCOCO) 11 . While researchers have tried balancing these large datasets 5,12 , bias in selection of images and questions makes it difficult to transfer models trained on these datasets to specific applications. For example, the creators of VizWiz 13 , the first VQA dataset designed from images taken by blind users and visual questions from those use cases, demonstrated challenges in using the state-of-the-art models trained on the larger VQA datasets to predict answers from their VizWiz blind users generated data.\n\nWhile visual questions are prevalent in medicine, most communications between clinicians are not usually documented in the patient's record or available for researchers. Medical education is one area where collections of visual questions exist. Through their training and continued medical education, clinicians will take exams testing their knowledge of medical images. However, these test questions often go beyond the scope of the image and require extra knowledge about epidemiology or next step treatments. Most exam questions may not be suitable for teaching machines to answer descriptive questions about images, but they still have a role for more abstract machine assisted question answering in the future.\n\nIn 2018, ImageCLEF-Med released a radiology dataset 14 and coordinated the first community-wide VQA challenge in medicine. While the dataset is an excellent starting point for the medical domain, several design issues prevent useful clinical applications. To overcome the lack of readily available natural visual questions, questions and answers were automatically generated from corresponding captions. Unfortunately, this resulted in many artificial questions that do not always make sense, to the point where a human could not reason what the questions were trying to ask. Another issue with the dataset is that images were automatically captured from PubMed Central articles. This automation may not help VQA support clinical tasks as the dataset included composites of a series of images and 3D reconstructions that are rarely useful for direct patient care. If VQA tools are to assist in clinical processes, the datasets need to be designed with aligned goals.\n\nWe introduce VQA-RAD, a manually constructed VQA dataset in radiology where questions and answers about images are naturally created and validated by clinicians. Trading off quantity of automatic generation, VQA-RAD is a high-quality design with only 60 hours of specialist contributions. Structured like other existing VQA datasets, each question can be answered with a single image alone. The images are a balanced sample from MedPix\u00ae, an open-access radiology archive of case reports and teaching cases. These images were presented to clinicians who wrote unguided questions they would ask a colleague or radiologist. To further explore the natural clinical language, clinicians also paraphrased questions in both free-form and template structures. All question-answer pairs are manually validated and categorized, helping to characterize clinically important areas of focus. The flow of building the VQA-RAD dataset is shown in Fig. 1. We demonstrate the value of VQA-RAD and use cases by applying several well-known algorithms.\n\n\nMethods\n\n\nImage Selection\n\nWe sampled images from teaching cases in MedPix, https://medpix.nlm.nih.gov/, an open-access database of radiology images and teaching cases. Our sampling criteria were as follows: (1) Only one image for each teaching case, so that all images represented unique patients. (2) All images are sharp enough to identify individual structures. (3) Images are clean of radiology markings, such as arrows or circles. (4) Images have captions that correspond to the image and are detailed enough to describe at least one structure.\n\nToday, diagnostic imaging often contains stacks of images, which is a challenge that VQA tools will need to incorporate to be useful for assisting radiology. For a dataset just emerging into medicine, we wanted to start with simple one image and one question pairs, enabling comparisons with current existing VQA dataset structures and algorithms. To reduce bias of having multiple images of the same pathology, we chose to limit one image per case, since each case is a unique patient. In addition, MedPix images are stored in JPG format, which needs to be taken into consideration when using the images for teaching and for training AI approaches. Captions include plane, modality, and image findings that were generated and reviewed by expert radiologists. In total, we selected 104 head axial single-slice CTs or MRIs, 107 chest x-rays, and 104 abdominal axial CTs. The balanced distribution from head, chest, and abdomen should help determine if visual questions differ for each organ system and if the algorithms perform differently on different regions.\n\n\nQuestion Type and Answer Type Generation\n\nIn addition to categorizing images, we define several question and answer types. Broad question types were initially developed from a combination of using Task Directed Image Understanding Challenge (TDIUC) categories 12 , annotating radiology reports from MedPix, and categorizing CME questions from open-access domains such as MedPix, AuntMinnie (https://www.auntminnie.com/), and Radiopaedia.org (https://radiopaedia.org/). Question types were further refined after analyzing an initial sample of visual questions from annotators. The final taxonomy of medical VQA in our dataset is shown in Table 1.\n\n\nAnnotators\n\nThe tasks of assisting clinicians with radiology are different and range from support for those just learning the basics of reading a plain film x-ray, to those who are senior radiologists investigating rare presentations of diseases. We selected medical students and fellows who had at least their core clinical rotations completed because they will better understand how radiology is incorporated into every day clinical decision making and patient care. These trainees' questions and tasks are closer to those of attending trainees, compared to students just starting medical school. However, we do expect that there will be additional needs from resident trainees as their personal skill in reading radiology images improves and they have more nuanced questions. For example, a junior trainee may ask if there is presence of a pneumothorax, while a senior, who can already recognize air in the lungs, may ask more questions about the size of the pneumothorax since this has implications for invasive treatment.\n\nQuestions and answers were generated by 15 volunteer clinical trainees using a web-interface developed for collecting the questions and the answers. We asked for volunteers from the clinical fellows from the NIH and students from class of 2018/2019 University of Massachusetts, classmates of one of the authors, Jason Lau. All participants had completed the core rotations of medical school, which typically occurs during the 3 rd year of school and exposes students to major fields of medicine such as surgery, internal medicine, neurology, etc. This ensures that all participants have basic clinical radiology reading skills and were exposed to a variety of settings where radiology was vital to the management of patients. Our participants had training from different regions of the U.S. and there was one board certified pathologist, three applying into ophthalmology, four applying into family medicine, two applying into internal medicine, two applying into emergency medicine, one applying into radiology, one applying into orthopedics, and one dermatology.\n\n\nQuestion and Answer Generation\n\nParticipants enerated questions and answers in a two-part evaluation (shown in Fig. 1) from December 2017 to April 2018. Each participant reviewed at least 40 randomized images. For the first 20 images, participants provided \"free-form\" questions and answers without any restrictions. We instructed participants to create \"free-form\" question about the images by phrasing them in a natural way as if they are asking a colleague or another physician. The image alone had to be sufficient to answer the question and there should only be a single correct answer. We asked that answers to the visual questions be based off their level of knowledge. Since many of the participants were still in medical training, we provided captions with some image findings, plane, and modality information to provide additional ground truth reassurance.\n\nFor the next 20 images, participants were randomly paired and given another participant's images and questions. They were asked to generate \"rephrased\" and \"framed\" questions based off the given \"freeform\" questions with corresponding image and caption. We asked the participants to paraphrase the question in a natural way and generate an answer that agreed with both the original and the paraphrased questions.\n\nParticipants generated \"framed\" questions by finding the closest question structure from a list of templates and filling in the blank spaces to retain the answer to the original questions.\n\n\nQuestionAnswer and Question Type Validation\n\nAfter completion of the evaluations, we used several methods to validate questions answer pairs and question types. During the paraphrasing part of the evaluation, participants answered another person's questions. The answers could have strict or loose agreement. We defined strict agreement when the question and answer format and topic were the same. In loose agreement, the topic of the questions is the same or similar even though the answers may differ. Three subcategories of loose agreement are defined: inversion, conversion, and subsuming.\n\nExamples of each as follows:\n\nInversion: Q1: \"Are there abnormal findings in the lower lung fields?\" is a negation of Q2: \"Are the lower lung fields normal?\" Conversion: Q1: \"How would you describe the abnormalities?\" is open-ended while Q2: \"Are the lesions ring-enhancing?\" is closed-ended Subsumption: Q1: \"Is the heart seen in the image?\" subsumes Q2: \"is the heart seen on the left?\"\n\nWe calculate F1 scores of user agreement for questions considered 'evaluated', meaning they were reviewed by two annotators. Disagreements amongst question category types were reviewed at weekly team meetings to determine general rules for all content. Disagreements in objective findings of images were reviewed by using MedPix metadata and advice from expert radiologist to find ground truth. This in-depth review was only required for 280 question-answer pairs. Questions are labeled as 'not evaluated' if they are not reviewed by a second participant or the paraphrased question is not similar enough to be used as validation. Both the evaluated and not evaluated questions are used as part of the test and training set.\n\n\nQuestion Type Description Modality\n\nHow an image is taken -CT, x-ray, T2 weighted MRI, etc.\n\n\nPlane\n\nOrientation of an image slicing through the bodyaxial, sagittal, coronal\n\n\nOrgan System\n\nCategorization that connects anatomical structures with pathophysiology, diagnosis, and treatmentpulmonary, cardiac, musculoskeletal system\n\nAbnormality Normalcy of an image or object. For example, \"is there something wrong with the image?\" or \"What is abnormal about the lung?\", \"Does the liver look normal?\"\n\nObject/Condition Presence Objects could be normal structures like organs or body parts but could also be abnormal objects such as masses or lesions. Clinicians may refer to the presence of conditions in an image or patientfractures, midline shift, infarction We validated question types assigned by the participants. Final categorization was determined through consensus with the research team to resolve disagreements.\n\nWe anticipate that a VQA-RAD pipeline is extensible to produce more test data and training data for traditional machine learning approaches. We believe that more training data sufficient for training deep learning approaches could be produced automatically, bootstrapping from the VQA-RAD collection and using recent approaches to image synthesis and data augmentation 15 .\n\n\nData Records\n\nThe full dataset is archived with Open Science Framework (Data Citation 1). All question-answer pairs referencing images are stored in a single dataset that we provide in three common formats: JSON, XML, and Excel. The dataset contains 14 variables of unique identifiers and categorization with a total of 2,248 elements. The 315 corresponding radiological images are contained in a separate folder. Naming conventions and descriptions of all files can be found in a provided Readme file.\n\n\nTechnical Validation\n\n\nAnalysis of Questions and Answers\n\nThe final VQA-RAD dataset contains 3,515 total visual questions. Of these, 1,515 (43.1%) are free-form. 733 questions are rephrased which means 48.3% of the free-form questions have corresponding paraphrasing. The remaining 1,267 questions are framed and have corresponding free-form and rephrased questions. On average, there are 10 questions per image.\n\nOf the free-form and rephrased questions, 75% (1,691) are evaluated using seven pairs of annotators. User agreement F1-scores range from 0.78 to 0.95 with a mean of 0.85.\n\nAll questions are short, with medians, mean, and modes ranging from 5 to 7 words per question. After converting all words to lowercase and comparing sentences for unique structures, the free-form and rephrased questions have more distinct questions (87% and 93% questions are distinct) compared to framed questions which had 49% uniquely structured questions. Answers are also short, with median and modes of 1 word per question (mean of 1.6). Comparing all lowercase answers for unique structures, we observed 486 distinct answers. This represents 32% distinct answers of the total 1,515, which we expect because about half of all answers are either 'yes' or 'no'.\n\nSubgroup analysis of free-form questions demonstrates the relationship between question types and answer types. Of the 11 question types, Presence questions dominate, with 36% (483) of free-form questions. The least frequent question type is Counting, with only 1% (15) questions. The QA pairs are split into 42% (637) open-ended answer types and 58% (878) close-ended. Yes/no questions represent 92% of the close-ended QA pairs, and 'yes' (405) is as frequent as 'no' (406.)\n\nSubgrouping question and answer types, we observe that people think about certain categories differently (See Fig. 2). Abnormalities, Presence, and Size questions are mostly closed-ended questions. The majority of Organ system and Positional questions are open-ended. While these imbalances are important for learning how to better balance future datasets to ensure that algorithms have enough data to learn to predict answers, these observations suggest that different approaches may be required for different question types. The favoring distribution of closed-ended size questions may suggest that in clinical practice, asking questions about whether something is \"enlarged\" or \"atrophied\" in reference to an acceptable relative size is more useful than asking an open-ended \"what is the size of the organ?\".\n\n\nRelationship of Closed and Open Questions\n\nWe next analyzed the types of words for each question type (The full analysis is provided with the dataset distribution). For each question category, we tokenized the words and listed the top 10 most common distinct words. Many of these words are unique to the question category and can be used as keywords to help guide question categorization. The frequencies of words give insight into the context in which certain questions are likely to be asked. For example, 'heart', 'cardiac', 'cardiomegaly', and 'dilated' are frequent words in the size questions, which suggests that participants thought about size of the heart more commonly than other organs.\n\n\nRelationship of Image Type and Organ System\n\nExamining the question categories in relation to image type, we observed several differences in what clinicians ask when they see images of the head, chest, or abdomen. Presence questions are asked evenly about abdomen and chest, with fewer head images, although remaining the most commonly asked question type. Positional and Color questions were more prevalent with images of the head. Contrastingly, positional questions were less frequent with abdominal images. Organs in the head, in particular structures of the brain, are much more fixed in location compared to organs in the abdomen. Therefore, positional questions are more relevant in the head where one can reference locations. In the abdomen, asking if something is seen in the image is more clinically relevant or easier to determine than knowing the location. Another trend is that size questions are asked most frequently in chest images, agreeing with the distinct words we described earlier. The relationship between question type and image type gives some insight into the patterns of clinical relevancy that clinicians may have. Not all questions are useful and maybe shouldn't be asked for every image. \n\n\nVQA-RAD Benchmarking\n\nWe validated the dataset by evaluating the performance of well-known VQA algorithms and their accuracy at answering visual questions from the VQA-RAD dataset.\n\n\nBaselines\n\nWe include two well-known VQA methods: Multimodal Compact Bilinear pooling (MCB) 16 and Stacked Attention Network (SAN) 17 . The MCB model, winner of the 2016 VQA challenge, uses a multimodal compact bilinear pooling method that first predicts attention visually and textually then combines these features with question representation. MCB includes three components: a CNN image model, an LSTM question model, and MCB pooling that first predicts the spatial attention and then combines the attention representation with the textual representation to predict the answers. For the image model, we used ResNet-152 pre-trained on imageNet. For the question model, a 2-layer LSTM model was used.\n\nThe SAN model is a stacked attention model that queries images multiple times to progressively narrow an attention. Similarly to MCB, SAN includes three components: the image model based on a CNN to extract high level image representations, the question model using an LSTM to extract a semantic vector of the question and the stacked attention model which locates the image regions that are relevant to answer the question. We used the last pooling layer of VGG-16 pre-trained on imageNet as image features, and the last LSTM layer as question features. The image features and the question vector were used to generate the attention distribution over the regions of the image.\n\nThe MCB and SAN were pre-trained on ResNet 18 and VGGNet 19 respectively to extract image features and then both trained on VQA V1.0 dataset training and validation 4 . We refer to these baseline models as MCB-VQA1.0 and SAN-VQA1.0, when trained on VQA V1.0. We then trained the models on the ImageCLEF-VQA-Med 14 training set to see how an existing medical VQA dataset influences the models. We refer to the ImageCLEF trained models as MCB-CLEF and SAN-CLEF. We then train the two networks with VQA-RAD training set creating MCB-RAD and SAN-RAD. Since VQA-RAD is a small dataset, we combined ImageCLEF-VQA-Med with VQA-RAD and trained the networks to explore any synergistic effects. As part of a baseline reference for closed-ended questions, we include predicted answer sets of all 'yes' and 'no'.\n\n\nMetrics\n\nFrom VQA-RAD we separated out a training set and test set. VQA-RAD test set is composed of 300 randomly chosen free-form questions and 151 corresponding paraphrased questions. The VQA-RAD training set is the remainder of the dataset. We conducted a manual review to calculate simple accuracies for each question type and use several accepted metrics to calculate overall performance for each model. For the manual review, we judge the predicted answers of the models to the gold standard VQA-RAD test. In addition to exact match, a full point is given for predicted answers that appropriately answer the original question. For example, \"right lung\" and \"right upper lobe\" are correct answers for the question \"where is the lesion?\" since the question does not indicate a degree of specification. Half a point is awarded for answers that incorporate part of the answer. For example, \"right lung\" would be given half a point for \"which lobe is the lesion in?\" where the correct answer is \"right upper lobe\".\n\nWe employ multiple overall scoring methods to demonstrate limitations of current metrics and need for newer metrics to evaluate VQA in medicine. Simple accuracy is calculated as the number of correct answers over total answers. Mean accuracy is the average of accuracy of the question types, similar to TDIUC dataset 12 . The BLEU 20 method, a commonly used metric for evaluating machine translation, measures the difference based on number of different words used. Following the ImageCLEF-Med methods, we pre-process the human and machine answers by converting answers to lower-case, removing punctuation, tokenizing individual words, and removing stop words using an English list of 172 words.\n\n\nAlgorithmic validation\n\nAs might be expected, the models that best predicted VQA-RAD test set are trained on VQA-RAD training alone or partially. For closed-ended questions, MCB_RAD had slightly better simple accuracy of 60.6%, while SAN_RAD had a slightly better mean accuracy at 54.6%. This performance is comparable to the MCB and SAN models trained and tested on VQA1.0 dataset which had reported 64.2% 16 and 58.9% 17 accuracies for open and closed ended public domain questions. Predicting open-ended VQA-RAD questions, the models performed much lower with the MCB_RAD scoring the best at 25.4% simple accuracy and 19.3% mean accuracy. The contrast between open and closed-ended questions suggests that these models are currently still guessing and may require more data. Potential improvements can come from learning medical terminology and jargon, greater quantity and diversity of questions, and extracting image features from radiological images as opposed to public domain.\n\nManual evaluation of the accuracy of automatic answers to naturally occurring questions in the VQA-RAD test set is shown in Tables 2 and 3.\n\nIn medicine, seemingly small changes in wording can result in very different questions and answers. The opposite also holds true, the same idea can be written in simple or complicated phrasing. The VQA-RAD test set contains 151 matched pairs of free-form and paraphrased questions. We analyzed the MCB_RAD model to better understand the variance in predicted answers for similarly written questions. We observed 48 pairs (31.8%) had differing behavior, Table 4 shows several select pairs. Medical terminology, grammatical variations, and clinical jargon present challenges for algorithms. Paraphrasing helps understand what VQA algorithms are learning and whether those rules are aligned or need some degree of supervision. In addition, paraphrasing gives insight into the natural language of clinicians and what they consider to be similar or different concepts.\n\nOverall scores may be useful for picking a winner of a challenge, but they may also bias towards algorithms that can provide answers that are more common. In medicine, the best answer is the most VQA-RAD demonstrates the value of natural questions for the specialized medical domain. Models without VQA-RAD training scored the same or lower than the all 'yes' or 'no' baselines, suggesting they were no better than guessing yes/no as answers. While the models may have been able to recognize yes/no structured questions, they lacked image features and vocabulary. For example, we observed that the MCB model trained on VQA1.0 dataset predicted many open-ended questions with \"face\", \"clock\", or \"banana\" as answers.\n\nAnalysis using BLEU scoring shows the need for improvement to metrics measuring visual tasks in medicine. We observe an opposite pattern between BLEU and manually judged accuracies where all 'no' resulted in better scores. BLEU penalizes answers with varying lengths, which favors the short one-word VQA-RAD answers and does not consider semantic similarity. While BLEU may be appropriate for certain machine learning techniques, it is not useful for medical VQA where there are many ways to phrase an answer and semantics are more valuable than exact wording. Using VQA-RAD, improved metrics can be developed that are aligned with the task of clinical care.   \n\n\nUsage Notes\n\nTo ensure that all researchers understand how to use the data, we provided instructions in the Readme file in the Open Science Framework (Data Citation 1).\n\nto the evaluation process and contributed to the editing of the manuscript. D.D. oversaw study design and implementation and contributed to writing and editing of manuscript.\n\n\nAdditional Information\n\nCompeting interests: The authors declare no competing interests. Publisher's note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons. \n\n\n| 5:180251 | DOI: 10.1038/sdata.2018.251\n\nFigure 1 .\n1Flow Diagram of VQA-RAD build. www.nature.com/sdata/ SCIENTIFIC DATA | 5:180251 | DOI: 10.1038/sdata.2018.251\n\n\n| 5:180251 | DOI: 10.1038/sdata.2018.251\n\nFigure 2 .\n2Breakdown of different types of Closed vs Open-ended free form questions shows that certain question types are more likely to be open-ended: positional, counting questions and other. www.nature.com/sdata/ SCIENTIFIC DATA | 5:180251 | DOI: 10.1038/sdata.2018.251\n\n\nHow to cite this article: Lau, J. J. et al. A dataset of clinically generated visual questions and answers about radiology images. Sci. Data. 5:180251 doi: 10.1038/sdata.2018.251 (2018).\n\n\nCommons Public Domain Dedication waiver http://creativecommons.org/publicdomain/ zero/1.0/ applies to the metadata files made available in this article. This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply 2018 www.nature.com/sdata/ SCIENTIFIC DATA | 5:180251 | DOI: 10.1038/sdata.2018.251\n\n\nPositional reasoning position or location of an object or organ, including what side of a patient, in respect to the image borders, or relative to other objects in the image Color signal intensity including enhancement or opaqueness Size measurement of size of an object, e.g., enlargement, atrophy Counting focusing on a quantity of objects, e.g., number of lesions Other catch-all categorization for questions that do not fall into the previous categories Answer Type Close-ended yes/no and other limited choices. For example, \"Is the mass on the left or right?\" Open-ended Do not have a limited question structure and could have multiple correct answersTable 1. Naturally occurring question and answer types about radiology images.Attribute Other \nother types of description questions \n\nwww.nature.com/sdata/ \n\nSCIENTIFIC DATA | 5:180251 | DOI: 10.1038/sdata.2018.251 \n\n\n\nclinically relevant, not necessarily the most common. Calculating accuracies for different question types can highlight unique features of algorithm designs. For example, the SAN_RAD model scored high for open and closed ended size questions suggesting for this type of task, the SAN design might have certain advantages over the MCB design. However, further refinement is needed to understand these differences, as it is not clear if the SAN model is able to better recognize image regions of interest or is better at answering questions.YES \nNO \nMCB_VQA1.0 MCB_CLEF SAN_CLEF MCB_CLEF + RAD SAN_CLEF + RAD MCB_RAD SAN_RAD \n\nABN \n33.3% \n58.3% \n58.3% \n4.2% \n29.2% \n58.3% \n66.7% \n79.2% \n62.5% \n\nATTRIB \n33.3% \n66.7% 50.0% \n0% \n16.7% \n50.0% \n50.0% \n50.0% \n16.7% \n\nCOLOR \n50.0% \n50.0% \n50.0% \n0% \n50.0% \n50.0% \n50.0% \n50.0% \n100.0% \n\nCOUNT \n0% \n50.0% 0% \n0% \n50.0% \n0% \n0% \n0% \n0% \n\nMODALITY 33.3% \n46.7% \n46.7% \n0% \n40.0% \n66.7% \n60.0% \n60.0% \n66.7% \n\nORGAN \n100.0% 0% \n100.0% \n0% \n0% \n50.0% \n100.0% \n100.0% \n100.0% \n\nOTHER \n55.6% \n11.1% \n33.3% \n0% \n11.1% \n44.4% \n22.2% \n33.3% \n11.1% \n\nPLANE \n41.7% \n41.7% \n25.0% \n0% \n50.0% \n50.0% \n58.3% \n41.7% \n50.0% \n\nPOS \n33.3% \n0% \n33.3% \n0% \n0% \n66.7% \n33.3% \n66.7% \n66.7% \n\nPRES \n41.8% \n58.2% \n41.8% \n40.5% \n50.6% \n60.8% \n62.0% \n65.8% \n58.2% \n\nSIZE \n57.7% \n34.6% \n50.0% \n7.7% \n42.3% \n57.7% \n61.5% \n50.0% \n69.2% \n\nSimple Acc \n42.8% \n48.9% \n44.4% \n19.4% \n41.1% \n57.8% \n58.9% \n60.6% \n57.2% \n\nMean Acc \n43.6% \n37.9% \n44.4% \n4.8% \n30.9% \n50.4% \n51.3% \n54.2% \n54.6% \n\nBLEU \n0.804 \n0.832 \n0.812 \n0.003 \n0.020 \n0.167 \n0.167 \n0.168 \n0.622 \n\nTable 2. Accuracy of the systems' answers to the closed-ended free-form questions sub-grouped into \nquestion types. Overall, MCB_RAD and SAN_RAD performed the best. MCB_VQA1.0 shows that one of the \n\nbaseline systems is not better than all yes/no baselines. \n\nwww.nature.com/sdata/ \n\nSCIENTIFIC DATA | 5:180251 | DOI: 10.1038/sdata.2018.251 \n\n\nTable 3 .\n3Accuracy of the systems' answers to the open-ended free-form questions sub-grouped into questions types. Overall, modality and plane questions are predicted well. Deficiencies are seen in other areas like Abnormality, Color, and Counting. MCB_VQA1.0 shows how a baseline system's BLEU scores disagree with manual judgments. The image is taken in what plane? axial dwi What plane is the above image acquired in? axial axial Is this a CT or an MRI? MRI ct Was a CT or MRI used to take the above image? MRI t2 weighted Does the liver show an enhancing mass or lesion? No normal Is there an enhancing lesion in the liver? No no What are the hyperdense lesions noted at the edges of the aorta? Calcified atherosclerosis calcifications What are the hyperintensities surrounding the aorta? Calcified atherosclerosis ribsQuestion \nAnswer (GS) \nAnswer (MCB_RAD) \n\nIs the heart enlarged? \nYes \nyes \n\nIs cardiomegaly present? \nYes \nno \n\n\n\nTable 4 .\n4Effect of paraphrasing on models. Sample of free-form and paraphrased questions that resulted in differing predicted answers from models. MCB model is trained on MCB_RAD. GS represents gold standard answer generated by clinical annotators. www.nature.com/sdata/ SCIENTIFIC DATA | 5:180251 | DOI: 10.1038/sdata.2018.251\nAcknowledgementsWe thank Dr James G. Smirniotopoulos, Chief Editor, MedPix\u00ae, Former Professor and Chair of Radiology (retired), Uniformed Services University of the Health Sciences for help with MedPix and radiology questions. We thank Dr Yassine Mrabet for insightful conversations and would also like to thank the following people for their assistance on this effort: G. Thomas Brown, Katherine Chen, Tiffany Chen, Nisarg Chhaya, Shazia Dharssi, Eric Ding, Joseph Featherall, Jonathan Gammel, Eileen Hu-Wang, Fabricio Kury, Lucy Li, Kathee Liang, Liesl Matzka, David Stein, Alison Treichel. This work was partially supported by the Intramural Research Program of the National Library of Medicine, National Institutes of Health. This research was made possible through the National Institutes of Health (NIH) Medical Research Scholars Program, a public-private partnership supported jointly by the NIH and generous contributions to the Foundation for the NIH from the Doris Duke Charitable Foundation, Genentech, the American Association for Dental Research, the Colgate-Palmolive Company, Elsevier, alumni of student research programs, and other individual supporters via contributions to the Foundation for the National Institutes of Health. For a complete list, please visit the Foundation website at: http://fnih.org/what-wedo/current-education-and-training-programs/mrsp\n. G Litjens, A Survey on Deep Learning in Medical Image Analysis. Med. Image Anal. 42Litjens, G. et al. A Survey on Deep Learning in Medical Image Analysis. Med. Image Anal. 42, 60-88 (2017).\n\nThe Multimodal Brain Tumor Image Segmentation Benchmark (BRATS). B H Menze, IEEE Trans Med Imaging. 34Menze, B. H. et al. The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS). IEEE Trans Med Imaging 34, 1993-2024 (2015).\n\nLearning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation. H.-C Shin, 10.1109/CVPR.2016.274CVPRProceedings of the IEEE conference on computer vision and pattern recognition 2497-2506. the IEEE conference on computer vision and pattern recognition 2497-2506Shin, H.-C. et al. Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation. in Proceedings of the IEEE conference on computer vision and pattern recognition 2497-2506, http://dx.doi.org/10.1109/ CVPR.2016.274CVPR (2016).\n\nVqa: Visual question answering. S Antol, 10.1007/s11263-016-0966-6Proc. IEEE Int. Conf. Comput. Vis. 2425-2433. IEEE Int. Conf. Comput. Vis. 2425-2433Antol, S. et al. Vqa: Visual question answering. Proc. IEEE Int. Conf. Comput. Vis. 2425-2433, http://dx.doi.org/10. 1007/s11263-016-0966-6 (2015).\n\nMaking the V in VQA matter: Elevating the role of image understanding in visual question answering. Y Goyal, T Khot, D Summers-Stay, D Batra, D Parikh, Proceedings of the IEEE 30 th conference on computer vision and pattern recognition. the IEEE 30 th conference on computer vision and pattern recognitionCVPRGoyal, Y., Khot, T., Summers-Stay, D., Batra, D. & Parikh, D. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. in Proceedings of the IEEE 30 th conference on computer vision and pattern recognition 6325-6334 (CVPR, 2017).\n\nAre You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering. H Gao, 10.1145/2733373.2807418Adv. Neural Inf. Process. Syst. Gao, H. et al. Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering. Adv. Neural Inf. Process. Syst 2296-2304, http://dx.doi.org/10.1145/2733373.2807418 (2015).\n\nVisual7W: Grounded Question Answering in Images. Y Zhu, O Groth, M Bernstein, L Fei-Fei, 10.1109/CVPR.2016.540CVPRProceedings of the IEEE conference on computer vision and pattern recognition 4995-5004. the IEEE conference on computer vision and pattern recognition 4995-5004Zhu, Y., Groth, O., Bernstein, M. & Fei-Fei, L. Visual7W: Grounded Question Answering in Images. In Proceedings of the IEEE conference on computer vision and pattern recognition 4995-5004, http://dx.doi.org/10.1109/CVPR.2016.540CVPR (2016).\n\nVisual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. R Krishna, Int. J. Comput. Vis. 123Krishna, R. et al. Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. Int. J. Comput. Vis. 123, 32-73 (2017).\n\nDeep Compositional Question Answering with Neural Module Networks. J Andreas, M Rohrbach, T Darrell, D Klein, Andreas, J., Rohrbach, M., Darrell, T. & Klein, D. Deep Compositional Question Answering with Neural Module Networks, Preprint at https://arxiv.org/abs/1511.02799 (2015).\n\nCLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. J Johnson, Proceedings of the IEEE 30 th conference on computer vision and pattern recognition. the IEEE 30 th conference on computer vision and pattern recognitionJohnson, J. et al. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE 30 th conference on computer vision and pattern recognition 1988-1997CVPR (2017).\n\nMicrosoft COCO: Common objects in context. T Y Lin, Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics). 8693LNCSLin, T. Y. et al. Microsoft COCO: Common objects in context. Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics) 8693 LNCS, 740-755 (2014).\n\nAn Analysis of Visual Question Answering Algorithms. K Kafle, C Kanan, Kafle, K. & Kanan, C. An Analysis of Visual Question Answering Algorithms, Preprint at https://arxiv.org/abs/1703.09684 (2017).\n\nVizWiz Grand Challenge: Answering Visual Questions from Blind People. D Gurari, Gurari, D. et al. VizWiz Grand Challenge: Answering Visual Questions from Blind People, Preprint at https://arxiv.org/abs/ 1802.08218 (2018).\n\nOverview of ImageCLEF 2018 Medical Domain Visual Question Answering Task. S A Hasan, 2125/paper_212.pdfHasan, S. A. et al. Overview of ImageCLEF 2018 Medical Domain Visual Question Answering Task, Preprint at http://ceur-ws.org/ Vol-2125/paper_212.pdf (2018).\n\nData augmentation for visual question answering. K Kafle, M Yousefhussien, C Kanan, Proceedings of the 10th International Conference on Natural Language Generation. the 10th International Conference on Natural Language GenerationKafle, K., Yousefhussien, M. & Kanan, C. Data augmentation for visual question answering. In Proceedings of the 10th Inter- national Conference on Natural Language Generation 198-202 (2017).\n\nMultimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding. A Fukui, 10.18653/v1/D16-1044Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingACLFukui, A. et al. Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding. in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing 457-468, http://dx.doi.org/10.18653/v1/D16-1044 (ACL, 2016).\n\nStacked Attention Networks for Image Question Answering. Z Yang, X He, J Gao, L Deng, A Smola, 10.1109/CVPR.2016.10Proceedings of the IEEE conference on computer vision and pattern recognition 21-29. the IEEE conference on computer vision and pattern recognition 21-29Yang, Z., He, X., Gao, J., Deng, L. & Smola, A. Stacked Attention Networks for Image Question Answering. in Proceedings of the IEEE conference on computer vision and pattern recognition 21-29, http://dx.doi.org/10.1109/CVPR.2016.10 CVPR (2016).\n\nDeep Residual Learning for Image Recognition. K He, X Zhang, S Ren, J Sun, 10.1109/CVPR.2016.90Proceedings of the IEEE conference on computer vision and pattern recognition 770-778. the IEEE conference on computer vision and pattern recognition 770-778He, K., Zhang, X., Ren, S. & Sun, J. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 770-778, http://dx.doi.org/10.1109/CVPR.2016.90 (2016).\n\nVery Deep Convolutional Networks for Large-Scale Image Recognition. K Simonyan, A Zisserman, Simonyan, K. & Zisserman, A. Very Deep Convolutional Networks for Large-Scale Image Recognition, Preprint at https://arxiv.org/ abs/1409.1556 (2014).\n\nBLEU: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of Association for Computational Linguistics 311-318. the 40th Annual Meeting of Association for Computational Linguistics 311-318Papineni, K., Roukos, S., Ward, T. & Zhu, W. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of Association for Computational Linguistics 311-318, http://dx.doi.org/10.3115/1073083.1073135 (ACL 2002).\n\nData Citations. Data Citations\n\n. J Lau, S Gayen, D Demner-Fushman, 10.17605/OSF.IO/89KPSLau, J., Gayen, S. & Demner-Fushman, D. Open Science Framework https://doi.org/10.17605/OSF.IO/89KPS (2018).\n", "annotations": {"author": "[{\"end\":268,\"start\":154},{\"end\":384,\"start\":269},{\"end\":496,\"start\":385},{\"end\":606,\"start\":497},{\"end\":729,\"start\":607}]", "publisher": null, "author_last_name": "[{\"end\":165,\"start\":162},{\"end\":281,\"start\":276},{\"end\":393,\"start\":390},{\"end\":626,\"start\":612}]", "author_first_name": "[{\"end\":159,\"start\":154},{\"end\":161,\"start\":160},{\"end\":275,\"start\":269},{\"end\":389,\"start\":385},{\"end\":503,\"start\":497},{\"end\":611,\"start\":607}]", "author_affiliation": "[{\"end\":267,\"start\":167},{\"end\":383,\"start\":283},{\"end\":495,\"start\":395},{\"end\":605,\"start\":505},{\"end\":728,\"start\":628}]", "title": "[{\"end\":124,\"start\":1},{\"end\":853,\"start\":730}]", "venue": null, "abstract": "[{\"end\":2525,\"start\":1122}]", "bib_ref": "[{\"end\":3507,\"start\":3506},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3845,\"start\":3842},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3848,\"start\":3845},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3851,\"start\":3848},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3854,\"start\":3851},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3857,\"start\":3854},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3887,\"start\":3885},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3889,\"start\":3887},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3992,\"start\":3990},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4057,\"start\":4055},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4059,\"start\":4057},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4232,\"start\":4230},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5275,\"start\":5273},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16060,\"start\":16058},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21312,\"start\":21310},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21351,\"start\":21349},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22645,\"start\":22643},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25526,\"start\":25524},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25539,\"start\":25537}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29937,\"start\":29895},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30060,\"start\":29938},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30103,\"start\":30061},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30378,\"start\":30104},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30567,\"start\":30379},{\"attributes\":{\"id\":\"fig_5\"},\"end\":30924,\"start\":30568},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31799,\"start\":30925},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33713,\"start\":31800},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34652,\"start\":33714},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34983,\"start\":34653}]", "paragraph": "[{\"end\":4502,\"start\":2549},{\"end\":5219,\"start\":4504},{\"end\":6187,\"start\":5221},{\"end\":7221,\"start\":6189},{\"end\":7774,\"start\":7251},{\"end\":8836,\"start\":7776},{\"end\":9484,\"start\":8881},{\"end\":10513,\"start\":9499},{\"end\":11579,\"start\":10515},{\"end\":12448,\"start\":11614},{\"end\":12862,\"start\":12450},{\"end\":13052,\"start\":12864},{\"end\":13648,\"start\":13100},{\"end\":13678,\"start\":13650},{\"end\":14038,\"start\":13680},{\"end\":14764,\"start\":14040},{\"end\":14858,\"start\":14803},{\"end\":14940,\"start\":14868},{\"end\":15096,\"start\":14957},{\"end\":15266,\"start\":15098},{\"end\":15687,\"start\":15268},{\"end\":16062,\"start\":15689},{\"end\":16567,\"start\":16079},{\"end\":16982,\"start\":16628},{\"end\":17154,\"start\":16984},{\"end\":17821,\"start\":17156},{\"end\":18298,\"start\":17823},{\"end\":19111,\"start\":18300},{\"end\":19811,\"start\":19157},{\"end\":21032,\"start\":19859},{\"end\":21215,\"start\":21057},{\"end\":21919,\"start\":21229},{\"end\":22598,\"start\":21921},{\"end\":23400,\"start\":22600},{\"end\":24417,\"start\":23412},{\"end\":25114,\"start\":24419},{\"end\":26101,\"start\":25141},{\"end\":26242,\"start\":26103},{\"end\":27107,\"start\":26244},{\"end\":27824,\"start\":27109},{\"end\":28487,\"start\":27826},{\"end\":28658,\"start\":28503},{\"end\":28834,\"start\":28660},{\"end\":29062,\"start\":28861},{\"end\":29894,\"start\":29064}]", "formula": null, "table_ref": "[{\"end\":9483,\"start\":9476},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26241,\"start\":26227},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":26704,\"start\":26697}]", "section_header": "[{\"end\":2547,\"start\":2527},{\"end\":7231,\"start\":7224},{\"end\":7249,\"start\":7234},{\"end\":8879,\"start\":8839},{\"end\":9497,\"start\":9487},{\"end\":11612,\"start\":11582},{\"end\":13098,\"start\":13055},{\"end\":14801,\"start\":14767},{\"end\":14866,\"start\":14861},{\"end\":14955,\"start\":14943},{\"end\":16077,\"start\":16065},{\"end\":16590,\"start\":16570},{\"end\":16626,\"start\":16593},{\"end\":19155,\"start\":19114},{\"end\":19857,\"start\":19814},{\"end\":21055,\"start\":21035},{\"end\":21227,\"start\":21218},{\"end\":23410,\"start\":23403},{\"end\":25139,\"start\":25117},{\"end\":28501,\"start\":28490},{\"end\":28859,\"start\":28837},{\"end\":29949,\"start\":29939},{\"end\":30115,\"start\":30105},{\"end\":33724,\"start\":33715},{\"end\":34663,\"start\":34654}]", "table": "[{\"end\":31799,\"start\":31661},{\"end\":33713,\"start\":32341},{\"end\":34652,\"start\":34539}]", "figure_caption": "[{\"end\":29937,\"start\":29897},{\"end\":30060,\"start\":29951},{\"end\":30103,\"start\":30063},{\"end\":30378,\"start\":30117},{\"end\":30567,\"start\":30381},{\"end\":30924,\"start\":30570},{\"end\":31661,\"start\":30927},{\"end\":32341,\"start\":31802},{\"end\":34539,\"start\":33726},{\"end\":34983,\"start\":34665}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7127,\"start\":7121},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11699,\"start\":11693},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18417,\"start\":18410}]", "bib_author_first_name": "[{\"end\":36364,\"start\":36363},{\"end\":36620,\"start\":36619},{\"end\":36622,\"start\":36621},{\"end\":36885,\"start\":36881},{\"end\":37370,\"start\":37369},{\"end\":37737,\"start\":37736},{\"end\":37746,\"start\":37745},{\"end\":37754,\"start\":37753},{\"end\":37770,\"start\":37769},{\"end\":37779,\"start\":37778},{\"end\":38313,\"start\":38312},{\"end\":38625,\"start\":38624},{\"end\":38632,\"start\":38631},{\"end\":38641,\"start\":38640},{\"end\":38654,\"start\":38653},{\"end\":39183,\"start\":39182},{\"end\":39435,\"start\":39434},{\"end\":39446,\"start\":39445},{\"end\":39458,\"start\":39457},{\"end\":39469,\"start\":39468},{\"end\":39738,\"start\":39737},{\"end\":40162,\"start\":40161},{\"end\":40164,\"start\":40163},{\"end\":40520,\"start\":40519},{\"end\":40529,\"start\":40528},{\"end\":40737,\"start\":40736},{\"end\":40964,\"start\":40963},{\"end\":40966,\"start\":40965},{\"end\":41200,\"start\":41199},{\"end\":41209,\"start\":41208},{\"end\":41226,\"start\":41225},{\"end\":41660,\"start\":41659},{\"end\":42165,\"start\":42164},{\"end\":42173,\"start\":42172},{\"end\":42179,\"start\":42178},{\"end\":42186,\"start\":42185},{\"end\":42194,\"start\":42193},{\"end\":42668,\"start\":42667},{\"end\":42674,\"start\":42673},{\"end\":42683,\"start\":42682},{\"end\":42690,\"start\":42689},{\"end\":43163,\"start\":43162},{\"end\":43175,\"start\":43174},{\"end\":43403,\"start\":43402},{\"end\":43415,\"start\":43414},{\"end\":43425,\"start\":43424},{\"end\":43433,\"start\":43432},{\"end\":43926,\"start\":43925},{\"end\":43933,\"start\":43932},{\"end\":43942,\"start\":43941}]", "bib_author_last_name": "[{\"end\":36372,\"start\":36365},{\"end\":36628,\"start\":36623},{\"end\":36890,\"start\":36886},{\"end\":37376,\"start\":37371},{\"end\":37743,\"start\":37738},{\"end\":37751,\"start\":37747},{\"end\":37767,\"start\":37755},{\"end\":37776,\"start\":37771},{\"end\":37786,\"start\":37780},{\"end\":38317,\"start\":38314},{\"end\":38629,\"start\":38626},{\"end\":38638,\"start\":38633},{\"end\":38651,\"start\":38642},{\"end\":38662,\"start\":38655},{\"end\":39191,\"start\":39184},{\"end\":39443,\"start\":39436},{\"end\":39455,\"start\":39447},{\"end\":39466,\"start\":39459},{\"end\":39475,\"start\":39470},{\"end\":39746,\"start\":39739},{\"end\":40168,\"start\":40165},{\"end\":40526,\"start\":40521},{\"end\":40535,\"start\":40530},{\"end\":40744,\"start\":40738},{\"end\":40972,\"start\":40967},{\"end\":41206,\"start\":41201},{\"end\":41223,\"start\":41210},{\"end\":41232,\"start\":41227},{\"end\":41666,\"start\":41661},{\"end\":42170,\"start\":42166},{\"end\":42176,\"start\":42174},{\"end\":42183,\"start\":42180},{\"end\":42191,\"start\":42187},{\"end\":42200,\"start\":42195},{\"end\":42671,\"start\":42669},{\"end\":42680,\"start\":42675},{\"end\":42687,\"start\":42684},{\"end\":42694,\"start\":42691},{\"end\":43172,\"start\":43164},{\"end\":43185,\"start\":43176},{\"end\":43412,\"start\":43404},{\"end\":43422,\"start\":43416},{\"end\":43430,\"start\":43426},{\"end\":43437,\"start\":43434},{\"end\":43930,\"start\":43927},{\"end\":43939,\"start\":43934},{\"end\":43957,\"start\":43943}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":36552,\"start\":36361},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1739295},\"end\":36785,\"start\":36554},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.274CVPR\",\"id\":\"b2\",\"matched_paper_id\":2750623},\"end\":37335,\"start\":36787},{\"attributes\":{\"doi\":\"10.1007/s11263-016-0966-6\",\"id\":\"b3\",\"matched_paper_id\":3180429},\"end\":37634,\"start\":37337},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":8081284},\"end\":38217,\"start\":37636},{\"attributes\":{\"doi\":\"10.1145/2733373.2807418\",\"id\":\"b5\",\"matched_paper_id\":209217},\"end\":38573,\"start\":38219},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.540CVPR\",\"id\":\"b6\",\"matched_paper_id\":5714907},\"end\":39090,\"start\":38575},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":4492210},\"end\":39365,\"start\":39092},{\"attributes\":{\"id\":\"b8\"},\"end\":39647,\"start\":39367},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":15458100},\"end\":40116,\"start\":39649},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14113767},\"end\":40464,\"start\":40118},{\"attributes\":{\"id\":\"b11\"},\"end\":40664,\"start\":40466},{\"attributes\":{\"id\":\"b12\"},\"end\":40887,\"start\":40666},{\"attributes\":{\"doi\":\"2125/paper_212.pdf\",\"id\":\"b13\"},\"end\":41148,\"start\":40889},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":22162396},\"end\":41569,\"start\":41150},{\"attributes\":{\"doi\":\"10.18653/v1/D16-1044\",\"id\":\"b15\",\"matched_paper_id\":2840197},\"end\":42105,\"start\":41571},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.10\",\"id\":\"b16\",\"matched_paper_id\":8849206},\"end\":42619,\"start\":42107},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.90\",\"id\":\"b17\",\"matched_paper_id\":206594692},\"end\":43092,\"start\":42621},{\"attributes\":{\"id\":\"b18\"},\"end\":43336,\"start\":43094},{\"attributes\":{\"doi\":\"10.3115/1073083.1073135\",\"id\":\"b19\",\"matched_paper_id\":11080756},\"end\":43889,\"start\":43338},{\"attributes\":{\"id\":\"b20\"},\"end\":43921,\"start\":43891},{\"attributes\":{\"doi\":\"10.17605/OSF.IO/89KPS\",\"id\":\"b21\"},\"end\":44088,\"start\":43923}]", "bib_title": "[{\"end\":36617,\"start\":36554},{\"end\":36879,\"start\":36787},{\"end\":37367,\"start\":37337},{\"end\":37734,\"start\":37636},{\"end\":38310,\"start\":38219},{\"end\":38622,\"start\":38575},{\"end\":39180,\"start\":39092},{\"end\":39735,\"start\":39649},{\"end\":40159,\"start\":40118},{\"end\":41197,\"start\":41150},{\"end\":41657,\"start\":41571},{\"end\":42162,\"start\":42107},{\"end\":42665,\"start\":42621},{\"end\":43400,\"start\":43338}]", "bib_author": "[{\"end\":36374,\"start\":36363},{\"end\":36630,\"start\":36619},{\"end\":36892,\"start\":36881},{\"end\":37378,\"start\":37369},{\"end\":37745,\"start\":37736},{\"end\":37753,\"start\":37745},{\"end\":37769,\"start\":37753},{\"end\":37778,\"start\":37769},{\"end\":37788,\"start\":37778},{\"end\":38319,\"start\":38312},{\"end\":38631,\"start\":38624},{\"end\":38640,\"start\":38631},{\"end\":38653,\"start\":38640},{\"end\":38664,\"start\":38653},{\"end\":39193,\"start\":39182},{\"end\":39445,\"start\":39434},{\"end\":39457,\"start\":39445},{\"end\":39468,\"start\":39457},{\"end\":39477,\"start\":39468},{\"end\":39748,\"start\":39737},{\"end\":40170,\"start\":40161},{\"end\":40528,\"start\":40519},{\"end\":40537,\"start\":40528},{\"end\":40746,\"start\":40736},{\"end\":40974,\"start\":40963},{\"end\":41208,\"start\":41199},{\"end\":41225,\"start\":41208},{\"end\":41234,\"start\":41225},{\"end\":41668,\"start\":41659},{\"end\":42172,\"start\":42164},{\"end\":42178,\"start\":42172},{\"end\":42185,\"start\":42178},{\"end\":42193,\"start\":42185},{\"end\":42202,\"start\":42193},{\"end\":42673,\"start\":42667},{\"end\":42682,\"start\":42673},{\"end\":42689,\"start\":42682},{\"end\":42696,\"start\":42689},{\"end\":43174,\"start\":43162},{\"end\":43187,\"start\":43174},{\"end\":43414,\"start\":43402},{\"end\":43424,\"start\":43414},{\"end\":43432,\"start\":43424},{\"end\":43439,\"start\":43432},{\"end\":43932,\"start\":43925},{\"end\":43941,\"start\":43932},{\"end\":43959,\"start\":43941}]", "bib_venue": "[{\"end\":37078,\"start\":37006},{\"end\":37487,\"start\":37449},{\"end\":37941,\"start\":37873},{\"end\":38850,\"start\":38778},{\"end\":39901,\"start\":39833},{\"end\":41379,\"start\":41315},{\"end\":41847,\"start\":41776},{\"end\":42375,\"start\":42307},{\"end\":42873,\"start\":42803},{\"end\":43631,\"start\":43555},{\"end\":36442,\"start\":36374},{\"end\":36652,\"start\":36630},{\"end\":37004,\"start\":36917},{\"end\":37447,\"start\":37403},{\"end\":37871,\"start\":37788},{\"end\":38372,\"start\":38342},{\"end\":38776,\"start\":38689},{\"end\":39212,\"start\":39193},{\"end\":39432,\"start\":39367},{\"end\":39831,\"start\":39748},{\"end\":40268,\"start\":40170},{\"end\":40517,\"start\":40466},{\"end\":40734,\"start\":40666},{\"end\":40961,\"start\":40889},{\"end\":41313,\"start\":41234},{\"end\":41774,\"start\":41688},{\"end\":42305,\"start\":42222},{\"end\":42801,\"start\":42716},{\"end\":43160,\"start\":43094},{\"end\":43553,\"start\":43462},{\"end\":43905,\"start\":43891}]"}}}, "year": 2023, "month": 12, "day": 17}