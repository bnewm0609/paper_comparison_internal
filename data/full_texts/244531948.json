{"id": 244531948, "updated": "2023-02-06 15:50:09.389", "metadata": {"title": "An Anomaly Detection System via Moving Surveillance Robots with Human Collaboration", "authors": "[{\"first\":\"Muhammad\",\"last\":\"Zaheer\",\"middle\":[\"Zaigham\"]},{\"first\":\"Arif\",\"last\":\"Mahmood\",\"middle\":[]},{\"first\":\"M.\",\"last\":\"Khan\",\"middle\":[\"Haris\"]},{\"first\":\"Marcella\",\"last\":\"Astrid\",\"middle\":[]},{\"first\":\"Seung-Ik\",\"last\":\"Lee\",\"middle\":[]}]", "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)", "journal": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Autonomous anomaly detection is a fundamental step in visual surveillance systems, and so we have witnessed great progress in the form of various promising algorithms. Nonetheless, majority of prior algorithms assume static surveillance cameras that severely restricts the coverage of the system unless the number of cameras is exponentially increased, consequently increasing both the installation and the monitoring costs. In the current work we propose an anomaly detection system based on mobile surveillance cameras, i.e., moving robots which continuously navigate a target area. We compare the newly acquired test images with a database of normal images using geo-tags. For anomaly detection, a Siamese network is trained which analyses two input images for anomalies while ignoring the viewpoint differences. Further, our system is capable of updating the normal images database with human collaboration. Finally, we propose a new tester dataset that is captured by repeated visits of the robot over a constrained outdoor industrial target area. Our experiments demonstrate the effectiveness of the proposed system for anomaly detection using mobile surveillance robots.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccvw/ZaheerMKAL21", "doi": "10.1109/iccvw54120.2021.00293"}}, "content": {"source": {"pdf_hash": "9ef9aaa22f5176c73291a93705218871433172f9", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "3fc0bafe56559d7e7a6fecf8ee6c2f9064780db1", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9ef9aaa22f5176c73291a93705218871433172f9.txt", "contents": "\nAn Anomaly Detection System via Moving Surveillance Robots with Human Collaboration\n\n\nMuhammad Zaigham Zaheer \nUniversity of Science and Technology\nDaejeonSouth Korea\n\nElectronics and Telecommunications Research Institute\nDaejeonSouth Korea\n\nArif Mahmood \nInformation Technology University\nPakistan\n\nM Haris Khan \nMohamed bin Zayed University of Artificial Intelligence\nAbu Dhabi\n\nMarcella Astrid \nUniversity of Science and Technology\nDaejeonSouth Korea\n\nElectronics and Telecommunications Research Institute\nDaejeonSouth Korea\n\nSeung-Ik Lee \nUniversity of Science and Technology\nDaejeonSouth Korea\n\nElectronics and Telecommunications Research Institute\nDaejeonSouth Korea\n\nAn Anomaly Detection System via Moving Surveillance Robots with Human Collaboration\n10.1109/ICCVW54120.2021.00293\nAutonomous anomaly detection is a fundamental step in visual surveillance systems, and so we have witnessed great progress in the form of various promising algorithms. Nonetheless, majority of prior algorithms assume static surveillance cameras that severely restricts the coverage of the system unless the number of cameras is exponentially increased, consequently increasing both the installation and the monitoring costs. In the current work we propose an anomaly detection system based on mobile surveillance cameras, i.e., moving robots which continuously navigate a target area. We compare the newly acquired test images with a database of normal images using geo-tags. For anomaly detection, a Siamese network is trained which analyses two input images for anomalies while ignoring the viewpoint differences. Further, our system is capable of updating the normal images database with human collaboration. Finally, we propose a new tester dataset that is captured by repeated visits of the robot over a constrained outdoor industrial target area. Our experiments demonstrate the effectiveness of the proposed system for anomaly detection using mobile surveillance robots.\n\nIntroduction\n\nSurveillance systems are becoming indispensable towards making the modern cities safer. These systems not only aid in preventing crimes, but also enable the authorities to respond well in time in the face of unwanted events. In a typical surveillance system, cameras are installed at remote locations and human operators carefully monitor the live feed generated by these cameras to identify any malicious activity such as fire, stealing, shooting, accident, etc. Upon spotting an anomaly, the operator alerts the relevant authorities.\n\nA significant drawback of the manually monitored surveillance systems is the inability of human operators to continuously monitor video streams. Attention span of a human brain is often limited [17]. Moreover, the surveillance videos may not have any significant information most of the time as the anomalous activities do not occur often [18,26]. Therefore, a human operator may lose focus which is not desirable in such systems. Given the possibility of human error in surveillance operations, designing an autonomous system is of paramount importance. Therefore, anomaly detection in surveillance videos has been gaining popularity in computer vision. The desideratum is to design a trainable system that can learn to detect anomalies in the streams of videos/images and generate an alarm if an anomaly appears in the scene.\n\nA major limitation of recent anomaly detection solutions is that the corresponding systems assume stationary CCTV cameras [26,18]. Therefore, the performance of these systems could deteriorate on video streams captured with moving cameras. To thoroughly monitor a relatively larger area, we require significantly large quantity of stationary cameras thereby resulting in increased initial deployment cost as well as recurring maintenance, energy and monitoring expenditures. Owing to current advancements in robotics, surveillance technologies are also adopting more sophisticated methods such as surveillance robots [2,6]. However, the moving camera surveillance introduces several classical computer vision nuisances such as, moving backgrounds, motion blur, illumination variations, moving objects, etc. [19]. Existing approaches [4,16,26,25,24,27], address anomaly detection problem only when video stream is captured from a stationary camera and could likely underperform under aforementioned challenges.\n\nAnomaly detection is often seen as one-class classifi-cation problem in which a model is trained only on normal data [7,24]. It is because the normal data is abundantly available as compared to the anomaly examples. At test time, both normal and anomalous examples are used to simulate the real-world situations of anomalies appearing suddenly among the normal happenings. As the number of views increases for the case of moving camera with a surveillance robot, learning one class classifier becomes more and more challenging especially for the detection of anomalous objects. Illumination variations may cause the whole background to be predicted as anomalous.\n\nAnother limitation of existing approaches is their limited capability of continual learning after training. Unlike the machine learning architectures for usual object detection or other similar problems which can be trained and run in controlled environments, an anomaly detection system requires deployment in dynamic outdoor environments. This essentially requires that the definition of anomalies may need to be updated as and when necessary. For example, a car parked in the background, a new solar panel installed over a parking lot, or flower pots placed or removed from certain locations, etc. can be perceived as anomalous by a system and need to be manually changed thus requiring human intervention.\n\nTo this end, we aim to design an approach capable of performing well on the video data captured with moving camera surveillance robots as well as supporting the lifelong adaptability through human interventions. The overall mechanism of our approach is illustrated in Figure 1. It takes live feed from a moving camera agent and passes it to a pre-trained anomaly detection algorithm. It is a network that takes a pair of images, one of which is the current image obtained via the camera agent and the other is its corresponding normalcy definition. This way, the system keeps looking for the major changes to be quantified as anomalies in the live feed. Once an anomaly is detected, the system notifies the human operator about the potential anomaly. The operator then decides whether the event/object flagged as anomalous is a false alarm or otherwise. If it is not a false alarm, which means that the anomaly is real, the operator may then alert the appropriate authorities to handle the matter. Otherwise, if it is a false alarm, the system receives the feedback, updates its normalcy definitions and continues testing the live feed.\n\nThe proposed system offers several benefits over the existing ones: 1) It can be updated based on human feedback, hence ensuring long-term learning. 2) It can detect anomalies in the imagery captured using moving camera agents. 3) It is a plug-and-play architecture, which means we can plug different change detection algorithms seamlessly without disintegration. 4) To handle a large number of views, each view is associated with a particular location point, viewing direction and angle. Such an arrangement avoids computa-tionally expensive image search in a large-scale surveillance environment with excessive number of backgrounds.\n\n\nRelated Work\n\nAnomaly detection in images and videos has recently become an active area of research. In this section, we discuss several categories of existing anomaly detection approaches, their application contexts and their differences with our approach.\n\n\nOne-Class Classification\n\nMost existing approaches utilize one-class classification (OCC) based training for anomaly detection, in which a normal class of data is used to carry out the training of oneclass classifiers. Then at test time, the instances deviating from the learned representations are classified as anomalies.\n\nConventionally, to carry out the OCC training, some works proposed the idea of either utilizing hand-crafted features [8,1,21,28,11], or use deep features extracted using pre-trained convolution models [16,12]. Recently, the idea of utilizing image regeneration based architectures has been explored by several researchers [3,13,22,5,9,10,23,14,15] to learn to reconstruct normal data in an unsupervised fashion. This way, at test time, it is expected to have more reconstruction error for the anomalous inputs. More recently, pseudo-supervised methods are also proposed in which pseudo anomalies are created using the normal training data and utilized to train binary classifiers [5,24].\n\nAll these approaches generally follow the model of onetime training, in which the definitions of anomalies or normal may not be updated without a fresh training. Consequently, these methods also lack human intervention mechanisms, therefore any false alarm may never be corrected unless the system is re-trained completely. In contrast, our proposed architecture is designed by keeping in mind such limitations of the OCC methods and can overcome such drawbacks.\n\n\nWeakly Supervised Anomaly Detection in Surveillance Videos\n\nThis is a relatively newer category of anomaly detection in which noisy annotations of videos are used to carry out the training in a weakly supervised setting [18,29,26,27,25,20]. In typical settings, video-level labels are provided for training. Normal videos are noise free, which means no anomalies are present in normal labeled videos. However, in anomalous labeled videos, most of the scenes are still normal and the temporal location of the anomalies is unknown. While this setting has its own set of challenges, our approach is significantly different from this category. First, this category of approaches is only applicable to videobased training. Particularly, the videos are recorded using static cameras. Secondly, similar to the OCC methods, such approaches are incapable of incorporating any new normal or anomalous into consideration without carrying out the retraining. Whereas our proposed approach considers moving camera imagery, while having the capability of incorporating human intervention based improvements.\n\n\nProposed Anomaly Detection System\n\nThe proposed system, as shown in Figure 1, has many parts, each of which is discussed next along with important implementation details.\n\n\nCamera Agent\n\nCamera agent is a moving robot (as shown in Figure  1A), that continuously records images and transfers to the server. It also labels each image with the corresponding Geo-tag, Pitch and Yaw (GPY), while not utilizing camera roll. The geo-tags are the local coordinates of the zone under surveillance. Mostly, the yaw values change according to the surveillance path, while pitch remains almost constant.\n\n\nReferences\n\n\nTemplates\n\n\nNormal Pairs\n\nAnomalous Pairs References Templates Figure 2. Normal and anomalous geo-taged image pairs taken from the proposed dataset. Each pair contains one normal image (reference) selected from the normalcy database while the second is captured by the robot (named as template) during test visits of the target area. Each pair of images is manually annotated for training and evaluation of the system.\n\n\nLearned Normalcy Definitions\n\nDuring training process, the camera agent ( Figure 1A) explores the whole under-surveillance zone and, for each geo-tag point, captures normal images. These images are saved in a database ( Figure 1H) and arranged according to the GPY values. This helps in retrieving the normal images previously saved at the similar location ( Figure 1I) and the pair is passed to the anomaly detection model ( Figure 1B).\n\n\nAnomaly Detection Model\n\nThe desideratum of this research is to develop an algorithm that may not only support human feedback based life-long learning, but also be able to handle moving camera agents. To this end, a change detection based machine learning algorithm is developed that can be used to match the existing normalcy definitions with the new incoming test data and produce a score corresponding to the quantification of change happened in the new scene. This way, anything unusual will be detected by our system.\n\nA Siamese based convolution network (SiamNet) is trained that can detect changes between two input images and is robust to view variations. An overview of this architecture is shown in Figure 1B. Based on this SiamNet, when the robot moves within a surveillance zone marked with geo-tags, each new test image is matched against the normal images already stored in the database. To simplify, a new incoming image during the surveillance operation along with an already stored normal image in the database is forwarded to the SiamNet as a pair to be matched. If the SiamNet identifies noticeable changes, the prediction score is higher which translates directly to the presence of some anomaly in the scene.\n\n\nArchitecture Details\n\nThe learning architecture used in our proposed framework is a conventional Siamese Network (SiamNet), designed to take a pair of images as input. In order to present the architecture details, we first define Conv(c i , c o , k, s) to represent a 2D convolution layer, where c i , c o , k, and s are input channels, output channels, kernal size and stride. We also define F C(c i , c o ) to represent fully connected linear layers with c i and c o as input and output nodes. Each branch of the SiamNet is given as : Conv(3, 32, 3, 0), Conv(32, 64, 3, 2), Conv(64, 64, 3, 2), F C(50176, 1024). The two branches are merged after difference using linear layers as: F C(1024, 256), F C(256, 1). ReLU is used after each layer, except for the last layer where Sigmoid is used to regress scores between 0 and 1.\n\n\nTraining\n\nTraining of the network is carried out using supervised examples of paired images to minimize binary cross entropy loss between the labels and the network predictions. Each pair is formed by using a geo-tagged template image captured by the robot and selecting the corresponding reference image from the learned normalcy definitions. Each template image is manually labeled as normal or anomalous based on its comparison with the reference image. The focus of the training is to learn a change detection model that can quantify the change between the two paired images. It may be noted that this way of training is different from the fully supervised binary classification as, in a binary classifier, the network may learn to focus on identifying specific anomalous objects. However, using change detection to quantify anomalies can be a generic solution as the definition of normalcy can be updated by replacing the reference images.\n\n\nHuman Collaboration\n\nMost of the existing machine learning algorithms for autonomous surveillance do not support long term learning. Which means if it is desired to change the normalcy definitions due to, for example, construction of a new building, a new installment in the under surveillance zone, etc. the whole model needs to be trained again with updated training examples. This is not only time consuming, but also cumbersome as optimizing the training model every time often requires expert supervision and several attempts. Therefore, we propose a model that does not require re-training every time and can sufficiently handle the new updates with minimal efforts. Hence, complementary to our change detection based machine learning algorithm, a human intervention mechanism is implemented that can update the normalcy definitions based on the feedback of the operator ( Figure  1E). This way, we can ensure that the normalcy definitions are up-to-date and that a false alarm will not happen repeatedly for the same anomaly.\n\nIn case the incoming image is labeled as anomalous (Figure 1C) by the anomaly detection model, a human operator will verify it for the presence of a real anomaly ( Figure 1E). In case the operator decides to mark the input image as normal, it will be added to the database of normal images at that particular GPY ( Figure 1G). Thus the anomaly detector will not raise a false alarm on the same event next time.\n\n\nExperiments\n\n\nProposed Dataset\n\nMost of the currently popular anomaly detection datasets do not take moving camera robots into account. Therefore in the current research we have proposed a new dataset captured by a moving robot in a constrained outdoor industrial environment. The dataset is captured over an outdoor target   \n\n\nExperimental Settings\n\nThe trainable architecture used in our proposed approach is a Siamese network. Stochastic Gradient Descent is used for the training with the learning rate of the system set to 10 \u22123 , weight decay of 5 \u00d7 10 \u22125 and momentum of 0.90. The network is trained for 50 epochs using a mini-batch size of 16 pairs. Training and testing are performed on a computer with Intel Core i7 processor, 128GB RAM, and NVIDIA TITAN Xp Graphics Card with 12GB memory.\n\n\nResults and Comparisons\n\nArea Under the Curve (AUC) % performance of our proposed anomaly detection approach is reported in Table 1 and the ROC curve plots are presented in Figure  3. Our approach demonstrates an AUC performance of 91.35%. In order to provide a vantage point to this perfor- Table 1. AUC % performance comparison of the proposed algorithm with three existing approaches. OGNet and ALOCC are recently proposed one-class classification methods that utilize only normal data for training. Whereas, CNN is a convolution based binary classifier that takes one image input at a time and predicts corresponding anomaly score. All these methods do not support human collaboration.\n\n\nMethod\n\nAUC % ALOCC [15] 78.08 OGNet [24] 83.24 CNN -Binary Classifier 91.01 Ours 91.35 mance, we also compare our approach to two popular conventional anomaly detection approaches including ALOCC [15] which is essentially a unified autoencoder and discriminator network and OGNet [24], as well as a convolution neural network (CNN) trained as a binary classifier. The network architecture of the binary classifier is set similar to a branch of our SiamNet. OGNet and ALOCC are oneclass classifiers and we train these models using only normal training examples within the training dataset. However, testing is carried out using both normal and anomalous examples from the test split. Only template images are used for training and testing while the reference images are completely discarded as these are only necessary/applicable in the case of change detection based anomaly detection algorithms. The performances of both OGNet and ALOCC are significantly lower which is understandable as these methods are not trained using actual anomaly examples. Binary classifier, on the other hand, demonstrates almost similar performance with our proposed approach. However, it may be noted that such a binary classifier is extremely dependent on the types of anomalies presented in the training set and cannot be generically used for different kinds of anomalies. Furthermore, it is also unable to support the proposed human collaboration mechanism. Whereas, our change detection based approach can not only support human intervention, but is also able to update the definitions of normalcy by updating the reference images.\n\n\nConclusions\n\nIn this work an anomaly detection system is proposed which is based on surveillance robots. The robots, while navigating a target area according to a local coordinate system, continuously capture images which are transmitted to a server. On the server, a normal images database is maintained which is arranged based on geo-tags, yaw and pitch of the robot camera. The test images and the corresponding normal images are matched using a Siamese deep neural network which is trained to find anomalies while ignoring the viewpoint differences. If the network detects the pres-ence of anomaly, the test image is sent to a collaborating human which verifies if the image is anomalous. Samples which are found to be not-anomalous but declared anomalous by the Siamese network are added to the database of the normal samples at the corresponding geo-tag. Thus over the time, a geo-tagged database of normal images is developed which helps reduce false alarms. The system has consistently achieved excellent performance in the experiments. A new dataset captured by surveillance robots by multiple visits of the target area is also proposed. This dataset is a tester version, which is recorded with an intention of introducing the problem of anomaly detection using moving robots to the community. In future, it is being planned to extend this dataset by adding more types of anomalies, complex environments and evaluation protocols. \n\n\nAcknowledgements\n\nFigure 1 .\n1Overall architecture of the proposed system: A) Moving camera agent that performs surveillance over a target area. B) Change detection based anomaly detection model using Siamese network. C) & D) If anomaly is detected, human operator is notified. E) The operator decides whether the alarm is false or not. F) If the alarm is real, operator may take necessary actions such as alerting the authorities. G) & H) If false alarm, the corresponding normalcy definition is updated and added to the Learned Normalcy Definitions. (I) Image retriever is used to retrieve the image corresponding to the the input image from the Learned Normalcy Definitions using Geo-tag, camera Pitch and camera Yaw (GPY).\n\nFigure 3 .\n3Receiver Operating Characteristic (ROC) curve of our proposed approach and other compared approaches including a convolutional network based binary classifier, OGNet[24], and ALOCC[15].\n\n\nThis work was supported by the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2017-0-00306, Development of Multimodal Sensorbased Intelligent Systems for Outdoor Surveillance Robots, 50%) and the UST Young Scientist Research Program 2020 through the University of Science and Technology. (No. 2020YS21, 50%)\n\nLearning object motion patterns for anomaly detection and improved object detection. Arslan Basharat, Alexei Gritai, Mubarak Shah, 2008 IEEE Conference on Computer Vision and Pattern Recognition. IEEEArslan Basharat, Alexei Gritai, and Mubarak Shah. Learning object motion patterns for anomaly detection and improved object detection. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages 1-8. IEEE, 2008.\n\nDecentralized multiple mobile depots route planning for replenishing persistent surveillance robots. Yifan Ding, Wenhao Luo, Katia Sycara, 2019 International Symposium on Multi-Robot and Multi-Agent Systems (MRS). IEEEYifan Ding, Wenhao Luo, and Katia Sycara. Decentralized multiple mobile depots route planning for replenishing per- sistent surveillance robots. In 2019 International Symposium on Multi-Robot and Multi-Agent Systems (MRS), pages 23- 29. IEEE, 2019.\n\nMoussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, The IEEE International Conference on Computer Vision (ICCV). Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. In The IEEE International Conference on Computer Vision (ICCV), October 2019.\n\nTube convolutional neural network (t-cnn) for action detection in videos. Rui Hou, Chen Chen, Mubarak Shah, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionRui Hou, Chen Chen, and Mubarak Shah. Tube convolu- tional neural network (t-cnn) for action detection in videos. In Proceedings of the IEEE International Conference on Computer Vision, pages 5822-5831, 2017.\n\nObject-centric auto-encoders and dummy anomalies for abnormal event detection in video. Fahad Radu Tudor Ionescu, Mariana-Iuliana Shahbaz Khan, Ling Georgescu, Shao, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionRadu Tudor Ionescu, Fahad Shahbaz Khan, Mariana-Iuliana Georgescu, and Ling Shao. Object-centric auto-encoders and dummy anomalies for abnormal event detection in video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7842-7851, 2019.\n\nCps oriented control design for networked surveillance robots with multiple physical constraints. Shuai Muhammad Umer Khan, Qixin Li, Zili Wang, Shao, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. 355Muhammad Umer Khan, Shuai Li, Qixin Wang, and Zili Shao. Cps oriented control design for networked surveillance robots with multiple physical constraints. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Sys- tems, 35(5):778-791, 2016.\n\nFuture frame prediction for anomaly detection-a new baseline. Wen Liu, Weixin Luo, Dongze Lian, Shenghua Gao, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionWen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. Fu- ture frame prediction for anomaly detection-a new baseline. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6536-6545, 2018.\n\nSomboon Hongeng, and Ramakant Nevatia. Event detection and analysis from video streams. G\u00e9rard Medioni, Isaac Cohen, Fran\u00e7ois Br\u00e9mond, IEEE Transactions. 238G\u00e9rard Medioni, Isaac Cohen, Fran\u00e7ois Br\u00e9mond, Somboon Hongeng, and Ramakant Nevatia. Event detection and anal- ysis from video streams. IEEE Transactions on pattern anal- ysis and machine intelligence, 23(8):873-889, 2001.\n\nAnomaly detection in video sequence with appearance-motion correspondence. Jean Trong-Nguyen Nguyen, Meunier, The IEEE International Conference on Computer Vision (ICCV). Trong-Nguyen Nguyen and Jean Meunier. Anomaly detec- tion in video sequence with appearance-motion correspon- dence. In The IEEE International Conference on Computer Vision (ICCV), October 2019.\n\nHybrid deep network for anomaly detection. Jean Trong Nguyen Nguyen, Meunier, arXiv:1908.06347arXiv preprintTrong Nguyen Nguyen and Jean Meunier. Hybrid deep network for anomaly detection. arXiv preprint arXiv:1908.06347, 2019.\n\nTrajectory-based anomalous event detection. Claudio Piciarelli, Christian Micheloni, Gian Luca Foresti, Technology. 1811Claudio Piciarelli, Christian Micheloni, and Gian Luca Foresti. Trajectory-based anomalous event detection. IEEE Transactions on Circuits and Systems for video Technology, 18(11):1544-1554, 2008.\n\nAbnormal event detection in videos using generative adversarial nets. Mahdyar Ravanbakhsh, Moin Nabi, Enver Sangineto, Lucio Marcenaro, Carlo Regazzoni, Nicu Sebe, 2017 IEEE International Conference on Image Processing (ICIP). IEEEMahdyar Ravanbakhsh, Moin Nabi, Enver Sangineto, Lu- cio Marcenaro, Carlo Regazzoni, and Nicu Sebe. Abnormal event detection in videos using generative adversarial nets. In 2017 IEEE International Conference on Image Process- ing (ICIP), pages 1577-1581. IEEE, 2017.\n\nUnsupervised behaviorspecific dictionary learning for abnormal event detection. Weifeng Huamin Ren, Liu, Sergio S\u00f8ren Ingvor Olsen, Thomas B Escalera, Moeslund, BMVC. Huamin Ren, Weifeng Liu, S\u00f8ren Ingvor Olsen, Sergio Es- calera, and Thomas B Moeslund. Unsupervised behavior- specific dictionary learning for abnormal event detection. In BMVC, pages 28-1, 2015.\n\nDeep-cascade: Cascading 3d deep neural networks for fast anomaly detection and localization in crowded scenes. Mohammad Sabokrou, Mohsen Fayyaz, Mahmood Fathy, Reinhard Klette, IEEE Transactions on Image Processing. 264Mohammad Sabokrou, Mohsen Fayyaz, Mahmood Fathy, and Reinhard Klette. Deep-cascade: Cascading 3d deep neu- ral networks for fast anomaly detection and localization in crowded scenes. IEEE Transactions on Image Processing, 26(4):1992-2004, 2017.\n\nAdversarially learned one-class classifier for novelty detection. Mohammad Sabokrou, Mohammad Khalooei, Mahmood Fathy, Ehsan Adeli, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionMohammad Sabokrou, Mohammad Khalooei, Mahmood Fathy, and Ehsan Adeli. Adversarially learned one-class classifier for novelty detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3379-3388, 2018.\n\nDeep appearance features for abnormal behavior detection in video. Sorina Smeureanu, Tudor Radu, Marius Ionescu, Bogdan Popescu, Alexe, International Conference on Image Analysis and Processing. SpringerSorina Smeureanu, Radu Tudor Ionescu, Marius Popescu, and Bogdan Alexe. Deep appearance features for abnormal behavior detection in video. In International Conference on Image Analysis and Processing, pages 779-789. Springer, 2017.\n\nHow effective is human video surveillance performance?. Noah Sulman, Thomas Sanocki, Dmitry Goldgof, Rangachar Kasturi, 2008 19th International Conference on Pattern Recognition. IEEENoah Sulman, Thomas Sanocki, Dmitry Goldgof, and Ran- gachar Kasturi. How effective is human video surveillance performance? In 2008 19th International Conference on Pattern Recognition, pages 1-3. IEEE, 2008.\n\nReal-world anomaly detection in surveillance videos. Waqas Sultani, Chen Chen, Mubarak Shah, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionWaqas Sultani, Chen Chen, and Mubarak Shah. Real-world anomaly detection in surveillance videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6479-6488, 2018.\n\nFab: A robust facial landmark detection framework for motionblurred videos. Keqiang Sun, Wayne Wu, Tinghao Liu, Shuo Yang, Quan Wang, Qiang Zhou, Zuochang Ye, Chen Qian, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionKeqiang Sun, Wayne Wu, Tinghao Liu, Shuo Yang, Quan Wang, Qiang Zhou, Zuochang Ye, and Chen Qian. Fab: A robust facial landmark detection framework for motion- blurred videos. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pages 5462-5471, 2019.\n\nWeakly-supervised video anomaly detection with robust temporal feature magnitude learning. Yu Tian, Guansong Pang, Yuanhong Chen, Rajvinder Singh, Johan W Verjans, Gustavo Carneiro, arXiv:2101.10030arXiv preprintYu Tian, Guansong Pang, Yuanhong Chen, Rajvinder Singh, Johan W Verjans, and Gustavo Carneiro. Weakly-supervised video anomaly detection with robust temporal feature mag- nitude learning. arXiv preprint arXiv:2101.10030, 2021.\n\nLearning fine-grained image similarity with deep ranking. Jiang Wang, Yang Song, Thomas Leung, Chuck Rosenberg, Jingbin Wang, James Philbin, Bo Chen, Ying Wu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJiang Wang, Yang Song, Thomas Leung, Chuck Rosenberg, Jingbin Wang, James Philbin, Bo Chen, and Ying Wu. Learn- ing fine-grained image similarity with deep ranking. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1386-1393, 2014.\n\nLearning deep representations of appearance and motion for anomalous event detection. Dan Xu, Elisa Ricci, Yan Yan, Jingkuan Song, Nicu Sebe, arXiv:1510.01553arXiv preprintDan Xu, Elisa Ricci, Yan Yan, Jingkuan Song, and Nicu Sebe. Learning deep representations of appearance and motion for anomalous event detection. arXiv preprint arXiv:1510.01553, 2015.\n\nDetecting anomalous events in videos by learning deep representations of appearance and motion. Computer Vision and Image Understanding. Dan Xu, Yan Yan, Elisa Ricci, Nicu Sebe, 156Dan Xu, Yan Yan, Elisa Ricci, and Nicu Sebe. Detecting anomalous events in videos by learning deep representations of appearance and motion. Computer Vision and Image Un- derstanding, 156:117-127, 2017.\n\nOld is gold: Redefining the adversarially learned one-class classifier training paradigm. Jin-Ha Muhammad Zaigham Zaheer, Marcella Lee, Seung-Ik Astrid, Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionMuhammad Zaigham Zaheer, Jin-ha Lee, Marcella Astrid, and Seung-Ik Lee. Old is gold: Redefining the adversarially learned one-class classifier training paradigm. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14183-14193, 2020.\n\nArif Mahmood, and Seung-Ik Lee. Cleaning label noise with clusters for minimally supervised anomaly detection. Jin-Ha Muhammad Zaigham Zaheer, Marcella Lee, Astrid, Conference on Computer Vision and Pattern Recognition Workshops. CVPRW2020Muhammad Zaigham Zaheer, Jin-ha Lee, Marcella Astrid, Arif Mahmood, and Seung-Ik Lee. Cleaning label noise with clusters for minimally supervised anomaly detection. In Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2020.\n\nClaws: Clustering assisted weakly supervised learning with normalcy suppression for anomalous event detection. Arif Muhammad Zaigham Zaheer, Marcella Mahmood, Seung-Ik Astrid, Lee, European Conference on Computer Vision. SpringerMuhammad Zaigham Zaheer, Arif Mahmood, Marcella Astrid, and Seung-Ik Lee. Claws: Clustering assisted weakly supervised learning with normalcy suppression for anoma- lous event detection. In European Conference on Computer Vision, pages 358-376. Springer, 2020.\n\nHochul Shin, and Seung-Ik Lee. A self-reasoning framework for anomaly detection using video-level labels. Arif Muhammad Zaigham Zaheer, Mahmood, IEEE Signal Processing Letters. 27Muhammad Zaigham Zaheer, Arif Mahmood, Hochul Shin, and Seung-Ik Lee. A self-reasoning framework for anomaly detection using video-level labels. IEEE Signal Processing Letters, 27:1705-1709, 2020.\n\nLearning semantic scene models by object classification and trajectory clustering. Tianzhu Zhang, Hanqing Lu, Stan Z Li, 2009 IEEE Conference on Computer Vision and Pattern Recognition. IEEETianzhu Zhang, Hanqing Lu, and Stan Z Li. Learning se- mantic scene models by object classification and trajectory clustering. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 1940-1947. IEEE, 2009.\n\nGraph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection. Jia-Xing Zhong, Nannan Li, Weijie Kong, Shan Liu, H Thomas, Ge Li, Li, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJia-Xing Zhong, Nannan Li, Weijie Kong, Shan Liu, Thomas H Li, and Ge Li. Graph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition, pages 1237-1246, 2019.\n", "annotations": {"author": "[{\"end\":242,\"start\":87},{\"end\":300,\"start\":243},{\"end\":381,\"start\":301},{\"end\":529,\"start\":382},{\"end\":674,\"start\":530}]", "publisher": null, "author_last_name": "[{\"end\":110,\"start\":96},{\"end\":255,\"start\":248},{\"end\":313,\"start\":303},{\"end\":397,\"start\":391},{\"end\":542,\"start\":539}]", "author_first_name": "[{\"end\":95,\"start\":87},{\"end\":247,\"start\":243},{\"end\":302,\"start\":301},{\"end\":390,\"start\":382},{\"end\":538,\"start\":530}]", "author_affiliation": "[{\"end\":167,\"start\":112},{\"end\":241,\"start\":169},{\"end\":299,\"start\":257},{\"end\":380,\"start\":315},{\"end\":454,\"start\":399},{\"end\":528,\"start\":456},{\"end\":599,\"start\":544},{\"end\":673,\"start\":601}]", "title": "[{\"end\":84,\"start\":1},{\"end\":758,\"start\":675}]", "venue": null, "abstract": "[{\"end\":1966,\"start\":789}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2717,\"start\":2713},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2862,\"start\":2858},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2865,\"start\":2862},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3474,\"start\":3470},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3477,\"start\":3474},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3968,\"start\":3965},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3970,\"start\":3968},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4159,\"start\":4155},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4184,\"start\":4181},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4187,\"start\":4184},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4190,\"start\":4187},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4193,\"start\":4190},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4196,\"start\":4193},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4199,\"start\":4196},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4479,\"start\":4476},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4482,\"start\":4479},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8216,\"start\":8213},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8218,\"start\":8216},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8221,\"start\":8218},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8224,\"start\":8221},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8227,\"start\":8224},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8301,\"start\":8297},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8304,\"start\":8301},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8421,\"start\":8418},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8424,\"start\":8421},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8427,\"start\":8424},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8429,\"start\":8427},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8431,\"start\":8429},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8434,\"start\":8431},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8437,\"start\":8434},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8440,\"start\":8437},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8443,\"start\":8440},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8779,\"start\":8776},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8782,\"start\":8779},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9474,\"start\":9470},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9477,\"start\":9474},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9480,\"start\":9477},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9483,\"start\":9480},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9486,\"start\":9483},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9489,\"start\":9486},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17786,\"start\":17782},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17803,\"start\":17799},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17963,\"start\":17959},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18047,\"start\":18043},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21732,\"start\":21728},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21747,\"start\":21743}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":21549,\"start\":20840},{\"attributes\":{\"id\":\"fig_2\"},\"end\":21748,\"start\":21550},{\"attributes\":{\"id\":\"fig_3\"},\"end\":22142,\"start\":21749}]", "paragraph": "[{\"end\":2517,\"start\":1982},{\"end\":3346,\"start\":2519},{\"end\":4357,\"start\":3348},{\"end\":5021,\"start\":4359},{\"end\":5732,\"start\":5023},{\"end\":6870,\"start\":5734},{\"end\":7507,\"start\":6872},{\"end\":7767,\"start\":7524},{\"end\":8093,\"start\":7796},{\"end\":8783,\"start\":8095},{\"end\":9247,\"start\":8785},{\"end\":10343,\"start\":9310},{\"end\":10516,\"start\":10381},{\"end\":10937,\"start\":10533},{\"end\":11371,\"start\":10979},{\"end\":11811,\"start\":11404},{\"end\":12336,\"start\":11839},{\"end\":13043,\"start\":12338},{\"end\":13871,\"start\":13068},{\"end\":14818,\"start\":13884},{\"end\":15853,\"start\":14842},{\"end\":16265,\"start\":15855},{\"end\":16594,\"start\":16300},{\"end\":17067,\"start\":16620},{\"end\":17759,\"start\":17095},{\"end\":19378,\"start\":17770},{\"end\":20820,\"start\":19394}]", "formula": null, "table_ref": "[{\"end\":17369,\"start\":17362}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1980,\"start\":1968},{\"attributes\":{\"n\":\"2.\"},\"end\":7522,\"start\":7510},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7794,\"start\":7770},{\"attributes\":{\"n\":\"2.2.\"},\"end\":9308,\"start\":9250},{\"attributes\":{\"n\":\"3.\"},\"end\":10379,\"start\":10346},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10531,\"start\":10519},{\"end\":10950,\"start\":10940},{\"end\":10962,\"start\":10953},{\"end\":10977,\"start\":10965},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11402,\"start\":11374},{\"attributes\":{\"n\":\"3.3.\"},\"end\":11837,\"start\":11814},{\"attributes\":{\"n\":\"3.3.1\"},\"end\":13066,\"start\":13046},{\"attributes\":{\"n\":\"3.3.2\"},\"end\":13882,\"start\":13874},{\"attributes\":{\"n\":\"3.4.\"},\"end\":14840,\"start\":14821},{\"attributes\":{\"n\":\"4.\"},\"end\":16279,\"start\":16268},{\"attributes\":{\"n\":\"4.1.\"},\"end\":16298,\"start\":16282},{\"attributes\":{\"n\":\"4.2.\"},\"end\":16618,\"start\":16597},{\"attributes\":{\"n\":\"4.3.\"},\"end\":17093,\"start\":17070},{\"end\":17768,\"start\":17762},{\"attributes\":{\"n\":\"5.\"},\"end\":19392,\"start\":19381},{\"attributes\":{\"n\":\"6.\"},\"end\":20839,\"start\":20823},{\"end\":20851,\"start\":20841},{\"end\":21561,\"start\":21551}]", "table": null, "figure_caption": "[{\"end\":21549,\"start\":20853},{\"end\":21748,\"start\":21563},{\"end\":22142,\"start\":21751}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6010,\"start\":6002},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10422,\"start\":10414},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10587,\"start\":10577},{\"end\":11024,\"start\":11016},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11457,\"start\":11448},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11603,\"start\":11594},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11742,\"start\":11733},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11809,\"start\":11800},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12532,\"start\":12523},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13600,\"start\":13581},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15710,\"start\":15700},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15917,\"start\":15906},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16028,\"start\":16019},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16179,\"start\":16170},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17252,\"start\":17243}]", "bib_author_first_name": "[{\"end\":22235,\"start\":22229},{\"end\":22252,\"start\":22246},{\"end\":22268,\"start\":22261},{\"end\":22677,\"start\":22672},{\"end\":22690,\"start\":22684},{\"end\":22701,\"start\":22696},{\"end\":23218,\"start\":23214},{\"end\":23233,\"start\":23225},{\"end\":23244,\"start\":23239},{\"end\":23259,\"start\":23249},{\"end\":23710,\"start\":23707},{\"end\":23720,\"start\":23716},{\"end\":23734,\"start\":23727},{\"end\":24165,\"start\":24160},{\"end\":24201,\"start\":24186},{\"end\":24220,\"start\":24216},{\"end\":24758,\"start\":24753},{\"end\":24784,\"start\":24779},{\"end\":24793,\"start\":24789},{\"end\":25211,\"start\":25208},{\"end\":25223,\"start\":25217},{\"end\":25235,\"start\":25229},{\"end\":25250,\"start\":25242},{\"end\":25713,\"start\":25707},{\"end\":25728,\"start\":25723},{\"end\":25744,\"start\":25736},{\"end\":26080,\"start\":26076},{\"end\":26415,\"start\":26411},{\"end\":26648,\"start\":26641},{\"end\":26670,\"start\":26661},{\"end\":26686,\"start\":26682},{\"end\":26691,\"start\":26687},{\"end\":26991,\"start\":26984},{\"end\":27009,\"start\":27005},{\"end\":27021,\"start\":27016},{\"end\":27038,\"start\":27033},{\"end\":27055,\"start\":27050},{\"end\":27071,\"start\":27067},{\"end\":27500,\"start\":27493},{\"end\":27524,\"start\":27518},{\"end\":27551,\"start\":27545},{\"end\":27553,\"start\":27552},{\"end\":27896,\"start\":27888},{\"end\":27913,\"start\":27907},{\"end\":27929,\"start\":27922},{\"end\":27945,\"start\":27937},{\"end\":28316,\"start\":28308},{\"end\":28335,\"start\":28327},{\"end\":28353,\"start\":28346},{\"end\":28366,\"start\":28361},{\"end\":28830,\"start\":28824},{\"end\":28847,\"start\":28842},{\"end\":28860,\"start\":28854},{\"end\":28876,\"start\":28870},{\"end\":29253,\"start\":29249},{\"end\":29268,\"start\":29262},{\"end\":29284,\"start\":29278},{\"end\":29303,\"start\":29294},{\"end\":29645,\"start\":29640},{\"end\":29659,\"start\":29655},{\"end\":29673,\"start\":29666},{\"end\":30107,\"start\":30100},{\"end\":30118,\"start\":30113},{\"end\":30130,\"start\":30123},{\"end\":30140,\"start\":30136},{\"end\":30151,\"start\":30147},{\"end\":30163,\"start\":30158},{\"end\":30178,\"start\":30170},{\"end\":30187,\"start\":30183},{\"end\":30694,\"start\":30692},{\"end\":30709,\"start\":30701},{\"end\":30724,\"start\":30716},{\"end\":30740,\"start\":30731},{\"end\":30753,\"start\":30748},{\"end\":30755,\"start\":30754},{\"end\":30772,\"start\":30765},{\"end\":31104,\"start\":31099},{\"end\":31115,\"start\":31111},{\"end\":31128,\"start\":31122},{\"end\":31141,\"start\":31136},{\"end\":31160,\"start\":31153},{\"end\":31172,\"start\":31167},{\"end\":31184,\"start\":31182},{\"end\":31195,\"start\":31191},{\"end\":31703,\"start\":31700},{\"end\":31713,\"start\":31708},{\"end\":31724,\"start\":31721},{\"end\":31738,\"start\":31730},{\"end\":31749,\"start\":31745},{\"end\":32112,\"start\":32109},{\"end\":32120,\"start\":32117},{\"end\":32131,\"start\":32126},{\"end\":32143,\"start\":32139},{\"end\":32453,\"start\":32447},{\"end\":32487,\"start\":32479},{\"end\":32501,\"start\":32493},{\"end\":33057,\"start\":33051},{\"end\":33091,\"start\":33083},{\"end\":33542,\"start\":33538},{\"end\":33576,\"start\":33568},{\"end\":33594,\"start\":33586},{\"end\":34028,\"start\":34024},{\"end\":34385,\"start\":34378},{\"end\":34400,\"start\":34393},{\"end\":34409,\"start\":34405},{\"end\":34411,\"start\":34410},{\"end\":34822,\"start\":34814},{\"end\":34836,\"start\":34830},{\"end\":34847,\"start\":34841},{\"end\":34858,\"start\":34854},{\"end\":34865,\"start\":34864},{\"end\":34876,\"start\":34874}]", "bib_author_last_name": "[{\"end\":22244,\"start\":22236},{\"end\":22259,\"start\":22253},{\"end\":22273,\"start\":22269},{\"end\":22682,\"start\":22678},{\"end\":22694,\"start\":22691},{\"end\":22708,\"start\":22702},{\"end\":23223,\"start\":23219},{\"end\":23237,\"start\":23234},{\"end\":23247,\"start\":23245},{\"end\":23264,\"start\":23260},{\"end\":23714,\"start\":23711},{\"end\":23725,\"start\":23721},{\"end\":23739,\"start\":23735},{\"end\":24184,\"start\":24166},{\"end\":24214,\"start\":24202},{\"end\":24230,\"start\":24221},{\"end\":24236,\"start\":24232},{\"end\":24777,\"start\":24759},{\"end\":24787,\"start\":24785},{\"end\":24798,\"start\":24794},{\"end\":24804,\"start\":24800},{\"end\":25215,\"start\":25212},{\"end\":25227,\"start\":25224},{\"end\":25240,\"start\":25236},{\"end\":25254,\"start\":25251},{\"end\":25721,\"start\":25714},{\"end\":25734,\"start\":25729},{\"end\":25752,\"start\":25745},{\"end\":26100,\"start\":26081},{\"end\":26109,\"start\":26102},{\"end\":26435,\"start\":26416},{\"end\":26444,\"start\":26437},{\"end\":26659,\"start\":26649},{\"end\":26680,\"start\":26671},{\"end\":26699,\"start\":26692},{\"end\":27003,\"start\":26992},{\"end\":27014,\"start\":27010},{\"end\":27031,\"start\":27022},{\"end\":27048,\"start\":27039},{\"end\":27065,\"start\":27056},{\"end\":27076,\"start\":27072},{\"end\":27511,\"start\":27501},{\"end\":27516,\"start\":27513},{\"end\":27543,\"start\":27525},{\"end\":27562,\"start\":27554},{\"end\":27572,\"start\":27564},{\"end\":27905,\"start\":27897},{\"end\":27920,\"start\":27914},{\"end\":27935,\"start\":27930},{\"end\":27952,\"start\":27946},{\"end\":28325,\"start\":28317},{\"end\":28344,\"start\":28336},{\"end\":28359,\"start\":28354},{\"end\":28372,\"start\":28367},{\"end\":28840,\"start\":28831},{\"end\":28852,\"start\":28848},{\"end\":28868,\"start\":28861},{\"end\":28884,\"start\":28877},{\"end\":28891,\"start\":28886},{\"end\":29260,\"start\":29254},{\"end\":29276,\"start\":29269},{\"end\":29292,\"start\":29285},{\"end\":29311,\"start\":29304},{\"end\":29653,\"start\":29646},{\"end\":29664,\"start\":29660},{\"end\":29678,\"start\":29674},{\"end\":30111,\"start\":30108},{\"end\":30121,\"start\":30119},{\"end\":30134,\"start\":30131},{\"end\":30145,\"start\":30141},{\"end\":30156,\"start\":30152},{\"end\":30168,\"start\":30164},{\"end\":30181,\"start\":30179},{\"end\":30192,\"start\":30188},{\"end\":30699,\"start\":30695},{\"end\":30714,\"start\":30710},{\"end\":30729,\"start\":30725},{\"end\":30746,\"start\":30741},{\"end\":30763,\"start\":30756},{\"end\":30781,\"start\":30773},{\"end\":31109,\"start\":31105},{\"end\":31120,\"start\":31116},{\"end\":31134,\"start\":31129},{\"end\":31151,\"start\":31142},{\"end\":31165,\"start\":31161},{\"end\":31180,\"start\":31173},{\"end\":31189,\"start\":31185},{\"end\":31198,\"start\":31196},{\"end\":31706,\"start\":31704},{\"end\":31719,\"start\":31714},{\"end\":31728,\"start\":31725},{\"end\":31743,\"start\":31739},{\"end\":31754,\"start\":31750},{\"end\":32115,\"start\":32113},{\"end\":32124,\"start\":32121},{\"end\":32137,\"start\":32132},{\"end\":32148,\"start\":32144},{\"end\":32477,\"start\":32454},{\"end\":32491,\"start\":32488},{\"end\":32508,\"start\":32502},{\"end\":32513,\"start\":32510},{\"end\":33081,\"start\":33058},{\"end\":33095,\"start\":33092},{\"end\":33103,\"start\":33097},{\"end\":33566,\"start\":33543},{\"end\":33584,\"start\":33577},{\"end\":33601,\"start\":33595},{\"end\":33606,\"start\":33603},{\"end\":34052,\"start\":34029},{\"end\":34061,\"start\":34054},{\"end\":34391,\"start\":34386},{\"end\":34403,\"start\":34401},{\"end\":34414,\"start\":34412},{\"end\":34828,\"start\":34823},{\"end\":34839,\"start\":34837},{\"end\":34852,\"start\":34848},{\"end\":34862,\"start\":34859},{\"end\":34872,\"start\":34866},{\"end\":34879,\"start\":34877},{\"end\":34883,\"start\":34881}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":9852942},\"end\":22569,\"start\":22144},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":198162223},\"end\":23037,\"start\":22571},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":102353587},\"end\":23631,\"start\":23039},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":206771624},\"end\":24070,\"start\":23633},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":54475483},\"end\":24653,\"start\":24072},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":10797823},\"end\":25144,\"start\":24655},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3865699},\"end\":25617,\"start\":25146},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1579146},\"end\":25999,\"start\":25619},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":201070056},\"end\":26366,\"start\":26001},{\"attributes\":{\"doi\":\"arXiv:1908.06347\",\"id\":\"b9\"},\"end\":26595,\"start\":26368},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1811633},\"end\":26912,\"start\":26597},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3477836},\"end\":27411,\"start\":26914},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":216612544},\"end\":27775,\"start\":27413},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":16951789},\"end\":28240,\"start\":27777},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3509717},\"end\":28755,\"start\":28242},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":10297336},\"end\":29191,\"start\":28757},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6073848},\"end\":29585,\"start\":29193},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1610415},\"end\":30022,\"start\":29587},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":204907092},\"end\":30599,\"start\":30024},{\"attributes\":{\"doi\":\"arXiv:2101.10030\",\"id\":\"b19\"},\"end\":31039,\"start\":30601},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3732882},\"end\":31612,\"start\":31041},{\"attributes\":{\"doi\":\"arXiv:1510.01553\",\"id\":\"b21\"},\"end\":31970,\"start\":31614},{\"attributes\":{\"id\":\"b22\"},\"end\":32355,\"start\":31972},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":215786155},\"end\":32938,\"start\":32357},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":233476329},\"end\":33425,\"start\":32940},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":221835703},\"end\":33916,\"start\":33427},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":221340880},\"end\":34293,\"start\":33918},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":2661367},\"end\":34708,\"start\":34295},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":81981674},\"end\":35310,\"start\":34710}]", "bib_title": "[{\"end\":22227,\"start\":22144},{\"end\":22670,\"start\":22571},{\"end\":23212,\"start\":23039},{\"end\":23705,\"start\":23633},{\"end\":24158,\"start\":24072},{\"end\":24751,\"start\":24655},{\"end\":25206,\"start\":25146},{\"end\":25705,\"start\":25619},{\"end\":26074,\"start\":26001},{\"end\":26639,\"start\":26597},{\"end\":26982,\"start\":26914},{\"end\":27491,\"start\":27413},{\"end\":27886,\"start\":27777},{\"end\":28306,\"start\":28242},{\"end\":28822,\"start\":28757},{\"end\":29247,\"start\":29193},{\"end\":29638,\"start\":29587},{\"end\":30098,\"start\":30024},{\"end\":31097,\"start\":31041},{\"end\":32445,\"start\":32357},{\"end\":33049,\"start\":32940},{\"end\":33536,\"start\":33427},{\"end\":34022,\"start\":33918},{\"end\":34376,\"start\":34295},{\"end\":34812,\"start\":34710}]", "bib_author": "[{\"end\":22246,\"start\":22229},{\"end\":22261,\"start\":22246},{\"end\":22275,\"start\":22261},{\"end\":22684,\"start\":22672},{\"end\":22696,\"start\":22684},{\"end\":22710,\"start\":22696},{\"end\":23225,\"start\":23214},{\"end\":23239,\"start\":23225},{\"end\":23249,\"start\":23239},{\"end\":23266,\"start\":23249},{\"end\":23716,\"start\":23707},{\"end\":23727,\"start\":23716},{\"end\":23741,\"start\":23727},{\"end\":24186,\"start\":24160},{\"end\":24216,\"start\":24186},{\"end\":24232,\"start\":24216},{\"end\":24238,\"start\":24232},{\"end\":24779,\"start\":24753},{\"end\":24789,\"start\":24779},{\"end\":24800,\"start\":24789},{\"end\":24806,\"start\":24800},{\"end\":25217,\"start\":25208},{\"end\":25229,\"start\":25217},{\"end\":25242,\"start\":25229},{\"end\":25256,\"start\":25242},{\"end\":25723,\"start\":25707},{\"end\":25736,\"start\":25723},{\"end\":25754,\"start\":25736},{\"end\":26102,\"start\":26076},{\"end\":26111,\"start\":26102},{\"end\":26437,\"start\":26411},{\"end\":26446,\"start\":26437},{\"end\":26661,\"start\":26641},{\"end\":26682,\"start\":26661},{\"end\":26701,\"start\":26682},{\"end\":27005,\"start\":26984},{\"end\":27016,\"start\":27005},{\"end\":27033,\"start\":27016},{\"end\":27050,\"start\":27033},{\"end\":27067,\"start\":27050},{\"end\":27078,\"start\":27067},{\"end\":27513,\"start\":27493},{\"end\":27518,\"start\":27513},{\"end\":27545,\"start\":27518},{\"end\":27564,\"start\":27545},{\"end\":27574,\"start\":27564},{\"end\":27907,\"start\":27888},{\"end\":27922,\"start\":27907},{\"end\":27937,\"start\":27922},{\"end\":27954,\"start\":27937},{\"end\":28327,\"start\":28308},{\"end\":28346,\"start\":28327},{\"end\":28361,\"start\":28346},{\"end\":28374,\"start\":28361},{\"end\":28842,\"start\":28824},{\"end\":28854,\"start\":28842},{\"end\":28870,\"start\":28854},{\"end\":28886,\"start\":28870},{\"end\":28893,\"start\":28886},{\"end\":29262,\"start\":29249},{\"end\":29278,\"start\":29262},{\"end\":29294,\"start\":29278},{\"end\":29313,\"start\":29294},{\"end\":29655,\"start\":29640},{\"end\":29666,\"start\":29655},{\"end\":29680,\"start\":29666},{\"end\":30113,\"start\":30100},{\"end\":30123,\"start\":30113},{\"end\":30136,\"start\":30123},{\"end\":30147,\"start\":30136},{\"end\":30158,\"start\":30147},{\"end\":30170,\"start\":30158},{\"end\":30183,\"start\":30170},{\"end\":30194,\"start\":30183},{\"end\":30701,\"start\":30692},{\"end\":30716,\"start\":30701},{\"end\":30731,\"start\":30716},{\"end\":30748,\"start\":30731},{\"end\":30765,\"start\":30748},{\"end\":30783,\"start\":30765},{\"end\":31111,\"start\":31099},{\"end\":31122,\"start\":31111},{\"end\":31136,\"start\":31122},{\"end\":31153,\"start\":31136},{\"end\":31167,\"start\":31153},{\"end\":31182,\"start\":31167},{\"end\":31191,\"start\":31182},{\"end\":31200,\"start\":31191},{\"end\":31708,\"start\":31700},{\"end\":31721,\"start\":31708},{\"end\":31730,\"start\":31721},{\"end\":31745,\"start\":31730},{\"end\":31756,\"start\":31745},{\"end\":32117,\"start\":32109},{\"end\":32126,\"start\":32117},{\"end\":32139,\"start\":32126},{\"end\":32150,\"start\":32139},{\"end\":32479,\"start\":32447},{\"end\":32493,\"start\":32479},{\"end\":32510,\"start\":32493},{\"end\":32515,\"start\":32510},{\"end\":33083,\"start\":33051},{\"end\":33097,\"start\":33083},{\"end\":33105,\"start\":33097},{\"end\":33568,\"start\":33538},{\"end\":33586,\"start\":33568},{\"end\":33603,\"start\":33586},{\"end\":33608,\"start\":33603},{\"end\":34054,\"start\":34024},{\"end\":34063,\"start\":34054},{\"end\":34393,\"start\":34378},{\"end\":34405,\"start\":34393},{\"end\":34416,\"start\":34405},{\"end\":34830,\"start\":34814},{\"end\":34841,\"start\":34830},{\"end\":34854,\"start\":34841},{\"end\":34864,\"start\":34854},{\"end\":34874,\"start\":34864},{\"end\":34881,\"start\":34874},{\"end\":34885,\"start\":34881}]", "bib_venue": "[{\"end\":22338,\"start\":22275},{\"end\":22783,\"start\":22710},{\"end\":23325,\"start\":23266},{\"end\":23808,\"start\":23741},{\"end\":24315,\"start\":24238},{\"end\":24883,\"start\":24806},{\"end\":25333,\"start\":25256},{\"end\":25771,\"start\":25754},{\"end\":26170,\"start\":26111},{\"end\":26409,\"start\":26368},{\"end\":26711,\"start\":26701},{\"end\":27139,\"start\":27078},{\"end\":27578,\"start\":27574},{\"end\":27991,\"start\":27954},{\"end\":28451,\"start\":28374},{\"end\":28950,\"start\":28893},{\"end\":29370,\"start\":29313},{\"end\":29757,\"start\":29680},{\"end\":30265,\"start\":30194},{\"end\":30690,\"start\":30601},{\"end\":31277,\"start\":31200},{\"end\":31698,\"start\":31614},{\"end\":32107,\"start\":31972},{\"end\":32596,\"start\":32515},{\"end\":33168,\"start\":33105},{\"end\":33646,\"start\":33608},{\"end\":34093,\"start\":34063},{\"end\":34479,\"start\":34416},{\"end\":34962,\"start\":34885},{\"end\":23862,\"start\":23810},{\"end\":24379,\"start\":24317},{\"end\":25397,\"start\":25335},{\"end\":28515,\"start\":28453},{\"end\":29821,\"start\":29759},{\"end\":30323,\"start\":30267},{\"end\":31341,\"start\":31279},{\"end\":32664,\"start\":32598},{\"end\":35026,\"start\":34964}]"}}}, "year": 2023, "month": 12, "day": 17}