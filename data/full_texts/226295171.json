{"id": 226295171, "updated": "2023-11-27 17:04:31.394", "metadata": {"title": "Visual Quality of 3D Meshes with Diffuse Colors in Virtual Reality: Subjective and Objective Evaluation", "authors": "[{\"first\":\"Yana\",\"last\":\"Nehm\u00b4e\",\"middle\":[]},{\"first\":\"Florent\",\"last\":\"Dupont\",\"middle\":[]},{\"first\":\"Jean-Philippe\",\"last\":\"Farrugia\",\"middle\":[]},{\"first\":\"Fellow\",\"last\":\"Patrick Le Callet\",\"middle\":[\"IEEE\"]},{\"first\":\"Guillaume\",\"last\":\"Lavou\u00b4e\",\"middle\":[]}]", "venue": "IEEE transactions on visualization and computer graphics", "journal": "IEEE transactions on visualization and computer graphics", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "\u2014Surface meshes associated with diffuse texture or color attributes are becoming popular multimedia contents. They provide a high degree of realism and allow six degrees of freedom (6DoF) interactions in immersive virtual reality environments. Just like other types of multimedia, 3D meshes are subject to a wide range of processing, e.g., simpli\ufb01cation and compression, which result in a loss of quality of the \ufb01nal rendered scene. Thus, both subjective studies and objective metrics are needed to understand and predict this visual loss. In this work, we introduce a large dataset of 480 animated meshes with diffuse color information, and associated with perceived quality judgments. The stimuli were generated from 5 source models subjected to geometry and color distortions. Each stimulus was associated with 6 hypothetical rendering trajectories (HRTs): combinations of 3 viewpoints and 2 animations. A total of 11520 quality judgments (24 per stimulus) were acquired in a subjective experiment conducted in virtual reality. The results allowed us to explore the in\ufb02uence of source models, animations and viewpoints on both the quality scores and their con\ufb01dence intervals. Based on these \ufb01ndings, we propose the \ufb01rst metric for quality assessment of 3D meshes with diffuse colors, which works entirely on the mesh domain. This metric incorporates perceptually-relevant curvature-based and color-based features. We evaluate its performance, as well as a number of Image Quality Metrics (IQMs), on two datasets: ours and a dataset of distorted textured meshes. Our metric demonstrates good results and a better stability than IQMs. Finally, we investigated how the knowledge of the viewpoint (i.e., the visible parts of the 3D model) may improve the results of objective metrics.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "3103721655", "acl": null, "pubmed": "33166254", "pubmedcentral": null, "dblp": "journals/tvcg/NehmeDFCL21", "doi": "10.1109/tvcg.2020.3036153"}}, "content": {"source": {"pdf_hash": "faa5beb4c227a805a8c9d1e186b217c76cc31827", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://hal.archives-ouvertes.fr/hal-03043911/document", "status": "GREEN"}}, "grobid": {"id": "ab51318ed05f9b1bbbc9e58134edec3f9438f3d3", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/faa5beb4c227a805a8c9d1e186b217c76cc31827.txt", "contents": "\nVisual Quality of 3D Meshes with Diffuse Colors in Virtual Reality: Subjective and Objective Evaluation\n\n\nYana Nehm yana.nehme@insa-lyon.fr \nFlorent Dupont florent.dupont@liris.cnrs.fr \nJean-Philippe Farrugia jean-philippe.farrugia@univ-lyon1.fr \nFellow, IEEEPatrick \u2022 Le Callet patrick.lecallet@univ-nantes.fr. \nGuillaume Lavou glavoue@liris.cnrs.fr \n\nCNRS\n\n\n\nUniv Lyon\nLIRIS\nFrance\n\n\nCNRS\nUniv Nantes\nLS2NFrance\n\nVisual Quality of 3D Meshes with Diffuse Colors in Virtual Reality: Subjective and Objective Evaluation\n36206625FA6EA99C448C63E0CC6107D310.1109/TVCG.2020.3036153Computer GraphicsPerceptionVirtual realityDiffuse Color3D MeshVisual Quality AssessmentSubjective Quality EvaluationObjective Quality EvaluationDatasetPerceptual Metric\nSurface meshes associated with diffuse texture or color attributes are becoming popular multimedia contents.They provide a high degree of realism and allow six degrees of freedom (6DoF) interactions in immersive virtual reality environments.Just like other types of multimedia, 3D meshes are subject to a wide range of processing, e.g., simplification and compression, which result in a loss of quality of the final rendered scene.Thus, both subjective studies and objective metrics are needed to understand and predict this visual loss.In this work, we introduce a large dataset of 480 animated meshes with diffuse color information, and associated with perceived quality judgments.The stimuli were generated from 5 source models subjected to geometry and color distortions.Each stimulus was associated with 6 hypothetical rendering trajectories (HRTs): combinations of 3 viewpoints and 2 animations.A total of 11520 quality judgments (24 per stimulus) were acquired in a subjective experiment conducted in virtual reality.The results allowed us to explore the influence of source models, animations and viewpoints on both the quality scores and their confidence intervals.Based on these findings, we propose the first metric for quality assessment of 3D meshes with diffuse colors, which works entirely on the mesh domain.This metric incorporates perceptually-relevant curvature-based and color-based features.We evaluate its performance, as well as a number of Image Quality Metrics (IQMs), on two datasets: ours and a dataset of distorted textured meshes.Our metric demonstrates good results and a better stability than IQMs.Finally, we investigated how the knowledge of the viewpoint (i.e., the visible parts of the 3D model) may improve the results of objective metrics.\n\nINTRODUCTION\n\nA S technological advances and capabilities in the field of computer graphics grow day by day, the need to master the manipulation, visualization and processing of 3D digital data increases at an equal pace.Indeed, the development of modeling software and acquisition devices (3D scan, reconstruction process) makes 3D graphics (mesh, voxel, point cloud) rich and realistic: complex models with millions of geometric primitives, enriched with various appearance attributes (color, texture, materiall, etc.).The way in which this 3D content is consumed is also evolving from standard screens to Virtual and Mixed Reality (VR/MR).However, the size and complexity of these rich 3D models often make their interactive visualization problematic.This is particularly the case in immersive environments (using head-mounted displays) and/or in case of online applications (where fast transmission is needed).Thus, to adapt the complexity of the 3D content for lightweight devices and to avoid latency due to transmission, diverse processing operations, including simplification and compression, are usually applied.These processes are lossy.They operate on both geometry and appearance attributes, which inevitably Manuscript submitted June 16, 2020.\n\nintroduce distortions that impact the perceived quality of the data and thus the quality of user experience (QoE).\n\nObjective quality metrics are thus critically needed to automatically predict the level of annoyance caused by these operations.Most metrics in the literature evaluate only geometric distortions (i.e. they consider meshes without appearance attributes), e.g.[1], [2], [3].When it comes to meshes with diffuse color information (either in the form of texture or vertex-colors), little work has been published [4] [5].Actually, for this kind of data, it is still unclear how color and geometry distortions affect quality.There is a lack of both objective metrics and subjective datasets.Another factor that has not yet been explored, and which is relevant in the case of 6 Degrees of Freedom (DoF) interactions, is how the viewpoint and movement of 3D models affect their perceived quality.\n\nIn this work, we address the problem of subjective and objective quality assessment of 3D models with diffuse colors.Our first goal is to produce a ground truth database of 3D graphics with quality judgments, and to understand the impact of several factors (such as the source models, distortions, viewpoints, and animations) on the perceived quality of these data.The experiment is based on a double stimulus impairment scale (DSIS) method and involves 480 animated stimuli created from five source models, each corrupted with color and geometry distortions and displayed in 3 different viewpoints that we animated with 2 short movements.We chose to conduct the experiment in Virtual of-the-art image and mesh quality metrics.The study on integrating the viewpoint in objective metrics is presented in section 7 along with its results.Finally, concluding remarks and perspectives are outlined in section 8.\n\n\nRELATED WORK\n\nIn this section, we provide an overview of existing datasets and metrics for predicting the perceived visual impact of distortions applied to graphical 3D content (3D meshes and point clouds).We are specifically interested in 3D content with diffuse colors, either in the form of texture maps or vertex/point colors.Note also that this state-of-the-art focuses on the visual impact of distortions applied on the 3D content itself (e.g.introduced by compression, simplification or filtering); it does not cover the visibility prediction of artifacts introduced during the rendering process (e.g. by global illumination approximation) or after rendering (e.g. by tone mapping).A dataset has been recently introduced that focuses on these types of artifacts [6].For a more complete survey of the field of perception and quality assessment in computer graphics, we refer the reader to [7].\n\n\nSubjective quality experiments and datasets\n\nAs stated in the introduction, datasets of human perceptual similarity judgments are of primary importance for understanding human behavior in evaluating perceived quality, as well as for training and benchmarking objective metrics.Many authors have conducted subjective quality assessment tests involving 3D meshes [2], [5], [8], [9], [10], [11], [12], [13], [14], [15], [16] or 3D point clouds [17], [18], [19], [20], [21].They considered a variety of methods: Absolute Category Rating (ACR) [11], [13], [22], Double-Stimulus Impairment Scale (DSIS) [2], [8], [9], [18], [19], [21] and Pairwise Comparison (PC) [5], [14], [15], [23].Very recently, a study [16] attempted to compare these subjective methods and showed that, for the particular case of 3D graphical content, the DSIS method tends to produce more accurate results than ACR (i.e., MOS with smaller confidence intervals).Existing subjective experiments also considered different ways of presenting 3D content: static images [8], animated content without interaction (usually low-speed rotation) [5], [9], [10], [16], [19], [20], [21], [22], [23] or interactive content [2], [11], [13], [14], [15], [17], [18].We denote that only in [15], [16], the experiments were conducted in a VR environment.So far, no attempts have been made to fully understand the impact of these design choices on the obtained mean opinion scores and their accuracy.\n\nUnfortunately, among the works presented above, very few have publicly released their datasets.For 3D meshes, the available datasets of mean opinion scores concern mostly geometry-only content [11], [12], [14] and are all rather small (resp.88, 26 and 30 models).The only public datasets involving 3D meshes with diffuse color information are provided by Guo et al. [5] and from Zerman et al. [22], and contain respectively 136 and 28 stimuli.For both cases, the color information is provided as texture maps.For colored point clouds, the available datasets are those provided by Javaheri et al. [20], Alexiou et al. [18] and Zerman et al. [22], and contain respectively 54, 244 and 136 stimuli.Note that the dataset from Zerman et al. [22] actually contains both meshes and point clouds, for a total of 164 stimuli that were rated in the same subjective test.All these datasets were generated through experiments conducted on screen.\n\nIn this work, we propose a dataset of 480 animated meshes with vertex colors.It is the largest one for quality assessment of 3D content with diffuse color information, and the first based on vertex color representation.Note that the mesh representation differs considerably from the point cloud representation in several aspects such as the way they are rendered, and the nature of commonly applied processing operations (and thus distortions).Our dataset allows us to provide an initial investigation on the influence of movement and viewpoint on the quality evaluation of 3D content.As in [15], [16] we considered a VR context and we used a DSIS method with 24 observers per stimulus, as recommended in [16].\n\n\nObjective quality metrics\n\nInspired by the vast amount of previous works on image and video quality assessment, several objective quality metrics have been introduced for 3D meshes.These are mostly full-reference (compare the distorted model with its original/reference) and follow the classical approach used in image quality assessment: local feature differences are calculated at vertex level, which are then pooled over the entire 3D model to obtain a global quality score.Existing metrics rely on various geometry characteristics: curvature [1], [24], dihedral angles [2] or roughness [3], [13].A survey [25] showed that MSDM2 [1], FMPD [3] and DAME [2] are excellent predictors of visual quality.Very recently, several authors proposed data-driven approaches based on machine learning [26], [27].Besides these works on global visual quality assessment (suited for supra-threshold distortions), Nader et al. [28] introduced a bottom-up visibility threshold predictor for 3D meshes.Guo et al. [29] also studied the local visibility of geometric artifacts and showed that curvature could be a good predictor of distortion visibility.The above-presented works only take geometry into account.With respect to 3D content with color or material information, very few works have been published.For meshes with diffuse texture, Tian et al. [4] and Guo et al. [5] proposed metrics based on a weighted combination of a global distance over geometry (Mean Squared Error (MSE) over mesh vertices in [4], and MSDM2 in [5]) and a global distance over texture image (MSE over texture pixels in [4], and SSIM in [5]).While the latter metric demonstrated good results on a subjective dataset of distorted textured meshes [5], combining errors computed on different domains (3D mesh and texture image) may be hazardous since many external factors (e.g.texel size, visibility of different parts) may impact the results.\n\nWith regard to this previous work [4], [5], we propose a data-driven metric that fully operates on the mesh domain, at vertex level.We consider an initial collection of perceptually-relevant features related to color and geometry.These features are taken from existing works on quality assessment of 3D meshes [1] and color images [30].A subset of these features is then optimally selected and combined, based on the results of our subjective study.Our metric provides excellent results and demonstrates a good stability, both for meshes with vertex colors and for textured meshes.\n\nFor quality assessment of 3D colored point clouds, a data-driven metric (PCQM) has been recently introduced by Meynet et al. [31].Our metric considers the same initial collection of color and geometric features as [31].Nevertheless, moving from point cloud domain to mesh domain implies major adaptations in the computation of these features.Other differences between our metric and PCQM are: the optimal selection and combination of features, the multiscale approach, and the viewpoint integration mechanism.Our metric is also related to the work of Vanhoey et al. [23], who proposed a quality metric for surface light-fields (i.e., per-vertex directional color).However, their metric considers color information only and is actually a simple MSE over both the directional and spatial domains.\n\nAll the metrics presented above are model-based, i.e., they operate on the 3D model itself (or its attributes like texture maps).However, to evaluate the quality of 3D content, several authors have also considered Image Quality Metrics (IQM) computed on rendered snapshots.For example, Yang et al. [32] and Caillaud et al. [33] respectively used image MSE and SSIM [34] to optimize textured mesh transmission.The advantage of image-based metrics over model-based metrics is their natural ability to handle the multimodal nature of data (geometry and color or texture information), as well as their natural incorporation of the complex rendering pipeline (computation of light material interactions, viewpoint selection and rasterization).On the other hand, IQMs pose other problems: (1) it is necessary to know in advance the final rendering of the stimuli in order to predict their quality with these metrics (because IQMs operate on 2D rendered snapshots).(2) IQMs also need the knowledge of the displayed viewpoint.Using them in a view-independent way introduces new parameters such as choice of the 2D views, or pooling of quality scores obtained from different views into a single global score.(3) IQMs are not practical for driving processing operations (e.g.mesh simplification).Model-based metrics are better suited for these operations since they operate on the mesh domain, i.e. the same representation space as mesh processing algorithms.This makes it possible to drive a process globally (on the entire mesh) as well as locally (at vertex level).( 4) Recent studies about these view-based approaches [5], [35] tend to show that their performance greatly depends on distortions and contents, and fall well behind model-based approaches.\n\n\nSUBJECTIVE EXPERIMENT\n\nWe conducted a large-scale subjective experiment to evaluate the visual impact of color and geometry distortions on the appearance of colored 3D models.Our dataset contains 480 animated 3D models created from five reference objects, on which are applied four types of distortion and two types of animation.This dataset extends the one presented in [16] composed of 80 stimuli.The subjective study was conducted in a virtual reality setting using a DSIS method.This section provides details on the subjective study.Fig. 1: Illustration of the 3D graphic source models and their selected viewpoints, respectively.Acronyms refer to Model Viewpoint.\n\n\nStimuli\n\n\n3D source model selection\n\nTo build our dataset of colored 3D models, we selected 5 high-resolution triangle meshes, each having diffuse color information represented by vertex colors (no texture mapping).These models were chosen so as to ensure a variety of shapes and colors.Table 1 details the characteristics of the models, while Figure 1 illustrates them.Note that, the sixth model (\"Dancing Drummer\") is not part of the dataset.It was used at the training stage of the experiment.\n\n\nDistortions\n\nThe source models presented above have been corrupted by the following 4 types of distortion applied on geometry and color.These selected distortions represent common simplification and compression operations typically used in 3D model modeling and post-processing.They are described below.\n\n\u2022 Uniform geometric quantization (QGeo): applied on geometry.This is a very common process for lossy compression.\u2022 Uniform LAB color quantization (QCol): applied on vertex colors.This is inspired by the usual 2D image compression processes.\n\n\u2022 \"Color-ignorant\" simplification (SGeo): a surface simplification algorithm that takes into account geometry only.It consists of iterative edge collapse operations driven by the quadric error metrics [36].\u2022 \"Color-aware\" simplification (SCol): a surface simplification algorithm that takes into account both geometry and color.It consists of iterative vertex removal operations, driven by a combination of (1) a geometry metric: the area loss caused by the removal; and (2) a color metric: the LAB distance between the color of the vertex to be removed and its interpolation after removal [37].Each distortion was applied with 4 different strengths, adjusted manually in order to span the whole range of visual quality from imperceptible levels to high levels of impairment (as is typically the case in subjective image quality studies [38]).Figure 2 illustrates some visual examples, while all details about the distortion parameters are available in Table 2. Thus, we generated 80 distorted models (5 source models \u00d7 4 distortion types \u00d7 4 strengths).\n\n\nStimuli generation\n\nIn existing subjective studies involving 3D content, different methods have been used to display the 3D models to the observers: still images, free interaction or animations.As shown by Rogowitz et al. [9], still images are not sufficient to evaluate the visual quality of 3D models.Thus, it is important that the object moves so that the observer can see the dynamic effects of shading on the shape.It is also important for the observer to see the whole object and not to focus on one single viewpoint.However, allowing free interaction leads to a cognitive overload which may alter human judgments.Inspired by the principle of pseudovideos, 3D animation is a simple yet efficient way to control  the interaction between subject and stimulus.So as a compromise, we selected, for each model, 3 viewpoints that we animated with 2 short movements.These 6 combinations of viewpoints and movements can be considered to be the hypothetical rendering trajectories (HRTs), concept introduced in [39] for free-viewpoint videos.HRTs reflect/represent the dimension of the test object related to the interactivity part such as the camera configurations, viewpoints, trajectories of 3D objects.\nAri_SGEO_4_VP3 Ari_SGEO_4_VP1 Ari_SCol_4_VP2 Fish_SGEO_3_VP1 Fish_SGEO_3_VP3 Fish_QGEO_2_VP1 Fish_QGEO_2_VP3 Aix_SCol_1_1 Chameleon_QCol_3_VP3 Samurai_QCol_4_VP2\nIn our experiment, the viewpoints were perceptually chosen and adjusted by experts, so that viewpoint 1 represents the one which contains the most geometry, color and semantic information.Viewpoint 2 and viewpoint 3 cover the remaining semantically relevant parts of the model (see Figure 1).For each viewpoint of a given stimulus, we applied 2 types of animation:\n\n\u2022 Slow rotation (R) of 15 degrees around the vertical axis in a clockwise and then in a counterclockwise direction.\u2022 Slow zoom in (Z) of 0.75 meters, followed by a zoom out.\n\nNote that, the animations we generate do not involve nonrigid transformations of the objects.Generating different stimulus orientations and animations will allow us to explore the impact of animations and viewpoints (HRT) on the perceived quality.\n\nOur dataset thus contains 480 dynamic stimuli (5 source models \u00d7 4 distortion types \u00d7 4 strengths \u00d7 3 viewpoints \u00d7 2 animation types).\n\n\nExperimental procedure and apparatus\n\nThe objective of our experiment is to produce a ground truth of subjective opinions on our set of 480 stimuli.We selected the Double Stimulus Impairment Scale (DSIS) methodology, as the subjective rating method.The observer sees the reference model and the same model impaired, simultaneously, side by side, for 10s and rates the impairment of the second stimulus in relation to the reference using a five-level impairment scale [40].This method was shown to be more stable and more accurate than the Absolute Category Rating with Hidden Reference (ACR-HR) method for assessing the quality of 3D models [16].Indeed, the authors showed that the presence of an explicit reference greatly improves the accuracy of results and reduces confidence intervals.This is due to the fact that people have less prior knowledge on 3D graphic quality compared to natural images/videos.\n\nWe chose to conduct the experiment in a fully immersive virtual environment (VE) since Virtual Reality is becoming a popular way of consuming and visualizing 3D content.We used the HTC Vive Pro headset, a high-end virtual reality headset with a resolution of 1440 x 1600 pixels per eye (2880 x 1600 pixels combined), a field of view of 110 degrees and a refresh rate of 90 Hz.The reference and the distorted model were rendered in a virtual scene, side by side, at a viewing distance fixed at 3 meters from the observer, under a given viewpoint and type of animation.Note that these 2 dynamic stimuli were specifically oriented in order to show exactly the same vertices of the 2 models at the same time.Their size is approximately 37 degrees of visual angle.Their material type complies with the Lambertian reflectance model (diffuse surfaces).The apparent brightness of such a surface to an observer is the same regardless of the observer's angle of view/position in the scene.The stimuli are visualized in a neutral virtual room (light gray walls) without shadows and under a directional light (all the vertices are illuminated as though the light were always from the same direction.).We aimed to design a neutral room so that the experimental environment does not influence the users' perception of the stimulus.\n\nWe integrated the rating billboard in the VE of our experiment.This board is displayed after the presentation of each pair of stimuli.There is no time limit to vote and the stimuli are not shown during that time.The same neutral room (light gray walls), utilized to show the stimuli, is used in the rating environment.To vote, the subject selects and saves the score using the trigger of the HTC Vive controller.As in [41], to facilitate the interaction with the rating panel, we attached a laser beam to the controller.Figure 3 illustrates the experimental environment.The entire experiment was developed with Unity3D using c# scripting.\n\n\nParticipants and training\n\nTraining: As recommended in the ITU-R BT.500 [42], the experiment started with training, during which observers familiarized themselves with the virtual environment and the task.We selected a training 3D model not included in our original test set: \"Dancing Drummer\" (see Figure 1) and generated 11 distorted models that span the whole range of distortions.After each stimulus (displayed for 10s), the rating panel is displayed for 5s.An example score assigned to this distortion is highlighted.We added a practice trial stage at the end of the training: we displayed 2 extra stimuli and asked the subject to rate the quality or the impairment.The results of these stimuli were not recorded.This stage was used to allow the observer to familiarize him/herself with the experimentation, to focus appropriately, and to ensure that observers fully understand the task of the experiment.\n\n\nCreation of test sessions:\n\nIn order to maintain a sufficient level of attention, we decided to limit the number of stimuli rated per participant to 160 stimuli out of 480.As we specifically aim to assess whether the animation influences the ratings, we decided to show each participant one viewpoint in both rotation and zoom animations.Furthermore, we wanted each participant to see all the reference models, where each model is corrupted by all the distortion types and levels.According to the recommendations of [16] about the required number of observers for assessing the quality of 3D data using the DSIS method, each stimulus must be rated by at least 24 observers.With this in mind, we have developed an algorithm which creates, for each subject, a random batch of 160 stimuli respecting 2 constraints: (1) each batch must contain 5 reference models x 4 distortion types x 4 strengths x 1 viewpoint x 2 animations.( 2) each stimulus must be rated 24 times (i.e.present in 24 batches).\n\nParticipants: A total of 72 (480*24/160) subjects took part in the experiment and they were remunerated.Participants were aged between 18 and 55.The majority were students from the University of Nantes, University of Lyon and LIRIS laboratory, while the rest were workers and professionals in different occupations.49 males and 23 females, 45 of whom had already tried (or were familiar with) a VR headset, they were naive about the purpose of the experiments.All observers had a normal or corrected to normal vision.\n\n\nDuration:\n\nTo avoid fatigue, boredom and cyber sickness, we divided the 160 stimuli into 2 sessions of 23 min each (informed consent/instructions + 11 training stimuli x (10s display + 5s rating) + 80 test stimuli x (10s display + \u22484s rating)).None of these sessions took place on the same day in order to prevent any learning effect between stimuli.Thus, these two sessions were held at least two days apart.The stimuli were displayed in a random order (3D models, distortion types, levels and animations all mixed) to each observer.Each stimulus was presented once; the observer was not able to replay the objects.\n\n\nANALYSIS OF SUBJECTIVE DATA\n\nThe following sections analyze and discuss the results of our experiment.First, we evaluate the agreement between the subjects.We also study their bias and inconsistency during the test.Then, we analyze the impact of main factors such as the reference models (also known as source contents), the viewpoints and the animations on the obtained opinion scores and their accuracy.\n\n\nScreening observers and computing mean ratings\n\nBefore starting any analysis, participants were screened using the ITU-R BT.500 recommendation [42].Applying this procedure on our data, we did not find any outlier participants.\n\nA common way to analyze the opinion scores of a DSIS test is to compute the Mean Opinion Score (MOS) of each stimulus.\nM OS e = 1 N N i=1 s ie(1)\ns ie refers to the score assigned by observer i to the stimulus e. N denotes the number of subjects.\n\nTo better understand the influence of user and source variability on the opinion scores, we used the recovery model based on maximum likelihood estimation (MLE) recently introduced by Li et al. [43].This approach recovers subjective quality scores from noisy raw measurements, by jointly estimating the subjective quality of impaired stimuli (true score), the bias and inconsistency of test subjects, and the ambiguity of the visual content all together.\nX e,s = x e + B e,s + A e,s(2)B e,s \u223c N (b s , v 2 s )(3)A e,s \u223c N (0, a 2 c )(4)\nX e,s is the raw opinion score.x e is the (true) quality score of the stimulus e. B e,s is the noise factor of subject s on rating stimulus e, it follows a Gaussian distribution in which the mean b s represents the subject's bias, and the variance v 2 s represents the subject's inconsistency.The factor A e,s refers to the source c that corresponds to the stimulus e. Its parameter a 2 c represents the ambiguity related to c.The estimate of each parameter (x e , b s , v s , a c ) is associated with a 95% confidence interval (calculated as described in [43] [44]).\n\nThe MLE model improves classical MOS calculation by removing the uncertainty from subjects and contents.In our case, these recovered MOSs (x e in eq. 2) remain close to classical MOSs (from eq. 1) (0.998 Spearman correlation).However, the bias, inconsistency, and content ambiguity values obtained constitute valuable information for further analysis (see paragraphs below).In the rest of this paper, we consider the recovered MOSs (x e instead of M OS e ) as the ground truth values for our database.\n\n\nObserver agreement\n\nBefore analyzing the results of the experiment, it is essential to evaluate the agreements between the subjects and whether they maintained their attentiveness during the test.\n\nTo do so, we consider two types of indicators: (1) the correlations between subjects' ratings, and (2) the bias and inconsistency from the MLE model.\n\nFirst, as in [21], we computed the Pearson Linear Correlation Coefficient (PLCC) and the Spearman Rank Order Correlation Coefficient (SROCC) between the scores of each observer and the MOSs of the stimuli rated by this observer.We then averaged the correlations over all the subjects.The (mean, standard deviation) of PLCC and SROCC are (0.85,0.055) and (0.81,0.063) respectively.The mean of the 2 correlations is high, while the standard deviation is rather low, which indicates a good agreement between the subjects.We then further explored the internal consistency of the subject data as proposed by [45].For each stimulus, we randomly divided the subjects who rated it into two equal size groups (12 observers per group) and calculated the SROCC between the recovered MOSs of the 2 groups.After repeating this test 500 times, the range of correlations was found to be between 0.915 and 0.944 with a mean and a median value of 0.929.Hence, there is a high degree of inter-subject agreement despite the immersive viewing environment.\n\nMoving to the second type of indicators, the subject's bias and inconsistency results, computed by the MLE model, are shown in Figures 4 and 5  The final version of record is available at http://dx.doi.org/10.1109/TVCG.2020.3036153\n\nCopyright (c) 2020 IEEE.Personal use is permitted.For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.Fig. 6: The mean opinion scores of all the stimuli, associated with their confidence intervals.For a given distortion strength, the dots are horizontally spaced apart to avoid overlapping.\nAix Ari Chameleon Fish Samurai QGeo QCol SGeo SCol\nis a systematic error generated by the subject throughout the experiment (i.e.picky/expert participants tend to be biased toward lower scores).Inconsistency, also known as random error, points out the inattentive subjects that give random scores or subjects showing absent-mindedness for a portion of the test.Figures 4 and 5 show that the range of bias and inconsistency values is within those of image/video experiments [43] [44] [46].These figures reported no implausible bias or inconsistency values, nor any loose confidence intervals, which means that subjects maintained attentiveness throughout the test and were sensitive to impairments.This is coherent with the results obtained using the BT.500's outlier detection method.\n\nFinally, we assess whether previous VR experience influences the subjects' judgments.Thus, we divided the observers into 2 groups: those who are familiar with VR (45 subjects) and those who have never tried a VR headset (27 subjects).For each group, we computed the correlations between subjects' ratings and MOS.We then averaged the correlations over all the subjects.Furthermore, we assessed the inconsistency of the 2 groups.Table 3 shows the results.We include, in the supplementary material, the corresponding boxplot of subjects's inconsistency.For the three computed indicators, no significant difference was found in the behavior of observers with no-VR expe- rience and those familiar with VR.We believe this is due to the fact that the task given to the participants is rather simple: observe and then vote using the trigger of the HTC Vive controller.As can be seen, no VR expertise is required, since there is no manipulation of the objects.Results also point out that our training stage was well-designed.\n\n\nFactors that influence subjective opinions\n\nOur objective is to provide a deep and evidence-based understanding of the factors that influencing subjective opinions.We quantitatively evaluate the effects of source models, distortions, viewpoints and movement on the mean opinion scores and their confidence intervals (CIs).Note that, the classic MOSs and CIs are used in this analysis since the quality scores and their corresponding CIs obtained by the MLE model are recovered from the influence of the source models (content ambiguity).\n\n\nResulting MOSs and confidence intervals\n\nFigure 6 shows the MOSs and confidence intervals for all the stimuli, averaged over all the observers.As expected, on the This is the author's version of an article that has been published in this journal.Changes were made to this version by the publisher prior to publication.\n\nThe final version of record is available at http://dx.doi.org/10.1109/TVCG.2020.3036153\n\nCopyright (c) 2020 IEEE.Personal use is permitted.For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\n\nwhole, MOSs decrease as distortion strengths increase.We notice that observers' behavior was virtually the same for the stimuli whether they were rotating or zooming in/out.However, we can observe that the effect of the viewpoints is strongly related to the source model (e.g.Fish SGeo vs. Chameleon SGeo) and the distortion type (e.g.Fish SGeo vs. Fish SCol).\n\nWe also notice variations in confidence interval length depending on the source content (i.e. the Chameleon's CIs are globally larger than those of Ari).In addition, it seems that, overall, viewpoint 3 provides smaller CIs than viewpoint 1.\n\nAll these factors and phenomena are quantitatively analyzed in the following paragraphs.To ensure better readability in interpreting the influence of the viewpoints, we separated the results of the 2 animations.The corresponding figures are provided in the supplementary material.\n\n\nInfluence on MOSs\n\nWe ran a Multivariate Analysis of Variance (ANOVA: Reference models \u00d7 distortion types \u00d7 distortion strengths \u00d7 viewpoints \u00d7 animation types) on the rating scores of the observers.Figure 7 summarizes the most important results using boxplots of MOSs.Source models, distortion types and strengths: as expected, the ANOVA test shows that these 3 factors are the most significant factor variables (the corresponding p-values << 0.0001) (see Figure 7.a).\n\nViewpoints: there are no significant differences in the subjective scores associated with the 3 selected viewpoints (p-value=0.189).However, a significant interaction effect was found between the viewpoint and the source content (p-value << 0.0001) (see Figure 7.b).This effect appears in Figure 6.In addition, the viewpoint is also strongly related to the distortion types (p-value << 0.0001).For instance, as illustrated in Figure 7.c, viewpoint 1 is much more sensitive to geometric simplification (SGeo) than viewpoint 3.This effect is reversed for geometric quantization (QGeo), since viewpoint 3 got the lowest average scores.Our hypothesis is that the geometry and silhouette alterations caused by QGeo are masked by rich colors and details of viewpoint 1 (viewpoint 1 is much richer than viewpoints 2 and 3).This is not the case for geometric simplification (SGeo), which markedly degrades colors and is thus more visible on viewpoint 1.The geometrically simplified Fish (see Figures 2 and 6) is a good case in point: observers were able to detect SGeo distortion when the stimulus was shown in viewpoints 1 and 2. This distortion is not so apparent/visible when the Fish was displayed in viewpoint 3 and is thus harder to detect in both rotation and zoom.\n\nFor QGeo, we clearly observe the opposite effect.Figure 7.d shows that a significant interaction exists between the viewpoint and the distortion strength (with a p-value << 0.0001).Indeed, stimuli with high strength of impairment (strength=4) obtained better scores when displayed in viewpoints 2 and 3 than in viewpoint 1.This is due to the fact that viewpoint 1 covers most of the shape and carries the most information and details.Thus, it is easier to detect loss in the visual quality of stimuli in viewpoint 1 than in viewpoints 2 and 3.This effect is obviously less visible for high quality stimuli (strength = 1).Figure 6 shows a concrete example: for Ari geometrically simplified and shown in viewpoints 2 and 3, as distortion forces increase, MOS values remain almost stable.These 2 viewpoints show the side and the back of the statue, respectively (see Figure 2).These areas are almost flat and contain very few geometric details/features, especially relating to the shape of the back (viewpoint 3).Therefore, simplifying these regions, even with high strength, will not introduce introduce any markedly visible distortions to the model.This is not the case of Ari displayed in viewpoint 1, since viewpoint 1 contains more salient features/details such as the face.\n\nAnimations: according to the ANOVA test, the animation itself does not affect significantly the perceived quality (pvalue = 0.165).However, a significant interaction was found between the animation and the distortion strength (p-value < 0.0001).Indeed, weak distortions (strength = 1) are easier to detect in zoom than in rotation, while stimuli with high distortion (strength=4) obtained roughly the same score in both animations.Moreover, there is a slight interaction effect between the animation and the viewpoint (p-value=0.09).The interaction between these 2 factors will be discussed in the following subsection since it has much more influence on the CI than on the MOS.\n\n\nInfluence on confidence intervals\n\nThis time, we ran the ANOVA test on the 95% confidence intervals of the MOSs.This allows us to evaluate the impact of the factors on the dispersion of individual ratings.As above, Figure 8 summarizes the most important results using boxplots of CIs.Source models: When looking at Figure 8.a, it appears obvious that the source models influence the agreement among the subjects (p-value=0.0016).Indeed, selection of source models is no trivial task: some contents tend to be more difficult to rate than others.This phenomenon is represented in the MLE model by the ambiguity of the content a c (see the supplementary material).Overall, the chameleon tends to be the source associated with the highest content ambiguity (subjects disagree).A reasonable explanation for this is that the chameleon model carries more information content than all the other models: it has a high average curvature, sharp edges, diversity of colors, and many small details to reflect its skin tone and geometrical characteristics.\n\nViewpoints: It is interesting to observe that the viewpoint has a significant impact on CIs (p-values=0.0035),unlike that on MOSs. Figure 8.b shows that the CIs of viewpoint 1 are larger than those of the other viewpoints.The fact that viewpoint 1 contains more details/information on color and geometry than the others implies that this viewpoint results in higher dispersion between the observers' scores.\n\nAnimations: Overall, the CIs of the stimuli in rotation are smaller than those in zoom.Still this difference is moderate (p-value=0.052).The impact of this factor is emphasized when considering the interaction between animations and viewpoints (p-value=0.0019).Indeed, Figure 8.c shows that models with animated zoom tend to be more ambiguous (result in larger CIs) than those that rotate, notably when the models are displayed in viewpoint 1, which is the viewpoint that covers most of the shape and carries the most information.This effect can be observed in Figure 6 for Samurai, Aix and Chameleon models shown in viewoint 1 and animated with a slow zoom.This can be explained by the fact that while zooming, especially in viewpoint 1, the observer can see more details and low-level features, which makes the task of evaluating differences between the reference and the impairment stimulus more difficult than the other HRTs.\n\nWe analyzed the ambiguity of our source contents (a c ), obtained by the MLE model, for each viewpoint and animation.The result is provided in the supplementary material.It is consistent with the findings of this section: sources with high confidence intervals are also associated with high ambiguity values.\n\nThus, the results point out a relationship between the ambiguity of the source (i.e.dispersion of subject ratings) and its geometric and color complexity.Models with more details are the most difficult to rate (larger confidence intervals).Furthermore, the rating is affected by the selected viewpoint: the most informative viewpoint tends to produce the largest confidence intervals, especially when combined with a zoom movement.For given distortions, an impact of the viewpoint on the MOSs can also be observed.Complex masking effects occur when considering the interaction between viewpoint and distortion.The animation has no significant impact on the perceived quality/degradation.We stress that, further studies should be carried out so that these results can be generalized to a non-VR scenario.\n\nOur findings suggest recommendations for the design of an objective quality assessment metric for 3D meshes.First, since the models and their distortions and strengths have crucial importance on perceived quality, the metric must be able to adapt to the models, and to their shapes and colors.Moreover, it must be able to detect/capture different distortions applied on both geometry and color map.We develop such a metric in section 5. Considering the animation as a non-influential factor, it is ineligible for integration in the metric.However, since the viewpoint has an impact, albeit moderate, it may be useful to take it into account in the objective model.We investigate this in section 7.\n\n\nTOWARD AN OBJECTIVE METRIC FOR ASSESS-MENT OF COLORED MESH QUALITY\n\nAs outlined in the introduction, constructing an objective metric for the quality assessment of 3D content with appearance attributes is no trivial task.The main reasons are: (1) the multimodal nature of the data (geometry and color or texture information) and ( 2) the complex processing pipeline that constructs the final rendered image from the 3D content (computation of light-material interactions, viewpoint selection, and rasterization).To overcome this problem, we consider a data-driven approach based on the results and data of our subjective study.Thus, we propose an objective metric for colored mesh quality assessment as a linear combination of accurate geometry and color quality measurements.\n\n\nOverview of our approach\n\nThe metric we propose is a full-reference multiscale metric based on curvature and color statistics computed on local corresponding neighborhoods from the original and distorted models.The metric is largely inspired by the MSDM2 frameworks from which we take the curvature features and the neighborhood correspondence mechanisms [1].To address the color-related aspects of our metric, we consider the features introduced in the 2D image-difference framework of Lissner et al. [30].Their color features have recently been used successfully for the quality assessment of colored 3D point clouds [31].We refer to our metric as CMDM (for Color Mesh Distortion Measure).\n\nOur framework is as follows: For given distorted M dist and reference M ref meshes, we first establish a correspondence between M dist and M ref (see section 5.2).Then for each scale h i , we define a spherical neighborhood around each vertex v of M dist (see section 5.3) and compute a set of local geometry and color based features over the points belonging to the neighborhood of v and their corresponding points on M ref (see section 5.4).Local single-scale feature values are pooled into global multiscale features f j .Finally, CMDM is defined as a linear combination of an optimal subset of features determined through logistic regression (see section 5.5).\n\n\nCorrespondence between meshes\n\nThe first objective is to establish a correspondence between the meshes being compared (M dist and M ref ).Thus, we match each vertex v of the distorted mesh M dist with its nearest 3D point v on the surface of the reference mesh M ref using a fast asymmetric projection (as in MSDM2, we consider the AABB tree structure from CGAL [47]).Then, for each projected 3D point (v), we compute its curvature and color using barycentric interpolation from vertices of the triangle it belongs to.This way, each vertex from M dist has a corresponding point on M ref (with a curvature and a color value).The correspondence is scale-independent: it takes place once only at the beginning of the process.Nevertheless, the curvature and color values of v are updated for each scale h i .\n\n\nNeighborhood Computation\n\nAs stated above, the features used in our metric are not computed globally on the entire mesh but locally at multiple scales over spherical neighborhoods around each vertex.Thus as in [1], we define, for each scale h, a neighborhood N (v, h) of radius h around each vertex v of M dist as the connected set of vertices belonging to the sphere with center v and radius h.We also add to this neighborhood the intersections between this sphere and the edges of M dist .The curvature and color values of the intersection points are interpolated.Then, we consider for the set of points belonging to N (v, h) their projected 3D points on M ref (corresponding neighborhood of v).Features are computed by considering curvature and color statistics over\nN (v, h) \u2208 M dist and N (v, h) \u2208 M ref .\nIn this paper, we consider the following three scales: h i \u2208 {0.003BB, 0.0045BB, 0.006BB}, where BB is the maximum length of the Axis-Aligned Bounding Box (AABB) of the stimulus.The choice of these scales is detailed and justified in the supplementary material.\n\n\nPerceptually relevant features\n\nFor each scale h, the following 8 features are computed over the local corresponding neighborhood of each vertex v of M dist .\n\n\nA. Geometry-based features\n\nThese features are based on mean curvature information defined at multiple scales.To compute curvature, we adopted the method developed by Alliez et al. [48], which evaluates the curvature tensor on a geodesic neighborhood around each vertex.This method is interesting and robust because it avoids the problem of sensitivity to connectivity (M dist and M ref do not necessarily share the same connectivity nor the same level of details).Note that, we used a radius r = h 3 for the computation of curvature as a good compromise between small radii which capture tiny details and larger radii which provide strong smoothing effects.As in [1], we consider the following geometry features:\nCurvature comparison f h 1 (v) = C h v \u2212 C h v max(C h v , C h v ) + k (5) Curvature contrast f h 2 (v) = \u03c3 C h v \u2212 \u03c3 C h v max(\u03c3 C h v , \u03c3 C h v ) + k (6) Curvature structure f h 3 (v) = \u03c3 C h v \u03c3 C h v \u2212 \u03c3 C h v C h v \u03c3 C h v \u03c3 C h v + k (7)\nwhere k is a constant to avoid instability when denominators are close to zero (k = 1 as in [1]).C h v and C h v are Gaussian-weighted averages of curvature over the points belonging to the neighborhood N (v, h) and\nN (v, h), respectively. Similarly, \u03c3 C h v , \u03c3 C h v and \u03c3 C h v C h\nv are Gaussian-weighted standard deviations and covariance of curvature over these neighborhoods.\n\n\nB. Color-based features\n\nTo compute the color features, we first transform the RGB color values of each vertex of the meshes being compared (M dist and M ref ) into the perceptually uniform color space LAB200HL [49].Lissener et al. recommended working in this color space since there is little cross contamination between the color attributes (lightness, chroma, hue).Each vertex v has of a lightness and two chromatic values (L v , a v , b v ).The chroma of the vertex is as follows:\nCh v = a 2 v + b 2 v .\nWe transposed for 3D meshes, the 2D image features proposed by [30].These features take into account not only the luminance but also the chroma and hue components to better assess the chromatic distortions.\nLightness comparison f h 4 (v) = 1 c 1 (L h v \u2212 L h v ) 2 + 1 (8) Lightness contrast f h 5 (v) = \u03c3 L h v \u03c3 L h v + c 2 \u03c3 2 L h v + \u03c3 2 L h v + c 2 (9) Lightness structure f h 6 (v) = \u03c3 L h v L h v + c 3 \u03c3 L h v \u03c3 L h v + c 3 (10) Chroma comparison f h 7 (v) = 1 c 4 (Ch h v \u2212 Ch h v ) 2 + 1 (11) Hue comparison f h 8 (v) = 1 c 5 \u2206H h vv 2 + 1(12)\nThis is the author's version of an article that has been published in this journal.Changes were made to this version by the publisher prior to publication.The final version of record is available at http://dx.doi.org/10.1109/TVCG.2020.3036153\n\nCopyright (c) 2020 IEEE.Personal use is permitted.For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\n\nwhere L h v , L h v , Ch h v and Ch h v denote the Gaussianweighted averages of Lightness and Chroma computed respectively over the set of points belonging to N (v, h) and\nN (v, h). \u03c3 L h v , \u03c3 L h v and \u03c3 L h v L h v\nare Gaussian-weighted standard deviations and covariance of lightness in the mentioned neighborhood.The term \u2206H h vv refers to the Gaussian-weighted average hue difference between N (v, h) and N (v, h).It is defined as follows:\n\u2206H vv = (a v \u2212 a v ) 2 + (b v \u2212 b v ) 2 \u2212 (Ch v \u2212 Ch v ) 2 .\nThe constants c 1 , c 2 , c 3 , c 4 and c 5 were set respectively to 0.002, 0.1, 0.1, 0.002 and 0,008 as in [30].\n\nWe invert the scaling of the color-based features so that they are consistent with curvature-based features (i.e. each color feature f h j = 1 \u2212 f h j ).This way, a value of 0 means that there is no local (geometric and color) distortion around vertex v.All features \u2208 [0, 1].\n\n\nGlobal perceptual quality score\n\nThe set of local geometric and color features, presented in the subsection above, is computed for each vertex of the distorted mesh and for each scale h i .The local multiscale measure of the features is simply the average of its singlescale values.\nf j (v) = 1 n n i=1 f hi j (v) (13)\nwhere n is the number of scales used.It is defined in section 5.3 as well as h i the scale values used (neighborhood radii) .\n\nWe aim to obtain a global score of visual distortion according to each feature (f j ).So, we average the local values of each feature over all the vertices.\nf j = 1 |M dist | v\u2208M dist f j (v)(14)\nwhere |M dist | is the number of vertices of the distorted mesh.The features f j are all within the range [0, 1].Our metric is then defined as a combination of the features f j .However, choosing the best combination model is a crucial problem.For prediction of the color-image differences [30], the authors used a factorial combination model, while Meynet et al. considered a linear model for their point cloud quality metric [31].In our case, we chose to consider a linear model: (1) to make the optimization easier and (2) because we tried nonlinear models such as Minkowski pooling, which did not provide better performance.Thus, the global multiscale distortion (GM D) score is computed as follows:\nGM D M dist \u2192M ref = j\u2208S w j f j (15)\nS is the set of feature indexes of our linear model.w j weights the contribution of each feature to the overall distortion prediction.GM D M dist \u2192M ref evaluates the distortion of the distorted model regarding the reference model.In order to strengthen the robustness of our method and to obtain a symmetric measure, we also compute GM D M ref \u2192M dist and we retain the average as the final distortion measure CM DM .\nCM DM = GM D M dist \u2192M ref + GM D M ref \u2192M dist 2(16)\nAs in [50], the optimal subset of features of CM DM and their corresponding weights are obtained through an optimization computed by logistic regression.The optimization is based on cross-validation, using the ground truth dataset from our subjective experiment (see section 6.3).\n\n\nRESULTS AND EVALUATION\n\nIn this section, we evaluate the performance of our metric and compare it to state-of-the-art approaches, including 2D image metrics.To train and evaluate our metric, we used the ground truth database obtained from our subjective experiment (section 3).In this section, we do not take into account either the influence of the viewpoints or that of the animations: for a given stimulus, we averaged its recovered MOSs over the 3 viewpoints and the 2 animations.Thus, the database used is composed of 80 stimuli.We also validate our metric on a dataset from [5], composed of distorted textured meshes.\n\n\nPerformance evaluation measures\n\nIn order to evaluate the performance of objective metrics, we compare the predicted quality scores given by these metrics to the ground truth subjective data.The standard performance evaluation measure consists in computing the Pearson Linear Correlation Coefficient (PLCC) and the Spearman Rank Order Correlation Coefficient (SROCC) between the metric predictions and subjective scores (MOS).These indices measure, respectively, the accuracy and the monotonicity of the predictions.Note that, the Pearson correlation (PLCC) is computed after a logistic regression which provides a non-linear mapping between the objective and subjective scores.This allows the evaluation to take into account the saturation effects associated with human senses.\n\nHowever, the correlations ignore the uncertainty of the subjective scores.Therefore, as a complementary assessment of the performance of the objective metrics, we also implement the framework recently proposed by Krasula et al. [51].This methodology consists in determining the classification abilities of the metrics according to two scenarios:\n\n\u2022 (A) Different vs. Similar: this analysis assesses how well can the metric distinguish between significantly different and similar pairs of stimuli.The first step consists in determining the pairs in the dataset rated significantly different.To do so, we conduct a statistical test (t-test) on the raw subjective scores.Then for each pair of stimuli (i, j), we compute the absolute difference of the predicted scores (|\u2206 PredictedScores (i, j)|) and measure how well these values are able to correctly classify the pairs of stimuli.\u2022 (B) Better vs. Worse: this analysis is performed on the significantly different pairs only.The significantly different pairs (i, j) are divided into 2 groups: i better than j ((\u2206 MOS (i, j) > 0)) and i worse than j ((\u2206 MOS (i, j) < 0)).\n\nWe then measure, according to (\u2206 PredictedScores (i, j)) values, how well the metric is able to predict this classification.As can be seen, these scenarios take into account the uncertainty of the subjective scores.Both scenarios refer to a binary classification problem (different/similar and better/worse).As a performance indicator, we consider the Receiver Operating Characteristic (ROC) and, more precisely, the Area Under the Curve (AUC) values.AUC is a direct indicator of the performance/ability of the classifiers (1.0 corresponds to a perfect classification, 0.5 corresponds to a random one).In what follows, it is noted by AU C DS and AU C BW for scenarios A and B, respectively.\n\n\nSingle feature prediction performance\n\nThis section evaluates the prediction performance of each feature implemented in our multiscale metric.Table 4 shows the correlations of the individual features with the recovered MOSs, as well as their classification abilities.Overall, the best features are those based on the lightness information (especially f 5 , f 6 ).They correlate well with the subjective scores and provide a good performance in identifying the significantly different stimuli as well as the stimuli of better quality.For the geometry-based features, f 1 and f 2 perform better than f 3 .However, this does not necessarily point to the ineffectiveness of f 3 .Finally, regarding the chromatic feature, chroma comparison f 7 seems more relevant than hue comparison f 8 .Note that, the geometry features are penalized by the color quantization (QCol), since this type of distortion is applied only on the vertex colors and does not affect the model geometry at all.Removal of this distortion improves their performance, notably with respect to correlations (increase to 0.7 for f 1 anf f 2 ).This latter analysis is available in the supplementary material.\n\n\nToward an Optimal Combination of features\n\nOur metric contains 8 different features f j .In this 8 dimensional space, some features are obviously more significant than others.Also, features may be redundant with one another, and if all the features are taken into account, this could potentially lead to an overfitting.Therefore, in the same vein as [50], we conduct two Leave-One Out Cross-Validation tests (LOOCV) on the data obtained from our subjective experiment to select an optimal subset of features.Each cross-validation test divides the database into a training set that serves to optimize feature weights using linear regression and a test used for testing the obtained metric.1) We split the training and test sets according to the source models.Given that there are 5 sources in our database, we leave 1 source model and its distortions out for testing, while the remaining stimuli (4 models * 16 distorted stimuli) are used for training.Thus after 5 folds, each source model has been used as a test set.2) Similar to test 1, but we divide the database according to the distortion types (regardless of the model).We train the metric on 3 distortion types out of 4 (5 models * 12 distorted stimuli) and test on the fourth type.After 4 folds, each distortion type has been used once for testing.\n\nThese 2 types of LOOCV tests provide a good measure of the robustness of our metric.We exhaustively search through all possible combinations of features (255 combinations), and select the feature-subset that generates the best average performance of CM DM over all the test sets (9 folds) in terms of the mean of PLCC and SROCC.We obtained that the final model of our metric is composed of only 4 features: Curvature contrast (f 2 ), Lightness contrast (f 5 ) and structure (f 6 ) and chroma comparison (f 7 ).The optimal features found are consistent with the results of the single feature performance.The results of our metric and comparisons with state-of-the-art approaches are reported in the following sections.\n\n\nComparisons of objective metrics\n\nIn this section, we present the results of the cross-validation tests, described in the previous subsection.As an ablation study, we compare our metric with two of its versions trained with different subsets of features: CMDM Geo that takes into account only the geometry features and CMDM Col based only on color features.As a baseline, we also include results of a classical color distance D LAB, which is the average of the color difference (in LAB2000HL) computed symmetrically between the reference and the distorted model.Finally, we compare our metric with 3 state-of-the-art full-reference image quality metrics (IQMs): SSIM [34], HDR-VDP2 [52], iCID [53].To apply these IQMs, we generate for each 3D object in our database, a set of 18 snapshots taken from different viewpoints (camera positions regularly sampled).The global quality score of a stimulus, given by an IQM, is then the average of the objective scores over all its snapshots.The parameters of IQMs, as well as snapshots of the camera positions are provided in the supplementary material.Figure 9 compares the overall performance of the tested metrics for the 2 cross-validation scenarios presented in 6.3.Tables 5 and 6 detail the results of each test set.\n\nFor the LOOCV test according to the source models, Figure 9 demonstrates that CMDM outperforms other modelbased metrics.It shows almost the same performance as IQMs in terms of correlations and detection of better quality stimuli (AU C BW ).IQMs provide better results in identifying the significantly different pairs of stimuli (AU C DS ).We believe this is primarily related to the advantage of IQMs over our metric and other model-based metrics regarding their natural incorporation/knowledge of the entire rendering pipeline.Indeed, IQMs operate on snapshots that consider the same rendering, apparent brightness and lighting conditions as those seen by participants.On the contrary, our metric only considers 3D data, without any knowledge of the rendering conditions.Considering the LOOCV test among the distortions, we notice that our metric performs better than the others, including IQMs.The color-based version of our metric (CMDM Col) also produces good results.IQMs show a significant decrease in performance, compared to the LOOCV based on source models.These observations corroborate previous results by Lavou\u00e9 et al. [35]: imagebased metrics perform very well when evaluating the quality of different versions of a single source, yet they are less accurate when differentiating/ranking distortions applied   on different sources.More details are provided in Tables 5  and 6.\n\nFrom Table 5, it can be seen that our metrics and IQMs demonstrate a good stability over the models.The performance (especially the correlations) of the D LAB and CMDM Col metrics drop dramatically for the Ari model.We also notice that the quality of the Chameleon model was hardest to predict, since almost all the metrics (except D LAB and CMDM Col) exhibit a poorer performance than the other models.This is coherent with our findings in section 4.3.3.\n\nWhen considering each distortion type separately (Table 6), several observations can be made.First, our metric performs very well on 3 types of distortion out of 4: For QCol, SCol and SGeo, it outperforms significantly the other metrics, and particularly IQMs.However, our metric shows a poor performance when distinguish between similar and different pairs corrupted by geometric quantization (QGeo).For this distorsion, HDR-VDP2 performs significantly better in terms of correlations and classification abilities.CMDM seems to underestimate the impact of geometric quantization (QGeo), which is particularly harmful for such highresolution models in our database.We believe that this is due to the fact that this distortion superimposes the vertices of the stimulus, meaning that we cannot know or control exactly which vertex color is taken into account in Unity's import and render pipelines.This case points out an advantage for image-based quality metrics and highlights the importance of taking rendering into account in the assessment of visual quality.\n\n\nRecommended weights\n\nTo provide the recommended model of our metric, we averaged the weights obtained for each training subset of the two LOOCV tests.CMDM is thus defined, for the three selected scales (h i \u2208 {0.003BB, 0.0045BB, 0.006BB}), as follows:\n\nCM DM rec = 0.091f 2 + 0.22f 5 + 0.032f 6 + 0.656f 7 (17) In order to reveal the relative importance of each of the 4 features, we scaled the weights presented in the equation above with the standard deviation of the features.Scaled weights are 0.333, 0.46, 0.07 and 0.136, respectively, for f 2 , f 5 , f 6 and f 7 .The curvature and lightness contrast features (f 2 and f 5 ) have the highest overall importance.It would seem that users are particularly sensitive to artifacts that harm the contrast (both geometric and color contrasts).\n\nWe evaluate the performance of the tested metrics, including CM DM rec , on the whole dataset (80 stimuli).The results are reported in Table 7. Figure 10 shows the subjective scores with respect to objective metric values.Copyright (c) 2020 IEEE.Personal use is permitted.For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.classification abilities in both Different vs. Similar and Better vs. Worse analyses.This shows the good robustness of our metric: it is able to differentiate and rank stimuli from different sources and different distortions.\n\n\nValidation on a dataset of textured 3D meshes\n\nTo evaluate the robustness of our recommended metric (eq.17) and to verify that it did not just learn the distortions that are specific to our dataset, we tested CM DM rec on a new dataset.Only few subject-rated datasets of 3D models with attributes are available to the scientific community [5] [22].We consider the LIRIS Textured Mesh Database [5], produced from a subjective study based on a pairwise comparison method.This database is composed of 136 textured meshes, obtained from 5 source models subjected to texture and geometry distortions.Indeed, the authors generated 20 distorted versions of each source.They also selected a model (the Dwarf) among the 5, and associated it with 36 mixed distortions (combination of geometry and texture distortions).As each of these 6 subsets of the database was rated separately, they cannot be assessed together.To evaluate the robustness of our approach, we selected the most difficult subset, namely the one containing mixed distortions.The source model of this subset is a scan of a Dwarf statue that has been scanned and reconstructed into a textured mesh of 250004 vertices.Distortions are combinations of 3 geometry distortions (geometric quantization, simplification, smoothing), each applied with 2 strengths and 2 texture distortions (JPEG compression, sub-sampling), each applied with 3 strengths.As can be seen, these compound distortions differ significantly from the distortions generated in our dataset.Before applying our metric, we transferred the texture color information into vertex colors (we generate the vertex color by picking the corresponding color from the texture).\n\nThe results are summarized in Table 8.We include results of the IQMs presented previously, as well as the results obtained by Guo et al. [5] for different metrics: three metrics applied on rendered videos of the stimuli (the Discrete Cosinus Transform-based (DCT) metric [54], the PSNR and the MS-SSIM [55] applied on all frames and averaged) and three metrics directly applied on textured meshes (FQM [4] based on a weighted combination of two simple mesh and texture image error measures, CM 1 and CM 2 [5] both defined as a linear combination of mesh quality and texture quality).\n\nNote that, Table 8 shows only the correlation measures since subjective scores are derived from a paired-comparison method and are therefore not associated with CIs.In the supplementary material, we illustrate the subjective scores with respect to the values of the tested metrics.Our metric provides the best results, although it was trained on a different dataset presenting different sources and different distortions and even a different color representation.SSIM and iCID show poor performances.They may be affected by the fact that the snapshots used do not have the same rendering and lighting conditions as those of the experiment.Note that results for SSIM, computed using snapshots of the stimuli, are consistent with those reported by [5], which are computed on the rendered videos used in the subjective test.Our metric also outperforms CM 2 , which represents the state-of-the-art of textured mesh quality assessment, and which was learned on similar data.This metric is a global combination of mesh and texture distortion measures (MSDM2 and MS-SSIM, respectively).This tends to validate the fact that operating fully on the mesh domain (like our metric) ensures a better performance than combining errors computed on different domains (i.e., mesh and texture image).These results also confirm the great robustness of our metric compared to IQMs.\n\n\nINTEGRATION OF THE VIEWPOINT\n\nAccording to the findings of our subjective experiment, the viewpoint of stimuli may have a significant impact on user quality assessment.Thus, we hypothesized that incorporating this factor into our objective metric should improve its results.Indeed, the invisible parts of the 3D model do not contribute to its visual appearance.Given a stimulus and a camera position, we determined, in a preprocessing step, which vertices are visible and which vertices are occluded by other faces of the mesh (using ray-vertex intersections).Note that, we have ignored the slight change in visibility of vertices on the borders/boundaries of objects caused by their animation.Thus, our objective metric is now computed only over the visible vertices.We can redefine Equation 14as follows:\nf i = 1 |M dist | v\u2208M dist f i (v)\u03a8(v) (18)\nwhere \u03a8 is a function that returns 0 or 1 according to the visibility of vertex v and |M dist | is the number of visible vertices of the distorted mesh.\n\nTo evaluate the performance of the new metric CM DM vis , we used a subset of our database consisting of 240 stimuli.Indeed, for a given stimulus, we considered its 3 viewpoints and averaged the recovered MOSs of the 2 animations.We tested the performance of the metric with and without integrating the visibility on these 240 stimuli.Similarly for the IQMs, we considered 2 scenarios: (1) without taking the visibility into account, so we computed the IQMs on multiple snapshots taken from different viewpoints and (2) computing the IQMs directly on the snapshot taken from the real viewpoint displayed to the observer (IQM vis ).Table 9 shows the improvement/evolution of metric results when incorporating the viewpoint.The improvement is defined as the difference in the evaluation measures (correlations and AUC) computed before and after integrating the viewpoint (e.g. for a given metric M : \u2206P LCC = P LCC Mvis \u2212 P LCC M ).The full results of the tested metrics in both scenarios are provided in the supplementary material.We obtained, through the 2 versions of all the metrics, roughly the same performance in terms of correlations and classification abilities (no significant performance improvement).Our hypothesis is that this lack of improvement is due to the fact that only a small subset of the dataset is actually rated significantly different for its different viewpoints.This led us to conduct a more precise study: we identified those stimuli with viewpoints associated with significantly different subjective scores.We found out that the viewpoint has a significant influence only on 88 pairs out of 22695 pairs of stimuli rated significantly different.\n\nThus, instead of considering all the possible pairs of stimuli (240*239/2), we compared each stimulus separately according to its 3 viewpoints V P (e.g.Fish SGeo 4 VP1 vs. Fish SGeo 4 VP2, Fish SGeo 4 VP1 vs. Fish SGeo 4 VP3 and Fish SGeo 4 VP2 vs. Fish SGeo 4 VP3).This limited the study to 240 pairs of stimuli (80 stimuli*3 possible combinations of pairs of viewpoints), 88 of which were significantly impacted by the viewpoints.The results are shown in Table 10.Note that, only the AUC values are reported since this study is based on pairs of stimuli and thus the correlations could not be computed.Without integrating the viewpoint information, the AUC values of all the metrics are equal to 0.5.Including the viewpoint slightly improved the results.Still, this improvement is low, except for HDR-VDP2 vis , which showed a good ability to recognize the stimulus of higher quality in the pair.This study takes the first step toward integrating the knowledge of the viewpoint into objective metrics.The fact that the IQMs exhibited a relatively poor performance, even though they were computed directly on the displayed view, shows that it is considerably difficult to distinguish the perceived quality of different viewpoints of the same 3D model.Further work is still needed to produce efficient metrics in this difficult scenario.In particular, we hypothesize that classical pooling should be replaced by more sophisticated pooling.It could also be useful to consider visual attention models.\n\n\nCONCLUSION AND FUTURE WORK\n\nIn this work, we designed and produced a large subjectively-rated database of colored 3D meshes.This database is composed of 480 dynamic stimuli and obtained through a subjective study based on the DSIS method, in a virtual reality environment.The stimuli were generated from 5 source models subjected to geometry and color distortions.Each stimulus was associated with 6 HRTs: combinations of 3 viewpoints and 2 animations.This study allowed us to draw interesting conclusions regarding the masking effects that occur when considering the interaction between viewpoint and distortion.Although animation, by itself, has a moderate impact on subjects' opinions and CIs, the impact of this factor is emphasized when it interacts with other factors such as distortion strength for subjective scores and viewpoint for CIs.Moreover, results show that the ambiguity of the source is potentially related to its geometric and color complexity.The more visible the content's information/complexity (as in zoom for example), the higher the ambiguity.\n\nWe developed a perceptually-validated full-reference metric CMDM for evaluating the quality of colored 3D meshes.To achieve this, we adapted a set of perceptuallyrelevant curvature-based and color-based features.We further show how to select an optimal subset of features and use them to train the metric (LOOCV tests using a ground truth dataset).Extensive evaluation shows that CMDM provides good results and good stability in terms of correlations and classification abilities.It also demonstrates a good robustness: CMDM is able to differentiate and rank stimuli from different sources and different distortions, unlike IQMs which perform very well when assessing the quality of different versions of a single source, but are less accurate when ranking distortions applied on different sources.Last but not least, we demonstrate that our metric can also be used for textured meshes.Our ground truth database, subjective scores, and the metric code are made publicly available online.\n\nAs future work, we plan to further explore how to effectively incorporate visibility information into objective measures.We would also like to produce a huge subjectrated database of 3D models, in order to be able to envisage the creation of end-to-end deep-learning approaches.Finally, we will work towards adapting the perceived quality of the objects according to the position/movement of the subject in the VR scene.\n\nFig. 2 :\n2\nFig. 2: Some examples of distorted models.Acronyms refer to Model Dist-Type Dist-Strength Viewpoint.\n\n\nFig. 3 :\n3\nFig. 3: The experimental environment of our subjective test based on the DSIS method.\n\n\nFig. 5 :\n5\nFig. 5: Inconsistency v s of each subject involved in our subjective experiment, and its confidence interval.Bias reflects the sensitivity of the subject to impairments.It\n\n\nFig. 7 :\n7\nFig. 7: Boxplots of MOSs obtained for the different factors or combination of factors.Mean values are displayed as circles.\n\n\nFig. 8 :\n8\nFig. 8: Boxplots of CIs obtained for the different factors or combination of factors.Mean values are displayed as circles.In (a), Cham.and Sam.refer to Chameleon and Samurai, respectively.\n\n\nFig. 9 :\n9\nFig. 9: Performance comparison of several metrics on two cross-validation tests.Mean performance evaluation measures are reported.Error bars indicate the standard deviation over the test sets.\n\n\nFig. 10 :\n10\nFig. 10: Scatter plot of subjective scores versus objective metric values for all the dataset.Each point represents one stimulus.The fitted logistic function is displayed in black.\n\n\nTABLE 1 :\n1\nCharacteristics of the 3D graphic source models\n\n\nModels #Vertices Geometry complexity Color characteristics Semantic category Created using\nAix686061Plane with small detailsMono-colorArt3D scanningAri645492IntermediateCool & light colorsHuman statues3D scanningChameleon588441High & sharp edgesCool & dull colorsAnimalModeling softwareFish216578Low & sharp edgesCool & warm colorsAnimalModeling softwareSamurai449997Highwarm colorsHuman statues3D scanningDancing Drummer1335436Intermediate/ HighCool colorsHuman statues3D scanning\n\nTABLE 2 :\n2\nDetails on the distortions applied to each source model.\nDistortion typeDistortion strengthAixAriChameleonFishSamurai110 bits10 bits9 bits9 bits10 bitsQGeo2 39 bits 8 bits9 bits 8 bits8 bits 7 bits8 bits 7 bits9 bits 8 bits47 bits7 bits6 bits6 bits7 bits1(L=5, A=4, B=4) bits (L=5, A=4, B=4) bits (L=4, A=3, B=3) bits (L=5, A=5, B=5) bits (L=4, A=3, B=3) bitsQCol2 3(L=4, A=3, B=3) bits (L=4, A=3, B=3) bits (L=3, A=2, B=2) bits (L=4, A=3, B=3) bits (L=4, A=2, B=2) bits (L=3, A=2, B=2) bits (L=2, A=3, B=3) bits (L=2, A=2, B=2) bits (L=3, A=2, B=2) bits (L=3, A=2, B=2) bits4(L=2, A=2, B=2) bits (L=3, A=3, B=3) bits (L=2, A=1, B=1) bits (L=2, A=2, B=2) bits (L=2, A=2, B=2) bits150% removed30% removed50% removed31% removed24% removedSGeo2 375% removed 88% removed50% removed 75% removed75% removed 87% removed50% removed 77% removed50% removed 75% removed494% removed87% removed92% removed88% removed88% removed171% removed50% removed67% removed77% removed66% removedSCol2 387% removed 94% removed64% removed 88% removed83% removed 92% removed79% removed 87% removed80% removed 90% removed498% removed94% removed95% removed96% removed96% removed\n\n\n\n.\n0.80.6\u22120.4 \u22120.2 0.0 0.2 0.4 Subject Bias\u22120.6\u22120.8#1 #2 #3 #4 #5 #6 #7 #8 #9 #10 #11 #12 #13 #14 #15 #16 #17 #18 #19 #20 #21 #22 #23 #24 #25 #26 #27 #28 #29 #30 #31 #32 #33 #34 #35 #36 #37 #38 #39 #40 #41 #42 #43 #44 #45 #46 #47 #48 #49 #50 #51 #52 #53 #54 #55 #56 #57 #58 #59 #60 #61 #62 #63 #64 #65 #66 #67 #68 #69 #70 #71 #72SubjectsFig. 4: Bias b s of each subject involved in our subjectiveexperiment, and its confidence interval.0.25 0.50 0.75 1.00 Subject Inconsistency0.00#1 #2 #3 #4 #5 #6 #7 #8 #9 #10 #11 #12 #13 #14 #15 #16 #17 #18 #19 #20 #21 #22 #23 #24 #25 #26 #27 #28 #29 #30 #31 #32 #33 #34 #35 #36 #37 #38 #39 #40 #41 #42 #43 #44 #45 #46 #47 #48 #49 #50 #51 #52 #53 #54 #55 #56 #57 #58 #59 #60 #61 #62 #63 #64 #65 #66 #67 #68 #69 #70 #71 #72Subjects\n\nTABLE 3 :\n3\nagreement and inconsistency of subjects familiar with VR and those without VR experience.\n(Mean, SD)PLCCSROCC InconsistencySubjects unfamiliar with VR (0.845, 0.057) (0.811, 0.065) (0.536, 0.156)Subjects familiar with VR (0.845, 0.056) (0.813, 0.062) (0.517, 0.165)\n\nTABLE 4 :\n4\nPerformance of individual features.\nFeatureIdPLCCSROCCAU C DSAU C BWCurvature comparison f10.50.440.60.75Curvature contrastf20.450.430.590.73Curvature structuref30.30.320.530.67Lightness comparisonf40.580.690.690.83Lightness contrastf50.70.710.70.87Lightness structuref60.680.710.690.87Chroma comparisonf70.380.590.640.78Hue comparisonf80.330.430.60.71\n\nTABLE 5 :\n5\nPerformance comparison of several metrics in a cross-validation test among source models AU CDS AU CBW PLCC SROCC AU CDS AU CBW PLCC SROCC AU CDS AU CBW PLCC SROCC AU CDS AU CBW PLCC SROCC AU CDS AU CBW\nAixAriChameleonFishSamuraiPLCC SROCC CMDM 0.958 0.9560.7830.9820.960.910.8230.9860.830.830.6920.9430.930.9140.8050.9870.933 0.9440.7460.976CMDM Geo 0.530.6210.5620.790.680.4680.5770.7880.457 0.4740.5040.760.554 0.5540.6220.7880.462 0.4070.5980.737CMDM Col 0.778 0.7910.7930.9130.491 0.5530.6330.7990.764 0.7880.6310.9140.941 0.9030.8660.990.760.7790.6310.887D LAB0.791 0.8260.770.9240.282 0.4970.5230.7370.776 0.7470.6090.8970.734 0.7790.7130.90.546 0.6590.590.787SSIM0.896 0.9090.7820.9590.973 0.9320.9240.9930.823 0.8680.6830.9510.959 0.9290.90.9920.957 0.9150.8460.989HDR-VDP2 0.893 0.8530.7280.9580.976 0.9470.8770.9980.849 0.8180.7270.9630.895 0.8970.7510.9810.978 0.9620.860.995iCID0.958 0.9320.8490.9830.953 0.9290.850.9890.924 0.9210.7430.9860.954 0.9350.9120.9880.966 0.9680.9140.999\n\nTABLE 6 :\n6\nPerformance comparison of several metrics in a cross-validation test among distortion types\nQGeoQColSGeoSColPLCC SROCC AU CDS AU CBW PLCC SROCC AU CDS AU CBW PLCC SROCC AU CDS AU CBW PLCC SROCC AU CDS AU CBWCMDM0.8820.8250.5370.9330.9170.9240.8930.9730.930.940.8710.9950.8410.8410.6410.939CMDM Geo 0.6860.4810.5970.80.1210.2880.4570.3730.5960.730.6380.8740.4550.4860.4860.745CMDM Col 0.7870.7580.4930.9040.8210.8450.7720.9430.8890.9080.8380.9840.8530.7950.7120.934D LAB0.6530.6770.5010.8260.7990.8510.720.9320.7650.8410.7020.9380.5980.6290.6130.832SSIM0.8750.7940.7090.9030.7920.7080.6960.8960.7220.7560.6230.9010.5880.7040.5980.855HDR-VDP20.9460.9380.8050.9870.8820.780.7240.9390.7360.7620.590.90.7670.7770.6320.918iCID0.880.8380.6320.9260.860.810.7850.9390.7380.7610.6450.9060.6310.7470.6570.872CMDMCMDM_GeoCMDM_ColD_LABSSIMHDR\u2212VDP2iCIDPLCCSROCC1.000.750.500.250.00AUC DSAUC BW1.000.750.500.250.00Source modelsDistortion typesSource modelsDistortion types\n\nTABLE 7 :\n7\nPerformance comparison of different metrics on the whole dataset.This is the author's version of an article that has been published in this journal.Changes were made to this version by the publisher prior to publication.The final version of record is available at http://dx.doi.org/10.1109/TVCG.2020.3036153\nPLCCSROCCAU C DSAU C BWCMDM rec0.9130.90.7820.968CMDM Geo0.5010.4370.6040.749CMDM Col0.7450.7460.7320.893D LAB0.550.6030.6510.805SSIM0.7970.7990.7160.912HDR-VDP20.8530.840.7030.944iCID0.8250.830.7470.924\nCMDM performs notably better than the others in terms of correlations.Moreover, the AUC values reflect its good\n\n\nTABLE 8 :\n8\n[5]formance comparison of different metrics on a new dataset.For metrics marked with a *, the values are reprinted from[5].\nPLCC SROCCCMDMrec0.8620.872SSIM0.6240.657HDR-VDP20.830.844iCID0.5020.552Video-DCT*0.320.50Video-PSNR*0.330.58Video-MS-SSIM*0.670.66FQM*0.640.66CM 1 *0.740.77CM 2 *0.800.85\n\nTABLE 9 :\n9\nPerformance evolution of different metrics before and after integrating the viewpoint\n\u2206P LCC\u2206SROCC\u2206AU C DS\u2206AU C BWCMDM0-0.005-0.0010SSIM0.0180.030.0250.012HDR-VDP2-0.0220.018-0.0530.001iCID0.0570.0710.0490.03\n\nTABLE 10 :\n10\nPerformance comparison of different metrics on the pairs of stimuli significantly affected by the viewpoints.\nCMDM visSSIM visHDR-VDP2 visiCID visAU C DS0.6020.560.5610.58AU C BW0.580.660.80.668\nACKNOWLEDGMENTSThis work was supported by French National Research Agency as part of ANR-PISCo project (ANR-17-CE33-0005).Yana Nehm \u00e9 received a double degree in electronics and digital technology from the faculty of engineering of the Lebanese University and Ecole polytechnique de l'Universit \u00e9 de Nantes.She is now a PhD student at the Institut National des Sciences Appliqu \u00e9es de Lyon (Insa Lyon) under the supervision of Prof. Guillaume Lavou \u00e9.Her research interests include visual perception and visual quality assessment, immersive data, augmented and virtual reality.\nA Multiscale Metric for 3D Mesh Visual Quality Assessment. G Lavou\u00e9, Computer Graphics Forum. 3052011\n\nDihedral Angle Mesh Error: a fast perception correlated distortion measure for fixed connectivity triangle meshes. L V\u00e1\u0161a, J Rus, Computer Graphics Forum. 3152012\n\nA Fast Roughness-Based Approach to the Assessment of 3D Mesh Visual Quality. K Wang, F Torkhani, A Montanvert, Computers & Graphics. 2012\n\nBatex3: Bit allocation for progressive transmission of textured 3-d models. D Tian, G Alregib, IEEE Transactions on Circuits and Systems for Video Technology. 200818\n\nSubjective and objective visual quality assessment of textured 3D meshes. J Guo, V Vidal, I Cheng, A Basu, A Baskurt, G Lavoue, ACM Transactions on Applied Perception. 1422016\n\nDataset and Metrics for Predicting Local Visible Differences. N Ye, H.-P Seidel, K Myszkowski, R Mantiuk, A Steed, K Wolski, P Didyk, R K Mantiuk, D Giunchi, ACM Transactions on Graphics. 3752018\n\nQuality assessment in computer graphics. G Lavou\u00e9, R Mantiuk, Visual Signal Quality Assessment: Quality of Experience (QoE). 2015\n\nMeasuring and predicting visual fidelity. B Watson, ACM Siggraph. 2001\n\nAre image quality metrics adequate to evaluate the quality of geometric objects. B E Rogowitz, H Rushmeier, Proceedings of SPIE. SPIE2001\n\nQuality metric for approximating subjective evaluation of 3-D objects. Y Pan, I Cheng, A Basu, IEEE Transactions on Multimedia. 72apr 2005\n\nPerceptually driven 3D distance metrics with application to watermarking. G Lavoue, E Gelasca, F Dupont, A Baskurt, T Ebrahimi, SPIE. 20066312\n\nA local roughness measure for 3D meshes and its application to visual masking. G Lavou\u00e9, ACM Transactions on Applied Perception. 542009\n\nWatermarked 3-D Mesh Quality Assessment. M Corsini, E D Gelasca, T Ebrahimi, M Barni, IEEE Transactions on Multimedia. 92feb 2007\n\nA Perceptual Data Repository for Polygonal Meshes. S Silva, B S Santos, C Ferreira, J Madeira, 2009 Second International Conference in Visualisation. jul 2009\n\nK Christaki, E Christakis, P Drakoulis, Subjective Visual Quality Assessment of Immersive 3D Media Compressed by Open-Source Static 3D Mesh Codecs. 201825th International Conference on MultiMedia Modeling (MMM)\n\nComparison of subjective methods, with and without explicit reference, for quality assessment of 3D graphics. Y Nehm\u00e9, J P Farrugia, F Dupont, P Le Callet, G Lavou\u00e9, ACM Conference on Applied Perception. 2019\n\nA novel methodology for quality assessment of voxelized point clouds. E M Torlig, E Alexiou, T A Fonseca, R L De Queiroz, T Ebrahimi, SPIE Optical Engineering + Applications. 182018\n\nA comprehensive study of the rate-distortion performance in MPEG point cloud compression. I Evangelos Alexiou, T M Viola, T A Borges, R L Fonseca, T De Queiroz, Ebrahimi, APSIPA Transactions on Signal and Information Processing. 82019\n\nPoint cloud quality evaluation : Towards a definition for test conditions. L A D S Cruz, E Dumic, E Alexiou, J Prazeres, R Duarte, M Pereira, A Pinheiro, T Ebrahimi, International Conference on Quality of Multimedia Experience. 2019\n\nA Javaheri, C Brites, F Pereira, J Ascenso, arXiv:1912.09137Point Cloud Rendering after Coding : Impacts on Subjective and Objective Quality. 2019\n\nH Su, Z Duanmu, W Liu, Q Liu, Z Wang, IEEE International Conference on Image Processing. 2019Perceptual Qualty Assessment of point Clouds\n\nTextured Mesh vs Coloured Point Cloud : A Subjective Study for Volumetric Video Compression. E Zerman, C Ozcinar, P Gao, A Smolic, International Conference on Quality of Multimedia Experience. 2020\n\nVisual quality assessment of 3D models: On the influence of light-material interaction. K Vanhoey, B Sauvage, P Kraemer, G Lavou\u00e9, ACM Transactions on Applied Perception. 1512017\n\nA Curvature Tensor Distance for Mesh Visual Quality Assessment. F Torkhani, K Wang, J.-M Chassery, 2012\n\nPerceptual Metrics for Static and Dynamic Triangle Meshes. M Corsini, M C Larabi, G Lavou\u00e9, O Petrik, L V\u00e1\u0161a, K Wang, Computer Graphics Forum. 321feb 2013\n\nNo-reference mesh visual quality assessment via ensemble of convolutional neural networks and compact multilinear pooling. I Abouelaziz, A Chetouani, M El Hassouni, L J Latecki, H Cherifi, Pattern Recognition. 1001071742020\n\nA machine learning framework for full-reference 3D shape quality assessment. Z C Yildiz, A C Oztireli, T Capin, Visual Computer. 3612020\n\nJust Noticeable Distortion Profile for Flat-Shaded 3D Mesh Surfaces. G Nader, K Wang, H Franck, F Dupont, IEEE Trans. on Visualization and Computer Graphics. 2016\n\nEvaluating the local visibility of geometric artifacts. J Guo, V Vidal, A Baskurt, G Lavou, ACM Symposium in Applied Perception. 2015\n\nImage-difference prediction: From grayscale to color. I Lissner, J Preiss, P Urban, M S Lichtenauer, P Zolliker, IEEE Transactions on Image Processing. 2222013\n\nPCQM: A Full-Reference Quality Metric for Colored 3D Point Clouds. G Meynet, Y Nehm\u00e9, J Digne, G Lavou\u00e9, International Conference on Quality of Multimedia Experience. 2020\n\nOptimized mesh and texture multiplexing for progressive textured model transmission. S Yang, C.-H Lee, C Kuo, 2004ACM Multimedia\n\nProgressive compression of arbitrary textured meshes. F Caillaud, V Vidal, F Dupont, G Lavou\u00e9, Computer Graphics Forum. 3572016\n\nImage quality assessment: From error visibility to structural similarity. Z Wang, A Bovik, H Sheikh, E Simoncelli, IEEE Transactions on Image Processing. 1342004\n\nOn the Efficiency of Image Metrics for Evaluating the Visual Quality of 3D Models. G Lavoue, M C Larabi, L Vasa, IEEE Transactions on Visualization and Computer Graphics. 2282016\n\nSurface simplification using quadric error metrics. M Garland, P S Heckbert, 1997\n\nRate-distortion optimization for progressive compression of 3D mesh with color attributes. H Lee, G Lavou\u00e9, F Dupont, The Visual Computer. 282may 2012\n\nStudy of subjective and objective quality assessment of video. K Seshadrinathan, R Soundararajan, A C Bovik, L K Cormack, IEEE Transactions on Image Processing. 1906 2010\n\nPrediction of the influence of navigation scan-path on perceived quality of freeviewpoint videos. S Ling, J Guti\u00e9rrez, K Gu, P Le Callet, IEEE Journal on Emerging and Selected Topics in Circuits and Systems. 912019\n\nSubjective video quality assessment methods for multimedia applications. ITU-T P.9102009International Telecommunication Union\n\nG Regal, R Schatz, J Schrammel, S Suette, VRate: A Unity3D Asset for integrating Subjective Assessment Questionnaires in Virtual Environments. 05 2018\n\nMethodology for the subjective assessment of the quality of television pictures BT Series Broadcasting service. ITU-R BT.500-132012International Telecommunication Union\n\nRecover subjective quality scores from noisy measurements. Z Li, C Bampis, 04 2017\n\nA simple model for subject behavior in subjective experiments. Z Li, C Bampis, L Janowski, I Katsavounidis, arXiv:2004.02067v22020\n\nStudy of 3d virtual reality picture quality. M Chen, Y Jin, T Goodall, X Yu, A C Bovik, IEEE Journal of Selected Topics in Signal Processing. 1412020\n\nSubjective assessment of adaptive media playout for video streaming. P P\u00e9rez, N Garc\u00eda, Villegas, 2019 Eleventh International Conference on Quality of Multimedia Experience (QoMEX). 2019\n\n. P Alliez, S Tayeb, C Wormser, 2009Aabb tree.cgal 3.5 edition\n\nAnisotropic polygonal remeshing. P Alliez, D Cohen-Steiner, O Devillers, B L\u00e9vy, M Desbrun, ACM Trans. Graph. 223Jul. 2003\n\nToward a unified color space for perception-based image processing. I Lissner, P Urban, IEEE Transactions on Image Processing. 2132012\n\nA noreference metric for evaluating the quality of motion deblurring. Y Liu, J Wang, S Cho, A Finkelstein, S Rusinkiewicz, ACM Transactions on Graphics. 3262013\n\nOn the accuracy of objective image and video quality models: New methodology for performance evaluation. L Krasula, K Fliegel, P Le Callet, M Kl\u00edma, 2016 Eighth International Conference on Quality of Multimedia Experience. 2016\n\nHdr-vdp-2: A calibrated visual metric for visibility and quality predictions in all luminance conditions. R Mantiuk, K J Kim, A G Rempel, W Heidrich, ACM Trans. Graph. 304Jul. 2011\n\nColor-image quality assessment: From prediction to optimization. J Preiss, F Fernandes, P Urban, IEEE Transactions on Image Processing. 2332014\n\nDct-based video quality evaluation. F Xiao, Final Project for EE392J. 2000769\n\nMultiscale structural similarity for image quality assessment. Z Wang, E P Simoncelli, A C Bovik, The Thrity-Seventh Asilomar Conference on Signals. 2003. 20032\n", "annotations": {"author": "[{\"end\":141,\"start\":107},{\"end\":186,\"start\":142},{\"end\":247,\"start\":187},{\"end\":313,\"start\":248},{\"end\":352,\"start\":314},{\"end\":360,\"start\":353},{\"end\":385,\"start\":361},{\"end\":415,\"start\":386}]", "publisher": null, "author_last_name": "[{\"end\":116,\"start\":112},{\"end\":156,\"start\":150},{\"end\":209,\"start\":201},{\"end\":279,\"start\":270},{\"end\":329,\"start\":324}]", "author_first_name": "[{\"end\":111,\"start\":107},{\"end\":149,\"start\":142},{\"end\":200,\"start\":187},{\"end\":267,\"start\":260},{\"end\":269,\"start\":268},{\"end\":323,\"start\":314}]", "author_affiliation": "[{\"end\":359,\"start\":354},{\"end\":384,\"start\":362},{\"end\":414,\"start\":387}]", "title": "[{\"end\":104,\"start\":1},{\"end\":519,\"start\":416}]", "venue": null, "abstract": "[{\"end\":2522,\"start\":746}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4159,\"start\":4156},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4164,\"start\":4161},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4169,\"start\":4166},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4309,\"start\":4306},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4313,\"start\":4310},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6370,\"start\":6367},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6496,\"start\":6493},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6864,\"start\":6861},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6869,\"start\":6866},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6874,\"start\":6871},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6879,\"start\":6876},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6885,\"start\":6881},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6891,\"start\":6887},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6897,\"start\":6893},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6903,\"start\":6899},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6909,\"start\":6905},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6915,\"start\":6911},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6921,\"start\":6917},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6945,\"start\":6941},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6951,\"start\":6947},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6957,\"start\":6953},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6963,\"start\":6959},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6969,\"start\":6965},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7043,\"start\":7039},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7049,\"start\":7045},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7055,\"start\":7051},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7100,\"start\":7097},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7105,\"start\":7102},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7110,\"start\":7107},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7116,\"start\":7112},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7122,\"start\":7118},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7128,\"start\":7124},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7161,\"start\":7158},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7167,\"start\":7163},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7173,\"start\":7169},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7179,\"start\":7175},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7207,\"start\":7203},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7536,\"start\":7533},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7607,\"start\":7604},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7612,\"start\":7609},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7618,\"start\":7614},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7624,\"start\":7620},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7630,\"start\":7626},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7636,\"start\":7632},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7642,\"start\":7638},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7648,\"start\":7644},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7654,\"start\":7650},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7681,\"start\":7678},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7687,\"start\":7683},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7693,\"start\":7689},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7699,\"start\":7695},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7705,\"start\":7701},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7711,\"start\":7707},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7717,\"start\":7713},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7745,\"start\":7741},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7751,\"start\":7747},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8148,\"start\":8144},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8154,\"start\":8150},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8160,\"start\":8156},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8320,\"start\":8317},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8348,\"start\":8344},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8551,\"start\":8547},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8572,\"start\":8568},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8595,\"start\":8591},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8691,\"start\":8687},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9482,\"start\":9478},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9488,\"start\":9484},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9596,\"start\":9592},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10149,\"start\":10146},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10155,\"start\":10151},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10176,\"start\":10173},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10193,\"start\":10190},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10199,\"start\":10195},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10213,\"start\":10209},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10235,\"start\":10232},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10245,\"start\":10242},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10258,\"start\":10255},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10395,\"start\":10391},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10401,\"start\":10397},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10517,\"start\":10513},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10601,\"start\":10597},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10940,\"start\":10937},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10959,\"start\":10956},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11095,\"start\":11092},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11113,\"start\":11110},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11187,\"start\":11184},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11204,\"start\":11201},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11312,\"start\":11309},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11544,\"start\":11541},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11549,\"start\":11546},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11820,\"start\":11817},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11842,\"start\":11838},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12219,\"start\":12215},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12308,\"start\":12304},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12660,\"start\":12656},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13188,\"start\":13184},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13213,\"start\":13209},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13255,\"start\":13251},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13672,\"start\":13669},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14501,\"start\":14498},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14507,\"start\":14503},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15011,\"start\":15007},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16558,\"start\":16554},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":16947,\"start\":16943},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17194,\"start\":17190},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17635,\"start\":17632},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18422,\"start\":18418},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":20174,\"start\":20170},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20348,\"start\":20344},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22354,\"start\":22350},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":22649,\"start\":22645},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24006,\"start\":24002},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":26175,\"start\":26171},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":26702,\"start\":26698},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":27601,\"start\":27597},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28362,\"start\":28359},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":28480,\"start\":28476},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":29070,\"start\":29066},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":30553,\"start\":30549},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":30563,\"start\":30559},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":42907,\"start\":42904},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":43055,\"start\":43051},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":43172,\"start\":43168},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":44275,\"start\":44271},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":44929,\"start\":44926},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":46137,\"start\":46133},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":46619,\"start\":46616},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":47005,\"start\":47002},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":47510,\"start\":47506},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":47870,\"start\":47866},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":49376,\"start\":49372},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":50594,\"start\":50590},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":50731,\"start\":50727},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":50785,\"start\":50782},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":51525,\"start\":51521},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":52381,\"start\":52378},{\"end\":53436,\"start\":53417},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":56543,\"start\":56539},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":58888,\"start\":58884},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":58903,\"start\":58899},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":58914,\"start\":58910},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":60618,\"start\":60614},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":62704,\"start\":62700},{\"end\":64139,\"start\":64131},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":64188,\"start\":64185},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":65620,\"start\":65617},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":65755,\"start\":65751},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":65786,\"start\":65782},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":65885,\"start\":65882},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":65988,\"start\":65985},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":66814,\"start\":66811}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":74202,\"start\":74089},{\"attributes\":{\"id\":\"fig_2\"},\"end\":74301,\"start\":74203},{\"attributes\":{\"id\":\"fig_3\"},\"end\":74486,\"start\":74302},{\"attributes\":{\"id\":\"fig_5\"},\"end\":74623,\"start\":74487},{\"attributes\":{\"id\":\"fig_6\"},\"end\":74825,\"start\":74624},{\"attributes\":{\"id\":\"fig_7\"},\"end\":75031,\"start\":74826},{\"attributes\":{\"id\":\"fig_8\"},\"end\":75227,\"start\":75032},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":75289,\"start\":75228},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":75772,\"start\":75290},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":76934,\"start\":75773},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":77704,\"start\":76935},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":77983,\"start\":77705},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":78349,\"start\":77984},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":79358,\"start\":78350},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":80329,\"start\":79359},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":80967,\"start\":80330},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":81276,\"start\":80968},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":81498,\"start\":81277},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":81708,\"start\":81499}]", "paragraph": "[{\"end\":3780,\"start\":2538},{\"end\":3896,\"start\":3782},{\"end\":4686,\"start\":3898},{\"end\":5595,\"start\":4688},{\"end\":6497,\"start\":5612},{\"end\":7949,\"start\":6545},{\"end\":8885,\"start\":7951},{\"end\":9597,\"start\":8887},{\"end\":11505,\"start\":9627},{\"end\":12088,\"start\":11507},{\"end\":12884,\"start\":12090},{\"end\":14633,\"start\":12886},{\"end\":15304,\"start\":14659},{\"end\":15803,\"start\":15344},{\"end\":16109,\"start\":15819},{\"end\":16351,\"start\":16111},{\"end\":17407,\"start\":16353},{\"end\":18613,\"start\":17430},{\"end\":19140,\"start\":18776},{\"end\":19315,\"start\":19142},{\"end\":19564,\"start\":19317},{\"end\":19700,\"start\":19566},{\"end\":20611,\"start\":19741},{\"end\":21930,\"start\":20613},{\"end\":22570,\"start\":21932},{\"end\":23483,\"start\":22600},{\"end\":24479,\"start\":23514},{\"end\":24998,\"start\":24481},{\"end\":25617,\"start\":25012},{\"end\":26025,\"start\":25649},{\"end\":26254,\"start\":26076},{\"end\":26374,\"start\":26256},{\"end\":26502,\"start\":26402},{\"end\":26958,\"start\":26504},{\"end\":27608,\"start\":27041},{\"end\":28111,\"start\":27610},{\"end\":28310,\"start\":28134},{\"end\":28461,\"start\":28312},{\"end\":29498,\"start\":28463},{\"end\":29731,\"start\":29500},{\"end\":30075,\"start\":29733},{\"end\":30860,\"start\":30127},{\"end\":31880,\"start\":30862},{\"end\":32420,\"start\":31927},{\"end\":32741,\"start\":32464},{\"end\":32830,\"start\":32743},{\"end\":32986,\"start\":32832},{\"end\":33348,\"start\":32988},{\"end\":33590,\"start\":33350},{\"end\":33872,\"start\":33592},{\"end\":34344,\"start\":33894},{\"end\":35610,\"start\":34346},{\"end\":36888,\"start\":35612},{\"end\":37568,\"start\":36890},{\"end\":38613,\"start\":37606},{\"end\":39022,\"start\":38615},{\"end\":39953,\"start\":39024},{\"end\":40263,\"start\":39955},{\"end\":41068,\"start\":40265},{\"end\":41767,\"start\":41070},{\"end\":42546,\"start\":41838},{\"end\":43240,\"start\":42575},{\"end\":43906,\"start\":43242},{\"end\":44713,\"start\":43940},{\"end\":45485,\"start\":44742},{\"end\":45788,\"start\":45527},{\"end\":45949,\"start\":45823},{\"end\":46665,\"start\":45980},{\"end\":47125,\"start\":46910},{\"end\":47292,\"start\":47195},{\"end\":47779,\"start\":47320},{\"end\":48009,\"start\":47803},{\"end\":48599,\"start\":48357},{\"end\":48755,\"start\":48601},{\"end\":48928,\"start\":48757},{\"end\":49202,\"start\":48975},{\"end\":49377,\"start\":49264},{\"end\":49655,\"start\":49379},{\"end\":49940,\"start\":49691},{\"end\":50102,\"start\":49977},{\"end\":50260,\"start\":50104},{\"end\":51003,\"start\":50300},{\"end\":51460,\"start\":51042},{\"end\":51795,\"start\":51515},{\"end\":52421,\"start\":51822},{\"end\":53202,\"start\":52457},{\"end\":53549,\"start\":53204},{\"end\":54322,\"start\":53551},{\"end\":55014,\"start\":54324},{\"end\":56186,\"start\":55056},{\"end\":57495,\"start\":56232},{\"end\":58214,\"start\":57497},{\"end\":59480,\"start\":58251},{\"end\":60871,\"start\":59482},{\"end\":61328,\"start\":60873},{\"end\":62391,\"start\":61330},{\"end\":62645,\"start\":62415},{\"end\":63186,\"start\":62647},{\"end\":63789,\"start\":63188},{\"end\":65478,\"start\":63839},{\"end\":66063,\"start\":65480},{\"end\":67425,\"start\":66065},{\"end\":68234,\"start\":67458},{\"end\":68431,\"start\":68279},{\"end\":70105,\"start\":68433},{\"end\":71606,\"start\":70107},{\"end\":72677,\"start\":71637},{\"end\":73666,\"start\":72679},{\"end\":74088,\"start\":73668},{\"end\":74201,\"start\":74101},{\"end\":74300,\"start\":74215},{\"end\":74485,\"start\":74314},{\"end\":74622,\"start\":74499},{\"end\":74824,\"start\":74636},{\"end\":75030,\"start\":74838},{\"end\":75226,\"start\":75046},{\"end\":75288,\"start\":75241},{\"end\":75842,\"start\":75786},{\"end\":76939,\"start\":76938},{\"end\":77807,\"start\":77718},{\"end\":78032,\"start\":77997},{\"end\":78565,\"start\":78363},{\"end\":79463,\"start\":79372},{\"end\":80650,\"start\":80343},{\"end\":80966,\"start\":80855},{\"end\":81104,\"start\":80981},{\"end\":81375,\"start\":81290},{\"end\":81623,\"start\":81514}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":26401,\"start\":26375},{\"attributes\":{\"id\":\"formula_1\"},\"end\":26989,\"start\":26959},{\"attributes\":{\"id\":\"formula_2\"},\"end\":27016,\"start\":26989},{\"attributes\":{\"id\":\"formula_3\"},\"end\":27040,\"start\":27016},{\"attributes\":{\"id\":\"formula_4\"},\"end\":45526,\"start\":45486},{\"attributes\":{\"id\":\"formula_5\"},\"end\":46908,\"start\":46666},{\"attributes\":{\"id\":\"formula_6\"},\"end\":46909,\"start\":46908},{\"attributes\":{\"id\":\"formula_7\"},\"end\":47194,\"start\":47126},{\"attributes\":{\"id\":\"formula_8\"},\"end\":47802,\"start\":47780},{\"attributes\":{\"id\":\"formula_9\"},\"end\":48356,\"start\":48010},{\"attributes\":{\"id\":\"formula_10\"},\"end\":48974,\"start\":48929},{\"attributes\":{\"id\":\"formula_11\"},\"end\":49263,\"start\":49203},{\"attributes\":{\"id\":\"formula_12\"},\"end\":49975,\"start\":49941},{\"attributes\":{\"id\":\"formula_13\"},\"end\":49976,\"start\":49975},{\"attributes\":{\"id\":\"formula_14\"},\"end\":50299,\"start\":50261},{\"attributes\":{\"id\":\"formula_15\"},\"end\":51041,\"start\":51004},{\"attributes\":{\"id\":\"formula_16\"},\"end\":51514,\"start\":51461},{\"attributes\":{\"id\":\"formula_17\"},\"end\":68277,\"start\":68235},{\"attributes\":{\"id\":\"formula_18\"},\"end\":68278,\"start\":68277}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":15601,\"start\":15600},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":17313,\"start\":17312},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":31297,\"start\":31296},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":55166,\"start\":55165},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":59443,\"start\":59436},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":60870,\"start\":60862},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":60885,\"start\":60884},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":61387,\"start\":61386},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":63330,\"start\":63329},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":65517,\"start\":65516},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":66083,\"start\":66082},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":69071,\"start\":69070},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":70572,\"start\":70570}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2536,\"start\":2524},{\"attributes\":{\"n\":\"2\"},\"end\":5610,\"start\":5598},{\"attributes\":{\"n\":\"2.1\"},\"end\":6543,\"start\":6500},{\"attributes\":{\"n\":\"2.2\"},\"end\":9625,\"start\":9600},{\"attributes\":{\"n\":\"3\"},\"end\":14657,\"start\":14636},{\"attributes\":{\"n\":\"3.1\"},\"end\":15314,\"start\":15307},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":15342,\"start\":15317},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":15817,\"start\":15806},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":17428,\"start\":17410},{\"attributes\":{\"n\":\"3.2\"},\"end\":19739,\"start\":19703},{\"attributes\":{\"n\":\"3.3\"},\"end\":22598,\"start\":22573},{\"end\":23512,\"start\":23486},{\"end\":25010,\"start\":25001},{\"attributes\":{\"n\":\"4\"},\"end\":25647,\"start\":25620},{\"attributes\":{\"n\":\"4.1\"},\"end\":26074,\"start\":26028},{\"attributes\":{\"n\":\"4.2\"},\"end\":28132,\"start\":28114},{\"attributes\":{\"n\":\"4.3\"},\"end\":31925,\"start\":31883},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":32462,\"start\":32423},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":33892,\"start\":33875},{\"attributes\":{\"n\":\"4.3.3\"},\"end\":37604,\"start\":37571},{\"attributes\":{\"n\":\"5\"},\"end\":41836,\"start\":41770},{\"attributes\":{\"n\":\"5.1\"},\"end\":42573,\"start\":42549},{\"attributes\":{\"n\":\"5.2\"},\"end\":43938,\"start\":43909},{\"attributes\":{\"n\":\"5.3\"},\"end\":44740,\"start\":44716},{\"attributes\":{\"n\":\"5.4\"},\"end\":45821,\"start\":45791},{\"end\":45978,\"start\":45952},{\"end\":47318,\"start\":47295},{\"attributes\":{\"n\":\"5.5\"},\"end\":49689,\"start\":49658},{\"attributes\":{\"n\":\"6\"},\"end\":51820,\"start\":51798},{\"attributes\":{\"n\":\"6.1\"},\"end\":52455,\"start\":52424},{\"attributes\":{\"n\":\"6.2\"},\"end\":55054,\"start\":55017},{\"attributes\":{\"n\":\"6.3\"},\"end\":56230,\"start\":56189},{\"attributes\":{\"n\":\"6.4\"},\"end\":58249,\"start\":58217},{\"attributes\":{\"n\":\"6.5\"},\"end\":62413,\"start\":62394},{\"attributes\":{\"n\":\"6.6\"},\"end\":63837,\"start\":63792},{\"attributes\":{\"n\":\"7\"},\"end\":67456,\"start\":67428},{\"attributes\":{\"n\":\"8\"},\"end\":71635,\"start\":71609},{\"end\":74098,\"start\":74090},{\"end\":74212,\"start\":74204},{\"end\":74311,\"start\":74303},{\"end\":74496,\"start\":74488},{\"end\":74633,\"start\":74625},{\"end\":74835,\"start\":74827},{\"end\":75042,\"start\":75033},{\"end\":75238,\"start\":75229},{\"end\":75381,\"start\":75291},{\"end\":75783,\"start\":75774},{\"end\":77715,\"start\":77706},{\"end\":77994,\"start\":77985},{\"end\":78360,\"start\":78351},{\"end\":79369,\"start\":79360},{\"end\":80340,\"start\":80331},{\"end\":80978,\"start\":80969},{\"end\":81287,\"start\":81278},{\"end\":81510,\"start\":81500}]", "table": "[{\"end\":75772,\"start\":75382},{\"end\":76934,\"start\":75843},{\"end\":77704,\"start\":76940},{\"end\":77983,\"start\":77808},{\"end\":78349,\"start\":78033},{\"end\":79358,\"start\":78566},{\"end\":80329,\"start\":79464},{\"end\":80854,\"start\":80651},{\"end\":81276,\"start\":81105},{\"end\":81498,\"start\":81376},{\"end\":81708,\"start\":81624}]", "figure_caption": "[{\"end\":74202,\"start\":74100},{\"end\":74301,\"start\":74214},{\"end\":74486,\"start\":74313},{\"end\":74623,\"start\":74498},{\"end\":74825,\"start\":74635},{\"end\":75031,\"start\":74837},{\"end\":75227,\"start\":75045},{\"end\":75289,\"start\":75240},{\"end\":75843,\"start\":75785},{\"end\":76940,\"start\":76937},{\"end\":77808,\"start\":77717},{\"end\":78033,\"start\":77996},{\"end\":78566,\"start\":78362},{\"end\":79464,\"start\":79371},{\"end\":80651,\"start\":80342},{\"end\":81105,\"start\":80980},{\"end\":81376,\"start\":81289},{\"end\":81624,\"start\":81513}]", "figure_ref": "[{\"end\":15179,\"start\":15178},{\"end\":15659,\"start\":15658},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17204,\"start\":17203},{\"end\":19066,\"start\":19065},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22460,\"start\":22459},{\"end\":22880,\"start\":22879},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29642,\"start\":29635},{\"end\":29893,\"start\":29892},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30452,\"start\":30445},{\"end\":32472,\"start\":32471},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34082,\"start\":34081},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34340,\"start\":34339},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34608,\"start\":34607},{\"end\":34643,\"start\":34642},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34780,\"start\":34779},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":35345,\"start\":35338},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":35669,\"start\":35668},{\"end\":36241,\"start\":36240},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":36484,\"start\":36483},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":37794,\"start\":37793},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":37894,\"start\":37893},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":38754,\"start\":38753},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":39301,\"start\":39300},{\"end\":39593,\"start\":39592},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":59319,\"start\":59318},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":59541,\"start\":59540},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":63341,\"start\":63339}]", "bib_author_first_name": "[{\"end\":82347,\"start\":82346},{\"end\":82506,\"start\":82505},{\"end\":82514,\"start\":82513},{\"end\":82632,\"start\":82631},{\"end\":82640,\"start\":82639},{\"end\":82652,\"start\":82651},{\"end\":82770,\"start\":82769},{\"end\":82778,\"start\":82777},{\"end\":82935,\"start\":82934},{\"end\":82942,\"start\":82941},{\"end\":82951,\"start\":82950},{\"end\":82960,\"start\":82959},{\"end\":82968,\"start\":82967},{\"end\":82979,\"start\":82978},{\"end\":83100,\"start\":83099},{\"end\":83109,\"start\":83105},{\"end\":83119,\"start\":83118},{\"end\":83133,\"start\":83132},{\"end\":83144,\"start\":83143},{\"end\":83153,\"start\":83152},{\"end\":83163,\"start\":83162},{\"end\":83172,\"start\":83171},{\"end\":83174,\"start\":83173},{\"end\":83185,\"start\":83184},{\"end\":83276,\"start\":83275},{\"end\":83286,\"start\":83285},{\"end\":83408,\"start\":83407},{\"end\":83519,\"start\":83518},{\"end\":83521,\"start\":83520},{\"end\":83533,\"start\":83532},{\"end\":83648,\"start\":83647},{\"end\":83655,\"start\":83654},{\"end\":83664,\"start\":83663},{\"end\":83791,\"start\":83790},{\"end\":83801,\"start\":83800},{\"end\":83812,\"start\":83811},{\"end\":83822,\"start\":83821},{\"end\":83833,\"start\":83832},{\"end\":83940,\"start\":83939},{\"end\":84039,\"start\":84038},{\"end\":84050,\"start\":84049},{\"end\":84052,\"start\":84051},{\"end\":84063,\"start\":84062},{\"end\":84075,\"start\":84074},{\"end\":84180,\"start\":84179},{\"end\":84189,\"start\":84188},{\"end\":84191,\"start\":84190},{\"end\":84201,\"start\":84200},{\"end\":84213,\"start\":84212},{\"end\":84289,\"start\":84288},{\"end\":84302,\"start\":84301},{\"end\":84316,\"start\":84315},{\"end\":84611,\"start\":84610},{\"end\":84620,\"start\":84619},{\"end\":84622,\"start\":84621},{\"end\":84634,\"start\":84633},{\"end\":84644,\"start\":84643},{\"end\":84647,\"start\":84645},{\"end\":84657,\"start\":84656},{\"end\":84781,\"start\":84780},{\"end\":84783,\"start\":84782},{\"end\":84793,\"start\":84792},{\"end\":84804,\"start\":84803},{\"end\":84806,\"start\":84805},{\"end\":84817,\"start\":84816},{\"end\":84819,\"start\":84818},{\"end\":84833,\"start\":84832},{\"end\":84984,\"start\":84983},{\"end\":85005,\"start\":85004},{\"end\":85007,\"start\":85006},{\"end\":85016,\"start\":85015},{\"end\":85018,\"start\":85017},{\"end\":85028,\"start\":85027},{\"end\":85030,\"start\":85029},{\"end\":85041,\"start\":85040},{\"end\":85205,\"start\":85204},{\"end\":85211,\"start\":85206},{\"end\":85219,\"start\":85218},{\"end\":85228,\"start\":85227},{\"end\":85239,\"start\":85238},{\"end\":85251,\"start\":85250},{\"end\":85261,\"start\":85260},{\"end\":85272,\"start\":85271},{\"end\":85284,\"start\":85283},{\"end\":85364,\"start\":85363},{\"end\":85376,\"start\":85375},{\"end\":85386,\"start\":85385},{\"end\":85397,\"start\":85396},{\"end\":85512,\"start\":85511},{\"end\":85518,\"start\":85517},{\"end\":85528,\"start\":85527},{\"end\":85535,\"start\":85534},{\"end\":85542,\"start\":85541},{\"end\":85744,\"start\":85743},{\"end\":85754,\"start\":85753},{\"end\":85765,\"start\":85764},{\"end\":85772,\"start\":85771},{\"end\":85938,\"start\":85937},{\"end\":85949,\"start\":85948},{\"end\":85960,\"start\":85959},{\"end\":85971,\"start\":85970},{\"end\":86094,\"start\":86093},{\"end\":86106,\"start\":86105},{\"end\":86117,\"start\":86113},{\"end\":86194,\"start\":86193},{\"end\":86205,\"start\":86204},{\"end\":86207,\"start\":86206},{\"end\":86217,\"start\":86216},{\"end\":86227,\"start\":86226},{\"end\":86237,\"start\":86236},{\"end\":86245,\"start\":86244},{\"end\":86414,\"start\":86413},{\"end\":86428,\"start\":86427},{\"end\":86441,\"start\":86440},{\"end\":86444,\"start\":86442},{\"end\":86456,\"start\":86455},{\"end\":86458,\"start\":86457},{\"end\":86469,\"start\":86468},{\"end\":86593,\"start\":86592},{\"end\":86595,\"start\":86594},{\"end\":86605,\"start\":86604},{\"end\":86607,\"start\":86606},{\"end\":86619,\"start\":86618},{\"end\":86723,\"start\":86722},{\"end\":86732,\"start\":86731},{\"end\":86740,\"start\":86739},{\"end\":86750,\"start\":86749},{\"end\":86874,\"start\":86873},{\"end\":86881,\"start\":86880},{\"end\":86890,\"start\":86889},{\"end\":86901,\"start\":86900},{\"end\":87007,\"start\":87006},{\"end\":87018,\"start\":87017},{\"end\":87028,\"start\":87027},{\"end\":87037,\"start\":87036},{\"end\":87039,\"start\":87038},{\"end\":87054,\"start\":87053},{\"end\":87181,\"start\":87180},{\"end\":87191,\"start\":87190},{\"end\":87200,\"start\":87199},{\"end\":87209,\"start\":87208},{\"end\":87372,\"start\":87371},{\"end\":87383,\"start\":87379},{\"end\":87390,\"start\":87389},{\"end\":87471,\"start\":87470},{\"end\":87483,\"start\":87482},{\"end\":87492,\"start\":87491},{\"end\":87502,\"start\":87501},{\"end\":87620,\"start\":87619},{\"end\":87628,\"start\":87627},{\"end\":87637,\"start\":87636},{\"end\":87647,\"start\":87646},{\"end\":87792,\"start\":87791},{\"end\":87802,\"start\":87801},{\"end\":87804,\"start\":87803},{\"end\":87814,\"start\":87813},{\"end\":87941,\"start\":87940},{\"end\":87952,\"start\":87951},{\"end\":87954,\"start\":87953},{\"end\":88063,\"start\":88062},{\"end\":88070,\"start\":88069},{\"end\":88080,\"start\":88079},{\"end\":88187,\"start\":88186},{\"end\":88205,\"start\":88204},{\"end\":88222,\"start\":88221},{\"end\":88224,\"start\":88223},{\"end\":88233,\"start\":88232},{\"end\":88235,\"start\":88234},{\"end\":88394,\"start\":88393},{\"end\":88402,\"start\":88401},{\"end\":88415,\"start\":88414},{\"end\":88421,\"start\":88420},{\"end\":88424,\"start\":88422},{\"end\":88639,\"start\":88638},{\"end\":88648,\"start\":88647},{\"end\":88658,\"start\":88657},{\"end\":88671,\"start\":88670},{\"end\":89020,\"start\":89019},{\"end\":89026,\"start\":89025},{\"end\":89108,\"start\":89107},{\"end\":89114,\"start\":89113},{\"end\":89124,\"start\":89123},{\"end\":89136,\"start\":89135},{\"end\":89222,\"start\":89221},{\"end\":89230,\"start\":89229},{\"end\":89237,\"start\":89236},{\"end\":89248,\"start\":89247},{\"end\":89254,\"start\":89253},{\"end\":89256,\"start\":89255},{\"end\":89397,\"start\":89396},{\"end\":89406,\"start\":89405},{\"end\":89518,\"start\":89517},{\"end\":89528,\"start\":89527},{\"end\":89537,\"start\":89536},{\"end\":89613,\"start\":89612},{\"end\":89623,\"start\":89622},{\"end\":89640,\"start\":89639},{\"end\":89653,\"start\":89652},{\"end\":89661,\"start\":89660},{\"end\":89772,\"start\":89771},{\"end\":89783,\"start\":89782},{\"end\":89910,\"start\":89909},{\"end\":89917,\"start\":89916},{\"end\":89925,\"start\":89924},{\"end\":89932,\"start\":89931},{\"end\":89947,\"start\":89946},{\"end\":90107,\"start\":90106},{\"end\":90118,\"start\":90117},{\"end\":90129,\"start\":90128},{\"end\":90132,\"start\":90130},{\"end\":90142,\"start\":90141},{\"end\":90337,\"start\":90336},{\"end\":90348,\"start\":90347},{\"end\":90350,\"start\":90349},{\"end\":90357,\"start\":90356},{\"end\":90359,\"start\":90358},{\"end\":90369,\"start\":90368},{\"end\":90478,\"start\":90477},{\"end\":90488,\"start\":90487},{\"end\":90501,\"start\":90500},{\"end\":90594,\"start\":90593},{\"end\":90700,\"start\":90699},{\"end\":90708,\"start\":90707},{\"end\":90710,\"start\":90709},{\"end\":90724,\"start\":90723},{\"end\":90726,\"start\":90725}]", "bib_author_last_name": "[{\"end\":82354,\"start\":82348},{\"end\":82511,\"start\":82507},{\"end\":82518,\"start\":82515},{\"end\":82637,\"start\":82633},{\"end\":82649,\"start\":82641},{\"end\":82663,\"start\":82653},{\"end\":82775,\"start\":82771},{\"end\":82786,\"start\":82779},{\"end\":82939,\"start\":82936},{\"end\":82948,\"start\":82943},{\"end\":82957,\"start\":82952},{\"end\":82965,\"start\":82961},{\"end\":82976,\"start\":82969},{\"end\":82986,\"start\":82980},{\"end\":83103,\"start\":83101},{\"end\":83116,\"start\":83110},{\"end\":83130,\"start\":83120},{\"end\":83141,\"start\":83134},{\"end\":83150,\"start\":83145},{\"end\":83160,\"start\":83154},{\"end\":83169,\"start\":83164},{\"end\":83182,\"start\":83175},{\"end\":83193,\"start\":83186},{\"end\":83283,\"start\":83277},{\"end\":83294,\"start\":83287},{\"end\":83415,\"start\":83409},{\"end\":83530,\"start\":83522},{\"end\":83543,\"start\":83534},{\"end\":83652,\"start\":83649},{\"end\":83661,\"start\":83656},{\"end\":83669,\"start\":83665},{\"end\":83798,\"start\":83792},{\"end\":83809,\"start\":83802},{\"end\":83819,\"start\":83813},{\"end\":83830,\"start\":83823},{\"end\":83842,\"start\":83834},{\"end\":83947,\"start\":83941},{\"end\":84047,\"start\":84040},{\"end\":84060,\"start\":84053},{\"end\":84072,\"start\":84064},{\"end\":84081,\"start\":84076},{\"end\":84186,\"start\":84181},{\"end\":84198,\"start\":84192},{\"end\":84210,\"start\":84202},{\"end\":84221,\"start\":84214},{\"end\":84299,\"start\":84290},{\"end\":84313,\"start\":84303},{\"end\":84326,\"start\":84317},{\"end\":84617,\"start\":84612},{\"end\":84631,\"start\":84623},{\"end\":84641,\"start\":84635},{\"end\":84654,\"start\":84648},{\"end\":84664,\"start\":84658},{\"end\":84790,\"start\":84784},{\"end\":84801,\"start\":84794},{\"end\":84814,\"start\":84807},{\"end\":84830,\"start\":84820},{\"end\":84842,\"start\":84834},{\"end\":85002,\"start\":84985},{\"end\":85013,\"start\":85008},{\"end\":85025,\"start\":85019},{\"end\":85038,\"start\":85031},{\"end\":85052,\"start\":85042},{\"end\":85062,\"start\":85054},{\"end\":85216,\"start\":85212},{\"end\":85225,\"start\":85220},{\"end\":85236,\"start\":85229},{\"end\":85248,\"start\":85240},{\"end\":85258,\"start\":85252},{\"end\":85269,\"start\":85262},{\"end\":85281,\"start\":85273},{\"end\":85293,\"start\":85285},{\"end\":85373,\"start\":85365},{\"end\":85383,\"start\":85377},{\"end\":85394,\"start\":85387},{\"end\":85405,\"start\":85398},{\"end\":85515,\"start\":85513},{\"end\":85525,\"start\":85519},{\"end\":85532,\"start\":85529},{\"end\":85539,\"start\":85536},{\"end\":85547,\"start\":85543},{\"end\":85751,\"start\":85745},{\"end\":85762,\"start\":85755},{\"end\":85769,\"start\":85766},{\"end\":85779,\"start\":85773},{\"end\":85946,\"start\":85939},{\"end\":85957,\"start\":85950},{\"end\":85968,\"start\":85961},{\"end\":85978,\"start\":85972},{\"end\":86103,\"start\":86095},{\"end\":86111,\"start\":86107},{\"end\":86126,\"start\":86118},{\"end\":86202,\"start\":86195},{\"end\":86214,\"start\":86208},{\"end\":86224,\"start\":86218},{\"end\":86234,\"start\":86228},{\"end\":86242,\"start\":86238},{\"end\":86250,\"start\":86246},{\"end\":86425,\"start\":86415},{\"end\":86438,\"start\":86429},{\"end\":86453,\"start\":86445},{\"end\":86466,\"start\":86459},{\"end\":86477,\"start\":86470},{\"end\":86602,\"start\":86596},{\"end\":86616,\"start\":86608},{\"end\":86625,\"start\":86620},{\"end\":86729,\"start\":86724},{\"end\":86737,\"start\":86733},{\"end\":86747,\"start\":86741},{\"end\":86757,\"start\":86751},{\"end\":86878,\"start\":86875},{\"end\":86887,\"start\":86882},{\"end\":86898,\"start\":86891},{\"end\":86907,\"start\":86902},{\"end\":87015,\"start\":87008},{\"end\":87025,\"start\":87019},{\"end\":87034,\"start\":87029},{\"end\":87051,\"start\":87040},{\"end\":87063,\"start\":87055},{\"end\":87188,\"start\":87182},{\"end\":87197,\"start\":87192},{\"end\":87206,\"start\":87201},{\"end\":87216,\"start\":87210},{\"end\":87377,\"start\":87373},{\"end\":87387,\"start\":87384},{\"end\":87394,\"start\":87391},{\"end\":87480,\"start\":87472},{\"end\":87489,\"start\":87484},{\"end\":87499,\"start\":87493},{\"end\":87509,\"start\":87503},{\"end\":87625,\"start\":87621},{\"end\":87634,\"start\":87629},{\"end\":87644,\"start\":87638},{\"end\":87658,\"start\":87648},{\"end\":87799,\"start\":87793},{\"end\":87811,\"start\":87805},{\"end\":87819,\"start\":87815},{\"end\":87949,\"start\":87942},{\"end\":87963,\"start\":87955},{\"end\":88067,\"start\":88064},{\"end\":88077,\"start\":88071},{\"end\":88087,\"start\":88081},{\"end\":88202,\"start\":88188},{\"end\":88219,\"start\":88206},{\"end\":88230,\"start\":88225},{\"end\":88243,\"start\":88236},{\"end\":88399,\"start\":88395},{\"end\":88412,\"start\":88403},{\"end\":88418,\"start\":88416},{\"end\":88431,\"start\":88425},{\"end\":88645,\"start\":88640},{\"end\":88655,\"start\":88649},{\"end\":88668,\"start\":88659},{\"end\":88678,\"start\":88672},{\"end\":89023,\"start\":89021},{\"end\":89033,\"start\":89027},{\"end\":89111,\"start\":89109},{\"end\":89121,\"start\":89115},{\"end\":89133,\"start\":89125},{\"end\":89150,\"start\":89137},{\"end\":89227,\"start\":89223},{\"end\":89234,\"start\":89231},{\"end\":89245,\"start\":89238},{\"end\":89251,\"start\":89249},{\"end\":89262,\"start\":89257},{\"end\":89403,\"start\":89398},{\"end\":89413,\"start\":89407},{\"end\":89423,\"start\":89415},{\"end\":89525,\"start\":89519},{\"end\":89534,\"start\":89529},{\"end\":89545,\"start\":89538},{\"end\":89620,\"start\":89614},{\"end\":89637,\"start\":89624},{\"end\":89650,\"start\":89641},{\"end\":89658,\"start\":89654},{\"end\":89669,\"start\":89662},{\"end\":89780,\"start\":89773},{\"end\":89789,\"start\":89784},{\"end\":89914,\"start\":89911},{\"end\":89922,\"start\":89918},{\"end\":89929,\"start\":89926},{\"end\":89944,\"start\":89933},{\"end\":89960,\"start\":89948},{\"end\":90115,\"start\":90108},{\"end\":90126,\"start\":90119},{\"end\":90139,\"start\":90133},{\"end\":90148,\"start\":90143},{\"end\":90345,\"start\":90338},{\"end\":90354,\"start\":90351},{\"end\":90366,\"start\":90360},{\"end\":90378,\"start\":90370},{\"end\":90485,\"start\":90479},{\"end\":90498,\"start\":90489},{\"end\":90507,\"start\":90502},{\"end\":90599,\"start\":90595},{\"end\":90705,\"start\":90701},{\"end\":90721,\"start\":90711},{\"end\":90732,\"start\":90727}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":16657222},\"end\":82388,\"start\":82287},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3286512},\"end\":82552,\"start\":82390},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":15637365},\"end\":82691,\"start\":82554},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":15480121},\"end\":82858,\"start\":82693},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":17568760},\"end\":83035,\"start\":82860},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":4807803},\"end\":83232,\"start\":83037},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":6864285},\"end\":83363,\"start\":83234},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":13271725},\"end\":83435,\"start\":83365},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":16399426},\"end\":83574,\"start\":83437},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6591149},\"end\":83714,\"start\":83576},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2675122},\"end\":83858,\"start\":83716},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":16577037},\"end\":83995,\"start\":83860},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":9630004},\"end\":84126,\"start\":83997},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":27537706},\"end\":84286,\"start\":84128},{\"attributes\":{\"id\":\"b14\"},\"end\":84498,\"start\":84288},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":202676052},\"end\":84708,\"start\":84500},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":53399730},\"end\":84891,\"start\":84710},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":209089146},\"end\":85127,\"start\":84893},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":131772076},\"end\":85361,\"start\":85129},{\"attributes\":{\"doi\":\"arXiv:1912.09137\",\"id\":\"b19\"},\"end\":85509,\"start\":85363},{\"attributes\":{\"id\":\"b20\"},\"end\":85648,\"start\":85511},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":215752296},\"end\":85847,\"start\":85650},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1640517},\"end\":86027,\"start\":85849},{\"attributes\":{\"id\":\"b23\"},\"end\":86132,\"start\":86029},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":87332},\"end\":86288,\"start\":86134},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":214035605},\"end\":86513,\"start\":86290},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":52109770},\"end\":86651,\"start\":86515},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":216119384},\"end\":86815,\"start\":86653},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1366405},\"end\":86950,\"start\":86817},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1359733},\"end\":87111,\"start\":86952},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":215535683},\"end\":87284,\"start\":87113},{\"attributes\":{\"id\":\"b31\"},\"end\":87414,\"start\":87286},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":21565866},\"end\":87543,\"start\":87416},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":207761262},\"end\":87706,\"start\":87545},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":9490200},\"end\":87886,\"start\":87708},{\"attributes\":{\"id\":\"b35\"},\"end\":87969,\"start\":87888},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2295260},\"end\":88121,\"start\":87971},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":206724285},\"end\":88293,\"start\":88123},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":52954716},\"end\":88509,\"start\":88295},{\"attributes\":{\"doi\":\"ITU-T P.910\",\"id\":\"b39\"},\"end\":88636,\"start\":88511},{\"attributes\":{\"id\":\"b40\"},\"end\":88788,\"start\":88638},{\"attributes\":{\"doi\":\"ITU-R BT.500-13\",\"id\":\"b41\"},\"end\":88958,\"start\":88790},{\"attributes\":{\"id\":\"b42\"},\"end\":89042,\"start\":88960},{\"attributes\":{\"doi\":\"arXiv:2004.02067v2\",\"id\":\"b43\"},\"end\":89174,\"start\":89044},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":203904878},\"end\":89325,\"start\":89176},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":195697335},\"end\":89513,\"start\":89327},{\"attributes\":{\"id\":\"b46\"},\"end\":89577,\"start\":89515},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":1533136},\"end\":89701,\"start\":89579},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":8163431},\"end\":89837,\"start\":89703},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":7377337},\"end\":89999,\"start\":89839},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":19411829},\"end\":90228,\"start\":90001},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":756729},\"end\":90410,\"start\":90230},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":17037200},\"end\":90555,\"start\":90412},{\"attributes\":{\"id\":\"b53\"},\"end\":90634,\"start\":90557},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":60600316},\"end\":90796,\"start\":90636}]", "bib_title": "[{\"end\":82344,\"start\":82287},{\"end\":82503,\"start\":82390},{\"end\":82629,\"start\":82554},{\"end\":82767,\"start\":82693},{\"end\":82932,\"start\":82860},{\"end\":83097,\"start\":83037},{\"end\":83273,\"start\":83234},{\"end\":83405,\"start\":83365},{\"end\":83516,\"start\":83437},{\"end\":83645,\"start\":83576},{\"end\":83788,\"start\":83716},{\"end\":83937,\"start\":83860},{\"end\":84036,\"start\":83997},{\"end\":84177,\"start\":84128},{\"end\":84608,\"start\":84500},{\"end\":84778,\"start\":84710},{\"end\":84981,\"start\":84893},{\"end\":85202,\"start\":85129},{\"end\":85741,\"start\":85650},{\"end\":85935,\"start\":85849},{\"end\":86191,\"start\":86134},{\"end\":86411,\"start\":86290},{\"end\":86590,\"start\":86515},{\"end\":86720,\"start\":86653},{\"end\":86871,\"start\":86817},{\"end\":87004,\"start\":86952},{\"end\":87178,\"start\":87113},{\"end\":87468,\"start\":87416},{\"end\":87617,\"start\":87545},{\"end\":87789,\"start\":87708},{\"end\":88060,\"start\":87971},{\"end\":88184,\"start\":88123},{\"end\":88391,\"start\":88295},{\"end\":89219,\"start\":89176},{\"end\":89394,\"start\":89327},{\"end\":89610,\"start\":89579},{\"end\":89769,\"start\":89703},{\"end\":89907,\"start\":89839},{\"end\":90104,\"start\":90001},{\"end\":90334,\"start\":90230},{\"end\":90475,\"start\":90412},{\"end\":90591,\"start\":90557},{\"end\":90697,\"start\":90636}]", "bib_author": "[{\"end\":82356,\"start\":82346},{\"end\":82513,\"start\":82505},{\"end\":82520,\"start\":82513},{\"end\":82639,\"start\":82631},{\"end\":82651,\"start\":82639},{\"end\":82665,\"start\":82651},{\"end\":82777,\"start\":82769},{\"end\":82788,\"start\":82777},{\"end\":82941,\"start\":82934},{\"end\":82950,\"start\":82941},{\"end\":82959,\"start\":82950},{\"end\":82967,\"start\":82959},{\"end\":82978,\"start\":82967},{\"end\":82988,\"start\":82978},{\"end\":83105,\"start\":83099},{\"end\":83118,\"start\":83105},{\"end\":83132,\"start\":83118},{\"end\":83143,\"start\":83132},{\"end\":83152,\"start\":83143},{\"end\":83162,\"start\":83152},{\"end\":83171,\"start\":83162},{\"end\":83184,\"start\":83171},{\"end\":83195,\"start\":83184},{\"end\":83285,\"start\":83275},{\"end\":83296,\"start\":83285},{\"end\":83417,\"start\":83407},{\"end\":83532,\"start\":83518},{\"end\":83545,\"start\":83532},{\"end\":83654,\"start\":83647},{\"end\":83663,\"start\":83654},{\"end\":83671,\"start\":83663},{\"end\":83800,\"start\":83790},{\"end\":83811,\"start\":83800},{\"end\":83821,\"start\":83811},{\"end\":83832,\"start\":83821},{\"end\":83844,\"start\":83832},{\"end\":83949,\"start\":83939},{\"end\":84049,\"start\":84038},{\"end\":84062,\"start\":84049},{\"end\":84074,\"start\":84062},{\"end\":84083,\"start\":84074},{\"end\":84188,\"start\":84179},{\"end\":84200,\"start\":84188},{\"end\":84212,\"start\":84200},{\"end\":84223,\"start\":84212},{\"end\":84301,\"start\":84288},{\"end\":84315,\"start\":84301},{\"end\":84328,\"start\":84315},{\"end\":84619,\"start\":84610},{\"end\":84633,\"start\":84619},{\"end\":84643,\"start\":84633},{\"end\":84656,\"start\":84643},{\"end\":84666,\"start\":84656},{\"end\":84792,\"start\":84780},{\"end\":84803,\"start\":84792},{\"end\":84816,\"start\":84803},{\"end\":84832,\"start\":84816},{\"end\":84844,\"start\":84832},{\"end\":85004,\"start\":84983},{\"end\":85015,\"start\":85004},{\"end\":85027,\"start\":85015},{\"end\":85040,\"start\":85027},{\"end\":85054,\"start\":85040},{\"end\":85064,\"start\":85054},{\"end\":85218,\"start\":85204},{\"end\":85227,\"start\":85218},{\"end\":85238,\"start\":85227},{\"end\":85250,\"start\":85238},{\"end\":85260,\"start\":85250},{\"end\":85271,\"start\":85260},{\"end\":85283,\"start\":85271},{\"end\":85295,\"start\":85283},{\"end\":85375,\"start\":85363},{\"end\":85385,\"start\":85375},{\"end\":85396,\"start\":85385},{\"end\":85407,\"start\":85396},{\"end\":85517,\"start\":85511},{\"end\":85527,\"start\":85517},{\"end\":85534,\"start\":85527},{\"end\":85541,\"start\":85534},{\"end\":85549,\"start\":85541},{\"end\":85753,\"start\":85743},{\"end\":85764,\"start\":85753},{\"end\":85771,\"start\":85764},{\"end\":85781,\"start\":85771},{\"end\":85948,\"start\":85937},{\"end\":85959,\"start\":85948},{\"end\":85970,\"start\":85959},{\"end\":85980,\"start\":85970},{\"end\":86105,\"start\":86093},{\"end\":86113,\"start\":86105},{\"end\":86128,\"start\":86113},{\"end\":86204,\"start\":86193},{\"end\":86216,\"start\":86204},{\"end\":86226,\"start\":86216},{\"end\":86236,\"start\":86226},{\"end\":86244,\"start\":86236},{\"end\":86252,\"start\":86244},{\"end\":86427,\"start\":86413},{\"end\":86440,\"start\":86427},{\"end\":86455,\"start\":86440},{\"end\":86468,\"start\":86455},{\"end\":86479,\"start\":86468},{\"end\":86604,\"start\":86592},{\"end\":86618,\"start\":86604},{\"end\":86627,\"start\":86618},{\"end\":86731,\"start\":86722},{\"end\":86739,\"start\":86731},{\"end\":86749,\"start\":86739},{\"end\":86759,\"start\":86749},{\"end\":86880,\"start\":86873},{\"end\":86889,\"start\":86880},{\"end\":86900,\"start\":86889},{\"end\":86909,\"start\":86900},{\"end\":87017,\"start\":87006},{\"end\":87027,\"start\":87017},{\"end\":87036,\"start\":87027},{\"end\":87053,\"start\":87036},{\"end\":87065,\"start\":87053},{\"end\":87190,\"start\":87180},{\"end\":87199,\"start\":87190},{\"end\":87208,\"start\":87199},{\"end\":87218,\"start\":87208},{\"end\":87379,\"start\":87371},{\"end\":87389,\"start\":87379},{\"end\":87396,\"start\":87389},{\"end\":87482,\"start\":87470},{\"end\":87491,\"start\":87482},{\"end\":87501,\"start\":87491},{\"end\":87511,\"start\":87501},{\"end\":87627,\"start\":87619},{\"end\":87636,\"start\":87627},{\"end\":87646,\"start\":87636},{\"end\":87660,\"start\":87646},{\"end\":87801,\"start\":87791},{\"end\":87813,\"start\":87801},{\"end\":87821,\"start\":87813},{\"end\":87951,\"start\":87940},{\"end\":87965,\"start\":87951},{\"end\":88069,\"start\":88062},{\"end\":88079,\"start\":88069},{\"end\":88089,\"start\":88079},{\"end\":88204,\"start\":88186},{\"end\":88221,\"start\":88204},{\"end\":88232,\"start\":88221},{\"end\":88245,\"start\":88232},{\"end\":88401,\"start\":88393},{\"end\":88414,\"start\":88401},{\"end\":88420,\"start\":88414},{\"end\":88433,\"start\":88420},{\"end\":88647,\"start\":88638},{\"end\":88657,\"start\":88647},{\"end\":88670,\"start\":88657},{\"end\":88680,\"start\":88670},{\"end\":89025,\"start\":89019},{\"end\":89035,\"start\":89025},{\"end\":89113,\"start\":89107},{\"end\":89123,\"start\":89113},{\"end\":89135,\"start\":89123},{\"end\":89152,\"start\":89135},{\"end\":89229,\"start\":89221},{\"end\":89236,\"start\":89229},{\"end\":89247,\"start\":89236},{\"end\":89253,\"start\":89247},{\"end\":89264,\"start\":89253},{\"end\":89405,\"start\":89396},{\"end\":89415,\"start\":89405},{\"end\":89425,\"start\":89415},{\"end\":89527,\"start\":89517},{\"end\":89536,\"start\":89527},{\"end\":89547,\"start\":89536},{\"end\":89622,\"start\":89612},{\"end\":89639,\"start\":89622},{\"end\":89652,\"start\":89639},{\"end\":89660,\"start\":89652},{\"end\":89671,\"start\":89660},{\"end\":89782,\"start\":89771},{\"end\":89791,\"start\":89782},{\"end\":89916,\"start\":89909},{\"end\":89924,\"start\":89916},{\"end\":89931,\"start\":89924},{\"end\":89946,\"start\":89931},{\"end\":89962,\"start\":89946},{\"end\":90117,\"start\":90106},{\"end\":90128,\"start\":90117},{\"end\":90141,\"start\":90128},{\"end\":90150,\"start\":90141},{\"end\":90347,\"start\":90336},{\"end\":90356,\"start\":90347},{\"end\":90368,\"start\":90356},{\"end\":90380,\"start\":90368},{\"end\":90487,\"start\":90477},{\"end\":90500,\"start\":90487},{\"end\":90509,\"start\":90500},{\"end\":90601,\"start\":90593},{\"end\":90707,\"start\":90699},{\"end\":90723,\"start\":90707},{\"end\":90734,\"start\":90723}]", "bib_venue": "[{\"end\":82379,\"start\":82356},{\"end\":82543,\"start\":82520},{\"end\":82685,\"start\":82665},{\"end\":82850,\"start\":82788},{\"end\":83026,\"start\":82988},{\"end\":83223,\"start\":83195},{\"end\":83357,\"start\":83296},{\"end\":83429,\"start\":83417},{\"end\":83564,\"start\":83545},{\"end\":83702,\"start\":83671},{\"end\":83848,\"start\":83844},{\"end\":83987,\"start\":83949},{\"end\":84114,\"start\":84083},{\"end\":84276,\"start\":84223},{\"end\":84434,\"start\":84328},{\"end\":84702,\"start\":84666},{\"end\":84883,\"start\":84844},{\"end\":85120,\"start\":85064},{\"end\":85355,\"start\":85295},{\"end\":85503,\"start\":85423},{\"end\":85598,\"start\":85549},{\"end\":85841,\"start\":85781},{\"end\":86018,\"start\":85980},{\"end\":86091,\"start\":86029},{\"end\":86275,\"start\":86252},{\"end\":86498,\"start\":86479},{\"end\":86642,\"start\":86627},{\"end\":86809,\"start\":86759},{\"end\":86944,\"start\":86909},{\"end\":87102,\"start\":87065},{\"end\":87278,\"start\":87218},{\"end\":87369,\"start\":87286},{\"end\":87534,\"start\":87511},{\"end\":87697,\"start\":87660},{\"end\":87877,\"start\":87821},{\"end\":87938,\"start\":87888},{\"end\":88108,\"start\":88089},{\"end\":88282,\"start\":88245},{\"end\":88501,\"start\":88433},{\"end\":88582,\"start\":88511},{\"end\":88779,\"start\":88680},{\"end\":88900,\"start\":88790},{\"end\":89017,\"start\":88960},{\"end\":89105,\"start\":89044},{\"end\":89316,\"start\":89264},{\"end\":89507,\"start\":89425},{\"end\":89687,\"start\":89671},{\"end\":89828,\"start\":89791},{\"end\":89990,\"start\":89962},{\"end\":90222,\"start\":90150},{\"end\":90396,\"start\":90380},{\"end\":90546,\"start\":90509},{\"end\":90625,\"start\":90601},{\"end\":90783,\"start\":90734},{\"end\":83570,\"start\":83566}]"}}}, "year": 2023, "month": 12, "day": 17}