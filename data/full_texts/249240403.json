{"id": 249240403, "updated": "2023-10-05 13:39:48.6", "metadata": {"title": "Fairness Transferability Subject to Bounded Distribution Shift", "authors": "[{\"first\":\"Yatong\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Reilly\",\"last\":\"Raab\",\"middle\":[]},{\"first\":\"Jialu\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Yang\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Given an algorithmic predictor that is\"fair\"on some source distribution, will it still be fair on an unknown target distribution that differs from the source within some bound? In this paper, we study the transferability of statistical group fairness for machine learning predictors (i.e., classifiers or regressors) subject to bounded distribution shifts. Such shifts may be introduced by initial training data uncertainties, user adaptation to a deployed predictor, dynamic environments, or the use of pre-trained models in new settings. Herein, we develop a bound that characterizes such transferability, flagging potentially inappropriate deployments of machine learning for socially consequential tasks. We first develop a framework for bounding violations of statistical fairness subject to distribution shift, formulating a generic upper bound for transferred fairness violations as our primary result. We then develop bounds for specific worked examples, focusing on two commonly used fairness definitions (i.e., demographic parity and equalized odds) and two classes of distribution shift (i.e., covariate shift and label shift). Finally, we compare our theoretical bounds to deterministic models of distribution shift and against real-world data, finding that we are able to estimate fairness violation bounds in practice, even when simplifying assumptions are only approximately satisfied.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2206.00129", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/ChenRW022", "doi": "10.48550/arxiv.2206.00129"}}, "content": {"source": {"pdf_hash": "24ccf45cac04b9ebf59ca27823859395acaa91e7", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2206.00129v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b8a64df7ca94b2a4f8f29c805cb56dc34e6374fa", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/24ccf45cac04b9ebf59ca27823859395acaa91e7.txt", "contents": "\nFairness Transferability Subject to Bounded Distribution Shift\n\n\nYatong Chen \nUniversity of California\nSanta Cruz\n\nReilly Raab reilly@ucsc.edu \nUniversity of California\nSanta Cruz\n\nJialu Wang \nUniversity of California\nSanta Cruz\n\nYang Liu yangliu@ucsc.edu \nUniversity of California\nSanta Cruz\n\nFairness Transferability Subject to Bounded Distribution Shift\n\nGiven an algorithmic predictor that is \"fair\" on some source distribution, will it still be fair on an unknown target distribution that differs from the source within some bound? In this paper, we study the transferability of statistical group fairness for machine learning predictors (i.e., classifiers or regressors) subject to bounded distribution shift. Such shifts may be introduced by initial training data uncertainties, user adaptation to a deployed predictor, dynamic environments, or the use of pre-trained models in new settings. Herein, we develop a bound that characterizes such transferability, flagging potentially inappropriate deployments of machine learning for socially consequential tasks. We first develop a framework for bounding violations of statistical fairness subject to distribution shift, formulating a generic upper bound for transferred fairness violations as our primary result. We then develop bounds for specific worked examples, focusing on two commonly used fairness definitions (i.e., demographic parity and equalized odds) and two classes of distribution shift (i.e., covariate shift and label shift). Finally, we compare our theoretical bounds to deterministic models of distribution shift and against real-world data, finding that we are able to estimate fairness violation bounds in practice, even when simplifying assumptions are only approximately satisfied.In this paper, we provide a general framework for quantifying the robustness of statistical group fairness guarantees. We assume that the target distribution is adversarially drawn from a bounded domain, thus reducing the hard problem of modelling distribution shift dynamics to a more tractable, static problem. With this framework, we can detect potentially inappropriate policy applications, prior to deployment, when fairness violation bounds are not sufficiently small.This work bridges a gap between recent literature on domain adaptation, which has largely focused on the transferability of prediction accuracy (rather than fairness), and algorithmic fairness, which \u2020 These authors contributed equally to this work.\n\nIntroduction\n\nDistribution shift is a common, real-world phenomenon that affects machine learning deployments when the target distribution of examples (features and labels) ultimately encountered by a datadriven policy diverges from the source distribution it was trained for. For socially consequential decisions guided by machine learning, such shifts in the underlying distribution can invalidate fairness guarantees and cause harm by exacerbating social disparities. Unfortunately, distribution shift can be technically difficult or impossible to model at training time (e.g., when depending on complex social dynamics or unrealized world events). Nonetheless, we still wish to certify the robustness of fairness metrics for a policy on possible target distributions. has typically considered static distributions or prescribed models of distribution shift. Our work is the first to systematically bound quantifiable violations of statistical group fairness while remaining agnostic to (1) the mechanisms responsible for distribution shift, (2) how group-specific distribution shifts are quantified, and (3) the specific statistical definition of group fairness applied.  Figure 1: In Section 7, we evaluate our bounds against historical, temporal distribution shifts in demographics and income recorded by the US Census Bureau [13]. The above figure depicts changes to income-prediction accuracy and demographic parity violation when a classifier initially trained on US state-specific demographic data for 2014 is reused on 2018 data, thus exemplifying the negative potential effects of distribution shift.\n\nOur primary result is a bound on a policy's potential \"violation of statistical group fairness\"-defined in terms of the differences in policy outcomes between groups-when applied to a target distribution shifted relative to the source distribution within known constraints. Such settings naturally arise whenever training data represents a random sample of a target population with different statistics or a sample from dynamic environments, when a policy is reused on a new distribution without retraining, or whenever policy deployment itself induces a distribution shift. As an example of this last case, strategic individuals seeking loans might change their features or abstain from future application (thus shifting the distribution of examples) in response to policies trained on historical data [18, 38,43]. Beyond policy selection, exogenous pressure such as economic trends and noise may also drive distribution shift in this example.\n\nIn Figure 1, we show how a real-world distribution shift in demographic and income data for US states between 2014 and 2018 may increase fairness violations while decreasing accuracy for a hypothetical classifier trained on the 2014 distribution. In such settings, it is useful to quantify how fairness guarantees transfer across distributions shifted within some bound, thus allowing the deployment of unfair machine learning policies to be avoided.\n\n\nRelated Work\n\nOur work considers a setting similar to recent studies of domain adaptation, which have largely focused on characterizing the effects of distribution shift on prediction performance rather than fairness. Our work also builds on efforts in algorithmic fairness, especially dynamical treatments of distribution shift in response to deployed machine learning policies [23, 11,30]. We reference specific prior work in these domains in Appendix B, and here discuss existing work that focuses on how certain measures of fairness are affected when policies are subject to specific distribution shift.\n\nFairness subject to Distribution Shift: A number of recent studies have considered specific examples of fairness transferability subject to distribution shift [34,9,36,31,21]. In particular, Schumann et al. [34] examine equality of opportunity and equalized odds as definitions of group fairness subject to distribution shifts quantified by an H-divergence function; Coston et al. [9] consider demographic parity subject to a covariate shift assumption while group identification remains unavailable to the classifier; Singh et al. [36] focus on common group fairness definitions for binary classifiers subject to a class of distribution shift that generalizes covariate shift and label shift by preserving some conditional probability between variables; and Rezaei et al. [31] similarly consider common binary classification fairness definitions such as equalized odds subject to covariate shift. While we address similar settings to these works as special cases of our bound, we propose a unifying formulation for a broader class of statistical group fairness definitions and distribution shifts. In doing so, we recognize that particular settings recommend themselves to more natural measures of distribution shift, providing examples in Section 4.1, Section 4.2, and Section 5).\n\nAnother thread in existing literature is the development of robust models with the goal of guaranteeing fairness on a modelled target distribution (e.g., [1,32,26,3,21]) , for example, by assuming covariate shift and the availability of some unlabelled target data [9,36,31]. In particular, Singh et al. [36] focus on learning stable models that will preserve prediction accuracy and fairness, utilizing a causal graph to describe anticipated distribution shifts. Rezaei et al.\n\n[31] takes a robust optimization approach, and Coston et al. [9] develops prevalence-constrained and target-fair covariate shift method for getting the robust model. In contrast, our goal is to quantify fairness violations after an adversarial distribution shift for any given policy, including those not trained with robustness in mind.\n\n\nOur Contributions\n\nOur primary contribution is formulating a general, worst-case upper bound for a given policy's violation of statistical group fairness subject to group-dependent distribution shifts within presupposed bounds (i.e., Equation (9)). Bounding violations of fairness subject to distribution shift allows us to recognize and avoid potentially inappropriate deployments of machine learning when the potential disparities of a prospective policy eclipse a given threshold within bounded distribution shifts of the training distribution.\n\nWe first characterize the space of statistical group fairness definitions and possible distribution shifts by appeal to premetric functions (Definition 2.1). After formulating the worst-case upper bound, we explore common sets of simplifying assumptions for this bound as special cases, yielding tractable calculations for several familiar combinations of fairness definitions and subcases of distribution shift (Theorem 4.1, Theorem 5.2) with readily interpretable results. Finally, we compare our theoretical bounds to prescribed models of distribution shift in Section 6 and to real-world data in Section 7. The details for reproducing our experimental results can be found at https://github.com/UCSC-REAL/Fairness_Transferability.\n\n\nFormulation\n\nThe appendices include a table of notation (Appendix A) and all proofs (Appendix F).\n\n\nAlgorithmic Prediction\n\nWe consider two distributions, S (source) and T (target), each defined as a probability distribution for examples, where each example defines values for three random variables: X, a feature (e.g., x) with arbitrary domain X ; Y , a label (e.g., y) with arbitrary domain Y; and G, a group (e.g., g or h) with finite, countable domain G. The predictor's policy \u03c0, intended for S but used on T , defines a fourth variable for each example: viz.,\u0176 , a predicted label (e.g.,\u0177) with domain\u0176 = Y.\n\nUsing P(\u00b7) to denote the space of probability distributions over some domain, we denote the space of distributions over examples as D := P(X \u00d7 Y \u00d7 G), such that S, T \u2208 D. It will also be useful for us to notate the space of distributions over example outcomes associated with a given policy as O := P(X \u00d7 Y \u00d7\u0176) and the space of distributions over of group-specific examples as G := P(X \u00d7 Y).\n\nWithout loss of generality, we allow the prediction policy \u03c0 to be stochastic, such that, for any combination (x, g), the predictor effectively samples\u0176 from a corresponding probability distribution \u03c0(x, g). Stochastic classifiers arise in various constrained optimization problems and proven useful for making problems with custom losses or fairness constraints tractable [10,17,29,40].\n\nWe denote the space of nondeterministic policies as \u03a0 := (X \u00d7 G \u2192 P(\u0176)) (e.g., \u03c0 \u2208 \u03a0) and utilize the natural transformations that relate the spaces of distributions D, policies \u03a0, and outcomes O:\nPr \u03c0,T (\u0176 =\u0177, X=x, G=g) = Pr Y \u223c\u03c0(x,g) (\u0176 =\u0177) \u00b7 Pr X,G\u223cT (X=x, G=g)(1)\nWe abuse the Pr notation for both probability density and probability mass functions as appropriate.\n\n\nStatistical Group-Fairness\n\nWe next define a broad class of disparity functions \u2206 : \u03a0 \u00d7 D \u2192 R representing how \"unfair\" a given policy is for a given distribution (e.g., writing \u2206 (\u03c0, T )), noting that this notion of fairness is limited to capturing statistical discrepancies of outcomes between groups. Definition 2.1. We define a premetric 3 \u03a8 on the space of distributions p with respect to q by the properties \u03a8(p q) \u2265 0 and \u03a8(p p) = 0 for all p, q, and refer to the value of \u03a8 as a \"shift\". Definition 2.2. We define a statistical group disparity \u2206 for policy \u03c0 and distribution T in terms of the symmetrized shifts between group-specific outcome distributions. We measure shifts between outcome distributions with a given premetric \u03a8 :\nO 2 \u2192 R. \u2206 (\u03c0, T ) := g,h\u2208G \u03a8 Pr \u03c0,T (X, Y,\u0176 | G=g) Pr \u03c0,T (X, Y,\u0176 | G=h)(2)\nIn Definition 2.2, \u03a8 quantifies the specific statistical differences in outcomes between groups that are \"unfair\", where a value of 0 implies perfect fairness. In this work, we assume that \u03a8 is the same for all g, h and that \u2206 is insensitive to relative group size Pr(G). Examples Familiar applications of Definition 2.2 include demographic parity (DP) and equalized odds (EO). A policy satisfying DP, in expectation, assigns a given binary classification y \u2208 {0, 1} to the same fraction of examples in each group. We may measure the violation of DP as\n\u2206 DP (\u03c0, T ) := g,h\u2208G Pr \u03c0,T (\u0176 =1 | G=g) \u2212 Pr \u03c0,T (\u0176 =1 | G=h)(3)\nThe associated premetric\n\u03a8 DP for p, q \u2208 O is \u03a8 DP (p q) = Pr p (\u0176 =1) \u2212 Pr q (\u0176 =1) .\nTo satisfy EO, for binary Y = {0, 1}, \u03c0 must maintain group-invariant true positive and false positive classification rates. We may measure the violation of EO as\n\u2206 EO (\u03c0, T ) := g,h\u2208G y\u2208Y Pr \u03c0,T (\u0176 =1 | G=g, Y =y) \u2212 Pr \u03c0,T (\u0176 =1 | G=h, Y =y)(4)\nThe associated premetric is\n\u03a8 EO (p q) = y\u2208Y Pr p (\u0176 =1 | Y =y) \u2212 Pr q (\u0176 =1 | Y =y) .\nNote that the restriction of EO to the (Y = 1) case is known as Equal Opportunity (EOp).\n\nWe remark that Definition 2.2 provides a unifying representation for a wide array of statistical group \"unfairness\" definitions and may be used with inequality constraints. That is, we may recover many working definitions of fairness that effectively specify a maximum value of disparity: Definition 2.3. A policy \u03c0 is -fair with respect to \u2206 on distribution T iff \u2206 (\u03c0, T ) \u2264 .\n\n\nVector-Bounded Distribution Shift\n\nSuppose, after developing policy \u03c0 for distribution S, we realize some new distribution T on which the policy is actually operating. This realization may be the consequence of sampling errors during the learning process, strategic feedback to our policy, random processes, or the reuse of our policy on a new distribution for which retraining is impractical. Our goal is to bound \u2206 (\u03c0, T ) given knowledge of \u2206 (\u03c0, S) and some notion of how much T possibly differs from S. Definition 2.4. K(p q) is a divergence if and only if for all p and q, K(p q) \u2265 0 and K(p q) = 0 \u21d0\u21d2 q = p. Definition 2.5. Define the group-vectorized shift D, as S mutates into T , as\nD(T S) := g e g D g Pr T (X, Y | G=g) Pr S (X, Y | G=g)(5)\nwhere e g represents a unit vector indexed by g, and each D g : G 2 \u2192 R is a divergence (Definition 2.4). Note that each D g also defines a premetric (but not necessarily a divergence) on D. Assumption 2.6. Let there exist some vector B 0 bounding D(T S) B, where and denote element-wise inequalities.\n\nIn Assumption 2.6, B limits the possible distribution shift as S mutates into T , without requiring us to specify a model for how distributions evolve. When modelling distribution shift requires complex dynamics (e.g., when agents learn and respond to classifier policy), we reduce a potentially difficult dynamical problem to a more tractable, adversarial problem to achieve a bound. Lemma 2.7. For all \u03c0, \u2206 , and D, when B = 0, \u2206 (\u03c0, S) = \u2206 (\u03c0, T ).\n\nLemma 2.7 indicates that, for a fixed policy \u03c0, a change in disparity requires a measurable shift in distributions from S to T , confirming intuition. Restricted Distribution Shift Common assumptions that restrict the set of distribution shifts include covariate shift and label shift. For covariate shift, the distribution of labels conditioned on features is preserved across distributions for all groups, while for label shift, the distributions of features conditioned on labels is preserved across distributions for all groups.\nCovariate shift implies Pr T (Y | X, G) = Pr S (Y | X, G)(6)\nLabel shift implies Pr\nT (X | Y, G) = Pr S (X | Y, G)(7)\nIn Section 4, we explore a deterministic model of a population's response to classification as an example of covariate shift. We do the same in Section 5 for label shift.\n\n\nGeneral Bounds\n\nWe first define a primary bound in Definition 3.1 before considering simplifying special cases.\n\nGiven an element-wise bound B on the vector-valued shift D(T S) (Assumption 2.6) we may bound the disparity \u2206 of policy \u03c0 on any realizable target distribution T by its supremum value. \nD(T S) B =\u21d2 \u2206 (\u03c0, T ) \u2264 v(\u2206 , D, \u03c0, S, B)(8)\nIn general, our strategy is to exploit the mathematical structure of the setting encoded by \u2206 (i.e., \u03a8) and D to obtain an upper bound for v defined in Equation (8). We first explore general cases of simplifying assumptions before presenting worked special examples for frequently encountered settings. Finally, we compare the resulting theoretical bounds to numerical results and simulations.\n\n\nLipshitz Conditions\n\nThe value of v defines a scalar field in B and therefore a conservative vector field F = \u2207 B v. For any curve in D from S to T , bounds of the form F L for some constant L along the curve imply a Lipshitz bound on \u2206 . We visualize a bound in Figure 2 for all possible curves in the region D(T S) B. \n\nSuccinctly, if we are guaranteed that disparity can never increase faster than a certain rate in some measure of distribution shift, then, given a maximum distribution shift, this rate bounds the maximum possible disparity. The utility of Theorem 3.2 arises when a Lipshitz condition L is known, but direct computation of v is difficult. We provide an example of a Lipshitz bound in Section 5. Theorem 3.4. Suppose, in the region D(T S) B, that w is subadditive in its last argument. That is, w(..., a) + w(..., c) \u2265 w(..., a + c) for a, c 0 and a + c B. If w is also locally differentiable, then a first-order approximation of w(..., b) evaluated at 0, i.e.,\n\n\nSubadditivity Conditions\nL = \u2207 b w(..., b) b=0 = \u2207 b v(..., b) b=0(11)\nprovides an upper bound for v(...,\nB), i.e., v(\u2206 , D, \u03c0, S, B) \u2264 \u2206 (\u03c0, S) + L \u00b7 B(12)\nTheorem 3.4 notes that \"diminishing returns\" in the change of \u2206 as the difference of T with respect to S is increased implies a bound on \u2206 in terms of its local sensitivity to D at S (i.e., using a firstorder Taylor approximation). Note that, if w is concave in the bounded region, it is also subadditive in the bounded region, but the converse is not true, nor does the converse imply Lipshitzness.\n\n\nGeometric Structure\n\nIt may happen that \u03a8 : O 2 \u2192 R and each D g : G 2 \u2192 R share structure that permits a geometric interpretation of distribution shift. While the utility of this observation depends on the specific properties of \u03a8 and D, we demonstrate a worked example building on Section 4.2 in Appendix D, in which we allow ourselves to select a suitable D for ease of interpretation. We proceed to consider worked examples that adopt common assumptions limiting the form of distribution shift and apply common definitions of statistical group fairness.\n\n\nCovariate Shift\n\nWe now present our fairness transferability results subject to covariate shift for both demongraphic parity (Section 4.1) and equalized opportunity (Section 4.2) as fairness criteria.\n\n\nDemographic Parity\n\nThe simplest way to work with Equation (8) is to bound the supremum v. We first consider demographic parity (Equation (3)) for Y = {0, 1} and G = {g, h}, subject to covariate shift (Equation (6)). We find that the form of \u2206 DP subject to covariate shift recommends itself to a natural choice of vector divergence, D. First, define a re-weighting coefficient \u03c9 g (T , S, x) := Pr T (X=x|G=g) Pr S (X=x|G=g) . Theorem 4.1. For demographic parity between two groups under covariate shift (denoting, for each g, \u03b2 g := Pr \u03c0,S (\u0176 =1 | G=g)),\n\u2206 DP (\u03c0, T ) \u2264 \u2206 DP (\u03c0, S) + g \u03b2 g (1 \u2212 \u03b2 g ) \u00b7 Var S [\u03c9 g (T , S, x)] 1 /2(13)\nWe notice that Var S [\u03c9 g (T , S, x)] recommends itself as a suitable divergence D g from S to T . Using basis vectors e g , for this example, we could define D(\nT S) = g e g Var S [\u03c9 g (T , S, x)]. When Var S [\u03c9 g (T , S, x)] \u2264 B g , it follows \u2206 DP (\u03c0, T ) \u2264 \u2206 DP (\u03c0, S) + g \u03b2 g (1 \u2212 \u03b2 g ) \u00b7 B g 1 /2 .\nComparing the inequality in Theorem 4.1 and the consequent of Equation (10), we can interpret Pr \u03c0,S (\u0176 =1) in Theorem 4.1 as an upper bound for the average value of \u2207 b v(\u2206 DP , D, \u03c0, T , b) along any curve from S to T . Interpreting this result, the closer Pr(\u0176 =1) is to 0.5 for any group, the more potentially sensitive the fairness of the policy is to distribution shifts for that group. We can further generalize the results to multi-class and multi-group setting: \n\u2206 DP (\u03c0, T ) := y\u2208Y g,h\u2208G Pr \u03c0,T (\u0176 =y | G=g) \u2212 Pr \u03c0,T (\u0176 =y | G=h) (14) \u2206 DP (\u03c0, T ) \u2264 \u2206 DP (\u03c0, S) + y g \u03b2 g,y (1 \u2212 \u03b2 g,y ) \u00b7 B g 1 /2(15)\nWe remark that in general, binary classification bounds may frequently be generalized to multi-class bounds by redefining fairness violations as a sum of binary-class fairness violations (i.e., same-class vs. different-class labels) and summing the bounds on each.\n\n\nEqual Opportunity\n\nConsider an example using the (Y =1)-conditioned case of Equalized Odds-termed Equal Opportunity (EOp). Denoting, for each group g, the true positive rate \u03b2 + g := Pr \u03c0,T (\u0176 =1 | Y =1, G=g) as an implicit function of \u03c0 and T , we define disparity for EOp as \u2206 EOp (\u03c0, T ) := g,h\u2208G |\u03b2 + g \u2212 \u03b2 + h |. We may bound the realized value of \u2206 EOp (\u03c0, T ) by bounding \u03b2 + g for each group: Theorem 4.3. Subject to covariate shift and any given D, B, assume extremal values for \u03b2 + g , i.e.,\n\u2200g, D g (T S) < B g =\u21d2 l g \u2264 \u03b2 + g (\u03c0, T ) \u2264 u g (16) it follows that v(\u2206 EOp , D, \u03c0, S, B) \u2264 max xg\u2208{lg,ug} x h \u2208{l h ,u h } g,h x g \u2212 x h (17) Corollary 4.4. The disparity measurement \u2206 EOp cannot exceed |G| 2 4 .\nIn Appendix D, we bound the extremal values of \u03b2 + g by geometrically interpreting this quantity as an inner product on an appropriate vector space, utilizing the freedom to select an appropriate D.\n\n\nLabel Shift\n\nUnder label shift (Pr S (X|Y ) = Pr T (X|Y )), violations of EO and EOp are invariant, because the independence of\u0176 and Y given X implies Pr \u03c0,T (\u0176 |Y ) = Pr \u03c0,S (\u0176 |Y ). We therefore focus on the violation of demographic parity (DP) (Equation (3)) subject to the label shift condition, treating a binary classification task over two groups for simplicity.\n\nIn this setting, we choose to measure group-specific distribution shifts from S to T by the change in proportion of ground-truth positive labels, which we refer to as the group qualification rate\nQ g (T ) := Pr T (Y = 1 | G = g): D g (T S) := Q g (S) \u2212 Q g (T ) \u2264 B g (18) Theorem 5.1. A Lipshitz condition bounds \u2207 b v(\u2206 DP , D, \u03c0, S, b) when D g (T S) := Q g (S) \u2212 Q g (T ) \u2264 B g(19)\nSpecifically,\n\u2202 \u2202b g v(\u2206 DP , D, \u03c0, S, b) \u2264 (|G| \u2212 1) \u03b2 + g \u2212 \u03b2 \u2212 g (20)\nfor true positive rates \u03b2 + g and false positive rates \u03b2 \u2212 g :\n\u03b2 + g := Pr \u03c0 (\u0176 =1 | Y =1, G=g); \u03b2 \u2212 g := Pr \u03c0 (\u0176 =1 | Y =0, G=g)(21)\nBecause \u03b2 + g and \u03b2 \u2212 g are invariant under label shift given a constant policy \u03c0, we elide their explicit dependence on the underlying distribution. Theorem 5.2. For DP under the bounded label-shift assumption \u2200g,\n|Q g (S) \u2212 Q g (T )| \u2264 B g , \u2206 DP (\u03c0, T ) \u2264 \u2206 DP (\u03c0, S) + (|G| \u2212 1) g B g \u03b2 + g \u2212 \u03b2 \u2212 g(22)\nIntuitively, the change in \u2206 DP subject to label shift depends on |\u03b2 + g \u2212 \u03b2 \u2212 g |, the marginal change in acceptance rates as agents change their qualifications Y . We measure the distribution shift as agents change their qualifications by |Q g (S) \u2212 Q g (T )|. When \u03b2 + g is close to \u03b2 \u2212 g , the policy looks like a random classifier, and a label shift has limited effect on statistical group disparity. When |\u03b2 + g \u2212 \u03b2 \u2212 g | is large, indicating high classifier accuracy, the effect on supremal disparity is larger. Our bound thus exposes a direct trade-off between accuracy and fairness transferability guarantees.\n\n\nComparisons to Synthetic Distribution Shifts (Demographic Parity)\n\nTo further interpret our results, in this section, we consider specific and popular agent models to characterize distribution shift and instantiate our bounds for particular forms of D, B, and \u2206 .\n\n\nCovariate Shift via Strategic Response\n\nLet us consider a specific example of covariate shift (Equation (6)) caused by a deterministic, groupindependent model of strategic response in which agents react to a binary classification policy \u03c0 characterized by group-specific feature thresholds:\nY \u223c \u03c0(x, g) = 1 with probability 1 if x \u2265 \u03c4 g 0 with probability 1 otherwise(23)\nFor simplicity, we assume the feature domain X = [0, 1]. In response to threshold \u03c4 g , agents in each group g may modify their feature x to x by incurring a cost c g (x, x ) \u2265 0. Similar to [18], we define the utility u g for agents in group g to be\nu g (x, x ) := \u03b2 g (x ) \u2212 \u03b2 g (x) \u2212 c g (x, x ); \u03b2 g (x) := Pr \u0176 =1 | X=x, G=g , \u2200g.(24)\nContrary to the standard strategic classification setting, we do not assume that feature updates represent false reports, but that such updates may correspond to actual changes underlying the true qualification Y of each agent. This assumption has been made in a recent line of research in incentivizing improvement from human agents subject to such classification [5].\n\nNext, we assume all agents are rational utility maximizers (Equation (24)). For a given threshold \u03c4 g and manipulation budget m g , the best response of an agent with original feature x is\nx = argmax z u g (x, z), such that c g (x, z) \u2264 m g(25)\nTo make the problem tractable, we make additional assumptions about the agents' best responses.\nAssumption 6.1. An agent's original feature x is sampled as X \u223c U [0,1] 4 . Assumption 6.2. The cost function c g (x, x ) is monotone in |x \u2212 x | as c g (x, x ) = |x \u2212 x|.\nUnder Assumption 6.2, only those agents with features x \u2208 [\u03c4 g \u2212 m g , \u03c4 g ) will attempt to change their feature. We also assume that feature updates are non-deterministic, such that agents with features closer to the decision boundary \u03c4 g have a greater chance of updating their feature and each updated feature x is sampled from a uniform distribution depending on \u03c4 g , m g , and x: Assumption 6.3. For agents who attempt to update their features, the probability of a successful feature update is Pr(X = X ) = 1 \u2212 |x\u2212\u03c4g| mg . Assumption 6.4. An agent's updated feature x , given original feature x, manipulation budget m g , and classification boundary \u03c4 g , is sampled as\nX \u223c U [\u03c4g,\u03c4g+mg\u2212x] .\nWith the above setting, we can specify the reweighting coefficient \u03c9 g (x) for our setting (Equation (102) in Appendix F.1 and get the following bound for the strategic response setting 5 : Proposition 6.5. For our assumed setting of strategic response involving DP for two groups {g, h}, Theorem 4.1 implies\n\u2206 DP (\u03c0, T ) \u2264 \u2206 DP (\u03c0, S) + \u03c4 g (1 \u2212 \u03c4 g ) 2 3 m g + \u03c4 h (1 \u2212 \u03c4 h ) 2 3 m h(26)\nThe above result shows that two factors lead to a smaller difference between the source and target fairness violations: a less stochastic classifier (when the threshold \u03c4 g is far away from 0.5) and a smaller manipulation budget m g (diminishing agents' ability to adapt their feature). In this case,\nB g = 2 3 \u03c4 g (1 \u2212 \u03c4 g ).\nThese factors lead to less potential manipulation and result in a tighter upper bound for the fairness violation on T .\n\n\nLabel Shift via Replicator Dynamics\n\nWe now evaluate our theoretical bound for demographic parity subject to label shift (Theorem 5.2) on the replicator dynamics model of Raab and Liu [30]. Briefly, replicator dynamics assumes that the proportion of agents in a population choosing one strategy over another grows in proportion to the ratio of average utilities realized by the two strategies. The cited model additionally assumes X = R, Y = {0, 1}, and a monotonicity condition for S given by d dx\nPr S (X=x|Y =1) Pr S (X=x|Y =0) > 0.\nLabel shift under the discrete-time (t) replicator dynamics may be expressed in terms of group qualification rates Q g := Pr t (Y =1 | G=g) and agent utilities (i.e., group-and feature-independent values U y,\u0177 ) such that, in each group, the popularity and average utility associated with a label determines its frequency at the next time step t+1.  Denote the fractions of group-conditioned, feature-independent outcomes with the expression \u03c1 y,\u0177 g := Pr t (\u0176 =\u0177, Y =y | G=g) and abbreviate the fraction-weighted utility as u y,\u0177 g (t) := U y,\u0177 \u00b7 \u03c1 y,\u0177 g . We may then represent the replicator dynamics as\nQ g [t + 1] = u 1,1 g (t) + u 1,0 g (t) u 1,1 g (t) + u 1,0 g (t) + u 0,0 g (t) + u 0,1 g (t)(27)\nTo apply Theorem 5.2, we also observe that |\u03b2\n+ g \u2212 \u03b2 \u2212 g | = |\u03c1 1,1 g \u2212\u03c1 0,1 g | \u03c1 1,1 g +\u03c1 0,1 g ,\nwhere \u03b2 + g and \u03b2 \u2212 g represent the true positive rate and false positive rate for group g, respectively, and we use the change in qualification rate as our measurement of label shift, i.e., B g = |Q g [t + 1] \u2212 Q g [t]|. When demographic parity is perfectly satisfied, we note that the acceptance rate (\u03c1 1,1 g + \u03c1 0,1 g ) is group-independent. Theorem 6.6. For DP subject to label replicator dynamics,\n\u2206 DP (\u03c0, T ) \u2264 \u2206 DP (\u03c0, S) + g Q g [t + 1] \u2212 Q g [t] |\u03c1 1,1 g \u2212 \u03c1 0,1 g | \u03c1 1,1 g + \u03c1 0,1 g(28)\nIn Figure 3(b), we graphically represent all possible states of an initially fair system (thus determining \u03b2 and \u03c1 as a result of the monotonicity condition) by the tuple of qualification rates for each group. With the dynamics prescribed by Equation (27), we depict the rate of change of disparity given a fixed, locally DP-fair policy, and compare this to the theoretical bound when\nB g = |Q g [t + 1] \u2212 Q g [t]|.\nInterpreting our results, we note that the bound lacks information about the relative directions of the change in acceptance rates for each group, and thus over-approximates possible fairness violations when group acceptance rates shift the same direction. When group acceptance rates move in opposing directions, however, the bound gives excellent agreement with the modelled replicator dynamics.\n\n\nComparisons to Real-World Distribution Shifts\n\nWe now compare our special-case theoretical bounds (i.e., label/covariate shift) to real-world distribution shifts and hypothetical classifiers. We use American Community Survey (ACS) data provided by the US Census Bureau [16]. We adopt the sampling and pre-processing approaches following the Folktables package provided by Ding et al. [13] 6 to obtain 1,599,229 data points. The data is partitioned by (1) all fifty US states and (2) years from 2014 to 2018. We use 10 features covering the demographic information used in the UCI Adult dataset [4], including age, occupation, education, etc., as X for our model, select sex as binary protected group, i.e., G \u2208 {g = female, h = male}.\n\nWe set the label Y to whether an individual's annual income is greater than $50K.\n\nTo apply our label-shift or covariate-shift bounds, we first need to verify whether the two datasets satisfy either of these assumptions. We adopted a conditional independence test [22], which takes data from source and target domains as input and returns a divergence score for each covariate and label variable, reflecting to what extent the variable is shifted between distributions. We find that the likelihood that the covariates shift across US states is approximately two orders of magnitude higher than for labels. More specifically, there are 4 covariates, including class of worker (probabilistic divergence score of 2.67e-2), hours worker per week (3.56e-2), sex (3.56e-2) and race (2.55e-1), that are more likely to be shifted than the label variable (1.29e-4). For temporal shifts within states, we find that the label variable is more likely to be shifted (0.1) than all the other covariates (which are below 0.01), approximately two orders of magnitude in favor of label shift over covariate shift. We therefore compare the disparities of hypothetical policies on these distributions to bounds generated from the corresponding, approximately satisfied assumptions.\n\nOn this data, we train a set of group-dependent, linear threshold classifiers Pr \u03c0(x,g) (\u0176 =1) = 1[\u03c3(w \u00b7 x) > \u03c4 g ], for a range of thresholds \u03c4 g and \u03c4 h for each source distribution. Here, \u03c3(\u00b7) is the logistic function and w denotes a weight vector. We then consider two types of real-world distribution shift: (1) geographic, in which a model trained for one state is evaluated on other US state in the same year, and (2) temporal, in which a model trained for 2014 is evaluated on the same state in 2018.  We graphically compare the theoretical bounds of Theorem 4.1 and Theorem 5.2 for the increased violation of DP subject to covariate and label shift, respectively, to the simulated violations for our model and data in Figure 4. We provide additional examples and an evaluation of bounds for EO subject to covariate shift (noting that label shift preserves EO in theory) in Appendix E.2. Despite the fact that geographic or temporal distribution shifts only approximately satisfy the assumptions of covariate or label shift, these comparisons demonstrate that our theoretical bounds are not vacuous, approximately bounding the change of fairness violation across real-world domain shifts. For geographic shifts, the covariate shift EO bounds (Appendix E.2) correctly overestimate disparity and tighten near accurate policies, while our DP bounds are useful only for a subset of policy thresholds (Figure 4(b)). add specific pointer. e.g, 4.a, that one is 4.b For temporal shift, the label shift bound for DP correctly overestimates the real change of DP violations but still remains at the same order of magnitude (Figure 4(c) and 4(d)).\n\n\nConclusion and Discussion\n\nIn this paper, we have developed a unifying framework for bounding the violation of statistical group fairness guarantees when the underlying distribution shifts within presupposed bounds. We hope that this work can generate meaningful discussion regarding the viability of fairness guarantees subject to distribution shift, the bounds of adversarial attacks against algorithmic fairness, and evaluations of robustness with respect to algorithmic fairness. We believe that, just as published empirical measurements are of limited use without reported uncertainties, fairness guarantees must be accompanied by bounds on their robustness to distribution shift.\n\nFuture work remains to apply our framework for to problem of fairness transferability in settings with more complicated distribution shift dynamics. For example, compound distribution shifts [33], which compose covariate shifts and label shifts, cannot be treated by composing the theoretical bounds developed herein without additional information regarding intermediate distributions. Another potential future direction is to develop reasonable bounds on anticipated distribution shift from models of human behavior and exogenous pressures. \n\n\nA Notation\n\nSymbol Usage X A random variable representing an example's features. X\n\nThe domain of features X. Y\n\nA random variable representing an example's ground truth label. Y\n\nThe domain of labels Y . Y A random variable representing the predicted label for an example. Y\n\nThe domain of predicted labels\u0176 (distinguished semantically from Y). G A random variable representing an example's group membership. G\n\nThe domain for group membership G. \u03c0 A learned (non-deterministic) policy for predicting\u0176 from X and G.\n\n\nPr\n\nA sample probability (density) according to a referenced distribution. P\n\nThe space of probability distributions over a given domain. The source distribution in D.\n\n\nT\n\nThe target distribution in D to which \u03c0 is now applied.   [2,27], or in conjunction with the dynamic response of a population to classification [25]. We are interested in a similar setting and concern, but address the transferability of fairness guarantees, rather than accuracy. In considering covariate shift and label shift as special cases in this paper, our work may be paired with studies that address the transferability of prediction accuracy under such assumptions [35,37,42].\n\nAlgorithmic Fairness: Many formulations of fairness have been proposed for the analysis of machine learning policies. When it is appropriate to ignore the specific social and dynamical context of a deployed policy, the statistical regularity of policy outcomes may be considered across individual examples [14] and across groups [41,15,8,19,6]. In our paper, we focus on such statistical definitions of fairness between groups, and develop bounds for demographic parity [6] and equalized odds [19] as specific examples.\n\nDynamic Modeling: When the dynamical context of a deployed policy must be accounted for, such as when the policy influences control over the future trajectories of a distribution of features and labels, we benefit from modelling how populations respond to classification. Among this line of work, [23] initiate the discussion of the long-term effect of imposing static fairness constraints on a dynamic social system, highlighting the importance of measurement and temporal modeling in the evaluation of fairness criteria. However, developing such models remains a challenging problem [11,36,31,12,43,39,24,7,20,28,30]. In particular, [11] discuss causal directed acyclic graphs (DAGs) as a unifying framework on fairness in dynamical systems. In this work, rather than relying precise models of distribution shift to quantify the transferability of fairness guarantees in dynamical contexts, we assume a bound on the difference between source and target distributions. We thus develop bounds on realized statistical group disparity while remaining agnostic to the specific dynamics of the system.\n\n\nC Additional Figures\n\nx \u03c9 g (x) 0 \u03c4 g \u2212 B g \u03c4 g \u03c4 g + B g 1 1 2 Figure 5: Distribution of the reweighting coefficient wg(x) for the setting of Covariate shift via Strategic Response.\n\n\nD A Geometric Interpretation\n\nIn this extension of Section 4.2, we fulfill the promise of Section 3.3 and consider a case in which shared structure of between \u03a8 : O 2 \u2192 R and each D g : G 2 \u2192 R permits a geometric interpretation of distribution shift for Equal Opportunity EOp, building on Theorem 4.3. We continue to defer rigorous proof to Appendix F.\n\nWe first recall the definition of the true positive rate of policy \u03c0, for each group, on distribution T .\n\u03b2 + g := Pr \u03c0,T (\u0176 =1 | Y =1, G=g)(29)\nThe true positive rate may be expressed as a ratio of inner products defined over the space of square-integrable L 2 functions on X . 7\n\u03b2 + g [T ] = Pr T (\u0176 =1, Y =1 | G=g) Pr T (Y =1 | G=g) = r g [T ], t g g r g [T ], 1 g (30) a, b g := X a(x)b(x)s g (x) dx(31)\nwhere we use the shorthands\nr g [T ](x) := Pr T (X=x | G=g)(32)s g (x) := Pr S (Y =1 | X=x, G=g)(33)1(x) := 1 (34) t g (x) := Pr \u03c0 (\u0176 =1 | Y =1, X = x, G = g)(35)\nand assume that s g (x) > 0 for all x and g.\n\nWe observe that the only degree of freedom in \u03b2 + g as T varies subject to covariate shift is r g : by the covariate assumption, s g is fixed; t meanwhile remains independent of T for fixed policy \u03c0, since \u03c0 is independent of Y conditioned on X and G.\n\nSelection of D We now select each D g to be the standard metric for the inner product defined by Equation (31), where, for each group, distributions in G are mapped to the corresponding vector r g :\nD g Pr T (X, Y | G=g) Pr S (X, Y | G=g) := r g [S], r g [T ] g + r g [T ], r g [T ] g \u2212 2 r g [S], r g [T ] g(36)\nIn this geometric picture, D(T S) B implies that all possible values for r g [T ] lie within a ball of radius B g centered at r g [S]. By the normalization condition of a probablity (density) function, denoting s \u22121 g (x) := (s g (x)) \u22121 , the vector r g [T ] must also lie on the hyperplane\nX r g [T ] dx = r g [T ], s \u22121 g g = 1(37)\nRecalling Equation (30), the group-specific true positive rate \u03b2 + g [T ] for policy \u03c0 is given by a ratio of the projected distances of r g along the t g and 1 vectors. Let us therefore denote the projection of r g [T ] onto the (1, t g )-plane as r \u22a5 g [T ]. We may then consider the possible values of r \u22a5 g [T ] as projections from the intersection of the r g [S]-centered hypersphere of radius B g and the hyperplane of normalized distributions (Equation (37)). Using \u2220(\u00b7, \u00b7) to denote the angle between vectors and denoting \u03c6 g := \u2220(r g , t g ), \u03b8 g := \u2220(r g , 1), \u03c6 g := \u2220(r \u22a5 g , t g ), and \u03b8 g := \u2220(r \u22a5 g , 1), we appeal to the geometric relationship a, b = cos (\u2220(a, b)) a b to write\n\u03b2 + g 1 t g = cos \u03c6 g cos \u03b8 g = cos \u03c6 g cos \u03b8 g(38)\nFrom these observations, we need only bound the ratio between cos(\u03c6 g ) and cos(\u03b8 g ) to bound \u03b2 + g . Relating these angles in the (1, t g )-plane by \u03c6 g = \u03be g \u2212 \u03b8 g where \u03be g := \u2220(t g , 1), we arrive at the following theorem:\n\nTheorem D.1. The true positive rate \u03b2 + g is bounded over the domain of covariate shift D cov [B], which we define by the bound D(T S) B, and the invariance of Pr(Y =1 | X=x, G=g) for all x, g, as\ncos \u03c6 u g cos \u03be g \u2212 \u03c6 u g \u2264 1 t g \u03b2 + g (\u03c0, T ) \u2264 cos \u03c6 l g cos \u03be g \u2212 \u03c6 l g(39)\nwith upper (\u03c6 u g ) and lower (\u03c6 l g ) bounds for \u03c6 g represented as\n\u03c6 l g := min T \u2208Dcov[B] \u03c6 g ; \u03c6 u g := max T \u2208Dcov[B] \u03c6 g(40)\nWe obtain a final bound on \u2206 EOp by substituting Equation (39) into Equation (17). We visualize the geometric bound on \u03b2 + g (Theorem D.1) in Figure 6. In Appendix E.1, we apply this bound to real-world credit score data assuming the model of strategic manipulation given in Section 6.1. Although the result is not an easily interpretted formula, it provides a demonstration of geometric reasoning applied to statistical fairness guarantees.\n\nFinally, we note that, in addition to the constraints considered above, each vector r g is subject to the positivity condition, \u2200x \u2208 X , r g (x) \u2265 0. The bound developed in this section, however, does not benefit from this additional constraint; we leave this to potential future work.\n1 tg(\u03c0) s \u22121 g s \u22121 g 2 rg(S) r \u22a5 g (S) \u03b8 \u03c6 Bg (a) 1 tg(\u03c0) s \u22121 g s \u22121 g 2 rg(S) r \u22a5 g (S) \u03b8 \u03c6 Bg (b) 1 tg(\u03c0) s \u22121 g s \u22121 g 2 r \u22a5 g (S)\nBg (c) Bound -Modelled Shift Figure 9: A policy satisfying DP is subject to distribution shift prescribed by replicator dynamics (Section 6.2). Realized disparity increases (blue) are compared to the theoretical bound (Theorem 5.2, gradated), which is tight when group have dissimilar qualification rates.\n\n\nE.2 Comparisons to Real-World Data\n\nWe provide additional graphics comparing bounds on demographic parity or equal opportunity to real-world distribution shifts. Figure 10 compares the covariate shift bound of Theorem 4.1 to the violation of demographic parity for hypothetical policies trained on one US state and deployed in another. Figure \n\n\nF Omitted Proofs\n\nProof of Lemma 2.7:\n\nStatement: For all \u03c0, \u2206 , and D, when B = 0, \u2206 (\u03c0, S) = \u2206 (\u03c0, T ).\n\nProof. By the definitions of group-vectorized shift (Definition 2.5) and divergence (Definition 2.4) together with the bounded distribution shift assumption (Assumption 2.6), we note\nB = 0 =\u21d2 D(T S) = 0(41)\nand\nD g (T S) = 0 =\u21d2 Pr S (X, Y | G=g) = Pr T (X, Y | G=g)(42)\nCombining these implications and invoking the independence of\u0176 \u223c \u03c0 and Y conditioned on X and G (Equation (1)), it follows that\nB = 0 =\u21d2 \u2200g, Pr \u03c0,S (X, Y,\u0176 | G=g) = Pr \u03c0,T (X, Y,\u0176 | G=g)(43)\nConsulting the definition of disparity (Definition 2.2), it follows that \u2206 (\u03c0, S) and \u2206 (\u03c0, T ) are equal when B = 0.\n\n\nProof of Theorem 3.2:\n\nStatement: If there exists an L such that \u2207 b v(\u2206 , D, \u03c0, S, b) L, everywhere along some curve from 0 to B, then\n\u2206 (\u03c0, T ) \u2264 \u2206 (\u03c0, S) + L \u00b7 B(44)\nProof. We reiterate that v(\u2206 , D, \u03c0, S, b) defines a scalar field over the non-negative cone b \u2208 (R + \u222a 0) |G| . Treating v as a scalar potential, we may define the conservative vector field F: \nF = \u2207 b v(45)L = \u2207 b w(..., b) b=0 = \u2207 b v(..., b) b=0(51)\nprovides an upper bound for v(..., B):\n\nv(\u2206 , D, \u03c0, S, B) \u2264 \u2206 (\u03c0, S) + L \u00b7 B (52)\nProof. Represent B = g e g B g(53)\nThen, invoking the definition of the derivative as a Weierstrass limit from elementary calculus, as well as Lemma 2.7, and by repeatedly appealing to the assumed subadditivity condition within our domain, we find \nB \u00b7 L = B \u00b7 \u2207 b v(\u03c0, S, b) b=0 (54a) = g B g d dx v(\u03c0, S, xe g ) x=0 (54b) = g B g lim N \u2192\u221e N v(\u03c0, S, 1 N e g ) \u2212 v(\u03c0, S, 0) (54c) = g B g lim N \u2192\u221e N w(\u03c0, S, 1 N e g ) (54d) \u2265 g B g w(\u03c0, S, e g ) (54e) \u2265 g w(\u03c0, S, B g e g ) (54f) \u2265 w(\u03c0, S, B)(54g)\nTherefore, we obtain v(\u03c0, S, B) \u2264 \u2206 (\u03c0, S) + B \u00b7 L (56)\n\nLemma F.1. For each group g \u2208 G, under covariate shift,\nPr \u03c0,T \u0176 =1 | G=g \u2212 Pr \u03c0,S \u0176 =1 | G=g = Cov \u03c0,S \u03c9 g (T , S, X), Pr \u03c0(X,g) (\u0176 =1) (57) Proof. First, note that E S [\u03c9 g (T , S, x)] = 1, since E S [\u03c9 g (T , S, x)] = X \u03c9 g (T , S, x) Pr S (X = x | G=g) dx = X Pr T (X=x | G=g) Pr S (X=x | G = g) Pr S (X=x | G=g) dx = X Pr T (X=x | G=g) dx = 1\nThen, adopting the shorthand \u03c9 g (x) = \u03c9 g (T , S, x), we have: Proof.\nVar[X] = E[(X \u2212 E[X]) 2 ] = E[X 2 ] \u2212 (E[X]) 2 \u2264 E[X] \u2212 (E[X]) 2 (X \u2208 [0, 1]) = E[X](1 \u2212 E(X))\nProof of Theorem 4.1:\n\nStatement: For demographic parity between two groups under covariate shift (denoting, for each g, \u03b2 g := Pr \u03c0,S (\u0176 =1 | G=g)), \u2206 DP (\u03c0, T ) \u2264 \u2206 DP (\u03c0, S) + \n\n\u2206 DP (\u03c0, T ) \u2264 \u2206 DP (\u03c0, S) + y\u2208Y g,h\u2208G \u03b2 g,y (1 \u2212 \u03b2 g,y )B g\n1 /2(72)\nwhere \u03b2 g,y = Pr \u0176 =y | G=g , and assuming Var S [w g (S, T , X)] \u2264 B g .\n\nProof. We again adopt the shorthand \u03c9 g (x) = \u03c9 g (T , S, x). We first generalize Lemma F.1. For each group g \u2208 G, under covariate shift, for all y \u2208 Y, \n\n\u2264 \u2206 DP (\u03c0, S) + y\u2208Y g,h\u2208G \u03b2 g,y (1 \u2212 \u03b2 g,y ) Var\nS [\u03c9 g (x)](75)\n\u2264 \u2206 DP (\u03c0, S) + y\u2208Y g,h\u2208G \u03b2 g,y (1 \u2212 \u03b2 g,y )B g (76)\n\n= \u2206 DP (\u03c0, S) + y\u2208Y g,h\u2208G \u03b2 g,y (1 \u2212 \u03b2 g,y )B g\n1 /2(77)\nand\n\nFor our assumed setting,\n\u03c9 g (x) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 1, x \u2208 [0, \u03c4 g \u2212 m g )\n\u03c4g\u2212x mg , x \u2208 [\u03c4 g \u2212 m g , \u03c4 g ) 1 mg (\u2212x + \u03c4 g + 2m g ), x \u2208 [\u03c4 g , \u03c4 g + m g ) 1,\n\nx \u2208 [\u03c4 g + m g , 1]\n\n(102)\n\nProof for Lemma F.3:\n\nProof. We discuss the target distribution by cases:\n\n\u2022 For the target distribution between [0, \u03c4 g \u2212 M g ]: since we assume the agents are rational, under assumption 6.2, agents with feature that is smaller than [0, \u03c4 g \u2212 M g ] will not perform any kinds of adaptations, and no other agents will adapt their features to this range of features either, so the distribution between [0, \u03c4 g \u2212 M g ] will remain the same as before.\n\n\u2022 For target distribution between [\u03c4 g \u2212 M g , \u03c4 g ], it can be directly calculated from assumption 6.3.\n\n\u2022 For distribution between [\u03c4 g , \u03c4 g + M g ], consider a particular feature x \u2208 [\u03c4 g , \u03c4 g + M g ], under Assumption 6.4, we know its new distribution becomes:\nPr T (x = x ) = 1 + \u03c4g x \u2212Mg 1 \u2212 \u03c4g\u2212z Mg M g \u2212 \u03c4 g + z dz = 1 + \u03c4g x \u2212Mg 1 M g dz = 1 M g (\u2212x + \u03c4 g + 2M g )\n\u2022 For the target distribution between [\u03c4 g + M g , 1]: under assumption 6.2 and 6.4, we know that no agents will change their feature to this feature region. So the distribution between [\u03c4 g + M g , 1] remains the same as the source distribution. Thus, the new feature distribution of x \n\n\n1,\n\nx \u2208 [0, \u03c4 g \u2212 M g ) and x \u2208 [\u03c4 g + M g , 1]\n\n\u03c4g\u2212x\n\nMg , x \u2208 [\u03c4 g \u2212 M g , \u03c4 g ) 1 Mg (\u2212x + \u03c4 g + 2M g ), x \u2208 [\u03c4 g , \u03c4 g + M g ) 0, otherwise\n\nProof of Proposition 6.5:\n\nStatement: For our assumed setting of strategic response involving DP for two groups {g, h}, Theorem 4.1 implies\n\u2206 DP (\u03c0, T ) \u2264 \u2206 DP (\u03c0, S) + \u03c4 g (1 \u2212 \u03c4 g ) 2 3 m g + \u03c4 h (1 \u2212 \u03c4 h ) 2 3 m h(104)\nProof. According to Lemma F.3, we can compute the variance of w g (x): Var(w g (x)) = E w g (x) \u2212 E[w g (x)] 2 = 2 3 M g . Then by plugging it to the general bound for Theorem 4.1 gives us the result.\n\nProof of Theorem 6.6:\n\nStatement: For DP subject to label replicator dynamics, \u2206 DP (\u03c0, T ) \u2264 \u2206 DP (\u03c0, S) + g Q g [t + 1] \u2212 Q g [t]\n\n|\u03c1 1,1 g \u2212 \u03c1 0,1 g | \u03c1 1,1 g + \u03c1 0,1 g (105)\n\nDefinition 3. 1 .\n1Define the supremum value v for \u2206 subject to D(T S) B as v(\u2206 , D, \u03c0, S, B) := sup D(T S) B \u2206 (\u03c0, T )\n\nFigure 2 :\n2A Lipshitz bound for all curves parameterized by distribution shift bound b in the (0, B) D-hyperrectangle on the surface v. In the figure, for groups i \u2208 {g, h}, max \u2202iv = Li, and the colored dotted lines corresponds to Libi, which, when summed, equal L \u00b7 B.\n\nTheorem 3 . 2 (\n32Lipshitz Upper Bound). If there exists an L such that \u2207 b v(\u2206 , D, \u03c0, S, b) L, everywhere along some curve as b varies from 0 to B, then \u2206 (\u03c0, T ) \u2264 \u2206 (\u03c0, S) + L \u00b7 B\n\nDefinition 3. 3 .\n3Define w as the maximum increase in disparity subject to D(T S) B, i.e., w(\u2206 , D, \u03c0, S, B) := v(\u2206 , D, \u03c0, S, B) \u2212 \u2206 (\u03c0, S).\n\nCorollary 4 . 2 .\n42Theorem 4.1 may be generalized to multiple classes Y = {1, 2, ..., m} and multiple groups G \u2208 {1, 2, ..., n}, where \u03b2 g,y = Pr \u0176 =y | G=g and assuming Var S [\u03c9 g (T , S, x)] \u2264 B g :\n\n\nstereoscopic (cross-eye view) comparison between the bound of Section 4.1 (gradated) and simulated results for the model of Section 6.1 (blue) in response to a DP-fair classifier with different initial group-independent acceptance rates. satisfying DP is subject to distribution shift prescribed by replicator dynamics (Section 6.2). Realized disparity increases (blue) are compared to the theoretical bound (Theorem 5.2, gradated), which is tight when group have dissimilar qualification rates.\n\nFigure 3 :\n3Comparisons to synthetic distribution. Larger versions are provided in Appendix E.\n\nFigure 4 :\n4Simulated change in DP violation (blue mesh) subject to geographic and temporal distribution shifts vs. direct application of bounds for approximately satisfied assumptions (respectively, Theorem 4.1 and Theorem 5.2) (gradated mesh). The x-axis and y-axis of both figures represent the policy thresholds \u03c4g and \u03c4 h .\n\n\nD The space of distributions of examples over X \u00d7 Y \u00d7 G. OThe space of distributions of outcomes over X \u00d7 Y \u00d7\u0176. GThe space of distributions of group-conditioned examples X \u00d7 Y. S\n\nFigure 6 :Figure 7 :Figure 8 :\n678A geometric bound in an infinite-dimensional vector space (i.e., a Hilbert space), represented with a stereoscopic (cross-eye) view in three dimensions (to provide intuition) and an examination of the (tg, 1)-plane.The extreme values of \u03b2 + g correspond to the extremal angles of \u03c6 and \u03b8. In this figure, the vector displayed parallel to s \u22121 g from the origin terminates on the hyperplane of normalized distributions. A stereoscopic (cross-eye view) comparison between the bound of Section 4.1 (gradated) and simulated results for the model of Section 6.1 (blue) in response to a DP-fair classifier with different initial group-independent acceptance rates. The x-axis represents the maximum shift Dg over all groups g in response to the classifier. A stereoscopic (cross-eye view) comparison between the theoretical bound of Section 4.2 (gradated) and simulated results for the model of Section 6.1 (blue) in response to a EOp-fair classifier with different initial group-independent true positive rates (TPR). The x-axis represents the maximum shift Dg over all groups g in response to the classifier. As Corollary 4.4 limits the maximum possible value of EO violation, we include this limit as part of the bound.\n\nFigure 10 :Figure 11 :\n101111 compares the label shift bound of Theorem 5.2 to the violation of demographic parity for hypothetical policies trained for a US state in 2014 and deployed in 2018. Figure 12 compares the covariate shift bound of Theorem 4.3 with Theorem D.1 to the violation of equal opportunity for hypothetical policies trained on one US state and deployed in another. Change in violation of demographic parity for hypothetical policies trained on one US state's data and reused for another state (blue) compared to covariate-shift bounds (Theorem 4.1, gradated). The x-axis and y-axis represent the thresholds \u03c4g and \u03c4 h , respectively. Change in violation of demographic parity for hypothetical policies trained on 2014 data and reused for 2018 (blue) compared to label-shift bounds (Theorem 5.2, gradated). The x-axis and y-axis represent the thresholds \u03c4g and \u03c4 h , respectively.\n\nFigure 12 :\n12Change in violation of equal opportunity for hypothetical policies trained on one US state's data and reused for another state (blue) compared to covariate-shift bounds(Theorems 4.3 and D.1, gradated). The x-axis and y-axis represent the thresholds \u03c4g and \u03c4 h , respectively.\n\n\nThis formulation, in terms of a potential, ensures the path-independence of the line integral of F along any continuous curve C from 0 to B. That is,v(..., B) \u2212 v(..., 0) = C F(b) \u00b7 db(46)Therefore, given a Lipshitz condition for F along any curve C with endpoints 0 and B, i.e. when there exists some finite L such that\u2200b \u2208 C, F(b) L (47) and therefore v(\u2206 , D, \u03c0, S, B) = v(..., 0) + C F(b) \u00b7 db (48) \u2264 \u2206 (\u03c0, S) + L \u00b7 B(49)By the bounded distribution shift assumption (Assumption 2.6), Lemma 2.7, and the definition of the supremum bound (Definition 3.1), we conclude \u2206 (\u03c0, T ) \u2264 \u2206 (\u03c0, S) + L \u00b7 B (50) Proof of Theorem 3.4: Statement: Suppose, in the region D(T S) B, that w is subadditive in its last argument. That is, w(..., a) + w(..., c) \u2265 w(..., a + c) for a, c 0 and a + c B. Then, a local, first-order approximation of w(..., b) evaluated at 0, i.e.,\n\n\n\u03c0, S, B) := v(\u03c0, S, B) \u2212 \u2206 (\u03c0, S)\n\n. 2 .\n2g (x)]) | G=g (E[f (x) \u2212 E[f (x)]If X is a random variable and X \u2208 [0, 1], then Var(X) \u2264 E[X](1 \u2212 E(X)).\n\n(. 2 :\n2Again adopting the shorthand \u03c9 g (x) = \u03c9 g (T , S, Cov[a, b] \u2264 Var[a] \u00b7 Var[b]) g (x)] \u00b7 \u03b2 g (1 \u2212 \u03b2 g ) + Var S [\u03c9 h (x)] \u00b7 \u03b2 h (1 \u2212 \u03b2 h ) (\u03b2 g = Pr \u03c0,S (\u0176 =1|G=g) = E S [1 \u03c0(x,g) Statement: Theorem 4.1 Theorem 4.1 may be generalized to multiple classes Y = {1, 2, ..., m} and multiple groups G \u2208 {1, 2, ..., n}, \u2206 DP (\u03c0, T )\n\n\nlogic of Theorem 4.1, for Var S [\u03c9 g (T , S, x)] \u2264 B g , it follows that \u2206 DP (\u03c0, T )\n\n[ 17 ]\n17Nina Grgi\u0107-Hla\u010da, Muhammad Bilal Zafar, Krishna P. Gummadi, and Adrian Weller. On fairness, diversity and randomness in algorithmic decision making, 2017. [18] Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. Strategic classification. In Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science, page 111-122, New York, NY, USA, 2016. Association for Computing Machinery. Lydia T Liu, Ashia Wilson, Nika Haghtalab, Adam Tauman Kalai, Christian Borgs, and Jennifer Chayes. The disparate equilibria of algorithmic decision making when individuals invest rationally. In Proceedings of the 2020 Conference on Fairness, Accountability, and Debmalya Mandal, Samuel Deng, Suman Jana, Jeannette Wing, and Daniel J Hsu. Ensuring fairness beyond the training data. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 18445-18456. Curran Associates, Inc., 2020.[19] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In \nAdvances in neural information processing systems, pages 3315-3323, 2016. \n\n[20] Lily Hu and Yiling Chen. A short-term intervention for long-term fairness in the labor market. \nIn Proceedings of the 2018 World Wide Web Conference on World Wide Web, pages 1389-1398. \nInternational World Wide Web Conferences Steering Committee, 2018. \n\n[21] Mintong Kang, Linyi Li, Maurice Weber, Yang Liu, Ce Zhang, and Bo Li. Certifying some \ndistributional fairness with subpopulation decomposition. arXiv preprint arXiv:2205.15494, \n2022. \n\n[22] Sean Kulinski, Saurabh Bagchi, and David I Inouye. Feature shift detection: Localizing which \nfeatures have shifted via conditional distribution tests. In H. Larochelle, M. Ranzato, R. Had-\nsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, \nvolume 33, pages 19523-19533. Curran Associates, Inc., 2020. URL https://proceedings. \nneurips.cc/paper/2020/file/e2d52448d36918c575fa79d88647ba66-Paper.pdf. \n\n[23] Lydia T Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed impact of \nfair machine learning. In International Conference on Machine Learning, pages 3150-3158. \nPMLR, 2018. \n\n[24] Transparency, pages 381-391, 2020. \n\n[25] Yang Liu, Yatong Chen, Zeyu Tang, and Kun Zhang. Model transferability with responsive \ndecision subjects, 2021. \n\n[26] [27] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning \nbounds and algorithms, 2009. \n\n[28] Hussein Mouzannar, Mesrob I Ohannessian, and Nathan Srebro. From fair decision making to \nsocial equality. In Proceedings of the Conference on Fairness, Accountability, and Transparency, \npages 359-368. ACM, 2019. \n\n[29] Harikrishna Narasimhan. Learning with complex loss functions and constraints. In Proceedings \nof the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 \nof Proceedings of Machine Learning Research, pages 1646-1654. PMLR, 09-11 Apr 2018. \n\n[30] Reilly Raab and Yang Liu. Unintended selection: Persistent qualification rate disparities and \ninterventions. Advances in Neural Information Processing Systems, 34, 2021. \n\n[31] Ashkan Rezaei, Anqi Liu, Omid Memarrast, and Brian D. Ziebart. Robust fairness under \ncovariate shift. In AAAI, 2021. \n\n[32] Yuji Roh, Kangwook Lee, Steven Whang, and Changho Suh. Sample selection for fair and \nrobust training. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman \nVaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages \n815-827. Curran Associates, Inc., 2021. \n\n\nTable 1 :\n1Primary Notation B Extended Discussion of Related Work Domain Adaptation: Prior work has considered the conditions under which a classifier trained on a source distribution will perform well on a given target distribution, for example, by deriving bounds on the number of training examples from the target distribution needed to bound prediction error\nDespite use on Wikipedia, this is not a standard term in the literature. In general, the axioms of a premetric as defined in Definition 2.1 are a subset (thus \"pre\") of those that define a metric.\nwhere U represents the uniform distribution.5 SeeFigure 5in Appendix C for a demonstration of Theorem 4.1.\nThis package is available at https://github.com/zykls/folktables.\nThis precludes distributions with non-zero probability mass concentrated at singular points.\nAcknowledgement This work is supported by the National Science Foundation (NSF) under grants IIS-2143895, IIS-2040800 (FAI program in collaboration with Amazon), and CCF-2023495.Proof of Theorem 4.3Statement: Subject to covariate shift and any given D, B, assume extremal values for \u03b2 + g , i.e., \u2200g, D g (T S) < B g =\u21d2 l g \u2264 \u03b2 + g (\u03c0, T ) < u g (78)then, for v corresponding to \u2206 EOp , v(\u2206 EOp , D, \u03c0, S, B) \u2264 max xg\u2208{lg,ug} g,hProof. Recall that, for this setting, v(\u2206 EOp , D, \u03c0, S, B) = supandThis latter expression is convex in each \u03b2 + g . Therefore, \u2206 EOp is maximized on the boundary of its domain, i.e. \u03b2 + g \u2208 {l g , u g } for each g, given the assumption of the theorem.Proof of Corollary 4.4Statement: The disparity measurement \u2206 EOp cannot exceed |G| 2 4 .Proof. We note that each \u03b2 + g is ultimately confined to the interval [0, 1]. Building on our proof for Theorem 4.3, to maximize \u2206 EOp , we must consider the boundary of this domain, where, for each g, \u03b2 + g \u2208 {0, 1}. Because the only terms that contribute to \u2206 EOp are those in which \u03b2 + g = 1 and \u03b2 + h = 0 (as opposed to \u03b2 + g = \u03b2 + h ), we seek to maximize the number of such terms. This occurs when as close to half of the groups as possible have one extremal true positive rate (e.g., without loss of generality, \u03b2 + g = 1) and the remaining groups have the other (e.g., \u03b2 + g = 0). In such cases, \u2206 EOp is given byProof of Theorem 5.1:Specifically,for true positive rates \u03b2 + g and false positive rates \u03b2 \u2212 g :Proof. We first establish that Dis an appropriate measure of group-conditioned distribution shift (Definition 2.5). That D satisfies the axioms of a divergence on group-conditioned distributions subject to the label shift assumption (Pr T (X | Y, G) = Pr S (X | Y, G)) and unchanging group sizes is easily verified:We next show that (|G| \u2212 1)|\u03b2 + g \u2212 \u03b2 \u2212 g | is the corresponding Lipshitz bound for the slope of v with respect to B g , where we recallThat is, we wish to showThis follows directly from recognition that \u2206 DP is locally always affine in the acceptance rate for each group, with slope bounded by one less than the number of groups.By the definition of conditional probability,It follows by the chain rule that, for all T mutated from S subject to label shift,By the linearity of derivatives, for fixed S, this implies that for all T attainable via label shift,Since this equation holds for all T , it must also hold when evaluated at v, the supremum of \u2206 . It follows thatProof of Theorem 5.2:Proof. This follows from the Lipshitz property implied by Theorem 5.1 (Equation (99)) and Theorem 3.2.F.1 Omitted details for Section 6.1Lemma F.3. Recall the covariate shift reweighting coefficient \u03c9 g (x), defined in Section 4.1.Proof. We may directly substituteProof of Theorem D.1:Statement:The true positive rate \u03b2 + g is bounded over the domain of covariate shift D cov [B], which we define by the bound D(T S) B, and the invariance of Pr(Y =1 | X=x, G=g) for all x, g, aswhereProof. To be rigorous, we may give an explicit expression for r \u22a5 g by implicitly forming a basis in the (1, t g )-plane via the Gram-Schmidt process.r \u22a5 g := r g , t g g t g t g 2 + r g , u g g u g u g 2(108)From which we may verify that u g , t g = 0 (111) r \u22a5 g , t g g = r g , t g g (112)r \u22a5 g , u g g = r g , u g g (113)Recalling the relationship between the cosine of an angle between two vectors and inner products:It follows from Equation (31) that, defining \u03be g := \u2220(t g , 1),By the monotonicity of the final expression above with respect to \u03c6 g , for fixed \u03be g :We note that Equation (117) is strictly negative, thus the expression in Equation (116) must be monotonic for fixed \u03be. We may conclude that \u03b2 + g is extremized with extremal values of \u03c6 g , denoted as \u03c6 u g and \u03c6 l g .\nTransferring fairness under distribution shifts via fair consistency regularization. Zora Bang An, Mucong Che, Furong Ding, Huang, arXiv:2206.12796arXiv preprintBang An, Zora Che, Mucong Ding, and Furong Huang. Transferring fairness under distribution shifts via fair consistency regularization. arXiv preprint arXiv:2206.12796, 2022.\n\nA theory of learning from different domains. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, Jennifer Vaughan, Machine Learning. 79Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Vaughan. A theory of learning from different domains. Machine Learning, 79:151-175, 2010.\n\nEnsuring fairness under prior probability shifts. Arpita Biswas, Suvam Mukherjee, Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. the 2021 AAAI/ACM Conference on AI, Ethics, and SocietyArpita Biswas and Suvam Mukherjee. Ensuring fairness under prior probability shifts. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pages 414-424, 2021.\n\nUci repository of machine learning databases. Catherine Blake, Catherine Blake. Uci repository of machine learning databases. 1998.\n\nLinear classifiers that encourage constructive adaptation. Yatong Chen, Jialu Wang, Yang Liu, Algorithmic Recourse workshop at ICML'21. Yatong Chen, Jialu Wang, and Yang Liu. Linear classifiers that encourage constructive adapta- tion. In Algorithmic Recourse workshop at ICML'21, 2021.\n\nFair prediction with disparate impact: A study of bias in recidivism prediction instruments. Alexandra Chouldechova, 5Big dataAlexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data, 5(2):153-163, 2017.\n\nWill affirmative-action policies eliminate negative stereotypes? The American Economic Review. Stephen Coate, C Glenn, Loury, Stephen Coate and Glenn C Loury. Will affirmative-action policies eliminate negative stereo- types? The American Economic Review, pages 1220-1240, 1993.\n\nAlgorithmic decision making and the cost of fairness. Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, Aziz Huq, Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining. the 23rd acm sigkdd international conference on knowledge discovery and data miningSam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making and the cost of fairness. In Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining, pages 797-806, 2017.\n\nFair transfer learning with missing protected attributes. Amanda Coston, Dennis Karthikeyan Natesan Ramamurthy, Kush R Wei, Skyler Varshney, Zairah Speakman, Supriyo Mustahsan, Chakraborty, 10.1145/3306618.3314236Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES '19. the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES '19New York, NY, USAAssociation for Computing MachineryAmanda Coston, Karthikeyan Natesan Ramamurthy, Dennis Wei, Kush R. Varshney, Skyler Speakman, Zairah Mustahsan, and Supriyo Chakraborty. Fair transfer learning with missing protected attributes. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES '19, page 91-98, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450363242. doi: 10.1145/3306618.3314236. URL https://doi.org/ 10.1145/3306618.3314236.\n\nOn making stochastic classifiers deterministic. Andrew Cotter, Maya Gupta, Harikrishna Narasimhan, Advances in Neural Information Processing Systems. Curran Associates, Inc32Andrew Cotter, Maya Gupta, and Harikrishna Narasimhan. On making stochastic classifiers deterministic. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\n\nCausal modeling for fairness in dynamical systems. Elliot Creager, David Madras, Toniann Pitassi, Richard Zemel, Proceedings of the 37th International Conference on Machine Learning, ICML'20. JMLR.org. the 37th International Conference on Machine Learning, ICML'20. JMLR.orgElliot Creager, David Madras, Toniann Pitassi, and Richard Zemel. Causal modeling for fairness in dynamical systems. In Proceedings of the 37th International Conference on Machine Learning, ICML'20. JMLR.org, 2020.\n\nFairness is not static: deeper understanding of long term fairness via simulation studies. Hansa Alexander D&apos;amour, James Srinivasan, Pallavi Atwood, Baljekar, Yoni Sculley, Halpern, Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. the 2020 Conference on Fairness, Accountability, and TransparencyAlexander D'Amour, Hansa Srinivasan, James Atwood, Pallavi Baljekar, D Sculley, and Yoni Halpern. Fairness is not static: deeper understanding of long term fairness via simulation studies. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pages 525-534, 2020.\n\nRetiring adult: New datasets for fair machine learning. Frances Ding, Moritz Hardt, John Miller, Ludwig Schmidt, Thirty-Fifth Conference on Neural Information Processing Systems. Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair machine learning. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=bYi_2708mKK.\n\nFairness through awareness. Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, Richard Zemel, Proceedings of the 3rd innovations in theoretical computer science conference. the 3rd innovations in theoretical computer science conferenceCynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pages 214-226, 2012.\n\nCertifying and removing disparate impact. Michael Feldman, A Sorelle, John Friedler, Carlos Moeller, Suresh Scheidegger, Venkatasubramanian, proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining. the 21th ACM SIGKDD international conference on knowledge discovery and data miningMichael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkata- subramanian. Certifying and removing disparate impact. In proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pages 259-268, 2015.\n\nIntegrated public use microdata series. Sarah Flood, Miriam King, Renae Rodgers Steven Ruggles, J. Robert Warren, current population survey: Version 8.0 [datasetSarah Flood, Miriam King, Renae Rodgers Steven Ruggles, and J. Robert Warren. Integrated public use microdata series, current population survey: Version 8.0 [dataset], 2020. URL https://www.ipums.org/projects/ipums-cps/d030.v8.0.\n\nMaintaining fairness across distribution shift: do we have viable solutions for real. Jessica Schrouff, Natalie Harris, Oluwasanmi Koyejo, Ibrahim Alabdulmohsin, Eva Schnider, Krista Opsahl-Ong, Alex Brown, Subhrajit Roy, Diana Mincu, Christina Chen, arXiv:2202.01034arXiv preprintJessica Schrouff, Natalie Harris, Oluwasanmi Koyejo, Ibrahim Alabdulmohsin, Eva Schnider, Krista Opsahl-Ong, Alex Brown, Subhrajit Roy, Diana Mincu, Christina Chen, et al. Maintaining fairness across distribution shift: do we have viable solutions for real-world applications? arXiv preprint arXiv:2202.01034, 2022.\n\nTransfer of machine learning fairness across domains. Candice Schumann, Xuezhi Wang, Alex Beutel, Jilin Chen, Hai Qian, Ed H Chi, Candice Schumann, Xuezhi Wang, Alex Beutel, Jilin Chen, Hai Qian, and Ed H. Chi. Transfer of machine learning fairness across domains, 2019.\n\nImproving predictive inference under covariate shift by weighting the log-likelihood function. Hidetoshi Shimodaira, Journal of statistical planning and inference. 902Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of statistical planning and inference, 90(2):227-244, 2000.\n\nFairness violations and mitigation under covariate shift. Harvineet Singh, Rina Singh, Vishwali Mhasawade, Rumi Chunara, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21. the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21New York, NY, USAAssociation for Computing MachineryHarvineet Singh, Rina Singh, Vishwali Mhasawade, and Rumi Chunara. Fairness violations and mitigation under covariate shift. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, New York, NY, USA, 2021. Association for Computing Machinery.\n\nDirect importance estimation for covariate shift adaptation. Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima, Hisashi Kashima, Motoaki Paul Von B\u00fcnau, Kawanabe, Annals of the Institute of Statistical Mathematics. 604Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima, Hisashi Kashima, Paul von B\u00fcnau, and Motoaki Kawanabe. Direct importance estimation for covariate shift adaptation. Annals of the Institute of Statistical Mathematics, 60(4):699-746, 2008.\n\nActionable recourse in linear classification. Berk Ustun, Alexander Spangher, Yang Liu, Proceedings of the Conference on Fairness, Accountability, and Transparency. the Conference on Fairness, Accountability, and TransparencyBerk Ustun, Alexander Spangher, and Yang Liu. Actionable recourse in linear classification. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 10-19, 2019.\n\nMin Wen, Osbert Bastani, Ufuk Topcu, arXiv:1901.08568Fairness with Dynamics. arXiv preprintMin Wen, Osbert Bastani, and Ufuk Topcu. Fairness with Dynamics. arXiv preprint arXiv:1901.08568, 2019.\n\nMetric-fair classifier derandomization. Jimmy Wu, Yatong Chen, Yang Liu, PMLRProceedings of the 39th International Conference on Machine Learning. the 39th International Conference on Machine Learning162Jimmy Wu, Yatong Chen, and Yang Liu. Metric-fair classifier derandomization. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research. PMLR, 17-23 Jul 2022.\n\nLearning fair representations. Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, Cynthia Dwork, International conference on machine learning. PMLRRich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representa- tions. In International conference on machine learning, pages 325-333. PMLR, 2013.\n\nDomain adaptation under target and conditional shift. Kun Zhang, Bernhard Sch\u00f6lkopf, Krikamol Muandet, Zhikun Wang, International Conference on Machine Learning. PMLRKun Zhang, Bernhard Sch\u00f6lkopf, Krikamol Muandet, and Zhikun Wang. Domain adaptation under target and conditional shift. In International Conference on Machine Learning, pages 819-827. PMLR, 2013.\n\nHow do fair decisions fare in long-term qualification? In NeurIPS. Xueru Zhang, Ruibo Tu, Yang Liu, Mingyan Liu, Hedvig Kjellstr\u00f6m, Kun Zhang, Cheng Zhang, Xueru Zhang, Ruibo Tu, Yang Liu, Mingyan Liu, Hedvig Kjellstr\u00f6m, Kun Zhang, and Cheng Zhang. How do fair decisions fare in long-term qualification? In NeurIPS, 2020.\n", "annotations": {"author": "[{\"end\":115,\"start\":66},{\"end\":181,\"start\":116},{\"end\":230,\"start\":182},{\"end\":294,\"start\":231}]", "publisher": null, "author_last_name": "[{\"end\":77,\"start\":73},{\"end\":127,\"start\":123},{\"end\":192,\"start\":188},{\"end\":239,\"start\":236}]", "author_first_name": "[{\"end\":72,\"start\":66},{\"end\":122,\"start\":116},{\"end\":187,\"start\":182},{\"end\":235,\"start\":231}]", "author_affiliation": "[{\"end\":114,\"start\":79},{\"end\":180,\"start\":145},{\"end\":229,\"start\":194},{\"end\":293,\"start\":258}]", "title": "[{\"end\":63,\"start\":1},{\"end\":357,\"start\":295}]", "venue": null, "abstract": "[{\"end\":2483,\"start\":359}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3533,\"start\":3530},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3821,\"start\":3817},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4910,\"start\":4907},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4913,\"start\":4910},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5885,\"start\":5882},{\"end\":5888,\"start\":5885},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6270,\"start\":6266},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6272,\"start\":6270},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6275,\"start\":6272},{\"end\":6278,\"start\":6275},{\"end\":6281,\"start\":6278},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6318,\"start\":6314},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6491,\"start\":6488},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6643,\"start\":6639},{\"end\":6884,\"start\":6880},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7548,\"start\":7545},{\"end\":7551,\"start\":7548},{\"end\":7554,\"start\":7551},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7556,\"start\":7554},{\"end\":7559,\"start\":7556},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7659,\"start\":7656},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7662,\"start\":7659},{\"end\":7665,\"start\":7662},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7699,\"start\":7695},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7934,\"start\":7931},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10882,\"start\":10878},{\"end\":10885,\"start\":10882},{\"end\":10888,\"start\":10885},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10891,\"start\":10888},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16435,\"start\":16432},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24951,\"start\":24948},{\"end\":27193,\"start\":27189},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":29936,\"start\":29932},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30051,\"start\":30047},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":30260,\"start\":30257},{\"end\":30667,\"start\":30663},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":34193,\"start\":34189},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":35296,\"start\":35293},{\"end\":35299,\"start\":35296},{\"end\":35383,\"start\":35379},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":35713,\"start\":35709},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":35716,\"start\":35713},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":35719,\"start\":35716},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":36032,\"start\":36028},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":36055,\"start\":36051},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":36058,\"start\":36055},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":36060,\"start\":36058},{\"end\":36063,\"start\":36060},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":36065,\"start\":36063},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":36195,\"start\":36192},{\"end\":36219,\"start\":36215},{\"end\":36544,\"start\":36540},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":36832,\"start\":36828},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":36835,\"start\":36832},{\"end\":36838,\"start\":36835},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":36841,\"start\":36838},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":36844,\"start\":36841},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":36847,\"start\":36844},{\"end\":36850,\"start\":36847},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":36852,\"start\":36850},{\"end\":36855,\"start\":36852},{\"end\":36858,\"start\":36855},{\"end\":36861,\"start\":36858},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":36882,\"start\":36878},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":40846,\"start\":40842},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":57477,\"start\":57476}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":47398,\"start\":47278},{\"attributes\":{\"id\":\"fig_2\"},\"end\":47671,\"start\":47399},{\"attributes\":{\"id\":\"fig_3\"},\"end\":47856,\"start\":47672},{\"attributes\":{\"id\":\"fig_4\"},\"end\":48000,\"start\":47857},{\"attributes\":{\"id\":\"fig_5\"},\"end\":48203,\"start\":48001},{\"attributes\":{\"id\":\"fig_6\"},\"end\":48701,\"start\":48204},{\"attributes\":{\"id\":\"fig_7\"},\"end\":48797,\"start\":48702},{\"attributes\":{\"id\":\"fig_9\"},\"end\":49127,\"start\":48798},{\"attributes\":{\"id\":\"fig_10\"},\"end\":49308,\"start\":49128},{\"attributes\":{\"id\":\"fig_11\"},\"end\":50560,\"start\":49309},{\"attributes\":{\"id\":\"fig_12\"},\"end\":51460,\"start\":50561},{\"attributes\":{\"id\":\"fig_13\"},\"end\":51751,\"start\":51461},{\"attributes\":{\"id\":\"fig_14\"},\"end\":52614,\"start\":51752},{\"attributes\":{\"id\":\"fig_15\"},\"end\":52650,\"start\":52615},{\"attributes\":{\"id\":\"fig_16\"},\"end\":52763,\"start\":52651},{\"attributes\":{\"id\":\"fig_17\"},\"end\":53098,\"start\":52764},{\"attributes\":{\"id\":\"fig_18\"},\"end\":53186,\"start\":53099},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":56870,\"start\":53187},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":57234,\"start\":56871}]", "paragraph": "[{\"end\":4097,\"start\":2499},{\"end\":5043,\"start\":4099},{\"end\":5495,\"start\":5045},{\"end\":6105,\"start\":5512},{\"end\":7389,\"start\":6107},{\"end\":7868,\"start\":7391},{\"end\":8207,\"start\":7870},{\"end\":8757,\"start\":8229},{\"end\":9493,\"start\":8759},{\"end\":9593,\"start\":9509},{\"end\":10110,\"start\":9620},{\"end\":10503,\"start\":10112},{\"end\":10892,\"start\":10505},{\"end\":11090,\"start\":10894},{\"end\":11262,\"start\":11162},{\"end\":12006,\"start\":11293},{\"end\":12636,\"start\":12084},{\"end\":12728,\"start\":12704},{\"end\":12953,\"start\":12791},{\"end\":13064,\"start\":13037},{\"end\":13212,\"start\":13124},{\"end\":13592,\"start\":13214},{\"end\":14287,\"start\":13630},{\"end\":14648,\"start\":14347},{\"end\":15101,\"start\":14650},{\"end\":15635,\"start\":15103},{\"end\":15719,\"start\":15697},{\"end\":15924,\"start\":15754},{\"end\":16038,\"start\":15943},{\"end\":16225,\"start\":16040},{\"end\":16664,\"start\":16271},{\"end\":16987,\"start\":16688},{\"end\":17648,\"start\":16989},{\"end\":17756,\"start\":17722},{\"end\":18207,\"start\":17808},{\"end\":18767,\"start\":18231},{\"end\":18970,\"start\":18787},{\"end\":19529,\"start\":18993},{\"end\":19771,\"start\":19610},{\"end\":20386,\"start\":19915},{\"end\":20791,\"start\":20527},{\"end\":21295,\"start\":20813},{\"end\":21710,\"start\":21512},{\"end\":22082,\"start\":21726},{\"end\":22279,\"start\":22084},{\"end\":22483,\"start\":22470},{\"end\":22605,\"start\":22543},{\"end\":22891,\"start\":22677},{\"end\":23602,\"start\":22984},{\"end\":23868,\"start\":23672},{\"end\":24161,\"start\":23911},{\"end\":24493,\"start\":24243},{\"end\":24952,\"start\":24583},{\"end\":25142,\"start\":24954},{\"end\":25294,\"start\":25199},{\"end\":26144,\"start\":25467},{\"end\":26474,\"start\":26166},{\"end\":26856,\"start\":26556},{\"end\":27002,\"start\":26883},{\"end\":27503,\"start\":27042},{\"end\":28147,\"start\":27541},{\"end\":28291,\"start\":28246},{\"end\":28750,\"start\":28347},{\"end\":29231,\"start\":28847},{\"end\":29660,\"start\":29263},{\"end\":30397,\"start\":29710},{\"end\":30480,\"start\":30399},{\"end\":31661,\"start\":30482},{\"end\":33308,\"start\":31663},{\"end\":33996,\"start\":33338},{\"end\":34540,\"start\":33998},{\"end\":34625,\"start\":34555},{\"end\":34654,\"start\":34627},{\"end\":34721,\"start\":34656},{\"end\":34818,\"start\":34723},{\"end\":34954,\"start\":34820},{\"end\":35059,\"start\":34956},{\"end\":35138,\"start\":35066},{\"end\":35229,\"start\":35140},{\"end\":35720,\"start\":35235},{\"end\":36241,\"start\":35722},{\"end\":37340,\"start\":36243},{\"end\":37525,\"start\":37365},{\"end\":37881,\"start\":37558},{\"end\":37988,\"start\":37883},{\"end\":38163,\"start\":38028},{\"end\":38318,\"start\":38291},{\"end\":38498,\"start\":38454},{\"end\":38751,\"start\":38500},{\"end\":38951,\"start\":38753},{\"end\":39357,\"start\":39066},{\"end\":40094,\"start\":39401},{\"end\":40374,\"start\":40147},{\"end\":40572,\"start\":40376},{\"end\":40721,\"start\":40653},{\"end\":41225,\"start\":40784},{\"end\":41512,\"start\":41227},{\"end\":41954,\"start\":41649},{\"end\":42300,\"start\":41993},{\"end\":42340,\"start\":42321},{\"end\":42408,\"start\":42342},{\"end\":42592,\"start\":42410},{\"end\":42620,\"start\":42617},{\"end\":42807,\"start\":42680},{\"end\":42988,\"start\":42871},{\"end\":43126,\"start\":43014},{\"end\":43354,\"start\":43160},{\"end\":43452,\"start\":43414},{\"end\":43495,\"start\":43454},{\"end\":43744,\"start\":43531},{\"end\":44048,\"start\":43993},{\"end\":44105,\"start\":44050},{\"end\":44468,\"start\":44398},{\"end\":44585,\"start\":44564},{\"end\":44743,\"start\":44587},{\"end\":44805,\"start\":44745},{\"end\":44888,\"start\":44815},{\"end\":45043,\"start\":44890},{\"end\":45093,\"start\":45045},{\"end\":45162,\"start\":45110},{\"end\":45211,\"start\":45164},{\"end\":45224,\"start\":45221},{\"end\":45250,\"start\":45226},{\"end\":45385,\"start\":45302},{\"end\":45406,\"start\":45387},{\"end\":45413,\"start\":45408},{\"end\":45435,\"start\":45415},{\"end\":45488,\"start\":45437},{\"end\":45863,\"start\":45490},{\"end\":45969,\"start\":45865},{\"end\":46131,\"start\":45971},{\"end\":46528,\"start\":46241},{\"end\":46578,\"start\":46535},{\"end\":46584,\"start\":46580},{\"end\":46674,\"start\":46586},{\"end\":46701,\"start\":46676},{\"end\":46815,\"start\":46703},{\"end\":47098,\"start\":46898},{\"end\":47121,\"start\":47100},{\"end\":47231,\"start\":47123},{\"end\":47277,\"start\":47233}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11161,\"start\":11091},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12083,\"start\":12007},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12703,\"start\":12637},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12790,\"start\":12729},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13036,\"start\":12954},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13123,\"start\":13065},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14346,\"start\":14288},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15696,\"start\":15636},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15753,\"start\":15720},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16270,\"start\":16226},{\"attributes\":{\"id\":\"formula_12\"},\"end\":17721,\"start\":17676},{\"attributes\":{\"id\":\"formula_13\"},\"end\":17807,\"start\":17757},{\"attributes\":{\"id\":\"formula_14\"},\"end\":19609,\"start\":19530},{\"attributes\":{\"id\":\"formula_15\"},\"end\":19914,\"start\":19772},{\"attributes\":{\"id\":\"formula_16\"},\"end\":20526,\"start\":20387},{\"attributes\":{\"id\":\"formula_17\"},\"end\":21511,\"start\":21296},{\"attributes\":{\"id\":\"formula_18\"},\"end\":22469,\"start\":22280},{\"attributes\":{\"id\":\"formula_19\"},\"end\":22542,\"start\":22484},{\"attributes\":{\"id\":\"formula_20\"},\"end\":22676,\"start\":22606},{\"attributes\":{\"id\":\"formula_21\"},\"end\":22983,\"start\":22892},{\"attributes\":{\"id\":\"formula_22\"},\"end\":24242,\"start\":24162},{\"attributes\":{\"id\":\"formula_23\"},\"end\":24582,\"start\":24494},{\"attributes\":{\"id\":\"formula_24\"},\"end\":25198,\"start\":25143},{\"attributes\":{\"id\":\"formula_25\"},\"end\":25466,\"start\":25295},{\"attributes\":{\"id\":\"formula_26\"},\"end\":26165,\"start\":26145},{\"attributes\":{\"id\":\"formula_27\"},\"end\":26555,\"start\":26475},{\"attributes\":{\"id\":\"formula_28\"},\"end\":26882,\"start\":26857},{\"attributes\":{\"id\":\"formula_29\"},\"end\":27540,\"start\":27504},{\"attributes\":{\"id\":\"formula_30\"},\"end\":28245,\"start\":28148},{\"attributes\":{\"id\":\"formula_31\"},\"end\":28346,\"start\":28292},{\"attributes\":{\"id\":\"formula_32\"},\"end\":28846,\"start\":28751},{\"attributes\":{\"id\":\"formula_33\"},\"end\":29262,\"start\":29232},{\"attributes\":{\"id\":\"formula_34\"},\"end\":38027,\"start\":37989},{\"attributes\":{\"id\":\"formula_35\"},\"end\":38290,\"start\":38164},{\"attributes\":{\"id\":\"formula_36\"},\"end\":38354,\"start\":38319},{\"attributes\":{\"id\":\"formula_37\"},\"end\":38391,\"start\":38354},{\"attributes\":{\"id\":\"formula_38\"},\"end\":38453,\"start\":38391},{\"attributes\":{\"id\":\"formula_39\"},\"end\":39065,\"start\":38952},{\"attributes\":{\"id\":\"formula_40\"},\"end\":39400,\"start\":39358},{\"attributes\":{\"id\":\"formula_41\"},\"end\":40146,\"start\":40095},{\"attributes\":{\"id\":\"formula_42\"},\"end\":40652,\"start\":40573},{\"attributes\":{\"id\":\"formula_43\"},\"end\":40783,\"start\":40722},{\"attributes\":{\"id\":\"formula_44\"},\"end\":41648,\"start\":41513},{\"attributes\":{\"id\":\"formula_45\"},\"end\":42616,\"start\":42593},{\"attributes\":{\"id\":\"formula_46\"},\"end\":42679,\"start\":42621},{\"attributes\":{\"id\":\"formula_47\"},\"end\":42870,\"start\":42808},{\"attributes\":{\"id\":\"formula_48\"},\"end\":43159,\"start\":43127},{\"attributes\":{\"id\":\"formula_49\"},\"end\":43368,\"start\":43355},{\"attributes\":{\"id\":\"formula_50\"},\"end\":43413,\"start\":43368},{\"attributes\":{\"id\":\"formula_51\"},\"end\":43530,\"start\":43496},{\"attributes\":{\"id\":\"formula_52\"},\"end\":43992,\"start\":43745},{\"attributes\":{\"id\":\"formula_54\"},\"end\":44397,\"start\":44106},{\"attributes\":{\"id\":\"formula_55\"},\"end\":44563,\"start\":44469},{\"attributes\":{\"id\":\"formula_57\"},\"end\":44814,\"start\":44806},{\"attributes\":{\"id\":\"formula_59\"},\"end\":45109,\"start\":45094},{\"attributes\":{\"id\":\"formula_60\"},\"end\":45220,\"start\":45212},{\"attributes\":{\"id\":\"formula_61\"},\"end\":45301,\"start\":45251},{\"attributes\":{\"id\":\"formula_62\"},\"end\":46240,\"start\":46132},{\"attributes\":{\"id\":\"formula_64\"},\"end\":46897,\"start\":46816}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2497,\"start\":2485},{\"attributes\":{\"n\":\"1.1\"},\"end\":5510,\"start\":5498},{\"attributes\":{\"n\":\"1.2\"},\"end\":8227,\"start\":8210},{\"attributes\":{\"n\":\"2\"},\"end\":9507,\"start\":9496},{\"attributes\":{\"n\":\"2.1\"},\"end\":9618,\"start\":9596},{\"attributes\":{\"n\":\"2.2\"},\"end\":11291,\"start\":11265},{\"attributes\":{\"n\":\"2.3\"},\"end\":13628,\"start\":13595},{\"attributes\":{\"n\":\"3\"},\"end\":15941,\"start\":15927},{\"attributes\":{\"n\":\"3.1\"},\"end\":16686,\"start\":16667},{\"attributes\":{\"n\":\"3.2\"},\"end\":17675,\"start\":17651},{\"attributes\":{\"n\":\"3.3\"},\"end\":18229,\"start\":18210},{\"attributes\":{\"n\":\"4\"},\"end\":18785,\"start\":18770},{\"attributes\":{\"n\":\"4.1\"},\"end\":18991,\"start\":18973},{\"attributes\":{\"n\":\"4.2\"},\"end\":20811,\"start\":20794},{\"attributes\":{\"n\":\"5\"},\"end\":21724,\"start\":21713},{\"attributes\":{\"n\":\"6\"},\"end\":23670,\"start\":23605},{\"attributes\":{\"n\":\"6.1\"},\"end\":23909,\"start\":23871},{\"attributes\":{\"n\":\"6.2\"},\"end\":27040,\"start\":27005},{\"attributes\":{\"n\":\"7\"},\"end\":29708,\"start\":29663},{\"attributes\":{\"n\":\"8\"},\"end\":33336,\"start\":33311},{\"end\":34553,\"start\":34543},{\"end\":35064,\"start\":35062},{\"end\":35233,\"start\":35232},{\"end\":37363,\"start\":37343},{\"end\":37556,\"start\":37528},{\"end\":41991,\"start\":41957},{\"end\":42319,\"start\":42303},{\"end\":43012,\"start\":42991},{\"end\":46533,\"start\":46531},{\"end\":47296,\"start\":47279},{\"end\":47410,\"start\":47400},{\"end\":47688,\"start\":47673},{\"end\":47875,\"start\":47858},{\"end\":48019,\"start\":48002},{\"end\":48713,\"start\":48703},{\"end\":48809,\"start\":48799},{\"end\":49340,\"start\":49310},{\"end\":50584,\"start\":50562},{\"end\":51473,\"start\":51462},{\"end\":52657,\"start\":52652},{\"end\":52771,\"start\":52765},{\"end\":53194,\"start\":53188},{\"end\":56881,\"start\":56872}]", "table": "[{\"end\":56870,\"start\":54192}]", "figure_caption": "[{\"end\":47398,\"start\":47298},{\"end\":47671,\"start\":47412},{\"end\":47856,\"start\":47691},{\"end\":48000,\"start\":47877},{\"end\":48203,\"start\":48022},{\"end\":48701,\"start\":48206},{\"end\":48797,\"start\":48715},{\"end\":49127,\"start\":48811},{\"end\":49308,\"start\":49130},{\"end\":50560,\"start\":49344},{\"end\":51460,\"start\":50589},{\"end\":51751,\"start\":51476},{\"end\":52614,\"start\":51754},{\"end\":52650,\"start\":52617},{\"end\":52763,\"start\":52659},{\"end\":53098,\"start\":52773},{\"end\":53186,\"start\":53101},{\"end\":54192,\"start\":53197},{\"end\":57234,\"start\":56883}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3669,\"start\":3661},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5056,\"start\":5048},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16938,\"start\":16930},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28858,\"start\":28850},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":32398,\"start\":32390},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":33079,\"start\":33067},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":33294,\"start\":33285},{\"end\":37415,\"start\":37407},{\"end\":40080,\"start\":40072},{\"end\":40934,\"start\":40926},{\"end\":41686,\"start\":41678},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":42128,\"start\":42119},{\"end\":42299,\"start\":42293}]", "bib_author_first_name": "[{\"end\":61554,\"start\":61550},{\"end\":61570,\"start\":61564},{\"end\":61582,\"start\":61576},{\"end\":61850,\"start\":61846},{\"end\":61866,\"start\":61862},{\"end\":61880,\"start\":61876},{\"end\":61894,\"start\":61890},{\"end\":61912,\"start\":61904},{\"end\":61930,\"start\":61922},{\"end\":62196,\"start\":62190},{\"end\":62210,\"start\":62205},{\"end\":62586,\"start\":62577},{\"end\":62729,\"start\":62723},{\"end\":62741,\"start\":62736},{\"end\":62752,\"start\":62748},{\"end\":63054,\"start\":63045},{\"end\":63328,\"start\":63321},{\"end\":63337,\"start\":63336},{\"end\":63563,\"start\":63560},{\"end\":63584,\"start\":63580},{\"end\":63597,\"start\":63594},{\"end\":63612,\"start\":63606},{\"end\":63623,\"start\":63619},{\"end\":64128,\"start\":64122},{\"end\":64143,\"start\":64137},{\"end\":64180,\"start\":64176},{\"end\":64182,\"start\":64181},{\"end\":64194,\"start\":64188},{\"end\":64211,\"start\":64205},{\"end\":64229,\"start\":64222},{\"end\":64981,\"start\":64975},{\"end\":64994,\"start\":64990},{\"end\":65013,\"start\":65002},{\"end\":65358,\"start\":65352},{\"end\":65373,\"start\":65368},{\"end\":65389,\"start\":65382},{\"end\":65406,\"start\":65399},{\"end\":65887,\"start\":65882},{\"end\":65917,\"start\":65912},{\"end\":65937,\"start\":65930},{\"end\":65960,\"start\":65956},{\"end\":66485,\"start\":66478},{\"end\":66498,\"start\":66492},{\"end\":66510,\"start\":66506},{\"end\":66525,\"start\":66519},{\"end\":66878,\"start\":66871},{\"end\":66892,\"start\":66886},{\"end\":66907,\"start\":66900},{\"end\":66921,\"start\":66917},{\"end\":66939,\"start\":66932},{\"end\":67349,\"start\":67342},{\"end\":67360,\"start\":67359},{\"end\":67374,\"start\":67370},{\"end\":67391,\"start\":67385},{\"end\":67407,\"start\":67401},{\"end\":67940,\"start\":67935},{\"end\":67954,\"start\":67948},{\"end\":67966,\"start\":67961},{\"end\":68000,\"start\":67991},{\"end\":68380,\"start\":68373},{\"end\":68398,\"start\":68391},{\"end\":68417,\"start\":68407},{\"end\":68433,\"start\":68426},{\"end\":68452,\"start\":68449},{\"end\":68469,\"start\":68463},{\"end\":68486,\"start\":68482},{\"end\":68503,\"start\":68494},{\"end\":68514,\"start\":68509},{\"end\":68531,\"start\":68522},{\"end\":68946,\"start\":68939},{\"end\":68963,\"start\":68957},{\"end\":68974,\"start\":68970},{\"end\":68988,\"start\":68983},{\"end\":68998,\"start\":68995},{\"end\":69007,\"start\":69005},{\"end\":69009,\"start\":69008},{\"end\":69261,\"start\":69252},{\"end\":69577,\"start\":69568},{\"end\":69589,\"start\":69585},{\"end\":69605,\"start\":69597},{\"end\":69621,\"start\":69617},{\"end\":70216,\"start\":70209},{\"end\":70232,\"start\":70227},{\"end\":70249,\"start\":70241},{\"end\":70267,\"start\":70260},{\"end\":70284,\"start\":70277},{\"end\":70657,\"start\":70653},{\"end\":70674,\"start\":70665},{\"end\":70689,\"start\":70685},{\"end\":71027,\"start\":71024},{\"end\":71039,\"start\":71033},{\"end\":71053,\"start\":71049},{\"end\":71265,\"start\":71260},{\"end\":71276,\"start\":71270},{\"end\":71287,\"start\":71283},{\"end\":71687,\"start\":71683},{\"end\":71697,\"start\":71695},{\"end\":71707,\"start\":71702},{\"end\":71721,\"start\":71717},{\"end\":71738,\"start\":71731},{\"end\":72030,\"start\":72027},{\"end\":72046,\"start\":72038},{\"end\":72066,\"start\":72058},{\"end\":72082,\"start\":72076},{\"end\":72408,\"start\":72403},{\"end\":72421,\"start\":72416},{\"end\":72430,\"start\":72426},{\"end\":72443,\"start\":72436},{\"end\":72455,\"start\":72449},{\"end\":72471,\"start\":72468},{\"end\":72484,\"start\":72479}]", "bib_author_last_name": "[{\"end\":61562,\"start\":61555},{\"end\":61574,\"start\":61571},{\"end\":61587,\"start\":61583},{\"end\":61594,\"start\":61589},{\"end\":61860,\"start\":61851},{\"end\":61874,\"start\":61867},{\"end\":61888,\"start\":61881},{\"end\":61902,\"start\":61895},{\"end\":61920,\"start\":61913},{\"end\":61938,\"start\":61931},{\"end\":62203,\"start\":62197},{\"end\":62220,\"start\":62211},{\"end\":62592,\"start\":62587},{\"end\":62734,\"start\":62730},{\"end\":62746,\"start\":62742},{\"end\":62756,\"start\":62753},{\"end\":63067,\"start\":63055},{\"end\":63334,\"start\":63329},{\"end\":63343,\"start\":63338},{\"end\":63350,\"start\":63345},{\"end\":63578,\"start\":63564},{\"end\":63592,\"start\":63585},{\"end\":63604,\"start\":63598},{\"end\":63617,\"start\":63613},{\"end\":63627,\"start\":63624},{\"end\":64135,\"start\":64129},{\"end\":64174,\"start\":64144},{\"end\":64186,\"start\":64183},{\"end\":64203,\"start\":64195},{\"end\":64220,\"start\":64212},{\"end\":64239,\"start\":64230},{\"end\":64252,\"start\":64241},{\"end\":64988,\"start\":64982},{\"end\":65000,\"start\":64995},{\"end\":65024,\"start\":65014},{\"end\":65366,\"start\":65359},{\"end\":65380,\"start\":65374},{\"end\":65397,\"start\":65390},{\"end\":65412,\"start\":65407},{\"end\":65910,\"start\":65888},{\"end\":65928,\"start\":65918},{\"end\":65944,\"start\":65938},{\"end\":65954,\"start\":65946},{\"end\":65968,\"start\":65961},{\"end\":65977,\"start\":65970},{\"end\":66490,\"start\":66486},{\"end\":66504,\"start\":66499},{\"end\":66517,\"start\":66511},{\"end\":66533,\"start\":66526},{\"end\":66884,\"start\":66879},{\"end\":66898,\"start\":66893},{\"end\":66915,\"start\":66908},{\"end\":66930,\"start\":66922},{\"end\":66945,\"start\":66940},{\"end\":67357,\"start\":67350},{\"end\":67368,\"start\":67361},{\"end\":67383,\"start\":67375},{\"end\":67399,\"start\":67392},{\"end\":67419,\"start\":67408},{\"end\":67439,\"start\":67421},{\"end\":67946,\"start\":67941},{\"end\":67959,\"start\":67955},{\"end\":67989,\"start\":67967},{\"end\":68007,\"start\":68001},{\"end\":68389,\"start\":68381},{\"end\":68405,\"start\":68399},{\"end\":68424,\"start\":68418},{\"end\":68447,\"start\":68434},{\"end\":68461,\"start\":68453},{\"end\":68480,\"start\":68470},{\"end\":68492,\"start\":68487},{\"end\":68507,\"start\":68504},{\"end\":68520,\"start\":68515},{\"end\":68536,\"start\":68532},{\"end\":68955,\"start\":68947},{\"end\":68968,\"start\":68964},{\"end\":68981,\"start\":68975},{\"end\":68993,\"start\":68989},{\"end\":69003,\"start\":68999},{\"end\":69013,\"start\":69010},{\"end\":69272,\"start\":69262},{\"end\":69583,\"start\":69578},{\"end\":69595,\"start\":69590},{\"end\":69615,\"start\":69606},{\"end\":69629,\"start\":69622},{\"end\":70225,\"start\":70217},{\"end\":70239,\"start\":70233},{\"end\":70258,\"start\":70250},{\"end\":70275,\"start\":70268},{\"end\":70299,\"start\":70285},{\"end\":70309,\"start\":70301},{\"end\":70663,\"start\":70658},{\"end\":70683,\"start\":70675},{\"end\":70693,\"start\":70690},{\"end\":71031,\"start\":71028},{\"end\":71047,\"start\":71040},{\"end\":71059,\"start\":71054},{\"end\":71268,\"start\":71266},{\"end\":71281,\"start\":71277},{\"end\":71291,\"start\":71288},{\"end\":71693,\"start\":71688},{\"end\":71700,\"start\":71698},{\"end\":71715,\"start\":71708},{\"end\":71729,\"start\":71722},{\"end\":71744,\"start\":71739},{\"end\":72036,\"start\":72031},{\"end\":72056,\"start\":72047},{\"end\":72074,\"start\":72067},{\"end\":72087,\"start\":72083},{\"end\":72414,\"start\":72409},{\"end\":72424,\"start\":72422},{\"end\":72434,\"start\":72431},{\"end\":72447,\"start\":72444},{\"end\":72466,\"start\":72456},{\"end\":72477,\"start\":72472},{\"end\":72490,\"start\":72485}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2206.12796\",\"id\":\"b0\"},\"end\":61799,\"start\":61465},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":8577357},\"end\":62138,\"start\":61801},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":218538212},\"end\":62529,\"start\":62140},{\"attributes\":{\"id\":\"b3\"},\"end\":62662,\"start\":62531},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":235390366},\"end\":62950,\"start\":62664},{\"attributes\":{\"id\":\"b5\"},\"end\":63224,\"start\":62952},{\"attributes\":{\"id\":\"b6\"},\"end\":63504,\"start\":63226},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3228123},\"end\":64062,\"start\":63506},{\"attributes\":{\"doi\":\"10.1145/3306618.3314236\",\"id\":\"b8\",\"matched_paper_id\":173170789},\"end\":64925,\"start\":64064},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":202770212},\"end\":65299,\"start\":64927},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":202712549},\"end\":65789,\"start\":65301},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":211041349},\"end\":66420,\"start\":65791},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":236975962},\"end\":66841,\"start\":66422},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":13496699},\"end\":67298,\"start\":66843},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2077168},\"end\":67893,\"start\":67300},{\"attributes\":{\"id\":\"b15\"},\"end\":68285,\"start\":67895},{\"attributes\":{\"doi\":\"arXiv:2202.01034\",\"id\":\"b16\"},\"end\":68883,\"start\":68287},{\"attributes\":{\"id\":\"b17\"},\"end\":69155,\"start\":68885},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":9238949},\"end\":69508,\"start\":69157},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":231704310},\"end\":70146,\"start\":69510},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":44112860},\"end\":70605,\"start\":70148},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":51734899},\"end\":71022,\"start\":70607},{\"attributes\":{\"doi\":\"arXiv:1901.08568\",\"id\":\"b22\"},\"end\":71218,\"start\":71024},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b23\",\"matched_paper_id\":249712251},\"end\":71650,\"start\":71220},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":490669},\"end\":71971,\"start\":71652},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":17069732},\"end\":72334,\"start\":71973},{\"attributes\":{\"id\":\"b26\"},\"end\":72657,\"start\":72336}]", "bib_title": "[{\"end\":61844,\"start\":61801},{\"end\":62188,\"start\":62140},{\"end\":62721,\"start\":62664},{\"end\":63558,\"start\":63506},{\"end\":64120,\"start\":64064},{\"end\":64973,\"start\":64927},{\"end\":65350,\"start\":65301},{\"end\":65880,\"start\":65791},{\"end\":66476,\"start\":66422},{\"end\":66869,\"start\":66843},{\"end\":67340,\"start\":67300},{\"end\":69250,\"start\":69157},{\"end\":69566,\"start\":69510},{\"end\":70207,\"start\":70148},{\"end\":70651,\"start\":70607},{\"end\":71258,\"start\":71220},{\"end\":71681,\"start\":71652},{\"end\":72025,\"start\":71973}]", "bib_author": "[{\"end\":61564,\"start\":61550},{\"end\":61576,\"start\":61564},{\"end\":61589,\"start\":61576},{\"end\":61596,\"start\":61589},{\"end\":61862,\"start\":61846},{\"end\":61876,\"start\":61862},{\"end\":61890,\"start\":61876},{\"end\":61904,\"start\":61890},{\"end\":61922,\"start\":61904},{\"end\":61940,\"start\":61922},{\"end\":62205,\"start\":62190},{\"end\":62222,\"start\":62205},{\"end\":62594,\"start\":62577},{\"end\":62736,\"start\":62723},{\"end\":62748,\"start\":62736},{\"end\":62758,\"start\":62748},{\"end\":63069,\"start\":63045},{\"end\":63336,\"start\":63321},{\"end\":63345,\"start\":63336},{\"end\":63352,\"start\":63345},{\"end\":63580,\"start\":63560},{\"end\":63594,\"start\":63580},{\"end\":63606,\"start\":63594},{\"end\":63619,\"start\":63606},{\"end\":63629,\"start\":63619},{\"end\":64137,\"start\":64122},{\"end\":64176,\"start\":64137},{\"end\":64188,\"start\":64176},{\"end\":64205,\"start\":64188},{\"end\":64222,\"start\":64205},{\"end\":64241,\"start\":64222},{\"end\":64254,\"start\":64241},{\"end\":64990,\"start\":64975},{\"end\":65002,\"start\":64990},{\"end\":65026,\"start\":65002},{\"end\":65368,\"start\":65352},{\"end\":65382,\"start\":65368},{\"end\":65399,\"start\":65382},{\"end\":65414,\"start\":65399},{\"end\":65912,\"start\":65882},{\"end\":65930,\"start\":65912},{\"end\":65946,\"start\":65930},{\"end\":65956,\"start\":65946},{\"end\":65970,\"start\":65956},{\"end\":65979,\"start\":65970},{\"end\":66492,\"start\":66478},{\"end\":66506,\"start\":66492},{\"end\":66519,\"start\":66506},{\"end\":66535,\"start\":66519},{\"end\":66886,\"start\":66871},{\"end\":66900,\"start\":66886},{\"end\":66917,\"start\":66900},{\"end\":66932,\"start\":66917},{\"end\":66947,\"start\":66932},{\"end\":67359,\"start\":67342},{\"end\":67370,\"start\":67359},{\"end\":67385,\"start\":67370},{\"end\":67401,\"start\":67385},{\"end\":67421,\"start\":67401},{\"end\":67441,\"start\":67421},{\"end\":67948,\"start\":67935},{\"end\":67961,\"start\":67948},{\"end\":67991,\"start\":67961},{\"end\":68009,\"start\":67991},{\"end\":68391,\"start\":68373},{\"end\":68407,\"start\":68391},{\"end\":68426,\"start\":68407},{\"end\":68449,\"start\":68426},{\"end\":68463,\"start\":68449},{\"end\":68482,\"start\":68463},{\"end\":68494,\"start\":68482},{\"end\":68509,\"start\":68494},{\"end\":68522,\"start\":68509},{\"end\":68538,\"start\":68522},{\"end\":68957,\"start\":68939},{\"end\":68970,\"start\":68957},{\"end\":68983,\"start\":68970},{\"end\":68995,\"start\":68983},{\"end\":69005,\"start\":68995},{\"end\":69015,\"start\":69005},{\"end\":69274,\"start\":69252},{\"end\":69585,\"start\":69568},{\"end\":69597,\"start\":69585},{\"end\":69617,\"start\":69597},{\"end\":69631,\"start\":69617},{\"end\":70227,\"start\":70209},{\"end\":70241,\"start\":70227},{\"end\":70260,\"start\":70241},{\"end\":70277,\"start\":70260},{\"end\":70301,\"start\":70277},{\"end\":70311,\"start\":70301},{\"end\":70665,\"start\":70653},{\"end\":70685,\"start\":70665},{\"end\":70695,\"start\":70685},{\"end\":71033,\"start\":71024},{\"end\":71049,\"start\":71033},{\"end\":71061,\"start\":71049},{\"end\":71270,\"start\":71260},{\"end\":71283,\"start\":71270},{\"end\":71293,\"start\":71283},{\"end\":71695,\"start\":71683},{\"end\":71702,\"start\":71695},{\"end\":71717,\"start\":71702},{\"end\":71731,\"start\":71717},{\"end\":71746,\"start\":71731},{\"end\":72038,\"start\":72027},{\"end\":72058,\"start\":72038},{\"end\":72076,\"start\":72058},{\"end\":72089,\"start\":72076},{\"end\":72416,\"start\":72403},{\"end\":72426,\"start\":72416},{\"end\":72436,\"start\":72426},{\"end\":72449,\"start\":72436},{\"end\":72468,\"start\":72449},{\"end\":72479,\"start\":72468},{\"end\":72492,\"start\":72479}]", "bib_venue": "[{\"end\":61548,\"start\":61465},{\"end\":61956,\"start\":61940},{\"end\":62292,\"start\":62222},{\"end\":62575,\"start\":62531},{\"end\":62798,\"start\":62758},{\"end\":63043,\"start\":62952},{\"end\":63319,\"start\":63226},{\"end\":63727,\"start\":63629},{\"end\":64357,\"start\":64277},{\"end\":65075,\"start\":65026},{\"end\":65501,\"start\":65414},{\"end\":66059,\"start\":65979},{\"end\":66599,\"start\":66535},{\"end\":67024,\"start\":66947},{\"end\":67539,\"start\":67441},{\"end\":67933,\"start\":67895},{\"end\":68371,\"start\":68287},{\"end\":68937,\"start\":68885},{\"end\":69319,\"start\":69274},{\"end\":69726,\"start\":69631},{\"end\":70361,\"start\":70311},{\"end\":70770,\"start\":70695},{\"end\":71099,\"start\":71077},{\"end\":71365,\"start\":71297},{\"end\":71790,\"start\":71746},{\"end\":72133,\"start\":72089},{\"end\":72401,\"start\":72336},{\"end\":62349,\"start\":62294},{\"end\":63812,\"start\":63729},{\"end\":64441,\"start\":64359},{\"end\":65575,\"start\":65503},{\"end\":66126,\"start\":66061},{\"end\":67088,\"start\":67026},{\"end\":67624,\"start\":67541},{\"end\":69825,\"start\":69728},{\"end\":70832,\"start\":70772},{\"end\":71420,\"start\":71367}]"}}}, "year": 2023, "month": 12, "day": 17}