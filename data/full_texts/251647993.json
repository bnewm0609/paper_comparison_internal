{"id": 251647993, "updated": "2022-08-23 17:27:47.591", "metadata": {"title": "A CTOR -C RITIC P OLICY O PTIMIZATION IN A L ARGE S CALE I MPERFECT -I NFORMATION G AME", "authors": "[{\"first\":\"Haobo\",\"last\":\"Fu\",\"middle\":[]},{\"first\":\"Weiming\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Shuang\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Yijia\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Tao\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Kai\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Junliang\",\"last\":\"Xing\",\"middle\":[]},{\"first\":\"Bin\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Bo\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Qiang\",\"last\":\"Fu\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Yang\",\"middle\":[]}]", "venue": "ICLR", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "The deep policy gradient method has demonstrated promising results in many large-scale games, where the agent learns purely from its own experience. Yet, policy gradient methods with self-play suffer convergence problems to a Nash Equilibrium (NE) in multi-agent situations. Counterfactual regret minimization (CFR) has a convergence guarantee to a NE in 2-player zero-sum games, but it usually needs domain-speci\ufb01c abstractions to deal with large-scale games. Inheriting merits from both methods, in this paper we extend the actor-critic algorithm framework in deep reinforcement learning to tackle a large-scale 2-player zero-sum imperfect-information game, 1-on-1 Mahjong, whose information set size and game length are much larger than poker. The proposed algorithm, named Actor-Critic Hedge (ACH), modi\ufb01es the policy optimization objective from originally maximizing the discounted returns to minimizing a type of weighted cumulative counterfactual regret. This modi\ufb01cation is achieved by approximating the regret via a deep neural network and minimizing the regret via generating self-play policies using Hedge. ACH is theoretically justi\ufb01ed as it is derived from a neural-based weighted CFR, for which we prove the convergence to a NE under certain conditions. Experimental results on the proposed 1-on-1 Mahjong benchmark and benchmarks from the literature demonstrate that ACH outperforms related state-of-the-art methods. Also, the agent obtained", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/FuLWWYLXLMF022", "doi": null}}, "content": {"source": {"pdf_hash": "d018602fe334d9aae2e16d0961e900ef6887d1c0", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "9646b8cef6004cf4e6981da3315d2191af805282", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d018602fe334d9aae2e16d0961e900ef6887d1c0.txt", "contents": "\nACTOR-CRITIC POLICY OPTIMIZATION IN A LARGE- SCALE IMPERFECT-INFORMATION GAME\n\n\nHaobo Fu \nTencent AI Lab\nShenzhenChina\n\nWeiming Liu \nUniversity of Science and Technology of China\nHefeiChina\n\nShuang Wu \nTencent AI Lab\nShenzhenChina\n\nYijia Wang \nPeking University\nBeijingChina\n\nTao Yang \nTencent AI Lab\nShenzhenChina\n\nKai Li \nJunliang Xing \nBin Li \nUniversity of Science and Technology of China\nHefeiChina\n\nBo Ma \nTencent AI Lab\nShenzhenChina\n\nQiang Fu \nTencent AI Lab\nShenzhenChina\n\nWei Yang \nTencent AI Lab\nShenzhenChina\n\n\nInstitute of Automation\nChinese Academy of Sciences\nBeijingChina\n\n\nSchool of Artificial Intelligence\nUniversity of Chinese Academy of Sciences\nBeijingChina\n\nACTOR-CRITIC POLICY OPTIMIZATION IN A LARGE- SCALE IMPERFECT-INFORMATION GAME\nPublished as a conference paper at ICLR 2022\nThe deep policy gradient method has demonstrated promising results in many largescale games, where the agent learns purely from its own experience. Yet, policy gradient methods with self-play suffer convergence problems to a Nash Equilibrium (NE) in multi-agent situations. Counterfactual regret minimization (CFR) has a convergence guarantee to a NE in 2-player zero-sum games, but it usually needs domain-specific abstractions to deal with large-scale games. Inheriting merits from both methods, in this paper we extend the actor-critic algorithm framework in deep reinforcement learning to tackle a large-scale 2-player zero-sum imperfectinformation game, 1-on-1 Mahjong, whose information set size and game length are much larger than poker. The proposed algorithm, named Actor-Critic Hedge (ACH), modifies the policy optimization objective from originally maximizing the discounted returns to minimizing a type of weighted cumulative counterfactual regret. This modification is achieved by approximating the regret via a deep neural network and minimizing the regret via generating self-play policies using Hedge. ACH is theoretically justified as it is derived from a neural-based weighted CFR, for which we prove the convergence to a NE under certain conditions. Experimental results on the proposed 1-on-1 Mahjong benchmark and benchmarks from the literature demonstrate that ACH outperforms related state-of-the-art methods. Also, the agent obtained by ACH defeats a human champion in 1-on-1 Mahjong.\n\nINTRODUCTION\n\nPolicy gradient methods using deep neural networks as policy and value approximators have been successfully applied to many large-scale games (Berner et al., 2019;Vinyals et al., 2019;Ye et al., 2020). Usually, a score function representing the discounted returns is maximized by the policy, i.e., the actor. In the meantime, a value function, known as the critic, is learned to guide the directions and magnitudes of the policy gradients. This type of actor-critic methods are efficiently scalable with regard to the game size and the amount of computational resources. However, as pointed out in Srinivasan et al. (2018) and Hennes et al. (2020), policy gradient methods with self-play have no convergence guarantee to optimal solutions in competitive Imperfect-Information Games (IIGs). The main reason is that the policy gradient theorem (Sutton et al., 1999) is established within the single agent situation, where the environment is Markovian. However, learning becomes non-stationary and non-Markovian when multiple agents learn simultaneously in a competitive environment.\n\nAn optimal solution to a 2-player zero-sum IIG usually refers to a Nash Equilibrium (NE), where no player could improve by unilaterally deviating to a different policy. Tremendous progress in computing NE solutions has been made by a family of tabular methods: Counterfactual regret minimization (CFR) (Zinkevich et al., 2008). CFR is a type of iterative self-play algorithm based on regret minimization, and it guarantees to converge to a NE with regard to the average policy in 2-player zero-sum IIGs. A perfect game model is required in CFR to sample many if not all actions from a state. To handle large-scale IIGs with CFR, abstractions (applied to either the action space or the state space) are usually employed to reduce the game to a manageable size (Morav\u010d\u00edk et al., 2017;Brown & Sandholm, 2018; 2019) 1 . However, abstractions are domain specific (Waugh et al., 2009;Johanson et al., 2013;Ganzfried & Sandholm, 2014). More importantly, some large-scale IIGs are inherently difficult to be abstracted, such as the game of Mahjong (Li et al., 2020b).\n\nIn this paper, we investigate a large-scale IIG, i.e., 2-player (1-on-1) zero-sum Mahjong, whose information set size and game length are much larger than poker 2 . Li et al. (2020b) has recently developed a strong 4-player Mahjong agent based on supervised learning and traditional Reinforcement Learning (RL). In comparison, we study 1-on-1 Mahjong from a game-theoretic perspective, i.e., aiming for a NE. We are interested in methods using only trajectory samples to learn, as it is infeasible to consistently sample multiple actions for each state in large-scale IIGs with long episodes. We employ deep neural networks to generalize across states, since the state abstraction in 1-on-1 Mahjong is inherently difficult, as explained in the Appendix A.2. We make the following contributions.\n\n\u2022 Inheriting the scalability of deep RL methods and the convergence property of CFR, we develop a new actor-critic algorithm, named Actor-Critic Hedge (ACH), for approaching a NE in large-scale 2-player zero-sum IIGs. ACH employs a deep neural network to approximate a type of weighted cumulative counterfactual regret. In the meantime, ACH minimizes the regret via generating self-play policies using Hedge (Freund & Schapire, 1997).\n\n\u2022 We introduce a Neural-based Weighted CFR (NW-CFR), of which ACH is a practical implementation. We prove that the exploitability of the average policy in NW-CFR decreases at the rate of O(T \u22121/2 ) under certain conditions, where T denotes the number of iterations in NW-CFR.\n\n\u2022 To facilitate research on large-scale 2-player zero-sum IIGs, we propose a 1-on-1 Mahjong benchmark. The corresponding game enjoys a large population in online games.\n\n\u2022 We build a 1-on-1 Mahjong agent, named JueJong, based on ACH. In an initial evaluation against human players including a Mahjong champion 3 , JueJong demonstrates superior performance.\n\n\nNOTATIONS AND BACKGROUND\n\n\nIMPERFECT-INFORMATION GAMES AND NASH EQUILIBRIUM\n\nAn IIG is usually described in an extensive-form game tree. A node (history) h \u2208 H in the tree represents all information of the current situation. For each history h, there is a player p \u2208 P or a chance player c that should act at h. Define P : H \u2192 P \u222a {c}. When P (h) \u2208 P, the player P (h) has to take an action a \u2208 A(h), and A(h) is the set of legal actions in h. The chance player is responsible for taking actions for random events. The set of terminal nodes is denoted by Z. For each player p \u2208 P, there is a payoff function defined on the set of terminal nodes, u p : Z \u2192 R. In this paper, we focus on 2-player zero-sum games, where P = {0, 1} and u 0 (z) + u 1 (z) = 0 for each z \u2208 Z.\n\nFor either player p, the set of histories H is partitioned into information sets (infosets). We denote the set of infosets for player p by I p and an infoset in I p by I p . Two histories h, h \u2208 H are in the same infoset if and only if h and h are indistinguishable from the perspective of player p. Hence, a player p's policy \u03c0 p is defined as a function that maps an infoset to a probability distribution over legal actions. We further define A(I p ) = A(h) and P (I p ) = P (h) for any h \u2208 I p . A policy profile \u03c0 is a tuple of policies (\u03c0 p , \u03c0 \u2212p ), where \u03c0 \u2212p represents the player p's opponent policy. The expected payoff for player p under \u03c0 is denoted by u p (\u03c0 p , \u03c0 \u2212p ). We use \u2206(I) to denote the range of payoffs reachable from a history h in infoset I. Let \u2206 = max I\u2208Ip,p\u2208P \u2206(I). f \u03c0 (h) denotes the joint probability of reaching h under \u03c0. f \u03c0 p (h) is the contribution of player p to f \u03c0 (h), and f \u03c0 \u2212p (h) is the contribution of the opponent and chance: f \u03c0 (h) = f \u03c0 p (h)f \u03c0 \u2212p (h). We focus on the perfect-recall setting, where each player recalls the sequence of their own infosets reached. We define f \u03c0 p (I p ) = f \u03c0 p (h), \u2200h \u2208 I p and f \u03c0 \u2212p (I p ) = h\u2208Ip f \u03c0 \u2212p (h). Hence, f \u03c0 (I p ) = h\u2208Ip f \u03c0 (h) = f \u03c0 p (I p )f \u03c0 \u2212p (I p ).\n\nA best response BR(\u03c0 \u2212p ) to \u03c0 \u2212p is a player p's policy that satisfies u p (BR(\u03c0 \u2212p ), \u03c0 \u2212p ) = max \u03c0 p u p (\u03c0 p , \u03c0 \u2212p ). A NE is a policy profile \u03c0 * , where each player plays a best response to the other: u p (\u03c0 * p , \u03c0 * \u2212p ) = max \u03c0 p u p (\u03c0 p , \u03c0 * \u2212p ), \u2200p \u2208 P. The exploitability of a player's policy, denoted by e(\u03c0 p ), measures the performance gap between \u03c0 p and a NE policy \u03c0 * p : e(\u03c0 p ) = u p (\u03c0 * p , \u03c0 * \u2212p ) \u2212 u p (\u03c0 p , BR(\u03c0 p )). The exploitability of \u03c0 is (\u03c0) = 1 |P| p\u2208P e(\u03c0 p ).\n\n\nREINFORCEMENT LEARNING AND POLICY GRADIENT METHODS\n\nRL usually assumes a Markov decision process, where the agent selects an action a i from the legal action set A(s i ) in state s i \u2208 S 4 at each time step i. The agent then receives a reward r i from the environment and transitions to a new state s i+1 . The objective in RL is to learn a policy that maximizes the expected discounted returns, i.e., the state value, starting from any state s:\nV \u03c0 (s) = E \u03c0 [ \u221e j=i \u03b3 j\u2212i r j |s i = s] = E \u03c0 [G]\n, with the discount factor \u03b3 \u2208 [0, 1). Many methods in RL belong to policy gradient methods, in which the parameters \u03b8 of policy \u03c0(a|s; \u03b8) are updated by performing gradient ascent directly on E \u03c0 \u03b8 [G]. One early example is the standard REINFORCE algorithm (Williams, 1992) that updates \u03b8 in the direction \u2207 \u03b8 log \u03c0(a|s; \u03b8)G, which is an unbiased estimate of \u2207 \u03b8 E \u03c0 \u03b8 [G]. To further reduce the variance of the gradient, an actionindependent baseline is often subtracted from G: \u2207 \u03b8 log \u03c0(a|s; \u03b8)[G \u2212 B(s)]. A recent policy gradient method, named Advantage Actor-Critic (A2C) (Mnih et al., 2016), learns a parameterized value function as the baseline: B(s) = V (s; w), where w often shares some parameters with \u03b8. Moreover, the value G \u2212 B(s) is replaced in A2C with the estimated advantage of action a in state s: A(s, a) = Q(s, a) \u2212 V (s). The Q value Q(s, a) is usually estimated using sampled rewards and predicted values of future states. A2C updates the parameters \u03b8 and w in a synchronous manner.\n\nIn an asynchronous training environment, the behavioral policy is usually different from the learning policy. Proximal Policy Optimization (PPO) (Schulman et al., 2017) takes this discrepancy into account by multiplying A2C gradient with an importance ratio r(a|s; \u03b8) = \u03c0(a|s; \u03b8)/\u03c0(a|s; \u03b8 old ), which results in r(a|s; \u03b8)\u2207 \u03b8 log \u03c0(a|s; \u03b8)A(s, a) = \u2207 \u03b8 r(a|s; \u03b8)A(s, a). Furthermore, PPO constrains the KL divergence between the learning policy \u03c0 \u03b8 and the old behavioral policy \u03c0 \u03b8 old by clipping the ratio to a small interval around 1.0.\n\n\nCOUNTERFACTUAL REGRET MINIMIZATION\n\nCFR is an iterative algorithm that minimizes the total regret of policy by minimizing the cumulative counterfactual regret in every state (infoset). The cumulative counterfactual regret of an action a in state s is defined as R c t (s, a) = t k=1 r c k (s, a), where r c k (s, a) denotes the instantaneous counterfactual regret of action a in state s at iteration k. r c t (s, a) is equal to f \u03c0t \u2212p (s)A \u03c0t (s, a) (as proven in Srinivasan et al. (2018)), where A \u03c0t (s, a) is the advantage function of player p = P (s) at state s as defined in traditional RL. Intuitively, r c t (s, a) represents how regretful the player p is that he does not select the action a in state s at iteration t. The term f \u03c0t \u2212p (s) = h\u2208s f \u03c0t \u2212p (h) is needed here to reflect the fact that reaching state s is also controlled by the opponent \u2212p and chance. To minimize the regret, a regret minimizer is utilized in each state at each iteration, generating a series of local policies \u03c0 1 (s), . . . , \u03c0 t (s). Once \u03c0 t is obtained, r c t (s, a) is generated by a game tree traversing using \u03c0 t , and R c t (s, a) is updated accordingly: R c t (s, a) = R c t\u22121 (s, a) + r c t (s, a).\n\nFor either player p, define the total regret as R T = max \u03c0 p T t=1 u p (\u03c0 p , \u03c0 \u2212p,t ) \u2212 u p (\u03c0 p,t , \u03c0 \u2212p,t ) . It is proven in Zinkevich et al. (2008) \nthat R T \u2264 s\u2208S [R c T (s)] + , where [\u00b7] + = max{\u00b7, 0} and R c T (s) = max a\u2208A(s) R c T (s, a)\n. As a result, the total regret can be minimized by minimizing the cumulative counterfactual regret at each state. Moreover, the average policy\u03c0 T (a|s) = t\u2208T f \u03c0t p (s)\u03c0 t (a|s)/ t\u2208T f \u03c0t p (s) for both players converges to a NE if R T for both players grows sub-linearly with T (Zinkevich et al., 2008). There are two commonly used regret minimizers: Regret Matching (RM) (Hart & Mas-Colell, 2000) and Hedge (Cesa-Bianchi & Lugosi, 2006). In RM, a player selects an action with probability in proportion to its positive cumulative counterfactual regret. In Hedge, the policy \u03c0 t+1 (a|s) is decided according to:\n\u03c0 t+1 (a|s) = e \u03b7(s)R c t (s,a)\na e \u03b7(s)R c t (s,a ) .\n\n(1)\n\nIf the player plays according to Hedge in state s and \u03b7(s) = 8 log(|A(s)|)/(\u2206 2 (s)T ), R c T (s) \u2264 \u2206(s) log(|A(s)|)T /2 (Cesa-Bianchi & Lugosi, 2006). In other words, the total regret grows sub-linearly with T , and therefore the average policy\u03c0 T (a|s) for both players converges to a NE.\n\n\nTHE MOTIVATION OF ACTOR-CRITIC HEDGE\n\nIn this section, we motivate ACH by introducing a new neural-based CFR algorithm, NW-CFR, which employs a neural network to generalize across states and relies on only trajectory samples for training.\n\nThe key idea in NW-CFR is that a neural network (called the policy net) is used to approximate the expectation of the sum of sampled advantages R a t (s, a) := E[ t k=1\u00c3\n\n\u03c0 k (s, a)].\u00c3 \u03c0 k (s, a) is set to A \u03c0 k (s, a) if s is visited at iteration k (given that only one trajectory is sampled at each iteration) and 0 otherwise. As a result, the expectation E[\u00c3 \u03c0 k (s, a)] depends on both the advantage A \u03c0 k (s, a) and the sampling policy \u00b5 k at iteration k.\n\nAt the beginning of iteration t, suppose R a t\u22121 (s, a) is well approximated by the output y(a|s; \u03b8 t\u22121 ) of the policy net, parameterized as \u03b8 t\u22121 in NW-CFR. The policy \u03c0 t for iteration t is first obtained via Hedge, i.e., softmaxing on \u03b7(s)y(a|s; \u03b8 t\u22121 ). The reason we use Hedge instead of RM is that softmaxing is shift-invariant, which may be more robust to the function approximation error compared to the threshold operation in RM. M trajectories are then sampled into a buffer B v via self-play using the policy profile \u03c0 t = (\u03c0 p,t , \u03c0 \u2212p,t ). Those samples in B v are used to train the action value net, parameterized as \u03c9, by minimizing the squared loss: 1 2 [Q(s, a; \u03c9) \u2212 G] 2 , where G is the sampled return. Afterwards, another M trajectories are sampled into a buffer B \u03c0 via self-play using another sampling policy profile \u00b5 t = (\u00b5 p,t , \u03c0 \u2212p,t ). We use an additional behavioral policy \u00b5 p,t for the player p for more flexibility in the convergence (more details are provided in the next section). The policy \u03b8 is then optimized according to the loss function L \u03c0 = s\u2208S L \u03c0 (s), where\nL \u03c0 (s) = a {y(a|s; \u03b8) \u2212 [y(a|s; \u03b8 t\u22121 ) + 1 M M i=1 1 s\u2208\u03c4i A \u03c0t (s, a)]} 2 if s \u2208 B \u03c0 , a {y(a|s; \u03b8) \u2212 [y(a|s; \u03b8 t\u22121 ) + 0]} 2 otherwise,(2)\nwhere \u03c4 i is the ith sampled trajectory in B \u03c0 . The next iteration of NW-CFR begins after the policy training finishes. The pseudocode of NW-CFR is given in Algorithm 1. Note that we present NW-CFR from the perspective of player p, and the same procedure applies to the player \u2212p. NW-CFR runs concurrently for both players and synchronizes at the beginning of each iteration.\n\nA potential benefit of training on the sampled advantage over the sampled instantaneous counterfactual regret (as done in ), Li et al. (2020a, and Steinberger et al. (2020)) is that the variance of the sampled advantage could be much lower. As r c k (s, a) = f \u03c0 k \u2212p (s)A \u03c0 k (s, a) (Srinivasan et al., 2018), the sampled instantaneous counterfactual regretr c\nk (s, a) is [f \u00b5 k (s)] \u22121 f \u03c0 k \u2212p (s)\u00c3 \u03c0 k (s, a) = [f \u00b5 k p (s)] \u22121\u00c3\u03c0 k (s, a), where f \u00b5 k p (s)\nis the probability of reaching s considering only the contribution of \u00b5 p,k . The larger variance (due to [f \u00b5 k p (s)] \u22121 ) ofr c k (s, a) may have a negative influence on the performance when function approximation is used with only trajectory samples. This influence is magnified in large-scale games, as [f \u00b5 k p (s)] \u22121 becomes large.\n\n\nTHEORETICAL PROPERTIES OF NW-CFR\n\nIn this section, we prove the convergence of NW-CFR to an approximate NE. To this end, we first deduce that the policy target in NW-CFR is R a t (s, a), which is a type of weighted cumulative counterfactual regret. We then define a new family of CFR algorithms: weighted CFR. Finally, we show that NW-CFR is equivalent to a type of weighted CFR with Hedge under certain conditions. The convergence properties of weighted CFR with Hedge and hence NW-CFR are proven accordingly.\n\nWe can rewrite the NW-CFR policy loss (in Equation 2) at iteration t as L \u03c0 = s\u2208S a {y(a|s; \u03b8) \u2212 [y(a|s; \u03b8 t\u22121 ) + 1 M M i=1 1 s\u2208\u03c4i A \u03c0t (s, a)]} 2 . In other words, the target of y(a|s; \u03b8) at iteration t is Algorithm 1: Neural-based Weighted CFR Function Neural-based Weighted CFR(p, T, M, \u03b8, \u03c9): \u03c9 0 \u2190 \u03c9, \u03b8 0 \u2190 \u03b8. for t \u2190 1 to T do # Obtain the policy: \u03c0 t \u2190 Softmax(\u03b7(s)y(a|s; \u03b8 t\u22121 )). # Train the action value net:\nReset B v \u2190 \u2205. for i \u2190 1 to M do \u03c4 i \u223c SelfPlay(\u03c0 p,t , \u03c0 \u2212p,t ), B v \u2190 B v \u222a \u03c4 i Train \u03c9 on the loss E (s,a,G)\u223cBv { 1 2 [Q(s, a; \u03c9) \u2212 G] 2 } \u03c9 t \u2190 \u03c9 # Train the policy net: Reset B \u03c0 \u2190 \u2205. for i \u2190 1 to M do \u03c4 i \u223c SelfPlay(\u00b5 p,t , \u03c0 \u2212p,t ), B \u03c0 \u2190 B \u03c0 \u222a \u03c4 i Estimate A \u03c0t (s, a), \u2200a \u2208 A(s), \u2200s \u2208 B \u03c0 as Q(s, a; \u03c9 t ) \u2212 b \u03c0 t (b|s)Q(s, b; \u03c9 t ). Sum aggregate advantage A \u03c0t (s, a) in B \u03c0 by state s. Train \u03b8 on the loss E s\u223cS {L \u03c0 (s)}, where L \u03c0 (s) is defined in Equation 2. \u03b8 t \u2190 \u03b8. # Save \u03b8 t to a policy buffer. y(a|s; \u03b8 t\u22121 ) + 1 M M i=1 1 s\u2208\u03c4i A \u03c0t (s, a), whose expectation is E \u03c4 k,i \u223c(\u00b5 p,k ,\u03c0 \u2212p,k ) t k=1 M i=1 1 M 1 s\u2208\u03c4 k,i A \u03c0 k (s, a) = E \u03c4 k \u223c(\u00b5 p,k ,\u03c0 \u2212p,k ) t k=1\u00c3 \u03c0 k (s, a) = R a t (s, a). Also, E \u03c4 k \u223c(\u00b5 p,k ,\u03c0 \u2212p,k ) 1 s\u2208\u03c4 k A \u03c0 k (s, a) = f \u00b5 k p (s)f \u03c0 k \u2212p (s)A \u03c0 k (s, a), where f \u00b5 k p (s)f \u03c0 k \u2212p (s)\nis the reaching probability of state s at iteration k. Therefore, R a t (s, a) = t k=1 f \u00b5 k p (s)f \u03c0 k \u2212p (s)A \u03c0 k (s, a). Since r c k (s, a) = f \u03c0 k \u2212p (s)A \u03c0 k (s, a) (Srinivasan et al., 2018), we have R a t (s, a) = t k=1 f \u00b5 k p (s)r c k (s, a). As a result, R a t (s, a) can be viewed as a type of weighted cumulative counterfactual regret that multiples each instantaneous counterfactual regret with f \u00b5 k p (s). Without loss of generality, we define a new family of CFR algorithms, weighted CFR, below. Afterwards, we present Theorem 1.\n\nDefinition 1. Weighted CFR follows the same procedure as the original CFR (Zinkevich et al., 2008), except that the instantaneous counterfactual regret r c t (s, a) is weighted by some weight w t (s), w t (s) > 0 and \u221e t=0 w t (s) = \u221e. The original CFR is a type of weighted CFR with w t (s) = 1.0. Theorem 1. NW-CFR is equivalent to a type of weighted CFR with Hedge when w t (s) = f \u00b5t p (s) > 0, given that enough trajectories are sampled and y(a|s; \u03b8 t ) is sufficiently close to R a t (s, a).\nFurther, if \u03b7(s) = 8 ln |A(s)|/{[w h (s)] 2 \u2206 2 (s)T } and w t (s) = f \u00b5t p (s) \u2208 [w l (s), w h (s)] \u2282 (0, 1], t = 1, .\n. . , T , the average policy 5 \u03c0 of the corresponding weighted CFR with Hedge and equivalently NW-CFR with \u03c0 p (a|s) = T t=1 f \u03c0t p (s)\u03c0 t (a|s) / T t=1 f \u03c0t p (s), \u2200p \u2208 P, has exploitability after T iterations, where\n\u2264 |S|\u2206 1 2T ln |A| + \u2206 s\u2208S w h (s) \u2212 w l (s) w h (s) .(3)\nIn Theorem 1, we proved that the exploitability of NW-CFR is bounded by Equation 3 when y(a|s; \u03b8 t ) is sufficiently close to R a t (s, a). There are two terms in the bound. The first term converges to zero at the rate of O(T \u22121/2 ), and the second term is O(1), which is weighted by the sum of the normalized range of the weights s\u2208S (w h (s) \u2212 w l (s))/w h (s). However, it is possible to reduce the second term to an arbitrarily small value via tightening the range of f \u00b5t p (s), which is experimentally demonstrated in the Appendix D.\n\nCorollary 1. If the behavioral policy \u00b5 p,t for each player p \u2208 P is constant across iterations, and f \u00b5t p (s) > 0, \u2200s \u2208 S, t > 0, NW-CFR is equivalent to CFR with Hedge when y(a|s; \u03b8 t ) is sufficiently close to R a t (s, a).\n\nAs shown in Corollary 1, when the behavioral policy \u00b5 p,t for each player p \u2208 P is time-invariant, i.e., w h (s) = f \u00b5t p (s) = w l (s), \u2200s \u2208 S, t > 0, the second term of in Equation 3 vanishes, and CFR with Hedge is recovered. All the proofs are given in the Appendix C.\n\n\nACH: A PRACTICAL IMPLEMENTATION OF NW-CFR\n\nWhen applying NW-CFR to large-scale problems, two practical issues need to be addressed:\n\nThe average policy. Theorem 1 and Corollary 1 state the convergence property of the average policy in NW-CFR. Yet, as pointed out in Srinivasan et al. (2018), Hennes et al. (2020), and Perolat et al. (2021, obtaining the average policy with deep neural nets in large-scale games is inherently difficult, due to either the computation or the memory demand. Alternatively, we could employ some additional technique to hopefully induce the current policy convergence towards a NE. Srinivasan et al. (2018) and Hennes et al. (2020) handled this by adding an entropy regularization to the current policy training, which is, to some extent, theoretically justified later in Perolat et al. (2021).\n\nTraining on states not sampled. Theoretically, in order to optimize Equation 2, we need to collect both sampled and non-sampled states. Optimizing with only sampled states makes y(a|s; \u03b8 t ) a biased estimation of R a t (s, a). Yet, collecting non-sampled states may be intractable in large-scale games (Li et al., 2020a) or in situations where a perfect environment model is not available.\n\nTo strike a balance between theoretical soundness and practical efficiency, we provide a practical implementation of NW-CFR, which is ACH. ACH adapts NW-CFR by training the current policy with an entropy regularization on only sampled states, without the calculation of the average policy. In order to utilize distributed clusters, ACH employs a framework of decoupled acting and learning (similar to IMPALA (Espeholt et al., 2018)), trains the network with mini-batches, and handles asynchronous training with the importance ratio clipping of PPO. The behavior policy \u00b5 p,t is set to \u03c0 p,t 6 in ACH. More details of ACH are presented in the Appendix E.\n\n\nRELATED WORK\n\nTo obviate the need of abstractions, various neural forms of CFR methods have been developed. An early work of this direction is regression CFR (Waugh et al., 2015), which calculates weights for a number of hand-crafted features to approximate the regret. Deep CFR ) is similar to regression CFR but employs a neural network to approximate the regret. Also, deep CFR traverses a part of the game tree using external sampling, in comparison to the full traversal of the game tree in regression CFR. When dealing with games with long episodes, a necessity may be that only trajectory samples are allowed. To improve the learning performance with only trajectory samples, DREAM (Steinberger et al., 2020) adapts deep CFR by using a learned Q-baseline, which is inspired by the variance reduction techniques in tabular MCCFR (Schmid et al., 2019;Davis et al., 2020). Another recent work using only trajectory samples is ARMAC (Gruslys et al., 2020). By replaying through past policies and using a history-based critic, ARMAC predicts conditional advantages, based on which the policy for each iteration is generated. Other popular neural network based methods, which learn from trajectory samples and are inspired by game-theoretic approaches other than CFR, include neural fictitious self-play (Heinrich & Silver, 2016), policy space response oracles (Lanctot et al., 2017), and exploitability descent (Lockhart et al., 2019), all of which require to compute an approximate best response at each iteration. Such computation may be prohibitive in large-scale games.\n\nThe most related methods to ACH are Regret Policy Gradient (RPG) (Srinivasan et al., 2018) and Neural Replicator Dynamics (NeuRD) (Hennes et al., 2020), both of which employ the actor-critic framework and thus have similar computation and memory complexities as ACH. RPG minimizes a loss that is an upper bound on the regret after threshold, and the corresponding policy gradient is\n\u2207 RP G \u03b8 (s) = \u2212 a \u2207 \u03b8 [Q(s, a; w) \u2212 b \u03c0(b|s; \u03b8)Q(s, b; w)] + .\nHowever, RPG requires an l 2 projection after every gradient step for the convergence to a NE, while such projection is not required in ACH. NeuRD is inspired by the replicator dynamics, a well studied model in evolutionary game theory (Gatti et al., 2013). The policy gradient in NeuRD is\n\u2207 N euRD \u03b8 (s) = a [\u2207 \u03b8 y(a|s; \u03b8)][Q(s, a; w) \u2212 b \u03c0(b|s)Q(s, b; w)]\n, where y(a|s; \u03b8) is the output of the policy net. There are important differences between ACH and NeuRD in how the algorithm is motivated and how the policy net at each iteration is optimized. Also, the convergence analysis is given only for the single-state all-actions tabular NeuRD (Hennes et al., 2020). Yet, we prove the convergence of NW-CFR, of which ACH is a practical implementation, in full extensive-form games.\n\n\nEXPERIMENTAL STUDIES\n\nWe firstly introduce a 1-on-1 Mahjong benchmark, on which we compare ACH with related stateof-the-art methods of similar computation complexity: PPO, RPG, and NeuRD. Since our goal is to approximate a NE, the standard and default performance metric, exploitability, is employed. We approximate a lower bound on the exploitability of an agent by training a best response against it as suggested in  and Steinberger et al. (2020), because traversing the full game tree to compute the exact exploitability is intractable in such a large-scale game as 1-on-1 Mahjong. As a complement, head-to-head performance of different methods on 1-on-1 Mahjong is also presented. Moreover, the agent obtained by ACH is evaluated against practised Mahjong human players. To further validate the performance of ACH in IIGs other than 1-on-1 Mahjong, experimental results on a non-trivial poker game, i.e., heads-up Flop Hold'em Poker (FHP)  are presented. Deep CFR with outcome sampling (OS-DCFR) and DREAM are added to enable a more thorough comparison. Additional results on smaller benchmarks from OpenSpiel (Lanctot et al., 2019) are given in the Appendix G. Note that results are reported for the current policy (of ACH, PPO, RPG, and NeuRD) and the average policy (of OS-DCFR and DREAM) respectively.\n\n\nA 2-PLAYER ZERO-SUM MAHJONG BENCHMARK\n\nMahjong is a tile-based game that is played world wide with many regional variations, such as Japanese Riichi Mahjong and Competition Mahjong. Like poker, Mahjong is an IIG and is full of strategy, chance, and calculation. To facilitate Mahjong research from a game-theoretic perspective, we propose a 2-player zero-sum Mahjong benchmark, whose game rules are similar to Competition Mahjong. The corresponding game, \"2-player Mahjong Master\", is played by humans in Tencent mobile games. A full description of the game rules is in the Appendix A.1. Apart from being the first benchmark for the 1-on-1 Mahjong game, our benchmark has a larger infoset size and a longer game length (the effects are explained in the Appendix A.3), compared with existing poker benchmarks (Lanctot et al., 2019). The infoset size (i.e., the number of distinct histories in an infoset) in 1-on-1 Mahjong is around 10 11 , compared to 10 3 in poker. This is due to the fact that only two private cards are invisible in poker, while there are 13 invisible tiles in 1-on-1 Mahjong. In addition, players can decide up to about 40 sequential actions in 1-on-1 Mahjong, whereas most 1-on-1 poker games end within 10 steps. More details about the 1-on-1 Mahjong benchmark are given in the Appendix A.\n\n7.2 RESULTS ON OUR 1-ON-1 MAHJONG BENCHMARK All methods run in an asynchronous training platform with overall 800 CPUs, 3200 GB memory, and 8 M40 GPUs in the Ubuntu 16.04 operating system. Each method shares the same neural network architecture, a full description of which is given in the Appendix B. We performed a mild hyper-parameter search on PPO and shared the best setting for all methods. The advantage value is estimated by the Generalized Advantage Estimator (GAE(\u03bb)) (Schulman et al., 2016) for all methods. An overview of the hyper-parameters is listed in the Appendix H.1.\n\nApproximate Lower Bound Exploitability. To approximate a lower bound on the exploitability of the agents obtained by each method, we train a best response against each agent. The agent of The training curves of each agent. The performance of an agent is evaluated by the average scores the agent wins against a common rule-based agent. Higher is better. We report the mean as solid curves and the range of the average scores across 5 independent runs as shaded regions.\n\neach method is selected at the 1e6th training step. Note that the agent is fixed as a part of the 1-on-1 Mahjong environment when training the best response. We train the best response using PPO with the same hyper-parameters that were used to train the PPO agent with self-play. During the training of the best response, we evaluate the best response every 500 training steps using 10, 000 head-to-head plays against each agent. According to the average scores each agent loses to its best response in Figure 1(a), we may conclude that ACH is significantly more difficult to exploit than other methods in the large-scale 1-on-1 Mahjong environment.\n\nHead-to-Head Evaluation. We then compare the head-to-head performance of PPO, RPG, NeuRD, and ACH in the 1-on-1 Mahjong environment. First, we compare the training process of each method by evaluating each agent every 500 training steps against a common rule-based agent 7 using 10, 000 head-to-head plays, the results of which are shown in Figure 1(b). As we can see, all methods beat the common rule-based agent significantly, while ACH has a clear advantage over other methods in terms of stability and final performance. The relatively slow convergence of RPG may due to the threshold operation on the advantage, which could reduce sample efficiency in large-scale IIGs. Second, the superior performance of ACH is further validated in head-to-head evaluations with other agents in Table 1, where all the agents are selected at the 1e6th training step. The agent of ACH wins all other agents by a significant margin.\n\nPPO RPG NeuRD RPG -0.21 \u00b1 0.05 --NeuRD -0.03 \u00b1 0.02 0.04 \u00b1 0.10 -ACH 0.39 \u00b1 0.02 0.66 \u00b1 0.05 0.41 \u00b1 0.07 Table 1: Mean (\u00b1 standard deviation) of the average winning scores of the row agents against the column agents. The statistics are estimated by 5 independent runs (resulting 5 different agents for each method). In each run, the average winning scores are obtained via 10, 000 head-to-head plays.\n\nHuman Evaluation. We evaluate the agent, selected at the 1e6th training step, of ACH against human players. First, the agent, named JueJong, is roughly evaluated by playing over 7, 700 games against 157 practiced Mahjong players, where JueJong won an average of 4.56 scores per game. Second, we select the top 4 out of 157 players according to their performances against JueJong and play JueJong against the four players for 200 games each. As shown in Figure 2(a), the average winning scores of JueJong oscillate in the first 120 games but all plateau above 0 afterwards. More importantly, we evaluate JueJong against the Mahjong champion Haihua Cheng for 1, 000 games, as shown in Figure 2(b). After playing 1, 000 games, JueJong won the champion by a score of 0.82 \u00b1 0.96 (mean \u00b1 standard deviation), with a p-value of 0.19 under one-tailed t-test. Hence, we may conclude that Haihua Cheng failed to exploit JueJong effectively within 1, 000 games.  \n\n\nRESULTS ON THE FHP BENCHMARK\n\nWe further evaluate ACH and compare it with OS-DCFR, DREAM, A2C 8 , RPG, and NeuRD on FHP. FHP is a simplified Heads-up Limit Texas Hold'em (HULH), which includes only the first two of the four bettings in HULH. It is a medium-sized game with over 10 12 nodes and 10 9 infosets. All the methods share the same neural network architecture proposed in . We perform a mild hyper-parameter search for ACH, A2C, RPG, and NeuRD. For OS-DCFR and DREAM, we follow the hyper-parameters presented in Steinberger et al. (2020). The exploitability is measured in the number of chips per game, where a big blind is 100 chips. All the hyper-parameters and the running environment are described in the Appendix H.2.\n\nAs shown in Figure 3, ACH performs competitively with OS-DCFR and slightly worse than DREAM on FHP in terms of exploitability per episodes generated. However, ACH is much more training efficient: ACH achieves an exploitability of 10 almost 100 times faster than DREAM and 1,000 times faster than OS-DCFR. Also, in comparison with methods of similar training complexity (A2C, RPG, and NeuRD), ACH converges significantly faster and achieves a lower exploitability.\n\n\nCONCLUSIONS\n\nIn this paper, we investigated the problem of adapting policy gradient methods in deep RL to tackle a large-scale IIG, i.e., 1-on-1 Mahjong. To this end, we developed a new model-free actor-critic algorithm, i.e., ACH, for approximating a NE in large-scale IIGs. ACH is memory and computation efficient, as it uses only trajectory samples at the current iteration and requires no computation of best response. ACH is theoretically justified as it is derived from a new neural-based CFR, i.e., NW-CFR, of which we proved the convergence to an approximate NE in 2-player zero-sum IIGs under certain conditions. The superior performance of ACH was validated on both our 1-on-1 Mahjong benchmark and other common benchmarks. Secondly, to facilitate research on large-scale IIGs, we proposed the first 1-on-1 zero-sum Mahjong benchmark, whose infoset size and game length are much larger than poker. Finally, using ACH we obtained the 1-on-1 Mahjong agent JueJong, which has demonstrated stronger performance against the Mahjong champion Haihua Cheng.\n\n\nACKNOWLEDGEMENT\n\nWe thank the Mahjong champion Haihua Cheng for his efforts in this work. We appreciate the support from Tencent Mahjong (https://majiang.qq.com). We are grateful to Tencent AI Arena (https://aiarena.tencent.com) for providing the powerful computing capability to the experiments on 1-on-1 Mahjong.\n\n\nREPRODUCIBILITY STATEMENT\n\nThe experiments on 1-on-1 Mahjong were run in a large cluster of thousands of machines, on which we have developed an efficient actor-learner training platform, similar to IMPALA (Espeholt et al., 2018). The code of the platform is not released currently but is planned to be open sourced in the near future. \n\n\nA INTRODUCTION OF THE 1-ON-1 MAHJONG BENCHMARK\n\nMahjong is a tile-based game that is played world wide with many regional variations, such as Japanese Riichi Mahjong and Competition Mahjong. Like poker, Mahjong is an IIG and is full of strategy, chance, and calculation. In this paper, we investigate a 1-on-1 Mahjong game, whose information set size is much larger than poker, as shown in Figure 4. The game rules of 1-on-1 Mahjong are similar to Competition Mahjong. The corresponding game, \"2-player Mahjong Master\", is played by humans in Tencent mobile games (https://majiang.qq.com). There are 24 unique tiles and 72 tiles in total in the 1-on-1 Mahjong game, as shown in Figure 5. At the beginning of the 1-on-1 Mahjong game, each player is dealt with 13 tiles, the content of which is invisible to the other. Afterwards, each player takes actions in turn. Typically, the first player draws a tile from the deck wall and then discards a tile, and the next player takes the same types of actions in sequence. There are exceptional cases where the player does not draw a tile from the deck wall but Chow, Pong, or Kong the tile the opponent just discarded. Afterwards, the player discards a tile, and the game proceeds. Also, after drawing a tile from the deck wall, there are cases where the player could Kong, after which the player draws and discards a tile in sequence. There are 10 types of actions with 105 different actions in total, a full description of which is listed in Table 2.  The goal of each player is to complete a legal hand prior to the opponent, by drawing a tile or using the tile the opponent just discarded. A legal hand is generally in the form of four melds and a pair, with an exception of 7 pairs. Different categories of legal hands, 64 in total, come with different points, which is fully described in Table 3. Besides, a legal hand can belong to multiple categories, and the score of a legal hand is the sum of points of corresponding categories. Legal hands with higher points are generally more difficult to form in terms of either luck or strategy, and it is critical for a player to trade off the winning probability and the corresponding winning points. If no player completes a legal hand before the tiles are exhausted, the game is tied. A flow chart of the game is illustrated in Figure 6. A player completes a winning hand with the opponent player's first discard. And before that any action of Chow, Pong or Kong is not available. Table 3: The categories of legal hands in ascending order of corresponding points. The winning points of a legal hand are obtained by summing the points of the matched categories in reverse order while excluding the conflict categories.\n\n\nCategory name Points Pattern descriptions Conflicts\n\n\nA.2 THE STATE AND ACTION SPACE\n\nThe state space size of 1-on-1 Mahjong, shown as the infoset count in Figure 4, is approximately 10 74 . Yet, the state space of 1-on-1 Mahjong is not as easily abstracted as in poker. The primary reason is that a single tile difference in the state could significantly impact the policy, e.g., making a legal hand illegal and vice versa. In contrast, states that have similar strength in poker could share a common policy. For instance, the optimal preflop policy could be very similar for \"Ace-Four\" and \"Ace-Three\" in poker. Another reason is that a state in 1-on-1 Mahjong is divided into different information groups, as demonstrated in Figure 7(a). Different information groups have significantly different meanings. For instance, one group denotes the player's hand, which is invisible to the opponent, while another one denotes the player's discarded tiles, which are visible to both players.\n\nThere are 105 different actions in total, as demonstrated in Table 2. The number of legal actions in a state is relatively small compared to poker. Yet, the game length in 1-on-1 Mahjong is larger than that in poker. Players can decide up to about 40 sequential actions in 1-on-1 Mahjong, whereas most 1-on-1 poker games end within 10 steps. As a result, the reaching probability of states in 1-on-1 Mahjong may vary more significantly than that in poker.\n\n\nA.3 THE EFFECTS OF A LARGER INFOSET SIZE AND A LONGER GAME LENGTH\n\nAs shown in Figure 4, 1-on-1 Mahjong has a larger infoset size than poker. The infoset size does not seem to have an influence on the convergence of a tabular CFR (Zinkevich et al., 2008). However, when trajectory sampling and function approximation are used together, the situation may be different. To be more specific, in a trajectory sampling algorithm, the variance of the sampled instantaneous counterfactual value (regret) of a larger infoset may tend to be higher, which may have a large P i draws a tile P i discards a tile P i selects a legal action\n\n\nGame starts\n\nIs Concealed-Kong?\n\nDeal cards for P 0 , P 1 i=0 P i selects a legal action  influence on performance when neural network function approximation is used. In other words, 1-on-1 Mahjong may be complementary to poker in evaluating algorithms using deep neural networks and only trajectory samples.\n\nThe game length has a direct impact on the sampling methods used. For poker, which has a relatively short game length, methods that sample multiple actions in a state are very common in the literature. Yet, sampling multiple actions consistently in game trees with long episodes is certainly prohibitive, as the number of samples goes exponentially with the game length. 1-on-1 Mahjong has a maximal game length about 40, which may be daunting for methods that try to sample multiple actions in a state. In other words, 1-on-1 Mahjong may be complementary to poker in evaluating algorithms when only trajectory samples are allowed.\n\n\nB THE MODEL DESIGN OF OUR 1-ON-1 MAHJONG AGENT JUEJONG\n\nThe model of JueJong is an end-to-end neural network that takes all relevant information as input and outputs both the probabilities of all actions and the state value. This is different from five separated neural networks in Suphx (Li et al., 2020b), representing five different types of actions. Also, we train JueJong from zero by pure self-play using ACH, while the five neural networks in Suphx were trained by supervised learning on human data, with only the \"Discard\" network further enhanced by RL. Figure 7 gives an overview of the model design of JueJong.   The marked regions in Figure 7(a) summarize the information an agent can observe. Regions 1 to 7 are encoded in image-like features, which represent the player's hand, the player's Chow, Pong, and Kong, the player's concealed-Kong, the player's Discard, the opponent's Chow, Pong, and Kong, the opponent's concealed-Kong, and the opponent's Discard respectively. A feature map, as shown in Figure 7(b), of height 4 and width 17 is employed, where a 0 or 1 in the ith row and jth column means whether there are i tiles of the jth type in the input set of tiles. Therefore, a single or two successive convolutional layers with 3 \u00d7 3 kernels efficiently derive row and column combinations. Note that, in Suphx (Li et al., 2020b), the feature maps are one-dimensional vectors, where 1 \u00d7 3 kernels are used. The black tile in Figure 7(b) is designed exclusively for Region 6 to indicate how many concealed-Kongs the opponent has. For discarded tiles in Region 4 or 7, 24 such feature maps are used to indicate the latest 24 discarded tiles in order, with one feature map encoding one discard tile. All the feature maps are concatenated in the channel dimension, and therefore the final image-like features are in the shape of 4 \u00d7 17 \u00d7 53 (h \u00d7 w \u00d7 c).\n+ (a) (c) (b)\nRegions 8 to 11 are encoded in one-hot features, which represent the player's position (0 or 1), the is_ready state of both players, and the bonus tiles of both players. Additionally, the last action (not shown as a region in Figure 7(a)) for each player is encoded in a one-hot feature vector as well.\n\nWe apply residual blocks (He et al., 2016) to transform the image-like features. There are totally 3 stages with 3 residual blocks and 1 transition layer in each stage, as shown in Figure 7(c). Each block contains two convolutional layers with kernel size 3 \u00d7 3. The transition layers are pointwise convolutions that scale the number of output channels to 64, 128, and 32, respectively for each stage. Subsequently, the transformed image-like features are reshaped to a vector, which is concatenated with the one-hot feature vectors. A fully-connected layer (dimension 1024) is then used to transform the concatenated feature vector, and two branches with two fully-connected layers (dimension 512 \u00d7 512) each output the action probabilities and the state value respectively. Besides, we apply batch normalization (Ioffe & Szegedy, 2015) and ReLU non-linearity after all convolutional layers and fully-connected layers.\n\nC THEORETICAL PROPERTIES OF NW-CFR C.1 PROOF FOR THEOREM 1\n\nIn order to prove Theorem 1, we first prove the following Lemma.\nLemma 1. For a weighted CFR, if w t (s) \u2208 [w l (s), w h (s)], 0 < w l (s) \u2264 w h (s) \u2264 1, then max a\u2208A(s) t k=1 r c k (s, a) \u2264 max a\u2208A(s) R w t (s, a) w h (s) + (w h (s) \u2212 w l (s))|A(s)|\u2206t w h (s) .(4)\nAs a result, according to the folk theorem in Zinkevich et al. (2008), the average policy has exploitability, where\n= 1 |P| p\u2208P R p,T T \u2264 |S|\u2206 1 2T ln |A| + \u2206 s\u2208S w h (s) \u2212 w l (s) w h (s) .(25)\nC.2 PROOF FOR COROLLARY 1\n\nProof. When the behavioral policy \u00b5 p,k of each player p \u2208 P is constant across iterations \u2200k > 0, the reaching probability f \u00b5 k p (s) of any state s \u2208 S is also constant. Assume f \u00b5 k p (s) = w(s), then,\nR a t\u22121 (s, a) = t\u22121 k=1 f \u00b5 k p (s)r c t (s, a) = w(s) t\u22121 k=1 r c t (s, a) = w(s)R c t\u22121 (s, a).(26)\nIn other words, R a t\u22121 (s, a) is equal to the cumulative counterfactual regret scaled by a time-invariant weight w(s). Hence, the policy at iteration t is\n\u03c0 t (a|s) = e \u03b7(s)w(s)R c t (s,a) a e \u03b7(s)w(s)R c t\u22121 (s,a ) = e \u03b7 (s)R c t (s,a) a e \u03b7 (s)R c t\u22121 (s,a ) ,(27)\nwhere the new \u03b7 (s) is set to 8 ln |A(s)|/[\u2206 2 (s)T ]. As a result, for constant \u00b5 p,k , NW-CFR is equivalent to CFR with Hedge when y(a|s; \u03b8 t ) is sufficiently close to R a t (s, a). Furthermore, since w l (s) = f \u00b5 k p (s) = w h (s), the second term in Equation 25 vanishes, i.e.,\n\u2264 |S|\u2206 1 2T ln |A|.(28)\nAs a result, the exploitability bound of CFR with Hedge is recovered.\n\n\nD EXPERIMENTAL RESULTS OF THE WEIGHTED CFR\n\nAs stated in Section 5, ACH is a practical implementation of NW-CFR, which is a straightforward neural extension to the weighted CFR defined in Definition 1, together with w t (s) = f \u00b5t p (s) and Hedge. In order to investigate the behavior of weighted CFR, in this section, we instantiate multiple weighted CFR algorithms with different settings of the weight w t (s) by varying \u00b5 p,t , since f \u00b5t p (s) depends only on \u00b5 p,t . Note that the weighted CFR traverses the full game tree at every iteration and that \u00b5 p,t is only used to calculate the state reaching probability f \u00b5t p (s). We test these algorithms on three small IIGs in OpenSpiel: Kuhn poker, Leduc poker, and Liar's Dice.  Figure 8: Exploitability of the weighted CFR with w t (s) = f \u00b5t p (s) and CFR (i.e., the weighted CFR with w t (s) = 1.0). The probability f \u00b5t p (s) is determined by the behavior policy \u00b5 p,t . The setting of \u00b5 p,t for each line is given in the legend, in which \"Uniform\" means \u00b5 p,t (s) = the uniform policy; \"Current\" means \u00b5 p,t (s) = \u03c0 p,t (s); \"Current(x)\" means \u00b5 p,t (s) = xUniform + (1 \u2212 x)\u03c0 p,t (s). Note that the exploitability is reported with regard to the average policy.\n\nAs shown in Figure 8, the weighted CFR with w t (s) induced by a stationary \u00b5 p,t (i.e., the uniform policy) converges at the same pace with CFR(Hedge). As a result, Corollary 1 is verified on the three small benchmarks. Also, as Theorem 1 states, the exploitability of the weighted CFR is influenced by the range of w t (s) = f \u00b5t p (s). This is experimentally demonstrated in Figure 8 by setting \u00b5 p,t to a mixed policy between the current policy \u03c0 p,t and the uniform policy. We can see that the weighted CFR still performs competitively with CFR(Hedge), when \u00b5 p,t (s) = 0.5Uniform + 0.5\u03c0 p,t (s).\n\n\nE ACH: A PRACTICAL IMPLEMENTATION OF NW-CFR\n\nTo address the practical issues mentioned in Section 5, we provide a practical and parallel implementation of NW-CFR, i.e., ACH, which employs a framework of decoupled acting and learning, similar to IMPALA (Espeholt et al., 2018). ACH maintains a policy net y(a|s; \u03b8) and a value net V (s; \u03c9), where \u03b8 and \u03c9 share a large portion of parameters (see Figure 7). Both players use the same \u03b8 and \u03c9. We do not use an additional time-invariant behavioral policy for sampling actions. Instead, we use the current policy \u03c0 t , i.e., \u00b5 p,t = \u03c0 p,t , \u2200p \u2208 P. As a result, we can use the same samples to train both the value net and the policy net. Also, \u03b7(s) is incorporated into the learned target value in the policy net, so the policy \u03c0(a|s) is obtained by directly softmaxing on y(a|s; \u03b8).\n\nThe advantage A(s, a) is estimated by GAE(\u03bb) (Schulman et al., 2016), using sampled rewards and V (s; \u03c9), for only sampled states and actions. The value and policy nets are updated as soon as a mini-batch of samples is available. In other words, we update \u03b8 and \u03c9 once using a single mini-batch at each iteration. As a result, the policy loss reduces to L \u03c0 (s) = \u03b7(s) y(a|s;\u03b8) \u03c0 old (a|s) A(a, s), where 1 \u03c0 old (a|s) accounts for the fact that the action a was sampled using \u03c0 old (a|s). ACH handles asynchronous training with the importance ratio clipping [1 \u2212 \u03b5, 1 + \u03b5] of PPO (Schulman et al., 2017). To avoid numerical issues, the mean\u0233(\u00b7 |s; \u03b8) is subtracted from the policy output, which is then clipped within a range [\u2212l th , l th ]. The pseudocode of ACH is given in Algorithm 2.\n\nAlgorithm 2: ACH Initialize the policy and critic parameters: \u03b8 and \u03c9. Start multiple actor and learner threads in parallel. Actors: while true do Fetch the latest model from the learners. Generate samples via self-play in the form: [a, s, A(s, a), G, \u03c0 old (a|s)].\n\nSend the samples to the replay buffer. Learners: for t \u2208 1, 2, 3, ... do Fetch a mini-batch of samples from the replay buffer. L sum = 0. for each sample [a, s, A(s, a), G, \u03c0 old (a|s)] \u2208 the mini-batch do c = 1{ \u03c0(a|s;\u03b8) \u03c0 old (a|s) < 1 + \u03b5}1{y(a|s; \u03b8) \u2212\u0233(\u00b7 |s; \u03b8) < l th } if A(s, a) \u2265 0, 1{ \u03c0(a|s;\u03b8) \u03c0 old (a|s) > 1 \u2212 \u03b5}1{y(a|s; \u03b8) \u2212\u0233(\u00b7 |s; \u03b8) > \u2212l th } if A(s, a) < 0. L sum += \u2212c\u03b7(s) y(a|s;\u03b8) \u03c0 old (a|s) A(a, s) + \u03b1 2 [V (s; \u03c9) \u2212 G)] 2 + \u03b2 a \u03c0(a|s; \u03b8) log \u03c0(a|s; \u03b8). Update \u03b8 and \u03c9 once using gradient on L sum .\n\nWe employ an entropy loss to encourage exploration during training and hopefully the convergence of current policy to a NE (Srinivasan et al., 2018). ACH updates \u03b8 and \u03c9 simultaneously, and the overall loss is:\nL ACH = \u2212 c\u03b7(s)\ny(a|s; \u03b8) \u03c0 old (a|s) A(a, s) + \u03b1 2 [V (s; \u03c9) \u2212 G)] 2 + \u03b2 a \u03c0(a|s; \u03b8) log \u03c0(a|s; \u03b8).\n\nTheoretically, y(a|s ; \u03b8) for non-sampled states s should also be trained with the target y(a|s ; \u03b8 old ). Yet, in ACH, we only update \u03b8 once using a single mini-batch at each iteration. Therefore, \u03b8 is equal to \u03b8 old before the update. In other words, the policy loss for non-sampled states is 0 in ACH.  Figure 9: The exploitability of the current policy in ACH with different behavior policies on FHP. We report the mean as solid curves and the range as shaded regions across 3 independent runs.\n\n\nF THE EFFECT OF THE BEHAVIOR POLICY ON ACH IN FHP\n\nAs we noted in the paper, the behavior policy \u00b5 p,t in ACH could be set to either the current policy \u03c0 p,t or simply a uniform sampling policy. As Corollary 1 states, a tighter bound on the exploitability of the average policy of NW-CFR can be obtained, if \u00b5 p,t is stationary over iterations. However, since we have decided to use the current policy (trained with an entropy regularization) for evaluation in ACH, the effect of the behavior policy is unclear from a theoretical perspective. Hence, we conduct an experiment to investigate this using FHP, which is a non-trivial poker benchmark but still has the property that the exact exploitability of an agent can be efficiently computed.\n\nIn Figure 9, we compare the exploitability of the current policy of ACH on FHP, by setting the behavior policy in ACH to either the current policy or a uniform sampling policy. From the comparison, it seems that the performance of the current policy of ACH is not sensitive to the choice of behavior policy. One reason might be that the additional entropy loss forces the current policy to be stable and prone to a uniform random policy. Another reason might be that the current policy instead of the average policy is evaluated. We will investigate this in more depth in the future.\n\n\nG ADDITIONAL RESULTS ON SMALL IIG BENCHMARKS IN OPENSPIEL\n\nWe further evaluate ACH and compare it with A2C, RPG, and NeuRD on three benchmarks from OpenSpiel: Kuhn poker, Leduc poker, and Liar's Dice. All the experiments were run single-threaded on a 2.24GHz CPU. We use the network architecture provided in OpenSpiel, which has a 128neurons fully-connected layer followed by ReLU and two separate linear layers for the policy and the state/action value. For A2C, RPG, and NeuRD, we use the default hyper-parameters in OpenSpiel. ACH shares most of the hyper-parameters with A2C. All the hyper-parameters are listed in the Appendix H.3.\n\nThe exploitability of an agent is exactly calculated using tools in OpenSpiel. For each method, we compute the exploitability of each agent every 1e5 training steps, the results of which are plotted in Figure 10. Clearly, ACH converges significantly faster and achieves a lower exploitability than other methods across the three benchmarks. There is still some gap between 0 and the exploitability ACH converges to. This may due to the neural network approximation error and the fact that we use the current policy instead of the average policy for the evaluation. As expected, A2C has the worst performance, since it is designed for single-agent environments. Moreover, the superiority of ACH is most significant on the Liar's Dice benchmark, which is the most complex one of the three benchmarks.\n\nAs a complement, we also present the head-to-head performance of A2C, RPG, NeuRD, and ACH on the three benchmarks in OpenSpiel. As demonstrated in Table 4, the agent of ACH won all other agents across the three benchmarks. (c) Liar's Dice Figure 10: The training curves of each agent on the three benchmarks from OpenSpiel. We report the mean as solid curves and the range of the exploitability across 8 independent runs as shaded regions. We notice that there exists some discrepancy between the NeuRD results on Kuhn poker and Leduc poker reported here and those reported in Hennes et al. (2020) Table 4: Mean (\u00b1 standard deviation) of the average winning scores of the row agents against the column agents. The mean and the standard deviation are estimated by 8 independent runs. In each run, the average winning scores are obtained via 10, 000 head-to-head plays. All the agents are selected at the 1e7th training step.\n\n\nH HYPER-PARAMETERS\n\nH.1 HYPER-PARAMETERS FOR THE 1-ON-1 MAHJONG EXPERIMENT\n\nWe used the Adam optimizer (Kingma & Ba, 2014) for the experiments on the 1-on-1 Mahjong benchmark. We performed a mild hyper-parameter search on PPO and used the best setting for the shared hyper-parameters of all methods (PPO, RPG, NeuRD, and ACH). Table 5 gives an overview of hyper-parameters for each method. Also, we report the performance of the current policy, instead of the average policy, for each method. Logit threshold (l th ) -6.0 ACH Logit threshold (l th ) -6.0 Hedge coefficient (\u03b7(s)) {1.0, 1e-1} 1.0 \n\n\nH.2 HYPER-PARAMETERS FOR THE FHP EXPERIMENT\n\nWe used the Adam optimizer (Kingma & Ba, 2014) for the experiments on FHP. All the experiments on FHP were run multi-threaded and synchronously using ten 2.24GHz CPUs. We update the neural networks of ACH and other methods (A2C, RPG, and NeuRD) every 1000 episodes, with a batch consisting all the samples collected within the latest 1000 episodes. Since the average game length of FHP is around 2, the batch size is roughly 2000. We performed a mild hyper-parameter search for ACH, A2C, RPG, and NeuRD, as shown in Table 6. Note that we do not need to clip the advantages in a synchronous method, so the ratio clip hyper-parameter \u03b5 is not needed. We multiply the rewards in FHP with a reward normalizer, as the rewards in FHP are in the range [\u2212700, 700]. Besides, we found that A2C, RPG, and NeuRD are very sensitive to the entropy coefficient, and we had to use a larger entropy coefficient in these algorithms than ACH. For OS-DCFR and DREAM, we use the same hyper-parameters presented in Steinberger et al. (2020). An overview of the hyper-parameters on FHP is given in Table 6. Note that we report the performance of the current policy for ACH, A2C, RPG, and NeuRD, while the average policy is used for evaluation in OS-DCFR and DREAM. {1e-2, 3e-2, 5e-2} 3e-2 Hedge coefficient (\u03b7(s)) {1.0, 0.1} 1.0 \n\n\nH.3 HYPER-PARAMETERS FOR THE EXPERIMENT ON OPENSPIEL\n\nWe used stochastic gradient descent with a constant learning rate for all the experiments on benchmarks from OpenSpiel. For A2C and RPG, we used the default implementations and hyper-parameters in OpenSpiel. NeuRD was originally implemented using the counterfactual regret in OpenSpiel. We re-implemented NeuRD using predicted advantages and set the shared hyper-parameters of NeuRD identical to those in RPG. All methods employ an entropy loss to encourage exploration during training and hopefully the convergence of the current policy to a NE. Also, we report the performance of the current policy, instead of the average policy, for each method.\n\n\nParameter Range Best\n\nLearning rate {1e-3, 5e-3} 1e-3 Value loss coefficient (\u03b1) {1.0, 2.0} 2.0 Hedge coefficient (\u03b7(s)) {1.0, 1e-1, 1e-2} 1.0 In the OpenSpiel implementations, the value parameters are updated separately and more frequently compared to the policy parameters for A2C, RPG, and NeuRD. Yet, the value loss and the policy loss are combined in ACH, and all parameters are updated simultaneously, as illustrated in Algorithm 2. 1e-2 1e-2 1e-2 1e-2 Logit threshold (l th ) --2.0 2.0 Also note that, in a single-threaded training environment, the actor and the learner run in sequence. As a result, \u03c0(a|s; \u03b8) is always identical to \u03c0 old (a|s) in ACH in Algorithm 2.\n\nWe performed a mild hyper-parameter search for ACH, which is illustrated in Table 7. The final hyper-parameters used for each method are listed in Table 8, with 3 additional hyper-parameters of ACH listed in the \"Best\" column in Table 7.\n\n\nI THE RELATIONSHIP BETWEEN ACH AND SUPHX\n\nRecently, Suphx (Li et al., 2020b) has achieved stunning performance on Japanese Riichi Mahjong. The development of Suphx is enabled by a novel integration of existing supervised learning and RL methods in addition to some newly developed techniques. Three new techniques were introduced in Suphx: global reward prediction, oracle guiding, and run-time policy adaptation. The global reward prediction technique is to handle the multi-round game situation, which is irrelevant to our 1-on-1 Mahjong setting (only one round per game in our test setting). The oracle guiding technique decays the invisible feature during training, which is of independent interest in dealing with imperfectinformation. The run-time policy adaptation technique adapts the trained policy at test time, and this may be combined with ACH, which is a training algorithm. In summary, Suphx is more of a novel system than a new algorithm. For this reason, we did not implement and compare Suphx with ACH in our 1-on-1 Mahjong experiment. Nonetheless, the oracle guiding technique may be complementary to ACH in handling imperfect-information, and we will investigate this in future work.\n\nFigure 1 :\n1(a): The training curves of the best response against each agent. Lower is better. (b):\n\nFigure 2 :Figure 3 :\n23Performance of JueJong against human players. The exploitability on FHP, with the x-axis being the number of episodes generated (left and right) and the number of samples consumed (middle). We report the mean as solid curves and the range as shaded regions across 3 independent runs. OS-DCFR and DREAM were run once as their performances are relatively stable, according to andSteinberger et al. (2020).\n\nFigure 4 :Figure 5 :\n45The game complexity of Heads-up Limit Texas Hold'em (HULH), Heads-up No-Limit Texas Hold'em, 1-on-1 Mahjong, and 4-Player Mahjong. A list of all tiles in the 1-on-1 Mahjong game.\n\nFigure 6 :\n6A flow chart of the 1-on-1 Mahjong game.\n\nFigure 7 :\n7(a) The graphical user interface of 1-on-1 Mahjong with marked regions representing different groups of information, which are encoded in either image-like features or one-hot features. (b) The image-like feature encoding scheme. (c) The model architecture design.\n\n\nMarc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael H. Bowling. Monte carlo sampling for regret minimization in extensive games. In Yoshua Bengio, Dale Schuurmans, John D. Lafferty, Christopher K. I. Williams, and Aron Culotta (eds.), Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009. Proceedings of a meeting held 7-10 December 2009, Vancouver, British Columbia, Canada, pp. 1078-1086. Curran Associates, Inc., 2009. Hui Li, Kailiang Hu, Shaohua Zhang, Yuan Qi, and Le Song. Double neural counterfactual regret minimization. In International Conference on Learning Representations (ICLR), 2020a. Junjie Li, Sotetsu Koyamada, Qiwei Ye, Guoqing Liu, Chao Wang, Ruihan Yang, Li Zhao, Tao Qin, Tie-Yan Liu, and Hsiao-Wuen Hon. Suphx: Mastering mahjong with deep reinforcement learning.The code of the 1-on-1 Mahjong benchmark is available at https://github.com/ \nyata0/Mahjong. The code of ACH is available at https://github.com/Liuweiming/ \nACH_poker. All the hyper-parameters for all the experiments are listed in the Appendix H. All the \ntheoretical results are presented in the main text, with all the proofs given in the Appendix C. \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image \nrecognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition \n(CVPR), pp. 770-778, 2016. \n\nJohannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfect-\ninformation games. CoRR, abs/1603.01121, 2016. \n\nDaniel Hennes, Dustin Morrill, Shayegan Omidshafiei, R\u00e9mi Munos, Julien Perolat, Marc Lanctot, \nAudrunas Gruslys, Jean-Baptiste Lespiau, Paavo Parmas, Edgar Du\u00e9\u00f1ez-Guzm\u00e1n, et al. Neural \nreplicator dynamics: Multiagent learning via hedging policy gradients. In International Joint \nConference on Autonomous Agents and Multi-agent Systems (AAMAS), pp. 492-501, 2020. \n\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by \nreducing internal covariate shift. In International Conference on Machine Learning (ICML), pp. \n448-456, 2015. \n\nMichael Johanson, Neil Burch, Richard Valenzano, and Michael Bowling. Evaluating state-space \nabstractions in extensive-form games. In Proceedings of the 2013 international conference on \nAutonomous agents and multi-agent systems, pp. 271-278, 2013. \n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint \narXiv:1412.6980, 2014. \n\nMarc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien P\u00e9rolat, \nDavid Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement \nlearning. In Advances in Neural Information Processing Systems (NeurIPS), pp. 4190-4203, 2017. \n\nMarc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vin\u00edcius Flores Zambaldi, Satyaki Upadhyay, \nJulien P\u00e9rolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafiei, and et al. \nOpenspiel: A framework for reinforcement learning in games. CoRR, abs/1908.09453, 2019. \n\nCoRR, abs/2003.13590, 2020b. \n\nEdward Lockhart, Marc Lanctot, Julien P\u00e9rolat, Jean-Baptiste Lespiau, Dustin Morrill, Finbarr \nTimbers, and Karl Tuyls. Computing approximate equilibria in sequential adversarial games by \nexploitability descent. In International Joint Conference on Artificial Intelligence (IJCAI), pp. \n464-470, 2019. \n\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim \nHarley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement \nlearning. In International Conference on Machine Learning (ICML), pp. 1928-1937, 2016. \n\nMatej Morav\u010d\u00edk, Martin Schmid, Neil Burch, Viliam Lis\u1ef3, Dustin Morrill, Nolan Bard, Trevor \nDavis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial \nintelligence in heads-up no-limit poker. Science, 356(6337):508-513, 2017. \n\nJulien Perolat, Remi Munos, Jean-Baptiste Lespiau, Shayegan Omidshafiei, Mark Rowland, Pedro \nOrtega, Neil Burch, Thomas Anthony, David Balduzzi, Bart De Vylder, et al. From poincar\u00e9 \nrecurrence to convergence in imperfect information games: Finding equilibrium via regularization. \nIn International Conference on Machine Learning, pp. 8525-8535. PMLR, 2021. \n\n\nTable 2 :\n2Different actions in the 1-on-1 Mahjong game.\n\n\nand Lanctot et al. (2019). The reason might be due to NeuRD's sensitivity to running environments including hyper-parameters and random seeds.Kuhn poker \nLeduc poker \nLiars Dice \nA2C \nRPG \nNeuRD \nA2C \nRPG \nNeuRD \nA2C \nRPG \nNeuRD \nRPG \n-0.024\u00b10.20 \n-\n-\n0.521\u00b10.23 \n-\n-\n0.192\u00b10.14 \n-\n-\nNeuRD -0.096\u00b10.16 -0.087\u00b10.16 \n-\n0.569\u00b10.16 -0.108\u00b10.17 \n-\n-0.226\u00b10.05 -0.355\u00b10.12 \n-\nACH \n0.104\u00b10.05 \n0.115\u00b10.05 0.118\u00b10.05 0.495\u00b10.22 \n0.050\u00b10.14 0.117\u00b10.15 \n0.275\u00b10.05 \n0.122\u00b10.05 0.407\u00b10.06 \n\n\n\nTable 5 :\n5The hyper-parameters used for the 1-on-1 Mahjong experiment.\n\nTable 6 :\n6The hyper-parameters used for the FHP experiment.\n\nTable 7 :\n7The hyper-parameter search ranges and best settings of ACH for the OpenSpiel experiment.\n\nTable 8 :\n8The hyper-parameters used for the OpenSpiel experiment.\nDeepStack (Morav\u010d\u00edk et al., 2017) employs sparse lookahead trees, much like the action abstraction. 2 Without specification, we mean Heads-up No-Limit Texas Hold'em in this paper. 3 Haihua Cheng is the Competition Mahjong champion of 2014 World Mahjong Master Tournament, 2018 Tencent Mahjong Tournament, and 2019 Tencent Mahjong Tournament.\nWe use state s and infoset Ip interchangeably in this paper.\nGiven \u03c0p,t at each iteration, we could obtain \u03c0p using the techniques introduced in Steinberger (2019).\nIn a preliminary experiment presented in the Appendix F, we find that the performance of the current policy in ACH (trained with an entropy regularization) is not sensitive to the choice of \u00b5p,t.\nThe rule-based agent is implemented such that it selects the action Hu, Ting, Kong, Chow, and Pong in descending priority whenever available and discards the tile that has the fewest neighbours.\nPPO is replaced with A2C in a synchronous training environment.\nProof. First, for any state s \u2208 S and action a \u2208 A(s), we haveSo,In other words,So,Then we can prove the Theorem:Proof. For NW-CFR, the policy at iteration t is generated according to Hedge:whereMeanwhile, weighted CFR with Hedge generates policy at iteration t according towhereSo, when w k (s) = f \u00b5 k p (s) \u2265 w l (s) > 0 for any s \u2208 S and k > 0, NW-CFR and weighted CFR with Hedge generate the same policy at each iteration, and therefore NW-CFR is equivalent to weighted CFR with Hedge.Then, we can prove the convergence property of NW-CFR. LetSinceTherefore,So,i.e., max a\u2208A(s)According to Lemma 1,When \u03b7(s) = 8 lnAccording to Theorem 2 inZinkevich et al. (2008), the total regret\nDota 2 with large scale deep reinforcement learning. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Christopher Hesse, abs/1912.06680CoRRChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Christopher Hesse, and et al. Dota 2 with large scale deep reinforcement learning. CoRR, abs/1912.06680, 2019.\n\nSuperhuman AI for heads-up no-limit poker: Libratus beats top professionals. Noam Brown, Tuomas Sandholm, Science. 3596374Noam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. Science, 359(6374):418-424, 2018.\n\nSuperhuman AI for multiplayer poker. Noam Brown, Tuomas Sandholm, Science. 3656456Noam Brown and Tuomas Sandholm. Superhuman AI for multiplayer poker. Science, 365(6456): 885-890, 2019.\n\nDeep counterfactual regret minimization. Noam Brown, Adam Lerer, Sam Gross, Tuomas Sandholm, International Conference on Machine Learning (ICML). Noam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret mini- mization. In International Conference on Machine Learning (ICML), pp. 793-802, 2019.\n\nPrediction, learning, and games. Nicolo Cesa, - Bianchi, G\u00e1bor Lugosi, Cambridge university pressNicolo Cesa-Bianchi and G\u00e1bor Lugosi. Prediction, learning, and games. Cambridge university press, 2006.\n\nLow-variance and zero-variance baselines for extensive-form games. Trevor Davis, Martin Schmid, Michael Bowling, International Conference on Machine Learning. PMLRTrevor Davis, Martin Schmid, and Michael Bowling. Low-variance and zero-variance baselines for extensive-form games. In International Conference on Machine Learning, pp. 2392-2401. PMLR, 2020.\n\nIMPALA: scalable distributed deep-rl with importance weighted actor-learner architectures. Lasse Espeholt, Hubert Soyer, R\u00e9mi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu, International Conference on Machine Learning (ICML). 80Lasse Espeholt, Hubert Soyer, R\u00e9mi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: scalable distributed deep-rl with importance weighted actor-learner architectures. In International Conference on Machine Learning (ICML), volume 80, pp. 1406-1415, 2018.\n\nA decision-theoretic generalization of on-line learning and an application to boosting. Yoav Freund, Robert E Schapire, Journal of Computer and System Sciences. 551Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119-139, 1997.\n\nPotential-aware imperfect-recall abstraction with earth mover's distance in imperfect-information games. Sam Ganzfried, Tuomas Sandholm, Twenty-Eighth AAAI Conference on Artificial Intelligence. Sam Ganzfried and Tuomas Sandholm. Potential-aware imperfect-recall abstraction with earth mover's distance in imperfect-information games. In Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014.\n\nEfficient evolutionary dynamics with extensiveform games. Nicola Gatti, Fabio Panozzo, Marcello Restelli, AAAI Conference on Artificial Intelligence. Nicola Gatti, Fabio Panozzo, and Marcello Restelli. Efficient evolutionary dynamics with extensive- form games. In AAAI Conference on Artificial Intelligence, 2013.\n\nThe advantage regret-matching actor-critic. CoRR, abs. Audrunas Gruslys, Marc Lanctot, R\u00e9mi Munos, Finbarr Timbers, Martin Schmid, Julien P\u00e9rolat, Dustin Morrill, Vin\u00edcius Flores Zambaldi, Jean-Baptiste Lespiau, John Schultz, Mohammad Gheshlaghi Azar, Michael Bowling, and Karl TuylsAudrunas Gruslys, Marc Lanctot, R\u00e9mi Munos, Finbarr Timbers, Martin Schmid, Julien P\u00e9rolat, Dustin Morrill, Vin\u00edcius Flores Zambaldi, Jean-Baptiste Lespiau, John Schultz, Mohammad Ghesh- laghi Azar, Michael Bowling, and Karl Tuyls. The advantage regret-matching actor-critic. CoRR, abs/2008.12234, 2020.\n\nA simple adaptive procedure leading to correlated equilibrium. Sergiu Hart, Andreu Mas-Colell, Econometrica. 685Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica, 68(5):1127-1150, 2000.\n\nVariance reduction in monte carlo counterfactual regret minimization (vr-mccfr) for extensive form games using baselines. Martin Schmid, Neil Burch, Marc Lanctot, Matej Moravcik, Rudolf Kadlec, Michael Bowling, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Martin Schmid, Neil Burch, Marc Lanctot, Matej Moravcik, Rudolf Kadlec, and Michael Bowling. Variance reduction in monte carlo counterfactual regret minimization (vr-mccfr) for extensive form games using baselines. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 2157-2164, 2019.\n\nHighdimensional continuous control using generalized advantage estimation. John Schulman, Philipp Moritz, Sergey Levine, Michael I Jordan, Pieter Abbeel, International Conference on Learning Representations. John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High- dimensional continuous control using generalized advantage estimation. In International Confer- ence on Learning Representations, (ICLR), 2016.\n\nProximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, abs/1707.06347CoRRJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017.\n\nActor-critic policy optimization in partially observable multiagent environments. Sriram Srinivasan, Marc Lanctot, Vinicius Zambaldi, Julien P\u00e9rolat, Karl Tuyls, R\u00e9mi Munos, Michael Bowling, Advances in Neural Information Processing Systems (NeurIPS). Sriram Srinivasan, Marc Lanctot, Vinicius Zambaldi, Julien P\u00e9rolat, Karl Tuyls, R\u00e9mi Munos, and Michael Bowling. Actor-critic policy optimization in partially observable multiagent environments. In Advances in Neural Information Processing Systems (NeurIPS), pp. 3422-3435, 2018.\n\nEric Steinberger, arXiv:1901.07621Single deep counterfactual regret minimization. arXiv preprintEric Steinberger. Single deep counterfactual regret minimization. arXiv preprint arXiv:1901.07621, 2019.\n\nDREAM: deep regret minimization with advantage baselines and model-free learning. Eric Steinberger, Adam Lerer, Noam Brown, CoRR, absEric Steinberger, Adam Lerer, and Noam Brown. DREAM: deep regret minimization with advantage baselines and model-free learning. CoRR, abs/2006.10410, 2020.\n\nPolicy gradient methods for reinforcement learning with function approximation. Richard S Sutton, David A Mcallester, P Satinder, Yishay Singh, Mansour, Advances in Neural Information Processing Systems (NeurIPS). Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradi- ent methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems (NeurIPS), pp. 1057-1063, 1999.\n\nApproximate exploitability: Learning a best response in large games. Finbarr Timbers, Edward Lockhart, Martin Schmid, Marc Lanctot, Michael Bowling, abs/2004.09677CoRR. Finbarr Timbers, Edward Lockhart, Martin Schmid, Marc Lanctot, and Michael Bowling. Approxi- mate exploitability: Learning a best response in large games. CoRR, abs/2004.09677, 2020.\n\nGrandmaster level in StarCraft II using multi-agent reinforcement learning. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Nature. Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, and et al. John P. Agapiou and5757782Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, and et al. John P. Agapiou and. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782): 350-354, 2019.\n\nA practical use of imperfect recall. Kevin Waugh, Martin Zinkevich, Michael Johanson, Morgan Kan, David Schnizlein, Michael Bowling, Eighth symposium on abstraction, reformulation, and approximation. Kevin Waugh, Martin Zinkevich, Michael Johanson, Morgan Kan, David Schnizlein, and Michael Bowling. A practical use of imperfect recall. In Eighth symposium on abstraction, reformulation, and approximation, 2009.\n\nSolving games with functional regret estimation. Kevin Waugh, Dustin Morrill, James Andrew Bagnell, Michael H Bowling, AAAI Conference on Artificial Intelligence. Kevin Waugh, Dustin Morrill, James Andrew Bagnell, and Michael H. Bowling. Solving games with functional regret estimation. In AAAI Conference on Artificial Intelligence, pp. 2138-2145, 2015.\n\nSimple statistical gradient-following algorithms for connectionist reinforcement learning. J Ronald, Williams, Machine Learning. 8Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229-256, 1992.\n\nTowards playing full MOBA games with deep reinforcement learning. Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao Qiu, Hongsheng Yu, Yinyuting Yin, Bei Shi, Liang Wang, Tengfei Shi, Qiang Fu, Wei Yang, Lanxiao Huang, Wei Liu, Advances in Neural Information Processing Systems (NeurIPS). 2020Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao Qiu, Hongsheng Yu, Yinyuting Yin, Bei Shi, Liang Wang, Tengfei Shi, Qiang Fu, Wei Yang, Lanxiao Huang, and Wei Liu. Towards playing full MOBA games with deep reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\nRegret minimization in games with incomplete information. Martin Zinkevich, Michael Johanson, Michael Bowling, Carmelo Piccione, Advances in Neural Information Processing Systems (NeurIPS). Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization in games with incomplete information. In Advances in Neural Information Processing Systems (NeurIPS), pp. 1729-1736, 2008.\n", "annotations": {"author": "[{\"end\":120,\"start\":81},{\"end\":191,\"start\":121},{\"end\":232,\"start\":192},{\"end\":276,\"start\":233},{\"end\":316,\"start\":277},{\"end\":324,\"start\":317},{\"end\":339,\"start\":325},{\"end\":405,\"start\":340},{\"end\":442,\"start\":406},{\"end\":482,\"start\":443},{\"end\":522,\"start\":483},{\"end\":589,\"start\":523},{\"end\":680,\"start\":590}]", "publisher": null, "author_last_name": "[{\"end\":89,\"start\":87},{\"end\":132,\"start\":129},{\"end\":201,\"start\":199},{\"end\":243,\"start\":239},{\"end\":285,\"start\":281},{\"end\":323,\"start\":321},{\"end\":338,\"start\":334},{\"end\":346,\"start\":344},{\"end\":411,\"start\":409},{\"end\":451,\"start\":449},{\"end\":491,\"start\":487}]", "author_first_name": "[{\"end\":86,\"start\":81},{\"end\":128,\"start\":121},{\"end\":198,\"start\":192},{\"end\":238,\"start\":233},{\"end\":280,\"start\":277},{\"end\":320,\"start\":317},{\"end\":333,\"start\":325},{\"end\":343,\"start\":340},{\"end\":408,\"start\":406},{\"end\":448,\"start\":443},{\"end\":486,\"start\":483}]", "author_affiliation": "[{\"end\":119,\"start\":91},{\"end\":190,\"start\":134},{\"end\":231,\"start\":203},{\"end\":275,\"start\":245},{\"end\":315,\"start\":287},{\"end\":404,\"start\":348},{\"end\":441,\"start\":413},{\"end\":481,\"start\":453},{\"end\":521,\"start\":493},{\"end\":588,\"start\":524},{\"end\":679,\"start\":591}]", "title": "[{\"end\":78,\"start\":1},{\"end\":758,\"start\":681}]", "venue": null, "abstract": "[{\"end\":2313,\"start\":804}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2492,\"start\":2471},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2513,\"start\":2492},{\"end\":2529,\"start\":2513},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2951,\"start\":2927},{\"end\":2976,\"start\":2956},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3192,\"start\":3171},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3737,\"start\":3713},{\"end\":4193,\"start\":4170},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4215,\"start\":4193},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4289,\"start\":4269},{\"end\":4311,\"start\":4289},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4338,\"start\":4311},{\"end\":4469,\"start\":4443},{\"end\":4654,\"start\":4637},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5701,\"start\":5676},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9648,\"start\":9632},{\"end\":9971,\"start\":9952},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10549,\"start\":10526},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11413,\"start\":11389},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12277,\"start\":12254},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12678,\"start\":12654},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12773,\"start\":12748},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12813,\"start\":12784},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13199,\"start\":13170},{\"end\":15808,\"start\":15789},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15839,\"start\":15814},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15976,\"start\":15951},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18427,\"start\":18402},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18876,\"start\":18852},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21005,\"start\":20982},{\"end\":21027,\"start\":21005},{\"end\":21054,\"start\":21027},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21351,\"start\":21327},{\"end\":21376,\"start\":21356},{\"end\":21862,\"start\":21844},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22364,\"start\":22341},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22767,\"start\":22747},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23304,\"start\":23278},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23445,\"start\":23424},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23464,\"start\":23445},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23547,\"start\":23525},{\"end\":23973,\"start\":23951},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24256,\"start\":24231},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24869,\"start\":24849},{\"end\":25277,\"start\":25251},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25846,\"start\":25821},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28524,\"start\":28501},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32556,\"start\":32531},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":34817,\"start\":34794},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":39343,\"start\":39319},{\"end\":40968,\"start\":40944},{\"end\":42892,\"start\":42875},{\"end\":43687,\"start\":43664},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":44166,\"start\":44143},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":47377,\"start\":47354},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":48001,\"start\":47978},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":48537,\"start\":48514},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":49660,\"start\":49635},{\"end\":54142,\"start\":54123},{\"end\":54710,\"start\":54691},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":55683,\"start\":55658},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":59327,\"start\":59302},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":64279,\"start\":64254}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":58900,\"start\":58800},{\"attributes\":{\"id\":\"fig_2\"},\"end\":59328,\"start\":58901},{\"attributes\":{\"id\":\"fig_3\"},\"end\":59531,\"start\":59329},{\"attributes\":{\"id\":\"fig_4\"},\"end\":59585,\"start\":59532},{\"attributes\":{\"id\":\"fig_6\"},\"end\":59863,\"start\":59586},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":64193,\"start\":59864},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":64251,\"start\":64194},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":64734,\"start\":64252},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":64807,\"start\":64735},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":64869,\"start\":64808},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":64970,\"start\":64870},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":65038,\"start\":64971}]", "paragraph": "[{\"end\":3409,\"start\":2329},{\"end\":4470,\"start\":3411},{\"end\":5266,\"start\":4472},{\"end\":5702,\"start\":5268},{\"end\":5979,\"start\":5704},{\"end\":6149,\"start\":5981},{\"end\":6337,\"start\":6151},{\"end\":7109,\"start\":6417},{\"end\":8368,\"start\":7111},{\"end\":8873,\"start\":8370},{\"end\":9321,\"start\":8928},{\"end\":10379,\"start\":9374},{\"end\":10921,\"start\":10381},{\"end\":12122,\"start\":10960},{\"end\":12278,\"start\":12124},{\"end\":12987,\"start\":12374},{\"end\":13042,\"start\":13020},{\"end\":13047,\"start\":13044},{\"end\":13339,\"start\":13049},{\"end\":13580,\"start\":13380},{\"end\":13751,\"start\":13582},{\"end\":14042,\"start\":13753},{\"end\":15146,\"start\":14044},{\"end\":15665,\"start\":15289},{\"end\":16028,\"start\":15667},{\"end\":16469,\"start\":16130},{\"end\":16982,\"start\":16506},{\"end\":17403,\"start\":16984},{\"end\":18776,\"start\":18232},{\"end\":19275,\"start\":18778},{\"end\":19613,\"start\":19396},{\"end\":20211,\"start\":19672},{\"end\":20440,\"start\":20213},{\"end\":20713,\"start\":20442},{\"end\":20847,\"start\":20759},{\"end\":21539,\"start\":20849},{\"end\":21931,\"start\":21541},{\"end\":22586,\"start\":21933},{\"end\":24164,\"start\":22603},{\"end\":24548,\"start\":24166},{\"end\":24902,\"start\":24613},{\"end\":25394,\"start\":24971},{\"end\":26707,\"start\":25419},{\"end\":28021,\"start\":26749},{\"end\":28608,\"start\":28023},{\"end\":29079,\"start\":28610},{\"end\":29730,\"start\":29081},{\"end\":30651,\"start\":29732},{\"end\":31053,\"start\":30653},{\"end\":32008,\"start\":31055},{\"end\":32741,\"start\":32041},{\"end\":33206,\"start\":32743},{\"end\":34268,\"start\":33222},{\"end\":34585,\"start\":34288},{\"end\":34924,\"start\":34615},{\"end\":37640,\"start\":34975},{\"end\":38629,\"start\":37729},{\"end\":39086,\"start\":38631},{\"end\":39715,\"start\":39156},{\"end\":39749,\"start\":39731},{\"end\":40026,\"start\":39751},{\"end\":40659,\"start\":40028},{\"end\":42531,\"start\":40718},{\"end\":42848,\"start\":42546},{\"end\":43769,\"start\":42850},{\"end\":43829,\"start\":43771},{\"end\":43895,\"start\":43831},{\"end\":44212,\"start\":44097},{\"end\":44317,\"start\":44292},{\"end\":44524,\"start\":44319},{\"end\":44783,\"start\":44628},{\"end\":45179,\"start\":44896},{\"end\":45273,\"start\":45204},{\"end\":46496,\"start\":45320},{\"end\":47099,\"start\":46498},{\"end\":47931,\"start\":47147},{\"end\":48723,\"start\":47933},{\"end\":48990,\"start\":48725},{\"end\":49510,\"start\":48992},{\"end\":49722,\"start\":49512},{\"end\":49823,\"start\":49739},{\"end\":50323,\"start\":49825},{\"end\":51068,\"start\":50377},{\"end\":51653,\"start\":51070},{\"end\":52292,\"start\":51715},{\"end\":53092,\"start\":52294},{\"end\":54017,\"start\":53094},{\"end\":54094,\"start\":54040},{\"end\":54616,\"start\":54096},{\"end\":55971,\"start\":54664},{\"end\":56677,\"start\":56028},{\"end\":57355,\"start\":56702},{\"end\":57594,\"start\":57357},{\"end\":58799,\"start\":57639}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9373,\"start\":9322},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12373,\"start\":12279},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13019,\"start\":12988},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15288,\"start\":15147},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16129,\"start\":16029},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18231,\"start\":17404},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19395,\"start\":19276},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19671,\"start\":19614},{\"attributes\":{\"id\":\"formula_8\"},\"end\":24612,\"start\":24549},{\"attributes\":{\"id\":\"formula_9\"},\"end\":24970,\"start\":24903},{\"attributes\":{\"id\":\"formula_10\"},\"end\":42545,\"start\":42532},{\"attributes\":{\"id\":\"formula_11\"},\"end\":44096,\"start\":43896},{\"attributes\":{\"id\":\"formula_12\"},\"end\":44291,\"start\":44213},{\"attributes\":{\"id\":\"formula_13\"},\"end\":44627,\"start\":44525},{\"attributes\":{\"id\":\"formula_14\"},\"end\":44895,\"start\":44784},{\"attributes\":{\"id\":\"formula_15\"},\"end\":45203,\"start\":45180},{\"attributes\":{\"id\":\"formula_16\"},\"end\":49738,\"start\":49723}]", "table_ref": "[{\"end\":30524,\"start\":30517},{\"end\":30765,\"start\":30758},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":36421,\"start\":36414},{\"end\":36771,\"start\":36764},{\"end\":37411,\"start\":37404},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":38699,\"start\":38692},{\"end\":53248,\"start\":53241},{\"end\":53699,\"start\":53692},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":54354,\"start\":54347},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":55187,\"start\":55180},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":55747,\"start\":55740},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":57440,\"start\":57433},{\"attributes\":{\"ref_id\":\"tab_15\"},\"end\":57511,\"start\":57504},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":57593,\"start\":57586}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2327,\"start\":2315},{\"attributes\":{\"n\":\"2\"},\"end\":6364,\"start\":6340},{\"attributes\":{\"n\":\"2.1\"},\"end\":6415,\"start\":6367},{\"attributes\":{\"n\":\"2.2\"},\"end\":8926,\"start\":8876},{\"attributes\":{\"n\":\"2.3\"},\"end\":10958,\"start\":10924},{\"attributes\":{\"n\":\"3\"},\"end\":13378,\"start\":13342},{\"attributes\":{\"n\":\"4\"},\"end\":16504,\"start\":16472},{\"attributes\":{\"n\":\"5\"},\"end\":20757,\"start\":20716},{\"attributes\":{\"n\":\"6\"},\"end\":22601,\"start\":22589},{\"attributes\":{\"n\":\"7\"},\"end\":25417,\"start\":25397},{\"attributes\":{\"n\":\"7.1\"},\"end\":26747,\"start\":26710},{\"attributes\":{\"n\":\"7.3\"},\"end\":32039,\"start\":32011},{\"attributes\":{\"n\":\"8\"},\"end\":33220,\"start\":33209},{\"end\":34286,\"start\":34271},{\"end\":34613,\"start\":34588},{\"end\":34973,\"start\":34927},{\"end\":37694,\"start\":37643},{\"end\":37727,\"start\":37697},{\"end\":39154,\"start\":39089},{\"end\":39729,\"start\":39718},{\"end\":40716,\"start\":40662},{\"end\":45318,\"start\":45276},{\"end\":47145,\"start\":47102},{\"end\":50375,\"start\":50326},{\"end\":51713,\"start\":51656},{\"end\":54038,\"start\":54020},{\"end\":54662,\"start\":54619},{\"end\":56026,\"start\":55974},{\"end\":56700,\"start\":56680},{\"end\":57637,\"start\":57597},{\"end\":58811,\"start\":58801},{\"end\":58922,\"start\":58902},{\"end\":59350,\"start\":59330},{\"end\":59543,\"start\":59533},{\"end\":59597,\"start\":59587},{\"end\":64204,\"start\":64195},{\"end\":64745,\"start\":64736},{\"end\":64818,\"start\":64809},{\"end\":64880,\"start\":64871},{\"end\":64981,\"start\":64972}]", "table": "[{\"end\":64193,\"start\":60727},{\"end\":64734,\"start\":64396}]", "figure_caption": "[{\"end\":58900,\"start\":58813},{\"end\":59328,\"start\":58925},{\"end\":59531,\"start\":59353},{\"end\":59585,\"start\":59545},{\"end\":59863,\"start\":59599},{\"end\":60727,\"start\":59866},{\"end\":64251,\"start\":64206},{\"end\":64396,\"start\":64254},{\"end\":64807,\"start\":64747},{\"end\":64869,\"start\":64820},{\"end\":64970,\"start\":64882},{\"end\":65038,\"start\":64983}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29592,\"start\":29584},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30081,\"start\":30073},{\"end\":31516,\"start\":31508},{\"end\":31746,\"start\":31738},{\"end\":32763,\"start\":32755},{\"end\":35325,\"start\":35317},{\"end\":35613,\"start\":35605},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":37259,\"start\":37251},{\"end\":37807,\"start\":37799},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":38379,\"start\":38371},{\"end\":39176,\"start\":39168},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":41233,\"start\":41225},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":41316,\"start\":41308},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":41684,\"start\":41676},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":42115,\"start\":42107},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":42780,\"start\":42772},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":43039,\"start\":43031},{\"end\":46018,\"start\":46010},{\"end\":46518,\"start\":46510},{\"end\":46884,\"start\":46876},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":47505,\"start\":47497},{\"end\":50139,\"start\":50131},{\"end\":51081,\"start\":51073},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":52505,\"start\":52496},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":53342,\"start\":53333}]", "bib_author_first_name": "[{\"end\":66751,\"start\":66740},{\"end\":66764,\"start\":66760},{\"end\":66781,\"start\":66775},{\"end\":66793,\"start\":66788},{\"end\":66812,\"start\":66802},{\"end\":66828,\"start\":66821},{\"end\":66844,\"start\":66839},{\"end\":66858,\"start\":66852},{\"end\":66874,\"start\":66868},{\"end\":66894,\"start\":66883},{\"end\":67256,\"start\":67252},{\"end\":67270,\"start\":67264},{\"end\":67482,\"start\":67478},{\"end\":67496,\"start\":67490},{\"end\":67673,\"start\":67669},{\"end\":67685,\"start\":67681},{\"end\":67696,\"start\":67693},{\"end\":67710,\"start\":67704},{\"end\":67988,\"start\":67982},{\"end\":67996,\"start\":67995},{\"end\":68011,\"start\":68006},{\"end\":68225,\"start\":68219},{\"end\":68239,\"start\":68233},{\"end\":68255,\"start\":68248},{\"end\":68605,\"start\":68600},{\"end\":68622,\"start\":68616},{\"end\":68634,\"start\":68630},{\"end\":68647,\"start\":68642},{\"end\":68667,\"start\":68658},{\"end\":68677,\"start\":68674},{\"end\":68689,\"start\":68684},{\"end\":68701,\"start\":68697},{\"end\":68713,\"start\":68710},{\"end\":68726,\"start\":68722},{\"end\":68741,\"start\":68736},{\"end\":68753,\"start\":68748},{\"end\":69265,\"start\":69261},{\"end\":69280,\"start\":69274},{\"end\":69282,\"start\":69281},{\"end\":69631,\"start\":69628},{\"end\":69649,\"start\":69643},{\"end\":69990,\"start\":69984},{\"end\":70003,\"start\":69998},{\"end\":70021,\"start\":70013},{\"end\":70305,\"start\":70297},{\"end\":70319,\"start\":70315},{\"end\":70333,\"start\":70329},{\"end\":70348,\"start\":70341},{\"end\":70364,\"start\":70358},{\"end\":70379,\"start\":70373},{\"end\":70395,\"start\":70389},{\"end\":70899,\"start\":70893},{\"end\":70912,\"start\":70906},{\"end\":71206,\"start\":71200},{\"end\":71219,\"start\":71215},{\"end\":71231,\"start\":71227},{\"end\":71246,\"start\":71241},{\"end\":71263,\"start\":71257},{\"end\":71279,\"start\":71272},{\"end\":71793,\"start\":71789},{\"end\":71811,\"start\":71804},{\"end\":71826,\"start\":71820},{\"end\":71842,\"start\":71835},{\"end\":71844,\"start\":71843},{\"end\":71859,\"start\":71853},{\"end\":72202,\"start\":72198},{\"end\":72218,\"start\":72213},{\"end\":72235,\"start\":72227},{\"end\":72250,\"start\":72246},{\"end\":72264,\"start\":72260},{\"end\":72528,\"start\":72522},{\"end\":72545,\"start\":72541},{\"end\":72563,\"start\":72555},{\"end\":72580,\"start\":72574},{\"end\":72594,\"start\":72590},{\"end\":72606,\"start\":72602},{\"end\":72621,\"start\":72614},{\"end\":72977,\"start\":72973},{\"end\":73261,\"start\":73257},{\"end\":73279,\"start\":73275},{\"end\":73291,\"start\":73287},{\"end\":73552,\"start\":73545},{\"end\":73554,\"start\":73553},{\"end\":73568,\"start\":73563},{\"end\":73570,\"start\":73569},{\"end\":73584,\"start\":73583},{\"end\":73601,\"start\":73595},{\"end\":74002,\"start\":73995},{\"end\":74018,\"start\":74012},{\"end\":74035,\"start\":74029},{\"end\":74048,\"start\":74044},{\"end\":74065,\"start\":74058},{\"end\":74360,\"start\":74355},{\"end\":74374,\"start\":74370},{\"end\":74395,\"start\":74387},{\"end\":74397,\"start\":74396},{\"end\":74416,\"start\":74409},{\"end\":74432,\"start\":74426},{\"end\":74449,\"start\":74441},{\"end\":74462,\"start\":74457},{\"end\":74464,\"start\":74463},{\"end\":74478,\"start\":74471},{\"end\":74491,\"start\":74487},{\"end\":74505,\"start\":74500},{\"end\":74523,\"start\":74516},{\"end\":74531,\"start\":74528},{\"end\":74546,\"start\":74540},{\"end\":75093,\"start\":75088},{\"end\":75107,\"start\":75101},{\"end\":75126,\"start\":75119},{\"end\":75143,\"start\":75137},{\"end\":75154,\"start\":75149},{\"end\":75174,\"start\":75167},{\"end\":75519,\"start\":75514},{\"end\":75533,\"start\":75527},{\"end\":75548,\"start\":75543},{\"end\":75555,\"start\":75549},{\"end\":75572,\"start\":75565},{\"end\":75574,\"start\":75573},{\"end\":75913,\"start\":75912},{\"end\":76174,\"start\":76168},{\"end\":76185,\"start\":76179},{\"end\":76195,\"start\":76192},{\"end\":76208,\"start\":76203},{\"end\":76217,\"start\":76215},{\"end\":76226,\"start\":76224},{\"end\":76235,\"start\":76232},{\"end\":76246,\"start\":76242},{\"end\":76257,\"start\":76252},{\"end\":76272,\"start\":76263},{\"end\":76286,\"start\":76277},{\"end\":76295,\"start\":76292},{\"end\":76306,\"start\":76301},{\"end\":76320,\"start\":76313},{\"end\":76331,\"start\":76326},{\"end\":76339,\"start\":76336},{\"end\":76353,\"start\":76346},{\"end\":76364,\"start\":76361},{\"end\":76842,\"start\":76836},{\"end\":76861,\"start\":76854},{\"end\":76879,\"start\":76872},{\"end\":76896,\"start\":76889}]", "bib_author_last_name": "[{\"end\":66758,\"start\":66752},{\"end\":66773,\"start\":66765},{\"end\":66786,\"start\":66782},{\"end\":66800,\"start\":66794},{\"end\":66819,\"start\":66813},{\"end\":66837,\"start\":66829},{\"end\":66850,\"start\":66845},{\"end\":66866,\"start\":66859},{\"end\":66881,\"start\":66875},{\"end\":66900,\"start\":66895},{\"end\":67262,\"start\":67257},{\"end\":67279,\"start\":67271},{\"end\":67488,\"start\":67483},{\"end\":67505,\"start\":67497},{\"end\":67679,\"start\":67674},{\"end\":67691,\"start\":67686},{\"end\":67702,\"start\":67697},{\"end\":67719,\"start\":67711},{\"end\":67993,\"start\":67989},{\"end\":68004,\"start\":67997},{\"end\":68018,\"start\":68012},{\"end\":68231,\"start\":68226},{\"end\":68246,\"start\":68240},{\"end\":68263,\"start\":68256},{\"end\":68614,\"start\":68606},{\"end\":68628,\"start\":68623},{\"end\":68640,\"start\":68635},{\"end\":68656,\"start\":68648},{\"end\":68672,\"start\":68668},{\"end\":68682,\"start\":68678},{\"end\":68695,\"start\":68690},{\"end\":68708,\"start\":68702},{\"end\":68720,\"start\":68714},{\"end\":68734,\"start\":68727},{\"end\":68746,\"start\":68742},{\"end\":68765,\"start\":68754},{\"end\":69272,\"start\":69266},{\"end\":69291,\"start\":69283},{\"end\":69641,\"start\":69632},{\"end\":69658,\"start\":69650},{\"end\":69996,\"start\":69991},{\"end\":70011,\"start\":70004},{\"end\":70030,\"start\":70022},{\"end\":70313,\"start\":70306},{\"end\":70327,\"start\":70320},{\"end\":70339,\"start\":70334},{\"end\":70356,\"start\":70349},{\"end\":70371,\"start\":70365},{\"end\":70387,\"start\":70380},{\"end\":70403,\"start\":70396},{\"end\":70904,\"start\":70900},{\"end\":70923,\"start\":70913},{\"end\":71213,\"start\":71207},{\"end\":71225,\"start\":71220},{\"end\":71239,\"start\":71232},{\"end\":71255,\"start\":71247},{\"end\":71270,\"start\":71264},{\"end\":71287,\"start\":71280},{\"end\":71802,\"start\":71794},{\"end\":71818,\"start\":71812},{\"end\":71833,\"start\":71827},{\"end\":71851,\"start\":71845},{\"end\":71866,\"start\":71860},{\"end\":72211,\"start\":72203},{\"end\":72225,\"start\":72219},{\"end\":72244,\"start\":72236},{\"end\":72258,\"start\":72251},{\"end\":72271,\"start\":72265},{\"end\":72539,\"start\":72529},{\"end\":72553,\"start\":72546},{\"end\":72572,\"start\":72564},{\"end\":72588,\"start\":72581},{\"end\":72600,\"start\":72595},{\"end\":72612,\"start\":72607},{\"end\":72629,\"start\":72622},{\"end\":72989,\"start\":72978},{\"end\":73273,\"start\":73262},{\"end\":73285,\"start\":73280},{\"end\":73297,\"start\":73292},{\"end\":73561,\"start\":73555},{\"end\":73581,\"start\":73571},{\"end\":73593,\"start\":73585},{\"end\":73607,\"start\":73602},{\"end\":73616,\"start\":73609},{\"end\":74010,\"start\":74003},{\"end\":74027,\"start\":74019},{\"end\":74042,\"start\":74036},{\"end\":74056,\"start\":74049},{\"end\":74073,\"start\":74066},{\"end\":74368,\"start\":74361},{\"end\":74385,\"start\":74375},{\"end\":74407,\"start\":74398},{\"end\":74424,\"start\":74417},{\"end\":74439,\"start\":74433},{\"end\":74455,\"start\":74450},{\"end\":74469,\"start\":74465},{\"end\":74485,\"start\":74479},{\"end\":74498,\"start\":74492},{\"end\":74514,\"start\":74506},{\"end\":74526,\"start\":74524},{\"end\":74538,\"start\":74532},{\"end\":74553,\"start\":74547},{\"end\":75099,\"start\":75094},{\"end\":75117,\"start\":75108},{\"end\":75135,\"start\":75127},{\"end\":75147,\"start\":75144},{\"end\":75165,\"start\":75155},{\"end\":75182,\"start\":75175},{\"end\":75525,\"start\":75520},{\"end\":75541,\"start\":75534},{\"end\":75563,\"start\":75556},{\"end\":75582,\"start\":75575},{\"end\":75920,\"start\":75914},{\"end\":75930,\"start\":75922},{\"end\":76177,\"start\":76175},{\"end\":76190,\"start\":76186},{\"end\":76201,\"start\":76196},{\"end\":76213,\"start\":76209},{\"end\":76222,\"start\":76218},{\"end\":76230,\"start\":76227},{\"end\":76240,\"start\":76236},{\"end\":76250,\"start\":76247},{\"end\":76261,\"start\":76258},{\"end\":76275,\"start\":76273},{\"end\":76290,\"start\":76287},{\"end\":76299,\"start\":76296},{\"end\":76311,\"start\":76307},{\"end\":76324,\"start\":76321},{\"end\":76334,\"start\":76332},{\"end\":76344,\"start\":76340},{\"end\":76359,\"start\":76354},{\"end\":76368,\"start\":76365},{\"end\":76852,\"start\":76843},{\"end\":76870,\"start\":76862},{\"end\":76887,\"start\":76880},{\"end\":76905,\"start\":76897}]", "bib_entry": "[{\"attributes\":{\"doi\":\"abs/1912.06680\",\"id\":\"b0\"},\"end\":67173,\"start\":66687},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":5003977},\"end\":67439,\"start\":67175},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":195892791},\"end\":67626,\"start\":67441},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":53183381},\"end\":67947,\"start\":67628},{\"attributes\":{\"id\":\"b4\"},\"end\":68150,\"start\":67949},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":198179564},\"end\":68507,\"start\":68152},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3645060},\"end\":69171,\"start\":68509},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6644398},\"end\":69521,\"start\":69173},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6421891},\"end\":69924,\"start\":69523},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1902965},\"end\":70240,\"start\":69926},{\"attributes\":{\"id\":\"b10\"},\"end\":70828,\"start\":70242},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2713175},\"end\":71076,\"start\":70830},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":52180170},\"end\":71712,\"start\":71078},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":3075448},\"end\":72155,\"start\":71714},{\"attributes\":{\"doi\":\"abs/1707.06347\",\"id\":\"b14\"},\"end\":72438,\"start\":72157},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":53046335},\"end\":72971,\"start\":72440},{\"attributes\":{\"doi\":\"arXiv:1901.07621\",\"id\":\"b16\"},\"end\":73173,\"start\":72973},{\"attributes\":{\"id\":\"b17\"},\"end\":73463,\"start\":73175},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1211821},\"end\":73924,\"start\":73465},{\"attributes\":{\"doi\":\"abs/2004.09677\",\"id\":\"b19\",\"matched_paper_id\":216035997},\"end\":74277,\"start\":73926},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":204972004},\"end\":75049,\"start\":74279},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6798514},\"end\":75463,\"start\":75051},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7252167},\"end\":75819,\"start\":75465},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2332513},\"end\":76100,\"start\":75821},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":227162956},\"end\":76776,\"start\":76102},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":7739250},\"end\":77185,\"start\":76778}]", "bib_title": "[{\"end\":67250,\"start\":67175},{\"end\":67476,\"start\":67441},{\"end\":67667,\"start\":67628},{\"end\":68217,\"start\":68152},{\"end\":68598,\"start\":68509},{\"end\":69259,\"start\":69173},{\"end\":69626,\"start\":69523},{\"end\":69982,\"start\":69926},{\"end\":70891,\"start\":70830},{\"end\":71198,\"start\":71078},{\"end\":71787,\"start\":71714},{\"end\":72520,\"start\":72440},{\"end\":73543,\"start\":73465},{\"end\":73993,\"start\":73926},{\"end\":74353,\"start\":74279},{\"end\":75086,\"start\":75051},{\"end\":75512,\"start\":75465},{\"end\":75910,\"start\":75821},{\"end\":76166,\"start\":76102},{\"end\":76834,\"start\":76778}]", "bib_author": "[{\"end\":66760,\"start\":66740},{\"end\":66775,\"start\":66760},{\"end\":66788,\"start\":66775},{\"end\":66802,\"start\":66788},{\"end\":66821,\"start\":66802},{\"end\":66839,\"start\":66821},{\"end\":66852,\"start\":66839},{\"end\":66868,\"start\":66852},{\"end\":66883,\"start\":66868},{\"end\":66902,\"start\":66883},{\"end\":67264,\"start\":67252},{\"end\":67281,\"start\":67264},{\"end\":67490,\"start\":67478},{\"end\":67507,\"start\":67490},{\"end\":67681,\"start\":67669},{\"end\":67693,\"start\":67681},{\"end\":67704,\"start\":67693},{\"end\":67721,\"start\":67704},{\"end\":67995,\"start\":67982},{\"end\":68006,\"start\":67995},{\"end\":68020,\"start\":68006},{\"end\":68233,\"start\":68219},{\"end\":68248,\"start\":68233},{\"end\":68265,\"start\":68248},{\"end\":68616,\"start\":68600},{\"end\":68630,\"start\":68616},{\"end\":68642,\"start\":68630},{\"end\":68658,\"start\":68642},{\"end\":68674,\"start\":68658},{\"end\":68684,\"start\":68674},{\"end\":68697,\"start\":68684},{\"end\":68710,\"start\":68697},{\"end\":68722,\"start\":68710},{\"end\":68736,\"start\":68722},{\"end\":68748,\"start\":68736},{\"end\":68767,\"start\":68748},{\"end\":69274,\"start\":69261},{\"end\":69293,\"start\":69274},{\"end\":69643,\"start\":69628},{\"end\":69660,\"start\":69643},{\"end\":69998,\"start\":69984},{\"end\":70013,\"start\":69998},{\"end\":70032,\"start\":70013},{\"end\":70315,\"start\":70297},{\"end\":70329,\"start\":70315},{\"end\":70341,\"start\":70329},{\"end\":70358,\"start\":70341},{\"end\":70373,\"start\":70358},{\"end\":70389,\"start\":70373},{\"end\":70405,\"start\":70389},{\"end\":70906,\"start\":70893},{\"end\":70925,\"start\":70906},{\"end\":71215,\"start\":71200},{\"end\":71227,\"start\":71215},{\"end\":71241,\"start\":71227},{\"end\":71257,\"start\":71241},{\"end\":71272,\"start\":71257},{\"end\":71289,\"start\":71272},{\"end\":71804,\"start\":71789},{\"end\":71820,\"start\":71804},{\"end\":71835,\"start\":71820},{\"end\":71853,\"start\":71835},{\"end\":71868,\"start\":71853},{\"end\":72213,\"start\":72198},{\"end\":72227,\"start\":72213},{\"end\":72246,\"start\":72227},{\"end\":72260,\"start\":72246},{\"end\":72273,\"start\":72260},{\"end\":72541,\"start\":72522},{\"end\":72555,\"start\":72541},{\"end\":72574,\"start\":72555},{\"end\":72590,\"start\":72574},{\"end\":72602,\"start\":72590},{\"end\":72614,\"start\":72602},{\"end\":72631,\"start\":72614},{\"end\":72991,\"start\":72973},{\"end\":73275,\"start\":73257},{\"end\":73287,\"start\":73275},{\"end\":73299,\"start\":73287},{\"end\":73563,\"start\":73545},{\"end\":73583,\"start\":73563},{\"end\":73595,\"start\":73583},{\"end\":73609,\"start\":73595},{\"end\":73618,\"start\":73609},{\"end\":74012,\"start\":73995},{\"end\":74029,\"start\":74012},{\"end\":74044,\"start\":74029},{\"end\":74058,\"start\":74044},{\"end\":74075,\"start\":74058},{\"end\":74370,\"start\":74355},{\"end\":74387,\"start\":74370},{\"end\":74409,\"start\":74387},{\"end\":74426,\"start\":74409},{\"end\":74441,\"start\":74426},{\"end\":74457,\"start\":74441},{\"end\":74471,\"start\":74457},{\"end\":74487,\"start\":74471},{\"end\":74500,\"start\":74487},{\"end\":74516,\"start\":74500},{\"end\":74528,\"start\":74516},{\"end\":74540,\"start\":74528},{\"end\":74555,\"start\":74540},{\"end\":75101,\"start\":75088},{\"end\":75119,\"start\":75101},{\"end\":75137,\"start\":75119},{\"end\":75149,\"start\":75137},{\"end\":75167,\"start\":75149},{\"end\":75184,\"start\":75167},{\"end\":75527,\"start\":75514},{\"end\":75543,\"start\":75527},{\"end\":75565,\"start\":75543},{\"end\":75584,\"start\":75565},{\"end\":75922,\"start\":75912},{\"end\":75932,\"start\":75922},{\"end\":76179,\"start\":76168},{\"end\":76192,\"start\":76179},{\"end\":76203,\"start\":76192},{\"end\":76215,\"start\":76203},{\"end\":76224,\"start\":76215},{\"end\":76232,\"start\":76224},{\"end\":76242,\"start\":76232},{\"end\":76252,\"start\":76242},{\"end\":76263,\"start\":76252},{\"end\":76277,\"start\":76263},{\"end\":76292,\"start\":76277},{\"end\":76301,\"start\":76292},{\"end\":76313,\"start\":76301},{\"end\":76326,\"start\":76313},{\"end\":76336,\"start\":76326},{\"end\":76346,\"start\":76336},{\"end\":76361,\"start\":76346},{\"end\":76370,\"start\":76361},{\"end\":76854,\"start\":76836},{\"end\":76872,\"start\":76854},{\"end\":76889,\"start\":76872},{\"end\":76907,\"start\":76889}]", "bib_venue": "[{\"end\":66738,\"start\":66687},{\"end\":67288,\"start\":67281},{\"end\":67514,\"start\":67507},{\"end\":67772,\"start\":67721},{\"end\":67980,\"start\":67949},{\"end\":68309,\"start\":68265},{\"end\":68818,\"start\":68767},{\"end\":69332,\"start\":69293},{\"end\":69716,\"start\":69660},{\"end\":70074,\"start\":70032},{\"end\":70295,\"start\":70242},{\"end\":70937,\"start\":70925},{\"end\":71350,\"start\":71289},{\"end\":71920,\"start\":71868},{\"end\":72196,\"start\":72157},{\"end\":72690,\"start\":72631},{\"end\":73053,\"start\":73007},{\"end\":73255,\"start\":73175},{\"end\":73677,\"start\":73618},{\"end\":74093,\"start\":74089},{\"end\":74561,\"start\":74555},{\"end\":75249,\"start\":75184},{\"end\":75626,\"start\":75584},{\"end\":75948,\"start\":75932},{\"end\":76429,\"start\":76370},{\"end\":76966,\"start\":76907},{\"end\":71398,\"start\":71352}]"}}}, "year": 2023, "month": 12, "day": 17}