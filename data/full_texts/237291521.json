{"id": 237291521, "updated": "2023-10-05 23:39:36.599", "metadata": {"title": "Multi-Task Self-Training for Learning General Representations", "authors": "[{\"first\":\"Golnaz\",\"last\":\"Ghiasi\",\"middle\":[]},{\"first\":\"Barret\",\"last\":\"Zoph\",\"middle\":[]},{\"first\":\"Ekin\",\"last\":\"Cubuk\",\"middle\":[\"D.\"]},{\"first\":\"Quoc\",\"last\":\"Le\",\"middle\":[\"V.\"]},{\"first\":\"Tsung-Yi\",\"last\":\"Lin\",\"middle\":[]}]", "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2021, "month": 8, "day": 25}, "abstract": "Despite the fast progress in training specialized models for various tasks, learning a single general model that works well for many tasks is still challenging for computer vision. Here we introduce multi-task self-training (MuST), which harnesses the knowledge in independent specialized teacher models (e.g., ImageNet model on classification) to train a single general student model. Our approach has three steps. First, we train specialized teachers independently on labeled datasets. We then use the specialized teachers to label an unlabeled dataset to create a multi-task pseudo labeled dataset. Finally, the dataset, which now contains pseudo labels from teacher models trained on different datasets/tasks, is then used to train a student model with multi-task learning. We evaluate the feature representations of the student model on 6 vision tasks including image recognition (classification, detection, segmentation)and 3D geometry estimation (depth and surface normal estimation). MuST is scalable with unlabeled or partially labeled datasets and outperforms both specialized supervised models and self-supervised models when training on large scale datasets. Lastly, we show MuST can improve upon already strong checkpoints trained with billions of examples. The results suggest self-training is a promising direction to aggregate labeled and unlabeled training data for learning general feature representations.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2108.11353", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/GhiasiZCLL21", "doi": "10.1109/iccv48922.2021.00873"}}, "content": {"source": {"pdf_hash": "12ce370b38cc69403e81980f33b413650900105c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2108.11353v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "0698ed7eb6d6afafd4d0a70756fc73ccf92ff7f4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/12ce370b38cc69403e81980f33b413650900105c.txt", "contents": "\nMulti-Task Self-Training for Learning General Representations\n\n\nGolnaz Ghiasi golnazg@google.com \nGoogle Research\nBrain Team\n\nBarret Zoph barretzoph@google.com \nGoogle Research\nBrain Team\n\nEkin D Cubuk cubuk@google.com \nGoogle Research\nBrain Team\n\nQuoc V Le \nGoogle Research\nBrain Team\n\nTsung-Yi Lin \nGoogle Research\nBrain Team\n\nMulti-Task Self-Training for Learning General Representations\n\nDespite the fast progress in training specialized models for various tasks, learning a single general model that works well for many tasks is still challenging for computer vision. Here we introduce multi-task self-training (MuST), which harnesses the knowledge in independent specialized teacher models (e.g., ImageNet model on classification) to train a single general student model. Our approach has three steps. First, we train specialized teachers independently on labeled datasets. We then use the specialized teachers to label an unlabeled dataset to create a multitask pseudo labeled dataset. Finally, the dataset, which now contains pseudo labels from teacher models trained on different datasets/tasks, is then used to train a student model with multi-task learning. We evaluate the feature representations of the student model on 6 vision tasks including image recognition (classification, detection, segmentation) and 3D geometry estimation (depth and surface normal estimation). MuST is scalable with unlabeled or partially labeled datasets and outperforms both specialized supervised models and self-supervised models when training on large scale datasets. Lastly, we show MuST can improve upon already strong checkpoints [24] trained with billions of examples. The results suggest self-training is a promising direction to aggregate labeled and unlabeled training data for learning general feature representations.\n\nIntroduction\n\nVisual representation learning is a core problem in computer vision. Supervised and self-supervised pre-training have shown promising results in transferring the learned feature representations to downstream tasks. Typically, a model is pre-trained with a supervised [30,11] or a selfsupervised objective [5,17,18]. Despite the wide adoption of transfer learning from supervised training, the features may not necessarily be useful for downstream tasks. For example, He et al. found that ImageNet pre-training fails * Authors contributed equally.  Specialized Teacher represents a supervised model trained on a single task and dataset (e.g., classification model trained on Im-ageNet). Specialized Teacher models are trained independently on their own tasks and datasets. They then generate pseudo labels on a shared dataset. Finally, a single General Student model is trained jointly using the pseudo (and supervised) labels on the shared dataset.\n\nto improve COCO instance segmentation [19]. In contrast, Shao et al. showed features learned from Objects365 detection dataset improve COCO instance segmentation by a large margin [49]. Pre-training with a specialized task that aligns with the downstream target task still yields the best performance in object detection [34,49] and semantic segmentation [4]. Intuitively, it is possible to learn general features by training a model to simultaneously do well on multiple tasks. Recent work in NLP started to show promising results on learning a generalist model with multi-task learning [60,9]. In computer vision, the biggest challenge of training a multi-task model is in the data collection and annotation. Despite datasets like COCO [37], collecting a wide variety of annotations (e.g., instance segmentation, person keypoints, image caption) for the same image dataset is quite challenging. Due to the time consuming nature of annotating images with labels, it is hard to scale such efforts with the number of images and the number of tasks. The lack of large scale multi-task datasets impedes the progress in multi-task learning for computer vision.\n\nIn this work, we study using self-training to remedy the issue. We propose to use pseudo labeling to enable large scale multi-task feature learning for computer vision. Zoph et al. [67] observed that self-training further improves pretraining for transfer learning, and that self-training works even when pre-training fails to outperform a randomly initialized model. The gap between pre-training and selftraining suggests that self-training can learn better features from pseudo labels. Inspired by this observation, we first investigate whether good features can be learned by only using pseudo labels. We train teacher models using datasets such as COCO or Objects365 to generate pseudo labels on unlabeled images. Figure 2 shows example pseudo labels on ImageNet. Surprisingly, we find a student model trained with only these pseudo labels preserves most of the transfer learning performance of its specialized teacher model. This finding suggests pseudo labels are effective at distilling the knowledge in a supervised dataset. Therefore, we can use pseudo labels to transfer knowledge from multiple teacher models to a single student model for representation learning.\n\nWe propose Multi-Task Self-Training (MuST) to train a generalist student model on the information distilled from teacher models trained on different tasks and datasets. Figure 1 shows the overview of the algorithm. MuST has three steps. First, it trains specialized teachers independently on labeled datasets. For example, one teacher can be trained with depth prediction and another teacher can be trained with object detection. The specialized teachers are then used to label a larger unlabeled dataset to create a multitask pseudo labeled dataset. For example, these teachers can generate depth estimations and object detections on the ImageNet dataset. Finally, the dataset, which now contains pseudo labels from teacher models trained on different datasets/tasks, is used to train a student model with multitask learning. Hence the student, for example, can do depth prediction and object detection at the same time.\n\nIn our experiments, we have four teacher models: classification, semantic segmentation, object box detection, and depth estimation. We design a simple model architecture ( Figure 3) based on ResNet [21] and feature pyramid networks (FPN) [36]. The parameters in the ResNet-FPN backbone are shared across different tasks. For each individual task, it has a small task-specific head consisting of a few convolution layers followed by a linear prediction layer. Our experiments show that this simple model architecture is able to absorb the knowledge of different tasks in the shared backbone. The generalist student model is on par with/outperforms its specialist teacher models for all transfer learning tasks.\n\nThe recent self-supervised algorithms like SimCLR [5], MoCo [18] are shown to create representations that are on par or better than its supervised counterpart. In our experiments, MuST also outperforms SimCLR [5] by a large margin on segmentation and depth estimation tasks. We also observe that the representations learned by SimCLR is on Figure 2. Examples of pseudo labels on ImageNet. Left: bounding boxes labeled with an Objects365 teacher model. Middle: semantic segmentation labeled with a COCO teacher model. Right: depth labeled with a MiDaS teacher model. par with those of supervised learning on ImageNet (1.3M images) but does not scale as well on JFT (300M images). On the contrary, MuST outperforms SimCLR [5] on both ImageNet and JFT. Moreover, MuST also outperforms supervised JFT pre-training for 5 out of 6 tasks except the image classification task. The results indicate the potential of MuST in learning general feature representations that improve with more unlabeled data.\n\nLastly, we show MuST can improve upon already strong checkpoints such as ALIGN [24]. We fine-tune ALIGN checkpoints, previously trained with billions of supervised examples, with MuST pseudo labels and find improvements on a suite of downstream tasks: detection, segmentation, and depth estimation tasks.\n\nWe summarize our contributions below:\n\n\u2022 We propose Multi-Task Self-Training (MuST), a simple algorithm for creating general visual representations by multi-task learning with pseudo labels.\n\n\u2022 We conduct experiments by jointly training across several datasets (e.g., ImageNet, Objects365, COCO, JFT) to learn general feature representations that outperforms representations learned by supervised and self-supervised methods.\n\n\u2022 We perform experiments to compare supervised, selfsupervised, and MuST on 6 computer vision tasks including tasks in image recognition (classification, detection, segmentation) and 3D geometry estimation (depth and surface normal estimation).\n\n\u2022 MuST can be used to improve upon already strong checkpoints and achieve competitive results on a variety of tasks compared to task-specific state-of-the-art models.\n\n\nRelated Work\n\nMulti-Task learning: Multi-task learning has a rich history in deep learning [46]. A common strategy for multitask learning is to share the hidden layers of a \"backbone\" model for different tasks [2]. More recently, multi-task learning has led to improved accuracy in NLP [9,38]. Although, Raffel et al. found that multi-task learning generally underperformed compared to pre-training followed by fine-tuning [42].\n\nIn the vision domain, Zamir et al. studied the transfer learning dependencies across 26 tasks with an indoor dataset [64]. Instead of exploring the task dependencies, we are interested in pushing a single model that can absorb knowledge of all tasks for learning general representations. Kokkinos et al. [29] and Xiaoet al. [57] trained models across multiple datasets by simply zeroing losses for examples that don't have labels for a particular task. We propose to apply pseudo labels so every image is annotated with all tasks. Girshick et al. used a multi-task loss for classification and bounding-box regression to improve the training of object detectors [15]. We follow the similar approach of using one large backbone model and smaller heads for multiple tasks.\n\nSelf-training: Self-training is a popular technique to incorporate unlabeled data into supervised learning [62,48,45,33]. The method works by using a supervised model to generate pseudo labels on unlabeled data. Then a student model is trained on the pseudo labeled data. Yalniz et al. [61] showed a model \"pre-trained\" with pseudo labels on a large unlabeled dataset (at hundreds millions scale) can improve classification accuracy. Noisy Student [58] used self-training to push state-of-the-art performance on ImageNet by training jointly with 130M pseudo labeled images. Chen et al. [3] obtained state-of-the-art panoptic segmentation results on Cityscapes with self-training. Zoph et al. [67] improved the state-of-the-art on object detection and semantic segmentation with self-training. All the above works focused on a single task. On the contrary, our work focuses on using self-training for multi-task learning to learn general representations.\n\nRepresentation learning: Transfer learning from Ima-geNet pre-training has been the most widely used method in computer vision. BiT [30] and ViT [11] pre-trained the model on JFT-300M dataset [51] and obtained strong performance when fine-tuned on downstream vision tasks. In particular, Mahajan et al. showed model pre-trained with Instagram benefits other classification tasks but possibly harms localization performance [39]. Li et al. found that OpenImagesV4 pre-training [32] outperforms ImageNet pre-training when transferring to object detection and semantic segmentation [34]. Shao et al. showed similar findings using the Objects365 dataset [49]. This finding indicates supervised pre-training on a single classification task may not create representations general enough for many kinds of downstream applications.  Cyan: multi-scale features for box detection and instance segmentation. Green: the high resolution features for pixel-wise tasks (e.g., segmentation, depth, and surface normal estimation.) Self-supervised training is a popular method for representation learning without supervised data [25,5,17,18,22,53]. By forcing the representations of an image to agree with each other under data augmentation [1], SimCLR and MoCo trained representations useful for downstream classification tasks [5,18]. Grill et al. proposed the use of online and target neural networks for learning representations, which they evaluated on classification tasks as well as semantic segmentation, object detection, and depth estimation [17]. On the other hand, recent work has demonstrated the limitations of current self-supervised learning methods [41]. They found that aggressive cropping, commonly used in self-supervised learning (such as those used in MoCo [18], PIRL [40], SimCLR [5] etc.), leads to representations that are occlusion invariant, which can be effective for downstream classification tasks. However, these representations are not necessarily invariant to other symmetries of natural images (such as viewpoint invariance), which might be necessary for other downstream tasks such as semantic segmentation [41].\n\n\nMethod\n\n\nSpecialized Teacher Models\n\nWe want to learn from a set of teachers that provide rich training signals with their pseudo labels. We adopt four teacher models including four important tasks in computer vision: classification, detection, segmentation, and depth estimation. These tasks require visual understanding of objects and 3D geometry. Examples of the pseudo labels can be found in Figure 2. We train the classification, detection, and segmentation teacher models from scratch on medium/large scale datasets (e.g., ImageNet [47], Ob-jects365 [49], COCO [28]). For depth teacher model, we download the pre-trained checkpoint from the open-source repository [44] 1 .\n\n\nPseudo labeling:\n\nWe transfer the knowledge in specialized teacher models to unlabeled or partially labeled datasets by pseudo labeling. We follow the practice in [67] to generate pseudo labels for detection and segmentation. For detection, we use a hard score threshold of 0.5 to generate pseudo box labels. For segmentation, we use a hard score threshold of 0.5 to generate semantic segmentation masks whereas pixels with a smaller prediction score are set to the ignore label. For classification, we use soft labels, which contain the probability distribution of all classes, because we find the performance is better than hard labels. For depth, we simply use the predicted depth as pseudo labels without further processing.\n\n\nMulti-Task Student Model\n\nModel architecture: Our goal is to train the student with multiple tasks to learn general visual representations. The first thing to design is a model architecture that can share most of the parameters across tasks. We define three task categories: (1) classification, (2) object detection, (3) pixelwise prediction. The pixel-wise prediction task includes semantic segmentation, depth estimation, and surface normal prediction. Each category of task shares the same feature representations in the backbone model.\n\nWe design the backbone model based on ResNet [21] and feature pyramid networks (FPN) [36]. Figure 3 shows the overview of our architecture. We follow the common practice to design the feature representations for classification and detection tasks. We use C5 feature map (orange) for classification and {P3, P4, P5, P6, P7} feature pyramid (cyan) for detection. We follow the practice in [67] to fuse {P3, P4, P5, P6, P7} into P2 feature map (green) for pixel-wise prediction. The fuse operation simply rescales all feature maps into level 2 and sums them (which does not introduce any new parameters).\n\nEach task category shares the same head architecture. The classification head follows the ResNet design. It is a linear prediction layer followed by average pooled C5 features. The object detection task follows the head architecture in the Mask R-CNN [20]. We use 2 hidden convolution layers for RPN and 4 hidden convolution layers and 1 fully connected layers for Fast R-CNN. The pixel-wise prediction head has 3 convolution layers followed by the C2 features before the final linear prediction layer. If the student model learns from multiple tasks in the same task category (e.g., semantic segmentation and depth prediction), each task owns its task specific head without sharing their parameters.\n\nTeacher-student training: We want to study the effectiveness of learning from pseudo labels. Therefore, we design the training of teacher and student models such that the main differences between them are in the dataset and the labels. Unlike model distillation [23] and noisy student [58], we use the same model capacity and data augmentation in both the teacher and the student training. Despite that a teacher can be trained with a more specialized architecture for its own task, we train the teacher and student models using the same architecture shown in Figure 3.\n\nLearning From Multiple Teachers: We propose Multi-Task Self-Training (MuST) to train a student model with multiple teachers. Prior multi-task learning works, which harnessed the information in multiple datasets, mainly focused on the scenario where each example is only labeled with one task or a few tasks [9,29]. In MuST, every image has supervision for all tasks. The labels may come from supervised or pseudo labels. For example, when training on ImageNet, we can use supervised labels for classification and pseudo labels for detection, segmentation, and depth.\n\nBalancing the loss contribution for each task is an open research area in multi-task learning [26,6,7,63]. The loss of multi-task learning is the weighted sum of the losses from all tasks L = i w i L i . The weight w i decides the loss contribution for the task i. In ImageNet experiments, we adopt w i = bslrit bitlrs , where b denotes the batch size, lr denotes the learning rate, and the subscript denotes student or teacher model. The equation is derived from the scaling rule in [16], which scales the learning rate propotionally with batch size. The only exception is the depth loss, of which we choose its weight by a parameter sweep. In our experiments on JFT-300M, we use the algorithm in [26] to learn w i for each task over the course of training.\n\nCross Dataset Training: MuST has the flexibility to leverage both labeled and unlabeled data. It can scale up the number of images by generating pseudo labels on the unlabeled data. Or it can use images which are partially labeled with one or more tasks. In our experiments, we show an example training across ImageNet, objects365, and COCO datasets. We use supervised labels whenever they are available and generate labels for all absent tasks using pseudo labels.\n\nOne challenge in cross dataset training is to balance the data coming from datasets of different sizes. Instead of designing sampling heuristics [9], we uniformly sample from the union of datasets. This works because every task is labeled on every image in MuST, thus we do not need to worry about under/over-sampling a task due to the imbalanced dataset size.\n\nThe second main difference compared to other selftraining algorithm is that the supervised and pseudo labels are treated equally. We do not batch the examples of supervised and pseudo labels independently and assign them different weights like in [67,58]. The images are uniformly sampled from the union of datasets and put into one minibatch. Each example shares the same weight on its loss regardless if the loss is computed against a supervised or a \n\n\nTransfer Learning\n\nTo evaluate the representational quality of MuST and other baseline representations, we fine-tune them on a suite of downstream computer visions tasks. We adopt end-toend fine-tuning instead of linear probe to the performance of each fine-tuning task. We fine-tune on CIFAR-100 classification, Pascal detection, Pascal semantic segmentation, NYU depth, ADE semantic segmentation and DIODE surface normal. Also note that all downstream datasets are different than the ones the specialized teacher models are trained on. Furthermore, surface normal prediction is a task that no specialized teacher model was trained for, testing the robustness of representations to held out tasks.\n\nWhen fine-tuning a representation on a downstream task we sweep over the learning rate and number of training steps (See Appendix for full details). This allows for fair comparison between different representations.\n\n\nExperiments\n\n\nExperimental Settings\n\nTraining Datasets: Table 1 provides an overview of the datasets we use in the experiments. We experiment with four different datasets and tasks for training our supervised teacher models. These supervised models will then be the ones to generate pseudo labels on unlabeled/partially labeled images.\n\nEvaluation Datasets: Next we describe the datasets that all of our representations will be fine-tuned on. Table 1 provides the list. We have different datasets with a total of five different tasks. Note the Surface Normal task is never used as a training task to test the task generality of the representations.\n\n\nLearning with Multi-Task Self-Training\n\nWe run experiments to compare our MuST representation learning algorithm to state-of-the-art self-supervised and supervised learning methods.\n\nMuST Improves Pre-training on ImageNet: Table 2 compares the MuST algorithm to self-supervised and supervised learning on ImageNet. On a suite of 6 downstream tasks MuST representations improves over state-of-the-art self-supervised learning and supervised learning on 4 and 5 tasks, respectively. MuST makes use of not only the Ima-geNet classification labels, but also pseudo labels generated from supervised models trained on Objects365 detection, COCO semantic segmentation, and MiDas depth. This additional information being trained on for ImageNet images leads to much more generalizable feature representations. We observe that self-supervised and supervised pre-training on ImageNet does not learn features that generalize nearly as well to tasks other than image classification.\n\nMuST Improves With More Tasks/Datasets For Learning General Features: The MuST algorithm makes use of pseudo labels generated from independent supervised models trained on different datasets. We next study the importance of having pseudo labels being generated from multiple different teacher models trained on different tasks/datasets. Table 3 shows the representational quality improvement starting from using only supervised ImageNet labels and then adding three different types of pseudo labels obtained from three different datasets. As we continue to add pseudo labels from different tasks/datasets our representations improve in quality. For each new task added we obtain strong improvement across all 6 of our downstream tasks.\n\nPseudo Labels Preserve Transfer Learning Performance of Teacher Model: We next study how effectively pseudo labels preserve the transfer learning performance of teacher models trained on supervised datasets. To test this we train two supervised teacher models: object detection model on Objects365 and semantic segmentation model on COCO. The first two rows in Table 4 shows their supervised learning performance and their transfer learning performance on 6 downstream tasks. Next we generate pseudo labels on two datasets without labels: ImageNet (1.2M images) and JFT (300M images) . Now we train models from scratch on the pseudo labels on ImageNet and JFT. The next 4 rows  Table 2. Multi-Task Self-Training (MuST) outperforms supervised and self-supervised representations on ImageNet. We compare MuST to state-of-the-art self-supervised and supervised learning using the same pre-training dataset (ImageNet). MuST learns more general features and achieves the best performance on 4/6 downstream fine-tuning tasks. The performance differences show the impact of different training objectives.\n\n\nSettings\n\nTransfer Learning Performance  Table 3. Multi-Task Self-Training (MuST) benefits from increasing the number of different pseudo label tasks. We add depth, segmentation, and detection pseudo labels in addition to supervised ImageNet classification labels and test the representational quality. The results reveal that adding pseudo labels from more tasks leads to more general pre-trained models. All models are trained for 90 epochs on ImageNet.\n\nin Table 4 reveal these results. We observe for both object detection and segmentation pseudo labels we obtain a degradation in the supervised learning quality (e.g. 26.1 vs 20.6/20.7), but that when the representations are transferred they obtain similar or better transfer learning performance than the teacher model. Furthermore, the representations obtained by training on JFT over ImageNet typically lead to better transfer learning performance, which reveals the scalability of the MuST method. As we get more and more unlabeled data, our method can easily take advantage of it and the representational quality improves.\n\nMulti-Task Self-Training Across Datasets: MuST utilized pseudo labels generated from teacher models trained on different supervised learning datasets. A natural comparison is then to see how MuST compared against supervised multi-task supervised training where a model is trained on the union of the datasets and labels [29]. Table 5 compares the representational quality of MuST versus supervised multi-task training on three datasets: ImageNet, COCO and Objects365. \n\n\nScaling Multi-Task Self-Training\n\nOne benefit of MuST is that it can scale to unbounded amounts of unlabeled images. To test this hypothesis we move from the ImageNet setup with 1.2M images to JFT with 300M images.\n\nScaling Dataset Size and Training Iterations: Now instead of generating pseudo labels on 1.2M images, we scale the MuST training to have all three supervised teacher models to generate pseudo labels on 300M images. This process is trivially parallelizable, which makes the overall runtime low compared to training of the models. Table 6 shows the comparison of MuST vs self-supervised learning and supervised learning on the JFT dataset. On 5/6 downstream tasks MuST outperforms the self-supervised SimCLR algorithm when using the same unlabeled data. We also train a supervised baseline on the multi-class labels available on JFT and find that MuST, using only the unlabeled images, outperforms the representation on 5/6 downstream tasks. This is quite impressive considering the total sum of supervised images that MuST indirectly makes use of from the pseudo labels is only about 3.7M images compared to the 300M labeled JFT images. Adding JFT supervised labels can fur-  Table 4. Models trained on supervised data or pseudo labeled data have similar transfer learning performance. Results comparing how representations transfer if they are trained on supervised data or on pseudo labels that are generated by the supervised model. Pseudo labels effectively compress the knowledge in a supervised dataset. The performance of student models increases with the size of the unlabeled dataset. As the unlabeled dataset size increased, the performance of student model increases. This reveals the scalability of MuST. All student models are trained for the same training iterations (90 ImageNet epochs and 0.36 JFT epochs).\n\n\nSettings\n\nTransfer  Table 5. Comparing Multi-Task Training versus Multi-Task Self-Training. We compare MuST against a baseline of doing supervised multi-task training on the union of all teacher datasets. We use three datasets: ImageNet, COCO and Objects365. Supervised model is jointly trained on the supervised labels of these three datasets. MuST trains jointly on all three supervised and pseudo labels generated by the teacher models. The transfer learning performance gets strong improvements by incorporating pseudo labels into every image.\n\n\nSettings\n\nTransfer Learning Performance  Table 6. Scaling Multi-Task Self-Training to 300M images. We repeat the experiments in Table 2 on the JFT dataset (300M images with classification labels). The supervised learning benefits more from the additional images and annotations compared to the self-supervised SimCLR algorithm. ther improve the performance on image classification and segmentation, showing the flexibility of MuST in using labeled and unlabeled data. Lastly, the student model not only learns general features for transferring, it is also capable of generating high quality predictions for multiple tasks. Figure 4 shows the predictions made by our strongest model.\n\nBootstrapping from Pre-trained Models. Next we study if MuST can improve upon checkpoints trained with billions of training examples. We use ALIGN checkpoints [24], which are trained with 1.8B image-text pairs, to initialize parameters for training both teacher models and the student model. We use the same teacher model tasks as our previous experiments. The pseudo labels are generated on JFT-300M dataset and the MuST student model is trained Relative transfer learning performance gains over the ImageNet pre-trained model [52]. Checkpoints trained with more data or labels typically provide gains on transfer learning to downstream tasks. Fine-tuning the EfficientNet-B7 ALIGN checkpoint with MuST can further improve transfer learning performance for 4/6 downstream tasks.\n\non JFT for 1 epoch. Figure 5 shows relative transfer learning performance gains of Noisy Student [58], ALIGN [24], and MuST w/ ALIGN compared to the ImageNet checkpoint [52]. The figure shows MuST w/ ALIGN improves the ALIGN checkpoint by respectable margins for 4 out of the 6 downstream tasks. The performances are slightly worse for CIFAR-100 and DIODE surface normal prediction. We repeat the experiments with EfficientNet-L2 architecture and train the student model for 0.36 epoch on JFT. We report 4 downstream tasks showing improvements over the ALIGN checkpoint in Table 7. We find the student model trained with MuST improves the large ALIGN EfficientNet-L2 checkpoint and is competitive to the stateof-the-art models specialized for each dataset and task. Notably, MuST provides checkpoints ready to be fine-tuned for short iterations to achieve state-of-the-art performance while typical self-training methods [67] require pseudo labeling and long training iterations for each downstream task.\n\n\nDiscussion\n\nWhich pre-training method performs the best with large scaling training? Although self-supervised learning can outperform supervised learning on the ImageNet size dataset (1.3 million images/1k classes), supervised learning is still a better pre-training method on JFT size dataset (300 million images/18k classes). The gap may be  Table 7. MuST checkpoints are versatile and achieve competitive performance compared to state-of-the-art models.\n\nMuST improves the transfer learning performance of the ALIGN EfficientNet-L2 checkpoint on these four downstream tasks. compensated by training with more unlabeled data for selfsupervised learning. However, self-training can also expand one or multiple supervised models by generating pseudo labels on unlabeled data. Overall, both self-supervised and self-training are able to scale, but at the moment selftraining presents better performance in learning general features. A promising direction is to combine self-supervised and self-training for representation learning [65,12,59].\n\nWhy use MuST over self-supervised learning? Both of the methods are scalable with the unlabeled training data, however, MuST can easily combine together all labeled and unlabeled data. However, self-supervised learning relies on the generalization from the pre-text task to downstream tasks, which does not always give good performance. It is easier to design pseudo labels if the downstream tasks of interest are known in advance. MuST also generalizes to unseen tasks (e.g., surface normal prediction) given the tasks of the teacher model in this paper.\n\n\nConclusion\n\nIn this paper, we present MuST, a scalable multi-task self-training method for learning general representations. We compare with supervised and self-supervised learning approaches on ImageNet and JFT and evaluate on 6 datasets including visual recognition, localization, and 3D geometry prediction. We show that MuST outperforms or is on par with supervised and self-supervised learning on 5 out of 6 transfer learning tasks, except the classification task. Moreover, MuST can improve upon already strong checkpoints trained with billions of examples. The results show multitask self-training is a scalable pre-training method and is able to learn general feature representations. We hope this work will encourage further research towards creating universal visual representations.\n\n\nA. Details of Training and Evaluation Datasets\n\n\nA.1. Training Datasets\n\nIn this section, we describe 5 datasets we use to train teacher models.\n\nImageNet: ImageNet [47] is a classification dataset with 1.2M training images and 1000 unique classes. All of its images are center cropped and have one primary object per image.\n\nObjects365: Objects365 [49] is an object detection dataset that has 365 different classes and 600k training images.\n\nCOCO: The COCO dataset [37] contains 118k images that has a variety of different labels (e.g. object detection, instance segmentation, panoptic segmentation). For all experiments we use its pantopic segmentation labels.\n\nMiDaS: The MiDaS depth model [44] that is used for generating our depth pseudo labels is trained on a diverse set of 5 depth datasets. The 5 depth datasets are DIML Indoor [27] (220k images), MegaDepth [35] (130k images), ReDWeb [56] (3600), WSVD [55] (1.5M), and 3D movies (75k). The model is trained to be invariant to the depth range and scale across all datasets, leading to a model that generates robust pseudo labels.\n\nJFT: JFT [51] is a large-scale image multi-label classification dataset with 300M labeled images. This dataset is used to test the scale of MuST and various self-supervised learning algorithms.\n\n\nA.2. Evaluation Datasets\n\nNext we describe the datasets that all of our representations will be fine-tuned on. We have different datasets with a total of five different tasks. Note the Surface Normal task is never used as a training task to test the task generality of the representations.\n\nCIFAR-100: CIFAR-100 is a classification dataset with 50k images and 100 unique classes.\n\nPASCAL Detection: The Pascal Detection dataset [13] is an object detection dataset with 20 unique classes. We train the models on the trainval sets of PASCAL VOC 2007 and PASCAL VOC 2012 which include 16.5k images.\n\nPASCAL Segmentation: The Pascal Segmentation dataset [13] is a semantic segmentation dataset with 20 unique classes. We train the models on the train set of the PASCAL VOC 2012 segmentation dataset which has 1.5k images. NYU Depth V2: The NYU Depth v2 dataset [50] is a depth estimation dataset that contains 47584 train images and 654 validation images.\n\nADE Segmentation: ADE20k [66] is a segmentation dataset that contains 20k images with 150 object and stuff classes. The dataset contains a wide variety of different indoor and outdoor scenes along with object classes.\n\nDIODE Surface Normal: The DIODE dataset [54] is a depth and surface normal dataset that contains 16884 images. The dataset contains a diverse set of both indoor and outdoor scenes for training and testing. We only make use of the surface normal labels.\n\n\nB. Implementation Details\n\n\nB.1. Training Teacher Models\n\nIn this section, we introduce the details of training teacher models, which are used to generate pseudo labels in MuST. All the models are trained with a ResNet-152 backbone model.\n\nObjects365 Detection: We use batch size 256 and train for 140 epochs. The image size is 640. We apply scale jittering [0.5, 2.0] (i.e. randomly resample image between 320 \u00d7 320 to 1280 \u00d7 1280 and crop it to 512 \u00d7 512). The learning rate is 0.32 and the weight decay is set as 4e-5. The model is trained from random initialization. The final performance is 26.1 AP.\n\nCOCO Segmentation: We use the annotations in COCO panoptic segmentation dataset [28]. We train a semantic segmentation model that only predicts the semantic class for each pixel, instead of also predicting the object instance. We use batch size 256 and train for 384 epochs. The image size is 896. We apply scale jittering [0.5, 2.0]. The learning rate is 0.32 and the weight decay is set as 4e-5. The model is trained from random initialization. The final performance is 53.8 mIoU.\n\n\nMiDaS Depth:\n\nWe directly download the pre-trained Mi-DaS from the github repository and use it as a teacher model to generate pseudo labels.\n\nImageNet Classification: We use batch size 2048 and train for 400 epochs. The image size is 224. The learning rate is 0.8 and weight decay is 4e-5. We apply random augmentation [10] (2L-15M, 2 layers with magnitude 15) and label smoothing (0.1) to regularize the model training. The final performance is 81.6 top-1 accuracy.\n\n\nB.2. Training Multi-Task Student Models\n\nWe use a batch size 256 for training student models in our experiments. The image size is 640. We apply scale jittering [0.5, 2.0] during training. The weight decay is 4e-5 in ImageNet experiments and 3e-6 in JFT experiments. No random augmentation [10] or label smoothing is applied.\n\n\nB.3. Fine-tuning on Evaluation Datasets\n\nFor fine-tuning we initialize the parameters in the ResNet and FPN backbone with a pre-trained model and randomly initialize the rest of the layers. We perform end-to-end finetuning with an extensive grid search of the combinations of learning rate and training steps to ensure each pre-trained model achieves its best fine-tuning performance. We experiment with different weight decays but do not find it making a big difference and set it to 1e-4. All models are trained with cosine learning rate for simplicity. Below we describe the dataset, evaluation metric, model architecture, and training parameters for each task.\n\nCIFAR-100: We use standard CIFAR-100 train and test sets and report the top-1 accuracy. We resize the image resolution to 256 \u00d7 256. We replace the classification head in the pre-trained model with a randomly initialized linear layer that predicts 101 classes, including background. We use a batch size of 512 and search the combination of training steps from 5000 to 20000 and learning rates from 0.005 to 0.32. We find the best learning rate for SimCLR (0.16) is much higher than the supervised model (0.005). This trend holds for the following tasks.\n\nPASCAL Segmentation: We use PASCAL VOC 2012 train and validation sets and report the mIoU metric. The training images are re-sampled into 512 \u00d7 512 with scale jittering [0.5, 2.0]. We initialize the model from the pretrained backbone and FPN [36] layers. We remove the pretrained segmentation head and train from a randomly initialized head. We use a batch size of 64 and search the combination of training steps from 5000 to 20000 and learning rates from 0.005 to 0.32. we initialize the model from the pre-trained backbone and FPN [36] layers and randomly initialize the heads. We use a batch size of 32 and search the combination of training steps from 5000 to 20000 and learning rates from 0.005 to 0.32. NYU Depth: We use NYU depth v2 dataset with 47584 train and 654 validation images. We report the percentage of predicted depth values within 1.25 relative ratio compared to the ground truth. The training images are resampled into 640 with scale jittering [0.5, 2.0]. we initialize the model from the pre-trained backbone and FPN [36] layers and randomly initialize the heads. We use a batch size of 64 and search the combination of training steps from 10000 to 40000 and learning rates from 0.005 to 0.32.\n\n\nDIODE:\n\nWe use DIODE outdoor dataset with 16884 train and 446 validation images. We report the percentage of the angle error less than 11.25 \u2022 . We use the original image resolution 768 for training and evaluation. The training image is applied with scale jittering [0.5, 2.0]. we initialize the model from the pre-trained backbone and FPN [36] layers and randomly initialize the heads. We use a batch size of 32 and search the combination of training steps from 20000 to 80000 and learning rates from 0.01 to 0.16.  \n\n\nC. Visualization of Student Model Predictions\n\nFigure 1 .\n1An overview of Multi-Task Self-Training (MuST).\n\nFigure 3 .\n3The ResNet-FPN backbone architecture for multitask learning. Orange: the top-level features for classification.\n\n\nFor multi-task training we sample examples from the datasets with equal probability. Sampling examples with probabilities proportional to the size of the datasets does not work well. Because ImageNet and Objects365 datasets are much larger than COCO dataset and as a result for a batch size of 256 only 15 examples have non zero loss values for segmentation. On the other hand, for MuST every image has any type of label and we can sample examples with probabilities proportional to the size of datasets. When comparing the representation qualities MuST obtains the best performance on 6/6 downstream tasks.\n\nFigure 4 .Figure 5 .\n45Visualization of the predictions generated by a multitask student model. The MuST student model not only learns general feature representations, but also makes high quality visual predictions with a single model.\n\nPASCAL\nDetection: We use PASCAL VOC 2007+2012 trainval set and VOC 2007 test set and report the AP 50 with 11 recall points to compute average precision. The training images are resampled into 896 with scale jittering [0.5, 2.0].\n\nFigure 6\n6shows more visual examples of the predictions made by a single multi-task student model. The images are sampled from the validation set in ImageNet dataset.\n\nFigure 6 .\n6The visualization of inference on ImageNet dataset made by single MuST student model.\n\n\nDatasets using for MuST and for downstream fine-tuning evaluation.pseudo label. This makes MuST significantly simpler to use and to scale with multiple tasks.Training Datasets \n\nEvaluation Datasets \n\nName \nTask \nNum Images Name \nTask \nNum Images \n\nImageNet [47] \nClassification \n1.2M \nCIFAR-100 [31] \nClassification \n50k \nObjects365 [49] \nDetection \n600k \nPascal [13] \nDetection \n16.5k \nCOCO [37] \nSegmentation \n118k \nPascal [13] \nSegmentation \n1.5k \nMiDaS [44] \nDepth \n1.9M \nNYU V2 [50] \nDepth \n47k \nJFT [51] \nClassification \n300M \nADE [66] \nSegmentation \n20k \nDIODE [54] \nSurface Normal \n17k \nTable 1. \nhttps://github.com/intel-isl/MiDaS\nAcknowledgementsWe would like to thank Yin Cui, Aravind Srinivas, Simon Kornblith, and Ting Chen for valuable feedback.\nSelf-organizing neural network that discovers surfaces in random-dot stereograms. Suzanna Becker, Geoffrey E Hinton, Nature. 3556356Suzanna Becker and Geoffrey E Hinton. Self-organizing neural network that discovers surfaces in random-dot stere- ograms. Nature, 355(6356):161-163, 1992. 3\n\nMultitask learning. Rich Caruana, Machine learning. 281Rich Caruana. Multitask learning. Machine learning, 28(1):41-75, 1997. 2\n\nNaive-student: Leveraging semi-supervised learning in video sequences for urban scene segmentation. Liang-Chieh Chen, Raphael Gontijo Lopes, Bowen Cheng, D Maxwell, Collins, D Ekin, Barret Cubuk, Hartwig Zoph, Jonathon Adam, Shlens, ECCV. Liang-Chieh Chen, Raphael Gontijo Lopes, Bowen Cheng, Maxwell D Collins, Ekin D Cubuk, Barret Zoph, Hartwig Adam, and Jonathon Shlens. Naive-student: Leveraging semi-supervised learning in video sequences for urban scene segmentation. In ECCV, 2020. 3\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille, IEEE transactions on pattern analysis and machine intelligence. 40Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolu- tion, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834-848, 2017. 1\n\nA simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, ICML. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 1, 2, 3, 6, 7\n\nGradNorm: Gradient normalization for adaptive loss balancing in deep multitask networks. Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, Andrew Rabinovich, JMLR. 4Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and An- drew Rabinovich. GradNorm: Gradient normalization for adaptive loss balancing in deep multitask networks. JMLR, 2018. 4\n\nJust pick a sign: Optimizing deep multitask models with gradient sign dropout. Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai, Dragomir Anguelov, NeurIPS. Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai, and Dragomir Anguelov. Just pick a sign: Optimizing deep multitask models with gra- dient sign dropout. In NeurIPS, 2020. 4\n\nPer-pixel classification is not all you need for semantic segmentation. Bowen Cheng, Alexander G Schwing, Alexander Kirillov, abs/2107.06278CoRRBowen Cheng, Alexander G. Schwing, and Alexander Kir- illov. Per-pixel classification is not all you need for semantic segmentation. CoRR, abs/2107.06278, 2021. 8\n\nBam! born-again multi-task networks for natural language understanding. Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D Manning, Quoc V Le, ACL. Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D. Manning, and Quoc V. Le. Bam! born-again multi-task networks for natural language understanding. In ACL, 2019. 1, 3, 4\n\nRandaugment: Practical automated data augmentation with a reduced search space. D Ekin, Barret Cubuk, Jonathon Zoph, Quoc V Shlens, Le, Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V. Le. Randaugment: Practical automated data augmentation with a reduced search space, 2019. 10\n\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, 13Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2020. 1, 3\n\nSelf-training improves pre-training for natural language understanding. Jingfei Du, Edouard Grave, Beliz Gunel, Vishrav Chaudhary, Onur Celebi, Michael Auli, Ves Stoyanov, Alexis Conneau, Jingfei Du, Edouard Grave, Beliz Gunel, Vishrav Chaud- hary, Onur Celebi, Michael Auli, Ves Stoyanov, and Alexis Conneau. Self-training improves pre-training for natural lan- guage understanding, 2020. 8\n\nThe pascal visual object classes (voc) challenge. IJCV. Mark Everingham, Luc Van Gool, K I Christopher, John Williams, Andrew Winn, Zisserman, 59Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. IJCV, 2010. 5, 9\n\nSimple copy-paste is a strong data augmentation method for instance segmentation. Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, Barret Zoph, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung- Yi Lin, Ekin D. Cubuk, Quoc V. Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2918-2928, June 2021. 8\n\nFast r-cnn. Ross Girshick, ICCV. Ross Girshick. Fast r-cnn. In ICCV, 2015. 3\n\nPriya Goyal, Piotr Doll\u00e1r, Ross B Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, arXiv:1706.02677Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour. arXiv preprintPriya Goyal, Piotr Doll\u00e1r, Ross B. Girshick, Pieter No- ordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tul- loch, Yangqing Jia, and Kaiming He. Accurate, large mini- batch SGD: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. 4\n\nBootstrap your own latent: A new approach to self-supervised learning. Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, Michal Valko, NeurIPS. 13Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Do- ersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham- mad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-supervised learning. In NeurIPS, 2020. 1, 3\n\nMomentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, CVPR, 2020. 1. 23Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual rep- resentation learning. In CVPR, 2020. 1, 2, 3\n\nRethinking imagenet pre-training. Kaiming He, Ross Girshick, Piotr Doll\u00e1r, ICCV. Kaiming He, Ross Girshick, and Piotr Doll\u00e1r. Rethinking imagenet pre-training. In ICCV, 2019. 1\n\nPiotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. Kaiming He, Georgia Gkioxari, ICCV. Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask r-cnn. In ICCV, 2017. 4\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. 24Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 2, 4\n\nJ Olivier, Aravind H\u00e9naff, Jeffrey De Srinivas, Ali Fauw, Carl Razavi, Doersch, Aaron Sm Eslami, Van Den Oord, arXiv:1905.09272Data-efficient image recognition with contrastive predictive coding. arXiv preprintOlivier J H\u00e9naff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, SM Eslami, and Aaron van den Oord. Data-efficient image recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019. 3\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. arXiv preprintGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill- ing the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. 4\n\nScaling up visual and vision-language representation learning with noisy text supervision. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yun-Hsuan Sung, Zhen Li, Tom Duerig, abs/2102.05918CoRRChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. CoRR, abs/2102.05918, 2021. 1, 2, 7, 8\n\nSelf-supervised visual feature learning with deep neural networks: A survey. Longlong Jing, Yingli Tian, IEEE Transactions on Pattern Analysis and Machine Intelligence. 20203Longlong Jing and Yingli Tian. Self-supervised visual fea- ture learning with deep neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. 3\n\nMulti-task learning using uncertainty to weigh losses for scene geometry and semantics. Alex Kendall, Yarin Gal, Roberto Cipolla, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geome- try and semantics. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 4\n\nDeep monocular depth estimation via integration of global and local predictions. Youngjung Kim, Hyungjoo Jung, Dongbo Min, Kwanghoon Sohn, IEEE transactions on Image Processing. 278Youngjung Kim, Hyungjoo Jung, Dongbo Min, and Kwanghoon Sohn. Deep monocular depth estimation via in- tegration of global and local predictions. IEEE transactions on Image Processing, 27(8):4131-4144, 2018. 9\n\nPanoptic segmentation. Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Dollar, CVPR. 39Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollar. Panoptic segmentation. In CVPR, June 2019. 3, 9\n\nUbernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. Iasonas Kokkinos, CVPR. 6Iasonas Kokkinos. Ubernet: Training a universal convolu- tional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. In CVPR, 2017. 3, 4, 6\n\nBig transfer (bit): General visual representation learning. Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby, ECCV. 13Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In ECCV, 2020. 1, 3\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Technical reportAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. 5\n\nThe open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV. Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, Vittorio Ferrari, Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui- jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 2020. 3\n\nPseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. Dong-Hyun Lee, Workshop on challenges in representation learning, ICML. Dong-Hyun Lee et al. Pseudo-label: The simple and effi- cient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learn- ing, ICML, 2013. 3\n\nAn analysis of pre-training on object detection. Hengduo Li, Bharat Singh, Mahyar Najibi, Zuxuan Wu, Larry S Davis, abs/1904.05871CoRR13Hengduo Li, Bharat Singh, Mahyar Najibi, Zuxuan Wu, and Larry S. Davis. An analysis of pre-training on object detec- tion. CoRR, abs/1904.05871, 2019. 1, 3\n\nMegadepth: Learning singleview depth prediction from internet photos. Zhengqi Li, Noah Snavely, CVPR. Zhengqi Li and Noah Snavely. Megadepth: Learning single- view depth prediction from internet photos. In CVPR, 2018. 9\n\nKaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, CVPR. 10Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In CVPR, 2017. 2, 4, 10\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, ECCV. 59Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 1, 5, 9\n\nImproving multi-task deep neural networks via knowledge distillation for natural language understanding. Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao, ACL. Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Improving multi-task deep neural networks via knowl- edge distillation for natural language understanding. In ACL, 2019. 3\n\nAshwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, ECCV. Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In ECCV, September 2018. 3\n\nSelf-supervised learning of pretext-invariant representations. Ishan Misra, Laurens Van Der Maaten, CVPR. 2020Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In CVPR, 2020. 3\n\nDemystifying contrastive self-supervised learning: Invariances, augmentations and dataset biases. Senthil Purushwalkam, Abhinav Gupta, arXiv:2007.13916arXiv preprintSenthil Purushwalkam and Abhinav Gupta. Demystifying contrastive self-supervised learning: Invariances, augmenta- tions and dataset biases. arXiv preprint arXiv:2007.13916, 2020. 3\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, JMLR. 3Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 2020. 3\n\nVision transformers for dense prediction. Ren\u00e9 Ranftl, Alexey Bochkovskiy, Vladlen Koltun, ArXiv preprintRen\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi- sion transformers for dense prediction. ArXiv preprint, 2021. 8\n\nTowards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, Vladlen Koltun, 2020. 3IEEE Transactions on Pattern Analysis and Machine Intelligence. 59TPAMIRen\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Ma- chine Intelligence (TPAMI), 2020. 3, 5, 9\n\nSemi-supervised self-training of object detection models. Chuck Rosenberg, Henry Hebert, Schneiderman, IEEE Workshops on Applications of Computer Vision (WACV/MOTION'05). Chuck Rosenberg, Martial Hebert, and Henry Schneider- man. Semi-supervised self-training of object detection mod- els. In IEEE Workshops on Applications of Computer Vision (WACV/MOTION'05), 2005. 3\n\nAn overview of multi-task learning in. Sebastian Ruder, arXiv:1706.05098deep neural networks. arXiv preprintSebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098, 2017. 2\n\n. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, 59Imagenet large scale visual recognition challenge. IJCV, 2015. 3Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 2015. 3, 5, 9\n\nProbability of error of some adaptive patternrecognition machines. H Scudder, IEEE Transactions on Information Theory. 113H Scudder. Probability of error of some adaptive pattern- recognition machines. IEEE Transactions on Information Theory, 11(3):363-371, 1965. 3\n\nObjects365: A large-scale, high-quality dataset for object detection. Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, Jian Sun, ICCV. 59Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In ICCV, 2019. 1, 3, 5, 9\n\nIndoor segmentation and support inference from rgbd images. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, Rob Fergus, ECCV. 59Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012. 5, 9\n\nRevisiting unreasonable effectiveness of data in deep learning era. Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta, ICCV. 39Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi- nav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In ICCV, 2017. 3, 5, 9\n\nRethinking model scaling for convolutional neural networks. Mingxing Tan, Quoc Le, Efficientnet, PMLR, 09-15Proceedings of the 36th International Conference on Machine Learning. Kamalika Chaudhuri and Ruslan Salakhutdinovthe 36th International Conference on Machine Learning97Mingxing Tan and Quoc Le. EfficientNet: Rethinking model scaling for convolutional neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 6105-6114. PMLR, 09-15 Jun 2019. 8\n\nContrastive multiview coding. Yonglong Tian, Dilip Krishnan, Phillip Isola, ECCV. Yonglong Tian, Dilip Krishnan, and Phillip Isola. Con- trastive multiview coding. In ECCV, 2019. 3\n\nIgor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Z Falcon, Andrea F Dai, Mohammadreza Daniele, Steven Mostajabi, Basart, R Matthew, Walter, arXiv:1908.00463A dense indoor and outdoor depth dataset. 59arXiv preprintIgor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Z Dai, Andrea F Daniele, Moham- madreza Mostajabi, Steven Basart, Matthew R Walter, et al. Diode: A dense indoor and outdoor depth dataset. arXiv preprint arXiv:1908.00463, 2019. 5, 9\n\nWeb stereo video supervision for depth prediction from dynamic scenes. Chaoyang Wang, Simon Lucey, Federico Perazzi, Oliver Wang, 2019 International Conference on 3D Vision (3DV). IEEEChaoyang Wang, Simon Lucey, Federico Perazzi, and Oliver Wang. Web stereo video supervision for depth prediction from dynamic scenes. In 2019 International Conference on 3D Vision (3DV), pages 348-357. IEEE, 2019. 9\n\nMonocular relative depth perception with web stereo data supervision. Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao, Ruibo Li, Zhenbo Luo, CVPR. Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao, Ruibo Li, and Zhenbo Luo. Monocular relative depth per- ception with web stereo data supervision. In CVPR, 2018. 9\n\nUnified perceptual parsing for scene understanding. Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun, European Conference on Computer Vision. SpringerTete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understand- ing. In European Conference on Computer Vision. Springer, 2018. 3\n\nSelf-training with noisy student improves imagenet classification. Qizhe Xie, Eduard Hovy, Minh-Thang Luong, Quoc V Le, CVPR, 2020. 3. 4Qizhe Xie, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Self-training with noisy student improves imagenet clas- sification. In CVPR, 2020. 3, 4, 8\n\nSelf-training and pre-training are complementary for speech recognition. Qiantong Xu, Alexei Baevski, Tatiana Likhomanenko, Paden Tomasello, Alexis Conneau, Ronan Collobert, Gabriel Synnaeve, Michael Auli, Qiantong Xu, Alexei Baevski, Tatiana Likhomanenko, Paden Tomasello, Alexis Conneau, Ronan Collobert, Gabriel Syn- naeve, and Michael Auli. Self-training and pre-training are complementary for speech recognition, 2020. 8\n\nAditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-totext transformer. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, NAACL. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to- text transformer. In NAACL, 2020. 1\n\nBillion-scale semi-supervised learning for image classification. I Zeki Yalniz, Herv\u00e9 J\u00e9gou, Kan Chen, Manohar Paluri, Dhruv Mahajan, abs/1905.00546CoRRI. Zeki Yalniz, Herv\u00e9 J\u00e9gou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-supervised learning for image classification. CoRR, abs/1905.00546, 2019. 3\n\nUnsupervised word sense disambiguation rivaling supervised methods. David Yarowsky, ACL. David Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In ACL, 1995. 3\n\nGradient surgery for multi-task learning. Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, Chelsea Finn, arXiv:2001.06782arXiv preprintTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. arXiv preprint arXiv:2001.06782, 2020. 4\n\nTaskonomy: Disentangling task transfer learning. R Amir, Alexander Zamir, William Sax, Leonidas J Shen, Jitendra Guibas, Silvio Malik, Savarese, In CVPR. 3Amir R. Zamir, Alexander Sax, William Shen, Leonidas J. Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In CVPR, 2018. 3\n\nPushing the limits of semi-supervised learning for automatic speech recognition. Yu Zhang, James Qin, Daniel S Park, Wei Han, Chung-Cheng Chiu, Ruoming Pang, V Quoc, Yonghui Le, Wu, Yu Zhang, James Qin, Daniel S. Park, Wei Han, Chung- Cheng Chiu, Ruoming Pang, Quoc V. Le, and Yonghui Wu. Pushing the limits of semi-supervised learning for automatic speech recognition, 2020. 8\n\nSemantic understanding of scenes through the ade20k dataset. Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, Antonio Torralba, IJCV. 12739Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi- dler, Adela Barriuso, and Antonio Torralba. Semantic un- derstanding of scenes through the ade20k dataset. IJCV, 127(3):302-321, 2019. 5, 9\n\nRethinking pre-training and self-training. Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin D Cubuk, Quoc V Le, NeurIPS. Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin D. Cubuk, and Quoc V. Le. Rethinking pre-training and self-training. In NeurIPS, 2020. 2, 3, 4, 8\n", "annotations": {"author": "[{\"end\":126,\"start\":65},{\"end\":189,\"start\":127},{\"end\":248,\"start\":190},{\"end\":287,\"start\":249},{\"end\":329,\"start\":288}]", "publisher": null, "author_last_name": "[{\"end\":78,\"start\":72},{\"end\":138,\"start\":134},{\"end\":202,\"start\":197},{\"end\":258,\"start\":256},{\"end\":300,\"start\":297}]", "author_first_name": "[{\"end\":71,\"start\":65},{\"end\":133,\"start\":127},{\"end\":194,\"start\":190},{\"end\":196,\"start\":195},{\"end\":253,\"start\":249},{\"end\":255,\"start\":254},{\"end\":296,\"start\":288}]", "author_affiliation": "[{\"end\":125,\"start\":99},{\"end\":188,\"start\":162},{\"end\":247,\"start\":221},{\"end\":286,\"start\":260},{\"end\":328,\"start\":302}]", "title": "[{\"end\":62,\"start\":1},{\"end\":391,\"start\":330}]", "venue": null, "abstract": "[{\"end\":1822,\"start\":393}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2109,\"start\":2105},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2112,\"start\":2109},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2146,\"start\":2143},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2149,\"start\":2146},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2152,\"start\":2149},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2830,\"start\":2826},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":2972,\"start\":2968},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3113,\"start\":3109},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3116,\"start\":3113},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3146,\"start\":3143},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":3380,\"start\":3376},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3382,\"start\":3380},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3530,\"start\":3526},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":4131,\"start\":4127},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6247,\"start\":6243},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6287,\"start\":6283},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6809,\"start\":6806},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6820,\"start\":6816},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6968,\"start\":6965},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7479,\"start\":7476},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7835,\"start\":7831},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8995,\"start\":8991},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9113,\"start\":9110},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9189,\"start\":9186},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9192,\"start\":9189},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9327,\"start\":9323},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":9451,\"start\":9447},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9638,\"start\":9634},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":9658,\"start\":9654},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9995,\"start\":9991},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":10212,\"start\":10208},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":10215,\"start\":10212},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10218,\"start\":10215},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10221,\"start\":10218},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":10391,\"start\":10387},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":10553,\"start\":10549},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10690,\"start\":10687},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":10797,\"start\":10793},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11192,\"start\":11188},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11205,\"start\":11201},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11252,\"start\":11248},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11483,\"start\":11479},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11536,\"start\":11532},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11639,\"start\":11635},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":11710,\"start\":11706},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12171,\"start\":12167},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12173,\"start\":12171},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12176,\"start\":12173},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12179,\"start\":12176},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12182,\"start\":12179},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":12185,\"start\":12182},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12282,\"start\":12279},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12370,\"start\":12367},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12373,\"start\":12370},{\"end\":12387,\"start\":12375},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12594,\"start\":12590},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12708,\"start\":12704},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12821,\"start\":12817},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12832,\"start\":12828},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12844,\"start\":12841},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13184,\"start\":13180},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":13730,\"start\":13726},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":13748,\"start\":13744},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13759,\"start\":13755},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13862,\"start\":13858},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":14036,\"start\":14032},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15190,\"start\":15186},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15230,\"start\":15226},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":15532,\"start\":15528},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15999,\"start\":15995},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16712,\"start\":16708},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":16735,\"start\":16731},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17327,\"start\":17324},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17330,\"start\":17327},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17683,\"start\":17679},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17685,\"start\":17683},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17687,\"start\":17685},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":17690,\"start\":17687},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18073,\"start\":18069},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18287,\"start\":18283},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18960,\"start\":18957},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":19425,\"start\":19421},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":19428,\"start\":19425},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25417,\"start\":25413},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":28801,\"start\":28797},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":29170,\"start\":29166},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":29520,\"start\":29516},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29532,\"start\":29528},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":29592,\"start\":29588},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":30344,\"start\":30340},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":31460,\"start\":31456},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31463,\"start\":31460},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":31465,\"start\":31463},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":32992,\"start\":32988},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":33176,\"start\":33172},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":33293,\"start\":33289},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":33520,\"start\":33516},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":33663,\"start\":33659},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":33693,\"start\":33689},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":33720,\"start\":33716},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":33738,\"start\":33734},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":33925,\"start\":33921},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":34540,\"start\":34536},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":34762,\"start\":34758},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":34969,\"start\":34965},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":35090,\"start\":35086},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":35324,\"start\":35320},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":36225,\"start\":36221},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":36950,\"start\":36946},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":37390,\"start\":37386},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":38891,\"start\":38887},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":39182,\"start\":39178},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":39687,\"start\":39683},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":40206,\"start\":40202}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":40488,\"start\":40428},{\"attributes\":{\"id\":\"fig_3\"},\"end\":40613,\"start\":40489},{\"attributes\":{\"id\":\"fig_5\"},\"end\":41223,\"start\":40614},{\"attributes\":{\"id\":\"fig_6\"},\"end\":41460,\"start\":41224},{\"attributes\":{\"id\":\"fig_7\"},\"end\":41691,\"start\":41461},{\"attributes\":{\"id\":\"fig_8\"},\"end\":41859,\"start\":41692},{\"attributes\":{\"id\":\"fig_9\"},\"end\":41958,\"start\":41860},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":42565,\"start\":41959}]", "paragraph": "[{\"end\":2786,\"start\":1838},{\"end\":3944,\"start\":2788},{\"end\":5120,\"start\":3946},{\"end\":6043,\"start\":5122},{\"end\":6754,\"start\":6045},{\"end\":7750,\"start\":6756},{\"end\":8056,\"start\":7752},{\"end\":8095,\"start\":8058},{\"end\":8248,\"start\":8097},{\"end\":8483,\"start\":8250},{\"end\":8729,\"start\":8485},{\"end\":8897,\"start\":8731},{\"end\":9328,\"start\":8914},{\"end\":10099,\"start\":9330},{\"end\":11054,\"start\":10101},{\"end\":13185,\"start\":11056},{\"end\":13866,\"start\":13225},{\"end\":14597,\"start\":13887},{\"end\":15139,\"start\":14626},{\"end\":15742,\"start\":15141},{\"end\":16444,\"start\":15744},{\"end\":17015,\"start\":16446},{\"end\":17583,\"start\":17017},{\"end\":18343,\"start\":17585},{\"end\":18810,\"start\":18345},{\"end\":19172,\"start\":18812},{\"end\":19627,\"start\":19174},{\"end\":20328,\"start\":19649},{\"end\":20545,\"start\":20330},{\"end\":20883,\"start\":20585},{\"end\":21196,\"start\":20885},{\"end\":21380,\"start\":21239},{\"end\":22169,\"start\":21382},{\"end\":22906,\"start\":22171},{\"end\":24005,\"start\":22908},{\"end\":24463,\"start\":24018},{\"end\":25091,\"start\":24465},{\"end\":25561,\"start\":25093},{\"end\":25778,\"start\":25598},{\"end\":27401,\"start\":25780},{\"end\":27951,\"start\":27414},{\"end\":28636,\"start\":27964},{\"end\":29417,\"start\":28638},{\"end\":30423,\"start\":29419},{\"end\":30882,\"start\":30438},{\"end\":31467,\"start\":30884},{\"end\":32024,\"start\":31469},{\"end\":32820,\"start\":32039},{\"end\":32967,\"start\":32896},{\"end\":33147,\"start\":32969},{\"end\":33264,\"start\":33149},{\"end\":33485,\"start\":33266},{\"end\":33910,\"start\":33487},{\"end\":34105,\"start\":33912},{\"end\":34397,\"start\":34134},{\"end\":34487,\"start\":34399},{\"end\":34703,\"start\":34489},{\"end\":35059,\"start\":34705},{\"end\":35278,\"start\":35061},{\"end\":35532,\"start\":35280},{\"end\":35773,\"start\":35593},{\"end\":36139,\"start\":35775},{\"end\":36623,\"start\":36141},{\"end\":36767,\"start\":36640},{\"end\":37093,\"start\":36769},{\"end\":37421,\"start\":37137},{\"end\":38088,\"start\":37465},{\"end\":38643,\"start\":38090},{\"end\":39859,\"start\":38645},{\"end\":40379,\"start\":39870}]", "formula": null, "table_ref": "[{\"end\":20611,\"start\":20604},{\"end\":20998,\"start\":20991},{\"end\":21429,\"start\":21422},{\"end\":22515,\"start\":22508},{\"end\":23276,\"start\":23269},{\"end\":23593,\"start\":23586},{\"end\":24056,\"start\":24049},{\"end\":24475,\"start\":24468},{\"end\":26116,\"start\":26109},{\"end\":26762,\"start\":26755},{\"end\":27431,\"start\":27424},{\"end\":28002,\"start\":27995},{\"end\":28089,\"start\":28082},{\"end\":29999,\"start\":29992},{\"end\":30777,\"start\":30770}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1836,\"start\":1824},{\"attributes\":{\"n\":\"2.\"},\"end\":8912,\"start\":8900},{\"attributes\":{\"n\":\"3.\"},\"end\":13194,\"start\":13188},{\"attributes\":{\"n\":\"3.1.\"},\"end\":13223,\"start\":13197},{\"end\":13885,\"start\":13869},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14624,\"start\":14600},{\"attributes\":{\"n\":\"3.3.\"},\"end\":19647,\"start\":19630},{\"attributes\":{\"n\":\"4.\"},\"end\":20559,\"start\":20548},{\"attributes\":{\"n\":\"4.1.\"},\"end\":20583,\"start\":20562},{\"attributes\":{\"n\":\"4.2.\"},\"end\":21237,\"start\":21199},{\"end\":24016,\"start\":24008},{\"attributes\":{\"n\":\"4.3.\"},\"end\":25596,\"start\":25564},{\"end\":27412,\"start\":27404},{\"end\":27962,\"start\":27954},{\"attributes\":{\"n\":\"5.\"},\"end\":30436,\"start\":30426},{\"attributes\":{\"n\":\"6.\"},\"end\":32037,\"start\":32027},{\"end\":32869,\"start\":32823},{\"end\":32894,\"start\":32872},{\"end\":34132,\"start\":34108},{\"end\":35560,\"start\":35535},{\"end\":35591,\"start\":35563},{\"end\":36638,\"start\":36626},{\"end\":37135,\"start\":37096},{\"end\":37463,\"start\":37424},{\"end\":39868,\"start\":39862},{\"end\":40427,\"start\":40382},{\"end\":40439,\"start\":40429},{\"end\":40500,\"start\":40490},{\"end\":41245,\"start\":41225},{\"end\":41468,\"start\":41462},{\"end\":41701,\"start\":41693},{\"end\":41871,\"start\":41861}]", "table": "[{\"end\":42565,\"start\":42119}]", "figure_caption": "[{\"end\":40488,\"start\":40441},{\"end\":40613,\"start\":40502},{\"end\":41223,\"start\":40616},{\"end\":41460,\"start\":41248},{\"end\":41691,\"start\":41469},{\"end\":41859,\"start\":41703},{\"end\":41958,\"start\":41873},{\"end\":42119,\"start\":41961}]", "figure_ref": "[{\"end\":4672,\"start\":4664},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5299,\"start\":5291},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":6225,\"start\":6217},{\"end\":7104,\"start\":7096},{\"end\":13592,\"start\":13584},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15240,\"start\":15232},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17014,\"start\":17006},{\"end\":28585,\"start\":28577},{\"end\":29447,\"start\":29439}]", "bib_author_first_name": "[{\"end\":42810,\"start\":42803},{\"end\":42827,\"start\":42819},{\"end\":42829,\"start\":42828},{\"end\":43035,\"start\":43031},{\"end\":43251,\"start\":43240},{\"end\":43265,\"start\":43258},{\"end\":43273,\"start\":43266},{\"end\":43286,\"start\":43281},{\"end\":43295,\"start\":43294},{\"end\":43315,\"start\":43314},{\"end\":43328,\"start\":43322},{\"end\":43343,\"start\":43336},{\"end\":43358,\"start\":43350},{\"end\":43756,\"start\":43745},{\"end\":43769,\"start\":43763},{\"end\":43789,\"start\":43782},{\"end\":43805,\"start\":43800},{\"end\":43818,\"start\":43814},{\"end\":43820,\"start\":43819},{\"end\":44261,\"start\":44257},{\"end\":44273,\"start\":44268},{\"end\":44293,\"start\":44285},{\"end\":44311,\"start\":44303},{\"end\":44589,\"start\":44585},{\"end\":44601,\"start\":44596},{\"end\":44625,\"start\":44618},{\"end\":44637,\"start\":44631},{\"end\":44915,\"start\":44911},{\"end\":44928,\"start\":44922},{\"end\":44943,\"start\":44936},{\"end\":44956,\"start\":44951},{\"end\":44970,\"start\":44964},{\"end\":44990,\"start\":44984},{\"end\":45005,\"start\":44997},{\"end\":45313,\"start\":45308},{\"end\":45330,\"start\":45321},{\"end\":45332,\"start\":45331},{\"end\":45351,\"start\":45342},{\"end\":45621,\"start\":45616},{\"end\":45639,\"start\":45629},{\"end\":45654,\"start\":45647},{\"end\":45678,\"start\":45667},{\"end\":45680,\"start\":45679},{\"end\":45694,\"start\":45690},{\"end\":45696,\"start\":45695},{\"end\":45973,\"start\":45972},{\"end\":45986,\"start\":45980},{\"end\":46002,\"start\":45994},{\"end\":46013,\"start\":46009},{\"end\":46015,\"start\":46014},{\"end\":46311,\"start\":46305},{\"end\":46330,\"start\":46325},{\"end\":46347,\"start\":46338},{\"end\":46364,\"start\":46360},{\"end\":46385,\"start\":46378},{\"end\":46398,\"start\":46392},{\"end\":46419,\"start\":46412},{\"end\":46438,\"start\":46430},{\"end\":46454,\"start\":46449},{\"end\":46844,\"start\":46837},{\"end\":46856,\"start\":46849},{\"end\":46869,\"start\":46864},{\"end\":46884,\"start\":46877},{\"end\":46900,\"start\":46896},{\"end\":46916,\"start\":46909},{\"end\":46926,\"start\":46923},{\"end\":46943,\"start\":46937},{\"end\":47218,\"start\":47214},{\"end\":47234,\"start\":47231},{\"end\":47246,\"start\":47245},{\"end\":47248,\"start\":47247},{\"end\":47266,\"start\":47262},{\"end\":47283,\"start\":47277},{\"end\":47548,\"start\":47542},{\"end\":47560,\"start\":47557},{\"end\":47573,\"start\":47566},{\"end\":47587,\"start\":47584},{\"end\":47602,\"start\":47594},{\"end\":47612,\"start\":47608},{\"end\":47614,\"start\":47613},{\"end\":47626,\"start\":47622},{\"end\":47628,\"start\":47627},{\"end\":47639,\"start\":47633},{\"end\":48142,\"start\":48138},{\"end\":48209,\"start\":48204},{\"end\":48222,\"start\":48217},{\"end\":48235,\"start\":48231},{\"end\":48237,\"start\":48236},{\"end\":48254,\"start\":48248},{\"end\":48272,\"start\":48266},{\"end\":48289,\"start\":48285},{\"end\":48750,\"start\":48738},{\"end\":48765,\"start\":48758},{\"end\":48780,\"start\":48773},{\"end\":48797,\"start\":48789},{\"end\":48812,\"start\":48806},{\"end\":48814,\"start\":48813},{\"end\":48831,\"start\":48826},{\"end\":48849,\"start\":48845},{\"end\":48867,\"start\":48859},{\"end\":48888,\"start\":48881},{\"end\":48895,\"start\":48889},{\"end\":48909,\"start\":48901},{\"end\":48920,\"start\":48910},{\"end\":48932,\"start\":48927},{\"end\":48944,\"start\":48939},{\"end\":48962,\"start\":48958},{\"end\":48976,\"start\":48970},{\"end\":49419,\"start\":49412},{\"end\":49429,\"start\":49424},{\"end\":49440,\"start\":49435},{\"end\":49452,\"start\":49445},{\"end\":49462,\"start\":49458},{\"end\":49689,\"start\":49682},{\"end\":49698,\"start\":49694},{\"end\":49714,\"start\":49709},{\"end\":49878,\"start\":49871},{\"end\":49890,\"start\":49883},{\"end\":50055,\"start\":50048},{\"end\":50067,\"start\":50060},{\"end\":50083,\"start\":50075},{\"end\":50093,\"start\":50089},{\"end\":50230,\"start\":50229},{\"end\":50247,\"start\":50240},{\"end\":50263,\"start\":50256},{\"end\":50266,\"start\":50264},{\"end\":50280,\"start\":50277},{\"end\":50291,\"start\":50287},{\"end\":50314,\"start\":50309},{\"end\":50672,\"start\":50664},{\"end\":50686,\"start\":50681},{\"end\":50700,\"start\":50696},{\"end\":51015,\"start\":51011},{\"end\":51027,\"start\":51021},{\"end\":51036,\"start\":51034},{\"end\":51049,\"start\":51042},{\"end\":51062,\"start\":51056},{\"end\":51075,\"start\":51071},{\"end\":51086,\"start\":51082},{\"end\":51088,\"start\":51087},{\"end\":51102,\"start\":51093},{\"end\":51113,\"start\":51109},{\"end\":51121,\"start\":51118},{\"end\":51488,\"start\":51480},{\"end\":51501,\"start\":51495},{\"end\":51852,\"start\":51848},{\"end\":51867,\"start\":51862},{\"end\":51880,\"start\":51873},{\"end\":52369,\"start\":52360},{\"end\":52383,\"start\":52375},{\"end\":52396,\"start\":52390},{\"end\":52411,\"start\":52402},{\"end\":52702,\"start\":52693},{\"end\":52720,\"start\":52713},{\"end\":52729,\"start\":52725},{\"end\":52747,\"start\":52740},{\"end\":52761,\"start\":52756},{\"end\":53055,\"start\":53048},{\"end\":53326,\"start\":53317},{\"end\":53344,\"start\":53339},{\"end\":53359,\"start\":53352},{\"end\":53370,\"start\":53366},{\"end\":53390,\"start\":53383},{\"end\":53404,\"start\":53397},{\"end\":53416,\"start\":53412},{\"end\":53687,\"start\":53683},{\"end\":53946,\"start\":53941},{\"end\":53965,\"start\":53959},{\"end\":53975,\"start\":53971},{\"end\":53991,\"start\":53985},{\"end\":54006,\"start\":54002},{\"end\":54020,\"start\":54015},{\"end\":54039,\"start\":54033},{\"end\":54054,\"start\":54048},{\"end\":54068,\"start\":54062},{\"end\":54087,\"start\":54078},{\"end\":54103,\"start\":54100},{\"end\":54120,\"start\":54112},{\"end\":54566,\"start\":54557},{\"end\":54878,\"start\":54871},{\"end\":54889,\"start\":54883},{\"end\":54903,\"start\":54897},{\"end\":54918,\"start\":54912},{\"end\":54928,\"start\":54923},{\"end\":54930,\"start\":54929},{\"end\":55192,\"start\":55185},{\"end\":55201,\"start\":55197},{\"end\":55442,\"start\":55434},{\"end\":55453,\"start\":55448},{\"end\":55466,\"start\":55462},{\"end\":55702,\"start\":55694},{\"end\":55715,\"start\":55708},{\"end\":55728,\"start\":55723},{\"end\":55744,\"start\":55739},{\"end\":55757,\"start\":55751},{\"end\":55770,\"start\":55766},{\"end\":55785,\"start\":55780},{\"end\":55804,\"start\":55794},{\"end\":56126,\"start\":56118},{\"end\":56141,\"start\":56132},{\"end\":56152,\"start\":56146},{\"end\":56167,\"start\":56159},{\"end\":56466,\"start\":56461},{\"end\":56480,\"start\":56476},{\"end\":56498,\"start\":56491},{\"end\":56518,\"start\":56511},{\"end\":56530,\"start\":56523},{\"end\":56545,\"start\":56539},{\"end\":56841,\"start\":56836},{\"end\":56856,\"start\":56849},{\"end\":57109,\"start\":57102},{\"end\":57131,\"start\":57124},{\"end\":57439,\"start\":57434},{\"end\":57452,\"start\":57448},{\"end\":57466,\"start\":57462},{\"end\":57485,\"start\":57476},{\"end\":57497,\"start\":57491},{\"end\":57513,\"start\":57506},{\"end\":57527,\"start\":57522},{\"end\":57537,\"start\":57534},{\"end\":57549,\"start\":57542},{\"end\":57831,\"start\":57827},{\"end\":57846,\"start\":57840},{\"end\":57867,\"start\":57860},{\"end\":58113,\"start\":58109},{\"end\":58128,\"start\":58122},{\"end\":58144,\"start\":58139},{\"end\":58159,\"start\":58153},{\"end\":58178,\"start\":58171},{\"end\":58596,\"start\":58591},{\"end\":58613,\"start\":58608},{\"end\":58951,\"start\":58942},{\"end\":59136,\"start\":59132},{\"end\":59153,\"start\":59150},{\"end\":59163,\"start\":59160},{\"end\":59176,\"start\":59168},{\"end\":59192,\"start\":59185},{\"end\":59207,\"start\":59203},{\"end\":59219,\"start\":59212},{\"end\":59233,\"start\":59227},{\"end\":59250,\"start\":59244},{\"end\":59266,\"start\":59259},{\"end\":59639,\"start\":59638},{\"end\":59913,\"start\":59908},{\"end\":59926,\"start\":59920},{\"end\":59939,\"start\":59931},{\"end\":59951,\"start\":59947},{\"end\":59962,\"start\":59958},{\"end\":59974,\"start\":59967},{\"end\":59986,\"start\":59982},{\"end\":59995,\"start\":59991},{\"end\":60269,\"start\":60263},{\"end\":60286,\"start\":60281},{\"end\":60302,\"start\":60294},{\"end\":60313,\"start\":60310},{\"end\":60546,\"start\":60542},{\"end\":60559,\"start\":60552},{\"end\":60580,\"start\":60573},{\"end\":60595,\"start\":60588},{\"end\":60838,\"start\":60830},{\"end\":60848,\"start\":60844},{\"end\":61408,\"start\":61400},{\"end\":61420,\"start\":61415},{\"end\":61438,\"start\":61431},{\"end\":61556,\"start\":61552},{\"end\":61573,\"start\":61569},{\"end\":61588,\"start\":61582},{\"end\":61603,\"start\":61596},{\"end\":61616,\"start\":61609},{\"end\":61624,\"start\":61623},{\"end\":61639,\"start\":61633},{\"end\":61641,\"start\":61640},{\"end\":61659,\"start\":61647},{\"end\":61675,\"start\":61669},{\"end\":61696,\"start\":61695},{\"end\":62130,\"start\":62122},{\"end\":62142,\"start\":62137},{\"end\":62158,\"start\":62150},{\"end\":62174,\"start\":62168},{\"end\":62524,\"start\":62522},{\"end\":62538,\"start\":62531},{\"end\":62551,\"start\":62545},{\"end\":62560,\"start\":62557},{\"end\":62569,\"start\":62565},{\"end\":62581,\"start\":62576},{\"end\":62592,\"start\":62586},{\"end\":62830,\"start\":62826},{\"end\":62846,\"start\":62837},{\"end\":62857,\"start\":62852},{\"end\":62870,\"start\":62864},{\"end\":62882,\"start\":62878},{\"end\":63190,\"start\":63185},{\"end\":63202,\"start\":63196},{\"end\":63219,\"start\":63209},{\"end\":63233,\"start\":63227},{\"end\":63485,\"start\":63477},{\"end\":63496,\"start\":63490},{\"end\":63513,\"start\":63506},{\"end\":63533,\"start\":63528},{\"end\":63551,\"start\":63545},{\"end\":63566,\"start\":63561},{\"end\":63585,\"start\":63578},{\"end\":63603,\"start\":63596},{\"end\":63937,\"start\":63930},{\"end\":63947,\"start\":63943},{\"end\":63962,\"start\":63958},{\"end\":63977,\"start\":63972},{\"end\":63988,\"start\":63984},{\"end\":64004,\"start\":63998},{\"end\":64293,\"start\":64292},{\"end\":64298,\"start\":64294},{\"end\":64312,\"start\":64307},{\"end\":64323,\"start\":64320},{\"end\":64337,\"start\":64330},{\"end\":64351,\"start\":64346},{\"end\":64622,\"start\":64617},{\"end\":64787,\"start\":64781},{\"end\":64799,\"start\":64792},{\"end\":64815,\"start\":64807},{\"end\":64829,\"start\":64823},{\"end\":64843,\"start\":64838},{\"end\":64860,\"start\":64853},{\"end\":65121,\"start\":65120},{\"end\":65137,\"start\":65128},{\"end\":65152,\"start\":65145},{\"end\":65166,\"start\":65158},{\"end\":65168,\"start\":65167},{\"end\":65183,\"start\":65175},{\"end\":65198,\"start\":65192},{\"end\":65477,\"start\":65475},{\"end\":65490,\"start\":65485},{\"end\":65502,\"start\":65496},{\"end\":65504,\"start\":65503},{\"end\":65514,\"start\":65511},{\"end\":65531,\"start\":65520},{\"end\":65545,\"start\":65538},{\"end\":65553,\"start\":65552},{\"end\":65567,\"start\":65560},{\"end\":65839,\"start\":65834},{\"end\":65850,\"start\":65846},{\"end\":65863,\"start\":65857},{\"end\":65874,\"start\":65870},{\"end\":65886,\"start\":65881},{\"end\":65900,\"start\":65895},{\"end\":65918,\"start\":65911},{\"end\":66187,\"start\":66181},{\"end\":66200,\"start\":66194},{\"end\":66217,\"start\":66209},{\"end\":66226,\"start\":66223},{\"end\":66239,\"start\":66232},{\"end\":66249,\"start\":66245},{\"end\":66251,\"start\":66250},{\"end\":66263,\"start\":66259},{\"end\":66265,\"start\":66264}]", "bib_author_last_name": "[{\"end\":42817,\"start\":42811},{\"end\":42836,\"start\":42830},{\"end\":43043,\"start\":43036},{\"end\":43256,\"start\":43252},{\"end\":43279,\"start\":43274},{\"end\":43292,\"start\":43287},{\"end\":43303,\"start\":43296},{\"end\":43312,\"start\":43305},{\"end\":43320,\"start\":43316},{\"end\":43334,\"start\":43329},{\"end\":43348,\"start\":43344},{\"end\":43363,\"start\":43359},{\"end\":43371,\"start\":43365},{\"end\":43761,\"start\":43757},{\"end\":43780,\"start\":43770},{\"end\":43798,\"start\":43790},{\"end\":43812,\"start\":43806},{\"end\":43827,\"start\":43821},{\"end\":44266,\"start\":44262},{\"end\":44283,\"start\":44274},{\"end\":44301,\"start\":44294},{\"end\":44318,\"start\":44312},{\"end\":44594,\"start\":44590},{\"end\":44616,\"start\":44602},{\"end\":44629,\"start\":44626},{\"end\":44648,\"start\":44638},{\"end\":44920,\"start\":44916},{\"end\":44934,\"start\":44929},{\"end\":44949,\"start\":44944},{\"end\":44962,\"start\":44957},{\"end\":44982,\"start\":44971},{\"end\":44995,\"start\":44991},{\"end\":45014,\"start\":45006},{\"end\":45319,\"start\":45314},{\"end\":45340,\"start\":45333},{\"end\":45360,\"start\":45352},{\"end\":45627,\"start\":45622},{\"end\":45645,\"start\":45640},{\"end\":45665,\"start\":45655},{\"end\":45688,\"start\":45681},{\"end\":45699,\"start\":45697},{\"end\":45978,\"start\":45974},{\"end\":45992,\"start\":45987},{\"end\":46007,\"start\":46003},{\"end\":46022,\"start\":46016},{\"end\":46026,\"start\":46024},{\"end\":46323,\"start\":46312},{\"end\":46336,\"start\":46331},{\"end\":46358,\"start\":46348},{\"end\":46376,\"start\":46365},{\"end\":46390,\"start\":46386},{\"end\":46410,\"start\":46399},{\"end\":46428,\"start\":46420},{\"end\":46447,\"start\":46439},{\"end\":46462,\"start\":46455},{\"end\":46847,\"start\":46845},{\"end\":46862,\"start\":46857},{\"end\":46875,\"start\":46870},{\"end\":46894,\"start\":46885},{\"end\":46907,\"start\":46901},{\"end\":46921,\"start\":46917},{\"end\":46935,\"start\":46927},{\"end\":46951,\"start\":46944},{\"end\":47229,\"start\":47219},{\"end\":47243,\"start\":47235},{\"end\":47260,\"start\":47249},{\"end\":47275,\"start\":47267},{\"end\":47288,\"start\":47284},{\"end\":47299,\"start\":47290},{\"end\":47555,\"start\":47549},{\"end\":47564,\"start\":47561},{\"end\":47582,\"start\":47574},{\"end\":47592,\"start\":47588},{\"end\":47606,\"start\":47603},{\"end\":47620,\"start\":47615},{\"end\":47631,\"start\":47629},{\"end\":47644,\"start\":47640},{\"end\":48151,\"start\":48143},{\"end\":48215,\"start\":48210},{\"end\":48229,\"start\":48223},{\"end\":48246,\"start\":48238},{\"end\":48264,\"start\":48255},{\"end\":48283,\"start\":48273},{\"end\":48296,\"start\":48290},{\"end\":48756,\"start\":48751},{\"end\":48771,\"start\":48766},{\"end\":48787,\"start\":48781},{\"end\":48804,\"start\":48798},{\"end\":48824,\"start\":48815},{\"end\":48843,\"start\":48832},{\"end\":48857,\"start\":48850},{\"end\":48879,\"start\":48868},{\"end\":48899,\"start\":48896},{\"end\":48925,\"start\":48921},{\"end\":48937,\"start\":48933},{\"end\":48956,\"start\":48945},{\"end\":48968,\"start\":48963},{\"end\":48982,\"start\":48977},{\"end\":49422,\"start\":49420},{\"end\":49433,\"start\":49430},{\"end\":49443,\"start\":49441},{\"end\":49456,\"start\":49453},{\"end\":49471,\"start\":49463},{\"end\":49692,\"start\":49690},{\"end\":49707,\"start\":49699},{\"end\":49721,\"start\":49715},{\"end\":49881,\"start\":49879},{\"end\":49899,\"start\":49891},{\"end\":50058,\"start\":50056},{\"end\":50073,\"start\":50068},{\"end\":50087,\"start\":50084},{\"end\":50097,\"start\":50094},{\"end\":50238,\"start\":50231},{\"end\":50254,\"start\":50248},{\"end\":50275,\"start\":50267},{\"end\":50285,\"start\":50281},{\"end\":50298,\"start\":50292},{\"end\":50307,\"start\":50300},{\"end\":50324,\"start\":50315},{\"end\":50338,\"start\":50326},{\"end\":50679,\"start\":50673},{\"end\":50694,\"start\":50687},{\"end\":50705,\"start\":50701},{\"end\":51019,\"start\":51016},{\"end\":51032,\"start\":51028},{\"end\":51040,\"start\":51037},{\"end\":51054,\"start\":51050},{\"end\":51069,\"start\":51063},{\"end\":51080,\"start\":51076},{\"end\":51091,\"start\":51089},{\"end\":51107,\"start\":51103},{\"end\":51116,\"start\":51114},{\"end\":51128,\"start\":51122},{\"end\":51493,\"start\":51489},{\"end\":51506,\"start\":51502},{\"end\":51860,\"start\":51853},{\"end\":51871,\"start\":51868},{\"end\":51888,\"start\":51881},{\"end\":52373,\"start\":52370},{\"end\":52388,\"start\":52384},{\"end\":52400,\"start\":52397},{\"end\":52416,\"start\":52412},{\"end\":52711,\"start\":52703},{\"end\":52723,\"start\":52721},{\"end\":52738,\"start\":52730},{\"end\":52754,\"start\":52748},{\"end\":52768,\"start\":52762},{\"end\":53064,\"start\":53056},{\"end\":53337,\"start\":53327},{\"end\":53350,\"start\":53345},{\"end\":53364,\"start\":53360},{\"end\":53381,\"start\":53371},{\"end\":53395,\"start\":53391},{\"end\":53410,\"start\":53405},{\"end\":53424,\"start\":53417},{\"end\":53698,\"start\":53688},{\"end\":53957,\"start\":53947},{\"end\":53969,\"start\":53966},{\"end\":53983,\"start\":53976},{\"end\":54000,\"start\":53992},{\"end\":54013,\"start\":54007},{\"end\":54031,\"start\":54021},{\"end\":54046,\"start\":54040},{\"end\":54060,\"start\":54055},{\"end\":54076,\"start\":54069},{\"end\":54098,\"start\":54088},{\"end\":54110,\"start\":54104},{\"end\":54128,\"start\":54121},{\"end\":54570,\"start\":54567},{\"end\":54881,\"start\":54879},{\"end\":54895,\"start\":54890},{\"end\":54910,\"start\":54904},{\"end\":54921,\"start\":54919},{\"end\":54936,\"start\":54931},{\"end\":55195,\"start\":55193},{\"end\":55209,\"start\":55202},{\"end\":55446,\"start\":55443},{\"end\":55460,\"start\":55454},{\"end\":55475,\"start\":55467},{\"end\":55706,\"start\":55703},{\"end\":55721,\"start\":55716},{\"end\":55737,\"start\":55729},{\"end\":55749,\"start\":55745},{\"end\":55764,\"start\":55758},{\"end\":55778,\"start\":55771},{\"end\":55792,\"start\":55786},{\"end\":55812,\"start\":55805},{\"end\":56130,\"start\":56127},{\"end\":56144,\"start\":56142},{\"end\":56157,\"start\":56153},{\"end\":56171,\"start\":56168},{\"end\":56474,\"start\":56467},{\"end\":56489,\"start\":56481},{\"end\":56509,\"start\":56499},{\"end\":56521,\"start\":56519},{\"end\":56537,\"start\":56531},{\"end\":56548,\"start\":56546},{\"end\":56847,\"start\":56842},{\"end\":56871,\"start\":56857},{\"end\":57122,\"start\":57110},{\"end\":57137,\"start\":57132},{\"end\":57446,\"start\":57440},{\"end\":57460,\"start\":57453},{\"end\":57474,\"start\":57467},{\"end\":57489,\"start\":57486},{\"end\":57504,\"start\":57498},{\"end\":57520,\"start\":57514},{\"end\":57532,\"start\":57528},{\"end\":57540,\"start\":57538},{\"end\":57553,\"start\":57550},{\"end\":57838,\"start\":57832},{\"end\":57858,\"start\":57847},{\"end\":57874,\"start\":57868},{\"end\":58120,\"start\":58114},{\"end\":58137,\"start\":58129},{\"end\":58151,\"start\":58145},{\"end\":58169,\"start\":58160},{\"end\":58185,\"start\":58179},{\"end\":58606,\"start\":58597},{\"end\":58620,\"start\":58614},{\"end\":58634,\"start\":58622},{\"end\":58957,\"start\":58952},{\"end\":59148,\"start\":59137},{\"end\":59158,\"start\":59154},{\"end\":59166,\"start\":59164},{\"end\":59183,\"start\":59177},{\"end\":59201,\"start\":59193},{\"end\":59210,\"start\":59208},{\"end\":59225,\"start\":59220},{\"end\":59242,\"start\":59234},{\"end\":59257,\"start\":59251},{\"end\":59276,\"start\":59267},{\"end\":59647,\"start\":59640},{\"end\":59918,\"start\":59914},{\"end\":59929,\"start\":59927},{\"end\":59945,\"start\":59940},{\"end\":59956,\"start\":59952},{\"end\":59965,\"start\":59963},{\"end\":59980,\"start\":59975},{\"end\":59989,\"start\":59987},{\"end\":59999,\"start\":59996},{\"end\":60279,\"start\":60270},{\"end\":60292,\"start\":60287},{\"end\":60308,\"start\":60303},{\"end\":60320,\"start\":60314},{\"end\":60550,\"start\":60547},{\"end\":60571,\"start\":60560},{\"end\":60586,\"start\":60581},{\"end\":60601,\"start\":60596},{\"end\":60842,\"start\":60839},{\"end\":60851,\"start\":60849},{\"end\":60865,\"start\":60853},{\"end\":61413,\"start\":61409},{\"end\":61429,\"start\":61421},{\"end\":61444,\"start\":61439},{\"end\":61567,\"start\":61557},{\"end\":61580,\"start\":61574},{\"end\":61594,\"start\":61589},{\"end\":61607,\"start\":61604},{\"end\":61621,\"start\":61617},{\"end\":61631,\"start\":61625},{\"end\":61645,\"start\":61642},{\"end\":61667,\"start\":61660},{\"end\":61685,\"start\":61676},{\"end\":61693,\"start\":61687},{\"end\":61704,\"start\":61697},{\"end\":61712,\"start\":61706},{\"end\":62135,\"start\":62131},{\"end\":62148,\"start\":62143},{\"end\":62166,\"start\":62159},{\"end\":62179,\"start\":62175},{\"end\":62529,\"start\":62525},{\"end\":62543,\"start\":62539},{\"end\":62555,\"start\":62552},{\"end\":62563,\"start\":62561},{\"end\":62574,\"start\":62570},{\"end\":62584,\"start\":62582},{\"end\":62596,\"start\":62593},{\"end\":62835,\"start\":62831},{\"end\":62850,\"start\":62847},{\"end\":62862,\"start\":62858},{\"end\":62876,\"start\":62871},{\"end\":62886,\"start\":62883},{\"end\":63194,\"start\":63191},{\"end\":63207,\"start\":63203},{\"end\":63225,\"start\":63220},{\"end\":63236,\"start\":63234},{\"end\":63488,\"start\":63486},{\"end\":63504,\"start\":63497},{\"end\":63526,\"start\":63514},{\"end\":63543,\"start\":63534},{\"end\":63559,\"start\":63552},{\"end\":63576,\"start\":63567},{\"end\":63594,\"start\":63586},{\"end\":63608,\"start\":63604},{\"end\":63941,\"start\":63938},{\"end\":63956,\"start\":63948},{\"end\":63970,\"start\":63963},{\"end\":63982,\"start\":63978},{\"end\":63996,\"start\":63989},{\"end\":64013,\"start\":64005},{\"end\":64305,\"start\":64299},{\"end\":64318,\"start\":64313},{\"end\":64328,\"start\":64324},{\"end\":64344,\"start\":64338},{\"end\":64359,\"start\":64352},{\"end\":64631,\"start\":64623},{\"end\":64790,\"start\":64788},{\"end\":64805,\"start\":64800},{\"end\":64821,\"start\":64816},{\"end\":64836,\"start\":64830},{\"end\":64851,\"start\":64844},{\"end\":64865,\"start\":64861},{\"end\":65126,\"start\":65122},{\"end\":65143,\"start\":65138},{\"end\":65156,\"start\":65153},{\"end\":65173,\"start\":65169},{\"end\":65190,\"start\":65184},{\"end\":65204,\"start\":65199},{\"end\":65214,\"start\":65206},{\"end\":65483,\"start\":65478},{\"end\":65494,\"start\":65491},{\"end\":65509,\"start\":65505},{\"end\":65518,\"start\":65515},{\"end\":65536,\"start\":65532},{\"end\":65550,\"start\":65546},{\"end\":65558,\"start\":65554},{\"end\":65570,\"start\":65568},{\"end\":65574,\"start\":65572},{\"end\":65844,\"start\":65840},{\"end\":65855,\"start\":65851},{\"end\":65868,\"start\":65864},{\"end\":65879,\"start\":65875},{\"end\":65893,\"start\":65887},{\"end\":65909,\"start\":65901},{\"end\":65927,\"start\":65919},{\"end\":66192,\"start\":66188},{\"end\":66207,\"start\":66201},{\"end\":66221,\"start\":66218},{\"end\":66230,\"start\":66227},{\"end\":66243,\"start\":66240},{\"end\":66257,\"start\":66252},{\"end\":66268,\"start\":66266}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":4332326},\"end\":43009,\"start\":42721},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":45998148},\"end\":43138,\"start\":43011},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":218763503},\"end\":43630,\"start\":43140},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3429309},\"end\":44184,\"start\":43632},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":211096730},\"end\":44494,\"start\":44186},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":4703661},\"end\":44830,\"start\":44496},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":222341884},\"end\":45234,\"start\":44832},{\"attributes\":{\"doi\":\"abs/2107.06278\",\"id\":\"b7\"},\"end\":45542,\"start\":45236},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":85464175},\"end\":45890,\"start\":45544},{\"attributes\":{\"id\":\"b9\"},\"end\":46177,\"start\":45892},{\"attributes\":{\"id\":\"b10\"},\"end\":46763,\"start\":46179},{\"attributes\":{\"id\":\"b11\"},\"end\":47156,\"start\":46765},{\"attributes\":{\"id\":\"b12\"},\"end\":47458,\"start\":47158},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":229152875},\"end\":48124,\"start\":47460},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206770307},\"end\":48202,\"start\":48126},{\"attributes\":{\"doi\":\"arXiv:1706.02677\",\"id\":\"b15\"},\"end\":48665,\"start\":48204},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":219687798},\"end\":49343,\"start\":48667},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":207930212},\"end\":49646,\"start\":49345},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":53739271},\"end\":49824,\"start\":49648},{\"attributes\":{\"id\":\"b19\"},\"end\":50000,\"start\":49826},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":206594692},\"end\":50227,\"start\":50002},{\"attributes\":{\"doi\":\"arXiv:1905.09272\",\"id\":\"b21\"},\"end\":50662,\"start\":50229},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b22\"},\"end\":50918,\"start\":50664},{\"attributes\":{\"doi\":\"abs/2102.05918\",\"id\":\"b23\"},\"end\":51401,\"start\":50920},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":62841734},\"end\":51758,\"start\":51403},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":4800342},\"end\":52277,\"start\":51760},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":44100626},\"end\":52668,\"start\":52279},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":4853375},\"end\":52906,\"start\":52670},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":8070108},\"end\":53255,\"start\":52908},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":214728308},\"end\":53626,\"start\":53257},{\"attributes\":{\"id\":\"b30\"},\"end\":53813,\"start\":53628},{\"attributes\":{\"id\":\"b31\"},\"end\":54458,\"start\":53815},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":18507866},\"end\":54820,\"start\":54460},{\"attributes\":{\"doi\":\"abs/1904.05871\",\"id\":\"b33\"},\"end\":55113,\"start\":54822},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":4572038},\"end\":55334,\"start\":55115},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":10716717},\"end\":55649,\"start\":55336},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":14113767},\"end\":56011,\"start\":55651},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":128345418},\"end\":56359,\"start\":56013},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":13751202},\"end\":56771,\"start\":56361},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":208617491},\"end\":57002,\"start\":56773},{\"attributes\":{\"doi\":\"arXiv:2007.13916\",\"id\":\"b40\"},\"end\":57349,\"start\":57004},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":204838007},\"end\":57783,\"start\":57351},{\"attributes\":{\"id\":\"b42\"},\"end\":58010,\"start\":57785},{\"attributes\":{\"doi\":\"2020. 3\",\"id\":\"b43\",\"matched_paper_id\":195776274},\"end\":58531,\"start\":58012},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":7648360},\"end\":58901,\"start\":58533},{\"attributes\":{\"doi\":\"arXiv:1706.05098\",\"id\":\"b45\",\"matched_paper_id\":90063862},\"end\":59128,\"start\":58903},{\"attributes\":{\"id\":\"b46\"},\"end\":59569,\"start\":59130},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":30807376},\"end\":59836,\"start\":59571},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":207967883},\"end\":60201,\"start\":59838},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":545361},\"end\":60472,\"start\":60203},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":6842201},\"end\":60768,\"start\":60474},{\"attributes\":{\"doi\":\"PMLR, 09-15\",\"id\":\"b51\"},\"end\":61368,\"start\":60770},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":189762205},\"end\":61550,\"start\":61370},{\"attributes\":{\"doi\":\"arXiv:1908.00463\",\"id\":\"b53\"},\"end\":62049,\"start\":61552},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":131775376},\"end\":62450,\"start\":62051},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":52860134},\"end\":62772,\"start\":62452},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":50781105},\"end\":63116,\"start\":62774},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":207853355},\"end\":63402,\"start\":63118},{\"attributes\":{\"id\":\"b58\"},\"end\":63829,\"start\":63404},{\"attributes\":{\"id\":\"b59\"},\"end\":64225,\"start\":63831},{\"attributes\":{\"doi\":\"abs/1905.00546\",\"id\":\"b60\"},\"end\":64547,\"start\":64227},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":1487550},\"end\":64737,\"start\":64549},{\"attributes\":{\"doi\":\"arXiv:2001.06782\",\"id\":\"b62\"},\"end\":65069,\"start\":64739},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":5046249},\"end\":65392,\"start\":65071},{\"attributes\":{\"id\":\"b64\"},\"end\":65771,\"start\":65394},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":11371972},\"end\":66136,\"start\":65773},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":219635973},\"end\":66445,\"start\":66138}]", "bib_title": "[{\"end\":42801,\"start\":42721},{\"end\":43029,\"start\":43011},{\"end\":43238,\"start\":43140},{\"end\":43743,\"start\":43632},{\"end\":44255,\"start\":44186},{\"end\":44583,\"start\":44496},{\"end\":44909,\"start\":44832},{\"end\":45614,\"start\":45544},{\"end\":47540,\"start\":47460},{\"end\":48136,\"start\":48126},{\"end\":48736,\"start\":48667},{\"end\":49410,\"start\":49345},{\"end\":49680,\"start\":49648},{\"end\":49869,\"start\":49826},{\"end\":50046,\"start\":50002},{\"end\":51478,\"start\":51403},{\"end\":51846,\"start\":51760},{\"end\":52358,\"start\":52279},{\"end\":52691,\"start\":52670},{\"end\":53046,\"start\":52908},{\"end\":53315,\"start\":53257},{\"end\":54555,\"start\":54460},{\"end\":55183,\"start\":55115},{\"end\":55432,\"start\":55336},{\"end\":55692,\"start\":55651},{\"end\":56116,\"start\":56013},{\"end\":56459,\"start\":56361},{\"end\":56834,\"start\":56773},{\"end\":57432,\"start\":57351},{\"end\":58107,\"start\":58012},{\"end\":58589,\"start\":58533},{\"end\":58940,\"start\":58903},{\"end\":59636,\"start\":59571},{\"end\":59906,\"start\":59838},{\"end\":60261,\"start\":60203},{\"end\":60540,\"start\":60474},{\"end\":60828,\"start\":60770},{\"end\":61398,\"start\":61370},{\"end\":62120,\"start\":62051},{\"end\":62520,\"start\":62452},{\"end\":62824,\"start\":62774},{\"end\":63183,\"start\":63118},{\"end\":63928,\"start\":63831},{\"end\":64615,\"start\":64549},{\"end\":65118,\"start\":65071},{\"end\":65832,\"start\":65773},{\"end\":66179,\"start\":66138}]", "bib_author": "[{\"end\":42819,\"start\":42803},{\"end\":42838,\"start\":42819},{\"end\":43045,\"start\":43031},{\"end\":43258,\"start\":43240},{\"end\":43281,\"start\":43258},{\"end\":43294,\"start\":43281},{\"end\":43305,\"start\":43294},{\"end\":43314,\"start\":43305},{\"end\":43322,\"start\":43314},{\"end\":43336,\"start\":43322},{\"end\":43350,\"start\":43336},{\"end\":43365,\"start\":43350},{\"end\":43373,\"start\":43365},{\"end\":43763,\"start\":43745},{\"end\":43782,\"start\":43763},{\"end\":43800,\"start\":43782},{\"end\":43814,\"start\":43800},{\"end\":43829,\"start\":43814},{\"end\":44268,\"start\":44257},{\"end\":44285,\"start\":44268},{\"end\":44303,\"start\":44285},{\"end\":44320,\"start\":44303},{\"end\":44596,\"start\":44585},{\"end\":44618,\"start\":44596},{\"end\":44631,\"start\":44618},{\"end\":44650,\"start\":44631},{\"end\":44922,\"start\":44911},{\"end\":44936,\"start\":44922},{\"end\":44951,\"start\":44936},{\"end\":44964,\"start\":44951},{\"end\":44984,\"start\":44964},{\"end\":44997,\"start\":44984},{\"end\":45016,\"start\":44997},{\"end\":45321,\"start\":45308},{\"end\":45342,\"start\":45321},{\"end\":45362,\"start\":45342},{\"end\":45629,\"start\":45616},{\"end\":45647,\"start\":45629},{\"end\":45667,\"start\":45647},{\"end\":45690,\"start\":45667},{\"end\":45701,\"start\":45690},{\"end\":45980,\"start\":45972},{\"end\":45994,\"start\":45980},{\"end\":46009,\"start\":45994},{\"end\":46024,\"start\":46009},{\"end\":46028,\"start\":46024},{\"end\":46325,\"start\":46305},{\"end\":46338,\"start\":46325},{\"end\":46360,\"start\":46338},{\"end\":46378,\"start\":46360},{\"end\":46392,\"start\":46378},{\"end\":46412,\"start\":46392},{\"end\":46430,\"start\":46412},{\"end\":46449,\"start\":46430},{\"end\":46464,\"start\":46449},{\"end\":46849,\"start\":46837},{\"end\":46864,\"start\":46849},{\"end\":46877,\"start\":46864},{\"end\":46896,\"start\":46877},{\"end\":46909,\"start\":46896},{\"end\":46923,\"start\":46909},{\"end\":46937,\"start\":46923},{\"end\":46953,\"start\":46937},{\"end\":47231,\"start\":47214},{\"end\":47245,\"start\":47231},{\"end\":47262,\"start\":47245},{\"end\":47277,\"start\":47262},{\"end\":47290,\"start\":47277},{\"end\":47301,\"start\":47290},{\"end\":47557,\"start\":47542},{\"end\":47566,\"start\":47557},{\"end\":47584,\"start\":47566},{\"end\":47594,\"start\":47584},{\"end\":47608,\"start\":47594},{\"end\":47622,\"start\":47608},{\"end\":47633,\"start\":47622},{\"end\":47646,\"start\":47633},{\"end\":48153,\"start\":48138},{\"end\":48217,\"start\":48204},{\"end\":48231,\"start\":48217},{\"end\":48248,\"start\":48231},{\"end\":48266,\"start\":48248},{\"end\":48285,\"start\":48266},{\"end\":48298,\"start\":48285},{\"end\":48758,\"start\":48738},{\"end\":48773,\"start\":48758},{\"end\":48789,\"start\":48773},{\"end\":48806,\"start\":48789},{\"end\":48826,\"start\":48806},{\"end\":48845,\"start\":48826},{\"end\":48859,\"start\":48845},{\"end\":48881,\"start\":48859},{\"end\":48901,\"start\":48881},{\"end\":48927,\"start\":48901},{\"end\":48939,\"start\":48927},{\"end\":48958,\"start\":48939},{\"end\":48970,\"start\":48958},{\"end\":48984,\"start\":48970},{\"end\":49424,\"start\":49412},{\"end\":49435,\"start\":49424},{\"end\":49445,\"start\":49435},{\"end\":49458,\"start\":49445},{\"end\":49473,\"start\":49458},{\"end\":49694,\"start\":49682},{\"end\":49709,\"start\":49694},{\"end\":49723,\"start\":49709},{\"end\":49883,\"start\":49871},{\"end\":49901,\"start\":49883},{\"end\":50060,\"start\":50048},{\"end\":50075,\"start\":50060},{\"end\":50089,\"start\":50075},{\"end\":50099,\"start\":50089},{\"end\":50240,\"start\":50229},{\"end\":50256,\"start\":50240},{\"end\":50277,\"start\":50256},{\"end\":50287,\"start\":50277},{\"end\":50300,\"start\":50287},{\"end\":50309,\"start\":50300},{\"end\":50326,\"start\":50309},{\"end\":50340,\"start\":50326},{\"end\":50681,\"start\":50664},{\"end\":50696,\"start\":50681},{\"end\":50707,\"start\":50696},{\"end\":51021,\"start\":51011},{\"end\":51034,\"start\":51021},{\"end\":51042,\"start\":51034},{\"end\":51056,\"start\":51042},{\"end\":51071,\"start\":51056},{\"end\":51082,\"start\":51071},{\"end\":51093,\"start\":51082},{\"end\":51109,\"start\":51093},{\"end\":51118,\"start\":51109},{\"end\":51130,\"start\":51118},{\"end\":51495,\"start\":51480},{\"end\":51508,\"start\":51495},{\"end\":51862,\"start\":51848},{\"end\":51873,\"start\":51862},{\"end\":51890,\"start\":51873},{\"end\":52375,\"start\":52360},{\"end\":52390,\"start\":52375},{\"end\":52402,\"start\":52390},{\"end\":52418,\"start\":52402},{\"end\":52713,\"start\":52693},{\"end\":52725,\"start\":52713},{\"end\":52740,\"start\":52725},{\"end\":52756,\"start\":52740},{\"end\":52770,\"start\":52756},{\"end\":53066,\"start\":53048},{\"end\":53339,\"start\":53317},{\"end\":53352,\"start\":53339},{\"end\":53366,\"start\":53352},{\"end\":53383,\"start\":53366},{\"end\":53397,\"start\":53383},{\"end\":53412,\"start\":53397},{\"end\":53426,\"start\":53412},{\"end\":53700,\"start\":53683},{\"end\":53959,\"start\":53941},{\"end\":53971,\"start\":53959},{\"end\":53985,\"start\":53971},{\"end\":54002,\"start\":53985},{\"end\":54015,\"start\":54002},{\"end\":54033,\"start\":54015},{\"end\":54048,\"start\":54033},{\"end\":54062,\"start\":54048},{\"end\":54078,\"start\":54062},{\"end\":54100,\"start\":54078},{\"end\":54112,\"start\":54100},{\"end\":54130,\"start\":54112},{\"end\":54572,\"start\":54557},{\"end\":54883,\"start\":54871},{\"end\":54897,\"start\":54883},{\"end\":54912,\"start\":54897},{\"end\":54923,\"start\":54912},{\"end\":54938,\"start\":54923},{\"end\":55197,\"start\":55185},{\"end\":55211,\"start\":55197},{\"end\":55448,\"start\":55434},{\"end\":55462,\"start\":55448},{\"end\":55477,\"start\":55462},{\"end\":55708,\"start\":55694},{\"end\":55723,\"start\":55708},{\"end\":55739,\"start\":55723},{\"end\":55751,\"start\":55739},{\"end\":55766,\"start\":55751},{\"end\":55780,\"start\":55766},{\"end\":55794,\"start\":55780},{\"end\":55814,\"start\":55794},{\"end\":56132,\"start\":56118},{\"end\":56146,\"start\":56132},{\"end\":56159,\"start\":56146},{\"end\":56173,\"start\":56159},{\"end\":56476,\"start\":56461},{\"end\":56491,\"start\":56476},{\"end\":56511,\"start\":56491},{\"end\":56523,\"start\":56511},{\"end\":56539,\"start\":56523},{\"end\":56550,\"start\":56539},{\"end\":56849,\"start\":56836},{\"end\":56873,\"start\":56849},{\"end\":57124,\"start\":57102},{\"end\":57139,\"start\":57124},{\"end\":57448,\"start\":57434},{\"end\":57462,\"start\":57448},{\"end\":57476,\"start\":57462},{\"end\":57491,\"start\":57476},{\"end\":57506,\"start\":57491},{\"end\":57522,\"start\":57506},{\"end\":57534,\"start\":57522},{\"end\":57542,\"start\":57534},{\"end\":57555,\"start\":57542},{\"end\":57840,\"start\":57827},{\"end\":57860,\"start\":57840},{\"end\":57876,\"start\":57860},{\"end\":58122,\"start\":58109},{\"end\":58139,\"start\":58122},{\"end\":58153,\"start\":58139},{\"end\":58171,\"start\":58153},{\"end\":58187,\"start\":58171},{\"end\":58608,\"start\":58591},{\"end\":58622,\"start\":58608},{\"end\":58636,\"start\":58622},{\"end\":58959,\"start\":58942},{\"end\":59150,\"start\":59132},{\"end\":59160,\"start\":59150},{\"end\":59168,\"start\":59160},{\"end\":59185,\"start\":59168},{\"end\":59203,\"start\":59185},{\"end\":59212,\"start\":59203},{\"end\":59227,\"start\":59212},{\"end\":59244,\"start\":59227},{\"end\":59259,\"start\":59244},{\"end\":59278,\"start\":59259},{\"end\":59649,\"start\":59638},{\"end\":59920,\"start\":59908},{\"end\":59931,\"start\":59920},{\"end\":59947,\"start\":59931},{\"end\":59958,\"start\":59947},{\"end\":59967,\"start\":59958},{\"end\":59982,\"start\":59967},{\"end\":59991,\"start\":59982},{\"end\":60001,\"start\":59991},{\"end\":60281,\"start\":60263},{\"end\":60294,\"start\":60281},{\"end\":60310,\"start\":60294},{\"end\":60322,\"start\":60310},{\"end\":60552,\"start\":60542},{\"end\":60573,\"start\":60552},{\"end\":60588,\"start\":60573},{\"end\":60603,\"start\":60588},{\"end\":60844,\"start\":60830},{\"end\":60853,\"start\":60844},{\"end\":60867,\"start\":60853},{\"end\":61415,\"start\":61400},{\"end\":61431,\"start\":61415},{\"end\":61446,\"start\":61431},{\"end\":61569,\"start\":61552},{\"end\":61582,\"start\":61569},{\"end\":61596,\"start\":61582},{\"end\":61609,\"start\":61596},{\"end\":61623,\"start\":61609},{\"end\":61633,\"start\":61623},{\"end\":61647,\"start\":61633},{\"end\":61669,\"start\":61647},{\"end\":61687,\"start\":61669},{\"end\":61695,\"start\":61687},{\"end\":61706,\"start\":61695},{\"end\":61714,\"start\":61706},{\"end\":62137,\"start\":62122},{\"end\":62150,\"start\":62137},{\"end\":62168,\"start\":62150},{\"end\":62181,\"start\":62168},{\"end\":62531,\"start\":62522},{\"end\":62545,\"start\":62531},{\"end\":62557,\"start\":62545},{\"end\":62565,\"start\":62557},{\"end\":62576,\"start\":62565},{\"end\":62586,\"start\":62576},{\"end\":62598,\"start\":62586},{\"end\":62837,\"start\":62826},{\"end\":62852,\"start\":62837},{\"end\":62864,\"start\":62852},{\"end\":62878,\"start\":62864},{\"end\":62888,\"start\":62878},{\"end\":63196,\"start\":63185},{\"end\":63209,\"start\":63196},{\"end\":63227,\"start\":63209},{\"end\":63238,\"start\":63227},{\"end\":63490,\"start\":63477},{\"end\":63506,\"start\":63490},{\"end\":63528,\"start\":63506},{\"end\":63545,\"start\":63528},{\"end\":63561,\"start\":63545},{\"end\":63578,\"start\":63561},{\"end\":63596,\"start\":63578},{\"end\":63610,\"start\":63596},{\"end\":63943,\"start\":63930},{\"end\":63958,\"start\":63943},{\"end\":63972,\"start\":63958},{\"end\":63984,\"start\":63972},{\"end\":63998,\"start\":63984},{\"end\":64015,\"start\":63998},{\"end\":64307,\"start\":64292},{\"end\":64320,\"start\":64307},{\"end\":64330,\"start\":64320},{\"end\":64346,\"start\":64330},{\"end\":64361,\"start\":64346},{\"end\":64633,\"start\":64617},{\"end\":64792,\"start\":64781},{\"end\":64807,\"start\":64792},{\"end\":64823,\"start\":64807},{\"end\":64838,\"start\":64823},{\"end\":64853,\"start\":64838},{\"end\":64867,\"start\":64853},{\"end\":65128,\"start\":65120},{\"end\":65145,\"start\":65128},{\"end\":65158,\"start\":65145},{\"end\":65175,\"start\":65158},{\"end\":65192,\"start\":65175},{\"end\":65206,\"start\":65192},{\"end\":65216,\"start\":65206},{\"end\":65485,\"start\":65475},{\"end\":65496,\"start\":65485},{\"end\":65511,\"start\":65496},{\"end\":65520,\"start\":65511},{\"end\":65538,\"start\":65520},{\"end\":65552,\"start\":65538},{\"end\":65560,\"start\":65552},{\"end\":65572,\"start\":65560},{\"end\":65576,\"start\":65572},{\"end\":65846,\"start\":65834},{\"end\":65857,\"start\":65846},{\"end\":65870,\"start\":65857},{\"end\":65881,\"start\":65870},{\"end\":65895,\"start\":65881},{\"end\":65911,\"start\":65895},{\"end\":65929,\"start\":65911},{\"end\":66194,\"start\":66181},{\"end\":66209,\"start\":66194},{\"end\":66223,\"start\":66209},{\"end\":66232,\"start\":66223},{\"end\":66245,\"start\":66232},{\"end\":66259,\"start\":66245},{\"end\":66270,\"start\":66259}]", "bib_venue": "[{\"end\":42844,\"start\":42838},{\"end\":43061,\"start\":43045},{\"end\":43377,\"start\":43373},{\"end\":43891,\"start\":43829},{\"end\":44324,\"start\":44320},{\"end\":44654,\"start\":44650},{\"end\":45023,\"start\":45016},{\"end\":45306,\"start\":45236},{\"end\":45704,\"start\":45701},{\"end\":45970,\"start\":45892},{\"end\":46303,\"start\":46179},{\"end\":46835,\"start\":46765},{\"end\":47212,\"start\":47158},{\"end\":47734,\"start\":47646},{\"end\":48157,\"start\":48153},{\"end\":48402,\"start\":48314},{\"end\":48991,\"start\":48984},{\"end\":49486,\"start\":49473},{\"end\":49727,\"start\":49723},{\"end\":49905,\"start\":49901},{\"end\":50103,\"start\":50099},{\"end\":50423,\"start\":50356},{\"end\":50767,\"start\":50723},{\"end\":51009,\"start\":50920},{\"end\":51570,\"start\":51508},{\"end\":51974,\"start\":51890},{\"end\":52455,\"start\":52418},{\"end\":52774,\"start\":52770},{\"end\":53070,\"start\":53066},{\"end\":53430,\"start\":53426},{\"end\":53681,\"start\":53628},{\"end\":53939,\"start\":53815},{\"end\":54627,\"start\":54572},{\"end\":54869,\"start\":54822},{\"end\":55215,\"start\":55211},{\"end\":55481,\"start\":55477},{\"end\":55818,\"start\":55814},{\"end\":56176,\"start\":56173},{\"end\":56554,\"start\":56550},{\"end\":56877,\"start\":56873},{\"end\":57100,\"start\":57004},{\"end\":57559,\"start\":57555},{\"end\":57825,\"start\":57785},{\"end\":58256,\"start\":58194},{\"end\":58702,\"start\":58636},{\"end\":58995,\"start\":58975},{\"end\":59688,\"start\":59649},{\"end\":60005,\"start\":60001},{\"end\":60326,\"start\":60322},{\"end\":60607,\"start\":60603},{\"end\":60946,\"start\":60878},{\"end\":61450,\"start\":61446},{\"end\":61770,\"start\":61730},{\"end\":62229,\"start\":62181},{\"end\":62602,\"start\":62598},{\"end\":62926,\"start\":62888},{\"end\":63251,\"start\":63238},{\"end\":63475,\"start\":63404},{\"end\":64020,\"start\":64015},{\"end\":64290,\"start\":64227},{\"end\":64636,\"start\":64633},{\"end\":64779,\"start\":64739},{\"end\":65223,\"start\":65216},{\"end\":65473,\"start\":65394},{\"end\":65933,\"start\":65929},{\"end\":66277,\"start\":66270},{\"end\":47809,\"start\":47736},{\"end\":52045,\"start\":51976},{\"end\":61044,\"start\":60991}]"}}}, "year": 2023, "month": 12, "day": 17}