{"id": 202558560, "updated": "2023-11-11 00:18:04.23", "metadata": {"title": "Joint Learning of Graph Representation and Node Features in Graph Convolutional Neural Networks", "authors": "[{\"first\":\"Jiaxiang\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Xiang\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Zongming\",\"last\":\"Guo\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": 9, "day": 11}, "abstract": "Graph Convolutional Neural Networks (GCNNs) extend classical CNNs to graph data domain, such as brain networks, social networks and 3D point clouds. It is critical to identify an appropriate graph for the subsequent graph convolution. Existing methods manually construct or learn one fixed graph for all the layers of a GCNN. In order to adapt to the underlying structure of node features in different layers, we propose dynamic learning of graphs and node features jointly in GCNNs. In particular, we cast the graph optimization problem as distance metric learning to capture pairwise similarities of features in each layer. We deploy the Mahalanobis distance metric and further decompose the metric matrix into a low-dimensional matrix, which converts graph learning to the optimization of a low-dimensional matrix for efficient implementation. Extensive experiments on point clouds and citation network datasets demonstrate the superiority of the proposed method in terms of both accuracies and robustness.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": null}, "content": {"source": {"pdf_hash": "6f8b327b9c950daecf1ed36d03ca7708a9aa06bf", "pdf_src": "ArXiv", "pdf_uri": "[\"https://arxiv.org/pdf/1909.04931v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "35fc251d5f78835acb6da9d66396b024a2edeba9", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/6f8b327b9c950daecf1ed36d03ca7708a9aa06bf.txt", "contents": "\nJOINT GRAPH AND FEATURE LEARNING IN GRAPH CONVOLUTIONAL NEURAL NETWORKS A PREPRINT\nSeptember 12, 2019\n\nJiaxiang Tang \nInstitute of Computer Science and Technology\nPeking University\nChina\n\nWei Hu \nInstitute of Computer Science and Technology\nPeking University\nChina\n\nXiang Gao \nInstitute of Computer Science and Technology\nPeking University\nChina\n\nZongming Guo guozongming@pku.edu.cn \nInstitute of Computer Science and Technology\nPeking University\nChina\n\nJOINT GRAPH AND FEATURE LEARNING IN GRAPH CONVOLUTIONAL NEURAL NETWORKS A PREPRINT\nSeptember 12, 2019384E42CFF7EFA46C5331D99DF92F4CD4\nGraph Convolutional Neural Networks (GCNNs) extend classical CNNs to graph data domain, such as brain networks, social networks and 3D point clouds.It is critical to identify an appropriate graph for the subsequent graph convolution.Existing methods manually construct or learn one fixed graph for all the layers of a GCNN.In order to adapt to the underlying structure of node features in different layers, we propose dynamic learning of graphs and node features jointly in GCNNs.In particular, we cast the graph optimization problem as distance metric learning to capture pairwise similarities of features in each layer.We deploy the Mahalanobis distance metric and further decompose the metric matrix into a low-dimensional matrix, which converts graph learning to the optimization of a lowdimensional matrix for efficient implementation.Extensive experiments on point clouds and citation network datasets demonstrate the superiority of the proposed method in terms of both accuracies and robustness.\n\nIntroduction\n\nGraph Convolutional Neural Networks (GCNNs) have been receiving increasing attention as a powerful tool for irregularly structured data on graphs, such as citation networks, social networks and 3D point clouds.The construction of an appropriate graph topology plays a critical role in GCNNs for efficient feature learning.In settings where the graph is inaccurate or even not readily available, it is necessary to infer or learn a graph topology from data before it is used for guiding graph convolution in GCNNs.\n\nMost of the previous studies construct underlying graphs from data empirically, such as k-Nearest-Neighbor (k-NN) graphs [1,2], which may lead to sub-optimal solutions.Few methods exploit graph learning for optimized representation [3,4,5,6], which learns a fixed and shared graph for all instances, or an individual graph for each instance, or a combination of shared and individual graphs.However, only one fixed graph is learned and applied to all layers of the entire network, which may not well capture the underlying structure of node features in different layers dynamically.\n\nExtending on these previous studies, we propose a Joint Learning Graph Convolutional Network (JLGCN), which exploits dynamic learning of graphs and node features jointly in GCNNs.In particular, we optimize an underlying graph kernel from data features via distance metric learning that characterizes pairwise similarities of data.We deploy the Mahalanobis distance [7], which takes into account data correlations for intrinsic representation.Given a Kdimensional feature vector f i per node i, f i \u2208 R K , the Mahalanobis distance between the features f i and f j of nodes i and j is defined as: d M (f i , f j ) = (f i \u2212 f j ) M(f i \u2212 f j ), where M \u2208 R K\u00d7K is the Mahalanobis distance metric matrix which reflects feature correlations.Hence, we convert the problem of graph learning to the optimization of M. As M is positive semi-definite (PSD), it is often nontrivial to solve efficiently.Instead, we decompose it as M = RR and learn R for ease of optimization, where R \u2208 R K\u00d7S has a lower dimension S << K to reduce number of parameters for efficient implementation.Given features f , we seek to minimize the Graph Laplacian Regularizer (GLR) [8] f L(R)f by optimizing R, which measures the smoothness of features f with respect to the graph Laplacian L1 .This essentially enforces the graph encoded in L(R) to capture pairwise similarities of f .Hence, we formulate the joint learning of graphs L(R) and node features f as an optimization problem, which minimizes a weighted sum of the GLR and cross-entropy.We set this objective as the loss function of the proposed JLGCN to guide network model optimization, which employs a localized first-order approximation of spectral graph convolution as in [9].Further, the learned graph at the previous layer is added to that of the current layer for multi-level feature learning.The proposed JLGCN can be integrated into any GCNN architecture for applications such as node classification and graph classification.To validate the effectiveness of the proposed JLGCN, we apply it to semi-supervised learning for citation networks and 3D point cloud learning problems.Extensive experimental results demonstrate the superiority and robustness of JLGCN compared with state-of-the-art methods on four datasets even with a small model size.\n\nOur contributions can be summarized as follows:\n\n\u2022 We propose joint learning of underlying graphs and node features at each layer of a GCNN, which captures pairwise similarities of node features dynamically.\u2022 We cast the graph learning problem as distance metric learning with the Mahalanobis distance deployed.We further decompose the distance metric into a low-dimensional matrix for efficient implementation, which is optimized from both the GLR and cross entropy along with node features.\n\n\u2022 Extensive experiments on semi-supervised learning and point cloud classification demonstrate the superiority and robustness of our method.\n\n\nRelated Work Graph Convolution Neural Networks\n\nAs a generalization of CNNs to irregular graph domain, GCNNs can be categorized into two main classes: spectral methods and spatial methods.\n\n\nSpectral methods\n\nThis class of methods define graph convolution based on the spectral representation of graphs.[10] defines the convolution in graph Fourier transform domain by eigen-decomposition of the graph Laplacian, but requires intense computations.[11] addresses this problem by leveraging Chebyshev polynomials to approximate spectral filters and achieves localized filtering.GCN [9] further simplifies the previous work by employing only first-order approximation of the filters.Our proposed joint learning of graphs and node features is based on GCN, which has spectral interpretation and scales linearly with the number of edges.\n\n\nSpatial methods\n\nInstead of spectral representation, spatial methods define graph convolution directly on each node and its neighbors for feature propagation.Mixture model network [12] provides a unified generalization of CNN architectures on graphs.Graph attention network (GAT) in [13] employs self-attention mechanism to solve the node classification problem on citation networks.DGCNN [1] proposes edge convolution to aggregate local features, which is applied to point cloud learning problem.\n\n\nGraph Learning in GCNNs\n\nGraph Learning in GCNNs can be categorized into three classes based on the graph domain to address: 1) fixed-domain graph learning, in which the underlying graph is fixed; 2) varying-domain graph learning, where graphs vary across data instances; and 3) hybrid graph learning, which combines the former two classes.\n\n\nFixed-domain Graph Learning\n\nThis class of graph learning assumes there is a shared graph structure underlying all instances with fixed vertices.\n\nIn applications such as semi-supervised node classification of citation networks, there is only one fixed graph to be learned and shared by all the instances to classify.GLCN [3] learns a non-negative function that represents the pairwise relationship between two vertices using attention mechanism via a single-layer neural network, and optimizes it by minimizing GLR in the loss function.Instead, our method models pairwise similarity more explicitly by adopting Mahalanobis distance, and we learn different graphs at different layers.\n\n\nVarying-domain Graph Learning\n\nInstances corresponding to varying domains may require different graphs with possibly arbitrary number of vertices.\n\nGraph learning for such data applications learns adaptive graphs tailored for different individuals.Spatio-temporal graph routing (STGR) is proposed [6] to learn one spatial graph and one temporal graph for each skeleton instance.Adaptive graph convolution network (AGCN) is proposed [4] to construct a unique residual Laplacian matrix by learning Mahalanobis distance metric for each instance.Different from the task-driven AGCN where the Mahalanobis distance metric M is learned by minimizing the cross-entropy only, we optimize M via both the GLR and crossentropy.Also, we assume M is low rank and set S << K to reduce number of parameters while AGCN adopts S = K;\n\n\nHybrid Graph Learning\n\nFor special applications like skeleton-based action recognition, both fixed-domain and varying-domain graph learning can be applied according to the way of graph construction.NLGCN [5] builds three graphs for each instance: the predefined physical skeleton graph, a shared graph for all instances, and an individual graph learned from each instance's own feature in a non-local [14] manner.\n\n3 Background in Spectral Graph Theory\n\n\nGraph and Graph Laplacian\n\nAn undirected graph G = {V, E, A} is composed of a vertex set V of cardinality |V| = N , an edge set E connecting vertices, and a weighted adjacency matrix A. A \u2208 R N \u00d7N is a real and symmetric matrix, where a i,j \u2265 0 is the weight assigned to the edge (i, j) connecting vertices i and j.Edge weights often measure the similarity between connected vertices.\n\nThe graph Laplacian matrix is defined from the adjacency matrix.Among different variants of Laplacian matrices, the combinatorial graph Laplacian is defined as L := D \u2212 A, where D is the degree matrix-a diagonal matrix where\nd i,i = N j=1 a i,j .\n\nGraph Laplacian Regularizer\n\nGraph signal refers to data that resides on the nodes of a graph, such as temperatures on a sensor network.A graph signal x \u2208 R N defined on a graph G is smooth with respect to G [15] if\nx Lx = N i=1 N j=1 w i,j (x i \u2212 x j ) 2 < ,(1)\nwhere is a small positive scalar.To satisfy Eq. ( 1), connected node pair x i and x j must be similar for a large edge weight w i,j ; for a small w i,j , x i and x j can differ significantly.Hence, Eq. ( 1) forces x to adapt to the topology of G, and is commonly called the graph Laplacian Regularizer (GLR) [8,16].\n\n\nJoint Learning of Graphs and Features\n\nIn this section, we first propose the problem formulations of graph learning and node feature learning respectively assuming one of the other is known, and then combine them to the formulation of jointly learning.\n\n\nProblem Formulation of Graph Learning\n\nAs a graph essentially captures pairwise similarities within data x \u2208 R N , we cast the problem of graph learning as distance metric learning, i.e., learning a distance function for each data pair {x i , x j } for similarity calculation.Specifically, given K-dimensional feature vectors f i l \u2208 R K and f j l \u2208 R K of node i and j at the l-th layer respectively, we employ the commonly used Gaussian kernel to define an edge weight a i,j as\na i,j = exp \u2212d 2 (f i l , f j l ) ,(2)\nwhere d(f i l , f j l ) is a distance metric between f i l and f j l .The Gaussian kernel enforces edge weights to be in range [0, 1], thus ensuring the resulting combinatorial graph Laplacian to be PSD [17].\n\nWhile there exist various definitions of distance metrics, such as Euclidean distance and bilateral filtering distance [18], we deploy the Mahalanobis distance, which is defined as\nd M l (f i l , f j l ) = (f i l \u2212 f j l ) M l (f i l \u2212 f j l ),(3)\nwhere M l \u2208 R K\u00d7K is a PSD matrix, and l is the index of layers.The Mahalanobis distance captures correlations among features via M l , which is widely adopted in the machine learning literature [4,19].\n\nEach edge weight is then computed as\na i,j = exp \u2212(f i l \u2212 f j l ) M l (f i l \u2212 f j l ) ,(4)\nfrom which we can calculate the corresponding adjacency matrix and graph Laplacian.\n\nAs the graph Laplacian can be computed from edge weights by definition, we convert the graph learning problem to an optimization problem over M l .We optimize M l by minimizing Graph Laplacian Regularizer in Eq. ( 1), which enforces the graph to capture the underlying structure of features.Let s i,j = (x i \u2212x j ) 2 , we have the following problem formulation for graph learning over all the layers of the network, with the objective as the sum of GLR over all the layers:\nmin M l l {i,j} exp \u2212(f i l \u2212 f j l ) M l (f i l \u2212 f j l ) s i,j s.t. M l 0, \u2200l.(5)\nSince M l is PSD and symmetric, Eq. ( 5) is computationally expensive to solve in general.Instead, we decompose\nM l into M l = R l R l ,(6)\nwhere R l \u2208 R K\u00d7S , and S \u2264 K is a hyper-parameter to control the complexity of computing distance between highdimensional features.When K is large and S << K, the number of parameters for M l is significantly reduced while assuming M l is low rank, which is a common assumption in distance metric learning [20,19].\n\nThen the Mahalanobis distance in Eq. ( 3) becomes\nd M l (f i l , f j l ) = (f i l \u2212 f j l ) R l R l (f i l \u2212 f j l ) = (R l (f i l \u2212 f j l )) R l (f i l \u2212 f j l ) = R l (f i l \u2212 f j l ) 2 ,(7)\nwhich is essentially a linear transformation of the Euclidean distance by R l .When R l is an identity matrix, d M l (f i l , f j l ) defaults to the Euclidean distance.\n\nHence, each edge weight can be computed as\na i,j = exp \u2212 R l (f i l \u2212 f j l ) 2 2 .(8)\nAccordingly, the optimization problem in Eq. ( 5) converts to\nmin {R l } l {i,j} exp \u2212 R l (f i l \u2212 f j l ) 2 2 s i,j ,(9)\nwhich allows to remove the PSD constraint of M l and is thus more efficient to solve.The optimal adjacency matrix A * l at the l-th layer is then computed from the optimized R l via Eq.( 8).\n\n\nProblem Formulation of Node Feature Learning\n\nAssuming the availability of the learned optimal graph encoded in A * l and the optimal graph A l\u22121 in the previous layer, we learn node features based on a modified graph spectral convolution in GCN [9] for its simplicity and effectiveness.Given features of all nodes F l\u22121 from the (l \u2212 1)-th layer, the graph convolution at the l-th layer computes the output feature F l as:\nF l = A l F l\u22121 W l(10\n) where W l is a trainable parameter, and A l is the re-normalized adjacency matrix calculated as\nA l = \u039b \u2212 1 2 (A l\u22121 + A * l )\u039b \u2212 1 2\n(11) \u039b is the normalization diagonal matrix.We essentially replace the re-normalization matrix I N in the original GCN layer with our learned optimal graph A * l .When l = 1, A 0 = 0 is a zero matrix.In special cases where a ground truth graph is provided such as in semi-supervised node classification, A 0 is the ground truth graph.The ground truth graph may be inaccurate or could be improved further.For instance, in a citation network, an unweighted graph is often available to describe the citation relationship among papers.We can further learn a weighted graph to learn hidden connections and characterize how well correlated connected papers are, which enhances the model capacity.\n\nThe objective of node feature learning is to minimize the cross entropy between predicted labels and ground truth labels, which is task-driven.We predict the label of each node by applying the softmax function to the output feature at the final L-the layer:\n\u0176 = softmax(F L ).(12\n) As F L is learned from Eq. ( 10), \u0176 is a function of the set of parameters {W l , R l } at each layer.By optimizing the cross-entropy loss, we have the following problem formulation of node feature learning:\nmin {W l ,R l } \u2212 N i=1 C c=1 Y ic log( \u0176ic (W l , R l )),(13)\nwhere N is the number of input instances, C is the number of classes, and Y denotes the ground truth label matrix.\n\n\nProblem Formulation of Joint Learning\n\nIntegrating Eq. ( 9) and Eq. ( 13), we pose the joint learning of underlying graphs and node features as minimizing both GLR and the cross entropy.The final problem formulation is\nmin {W l ,R l } \u2212 N i=1 C c=1 Y ic log( \u0176ic (W l , R l )) + \u03bb l {i,j} exp \u2212 R l (f i l \u2212 f j l ) 2 2 s i,j ,(14)\nwhere \u03bb > 0 is a hyper-parameter for the trade-off between the cross entropy and the GLR term.\n\n\nProposed Network Structure\n\nHaving discussed the problem formulation of joint learning, we elaborate on the proposed network architecture to realize dynamic learning of graphs and node features, with focus on the JLGCN Layer.\n\n\nJLGCN Layer\n\nAs illustrated in Fig. 1, the JLGCN layer is composed of two modules: the graph learning module and node feature learning module.The graph learning module learns a graph encoded by an N \u00d7 N dense adjacency matrix A from input node features.A is then employed by the node feature learning module to propagate node features via our proposed graph convolution in Eq. (10).Both modules are jointly optimized according to the objective in Eq. ( 14), i.e., the loss function is defined as\nE = \u2212 N i=1 C c=1 Y ic log( \u0176ic ({W l , R l })) + \u03bb l {i,j} exp \u2212 R l (f i l \u2212 f j l ) 2 2 s i,j ,(15)\nThe algorithmic details are provided in Alg. 1.\n\n\nGraph Learning\nF \ud835\udc59\ud835\udc59\u22121 Graph Convolution F \ud835\udc59\ud835\udc59 A \ud835\udc59\ud835\udc59 A \ud835\udc59\ud835\udc59\u22121 + JLGCN layer\n\nAlgorithm 1 JLGCN layer\n\nInput: features F l\u22121 and adjacency matrix A l\u22121 from the previous layer Output: features F l and learned adjacency matrix A l Initialize: trainable parameters R l , W l 1: Forward Pass: 2: Compute distance metric d M by Eq. ( 7) 3: Update A l by Eq. ( 4) 4: Update F l by Eq. ( 10) 5: Backward Pass: 6: Update R l and W l via backward propagation with the loss function defined in Eq. (15).\n\n\nJLGCN Network\n\nIn principle, our JLGCN layer can be integrated into any GCNN architecture for both node classification and graph classification tasks.Fig. 2 demonstrates the architecture of the JLGCN network.Node features are extracted by alternatively learning an optimal graph based on features and propagating features based on the learned graph, which are then fed into the classification module for label prediction.\n\n\nNode classification configuration\n\nWe stack multiple JLGCN layers and deploy leaky ReLU activation [21].Dropout is applied to the last layer's input to reduce over-fitting.Features from the last layer is then employed for prediction of labels at each node.\n\n\nGraph classification configuration\n\nWe stack multiple JLGCN layers followed by batch normalization [22] and leaky ReLU activation.Graph max pooling is applied to features from the last layer, which are then fed into a fully connected network for the classification of the entire graph.\n\nFurther, we perform feature concatenation at each layer l with f l\u22121 from the previous layer for multi-level feature learning to enhance the model capacity.This leads to\nf l = (f l\u22121 ||A l f l\u22121 )W l , (16)\nwhere || is the concatenation operator in feature dimension.This essentially assigns more weighting to each node's own feature when no ground truth graph is available and accelerates the convergence.\n\n\nExperimental Results\n\nIn order to evaluate the performance of JLGCN, we apply it to the problems of semi-supervised node classification and point cloud classification, which are discussed in order as follows.\n\n\nSemi-supervised node classification\n\n\nDatasets\n\nWe test our method on three citation network datasets, i.e., Citeseer, Cora and Pubmed [23].The statistics of the datasets are summarized in Tab. 1.\n\n\nExperimental settings\n\nWe follow the experimental setup of previous work [24,9,13], and use the same data partition as in [24].The features are L2-normalized before fed into the network.Since our method builds a dense adjacency matrix among all nodes, we sub-sample 10,000 nodes for Pubmed dataset to evaluate our model due to memory limitation.We set the number of JLGCN layers in our network to 2, and the number of hidden units in each layer to 16.For the dimension of the Mahalanobis distance metric matrix R \u2208 R K\u00d7S , we set S to 16 for both layers and initialize it randomly if not specified.We apply dropout with p = 0.5 for both layer's input, and use leaky ReLU activation with negative slope \u03b1 = 0.2 between two layers.We train our JLGCN for a maximum of 500 epochs using ADAM algorithm [25] with a learning rate of 0.1 and weight decay of 0.0005.The learning rate is decayed by 0.5 every 100 epochs.The hyper parameter \u03bb for GLR is set to 0.0001.We report the average classification accuracy of 10 runs with different random seeds.\n\n\nBaselines\n\nWe compare our JLGCN against the baseline of GCN [9] and Planetoid [24], and also against some other graph neural network based semi-supervised learning methods, including Graph Attention Network (GAT) [13] and Graph Markov Neural Network (GMNN) [26].These methods use the same dataset partition as in our method for fair comparison with available codes.\n\n\nResults\n\nTab. 2 shows the comparison results on three citation network datasets.The best results are marked in bold.Overall, we note that 1) JLGCN outperforms the GCN baseline on all datasets significantly by only adding a graph learning module to each layer.This clearly demonstrates the effectiveness of jointly learning graphs and node features.Compared to the unweighted ground truth graph used in GCN, our learned graphs (as shown in Fig. 4) enhance the semi-supervised\n\n\nAlgorithm Citeseer Cora\n\nPubmed* Planetoid [24] 64.7 75.7\n\nGAT [13] 72.5 83.0 78.8 GMNN [26] 73.1 83.7 79.4 JLGCN 73.7 83.9 79.7\n\nTable 2: Results of semi-supervised node classification.\n\n[*] means we sub-sample 10,000 nodes to perform the experiments.\n\n\nRobustness test\n\nWe design two experiments to test the robustness of our model: 1) we lower the label ratio by employing less training data.We adopt five missing ratios {0, 0.25, 0.5, 0.75, 0.9} of training nodes to train and evaluate; 2) we test the robustness of our model to incomplete ground truth graph, i.e., we randomly drop out edges in the ground truth graph, with edge missing ratios set as {0, 0.25, 0.5, 0.75, 0.9, 1.0}.When the edge missing ratio is 1.0, an empty ground truth graph is fed into the network.Since randomly initialized graph is hard to converge, we set S = K for the transformation matrix R and initialize it as an identity matrix in this case.Fig. 3 shows our model significantly outperforms GCN and GAT, especially when the edge missing ratio is high.This validates the superiority of our learned graph structure in terms of robustness.\n\n\nAblation study\n\nWe first examine the effectiveness of the graph learning module on Cora dataset.Specifically, we compare with 1) GCN, where the ground truth graph is employed; 2) GCN + Euclidean, where we replace the proposed graph learning with manual graph construction using Euclidean distance metric to compute edge weights; 3) JLGCN without dynamic updating of graphs, i.e., we only learn one graph from the input data feature.As reported in Tab. 3, our method leads to the best result when we adopt the Mahalanobis distance metric and learn different graphs at different layers jointly with node features from data.\n\nSecondly, we evaluate the performance of JLGCN with different \u03bb-the weighting parameter of GLR in the loss function-with respect to different edge missing ratios.As listed in Tab. 3, we see that JLGCN achieves the highest accuracy when \u03bb is set to 0.0001.Also, GLR helps improve the performance especially when the ground truth graph is incomplete or even unavailable.\n\n\nVisualization of the learned graph\n\nWe also visualize the learned graph at each layer in Fig. 4, and compare them with the ground truth graph.The value of the matrix is log-transformed to make it more interpretable.Note that the learned graph is both dense and weighted, which not only adds self-connection to the unweighted ground truth graph as the original GCN does, but also extracts additional hidden connections between similar nodes.Also, the learned graph at the second layer puts higher weighting to self-connection compared to the first layer, showing that different layers correspond to varying graphs as we assumed.\n\n\nPoint cloud classification\n\nFurthermore, we test our model on point cloud classification to validate the effectiveness of our model when no ground truth graph is available.\n\n\nDatasets\n\nWe test on ModelNet40 dataset for point cloud classification.ModelNet40 includes 12,311 CAD models from 40 man-made categories, which are split into 9,843 for training and 2,468 for testing.As in previous work [27,28,1], we uniformly sample 1024 points on mesh faces according to face area and normalize them into a unit sphere, where only coordinates of points are employed as input features.\n\n\nExperimental settings\n\nWe stack 3 JLGCN layers to extract point cloud features as well as learn the underlying graph without any ground truth graph.The hidden units for each layer is {64, 128, 1024}.After extracting point cloud features, we deploys a permutation-invariant graph max-pooling to generate a global context feature for the entire point cloud, and employs a fully connected network with hidden units of {512, 256, 40} to classify the point cloud.The negative slope of leaky ReLU activation is set to 0.2 and dropout with p = 0.5 is used in the fully connected classification network.We train the network with a learning rate of 0.001 for 400 epochs and decay it by 0.5 every 40 epochs using ADAM.The batch size is set to 32, weight decay to 0.0001 and \u03bb to 0.01.\n\n\nBaselines\n\nWe also provide a GCN based model as the baseline, which empirically constructs a k-NN graph as the underlying graph.k is set to 20 in the classification task based on the density of point clouds.We mainly compare our JLGCN model against the baseline GCN, and also compare against other state-of-the-art methods, including methods tailored for point cloud learning such as PointNet [27], PointNet++ [28], PointGCN [29], RGCNN [2], and DGCNN [1].\n\n\nResults\n\nTab. 4 presents results for ModelNet40 classification.Our method improves the baseline by about 2% with only 3MB of parameters added, demonstrating the learned graph is superior to the empirical k-NN graph.Also, our method achieves comparable results compared to state-of-the-art methods for point loud learning, while our model is simpler and contains much smaller amount of parameters.Note that, our method achieves the best performance among all spectral graph convolution based methods.\n\n\nRobustness test\n\nFurther, we test the robustness of our model when the point cloud is of low density.We randomly drop out points with missing ratios {0, 0.25, 0.5, 0.75, 0.9}.As shown in Fig. 5, our model outperforms the GCN baseline significantly, and keeps high accuracy even when the point cloud density is quite low.\n\n\nConclusion\n\nWe propose joint learning of graphs and node features in Graph Convolutional Neural Networks, which dynamically learns graphs adaptive to the structure of node features in different layers.In particular, we optimize an underlying graph kernel via distance metric learning with the Mahalanobis distance employed.The metric matrix is decomposed into a low-dimensional matrix and optimized based on Graph Laplacian Regularizer.Extensive experiments demonstrate the superiority and robustness of our method in semi-supervised learning and point cloud learning problems.\n\nFigure 1 :\n1\nFigure 1: Demonstration of a JLGCN layer.A graph is first learned from node features, and then employed to propagate node features via graph convolution.\n\n\nFigure 2 :\n2\nFigure 2: The proposed JLGCN network architecture for node classification and graph classification tasks.The graph and node features at each layer are jointly updated.\n\n\nFigure 3 :\n3\nFigure 3: Robustness test with different missing ratios of labeled nodes and edges on Cora dataset.(a) Accuracy with different edge missing ratios.(b) Accuracy with different node missing ratios.\n\n\nFigure 4 :\n4\nFigure 4: Visualization of the learned graphs at different layers for the first 50 nodes on Cora dataset.(a) shows the ground truth graph, (b) is the learned graph for the first layer, and (c) for the second layer.\n\n\nFigure 5 :\n5\nFigure 5: Robustness test with different missing ratios of point clouds.\n\n\nTable 1 :\n1\nSummary of citation network datasets.\nDataset NodesEdges Classes Features LabelRatioCiteseer 3,3274,732 63,7030.036Cora2,7085,429 71,4330.052Pubmed 19,71744,338 35000.003\n\nTable 3 :\n3\nAblation study.\n\n\nTable 4 :\n4\nResults of point cloud classification on ModelNet40.\nCategory AlgorithmModel Size (MB)Mean Class AccuracyOverall AccuracyPointwise MLPPointNet [27]4086.089.2Pointwise MLPPointNet++ [28]12-90.7Spectral GCNNPointGCN [29]4186.189.5Spectral GCNNRGCNN [2]2287.390.5Spatial GCNNDGCNN [1]2190.292.9Spectral GCNNGCN (baseline)1084.288.7Spectral GCNNJLGCN1387.290.8\nIn spectral graph theory, a graph Laplacian matrix is an algebraic representation of connectivities and node degrees of the corresponding graph, which will be defined formally later.\n\nDynamic graph cnn for learning on point clouds. Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, Justin M Solomon, ACM Transactions on Graphics. 2019\n\nRgcnn: Regularized graph cnn for point cloud segmentation. Gusi Te, Wei Hu, Amin Zheng, Zongming Guo, 2018 ACM Multimedia Conference on Multimedia Conference. ACM2018\n\nSemi-supervised learning with graph learningconvolutional networks. Bo Jiang, Ziyan Zhang, Doudou Lin, Jin Tang, Bin Luo, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2019\n\nAdaptive graph convolutional neural networks. Ruoyu Li, Sheng Wang, Feiyun Zhu, Junzhou Huang, Thirty-Second AAAI Conference on Artificial Intelligence. 2018\n\nNon-local graph convolutional networks for skeleton-based action recognition. Lei Shi, Yifan Zhang, Jian Cheng, Hanqing Lu, arXiv:1805.076942018arXiv preprint\n\nSpatio-temporal graph routing for skeleton-based action recognition. Bin Li, Xi Li, Zhongfei Zhang, Fei Wu, Thirty-Third AAAI Conference on Artificial Intelligence (AAAI). 2019\n\nOn the generalized distance in statistics. P C Mahalanobis, Proceedings of the National Institute of Sciences of India. the National Institute of Sciences of India19362\n\nThe emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. David I Shuman, K Sunil, Pascal Narang, Antonio Frossard, Pierre Ortega, Vandergheynst, IEEE signal processing magazine. 3032013\n\nSemi-supervised classification with graph convolutional networks. Thomas N Kipf, Max Welling, ArXiv, abs/1609.029072016\n\nJoan Bruna, Wojciech Zaremba, Arthur Szlam, Yann Lecun, arXiv:1312.6203Spectral networks and locally connected networks on graphs. 2013arXiv preprint\n\nConvolutional neural networks on graphs with fast localized spectral filtering. Micha\u00ebl Defferrard, Xavier Bresson, Pierre Vandergheynst, Advances in neural information processing systems. 2016\n\nGeometric deep learning on graphs and manifolds using mixture model cnns. Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, Michael M Bronstein, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2017\n\nGraph attention networks. Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, arXiv:1710.109032017arXiv preprint\n\nNon-local neural networks. Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018\n\nSpectral partitioning works: Planar graphs and finite element meshes. Linear Algebra and its. A Daniel, Shang-Hua Spielman, Teng, Applications. 4212-32007\n\nGraph laplacian regularization for image denoising: Analysis in the continuous domain. Jiahao Pang, Gene Cheung, IEEE Transactions on Image Processing. 2642017\n\nGraph spectral image processing. Gene Cheung, Enrico Magli, Yuichi Tanaka, Michael K Ng, Proceedings of the IEEE. 10652018\n\nBilateral filtering for gray and color images. Carlo Tomasi, Roberto Manduchi, Iccv. 199898\n\nRobust metric learning on grassmann manifolds with generalization guarantees. Lei Luo, Jie Xu, Cheng Deng, Heng Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933\n\nLow-rank similarity metric learning in high dimensions. Wei Liu, Cun Mu, Rongrong Ji, Shiqian Ma, John R Smith, Shih-Fu Chang, Twenty-ninth AAAI conference on artificial intelligence. 2015\n\nEmpirical evaluation of rectified activations in convolutional network. Bing Xu, Naiyan Wang, Tianqi Chen, Mu Li, arXiv:1505.008532015arXiv preprint\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, arXiv:1502.031672015arXiv preprint\n\nCollective classification in network data. Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, Tina Eliassi-Rad, AI magazine. 2932008\n\nRevisiting semi-supervised learning with graph embeddings. Zhilin Yang, William W Cohen, Ruslan Salakhutdinov, arXiv:1603.088612016arXiv preprint\n\nP Diederik, Jimmy Kingma, Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint\n\nMeng Qu, Yoshua Bengio, Jian Tang, Gmnn, arXiv:1905.06214Graph markov neural networks. 2019arXiv preprint\n\nPointnet: Deep learning on point sets for 3d classification and segmentation. Hao Charles R Qi, Kaichun Su, Leonidas J Mo, Guibas, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2017\n\nPointnet++: Deep hierarchical feature learning on point sets in a metric space. Charles Ruizhongtai, Qi , Li Yi, Hao Su, Leonidas J Guibas, Advances in neural information processing systems. 2017\n\nA graph-cnn for 3d point cloud classification. Yingxue Zhang, Michael Rabbat, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2018\n", "annotations": {"author": "[{\"end\":188,\"start\":104},{\"end\":266,\"start\":189},{\"end\":347,\"start\":267},{\"end\":454,\"start\":348}]", "publisher": null, "author_last_name": "[{\"end\":117,\"start\":113},{\"end\":195,\"start\":193},{\"end\":276,\"start\":273},{\"end\":360,\"start\":357}]", "author_first_name": "[{\"end\":112,\"start\":104},{\"end\":192,\"start\":189},{\"end\":272,\"start\":267},{\"end\":356,\"start\":348}]", "author_affiliation": "[{\"end\":187,\"start\":119},{\"end\":265,\"start\":197},{\"end\":346,\"start\":278},{\"end\":453,\"start\":385}]", "title": "[{\"end\":83,\"start\":1},{\"end\":537,\"start\":455}]", "venue": null, "abstract": "[{\"end\":1591,\"start\":589}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2246,\"start\":2243},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2248,\"start\":2246},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2357,\"start\":2354},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2359,\"start\":2357},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2361,\"start\":2359},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2363,\"start\":2361},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3074,\"start\":3071},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3857,\"start\":3854},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4413,\"start\":4410},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5934,\"start\":5930},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6078,\"start\":6074},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6210,\"start\":6207},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6646,\"start\":6642},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6749,\"start\":6745},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6854,\"start\":6851},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7630,\"start\":7627},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8292,\"start\":8289},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8427,\"start\":8424},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9017,\"start\":9014},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9215,\"start\":9211},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10111,\"start\":10107},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10473,\"start\":10470},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10476,\"start\":10473},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11461,\"start\":11457},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11587,\"start\":11583},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11910,\"start\":11907},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11913,\"start\":11910},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13103,\"start\":13099},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13106,\"start\":13103},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14125,\"start\":14122},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16859,\"start\":16855},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17614,\"start\":17610},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18145,\"start\":18141},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18404,\"start\":18400},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19347,\"start\":19343},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19484,\"start\":19480},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19486,\"start\":19484},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19489,\"start\":19486},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19533,\"start\":19529},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20208,\"start\":20204},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20515,\"start\":20512},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20534,\"start\":20530},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20669,\"start\":20665},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20713,\"start\":20709},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21344,\"start\":21340},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21364,\"start\":21360},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21389,\"start\":21385},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24444,\"start\":24440},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":24447,\"start\":24444},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24449,\"start\":24447},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25800,\"start\":25796},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25817,\"start\":25813},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25832,\"start\":25828},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25843,\"start\":25840},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25858,\"start\":25855}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27433,\"start\":27265},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27616,\"start\":27434},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27827,\"start\":27617},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28057,\"start\":27828},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28145,\"start\":28058},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":28329,\"start\":28146},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":28359,\"start\":28330},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":28729,\"start\":28360}]", "paragraph": "[{\"end\":2120,\"start\":1607},{\"end\":2704,\"start\":2122},{\"end\":4988,\"start\":2706},{\"end\":5037,\"start\":4990},{\"end\":5482,\"start\":5039},{\"end\":5624,\"start\":5484},{\"end\":5815,\"start\":5675},{\"end\":6459,\"start\":5836},{\"end\":6959,\"start\":6479},{\"end\":7302,\"start\":6987},{\"end\":7450,\"start\":7334},{\"end\":7989,\"start\":7452},{\"end\":8138,\"start\":8023},{\"end\":8807,\"start\":8140},{\"end\":9223,\"start\":8833},{\"end\":9262,\"start\":9225},{\"end\":9649,\"start\":9292},{\"end\":9875,\"start\":9651},{\"end\":10114,\"start\":9928},{\"end\":10477,\"start\":10162},{\"end\":10732,\"start\":10519},{\"end\":11214,\"start\":10774},{\"end\":11462,\"start\":11254},{\"end\":11644,\"start\":11464},{\"end\":11914,\"start\":11712},{\"end\":11952,\"start\":11916},{\"end\":12092,\"start\":12009},{\"end\":12567,\"start\":12094},{\"end\":12763,\"start\":12652},{\"end\":13107,\"start\":12792},{\"end\":13158,\"start\":13109},{\"end\":13471,\"start\":13302},{\"end\":13515,\"start\":13473},{\"end\":13621,\"start\":13560},{\"end\":13873,\"start\":13683},{\"end\":14299,\"start\":13922},{\"end\":14420,\"start\":14323},{\"end\":15149,\"start\":14459},{\"end\":15408,\"start\":15151},{\"end\":15640,\"start\":15431},{\"end\":15818,\"start\":15704},{\"end\":16039,\"start\":15860},{\"end\":16247,\"start\":16153},{\"end\":16475,\"start\":16278},{\"end\":16973,\"start\":16491},{\"end\":17124,\"start\":17077},{\"end\":17615,\"start\":17224},{\"end\":18039,\"start\":17633},{\"end\":18298,\"start\":18077},{\"end\":18586,\"start\":18337},{\"end\":18757,\"start\":18588},{\"end\":18994,\"start\":18795},{\"end\":19205,\"start\":19019},{\"end\":19404,\"start\":19256},{\"end\":20449,\"start\":19430},{\"end\":20817,\"start\":20463},{\"end\":21294,\"start\":20829},{\"end\":21354,\"start\":21322},{\"end\":21425,\"start\":21356},{\"end\":21483,\"start\":21427},{\"end\":21549,\"start\":21485},{\"end\":22418,\"start\":21569},{\"end\":23042,\"start\":22437},{\"end\":23412,\"start\":23044},{\"end\":24042,\"start\":23451},{\"end\":24217,\"start\":24073},{\"end\":24623,\"start\":24230},{\"end\":25400,\"start\":24649},{\"end\":25859,\"start\":25414},{\"end\":26361,\"start\":25871},{\"end\":26684,\"start\":26381},{\"end\":27264,\"start\":26699},{\"end\":27432,\"start\":27279},{\"end\":27615,\"start\":27448},{\"end\":27826,\"start\":27631},{\"end\":28056,\"start\":27842},{\"end\":28144,\"start\":28072},{\"end\":28196,\"start\":28159},{\"end\":28358,\"start\":28343},{\"end\":28425,\"start\":28373}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9897,\"start\":9876},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10161,\"start\":10115},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11253,\"start\":11215},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11711,\"start\":11645},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12008,\"start\":11953},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12651,\"start\":12568},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12791,\"start\":12764},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13301,\"start\":13159},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13559,\"start\":13516},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13682,\"start\":13622},{\"attributes\":{\"id\":\"formula_10\"},\"end\":14322,\"start\":14300},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14458,\"start\":14421},{\"attributes\":{\"id\":\"formula_12\"},\"end\":15430,\"start\":15409},{\"attributes\":{\"id\":\"formula_13\"},\"end\":15703,\"start\":15641},{\"attributes\":{\"id\":\"formula_14\"},\"end\":16152,\"start\":16040},{\"attributes\":{\"id\":\"formula_15\"},\"end\":17076,\"start\":16974},{\"attributes\":{\"id\":\"formula_16\"},\"end\":17197,\"start\":17142},{\"attributes\":{\"id\":\"formula_17\"},\"end\":18793,\"start\":18758},{\"attributes\":{\"id\":\"formula_18\"},\"end\":18794,\"start\":18793}]", "table_ref": "[{\"end\":21434,\"start\":21433}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1605,\"start\":1593},{\"attributes\":{\"n\":\"2\"},\"end\":5673,\"start\":5627},{\"end\":5834,\"start\":5818},{\"end\":6477,\"start\":6462},{\"end\":6985,\"start\":6962},{\"end\":7332,\"start\":7305},{\"end\":8021,\"start\":7992},{\"end\":8831,\"start\":8810},{\"end\":9290,\"start\":9265},{\"end\":9926,\"start\":9899},{\"attributes\":{\"n\":\"4\"},\"end\":10517,\"start\":10480},{\"end\":10772,\"start\":10735},{\"end\":13920,\"start\":13876},{\"end\":15858,\"start\":15821},{\"attributes\":{\"n\":\"5\"},\"end\":16276,\"start\":16250},{\"end\":16489,\"start\":16478},{\"end\":17141,\"start\":17127},{\"end\":17222,\"start\":17199},{\"end\":17631,\"start\":17618},{\"end\":18075,\"start\":18042},{\"end\":18335,\"start\":18301},{\"attributes\":{\"n\":\"6\"},\"end\":19017,\"start\":18997},{\"end\":19243,\"start\":19208},{\"end\":19254,\"start\":19246},{\"end\":19428,\"start\":19407},{\"end\":20461,\"start\":20452},{\"end\":20827,\"start\":20820},{\"end\":21320,\"start\":21297},{\"end\":21567,\"start\":21552},{\"end\":22435,\"start\":22421},{\"end\":23449,\"start\":23415},{\"end\":24071,\"start\":24045},{\"end\":24228,\"start\":24220},{\"end\":24647,\"start\":24626},{\"end\":25412,\"start\":25403},{\"end\":25869,\"start\":25862},{\"end\":26379,\"start\":26364},{\"attributes\":{\"n\":\"7\"},\"end\":26697,\"start\":26687},{\"end\":27276,\"start\":27266},{\"end\":27445,\"start\":27435},{\"end\":27628,\"start\":27618},{\"end\":27839,\"start\":27829},{\"end\":28069,\"start\":28059},{\"end\":28156,\"start\":28147},{\"end\":28340,\"start\":28331},{\"end\":28370,\"start\":28361}]", "table": "[{\"end\":28329,\"start\":28197},{\"end\":28729,\"start\":28426}]", "figure_caption": "[{\"end\":27433,\"start\":27278},{\"end\":27616,\"start\":27447},{\"end\":27827,\"start\":27630},{\"end\":28057,\"start\":27841},{\"end\":28145,\"start\":28071},{\"end\":28197,\"start\":28158},{\"end\":28359,\"start\":28342},{\"end\":28426,\"start\":28372}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16515,\"start\":16514},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17774,\"start\":17773},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21265,\"start\":21264},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22230,\"start\":22229},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23510,\"start\":23509},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26557,\"start\":26556}]", "bib_author_first_name": "[{\"end\":28965,\"start\":28962},{\"end\":28979,\"start\":28972},{\"end\":28990,\"start\":28985},{\"end\":29002,\"start\":28996},{\"end\":29004,\"start\":29003},{\"end\":29019,\"start\":29012},{\"end\":29021,\"start\":29020},{\"end\":29039,\"start\":29033},{\"end\":29041,\"start\":29040},{\"end\":29150,\"start\":29146},{\"end\":29158,\"start\":29155},{\"end\":29167,\"start\":29163},{\"end\":29183,\"start\":29175},{\"end\":29325,\"start\":29323},{\"end\":29338,\"start\":29333},{\"end\":29352,\"start\":29346},{\"end\":29361,\"start\":29358},{\"end\":29371,\"start\":29368},{\"end\":29501,\"start\":29496},{\"end\":29511,\"start\":29506},{\"end\":29524,\"start\":29518},{\"end\":29537,\"start\":29530},{\"end\":29690,\"start\":29687},{\"end\":29701,\"start\":29696},{\"end\":29713,\"start\":29709},{\"end\":29728,\"start\":29721},{\"end\":29841,\"start\":29838},{\"end\":29848,\"start\":29846},{\"end\":29861,\"start\":29853},{\"end\":29872,\"start\":29869},{\"end\":29991,\"start\":29990},{\"end\":29993,\"start\":29992},{\"end\":30267,\"start\":30266},{\"end\":30281,\"start\":30275},{\"end\":30297,\"start\":30290},{\"end\":30314,\"start\":30308},{\"end\":30452,\"start\":30446},{\"end\":30454,\"start\":30453},{\"end\":30464,\"start\":30461},{\"end\":30505,\"start\":30501},{\"end\":30521,\"start\":30513},{\"end\":30537,\"start\":30531},{\"end\":30549,\"start\":30545},{\"end\":30739,\"start\":30732},{\"end\":30758,\"start\":30752},{\"end\":30774,\"start\":30768},{\"end\":30929,\"start\":30921},{\"end\":30943,\"start\":30937},{\"end\":30962,\"start\":30954},{\"end\":30978,\"start\":30970},{\"end\":30990,\"start\":30987},{\"end\":31199,\"start\":31194},{\"end\":31219,\"start\":31212},{\"end\":31237,\"start\":31230},{\"end\":31255,\"start\":31248},{\"end\":31270,\"start\":31264},{\"end\":31282,\"start\":31276},{\"end\":31362,\"start\":31354},{\"end\":31373,\"start\":31369},{\"end\":31391,\"start\":31384},{\"end\":31406,\"start\":31399},{\"end\":31653,\"start\":31652},{\"end\":31671,\"start\":31662},{\"end\":31807,\"start\":31801},{\"end\":31818,\"start\":31814},{\"end\":31912,\"start\":31908},{\"end\":31927,\"start\":31921},{\"end\":31941,\"start\":31935},{\"end\":31957,\"start\":31950},{\"end\":31959,\"start\":31958},{\"end\":32051,\"start\":32046},{\"end\":32067,\"start\":32060},{\"end\":32173,\"start\":32170},{\"end\":32182,\"start\":32179},{\"end\":32192,\"start\":32187},{\"end\":32203,\"start\":32199},{\"end\":32387,\"start\":32384},{\"end\":32396,\"start\":32393},{\"end\":32409,\"start\":32401},{\"end\":32421,\"start\":32414},{\"end\":32430,\"start\":32426},{\"end\":32432,\"start\":32431},{\"end\":32447,\"start\":32440},{\"end\":32594,\"start\":32590},{\"end\":32605,\"start\":32599},{\"end\":32618,\"start\":32612},{\"end\":32627,\"start\":32625},{\"end\":32768,\"start\":32762},{\"end\":32785,\"start\":32776},{\"end\":32884,\"start\":32874},{\"end\":32897,\"start\":32890},{\"end\":32913,\"start\":32906},{\"end\":32926,\"start\":32922},{\"end\":32940,\"start\":32935},{\"end\":32956,\"start\":32952},{\"end\":33057,\"start\":33051},{\"end\":33071,\"start\":33064},{\"end\":33073,\"start\":33072},{\"end\":33087,\"start\":33081},{\"end\":33140,\"start\":33139},{\"end\":33156,\"start\":33151},{\"end\":33252,\"start\":33248},{\"end\":33263,\"start\":33257},{\"end\":33276,\"start\":33272},{\"end\":33436,\"start\":33433},{\"end\":33458,\"start\":33451},{\"end\":33471,\"start\":33463},{\"end\":33473,\"start\":33472},{\"end\":33720,\"start\":33713},{\"end\":33736,\"start\":33734},{\"end\":33741,\"start\":33739},{\"end\":33749,\"start\":33746},{\"end\":33762,\"start\":33754},{\"end\":33764,\"start\":33763},{\"end\":33884,\"start\":33877},{\"end\":33899,\"start\":33892}]", "bib_author_last_name": "[{\"end\":28970,\"start\":28966},{\"end\":28983,\"start\":28980},{\"end\":28994,\"start\":28991},{\"end\":29010,\"start\":29005},{\"end\":29031,\"start\":29022},{\"end\":29049,\"start\":29042},{\"end\":29153,\"start\":29151},{\"end\":29161,\"start\":29159},{\"end\":29173,\"start\":29168},{\"end\":29187,\"start\":29184},{\"end\":29331,\"start\":29326},{\"end\":29344,\"start\":29339},{\"end\":29356,\"start\":29353},{\"end\":29366,\"start\":29362},{\"end\":29375,\"start\":29372},{\"end\":29504,\"start\":29502},{\"end\":29516,\"start\":29512},{\"end\":29528,\"start\":29525},{\"end\":29543,\"start\":29538},{\"end\":29694,\"start\":29691},{\"end\":29707,\"start\":29702},{\"end\":29719,\"start\":29714},{\"end\":29731,\"start\":29729},{\"end\":29844,\"start\":29842},{\"end\":29851,\"start\":29849},{\"end\":29867,\"start\":29862},{\"end\":29875,\"start\":29873},{\"end\":30005,\"start\":29994},{\"end\":30264,\"start\":30250},{\"end\":30273,\"start\":30268},{\"end\":30288,\"start\":30282},{\"end\":30306,\"start\":30298},{\"end\":30321,\"start\":30315},{\"end\":30336,\"start\":30323},{\"end\":30459,\"start\":30455},{\"end\":30472,\"start\":30465},{\"end\":30511,\"start\":30506},{\"end\":30529,\"start\":30522},{\"end\":30543,\"start\":30538},{\"end\":30555,\"start\":30550},{\"end\":30750,\"start\":30740},{\"end\":30766,\"start\":30759},{\"end\":30788,\"start\":30775},{\"end\":30935,\"start\":30930},{\"end\":30952,\"start\":30944},{\"end\":30968,\"start\":30963},{\"end\":30985,\"start\":30979},{\"end\":30998,\"start\":30991},{\"end\":31019,\"start\":31000},{\"end\":31210,\"start\":31200},{\"end\":31228,\"start\":31220},{\"end\":31246,\"start\":31238},{\"end\":31262,\"start\":31256},{\"end\":31274,\"start\":31271},{\"end\":31289,\"start\":31283},{\"end\":31367,\"start\":31363},{\"end\":31382,\"start\":31374},{\"end\":31397,\"start\":31392},{\"end\":31409,\"start\":31407},{\"end\":31660,\"start\":31654},{\"end\":31680,\"start\":31672},{\"end\":31686,\"start\":31682},{\"end\":31812,\"start\":31808},{\"end\":31825,\"start\":31819},{\"end\":31919,\"start\":31913},{\"end\":31933,\"start\":31928},{\"end\":31948,\"start\":31942},{\"end\":31962,\"start\":31960},{\"end\":32058,\"start\":32052},{\"end\":32076,\"start\":32068},{\"end\":32177,\"start\":32174},{\"end\":32185,\"start\":32183},{\"end\":32197,\"start\":32193},{\"end\":32209,\"start\":32204},{\"end\":32391,\"start\":32388},{\"end\":32399,\"start\":32397},{\"end\":32412,\"start\":32410},{\"end\":32424,\"start\":32422},{\"end\":32438,\"start\":32433},{\"end\":32453,\"start\":32448},{\"end\":32597,\"start\":32595},{\"end\":32610,\"start\":32606},{\"end\":32623,\"start\":32619},{\"end\":32630,\"start\":32628},{\"end\":32774,\"start\":32769},{\"end\":32793,\"start\":32786},{\"end\":32888,\"start\":32885},{\"end\":32904,\"start\":32898},{\"end\":32920,\"start\":32914},{\"end\":32933,\"start\":32927},{\"end\":32950,\"start\":32941},{\"end\":32968,\"start\":32957},{\"end\":33062,\"start\":33058},{\"end\":33079,\"start\":33074},{\"end\":33101,\"start\":33088},{\"end\":33149,\"start\":33141},{\"end\":33163,\"start\":33157},{\"end\":33167,\"start\":33165},{\"end\":33255,\"start\":33253},{\"end\":33270,\"start\":33264},{\"end\":33281,\"start\":33277},{\"end\":33287,\"start\":33283},{\"end\":33449,\"start\":33437},{\"end\":33461,\"start\":33459},{\"end\":33476,\"start\":33474},{\"end\":33484,\"start\":33478},{\"end\":33732,\"start\":33721},{\"end\":33744,\"start\":33742},{\"end\":33752,\"start\":33750},{\"end\":33771,\"start\":33765},{\"end\":33890,\"start\":33885},{\"end\":33906,\"start\":33900}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":94822},\"end\":29085,\"start\":28914},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":47015108},\"end\":29253,\"start\":29087},{\"attributes\":{\"id\":\"b2\"},\"end\":29448,\"start\":29255},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1415308},\"end\":29607,\"start\":29450},{\"attributes\":{\"doi\":\"arXiv:1805.07694\",\"id\":\"b4\"},\"end\":29767,\"start\":29609},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":198190062},\"end\":29945,\"start\":29769},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":117765088},\"end\":30115,\"start\":29947},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1594725},\"end\":30378,\"start\":30117},{\"attributes\":{\"doi\":\"ArXiv, abs/1609.02907\",\"id\":\"b8\"},\"end\":30499,\"start\":30380},{\"attributes\":{\"doi\":\"arXiv:1312.6203\",\"id\":\"b9\"},\"end\":30650,\"start\":30501},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3016223},\"end\":30845,\"start\":30652},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":301319},\"end\":31166,\"start\":30847},{\"attributes\":{\"doi\":\"arXiv:1710.10903\",\"id\":\"b12\"},\"end\":31325,\"start\":31168},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":4852647},\"end\":31556,\"start\":31327},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":14861472},\"end\":31712,\"start\":31558},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":17879082},\"end\":31873,\"start\":31714},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":13847951},\"end\":31997,\"start\":31875},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":14308539},\"end\":32090,\"start\":31999},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":70102084},\"end\":32326,\"start\":32092},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":16155942},\"end\":32516,\"start\":32328},{\"attributes\":{\"doi\":\"arXiv:1505.00853\",\"id\":\"b20\"},\"end\":32666,\"start\":32518},{\"attributes\":{\"doi\":\"arXiv:1502.03167\",\"id\":\"b21\"},\"end\":32829,\"start\":32668},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":62016134},\"end\":32990,\"start\":32831},{\"attributes\":{\"doi\":\"arXiv:1603.08861\",\"id\":\"b23\"},\"end\":33137,\"start\":32992},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b24\"},\"end\":33246,\"start\":33139},{\"attributes\":{\"doi\":\"arXiv:1905.06214\",\"id\":\"b25\"},\"end\":33353,\"start\":33248},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":5115938},\"end\":33631,\"start\":33355},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1745976},\"end\":33828,\"start\":33633},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":52287238},\"end\":34004,\"start\":33830}]", "bib_title": "[{\"end\":28960,\"start\":28914},{\"end\":29144,\"start\":29087},{\"end\":29321,\"start\":29255},{\"end\":29494,\"start\":29450},{\"end\":29836,\"start\":29769},{\"end\":29988,\"start\":29947},{\"end\":30248,\"start\":30117},{\"end\":30730,\"start\":30652},{\"end\":30919,\"start\":30847},{\"end\":31352,\"start\":31327},{\"end\":31650,\"start\":31558},{\"end\":31799,\"start\":31714},{\"end\":31906,\"start\":31875},{\"end\":32044,\"start\":31999},{\"end\":32168,\"start\":32092},{\"end\":32382,\"start\":32328},{\"end\":32872,\"start\":32831},{\"end\":33431,\"start\":33355},{\"end\":33711,\"start\":33633},{\"end\":33875,\"start\":33830}]", "bib_author": "[{\"end\":28972,\"start\":28962},{\"end\":28985,\"start\":28972},{\"end\":28996,\"start\":28985},{\"end\":29012,\"start\":28996},{\"end\":29033,\"start\":29012},{\"end\":29051,\"start\":29033},{\"end\":29155,\"start\":29146},{\"end\":29163,\"start\":29155},{\"end\":29175,\"start\":29163},{\"end\":29189,\"start\":29175},{\"end\":29333,\"start\":29323},{\"end\":29346,\"start\":29333},{\"end\":29358,\"start\":29346},{\"end\":29368,\"start\":29358},{\"end\":29377,\"start\":29368},{\"end\":29506,\"start\":29496},{\"end\":29518,\"start\":29506},{\"end\":29530,\"start\":29518},{\"end\":29545,\"start\":29530},{\"end\":29696,\"start\":29687},{\"end\":29709,\"start\":29696},{\"end\":29721,\"start\":29709},{\"end\":29733,\"start\":29721},{\"end\":29846,\"start\":29838},{\"end\":29853,\"start\":29846},{\"end\":29869,\"start\":29853},{\"end\":29877,\"start\":29869},{\"end\":30007,\"start\":29990},{\"end\":30266,\"start\":30250},{\"end\":30275,\"start\":30266},{\"end\":30290,\"start\":30275},{\"end\":30308,\"start\":30290},{\"end\":30323,\"start\":30308},{\"end\":30338,\"start\":30323},{\"end\":30461,\"start\":30446},{\"end\":30474,\"start\":30461},{\"end\":30513,\"start\":30501},{\"end\":30531,\"start\":30513},{\"end\":30545,\"start\":30531},{\"end\":30557,\"start\":30545},{\"end\":30752,\"start\":30732},{\"end\":30768,\"start\":30752},{\"end\":30790,\"start\":30768},{\"end\":30937,\"start\":30921},{\"end\":30954,\"start\":30937},{\"end\":30970,\"start\":30954},{\"end\":30987,\"start\":30970},{\"end\":31000,\"start\":30987},{\"end\":31021,\"start\":31000},{\"end\":31212,\"start\":31194},{\"end\":31230,\"start\":31212},{\"end\":31248,\"start\":31230},{\"end\":31264,\"start\":31248},{\"end\":31276,\"start\":31264},{\"end\":31291,\"start\":31276},{\"end\":31369,\"start\":31354},{\"end\":31384,\"start\":31369},{\"end\":31399,\"start\":31384},{\"end\":31411,\"start\":31399},{\"end\":31662,\"start\":31652},{\"end\":31682,\"start\":31662},{\"end\":31688,\"start\":31682},{\"end\":31814,\"start\":31801},{\"end\":31827,\"start\":31814},{\"end\":31921,\"start\":31908},{\"end\":31935,\"start\":31921},{\"end\":31950,\"start\":31935},{\"end\":31964,\"start\":31950},{\"end\":32060,\"start\":32046},{\"end\":32078,\"start\":32060},{\"end\":32179,\"start\":32170},{\"end\":32187,\"start\":32179},{\"end\":32199,\"start\":32187},{\"end\":32211,\"start\":32199},{\"end\":32393,\"start\":32384},{\"end\":32401,\"start\":32393},{\"end\":32414,\"start\":32401},{\"end\":32426,\"start\":32414},{\"end\":32440,\"start\":32426},{\"end\":32455,\"start\":32440},{\"end\":32599,\"start\":32590},{\"end\":32612,\"start\":32599},{\"end\":32625,\"start\":32612},{\"end\":32632,\"start\":32625},{\"end\":32776,\"start\":32762},{\"end\":32795,\"start\":32776},{\"end\":32890,\"start\":32874},{\"end\":32906,\"start\":32890},{\"end\":32922,\"start\":32906},{\"end\":32935,\"start\":32922},{\"end\":32952,\"start\":32935},{\"end\":32970,\"start\":32952},{\"end\":33064,\"start\":33051},{\"end\":33081,\"start\":33064},{\"end\":33103,\"start\":33081},{\"end\":33151,\"start\":33139},{\"end\":33165,\"start\":33151},{\"end\":33169,\"start\":33165},{\"end\":33257,\"start\":33248},{\"end\":33272,\"start\":33257},{\"end\":33283,\"start\":33272},{\"end\":33289,\"start\":33283},{\"end\":33451,\"start\":33433},{\"end\":33463,\"start\":33451},{\"end\":33478,\"start\":33463},{\"end\":33486,\"start\":33478},{\"end\":33734,\"start\":33713},{\"end\":33739,\"start\":33734},{\"end\":33746,\"start\":33739},{\"end\":33754,\"start\":33746},{\"end\":33773,\"start\":33754},{\"end\":33892,\"start\":33877},{\"end\":33908,\"start\":33892}]", "bib_venue": "[{\"end\":29079,\"start\":29051},{\"end\":29244,\"start\":29189},{\"end\":29442,\"start\":29377},{\"end\":29601,\"start\":29545},{\"end\":29685,\"start\":29609},{\"end\":29939,\"start\":29877},{\"end\":30065,\"start\":30007},{\"end\":30369,\"start\":30338},{\"end\":30444,\"start\":30380},{\"end\":30630,\"start\":30572},{\"end\":30839,\"start\":30790},{\"end\":31098,\"start\":31021},{\"end\":31192,\"start\":31168},{\"end\":31488,\"start\":31411},{\"end\":31700,\"start\":31688},{\"end\":31864,\"start\":31827},{\"end\":31987,\"start\":31964},{\"end\":32082,\"start\":32078},{\"end\":32272,\"start\":32211},{\"end\":32510,\"start\":32455},{\"end\":32588,\"start\":32518},{\"end\":32760,\"start\":32668},{\"end\":32981,\"start\":32970},{\"end\":33049,\"start\":32992},{\"end\":33226,\"start\":33184},{\"end\":33333,\"start\":33305},{\"end\":33563,\"start\":33486},{\"end\":33822,\"start\":33773},{\"end\":33994,\"start\":33908},{\"end\":30110,\"start\":30067},{\"end\":31162,\"start\":31100},{\"end\":31552,\"start\":31490},{\"end\":32320,\"start\":32274},{\"end\":33627,\"start\":33565}]"}}}, "year": 2023, "month": 12, "day": 17}