{"id": 218934679, "updated": "2022-01-25 05:04:53.502", "metadata": {"title": "Accelerating Hyperdimensional Computing on FPGAs by Exploiting Computational Reuse", "authors": "[{\"middle\":[],\"last\":\"Salamat\",\"first\":\"Sahand\"},{\"middle\":[],\"last\":\"Imani\",\"first\":\"Mohsen\"},{\"middle\":[],\"last\":\"Rosing\",\"first\":\"Tajana\"}]", "venue": "IEEE Transactions on Computers", "journal": "IEEE Transactions on Computers", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Brain-inspired hyperdimensional (HD) computing emulates cognition by computing with long-size vectors. HD computing consists of two main modules: encoder and associative search. The encoder module maps inputs into high dimensional vectors, called hypervectors. The associative search finds the closest match between the <italic>trained model</italic> (set of hypervectors) and a <italic>query</italic> hypervector by calculating a similarity metric. To perform the reasoning task for practical classification problems, HD needs to store a non-binary model and uses costly similarity metrics as <italic>cosine</italic>. In this article we propose an FPGA-based acceleration of <underline>HD</underline> exploiting <underline>Co</underline>mputational <underline>Re</underline>use (<inline-formula><tex-math notation=\"LaTeX\">$\\mathtt {HD}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"monospace\">HD</mml:mi></mml:math><inline-graphic xlink:href=\"salamat-ieq1-2992662.gif\"/></alternatives></inline-formula>-<inline-formula><tex-math notation=\"LaTeX\">$\\mathtt {Core}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"monospace\">Core</mml:mi></mml:math><inline-graphic xlink:href=\"salamat-ieq2-2992662.gif\"/></alternatives></inline-formula>) which significantly improves the computation efficiency of both encoding and associative search modules. <inline-formula><tex-math notation=\"LaTeX\">$\\mathtt {HD}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"monospace\">HD</mml:mi></mml:math><inline-graphic xlink:href=\"salamat-ieq3-2992662.gif\"/></alternatives></inline-formula>-<inline-formula><tex-math notation=\"LaTeX\">$\\mathtt {Core}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"monospace\">Core</mml:mi></mml:math><inline-graphic xlink:href=\"salamat-ieq4-2992662.gif\"/></alternatives></inline-formula> enables computation reuse in both encoding and associative search modules. We observed that consecutive inputs have high similarity which can be used to reduce the complexity of the encoding step. The previously encoded hypervector is reused to eliminate the redundant operations in encoding the current input. <inline-formula><tex-math notation=\"LaTeX\">$\\mathtt {HD}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"monospace\">HD</mml:mi></mml:math><inline-graphic xlink:href=\"salamat-ieq5-2992662.gif\"/></alternatives></inline-formula>-<inline-formula><tex-math notation=\"LaTeX\">$\\mathtt {Core}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"monospace\">Core</mml:mi></mml:math><inline-graphic xlink:href=\"salamat-ieq6-2992662.gif\"/></alternatives></inline-formula>, additionally eliminates the majority of multiplication operations by clustering the class hypervector values, and sharing the values among all the class hypervectors. Our evaluations on several classification problems show that <inline-formula><tex-math notation=\"LaTeX\">$\\mathtt {HD}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"monospace\">HD</mml:mi></mml:math><inline-graphic xlink:href=\"salamat-ieq7-2992662.gif\"/></alternatives></inline-formula>-<inline-formula><tex-math notation=\"LaTeX\">$\\mathtt {Core}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"monospace\">Core</mml:mi></mml:math><inline-graphic xlink:href=\"salamat-ieq8-2992662.gif\"/></alternatives></inline-formula> can provide <inline-formula><tex-math notation=\"LaTeX\">$4.4\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>4</mml:mn><mml:mo>.</mml:mo><mml:mn>4</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"salamat-ieq9-2992662.gif\"/></alternatives></inline-formula> energy efficiency improvement and <inline-formula><tex-math notation=\"LaTeX\">$4.8\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>4</mml:mn><mml:mo>.</mml:mo><mml:mn>8</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"salamat-ieq10-2992662.gif\"/></alternatives></inline-formula> speedup over the optimized GPU implementation while ensuring the same quality of classification. <inline-formula><tex-math notation=\"LaTeX\">$\\mathtt {HD}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"monospace\">HD</mml:mi></mml:math><inline-graphic xlink:href=\"salamat-ieq11-2992662.gif\"/></alternatives></inline-formula>-<inline-formula><tex-math notation=\"LaTeX\">$\\mathtt {Core}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"monospace\">Core</mml:mi></mml:math><inline-graphic xlink:href=\"salamat-ieq12-2992662.gif\"/></alternatives></inline-formula> provides <inline-formula><tex-math notation=\"LaTeX\">$2.4\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>2</mml:mn><mml:mo>.</mml:mo><mml:mn>4</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"salamat-ieq13-2992662.gif\"/></alternatives></inline-formula> more throughput than the state-of-the-art FPGA implementation; on average, 40 percent of this improvement comes directly from enabling computation reuse in the encoding module and the rest comes from the computation reuse in the associative search module.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3021560762", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tc/SalamatIR20", "doi": "10.1109/tc.2020.2992662"}}, "content": {"source": {"pdf_hash": "4cf04e9957a740288b8909c4b4a7d4fbe2f40f0e", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "cb88f626ab255dd77083d9fba265fbc2f19cc741", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4cf04e9957a740288b8909c4b4a7d4fbe2f40f0e.txt", "contents": "\nAccelerating Hyperdimensional Computing on FPGAs by Exploiting Computational Reuse\n\n\nStudent Member, IEEE, Mohsen ImaniSahand Salamat \nStudent Member, IEEETajana Rosing \nAccelerating Hyperdimensional Computing on FPGAs by Exploiting Computational Reuse\n10.1109/TC.2020.2992662Index Terms-Brain-inspired computinghyperdimensional computingmachine learningFPGAenergy efficiency\nBrain-inspired hyperdimensional (HD) computing emulates cognition by computing with long-size vectors. HD computing consists of two main modules: encoder and associative search. The encoder module maps inputs into high dimensional vectors, called hypervectors. The associative search finds the closest match between the trained model (set of hypervectors) and a query hypervector by calculating a similarity metric. To perform the reasoning task for practical classification problems, HD needs to store a non-binary model and uses costly similarity metrics as cosine. In this article we propose an FPGA-based acceleration of HD exploiting Computational Reuse (HD-Core) which significantly improves the computation efficiency of both encoding and associative search modules. HD-Core enables computation reuse in both encoding and associative search modules. We observed that consecutive inputs have high similarity which can be used to reduce the complexity of the encoding step. The previously encoded hypervector is reused to eliminate the redundant operations in encoding the current input. HD-Core, additionally eliminates the majority of multiplication operations by clustering the class hypervector values, and sharing the values among all the class hypervectors. Our evaluations on several classification problems show that HD-Core can provide 4:4\u00c2 energy efficiency improvement and 4:8\u00c2 speedup over the optimized GPU implementation while ensuring the same quality of classification. HD-Core provides 2:4\u00c2 more throughput than the stateof-the-art FPGA implementation; on average, 40 percent of this improvement comes directly from enabling computation reuse in the encoding module and the rest comes from the computation reuse in the associative search module.\n\n\u00c7\n\n\nINTRODUCTION\n\nM ACHINE learning algorithms have shown a promising solution in many tasks, including computer vision, voice recognition, natural language processing, and health care [1], [2], [3], [4]. However, existing machine learning algorithms such as Deep Neural Networks (DNNs) are computationally expensive and require an enormous amount of resources to be executed [5], [6], [7]. Moreover, embedded devices (e.g., wearable devices, smartphones, etc.) are often constrained in terms of available processing resources and power budget [8], [9], [10], [11]. Brain-inspired hyperdimensional (HD) computing is a computational paradigm performing energy-efficient cognitive computation with comparable accuracy to computation-intensive machine learning algorithms [12], [13]. Brain performs cognition tasks based on the patterns of neural activity that are not readily associated with numbers [14]. HD computing models such neural activity patterns with vectors in high-dimensional space, called hypervectors.\n\nHD computing builds upon a well-defined set of operations with random hypervectors. In addition, it is robust in the presence of faults due to the holographic distribution of the patterns in high-dimensional space (random patterns with i.i.d distributions). HD computing offers a complete computational paradigm that applies to various learning problems, including: analogy-based reasoning [15], latent semantic analysis [16], language recognition [17], prediction from multimodal sensor fusion [18], speech recognition [19], activity recognition [20], DNA sequencing [21], and clustering [22]. In contrast to existing classification algorithms which require significantly complex and costly computation during training and inference [23], [24], [25], HD provides high parallelism with hardware-friendly training and inference operations that can be processed on lightweight embedded devices [13].\n\nAll the main steps of HD computing is illustrated in Fig. 1. The first step of HD computing is representing data with hypervectors. The encoding module generates a hypervector for each input as a set of pre-processed features. Encoding module keeps the information of the input features in a hypervector. To train the HD model, inputs belong to a class are encoded and added together to generate a hypervector representing each class. At the inference phase, the incoming input is encoded to a hypervector, called the query hypervector, and then the associative search module checks the similarity between the query and each class hypervector. The class with the highest similarity to the query hypervector is selected as the classification result. During training, encoding dominates the entire energy and execution time since the training is a simple addition operation. In the inference, both associative search and encoding steps are computationally complex. Therefore, accelerating HD requires accelerating both encoding and associative search using algorithmic and hardware optimizations. In this paper, we propose a algorithmichardware co-optimization platform to accelerate both encoding and associative search, resulting in significant acceleration of the HD computing.\n\nTo accelerate encoding module, we exploit similarity between features in consecutive inputs to eliminate the redundant computation during the encoding. Our observation on wide range of applications show that consecutive inputs have 78 percent average and up to 97 percent similarity. In our experiment we calculated the similarities between consecutive inputs of four different datasets. In these datasets most of the inputs are more than 40 percent similar to the previous input and only a few samples have less than 20 percent similarity(illustrated in Fig. 9). This similarity between consecutive inputs makes the encoded hypervectors similar in the high dimensional space as well. Therefore, instead of encoding each input to a hypervector, HD-Core generates the encoded hypervector of the current input by modifying the encoded hypervector of the previous input. This significantly reduces the required resources to generate each dimension of the encoded hypervector.\n\nAfter encoding the input, the associative search module calculates the similarity metric between the encoded input and the HD model. In order to achieve acceptable accuracy on practical classification problems (i.e., speech, activity, or face recognition), HD computing has to use class hypervectors with non-binary elements [26]. The non-binary model, unlike the binary model that uses a simple hamming distance metric, uses a more complex cosine metric to find the similarity between a query hypervector and class hypervectors. The cosine can be calculated using the dot product of an input hypervector with all stored class hypervectors, which involves a large number of multiplication/addition operations. To improve the efficiency of the associative search step, HD-Core, by taking the statistical properties of the hypervector, employs a clustering algorithm to share the values in each class hypervector. Thus, instead of multiplying all pairs of the query and the class hypervector, HD-Core adds all query elements which are going to multiply with a shared class element and finally multiplies the result of addition with the corresponding class value.\n\nThe encoding and associative search stages of HD computing consist of a substantial number of binary/fixedpoint addition and multiplication operations. HD computing operations can be deeply pipelined and parallelized at dimension level. These inherent characteristics of HD computing make FPGAs, that can provide flexibility in design and huge parallelism [13] with high energy efficiency [27], [28], an excellent match for implementing HD computing applications. In this paper, we propose novel techniques to exploit computation reuse for HD computing (HD-Core). HD-Core reduces the computation complexity of the encoding module by reusing the previously encoded hypervector. It also increases the performance of the associative search by replacing the multiplication operations with addition operations by clustering and sharing the values of class hypervector elements [29]. We also proposed an FPGA-based Acceleration of HD-Core, which significantly reduces the computational cost of both encoding and associative search.\n\nWe evaluate the impact of the HD-Core optimizations on the efficiency of the wide range of classification applications. Our evaluations on a wide range of classification problems show that HD-Core encoding provides 5:7\u00c2 (2:3\u00c2) energy efficiency, and 4:5\u00c2 (2:1\u00c2) speedup as compared to GPU (state-of-the-art FPGA [13]) implementation. HD-Core also provides 4:4\u00c2 (1:4\u00c2) energy efficiency improvement and 4:8\u00c2 (2:4\u00c2) speedup as compared to GPU (state-of-theart FPGA) implementation while ensuring the same quality of classification. We observe that 40 percent of HD-Core performance improvement comes directly from the encoding acceleration and the rest is coming from our previous optimization that accelerated the associative search [29].\n\n\nBACKGROUND AND RELATED WORK\n\n\nHD Computing Algorithm\n\nHD provides a general model of computing which can apply to various types of learning problems. Classification is one of the most commonly used learning algorithms. Fig. 1 shows the overall structure of the HD classification in both training and inference phases. Encoding module maps input data to a hypervector, a vector with D hv dimensions. Training is performed on hypervectors by adding all hypervectors corresponding to a particular class together. During training, a single hypervector is generated for each existing class. These class hypervectors are stored as the HD model. HD computing, during the inference, consists of two main steps: encoding and associative search. It uses the same encoding scheme to map a query to a hypervector with D hv dimensions. Finally, on the associative search step, it performs the reasoning task by searching for a class hypervector, which has the highest similarity to the input hypervector.\n\n\nEncoding\n\nThe first step in both training and inference of HD computing is encoding the input data to a hypervector. The main goal of encoding module is to map the input data to a hypervector with D hv dimensions (e.g., D hv \u00bc 10; 000), while keeping important information of a data point in the original space, e.g., the feature values and their indexes in the input feature vector. Based on the input data representation and the hardware platform, various encoding approaches have been introduced in the literature [12], [19], [30]. In this paper, we focus on a method from [31] since it requires less memory to store the base hypervectors and is more FPGAfriendly. The encoding is performed in three steps. 1) Base hypervector generation: First, the values of features based on the distribution of the feature values are quantized to L levels [31], [32]. Then, each level is assigned to a pregenerated hypervector, called base hypervector. To generate the base hypervector for feature level with the minimum value, BHV 0 a random binary hypervector with D hv dimensions is generated. To generate the base hypervector representing the maximum value of the features, BHV L , D hv =2 dimensions of BHV 0 are selected and flipped randomly to produce an orthogonal base hypervector to BHV 0 . To generate BHV l for feature with level l, D hv =2 l1 dimensions of the previous base hypervector, starting from the initial base hypervector, are flipped. As a result, features with closer values have more similar base hypervectors, while the minimum and maximum level of the features will are nearly orthogonal. Base hypervectors are generated offline and stored in the encoder module.\n\n2) Base hypervector permutation: After generating the base hypervectors, each element (Feature i ) of a given input feature vector is mapped to its corresponding base hypervector BHV \u00f0Feature i \u00de. To take the spatial position of input features into account, the encoding module uses a permutation operation P i \u00f0BHV \u00de, where i is the index of the feature in the input feature vector. P i \u00f0BHV \u00de can be an i-bit left rotational shift on the base hypervector. Since base hypervectors have large dimension, and they are randomly generated, permutation generates an orthogonal hypervector to its resultant shift orthogonal.\n\n3) Base hypervectors aggregation: Eventually, the permuted hypervectors are aggregated to generate the encoded hypervector. Each dimension of the encoded hypervector is generated by adding the corresponding dimension of all the permuted hypervectors together. Equation (1) represents the encoding function. For all the input features, we first read their corresponding base hypervectors, BHV \u00f0Feature i \u00de. Then each hypervector is permuted based on its location in the input feature vector , P i \u00f0BHV \u00f0Feature i \u00de\u00de. Then we perform an element-wise addition on all the permuted hypervectors (1)\n\n\nHD Model Training\n\nHD computing performs the training procedure on the encoded hypervectors. For all data corresponding to a particular class, HD computing adds all hypervectors elementwise to create a class hypervector. For example, assume\nQ i \u00bc fq i 1 ; q i 2 ;\n. . . ; q i D g is a hypervector belongs to the class ith. As shown in Equation (2), the HD model learns the common patterns and/or features of a class by adding all the encoded hypervectors of training data with the same class tag. To train the HD model, we first encode, all the inputs belongs to the same class and perform an element-wise addition on the encoded hypervectors to achieve a hypervector representing the class\nC i \u00bc fw i 1 ; w i 2 ; . . . ; w i D g \u00bc X j Q i j :(2)\nElements of the class hypervectors can have non-binary values. Non-binary hypervectors significantly increases the inference cost, as the rest of the reasoning task, i.e., similarity check, will be integer operations instead of binary operations. To reduce the computational cost, several prior works tried to binarize the class elements after training by applying a majority function on each dimension [33], [34]. However, these techniques lose some of the information stored in each class hypervector, thereby scarificing the accuracy forthe performance.\n\n\nAssociative Search\n\nAfter training, all class hypervectors are stored as the HD model (shown in Fig. 1). In inference, an input data is encoded to the query hypervector using the same encoding module used for training. The associative search module is responsible for comparing the similarity of the input query hypervector with all stored class hypervectors and selecting a class with the highest similarity. Associative search module can use different similarity metrics to find a class which has the most similarity to a query hypervector. For class hypervectors with binarized elements, Hamming distance is an inexpensive and suitable similarity metric, while class hypervectors with non-binary elements need to use cosine as the similarity metric. Most existing HD computing techniques are using binarized class hypervectors in order to eliminate the costly cosine metric [31], [33]. However, it has been shown that HD with binary model provides lower classification accuracy as compared to the non-binary model [26].\n\n\nHardware Acceleration\n\nHD comprise numerous but simple operations due to is high-dimensional nature. Prior work has proposed both algorithmic innovations as well as hardware accelerators to accelerate HD computing. The works [33], [35], [36], [37] proposed processing in-memory platforms, and the works [38], [39] proposed ASIC accelerators to run binarized HD models. The work in [36] fabricated a 3D VRRAM/ CMOS to support the primary operations of HD computing (multiplication, addition, and permutation) on 4-layer 3D VRRAM/FinFET. The work in [33] focuses on accelerating binarized HD model using digital, they design digital, resistive, and analog associative memories to accelerate Hamming distance similarity in HD inference.\n\nFPGAs provide high parallelism that can significantly improve the performance and energy efficiency of HD computing. Moreover, FPGA-based accelerators are advantageous over more specialized platforms, ASIC or processing in-memory, as they allow more flexibility in the HD model and easier customization of the model parameters such as the length of the hypervectors, the precision of the HD model (binary or non-binary) and the input features characteristics. The works [13], [29], [40], [41] use FPGAs to accelerate HD computing. The study in [40] proposes a synthesizable VHDL library for training and inference of HD on FPGAs. The accelerator is limited to the binarized model and it uses logical operations to generate the base hypervectors during the runtime to reduce the costly memory accesses to read the base hypervectors. The authors, additionally, propose approximate logics to compose the binary class hypervectors without requiring to hold the summation on hypervector components in a multi-bit format during the model training. The work in [13] proposed an automated tool to generate FPGA-based HD accelerator. Work in [13] supports HD training, retraining, and inference phases, and supports accelerators for HD model with different quantizations (power-of-two, binary, and fixedpoint). However, the inference of HD computing is still time and energy consuming. To reduce the time and energy consumption of HD inference, we exploited computational reuse methods to reduce the computation complexity of HD inference. As we proposed in our previous work [29], FACH represents class elements of a trained HD model using clustering algorithms. During runtime, instead of multiplying all inputs and class elements, FACH adds all the inputs belonging to the same class cluster centroid and multiplies the result once at the end. FACH assumes the encoded hypervector is stored in FPGA BRAM. However, in our experiments, we observe that the original encoding m odule, on average, takes 55 percent of the resources. Many of the previous works either assumed that the encoded hypervector is available in the memory [29], [33], [42] or they implemented the original encoding module [13], [38], [43]. In this work, we exploit the data locality between consecutive input data to significantly reduce the complexity of the encoding module and thereby accelerating the encoding module.\n\n\nPROPOSED HD-CORE\n\nIn HD computing, the first step in either training or inference is encoding the input feature vector,\n\u1e7c \u00bc fV 0 ; V 1 ; . . . ; V F },\nwhere F is the number of input features. The encoding uses basic permutation and binary addition on the base hypervectors to encode the input feature vector to a hypervector. Each possible value of the input feature vector, fl 0 ; l 1 ; . . . ; l l g, has a corresponding element in the set of base hypervectors fBHV 0 ;BHV 1 ; . . . ;BHV l g. To generate the encoded hypervector, we need to read the base hypervector for each feature and permute it. All the permuted hypervectors are aggregated to generate the encoded hypervector. For unchanged features between consecutive inputs, the permuted hypervectors remain unchanged therefore, it can be reused for generating the encoded hypervector of the current input.\n\nBy quantizing the value of the feature to L levels, consecutive inputs share more features with the same value. For unchanged features in consecutive inputs, the permuted hypervectors will have the same contribution in building the encoded hypervector for both current and previous input. Thus, higher similarity between consecutive inputs results in higher efficiency of the HD-Core encoding. Figure   shows the similarity of consecutive inputs in the training data for speech recognition (ISOLET [44]) and face detection (FACE [45]) dataset. The similarity in the test data would follow the same trend since the training data was shuffled for the sake of generalization. Although consecutive inputs in the ISOLET dataset have shown on average 36 percent similarity, they can be upto 67 percent similar. This metric for the FACE dataset is considerably higher, consecutive inputs share 79 percent of their features on average. By quantizing the value of the feature to L levels, consecutive inputs share more features with the same value. HD-Core leverages the high similarity between consecutive inputs, to reduce the computation complexity of the encoding step.\n\nHD-Core exploits the statistical characteristic of the HD computing in order to reduce the HD computational complexity. First, HD encodes all data points to hypervectors and send it to the training and inference phases. Fig. 3 shows an overview of the HD-Core framework consisting of five main steps: encoding, training, model quantization and validation, model refinement, and inference. After encoding the inputs( 1 ), HD-Core trains the HD model by combining data points corresponding to each class ( 2 ). HD refinement clusters the values that elements in each class hypervector can take by applying non-linear clustering on the trained class hypervectors. This method reduces the possible values that the elements of each class hypervector can take. Also, HD refinement estimates the accuracy of the new HD model on the validation data, which is a part of the training data ( 3 ). If the error rate is more significant than a pre-defined value, HD-Core adjusts the model and again clusters all values exist in each newly trained class hypervector. This clustering gives us new centroids, which better represent the distribution of the values in each class hypervector. This process continues iteratively until the convergence condition (DE < ) is satisfied, or the algorithm has run for a pre-defined number of iterations ( 4 ). When the convergence condition satisfied, HD-Core framework sends a new HD model with the clustered class elements to inference in order to perform the rest of the classification task. HD-Core uses the modified HD model with clustered class elements for inference ( 5 ). In the following subsections we explain the HD-Core encoding and associative search in detail.\n\n\nHD-Core Encoding\n\nTo reduce the complexity of encoding and reduce the required resources, we exploit locality to generate the currently encoded hypervector by reusing the encoded hypervector of the previous input. The encoded hypervector is the result of aggregating the permuted hypervectors over different features. The unchanged features have the same permuted hypervector while for the other features a new permuted hypervector is needed. To reuse the previously encoded hypervector, the permuted hypervector of the feature that changed from the previous input should be subtracted from the encoded hypervector. Then, the permuted hypervector of the current feature should be added to the encoded hypervector. Therefore, the operations for unchanged features are reused from the previous input. However, for changed features, one subtraction and one addition are required to generate every dimension of the encoded hypervector. Therefore, each change in the input feature vector has one operation overhead.\n\nThis technique reduces the number of required operations whenever the similarity between consecutive inputs is higher than 50 percent. Having higher than 50 percent similarity between consecutive inputs is a strict condition which is not practical in all applications. To eliminate the additional operation overhead, HD-Core introduces transition hypervectors (THV), which are hypervectors that represent the transition between one feature level to another feature level. A transition hypervector is the base hypervector for a feature level subtracted by the base hypervector of the previous feature level. L \u00c0 1 transitions are possible for each level of feature. For level i of feature, the transition from level i to {level 0; . . . ; level i \u00c0 1} and to {level i \u00fe 1; . . . ; level l} are possible. Equation (3) shows the transition hypervector when the value of a feature changes from level i to level i 0\nTHV i!i 0 \u00bc BHV i 0 \u00c0 BHV i THV i 0 !i \u00bc BHV i \u00c0 BHV i 0 ) THV i 0 !i \u00bc \u00c0THV i!i 0 :(3)\nAccording to the Equation (3), the transition hypervectors for transition from i to i 0 is the negative form of that for transition from level i 0 to level i. HD-Core, instead of storing all the transition hypervectors, only stores the transition hypervector for the transitions from a lower level to a higher one. To generate transition hypervectors to/from the level 0 from/to any other L \u00c0 1 levels, base hypervector of level 0 is subtracted from the base hypervector of level i to generate transition hypervector THV 0!i . Equation (4) shows all the transition hypervectors that we store in the memory which includes transitions from level i to the higher levels. The rest of the transition hypervectors are generated and stored as THV i 0 !i (8i 0 < i) which are the negative form of THV i!i 0 . In total, L 2 \u00c0 \u00c1 transition hypervectors are stored while in the original HD encoding L base hypervectors are stored. Therefore, as shown in Equation (5), L \u00c2 L\u00c03 2 more hypervectors as the regular HD model are needed to store in the memory\nTHV i!i 0 8j j L ! j > i (4) L \u00c2 L \u00c0 1 2 \u00c0 L \u00bc L \u00c2 L \u00c0 3 2 :(5)\nQuantizing the input features to L levels not only increases the consecutive inputs similarity, but it reduces the the memory overhead of using the HD-Core encoding as well. Using the HD-Core encoding reduces the number of required operations proportional to the similarity of consecutive inputs. Unlike the original encoding, the number of required operations is not fixed; it depends on the type application and sequence of inputs. By pre-processing the training data, a metric for similarity (SM) of the consecutive inputs is calculated. Although the SM is calculated based on the characteristics of the training data, the same pattern is observed during the inference. SM represents the similarity of all two consecutive inputs with a value. The similarity between each two consecutive input can be: (i) less than the SM, (ii) equal to the SM, and (iii) higher than the SM. In the first case, the encoding cannot be done in one step. Extra steps are required to encode the input; in these steps, the associative search module will be stalled. In the second case, the encoding hardware is fully utilized, and the input is encoded in one step. In the case of having a higher similarity than SM, the underutilized encoding module can encode the input in one step. The function Enc L \u00f0d i ; d i\u00c01 ; SM\u00de returns the number of required cycles to encode input d i while the encoded hypervector of input d i\u00c01 is available. The SM% is calculated based on the similarity to minimize the operations, based on the similarity of the consecutive inputs of training data. Calculating the optimum SM, additionally, depends on the hardware implementation. HD-Core uses the average similarity of the consecutive inputs in the training data for the SM parameter.\n\n\nHD-Core Associative Search\n\nPerforming cosine similarity between two vectors involves calculating the dot product of vectors divided by the size of each vector. Since HD trains the model offline, the normalization of the class hypervectors can be performed offline. On the other hand, input data is common between all class hypervectors, thus it does not need to be normalized. Therefore, cosine similarity between a query Q \u00bc fq 1 ; q 2 ; . . . ; q D g and ith class hypervector, C i \u00bc fw i 1 ; w i 2 ; . . . ; w i D g, requires calculating their dot product which involves D additions and D multiplications, where D is the dimension of the hypervectors.\n\nIn this work, model refinement in HD-Core reduces the class span by carefully selecting a subset from the input spaces, called \"best representatives\". HD-Core limits the number of values that each class element can take (i.e., fw 1 ; . . . ; w D g 2 fc 1 ; . . .; c k g and k < < D\u00de). This enables us to remove the majority of cosine multiplications by factorization. In other words, instead of multiplying the D elements of query and class hypervector, we add the input data for all dimensions for which class hypervector has the same element. Finally, the result of the addition is multiplied by the value of that particular class.\n\nHere we explain how HD-Core can limit the number of each class elements with no or minor impact of classification accuracy. To find representative class elements, the clustering algorithm is applied to the pre-trained class hypervectors. For each class hypervector, our design identifies a specified number of clusters, say k, based on clustering algorithms. The centroids of clusters are selected as the representative weights and stored into the weight table.\n\nAssuming that the actual numerical values belong to a set u, the objective of the clustering algorithm is to find a set of k cluster centroids fc 1 ; c 2 ; . . . ; c k g that can best represent the class values (c 2 N) fw i 1 ; w i 2 ; . . . ; w i D g 2 fc i 1 ; c i 2 ; . . . ; c i k g:\n\nFormally, the objective is to reduce the Within Cluster Sum of Squares (WCSS)\nmin c 1 ; c 2 ; ...;c k WCSS \u00bc X k j\u00bc1 X u i 2c j jju i \u00c0 c j jj 2 0 @ 1 A ;(7)\nwhere u i is the ith sample drawn from u and k is the number of clusters. We use the k-means clustering algorithm to solve the minimization objective for each HD class hypervector separately, as the distribution of values can vary across different classes. The calculation of dot product between query, Q, and a class hypervector, C i , can be simplified by adding all query elements which belong to the same cluster in class hypervector. For example, for class dimensions with c k elements, our design adds all corresponding query elements together (s k \u00bc P j \u00bc q j where w j \u00bc c k ). In a similar way, our design calculates the accumulative query elements on all k cluster centroids: fs 1 ; s 2 ; . . . ; s k g and s 2 N. Finally, these values multiply with each corresponding cluster values and accumulate together to generate a dot product between Q and C i hypervectors\nQ:C i \u00bc s 1 \u00c2 c 1 \u00fe s 2 \u00c2 c 2 ; \u00fe . . . s k \u00c2 c k :(8)\nThis method reduces the number of multiplications involved in dot product from D to k, where k can be about three orders of magnitudes smaller than D. Fig. 5 shows an example of the dot product between a class and a query vector using conventional method and clustered model. Since in the conventional method, the class elements can take any value, the dot product involves six multiplications (Fig. 5a). HD-Core exploits the advantage of clustered class values to first add the query elements corresponding to the same centroid and then multiply the result with the centroid values (Fig. 5b). This reduces the number of multiplications to two. Error Estimation. Sharing the elements of input and class hypervectors reduces the HD classification accuracy. After the training, our design replaces the elements of the class hypervectors with the closest representative values (cluster centroids). We estimate the error rate of the new model by cross-validating the cluster HD on a validation data, which is a part of the training data. The quality loss, DE is defined as the error rate difference between the HD using original and modified models (DE \u00bc E clustered \u00c0 E original ).\n\nModel Adjustment. If the error rate does not satisfy the tolerance DE < , HD-Core adjusts the new model by retraining the network over the same training dataset. In retraining process, HD composer looks at the similarity of each input hypervector to all stored class hypervectors; (i) if an input data correctly matches with the corresponding class in associative memory, our design does not change the mode. (ii) if an input hypervector, Q, wrongly matches with the ith class hypervector (C i ) while it actually belongs to jth class (C j ), our retraining procedure subtracts the input hypervector from the ith class and add it to jth class\n(C i \u00bc C i \u00c0 Q & C j \u00bc C j \u00fe Q)\n. After adjusting the model over the training data, HD refinement again clusters the data in each class hypervector and estimate the classification error rate. We expect the model retrained under the modified condition to better fit with the clustered values. Since we start clustering from first iterations, both baseline and HD-Core show almost the same pattern in increasing the accuracy of the model during the retraining. If an error criterion is not satisfied, we follow the same procedure until an error rate, , is satisfied or we reach to a pre-specified number of iterations. After the iterations, the new model, which is compatible with the proposed accelerator, is used for real-time inference. Fig. 6a shows the classification accuracy of applications during different retraining iterations when the class elements are clustered to 32 values. Our evaluation shows that HD refinement can compensate for quality loss due to clustering by using less than 30 iterations. Since the maximum number of retraining iterations is limited to 30 and the baseline HD model also requires the refinement iterations, the overhead of the HD-Core training is negligible. All pre-processing operations in the HD refinement module are  performed offline and their overhead is amortized among all future executions of HD-Core accelerator. Fig. 6b shows the final quality loss, DE, when HD-Core clusters the class hypervector to a different number of centroids. We consider the cluster sizes of 4, 8, 16 and 32. The results show that different applications can provide DE \u00bc 0% while using a different number of class clusters. For example, face recognition can achieve DE \u00bc 0% when the class elements are clustered to 16 centroids, while human activity recognition (UCIHAR) achieves DE \u00bc 0% using 32 cluster centroids. In Section 5, we will explain the accuracy-efficiency trade-off in HD-Core using different clusters.\n\n\nHD-CORE FPGA ACCELERATION\n\nWe use FPGA to accelerate HD computing encoding and inference. In this we implement the HD-Core on an FPGA and use the platform proposed in [13] as the baseline FPGA implementation. Fig. 7A shows that the FPGA-based implementation of the baseline HD requires multiplication for S parallel dimensions to calculate the dot product between the query and class hypervectors. Then, the results of all S multiplications accumulate in a tree-based adder. The size of the S, the number of input dimensions which FPGA reads at a time, depends on the number of classes, and the number of available resources (e.g., LUTs, BRAM, and DSPs) in FPGA. In this case, our design sequentially generates the first S elements of the query vector and multiply it to the corresponding class elements (S < D). Then, the computations on the rest of the query elements are performed sequentially. In the following, we explain how each design can be accelerated on FPGA.\n\n\nHD-Core Encoding Acceleration\n\nThe first step in HD is encoding the input feature vector\u1e7d into the query hypervectorQ, using fundamental permutation and addition on the transition hypervectors (the baseline encoding instead of using transitions hypervectors, uses base hypervectors). As previously shown in Section 3.1, for input features that are not changed from the previous input no additional operation is needed, while for the others, the transition hypervector should be either added or subtracted from the previously encoded hypervector. Recall that the dimensions of transition hypervectors binary numbers that aggregate in a dimensionwise pattern to generate each dimension of the encoded hypervector. Dimensions of the encoded hypervector can be in various widths and representations (e.g., fixed-point, binary, etc.). HD computing can be parallelized at dimension level since the similarity metric for each generated dimensions of the encoded hypervector can be calculated independent of the other dimensions. The final similarity metric is the aggregation of dimensions similarity. The encoding and associative search blocks are working in a pipeline structure; therefore, to maximize the resource utilization, the number of encoded dimensions generated in each cycle should be equal to the number of dimensions that the associative search module can process. Therefore, we segregate the encoded hypervector into the segments of S dimensions whereby at each clock cycle, one segment is generated. The generated S dimensions are passed to the associative search module to calculate the similarity metrics between the query hypervector and the class hypervectors. Thus, processing the entire query hypervector takes T \u00bc D S cycles. The value of S, which represents the parallelism level, is limited by the available resources. R E shows the required resources to generate a dimension of the encoded hypervector. R A:S shows the required resources for the associative search module to process a dimension of the encoded hypervector. Therefore S \u00c2 \u00f0R Enc \u00fe R A:S \u00de should be less than the available resources of the FPGA. Since the number of operations executed in each cycle in the HD-Core encoding as compared to the baseline encoding is reduced by SM%, the required resources to implement the HD-Core encoding module is also SM% less than the baseline R HD\u00c0Core\u00c0Enc \u00bc SM% \u00c2 R E . These resources dedicated to the HD-Core encoding module can generate S dimensions of the encoded hypervector when the similarity of current input and the previous input is higher than or equal to SM. In general, Equation (9) shows the number of iterations required to generate S dimensions of the encoded hypervector, while R HD\u00c0Core\u00c0Enc resources are dedicated to the encoding module\nLatency\u00f0d i ; d i\u00c01 ; SM\u00de \u00bc 1 \u00c0 Similarity\u00f0d i\u00c01 ; d i \u00de SM $ % :(9)\n4.2 HD-Core Associative Search Acceleration Fig. 7 illustrates the HD-Core architecture, which supports dot product between a query and a single class hypervector. The class hypervector has k clustered values, i.e., the class elements can take one of the k cluster centroids, fc 1 ; c 2 ; . . . ; c k g.\n\nTo accelerate HD-Core, our design creates k index buffers, where each buffer represents one of the cluster centroids ( B ).\n\nEach buffer stores the indices of the class elements which have clustered to the same value. For example, the first index buffer, shown in Fig. 7B, stores all class indices which have the value as c 1 . Since each class has D dimensions, we require log 2 D bits to store each index. Due to resource limitation of FPGA, we can only read S dimensions of the query hypervector at a time and process the remaining dimensions in sequential windows. However, sequentially accessing the query elements increases the number of resource requirements, since all S elements in a read window might belong to any of the clusters. In this case, each index buffer requires a tree-based adder with S inputs in order to take care of the worst case scenario, when all S query dimensions correspond to a single cluster. Instead, in this work, each read window accesses to b \u00bc S=k indices from each index buffer. This method ensures that the number of required resources to add the element of each index buffer is less than b. We define this b window size as the batch size. In order to speed up the computation, HD-Core stores the index buffers, which are a compressed/ trained HD model, inside the FPGA. These buffers are implemented using distributed memory using LookUp Table (LUT) and Flip-Flop (FF) blocks.\n\nEach element of the index buffer points to one dimension of the query hypervector. In order to maximize the FPGA resource utilization, for all elements of index buffer in a batch window, HD-Core pre-fetches the query elements and store them in query buffers ( C ). Next to each query buffer, a tree-based adder accumulates all S=k indices corresponding to a particular centroid ( D ). The results of these additions are stored in registers. Next, FPGA processes the next batch sequentially. HD-Core is implemented in a pipeline, where the pre-fetching of the elements to query buffer performs simultaneously with the addition of the query elements which have been pre-fetched to query buffers in the last iteration. This pipeline can perform very efficiently since these two tasks require different types of FPGA resources. The indexing and pre-fetching are memory-intensive tasks and mostly utilize BRAM, LUTs, and FFs, while the addition of query elements mostly utilizes DSPs.\n\nAfter every iteration, the values corresponding to the registers are accumulated. Once HD-Core has processed all D dimensions of the hypervector, each register has the accumulated query elements in all the dimensions for which class hypervector has the same clustered value. For each index buffer, our design multiplies the value of the register with the corresponding cluster value. The results of multiplication for all cluster centroids are then accumulated in order to generate the final dot product ( E ). Regardless of the method used for calculating dot product, our design needs to compare the dot products for all existing classes and select the class which has the maximum similarity with the input vector.\n\n\nRESULTS\n\n\nExperimental Setup\n\nThe proposed HD-Core has been implemented with software and hardware modules. For software support, we use Python to find the similarity metric between consecutive inputs in the training data. To eliminate the dependency of the SM to the distribution of the training data, we shuffle the training data for 10 times and calculate the average similarity each time and use the average as the SM. We exploit Scikit-learn library [46] for clustering and C++ software implementation for the HD model training and verification. For hardware support, we use FPGA to accelerate HD computing. We fully implement HD-Core inference functionality in RTL using Verilog HDL. HD-Core is deeply pipelined to run with 200 MHz clock frequency. We verify the timing and functionality of the design using both synthesis and real implementation of the HD-Core using Xilinx Vivado Design Suite [47]. To estimate the power consumption of the FPGA we use the builtin Xilinx Power Estimation tool in Vivado Desigh suite. HD-Core is implemented on the Kintex-7 FPGA KC705 Evaluation Kit. We compare the performance and energy efficiency of the FPGA-based implementation of HD-Core with the NVIDIA GTX 1080 GPU. The GPU-based implementation uses the same algorithmic optimization for the associative search as HD-Core; however, to maximize the performance of the GPU-based implementation, it uses the baseline encoding since we observed it provides a higher performance on GPU. The performance and energy of GPU are measured by the nvidia-smi tool. We also compare the HD-Core implementation with the state-ofthe-art FPGA-based accelerator proposed in [13] as the FPGA baseline.\n\n\nWorkloads\n\nWe evaluate the efficiency of the proposed HD-Core on four popular classification applications, as listed below:\n\nSpeech Recognition (ISOLET). The goal is to recognize voice audio of the 26 letters of the English alphabet. The training and testing datasets are taken from ISOLET dataset [44].\n\nFace Recognition (FACE): We exploit Caltech dataset of 10,000 web faces [45]. Negative training images, i.e., nonface images, are selected from CIFAR-100 and Pascal VOS 2012 datasets [48].\n\nActivity Recognition (UCIHAR) [49]: The dataset includes signals collected from motion sensors for 8 subjects performing 19 different activities. The objective is to recognize the class of human activities.\n\nPhysical Activity Monitoring (PAMAP) [50]. This dataset includes logs of 8 users and three 3D accelerometers positioned on arm, chest and ankle. They were collected over different human activities such as lying, walking and, ascending stairs, and each of them corresponded to an activity ID. The goal is to recognize 12 different activities.\n\n\nHD-Core Encoding\n\nThe computation complexity of the encoding step increases with the number of input features. In both training and inference, encoding consumes a great portion of resources. Fig. 8 shows the ratio of the LUT utilization in the encoding module to the LUT utilization of the entire accelerator. We compared the resource utilization of the baseline encoding with the HD-Core encoding when the associative search module is the HD-Core with 4 and 8 centroids (C4 and C8 respectively). In the ISOLET dataset, due to its higher number of classes, the complexity of the associative search is higher. Therefore, the associative search in ISOLET requires more resources to calculate the similarity metric for a dimensions than other applications. As shown in the figure, the baseline encoding module consumes 23 percent of the entire LUT resources, while the proposed HD-Core encoding module drops the LUT utilization to 18 percent. In the FACE dataset, most of the resources are dedicated to the encoding module as there are only 2 classes in the dataset. Therefore, the baseline encoding module consumes 82 percent of the entire resources. Nevertheless, by using the HD-Core encoding, only 54 percent of the resources are dedicated to the encoding module. Note that, HD-Core encoding not only utilizes less LUTs than the baseline encoding module, it generates more dimensions (2:1\u00c2) of the encoded hypervector per cycle than the baseline encoding. In PAMAP, since the number of input features are relatively low, the complexity of the encoding is significantly less than the other datasets.\n\nWe implement the HD-Core encoding on FPGA and exploit input similarity to accelerate the encoding step. As explained in Section 3.1, quantizing the value of the input features increases the similarity between consecutive inputs. However, quantization may drop the classification accuracy. Table 1 shows the impact of feature quantization on the classification accuracy as well as the average similarity between consecutive inputs. As illustrated in Table 1, for all of the datasets, accuracy starts dropping when the input features quantized to 2 bits. However, the classification accuracy for FACE drops 0.2 percent when the input features are quantized to 2 bits. In the rest of the paper, we use the minimum bit width for the input features that provides the maximum accuracy. Thus, in in our experiments, we quantize the input features to 2 bits for FACE application, and to 3 bits for the rest of the applications. In Fig. 9, blue bars show the histogram of the similarity between consecutive inputs in training data, and the red line shows the Cumulative Distribution of the similarities between consecutive inputs for each dataset. As illustrated in the figure, only a few of the inputs in all of the datasets have less than 20 percent similarity and most of the inputs have higher similarity than the SM mentioned in Table 1.\n\nHD-Core encoding stores the transition hypervectors instead of base hypervectors; thus, BRAM usage in HD-Core encoding is higher than the baseline encoding. In Fig. 10, the blue line shows the required memory to store the transition hypervectors in MB (left axis) for different quantization levels, while the dashed red line shows the available BRAM in the Kintex FPGA. The green line in Fig. 10 shows the average similarity between consecutive inputs in all the datasets for different quantization levels. In our experiments we observed that quantizing the input features to even to 4 bits has no impact on the prediction accuracy, while observing 34.8 percent similarity between consecutive inputs in all the datasets on average. Due to the limitation of the FPGA BRAMs, HD-Core encoder can support inputs with features 2 5 \u00bc 32 levels, where still inputs are on average 30.4 percent similar.\n\nHD-Core encoding exploits the similarities between consecutive inputs to reuse the previously encoded hypervector, thereby reducing the required hardware to generate each dimension of the encoded hypervector. Fig. 11 compares the throughput and energy consumption of HD-Core encoding module with the GPU and FPGA baseline implementations. Throughput and energy consumption of HD-Core and the FPGA baseline are normalized to those of the GPU implementation. HD-Core encoding can encode 4:5\u00c2 and 2:1\u00c2 more inputs in a second as compared to GPU and FPGA baseline respectively. HD-Core encoding reduced the energy required to encode each input for 5:7\u00c2 and 2:3\u00c2 as compared to the GPU and FPGA baseline respectively. Fig. 11, additionally, shows that for applications with a higher similarity between inputs the throughput   improvement will be higher. In the FACE dataset which has the highest SM among the other datasets, HD-Core encodes 9\u00c2 more inputs per second with 13:2\u00c2 less energy as compared to the GPU implementation. Table 2 shows the throughput and the energy consumption of the HD-Core when it first encodes the data and then performs the associative search (Encoding and Associative Search), as well as when the encoded hypervector of inputs are stored in memory and HD-Core only performs the associative search. Throughput and energy consumption of HD-Core are compared with the baseline FPGA implementation. C4, C8, C16, C32 show the number of shared elements (centroids) in each class hypervector. Associative Search column shows the impact of HD-Core when the encoded hypervector is stored in BRAM, and all of the FPGA resources are dedicated to the associative search module. Comparing the results of baseline HD with the HD-Core shows that HD-Core can improve the efficiency of the HD computing by reducing the number of operations in encoding and reducing the number of multiplication in the associative search step. HD-Core performance depends on the number of shared class elements. HD-Core with more number of centroids requires more FPGA resources to implement and therefore, its throughput is less than HD-Core with less number of centroids while providing a higher accuracy. The power consumption of the FPGA is highly correlated with the resource utilization and HD-Core with various number of centroids have close resource utilization and consequently close power consumption. Therefore, the energy consumption of the HD-Core with more centroids is higher since increasing the number of centroids reduces the throughput; consequently, increasing the execution time. As we discussed in Section 3.2, HD-Core accuracy depends on the number of shared class elements. Fig. 6b shows the impact of the number of centroids on the classification accuracy. As illustrated in this figure, as the number of centroids increases, the accuracy increases. For example, in UCIHAR, increasing the number of centroids from 4 to 32 increases the accuracy for 3 percent, at the cost of decreasing the performance and energy efficiency for 19, and 22 percent respectively. HD-Core with 4 centroids, on average, shows 2:4\u00c2 and 1:4\u00c2 performance and energy improvement as compared to the baseline FPGA while dropping the accuracy for 2 percent. HD-Core encoding contributes to 40 percent of this performance improvement. Enabling the computation reuse in the encoding module significantly accelerates the design. HD-Core, by exploiting the computational reuse in the encoding module is 96 percent faster than HD-Core with the original encoding.\n\n\nHD-Core Accuracy-Efficiency Trade-Off\n\nHD-Core with 32 centroids provides the same classification accuracy as the baseline HD while on average shows 1:8\u00c2 with 13 percent energy overhead. Comparing these results with the GPU-based approach shows that HD-Core with 32 centroids can provide 2:7\u00c2 higher energy efficiency and 3:6\u00c2 speedup. Performance and energy improvement can increase up to 4:8\u00c2 and 4:3\u00c2 by using 4 centroids. Fig. 12 compares the throughput and energy efficiency improvement normalized to those of the GPU. As illustrated in Fig. 12 the energy improvement for FACE dataset is significantly higher, while for PAMAP, the energy improvement is slightly higher than the FPGA baseline. In FACE dataset, since it has 2 classes, the required computation is less than the other datasets. Also, the similarity between consecutive inputs in this dataset is significantly higher than the other datasets. In PAMAP the number of input features is less than the other datasets; thus, the encoding will be significantly less complex. In this dataset, the associative memory is the bottleneck of performance since the encoding module requires fewer resources as compared to the others.   Table 3 shows the resource utilization of the HD-Core as well as the encoding resource utilization breakdown for FACE and ISOLET datasets with 4, 8, 16, and 32 cluster centroids. For the HD-Core encoding, all the operations are implemented on LUTs, while the associative search module uses FPGA DSPs to calculate the similarity metric. HD-Core tries to fully utilize the FPGA resources; however, in our experiments, when the resource utilization is above 90 percent, design may face routing congestion issues. In HD-Core Encoding, BRAM utilization is independent of the number of centroids since BRAMs are only used to store the transition hypervectors. Due to the higher number of transition hypervectors in ISOLET, HD-Core encoding requires more BRAMs to implement ISOLET data encoding as compared to encoding the FACE dataset. Associative search module utilizes DSPs to calculate the similarity metric between the class hypervectors and the query hypervector. Therefore, as the number of classes increases, the number of operations in the associative search will also increase. Therefore, for ISOLET with 26 classes, since the number of operations in the associative search is much higher than the that in the FACE dataset with 2 classes, the number of DSPs limits the parallelism. While in the FACE dataset the encoding module is the bottleneck of parallelism. Increasing the number of centroids increases the HD-Core computation complexity; thus, more resources are required to provide the same parallelism. Since, the number of the available resources are fixed, increasing the number of clusters reduces the parallelism, which reduces the required resources to implement HD-Core encoding.\n\n\nHD-Core Resource Utilization\n\n\nCONCLUSION\n\nWe propose a novel hyperdimensional computing FPGA implementation exploiting computation reuse, called HD-Core, which significantly reduces the cost of classification. The framework reuses previously encoded hypervector to reduce the complexity of the encoding step. It also extracts representative operands of a trained HD model using clustering algorithms. At runtime, instead of multiplying all inputs and class elements, our design adds all the inputs belonging to the same class cluster centroid and multiplies the result once in the end. Our evaluation over a wide range of applications shows that HD-Core can provide 4:8\u00c2 faster execution and 4:3\u00c2 higher energy efficiency as compared to the GPU implementation. The HD-Core encoding provides 2:1\u00c2 more throughput and 2:3\u00c2 energy efficiency as compared to the FPGA baseline implementation proposed in [13]. This efficiency in the encoding module contributes to 40 percent of the 2:4\u00c2 performance improvement of the HD-Core as compared to the state-of-the-art FPGA accelerator [13]. In future, we are going to apply HD-Core algorithmic optimizations on other hardware platforms including GPUs and ASIC. He is the author of more than 85 publications at top tier conferences and journals. His contributions resulted in several grants funded from multiple governmental agencies (four NSF, three SRC grants) and several companies including IBM, Intel, Micron, and Qualcomm. He has received the most prestigious awards from the UCSD School of Engineering including the Gordon Engineering Leadership Award and the Outstanding Graduate Research Award. He also got several nominations for the best paper awards from multiple conferences. His research interests include brain-inspired computing and computer architecture.\n\nTajana Rosing (Fellow, IEEE) received the master's degree in engineering management and the PhD degree from Stanford University, Stanford, California, in 2001. She is a professor, a holder of the Fratamico endowed chair, and a director of System Energy Efficiency Lab, UCSD. She is currently heading the effort in SmartCities as a part of DARPA and industry funded TerraSwarm center. During 2009-2012, she led the energy efficient datacenters theme as a part of the MuSyC center. Her research interests include energy efficient computing, embedded and distributed systems. Prior to this, she was a full time researcher with HP Labs while being leading research part-time with Stanford University. Her PhD topic was dynamic management of power consumption. Prior to pursuing the PhD, she worked as a senior design engineer with Altera Corporation.\n\nFig. 1 .\n1HD functionality in training and inference phases using encoding and associative search modules.\n\n\n\u00f0BHV \u00f0Feature i \u00de\u00de:\n\nFig. 2 .\n2Similarity between consecutive inputs in two datasets.\n\nFig. 3 .\n3HD-Core supporting Framework consisting of HD encoding, training and inference.\n\nFig. 4 .\n4Original and HD-Core encoding.\n\nFig. 5 .\n5An example of dot product between the class and query vectors with six dimensions (a) using conventional method, (b) when the elements of the class vector clustered.\n\nFig. 6 .\n6a) The classification accuracy of applications during retraining iterations. b) Impact of number of class elements on the quality loss of different applications.\n\nFig. 7 .\n7FPGA-based implementation to calculate the dot product between a query and class hypervectors in baseline HD and HD-Core with clustered class elements.\n\nFig. 8 .\n8LUT utilization ratio of the encoding module to the entire accelerator for the baseline encoding and HD-Core encoding with associative search modules with 4 and 8 centroids.\n\nFig. 9 .\n9Histogram and CDF of similarities between consecutive inputs for each dataset.Fig. 10. HD-Core BRAM utilization and average similarity of consecutive inputs for different quantization levels of input features.\n\nFig. 11 .\n11Throughput and energy improvement of HD-Core encoding in comparison with GPU and the FPGA baseline[13].\n\nTABLE 1\n1Impact of Input Quantization on the Average Similarity of Consecutive Inputs and the Classification Accuracy1 bit \n2 bits \n3 bits \n4 bits \n\nAcc(%) SM(%) Acc(%) SM(%) Acc(%) SM(%) Acc(%) SM(%) \n\nISOLET \n93.6 \n93 \n94.7 \n59 \n95.5 \n36 \n95.5 \n23 \nUCIHAR 95.9 \n99 \n97.3 \n75 \n98 \n51 \n98.1 \n36 \nFACE \n89.9 \n99 \n92.9 \n79 \n93.1 \n55 \n93.1 \n38 \nPAMAP 94.3 \n72 \n95.9 \n56 \n96.7 \n44 \n96.8 \n39 \n\n\n\nTABLE 2\n2Comparison of HD-Core With the FPGA Baseline Implementation in Terms of Throughput (1000\u00c2 Classification Per Second) and Energy Consumption for Classifying an Input (mJule)Dataset \nEncoding and Associative Search \nAssociative Search \nBaseline FPGA [13] \nC4 \nC8 \nC16 \nC32 Baseline FPGA [13] \nC4 \nC8 \nC16 \nC32 \n\nISOLET Throughput \n258.5 \n621.1 565.0 537.6 507.6 \n560.0 \n621.1 565.0 537.6 507.6 \nEnergy \n17.1 \n12.6 \n14.0 \n14.7 \n15.7 \n17.0 \n12.1 \n13.3 \n14.0 \n14.7 \n\nUCIHAR Throughput \n1119.4 \n2873.9 2816.9 2666.7 2020.2 \n1440.0 \n3076.9 2816.9 2666.7 2531.6 \nEnergy \n3.5 \n3.2 \n3.1 \n3.3 \n3.9 \n3.2 \n2.4 \n2.6 \n2.8 \n2.9 \n\nFACE \nThroughput \n3348.2 \n7692.3 6666.7 5263.2 5000.0 \n4320.0 \n7692.3 6896.6 6451.6 6451.6 \nEnergy \n3.2 \n1.2 \n1.3 \n1.5 \n1.7 \n1.9 \n1.0 \n1.1 \n1.1 \n1.2 \n\nPAMAP Throughput \n562.2 \n1242.2 1129.9 1075.3 1015.2 \n720.0 \n1242.2 1129.9 1075.3 1015.2 \nEnergy \n5.8 \n5.7 \n5.8 \n6.1 \n6.6 \n7.0 \n6.0 \n6.3 \n6.8 \n7.2 \n\n\nTABLE 3\n3Resource Utilization of HD-Core Encoding and HD-Core for FACE and ISOLET Datasets Mohsen Imani (Student Member, IEEE) is working toward the PhD degree with the Department of Computer Science and Engineering, UC San Diego, San Diego, California.\n\" For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.\nACKNOWLEDGMENTS\nA survey on machine learning: Concept, algorithms and applications. K Das, R N Behera, Int. J. Innovative Res. Comput. Commun. Eng. 52K. Das and R. N. Behera, \"A survey on machine learning: Con- cept, algorithms and applications,\" Int. J. Innovative Res. Comput. Commun. Eng., vol. 5, no. 2, pp. 1301-1309, 2017.\n\nAlways-on speech recognition using truenorth, a reconfigurable, neurosynaptic processor. W.-Y Tsai, IEEE Trans. Comput. 666W.-Y. Tsai et al., \"Always-on speech recognition using truenorth, a reconfigurable, neurosynaptic processor,\" IEEE Trans. Comput., vol. 66, no. 6, pp. 996-1007, Jun. 2017.\n\nNeural network methods for natural language processing. Y Goldberg, Synthesis Lectures Hum. Lang. Technol. 101Y. Goldberg, \"Neural network methods for natural language processing,\" Synthesis Lectures Hum. Lang. Technol., vol. 10, no. 1, pp. 1-309, 2017.\n\nA low-power, highperformance speech recognition accelerator. R Yazdani, J.-M Arnau, A Gonz, IEEE Trans. Comput. 6812R. Yazdani, J.-M. Arnau, and A. Gonz alez, \"A low-power, high- performance speech recognition accelerator,\" IEEE Trans. Comput., vol. 68, no. 12, pp. 1817-1831, Dec. 2019.\n\nDaDianNao: A neural network supercomputer. T Luo, IEEE Trans. Comput. 661T. Luo et al., \"DaDianNao: A neural network supercomputer,\" IEEE Trans. Comput., vol. 66, no. 1, pp. 73-88, Jan. 2017.\n\nAn energy-efficient stochastic computational deep belief network. Y Liu, Y Wang, F Lombardi, J Han, Proc. Des. DesY. Liu, Y. Wang, F. Lombardi, and J. Han, \"An energy-efficient stochastic computational deep belief network,\" in Proc. Des. Autom. Test Eur. Conf. Exhib., 2018, pp. 1175-1178.\n\nRNSnet: In-memory neural network acceleration using residue number system. S Salamat, M Imani, S Gupta, T Rosing, Proc. IEEE Int. Conf. Rebooting Comput. IEEE Int. Conf. Rebooting ComputS. Salamat, M. Imani, S. Gupta, and T. Rosing, \"RNSnet: In-mem- ory neural network acceleration using residue number system,\" in Proc. IEEE Int. Conf. Rebooting Comput., 2018, pp. 1-12.\n\nEnergyefficient pattern recognition hardware with elementary cellular automata. A M Costoya, C F Frasser, M Roca, J L Rossello, IEEE Trans. Comput. 693A. M. Costoya, C. F. Frasser, M. Roca, and J. L. Rossello, \"Energy- efficient pattern recognition hardware with elementary cellular automata,\" IEEE Trans. Comput., vol. 69, no. 3, pp. 392-401, Mar. 2020.\n\nSparCE: Sparsity aware general-purpose core extensions to accelerate deep neural networks. S Sen, S Jain, S Venkataramani, A Raghunathan, IEEE Trans. Comput. 686S. Sen, S. Jain, S. Venkataramani, and A. Raghunathan, \"SparCE: Sparsity aware general-purpose core extensions to accelerate deep neural networks,\" IEEE Trans. Comput., vol. 68, no. 6, pp. 912-925, Jun. 2019.\n\nFast and efficient convolutional accelerator for edge computing. A Ardakani, C Condo, W J Gross, IEEE Trans. Comput. 691A. Ardakani, C. Condo, and W. J. Gross, \"Fast and efficient convo- lutional accelerator for edge computing,\" IEEE Trans. Comput., vol. 69, no. 1, pp. 138-152, Jan. 2020.\n\nSmall memory footprint neural network accelerators. K Seto, H Nejatollahi, J An, S Kang, N Dutt, Proc. 20th Int. Symp. Qual. Electron. Des. 20th Int. Symp. Qual. Electron. DesK. Seto, H. Nejatollahi, J. An, S. Kang, and N. Dutt, \"Small mem- ory footprint neural network accelerators,\" in Proc. 20th Int. Symp. Qual. Electron. Des., 2019, pp. 253-258.\n\nBRIC: Locality-based encoding for energy-efficient braininspired hyperdimensional computing. M Imani, J Morris, J Messerly, H Shu, Y Deng, T Rosing, Proc. 56th Annu. Des. Autom. Conf. 56th Annu. Des. Autom. ConfM. Imani, J. Morris, J. Messerly, H. Shu, Y. Deng, and T. Rosing, \"BRIC: Locality-based encoding for energy-efficient brain- inspired hyperdimensional computing,\" in Proc. 56th Annu. Des. Autom. Conf., 2019, Art. no. 52.\n\nThroughput and energy improvement in comparison with the GPU and FPGA baseline. 13Fig. 12. Throughput and energy improvement in comparison with the GPU and FPGA baseline [13].\n\nF5-HD: Fast flexible FPGA-based framework for refreshing hyperdimensional computing. S Salamat, M Imani, B Khaleghi, T Rosing, Proc. ACM/SIGDA Int. Symp. Field-Programmable Gate Arrays. ACM/SIGDA Int. Symp. Field-Programmable Gate ArraysS. Salamat, M. Imani, B. Khaleghi, and T. Rosing, \"F5-HD: Fast flexible FPGA-based framework for refreshing hyperdimensional computing,\" in Proc. ACM/SIGDA Int. Symp. Field-Programmable Gate Arrays, 2019, pp. 53-62.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cogn. Comput. 12P. Kanerva, \"Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors,\" Cogn. Comput., vol. 1, no. 2, pp. 139-159, 2009.\n\nWhat we mean when we say \"what's the dollar of Mexico?\": Prototypes and mapping in concept space. P Kanerva, Proc. AAAI Fall Symp. AAAI Fall SympQuantum Informat. Cogn. Soc. Semantic ProcessesP. Kanerva, \"What we mean when we say \"what's the dollar of Mexico?\": Prototypes and mapping in concept space,\" in Proc. AAAI Fall Symp.: Quantum Informat. Cogn. Soc. Semantic Processes, 2010, pp. 2-6.\n\nRandom indexing of text samples for latent semantic analysis. P Kanerva, Proc. Annu. Meeting. Annu. Meeting1036P. Kanerva et al., \"Random indexing of text samples for latent semantic analysis,\" in Proc. Annu. Meeting Cogn. Sci. Soc., vol. 1036, 2000.\n\nLanguage geometry using random indexing. A Joshi, J T Halseth, P Kanerva, Proc. Int. Symp. Quantum Interact. Int. Symp. Quantum InteractA. Joshi, J. T. Halseth, and P. Kanerva, \"Language geometry using random indexing,\" in Proc. Int. Symp. Quantum Interact., pp. 265-274, 2016.\n\nSequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns. O J , J P Saarinen, IEEE Trans. Neural Netw. Learn. Syst. 279O. J. R\u20ac as\u20ac anen and J. P. Saarinen, \"Sequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns,\" IEEE Trans. Neural Netw. Learn. Syst., vol. 27, no. 9, pp. 1878-1889, Sep. 2016.\n\nVoiceHD: Hyperdimensional computing for efficient speech recognition. M Imani, D Kong, A Rahimi, T Rosing, Proc. IEEE Int. Conf. Rebooting Comput. IEEE Int. Conf. Rebooting ComputM. Imani, D. Kong, A. Rahimi, and T. Rosing, \"VoiceHD: Hyper- dimensional computing for efficient speech recognition,\" in Proc. IEEE Int. Conf. Rebooting Comput., 2017, pp. 1-6.\n\nEfficient human activity recognition using hyperdimensional computing. Y Kim, Proc. 8th Int. Conf. Internet of Things. 8th Int. Conf. Internet of ThingsY. Kim et al., \"Efficient human activity recognition using hyperdi- mensional computing,\" in Proc. 8th Int. Conf. Internet of Things, 2018, Art. no. 38.\n\nHDNA: Energyefficient DNA sequencing using hyperdimensional computing. M Imani, T Nassar, A Rahimi, T Rosing, Proc. IEEE EMBS Int. IEEE EMBS IntM. Imani, T. Nassar, A. Rahimi, and T. Rosing, \"HDNA: Energy- efficient DNA sequencing using hyperdimensional computing,\" in Proc. IEEE EMBS Int. Conf. Biomed. Health Inform., 2018, pp. 271-274.\n\nA memory-centric acceleration of clustering using high-dimensional vectors. M Imani, T Nassar, A Rahimi, T Rosing, Proc. Des. Autom. Test Eur. Conf. Exhib. Des. Autom. Test Eur. Conf. ExhibM. Imani, T. Nassar, A. Rahimi, and T. Rosing, \"A memory-centric acceleration of clustering using high-dimensional vectors,\" in Proc. Des. Autom. Test Eur. Conf. Exhib., 2019.\n\nLearning both weights and connections for efficient neural network. S Han, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. SystS. Han et al., \"Learning both weights and connections for efficient neural network,\" in Proc. Int. Conf. Neural Inf. Process. Syst., 2015, pp. 1135-1143.\n\nNeST: A neural network synthesis tool based on a grow-and-prune paradigm. X Dai, H Yin, N Jha, IEEE Trans. Comput. 6810X. Dai, H. Yin, and N. Jha, \"NeST: A neural network synthesis tool based on a grow-and-prune paradigm,\" IEEE Trans. Comput., vol. 68, no. 10, pp. 1487-1497, Oct. 2019.\n\nA scalable near-memory architecture for training deep neural networks on large in-memory datasets. F Schuiki, M Schaffner, F K , L Benini, IEEE Trans. Comput. 684F. Schuiki, M. Schaffner, F. K. G\u20ac urkaynak, and L. Benini, \"A scal- able near-memory architecture for training deep neural networks on large in-memory datasets,\" IEEE Trans. Comput., vol. 68, no. 4, pp. 484-497, Apr. 2019.\n\nSearcHD: A memory-centric hyperdimensional computing with stochastic training. M Imani, 10.1109/TCAD.2019.2952544doi: 10.1109/ TCAD.2019.2952544IEEE Trans. Comput.-Aided Design Integr. Circuits Syst. to be publishedM. Imani et al., \"SearcHD: A memory-centric hyperdimensional computing with stochastic training,\" IEEE Trans. Comput.-Aided Design Integr. Circuits Syst., to be published, doi: 10.1109/ TCAD.2019.2952544.\n\nWorkloadaware opportunistic energy efficiency in multi-FPGA platforms. S Salamat, B Khaleghi, M Imani, T Rosing, Proc. IEEE/ACM Int. Conf. Comput.-Aided Des. IEEE/ACM Int. Conf. Comput.-Aided DesS. Salamat, B. Khaleghi, M. Imani, and T. Rosing, \"Workload- aware opportunistic energy efficiency in multi-FPGA platforms,\" in Proc. IEEE/ACM Int. Conf. Comput.-Aided Des., 2019, pp. 1-8.\n\nFPGA energy efficiency by leveraging thermal margin. B Khaleghi, S Salamat, M Imani, T Rosing, Proc. IEEE 37th Int. Conf. Comput. Des. IEEE 37th Int. Conf. Comput. DesB. Khaleghi, S. Salamat, M. Imani, and T. Rosing, \"FPGA energy efficiency by leveraging thermal margin,\" in Proc. IEEE 37th Int. Conf. Comput. Des., 2019, pp. 376-384.\n\nFACH: FPGA-based acceleration of hyperdimensional computing by reducing computational complexity. M Imani, S Salamat, S Gupta, J Huang, T Rosing, Proc. 24th Asia South Pacific Des. Autom. Conf. 24th Asia South Pacific Des. Autom. ConfM. Imani, S. Salamat, S. Gupta, J. Huang, and T. Rosing, \"FACH: FPGA-based acceleration of hyperdimensional computing by reducing computational complexity,\" in Proc. 24th Asia South Pacific Des. Autom. Conf., 2019, pp. 493-498.\n\nA robust and energyefficient classifier using brain-inspired hyperdimensional computing. A Rahimi, P Kanerva, J M Rabaey, Proc. Int. Symp. Low Power Electron. Des. Int. Symp. Low Power Electron. DesA. Rahimi, P. Kanerva, and J. M. Rabaey, \"A robust and energy- efficient classifier using brain-inspired hyperdimensional computing,\" in Proc. Int. Symp. Low Power Electron. Des., 2016, pp. 64-69.\n\nHierarchical hyperdimensional computing for energy efficient classification. M Imani, Proc. 55th Annu. 55th AnnuM. Imani et al., \"Hierarchical hyperdimensional computing for energy efficient classification,\" in Proc. 55th Annu. Des. Autom. Conf., 2018, Art. no. 108.\n\nQuantHD: A quantization framework for hyperdimensional computing. M Imani, 10.1109/TCAD.2019.2954472IEEE Trans. Comput.-Aided Design Integr. Circuits Syst. to be publishedM. Imani et al., \"QuantHD: A quantization framework for hyper- dimensional computing,\" IEEE Trans. Comput.-Aided Design Integr. Circuits Syst., to be published, doi: 10.1109/TCAD.2019.2954472.\n\nExploring hyperdimensional associative memory. M Imani, A Rahimi, D Kong, T Rosing, J M Rabaey, Proc. IEEE Int. Symp. High-Perform. Comput. Archit. IEEE Int. Symp. High-Perform. Comput. ArchitM. Imani, A. Rahimi, D. Kong, T. Rosing, and J. M. Rabaey, \"Exploring hyperdimensional associative memory,\" in Proc. IEEE Int. Symp. High-Perform. Comput. Archit., 2017, pp. 445-456.\n\nHigh-dimensional computing as a nanoscalable paradigm. A Rahimi, IEEE Trans. Circuits Syst. I, Reg. Papers. 649A. Rahimi et al., \"High-dimensional computing as a nanoscalable paradigm,\" IEEE Trans. Circuits Syst. I, Reg. Papers, vol. 64, no. 9, pp. 2508-2521, Sep. 2017.\n\nBrain-inspired computing exploiting carbon nanotube FETs and resistive RAM: Hyperdimensional computing case study. T F Wu, Proc. IEEE Int. Solid-State Circuits Conf. IEEE Int. Solid-State Circuits ConfT. F. Wu et al., \"Brain-inspired computing exploiting carbon nanotube FETs and resistive RAM: Hyperdimensional comput- ing case study,\" in Proc. IEEE Int. Solid-State Circuits Conf., 2018, pp. 492-494.\n\nHyperdimensional computing with 3D VRRAM inmemory kernels: Device-architecture co-design for energyefficient, error-resilient language recognition. H Li, Proc. IEEE Int. Electron Devices Meeting. IEEE Int. Electron Devices MeetingH. Li et al., \"Hyperdimensional computing with 3D VRRAM in- memory kernels: Device-architecture co-design for energy- efficient, error-resilient language recognition,\" in Proc. IEEE Int. Electron Devices Meeting, 2016, pp. 16-1.\n\nFELIX: Fast and energy-efficient logic in memory. S Gupta, M Imani, T Rosing, Proc. nullS. Gupta, M. Imani, and T. Rosing, \"FELIX: Fast and energy-effi- cient logic in memory,\" in Proc. IEEE/ACM Int. Conf. Comput.- Aided Des., 2018, Art. no. 55.\n\nA binary learning framework for hyperdimensional computing. M Imani, J Messerly, F Wu, W Pi, T Rosing, Proc. Des. Autom. Test Eur. Conf. Exhib. Des. Autom. Test Eur. Conf. ExhibM. Imani, J. Messerly, F. Wu, W. Pi, and T. Rosing, \"A binary learning framework for hyperdimensional computing,\" in Proc. Des. Autom. Test Eur. Conf. Exhib., 2019, pp. 126-131.\n\nA programmable hyper-dimensional processor architecture for human-centric IoT. S Datta, R A Antonio, A R Ison, J M Rabaey, IEEE Trans. Emerg. Sel. Topics Circuits Syst. 93S. Datta, R. A. Antonio, A. R. Ison, and J. M. Rabaey, \"A program- mable hyper-dimensional processor architecture for human-cen- tric IoT,\" IEEE Trans. Emerg. Sel. Topics Circuits Syst., vol. 9, no. 3, pp. 439-452, Sep. 2019.\n\nHardware optimizations of dense binary hyperdimensional computing: Rematerialization of hypervectors, binarized bundling, and combinational associative memory. S Manuel, L Benini, A Rahimi, ACM J. Emerg. Technol. Comput. Syst. 1532S. Manuel, L. Benini, and A. Rahimi, \"Hardware optimizations of dense binary hyperdimensional computing: Rematerialization of hypervectors, binarized bundling, and combinational associative memory,\" ACM J. Emerg. Technol. Comput. Syst., vol. 15, 2019, Art. no. 32.\n\nSparseHD: Algorithm-hardware co-optimization for efficient high-dimensional computing. M Imani, S Salamat, B Khaleghi, M Samragh, F Koushanfar, T Rosing, Proc. ACM/SIGDA Int. Symp. Field-Programmable Gate Arrays. ACM/SIGDA Int. Symp. Field-Programmable Gate ArraysM. Imani, S. Salamat, B. Khaleghi, M. Samragh, F. Koushanfar, and T. Rosing, \"SparseHD: Algorithm-hardware co-optimization for efficient high-dimensional computing,\" in Proc. ACM/SIGDA Int. Symp. Field-Programmable Gate Arrays, 2019, pp. 53-62.\n\nEfficient associative search in brain-inspired hyperdimensional computing. M Imani, J Morris, H Shu, S Li, T Rosing, IEEE Des. Test. 371M. Imani, J. Morris, H. Shu, S. Li, and T. Rosing, \"Efficient associa- tive search in brain-inspired hyperdimensional computing,\" IEEE Des. Test, vol. 37, no. 1, pp. 28-35, Feb. 2020.\n\nSparseHD: Algorithm-hardware co-optimization for efficient high-dimensional computing. M Imani, S Salamat, B Khaleghi, M Samragh, F Koushanfar, T Rosing, Proc. IEEE 27th. IEEE 27thM. Imani, S. Salamat, B. Khaleghi, M. Samragh, F. Koushanfar, and T. Rosing, \"SparseHD: Algorithm-hardware co-optimization for efficient high-dimensional computing,\" in Proc. IEEE 27th\n\n. Annu. Int. Symp. Field-Programmable Custom Comput. Mach. Annu. Int. Symp. Field-Programmable Custom Comput. Mach., 2019, pp. 190-198.\n\nUCI machine learning repository. UCI machine learning repository, Accessed: May 2020. [Online].\n\nCaltech-256 object category dataset. G Griffin, A Holub, P Perona, California Institute of TechnologyG. Griffin, A. Holub, and P. Perona, \"Caltech-256 object category dataset,\" California Institute of Technology, 2007.\n\nScikit-learn: Machine learning in Python. F Pedregosa, J. Mach. Learn. Res. 12F. Pedregosa et al., \"Scikit-learn: Machine learning in Python,\" J. Mach. Learn. Res., vol. 12, pp. 2825-2830, 2011.\n\nVivado design suite. T Feist, White Paper. 5T. Feist, \"Vivado design suite,\" White Paper, vol. 5, 2012.\n\nThe pascal visual object classes challenge: A retrospective. M Everinghametal, Int. J. Comput. Vis. 1111M. Everinghametal ., \"The pascal visual object classes challenge: A retrospective,\" Int. J. Comput. Vis., vol. 111, no. 1, pp. 98-136, 2015.\n\nUCI machine learning repository. UCI machine learning repository, Accessed: May 2020. [Online].\n\nCreating and benchmarking a new dataset for physical activity monitoring. A Reiss, Proc. 5th Int. Conf. Pervasive Technol. Related Assistive Environ. 5th Int. Conf. Pervasive Technol. Related Assistive EnvironA. Reiss et al., \"Creating and benchmarking a new dataset for physical activity monitoring,\" in Proc. 5th Int. Conf. Pervasive Technol. Related Assistive Environ., 2012, Art. no. 40.\n\nHe is a member of System Energy Efficiency Laboratory (SEELab). He is working on accelerating bigdata applications (machine learning. Sahand Salamat, Student Member, IEEE) received the BSc degree in electrical and computer engineering from the University of Tehran. Tehran, Iran; San Diego, CaliforniaComputer Science and Engineering, University of California San DiegoHe is currently working toward the PhD degree with the Department of. and bioinformatic) on FPGAsSahand Salamat (Student Member, IEEE) received the BSc degree in electrical and com- puter engineering from the University of Tehran, Tehran, Iran. He is currently working toward the PhD degree with the Department of Computer Science and Engineering, University of California San Diego, San Diego, California, since 2017. He is a member of System Energy Efficiency Labora- tory (SEELab). He is working on accelerating big- data applications (machine learning, database, and bioinformatic) on FPGAs.\n", "annotations": {"author": "[{\"start\":\"86\",\"end\":\"135\"},{\"start\":\"136\",\"end\":\"170\"}]", "publisher": null, "author_last_name": "[{\"start\":\"127\",\"end\":\"134\"},{\"start\":\"163\",\"end\":\"169\"}]", "author_first_name": "[{\"start\":\"120\",\"end\":\"126\"},{\"start\":\"156\",\"end\":\"162\"}]", "author_affiliation": null, "title": "[{\"start\":\"1\",\"end\":\"83\"},{\"start\":\"171\",\"end\":\"253\"}]", "venue": null, "abstract": "[{\"start\":\"377\",\"end\":\"2144\"}]", "bib_ref": "[{\"start\":\"2331\",\"end\":\"2334\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"2336\",\"end\":\"2339\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"2341\",\"end\":\"2344\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"2346\",\"end\":\"2349\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"2522\",\"end\":\"2525\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"2527\",\"end\":\"2530\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"2532\",\"end\":\"2535\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"2690\",\"end\":\"2693\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"2695\",\"end\":\"2698\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"2700\",\"end\":\"2704\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"2706\",\"end\":\"2710\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"2915\",\"end\":\"2919\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"2921\",\"end\":\"2925\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"3044\",\"end\":\"3048\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"3552\",\"end\":\"3556\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"3583\",\"end\":\"3587\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"3610\",\"end\":\"3614\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"3657\",\"end\":\"3661\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"3682\",\"end\":\"3686\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"3709\",\"end\":\"3713\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"3730\",\"end\":\"3734\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"3751\",\"end\":\"3755\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"3896\",\"end\":\"3900\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"3902\",\"end\":\"3906\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"3908\",\"end\":\"3912\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"4054\",\"end\":\"4058\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"6640\",\"end\":\"6644\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"7833\",\"end\":\"7837\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"7866\",\"end\":\"7870\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"7872\",\"end\":\"7876\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"8349\",\"end\":\"8353\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"8816\",\"end\":\"8820\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"9236\",\"end\":\"9240\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"10755\",\"end\":\"10759\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"10761\",\"end\":\"10765\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"10767\",\"end\":\"10771\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"10814\",\"end\":\"10818\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"11084\",\"end\":\"11088\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"11090\",\"end\":\"11094\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"14286\",\"end\":\"14290\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"14292\",\"end\":\"14296\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"15319\",\"end\":\"15323\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"15325\",\"end\":\"15329\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"15459\",\"end\":\"15463\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"15692\",\"end\":\"15696\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"15698\",\"end\":\"15702\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"15704\",\"end\":\"15708\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"15710\",\"end\":\"15714\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"15770\",\"end\":\"15774\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"15776\",\"end\":\"15780\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"15848\",\"end\":\"15852\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"16015\",\"end\":\"16019\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"16672\",\"end\":\"16676\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"16678\",\"end\":\"16682\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"16684\",\"end\":\"16688\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"16690\",\"end\":\"16694\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"16746\",\"end\":\"16750\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"17256\",\"end\":\"17260\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"17335\",\"end\":\"17339\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"17769\",\"end\":\"17773\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"18322\",\"end\":\"18326\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"18328\",\"end\":\"18332\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"18334\",\"end\":\"18338\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"18388\",\"end\":\"18392\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"18394\",\"end\":\"18398\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"18400\",\"end\":\"18404\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"19957\",\"end\":\"19961\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"19988\",\"end\":\"19992\",\"attributes\":{\"ref_id\":\"b46\"}},{\"start\":\"34262\",\"end\":\"34266\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"37682\",\"end\":\"37685\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"41794\",\"end\":\"41798\",\"attributes\":{\"ref_id\":\"b47\"}},{\"start\":\"42240\",\"end\":\"42244\",\"attributes\":{\"ref_id\":\"b48\"}},{\"start\":\"42993\",\"end\":\"42997\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"43320\",\"end\":\"43324\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"43399\",\"end\":\"43403\",\"attributes\":{\"ref_id\":\"b46\"}},{\"start\":\"43510\",\"end\":\"43514\",\"attributes\":{\"ref_id\":\"b49\"}},{\"start\":\"43547\",\"end\":\"43551\",\"attributes\":{\"ref_id\":\"b50\"}},{\"start\":\"43762\",\"end\":\"43766\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"55235\",\"end\":\"55239\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"55410\",\"end\":\"55414\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"58353\",\"end\":\"58357\",\"attributes\":{\"ref_id\":\"b13\"}}]", "figure": "[{\"start\":\"56994\",\"end\":\"57101\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"57102\",\"end\":\"57123\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"57124\",\"end\":\"57189\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"57190\",\"end\":\"57280\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"57281\",\"end\":\"57322\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"57323\",\"end\":\"57499\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"57500\",\"end\":\"57672\",\"attributes\":{\"id\":\"fig_6\"}},{\"start\":\"57673\",\"end\":\"57835\",\"attributes\":{\"id\":\"fig_7\"}},{\"start\":\"57836\",\"end\":\"58020\",\"attributes\":{\"id\":\"fig_8\"}},{\"start\":\"58021\",\"end\":\"58241\",\"attributes\":{\"id\":\"fig_9\"}},{\"start\":\"58242\",\"end\":\"58358\",\"attributes\":{\"id\":\"fig_10\"}},{\"start\":\"58359\",\"end\":\"58749\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}},{\"start\":\"58750\",\"end\":\"59673\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}},{\"start\":\"59674\",\"end\":\"59928\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"2164\",\"end\":\"3160\"},{\"start\":\"3162\",\"end\":\"4059\"},{\"start\":\"4061\",\"end\":\"5339\"},{\"start\":\"5341\",\"end\":\"6313\"},{\"start\":\"6315\",\"end\":\"7475\"},{\"start\":\"7477\",\"end\":\"8502\"},{\"start\":\"8504\",\"end\":\"9241\"},{\"start\":\"9298\",\"end\":\"10235\"},{\"start\":\"10248\",\"end\":\"11917\"},{\"start\":\"11919\",\"end\":\"12538\"},{\"start\":\"12540\",\"end\":\"13133\"},{\"start\":\"13155\",\"end\":\"13376\"},{\"start\":\"13400\",\"end\":\"13826\"},{\"start\":\"13883\",\"end\":\"14439\"},{\"start\":\"14462\",\"end\":\"15464\"},{\"start\":\"15490\",\"end\":\"16200\"},{\"start\":\"16202\",\"end\":\"18587\"},{\"start\":\"18608\",\"end\":\"18709\"},{\"start\":\"18742\",\"end\":\"19457\"},{\"start\":\"19459\",\"end\":\"20623\"},{\"start\":\"20625\",\"end\":\"22324\"},{\"start\":\"22345\",\"end\":\"23337\"},{\"start\":\"23339\",\"end\":\"24249\"},{\"start\":\"24338\",\"end\":\"25380\"},{\"start\":\"25445\",\"end\":\"27193\"},{\"start\":\"27224\",\"end\":\"27851\"},{\"start\":\"27853\",\"end\":\"28486\"},{\"start\":\"28488\",\"end\":\"28949\"},{\"start\":\"28951\",\"end\":\"29238\"},{\"start\":\"29240\",\"end\":\"29317\"},{\"start\":\"29398\",\"end\":\"30272\"},{\"start\":\"30328\",\"end\":\"31506\"},{\"start\":\"31508\",\"end\":\"32150\"},{\"start\":\"32183\",\"end\":\"34092\"},{\"start\":\"34122\",\"end\":\"35065\"},{\"start\":\"35099\",\"end\":\"37845\"},{\"start\":\"37915\",\"end\":\"38218\"},{\"start\":\"38220\",\"end\":\"38343\"},{\"start\":\"38345\",\"end\":\"39637\"},{\"start\":\"39639\",\"end\":\"40618\"},{\"start\":\"40620\",\"end\":\"41336\"},{\"start\":\"41369\",\"end\":\"43019\"},{\"start\":\"43033\",\"end\":\"43145\"},{\"start\":\"43147\",\"end\":\"43325\"},{\"start\":\"43327\",\"end\":\"43515\"},{\"start\":\"43517\",\"end\":\"43723\"},{\"start\":\"43725\",\"end\":\"44066\"},{\"start\":\"44087\",\"end\":\"45668\"},{\"start\":\"45670\",\"end\":\"47003\"},{\"start\":\"47005\",\"end\":\"47899\"},{\"start\":\"47901\",\"end\":\"51445\"},{\"start\":\"51487\",\"end\":\"54332\"},{\"start\":\"54378\",\"end\":\"56145\"},{\"start\":\"56147\",\"end\":\"56993\"}]", "formula": "[{\"start\":\"13377\",\"end\":\"13399\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"13827\",\"end\":\"13882\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"18710\",\"end\":\"18741\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"24250\",\"end\":\"24337\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"25381\",\"end\":\"25444\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"29318\",\"end\":\"29397\",\"attributes\":{\"id\":\"formula_6\"}},{\"start\":\"30273\",\"end\":\"30327\",\"attributes\":{\"id\":\"formula_7\"}},{\"start\":\"32151\",\"end\":\"32182\",\"attributes\":{\"id\":\"formula_8\"}},{\"start\":\"37846\",\"end\":\"37914\",\"attributes\":{\"id\":\"formula_9\"}}]", "table_ref": "[{\"start\":\"39599\",\"end\":\"39606\"},{\"start\":\"45959\",\"end\":\"45966\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"46119\",\"end\":\"46126\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"46995\",\"end\":\"47002\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"48925\",\"end\":\"48932\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"52637\",\"end\":\"52644\",\"attributes\":{\"ref_id\":\"tab_2\"}}]", "section_header": "[{\"start\":\"2146\",\"end\":\"2147\"},{\"start\":\"2150\",\"end\":\"2162\",\"attributes\":{\"n\":\"1\"}},{\"start\":\"9244\",\"end\":\"9271\",\"attributes\":{\"n\":\"2\"}},{\"start\":\"9274\",\"end\":\"9296\",\"attributes\":{\"n\":\"2.1\"}},{\"start\":\"10238\",\"end\":\"10246\",\"attributes\":{\"n\":\"2.2\"}},{\"start\":\"13136\",\"end\":\"13153\",\"attributes\":{\"n\":\"2.3\"}},{\"start\":\"14442\",\"end\":\"14460\",\"attributes\":{\"n\":\"2.4\"}},{\"start\":\"15467\",\"end\":\"15488\",\"attributes\":{\"n\":\"2.5\"}},{\"start\":\"18590\",\"end\":\"18606\",\"attributes\":{\"n\":\"3\"}},{\"start\":\"22327\",\"end\":\"22343\",\"attributes\":{\"n\":\"3.1\"}},{\"start\":\"27196\",\"end\":\"27222\",\"attributes\":{\"n\":\"3.2\"}},{\"start\":\"34095\",\"end\":\"34120\",\"attributes\":{\"n\":\"4\"}},{\"start\":\"35068\",\"end\":\"35097\",\"attributes\":{\"n\":\"4.1\"}},{\"start\":\"41339\",\"end\":\"41346\",\"attributes\":{\"n\":\"5\"}},{\"start\":\"41349\",\"end\":\"41367\",\"attributes\":{\"n\":\"5.1\"}},{\"start\":\"43022\",\"end\":\"43031\",\"attributes\":{\"n\":\"5.2\"}},{\"start\":\"44069\",\"end\":\"44085\",\"attributes\":{\"n\":\"5.3\"}},{\"start\":\"51448\",\"end\":\"51485\",\"attributes\":{\"n\":\"5.4\"}},{\"start\":\"54335\",\"end\":\"54363\",\"attributes\":{\"n\":\"5.5\"}},{\"start\":\"54366\",\"end\":\"54376\",\"attributes\":{\"n\":\"6\"}},{\"start\":\"56995\",\"end\":\"57003\"},{\"start\":\"57125\",\"end\":\"57133\"},{\"start\":\"57191\",\"end\":\"57199\"},{\"start\":\"57282\",\"end\":\"57290\"},{\"start\":\"57324\",\"end\":\"57332\"},{\"start\":\"57501\",\"end\":\"57509\"},{\"start\":\"57674\",\"end\":\"57682\"},{\"start\":\"57837\",\"end\":\"57845\"},{\"start\":\"58022\",\"end\":\"58030\"},{\"start\":\"58243\",\"end\":\"58252\"},{\"start\":\"58360\",\"end\":\"58367\"},{\"start\":\"58751\",\"end\":\"58758\"},{\"start\":\"59675\",\"end\":\"59682\"}]", "table": "[{\"start\":\"58477\",\"end\":\"58749\"},{\"start\":\"58932\",\"end\":\"59673\"}]", "figure_caption": "[{\"start\":\"57005\",\"end\":\"57101\"},{\"start\":\"57104\",\"end\":\"57123\"},{\"start\":\"57135\",\"end\":\"57189\"},{\"start\":\"57201\",\"end\":\"57280\"},{\"start\":\"57292\",\"end\":\"57322\"},{\"start\":\"57334\",\"end\":\"57499\"},{\"start\":\"57511\",\"end\":\"57672\"},{\"start\":\"57684\",\"end\":\"57835\"},{\"start\":\"57847\",\"end\":\"58020\"},{\"start\":\"58032\",\"end\":\"58241\"},{\"start\":\"58255\",\"end\":\"58358\"},{\"start\":\"58369\",\"end\":\"58477\"},{\"start\":\"58760\",\"end\":\"58932\"},{\"start\":\"59684\",\"end\":\"59928\"}]", "figure_ref": "[{\"start\":\"4114\",\"end\":\"4120\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"5896\",\"end\":\"5902\",\"attributes\":{\"ref_id\":\"fig_9\"}},{\"start\":\"9463\",\"end\":\"9469\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"14538\",\"end\":\"14544\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"19853\",\"end\":\"19867\"},{\"start\":\"20845\",\"end\":\"20851\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"30479\",\"end\":\"30485\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"30722\",\"end\":\"30731\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"30911\",\"end\":\"30920\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"32889\",\"end\":\"32896\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"33513\",\"end\":\"33520\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"34304\",\"end\":\"34311\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"37959\",\"end\":\"37965\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"38484\",\"end\":\"38491\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"44260\",\"end\":\"44266\",\"attributes\":{\"ref_id\":\"fig_8\"}},{\"start\":\"46593\",\"end\":\"46599\",\"attributes\":{\"ref_id\":\"fig_9\"}},{\"start\":\"47165\",\"end\":\"47172\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"47393\",\"end\":\"47400\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"48110\",\"end\":\"48117\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"48614\",\"end\":\"48621\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"50589\",\"end\":\"50596\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"51874\",\"end\":\"51881\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"51990\",\"end\":\"51997\",\"attributes\":{\"ref_id\":\"fig_0\"}}]", "bib_author_first_name": "[{\"start\":\"60133\",\"end\":\"60134\"},{\"start\":\"60140\",\"end\":\"60141\"},{\"start\":\"60142\",\"end\":\"60143\"},{\"start\":\"60468\",\"end\":\"60472\"},{\"start\":\"60731\",\"end\":\"60732\"},{\"start\":\"60991\",\"end\":\"60992\"},{\"start\":\"61002\",\"end\":\"61006\"},{\"start\":\"61014\",\"end\":\"61015\"},{\"start\":\"61262\",\"end\":\"61263\"},{\"start\":\"61478\",\"end\":\"61479\"},{\"start\":\"61485\",\"end\":\"61486\"},{\"start\":\"61493\",\"end\":\"61494\"},{\"start\":\"61505\",\"end\":\"61506\"},{\"start\":\"61778\",\"end\":\"61779\"},{\"start\":\"61789\",\"end\":\"61790\"},{\"start\":\"61798\",\"end\":\"61799\"},{\"start\":\"61807\",\"end\":\"61808\"},{\"start\":\"62156\",\"end\":\"62157\"},{\"start\":\"62158\",\"end\":\"62159\"},{\"start\":\"62169\",\"end\":\"62170\"},{\"start\":\"62171\",\"end\":\"62172\"},{\"start\":\"62182\",\"end\":\"62183\"},{\"start\":\"62190\",\"end\":\"62191\"},{\"start\":\"62192\",\"end\":\"62193\"},{\"start\":\"62523\",\"end\":\"62524\"},{\"start\":\"62530\",\"end\":\"62531\"},{\"start\":\"62538\",\"end\":\"62539\"},{\"start\":\"62555\",\"end\":\"62556\"},{\"start\":\"62868\",\"end\":\"62869\"},{\"start\":\"62880\",\"end\":\"62881\"},{\"start\":\"62889\",\"end\":\"62890\"},{\"start\":\"62891\",\"end\":\"62892\"},{\"start\":\"63146\",\"end\":\"63147\"},{\"start\":\"63154\",\"end\":\"63155\"},{\"start\":\"63169\",\"end\":\"63170\"},{\"start\":\"63175\",\"end\":\"63176\"},{\"start\":\"63183\",\"end\":\"63184\"},{\"start\":\"63539\",\"end\":\"63540\"},{\"start\":\"63548\",\"end\":\"63549\"},{\"start\":\"63558\",\"end\":\"63559\"},{\"start\":\"63570\",\"end\":\"63571\"},{\"start\":\"63577\",\"end\":\"63578\"},{\"start\":\"63585\",\"end\":\"63586\"},{\"start\":\"64141\",\"end\":\"64142\"},{\"start\":\"64152\",\"end\":\"64153\"},{\"start\":\"64161\",\"end\":\"64162\"},{\"start\":\"64173\",\"end\":\"64174\"},{\"start\":\"64635\",\"end\":\"64636\"},{\"start\":\"64949\",\"end\":\"64950\"},{\"start\":\"65308\",\"end\":\"65309\"},{\"start\":\"65539\",\"end\":\"65540\"},{\"start\":\"65548\",\"end\":\"65549\"},{\"start\":\"65550\",\"end\":\"65551\"},{\"start\":\"65561\",\"end\":\"65562\"},{\"start\":\"65899\",\"end\":\"65900\"},{\"start\":\"65901\",\"end\":\"65902\"},{\"start\":\"65905\",\"end\":\"65906\"},{\"start\":\"65907\",\"end\":\"65908\"},{\"start\":\"66274\",\"end\":\"66275\"},{\"start\":\"66283\",\"end\":\"66284\"},{\"start\":\"66291\",\"end\":\"66292\"},{\"start\":\"66301\",\"end\":\"66302\"},{\"start\":\"66633\",\"end\":\"66634\"},{\"start\":\"66939\",\"end\":\"66940\"},{\"start\":\"66948\",\"end\":\"66949\"},{\"start\":\"66958\",\"end\":\"66959\"},{\"start\":\"66968\",\"end\":\"66969\"},{\"start\":\"67284\",\"end\":\"67285\"},{\"start\":\"67293\",\"end\":\"67294\"},{\"start\":\"67303\",\"end\":\"67304\"},{\"start\":\"67313\",\"end\":\"67314\"},{\"start\":\"67642\",\"end\":\"67643\"},{\"start\":\"67954\",\"end\":\"67955\"},{\"start\":\"67961\",\"end\":\"67962\"},{\"start\":\"67968\",\"end\":\"67969\"},{\"start\":\"68267\",\"end\":\"68268\"},{\"start\":\"68278\",\"end\":\"68279\"},{\"start\":\"68291\",\"end\":\"68292\"},{\"start\":\"68293\",\"end\":\"68294\"},{\"start\":\"68297\",\"end\":\"68298\"},{\"start\":\"68634\",\"end\":\"68635\"},{\"start\":\"69047\",\"end\":\"69048\"},{\"start\":\"69058\",\"end\":\"69059\"},{\"start\":\"69070\",\"end\":\"69071\"},{\"start\":\"69079\",\"end\":\"69080\"},{\"start\":\"69414\",\"end\":\"69415\"},{\"start\":\"69426\",\"end\":\"69427\"},{\"start\":\"69437\",\"end\":\"69438\"},{\"start\":\"69446\",\"end\":\"69447\"},{\"start\":\"69795\",\"end\":\"69796\"},{\"start\":\"69804\",\"end\":\"69805\"},{\"start\":\"69815\",\"end\":\"69816\"},{\"start\":\"69824\",\"end\":\"69825\"},{\"start\":\"69833\",\"end\":\"69834\"},{\"start\":\"70249\",\"end\":\"70250\"},{\"start\":\"70259\",\"end\":\"70260\"},{\"start\":\"70270\",\"end\":\"70271\"},{\"start\":\"70272\",\"end\":\"70273\"},{\"start\":\"70633\",\"end\":\"70634\"},{\"start\":\"70890\",\"end\":\"70891\"},{\"start\":\"71236\",\"end\":\"71237\"},{\"start\":\"71245\",\"end\":\"71246\"},{\"start\":\"71255\",\"end\":\"71256\"},{\"start\":\"71263\",\"end\":\"71264\"},{\"start\":\"71273\",\"end\":\"71274\"},{\"start\":\"71275\",\"end\":\"71276\"},{\"start\":\"71620\",\"end\":\"71621\"},{\"start\":\"71952\",\"end\":\"71953\"},{\"start\":\"71954\",\"end\":\"71955\"},{\"start\":\"72389\",\"end\":\"72390\"},{\"start\":\"72751\",\"end\":\"72752\"},{\"start\":\"72760\",\"end\":\"72761\"},{\"start\":\"72769\",\"end\":\"72770\"},{\"start\":\"73008\",\"end\":\"73009\"},{\"start\":\"73017\",\"end\":\"73018\"},{\"start\":\"73029\",\"end\":\"73030\"},{\"start\":\"73035\",\"end\":\"73036\"},{\"start\":\"73041\",\"end\":\"73042\"},{\"start\":\"73383\",\"end\":\"73384\"},{\"start\":\"73392\",\"end\":\"73393\"},{\"start\":\"73394\",\"end\":\"73395\"},{\"start\":\"73405\",\"end\":\"73406\"},{\"start\":\"73407\",\"end\":\"73408\"},{\"start\":\"73415\",\"end\":\"73416\"},{\"start\":\"73417\",\"end\":\"73418\"},{\"start\":\"73862\",\"end\":\"73863\"},{\"start\":\"73872\",\"end\":\"73873\"},{\"start\":\"73882\",\"end\":\"73883\"},{\"start\":\"74286\",\"end\":\"74287\"},{\"start\":\"74295\",\"end\":\"74296\"},{\"start\":\"74306\",\"end\":\"74307\"},{\"start\":\"74318\",\"end\":\"74319\"},{\"start\":\"74329\",\"end\":\"74330\"},{\"start\":\"74343\",\"end\":\"74344\"},{\"start\":\"74784\",\"end\":\"74785\"},{\"start\":\"74793\",\"end\":\"74794\"},{\"start\":\"74803\",\"end\":\"74804\"},{\"start\":\"74810\",\"end\":\"74811\"},{\"start\":\"74816\",\"end\":\"74817\"},{\"start\":\"75117\",\"end\":\"75118\"},{\"start\":\"75126\",\"end\":\"75127\"},{\"start\":\"75137\",\"end\":\"75138\"},{\"start\":\"75149\",\"end\":\"75150\"},{\"start\":\"75160\",\"end\":\"75161\"},{\"start\":\"75174\",\"end\":\"75175\"},{\"start\":\"75667\",\"end\":\"75668\"},{\"start\":\"75678\",\"end\":\"75679\"},{\"start\":\"75687\",\"end\":\"75688\"},{\"start\":\"75892\",\"end\":\"75893\"},{\"start\":\"76067\",\"end\":\"76068\"},{\"start\":\"76212\",\"end\":\"76213\"},{\"start\":\"76568\",\"end\":\"76569\"},{\"start\":\"77021\",\"end\":\"77027\"}]", "bib_author_last_name": "[{\"start\":\"60135\",\"end\":\"60138\"},{\"start\":\"60144\",\"end\":\"60150\"},{\"start\":\"60473\",\"end\":\"60477\"},{\"start\":\"60733\",\"end\":\"60741\"},{\"start\":\"60993\",\"end\":\"61000\"},{\"start\":\"61007\",\"end\":\"61012\"},{\"start\":\"61016\",\"end\":\"61020\"},{\"start\":\"61264\",\"end\":\"61267\"},{\"start\":\"61480\",\"end\":\"61483\"},{\"start\":\"61487\",\"end\":\"61491\"},{\"start\":\"61495\",\"end\":\"61503\"},{\"start\":\"61507\",\"end\":\"61510\"},{\"start\":\"61780\",\"end\":\"61787\"},{\"start\":\"61791\",\"end\":\"61796\"},{\"start\":\"61800\",\"end\":\"61805\"},{\"start\":\"61809\",\"end\":\"61815\"},{\"start\":\"62160\",\"end\":\"62167\"},{\"start\":\"62173\",\"end\":\"62180\"},{\"start\":\"62184\",\"end\":\"62188\"},{\"start\":\"62194\",\"end\":\"62202\"},{\"start\":\"62525\",\"end\":\"62528\"},{\"start\":\"62532\",\"end\":\"62536\"},{\"start\":\"62540\",\"end\":\"62553\"},{\"start\":\"62557\",\"end\":\"62568\"},{\"start\":\"62870\",\"end\":\"62878\"},{\"start\":\"62882\",\"end\":\"62887\"},{\"start\":\"62893\",\"end\":\"62898\"},{\"start\":\"63148\",\"end\":\"63152\"},{\"start\":\"63156\",\"end\":\"63167\"},{\"start\":\"63171\",\"end\":\"63173\"},{\"start\":\"63177\",\"end\":\"63181\"},{\"start\":\"63185\",\"end\":\"63189\"},{\"start\":\"63541\",\"end\":\"63546\"},{\"start\":\"63550\",\"end\":\"63556\"},{\"start\":\"63560\",\"end\":\"63568\"},{\"start\":\"63572\",\"end\":\"63575\"},{\"start\":\"63579\",\"end\":\"63583\"},{\"start\":\"63587\",\"end\":\"63593\"},{\"start\":\"64143\",\"end\":\"64150\"},{\"start\":\"64154\",\"end\":\"64159\"},{\"start\":\"64163\",\"end\":\"64171\"},{\"start\":\"64175\",\"end\":\"64181\"},{\"start\":\"64637\",\"end\":\"64644\"},{\"start\":\"64951\",\"end\":\"64958\"},{\"start\":\"65310\",\"end\":\"65317\"},{\"start\":\"65541\",\"end\":\"65546\"},{\"start\":\"65552\",\"end\":\"65559\"},{\"start\":\"65563\",\"end\":\"65570\"},{\"start\":\"65909\",\"end\":\"65917\"},{\"start\":\"66276\",\"end\":\"66281\"},{\"start\":\"66285\",\"end\":\"66289\"},{\"start\":\"66293\",\"end\":\"66299\"},{\"start\":\"66303\",\"end\":\"66309\"},{\"start\":\"66635\",\"end\":\"66638\"},{\"start\":\"66941\",\"end\":\"66946\"},{\"start\":\"66950\",\"end\":\"66956\"},{\"start\":\"66960\",\"end\":\"66966\"},{\"start\":\"66970\",\"end\":\"66976\"},{\"start\":\"67286\",\"end\":\"67291\"},{\"start\":\"67295\",\"end\":\"67301\"},{\"start\":\"67305\",\"end\":\"67311\"},{\"start\":\"67315\",\"end\":\"67321\"},{\"start\":\"67644\",\"end\":\"67647\"},{\"start\":\"67956\",\"end\":\"67959\"},{\"start\":\"67963\",\"end\":\"67966\"},{\"start\":\"67970\",\"end\":\"67973\"},{\"start\":\"68269\",\"end\":\"68276\"},{\"start\":\"68280\",\"end\":\"68289\"},{\"start\":\"68299\",\"end\":\"68305\"},{\"start\":\"68636\",\"end\":\"68641\"},{\"start\":\"69049\",\"end\":\"69056\"},{\"start\":\"69060\",\"end\":\"69068\"},{\"start\":\"69072\",\"end\":\"69077\"},{\"start\":\"69081\",\"end\":\"69087\"},{\"start\":\"69416\",\"end\":\"69424\"},{\"start\":\"69428\",\"end\":\"69435\"},{\"start\":\"69439\",\"end\":\"69444\"},{\"start\":\"69448\",\"end\":\"69454\"},{\"start\":\"69797\",\"end\":\"69802\"},{\"start\":\"69806\",\"end\":\"69813\"},{\"start\":\"69817\",\"end\":\"69822\"},{\"start\":\"69826\",\"end\":\"69831\"},{\"start\":\"69835\",\"end\":\"69841\"},{\"start\":\"70251\",\"end\":\"70257\"},{\"start\":\"70261\",\"end\":\"70268\"},{\"start\":\"70274\",\"end\":\"70280\"},{\"start\":\"70635\",\"end\":\"70640\"},{\"start\":\"70892\",\"end\":\"70897\"},{\"start\":\"71238\",\"end\":\"71243\"},{\"start\":\"71247\",\"end\":\"71253\"},{\"start\":\"71257\",\"end\":\"71261\"},{\"start\":\"71265\",\"end\":\"71271\"},{\"start\":\"71277\",\"end\":\"71283\"},{\"start\":\"71622\",\"end\":\"71628\"},{\"start\":\"71956\",\"end\":\"71958\"},{\"start\":\"72391\",\"end\":\"72393\"},{\"start\":\"72753\",\"end\":\"72758\"},{\"start\":\"72762\",\"end\":\"72767\"},{\"start\":\"72771\",\"end\":\"72777\"},{\"start\":\"73010\",\"end\":\"73015\"},{\"start\":\"73019\",\"end\":\"73027\"},{\"start\":\"73031\",\"end\":\"73033\"},{\"start\":\"73037\",\"end\":\"73039\"},{\"start\":\"73043\",\"end\":\"73049\"},{\"start\":\"73385\",\"end\":\"73390\"},{\"start\":\"73396\",\"end\":\"73403\"},{\"start\":\"73409\",\"end\":\"73413\"},{\"start\":\"73419\",\"end\":\"73425\"},{\"start\":\"73864\",\"end\":\"73870\"},{\"start\":\"73874\",\"end\":\"73880\"},{\"start\":\"73884\",\"end\":\"73890\"},{\"start\":\"74288\",\"end\":\"74293\"},{\"start\":\"74297\",\"end\":\"74304\"},{\"start\":\"74308\",\"end\":\"74316\"},{\"start\":\"74320\",\"end\":\"74327\"},{\"start\":\"74331\",\"end\":\"74341\"},{\"start\":\"74345\",\"end\":\"74351\"},{\"start\":\"74786\",\"end\":\"74791\"},{\"start\":\"74795\",\"end\":\"74801\"},{\"start\":\"74805\",\"end\":\"74808\"},{\"start\":\"74812\",\"end\":\"74814\"},{\"start\":\"74818\",\"end\":\"74824\"},{\"start\":\"75119\",\"end\":\"75124\"},{\"start\":\"75128\",\"end\":\"75135\"},{\"start\":\"75139\",\"end\":\"75147\"},{\"start\":\"75151\",\"end\":\"75158\"},{\"start\":\"75162\",\"end\":\"75172\"},{\"start\":\"75176\",\"end\":\"75182\"},{\"start\":\"75669\",\"end\":\"75676\"},{\"start\":\"75680\",\"end\":\"75685\"},{\"start\":\"75689\",\"end\":\"75695\"},{\"start\":\"75894\",\"end\":\"75903\"},{\"start\":\"76069\",\"end\":\"76074\"},{\"start\":\"76214\",\"end\":\"76228\"},{\"start\":\"76570\",\"end\":\"76575\"},{\"start\":\"77028\",\"end\":\"77035\"}]", "bib_entry": "[{\"start\":\"60065\",\"end\":\"60377\",\"attributes\":{\"matched_paper_id\":\"44614127\",\"id\":\"b0\"}},{\"start\":\"60379\",\"end\":\"60673\",\"attributes\":{\"matched_paper_id\":\"31343079\",\"id\":\"b1\"}},{\"start\":\"60675\",\"end\":\"60928\",\"attributes\":{\"matched_paper_id\":\"115844916\",\"id\":\"b2\"}},{\"start\":\"60930\",\"end\":\"61217\",\"attributes\":{\"matched_paper_id\":\"202781094\",\"id\":\"b3\"}},{\"start\":\"61219\",\"end\":\"61410\",\"attributes\":{\"matched_paper_id\":\"2509211\",\"id\":\"b4\"}},{\"start\":\"61412\",\"end\":\"61701\",\"attributes\":{\"matched_paper_id\":\"5040812\",\"id\":\"b5\"}},{\"start\":\"61703\",\"end\":\"62074\",\"attributes\":{\"matched_paper_id\":\"53320392\",\"id\":\"b6\"}},{\"start\":\"62076\",\"end\":\"62430\",\"attributes\":{\"id\":\"b7\"}},{\"start\":\"62432\",\"end\":\"62801\",\"attributes\":{\"matched_paper_id\":\"38875782\",\"id\":\"b8\"}},{\"start\":\"62803\",\"end\":\"63092\",\"attributes\":{\"matched_paper_id\":\"203704906\",\"id\":\"b9\"}},{\"start\":\"63094\",\"end\":\"63444\",\"attributes\":{\"matched_paper_id\":\"133605274\",\"id\":\"b10\"}},{\"start\":\"63446\",\"end\":\"63877\",\"attributes\":{\"matched_paper_id\":\"163164623\",\"id\":\"b11\"}},{\"start\":\"63879\",\"end\":\"64054\",\"attributes\":{\"id\":\"b12\"}},{\"start\":\"64056\",\"end\":\"64508\",\"attributes\":{\"matched_paper_id\":\"67872077\",\"id\":\"b13\"}},{\"start\":\"64510\",\"end\":\"64849\",\"attributes\":{\"matched_paper_id\":\"733980\",\"id\":\"b14\"}},{\"start\":\"64851\",\"end\":\"65244\",\"attributes\":{\"matched_paper_id\":\"7149851\",\"id\":\"b15\"}},{\"start\":\"65246\",\"end\":\"65496\",\"attributes\":{\"matched_paper_id\":\"60571601\",\"id\":\"b16\"}},{\"start\":\"65498\",\"end\":\"65775\",\"attributes\":{\"matched_paper_id\":\"39020350\",\"id\":\"b17\"}},{\"start\":\"65777\",\"end\":\"66202\",\"attributes\":{\"matched_paper_id\":\"15258913\",\"id\":\"b18\"}},{\"start\":\"66204\",\"end\":\"66560\",\"attributes\":{\"matched_paper_id\":\"21351739\",\"id\":\"b19\"}},{\"start\":\"66562\",\"end\":\"66866\",\"attributes\":{\"matched_paper_id\":\"52978766\",\"id\":\"b20\"}},{\"start\":\"66868\",\"end\":\"67206\",\"attributes\":{\"matched_paper_id\":\"4708051\",\"id\":\"b21\"}},{\"start\":\"67208\",\"end\":\"67572\",\"attributes\":{\"id\":\"b22\"}},{\"start\":\"67574\",\"end\":\"67878\",\"attributes\":{\"matched_paper_id\":\"2238772\",\"id\":\"b23\"}},{\"start\":\"67880\",\"end\":\"68166\",\"attributes\":{\"matched_paper_id\":\"19098488\",\"id\":\"b24\"}},{\"start\":\"68168\",\"end\":\"68553\",\"attributes\":{\"matched_paper_id\":\"3836051\",\"id\":\"b25\"}},{\"start\":\"68555\",\"end\":\"68974\",\"attributes\":{\"matched_paper_id\":\"209093915\",\"id\":\"b26\",\"doi\":\"10.1109/TCAD.2019.2952544\"}},{\"start\":\"68976\",\"end\":\"69359\",\"attributes\":{\"matched_paper_id\":\"201070388\",\"id\":\"b27\"}},{\"start\":\"69361\",\"end\":\"69695\",\"attributes\":{\"matched_paper_id\":\"208138236\",\"id\":\"b28\"}},{\"start\":\"69697\",\"end\":\"70158\",\"attributes\":{\"matched_paper_id\":\"58027670\",\"id\":\"b29\"}},{\"start\":\"70160\",\"end\":\"70554\",\"attributes\":{\"matched_paper_id\":\"9812826\",\"id\":\"b30\"}},{\"start\":\"70556\",\"end\":\"70822\",\"attributes\":{\"matched_paper_id\":\"49301394\",\"id\":\"b31\"}},{\"start\":\"70824\",\"end\":\"71187\",\"attributes\":{\"matched_paper_id\":\"211016154\",\"id\":\"b32\"}},{\"start\":\"71189\",\"end\":\"71563\",\"attributes\":{\"matched_paper_id\":\"1677864\",\"id\":\"b33\"}},{\"start\":\"71565\",\"end\":\"71835\",\"attributes\":{\"matched_paper_id\":\"10569020\",\"id\":\"b34\"}},{\"start\":\"71837\",\"end\":\"72239\",\"attributes\":{\"matched_paper_id\":\"3869844\",\"id\":\"b35\"}},{\"start\":\"72241\",\"end\":\"72699\",\"attributes\":{\"matched_paper_id\":\"25209638\",\"id\":\"b36\"}},{\"start\":\"72701\",\"end\":\"72946\",\"attributes\":{\"matched_paper_id\":\"53235957\",\"id\":\"b37\"}},{\"start\":\"72948\",\"end\":\"73302\",\"attributes\":{\"matched_paper_id\":\"155109576\",\"id\":\"b38\"}},{\"start\":\"73304\",\"end\":\"73700\",\"attributes\":{\"matched_paper_id\":\"201900134\",\"id\":\"b39\"}},{\"start\":\"73702\",\"end\":\"74197\",\"attributes\":{\"matched_paper_id\":\"49907924\",\"id\":\"b40\"}},{\"start\":\"74199\",\"end\":\"74707\",\"attributes\":{\"matched_paper_id\":\"189824904\",\"id\":\"b41\"}},{\"start\":\"74709\",\"end\":\"75028\",\"attributes\":{\"matched_paper_id\":\"191527916\",\"id\":\"b42\"}},{\"start\":\"75030\",\"end\":\"75394\",\"attributes\":{\"matched_paper_id\":\"189824904\",\"id\":\"b43\"}},{\"start\":\"75396\",\"end\":\"75531\",\"attributes\":{\"id\":\"b44\"}},{\"start\":\"75533\",\"end\":\"75628\",\"attributes\":{\"id\":\"b45\"}},{\"start\":\"75630\",\"end\":\"75848\",\"attributes\":{\"id\":\"b46\"}},{\"start\":\"75850\",\"end\":\"76044\",\"attributes\":{\"matched_paper_id\":\"10659969\",\"id\":\"b47\"}},{\"start\":\"76046\",\"end\":\"76149\",\"attributes\":{\"matched_paper_id\":\"110511037\",\"id\":\"b48\"}},{\"start\":\"76151\",\"end\":\"76395\",\"attributes\":{\"matched_paper_id\":\"207252270\",\"id\":\"b49\"}},{\"start\":\"76397\",\"end\":\"76492\",\"attributes\":{\"id\":\"b50\"}},{\"start\":\"76494\",\"end\":\"76885\",\"attributes\":{\"matched_paper_id\":\"12031548\",\"id\":\"b51\"}},{\"start\":\"76887\",\"end\":\"77851\",\"attributes\":{\"id\":\"b52\"}}]", "bib_title": "[{\"start\":\"60065\",\"end\":\"60131\"},{\"start\":\"60379\",\"end\":\"60466\"},{\"start\":\"60675\",\"end\":\"60729\"},{\"start\":\"60930\",\"end\":\"60989\"},{\"start\":\"61219\",\"end\":\"61260\"},{\"start\":\"61412\",\"end\":\"61476\"},{\"start\":\"61703\",\"end\":\"61776\"},{\"start\":\"62076\",\"end\":\"62154\"},{\"start\":\"62432\",\"end\":\"62521\"},{\"start\":\"62803\",\"end\":\"62866\"},{\"start\":\"63094\",\"end\":\"63144\"},{\"start\":\"63446\",\"end\":\"63537\"},{\"start\":\"64056\",\"end\":\"64139\"},{\"start\":\"64510\",\"end\":\"64633\"},{\"start\":\"64851\",\"end\":\"64947\"},{\"start\":\"65246\",\"end\":\"65306\"},{\"start\":\"65498\",\"end\":\"65537\"},{\"start\":\"65777\",\"end\":\"65897\"},{\"start\":\"66204\",\"end\":\"66272\"},{\"start\":\"66562\",\"end\":\"66631\"},{\"start\":\"66868\",\"end\":\"66937\"},{\"start\":\"67208\",\"end\":\"67282\"},{\"start\":\"67574\",\"end\":\"67640\"},{\"start\":\"67880\",\"end\":\"67952\"},{\"start\":\"68168\",\"end\":\"68265\"},{\"start\":\"68555\",\"end\":\"68632\"},{\"start\":\"68976\",\"end\":\"69045\"},{\"start\":\"69361\",\"end\":\"69412\"},{\"start\":\"69697\",\"end\":\"69793\"},{\"start\":\"70160\",\"end\":\"70247\"},{\"start\":\"70556\",\"end\":\"70631\"},{\"start\":\"70824\",\"end\":\"70888\"},{\"start\":\"71189\",\"end\":\"71234\"},{\"start\":\"71565\",\"end\":\"71618\"},{\"start\":\"71837\",\"end\":\"71950\"},{\"start\":\"72241\",\"end\":\"72387\"},{\"start\":\"72701\",\"end\":\"72749\"},{\"start\":\"72948\",\"end\":\"73006\"},{\"start\":\"73304\",\"end\":\"73381\"},{\"start\":\"73702\",\"end\":\"73860\"},{\"start\":\"74199\",\"end\":\"74284\"},{\"start\":\"74709\",\"end\":\"74782\"},{\"start\":\"75030\",\"end\":\"75115\"},{\"start\":\"75850\",\"end\":\"75890\"},{\"start\":\"76046\",\"end\":\"76065\"},{\"start\":\"76151\",\"end\":\"76210\"},{\"start\":\"76494\",\"end\":\"76566\"},{\"start\":\"76887\",\"end\":\"77019\"}]", "bib_author": "[{\"start\":\"60133\",\"end\":\"60140\"},{\"start\":\"60140\",\"end\":\"60152\"},{\"start\":\"60468\",\"end\":\"60479\"},{\"start\":\"60731\",\"end\":\"60743\"},{\"start\":\"60991\",\"end\":\"61002\"},{\"start\":\"61002\",\"end\":\"61014\"},{\"start\":\"61014\",\"end\":\"61022\"},{\"start\":\"61262\",\"end\":\"61269\"},{\"start\":\"61478\",\"end\":\"61485\"},{\"start\":\"61485\",\"end\":\"61493\"},{\"start\":\"61493\",\"end\":\"61505\"},{\"start\":\"61505\",\"end\":\"61512\"},{\"start\":\"61778\",\"end\":\"61789\"},{\"start\":\"61789\",\"end\":\"61798\"},{\"start\":\"61798\",\"end\":\"61807\"},{\"start\":\"61807\",\"end\":\"61817\"},{\"start\":\"62156\",\"end\":\"62169\"},{\"start\":\"62169\",\"end\":\"62182\"},{\"start\":\"62182\",\"end\":\"62190\"},{\"start\":\"62190\",\"end\":\"62204\"},{\"start\":\"62523\",\"end\":\"62530\"},{\"start\":\"62530\",\"end\":\"62538\"},{\"start\":\"62538\",\"end\":\"62555\"},{\"start\":\"62555\",\"end\":\"62570\"},{\"start\":\"62868\",\"end\":\"62880\"},{\"start\":\"62880\",\"end\":\"62889\"},{\"start\":\"62889\",\"end\":\"62900\"},{\"start\":\"63146\",\"end\":\"63154\"},{\"start\":\"63154\",\"end\":\"63169\"},{\"start\":\"63169\",\"end\":\"63175\"},{\"start\":\"63175\",\"end\":\"63183\"},{\"start\":\"63183\",\"end\":\"63191\"},{\"start\":\"63539\",\"end\":\"63548\"},{\"start\":\"63548\",\"end\":\"63558\"},{\"start\":\"63558\",\"end\":\"63570\"},{\"start\":\"63570\",\"end\":\"63577\"},{\"start\":\"63577\",\"end\":\"63585\"},{\"start\":\"63585\",\"end\":\"63595\"},{\"start\":\"64141\",\"end\":\"64152\"},{\"start\":\"64152\",\"end\":\"64161\"},{\"start\":\"64161\",\"end\":\"64173\"},{\"start\":\"64173\",\"end\":\"64183\"},{\"start\":\"64635\",\"end\":\"64646\"},{\"start\":\"64949\",\"end\":\"64960\"},{\"start\":\"65308\",\"end\":\"65319\"},{\"start\":\"65539\",\"end\":\"65548\"},{\"start\":\"65548\",\"end\":\"65561\"},{\"start\":\"65561\",\"end\":\"65572\"},{\"start\":\"65899\",\"end\":\"65905\"},{\"start\":\"65905\",\"end\":\"65919\"},{\"start\":\"66274\",\"end\":\"66283\"},{\"start\":\"66283\",\"end\":\"66291\"},{\"start\":\"66291\",\"end\":\"66301\"},{\"start\":\"66301\",\"end\":\"66311\"},{\"start\":\"66633\",\"end\":\"66640\"},{\"start\":\"66939\",\"end\":\"66948\"},{\"start\":\"66948\",\"end\":\"66958\"},{\"start\":\"66958\",\"end\":\"66968\"},{\"start\":\"66968\",\"end\":\"66978\"},{\"start\":\"67284\",\"end\":\"67293\"},{\"start\":\"67293\",\"end\":\"67303\"},{\"start\":\"67303\",\"end\":\"67313\"},{\"start\":\"67313\",\"end\":\"67323\"},{\"start\":\"67642\",\"end\":\"67649\"},{\"start\":\"67954\",\"end\":\"67961\"},{\"start\":\"67961\",\"end\":\"67968\"},{\"start\":\"67968\",\"end\":\"67975\"},{\"start\":\"68267\",\"end\":\"68278\"},{\"start\":\"68278\",\"end\":\"68291\"},{\"start\":\"68291\",\"end\":\"68297\"},{\"start\":\"68297\",\"end\":\"68307\"},{\"start\":\"68634\",\"end\":\"68643\"},{\"start\":\"69047\",\"end\":\"69058\"},{\"start\":\"69058\",\"end\":\"69070\"},{\"start\":\"69070\",\"end\":\"69079\"},{\"start\":\"69079\",\"end\":\"69089\"},{\"start\":\"69414\",\"end\":\"69426\"},{\"start\":\"69426\",\"end\":\"69437\"},{\"start\":\"69437\",\"end\":\"69446\"},{\"start\":\"69446\",\"end\":\"69456\"},{\"start\":\"69795\",\"end\":\"69804\"},{\"start\":\"69804\",\"end\":\"69815\"},{\"start\":\"69815\",\"end\":\"69824\"},{\"start\":\"69824\",\"end\":\"69833\"},{\"start\":\"69833\",\"end\":\"69843\"},{\"start\":\"70249\",\"end\":\"70259\"},{\"start\":\"70259\",\"end\":\"70270\"},{\"start\":\"70270\",\"end\":\"70282\"},{\"start\":\"70633\",\"end\":\"70642\"},{\"start\":\"70890\",\"end\":\"70899\"},{\"start\":\"71236\",\"end\":\"71245\"},{\"start\":\"71245\",\"end\":\"71255\"},{\"start\":\"71255\",\"end\":\"71263\"},{\"start\":\"71263\",\"end\":\"71273\"},{\"start\":\"71273\",\"end\":\"71285\"},{\"start\":\"71620\",\"end\":\"71630\"},{\"start\":\"71952\",\"end\":\"71960\"},{\"start\":\"72389\",\"end\":\"72395\"},{\"start\":\"72751\",\"end\":\"72760\"},{\"start\":\"72760\",\"end\":\"72769\"},{\"start\":\"72769\",\"end\":\"72779\"},{\"start\":\"73008\",\"end\":\"73017\"},{\"start\":\"73017\",\"end\":\"73029\"},{\"start\":\"73029\",\"end\":\"73035\"},{\"start\":\"73035\",\"end\":\"73041\"},{\"start\":\"73041\",\"end\":\"73051\"},{\"start\":\"73383\",\"end\":\"73392\"},{\"start\":\"73392\",\"end\":\"73405\"},{\"start\":\"73405\",\"end\":\"73415\"},{\"start\":\"73415\",\"end\":\"73427\"},{\"start\":\"73862\",\"end\":\"73872\"},{\"start\":\"73872\",\"end\":\"73882\"},{\"start\":\"73882\",\"end\":\"73892\"},{\"start\":\"74286\",\"end\":\"74295\"},{\"start\":\"74295\",\"end\":\"74306\"},{\"start\":\"74306\",\"end\":\"74318\"},{\"start\":\"74318\",\"end\":\"74329\"},{\"start\":\"74329\",\"end\":\"74343\"},{\"start\":\"74343\",\"end\":\"74353\"},{\"start\":\"74784\",\"end\":\"74793\"},{\"start\":\"74793\",\"end\":\"74803\"},{\"start\":\"74803\",\"end\":\"74810\"},{\"start\":\"74810\",\"end\":\"74816\"},{\"start\":\"74816\",\"end\":\"74826\"},{\"start\":\"75117\",\"end\":\"75126\"},{\"start\":\"75126\",\"end\":\"75137\"},{\"start\":\"75137\",\"end\":\"75149\"},{\"start\":\"75149\",\"end\":\"75160\"},{\"start\":\"75160\",\"end\":\"75174\"},{\"start\":\"75174\",\"end\":\"75184\"},{\"start\":\"75667\",\"end\":\"75678\"},{\"start\":\"75678\",\"end\":\"75687\"},{\"start\":\"75687\",\"end\":\"75697\"},{\"start\":\"75892\",\"end\":\"75905\"},{\"start\":\"76067\",\"end\":\"76076\"},{\"start\":\"76212\",\"end\":\"76230\"},{\"start\":\"76568\",\"end\":\"76577\"},{\"start\":\"77021\",\"end\":\"77037\"}]", "bib_venue": "[{\"start\":\"60152\",\"end\":\"60195\"},{\"start\":\"60479\",\"end\":\"60497\"},{\"start\":\"60743\",\"end\":\"60780\"},{\"start\":\"61022\",\"end\":\"61040\"},{\"start\":\"61269\",\"end\":\"61287\"},{\"start\":\"61512\",\"end\":\"61521\"},{\"start\":\"61817\",\"end\":\"61855\"},{\"start\":\"62204\",\"end\":\"62222\"},{\"start\":\"62570\",\"end\":\"62588\"},{\"start\":\"62900\",\"end\":\"62918\"},{\"start\":\"63191\",\"end\":\"63232\"},{\"start\":\"63595\",\"end\":\"63628\"},{\"start\":\"63879\",\"end\":\"63957\"},{\"start\":\"64183\",\"end\":\"64240\"},{\"start\":\"64646\",\"end\":\"64658\"},{\"start\":\"64960\",\"end\":\"64980\"},{\"start\":\"65319\",\"end\":\"65338\"},{\"start\":\"65572\",\"end\":\"65605\"},{\"start\":\"65919\",\"end\":\"65955\"},{\"start\":\"66311\",\"end\":\"66349\"},{\"start\":\"66640\",\"end\":\"66679\"},{\"start\":\"66978\",\"end\":\"66997\"},{\"start\":\"67323\",\"end\":\"67362\"},{\"start\":\"67649\",\"end\":\"67691\"},{\"start\":\"67975\",\"end\":\"67993\"},{\"start\":\"68307\",\"end\":\"68325\"},{\"start\":\"68699\",\"end\":\"68753\"},{\"start\":\"69089\",\"end\":\"69132\"},{\"start\":\"69456\",\"end\":\"69494\"},{\"start\":\"69843\",\"end\":\"69889\"},{\"start\":\"70282\",\"end\":\"70322\"},{\"start\":\"70642\",\"end\":\"70657\"},{\"start\":\"70924\",\"end\":\"70978\"},{\"start\":\"71285\",\"end\":\"71335\"},{\"start\":\"71630\",\"end\":\"71671\"},{\"start\":\"71960\",\"end\":\"72001\"},{\"start\":\"72395\",\"end\":\"72435\"},{\"start\":\"72779\",\"end\":\"72783\"},{\"start\":\"73051\",\"end\":\"73090\"},{\"start\":\"73427\",\"end\":\"73471\"},{\"start\":\"73892\",\"end\":\"73927\"},{\"start\":\"74353\",\"end\":\"74410\"},{\"start\":\"74826\",\"end\":\"74840\"},{\"start\":\"75184\",\"end\":\"75199\"},{\"start\":\"75398\",\"end\":\"75453\"},{\"start\":\"75533\",\"end\":\"75564\"},{\"start\":\"75630\",\"end\":\"75665\"},{\"start\":\"75905\",\"end\":\"75924\"},{\"start\":\"76076\",\"end\":\"76087\"},{\"start\":\"76230\",\"end\":\"76249\"},{\"start\":\"76397\",\"end\":\"76428\"},{\"start\":\"76577\",\"end\":\"76642\"},{\"start\":\"77037\",\"end\":\"77151\"},{\"start\":\"61523\",\"end\":\"61526\"},{\"start\":\"61857\",\"end\":\"61889\"},{\"start\":\"63234\",\"end\":\"63269\"},{\"start\":\"63630\",\"end\":\"63657\"},{\"start\":\"64242\",\"end\":\"64293\"},{\"start\":\"64982\",\"end\":\"64996\"},{\"start\":\"65340\",\"end\":\"65353\"},{\"start\":\"65607\",\"end\":\"65634\"},{\"start\":\"66351\",\"end\":\"66383\"},{\"start\":\"66681\",\"end\":\"66714\"},{\"start\":\"66999\",\"end\":\"67012\"},{\"start\":\"67364\",\"end\":\"67397\"},{\"start\":\"67693\",\"end\":\"67725\"},{\"start\":\"69134\",\"end\":\"69171\"},{\"start\":\"69496\",\"end\":\"69528\"},{\"start\":\"69891\",\"end\":\"69931\"},{\"start\":\"70324\",\"end\":\"70358\"},{\"start\":\"70659\",\"end\":\"70668\"},{\"start\":\"71337\",\"end\":\"71381\"},{\"start\":\"72003\",\"end\":\"72038\"},{\"start\":\"72437\",\"end\":\"72471\"},{\"start\":\"72785\",\"end\":\"72789\"},{\"start\":\"73092\",\"end\":\"73125\"},{\"start\":\"74412\",\"end\":\"74463\"},{\"start\":\"75201\",\"end\":\"75210\"},{\"start\":\"76644\",\"end\":\"76703\"},{\"start\":\"77153\",\"end\":\"77188\"}]"}}}, "year": 2023, "month": 12, "day": 17}