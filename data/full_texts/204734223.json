{"id": 204734223, "updated": "2023-10-06 22:05:59.088", "metadata": {"title": "On adversarial patches: real-world attack on ArcFace-100 face recognition system", "authors": "[{\"first\":\"Mikhail\",\"last\":\"Pautov\",\"middle\":[]},{\"first\":\"Grigorii\",\"last\":\"Melnikov\",\"middle\":[]},{\"first\":\"Edgar\",\"last\":\"Kaziakhmedov\",\"middle\":[]},{\"first\":\"Klim\",\"last\":\"Kireev\",\"middle\":[]},{\"first\":\"Aleksandr\",\"last\":\"Petiushko\",\"middle\":[]}]", "venue": "2019 International Multi-Conference on Engineering, Computer and Information Sciences (SIBIRCON)", "journal": "2019 International Multi-Conference on Engineering, Computer and Information Sciences (SIBIRCON)", "publication_date": {"year": 2019, "month": 10, "day": 15}, "abstract": "Recent works showed the vulnerability of image classifiers to adversarial attacks in the digital domain. However, the majority of attacks involve adding small perturbation to an image to fool the classifier. Unfortunately, such procedures can not be used to conduct a real-world attack, where adding an adversarial attribute to the photo is a more practical approach. In this paper, we study the problem of real-world attacks on face recognition systems. We examine security of one of the best public face recognition systems, LResNet100E-IR with ArcFace loss, and propose a simple method to attack it in the physical world. The method suggests creating an adversarial patch that can be printed, added as a face attribute and photographed; the photo of a person with such attribute is then passed to the classifier such that the classifier's recognized class changes from correct to the desired one. Proposed generating procedure allows projecting adversarial patches not only on different areas of the face, such as nose or forehead but also on some wearable accessory, such as eyeglasses.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1910.07067", "mag": "3101267861", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1910-07067", "doi": "10.1109/sibircon48586.2019.8958134"}}, "content": {"source": {"pdf_hash": "5b4e078a21deb16ac064ac6f6779e312f22016bb", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1910.07067v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1910.07067", "status": "GREEN"}}, "grobid": {"id": "9e3a2977328d29d3cbca60a560d4e58570b3e1a7", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5b4e078a21deb16ac064ac6f6779e312f22016bb.txt", "contents": "\nOn adversarial patches: real-world attack on ArcFace-100 face recognition system 1 st 4 th Klim Kireev\n\n\nMikhail Pautov mikhail.pautov@phystech.edu \nGrigorii Melnikov grigorii.melnikov@skoltech.ru \nRd Edgar Kaziakhmedov edgar.kaziakhmedov@phystech.edu \nAleksandr Petiushko petyushko.alexander1@huawei.com \n\nIntelligent Systems Lab Huawei Moscow Research Center Moscow\nRussia\n\n\nIntelligent Systems Lab Huawei Moscow Research Center Moscow\nRussia\n\n\nIntelligent Systems Lab Huawei Moscow Research Center Moscow\nRussia\n\n\nIntelligent Systems Lab Huawei Moscow Research Center Moscow\nRussia\n\n\nIntelligent Systems Lab Huawei Moscow Research Center Moscow\nRussia\n\nOn adversarial patches: real-world attack on ArcFace-100 face recognition system 1 st 4 th Klim Kireev\nIndex Terms-adversarial patchface recognitionphysical domainArcFace\nRecent works showed the vulnerability of image classifiers to adversarial attacks in the digital domain. However, the majority of attacks involve adding small perturbation to an image to fool the classifier. Unfortunately, such procedures can not be used to conduct a real-world attack, where adding an adversarial attribute to the photo is a more practical approach. In this paper, we study the problem of real-world attacks on face recognition systems. We examine security of one of the best public face recognition systems, LResNet100E-IR with ArcFace loss, and propose a simple method to attack it in the physical world. The method suggests creating an adversarial patch that can be printed, added as a face attribute and photographed; the photo of a person with such attribute is then passed to the classifier such that the classifier's recognized class changes from correct to the desired one. Proposed generating procedure allows projecting adversarial patches not only on different areas of the face, such as nose or forehead but also on some wearable accessory, such as eyeglasses.\n\nI. INTRODUCTION\n\nIn the last years, much of research was done in the field of attacks on face recognition systems. Nowadays, one of the best face recognition systems to test different attacking approaches is LResNet100E-IR with special additive angular margin loss, ArcFace. The network maps an image to a feature vector such that intra-class distance tends to be small and inter-class distance stays large. In the original paper [1] it is observed that the accuracy in classification problems of this network is comparable to state-of-the-art models.\n\nAlthough deep neural networks are efficient in image classification, they are vulnerable to adversarial samples [2]. Majority of prior works has focused on exploiting such vulnerabilities through imperceptible changes of an image, which are constrained by some norm, e.g. L 1 , L 2 , L \u221e . In such cases, an attacker is trying to fool classifier but at the same time keeps perceptual similarity between original and adversarial images. Later the authors of [3] succeeded in localizing of image changes. They generated the patch that can be placed anywhere within the field of view of a classifier and managed to conduct an attack in the real world.\n\nAttack on face recognition neural network is more complex in comparison to attack on just image classifier. It is known for a long time that several areas on a face influence classification more than the others [4]. So, the result of classification of the photo with an adversarial patch depends on an area where the patch is located.\n\nAuthors of [5] investigated the possibility to construct an attack inconspicuous for the human eye. They managed to conduct an attack in the real world with the use of eyeglasses as an adversarial attribute. These glasses were constructed to impersonate a person, but were designed for the face of attacker with a fixed orientation of his head.\n\nThe goal of this paper is to make a starting point in the improvement of the security of the open face recognition system, ArcFace. Our approach is to explore its vulnerability to adversarial attacks and mislead the network using the proposed method. It should be mentioned that the network and its weights are available on the Internet and the adversarial attack approach is considered as well-known, so the research conducted does not violate any regulations.\n\nThe outline of current research is presented below:\n\n\u2022 A simple adversarial patch generation procedure is proposed; \u2022 The procedure allows to project adversarial patch on different surfaces on a face; \u2022 Effectiveness of adversarial patches placed on different positions on the face was studied; \u2022 A real-world attack on one of the best public face recognition systems with the use of this procedure is conducted; \u2022 The attacking pipeline can be easily implemented since the gained patch is a gray-scale image.\n\n\nII. THE MAIN CONCEPT AND RELATED WORKS\n\nIn this section, we describe a concept of adversarial attacks and present an overview of methods and techniques which are often used in the physical domain.\n\n\nA. The concept of adversarial attacks\n\nAn adversarial attack on face recognition is a technique to fool some recognition system through a change in input such that the output of the system changes from correct to another one. Although adversarial attack may be conducted in the digital domain (where the input to the classifier may be changed, for example, pixel-wise), it is much harder to construct such an attack in the physical world. However, in [5], [6], it was shown that it is possible with the efficiency comparable to the one in the digital domain.\n\nThere are several possible classifications of adversarial attacks on face recognition systems (i.e. on classifiers). One way is to determine how the output class should change:\n\n\u2022 targeted attack -the adversary changes the output classification of input to the desired one; \u2022 untargeted attack -the adversary leads to misclassification of input. In [7], the classification of attacks based on what is known about the neural network is provided. In white-box attacks, parameters of the model, as well as its structure and training procedure, are known. In contrast, in black-box scenarios, none of the above is known.\n\n\nB. Related works\n\nIn recent years, much research had been done in the field of attacks in the physical domain.\n\nIn [8], one of the most straightforward attacking approaches, Fast Gradient Sign Method, FGSM, was proposed. This method refers to generating of adversarial examples through adding to the initial image the perturbation\n\u03b7 = \u03b5sign(\u2207 x J(\u03b8, x, y)),(1)\nwhere \u03b8 is vector of parameters of the model, x is input to the model, y are the targets associated with x and J(\u03b8, x, y) is the cost function used to train the neural network. Note that in case of targeted attack, y are desired targets and \u03b5 < 0 and in case of untargeted attack, y are incorrect targets and \u03b5 > 0.\n\nIn [9], it was discovered that adding the momentum term into the iterative process for attacks lead to more stable optimization trajectory. It is determined that adversarial examples generated with the iterative method with the use of momentum are more suitable for white-box attacks than the ones generated without the use of momentum.\n\nIn [10], the question of the robustness of adversarial examples under real-world image transformations was studied, and Expectation Over Transformation (EOT) algorithm was proposed. This algorithm helps to generate an adversarial example taking into account a set of transformations which usually spoils the transferability of an image to the real-world.\n\nGiven the distance function \u03c1 : X \u00d7X \u2192 R, distance bound \u03b5, set of transformations T and y adv as desired adversarial classes corresponding to x, EOT approach may be formulated as the following optimization problem:\nX adv = arg max z E t\u2208T P(y adv |t(x + z)) ,(2)\nsuch that E t\u2208T \u03c1(x + z, x) < \u03b5 and x + z \u2208 [0, 1] d . In this formulation z \u2208 [0, 1] d is an additive noise. This approach helps to imitate such transformations as camera noise or viewpoint shifts.\n\n\nIII. METHODOLOGY\n\nIt should be mentioned that usually face recognition process in the real world consists of two sequential parts. Firstly, the image of a person is passed to face detection neural network, which obtains a bounding box with a face. Secondly, the obtained bounding box is passed to the classifying neural network, LResNet100E-Ir in our case. Nowadays, there is a variety of face detection neural networks. In our experiments, we use the multi-task cascaded CNN based framework (MTCCN), proposed in [11].\n\nIn this paper, we propose a pipeline to generate a broad class of adversarial patches, which can fool white-box models and should be fitted for a given person. In this section, we describe the proposed pipeline and give a detailed explanation of each stage.\n\nThe attacking strategy may be explained as a sequence of the following steps:\n\n\u2022 choose desired patch location and its shape, \u2022 print patch with a chessboard pattern and apply it on a face or wearable attribute, \u2022 take a photo of your face and mark corners of the chessboard on a photo, \u2022 repeat steps mentioned before to get photos with different head rotations, \u2022 project adversarial patch on the chessboard pattern, \u2022 get facial landmarks using face detector (MTCNN), \u2022 warp image to standard 112 \u00d7 112 ArcFace input using similarity transformation and obtained landmarks, \u2022 solve the corresponding optimization problem.\n\nA. Projective transformation and chessboard pattern 1) Non-linear 2d transformation: To approximate nonlinear 2d transformation, we apply projective transformation separately to each cell of the chessboard grid. Regular grid\nG = {G i } of pixels G i = (x t i , y t i ) forming output map. (x t i , y t i )\ndenotes chessboard sticker's coordinates on the photo. By four corner pixels of j'th chessboard cell and corresponding four marked pixels of projected cell we can obtain M \u03b8j matrix of spatial transformation, so sampling points of j'th cell:\nx s ij y s ij = T \u03b8j G ij = M \u03b8j \uf8eb \uf8ed x t i y t i 1 \uf8f6 \uf8f8 ,(3)\nwhere (x s i , y s i ) are the source coordinates in the input map, they define the spatial location in the adversarial patch where sampling kernel is applied to get the output map. Given the expression above, the target grid for the adversarial patch is obtained as a union of target coordinates of each cell:\nx s i y s i = {T \u03b8j (G ij )}(4)\nTo speed up the training procedure, we should precalculate and store sampling points. Next step is differentiable image sampling. In more details, this procedure described in [12]. Scheme of transformations is illustrated in Fig. 1. 2) Linear 2d transformation: In case when the application of adversarial patch could be simulated by a linear transformation (e.g. in case of eyeglasses), we repeat steps mentioned in the previous section just for one cell.\n\nB. Similarity transform and face detector MTCNN face detection system finds facial landmarks, which are then used to obtain a matrix of similarity transformation A \u03c6 and sampling points T \u03c6 (G ) for this warp. The main reason why it is necessary to apply sticker before similarity transformation is image sampling. Sampling kernel takes values of neighbouring pixels and copies one of them or return averaged value. Application patch after similarity transformation tends to inconsistent values of pixels on the boundaries of the patch.\n\n\nC. Loss function and training procedure\n\nThe goal of the attack is to generate adversarial patch which can not only mislead face recognition system but also should be good-looking. We formulate the second objective by restricting neighbouring pixels to have a similar colour. We obtain the adversarial sticker minimizing the following objective function:\nL(X, p) = L adv (X, p) + \u03c4 T V (p), p \u2208 [0, 1] m\u00d7n -patch,(5)\nwhere X is a batch of photos of the attacker with different shooting conditions (i.e. set of transforms T described above) and total variation loss, or T V loss, preserve high perceptual quality and ensures that the difference between neighbouring pixels is imperceptible [13], as far as photos taken from the camera do not have tremendous shift in values of neighbouring pixels. TV loss is the function of pixel values of a patch p and is defined as follows:\nT V (p) = i,j p i,j \u2212 p i,j+1 2 + p i,j \u2212 p i+1,j 2(6)\nL adv is the cosine similarity loss that minimizes the similarity between an embedding of the photo with patch and initial embedding of the person. This term depends on type of attack. In case of untargeted attack:\nL adv (X, p) = E t\u2208T,x\u2208X \u2212 cos(e xt , e) ,(7)\nwhere x is a photo from batch X corresponding to some shooting condition, e is ground truth embedding of attacker.\n\nIn case of targeted attack:\nL adv (X, p) = E t\u2208T,x\u2208X cos(e xt , e x ) ,(8)\nwhere e x is an embedding corresponding to desired person or to the closest class except for ground truth class.\n\nHere e xt is an embedding corresponding to the photo x t of attacker with applied patch. Procedure of patch application may be denoted in a following way. Given a sampler function A, photo x and corresponding grid T \u03b8 (G), x t denotes image with applied patch:\nx t = A(x, t(p), T \u03b8 (G)).(9)\nIt should be mentioned that all the operations described above are differentiable. According to [9], gradients through this iterations \u2207 p L(X, p) are accumulated. Given decay factor \u00b5,\ng := \u00b5g + \u2207 p L(X, p) \u2207 p L(X, p) 1(10)\nand patch p is updated as follows:\np := p \u2212 \u03b5 sign(g).(11)\nThe whole attacking pipeline is presented in Fig 2. \n\n\nIV. EXPERIMENTS\n\nIn this section, we describe settings of experiments and its technical details.\n\n\nLResNet\n\n\nMI-FGSM\n\n\nSampler\n\nSampler\n\u03b8 ( ) \u03d5 ( \u2032)\n\nFig. 2. Attacking pipeline\n\n\nA. Data preprocessing\n\nAs a part of the experimental setup, we used pretrained LResNet100E-IR to collect embeddings of different people. For this purpose, we use first 200 classes from CASIA-WebFace [14] dataset and photos of the 1 st and the 2 nd authors as images of attackers.\n\n\nB. Setup of experiments\n\nIt should be mentioned that solving the optimization problem described in the previous section implies patch optimization in the whole RGB colour space. However, colour triplets of the patch in digital space differ significantly from the ones of the printed patch because of the printer's narrow colour spectrum. More than that, triplets change one more time when the printed patch is photographed on camera. Due to these issues, we decided to use not a colour but a grayscale patch.\n\nIt was discovered that iterative FGSM method with momentum works good in the digital domain, but gradient step size \u03b5, decay factor \u00b5 and weight \u03c4 of TV loss affect colours transition to physical domain dramatically. It was empirically estimated that values \u03b5 = 1 16 , \u00b5 = 0.9 and \u03c4 = 1e \u2212 3 during the training leads to the best results on test in the real world.\n\nIn addition, we want to mention that initial similarity with ground truth class cos(e xt , e) is between 0.65 and 0.70 for all training photos in all three scenarios, whereas initial similarity with desired class cos(e xt , e x ) in targeted attacks is always below 0.30.\n\n\nC. Adversarial sticker localization\n\nIn this work, we tested the method proposed above to generate patches for three different areas on the face. For the diversity, one of the generated patches is an eyeglass; two others are stickers on nose and forehead.\n\n\n1) Attacking eyeglasses:\n\nIn this case, we use a patch of the form of eyeglasses rim (front view), where the frame and bridge of eyeglasses are used for adversarial pattern application. It turned out that the size of these parts is crucial for the success of the attack. Thus, we decided to use an eyeglasses model with a big frame and of size 16.5 \u00d7 6.4 cm. Example of the adversarial patch of this form is presented in Fig. 3 and Fig. 4. In this scenario, 14 photos were used for training, 3 photos were used for validation in the digital domain, and 5 photos were used for the test in the physical domain.\n\n2) Attacking forehead: In this case, we used a rectangular patch of size 14 \u00d7 5 cells; the width of one cell is 1.0 cm. It turned out that the closer patch to eyebrows the better results in a targeted attack. In this scenario, 4 photos were used for The patch of this type is illustrated in Fig. 5 and in Fig. 6. 3) Attacking nose: In this case, we used a rectangular patch of size 12\u00d76 cells; the width of one cell is 0.7 cm. An example of adversarial sticker is depicted in Fig. 7 and Fig. 8. We varied length in cells of the patch. During experiments, we found out that the height of the patch less than 6 cell is not enough for the successful attack even in the digital domain. but it was found out that increasing of size of patch is not enough for transferability of attack to the physical domain.\n\n\nD. Results\n\nIn all three above scenarios, training procedure was conducted for 10000 iterations of the method described before. If on some iteration L adv (X, p) < 0, training stops and an obtained patch is printed and tested in the physical world.\n\nNumerical results of the attacks for the first and the second attacking scenarios are presented in Table I. There we report the mean similarities \u00b1 standard errors of the mean on training photos x train , on validation photos in digital domain x val and on test photos in physical domain x test with both ground truth class and desired class. \n\n\nE. Discussion\n\nNumerical experiments showed that it is possible to attack ArcFace in the real world with the use of adversarial stickers placed on eyeglasses or forehead. Although similarity with ground truth class cos(e xtest , e) is just slightly below the similarity with desired class cos(e xtest , e x ), it is noticeable that neural network can not recognize the attacker as ground truth class. It should be said that the improvement of stability of the proposed technique to different illumination conditions may increase the success of the attack.\n\nIn addition, it may be mentioned that one of the interesting outcomes of attack is appearing of sketches of parts of the face on adversarial patches located in corresponding areas. More than that, it is discovered that the position of a patch, as well as its size, dramatically affects the success of attack in the physical domain. In general, the bigger the patch and the closer it to eyes, the better the results of attack in the real world.\n\n\nV. CONCLUSION AND FUTURE WORK\n\nWe proposed a simple method of creating adversarial patches that can be used to attack state-of-the-art face recognition systems. Our approach was tested in targeted and untargeted attacks on FaceID model LResNet100E-IR, ArcFace@ms1m-refine-v2. In this paper, it was shown that a simple and easily reproducible attacking technique leads to incorrect work of that system not only in the digital domain but also in the physical world. Experiments show that further security research should be conducted to improve the robustness of such networks to attacks of this type. In the future, we focus on construction and utilization of a defense technique to make a network invulnerable to such attacks, especially in the physical world.\n\nFig. 1 .\n1Procedure of adversarial sticker application.\n\nFig. 3 .\n3Adversarial eyeglasses\n\nFig. 4 .\n4Example of adversarial eyeglasses training, 3 photos were used for validation in digital domain and 3 photos were used for the test in the physical domain.\n\nFig. 5 .\n5Adversarial forehead Fig. 6. Example of adversarial sticker on forehead\n\nFig. 7 .\n7Adversarial nose Fig. 8. Example of adversarial sticker on nose\n\nTABLE I NUMERICAL\nIRESULTS OF EXPERIMENTS Patch Type of attack cos(ex train , e) cos(ex train , e x ) cos(ex val , e) cos(ex val , e x ) cos(ex test , e) cos(ex test , e x )Eyeglasses \nTargeted \n0.041 \u00b1 0.052 \n0.648 \u00b1 0.020 \n0.317 \u00b1 0.004 \n0.451 \u00b1 0.021 \n0.305 \u00b1 0.024 \n0.363 \u00b1 0.024 \nForehead \nTargeted \n-0.053 \u00b1 0.009 \n0.221 \u00b1 0.011 \n0.273 \u00b1 0.007 \n0.421 \u00b1 0.025 \n0.323 \u00b1 0.035 \n0.391 \u00b1 0.021 \n\n\n\nArcface: Additive angular margin loss for deep face recognition. J Deng, J Guo, N Xue, S Zafeiriou, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJ. Deng, J. Guo, N. Xue, and S. Zafeiriou, \"Arcface: Additive angular margin loss for deep face recognition,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4690-4699.\n\nIntriguing properties of neural networks. C Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan, I Goodfellow, R Fergus, arXiv:1312.6199arXiv preprintC. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, \"Intriguing properties of neural networks,\" arXiv preprint arXiv:1312.6199, 2013.\n\nAdversarial patch. T B Brown, D Man\u00e9, A Roy, M Abadi, J Gilmer, arXiv:1712.09665arXiv preprintT. B. Brown, D. Man\u00e9, A. Roy, M. Abadi, and J. Gilmer, \"Adversarial patch,\" arXiv preprint arXiv:1712.09665, 2017.\n\nComponents for face recognition. B Heisele, T Koshizen, Sixth IEEE International Conference on Automatic Face and Gesture Recognition. B. Heisele and T. Koshizen, \"Components for face recognition,\" in Sixth IEEE International Conference on Automatic Face and Gesture Recognition, 2004. Proceedings. IEEE, 2004, pp. 153-158.\n\nAccessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. M Sharif, S Bhagavatula, L Bauer, M K Reiter, Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. the 2016 ACM SIGSAC Conference on Computer and Communications SecurityACMM. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, \"Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition,\" in Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. ACM, 2016, pp. 1528-1540.\n\nAdversarial examples in the physical world. A Kurakin, I Goodfellow, S Bengio, arXiv:1607.02533arXiv preprintA. Kurakin, I. Goodfellow, and S. Bengio, \"Adversarial examples in the physical world,\" arXiv preprint arXiv:1607.02533, 2016.\n\nReview of artificial intelligence adversarial attack and defense technologies. S Qiu, Q Liu, S Zhou, C Wu, Applied Sciences. 95909S. Qiu, Q. Liu, S. Zhou, and C. Wu, \"Review of artificial intelligence adversarial attack and defense technologies,\" Applied Sciences, vol. 9, no. 5, p. 909, 2019.\n\nExplaining and harnessing adversarial examples. I J Goodfellow, J Shlens, C Szegedy, abs/1412.6572CoRR. I. J. Goodfellow, J. Shlens, and C. Szegedy, \"Explaining and harnessing adversarial examples,\" CoRR, vol. abs/1412.6572, 2014.\n\nBoosting adversarial attacks with momentum. Y Dong, F Liao, T Pang, H Su, J Zhu, X Hu, J Li, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionY. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li, \"Boosting adversarial attacks with momentum,\" in Proceedings of the IEEE confer- ence on computer vision and pattern recognition, 2018, pp. 9185-9193.\n\nSynthesizing robust adversarial examples. A Athalye, L Engstrom, A Ilyas, K Kwok, ICML. A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok, \"Synthesizing robust adversarial examples,\" in ICML, 2017.\n\nJoint face detection and alignment using multitask cascaded convolutional networks. K Zhang, Z Zhang, Z Li, Y Qiao, IEEE Signal Processing Letters. 2310K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, \"Joint face detection and alignment using multitask cascaded convolutional networks,\" IEEE Signal Processing Letters, vol. 23, no. 10, pp. 1499-1503, 2016.\n\nSpatial transformer networks. M Jaderberg, K Simonyan, A Zisserman, Advances in neural information processing systems. M. Jaderberg, K. Simonyan, A. Zisserman et al., \"Spatial transformer networks,\" in Advances in neural information processing systems, 2015, pp. 2017-2025.\n\nNonlinear total variation based noise removal algorithms. L I Rudin, S Osher, E Fatemi, Physica D: nonlinear phenomena. 601-4L. I. Rudin, S. Osher, and E. Fatemi, \"Nonlinear total variation based noise removal algorithms,\" Physica D: nonlinear phenomena, vol. 60, no. 1-4, pp. 259-268, 1992.\n\nLearning face representation from scratch. D Yi, Z Lei, S Liao, S Z Li, arXiv:1411.7923arXiv preprintD. Yi, Z. Lei, S. Liao, and S. Z. Li, \"Learning face representation from scratch,\" arXiv preprint arXiv:1411.7923, 2014.\n", "annotations": {"author": "[{\"end\":149,\"start\":106},{\"end\":198,\"start\":150},{\"end\":253,\"start\":199},{\"end\":306,\"start\":254},{\"end\":376,\"start\":307},{\"end\":446,\"start\":377},{\"end\":516,\"start\":447},{\"end\":586,\"start\":517},{\"end\":656,\"start\":587}]", "publisher": null, "author_last_name": "[{\"end\":120,\"start\":114},{\"end\":167,\"start\":159},{\"end\":220,\"start\":208},{\"end\":273,\"start\":264}]", "author_first_name": "[{\"end\":113,\"start\":106},{\"end\":158,\"start\":150},{\"end\":201,\"start\":199},{\"end\":207,\"start\":202},{\"end\":263,\"start\":254}]", "author_affiliation": "[{\"end\":375,\"start\":308},{\"end\":445,\"start\":378},{\"end\":515,\"start\":448},{\"end\":585,\"start\":518},{\"end\":655,\"start\":588}]", "title": "[{\"end\":103,\"start\":1},{\"end\":759,\"start\":657}]", "venue": null, "abstract": "[{\"end\":1918,\"start\":828}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2353,\"start\":2350},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2588,\"start\":2585},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2933,\"start\":2930},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3337,\"start\":3334},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3473,\"start\":3470},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5433,\"start\":5430},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5438,\"start\":5435},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5891,\"start\":5888},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6276,\"start\":6273},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6842,\"start\":6839},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7181,\"start\":7177},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8512,\"start\":8508},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10529,\"start\":10525},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12040,\"start\":12036},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13235,\"start\":13232},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13858,\"start\":13854}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":19177,\"start\":19121},{\"attributes\":{\"id\":\"fig_1\"},\"end\":19211,\"start\":19178},{\"attributes\":{\"id\":\"fig_2\"},\"end\":19378,\"start\":19212},{\"attributes\":{\"id\":\"fig_3\"},\"end\":19461,\"start\":19379},{\"attributes\":{\"id\":\"fig_4\"},\"end\":19536,\"start\":19462},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":19935,\"start\":19537}]", "paragraph": "[{\"end\":2471,\"start\":1937},{\"end\":3121,\"start\":2473},{\"end\":3457,\"start\":3123},{\"end\":3803,\"start\":3459},{\"end\":4266,\"start\":3805},{\"end\":4319,\"start\":4268},{\"end\":4777,\"start\":4321},{\"end\":4976,\"start\":4820},{\"end\":5537,\"start\":5018},{\"end\":5715,\"start\":5539},{\"end\":6155,\"start\":5717},{\"end\":6268,\"start\":6176},{\"end\":6488,\"start\":6270},{\"end\":6834,\"start\":6519},{\"end\":7172,\"start\":6836},{\"end\":7528,\"start\":7174},{\"end\":7745,\"start\":7530},{\"end\":7992,\"start\":7794},{\"end\":8513,\"start\":8013},{\"end\":8772,\"start\":8515},{\"end\":8851,\"start\":8774},{\"end\":9397,\"start\":8853},{\"end\":9623,\"start\":9399},{\"end\":9946,\"start\":9705},{\"end\":10317,\"start\":10007},{\"end\":10806,\"start\":10350},{\"end\":11344,\"start\":10808},{\"end\":11701,\"start\":11388},{\"end\":12223,\"start\":11764},{\"end\":12493,\"start\":12279},{\"end\":12654,\"start\":12540},{\"end\":12683,\"start\":12656},{\"end\":12843,\"start\":12731},{\"end\":13105,\"start\":12845},{\"end\":13321,\"start\":13136},{\"end\":13396,\"start\":13362},{\"end\":13473,\"start\":13421},{\"end\":13572,\"start\":13493},{\"end\":13611,\"start\":13604},{\"end\":13934,\"start\":13678},{\"end\":14445,\"start\":13962},{\"end\":14811,\"start\":14447},{\"end\":15084,\"start\":14813},{\"end\":15342,\"start\":15124},{\"end\":15953,\"start\":15371},{\"end\":16758,\"start\":15955},{\"end\":17009,\"start\":16773},{\"end\":17354,\"start\":17011},{\"end\":17912,\"start\":17372},{\"end\":18357,\"start\":17914},{\"end\":19120,\"start\":18391}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6518,\"start\":6489},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7793,\"start\":7746},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9704,\"start\":9624},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10006,\"start\":9947},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10349,\"start\":10318},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11763,\"start\":11702},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12278,\"start\":12224},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12539,\"start\":12494},{\"attributes\":{\"id\":\"formula_8\"},\"end\":12730,\"start\":12684},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13135,\"start\":13106},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13361,\"start\":13322},{\"attributes\":{\"id\":\"formula_11\"},\"end\":13420,\"start\":13397},{\"attributes\":{\"id\":\"formula_12\"},\"end\":13624,\"start\":13612}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":17117,\"start\":17110}]", "section_header": "[{\"end\":1935,\"start\":1920},{\"end\":4818,\"start\":4780},{\"end\":5016,\"start\":4979},{\"end\":6174,\"start\":6158},{\"end\":8011,\"start\":7995},{\"end\":11386,\"start\":11347},{\"end\":13491,\"start\":13476},{\"end\":13582,\"start\":13575},{\"end\":13592,\"start\":13585},{\"end\":13602,\"start\":13595},{\"end\":13652,\"start\":13626},{\"end\":13676,\"start\":13655},{\"end\":13960,\"start\":13937},{\"end\":15122,\"start\":15087},{\"end\":15369,\"start\":15345},{\"end\":16771,\"start\":16761},{\"end\":17370,\"start\":17357},{\"end\":18389,\"start\":18360},{\"end\":19130,\"start\":19122},{\"end\":19187,\"start\":19179},{\"end\":19221,\"start\":19213},{\"end\":19388,\"start\":19380},{\"end\":19471,\"start\":19463},{\"end\":19555,\"start\":19538}]", "table": "[{\"end\":19935,\"start\":19711}]", "figure_caption": "[{\"end\":19177,\"start\":19132},{\"end\":19211,\"start\":19189},{\"end\":19378,\"start\":19223},{\"end\":19461,\"start\":19390},{\"end\":19536,\"start\":19473},{\"end\":19711,\"start\":19557}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10581,\"start\":10575},{\"end\":13472,\"start\":13466},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15772,\"start\":15766},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15783,\"start\":15777},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16252,\"start\":16246},{\"end\":16266,\"start\":16260},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16437,\"start\":16431},{\"end\":16448,\"start\":16442}]", "bib_author_first_name": "[{\"end\":20003,\"start\":20002},{\"end\":20011,\"start\":20010},{\"end\":20018,\"start\":20017},{\"end\":20025,\"start\":20024},{\"end\":20435,\"start\":20434},{\"end\":20446,\"start\":20445},{\"end\":20457,\"start\":20456},{\"end\":20470,\"start\":20469},{\"end\":20479,\"start\":20478},{\"end\":20488,\"start\":20487},{\"end\":20502,\"start\":20501},{\"end\":20731,\"start\":20730},{\"end\":20733,\"start\":20732},{\"end\":20742,\"start\":20741},{\"end\":20750,\"start\":20749},{\"end\":20757,\"start\":20756},{\"end\":20766,\"start\":20765},{\"end\":20955,\"start\":20954},{\"end\":20966,\"start\":20965},{\"end\":21335,\"start\":21334},{\"end\":21345,\"start\":21344},{\"end\":21360,\"start\":21359},{\"end\":21369,\"start\":21368},{\"end\":21371,\"start\":21370},{\"end\":21847,\"start\":21846},{\"end\":21858,\"start\":21857},{\"end\":21872,\"start\":21871},{\"end\":22119,\"start\":22118},{\"end\":22126,\"start\":22125},{\"end\":22133,\"start\":22132},{\"end\":22141,\"start\":22140},{\"end\":22383,\"start\":22382},{\"end\":22385,\"start\":22384},{\"end\":22399,\"start\":22398},{\"end\":22409,\"start\":22408},{\"end\":22611,\"start\":22610},{\"end\":22619,\"start\":22618},{\"end\":22627,\"start\":22626},{\"end\":22635,\"start\":22634},{\"end\":22641,\"start\":22640},{\"end\":22648,\"start\":22647},{\"end\":22654,\"start\":22653},{\"end\":23055,\"start\":23054},{\"end\":23066,\"start\":23065},{\"end\":23078,\"start\":23077},{\"end\":23087,\"start\":23086},{\"end\":23293,\"start\":23292},{\"end\":23302,\"start\":23301},{\"end\":23311,\"start\":23310},{\"end\":23317,\"start\":23316},{\"end\":23588,\"start\":23587},{\"end\":23601,\"start\":23600},{\"end\":23613,\"start\":23612},{\"end\":23891,\"start\":23890},{\"end\":23893,\"start\":23892},{\"end\":23902,\"start\":23901},{\"end\":23911,\"start\":23910},{\"end\":24169,\"start\":24168},{\"end\":24175,\"start\":24174},{\"end\":24182,\"start\":24181},{\"end\":24190,\"start\":24189},{\"end\":24192,\"start\":24191}]", "bib_author_last_name": "[{\"end\":20008,\"start\":20004},{\"end\":20015,\"start\":20012},{\"end\":20022,\"start\":20019},{\"end\":20035,\"start\":20026},{\"end\":20443,\"start\":20436},{\"end\":20454,\"start\":20447},{\"end\":20467,\"start\":20458},{\"end\":20476,\"start\":20471},{\"end\":20485,\"start\":20480},{\"end\":20499,\"start\":20489},{\"end\":20509,\"start\":20503},{\"end\":20739,\"start\":20734},{\"end\":20747,\"start\":20743},{\"end\":20754,\"start\":20751},{\"end\":20763,\"start\":20758},{\"end\":20773,\"start\":20767},{\"end\":20963,\"start\":20956},{\"end\":20975,\"start\":20967},{\"end\":21342,\"start\":21336},{\"end\":21357,\"start\":21346},{\"end\":21366,\"start\":21361},{\"end\":21378,\"start\":21372},{\"end\":21855,\"start\":21848},{\"end\":21869,\"start\":21859},{\"end\":21879,\"start\":21873},{\"end\":22123,\"start\":22120},{\"end\":22130,\"start\":22127},{\"end\":22138,\"start\":22134},{\"end\":22144,\"start\":22142},{\"end\":22396,\"start\":22386},{\"end\":22406,\"start\":22400},{\"end\":22417,\"start\":22410},{\"end\":22616,\"start\":22612},{\"end\":22624,\"start\":22620},{\"end\":22632,\"start\":22628},{\"end\":22638,\"start\":22636},{\"end\":22645,\"start\":22642},{\"end\":22651,\"start\":22649},{\"end\":22657,\"start\":22655},{\"end\":23063,\"start\":23056},{\"end\":23075,\"start\":23067},{\"end\":23084,\"start\":23079},{\"end\":23092,\"start\":23088},{\"end\":23299,\"start\":23294},{\"end\":23308,\"start\":23303},{\"end\":23314,\"start\":23312},{\"end\":23322,\"start\":23318},{\"end\":23598,\"start\":23589},{\"end\":23610,\"start\":23602},{\"end\":23623,\"start\":23614},{\"end\":23899,\"start\":23894},{\"end\":23908,\"start\":23903},{\"end\":23918,\"start\":23912},{\"end\":24172,\"start\":24170},{\"end\":24179,\"start\":24176},{\"end\":24187,\"start\":24183},{\"end\":24195,\"start\":24193}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":8923541},\"end\":20390,\"start\":19937},{\"attributes\":{\"doi\":\"arXiv:1312.6199\",\"id\":\"b1\"},\"end\":20709,\"start\":20392},{\"attributes\":{\"doi\":\"arXiv:1712.09665\",\"id\":\"b2\"},\"end\":20919,\"start\":20711},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":13088356},\"end\":21244,\"start\":20921},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":207241700},\"end\":21800,\"start\":21246},{\"attributes\":{\"doi\":\"arXiv:1607.02533\",\"id\":\"b5\"},\"end\":22037,\"start\":21802},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":116538444},\"end\":22332,\"start\":22039},{\"attributes\":{\"doi\":\"abs/1412.6572\",\"id\":\"b7\",\"matched_paper_id\":6706414},\"end\":22564,\"start\":22334},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4119221},\"end\":23010,\"start\":22566},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2645819},\"end\":23206,\"start\":23012},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":10585115},\"end\":23555,\"start\":23208},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6099034},\"end\":23830,\"start\":23557},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":13133466},\"end\":24123,\"start\":23832},{\"attributes\":{\"doi\":\"arXiv:1411.7923\",\"id\":\"b13\"},\"end\":24346,\"start\":24125}]", "bib_title": "[{\"end\":20000,\"start\":19937},{\"end\":20952,\"start\":20921},{\"end\":21332,\"start\":21246},{\"end\":22116,\"start\":22039},{\"end\":22380,\"start\":22334},{\"end\":22608,\"start\":22566},{\"end\":23052,\"start\":23012},{\"end\":23290,\"start\":23208},{\"end\":23585,\"start\":23557},{\"end\":23888,\"start\":23832}]", "bib_author": "[{\"end\":20010,\"start\":20002},{\"end\":20017,\"start\":20010},{\"end\":20024,\"start\":20017},{\"end\":20037,\"start\":20024},{\"end\":20445,\"start\":20434},{\"end\":20456,\"start\":20445},{\"end\":20469,\"start\":20456},{\"end\":20478,\"start\":20469},{\"end\":20487,\"start\":20478},{\"end\":20501,\"start\":20487},{\"end\":20511,\"start\":20501},{\"end\":20741,\"start\":20730},{\"end\":20749,\"start\":20741},{\"end\":20756,\"start\":20749},{\"end\":20765,\"start\":20756},{\"end\":20775,\"start\":20765},{\"end\":20965,\"start\":20954},{\"end\":20977,\"start\":20965},{\"end\":21344,\"start\":21334},{\"end\":21359,\"start\":21344},{\"end\":21368,\"start\":21359},{\"end\":21380,\"start\":21368},{\"end\":21857,\"start\":21846},{\"end\":21871,\"start\":21857},{\"end\":21881,\"start\":21871},{\"end\":22125,\"start\":22118},{\"end\":22132,\"start\":22125},{\"end\":22140,\"start\":22132},{\"end\":22146,\"start\":22140},{\"end\":22398,\"start\":22382},{\"end\":22408,\"start\":22398},{\"end\":22419,\"start\":22408},{\"end\":22618,\"start\":22610},{\"end\":22626,\"start\":22618},{\"end\":22634,\"start\":22626},{\"end\":22640,\"start\":22634},{\"end\":22647,\"start\":22640},{\"end\":22653,\"start\":22647},{\"end\":22659,\"start\":22653},{\"end\":23065,\"start\":23054},{\"end\":23077,\"start\":23065},{\"end\":23086,\"start\":23077},{\"end\":23094,\"start\":23086},{\"end\":23301,\"start\":23292},{\"end\":23310,\"start\":23301},{\"end\":23316,\"start\":23310},{\"end\":23324,\"start\":23316},{\"end\":23600,\"start\":23587},{\"end\":23612,\"start\":23600},{\"end\":23625,\"start\":23612},{\"end\":23901,\"start\":23890},{\"end\":23910,\"start\":23901},{\"end\":23920,\"start\":23910},{\"end\":24174,\"start\":24168},{\"end\":24181,\"start\":24174},{\"end\":24189,\"start\":24181},{\"end\":24197,\"start\":24189}]", "bib_venue": "[{\"end\":20178,\"start\":20116},{\"end\":21537,\"start\":21467},{\"end\":22800,\"start\":22738},{\"end\":20114,\"start\":20037},{\"end\":20432,\"start\":20392},{\"end\":20728,\"start\":20711},{\"end\":21054,\"start\":20977},{\"end\":21465,\"start\":21380},{\"end\":21844,\"start\":21802},{\"end\":22162,\"start\":22146},{\"end\":22436,\"start\":22432},{\"end\":22736,\"start\":22659},{\"end\":23098,\"start\":23094},{\"end\":23354,\"start\":23324},{\"end\":23674,\"start\":23625},{\"end\":23950,\"start\":23920},{\"end\":24166,\"start\":24125}]"}}}, "year": 2023, "month": 12, "day": 17}