{"id": 249694784, "updated": "2022-12-16 18:05:09.252", "metadata": {"title": "Code comment generation based on graph neural network enhanced transformer model for code understanding in open-source software ecosystems", "authors": "[{\"first\":\"Li\",\"last\":\"Kuang\",\"middle\":[]},{\"first\":\"Cong\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Xiaoxian\",\"last\":\"Yang\",\"middle\":[]}]", "venue": "Automated Software Engineering", "journal": "Automated Software Engineering", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "In open-source software ecosystems, the scale of source code is getting larger and larger, and developers often use various methods (good code comments or method names, etc.) to make the code easier to read and understand. However, high-quality code comments or method names are often unavailable due to tight project schedules or other reasons in open-source software ecosystems such as Github. Therefore, in this work, we try to use deep learning models to generate appropriate code comments or method names to help software development and maintenance, which requires a non-trivial understanding of the code. Therefore, we propose a Graph neural network enhanced Transformer model (GTrans for short) to learn code representation to understand code better. Specifically, GTrans learns code representation from code sequences and graphs. We use a Transformer encoder to capture the global representation from code sequence and a graph neural network (GNN) encoder to focus on the local details in the code graph, and then use a decoder to combine both global and local representations by attention mechanism. We use three public datasets collected from GitHub to evaluate our model. In an extensive evaluation, we show that GTrans outperforms the state-of-the-art models up to 3.8% increase in METEOR metrics on code comment generation and outperforms the state-of-the-art models by margins of 5.8%\u20139.4% in ROUGE metrics on method name generation after some adjustments on the structure. Empirically, we find the method name generation task depends on more local information than global, and the code comment generation task is in contrast. Our data and code are available at https://github.com/zc-work/GTrans.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/ase/KuangZY22", "doi": "10.1007/s10515-022-00341-1"}}, "content": {"source": {"pdf_hash": "9169d449b022ceea7b82fc8d22a958408647d412", "pdf_src": "SpringerNature", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "71391456df90c008293f2d58d07d6e08acc15255", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9169d449b022ceea7b82fc8d22a958408647d412.txt", "contents": "\nCode comment generation based on graph neural network enhanced transformer model for code understanding in open-source software ecosystems\n0123456789\n\nLi Kuang \n\u00b7 Cong Zhou \n\u00b7 Xiaoxian Yang \nCode comment generation based on graph neural network enhanced transformer model for code understanding in open-source software ecosystems\n012345678910.1007/s10515-022-00341-1Received: 15 July 2021 / Accepted: 11 April 2022 Automated Software Engineering (2022) 29:43 1 3 43Automated Software Engineering (2022) 29:43 1 3 Extended author information available on the last page of the article Page 2 of 27Open-source software ecosystems \u00b7 Code representation \u00b7 Transformer \u00b7 Graph neural network\nIn open-source software ecosystems, the scale of source code is getting larger and larger, and developers often use various methods (good code comments or method names, etc.) to make the code easier to read and understand. However, high-quality code comments or method names are often unavailable due to tight project schedules or other reasons in open-source software ecosystems such as Github. Therefore, in this work, we try to use deep learning models to generate appropriate code comments or method names to help software development and maintenance, which requires a non-trivial understanding of the code. Therefore, we propose a Graph neural network enhanced Transformer model (GTrans for short) to learn code representation to understand code better. Specifically, GTrans learns code representation from code sequences and graphs. We use a Transformer encoder to capture the global representation from code sequence and a graph neural network (GNN) encoder to focus on the local details in the code graph, and then use a decoder to combine both global and local representations by attention mechanism. We use three public datasets collected from GitHub to evaluate our model. In an extensive evaluation, we show that GTrans outperforms the state-of-the-art models up to 3.8% increase in METEOR metrics on code comment generation and outperforms the state-of-the-art models by margins of 5.8%-9.4% in ROUGE metrics on method name generation after some adjustments on the structure. Empirically, we find the method name generation task depends on more local information than global, and the code comment generation task is in contrast. Our data and code are available at https:// github. com/ zc-work/ GTrans.\n\nIntroduction\n\nIn the open-source software ecosystem, the scale of available data (such as source code tokens, metadata, bug fixes, and review processes) is massive, which attracted many researchers to deal with various problems in software ecosystems. And the development and maintenance of software requires the participation of a large number of software developers. However, developing software is a costly process (Allamanis et al. 2018) and previous work (Xia et al. 2017) found that program comprehension will take up as much as half of a developers' time during software development and maintenance. Unfortunately, many projects in opensource software ecosystem (such as Github) have mismatched, missing, or outdated code comments due to tight project schedules and other reasons (Hu et al. 2018a). Besides, improper method names are common due to inappropriate code cloning or other reasons, and an empirical study (Murphy-Hill et al. 2011) shows that the most frequently refactoring operation is rename method during program development. Improper comments or method names would make the software hard to understand. Therefore, the automatic generation of code summarization, in the form of code comment or method name, will greatly improve the readability and maintainability of code.\n\nWe focus on modeling the relation between source code and natural language and generating a natural language sequence from a given source code snippet. Therefore, we use deep learning models to learn code representation to understand code better. However, it is not that easy to learn code representation properly due to the complexity of code. We roughly divide the existing methods of code summarization into three categories:\n\n1. Learning code representation from code sequence: Previous works (Iyer et al. 2016;Hu et al. 2018b;Allamanis et al. 2016;Wan et al. 2018) treat code as text and input code into a Recurrent Neural Network(RNN) based encoder or a Convolutional Neural Network(CNN) based encoder, and then decode with an attention mechanism (Luong et al. 2015) to generate a summary, but these traditional models struggle to capture long-range dependencies between code tokens. And this is in contrast to Transformer (Vaswani et al. 2017), which leverages selfattention to capture long-range dependencies. Therefore, more and more related works (Ahmad et al. 2020;Lin et al. 2021;Chirkova and Troshin 2020) are based on Transformer and outperform traditional models. However, Transformer leverages self-attention to allow explicit communication between two distant nodes, ignoring the graph topology since all nodes are directly connected, thus it is said to be good at capturing global representation (Hellendoorn et al. 2019). 2. Learning code representation from code structure: Code can be parsed into Abstract Syntax Tree (AST), which depicts the code's syntactic structure, and the structure is also critical for code summarization (Hu et al. 2018a). Considering the structure of code, some works (Hu et al. 2018a(Hu et al. , 2020LeClair et al. 2019;Zhang et al. 2020) try to utilize AST structure by traversing the AST in some order. Other works (Alon et al. , 2019 model the paths in the AST to learn code structure. But the serialized AST and the paths in AST cannot represent structure information fully (Lin et al. 2021). To learn code structure better, some recent works (Allamanis et al. 2017;LeClair et al. 2020) propose to use GNN to model AST directly, which show a powerful representation of code and improve performance. However, GNN models consider the relations between neighbor nodes when capturing the graph structure, so they capture well local dependencies in code (Hellendoorn et al. 2019;Gao et al. 2021;Z\u00fcgner et al. 2021), but fail to have a global view on the code and capture long-range relations (Ribeiro et al. 2020). 3. Combining code structure and code sequence: Realizing that GNN models can capture well local features in code and Transformer have a global view of code, a few works (Fernandes et al. 2019;Hellendoorn et al. 2020) try to combine these two models. But their works only focus on the encoder part and try to build a powerful encoder by stacking the original Transformer layer and GNN layer in some order, which does not change the original layer structure. Therefore, their combinations are between layers, which are in large granularity. Besides, the performance of their models can be further improved on code comment generation or method name generation (Chirkova and Troshin 2020).\n\nIn this paper, we investigate a novel code summarization architecture that efficiently combines both global, unstructured and local, structured models. Different from previous works, our work focuses on the decoder, and we consider in-layer integration of the global and local representations for further fine-grained integration. Specifically, we propose a Graph neural network enhanced Transformer model (GTrans) to learn code representation to understand code better from code sequence and code graph. The code graph is built from the parsed AST, augment with some control-flow and data-flow edges. The GTrans employs a gated graph neural network (GGNN) to learn representations of graph nodes based on their neighboring nodes, thus obtaining the local representation of code. Meanwhile, GTrans uses a Transformer encoder to compute each code token representation based on all tokens in code sequence, thus obtaining the global representation of code. Finally, the GTrans leverages a modified decoder to combine these two representations by attention mechanism in each layer to generate a summary. We show that our proposed model outperforms the state-of-the-art models on code comment generation. And with some adjustments to the model structure to emphasize the local representation, the modified model performs the best on method name generation. Our experiments on two summarization tasks show that method name generation depends on more structural information than sequential, and code comment generation is the opposite. It is not appropriate to solve these two tasks by one approach.\n\nThe main contributions of our work can be summarized as follows:\n\n\u2022 We present GTrans for the code comment generation task, which efficiently combines both global and local representations of code in each layer of decoder by attention mechanism. It is biased towards the global representation of the code. \u2022 We propose a modified GTrans for the method naming task. The modified GTrans emphasizes the local representation by changing the order of attention and the way of copying. \u2022 We carry out extensive experiments on public datasets. The experiment results\n\nshow that our models outperform the state-of-the-art models on code comment generation and method naming. \u2022 We find that method name generation task might depend on more structural information than sequential, and code comment generation task might be the opposite.\n\nThe remainder of this paper is organized as follows: Sect. 2 presents the related work. Section 3 elaborates on our proposed approach in detail. We show our experiments and analyze the results in Sect. 4. Finally, we conclude the work of this paper in Sect. 5.\n\n\nRelated work\n\nMethod name generation task is to infer the name of a method function as accurately and concisely as possible (Allamanis et al. 2016). While code comment generation task is to predict a succinct and meaningful summary to descript the functionality of a method (Barone and Sennrich 2017). The two tasks are related to the source code summarization and seem similar.\n\n\nMethod name generation\n\nRecent works on method name generation focus on code structure. Allamanis et al. (2016) proposed a convolutional attention model to extract structure features from code. Meanwhile, a copy mechanism is used to copy some words from code as part of the summary.  proposed a general path-based representation for programs that is purely structural. They presented a program element using the set of paths in its AST that contain it. Based on this idea, Alon et al. (2019) proposed Code2vec approach to present the whole snippets of code rather than a single program element. They used attention mechanism to aggregate AST path representations and produced a code vector to predict the method name. But Code2vec cannot represent the paths that do not appear in the training dataset.  further developed Code2seq to decompose AST paths as a collection of their nodes. And Code2seq can generate a sequence rather than a single label compared to Code2vec.\n\nTo learn code structure better, Xu et al. (2019) proposed a hierarchical attention network to learn the hierarchical structure of methods. Fernandes et al. (2019) used a graph neural network to learn code representation from code graph.\n\n\nCode comment generation\n\nMost of the recent approaches on code comment generation model programs as sequences of tokens and use recurrent seq2seq networks with attention mechanisms to generate code summarization (Iyer et al. 2016;Allamanis et al. 2016). Hu et al. (2018b) captured the API knowledge in the source code and applied it to code summarization. Wan et al. (2018) used reinforcement learning to overcome the exposure bias issue in the encoder-decoder framework. And Wei et al. (2019) regarded code generation as a dual task of code summarization and proposed a dual framework to train the two tasks simultaneously. Ahmad et al. (2020) employ Transformer (Vaswani et al. 2017) to extract code features incorporating relative position encoding (Shaw et al. 2018) and copy mechanism (See et al. 2017). But these models treat code as a sequence and ignore the structure information.\n\nSome recent works leverage structural information using a flattened version of the AST as model input. Hu et al. (2018a) proposed a Structure-based Traversal (SBT) method for flattening AST into a sequence that combining source code tokens and AST structure and showed improvement over the sequence-based methods. To expand on this idea, Hu et al. (2020) used two different encoders to learn the code sequence and code structure information, respectively. LeClair et al. (2019) used the SBT sequence and decouple the structure of the code from the code itself to learn better structure representation. The limitation of these methods is that the flattened ASTs cannot fully represent structure information.\n\nTo overcome this problem, some works utilize structural information of a program using tree structure recurrent models (Eriguchi et al. 2016;Shido et al. 2019;Chen et al. 2018). And finally, Allamanis et al. (2017) proposed using GGNNs to learn code graphs with control-flow and data-flow edges. Fernandes et al. (2019) combined an RNN and a GNN architecture, where the RNN representation is first obtained, which is then used as input for the GNN model. It is worth noting that they tried combined Transformer and GGNN to do METHODDOC(i.e. comment generation) in their experiments but get worse performance. LeClair et al. (2020) also combined an RNN and a GNN model in a different way. They used an RNN for the code sequences and a GNN for the ASTs and concatenated them before the decoder. Hellendoorn et al. (2020) combined Transformer with GGNN by alternating Transformer and GGNN layers and outperformed standard Transformer in the variable misuse task. However, their approach performed poorly on method name generation in the experiments of Chirkova and Troshin (2020). Recently, some pre-trained models such as CodeBERT ) and GraphCode-BERT  have been proposed to learn code representation, but them do not make use of AST paths.\n\nIn this paper, we want to study the effective incorporation of code structure into the Transformer, which was mentioned in the future work of Ahmad et al. (2020). We combine sequence-based (Transformer) and graph-based (GNNs) representations of programs to do code summarization, and we include more diverse hybrids, as well as entirely different combinations of structural and sequential features.\n\n\n43\n\nPage 6 of 27\n\n\nProposed approach\n\nTo capture the global representation, we extend the Transformer model proposed by Ahmad et al. (2020), which added copy mechanism and relative position representations to the vanilla Transformer model. The copy mechanism allows the model to generate words from vocabulary and copying from the input source code. And the relative position representation allows the model to learn the semantics of the code better.\n\nTo extract the local representation, we follow Fernandes et al. (2019) to construct code graphs from ASTs with some control-flow and data-flow edges. Then we use a GGNN model to learn code embeddings from code graphs.\n\nTo combine global and local representations, we extend the original Transformer decoder layer and integrate the two representations by attention mechanism in each layer for fine-grained integration. The useful information from global and local representations will be accumulated through layers by the attention mechanism.\n\n\nArchitecture\n\nThe whole framework of our approach is illustrated in Fig. 1. We transform the code to graphs (i.e., ASTs) and code token sequences, respectively. Suppose that the code tokens in code are C = [c 1 , c 2 , c 3 , \u2026 , c n ] , and these tokens are used by code encoder later. The code graph is\nG = (V, E) , where E is a list of directed edge sets in the graph, V is a set of nodes in the graph. Specifically, V = [c 1 , c 2 , c 3 , \u2026 , c n , s n+1 , s n+2 , s n+m ] ,\nwhere c represent terminals (corresponding to leaf nodes, i.e., code tokens) and s represent non-terminals (corresponding to non-leaf nodes).\n\nOur goal is to implement a hybrid model capable of combining global and local features of the input code. Inspired by (LeClair et al. 2019(LeClair et al. , 2020, we treat code tokens and AST representation separately. The graph node sequences and graph edges are fed into the graph encoder to learn the local information. And the code token sequences are fed into the code encoder to learn the global information. In the decoder part, we modify the Transformer decoder layer. We use two attention modules in each layer to compute the attention over the two representations by order. When generating summaries, the useful information from two representations will be selected to form the summary token embedding with the help of the attention mechanism in each layer. And the summary token embedding of the last layer represent the integration of global and local representations and will be used to predict the next summary token.\n\n\nGraph encoder\n\nConsidering that traversing the AST cannot fully represent structure information, and the graph structure can learn the complex structure information better than the sequence structure, we use GGNN model, which has shown powerful representation of code (Allamanis et al. 2017;Fernandes et al. 2019; Hellendoorn et al.\n\n\nFig. 1\n\nThe overall framework of GTrans 2020), to learn the structure information in AST. For a code graph G = (V, E) , E = (e 1 , e 2 \u2026 e k ) is a set of edges, where k is the number of edge types. Each v \u2208 V has a vector x v representing the features of the graph node, which is used as the initial state h (1) v of a node. Each node v sends messages to its neighbors along with different types of edges through a simple linear layer at each timestep t.\n\nAfter the message passing step, each node v aggregates information from the received messages by element-wise summation. N (v) is the neighbor set of node v.\n\nCombined with the previous timestep hidden state\nh (t\u22121) v , the current state vector h (t)\nv can calculated as follows:\n\nwhere GRU denotes the Gated Recurrent Unit introduced by Cho et al. (2014). We use the final step resulting that rolled out for T timesteps as GGNN output graph node representations.\n\n\nCode encoder\n\nWe use the Transformer encoder to extract code sequence information. There are two modules in the code encoder: Self-Multi-head Attention and Feed Forward.\n\n(1) Self-Multi-head Attention: To learn code representation better, we follow Ahmad et al. (2020) and incorporate relative position encoding into self-attention. Each code token i \u2208 C has a vector x i representing the token feature. The output vector o head i of each attention head is calculated as follows:\n\nwhere\nW Q , W K , W V \u2208 R (d model * d k )\nare parameters that are unique for each layer and attention head, and the embedding size d k = d model \u2215h . The a K ij and a V ij are used to present relative position for i and j. The maximum relative position is\n(1) m (t) u = Liner k (h (t\u22121) v ) (2) M (t) u = \u2211 v\u2208N (v) m (t) u (3) h (t) v = GRU(M (t) u , h (t\u22121) v ) (4) e ij = x i W Q (x j W K + a K ij ) \u221a d k (5) ij = exp(e ij ) \u2211 n k=1 e ik (6) o head i = n \u2211 j=1 ij (x i W V + a V ij ), head = 1, 2 \u2026 h\nclipped to a maximum absolute value of k, and each relative position is computed as follows:\n\nWe learn relative position representations:\n(w k (\u2212k) , \u2026 , w k k ) , and (w V (\u2212k) , \u2026 , w V k )\n. Finally, we can get the output token feature o i of code token i by concatenating the output of each head.\n\nwhere W O \u2208 R (d model * d model ) are parameter matrices.\n\n(2) Feed-Forward Networks: This module is composed of two dense layers with a RELU activation in between:\n\nwhere W 1 , W 2 and b 1 , b 2 are weight matrices and bias for each layer. And o are the output code token features after the Self-Multi-head Attention layer.\n\nEach module also employs residual connection and layer normalization (Vaswani et al. 2017). For simplicity, we don't show these in Fig. 1. \n\n\nDecoder\n\nWe propose to combine GGNN and Transformer in the decoder. The GGNN model (i.e. graph encoder) outputs the graph node representation g, and the Self-MultiHead model (i.e. code encoder) outputs the code token representation h. In the decoder, we have two attention modules, one for the graph node representation g and another for code token representation h. The outputs from the two multi-head attention modules represent a merge of code structure and code sequence. Finally, the output is fed into a fully connected feed-forward network. The node representation in decoder layer is calculated as follows:\n(7) a K ij = w K (clip(j\u2212i,k)) , a V ij = w V (clip(j\u2212i,k)) (8) clip(x, k) = max(\u2212k, min(k, x)) (9) o i = Concat(o 1 i , \u2026 o h i )W O (10) FFN(o) = RELU(W 1 o + b 1 )W 2 + b 2 (11) g v = GGNN(x v , {x v \u2236 v \u2208 N(v)}) (12) h v = Self \u2212 MultiHead(x v , {x v \u2236 v \u2208 C}) (13) q l i = MultiHead(q l i , K g , V g ) (14) q l i = MultiHead(q l i , K h , V h )\nwhere x v is the embedding of node token v. {Q, K g \u2215K h , V g \u2215V h }are query, key, and value vectors and {K g \u2215K h , V g \u2215V h } are packed from the graph node representation g and code token representation h. The q l i is the embedding of summary token i after the self-multi-head attention layer 1 in the l th layer of decoder.\n\nFor the copy part, we use code token representation h and the last decoder layer output vectors q last i to compute the copy distribution. Finally, others are the same as the Transformer decoder (Ahmad et al. 2020) in predicting the next token in summary.\n\n\nModified GTrans\n\nWe observe a strange thing in the experiments of Fernandes et al. (2019). The vanilla Transformer, which captures global representation, performs so poorly on method name generation, even worse than the LSTM model, but it works well on code comment generation. However, the GGNN model, which captures local representation, performs well on method name generation but worse than vanilla Transformer on code comment generation. A similar thing happens in the experiments of Wang et al. (2020). The Transformer performers worse than other structured models on method naming, but it is still the best on code comment generation. We also note that the most recent works on method name generation focus on code structure. We study the datasets and find that the name generation task is usually keyword focus. The generated names are shorter and always some subtokens from the code, which might mean that the name generation task is more dependent on local information. However, the code comment task needs to generate a sentence to summarize the meaning of the whole code segment, which might mean that the code comment task is more dependent on global information.\n\nBased on these observations, we think that the method name generation and code comment generation may be different. So we hypothesize that the method name generation task depends on more structural information than sequential. In order to validate this hypothesis, we investigate a modified GTrans for method naming.\n\nIn order to keep the global and local representations unchanged, we do not change the code encoder and graph encoder. We have two changes on decoder to emphasize structure information:\n\n(1) GNN Last: We exchange the attention sequence order. The decoder computes attention over code token representation h first and then computes attention over graph node representation g later. The node representation is calculated as follows:\n(15) q (l+1) i = FFN(q l i ) (16) q l i = MultiHead(q l i , K h , V h )\nIntuitively, the most important information should be paid attention to in the back. Because the previous vector might be covered by the following attention. So, the node representation q (l+1) i is more relevant to graph node representation g.\n\n(2) Copy Graph: For the copy part, we use graph node representation g and last decoder layer output q last i to compute the copy distribution.\n\n\nExperiment\n\nWe conduct experiments to analyze the performance of GTrans. We focus on the following Research Questions (RQ):\n\nRQ1: How effective is the GTrans compared with the state-of-the-art approaches on code comment generation task and method naming task? RQ2: Is there evidence that the performance differences are due to the good performance of GGNN? RQ3: Is it more efficient to combine global and local representations in the decoder than in the encoder? RQ4: How does the GGNN model make a difference in the original Transformer? RQ5: What are the advantages of GTrans on the quality of generating a summary?\n\n\nSetup\n\nDatasets and Pre-processing We evaluate our approach on code comment generation task and method name generation task. For the first task, we choose a Java dataset (Hu et al. 2018b) and a Python dataset (Wan et al. 2018), and we reuse the dataset splits of Ahmad et al. (2020). The statistics of the two datasets are shown in Table 1. For the second task, we use the Java-small dataset of  and reuse their dataset splits. The three datasets are all collected from Github, and the statistics are shown in Table 2.\n(17) q l i = MultiHead(q l i , K g , V g ) (18) q (l+1) i = FFN(q l i )\nTo construct an AST graph from the code (sub)tokens, we follow the work of Fernandes et al. (2019). We parsed the codes and used edges to indicate that one node is a CHILD of the other. We also connect code tokens by order using a NEXTTOKEN edge. And for each full code token, we split the form CamelCase and pascal_case heuristics to respective sub-tokens and connect its sub-tokens using SUBTOKEN edges. Finally, we add LASTLEXICALUSE edges to connect tokens to their most recent use in the source code. For the code that can't be parsed, we simply construct a graph with the code tokens connected with NEXTTOKEN edges by order.\n\nMetrics For code comment generation task, we use BLEU (Papineni et al. 2002), METEOR (Banerjee and Lavie 2005), and ROUGE-L (Lin 2004), which are widely used in previous works to evaluate the performance of our approach. While in the training time, we also calculate precision, recall and f1 scores. The f1 score is computed as f1 = 2*(precision*recall)\u00f7(precision + recall).\n\nFor method name generation, we report ROUGE-1, ROUGE-2 and ROUGE-L scores (Lin 2004) to indicate the quality of generated method names.\n\nModel and Hyper-parameters Our GGNN encoder has 512 hidden embedding sizes and unrolled over 5 timesteps 2 . To ensure the fairness of the comparative experiments, we have the same configuration as Ahmad et al. (2020) in our Transformer encoder and decoder. Detailed hyperparameter settings can be found in \"Appendix A\".\n\nHardware The experiments are conducted on a single Tesla V100 GPU. Our proposed models are constructed by pytorch1.4.0 based on CUDA10.1 and cuDNN7.4.\n\n\nBaselines\n\nFor code comment generation task, we compare our approach with following eight methods:\n\n\u2022 CODE- NN Iyer et al. (2016) used an RNN with attention mechanism to summarize the code.  Wan et al. (2018) applied reinforcement learning to overcome the exposure bias issue in the encoder-decoder framework, and a hybrid attention component to fuse sequential and structural information. \u2022 DeepCom Hu et al. (2018a) used LSTM with attention mechanism to encode serialized ASTs from SBT. \u2022 API + CODE Hu et al. (2018b)  We compare our model with these baseline methods reported in Ahmad et al. (2020) and Chen et al. (2021). For method name generation, we compare our approach with following five methods: \n\n\nThe effectiveness of our approach (RQ1)\n\nWe show the experiment results on comment generation and method naming in Tables 3 and 4 separately, and the best results are marked in bold. For comment generation, we evaluate our approach on Java and Python datasets in terms of BLEU, METEOR and ROUGE-L, and compare it with eight baselines. As shown in Table 3, our model performs the best and achieves a BLEU score of 46.24, a METEOR score of 28.03, and a ROUGE-L score of 56.69 on the Java dataset, which outperforms the Transformer by 2.6%, 3.8%, 3.5% respectively. On the python dataset, we achieve higher METEOR and ROUGE-L scores and nearly the same score in BLEU. Transformer and GTrans have smaller differences on the Python dataset. We argue that the average length of the Python dataset is small than the Java dataset, so the GGNN encoder should capture more global information rather than local information under the fixed timesteps. Besides, we compare our best model to the models that adjusted on structure, which is mentioned in Sect. 3. If we copy from the graph, each indicator will drop a lot. If we exchange the attention sequence order and compute attention over graph representation last, each indicator will slightly lower compared to the best model. Our experiments show that if we emphasize the local representation, the model works worse on comment generation. Intuitively, the comment generation task depends on more sequential information than structural. Table 3 Results on the code comment generation task a We run the Transformer model with the original implementation on a single V100 GPU card and find the experiment results are higher than the original paper reported. For the sake of fairness, we report the higher results in our paper For method naming, we evaluate our approach on Java-small dataset in terms of ROUGE-1, ROUGE-2 and ROUGE-L, and compare it with five baselines. We can see from Table 4 that the Transformer performs not good, which is similar to the experiments of Fernandes et al. (2019) and . Equipped with GGNN, our model has improved significantly but still cannot compare with the stateof-the-art model. If we adjust the model structure and copy from the graph, each indicator has improved a lot. Based on that, if we exchange the attention sequence order and compute attention over graph representation last, the adjusted model performs best on Java-small dataset and improves relative ROUGE-1, ROUGE-2 and ROUGE-L score about 5.8%, 7.2% and 9.4% respectively. Our experiments show that if we emphasize the local representation, the model works best on method naming. Intuitively, the method naming task depends on more structural information than sequential.\n\nIn summary, the GTrans outperforms the eight baselines in terms of all the automated evaluation metrics on comment generation. With slight adjustments on the model structure to emphasize the local representation, GTrans also performs best on method name generation. Empirically, the method name generation task depends on more structural information than sequential, and code comment generation is the opposite.\n\n\nThe impact of GGNN (RQ2)\n\nTo verify whether the superiority of GTrans over existing methods is mainly caused by the use of the GGNN architecture, we remove the code encoder and keep the graph encoder in our approach. We call this a GGNN-only model. We show the training curves on the Java dataset under different epoch for the GGNN-only model, Transformer and GTrans in Fig. 2. Due to page limit, we omit similar results on Python and Java-small datasets.\n\nFrom the results, we can see that GTrans has much better performance than the Transformer and GGNN-only models. In all cases, GTrans equipped with GGNN perform well from very early on, and show a steady increase towards competitive precisions, recalls and f1 scores. The Transformer and GGNN-only models appear to be complementary. Even though the GGNN-only model's performance is not good compared to the Transformer, the combined model still achieves improvement. With the structure information, the hybrid model learns the code representation significantly better and faster.\n\nIn summary, the reason that GTrans outperforms existing methods is the combination of global and local representations. And the global and local representations are empirically complementary.\n\n\nThe impact of different combinations (RQ3)\n\nWe also investigate different combined architectures. Intuitively, we integrate the GGNN layer in the Transformer encoder, using the enhanced encoder to learn more rich code representation and then decode with the Transformer decoder. We tried 1 3\n\n\n43\n\nPage 16 of 27 two different methods for integrating global and local representations. The first is to concatenate vectors of the GGNN layer and Self-Multi-head attention layer, which we call a parallel representation. The second is to use the GGNN representation as an input of the Self-Multi-head layer. We call this approach a cascaded representation. We provide a concrete view of the modification in \"Appendix B\". We show the training curves of the two models compared with the Transformer model and our model on the Java dataset in Fig. 3. Due to page limit, we omit similar results on Python and Java-small datasets.\n\nWe can see the encoder-parallel and the encoder-cascaded perform almost the same, and they outperform the Transformer model in all cases except in BLEU. With the GGNN layer integrated into the encoder, the two models converge so fast and perform well from very early on but struggle to achieve a much higher BLEU score over time. And the two models perform worse than GTrans. Intuitively, the code structure information and code sequence information integrated into the decoder is more effective for source code summarization.\n\nIn summary, it is more efficient to combine global and local representations in the decoder than in the encoder for code summarization empirically.\n\n\nAttention analyze (RQ4)\n\nIn order to explore how does the GGNN model makes a difference in the original Transformer model, we analyze the attention difference between the GTrans and Transformer. We provide two Java examples to show the difference when these two models are decoding. The example-1 can be found in the first example of Fig. 6, and example-2 can be found in the first example of \"Appendix C\". To avoid being wordy, we do not show similar results on Python and Java-small datasets.\n\nWe add the weights of 8 attention heads of multi-head attention and present the attention distribution of the last decoder layer in Fig. 4. The abscissa corresponds to the code sequence, and the ordinate is each decoding input. So, we can see something different when the models operate in decoding. From Fig. 4a and c, we can see the Transformer model has basically unchanging attention weights for the original input in each decoding step. The high activation (dark red) positions which have high attention weights are always focused in most decoding time. However, the situation is different in GTrans. Figure 4b and d show the attention distribution in GTrans has more diversity. In each decoding step, GTrans tends to pay attention to different parts. GTrans model uses the code graph representation where the Transformer is not. And we speculate these situations are where the code graph is beneficial.\n\nIn summary, focusing on different parts in decoding steps makes the GTrans model perform better in large examples. \n\n\nStrength of GTrans (RQ5)\n\nWe elaborate on the strength of GTrans from two perspectives. First, we analyze the impact of code length of the Java data on the quality of generated comments since the code length may affect the representation learning. We report the performance varying values of code length on the test dataset of Java which contains 8151 examples shown in Table 1. As shown in Fig. 5, GTrans has best performance w.r.t varying values of code length compared to Transformer. Obviously, GTrans learns better code representation on different code length.\n\nSecond, in order to figure out what are the specific improvements in the generated summary, we compared the summary generated by Transformer and GTrans. We provide a couple of examples in Fig. 6 to show the advantages of our proposed approach qualitatively. In the example of Java, there are two \"for\" control structures. The first \"for\" control structure means \"delete the entry in the directory\", and the second means \"delete sub-entries\". And we can see that the summary generated by the Transformer model cannot reflect the meaning of the code. But GTrans can generate the summary that correctly describes the first \"for\" control structure. We argue that the reason why GTrans doesn't describe the second \"for\" control structure is that most of the summaries in the training dataset are in one sentence. So GTrans just generate a one-sentence summary. In the example of Python, there are three \"if...else...\" control structures. And we can see that the summary generated by the Transformer model is totally confused by such control structures. However, GTrans can generate the summary that reflects the meaning of the code, which is the closest to the reference.\n\nThere are so many examples like these. We provide more examples in \"Appendix C\". When there are some control structures in the code, such as \"while\", \"if... else...\", \"for\", the Transformer doesn't perform well, only rely on the code sequence. And GGNN, which captures local detail dependencies in code, can extract features of the control structure. With the enhancement of GGNN, the hybrid model performs significantly better.\n\nIn summary, GTrans can generate high quality of summary that correctly reflects the meaning of the code.\n\n\nConclusion\n\nIn this work, we introduce an architecture for investigating models that combine local and global information in order to understand code better for code summarization. We demonstrate that local models in combination with global models learn much more powerful code representations faster and achieve state-of-the-art results on public datasets. In our future work, we want to incorporate pre-trained methods in our model, and we also want to study different fusion strategies to incorporate the global and local models and apply the techniques in other tasks(e.g., bug fixing) to improve code quality in open-source software ecosystems. Table 5 shows the hyper-parameters that we used in our experiments.\n\n\nAppendix A: Hyper-parameters\n\nThe k refers to the clipping distance in relative position representations in Transformer. And T indicates GGNN's unrolled timestep. \n\n\ncaptured the API knowledge in the source code and combined with an RNN to generate code comment. \u2022 Dual Model Wei et al. (2019) proposed a dual framework to train code generation and code summarization simultaneously. \u2022 CodeBERT Feng et al. (2020) proposed a bimodal pre-trained model for programming language and natural language. \u2022 Transformer Ahmad et al. (2020) explored a Transformer-based approach with copy mechanism and relative positional encoding to summary.\n\n\u2022\nCode2seq Alon et al. (2018) aggregated a set of leaf-to-leaf paths in the AST with attention mechanism to generate method name. \u2022 BiLSTM\u2192LSTM + Copy Fernandes et al. (2019) used a BiLSTM model to generate method name incorporating copy mechanism. \u2022 Transformer The same model mentioned for code comment generation. \u2022 GNN\u2192LSTM + Copy Fernandes et al. (2019) applied a GGNN model to represent code graph for method name generation. \u2022 BiLSTM + GNN\u2192LSTM + Copy Fernandes et al. (2019) extended BiLSTM encoder with a GGNN component to combine structural and sequential features.We compare our model with these baseline methods reported inFernandes et al. (2019).\n\nFig. 2 Fig. 3\n23Comparison of GTrans and GGNN-only model at each epoch on Java dataset Performance of different combinations at each epoch on Java dataset\n\nFig. 4\n4Heatmap of the attention layer in Transformer and GTrans for the input of example-1 and example-2. The Transformer always focuses on several fixed positions, and GTrans will pay attention to different parts in each decoding\n\nFig. 5 Fig. 6\n56Performance w.r.t varying code length on Java dataset Some examples on Java and Python datasets\n\nTable 1 Statistics\n1of the \nexperiment datasets for code \ncomment generation \n\nDataset \nJava \nPython \n\nTrain \n69,708 \n55,538 \n\nValidation \n8714 \n18,505 \n\nTest \n8714 \n18,502 \n\nTrain (parsed ASTs) \n65,392 \n54,231 \n\nValidation (parsed ASTs) \n8174 \n18,043 \n\nTest (parsed ASTs) \n8151 \n18,060 \n\nAvg. tokens in code \n120.16 \n47.98 \n\nAvg. tokens in summary \n17.73 \n9.48 \n\n\nTable 2 Statistics\n2of the java-\nsmall dataset for method name \ngeneration \n\nDataSet \nNumber of parsed examples \n\nTrain \nValid \nTest \n\nJava-small \n691,911 \n23,844 \n57,088 \n\n\nTable 5\n5The hyper-parameters that we used in our experiments a We don't use beam search for method namingHyper-parameter \nValue \n\nEmbedding \nk \n16 \n\nModel \nl \n6 \n\nh \n8 \n\nd model \n512 \n\nd k , d v \n64 \n\nd ff \n2048 \n\nT \n5 \n\nTraing \ndropout \n0.2 \n\nOptimizer \nAdam \n\nLearning rate \n0.0001 \n\nBatch size \n32 \n\nTesting a \nbeam size \n4 \n\nThe self-multi-head attention layer in the decoder is not equipped with relative position encoding.\nWe found 5 steps are enough and more propagation steps do not help.\nPage 24 of 27\nAppendix B: The modification in the encoderFig. 7shows the modification in the encoder.\nA transformer-based approach for source code summarization. W U Ahmad, S Chakraborty, B Ray, K W Chang, arXiv: 20050 0653arXiv preprintAhmad, W.U., Chakraborty, S., Ray, B., Chang, K.W.: A transformer-based approach for source code summarization. arXiv preprint arXiv: 20050 0653 (2020)\n\nLearning to represent programs with graphs. M Allamanis, M Brockschmidt, M Khademi, arXiv:1711.00740arXiv preprintAllamanis, M., Brockschmidt, M., Khademi, M.: Learning to represent programs with graphs. arXiv pre- print arXiv: 1711. 00740 (2017)\n\nA convolutional attention network for extreme summarization of source code. M Allamanis, H Peng, C Sutton, PMLRInternational conference on machine learning. Allamanis, M., Peng, H., Sutton, C.: A convolutional attention network for extreme summarization of source code. In: International conference on machine learning, PMLR, pp. 2091-2100 (2016)\n\nA survey of machine learning for big code and naturalness. M Allamanis, E T Barr, P Devanbu, C Sutton, ACM Comput. Surv. (CSUR). 514Allamanis, M., Barr, E.T., Devanbu, P., Sutton, C.: A survey of machine learning for big code and natu- ralness. ACM Comput. Surv. (CSUR) 51(4), 1-37 (2018)\n\nU Alon, S Brody, O Levy, E Yahav, arXiv:1808.01400code2seq: Generating sequences from structured representations of code. arXiv preprintAlon, U., Brody, S., Levy, O., Yahav, E.: code2seq: Generating sequences from structured representations of code. arXiv preprint arXiv: 1808. 01400 (2018)\n\ncode2vec: Learning distributed representations of code. U Alon, M Zilberstein, O Levy, E Yahav, Proc. ACM Program. Lang. 3Alon, U., Zilberstein, M., Levy, O., Yahav, E.: code2vec: Learning distributed representations of code. Proc. ACM Program. Lang. 3(POPL), 1-29 (2019)\n\nA general path-based representation for predicting program properties. U Alon, M Zilberstein, O Levy, E Yahav, ACM SIGPLAN Notices. 534Alon, U., Zilberstein, M., Levy, O., Yahav, E.: A general path-based representation for predicting pro- gram properties. ACM SIGPLAN Notices 53(4), 404-419 (2018)\n\nMeteor: An automatic metric for mt evaluation with improved correlation with human judgments. S Banerjee, A Lavie, Proceedings of the acl Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. the acl Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or SummarizationBanerjee, S., Lavie, A.: Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In: Proceedings of the acl Workshop on Intrinsic and Extrinsic Evaluation Meas- ures for Machine Translation and/or Summarization, pp. 65-72 (2005)\n\nA parallel corpus of python functions and documentation strings for automated code documentation and code generation. A V M Barone, R Sennrich, arXiv: 170702275arXiv preprintBarone, A.V.M., Sennrich, R.: A parallel corpus of python functions and documentation strings for auto- mated code documentation and code generation. arXiv preprint arXiv: 17070 2275 (2017)\n\nNovel natural language summarization of program code via leveraging multiple input representations. F Chen, M Kim, J Choo, Findings of the Association for Computational Linguistics: EMNLP 2021. Chen, F., Kim, M., Choo, J.: Novel natural language summarization of program code via leveraging mul- tiple input representations. In: Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 2510-2520 (2021)\n\nTree-to-tree neural networks for program translation. X Chen, C Liu, D Song, arXiv: 180203691arXiv preprintChen, X., Liu, C., Song, D.: Tree-to-tree neural networks for program translation. arXiv preprint arXiv: 18020 3691 (2018)\n\nEmpirical study of transformers for source code. N Chirkova, S Troshin, arXiv: 20100 7987arXiv preprintChirkova, N., Troshin, S.: Empirical study of transformers for source code. arXiv preprint arXiv: 20100 7987 (2020)\n\nLearning phrase representations using rnn encoder-decoder for statistical machine translation. K Cho, B Van Merri\u00ebnboer, C Gulcehre, D Bahdanau, F Bougares, H Schwenk, Y Bengio, arXiv: 14061 078arXiv preprintCho, K., Van Merri\u00ebnboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio, Y.: Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv: 14061 078 (2014)\n\nTree-to-sequence attentional neural machine translation. A Eriguchi, K Hashimoto, Y Tsuruoka, arXiv: 160306075arXiv preprintEriguchi, A., Hashimoto, K., Tsuruoka, Y.: Tree-to-sequence attentional neural machine translation. arXiv preprint arXiv: 16030 6075 (2016)\n\nZ Feng, D Guo, D Tang, N Duan, X Feng, M Gong, L Shou, B Qin, T Liu, D Jiang, arXiv:2002.08155Codebert: a pre-trained model for programming and natural languages. arXiv preprintFeng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., et al.: Codebert: a pre-trained model for programming and natural languages. arXiv preprint arXiv: 2002. 08155 (2020)\n\nStructured neural summarization. P Fernandes, M Allamanis, M Brockschmidt, 7th International Conference on Learning Representations. New Orleans, LA, USAFernandes, P., Allamanis, M., Brockschmidt, M.: Structured neural summarization. In: 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, OpenReview.net (2019). https:// openr eview. net/ forum? id= H1ers oRqtm\n\nCode structure guided transformer for source code summarization. S Gao, C Gao, Y He, J Zeng, L Y Nie, X Xia, arXiv: 21040 9340arXiv preprintGao, S., Gao, C., He, Y., Zeng, J., Nie, L.Y., Xia, X.: Code structure guided transformer for source code summarization. arXiv preprint arXiv: 21040 9340 (2021)\n\nD Guo, S Ren, S Lu, Z Feng, D Tang, S Liu, L Zhou, N Duan, A Svyatkovskiy, S Fu, arXiv:2009.08366Graphcodebert: pre-training code representations with data flow. arXiv preprintGuo, D., Ren, S., Lu, S., Feng, Z., Tang, D., Liu, S., Zhou, L., Duan, N., Svyatkovskiy, A., Fu, S., et al.: Graph- codebert: pre-training code representations with data flow. arXiv preprint arXiv: 2009. 08366 (2020)\n\nGlobal relational models of source code. V J Hellendoorn, C Sutton, R Singh, P Maniatis, D ; V J Bieber, C Sutton, R Singh, P Maniatis, D Bieber, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020International Conference on Learning RepresentationsHellendoorn, V.J., Sutton, C., Singh, R., Maniatis, P., Bieber, D.: Global relational models of source code. In: 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, OpenReview.net (2020). https:// openr eview. net/ forum? id= B1lnb RNtwr Hellendoorn, V.J., Sutton, C., Singh, R., Maniatis, P., Bieber, D.: Global relational models of source code. In: International Conference on Learning Representations (2019)\n\nDeep code comment generation. X Hu, G Li, X Xia, D Lo, Z Jin, 2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC). IEEEHu, X., Li, G., Xia, X,, Lo, D., Jin, Z.: Deep code comment generation. In: 2018 IEEE/ACM 26th Inter- national Conference on Program Comprehension (ICPC), pp. 200-210. IEEE (2018a)\n\nSummarizing source code with transferred api knowledge. X Hu, G Li, X Xia, D Lo, S Lu, Z Jin, Hu, X., Li, G., Xia, X., Lo, D., Lu, S., Jin, Z.: Summarizing source code with transferred api knowledge (2018b)\n\nDeep code comment generation with hybrid lexical and syntactical information. X Hu, G Li, X Xia, D Lo, Z Jin, Emp. Softw. Eng. 253Hu, X., Li, G., Xia, X., Lo, D., Jin, Z.: Deep code comment generation with hybrid lexical and syntactical information. Emp. Softw. Eng. 25(3), 2179-2217 (2020)\n\nSummarizing source code using a neural attention model. S Iyer, I Konstas, A Cheung, L Zettlemoyer, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsLong Papers1Iyer, S., Konstas, I., Cheung, A., Zettlemoyer, L.: Summarizing source code using a neural attention model. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguis- tics (Volume 1: Long Papers), pp. 2073-2083 (2016)\n\nImproved code summarization via a graph neural network. A Leclair, S Haque, L Wu, C Mcmillan, Proceedings of the 28th International Conference on Program Comprehension. the 28th International Conference on Program ComprehensionLeClair, A., Haque, S., Wu, L., McMillan, C.: Improved code summarization via a graph neural network. In: Proceedings of the 28th International Conference on Program Comprehension, pp. 184-195 (2020)\n\nA neural model for generating natural language summaries of program subroutines. A Leclair, S Jiang, C Mcmillan, 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEELeClair, A., Jiang, S., McMillan, C.: A neural model for generating natural language summaries of pro- gram subroutines. In: 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), pp. 795-806. IEEE (2019)\n\nImproving code summarization with block-wise abstract syntax tree splitting. C Lin, Z Ouyang, J Zhuang, J Chen, H Li, R Wu, arXiv: 21030 7845arXiv preprintLin, C., Ouyang, Z., Zhuang, J., Chen, J., Li, H., Wu, R.: Improving code summarization with block-wise abstract syntax tree splitting. arXiv preprint arXiv: 21030 7845 (2021)\n\nRouge: a package for automatic evaluation of summaries. C Y Lin, Text Summarization Branches Out. Lin, C.Y.: Rouge: a package for automatic evaluation of summaries. In: Text Summarization Branches Out, pp. 74-81 (2004)\n\nEffective approaches to attention-based neural machine translation. M T Luong, H Pham, C D Manning, arXiv: 15080 4025arXiv preprintLuong, M.T., Pham, H., Manning, C.D.: Effective approaches to attention-based neural machine transla- tion. arXiv preprint arXiv: 15080 4025 (2015)\n\nHow we refactor, and how we know it. E Murphy-Hill, C Parnin, A P Black, IEEE Trans. Softw. Eng. 381Murphy-Hill, E., Parnin, C., Black, A.P.: How we refactor, and how we know it. IEEE Trans. Softw. Eng. 38(1), 5-18 (2011)\n\nBleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W J Zhu, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPapineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic evaluation of machine trans- lation. In: Proceedings of the 40th Annual Meeting of the Association for Computational Linguis- tics, pp. 311-318 (2002)\n\nModeling global and local node contexts for text generation from knowledge graphs. L F Ribeiro, Y Zhang, C Gardent, I Gurevych, Trans. Assoc. Comput. Ling. 8Ribeiro, L.F., Zhang, Y., Gardent, C., Gurevych, I.: Modeling global and local node contexts for text gen- eration from knowledge graphs. Trans. Assoc. Comput. Ling. 8, 589-604 (2020)\n\nGet to the point: summarization with pointer-generator networks. A See, P J Liu, C D Manning, arXiv:1704.04368arXiv preprintSee, A., Liu, P.J., Manning, C.D.: Get to the point: summarization with pointer-generator networks. arXiv preprint arXiv: 1704. 04368 (2017)\n\nP Shaw, J Uszkoreit, A Vaswani, arXiv: 18030Self-attention with relative position representations. 2155arXiv preprintShaw, P., Uszkoreit, J., Vaswani, A.: Self-attention with relative position representations. arXiv preprint arXiv: 18030 2155 (2018)\n\nAutomatic source code summarization with extended tree-lstm. Y Shido, Y Kobayashi, A Yamamoto, A Miyamoto, T Matsumura, 2019 International Joint Conference on Neural Networks (IJCNN). IEEEShido, Y., Kobayashi, Y., Yamamoto, A., Miyamoto, A., Matsumura, T.: Automatic source code sum- marization with extended tree-lstm. In: 2019 International Joint Conference on Neural Networks (IJCNN), pp. 1-8. IEEE (2019)\n\nA Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, arXiv: 17060Attention is all you need. 3762arXiv preprintVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. arXiv preprint arXiv: 17060 3762 (2017)\n\nImproving automatic source code summarization via deep reinforcement learning. Y Wan, Z Zhao, M Yang, G Xu, H Ying, J Wu, P S Yu, Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. the 33rd ACM/IEEE International Conference on Automated Software EngineeringWan, Y., Zhao, Z., Yang, M., Xu, G., Ying, H., Wu, J., Yu, P.S.: Improving automatic source code sum- marization via deep reinforcement learning. In: Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering, pp. 397-407 (2018)\n\nLearning to represent programs with heterogeneous graphs. W Wang, K Zhang, G Li, Z Jin, arXiv: 20120 4188arXiv preprintWang, W., Zhang, K., Li, G., Jin, Z.: Learning to represent programs with heterogeneous graphs. arXiv preprint arXiv: 20120 4188 (2020)\n\nCode generation as a dual task of code summarization. B Wei, G Li, X Xia, Z Fu, Z Jin, arXiv: 19100 5923arXiv preprintWei, B., Li, G., Xia, X., Fu, Z., Jin, Z.: Code generation as a dual task of code summarization. arXiv pre- print arXiv: 19100 5923 (2019)\n\nMeasuring program comprehension: a large-scale field study with professionals. X Xia, L Bao, D Lo, Z Xing, A E Hassan, S Li, IEEE Trans. Softw. Eng. 4410Xia, X., Bao, L., Lo, D., Xing, Z., Hassan, A.E., Li, S.: Measuring program comprehension: a large-scale field study with professionals. IEEE Trans. Softw. Eng. 44(10), 951-976 (2017)\n\nMethod name suggestion with hierarchical attention networks. S Xu, S Zhang, W Wang, X Cao, C Guo, J Xu, Proceedings of the 2019 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation. the 2019 ACM SIGPLAN Workshop on Partial Evaluation and Program ManipulationXu, S., Zhang, S., Wang, W., Cao, X., Guo, C., Xu, J.: Method name suggestion with hierarchical atten- tion networks. In: Proceedings of the 2019 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation, pp. 10-21 (2019)\n\nRetrieval-based neural source code summarization. J Zhang, X Wang, H Zhang, H Sun, X Liu, 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE). IEEEZhang, J., Wang, X., Zhang, H., Sun, H., Liu, X.: Retrieval-based neural source code summarization. In: 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE), pp. 1385-1397. IEEE (2020)\n\nLanguage-agnostic representation learning of source code from structure and context. D Z\u00fcgner, T Kirschstein, M Catasta, J Leskovec, S G\u00fcnnemann, arXiv: 21031 1318arXiv preprintZ\u00fcgner, D., Kirschstein, T., Catasta, M., Leskovec, J., G\u00fcnnemann, S.: Language-agnostic representation learning of source code from structure and context. arXiv preprint arXiv: 21031 1318 (2021)\n\nPublisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n", "annotations": {"author": "[{\"end\":161,\"start\":152},{\"end\":174,\"start\":162},{\"end\":191,\"start\":175}]", "publisher": null, "author_last_name": "[{\"end\":160,\"start\":155},{\"end\":173,\"start\":164},{\"end\":190,\"start\":186}]", "author_first_name": "[{\"end\":154,\"start\":152},{\"end\":163,\"start\":162},{\"end\":176,\"start\":175},{\"end\":185,\"start\":177}]", "author_affiliation": null, "title": "[{\"end\":139,\"start\":1},{\"end\":330,\"start\":192}]", "venue": null, "abstract": "[{\"end\":2402,\"start\":687}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2844,\"start\":2822},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2881,\"start\":2864},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3208,\"start\":3191},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3352,\"start\":3327},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4214,\"start\":4196},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4230,\"start\":4214},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4252,\"start\":4230},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4267,\"start\":4252},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4471,\"start\":4452},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4649,\"start\":4628},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4775,\"start\":4756},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4791,\"start\":4775},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4817,\"start\":4791},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5137,\"start\":5113},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5365,\"start\":5349},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5430,\"start\":5414},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5447,\"start\":5430},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5467,\"start\":5447},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5485,\"start\":5467},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5583,\"start\":5564},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5742,\"start\":5725},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5817,\"start\":5794},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5837,\"start\":5817},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6125,\"start\":6100},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6141,\"start\":6125},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6160,\"start\":6141},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6258,\"start\":6238},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6453,\"start\":6430},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6476,\"start\":6453},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6945,\"start\":6918},{\"end\":8850,\"start\":8849},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9781,\"start\":9758},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9934,\"start\":9908},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10126,\"start\":10103},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10506,\"start\":10488},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11035,\"start\":11019},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11149,\"start\":11126},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11456,\"start\":11438},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11478,\"start\":11456},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11497,\"start\":11480},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11599,\"start\":11582},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11719,\"start\":11702},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11870,\"start\":11851},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11910,\"start\":11890},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11995,\"start\":11978},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12033,\"start\":12016},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12236,\"start\":12219},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12470,\"start\":12454},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12965,\"start\":12943},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12983,\"start\":12965},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13000,\"start\":12983},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13038,\"start\":13015},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13143,\"start\":13120},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13454,\"start\":13433},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13642,\"start\":13617},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13900,\"start\":13873},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14225,\"start\":14206},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14604,\"start\":14585},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14987,\"start\":14964},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16220,\"start\":16200},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16242,\"start\":16220},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17306,\"start\":17283},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17327,\"start\":17306},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18541,\"start\":18522},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19978,\"start\":19957},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21542,\"start\":21523},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21675,\"start\":21652},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22093,\"start\":22075},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24781,\"start\":24765},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24821,\"start\":24804},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24877,\"start\":24858},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25894,\"start\":25872},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25928,\"start\":25903},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25952,\"start\":25942},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26279,\"start\":26269},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26549,\"start\":26530},{\"end\":26936,\"start\":26915},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":27015,\"start\":26998},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":27224,\"start\":27207},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":27326,\"start\":27309},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27408,\"start\":27389},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27431,\"start\":27413},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":29551,\"start\":29528},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":39285,\"start\":39262}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38625,\"start\":38155},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39286,\"start\":38626},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39442,\"start\":39287},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39675,\"start\":39443},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39788,\"start\":39676},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":40153,\"start\":39789},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":40327,\"start\":40154},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":40658,\"start\":40328}]", "paragraph": "[{\"end\":3697,\"start\":2418},{\"end\":4127,\"start\":3699},{\"end\":6946,\"start\":4129},{\"end\":8541,\"start\":6948},{\"end\":8607,\"start\":8543},{\"end\":9102,\"start\":8609},{\"end\":9369,\"start\":9104},{\"end\":9631,\"start\":9371},{\"end\":10012,\"start\":9648},{\"end\":10985,\"start\":10039},{\"end\":11223,\"start\":10987},{\"end\":12114,\"start\":11251},{\"end\":12822,\"start\":12116},{\"end\":14062,\"start\":12824},{\"end\":14462,\"start\":14064},{\"end\":14481,\"start\":14469},{\"end\":14915,\"start\":14503},{\"end\":15134,\"start\":14917},{\"end\":15458,\"start\":15136},{\"end\":15764,\"start\":15475},{\"end\":16080,\"start\":15939},{\"end\":17012,\"start\":16082},{\"end\":17347,\"start\":17030},{\"end\":17805,\"start\":17358},{\"end\":17964,\"start\":17807},{\"end\":18014,\"start\":17966},{\"end\":18086,\"start\":18058},{\"end\":18270,\"start\":18088},{\"end\":18442,\"start\":18287},{\"end\":18752,\"start\":18444},{\"end\":18759,\"start\":18754},{\"end\":19010,\"start\":18797},{\"end\":19351,\"start\":19259},{\"end\":19396,\"start\":19353},{\"end\":19559,\"start\":19451},{\"end\":19619,\"start\":19561},{\"end\":19726,\"start\":19621},{\"end\":19886,\"start\":19728},{\"end\":20027,\"start\":19888},{\"end\":20644,\"start\":20039},{\"end\":21326,\"start\":20996},{\"end\":21583,\"start\":21328},{\"end\":22762,\"start\":21603},{\"end\":23080,\"start\":22764},{\"end\":23266,\"start\":23082},{\"end\":23511,\"start\":23268},{\"end\":23828,\"start\":23584},{\"end\":23972,\"start\":23830},{\"end\":24098,\"start\":23987},{\"end\":24592,\"start\":24100},{\"end\":25113,\"start\":24602},{\"end\":25816,\"start\":25186},{\"end\":26193,\"start\":25818},{\"end\":26330,\"start\":26195},{\"end\":26652,\"start\":26332},{\"end\":26804,\"start\":26654},{\"end\":26905,\"start\":26818},{\"end\":27514,\"start\":26907},{\"end\":30228,\"start\":27558},{\"end\":30641,\"start\":30230},{\"end\":31099,\"start\":30670},{\"end\":31679,\"start\":31101},{\"end\":31872,\"start\":31681},{\"end\":32166,\"start\":31919},{\"end\":32795,\"start\":32173},{\"end\":33323,\"start\":32797},{\"end\":33472,\"start\":33325},{\"end\":33969,\"start\":33500},{\"end\":34879,\"start\":33971},{\"end\":34996,\"start\":34881},{\"end\":35564,\"start\":35025},{\"end\":36732,\"start\":35566},{\"end\":37162,\"start\":36734},{\"end\":37268,\"start\":37164},{\"end\":37988,\"start\":37283},{\"end\":38154,\"start\":38021}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15938,\"start\":15765},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18057,\"start\":18015},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18796,\"start\":18760},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19258,\"start\":19011},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19450,\"start\":19397},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20995,\"start\":20645},{\"attributes\":{\"id\":\"formula_6\"},\"end\":23583,\"start\":23512},{\"attributes\":{\"id\":\"formula_7\"},\"end\":25185,\"start\":25114}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":24934,\"start\":24927},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25112,\"start\":25105},{\"end\":27646,\"start\":27632},{\"end\":27871,\"start\":27864},{\"end\":29001,\"start\":28994},{\"end\":29448,\"start\":29441},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":35376,\"start\":35369},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":37928,\"start\":37921}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2416,\"start\":2404},{\"attributes\":{\"n\":\"2\"},\"end\":9646,\"start\":9634},{\"attributes\":{\"n\":\"2.1\"},\"end\":10037,\"start\":10015},{\"attributes\":{\"n\":\"2.2\"},\"end\":11249,\"start\":11226},{\"end\":14467,\"start\":14465},{\"attributes\":{\"n\":\"3\"},\"end\":14501,\"start\":14484},{\"attributes\":{\"n\":\"3.1\"},\"end\":15473,\"start\":15461},{\"attributes\":{\"n\":\"3.2\"},\"end\":17028,\"start\":17015},{\"end\":17356,\"start\":17350},{\"attributes\":{\"n\":\"3.3\"},\"end\":18285,\"start\":18273},{\"attributes\":{\"n\":\"3.4\"},\"end\":20037,\"start\":20030},{\"attributes\":{\"n\":\"3.5\"},\"end\":21601,\"start\":21586},{\"attributes\":{\"n\":\"4\"},\"end\":23985,\"start\":23975},{\"attributes\":{\"n\":\"4.1\"},\"end\":24600,\"start\":24595},{\"attributes\":{\"n\":\"4.2\"},\"end\":26816,\"start\":26807},{\"attributes\":{\"n\":\"4.3\"},\"end\":27556,\"start\":27517},{\"attributes\":{\"n\":\"4.4\"},\"end\":30668,\"start\":30644},{\"attributes\":{\"n\":\"4.5\"},\"end\":31917,\"start\":31875},{\"end\":32171,\"start\":32169},{\"attributes\":{\"n\":\"4.6\"},\"end\":33498,\"start\":33475},{\"attributes\":{\"n\":\"4.7\"},\"end\":35023,\"start\":34999},{\"attributes\":{\"n\":\"5\"},\"end\":37281,\"start\":37271},{\"end\":38019,\"start\":37991},{\"end\":38628,\"start\":38627},{\"end\":39301,\"start\":39288},{\"end\":39450,\"start\":39444},{\"end\":39690,\"start\":39677},{\"end\":39808,\"start\":39790},{\"end\":40173,\"start\":40155},{\"end\":40336,\"start\":40329}]", "table": "[{\"end\":40153,\"start\":39810},{\"end\":40327,\"start\":40175},{\"end\":40658,\"start\":40435}]", "figure_caption": "[{\"end\":38625,\"start\":38157},{\"end\":39286,\"start\":38629},{\"end\":39442,\"start\":39304},{\"end\":39675,\"start\":39452},{\"end\":39788,\"start\":39693},{\"end\":40435,\"start\":40338}]", "figure_ref": "[{\"end\":15535,\"start\":15529},{\"end\":20026,\"start\":20019},{\"end\":31020,\"start\":31014},{\"end\":32716,\"start\":32710},{\"end\":33815,\"start\":33809},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":34109,\"start\":34103},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":34283,\"start\":34276},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":34586,\"start\":34577},{\"end\":35396,\"start\":35390},{\"end\":35760,\"start\":35754}]", "bib_author_first_name": "[{\"end\":40990,\"start\":40989},{\"end\":40992,\"start\":40991},{\"end\":41001,\"start\":41000},{\"end\":41016,\"start\":41015},{\"end\":41023,\"start\":41022},{\"end\":41025,\"start\":41024},{\"end\":41262,\"start\":41261},{\"end\":41275,\"start\":41274},{\"end\":41291,\"start\":41290},{\"end\":41542,\"start\":41541},{\"end\":41555,\"start\":41554},{\"end\":41563,\"start\":41562},{\"end\":41873,\"start\":41872},{\"end\":41886,\"start\":41885},{\"end\":41888,\"start\":41887},{\"end\":41896,\"start\":41895},{\"end\":41907,\"start\":41906},{\"end\":42104,\"start\":42103},{\"end\":42112,\"start\":42111},{\"end\":42121,\"start\":42120},{\"end\":42129,\"start\":42128},{\"end\":42452,\"start\":42451},{\"end\":42460,\"start\":42459},{\"end\":42475,\"start\":42474},{\"end\":42483,\"start\":42482},{\"end\":42740,\"start\":42739},{\"end\":42748,\"start\":42747},{\"end\":42763,\"start\":42762},{\"end\":42771,\"start\":42770},{\"end\":43062,\"start\":43061},{\"end\":43074,\"start\":43073},{\"end\":43702,\"start\":43701},{\"end\":43706,\"start\":43703},{\"end\":43716,\"start\":43715},{\"end\":44049,\"start\":44048},{\"end\":44057,\"start\":44056},{\"end\":44064,\"start\":44063},{\"end\":44425,\"start\":44424},{\"end\":44433,\"start\":44432},{\"end\":44440,\"start\":44439},{\"end\":44651,\"start\":44650},{\"end\":44663,\"start\":44662},{\"end\":44917,\"start\":44916},{\"end\":44924,\"start\":44923},{\"end\":44943,\"start\":44942},{\"end\":44955,\"start\":44954},{\"end\":44967,\"start\":44966},{\"end\":44979,\"start\":44978},{\"end\":44990,\"start\":44989},{\"end\":45319,\"start\":45318},{\"end\":45331,\"start\":45330},{\"end\":45344,\"start\":45343},{\"end\":45527,\"start\":45526},{\"end\":45535,\"start\":45534},{\"end\":45542,\"start\":45541},{\"end\":45550,\"start\":45549},{\"end\":45558,\"start\":45557},{\"end\":45566,\"start\":45565},{\"end\":45574,\"start\":45573},{\"end\":45582,\"start\":45581},{\"end\":45589,\"start\":45588},{\"end\":45596,\"start\":45595},{\"end\":45954,\"start\":45953},{\"end\":45967,\"start\":45966},{\"end\":45980,\"start\":45979},{\"end\":46404,\"start\":46403},{\"end\":46411,\"start\":46410},{\"end\":46418,\"start\":46417},{\"end\":46424,\"start\":46423},{\"end\":46432,\"start\":46431},{\"end\":46434,\"start\":46433},{\"end\":46441,\"start\":46440},{\"end\":46641,\"start\":46640},{\"end\":46648,\"start\":46647},{\"end\":46655,\"start\":46654},{\"end\":46661,\"start\":46660},{\"end\":46669,\"start\":46668},{\"end\":46677,\"start\":46676},{\"end\":46684,\"start\":46683},{\"end\":46692,\"start\":46691},{\"end\":46700,\"start\":46699},{\"end\":46716,\"start\":46715},{\"end\":47076,\"start\":47075},{\"end\":47078,\"start\":47077},{\"end\":47093,\"start\":47092},{\"end\":47103,\"start\":47102},{\"end\":47112,\"start\":47111},{\"end\":47124,\"start\":47123},{\"end\":47130,\"start\":47125},{\"end\":47140,\"start\":47139},{\"end\":47150,\"start\":47149},{\"end\":47159,\"start\":47158},{\"end\":47171,\"start\":47170},{\"end\":47817,\"start\":47816},{\"end\":47823,\"start\":47822},{\"end\":47829,\"start\":47828},{\"end\":47836,\"start\":47835},{\"end\":47842,\"start\":47841},{\"end\":48168,\"start\":48167},{\"end\":48174,\"start\":48173},{\"end\":48180,\"start\":48179},{\"end\":48187,\"start\":48186},{\"end\":48193,\"start\":48192},{\"end\":48199,\"start\":48198},{\"end\":48398,\"start\":48397},{\"end\":48404,\"start\":48403},{\"end\":48410,\"start\":48409},{\"end\":48417,\"start\":48416},{\"end\":48423,\"start\":48422},{\"end\":48668,\"start\":48667},{\"end\":48676,\"start\":48675},{\"end\":48687,\"start\":48686},{\"end\":48697,\"start\":48696},{\"end\":49190,\"start\":49189},{\"end\":49201,\"start\":49200},{\"end\":49210,\"start\":49209},{\"end\":49216,\"start\":49215},{\"end\":49643,\"start\":49642},{\"end\":49654,\"start\":49653},{\"end\":49663,\"start\":49662},{\"end\":50059,\"start\":50058},{\"end\":50066,\"start\":50065},{\"end\":50076,\"start\":50075},{\"end\":50086,\"start\":50085},{\"end\":50094,\"start\":50093},{\"end\":50100,\"start\":50099},{\"end\":50370,\"start\":50369},{\"end\":50372,\"start\":50371},{\"end\":50602,\"start\":50601},{\"end\":50604,\"start\":50603},{\"end\":50613,\"start\":50612},{\"end\":50621,\"start\":50620},{\"end\":50623,\"start\":50622},{\"end\":50851,\"start\":50850},{\"end\":50866,\"start\":50865},{\"end\":50876,\"start\":50875},{\"end\":50878,\"start\":50877},{\"end\":51101,\"start\":51100},{\"end\":51113,\"start\":51112},{\"end\":51123,\"start\":51122},{\"end\":51131,\"start\":51130},{\"end\":51133,\"start\":51132},{\"end\":51612,\"start\":51611},{\"end\":51614,\"start\":51613},{\"end\":51625,\"start\":51624},{\"end\":51634,\"start\":51633},{\"end\":51645,\"start\":51644},{\"end\":51936,\"start\":51935},{\"end\":51943,\"start\":51942},{\"end\":51945,\"start\":51944},{\"end\":51952,\"start\":51951},{\"end\":51954,\"start\":51953},{\"end\":52137,\"start\":52136},{\"end\":52145,\"start\":52144},{\"end\":52158,\"start\":52157},{\"end\":52449,\"start\":52448},{\"end\":52458,\"start\":52457},{\"end\":52471,\"start\":52470},{\"end\":52483,\"start\":52482},{\"end\":52495,\"start\":52494},{\"end\":52798,\"start\":52797},{\"end\":52809,\"start\":52808},{\"end\":52820,\"start\":52819},{\"end\":52830,\"start\":52829},{\"end\":52843,\"start\":52842},{\"end\":52852,\"start\":52851},{\"end\":52854,\"start\":52853},{\"end\":52863,\"start\":52862},{\"end\":52873,\"start\":52872},{\"end\":53196,\"start\":53195},{\"end\":53203,\"start\":53202},{\"end\":53211,\"start\":53210},{\"end\":53219,\"start\":53218},{\"end\":53225,\"start\":53224},{\"end\":53233,\"start\":53232},{\"end\":53239,\"start\":53238},{\"end\":53241,\"start\":53240},{\"end\":53737,\"start\":53736},{\"end\":53745,\"start\":53744},{\"end\":53754,\"start\":53753},{\"end\":53760,\"start\":53759},{\"end\":53989,\"start\":53988},{\"end\":53996,\"start\":53995},{\"end\":54002,\"start\":54001},{\"end\":54009,\"start\":54008},{\"end\":54015,\"start\":54014},{\"end\":54272,\"start\":54271},{\"end\":54279,\"start\":54278},{\"end\":54286,\"start\":54285},{\"end\":54292,\"start\":54291},{\"end\":54300,\"start\":54299},{\"end\":54302,\"start\":54301},{\"end\":54312,\"start\":54311},{\"end\":54592,\"start\":54591},{\"end\":54598,\"start\":54597},{\"end\":54607,\"start\":54606},{\"end\":54615,\"start\":54614},{\"end\":54622,\"start\":54621},{\"end\":54629,\"start\":54628},{\"end\":55087,\"start\":55086},{\"end\":55096,\"start\":55095},{\"end\":55104,\"start\":55103},{\"end\":55113,\"start\":55112},{\"end\":55120,\"start\":55119},{\"end\":55500,\"start\":55499},{\"end\":55510,\"start\":55509},{\"end\":55525,\"start\":55524},{\"end\":55536,\"start\":55535},{\"end\":55548,\"start\":55547}]", "bib_author_last_name": "[{\"end\":40998,\"start\":40993},{\"end\":41013,\"start\":41002},{\"end\":41020,\"start\":41017},{\"end\":41031,\"start\":41026},{\"end\":41272,\"start\":41263},{\"end\":41288,\"start\":41276},{\"end\":41299,\"start\":41292},{\"end\":41552,\"start\":41543},{\"end\":41560,\"start\":41556},{\"end\":41570,\"start\":41564},{\"end\":41883,\"start\":41874},{\"end\":41893,\"start\":41889},{\"end\":41904,\"start\":41897},{\"end\":41914,\"start\":41908},{\"end\":42109,\"start\":42105},{\"end\":42118,\"start\":42113},{\"end\":42126,\"start\":42122},{\"end\":42135,\"start\":42130},{\"end\":42457,\"start\":42453},{\"end\":42472,\"start\":42461},{\"end\":42480,\"start\":42476},{\"end\":42489,\"start\":42484},{\"end\":42745,\"start\":42741},{\"end\":42760,\"start\":42749},{\"end\":42768,\"start\":42764},{\"end\":42777,\"start\":42772},{\"end\":43071,\"start\":43063},{\"end\":43080,\"start\":43075},{\"end\":43713,\"start\":43707},{\"end\":43725,\"start\":43717},{\"end\":44054,\"start\":44050},{\"end\":44061,\"start\":44058},{\"end\":44069,\"start\":44065},{\"end\":44430,\"start\":44426},{\"end\":44437,\"start\":44434},{\"end\":44445,\"start\":44441},{\"end\":44660,\"start\":44652},{\"end\":44671,\"start\":44664},{\"end\":44921,\"start\":44918},{\"end\":44940,\"start\":44925},{\"end\":44952,\"start\":44944},{\"end\":44964,\"start\":44956},{\"end\":44976,\"start\":44968},{\"end\":44987,\"start\":44980},{\"end\":44997,\"start\":44991},{\"end\":45328,\"start\":45320},{\"end\":45341,\"start\":45332},{\"end\":45353,\"start\":45345},{\"end\":45532,\"start\":45528},{\"end\":45539,\"start\":45536},{\"end\":45547,\"start\":45543},{\"end\":45555,\"start\":45551},{\"end\":45563,\"start\":45559},{\"end\":45571,\"start\":45567},{\"end\":45579,\"start\":45575},{\"end\":45586,\"start\":45583},{\"end\":45593,\"start\":45590},{\"end\":45602,\"start\":45597},{\"end\":45964,\"start\":45955},{\"end\":45977,\"start\":45968},{\"end\":45993,\"start\":45981},{\"end\":46408,\"start\":46405},{\"end\":46415,\"start\":46412},{\"end\":46421,\"start\":46419},{\"end\":46429,\"start\":46425},{\"end\":46438,\"start\":46435},{\"end\":46445,\"start\":46442},{\"end\":46645,\"start\":46642},{\"end\":46652,\"start\":46649},{\"end\":46658,\"start\":46656},{\"end\":46666,\"start\":46662},{\"end\":46674,\"start\":46670},{\"end\":46681,\"start\":46678},{\"end\":46689,\"start\":46685},{\"end\":46697,\"start\":46693},{\"end\":46713,\"start\":46701},{\"end\":46719,\"start\":46717},{\"end\":47090,\"start\":47079},{\"end\":47100,\"start\":47094},{\"end\":47109,\"start\":47104},{\"end\":47121,\"start\":47113},{\"end\":47137,\"start\":47131},{\"end\":47147,\"start\":47141},{\"end\":47156,\"start\":47151},{\"end\":47168,\"start\":47160},{\"end\":47178,\"start\":47172},{\"end\":47820,\"start\":47818},{\"end\":47826,\"start\":47824},{\"end\":47833,\"start\":47830},{\"end\":47839,\"start\":47837},{\"end\":47846,\"start\":47843},{\"end\":48171,\"start\":48169},{\"end\":48177,\"start\":48175},{\"end\":48184,\"start\":48181},{\"end\":48190,\"start\":48188},{\"end\":48196,\"start\":48194},{\"end\":48203,\"start\":48200},{\"end\":48401,\"start\":48399},{\"end\":48407,\"start\":48405},{\"end\":48414,\"start\":48411},{\"end\":48420,\"start\":48418},{\"end\":48427,\"start\":48424},{\"end\":48673,\"start\":48669},{\"end\":48684,\"start\":48677},{\"end\":48694,\"start\":48688},{\"end\":48709,\"start\":48698},{\"end\":49198,\"start\":49191},{\"end\":49207,\"start\":49202},{\"end\":49213,\"start\":49211},{\"end\":49225,\"start\":49217},{\"end\":49651,\"start\":49644},{\"end\":49660,\"start\":49655},{\"end\":49672,\"start\":49664},{\"end\":50063,\"start\":50060},{\"end\":50073,\"start\":50067},{\"end\":50083,\"start\":50077},{\"end\":50091,\"start\":50087},{\"end\":50097,\"start\":50095},{\"end\":50103,\"start\":50101},{\"end\":50376,\"start\":50373},{\"end\":50610,\"start\":50605},{\"end\":50618,\"start\":50614},{\"end\":50631,\"start\":50624},{\"end\":50863,\"start\":50852},{\"end\":50873,\"start\":50867},{\"end\":50884,\"start\":50879},{\"end\":51110,\"start\":51102},{\"end\":51120,\"start\":51114},{\"end\":51128,\"start\":51124},{\"end\":51137,\"start\":51134},{\"end\":51622,\"start\":51615},{\"end\":51631,\"start\":51626},{\"end\":51642,\"start\":51635},{\"end\":51654,\"start\":51646},{\"end\":51940,\"start\":51937},{\"end\":51949,\"start\":51946},{\"end\":51962,\"start\":51955},{\"end\":52142,\"start\":52138},{\"end\":52155,\"start\":52146},{\"end\":52166,\"start\":52159},{\"end\":52455,\"start\":52450},{\"end\":52468,\"start\":52459},{\"end\":52480,\"start\":52472},{\"end\":52492,\"start\":52484},{\"end\":52505,\"start\":52496},{\"end\":52806,\"start\":52799},{\"end\":52817,\"start\":52810},{\"end\":52827,\"start\":52821},{\"end\":52840,\"start\":52831},{\"end\":52849,\"start\":52844},{\"end\":52860,\"start\":52855},{\"end\":52870,\"start\":52864},{\"end\":52884,\"start\":52874},{\"end\":53200,\"start\":53197},{\"end\":53208,\"start\":53204},{\"end\":53216,\"start\":53212},{\"end\":53222,\"start\":53220},{\"end\":53230,\"start\":53226},{\"end\":53236,\"start\":53234},{\"end\":53244,\"start\":53242},{\"end\":53742,\"start\":53738},{\"end\":53751,\"start\":53746},{\"end\":53757,\"start\":53755},{\"end\":53764,\"start\":53761},{\"end\":53993,\"start\":53990},{\"end\":53999,\"start\":53997},{\"end\":54006,\"start\":54003},{\"end\":54012,\"start\":54010},{\"end\":54019,\"start\":54016},{\"end\":54276,\"start\":54273},{\"end\":54283,\"start\":54280},{\"end\":54289,\"start\":54287},{\"end\":54297,\"start\":54293},{\"end\":54309,\"start\":54303},{\"end\":54315,\"start\":54313},{\"end\":54595,\"start\":54593},{\"end\":54604,\"start\":54599},{\"end\":54612,\"start\":54608},{\"end\":54619,\"start\":54616},{\"end\":54626,\"start\":54623},{\"end\":54632,\"start\":54630},{\"end\":55093,\"start\":55088},{\"end\":55101,\"start\":55097},{\"end\":55110,\"start\":55105},{\"end\":55117,\"start\":55114},{\"end\":55124,\"start\":55121},{\"end\":55507,\"start\":55501},{\"end\":55522,\"start\":55511},{\"end\":55533,\"start\":55526},{\"end\":55545,\"start\":55537},{\"end\":55558,\"start\":55549}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv: 20050 0653\",\"id\":\"b0\"},\"end\":41215,\"start\":40929},{\"attributes\":{\"doi\":\"arXiv:1711.00740\",\"id\":\"b1\"},\"end\":41463,\"start\":41217},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b2\",\"matched_paper_id\":2723946},\"end\":41811,\"start\":41465},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":207591052},\"end\":42101,\"start\":41813},{\"attributes\":{\"doi\":\"arXiv:1808.01400\",\"id\":\"b4\"},\"end\":42393,\"start\":42103},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":4710028},\"end\":42666,\"start\":42395},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4383884},\"end\":42965,\"start\":42668},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7164502},\"end\":43581,\"start\":42967},{\"attributes\":{\"doi\":\"arXiv: 17070\",\"id\":\"b8\"},\"end\":43946,\"start\":43583},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":244119794},\"end\":44368,\"start\":43948},{\"attributes\":{\"doi\":\"arXiv: 18020\",\"id\":\"b10\"},\"end\":44599,\"start\":44370},{\"attributes\":{\"doi\":\"arXiv: 20100 7987\",\"id\":\"b11\"},\"end\":44819,\"start\":44601},{\"attributes\":{\"doi\":\"arXiv: 14061 078\",\"id\":\"b12\"},\"end\":45259,\"start\":44821},{\"attributes\":{\"doi\":\"arXiv: 16030\",\"id\":\"b13\"},\"end\":45524,\"start\":45261},{\"attributes\":{\"doi\":\"arXiv:2002.08155\",\"id\":\"b14\"},\"end\":45918,\"start\":45526},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":53216170},\"end\":46336,\"start\":45920},{\"attributes\":{\"doi\":\"arXiv: 21040 9340\",\"id\":\"b16\"},\"end\":46638,\"start\":46338},{\"attributes\":{\"doi\":\"arXiv:2009.08366\",\"id\":\"b17\"},\"end\":47032,\"start\":46640},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":213352113},\"end\":47784,\"start\":47034},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":49584534},\"end\":48109,\"start\":47786},{\"attributes\":{\"id\":\"b20\"},\"end\":48317,\"start\":48111},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":189927337},\"end\":48609,\"start\":48319},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":8820379},\"end\":49131,\"start\":48611},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":214802082},\"end\":49559,\"start\":49133},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":59606259},\"end\":49979,\"start\":49561},{\"attributes\":{\"doi\":\"arXiv: 21030 7845\",\"id\":\"b25\"},\"end\":50311,\"start\":49981},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":964287},\"end\":50531,\"start\":50313},{\"attributes\":{\"doi\":\"arXiv: 15080 4025\",\"id\":\"b27\"},\"end\":50811,\"start\":50533},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5856772},\"end\":51034,\"start\":50813},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":11080756},\"end\":51526,\"start\":51036},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":210942740},\"end\":51868,\"start\":51528},{\"attributes\":{\"doi\":\"arXiv:1704.04368\",\"id\":\"b31\"},\"end\":52134,\"start\":51870},{\"attributes\":{\"doi\":\"arXiv: 18030\",\"id\":\"b32\"},\"end\":52385,\"start\":52136},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":195069474},\"end\":52795,\"start\":52387},{\"attributes\":{\"doi\":\"arXiv: 17060\",\"id\":\"b34\"},\"end\":53114,\"start\":52797},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":52069701},\"end\":53676,\"start\":53116},{\"attributes\":{\"doi\":\"arXiv: 20120 4188\",\"id\":\"b36\"},\"end\":53932,\"start\":53678},{\"attributes\":{\"doi\":\"arXiv: 19100 5923\",\"id\":\"b37\"},\"end\":54190,\"start\":53934},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":52988209},\"end\":54528,\"start\":54192},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":56894426},\"end\":55034,\"start\":54530},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":211163834},\"end\":55412,\"start\":55036},{\"attributes\":{\"doi\":\"arXiv: 21031 1318\",\"id\":\"b41\"},\"end\":55786,\"start\":55414},{\"attributes\":{\"id\":\"b42\"},\"end\":56059,\"start\":55788}]", "bib_title": "[{\"end\":41539,\"start\":41465},{\"end\":41870,\"start\":41813},{\"end\":42449,\"start\":42395},{\"end\":42737,\"start\":42668},{\"end\":43059,\"start\":42967},{\"end\":44046,\"start\":43948},{\"end\":45951,\"start\":45920},{\"end\":47073,\"start\":47034},{\"end\":47814,\"start\":47786},{\"end\":48395,\"start\":48319},{\"end\":48665,\"start\":48611},{\"end\":49187,\"start\":49133},{\"end\":49640,\"start\":49561},{\"end\":50367,\"start\":50313},{\"end\":50848,\"start\":50813},{\"end\":51098,\"start\":51036},{\"end\":51609,\"start\":51528},{\"end\":52446,\"start\":52387},{\"end\":53193,\"start\":53116},{\"end\":54269,\"start\":54192},{\"end\":54589,\"start\":54530},{\"end\":55084,\"start\":55036}]", "bib_author": "[{\"end\":41000,\"start\":40989},{\"end\":41015,\"start\":41000},{\"end\":41022,\"start\":41015},{\"end\":41033,\"start\":41022},{\"end\":41274,\"start\":41261},{\"end\":41290,\"start\":41274},{\"end\":41301,\"start\":41290},{\"end\":41554,\"start\":41541},{\"end\":41562,\"start\":41554},{\"end\":41572,\"start\":41562},{\"end\":41885,\"start\":41872},{\"end\":41895,\"start\":41885},{\"end\":41906,\"start\":41895},{\"end\":41916,\"start\":41906},{\"end\":42111,\"start\":42103},{\"end\":42120,\"start\":42111},{\"end\":42128,\"start\":42120},{\"end\":42137,\"start\":42128},{\"end\":42459,\"start\":42451},{\"end\":42474,\"start\":42459},{\"end\":42482,\"start\":42474},{\"end\":42491,\"start\":42482},{\"end\":42747,\"start\":42739},{\"end\":42762,\"start\":42747},{\"end\":42770,\"start\":42762},{\"end\":42779,\"start\":42770},{\"end\":43073,\"start\":43061},{\"end\":43082,\"start\":43073},{\"end\":43715,\"start\":43701},{\"end\":43727,\"start\":43715},{\"end\":44056,\"start\":44048},{\"end\":44063,\"start\":44056},{\"end\":44071,\"start\":44063},{\"end\":44432,\"start\":44424},{\"end\":44439,\"start\":44432},{\"end\":44447,\"start\":44439},{\"end\":44662,\"start\":44650},{\"end\":44673,\"start\":44662},{\"end\":44923,\"start\":44916},{\"end\":44942,\"start\":44923},{\"end\":44954,\"start\":44942},{\"end\":44966,\"start\":44954},{\"end\":44978,\"start\":44966},{\"end\":44989,\"start\":44978},{\"end\":44999,\"start\":44989},{\"end\":45330,\"start\":45318},{\"end\":45343,\"start\":45330},{\"end\":45355,\"start\":45343},{\"end\":45534,\"start\":45526},{\"end\":45541,\"start\":45534},{\"end\":45549,\"start\":45541},{\"end\":45557,\"start\":45549},{\"end\":45565,\"start\":45557},{\"end\":45573,\"start\":45565},{\"end\":45581,\"start\":45573},{\"end\":45588,\"start\":45581},{\"end\":45595,\"start\":45588},{\"end\":45604,\"start\":45595},{\"end\":45966,\"start\":45953},{\"end\":45979,\"start\":45966},{\"end\":45995,\"start\":45979},{\"end\":46410,\"start\":46403},{\"end\":46417,\"start\":46410},{\"end\":46423,\"start\":46417},{\"end\":46431,\"start\":46423},{\"end\":46440,\"start\":46431},{\"end\":46447,\"start\":46440},{\"end\":46647,\"start\":46640},{\"end\":46654,\"start\":46647},{\"end\":46660,\"start\":46654},{\"end\":46668,\"start\":46660},{\"end\":46676,\"start\":46668},{\"end\":46683,\"start\":46676},{\"end\":46691,\"start\":46683},{\"end\":46699,\"start\":46691},{\"end\":46715,\"start\":46699},{\"end\":46721,\"start\":46715},{\"end\":47092,\"start\":47075},{\"end\":47102,\"start\":47092},{\"end\":47111,\"start\":47102},{\"end\":47123,\"start\":47111},{\"end\":47139,\"start\":47123},{\"end\":47149,\"start\":47139},{\"end\":47158,\"start\":47149},{\"end\":47170,\"start\":47158},{\"end\":47180,\"start\":47170},{\"end\":47822,\"start\":47816},{\"end\":47828,\"start\":47822},{\"end\":47835,\"start\":47828},{\"end\":47841,\"start\":47835},{\"end\":47848,\"start\":47841},{\"end\":48173,\"start\":48167},{\"end\":48179,\"start\":48173},{\"end\":48186,\"start\":48179},{\"end\":48192,\"start\":48186},{\"end\":48198,\"start\":48192},{\"end\":48205,\"start\":48198},{\"end\":48403,\"start\":48397},{\"end\":48409,\"start\":48403},{\"end\":48416,\"start\":48409},{\"end\":48422,\"start\":48416},{\"end\":48429,\"start\":48422},{\"end\":48675,\"start\":48667},{\"end\":48686,\"start\":48675},{\"end\":48696,\"start\":48686},{\"end\":48711,\"start\":48696},{\"end\":49200,\"start\":49189},{\"end\":49209,\"start\":49200},{\"end\":49215,\"start\":49209},{\"end\":49227,\"start\":49215},{\"end\":49653,\"start\":49642},{\"end\":49662,\"start\":49653},{\"end\":49674,\"start\":49662},{\"end\":50065,\"start\":50058},{\"end\":50075,\"start\":50065},{\"end\":50085,\"start\":50075},{\"end\":50093,\"start\":50085},{\"end\":50099,\"start\":50093},{\"end\":50105,\"start\":50099},{\"end\":50378,\"start\":50369},{\"end\":50612,\"start\":50601},{\"end\":50620,\"start\":50612},{\"end\":50633,\"start\":50620},{\"end\":50865,\"start\":50850},{\"end\":50875,\"start\":50865},{\"end\":50886,\"start\":50875},{\"end\":51112,\"start\":51100},{\"end\":51122,\"start\":51112},{\"end\":51130,\"start\":51122},{\"end\":51139,\"start\":51130},{\"end\":51624,\"start\":51611},{\"end\":51633,\"start\":51624},{\"end\":51644,\"start\":51633},{\"end\":51656,\"start\":51644},{\"end\":51942,\"start\":51935},{\"end\":51951,\"start\":51942},{\"end\":51964,\"start\":51951},{\"end\":52144,\"start\":52136},{\"end\":52157,\"start\":52144},{\"end\":52168,\"start\":52157},{\"end\":52457,\"start\":52448},{\"end\":52470,\"start\":52457},{\"end\":52482,\"start\":52470},{\"end\":52494,\"start\":52482},{\"end\":52507,\"start\":52494},{\"end\":52808,\"start\":52797},{\"end\":52819,\"start\":52808},{\"end\":52829,\"start\":52819},{\"end\":52842,\"start\":52829},{\"end\":52851,\"start\":52842},{\"end\":52862,\"start\":52851},{\"end\":52872,\"start\":52862},{\"end\":52886,\"start\":52872},{\"end\":53202,\"start\":53195},{\"end\":53210,\"start\":53202},{\"end\":53218,\"start\":53210},{\"end\":53224,\"start\":53218},{\"end\":53232,\"start\":53224},{\"end\":53238,\"start\":53232},{\"end\":53246,\"start\":53238},{\"end\":53744,\"start\":53736},{\"end\":53753,\"start\":53744},{\"end\":53759,\"start\":53753},{\"end\":53766,\"start\":53759},{\"end\":53995,\"start\":53988},{\"end\":54001,\"start\":53995},{\"end\":54008,\"start\":54001},{\"end\":54014,\"start\":54008},{\"end\":54021,\"start\":54014},{\"end\":54278,\"start\":54271},{\"end\":54285,\"start\":54278},{\"end\":54291,\"start\":54285},{\"end\":54299,\"start\":54291},{\"end\":54311,\"start\":54299},{\"end\":54317,\"start\":54311},{\"end\":54597,\"start\":54591},{\"end\":54606,\"start\":54597},{\"end\":54614,\"start\":54606},{\"end\":54621,\"start\":54614},{\"end\":54628,\"start\":54621},{\"end\":54634,\"start\":54628},{\"end\":55095,\"start\":55086},{\"end\":55103,\"start\":55095},{\"end\":55112,\"start\":55103},{\"end\":55119,\"start\":55112},{\"end\":55126,\"start\":55119},{\"end\":55509,\"start\":55499},{\"end\":55524,\"start\":55509},{\"end\":55535,\"start\":55524},{\"end\":55547,\"start\":55535},{\"end\":55560,\"start\":55547}]", "bib_venue": "[{\"end\":40987,\"start\":40929},{\"end\":41259,\"start\":41217},{\"end\":41620,\"start\":41576},{\"end\":41940,\"start\":41916},{\"end\":42223,\"start\":42153},{\"end\":42514,\"start\":42491},{\"end\":42798,\"start\":42779},{\"end\":43205,\"start\":43082},{\"end\":43699,\"start\":43583},{\"end\":44140,\"start\":44071},{\"end\":44422,\"start\":44370},{\"end\":44648,\"start\":44601},{\"end\":44914,\"start\":44821},{\"end\":45316,\"start\":45261},{\"end\":45687,\"start\":45620},{\"end\":46051,\"start\":45995},{\"end\":46401,\"start\":46338},{\"end\":46800,\"start\":46737},{\"end\":47236,\"start\":47180},{\"end\":47923,\"start\":47848},{\"end\":48165,\"start\":48111},{\"end\":48444,\"start\":48429},{\"end\":48798,\"start\":48711},{\"end\":49300,\"start\":49227},{\"end\":49748,\"start\":49674},{\"end\":50056,\"start\":49981},{\"end\":50409,\"start\":50378},{\"end\":50599,\"start\":50533},{\"end\":50908,\"start\":50886},{\"end\":51226,\"start\":51139},{\"end\":51682,\"start\":51656},{\"end\":51933,\"start\":51870},{\"end\":52233,\"start\":52180},{\"end\":52569,\"start\":52507},{\"end\":52923,\"start\":52898},{\"end\":53337,\"start\":53246},{\"end\":53734,\"start\":53678},{\"end\":53986,\"start\":53934},{\"end\":54339,\"start\":54317},{\"end\":54725,\"start\":54634},{\"end\":55200,\"start\":55126},{\"end\":55497,\"start\":55414},{\"end\":55922,\"start\":55788},{\"end\":43315,\"start\":43207},{\"end\":46073,\"start\":46053},{\"end\":47259,\"start\":47238},{\"end\":48872,\"start\":48800},{\"end\":49360,\"start\":49302},{\"end\":51300,\"start\":51228},{\"end\":53415,\"start\":53339},{\"end\":54803,\"start\":54727}]"}}}, "year": 2023, "month": 12, "day": 17}