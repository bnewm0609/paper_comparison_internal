{"id": 221662976, "updated": "2022-01-22 04:01:57.824", "metadata": {"title": "Human pose estimation based in-home lower body rehabilitation system", "authors": "[{\"middle\":[],\"last\":\"Li\",\"first\":\"Ying\"},{\"middle\":[],\"last\":\"Wang\",\"first\":\"Chenxi\"},{\"middle\":[],\"last\":\"Cao\",\"first\":\"Yu\"},{\"middle\":[],\"last\":\"Liu\",\"first\":\"Benyuan\"},{\"middle\":[],\"last\":\"Tan\",\"first\":\"Joanna\"},{\"middle\":[],\"last\":\"Luo\",\"first\":\"Yan\"}]", "venue": "2020 International Joint Conference on Neural Networks (IJCNN)", "journal": "2020 International Joint Conference on Neural Networks (IJCNN)", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "In this paper, we design, develop and evaluate an in-home lower body rehabilitation system based on a novel lightweight human pose estimation model. To achieve that, we first create a lower body rehabilitation dataset of 500,000 images with each image annotated with the ground truth joint point locations. The dataset consists of 31 different types of lower body rehabilitation activities from twenty volunteers. After that, we design a lightweight but powerful neural network model, which runs on a smartphone, to estimate human pose. Furthermore, we develop a series of principles for evaluating in-home rehabilitation activities of patients in terms of the range of motion and duration of activities. For the concern of privacy, all the data collected from patients are encrypted, stored and processed locally on patients\u2019 own smartphones. Only the sanitized evaluation reports are uploaded and shared with the patients\u2019 primary doctors. Our model achieves 70.8 in AP score on the COCO val2017 set with only 4.7M parameters and 1.0 GFLOPs. Using our system, patients can perform lower body rehabilitation activities at home and obtain evaluation report without the presence of physical therapists. We believe our system can greatly facilitate in-home rehabilitation and reduce the cost for patients.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3089935083", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ijcnn/LiWCLTL20", "doi": "10.1109/ijcnn48605.2020.9207296"}}, "content": {"source": {"pdf_hash": "017f993e21d3c76c603717b65520b6cc44de4d80", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "0815641a268f8181f0e242ebf35bf13b52aa88ea", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/017f993e21d3c76c603717b65520b6cc44de4d80.txt", "contents": "\nHuman pose estimation based in-home lower body rehabilitation system\n\n\nYing Li yingli@student.uml.edu \nChenxi Wang chenxiwang1@student.uml.edu \nYu Cao yucao@uml.edu \nBenyuan Liu benyuanliu@uml.edu \nJoanna Tan \nYan Luo luo@uml.edu \n\nDepartment of Computer Science\nDepartment of Electrical and Computer Engineering\nUniversity of Massachusetts Lowell Lowell\nUSA\n\n\nDepartment of Computer Science\nUniversity of Massachusetts Lowell Lowell\nUSA\n\n\nDepartment of Computer Science\nUniversity of Massachusetts Lowell Lowell\nUSA\n\n\nDepartment of Electrical and Computer Engineering\nPhysical Therapy Department Encompass Rehabilitation Hospital Woburn\nUniversity of Massachusetts Lowell Lowell\nUSA, USA\n\n\nUniversity of Massachusetts Lowell Lowell\nUSA yan\n\nHuman pose estimation based in-home lower body rehabilitation system\n\nIn this paper, we design, develop and evaluate an in-home lower body rehabilitation system based on a novel lightweight human pose estimation model. To achieve that, we first create a lower body rehabilitation dataset of 500,000 images with each image annotated with the ground truth joint point locations. The dataset consists of 31 different types of lower body rehabilitation activities from twenty volunteers. After that, we design a lightweight but powerful neural network model, which runs on a smartphone, to estimate human pose. Furthermore, we develop a series of principles for evaluating in-home rehabilitation activities of patients in terms of the range of motion and duration of activities. For the concern of privacy, all the data collected from patients are encrypted, stored and processed locally on patients' own smartphones. Only the sanitized evaluation reports are uploaded and shared with the patients' primary doctors. Our model achieves 70.8 in AP score on the COCO val2017 set with only 4.7M parameters and 1.0 GFLOPs. Using our system, patients can perform lower body rehabilitation activities at home and obtain evaluation report without the presence of physical therapists. We believe our system can greatly facilitate in-home rehabilitation and reduce the cost for patients.Index Terms-in-home lower body rehabilitation system, human pose estimation 978-1-7281-6926-2/20/$31.00\n\nI. INTRODUCTION\n\nNowadays, physical therapies play a critical role in rehabilitation for post-operation patients and patients with a wide variety of diseases. However, physical therapies can be very expensive and inconvenient. A physical therapy session usually costs from $50 to $350 or more in the U.S. and a complete physical therapy process typically requires 2 to 3 sessions at the clinical centers per week for months and even years. As a result, physical therapies can be unaffordable for many patients especially for those not covered by medical insurance. To reduce in-clinic visits, in-home computer-assisted physical therapy solutions have attracted extensive attention. However, in-home rehabilitation can still be costly because it requires the presence of therapists for supervision and evaluation. Another important challenge is that the in-home environment is often computing resource constrained and thus calls for a computationally inexpensive system such as a mobile smartphone. Motivated by these observations, the goal of our research is to make rehabilitation at home both affordable and portable in a computing resource poor environment.\n\nIn addressing the challenges of human supervision and evaluation of physical activities, we focus on the intrinsic problem of a human pose estimation that detects the positions of human body key joint points (shoulder, elbow, knee, wrists, etc.) from a single image, a series of images by a single camera, or multiple images from multiple cameras. In this paper, we specifically aim at the scenario of using singleimage from a single camera which can lead to low-cost and effective solutions. The techniques developed for this scenario will provide an important foundation and insights into more general multi-camera multi-image cases.\n\nDue to the powerful representational capabilities of Convolutional Neural Networks (CNNs), the research on human pose estimation has witnessed significant advances recently [7] [8] [10] [11] [12] [13] [26]. For instance, High Resolution Network (HRNet) [13] leverages multiple resolution branches throughout the whole network and achieves the state-of-theart performance on public datasets, such as COCO Keypoints Detection Dataset [14] and MPII [15]. However, these state-ofthe-art solutions are too complicated to be deployed on mobile devices. For example, the number of parameters in HRNet model and Simple Baseline [12] model are up to 63 million and 68 million, respectively, rendering them impractical for a mobile device environment with limited computing resources. While it is possible to deploy the models as web services on powerful servers in the cloud, the privacy of patients is at risk as the original patient images are uploaded, which may violate strict privacy laws such as HIPAA and GDPR if not carefully handled. For this concern, we believe that a better solution is to have the system run locally on mobile devices. In this paper, we propose a lightweight HRNet. In particular, we introduce depth-wise and dilated convolutional layers in the original model to significantly reduce both the parameter size and computation cost. To further improve the accuracy, we add attention branches in the proposed model with very little extra computation operations, which extract additional features by compressing and recovering the channels of the input feature map. With these modifications, our model can be deployed in smartphones and achieve high accuracy.\n\nWe also address in this paper the limitations of available datasets. Current public datasets for human pose estimation tasks such as Leeds Sports Poses (LSP) [1], MPII [15] and COCO Keypoints Detection [14], etc., are all designed for general purposes. Activities recorded in these datasets are very different from the rehabilitation activities. Moreover, we find that the ground truth locations of the body key joint points can be inaccurate in these datasets. For example, in COCO Keypoints Detection dataset, the shoulder location may be distributed across the whole shoulder part, which is unacceptable for the fine-grained activity evaluation and analysis required for rehabilitation assessment. To this end, we create a new dataset of lower body rehabilitation exercises using a 3D capture system. A total of 31 different types of rehabilitation activities from twenty volunteers are collected under the guidance of a physical therapist. After raw videos and annotations are collected, we review and refine the keypoint positions to ensure accurate annotations for each frame. To the best of our knowledge, this is the first rehabilitation activities key points detection and evaluation dataset. Experiments show that the models trained on our dataset can achieve excellent performance.\n\nWe implement our models in a prototype system, which consists of three major components: a mobile application (with a built-in lightweight HRNet model) for patients, a mobile application for doctors, and a web server to share data between patients and their primary doctors. The application for patients is the key part of our system, collecting images of rehabilitation activities by the patients and processing them locally. Only the final analysis reports are uploaded to the server and shared with doctors. After reviewing the reports, doctors can provide feedback and arrange new rehabilitation plans for their patients, which are carried out in the application for doctors and sent to patients through the web server.\n\nIn summary, our main contributions in this paper include:\n\n\u2022 We create the first lower body rehabilitation human key points detection dataset, which focuses on the recognition of lower body rehabilitation exercises, and helps to improve the neural network model performance for pose estimation on rehabilitation activities. \u2022 We propose a lightweight human pose estimation model that runs on a smartphone smoothly without sacrificing much in accuracy. \u2022 We design an in-home lower body rehabilitation system that allows patients to carry out rehabilitation by themselves at home through a smartphone. Evaluation reports for rehabilitation activities of a patient are sent to his/her primary doctors so that they can follow up on the patient's progress and arrange new rehabilitation activity plans.\n\n\nII. RELATED WORK\n\n\nA. Lower body rehabilitation\n\nThe past decade has seen the rapid development of lower body rehabilitation in many cases. After knee arthroplasty, ambulation recovery is the primary concern for lower body rehabilitation patients. This requires a series of knee exercises that enable patients to improve the range of motion easily and carry out activities of daily life [20].\n\nAlong with the growth in clinical rehabilitation, however, there are increasing concerns over the cost and efficiency. The main challenge faced by clinical rehabilitation is that the recovery usually calls for a long-term intensive exercise program, which is time-consuming, expensive, and difficult. Motivated by this, several studies have been conducted on inhome rehabilitation. For instance, [21] tracks patients' movements via video capture virtual reality technology to reduce the cost while increasing efficiency. To further enhance patient engagement, [22] shows that well-designed video games such as motion-controlled video games could be an effective supplement to traditional physical therapy.\n\n\nB. Human pose estimation\n\nHuman pose estimation remains one of the hottest research topic for decades. Before the advent of CNN, people have devised a variety of features to detect body key joint points in images [3] [4] [5]. After the AlexNet [6] won the ImageNet challenge in 2012, CNNs have been widely used in human pose estimation [ [26]. For example, [26] proposes a Cascade Pyramid Networks to combine feature information from multiple scale representation maps. HRNet [13] keeps a high resolution feature representation branch through the whole architecture. The high resolution features will be augmented with lower resolution features. Benefiting from multiple resolution features, HRNet achieves the state-of-the-art performance in major public datasets, such as COCO keypoint detection dataset [14], MPII [15], and PoseTrack [16].\n\n\nC. Human body key points detection dataset\n\nThe COCO Keypoint Detection Dataset [14] has gone through different versions since its creation. The latest popular version was published in 2017. It contains more than 57K, 5K and 20K images for training, validation, and test respectively. The MPII Human Pose dataset [15] contains 25K images with more than 40K subjects, in which there are 28K subjects for training, and the rest for testing. The extended LSP dataset [1] consists of 11K training images from sports activities and 1K images for testing. All images in these datasets are collected from real-world scenarios across a wide range of activities. However, these activities are very different from rehabilitation exercises. The Human3.6M dataset [17] provides more than 3.6M images with labeled key points generated indoor by a motion tracking system from several volunteers. However, this dataset only covers several special daily activities, which are also different from rehabilitation activities.\n\n\nIII. OUR METHOD\n\n\nA. System description\n\nOur system is based on the latest deep learning algorithms tailored for human pose estimation. It can identify and track the movement of human joint points captured in live video or stored video files, without requiring any wearable accessories on the human body. Given the coordinates of the joints, we can calculate the angle of the target joints and evaluate the activities of the patients. As shown in Fig. 1, our research is mainly divided into three phases. In the first phase, we collect and preprocess data to construct Lower Body Rehabilitation  Dataset. Following this, we train the proposed Lightweight Pose Estimation Model and then deploy it in the third phase. Our prototype system can be deployed on a mobile platform as a Health Insurance Portability and Accountability Act (HIPPA)-compliant app, allowing greatest flexibility in location and time of physical therapy rehabilitation exercises.\n\n\nB. Lower body rehabilitation dataset\n\nIn the research for the rehabilitation assessment, it is necessary to build a specific dataset for lower body rehabilitation. First of all, we collect data in a motion lab, which consists of nine high-performance cameras and an optical motion tracking system (Fig. 2). Each camera covers a wide angle of 70 degrees view with a resolution of 1.7 MP, offering an expansive camera coverage with a capture rate of 360 FPS [2]. The optical motion capture system is equipped with the tracker's industryleading 3D reconstruction and rigid body solution. As shown in Fig. 3, we attach reflective markers on the human body joints axes. The nine cameras are placed around the human body, eight of which continuously track the movement of markers in \"Object Mode\". In such a way, these eight cameras record raw 2D video frames, which are then processed by Motive software to generate 3D coordinates of each marker.\n\n\nFig. 3: Marker Position\n\nThe remaining camera is placed in front of the human body at a height of 60 inches, perpendicular to the plane of interest, and is responsible for recording the frontal RGB video of the target. As a result, we obtain the ground truth 2D/3D locations of all the markers through the motion tracking system.\n\nThe main challenge faced during the data collection is the shift of markers on the body due to movement of limbs. Proper marker placement is vital for the quality of motion data because each marker on a tracked subject is used as indicators for both position and orientation. For the purpose of mounting marker on skin, we adopt the rigid plastic marker base to reinforce the stability of marker. Another challenge is the motion tracking software does not label the tracked markers automatically. Therefore we have to manually label each detected marker with an accurate keypoint frame by frame in the videos. With the motion tracking system, we are able to label markers through several consecutive time frames.\n\n\nC. Clinical requirements of lower body rehabilitation exercises\n\nIn our data collection process and later evaluation, we take into consideration the clinical requirements of lower body rehabilitation as advised by a licensed physical therapist. This is because (1) we aim to ensure that the exercise performed by volunteers meet the standard of rehabilitation purposes; and (2) the assessment on the rehabilitation exercise images relies  on the clinical guidelines and common practice with respect to quantitative metrics such as range of motion and angle of limbs.\n\nUnder the guidance of a licensed physical therapist, we select the most common and widely adopted 31 therapeutic exercises (Table I) in our study. These exercises can be divided into 3 categories: Supine, Standing, and Seated. During data collection, we strictly follow the standards provided by therapists. Some details are described as follows.\n\nFor the speed of motion, all exercises are recommended to be performed slowly which is rule number one for a home exercise program, especially in the early stage of recovery. In a newly operated total knee replacement, the knee Range of Motion (ROM) is usually limited. This is particularly true for patients after the knee replacement operation. Take Seated Heel Raise as an example, patients only need to lift the heel off the ground to be considered as completed.\n\nFor the range of motion, note that in Table I, actions can be very similar to each other. For instance, Small Range Straight Leg Raise requires a 30 to 45 degrees raise of legs while Normal Range Straight Leg Raise normally asks for raising leg from 45 to 75 degrees. Meanwhile, for these two actions, it is important to point the toes up in the exercising leg. The inactive leg must be bent on the bed to protect the patient's lower back. Such quantitative metrics are counted during rehabilitation activity evaluation. For the activity type of standing and sitting, the upper body usually needs to be kept upright. This can be assisted by supporting objects. For example, in the activity of Sit to Stand and Seated Hip Flexion, patients can support their trunks by holding a chair with their hands to keep their bodies stable. In addition, another rule is that a patient should move his/her hip, knee and ankle joints in normal alignment. This rule applies to Supine Heel Slides, Sidelying Hip Abduction, Seated Active Assistive Knee Extension and Flexion Foot on Floor, Standing Hip Flexion with Chair Support, Seated Long Arc Quad, and Seated Knee Flexion Extension AROM, etc.\n\nThe range of motion angles and the body position are important indicators to a physical therapist to evaluate the compliance and effectiveness of rehabilitation exercises. We strive to pay attention to these metrics and requirements during the data collection process where volunteers are trained and instructed to perform in the motion lab. Our goal is to use such a dataset to train neural network models which can be utilized to recognize the keypoints and calculate range of motion and assess compliance to body position requirements.\n\nIn the data collection process, each volunteer performs  Fig. 4(e). Better viewed in color. 31 activities that lead to about 53 videos recorded (some require both left and right camera views), and the duration of each video is approximately 25 seconds. Generally, for each volunteer, it takes about 2 hours to collect satisfactory movement data as the volunteers need to be trained. After collecting raw 2D/3D data of each person, it takes about 10 hours to complete the post-processing which requires extensive time to label the keypoints in video frames based on marker locations.\n\n\nD. Lightweight human pose estimation model\n\nWe make effort to design a neural network model that is both accurate and lightweight in computation. MobileNet [18] is the most widely adopted deep convolutional neural network backbone for computer vision applications deployed on mobile devices. In [28], the authors implement a real-time human pose estimation model running on CPU based on MobileNetv1 [18], however, with moderate performance. Meanwhile, other deep CNN backbones for pose estimation, such as Hourglass [8], ResNet [12] and HRNet [13], have achieved great performance. In particular, HRNet leverages multiple resolution branches, which keeps the high resolution branch over the whole network, and gradually adds a lower resolution branch in each following stage. At the end of each stage, the feature maps from all other lower resolution branches are merged to the high resolution branch. Through this procedure, the HRNet has excellent representation capability. Nevertheless, HRNet network is too complicated to be deployed on mobile devices.\n\nTo benefit from the advantage of the HRNet and the MobileNet, we adopt HRNet as the backbone and leverage the modules in MobileNetv1 to simplify the original network. Specifically, the depth-wise separable convolution was first proposed in MobileNet [18], which is one of the most common choices in lightweight deep CNN models recently. The process of the standard convolution is showed in Fig. 4(a). While the two steps of the depth-wise separable convolution are showed in Fig. 4(b). The computation cost for the standard convolution in Fig. 4(a) is calculated by (assuming padding is used):\nD K \u00d7 D K \u00d7 M \u00d7 N \u00d7 D F \u00d7 D F(1)\nwhere D K \u00d7 D K is the kernel size, D F \u00d7 D F is the feature map size, M is the number of input channels and N is the number of output channels. In contrast, the computation cost for the depth-wise separable convolution in Fig. 4(b) is:\nD K \u00d7 D K \u00d7 M \u00d7 D F \u00d7 D F + M \u00d7 N \u00d7 D F \u00d7 D F (2)\nComparing these two, we get a reduction in computation of:\nD K \u00d7 D K \u00d7 M \u00d7 D F \u00d7 D F + M \u00d7 N \u00d7 D F \u00d7 D F D K \u00d7 D K \u00d7 M \u00d7 N \u00d7 D F \u00d7 D F = 1 N + 1 D 2 K(3)\nIn this paper, we use 3x3 filters in depth-wise separable convolution which saves about 90% of the computation compared to the standard convolution. The attention mechanism has demonstrated salient advantages in the computer vision community [24]. In this paper, we design a special channel attention module as illustrated in Fig. 4(c). In this module, the feature map's channels are compressed to a quarter of the original size before they are recovered to the original number of channels, which generates attention-aware features. All these operations are implemented by 1x1 convolutional layers.\n\nWe also leverage the dilated convolution [23] for its spatial features extraction capability, which is a technical improvement over the standard convolution to extract feature information from a wider area of the input with the same computation cost. In Fig. 4(d), the bottom block shows the process of the dilated convolution with a rate of 2 compared with the standard convolution at the top.\n\nCombining the modules above, we obtain a novel Attention EESP (Extremely Efficient Spatial Pyramid of Depth-wise Dilated Separable Convolutions [19]) block that not only uses  Fig. 7: MotionPal Application Architecture much fewer parameters and computation operations than those in the original HRNet architecture but also processes excellent representation capability. The original EESP unit splits the standard convolution to several branches and chooses depthwise separable convolutions to reduce most of the computational cost. Meanwhile, it adopts the dilated convolutions to improve the feature representation capability. We redesign the EESP unit by adding a channel attention branch. Fig. 4(e) shows the architecture of our attention EESP block. As shown in Fig. 4(c), the convolutional filters used in our attention branch are of 1x1 size, which has a computational cost of:\n2 \u00d7 C \u00d7 C 4 \u00d7 D F \u00d7 D F + C 4 \u00d7 C 4 \u00d7 D F \u00d7 D F = 9 16 \u00d7 C \u00d7 C \u00d7 D F \u00d7 D F(4)\nwhere D F \u00d7 D F is the feature map size and C is the input channel. The cost is much smaller than that of the standard convolution in Equation (1). Fig. 5 shows the architecture of our model. The red arrows represent our attention EESP blocks as shown in Fig. 4(e), and the rest are the same as in HRNet.\n\n\nE. MotionPal Application\n\nLeveraging the refined neural network model, we have designed an application MotionPal on a mobile platform to perform the rehabilitation motion capture and analysis functionalities without any wearable accessories (Fig. 7). It executes our novel deep learning algorithm and provides insights into the design of a new scenario of in-home rehabilitation. More importantly, because we introduced different roles and interfaces for therapists and patients, MotionPal provides a bridge between therapists and patients for evaluating the rehabilitation outcomes.\n\nMotionPal allows a patient to perform rehab exercise at home following the instructions and receive an analysis report based on the trained model. The only preparatory work a patient has to do is placing the smartphone at the optimal recording location at a height of 60 inches, as same as the position of our video mode camera during the data collection process. The analysis report will be shared with the therapist responsible for the patient. As shown in Fig. 6, the screenshots present the main features of MotionPal. A patient client can use the app to preview the exercise activity video, perform the exercise activity, review the analysis detail report, and modify   the basic health information. For a therapist, MotionPal allows him/her to review the patients list, the basic health information, the detailed analysis report, and the activities progress made by each patient. Moreover, the therapist is able to assign new exercise activity to patients. This application was built to be compliant with the Health Insurance Portability and Accountability Act (HIPAA), including strong password protection and 128-bit encryption. In fact, as the video capture and analysis are executed locally on the smartphone and all of the data will be stored in the smartphone of the user, there is no protected health information (PHI) that is transferred outside from the device.\n\n\nIV. EXPERIMENTS\n\nWe evaluate our proposed model on both public dataset (i.e. COCO2017) and our new lower body rehabilitation dataset. In this way, we can understand the generality and effectiveness of the model.\n\n\nA. COCO Keypoint Detection\n\nDataset. COCO keypoint detection dataset [14] contains over 200K images and 250K person instances labeled with 17 keypoints. We train our model on COCO train2017 dataset that consists of 57K images and 150K person instances. We evaluate our model on the val2017 set with 5000 images.\n\nEvaluation metric. The keypoint evaluation metrics used by COCO is Object Keypoint Similarity (OKS):\nOKS = \u03a3 i [exp(\u2212d 2 i /2s 2 k 2 i )\u03b4(v i > 0)]/\u03a3 i [\u03b4(v i > 0)].\nHere d i is the Euclidean distance between each corresponding ground truth and detected keypoint, v i is the visibility flag of the ground truth, s is the object scale, and k i is a per-keypoint constant that controls falloff. We report standard average precision and recall scores [25]  Training and Testing. We follow most of the default settings in HRNet [13] including the input size, data augmentation, and the Adam optimizer. But the learning schedule is different, which sets the initial base rate as 1e-2, and reduces it to 1e-3, 1e-4, and 1e-5 at the 20th, 170th, and 200th epochs, respectively. This schedule is chosen because we start the training from scratch. The training process is terminated within 210 epochs.\n\nTo test our model on the validation set, we adopt the two-stage top-down paradigm: detecting the person instance using a person detector, and then predicting keypoints. We use the same person detectors provided by HRNet [13] for the validation set.\n\nResults on the validation set. We compare the results of our model and other state-of-the-art methods in Table II. Our model adopts HRNet-W32 as the backbone. We train the model from scratch with an input size of 256x192, which achieves an AP score of 70.8, with 4.7M parameters and only 1.0G FLOPs. This result outperforms other lightweight models, such as Lightweight OpenPose [28] and LPN [29]. Especially, our model achieves the best AP 50 score.\n\n\nB. Lower Body Rehabilitation Activities Keypoint Detection\n\nDataset and evaluation metric.. Our lower body rehabilitation activities keypoint detection dataset is organized following the COCO dataset. Our dataset is split into a training set and a validation set of 500,000 images and 10,000 images, respectively. And the images in the training set and the validation set are from different volunteers. The ground truth results are stored in JSON files in the same structure as in the COCO dataset. There are 30 keypoints for each person instance in our dataset, and there is only one person in each image. All the images are of 720p resolution with the same size. The evaluation metric used in our dataset is the same as in the COCO dataset.\n\nTraining and Testing. Firstly, we modify the COCO API codes to process the images in our dataset and calculate the standard average precision and recall scores. Then we follow the same input size, data augmentation, Adam optimizer, and the learning schedule as for the COCO dataset. The input size is 256x192 for all models in our experiments. And all models are trained from scratch.\n\nResults on the validation set. We compare the results of our model and the benchmark methods in Table III. While the original HRNet-W32 [13] network achieves the best performance with an 89.3 AP score, our model performs slightly worse with an AP score of 88.3, better than the SimpleBaseline [12] with ResNet-50. Note that our model size (#Params) is 16% of HRNet-W32 and the complexity (FLOPs) is 14%.\n\n\nC. Ablation Study\n\nTo investigate the effectiveness of our architecture, we carry out the ablative analysis on the COCO validation set so that the results can be compared with prior work. We compare the models with and without different types of attention branches. In our attention branch, we first compress the number of channels to c/s, where c is the number of channels in the input feature map, s is the scale parameter. After a convolutional layer, the number of channels is recovered to c. Firstly, the original EESP blocks without attention branches are used to replace the deep convolutional blocks in the HRNet-W32. Then, we test our attention EESP block with different scales for channel attention branches. All these models are trained from scratch on COCO training set with the input size 256x192 and tested on COCO validation set. As shown in Table IV, all the models using attention branches achieve better AP scores than that without attention branches.\n\n\nV. CONCLUSION\n\nIn this paper, we propose a human pose estimation based lower body rehabilitation system to assistant patients with lower body rehabilitation activities at home by themselves only through a smartphone. We also redesign a lightweight deep CNN model for human pose estimation that runs on a mobile device smoothly. Experiment results show that our model, using much fewer parameters and less computation cost, achieve comparable performance with the state-of-theart method.\n\nThe future work includes improving the deep CNN model for mobile devices, extending the rehabilitation detection dataset to contain more types of rehabilitation activities from more patients, and increasing the 3D location information of the human body keypoints for 3D pose estimation.\n\nFig. 2 :\n2(a) Motion Lab environment. (b) Cameras placement. (c) Motion tracking system screenshot.\n\nFig. 4 :\n4Illustrating the details used in our proposed model.\n\nFig. 5 :\n5Illustrating the architecture of the proposed model. It keeps the multiple resolution subnetworks, down samplings and up samplings from the original HRNet. But the deep convolutional blocks are replaced by the attention EESP blocks in\n\nFig. 6 :\n6MotionPal User Interfaces (left to right: Assign Exercise Action, Preview Exercise Video, Perform Exercise Action)\n\n\nFig. 1: Illustration of the three phases of our research. In Phase I, we collect and preprocess data to produce Lower Body Rehabilitation Dataset. Phase II shows our training process of the proposed Lightweight Pose Estimation Model. After that, we deploy it in the last phase.Randomly \ninitiate \nparameters \n\nTraining on \ntrain-set \n\n... \n\nCalculate \nloss \n\nValidation \non val-set \n\nUpdate parameters \n\nAdam optimizer \n\nConverge \n\n... \n\nLower Body Rehabilitation \nDataset \n\n3 categories \n31 different activities \n20 volunteers \n\n\n\n\nSupine Small Range Straight Leg Raise Supine Alternating Small Range Straight Leg Raise Supine Short Arc Quad Supine Bridge Supine Hip Abduction Supine Knee to Chest with Leg Straight Supine Heel Slides Normal Range Straight Leg Raise VMO Straight Leg Raise Sidelying Hip Abduction Mini Squat with Counter SupportActivity Name \nActivity Type \nSupine Ankle Pumps \n\nStanding \n\nStanding March with Counter Support \nStanding Hip Abduction \nStanding Hip Adduction \nStanding Hip Flexion with Chair Support \nStanding Marching \nStanding Hip Extension \nStep Up \nStep Down \nLateral Step Ups \nSeated Ankle Circles \n\nSeated \n\nSeated Ankle Pumps \nSeated Active Assistive Knee Extension and Flex-\nion Foot on Floor \nSit to Stand \nSquat with Chair Touch \nSeated Long Arc Quad \nSeated March \nSeated Hip Flexion \nSeated Knee Flexion Extension AROM \nSeated Heel Raise \n\n\n\nTABLE I :\nIRehabilitation Activities in our study. It contains three main categories: Supine(11 exercises), Standing(10 exercises), and Seated(10 exercises).\n\nTABLE II :\nIIComparisons on the COCO validation set. Pretrain = pretrain the backbone on the ImageNet classification task.Method \nBackbone \n#Params \nGFLOPs \nAP \nSimpleBaseline [12] \nResNet-50 \n34.0M \n8.90 \n87.0 \nHRNet-W32 [13] \nHRNet-W32 \n28.5M \n7.10 \n89.3 \nOurs \nHRNet-W32 \n4.7M \n1.0 \n88.3 \n\n\n\nTABLE III :\nIIIComparisons on our Lower Body Rehabilitation \nActivities Keypoint Detection validation set. The input size is \n256x192 for all models. \n\n\n\n\n: AP (AP at OKS=.50:.05:.95, primary challenge metric), AP 50 (AP at OKS=.50, looseMethod \n#Params \nGFLOPs \nAP \nHRNet-W32 [13] \n28.5M \n7.10 \n73.4 \nHRNet-W32 + EESP block \n3.2M \n0.65 \n65.9 \nHRNet-W32 + Attention EESP block s=2 \n7.8M \n1.8 \n71.2 \nHRNet-W32 + Attention EESP block s=4 \n4.7M \n1.0 \n70.8 \nHRNet-W32 + Attention EESP block s=8 \n3.8M \n0.79 \n67.9 \n\n\n\nTABLE IV :\nIVComparisons on the COCO validation set. The input size is 256x192 for all models. metric), AP 75 (AP at OKS=.75, strict metric); AP M (AP for medium objects) , AP L (AP for large objects); and AR at OKS=.50:.05:.95.\n\nClustered pose and nonlinear appearance models for human pose estimation. S Johnson, M Everingham, British Machine Vision Conference. 25S. Johnson, M. Everingham, \"Clustered pose and nonlinear appearance models for human pose estimation,\" In British Machine Vision Confer- ence, volume 2, page 5, 2010.\n\nPrime 17W In Depth. Optitrack, OptiTrack. (2020, Jan.) \"Prime 17W In Depth\".[Online]. Available: https://optitrack.com/products/prime-17w/indepth.html\n\nProgressive search space reduction for human pose estimation. V Ferrari, M Marin-Jimenez, A Zisserman, CVPRV. Ferrari, M. Marin-Jimenez, A. Zisserman, \"Progressive search space reduction for human pose estimation,\" CVPR, 2008.\n\nPictorial Structures Revisited: People Detection and Articulated Pose Estimation. M Andriluka, S Roth, B Schiele, CVPRM. Andriluka, S. Roth, B. Schiele., \"Pictorial Structures Revisited: People Detection and Articulated Pose Estimation,\" CVPR, 2009.\n\nPose Estimation of Interacting People using Pictorial Structures. Preben Fihl, Thomas B Moeslund, Seventh IEEE International Conference on. Advanced Video and Signal Based Surveillance (AVSS)Preben Fihl, Thomas B. Moeslund, \"Pose Estimation of Interacting People using Pictorial Structures,\" Advanced Video and Signal Based Surveillance (AVSS) 2010 Seventh IEEE International Conference on, pp. 462-468, 2010.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in Neural Information Processing Systems(NeurIPS). Krizhevsky, A., Sutskever, I., Hinton, G.E., \"Imagenet classification with deep convolutional neural networks,\" Advances in Neural Information Processing Systems(NeurIPS), pp. 1097-1105, 2012.\n\nConvolutional pose machines. S E Wei, V Ramakrishna, T Kanade, Y Sheikh, CVPRWei, S.E., Ramakrishna, V., Kanade, T., Sheikh, Y., \"Convolutional pose machines,\" CVPR, 2016.\n\nStacked hourglass networks for human pose estimation. A Newell, K Yang, J Deng, ECCVA. Newell, K. Yang, and J. Deng, \"Stacked hourglass networks for human pose estimation,\" ECCV, 2016.\n\nRealtime multi-person 2d pose estimation using part affinity fields. Z Cao, T Simon, S E Wei, Y Sheikh, Cao, Z., Simon, T., Wei, S.E., Sheikh, Y., \"Realtime multi-person 2d pose estimation using part affinity fields,\" CVPR, 2017.\n\nLearning feature pyramids for human pose estimation. W Yang, S Li, W Ouyang, H Li, X Wang, IEEE International Conference on Computer Vision(ICCV. Yang, W., Li, S., Ouyang, W., Li, H., Wang, X., \"Learning feature pyramids for human pose estimation,\" IEEE International Conference on Computer Vision(ICCV), 2017.\n\nCascaded pyramid network for multi-person pose estimation. Y Chen, Z Wang, Y Peng, Z Zhang, G Yu, J Sun, CVPRChen, Y., Wang, Z., Peng, Y., Zhang, Z., Yu, G., Sun, J., \"Cascaded pyramid network for multi-person pose estimation,\" CVPR, 2018.\n\nSimple baselines for human pose estimation and tracking. B Xiao, H Wu, Y Wei, ECCV. B. Xiao, H. Wu, and Y. Wei, \"Simple baselines for human pose estimation and tracking,\" ECCV, pages 472-487, 2018\n\nDeep High-Resolution Representation Learning for Human Pose Estimation. K Sun, B Xiao, D Liu, J Wang, arXiv:1902.09212K. Sun, B. Xiao, D. Liu, J. Wang, \"Deep High-Resolution Representa- tion Learning for Human Pose Estimation,\" arXiv:1902.09212, 2019\n\nMicrosoft COCO: common objects in context. T Lin, M Maire, S J Belongie, J Hays, P Perona, D Ramanan, P Dollar, C L Zitnick, ECCV. T. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick, \"Microsoft COCO: common objects in context,\" ECCV, pages 740-755, 2014.\n\n2d human pose estimation: New benchmark and state of the art analysis. M Andriluka, L Pishchulin, P V Gehler, B Schiele, CVPR. M. Andriluka, L. Pishchulin, P. V. Gehler, and B. Schiele, \"2d human pose estimation: New benchmark and state of the art analysis,\" CVPR, pages 3686-3693, 2014.\n\nPosetrack: Joint multi-person pose estimation and tracking. U Iqbal, A Milan, J Gall, CVPR. U. Iqbal, A. Milan, and J. Gall, \"Posetrack: Joint multi-person pose estimation and tracking,\" CVPR, pages 4654-4663, 2017.\n\nC Ionescu, D Papava, V Olaru, C Sminchisescu, Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments,\" PAMI'13. C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, \"Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments,\" PAMI'13, 2013.\n\nMobilenets: Efficient convolutional neural networks for mobile vision applications. A Howard, M Zhu, B Chen, D Kalenichenko, W Wang, T Weyand, M Andreetto, H Adam, arXiv:1704.04861A. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, \"Mobilenets: Efficient convolutional neural networks for mobile vision applications,\" arXiv:1704.04861, 2017.\n\nEspnetv2: A light-weight, power efficient, and general purpose convolutional neural network. S Mehta, M Rastegari, L Shapiro, H Hajishirzi, CoRR, abs/1811.11431S. Mehta, M. Rastegari, L. Shapiro, and H. Hajishirzi, \"Espnetv2: A light-weight, power efficient, and general purpose convolutional neural network,\" CoRR, abs/1811.11431, 2018.\n\nRehabilitation after total knee replacement: time to go home?. D Shakespeare, V Kinzel, Knee. 123Shakespeare D, Kinzel V., \"Rehabilitation after total knee replacement: time to go home?\" Knee. 2005;12(3):185-189.\n\nVideo capture virtual reality as a flexible and effective rehabilitation tool. P L Weiss, D Rand, N Katz, Journal of NeuroEngineering and Rehabilitation. 1112Weiss PL, Rand D, Katz N, et al. \"Video capture virtual reality as a flexible and effective rehabilitation tool,\" Journal of NeuroEngineering and Rehabilitation. 2004;1(1):12.\n\nVideogames and rehabilitation: Using design principles to enhance patient engagement. K R Lohse, N Shirzad, A Verster, N J Hodges, Hfm Van Der Loos, Journal of Neurologic Physical Therapy. 37Lohse KR, Shirzad N, Verster A, Hodges NJ, Van der Loos HFM, \"Videogames and rehabilitation: Using design principles to enhance patient engagement,\" Journal of Neurologic Physical Therapy. 2013. 37: 166-175.\n\nMulti-scale context aggregation by dilated convolutions. F Yu, V Koltun, ICLRF. Yu and V. Koltun, \"Multi-scale context aggregation by dilated convolutions,\" ICLR, 2016.\n\nResidual attention network for image classification. F Wang, M Jiang, C Qian, S Yang, C Li, H Zhang, X Wang, X Tang, F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang, X. Wang, and X. Tang, \"Residual attention network for image classification,\" CVPR, 2017.\n\nCOCO Keypoints Evaluation. Mscoco, MSCOCO. (2020, Jan.) \"COCO Keypoints Evaluation\". [Online]. Avail- able: http://cocodataset.org/#keypoints-eval.\n\nCascaded pyramid network for multi-person pose estimation. Y Chen, Z Wang, Y Peng, Z Zhang, G Yu, J Sun, CVPRY. Chen, Z. Wang, Y. Peng, Z. Zhang, G. Yu, and J. Sun, \"Cascaded pyramid network for multi-person pose estimation\", CVPR, 2018.\n\nHuman Body Front And Back Coloring Pages. Coloringsky, ColoringSky. (2020, Jan.) \"Human Body Front And Back Coloring Pages\". [Online]. Available: https://www.coloringsky.com/human-body- front-and-back-coloring-pages.\n\nD Osokin, arXiv:1811.12004Real-time 2D Multi-Person Pose Estimation on CPU: Lightweight OpenPose. D. Osokin, \"Real-time 2D Multi-Person Pose Estimation on CPU: Lightweight OpenPose\", arXiv:1811.12004, 2018.\n\nSimple and Lightweight Human Pose Estimation. Z Zhang, J Tang, G Wu, arXiv:1911.10346Z. Zhang, J. Tang, G. Wu, \"Simple and Lightweight Human Pose Estimation\", arXiv:1911.10346, 2019.\n", "annotations": {"author": "[{\"start\":\"72\",\"end\":\"103\"},{\"start\":\"104\",\"end\":\"144\"},{\"start\":\"145\",\"end\":\"166\"},{\"start\":\"167\",\"end\":\"198\"},{\"start\":\"199\",\"end\":\"210\"},{\"start\":\"211\",\"end\":\"231\"},{\"start\":\"232\",\"end\":\"360\"},{\"start\":\"361\",\"end\":\"439\"},{\"start\":\"440\",\"end\":\"518\"},{\"start\":\"519\",\"end\":\"690\"},{\"start\":\"691\",\"end\":\"742\"}]", "publisher": null, "author_last_name": "[{\"start\":\"77\",\"end\":\"79\"},{\"start\":\"111\",\"end\":\"115\"},{\"start\":\"148\",\"end\":\"151\"},{\"start\":\"175\",\"end\":\"178\"},{\"start\":\"206\",\"end\":\"209\"},{\"start\":\"215\",\"end\":\"218\"}]", "author_first_name": "[{\"start\":\"72\",\"end\":\"76\"},{\"start\":\"104\",\"end\":\"110\"},{\"start\":\"145\",\"end\":\"147\"},{\"start\":\"167\",\"end\":\"174\"},{\"start\":\"199\",\"end\":\"205\"},{\"start\":\"211\",\"end\":\"214\"}]", "author_affiliation": "[{\"start\":\"233\",\"end\":\"359\"},{\"start\":\"362\",\"end\":\"438\"},{\"start\":\"441\",\"end\":\"517\"},{\"start\":\"520\",\"end\":\"689\"},{\"start\":\"692\",\"end\":\"741\"}]", "title": "[{\"start\":\"1\",\"end\":\"69\"},{\"start\":\"743\",\"end\":\"811\"}]", "venue": null, "abstract": "[{\"start\":\"813\",\"end\":\"2219\"}]", "bib_ref": "[{\"start\":\"4193\",\"end\":\"4196\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"4201\",\"end\":\"4205\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"4206\",\"end\":\"4210\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"4211\",\"end\":\"4215\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"4221\",\"end\":\"4225\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"4273\",\"end\":\"4277\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"4452\",\"end\":\"4456\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"4466\",\"end\":\"4470\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"4640\",\"end\":\"4644\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"5854\",\"end\":\"5857\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"5864\",\"end\":\"5868\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"5898\",\"end\":\"5902\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"8903\",\"end\":\"8907\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"9306\",\"end\":\"9310\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"9470\",\"end\":\"9474\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"9831\",\"end\":\"9834\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"9839\",\"end\":\"9842\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"9862\",\"end\":\"9865\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"9954\",\"end\":\"9955\"},{\"start\":\"9956\",\"end\":\"9960\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"9975\",\"end\":\"9979\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"10094\",\"end\":\"10098\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"10424\",\"end\":\"10428\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"10435\",\"end\":\"10439\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"10455\",\"end\":\"10459\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"10543\",\"end\":\"10547\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"10776\",\"end\":\"10780\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"10927\",\"end\":\"10930\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"11215\",\"end\":\"11219\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"12881\",\"end\":\"12884\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"18262\",\"end\":\"18266\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"18401\",\"end\":\"18405\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"18505\",\"end\":\"18509\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"18622\",\"end\":\"18625\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"18634\",\"end\":\"18638\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"18649\",\"end\":\"18653\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"19415\",\"end\":\"19419\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"20475\",\"end\":\"20479\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"20874\",\"end\":\"20878\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"21373\",\"end\":\"21377\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"24745\",\"end\":\"24749\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"25437\",\"end\":\"25441\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"25513\",\"end\":\"25517\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"26103\",\"end\":\"26107\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"26512\",\"end\":\"26516\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"26525\",\"end\":\"26529\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"27852\",\"end\":\"27856\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"28009\",\"end\":\"28013\",\"attributes\":{\"ref_id\":\"b11\"}}]", "figure": "[{\"start\":\"29869\",\"end\":\"29969\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"29970\",\"end\":\"30033\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"30034\",\"end\":\"30279\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"30280\",\"end\":\"30405\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"30406\",\"end\":\"30938\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}},{\"start\":\"30939\",\"end\":\"31793\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}},{\"start\":\"31794\",\"end\":\"31952\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}},{\"start\":\"31953\",\"end\":\"32247\",\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"}},{\"start\":\"32248\",\"end\":\"32401\",\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"}},{\"start\":\"32402\",\"end\":\"32760\",\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"}},{\"start\":\"32761\",\"end\":\"32990\",\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"2238\",\"end\":\"3381\"},{\"start\":\"3383\",\"end\":\"4018\"},{\"start\":\"4020\",\"end\":\"5694\"},{\"start\":\"5696\",\"end\":\"6988\"},{\"start\":\"6990\",\"end\":\"7713\"},{\"start\":\"7715\",\"end\":\"7772\"},{\"start\":\"7774\",\"end\":\"8513\"},{\"start\":\"8565\",\"end\":\"8908\"},{\"start\":\"8910\",\"end\":\"9615\"},{\"start\":\"9644\",\"end\":\"10460\"},{\"start\":\"10507\",\"end\":\"11469\"},{\"start\":\"11513\",\"end\":\"12422\"},{\"start\":\"12463\",\"end\":\"13366\"},{\"start\":\"13394\",\"end\":\"13698\"},{\"start\":\"13700\",\"end\":\"14412\"},{\"start\":\"14480\",\"end\":\"14981\"},{\"start\":\"14983\",\"end\":\"15329\"},{\"start\":\"15331\",\"end\":\"15797\"},{\"start\":\"15799\",\"end\":\"16979\"},{\"start\":\"16981\",\"end\":\"17519\"},{\"start\":\"17521\",\"end\":\"18103\"},{\"start\":\"18150\",\"end\":\"19163\"},{\"start\":\"19165\",\"end\":\"19758\"},{\"start\":\"19792\",\"end\":\"20028\"},{\"start\":\"20079\",\"end\":\"20137\"},{\"start\":\"20233\",\"end\":\"20831\"},{\"start\":\"20833\",\"end\":\"21227\"},{\"start\":\"21229\",\"end\":\"22112\"},{\"start\":\"22191\",\"end\":\"22495\"},{\"start\":\"22524\",\"end\":\"23081\"},{\"start\":\"23083\",\"end\":\"24459\"},{\"start\":\"24479\",\"end\":\"24673\"},{\"start\":\"24704\",\"end\":\"24987\"},{\"start\":\"24989\",\"end\":\"25089\"},{\"start\":\"25155\",\"end\":\"25881\"},{\"start\":\"25883\",\"end\":\"26131\"},{\"start\":\"26133\",\"end\":\"26583\"},{\"start\":\"26646\",\"end\":\"27328\"},{\"start\":\"27330\",\"end\":\"27714\"},{\"start\":\"27716\",\"end\":\"28119\"},{\"start\":\"28141\",\"end\":\"29091\"},{\"start\":\"29109\",\"end\":\"29580\"},{\"start\":\"29582\",\"end\":\"29868\"}]", "formula": "[{\"start\":\"19759\",\"end\":\"19791\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"20029\",\"end\":\"20078\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"20138\",\"end\":\"20232\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"22113\",\"end\":\"22190\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"25090\",\"end\":\"25154\",\"attributes\":{\"id\":\"formula_4\"}}]", "table_ref": "[{\"start\":\"15837\",\"end\":\"15844\",\"attributes\":{\"ref_id\":\"tab_3\"}},{\"start\":\"26238\",\"end\":\"26246\",\"attributes\":{\"ref_id\":\"tab_3\"}},{\"start\":\"27812\",\"end\":\"27821\",\"attributes\":{\"ref_id\":\"tab_3\"}},{\"start\":\"28979\",\"end\":\"28987\",\"attributes\":{\"ref_id\":\"tab_3\"}}]", "section_header": "[{\"start\":\"2221\",\"end\":\"2236\"},{\"start\":\"8516\",\"end\":\"8532\"},{\"start\":\"8535\",\"end\":\"8563\"},{\"start\":\"9618\",\"end\":\"9642\"},{\"start\":\"10463\",\"end\":\"10505\"},{\"start\":\"11472\",\"end\":\"11487\"},{\"start\":\"11490\",\"end\":\"11511\"},{\"start\":\"12425\",\"end\":\"12461\"},{\"start\":\"13369\",\"end\":\"13392\"},{\"start\":\"14415\",\"end\":\"14478\"},{\"start\":\"18106\",\"end\":\"18148\"},{\"start\":\"22498\",\"end\":\"22522\"},{\"start\":\"24462\",\"end\":\"24477\"},{\"start\":\"24676\",\"end\":\"24702\"},{\"start\":\"26586\",\"end\":\"26644\"},{\"start\":\"28122\",\"end\":\"28139\"},{\"start\":\"29094\",\"end\":\"29107\"},{\"start\":\"29870\",\"end\":\"29878\"},{\"start\":\"29971\",\"end\":\"29979\"},{\"start\":\"30035\",\"end\":\"30043\"},{\"start\":\"30281\",\"end\":\"30289\"},{\"start\":\"31795\",\"end\":\"31804\"},{\"start\":\"31954\",\"end\":\"31964\"},{\"start\":\"32249\",\"end\":\"32260\"},{\"start\":\"32762\",\"end\":\"32772\"}]", "table": "[{\"start\":\"30685\",\"end\":\"30938\"},{\"start\":\"31254\",\"end\":\"31793\"},{\"start\":\"32076\",\"end\":\"32247\"},{\"start\":\"32264\",\"end\":\"32401\"},{\"start\":\"32487\",\"end\":\"32760\"}]", "figure_caption": "[{\"start\":\"29880\",\"end\":\"29969\"},{\"start\":\"29981\",\"end\":\"30033\"},{\"start\":\"30045\",\"end\":\"30279\"},{\"start\":\"30291\",\"end\":\"30405\"},{\"start\":\"30408\",\"end\":\"30685\"},{\"start\":\"30941\",\"end\":\"31254\"},{\"start\":\"31806\",\"end\":\"31952\"},{\"start\":\"31967\",\"end\":\"32076\"},{\"start\":\"32404\",\"end\":\"32487\"},{\"start\":\"32775\",\"end\":\"32990\"}]", "figure_ref": "[{\"start\":\"11919\",\"end\":\"11925\"},{\"start\":\"12722\",\"end\":\"12730\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"13022\",\"end\":\"13028\"},{\"start\":\"17578\",\"end\":\"17587\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"19555\",\"end\":\"19564\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"19640\",\"end\":\"19649\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"19704\",\"end\":\"19713\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"20015\",\"end\":\"20024\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"20559\",\"end\":\"20568\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"21087\",\"end\":\"21096\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"21405\",\"end\":\"21411\"},{\"start\":\"21921\",\"end\":\"21927\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"21995\",\"end\":\"22004\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"22339\",\"end\":\"22345\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"22446\",\"end\":\"22455\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"22739\",\"end\":\"22747\"},{\"start\":\"23542\",\"end\":\"23548\",\"attributes\":{\"ref_id\":\"fig_4\"}}]", "bib_author_first_name": "[{\"start\":\"33066\",\"end\":\"33067\"},{\"start\":\"33077\",\"end\":\"33078\"},{\"start\":\"33510\",\"end\":\"33511\"},{\"start\":\"33521\",\"end\":\"33522\"},{\"start\":\"33538\",\"end\":\"33539\"},{\"start\":\"33758\",\"end\":\"33759\"},{\"start\":\"33771\",\"end\":\"33772\"},{\"start\":\"33779\",\"end\":\"33780\"},{\"start\":\"33993\",\"end\":\"33999\"},{\"start\":\"34006\",\"end\":\"34012\"},{\"start\":\"34013\",\"end\":\"34014\"},{\"start\":\"34403\",\"end\":\"34404\"},{\"start\":\"34417\",\"end\":\"34418\"},{\"start\":\"34430\",\"end\":\"34431\"},{\"start\":\"34432\",\"end\":\"34433\"},{\"start\":\"34725\",\"end\":\"34726\"},{\"start\":\"34727\",\"end\":\"34728\"},{\"start\":\"34734\",\"end\":\"34735\"},{\"start\":\"34749\",\"end\":\"34750\"},{\"start\":\"34759\",\"end\":\"34760\"},{\"start\":\"34923\",\"end\":\"34924\"},{\"start\":\"34933\",\"end\":\"34934\"},{\"start\":\"34941\",\"end\":\"34942\"},{\"start\":\"35124\",\"end\":\"35125\"},{\"start\":\"35131\",\"end\":\"35132\"},{\"start\":\"35140\",\"end\":\"35141\"},{\"start\":\"35142\",\"end\":\"35143\"},{\"start\":\"35149\",\"end\":\"35150\"},{\"start\":\"35339\",\"end\":\"35340\"},{\"start\":\"35347\",\"end\":\"35348\"},{\"start\":\"35353\",\"end\":\"35354\"},{\"start\":\"35363\",\"end\":\"35364\"},{\"start\":\"35369\",\"end\":\"35370\"},{\"start\":\"35657\",\"end\":\"35658\"},{\"start\":\"35665\",\"end\":\"35666\"},{\"start\":\"35673\",\"end\":\"35674\"},{\"start\":\"35681\",\"end\":\"35682\"},{\"start\":\"35690\",\"end\":\"35691\"},{\"start\":\"35696\",\"end\":\"35697\"},{\"start\":\"35896\",\"end\":\"35897\"},{\"start\":\"35904\",\"end\":\"35905\"},{\"start\":\"35910\",\"end\":\"35911\"},{\"start\":\"36109\",\"end\":\"36110\"},{\"start\":\"36116\",\"end\":\"36117\"},{\"start\":\"36124\",\"end\":\"36125\"},{\"start\":\"36131\",\"end\":\"36132\"},{\"start\":\"36332\",\"end\":\"36333\"},{\"start\":\"36339\",\"end\":\"36340\"},{\"start\":\"36348\",\"end\":\"36349\"},{\"start\":\"36350\",\"end\":\"36351\"},{\"start\":\"36362\",\"end\":\"36363\"},{\"start\":\"36370\",\"end\":\"36371\"},{\"start\":\"36380\",\"end\":\"36381\"},{\"start\":\"36391\",\"end\":\"36392\"},{\"start\":\"36401\",\"end\":\"36402\"},{\"start\":\"36403\",\"end\":\"36404\"},{\"start\":\"36660\",\"end\":\"36661\"},{\"start\":\"36673\",\"end\":\"36674\"},{\"start\":\"36687\",\"end\":\"36688\"},{\"start\":\"36689\",\"end\":\"36690\"},{\"start\":\"36699\",\"end\":\"36700\"},{\"start\":\"36938\",\"end\":\"36939\"},{\"start\":\"36947\",\"end\":\"36948\"},{\"start\":\"36956\",\"end\":\"36957\"},{\"start\":\"37095\",\"end\":\"37096\"},{\"start\":\"37106\",\"end\":\"37107\"},{\"start\":\"37116\",\"end\":\"37117\"},{\"start\":\"37125\",\"end\":\"37126\"},{\"start\":\"37509\",\"end\":\"37510\"},{\"start\":\"37519\",\"end\":\"37520\"},{\"start\":\"37526\",\"end\":\"37527\"},{\"start\":\"37534\",\"end\":\"37535\"},{\"start\":\"37550\",\"end\":\"37551\"},{\"start\":\"37558\",\"end\":\"37559\"},{\"start\":\"37568\",\"end\":\"37569\"},{\"start\":\"37581\",\"end\":\"37582\"},{\"start\":\"37901\",\"end\":\"37902\"},{\"start\":\"37910\",\"end\":\"37911\"},{\"start\":\"37923\",\"end\":\"37924\"},{\"start\":\"37934\",\"end\":\"37935\"},{\"start\":\"38210\",\"end\":\"38211\"},{\"start\":\"38225\",\"end\":\"38226\"},{\"start\":\"38440\",\"end\":\"38441\"},{\"start\":\"38442\",\"end\":\"38443\"},{\"start\":\"38451\",\"end\":\"38452\"},{\"start\":\"38459\",\"end\":\"38460\"},{\"start\":\"38782\",\"end\":\"38783\"},{\"start\":\"38784\",\"end\":\"38785\"},{\"start\":\"38793\",\"end\":\"38794\"},{\"start\":\"38804\",\"end\":\"38805\"},{\"start\":\"38815\",\"end\":\"38816\"},{\"start\":\"38817\",\"end\":\"38818\"},{\"start\":\"38827\",\"end\":\"38830\"},{\"start\":\"39153\",\"end\":\"39154\"},{\"start\":\"39159\",\"end\":\"39160\"},{\"start\":\"39319\",\"end\":\"39320\"},{\"start\":\"39327\",\"end\":\"39328\"},{\"start\":\"39336\",\"end\":\"39337\"},{\"start\":\"39344\",\"end\":\"39345\"},{\"start\":\"39352\",\"end\":\"39353\"},{\"start\":\"39358\",\"end\":\"39359\"},{\"start\":\"39367\",\"end\":\"39368\"},{\"start\":\"39375\",\"end\":\"39376\"},{\"start\":\"39735\",\"end\":\"39736\"},{\"start\":\"39743\",\"end\":\"39744\"},{\"start\":\"39751\",\"end\":\"39752\"},{\"start\":\"39759\",\"end\":\"39760\"},{\"start\":\"39768\",\"end\":\"39769\"},{\"start\":\"39774\",\"end\":\"39775\"},{\"start\":\"40133\",\"end\":\"40134\"},{\"start\":\"40387\",\"end\":\"40388\"},{\"start\":\"40396\",\"end\":\"40397\"},{\"start\":\"40404\",\"end\":\"40405\"}]", "bib_author_last_name": "[{\"start\":\"33068\",\"end\":\"33075\"},{\"start\":\"33079\",\"end\":\"33089\"},{\"start\":\"33316\",\"end\":\"33325\"},{\"start\":\"33512\",\"end\":\"33519\"},{\"start\":\"33523\",\"end\":\"33536\"},{\"start\":\"33540\",\"end\":\"33549\"},{\"start\":\"33760\",\"end\":\"33769\"},{\"start\":\"33773\",\"end\":\"33777\"},{\"start\":\"33781\",\"end\":\"33788\"},{\"start\":\"34000\",\"end\":\"34004\"},{\"start\":\"34015\",\"end\":\"34023\"},{\"start\":\"34405\",\"end\":\"34415\"},{\"start\":\"34419\",\"end\":\"34428\"},{\"start\":\"34434\",\"end\":\"34440\"},{\"start\":\"34729\",\"end\":\"34732\"},{\"start\":\"34736\",\"end\":\"34747\"},{\"start\":\"34751\",\"end\":\"34757\"},{\"start\":\"34761\",\"end\":\"34767\"},{\"start\":\"34925\",\"end\":\"34931\"},{\"start\":\"34935\",\"end\":\"34939\"},{\"start\":\"34943\",\"end\":\"34947\"},{\"start\":\"35126\",\"end\":\"35129\"},{\"start\":\"35133\",\"end\":\"35138\"},{\"start\":\"35144\",\"end\":\"35147\"},{\"start\":\"35151\",\"end\":\"35157\"},{\"start\":\"35341\",\"end\":\"35345\"},{\"start\":\"35349\",\"end\":\"35351\"},{\"start\":\"35355\",\"end\":\"35361\"},{\"start\":\"35365\",\"end\":\"35367\"},{\"start\":\"35371\",\"end\":\"35375\"},{\"start\":\"35659\",\"end\":\"35663\"},{\"start\":\"35667\",\"end\":\"35671\"},{\"start\":\"35675\",\"end\":\"35679\"},{\"start\":\"35683\",\"end\":\"35688\"},{\"start\":\"35692\",\"end\":\"35694\"},{\"start\":\"35698\",\"end\":\"35701\"},{\"start\":\"35898\",\"end\":\"35902\"},{\"start\":\"35906\",\"end\":\"35908\"},{\"start\":\"35912\",\"end\":\"35915\"},{\"start\":\"36111\",\"end\":\"36114\"},{\"start\":\"36118\",\"end\":\"36122\"},{\"start\":\"36126\",\"end\":\"36129\"},{\"start\":\"36133\",\"end\":\"36137\"},{\"start\":\"36334\",\"end\":\"36337\"},{\"start\":\"36341\",\"end\":\"36346\"},{\"start\":\"36352\",\"end\":\"36360\"},{\"start\":\"36364\",\"end\":\"36368\"},{\"start\":\"36372\",\"end\":\"36378\"},{\"start\":\"36382\",\"end\":\"36389\"},{\"start\":\"36393\",\"end\":\"36399\"},{\"start\":\"36405\",\"end\":\"36412\"},{\"start\":\"36662\",\"end\":\"36671\"},{\"start\":\"36675\",\"end\":\"36685\"},{\"start\":\"36691\",\"end\":\"36697\"},{\"start\":\"36701\",\"end\":\"36708\"},{\"start\":\"36940\",\"end\":\"36945\"},{\"start\":\"36949\",\"end\":\"36954\"},{\"start\":\"36958\",\"end\":\"36962\"},{\"start\":\"37097\",\"end\":\"37104\"},{\"start\":\"37108\",\"end\":\"37114\"},{\"start\":\"37118\",\"end\":\"37123\"},{\"start\":\"37127\",\"end\":\"37139\"},{\"start\":\"37511\",\"end\":\"37517\"},{\"start\":\"37521\",\"end\":\"37524\"},{\"start\":\"37528\",\"end\":\"37532\"},{\"start\":\"37536\",\"end\":\"37548\"},{\"start\":\"37552\",\"end\":\"37556\"},{\"start\":\"37560\",\"end\":\"37566\"},{\"start\":\"37570\",\"end\":\"37579\"},{\"start\":\"37583\",\"end\":\"37587\"},{\"start\":\"37903\",\"end\":\"37908\"},{\"start\":\"37912\",\"end\":\"37921\"},{\"start\":\"37925\",\"end\":\"37932\"},{\"start\":\"37936\",\"end\":\"37946\"},{\"start\":\"38212\",\"end\":\"38223\"},{\"start\":\"38227\",\"end\":\"38233\"},{\"start\":\"38444\",\"end\":\"38449\"},{\"start\":\"38453\",\"end\":\"38457\"},{\"start\":\"38461\",\"end\":\"38465\"},{\"start\":\"38786\",\"end\":\"38791\"},{\"start\":\"38795\",\"end\":\"38802\"},{\"start\":\"38806\",\"end\":\"38813\"},{\"start\":\"38819\",\"end\":\"38825\"},{\"start\":\"38831\",\"end\":\"38843\"},{\"start\":\"39155\",\"end\":\"39157\"},{\"start\":\"39161\",\"end\":\"39167\"},{\"start\":\"39321\",\"end\":\"39325\"},{\"start\":\"39329\",\"end\":\"39334\"},{\"start\":\"39338\",\"end\":\"39342\"},{\"start\":\"39346\",\"end\":\"39350\"},{\"start\":\"39354\",\"end\":\"39356\"},{\"start\":\"39360\",\"end\":\"39365\"},{\"start\":\"39369\",\"end\":\"39373\"},{\"start\":\"39377\",\"end\":\"39381\"},{\"start\":\"39554\",\"end\":\"39560\"},{\"start\":\"39737\",\"end\":\"39741\"},{\"start\":\"39745\",\"end\":\"39749\"},{\"start\":\"39753\",\"end\":\"39757\"},{\"start\":\"39761\",\"end\":\"39766\"},{\"start\":\"39770\",\"end\":\"39772\"},{\"start\":\"39776\",\"end\":\"39779\"},{\"start\":\"39957\",\"end\":\"39968\"},{\"start\":\"40135\",\"end\":\"40141\"},{\"start\":\"40389\",\"end\":\"40394\"},{\"start\":\"40398\",\"end\":\"40402\"},{\"start\":\"40406\",\"end\":\"40408\"}]", "bib_entry": "[{\"start\":\"32992\",\"end\":\"33294\",\"attributes\":{\"matched_paper_id\":\"7318714\",\"id\":\"b0\"}},{\"start\":\"33296\",\"end\":\"33446\",\"attributes\":{\"id\":\"b1\"}},{\"start\":\"33448\",\"end\":\"33674\",\"attributes\":{\"id\":\"b2\"}},{\"start\":\"33676\",\"end\":\"33925\",\"attributes\":{\"id\":\"b3\"}},{\"start\":\"33927\",\"end\":\"34336\",\"attributes\":{\"matched_paper_id\":\"15452616\",\"id\":\"b4\"}},{\"start\":\"34338\",\"end\":\"34694\",\"attributes\":{\"matched_paper_id\":\"195908774\",\"id\":\"b5\"}},{\"start\":\"34696\",\"end\":\"34867\",\"attributes\":{\"id\":\"b6\"}},{\"start\":\"34869\",\"end\":\"35053\",\"attributes\":{\"id\":\"b7\"}},{\"start\":\"35055\",\"end\":\"35284\",\"attributes\":{\"id\":\"b8\"}},{\"start\":\"35286\",\"end\":\"35596\",\"attributes\":{\"matched_paper_id\":\"250792\",\"id\":\"b9\"}},{\"start\":\"35598\",\"end\":\"35837\",\"attributes\":{\"id\":\"b10\"}},{\"start\":\"35839\",\"end\":\"36035\",\"attributes\":{\"matched_paper_id\":\"4934594\",\"id\":\"b11\"}},{\"start\":\"36037\",\"end\":\"36287\",\"attributes\":{\"id\":\"b12\",\"doi\":\"arXiv:1902.09212\"}},{\"start\":\"36289\",\"end\":\"36587\",\"attributes\":{\"matched_paper_id\":\"14113767\",\"id\":\"b13\"}},{\"start\":\"36589\",\"end\":\"36876\",\"attributes\":{\"matched_paper_id\":\"206592419\",\"id\":\"b14\"}},{\"start\":\"36878\",\"end\":\"37093\",\"attributes\":{\"matched_paper_id\":\"5984868\",\"id\":\"b15\"}},{\"start\":\"37095\",\"end\":\"37423\",\"attributes\":{\"id\":\"b16\"}},{\"start\":\"37425\",\"end\":\"37806\",\"attributes\":{\"id\":\"b17\",\"doi\":\"arXiv:1704.04861\"}},{\"start\":\"37808\",\"end\":\"38145\",\"attributes\":{\"id\":\"b18\",\"doi\":\"CoRR, abs/1811.11431\"}},{\"start\":\"38147\",\"end\":\"38359\",\"attributes\":{\"matched_paper_id\":\"12649036\",\"id\":\"b19\"}},{\"start\":\"38361\",\"end\":\"38694\",\"attributes\":{\"matched_paper_id\":\"10711381\",\"id\":\"b20\"}},{\"start\":\"38696\",\"end\":\"39094\",\"attributes\":{\"id\":\"b21\"}},{\"start\":\"39096\",\"end\":\"39264\",\"attributes\":{\"id\":\"b22\"}},{\"start\":\"39266\",\"end\":\"39525\",\"attributes\":{\"id\":\"b23\"}},{\"start\":\"39527\",\"end\":\"39674\",\"attributes\":{\"id\":\"b24\"}},{\"start\":\"39676\",\"end\":\"39913\",\"attributes\":{\"id\":\"b25\"}},{\"start\":\"39915\",\"end\":\"40131\",\"attributes\":{\"id\":\"b26\"}},{\"start\":\"40133\",\"end\":\"40339\",\"attributes\":{\"id\":\"b27\",\"doi\":\"arXiv:1811.12004\"}},{\"start\":\"40341\",\"end\":\"40523\",\"attributes\":{\"id\":\"b28\",\"doi\":\"arXiv:1911.10346\"}}]", "bib_title": "[{\"start\":\"32992\",\"end\":\"33064\"},{\"start\":\"33927\",\"end\":\"33991\"},{\"start\":\"34338\",\"end\":\"34401\"},{\"start\":\"35286\",\"end\":\"35337\"},{\"start\":\"35839\",\"end\":\"35894\"},{\"start\":\"36289\",\"end\":\"36330\"},{\"start\":\"36589\",\"end\":\"36658\"},{\"start\":\"36878\",\"end\":\"36936\"},{\"start\":\"38147\",\"end\":\"38208\"},{\"start\":\"38361\",\"end\":\"38438\"},{\"start\":\"38696\",\"end\":\"38780\"}]", "bib_author": "[{\"start\":\"33066\",\"end\":\"33077\"},{\"start\":\"33077\",\"end\":\"33091\"},{\"start\":\"33316\",\"end\":\"33327\"},{\"start\":\"33510\",\"end\":\"33521\"},{\"start\":\"33521\",\"end\":\"33538\"},{\"start\":\"33538\",\"end\":\"33551\"},{\"start\":\"33758\",\"end\":\"33771\"},{\"start\":\"33771\",\"end\":\"33779\"},{\"start\":\"33779\",\"end\":\"33790\"},{\"start\":\"33993\",\"end\":\"34006\"},{\"start\":\"34006\",\"end\":\"34025\"},{\"start\":\"34403\",\"end\":\"34417\"},{\"start\":\"34417\",\"end\":\"34430\"},{\"start\":\"34430\",\"end\":\"34442\"},{\"start\":\"34725\",\"end\":\"34734\"},{\"start\":\"34734\",\"end\":\"34749\"},{\"start\":\"34749\",\"end\":\"34759\"},{\"start\":\"34759\",\"end\":\"34769\"},{\"start\":\"34923\",\"end\":\"34933\"},{\"start\":\"34933\",\"end\":\"34941\"},{\"start\":\"34941\",\"end\":\"34949\"},{\"start\":\"35124\",\"end\":\"35131\"},{\"start\":\"35131\",\"end\":\"35140\"},{\"start\":\"35140\",\"end\":\"35149\"},{\"start\":\"35149\",\"end\":\"35159\"},{\"start\":\"35339\",\"end\":\"35347\"},{\"start\":\"35347\",\"end\":\"35353\"},{\"start\":\"35353\",\"end\":\"35363\"},{\"start\":\"35363\",\"end\":\"35369\"},{\"start\":\"35369\",\"end\":\"35377\"},{\"start\":\"35657\",\"end\":\"35665\"},{\"start\":\"35665\",\"end\":\"35673\"},{\"start\":\"35673\",\"end\":\"35681\"},{\"start\":\"35681\",\"end\":\"35690\"},{\"start\":\"35690\",\"end\":\"35696\"},{\"start\":\"35696\",\"end\":\"35703\"},{\"start\":\"35896\",\"end\":\"35904\"},{\"start\":\"35904\",\"end\":\"35910\"},{\"start\":\"35910\",\"end\":\"35917\"},{\"start\":\"36109\",\"end\":\"36116\"},{\"start\":\"36116\",\"end\":\"36124\"},{\"start\":\"36124\",\"end\":\"36131\"},{\"start\":\"36131\",\"end\":\"36139\"},{\"start\":\"36332\",\"end\":\"36339\"},{\"start\":\"36339\",\"end\":\"36348\"},{\"start\":\"36348\",\"end\":\"36362\"},{\"start\":\"36362\",\"end\":\"36370\"},{\"start\":\"36370\",\"end\":\"36380\"},{\"start\":\"36380\",\"end\":\"36391\"},{\"start\":\"36391\",\"end\":\"36401\"},{\"start\":\"36401\",\"end\":\"36414\"},{\"start\":\"36660\",\"end\":\"36673\"},{\"start\":\"36673\",\"end\":\"36687\"},{\"start\":\"36687\",\"end\":\"36699\"},{\"start\":\"36699\",\"end\":\"36710\"},{\"start\":\"36938\",\"end\":\"36947\"},{\"start\":\"36947\",\"end\":\"36956\"},{\"start\":\"36956\",\"end\":\"36964\"},{\"start\":\"37095\",\"end\":\"37106\"},{\"start\":\"37106\",\"end\":\"37116\"},{\"start\":\"37116\",\"end\":\"37125\"},{\"start\":\"37125\",\"end\":\"37141\"},{\"start\":\"37509\",\"end\":\"37519\"},{\"start\":\"37519\",\"end\":\"37526\"},{\"start\":\"37526\",\"end\":\"37534\"},{\"start\":\"37534\",\"end\":\"37550\"},{\"start\":\"37550\",\"end\":\"37558\"},{\"start\":\"37558\",\"end\":\"37568\"},{\"start\":\"37568\",\"end\":\"37581\"},{\"start\":\"37581\",\"end\":\"37589\"},{\"start\":\"37901\",\"end\":\"37910\"},{\"start\":\"37910\",\"end\":\"37923\"},{\"start\":\"37923\",\"end\":\"37934\"},{\"start\":\"37934\",\"end\":\"37948\"},{\"start\":\"38210\",\"end\":\"38225\"},{\"start\":\"38225\",\"end\":\"38235\"},{\"start\":\"38440\",\"end\":\"38451\"},{\"start\":\"38451\",\"end\":\"38459\"},{\"start\":\"38459\",\"end\":\"38467\"},{\"start\":\"38782\",\"end\":\"38793\"},{\"start\":\"38793\",\"end\":\"38804\"},{\"start\":\"38804\",\"end\":\"38815\"},{\"start\":\"38815\",\"end\":\"38827\"},{\"start\":\"38827\",\"end\":\"38845\"},{\"start\":\"39153\",\"end\":\"39159\"},{\"start\":\"39159\",\"end\":\"39169\"},{\"start\":\"39319\",\"end\":\"39327\"},{\"start\":\"39327\",\"end\":\"39336\"},{\"start\":\"39336\",\"end\":\"39344\"},{\"start\":\"39344\",\"end\":\"39352\"},{\"start\":\"39352\",\"end\":\"39358\"},{\"start\":\"39358\",\"end\":\"39367\"},{\"start\":\"39367\",\"end\":\"39375\"},{\"start\":\"39375\",\"end\":\"39383\"},{\"start\":\"39554\",\"end\":\"39562\"},{\"start\":\"39735\",\"end\":\"39743\"},{\"start\":\"39743\",\"end\":\"39751\"},{\"start\":\"39751\",\"end\":\"39759\"},{\"start\":\"39759\",\"end\":\"39768\"},{\"start\":\"39768\",\"end\":\"39774\"},{\"start\":\"39774\",\"end\":\"39781\"},{\"start\":\"39957\",\"end\":\"39970\"},{\"start\":\"40133\",\"end\":\"40143\"},{\"start\":\"40387\",\"end\":\"40396\"},{\"start\":\"40396\",\"end\":\"40404\"},{\"start\":\"40404\",\"end\":\"40410\"}]", "bib_venue": "[{\"start\":\"33091\",\"end\":\"33124\"},{\"start\":\"33296\",\"end\":\"33314\"},{\"start\":\"33448\",\"end\":\"33508\"},{\"start\":\"33676\",\"end\":\"33756\"},{\"start\":\"34025\",\"end\":\"34065\"},{\"start\":\"34442\",\"end\":\"34500\"},{\"start\":\"34696\",\"end\":\"34723\"},{\"start\":\"34869\",\"end\":\"34921\"},{\"start\":\"35055\",\"end\":\"35122\"},{\"start\":\"35377\",\"end\":\"35430\"},{\"start\":\"35598\",\"end\":\"35655\"},{\"start\":\"35917\",\"end\":\"35921\"},{\"start\":\"36037\",\"end\":\"36107\"},{\"start\":\"36414\",\"end\":\"36418\"},{\"start\":\"36710\",\"end\":\"36714\"},{\"start\":\"36964\",\"end\":\"36968\"},{\"start\":\"37141\",\"end\":\"37250\"},{\"start\":\"37425\",\"end\":\"37507\"},{\"start\":\"37808\",\"end\":\"37899\"},{\"start\":\"38235\",\"end\":\"38239\"},{\"start\":\"38467\",\"end\":\"38513\"},{\"start\":\"38845\",\"end\":\"38883\"},{\"start\":\"39096\",\"end\":\"39151\"},{\"start\":\"39266\",\"end\":\"39317\"},{\"start\":\"39527\",\"end\":\"39552\"},{\"start\":\"39676\",\"end\":\"39733\"},{\"start\":\"39915\",\"end\":\"39955\"},{\"start\":\"40159\",\"end\":\"40229\"},{\"start\":\"40341\",\"end\":\"40385\"}]"}}}, "year": 2023, "month": 12, "day": 17}