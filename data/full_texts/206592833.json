{"id": 206592833, "updated": "2023-09-29 10:57:07.941", "metadata": {"title": "3D ShapeNets: A deep representation for volumetric shapes", "authors": "[{\"first\":\"Zhirong\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Shuran\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Aditya\",\"last\":\"Khosla\",\"middle\":[]},{\"first\":\"Yu\",\"last\":\"Fisher\",\"middle\":[]},{\"first\":\"Linguang\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Xiaoou\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Jianxiong\",\"last\":\"Xiao\",\"middle\":[]}]", "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2015, "month": null, "day": null}, "abstract": "3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representation automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet - a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2951755740", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/WuSKYZTX15", "doi": "10.1109/cvpr.2015.7298801"}}, "content": {"source": {"pdf_hash": "b2d9f92bf374c08409fe50213f0e80f1f1fcc243", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1406.5670v3.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1406.5670", "status": "GREEN"}}, "grobid": {"id": "f98a41c9eb5363786dcba6e869cbdb807a906755", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b2d9f92bf374c08409fe50213f0e80f1f1fcc243.txt", "contents": "\n3D ShapeNets for 2.5D Object Recognition and Next-Best-View Prediction\n\n\nZhirong Wu \nPrinceton University \u2021 MIT CUHK\n\n\nShuran Song \nPrinceton University \u2021 MIT CUHK\n\n\nAditya Khosla \nPrinceton University \u2021 MIT CUHK\n\n\nXiaoou Tang \nPrinceton University \u2021 MIT CUHK\n\n\nJianxiong Xiao \nPrinceton University \u2021 MIT CUHK\n\n\n3D ShapeNets for 2.5D Object Recognition and Next-Best-View Prediction\n\n3D shape is a crucial but heavily underutilized cue in object recognition, mostly due to the lack of a good generic shape representation. With the recent boost of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is even more urgent to have a useful 3D shape model in an object recognition pipeline. Furthermore, when the recognition has low confidence, it is important to have a fail-safe mode for object recognition systems to intelligently choose the best view to obtain extra observation from another viewpoint, in order to reduce the uncertainty as much as possible. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model naturally supports object recognition from 2.5D depth map and also view planning for object recognition. We construct a large-scale 3D computer graphics dataset to train our model, and conduct extensive experiments to study this new representation. \u2020 This work was done when Zhirong Wu was a visiting student at Princeton University.\n\nIntroduction\n\nSince the establishment of computer vision as a field five decades ago, 3D geometric shape is considered to be one of the most important cues in object recognition. Even though there are many theories about 3D representation [4,18], the success of 3D-based methods is largely limited to instance recognition, using model-based keypoint matching [20,24]. For object category recognition, 3D shape is not used in any state-of-the-art recognition method (e.g. [9,15]), mostly due to the lack of a good generic representation for 3D geometric shapes. Furthermore, the recent boost of inexpensive 2.5D depth sensors, such as Microsoft Kinect, Google Project Tango, Apple PrimeSense and Intel RealSense, has led to a renewed interest in 2.5D object recognition from depth maps. Because the depth from these sensors is very reliable, 3D shape can play a more important role in recognition. It becomes more urgent to have a good 3D shape model in an object recognition pipeline.\n\nOn the other hand, object recognition is sometimes quite difficult, even for humans. It is possible that we cannot confidently recognize an object from a particular viewpoint and we have to resolve to another view to gather more observation for recognizing an object. This situation is even more common for computer vision. Automatic object recognition systems today fail frequently [27], and we desire a robust system that can recover from errors automatically. In particular, as shown in Figure 2, if a robot cannot identify an object confidently from a given view, a fail-safe mode is to allow the robot to move and observe the object from another viewpoint, in order to reduce the uncertainty for recognition. This naturally raises the question for view planning: which next view is the best for helping the robot to discriminate the object category?\n\nTo study shape representation and view planning for recognition, we propose to represent a geometric 3D shape as a probabilistic distribution of binary variables on a 3D voxel grid. This model, which we name 3D ShapeNets, uses a powerful Convolutional Deep Belief Network (Figure 1) Figure 1: 2.5D Object Recognition and Next-Best-View Prediction using 3D ShapeNets. Given a depth map of an object (e.g. from RGB-D sensors), we convert the depth map into a volumetric representation and identify the observed surface and free space. Conditioned on these observed voxels, we use our 3D ShapeNets model to identify the object category. If the recognition is ambiguous, we use our 3D ShapeNets model to predict which next view is the best view that has the greatest potential to reduce the recognition uncertainty. Then, a new view is selected and a new depth map is observed. We integrate both views to the volumetric representation and use our 3D ShapeNets to recognize the category. If the uncertainty is still high, the same process will be repeated.\n\ncomplex joint distribution of all 3D voxels from the data automatically. To train this deep model, we also construct a large scale high quality object dataset, called ModelNet, which includes 127,915 3D computer graphics CAD models. Extensive experiments show that we can use these CAD models for training, to recognize objects in single-view 2.5D depth images and hallucinate the missing parts of the shape of an object. Last but not least, our model can also predict the next-best-view in view planning for object recognition.\n\n\nRelated Works\n\nResearchers have built deep model for 2D shapes: most notably DBN [12] to generate handwritten digits and ShapeBM [8] to generate horses etc. Samples from these model are able to capture intra-class varieties. We also desire this generative ability but we are interested in modeling complex shapes in 3D. For deep learning on RGB-D images, [23] built a convolutional-recursive neural network. Although their algorithm runs on depth maps, they didn't explicitly build a model in 3D. Instead, we try to have a model to learn a shape distribution over a voxel grid. To deal with the difficulty of high resolution voxels, we apply the convolution technique in lower layers. [17] used a same technique but their deep model is primary for unsupervised feature extraction 1 , which means a separate discriminative classifier is being trained afterward on top of that. Here, we use a single convolutional DBN framework to do reconstruction and recognition. Unlike static object recognition by a single image, active object recognition allows the sensor to gain more information by moving to new view points. Therefore, the Next-Best-View problem [21] of how to do view planning based on current observation arises. Previous work of [13,7] considered only color information while we use depth information since we are more interested in object shapes. [6] approached the problem without having prior knowledge of object appearance distributions. [1,2] implemented the idea into real world robots, but they assumed there is only one object associated with each class so their problem boiled down to instance level and no intraclass variance was considered. Same with our algorithm, [7] also use mutual information to decide the NBV. However, the only extracted a single scalar (mean gray value of the picture) as feature representation, so most of their experiments are preliminary. We consider this problem to the voxel level so that we can tell how voxels in a 3D region would contribute to the reduction of recognition uncertainty. This is not possible without a powerful model directly operated on voxels. and by the average surface obtained from the zero-crossing (Red). We can see that 3D ShapeNets is able to capture complex structures in 3D space, from low-level surfaces and corners at L1, to objects parts at L2 and L3, and whole objects at L4 and above. \n\n\n3D ShapeNets: A Convolutional Deep Belief Network for 3D Shapes\n\nTo study shape representation and view planning for recognition, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid. Each 3D mesh is represented as a binary tensor: 1s indicate the voxels on or inside the mesh surface, and 0s indicate the voxels outside the mesh (i.e. empty space). The grid size in our experiments is 48 \u00d7 48 \u00d7 48.\n\nTo represent the probability distribution of these binary variables for 3D shapes, we designed a Convolutional Deep Belief Network (CDBN). Deep Belief Network (DBN) [12] is a powerful probabilistic model for binary variables that is typically used for 2D images where they modeled the joint probabilistic distribution over pixels and labels. However, adapting the model from 2D pixel data to 3D voxel data is non-trivial. A 3D voxel volume with reasonable resolution (say 48 \u00d7 48 \u00d7 48) would have the same dimension of a high-res image (332 \u00d7 332). A fully connected DBN would result in a huge number of parameters that are intractable to train effectively. Therefore, we propose to use convolution to reduce model parameters by weight sharing. But different from typical convolutional deep learning model (e.g. [17]), we don't introduce any kind of pooling in hidden layers, because although pooling may give us invariance properties for recognition, it will also give us more uncertainty during reconstruction, which is important for next-best-view prediction.\n\nThe energy of a convolutional layer in our model is defined as:\nE(v, h) = \u2212 f j h f j W f * v j + c f h f j \u2212 l b l v l(1)\nwhere v l denotes each visible unit, h f j denotes each hidden unit in a feature channel f , W f denotes the convolutional filter. The \" * \" sign represents convolution operation. In this energy definition, each visible unit v l is associated with a unique bias term b l to facilitate reconstruction, and all hidden units {h f j } in the same convolution channel share the same bias term c f . We also allow convolution stride as in [15].\n\nA 3D shape is represented as a 42 \u00d7 42 \u00d7 42 voxel grid with 3 extra cells of empty space on both direction for padding to reduce convolution border artifacts. The labels are presented as standard one of K softmax variables. All of our training data are manually aligned to the same direction. The final architecture of our model is in Figure 2(a). The first layer has 80 filters of size 6 and stride 3; the second layer has 320 filters of size 5 and stride 2 (i.e. each filter has 80\u00d75\u00d75\u00d75 parameters); the third layer has 640 filters of size 4; each convolution filter is connected to all the feature channels in the previous layer; the following two layers are standard fully connected RBM with 3000 and 1000 hidden units; the last layer takes the input as a combination of multinomial label variables and Bernoulli feature variables. The top layer forms an associate memory and all the other layer connections are directed top-down.\n\nWe train the deep model in a layer-wise pre-training fashion followed by a generative fine-tuning procedure. During pre-training, the lower four layers are trained using standard Contrastive Divergence [11], while the top layer is trained more carefully using FPCD [25]. Once the lower layer is learned, the weights are fixed and the hidden activations are fed into the next layer as input. Our fine-tuning procedure is similar to wake sleep algorithm [12] except that we keep the weight tied. In the wake phase, we propagate the data bottom up and use the activations to collect the positive learning signal; In the sleep phase, we maintain a persistent chain on the topmost layer and propagate the data top down to collect the negative learning signal. This fine-tuning procedure mimics the recognition and generation behavior of the model and works well in practice. We find that this overall generative fine-tuning is critical for shape completion performance. Some examples of the learned filters are shown in Figure 2\n(b).\nDuring pre-training of the first layer, we collect learning signal only to receptive fields which are non-empty. Because of the nature of data, empty spaces occupy a large portion of the whole volume, which has no information for RBM and would distract the learning. Our experiment shows that ignoring those learning signals during gradient computation results in more meaningful filters. During pre-training of the first two layers, we add sparsity regularization to encourage the mean activity of hidden unit over training samples to a small constant (we follow the method of [16].) During pre-training of the topmost RBM where the joint distribution of labels and high-level abstractions are learned, we duplicate the label units by a factor of 10 to increase the significancy of the labels.\n\n\nView-based 2.5D Object Recognition\n\nAfter training the CDBN, the model learns the joint distribution p(x, y) of voxel data x and object category label y \u2208 {1, \u00b7 \u00b7 \u00b7 , K}. Although the model is trained on complete 3D shapes, it is able to recognition objects in a single-view 2.5D depth map (e.g. from RGB-D sensors). As shown in Figure 3, the 2.5D depth map is firstly converted into a volumetric representation. We categorize each voxel as free space, surface or occluded, depends on whether it is in front of, on, or behind the visible surface (the depth value) from the depth map. The first two types of voxels are observed, and the occluded voxels are regarded as missing data. The testing data is represented in the form x = (x o , x u ) where x o contains the observed free space and surface voxel, and x u represent the unknown voxels. Recognizing the object category is to estimate p(y|x o ).\n\nWe approximate the posterior distribution p(y|x o ) by Gibbs sampling. The sampling procedure is as follows. We first initialize x u to random value and propagate the data x = (x u , x o ) bottom up to sample for a label y from p(y|x u , x o ). Then the high level signal is propagated down to sample for voxels x. We clamp the observation x o to this sample x and do another bottom up pass. This up-down sampling procedure runs for about 200 iterations and we can get the shape completion result x and its corresponding label y. The above sampling procedure runs in parallel for a lot of particles, and gives a variety of completion results corresponding to different classes. \n\n\nNext-Best-View Prediction for Recognition\n\nObject recognition is sometimes very challenging. However, if it is allowed to observe the object from another view point when the first recognition fails, we may be able to largely reduce the recognition uncertainty. Our model is able to predict and choose which of such next views is the best for discriminating the object category.\n\nThe inputs of our next-best-view system are observed voxels x o of an unknown object captured by a depth camera from a single view, and a finite list of next-view candidates {V i } represents camera rotation and translation in 3D. An algorithm chooses the next-view from the list that has the highest potential to reduce the recognition uncertainty most. Note that during this view planning process, we do not gain any improvement on confidence of p(y|x o = x o ) since we observe no new data.\n\nThe original recognition uncertainty is measured by the entropy of y conditioned on the observed\nx o , H = H (p(y|x o = x o )) = \u2212 K k=1 p(y = k|x o = x o )log p(y = k|x o = x o )(2)\nwhere the conditional probability p(y|x o = x o ) can be approximated as before by sampling from p(y, x u |x o = x o ) and marginalizing x u .\n\nWhen the camera is moved to another view V i , some of the previously unobserved voxels x u may become observed, subjective to its actual shape. Different views V i will cost different visibility of these unobserved voxels x u . A view with the potential to see distinctive parts of objects (e.g. arms of chairs) may be a better next view. But since the actual shape is partially unknown 2 , we will hallucinate that region from our model. As shown in Figure 4, conditioning on x o = x o , we can sample many shapes to generate hypotheses of the actual shape, and then render each hypothesis to obtain the depth maps observed from different view V i . In this way, we can simulate the new depth maps for different views on different samples and compute the benefit on reducing recognition uncertainty.\n\nMathematically, let x i n = Render(x u , x o , V i ) \\ x o to denote the new observed voxels (both free space and surface) in the next view V i . We have x i n \u2286 x u , and they are unknown variables that will be marginalized in the following equation. Then the potential recognition uncertainty for V i is  \nH i = H p(y|x i n , x o = x o ) = x i n p(x i n |x o = x o )H(y|x i n , x o = x o ).(3)\nThe above conditional entropy could be calculated by first sampling enough x u from p(x u |x o = x o ), do the 3D rendering to obtain 2.5D depth map in order to get x i n from x u , and then take each x i n to calculate H(y|x i n = x i n , x o = x o ) as before. According to information theory, the reduction of entropy H \u2212 H i = I(y; x i n |x o = x o ) \u2265 0 is the mutual information between y and x i n conditioned on x o . This meets our intuition that observing more data will always potentially reduce the uncertainty. With this definition, our view planning algorithm is simply to choose the view that can maximize this mutual information,\nV * = arg max V i I(y; x i n |x o = x o ).(4)\nOur view planning scheme can be naturally extended for a sequence of view planning steps. After deciding the best candidate to move for the first frame, we physically move the camera there and capture the other object surface from that view. The object surfaces from all previous history are merged together as our new observation x o , and then run our view planning scheme again.\n\n\nPrinceton ModelNet: A Large-scale 3D CAD Object Dataset\n\nTraining a 3D shape model that captures intra-class variance requires a large collection of 3D shapes. Previous CAD datasets (e.g. [22]) are limited both by category varieties and data size per category. Therefore, we construct a new large scale 3D CAD model data set named ModelNet.\n\nTo construct ModelNet, we download 3D models from Google 3D Warehouse by querying object category names. We query categories that are common object categories in SUN database [26] with no less than 20 object instances per category, and remove ones with too few searching results, resulting in 585 remaining categories. We also include models from Princeton Shape Benchmark [22]. After downloading, we remove models not belonging to their labelled categories. This step is done in Amazon Mechanical Turk, in which turkers are shown a sequence of thumbnails of the models and answer \"Yes\" or \"No\" to whether their category label is correct. The authors then manually check each 3D model and remove irrelevant objects in each CAD model (e.g, floor, thumbnail image, person standing next to the object), so that each mesh model contains only one object belongs to the labelled category. We also discard unrealistic (overly simplified models or ones that only contain images of the object) and duplicated meshes. Comparing with [22], which consists of 6670  Table 1: Accuracy for View-based 2.5D Recognition on NYU dataset [19]. The first three rows are algorithms that use only depth information. The last two rows are algorithms that use color information. Our 3D ShapeNets performs the best among all depth-based algorithms, and very close to [23] that used both RGB color and depth.\n\nGT: sofa Ours: sofa [23]: sofa GT:   [19]. In each example, we show the color, the depth map, and the completed shape by 3D ShapeNets. models in 161 categories, our new dataset which is 19 times larger contains 127,915 3D meshes in 585 categories. Example of major object categories and dataset statistics are shown in Figure 5.\n\n\nExperiments\n\nTo have the same categories with NYU Depth V2 dataset [19], we choose 10 common indoor object categories (see Figure 6) from ModelNet with 4899 CAD models. 3899 models are used for training and 1000 models (100 per category) are used for testing in Section 6.1. Pre-training and fine-tuning each took about two days on a Intel XEON E5-2690 CPU and a NVIDIA K40c GPU. Figure 6 shows some shapes sampled from our trained model.\n\n\n3D Shape Classification and Retrieval\n\nDeep learning has been widely used as a feature extraction technique. Here, we are also interested in how well the feature learned from 3D ShapeNets compared with other state-of-the-art 3D mesh features. After training the 3D ShapeNets, we use the top three layer activations as our feature separately for evaluation. When propagating the data upward, we set the labels to zeros so the features do not explicitly contain label information.\n\nFor comparison, we choose Light Field descriptor [5] (4700 dimensions) and Spherical Harmonic descriptor [14] (28672 dimensions), which performed best among all descriptors [22]. 3D classification and retrieval experiments are conducted to evaluate our features. For classification, we use linear SVM to train classifiers for each feature to calculate the classification accuracy on the testing set. Our 3D ShapeNets feature achieves an accuracy of 86.5%, 83.7% and 82.0% for the 5th, 6th and 7th layer, while Light Field [5] achieves 86.1% and Spherical Harmonic [14] achieves 82.0%. For retrieval, similarity is measured by the L2 distance of shape descriptors between any testing pairs. Given a query from the testing set, a ranked list of remaining testing data is returned according to the similarity measure. The retrieval performance is evaluated by a precision recall curve in Figure 6. Both experiments show that our 3D ShapeNets is able to learn features comparable to state-of-the-art hand-crafted features.\n\n\nView-based 2.5D Recognition\n\nTo evaluate 3D ShapeNets for 2.5D depth-based object recognition tasks, we set up an experiment on NYU RGBD dataset with Kinect depth maps [19]. We choose the same 10 categories from NYU and create each testing example by cropping the 3D point cloud from the 3D bounding boxes. Since the model is trained with aligned data while the pose of testing set are arbitrary, we need to firstly estimate the object pose. To do this, we run our recognition algorithm on every possible pose, and choose the one whose completion samples have the highest free energy 3 as the correct pose [10].\n\nAs a baseline method, we match the testing point cloud to each our 3D mesh models using Iterated Closest Point method [3] and use the first top 10 matches to predict the labels. We also compare our  result with [23] which is the state-of-the-art deep learning model applied on RGB-D data. To train and test their model, the 2D bounding box is obtained by projecting the 3D bounding box to the image plane, and the object segmentation is also used to extract features. 1390 instances are used to train the algorithm of [23], and the other 495 instances are used for testing all three methods. As shown in Table 1, by only using the depth information, our algorithm can predict a 3D pose and achieve a similar recognition accuracy as [23] which use both RGB color and depth. Figure 7 shows the visualization for success and failure cases.\n\n\nNext-Best-View Prediction\n\nFor our view planning strategy, computation of the term p(x i n |x o = x o ) is critical. When the observation x o is ambiguous, samples drawn from p(x i n |x o = x o ) should have varieties across different categories. When the observation is rich, samples should be limited to very few categories. Since x i n is the surface of the completions, we could just test the shape completion performance p(x u |x o = x o ). In Figure 8, our results give reasonable shapes across different categories. We also match the nearest neighbor in the training set to show that our algorithm is not just memorizing the shape and it can generalize well.\n\nTo evaluate our view planning strategy, we use CG models from the test set to create synthetic rendering of depth maps. We evaluate the accuracy by running our 3D ShapeNets model on the integration depth maps of both the first view and the selected second view. A good view-planning strategy will result in a better recognition accuracy. Note that next-best-view selection is always coupled with the recognition algorithm. We prepare three baseline methods for comparison : 1) random selection among the candidate views. 2) choose the view with the highest new visibility (yellow voxels, NBV for reconstruction). 3) choose the view which is furthest away with previous view (based on camera center distance). In our experiment, we generate 8 view candidates randomly distributed on the sphere of the object, pointing to the region near the object center. And we randomly choose 100 testing examples (10 for each category) from our testing set. Table 2 reports the recognition accuracy of different view planning strategies with the same recognition 3D ShapeNets. We can see that our entropy based strategy is the best for selecting new views.\n\n\nConclusion\n\nTo study 3D shape representation for objects, we propose a convolutional deep belief network to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid. We construct a large-scale 3D CG dataset to train our model, and use it to recognize objects in a singleview 2.5D depth map (e.g. from popular RGB-D sensors). Besides it outperforms state-of-the-art algorithms in our experiments, it also supports next-best-view planning for object recognition. Future work includes constructing a large-scale Kinect-based 2.5D dataset so that we can train 3D ShapeNets with all categories from ModelNet and properly evaluate it using this 2.5D dataset.\n\n\nData-driven visualization: For each neuron, we average the top 100 training examples with highest responses (>0.99) and crop the volume inside the receptive field. The averaged result is visualized by transparency in 3D (Gray)\n\nFigure 2 :\n23D ShapeNets. Architecture and sample visualization from different layers.\n\nFigure 3 :\n3View-based 2.5D Object Recognition. (1) illustrates that a depth map taken from a physical object in the 3D world. (2) shows the depth image captured from the back of the chair. A slice is used for visualization. (3) shows the profile of the slice and different types of voxels. The surface voxels of the chair x o are in red, and the occluded voxels x u are in blue. (4) shows the recognition and shape completion result, conditioned on the observed free space and surface.\n\nFigure 4 :\n4shapes predicted new freespace & visible surface for each shape under each view free space Next-Best-View Prediction. [Row1 Col1]: the observed and unknown voxels from a single view. [Row2-4 Col1]: three possible completion samples generated by condition on (x o , x u ) [Row1 Col2-6]: five possible camera positions V i , front top, left-sided, tilted bottom, front, top. [Row2-4 Col2-6]: predict what will happen from each view and each sample using rendering.\n\nFigure 5 :\n5ModelNet dataset. Left: visualization of the ModelNet dataset based on the number of images in each category. Larger font size indicates more instances in the corresponding category. Right: examples of 3D models from different categories.\n\nFigure 6 :\n6Results. Left: Example shapes generated by sampling our 3D ShapeNets for each category. Right: Precision-recall curves and average precision [in brackets] for 3D mesh retrieval. measured by this conditional entropy,\n\nFigure 7 :\n7Success and failure examples of 2.5D Kinect-based object recognition on NYU dataset\n\nFigure 8 :\n8Shape Completion. From left to right: input depth map from a single view, ground truth shape, shape completion result (4 examples), nearest neighbor result (1 example). bathtub bed chair desk dresser monitor nightstand sofa table toilet\n\n\nto learn thesofa? \n\nbathtub? \nWhere to look next? \n\nDepth map from \nthe back of a sofa \n\nVolumetric \nrepresentation \n\nNext-Best-View \n\nNew depth map \n\nAha! \nIt is a sofa! \n\nNot sure. \nLook from \nanother view? \nWhat is it? \n\n?! \n\ndresser? \n\n3D ShapeNets \n\n\n\nTable 2 :\n2Comparison of Different Next-Best-View Selections Based on Recognition Accuracy from Two Views. Based on the algorithms' choice, we obtain the actual depth map for the next view and recognize the objects using two views by our 3D ShapeNets to compute the accuracies.\nThe model is precisely a convolutional DBM where all the connections are undirected.\nIf the 3D shape is fully observed, adding more views will not help to reduce the recognition uncertainty in any algorithm purely based on 3D shapes, including our 3D ShapeNets.\nFree energy of RBM: F (x) = \u2212log h e \u2212E(x,h) , the negative logarithm of unnormalized probability.\n\nHypothesis testing framework for active object detection. N Atanasov, B Sankaran, J Le Ny, T Koletschka, G J Pappas, K Daniilidis, Robotics and Automation (ICRA), 2013 IEEE International Conference on. IEEEN. Atanasov, B. Sankaran, J. Le Ny, T. Koletschka, G. J. Pappas, and K. Daniilidis. Hypothesis testing framework for active object detection. In Robotics and Automation (ICRA), 2013 IEEE International Conference on, pages 4216-4222. IEEE, 2013.\n\nNonmyopic view planning for active object detection. N Atanasov, B Sankaran, J L Ny, G J Pappas, K Daniilidis, arXiv:1309.5401arXiv preprintN. Atanasov, B. Sankaran, J. L. Ny, G. J. Pappas, and K. Daniilidis. Nonmyopic view planning for active object detection. arXiv preprint arXiv:1309.5401, 2013.\n\nMethod for registration of 3-d shapes. P J Besl, N D Mckay, PAMI. P. J. Besl and N. D. McKay. Method for registration of 3-d shapes. In PAMI, 1992.\n\nRecognition-by-components: a theory of human image understanding. Psychological review. I Biederman, I. Biederman. Recognition-by-components: a theory of human image understanding. Psycho- logical review, 1987.\n\nOn visual similarity based 3d model retrieval. D.-Y Chen, X.-P Tian, Y.-T Shen, M Ouhyoung, Computer graphics forum. D.-Y. Chen, X.-P. Tian, Y.-T. Shen, and M. Ouhyoung. On visual similarity based 3d model retrieval. In Computer graphics forum, 2003.\n\nViewpoint selection-planning optimal sequences of views for object recognition. F Deinzer, J Denzler, H Niemann, Computer analysis of images and patterns. SpringerF. Deinzer, J. Denzler, and H. Niemann. Viewpoint selection-planning optimal sequences of views for object recognition. In Computer analysis of images and patterns, pages 65-73. Springer, 2003.\n\nInformation theoretic sensor data selection for active object recognition and state estimation. Pattern Analysis and Machine Intelligence. J Denzler, C M Brown, IEEE Transactions on. 242J. Denzler and C. M. Brown. Information theoretic sensor data selection for active object recog- nition and state estimation. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 24(2):145-157, 2002.\n\nThe shape boltzmann machine: a strong model of object shape. S M A Eslami, N Heess, J Winn, CVPR. S. M. A. Eslami, N. Heess, and J. Winn. The shape boltzmann machine: a strong model of object shape. In CVPR, 2012.\n\nObject detection with discriminatively trained part based models. P F Felzenszwalb, R B Girshick, D Mcallester, D Ramanan, PAMIP. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part based models. PAMI, 2010.\n\nA practical guide to training restricted boltzmann machines. Momentum. G Hinton, G. Hinton. A practical guide to training restricted boltzmann machines. Momentum, 2010.\n\nTraining products of experts by minimizing contrastive divergence. G E Hinton, Neural computation. G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural computation, 2002.\n\nA fast learning algorithm for deep belief nets. G E Hinton, S Osindero, Y.-W Teh, Neural computation. G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural computation, 2006.\n\nActive view selection for object and pose recognition. Z Jia, Y.-J Chang, T Chen, Computer Vision Workshops (ICCV Workshops). IEEEIEEE 12th International Conference onZ. Jia, Y.-J. Chang, and T. Chen. Active view selection for object and pose recognition. In Computer Vision Workshops (ICCV Workshops), 2009 IEEE 12th International Conference on, pages 641-648. IEEE, 2009.\n\nRotation invariant spherical harmonic representation of 3d shape descriptors. M Kazhdan, T Funkhouser, S Rusinkiewicz, Symposium on Geometry Processing. M. Kazhdan, T. Funkhouser, and S. Rusinkiewicz. Rotation invariant spherical harmonic rep- resentation of 3d shape descriptors. In Symposium on Geometry Processing, 2003.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G Hinton, NIPS. A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.\n\nSparse deep belief net model for visual area v2. H Lee, C Ekanadham, A Y Ng, NIPS. H. Lee, C. Ekanadham, and A. Y. Ng. Sparse deep belief net model for visual area v2. In NIPS, 2007.\n\nUnsupervised learning of hierarchical representations with convolutional deep belief networks. H Lee, R Grosse, R Ranganath, A Y Ng, Communications of the ACM. H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Unsupervised learning of hierarchical repre- sentations with convolutional deep belief networks. Communications of the ACM, 2011.\n\nObject recognition in the geometric era: A retrospective. In Toward category-level object recognition. J L Mundy, J. L. Mundy. Object recognition in the geometric era: A retrospective. In Toward category-level object recognition. 2006.\n\nIndoor segmentation and support inference from rgbd images. P K Nathan Silberman, Derek Hoiem, R Fergus, ECCV. P. K. Nathan Silberman, Derek Hoiem and R. Fergus. Indoor segmentation and support infer- ence from rgbd images. In ECCV, 2012.\n\n3d object modeling and recognition using local affine-invariant image descriptors and multi-view spatial constraints. F Rothganger, S Lazebnik, C Schmid, J Ponce, IJCVF. Rothganger, S. Lazebnik, C. Schmid, and J. Ponce. 3d object modeling and recognition using local affine-invariant image descriptors and multi-view spatial constraints. IJCV, 2006.\n\nView planning for automated 3d object reconstruction inspection. W Scott, G Roth, J.-F Rivest, ACM Computing SurveysW. Scott, G. Roth, and J.-F. Rivest. View planning for automated 3d object reconstruction inspection. ACM Computing Surveys, 2003.\n\nThe princeton shape benchmark. P Shilane, P Min, M Kazhdan, T Funkhouser, Shape Modeling Applications. P. Shilane, P. Min, M. Kazhdan, and T. Funkhouser. The princeton shape benchmark. In Shape Modeling Applications, 2004.\n\nConvolutional-recursive deep learning for 3d object classification. R Socher, B Huval, B Bhat, C D Manning, A Y Ng, NIPS. R. Socher, B. Huval, B. Bhat, C. D. Manning, and A. Y. Ng. Convolutional-recursive deep learning for 3d object classification. In NIPS. 2012.\n\nA textured object recognition pipeline for color and depth image data. J Tang, S Miller, A Singh, P Abbeel, ICRA. J. Tang, S. Miller, A. Singh, and P. Abbeel. A textured object recognition pipeline for color and depth image data. In ICRA, 2012.\n\nUsing fast weights to improve persistent contrastive divergence. T Tieleman, G Hinton, ICML. T. Tieleman and G. Hinton. Using fast weights to improve persistent contrastive divergence. In ICML, 2009.\n\nSUN database: Large-scale scene recognition from abbey to zoo. J Xiao, J Hays, K A Ehinger, A Oliva, A Torralba, CVPR. J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010.\n\nPredicting failures of vision systems. P Zhang, J Wang, A Farhadi, M Hebert, D Parikh, CVPR. P. Zhang, J. Wang, A. Farhadi, M. Hebert, and D. Parikh. Predicting failures of vision systems. In CVPR, 2014.\n", "annotations": {"author": "[{\"end\":119,\"start\":74},{\"end\":166,\"start\":120},{\"end\":215,\"start\":167},{\"end\":262,\"start\":216},{\"end\":312,\"start\":263},{\"end\":119,\"start\":74},{\"end\":166,\"start\":120},{\"end\":215,\"start\":167},{\"end\":262,\"start\":216},{\"end\":312,\"start\":263}]", "publisher": null, "author_last_name": "[{\"end\":84,\"start\":82},{\"end\":131,\"start\":127},{\"end\":180,\"start\":174},{\"end\":227,\"start\":223},{\"end\":277,\"start\":273},{\"end\":84,\"start\":82},{\"end\":131,\"start\":127},{\"end\":180,\"start\":174},{\"end\":227,\"start\":223},{\"end\":277,\"start\":273}]", "author_first_name": "[{\"end\":81,\"start\":74},{\"end\":126,\"start\":120},{\"end\":173,\"start\":167},{\"end\":222,\"start\":216},{\"end\":272,\"start\":263},{\"end\":81,\"start\":74},{\"end\":126,\"start\":120},{\"end\":173,\"start\":167},{\"end\":222,\"start\":216},{\"end\":272,\"start\":263}]", "author_affiliation": "[{\"end\":118,\"start\":86},{\"end\":165,\"start\":133},{\"end\":214,\"start\":182},{\"end\":261,\"start\":229},{\"end\":311,\"start\":279},{\"end\":118,\"start\":86},{\"end\":165,\"start\":133},{\"end\":214,\"start\":182},{\"end\":261,\"start\":229},{\"end\":311,\"start\":279}]", "title": "[{\"end\":71,\"start\":1},{\"end\":383,\"start\":313},{\"end\":71,\"start\":1},{\"end\":383,\"start\":313}]", "venue": null, "abstract": "[{\"end\":1480,\"start\":385},{\"end\":1480,\"start\":385}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1724,\"start\":1721},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1727,\"start\":1724},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":1845,\"start\":1841},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1848,\"start\":1845},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1956,\"start\":1953},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1959,\"start\":1956},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2855,\"start\":2851},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4993,\"start\":4989},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5040,\"start\":5037},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5267,\"start\":5263},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5597,\"start\":5593},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6065,\"start\":6061},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6151,\"start\":6147},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6153,\"start\":6151},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6269,\"start\":6266},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6363,\"start\":6360},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6365,\"start\":6363},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6598,\"start\":6595},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7912,\"start\":7908},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8559,\"start\":8555},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9367,\"start\":9363},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10513,\"start\":10509},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10576,\"start\":10572},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10763,\"start\":10759},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11918,\"start\":11914},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17384,\"start\":17380},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17713,\"start\":17709},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17911,\"start\":17907},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18561,\"start\":18557},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18656,\"start\":18652},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18879,\"start\":18875},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18941,\"start\":18937},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18958,\"start\":18954},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19319,\"start\":19315},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20221,\"start\":20218},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20278,\"start\":20274},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20346,\"start\":20342},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20694,\"start\":20691},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20737,\"start\":20733},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21362,\"start\":21358},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21800,\"start\":21796},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21924,\"start\":21921},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22018,\"start\":22014},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22325,\"start\":22321},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22539,\"start\":22535},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1724,\"start\":1721},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1727,\"start\":1724},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":1845,\"start\":1841},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1848,\"start\":1845},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1956,\"start\":1953},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1959,\"start\":1956},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2855,\"start\":2851},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4993,\"start\":4989},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5040,\"start\":5037},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5267,\"start\":5263},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5597,\"start\":5593},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6065,\"start\":6061},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6151,\"start\":6147},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6153,\"start\":6151},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6269,\"start\":6266},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6363,\"start\":6360},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6365,\"start\":6363},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6598,\"start\":6595},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7912,\"start\":7908},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8559,\"start\":8555},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9367,\"start\":9363},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10513,\"start\":10509},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10576,\"start\":10572},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10763,\"start\":10759},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11918,\"start\":11914},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17384,\"start\":17380},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17713,\"start\":17709},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17911,\"start\":17907},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18561,\"start\":18557},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18656,\"start\":18652},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18879,\"start\":18875},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18941,\"start\":18937},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18958,\"start\":18954},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19319,\"start\":19315},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20221,\"start\":20218},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20278,\"start\":20274},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20346,\"start\":20342},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20694,\"start\":20691},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20737,\"start\":20733},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21362,\"start\":21358},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21800,\"start\":21796},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21924,\"start\":21921},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22018,\"start\":22014},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22325,\"start\":22321},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22539,\"start\":22535}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25378,\"start\":25150},{\"attributes\":{\"id\":\"fig_1\"},\"end\":25466,\"start\":25379},{\"attributes\":{\"id\":\"fig_2\"},\"end\":25954,\"start\":25467},{\"attributes\":{\"id\":\"fig_3\"},\"end\":26430,\"start\":25955},{\"attributes\":{\"id\":\"fig_4\"},\"end\":26682,\"start\":26431},{\"attributes\":{\"id\":\"fig_5\"},\"end\":26911,\"start\":26683},{\"attributes\":{\"id\":\"fig_6\"},\"end\":27008,\"start\":26912},{\"attributes\":{\"id\":\"fig_7\"},\"end\":27258,\"start\":27009},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":27516,\"start\":27259},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":27795,\"start\":27517},{\"attributes\":{\"id\":\"fig_0\"},\"end\":25378,\"start\":25150},{\"attributes\":{\"id\":\"fig_1\"},\"end\":25466,\"start\":25379},{\"attributes\":{\"id\":\"fig_2\"},\"end\":25954,\"start\":25467},{\"attributes\":{\"id\":\"fig_3\"},\"end\":26430,\"start\":25955},{\"attributes\":{\"id\":\"fig_4\"},\"end\":26682,\"start\":26431},{\"attributes\":{\"id\":\"fig_5\"},\"end\":26911,\"start\":26683},{\"attributes\":{\"id\":\"fig_6\"},\"end\":27008,\"start\":26912},{\"attributes\":{\"id\":\"fig_7\"},\"end\":27258,\"start\":27009},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":27516,\"start\":27259},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":27795,\"start\":27517}]", "paragraph": "[{\"end\":2466,\"start\":1496},{\"end\":3322,\"start\":2468},{\"end\":4375,\"start\":3324},{\"end\":4905,\"start\":4377},{\"end\":7278,\"start\":4923},{\"end\":7741,\"start\":7346},{\"end\":8805,\"start\":7743},{\"end\":8870,\"start\":8807},{\"end\":9368,\"start\":8930},{\"end\":10305,\"start\":9370},{\"end\":11330,\"start\":10307},{\"end\":12130,\"start\":11336},{\"end\":13033,\"start\":12169},{\"end\":13713,\"start\":13035},{\"end\":14093,\"start\":13759},{\"end\":14588,\"start\":14095},{\"end\":14686,\"start\":14590},{\"end\":14915,\"start\":14773},{\"end\":15718,\"start\":14917},{\"end\":16027,\"start\":15720},{\"end\":16761,\"start\":16116},{\"end\":17189,\"start\":16808},{\"end\":17532,\"start\":17249},{\"end\":18915,\"start\":17534},{\"end\":19245,\"start\":18917},{\"end\":19686,\"start\":19261},{\"end\":20167,\"start\":19728},{\"end\":21187,\"start\":20169},{\"end\":21801,\"start\":21219},{\"end\":22639,\"start\":21803},{\"end\":23307,\"start\":22669},{\"end\":24451,\"start\":23309},{\"end\":25149,\"start\":24466},{\"end\":2466,\"start\":1496},{\"end\":3322,\"start\":2468},{\"end\":4375,\"start\":3324},{\"end\":4905,\"start\":4377},{\"end\":7278,\"start\":4923},{\"end\":7741,\"start\":7346},{\"end\":8805,\"start\":7743},{\"end\":8870,\"start\":8807},{\"end\":9368,\"start\":8930},{\"end\":10305,\"start\":9370},{\"end\":11330,\"start\":10307},{\"end\":12130,\"start\":11336},{\"end\":13033,\"start\":12169},{\"end\":13713,\"start\":13035},{\"end\":14093,\"start\":13759},{\"end\":14588,\"start\":14095},{\"end\":14686,\"start\":14590},{\"end\":14915,\"start\":14773},{\"end\":15718,\"start\":14917},{\"end\":16027,\"start\":15720},{\"end\":16761,\"start\":16116},{\"end\":17189,\"start\":16808},{\"end\":17532,\"start\":17249},{\"end\":18915,\"start\":17534},{\"end\":19245,\"start\":18917},{\"end\":19686,\"start\":19261},{\"end\":20167,\"start\":19728},{\"end\":21187,\"start\":20169},{\"end\":21801,\"start\":21219},{\"end\":22639,\"start\":21803},{\"end\":23307,\"start\":22669},{\"end\":24451,\"start\":23309},{\"end\":25149,\"start\":24466}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8929,\"start\":8871},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11335,\"start\":11331},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14772,\"start\":14687},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16115,\"start\":16028},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16807,\"start\":16762},{\"attributes\":{\"id\":\"formula_0\"},\"end\":8929,\"start\":8871},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11335,\"start\":11331},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14772,\"start\":14687},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16115,\"start\":16028},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16807,\"start\":16762}]", "table_ref": "[{\"end\":18594,\"start\":18587},{\"end\":22414,\"start\":22407},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24260,\"start\":24253},{\"end\":18594,\"start\":18587},{\"end\":22414,\"start\":22407},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24260,\"start\":24253}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1494,\"start\":1482},{\"attributes\":{\"n\":\"1.1\"},\"end\":4921,\"start\":4908},{\"attributes\":{\"n\":\"2\"},\"end\":7344,\"start\":7281},{\"attributes\":{\"n\":\"3\"},\"end\":12167,\"start\":12133},{\"attributes\":{\"n\":\"4\"},\"end\":13757,\"start\":13716},{\"attributes\":{\"n\":\"5\"},\"end\":17247,\"start\":17192},{\"attributes\":{\"n\":\"6\"},\"end\":19259,\"start\":19248},{\"attributes\":{\"n\":\"6.1\"},\"end\":19726,\"start\":19689},{\"attributes\":{\"n\":\"6.2\"},\"end\":21217,\"start\":21190},{\"attributes\":{\"n\":\"6.3\"},\"end\":22667,\"start\":22642},{\"attributes\":{\"n\":\"7\"},\"end\":24464,\"start\":24454},{\"end\":25390,\"start\":25380},{\"end\":25478,\"start\":25468},{\"end\":25966,\"start\":25956},{\"end\":26442,\"start\":26432},{\"end\":26694,\"start\":26684},{\"end\":26923,\"start\":26913},{\"end\":27020,\"start\":27010},{\"end\":27527,\"start\":27518},{\"attributes\":{\"n\":\"1\"},\"end\":1494,\"start\":1482},{\"attributes\":{\"n\":\"1.1\"},\"end\":4921,\"start\":4908},{\"attributes\":{\"n\":\"2\"},\"end\":7344,\"start\":7281},{\"attributes\":{\"n\":\"3\"},\"end\":12167,\"start\":12133},{\"attributes\":{\"n\":\"4\"},\"end\":13757,\"start\":13716},{\"attributes\":{\"n\":\"5\"},\"end\":17247,\"start\":17192},{\"attributes\":{\"n\":\"6\"},\"end\":19259,\"start\":19248},{\"attributes\":{\"n\":\"6.1\"},\"end\":19726,\"start\":19689},{\"attributes\":{\"n\":\"6.2\"},\"end\":21217,\"start\":21190},{\"attributes\":{\"n\":\"6.3\"},\"end\":22667,\"start\":22642},{\"attributes\":{\"n\":\"7\"},\"end\":24464,\"start\":24454},{\"end\":25390,\"start\":25380},{\"end\":25478,\"start\":25468},{\"end\":25966,\"start\":25956},{\"end\":26442,\"start\":26432},{\"end\":26694,\"start\":26684},{\"end\":26923,\"start\":26913},{\"end\":27020,\"start\":27010},{\"end\":27527,\"start\":27518}]", "table": "[{\"end\":27516,\"start\":27273},{\"end\":27516,\"start\":27273}]", "figure_caption": "[{\"end\":25378,\"start\":25152},{\"end\":25466,\"start\":25392},{\"end\":25954,\"start\":25480},{\"end\":26430,\"start\":25968},{\"end\":26682,\"start\":26444},{\"end\":26911,\"start\":26696},{\"end\":27008,\"start\":26925},{\"end\":27258,\"start\":27022},{\"end\":27273,\"start\":27261},{\"end\":27795,\"start\":27529},{\"end\":25378,\"start\":25152},{\"end\":25466,\"start\":25392},{\"end\":25954,\"start\":25480},{\"end\":26430,\"start\":25968},{\"end\":26682,\"start\":26444},{\"end\":26911,\"start\":26696},{\"end\":27008,\"start\":26925},{\"end\":27258,\"start\":27022},{\"end\":27273,\"start\":27261},{\"end\":27795,\"start\":27529}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":2966,\"start\":2958},{\"end\":3606,\"start\":3596},{\"end\":3615,\"start\":3607},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9713,\"start\":9705},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11330,\"start\":11322},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12470,\"start\":12462},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15377,\"start\":15369},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19244,\"start\":19236},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":19379,\"start\":19371},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":19636,\"start\":19628},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":21062,\"start\":21054},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":22584,\"start\":22576},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":23099,\"start\":23091},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":2966,\"start\":2958},{\"end\":3606,\"start\":3596},{\"end\":3615,\"start\":3607},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9713,\"start\":9705},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11330,\"start\":11322},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12470,\"start\":12462},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15377,\"start\":15369},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19244,\"start\":19236},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":19379,\"start\":19371},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":19636,\"start\":19628},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":21062,\"start\":21054},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":22584,\"start\":22576},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":23099,\"start\":23091}]", "bib_author_first_name": "[{\"end\":28217,\"start\":28216},{\"end\":28229,\"start\":28228},{\"end\":28241,\"start\":28240},{\"end\":28244,\"start\":28242},{\"end\":28250,\"start\":28249},{\"end\":28264,\"start\":28263},{\"end\":28266,\"start\":28265},{\"end\":28276,\"start\":28275},{\"end\":28664,\"start\":28663},{\"end\":28676,\"start\":28675},{\"end\":28688,\"start\":28687},{\"end\":28690,\"start\":28689},{\"end\":28696,\"start\":28695},{\"end\":28698,\"start\":28697},{\"end\":28708,\"start\":28707},{\"end\":28951,\"start\":28950},{\"end\":28953,\"start\":28952},{\"end\":28961,\"start\":28960},{\"end\":28963,\"start\":28962},{\"end\":29149,\"start\":29148},{\"end\":29323,\"start\":29319},{\"end\":29334,\"start\":29330},{\"end\":29345,\"start\":29341},{\"end\":29353,\"start\":29352},{\"end\":29605,\"start\":29604},{\"end\":29616,\"start\":29615},{\"end\":29627,\"start\":29626},{\"end\":30022,\"start\":30021},{\"end\":30033,\"start\":30032},{\"end\":30035,\"start\":30034},{\"end\":30343,\"start\":30342},{\"end\":30347,\"start\":30344},{\"end\":30357,\"start\":30356},{\"end\":30366,\"start\":30365},{\"end\":30563,\"start\":30562},{\"end\":30565,\"start\":30564},{\"end\":30581,\"start\":30580},{\"end\":30583,\"start\":30582},{\"end\":30595,\"start\":30594},{\"end\":30609,\"start\":30608},{\"end\":30841,\"start\":30840},{\"end\":31007,\"start\":31006},{\"end\":31009,\"start\":31008},{\"end\":31195,\"start\":31194},{\"end\":31197,\"start\":31196},{\"end\":31207,\"start\":31206},{\"end\":31222,\"start\":31218},{\"end\":31421,\"start\":31420},{\"end\":31431,\"start\":31427},{\"end\":31440,\"start\":31439},{\"end\":31819,\"start\":31818},{\"end\":31830,\"start\":31829},{\"end\":31844,\"start\":31843},{\"end\":32131,\"start\":32130},{\"end\":32145,\"start\":32144},{\"end\":32158,\"start\":32157},{\"end\":32348,\"start\":32347},{\"end\":32355,\"start\":32354},{\"end\":32368,\"start\":32367},{\"end\":32370,\"start\":32369},{\"end\":32578,\"start\":32577},{\"end\":32585,\"start\":32584},{\"end\":32595,\"start\":32594},{\"end\":32608,\"start\":32607},{\"end\":32610,\"start\":32609},{\"end\":32924,\"start\":32923},{\"end\":32926,\"start\":32925},{\"end\":33118,\"start\":33117},{\"end\":33120,\"start\":33119},{\"end\":33144,\"start\":33139},{\"end\":33153,\"start\":33152},{\"end\":33416,\"start\":33415},{\"end\":33430,\"start\":33429},{\"end\":33442,\"start\":33441},{\"end\":33452,\"start\":33451},{\"end\":33714,\"start\":33713},{\"end\":33723,\"start\":33722},{\"end\":33734,\"start\":33730},{\"end\":33928,\"start\":33927},{\"end\":33939,\"start\":33938},{\"end\":33946,\"start\":33945},{\"end\":33957,\"start\":33956},{\"end\":34189,\"start\":34188},{\"end\":34199,\"start\":34198},{\"end\":34208,\"start\":34207},{\"end\":34216,\"start\":34215},{\"end\":34218,\"start\":34217},{\"end\":34229,\"start\":34228},{\"end\":34231,\"start\":34230},{\"end\":34457,\"start\":34456},{\"end\":34465,\"start\":34464},{\"end\":34475,\"start\":34474},{\"end\":34484,\"start\":34483},{\"end\":34697,\"start\":34696},{\"end\":34709,\"start\":34708},{\"end\":34896,\"start\":34895},{\"end\":34904,\"start\":34903},{\"end\":34912,\"start\":34911},{\"end\":34914,\"start\":34913},{\"end\":34925,\"start\":34924},{\"end\":34934,\"start\":34933},{\"end\":35130,\"start\":35129},{\"end\":35139,\"start\":35138},{\"end\":35147,\"start\":35146},{\"end\":35158,\"start\":35157},{\"end\":35168,\"start\":35167},{\"end\":28217,\"start\":28216},{\"end\":28229,\"start\":28228},{\"end\":28241,\"start\":28240},{\"end\":28244,\"start\":28242},{\"end\":28250,\"start\":28249},{\"end\":28264,\"start\":28263},{\"end\":28266,\"start\":28265},{\"end\":28276,\"start\":28275},{\"end\":28664,\"start\":28663},{\"end\":28676,\"start\":28675},{\"end\":28688,\"start\":28687},{\"end\":28690,\"start\":28689},{\"end\":28696,\"start\":28695},{\"end\":28698,\"start\":28697},{\"end\":28708,\"start\":28707},{\"end\":28951,\"start\":28950},{\"end\":28953,\"start\":28952},{\"end\":28961,\"start\":28960},{\"end\":28963,\"start\":28962},{\"end\":29149,\"start\":29148},{\"end\":29323,\"start\":29319},{\"end\":29334,\"start\":29330},{\"end\":29345,\"start\":29341},{\"end\":29353,\"start\":29352},{\"end\":29605,\"start\":29604},{\"end\":29616,\"start\":29615},{\"end\":29627,\"start\":29626},{\"end\":30022,\"start\":30021},{\"end\":30033,\"start\":30032},{\"end\":30035,\"start\":30034},{\"end\":30343,\"start\":30342},{\"end\":30347,\"start\":30344},{\"end\":30357,\"start\":30356},{\"end\":30366,\"start\":30365},{\"end\":30563,\"start\":30562},{\"end\":30565,\"start\":30564},{\"end\":30581,\"start\":30580},{\"end\":30583,\"start\":30582},{\"end\":30595,\"start\":30594},{\"end\":30609,\"start\":30608},{\"end\":30841,\"start\":30840},{\"end\":31007,\"start\":31006},{\"end\":31009,\"start\":31008},{\"end\":31195,\"start\":31194},{\"end\":31197,\"start\":31196},{\"end\":31207,\"start\":31206},{\"end\":31222,\"start\":31218},{\"end\":31421,\"start\":31420},{\"end\":31431,\"start\":31427},{\"end\":31440,\"start\":31439},{\"end\":31819,\"start\":31818},{\"end\":31830,\"start\":31829},{\"end\":31844,\"start\":31843},{\"end\":32131,\"start\":32130},{\"end\":32145,\"start\":32144},{\"end\":32158,\"start\":32157},{\"end\":32348,\"start\":32347},{\"end\":32355,\"start\":32354},{\"end\":32368,\"start\":32367},{\"end\":32370,\"start\":32369},{\"end\":32578,\"start\":32577},{\"end\":32585,\"start\":32584},{\"end\":32595,\"start\":32594},{\"end\":32608,\"start\":32607},{\"end\":32610,\"start\":32609},{\"end\":32924,\"start\":32923},{\"end\":32926,\"start\":32925},{\"end\":33118,\"start\":33117},{\"end\":33120,\"start\":33119},{\"end\":33144,\"start\":33139},{\"end\":33153,\"start\":33152},{\"end\":33416,\"start\":33415},{\"end\":33430,\"start\":33429},{\"end\":33442,\"start\":33441},{\"end\":33452,\"start\":33451},{\"end\":33714,\"start\":33713},{\"end\":33723,\"start\":33722},{\"end\":33734,\"start\":33730},{\"end\":33928,\"start\":33927},{\"end\":33939,\"start\":33938},{\"end\":33946,\"start\":33945},{\"end\":33957,\"start\":33956},{\"end\":34189,\"start\":34188},{\"end\":34199,\"start\":34198},{\"end\":34208,\"start\":34207},{\"end\":34216,\"start\":34215},{\"end\":34218,\"start\":34217},{\"end\":34229,\"start\":34228},{\"end\":34231,\"start\":34230},{\"end\":34457,\"start\":34456},{\"end\":34465,\"start\":34464},{\"end\":34475,\"start\":34474},{\"end\":34484,\"start\":34483},{\"end\":34697,\"start\":34696},{\"end\":34709,\"start\":34708},{\"end\":34896,\"start\":34895},{\"end\":34904,\"start\":34903},{\"end\":34912,\"start\":34911},{\"end\":34914,\"start\":34913},{\"end\":34925,\"start\":34924},{\"end\":34934,\"start\":34933},{\"end\":35130,\"start\":35129},{\"end\":35139,\"start\":35138},{\"end\":35147,\"start\":35146},{\"end\":35158,\"start\":35157},{\"end\":35168,\"start\":35167}]", "bib_author_last_name": "[{\"end\":28226,\"start\":28218},{\"end\":28238,\"start\":28230},{\"end\":28247,\"start\":28245},{\"end\":28261,\"start\":28251},{\"end\":28273,\"start\":28267},{\"end\":28287,\"start\":28277},{\"end\":28673,\"start\":28665},{\"end\":28685,\"start\":28677},{\"end\":28693,\"start\":28691},{\"end\":28705,\"start\":28699},{\"end\":28719,\"start\":28709},{\"end\":28958,\"start\":28954},{\"end\":28969,\"start\":28964},{\"end\":29159,\"start\":29150},{\"end\":29328,\"start\":29324},{\"end\":29339,\"start\":29335},{\"end\":29350,\"start\":29346},{\"end\":29362,\"start\":29354},{\"end\":29613,\"start\":29606},{\"end\":29624,\"start\":29617},{\"end\":29635,\"start\":29628},{\"end\":30030,\"start\":30023},{\"end\":30041,\"start\":30036},{\"end\":30354,\"start\":30348},{\"end\":30363,\"start\":30358},{\"end\":30371,\"start\":30367},{\"end\":30578,\"start\":30566},{\"end\":30592,\"start\":30584},{\"end\":30606,\"start\":30596},{\"end\":30617,\"start\":30610},{\"end\":30848,\"start\":30842},{\"end\":31016,\"start\":31010},{\"end\":31204,\"start\":31198},{\"end\":31216,\"start\":31208},{\"end\":31226,\"start\":31223},{\"end\":31425,\"start\":31422},{\"end\":31437,\"start\":31432},{\"end\":31445,\"start\":31441},{\"end\":31827,\"start\":31820},{\"end\":31841,\"start\":31831},{\"end\":31857,\"start\":31845},{\"end\":32142,\"start\":32132},{\"end\":32155,\"start\":32146},{\"end\":32165,\"start\":32159},{\"end\":32352,\"start\":32349},{\"end\":32365,\"start\":32356},{\"end\":32373,\"start\":32371},{\"end\":32582,\"start\":32579},{\"end\":32592,\"start\":32586},{\"end\":32605,\"start\":32596},{\"end\":32613,\"start\":32611},{\"end\":32932,\"start\":32927},{\"end\":33137,\"start\":33121},{\"end\":33150,\"start\":33145},{\"end\":33160,\"start\":33154},{\"end\":33427,\"start\":33417},{\"end\":33439,\"start\":33431},{\"end\":33449,\"start\":33443},{\"end\":33458,\"start\":33453},{\"end\":33720,\"start\":33715},{\"end\":33728,\"start\":33724},{\"end\":33741,\"start\":33735},{\"end\":33936,\"start\":33929},{\"end\":33943,\"start\":33940},{\"end\":33954,\"start\":33947},{\"end\":33968,\"start\":33958},{\"end\":34196,\"start\":34190},{\"end\":34205,\"start\":34200},{\"end\":34213,\"start\":34209},{\"end\":34226,\"start\":34219},{\"end\":34234,\"start\":34232},{\"end\":34462,\"start\":34458},{\"end\":34472,\"start\":34466},{\"end\":34481,\"start\":34476},{\"end\":34491,\"start\":34485},{\"end\":34706,\"start\":34698},{\"end\":34716,\"start\":34710},{\"end\":34901,\"start\":34897},{\"end\":34909,\"start\":34905},{\"end\":34922,\"start\":34915},{\"end\":34931,\"start\":34926},{\"end\":34943,\"start\":34935},{\"end\":35136,\"start\":35131},{\"end\":35144,\"start\":35140},{\"end\":35155,\"start\":35148},{\"end\":35165,\"start\":35159},{\"end\":35175,\"start\":35169},{\"end\":28226,\"start\":28218},{\"end\":28238,\"start\":28230},{\"end\":28247,\"start\":28245},{\"end\":28261,\"start\":28251},{\"end\":28273,\"start\":28267},{\"end\":28287,\"start\":28277},{\"end\":28673,\"start\":28665},{\"end\":28685,\"start\":28677},{\"end\":28693,\"start\":28691},{\"end\":28705,\"start\":28699},{\"end\":28719,\"start\":28709},{\"end\":28958,\"start\":28954},{\"end\":28969,\"start\":28964},{\"end\":29159,\"start\":29150},{\"end\":29328,\"start\":29324},{\"end\":29339,\"start\":29335},{\"end\":29350,\"start\":29346},{\"end\":29362,\"start\":29354},{\"end\":29613,\"start\":29606},{\"end\":29624,\"start\":29617},{\"end\":29635,\"start\":29628},{\"end\":30030,\"start\":30023},{\"end\":30041,\"start\":30036},{\"end\":30354,\"start\":30348},{\"end\":30363,\"start\":30358},{\"end\":30371,\"start\":30367},{\"end\":30578,\"start\":30566},{\"end\":30592,\"start\":30584},{\"end\":30606,\"start\":30596},{\"end\":30617,\"start\":30610},{\"end\":30848,\"start\":30842},{\"end\":31016,\"start\":31010},{\"end\":31204,\"start\":31198},{\"end\":31216,\"start\":31208},{\"end\":31226,\"start\":31223},{\"end\":31425,\"start\":31422},{\"end\":31437,\"start\":31432},{\"end\":31445,\"start\":31441},{\"end\":31827,\"start\":31820},{\"end\":31841,\"start\":31831},{\"end\":31857,\"start\":31845},{\"end\":32142,\"start\":32132},{\"end\":32155,\"start\":32146},{\"end\":32165,\"start\":32159},{\"end\":32352,\"start\":32349},{\"end\":32365,\"start\":32356},{\"end\":32373,\"start\":32371},{\"end\":32582,\"start\":32579},{\"end\":32592,\"start\":32586},{\"end\":32605,\"start\":32596},{\"end\":32613,\"start\":32611},{\"end\":32932,\"start\":32927},{\"end\":33137,\"start\":33121},{\"end\":33150,\"start\":33145},{\"end\":33160,\"start\":33154},{\"end\":33427,\"start\":33417},{\"end\":33439,\"start\":33431},{\"end\":33449,\"start\":33443},{\"end\":33458,\"start\":33453},{\"end\":33720,\"start\":33715},{\"end\":33728,\"start\":33724},{\"end\":33741,\"start\":33735},{\"end\":33936,\"start\":33929},{\"end\":33943,\"start\":33940},{\"end\":33954,\"start\":33947},{\"end\":33968,\"start\":33958},{\"end\":34196,\"start\":34190},{\"end\":34205,\"start\":34200},{\"end\":34213,\"start\":34209},{\"end\":34226,\"start\":34219},{\"end\":34234,\"start\":34232},{\"end\":34462,\"start\":34458},{\"end\":34472,\"start\":34466},{\"end\":34481,\"start\":34476},{\"end\":34491,\"start\":34485},{\"end\":34706,\"start\":34698},{\"end\":34716,\"start\":34710},{\"end\":34901,\"start\":34897},{\"end\":34909,\"start\":34905},{\"end\":34922,\"start\":34915},{\"end\":34931,\"start\":34926},{\"end\":34943,\"start\":34935},{\"end\":35136,\"start\":35131},{\"end\":35144,\"start\":35140},{\"end\":35155,\"start\":35148},{\"end\":35165,\"start\":35159},{\"end\":35175,\"start\":35169}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":7354162},\"end\":28608,\"start\":28158},{\"attributes\":{\"doi\":\"arXiv:1309.5401\",\"id\":\"b1\"},\"end\":28909,\"start\":28610},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":21874346},\"end\":29058,\"start\":28911},{\"attributes\":{\"id\":\"b3\"},\"end\":29270,\"start\":29060},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":395534},\"end\":29522,\"start\":29272},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1909082},\"end\":29880,\"start\":29524},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":6978911},\"end\":30279,\"start\":29882},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":14026523},\"end\":30494,\"start\":30281},{\"attributes\":{\"id\":\"b8\"},\"end\":30767,\"start\":30496},{\"attributes\":{\"id\":\"b9\"},\"end\":30937,\"start\":30769},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":207596505},\"end\":31144,\"start\":30939},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2309950},\"end\":31363,\"start\":31146},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":10057375},\"end\":31738,\"start\":31365},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":8166368},\"end\":32063,\"start\":31740},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":195908774},\"end\":32296,\"start\":32065},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":12589862},\"end\":32480,\"start\":32298},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":9511725},\"end\":32818,\"start\":32482},{\"attributes\":{\"id\":\"b17\"},\"end\":33055,\"start\":32820},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":545361},\"end\":33295,\"start\":33057},{\"attributes\":{\"id\":\"b19\"},\"end\":33646,\"start\":33297},{\"attributes\":{\"id\":\"b20\"},\"end\":33894,\"start\":33648},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":7156990},\"end\":34118,\"start\":33896},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":16900529},\"end\":34383,\"start\":34120},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":11188352},\"end\":34629,\"start\":34385},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":415956},\"end\":34830,\"start\":34631},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1309931},\"end\":35088,\"start\":34832},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14563079},\"end\":35293,\"start\":35090},{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":7354162},\"end\":28608,\"start\":28158},{\"attributes\":{\"doi\":\"arXiv:1309.5401\",\"id\":\"b1\"},\"end\":28909,\"start\":28610},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":21874346},\"end\":29058,\"start\":28911},{\"attributes\":{\"id\":\"b3\"},\"end\":29270,\"start\":29060},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":395534},\"end\":29522,\"start\":29272},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1909082},\"end\":29880,\"start\":29524},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":6978911},\"end\":30279,\"start\":29882},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":14026523},\"end\":30494,\"start\":30281},{\"attributes\":{\"id\":\"b8\"},\"end\":30767,\"start\":30496},{\"attributes\":{\"id\":\"b9\"},\"end\":30937,\"start\":30769},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":207596505},\"end\":31144,\"start\":30939},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2309950},\"end\":31363,\"start\":31146},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":10057375},\"end\":31738,\"start\":31365},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":8166368},\"end\":32063,\"start\":31740},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":195908774},\"end\":32296,\"start\":32065},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":12589862},\"end\":32480,\"start\":32298},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":9511725},\"end\":32818,\"start\":32482},{\"attributes\":{\"id\":\"b17\"},\"end\":33055,\"start\":32820},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":545361},\"end\":33295,\"start\":33057},{\"attributes\":{\"id\":\"b19\"},\"end\":33646,\"start\":33297},{\"attributes\":{\"id\":\"b20\"},\"end\":33894,\"start\":33648},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":7156990},\"end\":34118,\"start\":33896},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":16900529},\"end\":34383,\"start\":34120},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":11188352},\"end\":34629,\"start\":34385},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":415956},\"end\":34830,\"start\":34631},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1309931},\"end\":35088,\"start\":34832},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14563079},\"end\":35293,\"start\":35090}]", "bib_title": "[{\"end\":28214,\"start\":28158},{\"end\":28948,\"start\":28911},{\"end\":29317,\"start\":29272},{\"end\":29602,\"start\":29524},{\"end\":30019,\"start\":29882},{\"end\":30340,\"start\":30281},{\"end\":31004,\"start\":30939},{\"end\":31192,\"start\":31146},{\"end\":31418,\"start\":31365},{\"end\":31816,\"start\":31740},{\"end\":32128,\"start\":32065},{\"end\":32345,\"start\":32298},{\"end\":32575,\"start\":32482},{\"end\":33115,\"start\":33057},{\"end\":33925,\"start\":33896},{\"end\":34186,\"start\":34120},{\"end\":34454,\"start\":34385},{\"end\":34694,\"start\":34631},{\"end\":34893,\"start\":34832},{\"end\":35127,\"start\":35090},{\"end\":28214,\"start\":28158},{\"end\":28948,\"start\":28911},{\"end\":29317,\"start\":29272},{\"end\":29602,\"start\":29524},{\"end\":30019,\"start\":29882},{\"end\":30340,\"start\":30281},{\"end\":31004,\"start\":30939},{\"end\":31192,\"start\":31146},{\"end\":31418,\"start\":31365},{\"end\":31816,\"start\":31740},{\"end\":32128,\"start\":32065},{\"end\":32345,\"start\":32298},{\"end\":32575,\"start\":32482},{\"end\":33115,\"start\":33057},{\"end\":33925,\"start\":33896},{\"end\":34186,\"start\":34120},{\"end\":34454,\"start\":34385},{\"end\":34694,\"start\":34631},{\"end\":34893,\"start\":34832},{\"end\":35127,\"start\":35090}]", "bib_author": "[{\"end\":28228,\"start\":28216},{\"end\":28240,\"start\":28228},{\"end\":28249,\"start\":28240},{\"end\":28263,\"start\":28249},{\"end\":28275,\"start\":28263},{\"end\":28289,\"start\":28275},{\"end\":28675,\"start\":28663},{\"end\":28687,\"start\":28675},{\"end\":28695,\"start\":28687},{\"end\":28707,\"start\":28695},{\"end\":28721,\"start\":28707},{\"end\":28960,\"start\":28950},{\"end\":28971,\"start\":28960},{\"end\":29161,\"start\":29148},{\"end\":29330,\"start\":29319},{\"end\":29341,\"start\":29330},{\"end\":29352,\"start\":29341},{\"end\":29364,\"start\":29352},{\"end\":29615,\"start\":29604},{\"end\":29626,\"start\":29615},{\"end\":29637,\"start\":29626},{\"end\":30032,\"start\":30021},{\"end\":30043,\"start\":30032},{\"end\":30356,\"start\":30342},{\"end\":30365,\"start\":30356},{\"end\":30373,\"start\":30365},{\"end\":30580,\"start\":30562},{\"end\":30594,\"start\":30580},{\"end\":30608,\"start\":30594},{\"end\":30619,\"start\":30608},{\"end\":30850,\"start\":30840},{\"end\":31018,\"start\":31006},{\"end\":31206,\"start\":31194},{\"end\":31218,\"start\":31206},{\"end\":31228,\"start\":31218},{\"end\":31427,\"start\":31420},{\"end\":31439,\"start\":31427},{\"end\":31447,\"start\":31439},{\"end\":31829,\"start\":31818},{\"end\":31843,\"start\":31829},{\"end\":31859,\"start\":31843},{\"end\":32144,\"start\":32130},{\"end\":32157,\"start\":32144},{\"end\":32167,\"start\":32157},{\"end\":32354,\"start\":32347},{\"end\":32367,\"start\":32354},{\"end\":32375,\"start\":32367},{\"end\":32584,\"start\":32577},{\"end\":32594,\"start\":32584},{\"end\":32607,\"start\":32594},{\"end\":32615,\"start\":32607},{\"end\":32934,\"start\":32923},{\"end\":33139,\"start\":33117},{\"end\":33152,\"start\":33139},{\"end\":33162,\"start\":33152},{\"end\":33429,\"start\":33415},{\"end\":33441,\"start\":33429},{\"end\":33451,\"start\":33441},{\"end\":33460,\"start\":33451},{\"end\":33722,\"start\":33713},{\"end\":33730,\"start\":33722},{\"end\":33743,\"start\":33730},{\"end\":33938,\"start\":33927},{\"end\":33945,\"start\":33938},{\"end\":33956,\"start\":33945},{\"end\":33970,\"start\":33956},{\"end\":34198,\"start\":34188},{\"end\":34207,\"start\":34198},{\"end\":34215,\"start\":34207},{\"end\":34228,\"start\":34215},{\"end\":34236,\"start\":34228},{\"end\":34464,\"start\":34456},{\"end\":34474,\"start\":34464},{\"end\":34483,\"start\":34474},{\"end\":34493,\"start\":34483},{\"end\":34708,\"start\":34696},{\"end\":34718,\"start\":34708},{\"end\":34903,\"start\":34895},{\"end\":34911,\"start\":34903},{\"end\":34924,\"start\":34911},{\"end\":34933,\"start\":34924},{\"end\":34945,\"start\":34933},{\"end\":35138,\"start\":35129},{\"end\":35146,\"start\":35138},{\"end\":35157,\"start\":35146},{\"end\":35167,\"start\":35157},{\"end\":35177,\"start\":35167},{\"end\":28228,\"start\":28216},{\"end\":28240,\"start\":28228},{\"end\":28249,\"start\":28240},{\"end\":28263,\"start\":28249},{\"end\":28275,\"start\":28263},{\"end\":28289,\"start\":28275},{\"end\":28675,\"start\":28663},{\"end\":28687,\"start\":28675},{\"end\":28695,\"start\":28687},{\"end\":28707,\"start\":28695},{\"end\":28721,\"start\":28707},{\"end\":28960,\"start\":28950},{\"end\":28971,\"start\":28960},{\"end\":29161,\"start\":29148},{\"end\":29330,\"start\":29319},{\"end\":29341,\"start\":29330},{\"end\":29352,\"start\":29341},{\"end\":29364,\"start\":29352},{\"end\":29615,\"start\":29604},{\"end\":29626,\"start\":29615},{\"end\":29637,\"start\":29626},{\"end\":30032,\"start\":30021},{\"end\":30043,\"start\":30032},{\"end\":30356,\"start\":30342},{\"end\":30365,\"start\":30356},{\"end\":30373,\"start\":30365},{\"end\":30580,\"start\":30562},{\"end\":30594,\"start\":30580},{\"end\":30608,\"start\":30594},{\"end\":30619,\"start\":30608},{\"end\":30850,\"start\":30840},{\"end\":31018,\"start\":31006},{\"end\":31206,\"start\":31194},{\"end\":31218,\"start\":31206},{\"end\":31228,\"start\":31218},{\"end\":31427,\"start\":31420},{\"end\":31439,\"start\":31427},{\"end\":31447,\"start\":31439},{\"end\":31829,\"start\":31818},{\"end\":31843,\"start\":31829},{\"end\":31859,\"start\":31843},{\"end\":32144,\"start\":32130},{\"end\":32157,\"start\":32144},{\"end\":32167,\"start\":32157},{\"end\":32354,\"start\":32347},{\"end\":32367,\"start\":32354},{\"end\":32375,\"start\":32367},{\"end\":32584,\"start\":32577},{\"end\":32594,\"start\":32584},{\"end\":32607,\"start\":32594},{\"end\":32615,\"start\":32607},{\"end\":32934,\"start\":32923},{\"end\":33139,\"start\":33117},{\"end\":33152,\"start\":33139},{\"end\":33162,\"start\":33152},{\"end\":33429,\"start\":33415},{\"end\":33441,\"start\":33429},{\"end\":33451,\"start\":33441},{\"end\":33460,\"start\":33451},{\"end\":33722,\"start\":33713},{\"end\":33730,\"start\":33722},{\"end\":33743,\"start\":33730},{\"end\":33938,\"start\":33927},{\"end\":33945,\"start\":33938},{\"end\":33956,\"start\":33945},{\"end\":33970,\"start\":33956},{\"end\":34198,\"start\":34188},{\"end\":34207,\"start\":34198},{\"end\":34215,\"start\":34207},{\"end\":34228,\"start\":34215},{\"end\":34236,\"start\":34228},{\"end\":34464,\"start\":34456},{\"end\":34474,\"start\":34464},{\"end\":34483,\"start\":34474},{\"end\":34493,\"start\":34483},{\"end\":34708,\"start\":34696},{\"end\":34718,\"start\":34708},{\"end\":34903,\"start\":34895},{\"end\":34911,\"start\":34903},{\"end\":34924,\"start\":34911},{\"end\":34933,\"start\":34924},{\"end\":34945,\"start\":34933},{\"end\":35138,\"start\":35129},{\"end\":35146,\"start\":35138},{\"end\":35157,\"start\":35146},{\"end\":35167,\"start\":35157},{\"end\":35177,\"start\":35167}]", "bib_venue": "[{\"end\":28358,\"start\":28289},{\"end\":28661,\"start\":28610},{\"end\":28975,\"start\":28971},{\"end\":29146,\"start\":29060},{\"end\":29387,\"start\":29364},{\"end\":29677,\"start\":29637},{\"end\":30063,\"start\":30043},{\"end\":30377,\"start\":30373},{\"end\":30560,\"start\":30496},{\"end\":30838,\"start\":30769},{\"end\":31036,\"start\":31018},{\"end\":31246,\"start\":31228},{\"end\":31489,\"start\":31447},{\"end\":31891,\"start\":31859},{\"end\":32171,\"start\":32167},{\"end\":32379,\"start\":32375},{\"end\":32640,\"start\":32615},{\"end\":32921,\"start\":32820},{\"end\":33166,\"start\":33162},{\"end\":33413,\"start\":33297},{\"end\":33711,\"start\":33648},{\"end\":33997,\"start\":33970},{\"end\":34240,\"start\":34236},{\"end\":34497,\"start\":34493},{\"end\":34722,\"start\":34718},{\"end\":34949,\"start\":34945},{\"end\":35181,\"start\":35177},{\"end\":28358,\"start\":28289},{\"end\":28661,\"start\":28610},{\"end\":28975,\"start\":28971},{\"end\":29146,\"start\":29060},{\"end\":29387,\"start\":29364},{\"end\":29677,\"start\":29637},{\"end\":30063,\"start\":30043},{\"end\":30377,\"start\":30373},{\"end\":30560,\"start\":30496},{\"end\":30838,\"start\":30769},{\"end\":31036,\"start\":31018},{\"end\":31246,\"start\":31228},{\"end\":31489,\"start\":31447},{\"end\":31891,\"start\":31859},{\"end\":32171,\"start\":32167},{\"end\":32379,\"start\":32375},{\"end\":32640,\"start\":32615},{\"end\":32921,\"start\":32820},{\"end\":33166,\"start\":33162},{\"end\":33413,\"start\":33297},{\"end\":33711,\"start\":33648},{\"end\":33997,\"start\":33970},{\"end\":34240,\"start\":34236},{\"end\":34497,\"start\":34493},{\"end\":34722,\"start\":34718},{\"end\":34949,\"start\":34945},{\"end\":35181,\"start\":35177}]"}}}, "year": 2023, "month": 12, "day": 17}