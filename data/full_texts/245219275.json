{"id": 245219275, "updated": "2023-10-05 18:41:15.436", "metadata": {"title": "Graph Structure Learning with Variational Information Bottleneck", "authors": "[{\"first\":\"Qingyun\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Jianxin\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Peng\",\"middle\":[]},{\"first\":\"Jia\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Xingcheng\",\"last\":\"Fu\",\"middle\":[]},{\"first\":\"Cheng\",\"last\":\"Ji\",\"middle\":[]},{\"first\":\"Philip\",\"last\":\"Yu\",\"middle\":[\"S.\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Graph Neural Networks (GNNs) have shown promising results on a broad spectrum of applications. Most empirical studies of GNNs directly take the observed graph as input, assuming the observed structure perfectly depicts the accurate and complete relations between nodes. However, graphs in the real world are inevitably noisy or incomplete, which could even exacerbate the quality of graph representations. In this work, we propose a novel Variational Information Bottleneck guided Graph Structure Learning framework, namely VIB-GSL, in the perspective of information theory. VIB-GSL advances the Information Bottleneck (IB) principle for graph structure learning, providing a more elegant and universal framework for mining underlying task-relevant relations. VIB-GSL learns an informative and compressive graph structure to distill the actionable information for specific downstream tasks. VIB-GSL deduces a variational approximation for irregular graph data to form a tractable IB objective function, which facilitates training stability. Extensive experimental results demonstrate that the superior effectiveness and robustness of VIB-GSL.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2112.08903", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/Sun0P0FJY22", "doi": "10.1609/aaai.v36i4.20335"}}, "content": {"source": {"pdf_hash": "7fc37fbd2f808984cfc5c78410a993f89eb0ef49", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2112.08903v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1f58fea7eaca9043b27e11fa5275b48bbca22bb9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7fc37fbd2f808984cfc5c78410a993f89eb0ef49.txt", "contents": "\nGraph Structure Learning with Variational Information Bottleneck\n\n\nQingyun Sun \nJianxin Li \nHao Peng \nBeijing Advanced Innovation Center for Big Data and Brain Computing\nBeihang University\n100191BeijingChina\n\nJia Wu jia.wu@mq.edu.au \nSchool of Computing\nMacquarie University\nSydneyAustralia\n\nXingcheng Fu \nBeijing Advanced Innovation Center for Big Data and Brain Computing\nBeihang University\n100191BeijingChina\n\nJi Cheng jicheng@act.buaa.edu.cn \nPhilip S Yu psyu@uic.edu \nBeijing Advanced Innovation Center for Big Data and Brain Computing\nBeihang University\n100191BeijingChina\n\nDepartment of Computer Science\nUniversity of Illinois at Chicago\nChicagoUSA\n\n\nSchool of Computer Science and Engineering\nBeihang University\n100191BeijingChina\n\n\nShenyuan Honors College\nBeihang University\n100191BeijingChina\n\nGraph Structure Learning with Variational Information Bottleneck\n\nGraph Neural Networks (GNNs) have shown promising results on a broad spectrum of applications. Most empirical studies of GNNs directly take the observed graph as input, assuming the observed structure perfectly depicts the accurate and complete relations between nodes. However, graphs in the real-world are inevitably noisy or incomplete, which could even exacerbate the quality of graph representations. In this work, we propose a novel Variational Information Bottleneck guided Graph Structure Learning framework, namely VIB-GSL, in the perspective of information theory. VIB-GSL advances the Information Bottleneck (IB) principle for graph structure learning, providing a more elegant and universal framework for mining underlying task-relevant relations. VIB-GSL learns an informative and compressive graph structure to distill the actionable information for specific downstream tasks. VIB-GSL deduces a variational approximation for irregular graph data to form a tractable IB objective function, which facilitates training stability. Extensive experimental results demonstrate that the superior effectiveness and robustness of VIB-GSL.\n\nIntroduction\n\nRecent years have seen a significant growing amount of interest in graph representation learning (Zhang et al. 2018;Tong et al. 2021), especially in efforts devoted to developing more effective graph neural networks (GNNs) (Zhou et al. 2020). Despite GNNs' powerful ability in learning graph representations, most of them directly take the observed graph as input, assuming the observed structure perfectly depicts the accurate and complete relations between nodes. However, these raw graphs are naturally admitted from network-structure data (e.g., social network) or constructed from the original feature space by some pre-defined rules, which are usually independent of the downstream tasks and lead to the gap between the raw graph and the optimal graph for specific tasks. Moreover, most of graphs in the real-word are noisy or incomplete due to the error-prone data collection (Chen, Wu, and Zaki 2020), which could even exacerbate the quality of representations produced by GNNs (Z\u00fcgner, Akbarnejad, and G\u00fcnnemann 2018;Sun et al. 2018). It's also found that the properties of a graph Copyright \u00a9 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. are mainly determined by some critical structures rather than the whole graph (Sun et al. 2021;Peng et al. 2021). Furthermore, many graph enhanced applications (e.g., text classification  and vision navigation (Gao et al. 2021)) may only have data without graph-structure and require additional graph construction to perform representation learning. The above issues pose a great challenge for applying GNNs to real-world applications, especially in some risk-critical scenarios. Therefore, learning a task-relevant graph structure is a fundamental problem for graph representation learning.\n\nTo adaptively learn graph structures for GNNs, many graph structure learning methods (Zhu et al. 2021;Franceschi et al. 2019;Chen, Wu, and Zaki 2020) are proposed, most of which optimize the adjacency matrix along with the GNN parameters toward downstream tasks with assumptions (e.g., community) or certain constraints (e.g., sparsity, low-rank, and smoothness) on the graphs. However, these assumptions or explicit certain constraints may not be applicable to all datasets and tasks. There is still a lack of a general framework that can mine underlying relations from the essence of representation learning.\n\nRecalling the above problems, the key of structure learning problem is learning the underlying relations invariant to task-irrelevant information. Information Bottleneck (IB) principle (Tishby, Pereira, and Bialek 2000) provides a framework for constraining such task-irrelevant information retained at the output by trading off between prediction and compression. Specifically, the IB principle seeks for a representation Z that is maximally informative about target Y (i.e., maximize mutual information I(Y ; Z)) while being minimally informative about input data X (i.e., minimize mutual information I(X; Z)). Based on the IB principle, the learned representation is naturally more robust to data noise. IB has been applied to representation learning Jeon et al. 2021;Pan et al. 2020;Bao 2021;Dubois et al. 2020) and numerous deep learning tasks such as model ensemble (Sinha et al. 2020), fine-tuning (Mahabadi, Belinkov, and Henderson 2021), salient region discovery (Zhmoginov, Fischer, and Sandler 2020).\n\nIn this paper, we advance the IB principle for graph to solve the graph structure learning problem. We propose a novel Variational Information Bottleneck guided Graph Structure Learning framework, namely VIB-GSL. VIB-GSL employs the irrelevant feature masking and structure learning method to generate a new IB-Graph G IB as a bottleneck to distill the actionable information for the downstream task. VIB-GSL consists of three steps: (1) the IB-Graph generator module learns the IB-graph G IB by masking irrelevant node features and learning a new graph structure based on the masked feature; (2) the GNN module takes the IBgraph G IB as input and learns the distribution of graph representations; (3) the graph representation is sampled from the learned distribution with a reparameterization trick and then used for classification. The overall framework can be trained efficiently with the supervised classification loss and the distribution KL-divergence loss for the IB objective. The main contributions are summarized as follows:\n\n\u2022 VIB-GSL advances the Information Bottleneck principle for graph structure learning, providing an elegant and universal framework in the perspective of information theory. \u2022 VIB-GSL is model-agnostic and has a tractable variational optimization upper bound that is easy and stable to optimize. It is sufficient to plug existing GNNs into the VIB-GSL framework to enhance their performances. \u2022 Extensive experiment results in graph classification and graph denoising demonstrate that the proposed VIB-GSL enjoys superior effectiveness and robustness compared to other strong baselines.\n\n2 Background and Problem Formulation\n\n\nGraph Structure Learning\n\nGraph structure learning (Zhu et al. 2021) targets jointly learning an optimized graph structure and corresponding representations to improving the robustness of GNN models. In this work, we focus on graph structure learning for graph-level tasks. Let G \u2208 G be a graph with label Y \u2208 Y. Given a graph G = (X, A) with node set V , node feature matrix X \u2208 R |V |\u00d7d , and adjacency matrix A \u2208 R |V |\u00d7|V | , or only given a feature matrix X, the graph structure learning problem we consider in this paper can be formulated as producing an optimized graph G * = (X * , A * ) and its corresponding node/graph representations Z * = f (G * ), with respect to the downstream graph-level tasks.\n\n\nInformation Bottleneck\n\nThe Information Bottleneck (Tishby, Pereira, and Bialek 2000) seeks the balance between data fit and generalization using the mutual information as both cost function and regularizer. We will use the following standard quantities in the information theory (Cover 1999) frequently: Shannon entropy H(X) = E X\u223cp(X) [\u2212 log p(X)], cross entropy H(p(X), q(X)) = E X\u223cp(X) [\u2212 log q(X)], Shannon mutual information I(X; Y ) = H(X) \u2212 H(X|Y ), and Kullback Leiber divergence D KL (p(X)||q(X) = E X\u223cp(X) log p(X) q(X) . Following standard practice in the IB literature (Tishby, Pereira, and Bialek 2000), given data X, representation Z of X and target Y , (X, Y, Z) are following the Markov Chain < Y \u2192 X \u2192 Z >.\n\nDefinition 1 (Information Bottleneck). For the input data X and its label Y , the Information Bottleneck principle aims to learn the minimal sufficient representation Z:\nZ = arg min Z \u2212I(Z; Y ) + \u03b2I(Z; X),(1)\nwhere \u03b2 is the Lagrangian multiplier trading off sufficiency and minimality.\n\nDeep VIB (Alemi et al. 2016) proposed a variational approximation to the IB objective by parameterizing the distribution via a neural network:\nL = 1 N N i=1 dZp(Z|X i ) log q(Y i |Z) + \u03b2D KL (p(Z|X i ), r(Z)) ,(2)\nwhere q(Y i |Z) is the variational approximation to p(Y i |Z) and r(Z) is the variational approximation of p(Z).\n\nThe IB framework has received significant attention in machine learning and deep learning (Alemi et al. 2016;Saxe et al. 2019). As for irregular graph data, there are some recent works Yang et al. 2021;Yu et al. 2021) introducing the IB principle to graph learning. GIB  extends the general IB to graph data with regularization of the structure and feature information for robust node representations. SIB  was proposed for the subgraph recognition problem. HGIB  was proposed to implement the consensus hypothesis of heterogeneous information networks in an unsupervised manner. We illustrate the difference between related graph IB methods and our method in Section 3.3.\n\n\nVariational Information Bottleneck Guided\n\nGraph Structure learning\n\nIn this section, we elaborate the proposed VIB-GSL, a novel variational information bottleneck principle guided graph structure learning framework. First, we formally define the IB-Graph and introduce a tractable upper bound for IB objective. Then, we introduce the graph generator to learn the optimal IB-Graph as a bottleneck and give the overall framework of VIB-GSL. Lastly, we compare VIB-GSL with two graph IB methods to illustrate its difference and properties.\n\n\nGraph Information Bottleneck\n\nIn this work, we focus on learning an optimal graph G IB = (X IB , A IB ) named IB-Graph for G, which is compressed with minimum information loss in terms of G's properties.\n\nDefinition 2 (IB-Graph). For a graph G = (X, A) and its label Y , the optimal graph G IB = (X IB , A IB ) found by Information Bottleneck is denoted as IB-Graph:\nG IB = arg min GIB \u2212I(G IB ; Y ) + \u03b2I(G IB ; G),(3)\nwhere X IB is the task-relevant feature set and A IB is the learned task-relevant graph adjacency matrix.\n\nIntuitively, the first term \u2212I(G IB ; Y ) is the prediction term, which encourages that essential information to the graph property is preserved. The second term I(G IB ; G) is the compression term, which encourages that labelirrelevant information in G is dropped. And the Lagrangian multiplier \u03b2 indicates the degree of information compression, where larger \u03b2 indicates more information in G was retained to G IB . Suppose G n \u2208 G is a task-irrelevant nuisance in G, the learning procedure of G IB follows the Markov Chain < (Y, G n ) \u2192 G \u2192 G IB >. IB-Graph only preserves the task-relevant information in the observed graph G and is invariant to nuisances in data.\n\nLemma 1 (Nuisance Invariance). Given a graph G \u2208 G with label Y \u2208 Y, let G n \u2208 G be a task-irrelevant nuisance for Y . Denote G IB as the IB-Graph learned from G, then the following inequality holds:\nI(G IB ; G n ) \u2264 I(G IB ; G) \u2212 I(G IB ; Y )(4)\nPlease refer to the Technical Appendix for the detailed proof. Lemma 1 indicates that optimizing the IB objective in Eq. (3) is equivalent to encourage G IB to be less related to task-irrelevant information in G, leading to the nuisanceinvariant property of IB-Graph.\n\nDue to the non-Euclidean nature of graph data and the intractability of mutual information, the IB objective in Eq. (3) is hard to optimize directly. Therefore, we introduce two tractable variational upper bounds of \u2212I(G IB ; Y ) and I(G IB ; G), respectively. First, we examine the prediction term \u2212I(G IB ; Y ) in Eq. (3), which encourages G IB is informative of Y . Please refer to Technical Appendix for the detailed proof of Proposition 1.\nProposition 1 (Upper bound of \u2212I(G IB ; Y )). For graph G \u2208 G with label Y \u2208 Y and IB-Graph G IB learned from G, we have \u2212I(Y ; G IB ) \u2264 \u2212 p(Y, G IB ) log q \u03b8 (Y |G IB )dY dG IB + H(Y ), (5) where q \u03b8 (Y |G IB ) is the variational approximation of the true posterior p(Y |G IB ).\nThen we examine the compression term I(G IB ; G) in Eq. (3), which constrains the information that G IB receives from G. Please refer to the Technical Appendix for the detailed proof of Proposition 2.\n\nProposition 2 (Upper bound of I(G IB ; G) ). For graph G \u2208 G and IB-Graph G IB learned from G, we have\nI(G IB ; G) \u2264 p(G IB , G) log p(G IB |G) r(G IB ) dG IB dG,(6)\nwhere r(G IB ) is the variational approximation to the prior distribution p(G IB ) of G IB .\n\nFinally, plug Eq. (5) and Eq. (6) into Eq.\n\n(3) to derive the following objective function, which we try to minimize:\n\u2212I(G IB ; Y ) + \u03b2I(G IB ; G) \u2264 \u2212 p(Y, G IB ) log q \u03b8 (Y |G IB )dY dG IB + \u03b2 p(G IB , G) log p(G IB |G) r(G IB ) dG IB dG.(7)\n3.2 Instantiating the VIB-GSL Framework\n\nFollowing the theory discussed in Section 3.1, we first obtain the graph representation Z IB of G IB to optimize the IB objective in Eq. (7). We assume that there is no information loss during this process, which is the general practice of mutual information estimation (Tian et al. 2020). Therefore, we have\nI(G IB ; Y ) \u2248 I(Z IB ; Y ) and I(G IB ; G) \u2248 I(Z IB ; G).\nIn practice, the integral over G IB and G can be approximated by Monte Carlo sampling (Shapiro 2003) on all train-\ning samples {G i \u2208 G, Y i \u2208 Y, i = 1, . . . , N }. \u2212 I(G IB ; Y ) + \u03b2I(G IB ; G) \u2248 \u2212I(Z IB ; Y ) + \u03b2I(Z IB ; G) \u2264 1 N N i=1 \u2212 log q \u03b8 (Y i |Z IBi )+\u03b2p(Z IBi |G i )log p(Z IBi |G i ) r(Z IB ) .\n(8) As shown in Figure 1, VIB-GSL consists of three steps:\n\nStep-1: Generate IB-Graph G IB . We introduce an IB-Graph generator to generate the IBgraph G IB for the input graph G. Following the assumption that nuisance information exists in both irrelevant feature and structure, the generation procedure consists of feature masking and structure learning. Feature Masking. We first use a feature masking scheme to discretely drop features that are irrelevant to the downstream task, which is formulated as:\nX IB = {X i M, i = 1, 2, \u00b7 \u00b7 \u00b7 , |V |},(9)\nwhere M \u2208 R d is a learnable binary feature mask and is the element-wise product. Intuitively, if a particular feature is not relevant to task, the corresponding weight in M takes value close to zero. We can reparameterize X IB using the reparameterization trick (Kingma and Welling 2013) to backpropagate through a d-dimensional random variable:\nX IB = X r + (X \u2212 X r ) M,(10)\nwhere X r is a random variable sampled from the empirical distribution of X. Structure Learning. We model all possible edges as a set of mutually independent Bernoulli random variables parameterized by the learned attention weights \u03c0:\nA IB = u,v\u2208V {a u,v \u223c Ber (\u03c0 u,v )} .(11)\nFor each pair of nodes, we optimized the edge sampling probability \u03c0 jointly with the graph representation learning. \u03c0 u,v describes the task-specific quality of edge (u, v) and smaller \u03c0 u,v indicates that the edge (u, v) is more likely to be noise and should be assigned small weight or even be removed. For a pair of nodes (u, v), the edge sampling probability \u03c0 u,v is calculated by:\nZ(u) = NN (X IB (u)) , \u03c0 u,v = sigmoid Z(u)Z(v) T ,(12)\nwhere NN(\u00b7) denotes a neural network and we use a twolayer perceptron in this work. One issue is that A IB is not differentiable with respect to \u03c0 as Bernoulli distribution. We thus use the concrete relaxation (Jang, Gu, and Poole 2017) of the Bernoulli distribution to update \u03c0:\nBer(\u03c0 u,v ) \u2248 sigmoid 1 t log \u03c0 u,v 1 \u2212 \u03c0 u,v + log 1 \u2212 ,(13)\nwhere \u223c Uniform(0, 1) and t \u2208 R + is the temperature for the concrete distribution. After concrete relaxation, the binary entries a u,v from a Bernoulli distribution are transformed into a deterministic function of \u03c0 u,v and .\n\nThe graph structure after the concrete relaxation is a weighted fully connected graph, which is computationally expensive. We hence extract a symmetric sparse adjacency matrix by masking off those elements which are smaller than a non-negative threshold a 0 .\n\nStep-2: Learn Distribution of IB-Graph Representation. For the compression term I(Z IB ; G) in Eq. (8), we consider a parametric Gaussian distribution as prior r(Z IB ) and p(Z IB |G) to allow an analytic computation of Kullback Leibler (KL) divergence (Hershey and Olsen 2007):\nr (Z IB ) = N (\u00b5 0 , \u03a3 0 ) , p (Z IB |G) = N f \u00b5 \u03c6 (G IB ) , f \u03a3 \u03c6 (G IB ) ,(14)\nwhere \u00b5 \u2208 R K and \u03a3 \u2208 R K\u00d7K is the mean vector and the diagonal co-variance matrix of Z IB encoded by f \u03c6 (G IB ).\n\nThe dimensionality of Z IB is denoted as K, which specifies the bottleneck size. We model the f \u03c6 (G IB ) as a graph neural network\n(GNN) with weights \u03c6, where f \u00b5 \u03c6 (G IB ) and f \u03a3 \u03c6 (G IB ) are the 2K-dimensional output value of the GNN: \u2200u \u2208 V, Z IB (u) = GNN (X IB , A IB ) , f \u00b5 \u03c6 (G IB ) , f \u03a3 \u03c6 (G IB ) = Pooling ({Z IB (u) , \u2200u \u2208 V }) ,(15)\nwhere the first K-dimension outputs encode \u00b5 and the remaining K-dimension outputs encode \u03a3 (we use a softplus transform for f \u03a3 \u03c6 (G IB ) to ensure the non-negativity). We treat r(Z IB ) as a fixed d-dimensional spherical Gaussian r(Z IB ) = N (Z IB |0, I) as in (Alemi et al. 2016).\n\nStep-3: Sample IB-Graph Representation.\n\nTo obtain Z IB , we can use the reparameterization trick (Kingma and Welling 2013) for gradients estimation:\nZ IB = f \u00b5 \u03c6 (G IB ) + f \u03a3 \u03c6 (G IB ) \u03b5,(16)\nwhere \u03b5 \u2208 N (0, I) is an independent Gaussian noise and denotes the element-wise product. By using the reparameterization trick, randomness is transferred to \u03b5, which does not affect the back-propagation. For the first term I(Z IB , Y ) in Eq. (8), q \u03b8 (Y |Z IB ) outputs the label distribution of learned graph G IB and we model it as a multi-layer perceptron classifier with parameters \u03b8. The multi-layer perceptron classifier takes Z IB as input and outputs the predicted label.\n\nTraining Objective. We can efficiently compute the upper bounds in Eq. (8) on the training data samples using the gradient descent based backpropagation techniques, as illustrated in Algorithm 1. The overall loss is:\nL = L CE (Z IB , Y ) + \u03b2D KL (p (Z IB |G) ||r (Z IB )) ,(17)\nwhere L CE is the cross-entropy loss and D KL (\u00b7||\u00b7) is the KL divergence. The variational approximation proposed above Algorithm 1: The overall process of VIB-GSL Input: Graph G = (X, A) with label Y ; Number of training epochs E; Output: IB-graph G IB , predicted label\u0176 1 Parameter initialization; 2 for e = 1, 2, \u00b7 \u00b7 \u00b7 , E do // Learn IB-Graph\n3 X IB \u2190 {X i M, i \u2208 |V |}; 4 A IB \u2190 u,v\u2208V {a u,v \u223c Ber(\u03c0 u,v )}; 5 G IB \u2190 (X IB , A IB ); // Learn distribution 6 Encode (f \u00b5 \u03c6 (G IB ), f \u03a3 \u03c6 (G IB )) by a GNN; // Sample graph representation 7 Reparameterize Z IB = f \u00b5 \u03c6 (G IB ) + f \u03a3 \u03c6 (G IB ) \u03b5; // Optimize 8 L = L CE (Z IB , Y )+\u03b2D KL (p (Z IB |G) ||r (Z IB )); 9\nUpdate model parameters to minimize L. 10 end facilitates the training stability effectively, as shown in Section 4.2. We also analyze the impact of compression coefficient \u03b2 on performance and learned structure in Section 4.2.\n\nProperty of VIB-GSL Different with traditional GNNs and graph structure learning methods (e.g., IDGL (Chen, Wu, and Zaki 2020), NeuralSparse (Zheng et al. 2020)), VIB-GSL is independent of the original graph structure since it learns a new graph structure. This property renders VIB-GSL extremely robust to noisy information and structure perturbations, which is verified in Section 4.2.\n\n\nComparison with multiple related methods.\n\nIn this subsection, we discuss the relationship between the proposed VIB-GSL and two related works using the IB principle for graph representation learning, i.e., GIB  and SIB . Remark that VIB-GSL follows the Markov Chain < (Y, G n ) \u2192 G \u2192 G IB >.\n\nVIB-GSL vs. GIB GIB  aims to learn robust node representations Z by the IB principle following the Markov Chain < (Y, G n ) \u2192 G \u2192 Z >. Specifically, GIB regularizes and controls the structure and feature information in the computation flow of latent representations layer by layer. Our VIB-GSL differs in that we aim to learn an optimal graph explicitly, which is more interpretable than denoising in the latent space. Besides, our VIB-GSL focuses on graph-level tasks while GIB focuses on node-level ones.\n\nVIB-GSL vs. SIB SIB  aims to recognise the critical subgraph G sub for input graph following the Markov Chain < (Y, G n ) \u2192 G \u2192 G sub >. Our VIB-GSL aims to learn a new graph structure and can be applied for non-graph structured data. Moreover, SIB directly estimates the mutual information between subgraph and graph by MINE (Belghazi et al. 2018) and uses a bi-level optimization scheme for the IB objective, leading to an unstable and inefficient training process. Our VIB-GSL is more stable to train with the tractable variational approximation, which is demonstrated by experiments in Figure 5.\n\n\nExperiments\n\nWe evaluate VIB-GSL 1 on two tasks: graph classification and graph denoising, to verify whether VIB-GSL can improve the effectiveness and robustness of graph representation learning. Then we analyze the impact of information compression quantitatively and qualitatively.\n\n\nExperimental Setups\n\nDatasets. We empirically perform experiments on VIB-GSL on four widely-used social datasets including IMDB-B, IMDB-M, REDDIT-B, and COLLAB (Rossi and Ahmed 2015). We choose the social datasets for evaluation because much noisy information may exist in social interactions.\n\nBaselines. We compare the proposed VIB-GSL with a number of graph-level structure learning baselines, including NeuralSparse (Zheng et al. 2020), SIB  and IDGL (Chen, Wu, and Zaki 2020), to demonstrate the effectiveness and robustness of VIB-GSL. We do not include GIB in our baselines since it focuses on node-level representation learning. Similar with SIB , we plug various GNN backbones 2 into VIB-GSL including GCN (Kipf and Welling 2016), GAT (Veli\u010dkovi\u0107 et al. 2017), GIN (Xu et al. 2019) to see whether the VIB-GSL can boost the performance of graph classification or not. For a fair comparison, we use the mean pooling operation to obtain the graph representation and use a 2-layer perceptron as the graph classifier for all baselines. Parameter Settings. We set both the information bottleneck size K and the embedding dimension of baseline methods as 16. For VIB-GSL, we set t = 0.1 in Eq. (13), a 0 = 0.1 and perform hyperparameter search of \u03b2 \u2208 {10 \u22121 , 10 \u22122 , 10 \u22123 , 10 \u22124 , 10 \u22125 , 10 \u22126 } for each dataset.\n\n\nResults and Analysis\n\nGraph Classification. We first examine VIB-GSL's capability of improving graph classification. We perform 10fold cross-validation and report the average accuracy and the standard deviation across the 10 folds in Table 1, where \u2206 denotes the performance improvement for specific backbone and \"-\" indicates that there is no performance improvement for backbones without structure learner. The best results in each backbone group are underlined and the best results of each dataset are shown in bold. As shown in Table 1, the proposed VIB-GSL consistently outperforms all baselines on all datasets by a large margin. Generally, the graph sparsification models (i.e., NeuralSparse and SIB) show only a small improvement in accuracy and even have a negative impact on performance (e.g., on COLLAB), which is because they are constrained by the observed structures without mining underlying relations. The performance superiority of VIB-GSL over different GNN backbones implies that Table 1: Summary of graph classification results: \"average accuracy \u00b1 standard deviation\" and \"improvements\" (%). Underlined: best performance of specific backbones, bold: best results of each dataset.  VIB-GSL can learn better graph structure to improve the representation quality.\n\nGraph Denoise. To evaluate the robustness of VIB-GSL, we generate a synthetics dataset by deleting or adding edges on REDDIT-B. Specifically, for each graph in the dataset, we randomly remove (if edges exist) or add (if no such edges) 25%, 50%, 75% edges. The reported results are the mean accuracy (solid lines) and standard deviation (shaded region) over 5 runs. As shown in Figure 3, the classification accuracy of GCN dropped by 5% with 25% missing edges and dropped by 10% with 25% noisy edges, indicating that GNNs are indeed sensitive to structure noise. Since the proposed VIB-GSL does not depend on the original graph structure, it achieves better results without performance degradation. IDGL is still sensitive to structure noise since it iteratively updates graph structure based on node embeddings, which is tightly dependent on the observed structure.\n\nParameter Sensitivity: Trade Off between Prediction and Compression. We explore the influence of the Lagrangian multiplier \u03b2 trading off prediction and compression in Eq.\n\n(3) and Eq. (8). Note that there is a relationship between increasing \u03b2 and decreasing K (Shamir, Sabato, and 0LVVLQJHGJHV $FFXUDF\\ 9,%*6/ ,'*/ *&1 $GGLQJHGJHV $FFXUDF\\ 9,%*6/ ,'*/ *&1 Figure 3: Test accuracy (\u00b1 standard deviation) in percent for the edge attack scenarios on REDDIT-B.\n\nTishby 2010), and the following analysis is with K = 16. Figure 2 depicts the changing trend of graph classification accuracy on IMDB-B and REDDIT-B. Based on the results, we make the following observations: (1) Remarkably, the graph classification accuracies of VIB-GSL variation across different \u03b2 collapsed onto a hunchback shape on both datasets. The accuracy first increases with the increase of \u03b2, indicating that removing irrelevant information indeed enhances the graph representation learning. Then the accuracy progressively decreases and reaches very low values, indicating that excessive information compression will lose effective information.\n\n(2) Appropriate value of \u03b2 can greatly increase the model's performance. VIB-GSL achieves the best balance of prediction and compression with \u03b2 = 10 \u22123 and \u03b2 = 10 \u22125 on IMDB-B and REDDIT-B, respectively. This indicates that different dataset consists of different percent of task-irrelevant information and hence needs a different degree of information compression.\n\nGraph Visualization. To examine the graph structure changes brought by VIB-GSL intuitively, we present two samples from the IMDB-B dataset and visualize the origi- Figure 4: Original graph and IB-Graphs with different \u03b2 when VIB-GSL achieves the same testing performance.\n\nnal graph and IB-Graphs learned by VIB-GSL in Figure 4, where |E| indicates the number of edges. To further analyze the impact of information compression degree, we visualize the learned IB-Graph with different \u03b2 when VIB-GSL achieves the same testing performance. Note that VIB-GSL does not set sparsity constraint as in most structure learning methods. As shown in Figure 4, we make the following observations: (1)VIB-GSL tends to generate edges that connect nodes playing the same structure roles, which is consistent with the homophily assumption.\n\n(2)When achieving the same testing performance, VIB-GSL with larger \u03b2 will generate a more dense graph structure. It is because with the degree of information compression increasing, the nodes need more neighbors to obtain enough information.\n\nTraining Stability. As mentioned in Section 3.3, VIB-GSL deduces a tractable variational approximation for the IB objective, which facilitates the training stability. In this subsection, we analyze the convergence of VIB-GSL and SIB  on REDDIT-B with a learning rate of 0.001. The IB objective in  is L = L CE + \u03b2L MI + \u03b1L con , where L CE is the cross-entropy loss, L MI is the MINE loss of estimating mutual information between original graph and learned subgraph and L con is a connectivity regularizer. Figure 5(a) depicts the losses of VIB-GSL (i.e., overall loss L, cross-entropy loss L CE for classification, and the KL-divergence loss D KL ) with \u03b2 = 10 \u22123 , where the dash lines indicates the mean value in the last 10 epochs when VIB-GSL converges. As mentioned in Section 3.3, SIB adopted a bi-level optimization scheme for IB objective. Figure 5(b) depicts the losses of SIB (i.e., overall loss L, classification loss L CE , the MI estimation loss L MI , and the connectivity loss L con ) with \u03b2 = 0.2 and \u03b1 = 5 as suggested in its source code. As shown in Figure 5(a), VIB-GSL converge steadily, showing the effectiveness of the variational approximation. As shown in Figure 5 \n\n\nConclusion\n\nIn this paper, we advance the Information Bottleneck principle for graph structure learning and propose a framework named VIB-GSL, which jointly optimizes the graph structure and graph representations. VIB-GSL deduces a variational approximation to form a tractable IB objective function that facilitates training stability and efficiency. We evaluate the proposed VIB-GSL in graph classification and graph denoising. Experimental results verify the superior effectiveness and robustness of VIB-GSL.\n\n\nTechnical Appendix A. Proofs\n\nProof of Lemma 1\n\nWe first provide the proof of Lemma 1 in Section 3.1.\n\nProof. We prove the Lemma 1 following the same strategy of Proposition 3.1 in (Achille and Soatto 2018). Suppose G is defined by Y and G n , and G IB depends on G n only through G. We can define the Markov Chain < (Y, G n ) \u2192 G \u2192 G IB >. According to the data processing inequality (DPI), we have:\nI(G IB ; G) \u2265 I(G IB ; Y, G n ) = I(G IB ; G n ) + I(G IB ; Y |G n ) = I(G IB ; G n ) + H(Y |G n ) \u2212 H(Y |G n ; G IB ). (18) Since G n is be a task-irrelevant nuisance, it is independent with Y , we have H(Y |G n ) = H(Y ) and H(Y |G n ; G IB ) \u2264 H(Y |G IB ). Then I(G IB ; G) \u2265 I(G IB ; G n ) + H(Y |G n ) \u2212 H(Y |G n ; G IB ) \u2265 I(G IB ; G n ) + H(Y ) \u2212 H(Y |G IB ) = I(G IB ; G n ) + I(G IB ; Y ).(19)\nThus we obtain I(G IB ; G n ) \u2264 I(G IB ; G) \u2212 I(G IB ; Y ).\n\n\nProof of Proposition 1\n\nThen we provide the proof of Proposition 1 in Section 3.1.\n\nProof. According to the definition of mutual information,\n\u2212I(Y, G IB ) = \u2212 p(Y, G IB ) log p(Y, G IB ) p(Y )p(G IB ) dY dG IB = \u2212 p(Y, G IB ) log p(Y |G IB ) p(Y ) dY dG IB ,(20)\nwhere p(Y |G IB ) can be fully defined by the Markov Chain < (Y, G n ) \u2192 G \u2192 G IB > as p(Y |G IB ) = p(Y |G IB )p(G IB |G)dG. Since p(Y |G IB ) is intractable, let q \u03b8 (Y |G IB ) be the variational approximation of the true posterior p(Y |G IB ). According to the non-negativity of Kullback Leiber divergence:\nD KL (p(Y |G IB )||q \u03b8 (Y |G IB )) \u2265 0 =\u21d2 p(Y |G IB ) log p(Y |G IB )dY \u2265 p(Y |G IB ) log q \u03b8 (Y |G IB )dY.(21)\nPlug Eq. (20) into Eq. (21), then we have\n\u2212I(Y, G IB ) \u2264 \u2212 p(Y, G IB ) log q \u03b8 (Y |G IB ) p(Y ) dY dG IB = \u2212 p(Y, G IB ) log q \u03b8 (Y |G IB )dY dG IB + H(Y ),(22)\nwhere H(Y ) is the entropy of label Y , which can be ignored in optimization procedure.\n\n\nProof of Proposition 2\n\nWe next provide the proof of Proposition 2 in Section 3.1.\n\nProof. According to the definition of mutual information, I(G IB , G) = p(G IB , G) log p(G IB |G) p(G IB ) dG IB dG. (23) In general, computing the distribution p(G IB ) = p(G IB |G)p(G)dG is very difficult, so we use r(G IB ) as the variational approximation to p(G IB ). Since the Kullback Leiber divergence D KL (p(Z)||r(Z)) \u2265 0, D KL (p(Z)||r(Z)) \u2265 0 =\u21d2 p(z) log p(z)dz \u2265 p(z) log r(z)dz.\n\nPlug Eq. (23) into Eq. (24), then we have\nI(G IB , G) \u2264 p(G IB , G) log p(G IB |G) r(G IB ) dG IB dG. (25)\n\nB. Training Efficiency\n\nFor VIB-GSL, the cost of learning an IB-Graph is O(nd + n 2 d) for a graph with n nodes in R d , while computing graph representation costs O(n 2 d + ndK), where d is the node feature dimension and K is the bottleneck size. If we assume that d \u2248 K and d n, the overall time complexity is O(Kn 2 ).\n\nWe compare the training efficiency of VIB-GSL with other baselines and show the mean training time of one epoch in seconds (10 runs) in Figure 6. For Subgraph-IB, we set the inner loop iterations as 10. For IDGL, we set the maximal number of iterations in the dynamic stopping strategy to 10 as suggested in its source code. As shown in Figure 6, VIB-GSL shows comparable efficiency with other methods when achieving the best performance. \n\nFigure 1 :\n1Overview of VIB-GSL. Given G as input, VIB-GSL consists of the following three steps: (1) Generate IB-Graph: the IB-Graph generator learns an IB-Graph G IB by masking irrelevant features and learning a new structure; (2) Learn distribution of IB-Graph representation: the GNN module learns the distribution of IB-Graph representation Z IB ; (3) Sample IB-Graph representation: Z IB is sampled from the learned distribution by a reparameterization trick for classification.\n\nFigure 2 :\n2Impact of \u03b2 on IMDB-B and REDDIT-B.\n\n\n(b), the MI estimation loss L MI is very unstable because of the bi-level optimization scheme, making SIB is\n\nFigure 5 :\n5Training dynamics of VIB-GSL and SIB. very difficult to converge.\n\nFigure 6 :\n6Training time of one epoch on various datasets.\nCode is available at https://github.com/RingBDStack/VIB-GSL.2  We follow the protocol in https://github.com/rusty1s/pytorch geometric/tree/master/benchmark/kernel.\nAcknowledgmentsThe corresponding author is Jianxin Li. The authors of this paper are supported by the (No.U20B2053 and 61872022), State Key Laboratory of Software Development Environment (SKLSDE-2020ZX-12), Outstanding Research Project of Shen Yuan Honors College, BUAA, ( 230121208), the ARC DECRA Project (No. DE200100964), and in part by NSF under grants III-1763325, III-1909323, III-2106758,  and SaTC-1930941.\nEmergence of invariance and disentanglement in deep representations. A Achille, S Soatto, The Journal of Machine Learning Research. 191Achille, A.; and Soatto, S. 2018. Emergence of invariance and disentanglement in deep representations. The Journal of Machine Learning Research, 19(1): 1947-1980.\n\nDeep variational information bottleneck. A A Alemi, I Fischer, J V Dillon, K Murphy, ICLR. Alemi, A. A.; Fischer, I.; Dillon, J. V.; and Murphy, K. 2016. Deep variational information bottleneck. In ICLR.\n\nF Bao, arXiv:2105.07599Disentangled Variational Information Bottleneck for Multiview Representation Learning. arXiv preprintBao, F. 2021. Disentangled Variational Information Bottle- neck for Multiview Representation Learning. arXiv preprint arXiv:2105.07599.\n\nMutual information neural estimation. M I Belghazi, A Baratin, S Rajeshwar, S Ozair, Y Bengio, A Courville, D Hjelm, ICML. Belghazi, M. I.; Baratin, A.; Rajeshwar, S.; Ozair, S.; Ben- gio, Y.; Courville, A.; and Hjelm, D. 2018. Mutual informa- tion neural estimation. In ICML, 531-540.\n\nIterative deep graph learning for graph neural networks: Better and robust node embeddings. Y Chen, L Wu, M Zaki, NeurIPS. Chen, Y.; Wu, L.; and Zaki, M. 2020. Iterative deep graph learning for graph neural networks: Better and robust node embeddings. In NeurIPS.\n\nElements of information theory. T M Cover, John Wiley & SonsCover, T. M. 1999. Elements of information theory. John Wiley & Sons.\n\nLearning optimal representations with the decodable information bottleneck. Y Dubois, D Kiela, D J Schwab, R Vedantam, NeurIPS. Dubois, Y.; Kiela, D.; Schwab, D. J.; and Vedantam, R. 2020. Learning optimal representations with the decodable information bottleneck. In NeurIPS.\n\nLearning discrete structures for graph neural networks. L Franceschi, M Niepert, M Pontil, X He, ICML. Franceschi, L.; Niepert, M.; Pontil, M.; and He, X. 2019. Learning discrete structures for graph neural networks. In ICML, 1972-1982.\n\nRoom-and-object aware knowledge reasoning for remote embodied referring expression. C Gao, J Chen, S Liu, L Wang, Q Zhang, Q Wu, CVPR. Gao, C.; Chen, J.; Liu, S.; Wang, L.; Zhang, Q.; and Wu, Q. 2021. Room-and-object aware knowledge reasoning for re- mote embodied referring expression. In CVPR, 3064-3073.\n\nApproximating the Kullback Leibler divergence between Gaussian mixture models. J R Hershey, P A Olsen, ICASSP. 317Hershey, J. R.; and Olsen, P. A. 2007. Approximating the Kullback Leibler divergence between Gaussian mixture models. In ICASSP, IV-317.\n\nCategorical reparameterization with gumbel-softmax. E Jang, S Gu, B Poole, ICLR. Jang, E.; Gu, S.; and Poole, B. 2017. Categorical reparame- terization with gumbel-softmax. In ICLR.\n\nIB-GAN: Disengangled Representation Learning with Information Bottleneck Generative Adversarial Networks. I Jeon, W Lee, M Pyeon, G Kim, AAAI. Jeon, I.; Lee, W.; Pyeon, M.; and Kim, G. 2021. IB- GAN: Disengangled Representation Learning with Informa- tion Bottleneck Generative Adversarial Networks. In AAAI, 7926-7934.\n\nDrop-Bottleneck: Learning Discrete Compressed Representation for Noise-Robust Exploration. J Kim, M Kim, D Woo, G Kim, ICLR. Kim, J.; Kim, M.; Woo, D.; and Kim, G. 2021. Drop- Bottleneck: Learning Discrete Compressed Representation for Noise-Robust Exploration. In ICLR.\n\nAuto-encoding variational bayes. D P Kingma, M Welling, ICLR. Kingma, D. P.; and Welling, M. 2013. Auto-encoding varia- tional bayes. In ICLR.\n\nSemi-Supervised Classification with Graph Convolutional Networks. T N Kipf, M Welling, ICLR. Kipf, T. N.; and Welling, M. 2016. Semi-Supervised Classi- fication with Graph Convolutional Networks. In ICLR.\n\nA survey on text classification: From shallow to deep learning. Q Li, H Peng, J Li, C Xia, R Yang, L Sun, P S Yu, L He, ACM Transactions on Intelligent Systems and Technology. Li, Q.; Peng, H.; Li, J.; Xia, C.; Yang, R.; Sun, L.; Yu, P. S.; and He, L. 2020. A survey on text classification: From shal- low to deep learning. ACM Transactions on Intelligent Sys- tems and Technology.\n\nVariational Information Bottleneck for Effective Low-Resource Fine-Tuning. R K Mahabadi, Y Belinkov, J Henderson, L Niu, J Zhang, L Zhang, arXiv:2012.07372Disentangled Information Bottleneck. arXiv preprintMahabadi, R. K.; Belinkov, Y.; and Henderson, J. 2021. Variational Information Bottleneck for Effective Low- Resource Fine-Tuning. In ICLR. Pan, Z.; Niu, L.; Zhang, J.; and Zhang, L. 2020. Disentangled Information Bottleneck. arXiv preprint arXiv:2012.07372.\n\nReinforced Neighborhood Selection Guided Multi-Relational Graph Neural Networks. H Peng, R Zhang, Y Dou, R Yang, J Zhang, P S Yu, ACM Transactions on Information Systems. Peng, H.; Zhang, R.; Dou, Y.; Yang, R.; Zhang, J.; and Yu, P. S. 2021. Reinforced Neighborhood Selection Guided Multi-Relational Graph Neural Networks. ACM Transac- tions on Information Systems.\n\nThe network data repository with interactive graph analytics and visualization. R Rossi, Ahmed , N , AAAI. Rossi, R.; and Ahmed, N. 2015. The network data repository with interactive graph analytics and visualization. In AAAI, 4292-4293.\n\nOn the information bottleneck theory of deep learning. A M Saxe, Y Bansal, J Dapello, M Advani, A Kolchinsky, B D Tracey, D D Cox, Journal of Statistical Mechanics: Theory and Experiment. 201912124020Saxe, A. M.; Bansal, Y.; Dapello, J.; Advani, M.; Kolchin- sky, A.; Tracey, B. D.; and Cox, D. D. 2019. On the informa- tion bottleneck theory of deep learning. Journal of Statisti- cal Mechanics: Theory and Experiment, 2019(12): 124020.\n\nLearning and generalization with the information bottleneck. O Shamir, S Sabato, N Tishby, Theoretical Computer Science. Shamir, O.; Sabato, S.; and Tishby, N. 2010. Learning and generalization with the information bottleneck. Theoretical Computer Science, 411(29-30): 2696-2711.\n\nMonte Carlo sampling methods. Handbooks in operations research and management science. A Shapiro, 10Shapiro, A. 2003. Monte Carlo sampling methods. Hand- books in operations research and management science, 10: 353-425.\n\nS Sinha, H Bharadhwaj, A Goyal, H Larochelle, A Garg, F Shkurti, Diversity inducing Information Bottleneck in Model Ensembles. In AAAI. Sinha, S.; Bharadhwaj, H.; Goyal, A.; Larochelle, H.; Garg, A.; and Shkurti, F. 2020. Diversity inducing Information Bottleneck in Model Ensembles. In AAAI, 9666-9674.\n\nL Sun, Y Dou, C Yang, J Wang, P S Yu, L He, B Li, arXiv:1812.10528Adversarial attack and defense on graph data: A survey. arXiv preprintSun, L.; Dou, Y.; Yang, C.; Wang, J.; Yu, P. S.; He, L.; and Li, B. 2018. Adversarial attack and defense on graph data: A survey. arXiv preprint arXiv:1812.10528.\n\nSUGAR: Subgraph neural network with reinforcement pooling and self-supervised mutual information mechanism. Q Sun, J Li, H Peng, J Wu, Y Ning, P S Yu, L He, Web Conference. Sun, Q.; Li, J.; Peng, H.; Wu, J.; Ning, Y.; Yu, P. S.; and He, L. 2021. SUGAR: Subgraph neural network with rein- forcement pooling and self-supervised mutual information mechanism. In Web Conference, 2081-2091.\n\nWhat makes for good views for contrastive learning?. Y Tian, C Sun, B Poole, D Krishnan, C Schmid, P Isola, NeurIPS. Tian, Y.; Sun, C.; Poole, B.; Krishnan, D.; Schmid, C.; and Isola, P. 2020. What makes for good views for contrastive learning? In NeurIPS.\n\nThe information bottleneck method. N Tishby, F C Pereira, W Bialek, physics/0004057arXiv preprintTishby, N.; Pereira, F. C.; and Bialek, W. 2000. The infor- mation bottleneck method. arXiv preprint physics/0004057.\n\nDirected Graph Contrastive Learning. Z Tong, Y Liang, H Ding, Y Dai, X Li, C Wang, Advances in Neural Information Processing Systems. 34Tong, Z.; Liang, Y.; Ding, H.; Dai, Y.; Li, X.; and Wang, C. 2021. Directed Graph Contrastive Learning. Advances in Neural Information Processing Systems, 34.\n\nGraph Attention Networks. P Veli\u010dkovi\u0107, G Cucurull, A Casanova, A Romero, P Lio, Y Bengio, ICLR. Veli\u010dkovi\u0107, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio, P.; and Bengio, Y. 2017. Graph Attention Networks. In ICLR.\n\nGraph information bottleneck. T Wu, H Ren, P Li, J Leskovec, In NeurIPSWu, T.; Ren, H.; Li, P.; and Leskovec, J. 2020. Graph infor- mation bottleneck. In NeurIPS.\n\nHow Powerful are Graph Neural Networks? In ICLR. K Xu, W Hu, J Leskovec, S Jegelka, Xu, K.; Hu, W.; Leskovec, J.; and Jegelka, S. 2019. How Powerful are Graph Neural Networks? In ICLR.\n\nHeterogeneous Graph Information Bottleneck. L Yang, F Wu, Z Zheng, B Niu, J Gu, C Wang, X Cao, Y Guo, IJCAI. Yang, L.; Wu, F.; Zheng, Z.; Niu, B.; Gu, J.; Wang, C.; Cao, X.; and Guo, Y. 2021. Heterogeneous Graph Information Bottleneck. In IJCAI, 1638-1645.\n\n. J Yu, T Xu, Y Rong, Y Bian, J Huang, R He, Yu, J.; Xu, T.; Rong, Y.; Bian, Y.; Huang, J.; and He, R.\n\nGraph Information Bottleneck for Subgraph Recognition. ICLR. Graph Information Bottleneck for Subgraph Recogni- tion. In ICLR.\n\nRecognizing Predictive Substructures with Subgraph Information Bottleneck. J Yu, T Xu, Y Rong, Y Bian, J Huang, R He, IEEE Transactions on Pattern Analysis and Machine Intelligence. Yu, J.; Xu, T.; Rong, Y.; Bian, Y.; Huang, J.; and He, R. 2021. Recognizing Predictive Substructures with Sub- graph Information Bottleneck. IEEE Transactions on Pat- tern Analysis and Machine Intelligence.\n\nNetwork representation learning: A survey. D Zhang, J Yin, X Zhu, C Zhang, IEEE transactions on Big Data. 61Zhang, D.; Yin, J.; Zhu, X.; and Zhang, C. 2018. Network representation learning: A survey. IEEE transactions on Big Data, 6(1): 3-28.\n\nRobust graph representation learning via neural sparsification. C Zheng, B Zong, W Cheng, D Song, J Ni, W Yu, H Chen, W Wang, ICML. Zheng, C.; Zong, B.; Cheng, W.; Song, D.; Ni, J.; Yu, W.; Chen, H.; and Wang, W. 2020. Robust graph representation learning via neural sparsification. In ICML, 11458-11468.\n\nInformation-bottleneck approach to salient region discovery. A Zhmoginov, I Fischer, M Sandler, ECML/PKDD. Zhmoginov, A.; Fischer, I.; and Sandler, M. 2020. Information-bottleneck approach to salient region discovery. In ECML/PKDD, 531-546.\n\nJ Zhou, G Cui, S Hu, Z Zhang, C Yang, Z Liu, L Wang, C Li, M Sun, Graph neural networks: A review of methods and applications. AI Open. 1Zhou, J.; Cui, G.; Hu, S.; Zhang, Z.; Yang, C.; Liu, Z.; Wang, L.; Li, C.; and Sun, M. 2020. Graph neural networks: A review of methods and applications. AI Open, 1: 57-81.\n\n. Y Zhu, W Xu, J Zhang, Q Liu, S Wu, L Wang, Zhu, Y.; Xu, W.; Zhang, J.; Liu, Q.; Wu, S.; and Wang, L.\n\nAdversarial attacks on neural networks for graph data. D Z\u00fcgner, A Akbarnejad, S G\u00fcnnemann, arXiv:2103.03036Deep Graph Structure Learning for Robust Representations: A Survey. arXiv preprintACM SIGKDDDeep Graph Structure Learning for Robust Represen- tations: A Survey. arXiv preprint arXiv:2103.03036. Z\u00fcgner, D.; Akbarnejad, A.; and G\u00fcnnemann, S. 2018. Ad- versarial attacks on neural networks for graph data. In ACM SIGKDD, 2847-2856.\n", "annotations": {"author": "[{\"end\":80,\"start\":68},{\"end\":92,\"start\":81},{\"end\":209,\"start\":93},{\"end\":292,\"start\":210},{\"end\":413,\"start\":293},{\"end\":447,\"start\":414},{\"end\":657,\"start\":448},{\"end\":740,\"start\":658},{\"end\":804,\"start\":741}]", "publisher": null, "author_last_name": "[{\"end\":79,\"start\":76},{\"end\":91,\"start\":89},{\"end\":101,\"start\":97},{\"end\":216,\"start\":214},{\"end\":305,\"start\":303},{\"end\":422,\"start\":417},{\"end\":459,\"start\":457}]", "author_first_name": "[{\"end\":75,\"start\":68},{\"end\":88,\"start\":81},{\"end\":96,\"start\":93},{\"end\":213,\"start\":210},{\"end\":302,\"start\":293},{\"end\":416,\"start\":414},{\"end\":454,\"start\":448},{\"end\":456,\"start\":455}]", "author_affiliation": "[{\"end\":208,\"start\":103},{\"end\":291,\"start\":235},{\"end\":412,\"start\":307},{\"end\":579,\"start\":474},{\"end\":656,\"start\":581},{\"end\":739,\"start\":659},{\"end\":803,\"start\":742}]", "title": "[{\"end\":65,\"start\":1},{\"end\":869,\"start\":805}]", "venue": null, "abstract": "[{\"end\":2013,\"start\":871}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2145,\"start\":2126},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2162,\"start\":2145},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2270,\"start\":2252},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2937,\"start\":2912},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3055,\"start\":3015},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3070,\"start\":3055},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3329,\"start\":3312},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3346,\"start\":3329},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3461,\"start\":3444},{\"end\":3929,\"start\":3912},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3952,\"start\":3929},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3976,\"start\":3952},{\"end\":4645,\"start\":4624},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5210,\"start\":5193},{\"end\":5226,\"start\":5210},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5235,\"start\":5226},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5254,\"start\":5235},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5330,\"start\":5311},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5384,\"start\":5344},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5449,\"start\":5411},{\"end\":7182,\"start\":7165},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7912,\"start\":7878},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8119,\"start\":8107},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8443,\"start\":8409},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8867,\"start\":8849},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9277,\"start\":9258},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9294,\"start\":9277},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9370,\"start\":9353},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9385,\"start\":9370},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13853,\"start\":13835},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14032,\"start\":14019},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15080,\"start\":15055},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16127,\"start\":16101},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16999,\"start\":16975},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17829,\"start\":17811},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19812,\"start\":19787},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":19846,\"start\":19827},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21224,\"start\":21203},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21947,\"start\":21925},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22204,\"start\":22185},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22245,\"start\":22220},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22503,\"start\":22480},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22533,\"start\":22509},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22554,\"start\":22539},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":29704,\"start\":29679}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33135,\"start\":32650},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33184,\"start\":33136},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33295,\"start\":33185},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33374,\"start\":33296},{\"attributes\":{\"id\":\"fig_4\"},\"end\":33435,\"start\":33375}]", "paragraph": "[{\"end\":3825,\"start\":2029},{\"end\":4437,\"start\":3827},{\"end\":5450,\"start\":4439},{\"end\":6486,\"start\":5452},{\"end\":7073,\"start\":6488},{\"end\":7111,\"start\":7075},{\"end\":7824,\"start\":7140},{\"end\":8551,\"start\":7851},{\"end\":8722,\"start\":8553},{\"end\":8838,\"start\":8762},{\"end\":8982,\"start\":8840},{\"end\":9166,\"start\":9054},{\"end\":9840,\"start\":9168},{\"end\":9910,\"start\":9886},{\"end\":10380,\"start\":9912},{\"end\":10586,\"start\":10413},{\"end\":10749,\"start\":10588},{\"end\":10907,\"start\":10802},{\"end\":11576,\"start\":10909},{\"end\":11777,\"start\":11578},{\"end\":12092,\"start\":11825},{\"end\":12538,\"start\":12094},{\"end\":13019,\"start\":12819},{\"end\":13123,\"start\":13021},{\"end\":13279,\"start\":13187},{\"end\":13323,\"start\":13281},{\"end\":13398,\"start\":13325},{\"end\":13563,\"start\":13524},{\"end\":13873,\"start\":13565},{\"end\":14047,\"start\":13933},{\"end\":14299,\"start\":14241},{\"end\":14748,\"start\":14301},{\"end\":15138,\"start\":14792},{\"end\":15404,\"start\":15170},{\"end\":15834,\"start\":15447},{\"end\":16170,\"start\":15891},{\"end\":16459,\"start\":16233},{\"end\":16720,\"start\":16461},{\"end\":17000,\"start\":16722},{\"end\":17196,\"start\":17082},{\"end\":17329,\"start\":17198},{\"end\":17831,\"start\":17547},{\"end\":17872,\"start\":17833},{\"end\":17982,\"start\":17874},{\"end\":18508,\"start\":18027},{\"end\":18726,\"start\":18510},{\"end\":19135,\"start\":18788},{\"end\":19684,\"start\":19457},{\"end\":20073,\"start\":19686},{\"end\":20367,\"start\":20119},{\"end\":20875,\"start\":20369},{\"end\":21476,\"start\":20877},{\"end\":21762,\"start\":21492},{\"end\":22058,\"start\":21786},{\"end\":23084,\"start\":22060},{\"end\":24368,\"start\":23109},{\"end\":25235,\"start\":24370},{\"end\":25407,\"start\":25237},{\"end\":25694,\"start\":25409},{\"end\":26352,\"start\":25696},{\"end\":26719,\"start\":26354},{\"end\":26992,\"start\":26721},{\"end\":27545,\"start\":26994},{\"end\":27789,\"start\":27547},{\"end\":28981,\"start\":27791},{\"end\":29495,\"start\":28996},{\"end\":29544,\"start\":29528},{\"end\":29599,\"start\":29546},{\"end\":29898,\"start\":29601},{\"end\":30361,\"start\":30302},{\"end\":30446,\"start\":30388},{\"end\":30505,\"start\":30448},{\"end\":30936,\"start\":30627},{\"end\":31090,\"start\":31049},{\"end\":31297,\"start\":31210},{\"end\":31382,\"start\":31324},{\"end\":31777,\"start\":31384},{\"end\":31820,\"start\":31779},{\"end\":32208,\"start\":31911},{\"end\":32649,\"start\":32210}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8761,\"start\":8723},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9053,\"start\":8983},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10801,\"start\":10750},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11824,\"start\":11778},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12818,\"start\":12539},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13186,\"start\":13124},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13523,\"start\":13399},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13932,\"start\":13874},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14240,\"start\":14048},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14791,\"start\":14749},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15169,\"start\":15139},{\"attributes\":{\"id\":\"formula_11\"},\"end\":15446,\"start\":15405},{\"attributes\":{\"id\":\"formula_12\"},\"end\":15890,\"start\":15835},{\"attributes\":{\"id\":\"formula_13\"},\"end\":16232,\"start\":16171},{\"attributes\":{\"id\":\"formula_14\"},\"end\":17081,\"start\":17001},{\"attributes\":{\"id\":\"formula_15\"},\"end\":17546,\"start\":17330},{\"attributes\":{\"id\":\"formula_16\"},\"end\":18026,\"start\":17983},{\"attributes\":{\"id\":\"formula_17\"},\"end\":18787,\"start\":18727},{\"attributes\":{\"id\":\"formula_18\"},\"end\":19456,\"start\":19136},{\"attributes\":{\"id\":\"formula_19\"},\"end\":30301,\"start\":29899},{\"attributes\":{\"id\":\"formula_20\"},\"end\":30626,\"start\":30506},{\"attributes\":{\"id\":\"formula_21\"},\"end\":31048,\"start\":30937},{\"attributes\":{\"id\":\"formula_22\"},\"end\":31209,\"start\":31091},{\"attributes\":{\"id\":\"formula_24\"},\"end\":31885,\"start\":31821}]", "table_ref": "[{\"end\":23328,\"start\":23321},{\"end\":23626,\"start\":23619},{\"end\":24093,\"start\":24086}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2027,\"start\":2015},{\"attributes\":{\"n\":\"2.1\"},\"end\":7138,\"start\":7114},{\"attributes\":{\"n\":\"2.2\"},\"end\":7849,\"start\":7827},{\"attributes\":{\"n\":\"3\"},\"end\":9884,\"start\":9843},{\"attributes\":{\"n\":\"3.1\"},\"end\":10411,\"start\":10383},{\"attributes\":{\"n\":\"3.3\"},\"end\":20117,\"start\":20076},{\"attributes\":{\"n\":\"4\"},\"end\":21490,\"start\":21479},{\"attributes\":{\"n\":\"4.1\"},\"end\":21784,\"start\":21765},{\"attributes\":{\"n\":\"4.2\"},\"end\":23107,\"start\":23087},{\"attributes\":{\"n\":\"5\"},\"end\":28994,\"start\":28984},{\"end\":29526,\"start\":29498},{\"end\":30386,\"start\":30364},{\"end\":31322,\"start\":31300},{\"end\":31909,\"start\":31887},{\"end\":32661,\"start\":32651},{\"end\":33147,\"start\":33137},{\"end\":33307,\"start\":33297},{\"end\":33386,\"start\":33376}]", "table": null, "figure_caption": "[{\"end\":33135,\"start\":32663},{\"end\":33184,\"start\":33149},{\"end\":33295,\"start\":33187},{\"end\":33374,\"start\":33309},{\"end\":33435,\"start\":33388}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14265,\"start\":14257},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21475,\"start\":21467},{\"end\":24755,\"start\":24747},{\"end\":25602,\"start\":25594},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25761,\"start\":25753},{\"end\":26893,\"start\":26885},{\"end\":27048,\"start\":27040},{\"end\":27369,\"start\":27361},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28306,\"start\":28298},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28648,\"start\":28640},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28868,\"start\":28860},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28980,\"start\":28972},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32354,\"start\":32346},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32555,\"start\":32547}]", "bib_author_first_name": "[{\"end\":34086,\"start\":34085},{\"end\":34097,\"start\":34096},{\"end\":34357,\"start\":34356},{\"end\":34359,\"start\":34358},{\"end\":34368,\"start\":34367},{\"end\":34379,\"start\":34378},{\"end\":34381,\"start\":34380},{\"end\":34391,\"start\":34390},{\"end\":34521,\"start\":34520},{\"end\":34820,\"start\":34819},{\"end\":34822,\"start\":34821},{\"end\":34834,\"start\":34833},{\"end\":34845,\"start\":34844},{\"end\":34858,\"start\":34857},{\"end\":34867,\"start\":34866},{\"end\":34877,\"start\":34876},{\"end\":34890,\"start\":34889},{\"end\":35161,\"start\":35160},{\"end\":35169,\"start\":35168},{\"end\":35175,\"start\":35174},{\"end\":35366,\"start\":35365},{\"end\":35368,\"start\":35367},{\"end\":35541,\"start\":35540},{\"end\":35551,\"start\":35550},{\"end\":35560,\"start\":35559},{\"end\":35562,\"start\":35561},{\"end\":35572,\"start\":35571},{\"end\":35799,\"start\":35798},{\"end\":35813,\"start\":35812},{\"end\":35824,\"start\":35823},{\"end\":35834,\"start\":35833},{\"end\":36065,\"start\":36064},{\"end\":36072,\"start\":36071},{\"end\":36080,\"start\":36079},{\"end\":36087,\"start\":36086},{\"end\":36095,\"start\":36094},{\"end\":36104,\"start\":36103},{\"end\":36368,\"start\":36367},{\"end\":36370,\"start\":36369},{\"end\":36381,\"start\":36380},{\"end\":36383,\"start\":36382},{\"end\":36593,\"start\":36592},{\"end\":36601,\"start\":36600},{\"end\":36607,\"start\":36606},{\"end\":36830,\"start\":36829},{\"end\":36838,\"start\":36837},{\"end\":36845,\"start\":36844},{\"end\":36854,\"start\":36853},{\"end\":37136,\"start\":37135},{\"end\":37143,\"start\":37142},{\"end\":37150,\"start\":37149},{\"end\":37157,\"start\":37156},{\"end\":37350,\"start\":37349},{\"end\":37352,\"start\":37351},{\"end\":37362,\"start\":37361},{\"end\":37527,\"start\":37526},{\"end\":37529,\"start\":37528},{\"end\":37537,\"start\":37536},{\"end\":37731,\"start\":37730},{\"end\":37737,\"start\":37736},{\"end\":37745,\"start\":37744},{\"end\":37751,\"start\":37750},{\"end\":37758,\"start\":37757},{\"end\":37766,\"start\":37765},{\"end\":37773,\"start\":37772},{\"end\":37775,\"start\":37774},{\"end\":37781,\"start\":37780},{\"end\":38125,\"start\":38124},{\"end\":38127,\"start\":38126},{\"end\":38139,\"start\":38138},{\"end\":38151,\"start\":38150},{\"end\":38164,\"start\":38163},{\"end\":38171,\"start\":38170},{\"end\":38180,\"start\":38179},{\"end\":38597,\"start\":38596},{\"end\":38605,\"start\":38604},{\"end\":38614,\"start\":38613},{\"end\":38621,\"start\":38620},{\"end\":38629,\"start\":38628},{\"end\":38638,\"start\":38637},{\"end\":38640,\"start\":38639},{\"end\":38963,\"start\":38962},{\"end\":38976,\"start\":38971},{\"end\":38980,\"start\":38979},{\"end\":39177,\"start\":39176},{\"end\":39179,\"start\":39178},{\"end\":39187,\"start\":39186},{\"end\":39197,\"start\":39196},{\"end\":39208,\"start\":39207},{\"end\":39218,\"start\":39217},{\"end\":39232,\"start\":39231},{\"end\":39234,\"start\":39233},{\"end\":39244,\"start\":39243},{\"end\":39246,\"start\":39245},{\"end\":39622,\"start\":39621},{\"end\":39632,\"start\":39631},{\"end\":39642,\"start\":39641},{\"end\":39929,\"start\":39928},{\"end\":40063,\"start\":40062},{\"end\":40072,\"start\":40071},{\"end\":40086,\"start\":40085},{\"end\":40095,\"start\":40094},{\"end\":40109,\"start\":40108},{\"end\":40117,\"start\":40116},{\"end\":40368,\"start\":40367},{\"end\":40375,\"start\":40374},{\"end\":40382,\"start\":40381},{\"end\":40390,\"start\":40389},{\"end\":40398,\"start\":40397},{\"end\":40400,\"start\":40399},{\"end\":40406,\"start\":40405},{\"end\":40412,\"start\":40411},{\"end\":40776,\"start\":40775},{\"end\":40783,\"start\":40782},{\"end\":40789,\"start\":40788},{\"end\":40797,\"start\":40796},{\"end\":40803,\"start\":40802},{\"end\":40811,\"start\":40810},{\"end\":40813,\"start\":40812},{\"end\":40819,\"start\":40818},{\"end\":41108,\"start\":41107},{\"end\":41116,\"start\":41115},{\"end\":41123,\"start\":41122},{\"end\":41132,\"start\":41131},{\"end\":41144,\"start\":41143},{\"end\":41154,\"start\":41153},{\"end\":41348,\"start\":41347},{\"end\":41358,\"start\":41357},{\"end\":41360,\"start\":41359},{\"end\":41371,\"start\":41370},{\"end\":41566,\"start\":41565},{\"end\":41574,\"start\":41573},{\"end\":41583,\"start\":41582},{\"end\":41591,\"start\":41590},{\"end\":41598,\"start\":41597},{\"end\":41604,\"start\":41603},{\"end\":41851,\"start\":41850},{\"end\":41865,\"start\":41864},{\"end\":41877,\"start\":41876},{\"end\":41889,\"start\":41888},{\"end\":41899,\"start\":41898},{\"end\":41906,\"start\":41905},{\"end\":42074,\"start\":42073},{\"end\":42080,\"start\":42079},{\"end\":42087,\"start\":42086},{\"end\":42093,\"start\":42092},{\"end\":42257,\"start\":42256},{\"end\":42263,\"start\":42262},{\"end\":42269,\"start\":42268},{\"end\":42281,\"start\":42280},{\"end\":42438,\"start\":42437},{\"end\":42446,\"start\":42445},{\"end\":42452,\"start\":42451},{\"end\":42461,\"start\":42460},{\"end\":42468,\"start\":42467},{\"end\":42474,\"start\":42473},{\"end\":42482,\"start\":42481},{\"end\":42489,\"start\":42488},{\"end\":42654,\"start\":42653},{\"end\":42660,\"start\":42659},{\"end\":42666,\"start\":42665},{\"end\":42674,\"start\":42673},{\"end\":42682,\"start\":42681},{\"end\":42691,\"start\":42690},{\"end\":42959,\"start\":42958},{\"end\":42965,\"start\":42964},{\"end\":42971,\"start\":42970},{\"end\":42979,\"start\":42978},{\"end\":42987,\"start\":42986},{\"end\":42996,\"start\":42995},{\"end\":43317,\"start\":43316},{\"end\":43326,\"start\":43325},{\"end\":43333,\"start\":43332},{\"end\":43340,\"start\":43339},{\"end\":43582,\"start\":43581},{\"end\":43591,\"start\":43590},{\"end\":43599,\"start\":43598},{\"end\":43608,\"start\":43607},{\"end\":43616,\"start\":43615},{\"end\":43622,\"start\":43621},{\"end\":43628,\"start\":43627},{\"end\":43636,\"start\":43635},{\"end\":43885,\"start\":43884},{\"end\":43898,\"start\":43897},{\"end\":43909,\"start\":43908},{\"end\":44066,\"start\":44065},{\"end\":44074,\"start\":44073},{\"end\":44081,\"start\":44080},{\"end\":44087,\"start\":44086},{\"end\":44096,\"start\":44095},{\"end\":44104,\"start\":44103},{\"end\":44111,\"start\":44110},{\"end\":44119,\"start\":44118},{\"end\":44125,\"start\":44124},{\"end\":44379,\"start\":44378},{\"end\":44386,\"start\":44385},{\"end\":44392,\"start\":44391},{\"end\":44401,\"start\":44400},{\"end\":44408,\"start\":44407},{\"end\":44414,\"start\":44413},{\"end\":44536,\"start\":44535},{\"end\":44546,\"start\":44545},{\"end\":44560,\"start\":44559}]", "bib_author_last_name": "[{\"end\":34094,\"start\":34087},{\"end\":34104,\"start\":34098},{\"end\":34365,\"start\":34360},{\"end\":34376,\"start\":34369},{\"end\":34388,\"start\":34382},{\"end\":34398,\"start\":34392},{\"end\":34525,\"start\":34522},{\"end\":34831,\"start\":34823},{\"end\":34842,\"start\":34835},{\"end\":34855,\"start\":34846},{\"end\":34864,\"start\":34859},{\"end\":34874,\"start\":34868},{\"end\":34887,\"start\":34878},{\"end\":34896,\"start\":34891},{\"end\":35166,\"start\":35162},{\"end\":35172,\"start\":35170},{\"end\":35180,\"start\":35176},{\"end\":35374,\"start\":35369},{\"end\":35548,\"start\":35542},{\"end\":35557,\"start\":35552},{\"end\":35569,\"start\":35563},{\"end\":35581,\"start\":35573},{\"end\":35810,\"start\":35800},{\"end\":35821,\"start\":35814},{\"end\":35831,\"start\":35825},{\"end\":35837,\"start\":35835},{\"end\":36069,\"start\":36066},{\"end\":36077,\"start\":36073},{\"end\":36084,\"start\":36081},{\"end\":36092,\"start\":36088},{\"end\":36101,\"start\":36096},{\"end\":36107,\"start\":36105},{\"end\":36378,\"start\":36371},{\"end\":36389,\"start\":36384},{\"end\":36598,\"start\":36594},{\"end\":36604,\"start\":36602},{\"end\":36613,\"start\":36608},{\"end\":36835,\"start\":36831},{\"end\":36842,\"start\":36839},{\"end\":36851,\"start\":36846},{\"end\":36858,\"start\":36855},{\"end\":37140,\"start\":37137},{\"end\":37147,\"start\":37144},{\"end\":37154,\"start\":37151},{\"end\":37161,\"start\":37158},{\"end\":37359,\"start\":37353},{\"end\":37370,\"start\":37363},{\"end\":37534,\"start\":37530},{\"end\":37545,\"start\":37538},{\"end\":37734,\"start\":37732},{\"end\":37742,\"start\":37738},{\"end\":37748,\"start\":37746},{\"end\":37755,\"start\":37752},{\"end\":37763,\"start\":37759},{\"end\":37770,\"start\":37767},{\"end\":37778,\"start\":37776},{\"end\":37784,\"start\":37782},{\"end\":38136,\"start\":38128},{\"end\":38148,\"start\":38140},{\"end\":38161,\"start\":38152},{\"end\":38168,\"start\":38165},{\"end\":38177,\"start\":38172},{\"end\":38186,\"start\":38181},{\"end\":38602,\"start\":38598},{\"end\":38611,\"start\":38606},{\"end\":38618,\"start\":38615},{\"end\":38626,\"start\":38622},{\"end\":38635,\"start\":38630},{\"end\":38643,\"start\":38641},{\"end\":38969,\"start\":38964},{\"end\":39184,\"start\":39180},{\"end\":39194,\"start\":39188},{\"end\":39205,\"start\":39198},{\"end\":39215,\"start\":39209},{\"end\":39229,\"start\":39219},{\"end\":39241,\"start\":39235},{\"end\":39250,\"start\":39247},{\"end\":39629,\"start\":39623},{\"end\":39639,\"start\":39633},{\"end\":39649,\"start\":39643},{\"end\":39937,\"start\":39930},{\"end\":40069,\"start\":40064},{\"end\":40083,\"start\":40073},{\"end\":40092,\"start\":40087},{\"end\":40106,\"start\":40096},{\"end\":40114,\"start\":40110},{\"end\":40125,\"start\":40118},{\"end\":40372,\"start\":40369},{\"end\":40379,\"start\":40376},{\"end\":40387,\"start\":40383},{\"end\":40395,\"start\":40391},{\"end\":40403,\"start\":40401},{\"end\":40409,\"start\":40407},{\"end\":40415,\"start\":40413},{\"end\":40780,\"start\":40777},{\"end\":40786,\"start\":40784},{\"end\":40794,\"start\":40790},{\"end\":40800,\"start\":40798},{\"end\":40808,\"start\":40804},{\"end\":40816,\"start\":40814},{\"end\":40822,\"start\":40820},{\"end\":41113,\"start\":41109},{\"end\":41120,\"start\":41117},{\"end\":41129,\"start\":41124},{\"end\":41141,\"start\":41133},{\"end\":41151,\"start\":41145},{\"end\":41160,\"start\":41155},{\"end\":41355,\"start\":41349},{\"end\":41368,\"start\":41361},{\"end\":41378,\"start\":41372},{\"end\":41571,\"start\":41567},{\"end\":41580,\"start\":41575},{\"end\":41588,\"start\":41584},{\"end\":41595,\"start\":41592},{\"end\":41601,\"start\":41599},{\"end\":41609,\"start\":41605},{\"end\":41862,\"start\":41852},{\"end\":41874,\"start\":41866},{\"end\":41886,\"start\":41878},{\"end\":41896,\"start\":41890},{\"end\":41903,\"start\":41900},{\"end\":41913,\"start\":41907},{\"end\":42077,\"start\":42075},{\"end\":42084,\"start\":42081},{\"end\":42090,\"start\":42088},{\"end\":42102,\"start\":42094},{\"end\":42260,\"start\":42258},{\"end\":42266,\"start\":42264},{\"end\":42278,\"start\":42270},{\"end\":42289,\"start\":42282},{\"end\":42443,\"start\":42439},{\"end\":42449,\"start\":42447},{\"end\":42458,\"start\":42453},{\"end\":42465,\"start\":42462},{\"end\":42471,\"start\":42469},{\"end\":42479,\"start\":42475},{\"end\":42486,\"start\":42483},{\"end\":42493,\"start\":42490},{\"end\":42657,\"start\":42655},{\"end\":42663,\"start\":42661},{\"end\":42671,\"start\":42667},{\"end\":42679,\"start\":42675},{\"end\":42688,\"start\":42683},{\"end\":42694,\"start\":42692},{\"end\":42962,\"start\":42960},{\"end\":42968,\"start\":42966},{\"end\":42976,\"start\":42972},{\"end\":42984,\"start\":42980},{\"end\":42993,\"start\":42988},{\"end\":42999,\"start\":42997},{\"end\":43323,\"start\":43318},{\"end\":43330,\"start\":43327},{\"end\":43337,\"start\":43334},{\"end\":43346,\"start\":43341},{\"end\":43588,\"start\":43583},{\"end\":43596,\"start\":43592},{\"end\":43605,\"start\":43600},{\"end\":43613,\"start\":43609},{\"end\":43619,\"start\":43617},{\"end\":43625,\"start\":43623},{\"end\":43633,\"start\":43629},{\"end\":43641,\"start\":43637},{\"end\":43895,\"start\":43886},{\"end\":43906,\"start\":43899},{\"end\":43917,\"start\":43910},{\"end\":44071,\"start\":44067},{\"end\":44078,\"start\":44075},{\"end\":44084,\"start\":44082},{\"end\":44093,\"start\":44088},{\"end\":44101,\"start\":44097},{\"end\":44108,\"start\":44105},{\"end\":44116,\"start\":44112},{\"end\":44122,\"start\":44120},{\"end\":44129,\"start\":44126},{\"end\":44383,\"start\":44380},{\"end\":44389,\"start\":44387},{\"end\":44398,\"start\":44393},{\"end\":44405,\"start\":44402},{\"end\":44411,\"start\":44409},{\"end\":44419,\"start\":44415},{\"end\":44543,\"start\":44537},{\"end\":44557,\"start\":44547},{\"end\":44570,\"start\":44561}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":53082914},\"end\":34313,\"start\":34016},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":7167114},\"end\":34518,\"start\":34315},{\"attributes\":{\"doi\":\"arXiv:2105.07599\",\"id\":\"b2\"},\"end\":34779,\"start\":34520},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":44220142},\"end\":35066,\"start\":34781},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":214003631},\"end\":35331,\"start\":35068},{\"attributes\":{\"id\":\"b5\"},\"end\":35462,\"start\":35333},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":221971086},\"end\":35740,\"start\":35464},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":85543335},\"end\":35978,\"start\":35742},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":235667974},\"end\":36286,\"start\":35980},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1290235},\"end\":36538,\"start\":36288},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2428314},\"end\":36721,\"start\":36540},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":235366031},\"end\":37042,\"start\":36723},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":232320210},\"end\":37314,\"start\":37044},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":216078090},\"end\":37458,\"start\":37316},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3144218},\"end\":37664,\"start\":37460},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":220961531},\"end\":38047,\"start\":37666},{\"attributes\":{\"doi\":\"arXiv:2012.07372\",\"id\":\"b16\",\"matched_paper_id\":235391000},\"end\":38513,\"start\":38049},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":233289721},\"end\":38880,\"start\":38515},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":10141934},\"end\":39119,\"start\":38882},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":49584497},\"end\":39558,\"start\":39121},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":850402},\"end\":39839,\"start\":39560},{\"attributes\":{\"id\":\"b21\"},\"end\":40060,\"start\":39841},{\"attributes\":{\"id\":\"b22\"},\"end\":40365,\"start\":40062},{\"attributes\":{\"doi\":\"arXiv:1812.10528\",\"id\":\"b23\"},\"end\":40665,\"start\":40367},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":231648189},\"end\":41052,\"start\":40667},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":218719252},\"end\":41310,\"start\":41054},{\"attributes\":{\"doi\":\"physics/0004057\",\"id\":\"b26\"},\"end\":41526,\"start\":41312},{\"attributes\":{\"id\":\"b27\"},\"end\":41822,\"start\":41528},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3292002},\"end\":42041,\"start\":41824},{\"attributes\":{\"id\":\"b29\"},\"end\":42205,\"start\":42043},{\"attributes\":{\"id\":\"b30\"},\"end\":42391,\"start\":42207},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":237100698},\"end\":42649,\"start\":42393},{\"attributes\":{\"id\":\"b32\"},\"end\":42753,\"start\":42651},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":222291521},\"end\":42881,\"start\":42755},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":232307847},\"end\":43271,\"start\":42883},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1479507},\"end\":43515,\"start\":43273},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":213898150},\"end\":43821,\"start\":43517},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":198179572},\"end\":44063,\"start\":43823},{\"attributes\":{\"id\":\"b38\"},\"end\":44374,\"start\":44065},{\"attributes\":{\"id\":\"b39\"},\"end\":44478,\"start\":44376},{\"attributes\":{\"doi\":\"arXiv:2103.03036\",\"id\":\"b40\",\"matched_paper_id\":29169801},\"end\":44917,\"start\":44480}]", "bib_title": "[{\"end\":34083,\"start\":34016},{\"end\":34354,\"start\":34315},{\"end\":34817,\"start\":34781},{\"end\":35158,\"start\":35068},{\"end\":35538,\"start\":35464},{\"end\":35796,\"start\":35742},{\"end\":36062,\"start\":35980},{\"end\":36365,\"start\":36288},{\"end\":36590,\"start\":36540},{\"end\":36827,\"start\":36723},{\"end\":37133,\"start\":37044},{\"end\":37347,\"start\":37316},{\"end\":37524,\"start\":37460},{\"end\":37728,\"start\":37666},{\"end\":38122,\"start\":38049},{\"end\":38594,\"start\":38515},{\"end\":38960,\"start\":38882},{\"end\":39174,\"start\":39121},{\"end\":39619,\"start\":39560},{\"end\":40773,\"start\":40667},{\"end\":41105,\"start\":41054},{\"end\":41563,\"start\":41528},{\"end\":41848,\"start\":41824},{\"end\":42435,\"start\":42393},{\"end\":42808,\"start\":42755},{\"end\":42956,\"start\":42883},{\"end\":43314,\"start\":43273},{\"end\":43579,\"start\":43517},{\"end\":43882,\"start\":43823},{\"end\":44533,\"start\":44480}]", "bib_author": "[{\"end\":34096,\"start\":34085},{\"end\":34106,\"start\":34096},{\"end\":34367,\"start\":34356},{\"end\":34378,\"start\":34367},{\"end\":34390,\"start\":34378},{\"end\":34400,\"start\":34390},{\"end\":34527,\"start\":34520},{\"end\":34833,\"start\":34819},{\"end\":34844,\"start\":34833},{\"end\":34857,\"start\":34844},{\"end\":34866,\"start\":34857},{\"end\":34876,\"start\":34866},{\"end\":34889,\"start\":34876},{\"end\":34898,\"start\":34889},{\"end\":35168,\"start\":35160},{\"end\":35174,\"start\":35168},{\"end\":35182,\"start\":35174},{\"end\":35376,\"start\":35365},{\"end\":35550,\"start\":35540},{\"end\":35559,\"start\":35550},{\"end\":35571,\"start\":35559},{\"end\":35583,\"start\":35571},{\"end\":35812,\"start\":35798},{\"end\":35823,\"start\":35812},{\"end\":35833,\"start\":35823},{\"end\":35839,\"start\":35833},{\"end\":36071,\"start\":36064},{\"end\":36079,\"start\":36071},{\"end\":36086,\"start\":36079},{\"end\":36094,\"start\":36086},{\"end\":36103,\"start\":36094},{\"end\":36109,\"start\":36103},{\"end\":36380,\"start\":36367},{\"end\":36391,\"start\":36380},{\"end\":36600,\"start\":36592},{\"end\":36606,\"start\":36600},{\"end\":36615,\"start\":36606},{\"end\":36837,\"start\":36829},{\"end\":36844,\"start\":36837},{\"end\":36853,\"start\":36844},{\"end\":36860,\"start\":36853},{\"end\":37142,\"start\":37135},{\"end\":37149,\"start\":37142},{\"end\":37156,\"start\":37149},{\"end\":37163,\"start\":37156},{\"end\":37361,\"start\":37349},{\"end\":37372,\"start\":37361},{\"end\":37536,\"start\":37526},{\"end\":37547,\"start\":37536},{\"end\":37736,\"start\":37730},{\"end\":37744,\"start\":37736},{\"end\":37750,\"start\":37744},{\"end\":37757,\"start\":37750},{\"end\":37765,\"start\":37757},{\"end\":37772,\"start\":37765},{\"end\":37780,\"start\":37772},{\"end\":37786,\"start\":37780},{\"end\":38138,\"start\":38124},{\"end\":38150,\"start\":38138},{\"end\":38163,\"start\":38150},{\"end\":38170,\"start\":38163},{\"end\":38179,\"start\":38170},{\"end\":38188,\"start\":38179},{\"end\":38604,\"start\":38596},{\"end\":38613,\"start\":38604},{\"end\":38620,\"start\":38613},{\"end\":38628,\"start\":38620},{\"end\":38637,\"start\":38628},{\"end\":38645,\"start\":38637},{\"end\":38971,\"start\":38962},{\"end\":38979,\"start\":38971},{\"end\":38983,\"start\":38979},{\"end\":39186,\"start\":39176},{\"end\":39196,\"start\":39186},{\"end\":39207,\"start\":39196},{\"end\":39217,\"start\":39207},{\"end\":39231,\"start\":39217},{\"end\":39243,\"start\":39231},{\"end\":39252,\"start\":39243},{\"end\":39631,\"start\":39621},{\"end\":39641,\"start\":39631},{\"end\":39651,\"start\":39641},{\"end\":39939,\"start\":39928},{\"end\":40071,\"start\":40062},{\"end\":40085,\"start\":40071},{\"end\":40094,\"start\":40085},{\"end\":40108,\"start\":40094},{\"end\":40116,\"start\":40108},{\"end\":40127,\"start\":40116},{\"end\":40374,\"start\":40367},{\"end\":40381,\"start\":40374},{\"end\":40389,\"start\":40381},{\"end\":40397,\"start\":40389},{\"end\":40405,\"start\":40397},{\"end\":40411,\"start\":40405},{\"end\":40417,\"start\":40411},{\"end\":40782,\"start\":40775},{\"end\":40788,\"start\":40782},{\"end\":40796,\"start\":40788},{\"end\":40802,\"start\":40796},{\"end\":40810,\"start\":40802},{\"end\":40818,\"start\":40810},{\"end\":40824,\"start\":40818},{\"end\":41115,\"start\":41107},{\"end\":41122,\"start\":41115},{\"end\":41131,\"start\":41122},{\"end\":41143,\"start\":41131},{\"end\":41153,\"start\":41143},{\"end\":41162,\"start\":41153},{\"end\":41357,\"start\":41347},{\"end\":41370,\"start\":41357},{\"end\":41380,\"start\":41370},{\"end\":41573,\"start\":41565},{\"end\":41582,\"start\":41573},{\"end\":41590,\"start\":41582},{\"end\":41597,\"start\":41590},{\"end\":41603,\"start\":41597},{\"end\":41611,\"start\":41603},{\"end\":41864,\"start\":41850},{\"end\":41876,\"start\":41864},{\"end\":41888,\"start\":41876},{\"end\":41898,\"start\":41888},{\"end\":41905,\"start\":41898},{\"end\":41915,\"start\":41905},{\"end\":42079,\"start\":42073},{\"end\":42086,\"start\":42079},{\"end\":42092,\"start\":42086},{\"end\":42104,\"start\":42092},{\"end\":42262,\"start\":42256},{\"end\":42268,\"start\":42262},{\"end\":42280,\"start\":42268},{\"end\":42291,\"start\":42280},{\"end\":42445,\"start\":42437},{\"end\":42451,\"start\":42445},{\"end\":42460,\"start\":42451},{\"end\":42467,\"start\":42460},{\"end\":42473,\"start\":42467},{\"end\":42481,\"start\":42473},{\"end\":42488,\"start\":42481},{\"end\":42495,\"start\":42488},{\"end\":42659,\"start\":42653},{\"end\":42665,\"start\":42659},{\"end\":42673,\"start\":42665},{\"end\":42681,\"start\":42673},{\"end\":42690,\"start\":42681},{\"end\":42696,\"start\":42690},{\"end\":42964,\"start\":42958},{\"end\":42970,\"start\":42964},{\"end\":42978,\"start\":42970},{\"end\":42986,\"start\":42978},{\"end\":42995,\"start\":42986},{\"end\":43001,\"start\":42995},{\"end\":43325,\"start\":43316},{\"end\":43332,\"start\":43325},{\"end\":43339,\"start\":43332},{\"end\":43348,\"start\":43339},{\"end\":43590,\"start\":43581},{\"end\":43598,\"start\":43590},{\"end\":43607,\"start\":43598},{\"end\":43615,\"start\":43607},{\"end\":43621,\"start\":43615},{\"end\":43627,\"start\":43621},{\"end\":43635,\"start\":43627},{\"end\":43643,\"start\":43635},{\"end\":43897,\"start\":43884},{\"end\":43908,\"start\":43897},{\"end\":43919,\"start\":43908},{\"end\":44073,\"start\":44065},{\"end\":44080,\"start\":44073},{\"end\":44086,\"start\":44080},{\"end\":44095,\"start\":44086},{\"end\":44103,\"start\":44095},{\"end\":44110,\"start\":44103},{\"end\":44118,\"start\":44110},{\"end\":44124,\"start\":44118},{\"end\":44131,\"start\":44124},{\"end\":44385,\"start\":44378},{\"end\":44391,\"start\":44385},{\"end\":44400,\"start\":44391},{\"end\":44407,\"start\":44400},{\"end\":44413,\"start\":44407},{\"end\":44421,\"start\":44413},{\"end\":44545,\"start\":44535},{\"end\":44559,\"start\":44545},{\"end\":44572,\"start\":44559}]", "bib_venue": "[{\"end\":34146,\"start\":34106},{\"end\":34404,\"start\":34400},{\"end\":34628,\"start\":34543},{\"end\":34902,\"start\":34898},{\"end\":35189,\"start\":35182},{\"end\":35363,\"start\":35333},{\"end\":35590,\"start\":35583},{\"end\":35843,\"start\":35839},{\"end\":36113,\"start\":36109},{\"end\":36397,\"start\":36391},{\"end\":36619,\"start\":36615},{\"end\":36864,\"start\":36860},{\"end\":37167,\"start\":37163},{\"end\":37376,\"start\":37372},{\"end\":37551,\"start\":37547},{\"end\":37840,\"start\":37786},{\"end\":38239,\"start\":38204},{\"end\":38684,\"start\":38645},{\"end\":38987,\"start\":38983},{\"end\":39307,\"start\":39252},{\"end\":39679,\"start\":39651},{\"end\":39926,\"start\":39841},{\"end\":40196,\"start\":40127},{\"end\":40487,\"start\":40433},{\"end\":40838,\"start\":40824},{\"end\":41169,\"start\":41162},{\"end\":41345,\"start\":41312},{\"end\":41660,\"start\":41611},{\"end\":41919,\"start\":41915},{\"end\":42071,\"start\":42043},{\"end\":42254,\"start\":42207},{\"end\":42500,\"start\":42495},{\"end\":42814,\"start\":42810},{\"end\":43063,\"start\":43001},{\"end\":43377,\"start\":43348},{\"end\":43647,\"start\":43643},{\"end\":43928,\"start\":43919},{\"end\":44199,\"start\":44131},{\"end\":44654,\"start\":44588}]"}}}, "year": 2023, "month": 12, "day": 17}