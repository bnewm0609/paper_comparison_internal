{"id": 202888986, "updated": "2023-10-06 22:55:51.206", "metadata": {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "authors": "[{\"first\":\"Zhenzhong\",\"last\":\"Lan\",\"middle\":[]},{\"first\":\"Mingda\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Sebastian\",\"last\":\"Goodman\",\"middle\":[]},{\"first\":\"Kevin\",\"last\":\"Gimpel\",\"middle\":[]},{\"first\":\"Piyush\",\"last\":\"Sharma\",\"middle\":[]},{\"first\":\"Radu\",\"last\":\"Soricut\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1909.11942", "mag": "2996428491", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/LanCGGSS20", "doi": null}}, "content": {"source": {"pdf_hash": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1909.11942v6.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "00bf7bd5d049bc585fdde0339912efc884828ddb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7a064df1aeada7e69e5173f7d4c8606f4470365b.txt", "contents": "\nPublished as a conference paper at ICLR 2020 ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS\n\n\nZhenzhong Lan \nGoogle Research\n\n\nMingda Chen mchen@ttic.edu \nToyota Technological Institute at Chicago\n\n\nSebastian Goodman \nGoogle Research\n\n\nKevin Gimpel kgimpel@ttic.edu \nToyota Technological Institute at Chicago\n\n\nPiyush Sharma piyushsharma@google.com \nGoogle Research\n\n\nRadu Soricut rsoricut@google.com \nGoogle Research\n\n\nPublished as a conference paper at ICLR 2020 ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS\n\nIncreasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameterreduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.Published as a conference paper at ICLR 2020 These solutions address the memory limitation problem, but not the communication overhead. In this paper, we address all of the aforementioned problems, by designing A Lite BERT (ALBERT) architecture that has significantly fewer parameters than a traditional BERT architecture.ALBERT incorporates two parameter reduction techniques that lift the major obstacles in scaling pre-trained models. The first one is a factorized embedding parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding. This separation makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network. Both techniques significantly reduce the number of parameters for BERT without seriously hurting performance, thus improving parameter-efficiency. An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster. The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization.To further improve the performance of ALBERT, we also introduce a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness  of the next sentence prediction (NSP) loss proposed in the original BERT.As a result of these design decisions, we are able to scale up to much larger ALBERT configurations that still have fewer parameters than BERT-large but achieve significantly better performance. We establish new state-of-the-art results on the well-known GLUE, SQuAD, and RACE benchmarks for natural language understanding. Specifically, we push the RACE accuracy to 89.4%, the GLUE benchmark to 89.4, and the F1 score of SQuAD 2.0 to 92.2.\n\nINTRODUCTION\n\nFull network pre-training (Dai & Le, 2015;Radford et al., 2018;Howard & Ruder, 2018) has led to a series of breakthroughs in language representation learning. Many nontrivial NLP tasks, including those that have limited training data, have greatly benefited from these pre-trained models. One of the most compelling signs of these breakthroughs is the evolution of machine performance on a reading comprehension task designed for middle and high-school English exams in China, the RACE test (Lai et al., 2017): the paper that originally describes the task and formulates the modeling challenge reports then state-of-the-art machine accuracy at 44.1%; the latest published result reports their model performance at 83.2% ; the work we present here pushes it even higher to 89.4%, a stunning 45.3% improvement that is mainly attributable to our current ability to build high-performance pretrained language representations.\n\nEvidence from these improvements reveals that a large network is of crucial importance for achieving state-of-the-art performance . It has become common practice to pre-train large models and distill them down to smaller ones (Sun et al., 2019;Turc et al., 2019) for real applications. Given the importance of model size, we ask: Is having better NLP models as easy as having larger models? An obstacle to answering this question is the memory limitations of available hardware. Given that current state-of-the-art models often have hundreds of millions or even billions of parameters, it is easy to hit these limitations as we try to scale our models. Training speed can also be significantly hampered in distributed training, as the communication overhead is directly proportional to the number of parameters in the model.\n\nExisting solutions to the aforementioned problems include model parallelization (Shazeer et al., 2018;Shoeybi et al., 2019) and clever memory management (Chen et al., 2016;.\n\n\nRELATED WORK\n\n\nSCALING UP REPRESENTATION LEARNING FOR NATURAL LANGUAGE\n\nLearning representations of natural language has been shown to be useful for a wide range of NLP tasks and has been widely adopted (Mikolov et al., 2013;Le & Mikolov, 2014;Dai & Le, 2015;Peters et al., 2018;Radford et al., 2018;. One of the most significant changes in the last two years is the shift from pre-training word embeddings, whether standard (Mikolov et al., 2013;Pennington et al., 2014) or contextualized (McCann et al., 2017;Peters et al., 2018), to full-network pre-training followed by task-specific fine-tuning (Dai & Le, 2015;Radford et al., 2018;. In this line of work, it is often shown that larger model size improves performance. For example,  show that across three selected natural language understanding tasks, using larger hidden size, more hidden layers, and more attention heads always leads to better performance. However, they stop at a hidden size of 1024, presumably because of the model size and computation cost problems.\n\nIt is difficult to experiment with large models due to computational constraints, especially in terms of GPU/TPU memory limitations. Given that current state-of-the-art models often have hundreds of millions or even billions of parameters, we can easily hit memory limits. To address this issue, Chen et al. (2016) propose a method called gradient checkpointing to reduce the memory requirement to be sublinear at the cost of an extra forward pass.  propose a way to reconstruct each layer's activations from the next layer so that they do not need to store the intermediate activations.\n\nBoth methods reduce the memory consumption at the cost of speed. Raffel et al. (2019) proposed to use model parallelization to train a giant model. In contrast, our parameter-reduction techniques reduce memory consumption and increase training speed.\n\n\nCROSS-LAYER PARAMETER SHARING\n\nThe idea of sharing parameters across layers has been previously explored with the Transformer architecture (Vaswani et al., 2017), but this prior work has focused on training for standard encoderdecoder tasks rather than the pretraining/finetuning setting. Different from our observations, Dehghani et al. (2018) show that networks with cross-layer parameter sharing (Universal Transformer, UT) get better performance on language modeling and subject-verb agreement than the standard transformer. Very recently, Bai et al. (2019) propose a Deep Equilibrium Model (DQE) for transformer networks and show that DQE can reach an equilibrium point for which the input embedding and the output embedding of a certain layer stay the same. Our observations show that our embeddings are oscillating rather than converging. Hao et al. (2019) combine a parameter-sharing transformer with the standard one, which further increases the number of parameters of the standard transformer.\n\n\nSENTENCE ORDERING OBJECTIVES\n\nALBERT uses a pretraining loss based on predicting the ordering of two consecutive segments of text. Several researchers have experimented with pretraining objectives that similarly relate to discourse coherence. Coherence and cohesion in discourse have been widely studied and many phenomena have been identified that connect neighboring text segments (Hobbs, 1979;Halliday & Hasan, 1976;Grosz et al., 1995). Most objectives found effective in practice are quite simple. Skipthought  and FastSent (Hill et al., 2016) sentence embeddings are learned by using an encoding of a sentence to predict words in neighboring sentences. Other objectives for sentence embedding learning include predicting future sentences rather than only neighbors (Gan et al., 2017) and predicting explicit discourse markers (Jernite et al., 2017;Nie et al., 2019). Our loss is most similar to the sentence ordering objective of Jernite et al. (2017), where sentence embeddings are learned in order to determine the ordering of two consecutive sentences. Unlike most of the above work, however, our loss is defined on textual segments rather than sentences. BERT  uses a loss based on predicting whether the second segment in a pair has been swapped with a segment from another document. We compare to this loss in our experiments and find that sentence ordering is a more challenging pretraining task and more useful for certain downstream tasks. Concurrently to our work,  also try to predict the order of two consecutive segments of text, but they combine it with the original next sentence prediction in a three-way classification task rather than empirically comparing the two.\n\n\nTHE ELEMENTS OF ALBERT\n\nIn this section, we present the design decisions for ALBERT and provide quantified comparisons against corresponding configurations of the original BERT architecture .\n\n\nMODEL ARCHITECTURE CHOICES\n\nThe backbone of the ALBERT architecture is similar to BERT in that it uses a transformer encoder (Vaswani et al., 2017) with GELU nonlinearities (Hendrycks & Gimpel, 2016). We follow the BERT notation conventions and denote the vocabulary embedding size as E, the number of encoder layers as L, and the hidden size as H. Following , we set the feed-forward/filter size to be 4H and the number of attention heads to be H/64.\n\nThere are three main contributions that ALBERT makes over the design choices of BERT.\n\nFactorized embedding parameterization. In BERT, as well as subsequent modeling improvements such as XLNet  and RoBERTa , the WordPiece embedding size E is tied with the hidden layer size H, i.e., E \u2261 H. This decision appears suboptimal for both modeling and practical reasons, as follows.\n\nFrom a modeling perspective, WordPiece embeddings are meant to learn context-independent representations, whereas hidden-layer embeddings are meant to learn context-dependent representations. As experiments with context length indicate , the power of BERT-like representations comes from the use of context to provide the signal for learning such context-dependent representations. As such, untying the WordPiece embedding size E from the hidden layer size H allows us to make a more efficient usage of the total model parameters as informed by modeling needs, which dictate that H E.\n\nFrom a practical perspective, natural language processing usually require the vocabulary size V to be large. 1 If E \u2261 H, then increasing H increases the size of the embedding matrix, which has size V \u00d7 E. This can easily result in a model with billions of parameters, most of which are only updated sparsely during training.\n\nTherefore, for ALBERT we use a factorization of the embedding parameters, decomposing them into two smaller matrices. Instead of projecting the one-hot vectors directly into the hidden space of size H, we first project them into a lower dimensional embedding space of size E, and then project it to the hidden space. By using this decomposition, we reduce the embedding parameters from\nO(V \u00d7 H) to O(V \u00d7 E + E \u00d7 H)\n. This parameter reduction is significant when H E. We choose to use the same E for all word pieces because they are much more evenly distributed across documents compared to whole-word embedding, where having different embedding size (Grave et al. (2017); Baevski & Auli (2018);  ) for different words is important.\n\nCross-layer parameter sharing. For ALBERT, we propose cross-layer parameter sharing as another way to improve parameter efficiency. There are multiple ways to share parameters, e.g., only sharing feed-forward network (FFN) parameters across layers, or only sharing attention parameters. The default decision for ALBERT is to share all parameters across layers. All our experiments use this default decision unless otherwise specified. We compare this design decision against other strategies in our experiments in Sec. 4.5.  Dehghani et al. (2018) show that UT outperforms a vanilla Transformer. Bai et al. (2019) show that their DQEs reach an equilibrium point for which the input and output embedding of a certain layer stay the same. Our measurement on the L2 distances and cosine similarity show that our embeddings are oscillating rather than converging.  Figure 1 shows the L2 distances and cosine similarity of the input and output embeddings for each layer, using BERT-large and ALBERT-large configurations (see Table 1). We observe that the transitions from layer to layer are much smoother for ALBERT than for BERT. These results show that weight-sharing has an effect on stabilizing network parameters. Although there is a drop for both metrics compared to BERT, they nevertheless do not converge to 0 even after 24 layers. This shows that the solution space for ALBERT parameters is very different from the one found by DQE.\n\nInter-sentence coherence loss. In addition to the masked language modeling (MLM) loss , BERT uses an additional loss called next-sentence prediction (NSP). NSP is a binary classification loss for predicting whether two segments appear consecutively in the original text, as follows: positive examples are created by taking consecutive segments from the training corpus; negative examples are created by pairing segments from different documents; positive and negative examples are sampled with equal probability. The NSP objective was designed to improve performance on downstream tasks, such as natural language inference, that require reasoning about the relationship between sentence pairs. However, subsequent studies  found NSP's impact unreliable and decided to eliminate it, a decision supported by an improvement in downstream task performance across several tasks.\n\nWe conjecture that the main reason behind NSP's ineffectiveness is its lack of difficulty as a task, as compared to MLM. As formulated, NSP conflates topic prediction and coherence prediction in a\n\n\nModel\n\nParameters Layers Hidden Embedding Parameter-sharing   BERT  base  108M  12  768  768  False  large  334M  24  1024  1024  False   ALBERT   base  12M  12  768  128  True  large  18M  24  1024  128  True  xlarge  60M  24  2048  128  True  xxlarge  235M  12  4096 128 True Table 1: The configurations of the main BERT and ALBERT models analyzed in this paper.\n\nsingle task 2 . However, topic prediction is easier to learn compared to coherence prediction, and also overlaps more with what is learned using the MLM loss.\n\nWe maintain that inter-sentence modeling is an important aspect of language understanding, but we propose a loss based primarily on coherence. That is, for ALBERT, we use a sentence-order prediction (SOP) loss, which avoids topic prediction and instead focuses on modeling inter-sentence coherence. The SOP loss uses as positive examples the same technique as BERT (two consecutive segments from the same document), and as negative examples the same two consecutive segments but with their order swapped. This forces the model to learn finer-grained distinctions about discourse-level coherence properties. As we show in Sec. 4.6, it turns out that NSP cannot solve the SOP task at all (i.e., it ends up learning the easier topic-prediction signal, and performs at randombaseline level on the SOP task), while SOP can solve the NSP task to a reasonable degree, presumably based on analyzing misaligned coherence cues. As a result, ALBERT models consistently improve downstream task performance for multi-sentence encoding tasks.\n\n\nMODEL SETUP\n\nWe present the differences between BERT and ALBERT models with comparable hyperparameter settings in Table 1. Due to the design choices discussed above, ALBERT models have much smaller parameter size compared to corresponding BERT models.\n\nFor example, ALBERT-large has about 18x fewer parameters compared to BERT-large, 18M versus 334M. An ALBERT-xlarge configuration with H = 2048 has only 60M parameters and an ALBERT-xxlarge configuration with H = 4096 has 233M parameters, i.e., around 70% of BERTlarge's parameters. Note that for ALBERT-xxlarge, we mainly report results on a 12-layer network because a 24-layer network (with the same configuration) obtains similar results but is computationally more expensive.\n\nThis improvement in parameter efficiency is the most important advantage of ALBERT's design choices. Before we can quantify this advantage, we need to introduce our experimental setup in more detail.\n\n\nEXPERIMENTAL RESULTS\n\n\nEXPERIMENTAL SETUP\n\nTo keep the comparison as meaningful as possible, we follow the BERT  setup in using the BOOKCORPUS  and English Wikipedia  for pretraining baseline models. These two corpora consist of around 16GB of uncompressed text. We format our inputs as \" [CLS] x 1 [SEP] x 2 [SEP]\", where x 1 = x 1,1 , x 1,2 \u00b7 \u00b7 \u00b7 and x 2 = x 1,1 , x 1,2 \u00b7 \u00b7 \u00b7 are two segments. 3 We always limit the maximum input length to 512, and randomly generate input sequences shorter than 512 with a probability of 10%. Like BERT, we use a vocabulary size of 30,000, tokenized using SentencePiece (Kudo & Richardson, 2018) as in XLNet .\n\nWe generate masked inputs for the MLM targets using n-gram masking , with the length of each n-gram mask selected randomly. The probability for the length n is given by p(n) = 1/n N k=1 1/k We set the maximum length of n-gram (i.e., n) to be 3 (i.e., the MLM target can consist of up to a 3-gram of complete words, such as \"White House correspondents\").\n\nAll the model updates use a batch size of 4096 and a LAMB optimizer with learning rate 0.00176 (You et al., 2019). We train all models for 125,000 steps unless otherwise specified. Training was done on Cloud TPU V3. The number of TPUs used for training ranged from 64 to 512, depending on model size.\n\nThe experimental setup described in this section is used for all of our own versions of BERT as well as ALBERT models, unless otherwise specified.\n\n\nEVALUATION BENCHMARKS\n\n\nINTRINSIC EVALUATION\n\nTo monitor the training progress, we create a development set based on the development sets from SQuAD and RACE using the same procedure as in Sec. 4.1. We report accuracies for both MLM and sentence classification tasks. Note that we only use this set to check how the model is converging; it has not been used in a way that would affect the performance of any downstream evaluation, such as via model selection.\n\n\nDOWNSTREAM EVALUATION\n\nFollowing Yang et al. (2019) and , we evaluate our models on three popular benchmarks: The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018), two versions of the Stanford Question Answering Dataset (SQuAD; , and the ReAding Comprehension from Examinations (RACE) dataset (Lai et al., 2017). For completeness, we provide description of these benchmarks in Appendix A.3. As in , we perform early stopping on the development sets, on which we report all comparisons except for our final comparisons based on the task leaderboards, for which we also report test set results. For GLUE datasets that have large variances on the dev set, we report median over 5 runs.\n\n\nOVERALL COMPARISON BETWEEN BERT AND ALBERT\n\nWe are now ready to quantify the impact of the design choices described in Sec. 3, specifically the ones around parameter efficiency. The improvement in parameter efficiency showcases the most important advantage of ALBERT's design choices, as shown in Table 2: with only around 70% of BERT-large's parameters, ALBERT-xxlarge achieves significant improvements over BERT-large, as measured by the difference on development set scores for several representative downstream tasks: SQuAD v1.1 (+1.9%), SQuAD v2.0 (+3.1%), MNLI (+1.4%), SST-2 (+2.2%), and RACE (+8.4%).\n\nAnother interesting observation is the speed of data throughput at training time under the same training configuration (same number of TPUs). Because of less communication and fewer computations, ALBERT models have higher data throughput compared to their corresponding BERT models. If we use BERT-large as the baseline, we observe that ALBERT-large is about 1.7 times faster in iterating through the data while ALBERT-xxlarge is about 3 times slower because of the larger structure.\n\nNext, we perform ablation experiments that quantify the individual contribution of each of the design choices for ALBERT. Table 3 shows the effect of changing the vocabulary embedding size E using an ALBERT-base configuration setting (see Table 1), using the same set of representative downstream tasks. Under the non-shared condition (BERT-style), larger embedding sizes give better performance, but not by  Table 2: Dev set results for models pretrained over BOOKCORPUS and Wikipedia for 125k steps.\n\n\nFACTORIZED EMBEDDING PARAMETERIZATION\n\nHere and everywhere else, the Avg column is computed by averaging the scores of the downstream tasks to its left (the two numbers of F1 and EM for each SQuAD are first averaged).\n\nmuch. Under the all-shared condition (ALBERT-style), an embedding of size 128 appears to be the best. Based on these results, we use an embedding size E = 128 in all future settings, as a necessary step to do further scaling.   Table 4 presents experiments for various cross-layer parameter-sharing strategies, using an ALBERT-base configuration (Table 1) with two embedding sizes (E = 768 and E = 128). We compare the all-shared strategy (ALBERT-style), the not-shared strategy (BERT-style), and intermediate strategies in which only the attention parameters are shared (but not the FNN ones) or only the FFN parameters are shared (but not the attention ones).\n\nThe all-shared strategy hurts performance under both conditions, but it is less severe for E = 128 (-1.5 on Avg) compared to E = 768 (-2.5 on Avg). In addition, most of the performance drop appears to come from sharing the FFN-layer parameters, while sharing the attention parameters results in no drop when E = 128 (+0.1 on Avg), and a slight drop when E = 768 (-0.7 on Avg).\n\nThere are other strategies of sharing the parameters cross layers. For example, We can divide the L layers into N groups of size M , and each size-M group shares parameters. Overall, our experimental results shows that the smaller the group size M is, the better the performance we get. However, decreasing group size M also dramatically increase the number of overall parameters. We choose all-shared strategy as our default choice.  Table 4: The effect of cross-layer parameter-sharing strategies, ALBERT-base configuration.\n\n\nSENTENCE ORDER PREDICTION (SOP)\n\nWe compare head-to-head three experimental conditions for the additional inter-sentence loss: none (XLNet-and RoBERTa-style), NSP (BERT-style), and SOP (ALBERT-style), using an ALBERTbase configuration. Results are shown in  The results on the intrinsic tasks reveal that the NSP loss brings no discriminative power to the SOP task (52.0% accuracy, similar to the random-guess performance for the \"None\" condition). This allows us to conclude that NSP ends up modeling only topic shift. In contrast, the SOP loss does solve the NSP task relatively well (78.9% accuracy), and the SOP task even better (86.5% accuracy). Even more importantly, the SOP loss appears to consistently improve downstream task performance for multi-sentence encoding tasks (around +1% for SQuAD1.1, +2% for SQuAD2.0, +1.7% for RACE), for an Avg score improvement of around +1%.\n\n\nWHAT IF WE TRAIN FOR THE SAME AMOUNT OF TIME?\n\nThe speed-up results in Table 2 indicate that data-throughput for BERT-large is about 3.17x higher compared to ALBERT-xxlarge. Since longer training usually leads to better performance, we perform a comparison in which, instead of controlling for data throughput (number of training steps), we control for the actual training time (i.e., let the models train for the same number of hours). In Table 6, we compare the performance of a BERT-large model after 400k training steps (after 34h of training), roughly equivalent with the amount of time needed to train an ALBERT-xxlarge model with 125k training steps (32h of training).  After training for roughly the same amount of time, ALBERT-xxlarge is significantly better than BERT-large: +1.5% better on Avg, with the difference on RACE as high as +5.2%.\n\n\nADDITIONAL TRAINING DATA AND DROPOUT EFFECTS\n\nThe experiments done up to this point use only the Wikipedia and BOOKCORPUS datasets, as in . In this section, we report measurements on the impact of the additional data used by both XLNet  and RoBERTa . Fig. 2a plots the dev set MLM accuracy under two conditions, without and with additional data, with the latter condition giving a significant boost. We also observe performance improvements on the downstream tasks in Table 7, except for the SQuAD benchmarks (which are Wikipedia-based, and therefore are negatively affected by out-of-domain training material).  Table 7: The effect of additional training data using the ALBERT-base configuration.\n\nWe also note that, even after training for 1M steps, our largest models still do not overfit to their training data. As a result, we decide to remove dropout to further increase our model capacity. The plot in Fig. 2b shows that removing dropout significantly improves MLM accuracy. Intermediate evaluation on ALBERT-xxlarge at around 1M training steps ( Table 8) also confirms that removing dropout helps the downstream tasks. There is empirical (Szegedy et al., 2017) and theoretical  evidence showing that a combination of batch normalization and dropout in Convolutional Neural Networks may have harmful results. To the best of our knowledge, we are the first to show that dropout can hurt performance in large Transformer-based models. However, the underlying network structure of ALBERT is a special case of the transformer and further experimentation is needed to see if this phenomenon appears with other transformer-based architectures or not.  \n\n\nCURRENT STATE-OF-THE-ART ON NLU TASKS\n\nThe results we report in this section make use of the training data used by , as well as the additional data used by  and Yang et al. (2019). We report state-of-the-art results under two settings for fine-tuning: single-model and ensembles. In both settings, we only do single-task fine-tuning 4 . Following , on the development set we report the median result over five runs.  The single-model ALBERT configuration incorporates the best-performing settings discussed: an ALBERT-xxlarge configuration (Table 1) using combined MLM and SOP losses, and no dropout.\n\nThe checkpoints that contribute to the final ensemble model are selected based on development set performance; the number of checkpoints considered for this selection range from 6 to 17, depending on the task. For the GLUE (Table 9) and RACE (Table 10) benchmarks, we average the model predictions for the ensemble models, where the candidates are fine-tuned from different training steps using the 12-layer and 24-layer architectures. For SQuAD (Table 10), we average the prediction scores for those spans that have multiple probabilities; we also average the scores of the \"unanswerable\" decision.\n\nBoth single-model and ensemble results indicate that ALBERT improves the state-of-the-art significantly for all three benchmarks, achieving a GLUE score of 89.4, a SQuAD 2.0 test F1 score of 92.2, and a RACE test accuracy of 89.4. The latter appears to be a particularly strong improvement, a jump of +17.4% absolute points over BERT Clark et al., 2019), +7.6% over XLNet , +6.2% over RoBERTa , and 5.3% over DCMI+ , an ensemble of multiple models specifically designed for reading comprehension tasks. Our single model achieves an accuracy of 86.5%, which is still 2.4% better than the state-of-the-art ensemble model.  \n\n\nDISCUSSION\n\nWhile ALBERT-xxlarge has less parameters than BERT-large and gets significantly better results, it is computationally more expensive due to its larger structure. An important next step is thus to speed up the training and inference speed of ALBERT through methods like sparse attention  and block attention (Shen et al., 2018). An orthogonal line of research, which could provide additional representation power, includes hard example mining (Mikolov et al., 2013) and more efficient language modeling training . Additionally, although we have convincing evidence that sentence order prediction is a more consistently-useful learning task that leads to better language representations, we hypothesize that there could be more dimensions not yet captured by the current self-supervised training losses that could create additional representation power for the resulting representations.\n\n\nA APPENDIX\n\n\nA.1 EFFECT OF NETWORK DEPTH AND WIDTH\n\nIn this section, we check how depth (number of layers) and width (hidden size) affect the performance of ALBERT. Table 11 shows the performance of an ALBERT-large configuration (see Table 1) using different numbers of layers. Networks with 3 or more layers are trained by fine-tuning using the parameters from the depth before (e.g., the 12-layer network parameters are fine-tuned from the checkpoint of the 6-layer network parameters). 5 Similar technique has been used in Gong et al. (2019). If we compare a 3-layer ALBERT model with a 1-layer ALBERT model, although they have the same number of parameters, the performance increases significantly. However, there are diminishing returns when continuing to increase the number of layers: the results of a 12-layer network are relatively close to the results of a 24-layer network, and the performance of a 48-layer network appears to decline.  A similar phenomenon, this time for width, can be seen in Table 12 for a 3-layer ALBERT-large configuration. As we increase the hidden size, we get an increase in performance with diminishing returns. At a hidden size of 6144, the performance appears to decline significantly. We note that none of these models appear to overfit the training data, and they all have higher training and development loss compared to the best-performing ALBERT configurations.  \n\n\nSimilar strategies have been explored by Dehghani et al. (2018) (Universal Transformer, UT) and Bai et al. (2019) (Deep Equilibrium Models, DQE) for Transformer networks. Different from our observations,\n\nFigure 1 :\n1The L2 distances and cosine similarity (in terms of degree) of the input and output embedding of each layer for BERT-large and ALBERT-large.\n\nFigure 2 :\n2The effects of adding data and removing dropout during training.\n\nTable 3 :\n3The effect of vocabulary embedding size on the performance of ALBERT-base.4.5 CROSS-LAYER PARAMETER SHARING \n\n\n\nTable 5 ,\n5both over intrinsic (accuracy for the MLM, NSP, and SOP tasks) and downstream tasks.Intrinsic Tasks \nDownstream Tasks \nSP tasks MLM NSP SOP SQuAD1.1 SQuAD2.0 MNLI SST-2 RACE Avg \nNone \n54.9 \n52.4 53.3 \n88.6/81.5 \n78.1/75.3 \n81.5 \n89.9 \n61.7 \n79.0 \nNSP \n54.5 \n90.5 52.0 \n88.4/81.5 \n77.2/74.6 \n81.6 \n91.1 \n62.3 \n79.2 \nSOP \n54.0 \n78.9 86.5 \n89.3/82.3 \n80.0/77.1 \n82.0 \n90.3 \n64.0 \n80.1 \n\n\n\nTable 5 :\n5The effect of sentence-prediction loss, NSP vs. SOP, on intrinsic and downstream tasks.\n\nTable 6 :\n6The effect of controlling for training time, BERT-large vs ALBERT-xxlarge configurations.\n\nTable 8 :\n8The effect of removing dropout, measured for an ALBERT-xxlarge configuration.\n\nTable 9 :\n9State-of-the-art results on the GLUE benchmark. For single-task single-model results, we \nreport ALBERT at 1M steps (comparable to RoBERTa) and at 1.5M steps. The ALBERT ensemble \nuses models trained with 1M, 1.5M, and other numbers of steps. \n\n\n\n\nModelsSQuAD1.1 dev SQuAD2.0 dev SQuAD2.0 test RACE test (Middle/High) Single model (from leaderboard as of Sept. 23, 2019) Ensembles (from leaderboard as of Sept. 23, 2019)BERT-large \n90.9/84.1 \n81.8/79.0 \n89.1/86.3 \n72.0 (76.6/70.1) \nXLNet \n94.5/89.0 \n88.8/86.1 \n89.1/86.3 \n81.8 (85.5/80.2) \nRoBERTa \n94.6/88.9 \n89.4/86.5 \n89.8/86.8 \n83.2 (86.5/81.3) \nUPM \n-\n-\n89.9/87.2 \n-\nXLNet + SG-Net Verifier++ \n-\n-\n90.1/87.2 \n-\nALBERT (1M) \n94.8/89.2 \n89.9/87.2 \n-\n86.0 (88.2/85.1) \nALBERT (1.5M) \n94.8/89.3 \n90.2/87.4 \n90.9/88.1 \n86.5 (89.0/85.5) \nBERT-large \n92.2/86.2 \n-\n-\n-\nXLNet + SG-Net Verifier \n-\n-\n90.7/88.2 \n-\nUPM \n-\n-\n90.7/88.2 \nXLNet + DAAF + Verifier \n-\n-\n90.9/88.6 \n-\nDCMN+ \n-\n-\n-\n84.1 (88.5/82.3) \nALBERT \n95.5/90.1 \n91.4/88.9 \n92.2/89.7 \n89.4 (91.2/88.6) \n\n\n\nTable 10 :\n10State-of-the-art results on the SQuAD and RACE benchmarks.\n\n\nNumber of layers Parameters SQuAD1.1 SQuAD2.0 MNLI SST-2 RACE Avg1 \n18M \n31.1/22.9 \n50.1/50.1 \n66.4 \n80.8 \n40.1 \n52.9 \n3 \n18M \n79.8/69.7 \n64.4/61.7 \n77.7 \n86.7 \n54.0 \n71.2 \n6 \n18M \n86.4/78.4 \n73.8/71.1 \n81.2 \n88.9 \n60.9 \n77.2 \n12 \n18M \n89.8/83.3 \n80.7/77.9 \n83.3 \n91.7 \n66.7 \n81.5 \n24 \n18M \n90.3/83.3 \n81.8/79.0 \n83.3 \n91.5 \n68.7 \n82.1 \n48 \n18M \n90.0/83.1 \n81.8/78.9 \n83.4 \n91.9 \n66.9 \n81.8 \n\n\n\nTable 11 :\n11The effect of increasing the number of layers for an ALBERT-large configuration.\n\nA . 4\n.HYPERPARAMETERSHyperparameters for downstream tasks are shown inTable 14. We adapt these hyperparameters from,, andYang et al. (2019).LR \nBSZ ALBERT DR Classifier DR \nTS \nWS \nMSL \nCoLA \n1.00E-05 \n16 \n0 \n0.1 \n5336 \n320 \n512 \nSTS \n2.00E-05 \n16 \n0 \n0.1 \n3598 \n214 \n512 \nSST-2 \n1.00E-05 \n32 \n0 \n0.1 \n20935 1256 \n512 \nMNLI \n3.00E-05 128 \n0 \n0.1 \n10000 1000 \n512 \nQNLI \n1.00E-05 \n32 \n0 \n0.1 \n33112 1986 \n512 \nQQP \n5.00E-05 128 \n0.1 \n0.1 \n14000 1000 \n512 \nRTE \n3.00E-05 \n32 \n0.1 \n0.1 \n800 \n200 \n512 \nMRPC \n2.00E-05 \n32 \n0 \n0.1 \n800 \n200 \n512 \nWNLI \n2.00E-05 \n16 \n0.1 \n0.1 \n2000 \n250 \n512 \nSQuAD v1.1 5.00E-05 \n48 \n0 \n0.1 \n3649 \n365 \n384 \nSQuAD v2.0 3.00E-05 \n48 \n0 \n0.1 \n8144 \n814 \n512 \nRACE \n2.00E-05 \n32 \n0.1 \n0.1 \n12000 1000 \n512 \n\n\n\nTable 14 :\n14Hyperparameters for ALBERT in downstream tasks. LR: Learning Rate. BSZ: Batch Size. DR: Dropout Rate. TS: Training Steps. WS: Warmup Steps. MSL: Maximum Sequence Length.\nSimilar to BERT, all the experiments in this paper use a vocabulary size V of 30,000.\nSince a negative example is constructed using material from a different document, the negative-example segment is misaligned both from a topic and from a coherence perspective. 3 A segment is usually comprised of more than one natural sentence, which has been shown to benefit performance by.\n Following Liu et al. (2019), we fine-tune for RTE, STS, and MRPC using an MNLI checkpoint.\nIf we compare the performance of ALBERT-large here to the performance inTable 2, we can see that this warm-start technique does not help to improve the downstream performance. However, it does help the 48-layer network to converge. A similar technique has been applied to our ALBERT-xxlarge, where we warm-start from a 6-layer network.\nACKNOWLEDGEMENTThe authors would like to thank Beer Changpinyo, Nan Ding, Noam Shazeer, and Tomer Levinboim for discussion and providing useful feedback on the project; Omer Levy and Naman Goyal for clarifying experimental setup for RoBERTa; Zihang Dai for clarifying XLNet; Brandon Norick, Emma Strubell, Shaojie Bai, Chas Leichner, and Sachin Mehta for providing useful feedback on the paper; Jacob Devlin for providing the English and multilingual version of training data; Liang Xu, Chenjie Cao and the CLUE community for providing the training data and evaluation benechmark of the Chinese version of ALBERT models.\nAlexei Baevski, Michael Auli, arXiv:1809.10853Adaptive input representations for neural language modeling. arXiv preprintAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. arXiv preprint arXiv:1809.10853, 2018.\n\nDeep equilibrium models. Shaojie Bai, J Zico Kolter, Vladlen Koltun, Neural Information Processing Systems (NeurIPS). Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Neural Information Processing Systems (NeurIPS), 2019.\n\nThe second PASCAL recognising textual entailment challenge. Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, Idan Szpektor, Proceedings of the second PASCAL challenges workshop on recognising textual entailment. the second PASCAL challenges workshop on recognising textual entailmentVenice6Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pp. 6-4. Venice, 2006.\n\nThe fifth PASCAL recognizing textual entailment challenge. Luisa Bentivogli, Peter Clark, Ido Dagan, Danilo Giampiccolo, TACLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The fifth PASCAL recognizing textual entailment challenge. In TAC, 2009.\n\nSemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. Daniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez-Gazpio, Lucia Specia, 10.18653/v1/S17-2001Proceedings of the 11th International Workshop on Semantic Evaluation. the 11th International Workshop on Semantic EvaluationVancouver, CanadaAssociation for Computational LinguisticsDaniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 1-14, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001. URL https://www.aclweb.org/anthology/S17-2001.\n\nTraining deep nets with sublinear memory cost. Tianqi Chen, Bing Xu, Chiyuan Zhang, Carlos Guestrin, arXiv:1604.06174arXiv preprintTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016.\n\nGenerating long sequences with sparse transformers. Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever, arXiv:1904.10509arXiv preprintRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.\n\nKevin Clark, Minh-Thang Luong, Urvashi Khandelwal, D Christopher, Quoc V Manning, Le, arXiv:1907.04829Bam! born-again multi-task networks for natural language understanding. arXiv preprintKevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D Manning, and Quoc V Le. Bam! born-again multi-task networks for natural language understanding. arXiv preprint arXiv:1907.04829, 2019.\n\nThe PASCAL recognising textual entailment challenge. Oren Ido Dagan, Bernardo Glickman, Magnini, Machine Learning Challenges Workshop. SpringerIdo Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges Workshop, pp. 177-190. Springer, 2005.\n\nSemi-supervised sequence learning. M Andrew, Quoc V Dai, Le, Advances in neural information processing systems. Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural infor- mation processing systems, pp. 3079-3087, 2015.\n\nTransformer-xl: Attentive language models beyond a fixed-length context. Zihang Dai, Zhilin Yang, Yiming Yang, W William, Jaime Cohen, Carbonell, V Quoc, Ruslan Le, Salakhutdinov, arXiv:1901.02860arXiv preprintZihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.\n\n. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, \u0141ukasz Kaiser, arXiv:1807.03819Universal transformers. arXiv preprintMostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and \u0141ukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //www.aclweb.org/anthology/N19-1423.\n\nAutomatically constructing a corpus of sentential paraphrases. B William, Chris Dolan, Brockett, Proceedings of the Third International Workshop on Paraphrasing (IWP2005). the Third International Workshop on Paraphrasing (IWP2005)William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://www.aclweb.org/anthology/I05-5002.\n\nLearning generic sentence representations using convolutional neural networks. Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li, Xiaodong He, Lawrence Carin, 10.18653/v1/D17-1254Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsZhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li, Xiaodong He, and Lawrence Carin. Learn- ing generic sentence representations using convolutional neural networks. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2390-2400, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1254. URL https://www.aclweb.org/anthology/D17-1254.\n\nThe third PASCAL recognizing textual entailment challenge. Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, Bill Dolan, Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. the ACL-PASCAL Workshop on Textual Entailment and ParaphrasingPragueAssociation for Computational LinguisticsDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entail- ment and Paraphrasing, pp. 1-9, Prague, June 2007. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/W07-1401.\n\nThe reversible residual network: Backpropagation without storing activations. N Aidan, Mengye Gomez, Raquel Ren, Roger B Urtasun, Grosse, Advances in neural information processing systems. Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual net- work: Backpropagation without storing activations. In Advances in neural information processing systems, pp. 2214-2224, 2017.\n\nEfficient training of bert by progressively stacking. Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, Tieyan Liu, International Conference on Machine Learning. Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efficient training of bert by progressively stacking. In International Conference on Machine Learning, pp. 2337-2346, 2019.\n\nEfficient softmax approximation for gpus. Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, Herv\u00e9 J\u00e9gou, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, Herv\u00e9 J\u00e9gou, et al. Efficient softmax approxima- tion for gpus. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1302-1310. JMLR. org, 2017.\n\nCentering: A framework for modeling the local coherence of discourse. Barbara J Grosz, Aravind K Joshi, Scott Weinstein, Computational Linguistics. 212Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203-225, 1995. URL https: //www.aclweb.org/anthology/J95-2003.\n\n. M A K Halliday, Ruqaiya Hasan, Cohesion in English. Routledge. M.A.K. Halliday and Ruqaiya Hasan. Cohesion in English. Routledge, 1976.\n\nModeling recurrence for transformer. Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, Zhaopeng Tu, 10.18653/v1/n19-1122doi: 10. 18653/v1/n19-1122Proceedings of the 2019 Conference of the North. the 2019 Conference of the NorthJie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng Tu. Modeling recurrence for transformer. Proceedings of the 2019 Conference of the North, 2019. doi: 10. 18653/v1/n19-1122. URL http://dx.doi.org/10.18653/v1/n19-1122.\n\nDan Hendrycks, Kevin Gimpel, arXiv:1606.08415Gaussian Error Linear Units (GELUs). arXiv preprintDan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs). arXiv preprint arXiv:1606.08415, 2016.\n\nLearning distributed representations of sentences from unlabelled data. Felix Hill, Kyunghyun Cho, Anna Korhonen, 10.18653/v1/N16-1162Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsFelix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1367-1377. Association for Computational Linguistics, 2016. doi: 10.18653/v1/N16-1162. URL http: //aclweb.org/anthology/N16-1162.\n\nCoherence and coreference. Jerry R Hobbs, Cognitive Science. 31Jerry R. Hobbs. Coherence and coreference. Cognitive Science, 3(1):67-90, 1979.\n\nUniversal language model fine-tuning for text classification. Jeremy Howard, Sebastian Ruder, arXiv:1801.06146arXiv preprintJeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146, 2018.\n\nFirst quora dataset release: Question pairs. Shankar Iyer, Nikhil Dandekar, Kornl Csernai, Shankar Iyer, Nikhil Dandekar, and Kornl Csernai. First quora dataset release: Ques- tion pairs, January 2017. URL https://www.quora.com/q/quoradata/ First-Quora-Dataset-Release-Question-Pairs.\n\nDiscourse-based objectives for fast unsupervised sentence representation learning. Yacine Jernite, David Samuel R Bowman, Sontag, arXiv:1705.00557arXiv preprintYacine Jernite, Samuel R Bowman, and David Sontag. Discourse-based objectives for fast unsuper- vised sentence representation learning. arXiv preprint arXiv:1705.00557, 2017.\n\nMandar Joshi, Danqi Chen, Yinhan Liu, S Daniel, Luke Weld, Omer Zettlemoyer, Levy, arXiv:1907.10529SpanBERT: Improving pre-training by representing and predicting spans. arXiv preprintMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. SpanBERT: Improving pre-training by representing and predicting spans. arXiv preprint arXiv:1907.10529, 2019.\n\nRaquel Urtasun, and Sanja Fidler. Skip-thought vectors. Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S Zemel, Antonio Torralba, Proceedings of the 28th International Conference on Neural Information Processing Systems. the 28th International Conference on Neural Information Processing SystemsCambridge, MA, USAMIT Press2Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Ur- tasun, and Sanja Fidler. Skip-thought vectors. In Proceedings of the 28th International Con- ference on Neural Information Processing Systems -Volume 2, NIPS'15, pp. 3294-3302, Cam- bridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id= 2969442.2969607.\n\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. Taku Kudo, John Richardson, 10.18653/v1/D18-2012Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2018 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsBrussels, BelgiumAssociation for Computational LinguisticsTaku Kudo and John Richardson. SentencePiece: A simple and language independent sub- word tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Con- ference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 66-71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL https://www.aclweb.org/anthology/D18-2012.\n\nRACE: Large-scale ReAding comprehension dataset from examinations. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard Hovy, 10.18653/v1/D17-1082Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 785-794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL https://www. aclweb.org/anthology/D17-1082.\n\nDistributed representations of sentences and documents. Quoc Le, Tomas Mikolov, Proceedings of the 31st ICML. the 31st ICMLBeijing, ChinaQuoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In Proceed- ings of the 31st ICML, Beijing, China, 2014.\n\nThe Winograd schema challenge. Hector Levesque, Ernest Davis, Leora Morgenstern, Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning. Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In Thir- teenth International Conference on the Principles of Knowledge Representation and Reasoning, 2012.\n\nUnderstanding the disharmony between dropout and batch normalization by variance shift. Xiang Li, Shuo Chen, Xiaolin Hu, Jian Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionXiang Li, Shuo Chen, Xiaolin Hu, and Jian Yang. Understanding the disharmony between dropout and batch normalization by variance shift. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2682-2690, 2019.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, Roberta, arXiv:1907.11692A robustly optimized BERT pretraining approach. arXiv preprintYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre- training approach. arXiv preprint arXiv:1907.11692, 2019.\n\nLearned in translation: Contextualized word vectors. Bryan Mccann, James Bradbury, Caiming Xiong, Richard Socher, Advances in Neural Information Processing Systems. I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. GarnettCurran Associates, Inc30Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contextualized word vectors. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 6294-6305. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/ 7209-learned-in-translation-contextualized-word-vectors.pdf.\n\nDistributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, Advances in neural information processing systems. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen- tations of words and phrases and their compositionality. In Advances in neural information pro- cessing systems, pp. 3111-3119, 2013.\n\nDisSent: Learning sentence representations from explicit discourse relations. Allen Nie, Erin Bennett, Noah Goodman, 10.18653/v1/P19-1442Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsAllen Nie, Erin Bennett, and Noah Goodman. DisSent: Learning sentence representations from ex- plicit discourse relations. In Proceedings of the 57th Annual Meeting of the Association for Com- putational Linguistics, pp. 4497-4510, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1442. URL https://www.aclweb.org/anthology/ P19-1442.\n\nGlove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher Manning, 10.3115/v1/D14-1162Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational LinguisticsJeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word rep- resentation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1532-1543, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1162. URL https://www.aclweb.org/anthology/ D14-1162.\n\nDeep contextualized word representations. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, 10.18653/v1/N18-1202Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics1Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Con- ference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1 (Long Papers), pp. 2227-2237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL https://www.aclweb.org/anthology/N18-1202.\n\nImproving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. https://s3-us-west-2.amazonaws.com/ openai-assets/research-covers/language-unsupervised/language_ understanding_paper.pdf, 2018.\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI Blog. 18Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, arXiv:1910.10683arXiv preprintColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\n\nSQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, 10.18653/v1/D16-1264Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383-2392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://www.aclweb. org/anthology/D16-1264.\n\nKnow what you don't know: Unanswerable questions for SQuAD. Pranav Rajpurkar, Robin Jia, Percy Liang, 10.18653/v1/P18-2124Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics2Short Papers)Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 784-789, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. URL https://www.aclweb. org/anthology/P18-2124.\n\nMesh-tensorflow: Deep learning for supercomputers. Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, Hyoukjoong Lee, Mingsheng Hong, Cliff Young, Advances in Neural Information Processing Systems. Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorflow: Deep learning for supercomputers. In Advances in Neural Information Processing Systems, pp. 10414- 10423, 2018.\n\nBi-directional block selfattention for fast and memory-efficient sequence modeling. Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Chengqi Zhang, arXiv:1804.00857arXiv preprintTao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, and Chengqi Zhang. Bi-directional block self- attention for fast and memory-efficient sequence modeling. arXiv preprint arXiv:1804.00857, 2018.\n\nMegatron-LM: Training multi-billion parameter language models using model parallelism. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick Legresley, Jared Casper, Bryan Catanzaro, Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM: Training multi-billion parameter language models using model par- allelism, 2019.\n\nRecursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, Christopher Potts, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingSeattle, Washington, USAAssociation for Computational LinguisticsRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631-1642, Seattle, Washington, USA, October 2013. Association for Computa- tional Linguistics. URL https://www.aclweb.org/anthology/D13-1170.\n\nPatient knowledge distillation for BERT model compression. Siqi Sun, Yu Cheng, Zhe Gan, Jingjing Liu, arXiv:1908.09355arXiv preprintSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for BERT model compression. arXiv preprint arXiv:1908.09355, 2019.\n\nInception-v4, inception-resnet and the impact of residual connections on learning. Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alexander A Alemi, Thirty-First AAAI Conference on Artificial Intelligence. Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In Thirty-First AAAI Confer- ence on Artificial Intelligence, 2017.\n\nWell-read students learn better: The impact of student initialization on knowledge distillation. Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1908.08962arXiv preprintIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: The impact of student initialization on knowledge distillation. arXiv preprint arXiv:1908.08962, 2019.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998-6008, 2017.\n\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, 10.18653/v1/W18-5446Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational LinguisticsAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceed- ings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353-355, Brussels, Belgium, November 2018. Association for Computational Lin- guistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anthology/ W18-5446.\n\nStructBERT: Incorporating language structures into pre-training for deep language understanding. Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Liwei Peng, Luo Si, arXiv:1908.04577arXiv preprintWei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Liwei Peng, and Luo Si. StructBERT: Incor- porating language structures into pre-training for deep language understanding. arXiv preprint arXiv:1908.04577, 2019.\n\nAlex Warstadt, Amanpreet Singh, Samuel R Bowman, arXiv:1805.12471Neural network acceptability judgments. arXiv preprintAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471, 2018.\n\nA broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel Bowman, 10.18653/v1/N18-1101Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics1Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen- tence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo- gies, Volume 1 (Long Papers), pp. 1112-1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https://www.aclweb. org/anthology/N18-1101.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V Le, Xlnet, arXiv:1906.08237Generalized autoregressive pretraining for language understanding. arXiv preprintZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. XLNet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019.\n\nReducing BERT pre-training time from 3 days to 76 minutes. Yang You, Jing Li, Jonathan Hseu, Xiaodan Song, James Demmel, Cho-Jui Hsieh, arXiv:1904.00962arXiv preprintYang You, Jing Li, Jonathan Hseu, Xiaodan Song, James Demmel, and Cho-Jui Hsieh. Reducing BERT pre-training time from 3 days to 76 minutes. arXiv preprint arXiv:1904.00962, 2019.\n\nDCMN+: Dual co-matching network for multi-choice reading comprehension. Shuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng Zhang, Xi Zhou, Xiang Zhou, arXiv:1908.11511arXiv preprintShuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng Zhang, Xi Zhou, and Xiang Zhou. DCMN+: Dual co-matching network for multi-choice reading comprehension. arXiv preprint arXiv:1908.11511, 2019.\n\nAligning books and movies: Towards story-like visual explanations by watching movies and reading books. Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionHidden size Parameters SQuAD1.1 SQuAD2.0 MNLI SST-2 RACE Avg 1024Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pp. 19-27, 2015. Hidden size Parameters SQuAD1.1 SQuAD2.0 MNLI SST-2 RACE Avg 1024\n\nThe effect of increasing the hidden-layer size for an ALBERT-large 3-layer configuration. Table. 12Table 12: The effect of increasing the hidden-layer size for an ALBERT-large 3-layer configuration.\n\nA.2 DO VERY WIDE ALBERT MODELS NEED TO BE DEEP(ER) TOO?. A.2 DO VERY WIDE ALBERT MODELS NEED TO BE DEEP(ER) TOO?\n\nthe difference between a 12-layer and a 24-layer configuration is small. Does this result still hold for much wider ALBERT configurations, such as ALBERT-xxlarge (H=4096)? Number of layers. SQuAD1.1 SQuAD2.0 MNLI SST-2 RACE Avg 12In Section A.1, we show that for ALBERT-large (H=1024). In Section A.1, we show that for ALBERT-large (H=1024), the difference between a 12-layer and a 24-layer configuration is small. Does this result still hold for much wider ALBERT configurations, such as ALBERT-xxlarge (H=4096)? Number of layers SQuAD1.1 SQuAD2.0 MNLI SST-2 RACE Avg 12\n\nThe effect of a deeper network using an ALBERT-xxlarge configuration. Table. 13Table 13: The effect of a deeper network using an ALBERT-xxlarge configuration.\n\nThe difference between 12-layer and 24-layer ALBERT-xxlarge configurations in terms of downstream accuracy is negligible, with the Avg score being the same. The answer is given by the results from Table 13. We conclude that, when sharing all cross-layer parameters (ALBERT-style), there is no need for models deeper than a 12-layer configurationThe answer is given by the results from Table 13. The difference between 12-layer and 24-layer ALBERT-xxlarge configurations in terms of downstream accuracy is negligible, with the Avg score being the same. We conclude that, when sharing all cross-layer parameters (ALBERT-style), there is no need for models deeper than a 12-layer configuration.\n\nDOWNSTREAM EVALUATION TASKS GLUE GLUE is comprised of 9 tasks, namely Corpus of Linguistic Acceptability (CoLA. A Warstadt, Stanford Sentiment Treebank. Microsoft Research Paraphrase Corpus (MRPCA.3 DOWNSTREAM EVALUATION TASKS GLUE GLUE is comprised of 9 tasks, namely Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018), Stanford Sentiment Treebank (SST; Socher et al., 2013), Microsoft Research Paraphrase Corpus (MRPC;\n\nSemantic Textual Similarity Benchmark. ; Dolan &amp; Brockett, Sts; Cer, Quora Question Pairs. QQPDolan & Brockett, 2005), Semantic Textual Similarity Bench- mark (STS; Cer et al., 2017), Quora Question Pairs (QQP;\n\nIt focuses on evaluating model capabilities for natural language understanding. When reporting MNLI results, we only report the \"match\" condition (MNLI-m). We follow the finetuning procedures from prior work. Iyer, Liu et al.2019) and report the held-out test set performance obtained from GLUE submissions. For test set submissions, we perform task-specific modifications for WNLI and QNLI as described byIyer et al., 2017), Multi-Genre NLI (MNLI; Williams et al., 2018), Question NLI (QNLI; Rajpurkar et al., 2016), Recognizing Textual Entailment (RTE; Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) and Winograd NLI (WNLI; Levesque et al., 2012). It focuses on evaluating model capabilities for natural language understanding. When reporting MNLI results, we only report the \"match\" condition (MNLI-m). We follow the finetuning procedures from prior work (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) and report the held-out test set performance obtained from GLUE submissions. For test set submissions, we perform task-specific modifications for WNLI and QNLI as described by Liu et al. (2019) and Yang et al. (2019).\n\nWe evaluate our models on two versions of SQuAD: v1.1 and v2.0. SQuAD v1.1 has 100,000 human-annotated question/answer pairs. SQuAD v2.0 additionally introduced 50,000 unanswerable questions. For SQuAD v1.1, we use the same training procedure as BERT, whereas for SQuAD v2.0, models are jointly trained with a span extraction loss and an additional classifier for predicting answerability. Yang , SQuAD SQuAD is an extractive question answering dataset built from Wikipedia. We report both development set and test set performanceSQuAD SQuAD is an extractive question answering dataset built from Wikipedia. The answers are segments from the context paragraphs and the task is to predict answer spans. We evaluate our models on two versions of SQuAD: v1.1 and v2.0. SQuAD v1.1 has 100,000 human-annotated question/answer pairs. SQuAD v2.0 additionally introduced 50,000 unanswerable questions. For SQuAD v1.1, we use the same training procedure as BERT, whereas for SQuAD v2.0, models are jointly trained with a span extraction loss and an additional classifier for predicting answerabil- ity (Yang et al., 2019; Liu et al., 2019). We report both development set and test set performance.\n\nwe use the concatenation of the passage, question, and each candidate answer as the input to models. Then, we use the representations from the \"[CLS]\" token for predicting the probability of each answer. The dataset consists of two domains: middle school and high school. Yang , RACE RACE is a large-scale dataset for multi-choice reading comprehension, collected from English examinations in China with nearly 100,000 questions. We train our models on both domains and report accuracies on both the development set and test setRACE RACE is a large-scale dataset for multi-choice reading comprehension, collected from En- glish examinations in China with nearly 100,000 questions. Each instance in RACE has 4 candidate answers. Following prior work (Yang et al., 2019; Liu et al., 2019), we use the concatenation of the passage, question, and each candidate answer as the input to models. Then, we use the represen- tations from the \"[CLS]\" token for predicting the probability of each answer. The dataset consists of two domains: middle school and high school. We train our models on both domains and report accuracies on both the development set and test set.\n", "annotations": {"author": "[{\"end\":157,\"start\":125},{\"end\":229,\"start\":158},{\"end\":266,\"start\":230},{\"end\":341,\"start\":267},{\"end\":398,\"start\":342},{\"end\":450,\"start\":399}]", "publisher": null, "author_last_name": "[{\"end\":138,\"start\":135},{\"end\":169,\"start\":165},{\"end\":247,\"start\":240},{\"end\":279,\"start\":273},{\"end\":355,\"start\":349},{\"end\":411,\"start\":404}]", "author_first_name": "[{\"end\":134,\"start\":125},{\"end\":164,\"start\":158},{\"end\":239,\"start\":230},{\"end\":272,\"start\":267},{\"end\":348,\"start\":342},{\"end\":403,\"start\":399}]", "author_affiliation": "[{\"end\":156,\"start\":140},{\"end\":228,\"start\":186},{\"end\":265,\"start\":249},{\"end\":340,\"start\":298},{\"end\":397,\"start\":381},{\"end\":449,\"start\":433}]", "title": "[{\"end\":122,\"start\":1},{\"end\":572,\"start\":451}]", "venue": null, "abstract": "[{\"end\":3593,\"start\":574}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3651,\"start\":3635},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3672,\"start\":3651},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3693,\"start\":3672},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4118,\"start\":4100},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":4776,\"start\":4758},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":4794,\"start\":4776},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":5460,\"start\":5438},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":5481,\"start\":5460},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5530,\"start\":5511},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5759,\"start\":5737},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5778,\"start\":5759},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5793,\"start\":5778},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5813,\"start\":5793},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5834,\"start\":5813},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5981,\"start\":5959},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6005,\"start\":5981},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6045,\"start\":6024},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6065,\"start\":6045},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6150,\"start\":6134},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6171,\"start\":6150},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6877,\"start\":6859},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7237,\"start\":7217},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7566,\"start\":7544},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7749,\"start\":7727},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7966,\"start\":7949},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8268,\"start\":8251},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8808,\"start\":8795},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8831,\"start\":8808},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8850,\"start\":8831},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8959,\"start\":8940},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9200,\"start\":9182},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9265,\"start\":9243},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9282,\"start\":9265},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9368,\"start\":9347},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":10444,\"start\":10422},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10496,\"start\":10470},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12709,\"start\":12689},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12732,\"start\":12711},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13319,\"start\":13297},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13385,\"start\":13368},{\"end\":18070,\"start\":18065},{\"end\":18174,\"start\":18173},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":18408,\"start\":18383},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":18892,\"start\":18874},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":19743,\"start\":19725},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":19884,\"start\":19865},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20033,\"start\":20015},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":26705,\"start\":26683},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":27372,\"start\":27354},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28749,\"start\":28730},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":29357,\"start\":29339},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":29496,\"start\":29474},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30464,\"start\":30446},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":34309,\"start\":34291}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31533,\"start\":31328},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31687,\"start\":31534},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31765,\"start\":31688},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31888,\"start\":31766},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":32286,\"start\":31889},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":32386,\"start\":32287},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":32488,\"start\":32387},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":32578,\"start\":32489},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":32836,\"start\":32579},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":33603,\"start\":32837},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":33676,\"start\":33604},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":34072,\"start\":33677},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":34167,\"start\":34073},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":34904,\"start\":34168},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":35088,\"start\":34905}]", "paragraph": "[{\"end\":4530,\"start\":3609},{\"end\":5356,\"start\":4532},{\"end\":5531,\"start\":5358},{\"end\":6561,\"start\":5606},{\"end\":7150,\"start\":6563},{\"end\":7402,\"start\":7152},{\"end\":8409,\"start\":7436},{\"end\":10100,\"start\":8442},{\"end\":10294,\"start\":10127},{\"end\":10748,\"start\":10325},{\"end\":10835,\"start\":10750},{\"end\":11125,\"start\":10837},{\"end\":11711,\"start\":11127},{\"end\":12037,\"start\":11713},{\"end\":12424,\"start\":12039},{\"end\":12770,\"start\":12454},{\"end\":14208,\"start\":12772},{\"end\":15083,\"start\":14210},{\"end\":15281,\"start\":15085},{\"end\":15648,\"start\":15291},{\"end\":15808,\"start\":15650},{\"end\":16838,\"start\":15810},{\"end\":17092,\"start\":16854},{\"end\":17572,\"start\":17094},{\"end\":17773,\"start\":17574},{\"end\":18422,\"start\":17819},{\"end\":18777,\"start\":18424},{\"end\":19079,\"start\":18779},{\"end\":19227,\"start\":19081},{\"end\":19689,\"start\":19276},{\"end\":20404,\"start\":19715},{\"end\":21015,\"start\":20451},{\"end\":21500,\"start\":21017},{\"end\":22003,\"start\":21502},{\"end\":22223,\"start\":22045},{\"end\":22886,\"start\":22225},{\"end\":23264,\"start\":22888},{\"end\":23792,\"start\":23266},{\"end\":24680,\"start\":23828},{\"end\":25534,\"start\":24730},{\"end\":26234,\"start\":25583},{\"end\":27190,\"start\":26236},{\"end\":27793,\"start\":27232},{\"end\":28394,\"start\":27795},{\"end\":29017,\"start\":28396},{\"end\":29917,\"start\":29032},{\"end\":31327,\"start\":29972}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12453,\"start\":12425}]", "table_ref": "[{\"end\":13799,\"start\":13792},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":15552,\"start\":15326},{\"end\":15569,\"start\":15562},{\"end\":16962,\"start\":16955},{\"end\":20711,\"start\":20704},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21631,\"start\":21624},{\"end\":21748,\"start\":21741},{\"end\":21918,\"start\":21911},{\"end\":22460,\"start\":22453},{\"end\":22580,\"start\":22571},{\"end\":23708,\"start\":23701},{\"end\":24761,\"start\":24754},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":25130,\"start\":25123},{\"end\":26012,\"start\":26005},{\"end\":26157,\"start\":26150},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":26598,\"start\":26591},{\"end\":27742,\"start\":27733},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":28026,\"start\":28018},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":28047,\"start\":28037},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":28251,\"start\":28241},{\"attributes\":{\"ref_id\":\"tab_16\"},\"end\":30093,\"start\":30085},{\"end\":30934,\"start\":30926}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3607,\"start\":3595},{\"attributes\":{\"n\":\"2\"},\"end\":5546,\"start\":5534},{\"attributes\":{\"n\":\"2.1\"},\"end\":5604,\"start\":5549},{\"attributes\":{\"n\":\"2.2\"},\"end\":7434,\"start\":7405},{\"attributes\":{\"n\":\"2.3\"},\"end\":8440,\"start\":8412},{\"attributes\":{\"n\":\"3\"},\"end\":10125,\"start\":10103},{\"attributes\":{\"n\":\"3.1\"},\"end\":10323,\"start\":10297},{\"end\":15289,\"start\":15284},{\"attributes\":{\"n\":\"3.2\"},\"end\":16852,\"start\":16841},{\"attributes\":{\"n\":\"4\"},\"end\":17796,\"start\":17776},{\"attributes\":{\"n\":\"4.1\"},\"end\":17817,\"start\":17799},{\"attributes\":{\"n\":\"4.2\"},\"end\":19251,\"start\":19230},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":19274,\"start\":19254},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":19713,\"start\":19692},{\"attributes\":{\"n\":\"4.3\"},\"end\":20449,\"start\":20407},{\"attributes\":{\"n\":\"4.4\"},\"end\":22043,\"start\":22006},{\"attributes\":{\"n\":\"4.6\"},\"end\":23826,\"start\":23795},{\"attributes\":{\"n\":\"4.7\"},\"end\":24728,\"start\":24683},{\"attributes\":{\"n\":\"4.8\"},\"end\":25581,\"start\":25537},{\"attributes\":{\"n\":\"4.9\"},\"end\":27230,\"start\":27193},{\"attributes\":{\"n\":\"5\"},\"end\":29030,\"start\":29020},{\"end\":29930,\"start\":29920},{\"end\":29970,\"start\":29933},{\"end\":31545,\"start\":31535},{\"end\":31699,\"start\":31689},{\"end\":31776,\"start\":31767},{\"end\":31899,\"start\":31890},{\"end\":32297,\"start\":32288},{\"end\":32397,\"start\":32388},{\"end\":32499,\"start\":32490},{\"end\":32589,\"start\":32580},{\"end\":33615,\"start\":33605},{\"end\":34084,\"start\":34074},{\"end\":34174,\"start\":34169},{\"end\":34916,\"start\":34906}]", "table": "[{\"end\":31888,\"start\":31852},{\"end\":32286,\"start\":31985},{\"end\":32836,\"start\":32591},{\"end\":33603,\"start\":33011},{\"end\":34072,\"start\":33744},{\"end\":34904,\"start\":34310}]", "figure_caption": "[{\"end\":31533,\"start\":31330},{\"end\":31687,\"start\":31547},{\"end\":31765,\"start\":31701},{\"end\":31852,\"start\":31778},{\"end\":31985,\"start\":31901},{\"end\":32386,\"start\":32299},{\"end\":32488,\"start\":32399},{\"end\":32578,\"start\":32501},{\"end\":33011,\"start\":32839},{\"end\":33676,\"start\":33618},{\"end\":33744,\"start\":33679},{\"end\":34167,\"start\":34087},{\"end\":34310,\"start\":34176},{\"end\":35088,\"start\":34919}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13641,\"start\":13633},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25795,\"start\":25788},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26453,\"start\":26446}]", "bib_author_first_name": "[{\"end\":36523,\"start\":36517},{\"end\":36540,\"start\":36533},{\"end\":36804,\"start\":36797},{\"end\":36811,\"start\":36810},{\"end\":36816,\"start\":36812},{\"end\":36832,\"start\":36825},{\"end\":37086,\"start\":37083},{\"end\":37100,\"start\":37097},{\"end\":37112,\"start\":37108},{\"end\":37124,\"start\":37120},{\"end\":37138,\"start\":37132},{\"end\":37160,\"start\":37152},{\"end\":37174,\"start\":37170},{\"end\":37706,\"start\":37701},{\"end\":37724,\"start\":37719},{\"end\":37735,\"start\":37732},{\"end\":37749,\"start\":37743},{\"end\":38011,\"start\":38005},{\"end\":38021,\"start\":38017},{\"end\":38033,\"start\":38028},{\"end\":38047,\"start\":38042},{\"end\":38067,\"start\":38062},{\"end\":38755,\"start\":38749},{\"end\":38766,\"start\":38762},{\"end\":38778,\"start\":38771},{\"end\":38792,\"start\":38786},{\"end\":39035,\"start\":39030},{\"end\":39048,\"start\":39043},{\"end\":39059,\"start\":39055},{\"end\":39073,\"start\":39069},{\"end\":39271,\"start\":39266},{\"end\":39289,\"start\":39279},{\"end\":39304,\"start\":39297},{\"end\":39318,\"start\":39317},{\"end\":39338,\"start\":39332},{\"end\":39712,\"start\":39708},{\"end\":39732,\"start\":39724},{\"end\":40006,\"start\":40005},{\"end\":40021,\"start\":40015},{\"end\":40302,\"start\":40296},{\"end\":40314,\"start\":40308},{\"end\":40327,\"start\":40321},{\"end\":40335,\"start\":40334},{\"end\":40350,\"start\":40345},{\"end\":40370,\"start\":40369},{\"end\":40383,\"start\":40377},{\"end\":40664,\"start\":40657},{\"end\":40682,\"start\":40675},{\"end\":40695,\"start\":40690},{\"end\":40710,\"start\":40705},{\"end\":40728,\"start\":40722},{\"end\":41026,\"start\":41021},{\"end\":41043,\"start\":41035},{\"end\":41057,\"start\":41051},{\"end\":41071,\"start\":41063},{\"end\":42000,\"start\":41999},{\"end\":42015,\"start\":42010},{\"end\":42480,\"start\":42477},{\"end\":42493,\"start\":42486},{\"end\":42505,\"start\":42498},{\"end\":42521,\"start\":42513},{\"end\":42534,\"start\":42526},{\"end\":42547,\"start\":42539},{\"end\":43283,\"start\":43277},{\"end\":43305,\"start\":43297},{\"end\":43318,\"start\":43315},{\"end\":43330,\"start\":43326},{\"end\":43932,\"start\":43931},{\"end\":43946,\"start\":43940},{\"end\":43960,\"start\":43954},{\"end\":43973,\"start\":43966},{\"end\":44322,\"start\":44315},{\"end\":44331,\"start\":44329},{\"end\":44343,\"start\":44336},{\"end\":44351,\"start\":44348},{\"end\":44362,\"start\":44357},{\"end\":44375,\"start\":44369},{\"end\":44671,\"start\":44664},{\"end\":44685,\"start\":44679},{\"end\":44703,\"start\":44694},{\"end\":44716,\"start\":44711},{\"end\":45153,\"start\":45146},{\"end\":45155,\"start\":45154},{\"end\":45170,\"start\":45163},{\"end\":45172,\"start\":45171},{\"end\":45185,\"start\":45180},{\"end\":45454,\"start\":45453},{\"end\":45458,\"start\":45455},{\"end\":45476,\"start\":45469},{\"end\":45630,\"start\":45627},{\"end\":45640,\"start\":45636},{\"end\":45654,\"start\":45647},{\"end\":45668,\"start\":45661},{\"end\":45682,\"start\":45675},{\"end\":45698,\"start\":45690},{\"end\":46078,\"start\":46075},{\"end\":46095,\"start\":46090},{\"end\":46357,\"start\":46352},{\"end\":46373,\"start\":46364},{\"end\":46383,\"start\":46379},{\"end\":47158,\"start\":47153},{\"end\":47160,\"start\":47159},{\"end\":47338,\"start\":47332},{\"end\":47356,\"start\":47347},{\"end\":47583,\"start\":47576},{\"end\":47596,\"start\":47590},{\"end\":47612,\"start\":47607},{\"end\":47906,\"start\":47900},{\"end\":47921,\"start\":47916},{\"end\":48159,\"start\":48153},{\"end\":48172,\"start\":48167},{\"end\":48185,\"start\":48179},{\"end\":48192,\"start\":48191},{\"end\":48205,\"start\":48201},{\"end\":48216,\"start\":48212},{\"end\":48594,\"start\":48590},{\"end\":48607,\"start\":48602},{\"end\":48619,\"start\":48613},{\"end\":48642,\"start\":48635},{\"end\":48644,\"start\":48643},{\"end\":48659,\"start\":48652},{\"end\":49343,\"start\":49339},{\"end\":49354,\"start\":49350},{\"end\":50146,\"start\":50140},{\"end\":50157,\"start\":50152},{\"end\":50170,\"start\":50163},{\"end\":50182,\"start\":50176},{\"end\":50195,\"start\":50189},{\"end\":50894,\"start\":50890},{\"end\":50904,\"start\":50899},{\"end\":51149,\"start\":51143},{\"end\":51166,\"start\":51160},{\"end\":51179,\"start\":51174},{\"end\":51577,\"start\":51572},{\"end\":51586,\"start\":51582},{\"end\":51600,\"start\":51593},{\"end\":51609,\"start\":51605},{\"end\":52003,\"start\":51997},{\"end\":52013,\"start\":52009},{\"end\":52024,\"start\":52019},{\"end\":52039,\"start\":52032},{\"end\":52050,\"start\":52044},{\"end\":52063,\"start\":52058},{\"end\":52074,\"start\":52070},{\"end\":52085,\"start\":52081},{\"end\":52097,\"start\":52093},{\"end\":52118,\"start\":52111},{\"end\":52509,\"start\":52504},{\"end\":52523,\"start\":52518},{\"end\":52541,\"start\":52534},{\"end\":52556,\"start\":52549},{\"end\":53227,\"start\":53222},{\"end\":53241,\"start\":53237},{\"end\":53256,\"start\":53253},{\"end\":53267,\"start\":53263},{\"end\":53269,\"start\":53268},{\"end\":53283,\"start\":53279},{\"end\":53653,\"start\":53648},{\"end\":53663,\"start\":53659},{\"end\":53677,\"start\":53673},{\"end\":54357,\"start\":54350},{\"end\":54377,\"start\":54370},{\"end\":54397,\"start\":54386},{\"end\":55071,\"start\":55064},{\"end\":55084,\"start\":55080},{\"end\":55099,\"start\":55094},{\"end\":55111,\"start\":55107},{\"end\":55132,\"start\":55121},{\"end\":55146,\"start\":55140},{\"end\":55156,\"start\":55152},{\"end\":56085,\"start\":56081},{\"end\":56102,\"start\":56095},{\"end\":56118,\"start\":56115},{\"end\":56133,\"start\":56129},{\"end\":56461,\"start\":56457},{\"end\":56478,\"start\":56471},{\"end\":56488,\"start\":56483},{\"end\":56501,\"start\":56496},{\"end\":56513,\"start\":56508},{\"end\":56526,\"start\":56522},{\"end\":56805,\"start\":56800},{\"end\":56818,\"start\":56814},{\"end\":56832,\"start\":56828},{\"end\":56851,\"start\":56842},{\"end\":56863,\"start\":56857},{\"end\":56879,\"start\":56872},{\"end\":56893,\"start\":56888},{\"end\":56903,\"start\":56900},{\"end\":56915,\"start\":56908},{\"end\":57266,\"start\":57260},{\"end\":57282,\"start\":57278},{\"end\":57300,\"start\":57290},{\"end\":57315,\"start\":57310},{\"end\":58005,\"start\":57999},{\"end\":58022,\"start\":58017},{\"end\":58033,\"start\":58028},{\"end\":58740,\"start\":58736},{\"end\":58757,\"start\":58750},{\"end\":58769,\"start\":58765},{\"end\":58784,\"start\":58778},{\"end\":58797,\"start\":58791},{\"end\":58814,\"start\":58807},{\"end\":58834,\"start\":58829},{\"end\":58854,\"start\":58844},{\"end\":58869,\"start\":58860},{\"end\":58881,\"start\":58876},{\"end\":59317,\"start\":59314},{\"end\":59330,\"start\":59324},{\"end\":59344,\"start\":59337},{\"end\":59355,\"start\":59351},{\"end\":59370,\"start\":59363},{\"end\":59697,\"start\":59689},{\"end\":59714,\"start\":59707},{\"end\":59728,\"start\":59724},{\"end\":59742,\"start\":59735},{\"end\":59759,\"start\":59754},{\"end\":59773,\"start\":59768},{\"end\":60067,\"start\":60060},{\"end\":60080,\"start\":60076},{\"end\":60096,\"start\":60092},{\"end\":60106,\"start\":60101},{\"end\":60126,\"start\":60115},{\"end\":60128,\"start\":60127},{\"end\":60144,\"start\":60138},{\"end\":60160,\"start\":60149},{\"end\":60886,\"start\":60882},{\"end\":60894,\"start\":60892},{\"end\":60905,\"start\":60902},{\"end\":60919,\"start\":60911},{\"end\":61193,\"start\":61184},{\"end\":61209,\"start\":61203},{\"end\":61224,\"start\":61217},{\"end\":61245,\"start\":61236},{\"end\":61247,\"start\":61246},{\"end\":61641,\"start\":61636},{\"end\":61656,\"start\":61648},{\"end\":61670,\"start\":61664},{\"end\":61684,\"start\":61676},{\"end\":61960,\"start\":61954},{\"end\":61974,\"start\":61970},{\"end\":61988,\"start\":61984},{\"end\":62002,\"start\":61997},{\"end\":62019,\"start\":62014},{\"end\":62032,\"start\":62027},{\"end\":62034,\"start\":62033},{\"end\":62048,\"start\":62042},{\"end\":62062,\"start\":62057},{\"end\":62445,\"start\":62441},{\"end\":62461,\"start\":62452},{\"end\":62475,\"start\":62469},{\"end\":62490,\"start\":62485},{\"end\":62501,\"start\":62497},{\"end\":62514,\"start\":62508},{\"end\":63342,\"start\":63339},{\"end\":63352,\"start\":63349},{\"end\":63361,\"start\":63357},{\"end\":63371,\"start\":63367},{\"end\":63380,\"start\":63376},{\"end\":63391,\"start\":63386},{\"end\":63401,\"start\":63398},{\"end\":63650,\"start\":63646},{\"end\":63670,\"start\":63661},{\"end\":63686,\"start\":63678},{\"end\":63983,\"start\":63978},{\"end\":64000,\"start\":63994},{\"end\":64015,\"start\":64009},{\"end\":64859,\"start\":64853},{\"end\":64872,\"start\":64866},{\"end\":64884,\"start\":64878},{\"end\":64896,\"start\":64891},{\"end\":64914,\"start\":64908},{\"end\":64936,\"start\":64930},{\"end\":65314,\"start\":65310},{\"end\":65324,\"start\":65320},{\"end\":65337,\"start\":65329},{\"end\":65351,\"start\":65344},{\"end\":65363,\"start\":65358},{\"end\":65379,\"start\":65372},{\"end\":65679,\"start\":65669},{\"end\":65690,\"start\":65687},{\"end\":65702,\"start\":65697},{\"end\":65716,\"start\":65707},{\"end\":65726,\"start\":65724},{\"end\":65738,\"start\":65733},{\"end\":66076,\"start\":66071},{\"end\":66086,\"start\":66082},{\"end\":66098,\"start\":66094},{\"end\":66112,\"start\":66106},{\"end\":66134,\"start\":66128},{\"end\":66151,\"start\":66144},{\"end\":66167,\"start\":66162},{\"end\":68584,\"start\":68583},{\"end\":68947,\"start\":68946},{\"end\":70700,\"start\":70696},{\"end\":71772,\"start\":71768}]", "bib_author_last_name": "[{\"end\":36531,\"start\":36524},{\"end\":36545,\"start\":36541},{\"end\":36808,\"start\":36805},{\"end\":36823,\"start\":36817},{\"end\":36839,\"start\":36833},{\"end\":37095,\"start\":37087},{\"end\":37106,\"start\":37101},{\"end\":37118,\"start\":37113},{\"end\":37130,\"start\":37125},{\"end\":37150,\"start\":37139},{\"end\":37168,\"start\":37161},{\"end\":37183,\"start\":37175},{\"end\":37717,\"start\":37707},{\"end\":37730,\"start\":37725},{\"end\":37741,\"start\":37736},{\"end\":37761,\"start\":37750},{\"end\":38015,\"start\":38012},{\"end\":38026,\"start\":38022},{\"end\":38040,\"start\":38034},{\"end\":38060,\"start\":38048},{\"end\":38074,\"start\":38068},{\"end\":38760,\"start\":38756},{\"end\":38769,\"start\":38767},{\"end\":38784,\"start\":38779},{\"end\":38801,\"start\":38793},{\"end\":39041,\"start\":39036},{\"end\":39053,\"start\":39049},{\"end\":39067,\"start\":39060},{\"end\":39083,\"start\":39074},{\"end\":39277,\"start\":39272},{\"end\":39295,\"start\":39290},{\"end\":39315,\"start\":39305},{\"end\":39330,\"start\":39319},{\"end\":39346,\"start\":39339},{\"end\":39350,\"start\":39348},{\"end\":39722,\"start\":39713},{\"end\":39741,\"start\":39733},{\"end\":39750,\"start\":39743},{\"end\":40013,\"start\":40007},{\"end\":40025,\"start\":40022},{\"end\":40029,\"start\":40027},{\"end\":40306,\"start\":40303},{\"end\":40319,\"start\":40315},{\"end\":40332,\"start\":40328},{\"end\":40343,\"start\":40336},{\"end\":40356,\"start\":40351},{\"end\":40367,\"start\":40358},{\"end\":40375,\"start\":40371},{\"end\":40386,\"start\":40384},{\"end\":40401,\"start\":40388},{\"end\":40673,\"start\":40665},{\"end\":40688,\"start\":40683},{\"end\":40703,\"start\":40696},{\"end\":40720,\"start\":40711},{\"end\":40735,\"start\":40729},{\"end\":41033,\"start\":41027},{\"end\":41049,\"start\":41044},{\"end\":41061,\"start\":41058},{\"end\":41081,\"start\":41072},{\"end\":42008,\"start\":42001},{\"end\":42021,\"start\":42016},{\"end\":42031,\"start\":42023},{\"end\":42484,\"start\":42481},{\"end\":42496,\"start\":42494},{\"end\":42511,\"start\":42506},{\"end\":42524,\"start\":42522},{\"end\":42537,\"start\":42535},{\"end\":42553,\"start\":42548},{\"end\":43295,\"start\":43284},{\"end\":43313,\"start\":43306},{\"end\":43324,\"start\":43319},{\"end\":43336,\"start\":43331},{\"end\":43938,\"start\":43933},{\"end\":43952,\"start\":43947},{\"end\":43964,\"start\":43961},{\"end\":43981,\"start\":43974},{\"end\":43989,\"start\":43983},{\"end\":44327,\"start\":44323},{\"end\":44334,\"start\":44332},{\"end\":44346,\"start\":44344},{\"end\":44355,\"start\":44352},{\"end\":44367,\"start\":44363},{\"end\":44379,\"start\":44376},{\"end\":44677,\"start\":44672},{\"end\":44692,\"start\":44686},{\"end\":44709,\"start\":44704},{\"end\":44722,\"start\":44717},{\"end\":45161,\"start\":45156},{\"end\":45178,\"start\":45173},{\"end\":45195,\"start\":45186},{\"end\":45467,\"start\":45459},{\"end\":45482,\"start\":45477},{\"end\":45634,\"start\":45631},{\"end\":45645,\"start\":45641},{\"end\":45659,\"start\":45655},{\"end\":45673,\"start\":45669},{\"end\":45688,\"start\":45683},{\"end\":45701,\"start\":45699},{\"end\":46088,\"start\":46079},{\"end\":46102,\"start\":46096},{\"end\":46362,\"start\":46358},{\"end\":46377,\"start\":46374},{\"end\":46392,\"start\":46384},{\"end\":47166,\"start\":47161},{\"end\":47345,\"start\":47339},{\"end\":47362,\"start\":47357},{\"end\":47588,\"start\":47584},{\"end\":47605,\"start\":47597},{\"end\":47620,\"start\":47613},{\"end\":47914,\"start\":47907},{\"end\":47937,\"start\":47922},{\"end\":47945,\"start\":47939},{\"end\":48165,\"start\":48160},{\"end\":48177,\"start\":48173},{\"end\":48189,\"start\":48186},{\"end\":48199,\"start\":48193},{\"end\":48210,\"start\":48206},{\"end\":48228,\"start\":48217},{\"end\":48234,\"start\":48230},{\"end\":48600,\"start\":48595},{\"end\":48611,\"start\":48608},{\"end\":48633,\"start\":48620},{\"end\":48650,\"start\":48645},{\"end\":48668,\"start\":48660},{\"end\":49348,\"start\":49344},{\"end\":49365,\"start\":49355},{\"end\":50150,\"start\":50147},{\"end\":50161,\"start\":50158},{\"end\":50174,\"start\":50171},{\"end\":50187,\"start\":50183},{\"end\":50200,\"start\":50196},{\"end\":50897,\"start\":50895},{\"end\":50912,\"start\":50905},{\"end\":51158,\"start\":51150},{\"end\":51172,\"start\":51167},{\"end\":51191,\"start\":51180},{\"end\":51580,\"start\":51578},{\"end\":51591,\"start\":51587},{\"end\":51603,\"start\":51601},{\"end\":51614,\"start\":51610},{\"end\":52007,\"start\":52004},{\"end\":52017,\"start\":52014},{\"end\":52030,\"start\":52025},{\"end\":52042,\"start\":52040},{\"end\":52056,\"start\":52051},{\"end\":52068,\"start\":52064},{\"end\":52079,\"start\":52075},{\"end\":52091,\"start\":52086},{\"end\":52109,\"start\":52098},{\"end\":52127,\"start\":52119},{\"end\":52136,\"start\":52129},{\"end\":52516,\"start\":52510},{\"end\":52532,\"start\":52524},{\"end\":52547,\"start\":52542},{\"end\":52563,\"start\":52557},{\"end\":53235,\"start\":53228},{\"end\":53251,\"start\":53242},{\"end\":53261,\"start\":53257},{\"end\":53277,\"start\":53270},{\"end\":53288,\"start\":53284},{\"end\":53657,\"start\":53654},{\"end\":53671,\"start\":53664},{\"end\":53685,\"start\":53678},{\"end\":54368,\"start\":54358},{\"end\":54384,\"start\":54378},{\"end\":54405,\"start\":54398},{\"end\":55078,\"start\":55072},{\"end\":55092,\"start\":55085},{\"end\":55105,\"start\":55100},{\"end\":55119,\"start\":55112},{\"end\":55138,\"start\":55133},{\"end\":55150,\"start\":55147},{\"end\":55168,\"start\":55157},{\"end\":56093,\"start\":56086},{\"end\":56113,\"start\":56103},{\"end\":56127,\"start\":56119},{\"end\":56143,\"start\":56134},{\"end\":56469,\"start\":56462},{\"end\":56481,\"start\":56479},{\"end\":56494,\"start\":56489},{\"end\":56506,\"start\":56502},{\"end\":56520,\"start\":56514},{\"end\":56536,\"start\":56527},{\"end\":56812,\"start\":56806},{\"end\":56826,\"start\":56819},{\"end\":56840,\"start\":56833},{\"end\":56855,\"start\":56852},{\"end\":56870,\"start\":56864},{\"end\":56886,\"start\":56880},{\"end\":56898,\"start\":56894},{\"end\":56906,\"start\":56904},{\"end\":56919,\"start\":56916},{\"end\":57276,\"start\":57267},{\"end\":57288,\"start\":57283},{\"end\":57308,\"start\":57301},{\"end\":57321,\"start\":57316},{\"end\":58015,\"start\":58006},{\"end\":58026,\"start\":58023},{\"end\":58039,\"start\":58034},{\"end\":58748,\"start\":58741},{\"end\":58763,\"start\":58758},{\"end\":58776,\"start\":58770},{\"end\":58789,\"start\":58785},{\"end\":58805,\"start\":58798},{\"end\":58827,\"start\":58815},{\"end\":58842,\"start\":58835},{\"end\":58858,\"start\":58855},{\"end\":58874,\"start\":58870},{\"end\":58887,\"start\":58882},{\"end\":59322,\"start\":59318},{\"end\":59335,\"start\":59331},{\"end\":59349,\"start\":59345},{\"end\":59361,\"start\":59356},{\"end\":59376,\"start\":59371},{\"end\":59705,\"start\":59698},{\"end\":59722,\"start\":59715},{\"end\":59733,\"start\":59729},{\"end\":59752,\"start\":59743},{\"end\":59766,\"start\":59760},{\"end\":59783,\"start\":59774},{\"end\":60074,\"start\":60068},{\"end\":60090,\"start\":60081},{\"end\":60099,\"start\":60097},{\"end\":60113,\"start\":60107},{\"end\":60136,\"start\":60129},{\"end\":60147,\"start\":60145},{\"end\":60166,\"start\":60161},{\"end\":60890,\"start\":60887},{\"end\":60900,\"start\":60895},{\"end\":60909,\"start\":60906},{\"end\":60923,\"start\":60920},{\"end\":61201,\"start\":61194},{\"end\":61215,\"start\":61210},{\"end\":61234,\"start\":61225},{\"end\":61253,\"start\":61248},{\"end\":61646,\"start\":61642},{\"end\":61662,\"start\":61657},{\"end\":61674,\"start\":61671},{\"end\":61694,\"start\":61685},{\"end\":61968,\"start\":61961},{\"end\":61982,\"start\":61975},{\"end\":61995,\"start\":61989},{\"end\":62012,\"start\":62003},{\"end\":62025,\"start\":62020},{\"end\":62040,\"start\":62035},{\"end\":62055,\"start\":62049},{\"end\":62073,\"start\":62063},{\"end\":62450,\"start\":62446},{\"end\":62467,\"start\":62462},{\"end\":62483,\"start\":62476},{\"end\":62495,\"start\":62491},{\"end\":62506,\"start\":62502},{\"end\":62521,\"start\":62515},{\"end\":63347,\"start\":63343},{\"end\":63355,\"start\":63353},{\"end\":63365,\"start\":63362},{\"end\":63374,\"start\":63372},{\"end\":63384,\"start\":63381},{\"end\":63396,\"start\":63392},{\"end\":63404,\"start\":63402},{\"end\":63659,\"start\":63651},{\"end\":63676,\"start\":63671},{\"end\":63693,\"start\":63687},{\"end\":63992,\"start\":63984},{\"end\":64007,\"start\":64001},{\"end\":64022,\"start\":64016},{\"end\":64864,\"start\":64860},{\"end\":64876,\"start\":64873},{\"end\":64889,\"start\":64885},{\"end\":64906,\"start\":64897},{\"end\":64928,\"start\":64915},{\"end\":64939,\"start\":64937},{\"end\":64946,\"start\":64941},{\"end\":65318,\"start\":65315},{\"end\":65327,\"start\":65325},{\"end\":65342,\"start\":65338},{\"end\":65356,\"start\":65352},{\"end\":65370,\"start\":65364},{\"end\":65385,\"start\":65380},{\"end\":65685,\"start\":65680},{\"end\":65695,\"start\":65691},{\"end\":65705,\"start\":65703},{\"end\":65722,\"start\":65717},{\"end\":65731,\"start\":65727},{\"end\":65743,\"start\":65739},{\"end\":66080,\"start\":66077},{\"end\":66092,\"start\":66087},{\"end\":66104,\"start\":66099},{\"end\":66126,\"start\":66113},{\"end\":66142,\"start\":66135},{\"end\":66160,\"start\":66152},{\"end\":66174,\"start\":66168},{\"end\":68593,\"start\":68585},{\"end\":68968,\"start\":68948},{\"end\":68978,\"start\":68970},{\"end\":69336,\"start\":69332}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1809.10853\",\"id\":\"b0\"},\"end\":36770,\"start\":36517},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":202539738},\"end\":37021,\"start\":36772},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":13385138},\"end\":37640,\"start\":37023},{\"attributes\":{\"id\":\"b3\"},\"end\":37904,\"start\":37642},{\"attributes\":{\"doi\":\"10.18653/v1/S17-2001\",\"id\":\"b4\",\"matched_paper_id\":4421747},\"end\":38700,\"start\":37906},{\"attributes\":{\"doi\":\"arXiv:1604.06174\",\"id\":\"b5\"},\"end\":38976,\"start\":38702},{\"attributes\":{\"doi\":\"arXiv:1904.10509\",\"id\":\"b6\"},\"end\":39264,\"start\":38978},{\"attributes\":{\"doi\":\"arXiv:1907.04829\",\"id\":\"b7\"},\"end\":39653,\"start\":39266},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":8587959},\"end\":39968,\"start\":39655},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":7138078},\"end\":40221,\"start\":39970},{\"attributes\":{\"doi\":\"arXiv:1901.02860\",\"id\":\"b10\"},\"end\":40653,\"start\":40223},{\"attributes\":{\"doi\":\"arXiv:1807.03819\",\"id\":\"b11\"},\"end\":40937,\"start\":40655},{\"attributes\":{\"doi\":\"10.18653/v1/N19-1423\",\"id\":\"b12\",\"matched_paper_id\":52967399},\"end\":41934,\"start\":40939},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":16639476},\"end\":42396,\"start\":41936},{\"attributes\":{\"doi\":\"10.18653/v1/D17-1254\",\"id\":\"b14\",\"matched_paper_id\":2116604},\"end\":43216,\"start\":42398},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":195352006},\"end\":43851,\"start\":43218},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":8869447},\"end\":44259,\"start\":43853},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":174799716},\"end\":44620,\"start\":44261},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6483732},\"end\":45074,\"start\":44622},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":18229335},\"end\":45449,\"start\":45076},{\"attributes\":{\"id\":\"b20\"},\"end\":45588,\"start\":45451},{\"attributes\":{\"doi\":\"10.18653/v1/n19-1122\",\"id\":\"b21\",\"matched_paper_id\":102350771},\"end\":46073,\"start\":45590},{\"attributes\":{\"id\":\"b22\"},\"end\":46278,\"start\":46075},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2937095},\"end\":47124,\"start\":46280},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":45706253},\"end\":47268,\"start\":47126},{\"attributes\":{\"id\":\"b25\"},\"end\":47529,\"start\":47270},{\"attributes\":{\"id\":\"b26\"},\"end\":47815,\"start\":47531},{\"attributes\":{\"id\":\"b27\"},\"end\":48151,\"start\":47817},{\"attributes\":{\"id\":\"b28\"},\"end\":48532,\"start\":48153},{\"attributes\":{\"id\":\"b29\"},\"end\":49226,\"start\":48534},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":52051958},\"end\":50071,\"start\":49228},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":6826032},\"end\":50832,\"start\":50073},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":2407601},\"end\":51110,\"start\":50834},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":15710851},\"end\":51482,\"start\":51112},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":2878258},\"end\":51995,\"start\":51484},{\"attributes\":{\"id\":\"b35\"},\"end\":52449,\"start\":51997},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":9447219},\"end\":53143,\"start\":52451},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":16447573},\"end\":53568,\"start\":53145},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":196181734},\"end\":54301,\"start\":53570},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":1957433},\"end\":55020,\"start\":54303},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3626819},\"end\":56018,\"start\":55022},{\"attributes\":{\"id\":\"b41\"},\"end\":56402,\"start\":56020},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":160025533},\"end\":56715,\"start\":56404},{\"attributes\":{\"id\":\"b43\"},\"end\":57197,\"start\":56717},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":11816014},\"end\":57937,\"start\":57199},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":47018994},\"end\":58683,\"start\":57939},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":53236433},\"end\":59228,\"start\":58685},{\"attributes\":{\"id\":\"b47\"},\"end\":59600,\"start\":59230},{\"attributes\":{\"id\":\"b48\"},\"end\":59979,\"start\":59602},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":990233},\"end\":60821,\"start\":59981},{\"attributes\":{\"id\":\"b50\"},\"end\":61099,\"start\":60823},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":1023605},\"end\":61537,\"start\":61101},{\"attributes\":{\"id\":\"b52\"},\"end\":61925,\"start\":61539},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":13756489},\"end\":62352,\"start\":61927},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":5034059},\"end\":63240,\"start\":62354},{\"attributes\":{\"id\":\"b55\"},\"end\":63644,\"start\":63242},{\"attributes\":{\"id\":\"b56\"},\"end\":63896,\"start\":63646},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":3432876},\"end\":64851,\"start\":63898},{\"attributes\":{\"id\":\"b58\"},\"end\":65249,\"start\":64853},{\"attributes\":{\"id\":\"b59\"},\"end\":65595,\"start\":65251},{\"attributes\":{\"id\":\"b60\"},\"end\":65965,\"start\":65597},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":6866988},\"end\":66729,\"start\":65967},{\"attributes\":{\"id\":\"b62\"},\"end\":66929,\"start\":66731},{\"attributes\":{\"id\":\"b63\"},\"end\":67043,\"start\":66931},{\"attributes\":{\"id\":\"b64\"},\"end\":67616,\"start\":67045},{\"attributes\":{\"id\":\"b65\"},\"end\":67776,\"start\":67618},{\"attributes\":{\"id\":\"b66\"},\"end\":68469,\"start\":67778},{\"attributes\":{\"id\":\"b67\"},\"end\":68905,\"start\":68471},{\"attributes\":{\"id\":\"b68\"},\"end\":69121,\"start\":68907},{\"attributes\":{\"id\":\"b69\"},\"end\":70304,\"start\":69123},{\"attributes\":{\"id\":\"b70\"},\"end\":71494,\"start\":70306},{\"attributes\":{\"id\":\"b71\"},\"end\":72657,\"start\":71496}]", "bib_title": "[{\"end\":36795,\"start\":36772},{\"end\":37081,\"start\":37023},{\"end\":38003,\"start\":37906},{\"end\":39706,\"start\":39655},{\"end\":40003,\"start\":39970},{\"end\":41019,\"start\":40939},{\"end\":41997,\"start\":41936},{\"end\":42475,\"start\":42398},{\"end\":43275,\"start\":43218},{\"end\":43929,\"start\":43853},{\"end\":44313,\"start\":44261},{\"end\":44662,\"start\":44622},{\"end\":45144,\"start\":45076},{\"end\":45625,\"start\":45590},{\"end\":46350,\"start\":46280},{\"end\":47151,\"start\":47126},{\"end\":48588,\"start\":48534},{\"end\":49337,\"start\":49228},{\"end\":50138,\"start\":50073},{\"end\":50888,\"start\":50834},{\"end\":51141,\"start\":51112},{\"end\":51570,\"start\":51484},{\"end\":52502,\"start\":52451},{\"end\":53220,\"start\":53145},{\"end\":53646,\"start\":53570},{\"end\":54348,\"start\":54303},{\"end\":55062,\"start\":55022},{\"end\":56455,\"start\":56404},{\"end\":57258,\"start\":57199},{\"end\":57997,\"start\":57939},{\"end\":58734,\"start\":58685},{\"end\":60058,\"start\":59981},{\"end\":61182,\"start\":61101},{\"end\":61952,\"start\":61927},{\"end\":62439,\"start\":62354},{\"end\":63976,\"start\":63898},{\"end\":66069,\"start\":65967},{\"end\":66819,\"start\":66731},{\"end\":67233,\"start\":67045},{\"end\":67686,\"start\":67618},{\"end\":67933,\"start\":67778},{\"end\":68581,\"start\":68471},{\"end\":68944,\"start\":68907},{\"end\":70694,\"start\":70306},{\"end\":71766,\"start\":71496}]", "bib_author": "[{\"end\":36533,\"start\":36517},{\"end\":36547,\"start\":36533},{\"end\":36810,\"start\":36797},{\"end\":36825,\"start\":36810},{\"end\":36841,\"start\":36825},{\"end\":37097,\"start\":37083},{\"end\":37108,\"start\":37097},{\"end\":37120,\"start\":37108},{\"end\":37132,\"start\":37120},{\"end\":37152,\"start\":37132},{\"end\":37170,\"start\":37152},{\"end\":37185,\"start\":37170},{\"end\":37719,\"start\":37701},{\"end\":37732,\"start\":37719},{\"end\":37743,\"start\":37732},{\"end\":37763,\"start\":37743},{\"end\":38017,\"start\":38005},{\"end\":38028,\"start\":38017},{\"end\":38042,\"start\":38028},{\"end\":38062,\"start\":38042},{\"end\":38076,\"start\":38062},{\"end\":38762,\"start\":38749},{\"end\":38771,\"start\":38762},{\"end\":38786,\"start\":38771},{\"end\":38803,\"start\":38786},{\"end\":39043,\"start\":39030},{\"end\":39055,\"start\":39043},{\"end\":39069,\"start\":39055},{\"end\":39085,\"start\":39069},{\"end\":39279,\"start\":39266},{\"end\":39297,\"start\":39279},{\"end\":39317,\"start\":39297},{\"end\":39332,\"start\":39317},{\"end\":39348,\"start\":39332},{\"end\":39352,\"start\":39348},{\"end\":39724,\"start\":39708},{\"end\":39743,\"start\":39724},{\"end\":39752,\"start\":39743},{\"end\":40015,\"start\":40005},{\"end\":40027,\"start\":40015},{\"end\":40031,\"start\":40027},{\"end\":40308,\"start\":40296},{\"end\":40321,\"start\":40308},{\"end\":40334,\"start\":40321},{\"end\":40345,\"start\":40334},{\"end\":40358,\"start\":40345},{\"end\":40369,\"start\":40358},{\"end\":40377,\"start\":40369},{\"end\":40388,\"start\":40377},{\"end\":40403,\"start\":40388},{\"end\":40675,\"start\":40657},{\"end\":40690,\"start\":40675},{\"end\":40705,\"start\":40690},{\"end\":40722,\"start\":40705},{\"end\":40737,\"start\":40722},{\"end\":41035,\"start\":41021},{\"end\":41051,\"start\":41035},{\"end\":41063,\"start\":41051},{\"end\":41083,\"start\":41063},{\"end\":42010,\"start\":41999},{\"end\":42023,\"start\":42010},{\"end\":42033,\"start\":42023},{\"end\":42486,\"start\":42477},{\"end\":42498,\"start\":42486},{\"end\":42513,\"start\":42498},{\"end\":42526,\"start\":42513},{\"end\":42539,\"start\":42526},{\"end\":42555,\"start\":42539},{\"end\":43297,\"start\":43277},{\"end\":43315,\"start\":43297},{\"end\":43326,\"start\":43315},{\"end\":43338,\"start\":43326},{\"end\":43940,\"start\":43931},{\"end\":43954,\"start\":43940},{\"end\":43966,\"start\":43954},{\"end\":43983,\"start\":43966},{\"end\":43991,\"start\":43983},{\"end\":44329,\"start\":44315},{\"end\":44336,\"start\":44329},{\"end\":44348,\"start\":44336},{\"end\":44357,\"start\":44348},{\"end\":44369,\"start\":44357},{\"end\":44381,\"start\":44369},{\"end\":44679,\"start\":44664},{\"end\":44694,\"start\":44679},{\"end\":44711,\"start\":44694},{\"end\":44724,\"start\":44711},{\"end\":45163,\"start\":45146},{\"end\":45180,\"start\":45163},{\"end\":45197,\"start\":45180},{\"end\":45469,\"start\":45453},{\"end\":45484,\"start\":45469},{\"end\":45636,\"start\":45627},{\"end\":45647,\"start\":45636},{\"end\":45661,\"start\":45647},{\"end\":45675,\"start\":45661},{\"end\":45690,\"start\":45675},{\"end\":45703,\"start\":45690},{\"end\":46090,\"start\":46075},{\"end\":46104,\"start\":46090},{\"end\":46364,\"start\":46352},{\"end\":46379,\"start\":46364},{\"end\":46394,\"start\":46379},{\"end\":47168,\"start\":47153},{\"end\":47347,\"start\":47332},{\"end\":47364,\"start\":47347},{\"end\":47590,\"start\":47576},{\"end\":47607,\"start\":47590},{\"end\":47622,\"start\":47607},{\"end\":47916,\"start\":47900},{\"end\":47939,\"start\":47916},{\"end\":47947,\"start\":47939},{\"end\":48167,\"start\":48153},{\"end\":48179,\"start\":48167},{\"end\":48191,\"start\":48179},{\"end\":48201,\"start\":48191},{\"end\":48212,\"start\":48201},{\"end\":48230,\"start\":48212},{\"end\":48236,\"start\":48230},{\"end\":48602,\"start\":48590},{\"end\":48613,\"start\":48602},{\"end\":48635,\"start\":48613},{\"end\":48652,\"start\":48635},{\"end\":48670,\"start\":48652},{\"end\":49350,\"start\":49339},{\"end\":49367,\"start\":49350},{\"end\":50152,\"start\":50140},{\"end\":50163,\"start\":50152},{\"end\":50176,\"start\":50163},{\"end\":50189,\"start\":50176},{\"end\":50202,\"start\":50189},{\"end\":50899,\"start\":50890},{\"end\":50914,\"start\":50899},{\"end\":51160,\"start\":51143},{\"end\":51174,\"start\":51160},{\"end\":51193,\"start\":51174},{\"end\":51582,\"start\":51572},{\"end\":51593,\"start\":51582},{\"end\":51605,\"start\":51593},{\"end\":51616,\"start\":51605},{\"end\":52009,\"start\":51997},{\"end\":52019,\"start\":52009},{\"end\":52032,\"start\":52019},{\"end\":52044,\"start\":52032},{\"end\":52058,\"start\":52044},{\"end\":52070,\"start\":52058},{\"end\":52081,\"start\":52070},{\"end\":52093,\"start\":52081},{\"end\":52111,\"start\":52093},{\"end\":52129,\"start\":52111},{\"end\":52138,\"start\":52129},{\"end\":52518,\"start\":52504},{\"end\":52534,\"start\":52518},{\"end\":52549,\"start\":52534},{\"end\":52565,\"start\":52549},{\"end\":53237,\"start\":53222},{\"end\":53253,\"start\":53237},{\"end\":53263,\"start\":53253},{\"end\":53279,\"start\":53263},{\"end\":53290,\"start\":53279},{\"end\":53659,\"start\":53648},{\"end\":53673,\"start\":53659},{\"end\":53687,\"start\":53673},{\"end\":54370,\"start\":54350},{\"end\":54386,\"start\":54370},{\"end\":54407,\"start\":54386},{\"end\":55080,\"start\":55064},{\"end\":55094,\"start\":55080},{\"end\":55107,\"start\":55094},{\"end\":55121,\"start\":55107},{\"end\":55140,\"start\":55121},{\"end\":55152,\"start\":55140},{\"end\":55170,\"start\":55152},{\"end\":56095,\"start\":56081},{\"end\":56115,\"start\":56095},{\"end\":56129,\"start\":56115},{\"end\":56145,\"start\":56129},{\"end\":56471,\"start\":56457},{\"end\":56483,\"start\":56471},{\"end\":56496,\"start\":56483},{\"end\":56508,\"start\":56496},{\"end\":56522,\"start\":56508},{\"end\":56538,\"start\":56522},{\"end\":56814,\"start\":56800},{\"end\":56828,\"start\":56814},{\"end\":56842,\"start\":56828},{\"end\":56857,\"start\":56842},{\"end\":56872,\"start\":56857},{\"end\":56888,\"start\":56872},{\"end\":56900,\"start\":56888},{\"end\":56908,\"start\":56900},{\"end\":56921,\"start\":56908},{\"end\":57278,\"start\":57260},{\"end\":57290,\"start\":57278},{\"end\":57310,\"start\":57290},{\"end\":57323,\"start\":57310},{\"end\":58017,\"start\":57999},{\"end\":58028,\"start\":58017},{\"end\":58041,\"start\":58028},{\"end\":58750,\"start\":58736},{\"end\":58765,\"start\":58750},{\"end\":58778,\"start\":58765},{\"end\":58791,\"start\":58778},{\"end\":58807,\"start\":58791},{\"end\":58829,\"start\":58807},{\"end\":58844,\"start\":58829},{\"end\":58860,\"start\":58844},{\"end\":58876,\"start\":58860},{\"end\":58889,\"start\":58876},{\"end\":59324,\"start\":59314},{\"end\":59337,\"start\":59324},{\"end\":59351,\"start\":59337},{\"end\":59363,\"start\":59351},{\"end\":59378,\"start\":59363},{\"end\":59707,\"start\":59689},{\"end\":59724,\"start\":59707},{\"end\":59735,\"start\":59724},{\"end\":59754,\"start\":59735},{\"end\":59768,\"start\":59754},{\"end\":59785,\"start\":59768},{\"end\":60076,\"start\":60060},{\"end\":60092,\"start\":60076},{\"end\":60101,\"start\":60092},{\"end\":60115,\"start\":60101},{\"end\":60138,\"start\":60115},{\"end\":60149,\"start\":60138},{\"end\":60168,\"start\":60149},{\"end\":60892,\"start\":60882},{\"end\":60902,\"start\":60892},{\"end\":60911,\"start\":60902},{\"end\":60925,\"start\":60911},{\"end\":61203,\"start\":61184},{\"end\":61217,\"start\":61203},{\"end\":61236,\"start\":61217},{\"end\":61255,\"start\":61236},{\"end\":61648,\"start\":61636},{\"end\":61664,\"start\":61648},{\"end\":61676,\"start\":61664},{\"end\":61696,\"start\":61676},{\"end\":61970,\"start\":61954},{\"end\":61984,\"start\":61970},{\"end\":61997,\"start\":61984},{\"end\":62014,\"start\":61997},{\"end\":62027,\"start\":62014},{\"end\":62042,\"start\":62027},{\"end\":62057,\"start\":62042},{\"end\":62075,\"start\":62057},{\"end\":62452,\"start\":62441},{\"end\":62469,\"start\":62452},{\"end\":62485,\"start\":62469},{\"end\":62497,\"start\":62485},{\"end\":62508,\"start\":62497},{\"end\":62523,\"start\":62508},{\"end\":63349,\"start\":63339},{\"end\":63357,\"start\":63349},{\"end\":63367,\"start\":63357},{\"end\":63376,\"start\":63367},{\"end\":63386,\"start\":63376},{\"end\":63398,\"start\":63386},{\"end\":63406,\"start\":63398},{\"end\":63661,\"start\":63646},{\"end\":63678,\"start\":63661},{\"end\":63695,\"start\":63678},{\"end\":63994,\"start\":63978},{\"end\":64009,\"start\":63994},{\"end\":64024,\"start\":64009},{\"end\":64866,\"start\":64853},{\"end\":64878,\"start\":64866},{\"end\":64891,\"start\":64878},{\"end\":64908,\"start\":64891},{\"end\":64930,\"start\":64908},{\"end\":64941,\"start\":64930},{\"end\":64948,\"start\":64941},{\"end\":65320,\"start\":65310},{\"end\":65329,\"start\":65320},{\"end\":65344,\"start\":65329},{\"end\":65358,\"start\":65344},{\"end\":65372,\"start\":65358},{\"end\":65387,\"start\":65372},{\"end\":65687,\"start\":65669},{\"end\":65697,\"start\":65687},{\"end\":65707,\"start\":65697},{\"end\":65724,\"start\":65707},{\"end\":65733,\"start\":65724},{\"end\":65745,\"start\":65733},{\"end\":66082,\"start\":66071},{\"end\":66094,\"start\":66082},{\"end\":66106,\"start\":66094},{\"end\":66128,\"start\":66106},{\"end\":66144,\"start\":66128},{\"end\":66162,\"start\":66144},{\"end\":66176,\"start\":66162},{\"end\":68595,\"start\":68583},{\"end\":68970,\"start\":68946},{\"end\":68980,\"start\":68970},{\"end\":69338,\"start\":69332},{\"end\":70703,\"start\":70696},{\"end\":71775,\"start\":71768}]", "bib_venue": "[{\"end\":36622,\"start\":36563},{\"end\":36888,\"start\":36841},{\"end\":37271,\"start\":37185},{\"end\":37699,\"start\":37642},{\"end\":38165,\"start\":38096},{\"end\":38747,\"start\":38702},{\"end\":39028,\"start\":38978},{\"end\":39438,\"start\":39368},{\"end\":39788,\"start\":39752},{\"end\":40080,\"start\":40031},{\"end\":40294,\"start\":40223},{\"end\":41245,\"start\":41103},{\"end\":42106,\"start\":42033},{\"end\":42661,\"start\":42575},{\"end\":43415,\"start\":43338},{\"end\":44040,\"start\":43991},{\"end\":44425,\"start\":44381},{\"end\":44792,\"start\":44724},{\"end\":45222,\"start\":45197},{\"end\":45514,\"start\":45484},{\"end\":45796,\"start\":45749},{\"end\":46155,\"start\":46120},{\"end\":46556,\"start\":46414},{\"end\":47185,\"start\":47168},{\"end\":47330,\"start\":47270},{\"end\":47574,\"start\":47531},{\"end\":47898,\"start\":47817},{\"end\":48321,\"start\":48252},{\"end\":48759,\"start\":48670},{\"end\":49496,\"start\":49387},{\"end\":50308,\"start\":50222},{\"end\":50942,\"start\":50914},{\"end\":51288,\"start\":51193},{\"end\":51693,\"start\":51616},{\"end\":52200,\"start\":52154},{\"end\":52614,\"start\":52565},{\"end\":53339,\"start\":53290},{\"end\":53794,\"start\":53707},{\"end\":54520,\"start\":54426},{\"end\":55332,\"start\":55190},{\"end\":56079,\"start\":56020},{\"end\":56549,\"start\":56538},{\"end\":56798,\"start\":56717},{\"end\":57429,\"start\":57343},{\"end\":58148,\"start\":58061},{\"end\":58938,\"start\":58889},{\"end\":59312,\"start\":59230},{\"end\":59687,\"start\":59602},{\"end\":60254,\"start\":60168},{\"end\":60880,\"start\":60823},{\"end\":61310,\"start\":61255},{\"end\":61634,\"start\":61539},{\"end\":62124,\"start\":62075},{\"end\":62645,\"start\":62543},{\"end\":63337,\"start\":63242},{\"end\":63749,\"start\":63711},{\"end\":64186,\"start\":64044},{\"end\":65029,\"start\":64964},{\"end\":65308,\"start\":65251},{\"end\":65667,\"start\":65597},{\"end\":66243,\"start\":66176},{\"end\":66826,\"start\":66821},{\"end\":66986,\"start\":66931},{\"end\":67329,\"start\":67275},{\"end\":67693,\"start\":67688},{\"end\":67983,\"start\":67935},{\"end\":68622,\"start\":68595},{\"end\":69000,\"start\":68980},{\"end\":69330,\"start\":69123},{\"end\":70779,\"start\":70703},{\"end\":71924,\"start\":71775},{\"end\":37350,\"start\":37273},{\"end\":38238,\"start\":38167},{\"end\":41396,\"start\":41247},{\"end\":42166,\"start\":42108},{\"end\":42753,\"start\":42663},{\"end\":43485,\"start\":43417},{\"end\":44847,\"start\":44794},{\"end\":45830,\"start\":45798},{\"end\":46685,\"start\":46558},{\"end\":48853,\"start\":48761},{\"end\":49609,\"start\":49498},{\"end\":50400,\"start\":50310},{\"end\":50971,\"start\":50944},{\"end\":51757,\"start\":51695},{\"end\":53883,\"start\":53796},{\"end\":54612,\"start\":54522},{\"end\":55483,\"start\":55334},{\"end\":57515,\"start\":57431},{\"end\":58242,\"start\":58150},{\"end\":60351,\"start\":60256},{\"end\":62751,\"start\":62647},{\"end\":64337,\"start\":64188},{\"end\":66297,\"start\":66245}]"}}}, "year": 2023, "month": 12, "day": 17}