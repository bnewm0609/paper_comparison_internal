{"id": 260887670, "updated": "2023-11-20 11:36:19.137", "metadata": {"title": "VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use", "authors": "[{\"first\":\"Yonatan\",\"last\":\"Bitton\",\"middle\":[]},{\"first\":\"Hritik\",\"last\":\"Bansal\",\"middle\":[]},{\"first\":\"Jack\",\"last\":\"Hessel\",\"middle\":[]},{\"first\":\"Rulin\",\"last\":\"Shao\",\"middle\":[]},{\"first\":\"Wanrong\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Anas\",\"last\":\"Awadalla\",\"middle\":[]},{\"first\":\"Josh\",\"last\":\"Gardner\",\"middle\":[]},{\"first\":\"Rohan\",\"last\":\"Taori\",\"middle\":[]},{\"first\":\"Ludwig\",\"last\":\"Schimdt\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "We introduce VisIT-Bench ( Vis ual I ns T ruction Bench mark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 \u201cinstruction families\u201d that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps between models and references using both human and automatic evaluations; e.g., the top-performing instruction-following model wins against the GPT-4 reference in just 27% of the comparison. VisIT-Bench is dynamic to participate, practitioners simply submit their model\u2019s response on the project website; Data, code and leaderboard is available at https://visit-bench.github.io/ .", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2308-06595", "doi": "10.48550/arxiv.2308.06595"}}, "content": {"source": {"pdf_hash": "d6c2523ab97416c2692cbbeab082ed1790e8e55e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2308.06595v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7d9ef3f7978c720b6d7e0a466bae4d19a1b53dfd", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/d6c2523ab97416c2692cbbeab082ed1790e8e55e.txt", "contents": "\nVisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use\n\n\nYonatan Bitton \nHritik Bansal \nJack Hessel \nRulin Shao \nWanrong Zhu \nAnas Awadalla \nJosh Gardner \nRohan Taori \nLudwig Schimdt \nVisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use\n\nWe introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for realworld use. Our starting point is curating 70 \"instruction families\" that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instructionspecific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps between models and references using both human and automatic evaluations; e.g., the top-performing instruction-following model wins against the GPT-4 reference in just 27% of the comparison. VisIT-Bench is dynamic to participate, practitioners simply submit their model's response on the project website; Data, code and leaderboard is available at https://visit-bench.github.io/.InstructionCreate a catchy title for a country song based upon the advice printed on the wheel cover. * Equal contribution. Contact\n\nInstruction-Conditioned Caption\n\nThere is a jeep parked on the street with the top down. There is a wheel cover on the spare tire in the back with an inscription that says, \"Quit your Job, Buy a Ticket, Get a Tan, Fall in Love, Never Return.\"  \n\n\nHuman-Verified GPT4 Response\n\n\nIntroduction\n\nA long-standing challenge for artificial intelligence is to build general-purpose assistants that can, in collaboration with humans, solve diverse and never-before-seen tasks [1]. For textual tasks, several recent works [2,3,4,5,6,7] have shown that fine-tuning language models such as GPT-3 and LLaMA with supervised instruction+response examples [8,9,10] enables them to respond to imperative requests and questions without task-specific training. Zero-shot generalization is promising not only for standard academic benchmarks, butperhaps more-so -for creative, useful, and real-world queries that downstream users of language technologies are likely to make.\n\nOn the multimodal side, recent instruction-following vision-language models also provide a zero-shot interface. Given an image (or multiple images) and a query (e.g., \"how many apples are in this image?\" or \"What is this?\" or \"Write a poem in the style of Robert Frost about this scene.\") a textual response is provided. Recent works like OpenFlamingo [11,12], LLaVA [13] and others [14,15,16,17,18], have implemented this interface with promising initial results. Although standard benchmarks like VQAv2 [19] and COCO captioning [20] are commonly used to assess performance, less is know about how models perform on broader, open-ended queries that resemble real-world user behavior. Evaluations of such queries typically rely on informal and qualitative approaches.\n\nTo support quantitative evaluation for this setting, we present VisIT-Bench (Visual InsTruction Benchmark), a dynamic benchmark consisting of 592 challenging visionlanguage instructions. Each instance contains an instruction, input image(s), a instructionconditioned caption (a human-crafted caption for the image(s)/instruction), and a human verified reference (Figure 1). Instructions are image-contextual imperative requests or questions, e.g., for an image of pancakes, a user asks \"how can I cook this in a healthy way?\". Different from existing zero-shot evaluations, many of the instructions focus on open-ended generation requests (e.g., \"write a poem...\" or \"what should I bring if I were to visit here?\").\n\nWe created VisIT-Bench to cover a wide array of \"instruction families\". Our starting point was a set of 70 \"wish-list\" tasks such as \"home renovation\" and \"gardening tips\" collected by the authors: 1 each requiring varied high-level skills from recognition to complex reasoning ( Figure 2). We derived 25/70 instruction families from benchmark tasks such as Visual Question Answering (VQA) [21] and robust change captioning [22] into a chatbot-style format (this reformatting differs from prior work [14,17,13], as we focus on open-ended chatbot style responses.). Notably, 10 of these repurposed tasks involve multiple images.\n\nWe started with 10 images for each instruction family. Our annotators, guided by an example, create a new instruction, and provide a (permissively licensed) image. For each instruction, we next collect instruction-conditioned captions -unlike prior work [23,24] these descriptions are designed not only to describe the image in general, but also, surface information targeted to the instruction. Finally, we use instruction-conditioned captions to generate a reference candidate output from GPT-4; an additional human verification step discards GPT-4 references deemed to be incorrect.\n\nWe conduct a large-scale empirical comparison of multimodal instruction-following models using VisIT-Bench ( \u00a74). We first gather predictions for each instance from 7 candidate models. Then, we collect 5K human judgements of output quality by pitting model outputs head-to-head, and (in a forced-choice setup) crowd-sourcing pairwise preference judgements. This analysis not only reveals significant differences between models (e.g., that LLaVA-13b [13] is generally preferred to Panda [18]), but also, that the human verified references in our corpus are preferred significantly more than the ones generated using multimodal models. We summarize head-to-head comparisons with two metrics: 1) Elo ratings [25,26],\n\n\nReasoning Over Plots\n\nHow much is the browser usage for Firefox and Safari?\n\n\nArt Knowledge\n\nTeach me about this painting.\n\n\nRecognition\n\nWhere is this? \n\n\nHome Renovation\n\nHere is a photo of my bathroom. How can I design it nicer? Figure 2: A sample from the 70 instruction families in VisIT-Bench representing tasks we envision instruction-following vision-language models should be able to follow.\n\nwhich provide relative \"skill\" rating estimates encoding the probability that model A will be preferred to model B; and 2) win rate versus our references, which provides an absolute metric. The best model according to human judgement is LLaMA-Adapter-v2 [16], yet it only wins in a pairwise setting against the reference in 27.4% of cases.\n\nFinally, we design an automated evaluation for VisIT-Bench, utilizing GPT-4 to rank pairs of model responses based on factors like correctness, relevance, and fluency. Using the instruction-conditioned caption and the instruction, GPT-4 determines the better response between two options, expediting iteration compared to human preferences. We explore reference-free and reference-backed versions of this metric. Compared to various metrics (BLEU-4 [27], ROUGE-L [28], METEOR [29], CIDEr [30], and BERTScore [31]), our evaluation aligns best with human preferences. For example, it achieves a 94% agreement rate in the cases where all five annotators agree. See Figure 7 for a schematic of the process.\n\nWhile it is difficult to a priori envision all possible scenarios under which more performant multimodal chatbots might be used, we hope VisIT-Bench can provide a path to improving vision-language models \"in the wild.\" Table 1 presents a summary of our contributions in comparison to the recent works [32,14,17,33,34,35] in the evaluation of multimodal chatbots. We publicly release VisIT-Bench data, code, and automatic metrics to facilitate future model evaluations, available in https://visit-bench.github.io/.   VisIT-Bench was built to emulate real-world applications of multimodal models through image-text tasks, creating an extensive and practical benchmark. These tasks, or 'instruction families', are seen as key capabilities of a high-performing vision-and-language model. Although our selections are not exhaustive, they provide a broad basis for evaluating beyond academic benchmarks. We prioritize family coverage vs. number of instances-per-task. The final corpus, comprising 592 instances and 1,159 public images, can be found at VisIT-Bench Sheet and VisIT-Bench Sheet Multi-Images. VisIT-Bench instances are either from 45 newly assembled instruction families or reformatted from 25 existing datasets (see Table 5). Notably, 10 instruction families cater to multi-image query scenarios (e.g., Figure 4).\n\u2717 \u2713 \u2717 \u2717 \u2717 \u2717 \u2713 Video \u2717 \u2717 \u2713 \u2713 \u2717 \u2717 \u2717 Multi-Turn Conversations \u2713 \u2713 \u2713 \u2713 \u2713 \u2717 \u2717 Multilingual Conversations \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 \u2717 Instruction-conditioned Captions \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2713 Chatbot-style Responses \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2713 Dataset-specific Evaluation \u2713 \u2713 \u2713 \u2713 \u2713 \u2717 \u2717 Human Evaluation \u2717 \u2713 \u2717 \u2717 \u2713 \u2717 \u2713 Auto/GPT-4 Evaluation \u2717 \u2713 \u2717 \u2713 \u2717 \u2713 \u2713 Win-rates* \u2717 \u2713 \u2717 \u2713 \u2717 \u2713 \u2713 Elo Rating \u2717 \u2717 \u2717 \u2717 \u2713 \u2717 \u2713 2\n\nInstruction-Conditioned Caption Generation\n\n\nData Collection\n\nThe authors of this work perform an initial annotation step of curating instruction families. For each instruction family not derived from an existing task (45 out of 70), we designate a name for the family (e.g., \"Contextual Knowledge of Events\") and identify an imageinstruction pair that exemplifies the category, along with a sample response (\"Martin Luther King Jr. is waving to acknowledge and greet the crowd of protesters [...]\"). 10 sample familes are in Figure 2.\n\nThe following steps are carried out in collaboration with crowdworkers, who receive an hourly wage of $18. These steps are outlined in Figure 3: (1) taking the image/instruction example as a guiding seed task crowdworkers formulate a new instruction that examines the same instruction family (\"instruction generation\"); (2) crowdworkers create detailed image captions that describe the image and allow an entity, relying solely on this text, to interpret and execute the given instruction successfully (\"instruction-conditioned caption generation\"); (3) crowdworkers assess the correctness of GPT-4's response to the instruction In this task you will be provided with two individual images i.e., BEFORE and AFTER. Please study them carefully and determine if the following sentence is correct (Answer \"Yes\"/\"No\"):\n\n\nNLVR2\n\nNone of the dogs in these images are wearing clothing.\n\nTwo small chihuahuas, one with all white fur, and the other with a light brown coat of fur on its back, but white fur on its head and neck, are near a small pink pillow that has large white words proclaiming, \"BE MINE\" on it. The all white chihuahua is resting its head on the pillow, while the other one is laying behind it.\n\n\n1\n\nA small all white chihuahua is standing and looking straight at the camera. It's wearing a pink and white polka-dotted dress, over which is a pink shirt with some white text on it. Under the dog is a cement walkway, and there is grass to the dog's right and to the right of the walkway.\n\n\n2\n\n\nGPT-4\n\nNo. The second image has a dog wearing a pink and white polka-dotted dress and a pink shirt with white text on it. Figure 4: An example of a multi-image instruction task from VisIT-Bench. Originally sourced from NLVR2 [36], this task aims to evaluate visual reasoning skills. The NLVR2 format includes a sentence to analyze, two images, and a binary response. Our adaptation adds a zero-shot instruction prompt, a instruction-conditioned caption for each image and a human-verified GPT-4 response. These enhancements, designed in a chatbot-style interaction, facilitate automatic evaluation of future chatbot responses to this instance.\n\n(\"model output evaluation\"). We further elaborate on these steps using human annotators below.\n\nRe-formatting existing datasets. 25/70 instruction families (corresponding to 25*10=250 instances) are re-formatted versions of existing vision-language tasks (See Appendix C for full list). 2 This process involves re-formatting tasks into chatbot-style instruction/response versions. In re-formatting, we re-write instructions to retain the original task's goal while maintaining the original images, see Figure 4. These repurposed tasks are integrated into our data collection process, ensuring uniformity between the chatbot-style answers in the full VisIT-Bench instances and the reinterpreted tasks.\n\nInstruction Generation. Here, annotators create a new instance from the same instruction family as a given example, along with an instruction and corresponding image. For instance, in Figure 3 (left), the instruction family is \"Contextual Knowledge of Events\", and the example instruction is \"Why is he waving? What happened in this event?\" alongside an image of Martin Luther King, Jr. To collect images, annotators were instructed to use Openverse (https://openverse.org/) for Creative Commons licened images.\n\nInstruction-Conditioned Caption Generation. Annotators are provided with the image and instruction, and are tasked to construct a caption that is rich enough to allow an entity, solely receiving the text they author, to follow the instruction. This caption will later facilitate GPT-4 reference candidate generation, and will be used for text-only auto-evaluation. We call these instructions instruction-conditioned captions. See Figure 3 (middle) for an example: an annotator doesn't just mention the skittles and a spoon, but, given the query regarding specific colors, they indicate the exact colors in detail.\n\nModel Output Evaluation. The goal of this stage is to gather human-validated reference chatbot responses for each multimodal instruction query. We initially obtain response candidates from GPT-4 given the instruction and the instruction-conditioned caption. GPT4's prompt is: \"Consider an image depicted by: <caption>'. Now, briefly follow this instruction, and you can add a short explanation: <instruction>'. Response: This prompt is employed for both single and multiple image instances, with appropriate modifications for the latter. Then we verify each response with human annotators. 3 If a response is marked incorrect, the annotator identifies whether the issue lies with the detail level of the instruction-conditioned captions or with GPT-4's response itself. For VisIT-Bench, we discard any case marked as incorrect for either reason. 4 An example is given in Figure 3 (right), where GPT-4's candidate reference response aims to answer a question about a chess position (which it does so incorrectly, and thus, the instance is discarded).\n\n\nData Collection Annotation and Results\n\nWe conduct the data collection steps in Figure 3 using Amazon's Mechanical Turk (MTurk) platform. Prior to annotating, each MTurk worker passed a qualification test, which involved five to ten sample tasks designed to assess their ability to generate high-quality annotations.\n\nMore detailed information about the execution process and full user interface examples can be found in Appendix B. Table 2: Human rating metrics for the VisIT-Bench dataset: overall, single-, and multi-image tasks.\n\n\nMetrics\n\nOverall Single Multi\nGPT-4 Correct (%) 87.3 91.5 63.0 Problem in Caption (%) 4.0 3.6 6.0 Problem in GPT-4 (%) 7.7 3.8 30.0\nOur annotation results are summarized in Table 2. We measure the throughput of the collection and filtration pipeline. For single-image instances, our pipeline's yield was 91.5% from the original candidate set. However, the success rate dropped to 63.0% in the more complex multi-image tasks, accompanied by an uptick in issues either in the captions (6.0%) or GPT-4's responses (30.0%). This drop suggests that multi-image queries may pose a more difficult data collection challenge.\n\n\nVisIT-Bench Analysis\n\nWe analyze the tasks, images, and instruction-conditioned captions of VisIT-Bench.\n\n\nAre instruction-conditioned captions necessary?\n\nTo clarify the role of the instruction-conditioned captions we collect, we conducted an experiment covering 150 single-image instances. Instead of using our instruction-conditioned captions, we use BLIP2 [15] image captions, which is a state-of-the-art image captioning model. We extract image captions, and feed them to GPT-4 as detailed earlier, to provide a text-based chatbot response. This process is depicted in Figure 5.\n\nWe manually evaluated whether the resulting output accurately followed the instructions. We find that while instruction-conditioned captions led to correct outputs in 91% of the cases, the success rate fell to 31% when using BLIP2 captions (Table 2). These results highlight the importance of instruction-conditioned captions in the construction of VisIT-Bench, and show that the instances in our dataset are sophisticated enough such that most are not solvable by using a simple Socratic model [38] baseline of caption \u2192 LLM.\n\nWould a disabled wheelchair bound individual find it easy to go into this store?\n\n\nBLIP2 Caption a sign on a brick wall\n\n\nInstruction-Conditioned Caption\n\nThis image is of an exterior store front and sidewalk, showing two portable wedge ramps, one green and one red. The ramps are positioned against the step to two retail doorways, and seem to bridge the vertical distance between the level of the sidewalk and the step up to the doorway. The ramps are printed with \"STOPGAP.CA\" on the top surface.\n\nIt is not possible to determine if a disabled wheelchair-bound individual would find it easy to go into the store based on the image of \"a sign on a brick wall.\" Additional information about the store's entrance, accessibility features, and layout is needed to make that assessment.\n\nYes, a disabled wheelchair-bound individual would likely find it easy to go into this store, as the portable wedge ramps provide a smooth transition from the sidewalk to the doorway, making it more accessible for them.\n\n\nGPT-4 GPT-4\n\nOverall Success: 31%\n\nOverall Success: 91% Figure 5: This experiment evaluates the value of instruction-conditioned captions in accurate instruction-following tasks. Given an image and instruction, GPT-4 generates responses using both a instruction-conditioned caption and a less detailed BLIP-2 [15] caption. The latter's imprecision leads to an error, emphasizing the need for detailed, task-specific captions.\n\n\nWhat skills are required for VisIT-Bench?\n\nThe full list of instruction families we cover are in Appendix Table 6. Following [39], for the VisIT-Bench instructions, we extract the most frequent root verbs and their direct nouns (a full plot is in Figure 6). The most common include: 'answer question', 'write story/poem', 'create title', etc. There's also a long-tail of diverse requests that demand comprehension, commonsense, and cross-modal understanding, e.g., 'identifying objects' to 'need ingredient' to 'connect device'. Additional qualitative examination reveals a range of underlying skills required ranging from 'emotion identification' to complex reasoning tasks such as 'paper folding'. \n\n\nWhat is contained in VisIT-Bench images?\n\nWe detect all the COCO [20] objects present in the images from our dataset using Yolov5-L [40]; The most common detected objects in VisIT-Bench are \"person\" (\u223c 900 detections), chair, and car (\u223c 100). But, a long tail of rarer objects exists as well: full distribution in Appendix Figure 10. Overall, to perform well at VisIT-Bench, a model must account for a broad range of scenes and objects.\n\n\nExperiments\n\nWe evaluate a range of state-of-theart publicly accessible vision-andlanguage chatbots on the 592 instances in VisIT-Bench. In \u00a74.1, we provide the details of the instructionfollowing models in our benchmark. Following this, we collect the human preferences for pairwise model generations to achieve a human-guided Elo ranking and the win-rates against the reference of the models in \u00a74.2. We\n\n\nGPT-4\n\n\nInstruction-Conditioned Caption\n\nA cave man with a headband is holding a money bill that has the face of a cave man on it.\n\n\nPandaGPT\n\nThe unusual aspect of this image is that it appears to be a combination of two different scenes or images. The main focus is a cartoon drawing of a man holding a sign that says \"ART.\" However, there are also elements of a kitchen, such as a refrigerator, a microwave, and a clock, which do not seem to be related to the \"ART\" theme. This juxtaposition of unrelated subjects in the same image creates a peculiar and unexpected visual effect.\n\n\nInstructBLIP\n\nAn unusual aspect of this image is the man's appearance. He has long hair, a beard, and is wearing a loincloth, which is an ancient form of clothing worn by men in some cultures. Additionally, he is holding a stack of money in his hand, which is a modern-day item. This combination of ancient and modern elements creates an interesting contrast and raises questions about the context and purpose of the image.\n\nWhat is unusual about this image? InstructBLIP Figure 7: ELO-based evaluation for VisIT-Bench: Our reference-free approach uses a GPT4 evaluator to compare two instruction-following models with an instruction and a instructionconditioned caption. The instance is obtained from an existing dataset, WHOOPS! [41].\n\nthen develop automatic evaluation on VisIT-Bench in \u00a74.3, that can be scaled and improved given new and improved models. Finally, we establish the trustworthiness of our automatic evaluation method by performing agreement analysis with the human judgments in \u00a74.3\n\n\nModels\n\nWe evaluate LLaVA-13B [13], InstructBLIP-13B [14], MiniGPT4-7B [42], mPLUG-Owl-7B [17], LlamaAdapter-v2-7B [16], PandaGPT-13B [18], VisualChatGPT [43], Multimodal GPT [44], OpenFlamingo v1 [11,45] , Otter v1 [46], Lynx [47] and idefics [48]. For the execution-based VisualChatGPT [43], we implement a chat window for each sample, hold inputs and intermediate chains of thoughts and actions in memory, and feed the images and the instruction sequentially. For OpenFlamingo [11] and Otter [46], we feed the image(s) and the instruction in an interleaved format. For the others, we feed the image to the vision feature extractor and feed the instruction as a prompt to the text encoder. 5\n\n\nHuman Evaluation\n\nWe collect 5K pairwise human preference judgements across an initial set of 6 models and the human-verified references. For 1K uniformly randomly sampled tuples of (query, model A, model B), we collect 5 crowdworker judgements each. Preferences are collected in a \"forced choice\" setting, annotators are instructed to decide based on accuracy, helpfulness, and detail. We provide the template for the human annotation process in Appendix Figure 15. We summarize the results with two metrics:\n\nRelative metric: Elo We follow [26] and compute Elo ratings, treating each pairwise human judgement as a \"match.\" 6 The difference between the Elo ratings of two different models provides an estimate for the win probability when pitting model A vs. model B. More details are in Appendix D.\n\nAbsolute metric: Win rate vs. reference. We provide a win-rate vs. the human-verified reference. We use the 1.4K pairwise human judgments where one of A or B is the reference. We report the percent of cases where the human judge prefers the output from that model vs. the human-verified GPT-4 reference output. Because we do not allow for ties in our forced-choice setup, if the annotator believes the responses are of equal quaity, they choose one arbitrarily. Results Table 3 contains the Elo and win-rate vs. reference. In terms of Elo, the Human Verified GPT-4 reference achieves a higher rating than all alternatives, validating the quality of our reference set: concretely, for our Elo settings, the reference (Elo =1223) has an estimated win-rate over one of the best performing models, LLaVA, (Elo =1085) of 69%, and an estimated win rate of 93% against the lowest performing model in this setup, PandaGPT (Elo =786). This result can partly be explained by the training process of the underlying models: The improved performance of LLaVA (13B) might be attributed to its fine-tuning process, which utilized 150K instruction-tuning data that is rich in both diversity and quality. Interestingly, despite achieving a slightly lower Elo (the computation of which is based on all head-to-head \"matches\", rather than just ones against the human reference), LlamaAdapter-v2 (7B) wins with the highest rate against the reference. However, the complexity and variety of models and tasks in VisIT-Bench makes it challenging to definitively pinpoint the factors influencing performance. While we make a preliminary attempt to unravel these intricacies in Section 4.3, a comprehensive understanding will necessitate more nuanced and extensive future research.    Because it is costly to gather human pairwise preference judgements for new model submissions, to support faster model development, we seek an automatic evaluation procedure that produces high correlation with our human evaluation setup.\n\n\nAutomatic Evaluation and Leaderboard\n\nAutomatic evaluation metric candidates. We consider several existing reference-backed evaluation metrics: BLEU-4 [27] , ROUGE-L [28], ME-TEOR [29], CIDEr [30], and BERTScore [31], we use the RoBERTa-Large english version [49], treating the human-verified GPT-4 reference as the evaluation reference. We additionally report two baseline metrics: random, which assigns a random score without accounting for the candidate, and length, which assigns a score equal to the number of non-whitespace tokens in the candidate. Beyond existing metrics and baselines, following the recent line of work utilizing API-accessed LLMs with a prompt for automatic evaluation [6,50], we consider two GPT-4 [7] backed evaluation metrics.\n\nSpecifically, we provide the LLM with: 1) a system prompt describing the desired evaluation behavior; 2) the instruction-conditioned caption for the image; 3) the instruction to be followed; and 4) two candidate generations dubbed \"Response A\" and \"Response B\". We also consider a reference-backed version where the human-verified reference is provided as well. We provide our prompts in Appendix E. To mitigate potential biases in \"A\" and \"B\" positioning, for all pairs of candidates, we run two queries covering both possible orderings.\n\nOur prompt encourages the model to think step-by-step so that its chain-of-thought process is made explicit [51,52]. Despite strongly encouraging the model to select between the two references in a forced-choice setup, it sometimes refuses and outputs \"tie\" which we account for later. We call the reference-free version of this metric \"GPT4-no-ref\", and the reference-backed version of this metric \"GPT4-ref\".\n\nEvaluating evaluation metrics. We measure the correlation between the candidate metrics and human judgements using a pairwise framework. Specifically, we use a subset of the 5K pairwise human judgements in \u00a7 4.2. For 690 pairwise instances where both candidate instances are model-generated (rather than human-verified references), we have 5 pairwise judgements from crowd-workers. For 336 pairs, there is 5/5 agreement, for 200 pairs, there is 4/5 agreement, and for 154 pairs, there is 3/5 agreement. For each metric, we measure the percent of time the metric is able to accurately reconstruct a majority vote judgement from the 5 crowdworkers. The newly proposed GPT-4 based metrics sometimes outputs \"tie\" (this happens in 10-15% of cases overall) -for fair comparison with the other metrics in forced choice setting, we randomly choose one of the two options when GPT-4 reports a tie.\n\nThe results are in Figure 9, with GPT-4-no-ref best aligns with human correlation. The best performing metric is our newly proposed GPT-4 based metric, which accurately reconstructs majority-vote pairwise human judgments better than alternatives (p < .05; binomial proportion CI nonoverlapping). For example, for instances where 5/5 annotators agree, GPT4-no-ref, with no reference, accurately reconstructs human judgment 93% of the time, whereas the next best metrics BERTScore/METEOR/ROUGE-L reconstruct accurately 80%/78%/70% of the time; among the metrics we consider, these are reasonable options for static/offline evaluation without relying on OpenAI API access, especially when compared to our length baseline metric, which achieves only 60%. Notably, the reference-backed version of the newly proposed GPT-4 based metric achieves comparable (but slightly worse) performance compared to the reference-free version. Thus, we adopt the reference-free version, which additionally enables us to place the references themselves into the the Elo setup, because they are not used in the prompts.\n\nSystem-level Correlation. We summarize the LLM's pairwise judgements using the same metrics as introduced in \u00a74.2, Elo ratings and win rate vs. reference, but instead of using a human judge, we use our reference-free GPT-4 based metric. The results are in ??. Notably, among the 7 systems for which we gathered human ratings for, the automatic metric produces the same ordering compared to human evaluation (\u03c1 = 1.0, p < .01).\n\nShortcomings of proposed metric. While the relative ranking of models produced by the automatic metric correlates strongly with the ranking produced by human judgements, the win rate vs. reference according to human judgement (Table 3) are higher overall compared to the win-rate vs. reference according to the automatic metric ??. One plausible explanation for this discrepancy is that GPT-4, as an evaluation model, may prefer responses that closely match its own response distribution.\n\nPer-category results. In Figure 8, we plot the win-rate vs reference for the models across all the single-image instruction families. We find that there is no model that performs the best and worst across all the instruction families. Thus, VisIT-Bench aids in highlighting the strengths and weaknesses of the instruction-following models along various real-world use-cases.\n\n\nRelated Work\n\nMultimodal Models for Image-Text Understanding: Recently, the field of machine learning has experienced a rapid proliferation of new models which can perform various image-text tasks [12,15,13,53,18,14]. This growth has been driven by several factors, including the emergence of large-scale multimodal datasets (e.g. LAION-5B [54], Multimodal C4 [11]), improved software and hardware frameworks, and advances in modality-specific models such as language models (e.g., [10]). Our work specifically evaluates models which can generate textual outputs, given one or more images, and text. Recent examples of such models include LLaVA [13], mPLUG-Owl [17], InstructBLIP, LLaMA-Adapter, Flamingo [12] and OpenFlamingo [11], PandaGPT [18], and GPT-4 [7] (which reports multimodal capabilities but has not yet seen a release of the multimodal variant).\n\nInstruction Following: \"Instruction-following\" is an emerging paradigm for training models via language, where instead of being trained to complete only a single, fixed task (such as image classification or captioning), models are trained to follow textual instructions that describe an arbitrary task, with the aim of generalizing to novel instructions. Examples of instruction-following models include Alpaca [5], LLaMA-Adapter [16], Koala [55], InstructBLIP [14], LLaVA [13], and mPLUG-owl [17]. As the downstream capabilities of these models are influenced by the quality of the training dataset, there has also been extensive work on developing instruction-following datasets [39,56,57,13,58] .\n\nTo build these models, two broad approaches have been shown to be effective. One approach focuses on leveraging existing pretrained task-specific tools such as image captioners [15], object detectors [59] and text-to-image generators [60] by either creating multimodal prompt interfaces [43,61] or by executing LLM-generated programs [62,63,64]. The other approach [13,16,65,46,66,17,11] focuses on building a single pretrained model that can follow instructions by supervised finetuning on multimodal vision-language data.\n\nDespite the success of both these approaches on the existing vision-language datasets e.g., VQA, GQA, Image Captioning [21,67,20], there is a lack of a high-quality benchmarking dataset for multimodal instruction-following tasks that reliably replicates the way in which humans would interact with multimodal chatbots in the wild. Similar to the image-text models discussed above, many instruction-following models have been released directly as open-source without undergoing peer review or thorough evaluation. As a result, the effectiveness of these models for many tasks is not well-understood.\n\nBenchmarks for Machine Learning: High-quality evaluation datasets have served both to (re)assess, and to accelerate, progress on many machine learning tasks [68]. For example, our work draws particularly from the fields of computer vision and natural language processing, where benchmarking datasets have been critical drivers of progress. On the vision side, datasets such as ImageNet [69] and CIFAR [70] have proven to be critical yardsticks of progress. On the language side, benchmarks such as SQuAD [71], SST [72], GLUE/SuperGLUE [73,74] and more [75,76] seen wide use. Recent work has indicated that improvements on these high-quality benchmark datasets is not the result of overfitting, and is a reliable indicator of genuine progress beyond the benchmark data [77,78,79,80].\n\nHowever, high-quality benchmarking datasets and evaluation methods do not yet exist for multimodal instruction-following. As a result, it is difficult to assess progress in this direction, which both reduces the field's ability to identify true breakthroughs and increases vulnerability to potential pitfalls of evaluation that have hampered progress in other areas of machine learning [68,81].\n\n\nConclusion\n\nWe introduce VisIT-Bench, a dynamic benchmark providing a broad evaluation of multimodal chatbots' capabilities. Going beyond prior efforts, VisIT-Bench's collection process centers potential real-world use cases, and 70 diverse instruction families encompassing a range of tasks from recognition to complex reasoning. Our benchmark not only offers human-verified reference outputs for all examples but also gives an Elo-based ranking system for multimodal chatbots that correlates with human judgements. Our experiments reveal a gap between model and human performance.We release data, code, and automatic metrics, encouraging community involvement. We hope VisIT-Bench can provide a new quantification of progress and shortcomings of multimodal AI systems.\n\n\nLimitations\n\nAlthough VisIT-Bench covers a wide spectrum of potential use-cases, it does not incorporate every possible vision-language task. We hope to add more categories of tasks over time. In terms of dialogue, VisIT-Bench concentrates on single-turn instances with one instruction and response. This does not encompass multi-turn interactions between users and chatbots, which presents a promising direction for future research. Our study focuses on image-text modalities. Future extensions could expand the scope to include other modalities like audio and video, enabling a more comprehensive evaluation. Additionally, while the dataset offers a wide variety of tasks, a larger number of examples per category could provide more depth. Finally, while our GPT-4 based metric correlates well with human judgement both at the instance level and at the system level, we see some evidence that the GPT-4 based metric has a stronger preference for GPT-4 based generations compared to humans. Thus, models which train, e.g., by distilling from GPT-4 outputs, may have an unfair advantage on our evaluation.\n\n\nA License and Intended Use\n\nThe VisIT-Bench dataset, along with its various contributions such as instructions, reference outputs, and model ranking annotations, is licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0). This license applies to all the images we have directly contributed, each of which carries a public license specification in the \"public images metadata\" field within the dataset sheets. However, the dataset also incorporates images sourced from pre-existing collections. For these images, the original licensing terms are respected and remain applicable.\n\nVisIT-Bench's primary purpose is to function as a dynamic benchmark that continuously evolves and evaluates instruction-following vision-language models. In the current landscape, commercial chatbots are often trained on non-disclosed and non-public datasets, which raises concerns about potential data contamination and inadvertent training on our evaluation data [8]. This risk is further highlighted by recent studies [82,83]. To mitigate such concerns, we have chosen to withhold the complete VisIT-Bench test set from public disclosure, while still making the images and instructions available for direct download. Researchers, however, can utilize VisIT-Bench to its full potential as a dynamic benchmark by submitting their model predictions for evaluation. We will assess their models using the undisclosed test set, ensuring the ongoing evolution of the benchmark. Moreover, we are open to releasing the test data upon receiving reasonable and justified requests, particularly when additional analysis is necessary, provided that requesters agree to our non-contamination policy which prohibits the use of this data for training commercial chatbots. This approach strikes a balance between the need for robust model evaluation and the mitigation of potential data contamination. Count (log-scale) Figure 10: Count of the various COCO objects present in the VisIT-Bench images on a log-scale. The object detection was performed by Yolov5-Large [40]. We observe that the object 'person' occurs most of the time in the dataset images i.e., 875 times.\n\n\nA Dataset Analysis\n\n\nB Interfaces for Collecting Human Annotations\n\nIn this section, we provide the templates we used to collect human annotations for the instruction generation (Figure 11), the dense caption generation (Figure 12), the model verification ( Figure 13 and Figure 14), and the model rating ( Figure 15). Figure 11: An interface that collects the image and corresponding instruction.    \n\n\nC Existing Datasets incorporated in VisIT-Bench\n\nIn Table 5, we listed the existing datasets that are incoprated in our VisIT-Bench. Among these datasets, 15 contain a single image in each sample pair, and 10 require reasoning based on multiple images. Single Dataset Topic VQA [21] Visual Question Answering VCR [84] Cognition-level Visual Understanding GD-VCR [85] Geo-Diverse Commonsense Reasoning WHOOPS [41] What Makes this Image Strange Newyork Caption [86] Humor Understanding CLEVR [87] Visual Question Answering Kilogram [88] Tangrams Identification Harmful Memes [89] Memes Understanding ScienceQA [90] Science Question Answering OK-VQA [91] Outside Knowledge Visual Question Answering AOK-VQA [92] Outside Knowledge Visual Question AOK-VQA [92] Question Generation VizWiz [93] Visual Question Answering GQA [67] Visual Question Answering on Scene Graphs TextCaps [94] Visual Question Answering on Texts\n\nMultiple Robust Change Captioning [22] Describing What has Change in a Scene NLVR2 [36] Testing Visual Language Bias ImageCoDE [95] Image Retrieval Spot-the-Diff [96] Identifying Differences VASR [97] Visual Analogies WinoGavil [98] Visual Associations IRFL (Metaphor) [99] Figurative Speech Understanding IRFL (Idioms) [99] Figurative Speech Understanding IconQA [100] Abstract Diagram Understanding Pick-a-Pic [101] Text-to-Image User Preferences  \n\n\nD Elo Rating\n\nFor many years, the Elo rating has been popular in ranking players in zero-sum games such as chess [25]. Recently, it has been adopted to rate large language models (LLMs) against each other on the user instructions. In this work, we adopt the same strategy to rank a set of instruction-following vision-language models, that can grow dynamically with further advances in the field.\n\nGiven two multimodal chatbots C a and C b with their absolute Elo rating R a and R b , respectively. Simply put, the probability of C a winning over C b in a head-to-head battle is given by:\nP (C a wins over C b ) = 1 1 + 10 (Ra\u2212R b )/400(1)\nIn practice, calculating the Elo rating requires us to set hyperparameters to decide the weightage for each win and loss in a head-to-head battle between two models. In our work, we use the open implementation of Elo for LLMs by FastChat at https://github.com/ lm-sys/FastChat/blob/main/fastchat/serve/monitor/elo_analysis.py.\n\n\nE GPT-4 Pairwise Evaluation Prompts\n\nThe specific prompts we use to extract pairwise judgements from our language model are provided in Table 16 (reference-free version) and Table 17 (reference-backed version). When applied to GPT-4 [7], these prompts usually solicit a definitive pairwise response by the model. But, in some cases, the model either produces a pairwise judgement in an unexpected format, or, refuses to issue a judgement at all. For cases like these, we issue an additional query to ChatGPT to extract an answer (or decide there is no answer) using an additional prompt, given in Table 18. If after this step there is still no definitive pairwise judgment, we call the result a tie.\n\nsystem prompt (human authored)\n\nYou are ImageTaskEvaluationGPT, an expert language model at judging whether or not a response adequately addresses an instruction in the context of an image. More specifically, you will be given the following:\n\n1. An image context: This will describe the contents of an image with sufficient detail to address the instruction.\n\n2. An instruction: This is a question, an imperative request, or something similar about the image which requires a response.\n\n3. Two responses, response A and response B: These two responses attempt to address the instruction in the context of the image. Your job is to judge whether response A or response B better. A and B are randomly ordered.\n\nSome things to remember:\n\n-Even though you are just a language model, the image description will be sufficiently detailed so that your judgements can be accurate.\n\n-You are capable of judging response quality, accounting for important factors like correctness, relevance, fluency, specificity, etc.\n\n-You think step-by-step, but ultimately respond with \"Response A\" or \"Response B\"\n\nuser (human authored) I will describe an image to you, and provide an instruction. Then, I will give you two candidate responses that address the instruction in the context of the image: these will be labelled \"Response A\" and \"Response B\". Your job is to first reason step-by-step about which response is best in terms of accuracy, specificity, fluency, etc. After reasoning step-by-step and comparing the pros/cons of each response, in the end, respond with \"Overall, Response X is better.\" where X is either A or B.\n\n\nassistant (human authored)\n\nSure, please provide the image context, the instruction, and the two candidate responses, Response A and Response B. Then, I will think step-by-step and provide my ultimate judgement as to which response is better.\n\nuser (from human-authored template)\n\nOK. Here is the image description, the instruction, and the two response options, Response A and Response B. Image context: A skier is wearing a green jacket, burgundy pants, white shoes, white gloves, and a gray beanie. There are people sitting on the background with their snowboards.\n\nInstruction: Answer the following question based on the provided image: Are the trousers the same color as the shoes?\n\nResponse A: no Response B: No, the trousers are not the same color as the shoes. The trousers are burgundy, while the shoes are white.\n\nThink step-by-step and finish your response with \"Overall, Response X is better.\" where X is either A or B.\n\nassistant (machine generated; used to derive the prediction)\n\nStep 1: Identify the key information from the image description.\n\nThe skier is wearing: -Burgundy pants -White shoes\n\nStep 2: Assess the accuracy of each response in addressing the instruction.\n\nBoth Response A and Response B convey that the trousers and shoes are not of the same color.\n\nStep 3: Compare specificity and detail of each response.\n\nResponse A is concise and provides a straightforward answer, but lacks detail. Response B provides a clear, comprehensive answer that includes the color of both the trousers and shoes.\n\nStep 4: Consider the fluency and correctness.\n\nBoth responses are fluent and grammatically correct.\n\nOverall, Response B is better. Figure 16: The prompt we use for reference-free evaluation, along with a sample instance/completion from GPT-4 [7].\n\nsystem prompt (human authored)\n\nYou are ImageTaskEvaluationGPT, an expert language model at judging whether or not a response adequately addresses an instruction in the context of an image. More specifically, you will be given the following:\n\n1. An image context: This will describe the contents of an image with sufficient detail to address the instruction.\n\n2. An instruction: This is a question, an imparative request, or something similar about the image which requires a response. 3. A reference output: This is a high-quality example output that humans have judged to be an accurate response for the input instruction. 4. Two responses, response A and response B: These two responses attempt to address the instruction in the context of the image.\n\nYour job is to judge whether response A or response B better. A and B are randomly ordered. Some things to remember: -Even though you are just a language model, the image description will be sufficiently detailed so that your judgements can be accurate.\n\n-Take the high-quality reference into account when making your judgements, but remember: some instructions are more open-ended than others, so for those cases, a high quality response can differ from the reference.\n\n-You are capable of judging response quality, accounting for important factors like correctness, relevance, fluency, specificity, etc.\n\n-You think step-by-step, but ultimately respond with \"Response A\" or \"Response B\"\n\nuser (human authored) I will describe an image to you, and provide an instruction. Then, I will provide a reference output which is an example of a high quality output for that instruction in the context of the image. Then, I will give you two candidate responses that address the instruction in the context of the image: these will be labelled \"Response A\" and \"Response B\". Your job is to first reason step-by-step about which response is best in terms of accuracy, specificity, fluency, etc. After reasoning step-by-step and comparing the pros/cons of each response, in the end, respond with \"Overall, Response X is better.\" where X is either A or B.\n\n\nassistant (human authored)\n\nSure, please provide the image context, the instruction, the reference, and the two candidate responses, Response A and Response B. Then, I will think step-by-step and provide my ultimate judgement as to which response is better.\n\nuser (from human-authored template)\n\nOK. Here is the image description, the instruction, the high-quality reference, and the two response options, Response A and Response B. Image context: There is a white plate that has on it a small pizza with many topics, mostly unrecognizable, due to the pie being very well done, charred to the point of almost being burnt, and the crust blackened in many places.\n\nInstruction: Someone told the creator of this item that they are very talented. What is the implied meaning? High quality reference: The implied meaning is that the person complimenting the creator is being sarcastic, as the pizza is overcooked and almost burnt, which is not a sign of talent.\n\nResponse A: When someone says that they are \"very talented\" in the context of the image, it implies that the person responsible for creating the pizza is skilled in the craft and able to create an appealing and tasty dish. The person praising the pizza's creator recognizes their culinary skills, which can be seen through the combination of ingredients and the overall look of the finished dish. The praise is a form of appreciation for their efforts and the enjoyable result they achieved. Response B: It is difficult to determine the implied meaning without further context or information about the image. It appears to be a pizza that has been burnt or charred on the surface and has various toppings, including pepperoni, onion, and mushrooms. The image is shown on a white plate or surface. The implied meaning of this image could be that it is a dish that has been cooked badly or that it is a dish with a unique or unusual combination of ingredients.\n\nThink step-by-step and finish your response with \"Overall, Response X is better.\" where X is either A or B.\n\nassistant (machine generated; used to derive the prediction)\n\nStep 1: Comparing both responses with the high-quality reference, it's clear that Response A describes the pizza as well-made and appealing, while Response B is more focused on the fact that it is burnt or charred, somewhat aligning it with the reference.\n\nStep 2: Evaluating specificity, Response A defines the creator as skilled, even though the image description contradicts this by stating that the pizza is overcooked. Response B, while not as specific, acknowledges the overcooked nature of the pizza.\n\nStep 3: In terms of fluency, both responses are easily understood and written in coherent language.\n\nStep 4: Considering accuracy, Response A is inaccurate in relation to the context and instruction because it implies a positive meaning when it should be sarcastic. Response B doesn't fully capture the sarcasm but is closer to the truth about the pizza being badly cooked.\n\nStep 5: Understanding relevance, Response A is not entirely relevant due to the inaccuracy in the creator's talent, while Response B, although less direct, is more relevant by mentioning the burnt or charred nature of the pizza. Overall, Response B is better. Figure 17: The prompt we use for reference-backed evaluation, along with a sample instance/completion from GPT-4 [7].\n\nsystem prompt (human authored)\n\nYou are FinalAnswerExtractionGPT, an expert language model at extracting multiple choice answers from written out from longer explanations. You will be given several sentences describing a thought process which should eventually contain a final answer, either A or B. Your job is to extract the final answer that the explanation arrives at.\n\nSome things to remember:\n\n-Keep your answers short: only output \"Final Answer: X\" where X is A or B -If the explanation doesn't mention a final choice explicitly, you can output \"Unknown\"\n\n. . . (+ 2 in-context examples (human authored)) . . .\n\nExample One: GPT-4 gives a definitive answer, but in the wrong format user (GPT-4's raw pairwise judgement)\n\nStep 1: Compare the two responses to the instruction -The instruction asks if people are required to stop and requests an explanation.\n\nStep 2: Assess Response A -Response A indicates that people should stop due to the presence of a stop sign. However, it doesn't consider the fact that the sign is pressed against the bus, and the LED lights are off.\n\nStep 3: Assess Response B -Response B simply says \"yes,\" acknowledging that people should stop, but it doesn't provide the explanation required by the instruction.\n\nStep 4: Compare specificity, fluency, and other factors -Response A is more specific and fluent but misconstrues the image description.\n\n-Response B is accurate in that it acknowledges the stop sign but lacks an explanation.\n\nOverall, since neither response is fully correct, it is difficult to declare a better response. However, given the fact that Response B acknowledges the prominent stop sign, I would say that Response B is slightly better, but it should also explain how the situation indicates that people should stop. Please extract the final answer from the above text.\n\nassistant (machine generated; used to derive the prediction)\n\nFinal Answer: Response B is slightly better, but it should also explain how the situation indicates that people should stop.\n\nExample Two: GPT-4 refuses to answer.\n\nuser (GPT-4's raw pairwise judgement)\n\nStep 1: Consider the image context, which states that the original thirteen colonies are shown with New Hampshire specifically highlighted in a darker green.\n\nStep 2: Assess the accuracy of Response A, which claims that the colony shown is \"Virginia.\" This is not accurate since the image context indicates New Hampshire being highlighted.\n\nStep 3: Assess the accuracy of Response B, which claims that the colony shown is the Massachusetts Bay Colony. This response is also not accurate, as the image context states New Hampshire is the highlighted colony.\n\nStep 4: In terms of accuracy, both responses are incorrect. Neither response is consistent with the image context.\n\nStep 5: Consider other factors like fluency and specificity. Both responses seem fluent and specific in nature.\n\nAfter considering these factors, none of the responses are accurate given the information provided in the image context. However, I am unable to provide an \"Overall, Response X is better.\" judgement in this case as both responses are incorrect and neither stands out as being better than the other.\n\nPlease extract the final answer from the above text.\n\nassistant (machine generated; used to derive the prediction)\n\nFinal Answer: Unknown. Figure 18: The prompt we use to extract answers from ill-formatted GPT-4 [7] responses, along with two example outputs. In one case, GPT-4's pairwise judgement is given, but in a format different than requested. In the other case, GPT-4 \"defies\" the prompt, and refuses to issue a judgement because both options are bad.\n\n\"\nTan Lines & Ticket Stubs: A Love Story Unbound\" -A country song about leaving it all behind, finding love, and embracing the freedom of the open road.\n\nFigure 1 :\n1An example from VisIT-Bench, featuring an image, a challenging instruction, an instruction-conditioned caption, and a human-verified GPT4 response. These elements are used for evaluating multimodal chatbots and updating a dynamic leaderboard.\n\nFigure 3 :\n3Data collection steps: (1) Instruction Generation -Annotators derive instances from a seed task, see Figure 3 (left). (2) Caption Generation -Rich instruction-conditioned captions are produced for GPT-4 references, shown in Figure 3 (middle). (3) Model Evaluation -GPT-4 responses are human-validated, illustrated in Figure 3 (right). Top blocks show rater instructions; bottom blocks depict outputs.\n\nFigure 6 :\n6Most frequently occurring verbs (inner circle) and their top 4 direct nouns (outer circle) in the VisIT-Bench instructions.\n\nFigure 8 :\n8Reference-free assesment win rate vs. human-verified GPT4 response for each instruction category. Axes: win rate (Y), instruction categories (X). Categories are from-thewild or existing datasets. VisIT-Bench facilitates analysis of diverse instruction tuning tasks.\n\nFigure 9 :\n9Correlations between evaluation metrics and human preferences are ranked in performance order, with our reference free evaluation (GPT-4-no-ref) showing the strongest alignment. Bottom line: random chance (50%), top line: upper performance bound.\n\nFigure 12 :\n12An interface that collects the dense caption given an image-and-instruction pair.\n\nFigure 13 :\n13A sample of the model verification where the GPT4 follows the instruction correctly.\n\nFigure 14 :\n14A sample of the model verification where the GPT4 failed to follow the instruction due to its incorrect reasoning.\n\nFigure 15 :\n15An interface that collects the feedback of the model rating.\n\n6 :\n6List of skills and existing datasets in VisIT-Bench 'scienceqa', 'ocr math', 'recognition', 'okvqa', 'house plan understanding', 'nlvr2', 'gardening tips', 'textcaps', 'architectural styles', 'dressing sense', 'winoground', 'food recipe', 'paper folding', 'whoops', 'spot the diff', 'winogavil', 'imagecode', 'exercise', 'art knowledge', 'gqa', 'physical knowledge', 'contextual knowledge of events', 'home renovation', 'aokvqa', 'animals', 'vasr', 'counting', 'board games', 'solving geometry problems', 'who to call?', 'clevr', 'building materials', 'hazard identification', 'pickapick', 'astronomy', 'figurative speech explanation', 'write a story', 'gestures understanding', 'newyork', 'cultural knowledge', 'aokvqg', 'traffic sign identification', 'pop culture', 'fashion products', 'harmful memes', 'write a poem', 'vizwiz', 'guesstimate of capacity', 'location understanding', 'graph reasoning', 'vqa', 'game playing', 'differently abled', 'chemical identification', 'history knowledge', 'climate and weather understanding', 'irfl metaphor', 'human emotion recognition', 'medical', 'gd vcr', 'vcr', 'technical support', 'catchy titles', 'kilogram', 'anagrams', 'color', 'tour guide', 'directions', 'irfl idiom', 'rcc'\n\nTable 1 :\n1Comparison with related works for evaluating instruction-following vision-language models. Win-rates* refers to the model win-rates against a reference output/model.MultiInstruct [32] Owl [17] InstructBLIP [14] M 3 IT [33] LVLM [34] GAVIE [35] VisIT-Bench \nNumber of Models \n1 \n5 \n3 \n4 \n8 \n5 \n10 \nNumber of Skills Tested \n9 \n6 \n13 \n13 \n47 \n16 \n70 \nMultiple-Images \n\n\n\nVisIT-Bench: A Real-World Inspired VL Instruction-Following BenchmarkInstruction Generation \"Create a new, challenging instruction inspired by the seed task for that family\"\"Provide a highly descriptive caption to allow a \ntext-only recipient to follow the instruction\" \n\nModel Output Evaluation \n\"Evaluate model predictions by answering the \nfollowing questions\u2026\". \n\nImage \n\nSeed \nInstruction \n\nWhy is he waving? What \nhappened in this event? \n\nOutput \nMartin Luther King Jr. is waving \nto acknowledge and greet the \ncrowd of protesters who have \ngathered to hear him speak [...] \n\nNew \nInstruction \n\nWhy is this guy raising his hand? \n\nSelected \nImage \n\nImage \n\nInstruction There is one color of candy \nin the bowl that is missing \nfrom the spoon. Which color \nis not represented in the \nspoon full of candies? \n\nAnnotated \nCaption \n\nBrightly colored round \ncandies, with colors of red, \norange, yellow, green, pink \nand dark brown. A purple \nspoon is raised above the bin \nof candy, bringing them in for \na close-up. The spoon has \nred, orange, yellow, green \nand dark brown candy in it. \n\nInstruction I am the player with the white chess \npieces. Is there a move I can take on \nthis turn to capture one of my \nopponent's pieces? \n\nImage \n\nGPT-4 \nOutput \n\nYes, you can capture the black pawn \nat e5 with your white pawn at e4. \n\nOutput \nEval \n\nDoes GPT-4 correctly follow the instruction? \n[ ] Yes [X] No \nIf answered 'No' for the above, mark one of \nthe following: \n[ ] N/A \n[X] The dense caption is detailed enough, \nthe problem is in GPT-4's answer \n[ ] The dense caption is underspecified, not \nGPT-4's fault \n\nTask \nFamily \n\nContextual Knowledge of Events \n\nInstruction-\nConditioned \nCaption \n\nThere is a wooden chess board [...] \nThe black rooks are at a8 and h8. The \nblack knights are at c6 and g8. [...] \n\n\n\nTable 3 :\n3Human scoring results for the models, shown as both an ELO rating and win-rate against the reference. In total, this summarizes 5.0K pairwise human judgments. matches column indicates the number of total matches in which a particular model participates. Win-rate vs. reference indicates the win-rate of a model against the reference outputs.Model \nElo matches Win-rate vs. reference (w/ # ratings) \n\nSingle Image \nHuman Verified GPT-4 Reference 1223 \n1439 \n-\nLLaVA (13B) \n1085 \n1462 \n26.23% (n=244) \nLlamaAdapter-v2 (7B) \n1061 \n1507 \n27.41% (n=259) \nmPLUG-Owl (7B) \n995 \n1345 \n14.95% (n=214) \nInstructBLIP (13B) \n957 \n1315 \n12.37% (n=194) \nMiniGPT-4 (7B) \n893 \n1513 \n14.72% (n=299) \nPandaGPT (13B) \n786 \n1441 \n10.48% (n=229) \n\nMultiple Images Human Verified GPT-4 Reference 1193 \n210 \n-\nmPLUG-Owl \n997 \n190 \n15.38% (n=78) \nOtter v1 \n917 \n147 \n3.17% (n=63) \nOpenFlamingo v1 \n893 \n171 \n4.35% (n=69) \n\n\n\nTable 4 :\n4Current reference-free Elo rankings as of September 25th, 2023. In total, these rankings summarize 31,735 \"matches\" between models; each match consists of 2 queries to GPT-4. Because VisIT-Bench is dynamic, these rankings are updated as more models are added to the leaderboard, and more pairs of models are evaluated head-to-head for more instances.Category \nModel \nElo # Matches Win vs. Reference (w/ # ratings) \n\nSingle Image \n\nHuman Verified GPT-4 Reference 1,382 \n5,880 \n-\nLLaVA-Plus (13B) \n1,203 \n678 \n35.07% (n=134) \nLLaVA (13B) \n1,095 \n5,420 \n18.53% (n=475) \nmPLUG-Owl (7B) \n1,087 \n5,440 \n15.83% (n=480) \nLlamaAdapter-v2 (7B) \n1,066 \n5,469 \n14.14% (n=488) \nLynx(8B) \n1,037 \n787 \n11.43% (n=140) \nidefics (9B) \n1,020 \n794 \n9.72% (n=144) \nInstructBLIP (13B) \n1,000 \n5,469 \n14.12% (n=503) \nOtter v1 (9B) \n962 \n5,443 \n7.01% (n=499) \nVisualGPT (Da Vinci 003) \n941 \n5,437 \n1.57% (n=510) \nMiniGPT-4 (7B) \n926 \n5,448 \n3.36% (n=506) \nOctopus V2 (9B) \n925 \n790 \n8.90% (n=146) \nOpenFlamingo V1 (9B) \n851 \n5,479 \n2.95% (n=509) \nPandaGPT (13B) \n775 \n5,465 \n2.70% (n=519) \nMultimodal GPT \n731 \n5,471 \n0.19% (n=527) \n\nMultiple Images \n\nHuman Verified GPT-4 Reference 1,192 \n180 \n-\nmPLUG-Owl \n995 \n180 \n6.67% (n=60) \nOtter v1 \n911 \n180 \n1.69% (n=59) \nOpenFlamingo v1 \n902 \n180 \n1.67% (n=60) \n\n5/5 \n(n=336) \n4/5 \n(n=200) \n3/5 \n(n=154) \n\nAgreement Level \n\n48 \n\n56 \n\n64 \n\n72 \n\n80 \n\n88 \n\n96 \n\n% Agree with Human (Higher=better) \n\nrandom \n\nupper bound \n\nGPT-4-no-ref \nGPT-4-ref \nBERTScore \nMETEOR \nROUGE \nBLEU \nLength \nCIDEr \n\n\n\nTable 5 :\n5List of existing datasets in VisIT-Bench, categorized as single and multiple image datasets.\n\nTable\n\nWe recognize that promising applications may not be covered by our set; and we don't necessarily advocate for deploying models in all cases we cover -we hope VisIT-Bench can help to quantify shortcomings and risks.\nUsers of VisIT-Bench should also cite the original datasets.\nAn alternate annotation scheme would have been to task annotators to write target responses from scratch. The rationale for using GPT-4 verification instead is derived from prior results that show promising humanmachine collaboration of this form[37].4 The annotators are also tasked to screen for any offensive, unsound, or harmful advice present in the responses. We did not find or discard any instances.\nFollowing the authors' instructions, we run all models using default settings to obtain the best possible responses. We include specific samples for reproducibility. We acknowledge hyperparameter impact and are willing to reassess submissions to VisIT-Bench if conditions were sub-optimal.6 We use the following code/hyperparameters for Elo ratings: https://github.com/lm-sys/FastChat/ blob/main/fastchat/serve/monitor/elo_analysis.py\nAcknowledgementsWe thank Pang Wei Koh, Ashima Suvarna, Nitzan Guetta and Roee Aharoni for their valuable feedback. Hritik Bansal is supported in part by AFOSR MURI grant FA9550-22-1-0380. RT is supported by the NSF GRFP under Grant No. DGE 1656518.Appendix\nA general language assistant as a laboratory for alignment. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova Dassarma, arXiv:2112.00861arXiv preprintAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.\n\nTraining language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 35Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.\n\nFinetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, M Andrew, Quoc V Dai, Le, arXiv:2109.01652arXiv preprintJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\n\nSuper-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085-5109, 2022.\n\nStanford alpaca: An instructionfollowing llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction- following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.\n\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.\n\n. Openai, arXivOpenAI. Gpt-4 technical report. arXiv, 2023.\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won, Charles Chung, Sebastian Sutton, Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. arXiv preprintAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Naman Baptiste Rozi\u00e8re, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Open and efficient foundation language models. arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\nOpenflamingo. Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, Ludwig Schmidt, 10.5281/zenodo.7733589Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Open- flamingo, March 2023. URL https://doi.org/10.5281/zenodo.7733589.\n\nFlamingo: a visual language model for few. Jeff Jean-Baptiste Alayrac, Pauline Donahue, Antoine Luc, Iain Miech, Yana Barr, Karel Hasson, Arthur Lenc, Katie Mensch, Malcolm Millican, Roman Reynolds, Eliza Ring, Serkan Rutherford, Tengda Cabi, Zhitao Han, Sina Gong, Marianne Samangooei, Monteiro, Oriol Vinyals. Andrew Zisserman, and Karen Simonyanshot learning. ArXiv, abs/2204.14198, 2022Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Mari- anne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. ArXiv, abs/2204.14198, 2022.\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.\n\nInstructblip: Towards general-purpose vision-language models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi, arXiv:2305.06500arXiv preprintWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023.\n\nBlip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXiv:2301.12597arXiv preprintJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n\nPeng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, arXiv:2304.15010Parameter-efficient visual instruction model. 2arXiv preprintPeng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023.\n\nmplug-owl: Modularization empowers large language models with multimodality. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, arXiv:2304.14178arXiv preprintQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization em- powers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.\n\nYixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, Deng Cai, Pandagpt, arXiv:2305.16355One model to instruction-follow them all. arXiv preprintYixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023.\n\nMaking the V in vqa matter: Elevating the role of image understanding in visual question answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904-6913, 2017.\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, Computer Vision-ECCV 2014: 13th European Conference. Zurich, SwitzerlandSpringerProceedings, Part V 13Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer, 2014.\n\nVqa: Visual question answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, Devi Parikh, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425-2433, 2015.\n\nRobust change captioning. Dong Huk Park, Trevor Darrell, Anna Rohrbach, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionDong Huk Park, Trevor Darrell, and Anna Rohrbach. Robust change captioning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4624-4633, 2019.\n\nConnecting vision and language with localized narratives. Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, Vittorio Ferrari, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerProceedings, Part V 16Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with localized narratives. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part V 16, pages 647-664. Springer, 2020.\n\nYushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, A Noah, Jiebo Smith, Luo, arXiv:2211.09699Promptcap: Prompt-guided task-aware image captioning. arXiv preprintYushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A Smith, and Jiebo Luo. Promptcap: Prompt-guided task-aware image captioning. arXiv preprint arXiv:2211.09699, 2022.\n\nThe proposed uscf rating system. its development, theory, and applications. E Arpad, Elo, Chess Life. 228Arpad E Elo. The proposed uscf rating system. its development, theory, and applica- tions. Chess Life, 22(8):242-247, 1967.\n\nChatbot arena: Benchmarking llms in the wild with elo ratings. Lianmin Zheng, Ying Sheng, Wei-Lin Chiang, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Lianmin Zheng, Ying Sheng, Wei-Lin Chiang, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: Benchmarking llms in the wild with elo ratings. 2023. URL https://lmsys.org/blog/2023-05-03-arena/.\n\nBleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, ACL. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, 2002.\n\nRouge: A package for automatic evaluation of summaries. Text Summarization Branches Out. Chin-Yew Lin, Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out, 2004.\n\nMETEOR: an automatic metric for mt evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, ACL workshop on Evaluation Measures for MT and Summarization. Satanjeev Banerjee and Alon Lavie. METEOR: an automatic metric for mt evaluation with improved correlation with human judgments. In ACL workshop on Evaluation Measures for MT and Summarization, 2005.\n\nCider: Consensusbased image description evaluation. Ramakrishna Vedantam, Lawrence Zitnick, Devi Parikh, CVPR. Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus- based image description evaluation. In CVPR, 2015.\n\nBERTScore: Evaluating text generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Q Kilian, Yoav Weinberger, Artzi, ICLR. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. BERTScore: Evaluating text generation with BERT. In ICLR, 2020.\n\nMultiinstruct: Improving multi-modal zero-shot learning via instruction tuning. Zhiyang Xu, Ying Shen, Lifu Huang, arXiv:2212.10773arXiv preprintZhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. arXiv preprint arXiv:2212.10773, 2022.\n\nLei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, arXiv:2306.04387A large-scale dataset towards multi-modal multilingual instruction tuning. arXiv preprintLei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, et al. M3it: A large-scale dataset towards multi-modal multilingual instruction tuning. arXiv preprint arXiv:2306.04387, 2023.\n\nLvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, Ping Luo, arXiv:2306.09265arXiv preprintPeng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. arXiv preprint arXiv:2306.09265, 2023.\n\nAligning large multi-modal model with robust instruction tuning. Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, Lijuan Wang, arXiv:2306.14565arXiv preprintFuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023.\n\nA corpus for reasoning about natural language grounded in photographs. Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, Yoav Artzi, arXiv:1811.00491arXiv preprintAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for reasoning about natural language grounded in photographs. arXiv preprint arXiv:1811.00491, 2018.\n\nReframing human-ai collaboration for generating free-text explanations. Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, Yejin Choi, arXiv:2112.08674arXiv preprintSarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. Re- framing human-ai collaboration for generating free-text explanations. arXiv preprint arXiv:2112.08674, 2021.\n\nSocratic models: Composing zero-shot multimodal reasoning with language. Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, arXiv:2204.00598arXiv preprintAndy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.\n\nSelf-instruct: Aligning language model with self generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, A Noah, Daniel Smith, Hannaneh Khashabi, Hajishirzi, arXiv:2212.10560arXiv preprintYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.\n\nYou only look once: Unified, real-time object detection. Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJoseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779-788, 2016.\n\nBreaking common sense: Whoops! a visionand-language benchmark of synthetic and compositional images. Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, Roy Schwartz, arXiv:2303.07274arXiv preprintNitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, and Roy Schwartz. Breaking common sense: Whoops! a vision- and-language benchmark of synthetic and compositional images. arXiv preprint arXiv:2303.07274, 2023.\n\nMinigpt-4: Enhancing vision-language understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.10592arXiv preprintDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt- 4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\n\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan, arXiv:2303.04671Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprintChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.\n\nMultimodal-gpt: A vision and language model for dialogue with humans. Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, Kai Chen, arXiv:2305.04790arXiv preprintTao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with humans. arXiv preprint arXiv:2305.04790, 2023.\n\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, arXiv:2308.01390An open-source framework for training large autoregressive vision-language models. arXiv preprintAnas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023.\n\nOtter: A multi-modal model with in-context instruction tuning. Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, Ziwei Liu, arXiv:2305.03726arXiv preprintBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023.\n\nSparkles: Unlocking chats across multiple images for multimodal instructionfollowing models. Yupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Nigel Collier, Yutong Lu, arXiv:2308.16463arXiv preprintYupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Nigel Collier, and Yutong Lu. Sparkles: Unlocking chats across multiple images for multimodal instruction- following models. arXiv preprint arXiv:2308.16463, 2023.\n\nObelics: An open web-scale filtered dataset of interleaved image-text documents. Hugo Lauren\u00e7on, Lucile Saulnier, L\u00e9o Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, Matthieu Cord, Victor Sanh, Hugo Lauren\u00e7on, Lucile Saulnier, L\u00e9o Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. Obelics: An open web-scale filtered dataset of interleaved image-text documents, 2023.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, Roberta, arXiv:1907.11692A robustly optimized bert pretraining approach. arXiv preprintYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n\nAlpacafarm: A simulation framework for methods that learn from human feedback. Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback, 2023.\n\nChain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, NeurIPS. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. URL https://arxiv.org/abs/2201.11903.\n\nLarge language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, NeurIPS, 2022. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In NeurIPS, 2022. URL https://arxiv.org/abs/2205.11916.\n\nCross-modal attention congruence regularization for vision-language relation alignment. Rohan Pandey, Rulin Shao, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency, arXiv:2212.10549arXiv preprintRohan Pandey, Rulin Shao, Paul Pu Liang, Ruslan Salakhutdinov, and Louis-Philippe Morency. Cross-modal attention congruence regularization for vision-language rela- tion alignment. arXiv preprint arXiv:2212.10549, 2022.\n\nLaion-5b: An open large-scale dataset for training next generation image-text models. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, arXiv:2210.08402arXiv preprintChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wight- man, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n\nKoala: A dialogue model for academic research. Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, Dawn Song, Blog postXinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. Koala: A dialogue model for academic research. Blog post, April 2023. URL https://bair.berkeley.edu/blog/2023/04/03/koala/.\n\nBaolin Peng, Chunyuan Li, arXiv:2304.03277Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprintBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruc- tion tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.\n\nDynosaur: A dynamic growth paradigm for instruction-tuning data curation. Xiao Da Yin, Fan Liu, Ming Yin, Hritik Zhong, Jiawei Bansal, Kai-Wei Han, Chang, arXiv:2305.14327arXiv preprintDa Yin, Xiao Liu, Fan Yin, Ming Zhong, Hritik Bansal, Jiawei Han, and Kai-Wei Chang. Dynosaur: A dynamic growth paradigm for instruction-tuning data curation. arXiv preprint arXiv:2305.14327, 2023.\n\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, arXiv:2305.11206Less is more for alignment. arXiv preprintChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.\n\nGrounded language-image pre-training. Pengchuan Liunian Harold Li, Haotian Zhang, Jianwei Zhang, Chunyuan Yang, Yiwu Li, Lijuan Zhong, Lu Wang, Lei Yuan, Jenq-Neng Zhang, Hwang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionLiunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10965-10975, 2022.\n\nHigh-resolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684-10695, 2022.\n\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang, arXiv:2303.11381Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprintZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023.\n\nVipergpt: Visual inference via python execution for reasoning. D\u00eddac Sur\u00eds, Sachit Menon, Carl Vondrick, arXiv:2303.08128arXiv preprintD\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023.\n\nVisual programming: Compositional visual reasoning without training. Tanmay Gupta, Aniruddha Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionTanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14953-14962, 2023.\n\nChameleon: Plug-and-play compositional reasoning with large language models. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, arXiv:2304.09842arXiv preprintPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reason- ing with large language models. arXiv preprint arXiv:2304.09842, 2023.\n\nMultimodal-gpt: A vision and language model for dialogue with humans. Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, Kai Chen, Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with humans, 2023.\n\nLanguage is not all you need: Aligning perception with language models. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Qiang Owais Khan Mohammed, Liu, arXiv:2302.14045arXiv preprintShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045, 2023.\n\nGqa: A new dataset for real-world visual reasoning and compositional question answering. A Drew, Christopher D Hudson, Manning, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionDrew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700-6709, 2019.\n\nAre we learning yet? a meta review of evaluation failures across machine learning. Thomas Liao, Rohan Taori, Deborah Inioluwa, Ludwig Raji, Schmidt, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021Round 2Thomas Liao, Rohan Taori, Inioluwa Deborah Raji, and Ludwig Schmidt. Are we learning yet? a meta review of evaluation failures across machine learning. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.\n\nImagenet large scale visual recognition challenge. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, International journal of computer vision. 115Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211-252, 2015.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nPranav Rajpurkar, Robin Jia, Percy Liang, arXiv:1806.03822Know what you don't know: Unanswerable questions for squad. arXiv preprintPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unan- swerable questions for squad. arXiv preprint arXiv:1806.03822, 2018.\n\nRecursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, D Christopher, Manning, Y Andrew, Christopher Ng, Potts, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processingRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic com- positionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631-1642, 2013.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, arXiv:1804.07461Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprintAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\n\nSuperglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in neural information processing systems. 32Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32, 2019.\n\nA framework for few-shot language model evaluation. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony Dipofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle Mcdonell, Niklas Muennighoff, Version v0. 0.1.Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A framework for few-shot language model evaluation. Version v0. 0.1. Sept, 2021.\n\nHolistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, arXiv:2211.09110arXiv preprintPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.\n\nThe effect of natural distribution shift on question answering models. John Miller, Karl Krauth, Benjamin Recht, Ludwig Schmidt, International Conference on Machine Learning. PMLRJohn Miller, Karl Krauth, Benjamin Recht, and Ludwig Schmidt. The effect of natural distribution shift on question answering models. In International Conference on Machine Learning, pages 6905-6916. PMLR, 2020.\n\nDo cifar-10 classifiers generalize to cifar-10?. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar, arXiv:1806.00451arXiv preprintBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do cifar-10 classifiers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018.\n\nDo imagenet classifiers generalize to imagenet?. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar, International conference on machine learning. PMLRBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International conference on machine learning, pages 5389-5400. PMLR, 2019.\n\nA meta-analysis of overfitting in machine learning. Rebecca Roelofs, Vaishaal Shankar, Benjamin Recht, Sara Fridovich-Keil, Moritz Hardt, John Miller, Ludwig Schmidt, Advances in Neural Information Processing Systems. 32Rebecca Roelofs, Vaishaal Shankar, Benjamin Recht, Sara Fridovich-Keil, Moritz Hardt, John Miller, and Ludwig Schmidt. A meta-analysis of overfitting in machine learning. Advances in Neural Information Processing Systems, 32, 2019.\n\nA survey on multimodal large language models. Chaoyou Shukang Yin, Sirui Fu, Ke Zhao, Xing Li, Tong Sun, Enhong Xu, Chen, arXiv:2306.13549arXiv preprintShukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023.\n\nData contamination: From memorization to exploitation. Inbal Magar, Roy Schwartz, arXiv:2203.08242arXiv preprintInbal Magar and Roy Schwartz. Data contamination: From memorization to exploita- tion. arXiv preprint arXiv:2203.08242, 2022.\n\nStop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks. Alon Jacovi, Avi Caciularu, Omer Goldman, Yoav Goldberg, arXiv:2305.10160arXiv preprintAlon Jacovi, Avi Caciularu, Omer Goldman, and Yoav Goldberg. Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks. arXiv preprint arXiv:2305.10160, 2023.\n\nFrom recognition to cognition: Visual commonsense reasoning. Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cog- nition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6720-6731, 2019.\n\nLiunian Harold Da Yin, Ziniu Li, Nanyun Hu, Kai-Wei Peng, Chang, arXiv:2109.06860Broaden the vision: Geo-diverse visual commonsense reasoning. arXiv preprintDa Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, and Kai-Wei Chang. Broaden the vision: Geo-diverse visual commonsense reasoning. arXiv preprint arXiv:2109.06860, 2021.\n\nDo androids laugh at electric sheep?. Jack Hessel, Ana Marasovi\u0107, Jena D Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, Yejin Choi, arXiv:2209.06293humor\" understanding\" benchmarks from the new yorker caption contest. arXiv preprintJack Hessel, Ana Marasovi\u0107, Jena D Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, and Yejin Choi. Do androids laugh at electric sheep? humor\" understanding\" benchmarks from the new yorker caption contest. arXiv preprint arXiv:2209.06293, 2022.\n\nClevr: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJustin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2901-2910, 2017.\n\nAnya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert D Hawkins, Yoav Artzi, arXiv:2211.16492Abstract visual reasoning with tangram shapes. arXiv preprintAnya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert D Hawkins, and Yoav Artzi. Abstract visual reasoning with tangram shapes. arXiv preprint arXiv:2211.16492, 2022.\n\nWhat do you meme? generating explanations for visual semantic role labelling in memes. Shivam Sharma, Siddhant Agarwal, Tharun Suresh, Preslav Nakov, Shad Md, Tanmoy Akhtar, Charkraborty, arXiv:2212.00715arXiv preprintShivam Sharma, Siddhant Agarwal, Tharun Suresh, Preslav Nakov, Md Shad Akhtar, and Tanmoy Charkraborty. What do you meme? generating explanations for visual semantic role labelling in memes. arXiv preprint arXiv:2212.00715, 2022.\n\nLearn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, The 36th Conference on Neural Information Processing Systems (NeurIPS). 2022Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022.\n\nOk-vqa: A visual question answering benchmark requiring external knowledge. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi, Proceedings of the IEEE/cvf conference on computer vision and pattern recognition. the IEEE/cvf conference on computer vision and pattern recognitionKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 3195- 3204, 2019.\n\nA-okvqa: A benchmark for visual question answering using world knowledge. Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, Roozbeh Mottaghi, arXivDustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. arXiv, 2022.\n\nVizwiz: nearly real-time answers to visual questions. P Jeffrey, Chandrika Bigham, Hanjie Jayant, Greg Ji, Andrew Little, Miller, C Robert, Robin Miller, Aubrey Miller, Brandyn Tatarowicz, Samual White, White, Proceedings of the 23nd annual ACM symposium on User interface software and technology. the 23nd annual ACM symposium on User interface software and technologyJeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, et al. Vizwiz: nearly real-time answers to visual questions. In Proceedings of the 23nd annual ACM symposium on User interface software and technology, pages 333-342, 2010.\n\nTextcaps: a dataset for image captioning with reading comprehension. Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, Amanpreet Singh, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerProceedings, Part II 16Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16, pages 742-758. Springer, 2020.\n\nBenno Krojer, Vaibhav Adlakha, Vibhav Vineet, Yash Goyal, Edoardo Ponti, Siva Reddy, arXiv:2203.15867Image retrieval from contextual descriptions. arXiv preprintBenno Krojer, Vaibhav Adlakha, Vibhav Vineet, Yash Goyal, Edoardo Ponti, and Siva Reddy. Image retrieval from contextual descriptions. arXiv preprint arXiv:2203.15867, 2022.\n\nLearning to describe differences between pairs of similar images. Harsh Jhamtani, Taylor Berg-Kirkpatrick, arXiv:1808.10584arXiv preprintHarsh Jhamtani and Taylor Berg-Kirkpatrick. Learning to describe differences be- tween pairs of similar images. arXiv preprint arXiv:1808.10584, 2018.\n\nYonatan Bitton, Ron Yosef, Eli Strugo, Dafna Shahaf, Roy Schwartz, Gabriel Stanovsky, Vasr, arXiv:2212.04542Visual analogies of situation recognition. arXiv preprintYonatan Bitton, Ron Yosef, Eli Strugo, Dafna Shahaf, Roy Schwartz, and Gabriel Stanovsky. Vasr: Visual analogies of situation recognition. arXiv preprint arXiv:2212.04542, 2022.\n\nWinogavil: Gamified association benchmark to challenge vision-and-language models. Yonatan Bitton, Nitzan Bitton Guetta, Ron Yosef, Yuval Elovici, Mohit Bansal, Gabriel Stanovsky, Roy Schwartz, Advances in Neural Information Processing Systems. 35Yonatan Bitton, Nitzan Bitton Guetta, Ron Yosef, Yuval Elovici, Mohit Bansal, Gabriel Stanovsky, and Roy Schwartz. Winogavil: Gamified association benchmark to challenge vision-and-language models. Advances in Neural Information Processing Systems, 35:26549-26564, 2022.\n\nIrfl: Image recognition of figurative language. Ron Yosef, Yonatan Bitton, Dafna Shahaf, arXiv:2303.15445arXiv preprintRon Yosef, Yonatan Bitton, and Dafna Shahaf. Irfl: Image recognition of figurative language. arXiv preprint arXiv:2303.15445, 2023.\n\nPan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, Song-Chun Zhu, arXiv:2110.13214Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprintPan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021.\n\nPick-a-pic: An open dataset of user preferences for text-to-image generation. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, Omer Levy, arXiv:2305.01569arXiv preprintYuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. arXiv preprint arXiv:2305.01569, 2023.\n", "annotations": {"author": "[{\"end\":112,\"start\":97},{\"end\":127,\"start\":113},{\"end\":140,\"start\":128},{\"end\":152,\"start\":141},{\"end\":165,\"start\":153},{\"end\":180,\"start\":166},{\"end\":194,\"start\":181},{\"end\":207,\"start\":195},{\"end\":223,\"start\":208}]", "publisher": null, "author_last_name": "[{\"end\":111,\"start\":105},{\"end\":126,\"start\":120},{\"end\":139,\"start\":133},{\"end\":151,\"start\":147},{\"end\":164,\"start\":161},{\"end\":179,\"start\":171},{\"end\":193,\"start\":186},{\"end\":206,\"start\":201},{\"end\":222,\"start\":215}]", "author_first_name": "[{\"end\":104,\"start\":97},{\"end\":119,\"start\":113},{\"end\":132,\"start\":128},{\"end\":146,\"start\":141},{\"end\":160,\"start\":153},{\"end\":170,\"start\":166},{\"end\":185,\"start\":181},{\"end\":200,\"start\":195},{\"end\":214,\"start\":208}]", "author_affiliation": null, "title": "[{\"end\":94,\"start\":1},{\"end\":317,\"start\":224}]", "venue": null, "abstract": "[{\"end\":1825,\"start\":319}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2297,\"start\":2294},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2342,\"start\":2339},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2344,\"start\":2342},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2346,\"start\":2344},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2348,\"start\":2346},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2350,\"start\":2348},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2352,\"start\":2350},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2470,\"start\":2467},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2472,\"start\":2470},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2475,\"start\":2472},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3139,\"start\":3135},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3142,\"start\":3139},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3154,\"start\":3150},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3170,\"start\":3166},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3173,\"start\":3170},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3176,\"start\":3173},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3179,\"start\":3176},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3182,\"start\":3179},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3292,\"start\":3288},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3317,\"start\":3313},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4663,\"start\":4659},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4697,\"start\":4693},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4773,\"start\":4769},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4776,\"start\":4773},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4779,\"start\":4776},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5156,\"start\":5152},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5159,\"start\":5156},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5938,\"start\":5934},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5975,\"start\":5971},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6194,\"start\":6190},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6197,\"start\":6194},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6861,\"start\":6857},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7397,\"start\":7393},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7411,\"start\":7407},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7424,\"start\":7420},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7436,\"start\":7432},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7456,\"start\":7452},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7953,\"start\":7949},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7956,\"start\":7953},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7959,\"start\":7956},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7962,\"start\":7959},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7965,\"start\":7962},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7968,\"start\":7965},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11600,\"start\":11596},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12304,\"start\":12303},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14437,\"start\":14436},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14693,\"start\":14692},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16416,\"start\":16412},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17136,\"start\":17132},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18484,\"start\":18480},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18728,\"start\":18724},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19371,\"start\":19367},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19438,\"start\":19434},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21470,\"start\":21466},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21773,\"start\":21769},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21796,\"start\":21792},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21814,\"start\":21810},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21833,\"start\":21829},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21858,\"start\":21854},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21877,\"start\":21873},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":21897,\"start\":21893},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":21918,\"start\":21914},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21940,\"start\":21936},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21943,\"start\":21940},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":21959,\"start\":21955},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":21970,\"start\":21966},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":21987,\"start\":21983},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":22031,\"start\":22027},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22223,\"start\":22219},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":22238,\"start\":22234},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22981,\"start\":22977},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23061,\"start\":23060},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25392,\"start\":25388},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25407,\"start\":25403},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25421,\"start\":25417},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25433,\"start\":25429},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25453,\"start\":25449},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":25500,\"start\":25496},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25935,\"start\":25932},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":25938,\"start\":25935},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25965,\"start\":25962},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":26646,\"start\":26642},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":26649,\"start\":26646},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":30431,\"start\":30427},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":30434,\"start\":30431},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30437,\"start\":30434},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":30440,\"start\":30437},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30443,\"start\":30440},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":30446,\"start\":30443},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":30574,\"start\":30570},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30594,\"start\":30590},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30716,\"start\":30712},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30879,\"start\":30875},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30895,\"start\":30891},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":30939,\"start\":30935},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30961,\"start\":30957},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30976,\"start\":30972},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":30991,\"start\":30988},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31505,\"start\":31502},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31525,\"start\":31521},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":31537,\"start\":31533},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31556,\"start\":31552},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":31568,\"start\":31564},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31588,\"start\":31584},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":31776,\"start\":31772},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":31779,\"start\":31776},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":31782,\"start\":31779},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":31785,\"start\":31782},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":31788,\"start\":31785},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31973,\"start\":31969},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":31996,\"start\":31992},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":32030,\"start\":32026},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":32083,\"start\":32079},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":32086,\"start\":32083},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":32130,\"start\":32126},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":32133,\"start\":32130},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":32136,\"start\":32133},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32161,\"start\":32157},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":32164,\"start\":32161},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":32167,\"start\":32164},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":32170,\"start\":32167},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":32173,\"start\":32170},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":32176,\"start\":32173},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32179,\"start\":32176},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":32440,\"start\":32436},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":32443,\"start\":32440},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":32446,\"start\":32443},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":33078,\"start\":33074},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":33307,\"start\":33303},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":33322,\"start\":33318},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":33425,\"start\":33421},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":33435,\"start\":33431},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":33456,\"start\":33452},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":33459,\"start\":33456},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":33473,\"start\":33469},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":33476,\"start\":33473},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":33689,\"start\":33685},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":33692,\"start\":33689},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":33695,\"start\":33692},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":33698,\"start\":33695},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":34091,\"start\":34087},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":34094,\"start\":34091},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":36956,\"start\":36953},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":37013,\"start\":37009},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":37016,\"start\":37013},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":38044,\"start\":38040},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":38833,\"start\":38829},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":38868,\"start\":38864},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":38917,\"start\":38913},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":38963,\"start\":38959},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":39014,\"start\":39010},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":39045,\"start\":39041},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":39085,\"start\":39081},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":39128,\"start\":39124},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":39163,\"start\":39159},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":39202,\"start\":39198},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":39259,\"start\":39255},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":39306,\"start\":39302},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":39338,\"start\":39334},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":39373,\"start\":39369},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":39429,\"start\":39425},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":39504,\"start\":39500},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":39553,\"start\":39549},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":39597,\"start\":39593},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":39632,\"start\":39628},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":39666,\"start\":39662},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":39698,\"start\":39694},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":39739,\"start\":39735},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":39790,\"start\":39786},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":39835,\"start\":39830},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":39883,\"start\":39878},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":40036,\"start\":40032},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":41124,\"start\":41121},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":44976,\"start\":44973},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":50429,\"start\":50426},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":53828,\"start\":53825},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":62483,\"start\":62479},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":62485,\"start\":62484},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":62931,\"start\":62930}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":54226,\"start\":54073},{\"attributes\":{\"id\":\"fig_1\"},\"end\":54482,\"start\":54227},{\"attributes\":{\"id\":\"fig_2\"},\"end\":54896,\"start\":54483},{\"attributes\":{\"id\":\"fig_3\"},\"end\":55033,\"start\":54897},{\"attributes\":{\"id\":\"fig_4\"},\"end\":55312,\"start\":55034},{\"attributes\":{\"id\":\"fig_5\"},\"end\":55572,\"start\":55313},{\"attributes\":{\"id\":\"fig_6\"},\"end\":55669,\"start\":55573},{\"attributes\":{\"id\":\"fig_7\"},\"end\":55769,\"start\":55670},{\"attributes\":{\"id\":\"fig_8\"},\"end\":55899,\"start\":55770},{\"attributes\":{\"id\":\"fig_9\"},\"end\":55975,\"start\":55900},{\"attributes\":{\"id\":\"fig_10\"},\"end\":57206,\"start\":55976},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":57584,\"start\":57207},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":59405,\"start\":57585},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":60317,\"start\":59406},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":61843,\"start\":60318},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":61948,\"start\":61844},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":61956,\"start\":61949}]", "paragraph": "[{\"end\":2071,\"start\":1860},{\"end\":2781,\"start\":2119},{\"end\":3550,\"start\":2783},{\"end\":4267,\"start\":3552},{\"end\":4896,\"start\":4269},{\"end\":5483,\"start\":4898},{\"end\":6198,\"start\":5485},{\"end\":6276,\"start\":6223},{\"end\":6323,\"start\":6294},{\"end\":6354,\"start\":6339},{\"end\":6601,\"start\":6374},{\"end\":6942,\"start\":6603},{\"end\":7646,\"start\":6944},{\"end\":8969,\"start\":7648},{\"end\":9866,\"start\":9393},{\"end\":10681,\"start\":9868},{\"end\":10745,\"start\":10691},{\"end\":11072,\"start\":10747},{\"end\":11364,\"start\":11078},{\"end\":12014,\"start\":11378},{\"end\":12110,\"start\":12016},{\"end\":12716,\"start\":12112},{\"end\":13229,\"start\":12718},{\"end\":13844,\"start\":13231},{\"end\":14895,\"start\":13846},{\"end\":15214,\"start\":14938},{\"end\":15430,\"start\":15216},{\"end\":15462,\"start\":15442},{\"end\":16049,\"start\":15565},{\"end\":16156,\"start\":16074},{\"end\":16635,\"start\":16208},{\"end\":17163,\"start\":16637},{\"end\":17245,\"start\":17165},{\"end\":17664,\"start\":17320},{\"end\":17948,\"start\":17666},{\"end\":18168,\"start\":17950},{\"end\":18204,\"start\":18184},{\"end\":18596,\"start\":18206},{\"end\":19299,\"start\":18642},{\"end\":19738,\"start\":19344},{\"end\":20146,\"start\":19754},{\"end\":20279,\"start\":20190},{\"end\":20732,\"start\":20292},{\"end\":21158,\"start\":20749},{\"end\":21471,\"start\":21160},{\"end\":21736,\"start\":21473},{\"end\":22432,\"start\":21747},{\"end\":22944,\"start\":22453},{\"end\":23235,\"start\":22946},{\"end\":25234,\"start\":23237},{\"end\":25992,\"start\":25275},{\"end\":26532,\"start\":25994},{\"end\":26944,\"start\":26534},{\"end\":27835,\"start\":26946},{\"end\":28933,\"start\":27837},{\"end\":29361,\"start\":28935},{\"end\":29851,\"start\":29363},{\"end\":30227,\"start\":29853},{\"end\":31089,\"start\":30244},{\"end\":31790,\"start\":31091},{\"end\":32315,\"start\":31792},{\"end\":32915,\"start\":32317},{\"end\":33699,\"start\":32917},{\"end\":34095,\"start\":33701},{\"end\":34868,\"start\":34110},{\"end\":35976,\"start\":34884},{\"end\":36586,\"start\":36007},{\"end\":38144,\"start\":36588},{\"end\":38548,\"start\":38215},{\"end\":39464,\"start\":38600},{\"end\":39916,\"start\":39466},{\"end\":40315,\"start\":39933},{\"end\":40507,\"start\":40317},{\"end\":40885,\"start\":40559},{\"end\":41587,\"start\":40925},{\"end\":41619,\"start\":41589},{\"end\":41830,\"start\":41621},{\"end\":41947,\"start\":41832},{\"end\":42074,\"start\":41949},{\"end\":42296,\"start\":42076},{\"end\":42322,\"start\":42298},{\"end\":42460,\"start\":42324},{\"end\":42596,\"start\":42462},{\"end\":42679,\"start\":42598},{\"end\":43199,\"start\":42681},{\"end\":43444,\"start\":43230},{\"end\":43481,\"start\":43446},{\"end\":43769,\"start\":43483},{\"end\":43888,\"start\":43771},{\"end\":44024,\"start\":43890},{\"end\":44133,\"start\":44026},{\"end\":44195,\"start\":44135},{\"end\":44261,\"start\":44197},{\"end\":44313,\"start\":44263},{\"end\":44390,\"start\":44315},{\"end\":44484,\"start\":44392},{\"end\":44542,\"start\":44486},{\"end\":44728,\"start\":44544},{\"end\":44775,\"start\":44730},{\"end\":44829,\"start\":44777},{\"end\":44977,\"start\":44831},{\"end\":45009,\"start\":44979},{\"end\":45220,\"start\":45011},{\"end\":45337,\"start\":45222},{\"end\":45732,\"start\":45339},{\"end\":45987,\"start\":45734},{\"end\":46203,\"start\":45989},{\"end\":46339,\"start\":46205},{\"end\":46422,\"start\":46341},{\"end\":47077,\"start\":46424},{\"end\":47337,\"start\":47108},{\"end\":47374,\"start\":47339},{\"end\":47741,\"start\":47376},{\"end\":48036,\"start\":47743},{\"end\":48996,\"start\":48038},{\"end\":49105,\"start\":48998},{\"end\":49167,\"start\":49107},{\"end\":49424,\"start\":49169},{\"end\":49676,\"start\":49426},{\"end\":49777,\"start\":49678},{\"end\":50051,\"start\":49779},{\"end\":50430,\"start\":50053},{\"end\":50462,\"start\":50432},{\"end\":50804,\"start\":50464},{\"end\":50830,\"start\":50806},{\"end\":50993,\"start\":50832},{\"end\":51049,\"start\":50995},{\"end\":51158,\"start\":51051},{\"end\":51294,\"start\":51160},{\"end\":51511,\"start\":51296},{\"end\":51676,\"start\":51513},{\"end\":51813,\"start\":51678},{\"end\":51902,\"start\":51815},{\"end\":52258,\"start\":51904},{\"end\":52320,\"start\":52260},{\"end\":52446,\"start\":52322},{\"end\":52485,\"start\":52448},{\"end\":52524,\"start\":52487},{\"end\":52683,\"start\":52526},{\"end\":52865,\"start\":52685},{\"end\":53082,\"start\":52867},{\"end\":53198,\"start\":53084},{\"end\":53311,\"start\":53200},{\"end\":53611,\"start\":53313},{\"end\":53665,\"start\":53613},{\"end\":53727,\"start\":53667},{\"end\":54072,\"start\":53729}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9329,\"start\":8970},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15564,\"start\":15463},{\"attributes\":{\"id\":\"formula_2\"},\"end\":40558,\"start\":40508}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":7874,\"start\":7867},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":8879,\"start\":8872},{\"end\":15338,\"start\":15331},{\"end\":15613,\"start\":15606},{\"end\":16886,\"start\":16877},{\"end\":18712,\"start\":18705},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23714,\"start\":23707},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":29597,\"start\":29589},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":38610,\"start\":38603},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":41032,\"start\":41024},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":41070,\"start\":41062},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":41493,\"start\":41485}]", "section_header": "[{\"end\":1858,\"start\":1827},{\"end\":2102,\"start\":2074},{\"attributes\":{\"n\":\"1\"},\"end\":2117,\"start\":2105},{\"end\":6221,\"start\":6201},{\"end\":6292,\"start\":6279},{\"end\":6337,\"start\":6326},{\"end\":6372,\"start\":6357},{\"end\":9373,\"start\":9331},{\"attributes\":{\"n\":\"2.1\"},\"end\":9391,\"start\":9376},{\"end\":10689,\"start\":10684},{\"end\":11076,\"start\":11075},{\"end\":11368,\"start\":11367},{\"end\":11376,\"start\":11371},{\"attributes\":{\"n\":\"2.2\"},\"end\":14936,\"start\":14898},{\"end\":15440,\"start\":15433},{\"attributes\":{\"n\":\"3\"},\"end\":16072,\"start\":16052},{\"attributes\":{\"n\":\"3.1\"},\"end\":16206,\"start\":16159},{\"end\":17284,\"start\":17248},{\"end\":17318,\"start\":17287},{\"end\":18182,\"start\":18171},{\"attributes\":{\"n\":\"3.2\"},\"end\":18640,\"start\":18599},{\"attributes\":{\"n\":\"3.3\"},\"end\":19342,\"start\":19302},{\"attributes\":{\"n\":\"4\"},\"end\":19752,\"start\":19741},{\"end\":20154,\"start\":20149},{\"end\":20188,\"start\":20157},{\"end\":20290,\"start\":20282},{\"end\":20747,\"start\":20735},{\"attributes\":{\"n\":\"4.1\"},\"end\":21745,\"start\":21739},{\"attributes\":{\"n\":\"4.2\"},\"end\":22451,\"start\":22435},{\"attributes\":{\"n\":\"4.3\"},\"end\":25273,\"start\":25237},{\"attributes\":{\"n\":\"5\"},\"end\":30242,\"start\":30230},{\"attributes\":{\"n\":\"6\"},\"end\":34108,\"start\":34098},{\"attributes\":{\"n\":\"7\"},\"end\":34882,\"start\":34871},{\"end\":36005,\"start\":35979},{\"end\":38165,\"start\":38147},{\"end\":38213,\"start\":38168},{\"end\":38598,\"start\":38551},{\"end\":39931,\"start\":39919},{\"end\":40923,\"start\":40888},{\"end\":43228,\"start\":43202},{\"end\":47106,\"start\":47080},{\"end\":54075,\"start\":54074},{\"end\":54238,\"start\":54228},{\"end\":54494,\"start\":54484},{\"end\":54908,\"start\":54898},{\"end\":55045,\"start\":55035},{\"end\":55324,\"start\":55314},{\"end\":55585,\"start\":55574},{\"end\":55682,\"start\":55671},{\"end\":55782,\"start\":55771},{\"end\":55912,\"start\":55901},{\"end\":55980,\"start\":55977},{\"end\":57217,\"start\":57208},{\"end\":59416,\"start\":59407},{\"end\":60328,\"start\":60319},{\"end\":61854,\"start\":61845},{\"end\":61955,\"start\":61950}]", "table": "[{\"end\":57584,\"start\":57384},{\"end\":59405,\"start\":57760},{\"end\":60317,\"start\":59759},{\"end\":61843,\"start\":60680}]", "figure_caption": "[{\"end\":54226,\"start\":54076},{\"end\":54482,\"start\":54240},{\"end\":54896,\"start\":54496},{\"end\":55033,\"start\":54910},{\"end\":55312,\"start\":55047},{\"end\":55572,\"start\":55326},{\"end\":55669,\"start\":55588},{\"end\":55769,\"start\":55685},{\"end\":55899,\"start\":55785},{\"end\":55975,\"start\":55915},{\"end\":57206,\"start\":55982},{\"end\":57384,\"start\":57219},{\"end\":57760,\"start\":57587},{\"end\":59759,\"start\":59418},{\"end\":60680,\"start\":60330},{\"end\":61948,\"start\":61856}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3923,\"start\":3914},{\"end\":4557,\"start\":4549},{\"end\":6441,\"start\":6433},{\"end\":7614,\"start\":7606},{\"end\":8967,\"start\":8959},{\"end\":9865,\"start\":9857},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10011,\"start\":10003},{\"end\":11501,\"start\":11493},{\"end\":12526,\"start\":12518},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12910,\"start\":12902},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13669,\"start\":13661},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14725,\"start\":14717},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14986,\"start\":14978},{\"end\":16634,\"start\":16626},{\"end\":18235,\"start\":18227},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18854,\"start\":18846},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19634,\"start\":19625},{\"end\":21215,\"start\":21207},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22900,\"start\":22891},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27864,\"start\":27856},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29886,\"start\":29878},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":37903,\"start\":37894},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38335,\"start\":38325},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38377,\"start\":38367},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38414,\"start\":38405},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38428,\"start\":38419},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38463,\"start\":38454},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38475,\"start\":38466},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":44871,\"start\":44862},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":50322,\"start\":50313},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":53761,\"start\":53752}]", "bib_author_first_name": "[{\"end\":63399,\"start\":63393},{\"end\":63414,\"start\":63408},{\"end\":63424,\"start\":63420},{\"end\":63435,\"start\":63431},{\"end\":63447,\"start\":63443},{\"end\":63460,\"start\":63457},{\"end\":63475,\"start\":63471},{\"end\":63491,\"start\":63483},{\"end\":63503,\"start\":63500},{\"end\":63514,\"start\":63510},{\"end\":63867,\"start\":63863},{\"end\":63883,\"start\":63876},{\"end\":63890,\"start\":63888},{\"end\":63903,\"start\":63898},{\"end\":63920,\"start\":63913},{\"end\":63939,\"start\":63933},{\"end\":63954,\"start\":63949},{\"end\":63970,\"start\":63962},{\"end\":63988,\"start\":63980},{\"end\":64000,\"start\":63996},{\"end\":64407,\"start\":64402},{\"end\":64420,\"start\":64413},{\"end\":64429,\"start\":64428},{\"end\":64445,\"start\":64439},{\"end\":64457,\"start\":64452},{\"end\":64461,\"start\":64458},{\"end\":64472,\"start\":64467},{\"end\":64480,\"start\":64477},{\"end\":64494,\"start\":64493},{\"end\":64509,\"start\":64503},{\"end\":64856,\"start\":64849},{\"end\":64870,\"start\":64863},{\"end\":64884,\"start\":64879},{\"end\":64910,\"start\":64903},{\"end\":64926,\"start\":64918},{\"end\":64943,\"start\":64936},{\"end\":64955,\"start\":64950},{\"end\":64967,\"start\":64963},{\"end\":64995,\"start\":64989},{\"end\":65012,\"start\":65007},{\"end\":65620,\"start\":65615},{\"end\":65634,\"start\":65628},{\"end\":65652,\"start\":65646},{\"end\":65664,\"start\":65660},{\"end\":65680,\"start\":65673},{\"end\":65691,\"start\":65685},{\"end\":65707,\"start\":65702},{\"end\":65724,\"start\":65715},{\"end\":65726,\"start\":65725},{\"end\":66061,\"start\":66054},{\"end\":66077,\"start\":66070},{\"end\":66084,\"start\":66082},{\"end\":66094,\"start\":66090},{\"end\":66110,\"start\":66102},{\"end\":66118,\"start\":66115},{\"end\":66133,\"start\":66126},{\"end\":66147,\"start\":66141},{\"end\":66163,\"start\":66156},{\"end\":66178,\"start\":66172},{\"end\":66180,\"start\":66179},{\"end\":66194,\"start\":66191},{\"end\":66207,\"start\":66203},{\"end\":66209,\"start\":66208},{\"end\":66622,\"start\":66619},{\"end\":66638,\"start\":66630},{\"end\":66649,\"start\":66645},{\"end\":66664,\"start\":66657},{\"end\":66679,\"start\":66674},{\"end\":66681,\"start\":66680},{\"end\":66698,\"start\":66690},{\"end\":66715,\"start\":66709},{\"end\":66735,\"start\":66729},{\"end\":66749,\"start\":66743},{\"end\":66764,\"start\":66758},{\"end\":67107,\"start\":67098},{\"end\":67125,\"start\":67119},{\"end\":67139,\"start\":67134},{\"end\":67155,\"start\":67148},{\"end\":67169,\"start\":67163},{\"end\":67182,\"start\":67178},{\"end\":67196,\"start\":67192},{\"end\":67223,\"start\":67216},{\"end\":67240,\"start\":67231},{\"end\":67588,\"start\":67584},{\"end\":67605,\"start\":67598},{\"end\":67621,\"start\":67614},{\"end\":67637,\"start\":67631},{\"end\":67658,\"start\":67648},{\"end\":67676,\"start\":67668},{\"end\":67691,\"start\":67686},{\"end\":67714,\"start\":67710},{\"end\":67728,\"start\":67722},{\"end\":68100,\"start\":68096},{\"end\":68116,\"start\":68111},{\"end\":68128,\"start\":68122},{\"end\":68142,\"start\":68138},{\"end\":68156,\"start\":68151},{\"end\":68172,\"start\":68165},{\"end\":68185,\"start\":68178},{\"end\":68202,\"start\":68195},{\"end\":68216,\"start\":68211},{\"end\":68229,\"start\":68224},{\"end\":68243,\"start\":68238},{\"end\":68259,\"start\":68255},{\"end\":68263,\"start\":68260},{\"end\":68276,\"start\":68269},{\"end\":68294,\"start\":68286},{\"end\":68311,\"start\":68305},{\"end\":68692,\"start\":68688},{\"end\":68723,\"start\":68716},{\"end\":68740,\"start\":68733},{\"end\":68750,\"start\":68746},{\"end\":68762,\"start\":68758},{\"end\":68774,\"start\":68769},{\"end\":68789,\"start\":68783},{\"end\":68801,\"start\":68796},{\"end\":68817,\"start\":68810},{\"end\":68833,\"start\":68828},{\"end\":68849,\"start\":68844},{\"end\":68862,\"start\":68856},{\"end\":68881,\"start\":68875},{\"end\":68894,\"start\":68888},{\"end\":68904,\"start\":68900},{\"end\":68919,\"start\":68911},{\"end\":69558,\"start\":69551},{\"end\":69572,\"start\":69564},{\"end\":69585,\"start\":69577},{\"end\":69594,\"start\":69590},{\"end\":69598,\"start\":69595},{\"end\":69816,\"start\":69808},{\"end\":69828,\"start\":69822},{\"end\":69839,\"start\":69833},{\"end\":69861,\"start\":69844},{\"end\":69874,\"start\":69869},{\"end\":69889,\"start\":69881},{\"end\":69902,\"start\":69896},{\"end\":69914,\"start\":69907},{\"end\":69927,\"start\":69921},{\"end\":70328,\"start\":70322},{\"end\":70339,\"start\":70333},{\"end\":70350,\"start\":70344},{\"end\":70367,\"start\":70361},{\"end\":70606,\"start\":70602},{\"end\":70619,\"start\":70612},{\"end\":70631,\"start\":70625},{\"end\":70643,\"start\":70639},{\"end\":70655,\"start\":70649},{\"end\":70667,\"start\":70662},{\"end\":70677,\"start\":70674},{\"end\":70688,\"start\":70685},{\"end\":70700,\"start\":70693},{\"end\":70712,\"start\":70705},{\"end\":71106,\"start\":71099},{\"end\":71118,\"start\":71111},{\"end\":71129,\"start\":71123},{\"end\":71139,\"start\":71134},{\"end\":71148,\"start\":71144},{\"end\":71160,\"start\":71154},{\"end\":71174,\"start\":71167},{\"end\":71186,\"start\":71181},{\"end\":71200,\"start\":71191},{\"end\":71210,\"start\":71206},{\"end\":71495,\"start\":71489},{\"end\":71504,\"start\":71500},{\"end\":71517,\"start\":71510},{\"end\":71527,\"start\":71522},{\"end\":71535,\"start\":71532},{\"end\":71546,\"start\":71542},{\"end\":71897,\"start\":71893},{\"end\":71910,\"start\":71905},{\"end\":71924,\"start\":71917},{\"end\":71944,\"start\":71939},{\"end\":71956,\"start\":71952},{\"end\":72439,\"start\":72431},{\"end\":72452,\"start\":72445},{\"end\":72465,\"start\":72460},{\"end\":72481,\"start\":72476},{\"end\":72494,\"start\":72488},{\"end\":72507,\"start\":72503},{\"end\":72522,\"start\":72517},{\"end\":72541,\"start\":72531},{\"end\":73016,\"start\":73007},{\"end\":73033,\"start\":73024},{\"end\":73049,\"start\":73043},{\"end\":73062,\"start\":73054},{\"end\":73078,\"start\":73073},{\"end\":73094,\"start\":73086},{\"end\":73108,\"start\":73104},{\"end\":73516,\"start\":73508},{\"end\":73529,\"start\":73523},{\"end\":73543,\"start\":73539},{\"end\":73922,\"start\":73917},{\"end\":73941,\"start\":73935},{\"end\":73959,\"start\":73952},{\"end\":73976,\"start\":73972},{\"end\":73994,\"start\":73986},{\"end\":74397,\"start\":74392},{\"end\":74406,\"start\":74402},{\"end\":74421,\"start\":74412},{\"end\":74434,\"start\":74428},{\"end\":74441,\"start\":74440},{\"end\":74453,\"start\":74448},{\"end\":74798,\"start\":74797},{\"end\":75021,\"start\":75014},{\"end\":75033,\"start\":75029},{\"end\":75048,\"start\":75041},{\"end\":75060,\"start\":75057},{\"end\":75074,\"start\":75068},{\"end\":75076,\"start\":75075},{\"end\":75090,\"start\":75087},{\"end\":75376,\"start\":75369},{\"end\":75392,\"start\":75387},{\"end\":75405,\"start\":75401},{\"end\":75420,\"start\":75412},{\"end\":75668,\"start\":75660},{\"end\":75887,\"start\":75878},{\"end\":75902,\"start\":75898},{\"end\":76236,\"start\":76225},{\"end\":76255,\"start\":76247},{\"end\":76269,\"start\":76265},{\"end\":76468,\"start\":76462},{\"end\":76482,\"start\":76476},{\"end\":76497,\"start\":76492},{\"end\":76503,\"start\":76502},{\"end\":76516,\"start\":76512},{\"end\":76771,\"start\":76764},{\"end\":76780,\"start\":76776},{\"end\":76791,\"start\":76787},{\"end\":76991,\"start\":76988},{\"end\":77001,\"start\":76996},{\"end\":77015,\"start\":77007},{\"end\":77025,\"start\":77020},{\"end\":77037,\"start\":77032},{\"end\":77051,\"start\":77044},{\"end\":77062,\"start\":77057},{\"end\":77074,\"start\":77067},{\"end\":77089,\"start\":77081},{\"end\":77096,\"start\":77094},{\"end\":77535,\"start\":77531},{\"end\":77545,\"start\":77540},{\"end\":77559,\"start\":77552},{\"end\":77571,\"start\":77567},{\"end\":77581,\"start\":77577},{\"end\":77591,\"start\":77587},{\"end\":77604,\"start\":77597},{\"end\":77617,\"start\":77611},{\"end\":77627,\"start\":77625},{\"end\":77638,\"start\":77634},{\"end\":77984,\"start\":77978},{\"end\":77995,\"start\":77990},{\"end\":78007,\"start\":78001},{\"end\":78020,\"start\":78012},{\"end\":78032,\"start\":78027},{\"end\":78047,\"start\":78041},{\"end\":78345,\"start\":78340},{\"end\":78361,\"start\":78352},{\"end\":78372,\"start\":78368},{\"end\":78384,\"start\":78380},{\"end\":78398,\"start\":78392},{\"end\":78408,\"start\":78404},{\"end\":78714,\"start\":78709},{\"end\":78730,\"start\":78726},{\"end\":78745,\"start\":78739},{\"end\":78763,\"start\":78759},{\"end\":78776,\"start\":78771},{\"end\":79082,\"start\":79078},{\"end\":79094,\"start\":79089},{\"end\":79110,\"start\":79105},{\"end\":79128,\"start\":79119},{\"end\":79148,\"start\":79142},{\"end\":79161,\"start\":79155},{\"end\":79178,\"start\":79170},{\"end\":79193,\"start\":79188},{\"end\":79210,\"start\":79203},{\"end\":79222,\"start\":79217},{\"end\":79620,\"start\":79613},{\"end\":79634,\"start\":79627},{\"end\":79649,\"start\":79642},{\"end\":79663,\"start\":79658},{\"end\":79670,\"start\":79669},{\"end\":79683,\"start\":79677},{\"end\":79699,\"start\":79691},{\"end\":80040,\"start\":80034},{\"end\":80056,\"start\":80049},{\"end\":80070,\"start\":80066},{\"end\":80084,\"start\":80081},{\"end\":80567,\"start\":80561},{\"end\":80590,\"start\":80583},{\"end\":80603,\"start\":80599},{\"end\":80618,\"start\":80612},{\"end\":80633,\"start\":80628},{\"end\":80650,\"start\":80643},{\"end\":80665,\"start\":80662},{\"end\":81061,\"start\":81056},{\"end\":81070,\"start\":81067},{\"end\":81085,\"start\":81077},{\"end\":81097,\"start\":81092},{\"end\":81109,\"start\":81102},{\"end\":81356,\"start\":81349},{\"end\":81370,\"start\":81361},{\"end\":81383,\"start\":81376},{\"end\":81396,\"start\":81388},{\"end\":81410,\"start\":81403},{\"end\":81420,\"start\":81417},{\"end\":81804,\"start\":81801},{\"end\":81818,\"start\":81811},{\"end\":81831,\"start\":81824},{\"end\":81845,\"start\":81839},{\"end\":81856,\"start\":81852},{\"end\":81868,\"start\":81864},{\"end\":81881,\"start\":81875},{\"end\":81893,\"start\":81887},{\"end\":81905,\"start\":81901},{\"end\":81914,\"start\":81911},{\"end\":82189,\"start\":82185},{\"end\":82205,\"start\":82200},{\"end\":82215,\"start\":82211},{\"end\":82229,\"start\":82225},{\"end\":82243,\"start\":82238},{\"end\":82259,\"start\":82252},{\"end\":82272,\"start\":82265},{\"end\":82289,\"start\":82282},{\"end\":82303,\"start\":82298},{\"end\":82317,\"start\":82311},{\"end\":82789,\"start\":82787},{\"end\":82801,\"start\":82794},{\"end\":82816,\"start\":82809},{\"end\":82830,\"start\":82823},{\"end\":82845,\"start\":82837},{\"end\":82857,\"start\":82852},{\"end\":83174,\"start\":83169},{\"end\":83189,\"start\":83182},{\"end\":83202,\"start\":83196},{\"end\":83214,\"start\":83208},{\"end\":83224,\"start\":83219},{\"end\":83240,\"start\":83234},{\"end\":83575,\"start\":83571},{\"end\":83593,\"start\":83587},{\"end\":83607,\"start\":83604},{\"end\":83622,\"start\":83618},{\"end\":83640,\"start\":83631},{\"end\":83653,\"start\":83648},{\"end\":83669,\"start\":83663},{\"end\":83685,\"start\":83676},{\"end\":83707,\"start\":83698},{\"end\":83709,\"start\":83708},{\"end\":83721,\"start\":83716},{\"end\":83737,\"start\":83729},{\"end\":83750,\"start\":83744},{\"end\":84042,\"start\":84036},{\"end\":84052,\"start\":84048},{\"end\":84063,\"start\":84058},{\"end\":84078,\"start\":84071},{\"end\":84089,\"start\":84083},{\"end\":84102,\"start\":84097},{\"end\":84113,\"start\":84109},{\"end\":84124,\"start\":84120},{\"end\":84136,\"start\":84132},{\"end\":84157,\"start\":84150},{\"end\":84571,\"start\":84567},{\"end\":84587,\"start\":84580},{\"end\":84597,\"start\":84592},{\"end\":84611,\"start\":84605},{\"end\":84625,\"start\":84619},{\"end\":84642,\"start\":84637},{\"end\":84653,\"start\":84647},{\"end\":84669,\"start\":84664},{\"end\":84686,\"start\":84677},{\"end\":84688,\"start\":84687},{\"end\":85000,\"start\":84995},{\"end\":85012,\"start\":85006},{\"end\":85023,\"start\":85019},{\"end\":85043,\"start\":85036},{\"end\":85053,\"start\":85051},{\"end\":85063,\"start\":85059},{\"end\":85073,\"start\":85068},{\"end\":85360,\"start\":85353},{\"end\":85374,\"start\":85369},{\"end\":85391,\"start\":85385},{\"end\":85402,\"start\":85396},{\"end\":85415,\"start\":85409},{\"end\":85728,\"start\":85723},{\"end\":85742,\"start\":85737},{\"end\":85753,\"start\":85749},{\"end\":85756,\"start\":85754},{\"end\":85770,\"start\":85764},{\"end\":85800,\"start\":85786},{\"end\":86156,\"start\":86147},{\"end\":86174,\"start\":86168},{\"end\":86192,\"start\":86185},{\"end\":86204,\"start\":86200},{\"end\":86217,\"start\":86213},{\"end\":86233,\"start\":86228},{\"end\":86246,\"start\":86242},{\"end\":86262,\"start\":86256},{\"end\":86277,\"start\":86270},{\"end\":86294,\"start\":86286},{\"end\":86682,\"start\":86675},{\"end\":86694,\"start\":86689},{\"end\":86709,\"start\":86706},{\"end\":86719,\"start\":86715},{\"end\":86735,\"start\":86729},{\"end\":86750,\"start\":86744},{\"end\":86763,\"start\":86759},{\"end\":87009,\"start\":87003},{\"end\":87024,\"start\":87016},{\"end\":87361,\"start\":87357},{\"end\":87373,\"start\":87370},{\"end\":87383,\"start\":87379},{\"end\":87395,\"start\":87389},{\"end\":87409,\"start\":87403},{\"end\":87425,\"start\":87418},{\"end\":87675,\"start\":87667},{\"end\":87689,\"start\":87682},{\"end\":87700,\"start\":87695},{\"end\":87710,\"start\":87705},{\"end\":87721,\"start\":87717},{\"end\":87733,\"start\":87727},{\"end\":87745,\"start\":87739},{\"end\":87754,\"start\":87750},{\"end\":87766,\"start\":87762},{\"end\":87775,\"start\":87771},{\"end\":88079,\"start\":88070},{\"end\":88106,\"start\":88099},{\"end\":88121,\"start\":88114},{\"end\":88137,\"start\":88129},{\"end\":88148,\"start\":88144},{\"end\":88159,\"start\":88153},{\"end\":88169,\"start\":88167},{\"end\":88179,\"start\":88176},{\"end\":88195,\"start\":88186},{\"end\":88723,\"start\":88718},{\"end\":88740,\"start\":88733},{\"end\":88759,\"start\":88752},{\"end\":88775,\"start\":88768},{\"end\":88788,\"start\":88783},{\"end\":89210,\"start\":89201},{\"end\":89223,\"start\":89217},{\"end\":89236,\"start\":89228},{\"end\":89248,\"start\":89243},{\"end\":89259,\"start\":89254},{\"end\":89277,\"start\":89271},{\"end\":89292,\"start\":89285},{\"end\":89300,\"start\":89298},{\"end\":89313,\"start\":89306},{\"end\":89326,\"start\":89320},{\"end\":89737,\"start\":89732},{\"end\":89751,\"start\":89745},{\"end\":89763,\"start\":89759},{\"end\":90028,\"start\":90022},{\"end\":90045,\"start\":90036},{\"end\":90503,\"start\":90500},{\"end\":90514,\"start\":90508},{\"end\":90524,\"start\":90521},{\"end\":90538,\"start\":90532},{\"end\":90554,\"start\":90547},{\"end\":90566,\"start\":90562},{\"end\":90571,\"start\":90567},{\"end\":90585,\"start\":90576},{\"end\":90599,\"start\":90591},{\"end\":90936,\"start\":90933},{\"end\":90950,\"start\":90943},{\"end\":90963,\"start\":90956},{\"end\":90977,\"start\":90971},{\"end\":90988,\"start\":90984},{\"end\":91000,\"start\":90996},{\"end\":91013,\"start\":91007},{\"end\":91025,\"start\":91019},{\"end\":91037,\"start\":91033},{\"end\":91046,\"start\":91043},{\"end\":91333,\"start\":91326},{\"end\":91343,\"start\":91341},{\"end\":91356,\"start\":91350},{\"end\":91367,\"start\":91363},{\"end\":91380,\"start\":91373},{\"end\":91397,\"start\":91390},{\"end\":91410,\"start\":91402},{\"end\":91418,\"start\":91415},{\"end\":91429,\"start\":91424},{\"end\":91825,\"start\":91824},{\"end\":91845,\"start\":91832},{\"end\":92341,\"start\":92335},{\"end\":92353,\"start\":92348},{\"end\":92368,\"start\":92361},{\"end\":92385,\"start\":92379},{\"end\":92831,\"start\":92827},{\"end\":92848,\"start\":92845},{\"end\":92858,\"start\":92855},{\"end\":92871,\"start\":92863},{\"end\":92887,\"start\":92880},{\"end\":92902,\"start\":92898},{\"end\":92914,\"start\":92907},{\"end\":92928,\"start\":92922},{\"end\":92945,\"start\":92939},{\"end\":92961,\"start\":92954},{\"end\":93343,\"start\":93339},{\"end\":93364,\"start\":93356},{\"end\":93482,\"start\":93476},{\"end\":93499,\"start\":93494},{\"end\":93510,\"start\":93505},{\"end\":93842,\"start\":93835},{\"end\":93855,\"start\":93851},{\"end\":93871,\"start\":93867},{\"end\":93881,\"start\":93876},{\"end\":93891,\"start\":93890},{\"end\":93915,\"start\":93914},{\"end\":93935,\"start\":93924},{\"end\":94420,\"start\":94416},{\"end\":94436,\"start\":94427},{\"end\":94450,\"start\":94444},{\"end\":94465,\"start\":94460},{\"end\":94476,\"start\":94472},{\"end\":94491,\"start\":94483},{\"end\":94920,\"start\":94916},{\"end\":94931,\"start\":94927},{\"end\":94953,\"start\":94947},{\"end\":94971,\"start\":94962},{\"end\":94985,\"start\":94979},{\"end\":95000,\"start\":94995},{\"end\":95011,\"start\":95007},{\"end\":95024,\"start\":95018},{\"end\":95408,\"start\":95405},{\"end\":95422,\"start\":95414},{\"end\":95434,\"start\":95428},{\"end\":95448,\"start\":95445},{\"end\":95463,\"start\":95456},{\"end\":95479,\"start\":95472},{\"end\":95496,\"start\":95488},{\"end\":95513,\"start\":95506},{\"end\":95523,\"start\":95519},{\"end\":95540,\"start\":95534},{\"end\":95853,\"start\":95848},{\"end\":95866,\"start\":95861},{\"end\":95882,\"start\":95878},{\"end\":95896,\"start\":95888},{\"end\":95912,\"start\":95906},{\"end\":95929,\"start\":95920},{\"end\":95944,\"start\":95940},{\"end\":95958,\"start\":95952},{\"end\":95976,\"start\":95970},{\"end\":95987,\"start\":95981},{\"end\":96334,\"start\":96330},{\"end\":96347,\"start\":96343},{\"end\":96364,\"start\":96356},{\"end\":96378,\"start\":96372},{\"end\":96707,\"start\":96699},{\"end\":96722,\"start\":96715},{\"end\":96738,\"start\":96732},{\"end\":96756,\"start\":96748},{\"end\":97012,\"start\":97004},{\"end\":97027,\"start\":97020},{\"end\":97043,\"start\":97037},{\"end\":97061,\"start\":97053},{\"end\":97378,\"start\":97371},{\"end\":97396,\"start\":97388},{\"end\":97414,\"start\":97406},{\"end\":97426,\"start\":97422},{\"end\":97449,\"start\":97443},{\"end\":97461,\"start\":97457},{\"end\":97476,\"start\":97470},{\"end\":97825,\"start\":97818},{\"end\":97844,\"start\":97839},{\"end\":97851,\"start\":97849},{\"end\":97862,\"start\":97858},{\"end\":97871,\"start\":97867},{\"end\":97883,\"start\":97877},{\"end\":98150,\"start\":98145},{\"end\":98161,\"start\":98158},{\"end\":98454,\"start\":98450},{\"end\":98466,\"start\":98463},{\"end\":98482,\"start\":98478},{\"end\":98496,\"start\":98492},{\"end\":98825,\"start\":98820},{\"end\":98842,\"start\":98835},{\"end\":98852,\"start\":98849},{\"end\":98867,\"start\":98862},{\"end\":99261,\"start\":99254},{\"end\":99268,\"start\":99262},{\"end\":99282,\"start\":99277},{\"end\":99293,\"start\":99287},{\"end\":99305,\"start\":99298},{\"end\":99624,\"start\":99620},{\"end\":99636,\"start\":99633},{\"end\":99652,\"start\":99648},{\"end\":99654,\"start\":99653},{\"end\":99669,\"start\":99662},{\"end\":99679,\"start\":99675},{\"end\":99689,\"start\":99684},{\"end\":99705,\"start\":99699},{\"end\":99720,\"start\":99715},{\"end\":100179,\"start\":100173},{\"end\":100196,\"start\":100189},{\"end\":100215,\"start\":100208},{\"end\":100234,\"start\":100232},{\"end\":100252,\"start\":100244},{\"end\":100266,\"start\":100262},{\"end\":100726,\"start\":100722},{\"end\":100739,\"start\":100731},{\"end\":100752,\"start\":100748},{\"end\":100764,\"start\":100759},{\"end\":100774,\"start\":100771},{\"end\":100779,\"start\":100775},{\"end\":100792,\"start\":100786},{\"end\":100794,\"start\":100793},{\"end\":100808,\"start\":100804},{\"end\":101171,\"start\":101165},{\"end\":101188,\"start\":101180},{\"end\":101204,\"start\":101198},{\"end\":101220,\"start\":101213},{\"end\":101232,\"start\":101228},{\"end\":101243,\"start\":101237},{\"end\":101620,\"start\":101617},{\"end\":101632,\"start\":101625},{\"end\":101645,\"start\":101641},{\"end\":101656,\"start\":101651},{\"end\":101669,\"start\":101662},{\"end\":101686,\"start\":101677},{\"end\":101698,\"start\":101692},{\"end\":101713,\"start\":101708},{\"end\":101727,\"start\":101721},{\"end\":102190,\"start\":102183},{\"end\":102207,\"start\":102199},{\"end\":102222,\"start\":102219},{\"end\":102239,\"start\":102232},{\"end\":102737,\"start\":102731},{\"end\":102753,\"start\":102747},{\"end\":102777,\"start\":102766},{\"end\":102792,\"start\":102785},{\"end\":102808,\"start\":102801},{\"end\":103059,\"start\":103058},{\"end\":103078,\"start\":103069},{\"end\":103093,\"start\":103087},{\"end\":103106,\"start\":103102},{\"end\":103117,\"start\":103111},{\"end\":103135,\"start\":103134},{\"end\":103149,\"start\":103144},{\"end\":103164,\"start\":103158},{\"end\":103180,\"start\":103173},{\"end\":103199,\"start\":103193},{\"end\":103777,\"start\":103770},{\"end\":103795,\"start\":103787},{\"end\":103806,\"start\":103800},{\"end\":103826,\"start\":103817},{\"end\":104217,\"start\":104212},{\"end\":104233,\"start\":104226},{\"end\":104249,\"start\":104243},{\"end\":104262,\"start\":104258},{\"end\":104277,\"start\":104270},{\"end\":104289,\"start\":104285},{\"end\":104619,\"start\":104614},{\"end\":104636,\"start\":104630},{\"end\":104844,\"start\":104837},{\"end\":104856,\"start\":104853},{\"end\":104867,\"start\":104864},{\"end\":104881,\"start\":104876},{\"end\":104893,\"start\":104890},{\"end\":104911,\"start\":104904},{\"end\":105271,\"start\":105264},{\"end\":105286,\"start\":105280},{\"end\":105305,\"start\":105302},{\"end\":105318,\"start\":105313},{\"end\":105333,\"start\":105328},{\"end\":105349,\"start\":105342},{\"end\":105364,\"start\":105361},{\"end\":105751,\"start\":105748},{\"end\":105766,\"start\":105759},{\"end\":105780,\"start\":105775},{\"end\":105955,\"start\":105952},{\"end\":105965,\"start\":105960},{\"end\":105976,\"start\":105971},{\"end\":105987,\"start\":105983},{\"end\":105999,\"start\":105993},{\"end\":106009,\"start\":106006},{\"end\":106021,\"start\":106017},{\"end\":106033,\"start\":106026},{\"end\":106050,\"start\":106041},{\"end\":106497,\"start\":106492},{\"end\":106512,\"start\":106508},{\"end\":106526,\"start\":106521},{\"end\":106545,\"start\":106535},{\"end\":106558,\"start\":106555},{\"end\":106570,\"start\":106566}]", "bib_author_last_name": "[{\"end\":63406,\"start\":63400},{\"end\":63418,\"start\":63415},{\"end\":63429,\"start\":63425},{\"end\":63441,\"start\":63436},{\"end\":63455,\"start\":63448},{\"end\":63469,\"start\":63461},{\"end\":63481,\"start\":63476},{\"end\":63498,\"start\":63492},{\"end\":63508,\"start\":63504},{\"end\":63523,\"start\":63515},{\"end\":63874,\"start\":63868},{\"end\":63886,\"start\":63884},{\"end\":63896,\"start\":63891},{\"end\":63911,\"start\":63904},{\"end\":63931,\"start\":63921},{\"end\":63947,\"start\":63940},{\"end\":63960,\"start\":63955},{\"end\":63978,\"start\":63971},{\"end\":63994,\"start\":63989},{\"end\":64004,\"start\":64001},{\"end\":64411,\"start\":64408},{\"end\":64426,\"start\":64421},{\"end\":64437,\"start\":64430},{\"end\":64450,\"start\":64446},{\"end\":64465,\"start\":64462},{\"end\":64475,\"start\":64473},{\"end\":64487,\"start\":64481},{\"end\":64491,\"start\":64489},{\"end\":64501,\"start\":64495},{\"end\":64513,\"start\":64510},{\"end\":64517,\"start\":64515},{\"end\":64861,\"start\":64857},{\"end\":64877,\"start\":64871},{\"end\":64901,\"start\":64885},{\"end\":64916,\"start\":64911},{\"end\":64934,\"start\":64927},{\"end\":64948,\"start\":64944},{\"end\":64961,\"start\":64956},{\"end\":64987,\"start\":64968},{\"end\":65005,\"start\":64996},{\"end\":65017,\"start\":65013},{\"end\":65626,\"start\":65621},{\"end\":65644,\"start\":65635},{\"end\":65658,\"start\":65653},{\"end\":65671,\"start\":65665},{\"end\":65683,\"start\":65681},{\"end\":65700,\"start\":65692},{\"end\":65713,\"start\":65708},{\"end\":65736,\"start\":65727},{\"end\":66068,\"start\":66062},{\"end\":66080,\"start\":66078},{\"end\":66088,\"start\":66085},{\"end\":66100,\"start\":66095},{\"end\":66113,\"start\":66111},{\"end\":66124,\"start\":66119},{\"end\":66139,\"start\":66134},{\"end\":66154,\"start\":66148},{\"end\":66170,\"start\":66164},{\"end\":66189,\"start\":66181},{\"end\":66201,\"start\":66195},{\"end\":66214,\"start\":66210},{\"end\":66527,\"start\":66521},{\"end\":66628,\"start\":66623},{\"end\":66643,\"start\":66639},{\"end\":66655,\"start\":66650},{\"end\":66672,\"start\":66665},{\"end\":66688,\"start\":66682},{\"end\":66707,\"start\":66699},{\"end\":66727,\"start\":66716},{\"end\":66741,\"start\":66736},{\"end\":66756,\"start\":66750},{\"end\":66771,\"start\":66765},{\"end\":67117,\"start\":67108},{\"end\":67132,\"start\":67126},{\"end\":67146,\"start\":67140},{\"end\":67161,\"start\":67156},{\"end\":67176,\"start\":67170},{\"end\":67190,\"start\":67183},{\"end\":67203,\"start\":67197},{\"end\":67214,\"start\":67205},{\"end\":67229,\"start\":67224},{\"end\":67247,\"start\":67241},{\"end\":67257,\"start\":67249},{\"end\":67596,\"start\":67589},{\"end\":67612,\"start\":67606},{\"end\":67629,\"start\":67622},{\"end\":67646,\"start\":67638},{\"end\":67666,\"start\":67659},{\"end\":67684,\"start\":67677},{\"end\":67708,\"start\":67692},{\"end\":67720,\"start\":67715},{\"end\":67735,\"start\":67729},{\"end\":67742,\"start\":67737},{\"end\":68109,\"start\":68101},{\"end\":68120,\"start\":68117},{\"end\":68136,\"start\":68129},{\"end\":68149,\"start\":68143},{\"end\":68163,\"start\":68157},{\"end\":68176,\"start\":68173},{\"end\":68193,\"start\":68186},{\"end\":68209,\"start\":68203},{\"end\":68222,\"start\":68217},{\"end\":68236,\"start\":68230},{\"end\":68253,\"start\":68244},{\"end\":68267,\"start\":68264},{\"end\":68284,\"start\":68277},{\"end\":68303,\"start\":68295},{\"end\":68319,\"start\":68312},{\"end\":68714,\"start\":68693},{\"end\":68731,\"start\":68724},{\"end\":68744,\"start\":68741},{\"end\":68756,\"start\":68751},{\"end\":68767,\"start\":68763},{\"end\":68781,\"start\":68775},{\"end\":68794,\"start\":68790},{\"end\":68808,\"start\":68802},{\"end\":68826,\"start\":68818},{\"end\":68842,\"start\":68834},{\"end\":68854,\"start\":68850},{\"end\":68873,\"start\":68863},{\"end\":68886,\"start\":68882},{\"end\":68898,\"start\":68895},{\"end\":68909,\"start\":68905},{\"end\":68930,\"start\":68920},{\"end\":68940,\"start\":68932},{\"end\":69562,\"start\":69559},{\"end\":69575,\"start\":69573},{\"end\":69588,\"start\":69586},{\"end\":69602,\"start\":69599},{\"end\":69820,\"start\":69817},{\"end\":69831,\"start\":69829},{\"end\":69842,\"start\":69840},{\"end\":69867,\"start\":69862},{\"end\":69879,\"start\":69875},{\"end\":69894,\"start\":69890},{\"end\":69905,\"start\":69903},{\"end\":69919,\"start\":69915},{\"end\":69931,\"start\":69928},{\"end\":70331,\"start\":70329},{\"end\":70342,\"start\":70340},{\"end\":70359,\"start\":70351},{\"end\":70371,\"start\":70368},{\"end\":70610,\"start\":70607},{\"end\":70623,\"start\":70620},{\"end\":70637,\"start\":70632},{\"end\":70647,\"start\":70644},{\"end\":70660,\"start\":70656},{\"end\":70672,\"start\":70668},{\"end\":70683,\"start\":70678},{\"end\":70691,\"start\":70689},{\"end\":70703,\"start\":70701},{\"end\":70716,\"start\":70713},{\"end\":71109,\"start\":71107},{\"end\":71121,\"start\":71119},{\"end\":71132,\"start\":71130},{\"end\":71142,\"start\":71140},{\"end\":71152,\"start\":71149},{\"end\":71165,\"start\":71161},{\"end\":71179,\"start\":71175},{\"end\":71189,\"start\":71187},{\"end\":71204,\"start\":71201},{\"end\":71214,\"start\":71211},{\"end\":71498,\"start\":71496},{\"end\":71508,\"start\":71505},{\"end\":71520,\"start\":71518},{\"end\":71530,\"start\":71528},{\"end\":71540,\"start\":71536},{\"end\":71550,\"start\":71547},{\"end\":71560,\"start\":71552},{\"end\":71903,\"start\":71898},{\"end\":71915,\"start\":71911},{\"end\":71937,\"start\":71925},{\"end\":71950,\"start\":71945},{\"end\":71963,\"start\":71957},{\"end\":72443,\"start\":72440},{\"end\":72458,\"start\":72453},{\"end\":72474,\"start\":72466},{\"end\":72486,\"start\":72482},{\"end\":72501,\"start\":72495},{\"end\":72515,\"start\":72508},{\"end\":72529,\"start\":72523},{\"end\":72549,\"start\":72542},{\"end\":73022,\"start\":73017},{\"end\":73041,\"start\":73034},{\"end\":73052,\"start\":73050},{\"end\":73071,\"start\":73063},{\"end\":73084,\"start\":73079},{\"end\":73102,\"start\":73095},{\"end\":73115,\"start\":73109},{\"end\":73521,\"start\":73517},{\"end\":73537,\"start\":73530},{\"end\":73552,\"start\":73544},{\"end\":73933,\"start\":73923},{\"end\":73950,\"start\":73942},{\"end\":73970,\"start\":73960},{\"end\":73984,\"start\":73977},{\"end\":74002,\"start\":73995},{\"end\":74400,\"start\":74398},{\"end\":74410,\"start\":74407},{\"end\":74426,\"start\":74422},{\"end\":74438,\"start\":74435},{\"end\":74446,\"start\":74442},{\"end\":74459,\"start\":74454},{\"end\":74464,\"start\":74461},{\"end\":74804,\"start\":74799},{\"end\":74809,\"start\":74806},{\"end\":75027,\"start\":75022},{\"end\":75039,\"start\":75034},{\"end\":75055,\"start\":75049},{\"end\":75066,\"start\":75061},{\"end\":75085,\"start\":75077},{\"end\":75097,\"start\":75091},{\"end\":75385,\"start\":75377},{\"end\":75399,\"start\":75393},{\"end\":75410,\"start\":75406},{\"end\":75424,\"start\":75421},{\"end\":75672,\"start\":75669},{\"end\":75896,\"start\":75888},{\"end\":75908,\"start\":75903},{\"end\":76245,\"start\":76237},{\"end\":76263,\"start\":76256},{\"end\":76276,\"start\":76270},{\"end\":76474,\"start\":76469},{\"end\":76490,\"start\":76483},{\"end\":76500,\"start\":76498},{\"end\":76510,\"start\":76504},{\"end\":76527,\"start\":76517},{\"end\":76534,\"start\":76529},{\"end\":76774,\"start\":76772},{\"end\":76785,\"start\":76781},{\"end\":76797,\"start\":76792},{\"end\":76994,\"start\":76992},{\"end\":77005,\"start\":77002},{\"end\":77018,\"start\":77016},{\"end\":77030,\"start\":77026},{\"end\":77042,\"start\":77038},{\"end\":77055,\"start\":77052},{\"end\":77065,\"start\":77063},{\"end\":77079,\"start\":77075},{\"end\":77092,\"start\":77090},{\"end\":77100,\"start\":77097},{\"end\":77538,\"start\":77536},{\"end\":77550,\"start\":77546},{\"end\":77565,\"start\":77560},{\"end\":77575,\"start\":77572},{\"end\":77585,\"start\":77582},{\"end\":77595,\"start\":77592},{\"end\":77609,\"start\":77605},{\"end\":77623,\"start\":77618},{\"end\":77632,\"start\":77628},{\"end\":77642,\"start\":77639},{\"end\":77988,\"start\":77985},{\"end\":77999,\"start\":77996},{\"end\":78010,\"start\":78008},{\"end\":78025,\"start\":78021},{\"end\":78039,\"start\":78033},{\"end\":78052,\"start\":78048},{\"end\":78350,\"start\":78346},{\"end\":78366,\"start\":78362},{\"end\":78378,\"start\":78373},{\"end\":78390,\"start\":78385},{\"end\":78402,\"start\":78399},{\"end\":78414,\"start\":78409},{\"end\":78724,\"start\":78715},{\"end\":78737,\"start\":78731},{\"end\":78757,\"start\":78746},{\"end\":78769,\"start\":78764},{\"end\":78781,\"start\":78777},{\"end\":79087,\"start\":79083},{\"end\":79103,\"start\":79095},{\"end\":79117,\"start\":79111},{\"end\":79140,\"start\":79129},{\"end\":79153,\"start\":79149},{\"end\":79168,\"start\":79162},{\"end\":79186,\"start\":79179},{\"end\":79201,\"start\":79194},{\"end\":79215,\"start\":79211},{\"end\":79232,\"start\":79223},{\"end\":79625,\"start\":79621},{\"end\":79640,\"start\":79635},{\"end\":79656,\"start\":79650},{\"end\":79667,\"start\":79664},{\"end\":79675,\"start\":79671},{\"end\":79689,\"start\":79684},{\"end\":79708,\"start\":79700},{\"end\":79720,\"start\":79710},{\"end\":80047,\"start\":80041},{\"end\":80064,\"start\":80057},{\"end\":80079,\"start\":80071},{\"end\":80092,\"start\":80085},{\"end\":80581,\"start\":80568},{\"end\":80597,\"start\":80591},{\"end\":80610,\"start\":80604},{\"end\":80626,\"start\":80619},{\"end\":80641,\"start\":80634},{\"end\":80660,\"start\":80651},{\"end\":80674,\"start\":80666},{\"end\":81065,\"start\":81062},{\"end\":81075,\"start\":81071},{\"end\":81090,\"start\":81086},{\"end\":81100,\"start\":81098},{\"end\":81119,\"start\":81110},{\"end\":81359,\"start\":81357},{\"end\":81374,\"start\":81371},{\"end\":81386,\"start\":81384},{\"end\":81401,\"start\":81397},{\"end\":81415,\"start\":81411},{\"end\":81425,\"start\":81421},{\"end\":81809,\"start\":81805},{\"end\":81822,\"start\":81819},{\"end\":81837,\"start\":81832},{\"end\":81850,\"start\":81846},{\"end\":81862,\"start\":81857},{\"end\":81873,\"start\":81869},{\"end\":81885,\"start\":81882},{\"end\":81899,\"start\":81894},{\"end\":81909,\"start\":81906},{\"end\":81919,\"start\":81915},{\"end\":82198,\"start\":82190},{\"end\":82209,\"start\":82206},{\"end\":82223,\"start\":82216},{\"end\":82236,\"start\":82230},{\"end\":82250,\"start\":82244},{\"end\":82263,\"start\":82260},{\"end\":82280,\"start\":82273},{\"end\":82296,\"start\":82290},{\"end\":82309,\"start\":82304},{\"end\":82324,\"start\":82318},{\"end\":82792,\"start\":82790},{\"end\":82807,\"start\":82802},{\"end\":82821,\"start\":82817},{\"end\":82835,\"start\":82831},{\"end\":82850,\"start\":82846},{\"end\":82861,\"start\":82858},{\"end\":83180,\"start\":83175},{\"end\":83194,\"start\":83190},{\"end\":83206,\"start\":83203},{\"end\":83217,\"start\":83215},{\"end\":83232,\"start\":83225},{\"end\":83243,\"start\":83241},{\"end\":83585,\"start\":83576},{\"end\":83602,\"start\":83594},{\"end\":83616,\"start\":83608},{\"end\":83629,\"start\":83623},{\"end\":83646,\"start\":83641},{\"end\":83661,\"start\":83654},{\"end\":83674,\"start\":83670},{\"end\":83696,\"start\":83686},{\"end\":83714,\"start\":83710},{\"end\":83727,\"start\":83722},{\"end\":83742,\"start\":83738},{\"end\":83755,\"start\":83751},{\"end\":84046,\"start\":84043},{\"end\":84056,\"start\":84053},{\"end\":84069,\"start\":84064},{\"end\":84081,\"start\":84079},{\"end\":84095,\"start\":84090},{\"end\":84107,\"start\":84103},{\"end\":84118,\"start\":84114},{\"end\":84130,\"start\":84125},{\"end\":84148,\"start\":84137},{\"end\":84166,\"start\":84158},{\"end\":84175,\"start\":84168},{\"end\":84578,\"start\":84572},{\"end\":84590,\"start\":84588},{\"end\":84603,\"start\":84598},{\"end\":84617,\"start\":84612},{\"end\":84635,\"start\":84626},{\"end\":84645,\"start\":84643},{\"end\":84662,\"start\":84654},{\"end\":84675,\"start\":84670},{\"end\":84698,\"start\":84689},{\"end\":85004,\"start\":85001},{\"end\":85017,\"start\":85013},{\"end\":85034,\"start\":85024},{\"end\":85049,\"start\":85044},{\"end\":85057,\"start\":85054},{\"end\":85066,\"start\":85064},{\"end\":85078,\"start\":85074},{\"end\":85367,\"start\":85361},{\"end\":85383,\"start\":85375},{\"end\":85394,\"start\":85392},{\"end\":85407,\"start\":85403},{\"end\":85422,\"start\":85416},{\"end\":85431,\"start\":85424},{\"end\":85735,\"start\":85729},{\"end\":85747,\"start\":85743},{\"end\":85762,\"start\":85757},{\"end\":85784,\"start\":85771},{\"end\":85808,\"start\":85801},{\"end\":86166,\"start\":86157},{\"end\":86183,\"start\":86175},{\"end\":86198,\"start\":86193},{\"end\":86211,\"start\":86205},{\"end\":86226,\"start\":86218},{\"end\":86240,\"start\":86234},{\"end\":86254,\"start\":86247},{\"end\":86268,\"start\":86263},{\"end\":86284,\"start\":86278},{\"end\":86303,\"start\":86295},{\"end\":86687,\"start\":86683},{\"end\":86704,\"start\":86695},{\"end\":86713,\"start\":86710},{\"end\":86727,\"start\":86720},{\"end\":86742,\"start\":86736},{\"end\":86757,\"start\":86751},{\"end\":86768,\"start\":86764},{\"end\":87014,\"start\":87010},{\"end\":87027,\"start\":87025},{\"end\":87368,\"start\":87362},{\"end\":87377,\"start\":87374},{\"end\":87387,\"start\":87384},{\"end\":87401,\"start\":87396},{\"end\":87416,\"start\":87410},{\"end\":87429,\"start\":87426},{\"end\":87436,\"start\":87431},{\"end\":87680,\"start\":87676},{\"end\":87693,\"start\":87690},{\"end\":87703,\"start\":87701},{\"end\":87715,\"start\":87711},{\"end\":87725,\"start\":87722},{\"end\":87737,\"start\":87734},{\"end\":87748,\"start\":87746},{\"end\":87760,\"start\":87755},{\"end\":87769,\"start\":87767},{\"end\":87778,\"start\":87776},{\"end\":88097,\"start\":88080},{\"end\":88112,\"start\":88107},{\"end\":88127,\"start\":88122},{\"end\":88142,\"start\":88138},{\"end\":88151,\"start\":88149},{\"end\":88165,\"start\":88160},{\"end\":88174,\"start\":88170},{\"end\":88184,\"start\":88180},{\"end\":88201,\"start\":88196},{\"end\":88208,\"start\":88203},{\"end\":88731,\"start\":88724},{\"end\":88750,\"start\":88741},{\"end\":88766,\"start\":88760},{\"end\":88781,\"start\":88776},{\"end\":88794,\"start\":88789},{\"end\":89215,\"start\":89211},{\"end\":89226,\"start\":89224},{\"end\":89241,\"start\":89237},{\"end\":89252,\"start\":89249},{\"end\":89269,\"start\":89260},{\"end\":89283,\"start\":89278},{\"end\":89296,\"start\":89293},{\"end\":89304,\"start\":89301},{\"end\":89318,\"start\":89314},{\"end\":89331,\"start\":89327},{\"end\":89743,\"start\":89738},{\"end\":89757,\"start\":89752},{\"end\":89772,\"start\":89764},{\"end\":90034,\"start\":90029},{\"end\":90054,\"start\":90046},{\"end\":90506,\"start\":90504},{\"end\":90519,\"start\":90515},{\"end\":90530,\"start\":90525},{\"end\":90545,\"start\":90539},{\"end\":90560,\"start\":90555},{\"end\":90574,\"start\":90572},{\"end\":90589,\"start\":90586},{\"end\":90603,\"start\":90600},{\"end\":90941,\"start\":90937},{\"end\":90954,\"start\":90951},{\"end\":90969,\"start\":90964},{\"end\":90982,\"start\":90978},{\"end\":90994,\"start\":90989},{\"end\":91005,\"start\":91001},{\"end\":91017,\"start\":91014},{\"end\":91031,\"start\":91026},{\"end\":91041,\"start\":91038},{\"end\":91051,\"start\":91047},{\"end\":91339,\"start\":91334},{\"end\":91348,\"start\":91344},{\"end\":91361,\"start\":91357},{\"end\":91371,\"start\":91368},{\"end\":91388,\"start\":91381},{\"end\":91400,\"start\":91398},{\"end\":91413,\"start\":91411},{\"end\":91422,\"start\":91419},{\"end\":91449,\"start\":91430},{\"end\":91454,\"start\":91451},{\"end\":91830,\"start\":91826},{\"end\":91852,\"start\":91846},{\"end\":91861,\"start\":91854},{\"end\":92346,\"start\":92342},{\"end\":92359,\"start\":92354},{\"end\":92377,\"start\":92369},{\"end\":92390,\"start\":92386},{\"end\":92399,\"start\":92392},{\"end\":92843,\"start\":92832},{\"end\":92853,\"start\":92849},{\"end\":92861,\"start\":92859},{\"end\":92878,\"start\":92872},{\"end\":92896,\"start\":92888},{\"end\":92905,\"start\":92903},{\"end\":92920,\"start\":92915},{\"end\":92937,\"start\":92929},{\"end\":92952,\"start\":92946},{\"end\":92971,\"start\":92962},{\"end\":93354,\"start\":93344},{\"end\":93371,\"start\":93365},{\"end\":93492,\"start\":93483},{\"end\":93503,\"start\":93500},{\"end\":93516,\"start\":93511},{\"end\":93849,\"start\":93843},{\"end\":93865,\"start\":93856},{\"end\":93874,\"start\":93872},{\"end\":93888,\"start\":93882},{\"end\":93903,\"start\":93892},{\"end\":93912,\"start\":93905},{\"end\":93922,\"start\":93916},{\"end\":93938,\"start\":93936},{\"end\":93945,\"start\":93940},{\"end\":94425,\"start\":94421},{\"end\":94442,\"start\":94437},{\"end\":94458,\"start\":94451},{\"end\":94470,\"start\":94466},{\"end\":94481,\"start\":94477},{\"end\":94498,\"start\":94492},{\"end\":94925,\"start\":94921},{\"end\":94945,\"start\":94932},{\"end\":94960,\"start\":94954},{\"end\":94977,\"start\":94972},{\"end\":94993,\"start\":94986},{\"end\":95005,\"start\":95001},{\"end\":95016,\"start\":95012},{\"end\":95031,\"start\":95025},{\"end\":95412,\"start\":95409},{\"end\":95426,\"start\":95423},{\"end\":95443,\"start\":95435},{\"end\":95454,\"start\":95449},{\"end\":95470,\"start\":95464},{\"end\":95486,\"start\":95480},{\"end\":95504,\"start\":95497},{\"end\":95517,\"start\":95514},{\"end\":95532,\"start\":95524},{\"end\":95552,\"start\":95541},{\"end\":95859,\"start\":95854},{\"end\":95876,\"start\":95867},{\"end\":95886,\"start\":95883},{\"end\":95904,\"start\":95897},{\"end\":95918,\"start\":95913},{\"end\":95938,\"start\":95930},{\"end\":95950,\"start\":95945},{\"end\":95968,\"start\":95959},{\"end\":95979,\"start\":95977},{\"end\":95993,\"start\":95988},{\"end\":96341,\"start\":96335},{\"end\":96354,\"start\":96348},{\"end\":96370,\"start\":96365},{\"end\":96386,\"start\":96379},{\"end\":96713,\"start\":96708},{\"end\":96730,\"start\":96723},{\"end\":96746,\"start\":96739},{\"end\":96764,\"start\":96757},{\"end\":97018,\"start\":97013},{\"end\":97035,\"start\":97028},{\"end\":97051,\"start\":97044},{\"end\":97069,\"start\":97062},{\"end\":97386,\"start\":97379},{\"end\":97404,\"start\":97397},{\"end\":97420,\"start\":97415},{\"end\":97441,\"start\":97427},{\"end\":97455,\"start\":97450},{\"end\":97468,\"start\":97462},{\"end\":97484,\"start\":97477},{\"end\":97837,\"start\":97826},{\"end\":97847,\"start\":97845},{\"end\":97856,\"start\":97852},{\"end\":97865,\"start\":97863},{\"end\":97875,\"start\":97872},{\"end\":97886,\"start\":97884},{\"end\":97892,\"start\":97888},{\"end\":98156,\"start\":98151},{\"end\":98170,\"start\":98162},{\"end\":98461,\"start\":98455},{\"end\":98476,\"start\":98467},{\"end\":98490,\"start\":98483},{\"end\":98505,\"start\":98497},{\"end\":98833,\"start\":98826},{\"end\":98847,\"start\":98843},{\"end\":98860,\"start\":98853},{\"end\":98872,\"start\":98868},{\"end\":99275,\"start\":99269},{\"end\":99285,\"start\":99283},{\"end\":99296,\"start\":99294},{\"end\":99310,\"start\":99306},{\"end\":99317,\"start\":99312},{\"end\":99631,\"start\":99625},{\"end\":99646,\"start\":99637},{\"end\":99660,\"start\":99655},{\"end\":99673,\"start\":99670},{\"end\":99682,\"start\":99680},{\"end\":99697,\"start\":99690},{\"end\":99713,\"start\":99706},{\"end\":99725,\"start\":99721},{\"end\":100187,\"start\":100180},{\"end\":100206,\"start\":100197},{\"end\":100230,\"start\":100216},{\"end\":100242,\"start\":100235},{\"end\":100260,\"start\":100253},{\"end\":100275,\"start\":100267},{\"end\":100729,\"start\":100727},{\"end\":100746,\"start\":100740},{\"end\":100757,\"start\":100753},{\"end\":100769,\"start\":100765},{\"end\":100784,\"start\":100780},{\"end\":100802,\"start\":100795},{\"end\":100814,\"start\":100809},{\"end\":101178,\"start\":101172},{\"end\":101196,\"start\":101189},{\"end\":101211,\"start\":101205},{\"end\":101226,\"start\":101221},{\"end\":101235,\"start\":101233},{\"end\":101250,\"start\":101244},{\"end\":101264,\"start\":101252},{\"end\":101623,\"start\":101621},{\"end\":101639,\"start\":101633},{\"end\":101649,\"start\":101646},{\"end\":101660,\"start\":101657},{\"end\":101675,\"start\":101670},{\"end\":101690,\"start\":101687},{\"end\":101706,\"start\":101699},{\"end\":101719,\"start\":101714},{\"end\":101734,\"start\":101728},{\"end\":102197,\"start\":102191},{\"end\":102217,\"start\":102208},{\"end\":102230,\"start\":102223},{\"end\":102248,\"start\":102240},{\"end\":102745,\"start\":102738},{\"end\":102764,\"start\":102754},{\"end\":102783,\"start\":102778},{\"end\":102799,\"start\":102793},{\"end\":102817,\"start\":102809},{\"end\":103067,\"start\":103060},{\"end\":103085,\"start\":103079},{\"end\":103100,\"start\":103094},{\"end\":103109,\"start\":103107},{\"end\":103124,\"start\":103118},{\"end\":103132,\"start\":103126},{\"end\":103142,\"start\":103136},{\"end\":103156,\"start\":103150},{\"end\":103171,\"start\":103165},{\"end\":103191,\"start\":103181},{\"end\":103205,\"start\":103200},{\"end\":103212,\"start\":103207},{\"end\":103785,\"start\":103778},{\"end\":103798,\"start\":103796},{\"end\":103815,\"start\":103807},{\"end\":103832,\"start\":103827},{\"end\":104224,\"start\":104218},{\"end\":104241,\"start\":104234},{\"end\":104256,\"start\":104250},{\"end\":104268,\"start\":104263},{\"end\":104283,\"start\":104278},{\"end\":104295,\"start\":104290},{\"end\":104628,\"start\":104620},{\"end\":104653,\"start\":104637},{\"end\":104851,\"start\":104845},{\"end\":104862,\"start\":104857},{\"end\":104874,\"start\":104868},{\"end\":104888,\"start\":104882},{\"end\":104902,\"start\":104894},{\"end\":104921,\"start\":104912},{\"end\":104927,\"start\":104923},{\"end\":105278,\"start\":105272},{\"end\":105300,\"start\":105287},{\"end\":105311,\"start\":105306},{\"end\":105326,\"start\":105319},{\"end\":105340,\"start\":105334},{\"end\":105359,\"start\":105350},{\"end\":105373,\"start\":105365},{\"end\":105757,\"start\":105752},{\"end\":105773,\"start\":105767},{\"end\":105787,\"start\":105781},{\"end\":105958,\"start\":105956},{\"end\":105969,\"start\":105966},{\"end\":105981,\"start\":105977},{\"end\":105991,\"start\":105988},{\"end\":106004,\"start\":106000},{\"end\":106015,\"start\":106010},{\"end\":106024,\"start\":106022},{\"end\":106039,\"start\":106034},{\"end\":106054,\"start\":106051},{\"end\":106506,\"start\":106498},{\"end\":106519,\"start\":106513},{\"end\":106533,\"start\":106527},{\"end\":106553,\"start\":106546},{\"end\":106564,\"start\":106559},{\"end\":106575,\"start\":106571}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":63792,\"start\":63333},{\"attributes\":{\"id\":\"b1\"},\"end\":64350,\"start\":63794},{\"attributes\":{\"id\":\"b2\"},\"end\":64756,\"start\":64352},{\"attributes\":{\"id\":\"b3\"},\"end\":65559,\"start\":64758},{\"attributes\":{\"id\":\"b4\"},\"end\":65977,\"start\":65561},{\"attributes\":{\"id\":\"b5\"},\"end\":66517,\"start\":65979},{\"attributes\":{\"id\":\"b6\"},\"end\":66578,\"start\":66519},{\"attributes\":{\"id\":\"b7\"},\"end\":67096,\"start\":66580},{\"attributes\":{\"id\":\"b8\"},\"end\":67582,\"start\":67098},{\"attributes\":{\"id\":\"b9\"},\"end\":68080,\"start\":67584},{\"attributes\":{\"id\":\"b10\"},\"end\":68643,\"start\":68082},{\"attributes\":{\"id\":\"b11\"},\"end\":69549,\"start\":68645},{\"attributes\":{\"id\":\"b12\"},\"end\":69720,\"start\":69551},{\"attributes\":{\"id\":\"b13\"},\"end\":70216,\"start\":69722},{\"attributes\":{\"id\":\"b14\"},\"end\":70600,\"start\":70218},{\"attributes\":{\"id\":\"b15\"},\"end\":71020,\"start\":70602},{\"attributes\":{\"id\":\"b16\"},\"end\":71487,\"start\":71022},{\"attributes\":{\"id\":\"b17\"},\"end\":71791,\"start\":71489},{\"attributes\":{\"id\":\"b18\"},\"end\":72386,\"start\":71793},{\"attributes\":{\"id\":\"b19\"},\"end\":72973,\"start\":72388},{\"attributes\":{\"id\":\"b20\"},\"end\":73480,\"start\":72975},{\"attributes\":{\"id\":\"b21\"},\"end\":73857,\"start\":73482},{\"attributes\":{\"id\":\"b22\"},\"end\":74390,\"start\":73859},{\"attributes\":{\"id\":\"b23\"},\"end\":74719,\"start\":74392},{\"attributes\":{\"id\":\"b24\"},\"end\":74949,\"start\":74721},{\"attributes\":{\"id\":\"b25\"},\"end\":75303,\"start\":74951},{\"attributes\":{\"id\":\"b26\"},\"end\":75569,\"start\":75305},{\"attributes\":{\"id\":\"b27\"},\"end\":75782,\"start\":75571},{\"attributes\":{\"id\":\"b28\"},\"end\":76171,\"start\":75784},{\"attributes\":{\"id\":\"b29\"},\"end\":76411,\"start\":76173},{\"attributes\":{\"id\":\"b30\"},\"end\":76682,\"start\":76413},{\"attributes\":{\"id\":\"b31\"},\"end\":76986,\"start\":76684},{\"attributes\":{\"id\":\"b32\"},\"end\":77447,\"start\":76988},{\"attributes\":{\"id\":\"b33\"},\"end\":77911,\"start\":77449},{\"attributes\":{\"id\":\"b34\"},\"end\":78267,\"start\":77913},{\"attributes\":{\"id\":\"b35\"},\"end\":78635,\"start\":78269},{\"attributes\":{\"id\":\"b36\"},\"end\":79003,\"start\":78637},{\"attributes\":{\"id\":\"b37\"},\"end\":79538,\"start\":79005},{\"attributes\":{\"id\":\"b38\"},\"end\":79975,\"start\":79540},{\"attributes\":{\"id\":\"b39\"},\"end\":80458,\"start\":79977},{\"attributes\":{\"id\":\"b40\"},\"end\":80966,\"start\":80460},{\"attributes\":{\"id\":\"b41\"},\"end\":81347,\"start\":80968},{\"attributes\":{\"id\":\"b42\"},\"end\":81729,\"start\":81349},{\"attributes\":{\"id\":\"b43\"},\"end\":82183,\"start\":81731},{\"attributes\":{\"id\":\"b44\"},\"end\":82722,\"start\":82185},{\"attributes\":{\"id\":\"b45\"},\"end\":83074,\"start\":82724},{\"attributes\":{\"id\":\"b46\"},\"end\":83488,\"start\":83076},{\"attributes\":{\"id\":\"b47\"},\"end\":84034,\"start\":83490},{\"attributes\":{\"id\":\"b48\"},\"end\":84486,\"start\":84036},{\"attributes\":{\"id\":\"b49\"},\"end\":84922,\"start\":84488},{\"attributes\":{\"id\":\"b50\"},\"end\":85304,\"start\":84924},{\"attributes\":{\"id\":\"b51\"},\"end\":85633,\"start\":85306},{\"attributes\":{\"id\":\"b52\"},\"end\":86059,\"start\":85635},{\"attributes\":{\"id\":\"b53\"},\"end\":86626,\"start\":86061},{\"attributes\":{\"id\":\"b54\"},\"end\":87001,\"start\":86628},{\"attributes\":{\"id\":\"b55\"},\"end\":87281,\"start\":87003},{\"attributes\":{\"id\":\"b56\"},\"end\":87665,\"start\":87283},{\"attributes\":{\"id\":\"b57\"},\"end\":88030,\"start\":87667},{\"attributes\":{\"id\":\"b58\"},\"end\":88654,\"start\":88032},{\"attributes\":{\"id\":\"b59\"},\"end\":89199,\"start\":88656},{\"attributes\":{\"id\":\"b60\"},\"end\":89667,\"start\":89201},{\"attributes\":{\"id\":\"b61\"},\"end\":89951,\"start\":89669},{\"attributes\":{\"id\":\"b62\"},\"end\":90421,\"start\":89953},{\"attributes\":{\"id\":\"b63\"},\"end\":90861,\"start\":90423},{\"attributes\":{\"id\":\"b64\"},\"end\":91252,\"start\":90863},{\"attributes\":{\"id\":\"b65\"},\"end\":91733,\"start\":91254},{\"attributes\":{\"id\":\"b66\"},\"end\":92250,\"start\":91735},{\"attributes\":{\"id\":\"b67\"},\"end\":92774,\"start\":92252},{\"attributes\":{\"id\":\"b68\"},\"end\":93282,\"start\":92776},{\"attributes\":{\"id\":\"b69\"},\"end\":93474,\"start\":93284},{\"attributes\":{\"id\":\"b70\"},\"end\":93754,\"start\":93476},{\"attributes\":{\"id\":\"b71\"},\"end\":94414,\"start\":93756},{\"attributes\":{\"id\":\"b72\"},\"end\":94830,\"start\":94416},{\"attributes\":{\"id\":\"b73\"},\"end\":95351,\"start\":94832},{\"attributes\":{\"id\":\"b74\"},\"end\":95806,\"start\":95353},{\"attributes\":{\"id\":\"b75\"},\"end\":96257,\"start\":95808},{\"attributes\":{\"id\":\"b76\"},\"end\":96648,\"start\":96259},{\"attributes\":{\"id\":\"b77\"},\"end\":96953,\"start\":96650},{\"attributes\":{\"id\":\"b78\"},\"end\":97317,\"start\":96955},{\"attributes\":{\"id\":\"b79\"},\"end\":97770,\"start\":97319},{\"attributes\":{\"id\":\"b80\"},\"end\":98088,\"start\":97772},{\"attributes\":{\"id\":\"b81\"},\"end\":98327,\"start\":98090},{\"attributes\":{\"id\":\"b82\"},\"end\":98757,\"start\":98329},{\"attributes\":{\"id\":\"b83\"},\"end\":99252,\"start\":98759},{\"attributes\":{\"id\":\"b84\"},\"end\":99580,\"start\":99254},{\"attributes\":{\"id\":\"b85\"},\"end\":100083,\"start\":99582},{\"attributes\":{\"id\":\"b86\"},\"end\":100720,\"start\":100085},{\"attributes\":{\"id\":\"b87\"},\"end\":101076,\"start\":100722},{\"attributes\":{\"id\":\"b88\"},\"end\":101525,\"start\":101078},{\"attributes\":{\"id\":\"b89\"},\"end\":102105,\"start\":101527},{\"attributes\":{\"id\":\"b90\"},\"end\":102655,\"start\":102107},{\"attributes\":{\"id\":\"b91\"},\"end\":103002,\"start\":102657},{\"attributes\":{\"id\":\"b92\"},\"end\":103699,\"start\":103004},{\"attributes\":{\"id\":\"b93\"},\"end\":104210,\"start\":103701},{\"attributes\":{\"id\":\"b94\"},\"end\":104546,\"start\":104212},{\"attributes\":{\"id\":\"b95\"},\"end\":104835,\"start\":104548},{\"attributes\":{\"id\":\"b96\"},\"end\":105179,\"start\":104837},{\"attributes\":{\"id\":\"b97\"},\"end\":105698,\"start\":105181},{\"attributes\":{\"id\":\"b98\"},\"end\":105950,\"start\":105700},{\"attributes\":{\"id\":\"b99\"},\"end\":106412,\"start\":105952},{\"attributes\":{\"id\":\"b100\"},\"end\":106812,\"start\":106414}]", "bib_title": "[{\"end\":63861,\"start\":63794},{\"end\":64847,\"start\":64758},{\"end\":66617,\"start\":66580},{\"end\":68686,\"start\":68645},{\"end\":71891,\"start\":71793},{\"end\":72429,\"start\":72388},{\"end\":73005,\"start\":72975},{\"end\":73506,\"start\":73482},{\"end\":73915,\"start\":73859},{\"end\":74795,\"start\":74721},{\"end\":75367,\"start\":75305},{\"end\":75876,\"start\":75784},{\"end\":76223,\"start\":76173},{\"end\":76460,\"start\":76413},{\"end\":80032,\"start\":79977},{\"end\":84993,\"start\":84924},{\"end\":85351,\"start\":85306},{\"end\":88068,\"start\":88032},{\"end\":88716,\"start\":88656},{\"end\":90020,\"start\":89953},{\"end\":91822,\"start\":91735},{\"end\":92333,\"start\":92252},{\"end\":92825,\"start\":92776},{\"end\":93833,\"start\":93756},{\"end\":94914,\"start\":94832},{\"end\":96328,\"start\":96259},{\"end\":97002,\"start\":96955},{\"end\":97369,\"start\":97319},{\"end\":98818,\"start\":98759},{\"end\":99618,\"start\":99582},{\"end\":100171,\"start\":100085},{\"end\":101615,\"start\":101527},{\"end\":102181,\"start\":102107},{\"end\":103056,\"start\":103004},{\"end\":103768,\"start\":103701},{\"end\":105262,\"start\":105181}]", "bib_author": "[{\"end\":63408,\"start\":63393},{\"end\":63420,\"start\":63408},{\"end\":63431,\"start\":63420},{\"end\":63443,\"start\":63431},{\"end\":63457,\"start\":63443},{\"end\":63471,\"start\":63457},{\"end\":63483,\"start\":63471},{\"end\":63500,\"start\":63483},{\"end\":63510,\"start\":63500},{\"end\":63525,\"start\":63510},{\"end\":63876,\"start\":63863},{\"end\":63888,\"start\":63876},{\"end\":63898,\"start\":63888},{\"end\":63913,\"start\":63898},{\"end\":63933,\"start\":63913},{\"end\":63949,\"start\":63933},{\"end\":63962,\"start\":63949},{\"end\":63980,\"start\":63962},{\"end\":63996,\"start\":63980},{\"end\":64006,\"start\":63996},{\"end\":64413,\"start\":64402},{\"end\":64428,\"start\":64413},{\"end\":64439,\"start\":64428},{\"end\":64452,\"start\":64439},{\"end\":64467,\"start\":64452},{\"end\":64477,\"start\":64467},{\"end\":64489,\"start\":64477},{\"end\":64493,\"start\":64489},{\"end\":64503,\"start\":64493},{\"end\":64515,\"start\":64503},{\"end\":64519,\"start\":64515},{\"end\":64863,\"start\":64849},{\"end\":64879,\"start\":64863},{\"end\":64903,\"start\":64879},{\"end\":64918,\"start\":64903},{\"end\":64936,\"start\":64918},{\"end\":64950,\"start\":64936},{\"end\":64963,\"start\":64950},{\"end\":64989,\"start\":64963},{\"end\":65007,\"start\":64989},{\"end\":65019,\"start\":65007},{\"end\":65628,\"start\":65615},{\"end\":65646,\"start\":65628},{\"end\":65660,\"start\":65646},{\"end\":65673,\"start\":65660},{\"end\":65685,\"start\":65673},{\"end\":65702,\"start\":65685},{\"end\":65715,\"start\":65702},{\"end\":65738,\"start\":65715},{\"end\":66070,\"start\":66054},{\"end\":66082,\"start\":66070},{\"end\":66090,\"start\":66082},{\"end\":66102,\"start\":66090},{\"end\":66115,\"start\":66102},{\"end\":66126,\"start\":66115},{\"end\":66141,\"start\":66126},{\"end\":66156,\"start\":66141},{\"end\":66172,\"start\":66156},{\"end\":66191,\"start\":66172},{\"end\":66203,\"start\":66191},{\"end\":66216,\"start\":66203},{\"end\":66529,\"start\":66521},{\"end\":66630,\"start\":66619},{\"end\":66645,\"start\":66630},{\"end\":66657,\"start\":66645},{\"end\":66674,\"start\":66657},{\"end\":66690,\"start\":66674},{\"end\":66709,\"start\":66690},{\"end\":66729,\"start\":66709},{\"end\":66743,\"start\":66729},{\"end\":66758,\"start\":66743},{\"end\":66773,\"start\":66758},{\"end\":67119,\"start\":67098},{\"end\":67134,\"start\":67119},{\"end\":67148,\"start\":67134},{\"end\":67163,\"start\":67148},{\"end\":67178,\"start\":67163},{\"end\":67192,\"start\":67178},{\"end\":67205,\"start\":67192},{\"end\":67216,\"start\":67205},{\"end\":67231,\"start\":67216},{\"end\":67249,\"start\":67231},{\"end\":67259,\"start\":67249},{\"end\":67598,\"start\":67584},{\"end\":67614,\"start\":67598},{\"end\":67631,\"start\":67614},{\"end\":67648,\"start\":67631},{\"end\":67668,\"start\":67648},{\"end\":67686,\"start\":67668},{\"end\":67710,\"start\":67686},{\"end\":67722,\"start\":67710},{\"end\":67737,\"start\":67722},{\"end\":67744,\"start\":67737},{\"end\":68111,\"start\":68096},{\"end\":68122,\"start\":68111},{\"end\":68138,\"start\":68122},{\"end\":68151,\"start\":68138},{\"end\":68165,\"start\":68151},{\"end\":68178,\"start\":68165},{\"end\":68195,\"start\":68178},{\"end\":68211,\"start\":68195},{\"end\":68224,\"start\":68211},{\"end\":68238,\"start\":68224},{\"end\":68255,\"start\":68238},{\"end\":68269,\"start\":68255},{\"end\":68286,\"start\":68269},{\"end\":68305,\"start\":68286},{\"end\":68321,\"start\":68305},{\"end\":68716,\"start\":68688},{\"end\":68733,\"start\":68716},{\"end\":68746,\"start\":68733},{\"end\":68758,\"start\":68746},{\"end\":68769,\"start\":68758},{\"end\":68783,\"start\":68769},{\"end\":68796,\"start\":68783},{\"end\":68810,\"start\":68796},{\"end\":68828,\"start\":68810},{\"end\":68844,\"start\":68828},{\"end\":68856,\"start\":68844},{\"end\":68875,\"start\":68856},{\"end\":68888,\"start\":68875},{\"end\":68900,\"start\":68888},{\"end\":68911,\"start\":68900},{\"end\":68932,\"start\":68911},{\"end\":68942,\"start\":68932},{\"end\":69564,\"start\":69551},{\"end\":69577,\"start\":69564},{\"end\":69590,\"start\":69577},{\"end\":69604,\"start\":69590},{\"end\":69822,\"start\":69808},{\"end\":69833,\"start\":69822},{\"end\":69844,\"start\":69833},{\"end\":69869,\"start\":69844},{\"end\":69881,\"start\":69869},{\"end\":69896,\"start\":69881},{\"end\":69907,\"start\":69896},{\"end\":69921,\"start\":69907},{\"end\":69933,\"start\":69921},{\"end\":70333,\"start\":70322},{\"end\":70344,\"start\":70333},{\"end\":70361,\"start\":70344},{\"end\":70373,\"start\":70361},{\"end\":70612,\"start\":70602},{\"end\":70625,\"start\":70612},{\"end\":70639,\"start\":70625},{\"end\":70649,\"start\":70639},{\"end\":70662,\"start\":70649},{\"end\":70674,\"start\":70662},{\"end\":70685,\"start\":70674},{\"end\":70693,\"start\":70685},{\"end\":70705,\"start\":70693},{\"end\":70718,\"start\":70705},{\"end\":71111,\"start\":71099},{\"end\":71123,\"start\":71111},{\"end\":71134,\"start\":71123},{\"end\":71144,\"start\":71134},{\"end\":71154,\"start\":71144},{\"end\":71167,\"start\":71154},{\"end\":71181,\"start\":71167},{\"end\":71191,\"start\":71181},{\"end\":71206,\"start\":71191},{\"end\":71216,\"start\":71206},{\"end\":71500,\"start\":71489},{\"end\":71510,\"start\":71500},{\"end\":71522,\"start\":71510},{\"end\":71532,\"start\":71522},{\"end\":71542,\"start\":71532},{\"end\":71552,\"start\":71542},{\"end\":71562,\"start\":71552},{\"end\":71905,\"start\":71893},{\"end\":71917,\"start\":71905},{\"end\":71939,\"start\":71917},{\"end\":71952,\"start\":71939},{\"end\":71965,\"start\":71952},{\"end\":72445,\"start\":72431},{\"end\":72460,\"start\":72445},{\"end\":72476,\"start\":72460},{\"end\":72488,\"start\":72476},{\"end\":72503,\"start\":72488},{\"end\":72517,\"start\":72503},{\"end\":72531,\"start\":72517},{\"end\":72551,\"start\":72531},{\"end\":73024,\"start\":73007},{\"end\":73043,\"start\":73024},{\"end\":73054,\"start\":73043},{\"end\":73073,\"start\":73054},{\"end\":73086,\"start\":73073},{\"end\":73104,\"start\":73086},{\"end\":73117,\"start\":73104},{\"end\":73523,\"start\":73508},{\"end\":73539,\"start\":73523},{\"end\":73554,\"start\":73539},{\"end\":73935,\"start\":73917},{\"end\":73952,\"start\":73935},{\"end\":73972,\"start\":73952},{\"end\":73986,\"start\":73972},{\"end\":74004,\"start\":73986},{\"end\":74402,\"start\":74392},{\"end\":74412,\"start\":74402},{\"end\":74428,\"start\":74412},{\"end\":74440,\"start\":74428},{\"end\":74448,\"start\":74440},{\"end\":74461,\"start\":74448},{\"end\":74466,\"start\":74461},{\"end\":74806,\"start\":74797},{\"end\":74811,\"start\":74806},{\"end\":75029,\"start\":75014},{\"end\":75041,\"start\":75029},{\"end\":75057,\"start\":75041},{\"end\":75068,\"start\":75057},{\"end\":75087,\"start\":75068},{\"end\":75099,\"start\":75087},{\"end\":75387,\"start\":75369},{\"end\":75401,\"start\":75387},{\"end\":75412,\"start\":75401},{\"end\":75426,\"start\":75412},{\"end\":75674,\"start\":75660},{\"end\":75898,\"start\":75878},{\"end\":75910,\"start\":75898},{\"end\":76247,\"start\":76225},{\"end\":76265,\"start\":76247},{\"end\":76278,\"start\":76265},{\"end\":76476,\"start\":76462},{\"end\":76492,\"start\":76476},{\"end\":76502,\"start\":76492},{\"end\":76512,\"start\":76502},{\"end\":76529,\"start\":76512},{\"end\":76536,\"start\":76529},{\"end\":76776,\"start\":76764},{\"end\":76787,\"start\":76776},{\"end\":76799,\"start\":76787},{\"end\":76996,\"start\":76988},{\"end\":77007,\"start\":76996},{\"end\":77020,\"start\":77007},{\"end\":77032,\"start\":77020},{\"end\":77044,\"start\":77032},{\"end\":77057,\"start\":77044},{\"end\":77067,\"start\":77057},{\"end\":77081,\"start\":77067},{\"end\":77094,\"start\":77081},{\"end\":77102,\"start\":77094},{\"end\":77540,\"start\":77531},{\"end\":77552,\"start\":77540},{\"end\":77567,\"start\":77552},{\"end\":77577,\"start\":77567},{\"end\":77587,\"start\":77577},{\"end\":77597,\"start\":77587},{\"end\":77611,\"start\":77597},{\"end\":77625,\"start\":77611},{\"end\":77634,\"start\":77625},{\"end\":77644,\"start\":77634},{\"end\":77990,\"start\":77978},{\"end\":78001,\"start\":77990},{\"end\":78012,\"start\":78001},{\"end\":78027,\"start\":78012},{\"end\":78041,\"start\":78027},{\"end\":78054,\"start\":78041},{\"end\":78352,\"start\":78340},{\"end\":78368,\"start\":78352},{\"end\":78380,\"start\":78368},{\"end\":78392,\"start\":78380},{\"end\":78404,\"start\":78392},{\"end\":78416,\"start\":78404},{\"end\":78726,\"start\":78709},{\"end\":78739,\"start\":78726},{\"end\":78759,\"start\":78739},{\"end\":78771,\"start\":78759},{\"end\":78783,\"start\":78771},{\"end\":79089,\"start\":79078},{\"end\":79105,\"start\":79089},{\"end\":79119,\"start\":79105},{\"end\":79142,\"start\":79119},{\"end\":79155,\"start\":79142},{\"end\":79170,\"start\":79155},{\"end\":79188,\"start\":79170},{\"end\":79203,\"start\":79188},{\"end\":79217,\"start\":79203},{\"end\":79234,\"start\":79217},{\"end\":79627,\"start\":79613},{\"end\":79642,\"start\":79627},{\"end\":79658,\"start\":79642},{\"end\":79669,\"start\":79658},{\"end\":79677,\"start\":79669},{\"end\":79691,\"start\":79677},{\"end\":79710,\"start\":79691},{\"end\":79722,\"start\":79710},{\"end\":80049,\"start\":80034},{\"end\":80066,\"start\":80049},{\"end\":80081,\"start\":80066},{\"end\":80094,\"start\":80081},{\"end\":80583,\"start\":80561},{\"end\":80599,\"start\":80583},{\"end\":80612,\"start\":80599},{\"end\":80628,\"start\":80612},{\"end\":80643,\"start\":80628},{\"end\":80662,\"start\":80643},{\"end\":80676,\"start\":80662},{\"end\":81067,\"start\":81056},{\"end\":81077,\"start\":81067},{\"end\":81092,\"start\":81077},{\"end\":81102,\"start\":81092},{\"end\":81121,\"start\":81102},{\"end\":81361,\"start\":81349},{\"end\":81376,\"start\":81361},{\"end\":81388,\"start\":81376},{\"end\":81403,\"start\":81388},{\"end\":81417,\"start\":81403},{\"end\":81427,\"start\":81417},{\"end\":81811,\"start\":81801},{\"end\":81824,\"start\":81811},{\"end\":81839,\"start\":81824},{\"end\":81852,\"start\":81839},{\"end\":81864,\"start\":81852},{\"end\":81875,\"start\":81864},{\"end\":81887,\"start\":81875},{\"end\":81901,\"start\":81887},{\"end\":81911,\"start\":81901},{\"end\":81921,\"start\":81911},{\"end\":82200,\"start\":82185},{\"end\":82211,\"start\":82200},{\"end\":82225,\"start\":82211},{\"end\":82238,\"start\":82225},{\"end\":82252,\"start\":82238},{\"end\":82265,\"start\":82252},{\"end\":82282,\"start\":82265},{\"end\":82298,\"start\":82282},{\"end\":82311,\"start\":82298},{\"end\":82326,\"start\":82311},{\"end\":82794,\"start\":82787},{\"end\":82809,\"start\":82794},{\"end\":82823,\"start\":82809},{\"end\":82837,\"start\":82823},{\"end\":82852,\"start\":82837},{\"end\":82863,\"start\":82852},{\"end\":83182,\"start\":83169},{\"end\":83196,\"start\":83182},{\"end\":83208,\"start\":83196},{\"end\":83219,\"start\":83208},{\"end\":83234,\"start\":83219},{\"end\":83245,\"start\":83234},{\"end\":83587,\"start\":83571},{\"end\":83604,\"start\":83587},{\"end\":83618,\"start\":83604},{\"end\":83631,\"start\":83618},{\"end\":83648,\"start\":83631},{\"end\":83663,\"start\":83648},{\"end\":83676,\"start\":83663},{\"end\":83698,\"start\":83676},{\"end\":83716,\"start\":83698},{\"end\":83729,\"start\":83716},{\"end\":83744,\"start\":83729},{\"end\":83757,\"start\":83744},{\"end\":84048,\"start\":84036},{\"end\":84058,\"start\":84048},{\"end\":84071,\"start\":84058},{\"end\":84083,\"start\":84071},{\"end\":84097,\"start\":84083},{\"end\":84109,\"start\":84097},{\"end\":84120,\"start\":84109},{\"end\":84132,\"start\":84120},{\"end\":84150,\"start\":84132},{\"end\":84168,\"start\":84150},{\"end\":84177,\"start\":84168},{\"end\":84580,\"start\":84567},{\"end\":84592,\"start\":84580},{\"end\":84605,\"start\":84592},{\"end\":84619,\"start\":84605},{\"end\":84637,\"start\":84619},{\"end\":84647,\"start\":84637},{\"end\":84664,\"start\":84647},{\"end\":84677,\"start\":84664},{\"end\":84700,\"start\":84677},{\"end\":85006,\"start\":84995},{\"end\":85019,\"start\":85006},{\"end\":85036,\"start\":85019},{\"end\":85051,\"start\":85036},{\"end\":85059,\"start\":85051},{\"end\":85068,\"start\":85059},{\"end\":85080,\"start\":85068},{\"end\":85369,\"start\":85353},{\"end\":85385,\"start\":85369},{\"end\":85396,\"start\":85385},{\"end\":85409,\"start\":85396},{\"end\":85424,\"start\":85409},{\"end\":85433,\"start\":85424},{\"end\":85737,\"start\":85723},{\"end\":85749,\"start\":85737},{\"end\":85764,\"start\":85749},{\"end\":85786,\"start\":85764},{\"end\":85810,\"start\":85786},{\"end\":86168,\"start\":86147},{\"end\":86185,\"start\":86168},{\"end\":86200,\"start\":86185},{\"end\":86213,\"start\":86200},{\"end\":86228,\"start\":86213},{\"end\":86242,\"start\":86228},{\"end\":86256,\"start\":86242},{\"end\":86270,\"start\":86256},{\"end\":86286,\"start\":86270},{\"end\":86305,\"start\":86286},{\"end\":86689,\"start\":86675},{\"end\":86706,\"start\":86689},{\"end\":86715,\"start\":86706},{\"end\":86729,\"start\":86715},{\"end\":86744,\"start\":86729},{\"end\":86759,\"start\":86744},{\"end\":86770,\"start\":86759},{\"end\":87016,\"start\":87003},{\"end\":87029,\"start\":87016},{\"end\":87370,\"start\":87357},{\"end\":87379,\"start\":87370},{\"end\":87389,\"start\":87379},{\"end\":87403,\"start\":87389},{\"end\":87418,\"start\":87403},{\"end\":87431,\"start\":87418},{\"end\":87438,\"start\":87431},{\"end\":87682,\"start\":87667},{\"end\":87695,\"start\":87682},{\"end\":87705,\"start\":87695},{\"end\":87717,\"start\":87705},{\"end\":87727,\"start\":87717},{\"end\":87739,\"start\":87727},{\"end\":87750,\"start\":87739},{\"end\":87762,\"start\":87750},{\"end\":87771,\"start\":87762},{\"end\":87780,\"start\":87771},{\"end\":88099,\"start\":88070},{\"end\":88114,\"start\":88099},{\"end\":88129,\"start\":88114},{\"end\":88144,\"start\":88129},{\"end\":88153,\"start\":88144},{\"end\":88167,\"start\":88153},{\"end\":88176,\"start\":88167},{\"end\":88186,\"start\":88176},{\"end\":88203,\"start\":88186},{\"end\":88210,\"start\":88203},{\"end\":88733,\"start\":88718},{\"end\":88752,\"start\":88733},{\"end\":88768,\"start\":88752},{\"end\":88783,\"start\":88768},{\"end\":88796,\"start\":88783},{\"end\":89217,\"start\":89201},{\"end\":89228,\"start\":89217},{\"end\":89243,\"start\":89228},{\"end\":89254,\"start\":89243},{\"end\":89271,\"start\":89254},{\"end\":89285,\"start\":89271},{\"end\":89298,\"start\":89285},{\"end\":89306,\"start\":89298},{\"end\":89320,\"start\":89306},{\"end\":89333,\"start\":89320},{\"end\":89745,\"start\":89732},{\"end\":89759,\"start\":89745},{\"end\":89774,\"start\":89759},{\"end\":90036,\"start\":90022},{\"end\":90056,\"start\":90036},{\"end\":90508,\"start\":90500},{\"end\":90521,\"start\":90508},{\"end\":90532,\"start\":90521},{\"end\":90547,\"start\":90532},{\"end\":90562,\"start\":90547},{\"end\":90576,\"start\":90562},{\"end\":90591,\"start\":90576},{\"end\":90605,\"start\":90591},{\"end\":90943,\"start\":90933},{\"end\":90956,\"start\":90943},{\"end\":90971,\"start\":90956},{\"end\":90984,\"start\":90971},{\"end\":90996,\"start\":90984},{\"end\":91007,\"start\":90996},{\"end\":91019,\"start\":91007},{\"end\":91033,\"start\":91019},{\"end\":91043,\"start\":91033},{\"end\":91053,\"start\":91043},{\"end\":91341,\"start\":91326},{\"end\":91350,\"start\":91341},{\"end\":91363,\"start\":91350},{\"end\":91373,\"start\":91363},{\"end\":91390,\"start\":91373},{\"end\":91402,\"start\":91390},{\"end\":91415,\"start\":91402},{\"end\":91424,\"start\":91415},{\"end\":91451,\"start\":91424},{\"end\":91456,\"start\":91451},{\"end\":91832,\"start\":91824},{\"end\":91854,\"start\":91832},{\"end\":91863,\"start\":91854},{\"end\":92348,\"start\":92335},{\"end\":92361,\"start\":92348},{\"end\":92379,\"start\":92361},{\"end\":92392,\"start\":92379},{\"end\":92401,\"start\":92392},{\"end\":92845,\"start\":92827},{\"end\":92855,\"start\":92845},{\"end\":92863,\"start\":92855},{\"end\":92880,\"start\":92863},{\"end\":92898,\"start\":92880},{\"end\":92907,\"start\":92898},{\"end\":92922,\"start\":92907},{\"end\":92939,\"start\":92922},{\"end\":92954,\"start\":92939},{\"end\":92973,\"start\":92954},{\"end\":93356,\"start\":93339},{\"end\":93373,\"start\":93356},{\"end\":93494,\"start\":93476},{\"end\":93505,\"start\":93494},{\"end\":93518,\"start\":93505},{\"end\":93851,\"start\":93835},{\"end\":93867,\"start\":93851},{\"end\":93876,\"start\":93867},{\"end\":93890,\"start\":93876},{\"end\":93905,\"start\":93890},{\"end\":93914,\"start\":93905},{\"end\":93924,\"start\":93914},{\"end\":93940,\"start\":93924},{\"end\":93947,\"start\":93940},{\"end\":94427,\"start\":94416},{\"end\":94444,\"start\":94427},{\"end\":94460,\"start\":94444},{\"end\":94472,\"start\":94460},{\"end\":94483,\"start\":94472},{\"end\":94500,\"start\":94483},{\"end\":94927,\"start\":94916},{\"end\":94947,\"start\":94927},{\"end\":94962,\"start\":94947},{\"end\":94979,\"start\":94962},{\"end\":94995,\"start\":94979},{\"end\":95007,\"start\":94995},{\"end\":95018,\"start\":95007},{\"end\":95033,\"start\":95018},{\"end\":95414,\"start\":95405},{\"end\":95428,\"start\":95414},{\"end\":95445,\"start\":95428},{\"end\":95456,\"start\":95445},{\"end\":95472,\"start\":95456},{\"end\":95488,\"start\":95472},{\"end\":95506,\"start\":95488},{\"end\":95519,\"start\":95506},{\"end\":95534,\"start\":95519},{\"end\":95554,\"start\":95534},{\"end\":95861,\"start\":95848},{\"end\":95878,\"start\":95861},{\"end\":95888,\"start\":95878},{\"end\":95906,\"start\":95888},{\"end\":95920,\"start\":95906},{\"end\":95940,\"start\":95920},{\"end\":95952,\"start\":95940},{\"end\":95970,\"start\":95952},{\"end\":95981,\"start\":95970},{\"end\":95995,\"start\":95981},{\"end\":96343,\"start\":96330},{\"end\":96356,\"start\":96343},{\"end\":96372,\"start\":96356},{\"end\":96388,\"start\":96372},{\"end\":96715,\"start\":96699},{\"end\":96732,\"start\":96715},{\"end\":96748,\"start\":96732},{\"end\":96766,\"start\":96748},{\"end\":97020,\"start\":97004},{\"end\":97037,\"start\":97020},{\"end\":97053,\"start\":97037},{\"end\":97071,\"start\":97053},{\"end\":97388,\"start\":97371},{\"end\":97406,\"start\":97388},{\"end\":97422,\"start\":97406},{\"end\":97443,\"start\":97422},{\"end\":97457,\"start\":97443},{\"end\":97470,\"start\":97457},{\"end\":97486,\"start\":97470},{\"end\":97839,\"start\":97818},{\"end\":97849,\"start\":97839},{\"end\":97858,\"start\":97849},{\"end\":97867,\"start\":97858},{\"end\":97877,\"start\":97867},{\"end\":97888,\"start\":97877},{\"end\":97894,\"start\":97888},{\"end\":98158,\"start\":98145},{\"end\":98172,\"start\":98158},{\"end\":98463,\"start\":98450},{\"end\":98478,\"start\":98463},{\"end\":98492,\"start\":98478},{\"end\":98507,\"start\":98492},{\"end\":98835,\"start\":98820},{\"end\":98849,\"start\":98835},{\"end\":98862,\"start\":98849},{\"end\":98874,\"start\":98862},{\"end\":99277,\"start\":99254},{\"end\":99287,\"start\":99277},{\"end\":99298,\"start\":99287},{\"end\":99312,\"start\":99298},{\"end\":99319,\"start\":99312},{\"end\":99633,\"start\":99620},{\"end\":99648,\"start\":99633},{\"end\":99662,\"start\":99648},{\"end\":99675,\"start\":99662},{\"end\":99684,\"start\":99675},{\"end\":99699,\"start\":99684},{\"end\":99715,\"start\":99699},{\"end\":99727,\"start\":99715},{\"end\":100189,\"start\":100173},{\"end\":100208,\"start\":100189},{\"end\":100232,\"start\":100208},{\"end\":100244,\"start\":100232},{\"end\":100262,\"start\":100244},{\"end\":100277,\"start\":100262},{\"end\":100731,\"start\":100722},{\"end\":100748,\"start\":100731},{\"end\":100759,\"start\":100748},{\"end\":100771,\"start\":100759},{\"end\":100786,\"start\":100771},{\"end\":100804,\"start\":100786},{\"end\":100816,\"start\":100804},{\"end\":101180,\"start\":101165},{\"end\":101198,\"start\":101180},{\"end\":101213,\"start\":101198},{\"end\":101228,\"start\":101213},{\"end\":101237,\"start\":101228},{\"end\":101252,\"start\":101237},{\"end\":101266,\"start\":101252},{\"end\":101625,\"start\":101617},{\"end\":101641,\"start\":101625},{\"end\":101651,\"start\":101641},{\"end\":101662,\"start\":101651},{\"end\":101677,\"start\":101662},{\"end\":101692,\"start\":101677},{\"end\":101708,\"start\":101692},{\"end\":101721,\"start\":101708},{\"end\":101736,\"start\":101721},{\"end\":102199,\"start\":102183},{\"end\":102219,\"start\":102199},{\"end\":102232,\"start\":102219},{\"end\":102250,\"start\":102232},{\"end\":102747,\"start\":102731},{\"end\":102766,\"start\":102747},{\"end\":102785,\"start\":102766},{\"end\":102801,\"start\":102785},{\"end\":102819,\"start\":102801},{\"end\":103069,\"start\":103058},{\"end\":103087,\"start\":103069},{\"end\":103102,\"start\":103087},{\"end\":103111,\"start\":103102},{\"end\":103126,\"start\":103111},{\"end\":103134,\"start\":103126},{\"end\":103144,\"start\":103134},{\"end\":103158,\"start\":103144},{\"end\":103173,\"start\":103158},{\"end\":103193,\"start\":103173},{\"end\":103207,\"start\":103193},{\"end\":103214,\"start\":103207},{\"end\":103787,\"start\":103770},{\"end\":103800,\"start\":103787},{\"end\":103817,\"start\":103800},{\"end\":103834,\"start\":103817},{\"end\":104226,\"start\":104212},{\"end\":104243,\"start\":104226},{\"end\":104258,\"start\":104243},{\"end\":104270,\"start\":104258},{\"end\":104285,\"start\":104270},{\"end\":104297,\"start\":104285},{\"end\":104630,\"start\":104614},{\"end\":104655,\"start\":104630},{\"end\":104853,\"start\":104837},{\"end\":104864,\"start\":104853},{\"end\":104876,\"start\":104864},{\"end\":104890,\"start\":104876},{\"end\":104904,\"start\":104890},{\"end\":104923,\"start\":104904},{\"end\":104929,\"start\":104923},{\"end\":105280,\"start\":105264},{\"end\":105302,\"start\":105280},{\"end\":105313,\"start\":105302},{\"end\":105328,\"start\":105313},{\"end\":105342,\"start\":105328},{\"end\":105361,\"start\":105342},{\"end\":105375,\"start\":105361},{\"end\":105759,\"start\":105748},{\"end\":105775,\"start\":105759},{\"end\":105789,\"start\":105775},{\"end\":105960,\"start\":105952},{\"end\":105971,\"start\":105960},{\"end\":105983,\"start\":105971},{\"end\":105993,\"start\":105983},{\"end\":106006,\"start\":105993},{\"end\":106017,\"start\":106006},{\"end\":106026,\"start\":106017},{\"end\":106041,\"start\":106026},{\"end\":106056,\"start\":106041},{\"end\":106508,\"start\":106492},{\"end\":106521,\"start\":106508},{\"end\":106535,\"start\":106521},{\"end\":106555,\"start\":106535},{\"end\":106566,\"start\":106555},{\"end\":106577,\"start\":106566}]", "bib_venue": "[{\"end\":65178,\"start\":65107},{\"end\":72106,\"start\":72044},{\"end\":72623,\"start\":72604},{\"end\":73238,\"start\":73186},{\"end\":73683,\"start\":73627},{\"end\":74068,\"start\":74057},{\"end\":80235,\"start\":80173},{\"end\":88359,\"start\":88293},{\"end\":88945,\"start\":88879},{\"end\":90205,\"start\":90139},{\"end\":92012,\"start\":91946},{\"end\":94106,\"start\":94035},{\"end\":99023,\"start\":98957},{\"end\":100418,\"start\":100356},{\"end\":102399,\"start\":102333},{\"end\":103373,\"start\":103302},{\"end\":103898,\"start\":103887},{\"end\":63391,\"start\":63333},{\"end\":64055,\"start\":64006},{\"end\":64400,\"start\":64352},{\"end\":65105,\"start\":65019},{\"end\":65613,\"start\":65561},{\"end\":66052,\"start\":65979},{\"end\":66822,\"start\":66773},{\"end\":67314,\"start\":67275},{\"end\":67805,\"start\":67760},{\"end\":68094,\"start\":68082},{\"end\":68955,\"start\":68942},{\"end\":69629,\"start\":69604},{\"end\":69806,\"start\":69722},{\"end\":70320,\"start\":70218},{\"end\":70778,\"start\":70734},{\"end\":71097,\"start\":71022},{\"end\":71618,\"start\":71578},{\"end\":72042,\"start\":71965},{\"end\":72602,\"start\":72551},{\"end\":73184,\"start\":73117},{\"end\":73625,\"start\":73554},{\"end\":74055,\"start\":74004},{\"end\":74534,\"start\":74482},{\"end\":74821,\"start\":74811},{\"end\":75012,\"start\":74951},{\"end\":75429,\"start\":75426},{\"end\":75658,\"start\":75571},{\"end\":75970,\"start\":75910},{\"end\":76282,\"start\":76278},{\"end\":76540,\"start\":76536},{\"end\":76762,\"start\":76684},{\"end\":77191,\"start\":77118},{\"end\":77529,\"start\":77449},{\"end\":77976,\"start\":77913},{\"end\":78338,\"start\":78269},{\"end\":78707,\"start\":78637},{\"end\":79076,\"start\":79005},{\"end\":79611,\"start\":79540},{\"end\":80171,\"start\":80094},{\"end\":80559,\"start\":80460},{\"end\":81054,\"start\":80968},{\"end\":81517,\"start\":81443},{\"end\":81799,\"start\":81731},{\"end\":82423,\"start\":82342},{\"end\":82785,\"start\":82724},{\"end\":83167,\"start\":83076},{\"end\":83569,\"start\":83490},{\"end\":84239,\"start\":84193},{\"end\":84565,\"start\":84488},{\"end\":85087,\"start\":85080},{\"end\":85446,\"start\":85433},{\"end\":85721,\"start\":85635},{\"end\":86145,\"start\":86061},{\"end\":86673,\"start\":86628},{\"end\":87121,\"start\":87045},{\"end\":87355,\"start\":87283},{\"end\":87822,\"start\":87796},{\"end\":88291,\"start\":88210},{\"end\":88877,\"start\":88796},{\"end\":89412,\"start\":89349},{\"end\":89730,\"start\":89669},{\"end\":90137,\"start\":90056},{\"end\":90498,\"start\":90423},{\"end\":90931,\"start\":90863},{\"end\":91324,\"start\":91254},{\"end\":91944,\"start\":91863},{\"end\":92495,\"start\":92401},{\"end\":93013,\"start\":92973},{\"end\":93337,\"start\":93284},{\"end\":93592,\"start\":93534},{\"end\":94033,\"start\":93947},{\"end\":94601,\"start\":94516},{\"end\":95082,\"start\":95033},{\"end\":95403,\"start\":95353},{\"end\":95846,\"start\":95808},{\"end\":96432,\"start\":96388},{\"end\":96697,\"start\":96650},{\"end\":97115,\"start\":97071},{\"end\":97535,\"start\":97486},{\"end\":97816,\"start\":97772},{\"end\":98143,\"start\":98090},{\"end\":98448,\"start\":98329},{\"end\":98955,\"start\":98874},{\"end\":99395,\"start\":99335},{\"end\":99811,\"start\":99743},{\"end\":100354,\"start\":100277},{\"end\":100877,\"start\":100832},{\"end\":101163,\"start\":101078},{\"end\":101806,\"start\":101736},{\"end\":102331,\"start\":102250},{\"end\":102729,\"start\":102657},{\"end\":103300,\"start\":103214},{\"end\":103885,\"start\":103834},{\"end\":104357,\"start\":104313},{\"end\":104612,\"start\":104548},{\"end\":104986,\"start\":104945},{\"end\":105424,\"start\":105375},{\"end\":105746,\"start\":105700},{\"end\":106160,\"start\":106072},{\"end\":106490,\"start\":106414}]"}}}, "year": 2023, "month": 12, "day": 17}