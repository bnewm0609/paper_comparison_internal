{"id": 9136312, "updated": "2022-03-22 16:49:18.886", "metadata": {"title": "The Konstanz natural video database (KoNViD-1k)", "authors": "[{\"first\":\"Vlad\",\"last\":\"Hosu\",\"middle\":[]},{\"first\":\"Franz\",\"last\":\"Hahn\",\"middle\":[]},{\"first\":\"Mohsen\",\"last\":\"Jenadeleh\",\"middle\":[]},{\"first\":\"Hanhe\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Hui\",\"last\":\"Men\",\"middle\":[]},{\"first\":\"Tam\u00e1s\",\"last\":\"Szir\u00e1nyi\",\"middle\":[]},{\"first\":\"Shujun\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Dietmar\",\"last\":\"Saupe\",\"middle\":[]}]", "venue": "2017 Ninth International Conference on Quality of Multimedia Experience (QoMEX)", "journal": "2017 Ninth International Conference on Quality of Multimedia Experience (QoMEX)", "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "Subjective video quality assessment (VQA) strongly depends on semantics, context, and the types of visual distortions. Currently, all existing VQA databases include only a small number of video sequences with artificial distortions. The development and evaluation of objective quality assessment methods would benefit from having larger datasets of real-world video sequences with corresponding subjective mean opinion scores (MOS), in particular for deep learning purposes. In addition, the training and validation of any VQA method intended to be \u2018general purpose\u2019 requires a large dataset of video sequences that are representative of the whole spectrum of available video content and all types of distortions. We report our work on KoNViD-1k, a subjectively annotated VQA database consisting of 1,200 public-domain video sequences, fairly sampled from a large public video dataset, YFCC100m. We present the challenges and choices we have made in creating such a database aimed at \u2018in the wild\u2019 authentic distortions, depicting a wide variety of content.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2611434713", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/qomex/HosuHJLMSLS17", "doi": "10.1109/qomex.2017.7965673"}}, "content": {"source": {"pdf_hash": "945a8bf43c2dcf58f86798743cabc034c46d8fae", "pdf_src": "Anansi", "pdf_uri": "[\"https://kar.kent.ac.uk/69606/1/konstanz-natural-video.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://kar.kent.ac.uk/69606/1/konstanz-natural-video.pdf", "status": "GREEN"}}, "grobid": {"id": "a5c4d3f86368d342ee4170899a1ab4223fd87914", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/945a8bf43c2dcf58f86798743cabc034c46d8fae.txt", "contents": "\nThe Konstanz Natural Video Database (KoNViD-1k)\nIEEECopyright IEEE2017\n\nHosu \nVlad \nHahn \nFranz \nMohsenJenadeleh \nHanheLin \nMen \nHui \nTam\u00e1sSzir\u00e1nyi \nShujunLi Saupe \nVlad Hosu \nDepartment of Computer and Information Science\nUniversity of Konstanz\nGermany\n\nFranz Hahn \nDepartment of Computer and Information Science\nUniversity of Konstanz\nGermany\n\nMohsen Jenadeleh \nDepartment of Computer and Information Science\nUniversity of Konstanz\nGermany\n\nFaculty of Computer Science and Engineering\nShahid Beheshti University\nTehranG. CIran\n\nHanhe Lin \nDepartment of Computer and Information Science\nUniversity of Konstanz\nGermany\n\nHui Men \nDepartment of Computer and Information Science\nUniversity of Konstanz\nGermany\n\nTam\u00e1s Szir\u00e1nyi \nInstitute for Computer Science and Control\nHungarian Academy of Sciences\nHungary\n\nShujun Li \nDepartment of Computer Science\nUniversity of Surrey\nUnited Kingdom\n\nDietmar Saupe \nDepartment of Computer and Information Science\nUniversity of Konstanz\nGermany\n\nThe Konstanz Natural Video Database (KoNViD-1k)\n\n2017 Ninth International Conference on Quality of Multimedia Experience (QoMEX). Proceedings of the 2017 9th International Conference on Quality of Multimedia Experience\nGermanyIEEE201710.1109/QoMEX.2017.7965673The version in the Kent Academic Repository may differ from the final published version. Users are advised to check http://kar.kent.ac.uk for the status of the paper. Users should always cite the published version of record. Enquiries For any further enquiries regarding the licence status of this document, please contact: researchsupport@kent.ac.uk If you believe this document infringes copyright then please contact the KAR admin team with the take-down information provided at http://kar.kent.ac.uk/contact.html Document Version Author's Accepted ManuscriptVideo databaseauthentic videovideo quality as- sessmentfair samplingcrowdsourcing\nSubjective video quality assessment (VQA) strongly depends on semantics, context, and the types of visual distortions. Currently, all existing VQA databases include only a small number of video sequences with artificial distortions. The development and evaluation of objective quality assessment methods would benefit from having larger datasets of real-world video sequences with corresponding subjective mean opinion scores (MOS), in particular for deep learning purposes. In addition, the training and validation of any VQA method intended to be 'general purpose' requires a large dataset of video sequences that are representative of the whole spectrum of available video content and all types of distortions. We report our work on KoNViD-1k, a subjectively annotated VQA database consisting of 1,200 publicdomain video sequences, fairly sampled from a large public video dataset, YFCC100m. We present the challenges and choices we have made in creating such a database aimed at 'in the wild' authentic distortions, depicting a wide variety of content.\n\nI. INTRODUCTION\n\nMost of the Internet traffic today stems from user-generated videos on sharing web-sites and social networks. Video sequences pass through several stages of processing before they reach consumers, which often deteriorate visual quality. Moreover, the vast amount of user-generated video content and the increased diversity of end user devices (ranging from smaller and power-constrained mobile devices to large displays such as 4K Ultra HDTVs and TV walls) calls for a broad range of video quality to be supported. Adapting video quality to different use cases has become an important topic for researchers, content providers and distributors [1].\n\nAutomatic and accurate prediction of video quality is a basic operation for many video processing applications such as video quality monitoring in transmission protocols, video quality filtering in sharing services, automatic and recommended camera parameter settings during video capturing, and video enhancement. Specifically, no-reference methods attempt to judge the quality of a video sequence without any additional information about the original recorded scene. Such blind methods may apply machine learning techniques to learn from large amounts of annotated data. However, current video quality assessment (VQA) databases contain only a small number of video sequences with little content diversity, thus offering limited support for designing and evaluating noreference VQA methods effectively and fairly.\n\nAdditionally, these databases were mostly designed to include only artificially distorted video sequences to simulate quality loss in compression, transmission, and other parts of the video processing and distribution pipeline. Some databases capture imagery with a variety of cameras to encompass authentic video acquisition distortions, however, with content restricted to a small number of physical scenes.\n\nWinkler [2] proposed several criteria for quantitative comparisons of source content, test conditions, and subjective ratings, applying them to 27 image and video databases. Most collections have not been found satisfactory in terms of content range and uniformity. Only few databases showed good uniformity for test conditions (image/video quality), but not over the whole quality range. Also the distortion variety was found lacking in most databases covering mainly compression and transmission, but not the many other types of natural distortions found \"in the wild\" [3].\n\nTo overcome these limitations we introduce KoNViD-1k, a large publicly available database of video sequences based on YFCC100m (Yahoo Flickr Creative Commons 100 Million) dataset [4] with a diverse set of video content. In this paper we report the filtering mechanisms and sampling procedures necessary to construct high-quality VQA databases of this kind, focusing on their usefulness in a variety of applications.\n\nIn the next section, we describe the database creation procedure and the set of attributes we have considered to maximise its diversity. Additional information regarding the removal of non-natural video sequences and sampling techniques are provided as well. Next, in Sec. III, we review our crowdsourcing-based process of collecting subjective mean opinion scores (MOS) and detail our results as well as crowd worker statistics. In Sec. IV we relate our database characteristics and creation methodology with other existing works and outline the differences, before discussing conclusions of our work and considering possible future work.\n\n\nII. DATABASE CREATION\n\nStarting with a large initial collection of video sequences, the goal of our database was to ensure diversity in several dimensions that could impact video quality. Thus, after removing outliers, we sampled across six attributes that were calculated on the entire collection, in a more uniform manner to ensure diverse coverage of selected samples. We call this methodology \"fair sampling\".\n\n\nA. Video collection\n\nWe began with a well-known public database of video sequences and images, YFCC100m [4], containing 793,436\n\nQoMEX2017 -Erfurt, Germany; 978-1-5386-4024-1/17/$31.00 c 2017 IEEE Creative Commons (CC) licensed videos. We selected videos based on practical requirements, such that they:\n\n\u2022 Were still available for download \u2022 Played at more than 15 frames per second (FPS) \u2022 Lasted longer than 8 seconds \u2022 Did not have a \"No Derivative Works\" CC attribute \u2022 Had a resolution higher than 960 \u00d7 540 (W \u00d7 H) \u2022 Were in landscape layout This filtering yielded a subset of 144,889 videos for further processing. The entire collection was downloaded from the Flickr servers for later processing. For longer videos, only the first 30 seconds were stored.\n\n\nB. Attribute computation\n\nWinkler [2] suggested three attributes related to temporal, color, and spatial aspects to measure the content diversity of video databases. Guided by his research, we chose six video attributes and used them not only for the analysis of the resulting database, but for its creation as well. For each attribute we relied on the best-performing technique available in the literature (to the best of our knowledge). All attributes, except the one related to colorfulness, were computed on grayscale frames.\n\nSince the computational complexity of some of the selected metrics is high relative to the number of frames to be analyzed, we created cropped and scaled versions of the videos to run them on. In some cases, only a subset of frames were used for processing.\n\n\n1) Blur:\n\nThe blur of a frame was assessed by the cumulative probability of blur detection (CPBD) metric [5]. Intuitively, the technique measures the probability of blur based on the distribution of edge widths. The CPBD metric works on individual frames. We applied it to videos by averaging the CPBD values on one frame every second over the entire duration of the video. We made this choice because of the high computational cost of running the blur measure.\n\n2) Colorfulness: For this attribute we used Hasler and Suesstrunk's metric reported in [6]. With the RGB channels of a frame as matrices R, G, and B, one computes two matrices rg = R \u2212 G and yb = 1 2 (R + G) \u2212 B. Then, the metric is calculated as \u03c3 2 rg + \u03c3 2 yb + 3 10 \u00b5 2 rg + \u00b5 2 yb , where \u03c3 2 \u00b7 and \u00b5 \u00b7 denote the variance and mean of the values in their respective matrices. Finally, the average value over all frames yields the colorfulness metric of a video.\n\n\n3) Contrast:\n\nFrame contrast was measured simply by the standard deviation of pixel grayscale intensities [7]. The average frame-level standard deviation then gave the contrast of a video.\n\n\n4) Spatial information:\n\nThe spatial information (SI) was obtained by applying a Sobel filter to each frame to extract the gradient magnitude for each pixel and then computing its standard deviation [8]. The average standard deviation over all frames yielded the SI of the video.\n\n\n5) Temporal information:\n\nSimilar to SI, the temporal information (TI) is the mean of frame-wise standard deviations of pixel-wise frame difference [8]. 6) Video quality: We used the Natural Image Quality Evaluator (NIQE) [9] as a proxy to assess video quality by computing the mean NIQE value of all frames. This method (VNIQE) does not require knowledge of distortion types nor quality ratings for training. The NIQE of a frame is simply the distance between certain ideal features and a particular frame's features. It can be interpreted as the degree of framelevel deviation from naturalness, according to the NIQE model.\n\n\nC. Filtering\n\nBased on the six attributes, videos were selected such that they are suitable for VQA. We removed videos depicting nonnatural scenes like screen recordings or stop-motion sequences, as well as overly dark or bright videos.\n\n\n1) Filtering extremes:\n\nMost of these situations were encountered at extremes of the attribute values. For instance, very low TI videos were often found to be screen-text recordings due to little change between frames. Dark videos have low contrast and SI. Uniformly and brightly colored videos have a high level of colorfulness. Therefore, we decided to remove videos that have extreme attribute values. The filtering thresholds were empirically chosen based on a qualitative inspection such that most of the filtered videos show obvious artificial content. The selected thresholds are available in Table I.\n\n\n2) Filtering stop-motion videos:\n\nWith regard to removing stop-motion videos, we relied on two observations; namely they show periodical changes in TI, while a high percentage of consecutive frames show no change at all. We used the difference of TI of consecutive frames to quantify both factors.\n\nWith respect to the periodicity, we found local maxima of TI with an inter-peak distance greater than 1 second. By computing the distances d i between consecutive local maxima together with their mean \u00b5 and variance \u03c3 2 , we extracted a measure of regularity as R = \u00b5/\u03c3 2 . If the variance \u03c3 2 is low, then the regularity R is high. The mean of the distances is the average spacing between peaks. If the peaks are further apart the formula tolerates smaller changes in the peak timing, and R is higher. R > 1/300 was found to be a good indicator for detecting stop-motion videos. Secondly, if more than 30% of consecutive frames showed no difference in TI, we assumed to be dealing with a non-natural video sequence.\n\nThese criteria further eliminated about 500 videos from our video collection, leading to a database of 124,865 videos, which we call KoNViD-125k. The distributions of the normalized attribute values are displayed in Fig. 1 on the left.\n\n\nD. Sampling\n\nOur eventual goal is to sample a set of 10,000 diverse videos from the KoNViD-125k filtered collection and to have the sample subjectively annotated by human observers. In this paper, we focus on what sampling procedures we should use and understand how well they would work for constructing VQA databases of this kind.\n\nWe devised a \"fair-sampling\" strategy with which we generated a subset of 10,000 videos. From the resulting set we took a random collection of 1,200 videos, which forms the KoNViD-1k database [10]. We performed subjective studies to assess the visual quality of the videos in this database (see Sec. III for details). Consequently, we arrived at a better understanding of the diversity of the 10,000 fair-sampled videos, and the efficiency of our approach.\n\nA \"fair sampling\" mechanism should produce a broader diversity of video properties than a random sampling mechanism. Our videos are represented as points in a 6-dimensional attribute space. We can think of the KoNViD-125k collection as a sample of M = 124, 865 points of a multivariate and approximately normal distribution, for which random subsampling of 10,000 items would yield a subset with a similar normal distribution. The sampling procedure is engineered to give a more uniform 6-dimensional sample distribution.\n\nEach attribute relates to a particular subjective property. Most videos having extreme values for one or more attributes show severe quality degradations. Random sampling is unlikely to select these \"unusual\" videos, which are as important as the \"normal\" ones. A preliminary qualitative inspection of several hundred videos (both randomly and fairly sampled) suggested that our strategy creates a balanced mix of videos.\n\nWith respect to the sampling procedure, the method of Vonikakis et al. [11] can ensure a uniform distribution for each attribute independently. However, we are also interested in sampling videos with joint distortions, having extreme values in several attributes simultaneously. Thus, we applied a different approach.\n\nNote that our attributes are correlated as shown in Fig. 2. For instance, contrast and spatial information (SI) have a 0.62 correlation, whereas VNIQE and SI have a negative correlation of \u22120.43. Thus, as a preprocessing step we applied a principal component analysis (PCA), that shows that 37.1%, 57.7%, 73.4%, 85.8%, and 95.2% of the variance of the data is explained by the 1st to 5th principal components, respectively. We decorrelated the attributes by taking five components, maintaining all but 5% of the signal energy.\n\nOne way to solve the subsampling problem is to design a sampling method that favors videos that are part of a lowdensity region in the attribute space as follows.\n\nLet \u03c1 : R 5 \u2192 R denote the probability density function of the 5-dimensional PCA attribute vectors v i for natural videos, estimated from a video collection such as our KoNViD-125k. We used a k-NN method to estimate the density in the neighbourhood of a video in the PCA attribute space. For the i-th video attribute v i we first found its k-th nearest neighbour v k(i) (we chose k = 500) and computed the distance ||v i \u2212 v k(i) ||. This distance is the smallest radius of a hypersphere about v i that encompasses k videos. The density \u03c1(v i ) was then taken inversely proportional to the volume ||v i \u2212v k(i) || n , with n = 5, the number of dimensions.\n\nThe task then is to assign suitable sampling probabilities p i , i = 1, . . . , M for the set of videos in KoNViD-125k having attribute vectors v i , i = 1, . . . , M . Then, 10,000 subsamples will be drawn with corresponding probabilities p i and without replacement. A natural choice is to set the probabilities p i proportional to the inverse of the attribute densities, 1/\u03c1(v i ), at the corresponding attribute vectors, v i , for all i = 1, . . . , M .\n\nFrom the 10,000 fairly sampled videos, we randomly subsampled 1,200 to form the KoNViD-1k dataset. A Gaussian kernel density estimation along each dimension shows the distribution of the samples (see Fig. 1, right sub-figure). We can see that attribute values are more evenly spread in most dimensions, except for the temporal information. This might be caused by local correlations in the data that have not been removed by PCA. \n\n\nE. Database analysis\n\nWe have shown that our fair sampling strategy has led to a better diversity with respect to six attributes. The diversity of the KoNViD-1k collection extends to other content-related characteristics such as Flickr meta-data tags and authorship. Table II summarizes related statistics.\n\nThe KoNViD-1k videos are encoded at three predominant frame rates: 24, 25 and 30 FPS corresponding to 27%, 5% and 68% of the items respectively. There are a total of 12 resolutions, with the largest percentage of videos having a frame size of 1280 \u00d7 720 pixels (85% of the videos), followed by 1920\u00d7 1080 (9%). Most of the videos (97%) have an audio channel. The proportions of all these characteristics are similar between KoNViD-1k and KoNViD-125k.\n\n\nIII. SUBJECTIVE QUALITY ASSESSMENT\n\nIn the KoNViD-1k database we make available a variety of meta-data together with subjective quality scores. Due to the large number of videos we crowdsourced the subjective scores using the widely used CrowdFlower platform (https: //www.crowdflower.com/).\n\n\nA. Crowdsourcing VQA\n\nQuality control is a key component when designing an experiment for the crowd, so we considered how to set up our procedure carefully. Initially, each worker was instructed according to VQEG recommendations [12], which were modified to fit our single stimulus presentation technique. In the instructions workers were informed about types of degradation (e.g., related to motion, color, brightness, and details) and about how they would be asked to evaluate the overall quality of each video. Next, examples of videos with \"Good\", \"Fair\" and \"Bad\" quality were displayed for anchoring. Further, workers were instructed on the steps required to rate a video. Initially, a button was displayed below the video, which started playing the video muted. Once the video was finished playing a rating scale was displayed and workers had to select one of five categories to proceed. Only if a worker had watched and rated all 10 videos on a page, could he proceed to the next. By this design we hoped to ensure a better engagement of the workers and a better quality control.\n\nIn order to control for the quality of workers' performance, it is common to use gold standard questions, which is also a feature provided by CrowdFlower. Since there is no ground truth for our database, we have devised a plan to filter unreliable workers. We randomly sampled a subset of 100 videos from our pilot for an uncontrolled (no gold standard questions) crowdsourcing experiment of 50 ACR scores per video. From these, we computed the 95% confidence intervals of the MOS values to select those videos with a size of the confidence interval smaller than 0.5 on the ACR scale. The resulting 65 highly agreed upon videos and their MOS values were used as the ground truth for test questions for the evaluation of the entire data set.\n\nWe employed the same setup for KoNViD-1k as above, with the addition of the test questions for quality control. We considered the rounded MOS \u00b11 as eligible answers to these test questions. Workers that fell below 70% accuracy on test questions were removed from the experiment along with the data they had generated. Moreover, we only allowed CrowdFlower workers of Level 1 and above (more than 70% accuracy in all previous tasks) to participate in our experiment. Due to the fixed number of 65 test questions, workers could rate at most 550 videos in batches of 10 per page (10 questions for the quiz page, 55 batches with one test question each).\n\n\nB. Crowdsourcing results and analysis\n\nIn our study we required a 95% confidence interval for the MOS values, averaged over all stimuli, with a length not exceeding 0.5 on the 5-point ACR scale. This was achieved by a minimum of 50 judgments per video. This setup resulted in a total of 642 workers from 64 countries participating in our experiment. On average each video received 114 votes (including test questions) with a mean accuracy of 94% on the test questions. See Fig. 3 for a histogram of common user statistics. With regards to screen size, 94% of workers had a resolution above 1024 \u00d7 600 pixels (width \u00d7 height), which allowed full size display of the videos. It is to be noted that we did not enforce 1:1 pixel display. However, nearly two-thirds of workers did in fact view the videos unzoomed at 1:1, while 20% used a ratio of 0.9 to 0.75 and 14% displayed the videos at a zoom of up to 2 (including font scaling).\n\nWhen comparing video quality ratings across different studies, commonly MOS, standard deviations and confidence intervals are considered [8]. However, it has been shown that when comparing different rating scales, the design and discretisation of rating scales have a strong influence on the standard deviations of the opinion scores (SOS). In [13] a quadratic model is proposed for the dependence of the variance \u03c3 2 on the MOS values. For the 5-point ACR scale the function \u03c3 2 (MOS) = a(MOS \u2212 1)(5 \u2212 MOS) is fitted to empirical data, which yields the SOS parameter a. The parameter a quantifies the variance of the user ratings more appropriately than the average over all stimuli. Moreover, it characterises application categories and correlates with task difficulty [14]. For VQA, the SOS parameter a was reported to fall in the range [0.11, 0.21] [13].\n\nWe compared the standard deviations of the crowdsourced ratings of our KoNViD-1k to the CVD2014 (normalized from a 0-99 scale to a 5-point ACR scale) and IRCCyN-IVC-1080i databases [15], [16], where the subjective scores were gathered in a lab setting, see Fig. 4. With a = 0.14 our SOS hypothesis parameter is lower than those of the two compared databases (0.17 for CVD2014 and 0.21 for IRCCyN-IVC-1080i, respectively). This suggests that our task was simpler than the compared lab studies and resulted, as is desirable, in lower variances of worker ratings with smaller confidence intervals for the MOS values.\n\n\nIV. RELATED WORK\n\nIn recent years, a broad range of VQA databases have been released [2]. The first comprehensive VQA database, dubbed EPFL-PoliMI, was published in [17]. The diversity of the items in this database was guaranteed by selecting the scenes that are representative of different levels of spatial and temporal complexity, which are the same as the SI and TI used in our KoNViD-1k database. LIVE [18], another wellknown database, generated four types of distortions based on reference videos. It was further extended to LIVE Mobile [19], which also modelled distortions in heavily trafficked wireless networks, containing dynamical changing distortions including frame-freezes and temporally varying compression rates. Other databases similar to LIVE have been released such as IVP [20], CSIQ [21], MCL-V [22]. The aforementioned VQA databases, have the following drawbacks. First, all these VQA databases include videos that were artificially created from a small number of distortion-free reference videos. The distortion types range from compression artifacts e.g. MPEG-2, H.264 to transmission-based distortions such as those induced by packet-loss over IP networks or error-prone wireless systems, additive Gaussian noise, and others. The authenticity and representativeness of said distortions are far from that of distorted videos \"in the wild\". Second, the limited number of reference videos cannot guarantee the content diversity of these databases, and large-scale machine learning techniques cannot reliably learn from such a limited number of distorted videos. To address the drawback of artificial distortions in VQA databases, the CVD2014 database features different types of cameras to produce distortions related to the video acquisition process. A total of 78 cameras captured the same five selected physical scenes. However, the small number of cameras and scenes it contains limits the diversity of video content and quality distortions. These deficiencies of current video databases would be difficult to assess quantitatively. However, an indirect confirmation is given by the fact that the performance of two established objective VQA algorithms on our KoNViD-1k database was significantly worse than on the traditional databases that were used for their development, even when the techniques were trained on our natural video dataset [23].\n\nAll of the above VQA databases provide subjective scores, namely MOS or difference mean opinion scores (DMOS), via lab-based studies which are time-consuming and expensive. Recently, it has been shown that reliable measures of quality of experience can be generated by crowdsourcing for images [24], [3] and videos [25]. It has been suggested that crowdsourcing workers can produce reliable VQA annotations by using paired comparisons and converting the results to DMOS [26].\n\n\nV. CONCLUSIONS, LIMITATIONS, AND FUTURE WORK\n\nWe created a fairly sampled, subjectively annotated video database, showing authentic distortions. For this purpose we first collected videos from the YFCC100m dataset following some practical minimum requirements (total of 144,889 videos). In order to guarantee the diversity in terms of content and multi-dimensional quality of the sampled dataset, six attributes (blurriness, colorfulness, contrast, temporal information, spatial information and VNIQE) were computed. Based on the values of the six attributes, we further filtered the extreme videos using empirical thresholds for each attribute. From the filtered collection of videos, we then devised a methodology based on k-NN density estimation to uniformly sample a subset of 10,000 videos. A random subset of 1,200 videos was further randomly sampled to produce a new VQA database, namely KoNViD-1k. Subjective scores of all videos in the KoNViD-1k database were obtained by a well-designed crowdsourcing experiment of ACR judgments. Compare with other two databases [15], [16], the standard deviations of the crowdsourced ratings for KoNViD-1k is the smallest, showing the highest agreement between workers.\n\nOur KoNViD-1k database consists of diverse videos from YFCC100m published by different users using various cameras with different shooting skills; hence the number of high quality videos is quite small. This will limit the diversity of quality in our database, thus further influence the quality assessment performance. When we extend our KoNViD-1k to 10,000 videos we will consider including more high quality ones.\n\nSince the videos were filtered and sampled based on six attributes, the metric we chose for each attribute has influenced the diversity of the database. As a pilot, the methods for computing the attributes were chosen mostly for their feasibility and low computational complexity. We will consider alternative methods and empirically validate their effectiveness in representing the intended subjective attribute and to evaluate the linearity of the corresponding subjective scale, e.g. contrast or blur, by crowdsourcing experiments. We will also consider including additional attributes, such as overall brightness, camera shake, and color appropriateness.\n\nOur choice of using quality ratings as test questions is a topic debated in the multimedia crowdsourcing community, as it may filter sincere users. Consensus seems to be that objective questions such as content related ones are a better way to perform reliability checks [27].\n\nFor the purpose of reducing the computational complexity of some of the metrics when computing the six attributes, we created cropped and scaled versions of the original videos to run them on, and chose a subset of frames to be processed in some cases. It remains to be checked that the resulting attribute values are entirely representative of the originals. This is the largest quality assessment database of authentic Internet videos to date. Our strategy to carefully sample and subjectively annotate videos produces a highly representative set of videos (content types and quality distortions). We hope our efforts will lead the VQA community to greater levels of ecological validity for existing benchmarks and provide opportunities for developing new VQA methods. For instance, tapping into deep learning is possible when substantial training data is available. Our efforts for KoNViD-1k and the upcoming KoNViD-10k will pave the way to \"deeper\" VQA techniques.\n\nFig. 1 :\n1Distribution of attribute values over the larger filtered dataset and the sampled 1,200 videos.\n\nFig. 2 :\n2Correlations between attribute values. Correlations considering KoNViD-125k dataset are above the main diagonal, and for KoNViD-1k below. Larger and darker circles represent a stronger absolute correlation coefficient. Red hues encode negative correlations, whereas blues are positive.\n\nFig. 3 :\n3Crowd-worker statistics on country of origin and screen resolution. First five groups are shown.\n\nFig. 4 :\n4Standard deviation of quality ratings as a function of MOS values for VQA in three databases with algebraic regression curves. The SOS hypothesis a values for our experiment on KoNViD-1k, CVD and IRCCyN are 0.14, 0.17 and 0.21 respectively. Smaller values imply better agreement. Each dot represents one stimulus (video).\n\nFig. 5 :\n5Example extreme quality videos: a. MOS 1.26 (lowest quality score in KoNViD-1k), b. MOS 1.52, c. MOS 4.12, d. 4.64 (highest score). Low scores are usually caused by strong motion-blur due to shake, out-of-focus, or compression artifacts. High-scoring videos are sharp and show few distortions.\n\nTABLE I :\nIAttribute thresholds for filtering outlier videos.Attribute \nLowest value \nHighest value \n1 \nBlur amount \n0.05 \n0.88 \n2 \nColorfulness \n4.37 \n123.00 \n3 \nContrast \n7.51 \n97.48 \n4 \nSpatial information \n7.70 \n187.76 \n5 \nTemporal information \n3.07 \n56.81 \n6 \nVNIQE \n3.58 \n23.08 \n\n\n\nTABLE II :\nIIDatabase information and comparison.KoNViD-1k \nKoNViD-125k \nUnique authors \n480 \n8418 \nAverage videos per user \n7.3 \n7.5 \nMax videos per user \n11 \n1500 \nVideos with user-tags \n620 (52%) \n62646 (50%) \nTotal unique tags \n2669 \n47336 \n\n\nACKNOWLEDGMENTSWe thank the German Research Foundation (DFG) for financial support within project A05 of SFB/Transregio 161.\nVideo adaptation: Concepts, technologies, and open issues. S.-F Chang, A Vetro, Proc. of the IEEE. of the IEEE93S.-F. Chang and A. Vetro, \"Video adaptation: Concepts, technologies, and open issues,\" Proc. of the IEEE, vol. 93, no. 1, pp. 148-158, 2005.\n\nAnalysis of public image and video databases for quality assessment. S Winkler, IEEE Journal of Selected Topics in Signal Processing. 66S. Winkler, \"Analysis of public image and video databases for quality assessment,\" IEEE Journal of Selected Topics in Signal Processing, vol. 6, no. 6, pp. 616-625, 2012.\n\nMassive online crowdsourced study of subjective and objective picture quality. D Ghadiyaram, A C Bovik, IEEE Transactions on Image Processing. 251D. Ghadiyaram and A. C. Bovik, \"Massive online crowdsourced study of subjective and objective picture quality,\" IEEE Transactions on Image Processing, vol. 25, no. 1, pp. 372-387, 2016.\n\nYFCC100m: The new data in multimedia research. B Thomee, D A Shamma, G Friedland, B Elizalde, K Ni, D Poland, D Borth, L.-J Li, Communications of the ACM. 592B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L.-J. Li, \"YFCC100m: The new data in multimedia research,\" Communications of the ACM, vol. 59, no. 2, pp. 64-73, 2016.\n\nA no-reference image blur metric based on the cumulative probability of blur detection (CPBD). N D Narvekar, L J Karam, IEEE Transactions on Image Processing. 209N. D. Narvekar and L. J. Karam, \"A no-reference image blur metric based on the cumulative probability of blur detection (CPBD),\" IEEE Transactions on Image Processing, vol. 20, no. 9, pp. 2678-2683, 2011.\n\nMeasuring colorfulness in natural images. D Hasler, S E Suesstrunk, Electronic Imaging 2003. International Society for Optics and Photonics. D. Hasler and S. E. Suesstrunk, \"Measuring colorfulness in natural images,\" in Electronic Imaging 2003. International Society for Optics and Photonics, 2003, pp. 87-95.\n\nContrast in complex images. E Peli, Journal of the Optical Society of America, A. 710E. Peli, \"Contrast in complex images,\" Journal of the Optical Society of America, A, vol. 7, no. 10, pp. 2032-2040, 1990.\n\nSubjective video quality assessment methods for multimedia applications. Itu-T, ITU-T Recommendation P.910. ITU-T, \"Subjective video quality assessment methods for multimedia applications,\" ITU-T Recommendation P.910, 2008.\n\nMaking a \"completely blind\" image quality analyzer. A Mittal, R Soundararajan, A C Bovik, IEEE Signal Processing Letters. 203A. Mittal, R. Soundararajan, and A. C. Bovik, \"Making a \"completely blind\" image quality analyzer,\" IEEE Signal Processing Letters, vol. 20, no. 3, pp. 209-212, 2013.\n\nV Hosu, F Hahn, M Jenadeleh, H Lin, H Men, T Szir\u00e1nyi, S Li, D Saupe, The Konstanz Natural Video Database KoNViD-1k. V. Hosu, F. Hahn, M. Jenadeleh, H. Lin, H. Men, T. Szir\u00e1nyi, S. Li, and D. Saupe, \"The Konstanz Natural Video Database KoNViD-1k,\" http://database.mmsp-kn.de/, 2017.\n\nShaping datasets: Optimal data selection for specific target distributions across dimensions. V Vonikakis, R Subramanian, S Winkler, Proceedings of 2016 International Conference on Image Processing. 2016 International Conference on Image ProcessingIEEEV. Vonikakis, R. Subramanian, and S. Winkler, \"Shaping datasets: Op- timal data selection for specific target distributions across dimensions,\" in Proceedings of 2016 International Conference on Image Processing. IEEE, 2016, pp. 3753-3757.\n\nObjective perceptual assessment of video quality: Full reference television. Itu-T, Tutorial, ITU-T Telecommunication Standardization Bureau. ITU-T, \"Objective perceptual assessment of video quality: Full refer- ence television,\" Tutorial, ITU-T Telecommunication Standardization Bureau, 2004.\n\nSOS: The MOS is not enough. T Ho\u00dffeld, R Schatz, S Egger, Third International Workshop on Quality of Multimedia Experience. T. Ho\u00dffeld, R. Schatz, and S. Egger, \"SOS: The MOS is not enough!\" in Third International Workshop on Quality of Multimedia Experience, 2011, pp. 131-136.\n\nThe accuracy of subjects in a quality experiment: A theoretical subject model. L Janowski, M Pinson, IEEE Transactions on Multimedia. 1712L. Janowski and M. Pinson, \"The accuracy of subjects in a quality experiment: A theoretical subject model,\" IEEE Transactions on Mul- timedia, vol. 17, no. 12, pp. 2210-2224, 2015.\n\nCVD2014 a database for evaluating no-reference video quality assessment algorithms. M Nuutinen, T Virtanen, M Vaahteranoksa, T Vuori, P Oittinen, J H\u00e4kkinen, IEEE Transactions on Image Processing. 257M. Nuutinen, T. Virtanen, M. Vaahteranoksa, T. Vuori, P. Oittinen, and J. H\u00e4kkinen, \"CVD2014 a database for evaluating no-reference video quality assessment algorithms,\" IEEE Transactions on Image Processing, vol. 25, no. 7, pp. 3073-3086, 2016.\n\nSuitable methodology in subjective video quality assessment: A resolution dependent paradigm. S P\u00e9chard, R P\u00e9pion, P Le Callet, Proceedings of 2008 International Workshop on Image Media Quality and its Applications. 2008 International Workshop on Image Media Quality and its ApplicationsIMQA 2008S. P\u00e9chard, R. P\u00e9pion, and P. Le Callet, \"Suitable methodology in subjective video quality assessment: A resolution dependent paradigm,\" in Proceedings of 2008 International Workshop on Image Media Quality and its Applications (IMQA 2008), 2008.\n\nA H.264/AVC video database for the evaluation of quality metrics. F D Simone, M Tagliasacchi, M Naccari, S Tubaro, T Ebrahimi, 2010 IEEE International Conference on Acoustics, Speech and Signal Processing. F. D. Simone, M. Tagliasacchi, M. Naccari, S. Tubaro, and T. Ebrahimi, \"A H.264/AVC video database for the evaluation of quality metrics,\" in 2010 IEEE International Conference on Acoustics, Speech and Signal Processing, 2010, pp. 2430-2433.\n\nStudy of subjective and objective quality assessment of video. K Seshadrinathan, R Soundararajan, A C Bovik, L K Cormack, IEEE Transactions on Image Processing. 196K. Seshadrinathan, R. Soundararajan, A. C. Bovik, and L. K. Cormack, \"Study of subjective and objective quality assessment of video,\" IEEE Transactions on Image Processing, vol. 19, no. 6, pp. 1427-1441, 2010.\n\nVideo quality assessment on mobile devices: Subjective, behavioral and objective studies. A K Moorthy, L K Choi, A C Bovik, G De Veciana, IEEE Journal of Selected Topics in Signal Processing. 66A. K. Moorthy, L. K. Choi, A. C. Bovik, and G. de Veciana, \"Video quality assessment on mobile devices: Subjective, behavioral and ob- jective studies,\" IEEE Journal of Selected Topics in Signal Processing, vol. 6, no. 6, pp. 652-671, 2012.\n\nIVP Subjective Quality Video Database. F Zhang, S Li, L Ma, Y C Wong, K N Ngan, The Chinese University of Hong Kong. F. Zhang, S. Li, L. Ma, Y. C. Wong, and K. N. Ngan, \"IVP Subjective Quality Video Database,\" The Chinese University of Hong Kong, http: //ivp.ee.cuhk.edu.hk/research/database/subjective/, 2011.\n\nViS3: an algorithm for video quality assessment via analysis of spatial and spatiotemporal slices. P V Vu, D M Chandler, Journal of Electronic Imaging. 231P. V. Vu and D. M. Chandler, \"ViS3: an algorithm for video quality assessment via analysis of spatial and spatiotemporal slices,\" Journal of Electronic Imaging, vol. 23, no. 1, pp. 013 016-013 016, 2014.\n\nMCL-V: A streaming video quality assessment database. J Y Lin, R Song, C.-H Wu, T Liu, H Wang, C.-C J Kuo, Journal of Visual Communication and Image Representation. 30J. Y. Lin, R. Song, C.-H. Wu, T. Liu, H. Wang, and C.-C. J. Kuo, \"MCL- V: A streaming video quality assessment database,\" Journal of Visual Communication and Image Representation, vol. 30, pp. 1-9, 2015.\n\nEmpirical evaluation of no-reference VQA methods on a natural video quality database. H Men, H Lin, D Saupe, QoMEX 2017: International Conference on Quality of Multimedia Experience. H. Men, H. Lin, and D. Saupe, \"Empirical evaluation of no-reference VQA methods on a natural video quality database,\" in QoMEX 2017: International Conference on Quality of Multimedia Experience, 2017.\n\nCrowdsourcing subjective image quality evaluation. F Ribeiro, D Florencio, V Nascimento, Proceedings of 18th IEEE International Conference on Image Processing. 18th IEEE International Conference on Image ProcessingIEEEF. Ribeiro, D. Florencio, and V. Nascimento, \"Crowdsourcing subjective image quality evaluation,\" in Proceedings of 18th IEEE International Conference on Image Processing. IEEE, 2011, pp. 3097-3100.\n\nCrowdsourcing multimedia QoE evaluation: A trusted framework. C C Wu, K T Chen, Y C Chang, C L Lei, IEEE Transactions on Multimedia. 155C. C. Wu, K. T. Chen, Y. C. Chang, and C. L. Lei, \"Crowdsourcing multimedia QoE evaluation: A trusted framework,\" IEEE Transactions on Multimedia, vol. 15, no. 5, pp. 1121-1137, Aug 2013.\n\nCrowd workers proven useful: A comparative study of subjective video quality assessment. D Saupe, F Hahn, V Hosu, I Zingman, M Rana, S Li, QoMEX 2016: International Conference on Quality of Multimedia Experience. D. Saupe, F. Hahn, V. Hosu, I. Zingman, M. Rana, and S. Li, \"Crowd workers proven useful: A comparative study of subjective video quality assessment,\" in QoMEX 2016: International Conference on Quality of Multimedia Experience, 2016.\n\nBest practices and recommendations for crowdsourced QoE -Lessons learned from the Qualinet Task Force 'Crowdsourcing. T Ho\u00dffeld, M Hirth, J Redi, F Mazza, P Korshunov, B Naderi, M Seufert, B Gardlo, S Egger, C Keimel, COST Action IC1003 European Network on Quality of Experience in Multimedia Systems and Services (QUALINET). T. Ho\u00dffeld, M. Hirth, J. Redi, F. Mazza, P. Korshunov, B. Naderi, M. Seufert, B. Gardlo, S. Egger, and C. Keimel, \"Best practices and recommendations for crowdsourced QoE -Lessons learned from the Qualinet Task Force 'Crowdsourcing',\" COST Action IC1003 European Network on Quality of Experience in Multimedia Systems and Services (QUALINET), 2014.\n", "annotations": {"author": "[{\"end\":78,\"start\":73},{\"end\":84,\"start\":79},{\"end\":90,\"start\":85},{\"end\":97,\"start\":91},{\"end\":114,\"start\":98},{\"end\":124,\"start\":115},{\"end\":129,\"start\":125},{\"end\":134,\"start\":130},{\"end\":149,\"start\":135},{\"end\":165,\"start\":150},{\"end\":255,\"start\":166},{\"end\":346,\"start\":256},{\"end\":530,\"start\":347},{\"end\":620,\"start\":531},{\"end\":708,\"start\":621},{\"end\":806,\"start\":709},{\"end\":885,\"start\":807},{\"end\":979,\"start\":886}]", "publisher": "[{\"end\":53,\"start\":49},{\"end\":1210,\"start\":1206}]", "author_last_name": "[{\"end\":164,\"start\":159},{\"end\":175,\"start\":171},{\"end\":266,\"start\":262},{\"end\":363,\"start\":354},{\"end\":540,\"start\":537},{\"end\":628,\"start\":625},{\"end\":723,\"start\":715},{\"end\":816,\"start\":814},{\"end\":899,\"start\":894}]", "author_first_name": "[{\"end\":77,\"start\":73},{\"end\":83,\"start\":79},{\"end\":89,\"start\":85},{\"end\":96,\"start\":91},{\"end\":113,\"start\":104},{\"end\":123,\"start\":120},{\"end\":128,\"start\":125},{\"end\":133,\"start\":130},{\"end\":148,\"start\":140},{\"end\":158,\"start\":156},{\"end\":170,\"start\":166},{\"end\":261,\"start\":256},{\"end\":353,\"start\":347},{\"end\":536,\"start\":531},{\"end\":624,\"start\":621},{\"end\":714,\"start\":709},{\"end\":813,\"start\":807},{\"end\":893,\"start\":886}]", "author_affiliation": "[{\"end\":254,\"start\":177},{\"end\":345,\"start\":268},{\"end\":442,\"start\":365},{\"end\":529,\"start\":444},{\"end\":619,\"start\":542},{\"end\":707,\"start\":630},{\"end\":805,\"start\":725},{\"end\":884,\"start\":818},{\"end\":978,\"start\":901}]", "title": "[{\"end\":48,\"start\":1},{\"end\":1027,\"start\":980}]", "venue": "[{\"end\":1198,\"start\":1029}]", "abstract": "[{\"end\":2940,\"start\":1884}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3605,\"start\":3602},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4847,\"start\":4844},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5410,\"start\":5407},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5595,\"start\":5592},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6995,\"start\":6992},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7691,\"start\":7688},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8553,\"start\":8550},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8998,\"start\":8995},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9486,\"start\":9483},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9770,\"start\":9767},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10001,\"start\":9998},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10075,\"start\":10072},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13113,\"start\":13109},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14396,\"start\":14392},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18169,\"start\":18165},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21491,\"start\":21488},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21699,\"start\":21695},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22126,\"start\":22122},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22208,\"start\":22204},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22396,\"start\":22392},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22402,\"start\":22398},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22915,\"start\":22912},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22996,\"start\":22992},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23238,\"start\":23234},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23374,\"start\":23370},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23624,\"start\":23620},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23635,\"start\":23631},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23647,\"start\":23643},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25199,\"start\":25195},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25500,\"start\":25496},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25505,\"start\":25502},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25521,\"start\":25517},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25676,\"start\":25672},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26757,\"start\":26753},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":26763,\"start\":26759},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28249,\"start\":28245}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29327,\"start\":29221},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29624,\"start\":29328},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29732,\"start\":29625},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30065,\"start\":29733},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30370,\"start\":30066},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30658,\"start\":30371},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30906,\"start\":30659}]", "paragraph": "[{\"end\":3606,\"start\":2959},{\"end\":4423,\"start\":3608},{\"end\":4834,\"start\":4425},{\"end\":5411,\"start\":4836},{\"end\":5828,\"start\":5413},{\"end\":6469,\"start\":5830},{\"end\":6885,\"start\":6495},{\"end\":7015,\"start\":6909},{\"end\":7191,\"start\":7017},{\"end\":7651,\"start\":7193},{\"end\":8183,\"start\":7680},{\"end\":8442,\"start\":8185},{\"end\":8906,\"start\":8455},{\"end\":9374,\"start\":8908},{\"end\":9565,\"start\":9391},{\"end\":9847,\"start\":9593},{\"end\":10475,\"start\":9876},{\"end\":10714,\"start\":10492},{\"end\":11325,\"start\":10741},{\"end\":11625,\"start\":11362},{\"end\":12343,\"start\":11627},{\"end\":12580,\"start\":12345},{\"end\":12915,\"start\":12596},{\"end\":13373,\"start\":12917},{\"end\":13896,\"start\":13375},{\"end\":14319,\"start\":13898},{\"end\":14638,\"start\":14321},{\"end\":15166,\"start\":14640},{\"end\":15330,\"start\":15168},{\"end\":15987,\"start\":15332},{\"end\":16446,\"start\":15989},{\"end\":16878,\"start\":16448},{\"end\":17187,\"start\":16903},{\"end\":17639,\"start\":17189},{\"end\":17933,\"start\":17678},{\"end\":19023,\"start\":17958},{\"end\":19765,\"start\":19025},{\"end\":20416,\"start\":19767},{\"end\":21349,\"start\":20458},{\"end\":22209,\"start\":21351},{\"end\":22824,\"start\":22211},{\"end\":25200,\"start\":22845},{\"end\":25677,\"start\":25202},{\"end\":26894,\"start\":25726},{\"end\":27312,\"start\":26896},{\"end\":27972,\"start\":27314},{\"end\":28250,\"start\":27974},{\"end\":29220,\"start\":28252}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":11324,\"start\":11317},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":17156,\"start\":17148}]", "section_header": "[{\"end\":2957,\"start\":2942},{\"end\":6493,\"start\":6472},{\"end\":6907,\"start\":6888},{\"end\":7678,\"start\":7654},{\"end\":8453,\"start\":8445},{\"end\":9389,\"start\":9377},{\"end\":9591,\"start\":9568},{\"end\":9874,\"start\":9850},{\"end\":10490,\"start\":10478},{\"end\":10739,\"start\":10717},{\"end\":11360,\"start\":11328},{\"end\":12594,\"start\":12583},{\"end\":16901,\"start\":16881},{\"end\":17676,\"start\":17642},{\"end\":17956,\"start\":17936},{\"end\":20456,\"start\":20419},{\"end\":22843,\"start\":22827},{\"end\":25724,\"start\":25680},{\"end\":29230,\"start\":29222},{\"end\":29337,\"start\":29329},{\"end\":29634,\"start\":29626},{\"end\":29742,\"start\":29734},{\"end\":30075,\"start\":30067},{\"end\":30381,\"start\":30372},{\"end\":30670,\"start\":30660}]", "table": "[{\"end\":30658,\"start\":30433},{\"end\":30906,\"start\":30709}]", "figure_caption": "[{\"end\":29327,\"start\":29232},{\"end\":29624,\"start\":29339},{\"end\":29732,\"start\":29636},{\"end\":30065,\"start\":29744},{\"end\":30370,\"start\":30077},{\"end\":30433,\"start\":30383},{\"end\":30709,\"start\":30673}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12567,\"start\":12561},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14698,\"start\":14692},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16673,\"start\":16648},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20898,\"start\":20892},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22474,\"start\":22468}]", "bib_author_first_name": "[{\"end\":31095,\"start\":31091},{\"end\":31104,\"start\":31103},{\"end\":31356,\"start\":31355},{\"end\":31674,\"start\":31673},{\"end\":31688,\"start\":31687},{\"end\":31690,\"start\":31689},{\"end\":31975,\"start\":31974},{\"end\":31985,\"start\":31984},{\"end\":31987,\"start\":31986},{\"end\":31997,\"start\":31996},{\"end\":32010,\"start\":32009},{\"end\":32022,\"start\":32021},{\"end\":32028,\"start\":32027},{\"end\":32038,\"start\":32037},{\"end\":32050,\"start\":32046},{\"end\":32385,\"start\":32384},{\"end\":32387,\"start\":32386},{\"end\":32399,\"start\":32398},{\"end\":32401,\"start\":32400},{\"end\":32700,\"start\":32699},{\"end\":32710,\"start\":32709},{\"end\":32712,\"start\":32711},{\"end\":32997,\"start\":32996},{\"end\":33454,\"start\":33453},{\"end\":33464,\"start\":33463},{\"end\":33481,\"start\":33480},{\"end\":33483,\"start\":33482},{\"end\":33695,\"start\":33694},{\"end\":33703,\"start\":33702},{\"end\":33711,\"start\":33710},{\"end\":33724,\"start\":33723},{\"end\":33731,\"start\":33730},{\"end\":33738,\"start\":33737},{\"end\":33750,\"start\":33749},{\"end\":33756,\"start\":33755},{\"end\":34073,\"start\":34072},{\"end\":34086,\"start\":34085},{\"end\":34101,\"start\":34100},{\"end\":34795,\"start\":34794},{\"end\":34806,\"start\":34805},{\"end\":34816,\"start\":34815},{\"end\":35126,\"start\":35125},{\"end\":35138,\"start\":35137},{\"end\":35451,\"start\":35450},{\"end\":35463,\"start\":35462},{\"end\":35475,\"start\":35474},{\"end\":35492,\"start\":35491},{\"end\":35501,\"start\":35500},{\"end\":35513,\"start\":35512},{\"end\":35908,\"start\":35907},{\"end\":35919,\"start\":35918},{\"end\":35929,\"start\":35928},{\"end\":35932,\"start\":35930},{\"end\":36423,\"start\":36422},{\"end\":36425,\"start\":36424},{\"end\":36435,\"start\":36434},{\"end\":36451,\"start\":36450},{\"end\":36462,\"start\":36461},{\"end\":36472,\"start\":36471},{\"end\":36869,\"start\":36868},{\"end\":36887,\"start\":36886},{\"end\":36904,\"start\":36903},{\"end\":36906,\"start\":36905},{\"end\":36915,\"start\":36914},{\"end\":36917,\"start\":36916},{\"end\":37271,\"start\":37270},{\"end\":37273,\"start\":37272},{\"end\":37284,\"start\":37283},{\"end\":37286,\"start\":37285},{\"end\":37294,\"start\":37293},{\"end\":37296,\"start\":37295},{\"end\":37305,\"start\":37304},{\"end\":37656,\"start\":37655},{\"end\":37665,\"start\":37664},{\"end\":37671,\"start\":37670},{\"end\":37677,\"start\":37676},{\"end\":37679,\"start\":37678},{\"end\":37687,\"start\":37686},{\"end\":37689,\"start\":37688},{\"end\":38028,\"start\":38027},{\"end\":38030,\"start\":38029},{\"end\":38036,\"start\":38035},{\"end\":38038,\"start\":38037},{\"end\":38343,\"start\":38342},{\"end\":38345,\"start\":38344},{\"end\":38352,\"start\":38351},{\"end\":38363,\"start\":38359},{\"end\":38369,\"start\":38368},{\"end\":38376,\"start\":38375},{\"end\":38387,\"start\":38383},{\"end\":38389,\"start\":38388},{\"end\":38747,\"start\":38746},{\"end\":38754,\"start\":38753},{\"end\":38761,\"start\":38760},{\"end\":39097,\"start\":39096},{\"end\":39108,\"start\":39107},{\"end\":39121,\"start\":39120},{\"end\":39526,\"start\":39525},{\"end\":39528,\"start\":39527},{\"end\":39534,\"start\":39533},{\"end\":39536,\"start\":39535},{\"end\":39544,\"start\":39543},{\"end\":39546,\"start\":39545},{\"end\":39555,\"start\":39554},{\"end\":39557,\"start\":39556},{\"end\":39878,\"start\":39877},{\"end\":39887,\"start\":39886},{\"end\":39895,\"start\":39894},{\"end\":39903,\"start\":39902},{\"end\":39914,\"start\":39913},{\"end\":39922,\"start\":39921},{\"end\":40355,\"start\":40354},{\"end\":40366,\"start\":40365},{\"end\":40375,\"start\":40374},{\"end\":40383,\"start\":40382},{\"end\":40392,\"start\":40391},{\"end\":40405,\"start\":40404},{\"end\":40415,\"start\":40414},{\"end\":40426,\"start\":40425},{\"end\":40436,\"start\":40435},{\"end\":40445,\"start\":40444}]", "bib_author_last_name": "[{\"end\":31101,\"start\":31096},{\"end\":31110,\"start\":31105},{\"end\":31364,\"start\":31357},{\"end\":31685,\"start\":31675},{\"end\":31696,\"start\":31691},{\"end\":31982,\"start\":31976},{\"end\":31994,\"start\":31988},{\"end\":32007,\"start\":31998},{\"end\":32019,\"start\":32011},{\"end\":32025,\"start\":32023},{\"end\":32035,\"start\":32029},{\"end\":32044,\"start\":32039},{\"end\":32053,\"start\":32051},{\"end\":32396,\"start\":32388},{\"end\":32407,\"start\":32402},{\"end\":32707,\"start\":32701},{\"end\":32723,\"start\":32713},{\"end\":33002,\"start\":32998},{\"end\":33254,\"start\":33249},{\"end\":33461,\"start\":33455},{\"end\":33478,\"start\":33465},{\"end\":33489,\"start\":33484},{\"end\":33700,\"start\":33696},{\"end\":33708,\"start\":33704},{\"end\":33721,\"start\":33712},{\"end\":33728,\"start\":33725},{\"end\":33735,\"start\":33732},{\"end\":33747,\"start\":33739},{\"end\":33753,\"start\":33751},{\"end\":33762,\"start\":33757},{\"end\":34083,\"start\":34074},{\"end\":34098,\"start\":34087},{\"end\":34109,\"start\":34102},{\"end\":34553,\"start\":34548},{\"end\":34803,\"start\":34796},{\"end\":34813,\"start\":34807},{\"end\":34822,\"start\":34817},{\"end\":35135,\"start\":35127},{\"end\":35145,\"start\":35139},{\"end\":35460,\"start\":35452},{\"end\":35472,\"start\":35464},{\"end\":35489,\"start\":35476},{\"end\":35498,\"start\":35493},{\"end\":35510,\"start\":35502},{\"end\":35522,\"start\":35514},{\"end\":35916,\"start\":35909},{\"end\":35926,\"start\":35920},{\"end\":35939,\"start\":35933},{\"end\":36432,\"start\":36426},{\"end\":36448,\"start\":36436},{\"end\":36459,\"start\":36452},{\"end\":36469,\"start\":36463},{\"end\":36481,\"start\":36473},{\"end\":36884,\"start\":36870},{\"end\":36901,\"start\":36888},{\"end\":36912,\"start\":36907},{\"end\":36925,\"start\":36918},{\"end\":37281,\"start\":37274},{\"end\":37291,\"start\":37287},{\"end\":37302,\"start\":37297},{\"end\":37316,\"start\":37306},{\"end\":37662,\"start\":37657},{\"end\":37668,\"start\":37666},{\"end\":37674,\"start\":37672},{\"end\":37684,\"start\":37680},{\"end\":37694,\"start\":37690},{\"end\":38033,\"start\":38031},{\"end\":38047,\"start\":38039},{\"end\":38349,\"start\":38346},{\"end\":38357,\"start\":38353},{\"end\":38366,\"start\":38364},{\"end\":38373,\"start\":38370},{\"end\":38381,\"start\":38377},{\"end\":38393,\"start\":38390},{\"end\":38751,\"start\":38748},{\"end\":38758,\"start\":38755},{\"end\":38767,\"start\":38762},{\"end\":39105,\"start\":39098},{\"end\":39118,\"start\":39109},{\"end\":39132,\"start\":39122},{\"end\":39531,\"start\":39529},{\"end\":39541,\"start\":39537},{\"end\":39552,\"start\":39547},{\"end\":39561,\"start\":39558},{\"end\":39884,\"start\":39879},{\"end\":39892,\"start\":39888},{\"end\":39900,\"start\":39896},{\"end\":39911,\"start\":39904},{\"end\":39919,\"start\":39915},{\"end\":39925,\"start\":39923},{\"end\":40363,\"start\":40356},{\"end\":40372,\"start\":40367},{\"end\":40380,\"start\":40376},{\"end\":40389,\"start\":40384},{\"end\":40402,\"start\":40393},{\"end\":40412,\"start\":40406},{\"end\":40423,\"start\":40416},{\"end\":40433,\"start\":40427},{\"end\":40442,\"start\":40437},{\"end\":40452,\"start\":40446}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1334541},\"end\":31284,\"start\":31032},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":11352985},\"end\":31592,\"start\":31286},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8531217},\"end\":31925,\"start\":31594},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":207230134},\"end\":32287,\"start\":31927},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":7555800},\"end\":32655,\"start\":32289},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":7055315},\"end\":32966,\"start\":32657},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1434077},\"end\":33174,\"start\":32968},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":30891831},\"end\":33399,\"start\":33176},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":16892725},\"end\":33692,\"start\":33401},{\"attributes\":{\"id\":\"b9\"},\"end\":33976,\"start\":33694},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":16530705},\"end\":34469,\"start\":33978},{\"attributes\":{\"id\":\"b11\"},\"end\":34764,\"start\":34471},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":11598691},\"end\":35044,\"start\":34766},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":22343847},\"end\":35364,\"start\":35046},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":14124129},\"end\":35811,\"start\":35366},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":13271168},\"end\":36354,\"start\":35813},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":17728147},\"end\":36803,\"start\":36356},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":206724285},\"end\":37178,\"start\":36805},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14691109},\"end\":37614,\"start\":37180},{\"attributes\":{\"id\":\"b19\"},\"end\":37926,\"start\":37616},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6066775},\"end\":38286,\"start\":37928},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":16474148},\"end\":38658,\"start\":38288},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":9095954},\"end\":39043,\"start\":38660},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6362262},\"end\":39461,\"start\":39045},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":15610216},\"end\":39786,\"start\":39463},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":15139961},\"end\":40234,\"start\":39788},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":2034507},\"end\":40910,\"start\":40236}]", "bib_title": "[{\"end\":31089,\"start\":31032},{\"end\":31353,\"start\":31286},{\"end\":31671,\"start\":31594},{\"end\":31972,\"start\":31927},{\"end\":32382,\"start\":32289},{\"end\":32697,\"start\":32657},{\"end\":32994,\"start\":32968},{\"end\":33247,\"start\":33176},{\"end\":33451,\"start\":33401},{\"end\":34070,\"start\":33978},{\"end\":34546,\"start\":34471},{\"end\":34792,\"start\":34766},{\"end\":35123,\"start\":35046},{\"end\":35448,\"start\":35366},{\"end\":35905,\"start\":35813},{\"end\":36420,\"start\":36356},{\"end\":36866,\"start\":36805},{\"end\":37268,\"start\":37180},{\"end\":37653,\"start\":37616},{\"end\":38025,\"start\":37928},{\"end\":38340,\"start\":38288},{\"end\":38744,\"start\":38660},{\"end\":39094,\"start\":39045},{\"end\":39523,\"start\":39463},{\"end\":39875,\"start\":39788},{\"end\":40352,\"start\":40236}]", "bib_author": "[{\"end\":31103,\"start\":31091},{\"end\":31112,\"start\":31103},{\"end\":31366,\"start\":31355},{\"end\":31687,\"start\":31673},{\"end\":31698,\"start\":31687},{\"end\":31984,\"start\":31974},{\"end\":31996,\"start\":31984},{\"end\":32009,\"start\":31996},{\"end\":32021,\"start\":32009},{\"end\":32027,\"start\":32021},{\"end\":32037,\"start\":32027},{\"end\":32046,\"start\":32037},{\"end\":32055,\"start\":32046},{\"end\":32398,\"start\":32384},{\"end\":32409,\"start\":32398},{\"end\":32709,\"start\":32699},{\"end\":32725,\"start\":32709},{\"end\":33004,\"start\":32996},{\"end\":33256,\"start\":33249},{\"end\":33463,\"start\":33453},{\"end\":33480,\"start\":33463},{\"end\":33491,\"start\":33480},{\"end\":33702,\"start\":33694},{\"end\":33710,\"start\":33702},{\"end\":33723,\"start\":33710},{\"end\":33730,\"start\":33723},{\"end\":33737,\"start\":33730},{\"end\":33749,\"start\":33737},{\"end\":33755,\"start\":33749},{\"end\":33764,\"start\":33755},{\"end\":34085,\"start\":34072},{\"end\":34100,\"start\":34085},{\"end\":34111,\"start\":34100},{\"end\":34555,\"start\":34548},{\"end\":34805,\"start\":34794},{\"end\":34815,\"start\":34805},{\"end\":34824,\"start\":34815},{\"end\":35137,\"start\":35125},{\"end\":35147,\"start\":35137},{\"end\":35462,\"start\":35450},{\"end\":35474,\"start\":35462},{\"end\":35491,\"start\":35474},{\"end\":35500,\"start\":35491},{\"end\":35512,\"start\":35500},{\"end\":35524,\"start\":35512},{\"end\":35918,\"start\":35907},{\"end\":35928,\"start\":35918},{\"end\":35941,\"start\":35928},{\"end\":36434,\"start\":36422},{\"end\":36450,\"start\":36434},{\"end\":36461,\"start\":36450},{\"end\":36471,\"start\":36461},{\"end\":36483,\"start\":36471},{\"end\":36886,\"start\":36868},{\"end\":36903,\"start\":36886},{\"end\":36914,\"start\":36903},{\"end\":36927,\"start\":36914},{\"end\":37283,\"start\":37270},{\"end\":37293,\"start\":37283},{\"end\":37304,\"start\":37293},{\"end\":37318,\"start\":37304},{\"end\":37664,\"start\":37655},{\"end\":37670,\"start\":37664},{\"end\":37676,\"start\":37670},{\"end\":37686,\"start\":37676},{\"end\":37696,\"start\":37686},{\"end\":38035,\"start\":38027},{\"end\":38049,\"start\":38035},{\"end\":38351,\"start\":38342},{\"end\":38359,\"start\":38351},{\"end\":38368,\"start\":38359},{\"end\":38375,\"start\":38368},{\"end\":38383,\"start\":38375},{\"end\":38395,\"start\":38383},{\"end\":38753,\"start\":38746},{\"end\":38760,\"start\":38753},{\"end\":38769,\"start\":38760},{\"end\":39107,\"start\":39096},{\"end\":39120,\"start\":39107},{\"end\":39134,\"start\":39120},{\"end\":39533,\"start\":39525},{\"end\":39543,\"start\":39533},{\"end\":39554,\"start\":39543},{\"end\":39563,\"start\":39554},{\"end\":39886,\"start\":39877},{\"end\":39894,\"start\":39886},{\"end\":39902,\"start\":39894},{\"end\":39913,\"start\":39902},{\"end\":39921,\"start\":39913},{\"end\":39927,\"start\":39921},{\"end\":40365,\"start\":40354},{\"end\":40374,\"start\":40365},{\"end\":40382,\"start\":40374},{\"end\":40391,\"start\":40382},{\"end\":40404,\"start\":40391},{\"end\":40414,\"start\":40404},{\"end\":40425,\"start\":40414},{\"end\":40435,\"start\":40425},{\"end\":40444,\"start\":40435},{\"end\":40454,\"start\":40444}]", "bib_venue": "[{\"end\":31142,\"start\":31131},{\"end\":34226,\"start\":34177},{\"end\":36100,\"start\":36029},{\"end\":39259,\"start\":39205},{\"end\":31129,\"start\":31112},{\"end\":31418,\"start\":31366},{\"end\":31735,\"start\":31698},{\"end\":32080,\"start\":32055},{\"end\":32446,\"start\":32409},{\"end\":32796,\"start\":32725},{\"end\":33048,\"start\":33004},{\"end\":33282,\"start\":33256},{\"end\":33521,\"start\":33491},{\"end\":33809,\"start\":33764},{\"end\":34175,\"start\":34111},{\"end\":34611,\"start\":34555},{\"end\":34888,\"start\":34824},{\"end\":35178,\"start\":35147},{\"end\":35561,\"start\":35524},{\"end\":36027,\"start\":35941},{\"end\":36560,\"start\":36483},{\"end\":36964,\"start\":36927},{\"end\":37370,\"start\":37318},{\"end\":37731,\"start\":37696},{\"end\":38078,\"start\":38049},{\"end\":38451,\"start\":38395},{\"end\":38841,\"start\":38769},{\"end\":39203,\"start\":39134},{\"end\":39594,\"start\":39563},{\"end\":39999,\"start\":39927},{\"end\":40560,\"start\":40454}]"}}}, "year": 2023, "month": 12, "day": 17}