{"id": 232307359, "updated": "2023-10-06 05:12:15.473", "metadata": {"title": "Language-Agnostic Representation Learning of Source Code from Structure and Context", "authors": "[{\"first\":\"Daniel\",\"last\":\"Zugner\",\"middle\":[]},{\"first\":\"Tobias\",\"last\":\"Kirschstein\",\"middle\":[]},{\"first\":\"Michele\",\"last\":\"Catasta\",\"middle\":[]},{\"first\":\"Jure\",\"last\":\"Leskovec\",\"middle\":[]},{\"first\":\"Stephan\",\"last\":\"Gunnemann\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": 3, "day": 21}, "abstract": "Source code (Context) and its parsed abstract syntax tree (AST; Structure) are two complementary representations of the same computer program. Traditionally, designers of machine learning models have relied predominantly either on Structure or Context. We propose a new model, which jointly learns on Context and Structure of source code. In contrast to previous approaches, our model uses only language-agnostic features, i.e., source code and features that can be computed directly from the AST. Besides obtaining state-of-the-art on monolingual code summarization on all five programming languages considered in this work, we propose the first multilingual code summarization model. We show that jointly training on non-parallel data from multiple programming languages improves results on all individual languages, where the strongest gains are on low-resource languages. Remarkably, multilingual training only from Context does not lead to the same improvements, highlighting the benefits of combining Structure and Context for representation learning on code.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2103.11318", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/ZugnerKCLG21", "doi": null}}, "content": {"source": {"pdf_hash": "05e396e79a2f88f0b8f8d99f5ab36ab3efa95c14", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2103.11318v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "44e2ab22d6076fc69c3d5d7ef45ed81088f7cafb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/05e396e79a2f88f0b8f8d99f5ab36ab3efa95c14.txt", "contents": "\nLANGUAGE-AGNOSTIC REPRESENTATION LEARNING OF SOURCE CODE FROM STRUCTURE AND CONTEXT\n\n\nDaniel Z\u00fcgner zuegnerd@in.tum.de \nTechnical University of Munich\nStanford University\nStanford University\nTechnical University of Munich\n\n\nTobias Kirschstein \nTechnical University of Munich\nStanford University\nStanford University\nTechnical University of Munich\n\n\nMichele Catasta \nTechnical University of Munich\nStanford University\nStanford University\nTechnical University of Munich\n\n\nJure Leskovec \nTechnical University of Munich\nStanford University\nStanford University\nTechnical University of Munich\n\n\nStephan G\u00fcnnemann guennemann@in.tum.de \nTechnical University of Munich\nStanford University\nStanford University\nTechnical University of Munich\n\n\nLANGUAGE-AGNOSTIC REPRESENTATION LEARNING OF SOURCE CODE FROM STRUCTURE AND CONTEXT\nPublished as a conference paper at ICLR 2021\nSource code (Context) and its parsed abstract syntax tree (AST; Structure) are two complementary representations of the same computer program. Traditionally, designers of machine learning models have relied predominantly either on Structure or Context. We propose a new model, which jointly learns on Context and Structure of source code. In contrast to previous approaches, our model uses only language-agnostic features, i.e., source code and features that can be computed directly from the AST. Besides obtaining state-of-the-art on monolingual code summarization on all five programming languages considered in this work, we propose the first multilingual code summarization model. We show that jointly training on non-parallel data from multiple programming languages improves results on all individual languages, where the strongest gains are on low-resource languages. Remarkably, multilingual training only from Context does not lead to the same improvements, highlighting the benefits of combining Structure and Context for representation learning on code.\n\nINTRODUCTION\n\nMachine learning for code is an active and growing area of research which aims at building models that can learn semantically meaningful representations of programs. These embeddings can be used on downstream tasks, such as code generation, bug detection, or code summarization. We focus our work on two complementary data representations of programs: the source code (referred to as Context in this work), and the abstract syntax tree (AST; referred to as Structure). Traditionally, researchers and practitioners have decided to predominantly leverage either Structure or Context in their machine learning models. In this work, we show that jointly learning on Context and Structure improves representation learning on source code (see Fig. 1).\n\nThe source code representation naturally lends itself to models from natural language processing (NLP), e.g., long short-term memory networks (Hochreiter & Schmidhuber, 1997) (LSTM) or Transformers (Vaswani et al., 2017;Radford et al., 2019;Shaw et al., 2018). On the other hand, models leveraging the structure representations are typically based on graph neural networks (GNNs) (Kipf & Welling, 2017;Xu et al., 2019;Veli\u010dkovi\u0107 et al., 2018;You et al., 2019;Hamilton et al., 2017;Li et al., 2015;Klicpera et al., 2020). While the AST representation makes the highly structured nature of source code explicit to the models, since most GNNs use the message-passing framework, their learned representations are inherently local and struggle to leverage long-range interactions.\n\nRecently, Hellendoorn et al. (2020) have explored models that can leverage several representations, including both Structure and Context. Their Graph Relational Embedding Attention Transformer (GREAT) extends Shaw et al. (2018), which biases the self-attention computation in a localized way given the underlying graph. The language-specific representations used by GREAT include a combination of the data flow graph, control flow graph, syntactic edges (inspired by Allamanis et al. (2018)), etc. which require specialized pipelines and static analysis tools to be obtained. Figure 1: Context and Structure both encapsulate valuable information about source code. In this realistic example, token 1 and 4 are distant in the sequence of tokens (Context), but only 5 hops away when traversing the Abstract Syntax Tree (Structure). As such, a method that relies only on the sequence of tokens could neglect the relationship between a method name and its return variable. Conversely, token 1 and 2 showcase the opposite setting. Hence, unifying Structure and Context leads to a more powerful representation of source code.\n\nWe propose the CODE TRANSFORMER 1 , which combines distances computed on Structure and Context in the self-attention operation. In contrast to the localized treatment via edges described above, we make the full Structure accessible to the model at each layer by computing pairwise distances on the AST, such as shortest path lengths. To this end, we draw inspiration from the XLNet architecture , which uses relative distances instead of absolute positions in the attention computation. Importantly, all our features are language-agnostic 2 , i.e., can easily be computed for any programming language based on the source code and AST.\n\nWe use two datasets comprising 5 different programming languages in total, and evaluate the representations learned by our model on the task of code summarization, where the model predicts a method's name based on its body. Besides setting the state-of-the-art on all five languages for singlelanguage training, we also train the first multilingual model for code summarization. This is enabled by the fact that our model uses only language-agnostic features that can easily be obtained for any programming language. Remarkably, training our model on multiple programming languages substantially improves the performance on all languages. Moreover, multilingual training only from Context does not lead to the same improvements, highlighting the benefits of combining Structure and Context for representation learning on code.\n\n\nRELATED WORK\n\nMachine Learning for Code. Early research learned language models on raw text data, e.g., Raychev et al., 2014;Dam et al., 2016), providing evidence for the naturalness assumption (Hindle et al., 2012). For example, Allamanis et al. (2015) learned distributed representations of variables and methods, finding that they were indeed able to encode common semantic properties from the regularities present in source code. Alon et al. (2019b) also found evidence of semantic arithmetic in their embedding space, dubbed code2vec. These representations-and their variants like (Mou et al., 2016)-can then be used to predict sequences of identifier sub-tokens (Allamanis et al., 2015) or API calls (Acharya et al., 2007;Nguyen et al., 2017). They can be used as advanced auto-completion tools (Hindle et al., 2012;Bhoopchand et al., 2016), including for user-provided tokens like Variable Names (Raychev et al., 2014;Allamanis et al., 2014). These are useful for deobfuscating Android applications (Bichsel et al., 2016) for example.\n\nSeveral works leverage structured graphical models for probabilistic models of source code, usually through parse trees (Maddison & Tarlow, 2014;Bielik et al., 2016). Unlike previous works where hand-crafted features were used as node features (Raychev et al., 2014) or as explicit semantic edges (Allamanis et al., 2018), our work does not augment the existing syntactic relationships between the different elements to enhance the predictive capabilities of the model. Other approaches (Alon et al., 2018;Li et al., 2017) also leverage the AST structure, but linearize the graph by first traversing it.\n\nLearning representations of structured languages. While models of language have dramatically improved in their ability to learn structure (syntax) and semantics from scratch, it can be argued that directly providing the model with the underlying structure of the language can help with generalization (Battaglia et al., 2018), managing long-ranging dependencies (Tai et al., 2015), or representing the compositional aspect of natural language (Socher et al., 2013). Notably, tree structures have shown promising results and inspired new architectures (Shen et al., 2019), including in the domain of source code (Fernandes et al., 2019), where the underlying syntax is directly available. Our work pursues this line of research, showing the benefits of explicitly integrating structural information as an inductive bias. Shiv & Quirk (2019) propose positional encodings for nodes on trees; however, their approach assumes regular trees, which is an unrealistic assumption when working with Abstract Syntax Trees, as an AST node can have arbitrarily many children, e.g., the arguments of a function.\n\nGraph Neural Networks. GNNs provide a powerful tool for machine learning on graphs, thanks to their ability to recursively incorporate information from neighboring nodes in the network (Battaglia et al., 2018), naturally capturing the graph structure simultaneously with the nodes' features. (Gori et al., 2005;Scarselli et al., 2008) are able to learn vector representations of nodes and graphs in an end-to-end fashion, encoding structural and feature information in the embedding space. Under this model, GNNs have achieved state-of-the-art performance across a variety of tasks, such as node classification (Kipf & Welling, 2017;Hamilton et al., 2017;Klicpera et al., 2019a), link prediction (Zhang & Chen, 2018;Schlichtkrull et al., 2018), graph clustering (Defferrard et al., 2016;Ying et al., 2018) or graph classification (Ying et al., 2018;Dai et al., 2016;Duvenaud et al., 2015).\n\n\nINTEGRATING STRUCTURE AND CONTEXT IN THE CODE TRANSFORMER\n\nSelf-attention is the core operation powering the Transformer. It enables the model to selectively focus on relevant parts of the input. The matrix form equation for attention with a single head is\nAttention(Q, K, V ) = softmax QK T \u221a d k V ,(1)\nwhere Q, K \u2208 R N \u00d7d k and V \u2208 R N \u00d7dv . N is the number of input tokens, d k the key dimension, and d v the value dimension (typically we have d k = d v ). The attention score of query Q i and key K j before softmax is\nA ij = Q T i K j = E T i W T q W k E j ,(2)\nwhere E i , E j \u2208 R d are the d-dimensional embeddings of tokens i and j, and W q , W k \u2208 R d k \u00d7d are the query and key projection matrices, respectively.\n\nObserve that Eq.\n\n(2) contains no assumption about potential structure in the input domain: in the attention operation we compute all dot products of query and key vectors equally, effectively viewing them as unordered sets of vectors. This means, however, that the model is oblivious to structured inputs (such as text or graphs) and therefore is unable to distinguish, for example, a variable name occurring as an argument and in the return statement of a method.\n\nIn NLP, it is common to bias Transformers towards sequential inputs by adding positional encodings to the token embeddings. These positional encodings are obtained by applying an encoding function \u03c6 : R \u2192 R d to each token's position p i . These positional encodings make the information about the sequence of tokens available to the model. Eq. (2) becomes:\nA ij = (E i + \u03c6(p i )) T W T q W k (E j + \u03c6(p j )), ,(3)\nwhich factorizes into\nA ij = E T i W T q W k E j (a) A cc ij + E T i W T q W k \u03c6(p j ) (b) A cp ij + \u03c6(p i ) T W T q W k E j (c) A pc ij + \u03c6(p i ) T W T q W k \u03c6(p j ) (d) A pp ij .(4)\nWe can interpret the terms (a)-(d) as follows. (a) A cc ij is the contribution from the 'match' between the content embeddings of tokens i and j; (b) A cp ij steers the attention towards certain positions based on the content of token i; (c) A pc ij biases towards content embeddings based on the position of token i; (d) A pp ij controls which positions should attend to which other positions. In our model, we adopt the formulation of ; . They modify Eq. (4) by replacing the absolute position encodings \u03c6(p i ) with relative position encodings \u03c6(r i\u2192j ):\nA rel ij = E T i W T q W k E j + E T i W T q W r \u03c6(r i\u2192j ) + u T W k E j + v T W r \u03c6(r i\u2192j ),(5)\nwhere r i\u2192j is the relative distance from token i to token j in the sequence, u, v \u2208 R d k are learnable bias vectors, and W r is a key projection matrix for the relative distances. Besides fixing issues with absolute position encodings such as ambiguity when processing two sentences at a time, Eq. (5) enables native application of the powerful self-attention operation on domains such as graphs, where absolute coordinates are not available. We adopt the (non-trainable) sinusoidal encoding function proposed by (Vaswani et al., 2017) for all relations; see Appendix A.1 for details on the distance encoding function.\n\n\nINTEGRATING SOURCE CODE AND AST REPRESENTATIONS OF PROGRAMS.\n\nTo enable the model to integrate information both the Context and Structure of programs, we modify Eq. (5) to be able to incorporate multiple different relations. To this end, we use one key projection matrix W (s) r per relation s, and sum their contributions in the raw attention score. This enables the CODE TRANSFORMER to combine information from multiple relations between tokens in the attention computation. Besides the token distance in the Context, we include pairwise relations based on the AST as described in the following. See Fig. 2 for a visualization of the Structure distances we use. Shortest path length. We include the number of hops required to reach node j starting from node i and vice versa. Here, we treat the AST as an undirected graph, since otherwise most distances would be undefined: e.g., all other nodes in the AST would be unreachable from the leaves.\n\nSimilar to the distance of two tokens on the source code sequence, the shortestpath length is a global distance. This makes the whole graph structure accessible to the model at each layer. In contrast, Hellendoorn et al. (2020) add bias terms to the attention computation only for edges (i.e. shortest-path distance of 1), which is a local operation that only exchanges information between immediate neighbors (similar to message passing in GNNs). The equivalent localized operation on the source code sequence would be to treat the sequence as a chain graph and only compute attention terms for neighboring tokens, which in turn highlights the benefit of non-local attention operations.\n\nAncestor distance. Since we treat the ASTs as undirected for the computation of the shortest-path length, we lose the direction information of the edges. To avoid this, we also include the distance on the ordered set of ancestors and descendants of a node in the AST (red arrow in Fig. 2). Again, we include number of (vertical) hops to avoid locality in the attention computation. For example, a node r i\u2192j = 2 for \"grand-children\" j of i, and r j\u2192i = \u22122 in the other direction.\n\nSibling distance. The neighbor sets in graphs are typically considered to be unordered, but in an AST, the order of children encodes their order of occurrence in the source code. To avoid information loss when encoding the AST, we further include the distance on the ordered set of siblings {v i } of a node, where we again avoid locality by encoding the number of hops, i.e. r v1\u2192v3 = 2 and r v3\u2192v1 = \u22122.\n\nPersonalized PageRank (Page et al., 1999) (PPR). PPR is a well-studied proximity measure which has been shown to be very effective in learning with graphs (Klicpera et al., 2019a;b;Bojchevski et al., 2020). PPR captures the local graph structure around a pair of nodes (i, j  Center: The CODE TRANSFORMER jointly leverages the sequence of tokens and the Abstract Syntax Tree to learn expressive representations of source code. In addition to the input token and node embeddings the model uses different distances between the tokens, e.g., shortest paths on the AST or personalized PageRank, to reason about their relative positions. The output embeddings can be used for downstream tasks such as code summarization (right).\n\nmany neighbors, its PPR score for j will be low even when they are only few hops apart, which complements the purely hop-based distances described above.\n\nInput embeddings to the model. To combine the Context and Structure information, we assign each token in the sequence to an AST node by selecting the AST node whose range in the source code is the shortest one containing the token. We concatenate the (sub-) token embeddings with the embedding of the token's assigned AST node type as well as the token type returned by the tokenizer. That is, among all the internal nodes, we use as input only those corresponding to a token in the sequence; however, the remaining internal nodes can used by the model since their presence affects the distances between the remaining AST nodes. See Appendices A.3 and A.4 for details.\n\n\nEFFICIENT RELATIVE ATTENTION COMPUTATION.\n\nNa\u00efvely, we need to compute and materialize a tensor of dimension N \u00d7 N \u00d7 d to hold all pairwise relative position encodings \u03c6(r i\u2192j ) in Eq. (5) , where N is the input length. This is prohibitive for fast GPU training. While for discrete distance values (e.g., sequence distance or shortest-path length on a graph) we only need to compute unique distance values occurring in the input, this does not generalize to continuous distances such as PPR. Therefore, we propose a constant-time approximation of the relational attention computation by grouping the values into k N 2 bins. Since closer samples are typically more relevant for a query sample, we increase the bin widths exponentially with growing distance values. Throughout our experiments we have found the CODE TRANSFORMER to be relatively insensitive to the number of bins; we thus set k = 32 in our experiments.\n\n\nEXPERIMENTAL SETUP\n\nCode summarization is one of the most popular tasks in machine learning for code. Given the body of a function, the task is to predict the function's name. As observed by Alon et al. (2019b) and Allamanis et al. (2016), this is a useful benchmark as method names in open-source projects tend to be precise and descriptive, and functions typically form complete logical units. See Fig. 3 (right) for a visual overview of the task. We use two complementary representations of programs: the source code as a sequence of tokens (Context) and the AST (Structure). As shown in Fig. 3 (left), tokens that are far away on the sequence may be very close on the AST and vice versa. In this task we make use of the CODE TRANSFORMER's ability jointly leverage both Structure and Context and show that it improves learning. Further, we show the benefit of using only language-agnostic features in our model by training the first multilingual model for code summarization.\n\nDatasets. To highlight the benefit of only relying on language-agnostic representations such as source code and abstract syntax trees, we evaluate on challenging datasets in four programming languages introduced in the CodeSearchNet (CSN) Challenge (Husain et al., 2019): Python, Javascript, Go, and Ruby. Similar to Java-small, the datasets from CodeSearchNet have been carefully deduplicated by the creators to avoid data leakage from the training set, e.g., via copy-and-paste code.  We further evaluate on Java-small (Allamanis et al., 2016), a popular and challenging code summarization dataset.\n\nIt contains 11 open-source Java projects. We use the split as in Alon et al. (2019a), where 9 of these projects are used for training, one for validation, and one for test. The dataset contains roughly 700K samples (function definitions). Moreover, we also experiment with pre-training our model on Java-medium and Java-large (Alon et al., 2019a) before fine-tuning on Java-small, making sure to avoid leakage by removing the test and validation projects of Java-small from the pre-training dataset. See Table 1 for a summary of the datasets we use in this work.\n\nPreprocessing. Each token of the source code is split into subtokens respective to code naming conventions, i.e., get TrainingData is converted to [get, training, data]. Following Alon et al. (2019a) we use at most six subtokens for the method names, truncating longer function names if necessary. In addition to the tokenized source code we produce an AST for each method using the open-source AST parser Semantic 3 . We limit the vocabulary to subtokens with at least 100 occurrences in the training set, and only consider snippets with 512 or fewer tokens (after removing punctuation). We refer the reader to the appendix for further details on the data preprocessing.\n\nPointer network. We add a pointer network (Vinyals et al., 2015) (as described in Fernandes et al. (2019)) to the decoders of all Transformer-based models. This enables them to enhance their predictions by pointing at positions in the input sequence. For instance, when predicting the method name get url, the model can point directly to occurrences of the variable url. This often improves results for less frequent tokens, and even enables the model to predict tokens which are not in the vocabulary by pointing at their positions in the input.\n\nBaselines. We compare with code2seq (Alon et al., 2019a), the Graph Relational Embedding Attention Transformer (GREAT) (Hellendoorn et al., 2020), and the BiLSTM+GNN\u2192LSTM+Pointer model presented in Fernandes et al. (2019). Code2seq is a non-Transformer model and state of the art for code summarization using only AST information. GREAT is a recent Transformer model using the framework presented in (Shaw et al., 2018) to bias the attention via edges. In the original formulation, GREAT additionally uses hand-crafted, language-specific edges such as dataflow, 'computed from', or 'next lexical use' edges, which require specialized preprocessing and static analysis tools. While this approach of leveraging language-specific features can certainly improve results on specific tasks and programming languages, our goal is to have a flexible model that can be used on any programming language. Since the specialized preprocessing used by GREAT is proprietary and not public, we produce the results for GREAT using edges from the AST instead, i.e. it has access to the same information as our proposed model. Note that the preprocessing of Fernandes et al. (2019) is language specific, which is why we only compare with their results on Java-small.\n\n\nRESULTS\n\n\nMONOLINGUAL CODE SUMMARIZATION\n\nCSN dataset. First, we study the performance (measured by F1 score) of our model and the baselines on the traditional setting, where training and evaluation are performed on a single programming language. The results are shown in the upper part of  multi-language training) substantially outperforms all other models on all but one language, highlighting the effectiveness of jointly learning from Structure and Context. The only exception is Ruby, where it performs on par with its Context-only variant. We attribute this to the fact that there are relatively few samples in the Ruby dataset, and that Ruby is an dynamically typed language, which could make the Structure less powerful for learning. Interestingly, the Context-only CODE TRANS-FORMER outperforms GREAT on all languages. We attribute this to the fact that GREAT uses the Structure of the programs only in a localized way (see Sec. 3.1). Another noteworthy finding is that code2seq performs comparably to the Transformer-based baselines on Go. We hypothesize that ASTs are more informative on Go since it is a compiled and strongly typed language.\n\nJava-small results. In Table 3 we present code summarization results on the Java-small dataset. Among all models equipped with a pointer network, the CODE TRANSFORMER (without pretraining) obtains state-of-the-art on code summarization, outperforming all baselines, including the previous state-of-the-art on Java-small proposed by Fernandes et al. (2019). Further, pre-training on Java-medium and Java-large on the permutation language modeling objective  substantially improves precision, recall, and F1 score after fine-tuning on Java-small. To avoid leakage, we exclude the projects used in the validation and test splits of Java-small from pre-training.\n\nAblation study. We further perform ablations where we remove our model's access to the Context or Structure, also presented in Table 3  Ablation of the AST-based distances. In Table 4 we compare the performance of our model when trained with each of the four different AST distances (sibling shortest paths, ancestor shortest paths, shortest paths, personalized PageRank; see Section 3.1). Here, the model is trained on Java-small in the Structure-only setting and without pointer network. For reference, we also show the results of training our model using all four AST distance functions (c.f. Table 3). We find that, while the personalized PageRank distance performs best on its own, each of the individual distances on their own performs substantially worse than their combination, highlighting the usefulness of combining the distances in our model as well as their complementary nature.  \n\n\nSetup.\n\nA key contribution of our proposed architecture is that it only uses language-agnostic features, i.e. the source code and features that can be directly computed from the AST. We use this fact to study the first multilanguage code summarization model. We train our model jointly on Python, Javascript, Ruby, and Go. The shared sub-token vocabulary is the union of the individual vocabularies, enabling us to evaluate the multi-language model on the individual languages and compare with the single-language models. As proposed by Conneau & Lample (2019), we add a learned language embedding to each input embedding.\n\nResults. In the lower part of Table 2 we can see the results of training our CODE TRANSFORMER jointly on all four programming languages. Our multi-lingual variants substantially outperform the mono-lingual models on all languages. The strongest improvement is on Ruby, which is also the programming language with the smallest number of samples in the dataset. Fine-tuning on the individual languages after joint training on code summarization only has a marginal effect on performance, indicating that the multilingual objective is well-aligned with the individual languages.\n\nIn the last row, we have a variant of our model where we pre-train on the multi-lingual masked language modeling task, followed by finetuning on code summarization on the individual languages.\n\nFurther, we observe that similar to the results on Java-small, removing the pointer network generally leads to weaker performance. One notable exception is Go, where the variant without the pointer network performs better in terms of F1 score. Our investigation revealed that there seems to be some violation of the i.i.d. assumption in the split provided by the creators of the dataset. In Figure 7 we show that in the test partition of the Go dataset, the share of tokens from the labels also occurring in the methods' bodies -exactly the scenario where the pointer network can improve predictions -is substantially lower compared to the train/validation partitions.\n\nRemarkably, the multi-language Context-only variant (i.e. without access to the Structure) performs substantially worse than the full multi-language variant. This highlights that Structure is crucial to exploit the commonalities of different programming languages. Also notably, the GREAT baseline's results also improve substantially when trained in the multi-language setting, though it is still outperformed by our model. However, our results indicate that any representation learning model for code can benefit from multi-language training, especially when evaluating on low-resource languages.\n\nIn Table 16 we present results using the sample-F1 score. At the time of submission, our monolingual model on Python outperforms the state of the art on the ogbg-code2 4 (Hu et al., 2020) leaderboard by 112%, and our multilanguage variant with LM pretraining outperforms it by 122%.\n\nQualitative analysis of multilingual representations. Learning the CODE TRANSFORMER on multiple programming languages jointly provides us with embeddings in a shared representation space. In Fig. 4 we show a t-SNE (Maaten & Hinton, 2008) visualization of the ca. 40,000 snippets from the validation sets of four programming languages from the CSN dataset. For the embedding of a snippet, we use the representation of the method name in the final layer of the encoder. Note that the true method names are masked, i.e., inaccessible to the model. Further, note that in contrast to the monolingual embeddings learned by Kanade et al. (2020), the embeddings we evaluate are learned on the task of code summarization (though a similar study could be performed by using our model that was trained on the traditional language modeling pretraining task on multiple languages).\n\nWhile snippets from the same language tend to be grouped together, there are interesting intersections of the different programming languages. For example, we highlight all methods whose names start with the subtoken parse or main. We see that snippets starting with parse are predominantly in an intersection region of Python and Javascript. From these snippets, we display the python javascript go ruby  Figure 5: Example snippet starting with parse (left) and its best embedding match from other languages (right). Both methods parse an input string to convert it into a boolean value. Note that even though they are semantically very similar, their method names are not; nonetheless, their representations in the CODE TRANSFORMER encoder reflect their semantic similarity.\n\ncross-language pair with smallest Euclidean embedding distance in Fig. 5. Remarkably, both snippets are effectively the same method in Javascript and Python -it is worth reminding that the model has never seen any parallel data during training. On the other hand, snippets starting with main tend to lie at an intersectional region of Python, Javascript, and Go. In Table 6 in the appendix we show additional cross-lingual pairs with similar embeddings, including a failure case of a main function, where embedding distance is not representative of semantic similarity. We attribute this to the fact that we used the encoder output embedding of the masked method name -the representation used by the decoder to predict the method name -as a snippet's representation. Thus, snippets with completely different semantics (as is to be expected for very generic method names starting with main) have similar representations because they are predictive of the method name.\n\nAs another qualitative insight into the representations learned by the CODE TRANSFORMER we have found that the language embeddings of languages with similar roots in language design are close; see Table 5 in the appendix for the pairwise similarity matrix of the learned language embeddings.\n\n\nCONCLUSION\n\nWe present the CODE TRANSFORMER, which learns jointly from Structure and Context of programs while only relying on language-agnostic features. Our model obtains state-of-the-art performance on code summarization on five different programming languages. Besides these results for training on individual languages, the language-agnostic nature of our model allows us to train it jointly on multiple programming languages. The resulting multilingual model substantially outperforms its mono-lingual variant on all programming languages, setting the state of the art on each language. We observe the largest improvement from multilingual training on the language with fewest resources, indicating that multilingual training can improve learning for less widely used programming languages. Remarkably, multilingual training only from Context does not lead to the same improvements, highlighting the benefits of combining Structure and Context.  \n\u03c6(r i\u2192j ) 2k = sin r i\u2192j M 2k/d \u03c6(r i\u2192j ) 2k+1 = cos r i\u2192j M 2k/d ,\nwhere 1 \u2264 k < d/2 is the position in the encoding vector and M is some constant; we adopt M = 10, 000 as chosen by (Vaswani et al., 2017). Note that the distance encoding functions have no trainable parameters.\n\n\nA.2 MULTILINGUAL REPRESENTATION ANALYSIS\n\nIn Table 5, we show the pairwise cosine similarities of the learned language embeddings of the CODE TRANSFORMER. We can see that the pairs Python-Ruby and Javascript-Go have similar language embeddings. This aligns well with roots of language design and common use cases of the languages.\n\nMoreover, in Table 6, we show selected snippets starting with is, main, or load (left) and their best embedding matches from other languages (right).\n\n\nA.3 DATA PREPROCESSING\n\nA.3.1 TEXTUAL CODE SNIPPET PREPROCESSING 1. Tokenize code snippets with Pygments language-specific tokenizer.\n\n2. Remove comments (both multi-line, single-line and doc comments). The comment token types. pygments.token.Comment and pygments.token.Literal.String.Doc that are generated by Pygments are used to identify comments. if not (os.path.exists(filename)):\nfunction _isEqualArray(a, b) { if (a === b) { return true; } if ((a === undefined) || (b === undefined)) { return false; } var i = a.length; if (i !== b.length){ return false; } while (i--) { if (a[i] !== b[i]) {return\nsys.stderr.write( \"rflint: %s: No such file or \" \"directory\\n\" % filename ) return try: basename = os.path.basename(filename) (name, ext) = os.path.splitext(basename) imp.load_source(name, filename) except Exception as e:\n\nsys.stderr.write( \"rflint: %s: exception while \" \"loading: %s\\n\" % (filename, str(e)) ) fmt.Sprintf(\"%s+%s\u02dc\", filename, string(numBackup))) } Table 6: Selected snippets starting with is, main, or load (left) and their best embedding matches from other languages (right). 8. Any code snippets where the Pygments tokenizer cannot parse a token are discarded.\n\n\nA.3.2 STAGE 1 PREPROCESSING (GENERATION OF ASTS)\n\n1. Stripped code snippets are used to generate language-specific ASTs. For Java, we use the AST parser from the java-parser project. The ASTs contain node types and source ranges. For Python, JavaScript, Ruby and Go, we use semantic.\n\n2. Snippets that lead to an AST parse error are discarded.\n\n3. We calculate a mapping between tokens and nodes in the AST. Every token is assigned to the node in the AST with shortest source range that still encompasses the source range of the token. To find such a node, we originally intended to make use of the assumption that source ranges of child nodes do not overlap. Then, one could easily find the node with smallest encompassing source range by greedily selecting at every layer in the AST the child that encompasses the token's source range (there can only be at most one child that fulfills this). However, this assumption does not hold for all ASTs (see Figure 6 for an example). As a heuristic, we greedily select the child node with the shorter source range in case there were multiple child nodes with encompassing source ranges. This approximation seems to be sufficient in our case, and limits runtime as we do not have to consider multiple paths in the AST. It is also sufficient to stop when no child node encompasses the source range of the token, as in ASTs the source ranges of child nodes are always contained in the source ranges of their parent.\n\n\nA.3.3 STAGE 2 PREPROCESSING (CALCULATION OF DISTANCE MATRICES)\n\n1. Tokens are vocabularized. Any token occurring less than 100 times in the training set is replaced by an <unk> token.\n\n2. We calculate multiple pair-wise relations between nodes in the AST:\n\n\u2022 Personalized Page Rank (PPR) We interpret the negative logarithm of PPR as a distance. We use a teleport probability of \u03b1 = 0.15 and a threshold of e \u22125 , i.e., anything with \u2212 log P P R > 5 is considered unreachable \u2022 Shortest path length between two nodes \u2022 Ancestor shortest paths (bidirectional). That is, the parent has an ancestor shortest path distance of 1 to all its children and the child has a distance of -1 to its parents. We consider nodes that are not ancestors or descendants of a node (i.e. not reachable by following only parent or only child relations) as not connected in the ancestor shortest paths relation. We encode this with a very large value in their distance; we have found a value of 1, 000 to work well in practice. \u2022 Next sibling shortest paths (bidirectional, analogous to the ancestor shortest paths)\n\nNote that the ancestor shortest paths and next sibling shortest paths are required because treating the AST as a normal graph leads to ambiguity. In a graph, the neighbors of a node have no ordering; however in the AST, the order of the children of a node reflects their order in the code. Therefore, we explicitly include the next sibling shortest paths. The ancestor shortest paths would not be required if we treated the AST as a directed graph; in this case, however, a leaf node could not reach any other node in the AST, and therefore both PPR and shortest path length are not useful in this case. Therefore, we model the AST as undirected and inject the ancestor / child edges to avoid ambiguity.\n\n3. Distance values are binned into 32 bins using area-based exponential binning with a growth factor of 1.3, i.e., the area of a bin's rectangle (x: bin range, y: number of values in bin) will be approximately 1.3 times bigger for the next bin (going away from the bin that contains the zero value). Additionally, for discrete distance measures (such as sequence distance or shortest path length), we hard-code 9 values around 0 to have their own bins. For instance, on the sequence distance the values \u22124, \u22123, . . . , 4 have their individual bins, and around those values we employ the exponential binning.\n\n4. Punctuation tokens (such as points or brackets) are removed from the input sequence, as experiments showed that their presence does not improve performance but slows down training due to bigger input sizes.\n\n5. Snippets that are longer than MAX NUM TOKENS after punctuation tokens are removed are discarded from the training set. Throughout our experiments, we use MAX NUM TOKENS = 512. During evaluation on the test set, we use MAX NUM TOKENS = 1000.\n\n\nA.4 INPUT EMBEDDINGS TO THE MODEL\n\nBesides its five subtokens (e.g., ['get', 'data', '[PAD]', '[PAD]', '[PAD]']), each input token has a token type (coming from the Pygments tokenizer) and an AST node type. The AST node type is the type of the node assigned to each respective token, as described in Section A.3.2. We concatenate the embeddings of the five subtokens, the token type, and the AST node type. Then, we apply a linear layer (without activation function) to project down to the model's embedding dimension.\n\n\nA.5 INPUT TO THE GREAT BASELINE\n\nAs mentioned in the main text, we also compare with GREAT Hellendoorn et al. (2020). Since their preprocessing pipeline is proprietary and could not be shared with us even after contacting the authors, we provide to GREAT the same AST distances as our model. Since GREAT uses edges instead of distances to encode relations in the Structure, we essentially threshold the ancestor, sibling, and shortest-paths distances and provide the edges where the distances are equal to 1 (including their edge types) to the model.  Table 7 shows hyperparameters of our models for code summarization. For all our experiments, we use a Transformer Decoder with one layer and teacher forcing to generate 6 output sub tokens. We also employ label smoothing of 0.1. As optimizer, we use Adam with a learning rate of 8e \u22125 and weight decay of 3e \u22125 . Batch size during training is 8 with a simulated batch size of 128 achieved by gradient accumulation.\n\nApart from comparing the CODE TRANSFORMER to baselines, we performed the following hyperparameter comparisons and ablation studies:\n\n\u2022 CODE TRANSFORMER (structure-only) Using only AST information as input, i.e., masking all tokens that do not correspond to a leaf of the AST, and removing the token distance as a relation to be used by the model. Further, token types are not fed into the model.\n\n\u2022 CODE TRANSFORMER (context-only) Here, we do not include any information on the AST (i.e. node types and distances on the AST). This is effectively the XLNet backbone plus encoding of the token type returned by the tokenizer.\n\n\u2022 CODE TRANSFORMER (Max-Dist.) Applying a Max Distance Mask of 5 to the shortest paths distance (i.e., model cannot see a node that is more than 5 hops away no matter how small the other distances are). Early results showed that, as expected, results deteriorate substantially when limiting our model's receptive field. Hence, we do not include these results in this work.\n\n\u2022 Using 16 and 64 bins instead of 32 bins. This had no noticeable effect on performance.\n\n\nA.7 CODE SUMMARIZATION EXAMPLES\n\nIn the Tables 8, 9, 10, 11, 12, 13, 14 and 15 we present example functions from the Java-small dataset along with the different models' predictions for the function name.            Table 2 we observe that the pointer network improves the F1 score for all languages except Go, where counterintuitively it leads to reduced performance as measured by F1 score on the test set (while it improves by about 3 points on validation). To investigate this, in Figure 7 we plot the share of tokens in the labels also occurring in the bodies of methods in the different languages. Intuitively, this gives an indication on how much gain we can expect from using a pointer network. If the share were zero, then no token in the labels ever occur in the bodies of the methods, so the pointer network cannot improve the prediction by pointing at the input. We see that for Go, there is a strong mismatch between the test partition and the train/validation partitions, which much fewer tokens from the labels occurring in the bodies of methods on test compared to train/validation. Thus, we attribute the drop in performance observed by adding a pointer network on Go to this apparent violation of the i.i.d. assumption.\n\nA.9 CODE SUMMARIZATION RESULTS ON THE CSN DATASET (SAMPLE-F1)\n\nIn Table 16, we present our results on the CSN dataset as measured by the sample-F1 score.\n\nFigure 2 :\n2Structure distances used by our model.\n\nFigure 3 :\n3Left: Sequence (Context) and AST (Structure) representation of an input code snippet.\n\nFigure 4 :\n4t-SNE visualization of the CODE TRANSFORMER's learned multilingual representations. def parseBool(s): l = s.lower() if l in (\"true\", \"t\", \"1\"): return True if l in (\"false\", \"f\", \"0\"): return False raise Exception( \"Unable to convert string '%s'\" \"to a boolean value\" % s ) function jscoverage_getBooleanValue(s) { s = s.toLowerCase(); if (s === 'false' || s === 'f' || s === 'no' || s === 'n' || s === 'off' || s === '0') { return false; } return true; }\n\nfunc\nTaskSayHello(t * tasking.T) { username := t.Flags.String(\"name\") if username == \"\" { user, _ := user.Current() username = user.Name } if t.Flags.Bool(\"verbose\") { t.Logf(\"Hello %s, the time now is %s\\n\", username, time.Now()) } else { t.Logf(\"Hello %s\\n\", username) } } def _load_rule_file(self, filename):\n\nFigure 6 :\n6Example snippet and its corresponding AST obtained from GitHub Semantic. 3. Empty lines are removed. 4. Hard coded strings and numbers are replaced with a special [MASK STRING] and [MASK NUMBER] token. 5. Indentation style of the code snippet is detected and whitespace characters at the beginning of a line are replaced with a single [INDENT] or [DEDENT] token when indentation changes. 6. Tokens are further split into sub tokens, e.g., setBottomHeight \u2212 \u2192 ['set', 'bottom', 'height']. Throughout our experiments, we use 5 input sub tokens. If a token consists of less than 5 sub tokens, the remaining spaces are filled with a special [PAD] token. 7. Any remaining tokens that only consist of white spaces are removed. The only white space characters that are kept are line breaks '\\n'.\n\nFigure 7 :\n7Share of tokens in the labels also occurring in the bodies of methods.\n\n\n). E.g., if i hasdef get_model(): \nif condition: \ntrain() \n... [20 tokens] \nelse: \nreturn model \n\n( \n) \n\nAttention Layer \n\nENCODER LAYER \n\nToken distance \nencodings \n\nSource Code as Sequence of Tokens \n\nSource Code as Abstract Syntax Tree \n\n1 \n\n2 \n\n3 \n4 \n\n1 \n\n2 \n\n3 \n\n4 \n\n26 hops \n\n5 hops \n\nAST distance \nencodings \n\nCode Transformer \n\n0.9 \n\n0.1 \n\nAttention \nvalues \n\nOutput \nencodings \n\nCode \nSummarization \nTask \n\ndef [MASK]: \n... \n\nENCODER \n\n(0.9) train_model \n(0.5) fit \n(0.3) cross_valid \n(0.1) zero_grad \n(0.1) infer \n\nDECODER \n\n2 \n\n1 \n\n3 \n\n4 \n\n2 \n\nInput \nencodings \n\nd(1,4) = 26 \n\nd(1,4) = 5 \nPPR(1,4) = 0.27 \n... \n\n\n\nTable 1 :\n1Dataset statistics.\n\nTable 2 .\n2The CODE TRANSFORMER (withoutModel \nPython \nJavascript \nRuby \nGo \nPrec. \nRec. \nF1 \nPrec. \nRec. \nF1 \nPrec. \nRec. \nF1 \nPrec. \nRec. \nF1 \n\ncode2seq \n35.79 24.85 29.34 30.18 19.88 23.97 23.23 10.31 14.28 52.30 43.43 47.45 \nGREAT \n35.07 31.59 33.24 31.20 26.84 28.86 24.64 22.23 23.38 50.01 46.51 48.20 \nOurs w/o structure \n37.38 31.98 34.47 33.17 26.70 29.59 29.85 25.87 27.72 51.78 47.57 49.59 \nOurs w/o pointer net \n37.74 31.85 34.55 33.12 28.70 30.75 23.32 25.21 24.23 54.31 50.12 52.13 \nOurs \n36.40 33.66 34.97 35.06 29.61 32.11 31.42 24.46 27.50 55.10 48.05 51.34 \n\ncode2seq (Multilanguage) \n34.49 25.49 29.32 31.62 22.16 26.06 23.97 17.06 19.93 52.70 44.36 48.17 \nGREAT (Multilanguage) \n36.75 31.54 33.94 33.58 27.78 30.41 30.05 24.33 26.89 52.65 48.30 50.38 \nOurs w/o structure (Mult.) \n38.48 30.14 33.80 35.38 27.41 30.89 32.61 26.76 29.40 55.03 47.34 50.90 \nOurs w/o pointer (Mult.) \n38.91 33.12 35.78 37.21 29.75 33.07 34.52 27.31 30.50 56.07 50.76 53.28 \nOurs (Multilanguage) \n38.89 33.82 36.18 36.95 29.98 33.10 33.93 28.94 31.24 56.00 50.44 53.07 \n\nOurs (Mult. + Finetune) \n39.85 32.79 35.98 37.00 29.79 33.00 35.85 27.75 31.28 55.63 51.12 53.28 \nOurs (Mult. + LM Pretrain) 39.67 35.29 37.35 37.06 31.94 34.31 35.19 29.36 32.01 57.73 51.89 54.65 \n\n\n\nTable 2 :\n2Code summarization results on the CSN dataset (micro F1).\n\n\n.Model \nPrec. \nRec. \nF1 \n\nWithout pointer net \ncode2seq \n51.23 37.31 43.18 \nOurs w/o structure \n50.70 45.49 47.96 \nOurs w/o context \n51.81 46.04 48.75 \nOurs \n50.33 46.80 48.50 \n\nWith pointer net \nFernandes et al. (2019) \n-\n-\n51.4 \nGREAT \n53.60 46.41 49.75 \nOurs w/o structure \n55.48 46.07 50.34 \nOurs w/o context \n54.45 45.29 49.45 \nOurs \n54.85 49.84 52.22 \n\nOurs + Pretrain \n57.02 50.87 53.77 \n\n\n\nTable 3 :\n3Results on Java-small and ablation study.With pointer network. We find that both ab-\nlations lead to a substantial drop in perfor-\nmance, highlighting the benefit of learning \njointly from Structure and Context. Interest-\ningly, the model without access to the Struc-\nture performs slightly better than the variant \nwithout Context. Note that our model with-\nout Structure is related to the XLNet (Yang \net al., 2019) model, where we add a pointer net-\nwork to the decoder and concatenate the token \ntypes to their respective input tokens (see Ap-\npendix A.4). Without pointer network. We re-\npeat the ablation on the variants without pointer \nnetwork. Here, the variant without Context per-\nforms better than the variant without Structure, \nindicating that the pointer network helps to compensate for the lack of access to Structure. The \nStructure-only variant (w/o pointer net) of our model even outperforms the full variant in this sce-\nnario. Inspection of the results revealed that the Structure-only variant has better performance on \nlonger method names, which have an outsize influence on the micro-F1 score used in this work. \n\n\n\nTable 4 :\n4AST distance ablation study.\n\nTable 5 :\n5Pairwise cosine similarities of the learned language embeddings of the CODE TRANS-JPMorgan Chase, Docomo, Hitachi, JD.com, KDDI, NVIDIA, Dell, Toshiba, Intel, and Unit-edHealth Group. Jure Leskovec is a Chan Zuckerberg Biohub investigator.FORMER. \nThis research was supported by the TUM International Graduate School of Science and Engineering \n(IGSSE). Stanford University is supported by DARPA under Nos. N660011924033 (MCS); ARO \nunder Nos. W911NF-16-1-0342 (MURI), W911NF-16-1-0171 (DURIP); NSF under Nos. OAC-\n1835598 (CINES), OAC-1934578 (HDR), CCF-1918940 (Expeditions), IIS-2030477 (RAPID); \nStanford Data Science Initiative, Wu Tsai Neurosciences Institute, Chan Zuckerberg Biohub, Ama-\nzon, A APPENDIX \n\nA.1 DISTANCE ENCODING FUNCTION \n\nFor encoding scalar relation values via vectors we employ encoding functions \u03c6 : R \u2192 R d , where d \nis the model's embedding dimension. We choose the popular sinusoidal encoding function presented \nin Vaswani et al. (2017): \n\n\n\nTable 8 :\n8The CODE TRANSFORMER is the only model to correctly identify the notion of getting the next entry.private Path findCacheFile(Path[] cacheFiles, String fileName) { \nif (cacheFiles != null && cacheFiles.length > 0) { \nfor (Path file : cacheFiles) { \nif (file.getName().equals(fileName)) { \nreturn file; \n} \n} \n} \nreturn null; \n} \n\nModel \nPrediction \n\nGREAT \nget path \ncode2seq \nfind file \nOurs w/o structure \nget file \nCODE TRANSFORMER \nfind cache \n\nGround Truth \nfind cache file \n\n\n\nTable 9 :\n9The CODE TRANSFORMER is the only model to both recognize that the task is to find a file as well as the fact that it is about the cache. However, it did not correctly predict the file part of the method name.public int compare(Pair<LoggedJob, JobTraceReader> p1, \nPair<LoggedJob, JobTraceReader> p2) { \nLoggedJob j1 = p1.first(); \nLoggedJob j2 = p2.first(); \nreturn (j1.getSubmitTime() < j2.getSubmitTime()) ? -1 \n: (j1.getSubmitTime() == j2.getSubmitTime()) ? 0 : 1; \n} \n\nModel \nPrediction \n\nGREAT \nrun \ncode2seq \nget submit time \nOurs w/o structure \ncompare \nCODE TRANSFORMER \ncompare \n\nGround Truth \ncompare \n\n\n\nTable 10 :\n10The CODE TRANSFORMER and the its context-only variant are the only models correctly recognizing the 'compare' template in the method body. static MNTPROC fromValue(int value) { if (value < 0 || value >= values().length) { return null; } return values()[value]; }public Model \nPrediction \n\nGREAT \nget value \ncode2seq \nget value \nOurs w/o structure \nto \nCODE TRANSFORMER from value \n\nGround Truth \nfrom value \n\n\n\nTable 11 :\n11The CODE TRANSFORMER is the only model to recognize that the snippet is similar to a static factory method which is often preceded with from. throws StorageException, URISyntaxException { CloudBlobDirectoryWrapper directory = this.container.getDirectoryReference(aPrefix); return directory.listBlobs(null, useFlatBlobListing, listingDetails, options, opContext); }private Iterable<ListBlobItem> listRootBlobs(String aPrefix, \nboolean useFlatBlobListing, \nEnumSet<BlobListingDetails> listingDetails, \nBlobRequestOptions options, \nOperationContext opContext) \nModel \nPrediction \n\nGREAT \nlist blobs \ncode2seq \nlist blobs \nOurs w/o structure \nlist blobs \nCODE TRANSFORMER list blobs by prefix \n\nGround Truth \nlist root blobs \n\n\n\nTable 12 :\n12All models could correctly identify the listBlobs() call in the return statement. However, the CODE TRANSFORMER additionally comprehended that the specified prefix is quite important. static void dumpOpCounts(EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts) { StringBuilder sb = new StringBuilder(); sb.append(\"Summary of operations loaded from edit log:\\n \"); Joiner.on(\"\\n \").withKeyValueSeparator(\"=\").appendTo(sb, opCounts); FSImage.LOG.debug(sb.toString()); }private Model \nPrediction \n\nGREAT \nappend \ncode2seq \nadd \nOurs w/o structure \nlog \nCODE TRANSFORMER \nlog op counts \n\nGround Truth \ndump op counts \n\n\n\nTable 13 :\n13Only the CODE TRANSFORMER could correctly identify that it is the op counts that should be logged. String execCommand(File f, String... cmd) throws IOException { String[] args = new String[cmd.length + 1]; System.arraycopy(cmd, 0, args, 0, cmd.length); args[cmd.length] = f.getCanonicalPath(); String output = Shell.execCommand(args); return output; }static Model \nPrediction \n\nGREAT \nget canonical path \ncode2seq \nexec \nOurs w/o structure \nget output \nCODE TRANSFORMER \nexec command \n\nGround Truth \nexec command \n\n\n\nTable 14 :\n14Only the CODE TRANSFORMER and code2seq could identify that the relevant part of the method is concerned with executing a command instead of returning something.Amount of label tokens appearing in bodyPointer Network Potentialsjava-small \ngo \npython \nruby \njavascript \n0% \n\n10% \n\n20% \n\n30% \n\n40% \n\n50% \n\n60% \n\ntrain \nvalid \ntest \n\n\n\n\nOurs w/o pointer net 38.77 31.72 33.27 32.70 25.50 27.33 32.12 30.17 29.36 53.09 48.70 49.26 Ours 36.68 33.86 33.84 33.36 27.55 29.02 31.53 24.72 26.43 52.00 47.35 47.93 Multilanguage) 35.73 30.81 31.74 31.49 26.17 27.41 29.72 24.20 25.43 50.32 47.94 47.66 Ours w/o structure (Mult.) 36.78 29.92 31.58 32.60 26.02 27.74 31.71 26.07 27.24 51.91 47.58 48.15 Ours w/o pointer (Mult.) 37.18 30.52 32.04 33.95 25.92 28.11 32.76 25.04 27.01 53.50 48.54 49.35 Ours (Multilanguage) 38.10 33.32 34.18 34.29 28.69 30.08 33.30 28.33 29.29 53.86 50.46 50.61 Ours (Mult. + Finetune) 38.29 32.41 33.65 34.43 28.28 29.91 32.89 27.15 28.49 53.85 50.85 50.81 Ours (Mult. + LM Pretrain) 38.97 34.77 35.34 35.23 30.26 31.38 33.73 29.15 29.94 55.31 52.03 52.13Model \nPython \nJavascript \nRuby \nGo \nPrec. \nRec. \nF1 \nPrec. \nRec. \nF1 \nPrec. \nRec. \nF1 \nPrec. \nRec. \nF1 \n\ncode2seq \n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nGREAT \n34.93 31.12 31.61 29.69 24.24 25.55 25.69 21.49 22.18 48.38 45.97 45.71 \nOurs w/o structure \n36.87 32.17 32.97 31.30 25.03 26.64 31.43 25.34 26.63 49.78 46.73 46.69 \ncode2seq (Multilanguage) \n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nGREAT (\n\nTable 16 :\n16Code summarization results on the CSN dataset (sample-F1).A.8 ESTIMATION OF POINTER NETWORK POTENTIALIn \nOF SOURCE CODE FROM STRUCTURE AND CONTEXT Daniel Z\u00fcgner, Tobias Kirschstein Technical University of Munich {zuegnerd,kirschto}@in.tum.de Michele Catasta\nCode at www.daml.in.tum.de/code-transformer, demo at code-transformer.org.2  We use the term language-agnostic to highlight that our model does not rely on language-specific features (e.g., program analysis edges), thus facilitating multi-language training, as it is possible to generate unified AST representations for different programming languages.\nhttps://github.com/github/semantic\nhttps://ogb.stanford.edu/docs/graphprop/#ogbg-code2\nhis thesis work(Bourgeois, 2019). We further thank Simon Geisler for his helpful suggestions and proofreading the paper, as well as the anonymous reviewers for their constructive feedback and fruitful discussions.\nACKNOWLEDGEMENTSWe are grateful to Dylan Bourgeois for having paved the way to this research contribution with\nMining api patterns as partial orders from source code: From usage scenarios to specifications. Mithun Acharya, Tao Xie, Jian Pei, Jun Xu, 10.1145/1287624.1287630Proceedings of the the 6th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering, ESEC-FSE '07. the the 6th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering, ESEC-FSE '07New York, NY, USAAssociation for Computing MachineryMithun Acharya, Tao Xie, Jian Pei, and Jun Xu. Mining api patterns as partial orders from source code: From usage scenarios to specifications. In Proceedings of the the 6th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Founda- tions of Software Engineering, ESEC-FSE '07, pp. 25-34, New York, NY, USA, 2007. Associ- ation for Computing Machinery. ISBN 9781595938114. doi: 10.1145/1287624.1287630. URL https://doi.org/10.1145/1287624.1287630.\n\nLearning natural coding conventions. Miltiadis Allamanis, Earl T Barr, Christian Bird, Charles Sutton, 10.1145/2635868.2635883Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering. the 22nd ACM SIGSOFT International Symposium on Foundations of Software EngineeringNew York, NY, USAAssociation for Computing MachineryMiltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. Learning natural coding conventions. In Proceedings of the 22nd ACM SIGSOFT International Symposium on Founda- tions of Software Engineering, FSE 2014, pp. 281-293, New York, NY, USA, 2014. Associa- tion for Computing Machinery. ISBN 9781450330565. doi: 10.1145/2635868.2635883. URL https://doi.org/10.1145/2635868.2635883.\n\nSuggesting accurate method and class names. Miltiadis Allamanis, Earl T Barr, Christian Bird, Charles Sutton, 10.1145/2786805.2786849Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2015. the 2015 10th Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2015New York, NY, USAAssociation for Computing MachineryMiltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. Suggesting accurate method and class names. In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2015, pp. 38-49, New York, NY, USA, 2015. Association for Computing Machinery. ISBN 9781450336758. doi: 10.1145/2786805.2786849. URL https://doi. org/10.1145/2786805.2786849.\n\nA convolutional attention network for extreme summarization of source code. Miltiadis Allamanis, Hao Peng, Charles Sutton, International conference on machine learning. Miltiadis Allamanis, Hao Peng, and Charles Sutton. A convolutional attention network for extreme summarization of source code. In International conference on machine learning, pp. 2091-2100, 2016.\n\nLearning to represent programs with graphs. Miltiadis Allamanis, Marc Brockschmidt, Mahmoud Khademi, International Conference on Learning Representations. Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs with graphs. In International Conference on Learning Representations, 2018. URL https: //openreview.net/forum?id=BJOFETxR-.\n\nA general path-based representation for predicting program properties. Uri Alon, Meital Zilberstein, Omer Levy, Eran Yahav, 10.1145/3192366.3192412Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 2018. the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 2018New York, NY, USAAssociation for Computing MachineryUri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. A general path-based representa- tion for predicting program properties. In Proceedings of the 39th ACM SIGPLAN Confer- ence on Programming Language Design and Implementation, PLDI 2018, pp. 404-419, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450356985. doi: 10.1145/3192366.3192412. URL https://doi.org/10.1145/3192366.3192412.\n\nGenerating sequences from structured representations of code. Uri Alon, Shaked Brody, Omer Levy, Eran Yahav, International Conference on Learning Representations. 2Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. code2seq: Generating sequences from struc- tured representations of code. In International Conference on Learning Representations, 2019a. URL https://openreview.net/forum?id=H1gKYo09tX.\n\nLearning distributed representations of code. Uri Alon, Meital Zilberstein, Omer Levy, Eran Yahav, Proceedings of the ACM on Programming Languages. 2Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. code2vec: Learning distributed repre- sentations of code. Proceedings of the ACM on Programming Languages, 3(POPL):1-29, 2019b.\n\nW Peter, Jessica B Battaglia, Victor Hamrick, Alvaro Bapst, Vinicius Sanchez-Gonzalez, Mateusz Zambaldi, Andrea Malinowski, David Tacchetti, Adam Raposo, Ryan Santoro, Faulkner, arXiv:1806.01261Relational inductive biases, deep learning, and graph networks. arXiv preprintPeter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.\n\nLearning python code suggestion with a sparse pointer network. Avishkar Bhoopchand, Tim Rockt\u00e4schel, Earl Barr, Sebastian Riedel, arXiv:1611.08307arXiv preprintAvishkar Bhoopchand, Tim Rockt\u00e4schel, Earl Barr, and Sebastian Riedel. Learning python code suggestion with a sparse pointer network. arXiv preprint arXiv:1611.08307, 2016.\n\nStatistical deobfuscation of android applications. Benjamin Bichsel, Veselin Raychev, Petar Tsankov, Martin Vechev, 10.1145/2976749.2978422Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, CCS '16. the 2016 ACM SIGSAC Conference on Computer and Communications Security, CCS '16New York, NY, USAAssociation for Computing MachineryBenjamin Bichsel, Veselin Raychev, Petar Tsankov, and Martin Vechev. Statistical deobfuscation of android applications. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, CCS '16, pp. 343-355, New York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450341394. doi: 10.1145/2976749.2978422. URL https: //doi.org/10.1145/2976749.2978422.\n\nPhog: probabilistic model for code. Pavol Bielik, Veselin Raychev, Martin Vechev, International Conference on Machine Learning. Pavol Bielik, Veselin Raychev, and Martin Vechev. Phog: probabilistic model for code. In Interna- tional Conference on Machine Learning, pp. 2933-2942, 2016.\n\nScaling graph neural networks with approximate pagerank. Aleksandar Bojchevski, Johannes Klicpera, Bryan Perozzi, Amol Kapoor, Martin Blais, Benedek R\u00f3zemberczki, Michal Lukasik, Stephan G\u00fcnnemann, 10.1145/3394486.3403296Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '20. the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '20New York, NY, USAAssociation for Computing MachineryAleksandar Bojchevski, Johannes Klicpera, Bryan Perozzi, Amol Kapoor, Martin Blais, Benedek R\u00f3zemberczki, Michal Lukasik, and Stephan G\u00fcnnemann. Scaling graph neural networks with approximate pagerank. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '20, pp. 2464-2473, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450379984. doi: 10.1145/3394486.3403296. URL https://doi.org/10.1145/3394486.3403296.\n\nLearning representations of source code from structure and context. Dylan Bourgeois, Dylan Bourgeois. Learning representations of source code from structure and context. 2019. URL http://infoscience.epfl.ch/record/277163.\n\nCross-lingual language model pretraining. Alexis Conneau, Guillaume Lample, Advances in Neural Information Processing Systems. H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. GarnettCurran Associates, Inc32Alexis Conneau and Guillaume Lample. Cross-lingual language model pretrain- ing. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 7059- 7069. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/ 8928-cross-lingual-language-model-pretraining.pdf.\n\nDiscriminative embeddings of latent variable models for structured data. Hanjun Dai, Bo Dai, Le Song, International conference on machine learning. Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for struc- tured data. In International conference on machine learning, pp. 2702-2711, 2016.\n\nTransformer-xl: Attentive language models beyond a fixed-length context. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, V Quoc, Ruslan Le, Salakhutdinov, arXiv:1901.02860arXiv preprintZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi- nov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.\n\nA deep language model for software code. Hoa Khanh Dam, Truyen Tran, Trang Pham, FSE 2016: Proceedings of the Foundations Software Engineering International Symposium. Hoa Khanh Dam, Truyen Tran, and Trang Pham. A deep language model for software code. In FSE 2016: Proceedings of the Foundations Software Engineering International Symposium [The Conference], 2016.\n\nConvolutional neural networks on graphs with fast localized spectral filtering. Micha\u00ebl Defferrard, Xavier Bresson, Pierre Vandergheynst, Advances in neural information processing systems. Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in neural information processing systems, pp. 3844-3852, 2016.\n\nConvolutional networks on graphs for learning molecular fingerprints. Dougal David K Duvenaud, Jorge Maclaurin, Rafael Iparraguirre, Timothy Bombarell, Al\u00e1n Hirzel, Ryan P Aspuru-Guzik, Adams, Advances in neural information processing systems. David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Al\u00e1n Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. In Advances in neural information processing systems, pp. 2224-2232, 2015.\n\nStructured neural summarization. Patrick Fernandes, Miltiadis Allamanis, Marc Brockschmidt, International Conference on Learning Representations. Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. Structured neural summa- rization. In International Conference on Learning Representations, 2019. URL https: //openreview.net/forum?id=H1ersoRqtm.\n\nA new model for learning in graph domains. Marco Gori, Gabriele Monfardini, Franco Scarselli, Proceedings. 2005 IEEE International Joint Conference on Neural Networks. 2005 IEEE International Joint Conference on Neural NetworksIEEE2Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains. In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2, pp. 729-734. IEEE, 2005.\n\nInductive representation learning on large graphs. William L Hamilton, Rex Ying, Jure Leskovec, NIPS. William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In NIPS, 2017.\n\nGlobal relational models of source code. J Vincent, Charles Hellendoorn, Rishabh Sutton, Petros Singh, David Maniatis, Bieber, International Conference on Learning Representations. Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. Global relational models of source code. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=B1lnbRNtwr.\n\nOn the naturalness of software. Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, Premkumar Devanbu, Proceedings of the 34th International Conference on Software Engineering, ICSE '12. the 34th International Conference on Software Engineering, ICSE '12IEEE PressISBN 9781467310673Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. On the natural- ness of software. In Proceedings of the 34th International Conference on Software Engineering, ICSE '12, pp. 837-847. IEEE Press, 2012. ISBN 9781467310673.\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. 98Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735-1780, 1997.\n\nOpen graph benchmark: Datasets for machine learning on graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, Advances in Neural Information Processing Systems. H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. LinCurran Associates, Inc33Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad- vances in Neural Information Processing Systems, volume 33, pp. 22118-22133. Curran As- sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf.\n\nCodesearchnet challenge: Evaluating the state of semantic code search. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, Marc Brockschmidt, arXiv:1909.09436arXiv preprintHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436, 2019.\n\nLearning and evaluating contextual embedding of source code. Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, Kensen Shi, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningAditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluating contextual embedding of source code. In Proceedings of the 37th International Conference on Machine Learning, 2020.\n\nSemi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, International Conference on Learning Representations (ICLR. Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net- works. In International Conference on Learning Representations (ICLR), 2017.\n\nPredict then propagate: Graph neural networks meet personalized pagerank. Johannes Klicpera, Aleksandar Bojchevski, Stephan G\u00fcnnemann, International Conference on Learning Representations (ICLR). Johannes Klicpera, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In International Conference on Learning Representations (ICLR), 2019a.\n\nDiffusion improves graph learning. Johannes Klicpera, Stefan Wei\u00dfenberger, Stephan G\u00fcnnemann, Conference on Neural Information Processing Systems (NeurIPS). Johannes Klicpera, Stefan Wei\u00dfenberger, and Stephan G\u00fcnnemann. Diffusion improves graph learn- ing. In Conference on Neural Information Processing Systems (NeurIPS), 2019b.\n\nDirectional message passing for molecular graphs. Johannes Klicpera, Janek Gro\u00df, Stephan G\u00fcnnemann, International Conference on Learning Representations (ICLR. 2020Johannes Klicpera, Janek Gro\u00df, and Stephan G\u00fcnnemann. Directional message passing for molec- ular graphs. In International Conference on Learning Representations (ICLR), 2020.\n\nJian Li, Yue Wang, Irwin Michael R Lyu, King, arXiv:1711.09573Code completion with neural attention and pointer networks. arXiv preprintJian Li, Yue Wang, Michael R Lyu, and Irwin King. Code completion with neural attention and pointer networks. arXiv preprint arXiv:1711.09573, 2017.\n\nGated graph sequence neural networks. Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel, arXiv:1511.05493arXiv preprintYujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.\n\nVisualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579-2605, 2008.\n\nStructured generative models of natural source code. Chris Maddison, Daniel Tarlow, International Conference on Machine Learning. Chris Maddison and Daniel Tarlow. Structured generative models of natural source code. In Inter- national Conference on Machine Learning, pp. 649-657, 2014.\n\nConvolutional neural networks over tree structures for programming language processing. Lili Mou, Ge Li, Lu Zhang, Tao Wang, Zhi Jin, Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI'16. the Thirtieth AAAI Conference on Artificial Intelligence, AAAI'16AAAI PressLili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. Convolutional neural networks over tree struc- tures for programming language processing. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI'16, pp. 1287-1293. AAAI Press, 2016.\n\nExploring api embedding for api usages and applications. Anh Tuan Trong Duc Nguyen, Hung Nguyen, Tien N Dang Phan, Nguyen, 10.1109/ICSE.2017.47Proceedings of the 39th International Conference on Software Engineering, ICSE '17. the 39th International Conference on Software Engineering, ICSE '17IEEE PressTrong Duc Nguyen, Anh Tuan Nguyen, Hung Dang Phan, and Tien N. Nguyen. Exploring api embedding for api usages and applications. In Proceedings of the 39th International Conference on Software Engineering, ICSE '17, pp. 438-449. IEEE Press, 2017. ISBN 9781538638682. doi: 10.1109/ICSE.2017.47. URL https://doi.org/10.1109/ICSE.2017.47.\n\nThe pagerank citation ranking: Bringing order to the web. Lawrence Page, Sergey Brin, Rajeev Motwani, Terry Winograd, Stanford InfoLabTechnical reportLawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford InfoLab, 1999.\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI Blog. 189Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.\n\nCode completion with statistical language models. Veselin Raychev, Martin Vechev, Eran Yahav, 10.1145/2594291.2594321Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI '14. the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI '14New York, NY, USAAssociation for Computing MachineryVeselin Raychev, Martin Vechev, and Eran Yahav. Code completion with statistical language models. In Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI '14, pp. 419-428, New York, NY, USA, 2014. Association for Computing Machinery. ISBN 9781450327848. doi: 10.1145/2594291.2594321. URL https://doi. org/10.1145/2594291.2594321.\n\nThe graph neural network model. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, Gabriele Monfardini, IEEE Transactions on Neural Networks. 201Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2008.\n\nModeling relational data with graph convolutional networks. Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den, Ivan Berg, Max Titov, Welling, 978-3-319-93417-4Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler, Rapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish AlamSpringer International PublishingChamThe Semantic WebMichael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler, Rapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam (eds.), The Semantic Web, pp. 593-607, Cham, 2018. Springer International Pub- lishing. ISBN 978-3-319-93417-4.\n\nSelf-attention with relative position representations. Peter Shaw, Jakob Uszkoreit, Ashish Vaswani, 10.18653/v1/N18-2074Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics2Short PapersPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representa- tions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 464-468, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2074. URL https://www.aclweb.org/anthology/N18-2074.\n\nOrdered neurons: Integrating tree structures into recurrent neural networks. Yikang Shen, Shawn Tan, Alessandro Sordoni, Aaron Courville, International Conference on Learning Representations. Yikang Shen, Shawn Tan, Alessandro Sordoni, and Aaron Courville. Ordered neurons: Integrating tree structures into recurrent neural networks. In International Conference on Learning Repre- sentations, 2019. URL https://openreview.net/forum?id=B1l6qiR5F7.\n\nNovel positional encodings to enable tree-based transformers. Vighnesh Shiv, Chris Quirk, Advances in Neural Information Processing Systems. Vighnesh Shiv and Chris Quirk. Novel positional encodings to enable tree-based transformers. In Advances in Neural Information Processing Systems, pp. 12081-12091, 2019.\n\nRecursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, D Christopher, Manning, Y Andrew, Christopher Ng, Potts, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processingRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language pro- cessing, pp. 1631-1642, 2013.\n\nImproved semantic representations from tree-structured long short-term memory networks. Kai Sheng Tai, Richard Socher, Christopher D Manning, arXiv:1503.00075arXiv preprintKai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075, 2015.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998-6008, 2017.\n\nPetar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio, Graph Attention Networks. International Conference on Learning Representations. Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph Attention Networks. International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJXMpikCZ.\n\nPointer networks. Oriol Vinyals, Meire Fortunato, Navdeep Jaitly, Advances in neural information processing systems. Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in neural information processing systems, pp. 2692-2700, 2015.\n\nBugram: Bug detection with n-gram language models. Song Wang, Devin Chollak, Dana Movshovitz-Attias, Lin Tan, 10.1145/2970276.2970341Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering, ASE 2016. the 31st IEEE/ACM International Conference on Automated Software Engineering, ASE 2016New York, NY, USAAssociation for Computing MachinerySong Wang, Devin Chollak, Dana Movshovitz-Attias, and Lin Tan. Bugram: Bug detection with n-gram language models. In Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering, ASE 2016, pp. 708-719, New York, NY, USA, 2016. Associ- ation for Computing Machinery. ISBN 9781450338455. doi: 10.1145/2970276.2970341. URL https://doi.org/10.1145/2970276.2970341.\n\nHow powerful are graph neural networks?. Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, International Conference on Learning Representations. Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. URL https: //openreview.net/forum?id=ryGs6iA5Km.\n\nXlnet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, R Russ, Quoc V Salakhutdinov, Le, Advances in neural information processing systems. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural information processing systems, pp. 5754-5764, 2019.\n\nHierarchical graph representation learning with differentiable pooling. Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, Jure Leskovec, Advances in neural information processing systems. Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hi- erarchical graph representation learning with differentiable pooling. In Advances in neural infor- mation processing systems, pp. 4800-4810, 2018.\n\nPosition-aware graph neural networks. Jiaxuan You, Rex Ying, Jure Leskovec, PMLRProceedings of the 36th International Conference on Machine Learning. Kamalika Chaudhuri and Ruslan Salakhutdinovthe 36th International Conference on Machine Learning97Jiaxuan You, Rex Ying, and Jure Leskovec. Position-aware graph neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 7134- 7143. PMLR, 09-15 Jun 2019. URL http://proceedings.mlr.press/v97/you19b. html.\n\nLink prediction based on graph neural networks. Muhan Zhang, Yixin Chen, Advances in Neural Information Processing Systems. Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In Advances in Neural Information Processing Systems, pp. 5165-5175, 2018.\n", "annotations": {"author": "[{\"end\":224,\"start\":87},{\"end\":348,\"start\":225},{\"end\":469,\"start\":349},{\"end\":588,\"start\":470},{\"end\":732,\"start\":589}]", "publisher": null, "author_last_name": "[{\"end\":100,\"start\":94},{\"end\":243,\"start\":232},{\"end\":364,\"start\":357},{\"end\":483,\"start\":475},{\"end\":606,\"start\":597}]", "author_first_name": "[{\"end\":93,\"start\":87},{\"end\":231,\"start\":225},{\"end\":356,\"start\":349},{\"end\":474,\"start\":470},{\"end\":596,\"start\":589}]", "author_affiliation": "[{\"end\":223,\"start\":121},{\"end\":347,\"start\":245},{\"end\":468,\"start\":366},{\"end\":587,\"start\":485},{\"end\":731,\"start\":629}]", "title": "[{\"end\":84,\"start\":1},{\"end\":816,\"start\":733}]", "venue": null, "abstract": "[{\"end\":1927,\"start\":862}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2864,\"start\":2832},{\"end\":2871,\"start\":2865},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2910,\"start\":2888},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2931,\"start\":2910},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2949,\"start\":2931},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3092,\"start\":3070},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":3108,\"start\":3092},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3132,\"start\":3108},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":3149,\"start\":3132},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3171,\"start\":3149},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3187,\"start\":3171},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3209,\"start\":3187},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3502,\"start\":3477},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3694,\"start\":3676},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3957,\"start\":3934},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6178,\"start\":6157},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6195,\"start\":6178},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6268,\"start\":6247},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6306,\"start\":6283},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6506,\"start\":6487},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6656,\"start\":6639},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6745,\"start\":6721},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6781,\"start\":6759},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6801,\"start\":6781},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6875,\"start\":6854},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6899,\"start\":6875},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6978,\"start\":6956},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7001,\"start\":6978},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7081,\"start\":7059},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7241,\"start\":7216},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7261,\"start\":7241},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7362,\"start\":7340},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7417,\"start\":7393},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7602,\"start\":7583},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7618,\"start\":7602},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8026,\"start\":8002},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8081,\"start\":8063},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8165,\"start\":8144},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8271,\"start\":8252},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8336,\"start\":8312},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8540,\"start\":8521},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9009,\"start\":8985},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9111,\"start\":9092},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9134,\"start\":9111},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9433,\"start\":9411},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9455,\"start\":9433},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9478,\"start\":9455},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":9516,\"start\":9496},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9543,\"start\":9516},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9587,\"start\":9562},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":9605,\"start\":9587},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":9649,\"start\":9630},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9666,\"start\":9649},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9688,\"start\":9666},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":12675,\"start\":12653},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13936,\"start\":13911},{\"end\":15333,\"start\":15308},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15465,\"start\":15441},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15467,\"start\":15465},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15491,\"start\":15467},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17966,\"start\":17947},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17994,\"start\":17971},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19006,\"start\":18985},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19281,\"start\":19257},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19422,\"start\":19403},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19684,\"start\":19664},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20101,\"start\":20082},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":20639,\"start\":20617},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20680,\"start\":20657},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21179,\"start\":21159},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21268,\"start\":21242},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21344,\"start\":21321},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21542,\"start\":21523},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22285,\"start\":22262},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23884,\"start\":23861},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":27938,\"start\":27921},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":28672,\"start\":28652},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":32103,\"start\":32081},{\"end\":38265,\"start\":38243},{\"end\":38811,\"start\":38780},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":54182,\"start\":54165}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":42196,\"start\":42145},{\"attributes\":{\"id\":\"fig_1\"},\"end\":42295,\"start\":42197},{\"attributes\":{\"id\":\"fig_2\"},\"end\":42764,\"start\":42296},{\"attributes\":{\"id\":\"fig_3\"},\"end\":43077,\"start\":42765},{\"attributes\":{\"id\":\"fig_4\"},\"end\":43879,\"start\":43078},{\"attributes\":{\"id\":\"fig_5\"},\"end\":43963,\"start\":43880},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":44589,\"start\":43964},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":44621,\"start\":44590},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":45890,\"start\":44622},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":45960,\"start\":45891},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":46359,\"start\":45961},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":47510,\"start\":46360},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":47551,\"start\":47511},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":48537,\"start\":47552},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":49030,\"start\":48538},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":49656,\"start\":49031},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":50080,\"start\":49657},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":50818,\"start\":50081},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":51449,\"start\":50819},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":51979,\"start\":51450},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":52324,\"start\":51980},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":53437,\"start\":52325},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":53556,\"start\":53438}]", "paragraph": "[{\"end\":2688,\"start\":1943},{\"end\":3465,\"start\":2690},{\"end\":4586,\"start\":3467},{\"end\":5222,\"start\":4588},{\"end\":6050,\"start\":5224},{\"end\":7094,\"start\":6067},{\"end\":7699,\"start\":7096},{\"end\":8798,\"start\":7701},{\"end\":9689,\"start\":8800},{\"end\":9948,\"start\":9751},{\"end\":10215,\"start\":9997},{\"end\":10415,\"start\":10260},{\"end\":10433,\"start\":10417},{\"end\":10882,\"start\":10435},{\"end\":11241,\"start\":10884},{\"end\":11320,\"start\":11299},{\"end\":12040,\"start\":11483},{\"end\":12758,\"start\":12138},{\"end\":13707,\"start\":12823},{\"end\":14396,\"start\":13709},{\"end\":14877,\"start\":14398},{\"end\":15284,\"start\":14879},{\"end\":16009,\"start\":15286},{\"end\":16164,\"start\":16011},{\"end\":16834,\"start\":16166},{\"end\":17753,\"start\":16880},{\"end\":18734,\"start\":17776},{\"end\":19336,\"start\":18736},{\"end\":19900,\"start\":19338},{\"end\":20573,\"start\":19902},{\"end\":21121,\"start\":20575},{\"end\":22370,\"start\":21123},{\"end\":23527,\"start\":22415},{\"end\":24187,\"start\":23529},{\"end\":25083,\"start\":24189},{\"end\":25708,\"start\":25094},{\"end\":26285,\"start\":25710},{\"end\":26479,\"start\":26287},{\"end\":27149,\"start\":26481},{\"end\":27749,\"start\":27151},{\"end\":28033,\"start\":27751},{\"end\":28903,\"start\":28035},{\"end\":29681,\"start\":28905},{\"end\":30649,\"start\":29683},{\"end\":30942,\"start\":30651},{\"end\":31897,\"start\":30957},{\"end\":32176,\"start\":31966},{\"end\":32509,\"start\":32221},{\"end\":32660,\"start\":32511},{\"end\":32796,\"start\":32687},{\"end\":33048,\"start\":32798},{\"end\":33489,\"start\":33268},{\"end\":33847,\"start\":33491},{\"end\":34133,\"start\":33900},{\"end\":34193,\"start\":34135},{\"end\":35306,\"start\":34195},{\"end\":35492,\"start\":35373},{\"end\":35564,\"start\":35494},{\"end\":36401,\"start\":35566},{\"end\":37106,\"start\":36403},{\"end\":37715,\"start\":37108},{\"end\":37926,\"start\":37717},{\"end\":38171,\"start\":37928},{\"end\":38692,\"start\":38209},{\"end\":39661,\"start\":38728},{\"end\":39794,\"start\":39663},{\"end\":40058,\"start\":39796},{\"end\":40286,\"start\":40060},{\"end\":40660,\"start\":40288},{\"end\":40750,\"start\":40662},{\"end\":41989,\"start\":40786},{\"end\":42052,\"start\":41991},{\"end\":42144,\"start\":42054}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9996,\"start\":9949},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10259,\"start\":10216},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11298,\"start\":11242},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11482,\"start\":11321},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12137,\"start\":12041},{\"attributes\":{\"id\":\"formula_5\"},\"end\":31965,\"start\":31898},{\"attributes\":{\"id\":\"formula_6\"},\"end\":33267,\"start\":33049}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19849,\"start\":19842},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":23559,\"start\":23552},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":24323,\"start\":24316},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":24372,\"start\":24365},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":24793,\"start\":24785},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25747,\"start\":25740},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27762,\"start\":27754},{\"end\":30056,\"start\":30049},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":30855,\"start\":30848},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":32231,\"start\":32224},{\"end\":32531,\"start\":32524},{\"end\":33640,\"start\":33633},{\"end\":39254,\"start\":39247},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":40804,\"start\":40793},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":40975,\"start\":40968},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":42065,\"start\":42057}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1941,\"start\":1929},{\"attributes\":{\"n\":\"2\"},\"end\":6065,\"start\":6053},{\"attributes\":{\"n\":\"3\"},\"end\":9749,\"start\":9692},{\"attributes\":{\"n\":\"3.1\"},\"end\":12821,\"start\":12761},{\"attributes\":{\"n\":\"3.2\"},\"end\":16878,\"start\":16837},{\"attributes\":{\"n\":\"4\"},\"end\":17774,\"start\":17756},{\"attributes\":{\"n\":\"5\"},\"end\":22380,\"start\":22373},{\"attributes\":{\"n\":\"5.1\"},\"end\":22413,\"start\":22383},{\"end\":25092,\"start\":25086},{\"attributes\":{\"n\":\"6\"},\"end\":30955,\"start\":30945},{\"end\":32219,\"start\":32179},{\"end\":32685,\"start\":32663},{\"end\":33898,\"start\":33850},{\"end\":35371,\"start\":35309},{\"end\":38207,\"start\":38174},{\"end\":38726,\"start\":38695},{\"end\":40784,\"start\":40753},{\"end\":42156,\"start\":42146},{\"end\":42208,\"start\":42198},{\"end\":42307,\"start\":42297},{\"end\":42770,\"start\":42766},{\"end\":43089,\"start\":43079},{\"end\":43891,\"start\":43881},{\"end\":44600,\"start\":44591},{\"end\":44632,\"start\":44623},{\"end\":45901,\"start\":45892},{\"end\":46370,\"start\":46361},{\"end\":47521,\"start\":47512},{\"end\":47562,\"start\":47553},{\"end\":48548,\"start\":48539},{\"end\":49041,\"start\":49032},{\"end\":49668,\"start\":49658},{\"end\":50092,\"start\":50082},{\"end\":50830,\"start\":50820},{\"end\":51461,\"start\":51451},{\"end\":51991,\"start\":51981},{\"end\":53449,\"start\":53439}]", "table": "[{\"end\":44589,\"start\":43983},{\"end\":45890,\"start\":44663},{\"end\":46359,\"start\":45964},{\"end\":47510,\"start\":46413},{\"end\":48537,\"start\":47803},{\"end\":49030,\"start\":48648},{\"end\":49656,\"start\":49251},{\"end\":50080,\"start\":49933},{\"end\":50818,\"start\":50459},{\"end\":51449,\"start\":51301},{\"end\":51979,\"start\":51815},{\"end\":52324,\"start\":52220},{\"end\":53437,\"start\":53067},{\"end\":53556,\"start\":53553}]", "figure_caption": "[{\"end\":42196,\"start\":42158},{\"end\":42295,\"start\":42210},{\"end\":42764,\"start\":42309},{\"end\":43077,\"start\":42771},{\"end\":43879,\"start\":43091},{\"end\":43963,\"start\":43893},{\"end\":43983,\"start\":43966},{\"end\":44621,\"start\":44602},{\"end\":44663,\"start\":44634},{\"end\":45960,\"start\":45903},{\"end\":45964,\"start\":45963},{\"end\":46413,\"start\":46372},{\"end\":47551,\"start\":47523},{\"end\":47803,\"start\":47564},{\"end\":48648,\"start\":48550},{\"end\":49251,\"start\":49043},{\"end\":49933,\"start\":49671},{\"end\":50459,\"start\":50095},{\"end\":51301,\"start\":50833},{\"end\":51815,\"start\":51464},{\"end\":52220,\"start\":51994},{\"end\":53067,\"start\":52327},{\"end\":53553,\"start\":53452}]", "figure_ref": "[{\"end\":2686,\"start\":2680},{\"end\":4051,\"start\":4043},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13369,\"start\":13363},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14686,\"start\":14679},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18162,\"start\":18156},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18353,\"start\":18347},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26880,\"start\":26872},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":28232,\"start\":28226},{\"end\":29319,\"start\":29311},{\"end\":29755,\"start\":29749},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":34810,\"start\":34802},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":41245,\"start\":41237}]", "bib_author_first_name": "[{\"end\":54577,\"start\":54571},{\"end\":54590,\"start\":54587},{\"end\":54600,\"start\":54596},{\"end\":54609,\"start\":54606},{\"end\":55568,\"start\":55559},{\"end\":55584,\"start\":55580},{\"end\":55586,\"start\":55585},{\"end\":55602,\"start\":55593},{\"end\":55616,\"start\":55609},{\"end\":56332,\"start\":56323},{\"end\":56348,\"start\":56344},{\"end\":56350,\"start\":56349},{\"end\":56366,\"start\":56357},{\"end\":56380,\"start\":56373},{\"end\":57114,\"start\":57105},{\"end\":57129,\"start\":57126},{\"end\":57143,\"start\":57136},{\"end\":57449,\"start\":57440},{\"end\":57465,\"start\":57461},{\"end\":57487,\"start\":57480},{\"end\":57842,\"start\":57839},{\"end\":57855,\"start\":57849},{\"end\":57873,\"start\":57869},{\"end\":57884,\"start\":57880},{\"end\":58648,\"start\":58645},{\"end\":58661,\"start\":58655},{\"end\":58673,\"start\":58669},{\"end\":58684,\"start\":58680},{\"end\":59034,\"start\":59031},{\"end\":59047,\"start\":59041},{\"end\":59065,\"start\":59061},{\"end\":59076,\"start\":59072},{\"end\":59321,\"start\":59320},{\"end\":59336,\"start\":59329},{\"end\":59338,\"start\":59337},{\"end\":59356,\"start\":59350},{\"end\":59372,\"start\":59366},{\"end\":59388,\"start\":59380},{\"end\":59414,\"start\":59407},{\"end\":59431,\"start\":59425},{\"end\":59449,\"start\":59444},{\"end\":59465,\"start\":59461},{\"end\":59478,\"start\":59474},{\"end\":59951,\"start\":59943},{\"end\":59967,\"start\":59964},{\"end\":59985,\"start\":59981},{\"end\":60001,\"start\":59992},{\"end\":60273,\"start\":60265},{\"end\":60290,\"start\":60283},{\"end\":60305,\"start\":60300},{\"end\":60321,\"start\":60315},{\"end\":61012,\"start\":61007},{\"end\":61028,\"start\":61021},{\"end\":61044,\"start\":61038},{\"end\":61325,\"start\":61315},{\"end\":61346,\"start\":61338},{\"end\":61362,\"start\":61357},{\"end\":61376,\"start\":61372},{\"end\":61391,\"start\":61385},{\"end\":61406,\"start\":61399},{\"end\":61427,\"start\":61421},{\"end\":61444,\"start\":61437},{\"end\":62292,\"start\":62287},{\"end\":62490,\"start\":62484},{\"end\":62509,\"start\":62500},{\"end\":63113,\"start\":63107},{\"end\":63121,\"start\":63119},{\"end\":63129,\"start\":63127},{\"end\":63440,\"start\":63434},{\"end\":63452,\"start\":63446},{\"end\":63465,\"start\":63459},{\"end\":63477,\"start\":63472},{\"end\":63490,\"start\":63489},{\"end\":63503,\"start\":63497},{\"end\":63804,\"start\":63801},{\"end\":63822,\"start\":63816},{\"end\":63834,\"start\":63829},{\"end\":64214,\"start\":64207},{\"end\":64233,\"start\":64227},{\"end\":64249,\"start\":64243},{\"end\":64610,\"start\":64604},{\"end\":64634,\"start\":64629},{\"end\":64652,\"start\":64646},{\"end\":64674,\"start\":64667},{\"end\":64690,\"start\":64686},{\"end\":64705,\"start\":64699},{\"end\":65091,\"start\":65084},{\"end\":65112,\"start\":65103},{\"end\":65128,\"start\":65124},{\"end\":65456,\"start\":65451},{\"end\":65471,\"start\":65463},{\"end\":65490,\"start\":65484},{\"end\":65916,\"start\":65909},{\"end\":65918,\"start\":65917},{\"end\":65932,\"start\":65929},{\"end\":65943,\"start\":65939},{\"end\":66119,\"start\":66118},{\"end\":66136,\"start\":66129},{\"end\":66157,\"start\":66150},{\"end\":66172,\"start\":66166},{\"end\":66185,\"start\":66180},{\"end\":66538,\"start\":66533},{\"end\":66551,\"start\":66547},{\"end\":66553,\"start\":66552},{\"end\":66568,\"start\":66560},{\"end\":66577,\"start\":66573},{\"end\":66594,\"start\":66585},{\"end\":67060,\"start\":67056},{\"end\":67079,\"start\":67073},{\"end\":67292,\"start\":67286},{\"end\":67305,\"start\":67297},{\"end\":67318,\"start\":67311},{\"end\":67333,\"start\":67327},{\"end\":67346,\"start\":67340},{\"end\":67357,\"start\":67352},{\"end\":67370,\"start\":67363},{\"end\":67384,\"start\":67380},{\"end\":68071,\"start\":68066},{\"end\":68089,\"start\":68080},{\"end\":68101,\"start\":68094},{\"end\":68118,\"start\":68109},{\"end\":68134,\"start\":68130},{\"end\":68444,\"start\":68438},{\"end\":68459,\"start\":68453},{\"end\":68475,\"start\":68470},{\"end\":68496,\"start\":68490},{\"end\":68901,\"start\":68900},{\"end\":68913,\"start\":68910},{\"end\":69242,\"start\":69234},{\"end\":69263,\"start\":69253},{\"end\":69283,\"start\":69276},{\"end\":69610,\"start\":69602},{\"end\":69627,\"start\":69621},{\"end\":69649,\"start\":69642},{\"end\":69956,\"start\":69948},{\"end\":69972,\"start\":69967},{\"end\":69986,\"start\":69979},{\"end\":70243,\"start\":70239},{\"end\":70251,\"start\":70248},{\"end\":70263,\"start\":70258},{\"end\":70568,\"start\":70563},{\"end\":70579,\"start\":70573},{\"end\":70592,\"start\":70588},{\"end\":70614,\"start\":70607},{\"end\":70830,\"start\":70823},{\"end\":70855,\"start\":70847},{\"end\":71098,\"start\":71093},{\"end\":71115,\"start\":71109},{\"end\":71420,\"start\":71416},{\"end\":71428,\"start\":71426},{\"end\":71435,\"start\":71433},{\"end\":71446,\"start\":71443},{\"end\":71456,\"start\":71453},{\"end\":71938,\"start\":71935},{\"end\":71943,\"start\":71939},{\"end\":71966,\"start\":71962},{\"end\":71979,\"start\":71975},{\"end\":71981,\"start\":71980},{\"end\":72584,\"start\":72576},{\"end\":72597,\"start\":72591},{\"end\":72610,\"start\":72604},{\"end\":72625,\"start\":72620},{\"end\":72890,\"start\":72886},{\"end\":72907,\"start\":72900},{\"end\":72917,\"start\":72912},{\"end\":72930,\"start\":72925},{\"end\":72942,\"start\":72937},{\"end\":72955,\"start\":72951},{\"end\":73206,\"start\":73199},{\"end\":73222,\"start\":73216},{\"end\":73235,\"start\":73231},{\"end\":73936,\"start\":73930},{\"end\":73953,\"start\":73948},{\"end\":73962,\"start\":73960},{\"end\":73981,\"start\":73975},{\"end\":74004,\"start\":73996},{\"end\":74306,\"start\":74299},{\"end\":74328,\"start\":74322},{\"end\":74330,\"start\":74329},{\"end\":74342,\"start\":74337},{\"end\":74356,\"start\":74350},{\"end\":74370,\"start\":74366},{\"end\":74380,\"start\":74377},{\"end\":75060,\"start\":75055},{\"end\":75072,\"start\":75067},{\"end\":75090,\"start\":75084},{\"end\":75994,\"start\":75988},{\"end\":76006,\"start\":76001},{\"end\":76022,\"start\":76012},{\"end\":76037,\"start\":76032},{\"end\":76429,\"start\":76421},{\"end\":76441,\"start\":76436},{\"end\":76757,\"start\":76750},{\"end\":76770,\"start\":76766},{\"end\":76786,\"start\":76782},{\"end\":76796,\"start\":76791},{\"end\":76806,\"start\":76805},{\"end\":76830,\"start\":76829},{\"end\":76850,\"start\":76839},{\"end\":77426,\"start\":77417},{\"end\":77439,\"start\":77432},{\"end\":77461,\"start\":77448},{\"end\":77720,\"start\":77714},{\"end\":77734,\"start\":77730},{\"end\":77748,\"start\":77744},{\"end\":77762,\"start\":77757},{\"end\":77779,\"start\":77774},{\"end\":77792,\"start\":77787},{\"end\":77794,\"start\":77793},{\"end\":77808,\"start\":77802},{\"end\":77822,\"start\":77817},{\"end\":78119,\"start\":78114},{\"end\":78139,\"start\":78132},{\"end\":78157,\"start\":78150},{\"end\":78175,\"start\":78168},{\"end\":78190,\"start\":78184},{\"end\":78202,\"start\":78196},{\"end\":78549,\"start\":78544},{\"end\":78564,\"start\":78559},{\"end\":78583,\"start\":78576},{\"end\":78844,\"start\":78840},{\"end\":78856,\"start\":78851},{\"end\":78870,\"start\":78866},{\"end\":78893,\"start\":78890},{\"end\":79603,\"start\":79597},{\"end\":79614,\"start\":79608},{\"end\":79623,\"start\":79619},{\"end\":79642,\"start\":79634},{\"end\":79998,\"start\":79992},{\"end\":80011,\"start\":80005},{\"end\":80023,\"start\":80017},{\"end\":80035,\"start\":80030},{\"end\":80048,\"start\":80047},{\"end\":80061,\"start\":80055},{\"end\":80452,\"start\":80446},{\"end\":80466,\"start\":80459},{\"end\":80483,\"start\":80472},{\"end\":80497,\"start\":80492},{\"end\":80507,\"start\":80503},{\"end\":80522,\"start\":80518},{\"end\":80872,\"start\":80865},{\"end\":80881,\"start\":80878},{\"end\":80892,\"start\":80888},{\"end\":81478,\"start\":81473},{\"end\":81491,\"start\":81486}]", "bib_author_last_name": "[{\"end\":54585,\"start\":54578},{\"end\":54594,\"start\":54591},{\"end\":54604,\"start\":54601},{\"end\":54612,\"start\":54610},{\"end\":55578,\"start\":55569},{\"end\":55591,\"start\":55587},{\"end\":55607,\"start\":55603},{\"end\":55623,\"start\":55617},{\"end\":56342,\"start\":56333},{\"end\":56355,\"start\":56351},{\"end\":56371,\"start\":56367},{\"end\":56387,\"start\":56381},{\"end\":57124,\"start\":57115},{\"end\":57134,\"start\":57130},{\"end\":57150,\"start\":57144},{\"end\":57459,\"start\":57450},{\"end\":57478,\"start\":57466},{\"end\":57495,\"start\":57488},{\"end\":57847,\"start\":57843},{\"end\":57867,\"start\":57856},{\"end\":57878,\"start\":57874},{\"end\":57890,\"start\":57885},{\"end\":58653,\"start\":58649},{\"end\":58667,\"start\":58662},{\"end\":58678,\"start\":58674},{\"end\":58690,\"start\":58685},{\"end\":59039,\"start\":59035},{\"end\":59059,\"start\":59048},{\"end\":59070,\"start\":59066},{\"end\":59082,\"start\":59077},{\"end\":59327,\"start\":59322},{\"end\":59348,\"start\":59339},{\"end\":59364,\"start\":59357},{\"end\":59378,\"start\":59373},{\"end\":59405,\"start\":59389},{\"end\":59423,\"start\":59415},{\"end\":59442,\"start\":59432},{\"end\":59459,\"start\":59450},{\"end\":59472,\"start\":59466},{\"end\":59486,\"start\":59479},{\"end\":59496,\"start\":59488},{\"end\":59962,\"start\":59952},{\"end\":59979,\"start\":59968},{\"end\":59990,\"start\":59986},{\"end\":60008,\"start\":60002},{\"end\":60281,\"start\":60274},{\"end\":60298,\"start\":60291},{\"end\":60313,\"start\":60306},{\"end\":60328,\"start\":60322},{\"end\":61019,\"start\":61013},{\"end\":61036,\"start\":61029},{\"end\":61051,\"start\":61045},{\"end\":61336,\"start\":61326},{\"end\":61355,\"start\":61347},{\"end\":61370,\"start\":61363},{\"end\":61383,\"start\":61377},{\"end\":61397,\"start\":61392},{\"end\":61419,\"start\":61407},{\"end\":61435,\"start\":61428},{\"end\":61454,\"start\":61445},{\"end\":62302,\"start\":62293},{\"end\":62498,\"start\":62491},{\"end\":62516,\"start\":62510},{\"end\":63117,\"start\":63114},{\"end\":63125,\"start\":63122},{\"end\":63134,\"start\":63130},{\"end\":63444,\"start\":63441},{\"end\":63457,\"start\":63453},{\"end\":63470,\"start\":63466},{\"end\":63487,\"start\":63478},{\"end\":63495,\"start\":63491},{\"end\":63506,\"start\":63504},{\"end\":63521,\"start\":63508},{\"end\":63814,\"start\":63805},{\"end\":63827,\"start\":63823},{\"end\":63839,\"start\":63835},{\"end\":64225,\"start\":64215},{\"end\":64241,\"start\":64234},{\"end\":64263,\"start\":64250},{\"end\":64627,\"start\":64611},{\"end\":64644,\"start\":64635},{\"end\":64665,\"start\":64653},{\"end\":64684,\"start\":64675},{\"end\":64697,\"start\":64691},{\"end\":64718,\"start\":64706},{\"end\":64725,\"start\":64720},{\"end\":65101,\"start\":65092},{\"end\":65122,\"start\":65113},{\"end\":65141,\"start\":65129},{\"end\":65461,\"start\":65457},{\"end\":65482,\"start\":65472},{\"end\":65500,\"start\":65491},{\"end\":65927,\"start\":65919},{\"end\":65937,\"start\":65933},{\"end\":65952,\"start\":65944},{\"end\":66127,\"start\":66120},{\"end\":66148,\"start\":66137},{\"end\":66164,\"start\":66158},{\"end\":66178,\"start\":66173},{\"end\":66194,\"start\":66186},{\"end\":66202,\"start\":66196},{\"end\":66545,\"start\":66539},{\"end\":66558,\"start\":66554},{\"end\":66571,\"start\":66569},{\"end\":66583,\"start\":66578},{\"end\":66602,\"start\":66595},{\"end\":67071,\"start\":67061},{\"end\":67091,\"start\":67080},{\"end\":67295,\"start\":67293},{\"end\":67309,\"start\":67306},{\"end\":67325,\"start\":67319},{\"end\":67338,\"start\":67334},{\"end\":67350,\"start\":67347},{\"end\":67361,\"start\":67358},{\"end\":67378,\"start\":67371},{\"end\":67393,\"start\":67385},{\"end\":68078,\"start\":68072},{\"end\":68092,\"start\":68090},{\"end\":68107,\"start\":68102},{\"end\":68128,\"start\":68119},{\"end\":68147,\"start\":68135},{\"end\":68451,\"start\":68445},{\"end\":68468,\"start\":68460},{\"end\":68488,\"start\":68476},{\"end\":68500,\"start\":68497},{\"end\":68908,\"start\":68902},{\"end\":68918,\"start\":68914},{\"end\":68927,\"start\":68920},{\"end\":69251,\"start\":69243},{\"end\":69274,\"start\":69264},{\"end\":69293,\"start\":69284},{\"end\":69619,\"start\":69611},{\"end\":69640,\"start\":69628},{\"end\":69659,\"start\":69650},{\"end\":69965,\"start\":69957},{\"end\":69977,\"start\":69973},{\"end\":69996,\"start\":69987},{\"end\":70246,\"start\":70244},{\"end\":70256,\"start\":70252},{\"end\":70277,\"start\":70264},{\"end\":70283,\"start\":70279},{\"end\":70571,\"start\":70569},{\"end\":70586,\"start\":70580},{\"end\":70605,\"start\":70593},{\"end\":70620,\"start\":70615},{\"end\":70845,\"start\":70831},{\"end\":70862,\"start\":70856},{\"end\":71107,\"start\":71099},{\"end\":71122,\"start\":71116},{\"end\":71424,\"start\":71421},{\"end\":71431,\"start\":71429},{\"end\":71441,\"start\":71436},{\"end\":71451,\"start\":71447},{\"end\":71460,\"start\":71457},{\"end\":71960,\"start\":71944},{\"end\":71973,\"start\":71967},{\"end\":71991,\"start\":71982},{\"end\":71999,\"start\":71993},{\"end\":72589,\"start\":72585},{\"end\":72602,\"start\":72598},{\"end\":72618,\"start\":72611},{\"end\":72634,\"start\":72626},{\"end\":72898,\"start\":72891},{\"end\":72910,\"start\":72908},{\"end\":72923,\"start\":72918},{\"end\":72935,\"start\":72931},{\"end\":72949,\"start\":72943},{\"end\":72965,\"start\":72956},{\"end\":73214,\"start\":73207},{\"end\":73229,\"start\":73223},{\"end\":73241,\"start\":73236},{\"end\":73946,\"start\":73937},{\"end\":73958,\"start\":73954},{\"end\":73973,\"start\":73963},{\"end\":73994,\"start\":73982},{\"end\":74015,\"start\":74005},{\"end\":74320,\"start\":74307},{\"end\":74335,\"start\":74331},{\"end\":74348,\"start\":74343},{\"end\":74364,\"start\":74357},{\"end\":74375,\"start\":74371},{\"end\":74386,\"start\":74381},{\"end\":74395,\"start\":74388},{\"end\":75065,\"start\":75061},{\"end\":75082,\"start\":75073},{\"end\":75098,\"start\":75091},{\"end\":75999,\"start\":75995},{\"end\":76010,\"start\":76007},{\"end\":76030,\"start\":76023},{\"end\":76047,\"start\":76038},{\"end\":76434,\"start\":76430},{\"end\":76447,\"start\":76442},{\"end\":76764,\"start\":76758},{\"end\":76780,\"start\":76771},{\"end\":76789,\"start\":76787},{\"end\":76803,\"start\":76797},{\"end\":76818,\"start\":76807},{\"end\":76827,\"start\":76820},{\"end\":76837,\"start\":76831},{\"end\":76853,\"start\":76851},{\"end\":76860,\"start\":76855},{\"end\":77430,\"start\":77427},{\"end\":77446,\"start\":77440},{\"end\":77469,\"start\":77462},{\"end\":77728,\"start\":77721},{\"end\":77742,\"start\":77735},{\"end\":77755,\"start\":77749},{\"end\":77772,\"start\":77763},{\"end\":77785,\"start\":77780},{\"end\":77800,\"start\":77795},{\"end\":77815,\"start\":77809},{\"end\":77833,\"start\":77823},{\"end\":78130,\"start\":78120},{\"end\":78148,\"start\":78140},{\"end\":78166,\"start\":78158},{\"end\":78182,\"start\":78176},{\"end\":78194,\"start\":78191},{\"end\":78209,\"start\":78203},{\"end\":78557,\"start\":78550},{\"end\":78574,\"start\":78565},{\"end\":78590,\"start\":78584},{\"end\":78849,\"start\":78845},{\"end\":78864,\"start\":78857},{\"end\":78888,\"start\":78871},{\"end\":78897,\"start\":78894},{\"end\":79606,\"start\":79604},{\"end\":79617,\"start\":79615},{\"end\":79632,\"start\":79624},{\"end\":79650,\"start\":79643},{\"end\":80003,\"start\":79999},{\"end\":80015,\"start\":80012},{\"end\":80028,\"start\":80024},{\"end\":80045,\"start\":80036},{\"end\":80053,\"start\":80049},{\"end\":80075,\"start\":80062},{\"end\":80079,\"start\":80077},{\"end\":80457,\"start\":80453},{\"end\":80470,\"start\":80467},{\"end\":80490,\"start\":80484},{\"end\":80501,\"start\":80498},{\"end\":80516,\"start\":80508},{\"end\":80531,\"start\":80523},{\"end\":80876,\"start\":80873},{\"end\":80886,\"start\":80882},{\"end\":80901,\"start\":80893},{\"end\":81484,\"start\":81479},{\"end\":81496,\"start\":81492}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1145/1287624.1287630\",\"id\":\"b0\",\"matched_paper_id\":532398},\"end\":55520,\"start\":54475},{\"attributes\":{\"doi\":\"10.1145/2635868.2635883\",\"id\":\"b1\",\"matched_paper_id\":2437629},\"end\":56277,\"start\":55522},{\"attributes\":{\"doi\":\"10.1145/2786805.2786849\",\"id\":\"b2\",\"matched_paper_id\":9279336},\"end\":57027,\"start\":56279},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2723946},\"end\":57394,\"start\":57029},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3495200},\"end\":57766,\"start\":57396},{\"attributes\":{\"doi\":\"10.1145/3192366.3192412\",\"id\":\"b5\",\"matched_paper_id\":4383884},\"end\":58581,\"start\":57768},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":51926976},\"end\":58983,\"start\":58583},{\"attributes\":{\"id\":\"b7\"},\"end\":59318,\"start\":58985},{\"attributes\":{\"doi\":\"arXiv:1806.01261\",\"id\":\"b8\"},\"end\":59878,\"start\":59320},{\"attributes\":{\"doi\":\"arXiv:1611.08307\",\"id\":\"b9\"},\"end\":60212,\"start\":59880},{\"attributes\":{\"doi\":\"10.1145/2976749.2978422\",\"id\":\"b10\",\"matched_paper_id\":18882938},\"end\":60969,\"start\":60214},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":13020969},\"end\":61256,\"start\":60971},{\"attributes\":{\"doi\":\"10.1145/3394486.3403296\",\"id\":\"b12\",\"matched_paper_id\":220347100},\"end\":62217,\"start\":61258},{\"attributes\":{\"id\":\"b13\"},\"end\":62440,\"start\":62219},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":58981712},\"end\":63032,\"start\":62442},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":2708270},\"end\":63359,\"start\":63034},{\"attributes\":{\"doi\":\"arXiv:1901.02860\",\"id\":\"b16\"},\"end\":63758,\"start\":63361},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":9146334},\"end\":64125,\"start\":63760},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3016223},\"end\":64532,\"start\":64127},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1690180},\"end\":65049,\"start\":64534},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":53216170},\"end\":65406,\"start\":65051},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":20480879},\"end\":65856,\"start\":65408},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":4755450},\"end\":66075,\"start\":65858},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":213352113},\"end\":66499,\"start\":66077},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2846066},\"end\":67030,\"start\":66501},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1915014},\"end\":67221,\"start\":67032},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":218487328},\"end\":67993,\"start\":67223},{\"attributes\":{\"doi\":\"arXiv:1909.09436\",\"id\":\"b27\"},\"end\":68375,\"start\":67995},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":220425306},\"end\":68832,\"start\":68377},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3144218},\"end\":69158,\"start\":68834},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":67855539},\"end\":69565,\"start\":69160},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":202783932},\"end\":69896,\"start\":69567},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":211842237},\"end\":70237,\"start\":69898},{\"attributes\":{\"doi\":\"arXiv:1711.09573\",\"id\":\"b33\"},\"end\":70523,\"start\":70239},{\"attributes\":{\"doi\":\"arXiv:1511.05493\",\"id\":\"b34\"},\"end\":70791,\"start\":70525},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":5855042},\"end\":71038,\"start\":70793},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":5737841},\"end\":71326,\"start\":71040},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":1914494},\"end\":71876,\"start\":71328},{\"attributes\":{\"doi\":\"10.1109/ICSE.2017.47\",\"id\":\"b38\",\"matched_paper_id\":25324992},\"end\":72516,\"start\":71878},{\"attributes\":{\"id\":\"b39\"},\"end\":72831,\"start\":72518},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":160025533},\"end\":73147,\"start\":72833},{\"attributes\":{\"doi\":\"10.1145/2594291.2594321\",\"id\":\"b41\",\"matched_paper_id\":13040187},\"end\":73896,\"start\":73149},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":206756462},\"end\":74237,\"start\":73898},{\"attributes\":{\"doi\":\"978-3-319-93417-4\",\"id\":\"b43\"},\"end\":74998,\"start\":74239},{\"attributes\":{\"doi\":\"10.18653/v1/N18-2074\",\"id\":\"b44\",\"matched_paper_id\":3725815},\"end\":75909,\"start\":75000},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":53034786},\"end\":76357,\"start\":75911},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":202784970},\"end\":76669,\"start\":76359},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":990233},\"end\":77327,\"start\":76671},{\"attributes\":{\"doi\":\"arXiv:1503.00075\",\"id\":\"b48\"},\"end\":77685,\"start\":77329},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":13756489},\"end\":78112,\"start\":77687},{\"attributes\":{\"id\":\"b50\"},\"end\":78524,\"start\":78114},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":5692837},\"end\":78787,\"start\":78526},{\"attributes\":{\"doi\":\"10.1145/2970276.2970341\",\"id\":\"b52\",\"matched_paper_id\":1667047},\"end\":79554,\"start\":78789},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":52895589},\"end\":79916,\"start\":79556},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":195069387},\"end\":80372,\"start\":79918},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":49420315},\"end\":80825,\"start\":80374},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b56\",\"matched_paper_id\":174800449},\"end\":81423,\"start\":80827},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":3573161},\"end\":81699,\"start\":81425}]", "bib_title": "[{\"end\":54569,\"start\":54475},{\"end\":55557,\"start\":55522},{\"end\":56321,\"start\":56279},{\"end\":57103,\"start\":57029},{\"end\":57438,\"start\":57396},{\"end\":57837,\"start\":57768},{\"end\":58643,\"start\":58583},{\"end\":59029,\"start\":58985},{\"end\":60263,\"start\":60214},{\"end\":61005,\"start\":60971},{\"end\":61313,\"start\":61258},{\"end\":62482,\"start\":62442},{\"end\":63105,\"start\":63034},{\"end\":63799,\"start\":63760},{\"end\":64205,\"start\":64127},{\"end\":64602,\"start\":64534},{\"end\":65082,\"start\":65051},{\"end\":65449,\"start\":65408},{\"end\":65907,\"start\":65858},{\"end\":66116,\"start\":66077},{\"end\":66531,\"start\":66501},{\"end\":67054,\"start\":67032},{\"end\":67284,\"start\":67223},{\"end\":68436,\"start\":68377},{\"end\":68898,\"start\":68834},{\"end\":69232,\"start\":69160},{\"end\":69600,\"start\":69567},{\"end\":69946,\"start\":69898},{\"end\":70821,\"start\":70793},{\"end\":71091,\"start\":71040},{\"end\":71414,\"start\":71328},{\"end\":71933,\"start\":71878},{\"end\":72884,\"start\":72833},{\"end\":73197,\"start\":73149},{\"end\":73928,\"start\":73898},{\"end\":75053,\"start\":75000},{\"end\":75986,\"start\":75911},{\"end\":76419,\"start\":76359},{\"end\":76748,\"start\":76671},{\"end\":77712,\"start\":77687},{\"end\":78542,\"start\":78526},{\"end\":78838,\"start\":78789},{\"end\":79595,\"start\":79556},{\"end\":79990,\"start\":79918},{\"end\":80444,\"start\":80374},{\"end\":80863,\"start\":80827},{\"end\":81471,\"start\":81425}]", "bib_author": "[{\"end\":54587,\"start\":54571},{\"end\":54596,\"start\":54587},{\"end\":54606,\"start\":54596},{\"end\":54614,\"start\":54606},{\"end\":55580,\"start\":55559},{\"end\":55593,\"start\":55580},{\"end\":55609,\"start\":55593},{\"end\":55625,\"start\":55609},{\"end\":56344,\"start\":56323},{\"end\":56357,\"start\":56344},{\"end\":56373,\"start\":56357},{\"end\":56389,\"start\":56373},{\"end\":57126,\"start\":57105},{\"end\":57136,\"start\":57126},{\"end\":57152,\"start\":57136},{\"end\":57461,\"start\":57440},{\"end\":57480,\"start\":57461},{\"end\":57497,\"start\":57480},{\"end\":57849,\"start\":57839},{\"end\":57869,\"start\":57849},{\"end\":57880,\"start\":57869},{\"end\":57892,\"start\":57880},{\"end\":58655,\"start\":58645},{\"end\":58669,\"start\":58655},{\"end\":58680,\"start\":58669},{\"end\":58692,\"start\":58680},{\"end\":59041,\"start\":59031},{\"end\":59061,\"start\":59041},{\"end\":59072,\"start\":59061},{\"end\":59084,\"start\":59072},{\"end\":59329,\"start\":59320},{\"end\":59350,\"start\":59329},{\"end\":59366,\"start\":59350},{\"end\":59380,\"start\":59366},{\"end\":59407,\"start\":59380},{\"end\":59425,\"start\":59407},{\"end\":59444,\"start\":59425},{\"end\":59461,\"start\":59444},{\"end\":59474,\"start\":59461},{\"end\":59488,\"start\":59474},{\"end\":59498,\"start\":59488},{\"end\":59964,\"start\":59943},{\"end\":59981,\"start\":59964},{\"end\":59992,\"start\":59981},{\"end\":60010,\"start\":59992},{\"end\":60283,\"start\":60265},{\"end\":60300,\"start\":60283},{\"end\":60315,\"start\":60300},{\"end\":60330,\"start\":60315},{\"end\":61021,\"start\":61007},{\"end\":61038,\"start\":61021},{\"end\":61053,\"start\":61038},{\"end\":61338,\"start\":61315},{\"end\":61357,\"start\":61338},{\"end\":61372,\"start\":61357},{\"end\":61385,\"start\":61372},{\"end\":61399,\"start\":61385},{\"end\":61421,\"start\":61399},{\"end\":61437,\"start\":61421},{\"end\":61456,\"start\":61437},{\"end\":62304,\"start\":62287},{\"end\":62500,\"start\":62484},{\"end\":62518,\"start\":62500},{\"end\":63119,\"start\":63107},{\"end\":63127,\"start\":63119},{\"end\":63136,\"start\":63127},{\"end\":63446,\"start\":63434},{\"end\":63459,\"start\":63446},{\"end\":63472,\"start\":63459},{\"end\":63489,\"start\":63472},{\"end\":63497,\"start\":63489},{\"end\":63508,\"start\":63497},{\"end\":63523,\"start\":63508},{\"end\":63816,\"start\":63801},{\"end\":63829,\"start\":63816},{\"end\":63841,\"start\":63829},{\"end\":64227,\"start\":64207},{\"end\":64243,\"start\":64227},{\"end\":64265,\"start\":64243},{\"end\":64629,\"start\":64604},{\"end\":64646,\"start\":64629},{\"end\":64667,\"start\":64646},{\"end\":64686,\"start\":64667},{\"end\":64699,\"start\":64686},{\"end\":64720,\"start\":64699},{\"end\":64727,\"start\":64720},{\"end\":65103,\"start\":65084},{\"end\":65124,\"start\":65103},{\"end\":65143,\"start\":65124},{\"end\":65463,\"start\":65451},{\"end\":65484,\"start\":65463},{\"end\":65502,\"start\":65484},{\"end\":65929,\"start\":65909},{\"end\":65939,\"start\":65929},{\"end\":65954,\"start\":65939},{\"end\":66129,\"start\":66118},{\"end\":66150,\"start\":66129},{\"end\":66166,\"start\":66150},{\"end\":66180,\"start\":66166},{\"end\":66196,\"start\":66180},{\"end\":66204,\"start\":66196},{\"end\":66547,\"start\":66533},{\"end\":66560,\"start\":66547},{\"end\":66573,\"start\":66560},{\"end\":66585,\"start\":66573},{\"end\":66604,\"start\":66585},{\"end\":67073,\"start\":67056},{\"end\":67093,\"start\":67073},{\"end\":67297,\"start\":67286},{\"end\":67311,\"start\":67297},{\"end\":67327,\"start\":67311},{\"end\":67340,\"start\":67327},{\"end\":67352,\"start\":67340},{\"end\":67363,\"start\":67352},{\"end\":67380,\"start\":67363},{\"end\":67395,\"start\":67380},{\"end\":68080,\"start\":68066},{\"end\":68094,\"start\":68080},{\"end\":68109,\"start\":68094},{\"end\":68130,\"start\":68109},{\"end\":68149,\"start\":68130},{\"end\":68453,\"start\":68438},{\"end\":68470,\"start\":68453},{\"end\":68490,\"start\":68470},{\"end\":68502,\"start\":68490},{\"end\":68910,\"start\":68900},{\"end\":68920,\"start\":68910},{\"end\":68929,\"start\":68920},{\"end\":69253,\"start\":69234},{\"end\":69276,\"start\":69253},{\"end\":69295,\"start\":69276},{\"end\":69621,\"start\":69602},{\"end\":69642,\"start\":69621},{\"end\":69661,\"start\":69642},{\"end\":69967,\"start\":69948},{\"end\":69979,\"start\":69967},{\"end\":69998,\"start\":69979},{\"end\":70248,\"start\":70239},{\"end\":70258,\"start\":70248},{\"end\":70279,\"start\":70258},{\"end\":70285,\"start\":70279},{\"end\":70573,\"start\":70563},{\"end\":70588,\"start\":70573},{\"end\":70607,\"start\":70588},{\"end\":70622,\"start\":70607},{\"end\":70847,\"start\":70823},{\"end\":70864,\"start\":70847},{\"end\":71109,\"start\":71093},{\"end\":71124,\"start\":71109},{\"end\":71426,\"start\":71416},{\"end\":71433,\"start\":71426},{\"end\":71443,\"start\":71433},{\"end\":71453,\"start\":71443},{\"end\":71462,\"start\":71453},{\"end\":71962,\"start\":71935},{\"end\":71975,\"start\":71962},{\"end\":71993,\"start\":71975},{\"end\":72001,\"start\":71993},{\"end\":72591,\"start\":72576},{\"end\":72604,\"start\":72591},{\"end\":72620,\"start\":72604},{\"end\":72636,\"start\":72620},{\"end\":72900,\"start\":72886},{\"end\":72912,\"start\":72900},{\"end\":72925,\"start\":72912},{\"end\":72937,\"start\":72925},{\"end\":72951,\"start\":72937},{\"end\":72967,\"start\":72951},{\"end\":73216,\"start\":73199},{\"end\":73231,\"start\":73216},{\"end\":73243,\"start\":73231},{\"end\":73948,\"start\":73930},{\"end\":73960,\"start\":73948},{\"end\":73975,\"start\":73960},{\"end\":73996,\"start\":73975},{\"end\":74017,\"start\":73996},{\"end\":74322,\"start\":74299},{\"end\":74337,\"start\":74322},{\"end\":74350,\"start\":74337},{\"end\":74366,\"start\":74350},{\"end\":74377,\"start\":74366},{\"end\":74388,\"start\":74377},{\"end\":74397,\"start\":74388},{\"end\":75067,\"start\":75055},{\"end\":75084,\"start\":75067},{\"end\":75100,\"start\":75084},{\"end\":76001,\"start\":75988},{\"end\":76012,\"start\":76001},{\"end\":76032,\"start\":76012},{\"end\":76049,\"start\":76032},{\"end\":76436,\"start\":76421},{\"end\":76449,\"start\":76436},{\"end\":76766,\"start\":76750},{\"end\":76782,\"start\":76766},{\"end\":76791,\"start\":76782},{\"end\":76805,\"start\":76791},{\"end\":76820,\"start\":76805},{\"end\":76829,\"start\":76820},{\"end\":76839,\"start\":76829},{\"end\":76855,\"start\":76839},{\"end\":76862,\"start\":76855},{\"end\":77432,\"start\":77417},{\"end\":77448,\"start\":77432},{\"end\":77471,\"start\":77448},{\"end\":77730,\"start\":77714},{\"end\":77744,\"start\":77730},{\"end\":77757,\"start\":77744},{\"end\":77774,\"start\":77757},{\"end\":77787,\"start\":77774},{\"end\":77802,\"start\":77787},{\"end\":77817,\"start\":77802},{\"end\":77835,\"start\":77817},{\"end\":78132,\"start\":78114},{\"end\":78150,\"start\":78132},{\"end\":78168,\"start\":78150},{\"end\":78184,\"start\":78168},{\"end\":78196,\"start\":78184},{\"end\":78211,\"start\":78196},{\"end\":78559,\"start\":78544},{\"end\":78576,\"start\":78559},{\"end\":78592,\"start\":78576},{\"end\":78851,\"start\":78840},{\"end\":78866,\"start\":78851},{\"end\":78890,\"start\":78866},{\"end\":78899,\"start\":78890},{\"end\":79608,\"start\":79597},{\"end\":79619,\"start\":79608},{\"end\":79634,\"start\":79619},{\"end\":79652,\"start\":79634},{\"end\":80005,\"start\":79992},{\"end\":80017,\"start\":80005},{\"end\":80030,\"start\":80017},{\"end\":80047,\"start\":80030},{\"end\":80055,\"start\":80047},{\"end\":80077,\"start\":80055},{\"end\":80081,\"start\":80077},{\"end\":80459,\"start\":80446},{\"end\":80472,\"start\":80459},{\"end\":80492,\"start\":80472},{\"end\":80503,\"start\":80492},{\"end\":80518,\"start\":80503},{\"end\":80533,\"start\":80518},{\"end\":80878,\"start\":80865},{\"end\":80888,\"start\":80878},{\"end\":80903,\"start\":80888},{\"end\":81486,\"start\":81473},{\"end\":81498,\"start\":81486}]", "bib_venue": "[{\"end\":54812,\"start\":54637},{\"end\":55746,\"start\":55648},{\"end\":56508,\"start\":56412},{\"end\":57196,\"start\":57152},{\"end\":57549,\"start\":57497},{\"end\":58022,\"start\":57915},{\"end\":58744,\"start\":58692},{\"end\":59131,\"start\":59084},{\"end\":59576,\"start\":59514},{\"end\":59941,\"start\":59880},{\"end\":60447,\"start\":60353},{\"end\":61097,\"start\":61053},{\"end\":61586,\"start\":61479},{\"end\":62285,\"start\":62219},{\"end\":62567,\"start\":62518},{\"end\":63180,\"start\":63136},{\"end\":63432,\"start\":63361},{\"end\":63926,\"start\":63841},{\"end\":64314,\"start\":64265},{\"end\":64776,\"start\":64727},{\"end\":65195,\"start\":65143},{\"end\":65574,\"start\":65502},{\"end\":65958,\"start\":65954},{\"end\":66256,\"start\":66204},{\"end\":66686,\"start\":66604},{\"end\":67111,\"start\":67093},{\"end\":67444,\"start\":67395},{\"end\":68064,\"start\":67995},{\"end\":68570,\"start\":68502},{\"end\":68987,\"start\":68929},{\"end\":69354,\"start\":69295},{\"end\":69722,\"start\":69661},{\"end\":70056,\"start\":69998},{\"end\":70359,\"start\":70301},{\"end\":70561,\"start\":70525},{\"end\":70900,\"start\":70864},{\"end\":71168,\"start\":71124},{\"end\":71542,\"start\":71462},{\"end\":72103,\"start\":72021},{\"end\":72574,\"start\":72518},{\"end\":72978,\"start\":72967},{\"end\":73372,\"start\":73266},{\"end\":74053,\"start\":74017},{\"end\":74297,\"start\":74239},{\"end\":75262,\"start\":75120},{\"end\":76101,\"start\":76049},{\"end\":76498,\"start\":76449},{\"end\":76948,\"start\":76862},{\"end\":77415,\"start\":77329},{\"end\":77884,\"start\":77835},{\"end\":78289,\"start\":78211},{\"end\":78641,\"start\":78592},{\"end\":79023,\"start\":78922},{\"end\":79704,\"start\":79652},{\"end\":80130,\"start\":80081},{\"end\":80582,\"start\":80533},{\"end\":80975,\"start\":80907},{\"end\":81547,\"start\":81498},{\"end\":54991,\"start\":54814},{\"end\":55848,\"start\":55748},{\"end\":56608,\"start\":56510},{\"end\":58133,\"start\":58024},{\"end\":60545,\"start\":60449},{\"end\":61697,\"start\":61588},{\"end\":65635,\"start\":65576},{\"end\":66755,\"start\":66688},{\"end\":68625,\"start\":68572},{\"end\":71609,\"start\":71544},{\"end\":72172,\"start\":72105},{\"end\":73482,\"start\":73374},{\"end\":75413,\"start\":75264},{\"end\":77021,\"start\":76950},{\"end\":79128,\"start\":79025},{\"end\":81073,\"start\":81020}]"}}}, "year": 2023, "month": 12, "day": 17}