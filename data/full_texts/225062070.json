{"id": 225062070, "updated": "2023-10-06 10:43:16.012", "metadata": {"title": "Noise2Same: Optimizing A Self-Supervised Bound for Image Denoising", "authors": "[{\"first\":\"Yaochen\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Zhengyang\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Shuiwang\",\"last\":\"Ji\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 10, "day": 22}, "abstract": "Self-supervised frameworks that learn denoising models with merely individual noisy images have shown strong capability and promising performance in various image denoising tasks. Existing self-supervised denoising frameworks are mostly built upon the same theoretical foundation, where the denoising models are required to be J-invariant. However, our analyses indicate that the current theory and the J-invariance may lead to denoising models with reduced performance. In this work, we introduce Noise2Same, a novel self-supervised denoising framework. In Noise2Same, a new self-supervised loss is proposed by deriving a self-supervised upper bound of the typical supervised loss. In particular, Noise2Same requires neither J-invariance nor extra information about the noise model and can be used in a wider range of denoising applications. We analyze our proposed Noise2Same both theoretically and experimentally. The experimental results show that our Noise2Same remarkably outperforms previous self-supervised denoising methods in terms of denoising performance and training efficiency. Our code is available at https://github.com/divelab/Noise2Same.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2010.11971", "mag": "3106060284", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/XieWJ20", "doi": null}}, "content": {"source": {"pdf_hash": "9d9ad34f43d9968cc58d4f8b929021f95cbd0b05", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2010.11971v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "541b15abf7049b451bb3a81aa09f0c7171cd26db", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9d9ad34f43d9968cc58d4f8b929021f95cbd0b05.txt", "contents": "\nNoise2Same: Optimizing A Self-Supervised Bound for Image Denoising\n\n\nYaochen Xie \nTexas A&M University College Station\nTexas A&M University College Station\nTexas A&M University College Station\n77843, 77843, 77843TX, TX, TX\n\nZhengyang Wang zhengyang.wang@tamu.edu \nTexas A&M University College Station\nTexas A&M University College Station\nTexas A&M University College Station\n77843, 77843, 77843TX, TX, TX\n\nShuiwang Ji \nTexas A&M University College Station\nTexas A&M University College Station\nTexas A&M University College Station\n77843, 77843, 77843TX, TX, TX\n\nNoise2Same: Optimizing A Self-Supervised Bound for Image Denoising\n\nSelf-supervised frameworks that learn denoising models with merely individual noisy images have shown strong capability and promising performance in various image denoising tasks. Existing self-supervised denoising frameworks are mostly built upon the same theoretical foundation, where the denoising models are required to be J -invariant. However, our analyses indicate that the current theory and the J -invariance may lead to denoising models with reduced performance. In this work, we introduce Noise2Same, a novel self-supervised denoising framework. In Noise2Same, a new self-supervised loss is proposed by deriving a self-supervised upper bound of the typical supervised loss. In particular, Noise2Same requires neither J -invariance nor extra information about the noise model and can be used in a wider range of denoising applications. We analyze our proposed Noise2Same both theoretically and experimentally. The experimental results show that our Noise2Same remarkably outperforms previous self-supervised denoising methods in terms of denoising performance and training efficiency. Our code is available at https://github.com/divelab/Noise2Same.\n\nIntroduction\n\nThe quality of deep learning methods for signal reconstruction from noisy images, also known as deep image denoising, has benefited from the advanced neural network architectures such as ResNet [8], U-Net [19] and their variants [29,16,26,31,25,14]. While more powerful deep image denoising models are developed over time, the problem of data availability becomes more critical.\n\nMost deep image denoising algorithms are supervised methods that require matched pairs of noisy and clean images for training [27,29,2,7]. The problem of these supervised methods is that, in many denoising applications, the clean images are hard to obtain due to instrument or cost limitations. To overcome this problem, Noise2Noise [13] explores an alternative training framework, where pairs of noisy images are used for training. Here, each pair of noisy images should correspond to the same but unknown clean image. Note that Noise2Noise is basically still a supervised method, just with noisy supervision.\n\nDespite the success of Noise2Noise, its application scenarios are still limited as pairs of noisy images are not available in some cases and may have registration problems. Recently, various of denoising frameworks that can be trained on individual noisy images [23,17,28,10,1,12] have been developed. These studies can be divided into two categories according to the amount of extra information required. Methods in the first category requires the noise model to be known. For example, the simulation-based methods [17,28] use the noise model to generate simulated noises and make individual noisy images noisier. Then a framework similar to Noise2Noise can be applied to train the model with pairs of noisier image and the original noisy image. The limitation is obvious as the noise model may be too complicated or even not available. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\n\nOn the other hand, algorithms in the second category target at more general cases where only individual noisy images are available without any extra information [23,10,1,12]. In this category, self-supervised learning [30,6,24] has been widely explored, such as Noise2Void [10], Noise2Self [1], and the convolutional blind-spot neural network [12]. Note that these self-supervised models can be improved as well if information about the noise model is given. For example, Laine et al. [12] and Krull et al. [11] propose the Bayesian post-processing to utilize the noise model. However, with the proposed post-processing, these methods fall into the first category where applicability is limited.\n\nIn this work, we stick to the most general cases where only individual noisy images are provided and focus on the self-supervised framework itself without any post-processing step. We note that all of these existing self-supervised denoising frameworks are built upon the same theoretical background, where the denoising models are required to be J -invariant (Section 2). We perform in-depth analyses on the J -invariance property and argue that it may lead to denoising models with reduced performance. Based on this insight, we propose Noise2Same, a novel self-supervised denoising framework, with a new theoretical foundation. Noise2Same comes with a new self-supervised loss by deriving a self-supervised upper bound of the typical supervised loss. In particular, Noise2Same requires neither J -invariance nor extra information about the noise model. We analyze the effect of the new loss theoretically and conduct thorough experiments to evaluate Noise2Same. Result show that our Noise2Same consistently outperforms previous self-supervised denoising methods.\n\n\nBackground and Related Studies\n\nSelf-Supervised Denoising with J -Invariant Functions. We consider the reconstruction of a noisy image x \u2208 R m , where m = (d\u00d7)h \u00d7 w \u00d7 c depends on the spatial and channel dimensions. Let y \u2208 R m denotes the clean image. Given noisy and clean image pairs (x, y), supervised methods learn a denoising function f : R m \u2192 R m by minimizing the supervised loss\nL(f ) = E x,y f (x) \u2212 y 2 .\nWhen neither clean images nor paired noisy images are available, various self-supervised denoising methods have been developed [10,1,12] by assuming that the noise is zero-mean and independent among all dimensions. These methods are trained on individual noisy images to minimize the self-supervised loss L(f ) = E x f (x) \u2212 x known as Noise2Same. In particular, our Noise2Same minimizes a new self-supervised loss without requiring the denoising function f to be J -invariant.\n\nBayesian Post-Processing. From the probabilistic view, the blind-spot network f attempts to model p(y J |x J c ), where the information from x J is not utilized thus limiting the performance. This limitation can be overcome through the Bayesian deep learning [9] if the noise model p(x|y) is known, as proposed by [12,11]. Specifically, they propose to compute the posterior by\np(y J |x J , x J c ) \u221d p(x J |y J ) p(y J |x J c ), \u2200J \u2208 J .(4)\nHere, the prior p(y J |x J c ) is Gaussian, whose the mean comes from the original outputs of the blind-spot network f and the variance is estimated by extra outputs added to f . The computation of the posterior is a post-processing step, which takes information from x J into consideration.\n\nDespite the improved performance, the Bayesian post-processing has limited applicability as it requires the noise model p(x J |y J ) to be knwon. Besides, it assumes that a single type of noise is present for all dimensions. In practice, it is common to have unknown noise models, inconsistent noises, or combined noises with different types, where the Bayesian post-processing is no longer applicable.\n\nIn contrast, our proposed Noise2Same can make use of the entire input image without any postprocessing. Most importantly, Noise2Same does not require the noise model to be known and thus can be used in a much wider range of denoising applications.\n\n\nAnalysis of the J -Invariance Property\n\nIn this section, we analyze the J -invariance property and motivate our work. In section 3.1, we experimentally show that the denoising functions trained through mask-based blind-spot methods are not strictly J -invariant. Next, in Section 3.2, we argue that minimizing E x f (x) \u2212 x 2 with J -invariant f is not optimal for self-supervised denoising. We show that, in mask-based blind-spot approaches, the optimal denoising function obtained through training is not strictly J -invariant, which contradicts the theory behind these methods. As introduced in Section 2, mask-based blind-spot methods implement blindness on J through masking. Original values on J are masked out and replaced by other values. Concretely, in Equation (3), x J c becomes 1 J c \u00b7 x + 1 J \u00b7 r, where r denotes the new values on the masked locations (J). As introduced in Section 2, Noise2Void [10] and Noise2Self [1] are current mask-based blind-spot methods. The main difference between them is the choice of the replacement strategy, i.e., how to select r. Specifically, Noise2Void applies the Uniform Pixel Selection (UPS) to randomly select r from local neighbors of the masked locations, while Noise2Self directly uses a random value.\n\nAlthough the masking prevents f from accessing the original values on J during training, we point out that, during inference, f still shows a weak dependency on values on J, and thus does not strictly satisfy the J -invariance property. In other words, mask-based blind-spot methods do not guarantee the learning of a J -invariant function f . We conduct experiments to verify the above statement. Concretely, given a denoising function f trained through mask-based blind-spot methods, we quantify the strictness of J -invariance by computing the following metric:\nD(f ) = E J E x f (x J c ) J \u2212 f (x) J 2 /|J|,(5)\nwhere x is the raw noisy image and x J c denotes the image whose values on J are replaced with random Gaussian noises (\u03c3 m =0.5). Note that the replacement here is irrelevant to the the replacement strategy used in mask-based blind-spot methods. If the function f is strictly J -invariant, D(f ) should be close to 0 for all x. Smaller D(f ) indicates more J -invariant f . To mitigate mutual influences among the locations within J, we use saturate sampling [10] to sample J and make the sampling sparse enough (at a portion of 0.01%). D(f ) is computed on the output of f before re-scaling back to [0,255]. In our experiments, we compare D(f ) and the testing PSNR for f trained with different replacement strategies and on different datasets.   Table 1 provides the comparison results between f trained with different replacement strategies on the BSD68 dataset [15]. We also include the scores of the convolutional blind-spot neural network [12] for reference, which guarantees the strict J -invariance through shifting receptive field, as discussed in Section 3.2. As expected, it has a close-to-zero D(f ), where the non-zero value comes from mutual influences among the locations within J and the numerical precision. The large D(f ) for all the mask-based blind-spot methods indicate that the J -invariance is not strictly guaranteed and the strictness varies significantly over different replacement strategies.\n\nWe also compare results on different datasets when we fix the replacement strategy, as shown in Table 2. We can see that different datasets have strong influences on the strictness of J -invariance as well. Note that such influences are not under the control of the denoising approach itself. In addition, although the shown results in Tables 1 and 2 are computed on testing dataset at the end of training, similar trends with D(f ) 0 is observed during training.\n\nGiven the results in Tables 1 and 2, we draw our conclusions from two aspects. We first consider the mask together with the network f as a J -invariant function g, i.e., g(x)\n:= f (1 J c \u00b7 x + 1 J \u00b7 r).\nIn this case, the function g is guaranteed to be J -invariant during training, and thus Equation (2) is valid. However, during testing, the mask is removed and a different non-J -invariant function f is used because f achieves better performance than g, according to [1]. This contradicts the theoretical results of [1]. On the other hand, we consider the network f and the mask separately and perform training and testing with the same function f . In this case, the use of mask aims to help f learn to be J -invariant during training so that Equation (2) becomes valid. However, our experiments show that f is neither strictly J -invariant during training nor till the end of training, indicating that Equation (2) is not valid. With findings interpreted from both aspects, we ask whether minimizing\nE x f (x) \u2212 x 2\nwith J -invariant f yields optimal performance for self-supervised denoising.\n\n\nShifting Receptive Field: How do the Strictly J -Invariant Models Perform?\n\nWe directly show that, with a strictly J -invariant f , minimizing E x f (x) \u2212 x 2 does not necessarily lead to the best performance. Different from mask-based blind-spot methods, Laine et al. [12] propose the convolutional blind-spot neural network, which achieves the blindness on J by shifting receptive field (RF). Specifically, each pixel in the output image excludes its corresponding pixel in the input image from its receptive field. As values outside the receptive field cannot affect the output, the convolutional blind-spot neural network is strictly J -invariant by design.\n\nAccording to Table 1, the shift RF method outperforms all the mask-based blind-spot approaches with Gaussian replacement strategies, indicating the advantage of the strict J -invariance. However, we notice that the UPS replacement strategy shows a different result. Here, a denoising function with less strict J -invariance performs the best. One possible explanation is that the UPS replacement has a certain probability to replace a masked location by its original value. It weakens the J -invariance of the mask-based denoising model but boosts the performance by yielding a result that is equivalent to computing a linear combination of the noisy input and the output of a strictly J -invariant blindspot network [1]. This result shows that minimizing E x f (x) \u2212 x 2 with a strictly J -invariant f does not necessarily give the best performance. Another evidence is the Bayesian post-processing introduced in Section 2, which also make the final denoising function not strictly J -invariant while boosting the performance. To conclude, we argue that minimizing E x f (x) \u2212 x 2 with J -invariant f can lead to reduction in performance for self-supervised denoising. In this work, we propose a new self-supervised loss. Our loss does not require the J -invariance. In addition, our proposed method can take advantage of the information from the entire noisy input without any post-processing step or extra assumption about the noise.\n\n\nThe Proposed Noise2Same Method\n\nIn this section, we introduce Noise2Same, a novel self-supervised denoising framework. Noise2Same comes with a new self-supervised loss. In particular, Noise2Same requires neither J -invariant denoising functions nor the noise models.\n\n\nNoise2Same: A Self-Supervised Upper Bound without the J -Invariance Requirement\n\nAs introduced in Section 2, the J -invariance requirement sets the inner product term f (x)\u2212y, x\u2212y in Equation (1) to zero. The resulting Equation (2) shows that minimizing E x f (x) \u2212 x 2 with Jinvariant f indirectly minimizes the supervised loss, leading to the current self-supervised denoising framework. However, we have pointed out that this framework yields reduced performance.\n\nIn order to overcome this limitation, we propose to control the right side of Equation (2) with a self-supervised upper bound, instead of approximating f (x) \u2212 y, x \u2212 y to zero. The upper bound holds without requiring the denoising function f to be J -invariant. Theorem 1. Consider a normalized noisy image x \u2208 R m (obtained by subtracting the mean and dividing by the standard deviation) and its ground truth signal y \u2208 R m . Assume the noise is zeromean and i.i.d among all the dimensions, and let J be a subset of m dimensions uniformly sampled from the image x. For any f :\nR m \u2192 R m , we have E x,y f (x) \u2212 y 2 + x \u2212 y 2 \u2264 E x f (x) \u2212 x 2 + 2m E J E x f (x) J \u2212 f (x J c ) J 2 |J| 1/2(6)\nThe proof of Theorem 1 is provided in Appendix A. With Theorem 1, we can perform self-supervised denoising by minimizing the right side of Inequality (6) instead. Following the theoretical result, we propose our new self-supervised denoising framework, Noise2Same, with the following selfsupervised loss:\nL(f ) = E x f (x) \u2212 x 2 /m + \u03bb inv E J E x f (x) J \u2212 f (x J c ) J 2 /|J| 1/2 .(7)\nThis new self-supervised loss consists of two terms: a reconstruction mean squared er-\nror (MSE) L rec = E x f (x) \u2212 x 2 and a squared-root of invariance MSE L inv = E J (E x f (x) J \u2212 f (x J c ) J 2 /|J|) 1/2 .\nIntuitively, L inv prevents our model from learning the identity function when minimizing L rec without any requirement on f . In fact, by comparing L inv with D(f ) in Equation (5), we can see that L inv implicitly controls how strictly f should be J -invariant, avoiding the explicit J -invariance requirement. We balance L rec and L inv with a positive scalar weight \u03bb inv . By default, we set \u03bb inv = 2 according to Theorem 1. In some cases, setting \u03bb inv to different values according to the scale of observed L inv during training could help achieve a better denoising performance. \n\n\nAnalysis of the Invariance Term\n\nThe invariance term L inv is a unique and important part in our proposed self-supervised loss. In this section, we further analyze the effect of this term. To make the analysis concrete, we perform analysis based on an example case, where the noise model is given as the additive Gaussian noise N (0, \u03c3). Note that the example is for analysis purpose only, and the application of our proposed Noise2Same does not require the noise model to be known.\n\nTheorem 2. Consider a noisy image x \u2208 R m and its ground truth signal y \u2208 R m . Assume the noise is i.i.d among all the dimensions, and let J be a subset of m dimensions uniformly sampled from the image x. If the noise is additive Gaussian with zero-mean and standard deviation \u03c3, we have\nE x,y f (x) \u2212 y 2 + x \u2212 y 2 \u2264 E x f (x) \u2212 x 2 + 2m\u03c3 E J E f (x) J \u2212 f (x J c ) J 2 |J| 1/2(8)\nThe proof of Theorem 2 is provided in Appendix B. Note that the noisy image x here does not require normalization as in Theorem 1. Compared to Theorem 1, the \u03c3 from the noise model is added to balance the invariance term. As introduced in Section 4.1, the invariance term controls how strictly f should be J -invariant and a higher weight of the invariance term pushes the model to learn a more strictly J -invariant f . Therefore, Theorem 2 indicates that, when the noise is stronger with a larger \u03c3, f should be more strictly J -invariant. Based on the definition of J -invariance, a more strictly J -invariant f will depend more on the context x J c and less on the noisy input x J .\n\nThis result is consistent with the findings in previous studies. Batson et al. [1] propose to compute the linear combination of the noisy image and the output of the blind-spot network as a post-processing step, leading to better performance. The weights in the linear combination are determined by the variance of noise. And a higher weight is given to the output of the blind-spot network with larger noise variance. Laine et al. [12] derive a similar result through the Bayesian post-processing. This explains how the invariance term in our proposed Noise2Same improves denoising performance.\n\nHowever, a critical difference between our Noise2Same and previous studies is that, the postprocessing in [1,12] cannot be performed when the noise model is unknown. To the contrary, Noise2Same is able to control how strictly f should be J -invariant through the invariance term without any assumption about the noise or requirement on f . This property allows Noise2Same to be used in a much wider range of denoising tasks with unknown noise models, inconsistent noise, or combined noises with different types.\n\n\nInput\n\n\nBM3D Noise2Self\n\nOurs Noise2Noise Noise2True Ground Truth Figure 2: RGB natural images and hand-written Chinese character images: Visualizations of testing results on ImageNet dataset (first two rows) and the H\u00e0nZ\u00ec Dataset (the third row). We compare the denoising quality among the traditional method BM3D, supervised methods Noise2True and Noise2Noise, self-supervised approaches Noise2Self and our Noise2Same. From the left to the right, the columns are in the ascending order in terms of the denoising quality.\n\n\nExperiments\n\nWe evaluate our Noise2Same on four datasets, including RGB natural images (ImageNet ILSVRC 2012 Val [21]), generated hand-written Chinese character images (H\u00e0nZ\u00ec [1]), physically captured 3D microscopy data (Planaria [27]) and grey-scale natural images (BSD68 [15]). The four datasets have different noise types and levels. The constructions of the four datasets are described in Appendix C.\n\n\nComparisons with Baselines\n\nThe baselines include traditional denoising algorithms (NLM [3], BM3D [5]), supervised methods (Noise2True, Noise2Noise [13]), and previous self-supervised methods (Noise2Void [10], Noise2Self [1], the convolutional blind-spot neural network [12]). Note that we consider Noise2Noise as a supervised model since it requires pairs of noisy images, where the supervision is noisy. While Noise2Void and Noise2Self are similar methods following the blind-spot approach, they mainly differ in the strategy of mask replacement. To be more specific, Noise2Void proposes to use Uniform Pixel Selection (UPS), and Noise2Self proposes to exclude the information of the masked pixel and uses a random value on the range of given image data. As an additional mask strategy using the local average excluding the center pixel (donut) is mentioned in [1], we also include it for comparison. We use the same neural network architecture for all deep learning methods. Detailed experimental settings are provided in Appendices D and E.\n\nNote that ImageNet and H\u00e0nZ\u00ec have combined noises and Planaria has unknown noise models. As a result, the post-processing steps in Noise2Self [1] and the convolutional blind-spot neural network [12] are not applicable, as explained in Section 2. In order to make fair comparisons under the self-supervised category, we train and evaluate all models only using the images, without extra information about the noise. In this case, among self-supervised methods, only our Noise2Same and Noise2Void with the UPS replacement strategy can make use of information from the entire input image, as demonstrated in Section 3.2. We also include the complete version of the convolutional Table 3: Comparisons among denoising methods on different datasets, in terms of Peak Signal-to-Noise Ratio (PSNR). The post-processing of Laine et al. [12] that requires information about the noise model is included under the Self-Supervised + noise model category and is excluded under the Self-Supervised category. Noise2Self-Noise and Noise2Self-Donut refer to two masking strategies mentioned in [1], where the original results presented in [1] are produced by the noise masking. Bold numbers indicate the best performance among self-supervised methods.\n\n\nDatasets Methods\n\nImageNet blind-spot neural network with post-processing, who is only available on BSD68, where the noise is not combined and the noise type is known.  Following previous studies, we use Peak Signal-to-Noise Ratio (PSNR) as the evaluation metric. The comparison results between our Noise2Same and the baselines in terms of PSNR on the four datasets are summarized in Table 3 and visualized in Figure 2 and Appendix F. The results show that our Noise2Same achieve remarkable improvements over previous self-supervised baselines on Im-ageNet, H\u00e0nZ\u00ec and CARE. In particular, on the ImageNet and the H\u00e0nZ\u00ec Datasets, our Noise2Same and Noise2Void demonstrate the advantage of utilizing information from the entire input image. Although the using of donut masking can achieve better performance on the BSD68 Dataset, it leads to model collapsing on the ImageNet Dataset and hence can be unstable. On the other hand, the convolutional blind-spot neural network [12] suffers from significant performance losses without the Bayesian post-processing, which requires information about the noise models that are unknown.\n\nWe note that, in our fair settings, supervised methods still have better performance over self-supervised models, especially on the Planaria and BSD68 datasets. One explanation is that the supervision usually carries extra information implicitly, such as information about the noise model. Here, we draw a conclusion different from Batson et al. [1]. That is, there are still performance gaps between self-supervised and supervised denoising methods. Our Noise2Same moves one step towards closing the gap by proposing a new self-supervised denoising framework.\n\nIn addition to the performance, we compares the training efficiency among self-supervised methods as well. Specifically, we plot how the PSNR changes during training on the ImageNet dataset. We compare Noise2Same with Noise2Self and the convolutional blind-spot neural network.  shows that our Noise2Same has similar convergence speed to the convolutional blind-spot neural network. On the other hand, as the mask-based method Noise2Self uses only a subset of output pixels to compute the loss function in each step, the training is expected to be slower [12].\n\n\nEffect of the Invariance Term\n\nIn Section 4.2, we analyzed the effect of the invariance term using an example, where the noise model is given as the additive Gaussian noise. In this example, the variance of the noise controls how the strictness of the optimal f through the coefficient \u03bb inv of the invariance term.\n\nHere, we conduct experiments to verify this insight. Specifically, we construct four noisy dataset from the H\u00e0nZ\u00ec dataset with only additive Gaussian noise at different levels (\u03c3 noise = 0.9, 0.7, 0.5, 0.3).\n\nThen we train Noise2Same with \u03bb inv = 2\u03c3 loss by varying \u03c3 loss from 0.1 to 1.0 for each dataset. According to Theorem 2, the best performance on each dataset should be achieved when \u03c3 loss is close to \u03c3 noise . The results, as reported Figure 4, are consistent with our theoretical results in Theorem 2.\n\n\nConclusion and Future Work\n\nWe analyzed the existing blind-spot-based denoising methods and introduced Noise2Same, a novel self-supervised denoising method, which removes the assumption and over-restriction on the neural network as a J -invariant function. We provided further analysis on Noise2Same and experimentally demonstrated the denoising capability of Noise2Same. As an orthogonal work, the combination of self-supervised denoising result and the noise model has be shown to provide additional performance gain. We would like to further explore noise model-augmented Noise2Same in future works.\n\n\nBroader Impact\n\nIn this paper, we introduce Noise2Same, a self-supervised framework for deep image denoising. As Noise2Same does not need paired clean data, paired noisy data, nor the noise model, its application scenarios could be much broader than both traditional supervised and existing self-supervised denoising frameworks. The most direct application of Noise2Same is to perform denoising on digital images captured under poor conditions. Individuals and corporations related to photography may benefit from our work. Besides, Noise2Same could be applied as a pre-processing step for computer vision tasks such as object detection and segmentation [18], making the downstream algorithms more robust to noisy images. Also, specific research communities could benefit from the development of Noise2Same as well. For example, the capture of high-quality microscopy data of live cells, tissue, or nanomaterials is expensive in terms of budget and time [27]. Proper denoising algorithms allow researchers to obtain high-quality data from low-quality data and hence remove the need to capture high-quality data directly. In addition to image denoising applications, the self-supervised denoising framework could be extended to other domains such as audio noise reduction and single-cell [1]. On the negative aspect, as many imaging-based research tasks and computer vision applications may be built upon the denoising algorithms, the failure of Noise2Same could potentially lead to biases or failures in these tasks and applications.\n\n\nA Proof of Theorem 1\n\nProof. We consider the third term on the right-hand side of Equation (1). Instead of reducing the third term 2 f (x) \u2212 y, x \u2212 y to 0 under the J -invariant assumption, we control this term with its upper bound with the only assumption that E[x|y] = y. Formally, we have\nE x,y f (x) \u2212 y, x \u2212 y = E y E x|y j (f (x) j \u2212 y j )(x j \u2212 y j ) (9) = j E y E x|y (f (x) j \u2212 y j )(x j \u2212 y j ) \u2212 E x|y (f (x) j \u2212 y j )E x|y (x j \u2212 y j ) (10) = j E y [Cov(f (x) j \u2212 y j , x j \u2212 y j |y)] (11) = j E y [Cov(f (x) j , x j |y)] .(12)\nEquation (10) holds due to the zero-mean assumption, where E x|y (x j \u2212 y j ) = 0. Now we let J be a uniformly sampled subset of the image dimensions {1, \u00b7 \u00b7 \u00b7 , m}, then we have the equation\nj E y [Cov(f (x) j , x j |y)] = m |J| E J j\u2208J E y [Cov(f (x) j , x j |y)] .(13)\nThe right-hand side of the equation above can be controlled by applying Cauchy-Schwarz inequality while the input images are normalized. We have, for all J,\n1 |J| j\u2208J E y Cov(f (x) j , x j |y) = 1 |J| j\u2208J E y Cov(f (x) j \u2212 f (x J c ) j , x j |y)(14)\u2264 1 |J| j\u2208J E y Var(f (x) j \u2212 f (x J c ) j |y) \u00b7 Var(x j |y) 1/2 (15) \u2264 \uf8eb \uf8ed 1 |J| j\u2208J E y Var(f (x) j \u2212 f (x J c ) j |y) \u00b7 Var(x j |y) \uf8f6 \uf8f8 1/2 (16) \u2264 \uf8eb \uf8ed 1 |J| j\u2208J E y E [f (x) j \u2212 f (x J c ) j ] 2 |y \uf8f6 \uf8f8 1/2 (17) = \uf8eb \uf8ed 1 |J| j\u2208J E f (x) j \u2212 f (x J c ) j 2 \uf8f6 \uf8f8 1/2 (18) = 1 |J| E f (x) J \u2212 f (x J c ) J 2 1/2 .(19)\nTo be more specific, Equation (14) follows since f (x J c ) J does not correlate to x j due to the independent noise assumption and j / \u2208 J c , and subtracting f (x J c ) j from f (x) j does not change the Covariance. Inequality (15) applies the Cauchy-Schwarz inequality. Inequality (16) holds due to (EX) 2 \u2264 EX 2 . The derivation of Inequality (17) uses the fact that Var(x j ) = 1 under normalization and Var(x j |y) \u2264 Var(x j ) = 1 for all j.\n\nConsequently, we can control Equation (1) as\nE x,y f (x) \u2212 y 2 + E x,y x \u2212 y 2 = E x f (x) \u2212 x 2 + 2 E x,y f (x) \u2212 y, x \u2212 y (20) \u2264 E x f (x) \u2212 x 2 + 2m E J 1 |J| E f (x) J \u2212 f (x J c ) J 2 1/2 .(21)\nThis completes the proof of Theorem 1.\n\n\nB Proof of Theorem 2\n\nProof. We start from Equation (13) in the proof of Theorem 1. Since we have a stronger assumption that the noise model is known to be additive with standard deviation \u03c3 and zero-mean, we have Var(x j \u2212 y j ) = \u03c3 2 for all j. Due to that the additive noise is orthogonal to the signal y, we futher have the conditional variance Var(x j \u2212 y j |y) = \u03c3 2 . Then, similar to the proof of Theorem 1, we have,\n1 |J| j\u2208J E y Cov(f (x) j , x j |y) = 1 |J| j\u2208J E y Cov(f (x) j \u2212 f (x J c ) j , x j \u2212 y j |y) (22) \u2264 1 |J| j\u2208J E y Var(f (x) j \u2212 f (x J c ) j |y) \u00b7 Var(x j \u2212 y j |y) 1/2 (23) \u2264 \uf8eb \uf8ed 1 |J| j\u2208J E y Var(f (x) j \u2212 f (x J c ) j |y) \u00b7 Var(x j \u2212 y j |y) \uf8f6 \uf8f8 1/2 (24) = \uf8eb \uf8ed 1 |J| j\u2208J E y E [f (x) j \u2212 f (x J c ) j ] 2 |y \u00b7 \u03c3 2 \uf8f6 \uf8f8 1/2 (25) = \u03c3 \uf8eb \uf8ed 1 |J| j\u2208J E f (x) j \u2212 f (x J c ) j 2 \uf8f6 \uf8f8 1/2 (26) = \u03c3 1 |J| E f (x) J \u2212 f (x J c ) J 2 1/2 .(27)\nConsequently, we can control Equation (1) as\nE x,y f (x) \u2212 y 2 + E x,y x \u2212 y 2 = E x f (x) \u2212 x 2 + 2 E x,y f (x) \u2212 y, x \u2212 y(28)\n\u2264\nE x f (x) \u2212 x 2 + 2m\u03c3 E J 1 |J| E f (x) J \u2212 f (x J c ) J 2 1/2 .(29)\nThis completes the proof of Theorem 2.\n\n\nC Dataset Constructions\n\nRGB Natural Images. We construct the RGB natural image dataset from the ImageNet ILSVRC2012 Validation dataset that consists of 50,000 natural images. In particular, we follow [1] to generate noisy images by applying a combination of three types of noises to the clear images. The noises are Poisson noise (\u03bb = 30), additive Gaussian noise (\u00b5 = 0, \u03c3 = 60) and Bernoulli noise 3D Fluorescence Microscopy Data. In order to show the capability of our approach to 3D images with inconsistent and untypical noise, we use the physically acquired 3D fluorescence microscopy data collected from Planaria (Schmidtea mediterranea) provided by [27]. The training data, consisting of 17005 3D patches of size 16 \u00d7 64 \u00d7 64, is a mix of noisy images at three noise levels, collected under different conditions (C1, C2, and C3) of exposure time and laser power. The trained models are evaluated on 20 testing images of size 96 \u00d7 1024 \u00d7 1024 at three different noise levels individually. From condition 1 (C1) to condition 3 (C3), the noise gets stronger, and input image quality gets worse.\n\nGrey-scale Natural Images. We follow [10] and use the same procedure as [4,29,22] \n\n\nD Implementation Details\n\nNetwork Architecture. We use U-Net [19] as our neural network architecture and mainly follow [10] for the basic settings. To be specific, we apply a U-Net of depth 3 with convolutions of kernel size 3. The number of output feature maps from the initial convolution is set to 96. Batch normalization is applied by default unless further mentioned. Compared to [10], we remove the skip connection used in Noise2Void that add input to the network output, which contradicts the J -invariant assumption in Noise2Self and prevents our model from learning from L inv .\n\nMask Strategy and Training. By default, we randomly mask 0.5% of pixels for each training patch by saturated sampling and replace them with Gaussian noise (\u03c3 = 0.2). The scalar weight \u03bb inv in the loss is set to 2 by default unless further mentioned. We apply data augmentations, including rotation and flipping, during the training for all datasets. For 3D images, the rotation is only applied on width and height dimension due to the anisotropy on depth. All the input images are normalized to satisfy the required condition in Theorem 1. We use learning rate decay during training, starting at 0.0004 and reducing the learning rate by 0.5 after each 5k iterations.\n\n\nE Model Configurations\n\nFor the convolutional blind-spot neural network, we use the same network architecture and basic settings in [12]. For all the other deep learning-based methods, we fix the neural network (U-Net) architecture configurations and basic training settings for each dataset individually. The network and training configuration of our method basically follows the default settings in D. Exceptions are the scalar weights \u03bb inv in the loss for BSD68. We adjust \u03bb inv to 0.95 for the BSD68 Dataset according to the level of invariance error L during training. We apply the basic settings, such as mask sampling percentage, epoch numbers and batch sizes, of the two methods according to [10] for each dataset.\n\nBesides, we skip the evaluation of some baseline methods on the Planaria dataset because they are not applicable. Among these methods, the BM3D algorithm does not apply to 3D images, and Noise2Noise is not applicable since no paired noisy image is available. Moreover, there is no public 3D version of the blind-spot network, which may need to deal with the anisotropy problem for the 3D images and the memory problem due to the additional branches required in a 3D setting.\n\n\nCondition 3\n\nCondition 2\n\n\nCondition 1\n\nInput NLM Noise2Self Ours Ground Truth Noise2True (CARE) Figure 5: 3D Microscopy Data: Visualizations of testing results on the Planaria dataset. We compare the denoising quality among the traditional method NLM, the supervised method CARE [27], selfsupervised baselines Noise2Self and our Noise2Same. From top to bottom, rows are reconstruction results from different noise levels (C1, C2 and C3 individually). From the left to the right, the columns are in the ascending order in terms of the denoising quality.\n\n\nF Denoising Results on the Planaria Dataset\n\nThe visualization of the denoising results on CARE (Planaria) is shown in Figure 5. The shown results are 2D projections from the 3D images.\n\n3. 1\n1Mask-Based Blind-Spot Denoising: Is the Optimal Function Strictly J -Invariant?\n\nFigure 1 :\n1Top: The framework of the mask-based blind-spot denoising methods. The neural network takes the masked noisy image and predicts the masked value. The reconstruction loss is only computed on the masked dimensions. Bottom: The Noise2Same framework. The neural network takes both the full noisy image and the masked image as inputs and produces two outputs. The reconstruction loss is computed between the full noisy image and its corresponding output. The invariance loss is computed between the two outputs.\n\nFigure 1\n1compares our proposed Noise2Same with mask-based blind-spot denoising methods. Maskbased blind-spot denoising methods employ the self-supervised loss in Equation(3), where the reconstruction MSE L rec is computed only on J. In contrast, our proposed Noise2Same computes L rec between the entire noisy image x and the output of the neural network f (x). To compute the invariance term L inv , we still feed the masked noisy image x J c to the neural network and compute MSE between f (x) and f (x J c ) on J, i.e., f (x) J and f (x J c ) J . Note that, while Noise2Same also samples J from x, it does not require f to be J -invariant.\n\nFigure 3 :\n3Training efficiency. For a fair comparison, we adjust the batch sizes for each method to fill the memory of a single GPU, namely, 128 for Noise2Self, 64 for Noise2Same and 32 for Laine et al. One unit of training cost represents 50 minibatch steps.\n\nFigure 4 :\n4Effect of the invariance term. Left: Given additive Gaussian noise with certain \u03c3 noise , how the performance of our Noise2Same varies over different \u03c3 loss . Right: We visualize some denoising examples from noisy images with \u03c3 noise = 0.3, 0.5. From left to right, the columns correspond to setting \u03c3 loss to 0.2, 0.3, 0.4, 0.5, 0.6, respectively.\n\nTable 1 :\n1D(f ) and PSNR of f trained through mask-based blind-spot methods with different replacement strategies on BSD68. The last column corresponds to a strictly J -invariant model.Replacement \nStrategy \n\nGaussian \n(\u03c3=0.2) \n\nGaussian \n(\u03c3=0.5) \n\nGaussian \n(\u03c3=0.8) \n\nGaussian \n(\u03c3=1.0) \n\nUPS \n(5 \u00d7 5) \n\nShifting \nRF \nD(f ) (\u00d710 \u22123 ) \n4.326 \n10.91 \n2.141 \n1.569 \n18.31 \n0.105 \nPSNR \n26.14 \n26.83 \n26.85 \n26.98 \n27.71 \n27.15 \n\n\n\nTable 2 :\n2D(f ) and PSNR of f on trained through mask-\nbased blind-spot methods with the same replacement \nstrategy on different datasets. \n\nDatasets \nBSD68 HanZi ImageNet \nD(f ) (\u00d710 \u22123 ) \n10.91 \n0.249 \n17.67 \nPSNR \n26.83 \n13.94 \n20.38 \n\n\n\n\nto construct the BSD68 Dataset. For the training set, patches of size 180 \u00d7 180 are cropped from each of the 400 grey-scale version of images of range [0, 255] from [15], where each image is treated with Gaussian noise (\u03c3 = 25). The trained models are then evaluated on 68 testing images first introduced by [20].\n. Particularly, in order to prevent the self-supervised training from collapsing into leaning the identity function, Batson et al.[1] point out that the denoising function f should be J -invariant, as defined below.\nAcknowledgments and Disclosure of FundingThis work was supported in part by National Science Foundation grant DBI-2028361.\nNoise2self: Blind denoising by self-supervision. Joshua Batson, Lo\u00efc Royer, Proceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine Learning97Joshua Batson and Lo\u00efc Royer. Noise2self: Blind denoising by self-supervision. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pages 524-533, 2019.\n\nUnprocessing images for learned raw denoising. Tim Brooks, Ben Mildenhall, Tianfan Xue, Jiawen Chen, Dillon Sharlet, Jonathan T Barron, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionTim Brooks, Ben Mildenhall, Tianfan Xue, Jiawen Chen, Dillon Sharlet, and Jonathan T Barron. Unprocessing images for learned raw denoising. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11036-11045, 2019.\n\nA non-local algorithm for image denoising. Antoni Buades, Bartomeu Coll, J-M Morel, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05). IEEE2Antoni Buades, Bartomeu Coll, and J-M Morel. A non-local algorithm for image denoising. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), volume 2, pages 60-65. IEEE, 2005.\n\nTrainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration. Yunjin Chen, Thomas Pock, IEEE transactions on pattern analysis and machine intelligence. 39Yunjin Chen and Thomas Pock. Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration. IEEE transactions on pattern analysis and machine intelligence, 39(6):1256-1272, 2016.\n\nImage denoising with block-matching and 3d filtering. Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, Karen Egiazarian, Image Processing: Algorithms and Systems, Neural Networks, and Machine Learning. 6064606414Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising with block-matching and 3d filtering. In Image Processing: Algorithms and Systems, Neural Networks, and Machine Learning, volume 6064, page 606414. International Society for Optics and Photonics, 2006.\n\nUnsupervised visual representation learning by context prediction. Carl Doersch, Abhinav Gupta, Alexei A Efros, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pages 1422-1430, 2015.\n\nToward convolutional blind denoising of real photographs. Zifei Shi Guo, Kai Yan, Wangmeng Zhang, Lei Zuo, Zhang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionShi Guo, Zifei Yan, Kai Zhang, Wangmeng Zuo, and Lei Zhang. Toward convolutional blind denoising of real photographs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1712-1722, 2019.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.\n\nWhat uncertainties do we need in bayesian deep learning for computer vision?. Alex Kendall, Yarin Gal, Advances in Neural Information Processing Systems. Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? In Advances in Neural Information Processing Systems, pages 5574-5584, 2017.\n\nNoise2void-learning denoising from single noisy images. Alexander Krull, Tim-Oliver Buchholz, Florian Jug, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionAlexander Krull, Tim-Oliver Buchholz, and Florian Jug. Noise2void-learning denoising from single noisy images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2129-2137, 2019.\n\nProbabilistic noise2void: Unsupervised contentaware denoising. Alexander Krull, Tomas Vicar, Florian Jug, arXiv:1906.00651arXiv preprintAlexander Krull, Tomas Vicar, and Florian Jug. Probabilistic noise2void: Unsupervised content- aware denoising. arXiv preprint arXiv:1906.00651, 2019.\n\nHigh-quality self-supervised deep image denoising. Samuli Laine, Tero Karras, Jaakko Lehtinen, Timo Aila, Advances in Neural Information Processing Systems. Samuli Laine, Tero Karras, Jaakko Lehtinen, and Timo Aila. High-quality self-supervised deep image denoising. In Advances in Neural Information Processing Systems, pages 6968-6978, 2019.\n\nJaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, Timo Aila, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine Learning80Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, Timo Aila, et al. Noise2noise. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 2971-2980, 2018.\n\nGlobal pixel transformers for virtual staining of microscopy images. Yi Liu, Hao Yuan, Zhengyang Wang, Shuiwang Ji, IEEE Transactions on Medical Imaging. 396Yi Liu, Hao Yuan, Zhengyang Wang, and Shuiwang Ji. Global pixel transformers for virtual staining of microscopy images. IEEE Transactions on Medical Imaging, 39(6):2256-2266, 2020.\n\nA database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. D Martin, C Fowlkes, D Tal, J Malik, Proceedings of the 8th IEEE International Conference on Computer Vision. the 8th IEEE International Conference on Computer Vision2D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the 8th IEEE International Conference on Computer Vision, volume 2, pages 416-423, July 2001.\n\nV-net: Fully convolutional neural networks for volumetric medical image segmentation. Fausto Milletari, Nassir Navab, Seyed-Ahmad Ahmadi, 2016 Fourth International Conference on 3D Vision (3DV). IEEEFausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 2016 Fourth International Conference on 3D Vision (3DV), pages 565-571. IEEE, 2016.\n\nNick Moran, Dan Schmidt, Yu Zhong, Patrick Coady, arXiv:1910.11908Noisier2noise: Learning to denoise from unpaired noisy data. arXiv preprintNick Moran, Dan Schmidt, Yu Zhong, and Patrick Coady. Noisier2noise: Learning to denoise from unpaired noisy data. arXiv preprint arXiv:1910.11908, 2019.\n\nLeveraging self-supervised denoising for image segmentation. Mangal Prakash, Tim-Oliver Buchholz, Manan Lalit, Pavel Tomancak, Florian Jug, Alexander Krull, 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI). IEEEMangal Prakash, Tim-Oliver Buchholz, Manan Lalit, Pavel Tomancak, Florian Jug, and Alexan- der Krull. Leveraging self-supervised denoising for image segmentation. In 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI), pages 428-432. IEEE, 2020.\n\nU-net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, International Conference on Medical image computing and computer-assisted intervention. SpringerOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234-241. Springer, 2015.\n\nFields of experts. Stefan Roth, J Michael, Black, International Journal of Computer Vision. 822205Stefan Roth and Michael J Black. Fields of experts. International Journal of Computer Vision, 82(2):205, 2009.\n\n. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, Li Fei-Fei, International Journal of Computer Vision (IJCV). 1153Imagenet large scale visual recognition challengeOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. International Journal of Computer Vision (IJCV), 115(3):211-252, 2015.\n\nShrinkage fields for effective image restoration. Uwe Schmidt, Stefan Roth, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionIEEE Computer SocietyUwe Schmidt and Stefan Roth. Shrinkage fields for effective image restoration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2774-2781. IEEE Computer Society, 2014.\n\nDeep image prior. Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9446-9454, 2018.\n\nStacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, Journal of Machine Learning Research. 11Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11(Dec):3371-3408, 2010.\n\nGlobal voxel transformer networks for augmented microscopy. Zhengyang Wang, Yaochen Xie, Shuiwang Ji, arXiv:2008.02340arXiv preprintZhengyang Wang, Yaochen Xie, and Shuiwang Ji. Global voxel transformer networks for augmented microscopy. arXiv preprint arXiv:2008.02340, 2020.\n\nNon-local u-nets for biomedical image segmentation. Zhengyang Wang, Na Zou, Dinggang Shen, Shuiwang Ji, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceZhengyang Wang, Na Zou, Dinggang Shen, and Shuiwang Ji. Non-local u-nets for biomedical image segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 6315-6322, 2020.\n\nContentaware image restoration: pushing the limits of fluorescence microscopy. Martin Weigert, Uwe Schmidt, Tobias Boothe, Andreas M\u00fcller, Alexandr Dibrov, Akanksha Jain, Benjamin Wilhelm, Deborah Schmidt, Coleman Broaddus, Si\u00e2n Culley, Nature methods. 1512Martin Weigert, Uwe Schmidt, Tobias Boothe, Andreas M\u00fcller, Alexandr Dibrov, Akanksha Jain, Benjamin Wilhelm, Deborah Schmidt, Coleman Broaddus, Si\u00e2n Culley, et al. Content- aware image restoration: pushing the limits of fluorescence microscopy. Nature methods, 15(12):1090-1097, 2018.\n\nNoisy-as-clean: learning unsupervised denoising from the corrupted image. Jun Xu, Yuan Huang, Li Liu, Fan Zhu, Xingsong Hou, Ling Shao, arXiv:1906.06878arXiv preprintJun Xu, Yuan Huang, Li Liu, Fan Zhu, Xingsong Hou, and Ling Shao. Noisy-as-clean: learning unsupervised denoising from the corrupted image. arXiv preprint arXiv:1906.06878, 2019.\n\nBeyond a gaussian denoiser: Residual learning of deep cnn for image denoising. Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang, IEEE Transactions on Image Processing. 267Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE Transactions on Image Processing, 26(7):3142-3155, 2017.\n\nSplit-brain autoencoders: Unsupervised learning by cross-channel prediction. Richard Zhang, Phillip Isola, Alexei A Efros, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionRichard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning by cross-channel prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1058-1067, 2017.\n\nResidual dense network for image restoration. Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu, IEEE Transactions on Pattern Analysis and Machine Intelligence. Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image restoration. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1-1, 2020.\n\nTo be consistent to [1], we randomly crop 60,000 patches of size 128 \u00d7 128 from the first 20,000 images in ILSVRC2012 Val to construct the training dataset. Additional two sets of 1,000 images from ILSVRC2012 Val are used for validation and testing. = 0.2). respectively= 0.2). To be consistent to [1], we randomly crop 60,000 patches of size 128 \u00d7 128 from the first 20,000 images in ILSVRC2012 Val to construct the training dataset. Additional two sets of 1,000 images from ILSVRC2012 Val are used for validation and testing, respectively\n\nWe generate the H\u00e0nZ\u00ec dataset with the code provided by [1]. The dataset is constructed with 13029 Chinese characters and consists of 78174 noisy images of size 64 \u00d7 64, where each noisy image is generated by applying Gaussian noise (\u03c3 = 0.7) and Bernoulli noise to a clear Chinese character image. Among the 78174 noisy images, 90% are used for training and validation. Chinese Character Images. and the rest 10% are for testingHand-written Chinese Character Images. We generate the H\u00e0nZ\u00ec dataset with the code provided by [1]. The dataset is constructed with 13029 Chinese characters and consists of 78174 noisy images of size 64 \u00d7 64, where each noisy image is generated by applying Gaussian noise (\u03c3 = 0.7) and Bernoulli noise to a clear Chinese character image. Among the 78174 noisy images, 90% are used for training and validation, and the rest 10% are for testing.\n", "annotations": {"author": "[{\"end\":224,\"start\":70},{\"end\":406,\"start\":225},{\"end\":561,\"start\":407}]", "publisher": null, "author_last_name": "[{\"end\":81,\"start\":78},{\"end\":239,\"start\":235},{\"end\":418,\"start\":416}]", "author_first_name": "[{\"end\":77,\"start\":70},{\"end\":234,\"start\":225},{\"end\":415,\"start\":407}]", "author_affiliation": "[{\"end\":223,\"start\":83},{\"end\":405,\"start\":265},{\"end\":560,\"start\":420}]", "title": "[{\"end\":67,\"start\":1},{\"end\":628,\"start\":562}]", "venue": null, "abstract": "[{\"end\":1788,\"start\":630}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2001,\"start\":1998},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2013,\"start\":2009},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2037,\"start\":2033},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2040,\"start\":2037},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2043,\"start\":2040},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2046,\"start\":2043},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2049,\"start\":2046},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2052,\"start\":2049},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2314,\"start\":2310},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2317,\"start\":2314},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2319,\"start\":2317},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2321,\"start\":2319},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2521,\"start\":2517},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3062,\"start\":3058},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3065,\"start\":3062},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3068,\"start\":3065},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3071,\"start\":3068},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3073,\"start\":3071},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3076,\"start\":3073},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3316,\"start\":3312},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3319,\"start\":3316},{\"end\":3638,\"start\":3634},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3892,\"start\":3888},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3895,\"start\":3892},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3897,\"start\":3895},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3900,\"start\":3897},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3949,\"start\":3945},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3951,\"start\":3949},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3954,\"start\":3951},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4004,\"start\":4000},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4020,\"start\":4017},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4074,\"start\":4070},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4216,\"start\":4212},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4238,\"start\":4234},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6040,\"start\":6036},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6042,\"start\":6040},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6045,\"start\":6042},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6650,\"start\":6647},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6706,\"start\":6702},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6709,\"start\":6706},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8691,\"start\":8687},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8710,\"start\":8707},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10113,\"start\":10109},{\"end\":10253,\"start\":10250},{\"end\":10257,\"start\":10253},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10519,\"start\":10515},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10599,\"start\":10595},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12010,\"start\":12007},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12059,\"start\":12056},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12911,\"start\":12907},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14021,\"start\":14018},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18998,\"start\":18995},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19352,\"start\":19348},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19622,\"start\":19619},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19625,\"start\":19622},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20669,\"start\":20665},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20730,\"start\":20727},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20786,\"start\":20782},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20829,\"start\":20825},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21050,\"start\":21047},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21060,\"start\":21057},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21111,\"start\":21107},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21167,\"start\":21163},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21233,\"start\":21229},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21825,\"start\":21822},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22150,\"start\":22147},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22203,\"start\":22199},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22836,\"start\":22832},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23084,\"start\":23081},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23129,\"start\":23126},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24216,\"start\":24212},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24717,\"start\":24714},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25489,\"start\":25485},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27589,\"start\":27585},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":27889,\"start\":27885},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28221,\"start\":28218},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28561,\"start\":28558},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":30076,\"start\":30072},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30194,\"start\":30190},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31838,\"start\":31835},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32296,\"start\":32292},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32777,\"start\":32773},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":32811,\"start\":32808},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":32814,\"start\":32811},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":32817,\"start\":32814},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":32886,\"start\":32882},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32944,\"start\":32940},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":33210,\"start\":33206},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":34216,\"start\":34212},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":34785,\"start\":34781},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":35566,\"start\":35562},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":39020,\"start\":39017}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36110,\"start\":36024},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36630,\"start\":36111},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37275,\"start\":36631},{\"attributes\":{\"id\":\"fig_4\"},\"end\":37537,\"start\":37276},{\"attributes\":{\"id\":\"fig_5\"},\"end\":37899,\"start\":37538},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":38328,\"start\":37900},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":38570,\"start\":38329},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":38886,\"start\":38571}]", "paragraph": "[{\"end\":2182,\"start\":1804},{\"end\":2794,\"start\":2184},{\"end\":3725,\"start\":2796},{\"end\":4422,\"start\":3727},{\"end\":5489,\"start\":4424},{\"end\":5880,\"start\":5524},{\"end\":6386,\"start\":5909},{\"end\":6765,\"start\":6388},{\"end\":7121,\"start\":6830},{\"end\":7525,\"start\":7123},{\"end\":7774,\"start\":7527},{\"end\":9033,\"start\":7817},{\"end\":9599,\"start\":9035},{\"end\":11070,\"start\":9650},{\"end\":11535,\"start\":11072},{\"end\":11711,\"start\":11537},{\"end\":12541,\"start\":11740},{\"end\":12635,\"start\":12558},{\"end\":13299,\"start\":12714},{\"end\":14737,\"start\":13301},{\"end\":15006,\"start\":14772},{\"end\":15475,\"start\":15090},{\"end\":16055,\"start\":15477},{\"end\":16475,\"start\":16171},{\"end\":16644,\"start\":16558},{\"end\":17358,\"start\":16770},{\"end\":17843,\"start\":17394},{\"end\":18133,\"start\":17845},{\"end\":18914,\"start\":18228},{\"end\":19511,\"start\":18916},{\"end\":20024,\"start\":19513},{\"end\":20549,\"start\":20052},{\"end\":20956,\"start\":20565},{\"end\":22003,\"start\":20987},{\"end\":23238,\"start\":22005},{\"end\":24366,\"start\":23259},{\"end\":24928,\"start\":24368},{\"end\":25490,\"start\":24930},{\"end\":25808,\"start\":25524},{\"end\":26017,\"start\":25810},{\"end\":26323,\"start\":26019},{\"end\":26928,\"start\":26354},{\"end\":28464,\"start\":26947},{\"end\":28758,\"start\":28489},{\"end\":29198,\"start\":29007},{\"end\":29435,\"start\":29279},{\"end\":30290,\"start\":29843},{\"end\":30336,\"start\":30292},{\"end\":30529,\"start\":30491},{\"end\":30956,\"start\":30554},{\"end\":31438,\"start\":31394},{\"end\":31523,\"start\":31522},{\"end\":31631,\"start\":31593},{\"end\":32734,\"start\":31659},{\"end\":32818,\"start\":32736},{\"end\":33408,\"start\":32847},{\"end\":34077,\"start\":33410},{\"end\":34803,\"start\":34104},{\"end\":35279,\"start\":34805},{\"end\":35306,\"start\":35295},{\"end\":35835,\"start\":35322},{\"end\":36023,\"start\":35883}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5908,\"start\":5881},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6829,\"start\":6766},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9649,\"start\":9600},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11739,\"start\":11712},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12557,\"start\":12542},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16170,\"start\":16056},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16557,\"start\":16476},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16769,\"start\":16645},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18227,\"start\":18134},{\"attributes\":{\"id\":\"formula_9\"},\"end\":29006,\"start\":28759},{\"attributes\":{\"id\":\"formula_10\"},\"end\":29278,\"start\":29199},{\"attributes\":{\"id\":\"formula_11\"},\"end\":29528,\"start\":29436},{\"attributes\":{\"id\":\"formula_12\"},\"end\":29842,\"start\":29528},{\"attributes\":{\"id\":\"formula_13\"},\"end\":30490,\"start\":30337},{\"attributes\":{\"id\":\"formula_14\"},\"end\":31393,\"start\":30957},{\"attributes\":{\"id\":\"formula_15\"},\"end\":31521,\"start\":31439},{\"attributes\":{\"id\":\"formula_16\"},\"end\":31592,\"start\":31524}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":10405,\"start\":10398},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":11175,\"start\":11168},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":11422,\"start\":11408},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":11572,\"start\":11558},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":13321,\"start\":13314},{\"end\":22688,\"start\":22681},{\"end\":23632,\"start\":23625}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1802,\"start\":1790},{\"attributes\":{\"n\":\"2\"},\"end\":5522,\"start\":5492},{\"attributes\":{\"n\":\"3\"},\"end\":7815,\"start\":7777},{\"attributes\":{\"n\":\"3.2\"},\"end\":12712,\"start\":12638},{\"attributes\":{\"n\":\"4\"},\"end\":14770,\"start\":14740},{\"attributes\":{\"n\":\"4.1\"},\"end\":15088,\"start\":15009},{\"attributes\":{\"n\":\"4.2\"},\"end\":17392,\"start\":17361},{\"end\":20032,\"start\":20027},{\"end\":20050,\"start\":20035},{\"attributes\":{\"n\":\"5\"},\"end\":20563,\"start\":20552},{\"attributes\":{\"n\":\"5.1\"},\"end\":20985,\"start\":20959},{\"end\":23257,\"start\":23241},{\"attributes\":{\"n\":\"5.2\"},\"end\":25522,\"start\":25493},{\"attributes\":{\"n\":\"6\"},\"end\":26352,\"start\":26326},{\"end\":26945,\"start\":26931},{\"end\":28487,\"start\":28467},{\"end\":30552,\"start\":30532},{\"end\":31657,\"start\":31634},{\"end\":32845,\"start\":32821},{\"end\":34102,\"start\":34080},{\"end\":35293,\"start\":35282},{\"end\":35320,\"start\":35309},{\"end\":35881,\"start\":35838},{\"end\":36029,\"start\":36025},{\"end\":36122,\"start\":36112},{\"end\":36640,\"start\":36632},{\"end\":37287,\"start\":37277},{\"end\":37549,\"start\":37539},{\"end\":37910,\"start\":37901},{\"end\":38339,\"start\":38330}]", "table": "[{\"end\":38328,\"start\":38087},{\"end\":38570,\"start\":38341}]", "figure_caption": "[{\"end\":36110,\"start\":36031},{\"end\":36630,\"start\":36124},{\"end\":37275,\"start\":36642},{\"end\":37537,\"start\":37289},{\"end\":37899,\"start\":37551},{\"end\":38087,\"start\":37912},{\"end\":38886,\"start\":38573}]", "figure_ref": "[{\"end\":20101,\"start\":20093},{\"end\":23659,\"start\":23651},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26264,\"start\":26256},{\"end\":35387,\"start\":35379},{\"end\":35965,\"start\":35957}]", "bib_author_first_name": "[{\"end\":39281,\"start\":39275},{\"end\":39294,\"start\":39290},{\"end\":39662,\"start\":39659},{\"end\":39674,\"start\":39671},{\"end\":39694,\"start\":39687},{\"end\":39706,\"start\":39700},{\"end\":39719,\"start\":39713},{\"end\":39737,\"start\":39729},{\"end\":39739,\"start\":39738},{\"end\":40186,\"start\":40180},{\"end\":40203,\"start\":40195},{\"end\":40213,\"start\":40210},{\"end\":40641,\"start\":40635},{\"end\":40654,\"start\":40648},{\"end\":41009,\"start\":41001},{\"end\":41027,\"start\":41017},{\"end\":41041,\"start\":41033},{\"end\":41058,\"start\":41053},{\"end\":41526,\"start\":41522},{\"end\":41543,\"start\":41536},{\"end\":41557,\"start\":41551},{\"end\":41559,\"start\":41558},{\"end\":41963,\"start\":41958},{\"end\":41976,\"start\":41973},{\"end\":41990,\"start\":41982},{\"end\":42001,\"start\":41998},{\"end\":42432,\"start\":42425},{\"end\":42444,\"start\":42437},{\"end\":42460,\"start\":42452},{\"end\":42470,\"start\":42466},{\"end\":42904,\"start\":42900},{\"end\":42919,\"start\":42914},{\"end\":43224,\"start\":43215},{\"end\":43242,\"start\":43232},{\"end\":43260,\"start\":43253},{\"end\":43696,\"start\":43687},{\"end\":43709,\"start\":43704},{\"end\":43724,\"start\":43717},{\"end\":43969,\"start\":43963},{\"end\":43981,\"start\":43977},{\"end\":43996,\"start\":43990},{\"end\":44011,\"start\":44007},{\"end\":44263,\"start\":44257},{\"end\":44279,\"start\":44274},{\"end\":44293,\"start\":44290},{\"end\":44312,\"start\":44306},{\"end\":44324,\"start\":44320},{\"end\":44338,\"start\":44333},{\"end\":44352,\"start\":44348},{\"end\":44829,\"start\":44827},{\"end\":44838,\"start\":44835},{\"end\":44854,\"start\":44845},{\"end\":44869,\"start\":44861},{\"end\":45238,\"start\":45237},{\"end\":45248,\"start\":45247},{\"end\":45259,\"start\":45258},{\"end\":45266,\"start\":45265},{\"end\":45794,\"start\":45788},{\"end\":45812,\"start\":45806},{\"end\":45831,\"start\":45820},{\"end\":46135,\"start\":46131},{\"end\":46146,\"start\":46143},{\"end\":46158,\"start\":46156},{\"end\":46173,\"start\":46166},{\"end\":46494,\"start\":46488},{\"end\":46514,\"start\":46504},{\"end\":46530,\"start\":46525},{\"end\":46543,\"start\":46538},{\"end\":46561,\"start\":46554},{\"end\":46576,\"start\":46567},{\"end\":46989,\"start\":46985},{\"end\":47010,\"start\":47003},{\"end\":47026,\"start\":47020},{\"end\":47394,\"start\":47388},{\"end\":47402,\"start\":47401},{\"end\":47585,\"start\":47581},{\"end\":47602,\"start\":47599},{\"end\":47612,\"start\":47609},{\"end\":47625,\"start\":47617},{\"end\":47641,\"start\":47634},{\"end\":47656,\"start\":47652},{\"end\":47668,\"start\":47661},{\"end\":47682,\"start\":47676},{\"end\":47699,\"start\":47693},{\"end\":47715,\"start\":47708},{\"end\":47736,\"start\":47727},{\"end\":47738,\"start\":47737},{\"end\":47747,\"start\":47745},{\"end\":48216,\"start\":48213},{\"end\":48232,\"start\":48226},{\"end\":48633,\"start\":48627},{\"end\":48649,\"start\":48643},{\"end\":48665,\"start\":48659},{\"end\":49118,\"start\":49112},{\"end\":49132,\"start\":49128},{\"end\":49153,\"start\":49145},{\"end\":49168,\"start\":49162},{\"end\":49191,\"start\":49177},{\"end\":49585,\"start\":49576},{\"end\":49599,\"start\":49592},{\"end\":49613,\"start\":49605},{\"end\":49855,\"start\":49846},{\"end\":49864,\"start\":49862},{\"end\":49878,\"start\":49870},{\"end\":49893,\"start\":49885},{\"end\":50290,\"start\":50284},{\"end\":50303,\"start\":50300},{\"end\":50319,\"start\":50313},{\"end\":50335,\"start\":50328},{\"end\":50352,\"start\":50344},{\"end\":50369,\"start\":50361},{\"end\":50384,\"start\":50376},{\"end\":50401,\"start\":50394},{\"end\":50418,\"start\":50411},{\"end\":50433,\"start\":50429},{\"end\":50826,\"start\":50823},{\"end\":50835,\"start\":50831},{\"end\":50845,\"start\":50843},{\"end\":50854,\"start\":50851},{\"end\":50868,\"start\":50860},{\"end\":50878,\"start\":50874},{\"end\":51177,\"start\":51174},{\"end\":51193,\"start\":51185},{\"end\":51205,\"start\":51199},{\"end\":51216,\"start\":51212},{\"end\":51226,\"start\":51223},{\"end\":51566,\"start\":51559},{\"end\":51581,\"start\":51574},{\"end\":51595,\"start\":51589},{\"end\":51597,\"start\":51596},{\"end\":52030,\"start\":52025},{\"end\":52044,\"start\":52038},{\"end\":52053,\"start\":52051},{\"end\":52066,\"start\":52060},{\"end\":52077,\"start\":52074}]", "bib_author_last_name": "[{\"end\":39288,\"start\":39282},{\"end\":39300,\"start\":39295},{\"end\":39669,\"start\":39663},{\"end\":39685,\"start\":39675},{\"end\":39698,\"start\":39695},{\"end\":39711,\"start\":39707},{\"end\":39727,\"start\":39720},{\"end\":39746,\"start\":39740},{\"end\":40193,\"start\":40187},{\"end\":40208,\"start\":40204},{\"end\":40219,\"start\":40214},{\"end\":40646,\"start\":40642},{\"end\":40659,\"start\":40655},{\"end\":41015,\"start\":41010},{\"end\":41031,\"start\":41028},{\"end\":41051,\"start\":41042},{\"end\":41069,\"start\":41059},{\"end\":41534,\"start\":41527},{\"end\":41549,\"start\":41544},{\"end\":41565,\"start\":41560},{\"end\":41971,\"start\":41964},{\"end\":41980,\"start\":41977},{\"end\":41996,\"start\":41991},{\"end\":42005,\"start\":42002},{\"end\":42012,\"start\":42007},{\"end\":42435,\"start\":42433},{\"end\":42450,\"start\":42445},{\"end\":42464,\"start\":42461},{\"end\":42474,\"start\":42471},{\"end\":42912,\"start\":42905},{\"end\":42923,\"start\":42920},{\"end\":43230,\"start\":43225},{\"end\":43251,\"start\":43243},{\"end\":43264,\"start\":43261},{\"end\":43702,\"start\":43697},{\"end\":43715,\"start\":43710},{\"end\":43728,\"start\":43725},{\"end\":43975,\"start\":43970},{\"end\":43988,\"start\":43982},{\"end\":44005,\"start\":43997},{\"end\":44016,\"start\":44012},{\"end\":44272,\"start\":44264},{\"end\":44288,\"start\":44280},{\"end\":44304,\"start\":44294},{\"end\":44318,\"start\":44313},{\"end\":44331,\"start\":44325},{\"end\":44346,\"start\":44339},{\"end\":44357,\"start\":44353},{\"end\":44833,\"start\":44830},{\"end\":44843,\"start\":44839},{\"end\":44859,\"start\":44855},{\"end\":44872,\"start\":44870},{\"end\":45245,\"start\":45239},{\"end\":45256,\"start\":45249},{\"end\":45263,\"start\":45260},{\"end\":45272,\"start\":45267},{\"end\":45804,\"start\":45795},{\"end\":45818,\"start\":45813},{\"end\":45838,\"start\":45832},{\"end\":46141,\"start\":46136},{\"end\":46154,\"start\":46147},{\"end\":46164,\"start\":46159},{\"end\":46179,\"start\":46174},{\"end\":46502,\"start\":46495},{\"end\":46523,\"start\":46515},{\"end\":46536,\"start\":46531},{\"end\":46552,\"start\":46544},{\"end\":46565,\"start\":46562},{\"end\":46582,\"start\":46577},{\"end\":47001,\"start\":46990},{\"end\":47018,\"start\":47011},{\"end\":47031,\"start\":47027},{\"end\":47399,\"start\":47395},{\"end\":47410,\"start\":47403},{\"end\":47417,\"start\":47412},{\"end\":47597,\"start\":47586},{\"end\":47607,\"start\":47603},{\"end\":47615,\"start\":47613},{\"end\":47632,\"start\":47626},{\"end\":47650,\"start\":47642},{\"end\":47659,\"start\":47657},{\"end\":47674,\"start\":47669},{\"end\":47691,\"start\":47683},{\"end\":47706,\"start\":47700},{\"end\":47725,\"start\":47716},{\"end\":47743,\"start\":47739},{\"end\":47755,\"start\":47748},{\"end\":48224,\"start\":48217},{\"end\":48237,\"start\":48233},{\"end\":48641,\"start\":48634},{\"end\":48657,\"start\":48650},{\"end\":48675,\"start\":48666},{\"end\":49126,\"start\":49119},{\"end\":49143,\"start\":49133},{\"end\":49160,\"start\":49154},{\"end\":49175,\"start\":49169},{\"end\":49200,\"start\":49192},{\"end\":49590,\"start\":49586},{\"end\":49603,\"start\":49600},{\"end\":49616,\"start\":49614},{\"end\":49860,\"start\":49856},{\"end\":49868,\"start\":49865},{\"end\":49883,\"start\":49879},{\"end\":49896,\"start\":49894},{\"end\":50298,\"start\":50291},{\"end\":50311,\"start\":50304},{\"end\":50326,\"start\":50320},{\"end\":50342,\"start\":50336},{\"end\":50359,\"start\":50353},{\"end\":50374,\"start\":50370},{\"end\":50392,\"start\":50385},{\"end\":50409,\"start\":50402},{\"end\":50427,\"start\":50419},{\"end\":50440,\"start\":50434},{\"end\":50829,\"start\":50827},{\"end\":50841,\"start\":50836},{\"end\":50849,\"start\":50846},{\"end\":50858,\"start\":50855},{\"end\":50872,\"start\":50869},{\"end\":50883,\"start\":50879},{\"end\":51183,\"start\":51178},{\"end\":51197,\"start\":51194},{\"end\":51210,\"start\":51206},{\"end\":51221,\"start\":51217},{\"end\":51232,\"start\":51227},{\"end\":51572,\"start\":51567},{\"end\":51587,\"start\":51582},{\"end\":51603,\"start\":51598},{\"end\":52036,\"start\":52031},{\"end\":52049,\"start\":52045},{\"end\":52058,\"start\":52054},{\"end\":52072,\"start\":52067},{\"end\":52080,\"start\":52078}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":59523708},\"end\":39610,\"start\":39226},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":53770387},\"end\":40135,\"start\":39612},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":11206708},\"end\":40530,\"start\":40137},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":15799108},\"end\":40945,\"start\":40532},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":14883988},\"end\":41453,\"start\":40947},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":9062671},\"end\":41898,\"start\":41455},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":49672261},\"end\":42377,\"start\":41900},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206594692},\"end\":42820,\"start\":42379},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":71134},\"end\":43157,\"start\":42822},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":53751136},\"end\":43622,\"start\":43159},{\"attributes\":{\"doi\":\"arXiv:1906.00651\",\"id\":\"b10\"},\"end\":43910,\"start\":43624},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":173990648},\"end\":44255,\"start\":43912},{\"attributes\":{\"id\":\"b12\"},\"end\":44756,\"start\":44257},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":203593501},\"end\":45095,\"start\":44758},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":64193},\"end\":45700,\"start\":45097},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206429151},\"end\":46129,\"start\":45702},{\"attributes\":{\"doi\":\"arXiv:1910.11908\",\"id\":\"b16\"},\"end\":46425,\"start\":46131},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":208310116},\"end\":46918,\"start\":46427},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3719281},\"end\":47367,\"start\":46920},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":13058320},\"end\":47577,\"start\":47369},{\"attributes\":{\"id\":\"b20\"},\"end\":48161,\"start\":47579},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3612623},\"end\":48607,\"start\":48163},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":4531078},\"end\":48994,\"start\":48609},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":17804904},\"end\":49514,\"start\":48996},{\"attributes\":{\"doi\":\"arXiv:2008.02340\",\"id\":\"b24\"},\"end\":49792,\"start\":49516},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":211171615},\"end\":50203,\"start\":49794},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":53749133},\"end\":50747,\"start\":50205},{\"attributes\":{\"doi\":\"arXiv:1906.06878\",\"id\":\"b27\"},\"end\":51093,\"start\":50749},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":996788},\"end\":51480,\"start\":51095},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":9658690},\"end\":51977,\"start\":51482},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":57189262},\"end\":52333,\"start\":51979},{\"attributes\":{\"id\":\"b31\"},\"end\":52875,\"start\":52335},{\"attributes\":{\"id\":\"b32\"},\"end\":53749,\"start\":52877}]", "bib_title": "[{\"end\":39273,\"start\":39226},{\"end\":39657,\"start\":39612},{\"end\":40178,\"start\":40137},{\"end\":40633,\"start\":40532},{\"end\":40999,\"start\":40947},{\"end\":41520,\"start\":41455},{\"end\":41956,\"start\":41900},{\"end\":42423,\"start\":42379},{\"end\":42898,\"start\":42822},{\"end\":43213,\"start\":43159},{\"end\":43961,\"start\":43912},{\"end\":44825,\"start\":44758},{\"end\":45235,\"start\":45097},{\"end\":45786,\"start\":45702},{\"end\":46486,\"start\":46427},{\"end\":46983,\"start\":46920},{\"end\":47386,\"start\":47369},{\"end\":48211,\"start\":48163},{\"end\":48625,\"start\":48609},{\"end\":49110,\"start\":48996},{\"end\":49844,\"start\":49794},{\"end\":50282,\"start\":50205},{\"end\":51172,\"start\":51095},{\"end\":51557,\"start\":51482},{\"end\":52023,\"start\":51979},{\"end\":53246,\"start\":52877}]", "bib_author": "[{\"end\":39290,\"start\":39275},{\"end\":39302,\"start\":39290},{\"end\":39671,\"start\":39659},{\"end\":39687,\"start\":39671},{\"end\":39700,\"start\":39687},{\"end\":39713,\"start\":39700},{\"end\":39729,\"start\":39713},{\"end\":39748,\"start\":39729},{\"end\":40195,\"start\":40180},{\"end\":40210,\"start\":40195},{\"end\":40221,\"start\":40210},{\"end\":40648,\"start\":40635},{\"end\":40661,\"start\":40648},{\"end\":41017,\"start\":41001},{\"end\":41033,\"start\":41017},{\"end\":41053,\"start\":41033},{\"end\":41071,\"start\":41053},{\"end\":41536,\"start\":41522},{\"end\":41551,\"start\":41536},{\"end\":41567,\"start\":41551},{\"end\":41973,\"start\":41958},{\"end\":41982,\"start\":41973},{\"end\":41998,\"start\":41982},{\"end\":42007,\"start\":41998},{\"end\":42014,\"start\":42007},{\"end\":42437,\"start\":42425},{\"end\":42452,\"start\":42437},{\"end\":42466,\"start\":42452},{\"end\":42476,\"start\":42466},{\"end\":42914,\"start\":42900},{\"end\":42925,\"start\":42914},{\"end\":43232,\"start\":43215},{\"end\":43253,\"start\":43232},{\"end\":43266,\"start\":43253},{\"end\":43704,\"start\":43687},{\"end\":43717,\"start\":43704},{\"end\":43730,\"start\":43717},{\"end\":43977,\"start\":43963},{\"end\":43990,\"start\":43977},{\"end\":44007,\"start\":43990},{\"end\":44018,\"start\":44007},{\"end\":44274,\"start\":44257},{\"end\":44290,\"start\":44274},{\"end\":44306,\"start\":44290},{\"end\":44320,\"start\":44306},{\"end\":44333,\"start\":44320},{\"end\":44348,\"start\":44333},{\"end\":44359,\"start\":44348},{\"end\":44835,\"start\":44827},{\"end\":44845,\"start\":44835},{\"end\":44861,\"start\":44845},{\"end\":44874,\"start\":44861},{\"end\":45247,\"start\":45237},{\"end\":45258,\"start\":45247},{\"end\":45265,\"start\":45258},{\"end\":45274,\"start\":45265},{\"end\":45806,\"start\":45788},{\"end\":45820,\"start\":45806},{\"end\":45840,\"start\":45820},{\"end\":46143,\"start\":46131},{\"end\":46156,\"start\":46143},{\"end\":46166,\"start\":46156},{\"end\":46181,\"start\":46166},{\"end\":46504,\"start\":46488},{\"end\":46525,\"start\":46504},{\"end\":46538,\"start\":46525},{\"end\":46554,\"start\":46538},{\"end\":46567,\"start\":46554},{\"end\":46584,\"start\":46567},{\"end\":47003,\"start\":46985},{\"end\":47020,\"start\":47003},{\"end\":47033,\"start\":47020},{\"end\":47401,\"start\":47388},{\"end\":47412,\"start\":47401},{\"end\":47419,\"start\":47412},{\"end\":47599,\"start\":47581},{\"end\":47609,\"start\":47599},{\"end\":47617,\"start\":47609},{\"end\":47634,\"start\":47617},{\"end\":47652,\"start\":47634},{\"end\":47661,\"start\":47652},{\"end\":47676,\"start\":47661},{\"end\":47693,\"start\":47676},{\"end\":47708,\"start\":47693},{\"end\":47727,\"start\":47708},{\"end\":47745,\"start\":47727},{\"end\":47757,\"start\":47745},{\"end\":48226,\"start\":48213},{\"end\":48239,\"start\":48226},{\"end\":48643,\"start\":48627},{\"end\":48659,\"start\":48643},{\"end\":48677,\"start\":48659},{\"end\":49128,\"start\":49112},{\"end\":49145,\"start\":49128},{\"end\":49162,\"start\":49145},{\"end\":49177,\"start\":49162},{\"end\":49202,\"start\":49177},{\"end\":49592,\"start\":49576},{\"end\":49605,\"start\":49592},{\"end\":49618,\"start\":49605},{\"end\":49862,\"start\":49846},{\"end\":49870,\"start\":49862},{\"end\":49885,\"start\":49870},{\"end\":49898,\"start\":49885},{\"end\":50300,\"start\":50284},{\"end\":50313,\"start\":50300},{\"end\":50328,\"start\":50313},{\"end\":50344,\"start\":50328},{\"end\":50361,\"start\":50344},{\"end\":50376,\"start\":50361},{\"end\":50394,\"start\":50376},{\"end\":50411,\"start\":50394},{\"end\":50429,\"start\":50411},{\"end\":50442,\"start\":50429},{\"end\":50831,\"start\":50823},{\"end\":50843,\"start\":50831},{\"end\":50851,\"start\":50843},{\"end\":50860,\"start\":50851},{\"end\":50874,\"start\":50860},{\"end\":50885,\"start\":50874},{\"end\":51185,\"start\":51174},{\"end\":51199,\"start\":51185},{\"end\":51212,\"start\":51199},{\"end\":51223,\"start\":51212},{\"end\":51234,\"start\":51223},{\"end\":51574,\"start\":51559},{\"end\":51589,\"start\":51574},{\"end\":51605,\"start\":51589},{\"end\":52038,\"start\":52025},{\"end\":52051,\"start\":52038},{\"end\":52060,\"start\":52051},{\"end\":52074,\"start\":52060},{\"end\":52082,\"start\":52074}]", "bib_venue": "[{\"end\":39425,\"start\":39372},{\"end\":39889,\"start\":39827},{\"end\":41688,\"start\":41636},{\"end\":42155,\"start\":42093},{\"end\":42617,\"start\":42555},{\"end\":43407,\"start\":43345},{\"end\":44482,\"start\":44429},{\"end\":45403,\"start\":45347},{\"end\":48380,\"start\":48318},{\"end\":48818,\"start\":48756},{\"end\":50007,\"start\":49961},{\"end\":51746,\"start\":51684},{\"end\":39370,\"start\":39302},{\"end\":39825,\"start\":39748},{\"end\":40306,\"start\":40221},{\"end\":40723,\"start\":40661},{\"end\":41150,\"start\":41071},{\"end\":41634,\"start\":41567},{\"end\":42091,\"start\":42014},{\"end\":42553,\"start\":42476},{\"end\":42974,\"start\":42925},{\"end\":43343,\"start\":43266},{\"end\":43685,\"start\":43624},{\"end\":44067,\"start\":44018},{\"end\":44427,\"start\":44359},{\"end\":44910,\"start\":44874},{\"end\":45345,\"start\":45274},{\"end\":45895,\"start\":45840},{\"end\":46256,\"start\":46197},{\"end\":46651,\"start\":46584},{\"end\":47119,\"start\":47033},{\"end\":47459,\"start\":47419},{\"end\":47804,\"start\":47757},{\"end\":48316,\"start\":48239},{\"end\":48754,\"start\":48677},{\"end\":49238,\"start\":49202},{\"end\":49574,\"start\":49516},{\"end\":49959,\"start\":49898},{\"end\":50456,\"start\":50442},{\"end\":50821,\"start\":50749},{\"end\":51271,\"start\":51234},{\"end\":51682,\"start\":51605},{\"end\":52144,\"start\":52082},{\"end\":52583,\"start\":52335},{\"end\":53272,\"start\":53248}]"}}}, "year": 2023, "month": 12, "day": 17}