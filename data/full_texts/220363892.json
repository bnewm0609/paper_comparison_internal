{"id": 220363892, "updated": "2023-10-06 13:22:42.809", "metadata": {"title": "Self-Challenging Improves Cross-Domain Generalization", "authors": "[{\"first\":\"Zeyi\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Haohan\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Eric\",\"last\":\"Xing\",\"middle\":[\"P.\"]},{\"first\":\"Dong\",\"last\":\"Huang\",\"middle\":[]}]", "venue": "Computer Vision \u2013 ECCV 2020", "journal": "Computer Vision \u2013 ECCV 2020", "publication_date": {"year": 2020, "month": 7, "day": 5}, "abstract": "Convolutional Neural Networks (CNN) conduct image classification by activating dominant features that correlated with labels. When the training and testing data are under similar distributions, their dominant features are similar, which usually facilitates decent performance on the testing data. The performance is nonetheless unmet when tested on samples from different distributions, leading to the challenges in cross-domain image classification. We introduce a simple training heuristic, Representation Self-Challenging (RSC), that significantly improves the generalization of CNN to the out-of-domain data. RSC iteratively challenges (discards) the dominant features activated on the training data, and forces the network to activate remaining features that correlates with labels. This process appears to activate feature representations applicable to out-of-domain data without prior knowledge of new domain and without learning extra network parameters. We present theoretical properties and conditions of RSC for improving cross-domain generalization. The experiments endorse the simple, effective and architecture-agnostic nature of our RSC method.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2007.02454", "mag": "3106845355", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/HuangWXH20", "doi": "10.1007/978-3-030-58536-5_8"}}, "content": {"source": {"pdf_hash": "09472ff0d3c3f975ef1fdc02cfb1605d3d4275fa", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2007.02454v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2007.02454", "status": "GREEN"}}, "grobid": {"id": "b1860e7e0edcbdf4693633f0d65fced8ff385f87", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/09472ff0d3c3f975ef1fdc02cfb1605d3d4275fa.txt", "contents": "\nSelf-Challenging Improves Cross-Domain Generalization\n\n\nZeyi Huang zeyih@andrew \nSchool of Computer Science\nCarnegie Mellon University\n\n\nHaohan Wang haohanw@cs \nSchool of Computer Science\nCarnegie Mellon University\n\n\nEric P Xing epxing@cs \nSchool of Computer Science\nCarnegie Mellon University\n\n\nDong Huang \nSchool of Computer Science\nCarnegie Mellon University\n\n\nSelf-Challenging Improves Cross-Domain Generalization\ncross-domain generalization, robustness\nConvolutional Neural Networks (CNN) conduct image classification by activating dominant features that correlated with labels. When the training and testing data are under similar distributions, their dominant features are similar, leading to decent test performance. The performance is nonetheless unmet when tested with different distributions, leading to the challenges in cross-domain image classification. We introduce a simple training heuristic, Representation Self-Challenging (RSC), that significantly improves the generalization of CNN to the outof-domain data. RSC iteratively challenges (discards) the dominant features activated on the training data, and forces the network to activate remaining features that correlates with labels. This process appears to activate feature representations applicable to out-of-domain data without prior knowledge of new domain and without learning extra network parameters. We present theoretical properties and conditions of RSC for improving cross-domain generalization. The experiments endorse the simple, effective, and architecture-agnostic nature of our RSC method.\n\nIntroduction\n\nImagine teaching a child to visually differentiate \"dog\" from \"cat\": when presented with a collection of illustrations from her picture books, she may immediately answer that \"cats tend to have chubby faces\" and end the learning. However, if we continue to ask for more differences, she may start to notice other features like ears or body-size. We conjecture this follow-up challenge question plays a significant role in helping human reach the remarkable generalization ability. Most people should be able to differentiate \"cat\" from \"dog\" visually even when the images are presented in irregular qualities. After all, we did not stop learning after we picked up the first clue when we were children, even the first clue was good enough to help us recognize all the images in our textbook.\n\nNowadays, deep neural networks have exhibited remarkable empirical results over various computer vision tasks, yet these impressive performances seem unmet when the models are tested with the samples in irregular qualities (i.e.,  Fig. 1. The essence of our Representation Self-Challenging (RSC) training method: top two panels: the algorithm mutes the feature representations associated with the highest gradient, such that the network is forced to predict the labels through other features; bottom panel: after training, the model is expected to leverage more features for prediction in comparison to models trained convetionally.\n\nout-of-domain data, samples collected from the distributions that are similar to, but different from the distributions of the training samples). To account for this discrepancy, technologies have been invented under the domain adaptation regime [2,3], where the goal is to train a model invariant to the distributional differences between the source domain (i.e., the distribution of the training samples) and the target domain (i.e., the distribution of the testing samples) [5,32].\n\nAs the influence of machine learning increases, the industry starts to demand the models that can be applied to the domains that are not seen during the training phase. Domain generalization [18], as an extension of domain adaptation, has been studied as a response. The central goal is to train a model that can align the signals from multiple source domains.\n\nFurther, Wang et al. extend the problem to ask how to train a model that generalizes to an arbitrary domain with only the training samples, but not the corresponding domain information, as these domain information may not be available in the real world [31]. Our paper builds upon this set-up and aims to offer a solution that allows the model to be robustly trained without domain information and to empirically perform well on unseen domains.\n\nIn this paper, we introduce a simple training heuristic that improves crossdomain generalization. This approach discards the representations associated with the higher gradients at each epoch, and forces the model to predict with remaining information. Intuitively, in a image classification problem, our heuristic works like a \"self-challenging\" mechanism as it prevents the fully-connected layers to predict with the most predictive subsets of features, such as the most frequent color, edges, or shapes in the training data. We name our method Representation Self Challenging (RSC) and illustrate its main idea in Figure 1.\n\nWe present mathematical analysis that RSC induces a smaller generalization bound. We further demonstrate the empirical strength of our method with domain-agnostic cross-domain evaluations, following previous setup [31]. We also conduct ablation study to examine the alignment between its empirical performance and our intuitive understanding. The inspections also shed light upon the choices of its extra hyperparameter.\n\n\nRelated Work\n\nWe summarize the related DG works from two perspectives: learning domain invariant features and augmenting source domain data. Further, as RSC can be broadly viewed as a generic training heuristic for CNN, we also briefly discuss the general-purpose regularizations that appear similar to our method.\n\nDG through Learning Domain Invariant Features: These methods typically minimize the discrepancy between source domains assuming that the resulting features will be domain-invariant and generalize well for unseen target distributions. Along this track, Muandet et al. employed Maximum Mean Discrepancy (MMD) [18]. Ghifary et al. proposed a multi-domain reconstruction auto-encoder [10]. Li et al. applied MMD constraints to an autoencoder via adversarial training [15].\n\nRecently, meta-learning based techniques start to be used to solve DG problems. Li et al. alternates domain-specific feature extractors and classifiers across domains via episodic training, but without using inner gradient descent update [14]. Balaji et al. proposed MetaReg that learns a regularization function (e.g., weighted 1 loss) particularly for the networks classification layer, while excluding the feature extractor [1].\n\nFurther, recent DG works forgo the requirement of source domains partitions and directly learn the cross-domain generalizable representations through a mixed collection of training data. Wang et al. extracted robust feature representation by projecting out superficial patterns like color and texture [31]. Wang et al. penalized models tendency in predicting with local features in order to extract robust globe representation [30]. RSC follows this more recent path and directly activates more features in all source domain data for DG without knowledge of the partition of source domains.\n\nDG through Augmenting Source Domain: These methods augment the source domain to a wider span of the training data space, enlarging the possibility of covering the span of the data in the target domain. For example, An auxiliary domain classifier has been introduced to augment the data by perturbing input data based on the domain classification signal [23]. Volpi et al. developed an adversarial approach, in which samples are perturbed according to fictitious target distributions within a certain Wasserstein distance from the source [29]. A recent method with state-of-the art performance is JiGen [4], which leverages self-supervised signals by solving jigsaw puzzles.\n\nKey difference: These approaches usually introduce a model-specific DG model and rely on prior knowledge of the target domain, for instance, the tar-get spatial permutation is assumed by JiGen [4]. In contrast, RSC is a modelagnostic training algorithm that aims to improve the cross-domain robustness of any given model. More importantly, RSC does not utilize any knowledge of partitions of domains, either source domain or target domain, which is the general scenario in real world application.\n\nGeneric Model Regularization: CNNs are powerful models and tend to overfit on source domain datasets. From this perspective, model regularization, e.g., weight decay [19], early stopping, and shake-shake regularization [8], could also improve the DG performance. Dropout [25] mutes features by randomly zeroing each hidden unit of the neural network during the training phase. In this way, the network benefit from the assembling effect of small subnetworks to achieve a good regularization effect. Cutout [6] and HaS [24] randomly drop patches of input images. SpatialDropout [26] randomly drops channels of a feature map. DropBlock [9] drops contiguous regions from feature maps instead of random units. DropPath [11] zeroes out an entire layer in training, not just a particular unit. MaxDrop [20] selectively drops features of high activations across the feature map or across the channels. Adversarial Dropout [21] dropouts for maximizing the divergence between the training supervision and the outputs from the network. [12] leverages Adversarial Dropout [21] to learn discriminative features by enforcing the cluster assumption.\n\nKey difference: RSC differs from above methods in that RSC locates and mutes most predictive parts of feature maps by gradients instead of randomness, activation or prediction divergence maximization. This selective process plays an important role in improving the convergence, as we will briefly argue later.\n\n\nMethod\n\nNotations: (x, y) denotes a sample-label pair from the data collection (X, Y) with n samples, and z (or Z) denotes the feature representation of (x, y) learned by a neural network. f (\u00b7; \u03b8) denotes the CNN model, whose parameters are denoted as \u03b8. h(\u00b7; \u03b8 top ) denotes the task component of f (\u00b7; \u03b8); h(\u00b7; \u03b8 top ) takes z as input and outputs the logits prior to a softmax function; \u03b8 top denotes the parameters of h(\u00b7; \u03b8 top ). l(\u00b7, \u00b7) denotes a generic loss function. RSC requires one extra scalar hyperparameter: the percentage of the representations to be discarded, denoted as p. Further, we use \u00b7 to denote the estimated quantities, use\u00b7 to denote the quantities after the representations are discarded, and use t in the subscript to index the iteration. For example, \u03b8 t means the estimated parameter at iteration t.\n\n\nSelf-Challenging Algorithm\n\nAs a generic deep learning training method, RSC solves the same standard loss function as the ones used by many other neural networks, i.e.,\n\u03b8 = arg min \u03b8 x,y \u223c X,Y l(f (x; \u03b8), y),\nbut RSC solves it in a different manner. At each iteration, RSC inspects the gradient, identifies and then mutes the most predictive subset of the representation z (by setting the corresponding values to zero), and finally updates the entire model.\n\nThis simple heuristic has three steps (for simplicity, we drop the indices of samples and assume the batch size is 1 in the following equations):\n\n1. Locate: RSC first calculates the gradient of upper layers with respect to the representation as follows:\ng z = \u2202(h(z; \u03b8 top t ) y)/\u2202z,(1)\nwhere denotes an element-wise product. Then RSC computes the (100 \u2212 p) th percentile, denoted as q p . Then it constructs a masking vector m in the same dimension of g as follows. For the i th element:\nm(i) = 0, if g z (i) \u2265 q p 1, otherwise(2)\nIn other words, RSC creates a masking vector m, whose element is set to 0 if the corresponding element in g is one of the top p percentage elements in g, and set to 1 otherwise. 2. Mute: For every representation z, RSC masks out the bits associated with larger gradients by:z\n= z m(3)\n3. Update: RSC computes the softmax with perturbed representation with\ns = softmax(h(z; \u03b8 top t )),(4)\nand then use the gradientg\n\u03b8 = \u2202l(s, y)/\u2202 \u03b8 t(5)\nto update the entire model for \u03b8 t+1 with optimizers such as SGD or ADAM.\n\nWe summarize the procedure of RSC in Algorithm 1. No that operations of RSC comprise of only few simple operations such as pooling, threshold and element-wise product. Besides the weights of the original network, no extra parameter needs to be learned.\n\n\nTheoretical Evidence\n\nTo expand the theoretical discussion smoothly, we will refer to the \"dog\" vs. \"cat\" classification example repeatedly as we progress. The basic set-up, as we introduced in the beginning of this paper, is the scenario of a child trying to learn the concepts of \"dog\" vs. \"cat\" from illustrations in her book: while the ; update \u03b8t+1 as a function of \u03b8t andg \u03b8 end end hypothesis \"cats tend to have chubby faces\" is good enough to classify all the animals in her picture book, other hypotheses mapping ears or body-size to labels are also predictive.\n\nOn the other hand, if she wants to differentiate all the \"dogs\" from \"cats\" in the real world, she will have to rely on a complicated combination of the features mentioned about. Our main motivation of this paper is as follows: this complicated combination of these features is already illustrated in her picture book, but she does not have to learn the true concept to do well in her finite collection of animal pictures.\n\nThis disparity is officially known as \"covariate shift\" in domain adaptation literature: the conditional distribution (i.e., the semantic of a cat) is the same across every domain, but the model may learn something else (i.e., chubby faces) due to the variation of marginal distributions.\n\nWith this connection built, we now proceed to the theoretical discussion, where we will constantly refer back to this \"dog\" vs. \"cat\" example.\n\nBackground As the large scale deep learning models, such as AlexNet or ResNet, are notoriously hard to be analyzed statistically, we only consider a simplified problem to argue for the theoretical strength of our method: we only concern with the upper layer h(\u00b7; \u03b8 top ) and illustrate that our algorithm helps improve the generalization of h(\u00b7; \u03b8 top ) when Z is fixed. Therefore, we can directly treat Z as the data (features). Also, for convenience, we overload \u03b8 to denote \u03b8 top within the theoretical evidence section.\n\nWe expand our notation set for the theoretical analysis. As we study the domain-agnostic cross-domain setting, we no longer work with i.i.d data. Therefore, we use Z and Y to denote the collection of distributions of features and labels respectively. Let \u0398 be a hypothesis class, where each hypothesis \u03b8 \u2208 \u0398 maps Z to Y. We use a set D (or S) to index Z, Y and \u03b8. Therefore, \u03b8 (D) denotes the hypothesis with minimum error in the distributions specified with D, but with no guarantees on the other distributions.\n\ne.g., \u03b8 (D) can be \"cats have chubby faces\" when D specifies the distribution to be picture book.\n\nFurther, \u03b8 denotes the classifier with minimum error on every distribution considered. If the hypothesis space is large enough, \u03b8 should perform no worse than \u03b8 (D) on distributions specified by D for any D.\n\ne.g., \u03b8 is the true concept of \"cat\", and it should predict no worse than \"cats have chubby faces\" even when the distribution is picture book.\n\nWe use \u03b8 to denote any ERM and use \u03b8 RSC to denote the ERM estimated by the RSC method. Finally, following conventions, we consider l(\u00b7, \u00b7) as the zeroone loss and use a shorthand notation L(\u03b8; D) = E z,y \u223c Z(D),Y(D) l(h(z; \u03b8), y) for convenience, and we only consider the finite hypothesis class case within the scope of this paper, which leads to the first formal result:\nCorollary 1. If |e(z(S); \u03b8 RSC ) \u2212 e(z(S); \u03b8 RSC )| \u2264 \u03be(p),(6)\nwhere e(\u00b7; \u00b7) is a function defined as e(z; \u03b8 ) := E z,y \u223cS l(f (z; \u03b8 ); y) and \u03be(p) is a small number and a function of RSC's hyperparameter p;z is the perturbed version of z generated by RSC, it is also a function of p, but we drop the notation for simplicity. If Assumptions A1, A2, and A3 (See Appendix) hold, we have, with probability at least 1 \u2212 \u03b4 L( \u03b8 RSC (S); S) \u2212 L(\u03b8 RSC (S); D)\n\n\u2264 (2\u03be(p) + 1) 2(log(2|\u0398 RSC |) + log(2/\u03b4)) n\n\nAs the result shows, whether RSC will succeed depends on the magnitude of \u03be(p). The smaller \u03be(p) is, the tighter the bound is, the better the generalization bound is. Interestingly, if \u03be(p) = 0, our result degenerates to the classical generalization bound of i.i.d data.\n\nWhile it seems the success of our method will depend on the choice of \u0398 to meet Condition 6, we will show RSC is applicable in general by presenting it forces the empirical counterpart \u03be(p) to be small. \u03be(p) is defined as\n\u03be(p) :=|h( \u03b8 RSC , z) \u2212 h( \u03b8 RSC ,z)|,\nwhere the function h(\u00b7, \u00b7) is defined as\nh( \u03b8 RSC , z) = (z,y)\u223cS l(f (z; \u03b8 RSC ); y).(7)\nWe will show \u03be(p) decreases at every iteration with more assumptions:\n\nA4: Discarding the most predictive features will increase the loss at current iteration. A5: The learning rate \u03b7 is sufficiently small (\u03b7 2 or higher order terms are negligible).\n\nFormally,\nCorollary 2. If Assumption A4 holds, we can simply denote h( \u03b8 RSC (t),z t ) = \u03b3 t (p)h( \u03b8 RSC (t), z t ),\nwhere h(\u00b7, \u00b7) is defined in Equation 7. \u03b3 t (p) is an arbitrary number greater than 1, also a function of RSC's hyperparameter p. Also, if Assumption A5 holds, we have:\n\u0393 ( \u03b8 RSC (t + 1)) = \u0393 ( \u03b8 RSC (t)) \u2212 (1 \u2212 1 \u03b3 t (p) )||g|| 2 2 \u03b7 where \u0393 ( \u03b8 RSC (t)) := |h( \u03b8 RSC (t), z t ) \u2212 h( \u03b8 RSC (t),z t )|\nt denotes the iteration, z t (orz t ) denotes the features (or perturbed features) at iteration t, andg = \u2202h( \u03b8 RSC (t),z t )/\u2202 \u03b8 RSC (t) Fig. 2. \u0393 ( \u03b8RSC(t)), i.e., \"Loss Difference\", plotted for the PACS experiment (details of the experiment setup will be discussed later). Except for the first epoch, \u0393 ( \u03b8RSC(t)) decreases consistently along the training process.\n\nNotice that \u03be(p) = \u0393 ( \u03b8 RSC ), where \u03b8 RSC is \u03b8 RSC (t) at the last iteration t. We can show that \u03be(p) is a small number because \u0393 ( \u03b8 RSC (t)) gets smaller at every iteration. This discussion is also verified empirically, as shown in Figure 2.\n\nThe decreasing speed of \u0393 ( \u03b8 RSC (t)) depends on the scalar \u03b3 t (p): the greater \u03b3 t (p) is, the faster \u0393 ( \u03b8 RSC (t)) descends. Further, intuitively, the scale of \u03b3 t (p) is highly related to the mechanism of RSC and its hyperparameter p. For example, RSC discards the most predictive representations, which intuitively guarantees the increment of the empirical loss (Assumption A4).\n\nFinally, the choice of p governs the increment of the empirical loss: if p is small, the perturbation will barely affect the model, thus the increment will be small; while if p is large, the perturbation can alter the model's response dramatically, leading to significant ascend of the loss. However, we cannot blindly choose the largest possible p because if p is too large, the model may not be able to learn anything predictive at each iteration.\n\nIn summary, we offer the intuitive guidance of the choice of hyperparamter p: for the same model and setting, the smaller p is, the smaller the training error will be; the bigger p is, the smaller the (cross-domain) generalization error (i.e., difference between testing error and training error) will be.\n\nTherefore, the success of our method depends on the choice of p as a balance of the above two goals.\n\n\nEngineering Specification & Extensions\n\nFor simplicity, we detail the RSC implementation on a ResNet backbone + FC classification network. RSC is applied to the training phase, and operates on the last convolution feature tensor of ResNet. Denote the feature tensor of an input sample as Z and its gradient tensor of as G. G is computed by back propagating the classification score with respect to the ground truth category. Both of them are of size [7 \u00d7 7 \u00d7 512]. Spatial-wise RSC: In the training phase, global average pooling is applied along the channel dimension to the gradient tensor G to produce a weighting matrix w i of size [7 \u00d7 7]. Using this matrix, we select top p percentage of the 7 \u00d7 7 = 49 cells, and mute its corresponding features in Z. Each of the 49 cells correspond to a [1\u00d71\u00d7512] feature vector in Z. After that, the new feature tensor Z new is forwarded to the new network output. Finally, the network is updated through back-propagation. We refer this setup as spatial-wise RSC, which is the default RSC for the rest of this paper.\n\nChannel-wise RSC: RSC can also be implemented by dropping features of the channels with high-gradients. The rational behind the channel-wise RSC lies in the convolutional nature of DNNs. The feature tensor of size [7 \u00d7 7 \u00d7 512] can be considered a decomposed version of input image, where instead of the RGB colors, there are 512 different characteristics of the each pixels. The C characteristics of each pixel contains different statistics of training data from that of the spatial feature statistics.\n\nFor channel-wise RSC, global average pooling is applied along the spatial dimension of G, and produce a weighting vector of size [1 \u00d7 512]. Using this vector, we select top p percentage of its 512 cells, and mute its corresponding features in Z. Here, each of the 512 cells correspond to a [7 \u00d7 7] feature matrix in Z. After that, the new feature tensor Z new is forwarded to the new network output. Finally, the network is updated through back-propagation.\n\nBatch Percentage: Some dropout methods like curriculum dropout [17] do not apply dropout at the beginning of training, which improves CNNs by learning basic discriminative clues from unchanged feature maps. Inspired by these methods, we randomly apply RSC to some samples in each batch, leaving the other unchanged. This introduces one extra hyperparameter, namely Batch Percentage: the percentage of samples to apply RSC in each batch. We also apply RSC to top percentage of batch samples based on cross-entropy loss. This setup is slight better than randomness.\n\nDetailed ablation study on above extensions will be conducted in the experiment section below.\n\n\nExperiments\n\n\nDatasets\n\nWe consider the following four data collections as the battleground to evaluate RSC against previous methods. -ImageNet-Sketch [30]: 1000 classes with two domains. The protocol is to train on standard ImageNet [22] training set and test on ImageNet-Sketch.\n\n\nAblation Study\n\nWe conducted five ablation studies on possible configurations for RSC on the PACS dataset [13]. All results were produced based on the ResNet18 baseline in [4] and were averaged over five runs.\n\n(1) Feature Dropping Strategies (Table 1). We compared the two attention mechanisms to select the most discriminative spatial features. The \"Top-Activiation\" [20] selects the features with highest norms, whereas the \"Top-Gradient\" (default in RSC) selects the features with high gradients. The comparison shows that \"Top-Gradient\" is better than \"Top-Activation\", while both are better than the random strategy. Without specific note, we will use \"Top-Gradient\" as default in the following ablation study.\n\n(2) Feature Dropping Percentage (choice of p) ( Table 2): We ran RSC at different dropping percentages to mute spatial feature maps. The highest average accuracy was reached at p = 33.3%. While the best choice of p is data-specific, our results align well with the theoretical discussion: the optimal p should be neither too large nor too small.\n\n(3) Batch Percentage (Table 3): RSC has the option to be only randomly applied to a subset of samples in each batch. Table 3 shows that the performance    Table 4. Ablation study of Spatial-wise RSC verse Spatial+Channel RSC. We used the best strategy and parameter by  Table 5. Ablation study of Dropout methods. \"S\" and \"C\" represent spatial-wise and channel-wise respectively. For fair comparison, results of above methods are report at their best setting and hyperparameters. RSC used the hyperparameters selected in above ablation studies:\"Top-Gradient\", Feature Dropping Percentage (33.3%) and Batch Percentage (33.3%).\n\nis relatively constant. Nevertheless we still choose 33.3% as the best option on the PACS dataset.\n\n(4) Spatial-wise plus Channel-wise RSC (Table 4): In \"Spatial+Channel\", both spatial-wise and channel-wise RSC were applied on a sample at 50% probability, respectively. (Better options of these probabilities could be explored.) Its improvement over Spatial-wise RSC indicates that it further activated features beneficial to target domains.\n\n\nPACS\n\nbackbone artpaint cartoon sketch photo Avg \u2191  Table 7. DG results on VLCS [27] (Best in bold).\n\n(5) Comparison with different dropout methods (Table 5): Dropout has inspired a number of regularization methods for CNNs. The main differences between those methods lie in applying stochastic or non-stochastic dropout mechanism at input data, convolutional or fully connected layers. Results shows that our gradient-based RSC is better. We believe that gradient is an efficient and straightforward way to encode the sensitivity of output prediction. To the best of our knowledge, we compare with the most related works and illustrate the impact of gradients. (a) Cutout [6]. Cutout conducts random dropout on input images, which shows limited improvement over the baseline. (b) DropBlock [9]. DropBlock tends to dropout discriminative activated parts spatially. It is better than random dropout but inferior to non-stochastic dropout methods in Table 5 such as AdversarialDropout, Top-Activation and our RSC. (c) Adver-sarialDropout [21,12]. AdversarialDropout is based on divergence maximization, while RSC is based on top gradients in generating dropout masks. Results show evidence that the RSC is more effective than AdversarialDropout. (d) Random and Top-Activation dropout strategies at their best hyperparameter settings.   Table 9. DG results on ImageNet-Sketch [30].\n\n\nCross-Domain Evaluation\n\nThrough the following experiments, we used \"Top-Gradient\" as feature dropping strategy, 33.3% as Feature Dropping Percentages, 33.3% as Batch Percentage, and Spatial+Channel RSC. All results were averaged over five runs. In our RSC implementation, we used the SGD solver, 30 epochs, and batch size 128. The learning rate starts with 0.004 for ResNet and 0.001 for AlexNet, learning rate decayed by 0.1 after 24 epochs. For PACS experiment, we used the same data augmentation protocol of randomly cropping the images to retain between 80% to 100%, randomly applied horizontal flipping and randomly (10% probability) convert the RGB image to greyscale, following [4]. In Table. 6,7,8, we compare RSC with the latest domain generalization work, such as Hex [31], PAR [30], JiGen [4] and MetaReg [1]. All these work only report results on different small networks and datasets. For fair comparison, we compared RSC to their reported performances with their most common choices of DNNs (i.e., AlexNet, ResNet18, and ResNet50) and datasets. RSC consistently outperforms other competing methods.\n\nThe empirical performance gain of RSC can be better appreciated if we have a closer look at the PACS experiment in Table. 6. The improvement of RSC from the latest baselines [4] are significant and consistent: 4.5 on AlexNet, 5.2 on ResNet18, and 4.5 on ResNet50. It is noticeable that, with both ResNet18 and ResNet50, RSC boosts the performance significantly for sketch domain, which is the only colorless domain. The model may have to understand the semantics of the object to perform well on the sketch domain. On the other hand, RSC performs only marginally better than competing methods in photo domain, which is probably because that photo domain is the simplest one and every method has already achieved high accuracy on it.\n\n\nDiscussion\n\nStandard ImageNet Benchmark: With the impressive performance observed in the cross-domain evaluation, we further explore to evaluate the benefit of RSC with other benchmark data and higher network capacity.  Table 10. Generalization results on ImageNet. Baseline was produced with official Pytorch implementation and their ImageNet models.\n\nWe conducted image classification experiments on the Imagenet database [22]. We chose three backbones with the same architectural design while with clear hierarchies in model capacities: ResNet50, ResNet101, and ResNet152. All models were finetuned for 80 epochs with learning rate decayed by 0.1 every 20 epochs. The initial learning rate for ResNet was 0.01. All models follow extra the same training prototype in default Pytorch ImageNet implementation 1 , using original batch size of 256, standard data augmentation and 224 \u00d7 224 as input size.\n\nThe results in Table 10 shows that RSC exhibits the ability reduce the performance gap between networks of same family but different sizes (i.e., ResNet50 with RSC approaches the results of baseline ResNet101, and ResNet101 with RSC approaches the results of baseline ResNet151). The practical implication is that, RSC could induce faster performance saturation than increasing model sizes. Therefore one could scale down the size of networks to be deployed at comparable performance.\n\n\nConclusion\n\nWe introduced a simple training heuristic method that can be directly applied to almost any CNN architecture with no extra model architecture, and almost no increment of computing efforts. We name our method Representation Selfchallenging (RSC). RSC iteratively forces a CNN to activate features that are less dominant in the training domain, but still correlated with labels. Theoretical and empirical analysis of RSC validate that it is a fundamental and effective way of expanding feature distribution of the training domain. RSC produced the state-of-the-art improvement over baseline CNNs under the standard DG settings of small networks and small datasets. Moreover, our work went beyond the standard DG settings, to illustrate effectiveness of RSC on more prevalent problem scales, e.g., the ImageNet database and network sizes up-to ResNet152.\n\nNotice that this does not contradict with our cross-domain set-up: while Assumption A3 implies that data from any distribution of interest is i.i.d (otherwise the operation E A [] is not valid), the cross-domain difficulty is raised when only different subsets of A are used for train and test. For example, considering A to be a uniform distribution of [0, 1], while the train set is uniformly sampled from [0, 0.5] and the test set is uniformly sampled from (0.5, 1].\n\n\nA2 Proof of Theoretical Results\n\n\nA2.1 Corollary 1\n\nProof. We first study the convergence part, where we consider a fixed hypothesis. We first expand For |L( \u03b8 RSC (S); S) \u2212 L(\u03b8 RSC (S); S)|, the strategy is relatively standard. We first consider the convergence of a fixed hypothesis \u03b8 RSC , then over n i.i.d samples, the empirical risk ( L(\u03b8 RSC )) will be bounded within [0, 1] with the expectation L(\u03b8 RSC ).\n\nBefore we consider the uniform convergence step, we first put the two terms together and apply the Hoeffding's inequality. When the random variable is with expectation L(\u03b8 RSC ) and bound [0, 1 + 2\u03be(p)], we have: Rearranging these terms following standard tricks will lead to the conclusion. and h( \u03b8 RSC (t), z) \u2212 h( \u03b8 RSC (t + 1), z) = 1 \u03b3 t (p) \u2202h( \u03b8 RSC (t),z) \u2202 \u03b8 RSC (t)g \u03b7 = 1 \u03b3 t (p) ||g|| 2 2 \u03b7\n\nWe write these terms back and get \u0393 ( \u03b8 RSC (t + 1)) = ( 1 \u03b3 t (p) \u2212 1)||g|| 2 2 \u03b7 + \u0393 ( \u03b8 RSC (t))\n\nWe can simply drop the absolute value sign because all these terms are greater than zero. Finally, we rearrange these terms and prove the conclusion.\n\n-\nPACS [13]: seven classes over four domains (Artpaint, Cartoon, Sketches, and Photo). The experimental protocol is to train a model on three domains and test on the remaining domain. -VLCS [27]: five classes over four domains. The domains are defined by four image origins, i.e., images were taken from the PASCAL VOC 2007, LabelMe, Caltech and Sun datasets. -Office-Home [28]: 65 object categories over 4 domains (Art, Clipart, Product, and Real-World).\n\n\n|L( \u03b8 RSC (S); S) \u2212 L(\u03b8 RSC (S); D)| = |L( \u03b8 RSC (S); S) \u2212 L( \u03b8 RSC (S); D) + L( \u03b8 RSC (S); D) \u2212 L(\u03b8 RSC (S); D)| \u2264 |L( \u03b8 RSC (S); S) \u2212 L(\u03b8 RSC (S); D)| + |L(\u03b8 RSC (S); D) \u2212 L(\u03b8 RSC (S); D)| We first consider the term |L(\u03b8 RSC (S); S) \u2212 L(\u03b8 RSC (S); D)|, where we can expand |L(\u03b8 RSC (S); S) \u2212 L(\u03b8 RSC (S); D)| \u2264 2|L(\u03b8 RSC (S); S) \u2212 L(\u03b8 RSC (S); O)| because of Assumption A4. Also, because of Assumption A4, if samples in S are perturbed versions of samples in O, then samples in O can also be seen as perturbed versions of samples in S, thus, Condition 6 can be directly re-written into: |L(\u03b8 RSC (S); S) \u2212 L(\u03b8 RSC (S); O)| \u2264 \u03be(p), which directly leads us to the fact that |L(\u03b8 RSC (S); S) \u2212 L(\u03b8 RSC (S); D)| has the expectation 0 (A4) and bounded by [0, \u03be(p)].\n\nP\n(| L(\u03b8 RSC ; S) \u2212 L(\u03b8 RSC ; D)| \u2265 ) we consider the uniform convergence case, where we have: P( sup \u03b8RSC\u2208\u0398RSC | L(\u03b8 RSC ; S) \u2212 L(\u03b8 RSC ; D)| \u2265 ) \u2264 2|\u0398 RSC | exp(\u2212 2n 2 (2\u03be(p) + 1) 2 )\n\n\nAlgorithm 1: RSC Update AlgorithmInput: data set X, Y , percentage of representations to discard p, other configurations such as learning rate \u03b7, maximum number of epoches T , etc;Output: Classifier f (\u00b7; \u03b8); \nrandom initialize the model \u03b80; \nwhile t \u2264 T do \nfor every sample (or batch) x, y do \ncalculate z through forward pass; \ncalculate gz with Equation 1; \ncalculate qp and m as in Equation 2; \ngeneratez with Equation 3; \ncalculate gradientg \u03b8 with Equation 4 and Equation 5\n\n\nFeature Drop Strategies backbone artpaint cartoon sketch photo Avg \u2191Baseline [4] \nResNet18 78.96 \n73.93 70.59 96.28 79.94 \nRandom \nResNet18 79.32 \n75.27 74.06 95.54 81.05 \nTop-Activation \nResNet18 80.31 \n76.05 76.13 95.72 82.03 \nTop-Gradient \nResNet18 81.23 77.23 77.56 95.61 82.91 \n\nTable 1. Ablation study of Spatial-wise RSC on Feature Dropping Strategies. Feature \nDropping Percentage 50.0% and Batch Percentage 50.0%. \n\nFeature Dropping Percentage backbone artpaint cartoon sketch photo Avg\u2191 \n\n66.7% \nResNet18 80.11 \n76.35 76.24 95.16 81.97 \n50.0% \nResNet18 81.23 \n77.23 77.56 95.61 82.91 \n33.3% \nResNet18 82.87 78.23 78.89 95.82 83.95 \n25.0% \nResNet18 81.63 \n78.06 78.12 96.06 83.46 \n20.0% \nResNet18 81.22 \n77.43 77.83 96.25 83.18 \n13.7% \nResNet18 80.71 \n77.18 77.12 96.36 82.84 \n\n\n\nTable 2 .\n2Ablationstudy of Spatial-wise RSC on Feature Dropping Percentage. We \nused \"Top-Gradient\" and fixed the Batch Percentage (50.0%) here. \n\nBatch Percentage backbone artpaint cartoon sketch photo Avg\u2191 \n\n50.0% \nResNet18 82.87 \n78.23 78.89 95.82 83.95 \n33.3% \nResNet18 82.32 \n78.75 79.56 96.05 84.17 \n25.0% \nResNet18 81.85 \n78.32 78.75 96.21 83.78 \n\n\n\nTable 3 .\n3Ablation study of Spatial-wise RSC on Batch Percentage. We used \"Top-Gradient\" and fixed Feature Dropping Percentage (33.3%).Method \nbackbone artpaint cartoon sketch photo Avg\u2191 \n\nSpatial \nResNet18 82.32 \n78.75 79.56 96.05 84.17 \nSpatial+Channel ResNet18 83.43 80.31 80.85 95.99 85.15 \n\n\n\nTable 3 :\n3\"Top-Gradient\", Feature Dropping \nPercentage(33.3%) and Batch Percentage(33.3%). \n\nMethod \nbackbone artpaint cartoon sketch photo Avg\u2191 \n\nBaseline [4] \nResNet18 78.96 \n73.93 70.59 96.28 79.94 \nCutout[6] \nResNet18 79.63 \n75.35 71.56 95.87 80.60 \nDropBlock[9] \nResNet18 80.25 \n77.54 76.42 95.64 82.46 \nAdversarialDropout[21] ResNet18 82.35 \n78.23 75.86 96.12 83.07 \nRandom(S+C) \nResNet18 79.55 \n75.56 74.39 95.36 81.22 \nTop-Activation(S+C) \nResNet18 81.03 \n77.86 76.65 96.11 82.91 \nRSC: Top-Gradient(S+C) ResNet18 83.43 80.31 80.85 95.99 85.15 \n\n\n\nTable 6 .\n6DG results on PACS[13] (Best in bold).VLCS \nbackbone Caltech Labelme Pascal Sun Avg \u2191 \n\nBaseline[4] AlexNet 96.25 \n59.72 70.58 64.51 72.76 \nEpi-FCR[14] AlexNet 94.10 \n64.30 67.10 65.90 72.90 \nJiGen[4] \nAlexNet 96.93 \n60.90 70.62 64.30 73.19 \nMASF[7] \nAlexNet 94.78 \n64.90 69.14 67.64 74.11 \nRSC(ours) AlexNet 97.61 \n61.86 73.93 68.32 75.43 \n\n\n\n\nOffice-Home backbone Art Clipart Product Real Avg \u2191Baseline[4] ResNet18 52.15 45.86 \n70.86 73.15 60.51 \nJiGen[4] ResNet18 53.04 47.51 \n71.47 72.79 61.20 \nRSC(ours) ResNet18 58.42 47.90 71.63 74.54 63.12 \n\n\n\nTable 8 .\n8DG results on Office-Home[28] (Best in bold). -Sketch backbone Top-1 Acc \u2191 Top-5 Acc \u2191ImageNetBaseline[31] \nAlexNet \n12.04 \n24.80 \nHex[31] \nAlexNet \n14.69 \n28.98 \nPAR [30] \nAlexNet \n15.01 \n29.57 \nRSC(ours) \nAlexNet \n16.12 \n30.78 \n\n\n\n\nImageNet backbone Top-1 Acc \u2191 Top-5 Acc \u2191 #Param. \u2193Baseline ResNet50 \n76.13 \n92.86 \n25.6M \nRSC(ours) ResNet50 \n77.18 \n93.53 \n25.6M \nBaseline ResNet101 \n77.37 \n93.55 \n44.5M \nRSC(ours) ResNet101 \n78.23 \n94.16 \n44.5M \nBaseline ResNet152 \n78.31 \n94.05 \n60.2M \nRSC(ours) ResNet152 \n78.89 \n94.43 \n60.2M \n\n\nhttps://github.com/pytorch/examples\nAppendix A1 AssumptionsA1: \u0398 is finite; l(\u00b7, \u00b7) is zero-one loss for binary classification.The assumption leads to classical discussions on the i.i.d setting in multiple textbooks (e.g.,[16]). However, modern machine learning concerns more than the i.i.d setting, therefore, we need to quantify the variations between train and test distributions. Analysis of domain adaptation is discussed[2], but still relies on the explicit knowledge of the target distribution to quantify the bound with an alignment of the distributions. The following discussion is devoted to the scenario when we do not have the target distribution to align.Since we are interested in the \u03b8 instead of the \u03b8 (D), we first assume \u0398 is large enough and we can find a global optimum hypothesis that is applicable to any distribution, or in formal words:This assumption can be met when the conditional distribution P(Y(D)|Z(D)) is the same for any D.e.g., The true concept of \"cat\" is the same for any collection of images.The challenge of cross-domain evaluation comes in when there exists multiple optimal hypothesis that are equivalently good for one distribution, but not every optimal hypothesis can be applied to other distributions. e.g., For the distribution of picture book, \"cats have chubby faces\" can predict the true concept of \"cat\". A model only needs to learn one of these signals to reduce training error, although the other signal also exists in the data.The follow-up discussion aims to show that RSC can force the model to learn multiple signals, so that it helps in cross-domain generalization.Further, Assumption A2 can be interpreted as there is at least some features z that appear in every distributions we consider. We use i to index this set of features. Assumption A2 also suggests that z i is i.i.d. (otherwise there will not exist \u03b8 ) across all the distributions of interest (but z is not i.i.d. because z \u2212i , where \u2212i denotes the indices other than i, can be sampled from arbitrary distributions).e.g., z is the image; z i is the ingredients of the true concept of a \"cat\", such as ears, paws, and furs; z \u2212i is other features such as \"sitting by the window\".We use O to specify the distribution that has values on the i th , but 0s elsewhere. We introduce the next assumption:\nMetareg: Towards domain generalization using meta-regularization. Y Balaji, S Sankaranarayanan, R Chellappa, Advances in Neural Information Processing Systems. Balaji, Y., Sankaranarayanan, S., Chellappa, R.: Metareg: Towards domain gener- alization using meta-regularization. In: Advances in Neural Information Processing Systems. pp. 998-1008 (2018)\n\nA theory of learning from different domains. S Ben-David, J Blitzer, K Crammer, A Kulesza, F Pereira, J W Vaughan, Machine learning. 791Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., Vaughan, J.W.: A theory of learning from different domains. Machine learning 79(1), 151-175 (2010)\n\nRecnorm: Simultaneous normalisation and classification applied to speech recognition. J S Bridle, S J Cox, Advances in Neural Information Processing Systems. Bridle, J.S., Cox, S.J.: Recnorm: Simultaneous normalisation and classification applied to speech recognition. In: Advances in Neural Information Processing Sys- tems. pp. 234-240 (1991)\n\nDomain generalization by solving jigsaw puzzles. F M Carlucci, A D&apos;innocente, S Bucci, B Caputo, T Tommasi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionCarlucci, F.M., D'Innocente, A., Bucci, S., Caputo, B., Tommasi, T.: Domain generalization by solving jigsaw puzzles. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2229-2238 (2019)\n\nG Csurka, arXiv:1702.05374Domain adaptation for visual applications: A comprehensive survey. arXiv preprintCsurka, G.: Domain adaptation for visual applications: A comprehensive survey. arXiv preprint arXiv:1702.05374 (2017)\n\nT Devries, G W Taylor, arXiv:1708.04552Improved regularization of convolutional neural networks with cutout. arXiv preprintDeVries, T., Taylor, G.W.: Improved regularization of convolutional neural net- works with cutout. arXiv preprint arXiv:1708.04552 (2017)\n\nDomain generalization via model-agnostic learning of semantic features. Q Dou, D C Castro, K Kamnitsas, B Glocker, arXiv:1910.13580arXiv preprintDou, Q., Castro, D.C., Kamnitsas, K., Glocker, B.: Domain generalization via model-agnostic learning of semantic features. arXiv preprint arXiv:1910.13580 (2019)\n\nX Gastaldi, arXiv:1705.07485Shake-shake regularization. arXiv preprintGastaldi, X.: Shake-shake regularization. arXiv preprint arXiv:1705.07485 (2017)\n\nDropblock: A regularization method for convolutional networks. G Ghiasi, T Y Lin, Q V Le, Advances in Neural Information Processing Systems. Ghiasi, G., Lin, T.Y., Le, Q.V.: Dropblock: A regularization method for convo- lutional networks. In: Advances in Neural Information Processing Systems. pp. 10727-10737 (2018)\n\nDomain generalization for object recognition with multi-task autoencoders. M Ghifary, W Bastiaan Kleijn, M Zhang, D Balduzzi, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionGhifary, M., Bastiaan Kleijn, W., Zhang, M., Balduzzi, D.: Domain generalization for object recognition with multi-task autoencoders. In: Proceedings of the IEEE international conference on computer vision. pp. 2551-2559 (2015)\n\nG Larsson, M Maire, G Shakhnarovich, arXiv:1605.07648Fractalnet: Ultra-deep neural networks without residuals. arXiv preprintLarsson, G., Maire, M., Shakhnarovich, G.: Fractalnet: Ultra-deep neural networks without residuals. arXiv preprint arXiv:1605.07648 (2016)\n\nDrop to adapt: Learning discriminative features for unsupervised domain adaptation. S Lee, D Kim, N Kim, S G Jeong, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionLee, S., Kim, D., Kim, N., Jeong, S.G.: Drop to adapt: Learning discriminative features for unsupervised domain adaptation. In: Proceedings of the IEEE Inter- national Conference on Computer Vision. pp. 91-100 (2019)\n\nDeeper, broader and artier domain generalization. D Li, Y Yang, Y Z Song, T M Hospedales, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionLi, D., Yang, Y., Song, Y.Z., Hospedales, T.M.: Deeper, broader and artier domain generalization. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 5542-5550 (2017)\n\nD Li, J Zhang, Y Yang, C Liu, Y Z Song, T M Hospedales, arXiv:1902.00113Episodic training for domain generalization. arXiv preprintLi, D., Zhang, J., Yang, Y., Liu, C., Song, Y.Z., Hospedales, T.M.: Episodic training for domain generalization. arXiv preprint arXiv:1902.00113 (2019)\n\nDomain generalization with adversarial feature learning. H Li, S Jialin Pan, S Wang, A C Kot, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLi, H., Jialin Pan, S., Wang, S., Kot, A.C.: Domain generalization with adversarial feature learning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 5400-5409 (2018)\n\nMachine learning. T M Mitchell, McGraw Hill45Burr Ridge, ILMitchell, T.M., et al.: Machine learning. 1997. Burr Ridge, IL: McGraw Hill 45(37), 870-877 (1997)\n\nCurriculum dropout. P Morerio, J Cavazza, R Volpi, R Vidal, V Murino, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionMorerio, P., Cavazza, J., Volpi, R., Vidal, R., Murino, V.: Curriculum dropout. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 3544-3552 (2017)\n\nDomain generalization via invariant feature representation. K Muandet, D Balduzzi, B Sch\u00f6lkopf, International Conference on Machine Learning. Muandet, K., Balduzzi, D., Sch\u00f6lkopf, B.: Domain generalization via invariant feature representation. In: International Conference on Machine Learning. pp. 10- 18 (2013)\n\nSimplifying neural networks by soft weight-sharing. S J Nowlan, G E Hinton, Neural computation. 44Nowlan, S.J., Hinton, G.E.: Simplifying neural networks by soft weight-sharing. Neural computation 4(4), 473-493 (1992)\n\nS Park, N Kwak, Analysis on the dropout effect in convolutional neural networks. In: Asian conference on computer vision. SpringerPark, S., Kwak, N.: Analysis on the dropout effect in convolutional neural networks. In: Asian conference on computer vision. pp. 189-204. Springer (2016)\n\nAdversarial dropout for supervised and semi-supervised learning. S Park, J Park, S J Shin, I C Moon, Thirty-Second AAAI Conference on Artificial Intelligence. Park, S., Park, J., Shin, S.J., Moon, I.C.: Adversarial dropout for supervised and semi-supervised learning. In: Thirty-Second AAAI Conference on Artificial Intel- ligence (2018)\n\nImageNet Large Scale Visual Recognition Challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, A C Berg, L Fei-Fei, 10.1007/s11263-015-0816-yInternational Journal of Computer Vision (IJCV). 1153Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) 115(3), 211-252 (2015). https://doi.org/10.1007/s11263-015-0816-y\n\nGeneralizing across domains via cross-gradient training. S Shankar, V Piratla, S Chakrabarti, S Chaudhuri, P Jyothi, S Sarawagi, arXiv:1804.10745arXiv preprintShankar, S., Piratla, V., Chakrabarti, S., Chaudhuri, S., Jyothi, P., Sarawagi, S.: Generalizing across domains via cross-gradient training. arXiv preprint arXiv:1804.10745 (2018)\n\nHide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization. K K Singh, Y J Lee, 2017 IEEE international conference on computer vision (ICCV). IEEESingh, K.K., Lee, Y.J.: Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization. In: 2017 IEEE international con- ference on computer vision (ICCV). pp. 3544-3553. IEEE (2017)\n\nDropout: a simple way to prevent neural networks from overfitting. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, The journal of machine learning research. 151Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research 15(1), 1929-1958 (2014)\n\nEfficient object localization using convolutional networks. J Tompson, R Goroshin, A Jain, Y Lecun, C Bregler, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionTompson, J., Goroshin, R., Jain, A., LeCun, Y., Bregler, C.: Efficient object local- ization using convolutional networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 648-656 (2015)\n\nUnbiased look at dataset bias. A Torralba, A A Efros, CVPR. 17Torralba, A., Efros, A.A., et al.: Unbiased look at dataset bias. In: CVPR. vol. 1, p. 7. Citeseer (2011)\n\nDeep hashing network for unsupervised domain adaptation. H Venkateswara, J Eusebio, S Chakraborty, S Panchanathan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionVenkateswara, H., Eusebio, J., Chakraborty, S., Panchanathan, S.: Deep hashing network for unsupervised domain adaptation. In: Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition. pp. 5018-5027 (2017)\n\nGeneralizing to unseen domains via adversarial data augmentation. R Volpi, H Namkoong, O Sener, J C Duchi, V Murino, S Savarese, Advances in Neural Information Processing Systems. Volpi, R., Namkoong, H., Sener, O., Duchi, J.C., Murino, V., Savarese, S.: Gen- eralizing to unseen domains via adversarial data augmentation. In: Advances in Neural Information Processing Systems. pp. 5334-5344 (2018)\n\nLearning robust global representations by penalizing local predictive power. H Wang, S Ge, E P Xing, Z C Lipton, Advances in Neural Information Processing Systems. Wang, H., Ge, S., Xing, E.P., Lipton, Z.C.: Learning robust global representations by penalizing local predictive power. In: Advances in Neural Information Process- ing Systems (NeurIPS 2019) (2019)\n\nLearning robust representations by projecting superficial statistics out. H Wang, Z He, Z C Lipton, E P Xing, International Conference on Learning Representations. Wang, H., He, Z., Lipton, Z.C., Xing, E.P.: Learning robust representations by projecting superficial statistics out. In: International Conference on Learning Rep- resentations (2019)\n\nDeep visual domain adaptation: A survey. M Wang, W Deng, Neurocomputing. 312Wang, M., Deng, W.: Deep visual domain adaptation: A survey. Neurocomputing 312, 135-153 (2018)\n", "annotations": {"author": "[{\"end\":137,\"start\":57},{\"end\":217,\"start\":138},{\"end\":296,\"start\":218},{\"end\":364,\"start\":297}]", "publisher": null, "author_last_name": "[{\"end\":67,\"start\":62},{\"end\":149,\"start\":145},{\"end\":229,\"start\":225},{\"end\":307,\"start\":302}]", "author_first_name": "[{\"end\":61,\"start\":57},{\"end\":144,\"start\":138},{\"end\":222,\"start\":218},{\"end\":224,\"start\":223},{\"end\":301,\"start\":297}]", "author_affiliation": "[{\"end\":136,\"start\":82},{\"end\":216,\"start\":162},{\"end\":295,\"start\":241},{\"end\":363,\"start\":309}]", "title": "[{\"end\":54,\"start\":1},{\"end\":418,\"start\":365}]", "venue": null, "abstract": "[{\"end\":1577,\"start\":459}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3268,\"start\":3265},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3270,\"start\":3268},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3499,\"start\":3496},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3502,\"start\":3499},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3700,\"start\":3696},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4124,\"start\":4120},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5159,\"start\":5155},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5991,\"start\":5987},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6064,\"start\":6060},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6147,\"start\":6143},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6392,\"start\":6388},{\"end\":6407,\"start\":6394},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6580,\"start\":6577},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6888,\"start\":6884},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7014,\"start\":7010},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7532,\"start\":7528},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7716,\"start\":7712},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7780,\"start\":7777},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8046,\"start\":8043},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8518,\"start\":8514},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8570,\"start\":8567},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8623,\"start\":8619},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8857,\"start\":8854},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8870,\"start\":8866},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8929,\"start\":8925},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8985,\"start\":8982},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9067,\"start\":9063},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9148,\"start\":9144},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9267,\"start\":9263},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9378,\"start\":9374},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9413,\"start\":9409},{\"end\":17213,\"start\":17203},{\"end\":19983,\"start\":19976},{\"end\":21043,\"start\":21034},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21431,\"start\":21427},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22181,\"start\":22177},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":22264,\"start\":22260},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22419,\"start\":22415},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22484,\"start\":22481},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22682,\"start\":22678},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24529,\"start\":24525},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25121,\"start\":25118},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25239,\"start\":25236},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25485,\"start\":25481},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25488,\"start\":25485},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25822,\"start\":25818},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26515,\"start\":26512},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26609,\"start\":26605},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":26619,\"start\":26615},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26630,\"start\":26627},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26646,\"start\":26643},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27118,\"start\":27115},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28104,\"start\":28100},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":35404,\"start\":35400},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":35974,\"start\":35970}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31931,\"start\":31475},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32696,\"start\":31932},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32883,\"start\":32697},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33366,\"start\":32884},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34156,\"start\":33367},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34514,\"start\":34157},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34813,\"start\":34515},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":35369,\"start\":34814},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":35724,\"start\":35370},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":35932,\"start\":35725},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":36176,\"start\":35933},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":36478,\"start\":36177}]", "paragraph": "[{\"end\":2384,\"start\":1593},{\"end\":3018,\"start\":2386},{\"end\":3503,\"start\":3020},{\"end\":3865,\"start\":3505},{\"end\":4311,\"start\":3867},{\"end\":4939,\"start\":4313},{\"end\":5361,\"start\":4941},{\"end\":5678,\"start\":5378},{\"end\":6148,\"start\":5680},{\"end\":6581,\"start\":6150},{\"end\":7173,\"start\":6583},{\"end\":7848,\"start\":7175},{\"end\":8346,\"start\":7850},{\"end\":9483,\"start\":8348},{\"end\":9794,\"start\":9485},{\"end\":10628,\"start\":9805},{\"end\":10799,\"start\":10659},{\"end\":11088,\"start\":10840},{\"end\":11235,\"start\":11090},{\"end\":11344,\"start\":11237},{\"end\":11579,\"start\":11378},{\"end\":11898,\"start\":11623},{\"end\":11978,\"start\":11908},{\"end\":12037,\"start\":12011},{\"end\":12133,\"start\":12060},{\"end\":12387,\"start\":12135},{\"end\":12960,\"start\":12412},{\"end\":13384,\"start\":12962},{\"end\":13674,\"start\":13386},{\"end\":13818,\"start\":13676},{\"end\":14343,\"start\":13820},{\"end\":14857,\"start\":14345},{\"end\":14956,\"start\":14859},{\"end\":15165,\"start\":14958},{\"end\":15309,\"start\":15167},{\"end\":15684,\"start\":15311},{\"end\":16137,\"start\":15748},{\"end\":16183,\"start\":16139},{\"end\":16455,\"start\":16185},{\"end\":16678,\"start\":16457},{\"end\":16758,\"start\":16718},{\"end\":16876,\"start\":16807},{\"end\":17056,\"start\":16878},{\"end\":17067,\"start\":17058},{\"end\":17343,\"start\":17175},{\"end\":17844,\"start\":17477},{\"end\":18091,\"start\":17846},{\"end\":18478,\"start\":18093},{\"end\":18929,\"start\":18480},{\"end\":19236,\"start\":18931},{\"end\":19338,\"start\":19238},{\"end\":20398,\"start\":19381},{\"end\":20903,\"start\":20400},{\"end\":21362,\"start\":20905},{\"end\":21927,\"start\":21364},{\"end\":22023,\"start\":21929},{\"end\":22306,\"start\":22050},{\"end\":22518,\"start\":22325},{\"end\":23025,\"start\":22520},{\"end\":23372,\"start\":23027},{\"end\":23999,\"start\":23374},{\"end\":24099,\"start\":24001},{\"end\":24442,\"start\":24101},{\"end\":24545,\"start\":24451},{\"end\":25823,\"start\":24547},{\"end\":26939,\"start\":25851},{\"end\":27673,\"start\":26941},{\"end\":28027,\"start\":27688},{\"end\":28578,\"start\":28029},{\"end\":29064,\"start\":28580},{\"end\":29930,\"start\":29079},{\"end\":30401,\"start\":29932},{\"end\":30817,\"start\":30456},{\"end\":31222,\"start\":30819},{\"end\":31323,\"start\":31224},{\"end\":31474,\"start\":31325}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10839,\"start\":10800},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11377,\"start\":11345},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11622,\"start\":11580},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11907,\"start\":11899},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12010,\"start\":11979},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12059,\"start\":12038},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15747,\"start\":15685},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16717,\"start\":16679},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16806,\"start\":16759},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17174,\"start\":17068},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17476,\"start\":17344}]", "table_ref": "[{\"end\":22561,\"start\":22552},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23082,\"start\":23075},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":23404,\"start\":23395},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":23498,\"start\":23491},{\"end\":23536,\"start\":23529},{\"end\":23651,\"start\":23644},{\"end\":24148,\"start\":24140},{\"end\":24504,\"start\":24497},{\"end\":24601,\"start\":24593},{\"end\":25400,\"start\":25393},{\"end\":25786,\"start\":25779},{\"end\":26526,\"start\":26520},{\"end\":27062,\"start\":27056},{\"end\":27904,\"start\":27896},{\"end\":28603,\"start\":28595}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1591,\"start\":1579},{\"attributes\":{\"n\":\"2\"},\"end\":5376,\"start\":5364},{\"attributes\":{\"n\":\"3\"},\"end\":9803,\"start\":9797},{\"attributes\":{\"n\":\"3.1\"},\"end\":10657,\"start\":10631},{\"attributes\":{\"n\":\"3.2\"},\"end\":12410,\"start\":12390},{\"attributes\":{\"n\":\"3.3\"},\"end\":19379,\"start\":19341},{\"attributes\":{\"n\":\"4\"},\"end\":22037,\"start\":22026},{\"attributes\":{\"n\":\"4.1\"},\"end\":22048,\"start\":22040},{\"attributes\":{\"n\":\"4.2\"},\"end\":22323,\"start\":22309},{\"end\":24449,\"start\":24445},{\"attributes\":{\"n\":\"4.3\"},\"end\":25849,\"start\":25826},{\"attributes\":{\"n\":\"5\"},\"end\":27686,\"start\":27676},{\"attributes\":{\"n\":\"6\"},\"end\":29077,\"start\":29067},{\"end\":30435,\"start\":30404},{\"end\":30454,\"start\":30438},{\"end\":31477,\"start\":31476},{\"end\":32699,\"start\":32698},{\"end\":34167,\"start\":34158},{\"end\":34525,\"start\":34516},{\"end\":34824,\"start\":34815},{\"end\":35380,\"start\":35371},{\"end\":35943,\"start\":35934}]", "table": "[{\"end\":33366,\"start\":33066},{\"end\":34156,\"start\":33437},{\"end\":34514,\"start\":34177},{\"end\":34813,\"start\":34652},{\"end\":35369,\"start\":34826},{\"end\":35724,\"start\":35420},{\"end\":35932,\"start\":35778},{\"end\":36176,\"start\":36031},{\"end\":36478,\"start\":36230}]", "figure_caption": "[{\"end\":31931,\"start\":31478},{\"end\":32696,\"start\":31934},{\"end\":32883,\"start\":32700},{\"end\":33066,\"start\":32886},{\"end\":33437,\"start\":33369},{\"end\":34177,\"start\":34169},{\"end\":34652,\"start\":34527},{\"end\":35420,\"start\":35382},{\"end\":35778,\"start\":35727},{\"end\":36031,\"start\":35945},{\"end\":36230,\"start\":36179}]", "figure_ref": "[{\"end\":2623,\"start\":2617},{\"end\":4938,\"start\":4930},{\"end\":17621,\"start\":17615},{\"end\":18090,\"start\":18082}]", "bib_author_first_name": "[{\"end\":38863,\"start\":38862},{\"end\":38873,\"start\":38872},{\"end\":38893,\"start\":38892},{\"end\":39195,\"start\":39194},{\"end\":39208,\"start\":39207},{\"end\":39219,\"start\":39218},{\"end\":39230,\"start\":39229},{\"end\":39241,\"start\":39240},{\"end\":39252,\"start\":39251},{\"end\":39254,\"start\":39253},{\"end\":39539,\"start\":39538},{\"end\":39541,\"start\":39540},{\"end\":39551,\"start\":39550},{\"end\":39553,\"start\":39552},{\"end\":39848,\"start\":39847},{\"end\":39850,\"start\":39849},{\"end\":39862,\"start\":39861},{\"end\":39882,\"start\":39881},{\"end\":39891,\"start\":39890},{\"end\":39901,\"start\":39900},{\"end\":40276,\"start\":40275},{\"end\":40502,\"start\":40501},{\"end\":40513,\"start\":40512},{\"end\":40515,\"start\":40514},{\"end\":40836,\"start\":40835},{\"end\":40843,\"start\":40842},{\"end\":40845,\"start\":40844},{\"end\":40855,\"start\":40854},{\"end\":40868,\"start\":40867},{\"end\":41072,\"start\":41071},{\"end\":41287,\"start\":41286},{\"end\":41297,\"start\":41296},{\"end\":41299,\"start\":41298},{\"end\":41306,\"start\":41305},{\"end\":41308,\"start\":41307},{\"end\":41617,\"start\":41616},{\"end\":41628,\"start\":41627},{\"end\":41647,\"start\":41646},{\"end\":41656,\"start\":41655},{\"end\":42018,\"start\":42017},{\"end\":42029,\"start\":42028},{\"end\":42038,\"start\":42037},{\"end\":42368,\"start\":42367},{\"end\":42375,\"start\":42374},{\"end\":42382,\"start\":42381},{\"end\":42389,\"start\":42388},{\"end\":42391,\"start\":42390},{\"end\":42789,\"start\":42788},{\"end\":42795,\"start\":42794},{\"end\":42803,\"start\":42802},{\"end\":42805,\"start\":42804},{\"end\":42813,\"start\":42812},{\"end\":42815,\"start\":42814},{\"end\":43143,\"start\":43142},{\"end\":43149,\"start\":43148},{\"end\":43158,\"start\":43157},{\"end\":43166,\"start\":43165},{\"end\":43173,\"start\":43172},{\"end\":43175,\"start\":43174},{\"end\":43183,\"start\":43182},{\"end\":43185,\"start\":43184},{\"end\":43484,\"start\":43483},{\"end\":43490,\"start\":43489},{\"end\":43504,\"start\":43503},{\"end\":43512,\"start\":43511},{\"end\":43514,\"start\":43513},{\"end\":43887,\"start\":43886},{\"end\":43889,\"start\":43888},{\"end\":44048,\"start\":44047},{\"end\":44059,\"start\":44058},{\"end\":44070,\"start\":44069},{\"end\":44079,\"start\":44078},{\"end\":44088,\"start\":44087},{\"end\":44454,\"start\":44453},{\"end\":44465,\"start\":44464},{\"end\":44477,\"start\":44476},{\"end\":44759,\"start\":44758},{\"end\":44761,\"start\":44760},{\"end\":44771,\"start\":44770},{\"end\":44773,\"start\":44772},{\"end\":44926,\"start\":44925},{\"end\":44934,\"start\":44933},{\"end\":45277,\"start\":45276},{\"end\":45285,\"start\":45284},{\"end\":45293,\"start\":45292},{\"end\":45295,\"start\":45294},{\"end\":45303,\"start\":45302},{\"end\":45305,\"start\":45304},{\"end\":45602,\"start\":45601},{\"end\":45617,\"start\":45616},{\"end\":45625,\"start\":45624},{\"end\":45631,\"start\":45630},{\"end\":45641,\"start\":45640},{\"end\":45653,\"start\":45652},{\"end\":45659,\"start\":45658},{\"end\":45668,\"start\":45667},{\"end\":45680,\"start\":45679},{\"end\":45690,\"start\":45689},{\"end\":45703,\"start\":45702},{\"end\":45705,\"start\":45704},{\"end\":45713,\"start\":45712},{\"end\":46171,\"start\":46170},{\"end\":46182,\"start\":46181},{\"end\":46193,\"start\":46192},{\"end\":46208,\"start\":46207},{\"end\":46221,\"start\":46220},{\"end\":46231,\"start\":46230},{\"end\":46558,\"start\":46557},{\"end\":46560,\"start\":46559},{\"end\":46569,\"start\":46568},{\"end\":46571,\"start\":46570},{\"end\":46935,\"start\":46934},{\"end\":46949,\"start\":46948},{\"end\":46959,\"start\":46958},{\"end\":46973,\"start\":46972},{\"end\":46986,\"start\":46985},{\"end\":47319,\"start\":47318},{\"end\":47330,\"start\":47329},{\"end\":47342,\"start\":47341},{\"end\":47350,\"start\":47349},{\"end\":47359,\"start\":47358},{\"end\":47768,\"start\":47767},{\"end\":47780,\"start\":47779},{\"end\":47782,\"start\":47781},{\"end\":47963,\"start\":47962},{\"end\":47979,\"start\":47978},{\"end\":47990,\"start\":47989},{\"end\":48005,\"start\":48004},{\"end\":48458,\"start\":48457},{\"end\":48467,\"start\":48466},{\"end\":48479,\"start\":48478},{\"end\":48488,\"start\":48487},{\"end\":48490,\"start\":48489},{\"end\":48499,\"start\":48498},{\"end\":48509,\"start\":48508},{\"end\":48869,\"start\":48868},{\"end\":48877,\"start\":48876},{\"end\":48883,\"start\":48882},{\"end\":48885,\"start\":48884},{\"end\":48893,\"start\":48892},{\"end\":48895,\"start\":48894},{\"end\":49230,\"start\":49229},{\"end\":49238,\"start\":49237},{\"end\":49244,\"start\":49243},{\"end\":49246,\"start\":49245},{\"end\":49256,\"start\":49255},{\"end\":49258,\"start\":49257},{\"end\":49546,\"start\":49545},{\"end\":49554,\"start\":49553}]", "bib_author_last_name": "[{\"end\":38870,\"start\":38864},{\"end\":38890,\"start\":38874},{\"end\":38903,\"start\":38894},{\"end\":39205,\"start\":39196},{\"end\":39216,\"start\":39209},{\"end\":39227,\"start\":39220},{\"end\":39238,\"start\":39231},{\"end\":39249,\"start\":39242},{\"end\":39262,\"start\":39255},{\"end\":39548,\"start\":39542},{\"end\":39557,\"start\":39554},{\"end\":39859,\"start\":39851},{\"end\":39879,\"start\":39863},{\"end\":39888,\"start\":39883},{\"end\":39898,\"start\":39892},{\"end\":39909,\"start\":39902},{\"end\":40283,\"start\":40277},{\"end\":40510,\"start\":40503},{\"end\":40522,\"start\":40516},{\"end\":40840,\"start\":40837},{\"end\":40852,\"start\":40846},{\"end\":40865,\"start\":40856},{\"end\":40876,\"start\":40869},{\"end\":41081,\"start\":41073},{\"end\":41294,\"start\":41288},{\"end\":41303,\"start\":41300},{\"end\":41311,\"start\":41309},{\"end\":41625,\"start\":41618},{\"end\":41644,\"start\":41629},{\"end\":41653,\"start\":41648},{\"end\":41665,\"start\":41657},{\"end\":42026,\"start\":42019},{\"end\":42035,\"start\":42030},{\"end\":42052,\"start\":42039},{\"end\":42372,\"start\":42369},{\"end\":42379,\"start\":42376},{\"end\":42386,\"start\":42383},{\"end\":42397,\"start\":42392},{\"end\":42792,\"start\":42790},{\"end\":42800,\"start\":42796},{\"end\":42810,\"start\":42806},{\"end\":42826,\"start\":42816},{\"end\":43146,\"start\":43144},{\"end\":43155,\"start\":43150},{\"end\":43163,\"start\":43159},{\"end\":43170,\"start\":43167},{\"end\":43180,\"start\":43176},{\"end\":43196,\"start\":43186},{\"end\":43487,\"start\":43485},{\"end\":43501,\"start\":43491},{\"end\":43509,\"start\":43505},{\"end\":43518,\"start\":43515},{\"end\":43898,\"start\":43890},{\"end\":44056,\"start\":44049},{\"end\":44067,\"start\":44060},{\"end\":44076,\"start\":44071},{\"end\":44085,\"start\":44080},{\"end\":44095,\"start\":44089},{\"end\":44462,\"start\":44455},{\"end\":44474,\"start\":44466},{\"end\":44487,\"start\":44478},{\"end\":44768,\"start\":44762},{\"end\":44780,\"start\":44774},{\"end\":44931,\"start\":44927},{\"end\":44939,\"start\":44935},{\"end\":45282,\"start\":45278},{\"end\":45290,\"start\":45286},{\"end\":45300,\"start\":45296},{\"end\":45310,\"start\":45306},{\"end\":45614,\"start\":45603},{\"end\":45622,\"start\":45618},{\"end\":45628,\"start\":45626},{\"end\":45638,\"start\":45632},{\"end\":45650,\"start\":45642},{\"end\":45656,\"start\":45654},{\"end\":45665,\"start\":45660},{\"end\":45677,\"start\":45669},{\"end\":45687,\"start\":45681},{\"end\":45700,\"start\":45691},{\"end\":45710,\"start\":45706},{\"end\":45721,\"start\":45714},{\"end\":46179,\"start\":46172},{\"end\":46190,\"start\":46183},{\"end\":46205,\"start\":46194},{\"end\":46218,\"start\":46209},{\"end\":46228,\"start\":46222},{\"end\":46240,\"start\":46232},{\"end\":46566,\"start\":46561},{\"end\":46575,\"start\":46572},{\"end\":46946,\"start\":46936},{\"end\":46956,\"start\":46950},{\"end\":46970,\"start\":46960},{\"end\":46983,\"start\":46974},{\"end\":47000,\"start\":46987},{\"end\":47327,\"start\":47320},{\"end\":47339,\"start\":47331},{\"end\":47347,\"start\":47343},{\"end\":47356,\"start\":47351},{\"end\":47367,\"start\":47360},{\"end\":47777,\"start\":47769},{\"end\":47788,\"start\":47783},{\"end\":47976,\"start\":47964},{\"end\":47987,\"start\":47980},{\"end\":48002,\"start\":47991},{\"end\":48018,\"start\":48006},{\"end\":48464,\"start\":48459},{\"end\":48476,\"start\":48468},{\"end\":48485,\"start\":48480},{\"end\":48496,\"start\":48491},{\"end\":48506,\"start\":48500},{\"end\":48518,\"start\":48510},{\"end\":48874,\"start\":48870},{\"end\":48880,\"start\":48878},{\"end\":48890,\"start\":48886},{\"end\":48902,\"start\":48896},{\"end\":49235,\"start\":49231},{\"end\":49241,\"start\":49239},{\"end\":49253,\"start\":49247},{\"end\":49263,\"start\":49259},{\"end\":49551,\"start\":49547},{\"end\":49559,\"start\":49555}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":53979606},\"end\":39147,\"start\":38796},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":8577357},\"end\":39450,\"start\":39149},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":17119110},\"end\":39796,\"start\":39452},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":81978372},\"end\":40273,\"start\":39798},{\"attributes\":{\"doi\":\"arXiv:1702.05374\",\"id\":\"b4\"},\"end\":40499,\"start\":40275},{\"attributes\":{\"doi\":\"arXiv:1708.04552\",\"id\":\"b5\"},\"end\":40761,\"start\":40501},{\"attributes\":{\"doi\":\"arXiv:1910.13580\",\"id\":\"b6\"},\"end\":41069,\"start\":40763},{\"attributes\":{\"doi\":\"arXiv:1705.07485\",\"id\":\"b7\"},\"end\":41221,\"start\":41071},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":53111490},\"end\":41539,\"start\":41223},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":12825123},\"end\":42015,\"start\":41541},{\"attributes\":{\"doi\":\"arXiv:1605.07648\",\"id\":\"b10\"},\"end\":42281,\"start\":42017},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":204509242},\"end\":42736,\"start\":42283},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6037691},\"end\":43140,\"start\":42738},{\"attributes\":{\"doi\":\"arXiv:1902.00113\",\"id\":\"b13\"},\"end\":43424,\"start\":43142},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":52833113},\"end\":43866,\"start\":43426},{\"attributes\":{\"id\":\"b15\"},\"end\":44025,\"start\":43868},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":18331438},\"end\":44391,\"start\":44027},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":2630174},\"end\":44704,\"start\":44393},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":5597033},\"end\":44923,\"start\":44706},{\"attributes\":{\"id\":\"b19\"},\"end\":45209,\"start\":44925},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":10970469},\"end\":45548,\"start\":45211},{\"attributes\":{\"doi\":\"10.1007/s11263-015-0816-y\",\"id\":\"b21\",\"matched_paper_id\":2930547},\"end\":46111,\"start\":45550},{\"attributes\":{\"doi\":\"arXiv:1804.10745\",\"id\":\"b22\"},\"end\":46451,\"start\":46113},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":976598},\"end\":46865,\"start\":46453},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6844431},\"end\":47256,\"start\":46867},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206592615},\"end\":47734,\"start\":47258},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":2777306},\"end\":47903,\"start\":47736},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":2928248},\"end\":48389,\"start\":47905},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":44178122},\"end\":48789,\"start\":48391},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":173188134},\"end\":49153,\"start\":48791},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":80628419},\"end\":49502,\"start\":49155},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":3654323},\"end\":49675,\"start\":49504}]", "bib_title": "[{\"end\":38860,\"start\":38796},{\"end\":39192,\"start\":39149},{\"end\":39536,\"start\":39452},{\"end\":39845,\"start\":39798},{\"end\":41284,\"start\":41223},{\"end\":41614,\"start\":41541},{\"end\":42365,\"start\":42283},{\"end\":42786,\"start\":42738},{\"end\":43481,\"start\":43426},{\"end\":44045,\"start\":44027},{\"end\":44451,\"start\":44393},{\"end\":44756,\"start\":44706},{\"end\":45274,\"start\":45211},{\"end\":45599,\"start\":45550},{\"end\":46555,\"start\":46453},{\"end\":46932,\"start\":46867},{\"end\":47316,\"start\":47258},{\"end\":47765,\"start\":47736},{\"end\":47960,\"start\":47905},{\"end\":48455,\"start\":48391},{\"end\":48866,\"start\":48791},{\"end\":49227,\"start\":49155},{\"end\":49543,\"start\":49504}]", "bib_author": "[{\"end\":38872,\"start\":38862},{\"end\":38892,\"start\":38872},{\"end\":38905,\"start\":38892},{\"end\":39207,\"start\":39194},{\"end\":39218,\"start\":39207},{\"end\":39229,\"start\":39218},{\"end\":39240,\"start\":39229},{\"end\":39251,\"start\":39240},{\"end\":39264,\"start\":39251},{\"end\":39550,\"start\":39538},{\"end\":39559,\"start\":39550},{\"end\":39861,\"start\":39847},{\"end\":39881,\"start\":39861},{\"end\":39890,\"start\":39881},{\"end\":39900,\"start\":39890},{\"end\":39911,\"start\":39900},{\"end\":40285,\"start\":40275},{\"end\":40512,\"start\":40501},{\"end\":40524,\"start\":40512},{\"end\":40842,\"start\":40835},{\"end\":40854,\"start\":40842},{\"end\":40867,\"start\":40854},{\"end\":40878,\"start\":40867},{\"end\":41083,\"start\":41071},{\"end\":41296,\"start\":41286},{\"end\":41305,\"start\":41296},{\"end\":41313,\"start\":41305},{\"end\":41627,\"start\":41616},{\"end\":41646,\"start\":41627},{\"end\":41655,\"start\":41646},{\"end\":41667,\"start\":41655},{\"end\":42028,\"start\":42017},{\"end\":42037,\"start\":42028},{\"end\":42054,\"start\":42037},{\"end\":42374,\"start\":42367},{\"end\":42381,\"start\":42374},{\"end\":42388,\"start\":42381},{\"end\":42399,\"start\":42388},{\"end\":42794,\"start\":42788},{\"end\":42802,\"start\":42794},{\"end\":42812,\"start\":42802},{\"end\":42828,\"start\":42812},{\"end\":43148,\"start\":43142},{\"end\":43157,\"start\":43148},{\"end\":43165,\"start\":43157},{\"end\":43172,\"start\":43165},{\"end\":43182,\"start\":43172},{\"end\":43198,\"start\":43182},{\"end\":43489,\"start\":43483},{\"end\":43503,\"start\":43489},{\"end\":43511,\"start\":43503},{\"end\":43520,\"start\":43511},{\"end\":43900,\"start\":43886},{\"end\":44058,\"start\":44047},{\"end\":44069,\"start\":44058},{\"end\":44078,\"start\":44069},{\"end\":44087,\"start\":44078},{\"end\":44097,\"start\":44087},{\"end\":44464,\"start\":44453},{\"end\":44476,\"start\":44464},{\"end\":44489,\"start\":44476},{\"end\":44770,\"start\":44758},{\"end\":44782,\"start\":44770},{\"end\":44933,\"start\":44925},{\"end\":44941,\"start\":44933},{\"end\":45284,\"start\":45276},{\"end\":45292,\"start\":45284},{\"end\":45302,\"start\":45292},{\"end\":45312,\"start\":45302},{\"end\":45616,\"start\":45601},{\"end\":45624,\"start\":45616},{\"end\":45630,\"start\":45624},{\"end\":45640,\"start\":45630},{\"end\":45652,\"start\":45640},{\"end\":45658,\"start\":45652},{\"end\":45667,\"start\":45658},{\"end\":45679,\"start\":45667},{\"end\":45689,\"start\":45679},{\"end\":45702,\"start\":45689},{\"end\":45712,\"start\":45702},{\"end\":45723,\"start\":45712},{\"end\":46181,\"start\":46170},{\"end\":46192,\"start\":46181},{\"end\":46207,\"start\":46192},{\"end\":46220,\"start\":46207},{\"end\":46230,\"start\":46220},{\"end\":46242,\"start\":46230},{\"end\":46568,\"start\":46557},{\"end\":46577,\"start\":46568},{\"end\":46948,\"start\":46934},{\"end\":46958,\"start\":46948},{\"end\":46972,\"start\":46958},{\"end\":46985,\"start\":46972},{\"end\":47002,\"start\":46985},{\"end\":47329,\"start\":47318},{\"end\":47341,\"start\":47329},{\"end\":47349,\"start\":47341},{\"end\":47358,\"start\":47349},{\"end\":47369,\"start\":47358},{\"end\":47779,\"start\":47767},{\"end\":47790,\"start\":47779},{\"end\":47978,\"start\":47962},{\"end\":47989,\"start\":47978},{\"end\":48004,\"start\":47989},{\"end\":48020,\"start\":48004},{\"end\":48466,\"start\":48457},{\"end\":48478,\"start\":48466},{\"end\":48487,\"start\":48478},{\"end\":48498,\"start\":48487},{\"end\":48508,\"start\":48498},{\"end\":48520,\"start\":48508},{\"end\":48876,\"start\":48868},{\"end\":48882,\"start\":48876},{\"end\":48892,\"start\":48882},{\"end\":48904,\"start\":48892},{\"end\":49237,\"start\":49229},{\"end\":49243,\"start\":49237},{\"end\":49255,\"start\":49243},{\"end\":49265,\"start\":49255},{\"end\":49553,\"start\":49545},{\"end\":49561,\"start\":49553}]", "bib_venue": "[{\"end\":38954,\"start\":38905},{\"end\":39280,\"start\":39264},{\"end\":39608,\"start\":39559},{\"end\":39988,\"start\":39911},{\"end\":40366,\"start\":40301},{\"end\":40608,\"start\":40540},{\"end\":40833,\"start\":40763},{\"end\":41125,\"start\":41099},{\"end\":41362,\"start\":41313},{\"end\":41734,\"start\":41667},{\"end\":42126,\"start\":42070},{\"end\":42466,\"start\":42399},{\"end\":42895,\"start\":42828},{\"end\":43257,\"start\":43214},{\"end\":43597,\"start\":43520},{\"end\":43884,\"start\":43868},{\"end\":44164,\"start\":44097},{\"end\":44533,\"start\":44489},{\"end\":44800,\"start\":44782},{\"end\":45045,\"start\":44941},{\"end\":45368,\"start\":45312},{\"end\":45795,\"start\":45748},{\"end\":46168,\"start\":46113},{\"end\":46637,\"start\":46577},{\"end\":47042,\"start\":47002},{\"end\":47446,\"start\":47369},{\"end\":47794,\"start\":47790},{\"end\":48097,\"start\":48020},{\"end\":48569,\"start\":48520},{\"end\":48953,\"start\":48904},{\"end\":49317,\"start\":49265},{\"end\":49575,\"start\":49561},{\"end\":40052,\"start\":39990},{\"end\":41788,\"start\":41736},{\"end\":42520,\"start\":42468},{\"end\":42949,\"start\":42897},{\"end\":43661,\"start\":43599},{\"end\":44218,\"start\":44166},{\"end\":47510,\"start\":47448},{\"end\":48161,\"start\":48099}]"}}}, "year": 2023, "month": 12, "day": 17}