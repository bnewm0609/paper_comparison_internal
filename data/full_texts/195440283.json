{"id": 195440283, "updated": "2023-07-19 02:08:15.99", "metadata": {"title": "Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition", "authors": "[{\"first\":\"Lei\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Yifan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Jian\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"Hanqing\",\"last\":\"Lu\",\"middle\":[]}]", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2019, "month": 6, "day": 1}, "abstract": "In skeleton-based action recognition, graph convolutional networks (GCNs), which model the human body skeletons as spatiotemporal graphs, have achieved remarkable performance. However, in existing GCN-based methods, the topology of the graph is set manually, and it is fixed over all layers and input samples. This may not be optimal for the hierarchical GCN and diverse samples in action recognition tasks. In addition, the second-order information (the lengths and directions of bones) of the skeleton data, which is naturally more informative and discriminative for action recognition, is rarely investigated in existing methods. In this work, we propose a novel two-stream adaptive graph convolutional network (2s-AGCN) for skeleton-based action recognition. The topology of the graph in our model can be either uniformly or individually learned by the BP algorithm in an end-to-end manner. This data-driven method increases the flexibility of the model for graph construction and brings more generality to adapt to various data samples. Moreover, a two-stream framework is proposed to model both the first-order and the second-order information simultaneously, which shows notable improvement for the recognition accuracy. Extensive experiments on the two large-scale datasets, NTU-RGBD and Kinetics-Skeleton, demonstrate that the performance of our model exceeds the state-of-the-art with a significant margin.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2957203102", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ShiZCL19a", "doi": "10.1109/cvpr.2019.01230"}}, "content": {"source": {"pdf_hash": "80ebe60f29eeed16a086855bf15db6f81b4eb8fc", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1805.07694", "status": "GREEN"}}, "grobid": {"id": "e7389cc77b207191bdec4a53662ec8def4f7272c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/80ebe60f29eeed16a086855bf15db6f81b4eb8fc.txt", "contents": "\nTwo-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition\n\n\nLei Shi lei.shi@nlpr.ia.ac.cn \nInstitute of Automation\nNational Laboratory of Pattern Recognition\nChinese Academy of Sciences\n\n\nUniversity of Chinese Academy of Sciences\n\n\nYifan Zhang yfzhang@nlpr.ia.ac.cn \nInstitute of Automation\nNational Laboratory of Pattern Recognition\nChinese Academy of Sciences\n\n\nUniversity of Chinese Academy of Sciences\n\n\nJian Cheng jcheng@nlpr.ia.ac.cn \nInstitute of Automation\nNational Laboratory of Pattern Recognition\nChinese Academy of Sciences\n\n\nUniversity of Chinese Academy of Sciences\n\n\nCAS Center for Excellence in Brain Science and Intelligence Technology\n\n\nHanqing Lu \nInstitute of Automation\nNational Laboratory of Pattern Recognition\nChinese Academy of Sciences\n\n\nUniversity of Chinese Academy of Sciences\n\n\nTwo-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition\n10.1109/CVPR.2019.01230\nIn skeleton-based action recognition, graph convolutional networks (GCNs), which model the human body skeletons as spatiotemporal graphs, have achieved remarkable performance. However, in existing GCN-based methods, the topology of the graph is set manually, and it is fixed over all layers and input samples. This may not be optimal for the hierarchical GCN and diverse samples in action recognition tasks. In addition, the second-order information (the lengths and directions of bones) of the skeleton data, which is naturally more informative and discriminative for action recognition, is rarely investigated in existing methods. In this work, we propose a novel two-stream adaptive graph convolutional network (2s-AGCN) for skeletonbased action recognition. The topology of the graph in our model can be either uniformly or individually learned by the BP algorithm in an end-to-end manner. This data-driven method increases the flexibility of the model for graph construction and brings more generality to adapt to various data samples. Moreover, a two-stream framework is proposed to model both the first-order and the second-order information simultaneously, which shows notable improvement for the recognition accuracy. Extensive experiments on the two large-scale datasets, NTU-RGBD and Kinetics-Skeleton, demonstrate that the performance of our model exceeds the state-of-the-art with a significant margin.\n\nIntroduction\n\nAction recognition methods based on skeleton data have been widely investigated and attracted considerable attention due to their strong adaptability to the dynamic circumstance and complicated background [31,8,6,27,22,29,33,19,20,21,14,13,23,18,17,32,30,34]. Conventional * Corresponding Author deep-learning-based methods manually structure the skeleton as a sequence of joint-coordinate vectors [6,27,22,29,33,19,20] or as a pseudo-image [21,14,13,23,18,17], which is fed into RNNs or CNNs to generate the prediction. However, representing the skeleton data as a vector sequence or a 2D grid cannot fully express the dependency between correlated joints. The skeleton is naturally structured as a graph in a non-Euclidean space with the joints as vertexes and their natural connections in the human body as edges. The previous methods cannot exploit the graph structure of the skeleton data and are difficult to generalize to skeletons with arbitrary forms. Recently, graph convolutional networks (GCNs), which generalize convolution from image to graph, have been successfully adopted in many applications [16,7,25,1,9,24,15]. For the skeletonbased action recognition task, Yan et al. [32] first apply GCNs to model the skeleton data. They construct a spatial graph based on the natural connections of joints in the human body and add the temporal edges between corresponding joints in consecutive frames. A distance-based sampling function is proposed for constructing the graph convolutional layer, which is then employed as a basic module to build the final spatiotemporal graph convolutional network (ST-GCN).\n\nHowever, there are three disadvantages for the process of the graph construction in ST-GCN [32]: (1) The skeleton graph employed in ST-GCN is heuristically predefined and represents only the physical structure of the human body. Thus it is not guaranteed to be optimal for the action recognition task. For example, the relationship between the two hands is important for recognizing classes such as \"clapping\" and \"reading.\" However, it is difficult for ST-GCN to capture the dependency between the two hands since they are located far away from each other in the predefined human-body-based graphs. (2) The structure of GCNs is hierarchical where different layers contain multilevel semantic information. However, the topology of the graph ap-plied in ST-GCN is fixed over all the layers, which lacks the flexibility and capacity to model the multilevel semantic information contained in all of the layers; (3) One fixed graph structure may not be optimal for all the samples of different action classes. For classes such as \"wiping face\" and \"touching head\", the connection between the hands and head should be stronger, but it is not true for some other classes, such as \"jumping up\" and \"sitting down\". This fact suggests that the graph structure should be data dependent, which, however, is not supported in ST-GCN.\n\nTo solve the above problems, a novel adaptive graph convolutional network is proposed in this work. It parameterizes two types of graphs, the structure of which are trained and updated jointly with convolutional parameters of the model. One type is a global graph, which represents the common pattern for all the data. Another type is an individual graph, which represents the unique pattern for each data. Both of the two types of graphs are optimized individually for different layers, which can better fit the hierarchical structure of the model. This data-driven method increases the flexibility of the model for graph construction and brings more generality to adapt to various data samples.\n\nAnother notable problem in ST-GCN is that the feature vector attached to each vertex only contains 2D or 3D coordinates of the joints, which can be regarded as the first-order information of the skeleton data. However, the second-order information, which represents the feature of bones between two joints, is not exploited. Typically, the lengths and directions of bones are naturally more informative and discriminative for action recognition. In order to exploit the second-order information of the skeleton data, the lengths and directions of bones are formulated as a vector pointing from its source joint to its target joint. Similar to the firstorder information, the vector is fed into an adaptive graph convolutional network to predict the action label. Moreover, a two-stream framework is proposed to fuse the first-order and second-order information to further improve the performance.\n\nTo verify the superiority of the proposed model, namely, the two-stream adaptive graph convolutional network (2s-AGCN), extensive experiments are performed on two large-scale datasets: NTU-RGBD [27] and Kinetics-Skeleton [12]. Our model achieves state-of-the-art performance on both of the datasets.\n\nThe main contributions of our work lie in three folds: (1) An adaptive graph convolutional network is proposed to adaptively learn the topology of the graph for different GCN layers and skeleton samples in an end-to-end manner, which can better suit the action recognition task and the hierarchical structure of the GCNs. (2) The secondorder information of the skeleton data is explicitly formulated and combined with the first-order information using a two-stream framework, which brings notable improvement for the recognition performance. (3) On two large-scale datasets for skeleton-based action recognition, the proposed 2s-AGCN exceeds the state-of-the-art by a significant margin. The code will be released for future work and to facilitate communication 1 .\n\n\nRelated work\n\n\nSkeleton-based action recognition\n\nConventional methods for skeleton-based action recognition usually design handcrafted features to model the human body [31,8]. However, the performance of these handcrafted-feature-based methods is barely satisfactory since it cannot consider all factors at the same time. With the development of deep learning, data-driven methods have become the mainstream methods, where the most widely used models are RNNs and CNNs. RNN-based methods usually model the skeleton data as a sequence of the coordinate vectors each represents a human body joint [6,27,22,29,33,19,20,3]. CNN-based methods model the skeleton data as a pseudo-image based on the manually designed transformation rules [21,14,13,23,18,17]. The CNNbased methods are generally more popular than RNN-based methods because the CNNs have better parallelizability and are easier to train than RNNs.\n\nHowever, both RNNs and CNNs fail to fully represent the structure of the skeleton data because the skeleton data are naturally embedded in the form of graphs rather than a vector sequence or 2D grids. Recently, Yan et al. [32] propose a spatiotemporal graph convolutional network (ST-GCN) to directly model the skeleton data as the graph structure. It eliminates the need for designing handcrafted part assignment or traversal rules, thus achieves better performance than previous methods.\n\n\nGraph convolutional neural networks\n\nThere have been many works on graph convolution, and the principle of constructing GCNs mainly follows two streams: spatial perspective and spectral perspective [28,2,11,25,1,16,7,5,9,24,15]. Spatial perspective methods directly perform the convolution filters on the graph vertexes and their neighbors, which are extracted and normalized based on manually designed rules [7,25,9,24,15]. In contrast to the spatial perspective methods, spectral perspective methods utilize the eigenvalues and eigenvectors of the graph Laplace matrices. These methods perform the graph convolution in the frequency domain with the help of the graph Fourier transform [28], which does not need to extract locally connected regions from graphs at each convolutional step [2,11,16,5]. This work follows the spatial perspective methods.\n\n\nGraph Convolutional Networks\n\n\nGraph construction\n\nThe raw skeleton data in one frame are always provided as a sequence of vectors. Each vector represents the 2D or 3D coordinates of the corresponding human joint. A complete action contains multiple frames with different lengths for different samples. We employ a spatiotemporal graph to model the structured information among these joints along both the spatial and temporal dimensions. The structure of the graph follows the work of ST-GCN [32]. The left sketch in Fig. 1 presents an example of the constructed spatiotemporal skeleton graph, where the joints are represented as vertexes and their natural connections in the human body are represented as spatial edges (the orange lines in Fig. 1, left). For the temporal dimension, the corresponding joints between two adjacent frames are connected with temporal edges (the blue lines in Fig. 1, left). The coordinate vector of each joint is set as the attribute of the corresponding vertex. \n\n\nGraph convolution\n\nGiven the graph defined above, multiple layers of spatiotemporal graph convolution operations are applied on the graph to extract the high-level features. The global average pooling layer and the sof tmax classifier are then employed to predict the action categories based on the extracted features.\n\nIn the spatial dimension, the graph convolution operation on vertex v i is formulated as [32]:\nf out (v i ) = vj \u2208Bi 1 Z ij f in (v j ) \u00b7 w(l i (v j ))(1)\nwhere f denotes the feature map and v denotes the vertex of the graph. B i denotes the sampling area of the convolution for v i , which is defined as the 1-distance neighbor vertexes (v j ) of the target vertex (v i ). w is the weighting function similar to the original convolution operation, which provides a weight vector based on the given input. Note that the number of weight vectors of convolution is fixed, while the number of vertexes in B i is varied. To map each vertex with a unique weight vector, a mapping function l i is designed specially in ST-GCN [32]. The right sketch in Fig. 1 shows this strategy, where \u00d7 represents the center of gravity of the skeleton. B i is the area enclosed by the curve. In detail, the strategy empirically sets the kernel size as 3 and naturally divides B i into 3 subsets: S i1 is the vertex itself (the red circle in Fig. 1, right); S i2 is the centripetal subset, which contains the neighboring vertexes that are closer to the center of gravity (the green circle); S i3 is the centrifugal subset, which contains the neighboring vertexes that are farther from the center of gravity (the blue circle). Z ij denotes the cardinality of S ik that contains v j . It aims to balance the contribution of each subset.\n\n\nImplementation\n\nThe implementation of the graph convolution in the spatial dimension is not straightforward. Concretely, the feature map of the network is actually a C \u00d7 T \u00d7 N tensor, where N denotes the number of vertexes, T denotes the temporal length and C denotes the number of channels. To implement the ST-GCN, Eq. 1 is transformed into\nf out = Kv k W k (f in A k ) M k(2)\nwhere K v denotes the kernel size of the spatial dimension. With the partition strategy designed above, K v is set to 3.\nA k = \u039b \u2212 1 2 k\u0100 k \u039b \u2212 1 2\nk , where\u0100 k is similar to the N \u00d7 N adjacency matrix, and its element\u0100 ij k indicates whether the vertex v j is in the subset S ik of vertex v i . It is used to extract the connected vertexes in a particular subset from f in for the corresponding weight vector. \u039b ii k = j (\u0100 ij k ) + \u03b1 is the normalized diagonal matrix. \u03b1 is set to 0.001 to avoid empty rows. W k is the C out \u00d7 C in \u00d7 1 \u00d7 1 weight vector of the 1 \u00d7 1 convolution operation, which represents the weighting function w in Eq. 1. M k is an N \u00d7 N attention map that indicates the importance of each vertex. denotes the dot product.\n\nFor the temporal dimension, since the number of neighbors for each vertex is fixed as 2 (corresponding joints in the two consecutive frames), it is straightforward to perform the graph convolution similar to the classical convolution operation. Concretely, we perform a K t \u00d7 1 convolution on the output feature map calculated above, where K t is the kernel size of temporal dimension.\n\n\nTwo-stream adaptive graph convolutional network\n\nIn this section, we introduce the components of our proposed two-stream adaptive graph convolutional network (2s-AGCN) in detail.\n\n\nAdaptive graph convolutional layer\n\nThe spatiotemporal graph convolution for the skeleton data described above is calculated based on a predefined graph, which may not be the best choice as explained in Sec. 1. To solve this problem, we propose an adaptive graph convolutional layer. It makes the topology of the graph optimized together with the other parameters of the network in an end-to-end learning manner. The graph is unique for different layers and samples, which greatly increases the flexibility of the model. Meanwhile, it is designed as a residual branch, which guarantees the stability of the original model.\n\nIn detail, according to Eq. 2, the topology of the graph is actually decided by the adjacency matrix and the mask, i.e., A k and M k , respectively. A k determines whether there are connections between two vertexes and M k determines the strength of the connections. To make the graph structure adaptive, we change Eq. 2 into the following form:\nf out = Kv k W k f in (A k + B k + C k )(3)\nThe main difference lies in the adjacency matrix of the graph, which is divided into three parts: A k , B k and C k . The first part (A k ) is the same as the original normalized N \u00d7 N adjacency matrix A k in Eq. 2. It represents the physical structure of the human body.\n\nThe second part (B k ) is also an N \u00d7 N adjacency matrix. In contrast to A k , the elements of B k are parameterized and optimized together with the other parameters in the training process. There are no constraints on the value of B k , which means that the graph is completely learned according to the training data. With this data-driven manner, the model can learn graphs that are fully targeted to the recognition task and more individualized for different information contained in different layers. Note that the element in the matrix can be an arbitrary value. It indicates not only the existence of the connections between two joints but also the strength of the connections. It can play the same role of the attention mechanism performed by M k in Eq. 2However, the original attention matrix M k is dot multiplied to A k , which means that if one of the elements in A k is 0, it will always be 0 irrespective the value of M k . Thus, it cannot generate the new connections that not exist in the original physical graph. From this perspective, B k is more flexible than M k .\n\nThe third part (C k ) is a data-dependent graph which learn a unique graph for each sample. To determine whether there is a connection between two vertexes and how strong the connection is, we apply the normalized embedded Gaussian function to calculate the similarity of the two vertexes: where N is the total number of the vertexes. We use the dot product to measure the similarity of the two vertexes in an embedding space. In detail, given the input feature map f in whose size is C in \u00d7T \u00d7N , we first embed it into C e \u00d7T \u00d7N with two embedding functions, i.e., \u03b8 and \u03c6. Here, through extensive experiments, we choose one 1 \u00d7 1 convolutional layer as the embedding function. The two embedded feature maps are rearranged and reshaped to an N \u00d7C e T matrix and a C e T \u00d7 N matrix. They are then multiplied to obtain an N \u00d7 N similarity matrix C k , whose element C ij k represents the similarity of vertex v i and vertex v j . The value of the matrix is normalized to 0 \u2212 1, which is used as the soft edge of the two vertexes. Since the normalized Gaussian is equipped with a sof tmax operation, we can calculate C k based on Eq.4 as follows:\nf (v i , v j ) = e \u03b8(vi) T \u03c6(vj ) N j=1 e \u03b8(vi) T \u03c6(vj )(4)C k = sof tmax(f in T W T \u03b8k W \u03c6k f in )(5)\nwhere W \u03b8 and W \u03c6 are the parameters of the embedding functions \u03b8 and \u03c6, respectively. Rather than directly replacing the original A k with B k or C k , we add them to it. The value of B k and the parameters of \u03b8 and \u03c6 are initialized to 0. In this way, it can strengthen the flexibility of the model without degrading the original performance.\n\nThe overall architecture of the adaptive graph convolu-tion layer is shown in Fig. 2. Except for the A k , B k and C k introduced above, the kernel size of convolution (K v ) is set the same as before, i.e., 3. w k is the weighting function introduced in Eq. 1, whose parameter is W k in Eq. 3. A residual connection, similar to [10], is added for each layer, which allows the layer to be inserted into any existing models without breaking its initial behavior. If the number of input channels is different than the number of output channels, a 1 \u00d7 1 convolution (orange box with dashed line in Fig. 2) is inserted in the residual path to transform the input to match the output in the channel dimension.\n\n\nAdaptive graph convolutional block\n\nThe convolution for the temporal dimension is the same as ST-GCN, i.e., performing the K t \u00d7 1 convolution on the C \u00d7 T \u00d7 N feature maps. Both the spatial GCN and temporal GCN are followed by a batch normalization (BN) layer and a ReLU layer. As shown in Fig. 3, one basic block is the combination of one spatial GCN (Convs), one temporal GCN (Convt) and an additional dropout layer with the drop rate set as 0.5. To stabilize the training, a residual connection is added for each block.  Convs represents the spatial GCN, and Convt represents the temporal GCN, both of which are followed by a BN layer and a ReLU layer. Moreover, a residual connection is added for each block.\n\n\nAdaptive graph convolutional network\n\nThe adaptive graph convolutional network (AGCN) is the stack of these basic blocks, as shown in Fig. 4. There are a total of 9 blocks. The numbers of output channels for each block are 64, 64, 64, 128, 128, 128, 256, 256 and 256. A data BN layer is added at the beginning to normalize the input data. A global average pooling layer is performed at the end to pool feature maps of different samples to the same size. The final output is sent to a sof tmax classifier to obtain the prediction.\n\n\nTwo-stream networks\n\nAs introduced in Sec. 1, the second-order information, i.e., the bone information, is also important for skeletonbased action recognition but is neglected in previous works. In this paper, we propose explicitly modeling the secondorder information, namely, the bone information, with a two-stream framework to enhance the recognition.\n\nIn particular, since each bone is bound with two joints, we define that the joint close to the center of gravity of the skeleton is the source joint and the joint far away from the center of gravity is the target joint. Each bone is represented as a vector pointing to its target joint from its source joint, which contains not only the length information, but also the direction information. For example, given a bone with its source joint v 1 = (x 1 , y 1 , z 1 ) and its target joint v 2 = (x 2 , y 2 , z 2 ), the vector of the bone is calculated as e v1,v2 = (x 2 \u2212 x 1 , y 2 \u2212 y 1 , z 2 \u2212 z 1 ).\n\nSince the graph of the skeleton data have no cycles, each bone can be assigned with a unique target joint. The number of joints is one more than the number of bones because the central joint is not assigned to any bones. To simplify the design of the network, we add an empty bone with its value as 0 to the central joint. In this way, both the graph and the network of bones can be designed the same as that of joints because each bone can be bound with a unique joint. We use J-stream and B-stream to represent the networks of joints and bones, respectively. The overall architecture (2s-AGCN) is shown in Fig. 5. Given a sample, we first calculate the data of bones based on the data of joints. Then, the joint data and bone data are fed into the J-stream and B-stream, respectively. Finally, the sof tmax scores of the two streams are added to obtain the fused score and predict the action label.\n\n\nExperiments\n\nTo perform a head-to-head comparison with ST-GCN, our experiments are conducted on the same two large-scale action recognition datasets: NTU-RGBD [27] and Kinetics-Skeleton [12,32]. First, since the NTU-RGBD dataset is  \n\n\nDatasets\n\nNTU-RGBD: NTU-RGBD [27] is currently the largest and most widely used in-door-captured action recognition dataset, which contains 56,000 action clips in 60 action classes. The clips are performed by 40 volunteers in different age groups ranging from 10 to 35. Each action is captured by 3 cameras at the same height but from different horizontal angles: \u221245 \u2022 , 0 \u2022 , 45 \u2022 . This dataset provides 3D joint locations of each frame detected by Kinect depth sensors. There are 25 joints for each subject in the skeleton sequences, while each video has no more than 2 subjects. The original paper [27] of the dataset recommends two benchmarks: 1). Cross-subject (X-Sub): the dataset in this benchmark is divided into a training set (40,320 videos) and a validation set (16,560 videos), where the actors in the two subsets are different. 2).Cross-view (X-View): the training set in this benchmark contains 37,920 videos that are captured by cameras 2 and 3, and the validation set contains 18,960 videos that are captured by camera 1. We follow this convention and report the top-1 accuracy on both benchmarks.\n\nKinetics-Skeleton: Kinetics [12] is a large-scale human action dataset that contains 300,000 videos clips in 400 classes. The video clips are sourced from YouTube videos and have a great variety. It only provides raw video clips without skeleton data. [32] estimate the locations of 18 joints on every frame of the clips using the publicly available OpenPose toolbox [4]. Two peoples are selected for multiperson clips based on the average joint confidence. We use their released data (Kinetics-Skeleton) to evaluate our model. The dataset is divided into a training set (240,000 clips) and a validation set (20,000 clips). Following the evaluation method in [32], we train the models on the training set and report the top-1 and top-5 accuracies on the validation set.\n\n\nTraining details\n\nAll experiments are conducted on the PyTorch deep learning framework [26]. Stochastic gradient descent (SGD) with Nesterov momentum (0.9) is applied as the optimization strategy. The batch size is 64. Cross-entropy is selected as the loss function to backpropagate gradients. The weight decay is set to 0.0001.\n\nFor the NTU-RGBD dataset, there are at most two people in each sample of the dataset. If the number of bodies in the sample is less than 2, we pad the second body with 0. The max number of frames in each sample is 300. For samples with less than 300 frames, we repeat the samples until it reaches 300 frames. The learning rate is set as 0.1 and is divided by 10 at the 30 th epoch and 40 th epoch. The training process is ended at the 50 th epoch.\n\nFor the Kinetics-Skeleton dataset, the size of the input tensor of Kinetics is set the same as [32], which contains 150 frames with 2 bodies in each frame. We perform the same data-augmentation methods as in [32]. In detail, we randomly choose 150 frames from the input skeleton sequence and slightly disturb the joint coordinates with randomly chosen rotations and translations. The learning rate is also set as 0.1 and is divided by 10 at the 45 th epoch and 55 th epoch. The training process is ended at the 65 th epoch.\n\n\nAblation Study\n\nWe examine the effectiveness of the proposed components in two-stream adaptive graph convolutional network (2s-AGCN) in this section with the X-View benchmark on the NTU-RGBD dataset. The original performance of ST-GCN on the NTU-RGBD dataset is 88.3%. By using the rearranged learning-rate scheduler and the specially designed data preprocessing methods, it is improved to 92.7%, which is used as the baseline in the experiments. The detail is introduced in the supplementary material.\n\n\nAdaptive graph convolutional block.\n\nAs introduced in Section 4.1, there are 3 types of graphs in the adaptive graph convolutional block, i.e., A, B and C. We manually delete one of the graphs and show their performance in Tab. 1. This table shows that adaptively learning the graph is beneficial for action recognition and that deleting any one of the three graphs will harm the performance. With all three graphs added together, the model obtains the best performance. We also test the importance of M used in the original ST-GCN. The result shows that given each connection, a weight parameter is important, which also proves the importance of the adaptive graph structure.\n\n\nMethods\n\nAccuracy ( Fig. 8 is a visualization of the skeleton graph for different layers of one sample (from left to right is the 3 rd , 5 th and 7 th layers in Fig. 4, respectively). The skeletons are plotted based on the physical connections of the human body. Each circle represents one joint, and its size represents the strength of the connection between the current joint and the 25 th joint in the learned adaptive graph of our model. It shows that a traditional physical connection of the human body is not the best choice for the action recognition task, and different layers need graphs with different topology structures. The skeleton graph in the 3 rd layer pays more attention to the adjacent joints in the physical graph. This result is intuitive since the lower layer only contains the low-level feature, while the global information cannot be observed. For the 5 th layer, more joints along the same arm are strongly connected. For the 7 th layer, the left hand and the right hand show a stronger connection, although they are far away from each other in the physical structure of the human body. We argue that a higher layer contains higher-level information. Hence, the graph is more relevant to the final classification task.  Fig. 9 shows a similar visualization of Fig. 8 but for different samples. The learned adjacency matrix is extracted from the second subset of the 5 th layer in the model (Fig. 4). It shows that the graph structures learned by our model for different samples are also different, even for the same convolutional subset and the same layer. It verified our point of view that different samples need different topologies of the graph, and a data-driven graph structure is better than a fixed one. Figure 9. Visualization of the graphs for different samples.\n\n\nVisualization of the learned graphs\n\n\nTwo-stream framework\n\nAnother important improvement is the utilization of secondorder information. Here, we compare the performance of using each type of input data alone, shown as Js-AGCN and Bs-AGCN in Tab. 2, and the performance when combining them as described in Section 4.4, shown as 2s-AGCN in Tab. 2. Clearly, the two-stream method outperforms the one-stream-based methods.\n\n\nMethods\n\nAccuracy (%) Js-AGCN 93.7 Bs-AGCN 93.2 2s-AGCN 95.1 Table 2. Comparisons of the validation accuracy with different input modalities.\n\n\nComparison with the state-of-the-art\n\nWe compare the final model with the state-of-the-art skeleton-based action recognition methods on both the NTU-RGBD dataset and Kinetics-Skeleton dataset. The results of these two comparisons are shown in Tab 3 and Tab 4, respectively. The methods used for comparison include the handcraft-feature-based methods [31,8], RNNbased methods [6,27,22,29,33,19,20], CNN-based methods [21,14,13,23,18,17] and GCN-based methods [32,30]. Our model achieves state-of-the-art performance with a large margin on both of the datasets, which verifies the superiority of our model.\n\n\nConclusion\n\nIn this work, we propose a novel adaptive graph convolutional neural network (2s-AGCN) for skeleton-based ac-Methods X-Sub (%) X-View (%) Lie Group [31] 50.1 82.8 HBRNN [6] 59.1 64.0 Deep LSTM [27] 60.7 67.3 ST-LSTM [22] 69.2 77.7 STA-LSTM [29] 73.4 81.2 VA-LSTM [33] 79.2 87.7 ARRN-LSTM [19] 80.7 88.8 Ind-RNN [20] 81.8 88.0 Two-Stream 3DCNN [21] 66.8 72.6 TCN [14] 74.3 83.1 Clips+CNN+MTLN [13] 79.6 84.8 Synthesized CNN [23] 80.0 87. tion recognition. It parameterizes the graph structure of the skeleton data and embeds it into the network to be jointly learned and updated with the model. This data-driven approach increases the flexibility of the graph convolutional network and is more suitable for the action recognition task. Furthermore, the traditional methods always ignore or underestimate the importance of second-order information of skeleton data, i.e., the bone information. In this work, we propose a two-stream framework to explicitly employ this type of information, which further enhances the performance. The final model is evaluated on two large-scale action recognition datasets, NTU-RGBD and Kinetics, and it achieves the state-of-the-art performance on both of them.\n\nFigure 1 .\n1(a).Illustration of the spatiotemporal graph used in ST-GCN. (b).Illustration of the mapping strategy. Different colors denote different subsets.\n\nFigure 2 .\n2Illustration of the adaptive graph convolutional layer. There are a total of three types of graphs in each layer, i.e., A k , B k and C k . The orange box indicates that the parameter is learnable. (1 \u00d7 1) denotes the kernel size of convolution. Kv denotes the number of subsets. \u2295 denotes the elementwise summation. \u2297 denotes the matirx multiplication. The residual box (dotted line) is only needed when Cin is not the same as Cout.\n\nFigure 3 .\n3Illustration of the adaptive graph convolutional block.\n\nFigure 4 .\n4Illustration of the AGCN. There are a total of 9 blocks (B1-B9). The three numbers of each block represent the number of input channels, the number of output channels and the stride, respectively. GAP represents the global average pooling layer.\n\n\u2a01Figure 5 .\n5Illustration of the overall architecture of the 2s-AGCN. The scores of two streams are added to obtain the final prediction.smaller than the Kinetics-Skeleton dataset, we perform exhaustive ablation studies on it to examine the contributions of the proposed model components based on the recognition performance. Then, the final model is evaluated on both of the datasets to verify the generality and is compared with the other state-of-the-art approaches. The definitions of joints and their natural connections in the two datasets are shown inFig. 6.\n\nFigure 6 .\n6The left sketch shows the joint label of the Kinetics-Skeleton dataset and the right sketch shows the joint label of the NTU-RGBD dataset.\n\nFig. 7\n7shows an example of the adjacency matrix learned by our model for the second subset. The gray scale of each element in the matrix represents the strength of the connection. The left is the original adjacency matrix for the second subset employed in ST-GCN, and the right is an example of the corresponding adaptive adjacency matrix learned by our model. It is clear that the learned structure of the graph is more flexible and not constrained to the physical connections of the human body.\n\nFigure 7 .\n7Example of the learned adjacency matrix. The left matrix is the original adjacency matrix for the second subset in the NTU-RGBD dataset. The right matrix is an example of the corresponding adaptive adjacency matrix learned by our model.\n\nFigure 8 .\n8Visualization of the graphs for different layers.\n\n\nTable 3. Comparisons of the validation accuracy with state-of-theart methods on the NTU-RGBD dataset.Table 4. Comparisons of the validation accuracy with state-of-theart methods on the Kinetics-Skeleton dataset.2 \nCNN+Motion+Trans [18] \n83.2 \n89.3 \n3scale ResNet152 [17] \n85.0 \n92.3 \nST-GCN [32] \n81.5 \n88.3 \nDPRL+GCNN [30] \n83.5 \n89.8 \n2s-AGCN (ours) \n88.5 \n95.1 \n\nMethods \nTop-1 (%) Top-5 (%) \nFeature Enc. [8] \n14.9 \n25.8 \nDeep LSTM [27] \n16.4 \n35.3 \nTCN [14] \n20.3 \n40.0 \nST-GCN [32] \n30.7 \n52.8 \nJs-AGCN (ours) \n35.1 \n57.1 \nBs-AGCN (ours) \n33.3 \n55.7 \n2s-AGCN (ours) \n36.1 \n58.7 \n\n\nhttps://github.com/lshiwjx/2s-AGCN\n\nDiffusion-convolutional neural networks. James Atwood, Don Towsley, Advances in Neural Information Processing Systems. James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in Neural Information Pro- cessing Systems, pages 1993-2001, 2016. 1, 2\n\nSpectral Networks and Locally Connected Networks on Graphs. Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann Le-Cun, ICLR. Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann Le- Cun. Spectral Networks and Locally Connected Networks on Graphs. In ICLR, 2014. 2\n\nC Cao, C Lan, Y Zhang, W Zeng, H Lu, Y Zhang, Skeleton-Based Action Recognition with Gated Convolutional Neural Networks. IEEE Transactions on Circuits and Systems for Video Technology. C. Cao, C. Lan, Y. Zhang, W. Zeng, H. Lu, and Y. Zhang. Skeleton-Based Action Recognition with Gated Convolu- tional Neural Networks. IEEE Transactions on Circuits and Systems for Video Technology, pages 1-1, 2018. 2\n\nRealtime multi-person 2d pose estimation using part affinity fields. Zhe Cao, Tomas Simon, Shih-En Wei, Yaser Sheikh, CVPR. Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using part affinity fields. In CVPR, 2017. 6\n\nConvolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. Michal Defferrard, Xavier Bresson, Pierre Vandergheynst, ; D D Lee, M Sugiyama, U V Luxburg, I Guyon, R Garnett, Advances in Neural Information Processing Systems. 292Curran Associates, IncMichal Defferrard, Xavier Bresson, and Pierre Van- dergheynst. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, edi- tors, Advances in Neural Information Processing Systems 29, pages 3844-3852. Curran Associates, Inc., 2016. 2\n\nHierarchical recurrent neural network for skeleton based action recognition. Yong Du, Wei Wang, Liang Wang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionYong Du, Wei Wang, and Liang Wang. Hierarchical recur- rent neural network for skeleton based action recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1110-1118, 2015. 1, 2, 8\n\nConvolutional Networks on Graphs for Learning Molecular Fingerprints. Dougal David K Duvenaud, Jorge Maclaurin, Rafael Iparraguirre, Timothy Bombarell, Alan Hirzel, Ryan P Aspuru-Guzik, Adams, Advances in Neural Information Processing Systems. C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editorsCurran Associates, Inc28David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan Aspuru-Guzik, and Ryan P Adams. Convolutional Networks on Graphs for Learning Molecular Fingerprints. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, edi- tors, Advances in Neural Information Processing Systems 28, pages 2224-2232. Curran Associates, Inc., 2015. 1, 2\n\nModeling video evolution for action recognition. Basura Fernando, Efstratios Gavves, Jose M Oramas, Amir Ghodrati, Tinne Tuytelaars, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionBasura Fernando, Efstratios Gavves, Jose M. Oramas, Amir Ghodrati, and Tinne Tuytelaars. Modeling video evolution for action recognition. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 5378-5387, 2015. 1, 2, 8\n\nInductive representation learning on large graphs. Will Hamilton, Zhitao Ying, Jure Leskovec, Advances in Neural Information Processing Systems. 1Will Hamilton, Zhitao Ying, and Jure Leskovec. Induc- tive representation learning on large graphs. In Advances in Neural Information Processing Systems, pages 1025-1035, 2017. 1, 2\n\nDeep Residual Learning for Image Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In The IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR), June 2016. 5\n\nMikael Henaff, Joan Bruna, Yann Lecun, arXiv:1506.05163Deep convolutional networks on graph-structured data. arXiv preprintMikael Henaff, Joan Bruna, and Yann LeCun. Deep convo- lutional networks on graph-structured data. arXiv preprint arXiv:1506.05163, 2015. 2\n\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, arXiv:1705.06950Paul Natsev, and others. The Kinetics Human Action Video Dataset. 56arXiv preprintWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, and others. The Kinetics Human Action Video Dataset. arXiv preprint arXiv:1705.06950, 2017. 2, 5, 6\n\nQiuhong Ke, Mohammed Bennamoun, Senjian An, Ferdous Ahmed Sohel, and Farid Boussad. A New Representation of Skeleton Sequences for 3d Action Recognition. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Qiuhong Ke, Mohammed Bennamoun, Senjian An, Fer- dous Ahmed Sohel, and Farid Boussad. A New Representa- tion of Skeleton Sequences for 3d Action Recognition. 2017 IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), pages 4570-4579, 2017. 1, 2, 8\n\nInterpretable 3d human action analysis with temporal convolutional networks. Soo Tae, Austin Kim, Reiter, Computer Vision and Pattern Recognition Workshops. CVPRW2017 IEEE Conference onTae Soo Kim and Austin Reiter. Interpretable 3d human ac- tion analysis with temporal convolutional networks. In Com- puter Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on, pages 1623-1631, 2017. 1, 2, 8\n\nNeural relational inference for interacting systems. Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, Richard Zemel, International Conference on Machine Learning (ICML). 1Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. In International Conference on Machine Learning (ICML), 2018. 1, 2\n\nSemi-Supervised Classification with Graph Convolutional Networks. N Thomas, Max Kipf, Welling, arXiv:1609.02907arXiv preprintThomas N. Kipf and Max Welling. Semi-Supervised Classi- fication with Graph Convolutional Networks. arXiv preprint arXiv:1609.02907, Sept. 2016. 1, 2\n\nSkeleton based action recognition using translation-scale invariant image mapping and multi-scale deep CNN. Bo Li, Yuchao Dai, Xuelian Cheng, Huahui Chen, Yi Lin, Mingyi He, 2017 IEEE International Conference on. ICMEW2Multimedia & Expo WorkshopsBo Li, Yuchao Dai, Xuelian Cheng, Huahui Chen, Yi Lin, and Mingyi He. Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep CNN. In Multimedia & Expo Workshops (ICMEW), 2017 IEEE International Conference on, pages 601-604. IEEE, 2017. 1, 2, 8\n\nSkeleton-based action recognition with convolutional neural networks. Chao Li, Qiaoyong Zhong, Di Xie, Shiliang Pu, In Multimedia & Expo Workshops. 2017ICMEWChao Li, Qiaoyong Zhong, Di Xie, and Shiliang Pu. Skeleton-based action recognition with convolutional neural networks. In Multimedia & Expo Workshops (ICMEW), 2017\n\nIEEE International Conference on. 2IEEE International Conference on, pages 597-600. IEEE, 2017. 1, 2, 8\n\nSkeleton-Based Relational Modeling for Action Recognition. Lin Li, Wu Zheng, Zhaoxiang Zhang, Yan Huang, Liang Wang, arXiv:1805.02556Lin Li, Wu Zheng, Zhaoxiang Zhang, Yan Huang, and Liang Wang. Skeleton-Based Relational Modeling for Ac- tion Recognition. arXiv:1805.02556 [cs], 2018. 1, 2, 8\n\nIndependently recurrent neural network (indrnn): Building A longer and deeper RNN. Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, Yanbo Gao, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionShuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. Independently recurrent neural network (indrnn): Building A longer and deeper RNN. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 5457-5466, 2018. 1, 2, 8\n\nTwo-Stream 3d Convolutional Neural Network for Skeleton-Based Action Recognition. Hong Liu, Juanhui Tu, Mengyuan Liu, arXiv:1705.08106Hong Liu, Juanhui Tu, and Mengyuan Liu. Two-Stream 3d Convolutional Neural Network for Skeleton-Based Action Recognition. arXiv:1705.08106 [cs], May 2017. 1, 2, 8\n\nSpatio-Temporal LSTM with Trust Gates for 3d Human Action Recognition. Jun Liu, Amir Shahroudy, Dong Xu, Gang Wang, Computer Vision ECCV 2016. ChamSpringer International Publishing9907Jun Liu, Amir Shahroudy, Dong Xu, and Gang Wang. Spatio-Temporal LSTM with Trust Gates for 3d Human Ac- tion Recognition. In Computer Vision ECCV 2016, vol- ume 9907, pages 816-833. Springer International Publish- ing, Cham, 2016. 1, 2, 8\n\nEnhanced skeleton visualization for view invariant human action recognition. Mengyuan Liu, Hong Liu, Chen Chen, Pattern Recognition. 68Mengyuan Liu, Hong Liu, and Chen Chen. Enhanced skele- ton visualization for view invariant human action recogni- tion. Pattern Recognition, 68:346-362, 2017. 1, 2, 8\n\nGeometric deep learning on graphs and manifolds using mixture model CNNs. Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, Michael M Bronstein, Proc. CVPR. CVPR1Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M. Bronstein. Geometric deep learning on graphs and manifolds using mixture model CNNs. In Proc. CVPR, volume 1, page 3, 2017. 1, 2\n\nLearning convolutional neural networks for graphs. Mathias Niepert, Mohamed Ahmed, Konstantin Kutzkov, International conference on machine learning. 1Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks for graphs. In International conference on machine learning, pages 2014-2023, 2016. 1, 2\n\nAutomatic differentiation in PyTorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al- ban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. In NIPS-W, 2017. 6\n\nNTU RGB+D: A Large Scale Dataset for 3d Human Activity Analysis. Amir Shahroudy, Jun Liu, Tian-Tsong Ng, Gang Wang, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 6Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. NTU RGB+D: A Large Scale Dataset for 3d Human Activity Analysis. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 1, 2, 5, 6, 8\n\nThe emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. David I Shuman, K Sunil, Pascal Narang, Antonio Frossard, Pierre Ortega, Vandergheynst, IEEE Signal Processing Magazine. 303David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The emerging field of sig- nal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE Sig- nal Processing Magazine, 30(3):83-98, 2013. 2\n\nAn End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data. Sijie Song, Cuiling Lan, Junliang Xing, Wenjun Zeng, Jiaying Liu, AAAI. 1Sijie Song, Cuiling Lan, Junliang Xing, Wenjun Zeng, and Jiaying Liu. An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data. In AAAI, volume 1, pages 4263-4270, 2017. 1, 2, 8\n\nDeep Progressive Reinforcement Learning for Skeleton-Based Action Recognition. Yansong Tang, Yi Tian, Jiwen Lu, Peiyang Li, Jie Zhou, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Yansong Tang, Yi Tian, Jiwen Lu, Peiyang Li, and Jie Zhou. Deep Progressive Reinforcement Learning for Skeleton- Based Action Recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 1, 8\n\nHuman action recognition by representing 3d skeletons as points in a lie group. Raviteja Vemulapalli, Felipe Arrate, Rama Chellappa, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionRaviteja Vemulapalli, Felipe Arrate, and Rama Chellappa. Human action recognition by representing 3d skeletons as points in a lie group. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 588-595, 2014. 1, 2, 8\n\nSpatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition. Sijie Yan, Yuanjun Xiong, Dahua Lin, AAAI. 7Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition. In AAAI, 2018. 1, 2, 3, 5, 6, 7, 8\n\nView Adaptive Recurrent Neural Networks for High Performance Human Action Recognition From Skeleton Data. Pengfei Zhang, Cuiling Lan, Junliang Xing, Wenjun Zeng, Jianru Xue, Nanning Zheng, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionPengfei Zhang, Cuiling Lan, Junliang Xing, Wenjun Zeng, Jianru Xue, and Nanning Zheng. View Adaptive Recur- rent Neural Networks for High Performance Human Action Recognition From Skeleton Data. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition, pages 2117-2126, 2017. 1, 2, 8\n\nEgoGesture: A New Dataset and Benchmark for Egocentric Hand Gesture Recognition. Yifan Zhang, Congqi Cao, Jian Cheng, Hanqing Lu, IEEE Transactions on Multimedia. 1Yifan Zhang, Congqi Cao, Jian Cheng, and Hanqing Lu. EgoGesture: A New Dataset and Benchmark for Egocentric Hand Gesture Recognition. IEEE Transactions on Multime- dia, pages 1-1, 2018. 1\n", "annotations": {"author": "[{\"end\":261,\"start\":90},{\"end\":437,\"start\":262},{\"end\":684,\"start\":438},{\"end\":837,\"start\":685}]", "publisher": null, "author_last_name": "[{\"end\":97,\"start\":94},{\"end\":273,\"start\":268},{\"end\":448,\"start\":443},{\"end\":695,\"start\":693}]", "author_first_name": "[{\"end\":93,\"start\":90},{\"end\":267,\"start\":262},{\"end\":442,\"start\":438},{\"end\":692,\"start\":685}]", "author_affiliation": "[{\"end\":216,\"start\":121},{\"end\":260,\"start\":218},{\"end\":392,\"start\":297},{\"end\":436,\"start\":394},{\"end\":566,\"start\":471},{\"end\":610,\"start\":568},{\"end\":683,\"start\":612},{\"end\":792,\"start\":697},{\"end\":836,\"start\":794}]", "title": "[{\"end\":87,\"start\":1},{\"end\":924,\"start\":838}]", "venue": null, "abstract": "[{\"end\":2364,\"start\":949}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2589,\"start\":2585},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2591,\"start\":2589},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2593,\"start\":2591},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2596,\"start\":2593},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2599,\"start\":2596},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2602,\"start\":2599},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2605,\"start\":2602},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2608,\"start\":2605},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2611,\"start\":2608},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2614,\"start\":2611},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2617,\"start\":2614},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2620,\"start\":2617},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2623,\"start\":2620},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2626,\"start\":2623},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2629,\"start\":2626},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2632,\"start\":2629},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2635,\"start\":2632},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2638,\"start\":2635},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2781,\"start\":2778},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2784,\"start\":2781},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2787,\"start\":2784},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2790,\"start\":2787},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2793,\"start\":2790},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2796,\"start\":2793},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2799,\"start\":2796},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2825,\"start\":2821},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2828,\"start\":2825},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2831,\"start\":2828},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2834,\"start\":2831},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2837,\"start\":2834},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2840,\"start\":2837},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3494,\"start\":3490},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3496,\"start\":3494},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3499,\"start\":3496},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3501,\"start\":3499},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3503,\"start\":3501},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3506,\"start\":3503},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3509,\"start\":3506},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3573,\"start\":3569},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4094,\"start\":4090},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4099,\"start\":4096},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4602,\"start\":4599},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7115,\"start\":7111},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7142,\"start\":7138},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8159,\"start\":8155},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8161,\"start\":8159},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8585,\"start\":8582},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8588,\"start\":8585},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8591,\"start\":8588},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8594,\"start\":8591},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8597,\"start\":8594},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8600,\"start\":8597},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8603,\"start\":8600},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8605,\"start\":8603},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8723,\"start\":8719},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8726,\"start\":8723},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8729,\"start\":8726},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8732,\"start\":8729},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8735,\"start\":8732},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8738,\"start\":8735},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9120,\"start\":9116},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9588,\"start\":9584},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9590,\"start\":9588},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9593,\"start\":9590},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9596,\"start\":9593},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9598,\"start\":9596},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9601,\"start\":9598},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9603,\"start\":9601},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9605,\"start\":9603},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9607,\"start\":9605},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9610,\"start\":9607},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9613,\"start\":9610},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9798,\"start\":9795},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9801,\"start\":9798},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9803,\"start\":9801},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9806,\"start\":9803},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9809,\"start\":9806},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10077,\"start\":10073},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10178,\"start\":10175},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10181,\"start\":10178},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10184,\"start\":10181},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10186,\"start\":10184},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10738,\"start\":10734},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11652,\"start\":11648},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12283,\"start\":12279},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18968,\"start\":18964},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22615,\"start\":22611},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22642,\"start\":22638},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":22645,\"start\":22642},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22721,\"start\":22717},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23295,\"start\":23291},{\"end\":23478,\"start\":23463},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23837,\"start\":23833},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24061,\"start\":24057},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24175,\"start\":24172},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24468,\"start\":24464},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24668,\"start\":24664},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25455,\"start\":25451},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25568,\"start\":25564},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":29787,\"start\":29783},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":29789,\"start\":29787},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29811,\"start\":29808},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29814,\"start\":29811},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":29817,\"start\":29814},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":29820,\"start\":29817},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29823,\"start\":29820},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":29826,\"start\":29823},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":29829,\"start\":29826},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29853,\"start\":29849},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29856,\"start\":29853},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29859,\"start\":29856},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29862,\"start\":29859},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":29865,\"start\":29862},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":29868,\"start\":29865},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":29895,\"start\":29891},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":29898,\"start\":29895},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":30204,\"start\":30200},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":30224,\"start\":30221},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":30249,\"start\":30245},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":30272,\"start\":30268},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":30296,\"start\":30292},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":30319,\"start\":30315},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":30344,\"start\":30340},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30367,\"start\":30363},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30399,\"start\":30395},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":30418,\"start\":30414},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30448,\"start\":30444},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30479,\"start\":30475}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31403,\"start\":31245},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31850,\"start\":31404},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31919,\"start\":31851},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32178,\"start\":31920},{\"attributes\":{\"id\":\"fig_5\"},\"end\":32745,\"start\":32179},{\"attributes\":{\"id\":\"fig_6\"},\"end\":32897,\"start\":32746},{\"attributes\":{\"id\":\"fig_7\"},\"end\":33396,\"start\":32898},{\"attributes\":{\"id\":\"fig_8\"},\"end\":33646,\"start\":33397},{\"attributes\":{\"id\":\"fig_9\"},\"end\":33709,\"start\":33647},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34298,\"start\":33710}]", "paragraph": "[{\"end\":3997,\"start\":2380},{\"end\":5319,\"start\":3999},{\"end\":6017,\"start\":5321},{\"end\":6915,\"start\":6019},{\"end\":7216,\"start\":6917},{\"end\":7983,\"start\":7218},{\"end\":8892,\"start\":8036},{\"end\":9383,\"start\":8894},{\"end\":10238,\"start\":9423},{\"end\":11236,\"start\":10292},{\"end\":11557,\"start\":11258},{\"end\":11653,\"start\":11559},{\"end\":12971,\"start\":11714},{\"end\":13316,\"start\":12990},{\"end\":13473,\"start\":13353},{\"end\":14097,\"start\":13501},{\"end\":14484,\"start\":14099},{\"end\":14665,\"start\":14536},{\"end\":15290,\"start\":14704},{\"end\":15637,\"start\":15292},{\"end\":15953,\"start\":15682},{\"end\":17038,\"start\":15955},{\"end\":18185,\"start\":17040},{\"end\":18633,\"start\":18289},{\"end\":19339,\"start\":18635},{\"end\":20055,\"start\":19378},{\"end\":20587,\"start\":20096},{\"end\":20945,\"start\":20611},{\"end\":21547,\"start\":20947},{\"end\":22449,\"start\":21549},{\"end\":22685,\"start\":22465},{\"end\":23803,\"start\":22698},{\"end\":24574,\"start\":23805},{\"end\":24905,\"start\":24595},{\"end\":25354,\"start\":24907},{\"end\":25879,\"start\":25356},{\"end\":26384,\"start\":25898},{\"end\":27063,\"start\":26424},{\"end\":28864,\"start\":27075},{\"end\":29286,\"start\":28927},{\"end\":29430,\"start\":29298},{\"end\":30037,\"start\":29471},{\"end\":31244,\"start\":30052}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11713,\"start\":11654},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13352,\"start\":13317},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13500,\"start\":13474},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15681,\"start\":15638},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18245,\"start\":18186},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18288,\"start\":18245}]", "table_ref": "[{\"end\":29357,\"start\":29350}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2378,\"start\":2366},{\"attributes\":{\"n\":\"2.\"},\"end\":7998,\"start\":7986},{\"attributes\":{\"n\":\"2.1.\"},\"end\":8034,\"start\":8001},{\"attributes\":{\"n\":\"2.2.\"},\"end\":9421,\"start\":9386},{\"attributes\":{\"n\":\"3.\"},\"end\":10269,\"start\":10241},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10290,\"start\":10272},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11256,\"start\":11239},{\"attributes\":{\"n\":\"3.3.\"},\"end\":12988,\"start\":12974},{\"attributes\":{\"n\":\"4.\"},\"end\":14534,\"start\":14487},{\"attributes\":{\"n\":\"4.1.\"},\"end\":14702,\"start\":14668},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19376,\"start\":19342},{\"attributes\":{\"n\":\"4.3.\"},\"end\":20094,\"start\":20058},{\"attributes\":{\"n\":\"4.4.\"},\"end\":20609,\"start\":20590},{\"attributes\":{\"n\":\"5.\"},\"end\":22463,\"start\":22452},{\"attributes\":{\"n\":\"5.1.\"},\"end\":22696,\"start\":22688},{\"attributes\":{\"n\":\"5.2.\"},\"end\":24593,\"start\":24577},{\"attributes\":{\"n\":\"5.3.\"},\"end\":25896,\"start\":25882},{\"attributes\":{\"n\":\"5.3.1\"},\"end\":26422,\"start\":26387},{\"end\":27073,\"start\":27066},{\"attributes\":{\"n\":\"5.3.2\"},\"end\":28902,\"start\":28867},{\"attributes\":{\"n\":\"5.3.3\"},\"end\":28925,\"start\":28905},{\"end\":29296,\"start\":29289},{\"attributes\":{\"n\":\"5.4.\"},\"end\":29469,\"start\":29433},{\"attributes\":{\"n\":\"5.5.\"},\"end\":30050,\"start\":30040},{\"end\":31256,\"start\":31246},{\"end\":31415,\"start\":31405},{\"end\":31862,\"start\":31852},{\"end\":31931,\"start\":31921},{\"end\":32191,\"start\":32180},{\"end\":32757,\"start\":32747},{\"end\":32905,\"start\":32899},{\"end\":33408,\"start\":33398},{\"end\":33658,\"start\":33648}]", "table": "[{\"end\":34298,\"start\":33923}]", "figure_caption": "[{\"end\":31403,\"start\":31258},{\"end\":31850,\"start\":31417},{\"end\":31919,\"start\":31864},{\"end\":32178,\"start\":31933},{\"end\":32745,\"start\":32193},{\"end\":32897,\"start\":32759},{\"end\":33396,\"start\":32907},{\"end\":33646,\"start\":33410},{\"end\":33709,\"start\":33660},{\"end\":33923,\"start\":33712}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10765,\"start\":10759},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10989,\"start\":10983},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11138,\"start\":11132},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12311,\"start\":12305},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12585,\"start\":12579},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18719,\"start\":18713},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19237,\"start\":19230},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19639,\"start\":19633},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20198,\"start\":20192},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22163,\"start\":22157},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":27092,\"start\":27086},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27248,\"start\":27227},{\"end\":28318,\"start\":28312},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":28358,\"start\":28352},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28490,\"start\":28482},{\"end\":28812,\"start\":28804}]", "bib_author_first_name": "[{\"end\":34381,\"start\":34376},{\"end\":34393,\"start\":34390},{\"end\":34674,\"start\":34670},{\"end\":34690,\"start\":34682},{\"end\":34706,\"start\":34700},{\"end\":34718,\"start\":34714},{\"end\":34874,\"start\":34873},{\"end\":34881,\"start\":34880},{\"end\":34888,\"start\":34887},{\"end\":34897,\"start\":34896},{\"end\":34905,\"start\":34904},{\"end\":34911,\"start\":34910},{\"end\":35349,\"start\":35346},{\"end\":35360,\"start\":35355},{\"end\":35375,\"start\":35368},{\"end\":35386,\"start\":35381},{\"end\":35627,\"start\":35621},{\"end\":35646,\"start\":35640},{\"end\":35662,\"start\":35656},{\"end\":35679,\"start\":35678},{\"end\":35683,\"start\":35680},{\"end\":35690,\"start\":35689},{\"end\":35702,\"start\":35701},{\"end\":35704,\"start\":35703},{\"end\":35715,\"start\":35714},{\"end\":35724,\"start\":35723},{\"end\":36218,\"start\":36214},{\"end\":36226,\"start\":36223},{\"end\":36238,\"start\":36233},{\"end\":36690,\"start\":36684},{\"end\":36714,\"start\":36709},{\"end\":36732,\"start\":36726},{\"end\":36754,\"start\":36747},{\"end\":36770,\"start\":36766},{\"end\":36785,\"start\":36779},{\"end\":37397,\"start\":37391},{\"end\":37418,\"start\":37408},{\"end\":37431,\"start\":37427},{\"end\":37433,\"start\":37432},{\"end\":37446,\"start\":37442},{\"end\":37462,\"start\":37457},{\"end\":37925,\"start\":37921},{\"end\":37942,\"start\":37936},{\"end\":37953,\"start\":37949},{\"end\":38252,\"start\":38245},{\"end\":38264,\"start\":38257},{\"end\":38280,\"start\":38272},{\"end\":38290,\"start\":38286},{\"end\":38564,\"start\":38558},{\"end\":38577,\"start\":38573},{\"end\":38589,\"start\":38585},{\"end\":38826,\"start\":38822},{\"end\":38836,\"start\":38832},{\"end\":38852,\"start\":38847},{\"end\":38868,\"start\":38863},{\"end\":38881,\"start\":38876},{\"end\":38901,\"start\":38891},{\"end\":38925,\"start\":38920},{\"end\":38936,\"start\":38933},{\"end\":38950,\"start\":38944},{\"end\":39311,\"start\":39304},{\"end\":39324,\"start\":39316},{\"end\":39343,\"start\":39336},{\"end\":39874,\"start\":39871},{\"end\":39886,\"start\":39880},{\"end\":40269,\"start\":40263},{\"end\":40281,\"start\":40276},{\"end\":40300,\"start\":40290},{\"end\":40310,\"start\":40307},{\"end\":40327,\"start\":40320},{\"end\":40653,\"start\":40652},{\"end\":40665,\"start\":40662},{\"end\":40972,\"start\":40970},{\"end\":40983,\"start\":40977},{\"end\":40996,\"start\":40989},{\"end\":41010,\"start\":41004},{\"end\":41019,\"start\":41017},{\"end\":41031,\"start\":41025},{\"end\":41475,\"start\":41471},{\"end\":41488,\"start\":41480},{\"end\":41498,\"start\":41496},{\"end\":41512,\"start\":41504},{\"end\":41891,\"start\":41888},{\"end\":41898,\"start\":41896},{\"end\":41915,\"start\":41906},{\"end\":41926,\"start\":41923},{\"end\":41939,\"start\":41934},{\"end\":42211,\"start\":42206},{\"end\":42223,\"start\":42216},{\"end\":42233,\"start\":42228},{\"end\":42242,\"start\":42240},{\"end\":42253,\"start\":42248},{\"end\":42742,\"start\":42738},{\"end\":42755,\"start\":42748},{\"end\":42768,\"start\":42760},{\"end\":43028,\"start\":43025},{\"end\":43038,\"start\":43034},{\"end\":43054,\"start\":43050},{\"end\":43063,\"start\":43059},{\"end\":43463,\"start\":43455},{\"end\":43473,\"start\":43469},{\"end\":43483,\"start\":43479},{\"end\":43763,\"start\":43755},{\"end\":43777,\"start\":43771},{\"end\":43796,\"start\":43788},{\"end\":43812,\"start\":43804},{\"end\":43824,\"start\":43821},{\"end\":43841,\"start\":43834},{\"end\":43843,\"start\":43842},{\"end\":44154,\"start\":44147},{\"end\":44171,\"start\":44164},{\"end\":44189,\"start\":44179},{\"end\":44473,\"start\":44469},{\"end\":44485,\"start\":44482},{\"end\":44500,\"start\":44493},{\"end\":44518,\"start\":44511},{\"end\":44533,\"start\":44527},{\"end\":44547,\"start\":44540},{\"end\":44562,\"start\":44556},{\"end\":44573,\"start\":44568},{\"end\":44589,\"start\":44585},{\"end\":44602,\"start\":44598},{\"end\":44884,\"start\":44880},{\"end\":44899,\"start\":44896},{\"end\":44915,\"start\":44905},{\"end\":44924,\"start\":44920},{\"end\":45368,\"start\":45367},{\"end\":45382,\"start\":45376},{\"end\":45398,\"start\":45391},{\"end\":45415,\"start\":45409},{\"end\":45858,\"start\":45853},{\"end\":45872,\"start\":45865},{\"end\":45886,\"start\":45878},{\"end\":45899,\"start\":45893},{\"end\":45913,\"start\":45906},{\"end\":46228,\"start\":46221},{\"end\":46237,\"start\":46235},{\"end\":46249,\"start\":46244},{\"end\":46261,\"start\":46254},{\"end\":46269,\"start\":46266},{\"end\":46660,\"start\":46652},{\"end\":46680,\"start\":46674},{\"end\":46693,\"start\":46689},{\"end\":47185,\"start\":47180},{\"end\":47198,\"start\":47191},{\"end\":47211,\"start\":47206},{\"end\":47499,\"start\":47492},{\"end\":47514,\"start\":47507},{\"end\":47528,\"start\":47520},{\"end\":47541,\"start\":47535},{\"end\":47554,\"start\":47548},{\"end\":47567,\"start\":47560},{\"end\":48113,\"start\":48108},{\"end\":48127,\"start\":48121},{\"end\":48137,\"start\":48133},{\"end\":48152,\"start\":48145}]", "bib_author_last_name": "[{\"end\":34388,\"start\":34382},{\"end\":34401,\"start\":34394},{\"end\":34680,\"start\":34675},{\"end\":34698,\"start\":34691},{\"end\":34712,\"start\":34707},{\"end\":34725,\"start\":34719},{\"end\":34878,\"start\":34875},{\"end\":34885,\"start\":34882},{\"end\":34894,\"start\":34889},{\"end\":34902,\"start\":34898},{\"end\":34908,\"start\":34906},{\"end\":34917,\"start\":34912},{\"end\":35353,\"start\":35350},{\"end\":35366,\"start\":35361},{\"end\":35379,\"start\":35376},{\"end\":35393,\"start\":35387},{\"end\":35638,\"start\":35628},{\"end\":35654,\"start\":35647},{\"end\":35676,\"start\":35663},{\"end\":35687,\"start\":35684},{\"end\":35699,\"start\":35691},{\"end\":35712,\"start\":35705},{\"end\":35721,\"start\":35716},{\"end\":35732,\"start\":35725},{\"end\":36221,\"start\":36219},{\"end\":36231,\"start\":36227},{\"end\":36243,\"start\":36239},{\"end\":36707,\"start\":36691},{\"end\":36724,\"start\":36715},{\"end\":36745,\"start\":36733},{\"end\":36764,\"start\":36755},{\"end\":36777,\"start\":36771},{\"end\":36798,\"start\":36786},{\"end\":36805,\"start\":36800},{\"end\":37406,\"start\":37398},{\"end\":37425,\"start\":37419},{\"end\":37440,\"start\":37434},{\"end\":37455,\"start\":37447},{\"end\":37473,\"start\":37463},{\"end\":37934,\"start\":37926},{\"end\":37947,\"start\":37943},{\"end\":37962,\"start\":37954},{\"end\":38255,\"start\":38253},{\"end\":38270,\"start\":38265},{\"end\":38284,\"start\":38281},{\"end\":38294,\"start\":38291},{\"end\":38571,\"start\":38565},{\"end\":38583,\"start\":38578},{\"end\":38595,\"start\":38590},{\"end\":38830,\"start\":38827},{\"end\":38845,\"start\":38837},{\"end\":38861,\"start\":38853},{\"end\":38874,\"start\":38869},{\"end\":38889,\"start\":38882},{\"end\":38918,\"start\":38902},{\"end\":38931,\"start\":38926},{\"end\":38942,\"start\":38937},{\"end\":38955,\"start\":38951},{\"end\":39314,\"start\":39312},{\"end\":39334,\"start\":39325},{\"end\":39346,\"start\":39344},{\"end\":39878,\"start\":39875},{\"end\":39890,\"start\":39887},{\"end\":39898,\"start\":39892},{\"end\":40274,\"start\":40270},{\"end\":40288,\"start\":40282},{\"end\":40305,\"start\":40301},{\"end\":40318,\"start\":40311},{\"end\":40333,\"start\":40328},{\"end\":40660,\"start\":40654},{\"end\":40670,\"start\":40666},{\"end\":40679,\"start\":40672},{\"end\":40975,\"start\":40973},{\"end\":40987,\"start\":40984},{\"end\":41002,\"start\":40997},{\"end\":41015,\"start\":41011},{\"end\":41023,\"start\":41020},{\"end\":41034,\"start\":41032},{\"end\":41478,\"start\":41476},{\"end\":41494,\"start\":41489},{\"end\":41502,\"start\":41499},{\"end\":41515,\"start\":41513},{\"end\":41894,\"start\":41892},{\"end\":41904,\"start\":41899},{\"end\":41921,\"start\":41916},{\"end\":41932,\"start\":41927},{\"end\":41944,\"start\":41940},{\"end\":42214,\"start\":42212},{\"end\":42226,\"start\":42224},{\"end\":42238,\"start\":42234},{\"end\":42246,\"start\":42243},{\"end\":42257,\"start\":42254},{\"end\":42746,\"start\":42743},{\"end\":42758,\"start\":42756},{\"end\":42772,\"start\":42769},{\"end\":43032,\"start\":43029},{\"end\":43048,\"start\":43039},{\"end\":43057,\"start\":43055},{\"end\":43068,\"start\":43064},{\"end\":43467,\"start\":43464},{\"end\":43477,\"start\":43474},{\"end\":43488,\"start\":43484},{\"end\":43769,\"start\":43764},{\"end\":43786,\"start\":43778},{\"end\":43802,\"start\":43797},{\"end\":43819,\"start\":43813},{\"end\":43832,\"start\":43825},{\"end\":43853,\"start\":43844},{\"end\":44162,\"start\":44155},{\"end\":44177,\"start\":44172},{\"end\":44197,\"start\":44190},{\"end\":44480,\"start\":44474},{\"end\":44491,\"start\":44486},{\"end\":44509,\"start\":44501},{\"end\":44525,\"start\":44519},{\"end\":44538,\"start\":44534},{\"end\":44554,\"start\":44548},{\"end\":44566,\"start\":44563},{\"end\":44583,\"start\":44574},{\"end\":44596,\"start\":44590},{\"end\":44608,\"start\":44603},{\"end\":44894,\"start\":44885},{\"end\":44903,\"start\":44900},{\"end\":44918,\"start\":44916},{\"end\":44929,\"start\":44925},{\"end\":45365,\"start\":45351},{\"end\":45374,\"start\":45369},{\"end\":45389,\"start\":45383},{\"end\":45407,\"start\":45399},{\"end\":45422,\"start\":45416},{\"end\":45437,\"start\":45424},{\"end\":45863,\"start\":45859},{\"end\":45876,\"start\":45873},{\"end\":45891,\"start\":45887},{\"end\":45904,\"start\":45900},{\"end\":45917,\"start\":45914},{\"end\":46233,\"start\":46229},{\"end\":46242,\"start\":46238},{\"end\":46252,\"start\":46250},{\"end\":46264,\"start\":46262},{\"end\":46274,\"start\":46270},{\"end\":46672,\"start\":46661},{\"end\":46687,\"start\":46681},{\"end\":46703,\"start\":46694},{\"end\":47189,\"start\":47186},{\"end\":47204,\"start\":47199},{\"end\":47215,\"start\":47212},{\"end\":47505,\"start\":47500},{\"end\":47518,\"start\":47515},{\"end\":47533,\"start\":47529},{\"end\":47546,\"start\":47542},{\"end\":47558,\"start\":47555},{\"end\":47573,\"start\":47568},{\"end\":48119,\"start\":48114},{\"end\":48131,\"start\":48128},{\"end\":48143,\"start\":48138},{\"end\":48155,\"start\":48153}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":15483870},\"end\":34608,\"start\":34335},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":17682909},\"end\":34871,\"start\":34610},{\"attributes\":{\"id\":\"b2\"},\"end\":35275,\"start\":34873},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":16224674},\"end\":35539,\"start\":35277},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3016223},\"end\":36135,\"start\":35541},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":8040013},\"end\":36612,\"start\":36137},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1690180},\"end\":37340,\"start\":36614},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1945911},\"end\":37868,\"start\":37342},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4755450},\"end\":38197,\"start\":37870},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":206594692},\"end\":38556,\"start\":38199},{\"attributes\":{\"doi\":\"arXiv:1506.05163\",\"id\":\"b10\"},\"end\":38820,\"start\":38558},{\"attributes\":{\"doi\":\"arXiv:1705.06950\",\"id\":\"b11\"},\"end\":39302,\"start\":38822},{\"attributes\":{\"id\":\"b12\"},\"end\":39792,\"start\":39304},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":3243570},\"end\":40208,\"start\":39794},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":215716912},\"end\":40584,\"start\":40210},{\"attributes\":{\"doi\":\"arXiv:1609.02907\",\"id\":\"b15\"},\"end\":40860,\"start\":40586},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1709967},\"end\":41399,\"start\":40862},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":12354538},\"end\":41722,\"start\":41401},{\"attributes\":{\"id\":\"b18\"},\"end\":41827,\"start\":41724},{\"attributes\":{\"doi\":\"arXiv:1805.02556\",\"id\":\"b19\"},\"end\":42121,\"start\":41829},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3880365},\"end\":42654,\"start\":42123},{\"attributes\":{\"doi\":\"arXiv:1705.08106\",\"id\":\"b21\"},\"end\":42952,\"start\":42656},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2654595},\"end\":43376,\"start\":42954},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":43823483},\"end\":43679,\"start\":43378},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":301319},\"end\":44094,\"start\":43681},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1430801},\"end\":44429,\"start\":44096},{\"attributes\":{\"id\":\"b26\"},\"end\":44813,\"start\":44431},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":15928602},\"end\":45216,\"start\":44815},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1594725},\"end\":45756,\"start\":45218},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":14298099},\"end\":46140,\"start\":45758},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":52245597},\"end\":46570,\"start\":46142},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1732632},\"end\":47093,\"start\":46572},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":19167105},\"end\":47384,\"start\":47095},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":12699455},\"end\":48025,\"start\":47386},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":5035347},\"end\":48378,\"start\":48027}]", "bib_title": "[{\"end\":34374,\"start\":34335},{\"end\":34668,\"start\":34610},{\"end\":35344,\"start\":35277},{\"end\":35619,\"start\":35541},{\"end\":36212,\"start\":36137},{\"end\":36682,\"start\":36614},{\"end\":37389,\"start\":37342},{\"end\":37919,\"start\":37870},{\"end\":38243,\"start\":38199},{\"end\":39869,\"start\":39794},{\"end\":40261,\"start\":40210},{\"end\":40968,\"start\":40862},{\"end\":41469,\"start\":41401},{\"end\":42204,\"start\":42123},{\"end\":43023,\"start\":42954},{\"end\":43453,\"start\":43378},{\"end\":43753,\"start\":43681},{\"end\":44145,\"start\":44096},{\"end\":44878,\"start\":44815},{\"end\":45349,\"start\":45218},{\"end\":45851,\"start\":45758},{\"end\":46219,\"start\":46142},{\"end\":46650,\"start\":46572},{\"end\":47178,\"start\":47095},{\"end\":47490,\"start\":47386},{\"end\":48106,\"start\":48027}]", "bib_author": "[{\"end\":34390,\"start\":34376},{\"end\":34403,\"start\":34390},{\"end\":34682,\"start\":34670},{\"end\":34700,\"start\":34682},{\"end\":34714,\"start\":34700},{\"end\":34727,\"start\":34714},{\"end\":34880,\"start\":34873},{\"end\":34887,\"start\":34880},{\"end\":34896,\"start\":34887},{\"end\":34904,\"start\":34896},{\"end\":34910,\"start\":34904},{\"end\":34919,\"start\":34910},{\"end\":35355,\"start\":35346},{\"end\":35368,\"start\":35355},{\"end\":35381,\"start\":35368},{\"end\":35395,\"start\":35381},{\"end\":35640,\"start\":35621},{\"end\":35656,\"start\":35640},{\"end\":35678,\"start\":35656},{\"end\":35689,\"start\":35678},{\"end\":35701,\"start\":35689},{\"end\":35714,\"start\":35701},{\"end\":35723,\"start\":35714},{\"end\":35734,\"start\":35723},{\"end\":36223,\"start\":36214},{\"end\":36233,\"start\":36223},{\"end\":36245,\"start\":36233},{\"end\":36709,\"start\":36684},{\"end\":36726,\"start\":36709},{\"end\":36747,\"start\":36726},{\"end\":36766,\"start\":36747},{\"end\":36779,\"start\":36766},{\"end\":36800,\"start\":36779},{\"end\":36807,\"start\":36800},{\"end\":37408,\"start\":37391},{\"end\":37427,\"start\":37408},{\"end\":37442,\"start\":37427},{\"end\":37457,\"start\":37442},{\"end\":37475,\"start\":37457},{\"end\":37936,\"start\":37921},{\"end\":37949,\"start\":37936},{\"end\":37964,\"start\":37949},{\"end\":38257,\"start\":38245},{\"end\":38272,\"start\":38257},{\"end\":38286,\"start\":38272},{\"end\":38296,\"start\":38286},{\"end\":38573,\"start\":38558},{\"end\":38585,\"start\":38573},{\"end\":38597,\"start\":38585},{\"end\":38832,\"start\":38822},{\"end\":38847,\"start\":38832},{\"end\":38863,\"start\":38847},{\"end\":38876,\"start\":38863},{\"end\":38891,\"start\":38876},{\"end\":38920,\"start\":38891},{\"end\":38933,\"start\":38920},{\"end\":38944,\"start\":38933},{\"end\":38957,\"start\":38944},{\"end\":39316,\"start\":39304},{\"end\":39336,\"start\":39316},{\"end\":39348,\"start\":39336},{\"end\":39880,\"start\":39871},{\"end\":39892,\"start\":39880},{\"end\":39900,\"start\":39892},{\"end\":40276,\"start\":40263},{\"end\":40290,\"start\":40276},{\"end\":40307,\"start\":40290},{\"end\":40320,\"start\":40307},{\"end\":40335,\"start\":40320},{\"end\":40662,\"start\":40652},{\"end\":40672,\"start\":40662},{\"end\":40681,\"start\":40672},{\"end\":40977,\"start\":40970},{\"end\":40989,\"start\":40977},{\"end\":41004,\"start\":40989},{\"end\":41017,\"start\":41004},{\"end\":41025,\"start\":41017},{\"end\":41036,\"start\":41025},{\"end\":41480,\"start\":41471},{\"end\":41496,\"start\":41480},{\"end\":41504,\"start\":41496},{\"end\":41517,\"start\":41504},{\"end\":41896,\"start\":41888},{\"end\":41906,\"start\":41896},{\"end\":41923,\"start\":41906},{\"end\":41934,\"start\":41923},{\"end\":41946,\"start\":41934},{\"end\":42216,\"start\":42206},{\"end\":42228,\"start\":42216},{\"end\":42240,\"start\":42228},{\"end\":42248,\"start\":42240},{\"end\":42259,\"start\":42248},{\"end\":42748,\"start\":42738},{\"end\":42760,\"start\":42748},{\"end\":42774,\"start\":42760},{\"end\":43034,\"start\":43025},{\"end\":43050,\"start\":43034},{\"end\":43059,\"start\":43050},{\"end\":43070,\"start\":43059},{\"end\":43469,\"start\":43455},{\"end\":43479,\"start\":43469},{\"end\":43490,\"start\":43479},{\"end\":43771,\"start\":43755},{\"end\":43788,\"start\":43771},{\"end\":43804,\"start\":43788},{\"end\":43821,\"start\":43804},{\"end\":43834,\"start\":43821},{\"end\":43855,\"start\":43834},{\"end\":44164,\"start\":44147},{\"end\":44179,\"start\":44164},{\"end\":44199,\"start\":44179},{\"end\":44482,\"start\":44469},{\"end\":44493,\"start\":44482},{\"end\":44511,\"start\":44493},{\"end\":44527,\"start\":44511},{\"end\":44540,\"start\":44527},{\"end\":44556,\"start\":44540},{\"end\":44568,\"start\":44556},{\"end\":44585,\"start\":44568},{\"end\":44598,\"start\":44585},{\"end\":44610,\"start\":44598},{\"end\":44896,\"start\":44880},{\"end\":44905,\"start\":44896},{\"end\":44920,\"start\":44905},{\"end\":44931,\"start\":44920},{\"end\":45367,\"start\":45351},{\"end\":45376,\"start\":45367},{\"end\":45391,\"start\":45376},{\"end\":45409,\"start\":45391},{\"end\":45424,\"start\":45409},{\"end\":45439,\"start\":45424},{\"end\":45865,\"start\":45853},{\"end\":45878,\"start\":45865},{\"end\":45893,\"start\":45878},{\"end\":45906,\"start\":45893},{\"end\":45919,\"start\":45906},{\"end\":46235,\"start\":46221},{\"end\":46244,\"start\":46235},{\"end\":46254,\"start\":46244},{\"end\":46266,\"start\":46254},{\"end\":46276,\"start\":46266},{\"end\":46674,\"start\":46652},{\"end\":46689,\"start\":46674},{\"end\":46705,\"start\":46689},{\"end\":47191,\"start\":47180},{\"end\":47206,\"start\":47191},{\"end\":47217,\"start\":47206},{\"end\":47507,\"start\":47492},{\"end\":47520,\"start\":47507},{\"end\":47535,\"start\":47520},{\"end\":47548,\"start\":47535},{\"end\":47560,\"start\":47548},{\"end\":47575,\"start\":47560},{\"end\":48121,\"start\":48108},{\"end\":48133,\"start\":48121},{\"end\":48145,\"start\":48133},{\"end\":48157,\"start\":48145}]", "bib_venue": "[{\"end\":36386,\"start\":36324},{\"end\":37616,\"start\":37554},{\"end\":42400,\"start\":42338},{\"end\":43101,\"start\":43097},{\"end\":43871,\"start\":43867},{\"end\":46846,\"start\":46784},{\"end\":47716,\"start\":47654},{\"end\":34452,\"start\":34403},{\"end\":34731,\"start\":34727},{\"end\":35057,\"start\":34919},{\"end\":35399,\"start\":35395},{\"end\":35783,\"start\":35734},{\"end\":36322,\"start\":36245},{\"end\":36856,\"start\":36807},{\"end\":37552,\"start\":37475},{\"end\":38013,\"start\":37964},{\"end\":38365,\"start\":38296},{\"end\":38665,\"start\":38613},{\"end\":39037,\"start\":38973},{\"end\":39528,\"start\":39348},{\"end\":39949,\"start\":39900},{\"end\":40386,\"start\":40335},{\"end\":40650,\"start\":40586},{\"end\":41073,\"start\":41036},{\"end\":41547,\"start\":41517},{\"end\":41756,\"start\":41724},{\"end\":41886,\"start\":41829},{\"end\":42336,\"start\":42259},{\"end\":42736,\"start\":42656},{\"end\":43095,\"start\":43070},{\"end\":43509,\"start\":43490},{\"end\":43865,\"start\":43855},{\"end\":44243,\"start\":44199},{\"end\":44467,\"start\":44431},{\"end\":45000,\"start\":44931},{\"end\":45470,\"start\":45439},{\"end\":45923,\"start\":45919},{\"end\":46345,\"start\":46276},{\"end\":46782,\"start\":46705},{\"end\":47221,\"start\":47217},{\"end\":47652,\"start\":47575},{\"end\":48188,\"start\":48157}]"}}}, "year": 2023, "month": 12, "day": 17}