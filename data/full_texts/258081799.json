{"id": 258081799, "updated": "2023-04-19 13:38:00.224", "metadata": {"title": "SupportHDC: Hyperdimensional Computing with Scalable Hypervector Sparsity", "authors": "[{\"first\":\"Ali\",\"last\":\"Safa\",\"middle\":[]},{\"first\":\"Ilja\",\"last\":\"Ocket\",\"middle\":[]},{\"first\":\"Francky\",\"last\":\"Catthoor\",\"middle\":[]},{\"first\":\"Georges\",\"last\":\"Gielen\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 2023 Annual Neuro-Inspired Computational Elements Conference", "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Hyperdimensional Computing (HDC) is an emerging brain-inspired machine learning method that is recently gaining much attention for performing tasks such as pattern recognition and bio-signal classification with ultra-low energy and area overheads when implemented in hardware. HDC relies on the encoding of input signals into binary or few-bit Hypervectors (HVs) and performs low-complexity manipulations on HVs in order to classify the input signals. In this context, the sparsity of HVs directly impacts energy consumption, since the sparser the HVs, the more zero-valued computations can be skipped. This short paper introduces SupportHDC, a novel HDC design framework that can jointly optimize system accuracy and sparsity in an automated manner, in order to trade off classification performance and hardware implementation overheads. We illustrate the inner working of the framework on two bio-signal classification tasks: cancer detection and arrhythmia detection. We show that SupportHDC can reach a higher accuracy compared to the conventional splatter-code architectures used in many works, while enabling the system designer to choose the final design solution from the accuracy-sparsity trade-off curve produced by the framework. We release the source code for reproducing our experiments with the hope of being beneficial to future research.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nice/SafaOCG23", "doi": "10.1145/3584954.3584961"}}, "content": {"source": {"pdf_hash": "c22262a457a0485f11fa9bc386a4afe3dc127bfa", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "a8e9b3b8b3e6781625975092fd178e362040c9d6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c22262a457a0485f11fa9bc386a4afe3dc127bfa.txt", "contents": "\nSupportHDC: Hyperdimensional Computing with Scalable Hypervector Sparsity\nACMCopyright ACMApril 11-14, 2023. April 11-14, 2023\n\nAli Safa \nIlja Ocket\n\n\nAli Safa@imec \nIlja Ocket\n\n\nEsat Be Imec \nIlja Ocket\n\n\nLeuven Ku \nIlja Ocket\n\n\nLeuven \nIlja Ocket\n\n\nBelgium \nIlja Ocket\n\n\nSupportHDC: Hyperdimensional Computing with Scalable Hypervector Sparsity\n\nNeuro-Inspired Computational Elements Conference (NICE 2023)\nSan Antonio, TX, USA; New York, NY, USA, 6 pages. NICE 2023; San Antonio, TX, USAACMApril 11-14, 2023. April 11-14, 202310.1145/3584954.3584961Ilja.Ocket@imec.be imec Leuven, Belgium Francky Catthoor Francky.Catthoor@imec.be imec and ESAT KU Leuven Leuven, Belgium Georges Gielen Georges.Gielen@kuleuven.be imec and ESAT KU Leuven Leuven, Belgium. . . $15.00Hyperdimensional computingsparsity-aware computingultra- low-power computingautomated system design ACM Reference Format: Ali SafaIlja OcketFrancky Catthoorand Georges Gielen 2023 Support- HDC: Hyperdimensional Computing with Scalable Hypervector Sparsity\nHyperdimensional Computing (HDC) is an emerging brain-inspired machine learning method that is recently gaining much attention for performing tasks such as pattern recognition and bio-signal classification with ultra-low energy and area overheads when implemented in hardware. HDC relies on the encoding of input signals into binary or few-bit Hypervectors (HVs) and performs lowcomplexity manipulations on HVs in order to classify the input signals. In this context, the sparsity of HVs directly impacts energy consumption, since the sparser the HVs, the more zero-valued computations can be skipped. This short paper introduces Support-HDC, a novel HDC design framework that can jointly optimize system accuracy and sparsity in an automated manner, in order to trade off classification performance and hardware implementation overheads. We illustrate the inner working of the framework on two bio-signal classification tasks: cancer detection and arrhythmia detection. We show that SupportHDC can reach a higher accuracy compared to the conventional splatter-code architectures used in many works, while enabling the system designer to choose the final design solution from the accuracy-sparsity trade-off curve produced by the framework. We release the source code for reproducing our experiments with the hope of being beneficial to future research.\n\nINTRODUCTION\n\nIn recent years, Hyperdimensional Computing (HDC) is gaining much attention for performing machine learning tasks in extremeedge devices with ultra-low computational complexity and energy consumption [17]. HDC is a brain-inspired method, based on the observation that patterns of neural spikes observed in the brain can be understood as vectors of binary numbers in a high-dimensional space, often referred to as hypervectors (HVs) [12,14]. After encoding sensory signals into HVs, HDC systems go on to perform various low-complexity operations on these HVs (such as XOR-ing, summing and so on) in order to increase the linear separability of the sensory data and to perform data classification tasks robustly and efficiently [14]. Hence, HDC has been investigated in a number of ultra-low-power computing contexts [20], from emotion recognition [4] to EMG-based gesture recognition [18] and EEG seizure detection [2,3].\n\nNevertheless, achieving high HDC classification accuracy can still be a challenge, and many works [8][9][10]13] are investigating ways to enhance the training process of HDC systems compared to the baseline approach used in most works [1], where a few passes through the dataset are used to learn the prototype HVs of each class against which the similarity of incoming HVs are measured during inference [14,16]. On the other hand, the beneficial effect of HV sparsity has been studied as a way to lower even more the energy and area overheads of on-chip HDC implementations, by skipping the computations involving zero-valued elements [11].\n\nTherefore, the goal of this work is to provide an automated HDC design framework combining the best of both worlds by enabling system designers to trade off between system classification accuracy and system sparsity (and hence, hardware overheads), given a specific task that needs to be solved. Since bio-signal processing forms an important set of tasks for ultra-low-power computing applications [17,21], we illustrate our proposed SupportHDC framework on two bio-signal classification tasks: cancer detection [24] and arrhythmia detection [7].\n\nThe main contributions of this paper are the following:\n\n(1) We propose a new approach for the design of HDC systems using insights from Support Vector Machine (SVM) theory. (2) We show that our framework enables an automated scaling of the HDC accuracy and sparsity in order to reduce energy and area consumption in hardware. (3) For reproducible research purposes, we release our code as open-source at https://tinyurl.com/2fwfhynt\n\nThis paper is organized as follows. The SupportHDC framework is presented in Section 2. The automated circuit parameter tuning is described in Section 3. Experimental results are shown in Section 4. Conclusions are given in Section 5.\n\n\nSUPPORTHDC FRAMEWORK 2.1 SupportHDC encoder design\n\nMachine learning techniques such as HDC provide a non-linear mapping of the -dimensional input data vectors\u00afinto a highdimensional feature space (\u00af) and then, perform a classification of these encoded data samples, exploiting the increased data separability provided by the feature mapping [27].\n\nConventional HDC encoders are usually derived heuristically and provide (\u00af) through a random binary mapping into hypervectors (HV) of dimension [14,28]. In contrast, we adopt a topdown approach, by designing an HDC encoder starting from the popular Random Fourier Feature (RFF) mapping, extensively used in SVMs [22] (see Fig. 1). This choice is motivated by recent theoretical discussions linking HDC with both kernel machines and feature maps such as RFF [6,23,28]. The RFF mapping is given by:\nRFF (\u00af) = [cos (\u00af1\u00af+ 1 ), \u00b7 \u00b7 \u00b7 , cos (\u00af\u00af+ )](1)\nwhere denotes the transpose, \u223c [0,2 [ are random biases uniformly distributed between 0 and 2 ,\u00af\u223c (0, 1) are random weight vectors of dimension drawn from a standard Gaussian distribution, and is a multiplicative constant setting the nonlinearity degree of the RFF mapping (the higher , the more the non-linearity) [22].\n\nIn order to convert (1) into HVs, we threshold at a user-tunable constant 1 the cosine values in (1) to \u22121, 0, +1 as follows:\nB (\u00af) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 +1 if cos (\u00af\u00af+ ) > 1 0 if | cos (\u00af\u00af+ )| \u2264 1 \u22121 if cos (\u00af\u00af+ ) < 1(2)\nTo further remove the cosines from (2) to simplify the hardware costs, we remark that the threshold 1 can be directly applied on the argument of the cosines, under the condition that the arguments wrap around 2 or any another constant , resulting from the cyclic behavior of the cosine. Such cyclic behavior can be provided by computing the inner product\u00af\u00af+ using a -bit accumulator:\nAcc [ ] = Acc [ \u2212 1] + , for = 1, ...,(3)\nwhere = 1, ..., is the index for the \u210e element of the input vector . The accumulator will under-or overflow once the argument reaches either 0 or = 2 , providing the desired cyclic behavior. Finally, the biases can be taken into account as the initial value of the accumulator Acc [0] \u223c [0,2 \u22121[ , drawn uniformly between 0 and 2 \u2212 1. This is analogous to \u223c [0,2 [ in the original RFF case of (1). Using the cyclic bundling in (3), the thresholding in (2) becomes:\nB (\u00af) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 +1 if Acc [ ] > 2 + (2 /2) 0 if \u2212 2 + (2 /2) \u2264 Acc [ ] \u2264 2 + (2 /2) \u22121 if Acc [ ] < \u2212 2 + (2 /2) (4)\nwhere Acc [ ] is the result of the cyclic bundling (3) after the iteration steps, 2 is the quantization threshold and (2 /2) is used to account for the fact that the values in Acc [ ] are -bit unsigned integers vs. signed float in (2).\n\nAt this point, we have quantized the cosines in RFF (1) to HDClike HVs and we have removed the need for the explicit computation of the cosine in (2) through a cyclic bundling step (3). Next, we will binarize the computation of\u00af\u00af, so as to be compatible with the conventional HDC approach.\n\nIn order to project\u00afinto binary HVs, we adapt the standard approach used in HDC [10,14] and consider all entries in\u00afto be -bit integers. Then, we can define a look-up table (LUT) which associate to each of the = 1, ..., 2 entries in the LUT, a random binary HV\u00af\u2208 {\u22121, +1} of dimension (generated only at initialization time). Since the\u00afmust be used to estimate the inner product\u00af\u00afin (3), each random binary vector\u00afmust represent its associated value = 1, ..., 2 via its binary statistics. Therefore, the\u00afin the LUT are not identically distributed as in traditional HDC systems, but rather the probability of having a +1 in\u00afis governed by a Bernoulli process , with = 2 . Finally, the weights\u00afundergo a similar binarization, where all entries of\u00af, \u2200 are identically distributed and drawn from a Bernoulli distribution with = 0.5 to have the same probability of \u22121 and +1, as in classical HDC. This can be seen as a binary version of the original RFF weights in (1). Furthermore, since multiplication in (3) is equivalent to XOR-ing \u2295 when using binary numbers \u22121, +1 [14], the cyclic bundling (3) becomes:\nAcc [ ] = Acc [ \u2212 1] + ( , \u2295 , ) for = 1, ...,(5)\nwhere , denotes the \u210e element of the -dimensional HV\u0113 ncoding the value . In addition, (5) shows that the parameter, which sets the non-linearity of the RFF mapping in (1), is used to define the accumulation magnitude of the bundling process (5).\n\nAfter the execution of the XOR-ing and the cyclic bundling step (5), the final encoded HV is obtained via the thresolding (4). \n\n\nTraining approach\n\nGiven a training set of data samples\u00af, = 1, ..., together with their labels , our encoder circuit presented in Section 2.1 (see Fig. 1) is used to encode the training set into a set of -dimensional HVs (\u00af), = 1, ..., .\n\nThen, we must learn the -dimensional prototype HVs\u00afof each class = 1, ..., against which the similarity of an incoming test sample (\u00aft est ) will be measured. In order to do so, we remark that when using the inner product: * = arg max\u00af(\u00aft est ) (6) Figure 1: SupportHDC encoder a) The -dimensional -bit input vector\u00afis encoded via the randomly-initialized LUT into binary HVs of dimension . b) The encoded input undergoes element-wise XOR-ing \u2295 with the random binary weights c) Bundling is done using the -step cyclic accumulators (5) and the accumulation results is thresholded via (4) to get (\u00af).\n\nas a typical similarity measure used in HDC [15] (where * is the inferred class), the prototype HVs\u00afcan be seen as weight vectors, similar to the weights learned in SVMs. Therefore, we propose to train our HDC system using SVM theory. Since conventional SVM training requires the resolution of a Quadratic Programming (QP) problem [27], we opt for the least-square SVM variant (LS-SVM) [26]. Indeed, the use of LS-SVMs reduces the compute-expensive QP problem to the cheaper resolution of a linear system of equations [26], much more suited for the implementation of our HDC system in e.g., ubiquitous micro-controller hardware and low-power devices [5,25]. The LS-SVM seeks [26] (with the number of classes):\n, = arg min , 1 2\u00af\u00af+ \u2211\ufe01 =1 2 \u2200 = 1, ..., with , (\u00af(\u00af) + ) = 1 \u2212 , \u2200 = 1, ...,(7)\nwhere\u00afis the prototype HV of class , is the LS-SVM bias for class , is a hyper-parameter used for regularization against over-fitting (the smaller , the less the system over-fits), are the LS-SVM slack variables and , \u2208 {\u22121, +1} are the labels, with , = +1 if sample belongs to class and , = \u22121 otherwise [26].\n\nIntuitively, the first term in (7) seeks to minimize the energy of\u00afto reduce over-fitting, while the second term in (7) acts as a linear regression with target values \u22121, +1 given by the labels.\n\nFor each class , the problem in (7) can be solved via the resolution of the following linear system of equations [26]:\n0 \u03a9 + \u22121 \u00af = 0 1(8)\nwith the -dimensional label vector with entries noted , in (7), a -dimensional identity matrix,1 a -dimensional vector of ones, \u03a9 , = , , (\u00af) (\u00af) the \u00d7 kernel matrix, and a -dimensional vector used to obtain the LS-SVM weights (or HDC prototypes) as [26]:\n= \u2211\ufe01 =1 , ,(\u00af)\nThe system (8) is typically solved using low-complexity methods such as the conjugate gradient algorithm [5]. Then, we quantize the HDC prototypes\u00afto -bit signed integers to reduce computational overheads during inference [9]:\n\u2190 \u2212 \u230am ax (|\u00af|) 2 2 \u230b(10)\nOnce (7) is resolved via (8)-(10), the class * of an incoming test sample (\u00aft est ) is inferred using: * = arg max(\u00af(\u00aft est ) + )\n\nIn summary, our SupportHDC framework first encodes an incoming -bit input vector\u00aft est using the HDC encoder circuit in Fig. 1 (see Section 2.1). Then, the resulting -dimensional HV (\u00aft est ) is compared against the -bit quantized prototype HVs , \u2200 via (11) during inference to decide upon the class to which the input vector belongs.\n\nThe proposed SupportHDC framework deviates from conventional HDC encoding architectures through the introduction of hyper-parameters , , 2 , which respectively enable the tuning of the encoder non-linearity, the tuning of the SupportHDC training against over-fitting and the tuning of encoder sparsity. Indeed, the larger 2 , the more B (\u00af) and\u00af, \u2200 will contain zeros, reducing the hardware overheads (see Section 3).\n\n\nAUTOMATED HDC PARAMETER TUNING\n\nIn the discussion below, we consider the HV dimensionality , the input bit width , the accumulator bit width and the prototype quantization bit width fixed by design and depending on the hardware resource availability. We now seek to automatically tune the SupportHDC hyper-parameters , 2 and given a dataset of data samples and their associated labels D = {\u00af, }, = 1, ..., with the aim of maximizing both the classification accuracy A ( , 2 , ) and the implementation sparsity S( , 2 , ) (i.e., the average number of zero-elements in\u00af, \u2200 ). Indeed, maximizing the sparsity S( , 2 , ) enables a heavy pruning of the HDC encoder circuit, significantly reducing area utilization and power consumption (see Fig. 2) [11]. Centroid: Replace E by E and go to line 3.\nE \u2190 \u2212 1 \u22121 \u22121 =1 E 8: Reflection: E \u2190 \u2212 E + (E \u2212 E ) 9: if C (E 1 ) \u2264 C (E ) < C (E ) then\n\n11:\n\nend if 12: if C (E ) < C (E 1 ) then 13: Expansion: E \u2190 \u2212 E + (E \u2212 E ) 14: if C (E ) < C (E ) then 15: Replace E by E and go to line 3. if C (E ) < C (E ) then 21: Contraction: E \u2190 \u2212 E + (E \u2212 E ) 22: if C (E ) < C (E ) then 23: Replace E by E and go to line 3. Shrink: E \u2190 \u2212 E 1 + (E \u2212 E 1 ) for = 2, ..., 37: end for\n\nWe use the popular Nelder-Mead (NM) optimization method [19] for finding a set of hyper-parameters { * , * 2 , * } that jointly maximizes A and S via the cost:\nC ( , 2 , ) = \u2212(A ( , 2 , ) + S( , 2 , ))(12)\nwhere sets the balance between the accuracy and sparsity objectives. Using NM is an appropriate choice since it does not require an explicit knowledge of the functions A ( , 2 , ) and S( , 2 , ) (evaluating them for each choice of { , 2 , } is enough). We use the Figure 2: Effect of the HDC system sparsity on area and energy efficiency. The sparsity S( , 2 , ) corresponds to the number of zeros in\u00af. Since the zero entries in\u00afdo not play a role in the inner product\u00af(\u00af) = =1 (\u00af)\u00af, in (11), this sparsity can be exploited to prune the HDC encoder. This pruning is done by removing i) thresholds; ii) accumulators; iii) weights and iv) encoded input entries that correspond to the zero-valued entries in\u00af.\n\nconventional NM algorithm given in [19] with the cost to be maximized specified by (12). Our complete NM procedure is detailed in Algorithm 1.\n\nDuring the NM optimization procedure, A ( , 2 , ) and S( , 2 , ) in (12) are evaluated as follows: given the dataset D with data samples, we randomly split D into a training set and a test set (following a 80%-20% ratio). Then, the SupportHDC system is learned via (7)-(9) using the training set with the current hyperparameters under test E = { , 2, , }. At this point, we compute S( , 2, , ) as the average number of zeros in\u00af, \u2200 . Then, we evaluate the system accuracy on the test set. We repeat this process = 6 times with different random training-test splits and report the final accuracy A ( , 2, , ) and sparsity S( , 2, , ) as the average over the trials. As NM learning hyper-parameters in Algorithm 1, we choose = 0.5, = 1.2, = 0.5, = 0.5, tuned manually starting from the baseline values in [19]. Furthermore, we use a maximum number of NM iterations of max it = 350. The details behind the NM optimization algorithm are given in [19].\n\n\nEXPERIMENTAL RESULTS\n\nThe python code for running our experiments is provided in the Supplementary Material at https://tinyurl.com/2fwfhynt.\n\n\nWisconsin Breast Cancer (WBC) dataset\n\nAs first validation, we apply the SupportHDC framework of Section 3 to the popular Wisconsin Breast Cancer (WBC) dataset [24]. As HDC circuit parameters, we use = 1000 (HV dimensionality), = 8 (input bit width), = 8 (accumulator bit width) and = 4 (HDC prototype quantization bit width). Then, our SupportHDC framework automatically tunes the accumulator speed in (3), the encoding threshold 2 in (4) and the learning hyper-parameter in (8) for multiple accuracy-sparsity trade offs, by sweeping in (12) from \u22121 to 1 with 200 steps in between. Fig. 3 shows the different HDC circuit designs proposed by Sup-portHDC (blue crosses), trading off test accuracy as a function of HDC prototype sparsity (which translates to energy and area consumption when implemented in e.g., an FPGA). It can be seen in Fig. 3 that initially, the accuracy grows as sparsity grows as well, reaching a maximum accuracy of 94.2% near a sparsity of 55% (corresponding to = 0 in Eq. 12, where the NM procedure is solely seeking to maximize accuracy). Then, accuracy starts decreasing as we make grow in order to increase the sparsity. This shows that a moderated amount of sparsity is beneficial to the HDC system, while enforcing too much sparsity jeopardizes accuracy. \n\n\nCardiac Arrhythmia (CA) dataset\n\nAs a second demonstration, the automated SupportHDC framework is applied to the publicly available Cardiac Arrhythmia (CA) dataset [7]. Compared to WBC, the CA dataset is more challenging as it contains missing values due to sensor defaulting and distortion. We replace all missing values with zeros. All parameters are kept identical to the WBC dataset (Section 4.1) with the exception of = 5000. Fig. 4 shows the different HDC designs produced by our SupportHDC framework. Similar to Fig. 3, Fig. 4 shows that initially, the accuracy grows as sparsity grows as well, reaching a maximum accuracy of 74.4% near a sparsity of 30% (corresponding to = 0 in Eq. 12, where the NM procedure is solely seeking to maximize accuracy). Then, accuracy starts decreasing as we make grow in order to increase the sparsity. Again, this confirms that a moderated amount of sparsity is beneficial to the HDC system, while enforcing too much sparsity reduces system accuracy.\n\nFurthermore, Table 1 compares the top accuracy of the proposed system against the accuracy obtained using a conventional splatter-code HDC architecture [14], where our proposed cyclic accumulation (3) and thresholding (4) are not used. As baseline, we use the model of [16], publicly available at [1]. Table 1 shows that our proposed SupportHDC framework compares favorably with the baseline splatter-code HDC approach used in many works [14]. This clearly demonstrates the usefulness of augmenting the baseline HDC with cyclic accumulation (3) and thresholding (4). Finally, in contrast to previously proposed systems [2,10,14,16], SupportHDC provides the user with a control on the trade-off between accuracy and energy/area consumption via the sparsity scalability (see Fig. 2), enabling the system designer to chose the final design solution along the accuracy-sparsity trade-off curve (see Fig. 3 and 4).   Table 1: Top accuracy vs. baseline HDC architecture. Our method outperforms the baseline splatter-code approach [1] while enabling an automated scaling of sparsity, reducing hardware costs.\n\n\nCONCLUSION\n\nThis short paper has presented an automated HDC design framework that can jointly optimize system accuracy and sparsity in order to trade off classification performance and hardware implementation overheads. After describing the SupportHDC framework and the automated HDC parameter tuning approach, the method has been illustrated on two biosignal processing tasks, i.e. cancer detection and arrhythmia detection. It has been shown that Support-HDC provides the designer with control of the trade-off between HDC accuracy and sparsity, while outperforming the widely-used splatter-code HDC approach in terms of accuracy. Therefore, Sup-portHDC enables the system designer to chose a final design along the accuracy-sparsity trade-off curve produced by the framework. The source code for reproducing our experiments is released as Supplementary Material with the hope of being beneficial to future research.\n\n\nFig. 1 summarizes the steps executed by the proposed SupportHDC encoder circuit.\n\n\nCost C computed on dataset D = {\u00af, }, = 1, ..., via (12); size of NM set ; maximum iterations max it ; break threshold , NM parameters , , , . Output: Best result: * , * 2 , * , A ( * , * 2 , * ), S( * , * 2 , * ) 1: Randomly generate the set of tuples E = { , 2, , }, \u2200 with \u223c [0,2\u00d7 2 ] , 2 \u223c [0,2 \u22121 ] , \u223c [10 \u22124 ,1] and compute C (E ), \u2200 = 1, ..., . 2: for it = 1, ..., max it do 3:Order the tuples in E as C (E 1 ) \u2264 ... \u2264 C (E )4:    if variance of {C (E 1 ), ...,\n\nFigure 3 :\n3Accuracy-sparsity trade-off on the WBC dataset. Each blue cross denotes a possible HDC circuit design. The orange curve found via regression shows the accuracy-sparsity trend.\n\nFigure 4 :\n4Accuracy-sparsity trade-off on the CA dataset. Each blue cross denotes a possible HDC circuit design. The orange curve found via regression shows the accuracy-sparsity trend.\n\n\nReplace E by E and go to line 3.16: \n\nelse \n\n17: \n\n18: \n\nend if \n\n19: \n\nend if \n\n20: \n\n\n\n\nReplace E by E and go to line 3.24: \n\nelse \n\n25: \n\ngo to line 36. \n\n26: \n\nend if \n\n27: \n\nend if \n\n28: \n\nif C (E ) \u2265 C (E ) then \n\n29: \n\nContraction: E \u2190 \u2212 E + (E \u2212 E ) \n\n30: \n\nif C (E ) < C (E ) then \n\n31: \n\n32: \n\nelse \n\n33: \n\ngo to line 36. \n\n34: \n\nend if \n\n35: \n\nend if \n\n36: \n\n\n\n\nMethodWBC [%] CA [%] sparsity scale Base HDC (e.g.,[1])91.4 \n67.2 \nYES \nThis work \n94.2 \n74.4 \nNO \n\n\nACKNOWLEDGMENTSThe authors thank Lars Keuninckx for useful discussions. This research received funding from the Flemish Government under the \"Onderzoeksprogramma Artifici\u00eble Intelligentie (AI) Vlaanderen\" programme and the European Union's ECSEL Joint Undertaking under grant agreement n\u00b0826655 -project TEMPO.\nHDTest: Differential Fuzz Testing of Brain-Inspired Hyperdimensional Computing. D Ma, 2021 58th ACM/IEEE DAC. Publicly available python code implementing the conventional HDC enPublicly available python code implementing the conventional HDC en- coder used in D. Ma et al., \"HDTest: Differential Fuzz Testing of Brain- Inspired Hyperdimensional Computing,\" 2021 58th ACM/IEEE DAC https:// github.com/AikawaMafuyu/HDMNIST (accessed 19/10/2022). https://github. com/AikawaMafuyu/HDMNIST\n\nAn Ensemble of Hyperdimensional Classifiers: Hardware-Friendly Short-Latency Seizure Detection With Automatic iEEG Electrode Selection. Alessio Burrello, Simone Benatti, Kaspar Schindler, Luca Benini, Abbas Rahimi, 10.1109/JBHI.2020.3022211IEEE Journal of Biomedical and Health Informatics. 25Alessio Burrello, Simone Benatti, Kaspar Schindler, Luca Benini, and Abbas Rahimi. 2021. An Ensemble of Hyperdimensional Classifiers: Hardware-Friendly Short-Latency Seizure Detection With Automatic iEEG Electrode Selection. IEEE Journal of Biomedical and Health Informatics 25, 4 (2021), 935-946. https://doi. org/10.1109/JBHI.2020.3022211\n\nOneshot Learning for iEEG Seizure Detection Using End-to-end Binary Operations: Local Binary Patterns with Hyperdimensional Computing. Alessio Burrello, Kaspar Schindler, Luca Benini, Abbas Rahimi, 10.1109/BIOCAS.2018.8584751IEEE Biomedical Circuits and Systems Conference (BioCAS). 1-4. Alessio Burrello, Kaspar Schindler, Luca Benini, and Abbas Rahimi. 2018. One- shot Learning for iEEG Seizure Detection Using End-to-end Binary Operations: Local Binary Patterns with Hyperdimensional Computing. In 2018 IEEE Biomed- ical Circuits and Systems Conference (BioCAS). 1-4. https://doi.org/10.1109/ BIOCAS.2018.8584751\n\nHyperdimensional Computing-based Multimodality Emotion Recognition with Physiological Signals. En-Jui Chang, Abbas Rahimi, Luca Benini, An-Yeu Andy Wu, 10.1109/AICAS.2019.87716222019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS). En-Jui Chang, Abbas Rahimi, Luca Benini, and An-Yeu Andy Wu. 2019. Hyperdi- mensional Computing-based Multimodality Emotion Recognition with Physiolog- ical Signals. In 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS). 137-141. https://doi.org/10.1109/AICAS.2019.8771622\n\nAn improved conjugate gradient scheme to the solution of least squares SVM. Wei Chu, Chong Jin Ong, S S Keerthi, 10.1109/TNN.2004.841785IEEE Transactions on Neural Networks. 16Wei Chu, Chong Jin Ong, and S.S. Keerthi. 2005. An improved conjugate gradient scheme to the solution of least squares SVM. IEEE Transactions on Neural Networks 16, 2 (2005), 498-501. https://doi.org/10.1109/TNN.2004.841785\n\nComputing on Functions Using Randomized Vector Representations (in Brief). E , Paxon Frady, Denis Kleyko, Christopher J Kymn, Bruno A Olshausen, Friedrich T Sommer, 10.1145/3517343.3522597Neuro-Inspired Computational Elements Conference (Virtual Event, USA) (NICE 2022). New York, NY, USAAssociation for Computing MachineryE. Paxon Frady, Denis Kleyko, Christopher J. Kymn, Bruno A. Olshausen, and Friedrich T. Sommer. 2022. Computing on Functions Using Randomized Vector Representations (in Brief). In Neuro-Inspired Computational Elements Conference (Virtual Event, USA) (NICE 2022). Association for Computing Machinery, New York, NY, USA, 115-122. https://doi.org/10.1145/3517343.3522597\n\nA supervised machine learning algorithm for arrhythmia analysis. H A Guvenir, B Acar, G Demiroz, A Cekin, 10.1109/CIC.1997.647926Computers in Cardiology. H.A. Guvenir, B. Acar, G. Demiroz, and A. Cekin. 1997. A supervised machine learning algorithm for arrhythmia analysis. In Computers in Cardiology 1997. 433-436. https://doi.org/10.1109/CIC.1997.647926\n\nFL-HDC: Hyperdimensional Computing Design for the Application of Federated Learning. Cheng-Yen Hsieh, Yu-Chuan Chuang, An-Yeu Andy Wu, 10.1109/AICAS51828.2021.94585262021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS). 1-5. Cheng-Yen Hsieh, Yu-Chuan Chuang, and An-Yeu Andy Wu. 2021. FL-HDC: Hyperdimensional Computing Design for the Application of Federated Learning. In 2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS). 1-5. https://doi.org/10.1109/AICAS51828.2021.9458526\n\nQuantHD: A Quantization Framework for Hyperdimensional Computing. Mohsen Imani, Samuel Bosch, Sohum Datta, Sharadhi Ramakrishna, Sahand Salamat, Jan M Rabaey, Tajana Rosing, 10.1109/TCAD.2019.2954472IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. 39Mohsen Imani, Samuel Bosch, Sohum Datta, Sharadhi Ramakrishna, Sahand Salamat, Jan M. Rabaey, and Tajana Rosing. 2020. QuantHD: A Quantization Framework for Hyperdimensional Computing. IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems 39, 10 (2020), 2268-2278. https: //doi.org/10.1109/TCAD.2019.2954472\n\nAdaptHD: Adaptive Efficient Training for Brain-Inspired Hyperdimensional Computing. Mohsen Imani, Justin Morris, Samuel Bosch, Helen Shu, Giovanni De Micheli, Tajana Rosing, 10.1109/BIOCAS.2019.89189742019 IEEE Biomedical Circuits and Systems Conference (BioCAS). 1-4. Mohsen Imani, Justin Morris, Samuel Bosch, Helen Shu, Giovanni De Micheli, and Tajana Rosing. 2019. AdaptHD: Adaptive Efficient Training for Brain-Inspired Hyperdimensional Computing. In 2019 IEEE Biomedical Circuits and Systems Conference (BioCAS). 1-4. https://doi.org/10.1109/BIOCAS.2019.8918974\n\nSparseHD: Algorithm-Hardware Co-optimization for Efficient High-Dimensional Computing. Mohsen Imani, Sahand Salamat, Behnam Khaleghi, Mohammad Samragh, Farinaz Koushanfar, Tajana Rosing, 10.1109/FCCM.2019.00034IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM). Mohsen Imani, Sahand Salamat, Behnam Khaleghi, Mohammad Samragh, Fari- naz Koushanfar, and Tajana Rosing. 2019. SparseHD: Algorithm-Hardware Co-optimization for Efficient High-Dimensional Computing. In 2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Ma- chines (FCCM). 190-198. https://doi.org/10.1109/FCCM.2019.00034\n\nComputing with High-Dimensional Vectors. Pentti Kanerva, 10.1109/MDAT.2018.2890221IEEE Design & Test. 36Pentti Kanerva. 2019. Computing with High-Dimensional Vectors. IEEE Design & Test 36, 3 (2019), 7-14. https://doi.org/10.1109/MDAT.2018.2890221\n\nCascadeHD: Efficient Many-Class Learning Framework Using Hyperdimensional Computing. Yeseong Kim, Jiseung Kim, Mohsen Imani, 58Yeseong Kim, Jiseung Kim, and Mohsen Imani. 2021. CascadeHD: Efficient Many- Class Learning Framework Using Hyperdimensional Computing. In 2021 58th\n\n10.1109/DAC18074.2021.9586235ACM/IEEE Design Automation Conference (DAC). ACM/IEEE Design Automation Conference (DAC). 775-780. https://doi.org/10. 1109/DAC18074.2021.9586235\n\nA Survey on Hyperdimensional Computing Aka Vector Symbolic Architectures, Part I: Models and Data Transformations. Denis Kleyko, Dmitri A Rachkovskij, Evgeny Osipov, Abbas Rahimi, 10.1145/3538531ACM Comput. Surv. Denis Kleyko, Dmitri A. Rachkovskij, Evgeny Osipov, and Abbas Rahimi. 2022. A Survey on Hyperdimensional Computing Aka Vector Symbolic Architectures, Part I: Models and Data Transformations. ACM Comput. Surv. (may 2022). https: //doi.org/10.1145/3538531 Just Accepted.\n\nClassification and Recall With Binary Hyperdimensional Computing: Tradeoffs in Choice of Density and Mapping Characteristics. Denis Kleyko, Abbas Rahimi, Dmitri A Rachkovskij, Evgeny Osipov, Jan M Rabaey, 10.1109/TNNLS.2018.2814400IEEE Transactions on Neural Networks and Learning Systems. 29Denis Kleyko, Abbas Rahimi, Dmitri A. Rachkovskij, Evgeny Osipov, and Jan M. Rabaey. 2018. Classification and Recall With Binary Hyperdimensional Com- puting: Tradeoffs in Choice of Density and Mapping Characteristics. IEEE Transactions on Neural Networks and Learning Systems 29, 12 (2018), 5880-5898. https://doi.org/10.1109/TNNLS.2018.2814400\n\nHDTest: Differential Fuzz Testing of Brain-Inspired Hyperdimensional Computing. Dongning Ma, Jianmin Guo, Yu Jiang, Xun Jiao, 58Dongning Ma, Jianmin Guo, Yu Jiang, and Xun Jiao. 2021. HDTest: Differen- tial Fuzz Testing of Brain-Inspired Hyperdimensional Computing. In 2021 58th\n\n10.1109/DAC18074.2021.9586169ACM/IEEE Design Automation Conference (DAC). ACM/IEEE Design Automation Conference (DAC). 391-396. https://doi.org/10. 1109/DAC18074.2021.9586169\n\nA Highly Energy-Efficient Hyperdimensional Computing Processor for Biosignal Classification. Alisha Menon, Daniel Sun, Sarina Sabouri, Kyoungtae Lee, Melvin Aristio, Harrison Liew, Jan M Rabaey, 10.1109/TBCAS.2022.3187944IEEE Transactions on Biomedical Circuits and Systems. 16Alisha Menon, Daniel Sun, Sarina Sabouri, Kyoungtae Lee, Melvin Aristio, Harri- son Liew, and Jan M. Rabaey. 2022. A Highly Energy-Efficient Hyperdimensional Computing Processor for Biosignal Classification. IEEE Transactions on Biomedi- cal Circuits and Systems 16, 4 (2022), 524-534. https://doi.org/10.1109/TBCAS. 2022.3187944\n\nAnalysis of Contraction Effort Level in EMG-Based Gesture Recognition Using Hyperdimensional Computing. Ali Moin, Andy Zhou, Simone Benatti, Abbas Rahimi, Luca Benini, Jan M Rabaey, 10.1109/BIOCAS.2019.89192142019 IEEE Biomedical Circuits and Systems Conference (BioCAS). 1-4. Ali Moin, Andy Zhou, Simone Benatti, Abbas Rahimi, Luca Benini, and Jan M. Rabaey. 2019. Analysis of Contraction Effort Level in EMG-Based Gesture Recog- nition Using Hyperdimensional Computing. In 2019 IEEE Biomedical Circuits and Systems Conference (BioCAS). 1-4. https://doi.org/10.1109/BIOCAS.2019.8919214\n\nA Simplex Method for Function Minimization. J A Nelder, R Mead, 10.1093/comjnl/7.4.308Comput. J. 74J. A. Nelder and R. Mead. 1965. A Simplex Method for Function Minimiza- tion. Comput. J. 7, 4 (01 1965), 308-313. https://doi.org/10.1093/comjnl/7. 4.308 arXiv:https://academic.oup.com/comjnl/article-pdf/7/4/308/1013182/7-4- 308.pdf\n\nAlgorithm-Hardware Co-Design for Efficient Brain-Inspired Hyperdimensional Learning on Edge. Yang Ni, Yeseong Kim, Tajana Rosing, Mohsen Imani, 10.23919/DATE54114.2022.97745242022 Design, Automation & Test in Europe Conference & Exhibition (DATE). Yang Ni, Yeseong Kim, Tajana Rosing, and Mohsen Imani. 2022. Algorithm- Hardware Co-Design for Efficient Brain-Inspired Hyperdimensional Learning on Edge. In 2022 Design, Automation & Test in Europe Conference & Exhibition (DATE). 292-297. https://doi.org/10.23919/DATE54114.2022.9774524\n\nAttention State Classification with In-Ear EEG. Akshay Paul, Gopabandhu Hota, Behnam Khaleghi, Yuchen Xu, Tajana Rosing, Gert Cauwenberghs, 10.1109/BioCAS49922.2021.96449732021 IEEE Biomedical Circuits and Systems Conference (BioCAS). 1-5. Akshay Paul, Gopabandhu Hota, Behnam Khaleghi, Yuchen Xu, Tajana Rosing, and Gert Cauwenberghs. 2021. Attention State Classification with In-Ear EEG. In 2021 IEEE Biomedical Circuits and Systems Conference (BioCAS). 1-5. https: //doi.org/10.1109/BioCAS49922.2021.9644973\n\nRandom Features for Large-Scale Kernel Machines. Ali Rahimi, Benjamin Recht, Advances in Neural Information Processing Systems. J. Platt, D. Koller, Y. Singer, and S. RoweisCurran Associates, Inc20Ali Rahimi and Benjamin Recht. 2007. Random Features for Large-Scale Kernel Machines. In Advances in Neural Information Processing Systems, J. Platt, D. Koller, Y. Singer, and S. Roweis (Eds.), Vol. 20. Curran Associates, Inc. https://proceedings. neurips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf\n\nSparse Vector Binding on Spiking Neuromorphic Hardware Using Synaptic Delays. Alpha Renner, Yulia Sandamirskaya, Friedrich Sommer, E Paxon Frady, 10.1145/3546790.3546820Proceedings of the International Conference on Neuromorphic Systems 2022. the International Conference on Neuromorphic Systems 2022Knoxville, TN, USA; New York, NY, USA, ArticleAssociation for Computing Machinery27ICONS '22)Alpha Renner, Yulia Sandamirskaya, Friedrich Sommer, and E. Paxon Frady. 2022. Sparse Vector Binding on Spiking Neuromorphic Hardware Using Synaptic Delays. In Proceedings of the International Conference on Neuromorphic Systems 2022 (Knoxville, TN, USA) (ICONS '22). Association for Computing Machinery, New York, NY, USA, Article 27, 5 pages. https://doi.org/10.1145/3546790.3546820\n\nNuclear feature extraction for breast tumor diagnosis. W Nick Street, W H Wolberg, O L Mangasarian, 10.1117/12.148698Biomedical Image Processing and Biomedical Visualization. Raj S. Acharya and Dmitry B. GoldgofSPIEW. Nick Street, W. H. Wolberg, and O. L. Mangasarian. 1993. Nuclear fea- ture extraction for breast tumor diagnosis. In Biomedical Image Processing and Biomedical Visualization, Raj S. Acharya and Dmitry B. Goldgof (Eds.), Vol. 1905. International Society for Optics and Photonics, SPIE, 861 -870. https://doi.org/10.1117/12.148698\n\nNavion: A 2-mW Fully Integrated Real-Time Visual-Inertial Odometry Accelerator for Autonomous Navigation of Nano Drones. Amr Suleiman, Zhengdong Zhang, Luca Carlone, Sertac Karaman, Vivienne Sze, 10.1109/JSSC.2018.2886342IEEE Journal of Solid-State Circuits. 54Amr Suleiman, Zhengdong Zhang, Luca Carlone, Sertac Karaman, and Vivienne Sze. 2019. Navion: A 2-mW Fully Integrated Real-Time Visual-Inertial Odometry Accelerator for Autonomous Navigation of Nano Drones. IEEE Journal of Solid- State Circuits 54, 4 (2019), 1106-1119. https://doi.org/10.1109/JSSC.2018.2886342\n\nLeast Squares Support Vector Machines. A K Johan, Tony Suykens, Jos De Van Gestel, Bart De Brabanter, Joos Moor, Vandewalle, World ScientificJohan A. K. Suykens, Tony Van Gestel, Jos De Brabanter, Bart De Moor, and Joos Vandewalle. 2002. Least Squares Support Vector Machines. World Scientific. 1-308 pages.\n\nThe nature of statistical learning theory. Vladimir N Vapnik, Springer-VerlagNew York, IncVladimir N. Vapnik. 1995. The nature of statistical learning theory. Springer-Verlag New York, Inc.\n\nUnderstanding Hyperdimensional Computing for Parallel Single-Pass Learning. Tao Yu, Yichi Zhang, Zhiru Zhang, Christopher De Sa, arXiv:2202.04805cs.LGTao Yu, Yichi Zhang, Zhiru Zhang, and Christopher De Sa. 2022. Understand- ing Hyperdimensional Computing for Parallel Single-Pass Learning. (2022). arXiv:2202.04805 [cs.LG]\n", "annotations": {"author": "[{\"end\":151,\"start\":129},{\"end\":179,\"start\":152},{\"end\":206,\"start\":180},{\"end\":230,\"start\":207},{\"end\":251,\"start\":231},{\"end\":273,\"start\":252}]", "publisher": "[{\"end\":78,\"start\":75},{\"end\":494,\"start\":491}]", "author_last_name": "[{\"end\":137,\"start\":133},{\"end\":165,\"start\":156},{\"end\":192,\"start\":185},{\"end\":216,\"start\":214},{\"end\":237,\"start\":231},{\"end\":259,\"start\":252}]", "author_first_name": "[{\"end\":132,\"start\":129},{\"end\":155,\"start\":152},{\"end\":184,\"start\":180},{\"end\":213,\"start\":207}]", "author_affiliation": "[{\"end\":150,\"start\":139},{\"end\":178,\"start\":167},{\"end\":205,\"start\":194},{\"end\":229,\"start\":218},{\"end\":250,\"start\":239},{\"end\":272,\"start\":261}]", "title": "[{\"end\":74,\"start\":1},{\"end\":347,\"start\":274}]", "venue": "[{\"end\":409,\"start\":349}]", "abstract": "[{\"end\":2377,\"start\":1024}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2597,\"start\":2593},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2829,\"start\":2825},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2832,\"start\":2829},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3123,\"start\":3119},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3212,\"start\":3208},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3242,\"start\":3239},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3280,\"start\":3276},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3310,\"start\":3307},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3312,\"start\":3310},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3416,\"start\":3413},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3419,\"start\":3416},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3423,\"start\":3419},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3426,\"start\":3423},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3553,\"start\":3550},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3723,\"start\":3719},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3726,\"start\":3723},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3955,\"start\":3951},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4361,\"start\":4357},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4364,\"start\":4361},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4475,\"start\":4471},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4504,\"start\":4501},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4684,\"start\":4681},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5525,\"start\":5521},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5676,\"start\":5672},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5679,\"start\":5676},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5844,\"start\":5840},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5988,\"start\":5985},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5991,\"start\":5988},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5994,\"start\":5991},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6393,\"start\":6389},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7868,\"start\":7865},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8246,\"start\":8242},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8249,\"start\":8246},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9232,\"start\":9228},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9407,\"start\":9404},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9562,\"start\":9559},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10583,\"start\":10579},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10870,\"start\":10866},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10925,\"start\":10921},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11057,\"start\":11053},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11188,\"start\":11185},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11191,\"start\":11188},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11214,\"start\":11210},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11635,\"start\":11631},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11951,\"start\":11947},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12227,\"start\":12223},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12352,\"start\":12349},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12469,\"start\":12466},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14132,\"start\":14128},{\"end\":14284,\"start\":14281},{\"end\":14314,\"start\":14311},{\"end\":14348,\"start\":14345},{\"end\":14376,\"start\":14373},{\"end\":14437,\"start\":14434},{\"end\":14473,\"start\":14470},{\"end\":14501,\"start\":14498},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14653,\"start\":14649},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15546,\"start\":15542},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15594,\"start\":15590},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16458,\"start\":16454},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16597,\"start\":16593},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16908,\"start\":16904},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17223,\"start\":17220},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17286,\"start\":17282},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18199,\"start\":18196},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19181,\"start\":19177},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19298,\"start\":19294},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19325,\"start\":19322},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19467,\"start\":19463},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19590,\"start\":19587},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19647,\"start\":19644},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19650,\"start\":19647},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19653,\"start\":19650},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19656,\"start\":19653},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20052,\"start\":20049},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22409,\"start\":22406}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":21130,\"start\":21048},{\"attributes\":{\"id\":\"fig_1\"},\"end\":21602,\"start\":21131},{\"attributes\":{\"id\":\"fig_3\"},\"end\":21791,\"start\":21603},{\"attributes\":{\"id\":\"fig_4\"},\"end\":21979,\"start\":21792},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":22069,\"start\":21980},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":22352,\"start\":22070},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":22455,\"start\":22353}]", "paragraph": "[{\"end\":3313,\"start\":2393},{\"end\":3956,\"start\":3315},{\"end\":4505,\"start\":3958},{\"end\":4562,\"start\":4507},{\"end\":4940,\"start\":4564},{\"end\":5176,\"start\":4942},{\"end\":5526,\"start\":5231},{\"end\":6024,\"start\":5528},{\"end\":6394,\"start\":6074},{\"end\":6521,\"start\":6396},{\"end\":6999,\"start\":6616},{\"end\":7506,\"start\":7042},{\"end\":7869,\"start\":7634},{\"end\":8160,\"start\":7871},{\"end\":9266,\"start\":8162},{\"end\":9563,\"start\":9317},{\"end\":9692,\"start\":9565},{\"end\":9932,\"start\":9714},{\"end\":10533,\"start\":9934},{\"end\":11244,\"start\":10535},{\"end\":11636,\"start\":11326},{\"end\":11832,\"start\":11638},{\"end\":11952,\"start\":11834},{\"end\":12228,\"start\":11973},{\"end\":12470,\"start\":12244},{\"end\":12626,\"start\":12497},{\"end\":12962,\"start\":12628},{\"end\":13381,\"start\":12964},{\"end\":14176,\"start\":13416},{\"end\":14591,\"start\":14274},{\"end\":14752,\"start\":14593},{\"end\":15505,\"start\":14799},{\"end\":15649,\"start\":15507},{\"end\":16598,\"start\":15651},{\"end\":16741,\"start\":16623},{\"end\":18029,\"start\":16783},{\"end\":19023,\"start\":18065},{\"end\":20126,\"start\":19025},{\"end\":21047,\"start\":20141}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6073,\"start\":6025},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6615,\"start\":6522},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7041,\"start\":7000},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7633,\"start\":7507},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9316,\"start\":9267},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11325,\"start\":11245},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11972,\"start\":11953},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12243,\"start\":12229},{\"attributes\":{\"id\":\"formula_9\"},\"end\":12496,\"start\":12471},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14267,\"start\":14177},{\"attributes\":{\"id\":\"formula_12\"},\"end\":14798,\"start\":14753}]", "table_ref": "[{\"end\":19045,\"start\":19038},{\"end\":19334,\"start\":19327},{\"end\":19944,\"start\":19937}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2391,\"start\":2379},{\"attributes\":{\"n\":\"2\"},\"end\":5229,\"start\":5179},{\"attributes\":{\"n\":\"2.2\"},\"end\":9712,\"start\":9695},{\"attributes\":{\"n\":\"3\"},\"end\":13414,\"start\":13384},{\"end\":14272,\"start\":14269},{\"attributes\":{\"n\":\"4\"},\"end\":16621,\"start\":16601},{\"attributes\":{\"n\":\"4.1\"},\"end\":16781,\"start\":16744},{\"attributes\":{\"n\":\"4.2\"},\"end\":18063,\"start\":18032},{\"attributes\":{\"n\":\"5\"},\"end\":20139,\"start\":20129},{\"end\":21614,\"start\":21604},{\"end\":21803,\"start\":21793}]", "table": "[{\"end\":22069,\"start\":22014},{\"end\":22352,\"start\":22104},{\"end\":22455,\"start\":22410}]", "figure_caption": "[{\"end\":21130,\"start\":21050},{\"end\":21602,\"start\":21133},{\"end\":21791,\"start\":21616},{\"end\":21979,\"start\":21805},{\"end\":22014,\"start\":21982},{\"end\":22104,\"start\":22072},{\"end\":22410,\"start\":22355}]", "figure_ref": "[{\"end\":5856,\"start\":5850},{\"end\":9848,\"start\":9842},{\"end\":10191,\"start\":10183},{\"end\":12754,\"start\":12748},{\"end\":14126,\"start\":14120},{\"end\":15071,\"start\":15063},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17333,\"start\":17327},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17589,\"start\":17583},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":18469,\"start\":18463},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18565,\"start\":18551},{\"end\":19804,\"start\":19798},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19933,\"start\":19920}]", "bib_author_first_name": "[{\"end\":22848,\"start\":22847},{\"end\":23396,\"start\":23389},{\"end\":23413,\"start\":23407},{\"end\":23429,\"start\":23423},{\"end\":23445,\"start\":23441},{\"end\":23459,\"start\":23454},{\"end\":24030,\"start\":24023},{\"end\":24047,\"start\":24041},{\"end\":24063,\"start\":24059},{\"end\":24077,\"start\":24072},{\"end\":24606,\"start\":24600},{\"end\":24619,\"start\":24614},{\"end\":24632,\"start\":24628},{\"end\":24652,\"start\":24641},{\"end\":25168,\"start\":25165},{\"end\":25179,\"start\":25174},{\"end\":25183,\"start\":25180},{\"end\":25190,\"start\":25189},{\"end\":25192,\"start\":25191},{\"end\":25566,\"start\":25565},{\"end\":25574,\"start\":25569},{\"end\":25587,\"start\":25582},{\"end\":25607,\"start\":25596},{\"end\":25609,\"start\":25608},{\"end\":25621,\"start\":25616},{\"end\":25623,\"start\":25622},{\"end\":25644,\"start\":25635},{\"end\":25646,\"start\":25645},{\"end\":26248,\"start\":26247},{\"end\":26250,\"start\":26249},{\"end\":26261,\"start\":26260},{\"end\":26269,\"start\":26268},{\"end\":26280,\"start\":26279},{\"end\":26633,\"start\":26624},{\"end\":26649,\"start\":26641},{\"end\":26669,\"start\":26658},{\"end\":27176,\"start\":27170},{\"end\":27190,\"start\":27184},{\"end\":27203,\"start\":27198},{\"end\":27219,\"start\":27211},{\"end\":27239,\"start\":27233},{\"end\":27252,\"start\":27249},{\"end\":27254,\"start\":27253},{\"end\":27269,\"start\":27263},{\"end\":27808,\"start\":27802},{\"end\":27822,\"start\":27816},{\"end\":27837,\"start\":27831},{\"end\":27850,\"start\":27845},{\"end\":27864,\"start\":27856},{\"end\":27867,\"start\":27865},{\"end\":27883,\"start\":27877},{\"end\":28380,\"start\":28374},{\"end\":28394,\"start\":28388},{\"end\":28410,\"start\":28404},{\"end\":28429,\"start\":28421},{\"end\":28446,\"start\":28439},{\"end\":28465,\"start\":28459},{\"end\":28997,\"start\":28991},{\"end\":29291,\"start\":29284},{\"end\":29304,\"start\":29297},{\"end\":29316,\"start\":29310},{\"end\":29772,\"start\":29767},{\"end\":29787,\"start\":29781},{\"end\":29789,\"start\":29788},{\"end\":29809,\"start\":29803},{\"end\":29823,\"start\":29818},{\"end\":30266,\"start\":30261},{\"end\":30280,\"start\":30275},{\"end\":30295,\"start\":30289},{\"end\":30297,\"start\":30296},{\"end\":30317,\"start\":30311},{\"end\":30329,\"start\":30326},{\"end\":30331,\"start\":30330},{\"end\":30862,\"start\":30854},{\"end\":30874,\"start\":30867},{\"end\":30882,\"start\":30880},{\"end\":30893,\"start\":30890},{\"end\":31329,\"start\":31323},{\"end\":31343,\"start\":31337},{\"end\":31355,\"start\":31349},{\"end\":31374,\"start\":31365},{\"end\":31386,\"start\":31380},{\"end\":31404,\"start\":31396},{\"end\":31414,\"start\":31411},{\"end\":31416,\"start\":31415},{\"end\":31945,\"start\":31942},{\"end\":31956,\"start\":31952},{\"end\":31969,\"start\":31963},{\"end\":31984,\"start\":31979},{\"end\":31997,\"start\":31993},{\"end\":32009,\"start\":32006},{\"end\":32011,\"start\":32010},{\"end\":32471,\"start\":32470},{\"end\":32473,\"start\":32472},{\"end\":32483,\"start\":32482},{\"end\":32856,\"start\":32852},{\"end\":32868,\"start\":32861},{\"end\":32880,\"start\":32874},{\"end\":32895,\"start\":32889},{\"end\":33350,\"start\":33344},{\"end\":33367,\"start\":33357},{\"end\":33380,\"start\":33374},{\"end\":33397,\"start\":33391},{\"end\":33408,\"start\":33402},{\"end\":33421,\"start\":33417},{\"end\":33860,\"start\":33857},{\"end\":33877,\"start\":33869},{\"end\":34407,\"start\":34402},{\"end\":34421,\"start\":34416},{\"end\":34446,\"start\":34437},{\"end\":34456,\"start\":34455},{\"end\":35158,\"start\":35157},{\"end\":35163,\"start\":35159},{\"end\":35173,\"start\":35172},{\"end\":35175,\"start\":35174},{\"end\":35186,\"start\":35185},{\"end\":35188,\"start\":35187},{\"end\":35774,\"start\":35771},{\"end\":35794,\"start\":35785},{\"end\":35806,\"start\":35802},{\"end\":35822,\"start\":35816},{\"end\":35840,\"start\":35832},{\"end\":36263,\"start\":36262},{\"end\":36265,\"start\":36264},{\"end\":36277,\"start\":36273},{\"end\":36290,\"start\":36287},{\"end\":36293,\"start\":36291},{\"end\":36310,\"start\":36306},{\"end\":36313,\"start\":36311},{\"end\":36329,\"start\":36325},{\"end\":36583,\"start\":36575},{\"end\":36585,\"start\":36584},{\"end\":36802,\"start\":36799},{\"end\":36812,\"start\":36807},{\"end\":36825,\"start\":36820},{\"end\":36847,\"start\":36833}]", "bib_author_last_name": "[{\"end\":22851,\"start\":22849},{\"end\":23405,\"start\":23397},{\"end\":23421,\"start\":23414},{\"end\":23439,\"start\":23430},{\"end\":23452,\"start\":23446},{\"end\":23466,\"start\":23460},{\"end\":24039,\"start\":24031},{\"end\":24057,\"start\":24048},{\"end\":24070,\"start\":24064},{\"end\":24084,\"start\":24078},{\"end\":24612,\"start\":24607},{\"end\":24626,\"start\":24620},{\"end\":24639,\"start\":24633},{\"end\":24655,\"start\":24653},{\"end\":25172,\"start\":25169},{\"end\":25187,\"start\":25184},{\"end\":25200,\"start\":25193},{\"end\":25580,\"start\":25575},{\"end\":25594,\"start\":25588},{\"end\":25614,\"start\":25610},{\"end\":25633,\"start\":25624},{\"end\":25653,\"start\":25647},{\"end\":26258,\"start\":26251},{\"end\":26266,\"start\":26262},{\"end\":26277,\"start\":26270},{\"end\":26286,\"start\":26281},{\"end\":26639,\"start\":26634},{\"end\":26656,\"start\":26650},{\"end\":26672,\"start\":26670},{\"end\":27182,\"start\":27177},{\"end\":27196,\"start\":27191},{\"end\":27209,\"start\":27204},{\"end\":27231,\"start\":27220},{\"end\":27247,\"start\":27240},{\"end\":27261,\"start\":27255},{\"end\":27276,\"start\":27270},{\"end\":27814,\"start\":27809},{\"end\":27829,\"start\":27823},{\"end\":27843,\"start\":27838},{\"end\":27854,\"start\":27851},{\"end\":27875,\"start\":27868},{\"end\":27890,\"start\":27884},{\"end\":28386,\"start\":28381},{\"end\":28402,\"start\":28395},{\"end\":28419,\"start\":28411},{\"end\":28437,\"start\":28430},{\"end\":28457,\"start\":28447},{\"end\":28472,\"start\":28466},{\"end\":29005,\"start\":28998},{\"end\":29295,\"start\":29292},{\"end\":29308,\"start\":29305},{\"end\":29322,\"start\":29317},{\"end\":29779,\"start\":29773},{\"end\":29801,\"start\":29790},{\"end\":29816,\"start\":29810},{\"end\":29830,\"start\":29824},{\"end\":30273,\"start\":30267},{\"end\":30287,\"start\":30281},{\"end\":30309,\"start\":30298},{\"end\":30324,\"start\":30318},{\"end\":30338,\"start\":30332},{\"end\":30865,\"start\":30863},{\"end\":30878,\"start\":30875},{\"end\":30888,\"start\":30883},{\"end\":30898,\"start\":30894},{\"end\":31335,\"start\":31330},{\"end\":31347,\"start\":31344},{\"end\":31363,\"start\":31356},{\"end\":31378,\"start\":31375},{\"end\":31394,\"start\":31387},{\"end\":31409,\"start\":31405},{\"end\":31423,\"start\":31417},{\"end\":31950,\"start\":31946},{\"end\":31961,\"start\":31957},{\"end\":31977,\"start\":31970},{\"end\":31991,\"start\":31985},{\"end\":32004,\"start\":31998},{\"end\":32018,\"start\":32012},{\"end\":32480,\"start\":32474},{\"end\":32488,\"start\":32484},{\"end\":32859,\"start\":32857},{\"end\":32872,\"start\":32869},{\"end\":32887,\"start\":32881},{\"end\":32901,\"start\":32896},{\"end\":33355,\"start\":33351},{\"end\":33372,\"start\":33368},{\"end\":33389,\"start\":33381},{\"end\":33400,\"start\":33398},{\"end\":33415,\"start\":33409},{\"end\":33434,\"start\":33422},{\"end\":33867,\"start\":33861},{\"end\":33883,\"start\":33878},{\"end\":34414,\"start\":34408},{\"end\":34435,\"start\":34422},{\"end\":34453,\"start\":34447},{\"end\":34468,\"start\":34457},{\"end\":35170,\"start\":35164},{\"end\":35183,\"start\":35176},{\"end\":35200,\"start\":35189},{\"end\":35783,\"start\":35775},{\"end\":35800,\"start\":35795},{\"end\":35814,\"start\":35807},{\"end\":35830,\"start\":35823},{\"end\":35844,\"start\":35841},{\"end\":36271,\"start\":36266},{\"end\":36285,\"start\":36278},{\"end\":36304,\"start\":36294},{\"end\":36323,\"start\":36314},{\"end\":36334,\"start\":36330},{\"end\":36346,\"start\":36336},{\"end\":36592,\"start\":36586},{\"end\":36805,\"start\":36803},{\"end\":36818,\"start\":36813},{\"end\":36831,\"start\":36826},{\"end\":36850,\"start\":36848}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":232240664},\"end\":23251,\"start\":22767},{\"attributes\":{\"doi\":\"10.1109/JBHI.2020.3022211\",\"id\":\"b1\",\"matched_paper_id\":221540863},\"end\":23886,\"start\":23253},{\"attributes\":{\"doi\":\"10.1109/BIOCAS.2018.8584751\",\"id\":\"b2\",\"matched_paper_id\":52168429},\"end\":24503,\"start\":23888},{\"attributes\":{\"doi\":\"10.1109/AICAS.2019.8771622\",\"id\":\"b3\",\"matched_paper_id\":155774532},\"end\":25087,\"start\":24505},{\"attributes\":{\"doi\":\"10.1109/TNN.2004.841785\",\"id\":\"b4\",\"matched_paper_id\":11138402},\"end\":25488,\"start\":25089},{\"attributes\":{\"doi\":\"10.1145/3517343.3522597\",\"id\":\"b5\",\"matched_paper_id\":248507508},\"end\":26180,\"start\":25490},{\"attributes\":{\"doi\":\"10.1109/CIC.1997.647926\",\"id\":\"b6\",\"matched_paper_id\":14378101},\"end\":26537,\"start\":26182},{\"attributes\":{\"doi\":\"10.1109/AICAS51828.2021.9458526\",\"id\":\"b7\",\"matched_paper_id\":235617348},\"end\":27102,\"start\":26539},{\"attributes\":{\"doi\":\"10.1109/TCAD.2019.2954472\",\"id\":\"b8\",\"matched_paper_id\":211016154},\"end\":27716,\"start\":27104},{\"attributes\":{\"doi\":\"10.1109/BIOCAS.2019.8918974\",\"id\":\"b9\",\"matched_paper_id\":203651142},\"end\":28285,\"start\":27718},{\"attributes\":{\"doi\":\"10.1109/FCCM.2019.00034\",\"id\":\"b10\",\"matched_paper_id\":189824904},\"end\":28948,\"start\":28287},{\"attributes\":{\"doi\":\"10.1109/MDAT.2018.2890221\",\"id\":\"b11\",\"matched_paper_id\":69346800},\"end\":29197,\"start\":28950},{\"attributes\":{\"id\":\"b12\"},\"end\":29474,\"start\":29199},{\"attributes\":{\"doi\":\"10.1109/DAC18074.2021.9586235\",\"id\":\"b13\"},\"end\":29650,\"start\":29476},{\"attributes\":{\"doi\":\"10.1145/3538531\",\"id\":\"b14\",\"matched_paper_id\":243985859},\"end\":30133,\"start\":29652},{\"attributes\":{\"doi\":\"10.1109/TNNLS.2018.2814400\",\"id\":\"b15\",\"matched_paper_id\":51614496},\"end\":30772,\"start\":30135},{\"attributes\":{\"id\":\"b16\"},\"end\":31052,\"start\":30774},{\"attributes\":{\"doi\":\"10.1109/DAC18074.2021.9586169\",\"id\":\"b17\"},\"end\":31228,\"start\":31054},{\"attributes\":{\"doi\":\"10.1109/TBCAS.2022.3187944\",\"id\":\"b18\",\"matched_paper_id\":250218126},\"end\":31836,\"start\":31230},{\"attributes\":{\"doi\":\"10.1109/BIOCAS.2019.8919214\",\"id\":\"b19\",\"matched_paper_id\":202538018},\"end\":32424,\"start\":31838},{\"attributes\":{\"doi\":\"10.1093/comjnl/7.4.308\",\"id\":\"b20\",\"matched_paper_id\":2208295},\"end\":32757,\"start\":32426},{\"attributes\":{\"doi\":\"10.23919/DATE54114.2022.9774524\",\"id\":\"b21\",\"matched_paper_id\":248923006},\"end\":33294,\"start\":32759},{\"attributes\":{\"doi\":\"10.1109/BioCAS49922.2021.9644973\",\"id\":\"b22\",\"matched_paper_id\":245446230},\"end\":33806,\"start\":33296},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":877929},\"end\":34322,\"start\":33808},{\"attributes\":{\"doi\":\"10.1145/3546790.3546820\",\"id\":\"b24\",\"matched_paper_id\":252112200},\"end\":35100,\"start\":34324},{\"attributes\":{\"doi\":\"10.1117/12.148698\",\"id\":\"b25\",\"matched_paper_id\":14922543},\"end\":35648,\"start\":35102},{\"attributes\":{\"doi\":\"10.1109/JSSC.2018.2886342\",\"id\":\"b26\",\"matched_paper_id\":52288121},\"end\":36221,\"start\":35650},{\"attributes\":{\"id\":\"b27\"},\"end\":36530,\"start\":36223},{\"attributes\":{\"id\":\"b28\"},\"end\":36721,\"start\":36532},{\"attributes\":{\"doi\":\"arXiv:2202.04805\",\"id\":\"b29\"},\"end\":37046,\"start\":36723}]", "bib_title": "[{\"end\":22845,\"start\":22767},{\"end\":23387,\"start\":23253},{\"end\":24021,\"start\":23888},{\"end\":24598,\"start\":24505},{\"end\":25163,\"start\":25089},{\"end\":25563,\"start\":25490},{\"end\":26245,\"start\":26182},{\"end\":26622,\"start\":26539},{\"end\":27168,\"start\":27104},{\"end\":27800,\"start\":27718},{\"end\":28372,\"start\":28287},{\"end\":28989,\"start\":28950},{\"end\":29765,\"start\":29652},{\"end\":30259,\"start\":30135},{\"end\":31321,\"start\":31230},{\"end\":31940,\"start\":31838},{\"end\":32468,\"start\":32426},{\"end\":32850,\"start\":32759},{\"end\":33342,\"start\":33296},{\"end\":33855,\"start\":33808},{\"end\":34400,\"start\":34324},{\"end\":35155,\"start\":35102},{\"end\":35769,\"start\":35650}]", "bib_author": "[{\"end\":22853,\"start\":22847},{\"end\":23407,\"start\":23389},{\"end\":23423,\"start\":23407},{\"end\":23441,\"start\":23423},{\"end\":23454,\"start\":23441},{\"end\":23468,\"start\":23454},{\"end\":24041,\"start\":24023},{\"end\":24059,\"start\":24041},{\"end\":24072,\"start\":24059},{\"end\":24086,\"start\":24072},{\"end\":24614,\"start\":24600},{\"end\":24628,\"start\":24614},{\"end\":24641,\"start\":24628},{\"end\":24657,\"start\":24641},{\"end\":25174,\"start\":25165},{\"end\":25189,\"start\":25174},{\"end\":25202,\"start\":25189},{\"end\":25569,\"start\":25565},{\"end\":25582,\"start\":25569},{\"end\":25596,\"start\":25582},{\"end\":25616,\"start\":25596},{\"end\":25635,\"start\":25616},{\"end\":25655,\"start\":25635},{\"end\":26260,\"start\":26247},{\"end\":26268,\"start\":26260},{\"end\":26279,\"start\":26268},{\"end\":26288,\"start\":26279},{\"end\":26641,\"start\":26624},{\"end\":26658,\"start\":26641},{\"end\":26674,\"start\":26658},{\"end\":27184,\"start\":27170},{\"end\":27198,\"start\":27184},{\"end\":27211,\"start\":27198},{\"end\":27233,\"start\":27211},{\"end\":27249,\"start\":27233},{\"end\":27263,\"start\":27249},{\"end\":27278,\"start\":27263},{\"end\":27816,\"start\":27802},{\"end\":27831,\"start\":27816},{\"end\":27845,\"start\":27831},{\"end\":27856,\"start\":27845},{\"end\":27877,\"start\":27856},{\"end\":27892,\"start\":27877},{\"end\":28388,\"start\":28374},{\"end\":28404,\"start\":28388},{\"end\":28421,\"start\":28404},{\"end\":28439,\"start\":28421},{\"end\":28459,\"start\":28439},{\"end\":28474,\"start\":28459},{\"end\":29007,\"start\":28991},{\"end\":29297,\"start\":29284},{\"end\":29310,\"start\":29297},{\"end\":29324,\"start\":29310},{\"end\":29781,\"start\":29767},{\"end\":29803,\"start\":29781},{\"end\":29818,\"start\":29803},{\"end\":29832,\"start\":29818},{\"end\":30275,\"start\":30261},{\"end\":30289,\"start\":30275},{\"end\":30311,\"start\":30289},{\"end\":30326,\"start\":30311},{\"end\":30340,\"start\":30326},{\"end\":30867,\"start\":30854},{\"end\":30880,\"start\":30867},{\"end\":30890,\"start\":30880},{\"end\":30900,\"start\":30890},{\"end\":31337,\"start\":31323},{\"end\":31349,\"start\":31337},{\"end\":31365,\"start\":31349},{\"end\":31380,\"start\":31365},{\"end\":31396,\"start\":31380},{\"end\":31411,\"start\":31396},{\"end\":31425,\"start\":31411},{\"end\":31952,\"start\":31942},{\"end\":31963,\"start\":31952},{\"end\":31979,\"start\":31963},{\"end\":31993,\"start\":31979},{\"end\":32006,\"start\":31993},{\"end\":32020,\"start\":32006},{\"end\":32482,\"start\":32470},{\"end\":32490,\"start\":32482},{\"end\":32861,\"start\":32852},{\"end\":32874,\"start\":32861},{\"end\":32889,\"start\":32874},{\"end\":32903,\"start\":32889},{\"end\":33357,\"start\":33344},{\"end\":33374,\"start\":33357},{\"end\":33391,\"start\":33374},{\"end\":33402,\"start\":33391},{\"end\":33417,\"start\":33402},{\"end\":33436,\"start\":33417},{\"end\":33869,\"start\":33857},{\"end\":33885,\"start\":33869},{\"end\":34416,\"start\":34402},{\"end\":34437,\"start\":34416},{\"end\":34455,\"start\":34437},{\"end\":34470,\"start\":34455},{\"end\":35172,\"start\":35157},{\"end\":35185,\"start\":35172},{\"end\":35202,\"start\":35185},{\"end\":35785,\"start\":35771},{\"end\":35802,\"start\":35785},{\"end\":35816,\"start\":35802},{\"end\":35832,\"start\":35816},{\"end\":35846,\"start\":35832},{\"end\":36273,\"start\":36262},{\"end\":36287,\"start\":36273},{\"end\":36306,\"start\":36287},{\"end\":36325,\"start\":36306},{\"end\":36336,\"start\":36325},{\"end\":36348,\"start\":36336},{\"end\":36594,\"start\":36575},{\"end\":36807,\"start\":36799},{\"end\":36820,\"start\":36807},{\"end\":36833,\"start\":36820},{\"end\":36852,\"start\":36833}]", "bib_venue": "[{\"end\":25778,\"start\":25761},{\"end\":34670,\"start\":34567},{\"end\":22875,\"start\":22853},{\"end\":23542,\"start\":23493},{\"end\":24174,\"start\":24113},{\"end\":24773,\"start\":24683},{\"end\":25261,\"start\":25225},{\"end\":25759,\"start\":25678},{\"end\":26334,\"start\":26311},{\"end\":26804,\"start\":26705},{\"end\":27380,\"start\":27303},{\"end\":27985,\"start\":27919},{\"end\":28592,\"start\":28497},{\"end\":29050,\"start\":29032},{\"end\":29282,\"start\":29199},{\"end\":29548,\"start\":29505},{\"end\":29863,\"start\":29847},{\"end\":30423,\"start\":30366},{\"end\":30852,\"start\":30774},{\"end\":31126,\"start\":31083},{\"end\":31503,\"start\":31451},{\"end\":32113,\"start\":32047},{\"end\":32521,\"start\":32512},{\"end\":33005,\"start\":32934},{\"end\":33534,\"start\":33468},{\"end\":33934,\"start\":33885},{\"end\":34565,\"start\":34493},{\"end\":35275,\"start\":35219},{\"end\":35907,\"start\":35871},{\"end\":36260,\"start\":36223},{\"end\":36573,\"start\":36532},{\"end\":36797,\"start\":36723}]"}}}, "year": 2023, "month": 12, "day": 17}