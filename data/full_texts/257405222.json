{"id": 257405222, "updated": "2023-10-05 06:47:20.326", "metadata": {"title": "AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation", "authors": "[{\"first\":\"Rui\",\"last\":\"Meng\",\"middle\":[]},{\"first\":\"Ye\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Semih\",\"last\":\"Yavuz\",\"middle\":[]},{\"first\":\"Divyansh\",\"last\":\"Agarwal\",\"middle\":[]},{\"first\":\"Lifu\",\"last\":\"Tu\",\"middle\":[]},{\"first\":\"Ning\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Jianguo\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Meghana\",\"last\":\"Bhat\",\"middle\":[]},{\"first\":\"Yingbo\",\"last\":\"Zhou\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Dense retrievers have made significant strides in text retrieval and open-domain question answering, even though most achievements were made possible only with large amounts of human supervision. In this work, we aim to develop unsupervised methods by proposing two methods that create pseudo query-document pairs and train dense retrieval models in an annotation-free and scalable manner: query extraction and transferred query generation. The former method produces pseudo queries by selecting salient spans from the original document. The latter utilizes generation models trained for other NLP tasks (e.g., summarization) to produce pseudo queries. Extensive experiments show that models trained with the proposed augmentation methods can perform comparably well (or better) to multiple strong baselines. Combining those strategies leads to further improvements, achieving the state-of-the-art performance of unsupervised dense retrieval on both BEIR and ODQA datasets.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2212.08841", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": null}}, "content": {"source": {"pdf_hash": "209afcf7480c6b9ebefce8c8daa54a3ccefecd79", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2212.08841v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "31d69f3d03798a8cd6400412ffd1c1a0af18827f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/209afcf7480c6b9ebefce8c8daa54a3ccefecd79.txt", "contents": "\nAugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation\n\n\nRui Meng ruimeng@salesforce.com \nYe Liu \nSemih Yavuz \nDivyansh Agarwal \nLifu Tu \nNing Yu \nJianguo Zhang \nMeghana Bhat \nYingbo Zhou \nSalesforce Research \nAugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation\n\nDense retrievers have made significant strides in text retrieval and open-domain question answering, even though most achievements were made possible only with large amounts of human supervision. In this work, we aim to develop unsupervised methods by proposing two methods that create pseudo query-document pairs and train dense retrieval models in an annotation-free and scalable manner: query extraction and transferred query generation. The former method produces pseudo queries by selecting salient spans from the original document. The latter utilizes generation models trained for other NLP tasks (e.g., summarization) to produce pseudo queries. Extensive experiments show that models trained with the proposed augmentation methods can perform comparably well (or better) to multiple strong baselines. Combining those strategies leads to further improvements, achieving the state-ofthe-art performance of unsupervised dense retrieval on both BEIR and ODQA datasets. , et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67.\n\nIntroduction\n\nText retrieval is one of the most impactful artificial intelligence applications nowadays. Billions of users access massive amounts of data on the Internet through common internet services powered by information retrieval techniques, such as web search and product search. Despite the fact that traditional lexical retrieval remains a simple yet effective solution, neural network based models, namely dense retrievers, have made significant progress in recent years and demonstrated advantages in scenarios concerning semantic matching.\n\nNevertheless, most dense retrievers heavily rely on training with a large amount of annotated data. For example, MS MARCO (Nguyen et al., 2016) and Natural Questions (Kwiatkowski et al., 2019) are the two most widely used datasets, and models trained with them have obtained outstanding performance. But each of these datasets has hundreds of thousands of human annotated pairs, which can be prohibitively costly to collect, and the models trained with them may not perform well in unseen domains (Thakur et al., 2021). Training dense retrieval models without human-annotated data remains an unsolved challenge. Nevertheless, most dense retrievers heavily rely on training with a large amount of annotated data. For example, MS MARCO (Nguyen et al., 2016) and Natural Questions (Kwiatkowski et al., 2019) are the two most widely used datasets, and models trained with them have obtained outstanding performance. But each of these datasets has hundreds of thousands of human annotated query-document pairs, which can be prohibitively costly to collect, and the models trained with them may not perform well in unseen domains (Thakur et al., 2021). Training dense retrieval models without human-annotated data remains an unsolved challenge.\n\nRecently, there have been some efforts showing the feasibility of training dense retrievers in an annotation-free way (Izacard et al., 2021;Ram et al., 2021;Neelakantan et al., 2022). Following the regular paradigm of self-supervised learning, a pretext task is designed by taking two different views of a single document (a positive pair). Then a dual-encoder model is trained using contrastive learning, mapping the two views of data to hidden representations as similar as possible. When directly applied on downstream retrieval tasks, the scores of those unsupervised models lag behind the classic lexical method BM25 by large margins, but a boost in performance is observed after tuning with annotated query-doc pairs. This motivates us to focus on the gap between the pretext task and downstream retrieval tasks. Existing strategies for constructing positive pairs are very heuristic, e.g. Contriever (Izacard et al., 2021) samples two random text spans from a document to form a positive pair. One can see that the quality of the positive pairs is poorly controlled, and the resulting pseudo queries bear little resemblance to real-world ones. As a result, the models are negatively impacted by the noisy pseudo pairs, and they achieve inferior performance on down-stream tasks.\n\nIn this study, we propose two new categories of strategies for constructing pseudo query-doc pairs, namely query extraction and transferred query generation. Given a document of an arbitrary domain, both methods can produce pseudo queries in an unsupervised manner, and the resulting queries are paired with the original document to train dense retrievers using contrastive learning. The contributions of this study can be summarized as follows:\n\n1. We propose query extraction (QEXT), a novel data augmentation method for training dense retrievers. Given a document, a list of random spans is sampled from it. We utilize various methods to gauge their salience to the document and select the spans with the highest scores as pseudo queries.\n\n2. We propose transferred query generation (TQGEN). Pseudo queries are produced by language generation models for other NLP tasks, e.g., summarization. Even though query generation (QGen) has been studied for long, existing studies require annotated data for training query generation models. Instead, we utilize out-of-distribution generation models for this end, and the result demonstrates that the inductive bias of specific NLP tasks can be effective for training dense retrievers.\n\n3. We introduce two datasets AUGQ-WIKI and AUGQ-CC, consisting of 22.6M and 52.4M pseudo query-document pairs, by applying the proposed augmentation methods on two large corpora.\n\n4. Extensive experiments show that the retrievers trained with AUGQ, named AUGTRIEVER, outperform strong baselines and achieve state-ofthe-art results on both BEIR and open-domain QA benchmarks, greatly bridging the gap between unsupervised dense methods and BM25.  \n\n\nMethod\n\nIn this section, we introduce several data argumentation methods to create pseudo queries given a For land surfaces, it has been shown that the albedo at a particular solar zenith angle \"\u03b8\" can be approximated by the proportionate sum of two terms: (-) the directional-hemispherical reflectance at that solar zenith angle, sometimes referred to as black-sky albedo, and (-) the bi-hemispherical reflectance, sometimes referred to as white-sky albedo. with 1-D being the proportion of direct radiation from a given solar angle, and D being the proportion of diffuse illumination, \u2026  Figure 1: An overview of proposed augmentation methods for AUGTRIEVER. document, without using any annotated queries or questions. We apply those methods on Wikipedia passages and CommonCrawl web documents to obtain two large augmented AUGQ datasets. Then we train bi-encoder dense retrievers (using either InBatch or MoCo architecture) using AUGQ and we name the resulting models AUGTRIEVER.\n\n\nQuery Extraction (QEXT)\n\nGiven a document, we hypothesize that certain parts of it contain more representative information. Therefore, we can extract and utilize them as pseudo queries to train retrievers. \n\n\nQuery Extraction by Salient Span Selection\n\nThe previous method heavily relies on the quality and availability of distant labels embedded in the document structures, which may limit the training scale. To overcome this issue, we propose an alternative that directly extracts informative spans from a document. The hypothesis is that, a document can be segmented into multiple spans and some of them are more representative than others. Then we can utilize different methods to select the most salient spans to serve as pseudo queries. Note that we do not mask the selected span in the document. Formally, given a document d, we randomly sample a number of text spans s 1 , s 2 , ..., s N from it. In this study, we consider 16 random spans and their length ranges from 4 to 16 words. We propose three ways to measure the salience between d and each of those spans:\n\n\u2022 QEXT-SELF: selecting by the model itself.\n\nWe simply feed the bi-encoder model with each span s i paired with d, and use the dotproduct score as their salience.\n\n\u2022 QEXT-BM25: selecting by lexical models. BM25 is broadly used to measure the lexical relevancy between queries and documents. Thus it can also serve to select spans based on lexical statistics.\n\n\u2022 QEXT-PLM: selecting by pre-trained language models. Pre-trained language models have demonstrated great performance in text understanding and generation. In our setting, we consider PLMs as a means for measuring the relevancy by checking how likely a span can be generated given a document as the context. Technically, we can feed a T5-small LM-Adapted model (Raffel et al., 2020) with the document as a prefix, and use the likelihood p(s i |d) as the salience score.\n\n\nTransferred Query Generation (TQGEN)\n\nPrevious works have shown that query generation can work as an effective means for augmenting training data (Lewis et al., 2021; Nogueira et al., 2019) whereas it requires a great amount of annotated data to obtain such a query generator. Instead, we propose to use text generator models for other tasks to produce pseudo queries, expecting the inductive bias for other tasks can be effective to bootstrap the training of dense retrievers. Hypothetically, dense retrievers may benefit from the inductive bias of tasks such as summarization and keyphrase generation, since the outputs of those tasks are commonly regarded relevant and representative about the source text. As for implementation, we use a single T0 model (3B parameters) as a meta generator, to avoid the hassle of choosing specific models for each generation task. We provide the T0 model with various prompts to elicit outputs for different tasks, including:\n\n\u2022 TQGEN-TOPIC: What is the main topic of the above text?\n\n\u2022 TQGEN-TITLE: Please write a title of the text above.\n\n\u2022 TQGEN-ABSUM (Abstractive summary):\n\nPlease write a short summary of the text above.\n\n\u2022 TQGEN-EXSUM (Extractive summary):\n\nPlease use a sentence in the above text to summarize its content.\n\nThe reason we design a prompt for extractive summaries intentionally is that we ask the model to use words in the original text as much as possible to reduce the hallucinations in the outputs. We utilize nucleus sampling to generate a single pseudo query for each document with Top-p=0.9 and Top-K=0. We explore two hybrid settings by ensembling multiple augmentation strategies. (1) Hybrid-All: RANDOMCROP 20%, QEXT-PLM 10%, DOC-TITLE+TQGEN 70%; (2) Hybrid-TQGen: RAN-DOMCROP 20%, TQGEN 80%. We also experiment with longer training time (200k steps), resulting the third hybrid setting Hybrid-TQGen+.\n\n\nExperiments\n\n\nDatasets\n\nTraining Data: We consider two large text datasets to produce augmented query-document data AUGQ: Wikipedia 1 and CommonCrawl by Pile (Gao et al., 2020) (Pile-CC). For Wikipedia, we process the original text dump by segmenting articles into paragraphs by line breaks and reserving titles and anchor texts (texts with hyperlinks, italics, or boldface), resulting in 22.6M paragraphs for training. Pile-CC contains 52.4M web documents. Since it has removed structure information, DOC-ANCHOR is unavailable. For DOC-TITLE, we simply take the first line of each document as its title, and truncate it to no longer than 64 words. We manually check a few hundred examples and find it extracts correctly in about 50% of all cases. Test Data: Two benchmarks are used for evaluation -BEIR (Thakur et al., 2021) and six ODQA datasets. We consider BEIR to be a better benchmark for information retrieval, as it covers a broder range of domains and a wide variety of query types. We discuss scores of MS MARCO (MM) separately, since it is one of the most studied IR testsets.\n\nOn the other side, all ODQA datasets are based on Wikipedia and mainly designed for evaluating question answering systems, thus it may be subject to certain domain/task bias. We only use them for retrieval following previous studies (Karpukhin et al., 2020; Ram et al., 2021). We report scores on SQuAD v1.1 (SQ) and EntityQuestions (EQ) separately, as they tend to favor lexical models, whereas the other four may favor semantic matching.\n\n\nImplementation Details\n\nOur models AUGTRIEVER use either INBATCH or MOCO as the backbone architecture and are initialized with BERT-base (Devlin et al., 2019). We adopt most settings used by CONTRIEVER (Izacard et al., 2021) for unsupervised training, except for a smaller training scale considering the number of model variants we experiment with. Specifically, AUGTRIEVER is trained with (1) 100k steps for most, and 200k steps for HYBRID-TQGEN+, (2) batch size of 1,024 for most, 2,048 for AUGQ-CC with TQGEN/QGEN/HYBRID, (3) learning rate (lr) of 5e \u22125 and 10k steps of linear warmup, (4) ADAM optimizer, and (5) queue size of 16,384 (for MoCo), which are considerably smaller comparing with CONTRIEVER (500k steps, batch size 2048, queue size 131,072). We do observe certain performance gains with longer training and larger batch size, but a queue size of larger than 16,384 deteriorates the performance. For domain adaptation, we train a pre-trained model with 5k steps, batch size of 1,024 and lr of 1e \n\n\nBaselines\n\nA number of unsupervised dense methods are discussed in Sec 2.2. We consider BM25 as a lexical baseline and three dense baselines -CONTRIEVER, SPIDER, SPAR \u039b (Wikipedia version). We report their scores if publically available (BEIR results of BM25 and CONTRIEVER), or reproduce the results using public code and checkpoints ( \u2020 indicates a reproduced result). MOCO+RANDOMCROP can be regarded as our reproduced CONTRIEVER in a smaller scale. We also include baselines with generated queries (using a supervised Doc2Query (Nogueira et al., 2019)) and questions PAQ (Lewis et al., 2021), referred to as QGEN-D2Q and QGEN-PAQ respectively. We rerun all baselines on Touch\u00e9-2020 since the data has been updated in BEIR.\n\n\nResults\n\n\nUnsupervised Retrieval Results\n\nWe present the main unsupervised results in Table 1 and will discuss the other results in Sec 4.5. As for baseline results, we can see that BM25 leads on most benchmarks by a large margin. Among the three dense retrievers, the lexical-oriented retriever SPAR \u039b performs the best on BEIR14 and SQ&EQ, which indicates that dense retrievers can achieve robust retrieval performance by lexical matching. CONTRIEVER performs comparably with SPAR \u039b on BEIR, and it outperforms others on MM and QA4. Supervised augmentation baseline QGEN-D2Q delivers very competitive results on both benchmarks, suggesting that query generation trained with MS MARCO can work decently for both in-domain and out-of-domain datasets.\n\nAs for AUGTRIEVER 2 , we find that multiple variants can outperform three dense baselines on BEIR significantly, especially the ones trained with domain-general data CC. Also, multiple variants achieve better scores than BM25 on MM and QA4. Most notably, our best results are achieved by combining different strategies (Hybrid-*) and longer training (Hybrid-TQGen+). Hybrid-TQGen+ even outperforms CPT-text L (Neelakan-2 Here we train InBatch models using augmented querydocument pairs, whereas we train MoCo models using a 50/50 mixed strategy (50% of pairs by RANDOMCROP and 50% by one of the proposed augmentation strategies). More comparisons between the two settings are discussed in Sec 4.5. Hybrid settings are detailed in the end of Sec 3.  time, thus domain adaptation can be of great use in real-world scenarios. We explore whether the proposed augmentation methods can be effective for this purpose. Here we consider an InBatch model pre-trained with AUGQ-CC and TQGEN-TOPIC, and tune it with in-domain pseudo querydocument pairs, i.e. we generate pseudo queries (main topics) for documents in each BEIR dataset. We use this setting because TQGEN-TOPIC models performs comparably well (Table 1), and the topic queries are short and thus less costly to generate. The results are shown in Table 2. We can see that on most domains (BEIR testsets) this method can still lead to significant improvements (up to 30%), suggesting the efficacy of adapting models with in-domain data. Out of 15 BEIR datasets, domain adaptation leads to positive impacts on 11 of them, and it outperforms BM25 on 7 datasets (only 3 if w/o DA). The model gains the most in domains that are specific and distant from the pretraining distribution, such as finance (FiQA) and science (COVID, Scidocs). However, negative impacts are observed in four domains that only a small number of documents are available. Except for Touch\u00e9-2020, the rest three testsets offer no more than 10k documents, which might have caused overfitting during the course. Additional in-domain data, regularizations and hyperparameter tuning can be helpful, and we leave it to future work.\n\n\nFine-Tuned Results\n\nTo demonstrate whether the proposed augmentation methods can work as effective pretraining measures, we provide MS MARCO fine-tuned results in Table 3. Note that we only adopt basic settings for fine-tuning, without sophisticated techniques such as negative mining (Izacard et al., 2021) or asynchronous index refresh (Xiong et al., 2020).\n\nAll AUGTRIEVER models attain significant improvements after fine-tuning. Our models outperform most baseline models by a clear margin, indicating the effectiveness of using AUGQ for pretraining. Our best model Hybrid-TQGen+ performs better than CONTRIEVER on BEIR (45.0 vs. 44.5), but it lags behind on MS MARCO and ODQA  datasets, and we think it can be partially attributed to the fact that CONTRIEVER was pre-trained with both Wikipedia and CC and its training is much longer (500k steps vs. 200k steps for ours). In most cases, the trend of fine-tuned scores is consistent with that of unsupervised ones, strongly evidences that the inductive bias from various augmentation measures is beneficial for downstream retrieval tasks. InBatch outperforms MoCo counterpart on MS MARCO and BEIR, but lags behind on ODQA datasets. INBATCH+TQGEN-TOPIC is the most performant variant among all, followed by other TQGEN variants, DOC-TITLE and QEXT-PLM. The relative advantage of MoCo under the unsupervised setting is not carried over to the finetuned results, probably because of the architecture inconsistency between pretraining and fine-tuning.\n\n\nResult Analysis\n\nTo better understand how individual augmentation strategies help retrieval tasks and in what specific scenarios they perform well or poorly, we conduct a detailed analysis. We report performance of AUGTRIEVER (trained on AUGQ-WIKI) and baselines in Fig 2, by averaging scores of 14 BEIR datasets and 6 ODQA datasets. We can see that BM25 leads with a clear margin among all mod-  els, remaining a competitive unsupervised baseline. Besides, we make the following observations: 1. BEIR is considered a better benchmark for evaluating retrieval models. In general, trends on BEIR are similar to that on ODQA. However, all ODQA datasets are built for question-answering using Wikipedia, therefore it may suffer from certain domain/task bias. Instead, BEIR covers a wider range of domains and topics, it is expected to serve better as a generic retrieval benchmark, providing more insights into models' generalization ability. For example, QGEN-PAQ, which is trained with 65M generated query-document pairs on Wikipedia, excels on ODQA (in-distribution) but fails to generalize well on BEIR. 2. Among all AUGTRIEVER variants, TQGEN achieves the best scores, outperforming all dense baselines significantly. The result suggests that the outputs of selected text generation tasks can be utilized as surrogate queries for training dense retrievers. Among the four strategies, shorter pseudo queries (TOPIC/TITLE) appear to be performing better on BEIR (even better than QGEN-D2Q). On the contrary, longer ones (ABSUM/EXSUM) suit better on ODQA, probably because they bear a resemblance to questions by carrying more details. EXSUM works slightly better than ABSUM, and we conjecture this is because EXSUM tends to use original text and thus has fewer hallucinations. 3. InBatch performs well with queries of better quality, whereas MoCo benefits from noisy ones. RANDOMCROP yields massive noisy queries (e.g., incomplete sentences, non-informative texts), but MoCo can achieve good results with it (Izacard et al., 2021). This suggests that a momentum encoder allows greater robustness against noisy pairs. In contrast, InBatch performs notably better with \"cleaner\" queries (i.e. TQGEN and QGEN), revealing the advantage of each architecture. 4. Following the previous finding, we wonder whether combining RANDOMCROP (noisy but diverse) with other better quality queries can lead to better results. In Figure 3, we find that MoCo obtains consistent gains by mixing individual augmentation methods with RANDOMCROP, performing on a par with the best results of InBatch without RANDOMCROP. This manifests that MoCo can take advantage of multiple strategies, resulting in better generalization ability. Conversely, the mixed strategy causes little help or even a performance drop on InBatch, which echoes our previous argument on the traits of each architecture. \n\n\nRelated Work\n\n\nDiscussion and Conclusion\n\nIn this study, a series of scalable augmentation methods are proposed to produce surrogate queries for training dense retrievers without using any annotated query-document pairs. We achieve state-ofthe-art performance on two collections of widely used benchmarks (BEIR and six ODQA datasets), demonstrating that the efficacy of synthetic querydocument pairs for training dense retrievers, greatly bridging the gap between unsupervised dense models and BM25 and inspiring us to rethink the necessity of using real queries.\n\nFor future work, we would like to continue investigating unsupervised methods for dense retrieval, to understand what causes the gap between lexical and dense retrievers. We observe that lexical features can be complementary to distributed representations, which motivates us to explore effective methods to represent lexical information.\n\nBesides, it remains an open question that what the difference is between synthetic and annotated query-document pairs, and it is interesting to understand how different augmentation methods help with dense retrieval. Particularly, we would like to dive deeper into the salient span selection for query extraction, which is easy to scale and shows impressive results when training with domain-general data. Factors such as the length of spans, cropping methods, and ranking models are worth studying in the next step.\n\n\nLimitations\n\nWe demonstrate that several data augmentation can be taken as an effective means for training unsupervised dense retrieval models. We acknowledge that this study suffers from the following limitations:\n\n\u2022 Our experiments with QEXT is limited. Many key factors are underexplored, such as scoring models, span lengths and number of candidates.\n\n\u2022 We manually select certain language generation tasks in TQGEN, nevertheless pseudo queries transferred from the other tasks, such as back-translation and paraphrase generation, may also be useful but are missing from this study. We heuristically define four prompts to elicit the corresponding outputs from T0, but the outputs often do not match what we expect (e.g. long sentences in TOPIC, abstractive contents in EXSUM). Besides, we only keep single output sequence for each document. Experimenting with other generation models as well as more diverse outputs will be interesting.\n\n\u2022 Our models with hybrid strategies show the overall best results, but we fail to explore other combinations that might lead to better performance.\n\n\u2022 Due to computational constraints, we only explore very limited settings of backbones and hyperparameters and train most models for 100k steps. It is yet not clear whether other configurations can lead to significant performance changes.\n\n\nEthics Statement\n\nDataset Biases Two AUGQ datasets were generated based on public web-scale data (Wikipedia and CommonCrawl), both of which mainly represent the culture of the Englishspeaking populace. Political or gender biases may also exist in the dataset, and models trained on these datasets may propagate these biases. Additionally, the TQGEN models (T0 in this study) may carry biases from the data it was trained on. Environmental Cost The experiments described in the paper primarily make use of A100 GPUs. We typically used eight GPUs per experiment, and the experiments may take up to two days. The backbone model BERT-base and T0 have 110 million and 3 billion parameters respectively. While our work required extensive experiments, future work and applications can draw upon ourinsights and need not repeat these comparisons. \n\n\nReferences\n\n\nA Appendix\n\n\nA.1 Additional Implementation Details\n\nAll experiments are conducted on cloud instances equipped with eight NVIDIA A100 GPUs (40GB) and most training jobs were finished within 48 hours.\n\n\nA.2 Detailed Analysis on BEIR\n\nWe plot the relative performance of different models in comparison with CONTRIEVER on BEIR in Figure 4. The left heatmap shows the relative performance on each BEIR dataset, and the right one presents the averaged scores after grouping from different aspects. AUGTRIEVER models perform significantly better on QA datasets (e.g. TREC-COVID, FiQA, NQ and HotpotQA), worse on fact checking datasets (e.g. FEVER, Climate-FEVER, Scifact and Quora), and similarly on the rest datasets. CONTRIEVER explicitly blends Wikipedia and CCNet in training, to favor knowledge-rich testsets, but it does not show consistent benefits across all five Wikipedia related testsets. It is enlightening to see that, DOC-TITLE and QEXT-PLM with MoCo, which extract certain parts of original documents for pseudo queries, can deliver comparable or better performance to CON-TRIEVER. As for our TQGEN models, both models perform very well on TREC-COVID, which contributes to the major part of the improvement. But they underperform DOC-TITLE and QEXT-PLM considerably on Climate-FEVER and Quora, indicating that each augmentation method may be most beneficial for certain tasks. Training with hybrid strategies does not appear to simply bring the advantage of all. With regard to the effect of pseudo query length, TQGEN-TOPIC indeed performs better on datasets with short and medium length queries (SQ/MQ), and TQGEN-EXSUM shows more strength on medium and long queries.\n\n\nA.3 Examples of QEXT\n\nTwo examples of random spans ranked by two proposed QEXT methods are shown in Table 4. In the first example, we notice that both BM25 and QEXT-PLM are able to rank informative spans to the higher places and place generic spans to the bottom (e.g. \"a few years later\" and \"over the first three\"). In the second example, we observe that BM25 is more influenced by the low-frequent words (e.g. HS, HO, NaH), whereas QEXT-PLM is more resistant to the noise, placing more informative spans to the top.\n\n\nA.4 Examples of TQGEN\n\nSix documents from Pile-CC and the corresponding generated pseudo queries are shown in Table 5. We find that most generated outputs are semantically relevant, in spite of a certain degree of hallucination. In most cases, TOPIC outputs one or two important phrases and TITLE outputs one short sentence. Both ABSUM and EXSUM generate rela- tively longer sentences as summaries, and EXSUM does not necessarily use contents of the original texts. In the last example, four outputs by T0 are almost the same, indicating that the model ignores the specified prompts.\n\n\nA.5 BEIR dataset groups\n\nThe groups disussed in Figure 4 are defined as follows:\n\n\u2022 SQ (Short Query): 3 datasets of which the average query length is shorter than 7 words, including: NFCorpus, Touch\u00e8-2020, DBPedia-Entity. ASD can sometimes be diagnosed by age 14 months, although diagnosis becomes increasingly stable over the first three years of life: for example, a one-year-old who meets diagnostic criteria for ASD is less likely than a three-year-old to continue to do so a few years later.\n\nIn the UK the National Autism Plan for Children recommends at most 30 weeks from first concern to completed diagnosis and assessment, though few cases are handled that quickly in practice. Although the symptoms of autism and ASD begin early in childhood, they are sometimes missed; years later, adults may seek diagnoses to help them or their friends and family understand themselves, to help their employers make adjustments, or in some locations to claim disability living allowances or other benefits.\n\nReaction with oxygen Upon reacting with oxygen, alkali metals form oxides, peroxides, superoxides and suboxides. However, the first three are more common. The table below shows the types of compounds formed in reaction with oxygen. The compound in brackets represents the minor product of combustion. The alkali metal peroxides are ionic compounds that are unstable in water. The peroxide anion is weakly bound to the cation, and it is hydrolysed, forming stronger covalent bonds. NaO + 2HO \u2192 2NaOH + HO The other oxygen compounds are also unstable in water. 2KO + 2HO \u2192 2KOH + HO + O LiO + HO \u2192 2LiOH Reaction with sulphur With sulphur, they form sulphides and polysulphides. 2Na + 1/8S \u2192 NaS + 1/8S \u2192 NaS...NaS Because alkali metal sulphides are essentially salts of a weak acid and a strong base, they form basic solutions. S + HO \u2192 HS + HO HS + HO \u2192 HS + HO Reaction with nitrogen Lithium is the only metal that combines directly with nitrogen at room temperature. Table 4: Examples of random spans ranked by BM25 and a Pretrained Language Model (T5-Small). The numbers in brackets are scores of BM25 or T5-Small (negative log likelihood).\n\n\nDocument Output\n\nTopic: reinvent midnight madness\\n\\nAmazon announced a new service at the AWS re:Invent Midnight Madness event. Amazon Sumerian is a solution that aims to make it easier for developers to build virtual reality, augmented reality, and 3D applications. It features a user friendly editor, which can be used to drag and drop 3D objects and characters into scenes.\n\n\nTopic: Sumerian\n\nTitle: aws announces sumerian 3d editor AbSum: Developing immersive experiences for virtual and augmented reality (VR) and 3D gaming has become an increasing challenge for tech companies.\n\n\nExSum: a new service called amazon sumerian is launched\n\nQGen: what is amazon sumerian 3d?\n\nRunning\\n\\nStat\\n\\nDinner with people is always better than eating alone, especially when the food is good. Good food tastes even better when enjoyed with people. Tonight Amy came over to try my second attempt at the Brussels Sprouts Veggie Soup to which I have made some changes (see recipe below in previous post) for a better result, I believe.\\n\\nWe were at the store earlier and saw some nice looking haricot verts and heirloom tomatoes, so we decide to assemble a simple salad from those. Of course while I\\u2019m at the market, I can\\u2019t not get some five peppercorn salami. Our simple dinner of soup, salami, bread, cheese, salad, and wine was on the table in 15 minutes.\n\n\nTopic: Simple dinner\n\nTitle: An easy recipe for a delicious winter soup enjoyed by friends is always a winner.\n\nAbSum: If you try making a Brussels Sprouts Veggie Soup this weekend, please let me know how it turns out.\n\nExSum: If you try out my Brussels Sprouts Veggie Soup recipe it should turn out even better, and it was certainly better tonight.\n\nQGen: are you better at dinner with someone or alone?\n\nGordhan asks for fresh thinking\\n\\nBusiness News / 9 July 2012, 4:48pm\\n\\nSAPA\\n\\nCape Town 141010\\nFinance Minister, Pravin Gordhan briefing parliment on the annal business report.South African Finance Minister Pravin Gordhan said on Thursday the world was heading towards a \\\"currency war\\\" unless developed nations gave ground in negotiations at the Group of 20 (G20).\\npicture : neil baynes\\n\\nGordhan told the 16th World Economic History Congress in Stellenbosch there was an imbalance between the locus of production and that of growth, and between political beliefs and the predominant reality...\n\n\nTopic: Finance Minister Pravin Gordhan said there was an imbalance between the loc\n\nTitle: Finance Minister Pravin Gordhan has warned that the world is heading towards a \\\"currency war\\\" unless developed nations give ground in negotiations\n\nAbSum: Finance Minister Pravin Gordhan has warned that the world is heading towards a \\\"currency war\\\" unless developed nations give ground in negotiations at the G20. In a speech at the World Economic History Congress in Stellenbosch on Thursday, Gordhan said that he was encouraged\n\nExSum: Pravin Gordhan has criticised South Africa's cabinet.\n\nQGen: who is the finance minister of south africa\n\nThe Difference Between Botox and Dermal Fillers\\n\\nWritten by CG Cosmetic on February 19, 2015\\n\\nCG Cosmetic understands the difficulties that come with aging. Everyone has days where they look in the mirror and are concerned with what they see. Whether or not you see deep lines and wrinkles, or fine lines, aging is inevitable. The question then, is what can you do about these signs of aging? Perhaps the most common solution people have heard of is Botox. However, Botox is not the only option. While CG Cosmetic offers Botox procedures, we also offer Dermal Fillers.\\n\\nBotox\\n\\nMost men and women have heard about celebrities using Botox to rejuvenate their skin and appear more youthful, but Botox isn\\u2019t just for celebrities. Botox has provided amazing and effective results that are also safe and convenient for many individuals. Facial lines and wrinkles often occur because of the way your muscles work underneath your skin...\n\n\nTopic: Botox vs. Dermal Fillers\n\nTitle: Botox is popular for many reasons, including it helping treat aging lines and wrinkles. Dermal fillers, meanwhile, add volume to AbSum: Botox relaxes muscles under the skin and helps treat aging lines and wrinkles. Dermal Fillers, meanwhile, add volume to treat sagging skin. CG Cosmetic offers both treatments to help patients maintain a youthful appearance.\n\nExSum: Botox relaxes muscles under the skin and helps treat aging lines and wrinkles. Dermal Fillers aims to add volume to facial features such as lips, chin, and jaw line. QGen: are the australian chamber orchestra part of the great romantics tour? \n\n\nadopt a Transformer based bi-encoder architecture and contrastive learning for training dense retrievers (Karpukhin et al., 2020; Xiong et al., 2020; Izacard et al., 2021). Technically, two transformers (E q and E d ) are used for encoding queries q and documents d respectively, and they are represented by low-dimensional vectors by average pooling all output embeddings of the top layer. Then we obtain the similarity score between q and d by cal-culating the inner product of the two vectors. The parameters of encoders are initialized with BERTbase (Devlin et al., 2019) and are shared. The model is optimized by a contrastive objective, with other documents in the same batch as negative examples. Alternatively, recent works (Izacard et al., 2021; Yang et al., 2021; Xu et al., 2022) use a momentum encoder and a large vector queue to enable using more negative examples. We refer to as INBATCH the architecture using negative examples in the same batch and as MOCO the architecture with a momentum document encoder.\n\n\u2212 5 .\n5For fine-tuning, we train models with extra 20k steps, batch size of 1,024 and 1,024 extra random negative examples (1 positive+2,047 negative), and lr of 1e \u22125 .\n\nFigure 2 :\n2Performance of AUGTRIEVER trained on Wikipedia with individual augmentation strategies. The upper shows averaged nDCG@10 scores over 14 test sets of BEIR benchmark, and the lower shows averaged Recall@20 scores over 6 ODQA datasets. Dashed lines indicate the scores of BM25, CONTRIEVER and MOCO+RC.\n\nFigure 3 :\n3Performance of AUGTRIEVER trained on Wikipedia with hybrid strategies (50/50 mix of RANDOMCROP and another augmentation).\n\n\nRecent years have seen a flourishing of research works for neural network based information re-trieval and question answering. The interested reader may refer to (Lin et al., 2021; Guo et al., 2022; Zhao et al., 2022) for a comprehensive overview. Our study, along with a line of recent studies (Izacard et al., 2021; Ram et al., 2021), falls under the category of self-supervised learning using contrastive learning (Shen et al., 2022), in which a model is trained to maximize the scores of positive pairs and minimize the scores of negative ones. It has demonstrated effective for supervised dense retrieval (Karpukhin et al., 2020; Xiong et al., 2020; Liu et al., 2021) and pretraining (Izacard et al., 2021; Yu et al., 2022). Different from most prior studies, we target at unsupervised methods that can be directly and independently applied in retrieval tasks, without any further tuning using annotated data. Previous works propose different ways to construct query-document pairs to fit the requirement of contrative learning. Lee et al. propose inverse cloze task (ICT), using a random sentence as a pseudo query to predict the surrounding context in a batch. REALM (Guu et al., 2020) pretrains a retriver and generator with a pair of a salient span (named entities) and its context. Spider (Ram et al., 2021) proposes to use recurring spans as pseudo queries. The above studies focus on ODQA tasks and their pseudo queries tend to be entity-like, but results of this study and Izacard et al. show that entity-like queries (e.g. anchor texts) fail to generalize well in a broad range of domains. Some studies propose more generic ideas for training unsupervised models. Specifically, Neelakantan et al.; Ma et al. use neighboring pieces of text as positive pairs. Izacard et al. adopt a random cropping strategy to sample two text spans, encouraging the model to learn lexical matching. Chen et al. use random sentences or real questions as queries and pair them with documents ranked by BM25. A few research works investigate techniques of data augmentation and domain adaptation for text retrieval and understanding (Tang et al., 2022; Wang et al., 2022; Iida and Okazaki, 2022). Query and question generation have been shown as an effective method for augmenting retrieval training data (Thakur et al., 2021; Nogueira et al., 2019; Lewis et al., 2021; Ma et al., 2021a; Gangi Reddy et al., 2022; Cho et al., 2022; Liang et al., 2020). Wang et al. use cross-encoder to select a good set of synthetic query-document pairs for domain adaptation. Bonifacio et al.; Dai et al. propose to generate questions using large language models in a few-shot manner. Xu et al. propose to use Dropout-as-Positive-Instance for pretraining retrievers. Fang et al. use positive pairs generated by back-translation. For hyperlinks, Chang et al. compare three pretraining tasks for retrieval -inverse cloze task, body first selection, and wiki link prediction. Zhou et al. and Wu et al. utilize hyperlinks to construct pseudo query-document pairs. Ma et al. and Ma et al. propose a representative words prediction task to optimize the semantic distance between a document and a pair of random word sets, estimated by language models. Recent studies (Sachan et al., 2022a,b) use pretrained language models for reranking and Open-domain QA, using the likelihood of question generation to approximate the relevance between questions and documents.\n\nFigure 4 :\n4Two heatmaps show the relative performance gain/loss of different models against CONTRIEVER. The left heatmap shows nDCG@10 difference on each BEIR dataset, and in the right figure we group BEIR datasets in different ways. SQ/MQ/LQ: datasets with short/medium/long queries. SD/MD/LD: datasets with short/medium/long documents. Phrase/Question/Sentence denotes datasets that use this form of queries. And the rest are categorized by text domains as defined in(Thakur et al., 2021). Refer to A.5 for specific grouping of datasets.\n\n\n\u2022 MQ (Medium Query): 8 datasets of which the average query length is between 8 to 13 words, including: TREC-COVID, NQ, FiQA-2018, SCIDOCS, FEVER, SciFact, Quora, CQADupStack. \u2022 LQ (Long Query): 3 datasets of which the average query length is longer than 17 words, including: HotpotQA, ArguAna, Climate-FEVER. \u2022 SD (Short Document): 3 datasets of which the average document length is shorter than 70 words, including: NFCorpus, HotpotQA, DBPedia-Entity. \u2022 MD (Medium Document): 6 datasets of which the average document length is between 75 and 150 words, including: NQ, FiQA-2018, SCIDOCS, FEVER, Climate-FEVER, CQADupStack. \u2022 LD (Long Document): 6 datasets of which the average document length is longer than 160 words, including: TREC-\n\n\nwe find that TQGEN achieves the overall best performance, which suggests that the outputs of transferred generation tasks, e.g.,keyword and summary generation (Meng et al., 2017; See et al., 2017), can be directly carried over into training dense retrieval models. (2) Note that TQGEN-TOPIC generalizes well under all settings, indicating that keywords could serve as robust surrogate queries. (3) MOCO+QEXT-PLM exhibits better results to all dense baselines on BEIR. This signifies that query extraction can work as a decent unsupervised method, especially when trained with domain-general data. MM BEIR14 TREC-COVID NFCorpus NQ HotpotQA FiQA ArguAna Touche DBPedia Scidocs FEVER Cli-FEVER Scifact Quora CQADupGroup \nModel \nMM BEIR14 BEIR11 QA4 SQ&EQ \n\nBaseline \n\nBM25 \u2020 \n22.8 \n42.9 \n46.1 \n70.7 \n71.3 \nContriever \u2020 20.6 \n36.9 \n40.7 \n73.1 \n63.9 \nSPAR \u2020 \n19.3 \n37.3 \n41.4 \n69.1 \n67.7 \nSpider \u2020 \n15.0 \n28.1 \n31.3 \n73.0 \n63.6 \nCPT-text S \n-\n-\n42.2 \n-\n-\nCPT-text M \n-\n-\n43.2 \n-\n-\nCPT-text L \n-\n-\n44.2 \n-\n-\n\nInBatch \n14.0 \n25.3 \n28.0 \n61.9 \n44.9 \nRandomCrop(Wiki) MoCo \n17.5 \n30.9 \n34.1 \n64.6 \n52.5 \nInBatch \n16.3 \n27.4 \n30.6 \n73.2 \n57.7 \nRandomCrop(CC) \nMoCo \n19.2 \n34.0 \n37.5 \n71.5 \n61.9 \nInBatch \n25.4 \n38.5 \n42.5 \n78.6 \n67.1 \nQGen-D2Q(Wiki) \nMoCo \n23.7 \n38.5 \n42.3 \n77.4 \n67.4 \nInBatch \n24.4 \n39.5 \n42.8 \n75.6 \n63.2 \nQGen-D2Q(CC) \nMoCo \n23.2 \n39.8 \n43.7 \n76.6 \n65.6 \n\nAUGTRIEVER-CC \n\nInBatch \n19.7 \n33.2 \n36.5 \n73.5 \n59.7 \nDoc-Title \nMoCo \n21.8 \n38.7 \n42.7 \n74.8 \n64.3 \nInBatch \n16.2 \n27.2 \n40.4 \n73.4 \n57.9 \nQExt-PLM \nMoCo \n20.6 \n38.2 \n42.3 \n73.0 \n64.1 \nInBatch \n20.7 \n39.0 \n43.0 \n71.6 \n60.5 \nTQGen-Topic \nMoCo \n21.2 \n38.9 \n43.1 \n73.3 \n63.4 \nInBatch \n18.1 \n35.3 \n38.6 \n72.0 \n57.4 \nTQGen-AbSum \nMoCo \n23.2 \n39.6 \n43.5 \n74.4 \n64.9 \nInBatch \n18.9 \n36.3 \n39.6 \n72.8 \n58.7 \nTQGen-ExSum \nMoCo \n23.0 \n39.4 \n43.4 \n74.8 \n64.9 \n\nAUGTRIEVER-CC with Hybrid Strategies \n\nHybrid-All \nMoCo \n23.5 \n39.4 \n43.3 \n74.1 \n64.4 \nHybrid-TQGen \nMoCo \n23.3 \n39.0 \n43.7 \n74.3 \n64.4 \nHybrid-TQGen+ \nMoCo \n24.6 \n41.1 \n45.2 \n76.0 \n65.9 \n\nAUGTRIEVER-Wikipedia \n\nInBatch \n15.6 \n29.8 \n33.2 \n64.8 \n49.9 \nDoc-Anchor \nMoCo \n17.9 \n35.4 \n39.2 \n68.5 \n57.4 \nInBatch \n14.7 \n30.0 \n33.9 \n61.9 \n52.1 \nDoc-Title \nMoCo \n18.5 \n33.7 \n37.1 \n68.6 \n58.4 \nInBatch \n15.0 \n26.3 \n28.2 \n61.5 \n43.8 \nQExt-PLM \nMoCo \n18.6 \n34.3 \n37.8 \n66.6 \n55.7 \nInBatch \n21.3 \n38.9 \n43.2 \n72.4 \n64.4 \nTQGen-Topic \nMoCo \n21.3 \n38.3 \n42.5 \n73.6 \n65.3 \nInBatch \n17.4 \n36.3 \n40.2 \n74.9 \n65.3 \nTQGen-AbSum \nMoCo \n21.2 \n37.2 \n41.3 \n74.5 \n65.4 \nInBatch \n18.2 \n36.7 \n40.2 \n75.6 \n65.4 \nTQGen-ExSum \nMoCo \n22.5 \n37.9 \n41.8 \n75.1 \n66.7 \n\nTable 1: Unsupervised scores (MM/BEIR nDCG@10 \nand ODQA Recall@20) of AUGTRIEVER and base-\nlines. MM denotes scores on MS MARCO. QA4 de-\nnotes averaged scores of NQ, TQA, WebQ and TREC. \nWe highlight the best and second best in each col-\numn, and best in each group per column. \n\ntan et al., 2022) on BEIR, which is 20 times larger \nthan AUGTRIEVER models. The empirical results \nstrongly suggest the effectiveness of the proposed \naugmentation methods. \nSpecifically, (1) 4.4.2 Unsupervised Domain-Adaptation \nResults \n\nRetrieval models are expected to generalize well \nwith data from a new domain or a new period of \n\nBM25 \n22.8 42.9 \n65.6 \n32.5 \n32.9 \n60.3 \n23.6 \n31.5 \n34.7 \n31.3 \n15.8 \n75.3 \n21.3 \n66.5 \n78.9 \n29.9 \nTQGen-Topic 21.5 39.6 \n61.3 \n32.1 \n27.9 \n51.7 \n25.6 \n39.4 \n21.0 \n30.3 \n15.1 \n62.5 \n15.2 \n66.8 \n78.1 \n27.6 \n+DA 23.1 41.1 \n71.0 \n31.1 \n28.2 \n53.1 \n33.6 \n18.4 \n19.4 \n32.3 \n17.3 \n72.9 \n16.1 \n64.8 \n80.8 \n35.8 \ndiff% \n7.3% 3.6% \n15.7% \n-3.1% 1.3% \n2.7% \n31.4% -53.2% -7.8% \n6.7% \n14.4% 16.7% \n5.6% \n-3.0% 3.5% \n29.7% \n\n\n\nTable 2 :\n2Result of INBATCH-TQGEN-TOPIC with unsupervised domain-adapation (DA) with BEIR in-domain data. Bold text indicates best scores among the three.\n\nTable 3 :\n3Retrieval scores after fine-tuning with MS MARCO. All AUGTRIEVER models are pre-trained on AUGQ-CC. We highlight the best and second best in each column.\n\n\nSukmin Cho, Soyeong Jeong, Wonsuk Yang, and Jong C Park. 2022. Query generation with external knowledge for dense retrieval. In Proceedings of Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Xiang Ji, and Xueqi Cheng. 2021b. Prop: pre-training with representative words prediction for ad-hoc retrieval. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining, pages 283-291. Jiawei Zhou, Xiaoguang Li, Lifeng Shang, Lan Luo, Ke Zhan, Enrui Hu, Xinyu Zhang, Hao Jiang, Zhao Cao, Fan Yu, Xin Jiang, Qun Liu, and Lei Chen. 2022. Hyperlink-induced pre-training for passage retrieval in open-domain question answering. In Proceedings of the 60th Annual Meeting of the As-Ricardo Baeza-Yates, Berthier Ribeiro-Neto, et al. \n1999. Modern information retrieval, volume 463. \nACM press New York. \n\nLuiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and \nRodrigo Nogueira. 2022. Inpars: Data augmenta-\ntion for information retrieval using large language \nmodels. arXiv preprint arXiv:2202.05144. \n\nWei-Cheng Chang, X Yu Felix, Yin-Wen Chang, Yim-\ning Yang, and Sanjiv Kumar. 2019. Pre-training \ntasks for embedding-based large-scale retrieval. In \nInternational Conference on Learning Representa-\ntions. \n\nXilun Chen, Kushal Lakhotia, Barlas Oguz, Anchit \nGupta, Patrick Lewis, Stan Peshterliev, Yashar \nMehdad, Sonal Gupta, and Wen-tau Yih. 2021. \nSalient phrase aware dense retrieval: Can a dense \nretriever imitate a sparse one? \narXiv preprint \narXiv:2110.06918. \n\nDeep Learning Inside Out (DeeLIO 2022): The 3rd \nWorkshop on Knowledge Extraction and Integration \nfor Deep Learning Architectures, pages 22-32. \n\nZhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo \nNi, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B \nHall, and Ming-Wei Chang. 2022. Promptagator: \nFew-shot dense retrieval from 8 examples. arXiv \npreprint arXiv:2209.11755. \n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and \nKristina Toutanova. 2019. BERT: Pre-training of \ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference \nof the North American Chapter of the Association \nfor Computational Linguistics: Human Language \nTechnologies, Volume 1 (Long and Short Papers), \npages 4171-4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics. \n\nHongchao Fang, Sicheng Wang, Meng Zhou, Jiayuan \nDing, and Pengtao Xie. 2020. Cert: Contrastive \nself-supervised learning for language understanding. \narXiv preprint arXiv:2005.12766. \n\nRevanth Gangi Reddy, Vikas Yadav, Md Arafat Sul-\ntan, Martin Franz, Vittorio Castelli, Heng Ji, and \nAvirup Sil. 2022. Towards robust neural retrieval \nwith source domain synthetic pre-finetuning. In \nProceedings of the 29th International Conference \non Computational Linguistics, pages 1065-1070, \nGyeongju, Republic of Korea. International Com-\nmittee on Computational Linguistics. \n\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020. \nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027. \n\nJiafeng Guo, Yinqiong Cai, Yixing Fan, Fei Sun, \nRuqing Zhang, and Xueqi Cheng. 2022. Semantic \nmodels for the first-stage retrieval: A comprehensive \nreview. ACM Transactions on Information Systems \n(TOIS), 40(4):1-42. \n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\nsupat, and Mingwei Chang. 2020. Retrieval aug-\nmented language model pre-training. In Inter-\nnational Conference on Machine Learning, pages \n3929-3938. PMLR. \n\nHiroki Iida and Naoaki Okazaki. 2022. Unsupervised \ndomain adaptation for sparse retrieval by filling vo-\ncabulary and word frequency gaps. In Proceedings \nof the 2nd Conference of the Asia-Pacific Chapter of \nthe Association for Computational Linguistics and \nthe 12th International Joint Conference on Natural \nLanguage Processing, pages 752-765. \n\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin, \nand Edouard Grave. 2021. Towards unsupervised \ndense information retrieval with contrastive learning. \narXiv e-prints, pages arXiv-2112. \n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick \nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and \nWen-tau Yih. 2020. Dense passage retrieval for \nopen-domain question answering. In Proceedings of \nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 6769-\n6781. \n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti, \nDanielle Epstein, Illia Polosukhin, Jacob Devlin, \nKenton Lee, et al. 2019. Natural questions: A bench-\nmark for question answering research. Transactions \nof the Association for Computational Linguistics, \n7:452-466. \n\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova. \n2019. Latent retrieval for weakly supervised open \ndomain question answering. In Proceedings of the \n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 6086-6096. \n\nPatrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale \nMinervini, Heinrich K\u00fcttler, Aleksandra Piktus, Pon-\ntus Stenetorp, and Sebastian Riedel. 2021. Paq: 65 \nmillion probably-asked questions and what you can \ndo with them. Transactions of the Association for \nComputational Linguistics, 9:1098-1115. \n\nDavis Liang, Peng Xu, Siamak Shakeri, Cicero \nNogueira dos Santos, Ramesh Nallapati, Zhiheng \nHuang, and Bing Xiang. 2020. Embedding-based \nzero-shot retrieval through query generation. arXiv \npreprint arXiv:2009.10270. \n\nJimmy Lin, Rodrigo Nogueira, and Andrew Yates. \n2021. Pretrained transformers for text ranking: Bert \nand beyond. Synthesis Lectures on Human Lan-\nguage Technologies, 14(4):1-325. \n\nYe Liu, Kazuma Hashimoto, Yingbo Zhou, Semih \nYavuz, Caiming Xiong, and S Yu Philip. 2021. \nDense hierarchical retrieval for open-domain ques-\ntion answering. In Findings of the Association for \nComputational Linguistics: EMNLP 2021, pages \n188-200. \n\nJi Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and \nRyan McDonald. 2021a. Zero-shot neural passage \nretrieval via domain-targeted synthetic question gen-\neration. In Proceedings of the 16th Conference of \nthe European Chapter of the Association for Com-\nputational Linguistics: Main Volume, pages 1075-\n1088. \n\nXinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, \nand Xueqi Cheng. 2022. Pre-train a discriminative \ntext encoder for dense retrieval via contrastive span \nprediction. arXiv preprint arXiv:2204.10641. \n\nXinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, \nYingyan Li, and Xueqi Cheng. 2021c. B-prop: boot-\nstrapped pre-training with representative words pre-\ndiction for ad-hoc retrieval. In Proceedings of the \n44th International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval, \npages 1513-1522. \n\nRui Meng, Sanqiang Zhao, Shuguang Han, Daqing \nHe, Peter Brusilovsky, and Yu Chi. 2017. Deep \nkeyphrase generation. In Proceedings of the 55th \nAnnual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pages \n582-592. \n\nArvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, \nJesse Michael Han, Jerry Tworek, Qiming Yuan, \nNikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. \n2022. Text and code embeddings by contrastive pre-\ntraining. arXiv preprint arXiv:2201.10005. \n\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, \nSaurabh Tiwary, Rangan Majumder, and Li Deng. \n2016. Ms marco: A human generated machine read-\ning comprehension dataset. \nOri Ram, Gal Shachaf, Omer Levy, Jonathan Be-\nrant, and Amir Globerson. 2021. Learning to re-\ntrieve passages without supervision. arXiv preprint \narXiv:2112.07708. \n\nDevendra Singh Sachan, Mike Lewis, Mandar Joshi, \nArmen Aghajanyan, Wen-tau Yih, Joelle Pineau, and \nLuke Zettlemoyer. 2022a. Improving passage re-\ntrieval with zero-shot question generation. arXiv \npreprint arXiv:2204.07496. \n\nDevendra Singh Sachan, Mike Lewis, Dani Yogatama, \nLuke Zettlemoyer, Joelle Pineau, and Manzil Zaheer. \n2022b. Questions are all you need to train a dense \npassage retriever. arXiv preprint arXiv:2206.10658. \n\nAbigail See, Peter J Liu, and Christopher D Manning. \n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational \nLinguistics (Volume 1: Long Papers), pages 1073-\n1083. \n\nXiaoyu Shen, Svitlana Vakulenko, Marco Del Tredici, \nGianni Barlacchi, Bill Byrne, and Adri\u00e0 de Gis-\npert. 2022. Low-resource dense retrieval for open-\ndomain question answering: A comprehensive sur-\nvey. arXiv preprint arXiv:2208.03197. \n\nZilu Tang, Muhammed Yusuf Kocyigit, and \nDerry Tanti Wijaya. 2022. Augcse: Contrastive \nsentence embedding with diverse augmentations. \nIn Proceedings of the 2nd Conference of the \nAsia-Pacific Chapter of the Association for Compu-\ntational Linguistics and the 12th International Joint \nConference on Natural Language Processing, pages \n375-398. \n\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. Beir: \nA heterogeneous benchmark for zero-shot evalua-\ntion of information retrieval models. In Thirty-fifth \nConference on Neural Information Processing Sys-\ntems Datasets and Benchmarks Track (Round 2). \n\nKexin Wang, Nandan Thakur, Nils Reimers, and Iryna \nGurevych. 2022. GPL: Generative pseudo label-\ning for unsupervised domain adaptation of dense re-\ntrieval. In Proceedings of the 2022 Conference of \nthe North American Chapter of the Association for \nComputational Linguistics: Human Language Tech-\nnologies, pages 2345-2360, Seattle, United States. \nAssociation for Computational Linguistics. \n\nJiawen Wu, Xinyu Zhang, Yutao Zhu, Zheng Liu, Zikai \nGuo, Zhaoye Fei, Ruofei Lai, Yongkang Wu, Zhao \nCao, and Zhicheng Dou. 2022. Pre-training for in-\nformation retrieval: Are hyperlinks fully explored? \narXiv preprint arXiv:2209.06583. \n\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, \nJialin Liu, Paul Bennett, Junaid Ahmed, and Arnold \nOverwijk. 2020. Approximate nearest neighbor neg-\native contrastive learning for dense text retrieval. \narXiv preprint arXiv:2007.00808. \n\nCanwen Xu, Daya Guo, Nan Duan, and Julian \nMcAuley. 2022. Laprador: Unsupervised pretrained \ndense retriever for zero-shot text retrieval. In Find-\nings of the Association for Computational Linguis-\ntics: ACL 2022, pages 3557-3569. \n\nNan Yang, Furu Wei, Binxing Jiao, Daxing Jiang, and \nLinjun Yang. 2021. xmoco: Cross momentum con-\ntrastive learning for open-domain question answer-\ning. In Proceedings of the 59th Annual Meeting of \nthe Association for Computational Linguistics and \nthe 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers), \npages 6120-6129. \n\nYue Yu, Chenyan Xiong, Si Sun, Chao Zhang, and \nArnold Overwijk. 2022. Coco-dr: Combating dis-\ntribution shifts in zero-shot dense retrieval with con-\ntrastive and distributionally robust learning. arXiv \npreprint arXiv:2210.15212. \n\nWayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-\nRong Wen. 2022. Dense text retrieval based on pre-\ntrained language models: A survey. arXiv preprint \narXiv:2211.14876. \n\nsociation for Computational Linguistics (Volume 1: \nLong Papers), pages 7135-7146, Dublin, Ireland. \nAssociation for Computational Linguistics. \n\n\n\nTable 5 :\n5Examples of generated pseudo queries by TQGEN and QGEN.NQ \n\nTriviaQA \nWQ \nCuratedTREC \nSQuAD v1.1 \nEQ (Macro-Avg) \nAverage \n\nenwiki-20211020-pages-articles-multistream.xml.bz2\n\n73] adjustments, or in some locations to claim disability [14.05] partly because autistic symptoms overlap with those [85.62] completed diagnosis and assessment, though few cases are [13.55] completed diagnosis and assessment, though few cases are [90.46] some locations to claim disability living allowances or. though few cases are handled [79.. 12.06] adjustments, or in some locations to claim disability [102.39] diagnosis and assessment, though few cases are handled [11.05] some locations to claim disability living allowances or [105.74] partly because autistic symptoms overlap with those [10.48] the challenge of obtaining payment can [112.83] and assessment, though few cases are handled09] diagnosis and assessment, though few cases are handled [79.73] adjustments, or in some locations to claim disability [14.05] partly because autistic symptoms overlap with those [85.62] completed diagnosis and assessment, though few cases are [13.55] completed diagnosis and assessment, though few cases are [90.46] some locations to claim disability living allowances or [12.06] adjustments, or in some locations to claim disability [102.39] diagnosis and assessment, though few cases are handled [11.05] some locations to claim disability living allowances or [105.74] partly because autistic symptoms overlap with those [10.48] the challenge of obtaining payment can [112.83] and assessment, though few cases are handled\n\nASD begin early in childhood. ASD begin early in childhood. 120.57] family understand themselves, to help their [9.42] in some locations to claim disability [127.31] Conversely, the cost of screening [9.38] and assessment, though few cases are handled [135.09] the challenge of obtaining payment can [8.45] overlap with those of common blindness [139.24] overlap with those of common blindness [8.15] family understand themselves, to help their [139.38] friends and family understand themselves, [7.03] facial expressions and eye [141.17] in some locations to claim disability [6.77] friends and family understand themselves. 147.92] a few years laterConversely, the cost of screening [115.09] ASD begin early in childhood, [9.67] ASD begin early in childhood, [120.57] family understand themselves, to help their [9.42] in some locations to claim disability [127.31] Conversely, the cost of screening [9.38] and assessment, though few cases are handled [135.09] the challenge of obtaining payment can [8.45] overlap with those of common blindness [139.24] overlap with those of common blindness [8.15] family understand themselves, to help their [139.38] friends and family understand themselves, [7.03] facial expressions and eye [141.17] in some locations to claim disability [6.77] friends and family understand themselves, [147.92] a few years later.\n\nNaH + HO \u2192 NaOH + H Reaction [227.13] they form sulphides and polysulphides. 2Na + 1/8S [23.74] 2NaCl Alkali metals in liquid ammonia Alkali metals [256.10] give dilithium acetylide. Na and K can react [22.44] + HO \u2192 NaOH + H Reaction [298.67] Reaction with sulphur With sulphur. Hs + Ho, Hs, Ho \u2192, 207.94] 2NaCl Alkali metals in liquid ammonia Alkali metals [25.02. 20.70] Na + xNH \u2192 Na + [304.82] dilithium acetylide. Na and K [18.01] + 1/3N \u2192 LiN LiN [309.33] 4NaCl + Ti Reaction with organohalide [18.00] (at 150C) Na + NaCH \u2192 [312.97] Because alkali metal sulphides are essentially salts ofHS + HO HS + HO \u2192 [207.94] 2NaCl Alkali metals in liquid ammonia Alkali metals [25.02] NaH + HO \u2192 NaOH + H Reaction [227.13] they form sulphides and polysulphides. 2Na + 1/8S [23.74] 2NaCl Alkali metals in liquid ammonia Alkali metals [256.10] give dilithium acetylide. Na and K can react [22.44] + HO \u2192 NaOH + H Reaction [298.67] Reaction with sulphur With sulphur, [20.70] Na + xNH \u2192 Na + [304.82] dilithium acetylide. Na and K [18.01] + 1/3N \u2192 LiN LiN [309.33] 4NaCl + Ti Reaction with organohalide [18.00] (at 150C) Na + NaCH \u2192 [312.97] Because alkali metal sulphides are essentially salts of\n\nreaction with oxygen. The compound in brackets [320.61] NaH + HO \u2192 NaOH + H Reaction [16.25] give dilithium acetylide. Na and K can react [321.21] + 2HO \u2192 2KOH + HO + [15.73] they form sulphides and polysulphides. 2Na + 1/8S [327.07] give dilithium acetylide. Na [15.60] Reaction with sulphur With sulphur. + 2ho \u2192 2koh + Ho + ; Nah, + Ho \u2192 ; Hs + Ho, Hs, Ho \u2192, 17.30Alkali metals dissolve in liquid ammonia or [337.68] the case of Rb and Cs. Na [15.30] Because alkali metal sulphides are essentially salts of. 347.56] (at 150C) Na + NaCH \u2192 [14.90] the case of Rb and Cs. Na [348.45] 1/2H (at 150C) Na + NaCH+ 2HO \u2192 2KOH + HO + [315.61] peroxides are ionic compounds that are unstable in [17.36] high temperatures) NaH + HO \u2192 [316.52] HS + HO HS + HO \u2192 [17.30] reaction with oxygen. The compound in brackets [320.61] NaH + HO \u2192 NaOH + H Reaction [16.25] give dilithium acetylide. Na and K can react [321.21] + 2HO \u2192 2KOH + HO + [15.73] they form sulphides and polysulphides. 2Na + 1/8S [327.07] give dilithium acetylide. Na [15.60] Reaction with sulphur With sulphur, [336.63] Na + e(NH) Due to the presence [15.39] Alkali metals dissolve in liquid ammonia or [337.68] the case of Rb and Cs. Na [15.30] Because alkali metal sulphides are essentially salts of [347.56] (at 150C) Na + NaCH \u2192 [14.90] the case of Rb and Cs. Na [348.45] 1/2H (at 150C) Na + NaCH\n", "annotations": {"author": "[{\"end\":106,\"start\":74},{\"end\":114,\"start\":107},{\"end\":127,\"start\":115},{\"end\":145,\"start\":128},{\"end\":154,\"start\":146},{\"end\":163,\"start\":155},{\"end\":178,\"start\":164},{\"end\":192,\"start\":179},{\"end\":205,\"start\":193},{\"end\":226,\"start\":206}]", "publisher": null, "author_last_name": "[{\"end\":82,\"start\":78},{\"end\":113,\"start\":110},{\"end\":126,\"start\":121},{\"end\":144,\"start\":137},{\"end\":153,\"start\":151},{\"end\":162,\"start\":160},{\"end\":177,\"start\":172},{\"end\":191,\"start\":187},{\"end\":204,\"start\":200},{\"end\":225,\"start\":217}]", "author_first_name": "[{\"end\":77,\"start\":74},{\"end\":109,\"start\":107},{\"end\":120,\"start\":115},{\"end\":136,\"start\":128},{\"end\":150,\"start\":146},{\"end\":159,\"start\":155},{\"end\":171,\"start\":164},{\"end\":186,\"start\":179},{\"end\":199,\"start\":193},{\"end\":216,\"start\":206}]", "author_affiliation": null, "title": "[{\"end\":71,\"start\":1},{\"end\":297,\"start\":227}]", "venue": null, "abstract": "[{\"end\":1405,\"start\":299}]", "bib_ref": "[{\"end\":2103,\"start\":2076},{\"end\":3105,\"start\":3084},{\"end\":3340,\"start\":3318},{\"end\":3357,\"start\":3340},{\"end\":3382,\"start\":3357}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35669,\"start\":34644},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35840,\"start\":35670},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36152,\"start\":35841},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36287,\"start\":36153},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39724,\"start\":36288},{\"attributes\":{\"id\":\"fig_5\"},\"end\":40266,\"start\":39725},{\"attributes\":{\"id\":\"fig_6\"},\"end\":41005,\"start\":40267},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":44597,\"start\":41006},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":44754,\"start\":44598},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":44920,\"start\":44755},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":56198,\"start\":44921},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":56335,\"start\":56199}]", "paragraph": "[{\"end\":1958,\"start\":1421},{\"end\":3198,\"start\":1960},{\"end\":4485,\"start\":3200},{\"end\":4932,\"start\":4487},{\"end\":5228,\"start\":4934},{\"end\":5716,\"start\":5230},{\"end\":5896,\"start\":5718},{\"end\":6164,\"start\":5898},{\"end\":7149,\"start\":6175},{\"end\":7358,\"start\":7177},{\"end\":8225,\"start\":7405},{\"end\":8270,\"start\":8227},{\"end\":8389,\"start\":8272},{\"end\":8585,\"start\":8391},{\"end\":9056,\"start\":8587},{\"end\":10022,\"start\":9097},{\"end\":10080,\"start\":10024},{\"end\":10136,\"start\":10082},{\"end\":10174,\"start\":10138},{\"end\":10223,\"start\":10176},{\"end\":10260,\"start\":10225},{\"end\":10327,\"start\":10262},{\"end\":10930,\"start\":10329},{\"end\":12020,\"start\":10957},{\"end\":12461,\"start\":12022},{\"end\":13475,\"start\":12488},{\"end\":14203,\"start\":13489},{\"end\":14956,\"start\":14248},{\"end\":17102,\"start\":14958},{\"end\":17464,\"start\":17125},{\"end\":18607,\"start\":17466},{\"end\":21480,\"start\":18627},{\"end\":22046,\"start\":21525},{\"end\":22386,\"start\":22048},{\"end\":22904,\"start\":22388},{\"end\":23121,\"start\":22920},{\"end\":23261,\"start\":23123},{\"end\":23848,\"start\":23263},{\"end\":23997,\"start\":23850},{\"end\":24237,\"start\":23999},{\"end\":25079,\"start\":24258},{\"end\":25293,\"start\":25147},{\"end\":26772,\"start\":25327},{\"end\":27293,\"start\":26797},{\"end\":27879,\"start\":27319},{\"end\":27962,\"start\":27907},{\"end\":28378,\"start\":27964},{\"end\":28884,\"start\":28380},{\"end\":30029,\"start\":28886},{\"end\":30409,\"start\":30049},{\"end\":30616,\"start\":30429},{\"end\":30709,\"start\":30676},{\"end\":31393,\"start\":30711},{\"end\":31506,\"start\":31418},{\"end\":31614,\"start\":31508},{\"end\":31745,\"start\":31616},{\"end\":31800,\"start\":31747},{\"end\":32405,\"start\":31802},{\"end\":32647,\"start\":32492},{\"end\":32932,\"start\":32649},{\"end\":32994,\"start\":32934},{\"end\":33045,\"start\":32996},{\"end\":33989,\"start\":33047},{\"end\":34391,\"start\":34025},{\"end\":34643,\"start\":34393}]", "formula": null, "table_ref": "[{\"end\":14299,\"start\":14292},{\"end\":16163,\"start\":16154},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":16263,\"start\":16256},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":17275,\"start\":17268},{\"end\":26882,\"start\":26875},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":27413,\"start\":27406},{\"end\":29862,\"start\":29855}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1419,\"start\":1407},{\"attributes\":{\"n\":\"3\"},\"end\":6173,\"start\":6167},{\"attributes\":{\"n\":\"3.1\"},\"end\":7175,\"start\":7152},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":7403,\"start\":7361},{\"attributes\":{\"n\":\"3.2\"},\"end\":9095,\"start\":9059},{\"attributes\":{\"n\":\"4\"},\"end\":10944,\"start\":10933},{\"attributes\":{\"n\":\"4.1\"},\"end\":10955,\"start\":10947},{\"attributes\":{\"n\":\"4.2\"},\"end\":12486,\"start\":12464},{\"attributes\":{\"n\":\"4.3\"},\"end\":13487,\"start\":13478},{\"attributes\":{\"n\":\"4.4\"},\"end\":14213,\"start\":14206},{\"attributes\":{\"n\":\"4.4.1\"},\"end\":14246,\"start\":14216},{\"attributes\":{\"n\":\"4.4.3\"},\"end\":17123,\"start\":17105},{\"attributes\":{\"n\":\"4.5\"},\"end\":18625,\"start\":18610},{\"attributes\":{\"n\":\"5\"},\"end\":21495,\"start\":21483},{\"attributes\":{\"n\":\"6\"},\"end\":21523,\"start\":21498},{\"end\":22918,\"start\":22907},{\"end\":24256,\"start\":24240},{\"end\":25092,\"start\":25082},{\"end\":25105,\"start\":25095},{\"end\":25145,\"start\":25108},{\"end\":25325,\"start\":25296},{\"end\":26795,\"start\":26775},{\"end\":27317,\"start\":27296},{\"end\":27905,\"start\":27882},{\"end\":30047,\"start\":30032},{\"end\":30427,\"start\":30412},{\"end\":30674,\"start\":30619},{\"end\":31416,\"start\":31396},{\"end\":32490,\"start\":32408},{\"end\":34023,\"start\":33992},{\"end\":35676,\"start\":35671},{\"end\":35852,\"start\":35842},{\"end\":36164,\"start\":36154},{\"end\":39736,\"start\":39726},{\"end\":44608,\"start\":44599},{\"end\":44765,\"start\":44756},{\"end\":56209,\"start\":56200}]", "table": "[{\"end\":44597,\"start\":41719},{\"end\":56198,\"start\":45620},{\"end\":56335,\"start\":56266}]", "figure_caption": "[{\"end\":35669,\"start\":34646},{\"end\":35840,\"start\":35678},{\"end\":36152,\"start\":35854},{\"end\":36287,\"start\":36166},{\"end\":39724,\"start\":36290},{\"end\":40266,\"start\":39738},{\"end\":41005,\"start\":40269},{\"end\":41719,\"start\":41008},{\"end\":44754,\"start\":44610},{\"end\":44920,\"start\":44767},{\"end\":45620,\"start\":44923},{\"end\":56266,\"start\":56211}]", "figure_ref": "[{\"end\":6765,\"start\":6757},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18882,\"start\":18876},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21031,\"start\":21023},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":25429,\"start\":25421},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27938,\"start\":27930}]", "bib_author_first_name": "[{\"end\":60685,\"start\":60671},{\"end\":60699,\"start\":60698}]", "bib_author_last_name": "[{\"end\":59458,\"start\":59451},{\"end\":59462,\"start\":59460},{\"end\":59468,\"start\":59464},{\"end\":60696,\"start\":60686},{\"end\":60714,\"start\":60700},{\"end\":60718,\"start\":60716},{\"end\":60724,\"start\":60720}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":57812,\"start\":56388},{\"attributes\":{\"id\":\"b1\"},\"end\":59169,\"start\":57814},{\"attributes\":{\"id\":\"b2\"},\"end\":60362,\"start\":59171},{\"attributes\":{\"id\":\"b3\"},\"end\":61721,\"start\":60364}]", "bib_title": null, "bib_author": "[{\"end\":59460,\"start\":59451},{\"end\":59464,\"start\":59460},{\"end\":59470,\"start\":59464},{\"end\":60698,\"start\":60671},{\"end\":60716,\"start\":60698},{\"end\":60720,\"start\":60716},{\"end\":60726,\"start\":60720}]", "bib_venue": "[{\"end\":56699,\"start\":56388},{\"end\":57842,\"start\":57814},{\"end\":59449,\"start\":59171},{\"end\":60669,\"start\":60364}]"}}}, "year": 2023, "month": 12, "day": 17}