{"id": 1148017, "updated": "2023-11-10 23:11:29.894", "metadata": {"title": "A Normative Theory of Adaptive Dimensionality Reduction in Neural Networks", "authors": "[{\"first\":\"Cengiz\",\"last\":\"Pehlevan\",\"middle\":[]},{\"first\":\"Dmitri\",\"last\":\"Chklovskii\",\"middle\":[\"B.\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2015, "month": 11, "day": 30}, "abstract": "To make sense of the world our brains must analyze high-dimensional datasets streamed by our sensory organs. Because such analysis begins with dimensionality reduction, modelling early sensory processing requires biologically plausible online dimensionality reduction algorithms. Recently, we derived such an algorithm, termed similarity matching, from a Multidimensional Scaling (MDS) objective function. However, in the existing algorithm, the number of output dimensions is set a priori by the number of output neurons and cannot be changed. Because the number of informative dimensions in sensory inputs is variable there is a need for adaptive dimensionality reduction. Here, we derive biologically plausible dimensionality reduction algorithms which adapt the number of output dimensions to the eigenspectrum of the input covariance matrix. We formulate three objective functions which, in the offline setting, are optimized by the projections of the input dataset onto its principal subspace scaled by the eigenvalues of the output covariance matrix. In turn, the output eigenvalues are computed as i) soft-thresholded, ii) hard-thresholded, iii) equalized thresholded eigenvalues of the input covariance matrix. In the online setting, we derive the three corresponding adaptive algorithms and map them onto the dynamics of neuronal activity in networks with biologically plausible local learning rules. Remarkably, in the last two networks, neurons are divided into two classes which we identify with principal neurons and interneurons in biological circuits.", "fields_of_study": "[\"Biology\",\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2964083467", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/PehlevanC15a", "doi": null}}, "content": {"source": {"pdf_hash": "cc37bc84ec1de3af58e8d1dd6bb916b074048f99", "pdf_src": "ArXiv", "pdf_uri": "[\"https://arxiv.org/pdf/1511.09426v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b6ecd55f4e64569d2174724957f65a9b9a0c176a", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/cc37bc84ec1de3af58e8d1dd6bb916b074048f99.txt", "contents": "\nA Normative Theory of Adaptive Dimensionality Reduction in Neural Networks\n26 Jan 2016\n\nCengiz Pehlevan cpehlevan@simonsfoundation.org \nSimons Center for Data Analysis\nSimons Foundation New York\n10010NY\n\nDmitri B Chklovskii dchklovskii@simonsfoundation.org \nSimons Center for Data Analysis\nSimons Foundation New York\n10010NY\n\nA Normative Theory of Adaptive Dimensionality Reduction in Neural Networks\n26 Jan 2016C2113CCE96D224F5C4F71D2447384461arXiv:1511.09426v2[q-bio.NC]\nTo make sense of the world our brains must analyze high-dimensional datasets streamed by our sensory organs.Because such analysis begins with dimensionality reduction, modeling early sensory processing requires biologically plausible online dimensionality reduction algorithms.Recently, we derived such an algorithm, termed similarity matching, from a Multidimensional Scaling (MDS) objective function.However, in the existing algorithm, the number of output dimensions is set a priori by the number of output neurons and cannot be changed.Because the number of informative dimensions in sensory inputs is variable there is a need for adaptive dimensionality reduction.Here, we derive biologically plausible dimensionality reduction algorithms which adapt the number of output dimensions to the eigenspectrum of the input covariance matrix.We formulate three objective functions which, in the offline setting, are optimized by the projections of the input dataset onto its principal subspace scaled by the eigenvalues of the output covariance matrix.In turn, the output eigenvalues are computed as i) soft-thresholded, ii) hard-thresholded, iii) equalized thresholded eigenvalues of the input covariance matrix.In the online setting, we derive the three corresponding adaptive algorithms and map them onto the dynamics of neuronal activity in networks with biologically plausible local learning rules.Remarkably, in the last two networks, neurons are divided into two classes which we identify with principal neurons and interneurons in biological circuits.\n\nIntroduction\n\nOur brains analyze high-dimensional datasets streamed by our sensory organs with efficiency and speed rivaling modern computers.At the early stage of such analysis, the dimensionality of sensory inputs is drastically reduced as evidenced by anatomical measurements.Human retina, for example, conveys signals from \u2248125 million photoreceptors to the rest of the brain via \u22481 million ganglion cells [1] suggesting a hundred-fold dimensionality reduction.Therefore, biologically plausible dimensionality reduction algorithms may offer a model of early sensory processing.\n\nIn a seminal work [2] Oja proposed that a single neuron may compute the first principal component of activity in upstream neurons.At each time point, Oja's neuron projects a vector composed of firing rates of upstream neurons onto the vector of synaptic weights by summing up currents generated by its synapses.In turn, synaptic weights are adjusted according to a Hebbian rule depending on the activities of only the postsynaptic and corresponding presynaptic neurons [2].Following Oja's work, many multineuron circuits were proposed to extract multiple principal components of the input, for a review see [3].However, most multineuron algorithms did not meet the same level of rigor and biological plausibility as the single-neuron algorithm [2,4] which can be derived using a normative approach, from a principled objective function [5], and contains only lo-cal Hebbian learning rules.Algorithms derived from principled objective functions either did not posess local learning rules [6,4,7,8] or had other biologically implausible features [9].In other algorithms, local rules were chosen heuristically rather than derived from a principled objective function [10,11,12,9,3,13,14,15,16].\n\nThere is a notable exception to the above observation but it has other shortcomings.The twolayer circuit with reciprocal synapses [17,18,19] can be derived from the minimization of the representation error.However, the activity of principal neurons in the circuit is a dummy variable without its own dynamics.Therefore, such principal neurons do not integrate their input in time, contradicting existing experimental observations.\n\nOther normative approaches use an information theoretical objective to compare theoretical limits with experimentally measured information in single neurons or populations [20,21,22] or to calculate optimal synaptic weights in a postulated neural network [23,22].\n\nRecently, a novel approach to the problem has been proposed [24].Starting with the Multidimensional Scaling (MDS) strain cost function [25,26] we derived an algorithm which maps onto a neuronal circuit with local learning rules.However, [24] had major limitations, which are shared by vairous other multineuron algorithms:\n\n1.The number of output dimensions was determined by the fixed number of output neurons precluding adaptation to the varying number of informative components.A better solution would be to let the network decide, depending on the input statistics, how many dimensions to represent [14,15].The dimensionality of neural activity in such a network would be usually less than the maximum set by the number of neurons.2. Because output neurons were coupled by anti-Hebbian synapses which are most naturally implemented by inhibitory synapses, if these neurons were to have excitatory outputs, as suggested by cortical anatomy, they would violate Dale's law (i.e. each neuron uses only one fast neurotransmitter).Here, following [10], by anti-Hebbian we mean synaptic weights that get more negative with correlated activity of pre-and postsynaptic neurons.3. The output had a wide dynamic range which is difficult to implement using biological neurons with a limited range.A better solution [27,13] is to equalize the output variance across neurons.\n\nIn this paper, we advance the normative approach of [24] by proposing three new objective functions which allow us to overcome the above limitations.We optimize these objective functions by proceeding as follows.In Section 2, we formulate and solve three optimization problems of the form:\n\nOffline setting :\nY * = arg min Y L (X, Y) .(1)\nHere, the input to the network, X = [x 1 , . . ., x T ] is an n \u00d7 T matrix with T centered input data samples in R n as its columns and the output of the network, Y = [y 1 , . . ., y T ] is a k\u00d7T matrix with corresponding outputs in R k as its columns.We assume T >> k and T >> n.Such optimization problems are posed in the so-called offline setting where outputs are computed after seeing all data.\n\nWhereas the optimization problems in the offline setting admit closed-form solution, such setting is ill-suited for modeling neural computation on the mechanistic level and must be replaced by the online setting.Indeed, neurons compute an output, y T , for each data sample presentation, x T , before the next data sample is presented and past outputs cannot be altered.In such online setting, optimization is performed at every time step, T , on the objective which is a function of all inputs and outputs up to time T .Moreover, an online algorithm (also known as streaming) is not capable of storing all previous inputs and outputs and must rely on a smaller number of state variables.\n\nIn Section 3, we formulate three corresponding online optimization problems with respect to y T , while keeping all the previous outputs fixed:\n\nOnline setting : y T \u2190 arg min\ny T L (X, Y) .(2)\nThen we derive algorithms solving these problems online and map their steps onto the dynamics of neuronal activity and local learning rules for synaptic weights in three neural networks.\n\nWe show that the solutions of the optimization problems and the corresponding online algorithms remove the limitations outlined above by performing the following computational tasks:\n\nx 1\n\nx n . . .  1. Soft-thresholding the eigenvalues of the input covariance matrix, Figure 1A: eigenvalues below the threshold are set to zero and the rest are shrunk by the threshold magnitude.Thus, the number of output dimensions is chosen adaptively.This algorithm maps onto a single-layer neural network with the same architecture as in [24], Figure 1D, but with modified learning rules.2. Hard-thresholding of input eigenvalues, Figure 1B: eigenvalues below the threshold vanish as before, but eigenvalues above the threshold remain unchanged.The steps of such algorithm map onto the dynamics of neuronal activity in a network which, in addition to principal neurons, has a layer of interneurons reciprocally connected with principal neurons and each other, Figure 1E.In Section 4, we demonstrate that the online algorithms perform well on a synthetic dataset and, in Discussion, we compare our neural circuits with biological observations.\n\n\nDimensionality reduction in the offline setting\n\nIn this Section, we introduce and solve, in the offline setting, three novel optimization problems whose solutions reduce the dimensionality of the input.We state our results in three Theorems which are proved in the Supplementary Material.\n\n\nSoft-thresholding of covariance eigenvalues\n\nWe consider the following optimization problem in the offline setting:\nmin Y X X \u2212 Y Y \u2212 \u03b1T I T 2 F ,(3)\nwhere \u03b1 \u2265 0 and I T is the T \u00d7T identity matrix.To gain intuition behind this choice of the objective function let us expand the squared norm and keep only the Y-dependent terms:\narg min Y X X \u2212 Y Y \u2212 \u03b1T I T 2 F = arg min Y X X \u2212 Y Y 2 F + 2\u03b1T Tr Y Y ,(4)\nwhere the first term matches the similarity of input and output [24] and the second term is a nuclear norm of Y Y known to be a convex relaxation of the matrix rank used for low-rank matrix modeling [28].Thus, objective function (3) enforces low-rank similarity matching.\n\nWe show that the optimal output Y is a projection of the input data, X, onto its principal subspace.The subspace dimensionality is set by m, the number of eigenvalues of the data covariance matrix,\nC = 1 T XX = 1 T T t=1\nx t x t , that are greater than or equal to the parameter \u03b1.\n\nTheorem 1. Suppose an eigen-decomposition of X X = V X \u039b X V X , where \u039b X = diag \u03bb X 1 , . . ., \u03bb X T with \u03bb X 1 \u2265 . . .\u2265 \u03bb X T .Note that \u039b X has at most n nonzero eigenvalues coinciding with those of T C.Then,\nY * = U k ST k (\u039b X , \u03b1T ) 1/2 V X k ,(5)\nare optima of (3), where (5) uniquely defines all optima of (3), except when k < m, \u03bb X k > \u03b1T and \u03bb X k = \u03bb X k+1 .\nST k (\u039b X , \u03b1T ) = diag ST \u03bb X 1 , \u03b1T , . . . , ST \u03bb X k , \u03b1T , ST is the soft- thresholding function, ST(a, b) = max(a \u2212 b, 0), V X k consists of the columns of V X corresponding to the top k eigenvalues, i.e. V X k = v X 1 , . . . , v X k and U k is any k \u00d7 k orthogonal matrix, i.e. U k \u2208 O(k). The form\n\nHard-thresholding of covariance eigenvalues\n\nConsider the following minimax problem in the offline setting:\nmin Y max Z X X \u2212 Y Y 2 F \u2212 Y Y \u2212 Z Z \u2212 \u03b1T I T 2 F ,(6)\nwhere \u03b1 \u2265 0 and we introduced an internal variable Z, which is an l \u00d7 T matrix Z = [z 1 , . . ., z T ] with z t \u2208 R l .The intuition behind this objective function is again based on similarity matching but rank regularization is applied indirectly via the internal variable, Z.\n\nTheorem 2. Suppose an eigen-decomposition of\nX X = V X \u039b X V X , where \u039b X = diag \u03bb X 1 , . . . , \u03bb X T with \u03bb X 1 \u2265 . . . \u2265 \u03bb X T \u2265 0. Assume l \u2265 min(k, m). Then, Y * = U k HT k (\u039b X , \u03b1T ) 1/2 V X k , Z * = U l ST l,min(k,m) (\u039b X , \u03b1T ) 1/2 V X l ,(7)\nare optima of (6), where\nHT k (\u039b X , \u03b1T ) = diag HT \u03bb X 1 , \u03b1T , . . . , HT \u03bb X k , \u03b1T , HT(a, b) = a\u0398(a \u2212 b) with \u0398() being the step function: \u0398(a \u2212 b) = 1 if a \u2265 b and \u0398(a \u2212 b) = 0 if a < b, ST l,min(k,m) (\u039b X , \u03b1T ) = diag ST \u03bb X 1 , \u03b1T , . . . , ST \u03bb X min(k,m) , \u03b1T , 0, . . . , 0 l\u2212min(k,m) ,V X p = v X 1 , . . . , v X p\nand U p \u2208 O(p).The form (7) uniquely defines all optima (6) except when either 1) \u03b1 is an eigenvalue of C or 2) k < m and \u03bb X k = \u03bb X k+1 .\n\n\nEqualizing thresholded covariance eigenvalues\n\nConsider the following minimax problem in the offline setting: min\nY max Z Tr \u2212X XY Y + Y YZ Z + \u03b1T Y Y \u2212 \u03b2T Z Z ,(8)\nwhere \u03b1 \u2265 0 and \u03b2 > 0. This objective function follows from (6) after dropping the quartic Z term.\nTheorem 3. Suppose an eigen-decomposition of X X is X X = V X \u039b X V X , where \u039b X = diag \u03bb X 1 , . . . , \u03bb X T with \u03bb X 1 \u2265 . . . \u2265 \u03bb X T \u2265 0. Assume l \u2265 min(k, m). Then, Y * = U k \u03b2T \u0398 k (\u039b X , \u03b1T ) 1/2 V X k , Z * = U l \u03a3 l\u00d7T O \u039b Y * V X ,(9)\nare optima of (8), where \u0398 k (\u039b X , \u03b1T ) = diag \u0398 \u03bb X 1 \u2212 \u03b1T , . . ., \u0398 \u03bb X k \u2212 \u03b1T , \u03a3 l\u00d7T is an l \u00d7 T rectangular diagonal matrix with top min(k, m) diagonals are set to arbitrary nonnegative constants and the rest are zero, O \u039b Y * is a block-diagonal orthogonal matrix that has two blocks: the top block is min(k, m) dimensional and the bottom block is T \u2212 min(k, m) dimensional, V p = v X 1 , . . ., v X p , and U p \u2208 O(p).The form (9) uniquely defines all optima of (8) except when either 1) \u03b1 is an eigenvalue of C or 2) k < m and \u03bb X k = \u03bb X k+1 .Remark 1.If k = m, then Y is full-rank and 1\n\nT YY = \u03b2I k , implying that the output is whitened, equalizing variance across all channels.\n\n3 Online dimensionality reduction using Hebbian/anti-Hebbian neural nets\n\nIn this Section, we formulate online versions of the dimensionality reduction optimization problems presented in the previous Section, derive corresponding online algorithms and map them onto the dynamics of neural networks with biologically plausible local learning rules.The order of subsections corresponds to that in the previous Section.\n\n\nOnline soft-thresholding of eigenvalues\n\nConsider the following optimization problem in the online setting:\ny T \u2190 arg min y T X X \u2212 Y Y \u2212 \u03b1T I T 2 F .(10)\nBy keeping only the terms that depend on y T we get the following objective for (2):\nL = \u22124x T T \u22121 t=1 x t y t y T + 2y T T \u22121 t=1 y t y t + \u03b1T I m y T \u2212 2 x T 2 y T 2 + y T 4 . (11)\nIn the large-T limit, the last two terms can be dropped since the first two terms grow linearly with T and dominate.The remaining cost is a positive definite quadratic form in y T and the optimization problem is convex.At its minimum, the following equality holds:\nT \u22121 t=1 y t y t + \u03b1T I m y T = T \u22121 t=1 y t x t x T .(12)\nWhile a closed-form analytical solution via matrix inversion exists for y T , we are interested in biologically plausible algorithms.Instead, we use a weighted Jacobi iteration where y T is updated according to:\ny T \u2190 (1 \u2212 \u03b7) y T + \u03b7 W Y X T x T \u2212 W Y Y T y T , (13)\nwhere \u03b7 is the weight parameter, and W Y X T and W Y Y T are normalized input-output and outputoutput covariances,\nW Y X T,ik = T \u22121 t=1 y t,i x t,k \u03b1T + T \u22121 t=1 y 2 t,i , W Y Y T,i,j =i = T \u22121 t=1 y t,i y t,j \u03b1T + T \u22121 t=1 y 2 t,i , W Y Y T,ii = 0.(14)\nIteration ( 13) can be implemented by the dynamics of neuronal activity in a single-layer network, Figure 1D.Then, W Y X T and W Y Y T represent the weights of feedforward (x t \u2192 y t ) and lateral (y t \u2192 y t ) synaptic connections, respectively.Remarkably, synaptic weights appear in the online solution despite their absence in the optimization problem formulation (3).Previously, nonnormalized covariances have been used as state variables in an online dictionary learning algorithm [29].\n\nTo formulate a fully online algorithm, we rewrite (14) in a recursive form.This requires introducing a scalar variable D Y T,i representing cumulative activity of a neuron i up to time\nT \u2212 1, D Y T,i = \u03b1T + T \u22121 t=1 y 2 t,i\n. Then, at each data sample presentation, T , after the output y T converges to a steady state, the following updates are performed:\nD Y T +1,i \u2190 D Y T,i + \u03b1 + y 2 T,i , W Y X T +1,ij \u2190 W Y X T,ij + y T,i x T,j \u2212 \u03b1 + y 2 T,i W Y X T,ij /D Y T +1,i , W Y Y T +1,i,j =i \u2190 W Y Y T,ij + y T,i y T,j \u2212 \u03b1 + y 2 T,i W Y Y T,ij /D Y T +1,i .(15)\nHence, we arrive at a neural network algorithm that solves the optimization problem (10) for streaming data by alternating between two phases.After a data sample is presented at time T , in the first phase of the algorithm ( 13), neuron activities are updated until convergence to a fixed point.In the second phase of the algorithm, synaptic weights are updated for feedforward connections according to a local Hebbian rule (15) and for lateral connections according to a local anti-Hebbian rule (due to the (\u2212) sign in equation ( 13)).Interestingly, in the \u03b1 = 0 limit, these updates have the same form as the single-neuron Oja rule [24,2], except that the learning rate is not a free parameter but is determined by the cumulative neuronal activity 1/D Y T +1,i [4,5].\n\n\nOnline hard-thresholding of eigenvalues\n\nConsider the following minimax problem in the online setting, where we assume \u03b1 > 0:\n{y T , z T } \u2190 arg min y T arg max z T X X \u2212 Y Y 2 F \u2212 Y Y \u2212 Z Z \u2212 \u03b1T I T 2 F .(16)\nBy keeping only those terms that depend on y T or z T and considering the large-T limit, we get the following objective:\nL = 2\u03b1T y T 2 \u2212 4x T T \u22121 t=1 x t y t y T \u2212 2z T T \u22121 t=1 z t z t + \u03b1T I k z T + 4y T T \u22121 t=1 y t z t z T .\n(17) Note that this objective is strongly convex in y T and strongly concave in z T .The solution of this minimax problem is the saddle-point of the objective function, which is found by setting the gradient of the objective with respect to {y T , z T } to zero [30]:\n\u03b1T y T = T \u22121 t=1 y t x t x T \u2212 T \u22121 t=1 y t z t z T , T \u22121 t=1 z t z t + \u03b1T I k z T = T \u22121 t=1 z t y t y T . (18)\nTo obtain a neurally plausible algorithm, we solve these equations by a weighted Jacobi iteration:\ny T \u2190 (1 \u2212 \u03b7) y T + \u03b7 W Y X T x T \u2212 W Y Z T z T , z T \u2190 (1 \u2212 \u03b7) z T + \u03b7 W ZY T y T \u2212 W ZZ T z T .(19)\nHere, similarly to ( 14), W T are normalized covariances that can be updated recursively:\nD Y T +1,i \u2190 D Y T,i + \u03b1, D Z T +1,i \u2190 D Z T,i + \u03b1 + z 2 T,i W Y X T +1,ij \u2190 W Y X T,ij + y T,i x T,j \u2212 \u03b1W Y X T,ij /D Y T +1,i W Y Z T +1,ij \u2190 W Y Z T,ij + y T,i z T,j \u2212 \u03b1W Y Z T,ij /D Y T +1,i W ZY T +1,i,j \u2190 W ZY T,ij + z T,i y T,j \u2212 \u03b1 + z 2 T,i W ZY T,ij /D Z T +1,i W ZZ T +1,i,j =i \u2190 W ZZ T,ij + z T,i z T,j \u2212 \u03b1 + z 2 T,i W ZZ T,ij /D Z T +1,i , W ZZ T,ii = 0.(20)\nEquations ( 19) and ( 20) define an online algorithm that can be naturally implemented by a neural network with two populations of neurons: principal and interneurons, Figure 1E.Again, after each data sample presentation, T , the algorithm proceeds in two phases.First, ( 19) is iterated until convergence by the dynamics of neuronal activities.Second, synaptic weights are updated according to local, anti-Hebbian (for synapses from interneurons) and Hebbian (for all other synapses) rules.\n\n\nOnline thresholding and equalization of eigenvalues\n\nConsider the following minimax problem in the online setting, where we assume \u03b1 > 0 and \u03b2 > 0:\n{y T , z T } \u2190 arg min y T arg max z T Tr \u2212X XY Y + Y YZ Z + \u03b1TY Y \u2212 \u03b2TZ Z . (21)\nBy keeping only those terms that depend on y T or z T and considering the large-T limit, we get the following objective:\nL = \u03b1T y T 2 \u2212 2x T T \u22121 t=1 x t y t y T \u2212 \u03b2T z T 2 + 2y T T \u22121 t=1 y t z t z T . (22)\nThis objective is strongly convex in y T and strongly concave in z T and its saddle point is given by:\n\u03b1T y T = T \u22121 t=1 y t x t x T \u2212 T \u22121 t=1 y t z t z T , \u03b2T z T = T \u22121 t=1 z t y t y T .(23)\nTo obtain a neurally plausible algorithm, we solve these equations by a weighted Jacobi iteration:\ny T \u2190 (1 \u2212 \u03b7) y T + \u03b7 W Y X T x T \u2212 W Y Z T z T , z T \u2190 (1 \u2212 \u03b7) z T + \u03b7W ZY T y T ,(24)\nAs before, W T are normalized covariances which can be updated recursively:\nD Y T +1,i \u2190 D Y T,i + \u03b1, D Z T +1,i \u2190 D Z T,i + \u03b2 W Y X T +1,ij \u2190 W Y X T,ij + y T,i x T,j \u2212 \u03b1W Y X T,ij /D Y T +1,i W Y Z T +1,ij \u2190 W Y Z T,ij + y T,i z T,j \u2212 \u03b1W Y Z T,ij /D Y T +1,i W ZY T +1,i,j \u2190 W ZY T,ij + z T,i y T,j \u2212 \u03b2W ZY T,ij /D Z T +1,i .(25\n) Equations ( 24) and ( 25) define an online algorithm that can be naturally implemented by a neural network with principal neurons and interneurons.As beofre, after each data sample presentation at time T , the algorithm, first, iterates (24) by the dynamics of neuronal activities until convergence and, second, updates synaptic weights according to local anti-Hebbian (for synapses from interneurons) and Hebbian (25) (for all other synapses) rules.\n\nWhile an algorithm similar to ( 24), ( 25), but with predetermined learning rates, was previously given in [15,14], it has not been derived from an optimization problem.Plumbley's convergence analysis of his algorithm [14] suggests that at the fixed point of synaptic updates, the interneuron activity is also a projection onto the principal subspace.This result is a special case of our offline solution, (9), supported by the online numerical simulations (next Section).\n\n\nNumerical simulations\n\nHere, we evaluate the performance of the three online algorithms on a synthetic dataset, which is generated by an n = 64 dimensional colored Gaussian process with a specified covariance matrix.In this covariance matrix, the eigenvalues, \u03bb 1..4 = {5, 4, 3, 2} and the remaining \u03bb 5..60 are chosen uniformly from the interval [0, 0.5].Correlations are introduced in the covariance matrix by generating random orthonormal eigenvectors.For all three algorithms, we choose \u03b1 = 1 and, for the equalizing algorithm, we choose \u03b2 = 1.In all simulated networks, the number of principal neurons, k = 20, and, for the hard-thresholding and the equalizing algorithms, the number of interneurons, l = 5.Synaptic weight matrices were initialized randomly, and synaptic update learning rates, 1/D Y 0,i and 1/D Z 0,i were initialized to 0.1.Network dynamics is run with a weight \u03b7 = 0.1 until the relative change in y T and z T in one cycle is < 10 \u22125 .\n\nTo quantify the performance of these algorithms, we use two different metrics.The first metric, eigenvalue error, measures the deviation of output covariance eigenvalues from their optimal offline values given in Theorems 1, 2 and 3.The eigenvalue error at time T is calculated by summing squared differences between the eigenvalues of 1  T YY or 1 T ZZ , and their optimal offline values at time T .The second metric, subspace error, quantifies the deviation of the learned subspace from the true principal subspace.To form such metric, at each T , we calculate the linear transformation that maps inputs, x T , to outputs, y T = F Y X T x T and z T = F ZX T x T , at the fixed points of the neural dynamics stages (( 13), ( 19), ( 24)) of the three algorithms.Exact expressions for these matrices for all algorithms are given in the Supplementary Material.Then, at each T , the deviation is\nF m,T F m,T \u2212 U X m,T U X m,T2F\n, where F m,T is an n \u00d7 m matrix whose columns are the top m right singular vectors of F T , F m,T F m,T is the projection matrix to the subspace spanned by these singular vectors, U X m,T is an n\u00d7m matrix whose columns are the principal eigenvectors of the input covariance matrix C at time T , U X m,T U X m,T is the projection matrix to the principal subspace.\n\nFurther numerical simulations comparing the performance of the soft-thresholding algorithm with \u03b1 = 0 with other neural principal subspace algorithms can be found in [24].\n\n\nDiscussion and conclusions\n\nWe developed a normative approach for dimensionality reduction by formulating three novel optimization problems, the solutions of which project the input onto its principal subspace, and rescale the data by i) soft-thresholding, ii) hard-thresholding, iii) equalization after thresholding of the input eigenvalues.Remarkably we found that these optimization problems can be solved online using biologically plausible neural circuits.The dimensionality of neural activity is the number of either input covariance eigenvalues above the threshold, m,\n(if m < k) or output neurons, k (if k \u2264 m).\nThe former case is ubiquitous in the analysis of experimental recordings, for a review see [31].\n\nInterestingly, the division of neurons into two populations, principal and interneurons, in the last two models has natural parallels in biological neural networks.In biology, principal neurons and interneurons usually are excitatory and inhibitory respectively.However, we cannot make such an assignment in our theory, because the signs of neural activities, x T and y T , and, hence, the signs of synaptic weights, W, are unconstrained.Previously, interneurons were included into neural circuits [32], [33] outside of the normative approach.\n\nSimilarity matching in the offline setting has been used to analyze experimentally recorded neuron activity lending support to our proposal.Semantically similar stimuli result in similar neural activity patterns in human (fMRI) and monkey (electrophysiology) IT cortices [34,35].In addition, [36] computed similarities among visual stimuli by matching them with the similarity among corresponding retinal activity patterns (using an information theoretic metric).\n\nWe see several possible extensions to the algorithms presented here: 1) Our online objective functions may be optimized by alternative algorithms, such as gradient descent, which map onto different circuit architectures and learning rules.Interestingly, gradient descent-ascent on convex-concave objectives has been previously related to the dynamics of principal and interneurons [37].2) Inputs coming from a non-stationary distribution (with time-varying covariance matrix) can be processed by algorithms derived from the objective functions where contributions from older data points are \"forgotten\", or \"discounted\".Such discounting results in higher learning rates in the corresponding online algorithms, even at large T , giving them the ability to respond to variations in data statistics [24,4].Hence, the output dimensionality can track the number of input dimensions whose eigenvalues exceed the threshold.3) In general, the output of our algorithms is not decorrelated.Such decorrelation can be achieved by including a correlation-penalizing term in our objective functions [38].4) Choosing the threshold parameter \u03b1 requires an a priori knowledge of input statistics.A better solution, to be presented elsewhere, would be to let the network adjust such threshold adaptively, e.g. by filtering out all the eigenmodes with power below the mean eigenmode power.5) Here, we focused on dimensionality reduction using only spatial, as opposed to the spatio-temporal, correlation structure.\n\nmatrix, i.e. a non-negative matrix whose rows and columns separately add to one.Then  Proof.To prove the lemma, it is convenient to express the cost in terms of matrix elements:\n(\u03bb i \u2212 \u03bb j ) j k=1 D ik + \u03bb j j (using p i=1 D ik = 1) \u2264 p j=1 \u03bbj \u2212 \u03bbj+1 j i=1 (\u03bb i \u2212 \u03bb j ) j k=1 D ik + \u03bb j j (using \u03bb i>j \u2212 \u03bb j \u2264 0) = p j=1 \u03bbj \u2212 \u03bbj+1 j i=1 \u03bb i j k=1 D ik + \u03bb j j i=11Tr \u039bO \u039bO = i,j \u03bb i \u03bbj O 2 ij (S.4)\nNow consider a matrix D, whose elements are given by\nD ij = O 2 ij . Because O is orthog- onal, D is doubly stochastic: i D ij = i O 2 ij = O O jj = 1 and j D ij = j O 2 ij = OO ii = 1.\nFor any doubly stochastic matrix D, and decreasingly ordered {\u03bb i } and { \u03bbi } according to Schur's lemma: where O(p) is the set of p \u00d7 p orthogonal matrices.Furthermore, an orthogonal matrix is optimal if and only if it can be written as a product of two orthogonal matrices\nO * = O \u039b O \u039b, (S.8)\nwhich commute with \u039b and \u039b respectively:\n[\u039b, O \u039b ] = 0, \u039b, O \u039b = 0. (S.9)\nProof.The first equality in (S.To prove that all optimal orthogonal matrices are of the form (S.8), we take the following steps:\n\n1 Without loss of generality it suffices to prove our claim for proper rotations only.As we show now, if all optimal proper rotations are of the form (S.8), then all optimal improper rotations are also of the form (S.8).\n\nConsider an optimal improper rotation R * : and therefore R * is also optimal.If R * is of the form (S.8)\nTr \u039b R * \u039b R * =R * = O \u039b O \u039b, (S.15) then corresponding R * , R * = diag (\u22121, 1, . . . , 1) O \u039b O \u039b (S.16)\nis also of the form (S.8), since diag (\u22121, 1, . . ., 1) O \u039b commutes with \u039b.\n\nHence, we only consider proper rotation matrices without loss of generality.Since R * is maximal, the change in left had side must vanish to first order in \u03b4A:\n0 = Tr \u03b4A R * \u039bR * \u039b \u2212 \u039bR * \u039bR * = ij \u03b4A ij R * \u039bR * \u039b \u2212 \u039bR * \u039bR * ji = 2 i,j<i \u03b4A ij R * \u039bR * \u039b \u2212 \u039bR * \u039bR * ji , (S.19)\nwhere we used the antisymmetry of \u03b4A and of R * \u039bR * \u039b \u2212 \u039bR * \u039bR * .Since 3. We will use (S.20) to prove our claim that all optimal proper rotation matrices are of the form (S.8).We remind that if a matrix commutes with a diagonal matrix, it must be block-diagonal.There is a separate block for each distinct diagonal element of the diagonal matrix, and the size of the block is given by the degeneracy of the diagonal element.Then,\nB \u039b \u2261 R * \u039bR * (S.21)\nis block diagonal with with blocks defined by the degenerate diagonal elements of \u039b.\n\nFurther, singular values of B \u039b are given by diagonals of \u039b.\n\nB \u039b can be diagonalized by another orthogonal matrix., which is block diagonal with the same blocks as in B \u039b .However, such a block diagonal matrix would also commute with \u039b.Then, with notation from (S.8),\nB \u039b = O \u039b \u039bO \u039b . (S.O \u039b R * = O \u039b, (S.24)\nand hence\nR * = O \u039b O \u039b. (S.25)\nTherefore we can conclude that all rotation matrices that optimize (S.3) are of the form (S.8).\n\n\nII. PROOF OF THEOREM 1 -SOFT-THRESHOLDING OF COVARIANCE\n\n\nEIGENVALUES\n\nWe reproduce the offline objective function (3) for ease of referencing.where \u03b1 \u2265 0, X \u2208 R n\u00d7T and Y \u2208 R k\u00d7T .Define m to be the number of eigenvalues of C = 1 T XX greater than or equal to \u03b1 Now, we present the main result of this subsection and its proof.\nTheorem 1. Suppose an eigen-decomposition of X X = V X \u039b X V X , where \u039b X = diag \u03bb X 1 , . . . , \u03bb X T with \u03bb X 1 \u2265 . . . \u2265 \u03bb X T .\nNote that \u039b X has at most n nonzero eigenvalues coinciding with those of T C.Then,\nY * = U k ST k (\u039b X , \u03b1T ) 1/2 V X k , (S.27)\nare optima of (S.26), where\nST k (\u039b X , \u03b1T ) = diag ST \u03bb X 1 , \u03b1T , . . . , ST \u03bb X k , \u03b1T , ST is the soft-thresholding function, ST(a, b) = max(a \u2212 b, 0), V X k consists of the columns of V X corresponding to the top k eigenvalues, i.e. V X k = v X 1 , . . . , v X k and U k is any k \u00d7 k orthogonal matrix, i.e. U k \u2208 O(k).\nThe form (S.27) uniquely defines all optima of (S.26),\nexcept when k < m, \u03bb X k > \u03b1T and \u03bb X k = \u03bb X k+1 .\nProof.Here we assume that if k < m and \u03bb X k > \u03b1T , then \u03bb X k = \u03bb X k+1 , and prove that the form (S.27) uniquely defines all optima of (S.26).The exceptional case of k < m, \u03bb X k > \u03b1T and \u03bb X k = \u03bb X k+1 is treated in a remark below.Since the cost (S.where\nO = V X V Y \u2208 O(T ).\nThe minimization with respect to V Y is equivalent to a minimization over O from which V Y can be recovered uniquely by\nV X O. According to Lemma 2, each orthogonal matrix O = V X V Y that is a product of two orthogonal matrices O = O \u039b X O \u039b Y with \u039b X \u2212 \u03b1T I T , O \u039b X = 0 and \u039b Y , O \u039b Y = 0, is optimal. Then, optimal\nV Y is given by\nV Y * = V X O \u039b X O \u039b Y . (S.29)\nand the optimal value of (S.28) is:\nT i=1 \u03bb X i \u2212 \u03bb Y i \u2212 \u03b1T 2 .\n(S.30)\n\nIt remains to find optimal \u039b Y , which minimizes (S.30):\nmin \u03bb Y 1 ,...,\u03bb Y T T i=1 \u03bb X i \u2212 \u03bb Y i \u2212 \u03b1T 2 , (S.31)\nwhere {\u03bb Y 1 , . . ., \u03bb Y T } are non-negative and at most k of them are non-zero.Consider a term \u03bb X i \u2212 \u03bb Y i \u2212 \u03b1T 2 in the sum.If \u03bb X i \u2264 \u03b1T , choosing a positive \u03bb Y i will only increase the term, hence optimal \u03bb Y i = 0 for such terms.If \u03bb X i > \u03b1T , then choosing \u03bb Y i = \u03bb X i \u2212 \u03b1T will set the term to 0, i.e. its minimum.On the other hand, at most k of {\u03bb Y 1 , . . ., \u03bb Y T } can be non-zero.These k eigenvalues should be allocated to largest non-negative values of \u03bb X i \u2212\u03b1T .\n\nTherefore, optimal {\u03bb Y 1 , . . ., \u03bb Y T } are\n\u03bb Y * i = \uf8f1 \uf8f2 \uf8f3 ST \u03bb X i , \u03b1T , i \u2264 k 0, i > k . (S.32)\nTo reconstruct Y * , using (S.29) and (S.\nY * Y * = V X O \u039b X O \u039b Y * \u039b Y * O \u039b Y * O \u039b X V X = V X O \u039b X \u039b Y * O \u039b X V X . (S.33)\nSince if diagonal elements of \u039b X are degenerate, the corresponding diagonal elements of \u039b Y * must be degenerate\n\u039b Y * , O \u039b X = 0. (S.34)\nHence,\nY * Y * = V X \u039b Y * V X . (S.35)\nThese Y matrices can be constructed as in (S.27): its columns are coordinates in the arbitrarily rotated orthogonal basis spanning the k-dimensional principal subspace of XX .\n\nRemark 1.In the case k < m, \u03bb X k > \u03b1T and \u03bb X k = \u03bb X k+1 , (S.34) is not generally true anymore, because while \u03bb\nX k = \u03bb X k+1 , \u03bb Y k = \u03bb Y k+1 .\nY matrices constructed as in (S.27) are still minima, as can be seen by choosing O \u039b X = I T for which (S.34) holds, but there are other solutions which cannot be put in the form (S.27).We observe that blocks of O \u039b X that do not correspond to \u03bb X k still commute with \u039b Y * .Thus, when k < m, \u03bb X k > \u03b1T and \u03bb X k = \u03bb X k+1 , we can write the most general solution as\nY * = U k \u039b Y * k\u00d7T 1/2 O \u039b X k V X , (S.36)\nwhere U k is a k \u00d7 k orthogonal matrix, \u039b Y * k\u00d7T is a k \u00d7 T diagonal matrix with its k diagonals set to first k diagonals of \u039b Y * , and O \u039b X k is a T \u00d7 T orthogonal matrix that is diagonal except one block that corresponds to diagonal elements of \u039b X that are degenerate with \u03bb X k .\n\n\nIII. PROOF OF THEOREM 2 -HARD-THRESHOLDING OF COVARIANCE\n\n\nEIGENVALUES\n\nWe reproduce the offline objective (6) for ease of referencing.\nmin Y max Z X X \u2212 Y Y 2 F \u2212 Y Y \u2212 Z Z \u2212 \u03b1T I T 2 F , (S.37)\nwhere \u03b1 \u2265 0, X \u2208 R n\u00d7T , Y \u2208 R k\u00d7T and Z \u2208 R l\u00d7T .Let m be the number of eigenvalues C greater than or equal to \u03b1.\n\nTheorem 2. Suppose an eigen-decomposition of\nX X = V X \u039b X V X , where \u039b X = diag \u03bb X 1 , . . . , \u03bb X T with \u03bb X 1 \u2265 . . . \u2265 \u03bb X T \u2265 0. Assume l \u2265 min(k, m). Then, Y * = U k HT k (\u039b X , \u03b1T ) 1/2 V X k , Z * = U l ST l,min(k,m) (\u039b X , \u03b1T ) 1/2 V X l , (S.38)\nare optima of (S.37), where\nHT k (\u039b X , \u03b1T ) = diag HT \u03bb X 1 , \u03b1T , . . . , HT \u03bb X k , \u03b1T , HT(a, b) = a\u0398(a\u2212b) with \u0398() being the step function: \u0398(a\u2212b) = 1 if a \u2265 b and \u0398(a\u2212b) = 0 if a < b, ST l,min(k,m) (\u039b X , \u03b1T ) = diag ST \u03bb X 1 , \u03b1T , . . . , ST \u03bb X min(k,m) , \u03b1T , 0, . . . , 0 l\u2212min(k,m) ,V X p = v X 1 , . . . , v X p and U p \u2208 O(p).\nThe form (S.38) uniquely defines all optima (S.37) except when either 1) \u03b1 is an eigenvalue of C or 2) k < m and \u03bb X k = \u03bb X k+1 .\n\nProof.Here we assume 1) l \u2265 min(k, m), 2) \u03b1 is not an eigenvalue of C and 3) if k < m, then \u03bb X k = \u03bb X k+1 .We prove that with these assumptions, the form (S.38) uniquely defines all optima (S.37).Violations of these assumptions are treated in three remarks below.\n\nThe proof is similar to that of Theorem 1.Since the objective (S.37) depends on Y only through the similarity matrix Y Y and on Z through Z Z, we first find optimal Y Y and Z Z from which we reconstruct Y and Z.Our strategy is to start with eigendecompositions\nof Y Y = V Y \u039b Y V Y and Z Z = V Z \u039b Z V Z , and find optimal V Y , \u039b Y , V Z and \u039b Z .\nWe first optimize (S.37) with respect to V Y \u2208 R T \u00d7T and V Z \u2208 R T \u00d7T for fixed \u039b Y and \u039b Z .Because Frobenius norm is invariant under rotations, the terms in (S.37) can be rewritten as\nX X \u2212 Y Y 2 F = \u039b X \u2212 O\u039b Y O 2 F ,(S.39)\nand\n\u2212 Y Y \u2212 Z Z \u2212 \u03b1T I T 2 F = \u2212 \u039b Y \u2212 Q\u039b Z Q \u2212 \u03b1T I T 2 F , (S.40) where O = V X V Y \u2208 O(T ) and where Q = V Y V Z \u2208 O(T )\n. First, we maximize (S.37)\n\nwith respect to V Z , which enters via Q in (S.40).According to Lemma 2, any orthogonal\nmatrix Q = V Y V Z that is a product of two orthogonal matrices Q = Q \u039b Y Q \u039b Z with \u039b Y \u2212 \u03b1T I T , Q \u039b Y = 0 and \u039b Z , Q \u039b Z = 0, is optimal.\nThen, optimal V Z is given by\nV Z * = V Y Q \u039b Y Q \u039b Z . (S.41)\nand subsituting this expression into (S.40),\n\u2212 T i=1 \u03bb Y i \u2212 \u03bb Z i \u2212 \u03b1T 2 . (S.42)\nBecause the optimality of V Z * (S.41) holds for any V Y the minimization of (S.41) with respect to V Y is reduced to (S.39).According to Lemma 2, any orthogonal matrix\nO = V X V Y that is a product of two orthogonal matrices O = O \u039b X O \u039b Y with \u039b X , O \u039b X = 0 and \u039b Y , O \u039b Y = 0, is optimal. Then, optimal V Y is given by V Y * = V X O \u039b X O \u039b Y . (S.43)\nFor these choices of V Y * and V Z * , the full objective (S.37) reduces to:\nmin \u03bb Y 1 ,...,\u03bb Y T max \u03bb Z 1 ,...,\u03bb Z T T i=1 \u03bb X i \u2212 \u03bb Y i 2 \u2212 \u03bb Y i \u2212 \u03b1T \u2212 \u03bb Z i 2 , (S.44)\nwhere {\u03bb Z 1 , . . ., \u03bb Z T } are constrained to be non-negative and at most l of them are non-zero, and {\u03bb Y 1 , . . ., \u03bb Y T } are also constrained to be non-negative and at most k of them are non-zero.We analyze the terms in the sum separately:\n\n1. Consider the i > m terms in the sum for which \u03bb X i < \u03b1T .For such terms, choosing\n\u03bb Y i = \u03bb Z i = 0 gives the optimal cost, \u03bb X i 2 \u2212 \u03b1 2 T 2 < 0. To see this, let's calculate costs associated with other choices of \u03bb Y i and \u03bb Z i . Suppose \u03bb Y i \u2265 \u03b1T . Then, maximization with respect to \u03bb Z i would set \u03bb Y i \u2212 \u03b1T \u2212 \u03bb Z i 2\n= 0 and therefore the cost would be\n\u03bb X i \u2212 \u03bb Y i 2 \u2265 0. Suppose \u03bb Y i \u2264 \u03b1T .\nThen, maximization with respect to \u03bb Z i would set \u03bb Z i = 0, and the cost would be \u03bb\nX i 2 \u2212 \u03b1 2 T 2 + 2\u03bb Y i \u03b1T \u2212 \u03bb X i\n. This is minimized for \u03bb Y i = 0. Hence, our claim holds.\n\n2. Consider the i \u2264 m terms in the sum for which \u03bb X i > \u03b1T .Since \u03bb Y i>m = \u03bb Z i>m = 0, and we assumed l \u2265 m, we can assign all \u03bb Z i\u2264m to non-zero values if needed.On the other hand, k can be less than m and we might be forced to set some \u03bb Y i\u2264m to zero.\n\n(a) Suppose \u03bb Y i > 0. For these terms, choosing \u03bb Y i = \u03bb X i and \u03bb Z i = \u03bb X i \u2212 \u03b1T gives the optimal cost, 0. To see this, let's calculate costs associated with other choices of \u03bb Y i and \u03bb\nZ i . If \u03bb Y i > \u03bb X i , or \u03b1T \u2264 \u03bb Y i < \u03bb X i maximization with respect to \u03bb Z i would set \u03bb Y i \u2212 \u03b1T \u2212 \u03bb Z i 2 = 0 and therefore the cost would be \u03bb X i \u2212 \u03bb Y i 2 > 0.\nIf \u03bb Y i < \u03b1T , maximization with respect to \u03bb Z i would set \u03bb Z i = 0, and the cost would be \u03bb\nX i 2 \u2212 \u03b1 2 T 2 \u2212 2\u03bb Y i \u03bb X i \u2212 \u03b1T . This cost is greater than its value at \u03bb Y i = \u03b1T , which is \u03bb X i \u2212 \u03b1T 2 > 0.\nHence, our claim holds.\n\n(b) Suppose \u03bb Y i = 0.For these terms, choosing \u03bb Z i = 0 gives the optimal cost,\n\u03bb X i 2 \u2212 \u03b1 2 T 2 > 0.\nTherefore, one assigns non-zero \u03bb Y i to the the first min(k, m) terms in the sum.For such terms \u03bb\nY i = \u03bb X i and \u03bb Z i = \u03bb X i \u2212 \u03b1T . \u03bb Y i = \u03bb Z i = 0 otherwise.\nSummarizing this argument, we can state that:\n\u03bb Y * i = \uf8f1 \uf8f2 \uf8f3 HT \u03bb X i , \u03b1T , i \u2264 min(k, m) 0, otherwise , \u03bb Z * i = \uf8f1 \uf8f2 \uf8f3 ST \u03bb X i ,\u03b1T , i \u2264 min(k, m) 0, otherwise (S.45)\noptimizes the cost (S.44).\n\nTo reconstruct Y * , using (S.43) and (S.45) we rewrite the eigenvalue decomposition of Y * Y * .Using \u039b Y * to denote optimal singular values defined by (S.45), and O \u039b Y * to denote an orthogonal matrix that commutes with \u039b Y * , we get:\nY * Y * = V X O \u039b X O \u039b Y * \u039b Y * O \u039b Y * O \u039b X V X = V X O \u039b X \u039b Y * O \u039b X V X . (S.46) But, \u039b Y * , O \u039b X = 0. (S.47)\nsince if diagonal elements of \u039b X are degenerate, corresponding diagonal elements of \u039b Y * are degenerate.Hence,\nY * Y * = V X \u039b Y * V X . (S.48)\nThese Y * matrices can be constructed as in (S.38): its columns are coordinates in the arbitrarily rotated orthogonal basis spanning the k-dimensional principal subspace of XX .\n\nTo reconstruct Z * , using (S.41) and (S.45) we rewrite the eigenvalue decomposition of Z * Z * .Using \u039b Z * to denote optimal singular values defined by (S.45), and Q \u039b Z * to denote an orthogonal matrix that commutes with \u039b Z * , we get:\nZ * Z * = V Y * Q \u039b Y * Q \u039b Z * \u039b Z * Q \u039b Z * Q \u039b Y * V Y * = V Y * Q \u039b Y * \u039b Z * Q \u039b Y * V Y * . (S.49) But, \u039b Z * , Q \u039b Y * = 0. (S.50)\nsince if diagonal elements of \u039b Y * are degenerate, corresponding diagonal elements of \u039b Z * are degenerate .Hence,\nZ * Z * = V Y * \u039b Z * V Y * . (S.51)\nPlugging in for V Y * , one gets\nZ * Z * = V X O \u039b X O \u039b Y * \u039b Z * O \u039b Y * O \u039b X V X . (S.52) But, \u039b Z * , O \u039b Y * = 0 (S.Z * Z * = V X \u039b Z * V X . (S.55)\nThese Z * matrices can be constructed as in (S.38).\n\nRemark 2A.Here we comment on the case l < min(k, m).In the eigenvalue cost (S.44),among the terms for which \u03bb X i > \u03b1T , there will be cases where \u03bb Z i is forced to be zero, while \u03bb Y i \u2265 0. The cost for such terms are \u03bb X i 2 \u2212 \u03b1 2 T 2 \u2212 2\u03bb Y i \u03bb X i \u2212 \u03b1T , which minimizes when \u03bb Y i \u2192 \u221e.We found through numerical simulations that the corresponding online algorithm is unstable in this regime.\n\nRemark 2B.Here we comment on the case where \u03b1 is an eigenvalue of C.Here we need to consider optimization of terms for which \u03bb X i = \u03b1T in the eigenvalue cost (S.44).The cost for such terms are 2\u03bb\nZ i \u03bb Y i \u2212 \u03b1T \u2212 \u03bb Z i 2\n, which is optimized for any \u03bb Y i \u2264 \u03b1T and \u03bb Z i = 0 with a 0 value for the cost.To see this, consider the other case \u03bb Y i > \u03b1T .Then the optimization with respect to \u03bb Z i would give \u03bb Z i = \u03bb Y i \u2212 \u03b1T and the cost of the term would be \u03bb Y i \u2212 \u03b1T 2 > 0, which would be suboptimal.Hence, if k \u2265 m, or k < m and \u03bb X k = \u03b1T , Y \u2208 R k\u00d7T and Z \u2208 R l\u00d7T constructed as in (S.38) are still optimal, however there are other optimal solutions.Non-zero {\u03bb Y i } corresponding to \u03b1 eigenvalue of C can take values 0 \u2264 \u03bb Y i \u2264 \u03b1T .\n\nRemark 2C.Here we comment on the case k < m and \u03bb X k = \u03bb X k+1 .We first discuss how optimal Y change.In this case, (S.47) is not generally true anymore, because while \u03bb\nX k = \u03bb X k+1 , \u03bb Y k = \u03bb Y k+1 .\nY matrices constructed as in (S.38) are still minima, as can be seen by choosing O \u039b X = I T for which (S.47) holds, but there are other solutions which cannot be put in the form (S.38).We observe that blocks of O \u039b X that do not correspond to \u03bb X k still commute with \u039b Y * .Thus, when k < m and \u03bb X k = \u03bb X k+1 , we can write the most general solution as\nY * = U k \u039b Y * k\u00d7T 1/2 O \u039b X k V X , (S.56)\nwhere U k is a k \u00d7 k orthogonal matrix, \u039b Y * k\u00d7T is a k \u00d7 T diagonal matrix with its k diagonals set to first k diagonals of \u039b Y * , and O \u039b X k is a T \u00d7 T orthogonal matrix that is diagonal except one block that corresponds to diagonal elements of \u039b X that are degenerate with \u03bb X k .Next we discuss how optimal Z change.In this case, while (S.53) is still true, (S.54) is not generally true anymore, because while \u03bb\nX k = \u03bb X k+1 , \u03bb Z k = \u03bb Z k+1 .\nZ matrices constructed as in (S.38) are still minima, as can be seen by choosing O \u039b X = I T for which (S.54) holds, but there are other solutions which cannot be put in the form (S.38).We observe that blocks of O \u039b X that do not correspond to \u03bb X k still commute with \u039b Z * .Thus, when k < m and \u03bb X k = \u03bb X k+1 , we can write the most general solution as\nZ * = U l \u039b Z * l\u00d7T 1/2 O \u039b X k V X , (S.57)\nwhere U l is an l \u00d7 l orthogonal matrix, \u039b Z * l\u00d7T is a l \u00d7 T diagonal matrix with its l diagonals set to first l diagonals of \u039b Z * , and O \u039b X l is a T \u00d7 T orthogonal matrix that is diagonal except one block that corresponds to diagonal elements of \u039b X that are degenerate with \u03bb X k .\n\n\nIV. PROOF OF THEOREM 3 -THRESHOLDING AND EQUALIZATION OF COVARIANCE EIGENVALUES\n\nWe reproduce the offline objective (8) for ease of referencing:\nmin Y max Z Tr \u2212X XY Y + Y YZ Z + \u03b1T Y Y \u2212 \u03b2T Z Z , (S.58)\nwhere \u03b1 \u2265 0 and \u03b2 \u2265 0. Let m be the number of eigenvalues C greater than \u03b1.\nTheorem 3. Suppose an eigen-decomposition of X X is X X = V X \u039b X V X , where \u039b X = diag \u03bb X 1 , . . . , \u03bb X T with \u03bb X 1 \u2265 . . . \u2265 \u03bb X T \u2265 0. Assume l \u2265 min(k, m). Then, Y * = U k \u03b2T \u0398 k (\u039b X , \u03b1T ) 1/2 V X k , Z * = U l \u03a3 l\u00d7T O \u039b Y * V X , (S.59)\nare optima of (S.58), where \u0398 k (\u039b X , \u03b1T ) = diag \u0398 \u03bb X 1 \u2212 \u03b1T , . . ., \u0398 \u03bb X k \u2212 \u03b1T , \u03a3 l\u00d7T is an l \u00d7 T rectangular diagonal matrix with top min(k, m) diagonals are set to arbitrary nonnegative constants and the rest are zero, O \u039b Y * is a block-diagonal orthogonal matrix that has two blocks: the top block is min(k, m) dimensional and the bottom block is T \u2212 min(k, m) dimensional, V p = v X 1 , . . ., v X p , and U p \u2208 O(p).The form (S.59) uniquely defines all optima of (S.58) except when either 1) \u03b1 is an eigenvalue of C or 2) k < m and \u03bb X k = \u03bb X k+1 .\n\nProof.Here we assume 1) l \u2265 min(k, m), 2) \u03b1 is not an eigenvalue of C and 3) if k < m, then \u03bb X k = \u03bb X k+1 .We prove that with these assumptions, the form (S.59) uniquely defines all optima (S.58).Violations of these assumptions are treated in three remarks below.\n\nThe proof is similar to that of Theorem 1.Since the cost (S.58)depends on Y only through the similarity matrix Y Y and Z through Z Z, we find optimizing Y Y and Z Z from which we reconstruct Y and Z.Our strategy is to start with eigendecompositions of\nY Y = V Y \u039b Y V Y and Z Z = V Z \u039b Z V Z , and find optimal V Y , \u039b Y , V Z and \u039b Z .\nWe first optimize for V Y \u2208 R T \u00d7T and V Z \u2208 R T \u00d7T for fixed \u039b Y and \u039b Z .Because Frobenius norm is invariant under rotations, the terms in objective (S.59) can be rewritten as\nTr \u2212 \u039b X \u2212 \u03b1T I T O\u039b Y O + \u039b Y \u2212 \u03b2T I T Q\u039b Z Q , (S.60) where O = V X V Y \u2208 O(T ) and where Q = V Y V Z \u2208 O(T )\n. First, we do the maximization over V Z , which entails the second term of (S.60).According to Lemma 2, all orthogonal\nmatrices Q = V Y V Z that are a product of two orthogonal matrices Q = Q \u039b Y Q \u039b Z with \u039b Y \u2212 \u03b2T I T , Q \u039b Y = 0 and \u039b Z , Q \u039b Z = 0, are optimal.\nThen, optimal V Z are given by\nV Z * = V Y Q \u039b Y Q \u039b Z . (S.61)\nFor this choice of V Z , the second term in (S.60) is Tr \u039b Y \u2212 \u03b2T I T \u039b Z and therefore the minimization over V Y only entails the first term in (S.60).According to Lemma 2,\nall orthogonal matrices O = V X V Y that are a product of two orthogonal matrices O = O \u039b X O \u039b Y with \u039b X , O \u039b X = 0 and \u039b Y , O \u039b Y = 0, are optimal. Then, optimal V Y are\ngiven by\nV Y * = V X O \u039b X O \u039b Y . (S.62)\nFor these choices of V Y * and V Z * , the full objective (S.58) reduces to:\nmin \u03bb Y 1 ,...,\u03bb Y T max \u03bb Z 1 ,...,\u03bb Z T T i=1 \u2212 \u03bb X i \u2212 \u03b1T \u03bb Y i + \u03bb Y i \u2212 \u03b2T \u03bb Z i ,(S.63)\nwhere {\u03bb Z 1 , . . ., \u03bb Z T } are constrained to be non-negative and at most l of them are non-zero, and {\u03bb Y 1 , . . ., \u03bb Y T } are also constrained to be non-negative and at most k of them are non-zero.We analyze the terms in the sum separately:\n\n1. Consider the i > m terms in the sum for which \u03bb X i < \u03b1T .For such terms, choosing \u03bb Y i = \u03bb Z i = 0 gives the optimal cost, 0. To see this, let's calculate costs associated with other choices of \u03bb Y i and \u03bb Z i .Suppose \u03bb Y i > \u03b2T .Then, maximization with respect to \u03bb Z i would set the cost to \u221e. Suppose \u03bb Y i = \u03b2T .Then, the coefficient in front of \u03bb Z i is 0, and the cost is \u2212 \u03bb X i \u2212 \u03b1T \u03b2T > 0. Suppose \u03bb Y i < \u03b2T .Then, maximization with respect to \u03bb Z i would set \u03bb Z i = 0 and the cost is \u2212 \u03bb X i \u2212 \u03b1T \u03bb Y i , which is minimal at \u03bb Y i = 0. Hence, our claim holds.\n\n2. Consider the i \u2264 m terms in the sum for which \u03bb X i > \u03b1T .Note that we assumed \u03b1 is not an eigenvalue of C, therefore we omit the equality case.Since \u03bb Y i>m = \u03bb Z i>m = 0, and we assumed l \u2265 m, we can assign all \u03bb Z i\u2264m to non-zero values if needed.On the other hand, k can be less than m and we might be forced to set some \u03bb Y i\u2264m to zero.\n\n(a) Suppose \u03bb Y i > 0. For these terms, choosing \u03bb Y i = \u03b2T and any \u03bb Z i gives the optimal cost, \u2212 \u03bb X i \u2212 \u03b1T \u03b2T < 0. To see this, let's calculate costs associated with other choices of \u03bb Y i and \u03bb Z i .If \u03bb Y i > \u03b2T , maximization with respect to \u03bb Z i would set the cost to \u221e.If \u03bb Y i < \u03b2T , maximization with respect to \u03bb Z i would set \u03bb Z i = 0, and the cost would be \u2212 \u03bb X i \u2212 \u03b1T \u03bb Y i , which is greater than its value at \u03bb Y i = \u03b2T , given by \u2212 \u03bb X i \u2212 \u03b1T \u03b2T .Hence, our claim holds.\n\n(b) Suppose \u03bb Y i = 0.For these terms, choosing \u03bb Z i = 0 gives the optimal cost, 0.\n\nTherefore, one assigns non-zero \u03bb Y i to the the first min(k, m) terms in the sum.For such terms \u03bb Y i = \u03b2T and \u03bb Z i can take any value.\n\nSummarizing this argument, we can state that:\n\u03bb Y * i = \uf8f1 \uf8f2 \uf8f3 \u03b2T, i \u2264 min(k, m) 0, otherwise , \u03bb Z * i = \uf8f1 \uf8f2 \uf8f3 any non-negative value, i \u2264 min(k, m)0Y * Y * = V X O \u039b X O \u039b Y * \u039b Y * O \u039b Y * O \u039b X V X = V X O \u039b X \u039b Y * O \u039b X V X . (S.65) But, \u039b Y * , O \u039b X = 0. (S.66) since if diagonal elements of \u039b X are degenerate, corresponding diagonal elements of \u039b Y * are degenerate. Hence, Y * Y * = V X \u039b Y * V X . (S.67)\nThese Y * matrices can be constructed as in (S.59): its columns are coordinates in the arbitrarily rotated orthogonal basis spanning the k-dimensional principal subspace of XX .\n\nTo reconstruct Z * , using (S.61) and (S.64) we rewrite the eigenvalue decomposition of Z * Z * .Using \u039b Z * to denote optimal singular values defined by (S.64), and Q \u039b Z * to denote an orthogonal matrix that commutes with \u039b Z * , we get:\nZ * Z * = V Y * Q \u039b Y * Q \u039b Z * \u039b Z * Q \u039b Z * Q \u039b Y * V Y * = V Y * Q \u039b Y * \u039b Z * Q \u039b Y * V Y * . (S.68) Unlike before, \u039b Z * , Q \u039b Y * = 0 in general.\nPlugging in for V Y * , we get: dimensional.These Z * matrices can be constructed as in (S.38).\nZ * Z * = V X O \u039b X O \u039b Y * Q \u039b Y * \u039b Z * Q \u039b Y * O \u039b Y * O \u039b X V X . (S\nRemark 3A.Here we comment on the case l < min(k, m).In the eigenvalue cost (S.63),among the terms for which \u03bb X i > \u03b1T , there will be cases where \u03bb Z i is forced to be zero, while \u03bb Y i \u2265 0. The cost for such terms are \u2212 \u03bb X i \u2212 \u03b1T \u03bb Y i , which minimizes when \u03bb Y i \u2192 \u221e.We found through numerical simulations that the corresponding online algorithm is unstable in this regime.\n\nRemark 3B.Here we comment on the case where \u03b1 is an eigenvalue of C.Here we need to consider optimization of terms for which \u03bb X i = \u03b1T in the eigenvalue cost (S.63).The cost for such terms are \u03bb Y i \u2212 \u03b2T \u03bb Z i , which is optimized for any \u03bb Y i \u2264 \u03b2T and \u03bb Z i = 0 with a 0 value for the cost.To see this, consider the other case \u03bb Y i > \u03b2T .Then the optimization with respect to \u03bb Z i would give \u221e cost.Hence, if k > m, or k < m and \u03bb X k = \u03b1T , Y \u2208 R k\u00d7T and Z \u2208 R l\u00d7T constructed as in (S.59) are still optimal, however there are other optimal solutions.Non-zero {\u03bb Y i } corresponding to \u03b1 eigenvalue of C can take values 0 \u2264 \u03bb Y i \u2264 \u03b1T .\n\nRemark 3C.Here we comment on the case k < m and \u03bb X k = \u03bb X k+1 .We first discuss how optimal Y change.In this case, (S.66) is not generally true anymore, because while \u03bb\nX k = \u03bb X k+1 , \u03bb Y k = \u03bb Y k+1 .\nY matrices constructed as in (S.59) are still minima, as can be seen by choosing O \u039b X = I T for which (S.66) holds, but there are other solutions which cannot be put in the form (S.59).We observe that blocks of O \u039b X that do not correspond to \u03bb X k still commute with \u039b Y * .Thus, when k < m and \u03bb X k = \u03bb X k+1 , we can write the most general solution as\nY * = U k \u039b Y * k\u00d7T 1/2 O \u039b X k V X , (S.71)\nwhere U k is a k \u00d7 k orthogonal matrix, \u039b Y * k\u00d7T is a k \u00d7 T diagonal matrix with its k diagonals set to first k diagonals of \u039b Y * , and O \u039b X k is a T \u00d7 T orthogonal matrix that is diagonal except one block that corresponds to diagonal elements of \u039b X that are degenerate with \u03bb X k .Next we discuss how optimal Z change.In this case, while (S.69) is still true, (S.70) is not generally true anymore, because (S.66) does not hold in general.Z matrices constructed as in (S.59) are still minima, as can be seen by choosing O \u039b X = I T for which (S.70) holds, but there are other solutions which cannot be put in the form (S.59).We observe that blocks of O \u039b X that do not correspond to \u03bb X k still commute with \u039b Y * .Thus, when k < m and \u03bb X k = \u03bb X k+1 , we can write the most general solution as\nZ * = U l \u039b Z * l\u00d7T 1/2 O \u039b X k O \u039b Y * V X , (S.72)\nwhere U l is an l \u00d7 l orthogonal matrix, \u039b Z * l\u00d7T is a l \u00d7 T diagonal matrix with its l diagonals set to first l diagonals of \u039b Z * , and O \u039b X k is a T \u00d7 T orthogonal matrix that is diagonal except one block that corresponds to diagonal elements of \u039b X that are degenerate with \u03bb X k .\n\n\nV. FULL EXPRESSIONS FOR INPUT-TO-OUPUT MAPPING MATRICES\n\nHere we give the full expressions for linear transformation that maps inputs, x T , to outputs,\ny T = F Y X T x T , z T = F ZX T x T . (S.73)\nTo do this, we find fixed points of the neural dynamics stages of the three algorithms.\n\n\nA. Online soft-thresholding of eigenvalues\n\nThe neural dynamics stage of this algorithm is\n\n3 .\n3\nEqualization of non-zero eigenvalues, Figure 1C.The corresponding network's architecture, Figure 1F, lacks reciprocal connections among interneurons.As before, the number of abovethreshold eigenvalues is chosen adaptively and cannot exceed the number of principal neurons.If the two are equal, this network whitens the output.\n\n\nFigure 2 :\n2\nFigure 2: Performance of the three neural networks: soft-thresholding (A), hard-thresholding (B), equalization after thresholding (C).Top: eigenvalue error, bottom: subspace error as a function of data presentations.Solid lines -means and shades -stds over 10 runs.Red -principal, blueinter-neurons.Dashed lines -best-fit power laws.For metric definitions see text.\n\n\n(Lemma 1 .\n1\nusing \u03bb i\u2264j \u2212 \u03bb j \u2265 0 and 1 \u2212 j k=1 D ik \u2265 0) Let \u039b = diag (\u03bb 1 , . . ., \u03bb p ), where \u03bb 1 \u2265 . . .\u2265 \u03bb p are real numbers, and let \u039b = diag \u03bb1 , . . ., \u03bbp , where \u03bb1 \u2265 . . .\u2265 \u03bbp are real numbers.Then, max O\u2208O(p) Tr \u039bO \u039bO = Tr \u039b \u039b , (S.3)where O(p) is the set of p \u00d7 p orthogonal matrices.\n\n\n6 )\n6\nThe bound is saturated when O = I p , which proves the Lemma 1.Lemma 2. Let \u039b = diag (\u03bb 1 , . . ., \u03bb p ), where \u03bb 1 \u2265 . . .\u2265 \u03bb p are real numbers, and let \u039b = diag \u03bb1 , . . ., \u03bbp , where \u03bb1 \u2265 . . .\u2265 \u03bbp are real numbers.Then,\n\n\n7 )\u039b \u2212 O \u039bO 2 FTr \u039b 2 \u2212\n722\nfollows from the definition of the Frobenius norm and orthogonality of O: 2O \u039bO \u039b + \u039b2 = arg max O\u2208O(p) Tr \u039bO \u039bO .(S.10) It is easy to see that any matrix of the form (S.8) optimizes (S.10): Tr \u039bO \u039b O \u039b \u039bO \u039bO \u039b = Tr O \u039b \u039b \u039bO \u039bO \u039bO \u039b = Tr \u039b \u039b , (S.11) which optimizes (S.10) according to Lemma 1.\n\n\n\n\n. Recall that any orthogonal matrix O must have det O = \u00b11.Orthogonal matrices with det O = 1 are proper rotations which we denote by R. Orthogonal matrices with det O = \u22121 are improper rotations which we denote by R.\n\n\n\n\nTr \u039b \u039b .(S.12) Next, we define a one-to-one mapping between each improper rotation R and a proper rotation by multiplying R on the right by the matrix diag (\u22121, 1, . . ., 1) R \u2261 diag (\u22121, 1, . . ., 1) R. (S.13)Then, Tr \u039bR * \u039bR * = Tr \u039b R * \u039b R * = Tr \u039b \u039b , (S.14)\n\n\n2 .\n2\nConsider an optimal proper rotation matrix R * : Tr \u039bR * \u039bR * = Tr \u039b \u039b .(S.17) Proper rotations form a connected set, and can be parametrized by e A , where A is an antisymmetric matrix.Suppose we rotate R * by an infinitesimal amount, i.e. e \u03b4A R * : Tr \u039be \u03b4A R * \u039bR * e \u2212\u03b4A = Tr \u039b (I + \u03b4A) R * \u039bR * (I \u2212 \u03b4A) + O(\u03b4 2 ) = Tr \u039bR * \u039bR * + Tr \u03b4A R * \u039bR * \u039b \u2212 \u039bR * \u039bR * + O(\u03b4 2 ) (S.18)\n\n\n\n\n\u03b4A i,j<i are independent perturbations, their coefficients R * \u039bR * \u039b \u2212 \u039bR * \u039bR * ji must each be zero.From here we conclude that for maximal R * \u039bR * \u039bR * \u2212 R * \u039bR * \u039b = \u039b, R * \u039bR * = 0. (S.20)\n\n\n2 F\n2\n26) depends on Y only through the similarity matrix Y Y, we first optimize (S.26) with respect to Y Y and then reconstruct the optimal Y.In turn we optimize with respect to Y Y considering eigendecomposition of Y Y = V Y \u039b Y V Y , and finding optimal V Y and \u039b Y separately.We first optimize (S.26) with respect to V Y \u2208 O(T ) for fixed \u039b Y .Because the Frobenius norm is invariant to orthogonal rotations, for any Y Y, the objective (S.26) can be rewritten as X X \u2212 Y Y \u2212 \u03b1T I T = \u039b X \u2212 O\u039b Y O \u2212 \u03b1T I T 2 F , (S.28)\n\n\n\n\n32) we rewrite the eigenvalue decomposition of Y * Y * .Using \u039b Y * to denote optimal singular values defined by (S.32), and O \u039b Y * to denote an orthogonal matrix that commutes with \u039b Y * , we get:\n\n\n\n\n, otherwise (S.64) optimizes the cost (S.63).To reconstruct Y * , using (S.62) and (S.64) we rewrite the eigenvalue decomposition of Y * Y * .Using \u039b Y * to denote optimal singular values defined by (S.64), and O \u039b Y * to denote an orthogonal matrix that commutes with \u039b Y * , we get:\n\n\n\n\n.69) This expression can be simplified further.Remembering that \u039b Y * , O \u039b X = 0 from (S.66), we can absorb the productO \u039b X O \u039b Y * Q \u039b Y * into O \u039b Y * , a single orthogonal matrix that commutes with \u039b Y * , Z * Z * = V X O \u039b Y * \u039b Z * O \u039b Y * V X .(S.70) What is the structure of O \u039b Y * ?Since \u039b Y * has top min(k, m) diagonals \u03b2T and rest zero, (S.64), O \u039b Y * has two blocks, first is min(k, m) dimensional and the second is T \u2212 min(k, m)\n\n\ny=\u2212 1 W[ 1 ]\n11\nT \u2190 (1 \u2212 \u03b7) y T + \u03b7 W Y X T x T \u2212 W Y Y T y T .(S.74)At the fixed point of this iteration,I m + W Y Y T y T = W Y X T x T , hard-thresholding of eigenvaluesThe neural dynamics stage of this algorithm isy T \u2190 (1 \u2212 \u03b7) y T + \u03b7 W Y X T x T \u2212 W Y Z T z T , z T \u2190 (1 \u2212 \u03b7) z T + \u03b7 W ZY T y T \u2212 W ZZ T z T .(S.77)At the fixed point of this iteration,y T = W Y X T x T \u2212 W Y Z T z T , I k + W ZZ T z T = W ZY T y T , I m + W Y Z I k + W ZZ T ZY thresholdingand equalization of eigenvaluesThe neural dynamics stage of this algorithm isy T \u2190 (1 \u2212 \u03b7) y T + \u03b7 W Y X T x T \u2212 W Y Z T z T , z T \u2190 (1 \u2212 \u03b7) z T + \u03b7W ZY T y T .(S.80)At the fixed point of this iteration,y T = W Y X T x T \u2212 W Y Z T z T , z T = W ZY T y T , (S.81)and thereforeF Y X T = I m + W Y Z W ZY \u22121 W Y X T , R. A. Horn and C. R. Johnson, Matrix analysis (Cambridge university press, 2012).\n\n\n\n\nY * are degenerate, corresponding diagonal elements of \u039b Z * are degenerate and if diagonal elements of \u039b X are degenerate, corresponding diagonal elements \u039b Z * are degenerate.Then,\n53)and\u039b Z *  , O \u039b X = 0.(S.54)since if diagonal elements of \u039b\nWe thank L. Greengard, A. Sengupta, A. Grinshpan, S. Wright, A. Barnett and E. Pnevmatikakis.ReferencesHere we reproduce Schur's lemma[1]and prove two new lemmas that will be central to our analysis in the coming sections.Lemma (Schur's lemma).Let \u03bb 1 \u2265 . . .\u2265 \u03bb p and D a p \u00d7 p dimensional doubly stochastic\nScientific American Library/Scientific American Books. H David, Hubel, 1995Eye, brain, and vision\n\nSimplified neuron model as a principal component analyzer. Oja, J Math Biol. 1531982\n\nPrincipal component neural networks: theory and applications. K I Diamantaras, Kung, 1996John Wiley & Sons, Inc\n\nProjection approximation subspace tracking. Yang, IEEE Trans. Signal Process. 4311995\n\nA neuron as a signal processing device. Hu, Towfic, Pehlevan, Genkin, Chklovskii, Asilomar Conference on Signals, Systems and Computers. IEEE2013\n\nPrincipal components, minor components, and linear neural networks. Oja, Neural Networks. 561992\n\nStochastic optimization for pca and pls. Arora, Cotter, Livescu, Srebro, Allerton Conf. on Communication, Control, and Computing. IEEE2012\n\nRobust stochastic principal component analysis. Goes, Zhang, Arora, Lerman, Proc. 17th Int. Conf. on Artificial Intelligence and Statistics. 17th Int. Conf. on Artificial Intelligence and Statistics2014\n\nDynamics of learning in recurrent feature-discovery networks. Todd K Leen, NIPS. 31990\n\nAdaptive network for optimal linear feature extraction. F\u00f6ldiak, Int. Joint Conf. on Neural Networks. IEEE1989\n\nOptimal unsupervised learning in a single-layer linear feedforward neural network. Td Sanger, Neural networks. 261989\n\nA self-organizing network for principal-component analysis. J Rubner, Tavan, EPL. 106931989\n\nA hebbian/anti-hebbian network which optimizes information capacity by orthonormalizing the principal subspace. Md Plumbley, Proc. 3rd Int. Conf. on Artificial Neural Networks. 3rd Int. Conf. on Artificial Neural Networks1993\n\nA subspace network that determines its own output dimension. Md Plumbley, 1994Tech. Rep.\n\nInformation processing in negative feedback neural networks. Md Plumbley, Network-Comp Neural. 721996\n\nUnsupervised learning of an efficient short-term memory network. Vertechi, Brendel, Machens, NIPS. 2014\n\nSparse coding with an overcomplete basis set: A strategy employed by v1?. Ba Olshausen, Field, Vision Res. 37231997\n\nSparse incomplete representations: a potential role of olfactory granule cells. Aa Koulakov, Rinberg, Neuron. 7212011\n\nA mechanistic model of early sensory processing based on subtracting sparse representations. S Druckmann, Hu, Chklovskii, NIPS. 2012\n\nEfficiency and ambiguity in an adaptive neural code. Al Fairhall, Gd Lewen, Bialek, Van Steveninck, Nature. 41268492001\n\nPredictive information in a sensory population. Se Palmer, Marre, Berry, Bialek, PNAS. 112222015\n\nEfficient coding of spatial information in the primate retina. Doi, Gauthier, Field, Shlens, J Neurosci. 32462012\n\nSelf-organization in a perceptual network. Linsker, Computer. 2131988\n\nA hebbian/anti-hebbian neural network for linear subspace learning: A derivation from multidimensional scaling of streaming data. C Pehlevan, Hu, Chklovskii, Neural Comput. 272015\n\nDiscussion of a set of points in terms of their mutual distances. G Young, Householder, Psychometrika. 311938\n\nMultidimensional scaling: I. theory and method. Ws Torgerson, Psychometrika. 1741952\n\nAutomatic gain control by a basic neural circuit. H G Barrow, Budd, Artificial Neural Networks. 21992\n\nExact matrix completion via convex optimization. Ej Cand\u00e8s, Recht, Found Comput Math. 962009\n\nOnline learning for matrix factorization and sparse coding. Mairal, Bach, Ponce, Sapiro, JMLR. 112010\n\nConvex optimization. S Boyd, L Vandenberghe, 2004Cambridge university press\n\nOn simplicity and complexity in the brave new world of large-scale neuroscience. P Gao, Ganguli, Curr Opin Neurobiol. 322015\n\nModeling inhibitory interneurons in efficient sensory coding models. M Zhu, Rozell, PLoS Comput Biol. 117e10043532015\n\nInhibitory interneurons decorrelate excitatory cells to drive sparse code formation in a spiking model of v1. Pd King, Zylberberg, Deweese, J Neurosci. 33132013\n\nMatching categorical object representations in inferior temporal cortex of man and monkey. Kriegeskorte, Mur, Da Ruff, Kiani, Neuron. 6062008\n\nObject category structure in response patterns of neuronal population in monkey inferior temporal cortex. Kiani, Esteky, Mirpour, Tanaka, J Neurophysiol. 9762007\n\nRetinal metric: a stimulus distance measure derived from population neural responses. E Tka\u010dik, Granot-Atedgi, Segev, Schneidman, PRL. 1105581042013\n\nMinimax and hamiltonian dynamics of excitatory-inhibitory networks. Hs Seung, J C Richardson, Lagarias, Hopfield, NIPS. 101998\n\nOptimization theory of hebbian/anti-hebbian networks for pca and whitening. C Pehlevan, Chklovskii, Allerton Conf. on Communication, Control, and Computing. 2015\n", "annotations": {"author": "[{\"end\":204,\"start\":89},{\"end\":326,\"start\":205}]", "publisher": null, "author_last_name": "[{\"end\":104,\"start\":96},{\"end\":224,\"start\":214}]", "author_first_name": "[{\"end\":95,\"start\":89},{\"end\":211,\"start\":205},{\"end\":213,\"start\":212}]", "author_affiliation": "[{\"end\":203,\"start\":137},{\"end\":325,\"start\":259}]", "title": "[{\"end\":75,\"start\":1},{\"end\":401,\"start\":327}]", "venue": null, "abstract": "[{\"end\":2031,\"start\":474}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2446,\"start\":2443},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2637,\"start\":2634},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3088,\"start\":3085},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3226,\"start\":3223},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3363,\"start\":3360},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3365,\"start\":3363},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3455,\"start\":3452},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3606,\"start\":3603},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3608,\"start\":3606},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3610,\"start\":3608},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3612,\"start\":3610},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3663,\"start\":3660},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3784,\"start\":3780},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3787,\"start\":3784},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3790,\"start\":3787},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3792,\"start\":3790},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3794,\"start\":3792},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3797,\"start\":3794},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3800,\"start\":3797},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3803,\"start\":3800},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3806,\"start\":3803},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3943,\"start\":3939},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3946,\"start\":3943},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3949,\"start\":3946},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4417,\"start\":4413},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4420,\"start\":4417},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4423,\"start\":4420},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4500,\"start\":4496},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4503,\"start\":4500},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4570,\"start\":4566},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4645,\"start\":4641},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4648,\"start\":4645},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4747,\"start\":4743},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5113,\"start\":5109},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5116,\"start\":5113},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5555,\"start\":5551},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5817,\"start\":5813},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5820,\"start\":5817},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5929,\"start\":5925},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8215,\"start\":8211},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9584,\"start\":9580},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9719,\"start\":9715},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10355,\"start\":10352},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11466,\"start\":11463},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11804,\"start\":11801},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12445,\"start\":12442},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12867,\"start\":12864},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15215,\"start\":15211},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15272,\"start\":15268},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15868,\"start\":15864},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16208,\"start\":16204},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16418,\"start\":16414},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16420,\"start\":16418},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16546,\"start\":16543},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16548,\"start\":16546},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17258,\"start\":17254},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19924,\"start\":19920},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20101,\"start\":20097},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20246,\"start\":20242},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20249,\"start\":20246},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20357,\"start\":20353},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20544,\"start\":20541},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23032,\"start\":23028},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23751,\"start\":23747},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24256,\"start\":24252},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24262,\"start\":24258},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24574,\"start\":24570},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":24577,\"start\":24574},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24595,\"start\":24591},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":25149,\"start\":25145},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25564,\"start\":25560},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25566,\"start\":25564},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25853,\"start\":25849},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":42475,\"start\":42472},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":54627,\"start\":54624}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":51773,\"start\":51439},{\"attributes\":{\"id\":\"fig_2\"},\"end\":52154,\"start\":51774},{\"attributes\":{\"id\":\"fig_4\"},\"end\":52456,\"start\":52155},{\"attributes\":{\"id\":\"fig_5\"},\"end\":52689,\"start\":52457},{\"attributes\":{\"id\":\"fig_6\"},\"end\":53015,\"start\":52690},{\"attributes\":{\"id\":\"fig_7\"},\"end\":53237,\"start\":53016},{\"attributes\":{\"id\":\"fig_8\"},\"end\":53505,\"start\":53238},{\"attributes\":{\"id\":\"fig_9\"},\"end\":53896,\"start\":53506},{\"attributes\":{\"id\":\"fig_10\"},\"end\":54095,\"start\":53897},{\"attributes\":{\"id\":\"fig_12\"},\"end\":54620,\"start\":54096},{\"attributes\":{\"id\":\"fig_13\"},\"end\":54823,\"start\":54621},{\"attributes\":{\"id\":\"fig_14\"},\"end\":55112,\"start\":54824},{\"attributes\":{\"id\":\"fig_15\"},\"end\":55562,\"start\":55113},{\"attributes\":{\"id\":\"fig_16\"},\"end\":56425,\"start\":55563},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":56674,\"start\":56426}]", "paragraph": "[{\"end\":2614,\"start\":2047},{\"end\":3807,\"start\":2616},{\"end\":4239,\"start\":3809},{\"end\":4504,\"start\":4241},{\"end\":4828,\"start\":4506},{\"end\":5871,\"start\":4830},{\"end\":6162,\"start\":5873},{\"end\":6181,\"start\":6164},{\"end\":6611,\"start\":6212},{\"end\":7301,\"start\":6613},{\"end\":7446,\"start\":7303},{\"end\":7478,\"start\":7448},{\"end\":7683,\"start\":7497},{\"end\":7867,\"start\":7685},{\"end\":7872,\"start\":7869},{\"end\":8815,\"start\":7874},{\"end\":9107,\"start\":8867},{\"end\":9225,\"start\":9155},{\"end\":9438,\"start\":9260},{\"end\":9787,\"start\":9516},{\"end\":9986,\"start\":9789},{\"end\":10070,\"start\":10010},{\"end\":10284,\"start\":10072},{\"end\":10443,\"start\":10327},{\"end\":10859,\"start\":10797},{\"end\":11193,\"start\":10916},{\"end\":11239,\"start\":11195},{\"end\":11473,\"start\":11449},{\"end\":11916,\"start\":11777},{\"end\":12032,\"start\":11966},{\"end\":12182,\"start\":12084},{\"end\":13026,\"start\":12428},{\"end\":13120,\"start\":13028},{\"end\":13194,\"start\":13122},{\"end\":13538,\"start\":13196},{\"end\":13648,\"start\":13582},{\"end\":13780,\"start\":13696},{\"end\":14144,\"start\":13880},{\"end\":14415,\"start\":14204},{\"end\":14585,\"start\":14471},{\"end\":15216,\"start\":14726},{\"end\":15402,\"start\":15218},{\"end\":15574,\"start\":15442},{\"end\":16549,\"start\":15780},{\"end\":16677,\"start\":16593},{\"end\":16882,\"start\":16762},{\"end\":17259,\"start\":16992},{\"end\":17473,\"start\":17375},{\"end\":17665,\"start\":17576},{\"end\":18528,\"start\":18037},{\"end\":18678,\"start\":18584},{\"end\":18881,\"start\":18761},{\"end\":19071,\"start\":18969},{\"end\":19261,\"start\":19163},{\"end\":19425,\"start\":19350},{\"end\":20133,\"start\":19681},{\"end\":20607,\"start\":20135},{\"end\":21570,\"start\":20633},{\"end\":22464,\"start\":21572},{\"end\":22860,\"start\":22497},{\"end\":23033,\"start\":22862},{\"end\":23611,\"start\":23064},{\"end\":23752,\"start\":23656},{\"end\":24297,\"start\":23754},{\"end\":24762,\"start\":24299},{\"end\":26259,\"start\":24764},{\"end\":26438,\"start\":26261},{\"end\":26712,\"start\":26660},{\"end\":27121,\"start\":26846},{\"end\":27183,\"start\":27143},{\"end\":27345,\"start\":27217},{\"end\":27567,\"start\":27347},{\"end\":27674,\"start\":27569},{\"end\":27859,\"start\":27783},{\"end\":28020,\"start\":27861},{\"end\":28574,\"start\":28142},{\"end\":28681,\"start\":28597},{\"end\":28743,\"start\":28683},{\"end\":28951,\"start\":28745},{\"end\":29003,\"start\":28994},{\"end\":29121,\"start\":29026},{\"end\":29452,\"start\":29195},{\"end\":29668,\"start\":29586},{\"end\":29742,\"start\":29715},{\"end\":30094,\"start\":30040},{\"end\":30405,\"start\":30147},{\"end\":30546,\"start\":30427},{\"end\":30764,\"start\":30749},{\"end\":30833,\"start\":30798},{\"end\":30869,\"start\":30863},{\"end\":30927,\"start\":30871},{\"end\":31472,\"start\":30985},{\"end\":31520,\"start\":31474},{\"end\":31618,\"start\":31577},{\"end\":31821,\"start\":31708},{\"end\":31854,\"start\":31848},{\"end\":32063,\"start\":31888},{\"end\":32179,\"start\":32065},{\"end\":32582,\"start\":32214},{\"end\":32914,\"start\":32628},{\"end\":33052,\"start\":32989},{\"end\":33227,\"start\":33113},{\"end\":33273,\"start\":33229},{\"end\":33514,\"start\":33487},{\"end\":33958,\"start\":33828},{\"end\":34225,\"start\":33960},{\"end\":34487,\"start\":34227},{\"end\":34762,\"start\":34576},{\"end\":34807,\"start\":34804},{\"end\":34955,\"start\":34928},{\"end\":35044,\"start\":34957},{\"end\":35217,\"start\":35188},{\"end\":35295,\"start\":35251},{\"end\":35502,\"start\":35334},{\"end\":35769,\"start\":35693},{\"end\":36113,\"start\":35866},{\"end\":36200,\"start\":36115},{\"end\":36480,\"start\":36445},{\"end\":36608,\"start\":36523},{\"end\":36703,\"start\":36645},{\"end\":36963,\"start\":36705},{\"end\":37157,\"start\":36965},{\"end\":37423,\"start\":37328},{\"end\":37564,\"start\":37541},{\"end\":37647,\"start\":37566},{\"end\":37769,\"start\":37671},{\"end\":37881,\"start\":37836},{\"end\":38034,\"start\":38008},{\"end\":38275,\"start\":38036},{\"end\":38508,\"start\":38396},{\"end\":38719,\"start\":38542},{\"end\":38960,\"start\":38721},{\"end\":39214,\"start\":39099},{\"end\":39284,\"start\":39252},{\"end\":39458,\"start\":39407},{\"end\":39857,\"start\":39460},{\"end\":40055,\"start\":39859},{\"end\":40602,\"start\":40081},{\"end\":40774,\"start\":40604},{\"end\":41165,\"start\":40809},{\"end\":41629,\"start\":41211},{\"end\":42020,\"start\":41664},{\"end\":42353,\"start\":42066},{\"end\":42500,\"start\":42437},{\"end\":42635,\"start\":42560},{\"end\":43448,\"start\":42885},{\"end\":43715,\"start\":43450},{\"end\":43968,\"start\":43717},{\"end\":44231,\"start\":44054},{\"end\":44463,\"start\":44344},{\"end\":44641,\"start\":44611},{\"end\":44848,\"start\":44675},{\"end\":45032,\"start\":45024},{\"end\":45142,\"start\":45066},{\"end\":45484,\"start\":45237},{\"end\":46063,\"start\":45486},{\"end\":46409,\"start\":46065},{\"end\":46902,\"start\":46411},{\"end\":46988,\"start\":46904},{\"end\":47127,\"start\":46990},{\"end\":47174,\"start\":47129},{\"end\":47722,\"start\":47545},{\"end\":47963,\"start\":47724},{\"end\":48211,\"start\":48116},{\"end\":48663,\"start\":48285},{\"end\":49307,\"start\":48665},{\"end\":49479,\"start\":49309},{\"end\":49870,\"start\":49514},{\"end\":50715,\"start\":49916},{\"end\":51056,\"start\":50769},{\"end\":51211,\"start\":51116},{\"end\":51345,\"start\":51258},{\"end\":51438,\"start\":51392},{\"end\":51772,\"start\":51446},{\"end\":52153,\"start\":51788},{\"end\":52455,\"start\":52169},{\"end\":52688,\"start\":52464},{\"end\":53014,\"start\":52719},{\"end\":53236,\"start\":53019},{\"end\":53504,\"start\":53241},{\"end\":53895,\"start\":53513},{\"end\":54094,\"start\":53900},{\"end\":54619,\"start\":54103},{\"end\":54822,\"start\":54624},{\"end\":55111,\"start\":54827},{\"end\":55561,\"start\":55116},{\"end\":56424,\"start\":55580},{\"end\":56611,\"start\":56429}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6211,\"start\":6182},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7496,\"start\":7479},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9259,\"start\":9226},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9515,\"start\":9439},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10009,\"start\":9987},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10326,\"start\":10285},{\"attributes\":{\"id\":\"formula_6\"},\"end\":10750,\"start\":10444},{\"attributes\":{\"id\":\"formula_7\"},\"end\":10915,\"start\":10860},{\"attributes\":{\"id\":\"formula_8\"},\"end\":11448,\"start\":11240},{\"attributes\":{\"id\":\"formula_9\"},\"end\":11776,\"start\":11474},{\"attributes\":{\"id\":\"formula_10\"},\"end\":12083,\"start\":12033},{\"attributes\":{\"id\":\"formula_11\"},\"end\":12427,\"start\":12183},{\"attributes\":{\"id\":\"formula_12\"},\"end\":13695,\"start\":13649},{\"attributes\":{\"id\":\"formula_13\"},\"end\":13878,\"start\":13781},{\"attributes\":{\"id\":\"formula_14\"},\"end\":13879,\"start\":13878},{\"attributes\":{\"id\":\"formula_15\"},\"end\":14203,\"start\":14145},{\"attributes\":{\"id\":\"formula_16\"},\"end\":14469,\"start\":14416},{\"attributes\":{\"id\":\"formula_17\"},\"end\":14470,\"start\":14469},{\"attributes\":{\"id\":\"formula_18\"},\"end\":14725,\"start\":14586},{\"attributes\":{\"id\":\"formula_19\"},\"end\":15441,\"start\":15403},{\"attributes\":{\"id\":\"formula_20\"},\"end\":15779,\"start\":15575},{\"attributes\":{\"id\":\"formula_21\"},\"end\":16761,\"start\":16678},{\"attributes\":{\"id\":\"formula_22\"},\"end\":16991,\"start\":16883},{\"attributes\":{\"id\":\"formula_23\"},\"end\":17374,\"start\":17260},{\"attributes\":{\"id\":\"formula_24\"},\"end\":17575,\"start\":17474},{\"attributes\":{\"id\":\"formula_25\"},\"end\":18036,\"start\":17666},{\"attributes\":{\"id\":\"formula_26\"},\"end\":18759,\"start\":18679},{\"attributes\":{\"id\":\"formula_27\"},\"end\":18760,\"start\":18759},{\"attributes\":{\"id\":\"formula_28\"},\"end\":18967,\"start\":18882},{\"attributes\":{\"id\":\"formula_29\"},\"end\":18968,\"start\":18967},{\"attributes\":{\"id\":\"formula_30\"},\"end\":19162,\"start\":19072},{\"attributes\":{\"id\":\"formula_31\"},\"end\":19349,\"start\":19262},{\"attributes\":{\"id\":\"formula_32\"},\"end\":19680,\"start\":19426},{\"attributes\":{\"id\":\"formula_33\"},\"end\":22495,\"start\":22465},{\"attributes\":{\"id\":\"formula_34\"},\"end\":22496,\"start\":22495},{\"attributes\":{\"id\":\"formula_35\"},\"end\":23655,\"start\":23612},{\"attributes\":{\"id\":\"formula_36\"},\"end\":26625,\"start\":26439},{\"attributes\":{\"id\":\"formula_37\"},\"end\":26659,\"start\":26625},{\"attributes\":{\"id\":\"formula_38\"},\"end\":26845,\"start\":26713},{\"attributes\":{\"id\":\"formula_39\"},\"end\":27142,\"start\":27122},{\"attributes\":{\"id\":\"formula_40\"},\"end\":27216,\"start\":27184},{\"attributes\":{\"id\":\"formula_41\"},\"end\":27691,\"start\":27675},{\"attributes\":{\"id\":\"formula_42\"},\"end\":27782,\"start\":27691},{\"attributes\":{\"id\":\"formula_43\"},\"end\":28141,\"start\":28021},{\"attributes\":{\"id\":\"formula_44\"},\"end\":28596,\"start\":28575},{\"attributes\":{\"id\":\"formula_45\"},\"end\":28972,\"start\":28952},{\"attributes\":{\"id\":\"formula_46\"},\"end\":28993,\"start\":28972},{\"attributes\":{\"id\":\"formula_47\"},\"end\":29025,\"start\":29004},{\"attributes\":{\"id\":\"formula_48\"},\"end\":29585,\"start\":29453},{\"attributes\":{\"id\":\"formula_49\"},\"end\":29714,\"start\":29669},{\"attributes\":{\"id\":\"formula_50\"},\"end\":30039,\"start\":29743},{\"attributes\":{\"id\":\"formula_51\"},\"end\":30146,\"start\":30095},{\"attributes\":{\"id\":\"formula_52\"},\"end\":30426,\"start\":30406},{\"attributes\":{\"id\":\"formula_53\"},\"end\":30748,\"start\":30547},{\"attributes\":{\"id\":\"formula_54\"},\"end\":30797,\"start\":30765},{\"attributes\":{\"id\":\"formula_55\"},\"end\":30862,\"start\":30834},{\"attributes\":{\"id\":\"formula_56\"},\"end\":30984,\"start\":30928},{\"attributes\":{\"id\":\"formula_57\"},\"end\":31576,\"start\":31521},{\"attributes\":{\"id\":\"formula_58\"},\"end\":31707,\"start\":31619},{\"attributes\":{\"id\":\"formula_59\"},\"end\":31847,\"start\":31822},{\"attributes\":{\"id\":\"formula_60\"},\"end\":31887,\"start\":31855},{\"attributes\":{\"id\":\"formula_61\"},\"end\":32213,\"start\":32180},{\"attributes\":{\"id\":\"formula_62\"},\"end\":32627,\"start\":32583},{\"attributes\":{\"id\":\"formula_63\"},\"end\":33112,\"start\":33053},{\"attributes\":{\"id\":\"formula_64\"},\"end\":33486,\"start\":33274},{\"attributes\":{\"id\":\"formula_65\"},\"end\":33827,\"start\":33515},{\"attributes\":{\"id\":\"formula_66\"},\"end\":34575,\"start\":34488},{\"attributes\":{\"id\":\"formula_67\"},\"end\":34803,\"start\":34763},{\"attributes\":{\"id\":\"formula_68\"},\"end\":34927,\"start\":34808},{\"attributes\":{\"id\":\"formula_69\"},\"end\":35187,\"start\":35045},{\"attributes\":{\"id\":\"formula_70\"},\"end\":35250,\"start\":35218},{\"attributes\":{\"id\":\"formula_71\"},\"end\":35333,\"start\":35296},{\"attributes\":{\"id\":\"formula_72\"},\"end\":35692,\"start\":35503},{\"attributes\":{\"id\":\"formula_73\"},\"end\":35865,\"start\":35770},{\"attributes\":{\"id\":\"formula_74\"},\"end\":36444,\"start\":36201},{\"attributes\":{\"id\":\"formula_75\"},\"end\":36522,\"start\":36481},{\"attributes\":{\"id\":\"formula_76\"},\"end\":36644,\"start\":36609},{\"attributes\":{\"id\":\"formula_77\"},\"end\":37327,\"start\":37158},{\"attributes\":{\"id\":\"formula_78\"},\"end\":37540,\"start\":37424},{\"attributes\":{\"id\":\"formula_79\"},\"end\":37670,\"start\":37648},{\"attributes\":{\"id\":\"formula_80\"},\"end\":37835,\"start\":37770},{\"attributes\":{\"id\":\"formula_81\"},\"end\":38007,\"start\":37882},{\"attributes\":{\"id\":\"formula_82\"},\"end\":38395,\"start\":38276},{\"attributes\":{\"id\":\"formula_83\"},\"end\":38541,\"start\":38509},{\"attributes\":{\"id\":\"formula_84\"},\"end\":39098,\"start\":38961},{\"attributes\":{\"id\":\"formula_85\"},\"end\":39251,\"start\":39215},{\"attributes\":{\"id\":\"formula_86\"},\"end\":39374,\"start\":39285},{\"attributes\":{\"id\":\"formula_87\"},\"end\":39406,\"start\":39374},{\"attributes\":{\"id\":\"formula_88\"},\"end\":40080,\"start\":40056},{\"attributes\":{\"id\":\"formula_89\"},\"end\":40808,\"start\":40775},{\"attributes\":{\"id\":\"formula_90\"},\"end\":41210,\"start\":41166},{\"attributes\":{\"id\":\"formula_91\"},\"end\":41663,\"start\":41630},{\"attributes\":{\"id\":\"formula_92\"},\"end\":42065,\"start\":42021},{\"attributes\":{\"id\":\"formula_93\"},\"end\":42559,\"start\":42501},{\"attributes\":{\"id\":\"formula_94\"},\"end\":42884,\"start\":42636},{\"attributes\":{\"id\":\"formula_95\"},\"end\":44053,\"start\":43969},{\"attributes\":{\"id\":\"formula_96\"},\"end\":44343,\"start\":44232},{\"attributes\":{\"id\":\"formula_97\"},\"end\":44610,\"start\":44464},{\"attributes\":{\"id\":\"formula_98\"},\"end\":44674,\"start\":44642},{\"attributes\":{\"id\":\"formula_99\"},\"end\":45023,\"start\":44849},{\"attributes\":{\"id\":\"formula_100\"},\"end\":45065,\"start\":45033},{\"attributes\":{\"id\":\"formula_101\"},\"end\":45236,\"start\":45143},{\"attributes\":{\"id\":\"formula_102\"},\"end\":47278,\"start\":47175},{\"attributes\":{\"id\":\"formula_103\"},\"end\":47544,\"start\":47278},{\"attributes\":{\"id\":\"formula_104\"},\"end\":48115,\"start\":47964},{\"attributes\":{\"id\":\"formula_105\"},\"end\":48284,\"start\":48212},{\"attributes\":{\"id\":\"formula_106\"},\"end\":49513,\"start\":49480},{\"attributes\":{\"id\":\"formula_107\"},\"end\":49915,\"start\":49871},{\"attributes\":{\"id\":\"formula_108\"},\"end\":50768,\"start\":50716},{\"attributes\":{\"id\":\"formula_109\"},\"end\":51257,\"start\":51212}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2045,\"start\":2033},{\"attributes\":{\"n\":\"2\"},\"end\":8865,\"start\":8818},{\"attributes\":{\"n\":\"2.1\"},\"end\":9153,\"start\":9110},{\"attributes\":{\"n\":\"2.2\"},\"end\":10795,\"start\":10752},{\"attributes\":{\"n\":\"2.3\"},\"end\":11964,\"start\":11919},{\"attributes\":{\"n\":\"3.1\"},\"end\":13580,\"start\":13541},{\"attributes\":{\"n\":\"3.2\"},\"end\":16591,\"start\":16552},{\"attributes\":{\"n\":\"3.3\"},\"end\":18582,\"start\":18531},{\"attributes\":{\"n\":\"4\"},\"end\":20631,\"start\":20610},{\"attributes\":{\"n\":\"5\"},\"end\":23062,\"start\":23036},{\"end\":29179,\"start\":29124},{\"end\":29193,\"start\":29182},{\"end\":32973,\"start\":32917},{\"end\":32987,\"start\":32976},{\"end\":42435,\"start\":42356},{\"end\":51114,\"start\":51059},{\"end\":51390,\"start\":51348},{\"end\":51443,\"start\":51440},{\"end\":51785,\"start\":51775},{\"end\":52166,\"start\":52156},{\"end\":52461,\"start\":52458},{\"end\":52714,\"start\":52691},{\"end\":53510,\"start\":53507},{\"end\":54100,\"start\":54097},{\"end\":55576,\"start\":55564}]", "table": "[{\"end\":56674,\"start\":56612}]", "figure_caption": "[{\"end\":51773,\"start\":51445},{\"end\":52154,\"start\":51787},{\"end\":52456,\"start\":52168},{\"end\":52689,\"start\":52463},{\"end\":53015,\"start\":52718},{\"end\":53237,\"start\":53018},{\"end\":53505,\"start\":53240},{\"end\":53896,\"start\":53512},{\"end\":54095,\"start\":53899},{\"end\":54620,\"start\":54102},{\"end\":54823,\"start\":54623},{\"end\":55112,\"start\":54826},{\"end\":55562,\"start\":55115},{\"end\":56425,\"start\":55579},{\"end\":56612,\"start\":56428}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":7963,\"start\":7961},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":8226,\"start\":8224},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":8313,\"start\":8311},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":8642,\"start\":8640},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":14834,\"start\":14832},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":18214,\"start\":18212}]", "bib_author_first_name": "[{\"end\":57040,\"start\":57039},{\"end\":57232,\"start\":57231},{\"end\":57234,\"start\":57233},{\"end\":58338,\"start\":58337},{\"end\":59918,\"start\":59917},{\"end\":60099,\"start\":60098},{\"end\":60101,\"start\":60100},{\"end\":60370,\"start\":60369},{\"end\":60378,\"start\":60377},{\"end\":60507,\"start\":60506},{\"end\":60621,\"start\":60620},{\"end\":61225,\"start\":61224},{\"end\":61367,\"start\":61366},{\"end\":61369,\"start\":61368},{\"end\":61493,\"start\":61492}]", "bib_author_last_name": "[{\"end\":57046,\"start\":57041},{\"end\":57053,\"start\":57048},{\"end\":57145,\"start\":57142},{\"end\":57246,\"start\":57235},{\"end\":57252,\"start\":57248},{\"end\":57330,\"start\":57326},{\"end\":57411,\"start\":57409},{\"end\":57419,\"start\":57413},{\"end\":57429,\"start\":57421},{\"end\":57437,\"start\":57431},{\"end\":57449,\"start\":57439},{\"end\":57587,\"start\":57584},{\"end\":57660,\"start\":57655},{\"end\":57668,\"start\":57662},{\"end\":57677,\"start\":57670},{\"end\":57685,\"start\":57679},{\"end\":57806,\"start\":57802},{\"end\":57813,\"start\":57808},{\"end\":57820,\"start\":57815},{\"end\":57828,\"start\":57822},{\"end\":58031,\"start\":58020},{\"end\":58109,\"start\":58102},{\"end\":58250,\"start\":58241},{\"end\":58345,\"start\":58339},{\"end\":58352,\"start\":58347},{\"end\":58493,\"start\":58482},{\"end\":58669,\"start\":58658},{\"end\":58759,\"start\":58748},{\"end\":58863,\"start\":58855},{\"end\":58872,\"start\":58865},{\"end\":58881,\"start\":58874},{\"end\":58981,\"start\":58969},{\"end\":58988,\"start\":58983},{\"end\":59103,\"start\":59092},{\"end\":59112,\"start\":59105},{\"end\":59235,\"start\":59224},{\"end\":59239,\"start\":59237},{\"end\":59251,\"start\":59241},{\"end\":59329,\"start\":59318},{\"end\":59339,\"start\":59331},{\"end\":59347,\"start\":59341},{\"end\":59363,\"start\":59349},{\"end\":59443,\"start\":59434},{\"end\":59450,\"start\":59445},{\"end\":59457,\"start\":59452},{\"end\":59465,\"start\":59459},{\"end\":59550,\"start\":59547},{\"end\":59560,\"start\":59552},{\"end\":59567,\"start\":59562},{\"end\":59575,\"start\":59569},{\"end\":59649,\"start\":59642},{\"end\":59810,\"start\":59800},{\"end\":59814,\"start\":59812},{\"end\":59826,\"start\":59816},{\"end\":59924,\"start\":59919},{\"end\":59937,\"start\":59926},{\"end\":60022,\"start\":60010},{\"end\":60108,\"start\":60102},{\"end\":60114,\"start\":60110},{\"end\":60209,\"start\":60200},{\"end\":60216,\"start\":60211},{\"end\":60311,\"start\":60305},{\"end\":60317,\"start\":60313},{\"end\":60324,\"start\":60319},{\"end\":60332,\"start\":60326},{\"end\":60375,\"start\":60371},{\"end\":60391,\"start\":60379},{\"end\":60511,\"start\":60508},{\"end\":60520,\"start\":60513},{\"end\":60625,\"start\":60622},{\"end\":60633,\"start\":60627},{\"end\":60787,\"start\":60780},{\"end\":60799,\"start\":60789},{\"end\":60808,\"start\":60801},{\"end\":60935,\"start\":60923},{\"end\":60940,\"start\":60937},{\"end\":60949,\"start\":60942},{\"end\":60956,\"start\":60951},{\"end\":61086,\"start\":61081},{\"end\":61094,\"start\":61088},{\"end\":61103,\"start\":61096},{\"end\":61111,\"start\":61105},{\"end\":61232,\"start\":61226},{\"end\":61247,\"start\":61234},{\"end\":61254,\"start\":61249},{\"end\":61266,\"start\":61256},{\"end\":61364,\"start\":61356},{\"end\":61380,\"start\":61370},{\"end\":61390,\"start\":61382},{\"end\":61400,\"start\":61392},{\"end\":61502,\"start\":61494},{\"end\":61514,\"start\":61504}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":57081,\"start\":56984},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":16577977},\"end\":57167,\"start\":57083},{\"attributes\":{\"id\":\"b2\"},\"end\":57280,\"start\":57169},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":39443157},\"end\":57367,\"start\":57282},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":6710642},\"end\":57514,\"start\":57369},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":40000333},\"end\":57612,\"start\":57516},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":361547},\"end\":57752,\"start\":57614},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":17043930},\"end\":57956,\"start\":57754},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":14341741},\"end\":58044,\"start\":57958},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":8917254},\"end\":58156,\"start\":58046},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":10138295},\"end\":58275,\"start\":58158},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":121154409},\"end\":58368,\"start\":58277},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":14543399},\"end\":58595,\"start\":58370},{\"attributes\":{\"id\":\"b13\"},\"end\":58685,\"start\":58597},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":46082703},\"end\":58788,\"start\":58687},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1116926},\"end\":58893,\"start\":58790},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14208692},\"end\":59010,\"start\":58895},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":14504579},\"end\":59129,\"start\":59012},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14270875},\"end\":59263,\"start\":59131},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4354013},\"end\":59384,\"start\":59265},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14611524},\"end\":59482,\"start\":59386},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1738289},\"end\":59597,\"start\":59484},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1527671},\"end\":59668,\"start\":59599},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":7064552},\"end\":59849,\"start\":59670},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":122400126},\"end\":59960,\"start\":59851},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":120849755},\"end\":60046,\"start\":59962},{\"attributes\":{\"id\":\"b26\"},\"end\":60149,\"start\":60048},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":8061516},\"end\":60243,\"start\":60151},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":556331},\"end\":60346,\"start\":60245},{\"attributes\":{\"id\":\"b29\"},\"end\":60423,\"start\":60348},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":13563091},\"end\":60549,\"start\":60425},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1960378},\"end\":60668,\"start\":60551},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":14836480},\"end\":60830,\"start\":60670},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":313180},\"end\":60973,\"start\":60832},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":13929945},\"end\":61136,\"start\":60975},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":263791871},\"end\":61286,\"start\":61138},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1097636},\"end\":61414,\"start\":61288},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":9992589},\"end\":61577,\"start\":61416}]", "bib_title": "[{\"end\":57140,\"start\":57083},{\"end\":57324,\"start\":57282},{\"end\":57407,\"start\":57369},{\"end\":57582,\"start\":57516},{\"end\":57653,\"start\":57614},{\"end\":57800,\"start\":57754},{\"end\":58018,\"start\":57958},{\"end\":58100,\"start\":58046},{\"end\":58239,\"start\":58158},{\"end\":58335,\"start\":58277},{\"end\":58480,\"start\":58370},{\"end\":58746,\"start\":58687},{\"end\":58853,\"start\":58790},{\"end\":58967,\"start\":58895},{\"end\":59090,\"start\":59012},{\"end\":59222,\"start\":59131},{\"end\":59316,\"start\":59265},{\"end\":59432,\"start\":59386},{\"end\":59545,\"start\":59484},{\"end\":59640,\"start\":59599},{\"end\":59798,\"start\":59670},{\"end\":59915,\"start\":59851},{\"end\":60008,\"start\":59962},{\"end\":60096,\"start\":60048},{\"end\":60198,\"start\":60151},{\"end\":60303,\"start\":60245},{\"end\":60504,\"start\":60425},{\"end\":60618,\"start\":60551},{\"end\":60778,\"start\":60670},{\"end\":60921,\"start\":60832},{\"end\":61079,\"start\":60975},{\"end\":61222,\"start\":61138},{\"end\":61354,\"start\":61288},{\"end\":61490,\"start\":61416}]", "bib_author": "[{\"end\":57048,\"start\":57039},{\"end\":57055,\"start\":57048},{\"end\":57147,\"start\":57142},{\"end\":57248,\"start\":57231},{\"end\":57254,\"start\":57248},{\"end\":57332,\"start\":57326},{\"end\":57413,\"start\":57409},{\"end\":57421,\"start\":57413},{\"end\":57431,\"start\":57421},{\"end\":57439,\"start\":57431},{\"end\":57451,\"start\":57439},{\"end\":57589,\"start\":57584},{\"end\":57662,\"start\":57655},{\"end\":57670,\"start\":57662},{\"end\":57679,\"start\":57670},{\"end\":57687,\"start\":57679},{\"end\":57808,\"start\":57802},{\"end\":57815,\"start\":57808},{\"end\":57822,\"start\":57815},{\"end\":57830,\"start\":57822},{\"end\":58033,\"start\":58020},{\"end\":58111,\"start\":58102},{\"end\":58252,\"start\":58241},{\"end\":58347,\"start\":58337},{\"end\":58354,\"start\":58347},{\"end\":58495,\"start\":58482},{\"end\":58671,\"start\":58658},{\"end\":58761,\"start\":58748},{\"end\":58865,\"start\":58855},{\"end\":58874,\"start\":58865},{\"end\":58883,\"start\":58874},{\"end\":58983,\"start\":58969},{\"end\":58990,\"start\":58983},{\"end\":59105,\"start\":59092},{\"end\":59114,\"start\":59105},{\"end\":59237,\"start\":59224},{\"end\":59241,\"start\":59237},{\"end\":59253,\"start\":59241},{\"end\":59331,\"start\":59318},{\"end\":59341,\"start\":59331},{\"end\":59349,\"start\":59341},{\"end\":59365,\"start\":59349},{\"end\":59445,\"start\":59434},{\"end\":59452,\"start\":59445},{\"end\":59459,\"start\":59452},{\"end\":59467,\"start\":59459},{\"end\":59552,\"start\":59547},{\"end\":59562,\"start\":59552},{\"end\":59569,\"start\":59562},{\"end\":59577,\"start\":59569},{\"end\":59651,\"start\":59642},{\"end\":59812,\"start\":59800},{\"end\":59816,\"start\":59812},{\"end\":59828,\"start\":59816},{\"end\":59926,\"start\":59917},{\"end\":59939,\"start\":59926},{\"end\":60024,\"start\":60010},{\"end\":60110,\"start\":60098},{\"end\":60116,\"start\":60110},{\"end\":60211,\"start\":60200},{\"end\":60218,\"start\":60211},{\"end\":60313,\"start\":60305},{\"end\":60319,\"start\":60313},{\"end\":60326,\"start\":60319},{\"end\":60334,\"start\":60326},{\"end\":60377,\"start\":60369},{\"end\":60393,\"start\":60377},{\"end\":60513,\"start\":60506},{\"end\":60522,\"start\":60513},{\"end\":60627,\"start\":60620},{\"end\":60635,\"start\":60627},{\"end\":60789,\"start\":60780},{\"end\":60801,\"start\":60789},{\"end\":60810,\"start\":60801},{\"end\":60937,\"start\":60923},{\"end\":60942,\"start\":60937},{\"end\":60951,\"start\":60942},{\"end\":60958,\"start\":60951},{\"end\":61088,\"start\":61081},{\"end\":61096,\"start\":61088},{\"end\":61105,\"start\":61096},{\"end\":61113,\"start\":61105},{\"end\":61234,\"start\":61224},{\"end\":61249,\"start\":61234},{\"end\":61256,\"start\":61249},{\"end\":61268,\"start\":61256},{\"end\":61366,\"start\":61356},{\"end\":61382,\"start\":61366},{\"end\":61392,\"start\":61382},{\"end\":61402,\"start\":61392},{\"end\":61504,\"start\":61492},{\"end\":61516,\"start\":61504}]", "bib_venue": "[{\"end\":57037,\"start\":56984},{\"end\":57158,\"start\":57147},{\"end\":57229,\"start\":57169},{\"end\":57358,\"start\":57332},{\"end\":57504,\"start\":57451},{\"end\":57604,\"start\":57589},{\"end\":57742,\"start\":57687},{\"end\":57893,\"start\":57830},{\"end\":58037,\"start\":58033},{\"end\":58146,\"start\":58111},{\"end\":58267,\"start\":58252},{\"end\":58357,\"start\":58354},{\"end\":58545,\"start\":58495},{\"end\":58656,\"start\":58597},{\"end\":58780,\"start\":58761},{\"end\":58887,\"start\":58883},{\"end\":59000,\"start\":58990},{\"end\":59120,\"start\":59114},{\"end\":59257,\"start\":59253},{\"end\":59371,\"start\":59365},{\"end\":59471,\"start\":59467},{\"end\":59587,\"start\":59577},{\"end\":59659,\"start\":59651},{\"end\":59841,\"start\":59828},{\"end\":59952,\"start\":59939},{\"end\":60037,\"start\":60024},{\"end\":60142,\"start\":60116},{\"end\":60235,\"start\":60218},{\"end\":60338,\"start\":60334},{\"end\":60367,\"start\":60348},{\"end\":60541,\"start\":60522},{\"end\":60651,\"start\":60635},{\"end\":60820,\"start\":60810},{\"end\":60964,\"start\":60958},{\"end\":61127,\"start\":61113},{\"end\":61271,\"start\":61268},{\"end\":61406,\"start\":61402},{\"end\":61571,\"start\":61516},{\"end\":57952,\"start\":57895},{\"end\":58591,\"start\":58547}]"}}}, "year": 2023, "month": 12, "day": 17}