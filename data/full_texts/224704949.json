{"id": 224704949, "updated": "2023-11-29 18:27:57.893", "metadata": {"title": "Chart-to-Text: Generating Natural Language Descriptions for Charts by Adapting the Transformer Model", "authors": "[{\"first\":\"Jason\",\"last\":\"Obeid\",\"middle\":[]},{\"first\":\"Enamul\",\"last\":\"Hoque\",\"middle\":[]}]", "venue": "INLG", "journal": "Proceedings of the 13th International Conference on Natural Language Generation", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Information visualizations such as bar charts and line charts are very popular for exploring data and communicating insights. Interpreting and making sense of such visualizations can be challenging for some people, such as those who are visually impaired or have low visualization literacy. In this work, we introduce a new dataset and present a neural model for automatically generating natural language summaries for charts. The generated summaries provide an interpretation of the chart and convey the key insights found within that chart. Our neural model is developed by extending the state-of-the-art model for the data-to-text generation task, which utilizes a transformer-based encoder-decoder architecture. We found that our approach outperforms the base model on a content selection metric by a wide margin (55.42% vs. 8.49%) and generates more informative, concise, and coherent summaries.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3094916564", "acl": "2020.inlg-1.20", "pubmed": null, "pubmedcentral": null, "dblp": "conf/inlg/ObeidH20", "doi": "10.18653/v1/2020.inlg-1.20"}}, "content": {"source": {"pdf_hash": "61b759af8cbe28029c89d885cb8175d7d6768259", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/2020.inlg-1.20.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b7032fd72bcba4a950308789e956198ddc391249", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/61b759af8cbe28029c89d885cb8175d7d6768259.txt", "contents": "\nChart-to-Text: Generating Natural Language Descriptions for Charts by Adapting the Transformer Model\nAssociation for Computational LinguisticsCopyright Association for Computational LinguisticsDecember, 2020. 2020\n\nJason Obeid jobeid98@my.yorku.ca \nSchool of Information Technology\nSchool of Information Technology\nYork University\nCanada\n\nEnamul Hoque enamulh@yorku.ca \nYork University\nCanada\n\nChart-to-Text: Generating Natural Language Descriptions for Charts by Adapting the Transformer Model\n\nProceedings of The 13th International Conference on Natural Language Generation\nThe 13th International Conference on Natural Language GenerationDublin, IrelandAssociation for Computational LinguisticsDecember, 2020. 2020138\nInformation visualizations such as bar charts and line charts are very popular for exploring data and communicating insights. Interpreting and making sense of such visualizations can be challenging for some people, such as those who are visually impaired or have low visualization literacy. In this work, we introduce a new dataset and present a neural model for automatically generating natural language summaries for charts. The generated summaries provide an interpretation of the chart and convey the key insights found within that chart. Our neural model is developed by extending the state-of-the-art model for the data-to-text generation task, which utilizes a transformerbased encoder-decoder architecture. We found that our approach outperforms the base model on a content selection metric by a wide margin (55.42% vs. 8.49%) and generates more informative, concise, and coherent summaries.\n\nIntroduction\n\nInformation visualizations such as bar charts and line charts are commonly utilized by people to get insights from data and make informed decisions. However, understanding and getting insights from charts can be difficult and time-consuming. This is because people often need to visually compare between several graphical marks (e.g. bars) of a chart to infer key insights from data, which may be challenging when the chart involves many data items (Kim et al., 2020).\n\nGenerating a natural language summary to explain a chart has numerous benefits and potential applications. It can help users in understanding and interpreting charts by conveying key points about the chart by focusing on temporal, causal, and evaluative aspects (Carenini et al., 2013). It can help people identify insights from charts that they otherwise might have missed. A previous study conducted on a chart corpus found that the text associated with the chart failed to convey any insight from that chart in 35% of the instances, while in another 26% of cases the text conveyed only a portion of the chart's intended message (Carberry et al., 2006). Therefore, effective chart summarization could help data analysts, business analysts, or journalists in better preparing reports from data. Such summaries could also enable people who are visually impaired or have low cognitive abilities to comprehend charts and perform analytical tasks through audio (Ferres et al., 2013).\n\nHowever, natural language generation (NLG) systems for charts are still in their infancy. Early work mostly focused on statistical methods for finding salient information from charts and planningbased approaches for structuring content (Reiter, 2007) to generate textual captions from basic charts (Fasciano and Lapalme, 1996;Mittal et al., 1998;Green et al., 2004;Demir et al., 2012). Commercial systems such as Quill (Quill, 2020) and Wordsmith (Wordsmith, 2020) have similarly adopted statistical algorithms and template-based NLG methods which are used to produce data facts in textual form. Unfortunately, the predefined template-based NLG methods and planning-based architecture for generating summaries often lack generality and may not offer variations in style. Moving beyond template and planning-based approaches (Reiter, 2007), more recently researchers considered data-driven neural models for generating text from data tables (Mei et al., 2016;Gong et al., 2019). However, they do are not designed for chart summarization as they do not consider the chart specific features (e.g. chart type). To our knowledge, there have not been any efforts to develop deep neural models that are specifically tailored for generating chart summaries.\n\nIn this paper, we present a neural model for chart summarization by extending a transformer-based Figure 1: An example of a generated natural language explanation from a chart from our model. model that was originally designed for the datato-text generation task. A key challenge in developing neural models for chart summarization is the lack of a suitable dataset containing a large set of charts and corresponding human-written summaries. To address the challenge we first developed a corpus of 8,305 charts, where for each chart we crawled the corresponding data table and the human-written summary. Our model learns content selection and text generation from such a collection of chart-summary pairs to generate summaries of charts. We also introduce a data variable substitution method that replaces the tokens referring to chart data with variables to generate more factually correct statements than a competitive baseline. As shown in Figure 1, the resulting summary is quite informative, concise, and coherent.\n\nThe contributions of our paper are three-fold: \u2022 First, we introduce a new large-scale corpus on chart summarization consisting of humanwritten summaries of charts along with chart images and their underlying data.\n\n\u2022 Second, we adopt a transformer-based model to generate chart summaries, which learns from chart-summary pairs from our dataset. To our knowledge, our work is the first to investigate the problem of chart summarization using a data-driven deep neural model.\n\n\u2022 Finally, we perform a series of evaluations to compare our model's performance with a baseline model derived from Gong et al. (Gong et al., 2019). As a secondary contribution, we will make our source codes and the new dataset used in this research publicly available.\n\n2 Related Work\n\n\nChart Summarization\n\nEarly work on generating natural language summarization follows some planning-based approaches which can be captured by Reiter's NLG architecture for data-to-text generation (Reiter, 2007). For example, (Mittal et al., 1998) developed a caption generation system which determines the structure of the caption based on the mappings between the data and marks of the charts (such as text, lines, and bars) and chooses a complexity metric to select details of the caption. It then uses a text planner to generate the description of the chart. The iGRAPH-Lite system (Ferres et al., 2013) aims to make charts accessible to blind users via generating captions and supporting navigation through the chart using a keyboard. Like (Mittal et al., 1998) it uses templates to provide a short description of what the chart looks like. Nonetheless, these systems describe the chart only in terms of how to interpret the chart rather than explaining high-level insights conveyed by the chart. Other research has focused on generating multimedia presentations combining both text summary and charts (Fasciano and Lapalme, 2000;Green et al., 2004). PostGraphe takes user-specified intention (e.g. increasing trend) and a spreadsheet file as input and then generates a simple chart and caption separately using some heuristics (Fasciano and Lapalme, 2000). Autobrief generates presentations in text and information graphics (Green et al., 2004) using an integrated planning-based approach in the domain of transportation scheduling.\n\nThere has also been growing interest to automatically extract insights from a dataset and present them using template-based NLG. Examples of such automatic insight generation include research prototypes such as DataSite , DataShot (Wang et al., 2019) and Voder (Srinivasan et al., 2018) as well as commercial systems like Quill 1 , Arria 2 , and Wordsmith 3 . These systems typically perform some statistical analysis to infer potentially important or interesting facts about the data and then present them in natural language sentences and charts. (Demir et al., 2012) present a method that computes some statistics (e.g. min, max, trends) and identifies the intended message that a bar chart is conveying using a Bayesian inference system. Then, it generates the summary in a bottom-up approach to simultaneously construct the discourse and sentence structures of textual summaries. More recently, (Chen et al., 2019) uses the encoder-decoder architecture where a Residual Network (ResNet) (He et al., 2016) in the encoder is used to recognize the input chart from an image, and Long Short-Term Memory (LSTM) and attention in the decoder to create template-based captions.\n\nA common limitation of the above body of work is that the sentences are generated using predefined template-based approaches which may lack generality and offer fewer variations in grammatical style and lexical choices compared to data-driven models. In contrast, we focus on learning language variations and automatically finding important insights from charts using a deep learning model on a large collection of chart-summary pairs.\n\n\nData-to-Text Generation\n\nThe objective of data-to-text generation is to generate a descriptive summary given structured data. Data-to-text generation focuses on creating a descriptive summary from structured data which can be encoded as a table of records. Data to text generation has been explored for various domain-specific tasks such as summarizing sport game data (Barzilay and Lapata, 2005;Liang et al., 2009;Wiseman et al., 2017), weather-forecast data (Reiter et al., 2005), recipe generation (Yang et al., 2017) and biography generation (Lebret et al., 2016).\n\nSeveral recent methods have primarily focused on using sequence-to-sequence learning methods (Mei et al., 2016;Lebret et al., 2016;Wiseman et al., 2017). For example, (Mei et al., 2016) propose an encoder-decoder model that uses recurrent neural networks with LSTM units for jointly learning content selection and surface realization for weather forecast and soccer commentaries. (Wiseman et al., 2017) present a new dataset on NBA game summarization and evaluate several models including attention-based encoder-decoder models. They found that neural text generation techniques from data perform poorly at content selection and lack inter-sentential coherence. (Puduppully et al., 2019) attempt to address this problem by incorporating content selection and planning mechanisms within the neural model. (Gong et al., 2019) found that the transformer model yielded outputs more fluent and coherent when compared to their seq2seq counterparts, which is why we use it as the base model.\n\n\nChart Summarization Dataset\n\nThere have been several benchmark datasets that are made available recently for the data-to-text generation tasks (Lebret et al., 2016;Wiseman et al., 2017;Chen et al., 2020;Parikh et al., 2020). However, to the best of our knowledge, there are no publicly available large datasets of chart data paired with human-generated summaries. While some prior work on generating captions and summaries from charts (e.g. (Mittal et al., 1998;Green et al., 2004;Demir et al., 2012)) exist, to our knowledge they do not provide any large datasets with chartsummary pairs.\n\nWith the above challenges in mind, we create a new dataset for chart summarization which is available at https://github.com/JasonObeid/ Chart2Text. In order to find the best source of data for our corpus, we have analyzed publicly available charts from various sources such as textbooks, research papers, news articles, and websites that contain data charts and facts. This led us to choose Statista 4 as our data source. Statista regularly publishes charts from data collected by market and opinion research institutes, and data derived from the economic sector. Since Statista has all the necessary metadata including the data tables, titles, and concise summaries of charts it was a suitable source for our purpose. The dataset was crawled from 23,382 freely accessible pages from statista.com in early March of 2020, yielding a total of 8,305 charts, and associated summaries. For each chart, we downloaded the chart image, the underlying data table, the title, the axis labels, and a human-written summary describing the statistic.\n\nAfter examining the crawled dataset, we found that out of 8,305 charts, 7,726 charts were lacking the x-axis labels. To address this problem we searched by regular expressions and applied named entity recognition using CoreNLP (Manning et al., 2014) on the data table to automatically identify if the x-axis represented common temporal dimensions such as years or months. For the remaining 1,353 missing labels we used human annotators who examined the title, summary and the chart to  The resulting dataset can be described as follows: let the set of chart-summaries CS = [I, D, T, L, S], where for each chart-summary in CS there is a chart image i \u2208 I, data table d \u2208 D, title t \u2208 T , axis labels l \u2208 L, and summary s \u2208 S. The dataset consists of both bar and line charts (Table 2). A simple bar chart only has a set of bars, and a simple line chart contains a single line. Complex bar charts include stacked and grouped bar charts while complex line charts have multiple lines.\n\nThe summaries of the charts are concise, with an average sentence count of 5.2, and an average token count of 113.4. The data tables were moderate in size with an average of 32.3 cells. The discourse structures of these summaries were usually similar. They usually start by describing the chart at a high-level in terms of what this chart is about and then describing and comparing salient points in the chart. Some common salient information and statistics are extremes (e.g. highest/lowest values) or trends (upward/downward tendencies) or simple value retrieval (e.g. mentioning the first/last data point's value).\n\n\nThe Chart-to-Text Model\n\nIn this section, we first describe the Transformerbased Model for Data-to-Text (Gong et al., 2019) which we use as our base model, followed by our adaptations to this model for generating chart summaries. Then, we describe our experiments with model parameters and the training procedure.\n\n\nBase Model\n\nOur base model (Gong et al., 2019) extends the standard transformer (Vaswani et al., 2017) by adding a binary prediction layer and a content selection training step. The input layer of the model accepts a tuple of four features (entity, type, value, information) as input. The model then generates the latent representation of each record in the input sequence and passes it through the binary prediction layer. The decoder of this model is the same as the original Transformer model and predicts the summary based on encoder's output. The model also removes the positional embedding of the transformer encoder, as there is no ordered relationship within the records of their dataset. Figure 2 shows an overview of our proposed approach. The model takes data records and other chart information as input. Like the base model (Gong et al., 2019), it uses the self-attention mechanism of the transformer encoder to generate the latent representations of the input, and the binary prediction at the top of the Transformer encoder output to decide whether or not a record will be mentioned in the target summary. Finally, the decoder predicts the next token in the context of the encoder output and the previous tokens from the summary sequence.\n\n\nOur Proposed Approach\n\nIn order to adapt the enhanced transformer model (Gong et al., 2019) for generating chart summarization, we introduce three main changes. First, we modify the four features of the record tuples used as input to the model. This is done to include additional information which is important for chart summarization. Second, we reintroduce positional embeddings to the encoder since charts tend to contain ordered relationships. Finally, we introduce a process of substituting tokens with data variables to minimize the amount of generated hallucinations, a problem commonly found in NLG where a predicted word is not grounded in truth (Wiseman et al., 2017;Parikh et al., 2020). We now discuss each of these changes in more details:\n\nInput embedding: For each data table d \u2208 D, we pre-process it into a set of records r \u2208 R. Each of these records are placed in a tuple with four features as follows:\n\n\u2022 r i (0) contains the column header, \u2022 r i (1) contains the table cell value, \u2022 r i (2) contains the column index, and \n\n\n\u2022 r i (3) contains the chart type\n\nEach feature is embedded into a vector, and together they are concatenated to represent the record as shown below:\n\n\u2200 r \u2208 R:\nr i = [r i (0); r i (1); r i (2); r i (3)]\nOur model takes each of these records r i as input, and outputs a set of predicted tokens which we will denote as y \u2208 Y. To get the final summary, for each token y i in y, if the token is a data variable then it is substituted with the corresponding data value.\n\nPositional embedding: Unlike the sports dataset used by (Gong et al., 2019), chart data often involves an ordered dimension such as temporal data (e.g. year, month), or ordinal categories in bar charts. In order to generate summaries that better capture salient features from such sequential data, we re-introduce positional embeddings to our model.\n\nData variable substitution: A critical issue with various existing models for data-to-text generation problem such as (Gong et al., 2019) is that they treat data records mentioned in the summaries as regular tokens which often results in hallucination problem. As a consequence, the model sometimes predicts tokens that are irrelevant to the chart and thus results in factually incorrect sentences. This problem becomes even more serious for our chart dataset which is not focused on a specific domain (e.g. sports). In order to improve the factual accuracy of the generated summaries we introduce a data variable substitution method.\n\nThe idea is that before training we first modify the gold summaries so that whenever a token references something from the data table, chart title, or axis labels we replace them with one of the predefined data variables. We then use these modified summaries to train the model so that it learns how to generate the summary more generally with data variables as opposed to actual values in the data table or tokens from the title. Then, during the testing phase, if a generated token matches a predefined data variable, we make a look-up operation to convert the data variable into the referenced chart data (see Figure 3).\n\nWe define seven categories of data variables listed in order of priority: subjects, dates, axis labels, titles, table cells, trends, and scales. Here subjects refer to entities relevant to the data table (e.g. 'Liverpool FC') and trends refer to tokens that indicate positive or negative trends (e.g. growing, decreasing, etc). Scales refer to tokens such as 'millions' and 'percentage' that are associated with numeric tokens. For each token t i in a gold summary s, if that token matches any of these variable categories, then it is substituted by that variable. We apply named entity recognition (Manning et al., 2014) to detect the subjects and dates. The rest of the variables are detected using simple string pattern matching techniques. For each variable, we also assign the relevant index position in the data table or other chart data. For example, given a sentence in the gold summary \"Broadcasting is the largest source of revenue for Liverpool FC\", During the testing phase, our approach performs the reverse operation where it substitutes data variables with their referenced tokens. Figure 3 demonstrates how the data variables are substituted with actual tokens from the chart data. Here, the corresponding tokens and variables are highlighted by the same color.\n\n\nTraining\n\nThe model requires two types of labels for its training step. The first type of label is for the chart's data, which for each record r i in the set of records r there is either a label of 1 if r i is mentioned in the summary s, or a 0 if not. The second type of label is for the chart's summary, where for each token t i \u2208 a gold summary s there is a label of 1 if t is also present in some records in r, or a 0 otherwise.\n\nThe dataset was divided into training, validation, and testing sets with a 70%:15%:15% ratio for training. We trained the model for 80 epochs with an epoch size of 1000, using the following hyper-parameters: 1 encoder layer, 6 decoder layers, embedding size of 512, batch size of 6, beam size of 4, sinusoidal positional embeddings, and GELU (Hendrycks and Gimpel, 2016) activations.\n\n\nEvaluation\n\nWe first perform an automatic evaluation to approximately measure the performance of our approach compared to the base model (Gong et al., 2019) and then we conduct a human evaluation. Finally, we perform some qualitative analysis to have a better  Table 3: Results of automatic evaluation for two summarization methods (The content section measure is shown in percentage and the standard deviation is mentioned in the bracket).\n\nunderstanding of the effectiveness and trade-offs of our approach.\n\n\nAutomatic Evaluation\n\nFor automated evaluation of our summary quality, we employ two different metrics: (1) the BLEU score (?), and (2) a content selection metric inspired by (Wiseman et al., 2017). We calculate our content selection metric as the percentage of records mentioned in the generated summary that are mentioned in the gold summary. Table 3 shows the results of automatic evaluation of our model compared to the base model. We observe that our model slightly improves over the baseline on the BLEU metric and outperforms it on content selection by a wide margin. In particular, the huge improvement on the content selection metric suggests that by learning how to generate summaries in terms of data variables rather than actual data values, our approach largely addresses the hallucination problem.\n\n\nHuman Evaluation\n\nTo further investigate the quality of the generated text, we perform a human evaluation. For this purpose, we randomly sample 40 different charts where we have 10 charts from each of four chart types (simple bar, complex bar, simple line, complex line). We use the model from (Gong et al., 2019) trained on our dataset with positional embeddings enabled as our baseline. We created a Mechanical Turk study and surveyed three unique respondents per statistic. We use the survey to assess the quality of each summary from four independent perspectives: (1) Informativeness: How informative is the summary of the chart? (2) Conciseness: How concise is the summary of the chart?, (3) Coherence: How coherent is the summary of the chart? and (4) Fluency: How fluent or grammatically correct are the sentences in the summary of the chart? The questions were asked using a   As shown in Table 4, on average our model outperforms the base model in terms of informativeness, conciseness, and coherence by at least over 1 point. Overall, it indicates that our method can generate summaries that are more insightful, that have better connections between sentences and with fewer repetitions. In terms of fluency, the base model performs slightly better (3.78) than our model (3.73).\n\nWe also wanted to evaluate the factuality aspect of the generated summaries. For this purpose, we asked respondents to evaluate whether the facts stated in each sentence of the summary were supported by the chart. There were four possible responses to this question: yes, no, partially, and can't decide (explain why). Table 5 shows the percentage of statements within the summary that were perceived as factually correct or not. We find that over 50% sentences generated by our model were verified as factually correct, whereas only 22% sentences from the baseline model's summaries were perceived to be correct. Another 15.56% statements generated from our model were verified as partially correct compared to 7.24% from the base model. This result suggests that our model generates more factually correct statements compared to the base model. \n\n\nCase Study\n\nIn order to better understand how our approach works we reviewed one random sample from our dataset (see Figure 4). We notice that the gold summary is relatively longer than the two generated summaries with some additional external facts that are not mentioned in the chart data (e.g. 'the New England Patriots' and 'their Twitter following'). This illustrates the challenge for generating chart summaries because without direct connections between the chart and its summary, it becomes more difficult for the model to learn what is relevant. Our model's summary is similar in structure to the gold, but without mention of these external statistics. The summary is mostly accurate, as it correctly mentions the most and least followed teams along with their corresponding fan counts (highlighted in green), but it incorrectly predicts tokens related to game wins, and also incorrectly mentions the third highest team (highlighted in red) right after it cor- Figure 5: Comparison of generated summaries with positional embeddings enabled and disabled (Here the text that refers to a bar in the chart is highlighted with the same fill-color of that bar). rectly mentioned the lowest team. When analyzing the summary generated from (Gong et al., 2019), we can readily see that it makes more false predictions than our model. It correctly mentions the first team name, but then after that it suffers from the hallucination problem as it repeats a memorized summary which it was trained on. As a result, this summary is mostly irrelevant to the given chart. This is also reflected from the automatic evaluation (Table 3) where the model from Gong et al. performed poorly on the content selection metric.\n\nWe also investigated the impact of positional embeddings on our model's performance. As we can see from Figure 5, for a sample chart the model that enables positional embeddings yielded summaries with a more meaningful discourse structure by better connecting between sentences and with more meaningful salient information. This support that positional embeddings allow the model to track sequential data as suggested by (Vaswani et al., 2017) for the original Transformer model.\n\n\nError Analysis\n\nWhile our evaluation reveals that our model is more effective at chart summary generation than the base model, it still has much room for improvement. In particular, we identify two categories of common errors which are present in our generated sum- maries. The most common error is the fact hallucination, which is demonstrated in Figure 4. While our model has largely addressed this issue, irrelevant tokens still occasionally occur. Our error analysis on 50 random samples indicates that hallucination can occur in two main ways in our model. First, sometimes the model predicts a data variable at an incorrect index, for example it may try to refer to one index of a table row, but the correct index is actually that of another index. The second hallucination method is when the model simply predicts a token which is irrelevant to the data, which we see commonly in other data-to-text works (Wiseman et al., 2017;Parikh et al., 2020). This error usually occurs when the model generates summaries of charts about a domain that has low coverage in the training set. For example, our training set has several charts on finance and sports. For these charts, the generated summaries are quite fluent and tokens that are irrelevant to the corresponding chart are rare. Figure 6 shows how the generated summaries differ in fluency and amount of irrelevant tokens for two different charts: one from a domain with low training data coverage and the other with high training data coverage.\n\n\nConclusion and Future Work\n\nIn this paper, we tackle the challenge of automatic chart summarization by introducing a new dataset and proposing a neural approach based on the transformer architecture. Our approach learns how to generate natural language descriptions from charts by modifying the gold summaries to replace references to chart data values with data variables. As a result, the model learns how to summarize in a more generalized way, and generates more factually correct statements compared to the base model. Our model also generates summaries that are more informative, concise, and coherent according to human evaluations. We hope that our work will motivate researchers to further improve the quality of summaries automatically generated from charts, which is a highly under-explored research area.\n\nIn the future, we would like to develop larger datasets that cover more diverse domains and additional chart types (e.g. pie charts, scatterplots, heatmaps etc.) to further improve the quality and generalizability of our model. Also, while we compared our method with a strong baseline from (Gong et al., 2019), we will perform further comparisons with other models which were developed for the data-to-text generation problem. Finally, we would like to build applications such as an interactive chart summarization system with a focus on enhancing the accessibility of charts so that blind and visually impaired people can comprehend charts via audio.\n\nFigure 2 :\n2An overview of the proposed approach for chart summarization using a transformer-based model. The model takes the data table and some chart metadata as input (on the left) and generates a summary containing data variables that refer values within the data table.\n\nFigure 3 :\n3Demonstration of data variable substitution. the system replaces the token 'Broadcasting' with < templateLabel[2][0] > which refers to the third column's header label.\n\nFigure 4 :\n4Comparison of summary generation methods. Here, factually correct statements are highlighted in green and incorrect statements are highlighted in red.\n\nFigure 6 :\n6Comparison of generated summaries by our model based on data coverage.\n\nTable 2 :\n2Dataset statistics come up with short descriptive labels for the x-axis.\n\n\nEvaluation Category Our ModelGong et al.    Summary Method \n\nInformativeness \n3.42 (1.16) 2.13 (1.44) \nConciseness \n3.58 (1.11) 2.40 (1.47) \nCoherence \n3.32 (1.25) 2.27 (1.43) \nFluency \n3.73 (1.07) 3.78 (0.85) \n\n\n\nTable 4 :\n4Results of human evaluation (standard devia-\ntion in brackets) \n\nSummary Method \n\nResponse \nOur Model Gong et al. \n\nYes \n51.02% \n22.70% \nNo \n26.30% \n66.02% \nPartial \n15.56% \n7.24% \nCan't decide \n7.13% \n4.04% \n\n\n\nTable 5 :\n5Responses for factual correctness \n\n5-point Likert scale from 1 (the worst) to 5 (the \nbest). \n\n\nNarrative Science. Quill 2 Arria, Arria 3 Wordsmith Wordsmith\nhttps://www.statista.com/\nAcknowledgmentsThis work was supported by the Natural Sciences and Engineering Research Council (NSERC), Canada. We thank Dhruv Nayyar for helping with preparing the dataset.\nCollective content selection for concept-to-text generation. Regina Barzilay, Mirella Lapata, Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing. Human Language Technology Conference and Conference on Empirical Methods in Natural Language ProcessingBritish Columbia, CanadaAssociation for Computational LinguisticsRegina Barzilay and Mirella Lapata. 2005. Collective content selection for concept-to-text generation. In Proceedings of Human Language Technology Con- ference and Conference on Empirical Methods in Natural Language Processing, pages 331-338, Van- couver, British Columbia, Canada. Association for Computational Linguistics.\n\nInformation graphics: an untapped resource for digital libraries. Sandra Carberry, Stephanie Elzer, Seniz Demir, Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. the 29th annual international ACM SIGIR conference on Research and development in information retrievalSandra Carberry, Stephanie Elzer, and Seniz Demir. 2006. Information graphics: an untapped resource for digital libraries. In Proceedings of the 29th an- nual international ACM SIGIR conference on Re- search and development in information retrieval, pages 581-588.\n\nUser task adaptation in multimedia presentations. Giuseppe Carenini, Cristina Conati, Enamul Hoque, Ben Steichen, Proceedings of the 1st International Workshop on User-Adaptive Information Visualization in conjunction with the 21st conference on User Modeling, Adaptation and Personalization (UMAP). the 1st International Workshop on User-Adaptive Information Visualization in conjunction with the 21st conference on User Modeling, Adaptation and Personalization (UMAP)Giuseppe Carenini, Cristina Conati, Enamul Hoque, and Ben Steichen. 2013. User task adaptation in multimedia presentations. In Proceedings of the 1st International Workshop on User-Adaptive Informa- tion Visualization in conjunction with the 21st con- ference on User Modeling, Adaptation and Person- alization (UMAP).\n\nFigure captioning with reasoning and sequence-level training. Charles Chen, Ruiyi Zhang, Eunyee Koh, Sungchul Kim, Scott Cohen, Tong Yu, Ryan A Rossi, Razvan C Bunescu, abs/1906.02850CoRRCharles Chen, Ruiyi Zhang, Eunyee Koh, Sungchul Kim, Scott Cohen, Tong Yu, Ryan A. Rossi, and Razvan C. Bunescu. 2019. Figure captioning with reasoning and sequence-level training. CoRR, abs/1906.02850.\n\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, William Yang Wang, arXiv:2004.10404Logical natural language generation from open-domain tables. arXiv preprintWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and William Yang Wang. 2020. Logical natural lan- guage generation from open-domain tables. arXiv preprint arXiv:2004.10404.\n\nDatasite: Proactive visual data exploration with computation of insightbased recommendations. Zhe Cui, Karthik Sriram, Badam, Niklas Yal\u00e7in, Elmqvist, Information Visualization. 182Zhe Cui, Sriram Karthik Badam, M Adil Yal\u00e7in, and Niklas Elmqvist. 2019. Datasite: Proactive vi- sual data exploration with computation of insight- based recommendations. Information Visualization, 18(2):251-267.\n\nSummarizing information graphics textually. Seniz Demir, Sandra Carberry, Kathleen F Mccoy, 10.1162/COLI_a_00091Computational Linguistics. 383Seniz Demir, Sandra Carberry, and Kathleen F. McCoy. 2012. Summarizing information graphics textually. Computational Linguistics, 38(3):527-574.\n\nPostgraphe: a system for the generation of statistical graphics and text. Massimo Fasciano, Guy Lapalme, Eighth International Natural Language Generation Workshop. Massimo Fasciano and Guy Lapalme. 1996. Post- graphe: a system for the generation of statistical graphics and text. In Eighth International Natural Language Generation Workshop.\n\nIntentions in the coordinated generation of graphics and text from tabular data. Knowledge and Information Systems. Massimo Fasciano, Guy Lapalme, 2Massimo Fasciano and Guy Lapalme. 2000. Intentions in the coordinated generation of graphics and text from tabular data. Knowledge and Information Sys- tems, 2:310-339.\n\nEvaluating a tool for improving accessibility to charts and graphs. Leo Ferres, Gitte Lindgaard, Livia Sumegi, Bruce Tsuji, 10.1145/2533682.2533683ACM Trans. Comput.-Hum. Interact. 205Leo Ferres, Gitte Lindgaard, Livia Sumegi, and Bruce Tsuji. 2013. Evaluating a tool for improving acces- sibility to charts and graphs. ACM Trans. Comput.- Hum. Interact., 20(5).\n\nEnhanced transformer model for data-to-text generation. Li Gong, M Josep, Jean Crego, Senellart, Proceedings of the 3rd Workshop on Neural Generation and Translation. the 3rd Workshop on Neural Generation and TranslationLi Gong, Josep M Crego, and Jean Senellart. 2019. Enhanced transformer model for data-to-text genera- tion. In Proceedings of the 3rd Workshop on Neural Generation and Translation, pages 148-156.\n\nAutobrief: an experimental system for the automatic generation of briefings in integrated text and information graphics. Giuseppe Nancy L Green, Stephan Carenini, Joe Kerpedjiev, Johanna D Mattis, Steven F Moore, Roth, International Journal of Human-Computer Studies. 611Nancy L Green, Giuseppe Carenini, Stephan Kerped- jiev, Joe Mattis, Johanna D Moore, and Steven F Roth. 2004. Autobrief: an experimental system for the automatic generation of briefings in integrated text and information graphics. International Jour- nal of Human-Computer Studies, 61(1):32-70.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770- 778.\n\nBridging nonlinearities and stochastic regularizers with gaussian error linear units. Dan Hendrycks, Kevin Gimpel, abs/1606.08415CoRRDan Hendrycks and Kevin Gimpel. 2016. Bridging nonlinearities and stochastic regularizers with gaus- sian error linear units. CoRR, abs/1606.08415.\n\nAnswering questions about charts and generating visual explanations. Enamul Dae Hyun Kim, Maneesh Hoque, Agrawala, 10.1145/3313831.3376467Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. the 2020 CHI Conference on Human Factors in Computing Systems20Dae Hyun Kim, Enamul Hoque, and Maneesh Agrawala. 2020. Answering questions about charts and generating visual explanations. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, CHI 20, page 1-13.\n\nNeural text generation from structured data with application to the biography domain. R\u00e9mi Lebret, David Grangier, Michael Auli, 10.18653/v1/D16-1128Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsR\u00e9mi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In Proceed- ings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203-1213, Austin, Texas. Association for Computational Lin- guistics.\n\nLearning semantic correspondences with less supervision. Percy Liang, Dan Michael I Jordan, Klein, Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLPPercy Liang, Michael I Jordan, and Dan Klein. 2009. Learning semantic correspondences with less super- vision. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th In- ternational Joint Conference on Natural Language Processing of the AFNLP, pages 91-99.\n\nThe Stanford CoreNLP natural language processing toolkit. Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J Bethard, David Mc-Closky, Association for Computational Linguistics (ACL) System Demonstrations. Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David Mc- Closky. 2014. The Stanford CoreNLP natural lan- guage processing toolkit. In Association for Compu- tational Linguistics (ACL) System Demonstrations, pages 55-60.\n\nWhat to talk about and how? selective generation using lstms with coarseto-fine alignment. Hongyuan Mei, Mohit Uchicago, Matthew R Bansal, Walter, Proceedings of NAACL-HLT. NAACL-HLTHongyuan Mei, TTI UChicago, Mohit Bansal, and Matthew R Walter. 2016. What to talk about and how? selective generation using lstms with coarse- to-fine alignment. In Proceedings of NAACL-HLT, pages 720-730.\n\nDescribing complex charts in natural language: A caption generation system. O Vibhu, Johanna D Mittal, Giuseppe Moore, Steven Carenini, Roth, Computational Linguistics. 243Vibhu O. Mittal, Johanna D. Moore, Giuseppe Carenini, and Steven Roth. 1998. Describing com- plex charts in natural language: A caption genera- tion system. Computational Linguistics, 24(3):431- 467.\n\nP Ankur, Xuezhi Parikh, Sebastian Wang, Manaal Gehrmann, Bhuwan Faruqui, Dhingra, arXiv:2004.14373Diyi Yang, and Dipanjan Das. 2020. Totto: A controlled table-to-text generation dataset. arXiv preprintAnkur P Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. Totto: A controlled table-to-text generation dataset. arXiv preprint arXiv:2004.14373.\n\nData-to-text generation with content selection and planning. Ratish Puduppully, Li Dong, Mirella Lapata, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Ratish Puduppully, Li Dong, and Mirella Lapata. 2019. Data-to-text generation with content selection and planning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6908-6915.\n\nNarrative science. 2020Quill. 2020. Narrative science.\n\nAn architecture for data-to-text systems. Ehud Reiter, Proceedings of the Eleventh European Workshop on Natural Language Generation. the Eleventh European Workshop on Natural Language GenerationAssociation for Computational LinguisticsEhud Reiter. 2007. An architecture for data-to-text systems. In Proceedings of the Eleventh European Workshop on Natural Language Generation, pages 97-104. Association for Computational Linguistics.\n\nChoosing words in computergenerated weather forecasts. Ehud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu, Ian Davy, Artificial Intelligence. 1671-2Ehud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu, and Ian Davy. 2005. Choosing words in computer- generated weather forecasts. Artificial Intelligence, 167(1-2):137-169.\n\nAugmenting visualizations with interactive data facts to facilitate interpretation and communication. Arjun Srinivasan, M Steven, Alex Drucker, John Endert, Stasko, IEEE transactions on visualization and computer graphics. 251Arjun Srinivasan, Steven M Drucker, Alex Endert, and John Stasko. 2018. Augmenting visualizations with interactive data facts to facilitate interpretation and communication. IEEE transactions on visualization and computer graphics, 25(1):672-681.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information pro- cessing systems, pages 5998-6008.\n\nDatashot: Automatic generation of fact sheets from tabular data. Yun Wang, Zhida Sun, Haidong Zhang, Weiwei Cui, Ke Xu, Xiaojuan Ma, Dongmei Zhang, IEEE transactions on visualization and computer graphics. 261Yun Wang, Zhida Sun, Haidong Zhang, Weiwei Cui, Ke Xu, Xiaojuan Ma, and Dongmei Zhang. 2019. Datashot: Automatic generation of fact sheets from tabular data. IEEE transactions on visualization and computer graphics, 26(1):895-905.\n\nChallenges in data-to-document generation. Sam Wiseman, M Stuart, Alexander M Shieber, Rush, arXiv:1707.08052arXiv preprintSam Wiseman, Stuart M Shieber, and Alexander M Rush. 2017. Challenges in data-to-document gen- eration. arXiv preprint arXiv:1707.08052.\n\nWordsmith by automated insights. Wordsmith, incWordsmith. 2020. Wordsmith by automated insights, inc.\n\nReference-aware language models. Zichao Yang, Phil Blunsom, Chris Dyer, Wang Ling, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingZichao Yang, Phil Blunsom, Chris Dyer, and Wang Ling. 2017. Reference-aware language models. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1850-1859.\n", "annotations": {"author": "[{\"end\":339,\"start\":216},{\"end\":394,\"start\":340}]", "publisher": "[{\"end\":143,\"start\":102},{\"end\":697,\"start\":656}]", "author_last_name": "[{\"end\":227,\"start\":222},{\"end\":352,\"start\":347}]", "author_first_name": "[{\"end\":221,\"start\":216},{\"end\":346,\"start\":340}]", "author_affiliation": "[{\"end\":338,\"start\":250},{\"end\":393,\"start\":371}]", "title": "[{\"end\":101,\"start\":1},{\"end\":495,\"start\":395}]", "venue": "[{\"end\":576,\"start\":497}]", "abstract": "[{\"end\":1620,\"start\":721}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2103,\"start\":2085},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2391,\"start\":2368},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2760,\"start\":2737},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3085,\"start\":3064},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3338,\"start\":3324},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3414,\"start\":3386},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3434,\"start\":3414},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3453,\"start\":3434},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3472,\"start\":3453},{\"end\":3520,\"start\":3507},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3552,\"start\":3535},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3926,\"start\":3912},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4046,\"start\":4028},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4064,\"start\":4046},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5983,\"start\":5952},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6333,\"start\":6319},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6368,\"start\":6348},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6729,\"start\":6708},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6888,\"start\":6867},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7257,\"start\":7229},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7276,\"start\":7257},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7483,\"start\":7455},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7572,\"start\":7552},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7912,\"start\":7893},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7948,\"start\":7923},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8230,\"start\":8211},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8581,\"start\":8562},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8671,\"start\":8654},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9672,\"start\":9645},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9691,\"start\":9672},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9712,\"start\":9691},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9757,\"start\":9736},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9796,\"start\":9777},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9843,\"start\":9822},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9957,\"start\":9939},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9977,\"start\":9957},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9998,\"start\":9977},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10031,\"start\":10013},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10533,\"start\":10508},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10669,\"start\":10650},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10997,\"start\":10976},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11018,\"start\":10997},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11036,\"start\":11018},{\"end\":11056,\"start\":11036},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11295,\"start\":11274},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11314,\"start\":11295},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11333,\"start\":11314},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12711,\"start\":12689},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14187,\"start\":14168},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14426,\"start\":14407},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14482,\"start\":14460},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15236,\"start\":15217},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15727,\"start\":15708},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16313,\"start\":16291},{\"end\":16333,\"start\":16313},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17221,\"start\":17202},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17634,\"start\":17615},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19379,\"start\":19357},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20842,\"start\":20814},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21014,\"start\":20995},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21566,\"start\":21544},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22496,\"start\":22477},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25584,\"start\":25565},{\"end\":25984,\"start\":25973},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26479,\"start\":26457},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":27452,\"start\":27430},{\"end\":27472,\"start\":27452},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29149,\"start\":29130}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29767,\"start\":29492},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29948,\"start\":29768},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30112,\"start\":29949},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30196,\"start\":30113},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30281,\"start\":30197},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30496,\"start\":30282},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":30719,\"start\":30497},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":30828,\"start\":30720}]", "paragraph": "[{\"end\":2104,\"start\":1636},{\"end\":3086,\"start\":2106},{\"end\":4337,\"start\":3088},{\"end\":5358,\"start\":4339},{\"end\":5574,\"start\":5360},{\"end\":5834,\"start\":5576},{\"end\":6105,\"start\":5836},{\"end\":6121,\"start\":6107},{\"end\":7660,\"start\":6145},{\"end\":8836,\"start\":7662},{\"end\":9273,\"start\":8838},{\"end\":9844,\"start\":9301},{\"end\":10830,\"start\":9846},{\"end\":11422,\"start\":10862},{\"end\":12460,\"start\":11424},{\"end\":13442,\"start\":12462},{\"end\":14061,\"start\":13444},{\"end\":14377,\"start\":14089},{\"end\":15633,\"start\":14392},{\"end\":16388,\"start\":15659},{\"end\":16555,\"start\":16390},{\"end\":16677,\"start\":16557},{\"end\":16829,\"start\":16715},{\"end\":16839,\"start\":16831},{\"end\":17144,\"start\":16883},{\"end\":17495,\"start\":17146},{\"end\":18131,\"start\":17497},{\"end\":18756,\"start\":18133},{\"end\":20035,\"start\":18758},{\"end\":20470,\"start\":20048},{\"end\":20855,\"start\":20472},{\"end\":21298,\"start\":20870},{\"end\":21366,\"start\":21300},{\"end\":22180,\"start\":21391},{\"end\":23472,\"start\":22201},{\"end\":24321,\"start\":23474},{\"end\":26034,\"start\":24336},{\"end\":26515,\"start\":26036},{\"end\":28018,\"start\":26534},{\"end\":28837,\"start\":28049},{\"end\":29491,\"start\":28839}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16882,\"start\":16840}]", "table_ref": "[{\"end\":21126,\"start\":21119},{\"end\":21721,\"start\":21714},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":23088,\"start\":23081},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23800,\"start\":23793},{\"end\":25951,\"start\":25942}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1634,\"start\":1622},{\"attributes\":{\"n\":\"2.1\"},\"end\":6143,\"start\":6124},{\"attributes\":{\"n\":\"2.2\"},\"end\":9299,\"start\":9276},{\"attributes\":{\"n\":\"3\"},\"end\":10860,\"start\":10833},{\"attributes\":{\"n\":\"4\"},\"end\":14087,\"start\":14064},{\"attributes\":{\"n\":\"4.1\"},\"end\":14390,\"start\":14380},{\"attributes\":{\"n\":\"4.2\"},\"end\":15657,\"start\":15636},{\"end\":16713,\"start\":16680},{\"attributes\":{\"n\":\"4.3\"},\"end\":20046,\"start\":20038},{\"attributes\":{\"n\":\"5\"},\"end\":20868,\"start\":20858},{\"attributes\":{\"n\":\"5.1\"},\"end\":21389,\"start\":21369},{\"attributes\":{\"n\":\"5.2\"},\"end\":22199,\"start\":22183},{\"attributes\":{\"n\":\"5.3\"},\"end\":24334,\"start\":24324},{\"attributes\":{\"n\":\"5.4\"},\"end\":26532,\"start\":26518},{\"attributes\":{\"n\":\"6\"},\"end\":28047,\"start\":28021},{\"end\":29503,\"start\":29493},{\"end\":29779,\"start\":29769},{\"end\":29960,\"start\":29950},{\"end\":30124,\"start\":30114},{\"end\":30207,\"start\":30198},{\"end\":30507,\"start\":30498},{\"end\":30730,\"start\":30721}]", "table": "[{\"end\":30496,\"start\":30328},{\"end\":30719,\"start\":30509},{\"end\":30828,\"start\":30732}]", "figure_caption": "[{\"end\":29767,\"start\":29505},{\"end\":29948,\"start\":29781},{\"end\":30112,\"start\":29962},{\"end\":30196,\"start\":30126},{\"end\":30281,\"start\":30209},{\"end\":30328,\"start\":30284}]", "figure_ref": "[{\"end\":4445,\"start\":4437},{\"end\":5290,\"start\":5282},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15085,\"start\":15077},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18754,\"start\":18746},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19863,\"start\":19855},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24449,\"start\":24441},{\"end\":25302,\"start\":25294},{\"end\":26148,\"start\":26140},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26874,\"start\":26866},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27810,\"start\":27802}]", "bib_author_first_name": "[{\"end\":31159,\"start\":31153},{\"end\":31177,\"start\":31170},{\"end\":31872,\"start\":31866},{\"end\":31892,\"start\":31883},{\"end\":31905,\"start\":31900},{\"end\":32460,\"start\":32452},{\"end\":32479,\"start\":32471},{\"end\":32494,\"start\":32488},{\"end\":32505,\"start\":32502},{\"end\":33260,\"start\":33253},{\"end\":33272,\"start\":33267},{\"end\":33286,\"start\":33280},{\"end\":33300,\"start\":33292},{\"end\":33311,\"start\":33306},{\"end\":33323,\"start\":33319},{\"end\":33332,\"start\":33328},{\"end\":33334,\"start\":33333},{\"end\":33348,\"start\":33342},{\"end\":33350,\"start\":33349},{\"end\":33587,\"start\":33582},{\"end\":33601,\"start\":33594},{\"end\":33610,\"start\":33608},{\"end\":33620,\"start\":33615},{\"end\":33634,\"start\":33627},{\"end\":33639,\"start\":33635},{\"end\":34005,\"start\":34002},{\"end\":34018,\"start\":34011},{\"end\":34040,\"start\":34034},{\"end\":34352,\"start\":34347},{\"end\":34366,\"start\":34360},{\"end\":34385,\"start\":34377},{\"end\":34387,\"start\":34386},{\"end\":34672,\"start\":34665},{\"end\":34686,\"start\":34683},{\"end\":35057,\"start\":35050},{\"end\":35071,\"start\":35068},{\"end\":35323,\"start\":35320},{\"end\":35337,\"start\":35332},{\"end\":35354,\"start\":35349},{\"end\":35368,\"start\":35363},{\"end\":35674,\"start\":35672},{\"end\":35682,\"start\":35681},{\"end\":35694,\"start\":35690},{\"end\":36162,\"start\":36154},{\"end\":36185,\"start\":36178},{\"end\":36199,\"start\":36196},{\"end\":36219,\"start\":36212},{\"end\":36221,\"start\":36220},{\"end\":36238,\"start\":36230},{\"end\":36653,\"start\":36646},{\"end\":36665,\"start\":36658},{\"end\":36681,\"start\":36673},{\"end\":36691,\"start\":36687},{\"end\":37135,\"start\":37132},{\"end\":37152,\"start\":37147},{\"end\":37403,\"start\":37397},{\"end\":37425,\"start\":37418},{\"end\":37923,\"start\":37919},{\"end\":37937,\"start\":37932},{\"end\":37955,\"start\":37948},{\"end\":38567,\"start\":38562},{\"end\":38578,\"start\":38575},{\"end\":39272,\"start\":39261},{\"end\":39274,\"start\":39273},{\"end\":39289,\"start\":39284},{\"end\":39304,\"start\":39300},{\"end\":39317,\"start\":39312},{\"end\":39332,\"start\":39326},{\"end\":39334,\"start\":39333},{\"end\":39349,\"start\":39344},{\"end\":39794,\"start\":39786},{\"end\":39805,\"start\":39800},{\"end\":39825,\"start\":39816},{\"end\":40162,\"start\":40161},{\"end\":40177,\"start\":40170},{\"end\":40179,\"start\":40178},{\"end\":40196,\"start\":40188},{\"end\":40210,\"start\":40204},{\"end\":40459,\"start\":40458},{\"end\":40473,\"start\":40467},{\"end\":40491,\"start\":40482},{\"end\":40504,\"start\":40498},{\"end\":40521,\"start\":40515},{\"end\":40930,\"start\":40924},{\"end\":40945,\"start\":40943},{\"end\":40959,\"start\":40952},{\"end\":41391,\"start\":41387},{\"end\":41839,\"start\":41835},{\"end\":41858,\"start\":41848},{\"end\":41871,\"start\":41868},{\"end\":41883,\"start\":41880},{\"end\":41891,\"start\":41888},{\"end\":42210,\"start\":42205},{\"end\":42224,\"start\":42223},{\"end\":42237,\"start\":42233},{\"end\":42251,\"start\":42247},{\"end\":42610,\"start\":42604},{\"end\":42624,\"start\":42620},{\"end\":42638,\"start\":42634},{\"end\":42652,\"start\":42647},{\"end\":42669,\"start\":42664},{\"end\":42682,\"start\":42677},{\"end\":42684,\"start\":42683},{\"end\":42698,\"start\":42692},{\"end\":42712,\"start\":42707},{\"end\":43076,\"start\":43073},{\"end\":43088,\"start\":43083},{\"end\":43101,\"start\":43094},{\"end\":43115,\"start\":43109},{\"end\":43123,\"start\":43121},{\"end\":43136,\"start\":43128},{\"end\":43148,\"start\":43141},{\"end\":43495,\"start\":43492},{\"end\":43506,\"start\":43505},{\"end\":43526,\"start\":43515},{\"end\":43852,\"start\":43846},{\"end\":43863,\"start\":43859},{\"end\":43878,\"start\":43873},{\"end\":43889,\"start\":43885}]", "bib_author_last_name": "[{\"end\":31168,\"start\":31160},{\"end\":31184,\"start\":31178},{\"end\":31881,\"start\":31873},{\"end\":31898,\"start\":31893},{\"end\":31911,\"start\":31906},{\"end\":32469,\"start\":32461},{\"end\":32486,\"start\":32480},{\"end\":32500,\"start\":32495},{\"end\":32514,\"start\":32506},{\"end\":33265,\"start\":33261},{\"end\":33278,\"start\":33273},{\"end\":33290,\"start\":33287},{\"end\":33304,\"start\":33301},{\"end\":33317,\"start\":33312},{\"end\":33326,\"start\":33324},{\"end\":33340,\"start\":33335},{\"end\":33358,\"start\":33351},{\"end\":33592,\"start\":33588},{\"end\":33606,\"start\":33602},{\"end\":33613,\"start\":33611},{\"end\":33625,\"start\":33621},{\"end\":33644,\"start\":33640},{\"end\":34009,\"start\":34006},{\"end\":34025,\"start\":34019},{\"end\":34032,\"start\":34027},{\"end\":34047,\"start\":34041},{\"end\":34057,\"start\":34049},{\"end\":34358,\"start\":34353},{\"end\":34375,\"start\":34367},{\"end\":34393,\"start\":34388},{\"end\":34681,\"start\":34673},{\"end\":34694,\"start\":34687},{\"end\":35066,\"start\":35058},{\"end\":35079,\"start\":35072},{\"end\":35330,\"start\":35324},{\"end\":35347,\"start\":35338},{\"end\":35361,\"start\":35355},{\"end\":35374,\"start\":35369},{\"end\":35679,\"start\":35675},{\"end\":35688,\"start\":35683},{\"end\":35700,\"start\":35695},{\"end\":35711,\"start\":35702},{\"end\":36176,\"start\":36163},{\"end\":36194,\"start\":36186},{\"end\":36210,\"start\":36200},{\"end\":36228,\"start\":36222},{\"end\":36244,\"start\":36239},{\"end\":36250,\"start\":36246},{\"end\":36656,\"start\":36654},{\"end\":36671,\"start\":36666},{\"end\":36685,\"start\":36682},{\"end\":36695,\"start\":36692},{\"end\":37145,\"start\":37136},{\"end\":37159,\"start\":37153},{\"end\":37416,\"start\":37404},{\"end\":37431,\"start\":37426},{\"end\":37441,\"start\":37433},{\"end\":37930,\"start\":37924},{\"end\":37946,\"start\":37938},{\"end\":37960,\"start\":37956},{\"end\":38573,\"start\":38568},{\"end\":38595,\"start\":38579},{\"end\":38602,\"start\":38597},{\"end\":39282,\"start\":39275},{\"end\":39298,\"start\":39290},{\"end\":39310,\"start\":39305},{\"end\":39324,\"start\":39318},{\"end\":39342,\"start\":39335},{\"end\":39359,\"start\":39350},{\"end\":39798,\"start\":39795},{\"end\":39814,\"start\":39806},{\"end\":39832,\"start\":39826},{\"end\":39840,\"start\":39834},{\"end\":40168,\"start\":40163},{\"end\":40186,\"start\":40180},{\"end\":40202,\"start\":40197},{\"end\":40219,\"start\":40211},{\"end\":40225,\"start\":40221},{\"end\":40465,\"start\":40460},{\"end\":40480,\"start\":40474},{\"end\":40496,\"start\":40492},{\"end\":40513,\"start\":40505},{\"end\":40529,\"start\":40522},{\"end\":40538,\"start\":40531},{\"end\":40941,\"start\":40931},{\"end\":40950,\"start\":40946},{\"end\":40966,\"start\":40960},{\"end\":41398,\"start\":41392},{\"end\":41846,\"start\":41840},{\"end\":41866,\"start\":41859},{\"end\":41878,\"start\":41872},{\"end\":41886,\"start\":41884},{\"end\":41896,\"start\":41892},{\"end\":42221,\"start\":42211},{\"end\":42231,\"start\":42225},{\"end\":42245,\"start\":42238},{\"end\":42258,\"start\":42252},{\"end\":42266,\"start\":42260},{\"end\":42618,\"start\":42611},{\"end\":42632,\"start\":42625},{\"end\":42645,\"start\":42639},{\"end\":42662,\"start\":42653},{\"end\":42675,\"start\":42670},{\"end\":42690,\"start\":42685},{\"end\":42705,\"start\":42699},{\"end\":42723,\"start\":42713},{\"end\":43081,\"start\":43077},{\"end\":43092,\"start\":43089},{\"end\":43107,\"start\":43102},{\"end\":43119,\"start\":43116},{\"end\":43126,\"start\":43124},{\"end\":43139,\"start\":43137},{\"end\":43154,\"start\":43149},{\"end\":43503,\"start\":43496},{\"end\":43513,\"start\":43507},{\"end\":43534,\"start\":43527},{\"end\":43540,\"start\":43536},{\"end\":43752,\"start\":43743},{\"end\":43857,\"start\":43853},{\"end\":43871,\"start\":43864},{\"end\":43883,\"start\":43879},{\"end\":43894,\"start\":43890}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1589010},\"end\":31798,\"start\":31092},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":10329235},\"end\":32400,\"start\":31800},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":9164293},\"end\":33189,\"start\":32402},{\"attributes\":{\"doi\":\"abs/1906.02850\",\"id\":\"b3\"},\"end\":33580,\"start\":33191},{\"attributes\":{\"doi\":\"arXiv:2004.10404\",\"id\":\"b4\"},\"end\":33906,\"start\":33582},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3620071},\"end\":34301,\"start\":33908},{\"attributes\":{\"doi\":\"10.1162/COLI_a_00091\",\"id\":\"b6\",\"matched_paper_id\":13898507},\"end\":34589,\"start\":34303},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3177185},\"end\":34932,\"start\":34591},{\"attributes\":{\"id\":\"b8\"},\"end\":35250,\"start\":34934},{\"attributes\":{\"doi\":\"10.1145/2533682.2533683\",\"id\":\"b9\",\"matched_paper_id\":5571882},\"end\":35614,\"start\":35252},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":207892291},\"end\":36031,\"start\":35616},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9740537},\"end\":36598,\"start\":36033},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206594692},\"end\":37044,\"start\":36600},{\"attributes\":{\"doi\":\"abs/1606.08415\",\"id\":\"b13\"},\"end\":37326,\"start\":37046},{\"attributes\":{\"doi\":\"10.1145/3313831.3376467\",\"id\":\"b14\",\"matched_paper_id\":211018413},\"end\":37831,\"start\":37328},{\"attributes\":{\"doi\":\"10.18653/v1/D16-1128\",\"id\":\"b15\",\"matched_paper_id\":1238927},\"end\":38503,\"start\":37833},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":238873},\"end\":39201,\"start\":38505},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":14068874},\"end\":39693,\"start\":39203},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1354459},\"end\":40083,\"start\":39695},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6375093},\"end\":40456,\"start\":40085},{\"attributes\":{\"doi\":\"arXiv:2004.14373\",\"id\":\"b20\"},\"end\":40861,\"start\":40458},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":52153976},\"end\":41287,\"start\":40863},{\"attributes\":{\"id\":\"b22\"},\"end\":41343,\"start\":41289},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3116153},\"end\":41778,\"start\":41345},{\"attributes\":{\"id\":\"b24\"},\"end\":42101,\"start\":41780},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":52071158},\"end\":42575,\"start\":42103},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":13756489},\"end\":43006,\"start\":42577},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":201093978},\"end\":43447,\"start\":43008},{\"attributes\":{\"doi\":\"arXiv:1707.08052\",\"id\":\"b28\"},\"end\":43708,\"start\":43449},{\"attributes\":{\"id\":\"b29\"},\"end\":43811,\"start\":43710},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":1899153},\"end\":44255,\"start\":43813}]", "bib_title": "[{\"end\":31151,\"start\":31092},{\"end\":31864,\"start\":31800},{\"end\":32450,\"start\":32402},{\"end\":34000,\"start\":33908},{\"end\":34345,\"start\":34303},{\"end\":34663,\"start\":34591},{\"end\":35318,\"start\":35252},{\"end\":35670,\"start\":35616},{\"end\":36152,\"start\":36033},{\"end\":36644,\"start\":36600},{\"end\":37395,\"start\":37328},{\"end\":37917,\"start\":37833},{\"end\":38560,\"start\":38505},{\"end\":39259,\"start\":39203},{\"end\":39784,\"start\":39695},{\"end\":40159,\"start\":40085},{\"end\":40922,\"start\":40863},{\"end\":41385,\"start\":41345},{\"end\":41833,\"start\":41780},{\"end\":42203,\"start\":42103},{\"end\":42602,\"start\":42577},{\"end\":43071,\"start\":43008},{\"end\":43844,\"start\":43813}]", "bib_author": "[{\"end\":31170,\"start\":31153},{\"end\":31186,\"start\":31170},{\"end\":31883,\"start\":31866},{\"end\":31900,\"start\":31883},{\"end\":31913,\"start\":31900},{\"end\":32471,\"start\":32452},{\"end\":32488,\"start\":32471},{\"end\":32502,\"start\":32488},{\"end\":32516,\"start\":32502},{\"end\":33267,\"start\":33253},{\"end\":33280,\"start\":33267},{\"end\":33292,\"start\":33280},{\"end\":33306,\"start\":33292},{\"end\":33319,\"start\":33306},{\"end\":33328,\"start\":33319},{\"end\":33342,\"start\":33328},{\"end\":33360,\"start\":33342},{\"end\":33594,\"start\":33582},{\"end\":33608,\"start\":33594},{\"end\":33615,\"start\":33608},{\"end\":33627,\"start\":33615},{\"end\":33646,\"start\":33627},{\"end\":34011,\"start\":34002},{\"end\":34027,\"start\":34011},{\"end\":34034,\"start\":34027},{\"end\":34049,\"start\":34034},{\"end\":34059,\"start\":34049},{\"end\":34360,\"start\":34347},{\"end\":34377,\"start\":34360},{\"end\":34395,\"start\":34377},{\"end\":34683,\"start\":34665},{\"end\":34696,\"start\":34683},{\"end\":35068,\"start\":35050},{\"end\":35081,\"start\":35068},{\"end\":35332,\"start\":35320},{\"end\":35349,\"start\":35332},{\"end\":35363,\"start\":35349},{\"end\":35376,\"start\":35363},{\"end\":35681,\"start\":35672},{\"end\":35690,\"start\":35681},{\"end\":35702,\"start\":35690},{\"end\":35713,\"start\":35702},{\"end\":36178,\"start\":36154},{\"end\":36196,\"start\":36178},{\"end\":36212,\"start\":36196},{\"end\":36230,\"start\":36212},{\"end\":36246,\"start\":36230},{\"end\":36252,\"start\":36246},{\"end\":36658,\"start\":36646},{\"end\":36673,\"start\":36658},{\"end\":36687,\"start\":36673},{\"end\":36697,\"start\":36687},{\"end\":37147,\"start\":37132},{\"end\":37161,\"start\":37147},{\"end\":37418,\"start\":37397},{\"end\":37433,\"start\":37418},{\"end\":37443,\"start\":37433},{\"end\":37932,\"start\":37919},{\"end\":37948,\"start\":37932},{\"end\":37962,\"start\":37948},{\"end\":38575,\"start\":38562},{\"end\":38597,\"start\":38575},{\"end\":38604,\"start\":38597},{\"end\":39284,\"start\":39261},{\"end\":39300,\"start\":39284},{\"end\":39312,\"start\":39300},{\"end\":39326,\"start\":39312},{\"end\":39344,\"start\":39326},{\"end\":39361,\"start\":39344},{\"end\":39800,\"start\":39786},{\"end\":39816,\"start\":39800},{\"end\":39834,\"start\":39816},{\"end\":39842,\"start\":39834},{\"end\":40170,\"start\":40161},{\"end\":40188,\"start\":40170},{\"end\":40204,\"start\":40188},{\"end\":40221,\"start\":40204},{\"end\":40227,\"start\":40221},{\"end\":40467,\"start\":40458},{\"end\":40482,\"start\":40467},{\"end\":40498,\"start\":40482},{\"end\":40515,\"start\":40498},{\"end\":40531,\"start\":40515},{\"end\":40540,\"start\":40531},{\"end\":40943,\"start\":40924},{\"end\":40952,\"start\":40943},{\"end\":40968,\"start\":40952},{\"end\":41400,\"start\":41387},{\"end\":41848,\"start\":41835},{\"end\":41868,\"start\":41848},{\"end\":41880,\"start\":41868},{\"end\":41888,\"start\":41880},{\"end\":41898,\"start\":41888},{\"end\":42223,\"start\":42205},{\"end\":42233,\"start\":42223},{\"end\":42247,\"start\":42233},{\"end\":42260,\"start\":42247},{\"end\":42268,\"start\":42260},{\"end\":42620,\"start\":42604},{\"end\":42634,\"start\":42620},{\"end\":42647,\"start\":42634},{\"end\":42664,\"start\":42647},{\"end\":42677,\"start\":42664},{\"end\":42692,\"start\":42677},{\"end\":42707,\"start\":42692},{\"end\":42725,\"start\":42707},{\"end\":43083,\"start\":43073},{\"end\":43094,\"start\":43083},{\"end\":43109,\"start\":43094},{\"end\":43121,\"start\":43109},{\"end\":43128,\"start\":43121},{\"end\":43141,\"start\":43128},{\"end\":43156,\"start\":43141},{\"end\":43505,\"start\":43492},{\"end\":43515,\"start\":43505},{\"end\":43536,\"start\":43515},{\"end\":43542,\"start\":43536},{\"end\":43754,\"start\":43743},{\"end\":43859,\"start\":43846},{\"end\":43873,\"start\":43859},{\"end\":43885,\"start\":43873},{\"end\":43896,\"start\":43885}]", "bib_venue": "[{\"end\":31433,\"start\":31306},{\"end\":32136,\"start\":32033},{\"end\":32871,\"start\":32702},{\"end\":35836,\"start\":35783},{\"end\":36838,\"start\":36776},{\"end\":37605,\"start\":37544},{\"end\":38154,\"start\":38070},{\"end\":38911,\"start\":38766},{\"end\":39877,\"start\":39868},{\"end\":41077,\"start\":41031},{\"end\":41539,\"start\":41478},{\"end\":44055,\"start\":43984},{\"end\":31304,\"start\":31186},{\"end\":32031,\"start\":31913},{\"end\":32700,\"start\":32516},{\"end\":33251,\"start\":33191},{\"end\":33721,\"start\":33662},{\"end\":34084,\"start\":34059},{\"end\":34440,\"start\":34415},{\"end\":34753,\"start\":34696},{\"end\":35048,\"start\":34934},{\"end\":35431,\"start\":35399},{\"end\":35781,\"start\":35713},{\"end\":36299,\"start\":36252},{\"end\":36774,\"start\":36697},{\"end\":37130,\"start\":37046},{\"end\":37542,\"start\":37466},{\"end\":38068,\"start\":37982},{\"end\":38764,\"start\":38604},{\"end\":39430,\"start\":39361},{\"end\":39866,\"start\":39842},{\"end\":40252,\"start\":40227},{\"end\":40643,\"start\":40556},{\"end\":41029,\"start\":40968},{\"end\":41306,\"start\":41289},{\"end\":41476,\"start\":41400},{\"end\":41921,\"start\":41898},{\"end\":42324,\"start\":42268},{\"end\":42774,\"start\":42725},{\"end\":43212,\"start\":43156},{\"end\":43490,\"start\":43449},{\"end\":43741,\"start\":43710},{\"end\":43982,\"start\":43896}]"}}}, "year": 2023, "month": 12, "day": 17}