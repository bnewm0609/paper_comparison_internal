{"id": 230799166, "updated": "2023-10-06 06:36:46.267", "metadata": {"title": "Self-Supervised Pretraining of 3D Features on any Point-Cloud", "authors": "[{\"first\":\"Zaiwei\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Rohit\",\"last\":\"Girdhar\",\"middle\":[]},{\"first\":\"Armand\",\"last\":\"Joulin\",\"middle\":[]},{\"first\":\"Ishan\",\"last\":\"Misra\",\"middle\":[]}]", "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2021, "month": 1, "day": 7}, "abstract": "Pretraining on large labeled datasets is a prerequisite to achieve good performance in many computer vision tasks like 2D object recognition, video classification etc. However, pretraining is not widely used for 3D recognition tasks where state-of-the-art methods train models from scratch. A primary reason is the lack of large annotated datasets because 3D data is both difficult to acquire and time consuming to label. We present a simple self-supervised pertaining method that can work with any 3D data - single or multiview, indoor or outdoor, acquired by varied sensors, without 3D registration. We pretrain standard point cloud and voxel based model architectures, and show that joint pretraining further improves performance. We evaluate our models on 9 benchmarks for object detection, semantic segmentation, and object classification, where they achieve state-of-the-art results and can outperform supervised pretraining. We set a new state-of-the-art for object detection on ScanNet (69.0% mAP) and SUNRGBD (63.5% mAP). Our pretrained models are label efficient and improve performance for classes with few examples.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2101.02691", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/ZhangGJM21", "doi": "10.1109/iccv48922.2021.01009"}}, "content": {"source": {"pdf_hash": "79d155ee690042163843ae9e6fbd15600e54ae7a", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2101.02691", "status": "GREEN"}}, "grobid": {"id": "33a763a73f62123ef357ea537901659c565777f9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/79d155ee690042163843ae9e6fbd15600e54ae7a.txt", "contents": "\nSelf-Supervised Pretraining of 3D Features on any Point-Cloud\n\n\nZaiwei Zhang \nFacebook AI Research\n\n\nThe University of Texas at Austin\n\n\nRohit Girdhar \nFacebook AI Research\n\n\nArmand Joulin \nFacebook AI Research\n\n\nIshan Misra \nFacebook AI Research\n\n\nSelf-Supervised Pretraining of 3D Features on any Point-Cloud\n10.1109/ICCV48922.2021.01009\nPretraining on large labeled datasets is a prerequisite to achieve good performance in many computer vision tasks like image recognition, video understanding etc. However, pretraining is not widely used for 3D recognition tasks where state-of-the-art methods train models from scratch. A primary reason is the lack of large annotated datasets because 3D data labelling is time-consuming. Recent work shows that self-supervised learning is useful to pretrain models in 3D but requires multi-view data and point correspondences. We present a simple self-supervised pretraining method that can work with single-view depth scans acquired by varied sensors, without 3D registration and point correspondences. We pretrain standard point cloud and voxel based model architectures, and show that joint pretraining further improves performance. We evaluate our models on 9 benchmarks for object detection, semantic segmentation, and object classification, where they achieve state-of-the-art results. Most notably, we set a new state-ofthe-art for object detection on ScanNet (69.0% mAP) and SUNRGBD (63.5% mAP). Our pretrained models are label efficient and improve performance for classes with few examples.\n\nIntroduction\n\nPretraining visual features on large labeled datasets is a pre-requisite to achieve good performance when access to annotations is limited [27,46,52,87]. More recently, selfsupervised pretraining has become a popular alternative to supervised pretraining especially for tasks where annotations are time-consuming, such as detection and segmentation in images [9,36,37,56,93] or tracking in videos [41]. In 3D vision too, annotations are difficult to acquire. Labeling a 3D scene composed of thousands of 3D points is timeconsuming and can take around 22 minutes per scene [18].\n\nThis cumbersome annotation process results in a lack of large annotated 3D datasets. However, acquiring 3D data in the form of single-view depth maps has become easier than ever due to the advent of consumer grade depth sen-* Work done during an internship at Facebook. Same perf. 2x fewer labels Same perf. 2x fewer labels Figure 1: Label-efficiency of our self-supervised pretraining. We finetune detection models from scratch or using our pretraining as initialization. Our pretraining which uses unlabeled single-view 3D data, outperforms training from scratch, and achieves the same detection performance with about half the detection labels.\n\nsors, e.g., in phones [24,73,83]. While these depth maps can be leveraged to pretrain self-supervised 3D features, there is surprisingly little work that can be applied. Recent work [105] applies self-supervised pretraining to 3D models but uses multi-view depth scans with point correspondences. Since 3D sensors only acquire single-view depth scans, multi-view depth scans and point correspondences are typically obtained via 3D reconstruction. Unfortunately, even with good sensors, 3D reconstruction can fail easily for a variety of reasons such as non-static environments, fast camera motion or odometry drift [16].\n\nIn this paper, we introduce a simple contrastive framework, DepthContrast, to representations from single-view depth scans. From a practical perspective, self-supervised learning from single-view depth scans is more broadly applicable for 3D data. It is also an interesting scientific question whether just using single-view information can provide benefits for self-supervised learning in 3D. Our approach is based on the Instance Discrimination method by Wu et al. [103] applied to depth maps. We side-step the need of registered point clouds or correspondences, by considering each depth map as an instance and discriminating between them, even if they come from the same scene. Since different 3D applications require different 3D scene representa-1 tions such as voxels for segmentation [17], point clouds for detection [64], we use our method for both voxels and point clouds. We jointly learn features by considering voxels and point clouds of the same 3D scene as data augmentations that are processed with their associated networks [93].\n\nOur contributions can be summarized as follows:\n\n\u2022 We show that single-view 3D depth scans can be used for self-supervised learning.\n\n\u2022 Our single-view representations perform comparably, or in some settings better than their multi-view counterparts, showing that single-view depth scans are indeed powerful for learning features.\n\n\u2022 Our method is applicable across different model architectures, indoor/outdoor 3D data, single/multi-view 3D data. We also show that it can be used to pretrain high capacity 3D architectures which otherwise overfit on tasks like detection and segmentation.\n\n\u2022 We show that joint training of different input representations like points and voxels is important for learning good representations, and a naive application of contrastive learning may not yield good results.\n\n\u2022 We show performance improvements over nine downstream tasks, and set a new state-of-the-art for two object detection tasks (ScanNet and SUNRGBD). Our models are efficient few-shot learners.\n\n\nRelated Work\n\nOur method builds on the work from the self-supervised learning literature, with 3D data as an application. In this section, we give an overview of the recent advances in both self-supervision and 3D representations. Self-supervised learning for images. Self-supervised learning is a well studied problem in machine learning and computer vision [53,60,69,72,95]. There are many classes of methods for learning representations -clustering [7,8,43], GANs [20,55], pretext tasks [19,59,97] etc. Recent advances [9,13,30,36,37,47,56,94] have shown that self-supervised pretraining is a viable alternative to supervised pretraining for 2D recognition tasks. Our work builds upon contrastive learning [34,61] where models are trained to discriminate between each instance [21] with no explicit classifier [103]. These instance discrimination methods can be extended to multiple modalities [57,62,93]. Our method extends the work of Wu et al. [103] to multiple 3D input formats following Tian et al. [93] using a momentum encoder [36] instead of a memory bank. Self-supervised learning for 3D data. Most methods on self-supervised learning focus on single 3D object representation with different applications to reconstruction, classification or part segmentation [1,2,25,33,35,44,49,74,99,110]. Recently, Xie et al. [105] proposed a self-supervised method to build representations of scene level point clouds. Their method relies on the complete 3D reconstruction of a scene with point-wise correspondences between the different views of a point cloud. These point-wise correspondences requires post-processing the data by registering the different depth maps into a single 3D scene. Their method can only be applied to static scenes that have been registered, which greatly limits the applications of their work. We show a simple self-supervised method that learns stateof-the-art representations from single-view 3D data, and can also be applied to multi-view data. Representations of 3D scenes. There are multiple ways to represent 3D information in different vectorized forms such as point-clouds, voxels or meshes. Point-cloud based models [66,68] are widely used in classification and segmentation tasks [6,40,45,66,68,91,98,100,106,107], 3D reconstruction [23,90,110] and 3D object detection [63,64,77,98,112,112,114]. Since many 3D sensors acquire data in terms of 3D points, point clouds are a convenient input for deep networks. However, since using convolution operations on point-clouds directly is difficult [31,66], voxelized data is another popular input representation. 3D convolutional models [3,17,28,31,38,50,70,85,92] are widely used in 3D scene understanding [32,82,109,116]. There are also efforts to combine different 3D input representations [29,76,96,[113][114][115]. In this work, we propose to jointly pretrain two architectures for points and voxels, that are PointNet++ [68] for points and Sparse Convolution based U-Net [17] for voxels. 3D transfer tasks and datasets. We use shape classification, scene segmentation, and object detection as the recognition tasks for transfer learning. Shape classification techniques [11,54,[66][67][68]86] are widely evaluated on the Mod-elNet [102] dataset, which we use. It contains synthetic 3D data and each sample contains exactly one object. We also evaluate on complete 3D scenes using the more general 3D scene understanding task. Scene-centric datasets can be broadly divided into indoor scens [5,10,18,39,58,63,75,81,84,104], and outdoor (self-driving focussed) scenes [26,71,88]. We use these datasets and evaluate the performance of our methods on the indoor detection [12,22,64,65,114], scene segmentation [17,68,92,101,108], and outdoor detection tasks [15, 48, 76-78, 109, 111].\n\n\nDepthContrast\n\nWe present our approach to unsupervised 3D representation learning. DepthContrast can learn from either unprocessed single-view or multi-view depth maps. Our method, illustrated in Fig 2, is based on the instance discrimination framework from Wu et al. [103] with a momentum encoder [36]. We also show an extension of DepthContrast built upon [93] that learns representations across 3D input formats like points and voxels, and across 3D architectures.  Figure 2: Approach Overview. We propose DepthContrast -a simple 3D representation learning method that uses large amounts of unprocessed single/multi-view depth maps. Given a depth map we construct two augmented versions using data augmentation and represent them with selected input formats (point coordinates or voxels). We use format-specific encoders to get spatial features which are pooled and projected to obtain global features v. The global features are used to setup an instance discrimination task and pretrain the encoders.\n\n\nInstance Discrimination\n\nGiven a dataset D = {X} N i=1 containing N samples X, we wish to learn a function g(X) that produces useful representations v = g(X) of the input sample. As shown in Fig 2, our method uses 3D data where X can be represented by point coordinates or voxels 1 . We apply a data augmentation t sampled randomly from a large set of augmentations T , to obtain an augmented sample X = t(X). The augmented sample is input to a deep network g that extracts unit-norm global features v = g( X) by pooling over the 3D spatial coordinates. We setup an instance discrimination problem where the features v i,1 and v i,2 obtained from two data augmented versions of sample i must be similar to each other, and different from features v j obtained using K other (negative) samples j in the dataset. We use a contrastive loss [34,61,80] to achieve this goal:\nli = \u2212 log exp(v \u22a4 i,1 vi,2/\u03c4 ) exp(v \u22a4 i,1 vi,2/\u03c4 ) + K j\u0338 =i exp(v \u22a4 i,1 vj/\u03c4 ) ,(1)\nwhere \u03c4 is the temperature that controls the smoothness of the softmax distribution. This loss encourages features from different augmentations of the same scene to be similar, while being dissimilar to features of other scenes. Thus, it learns features that focus on discriminative regions of a scene that make it different from other scenes in the dataset. Minimal assumptions on input data. Our method makes minimal assumptions about the input X, i.e., it is an unprocessed single-view depth map. It does not require careful sampling of overlapping multi-view 3D inputs [105] or object centric depth maps [35,44]. These minimal assumptions enable us to learn from large scale single-view 3D depth maps in \u00a7 4 and outdoor 3D depth maps obtained from different sensors without relying on 3D calibration in \u00a7 5.3. Momentum encoder. As using a large number of negatives is important for contrastive learning [13,36,56,103], we use the method of He et al. [36] where the features of the other augmentation v i,2 and negative samples v j in Eq 1 are obtained using a momentum encoder and a queue respec- 1 Points in a depth map are a set, but for simplicity we denote them as a matrix. Our method does not rely on any specific ordering of the points.\n\ntively. This allows us to use a large number K of negative samples without increasing the training batch size.\n\n\nExtension to Multiple 3D Input Formats\n\nMultiple input formats are commonly used to represent 3D data -point clouds, voxels, meshes etc. Different input formats can be easily transfered from one to another and have their specific deep learning architectures and applications. Our self-supervised method can be naturally extended to accommodate these input formats and architectures. For each input format f , we denote the corresponding input sample as X f , the format-specific encoder network as g f , and the extracted feature as v f . Extending Eq 1, we can minimize a single objective that performs instance discrimination within and across input formats a, b:\nl ab i = \u2212 log exp(v a\u22a4 i,1 v b i,2 /\u03c4 ) exp(v a\u22a4 i,1 v b i,2 /\u03c4 ) + K j\u0338 =i exp(v a\u22a4 i,1 v b j /\u03c4 ) .(2)\nWhen the input formats a, b are identical, this objective reduces to the within format loss of Eq 1, and when a \u0338 = b this objective aligns the feature representations v f = g f (X f ) obtained across formats f using different network architectures g f . As illustrated in Fig 3, we use two popular input formats -point clouds and voxels, and train these format-specific models with a single joint loss function\nLi = l ab i + l ba i across format + l aa i + l bb i within format .(3)\nSimilar techniques have been explored in the context of different modalities of data, e.g., color and grayscale im- ages [93], audio and video [57,62] etc. While these methods use different modalities, our extension uses the same 3D data and only changes the input format.\n\n\nModel Architecture\n\nWe describe the model architecture used for our input format-specific encoders. Both encoders operate on the same augmented input 3D data, and differ only in the way the input is represented. We provide the full, layer-wise architecture details in the supplemental material. Point input. We use PointNet++ [64] as the backbone network which takes XYZ coordinates as input. Our network takes in 20K points, and produces C dimensional per-point features for 1024 points after aggregation. We obtain the scene level 256 dimensional feature v in Eq 2 by global max pooling to these last layer features, followed by a two layer MLP as in [13] and L2 normalization. Voxel input. We use a sparse convolution U-Net model [17] as the backbone for the voxel 3D input. The network takes a 3D occupancy grid and corresponding RGB values as the input representation of the 3D data. We use a voxel size of 5cm to voxelize the input data following [17].To obtain the scene level 256 dimensional feature v ( Eq 2), similar to the point input, we apply global max-pooling to the last layer feature, followed by a two layer MLP and L2 normalization.\n\n\nData Augmentation for 3D\n\nData augmentation is as an essential component of our framework. We first adopt standard pointcloud data argumentation methods proposed in [64], which are random point up/down sampling, random flip in xy axis, and random rotation. However, after adding these methods, it is still easy for the network to distinguish different training instances. Thus, we add two new data augmentation methods: random cuboid and random drop patches. Inspired by the random crop in 2D images [89], we define a random cuboid augmentation that extracts random cuboids from the input point cloud. Cuboids are sampled using a random scale [0.5, 1.0] of the original scene, and a random aspect ratio [0.75, 1.0]. We also drop (erase) cuboids to force the network learn local geometric features. The dropped cuboid is randomly cropped with 0.2 of the scene scale. The performance boost from each augmentation is analyzed in \u00a7 5. For voxelized inputs, in addition to all the point augmentations, we use the augmentations from [17].\n\n\nImplementation Details\n\nWe use 130K negatives for contrastive learning in Eq 3 and a momentum of 0.9 for the momentum encoder following [36]. As noted in \u00a7 3.3, we follow Chen et al. [13] and use an additional non-linear projection and L2 normalization to obtain the features v. The features v are 128 dimensional and we use a temperature value of 0.1 while computing the non-parametric softmax in Eq 1. We use a standard\n\n\nDataset\n\nStats Task Gain of DepthContrast Self-supervised Pretraining ScanNet-vid [18] 190K single-view depth maps (Indoor) Redwood-vid [16] 370K single-view depth maps (Indoor/Outdoor) Transfer tasks ScanNet [18] 1  \n\n\nExperiments\n\nWe evaluate DepthContrast pretraining by transfer learning, i.e., fine-tuning the learned representation on downstream tasks and datasets. As Table 1 shows, we use a diverse set of 3D understanding tasks like object classification, semantic segmentation, and object detection. We first study a single input 3D format and a single network architecture in \u00a7 4.1. We show DepthContrast's performance on multiple downstream tasks, even when compared to multi-view methods [105]; further improvements by scaling amount of pretraining data and model capacity; as well as its benefits in few-shot downstream tasks with limited labeled data. Finally, in \u00a7 4.2, we show the benefits of our pre-training across different 3D input formats.\n\nPretraining Details. We use single-view depth map videos from the popular ScanNet [18] dataset and term it as ScanNet-vid. ScanNet-vid contains about 2.5 million RGB-D scans for more than 1500 indoor scenes. Following the train/val split from [64], we extract around 190K RGB-D scans (2FPS) from about 1200 video sequences in the train set. We do not use camera calibration or 3D registration methods and operate directly on single-view depth maps. We use our data augmentation described in \u00a7 3.4 and use the training objectives from \u00a7 3. Additional details are provided in \u00a7 3.5 and the supplemental material.\n\nDownstream Tasks. We evaluate our pretrained model by transfer learning and finetune it on different downstream datasets and tasks summarized in Table 1. We use diverse downstream datasets -full scenes/object cen- Overfitting is more pronounced on small datasets like S3DIS. Our DepthContrast pretraining on ScanNet-vid improves the performance for larger models and reduces overfitting. We increase the pretraining data by combining the readily available single-view depth maps from ScanNet-vid and Redwood-vid. DepthContrast's performance improves significantly when using both large data and large models.  Table 2: Detection AP25 using VoteNet [64]. We evaluate different pretrained models -random initialization, supervised VoteNet on ScanNet, our self-supervised DepthContrast using the point input format, and self-supervised with PointContrast. We provide in green the improvements over a detector trained from scratch. Note that PointContrast uses a UNet backbone. DepthContrast outperforms the scratch model on all benchmarks and is better than the detection-specific supervised pretraining on two datasets.\n\ntric; single/multi-view; real/synthetic; indoor/outdoor. On these diverse datasets, we use three major tasks, which are classification, semantic segmentation, and object detection. These tasks test different aspects of the pretrained modelwhile object detection and semantic segmentation use local features, classification is performed on global features.\n\n\nPretraining with Point Input Format\n\nSetup. We pretrain a PointNet++ model using the instance discrimination objective in Eq 1 on the single-view depth maps from ScanNet-vid. We study the transfer performance of the pretrained model on object detection using the VoteNet [64] framework that uses a PointNet++ backbone.\n\nBaselines. Scratch -Training from scratch or random initialization is standard practice in VoteNet [64] and serves as a baseline for comparing other pretraining methods. Supervised -We introduce a supervised pretraining baseline by pretraining a PointNet++ backbone on the ScanNet detection task. As the supervised baseline is pretrained specifically on object detection, it serves as a strong baseline.\n\nPointContrast -We compare with a PointContrast [105] pretrained model that uses strictly more information (multi-view) than our model (single-view) and serves as an important upper bound. We note that the architecture of this model is different, and as reported in their work [105], PointContrast performs poorly with single-view data.\n\nIn Table 2 we report the detection results by finetuning the VoteNet model with different backbone initializations. We use the implementation of [64] for finetuning and report the detection performance using the mean Average Precision at IoU=0.25 (AP 25 ) metric. Scratch training provides competitive results on the larger detection datasets like ScanNet and SUNRGBD [42,79,81,104], however, its performance on the smaller S3DIS dataset is low. In comparison, supervised pretraining provides large gains in the detection performance across all datasets. DepthContrast outperforms training from scratch on all the four datasets, and improves performance by 12.1% mAP on the small S3DIS dataset that has only 200 labeled training samples. We further analyze label efficiency of our model in \u00a7 4.1.4. Interestingly, despite using no labels during pretraining, DepthContrast is better than the detection-specific supervised pretraining for two datasets (SUNRGBD and Matter-port3D). Compared to PointContrast, our model achieves a similar gain over the scratch baseline. This shows that our single-view DepthContrast can learn representations that are at par with a multi-view method for object detection.\n\n\nTraining Higher Capacity Models\n\nWe now apply DepthContrast to higher capacity models. Following standard practice in 2D self-supervised learning [47], we increase the capacity of PointNet++ model by multiplying the channel width of all the layers by {2, 3, 4}. We pretrain all models on the ScanNet-vid dataset and measure their transfer performance in Fig 4. Training large models from scratch provides some benefit, but quickly leads to reduced or plateauing performance. We observe  Table 3: Transfer using state-of-the-art detection frameworks.\n\nWe use our pretrained model (PointNet++ 3\u00d7 on Redwood-vid +ScanNet-vid) and transfer it using two state-of-the-art detection frameworks -H3DNet [114] and VoteNet [64]. Our DepthContrast pretraining outperforms all prior work and sets a new state-of-theart on both ScanNet and SUNRGBD detection datasets.\n\noverfitting on the small datasets like S3DIS where increasing the model capacity does not improve performance. However, our self-supervised pretraining on ScanNet-vid reduces this overfitting and performance improves or stays the same for larger models. This suggests that pretraining is crucial for training large 3D detection models and Depth-Contrast can provide an easy way to train such models.\n\n\nUsing More Single-view Pretraining Data\n\nWe increase the pretraining data by using readily available single-view 3D data from the Redwood-vid dataset [16]. Redwood-vid contains over 23 million depth scans from RGB-D videos taken in both indoor and outdoor settings. As this dataset is extremely large, we use a subset of 2500 video sequences consisting of 10 categories and extract 370K RGB-D scans. Since the Redwood-vid dataset does not contain camera extrinsic parameters, multi-view methods like PointContrast [105] cannot be used to such dataset. Combining the Redwood-vid and ScanNet-vid datasets allows us to triple our pretraining data. We pretrain all models on this combined dataset and report their performance (AP 25 ) in Fig 4. DepthContrast's performance improves with both model capacity and number of pretraining samples across all four detection datasets. The higher capacity models show a larger improvement in performance particularly on the smaller S3DIS dataset. These results highlight that DepthContrast can leverage large amounts of readily available single-view data to train high capacity 3D models. Compared to multi-view methods [105], this makes Depth-Contrast more broadly applicable.\n\n\nState-of-the-art Detection Frameworks\n\nWe use two state-of-the-art detection frameworks -H3DNet [114] and VoteNet [64] and study the benefit of using our pretrained model. We use our PointNet++ 3\u00d7 model pretrained on the combined Redwood-vid and ScanNet-vid dataset and transfer it using these detection frameworks. The detection results in Table 3 show that our pretrained model achieves state-of-the-art performance on SUNRGBD and ScanNet. In particular, as the gains are larger on stricter mAP at IoU=0.5, our pretrained models result in detection models that are better at localization.\n\n\nLabel Efficiency of Pretrained Models\n\nPretraining allows models to be finetuned with small amount of labeled data. In Table 2, we observe that small labeled datasets benefit more from pretraining. We study the label efficiency of DepthContrast pretrained models by varying the amount of labeled data used for finetuning. While varying the data, we draw 3 independent samples and report average results. We use the PointNet++ models pretrained on ScanNet-vid ( \u00a7 4.1) and report the detection performance in Fig 1. DepthContrast pretraining provides large gains in performance at every setting. On both the ScanNet and SUNRGBD datasets, our model with just 50% samples gets the same performance as training from scratch with the full dataset. When using 20% samples for finetuning, our pretrained models provide a gain of over 10% mAP. This shows that our pretraining is label efficient and can improve performance especially on tasks with limited supervision. Does pretraining benefit tail classes? 3D detection datasets like SUNRGBD and ScanNet exhibit a long tailed distribution where many 'tail' classes have few training instances. In SUNRGBD, the 'tail' classes like bathtub, toilet, dresser have less than 200 training instances, while classes like chair have over 9000 instances. Fig 5  shows the gain of our pretrained model over the scratch model across object classes on SUNRGBD. Our pretraining improves the performance of classes with fewer instances, i.e., the tail classes, by 4\u22128% AP. This suggests that Depth-Contrast pretraining is especially effective in few-shot set-  Table 4: Multiple input formats. We study the importance of training 3D representations jointly using multiple input formatspoints and voxels. We vary the within format and across format loss terms in Eq 3. We report detection mAP@0.25 on the point transfer tasks and segmentation mIOU for the voxel transfer tasks. We observe that performing instance discrimination across the input formats (third row) greatly improves over the within format loss term. Note that PointContrast is trained with ScanNet multiview scans.\n\ntings and can partially address the long tailed label distributions of current 3D scene understanding benchmarks.\n\n\nPretraining with Multiple Input Formats\n\nWe pretrain DepthContrast using both the point and voxel input formats and use two format-specific encoders -PointNet++ for points and UNet for voxels. Baselines. As explained in Eq 3, when using multiple 3D input formats, we can define two loss terms -a within format loss and an across format loss. To analyze which loss terms matter for pretraining, we consider three variants -(1) Within format which independently trains format-specific models for each input format and is a straightforward application of instance discrimination to 3D; (2) Across format which trains the format-specific models jointly using the second term of Eq 3; (3) Ours which trains the format-specific models jointly using our combined loss function. PointContrast -Similar to \u00a7 4.1, we use a pretrained PointContrast [105] UNet model trained with multiview data. This model is trained with multi-view point correspondences to enable it to learn better point features.\n\nSince PointContrast uses strictly more information than our single-view method, it serves as an important upper bound. Setup. We evaluate the pretrained models by transfer learning. As in \u00a7 4.1, we finetune the pretrained point input format PointNet++ models on SUNRGBD and ScanNet detection using VoteNet. We finetune the voxel UNet models on segmentation using the framework from Spatio-Temporal Segmentation [17] which uses a UNet backbone network. The results are summarized in Table 4.\n\nCompared to training from scratch, the within format pretraining only provides a benefit for the point input format PointNet++ models. For the voxel models, this pretraining does not improve consistently over training from scratch, which is in line with observations from recent work [105]. This shows that a naive application of instance discrimination to 3D representation learning may not yield good pretrained models. The across format loss improves performance for both the point and voxel models, suggesting the  benefit of using multiple input formats. Our proposed joint loss provides the best transfer performance. The gains are particularly significant on the voxel format model which improves by 4% over the within format loss. In the supplemental material, we show that this benefit of joint training over the within format loss also holds across different pretraining data and architectures. Compared to the multi-view PointContrast upper bound [105], our results on the voxel transfer task are slightly worse. PointContrast uses multi-view point correspondences to enforce point-level supervision during pretraining. This enables their model to learn point features that are more suitable for point prediction tasks like segmentation. However, despite not relying on multi-view information, DepthContrast pretraining still provides competitive performance. We believe these results are encouraging given the broad applicability of DepthContrast to vast amounts of single-view data captured by modern sensors. We note that our UNet architecture is different from [105] since their architecture underfit on our pretraining task.\n\n\nAnalysis\n\nIn this section we present a series of experiments designed to understand DepthContrast better. We first pretrain point format (PointNet++) models on the ScanNet-vid dataset following the settings from \u00a7 4.1. We use two transfer tasks for evaluation -(1) object detection on SUNRGBD using VoteNet [64] where we finetune the full model and test the quality of the pretraining; (2) object classification on Shape Classification dataset [102] where we keep the model fixed and only train linear classifiers on fixed features, thus testing the quality of the learned representations [30,47]. Finally, we also evaluate DepthContrast's generalizability to outdoor 3D data.\n\n\nImportance of Data Augmentation\n\nData augmentations play an important role for selfsupervised representation learning and have been studied extensively in the case of 2D images [9,14,56,93,94]. However, the impact of data augmentation for 3D representation learning is less well understood. Thus, we analyze the effect of our proposed augmentations from \u00a7 3.4 on transfer  Table 6: Single-view or multi-view 3D data. We study whether our pretraining is sensitive to single-view or multi-view data. We use ScanNet and ScanNet-vid which are multi-view and singleview versions of the same dataset [18], and Redwood-vid [16] which is a single-view only 3D dataset. Our pretrained model is robust to 3D preprocessing, and using single-view or multi-view data gives similar performance.\n\nperformance. We train different DepthContrast point models with the same training setup and only vary the data augmentation used. Our results are summarized in Table 5.\n\nThe widely used VoteNet [64] augmentations perform worse than our proposed augmentations. Our augmentations lead to both a better feature representation: a gain of 5% accuracy on Shape Classification [102], and a better pretrained model: 2% mAP on SUNRGBD detection. We also consistently observe gains from our improved data augmentation on all the downstream tasks from \u00a7 4 which underscores the importance of designing good data augmentation.\n\n\nImpact of Single-view or Multi-view 3D Data\n\nWe now study whether pretraining on reconstructed multi-view 3D scenes impacts the downstream performance. We use the ScanNet [18] dataset which contains multi-view 3D data obtained by 3D registration of the ScanNet-vid depth maps. As another single-view dataset, we pretrain on the Redwood-vid dataset from \u00a7 4.1.2. We pretrain DepthContrast point models on these datasets and compare their performance by transfer learning in Table 6.\n\nThe transfer performance is similar when models are pretrained on ScanNet-vid or ScanNet. Since ScanNet-vid and ScanNet only differ in the 3D preprocessing involved, the result suggests that DepthContrast is not sensitive to singleview or multi-view input data. This is not surprising given that our objective does not rely on multi-view information. Pretraining on the single-view Redwood-vid dataset also gives similar performance suggesting that DepthContrast is robust to different data distributions during pretraining. All the DepthContrast models outperform the scratch model.\n\n\nGeneralization to Outdoor LiDAR data\n\nWe test DepthContrast's generalization to outdoor Li-DAR data by pretraining on the Waymo Open Dataset [88] where we extract 79K single-view scans from the videos. We use the same data augmentation parameters from \u00a7 3.4 and only modify random cuboid to work on the full scale of the Z dimension of the scene. We use the standard LiDARspecific model architectures as our format-specific encoders -PointnetMSG [77] for point clouds and Spconv-UNet [78] for voxels. Similar to \u00a7 3.5, we obtain features from these Same perf. 2x fewer labels Same perf. 5x fewer labels Figure 6: Using outdoor LiDAR data. We finetune detection models from scratch or using our pretraining and report mAP (with 40 recall positions) on the cyclist class at moderate difficulty level of the KITTI val split. Our models are pretrained using unlabeled outdoor data from the Waymo dataset and outperform scratch training using either point (left) or voxel (right) inputs. models after global max pooling and a two layer MLP. The models are optimized jointly with Eq 3 using both within and across-format losses. For transfer learning, we use the standard KITTI [26] object detection benchmark, and PointRCNN [77] and Part-A 2 [78] for down-stream models. We report results on the cyclist class since it has fewer examples in the training set compared to the other classes. We provide results for other classes and finetuning details in the supplemental material. Similar to \u00a7 4.1.4, while varying the fraction of pretraining data, we report average performance across 3 independent runs. Fig 6 shows that our pretrained models outperform training from scratch especially when finetuning on fewer training samples. For Spconv-Unet, we achieve a 20% gain with 5% of labeled data. This suggests that DepthContrast pretraining generalizes across multiple input formats, and our proposed data augmentation generalizes to different depth sensors and scene types.\n\n\nConclusion\n\nWe propose DepthContrast-an easy to implement selfsupervised method that works across model architectures, input data formats, indoor/outdoor, single/multi-view 3D data. DepthContrast pretrains high capacity models for 3D recognition tasks, and leverages large scale 3D data that may not have multi-view information. We show state-ofthe-art performance on detection and segmentation benchmarks, outperforming all prior work on detection. We provide crucial insights that make our simple implementation work well -training jointly with multiple input data formats and novel data augmentations. We hope DepthContrast helps future work in 3D self-supervised learning.\n\n8\n\nFigure 3 :\n3Multiple 3D Input Formats We extend our approach to joint training with point and voxel input formats. 3 10234\n\nFigure 4 :\n4Scaling the model size and pretraining data. We increase the model capacity of the PointNet++ model by increasing the width by {2\u00d7, 3\u00d7, 4\u00d7}. When training from scratch, increasing the model capacity increases the performance but ultimately leads to overfitting.\n\nFigure 5 :\n5Pretraining benefits long tail classes. We analyze the gain of our pretraining across different classes for SUNRGBD object detection. The training data has a long tailed distribution where the least frequent classes occur 50\u00d7 less than the most frequent classes. Our pretraining improves performance for classes with fewer labeled instances by 4 \u2212 8%. (Trending line in green.)\n\n\n.2K train, 312 val (Indoor) Det. +3.6% mAP Seg. +0.9% mIOU \u2020SUNRGBD [81] \n5.2K train, 5K val (Indoor) Det \n\n+3.3% mAP \n\nS3DIS [4] \n199 train, 67 val (Indoor) Det \n\n+12.1% mAP \n\nSeg. \n\n+2.4% mIOU \n\nSynthia [71] \n19.8K train, 1.8K val (Synth.) Seg. \n\n+2.4% mIOU \n\nMatterport3D [10] \n1.4K train, 232 val (Indoor) Det. \n\n+3.9% mAP \n\nShape Classification [102] 9.8K train, 2.4K val (Synth.) Cls. \n\n+3.1% Acc  \u2020 \n\nDet.: Object Detection, Seg: Semantic Segmentation \nCls: Classification, Synth.: Synthetic,  \u2020 Results in supplemental. \n\n\n\nTable 1 :\n1Pretraining datasets and transfer tasks used in this paper.We use two different pretraining datasets without post-processing \nlike 3D registration, camera calibration. We use 8 different trans-\nfer tasks for evaluation where our DepthContrast pretraining gives \nconsistent gains (last column) over scratch pretraining. Additon-\nally, we show evaluation results on LiDAR data in  \u00a7 5.3. \n\nSGD optimizer with momentum 0.9, cosine learning rate \nscheduler [51] starting from 0.12 to 0.00012 and train the \nmodel for 1000 epochs with a batch size of 1024. \n\n\n\nTable 5 :\n5Data augmentation. We vary the data augmentation \nused for pretraining DepthContrast point models and report their \ntransfer performance. The standard data augmentation used in su-\npervised learning (VoteNet) is not sufficient to learn good self-\nsupervised models. Our proposed Random Cuboid and Random \nDrop augmentations improve performance. \n\n\n\n7\nPretraining TaskScratch ScanNet ScanNet-vid Redwood-vid (Multi-view) (Single-view) (Single-view)Shape Classification Linear (Accuracy) 50.7 \n85.1 \n85.0 \n86.4 \nSUNRGBD Detection (mAP) \n57.4 \n60.5 \n60.7 \n60.4 \n\n\n\nSelfsupervised learning for domain adaptation on point-clouds. Haggai Idan Achituve, Gal Maron, Chechik, arXiv:2003.12641arXiv preprintIdan Achituve, Haggai Maron, and Gal Chechik. Self- supervised learning for domain adaptation on point-clouds. arXiv preprint arXiv:2003.12641, 2020.\n\nLearning representations and generative models for 3d point clouds. Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, Leonidas Guibas, Proceedings of the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)PMLRPanos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representations and generative models for 3d point clouds. In Proceedings of the Inter- national Conference on Machine Learning (ICML), pages 40-49. PMLR, 2018.\n\nFast high-dimensional filtering using the permutohedral lattice. Andrew Adams, Jongmin Baek, Myers Abraham Davis, Computer Graphics Forum. Wiley Online Library29Andrew Adams, Jongmin Baek, and Myers Abraham Davis. Fast high-dimensional filtering using the permuto- hedral lattice. In Computer Graphics Forum, volume 29, pages 753-762. Wiley Online Library, 2010.\n\nJoint 2d-3d-semantic data for indoor scene understanding. Iro Armeni, Sasha Sax, Silvio Amir Roshan Zamir, Savarese, abs/1702.01105CoRRIro Armeni, Sasha Sax, Amir Roshan Zamir, and Silvio Savarese. Joint 2d-3d-semantic data for indoor scene un- derstanding. CoRR, abs/1702.01105, 2017.\n\nIoannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces. Iro Armeni, Ozan Sener, Helen Amir R Zamir, Jiang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionIro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioan- nis Brilakis, Martin Fischer, and Silvio Savarese. 3d se- mantic parsing of large-scale indoor spaces. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1534-1543, 2016.\n\nPoint convolutional neural networks by extension operators. Matan Atzmon, Haggai Maron, Yaron Lipman, arXiv:1803.10091arXiv preprintMatan Atzmon, Haggai Maron, and Yaron Lipman. Point convolutional neural networks by extension operators. arXiv preprint arXiv:1803.10091, 2018.\n\nUnsupervised learning by predicting noise. Piotr Bojanowski, Armand Joulin, Proceedings of the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)Piotr Bojanowski and Armand Joulin. Unsupervised learn- ing by predicting noise. In Proceedings of the International Conference on Machine Learning (ICML), 2017.\n\nDeep clustering for unsupervised learning of visual features. Mathilde Caron, Piotr Bojanowski, Armand Joulin, Matthijs Douze, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In Proceedings of the European Confer- ence on Computer Vision (ECCV), 2018.\n\nUnsupervised learning of visual features by contrasting cluster assignments. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin, NeurIPS. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learn- ing of visual features by contrasting cluster assignments. In NeurIPS, 2020.\n\nMatterport3d: Learning from rgb-d data in indoor environments. International Conference on 3D Vision (3DV), 2017. Matterport license avail. Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang, Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. International Con- ference on 3D Vision (3DV), 2017. Matterport license avail- able at http://kaldir.vc.in.tum.de/matterport/MP TOS.pdf.\n\nX Angel, Thomas Chang, Leonidas Funkhouser, Pat Guibas, Qixing Hanrahan, Zimo Huang, Silvio Li, Manolis Savarese, Shuran Savva, Hao Song, Su, arXiv:1512.03012An information-rich 3d model repository. arXiv preprintAngel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.\n\nA hierarchical graph network for 3d object detection on point clouds. Jintai Chen, Biwen Lei, Qingyu Song, Haochao Ying, Danny Z Chen, Jian Wu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionJintai Chen, Biwen Lei, Qingyu Song, Haochao Ying, Danny Z Chen, and Jian Wu. A hierarchical graph network for 3d object detection on point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 392-401, 2020.\n\nA simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, arXiv:2002.05709arXiv preprintTing Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.\n\nXinlei Chen, Haoqi Fan, arXiv:2003.04297Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprintXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.\n\nFast point r-cnn. Yilun Chen, Shu Liu, Xiaoyong Shen, Jiaya Jia, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionYilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Fast point r-cnn. In Proceedings of the IEEE International Con- ference on Computer Vision, pages 9775-9784, 2019.\n\nSungjoon Choi, Qian-Yi Zhou, Stephen Miller, Vladlen Koltun, arXiv:1602.02481A large dataset of object scans. Sungjoon Choi, Qian-Yi Zhou, Stephen Miller, and Vladlen Koltun. A large dataset of object scans. arXiv:1602.02481, 2016.\n\n4d spatio-temporal convnets: Minkowski convolutional neural networks. Christopher Choy, Junyoung Gwak, Silvio Savarese, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionChristopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3075- 3084, 2019.\n\nScannet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Niessner, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal- ber, Thomas Funkhouser, and Matthias Niessner. Scan- net: Richly-annotated 3d reconstructions of indoor scenes. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.\n\nUnsupervised visual representation learning by context prediction. Carl Doersch, Abhinav Gupta, Alexei A Efros, Proceedings of the International Conference on Computer Vision (ICCV). the International Conference on Computer Vision (ICCV)Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsu- pervised visual representation learning by context predic- tion. In Proceedings of the International Conference on Computer Vision (ICCV), 2015.\n\nAdversarial feature learning. J Donahue, P Krahenb\u00fchl, T Darrell, International Conference on Learning Representations (ICLR). J. Donahue, P. Krahenb\u00fchl, and T. Darrell. Adversarial feature learning. In International Conference on Learning Representations (ICLR), 2016.\n\nDiscriminative unsupervised feature learning with exemplar convolutional neural networks. Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, Thomas Brox, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). 38Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springen- berg, Martin Riedmiller, and Thomas Brox. Discrimina- tive unsupervised feature learning with exemplar convolu- tional neural networks. IEEE Transactions on Pattern Anal- ysis and Machine Intelligence (TPAMI), 38(9):1734-1747, 2016.\n\nBastian Leibe, and Matthias Nie\u00dfner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. Francis Engelmann, Martin Bokeloh, Alireza Fathi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionFrancis Engelmann, Martin Bokeloh, Alireza Fathi, Bas- tian Leibe, and Matthias Nie\u00dfner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9031-9040, 2020.\n\nA point set generation network for 3d object reconstruction from a single image. Haoqiang Fan, Hao Su, Leonidas J Guibas, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHaoqiang Fan, Hao Su, and Leonidas J Guibas. A point set generation network for 3d object reconstruction from a single image. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 605-613, 2017.\n\nApple unveils new ipad pro with breakthrough lidar scanner and brings trackpad support to ipados. Cat Franklin, Cat Franklin. Apple unveils new ipad pro with break- through lidar scanner and brings trackpad support to ipados. https://www.apple.com/, 2020.\n\nMultiresolution tree networks for 3d point cloud processing. Matheus Gadelha, Rui Wang, Subhransu Maji, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Matheus Gadelha, Rui Wang, and Subhransu Maji. Mul- tiresolution tree networks for 3d point cloud processing. In Proceedings of the European Conference on Computer Vi- sion (ECCV), pages 103-118, 2018.\n\nVision meets robotics: The kitti dataset. Andreas Geiger, Philip Lenz, Christoph Stiller, Raquel Urtasun, International Journal of Robotics Research. Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. Interna- tional Journal of Robotics Research (IJRR), 2013.\n\nLarge-scale weaklysupervised pre-training for video action recognition. Deepti Ghadiyaram, Matt Feiszli, Du Tran, Xueting Yan, Heng Wang, Dhruv Mahajan, arXiv:1905.00561arXiv preprintDeepti Ghadiyaram, Matt Feiszli, Du Tran, Xueting Yan, Heng Wang, and Dhruv Mahajan. Large-scale weakly- supervised pre-training for video action recognition. arXiv preprint arXiv:1905.00561, 2019.\n\nLearning a predictable and generative vector representation for objects. Rohit Girdhar, David Ford Fouhey, Mikel Rodriguez, Abhinav Gupta, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Rohit Girdhar, David Ford Fouhey, Mikel Rodriguez, and Abhinav Gupta. Learning a predictable and generative vec- tor representation for objects. In Proceedings of the Euro- pean Conference on Computer Vision (ECCV), 2016.\n\nMesh r-cnn. Georgia Gkioxari, Jitendra Malik, Justin Johnson, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionGeorgia Gkioxari, Jitendra Malik, and Justin Johnson. Mesh r-cnn. In Proceedings of the IEEE International Con- ference on Computer Vision, pages 9785-9795, 2019.\n\n. Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan 9Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan 9\n\nScaling and benchmarking self-supervised visual representation learning. Misra, Proceedings of the International Conference on Computer Vision (ICCV). the International Conference on Computer Vision (ICCV)Misra. Scaling and benchmarking self-supervised visual representation learning. Proceedings of the International Conference on Computer Vision (ICCV), 2019.\n\nBen Graham, arXiv:1505.02890Sparse 3d convolutional neural networks. arXiv preprintBen Graham. Sparse 3d convolutional neural networks. arXiv preprint arXiv:1505.02890, 2015.\n\n3d semantic segmentation with submanifold sparse convolutional networks. Benjamin Graham, Martin Engelcke, Laurens Van Der Maaten, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionBenjamin Graham, Martin Engelcke, and Laurens van der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 9224-9232, 2018.\n\nA papier-m\u00e2ch\u00e9 approach to learning 3d surface generation. Thibault Groueix, Matthew Fisher, G Vladimir, Kim, C Bryan, Mathieu Russell, Aubry, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)Thibault Groueix, Matthew Fisher, Vladimir G Kim, Bryan C Russell, and Mathieu Aubry. A papier-m\u00e2ch\u00e9 ap- proach to learning 3d surface generation. In Proceedings of the Conference on Computer Vision and Pattern Recogni- tion (CVPR), pages 216-224, 2018.\n\nDimensionality reduction by learning an invariant mapping. Raia Hadsell, Sumit Chopra, Yann Lecun, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimension- ality reduction by learning an invariant mapping. In Pro- ceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2006.\n\nUnsupervised multi-task feature learning on point clouds. Kaveh Hassani, Mike Haley, Proceedings of the International Conference on Computer Vision (ICCV). the International Conference on Computer Vision (ICCV)Kaveh Hassani and Mike Haley. Unsupervised multi-task feature learning on point clouds. In Proceedings of the In- ternational Conference on Computer Vision (ICCV), pages 8160-8171, 2019.\n\nMomentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, arXiv:1911.05722arXiv preprintKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual rep- resentation learning. arXiv preprint arXiv:1911.05722, 2019.\n\nJ Olivier, Ali H\u00e9naff, Carl Razavi, Doersch, Aaron Sm Eslami, Van Den Oord, arXiv:1905.09272Data-efficient image recognition with contrastive predictive coding. arXiv preprintOlivier J H\u00e9naff, Ali Razavi, Carl Doersch, SM Eslami, and Aaron van den Oord. Data-efficient image recog- nition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019.\n\nMonte carlo convolution for learning on non-uniformly sampled point clouds. Pedro Hermosilla, Tobias Ritschel, Pere-Pau V\u00e1zquez, Alvar Vinacua, Timo Ropinski, ACM Transactions on Graphics (TOG). 376Pedro Hermosilla, Tobias Ritschel, Pere-Pau V\u00e1zquez, Alvar Vinacua, and Timo Ropinski. Monte carlo convo- lution for learning on non-uniformly sampled point clouds. ACM Transactions on Graphics (TOG), 37(6):1-12, 2018.\n\nScenenn: A scene meshes dataset with annotations. International Conference on 3D Vision (3DV). Binh-Son Hua, Quang-Hieu Pham, Duc Thanh Nguyen, Minh-Khoi Tran, Lap-Fai Yu, and Sai-Kit YeungBinh-Son Hua, Quang-Hieu Pham, Duc Thanh Nguyen, Minh-Khoi Tran, Lap-Fai Yu, and Sai-Kit Yeung. Scenenn: A scene meshes dataset with annotations. In International Conference on 3D Vision (3DV), 2016.\n\nPointwise convolutional neural networks. Binh-Son, Minh-Khoi Hua, Sai-Kit Tran, Yeung, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionBinh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. Point- wise convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition, pages 984-993, 2018.\n\nAllan Jabri, Andrew Owens, Alexei A Efros, arXiv:2006.14613Spacetime correspondence as a contrastive random walk. arXiv preprintAllan Jabri, Andrew Owens, and Alexei A Efros. Space- time correspondence as a contrastive random walk. arXiv preprint arXiv:2006.14613, 2020.\n\nA category-level 3d object dataset: Putting the kinect to work. Allison Janoch, Sergey Karayev, Yangqing Jia, Jonathan T Barron, Mario Fritz, Kate Saenko, Trevor Darrell, Consumer depth cameras for computer vision. SpringerAllison Janoch, Sergey Karayev, Yangqing Jia, Jonathan T Barron, Mario Fritz, Kate Saenko, and Trevor Darrell. A category-level 3d object dataset: Putting the kinect to work. In Consumer depth cameras for computer vision, pages 141-165. Springer, 2013.\n\nInvariant information clustering for unsupervised image classification and segmentation. Xu Ji, F Jo\u00e3o, Andrea Henriques, Vedaldi, Proceedings of the International Conference on Computer Vision (ICCV). the International Conference on Computer Vision (ICCV)Xu Ji, Jo\u00e3o F Henriques, and Andrea Vedaldi. Invariant in- formation clustering for unsupervised image classification and segmentation. In Proceedings of the International Con- ference on Computer Vision (ICCV), 2019.\n\nSelf-supervised modal and view invariant feature learning. Longlong Jing, Yucheng Chen, Ling Zhang, Mingyi He, Yingli Tian, arXiv:2005.14169arXiv preprintLonglong Jing, Yucheng Chen, Ling Zhang, Mingyi He, and Yingli Tian. Self-supervised modal and view invariant feature learning. arXiv preprint arXiv:2005.14169, 2020.\n\nEscape from cells: Deep kd-networks for the recognition of 3d point cloud models. Roman Klokov, Victor Lempitsky, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionRoman Klokov and Victor Lempitsky. Escape from cells: Deep kd-networks for the recognition of 3d point cloud models. In Proceedings of the IEEE International Confer- ence on Computer Vision, pages 863-872, 2017.\n\nBig transfer (bit): General visual representation learning. Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby, arXiv:1912.113706arXiv preprintAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. arXiv preprint arXiv:1912.11370, 6, 2019.\n\nRevisiting self-supervised visual representation learning. Alexander Kolesnikov, Xiaohua Zhai, Lucas Beyer, arXiv:1901.09005arXiv preprintAlexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representation learning. arXiv preprint arXiv:1901.09005, 2019.\n\nGs3d: An efficient 3d object detection framework for autonomous driving. Buyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng, Xiaogang Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionBuyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng, and Xiaogang Wang. Gs3d: An efficient 3d object detection framework for autonomous driving. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition, pages 1019-1028, 2019.\n\nSo-net: Selforganizing network for point cloud analysis. Jiaxin Li, M Ben, Gim Hee Chen, Lee, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)Jiaxin Li, Ben M Chen, and Gim Hee Lee. So-net: Self- organizing network for point cloud analysis. In Proceedings of the Conference on Computer Vision and Pattern Recog- nition (CVPR), pages 9397-9406, 2018.\n\nPointcnn: Convolution on xtransformed points. Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, Baoquan Chen, Advances in Neural Information Processing Systems. Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolution on x- transformed points. In Advances in Neural Information Processing Systems, pages 820-830, 2018.\n\nSgdr: Stochastic gradient descent with warm restarts. Ilya Loshchilov, Frank Hutter, arXiv:1608.03983arXiv preprintIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.\n\nAshwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, ECCV. Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the lim- its of weakly supervised pretraining. In ECCV, 2018.\n\nStacked convolutional auto-encoders for hierarchical feature extraction. J Masci, U Meier, D Cires, J Schmidhuber, ICANN. J. Masci, U. Meier, D. Cires, and J. Schmidhuber. Stacked convolutional auto-encoders for hierarchical feature extrac- tion. In ICANN, pages 52-59, 2011.\n\nVoxnet: A 3d convolutional neural network for real-time object recognition. Daniel Maturana, Sebastian Scherer, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEDaniel Maturana and Sebastian Scherer. Voxnet: A 3d con- volutional neural network for real-time object recognition. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 922-928. IEEE, 2015.\n\nAdversarial variational Bayes: Unifying variational autoencoders and generative adversarial networks. L Mescheder, S Nowozin, A Geiger, Proceedings of the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)L. Mescheder, S. Nowozin, and A. Geiger. Adversarial vari- ational Bayes: Unifying variational autoencoders and gen- erative adversarial networks. In Proceedings of the Interna- tional Conference on Machine Learning (ICML), 2017.\n\nSelf-supervised learning of pretext-invariant representations. Ishan Misra, Laurens Van Der Maaten, arXiv:1912.01991arXiv preprintIshan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. arXiv preprint arXiv:1912.01991, 2019.\n\nAudio-visual instance discrimination with cross-modal agreement. Pedro Morgado, Nuno Vasconcelos, Ishan Misra, Pedro Morgado, Nuno Vasconcelos, and Ishan Misra. Audio-visual instance discrimination with cross-modal agreement. https://arxiv.org/abs/2004.12943, 2020.\n\nIndoor segmentation and support inference from rgbd images. Derek Pushmeet Kohli Nathan Silberman, Rob Hoiem, Fergus, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images. In Proceedings of the European Conference on Computer Vision (ECCV), 2012.\n\nUnsupervised learning of visual representations by solving jigsaw puzzles. Mehdi Noroozi, Paolo Favaro, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In Pro- ceedings of the European Conference on Computer Vision (ECCV), 2016.\n\nEmergence of simplecell receptive field properties by learning a sparse code for natural images. B A Olshausen, D J Field, Nature. 3816583607B. A. Olshausen and D. J. Field. Emergence of simple- cell receptive field properties by learning a sparse code for natural images. Nature, 381(6583):607, 1996.\n\nRepresentation learning with contrastive predictive coding. Aaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.03748arXiv preprintAaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre- sentation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\n\nMultimodal self-supervision from generalized data transformations. Mandela Patrick, Yuki M Asano, Ruth Fong, F Jo\u00e3o, Geoffrey Henriques, Andrea Zweig, Vedaldi, arXiv:2003.04298arXiv preprintMandela Patrick, Yuki M Asano, Ruth Fong, Jo\u00e3o F Hen- riques, Geoffrey Zweig, and Andrea Vedaldi. Multi- modal self-supervision from generalized data transforma- tions. arXiv preprint arXiv:2003.04298, 2020.\n\nJsis3d: Joint semantic-instance segmentation of 3d point clouds with multi-task pointwise networks and multi-value conditional random fields. Quang-Hieu Pham, Thanh Nguyen, Binh-Son, Gemma Hua, Sai-Kit Roig, Yeung, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionQuang-Hieu Pham, Thanh Nguyen, Binh-Son Hua, Gemma Roig, and Sai-Kit Yeung. Jsis3d: Joint semantic-instance segmentation of 3d point clouds with multi-task pointwise networks and multi-value conditional random fields. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8827-8836, 2019.\n\nDeep hough voting for 3d object detection in point clouds. Or Charles R Qi, Kaiming Litany, Leonidas J He, Guibas, arXiv:1904.09664arXiv preprintCharles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. arXiv preprint arXiv:1904.09664, 2019.\n\nFrustum pointnets for 3d object detection from rgb-d data. Wei Charles R Qi, Chenxia Liu, Hao Wu, Leonidas J Su, Guibas, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionCharles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J Guibas. Frustum pointnets for 3d object de- tection from rgb-d data. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 918-927, 2018.\n\nPointnet: Deep learning on point sets for 3d classification and segmentation. Hao Charles R Qi, Kaichun Su, Leonidas J Mo, Guibas, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionCharles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 652- 660, 2017.\n\nVolumetric and multi-view cnns for object classification on 3d data. Hao Charles R Qi, Matthias Su, Angela Nie\u00dfner, Mengyuan Dai, Leonidas J Yan, Guibas, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionCharles R Qi, Hao Su, Matthias Nie\u00dfner, Angela Dai, Mengyuan Yan, and Leonidas J Guibas. Volumetric and multi-view cnns for object classification on 3d data. In Pro- ceedings of the IEEE conference on computer vision and pattern recognition, pages 5648-5656, 2016.\n\nPointnet++: Deep hierarchical feature learning on point sets in a metric space. Li Charles Ruizhongtai Qi, Hao Yi, Leonidas J Su, Guibas, Advances in neural information processing systems. Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In Advances in neural informa- tion processing systems, pages 5099-5108, 2017.\n\nUnsupervised learning of invariant feature hierarchies with applications to object recognition. Aurelio Marc, Fu-Jie Ranzato, Y-Lan Huang, Yann Boureau, Lecun, CVPR. Marc'Aurelio Ranzato, Fu-Jie Huang, Y-Lan Boureau, and Yann LeCun. Unsupervised learning of invariant feature hi- erarchies with applications to object recognition. In CVPR, 2007.\n\nOctnet: Learning deep 3d representations at high resolutions. Gernot Riegler, Ali Osman Ulusoy, Andreas Geiger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionGernot Riegler, Ali Osman Ulusoy, and Andreas Geiger. Octnet: Learning deep 3d representations at high resolu- tions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3577-3586, 2017.\n\nThe SYNTHIA Dataset: A large collection of synthetic images for semantic segmentation of urban scenes. German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, Antonio Lopez, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio Lopez. The SYNTHIA Dataset: A large collection of synthetic images for semantic segmenta- tion of urban scenes. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nDeep Boltzmann machines. R Salakhutdinov, G Hinton, AI-STATSR. Salakhutdinov and G. Hinton. Deep Boltzmann ma- chines. In AI-STATS, pages 448-455, 2009.\n\nWhat is depthvision camera on galaxy s20+ and s20 ultra?. Samsung, Samsung. What is depthvision camera on galaxy s20+ and s20 ultra? https://www.samsung.com/, 2020.\n\nSelf-supervised deep learning on point clouds by reconstructing space. Jonathan Sauder, Bjarne Sievers, Advances in Neural Information Processing Systems (NeurIPS). Jonathan Sauder and Bjarne Sievers. Self-supervised deep learning on point clouds by reconstructing space. In Advances in Neural Information Processing Systems (NeurIPS), pages 12962-12972, 2019.\n\nPigraphs: learning interaction snapshots from observations. Manolis Savva, X Angel, Pat Chang, Matthew Hanrahan, Matthias Fisher, Nie\u00dfner, ACM Transactions on Graphics (TOG). 354Manolis Savva, Angel X Chang, Pat Hanrahan, Matthew Fisher, and Matthias Nie\u00dfner. Pigraphs: learning interac- tion snapshots from observations. ACM Transactions on Graphics (TOG), 35(4):1-12, 2016.\n\nPv-rcnn: Pointvoxel feature set abstraction for 3d object detection. Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, Hongsheng Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionShaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point- voxel feature set abstraction for 3d object detection. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10529-10538, 2020.\n\nPointrcnn: 3d object proposal generation and detection from point cloud. Shaoshuai Shi, Xiaogang Wang, Hongsheng Li, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionShaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointr- cnn: 3d object proposal generation and detection from point cloud. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770-779, 2019.\n\nFrom points to parts: 3d object detection from point cloud with part-aware and part-aggregation network. Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, Hongsheng Li, arXiv:1907.03670arXiv preprintShaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. From points to parts: 3d object detec- tion from point cloud with part-aware and part-aggregation network. arXiv preprint arXiv:1907.03670, 2019.\n\nIndoor segmentation and support inference from rgbd images. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, Rob Fergus, European conference on computer vision. SpringerNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In European conference on computer vision, pages 746-760. Springer, 2012.\n\nImproved deep metric learning with multiclass n-pair loss objective. Kihyuk Sohn, Advances in Neural Information Processing Systems (NeurIPS). Kihyuk Sohn. Improved deep metric learning with multi- class n-pair loss objective. In Advances in Neural Informa- tion Processing Systems (NeurIPS), 2016.\n\nSun rgb-d: A rgb-d scene understanding benchmark suite. Shuran Song, P Samuel, Jianxiong Lichtenberg, Xiao, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionShuran Song, Samuel P Lichtenberg, and Jianxiong Xiao. Sun rgb-d: A rgb-d scene understanding benchmark suite. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 567-576, 2015.\n\nDeep sliding shapes for amodal 3d object detection in rgb-d images. Shuran Song, Jianxiong Xiao, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionShuran Song and Jianxiong Xiao. Deep sliding shapes for amodal 3d object detection in rgb-d images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 808-816, 2016.\n\nLidar on the iphone. Scott Stein, 12Scott Stein. Lidar on the iphone 12 pro. https://www. cnet.com/, 2020.\n\nA benchmark for the evaluation of rgb-d slam systems. J\u00fcrgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, Daniel Cremers, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEEJ\u00fcrgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. A benchmark for the evalua- tion of rgb-d slam systems. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 573- 580. IEEE, 2012.\n\nSplatnet: Sparse lattice networks for point cloud processing. Hang Su, Varun Jampani, Deqing Sun, Subhransu Maji, Evangelos Kalogerakis, Ming-Hsuan Yang, Jan Kautz, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionHang Su, Varun Jampani, Deqing Sun, Subhransu Maji, Evangelos Kalogerakis, Ming-Hsuan Yang, and Jan Kautz. Splatnet: Sparse lattice networks for point cloud process- ing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2530-2539, 2018.\n\nEvangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. Hang Su, Subhransu Maji, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionHang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In Proceedings of the IEEE in- ternational conference on computer vision, pages 945-953, 2015.\n\nRevisiting unreasonable effectiveness of data in deep learning era. Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta, Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi- nav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In ICCV, 2017.\n\nScalability in perception for autonomous driving: Waymo open dataset. Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionThis publication was made using the Waymo Open Dataset. provided by Waymo LLC under license terms available at waymo.com/openPei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aure- lien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2446-2454, 2020. This publication was made using the Waymo Open Dataset, provided by Waymo LLC under license terms available at waymo.com/open.\n\nGoing deeper with convolutions. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the Conference on Com- puter Vision and Pattern Recognition (CVPR), 2015.\n\n. Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas 11Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas 11\n\nOctree generating networks: Efficient convolutional architectures for high-resolution 3d outputs. Brox, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionBrox. Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs. In Proceedings of the IEEE International Conference on Computer Vision, pages 2088-2096, 2017.\n\nTangent convolutions for dense prediction in 3d. Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, Qian-Yi Zhou, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionMaxim Tatarchenko, Jaesik Park, Vladlen Koltun, and Qian-Yi Zhou. Tangent convolutions for dense prediction in 3d. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3887-3896, 2018.\n\nSegcloud: Semantic segmentation of 3d point clouds. Lyne Tchapmi, Christopher Choy, Iro Armeni, Junyoung Gwak, Silvio Savarese, 2017 international conference on 3D vision (3DV). IEEELyne Tchapmi, Christopher Choy, Iro Armeni, JunYoung Gwak, and Silvio Savarese. Segcloud: Semantic segmen- tation of 3d point clouds. In 2017 international conference on 3D vision (3DV), pages 537-547. IEEE, 2017.\n\nYonglong Tian, Dilip Krishnan, Phillip Isola, arXiv:1906.05849Contrastive multiview coding. arXiv preprintYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint arXiv:1906.05849, 2019.\n\nWhat makes for good views for contrastive learning?. Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, Phillip Isola, arXiv:2005.10243arXiv preprintYonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning? arXiv preprint arXiv:2005.10243, 2020.\n\nExtracting and composing robust features with denoising autoencoders. P Vincent, H Larochelle, Y Bengio, P.-A Manzagol, ICML. P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, 2008.\n\nPixel2mesh: Generating 3d mesh models from single rgb images. Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, Yu-Gang Jiang, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh models from single rgb images. In Proceedings of the Eu- ropean Conference on Computer Vision (ECCV), pages 52- 67, 2018.\n\nUnsupervised learning of visual representations using videos. Xiaolong Wang, Abhinav Gupta, Proceedings of the International Conference on Computer Vision (ICCV). the International Conference on Computer Vision (ICCV)Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos. In Proceedings of the International Conference on Computer Vision (ICCV), 2015.\n\nAssociatively segmenting instances and semantics in point clouds. Xinlong Wang, Shu Liu, Xiaoyong Shen, Chunhua Shen, Jiaya Jia, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionXinlong Wang, Shu Liu, Xiaoyong Shen, Chunhua Shen, and Jiaya Jia. Associatively segmenting instances and se- mantics in point clouds. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 4096-4105, 2019.\n\nDeep closest point: Learning representations for point cloud registration. Yue Wang, Justin M Solomon, Proceedings of the International Conference on Computer Vision (ICCV). the International Conference on Computer Vision (ICCV)Yue Wang and Justin M Solomon. Deep closest point: Learning representations for point cloud registration. In Proceedings of the International Conference on Computer Vision (ICCV), pages 3523-3532, 2019.\n\nYue Wang, Yongbin Sun, Ziwei Liu, E Sanjay, Sarma, Justin M Michael M Bronstein, Solomon, arXiv:1801.07829Dynamic graph cnn for learning on point clouds. arXiv preprintYue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. arXiv preprint arXiv:1801.07829, 2018.\n\nPointconv: Deep convolutional networks on 3d point clouds. Wenxuan Wu, Zhongang Qi, Li Fuxin, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionWenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep convolutional networks on 3d point clouds. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9621-9630, 2019.\n\n3d shapenets: A deep representation for volumetric shapes. Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, Jianxiong Xiao, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015. Boston, MA, USAZhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In IEEE Conference on Computer Vision and Pattern Recog- nition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 1912-1920, 2015.\n\nUnsupervised feature learning via non-parametric instance discrimination. Zhirong Wu, Yuanjun Xiong, X Stella, Dahua Yu, Lin, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the Conference on Com- puter Vision and Pattern Recognition (CVPR), 2018.\n\nSun3d: A database of big spaces reconstructed using sfm and object labels. Jianxiong Xiao, Andrew Owens, Antonio Torralba, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionJianxiong Xiao, Andrew Owens, and Antonio Torralba. Sun3d: A database of big spaces reconstructed using sfm and object labels. In Proceedings of the IEEE international conference on computer vision, pages 1625-1632, 2013.\n\nPointcontrast: Unsupervised pre-training for 3d point cloud understanding. Saining Xie, Jiatao Gu, Demi Guo, Leonidas J Charles R Qi, Or Guibas, Litany, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2020Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas J Guibas, and Or Litany. Pointcontrast: Unsu- pervised pre-training for 3d point cloud understanding. In Proceedings of the European Conference on Computer Vi- sion (ECCV), 2020.\n\nAttentional shapecontextnet for point cloud recognition. Saining Xie, Sainan Liu, Zeyu Chen, Zhuowen Tu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSaining Xie, Sainan Liu, Zeyu Chen, and Zhuowen Tu. At- tentional shapecontextnet for point cloud recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4606-4615, 2018.\n\nSpidercnn: Deep learning on point sets with parameterized convolutional filters. Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, Yu Qiao, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao. Spidercnn: Deep learning on point sets with param- eterized convolutional filters. In Proceedings of the Euro- pean Conference on Computer Vision (ECCV), pages 87- 102, 2018.\n\nPointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling. Xu Yan, Chaoda Zheng, Zhen Li, Sheng Wang, Shuguang Cui, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionXu Yan, Chaoda Zheng, Zhen Li, Sheng Wang, and Shuguang Cui. Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 5589-5598, 2020.\n\nSecond: Sparsely embedded convolutional detection. Yan Yan, Yuxing Mao, Bo Li, Sensors. 18103337Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embedded convolutional detection. Sensors, 18(10):3337, 2018.\n\nFoldingnet: Point cloud auto-encoder via deep grid deformation. Yaoqing Yang, Chen Feng, Yiru Shen, Dong Tian, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)Yaoqing Yang, Chen Feng, Yiru Shen, and Dong Tian. Foldingnet: Point cloud auto-encoder via deep grid defor- mation. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), pages 206-215, 2018.\n\nStd: Sparse-to-dense 3d object detector for point cloud. Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, Jiaya Jia, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionZetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, and Ji- aya Jia. Std: Sparse-to-dense 3d object detector for point cloud. In Proceedings of the IEEE International Confer- ence on Computer Vision, pages 1951-1960, 2019.\n\nGspn: Generative shape proposal network for 3d instance segmentation in point cloud. Li Yi, Wang Zhao, He Wang, Minhyuk Sung, Leonidas J Guibas, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLi Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative shape proposal net- work for 3d instance segmentation in point cloud. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3947-3956, 2019.\n\nPath-invariant map networks. Zaiwei Zhang, Zhenxiao Liang, Lemeng Wu, Xiaowei Zhou, Qixing Huang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZaiwei Zhang, Zhenxiao Liang, Lemeng Wu, Xiaowei Zhou, and Qixing Huang. Path-invariant map networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11084-11094, 2019.\n\nH3dnet: 3d object detection using hybrid geometric primitives. Zaiwei Zhang, Bo Sun, Haitao Yang, Qixing Huang, arXiv:2006.05682arXiv preprintZaiwei Zhang, Bo Sun, Haitao Yang, and Qixing Huang. H3dnet: 3d object detection using hybrid geometric primi- tives. arXiv preprint arXiv:2006.05682, 2020.\n\nDeep generative modeling for scene synthesis via hybrid representations. Zaiwei Zhang, Zhenpei Yang, Chongyang Ma, Linjie Luo, Alexander Huth, Etienne Vouga, Qixing Huang, ACM Transactions on Graphics (TOG). 392Zaiwei Zhang, Zhenpei Yang, Chongyang Ma, Linjie Luo, Alexander Huth, Etienne Vouga, and Qixing Huang. Deep generative modeling for scene synthesis via hybrid rep- resentations. ACM Transactions on Graphics (TOG), 39(2):1-21, 2020.\n\nVoxelnet: End-to-end learning for point cloud based 3d object detection. Yin Zhou, Oncel Tuzel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning for point cloud based 3d object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4490-4499, 2018.\n", "annotations": {"author": "[{\"end\":137,\"start\":65},{\"end\":175,\"start\":138},{\"end\":213,\"start\":176},{\"end\":249,\"start\":214}]", "publisher": null, "author_last_name": "[{\"end\":77,\"start\":72},{\"end\":151,\"start\":144},{\"end\":189,\"start\":183},{\"end\":225,\"start\":220}]", "author_first_name": "[{\"end\":71,\"start\":65},{\"end\":143,\"start\":138},{\"end\":182,\"start\":176},{\"end\":219,\"start\":214}]", "author_affiliation": "[{\"end\":100,\"start\":79},{\"end\":136,\"start\":102},{\"end\":174,\"start\":153},{\"end\":212,\"start\":191},{\"end\":248,\"start\":227}]", "title": "[{\"end\":62,\"start\":1},{\"end\":311,\"start\":250}]", "venue": null, "abstract": "[{\"end\":1541,\"start\":341}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1700,\"start\":1696},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":1703,\"start\":1700},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":1706,\"start\":1703},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":1709,\"start\":1706},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1919,\"start\":1916},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":1922,\"start\":1919},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":1925,\"start\":1922},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":1928,\"start\":1925},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":1931,\"start\":1928},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":1958,\"start\":1954},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2133,\"start\":2129},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2811,\"start\":2807},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":2814,\"start\":2811},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":2817,\"start\":2814},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":2972,\"start\":2967},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3404,\"start\":3400},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":3879,\"start\":3874},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4203,\"start\":4199},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":4236,\"start\":4232},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":4452,\"start\":4448},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":5816,\"start\":5812},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":5819,\"start\":5816},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":5822,\"start\":5819},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":5825,\"start\":5822},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":5828,\"start\":5825},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5908,\"start\":5905},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5910,\"start\":5908},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5913,\"start\":5910},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5924,\"start\":5920},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":5927,\"start\":5924},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5947,\"start\":5943},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":5950,\"start\":5947},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":5953,\"start\":5950},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5978,\"start\":5975},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5981,\"start\":5978},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5984,\"start\":5981},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5987,\"start\":5984},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5990,\"start\":5987},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":5993,\"start\":5990},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":5996,\"start\":5993},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":5999,\"start\":5996},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6166,\"start\":6162},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":6169,\"start\":6166},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6237,\"start\":6233},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":6271,\"start\":6266},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":6354,\"start\":6350},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":6357,\"start\":6354},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":6360,\"start\":6357},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":6408,\"start\":6403},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":6464,\"start\":6460},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6494,\"start\":6490},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6727,\"start\":6724},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6729,\"start\":6727},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6732,\"start\":6729},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6735,\"start\":6732},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6738,\"start\":6735},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6741,\"start\":6738},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6744,\"start\":6741},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":6747,\"start\":6744},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":6750,\"start\":6747},{\"attributes\":{\"ref_id\":\"b111\"},\"end\":6754,\"start\":6750},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":6782,\"start\":6777},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":7610,\"start\":7606},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":7613,\"start\":7610},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7674,\"start\":7671},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7677,\"start\":7674},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7680,\"start\":7677},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":7683,\"start\":7680},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":7686,\"start\":7683},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":7689,\"start\":7686},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":7692,\"start\":7689},{\"attributes\":{\"ref_id\":\"b101\"},\"end\":7696,\"start\":7692},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":7700,\"start\":7696},{\"attributes\":{\"ref_id\":\"b108\"},\"end\":7704,\"start\":7700},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7728,\"start\":7724},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":7731,\"start\":7728},{\"attributes\":{\"ref_id\":\"b111\"},\"end\":7735,\"start\":7731},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":7764,\"start\":7760},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":7767,\"start\":7764},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":7770,\"start\":7767},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":7773,\"start\":7770},{\"attributes\":{\"ref_id\":\"b113\"},\"end\":7777,\"start\":7773},{\"attributes\":{\"ref_id\":\"b113\"},\"end\":7781,\"start\":7777},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":7785,\"start\":7781},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7986,\"start\":7982},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":7989,\"start\":7986},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8074,\"start\":8071},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8077,\"start\":8074},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8080,\"start\":8077},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8083,\"start\":8080},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8086,\"start\":8083},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8089,\"start\":8086},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":8092,\"start\":8089},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":8095,\"start\":8092},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":8098,\"start\":8095},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8145,\"start\":8141},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":8148,\"start\":8145},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":8152,\"start\":8148},{\"attributes\":{\"ref_id\":\"b117\"},\"end\":8156,\"start\":8152},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8231,\"start\":8227},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":8234,\"start\":8231},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":8237,\"start\":8234},{\"attributes\":{\"ref_id\":\"b114\"},\"end\":8242,\"start\":8237},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":8247,\"start\":8242},{\"attributes\":{\"ref_id\":\"b116\"},\"end\":8252,\"start\":8247},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":8364,\"start\":8360},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8415,\"start\":8411},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8614,\"start\":8610},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8617,\"start\":8614},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":8621,\"start\":8617},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":8625,\"start\":8621},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":8629,\"start\":8625},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":8632,\"start\":8629},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":8676,\"start\":8671},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8933,\"start\":8930},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8936,\"start\":8933},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8939,\"start\":8936},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8942,\"start\":8939},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":8945,\"start\":8942},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":8948,\"start\":8945},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":8951,\"start\":8948},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":8954,\"start\":8951},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":8957,\"start\":8954},{\"attributes\":{\"ref_id\":\"b105\"},\"end\":8961,\"start\":8957},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9010,\"start\":9006},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":9013,\"start\":9010},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":9016,\"start\":9013},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9112,\"start\":9108},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9115,\"start\":9112},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":9118,\"start\":9115},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":9121,\"start\":9118},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":9125,\"start\":9121},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9150,\"start\":9146},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":9153,\"start\":9150},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":9156,\"start\":9153},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":9160,\"start\":9156},{\"attributes\":{\"ref_id\":\"b109\"},\"end\":9164,\"start\":9160},{\"end\":9219,\"start\":9194},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":9496,\"start\":9491},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9525,\"start\":9521},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":9585,\"start\":9581},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11070,\"start\":11066},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":11073,\"start\":11070},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":11076,\"start\":11073},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":11764,\"start\":11759},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11798,\"start\":11794},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11801,\"start\":11798},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12097,\"start\":12093},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12100,\"start\":12097},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":12103,\"start\":12100},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":12107,\"start\":12103},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12144,\"start\":12140},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12288,\"start\":12287},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":13929,\"start\":13925},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":13951,\"start\":13947},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":13954,\"start\":13951},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":14409,\"start\":14405},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14736,\"start\":14732},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14816,\"start\":14812},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15036,\"start\":15032},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":15402,\"start\":15398},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":15737,\"start\":15733},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16264,\"start\":16260},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":16408,\"start\":16404},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16455,\"start\":16451},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16778,\"start\":16774},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16832,\"start\":16828},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16905,\"start\":16901},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":17398,\"start\":17393},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17741,\"start\":17737},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":17902,\"start\":17898},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":18919,\"start\":18915},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":20019,\"start\":20015},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":20167,\"start\":20163},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":20521,\"start\":20516},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":20750,\"start\":20745},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":20955,\"start\":20951},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":21178,\"start\":21174},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":21181,\"start\":21178},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":21184,\"start\":21181},{\"attributes\":{\"ref_id\":\"b105\"},\"end\":21188,\"start\":21184},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":22160,\"start\":22156},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":22710,\"start\":22705},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":22727,\"start\":22723},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":23422,\"start\":23418},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":23787,\"start\":23782},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":24430,\"start\":24425},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":24586,\"start\":24581},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":24603,\"start\":24599},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":28147,\"start\":28142},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28709,\"start\":28705},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":29075,\"start\":29070},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":29749,\"start\":29744},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":30367,\"start\":30362},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":30740,\"start\":30736},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":30878,\"start\":30873},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31022,\"start\":31018},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":31025,\"start\":31022},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":31288,\"start\":31285},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31291,\"start\":31288},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":31294,\"start\":31291},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":31297,\"start\":31294},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":31300,\"start\":31297},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":31706,\"start\":31702},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31728,\"start\":31724},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":32088,\"start\":32084},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":32265,\"start\":32260},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32682,\"start\":32678},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":33721,\"start\":33717},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":34026,\"start\":34022},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":34064,\"start\":34060},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":34752,\"start\":34748},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":34799,\"start\":34795},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":34817,\"start\":34813}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36349,\"start\":36226},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36624,\"start\":36350},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37015,\"start\":36625},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":37548,\"start\":37016},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":38115,\"start\":37549},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":38475,\"start\":38116},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":38688,\"start\":38476}]", "paragraph": "[{\"end\":2134,\"start\":1557},{\"end\":2783,\"start\":2136},{\"end\":3405,\"start\":2785},{\"end\":4453,\"start\":3407},{\"end\":4502,\"start\":4455},{\"end\":4587,\"start\":4504},{\"end\":4785,\"start\":4589},{\"end\":5044,\"start\":4787},{\"end\":5257,\"start\":5046},{\"end\":5450,\"start\":5259},{\"end\":9220,\"start\":5467},{\"end\":10227,\"start\":9238},{\"end\":11098,\"start\":10255},{\"end\":12433,\"start\":11186},{\"end\":12545,\"start\":12435},{\"end\":13213,\"start\":12588},{\"end\":13731,\"start\":13320},{\"end\":14076,\"start\":13804},{\"end\":15230,\"start\":14099},{\"end\":16265,\"start\":15259},{\"end\":16689,\"start\":16292},{\"end\":16909,\"start\":16701},{\"end\":17653,\"start\":16925},{\"end\":18265,\"start\":17655},{\"end\":19384,\"start\":18267},{\"end\":19741,\"start\":19386},{\"end\":20062,\"start\":19781},{\"end\":20467,\"start\":20064},{\"end\":20804,\"start\":20469},{\"end\":22007,\"start\":20806},{\"end\":22559,\"start\":22043},{\"end\":22864,\"start\":22561},{\"end\":23265,\"start\":22866},{\"end\":24482,\"start\":23309},{\"end\":25075,\"start\":24524},{\"end\":27186,\"start\":25117},{\"end\":27301,\"start\":27188},{\"end\":28292,\"start\":27345},{\"end\":28784,\"start\":28294},{\"end\":30426,\"start\":28786},{\"end\":31105,\"start\":30439},{\"end\":31888,\"start\":31141},{\"end\":32058,\"start\":31890},{\"end\":32504,\"start\":32060},{\"end\":32988,\"start\":32552},{\"end\":33573,\"start\":32990},{\"end\":35543,\"start\":33614},{\"end\":36222,\"start\":35558},{\"end\":36225,\"start\":36224}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11185,\"start\":11099},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13319,\"start\":13214},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13803,\"start\":13732}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":17074,\"start\":17067},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18419,\"start\":18412},{\"end\":18884,\"start\":18877},{\"end\":20816,\"start\":20809},{\"end\":22504,\"start\":22497},{\"end\":24833,\"start\":24826},{\"end\":25204,\"start\":25197},{\"end\":26674,\"start\":26667},{\"end\":28783,\"start\":28776},{\"end\":31488,\"start\":31481},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":32057,\"start\":32050},{\"end\":32987,\"start\":32980}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1555,\"start\":1543},{\"attributes\":{\"n\":\"2.\"},\"end\":5465,\"start\":5453},{\"attributes\":{\"n\":\"3.\"},\"end\":9236,\"start\":9223},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10253,\"start\":10230},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12586,\"start\":12548},{\"attributes\":{\"n\":\"3.3.\"},\"end\":14097,\"start\":14079},{\"attributes\":{\"n\":\"3.4.\"},\"end\":15257,\"start\":15233},{\"attributes\":{\"n\":\"3.5.\"},\"end\":16290,\"start\":16268},{\"end\":16699,\"start\":16692},{\"attributes\":{\"n\":\"4.\"},\"end\":16923,\"start\":16912},{\"attributes\":{\"n\":\"4.1.\"},\"end\":19779,\"start\":19744},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":22041,\"start\":22010},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":23307,\"start\":23268},{\"attributes\":{\"n\":\"4.1.3\"},\"end\":24522,\"start\":24485},{\"attributes\":{\"n\":\"4.1.4\"},\"end\":25115,\"start\":25078},{\"attributes\":{\"n\":\"4.2.\"},\"end\":27343,\"start\":27304},{\"attributes\":{\"n\":\"5.\"},\"end\":30437,\"start\":30429},{\"attributes\":{\"n\":\"5.1.\"},\"end\":31139,\"start\":31108},{\"attributes\":{\"n\":\"5.2.\"},\"end\":32550,\"start\":32507},{\"attributes\":{\"n\":\"5.3.\"},\"end\":33612,\"start\":33576},{\"attributes\":{\"n\":\"6.\"},\"end\":35556,\"start\":35546},{\"end\":36237,\"start\":36227},{\"end\":36361,\"start\":36351},{\"end\":36636,\"start\":36626},{\"end\":37559,\"start\":37550},{\"end\":38126,\"start\":38117},{\"end\":38478,\"start\":38477}]", "table": "[{\"end\":37548,\"start\":37078},{\"end\":38115,\"start\":37620},{\"end\":38475,\"start\":38128},{\"end\":38688,\"start\":38575}]", "figure_caption": "[{\"end\":36349,\"start\":36239},{\"end\":36624,\"start\":36363},{\"end\":37015,\"start\":36638},{\"end\":37078,\"start\":37018},{\"end\":37620,\"start\":37561},{\"end\":38575,\"start\":38479}]", "figure_ref": "[{\"end\":2468,\"start\":2460},{\"end\":9425,\"start\":9419},{\"end\":9700,\"start\":9692},{\"end\":10427,\"start\":10421},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13599,\"start\":13593},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22370,\"start\":22364},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24008,\"start\":24002},{\"end\":25592,\"start\":25586},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26378,\"start\":26366},{\"end\":34187,\"start\":34179},{\"end\":35180,\"start\":35175}]", "bib_author_first_name": "[{\"end\":38759,\"start\":38753},{\"end\":38778,\"start\":38775},{\"end\":39049,\"start\":39044},{\"end\":39066,\"start\":39062},{\"end\":39084,\"start\":39077},{\"end\":39105,\"start\":39097},{\"end\":39561,\"start\":39555},{\"end\":39576,\"start\":39569},{\"end\":39596,\"start\":39583},{\"end\":39915,\"start\":39912},{\"end\":39929,\"start\":39924},{\"end\":39941,\"start\":39935},{\"end\":40249,\"start\":40246},{\"end\":40262,\"start\":40258},{\"end\":40275,\"start\":40270},{\"end\":40769,\"start\":40764},{\"end\":40784,\"start\":40778},{\"end\":40797,\"start\":40792},{\"end\":41030,\"start\":41025},{\"end\":41049,\"start\":41043},{\"end\":41418,\"start\":41410},{\"end\":41431,\"start\":41426},{\"end\":41450,\"start\":41444},{\"end\":41467,\"start\":41459},{\"end\":41884,\"start\":41876},{\"end\":41897,\"start\":41892},{\"end\":41911,\"start\":41905},{\"end\":41925,\"start\":41920},{\"end\":41938,\"start\":41933},{\"end\":41957,\"start\":41951},{\"end\":42312,\"start\":42307},{\"end\":42326,\"start\":42320},{\"end\":42338,\"start\":42332},{\"end\":42357,\"start\":42351},{\"end\":42374,\"start\":42366},{\"end\":42392,\"start\":42385},{\"end\":42406,\"start\":42400},{\"end\":42417,\"start\":42413},{\"end\":42429,\"start\":42424},{\"end\":42773,\"start\":42772},{\"end\":42787,\"start\":42781},{\"end\":42803,\"start\":42795},{\"end\":42819,\"start\":42816},{\"end\":42834,\"start\":42828},{\"end\":42849,\"start\":42845},{\"end\":42863,\"start\":42857},{\"end\":42875,\"start\":42868},{\"end\":42892,\"start\":42886},{\"end\":42903,\"start\":42900},{\"end\":43300,\"start\":43294},{\"end\":43312,\"start\":43307},{\"end\":43324,\"start\":43318},{\"end\":43338,\"start\":43331},{\"end\":43350,\"start\":43345},{\"end\":43352,\"start\":43351},{\"end\":43363,\"start\":43359},{\"end\":43848,\"start\":43844},{\"end\":43860,\"start\":43855},{\"end\":43880,\"start\":43872},{\"end\":43898,\"start\":43890},{\"end\":44123,\"start\":44117},{\"end\":44135,\"start\":44130},{\"end\":44430,\"start\":44425},{\"end\":44440,\"start\":44437},{\"end\":44454,\"start\":44446},{\"end\":44466,\"start\":44461},{\"end\":44768,\"start\":44760},{\"end\":44782,\"start\":44775},{\"end\":44796,\"start\":44789},{\"end\":44812,\"start\":44805},{\"end\":45074,\"start\":45063},{\"end\":45089,\"start\":45081},{\"end\":45102,\"start\":45096},{\"end\":45554,\"start\":45548},{\"end\":45565,\"start\":45560},{\"end\":45567,\"start\":45566},{\"end\":45582,\"start\":45575},{\"end\":45596,\"start\":45590},{\"end\":45611,\"start\":45605},{\"end\":45632,\"start\":45624},{\"end\":46038,\"start\":46034},{\"end\":46055,\"start\":46048},{\"end\":46069,\"start\":46063},{\"end\":46071,\"start\":46070},{\"end\":46436,\"start\":46435},{\"end\":46447,\"start\":46446},{\"end\":46461,\"start\":46460},{\"end\":46772,\"start\":46766},{\"end\":46793,\"start\":46786},{\"end\":46807,\"start\":46803},{\"end\":46814,\"start\":46808},{\"end\":46835,\"start\":46829},{\"end\":46854,\"start\":46848},{\"end\":47346,\"start\":47339},{\"end\":47364,\"start\":47358},{\"end\":47381,\"start\":47374},{\"end\":47902,\"start\":47894},{\"end\":47911,\"start\":47908},{\"end\":47924,\"start\":47916},{\"end\":47926,\"start\":47925},{\"end\":48407,\"start\":48404},{\"end\":48631,\"start\":48624},{\"end\":48644,\"start\":48641},{\"end\":48660,\"start\":48651},{\"end\":49034,\"start\":49027},{\"end\":49049,\"start\":49043},{\"end\":49065,\"start\":49056},{\"end\":49081,\"start\":49075},{\"end\":49383,\"start\":49377},{\"end\":49400,\"start\":49396},{\"end\":49412,\"start\":49410},{\"end\":49426,\"start\":49419},{\"end\":49436,\"start\":49432},{\"end\":49448,\"start\":49443},{\"end\":49765,\"start\":49760},{\"end\":49780,\"start\":49775},{\"end\":49785,\"start\":49781},{\"end\":49799,\"start\":49794},{\"end\":49818,\"start\":49811},{\"end\":50183,\"start\":50176},{\"end\":50202,\"start\":50194},{\"end\":50216,\"start\":50210},{\"end\":50518,\"start\":50513},{\"end\":50531,\"start\":50526},{\"end\":50548,\"start\":50541},{\"end\":50989,\"start\":50986},{\"end\":51243,\"start\":51235},{\"end\":51258,\"start\":51252},{\"end\":51276,\"start\":51269},{\"end\":51744,\"start\":51736},{\"end\":51761,\"start\":51754},{\"end\":51771,\"start\":51770},{\"end\":51788,\"start\":51787},{\"end\":51803,\"start\":51796},{\"end\":52283,\"start\":52279},{\"end\":52298,\"start\":52293},{\"end\":52311,\"start\":52307},{\"end\":52725,\"start\":52720},{\"end\":52739,\"start\":52735},{\"end\":53134,\"start\":53127},{\"end\":53144,\"start\":53139},{\"end\":53155,\"start\":53150},{\"end\":53167,\"start\":53160},{\"end\":53177,\"start\":53173},{\"end\":53393,\"start\":53392},{\"end\":53406,\"start\":53403},{\"end\":53419,\"start\":53415},{\"end\":53442,\"start\":53437},{\"end\":53838,\"start\":53833},{\"end\":53857,\"start\":53851},{\"end\":53876,\"start\":53868},{\"end\":53891,\"start\":53886},{\"end\":53905,\"start\":53901},{\"end\":54625,\"start\":54616},{\"end\":54638,\"start\":54631},{\"end\":54996,\"start\":54991},{\"end\":55010,\"start\":55004},{\"end\":55024,\"start\":55018},{\"end\":55026,\"start\":55025},{\"end\":55334,\"start\":55327},{\"end\":55349,\"start\":55343},{\"end\":55367,\"start\":55359},{\"end\":55381,\"start\":55373},{\"end\":55383,\"start\":55382},{\"end\":55397,\"start\":55392},{\"end\":55409,\"start\":55405},{\"end\":55424,\"start\":55418},{\"end\":55831,\"start\":55829},{\"end\":55837,\"start\":55836},{\"end\":55850,\"start\":55844},{\"end\":56282,\"start\":56274},{\"end\":56296,\"start\":56289},{\"end\":56307,\"start\":56303},{\"end\":56321,\"start\":56315},{\"end\":56332,\"start\":56326},{\"end\":56624,\"start\":56619},{\"end\":56639,\"start\":56633},{\"end\":57054,\"start\":57045},{\"end\":57072,\"start\":57067},{\"end\":57087,\"start\":57080},{\"end\":57098,\"start\":57094},{\"end\":57118,\"start\":57111},{\"end\":57132,\"start\":57125},{\"end\":57144,\"start\":57140},{\"end\":57469,\"start\":57460},{\"end\":57489,\"start\":57482},{\"end\":57501,\"start\":57496},{\"end\":57768,\"start\":57764},{\"end\":57778,\"start\":57773},{\"end\":57789,\"start\":57787},{\"end\":57803,\"start\":57797},{\"end\":57818,\"start\":57810},{\"end\":58275,\"start\":58269},{\"end\":58281,\"start\":58280},{\"end\":58294,\"start\":58287},{\"end\":58713,\"start\":58706},{\"end\":58721,\"start\":58718},{\"end\":58734,\"start\":58726},{\"end\":58743,\"start\":58740},{\"end\":58754,\"start\":58748},{\"end\":58766,\"start\":58759},{\"end\":59077,\"start\":59073},{\"end\":59095,\"start\":59090},{\"end\":59367,\"start\":59362},{\"end\":59381,\"start\":59377},{\"end\":59399,\"start\":59392},{\"end\":59419,\"start\":59412},{\"end\":59431,\"start\":59424},{\"end\":59446,\"start\":59440},{\"end\":59738,\"start\":59737},{\"end\":59747,\"start\":59746},{\"end\":59756,\"start\":59755},{\"end\":59765,\"start\":59764},{\"end\":60023,\"start\":60017},{\"end\":60043,\"start\":60034},{\"end\":60465,\"start\":60464},{\"end\":60478,\"start\":60477},{\"end\":60489,\"start\":60488},{\"end\":60924,\"start\":60919},{\"end\":60939,\"start\":60932},{\"end\":61199,\"start\":61194},{\"end\":61213,\"start\":61209},{\"end\":61232,\"start\":61227},{\"end\":61461,\"start\":61456},{\"end\":61498,\"start\":61495},{\"end\":61906,\"start\":61901},{\"end\":61921,\"start\":61916},{\"end\":62328,\"start\":62327},{\"end\":62330,\"start\":62329},{\"end\":62343,\"start\":62342},{\"end\":62345,\"start\":62344},{\"end\":62598,\"start\":62593},{\"end\":62618,\"start\":62613},{\"end\":62628,\"start\":62623},{\"end\":62893,\"start\":62886},{\"end\":62907,\"start\":62903},{\"end\":62909,\"start\":62908},{\"end\":62921,\"start\":62917},{\"end\":62929,\"start\":62928},{\"end\":62944,\"start\":62936},{\"end\":62962,\"start\":62956},{\"end\":63370,\"start\":63360},{\"end\":63382,\"start\":63377},{\"end\":63406,\"start\":63401},{\"end\":63419,\"start\":63412},{\"end\":63959,\"start\":63957},{\"end\":63981,\"start\":63974},{\"end\":63998,\"start\":63990},{\"end\":64000,\"start\":63999},{\"end\":64264,\"start\":64261},{\"end\":64286,\"start\":64279},{\"end\":64295,\"start\":64292},{\"end\":64308,\"start\":64300},{\"end\":64310,\"start\":64309},{\"end\":64778,\"start\":64775},{\"end\":64800,\"start\":64793},{\"end\":64813,\"start\":64805},{\"end\":64815,\"start\":64814},{\"end\":65281,\"start\":65278},{\"end\":65304,\"start\":65296},{\"end\":65315,\"start\":65309},{\"end\":65333,\"start\":65325},{\"end\":65347,\"start\":65339},{\"end\":65349,\"start\":65348},{\"end\":65852,\"start\":65850},{\"end\":65880,\"start\":65877},{\"end\":65893,\"start\":65885},{\"end\":65895,\"start\":65894},{\"end\":66284,\"start\":66277},{\"end\":66297,\"start\":66291},{\"end\":66312,\"start\":66307},{\"end\":66324,\"start\":66320},{\"end\":66596,\"start\":66590},{\"end\":66609,\"start\":66606},{\"end\":66631,\"start\":66624},{\"end\":67114,\"start\":67108},{\"end\":67125,\"start\":67120},{\"end\":67141,\"start\":67135},{\"end\":67160,\"start\":67155},{\"end\":67177,\"start\":67170},{\"end\":67633,\"start\":67632},{\"end\":67650,\"start\":67649},{\"end\":68006,\"start\":67998},{\"end\":68021,\"start\":68015},{\"end\":68356,\"start\":68349},{\"end\":68365,\"start\":68364},{\"end\":68376,\"start\":68373},{\"end\":68391,\"start\":68384},{\"end\":68410,\"start\":68402},{\"end\":68744,\"start\":68735},{\"end\":68756,\"start\":68750},{\"end\":68764,\"start\":68762},{\"end\":68775,\"start\":68772},{\"end\":68790,\"start\":68782},{\"end\":68804,\"start\":68796},{\"end\":68820,\"start\":68811},{\"end\":69335,\"start\":69326},{\"end\":69349,\"start\":69341},{\"end\":69365,\"start\":69356},{\"end\":69852,\"start\":69843},{\"end\":69861,\"start\":69858},{\"end\":69876,\"start\":69868},{\"end\":69890,\"start\":69882},{\"end\":69906,\"start\":69897},{\"end\":70226,\"start\":70220},{\"end\":70243,\"start\":70238},{\"end\":70259,\"start\":70251},{\"end\":70270,\"start\":70267},{\"end\":70600,\"start\":70594},{\"end\":70887,\"start\":70881},{\"end\":70895,\"start\":70894},{\"end\":70913,\"start\":70904},{\"end\":71363,\"start\":71357},{\"end\":71379,\"start\":71370},{\"end\":71757,\"start\":71752},{\"end\":71899,\"start\":71893},{\"end\":71914,\"start\":71907},{\"end\":71931,\"start\":71926},{\"end\":71947,\"start\":71940},{\"end\":71963,\"start\":71957},{\"end\":72363,\"start\":72359},{\"end\":72373,\"start\":72368},{\"end\":72389,\"start\":72383},{\"end\":72404,\"start\":72395},{\"end\":72420,\"start\":72411},{\"end\":72444,\"start\":72434},{\"end\":72454,\"start\":72451},{\"end\":72999,\"start\":72995},{\"end\":73013,\"start\":73004},{\"end\":73449,\"start\":73445},{\"end\":73462,\"start\":73455},{\"end\":73483,\"start\":73476},{\"end\":73498,\"start\":73491},{\"end\":73730,\"start\":73727},{\"end\":73742,\"start\":73736},{\"end\":73762,\"start\":73756},{\"end\":73782,\"start\":73774},{\"end\":73800,\"start\":73792},{\"end\":73814,\"start\":73810},{\"end\":73826,\"start\":73821},{\"end\":73835,\"start\":73832},{\"end\":73848,\"start\":73842},{\"end\":73863,\"start\":73855},{\"end\":74646,\"start\":74637},{\"end\":74659,\"start\":74656},{\"end\":74673,\"start\":74665},{\"end\":74685,\"start\":74679},{\"end\":74701,\"start\":74696},{\"end\":74716,\"start\":74708},{\"end\":74734,\"start\":74727},{\"end\":74749,\"start\":74742},{\"end\":74767,\"start\":74761},{\"end\":75204,\"start\":75199},{\"end\":75224,\"start\":75218},{\"end\":75784,\"start\":75779},{\"end\":75804,\"start\":75798},{\"end\":75818,\"start\":75811},{\"end\":75834,\"start\":75827},{\"end\":76259,\"start\":76255},{\"end\":76280,\"start\":76269},{\"end\":76290,\"start\":76287},{\"end\":76307,\"start\":76299},{\"end\":76320,\"start\":76314},{\"end\":76608,\"start\":76600},{\"end\":76620,\"start\":76615},{\"end\":76638,\"start\":76631},{\"end\":76887,\"start\":76879},{\"end\":76898,\"start\":76894},{\"end\":76907,\"start\":76904},{\"end\":76920,\"start\":76915},{\"end\":76939,\"start\":76931},{\"end\":76955,\"start\":76948},{\"end\":77244,\"start\":77243},{\"end\":77255,\"start\":77254},{\"end\":77269,\"start\":77268},{\"end\":77282,\"start\":77278},{\"end\":77512,\"start\":77505},{\"end\":77524,\"start\":77519},{\"end\":77538,\"start\":77532},{\"end\":77549,\"start\":77543},{\"end\":77557,\"start\":77554},{\"end\":77570,\"start\":77563},{\"end\":77994,\"start\":77986},{\"end\":78008,\"start\":78001},{\"end\":78390,\"start\":78383},{\"end\":78400,\"start\":78397},{\"end\":78414,\"start\":78406},{\"end\":78428,\"start\":78421},{\"end\":78440,\"start\":78435},{\"end\":78908,\"start\":78905},{\"end\":78921,\"start\":78915},{\"end\":78923,\"start\":78922},{\"end\":79265,\"start\":79262},{\"end\":79279,\"start\":79272},{\"end\":79290,\"start\":79285},{\"end\":79297,\"start\":79296},{\"end\":79319,\"start\":79313},{\"end\":79321,\"start\":79320},{\"end\":79677,\"start\":79670},{\"end\":79690,\"start\":79682},{\"end\":79697,\"start\":79695},{\"end\":80118,\"start\":80111},{\"end\":80129,\"start\":80123},{\"end\":80142,\"start\":80136},{\"end\":80157,\"start\":80151},{\"end\":80170,\"start\":80162},{\"end\":80184,\"start\":80178},{\"end\":80200,\"start\":80191},{\"end\":80667,\"start\":80660},{\"end\":80679,\"start\":80672},{\"end\":80688,\"start\":80687},{\"end\":80702,\"start\":80697},{\"end\":81163,\"start\":81154},{\"end\":81176,\"start\":81170},{\"end\":81191,\"start\":81184},{\"end\":81628,\"start\":81621},{\"end\":81640,\"start\":81634},{\"end\":81649,\"start\":81645},{\"end\":81663,\"start\":81655},{\"end\":81665,\"start\":81664},{\"end\":81682,\"start\":81680},{\"end\":82119,\"start\":82112},{\"end\":82131,\"start\":82125},{\"end\":82141,\"start\":82137},{\"end\":82155,\"start\":82148},{\"end\":82604,\"start\":82599},{\"end\":82615,\"start\":82609},{\"end\":82627,\"start\":82621},{\"end\":82636,\"start\":82632},{\"end\":82645,\"start\":82643},{\"end\":83099,\"start\":83097},{\"end\":83111,\"start\":83105},{\"end\":83123,\"start\":83119},{\"end\":83133,\"start\":83128},{\"end\":83148,\"start\":83140},{\"end\":83627,\"start\":83624},{\"end\":83639,\"start\":83633},{\"end\":83647,\"start\":83645},{\"end\":83852,\"start\":83845},{\"end\":83863,\"start\":83859},{\"end\":83874,\"start\":83870},{\"end\":83885,\"start\":83881},{\"end\":84323,\"start\":84317},{\"end\":84335,\"start\":84330},{\"end\":84344,\"start\":84341},{\"end\":84358,\"start\":84350},{\"end\":84370,\"start\":84365},{\"end\":84804,\"start\":84802},{\"end\":84813,\"start\":84809},{\"end\":84822,\"start\":84820},{\"end\":84836,\"start\":84829},{\"end\":84851,\"start\":84843},{\"end\":84853,\"start\":84852},{\"end\":85297,\"start\":85291},{\"end\":85313,\"start\":85305},{\"end\":85327,\"start\":85321},{\"end\":85339,\"start\":85332},{\"end\":85352,\"start\":85346},{\"end\":85780,\"start\":85774},{\"end\":85790,\"start\":85788},{\"end\":85802,\"start\":85796},{\"end\":85815,\"start\":85809},{\"end\":86090,\"start\":86084},{\"end\":86105,\"start\":86098},{\"end\":86121,\"start\":86112},{\"end\":86132,\"start\":86126},{\"end\":86147,\"start\":86138},{\"end\":86161,\"start\":86154},{\"end\":86175,\"start\":86169},{\"end\":86531,\"start\":86528},{\"end\":86543,\"start\":86538}]", "bib_author_last_name": "[{\"end\":38773,\"start\":38760},{\"end\":38784,\"start\":38779},{\"end\":38793,\"start\":38786},{\"end\":39060,\"start\":39050},{\"end\":39075,\"start\":39067},{\"end\":39095,\"start\":39085},{\"end\":39112,\"start\":39106},{\"end\":39567,\"start\":39562},{\"end\":39581,\"start\":39577},{\"end\":39602,\"start\":39597},{\"end\":39922,\"start\":39916},{\"end\":39933,\"start\":39930},{\"end\":39959,\"start\":39942},{\"end\":39969,\"start\":39961},{\"end\":40256,\"start\":40250},{\"end\":40268,\"start\":40263},{\"end\":40288,\"start\":40276},{\"end\":40295,\"start\":40290},{\"end\":40776,\"start\":40770},{\"end\":40790,\"start\":40785},{\"end\":40804,\"start\":40798},{\"end\":41041,\"start\":41031},{\"end\":41056,\"start\":41050},{\"end\":41424,\"start\":41419},{\"end\":41442,\"start\":41432},{\"end\":41457,\"start\":41451},{\"end\":41473,\"start\":41468},{\"end\":41890,\"start\":41885},{\"end\":41903,\"start\":41898},{\"end\":41918,\"start\":41912},{\"end\":41931,\"start\":41926},{\"end\":41949,\"start\":41939},{\"end\":41964,\"start\":41958},{\"end\":42318,\"start\":42313},{\"end\":42330,\"start\":42327},{\"end\":42349,\"start\":42339},{\"end\":42364,\"start\":42358},{\"end\":42383,\"start\":42375},{\"end\":42398,\"start\":42393},{\"end\":42411,\"start\":42407},{\"end\":42422,\"start\":42418},{\"end\":42435,\"start\":42430},{\"end\":42779,\"start\":42774},{\"end\":42793,\"start\":42788},{\"end\":42814,\"start\":42804},{\"end\":42826,\"start\":42820},{\"end\":42843,\"start\":42835},{\"end\":42855,\"start\":42850},{\"end\":42866,\"start\":42864},{\"end\":42884,\"start\":42876},{\"end\":42898,\"start\":42893},{\"end\":42908,\"start\":42904},{\"end\":42912,\"start\":42910},{\"end\":43305,\"start\":43301},{\"end\":43316,\"start\":43313},{\"end\":43329,\"start\":43325},{\"end\":43343,\"start\":43339},{\"end\":43357,\"start\":43353},{\"end\":43366,\"start\":43364},{\"end\":43853,\"start\":43849},{\"end\":43870,\"start\":43861},{\"end\":43888,\"start\":43881},{\"end\":43905,\"start\":43899},{\"end\":44128,\"start\":44124},{\"end\":44139,\"start\":44136},{\"end\":44435,\"start\":44431},{\"end\":44444,\"start\":44441},{\"end\":44459,\"start\":44455},{\"end\":44470,\"start\":44467},{\"end\":44773,\"start\":44769},{\"end\":44787,\"start\":44783},{\"end\":44803,\"start\":44797},{\"end\":44819,\"start\":44813},{\"end\":45079,\"start\":45075},{\"end\":45094,\"start\":45090},{\"end\":45111,\"start\":45103},{\"end\":45558,\"start\":45555},{\"end\":45573,\"start\":45568},{\"end\":45588,\"start\":45583},{\"end\":45603,\"start\":45597},{\"end\":45622,\"start\":45612},{\"end\":45641,\"start\":45633},{\"end\":46046,\"start\":46039},{\"end\":46061,\"start\":46056},{\"end\":46077,\"start\":46072},{\"end\":46444,\"start\":46437},{\"end\":46458,\"start\":46448},{\"end\":46469,\"start\":46462},{\"end\":46784,\"start\":46773},{\"end\":46801,\"start\":46794},{\"end\":46827,\"start\":46815},{\"end\":46846,\"start\":46836},{\"end\":46859,\"start\":46855},{\"end\":47356,\"start\":47347},{\"end\":47372,\"start\":47365},{\"end\":47387,\"start\":47382},{\"end\":47906,\"start\":47903},{\"end\":47914,\"start\":47912},{\"end\":47933,\"start\":47927},{\"end\":48416,\"start\":48408},{\"end\":48639,\"start\":48632},{\"end\":48649,\"start\":48645},{\"end\":48665,\"start\":48661},{\"end\":49041,\"start\":49035},{\"end\":49054,\"start\":49050},{\"end\":49073,\"start\":49066},{\"end\":49089,\"start\":49082},{\"end\":49394,\"start\":49384},{\"end\":49408,\"start\":49401},{\"end\":49417,\"start\":49413},{\"end\":49430,\"start\":49427},{\"end\":49441,\"start\":49437},{\"end\":49456,\"start\":49449},{\"end\":49773,\"start\":49766},{\"end\":49792,\"start\":49786},{\"end\":49809,\"start\":49800},{\"end\":49824,\"start\":49819},{\"end\":50192,\"start\":50184},{\"end\":50208,\"start\":50203},{\"end\":50224,\"start\":50217},{\"end\":50524,\"start\":50519},{\"end\":50539,\"start\":50532},{\"end\":50554,\"start\":50549},{\"end\":50701,\"start\":50696},{\"end\":50996,\"start\":50990},{\"end\":51250,\"start\":51244},{\"end\":51267,\"start\":51259},{\"end\":51291,\"start\":51277},{\"end\":51752,\"start\":51745},{\"end\":51768,\"start\":51762},{\"end\":51780,\"start\":51772},{\"end\":51785,\"start\":51782},{\"end\":51794,\"start\":51789},{\"end\":51811,\"start\":51804},{\"end\":51818,\"start\":51813},{\"end\":52291,\"start\":52284},{\"end\":52305,\"start\":52299},{\"end\":52317,\"start\":52312},{\"end\":52733,\"start\":52726},{\"end\":52745,\"start\":52740},{\"end\":53137,\"start\":53135},{\"end\":53148,\"start\":53145},{\"end\":53158,\"start\":53156},{\"end\":53171,\"start\":53168},{\"end\":53186,\"start\":53178},{\"end\":53401,\"start\":53394},{\"end\":53413,\"start\":53407},{\"end\":53426,\"start\":53420},{\"end\":53435,\"start\":53428},{\"end\":53452,\"start\":53443},{\"end\":53466,\"start\":53454},{\"end\":53849,\"start\":53839},{\"end\":53866,\"start\":53858},{\"end\":53884,\"start\":53877},{\"end\":53899,\"start\":53892},{\"end\":53914,\"start\":53906},{\"end\":54614,\"start\":54606},{\"end\":54629,\"start\":54626},{\"end\":54643,\"start\":54639},{\"end\":54650,\"start\":54645},{\"end\":55002,\"start\":54997},{\"end\":55016,\"start\":55011},{\"end\":55032,\"start\":55027},{\"end\":55341,\"start\":55335},{\"end\":55357,\"start\":55350},{\"end\":55371,\"start\":55368},{\"end\":55390,\"start\":55384},{\"end\":55403,\"start\":55398},{\"end\":55416,\"start\":55410},{\"end\":55432,\"start\":55425},{\"end\":55834,\"start\":55832},{\"end\":55842,\"start\":55838},{\"end\":55860,\"start\":55851},{\"end\":55869,\"start\":55862},{\"end\":56287,\"start\":56283},{\"end\":56301,\"start\":56297},{\"end\":56313,\"start\":56308},{\"end\":56324,\"start\":56322},{\"end\":56337,\"start\":56333},{\"end\":56631,\"start\":56625},{\"end\":56649,\"start\":56640},{\"end\":57065,\"start\":57055},{\"end\":57078,\"start\":57073},{\"end\":57092,\"start\":57088},{\"end\":57109,\"start\":57099},{\"end\":57123,\"start\":57119},{\"end\":57138,\"start\":57133},{\"end\":57152,\"start\":57145},{\"end\":57480,\"start\":57470},{\"end\":57494,\"start\":57490},{\"end\":57507,\"start\":57502},{\"end\":57771,\"start\":57769},{\"end\":57785,\"start\":57779},{\"end\":57795,\"start\":57790},{\"end\":57808,\"start\":57804},{\"end\":57823,\"start\":57819},{\"end\":58278,\"start\":58276},{\"end\":58285,\"start\":58282},{\"end\":58299,\"start\":58295},{\"end\":58304,\"start\":58301},{\"end\":58716,\"start\":58714},{\"end\":58724,\"start\":58722},{\"end\":58738,\"start\":58735},{\"end\":58746,\"start\":58744},{\"end\":58757,\"start\":58755},{\"end\":58771,\"start\":58767},{\"end\":59088,\"start\":59078},{\"end\":59102,\"start\":59096},{\"end\":59375,\"start\":59368},{\"end\":59390,\"start\":59382},{\"end\":59410,\"start\":59400},{\"end\":59422,\"start\":59420},{\"end\":59438,\"start\":59432},{\"end\":59449,\"start\":59447},{\"end\":59744,\"start\":59739},{\"end\":59753,\"start\":59748},{\"end\":59762,\"start\":59757},{\"end\":59777,\"start\":59766},{\"end\":60032,\"start\":60024},{\"end\":60051,\"start\":60044},{\"end\":60475,\"start\":60466},{\"end\":60486,\"start\":60479},{\"end\":60496,\"start\":60490},{\"end\":60930,\"start\":60925},{\"end\":60954,\"start\":60940},{\"end\":61207,\"start\":61200},{\"end\":61225,\"start\":61214},{\"end\":61238,\"start\":61233},{\"end\":61493,\"start\":61462},{\"end\":61504,\"start\":61499},{\"end\":61512,\"start\":61506},{\"end\":61914,\"start\":61907},{\"end\":61928,\"start\":61922},{\"end\":62340,\"start\":62331},{\"end\":62351,\"start\":62346},{\"end\":62611,\"start\":62599},{\"end\":62621,\"start\":62619},{\"end\":62636,\"start\":62629},{\"end\":62901,\"start\":62894},{\"end\":62915,\"start\":62910},{\"end\":62926,\"start\":62922},{\"end\":62934,\"start\":62930},{\"end\":62954,\"start\":62945},{\"end\":62968,\"start\":62963},{\"end\":62977,\"start\":62970},{\"end\":63375,\"start\":63371},{\"end\":63389,\"start\":63383},{\"end\":63399,\"start\":63391},{\"end\":63410,\"start\":63407},{\"end\":63424,\"start\":63420},{\"end\":63431,\"start\":63426},{\"end\":63972,\"start\":63960},{\"end\":63988,\"start\":63982},{\"end\":64003,\"start\":64001},{\"end\":64011,\"start\":64005},{\"end\":64277,\"start\":64265},{\"end\":64290,\"start\":64287},{\"end\":64298,\"start\":64296},{\"end\":64313,\"start\":64311},{\"end\":64321,\"start\":64315},{\"end\":64791,\"start\":64779},{\"end\":64803,\"start\":64801},{\"end\":64818,\"start\":64816},{\"end\":64826,\"start\":64820},{\"end\":65294,\"start\":65282},{\"end\":65307,\"start\":65305},{\"end\":65323,\"start\":65316},{\"end\":65337,\"start\":65334},{\"end\":65353,\"start\":65350},{\"end\":65361,\"start\":65355},{\"end\":65875,\"start\":65853},{\"end\":65883,\"start\":65881},{\"end\":65898,\"start\":65896},{\"end\":65906,\"start\":65900},{\"end\":66289,\"start\":66285},{\"end\":66305,\"start\":66298},{\"end\":66318,\"start\":66313},{\"end\":66332,\"start\":66325},{\"end\":66339,\"start\":66334},{\"end\":66604,\"start\":66597},{\"end\":66622,\"start\":66610},{\"end\":66638,\"start\":66632},{\"end\":67118,\"start\":67115},{\"end\":67133,\"start\":67126},{\"end\":67153,\"start\":67142},{\"end\":67168,\"start\":67161},{\"end\":67183,\"start\":67178},{\"end\":67647,\"start\":67634},{\"end\":67657,\"start\":67651},{\"end\":67826,\"start\":67819},{\"end\":68013,\"start\":68007},{\"end\":68029,\"start\":68022},{\"end\":68362,\"start\":68357},{\"end\":68371,\"start\":68366},{\"end\":68382,\"start\":68377},{\"end\":68400,\"start\":68392},{\"end\":68417,\"start\":68411},{\"end\":68426,\"start\":68419},{\"end\":68748,\"start\":68745},{\"end\":68760,\"start\":68757},{\"end\":68770,\"start\":68765},{\"end\":68780,\"start\":68776},{\"end\":68794,\"start\":68791},{\"end\":68809,\"start\":68805},{\"end\":68823,\"start\":68821},{\"end\":69339,\"start\":69336},{\"end\":69354,\"start\":69350},{\"end\":69368,\"start\":69366},{\"end\":69856,\"start\":69853},{\"end\":69866,\"start\":69862},{\"end\":69880,\"start\":69877},{\"end\":69895,\"start\":69891},{\"end\":69909,\"start\":69907},{\"end\":70236,\"start\":70227},{\"end\":70249,\"start\":70244},{\"end\":70265,\"start\":70260},{\"end\":70277,\"start\":70271},{\"end\":70605,\"start\":70601},{\"end\":70892,\"start\":70888},{\"end\":70902,\"start\":70896},{\"end\":70925,\"start\":70914},{\"end\":70931,\"start\":70927},{\"end\":71368,\"start\":71364},{\"end\":71384,\"start\":71380},{\"end\":71763,\"start\":71758},{\"end\":71905,\"start\":71900},{\"end\":71924,\"start\":71915},{\"end\":71938,\"start\":71932},{\"end\":71955,\"start\":71948},{\"end\":71971,\"start\":71964},{\"end\":72366,\"start\":72364},{\"end\":72381,\"start\":72374},{\"end\":72393,\"start\":72390},{\"end\":72409,\"start\":72405},{\"end\":72432,\"start\":72421},{\"end\":72449,\"start\":72445},{\"end\":72460,\"start\":72455},{\"end\":73002,\"start\":73000},{\"end\":73018,\"start\":73014},{\"end\":73453,\"start\":73450},{\"end\":73474,\"start\":73463},{\"end\":73489,\"start\":73484},{\"end\":73504,\"start\":73499},{\"end\":73734,\"start\":73731},{\"end\":73754,\"start\":73743},{\"end\":73772,\"start\":73763},{\"end\":73790,\"start\":73783},{\"end\":73808,\"start\":73801},{\"end\":73819,\"start\":73815},{\"end\":73830,\"start\":73827},{\"end\":73840,\"start\":73836},{\"end\":73853,\"start\":73849},{\"end\":73869,\"start\":73864},{\"end\":74654,\"start\":74647},{\"end\":74663,\"start\":74660},{\"end\":74677,\"start\":74674},{\"end\":74694,\"start\":74686},{\"end\":74706,\"start\":74702},{\"end\":74725,\"start\":74717},{\"end\":74740,\"start\":74735},{\"end\":74759,\"start\":74750},{\"end\":74778,\"start\":74768},{\"end\":75216,\"start\":75205},{\"end\":75236,\"start\":75225},{\"end\":75407,\"start\":75403},{\"end\":75796,\"start\":75785},{\"end\":75809,\"start\":75805},{\"end\":75825,\"start\":75819},{\"end\":75839,\"start\":75835},{\"end\":76267,\"start\":76260},{\"end\":76285,\"start\":76281},{\"end\":76297,\"start\":76291},{\"end\":76312,\"start\":76308},{\"end\":76329,\"start\":76321},{\"end\":76613,\"start\":76609},{\"end\":76629,\"start\":76621},{\"end\":76644,\"start\":76639},{\"end\":76892,\"start\":76888},{\"end\":76902,\"start\":76899},{\"end\":76913,\"start\":76908},{\"end\":76929,\"start\":76921},{\"end\":76946,\"start\":76940},{\"end\":76961,\"start\":76956},{\"end\":77252,\"start\":77245},{\"end\":77266,\"start\":77256},{\"end\":77276,\"start\":77270},{\"end\":77291,\"start\":77283},{\"end\":77517,\"start\":77513},{\"end\":77530,\"start\":77525},{\"end\":77541,\"start\":77539},{\"end\":77552,\"start\":77550},{\"end\":77561,\"start\":77558},{\"end\":77576,\"start\":77571},{\"end\":77999,\"start\":77995},{\"end\":78014,\"start\":78009},{\"end\":78395,\"start\":78391},{\"end\":78404,\"start\":78401},{\"end\":78419,\"start\":78415},{\"end\":78433,\"start\":78429},{\"end\":78444,\"start\":78441},{\"end\":78913,\"start\":78909},{\"end\":78931,\"start\":78924},{\"end\":79270,\"start\":79266},{\"end\":79283,\"start\":79280},{\"end\":79294,\"start\":79291},{\"end\":79304,\"start\":79298},{\"end\":79311,\"start\":79306},{\"end\":79341,\"start\":79322},{\"end\":79350,\"start\":79343},{\"end\":79680,\"start\":79678},{\"end\":79693,\"start\":79691},{\"end\":79703,\"start\":79698},{\"end\":80121,\"start\":80119},{\"end\":80134,\"start\":80130},{\"end\":80149,\"start\":80143},{\"end\":80160,\"start\":80158},{\"end\":80176,\"start\":80171},{\"end\":80189,\"start\":80185},{\"end\":80205,\"start\":80201},{\"end\":80670,\"start\":80668},{\"end\":80685,\"start\":80680},{\"end\":80695,\"start\":80689},{\"end\":80705,\"start\":80703},{\"end\":80710,\"start\":80707},{\"end\":81168,\"start\":81164},{\"end\":81182,\"start\":81177},{\"end\":81200,\"start\":81192},{\"end\":81632,\"start\":81629},{\"end\":81643,\"start\":81641},{\"end\":81653,\"start\":81650},{\"end\":81678,\"start\":81666},{\"end\":81689,\"start\":81683},{\"end\":81697,\"start\":81691},{\"end\":82123,\"start\":82120},{\"end\":82135,\"start\":82132},{\"end\":82146,\"start\":82142},{\"end\":82158,\"start\":82156},{\"end\":82607,\"start\":82605},{\"end\":82619,\"start\":82616},{\"end\":82630,\"start\":82628},{\"end\":82641,\"start\":82637},{\"end\":82650,\"start\":82646},{\"end\":83103,\"start\":83100},{\"end\":83117,\"start\":83112},{\"end\":83126,\"start\":83124},{\"end\":83138,\"start\":83134},{\"end\":83152,\"start\":83149},{\"end\":83631,\"start\":83628},{\"end\":83643,\"start\":83640},{\"end\":83650,\"start\":83648},{\"end\":83857,\"start\":83853},{\"end\":83868,\"start\":83864},{\"end\":83879,\"start\":83875},{\"end\":83890,\"start\":83886},{\"end\":84328,\"start\":84324},{\"end\":84339,\"start\":84336},{\"end\":84348,\"start\":84345},{\"end\":84363,\"start\":84359},{\"end\":84374,\"start\":84371},{\"end\":84807,\"start\":84805},{\"end\":84818,\"start\":84814},{\"end\":84827,\"start\":84823},{\"end\":84841,\"start\":84837},{\"end\":84860,\"start\":84854},{\"end\":85303,\"start\":85298},{\"end\":85319,\"start\":85314},{\"end\":85330,\"start\":85328},{\"end\":85344,\"start\":85340},{\"end\":85358,\"start\":85353},{\"end\":85786,\"start\":85781},{\"end\":85794,\"start\":85791},{\"end\":85807,\"start\":85803},{\"end\":85821,\"start\":85816},{\"end\":86096,\"start\":86091},{\"end\":86110,\"start\":86106},{\"end\":86124,\"start\":86122},{\"end\":86136,\"start\":86133},{\"end\":86152,\"start\":86148},{\"end\":86167,\"start\":86162},{\"end\":86181,\"start\":86176},{\"end\":86536,\"start\":86532},{\"end\":86549,\"start\":86544}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2003.12641\",\"id\":\"b0\"},\"end\":38974,\"start\":38690},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":23102425},\"end\":39488,\"start\":38976},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2772063},\"end\":39852,\"start\":39490},{\"attributes\":{\"doi\":\"abs/1702.01105\",\"id\":\"b3\"},\"end\":40139,\"start\":39854},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":9649070},\"end\":40702,\"start\":40141},{\"attributes\":{\"doi\":\"arXiv:1803.10091\",\"id\":\"b5\"},\"end\":40980,\"start\":40704},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2168245},\"end\":41346,\"start\":40982},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":49865868},\"end\":41797,\"start\":41348},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":219721240},\"end\":42165,\"start\":41799},{\"attributes\":{\"id\":\"b9\"},\"end\":42770,\"start\":42167},{\"attributes\":{\"doi\":\"arXiv:1512.03012\",\"id\":\"b10\"},\"end\":43222,\"start\":42772},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":219964818},\"end\":43771,\"start\":43224},{\"attributes\":{\"doi\":\"arXiv:2002.05709\",\"id\":\"b12\"},\"end\":44115,\"start\":43773},{\"attributes\":{\"doi\":\"arXiv:2003.04297\",\"id\":\"b13\"},\"end\":44405,\"start\":44117},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":199501855},\"end\":44758,\"start\":44407},{\"attributes\":{\"doi\":\"arXiv:1602.02481\",\"id\":\"b15\"},\"end\":44991,\"start\":44760},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":121123422},\"end\":45483,\"start\":44993},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":7684883},\"end\":45965,\"start\":45485},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":9062671},\"end\":46403,\"start\":45967},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":84591},\"end\":46674,\"start\":46405},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":12881926},\"end\":47226,\"start\":46676},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":214727628},\"end\":47811,\"start\":47228},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6746759},\"end\":48304,\"start\":47813},{\"attributes\":{\"id\":\"b23\"},\"end\":48561,\"start\":48306},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":49666173},\"end\":48983,\"start\":48563},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":9455111},\"end\":49303,\"start\":48985},{\"attributes\":{\"doi\":\"arXiv:1905.00561\",\"id\":\"b26\"},\"end\":49685,\"start\":49305},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":6598262},\"end\":50162,\"start\":49687},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":174802699},\"end\":50509,\"start\":50164},{\"attributes\":{\"id\":\"b29\"},\"end\":50621,\"start\":50511},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":145048710},\"end\":50984,\"start\":50623},{\"attributes\":{\"doi\":\"arXiv:1505.02890\",\"id\":\"b31\"},\"end\":51160,\"start\":50986},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":10154243},\"end\":51675,\"start\":51162},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":3656527},\"end\":52218,\"start\":51677},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":8281592},\"end\":52660,\"start\":52220},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":204788552},\"end\":53058,\"start\":52662},{\"attributes\":{\"doi\":\"arXiv:1911.05722\",\"id\":\"b36\"},\"end\":53390,\"start\":53060},{\"attributes\":{\"doi\":\"arXiv:1905.09272\",\"id\":\"b37\"},\"end\":53755,\"start\":53392},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":46940349},\"end\":54173,\"start\":53757},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":11245438},\"end\":54563,\"start\":54175},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":4445385},\"end\":54989,\"start\":54565},{\"attributes\":{\"doi\":\"arXiv:2006.14613\",\"id\":\"b41\"},\"end\":55261,\"start\":54991},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":5206691},\"end\":55738,\"start\":55263},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":85517234},\"end\":56213,\"start\":55740},{\"attributes\":{\"doi\":\"arXiv:2005.14169\",\"id\":\"b44\"},\"end\":56535,\"start\":56215},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":9597281},\"end\":56983,\"start\":56537},{\"attributes\":{\"doi\":\"arXiv:1912.11370\",\"id\":\"b46\"},\"end\":57399,\"start\":56985},{\"attributes\":{\"doi\":\"arXiv:1901.09005\",\"id\":\"b47\"},\"end\":57689,\"start\":57401},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":85517712},\"end\":58210,\"start\":57691},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":3934562},\"end\":58658,\"start\":58212},{\"attributes\":{\"id\":\"b50\"},\"end\":59017,\"start\":58660},{\"attributes\":{\"doi\":\"arXiv:1608.03983\",\"id\":\"b51\"},\"end\":59260,\"start\":59019},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":13751202},\"end\":59662,\"start\":59262},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":12640199},\"end\":59939,\"start\":59664},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":14620252},\"end\":60360,\"start\":59941},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":605416},\"end\":60854,\"start\":60362},{\"attributes\":{\"doi\":\"arXiv:1912.01991\",\"id\":\"b56\"},\"end\":61127,\"start\":60856},{\"attributes\":{\"id\":\"b57\"},\"end\":61394,\"start\":61129},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":545361},\"end\":61824,\"start\":61396},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":187547},\"end\":62228,\"start\":61826},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":4358477},\"end\":62531,\"start\":62230},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b61\"},\"end\":62817,\"start\":62533},{\"attributes\":{\"doi\":\"arXiv:2003.04298\",\"id\":\"b62\"},\"end\":63216,\"start\":62819},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":90262893},\"end\":63896,\"start\":63218},{\"attributes\":{\"doi\":\"arXiv:1904.09664\",\"id\":\"b64\"},\"end\":64200,\"start\":63898},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":4868248},\"end\":64695,\"start\":64202},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":5115938},\"end\":65207,\"start\":64697},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":1009127},\"end\":65768,\"start\":65209},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":1745976},\"end\":66179,\"start\":65770},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":11398758},\"end\":66526,\"start\":66181},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":206596552},\"end\":67003,\"start\":66528},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":206594095},\"end\":67605,\"start\":67005},{\"attributes\":{\"id\":\"b72\"},\"end\":67759,\"start\":67607},{\"attributes\":{\"id\":\"b73\"},\"end\":67925,\"start\":67761},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":173990644},\"end\":68287,\"start\":67927},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":7509088},\"end\":68664,\"start\":68289},{\"attributes\":{\"id\":\"b76\"},\"end\":69251,\"start\":68666},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":54607410},\"end\":69736,\"start\":69253},{\"attributes\":{\"doi\":\"arXiv:1907.03670\",\"id\":\"b78\"},\"end\":70158,\"start\":69738},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":545361},\"end\":70523,\"start\":70160},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":911406},\"end\":70823,\"start\":70525},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":6242669},\"end\":71287,\"start\":70825},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":206594775},\"end\":71729,\"start\":71289},{\"attributes\":{\"id\":\"b83\"},\"end\":71837,\"start\":71731},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":206942855},\"end\":72295,\"start\":71839},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":3525655},\"end\":72878,\"start\":72297},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":2407217},\"end\":73375,\"start\":72880},{\"attributes\":{\"id\":\"b87\"},\"end\":73655,\"start\":73377},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":209140225},\"end\":74603,\"start\":73657},{\"attributes\":{\"id\":\"b89\",\"matched_paper_id\":206592484},\"end\":75195,\"start\":74605},{\"attributes\":{\"id\":\"b90\"},\"end\":75303,\"start\":75197},{\"attributes\":{\"id\":\"b91\",\"matched_paper_id\":60945},\"end\":75728,\"start\":75305},{\"attributes\":{\"id\":\"b92\",\"matched_paper_id\":4592390},\"end\":76201,\"start\":75730},{\"attributes\":{\"id\":\"b93\",\"matched_paper_id\":30687430},\"end\":76598,\"start\":76203},{\"attributes\":{\"doi\":\"arXiv:1906.05849\",\"id\":\"b94\"},\"end\":76824,\"start\":76600},{\"attributes\":{\"doi\":\"arXiv:2005.10243\",\"id\":\"b95\"},\"end\":77171,\"start\":76826},{\"attributes\":{\"id\":\"b96\",\"matched_paper_id\":207168299},\"end\":77441,\"start\":77173},{\"attributes\":{\"id\":\"b97\",\"matched_paper_id\":4633214},\"end\":77922,\"start\":77443},{\"attributes\":{\"id\":\"b98\",\"matched_paper_id\":2057504},\"end\":78315,\"start\":77924},{\"attributes\":{\"id\":\"b99\",\"matched_paper_id\":67856373},\"end\":78828,\"start\":78317},{\"attributes\":{\"id\":\"b100\",\"matched_paper_id\":148573852},\"end\":79260,\"start\":78830},{\"attributes\":{\"doi\":\"arXiv:1801.07829\",\"id\":\"b101\"},\"end\":79609,\"start\":79262},{\"attributes\":{\"id\":\"b102\",\"matched_paper_id\":53720607},\"end\":80050,\"start\":79611},{\"attributes\":{\"id\":\"b103\",\"matched_paper_id\":206592833},\"end\":80584,\"start\":80052},{\"attributes\":{\"id\":\"b104\",\"matched_paper_id\":4591284},\"end\":81077,\"start\":80586},{\"attributes\":{\"id\":\"b105\",\"matched_paper_id\":6033252},\"end\":81544,\"start\":81079},{\"attributes\":{\"id\":\"b106\",\"matched_paper_id\":220666215},\"end\":82053,\"start\":81546},{\"attributes\":{\"id\":\"b107\",\"matched_paper_id\":4486619},\"end\":82516,\"start\":82055},{\"attributes\":{\"id\":\"b108\",\"matched_paper_id\":4536146},\"end\":82998,\"start\":82518},{\"attributes\":{\"id\":\"b109\",\"matched_paper_id\":211677426},\"end\":83571,\"start\":83000},{\"attributes\":{\"id\":\"b110\",\"matched_paper_id\":52957856},\"end\":83779,\"start\":83573},{\"attributes\":{\"id\":\"b111\",\"matched_paper_id\":8338993},\"end\":84258,\"start\":83781},{\"attributes\":{\"id\":\"b112\",\"matched_paper_id\":198229603},\"end\":84715,\"start\":84260},{\"attributes\":{\"id\":\"b113\",\"matched_paper_id\":54462144},\"end\":85260,\"start\":84717},{\"attributes\":{\"id\":\"b114\",\"matched_paper_id\":57189582},\"end\":85709,\"start\":85262},{\"attributes\":{\"doi\":\"arXiv:2006.05682\",\"id\":\"b115\"},\"end\":86009,\"start\":85711},{\"attributes\":{\"id\":\"b116\",\"matched_paper_id\":51934625},\"end\":86453,\"start\":86011},{\"attributes\":{\"id\":\"b117\",\"matched_paper_id\":42427078},\"end\":86895,\"start\":86455}]", "bib_title": "[{\"end\":39042,\"start\":38976},{\"end\":39553,\"start\":39490},{\"end\":40244,\"start\":40141},{\"end\":41023,\"start\":40982},{\"end\":41408,\"start\":41348},{\"end\":41874,\"start\":41799},{\"end\":43292,\"start\":43224},{\"end\":44423,\"start\":44407},{\"end\":45061,\"start\":44993},{\"end\":45546,\"start\":45485},{\"end\":46032,\"start\":45967},{\"end\":46433,\"start\":46405},{\"end\":46764,\"start\":46676},{\"end\":47337,\"start\":47228},{\"end\":47892,\"start\":47813},{\"end\":48622,\"start\":48563},{\"end\":49025,\"start\":48985},{\"end\":49758,\"start\":49687},{\"end\":50174,\"start\":50164},{\"end\":50694,\"start\":50623},{\"end\":51233,\"start\":51162},{\"end\":51734,\"start\":51677},{\"end\":52277,\"start\":52220},{\"end\":52718,\"start\":52662},{\"end\":53831,\"start\":53757},{\"end\":54223,\"start\":54175},{\"end\":54604,\"start\":54565},{\"end\":55325,\"start\":55263},{\"end\":55827,\"start\":55740},{\"end\":56617,\"start\":56537},{\"end\":57762,\"start\":57691},{\"end\":58267,\"start\":58212},{\"end\":58704,\"start\":58660},{\"end\":59360,\"start\":59262},{\"end\":59735,\"start\":59664},{\"end\":60015,\"start\":59941},{\"end\":60462,\"start\":60362},{\"end\":61454,\"start\":61396},{\"end\":61899,\"start\":61826},{\"end\":62325,\"start\":62230},{\"end\":63358,\"start\":63218},{\"end\":64259,\"start\":64202},{\"end\":64773,\"start\":64697},{\"end\":65276,\"start\":65209},{\"end\":65848,\"start\":65770},{\"end\":66275,\"start\":66181},{\"end\":66588,\"start\":66528},{\"end\":67106,\"start\":67005},{\"end\":67996,\"start\":67927},{\"end\":68347,\"start\":68289},{\"end\":68733,\"start\":68666},{\"end\":69324,\"start\":69253},{\"end\":70218,\"start\":70160},{\"end\":70592,\"start\":70525},{\"end\":70879,\"start\":70825},{\"end\":71355,\"start\":71289},{\"end\":71891,\"start\":71839},{\"end\":72357,\"start\":72297},{\"end\":72993,\"start\":72880},{\"end\":73725,\"start\":73657},{\"end\":74635,\"start\":74605},{\"end\":75401,\"start\":75305},{\"end\":75777,\"start\":75730},{\"end\":76253,\"start\":76203},{\"end\":77241,\"start\":77173},{\"end\":77503,\"start\":77443},{\"end\":77984,\"start\":77924},{\"end\":78381,\"start\":78317},{\"end\":78903,\"start\":78830},{\"end\":79668,\"start\":79611},{\"end\":80109,\"start\":80052},{\"end\":80658,\"start\":80586},{\"end\":81152,\"start\":81079},{\"end\":81619,\"start\":81546},{\"end\":82110,\"start\":82055},{\"end\":82597,\"start\":82518},{\"end\":83095,\"start\":83000},{\"end\":83622,\"start\":83573},{\"end\":83843,\"start\":83781},{\"end\":84315,\"start\":84260},{\"end\":84800,\"start\":84717},{\"end\":85289,\"start\":85262},{\"end\":86082,\"start\":86011},{\"end\":86526,\"start\":86455}]", "bib_author": "[{\"end\":38775,\"start\":38753},{\"end\":38786,\"start\":38775},{\"end\":38795,\"start\":38786},{\"end\":39062,\"start\":39044},{\"end\":39077,\"start\":39062},{\"end\":39097,\"start\":39077},{\"end\":39114,\"start\":39097},{\"end\":39569,\"start\":39555},{\"end\":39583,\"start\":39569},{\"end\":39604,\"start\":39583},{\"end\":39924,\"start\":39912},{\"end\":39935,\"start\":39924},{\"end\":39961,\"start\":39935},{\"end\":39971,\"start\":39961},{\"end\":40258,\"start\":40246},{\"end\":40270,\"start\":40258},{\"end\":40290,\"start\":40270},{\"end\":40297,\"start\":40290},{\"end\":40778,\"start\":40764},{\"end\":40792,\"start\":40778},{\"end\":40806,\"start\":40792},{\"end\":41043,\"start\":41025},{\"end\":41058,\"start\":41043},{\"end\":41426,\"start\":41410},{\"end\":41444,\"start\":41426},{\"end\":41459,\"start\":41444},{\"end\":41475,\"start\":41459},{\"end\":41892,\"start\":41876},{\"end\":41905,\"start\":41892},{\"end\":41920,\"start\":41905},{\"end\":41933,\"start\":41920},{\"end\":41951,\"start\":41933},{\"end\":41966,\"start\":41951},{\"end\":42320,\"start\":42307},{\"end\":42332,\"start\":42320},{\"end\":42351,\"start\":42332},{\"end\":42366,\"start\":42351},{\"end\":42385,\"start\":42366},{\"end\":42400,\"start\":42385},{\"end\":42413,\"start\":42400},{\"end\":42424,\"start\":42413},{\"end\":42437,\"start\":42424},{\"end\":42781,\"start\":42772},{\"end\":42795,\"start\":42781},{\"end\":42816,\"start\":42795},{\"end\":42828,\"start\":42816},{\"end\":42845,\"start\":42828},{\"end\":42857,\"start\":42845},{\"end\":42868,\"start\":42857},{\"end\":42886,\"start\":42868},{\"end\":42900,\"start\":42886},{\"end\":42910,\"start\":42900},{\"end\":42914,\"start\":42910},{\"end\":43307,\"start\":43294},{\"end\":43318,\"start\":43307},{\"end\":43331,\"start\":43318},{\"end\":43345,\"start\":43331},{\"end\":43359,\"start\":43345},{\"end\":43368,\"start\":43359},{\"end\":43855,\"start\":43844},{\"end\":43872,\"start\":43855},{\"end\":43890,\"start\":43872},{\"end\":43907,\"start\":43890},{\"end\":44130,\"start\":44117},{\"end\":44141,\"start\":44130},{\"end\":44437,\"start\":44425},{\"end\":44446,\"start\":44437},{\"end\":44461,\"start\":44446},{\"end\":44472,\"start\":44461},{\"end\":44775,\"start\":44760},{\"end\":44789,\"start\":44775},{\"end\":44805,\"start\":44789},{\"end\":44821,\"start\":44805},{\"end\":45081,\"start\":45063},{\"end\":45096,\"start\":45081},{\"end\":45113,\"start\":45096},{\"end\":45560,\"start\":45548},{\"end\":45575,\"start\":45560},{\"end\":45590,\"start\":45575},{\"end\":45605,\"start\":45590},{\"end\":45624,\"start\":45605},{\"end\":45643,\"start\":45624},{\"end\":46048,\"start\":46034},{\"end\":46063,\"start\":46048},{\"end\":46079,\"start\":46063},{\"end\":46446,\"start\":46435},{\"end\":46460,\"start\":46446},{\"end\":46471,\"start\":46460},{\"end\":46786,\"start\":46766},{\"end\":46803,\"start\":46786},{\"end\":46829,\"start\":46803},{\"end\":46848,\"start\":46829},{\"end\":46861,\"start\":46848},{\"end\":47358,\"start\":47339},{\"end\":47374,\"start\":47358},{\"end\":47389,\"start\":47374},{\"end\":47908,\"start\":47894},{\"end\":47916,\"start\":47908},{\"end\":47935,\"start\":47916},{\"end\":48418,\"start\":48404},{\"end\":48641,\"start\":48624},{\"end\":48651,\"start\":48641},{\"end\":48667,\"start\":48651},{\"end\":49043,\"start\":49027},{\"end\":49056,\"start\":49043},{\"end\":49075,\"start\":49056},{\"end\":49091,\"start\":49075},{\"end\":49396,\"start\":49377},{\"end\":49410,\"start\":49396},{\"end\":49419,\"start\":49410},{\"end\":49432,\"start\":49419},{\"end\":49443,\"start\":49432},{\"end\":49458,\"start\":49443},{\"end\":49775,\"start\":49760},{\"end\":49794,\"start\":49775},{\"end\":49811,\"start\":49794},{\"end\":49826,\"start\":49811},{\"end\":50194,\"start\":50176},{\"end\":50210,\"start\":50194},{\"end\":50226,\"start\":50210},{\"end\":50526,\"start\":50513},{\"end\":50541,\"start\":50526},{\"end\":50556,\"start\":50541},{\"end\":50703,\"start\":50696},{\"end\":50998,\"start\":50986},{\"end\":51252,\"start\":51235},{\"end\":51269,\"start\":51252},{\"end\":51293,\"start\":51269},{\"end\":51754,\"start\":51736},{\"end\":51770,\"start\":51754},{\"end\":51782,\"start\":51770},{\"end\":51787,\"start\":51782},{\"end\":51796,\"start\":51787},{\"end\":51813,\"start\":51796},{\"end\":51820,\"start\":51813},{\"end\":52293,\"start\":52279},{\"end\":52307,\"start\":52293},{\"end\":52319,\"start\":52307},{\"end\":52735,\"start\":52720},{\"end\":52747,\"start\":52735},{\"end\":53139,\"start\":53127},{\"end\":53150,\"start\":53139},{\"end\":53160,\"start\":53150},{\"end\":53173,\"start\":53160},{\"end\":53188,\"start\":53173},{\"end\":53403,\"start\":53392},{\"end\":53415,\"start\":53403},{\"end\":53428,\"start\":53415},{\"end\":53437,\"start\":53428},{\"end\":53454,\"start\":53437},{\"end\":53468,\"start\":53454},{\"end\":53851,\"start\":53833},{\"end\":53868,\"start\":53851},{\"end\":53886,\"start\":53868},{\"end\":53901,\"start\":53886},{\"end\":53916,\"start\":53901},{\"end\":54616,\"start\":54606},{\"end\":54631,\"start\":54616},{\"end\":54645,\"start\":54631},{\"end\":54652,\"start\":54645},{\"end\":55004,\"start\":54991},{\"end\":55018,\"start\":55004},{\"end\":55034,\"start\":55018},{\"end\":55343,\"start\":55327},{\"end\":55359,\"start\":55343},{\"end\":55373,\"start\":55359},{\"end\":55392,\"start\":55373},{\"end\":55405,\"start\":55392},{\"end\":55418,\"start\":55405},{\"end\":55434,\"start\":55418},{\"end\":55836,\"start\":55829},{\"end\":55844,\"start\":55836},{\"end\":55862,\"start\":55844},{\"end\":55871,\"start\":55862},{\"end\":56289,\"start\":56274},{\"end\":56303,\"start\":56289},{\"end\":56315,\"start\":56303},{\"end\":56326,\"start\":56315},{\"end\":56339,\"start\":56326},{\"end\":56633,\"start\":56619},{\"end\":56651,\"start\":56633},{\"end\":57067,\"start\":57045},{\"end\":57080,\"start\":57067},{\"end\":57094,\"start\":57080},{\"end\":57111,\"start\":57094},{\"end\":57125,\"start\":57111},{\"end\":57140,\"start\":57125},{\"end\":57154,\"start\":57140},{\"end\":57482,\"start\":57460},{\"end\":57496,\"start\":57482},{\"end\":57509,\"start\":57496},{\"end\":57773,\"start\":57764},{\"end\":57787,\"start\":57773},{\"end\":57797,\"start\":57787},{\"end\":57810,\"start\":57797},{\"end\":57825,\"start\":57810},{\"end\":58280,\"start\":58269},{\"end\":58287,\"start\":58280},{\"end\":58301,\"start\":58287},{\"end\":58306,\"start\":58301},{\"end\":58718,\"start\":58706},{\"end\":58726,\"start\":58718},{\"end\":58740,\"start\":58726},{\"end\":58748,\"start\":58740},{\"end\":58759,\"start\":58748},{\"end\":58773,\"start\":58759},{\"end\":59090,\"start\":59073},{\"end\":59104,\"start\":59090},{\"end\":59377,\"start\":59362},{\"end\":59392,\"start\":59377},{\"end\":59412,\"start\":59392},{\"end\":59424,\"start\":59412},{\"end\":59440,\"start\":59424},{\"end\":59451,\"start\":59440},{\"end\":59746,\"start\":59737},{\"end\":59755,\"start\":59746},{\"end\":59764,\"start\":59755},{\"end\":59779,\"start\":59764},{\"end\":60034,\"start\":60017},{\"end\":60053,\"start\":60034},{\"end\":60477,\"start\":60464},{\"end\":60488,\"start\":60477},{\"end\":60498,\"start\":60488},{\"end\":60932,\"start\":60919},{\"end\":60956,\"start\":60932},{\"end\":61209,\"start\":61194},{\"end\":61227,\"start\":61209},{\"end\":61240,\"start\":61227},{\"end\":61495,\"start\":61456},{\"end\":61506,\"start\":61495},{\"end\":61514,\"start\":61506},{\"end\":61916,\"start\":61901},{\"end\":61930,\"start\":61916},{\"end\":62342,\"start\":62327},{\"end\":62353,\"start\":62342},{\"end\":62613,\"start\":62593},{\"end\":62623,\"start\":62613},{\"end\":62638,\"start\":62623},{\"end\":62903,\"start\":62886},{\"end\":62917,\"start\":62903},{\"end\":62928,\"start\":62917},{\"end\":62936,\"start\":62928},{\"end\":62956,\"start\":62936},{\"end\":62970,\"start\":62956},{\"end\":62979,\"start\":62970},{\"end\":63377,\"start\":63360},{\"end\":63391,\"start\":63377},{\"end\":63401,\"start\":63391},{\"end\":63412,\"start\":63401},{\"end\":63426,\"start\":63412},{\"end\":63433,\"start\":63426},{\"end\":63974,\"start\":63957},{\"end\":63990,\"start\":63974},{\"end\":64005,\"start\":63990},{\"end\":64013,\"start\":64005},{\"end\":64279,\"start\":64261},{\"end\":64292,\"start\":64279},{\"end\":64300,\"start\":64292},{\"end\":64315,\"start\":64300},{\"end\":64323,\"start\":64315},{\"end\":64793,\"start\":64775},{\"end\":64805,\"start\":64793},{\"end\":64820,\"start\":64805},{\"end\":64828,\"start\":64820},{\"end\":65296,\"start\":65278},{\"end\":65309,\"start\":65296},{\"end\":65325,\"start\":65309},{\"end\":65339,\"start\":65325},{\"end\":65355,\"start\":65339},{\"end\":65363,\"start\":65355},{\"end\":65877,\"start\":65850},{\"end\":65885,\"start\":65877},{\"end\":65900,\"start\":65885},{\"end\":65908,\"start\":65900},{\"end\":66291,\"start\":66277},{\"end\":66307,\"start\":66291},{\"end\":66320,\"start\":66307},{\"end\":66334,\"start\":66320},{\"end\":66341,\"start\":66334},{\"end\":66606,\"start\":66590},{\"end\":66624,\"start\":66606},{\"end\":66640,\"start\":66624},{\"end\":67120,\"start\":67108},{\"end\":67135,\"start\":67120},{\"end\":67155,\"start\":67135},{\"end\":67170,\"start\":67155},{\"end\":67185,\"start\":67170},{\"end\":67649,\"start\":67632},{\"end\":67659,\"start\":67649},{\"end\":67828,\"start\":67819},{\"end\":68015,\"start\":67998},{\"end\":68031,\"start\":68015},{\"end\":68364,\"start\":68349},{\"end\":68373,\"start\":68364},{\"end\":68384,\"start\":68373},{\"end\":68402,\"start\":68384},{\"end\":68419,\"start\":68402},{\"end\":68428,\"start\":68419},{\"end\":68750,\"start\":68735},{\"end\":68762,\"start\":68750},{\"end\":68772,\"start\":68762},{\"end\":68782,\"start\":68772},{\"end\":68796,\"start\":68782},{\"end\":68811,\"start\":68796},{\"end\":68825,\"start\":68811},{\"end\":69341,\"start\":69326},{\"end\":69356,\"start\":69341},{\"end\":69370,\"start\":69356},{\"end\":69858,\"start\":69843},{\"end\":69868,\"start\":69858},{\"end\":69882,\"start\":69868},{\"end\":69897,\"start\":69882},{\"end\":69911,\"start\":69897},{\"end\":70238,\"start\":70220},{\"end\":70251,\"start\":70238},{\"end\":70267,\"start\":70251},{\"end\":70279,\"start\":70267},{\"end\":70607,\"start\":70594},{\"end\":70894,\"start\":70881},{\"end\":70904,\"start\":70894},{\"end\":70927,\"start\":70904},{\"end\":70933,\"start\":70927},{\"end\":71370,\"start\":71357},{\"end\":71386,\"start\":71370},{\"end\":71765,\"start\":71752},{\"end\":71907,\"start\":71893},{\"end\":71926,\"start\":71907},{\"end\":71940,\"start\":71926},{\"end\":71957,\"start\":71940},{\"end\":71973,\"start\":71957},{\"end\":72368,\"start\":72359},{\"end\":72383,\"start\":72368},{\"end\":72395,\"start\":72383},{\"end\":72411,\"start\":72395},{\"end\":72434,\"start\":72411},{\"end\":72451,\"start\":72434},{\"end\":72462,\"start\":72451},{\"end\":73004,\"start\":72995},{\"end\":73020,\"start\":73004},{\"end\":73455,\"start\":73445},{\"end\":73476,\"start\":73455},{\"end\":73491,\"start\":73476},{\"end\":73506,\"start\":73491},{\"end\":73736,\"start\":73727},{\"end\":73756,\"start\":73736},{\"end\":73774,\"start\":73756},{\"end\":73792,\"start\":73774},{\"end\":73810,\"start\":73792},{\"end\":73821,\"start\":73810},{\"end\":73832,\"start\":73821},{\"end\":73842,\"start\":73832},{\"end\":73855,\"start\":73842},{\"end\":73871,\"start\":73855},{\"end\":74656,\"start\":74637},{\"end\":74665,\"start\":74656},{\"end\":74679,\"start\":74665},{\"end\":74696,\"start\":74679},{\"end\":74708,\"start\":74696},{\"end\":74727,\"start\":74708},{\"end\":74742,\"start\":74727},{\"end\":74761,\"start\":74742},{\"end\":74780,\"start\":74761},{\"end\":75218,\"start\":75199},{\"end\":75238,\"start\":75218},{\"end\":75409,\"start\":75403},{\"end\":75798,\"start\":75779},{\"end\":75811,\"start\":75798},{\"end\":75827,\"start\":75811},{\"end\":75841,\"start\":75827},{\"end\":76269,\"start\":76255},{\"end\":76287,\"start\":76269},{\"end\":76299,\"start\":76287},{\"end\":76314,\"start\":76299},{\"end\":76331,\"start\":76314},{\"end\":76615,\"start\":76600},{\"end\":76631,\"start\":76615},{\"end\":76646,\"start\":76631},{\"end\":76894,\"start\":76879},{\"end\":76904,\"start\":76894},{\"end\":76915,\"start\":76904},{\"end\":76931,\"start\":76915},{\"end\":76948,\"start\":76931},{\"end\":76963,\"start\":76948},{\"end\":77254,\"start\":77243},{\"end\":77268,\"start\":77254},{\"end\":77278,\"start\":77268},{\"end\":77293,\"start\":77278},{\"end\":77519,\"start\":77505},{\"end\":77532,\"start\":77519},{\"end\":77543,\"start\":77532},{\"end\":77554,\"start\":77543},{\"end\":77563,\"start\":77554},{\"end\":77578,\"start\":77563},{\"end\":78001,\"start\":77986},{\"end\":78016,\"start\":78001},{\"end\":78397,\"start\":78383},{\"end\":78406,\"start\":78397},{\"end\":78421,\"start\":78406},{\"end\":78435,\"start\":78421},{\"end\":78446,\"start\":78435},{\"end\":78915,\"start\":78905},{\"end\":78933,\"start\":78915},{\"end\":79272,\"start\":79262},{\"end\":79285,\"start\":79272},{\"end\":79296,\"start\":79285},{\"end\":79306,\"start\":79296},{\"end\":79313,\"start\":79306},{\"end\":79343,\"start\":79313},{\"end\":79352,\"start\":79343},{\"end\":79682,\"start\":79670},{\"end\":79695,\"start\":79682},{\"end\":79705,\"start\":79695},{\"end\":80123,\"start\":80111},{\"end\":80136,\"start\":80123},{\"end\":80151,\"start\":80136},{\"end\":80162,\"start\":80151},{\"end\":80178,\"start\":80162},{\"end\":80191,\"start\":80178},{\"end\":80207,\"start\":80191},{\"end\":80672,\"start\":80660},{\"end\":80687,\"start\":80672},{\"end\":80697,\"start\":80687},{\"end\":80707,\"start\":80697},{\"end\":80712,\"start\":80707},{\"end\":81170,\"start\":81154},{\"end\":81184,\"start\":81170},{\"end\":81202,\"start\":81184},{\"end\":81634,\"start\":81621},{\"end\":81645,\"start\":81634},{\"end\":81655,\"start\":81645},{\"end\":81680,\"start\":81655},{\"end\":81691,\"start\":81680},{\"end\":81699,\"start\":81691},{\"end\":82125,\"start\":82112},{\"end\":82137,\"start\":82125},{\"end\":82148,\"start\":82137},{\"end\":82160,\"start\":82148},{\"end\":82609,\"start\":82599},{\"end\":82621,\"start\":82609},{\"end\":82632,\"start\":82621},{\"end\":82643,\"start\":82632},{\"end\":82652,\"start\":82643},{\"end\":83105,\"start\":83097},{\"end\":83119,\"start\":83105},{\"end\":83128,\"start\":83119},{\"end\":83140,\"start\":83128},{\"end\":83154,\"start\":83140},{\"end\":83633,\"start\":83624},{\"end\":83645,\"start\":83633},{\"end\":83652,\"start\":83645},{\"end\":83859,\"start\":83845},{\"end\":83870,\"start\":83859},{\"end\":83881,\"start\":83870},{\"end\":83892,\"start\":83881},{\"end\":84330,\"start\":84317},{\"end\":84341,\"start\":84330},{\"end\":84350,\"start\":84341},{\"end\":84365,\"start\":84350},{\"end\":84376,\"start\":84365},{\"end\":84809,\"start\":84802},{\"end\":84820,\"start\":84809},{\"end\":84829,\"start\":84820},{\"end\":84843,\"start\":84829},{\"end\":84862,\"start\":84843},{\"end\":85305,\"start\":85291},{\"end\":85321,\"start\":85305},{\"end\":85332,\"start\":85321},{\"end\":85346,\"start\":85332},{\"end\":85360,\"start\":85346},{\"end\":85788,\"start\":85774},{\"end\":85796,\"start\":85788},{\"end\":85809,\"start\":85796},{\"end\":85823,\"start\":85809},{\"end\":86098,\"start\":86084},{\"end\":86112,\"start\":86098},{\"end\":86126,\"start\":86112},{\"end\":86138,\"start\":86126},{\"end\":86154,\"start\":86138},{\"end\":86169,\"start\":86154},{\"end\":86183,\"start\":86169},{\"end\":86538,\"start\":86528},{\"end\":86551,\"start\":86538}]", "bib_venue": "[{\"end\":39241,\"start\":39186},{\"end\":40438,\"start\":40376},{\"end\":41185,\"start\":41130},{\"end\":41590,\"start\":41541},{\"end\":43517,\"start\":43451},{\"end\":44593,\"start\":44541},{\"end\":45254,\"start\":45192},{\"end\":46204,\"start\":46150},{\"end\":47538,\"start\":47472},{\"end\":48076,\"start\":48014},{\"end\":48782,\"start\":48733},{\"end\":49941,\"start\":49892},{\"end\":50347,\"start\":50295},{\"end\":50828,\"start\":50774},{\"end\":51434,\"start\":51372},{\"end\":51965,\"start\":51901},{\"end\":52464,\"start\":52400},{\"end\":52872,\"start\":52818},{\"end\":54793,\"start\":54731},{\"end\":55996,\"start\":55942},{\"end\":56772,\"start\":56720},{\"end\":57966,\"start\":57904},{\"end\":58451,\"start\":58387},{\"end\":60625,\"start\":60570},{\"end\":61629,\"start\":61580},{\"end\":62045,\"start\":61996},{\"end\":63574,\"start\":63512},{\"end\":64464,\"start\":64402},{\"end\":64969,\"start\":64907},{\"end\":65504,\"start\":65442},{\"end\":66781,\"start\":66719},{\"end\":67330,\"start\":67266},{\"end\":68974,\"start\":68908},{\"end\":69511,\"start\":69449},{\"end\":71074,\"start\":71012},{\"end\":71527,\"start\":71465},{\"end\":72603,\"start\":72541},{\"end\":73141,\"start\":73089},{\"end\":74020,\"start\":73954},{\"end\":74925,\"start\":74861},{\"end\":75530,\"start\":75478},{\"end\":75982,\"start\":75920},{\"end\":77693,\"start\":77644},{\"end\":78141,\"start\":78087},{\"end\":78587,\"start\":78525},{\"end\":79058,\"start\":79004},{\"end\":79846,\"start\":79784},{\"end\":80293,\"start\":80278},{\"end\":80857,\"start\":80793},{\"end\":81323,\"start\":81271},{\"end\":81814,\"start\":81765},{\"end\":82301,\"start\":82239},{\"end\":82767,\"start\":82718},{\"end\":83303,\"start\":83237},{\"end\":84037,\"start\":83973},{\"end\":84497,\"start\":84445},{\"end\":85003,\"start\":84941},{\"end\":85501,\"start\":85439},{\"end\":86692,\"start\":86630},{\"end\":38751,\"start\":38690},{\"end\":39184,\"start\":39114},{\"end\":39627,\"start\":39604},{\"end\":39910,\"start\":39854},{\"end\":40374,\"start\":40297},{\"end\":40762,\"start\":40704},{\"end\":41128,\"start\":41058},{\"end\":41539,\"start\":41475},{\"end\":41973,\"start\":41966},{\"end\":42305,\"start\":42167},{\"end\":42969,\"start\":42930},{\"end\":43449,\"start\":43368},{\"end\":43842,\"start\":43773},{\"end\":44241,\"start\":44157},{\"end\":44539,\"start\":44472},{\"end\":44868,\"start\":44837},{\"end\":45190,\"start\":45113},{\"end\":45712,\"start\":45643},{\"end\":46148,\"start\":46079},{\"end\":46530,\"start\":46471},{\"end\":46931,\"start\":46861},{\"end\":47470,\"start\":47389},{\"end\":48012,\"start\":47935},{\"end\":48402,\"start\":48306},{\"end\":48731,\"start\":48667},{\"end\":49133,\"start\":49091},{\"end\":49375,\"start\":49305},{\"end\":49890,\"start\":49826},{\"end\":50293,\"start\":50226},{\"end\":50772,\"start\":50703},{\"end\":51053,\"start\":51014},{\"end\":51370,\"start\":51293},{\"end\":51899,\"start\":51820},{\"end\":52398,\"start\":52319},{\"end\":52816,\"start\":52747},{\"end\":53125,\"start\":53060},{\"end\":53551,\"start\":53484},{\"end\":53950,\"start\":53916},{\"end\":54268,\"start\":54225},{\"end\":54729,\"start\":54652},{\"end\":55103,\"start\":55050},{\"end\":55476,\"start\":55434},{\"end\":55940,\"start\":55871},{\"end\":56272,\"start\":56215},{\"end\":56718,\"start\":56651},{\"end\":57043,\"start\":56985},{\"end\":57458,\"start\":57401},{\"end\":57902,\"start\":57825},{\"end\":58385,\"start\":58306},{\"end\":58822,\"start\":58773},{\"end\":59071,\"start\":59019},{\"end\":59455,\"start\":59451},{\"end\":59784,\"start\":59779},{\"end\":60127,\"start\":60053},{\"end\":60568,\"start\":60498},{\"end\":60917,\"start\":60856},{\"end\":61192,\"start\":61129},{\"end\":61578,\"start\":61514},{\"end\":61994,\"start\":61930},{\"end\":62359,\"start\":62353},{\"end\":62591,\"start\":62533},{\"end\":62884,\"start\":62819},{\"end\":63510,\"start\":63433},{\"end\":63955,\"start\":63898},{\"end\":64400,\"start\":64323},{\"end\":64905,\"start\":64828},{\"end\":65440,\"start\":65363},{\"end\":65957,\"start\":65908},{\"end\":66345,\"start\":66341},{\"end\":66717,\"start\":66640},{\"end\":67264,\"start\":67185},{\"end\":67630,\"start\":67607},{\"end\":67817,\"start\":67761},{\"end\":68090,\"start\":68031},{\"end\":68462,\"start\":68428},{\"end\":68906,\"start\":68825},{\"end\":69447,\"start\":69370},{\"end\":69841,\"start\":69738},{\"end\":70317,\"start\":70279},{\"end\":70666,\"start\":70607},{\"end\":71010,\"start\":70933},{\"end\":71463,\"start\":71386},{\"end\":71750,\"start\":71731},{\"end\":72045,\"start\":71973},{\"end\":72539,\"start\":72462},{\"end\":73087,\"start\":73020},{\"end\":73443,\"start\":73377},{\"end\":73952,\"start\":73871},{\"end\":74859,\"start\":74780},{\"end\":75476,\"start\":75409},{\"end\":75918,\"start\":75841},{\"end\":76379,\"start\":76331},{\"end\":76690,\"start\":76662},{\"end\":76877,\"start\":76826},{\"end\":77297,\"start\":77293},{\"end\":77642,\"start\":77578},{\"end\":78085,\"start\":78016},{\"end\":78523,\"start\":78446},{\"end\":79002,\"start\":78933},{\"end\":79414,\"start\":79368},{\"end\":79782,\"start\":79705},{\"end\":80276,\"start\":80207},{\"end\":80791,\"start\":80712},{\"end\":81269,\"start\":81202},{\"end\":81763,\"start\":81699},{\"end\":82237,\"start\":82160},{\"end\":82716,\"start\":82652},{\"end\":83235,\"start\":83154},{\"end\":83659,\"start\":83652},{\"end\":83971,\"start\":83892},{\"end\":84443,\"start\":84376},{\"end\":84939,\"start\":84862},{\"end\":85437,\"start\":85360},{\"end\":85772,\"start\":85711},{\"end\":86217,\"start\":86183},{\"end\":86628,\"start\":86551}]"}}}, "year": 2023, "month": 12, "day": 17}