{"id": 257255328, "updated": "2023-10-05 03:32:56.716", "metadata": {"title": "S-NeRF: Neural Radiance Fields for Street Views", "authors": "[{\"first\":\"Ziyang\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Junge\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Wenye\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Feihu\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Li\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Neural Radiance Fields (NeRFs) aim to synthesize novel views of objects and scenes, given the object-centric camera views with large overlaps. However, we conjugate that this paradigm does not fit the nature of the street views that are collected by many self-driving cars from the large-scale unbounded scenes. Also, the onboard cameras perceive scenes without much overlapping. Thus, existing NeRFs often produce blurs, 'floaters' and other artifacts on street-view synthesis. In this paper, we propose a new street-view NeRF (S-NeRF) that considers novel view synthesis of both the large-scale background scenes and the foreground moving vehicles jointly. Specifically, we improve the scene parameterization function and the camera poses for learning better neural representations from street views. We also use the the noisy and sparse LiDAR points to boost the training and learn a robust geometry and reprojection based confidence to address the depth outliers. Moreover, we extend our S-NeRF for reconstructing moving vehicles that is impracticable for conventional NeRFs. Thorough experiments on the large-scale driving datasets (e.g., nuScenes and Waymo) demonstrate that our method beats the state-of-the-art rivals by reducing 7% to 40% of the mean-squared error in the street-view synthesis and a 45% PSNR gain for the moving vehicles rendering.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2303.00749", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/XieZ0Z023", "doi": "10.48550/arxiv.2303.00749"}}, "content": {"source": {"pdf_hash": "380578ed28081a70b877be7e8a3c4d3a9997b041", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2303.00749v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "8a74f760cfa5e1bf470a1e646c8d6d36a51efca2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/380578ed28081a70b877be7e8a3c4d3a9997b041.txt", "contents": "\nS-NERF: NEURAL RADIANCE FIELDS FOR STREET VIEWS\n\n\nZiyang Xie \nFudan University\n\n\nJunge Zhang \nFudan University\n\n\nWenye Li \nFudan University\n\n\nFeihu Zhang \nUniversity of Oxford\n\n\nLi Zhang \nFudan University\n\n\nS-NERF: NEURAL RADIANCE FIELDS FOR STREET VIEWS\nPublished as a conference paper at ICLR 2023\nNeural Radiance Fields (NeRFs) aim to synthesize novel views of objects and scenes, given the object-centric camera views with large overlaps. However, we conjugate that this paradigm does not fit the nature of the street views that are collected by many self-driving cars from the large-scale unbounded scenes. Also, the onboard cameras perceive scenes without much overlapping. Thus, existing NeRFs often produce blurs, \"floaters\" and other artifacts on street-view synthesis. In this paper, we propose a new street-view NeRF (S-NeRF) that considers novel view synthesis of both the large-scale background scenes and the foreground moving vehicles jointly. Specifically, we improve the scene parameterization function and the camera poses for learning better neural representations from street views. We also use the the noisy and sparse LiDAR points to boost the training and learn a robust geometry and reprojection based confidence to address the depth outliers. Moreover, we extend our S-NeRF for reconstructing moving vehicles that is impracticable for conventional NeRFs. Thorough experiments on the large-scale driving datasets (e.g., nuScenes and Waymo) demonstrate that our method beats the state-of-the-art rivals by reducing 7\u223c 40% of the mean-squared error in the street-view synthesis and a 45% PSNR gain for the moving vehicles rendering. * Equal contribution\n\nINTRODUCTION\n\nNeural Radiance Fields (Mildenhall et al., 2020) have shown impressive performance on photorealistic novel view rendering. However, original NeRF is usually designed for object-centric scenes and require camera views to be heavily overlapped (as shown in Figure 1(a)).\n\nRecently, more and more street view data are collected by self-driving cars. The reconstruction and novel view rendering for street views can be very useful in driving simulation, data generation, AR and VR. However, these data are often collected in the unbounded outdoor scenes (e.g. nuScenes (Caesar et al., 2019) and Waymo (Sun et al., 2020) datasets). The camera placements of such data acquisition systems are usually in a panoramic settings without object-centric camera views (Figure 1(b)). Moreover, the overlaps between adjacent camera views are too small to be effective for training NeRFs. Since the ego car is moving fast, some objects or contents only appear in a limited number of image views. (e.g. Most of the vehicles need to be reconstructed from just 2 \u223c 6 views.) All these problems make it difficult to optimize existing NeRFs for street-view synthesis.\n\nMipNeRF-360 (Barron et al., 2022) is designed for training in unbounded scenes. BlockNeRF (Tancik et al., 2022) proposes a block-combination strategy with refined poses, appearances, and exposure on the MipNeRF (Barron et al., 2021) base model for processing large-scale outdoor scenes. However, they still require enough intersected camera rays (Figure 1(a)) and large overlaps across different cameras (e.g. Block-NeRF uses a special system with twelve cameras for data acquisition to guarantee enough overlaps between different camera views). They produce many blurs, \"floaters\" and other artifacts when training on existing self-driving datasets (e.g. nuScenes (Caesar et al., 2019) and Waymo (Sun et al., 2020), as shown in Figure 2(a)). Urban-NeRF (Rematas et al., 2022) takes accurate dense LiDAR depth as supervision for the reconstruction of urban scenes. However, these dense LiDAR depths are difficult and expensive to collect.  2020)), the camera placements for data collection are usually in a panoramic view settings. Rays from different cameras barely intersect with others in the unbounded scenes. The overlapped field of view between adjacent cameras is too small to be effective for training the existing NeRF models.\n\nAs shown in Figure 3(a), data Caesar et al. (2019); Sun et al. (2020) collected by self-driving cars can not be used for training Urban-NeRF because they only acquire sparse LiDAR points with plenty of outliers when projected to images (e.g. only 2\u223c5K points are captured for each nuScenes image).\n\nIn this paper, we contribute a new NeRF design (S-NeRF) for the novel view synthesis of both the large-scale (background) scenes and the foreground moving vehicles. Different from other largescale NeRFs (Tancik et al., 2022;Rematas et al., 2022), our method does not require specially designed data acquisition platform used in them. Our S-NeRF can be trained on the standard selfdriving datasets (e.g. nuScenes (Caesar et al., 2019) and Waymo (Sun et al., 2020)) that are collected by common self-driving cars with fewer cameras and noisy sparse LiDAR points to synthesize novel street views.\n\nWe improve the scene parameterization function and the camera poses for learning better neural representations from street views. We also develop a novel depth rendering and supervision method using the noisy sparse LiDAR signals to effectively train our S-NeRF for street-view synthesis. To deal with the depth outliers, we propose a new confidence metric learned from the robust geometry and reprojection consistencies.\n\nNot only for the background scenes, we further extend our S-NeRF for high-quality reconstruction of the moving vehicles (e.g. moving cars) using the proposed virtual camera transformation.\n\nIn the experiments, we demonstrate the performance of our S-NeRF on the standard driving datasets (Caesar et al., 2019;Sun et al., 2020). For the static scene reconstruction, our S-NeRF far outperforms the large-scale NeRFs (Barron et al., 2021;Rematas et al., 2022). It reduces the mean-squared error by 7 \u223c 40% and produces impressive depth renderings (Figure 2(b)). For the foreground objects, S-NeRF is shown capable of reconstructing moving vehicles in high quality, which is impracticable for conventional NeRFs (Mildenhall et al., 2020;Barron et al., 2021;Deng et al., 2022). It also beats the latest mesh-based reconstruction method Chen et al. (2021b), improving the PSNR by 45% and the structure similarity by 18%.\n\n2 RELATED WORK 2.1 3D RECONSTRUCTION Traditional reconstruction and novel view rendering (Agarwal et al., 2011) often rely on Structurefrom-Motion (SfM), multi-view stereo and graphic rendering (Losasso & Hoppe, 2004).\n\nLearning-based approaches have been widely used in 3D scene and object reconstruction (Sitzmann et al., 2019;Engelmann et al., 2021). They encode the feature through a deep neural network and learn various geometry representations, such as voxels (Kar et al., 2017;Sitzmann et al., 2019), patches (Groueix et al., 2018) and meshes Chen et al., 2021b).\n\n\nNEURAL RADIANCE FIELDS\n\nNeural Radiance Fields (NeRF) is proposed in (Mildenhall et al., 2020) as an implicit neural representation for novel view synthesis. Various types of NeRFs haven been proposed for acceleration. (Yu et al., 2021a;Rebain et al., 2021), better generalization abilities (Yu et al., 2021b Barron et al. (2022) produces poor results with blurred texture details and plenty of depth errors, (b) our S-NeRF can achieve accurate depth maps and fine texture details with fewer artifacts. (d) Our method can also be used for the reconstruction of moving vehicles which is impossible for previous NeRFs. It can synthesize better novel views compared with the mesh method Chen et al. (2021b).\n\nYang, 2021), new implicit modeling functions (Yariv et al., 2021;Wang et al., 2021a), large-scale scenes (Tancik et al., 2022;, and depth supervised training (Deng et al., 2022;Rematas et al., 2022).  ron et al., 2022) improves the Mip-NeRF for unbounded scenes by contracting the whole space into a bounded area to get a more representative position encoding.\n\nDepth supervised NeRF DS-NeRF (Deng et al., 2022) utilizes the sparse depth generated by COLMAP (Sch\u00f6nberger & Frahm, 2016) to supervise the NeRF training. PointNeRF (Xu et al., 2022) uses point clouds to boost the training and rendering with geometry constraints. DoN-eRF (Neff et al., 2021) realizes the ray sampling in a log scale and uses the depth priors to improve ray sampling. NerfingMVS (Wei et al., 2021) also instructs the ray sampling during training via depth and confidence. Dense depth priors are used in (Roessle et al., 2022) which are recovered from sparse depths.\n\nThese methods, however, are designed for processing small-scale objects or indoor scenes. Urban-NeRF (Rematas et al., 2022) uses accurate dense LiDAR depth as supervision to learn better reconstruction of the large-scale urban scenes. But these dense LiDAR depths are difficult and expensive to collect. The common data collected by self-driving cars (Caesar et al., 2019;Sun et al., 2020) can not be used in training Urban-NeRF because they only acquire noisy and sparse LiDAR points. In contrast, our S-NeRF can use such defect depths along with a learnable confidence measurement to learn better neural representations for the large-scale street-view synthesis.\n\n\nNERF FOR STREET VIEWS\n\nIn this section, we present our S-NeRF that can synthesize photo-realistic novel-views for both the large-scale background scenes and the foreground moving vehicles (Section 3.3). In the street views, there are many dynamic objects. In order to be used for self-driving simulation or VR applications, dynamic objects must be fully controllable and move in controlled locations, speed, and trajectories. Therefore, the background scenes and the foreground vehicles must be reconstructed separately and independently.\n\nWe propose a novel NeRF design that uses sparse noisy LiDAR signals to boost the robust reconstruction and novel street-view rendering. Pose refinement and the virtual camera transform are added to achieve accurate camera poses (Section 3.2). To deal with the outliers in the sparse LiDAR depths, we use a depth completion network Park et al. (2020) to propagate the sparse depth and employ a novel confidence measurement based on the robust reprojection and geometry confidence (Section 3.4). Finally, S-NeRF is trained using the proposed depth and RGB losses (Section 3.5).\n\n3.1 PRELIMINARY Neural Radiance Field (NeRF) represents a scene as a continuous radiance field and learns a mapping function f : (x, \u03b8) \u2192 (c, \u03c3). It takes the 3D position x i \u2208 R 3 and the viewing direction \u03b8 i as input and outputs the corresponding color c i with its differential density \u03c3 i . The mapping function is realized by two successive multi-layer perceptrons (MLPs).\n\nNeRF uses the volume rendering to render image pixels. For each 3D point in the space, its color can be rendered through the camera ray r(t) = o + td with N stratified sampled bins between the near and far bounds of the distance. The output color is rendered as:\nI(r) = N i=1 T i (1 \u2212 e \u2212\u03c3i\u03b4i )c i , T i = exp \uf8eb \uf8ed \u2212 i\u22121 j=1 \u03c3 j \u03b4 j \uf8f6 \uf8f8 (1)\nwhere o is the origin of the ray, T i is the accumulated transmittance along the ray, c i and \u03c3 i are the corresponding color and density at the sampled point t i . \u03b4 j = t j+1 \u2212 t j refers to the distance between the adjacent point samples. SfM (Sch\u00f6nberger & Frahm, 2016) used in previous NeRFs fails in computing camera poses for the self-driving data since the camera views have fewer overlaps (Figure 1(b)). Therefore, we proposed two different methods to reconstruct the camera poses for the static background and the foreground moving vheicles.\n\n\nCAMERA POSE PROCESSING\n\nBackground scenes For the static background, we use the camera parameters achieved by sensorfusion SLAM and IMU of the self-driving cars (Caesar et al., 2019;Sun et al., 2020) and further reduce the inconsistency between multi-cameras with a learning-based pose refinement network. We follow Wang et al. Wang et al. (2021b) to implicitly learn a pair of refinement offsets \u2206P = (\u2206R, \u2206T ) for each original camera pose P = [R, T ], where R \u2208 SO(3) and T \u2208 R 3 . The pose refine network can help us to ameliorate the error introduced in the SLAM algorithm and make our system more robust.\n\nMoving vehicles While the method proposed above is appropriate for the static background, camera pose estimation of moving objects is especially difficult due to the complicated movements of both the ego car and the target objects. As illustrated in Figure 4, we compute the relative position P between the camera and the target object. We use the center of the target object as the origin of the coordinate system. In the experiments, we use the 3D detectors (e.g. Yin et al. (2021)) to detect the 3D bounding box and the center of the target object.\n\nUsing the ego car's center as the coordinate system's origin. P b represents the position of the target object. P i is the position of the camera i (there are 5\u223c6 cameras on the ego car). We now transform  Figure 4: Illustration of our camera transformation process for moving vehicles. During the data collection, the ego car (camera) is moving and the target car (object) is also moving. The virtual camera system treats the target car (moving object) as static and then compute the relative camera poses for the ego car's camera. These relative camera poses can be estimated through the 3D object detectors. After the transformation, only the camera is moving which is favorable in training NeRFs.\n\nthe coordinate system by setting the target object's center as the coordinate system's origin (as illustrated in Figure 4).\nP i = (P i P \u22121 b ) \u22121 = P b P i \u22121 , P \u22121 = R T \u2212R T T 0 T 1 .(2)\nHere, P = R T 0 T 1 represents the old position of the camera or the target object.P i is the new relative camera position.\n\n\nREPRESENTATION OF STREET SCENES\n\nIn a self-driving system (e.g. Geosim Chen et al. (2021b)), vehicles on the road play the most important role in driving decisions. There are some important application scenarios (e.g. expressway) only contain cars. In this paper, we mainly focus on the background scenes and moving vehicles. Other rigid and nonrigid objects can also use the similar techniques.\n\nBackground scenes The street view sequences in nuScenes and Waymo datasets usually span a long range (>200m), so it is necessary to constrain the whole scene into a bounded range before the position encoding. The Normalized Device Coordinates (NDC) (Mildenhall et al., 2020) fails in our scenes due to the complicated motions of the ego car and the challenging camera settings ( Figure  1(b)).\n\nWe improve the scene parameterization function used in Barron et al. (2022) as:\nf (x) = x/r, if x \u2264 r, (2 \u2212 r x ) x x , otherwise.(3)\nWhere r is the radius parameter to decide the mapping boundary. The mapping is independent of the far bounds and avoids the heavy range compressing of the close objects, which gives more details in rendering the close objects (controlled by r). We use frustum from Mip-NeRF (Barron et al., 2021), sampling along rays evenly in a log scale to get more points close to the near plane.\n\nMoving Vehicles In order to train NeRF with a limited number of views (e.g. 2 \u223c 6 image views), we compute the dense depth maps for the moving cars as an extra supervision. We follow GeoSim (Chen et al., 2021b) to reconstruct coarse mesh from multi-view images and the sparse LiDAR points. After that, a differentiable neural renderer (Liu et al., 2019) is used to render the corresponding depth map with the camera parameter (Section 3.2). The backgrounds are masked during the training by an instance segmentation network (Wang et al., 2020).\n\n\nDEPTH SUPERVISION\n\nAs illustrated in Figure 3, to provide credible depth supervisions from defect LiDAR depths, we first propagate the sparse depths and then construct a confidence map to address the depth outliers. Our depth confidence is defined as a learnable combination of the reprojection confidence and the geometry confidence. During the training, the confidence maps are jointly optimized with the color rendering for each input view.\n\n\nLiDAR depth completion\n\nWe use NLSPN (Park et al., 2020) to propagate the depth information from LiDAR points to surrounding pixels. While NLSPN performs well with 64-channel LiDAR data (e.g. KITTI Geiger et al. (2012;2013) dataset), it doesn't generate good results with nuScenes' 32-channel LiDAR data, which are too sparse for the depth completion network. As a result, we accumulate neighbour LiDAR frames to get much denser depths for NLSPN. These accumulated LiDAR data, however, contain a great quantity of outliers due to the moving objects, ill poses and occlusions, which give wrong depth supervisions. To address this problem, we design a robust confidence measurement that can be jointly optimized while training our S-NeRF.\n\nReprojection confidence To measure the accuracy of the depths and locate the outliers, we first use an warping operation \u03c8 to reproject pixels X = (x, y, d) from the source images I s to the target image I t . Let P s , P t be the source and the target camera parameters, d \u2208 D s as the depth from the source view. The warping operation can be represented as:\nX t = \u03c8(\u03c8 \u22121 (X s , P s ), P t )\n(4) \u03c8 represents the warping function that maps 3D points to the camera plane and \u03c8 \u22121 refers to the inverse operation from 2D to 3D points. Since the warping process relies on depth maps D s , the depth outliers can be located by comparing the source image and the inverse warping one. We introduce the RGB, SSIM (Wang et al., 2004) and the pre-trained VGG feature (Simonyan & Zisserman, 2015) similarities to measure the projection confidence in the pixel, patch structure, and feature levels:\nC rgb = 1 \u2212 |I s \u2212\u00ce s |, C ssim = SSIM(I s ,\u00ce s )), C vgg = 1 \u2212 ||F s \u2212F s ||.(5)\nWhere\u00ce s = I t (X t ) is the warped RGB image, and theF s = F t (X t ) refers to the feature reprojection. The receptive fields of these confidence maps gradually expand from the pixels and the local patches to the non-local regions to construct robust confidence measurements.\n\nGeometry confidence We further impose a geometry constrain to measure the geometry consistency of the depths and flows across different views. Given a pixel X s = (x s , y s , d s ) on the depth image D s we project it to a set of target views using equation 4. The coordinates of the projected pixel X t = (x t , y t , d t ) are then used to measure the geometry consistency. For the projected depth d t , we compute its consistency with the original target view's depthd t = D t (x t , y t ):\nC depth = \u03b3(|d t \u2212d t )|/d s ), \u03b3(x) = 0, if x \u2265 \u03c4, 1 \u2212 x/\u03c4, otherwise.(6)\nFor the flow consistency, we use the optical flow method (Zhang et al., 2021) to compute the pixel's motions from the source image to the adjacent target views f s\u2192t . The flow consistency is then formulated as:\nC f low = \u03b3( \u2206 x,y \u2212 f s\u2192t (x s , y s ) \u2206 x,y ), \u2206 x,y = (x t \u2212 x s , y t \u2212 y s ).(7)\nWhere \u03c4 is a threshold in \u03b3 to identify the outliers through the depth and flow consistencies.\n\nLearnable confidence combination To compute robust confidence map, we define the learnable weights \u03c9 for each individual confidence metric and jointly optimize them during the training. The final confidence map can be learned as\u0108\n= i \u03c9 i C i , where i \u03c9 i = 1\nThe i \u2208 {rgb, ssim, vgg, depth, f low} represents the optional confidence metrics. The learnable weights \u03c9 adapt the model to automatically focus on correct confidence.\n\n\nLOSS FUNCTIONS\n\nOur loss function consists of a RGB loss that follows (Mildenhall et al., 2020;Barron et al., 2021;Deng et al., 2022), and a confidence-conditioned depth loss. To boost the depth learning, we also employ edge-aware smoothness constraints, as in (Li et al., 2021a;Godard et al., 2019), to penalize large variances in depth according to the image gradient |\u2202I|:  Table 1: Novel view synthesis results on foreground cars. We compare our method with the NeRF and GeoSim baselines. Since COLMAP fails on foreground vehicles, we apply our camera parameters to the NeRF baseline when training the static vehicles. We report the quantitative results on PSNR, SSIM (Wang et al., 2004) (higher is better) and LPIPS  (lower is better). Where R is a set of rays in the training set. For the reconstruction of the foreground vehicles, D refers to the depth. And in background scenes, D represents the disparity (inverse depth) to make the model focus on learning important close objects. \u03bb 1 and \u03bb 2 are two user-defined balance weights.\nL color = r\u2208R I(r) \u2212\u00ce(r) 2 2 (8) L depth = \u0108 \u00b7 |D \u2212D| (9) L smooth = |\u2202 xD | exp \u2212|\u2202xI| +|\u2202 yD | exp \u2212|\u2202yI|(10)L total = L color + \u03bb 1 L depth + \u03bb 2 L smooth(11)\n\nEXPERIMENTS\n\nWe perform our experiments on two open source self-driving datasets: nuScenes (Caesar et al., 2019) and Waymo (Sun et al., 2020). We compare our S-NeRF with the state-of-the-art methods (Barron et al., 2021;Rematas et al., 2022;Chen et al., 2021b). For the foreground vehicles, we extract car crops from nuScenes and Waymo video sequences. For the large-scale background scenes, we use scenes with 90\u223c180 images. In each scene, the ego vehicle moves around 10\u223c40 meters, and the whole scenes span more than 200m. We do not test much longer scenes limited by the single NeRF's representation ability. Our model can merge different sequences like Block-NeRF (Tancik et al., 2022) and achieve a larger city-level representation.\n\nIn all the experiments, the depth and smooth loss weight \u03bb 1 and \u03bb 2 are set to 1 and 0.15 respectively for foreground vehicles. And for background street scenes, we set \u03c4 = 20% for confidence measurement and the radius r = 3 in all scenes. \u03bb 1 = 0.2 and \u03bb 2 = 0.01 are used as the loss balance weights. More training details are available in the supplementary materials.\n\n\nNOVEL VIEW RENDERING FOR FOREGROUND VEHICLES\n\nIn this section, we present our evaluation results for foreground vehicles. We compare our method with the latest non-NeRF car reconstruction method (Chen et al., 2021b). Note that existing NeRFs (Mildenhall et al., 2020;Deng et al., 2022) cannot be used to reconstruct the moving vehicles. To compare with the NeRF baseline (Mildenhall et al., 2020), we also test our method on the static cars. Since COLMAP (Sch\u00f6nberger & Frahm, 2016) fails in reconstruct the camera parameters here, the same camera poses used by our S-NeRF are applied to the NeRF baseline to implement comparisons. Figure 5 and Table 1 show quantitative and visualized comparisons between our method and others in novel view synthesis. Optimizing NeRF baseline on a few (4\u223c7) image views leads to severe blurs and artifacts. GeoSim produces texture holes when warping textures for novel view rendering. The shapes of the cars are also broken due to the inaccurate meshes. In contrast, our S-NeRF shows more fine texture details and accurate object shapes. It can improve the PSNR and SSIM by 45\u223c65% compared with the NeRF and GeoSim baselines.   Moving vehicles As compared in Figure 2(c)\u223c2(d) and Table 1, novel view synthesis by GeoSim (Chen et al., 2021b) is sensitive to mesh errors that will make part of the texture missing or distorted during novel view warping and rendering. In contrast, our S-NeRF provides larger ranges for novel view rendering than geosim (see supplementary) and generates better synthesis results. S-NeRF can also simulate the lighting changing for different viewing directions, which is impossible for GeoSim. Furthermore, S-NeRF does not heavily rely on accurate mesh priors to render photorealistic views. The confidence-based design enables S-NeRF to eliminate the geometry inconsistency caused by depth outliers. S-NeRF surpasses the latest mesh based method (Chen et al., 2021b) by 45% in PSNR and 18% in SSIM.\n\n\nStatic vehicles\n\n\nNOVEL VIEW RENDERING FOR BACKGROUND SCENES\n\nHere we demonstrate the performance of our S-NeRF by comparing with the state-of-the-art methods Mip-NeRF (Barron et al., 2021), Urban-NeRF (Rematas et al., 2022) and Mip-NeRF 360 (Barron et al., 2022). We use the offical code of Mip-Nerf for evaluation and expand the hidden units of the MLP to 1024 (the same as ours). Since there is no official code published, we tried our best to reproduce (Rematas et al., 2022; Barron et al., 2022) based on our common settings. We test four nuScenes sequences and report the evaluation results in Table 2. Our method outperforms Mip-NeRF, Mip-NeRF 360 and Urban NeRF in all three evaluation metrics. We see significant improvements of 40% in PSNR and 26% in SSIM and a 45% reduction in LPIPS compared with the Mip-NeRF baseline. It also outperforms the current best Mip-NeRF 360 by 7.5% in PSNR, 4.5% in SSIM and 5% in LPIPS. We also show 360-degree panoramic rendering in Figure 6. S-NeRF significantly ameliorates artifacts, suppresses \"floaters\" and presents more fine details compared with Urban-NeRF and Mip-NeRF 360. Results of Waymo scenes are shown in the appendix.   \n\n\nBACKGROUND AND FOREGROUND FUSION\n\nThere are two different routes to realize controllable foreground and background fusion: depthguided placement and inpainting (e.g. GeoSim Chen et al. (2021b)) and joint NeRF rendering (e.g. GIRAFFE Niemeyer & Geiger (2021)). Both of them heavily rely on accurate depth maps and 3D geometry information for object placement, occlusion handling, etc.. Our method can predict far better depth maps and 3D geometry (e.g. meshes of cars) than existing NeRFs Barron et al., 2022). We provide a video (in the supplementary materials) to show the controlled placement of vehicles to the background using the depth-guided placement.\n\n\nABLATION STUDY\n\nTo further demonstrate the effectiveness of our method, we conduct a series of ablation studies with or without certain component. Table 3 shows the quantitative results of our model ablations. We evaluate our model on four foreground vehicles and two background scenes under different settings. Visualized results are also provided in Figure 7. More ablation studies are available in the appendix.\n\nFor background scene rendering, our RGB baseline outperforms the Mip-NeRF by 56% in meansquared error and 42% in structure similarity. Using inaccurate depth supervision without confidence leads to a drop of the accuracy due to the depth outliers. Confidence contributes to about 3% improvement in PSNR and SSIM. PSNR of our model slightly drops after adding the edge-aware smooth loss to it. However, it effectively suppresses \"floaters\" and outliers to improve our depth quality. For moving vehicles, the depth supervision and confidence measurement improves the RGB baseline by 18% and 8% in PSNR individually. The smoothness loss mainly improves the structure similarity and reduces the LPIPS errors.\n\n\nCONCLUSION, LIMITATIONS AND FUTURE WORK\n\nWe contribute a novel NeRF design for novel view rendering of both the large-scale scenes and foreground moving vehicles using the steet view datasets collected by self-driving cars. In the experiments, we demonstrate that our S-NeRF far outperforms the state-of-the-art NeRFs with higherquality RGB and impressive depth renderings. Though S-NeRF significantly outperforms Mip-NeRF and other prior work, it still produces some artifacts in the depth rendering. For example, it produces depth errors in some reflective windows. In the future, we will use the block merging as proposed in Block-NeRF (Tancik et al., 2022) to learn a larger city-level neural representation.  Method Overview The overview of the method is shown in Figure 8. We first propagate the sparse LiDAR points into a dense depth map and compute the geometry and the projection confidence maps. We use learnable combination to achieve the final confidence maps to reduce the influence of depth outliers. Computation of Confidence Figure 9 shows the process of computing confidence maps. We compute the depth confidence by reprojection the depth maps to other views. Given the LiDAR positions of the consecutive frames [P t\u22121 , P t , P t+1 ] at different time [t \u2212 1, t, t + 1], we are able to project the 3D LiDAR points at time t-1 and t+1 to t by \u2206P between P t and P t\u22121 /P t+1 . This is similar to the mapping function \u03c8 in Eq. (4) of the paper. Most outliers of the moving objects and other regions can be removed by the consistency check using the optical flow and the depth projection (Eq. (6-7) of the paper). The rest outliers can also be handled by the proposed confidenceguided learning.\n\n\nAPPENDIX\n\n\nA MORE ILLUSTRATIONS\n\nIllustration of Confidence Maps In Figure 10, we visualize different confidence components. Depth and optical flow confidence maps focus on geometry consistency between adjacent frames, while RGB, SSIM and VGG confidence maps compute the photometric and feature consistency.\n\nImages in the appendix are slightly compressed. Lossless images are available at https:// ziyang-xie.github.io/s-nerf\n\n\nConfidence Maps\n\nDepth Flow RGB SSIM VGG Geometry Confidence Projection Confidence Figure 10: Visualization of each confidence component. Brighter regions mean higher confidence. Geometry confidences (flow and depth) represents the geometry consistency as computed in Fig.  2(a.1). The projection confidence measure the photometric and feature consistency as computed in Fig. 2(a.2).\n\nOther Dynamic Objects. In Fig. 11, we reconstruct a moving truck using only 4 image views.\n\nThe novel-view rendering quality (Fig. 11) is good enough for our driving simulation. For the dynamic person, since only a limited number of image views can be captured for a single person in the nuScenes and Waymo datasets, and the person is also walking with varying poses. It's difficult to reconstruct high-quality 3D person using only a few (2-5) views. We instead use existing monocular video data to reconstruct 3D person and rendering novel views with novel poses. Fig. 12 shows examples using Anim-NeRF Chen et al. (2021a) to reconstruct persons in People-Snapshot dataset Alldieck et al. (2018). Then, we can put the 3D persons (with novel views and novel poses) to the S-NeRF scenes for the realistic driving simulation (Fig. 13). Similarly, other objects (e.g. bicycles) can be first reconstructed and then combined into our S-NeRF scenes for driving simulation.   32-channel LiDAR data, which are too sparse to complete the depth. We accumulate 5\u223c10 neighbour LiDAR frames to get a much denser LiDAR data for the NLSPN network. These accumulated LiDAR points, however, contains many outliers due to the moving objects, ill poses, occlusions and reprojection errors. These outliers will give wrong depth information. To remove these outliers, we first compute the optical flow Zhang et al. (2021) of one RGB image using its neighbour frame. After that, we reproject each LiDAR points to neighbour image plane to get the LiDAR flow, and then compare two kinds of flows to locate the LiDAR outliers using a threshold of 20% (following Eq. (6\u223c8) of the paper).\n\nApplying above procedure, we can remove many outliers. However, some of these outliers still exists due to the errors of the optical flow and ill poses. These outliers still exist after depth completion. The depth completion algorithm also introduces new outliers to the final dense depth map, which are great challenges for depth supervision. We therefore learn an confidence metric for more robust depth supervision.\n\n\nB.2 REMOVE MOVING OBJECTS IN STREET VIEWS\n\nCurrently, we focus on static scenes. When training the background scenes, we masked the moving objects, while static objects (e.g.static vehicles) are kept and trained along with the background. The moving vehicles are trained independently. Other moving objects (e.g.person) can be removed by instance segmentation and optical flow.\n\n\nB.3 FOREGROUND VEHICLES\n\nFor foreground vehicles, we use four layers with 256 hidden units in the MLP. The depth and smooth loss weights \u03bb 1 and \u03bb 2 are set to 1 and 0.15 respectively. We sample 64 times along each ray during the RGB and depth rendering. We train our S-NeRF for 30k iterations using Adam optimizer with 5 \u22124 as the learning rate and 1024 as the batch size. The training takes about 2 hours for each vehicle on a single RTX3090 gpu. For each vehicle, there are around 2\u223c8 views used for training and 1\u223c3 views for testing.\n\n\nB.4 STREET VIEWS (BACKGROUND)\n\nFor background street scenes, we set \u03c4 = 20% for the confidence measurement and the radius r = 3 for the scene parameterization function. Our coarse-fine network share the same parameters like mip-NeRF. The density MLP has eight layers with 1024 hidden units and the color MLP consists of three layers with 128 hidden units. We keep this setting for all evaluation methods for fair comparisons. We train our S-NeRF for 100K iterations using Adam optimizer with a batch size of 2048. The learning rate is reduced log-linearly from 5\u00d7 10 \u22124 to 5\u00d7 10 \u22126 with a warm-up phase of 2500 iterations. \u03bb 1 = 0.2 and \u03bb 2 = 0.01 are used as the loss balance weights. We sample 128 times along each ray in a log scale. Our S-NeRF is trained on two RTX3090 GPUs which takes about 17 hours for a scene with about 250 images (with a resolution of 1280\u00d71920).\n\n\nC EXPERIMENTS\n\nWe present more visualized results in Figure 14 and Figure 16. We also provide a video demo for performance illustration.   \n\n\nC.1 PARAMETERS AND EFFICIENCY\n\nIn the experimennts, we use the same settings for the MLP encoding in our S-NeRF and other stateof-the-art methods Barron et al. (2021;; Rematas et al. (2022). There are 8.76M learnable parameters in our S-NeRF that is similar to other state-of-the-art methods Barron et al. (2021;; Rematas et al. (2022) (8.7\u223c9.9M).\n\nAll the methods are trained for 100k iterations. Our method takes about 17 hours in training for one street scene. This is the same as Mip-NeRF and Urban-NeRF because we use the same settings during the experiments. Mip-NeRF 360 is faster than ours because it doesn't require the coarse rendering for supervision. This strategy can also be used in our S-NeRF to accelerate the convergence and further improve the quality of the novel view rendering.\n\n\nC.2 WAYMO RESULTS\n\nWe also test our S-NeRF on two Waymo street-view sequences. The results are reported in Table  4 and visually compared with the state-of-the-arts Rematas et al. (2022); Barron et al. (2022) in Figure 15 and 16. Waymo dataset use a 64-channel LiDAR and five cameras for capture driving scenes. Our S-NeRF outperforms Mip-NeRF by 40% in PSNR, 80% in SSIM and 44% in LPIPS. It's also far better than the Urban-NeRF baseline (32\u223c50% \u2191 in PSNR, SSIM and LPIPS) which also uses the sparse LiDAR depth as supervision. Compared with the current best Mip-NeRF 360, our method also achieves a 6.8% improvements in PSNR and a 5.2% reduction of the LPIPS error. More importantly, our S-NeRF predicts much more accurate depths for the large-scale street views.\n\nWe also use Waymo dataset to compare our method with the NeRF baseline on the vehicle synthesis.\n\nThe quantitive evaluations are shown in Table 5 and visually compared in Figure 14. Our method outperforms the NeRF baseline by 63% in PSNR, 18% in SSIM and 51% in LPIPS. Compared with the recently proposed mesh-based car reconstruction method Chen et al. (2021b), our S-NeRF improves the PSNR by 62% and reduces the LPIPS error by 16%.\n\n\nC.3 COMPARISONS WITH URBAN-NERF\n\nUrban-NeRF also use the LiDAR depth to boost the NeRF training. However, it requires accurate LiDAR depth. Most of the street view datasets (e.g.Waymo and nuScenes) contains plenty of depth  Table 7: Ablations on loss balance weights \u03bb 1 , \u03bb 2 for foreground vehicles.\n\noutliers due to the influence of ill poses, reprojection errors, occlusion and moving objects. We find that these depth outliers can heavily influence the rendering quality of Urban-NeRF. Our S-NeRF introduces confidence metrics to make it more robust to the depth outliers. Moreover, our S-NeRF improves the scene parameterization function and the camera poses to learn better neural representation for rendering large-scale street views. As shown in Figure 16 and 15, compared with Urban-NeRF, our S-NeRF gives more details in street-view objects (such as the car, the tree and road lines) and are more robust for the large-scale street views captured by the self-driving cars.\n\n\nC.4 MORE ABLATION STUDY\n\nIn this section, we use two nuScenes street-view scenes and three foreground vehicles to test the effects of different settings when training our S-NeRF. These include different loss balance weights, different confidence components, different depth qualities and different scene parameterizations. In these ablation experiments, S-NeRF is trained for 30k iterations.\n\n\nLoss balance weights\n\nAs shown in Table 6 and 7, we compare the performance of our S-NeRF using different loss balance weights. Our S-NeRF is not very sensitive to the changes of the loss balance weights. Using larger or smaller \u03bb 1 or \u03bb 2 just slightly reduces the PSNR and SSIM by 0.2\u223c1% on the background novel view rendering. [0.1, 0.4] (for \u03bb 1 ) and (0.005, 0.02) (for \u03bb 2 ) are the reasonable range for our loss balance weights for training street background views. For the foreground vehicles, [0.5, 2] (for \u03bb 1 ) and (0.075, 0.015) (for \u03bb 2 ) are the reasonable range.\n\n\nConfidence components\n\nWe also test the performances of our S-NeRF when using different confidence components (geometry and reprojection measurements) for learning our confidence metric. As shown in Table 8, when we remove the reprojection confidence module, the PSNR slightly dropped by 0.4%. And when we train S-NeRF without geometry confidence, the PSNR and SSIM is about 0.7% lower. We also test the effects of the threshold \u03c4 used in the geometry confidence (Eq. (7) of the paper). We find that the geometry confidence is not sensitive to the threshold \u03c4 . [10%, 40%] is a reasonable range for the threshold \u03c4 .   For the foreground vehicles, we only use RGB and SSIM confidences. This is because the depth map used in training vehicles are relatively better than the backgrounds. Thus, we do not need strong geometry confidences. We report these ablation results in Table 9 by using only RGB confidence or SSIM confidence. We find that using only RGB or SSIM confidence could achieve a little better PSNR (1\u223c2%\u2191), but a relative worse LPLPS (6.6 \u223c 11%\u2193). Taken all these three evaluation metrics into consideration, using both RGB and SSIM confidence gives a better performance in training our S-NeRF. We also report depth error rate in Table 10. For the accurate depths, it predicts high confidence to encourage the NeRF geometry to be consistent with the LiDAR signal.\n\n\nDepth quality\n\nAs shown in Table 12, we study the effects of different depth map qualities. We train our S-NeRF using depth map in different qualities. To simulate the depth errors in different qualities, we add random Gaussian Noise to the original depth map inputs. The strength of the noise (the quality of the depths) is measured by PSNR and error rates compared with the original depth inputs. Error rate means how many outliers are introduced by the noise. We use threshold = 1 to compute the outlier rates compared with the original depth inputs. Our method achieve similar rendering qualities when training with light noises, which means the light noises doesn't influence the performance of our S-NeRF. When training with strong noises, the PSNR and SSIM just slightly dropped by 0.05\u223c0.5%. This shows that our method is robust to depth noises because our confidence strategy can locate and measure the depth outliers accurately and avoid the negative influence of the depth noises in training our S-NeRF.\n\nBesides, we also test the performance of our S-NeRF when using only sparse depth for supervision (Table 11). It performs better than our RGB-only S-NeRF (improving the PSNR by 5%). But, it is 2% worse in PSNR and 8% worse in LPIPS than our default settings where dense depth map and the proposed confidence metric are used. This means that dense depth maps can provide more useful geometry information for training our S-NeRF even though they may contain more depth outliers.\n\nIn addition, we also test another two worse depth completion methods by replacing the NLSPN with Ku et al.    We studied the effects of using different radius in the scene parameterization function (Eq.\n\n(2) of the paper). Radius r is used to constrain the whole large-scale scene into a bounded range. Close and far points are parameterized by different distance mapping function to make our S-NeRF able to keep more details for the close objects. This is controlled by the radius parameter. In Table 14, we report the performance of our S-NeRF when using different radius for scene parameterization. We find that choosing r in 3\u223c10m could produce a better results than the original setting in Mip-Nerf 360Barron et al. (2022). This is because the street views in nuScenes and Waymo datasets usually span a long range (>200m). While, Mip-Nerf 360Barron et al. (2022) do not have r to adjust the scene parameterization as ours.\n\nInfluence of quality of 3D detection results As shown in Table 15, the quality of 3D object detector does not significantly influence the rendering quality. This is because we use pose refinement to improve the inaccurate initial results of the virtual camera pose estimation. The pose refinement is guided by depth supervision and visual multiview constraints in training our S-NeRF. We test the rendering results by using 3D bounding boxes of different qualities (mIoU: 0.79, 0.67 and 0.55) and compare them with the ground truth bounding boxes (achieved from the dataset labels). We find that the PSNR doesn't drop significantly compared with the ground truth bounding boxes. When the mIoU of the bounding boxes is 0.79 (using the default detector), the PSNR just slightly drops by 0.23, the SSIM drops by 0.04, and the LPIPS doesn't change. When we use poor detection results, our S-NeRF still produces good results with a slight drop in the rendering quality. (E.g. 0.49 \u2193when the mIoU is 0.67 and 0.97 \u2193for a poor mIoU of 0.55). We observed that the detected bounding boxes ( > 0.5 in IoU) are good enough for training our S-NeRF. This can be easily realized by many existing 3D object detectors.\n\nDifferent pose refinements We have tried different pose refinements like NeRF-, BARF and SC-NeRF. As shown in Table 17, We find that NeRF-helps achieve the best quality when training on the self-driving dataset. Possibly because NeRF-is more straightforward. It directly learns shifts to the translation and rotation. The shifts are relatively easier to learn under depth supervision. It's   also possible that such shifts are easier to estimate when there is a small overlapped field of view between different cameras (as illustrated in Figure 1 of the paper.)\n\nTraining and Inference time Referring to Table 16, we show more details of our method against the existing large-scale NeRFs with the training time and inference time. Our method (default settings) uses the same training setting and network parameters as those of MipNeRF. They take the same time in training and inference. When we implement the MipNeRF-360 strategy (the distillation mode, all other settings are kept the same), the light proposal MLP reduces both the training time and the inference time. It runs 30% faster in training and 40% faster in inference than the default settings (with a drop of 0.33 in PSNR).\n\n\nD LIMITATIONS AND SOCIAL IMPACTS\n\nFailure cases. Considering that the ego vehicle sometimes goes very fast, fewer images are captured by the side (left or right) cameras. Some objects/contents only appeared in one or two left/right images. This makes it hard to render high-quality left and right novel views. As shown in Table 18, since there are fewer views, the rendered side (left/right) views report worse PSNR, SSIM and LPIPS ( 5 \u223c 12% \u2193) compared with the front and back views. In the future, we will try some techniques (e.g. using a better method to densify depth or provide semantic supervision) to further improve the quality of rendering left and right views.\n\nPotential risks. Since our method can be used to render realistic street views and vehicles, it's possible that S-NeRF can be used to synthesize fake photos or videos. We therefore hope our S-NeRF could be used with cautiousness. Work that bases itself on our method should also carefully consider the consequences of this potential negative social influence.       \n\nFigure 1 :\n1Problem illustration. (a) Conventional NeRFs Mildenhall et al. (2020); Barron et al. (2021) require object-centric camera views with large overlaps. (b) In the challenging large-scale outdoor driving scenes Caesar et al. (2019); Sun et al. (\n\nFigure 2 :\n2Performance illustration in novel view rendering on a challenging nuScenes scene Caesar et al. (2019), (a) the state-of-the-art method\n\n( a )Figure 3 :\na3Noisy sparse points (b) Our depth supervision (c) Learned confidence (d) Our depth rendering Depth supervision and rendering.\n\nFigure 5 :\n5Novel-view synthesis results for static foreground vehicles. Results are reconstructed from 4\u223c7 views. Our method outperforms others(Chen et al., 2021b; Mildenhall et al., 2020)  with more texture details and accurate shapes.\n\nFigure 6 :\n6We render the 360 degree panoramas (in a resolution of 8000\u00d7800) for comparisons (zoom in to see details). Significant improvements are highlighted by red rectangles and cropped patches are shown to highlight details. See the appendix for more results.\n\nFigure 7 :\n7Ablation study on different training settings of (a) RGB only, (b) noisy depth supervision, (c) using depth confidence, and (d) full supervision settings.\n\nFigure 8 :\n8Overview of our S-NeRF framework.\n\nFigure 9 :\n9Illustration of the confidence computation.\n\nFigure 11 :\n11Novel view rendering of a reconstructed moving truck.\n\nFigure 12 :\n12Novel-view and novel-pose rendering for the reconstructed dynamic persons.\n\nFigure 13 :\n13After novel-view and novel-pose rendering for the reconstructed persons, we also combine the dynamic persons into our rendered S-NeRF scenes. NLSPN[Park et al. (2020)] network for depth completion, which propagates the depth information from sparse LiDAR points to surrounding pixels. While NLSPN performs well with 64-channel LiDAR data, such as KITTIGeiger et al. (2012; 2013) and waymoSun et al. (2020); Ettinger et al. (2021) datasets, it doesn't generate good results with nuScenesCaesar et al. (2019)\n\n\n(2018); Van Gansbeke et al. (2019)(\n\nFigure 14 :Figure 15 :Figure 16 :\n141516Comparisons with NeRF baseline for foreground car rendering. Four different novel views are rendered for five different cars. Our S-NeRF significantly reduce the \"floats\", blurs and other artifacts.(a) Urban NeRF depth & RGB (b) Mip-NeRF 360 depth & RGB (c) ours depth & RGB Comparisons with state-of-the-art Mip-NeRF 360 Barron et al. (2022) and Urban-NeRF Rematas et al. (2022). We render more 180 degree panorama views for visual comparisons. Scenes are from the Waymo datasets.\n\n\nLarge-scale NeRF Many NeRFs have been proposed to address the challenges of large-scale outdoor scenes. NeRF in the wild (Martin-Brualla et al., 2021) applies appearance and transient embeddings to solve the lighting changes and transient occlusions. Neural plenoptic sampling (Li et al., 2021b) proposes to use a Multi-Layer Perceptron (MLP) as an approximator to learn the plenoptic function and represent the light-field in NeRF. Mip-NeRF (Barron et al., 2021) develops a conical frustum encoding to better encode the scenes at a continuously-valued scale. Using Mip-NeRF as a base block, Block-NeRF (Tancik et al., 2022) employs a block-combination strategy along with pose refinement, appearance, and exposure embedding on large-scale scenes. Mip-NeRF 360 (Bar\n\nTable 2 :\n2Our method quantitatively outperforms state-of-the-art methods. Methods are tested on four nuScenes Sequences. Average PSNR, SSIM and LPIPS are reported.\n\nTable 3 :\n3Ablations on different settings. For background scenes, we use two nuScenes sequences for evaluations. For moving vehicles, four cars are trained under different settings.(a) RGB only \n(b) w/o depth confidence \n(c) w/o smooth loss \n(d) Full Settings \n\n\n\nTable 4 :\n4Our method quantitatively outperforms state-of-the-art methods Barron et al. (2021; 2022); \nRematas et al. (2022). Methods are trained on two waymo scenes. Average PSNR, SSIM and LPIPS \nare reported. \n\nForeground Vehicles \nMethods \nPSNR\u2191 SSIM\u2191 LPIPS\u2193 \nNeRF Mildenhall et al. (2020) \n14.22 \n0.739 \n0.32 \nGeoSim Chen et al. (2021b) \n14.27 \n0.742 \n0.186 \nOurs \n23.16 \n0.870 \n0.156 \n\n\n\nTable 5 :\n5Novel view synthesis results on foreground cars from Waymo dataset. We compare our \nmethod with the NeRF and GeoSim baselines. Since COLMAP fails on foreground vehicles, we \napply our camera parameters to the NeRF baseline when training the static vehicles. We report the \nquantitative results on PSNR, SSIM Wang et al. (2004) (higher is better) and LPIPS Zhang et al. \n(2018) (lower is better). \n\n\n\nTable 8 :\n8Ablations on confidence settings for novel street-view rendering (background)..RGB confidence \nSSIM confidence \nPSNR\u2191 \nSSIM\u2191 \nLPIPS\u2193 \n\nYes \nNo \n19.79 \n0.803 \n0.152 \nNo \nYes \n19.97 \n0.807 \n0.145 \n\nYes \nYes \n19.64 \n0.803 \n0.136 \n\n\n\nTable 9 :\n9Ablations on confidence components for foreground vehicles.\n\nTable 13 )\n13. Even using the worse traditional depth completion methodKu et al. (2018)(without learning a deep neural network), our S-NeRF still achieves a similar rendering quality (24.41\u219224.40). This experiment shows that our method doesn't rely on NLSPN depth quality. Benefiting from our confidence-guided depth supervision, many other depth completion methods can also be used in our S-NeRF.Scene and ray parameterization \n\n\nTable 10 :\n10Ablation study on confidence components.Depth Settings \nPSNR\u2191 \nSSIM\u2191 \nLPIPS\u2193 \n\nNo Depth \n22.45 \n0.742 \n0.433 \nSparse \n23.65 \n0.756 \n0.415 \nDense w/o confidence \n23.15 \n0.757 \n0.424 \n\nDense w/ confidence \n24.05 \n0.771 \n0.384 \n\n\n\nTable 11 :\n11Ablations on depth supervision for novel street-view rendering (background).\n\nTable 12 :\n12We study the effects when training our S-NeRF with depth map in different qualities. We add random Gaussian Noise to the original input depth maps. The strength of the noise is measured as PSNR and error rates. Error rates represent how many outliers are introduced by the noise. Our S-NeRF is robust to depth noises.Method \nMean absolute error/[mm] (KITTI) \nRank on KITTI leaderboard \nPSNR \nSSIM \nLPIPS \n\nTraditional method Ku et al. (2018) \n302.60 \n116 \n24.40 \n0.782 \n0.344 \nVan Gansbeke et al. (2019) \n215.02 \n61 \n24.50 \n0.785 \n0.344 \nOur default NLSPN \n199.59 \n40 \n24.41 \n0.783 \n0.345 \n\n\n\nTable 13 :\n13Effects of different depth completion methods\n\nTable 14 :\n14Ablations on radius r in our scene parameterization function (EQ. (14) of the paper) for novel street-view rendering (background).Bounding box types \nmIoU \nPSNR \nSSIM \nLPIPS \n\nGT bouding box \n1.0 \n23.50 \n0.862 \n0.111 \nCenterPoints Yin et al. (2021) \n0.787 \n23.27 \n0.858 \n0.111 \npoor bounding box \n0.672 \n23.01 \n0.845 \n0.126 \npoor bounding box \n0.554 \n22.53 \n0.841 \n0.129 \n\n\n\nTable 15 :\n15Performance of our S-NeRF by using 3D bounding boxes in different qualities (mIoU).Method \nTraining time \nInference time (resolution) \nPSNR \n\nMipNeRF \n17 hours \n370s \n17.34 \nMipNeRF-360 \n12 hours \n210s \n23.17 \nOurs \n17 hours \n370s \n25.68 \nOurs with distill mode of MipNeRF-360 \n12 hours \n210s \n25.35 \n\n\n\nTable 16 :\n16Evaluation of the training time and the inference speed (on an RTX A6000 GPU).Method \nPSNR \nSSIM \nLPIPS \n\nBARF Lin et al. (2021) \n22.23 \n0.751 \n0.451 \nBARF w/o initialization \n12.16 \n0.527 \n0.634 \nSCNeRF Jeong et al. (2021) \n25.15 \n0.784 \n0.377 \nSCNeRF w/o initialization \n14.24 \n0.578 \n0.583 \n\nOurs \n25.68 \n0.788 \n0.375 \n\n\n\nTable 17 :\n17Comparisions of NeRF-, BARF and SCNeRF in training our S-NeRFCamera \nPSNR \nSSIM \nLPIPS \n\nFront/Back views \n22.22 \n0.731 \n0.389 \nLeft/Right views \n21.17 \n0.681 \n0.498 \n\n\n\nTable 18 :\n18Rendering qualities of the front views and side views.\nAcknowledgments This work was supported in part by National Natural Science Foundation of China (Grant No. 62106050), Lingang Laboratory(Grant No.LG-QS-202202-07), Natural Science Foundation of Shanghai (Grant No. 22ZR1407500).\nBuilding rome in a day. Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, M Steven, Richard Seitz, Szeliski, Communications of the ACM. 2Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M Seitz, and Richard Szeliski. Building rome in a day. Communications of the ACM, 2011. 2\n\nVideo based reconstruction of 3d people models. Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, Gerard Pons-Moll, CVPR. Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, and Gerard Pons-Moll. Video based reconstruction of 3d people models. In CVPR, pp. 8387-8397, Jun 2018. 14\n\nMip-nerf: A multiscale representation for anti-aliasing neural radiance fields. Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P Srinivasan, ICCV. 816Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In ICCV, 2021. 1, 2, 3, 5, 6, 7, 8, 16\n\nMip-nerf 360: Unbounded anti-aliased neural radiance fields. Jonathan T Barron, Ben Mildenhall, Dor Verbin, P Pratul, Peter Srinivasan, Hedman, CVPR. 1623Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In CVPR, 2022. 1, 2, 3, 5, 7, 8, 9, 16, 19, 23\n\nHolger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, Oscar Beijbom, nuscenes: A multimodal dataset for autonomous driving. 715arXiv preprintHolger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. arXiv preprint, 2019. 1, 2, 3, 4, 7, 15\n\nAnimatable neural radiance fields from monocular rgb videos. Jianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao Bao, Xu Jia, Huchuan Lu, 2021a. 14Jianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao Bao, Xu Jia, and Huchuan Lu. Ani- matable neural radiance fields from monocular rgb videos, 2021a. 14\n\nGeosim: Realistic video simulation via geometry-aware composition for self-driving. Yun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang, Xinchen Yan, Sivabalan Manivasagam, Shangjie Xue, Ersin Yumer, Raquel Urtasun, CVPR. 16Yun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang, Xinchen Yan, Sivabalan Manivasagam, Shangjie Xue, Ersin Yumer, and Raquel Urtasun. Geosim: Realistic video simulation via geometry-aware composition for self-driving. In CVPR, 2021b. 2, 3, 5, 7, 8, 9, 16\n\nDepth-supervised NeRF: Fewer views and faster training for free. Kangle Deng, Andrew Liu, Jun-Yan Zhu, Deva Ramanan, CVPR. 67Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised NeRF: Fewer views and faster training for free. In CVPR, 2022. 2, 3, 6, 7\n\nFrom points to multi-object 3d reconstruction. Francis Engelmann, Konstantinos Rematas, Bastian Leibe, Vittorio Ferrari, CVPR. Francis Engelmann, Konstantinos Rematas, Bastian Leibe, and Vittorio Ferrari. From points to multi-object 3d reconstruction. In CVPR, 2021. 2\n\nLarge scale interactive motion forecasting for autonomous driving: The waymo open motion dataset. Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp, Yin Charles R Qi, Zhou, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision15Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp, Charles R Qi, Yin Zhou, et al. Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pp. 9710-9719, 2021. 15\n\nSparse and noisy lidar completion with rgb guidance and uncertainty. Davy Wouter Van Gansbeke, Bert Neven, Luc De Brabandere, Van Gool, 2019 16th international conference on machine vision applications (MVA). IEEE1820Wouter Van Gansbeke, Davy Neven, Bert De Brabandere, and Luc Van Gool. Sparse and noisy lidar completion with rgb guidance and uncertainty. In 2019 16th international conference on machine vision applications (MVA), pp. 1-6. IEEE, 2019. 18, 20\n\nPixel2mesh: Generating 3d mesh models from single rgb images. Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, Yu-Gang Jiang, In ECCV. 2Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh models from single rgb images. In ECCV, 2018. 2\n\nNeus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, Wenping Wang, NeurIPS. Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. In NeurIPS, 2021a. 3\n\nSolov2: Dynamic and fast instance segmentation. Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, Chunhua Shen, NeurIPS. Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast instance segmentation. In NeurIPS, 2020. 5\n\nImage quality assessment: from error visibility to structural similarity. A C Zhou Wang, H R Bovik, E P Sheikh, Simoncelli, IEEE TIP. 7616Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE TIP, 2004. 6, 7, 16\n\nZirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, Victor Adrian Prisacariu, 2021b. 4NeRF\u2212\u2212: Neural radiance fields without known camera parameters. arXiv preprintZirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. NeRF\u2212\u2212: Neural radiance fields without known camera parameters. arXiv preprint, 2021b. 4\n\nNerfingmvs: Guided optimization of neural radiance fields for indoor multi-view stereo. Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu, Jie Zhou, arXiv preprintYi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu, and Jie Zhou. Nerfingmvs: Guided optimization of neural radiance fields for indoor multi-view stereo. arXiv preprint, 2021. 3\n\nDISN: deep implicit surface network for high-quality single-view 3d reconstruction. Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radom\u00edr Mech, Ulrich Neumann, NeurIPS. Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radom\u00edr Mech, and Ulrich Neumann. DISN: deep implicit surface network for high-quality single-view 3d reconstruction. In NeurIPS, 2019. 2\n\nQiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, Ulrich Neumann, Point-nerf: Point-based neural radiance fields. 2022arXiv preprintQiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neu- mann. Point-nerf: Point-based neural radiance fields. arXiv preprint, 2022. 3\n\nVolume rendering of neural implicit surfaces. Lior Yariv, Jiatao Gu, Yoni Kasten, Yaron Lipman, NeurIPS. Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. In NeurIPS, 2021. 3\n\nCenter-based 3d object detection and tracking. Tianwei Yin, Xingyi Zhou, Philipp Kr\u00e4henb\u00fchl, CVPR, 2021. 421Tianwei Yin, Xingyi Zhou, and Philipp Kr\u00e4henb\u00fchl. Center-based 3d object detection and tracking. In CVPR, 2021. 4, 21\n\nPlenoctrees for real-time rendering of neural radiance fields. Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, Angjoo Kanazawa, ICCV. Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for real-time rendering of neural radiance fields. In ICCV, 2021a. 2\n\nAlex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa, Neural radiance fields from one or few images. CVPRAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In CVPR, 2021b. 2\n\nSeparable flow: Learning motion cost volumes for optical flow estimation. Feihu Zhang, Oliver J Woodford, Philip H S Victor Adrian Prisacariu, Torr, ICCV. 615Feihu Zhang, Oliver J. Woodford, Victor Adrian Prisacariu, and Philip H.S. Torr. Separable flow: Learning motion cost volumes for optical flow estimation. In ICCV, 2021. 6, 15\n\nKai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun, Nerf++: Analyzing and improving neural radiance fields. 39arXiv preprintKai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving neural radiance fields. arXiv preprint, 2020. 3, 9\n\nThe unreasonable effectiveness of deep features as a perceptual metric. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang, CVPR. 716Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018. 7, 16\n", "annotations": {"author": "[{\"end\":81,\"start\":51},{\"end\":113,\"start\":82},{\"end\":142,\"start\":114},{\"end\":178,\"start\":143},{\"end\":207,\"start\":179}]", "publisher": null, "author_last_name": "[{\"end\":61,\"start\":58},{\"end\":93,\"start\":88},{\"end\":122,\"start\":120},{\"end\":154,\"start\":149},{\"end\":187,\"start\":182}]", "author_first_name": "[{\"end\":57,\"start\":51},{\"end\":87,\"start\":82},{\"end\":119,\"start\":114},{\"end\":148,\"start\":143},{\"end\":181,\"start\":179}]", "author_affiliation": "[{\"end\":80,\"start\":63},{\"end\":112,\"start\":95},{\"end\":141,\"start\":124},{\"end\":177,\"start\":156},{\"end\":206,\"start\":189}]", "title": "[{\"end\":48,\"start\":1},{\"end\":255,\"start\":208}]", "venue": null, "abstract": "[{\"end\":1676,\"start\":301}]", "bib_ref": "[{\"end\":1740,\"start\":1715},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2278,\"start\":2257},{\"end\":2307,\"start\":2283},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2872,\"start\":2851},{\"end\":2950,\"start\":2929},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3071,\"start\":3050},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3525,\"start\":3504},{\"end\":3554,\"start\":3530},{\"end\":3615,\"start\":3582},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4126,\"start\":4106},{\"end\":4145,\"start\":4128},{\"end\":4599,\"start\":4578},{\"end\":4620,\"start\":4599},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4808,\"start\":4787},{\"end\":4837,\"start\":4813},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5702,\"start\":5681},{\"end\":5719,\"start\":5702},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5828,\"start\":5807},{\"end\":5849,\"start\":5828},{\"end\":6126,\"start\":6095},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6146,\"start\":6126},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6164,\"start\":6146},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6243,\"start\":6224},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6420,\"start\":6398},{\"end\":6526,\"start\":6503},{\"end\":6638,\"start\":6615},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6661,\"start\":6638},{\"end\":6794,\"start\":6776},{\"end\":6816,\"start\":6794},{\"end\":6848,\"start\":6826},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6879,\"start\":6860},{\"end\":6977,\"start\":6952},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7120,\"start\":7102},{\"end\":7140,\"start\":7120},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7191,\"start\":7174},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7212,\"start\":7192},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7586,\"start\":7567},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7654,\"start\":7634},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7673,\"start\":7654},{\"end\":7715,\"start\":7694},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7766,\"start\":7747},{\"end\":7787,\"start\":7766},{\"end\":7807,\"start\":7790},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8000,\"start\":7981},{\"end\":8074,\"start\":8047},{\"end\":8134,\"start\":8117},{\"end\":8243,\"start\":8216},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8365,\"start\":8347},{\"end\":8493,\"start\":8471},{\"end\":8658,\"start\":8625},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8907,\"start\":8886},{\"end\":8924,\"start\":8907},{\"end\":10091,\"start\":10073},{\"end\":11312,\"start\":11281},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11775,\"start\":11754},{\"end\":11792,\"start\":11775},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11940,\"start\":11921},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12688,\"start\":12671},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13867,\"start\":13848},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14644,\"start\":14624},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14998,\"start\":14977},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15297,\"start\":15277},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15630,\"start\":15611},{\"end\":16136,\"start\":16111},{\"end\":16298,\"start\":16272},{\"end\":16303,\"start\":16298},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17544,\"start\":17525},{\"end\":17605,\"start\":17577},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18715,\"start\":18695},{\"end\":19558,\"start\":19533},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19578,\"start\":19558},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19596,\"start\":19578},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19742,\"start\":19724},{\"end\":19762,\"start\":19742},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20154,\"start\":20135},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20779,\"start\":20758},{\"end\":20808,\"start\":20784},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20887,\"start\":20866},{\"end\":20908,\"start\":20887},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20927,\"start\":20908},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21996,\"start\":21976},{\"end\":22048,\"start\":22023},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22066,\"start\":22048},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23056,\"start\":23036},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23712,\"start\":23692},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23936,\"start\":23915},{\"end\":23971,\"start\":23938},{\"end\":24010,\"start\":23976},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24247,\"start\":24227},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25121,\"start\":25102},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25437,\"start\":25417},{\"end\":27373,\"start\":27341},{\"end\":29862,\"start\":29843},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29935,\"start\":29913},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30639,\"start\":30620},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":33428,\"start\":33408},{\"end\":33451,\"start\":33430},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":33574,\"start\":33554},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":34271,\"start\":34251},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":35192,\"start\":35173},{\"end\":40826,\"start\":40794},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":45174,\"start\":45154}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":44715,\"start\":44461},{\"attributes\":{\"id\":\"fig_1\"},\"end\":44863,\"start\":44716},{\"attributes\":{\"id\":\"fig_2\"},\"end\":45008,\"start\":44864},{\"attributes\":{\"id\":\"fig_3\"},\"end\":45247,\"start\":45009},{\"attributes\":{\"id\":\"fig_4\"},\"end\":45513,\"start\":45248},{\"attributes\":{\"id\":\"fig_5\"},\"end\":45681,\"start\":45514},{\"attributes\":{\"id\":\"fig_6\"},\"end\":45728,\"start\":45682},{\"attributes\":{\"id\":\"fig_7\"},\"end\":45785,\"start\":45729},{\"attributes\":{\"id\":\"fig_8\"},\"end\":45854,\"start\":45786},{\"attributes\":{\"id\":\"fig_9\"},\"end\":45944,\"start\":45855},{\"attributes\":{\"id\":\"fig_10\"},\"end\":46466,\"start\":45945},{\"attributes\":{\"id\":\"fig_11\"},\"end\":46504,\"start\":46467},{\"attributes\":{\"id\":\"fig_12\"},\"end\":47027,\"start\":46505},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":47795,\"start\":47028},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":47961,\"start\":47796},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":48226,\"start\":47962},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":48619,\"start\":48227},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":49030,\"start\":48620},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":49271,\"start\":49031},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":49343,\"start\":49272},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":49774,\"start\":49344},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":50015,\"start\":49775},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":50106,\"start\":50016},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":50712,\"start\":50107},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":50772,\"start\":50713},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":51160,\"start\":50773},{\"attributes\":{\"id\":\"tab_24\",\"type\":\"table\"},\"end\":51477,\"start\":51161},{\"attributes\":{\"id\":\"tab_25\",\"type\":\"table\"},\"end\":51815,\"start\":51478},{\"attributes\":{\"id\":\"tab_26\",\"type\":\"table\"},\"end\":51998,\"start\":51816},{\"attributes\":{\"id\":\"tab_27\",\"type\":\"table\"},\"end\":52067,\"start\":51999}]", "paragraph": "[{\"end\":1960,\"start\":1692},{\"end\":2837,\"start\":1962},{\"end\":4074,\"start\":2839},{\"end\":4373,\"start\":4076},{\"end\":4968,\"start\":4375},{\"end\":5391,\"start\":4970},{\"end\":5581,\"start\":5393},{\"end\":6307,\"start\":5583},{\"end\":6527,\"start\":6309},{\"end\":6880,\"start\":6529},{\"end\":7587,\"start\":6907},{\"end\":7949,\"start\":7589},{\"end\":8533,\"start\":7951},{\"end\":9199,\"start\":8535},{\"end\":9740,\"start\":9225},{\"end\":10317,\"start\":9742},{\"end\":10697,\"start\":10319},{\"end\":10961,\"start\":10699},{\"end\":11590,\"start\":11039},{\"end\":12203,\"start\":11617},{\"end\":12756,\"start\":12205},{\"end\":13458,\"start\":12758},{\"end\":13583,\"start\":13460},{\"end\":13774,\"start\":13651},{\"end\":14172,\"start\":13810},{\"end\":14567,\"start\":14174},{\"end\":14648,\"start\":14569},{\"end\":15085,\"start\":14703},{\"end\":15631,\"start\":15087},{\"end\":16077,\"start\":15653},{\"end\":16816,\"start\":16104},{\"end\":17177,\"start\":16818},{\"end\":17706,\"start\":17211},{\"end\":18066,\"start\":17789},{\"end\":18562,\"start\":18068},{\"end\":18849,\"start\":18638},{\"end\":19030,\"start\":18936},{\"end\":19261,\"start\":19032},{\"end\":19460,\"start\":19292},{\"end\":20503,\"start\":19479},{\"end\":21405,\"start\":20680},{\"end\":21778,\"start\":21407},{\"end\":23744,\"start\":21827},{\"end\":24926,\"start\":23809},{\"end\":25587,\"start\":24963},{\"end\":26004,\"start\":25606},{\"end\":26710,\"start\":26006},{\"end\":28422,\"start\":26754},{\"end\":28732,\"start\":28458},{\"end\":28851,\"start\":28734},{\"end\":29237,\"start\":28871},{\"end\":29329,\"start\":29239},{\"end\":30900,\"start\":29331},{\"end\":31320,\"start\":30902},{\"end\":31700,\"start\":31366},{\"end\":32241,\"start\":31728},{\"end\":33117,\"start\":32275},{\"end\":33259,\"start\":33135},{\"end\":33609,\"start\":33293},{\"end\":34060,\"start\":33611},{\"end\":34829,\"start\":34082},{\"end\":34927,\"start\":34831},{\"end\":35265,\"start\":34929},{\"end\":35569,\"start\":35301},{\"end\":36250,\"start\":35571},{\"end\":36644,\"start\":36278},{\"end\":37224,\"start\":36669},{\"end\":38603,\"start\":37250},{\"end\":39620,\"start\":38621},{\"end\":40097,\"start\":39622},{\"end\":40301,\"start\":40099},{\"end\":41026,\"start\":40303},{\"end\":42230,\"start\":41028},{\"end\":42793,\"start\":42232},{\"end\":43418,\"start\":42795},{\"end\":44092,\"start\":43455},{\"end\":44460,\"start\":44094}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11038,\"start\":10962},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13650,\"start\":13584},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14702,\"start\":14649},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17210,\"start\":17178},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17788,\"start\":17707},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18637,\"start\":18563},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18935,\"start\":18850},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19291,\"start\":19262},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20615,\"start\":20504},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20665,\"start\":20615}]", "table_ref": "[{\"end\":19847,\"start\":19840},{\"end\":22433,\"start\":22426},{\"end\":23003,\"start\":22996},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":24354,\"start\":24347},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":25744,\"start\":25737},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":34178,\"start\":34170},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":34976,\"start\":34969},{\"end\":35499,\"start\":35492},{\"end\":36688,\"start\":36681},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":37433,\"start\":37426},{\"attributes\":{\"ref_id\":\"tab_15\"},\"end\":38106,\"start\":38099},{\"attributes\":{\"ref_id\":\"tab_17\"},\"end\":38478,\"start\":38470},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":38641,\"start\":38633},{\"attributes\":{\"ref_id\":\"tab_18\"},\"end\":39729,\"start\":39719},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":40603,\"start\":40595},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":41093,\"start\":41085},{\"attributes\":{\"ref_id\":\"tab_26\"},\"end\":42350,\"start\":42342},{\"attributes\":{\"ref_id\":\"tab_25\"},\"end\":42844,\"start\":42836},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":43751,\"start\":43743}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1690,\"start\":1678},{\"attributes\":{\"n\":\"2.2\"},\"end\":6905,\"start\":6883},{\"attributes\":{\"n\":\"3\"},\"end\":9223,\"start\":9202},{\"attributes\":{\"n\":\"3.2\"},\"end\":11615,\"start\":11593},{\"attributes\":{\"n\":\"3.3\"},\"end\":13808,\"start\":13777},{\"attributes\":{\"n\":\"3.4\"},\"end\":15651,\"start\":15634},{\"end\":16102,\"start\":16080},{\"attributes\":{\"n\":\"3.5\"},\"end\":19477,\"start\":19463},{\"attributes\":{\"n\":\"4\"},\"end\":20678,\"start\":20667},{\"attributes\":{\"n\":\"4.1\"},\"end\":21825,\"start\":21781},{\"end\":23762,\"start\":23747},{\"attributes\":{\"n\":\"4.2\"},\"end\":23807,\"start\":23765},{\"attributes\":{\"n\":\"4.3\"},\"end\":24961,\"start\":24929},{\"attributes\":{\"n\":\"4.4\"},\"end\":25604,\"start\":25590},{\"attributes\":{\"n\":\"5\"},\"end\":26752,\"start\":26713},{\"end\":28433,\"start\":28425},{\"end\":28456,\"start\":28436},{\"end\":28869,\"start\":28854},{\"end\":31364,\"start\":31323},{\"end\":31726,\"start\":31703},{\"end\":32273,\"start\":32244},{\"end\":33133,\"start\":33120},{\"end\":33291,\"start\":33262},{\"end\":34080,\"start\":34063},{\"end\":35299,\"start\":35268},{\"end\":36276,\"start\":36253},{\"end\":36667,\"start\":36647},{\"end\":37248,\"start\":37227},{\"end\":38619,\"start\":38606},{\"end\":43453,\"start\":43421},{\"end\":44472,\"start\":44462},{\"end\":44727,\"start\":44717},{\"end\":44880,\"start\":44865},{\"end\":45020,\"start\":45010},{\"end\":45259,\"start\":45249},{\"end\":45525,\"start\":45515},{\"end\":45693,\"start\":45683},{\"end\":45740,\"start\":45730},{\"end\":45798,\"start\":45787},{\"end\":45867,\"start\":45856},{\"end\":45957,\"start\":45946},{\"end\":46539,\"start\":46506},{\"end\":47806,\"start\":47797},{\"end\":47972,\"start\":47963},{\"end\":48237,\"start\":48228},{\"end\":48630,\"start\":48621},{\"end\":49041,\"start\":49032},{\"end\":49282,\"start\":49273},{\"end\":49355,\"start\":49345},{\"end\":49786,\"start\":49776},{\"end\":50027,\"start\":50017},{\"end\":50118,\"start\":50108},{\"end\":50724,\"start\":50714},{\"end\":50784,\"start\":50774},{\"end\":51172,\"start\":51162},{\"end\":51489,\"start\":51479},{\"end\":51827,\"start\":51817},{\"end\":52010,\"start\":52000}]", "table": "[{\"end\":48226,\"start\":48145},{\"end\":48619,\"start\":48239},{\"end\":49030,\"start\":48632},{\"end\":49271,\"start\":49122},{\"end\":49774,\"start\":49742},{\"end\":50015,\"start\":49829},{\"end\":50712,\"start\":50438},{\"end\":51160,\"start\":50917},{\"end\":51477,\"start\":51258},{\"end\":51815,\"start\":51570},{\"end\":51998,\"start\":51891}]", "figure_caption": "[{\"end\":44715,\"start\":44474},{\"end\":44863,\"start\":44729},{\"end\":45008,\"start\":44883},{\"end\":45247,\"start\":45022},{\"end\":45513,\"start\":45261},{\"end\":45681,\"start\":45527},{\"end\":45728,\"start\":45695},{\"end\":45785,\"start\":45742},{\"end\":45854,\"start\":45801},{\"end\":45944,\"start\":45870},{\"end\":46466,\"start\":45960},{\"end\":46504,\"start\":46469},{\"end\":47027,\"start\":46546},{\"end\":47795,\"start\":47030},{\"end\":47961,\"start\":47808},{\"end\":48145,\"start\":47974},{\"end\":49122,\"start\":49043},{\"end\":49343,\"start\":49284},{\"end\":49742,\"start\":49358},{\"end\":49829,\"start\":49789},{\"end\":50106,\"start\":50030},{\"end\":50438,\"start\":50121},{\"end\":50772,\"start\":50727},{\"end\":50917,\"start\":50787},{\"end\":51258,\"start\":51175},{\"end\":51570,\"start\":51492},{\"end\":51891,\"start\":51830},{\"end\":52067,\"start\":52013}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":1955,\"start\":1947},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2458,\"start\":2446},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3197,\"start\":3185},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3576,\"start\":3568},{\"end\":4096,\"start\":4088},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5949,\"start\":5937},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11449,\"start\":11437},{\"end\":12463,\"start\":12455},{\"end\":12972,\"start\":12964},{\"end\":13581,\"start\":13573},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14562,\"start\":14553},{\"end\":15679,\"start\":15671},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22421,\"start\":22413},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22983,\"start\":22975},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24731,\"start\":24723},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":25950,\"start\":25942},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":27490,\"start\":27482},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":27762,\"start\":27754},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28502,\"start\":28493},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28946,\"start\":28937},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29134,\"start\":29122},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29236,\"start\":29225},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29272,\"start\":29265},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29372,\"start\":29364},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29811,\"start\":29804},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30071,\"start\":30062},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33182,\"start\":33173},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33196,\"start\":33187},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34284,\"start\":34275},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35011,\"start\":35002},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36032,\"start\":36023},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42778,\"start\":42770}]", "bib_author_first_name": "[{\"end\":52326,\"start\":52320},{\"end\":52344,\"start\":52336},{\"end\":52359,\"start\":52355},{\"end\":52372,\"start\":52369},{\"end\":52385,\"start\":52380},{\"end\":52396,\"start\":52395},{\"end\":52412,\"start\":52405},{\"end\":52685,\"start\":52679},{\"end\":52702,\"start\":52696},{\"end\":52718,\"start\":52711},{\"end\":52732,\"start\":52723},{\"end\":52749,\"start\":52743},{\"end\":53027,\"start\":53019},{\"end\":53029,\"start\":53028},{\"end\":53041,\"start\":53038},{\"end\":53061,\"start\":53054},{\"end\":53075,\"start\":53070},{\"end\":53091,\"start\":53084},{\"end\":53114,\"start\":53108},{\"end\":53116,\"start\":53115},{\"end\":53443,\"start\":53435},{\"end\":53445,\"start\":53444},{\"end\":53457,\"start\":53454},{\"end\":53473,\"start\":53470},{\"end\":53483,\"start\":53482},{\"end\":53497,\"start\":53492},{\"end\":53731,\"start\":53725},{\"end\":53745,\"start\":53740},{\"end\":53759,\"start\":53755},{\"end\":53761,\"start\":53760},{\"end\":53775,\"start\":53768},{\"end\":53788,\"start\":53782},{\"end\":53793,\"start\":53789},{\"end\":53806,\"start\":53801},{\"end\":53816,\"start\":53811},{\"end\":53829,\"start\":53827},{\"end\":53844,\"start\":53835},{\"end\":53858,\"start\":53853},{\"end\":54254,\"start\":54245},{\"end\":54265,\"start\":54261},{\"end\":54275,\"start\":54273},{\"end\":54288,\"start\":54282},{\"end\":54301,\"start\":54294},{\"end\":54309,\"start\":54307},{\"end\":54322,\"start\":54315},{\"end\":54583,\"start\":54580},{\"end\":54596,\"start\":54590},{\"end\":54609,\"start\":54603},{\"end\":54626,\"start\":54618},{\"end\":54640,\"start\":54633},{\"end\":54655,\"start\":54646},{\"end\":54677,\"start\":54669},{\"end\":54688,\"start\":54683},{\"end\":54702,\"start\":54696},{\"end\":55049,\"start\":55043},{\"end\":55062,\"start\":55056},{\"end\":55075,\"start\":55068},{\"end\":55085,\"start\":55081},{\"end\":55305,\"start\":55298},{\"end\":55329,\"start\":55317},{\"end\":55346,\"start\":55339},{\"end\":55362,\"start\":55354},{\"end\":55624,\"start\":55619},{\"end\":55642,\"start\":55635},{\"end\":55658,\"start\":55650},{\"end\":55672,\"start\":55666},{\"end\":55682,\"start\":55678},{\"end\":55695,\"start\":55689},{\"end\":55711,\"start\":55705},{\"end\":55721,\"start\":55718},{\"end\":55731,\"start\":55728},{\"end\":56297,\"start\":56293},{\"end\":56323,\"start\":56319},{\"end\":56334,\"start\":56331},{\"end\":56755,\"start\":56748},{\"end\":56767,\"start\":56762},{\"end\":56781,\"start\":56775},{\"end\":56792,\"start\":56786},{\"end\":56800,\"start\":56797},{\"end\":56813,\"start\":56806},{\"end\":57083,\"start\":57079},{\"end\":57097,\"start\":57090},{\"end\":57107,\"start\":57103},{\"end\":57122,\"start\":57113},{\"end\":57137,\"start\":57133},{\"end\":57153,\"start\":57146},{\"end\":57422,\"start\":57415},{\"end\":57435,\"start\":57429},{\"end\":57446,\"start\":57443},{\"end\":57456,\"start\":57453},{\"end\":57468,\"start\":57461},{\"end\":57692,\"start\":57691},{\"end\":57694,\"start\":57693},{\"end\":57707,\"start\":57706},{\"end\":57709,\"start\":57708},{\"end\":57718,\"start\":57717},{\"end\":57720,\"start\":57719},{\"end\":57917,\"start\":57912},{\"end\":57932,\"start\":57924},{\"end\":57942,\"start\":57937},{\"end\":57951,\"start\":57948},{\"end\":57964,\"start\":57958},{\"end\":57971,\"start\":57965},{\"end\":58326,\"start\":58324},{\"end\":58339,\"start\":58332},{\"end\":58353,\"start\":58345},{\"end\":58363,\"start\":58359},{\"end\":58375,\"start\":58370},{\"end\":58383,\"start\":58380},{\"end\":58679,\"start\":58671},{\"end\":58690,\"start\":58684},{\"end\":58702,\"start\":58697},{\"end\":58718,\"start\":58711},{\"end\":58731,\"start\":58725},{\"end\":58937,\"start\":58929},{\"end\":58949,\"start\":58942},{\"end\":58960,\"start\":58954},{\"end\":58972,\"start\":58969},{\"end\":58983,\"start\":58977},{\"end\":58995,\"start\":58989},{\"end\":59014,\"start\":59008},{\"end\":59314,\"start\":59310},{\"end\":59328,\"start\":59322},{\"end\":59337,\"start\":59333},{\"end\":59351,\"start\":59346},{\"end\":59544,\"start\":59537},{\"end\":59556,\"start\":59550},{\"end\":59570,\"start\":59563},{\"end\":59784,\"start\":59780},{\"end\":59796,\"start\":59789},{\"end\":59808,\"start\":59801},{\"end\":59820,\"start\":59817},{\"end\":59828,\"start\":59825},{\"end\":59839,\"start\":59833},{\"end\":60016,\"start\":60012},{\"end\":60027,\"start\":60021},{\"end\":60039,\"start\":60032},{\"end\":60054,\"start\":60048},{\"end\":60329,\"start\":60324},{\"end\":60343,\"start\":60337},{\"end\":60345,\"start\":60344},{\"end\":60362,\"start\":60356},{\"end\":60366,\"start\":60363},{\"end\":60588,\"start\":60585},{\"end\":60602,\"start\":60596},{\"end\":60616,\"start\":60612},{\"end\":60633,\"start\":60626},{\"end\":60938,\"start\":60931},{\"end\":60953,\"start\":60946},{\"end\":60967,\"start\":60961},{\"end\":60969,\"start\":60968},{\"end\":60980,\"start\":60977},{\"end\":60998,\"start\":60992}]", "bib_author_last_name": "[{\"end\":52334,\"start\":52327},{\"end\":52353,\"start\":52345},{\"end\":52367,\"start\":52360},{\"end\":52378,\"start\":52373},{\"end\":52393,\"start\":52386},{\"end\":52403,\"start\":52397},{\"end\":52418,\"start\":52413},{\"end\":52428,\"start\":52420},{\"end\":52694,\"start\":52686},{\"end\":52709,\"start\":52703},{\"end\":52721,\"start\":52719},{\"end\":52741,\"start\":52733},{\"end\":52759,\"start\":52750},{\"end\":53036,\"start\":53030},{\"end\":53052,\"start\":53042},{\"end\":53068,\"start\":53062},{\"end\":53082,\"start\":53076},{\"end\":53106,\"start\":53092},{\"end\":53127,\"start\":53117},{\"end\":53452,\"start\":53446},{\"end\":53468,\"start\":53458},{\"end\":53480,\"start\":53474},{\"end\":53490,\"start\":53484},{\"end\":53508,\"start\":53498},{\"end\":53516,\"start\":53510},{\"end\":53738,\"start\":53732},{\"end\":53753,\"start\":53746},{\"end\":53766,\"start\":53762},{\"end\":53780,\"start\":53776},{\"end\":53799,\"start\":53794},{\"end\":53809,\"start\":53807},{\"end\":53825,\"start\":53817},{\"end\":53833,\"start\":53830},{\"end\":53851,\"start\":53845},{\"end\":53866,\"start\":53859},{\"end\":54259,\"start\":54255},{\"end\":54271,\"start\":54266},{\"end\":54280,\"start\":54276},{\"end\":54292,\"start\":54289},{\"end\":54305,\"start\":54302},{\"end\":54313,\"start\":54310},{\"end\":54325,\"start\":54323},{\"end\":54588,\"start\":54584},{\"end\":54601,\"start\":54597},{\"end\":54616,\"start\":54610},{\"end\":54631,\"start\":54627},{\"end\":54644,\"start\":54641},{\"end\":54667,\"start\":54656},{\"end\":54681,\"start\":54678},{\"end\":54694,\"start\":54689},{\"end\":54710,\"start\":54703},{\"end\":55054,\"start\":55050},{\"end\":55066,\"start\":55063},{\"end\":55079,\"start\":55076},{\"end\":55093,\"start\":55086},{\"end\":55315,\"start\":55306},{\"end\":55337,\"start\":55330},{\"end\":55352,\"start\":55347},{\"end\":55370,\"start\":55363},{\"end\":55633,\"start\":55625},{\"end\":55648,\"start\":55643},{\"end\":55664,\"start\":55659},{\"end\":55676,\"start\":55673},{\"end\":55687,\"start\":55683},{\"end\":55703,\"start\":55696},{\"end\":55716,\"start\":55712},{\"end\":55726,\"start\":55722},{\"end\":55744,\"start\":55732},{\"end\":55750,\"start\":55746},{\"end\":56317,\"start\":56298},{\"end\":56329,\"start\":56324},{\"end\":56348,\"start\":56335},{\"end\":56358,\"start\":56350},{\"end\":56760,\"start\":56756},{\"end\":56773,\"start\":56768},{\"end\":56784,\"start\":56782},{\"end\":56795,\"start\":56793},{\"end\":56804,\"start\":56801},{\"end\":56819,\"start\":56814},{\"end\":57088,\"start\":57084},{\"end\":57101,\"start\":57098},{\"end\":57111,\"start\":57108},{\"end\":57131,\"start\":57123},{\"end\":57144,\"start\":57138},{\"end\":57158,\"start\":57154},{\"end\":57427,\"start\":57423},{\"end\":57441,\"start\":57436},{\"end\":57451,\"start\":57447},{\"end\":57459,\"start\":57457},{\"end\":57473,\"start\":57469},{\"end\":57704,\"start\":57695},{\"end\":57715,\"start\":57710},{\"end\":57727,\"start\":57721},{\"end\":57739,\"start\":57729},{\"end\":57922,\"start\":57918},{\"end\":57935,\"start\":57933},{\"end\":57946,\"start\":57943},{\"end\":57956,\"start\":57952},{\"end\":57982,\"start\":57972},{\"end\":58330,\"start\":58327},{\"end\":58343,\"start\":58340},{\"end\":58357,\"start\":58354},{\"end\":58368,\"start\":58364},{\"end\":58378,\"start\":58376},{\"end\":58388,\"start\":58384},{\"end\":58682,\"start\":58680},{\"end\":58695,\"start\":58691},{\"end\":58709,\"start\":58703},{\"end\":58723,\"start\":58719},{\"end\":58739,\"start\":58732},{\"end\":58940,\"start\":58938},{\"end\":58952,\"start\":58950},{\"end\":58967,\"start\":58961},{\"end\":58975,\"start\":58973},{\"end\":58987,\"start\":58984},{\"end\":59006,\"start\":58996},{\"end\":59022,\"start\":59015},{\"end\":59320,\"start\":59315},{\"end\":59331,\"start\":59329},{\"end\":59344,\"start\":59338},{\"end\":59358,\"start\":59352},{\"end\":59548,\"start\":59545},{\"end\":59561,\"start\":59557},{\"end\":59581,\"start\":59571},{\"end\":59787,\"start\":59785},{\"end\":59799,\"start\":59797},{\"end\":59815,\"start\":59809},{\"end\":59823,\"start\":59821},{\"end\":59831,\"start\":59829},{\"end\":59848,\"start\":59840},{\"end\":60019,\"start\":60017},{\"end\":60030,\"start\":60028},{\"end\":60046,\"start\":60040},{\"end\":60063,\"start\":60055},{\"end\":60335,\"start\":60330},{\"end\":60354,\"start\":60346},{\"end\":60391,\"start\":60367},{\"end\":60397,\"start\":60393},{\"end\":60594,\"start\":60589},{\"end\":60610,\"start\":60603},{\"end\":60624,\"start\":60617},{\"end\":60640,\"start\":60634},{\"end\":60944,\"start\":60939},{\"end\":60959,\"start\":60954},{\"end\":60975,\"start\":60970},{\"end\":60990,\"start\":60981},{\"end\":61003,\"start\":60999}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":7448214},\"end\":52629,\"start\":52296},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2744065},\"end\":52937,\"start\":52631},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":232352655},\"end\":53372,\"start\":52939},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":244488448},\"end\":53723,\"start\":53374},{\"attributes\":{\"id\":\"b4\"},\"end\":54182,\"start\":53725},{\"attributes\":{\"doi\":\"2021a. 14\",\"id\":\"b5\"},\"end\":54494,\"start\":54184},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":234767654},\"end\":54976,\"start\":54496},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":235743051},\"end\":55249,\"start\":54978},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":229340505},\"end\":55519,\"start\":55251},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":233307215},\"end\":56222,\"start\":55521},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":61153583},\"end\":56684,\"start\":56224},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":4633214},\"end\":56986,\"start\":56686},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":235490453},\"end\":57365,\"start\":56988},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":225086598},\"end\":57615,\"start\":57367},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":207761262},\"end\":57910,\"start\":57617},{\"attributes\":{\"doi\":\"2021b. 4\",\"id\":\"b15\"},\"end\":58234,\"start\":57912},{\"attributes\":{\"id\":\"b16\"},\"end\":58585,\"start\":58236},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":166228177},\"end\":58927,\"start\":58587},{\"attributes\":{\"id\":\"b18\"},\"end\":59262,\"start\":58929},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":235605960},\"end\":59488,\"start\":59264},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":219956621},\"end\":59715,\"start\":59490},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":232352425},\"end\":60010,\"start\":59717},{\"attributes\":{\"id\":\"b22\"},\"end\":60248,\"start\":60012},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":245017011},\"end\":60583,\"start\":60250},{\"attributes\":{\"id\":\"b24\"},\"end\":60857,\"start\":60585},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":4766599},\"end\":61184,\"start\":60859}]", "bib_title": "[{\"end\":52318,\"start\":52296},{\"end\":52677,\"start\":52631},{\"end\":53017,\"start\":52939},{\"end\":53433,\"start\":53374},{\"end\":54578,\"start\":54496},{\"end\":55041,\"start\":54978},{\"end\":55296,\"start\":55251},{\"end\":55617,\"start\":55521},{\"end\":56291,\"start\":56224},{\"end\":56746,\"start\":56686},{\"end\":57077,\"start\":56988},{\"end\":57413,\"start\":57367},{\"end\":57689,\"start\":57617},{\"end\":58669,\"start\":58587},{\"end\":59308,\"start\":59264},{\"end\":59535,\"start\":59490},{\"end\":59778,\"start\":59717},{\"end\":60322,\"start\":60250},{\"end\":60929,\"start\":60859}]", "bib_author": "[{\"end\":52336,\"start\":52320},{\"end\":52355,\"start\":52336},{\"end\":52369,\"start\":52355},{\"end\":52380,\"start\":52369},{\"end\":52395,\"start\":52380},{\"end\":52405,\"start\":52395},{\"end\":52420,\"start\":52405},{\"end\":52430,\"start\":52420},{\"end\":52696,\"start\":52679},{\"end\":52711,\"start\":52696},{\"end\":52723,\"start\":52711},{\"end\":52743,\"start\":52723},{\"end\":52761,\"start\":52743},{\"end\":53038,\"start\":53019},{\"end\":53054,\"start\":53038},{\"end\":53070,\"start\":53054},{\"end\":53084,\"start\":53070},{\"end\":53108,\"start\":53084},{\"end\":53129,\"start\":53108},{\"end\":53454,\"start\":53435},{\"end\":53470,\"start\":53454},{\"end\":53482,\"start\":53470},{\"end\":53492,\"start\":53482},{\"end\":53510,\"start\":53492},{\"end\":53518,\"start\":53510},{\"end\":53740,\"start\":53725},{\"end\":53755,\"start\":53740},{\"end\":53768,\"start\":53755},{\"end\":53782,\"start\":53768},{\"end\":53801,\"start\":53782},{\"end\":53811,\"start\":53801},{\"end\":53827,\"start\":53811},{\"end\":53835,\"start\":53827},{\"end\":53853,\"start\":53835},{\"end\":53868,\"start\":53853},{\"end\":54261,\"start\":54245},{\"end\":54273,\"start\":54261},{\"end\":54282,\"start\":54273},{\"end\":54294,\"start\":54282},{\"end\":54307,\"start\":54294},{\"end\":54315,\"start\":54307},{\"end\":54327,\"start\":54315},{\"end\":54590,\"start\":54580},{\"end\":54603,\"start\":54590},{\"end\":54618,\"start\":54603},{\"end\":54633,\"start\":54618},{\"end\":54646,\"start\":54633},{\"end\":54669,\"start\":54646},{\"end\":54683,\"start\":54669},{\"end\":54696,\"start\":54683},{\"end\":54712,\"start\":54696},{\"end\":55056,\"start\":55043},{\"end\":55068,\"start\":55056},{\"end\":55081,\"start\":55068},{\"end\":55095,\"start\":55081},{\"end\":55317,\"start\":55298},{\"end\":55339,\"start\":55317},{\"end\":55354,\"start\":55339},{\"end\":55372,\"start\":55354},{\"end\":55635,\"start\":55619},{\"end\":55650,\"start\":55635},{\"end\":55666,\"start\":55650},{\"end\":55678,\"start\":55666},{\"end\":55689,\"start\":55678},{\"end\":55705,\"start\":55689},{\"end\":55718,\"start\":55705},{\"end\":55728,\"start\":55718},{\"end\":55746,\"start\":55728},{\"end\":55752,\"start\":55746},{\"end\":56319,\"start\":56293},{\"end\":56331,\"start\":56319},{\"end\":56350,\"start\":56331},{\"end\":56360,\"start\":56350},{\"end\":56762,\"start\":56748},{\"end\":56775,\"start\":56762},{\"end\":56786,\"start\":56775},{\"end\":56797,\"start\":56786},{\"end\":56806,\"start\":56797},{\"end\":56821,\"start\":56806},{\"end\":57090,\"start\":57079},{\"end\":57103,\"start\":57090},{\"end\":57113,\"start\":57103},{\"end\":57133,\"start\":57113},{\"end\":57146,\"start\":57133},{\"end\":57160,\"start\":57146},{\"end\":57429,\"start\":57415},{\"end\":57443,\"start\":57429},{\"end\":57453,\"start\":57443},{\"end\":57461,\"start\":57453},{\"end\":57475,\"start\":57461},{\"end\":57706,\"start\":57691},{\"end\":57717,\"start\":57706},{\"end\":57729,\"start\":57717},{\"end\":57741,\"start\":57729},{\"end\":57924,\"start\":57912},{\"end\":57937,\"start\":57924},{\"end\":57948,\"start\":57937},{\"end\":57958,\"start\":57948},{\"end\":57984,\"start\":57958},{\"end\":58332,\"start\":58324},{\"end\":58345,\"start\":58332},{\"end\":58359,\"start\":58345},{\"end\":58370,\"start\":58359},{\"end\":58380,\"start\":58370},{\"end\":58390,\"start\":58380},{\"end\":58684,\"start\":58671},{\"end\":58697,\"start\":58684},{\"end\":58711,\"start\":58697},{\"end\":58725,\"start\":58711},{\"end\":58741,\"start\":58725},{\"end\":58942,\"start\":58929},{\"end\":58954,\"start\":58942},{\"end\":58969,\"start\":58954},{\"end\":58977,\"start\":58969},{\"end\":58989,\"start\":58977},{\"end\":59008,\"start\":58989},{\"end\":59024,\"start\":59008},{\"end\":59322,\"start\":59310},{\"end\":59333,\"start\":59322},{\"end\":59346,\"start\":59333},{\"end\":59360,\"start\":59346},{\"end\":59550,\"start\":59537},{\"end\":59563,\"start\":59550},{\"end\":59583,\"start\":59563},{\"end\":59789,\"start\":59780},{\"end\":59801,\"start\":59789},{\"end\":59817,\"start\":59801},{\"end\":59825,\"start\":59817},{\"end\":59833,\"start\":59825},{\"end\":59850,\"start\":59833},{\"end\":60021,\"start\":60012},{\"end\":60032,\"start\":60021},{\"end\":60048,\"start\":60032},{\"end\":60065,\"start\":60048},{\"end\":60337,\"start\":60324},{\"end\":60356,\"start\":60337},{\"end\":60393,\"start\":60356},{\"end\":60399,\"start\":60393},{\"end\":60596,\"start\":60585},{\"end\":60612,\"start\":60596},{\"end\":60626,\"start\":60612},{\"end\":60642,\"start\":60626},{\"end\":60946,\"start\":60931},{\"end\":60961,\"start\":60946},{\"end\":60977,\"start\":60961},{\"end\":60992,\"start\":60977},{\"end\":61005,\"start\":60992}]", "bib_venue": "[{\"end\":52455,\"start\":52430},{\"end\":52765,\"start\":52761},{\"end\":53133,\"start\":53129},{\"end\":53522,\"start\":53518},{\"end\":53921,\"start\":53868},{\"end\":54243,\"start\":54184},{\"end\":54716,\"start\":54712},{\"end\":55099,\"start\":55095},{\"end\":55376,\"start\":55372},{\"end\":55823,\"start\":55752},{\"end\":56431,\"start\":56360},{\"end\":56828,\"start\":56821},{\"end\":57167,\"start\":57160},{\"end\":57482,\"start\":57475},{\"end\":57749,\"start\":57741},{\"end\":58054,\"start\":57992},{\"end\":58322,\"start\":58236},{\"end\":58748,\"start\":58741},{\"end\":59070,\"start\":59024},{\"end\":59367,\"start\":59360},{\"end\":59593,\"start\":59583},{\"end\":59854,\"start\":59850},{\"end\":60110,\"start\":60065},{\"end\":60403,\"start\":60399},{\"end\":60696,\"start\":60642},{\"end\":61009,\"start\":61005},{\"end\":55881,\"start\":55825}]"}}}, "year": 2023, "month": 12, "day": 17}