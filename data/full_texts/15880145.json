{"id": 15880145, "updated": "2023-11-10 22:43:25.255", "metadata": {"title": "Blendenpik: Supercharging LAPACK's Least-Squares Solver", "authors": "[{\"first\":\"\u2020\",\"last\":\"HAIMAVRON\",\"middle\":[]},{\"first\":\"\u2020\",\"last\":\"PETARMAYMOUNKOV\u2021SIVANTOLEDO\",\"middle\":[]}]", "venue": "SIAM J. Sci. Comput.", "journal": "SIAM J. Sci. Comput.", "publication_date": {"year": 2010, "month": null, "day": null}, "abstract": ". Several innovative random-sampling and random-mixing techniques for solving problems in linear algebra have been proposed in the last decade, but they have not yet made a signi\ufb01cant impact on numerical linear algebra. We show that by using a high-quality implementation of one of these techniques, we obtain a solver that performs extremely well in the traditional yardsticks of numerical linear algebra: it is signi\ufb01cantly faster than high-performance implementations of existing state-of-the-art algorithms, and it is numerically backward stable. More speci\ufb01cally, we describe a least-squares solver for dense highly overdetermined systems that achieves residuals similar to those of direct QR factorization-based solvers ( lapack ), outperforms lapack by large factors, and scales signi\ufb01cantly better than any QR-based solver.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3091090955", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/siamsc/AvronMT10", "doi": "10.1137/090767911"}}, "content": {"source": {"pdf_hash": "956a8be49f7d0c2bdd923d39f155980a551cf430", "pdf_src": "ScienceParsePlus", "pdf_uri": "[\"http://dspace.mit.edu/openaccess-disseminate/1721.1/60954\"]", "oa_url_match": true, "oa_info": {"license": "CCBYNC", "open_access_url": "https://dspace.mit.edu/bitstream/1721.1/60954/2/Avron-2010-BLENDENPIK_%20SUPERCHA.pdf", "status": "GREEN"}}, "grobid": {"id": "e29727f51c4589524564ca6994f497f3db6bca07", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/956a8be49f7d0c2bdd923d39f155980a551cf430.txt", "contents": "\nBLENDENPIK: SUPERCHARGING LAPACK'S LEAST-SQUARES SOLVER *\nApril 23, 2010\n\nHaim Avron haima@post.tau.ac.il \nBlavatnik School of Computer Science\nRaymond and Beverly Sackler Faculty of Exact Sciences\nTel-Aviv University\n69978Tel-AvivIsrael\n\nPETAR MAYMOUNKOV \u2021Sivan Toledo stoledo@tau.ac.il \nBlavatnik School of Computer Science\nRaymond and Beverly Sackler Faculty of Exact Sciences\nTel-Aviv University\n69978Tel-AvivIsrael\n\nComputer Science and Artificial Intelligence Laboratory\nMassachusetts Institute of Technology\n02139CambridgeMA\n\nBLENDENPIK: SUPERCHARGING LAPACK'S LEAST-SQUARES SOLVER *\nApril 23, 2010072CD41BC679FEAF939924963A61914610.1137/090767911* Received by the editors August 12, 2009; accepted for publication (in revised form) January 6, 2010;dense linear least squaresrandomized numerical linear algebrarandomized preconditioners AMS subject classifications. 65F2068W2065F10\nSeveral innovative random-sampling and random-mixing techniques for solving problems in linear algebra have been proposed in the last decade, but they have not yet made a significant impact on numerical linear algebra.We show that by using a high-quality implementation of one of these techniques, we obtain a solver that performs extremely well in the traditional yardsticks of numerical linear algebra: it is significantly faster than high-performance implementations of existing state-of-the-art algorithms, and it is numerically backward stable.More specifically, we describe a least-squares solver for dense highly overdetermined systems that achieves residuals similar to those of direct QR factorization-based solvers (lapack), outperforms lapack by large factors, and scales significantly better than any QR-based solver.\n\nComparison between lapack and the new solver for increasingly larger matrices.Graphs show the ratio of lapack's running time to Blendenpik's running time on random matrices with two kinds of aspect ratios.\n\n2. Overview of the algorithm.Let x opt = arg min x Ax \u2212 b 2 be a large highly overdetermined system, with A \u2208 R m\u00d7n and b \u2208 R m .Can we sample a small set of rows, R, and use only those rows to find an approximate solution?That is, is the solution x R = arg min x A R, * x \u2212 b R 2 a good approximation of x opt ?The following simple experiment in matlab [16] illustrates that for random matrices x R is indeed a good approximation in some sense as long as R is big enough: The norm of the residual is within 1.01 of the optimal residual.In general, under certain conditions on A, a uniform random sample of \u03a9(n log(m) log(n log(m))) rows leads to a residual that is within a factor of 1 + of the optimal with high probability [10].These conditions hold in the experiment above, and the residual is indeed small.The paper [10] also proposes a more sophisticated algorithm that leads to a small residual for any matrix, but a small uniform sample does not work on any matrix.\n\nThere are two problems with this approach.First, the analysis in [10] bounds the relative error in residual norm, that is,\nAx R \u2212 b 2 / Ax opt \u2212 b 2 \u2264 1 + ,\nwhere x opt is the true solution and x R is the computed solution.Drineas et al. show that this implies a bound on the forward error,\nx opt \u2212 x R 2 x opt 2 \u2264 tan(\u03b8)\u03ba(A) \u221a ,\nwhere \u03b8 = cos \u22121 ( Ax opt 2 / b 2 ).While such an analysis might be useful in some fields, it is difficult to relate it to standard stability analyses in numerical linear algebra.The standard stability analysis of least-squares algorithms is done in terms of backward error : an approximate solution x is shown to be the exact solution of a perturbed system x = arg min\nx (A + \u03b4A)x \u2212 b 2 ,\nwhere \u03b4A \u2264 \u02dc A .This implies a bound on the forward error\nx opt \u2212 x 2 x opt 2 \u2264 \u03ba(A) + \u03ba(A) 2 tan \u03b8 \u03b7 \u02dc ,\nwhere \u03b7 = A 2 x 2 / Ax 2 .The two forward error bounds are not comparable in an obvious way.Moreover, the \u221a appears to make it difficult to prove small forward error bounds in well-conditioned cases.\n\nSecond, running time depends on \u22121 .The backward stability requirement (e.g., value of ) of linear algebra software may be a constant, but it is a tiny constant.So to achieve the required , the constants in the asymptotic bounds in [10] might be too large.\n\nRokhlin and Tygert [22] use a difference approach.They use the R factor of the sampled rows as a preconditioner in a Krylov-subspace method like LSQR [18]: >> [Q, R] = qr(A1, 0); >> x1 = lsqr(A, b, eps, 100, R); lsqr converged at iteration 17 to a solution with relative residual 0.5 A uniform sample of the rows is not always a good strategy.If A has a column j that is zero except for A ij = 0, any subset of rows that excludes row i is rank deficient.If m n, the algorithm needs to sample close to m rows in order to guarantee a high probability that row i is in the subset.If the row sample is too small, the preconditioner is rank deficient and LSQR fails.\n\n>> A(1:end-1, end) = 0; >> A1 = A(sampled rows, :); >> [Q, R] = qr(A1, 0); >> x1 = lsqr(A, b, eps, 100, R); Warning: Matrix is singular to working precision.> In sparfun\\private\\iterapp at 33 In lsqr at 217 In overview at 35 lsqr stopped at iteration 0 without converging to the desired tolerance 2.2e-016 because the system involving the preconditioner was ill conditioned.The iterate returned (number 0) has relative residual 1 Uniform random sampling works well only when the coherence of the matrix is small, which is equal to the maximum norm of a row in Q, where Q forms an orthonormal basis for the column space of A (e.g., the leftmost factor in a reduced QR or singular-value decomposition; a formal definition of coherence appears in section 3).The coherence of the matrix is between n/m and 1.The lower it is, the better uniform sampling works.\n>> [Q, R] = qr(A, 0); >> coherence = max(sum(Q .^2, 2)) coherence = 1.0000\nThe coherence of our matrix, after the insertion of zeros into column 1, is the worst possible.\n\nThe coherence of matrices with random independent uniform entries tends to be small, but as we have seen, other matrices have high coherence.We can use a randomized row-mixing preprocessing phase to reduce the coherence [10,22]: >> D = spdiags(sign(rand(m, 1)), 0, m, m); >> B = dct(D * A); B(1, :) = B(1, :) / sqrt(2); >> [Q, R] = qr(B, 0); >> coherence = max(sum(Q .^2,2)) coherence = 0.0083 First, we randomly multiply each row by +1 or \u22121, and then apply a discrete cosine transform (DCT) to each column.The first row is divided by \u221a 2 to make the transformation orthogonal.With high probability the coherence of B is small.In the example above, it is less than twice the minimal coherence (0.005).There are many ways to mix rows to reduce the coherence.We discuss other methods in section 3.2.\n\nWith high probability, a uniform sample B1 of the rows of the row-mixed matrix B makes a good preconditioner.In the code below, we use the R factor of the sample to allow LSQR to apply the preconditioner efficiently:\n\n>> B1 = B(sampled rows, :); >> [Q, R] = qr(B1, 0); >> x1 = lsqr(A, b, eps, 100, R); lsqr converged at iteration 15 to a solution with relative residual 1\n\n\n3.\n\nTheory.This section explains the theory behind the algorithms that this paper investigates.The ideas themselves are not new; they have been proposed in several recent papers [10,22,17].We do present some simple generalizations and improvements to existing results, but since the original proofs are strong enough for the generalizations, we omit the proofs, but they appear in [4].\n\n\nUniform sampling preconditioners.\n\nThe quality of uniform sampling preconditioners depends on how much the solution depends on specific rows.For example, if the sample is rank deficient unless row i is in it, then the size of a uniform sample must be too large to be effective.Coherence [6] is the key concept for measuring the dependence of the solution on specific rows.Definition 3.1.Let A be an m \u00d7 n full rank matrix, and let U be an m \u00d7 n matrix whose columns form an orthonormal basis for the column space of A. The coherence of A is defined as\n\u03bc(A) = max U i, * 2 2 .\nThe coherence of a matrix is always smaller than 1 and bigger than n/m.If a row contains the only nonzero in one of the columns of A, then the coherence of the matrix is 1.Coherence does not relate in any way to the condition number of A.\n\nUniform random sampling yields a good preconditioner on incoherent matrices (matrices with small coherence).For example, if \u03bc(A) = n/m, then a sample of \u0398(n log n) rows is sufficient to obtain a good preconditioner.The following theorem describes a relationship between the coherence, the sample size, and the condition number of the preconditioned system.\n\nTheorem 3.2.Let A be an m\u00d7n full rank matrix, and let S be a random sampling operator that samples r \u2265 n rows from A uniformly.Let \u03c4 = C m\u03bc(A) log(r)/r, where C is some constant defined in the proof.Assume that \u03b4 \u22121 \u03c4 < 1.With probability of at least 1 \u2212 \u03b4, the sampled matrix SA is full rank, and if SA = QR is a reduced QR factorization of SA, we have\n\u03ba(AR \u22121 ) \u2264 1 + \u03b4 \u22121 \u03c4 1 \u2212 \u03b4 \u22121 \u03c4 .\nThis result does not appear in this exact form in the literature, but its proof is a simple variation of the results in [10,22].Therefore, here we give only a sketch of the proof; the full version of the proof appears in [4].The first phase is to bound I n\u00d7n \u2212(m/r)Q T S T SQ 2 with high probability, using the proof technique of Lemma 5 from [10], in two steps.The first step bounds E( I n\u00d7n \u2212 (m/r)U T S T SU 2 ) using Lemma 4 from [10], and the second step uses Markov's inequality to bound I n\u00d7n \u2212 (m/r)Q T S T SQ 2 with high probability.Using a simple Rayleigh quotient argument, we then bound \u03ba(SQ) with high probability.Finally, Theorem 1 in [22] shows that \u03ba(AR \u22121 ) = \u03ba(SQ).Remark 1.Notice that the condition number of the original matrix A does not affect the bound on the condition number of the preconditioned matrix.\n\nRemark 2. Theorem 3.2 describes a relationship between sample size (r), the probability of failure (\u03b4), and the condition number of the preconditioned system.With a small sample, the probability of obtaining a high condition number is high.A high condition number may lead to a large number of iterations in LSQR, but the number of iterations may also be small: the convergence of LSQR depends on the distribution of the singular values of AR \u22121 , not just on the extreme singular values.In fact, [3] uses the fact that a few very large or very small singular values do not affect convergence much.\n\nIf the coherence is high, uniform sampling produces poor preconditioners.One alternative is to use nonuniform sampling.Let A = U R be a reduced QR factorization of A. Drineas, Mahoney, and Muthukrishnan [11] suggest sampling row i with probability p i = U i 2 2 /m, where U i is row i of U .Computing these probabilities requires too much work (a QR factorization of A), so to make this approach practical, probabilities should be somehow approximated; to the best of our knowledge, no efficient approximation algorithm has been developed yet.Therefore, in the next subsection we turn to a simpler approach, the one used by our solver, which is based on mixing rows.\n\n3.2.Row mixing.Theorem 3.2 implies that even if there are important rows, that is, even if coherence is high, if we sample enough rows, then with high probability the preconditioner is a good preconditioner.The higher \u03bc(A) is, the more rows should be sampled.This poses two problems.First, finding \u03bc(A) is computationally hard.Second, if \u03bc(A) is high too, then many rows need to be sampled.Indeed, if \u03bc(A) = 1 (the worst), then as many as O(m log m) rows need to be sampled in order to get a bound on the condition number by using Theorem 3.2.When \u03bc(A) = 1, there is a row in the matrix that must be included in the sample for R to be full rank.We do not know which row it is, so no row can be excluded from the sample; this is not useful.If \u03bc(A) = n/m (minimal coherence), on the other hand, then only \u0398(n log n) rows need to be sampled to get \u03ba = O(1) with high probability.\n\nIn general, we cannot guarantee a bound on \u03bc(A) in advance.The solution is to perform a preprocessing step in which rows are mixed so that their importance is nearly equal.The crucial observation is that a unitary transformation preserves the condition number but changes the coherence.If F is a unitary transformation and R is a preconditioner F A, then R is an equally good preconditioner for A because the singular values of AR \u22121 and F AR \u22121 are the same.But \u03bc(A) and \u03bc(F A) are not necessarily the same; if we select F so that \u03bc(F A) is small, then we can construct a good preconditioner by uniformly random sampling the rows of F A.\n\nAny fixed unitary transformation F leads to a high \u03bc(F A) on some A's, so we use a random unitary transformation.We construct F from a product of a fixed seed unitary transformation F and a random diagonal matrix D with \u00b11 diagonal entries.The diagonal entries of D are random, unbiased, independent random variables.The following theorem shows that with high probability, the coherence of F A is small, as long as the maximum value in F is not too large.It is a simple generalization of Lemma 3 in [10] using ideas from [17]; we omit the proof.\n\nTheorem 3.3.Let A be an m \u00d7 n full rank matrix, where m \u2265 n.Let F be an m \u00d7 m unitary matrix, let D be a diagonal matrix whose diagonal entries are independent and identically distributed Rademacher random variables (Pr(D ii = \u00b11) = 1/2), and let F = F D. With a probability of at least 0.95, we have\n\u03bc(F A) \u2264 Cn\u03b7 log m , where \u03b7 = max |F ij | 2 and some constant C. Note 1.\nA must be full rank for the \u03bc to be well defined.The theorem can be generalized to success guarantees other than 0.95.A higher probability leads to a higher constant C.\n\nA seed matrix F is effective if it is easy to apply to A and if\n\u03b7 = max |F ij | 2 is small. The minimal value of \u03b7 is 1/m. If \u03b7 is 1/m,\nthen all the entries of F must have squared absolute values of 1/m.A normalized DFT matrix has this property, and it can be applied quickly, but it involves complex numbers.A normalized Hadamard matrix has entries that are all \u00b11/ \u221a m, and in particular are all real.Hadamard matrices do not exist for all dimensions, but they do exist for powers of two, and they can be applied quickly at powers of two.The Walsh-Hadamard series\nH l = 1 \u221a 2 1 1 1 \u22121 , H l+1 = 1 \u221a 2 H l H l H l \u2212H l ,\nenables the Walsh-Hadamard transform (WHT).Two other options for F are the discrete cosine transform (DCT) and discrete Hartley transform (DHT), which are real, exist for every size, and can be applied quickly.Their \u03b7 value is 2/m, twice as large as that of the WHT.\n\nIf we use one of the transformations described above, we need a sample of \u0398(n log(m) log(n log(m))) rows to obtain \u03ba = O(1) with high probability.In practice, smaller samples are sufficient.In section 4, we discuss implementation issues and considerations for selecting the seed unitary transformation.\n\nA possible alternative mixing strategy is a Kac random walk [14].We define\nF = G T (m,n) G T (m,n)\u22121 \u2022 \u2022 \u2022 G 3 G 2 G 1 ,\nwhere each G t is a random Givens rotation.To construct G t , we select two random indices i t and j t and a random angle \u03b8 t , and we apply the corresponding Givens rotation.The number of rotations is chosen to make the coherence of F A sufficiently small with high probability.How small can we make T (m, n)? Ailon and Chazelle [1] conjecture that T (m, n) = O(m log m) will suffice, but they do not have a proof, so we do not currently use this approach.We propose an even simpler random walk, where instead of using a random angle \u03b8 t , we fix \u03b8 t = \u03c0/4.We conjecture that still T (m, n) = O(m log m) will suffice, and we have verified this conjecture experimentally.\n\n\nHigh coherence due to a few rows.\n\nThe coherence is the maximal row norm of U , an orthonormal basis for the column space of A. If all the rows of U have low coherence, a uniform random sample of the rows of A leads to a good preconditioner.We now show that even if a few rows in U have a large norm, a uniform random sample still leads to an effective preconditioner.The fundamental reason for this behavior is that a few rows with a large norm may allow a few singular values of the preconditioned system AR \u22121 to be very large, but the number of large singular values is bounded by the number of large rows.A few large singular vectors cause the condition number of AR \u22121 to become large, but they do not affect much the convergence of LSQR [3].\n\nLemma 3.4.Let A be an m \u00d7 n full rank matrix, where m \u2265 n, and suppose we can write A = A1 A2 , where\nA 2 has l \u2264 min(m \u2212 n, n) rows. Let S \u2208 R k\u00d7(m\u2212l) be a matrix such that SA 1 is full rank. Let SA 1 = QR be the QR factorization of SA 1 .\nThen at least n \u2212 l singular values of AR \u22121 are between the smallest singular value of A 1 R \u22121 and the largest singular value of\nA 1 R \u22121 .\nTo prove Lemma 3.4 we need the following simplified version of Theorem 4.3 in [3].\n\nTheorem 3.5 (simplified version of Theorem 4.3 in [3]).Let A \u2208 C m\u00d7n , and let B \u2208 C k\u00d7n for some 1 \u2264 k < n be full rank matrices.Let M \u2208 C n\u00d7n be a symmetric positive semidefinite matrix.If all the eigenvalues of (A T A, M ) are between \u03b1 and \u03b2, then so are the n \u2212 k smallest eigenvalues of (A T A + B T B, M ).\n\nProof of Lemma 3.4.The singular values of A 1 R \u22121 are the square root of the generalized eigenvalues of (A T 1 A 1 , (SA 1 ) T (SA 1 )).The singular values of AR \u22121 are the square root of the generalized eigenvalues of (A\nT 1 A 1 + A T 2 A 2 , (SA 1 ) T (SA 1 )). The matrix A T A = A T 1 A 1 + A T 2 A 2 is an l-rank perturbation of A T 1 A 1\n, so according to Theorem 3.5 at least n\u2212l generalized eigenvalues of (A T 1 A 1 +A T 2 A 2 , (SA 1 ) T (SA 1 )) are between the smallest and largest generalized eigenvalues of (A T 1 A 1 , (SA 1 ) T (SA 1 )).\n\nSuppose that A 1 is incoherent but A is coherent.In this case, coherency can be attributed to only a small number of rows (l rows).If A 1 is incoherent and full rank, then random sampling will produce a good preconditioner without row mixing.Lemma 3.4 implies that the same preconditioner will be a good preconditioner for A as long as l is small.In practice, we do not know the partition of A to A 1 and A 2 .We simply sample from all the rows of A. But if m is large and the sample is small, the probability of missing any specific row is large; in particular, if l is small, then rows from A 2 are likely to be missed.The lemma shows that R is still a good preconditioner.If rows from A 2 are in the sample, the preconditioner is even better.\n\nThe lemma assumes that the row sample is full rank.In fact, almost the same result applies even if the sample is rank deficient, as long as we perturb R to make it full rank; see [3] for details.\nAlgorithm 1. Blendenpik's algorithm. x=blendenpik(A \u2208 R m\u00d7n ,b \u2208 R n ) m \u2265 n, A is nonsingular parameters: \u03b3 and transform type m \u2190\u2212 \u23a7 \u23a8 \u23a9 2 log 2 m , WHT m/1000 \u00d7 1000, DCT or DHT M \u2190\u2212 A 0 \u2208 R m\u00d7n while not returned M \u2190\u2212 F m(DM )\nD is a diagonal matrix with \u00b11 on its diagonal with equal probability F m is the seed unitary transform (WHT/DCT/DHT), \u0398(mn log m) operations Let S \u2208 R m\u00d7 m be a random diagonal matrix:\nS ii = \u23a7 \u23a8 \u23a9 1 with probability \u03b3n/ m 0 with probability 1 \u2212 \u03b3n/ m Factorize: SM = QR, reduced QR factorization (R \u2208 R n\u00d7n ) \u03ba \u2190\u2212 \u03ba estimate (R), condition number estimation (lapack's dtrcon) if \u03ba\u22121 > 5 machine x \u2190\u2212 LSQR(A, b, R, 10 \u221214 )\nreturn else if #iterations > 3 failure: solve using lapack and return end if end if end while Row mixing.In section 3.2 we suggest five row-mixing strategies: DFT, DCT, DHT, WHT, and Kac.We chose not to implement DFT and Kac.The DFT of a vector is a complex vector even if the vector is real.Thus, using DFT entails operation-count and memory penalties on subsequent phases when applied on real matrices.Therefore, it is unlikely that an FFT-based algorithm would outperform one based on DCT or DHT.Kac's random walk appears to suffer from poor cache locality due to random index selection.\n\nWHT is theoretically optimal, in the sense that its \u03b7 value is 1/m, but it can be applied only if the number of rows is a power of two.By padding the matrix with zeros we can apply WHT to smaller matrices.This causes discontinuous increases in the running time and memory usage as m grows.We use spiral wht [13] to apply WHT.To get good performance it is essential to use the package's self-optimization feature, which incurs a small one-time overhead.\n\nInstead of using WHT, any Hadamard matrix can be used.If H 1 and H 2 are Hadamard matrices, then so is H 1 \u2297 H 2 , so by using kernels of small Hadamard transforms, efficient large Hadamard transforms can be implemented.But to the best of our knowledge, there is currently no efficient implementation of this idea.\n\nDCT and DHT are near-optimal alternatives (their \u03b7 value is 2/m).Their advantages over WHT are that they exist for all vector sizes and that, in principle, they can always be applied in O(m log m) operations.However, in practice these transforms are quite slow for some sizes.The performance of fast transforms (DCT and DHT) depends on how the input size m can be factored into integers.The performance is not monotone in m.Also, the fast-transform library that we use (fftw) requires tuning for each input size; the tuning step also takes time.To address these issues, we use the following strategy.During the installation of our solver, we generate tuned DCT and DHT solvers for sizes of the form m = 1000k, where k is an integer.The information used by fftw to generate the tuned solvers (called \"wisdom\" in fftw) is kept in a file.Before the solver uses fftw to compute DHT or DCT, this information is loaded into fftw, so no additional tuning is done at solve time.Before applying DCT or DHT to a matrix, we pad the matrix to the next multiple of 1000 or to a slightly higher multiple if the tuning step suggested that the higher multiple would result in higher performance.One can imagine more sophisticated strategies, based on knowing what kernel sizes fftw uses as fast building blocks and using sizes that are multiples of those building blocks.The method that we used is not optimal, but it does deliver good performance while keeping tuning time reasonable.\n\nWe tune fftw using aggressive settings, so tuning takes a long time (hours).We also experimented with milder tuning settings.If fftw's weakest tuning is used, the tuning time of DHT reduces to about 11 minutes, but the time spent in computing the DHTs is sometimes doubled.As we shall see in section 5.6, this slows our solver, relative to aggressive setting, by at most 15% (usually less).\n\nSampling rows and QR factorization.We sample rows by generating a size m vector with random uniform entries in [0, 1], where m is the number of rows after padding.We use matlab's rand function to generate the vector.A row is sampled if the corresponding entry in the vector is smaller than \u03b3n/ m, where \u03b3 is a parameter.The expected number of rows that are sampled is \u03b3n, but the actual value can be higher or smaller.This is the same strategy that was suggested in [10].Once the rows are sampled we compute their QR factorization using lapack's dgeqrf function.\n\nRow sampling can be combined with row mixing to improve the asymptotic running time.Any k indices of the FFT of a m element vector can be computed using only O(m log k) operations [24].This is also true for WHT [2].If we select the sampled rows before the row mixing, we can compute only the mixed rows that are in the sample.We do not use this strategy because the libraries that we use do not have this option.\n\nIterative solution.We use LSQR to find the solution.Given an iterate x j with a corresponding residual r j = b \u2212 Ax j , stopping the algorithm when (4.1)\nA T r j 2 A F r j 2 \u2264 \u03c1\nguarantees that x j is an exact solution of\nx j = arg min x (A + \u03b4A)x \u2212 b 2\nwhere \u03b4A F \u2264 \u03c1 A F .That is, the solution is backward stable [7].The value of \u03c1 is a parameter that controls the stability of the algorithm.To use this stopping criterion, we need to compute r j and A T r j in every iteration.It is therefore standard practice in LSQR codes to estimate r j 2 and A T r j 2 instead of actually computing them.The estimate formulas used are accurate in exact arithmetic, and in practice they are remarkably reliable [18].If a preconditioner R is used, as in our algorithm, A T r j 2 cannot be estimated but (AR \u22121 ) T r j 2 can be.Preconditioned LSQR codes estimate AR \u22121 F as well and use the stopping criterion\nAR \u22121 T r j 2 AR \u22121 F r j 2 \u2264 \u03c1\nthat guarantees a backward stable solution to\ny j = arg min x AR \u22121 y \u2212 b 2\nand returns x j = R \u22121 y j .We use the same strategy in our solver.We set \u03c1 = 10 \u221214 , which is close to machine but not close enough to risk stagnation of LSQR.This setting results in a solver that is about as stable as a QR-based solver.Most of the LSQR running time is spent on multiplying vectors by A and A T .If A is sparse and very tall, using a sparse matrix-vector multiplication code can reduce the LSQR running time, even though R is dense.We have not exploited this opportunity in our code.\n\nHandling failures.The bounds in section 3 hold with some probability bounded from below.With some probability, the algorithm can fail to produce an effective preconditioner in one of two ways: (1) the preconditioner can be rank deficient or highly ill conditioned, or (2) the condition number \u03ba(AR \u22121 ) can be high.When the condition number is high, LSQR converges slowly, but the overall algorithm does not fail.But a rank-deficient preconditioner cannot be used with LSQR.To address this issue, we estimate the condition number of the preconditioner R using lapack's dtrcon function.If the condition number is too high (larger than \u22121 machine /5), we perform another row-mixing phase and resample.If we repeat this three times and still do not get a full rank preconditioner, we give up, assume that the matrix itself is rank deficient, and use lapack.This never happened in our experiments on full rank matrices, but on some matrices we had to mix and sample more than once.\n\n\nNumerical experiments.\n\nWe experimented with the new algorithm extensively in order to explore its behaviors and to understand its performance.This section reports the results of these experiments (Figure 1.1 shows additional results).\n\n\nExperimental setup.\n\nWe compare the new solver, which we call Blendenpik, to a high-performance dense QR solver and to LSQR with no preconditioning.The dense QR solver is lapack's dgels: a high-performance, high-quality, portable code.We call lapack from matlab using a special cmex interface that measures only lapack's running time.No Matlab-related overheads are included; Matlab is used here only as a scripting tool.\n\nRunning times were measured on a machine with two AMD Opteron 242 processors (we used only one) running at 1.6 GHz with 8 GB of memory.We use goto blas 1.30 and lapack 3.2.1 for basic matrix operations and fftw 3.2.1 for the DCT and DHT.\n\nThe measured running times are wall-clock times that were measured using the ftime Linux system call.\n\nWe evaluated our solver on several classes of random matrices.Random matrices were generated using matlab's rand function (random independent uniform numbers).Ill-conditioned matrices are obtained by generating their SVD decomposition: two random orthonormal matrices and an equally spaced diagonal matrix with the appropriate condition number.\n\nOur solver relies on automatic tuning of the fast-transform libraries that it uses (fftw and spiral).This is an installation-time overhead that is not included in our running-time measurements.Automatic tuning is a technique of growing importance in various numerical libraries, such as the atlas [26] implementation of the blas.\n\nTheoretical bounds relate to the coherence, which is the maximum row norm in the orthogonal factor of the matrix.Our experiments suggest that, in practice, running time is related to the number of rows that have a large norm in the orthogonal factor.Therefore, we experimented with three types of matrices: incoherent matrices, semicoherent matrices and coherent matrices.Incoherent matrices X m\u00d7n , either well conditioned or ill conditioned, are generated using the rand function with no restriction on the structure.Semicoherent matrices, are of the form\nY m\u00d7n = B I n/2 + 10 \u22128 \u23a1 \u23a2 \u23a3 1 \u2022 \u2022 \u2022 1 . . . . . . 1 \u2022 \u2022 \u2022 1 \u23a4 \u23a5 \u23a6 ,\nwhere B is an (m\u2212n/2)\u00d7n/2 rectangular random matrix and I n/2 is a square identity of dimension n/2.Y m\u00d7n is, in fact, coherent (\u03bc(Y m\u00d7n ) = 1), but only n/2 rows have a large norm in the orthogonal factor.Our coherent matrices have the form\nZ m\u00d7n = D n\u00d7n 0 (m\u2212n)\u00d7n + 10 \u22128 \u23a1 \u23a2 \u23a3 1 \u2022 \u2022 \u2022 1 . . . . . . 1 \u2022 \u2022 \u2022 1 \u23a4 \u23a5 \u23a6 ,\nwhere D n\u00d7n is a random diagonal matrix.The orthogonal factors of these matrices have n rows with a large norm.In both semicoherent and coherent matrices, the constant 10 \u22128 matrix is added to make the matrices dense.Some solvers, including lapack's (in version 3.2.1),exploit sparsity.Our solver does not.We added the constant matrix to avoid this source of variance; we acknowledge the fact that, for some sparse matrices, lapack's dense solver is faster than our solver.\n\n\nTuning experiments.\n\nThe behavior of our solver depends on the seed unitary transformation that mixes the rows, on the number of row-mixing steps, and on the sample size.These parameters interact in complex ways, which are not fully captured by asymptotic analyses.We begin with experiments that are designed to help us choose these parameters in the algorithm.\n\n\nUnitary transformation type.\n\nThe row-mixing phase uses a fixed seed unitary matrix that depends only on the row dimension of the problem.In 3.2 we suggested five different seed unitary matrices.As explained in section 4, we implemented only three of them, all using external libraries: the WHT, the DCT, and the DHT.\n\n\n. Overall running time of the algorithm with different fast unitary transforms (row mixing) on increasingly larger matrices. We tested on incoherent matrices (top left graph), semicoherent matrices (top right graph), and coherent matrices (bottom graph).\n\nDifferent unitary transforms improve coherence in different ways.Figure 5.2 examines the overall running time of the solver on incoherent, semicoherent, and coherent matrices.For incoherent and semicoherent matrices there does not seem to be a significant difference between the different mixing methods.WHT's overall time is smaller because it is faster than other methods.On coherent matrices, WHT exhibits poor and erratic performance.Usually, a single WHT phase generated a very ill-conditioned preconditioner (very close to rank deficiency).This was sometimes detected by the condition number estimator, in which case a second WHT phase was done.In some cases the condition number estimator test failed, and convergence was very slow.DHT and DCT continue to work well on coherent matrices; the two methods behave the same.It is interesting to note that, from a theoretical standpoint, WHT is superior, but in practice, DHT and DCT work better.\n\nClearly, WHT's advantage (fast application and a low \u03b7) are offset by its disadvantages (reduced robustness and a large memory footprint).We therefore decided to use DHT (which is faster than DCT) for all subsequent experiments except for the right graph in Figure 1.1, where we used WHT for experimental reasons.\n\n\nSample size and number of row-mixing steps.\n\nThe theoretical analysis shows that sampling \u03a9(n log(m) log(n log(m))) rows is sufficient with high probability, but we do not know the constants in the asymptotic notation.The analysis may give bounds in the probability of failure, but even if there is failure (e.g., the condition number is bigger than the bound), running time might still be good.Convergence behavior is governed by the distribution of singular values, and it is not fully captured by the condition number.The contributions of each phase to the running time interact in a complex way that is not fully predictable by worst-case asymptotic analysis.Therefore, we performed experiments whose goal is to determine a reasonable sampling size.\n\nWe also need to decide on the number of row-mixing steps.Row-mixing steps reduce the coherence and improve the outcome of random sampling.Theoretical bounds state that after a single row-mixing step, the coherence is within a O(log m) factor of the optimal with high probability.Therefore, after the first row-mixing step, there is still room for improvement.Although there are no theoretical results that state so, it reasonable to assume that additional row-mixing steps will reduce coherence further, which will cause LSQR to converge faster, perhaps offsetting the cost of the extra mixing steps.\n\nFigure 5.3 presents the results of these experiments.We ran experiments with two matrix sizes, 30,000 \u00d7 750 (left graphs) and 40,000 \u00d7 2,000 (right graphs), and all matrix types, incoherent (top left and middle right graphs), semicoherent (top right and bottom left graphs), and coherent (middle left and bottom right graphs).All the matrices were ill conditioned.\n\nWe used sample size \u03b3n, where \u03b3 ranges from 1.5 to 10.Although the theoretical bound is superlinear, it is not necessarily tight.As the results show, for the range of matrices tested in our experiments, the best sample size displays a sublinear (in n) behavior (which might change for larger matrices).\n\nFor 30,000 \u00d7 750 matrices the best sample size is around \u03b3 = 6.For 40,000 \u00d7 2,000 it is \u03b3 = 3. Apparently, for larger matrices a smaller sample is needed (relative to n), contrary to the theoretical analysis.A sample size with \u03b3 = 4 is close to optimal for all matrices.For incoherent and semicoherent matrices there is a (small) advantage for using only one preprocessing phase.For coherent matrices the best results are achieved when using two preprocessing phases.In fact, using only one preprocessing phase can be disastrous when combined with a sample size that is too small.But with a sample size \u03b3 = 4, near-optimal results can be achieved with only one preprocessing phase.\n\nFollowing these experiments we decided to fix \u03b3 = 4 and to use one preprocessing phase.We used these settings for the rest of the experiments.These parameters are not optimal in all cases, but they seem to be nearly optimal for most cases.The rest of the experiments in this paper use these values.\n\n\n5.3.\n\nIll-conditioned matrices.Figure 5.4 shows that the condition number of A does not affect our new solver at all, but it does affect unpreconditioned LSQR.On very well conditioned matrices, unpreconditioned LSQR is faster, but its performance deteriorates quickly as the condition number grows.\n\n\n5.4.\n\nEasy and hard cases.Figure 5.5 compares the performance of our solver and of lapack on incoherent, semicoherent, and coherent matrices of four different aspect ratios.The number of elements in all matrices is the same.(lapack's running time depends only on the matrix's dimensions, not on its coherence, so the graph shows only one lapack running time for each size.)Our solver is slower on matrices with high coherence than on matrices with low coherence but not by much.Even when the coherence is high, our solver is considerably faster than lapack.Hard cases (high coherence) run slower because LSQR converges slower, so more LSQR iterations are  performed.(The other phases of the algorithm are oblivious to coherence.)It appears that a single row-mixing phase does not remove the coherence completely.\n\n\nConvergence rate.\n\nIn the experiments whose results are shown in the left graph in Figure 5.6, we examine the LSQR convergence rate on a single matrix.The graph shows the norm of the residual after each iteration.Except for the final iterations, where the solver stagnates near convergence, the convergence rate is stable and predictable.This is a useful property that allows us to predict when the solver will converge and to predict how the convergence threshold affects the running time.The rate itself is slower on coherent matrices than on incoherent and semicoherent ones.This is the same issue we saw in Figure 5.5.\nT r (i) 2 / A F r (i) 2\n, where r (i) is the residual after the ith iteration of LSQR on three 100,000 \u00d7 2,500 matrices of different coherence profiles.The right graph shows the number of LSQR iterations needed for convergence on increasingly larger matrices.The graph on the right examines the number of iterations required for LSQR to converge as a function of problem size.On incoherent and semicoherent matrices the number of iterations grows very slowly.On coherent matrices the number of iterations grows faster.5.6.The cost of the different phases.Figure 5.7 shows a breakdown of the running time of our solver for incoherent matrices (left graph) and coherent matrices (right graph) of increasingly larger size.The row-mixing preprocessing phase is not a bottleneck of the algorithm.Most of the time is spent on factoring the preconditioner and on LSQR iterations.The most expensive phase is the LSQR phase.The asymptotic running time of the row-mixing phase is \u0398(mn log m), and for the QR phase it is \u0398(n 3 ).Each LSQR iteration takes \u0398(mn) time, and the number of iterations grows slowly.In both graphs n = m/40, so the QR phase is asymptotically the most expensive.\n\nThe dominance of the LSQR phase implies that considerable speedup can be achieved by relaxing the convergence threshold.In our experiments the convergence threshold was set to 10 \u221214 .If a convergence threshold of 10 \u22126 is acceptable, for example, we can roughly halve the number of iterations of the LSQR phase, thereby accelerating our solver considerably.\n\nThe row mixing phase takes about 15% of overall solver time.Even if we double the row-mixing time, our solver will still be faster than lapack on nearly all of the matrices used in our experiments.\n\n\nNo row mixing.\n\nIf a matrix is completely incoherent to begin with, we do not need to mix its rows.On such matrices, row mixing takes time but does not reduce the running time of subsequent phases.The left graph in Figure 5.8 shows that this is essentially true on random matrices, which have low (but not minimal) coherence; the algorithm runs faster without mixing at all.\n\nThe right graph in Figure 5.8 examines performance on coherent matrices whose coherence can be attributed to a small number c of rows.The matrices are of the form S (m+c)\u00d7(n+c) = S 0 S 1 0 10 3 \u00d7 I c , where S 0 \u2208 R m\u00d7(n\u2212c) and S 1 \u2208 R m\u00d7c are a random rectangular matrix and I c is a cby-c identity.When c is tiny (1 and 2), row mixing does not improve the running time substantially.But for c > 2, with row mixing the running time remains constantly low, while a performance of random sampling without mixing deteriorates as the size of the 10 3 \u00d7 I c block grows.The reason for the deterioration is numerical inaccuracy and not poor preconditioning.The basis vectors generated by LSQR lose orthogonality because a short recurrence (Lanczos recurrence) is used.A celebrated result of Paige [19] shows that loss of orthogonality is large only in the directions of converged or nearly converged Ritz vectors.As long as no Ritz has converged, a satisfactory level of orthogonality is maintained.This result explains why isolated singular values in the preconditioned matrix cause numerical problems: the algorithm tends to converge fast for the isolated eigenvalues.Possible solutions for this problem full orthogonalization (expensive), selective orthogonalization [20], and others (see section 5.3 in [25]).We have verified this observation by running LSQR with full orthogonanlization (graph not included).1235 because it often works better.The row-mixing method in [22] uses FFT, which forces the solver to work on complex numbers.Furthermore, their analysis requires two FFT applications.\n\nOur observation that the solver can work well even if the post-mixing coherence is high, as long as the number of high-norm rows in U is small, is new.\n\nUnlike previous work in this area, we compared our solver to a state-of-the-art direct solver (lapack), showed that it is faster, and explored its behavior on a wide range of matrices.Drineas et al. [10] do not implement their algorithm.Rokhlin and Tygert [22] implemented their algorithm, but they compared it a direct solver that they implemented, which is probably slower than lapack's.They also experimented with only a small range of matrices (incoherent matrices whose number of rows is a power of two).\n\n\n\nFig.1.1.Comparison between lapack and the new solver for increasingly larger matrices.Graphs show the ratio of lapack's running time to Blendenpik's running time on random matrices with two kinds of aspect ratios.\n\n\n\n\n>> rand('state', 2378) >> randn('state', 23984) >> m = 20000; n = 100; >> A = rand(m, n); b = rand(m, 1); >> [U, S, V] = svd(A, 0); >> S = diag(linspace(1, 10^6, 100)); >> A = U * S * V'; >> sampled rows = find(rand(m, 1) < 10 * n * log(n) / m); >> A1 = A(sampled rows, :); b1 = b(sampled rows); >> x = A \\ b; >> x1 = A1 \\ b1; >> norm(A * x1 -b) / norm(A * x -b) ans = 1.0084\n\n\nFig. 5 . 1 .\n51\nFig. 5.1.Time spent on the fast unitary transformation (row mixing) for increasingly larger matrices.We tested all three implemented transforms: WHT, DCT, and DHT.\n\n\n\n\nFig. 5.2.Overall running time of the algorithm with different fast unitary transforms (row mixing) on increasingly larger matrices.We tested on incoherent matrices (top left graph), semicoherent matrices (top right graph), and coherent matrices (bottom graph).\n\n\nFig. 5 . 3 .\n53\nFig. 5.3.Running time as a function of sample size and number of row-mixing steps for 30,000 \u00d7 750 matrices (left graphs) and 40,000 \u00d7 2,000 matrices (right graphs).We ran the same experiment on incoherent matrices (top left and middle right graphs), semicoherent matrices (top right and bottom left graph), and coherent matrices (middle left and bottom right graphs).\n\n\nFig. 5 . 4 .\n54\nFig. 5.4.Running time on increasingly ill-conditioned matrices.\n\n\nFig. 5 . 5 .\n55\nFig. 5.5.Running time on different coherence profiles.\n\n\nFig. 5 . 6 .\n56\nFig. 5.6.Convergence rate experiments.The left graph showsA T r (i) 2 / A F r (i) 2, where r(i) is the residual after the ith iteration of LSQR on three 100,000 \u00d7 2,500 matrices of different coherence profiles.The right graph shows the number of LSQR iterations needed for convergence on increasingly larger matrices.\n\n\nFig. 5 . 7 .\n57\nFig. 5.7.Breakdown of running time on increasingly larger matrices.The plotted series shows the running time of each phase.The left graph shows the breakdown for incoherent matrices, while the right graph shows the breakdown for coherent matrices.\n\n\nFig. 5 . 8 .\n58\nFig. 5.8.Experiments examining strategies with no row mixing versus the regular strategy.The left graph compares the solver without a row-mixing phase to the solver with a row-mixing phase on incoherent matrices.The right graph compares the same two solvers on matrices with a few important rows.\n\n. Algorithm and implementation. In this section we summarize the three major steps of the algorithm: row mixing (preprocessing), row sampling and QR factorization, and iterative solution. We also discuss how we handle random-sampling failures. The overall solver is presented in Algorithm 1.Implementation.Our solver currently runs under matlab 7.7[16], but it is implemented almost entirely in C. The C code is called from matlab using matlab's cmex interface.\nAcknowledgments.This research was motivated by discussions with Michael Mahoney concerning the theoretical analysis of random sampling algorithms for leastsquares regression.We thank Mike for these discussions, and we thank Schloss Dagstuhl for making them possible (as part of the workshop on Combinatorial Scientific Computing).It is also a pleasure to thank Vladimir Rokhlin and Mark Tygert for illuminating discussions of their results.We thank the referees for valuable comments and suggestions.. This research was supported in part by an IBM Faculty Partnership Award and by grant 1045/09 from the Israel Science Foundation (founded by the Israel Academy of Sciences and Humanities).6. Discussion and related work.Experiments show that our solver is faster than lapack and faster than LSQR with no preconditioning.The algorithm is robust and predictable.The algorithm is competitive in the usual metric of numerical linear algebra, and it demonstrates that randomized algorithms can be effective in numerical linear algebra software.We have not encountered cases of large dense matrices where the solver fails to beat lapack, even in hard test cases, and we have not encountered large variance in running time of the algorithm on a given matrix.Even the convergence rate in the iterative phase is stable and predictable (unlike many algorithms that use an iterative method).Although, the numerical experiments demonstrate the validity of the worst-case theoretical analysis, they also demonstrate that actual performance is not fully described by it.In some issues actual performance acts differently than suggested by theoretical bounds, or the observed behavior is not explained by the analysis:\u2022 The theoretical analysis suggests that WHT is better in reducing coherence.In practice DHT and DCT work better, even though it takes longer to compute them.In fact, on highly coherent matrices, WHT sometimes fails to mix rows well enough (so we need to apply it again), while this never happened for DHT and DCT.\u2022 The algorithm may fail with some small probability.It may fail to produce an incoherent matrix after row sampling, and important rows may be left out of the random sample (thereby producing a poor preconditioner).Some failures may slow down the solver considerably (for example, when the preconditioner is rank deficient and another row-mixing phase is necessary), but it is impossible for the algorithm not to finish in finite time on full rank matrices.Current theory does not guarantee that the probability of slowdown is negligible.When using WHT for row mixing, the solver did slow down sometimes due to such failures.When DHT is used for row mixing, we did not encounter such failures, and running time was always good, with a small variance.Apparently the actual probability of failure is much smaller than the theoretical bounds.\u2022 Theoretical bounds require a superlinear sample size.In practice, a linear sample works better.It is unclear whether the reason is that the bounds are not tight or whether constants come into play.\u2022 The theory relates performance to the coherence of the matrix.Coherence uses the maximum function, which from our experiment is too crude for analyzing random sampling.Actual performance depends on the distribution of row norms in the orthogonal factor, not just the maximum values.In a sense, the role coherence is similar to the role of the condition number in Krylov methods: it provides bounds using extreme values (easy to handle) while actual performance depends on internal values (hard to handle).The algorithm used by our solver is new, but its building blocks are not.We chose building blocks that are geared toward an efficient implementation.Using WHT for row mixing (and padding the matrix by zeros) was suggested by Drineas et al.[10].Their complete method is not suitable for a general-purpose solver because sample size depends on the required accuracy.Using DCT or DHT for row mixing in low rank matrix approximations was suggested by Nguyen, Do, and Tran[17].Their observation carries to a least-squares solution.DHT has a smaller memory footprint than WHT, and it works better than WHT and DCT, so we decided to use it.Using the sampled matrix as a preconditioner for an iterative Krylov-subspace method was suggested by Rokhlin and Tygert[22].They use CGLS; we decided to use LSQR\nApproximate nearest neighbors and the fast Johnson-Lindenstrauss transform. N Ailon, B Chazelle, Proceedings of the 38th Annual ACM Symposium on Theory of Computing. the 38th Annual ACM Symposium on Theory of ComputingNew York2006\n\nFast dimension reduction using Rademacher series on dual BCH codes. N Ailon, E Liberty, Proceedings of the 19th Annual ACM-SIAM Symposium on Discrete Algorithms. the 19th Annual ACM-SIAM Symposium on Discrete AlgorithmsSIAM, Philadelphia2008\n\nUsing perturbed QR factorizations to solve linear leastsquares problems. H Avron, E Ng, S Toledo, SIAM J. Matrix Anal. Appl. 312009\n\nEfficient and Robust Hybrid Iterative-Direct Multipurpose Linear Solvers. H Avron, October 2010IsraelTel-Aviv UniversityPh.D. thesis\n\nRandom projections for the nonnegative least-squares problem. C Boutsidis, P Drineas, Linear Algebra Appl. 4312009\n\nExact matrix completion via convex optimization. E J Cand\u00e8s, B Recht, CoRR, abs/0805.44712008Tecnical report\n\nStopping criteria for the iterative solution of linear least squares problems. X.-W Chang, C C Paige, D Titley-Peloquin, SIAM J. Matrix Anal. Appl. 312009\n\nSampling algorithms and coresets for p regression. A Dasgupta, P Drineas, B Harb, R Kumar, M W Mahoney, SIAM J. Comput. 382009\n\nFast linear algebra is stable. J Demmel, I Dumitriu, O Holtz, Numer. Math. 1082007\n\nFaster least squares approximation. P Drineas, M W Mahoney, S Muthukrishnan, T Sarl\u00f3s, CoRR, abs/0710.14352007Tecnical report\n\nSampling algorithms for l2 regression and applications. P Drineas, M W Mahoney, S Muthukrishnan, Proceedings of the 17th Annual ACM-SIAM Symposium on Discrete Algorithms. the 17th Annual ACM-SIAM Symposium on Discrete AlgorithmsNew YorkACM2006\n\nRelative-error CUR matrix decompositions. P Drineas, M W Mahoney, S Muthukrishnan, SIAM J. Matrix Anal. Appl. 302008\n\nIn search of the optimal Walsh-Hadamard transform. J Johnson, M Puschel, Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing. the IEEE International Conference on Acoustics, Speech, and Signal ProcessingWashington, DC2000\n\nProbability and Related Topics in Physical Science. M Kac, 1959Wiley InterscienceNew York\n\nCUR matrix decompositions for improved data analysis. M W Mahoney, P Drineas, Proc. Nat. Acad. Sci. Nat. Acad. SciUSA2009\n\nMATLAB. 2008version 7.7, software package, The MathWorks\n\nA fast and efficient algorithm for low-rank approximation of a matrix. N H Nguyen, T T Do, T D Tran, Proceedings of the 41st ACM Symposium on Theory of Computing. the 41st ACM Symposium on Theory of ComputingNew York2009\n\nLSQR: An algorithm for sparse linear equations and sparse least squares. C C Paige, M A Saunders, ACM Trans. Math. Software. 81982\n\nThe Computation of Eigenvalues and Eigenvectors of Very Large Sparse Matrices. C C Paige, 1971University of LondonPh.D. thesis\n\nThe Lanczos algorithm with selective orthogonalization. B N Parlett, D S Scott, Math. Comp. 331979\n\nA randomized algorithm for principal component analysis. A V. Rokhlin, M Szlam, Tygert, SIAM J. Matrix Anal. Appl. 312009\n\nA fast randomized algorithm for overdetermined linear leastsquares regression. V Rokhlin, M Tygert, Proc. Nat. Acad. Sci. Nat. Acad. SciUSA2008\n\nImproved approximation algorithms for large matrices via random projections. T Sarlos, Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science. the 47th Annual IEEE Symposium on Foundations of Computer ScienceWashington, DC2006\n\nEfficient computation of the DFT with only a subset of input or output points. H V Sorensen, C S Burrus, IEEE Trans. Signal Proces. 411993\n\n. G W Stewart, Matrix Algorithms. II2001Eigensystems\n\nMinimizing development and maintenance costs in supporting persistently optimized BLAS, Software: Practice and Experience. R C Whaley, A Petitet, 200535\n", "annotations": {"author": "[{\"end\":239,\"start\":75},{\"end\":533,\"start\":240}]", "publisher": null, "author_last_name": "[{\"end\":85,\"start\":80},{\"end\":270,\"start\":264}]", "author_first_name": "[{\"end\":79,\"start\":75},{\"end\":263,\"start\":258}]", "author_affiliation": "[{\"end\":238,\"start\":108},{\"end\":420,\"start\":290},{\"end\":532,\"start\":422}]", "title": "[{\"end\":58,\"start\":1},{\"end\":591,\"start\":534}]", "venue": null, "abstract": "[{\"end\":1719,\"start\":890}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2286,\"start\":2282},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2658,\"start\":2654},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2753,\"start\":2749},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2972,\"start\":2968},{\"end\":3140,\"start\":3126},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4166,\"start\":4162},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4211,\"start\":4207},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4342,\"start\":4338},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6103,\"start\":6099},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6106,\"start\":6103},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7235,\"start\":7231},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7238,\"start\":7235},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7241,\"start\":7238},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7437,\"start\":7434},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7731,\"start\":7728},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9129,\"start\":9125},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9132,\"start\":9129},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9229,\"start\":9226},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9352,\"start\":9348},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9443,\"start\":9439},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9658,\"start\":9654},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10336,\"start\":10333},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10643,\"start\":10639},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13125,\"start\":13121},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13147,\"start\":13143},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14972,\"start\":14968},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15362,\"start\":15359},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16450,\"start\":16447},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16917,\"start\":16914},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16973,\"start\":16970},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18720,\"start\":18717},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20293,\"start\":20289},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23085,\"start\":23081},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23363,\"start\":23359},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23393,\"start\":23390},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23911,\"start\":23908},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24298,\"start\":24294},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27733,\"start\":27729},{\"end\":34445,\"start\":34435},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":39053,\"start\":39049},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":39526,\"start\":39522},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":39563,\"start\":39559},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":39729,\"start\":39725},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":40207,\"start\":40203},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":40264,\"start\":40260},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":43370,\"start\":43366}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":40731,\"start\":40514},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41111,\"start\":40732},{\"attributes\":{\"id\":\"fig_2\"},\"end\":41293,\"start\":41112},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41558,\"start\":41294},{\"attributes\":{\"id\":\"fig_4\"},\"end\":41945,\"start\":41559},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42027,\"start\":41946},{\"attributes\":{\"id\":\"fig_6\"},\"end\":42100,\"start\":42028},{\"attributes\":{\"id\":\"fig_7\"},\"end\":42436,\"start\":42101},{\"attributes\":{\"id\":\"fig_8\"},\"end\":42702,\"start\":42437},{\"attributes\":{\"id\":\"fig_9\"},\"end\":43017,\"start\":42703}]", "paragraph": "[{\"end\":1926,\"start\":1721},{\"end\":2901,\"start\":1928},{\"end\":3025,\"start\":2903},{\"end\":3193,\"start\":3060},{\"end\":3602,\"start\":3233},{\"end\":3680,\"start\":3623},{\"end\":3928,\"start\":3729},{\"end\":4186,\"start\":3930},{\"end\":4849,\"start\":4188},{\"end\":5706,\"start\":4851},{\"end\":5877,\"start\":5782},{\"end\":6677,\"start\":5879},{\"end\":6895,\"start\":6679},{\"end\":7050,\"start\":6897},{\"end\":7438,\"start\":7057},{\"end\":7992,\"start\":7476},{\"end\":8255,\"start\":8017},{\"end\":8613,\"start\":8257},{\"end\":8968,\"start\":8615},{\"end\":9834,\"start\":9005},{\"end\":10434,\"start\":9836},{\"end\":11102,\"start\":10436},{\"end\":11980,\"start\":11104},{\"end\":12620,\"start\":11982},{\"end\":13167,\"start\":12622},{\"end\":13469,\"start\":13169},{\"end\":13712,\"start\":13544},{\"end\":13777,\"start\":13714},{\"end\":14279,\"start\":13850},{\"end\":14602,\"start\":14336},{\"end\":14906,\"start\":14604},{\"end\":14982,\"start\":14908},{\"end\":15700,\"start\":15029},{\"end\":16451,\"start\":15738},{\"end\":16554,\"start\":16453},{\"end\":16824,\"start\":16694},{\"end\":16918,\"start\":16836},{\"end\":17233,\"start\":16920},{\"end\":17457,\"start\":17235},{\"end\":17789,\"start\":17580},{\"end\":18536,\"start\":17791},{\"end\":18733,\"start\":18538},{\"end\":19150,\"start\":18965},{\"end\":19980,\"start\":19390},{\"end\":20434,\"start\":19982},{\"end\":20750,\"start\":20436},{\"end\":22221,\"start\":20752},{\"end\":22613,\"start\":22223},{\"end\":23177,\"start\":22615},{\"end\":23591,\"start\":23179},{\"end\":23746,\"start\":23593},{\"end\":23814,\"start\":23771},{\"end\":24490,\"start\":23847},{\"end\":24568,\"start\":24523},{\"end\":25101,\"start\":24599},{\"end\":26080,\"start\":25103},{\"end\":26318,\"start\":26107},{\"end\":26742,\"start\":26342},{\"end\":26981,\"start\":26744},{\"end\":27084,\"start\":26983},{\"end\":27430,\"start\":27086},{\"end\":27761,\"start\":27432},{\"end\":28320,\"start\":27763},{\"end\":28632,\"start\":28391},{\"end\":29184,\"start\":28711},{\"end\":29548,\"start\":29208},{\"end\":29868,\"start\":29581},{\"end\":31075,\"start\":30127},{\"end\":31390,\"start\":31077},{\"end\":32146,\"start\":31438},{\"end\":32748,\"start\":32148},{\"end\":33114,\"start\":32750},{\"end\":33418,\"start\":33116},{\"end\":34101,\"start\":33420},{\"end\":34401,\"start\":34103},{\"end\":34702,\"start\":34410},{\"end\":35517,\"start\":34711},{\"end\":36142,\"start\":35539},{\"end\":37319,\"start\":36167},{\"end\":37679,\"start\":37321},{\"end\":37878,\"start\":37681},{\"end\":38255,\"start\":37897},{\"end\":39849,\"start\":38257},{\"end\":40002,\"start\":39851},{\"end\":40513,\"start\":40004},{\"end\":40730,\"start\":40517},{\"end\":41110,\"start\":40735},{\"end\":41292,\"start\":41129},{\"end\":41557,\"start\":41297},{\"end\":41944,\"start\":41576},{\"end\":42026,\"start\":41963},{\"end\":42099,\"start\":42045},{\"end\":42435,\"start\":42118},{\"end\":42701,\"start\":42454},{\"end\":43016,\"start\":42720}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3059,\"start\":3026},{\"attributes\":{\"id\":\"formula_1\"},\"end\":3232,\"start\":3194},{\"attributes\":{\"id\":\"formula_2\"},\"end\":3622,\"start\":3603},{\"attributes\":{\"id\":\"formula_3\"},\"end\":3728,\"start\":3681},{\"attributes\":{\"id\":\"formula_4\"},\"end\":5781,\"start\":5707},{\"attributes\":{\"id\":\"formula_5\"},\"end\":8016,\"start\":7993},{\"attributes\":{\"id\":\"formula_6\"},\"end\":9004,\"start\":8969},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13543,\"start\":13470},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13849,\"start\":13778},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14335,\"start\":14280},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15028,\"start\":14983},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16693,\"start\":16555},{\"attributes\":{\"id\":\"formula_12\"},\"end\":16835,\"start\":16825},{\"attributes\":{\"id\":\"formula_13\"},\"end\":17579,\"start\":17458},{\"attributes\":{\"id\":\"formula_14\"},\"end\":18964,\"start\":18734},{\"attributes\":{\"id\":\"formula_15\"},\"end\":19389,\"start\":19151},{\"attributes\":{\"id\":\"formula_16\"},\"end\":23770,\"start\":23747},{\"attributes\":{\"id\":\"formula_17\"},\"end\":23846,\"start\":23815},{\"attributes\":{\"id\":\"formula_18\"},\"end\":24522,\"start\":24491},{\"attributes\":{\"id\":\"formula_19\"},\"end\":24598,\"start\":24569},{\"attributes\":{\"id\":\"formula_20\"},\"end\":28390,\"start\":28321},{\"attributes\":{\"id\":\"formula_21\"},\"end\":28710,\"start\":28633},{\"attributes\":{\"id\":\"formula_22\"},\"end\":36166,\"start\":36143}]", "table_ref": null, "section_header": "[{\"end\":7055,\"start\":7053},{\"attributes\":{\"n\":\"3.1.\"},\"end\":7474,\"start\":7441},{\"attributes\":{\"n\":\"3.3.\"},\"end\":15736,\"start\":15703},{\"attributes\":{\"n\":\"5.\"},\"end\":26105,\"start\":26083},{\"attributes\":{\"n\":\"5.1.\"},\"end\":26340,\"start\":26321},{\"attributes\":{\"n\":\"5.2.\"},\"end\":29206,\"start\":29187},{\"attributes\":{\"n\":\"5.2.1.\"},\"end\":29579,\"start\":29551},{\"end\":30125,\"start\":29871},{\"attributes\":{\"n\":\"5.2.2.\"},\"end\":31436,\"start\":31393},{\"end\":34408,\"start\":34404},{\"end\":34709,\"start\":34705},{\"attributes\":{\"n\":\"5.5.\"},\"end\":35537,\"start\":35520},{\"attributes\":{\"n\":\"5.7.\"},\"end\":37895,\"start\":37881},{\"end\":41125,\"start\":41113},{\"end\":41572,\"start\":41560},{\"end\":41959,\"start\":41947},{\"end\":42041,\"start\":42029},{\"end\":42114,\"start\":42102},{\"end\":42450,\"start\":42438},{\"end\":42716,\"start\":42704}]", "table": null, "figure_caption": "[{\"end\":40731,\"start\":40516},{\"end\":41111,\"start\":40734},{\"end\":41293,\"start\":41128},{\"end\":41558,\"start\":41296},{\"end\":41945,\"start\":41575},{\"end\":42027,\"start\":41962},{\"end\":42100,\"start\":42044},{\"end\":42436,\"start\":42117},{\"end\":42702,\"start\":42453},{\"end\":43017,\"start\":42719}]", "figure_ref": "[{\"end\":26289,\"start\":26288},{\"end\":30200,\"start\":30199},{\"end\":31343,\"start\":31342},{\"end\":32758,\"start\":32757},{\"end\":34739,\"start\":34738},{\"end\":35611,\"start\":35610},{\"end\":36139,\"start\":36138},{\"end\":36706,\"start\":36705},{\"end\":38104,\"start\":38103},{\"end\":38284,\"start\":38283}]", "bib_author_first_name": "[{\"end\":47915,\"start\":47914},{\"end\":47924,\"start\":47923},{\"end\":48139,\"start\":48138},{\"end\":48148,\"start\":48147},{\"end\":48387,\"start\":48386},{\"end\":48396,\"start\":48395},{\"end\":48402,\"start\":48401},{\"end\":48521,\"start\":48520},{\"end\":48643,\"start\":48642},{\"end\":48656,\"start\":48655},{\"end\":48746,\"start\":48745},{\"end\":48748,\"start\":48747},{\"end\":48758,\"start\":48757},{\"end\":48889,\"start\":48885},{\"end\":48898,\"start\":48897},{\"end\":48900,\"start\":48899},{\"end\":48909,\"start\":48908},{\"end\":49014,\"start\":49013},{\"end\":49026,\"start\":49025},{\"end\":49037,\"start\":49036},{\"end\":49045,\"start\":49044},{\"end\":49054,\"start\":49053},{\"end\":49056,\"start\":49055},{\"end\":49122,\"start\":49121},{\"end\":49132,\"start\":49131},{\"end\":49144,\"start\":49143},{\"end\":49211,\"start\":49210},{\"end\":49222,\"start\":49221},{\"end\":49224,\"start\":49223},{\"end\":49235,\"start\":49234},{\"end\":49252,\"start\":49251},{\"end\":49358,\"start\":49357},{\"end\":49369,\"start\":49368},{\"end\":49371,\"start\":49370},{\"end\":49382,\"start\":49381},{\"end\":49589,\"start\":49588},{\"end\":49600,\"start\":49599},{\"end\":49602,\"start\":49601},{\"end\":49613,\"start\":49612},{\"end\":49716,\"start\":49715},{\"end\":49727,\"start\":49726},{\"end\":49981,\"start\":49980},{\"end\":50074,\"start\":50073},{\"end\":50076,\"start\":50075},{\"end\":50087,\"start\":50086},{\"end\":50272,\"start\":50271},{\"end\":50274,\"start\":50273},{\"end\":50284,\"start\":50283},{\"end\":50286,\"start\":50285},{\"end\":50292,\"start\":50291},{\"end\":50294,\"start\":50293},{\"end\":50496,\"start\":50495},{\"end\":50498,\"start\":50497},{\"end\":50507,\"start\":50506},{\"end\":50509,\"start\":50508},{\"end\":50634,\"start\":50633},{\"end\":50636,\"start\":50635},{\"end\":50739,\"start\":50738},{\"end\":50741,\"start\":50740},{\"end\":50752,\"start\":50751},{\"end\":50754,\"start\":50753},{\"end\":50840,\"start\":50839},{\"end\":50854,\"start\":50853},{\"end\":50985,\"start\":50984},{\"end\":50996,\"start\":50995},{\"end\":51128,\"start\":51127},{\"end\":51384,\"start\":51383},{\"end\":51386,\"start\":51385},{\"end\":51398,\"start\":51397},{\"end\":51400,\"start\":51399},{\"end\":51447,\"start\":51446},{\"end\":51449,\"start\":51448},{\"end\":51622,\"start\":51621},{\"end\":51624,\"start\":51623},{\"end\":51634,\"start\":51633}]", "bib_author_last_name": "[{\"end\":47921,\"start\":47916},{\"end\":47933,\"start\":47925},{\"end\":48145,\"start\":48140},{\"end\":48156,\"start\":48149},{\"end\":48393,\"start\":48388},{\"end\":48399,\"start\":48397},{\"end\":48409,\"start\":48403},{\"end\":48527,\"start\":48522},{\"end\":48653,\"start\":48644},{\"end\":48664,\"start\":48657},{\"end\":48755,\"start\":48749},{\"end\":48764,\"start\":48759},{\"end\":48895,\"start\":48890},{\"end\":48906,\"start\":48901},{\"end\":48925,\"start\":48910},{\"end\":49023,\"start\":49015},{\"end\":49034,\"start\":49027},{\"end\":49042,\"start\":49038},{\"end\":49051,\"start\":49046},{\"end\":49064,\"start\":49057},{\"end\":49129,\"start\":49123},{\"end\":49141,\"start\":49133},{\"end\":49150,\"start\":49145},{\"end\":49219,\"start\":49212},{\"end\":49232,\"start\":49225},{\"end\":49249,\"start\":49236},{\"end\":49259,\"start\":49253},{\"end\":49366,\"start\":49359},{\"end\":49379,\"start\":49372},{\"end\":49396,\"start\":49383},{\"end\":49597,\"start\":49590},{\"end\":49610,\"start\":49603},{\"end\":49627,\"start\":49614},{\"end\":49724,\"start\":49717},{\"end\":49735,\"start\":49728},{\"end\":49985,\"start\":49982},{\"end\":50084,\"start\":50077},{\"end\":50095,\"start\":50088},{\"end\":50281,\"start\":50275},{\"end\":50289,\"start\":50287},{\"end\":50299,\"start\":50295},{\"end\":50504,\"start\":50499},{\"end\":50518,\"start\":50510},{\"end\":50642,\"start\":50637},{\"end\":50749,\"start\":50742},{\"end\":50760,\"start\":50755},{\"end\":50851,\"start\":50841},{\"end\":50860,\"start\":50855},{\"end\":50868,\"start\":50862},{\"end\":50993,\"start\":50986},{\"end\":51003,\"start\":50997},{\"end\":51135,\"start\":51129},{\"end\":51395,\"start\":51387},{\"end\":51407,\"start\":51401},{\"end\":51457,\"start\":51450},{\"end\":51631,\"start\":51625},{\"end\":51642,\"start\":51635}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":490517},\"end\":48068,\"start\":47838},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":263888029},\"end\":48311,\"start\":48070},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":9832977},\"end\":48444,\"start\":48313},{\"attributes\":{\"id\":\"b3\"},\"end\":48578,\"start\":48446},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":6826978},\"end\":48694,\"start\":48580},{\"attributes\":{\"doi\":\"CoRR, abs/0805.4471\",\"id\":\"b5\"},\"end\":48804,\"start\":48696},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14490551},\"end\":48960,\"start\":48806},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":10818093},\"end\":49088,\"start\":48962},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6057731},\"end\":49172,\"start\":49090},{\"attributes\":{\"doi\":\"CoRR, abs/0710.1435\",\"id\":\"b9\"},\"end\":49299,\"start\":49174},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2415336},\"end\":49544,\"start\":49301},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":58270},\"end\":49662,\"start\":49546},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":14346369},\"end\":49926,\"start\":49664},{\"attributes\":{\"id\":\"b13\"},\"end\":50017,\"start\":49928},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2502987},\"end\":50140,\"start\":50019},{\"attributes\":{\"id\":\"b15\"},\"end\":50198,\"start\":50142},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":12902805},\"end\":50420,\"start\":50200},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":21774},\"end\":50552,\"start\":50422},{\"attributes\":{\"id\":\"b18\"},\"end\":50680,\"start\":50554},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":122260461},\"end\":50780,\"start\":50682},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":16748115},\"end\":50903,\"start\":50782},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":8351640},\"end\":51048,\"start\":50905},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1299951},\"end\":51302,\"start\":51050},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":46045168},\"end\":51442,\"start\":51304},{\"attributes\":{\"id\":\"b24\"},\"end\":51496,\"start\":51444},{\"attributes\":{\"id\":\"b25\"},\"end\":51650,\"start\":51498}]", "bib_title": "[{\"end\":47912,\"start\":47838},{\"end\":48136,\"start\":48070},{\"end\":48384,\"start\":48313},{\"end\":48640,\"start\":48580},{\"end\":48883,\"start\":48806},{\"end\":49011,\"start\":48962},{\"end\":49119,\"start\":49090},{\"end\":49355,\"start\":49301},{\"end\":49586,\"start\":49546},{\"end\":49713,\"start\":49664},{\"end\":50071,\"start\":50019},{\"end\":50269,\"start\":50200},{\"end\":50493,\"start\":50422},{\"end\":50736,\"start\":50682},{\"end\":50837,\"start\":50782},{\"end\":50982,\"start\":50905},{\"end\":51125,\"start\":51050},{\"end\":51381,\"start\":51304}]", "bib_author": "[{\"end\":47923,\"start\":47914},{\"end\":47935,\"start\":47923},{\"end\":48147,\"start\":48138},{\"end\":48158,\"start\":48147},{\"end\":48395,\"start\":48386},{\"end\":48401,\"start\":48395},{\"end\":48411,\"start\":48401},{\"end\":48529,\"start\":48520},{\"end\":48655,\"start\":48642},{\"end\":48666,\"start\":48655},{\"end\":48757,\"start\":48745},{\"end\":48766,\"start\":48757},{\"end\":48897,\"start\":48885},{\"end\":48908,\"start\":48897},{\"end\":48927,\"start\":48908},{\"end\":49025,\"start\":49013},{\"end\":49036,\"start\":49025},{\"end\":49044,\"start\":49036},{\"end\":49053,\"start\":49044},{\"end\":49066,\"start\":49053},{\"end\":49131,\"start\":49121},{\"end\":49143,\"start\":49131},{\"end\":49152,\"start\":49143},{\"end\":49221,\"start\":49210},{\"end\":49234,\"start\":49221},{\"end\":49251,\"start\":49234},{\"end\":49261,\"start\":49251},{\"end\":49368,\"start\":49357},{\"end\":49381,\"start\":49368},{\"end\":49398,\"start\":49381},{\"end\":49599,\"start\":49588},{\"end\":49612,\"start\":49599},{\"end\":49629,\"start\":49612},{\"end\":49726,\"start\":49715},{\"end\":49737,\"start\":49726},{\"end\":49987,\"start\":49980},{\"end\":50086,\"start\":50073},{\"end\":50097,\"start\":50086},{\"end\":50283,\"start\":50271},{\"end\":50291,\"start\":50283},{\"end\":50301,\"start\":50291},{\"end\":50506,\"start\":50495},{\"end\":50520,\"start\":50506},{\"end\":50644,\"start\":50633},{\"end\":50751,\"start\":50738},{\"end\":50762,\"start\":50751},{\"end\":50853,\"start\":50839},{\"end\":50862,\"start\":50853},{\"end\":50870,\"start\":50862},{\"end\":50995,\"start\":50984},{\"end\":51005,\"start\":50995},{\"end\":51137,\"start\":51127},{\"end\":51397,\"start\":51383},{\"end\":51409,\"start\":51397},{\"end\":51459,\"start\":51446},{\"end\":51633,\"start\":51621},{\"end\":51644,\"start\":51633}]", "bib_venue": "[{\"end\":48002,\"start\":47935},{\"end\":48230,\"start\":48158},{\"end\":48436,\"start\":48411},{\"end\":48518,\"start\":48446},{\"end\":48685,\"start\":48666},{\"end\":48743,\"start\":48696},{\"end\":48952,\"start\":48927},{\"end\":49080,\"start\":49066},{\"end\":49163,\"start\":49152},{\"end\":49208,\"start\":49174},{\"end\":49470,\"start\":49398},{\"end\":49654,\"start\":49629},{\"end\":49829,\"start\":49737},{\"end\":49978,\"start\":49928},{\"end\":50117,\"start\":50097},{\"end\":50148,\"start\":50142},{\"end\":50361,\"start\":50301},{\"end\":50545,\"start\":50520},{\"end\":50631,\"start\":50554},{\"end\":50772,\"start\":50762},{\"end\":50895,\"start\":50870},{\"end\":51025,\"start\":51005},{\"end\":51217,\"start\":51137},{\"end\":51434,\"start\":51409},{\"end\":51476,\"start\":51459},{\"end\":51619,\"start\":51498},{\"end\":48064,\"start\":48004},{\"end\":48307,\"start\":48232},{\"end\":49537,\"start\":49472},{\"end\":49922,\"start\":49831},{\"end\":50136,\"start\":50119},{\"end\":50416,\"start\":50363},{\"end\":51044,\"start\":51027},{\"end\":51298,\"start\":51219}]"}}}, "year": 2023, "month": 12, "day": 17}